[ Thu May 16 14:25:44 2024 ] Eval epoch: 1
[ Thu May 16 14:27:08 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/train_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/train_label.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/val_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/val_label.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 60, 'channel': 6, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': False, 'only_attention': True, 'tcn_attention': True, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': True, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'scheduler': 1, 'base_lr': 0.01, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 16, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': False, 'display_recall_precision': False}

[ Thu May 16 14:27:08 2024 ] Training epoch: 1
[ Thu May 16 14:27:09 2024 ] 	Batch(0/2353) done. Loss: 8.0453  lr:0.010000
[ Thu May 16 14:27:45 2024 ] 	Batch(100/2353) done. Loss: 4.3870  lr:0.010000
[ Thu May 16 14:28:21 2024 ] 	Batch(200/2353) done. Loss: 3.8464  lr:0.010000
[ Thu May 16 14:28:58 2024 ] 	Batch(300/2353) done. Loss: 2.8736  lr:0.010000
[ Thu May 16 14:29:34 2024 ] 	Batch(400/2353) done. Loss: 2.8629  lr:0.010000
[ Thu May 16 14:30:10 2024 ] 	Batch(500/2353) done. Loss: 2.8655  lr:0.010000
[ Thu May 16 14:30:46 2024 ] 	Batch(600/2353) done. Loss: 2.9976  lr:0.010000
[ Thu May 16 14:31:22 2024 ] 	Batch(700/2353) done. Loss: 2.7406  lr:0.010000
[ Thu May 16 14:31:58 2024 ] 	Batch(800/2353) done. Loss: 2.2956  lr:0.010000
[ Thu May 16 14:32:34 2024 ] 	Batch(900/2353) done. Loss: 2.9124  lr:0.010000
[ Thu May 16 14:33:10 2024 ] 	Batch(1000/2353) done. Loss: 2.8283  lr:0.010000
[ Thu May 16 14:33:46 2024 ] 	Batch(1100/2353) done. Loss: 1.8752  lr:0.010000
[ Thu May 16 14:34:22 2024 ] 	Batch(1200/2353) done. Loss: 1.9238  lr:0.010000
[ Thu May 16 14:34:57 2024 ] 	Batch(1300/2353) done. Loss: 2.6794  lr:0.010000
[ Thu May 16 14:35:33 2024 ] 	Batch(1400/2353) done. Loss: 2.4435  lr:0.010000
[ Thu May 16 14:36:09 2024 ] 	Batch(1500/2353) done. Loss: 2.3462  lr:0.010000
[ Thu May 16 14:36:45 2024 ] 	Batch(1600/2353) done. Loss: 2.1299  lr:0.010000
[ Thu May 16 14:37:21 2024 ] 	Batch(1700/2353) done. Loss: 3.2404  lr:0.010000
[ Thu May 16 14:37:57 2024 ] 	Batch(1800/2353) done. Loss: 1.9210  lr:0.010000
[ Thu May 16 14:38:33 2024 ] 	Batch(1900/2353) done. Loss: 3.1352  lr:0.010000
[ Thu May 16 14:39:09 2024 ] 	Batch(2000/2353) done. Loss: 2.8967  lr:0.010000
[ Thu May 16 14:39:45 2024 ] 	Batch(2100/2353) done. Loss: 1.8287  lr:0.010000
[ Thu May 16 14:40:21 2024 ] 	Batch(2200/2353) done. Loss: 1.4302  lr:0.010000
[ Thu May 16 14:40:57 2024 ] 	Batch(2300/2353) done. Loss: 1.6788  lr:0.010000
[ Thu May 16 14:41:16 2024 ] 	Mean training loss: 2.5941.
[ Thu May 16 14:41:16 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Thu May 16 14:41:16 2024 ] Training epoch: 2
[ Thu May 16 14:41:17 2024 ] 	Batch(0/2353) done. Loss: 1.5238  lr:0.010000
[ Thu May 16 14:41:54 2024 ] 	Batch(100/2353) done. Loss: 1.9044  lr:0.010000
[ Thu May 16 14:42:31 2024 ] 	Batch(200/2353) done. Loss: 2.7546  lr:0.010000
[ Thu May 16 14:43:09 2024 ] 	Batch(300/2353) done. Loss: 1.9272  lr:0.010000
[ Thu May 16 14:43:46 2024 ] 	Batch(400/2353) done. Loss: 1.6252  lr:0.010000
[ Thu May 16 14:44:24 2024 ] 	Batch(500/2353) done. Loss: 1.6715  lr:0.010000
[ Thu May 16 14:45:01 2024 ] 	Batch(600/2353) done. Loss: 0.9362  lr:0.010000
[ Thu May 16 14:45:39 2024 ] 	Batch(700/2353) done. Loss: 1.4300  lr:0.010000
[ Thu May 16 14:46:16 2024 ] 	Batch(800/2353) done. Loss: 1.5960  lr:0.010000
[ Thu May 16 14:46:54 2024 ] 	Batch(900/2353) done. Loss: 1.3496  lr:0.010000
[ Thu May 16 14:47:31 2024 ] 	Batch(1000/2353) done. Loss: 1.5370  lr:0.010000
[ Thu May 16 14:48:09 2024 ] 	Batch(1100/2353) done. Loss: 1.1373  lr:0.010000
[ Thu May 16 14:48:46 2024 ] 	Batch(1200/2353) done. Loss: 2.2140  lr:0.010000
[ Thu May 16 14:49:23 2024 ] 	Batch(1300/2353) done. Loss: 1.4129  lr:0.010000
[ Thu May 16 14:50:01 2024 ] 	Batch(1400/2353) done. Loss: 1.6446  lr:0.010000
[ Thu May 16 14:50:38 2024 ] 	Batch(1500/2353) done. Loss: 1.0163  lr:0.010000
[ Thu May 16 14:51:16 2024 ] 	Batch(1600/2353) done. Loss: 2.2866  lr:0.010000
[ Thu May 16 14:51:53 2024 ] 	Batch(1700/2353) done. Loss: 1.3284  lr:0.010000
[ Thu May 16 14:52:31 2024 ] 	Batch(1800/2353) done. Loss: 1.7760  lr:0.010000
[ Thu May 16 14:53:08 2024 ] 	Batch(1900/2353) done. Loss: 1.6292  lr:0.010000
[ Thu May 16 14:53:46 2024 ] 	Batch(2000/2353) done. Loss: 1.0739  lr:0.010000
[ Thu May 16 14:54:23 2024 ] 	Batch(2100/2353) done. Loss: 1.6983  lr:0.010000
[ Thu May 16 14:55:00 2024 ] 	Batch(2200/2353) done. Loss: 1.6638  lr:0.010000
[ Thu May 16 14:55:38 2024 ] 	Batch(2300/2353) done. Loss: 1.3504  lr:0.010000
[ Thu May 16 14:55:57 2024 ] 	Mean training loss: 1.5873.
[ Thu May 16 14:55:57 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 14:55:57 2024 ] Training epoch: 3
[ Thu May 16 14:55:58 2024 ] 	Batch(0/2353) done. Loss: 1.0187  lr:0.010000
[ Thu May 16 14:56:36 2024 ] 	Batch(100/2353) done. Loss: 1.4437  lr:0.010000
[ Thu May 16 14:57:14 2024 ] 	Batch(200/2353) done. Loss: 1.0443  lr:0.010000
[ Thu May 16 14:57:52 2024 ] 	Batch(300/2353) done. Loss: 0.9453  lr:0.010000
[ Thu May 16 14:58:31 2024 ] 	Batch(400/2353) done. Loss: 1.1854  lr:0.010000
[ Thu May 16 14:59:09 2024 ] 	Batch(500/2353) done. Loss: 0.8907  lr:0.010000
[ Thu May 16 14:59:47 2024 ] 	Batch(600/2353) done. Loss: 1.5767  lr:0.010000
[ Thu May 16 15:00:25 2024 ] 	Batch(700/2353) done. Loss: 1.5469  lr:0.010000
[ Thu May 16 15:01:03 2024 ] 	Batch(800/2353) done. Loss: 1.2413  lr:0.010000
[ Thu May 16 15:01:41 2024 ] 	Batch(900/2353) done. Loss: 1.3522  lr:0.010000
[ Thu May 16 15:02:19 2024 ] 	Batch(1000/2353) done. Loss: 1.5159  lr:0.010000
[ Thu May 16 15:02:57 2024 ] 	Batch(1100/2353) done. Loss: 1.3419  lr:0.010000
[ Thu May 16 15:03:36 2024 ] 	Batch(1200/2353) done. Loss: 0.7659  lr:0.010000
[ Thu May 16 15:04:13 2024 ] 	Batch(1300/2353) done. Loss: 1.8220  lr:0.010000
[ Thu May 16 15:04:51 2024 ] 	Batch(1400/2353) done. Loss: 1.0080  lr:0.010000
[ Thu May 16 15:05:28 2024 ] 	Batch(1500/2353) done. Loss: 1.2016  lr:0.010000
[ Thu May 16 15:06:06 2024 ] 	Batch(1600/2353) done. Loss: 0.8078  lr:0.010000
[ Thu May 16 15:06:44 2024 ] 	Batch(1700/2353) done. Loss: 1.7723  lr:0.010000
[ Thu May 16 15:07:22 2024 ] 	Batch(1800/2353) done. Loss: 1.7105  lr:0.010000
[ Thu May 16 15:08:00 2024 ] 	Batch(1900/2353) done. Loss: 1.6804  lr:0.010000
[ Thu May 16 15:08:38 2024 ] 	Batch(2000/2353) done. Loss: 2.2059  lr:0.010000
[ Thu May 16 15:09:16 2024 ] 	Batch(2100/2353) done. Loss: 0.9268  lr:0.010000
[ Thu May 16 15:09:53 2024 ] 	Batch(2200/2353) done. Loss: 0.6079  lr:0.010000
[ Thu May 16 15:10:30 2024 ] 	Batch(2300/2353) done. Loss: 1.2556  lr:0.010000
[ Thu May 16 15:10:50 2024 ] 	Mean training loss: 1.2824.
[ Thu May 16 15:10:50 2024 ] 	Time consumption: [Data]01%, [Network]92%
[ Thu May 16 15:10:50 2024 ] Training epoch: 4
[ Thu May 16 15:10:51 2024 ] 	Batch(0/2353) done. Loss: 0.7662  lr:0.010000
[ Thu May 16 15:11:28 2024 ] 	Batch(100/2353) done. Loss: 1.1007  lr:0.010000
[ Thu May 16 15:12:06 2024 ] 	Batch(200/2353) done. Loss: 1.2141  lr:0.010000
[ Thu May 16 15:12:43 2024 ] 	Batch(300/2353) done. Loss: 1.7155  lr:0.010000
[ Thu May 16 15:13:20 2024 ] 	Batch(400/2353) done. Loss: 1.2285  lr:0.010000
[ Thu May 16 15:13:58 2024 ] 	Batch(500/2353) done. Loss: 0.7902  lr:0.010000
[ Thu May 16 15:14:35 2024 ] 	Batch(600/2353) done. Loss: 0.4512  lr:0.010000
[ Thu May 16 15:15:13 2024 ] 	Batch(700/2353) done. Loss: 1.2353  lr:0.010000
[ Thu May 16 15:15:50 2024 ] 	Batch(800/2353) done. Loss: 0.9421  lr:0.010000
[ Thu May 16 15:16:27 2024 ] 	Batch(900/2353) done. Loss: 1.6457  lr:0.010000
[ Thu May 16 15:17:05 2024 ] 	Batch(1000/2353) done. Loss: 1.6837  lr:0.010000
[ Thu May 16 15:17:42 2024 ] 	Batch(1100/2353) done. Loss: 0.7342  lr:0.010000
[ Thu May 16 15:18:20 2024 ] 	Batch(1200/2353) done. Loss: 1.2076  lr:0.010000
[ Thu May 16 15:18:57 2024 ] 	Batch(1300/2353) done. Loss: 0.7920  lr:0.010000
[ Thu May 16 15:19:34 2024 ] 	Batch(1400/2353) done. Loss: 1.7356  lr:0.010000
[ Thu May 16 15:20:12 2024 ] 	Batch(1500/2353) done. Loss: 0.9413  lr:0.010000
[ Thu May 16 15:20:49 2024 ] 	Batch(1600/2353) done. Loss: 1.1189  lr:0.010000
[ Thu May 16 15:21:27 2024 ] 	Batch(1700/2353) done. Loss: 1.6207  lr:0.010000
[ Thu May 16 15:22:04 2024 ] 	Batch(1800/2353) done. Loss: 1.0336  lr:0.010000
[ Thu May 16 15:22:42 2024 ] 	Batch(1900/2353) done. Loss: 1.8521  lr:0.010000
[ Thu May 16 15:23:19 2024 ] 	Batch(2000/2353) done. Loss: 0.5904  lr:0.010000
[ Thu May 16 15:23:56 2024 ] 	Batch(2100/2353) done. Loss: 0.7327  lr:0.010000
[ Thu May 16 15:24:34 2024 ] 	Batch(2200/2353) done. Loss: 0.9496  lr:0.010000
[ Thu May 16 15:25:12 2024 ] 	Batch(2300/2353) done. Loss: 0.8442  lr:0.010000
[ Thu May 16 15:25:31 2024 ] 	Mean training loss: 1.1081.
[ Thu May 16 15:25:31 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 15:25:32 2024 ] Training epoch: 5
[ Thu May 16 15:25:32 2024 ] 	Batch(0/2353) done. Loss: 1.1669  lr:0.010000
[ Thu May 16 15:26:10 2024 ] 	Batch(100/2353) done. Loss: 0.9688  lr:0.010000
[ Thu May 16 15:26:47 2024 ] 	Batch(200/2353) done. Loss: 0.4237  lr:0.010000
[ Thu May 16 15:27:24 2024 ] 	Batch(300/2353) done. Loss: 0.5993  lr:0.010000
[ Thu May 16 15:28:02 2024 ] 	Batch(400/2353) done. Loss: 1.2103  lr:0.010000
[ Thu May 16 15:28:39 2024 ] 	Batch(500/2353) done. Loss: 1.2999  lr:0.010000
[ Thu May 16 15:29:17 2024 ] 	Batch(600/2353) done. Loss: 1.5053  lr:0.010000
[ Thu May 16 15:29:54 2024 ] 	Batch(700/2353) done. Loss: 0.8824  lr:0.010000
[ Thu May 16 15:30:32 2024 ] 	Batch(800/2353) done. Loss: 0.9625  lr:0.010000
[ Thu May 16 15:31:10 2024 ] 	Batch(900/2353) done. Loss: 0.7796  lr:0.010000
[ Thu May 16 15:31:48 2024 ] 	Batch(1000/2353) done. Loss: 0.7930  lr:0.010000
[ Thu May 16 15:32:26 2024 ] 	Batch(1100/2353) done. Loss: 0.8766  lr:0.010000
[ Thu May 16 15:33:04 2024 ] 	Batch(1200/2353) done. Loss: 0.5882  lr:0.010000
[ Thu May 16 15:33:42 2024 ] 	Batch(1300/2353) done. Loss: 0.9423  lr:0.010000
[ Thu May 16 15:34:20 2024 ] 	Batch(1400/2353) done. Loss: 0.6886  lr:0.010000
[ Thu May 16 15:34:58 2024 ] 	Batch(1500/2353) done. Loss: 0.7747  lr:0.010000
[ Thu May 16 15:35:35 2024 ] 	Batch(1600/2353) done. Loss: 0.4234  lr:0.010000
[ Thu May 16 15:36:13 2024 ] 	Batch(1700/2353) done. Loss: 1.2803  lr:0.010000
[ Thu May 16 15:36:51 2024 ] 	Batch(1800/2353) done. Loss: 0.7683  lr:0.010000
[ Thu May 16 15:37:30 2024 ] 	Batch(1900/2353) done. Loss: 1.1257  lr:0.010000
[ Thu May 16 15:38:08 2024 ] 	Batch(2000/2353) done. Loss: 1.5340  lr:0.010000
[ Thu May 16 15:38:46 2024 ] 	Batch(2100/2353) done. Loss: 0.9411  lr:0.010000
[ Thu May 16 15:39:23 2024 ] 	Batch(2200/2353) done. Loss: 1.1685  lr:0.010000
[ Thu May 16 15:40:01 2024 ] 	Batch(2300/2353) done. Loss: 0.7708  lr:0.010000
[ Thu May 16 15:40:21 2024 ] 	Mean training loss: 0.9894.
[ Thu May 16 15:40:21 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 15:40:21 2024 ] Training epoch: 6
[ Thu May 16 15:40:21 2024 ] 	Batch(0/2353) done. Loss: 1.7700  lr:0.010000
[ Thu May 16 15:40:59 2024 ] 	Batch(100/2353) done. Loss: 0.7791  lr:0.010000
[ Thu May 16 15:41:36 2024 ] 	Batch(200/2353) done. Loss: 0.9830  lr:0.010000
[ Thu May 16 15:42:14 2024 ] 	Batch(300/2353) done. Loss: 0.7110  lr:0.010000
[ Thu May 16 15:42:51 2024 ] 	Batch(400/2353) done. Loss: 1.2026  lr:0.010000
[ Thu May 16 15:43:29 2024 ] 	Batch(500/2353) done. Loss: 0.6116  lr:0.010000
[ Thu May 16 15:44:06 2024 ] 	Batch(600/2353) done. Loss: 1.1822  lr:0.010000
[ Thu May 16 15:44:43 2024 ] 	Batch(700/2353) done. Loss: 0.5359  lr:0.010000
[ Thu May 16 15:45:21 2024 ] 	Batch(800/2353) done. Loss: 0.9855  lr:0.010000
[ Thu May 16 15:45:59 2024 ] 	Batch(900/2353) done. Loss: 0.8088  lr:0.010000
[ Thu May 16 15:46:36 2024 ] 	Batch(1000/2353) done. Loss: 0.9041  lr:0.010000
[ Thu May 16 15:47:14 2024 ] 	Batch(1100/2353) done. Loss: 0.5005  lr:0.010000
[ Thu May 16 15:47:51 2024 ] 	Batch(1200/2353) done. Loss: 1.0876  lr:0.010000
[ Thu May 16 15:48:29 2024 ] 	Batch(1300/2353) done. Loss: 0.8915  lr:0.010000
[ Thu May 16 15:49:06 2024 ] 	Batch(1400/2353) done. Loss: 0.8998  lr:0.010000
[ Thu May 16 15:49:43 2024 ] 	Batch(1500/2353) done. Loss: 0.9219  lr:0.010000
[ Thu May 16 15:50:21 2024 ] 	Batch(1600/2353) done. Loss: 0.6777  lr:0.010000
[ Thu May 16 15:50:58 2024 ] 	Batch(1700/2353) done. Loss: 0.5531  lr:0.010000
[ Thu May 16 15:51:36 2024 ] 	Batch(1800/2353) done. Loss: 1.1036  lr:0.010000
[ Thu May 16 15:52:13 2024 ] 	Batch(1900/2353) done. Loss: 0.9115  lr:0.010000
[ Thu May 16 15:52:51 2024 ] 	Batch(2000/2353) done. Loss: 0.6268  lr:0.010000
[ Thu May 16 15:53:28 2024 ] 	Batch(2100/2353) done. Loss: 0.6248  lr:0.010000
[ Thu May 16 15:54:06 2024 ] 	Batch(2200/2353) done. Loss: 0.4727  lr:0.010000
[ Thu May 16 15:54:43 2024 ] 	Batch(2300/2353) done. Loss: 0.7594  lr:0.010000
[ Thu May 16 15:55:03 2024 ] 	Mean training loss: 0.9028.
[ Thu May 16 15:55:03 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 15:55:03 2024 ] Training epoch: 7
[ Thu May 16 15:55:03 2024 ] 	Batch(0/2353) done. Loss: 0.1832  lr:0.010000
[ Thu May 16 15:55:41 2024 ] 	Batch(100/2353) done. Loss: 0.7484  lr:0.010000
[ Thu May 16 15:56:18 2024 ] 	Batch(200/2353) done. Loss: 0.5292  lr:0.010000
[ Thu May 16 15:56:56 2024 ] 	Batch(300/2353) done. Loss: 1.0990  lr:0.010000
[ Thu May 16 15:57:33 2024 ] 	Batch(400/2353) done. Loss: 0.7909  lr:0.010000
[ Thu May 16 15:58:11 2024 ] 	Batch(500/2353) done. Loss: 1.2397  lr:0.010000
[ Thu May 16 15:58:48 2024 ] 	Batch(600/2353) done. Loss: 0.7045  lr:0.010000
[ Thu May 16 15:59:25 2024 ] 	Batch(700/2353) done. Loss: 0.5070  lr:0.010000
[ Thu May 16 16:00:03 2024 ] 	Batch(800/2353) done. Loss: 0.7205  lr:0.010000
[ Thu May 16 16:00:40 2024 ] 	Batch(900/2353) done. Loss: 0.7809  lr:0.010000
[ Thu May 16 16:01:18 2024 ] 	Batch(1000/2353) done. Loss: 0.8549  lr:0.010000
[ Thu May 16 16:01:55 2024 ] 	Batch(1100/2353) done. Loss: 0.6511  lr:0.010000
[ Thu May 16 16:02:33 2024 ] 	Batch(1200/2353) done. Loss: 0.5195  lr:0.010000
[ Thu May 16 16:03:11 2024 ] 	Batch(1300/2353) done. Loss: 1.4473  lr:0.010000
[ Thu May 16 16:03:49 2024 ] 	Batch(1400/2353) done. Loss: 0.6554  lr:0.010000
[ Thu May 16 16:04:27 2024 ] 	Batch(1500/2353) done. Loss: 1.0536  lr:0.010000
[ Thu May 16 16:05:05 2024 ] 	Batch(1600/2353) done. Loss: 0.4816  lr:0.010000
[ Thu May 16 16:05:43 2024 ] 	Batch(1700/2353) done. Loss: 0.5791  lr:0.010000
[ Thu May 16 16:06:20 2024 ] 	Batch(1800/2353) done. Loss: 0.5034  lr:0.010000
[ Thu May 16 16:06:58 2024 ] 	Batch(1900/2353) done. Loss: 1.1300  lr:0.010000
[ Thu May 16 16:07:35 2024 ] 	Batch(2000/2353) done. Loss: 0.5036  lr:0.010000
[ Thu May 16 16:08:12 2024 ] 	Batch(2100/2353) done. Loss: 0.3625  lr:0.010000
[ Thu May 16 16:08:50 2024 ] 	Batch(2200/2353) done. Loss: 0.9045  lr:0.010000
[ Thu May 16 16:09:27 2024 ] 	Batch(2300/2353) done. Loss: 1.0159  lr:0.010000
[ Thu May 16 16:09:47 2024 ] 	Mean training loss: 0.8639.
[ Thu May 16 16:09:47 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 16:09:47 2024 ] Training epoch: 8
[ Thu May 16 16:09:48 2024 ] 	Batch(0/2353) done. Loss: 1.2318  lr:0.010000
[ Thu May 16 16:10:25 2024 ] 	Batch(100/2353) done. Loss: 0.9961  lr:0.010000
[ Thu May 16 16:11:02 2024 ] 	Batch(200/2353) done. Loss: 0.6250  lr:0.010000
[ Thu May 16 16:11:40 2024 ] 	Batch(300/2353) done. Loss: 0.8293  lr:0.010000
[ Thu May 16 16:12:17 2024 ] 	Batch(400/2353) done. Loss: 0.3924  lr:0.010000
[ Thu May 16 16:12:55 2024 ] 	Batch(500/2353) done. Loss: 0.5795  lr:0.010000
[ Thu May 16 16:13:32 2024 ] 	Batch(600/2353) done. Loss: 0.6654  lr:0.010000
[ Thu May 16 16:14:10 2024 ] 	Batch(700/2353) done. Loss: 0.7909  lr:0.010000
[ Thu May 16 16:14:47 2024 ] 	Batch(800/2353) done. Loss: 1.4375  lr:0.010000
[ Thu May 16 16:15:25 2024 ] 	Batch(900/2353) done. Loss: 0.9351  lr:0.010000
[ Thu May 16 16:16:02 2024 ] 	Batch(1000/2353) done. Loss: 0.8126  lr:0.010000
[ Thu May 16 16:16:39 2024 ] 	Batch(1100/2353) done. Loss: 1.2054  lr:0.010000
[ Thu May 16 16:17:17 2024 ] 	Batch(1200/2353) done. Loss: 0.8745  lr:0.010000
[ Thu May 16 16:17:54 2024 ] 	Batch(1300/2353) done. Loss: 0.6908  lr:0.010000
[ Thu May 16 16:18:32 2024 ] 	Batch(1400/2353) done. Loss: 0.9401  lr:0.010000
[ Thu May 16 16:19:09 2024 ] 	Batch(1500/2353) done. Loss: 1.1549  lr:0.010000
[ Thu May 16 16:19:47 2024 ] 	Batch(1600/2353) done. Loss: 0.5772  lr:0.010000
[ Thu May 16 16:20:24 2024 ] 	Batch(1700/2353) done. Loss: 0.6468  lr:0.010000
[ Thu May 16 16:21:01 2024 ] 	Batch(1800/2353) done. Loss: 0.5035  lr:0.010000
[ Thu May 16 16:21:39 2024 ] 	Batch(1900/2353) done. Loss: 0.7518  lr:0.010000
[ Thu May 16 16:22:16 2024 ] 	Batch(2000/2353) done. Loss: 0.2794  lr:0.010000
[ Thu May 16 16:22:54 2024 ] 	Batch(2100/2353) done. Loss: 0.4411  lr:0.010000
[ Thu May 16 16:23:32 2024 ] 	Batch(2200/2353) done. Loss: 0.5103  lr:0.010000
[ Thu May 16 16:24:11 2024 ] 	Batch(2300/2353) done. Loss: 0.1633  lr:0.010000
[ Thu May 16 16:24:30 2024 ] 	Mean training loss: 0.7965.
[ Thu May 16 16:24:30 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 16:24:31 2024 ] Training epoch: 9
[ Thu May 16 16:24:31 2024 ] 	Batch(0/2353) done. Loss: 1.0966  lr:0.010000
[ Thu May 16 16:25:09 2024 ] 	Batch(100/2353) done. Loss: 0.6664  lr:0.010000
[ Thu May 16 16:25:46 2024 ] 	Batch(200/2353) done. Loss: 0.7502  lr:0.010000
[ Thu May 16 16:26:24 2024 ] 	Batch(300/2353) done. Loss: 1.1835  lr:0.010000
[ Thu May 16 16:27:01 2024 ] 	Batch(400/2353) done. Loss: 0.5048  lr:0.010000
[ Thu May 16 16:27:39 2024 ] 	Batch(500/2353) done. Loss: 0.8510  lr:0.010000
[ Thu May 16 16:28:16 2024 ] 	Batch(600/2353) done. Loss: 0.8483  lr:0.010000
[ Thu May 16 16:28:53 2024 ] 	Batch(700/2353) done. Loss: 0.7675  lr:0.010000
[ Thu May 16 16:29:31 2024 ] 	Batch(800/2353) done. Loss: 0.6097  lr:0.010000
[ Thu May 16 16:30:09 2024 ] 	Batch(900/2353) done. Loss: 0.7997  lr:0.010000
[ Thu May 16 16:30:47 2024 ] 	Batch(1000/2353) done. Loss: 0.9603  lr:0.010000
[ Thu May 16 16:31:25 2024 ] 	Batch(1100/2353) done. Loss: 0.7802  lr:0.010000
[ Thu May 16 16:32:03 2024 ] 	Batch(1200/2353) done. Loss: 0.9492  lr:0.010000
[ Thu May 16 16:32:42 2024 ] 	Batch(1300/2353) done. Loss: 1.0412  lr:0.010000
[ Thu May 16 16:33:20 2024 ] 	Batch(1400/2353) done. Loss: 0.8616  lr:0.010000
[ Thu May 16 16:33:58 2024 ] 	Batch(1500/2353) done. Loss: 1.0318  lr:0.010000
[ Thu May 16 16:34:36 2024 ] 	Batch(1600/2353) done. Loss: 0.6334  lr:0.010000
[ Thu May 16 16:35:13 2024 ] 	Batch(1700/2353) done. Loss: 0.3341  lr:0.010000
[ Thu May 16 16:35:51 2024 ] 	Batch(1800/2353) done. Loss: 1.0227  lr:0.010000
[ Thu May 16 16:36:28 2024 ] 	Batch(1900/2353) done. Loss: 0.7178  lr:0.010000
[ Thu May 16 16:37:06 2024 ] 	Batch(2000/2353) done. Loss: 0.9970  lr:0.010000
[ Thu May 16 16:37:43 2024 ] 	Batch(2100/2353) done. Loss: 0.9622  lr:0.010000
[ Thu May 16 16:38:21 2024 ] 	Batch(2200/2353) done. Loss: 1.0880  lr:0.010000
[ Thu May 16 16:38:58 2024 ] 	Batch(2300/2353) done. Loss: 0.7875  lr:0.010000
[ Thu May 16 16:39:18 2024 ] 	Mean training loss: 0.7463.
[ Thu May 16 16:39:18 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 16:39:18 2024 ] Training epoch: 10
[ Thu May 16 16:39:18 2024 ] 	Batch(0/2353) done. Loss: 0.7767  lr:0.010000
[ Thu May 16 16:39:56 2024 ] 	Batch(100/2353) done. Loss: 0.5435  lr:0.010000
[ Thu May 16 16:40:33 2024 ] 	Batch(200/2353) done. Loss: 0.8066  lr:0.010000
[ Thu May 16 16:41:11 2024 ] 	Batch(300/2353) done. Loss: 0.2276  lr:0.010000
[ Thu May 16 16:41:48 2024 ] 	Batch(400/2353) done. Loss: 1.2185  lr:0.010000
[ Thu May 16 16:42:26 2024 ] 	Batch(500/2353) done. Loss: 0.6591  lr:0.010000
[ Thu May 16 16:43:03 2024 ] 	Batch(600/2353) done. Loss: 0.8516  lr:0.010000
[ Thu May 16 16:43:40 2024 ] 	Batch(700/2353) done. Loss: 1.0552  lr:0.010000
[ Thu May 16 16:44:18 2024 ] 	Batch(800/2353) done. Loss: 0.9170  lr:0.010000
[ Thu May 16 16:44:56 2024 ] 	Batch(900/2353) done. Loss: 0.5533  lr:0.010000
[ Thu May 16 16:45:34 2024 ] 	Batch(1000/2353) done. Loss: 0.4714  lr:0.010000
[ Thu May 16 16:46:12 2024 ] 	Batch(1100/2353) done. Loss: 0.5934  lr:0.010000
[ Thu May 16 16:46:50 2024 ] 	Batch(1200/2353) done. Loss: 0.6485  lr:0.010000
[ Thu May 16 16:47:29 2024 ] 	Batch(1300/2353) done. Loss: 0.8660  lr:0.010000
[ Thu May 16 16:48:07 2024 ] 	Batch(1400/2353) done. Loss: 0.1093  lr:0.010000
[ Thu May 16 16:48:45 2024 ] 	Batch(1500/2353) done. Loss: 1.0195  lr:0.010000
[ Thu May 16 16:49:23 2024 ] 	Batch(1600/2353) done. Loss: 0.8150  lr:0.010000
[ Thu May 16 16:50:01 2024 ] 	Batch(1700/2353) done. Loss: 0.4999  lr:0.010000
[ Thu May 16 16:50:39 2024 ] 	Batch(1800/2353) done. Loss: 0.6501  lr:0.010000
[ Thu May 16 16:51:16 2024 ] 	Batch(1900/2353) done. Loss: 1.0757  lr:0.010000
[ Thu May 16 16:51:54 2024 ] 	Batch(2000/2353) done. Loss: 0.4414  lr:0.010000
[ Thu May 16 16:52:32 2024 ] 	Batch(2100/2353) done. Loss: 0.2624  lr:0.010000
[ Thu May 16 16:53:10 2024 ] 	Batch(2200/2353) done. Loss: 0.6065  lr:0.010000
[ Thu May 16 16:53:48 2024 ] 	Batch(2300/2353) done. Loss: 1.6119  lr:0.010000
[ Thu May 16 16:54:08 2024 ] 	Mean training loss: 0.7084.
[ Thu May 16 16:54:08 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 16:54:08 2024 ] Eval epoch: 10
[ Thu May 16 16:56:11 2024 ] 	Mean val loss of 2367 batches: 0.4644684418869674.
[ Thu May 16 16:56:11 2024 ] Training epoch: 11
[ Thu May 16 16:56:12 2024 ] 	Batch(0/2353) done. Loss: 0.7216  lr:0.010000
[ Thu May 16 16:56:49 2024 ] 	Batch(100/2353) done. Loss: 0.7901  lr:0.010000
[ Thu May 16 16:57:27 2024 ] 	Batch(200/2353) done. Loss: 0.3539  lr:0.010000
[ Thu May 16 16:58:04 2024 ] 	Batch(300/2353) done. Loss: 1.1256  lr:0.010000
[ Thu May 16 16:58:42 2024 ] 	Batch(400/2353) done. Loss: 0.5657  lr:0.010000
[ Thu May 16 16:59:19 2024 ] 	Batch(500/2353) done. Loss: 0.4193  lr:0.010000
[ Thu May 16 16:59:56 2024 ] 	Batch(600/2353) done. Loss: 0.6252  lr:0.010000
[ Thu May 16 17:00:34 2024 ] 	Batch(700/2353) done. Loss: 0.4160  lr:0.010000
[ Thu May 16 17:01:11 2024 ] 	Batch(800/2353) done. Loss: 0.8201  lr:0.010000
[ Thu May 16 17:01:49 2024 ] 	Batch(900/2353) done. Loss: 0.5230  lr:0.010000
[ Thu May 16 17:02:27 2024 ] 	Batch(1000/2353) done. Loss: 0.6623  lr:0.010000
[ Thu May 16 17:03:05 2024 ] 	Batch(1100/2353) done. Loss: 0.1997  lr:0.010000
[ Thu May 16 17:03:43 2024 ] 	Batch(1200/2353) done. Loss: 0.5507  lr:0.010000
[ Thu May 16 17:04:21 2024 ] 	Batch(1300/2353) done. Loss: 0.4917  lr:0.010000
[ Thu May 16 17:04:59 2024 ] 	Batch(1400/2353) done. Loss: 0.3643  lr:0.010000
[ Thu May 16 17:05:36 2024 ] 	Batch(1500/2353) done. Loss: 0.7571  lr:0.010000
[ Thu May 16 17:06:14 2024 ] 	Batch(1600/2353) done. Loss: 0.7130  lr:0.010000
[ Thu May 16 17:06:52 2024 ] 	Batch(1700/2353) done. Loss: 0.3586  lr:0.010000
[ Thu May 16 17:07:30 2024 ] 	Batch(1800/2353) done. Loss: 0.8310  lr:0.010000
[ Thu May 16 17:08:07 2024 ] 	Batch(1900/2353) done. Loss: 1.1662  lr:0.010000
[ Thu May 16 17:08:45 2024 ] 	Batch(2000/2353) done. Loss: 0.7652  lr:0.010000
[ Thu May 16 17:09:23 2024 ] 	Batch(2100/2353) done. Loss: 0.8369  lr:0.010000
[ Thu May 16 17:10:00 2024 ] 	Batch(2200/2353) done. Loss: 0.9100  lr:0.010000
[ Thu May 16 17:10:38 2024 ] 	Batch(2300/2353) done. Loss: 0.6186  lr:0.010000
[ Thu May 16 17:10:58 2024 ] 	Mean training loss: 0.6761.
[ Thu May 16 17:10:58 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 17:10:58 2024 ] Training epoch: 12
[ Thu May 16 17:10:59 2024 ] 	Batch(0/2353) done. Loss: 0.3717  lr:0.010000
[ Thu May 16 17:11:37 2024 ] 	Batch(100/2353) done. Loss: 0.5274  lr:0.010000
[ Thu May 16 17:12:14 2024 ] 	Batch(200/2353) done. Loss: 0.6723  lr:0.010000
[ Thu May 16 17:12:51 2024 ] 	Batch(300/2353) done. Loss: 0.6405  lr:0.010000
[ Thu May 16 17:13:29 2024 ] 	Batch(400/2353) done. Loss: 0.7849  lr:0.010000
[ Thu May 16 17:14:06 2024 ] 	Batch(500/2353) done. Loss: 0.9542  lr:0.010000
[ Thu May 16 17:14:44 2024 ] 	Batch(600/2353) done. Loss: 0.5950  lr:0.010000
[ Thu May 16 17:15:21 2024 ] 	Batch(700/2353) done. Loss: 1.2538  lr:0.010000
[ Thu May 16 17:15:59 2024 ] 	Batch(800/2353) done. Loss: 0.4656  lr:0.010000
[ Thu May 16 17:16:36 2024 ] 	Batch(900/2353) done. Loss: 0.8119  lr:0.010000
[ Thu May 16 17:17:13 2024 ] 	Batch(1000/2353) done. Loss: 0.2367  lr:0.010000
[ Thu May 16 17:17:51 2024 ] 	Batch(1100/2353) done. Loss: 0.8909  lr:0.010000
[ Thu May 16 17:18:28 2024 ] 	Batch(1200/2353) done. Loss: 0.6609  lr:0.010000
[ Thu May 16 17:19:06 2024 ] 	Batch(1300/2353) done. Loss: 0.5030  lr:0.010000
[ Thu May 16 17:19:43 2024 ] 	Batch(1400/2353) done. Loss: 0.6789  lr:0.010000
[ Thu May 16 17:20:21 2024 ] 	Batch(1500/2353) done. Loss: 0.2429  lr:0.010000
[ Thu May 16 17:20:58 2024 ] 	Batch(1600/2353) done. Loss: 0.8469  lr:0.010000
[ Thu May 16 17:21:36 2024 ] 	Batch(1700/2353) done. Loss: 1.0867  lr:0.010000
[ Thu May 16 17:22:13 2024 ] 	Batch(1800/2353) done. Loss: 0.5413  lr:0.010000
[ Thu May 16 17:22:50 2024 ] 	Batch(1900/2353) done. Loss: 0.9161  lr:0.010000
[ Thu May 16 17:23:28 2024 ] 	Batch(2000/2353) done. Loss: 0.4737  lr:0.010000
[ Thu May 16 17:24:05 2024 ] 	Batch(2100/2353) done. Loss: 0.2697  lr:0.010000
[ Thu May 16 17:24:42 2024 ] 	Batch(2200/2353) done. Loss: 0.4120  lr:0.010000
[ Thu May 16 17:25:20 2024 ] 	Batch(2300/2353) done. Loss: 0.6228  lr:0.010000
[ Thu May 16 17:25:39 2024 ] 	Mean training loss: 0.6429.
[ Thu May 16 17:25:39 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 17:25:39 2024 ] Training epoch: 13
[ Thu May 16 17:25:40 2024 ] 	Batch(0/2353) done. Loss: 0.2452  lr:0.010000
[ Thu May 16 17:26:18 2024 ] 	Batch(100/2353) done. Loss: 0.5366  lr:0.010000
[ Thu May 16 17:26:55 2024 ] 	Batch(200/2353) done. Loss: 0.5115  lr:0.010000
[ Thu May 16 17:27:32 2024 ] 	Batch(300/2353) done. Loss: 0.9450  lr:0.010000
[ Thu May 16 17:28:10 2024 ] 	Batch(400/2353) done. Loss: 0.5909  lr:0.010000
[ Thu May 16 17:28:47 2024 ] 	Batch(500/2353) done. Loss: 0.6442  lr:0.010000
[ Thu May 16 17:29:24 2024 ] 	Batch(600/2353) done. Loss: 0.9431  lr:0.010000
[ Thu May 16 17:30:02 2024 ] 	Batch(700/2353) done. Loss: 0.4871  lr:0.010000
[ Thu May 16 17:30:39 2024 ] 	Batch(800/2353) done. Loss: 0.7340  lr:0.010000
[ Thu May 16 17:31:17 2024 ] 	Batch(900/2353) done. Loss: 1.3838  lr:0.010000
[ Thu May 16 17:31:54 2024 ] 	Batch(1000/2353) done. Loss: 0.5305  lr:0.010000
[ Thu May 16 17:32:31 2024 ] 	Batch(1100/2353) done. Loss: 1.1210  lr:0.010000
[ Thu May 16 17:33:09 2024 ] 	Batch(1200/2353) done. Loss: 0.3719  lr:0.010000
[ Thu May 16 17:33:47 2024 ] 	Batch(1300/2353) done. Loss: 0.4646  lr:0.010000
[ Thu May 16 17:34:25 2024 ] 	Batch(1400/2353) done. Loss: 0.5916  lr:0.010000
[ Thu May 16 17:35:02 2024 ] 	Batch(1500/2353) done. Loss: 0.8476  lr:0.010000
[ Thu May 16 17:35:40 2024 ] 	Batch(1600/2353) done. Loss: 0.4425  lr:0.010000
[ Thu May 16 17:36:17 2024 ] 	Batch(1700/2353) done. Loss: 0.6697  lr:0.010000
[ Thu May 16 17:36:55 2024 ] 	Batch(1800/2353) done. Loss: 0.5797  lr:0.010000
[ Thu May 16 17:37:32 2024 ] 	Batch(1900/2353) done. Loss: 0.5441  lr:0.010000
[ Thu May 16 17:38:10 2024 ] 	Batch(2000/2353) done. Loss: 0.5604  lr:0.010000
[ Thu May 16 17:38:47 2024 ] 	Batch(2100/2353) done. Loss: 0.5053  lr:0.010000
[ Thu May 16 17:39:25 2024 ] 	Batch(2200/2353) done. Loss: 0.6936  lr:0.010000
[ Thu May 16 17:40:02 2024 ] 	Batch(2300/2353) done. Loss: 0.4380  lr:0.010000
[ Thu May 16 17:40:22 2024 ] 	Mean training loss: 0.6080.
[ Thu May 16 17:40:22 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 17:40:22 2024 ] Training epoch: 14
[ Thu May 16 17:40:22 2024 ] 	Batch(0/2353) done. Loss: 0.3368  lr:0.010000
[ Thu May 16 17:41:00 2024 ] 	Batch(100/2353) done. Loss: 0.7783  lr:0.010000
[ Thu May 16 17:41:37 2024 ] 	Batch(200/2353) done. Loss: 0.3446  lr:0.010000
[ Thu May 16 17:42:14 2024 ] 	Batch(300/2353) done. Loss: 1.2844  lr:0.010000
[ Thu May 16 17:42:52 2024 ] 	Batch(400/2353) done. Loss: 0.7524  lr:0.010000
[ Thu May 16 17:43:29 2024 ] 	Batch(500/2353) done. Loss: 0.3640  lr:0.010000
[ Thu May 16 17:44:07 2024 ] 	Batch(600/2353) done. Loss: 1.0343  lr:0.010000
[ Thu May 16 17:44:44 2024 ] 	Batch(700/2353) done. Loss: 0.3648  lr:0.010000
[ Thu May 16 17:45:21 2024 ] 	Batch(800/2353) done. Loss: 0.5019  lr:0.010000
[ Thu May 16 17:45:59 2024 ] 	Batch(900/2353) done. Loss: 1.0840  lr:0.010000
[ Thu May 16 17:46:36 2024 ] 	Batch(1000/2353) done. Loss: 0.6305  lr:0.010000
[ Thu May 16 17:47:14 2024 ] 	Batch(1100/2353) done. Loss: 0.6776  lr:0.010000
[ Thu May 16 17:47:51 2024 ] 	Batch(1200/2353) done. Loss: 0.7800  lr:0.010000
[ Thu May 16 17:48:29 2024 ] 	Batch(1300/2353) done. Loss: 0.5845  lr:0.010000
[ Thu May 16 17:49:06 2024 ] 	Batch(1400/2353) done. Loss: 0.4269  lr:0.010000
[ Thu May 16 17:49:43 2024 ] 	Batch(1500/2353) done. Loss: 0.4437  lr:0.010000
[ Thu May 16 17:50:21 2024 ] 	Batch(1600/2353) done. Loss: 0.6412  lr:0.010000
[ Thu May 16 17:50:58 2024 ] 	Batch(1700/2353) done. Loss: 0.4334  lr:0.010000
[ Thu May 16 17:51:36 2024 ] 	Batch(1800/2353) done. Loss: 0.7050  lr:0.010000
[ Thu May 16 17:52:13 2024 ] 	Batch(1900/2353) done. Loss: 0.2088  lr:0.010000
[ Thu May 16 17:52:50 2024 ] 	Batch(2000/2353) done. Loss: 0.2275  lr:0.010000
[ Thu May 16 17:53:28 2024 ] 	Batch(2100/2353) done. Loss: 0.6312  lr:0.010000
[ Thu May 16 17:54:05 2024 ] 	Batch(2200/2353) done. Loss: 0.9368  lr:0.010000
[ Thu May 16 17:54:43 2024 ] 	Batch(2300/2353) done. Loss: 0.5466  lr:0.010000
[ Thu May 16 17:55:02 2024 ] 	Mean training loss: 0.5789.
[ Thu May 16 17:55:02 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 17:55:02 2024 ] Training epoch: 15
[ Thu May 16 17:55:03 2024 ] 	Batch(0/2353) done. Loss: 0.6139  lr:0.010000
[ Thu May 16 17:55:41 2024 ] 	Batch(100/2353) done. Loss: 0.4698  lr:0.010000
[ Thu May 16 17:56:18 2024 ] 	Batch(200/2353) done. Loss: 0.2847  lr:0.010000
[ Thu May 16 17:56:55 2024 ] 	Batch(300/2353) done. Loss: 0.3535  lr:0.010000
[ Thu May 16 17:57:33 2024 ] 	Batch(400/2353) done. Loss: 0.6898  lr:0.010000
[ Thu May 16 17:58:10 2024 ] 	Batch(500/2353) done. Loss: 0.7105  lr:0.010000
[ Thu May 16 17:58:47 2024 ] 	Batch(600/2353) done. Loss: 0.3526  lr:0.010000
[ Thu May 16 17:59:25 2024 ] 	Batch(700/2353) done. Loss: 0.3779  lr:0.010000
[ Thu May 16 18:00:02 2024 ] 	Batch(800/2353) done. Loss: 0.2131  lr:0.010000
[ Thu May 16 18:00:40 2024 ] 	Batch(900/2353) done. Loss: 0.2151  lr:0.010000
[ Thu May 16 18:01:17 2024 ] 	Batch(1000/2353) done. Loss: 0.5993  lr:0.010000
[ Thu May 16 18:01:54 2024 ] 	Batch(1100/2353) done. Loss: 0.5014  lr:0.010000
[ Thu May 16 18:02:32 2024 ] 	Batch(1200/2353) done. Loss: 0.5783  lr:0.010000
[ Thu May 16 18:03:10 2024 ] 	Batch(1300/2353) done. Loss: 0.4741  lr:0.010000
[ Thu May 16 18:03:47 2024 ] 	Batch(1400/2353) done. Loss: 0.2905  lr:0.010000
[ Thu May 16 18:04:24 2024 ] 	Batch(1500/2353) done. Loss: 0.9695  lr:0.010000
[ Thu May 16 18:05:02 2024 ] 	Batch(1600/2353) done. Loss: 0.3506  lr:0.010000
[ Thu May 16 18:05:39 2024 ] 	Batch(1700/2353) done. Loss: 0.2341  lr:0.010000
[ Thu May 16 18:06:17 2024 ] 	Batch(1800/2353) done. Loss: 0.5717  lr:0.010000
[ Thu May 16 18:06:54 2024 ] 	Batch(1900/2353) done. Loss: 0.8917  lr:0.010000
[ Thu May 16 18:07:32 2024 ] 	Batch(2000/2353) done. Loss: 1.0670  lr:0.010000
[ Thu May 16 18:08:10 2024 ] 	Batch(2100/2353) done. Loss: 0.1725  lr:0.010000
[ Thu May 16 18:08:47 2024 ] 	Batch(2200/2353) done. Loss: 0.8010  lr:0.010000
[ Thu May 16 18:09:25 2024 ] 	Batch(2300/2353) done. Loss: 0.3563  lr:0.010000
[ Thu May 16 18:09:44 2024 ] 	Mean training loss: 0.5711.
[ Thu May 16 18:09:44 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 18:09:44 2024 ] Training epoch: 16
[ Thu May 16 18:09:45 2024 ] 	Batch(0/2353) done. Loss: 0.7989  lr:0.010000
[ Thu May 16 18:10:22 2024 ] 	Batch(100/2353) done. Loss: 0.6465  lr:0.010000
[ Thu May 16 18:11:00 2024 ] 	Batch(200/2353) done. Loss: 0.5830  lr:0.010000
[ Thu May 16 18:11:37 2024 ] 	Batch(300/2353) done. Loss: 0.4608  lr:0.010000
[ Thu May 16 18:12:15 2024 ] 	Batch(400/2353) done. Loss: 0.4156  lr:0.010000
[ Thu May 16 18:12:52 2024 ] 	Batch(500/2353) done. Loss: 0.1742  lr:0.010000
[ Thu May 16 18:13:29 2024 ] 	Batch(600/2353) done. Loss: 0.8395  lr:0.010000
[ Thu May 16 18:14:07 2024 ] 	Batch(700/2353) done. Loss: 0.8362  lr:0.010000
[ Thu May 16 18:14:44 2024 ] 	Batch(800/2353) done. Loss: 0.9598  lr:0.010000
[ Thu May 16 18:15:22 2024 ] 	Batch(900/2353) done. Loss: 0.3548  lr:0.010000
[ Thu May 16 18:16:00 2024 ] 	Batch(1000/2353) done. Loss: 0.6868  lr:0.010000
[ Thu May 16 18:16:38 2024 ] 	Batch(1100/2353) done. Loss: 0.3411  lr:0.010000
[ Thu May 16 18:17:15 2024 ] 	Batch(1200/2353) done. Loss: 0.4136  lr:0.010000
[ Thu May 16 18:17:53 2024 ] 	Batch(1300/2353) done. Loss: 0.2826  lr:0.010000
[ Thu May 16 18:18:30 2024 ] 	Batch(1400/2353) done. Loss: 0.3933  lr:0.010000
[ Thu May 16 18:19:07 2024 ] 	Batch(1500/2353) done. Loss: 0.6938  lr:0.010000
[ Thu May 16 18:19:45 2024 ] 	Batch(1600/2353) done. Loss: 0.8707  lr:0.010000
[ Thu May 16 18:20:22 2024 ] 	Batch(1700/2353) done. Loss: 0.8698  lr:0.010000
[ Thu May 16 18:20:59 2024 ] 	Batch(1800/2353) done. Loss: 0.8725  lr:0.010000
[ Thu May 16 18:21:36 2024 ] 	Batch(1900/2353) done. Loss: 0.1082  lr:0.010000
[ Thu May 16 18:22:14 2024 ] 	Batch(2000/2353) done. Loss: 0.9458  lr:0.010000
[ Thu May 16 18:22:51 2024 ] 	Batch(2100/2353) done. Loss: 0.5680  lr:0.010000
[ Thu May 16 18:23:29 2024 ] 	Batch(2200/2353) done. Loss: 0.7912  lr:0.010000
[ Thu May 16 18:24:06 2024 ] 	Batch(2300/2353) done. Loss: 0.4774  lr:0.010000
[ Thu May 16 18:24:26 2024 ] 	Mean training loss: 0.5426.
[ Thu May 16 18:24:26 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 18:24:26 2024 ] Training epoch: 17
[ Thu May 16 18:24:26 2024 ] 	Batch(0/2353) done. Loss: 0.2803  lr:0.010000
[ Thu May 16 18:25:04 2024 ] 	Batch(100/2353) done. Loss: 0.1573  lr:0.010000
[ Thu May 16 18:25:41 2024 ] 	Batch(200/2353) done. Loss: 0.5725  lr:0.010000
[ Thu May 16 18:26:19 2024 ] 	Batch(300/2353) done. Loss: 0.1868  lr:0.010000
[ Thu May 16 18:26:56 2024 ] 	Batch(400/2353) done. Loss: 0.6203  lr:0.010000
[ Thu May 16 18:27:34 2024 ] 	Batch(500/2353) done. Loss: 0.2971  lr:0.010000
[ Thu May 16 18:28:11 2024 ] 	Batch(600/2353) done. Loss: 0.5568  lr:0.010000
[ Thu May 16 18:28:49 2024 ] 	Batch(700/2353) done. Loss: 0.6731  lr:0.010000
[ Thu May 16 18:29:27 2024 ] 	Batch(800/2353) done. Loss: 0.4794  lr:0.010000
[ Thu May 16 18:30:05 2024 ] 	Batch(900/2353) done. Loss: 0.6958  lr:0.010000
[ Thu May 16 18:30:43 2024 ] 	Batch(1000/2353) done. Loss: 0.7654  lr:0.010000
[ Thu May 16 18:31:21 2024 ] 	Batch(1100/2353) done. Loss: 0.3353  lr:0.010000
[ Thu May 16 18:31:59 2024 ] 	Batch(1200/2353) done. Loss: 0.7779  lr:0.010000
[ Thu May 16 18:32:37 2024 ] 	Batch(1300/2353) done. Loss: 0.1834  lr:0.010000
[ Thu May 16 18:33:14 2024 ] 	Batch(1400/2353) done. Loss: 1.0202  lr:0.010000
[ Thu May 16 18:33:51 2024 ] 	Batch(1500/2353) done. Loss: 0.1119  lr:0.010000
[ Thu May 16 18:34:29 2024 ] 	Batch(1600/2353) done. Loss: 0.1886  lr:0.010000
[ Thu May 16 18:35:06 2024 ] 	Batch(1700/2353) done. Loss: 0.7029  lr:0.010000
[ Thu May 16 18:35:44 2024 ] 	Batch(1800/2353) done. Loss: 0.5729  lr:0.010000
[ Thu May 16 18:36:22 2024 ] 	Batch(1900/2353) done. Loss: 0.4871  lr:0.010000
[ Thu May 16 18:37:00 2024 ] 	Batch(2000/2353) done. Loss: 0.4406  lr:0.010000
[ Thu May 16 18:37:38 2024 ] 	Batch(2100/2353) done. Loss: 0.2811  lr:0.010000
[ Thu May 16 18:38:16 2024 ] 	Batch(2200/2353) done. Loss: 0.2084  lr:0.010000
[ Thu May 16 18:38:55 2024 ] 	Batch(2300/2353) done. Loss: 0.2572  lr:0.010000
[ Thu May 16 18:39:15 2024 ] 	Mean training loss: 0.5220.
[ Thu May 16 18:39:15 2024 ] 	Time consumption: [Data]01%, [Network]92%
[ Thu May 16 18:39:15 2024 ] Training epoch: 18
[ Thu May 16 18:39:15 2024 ] 	Batch(0/2353) done. Loss: 0.5908  lr:0.010000
[ Thu May 16 18:39:53 2024 ] 	Batch(100/2353) done. Loss: 0.1183  lr:0.010000
[ Thu May 16 18:40:30 2024 ] 	Batch(200/2353) done. Loss: 0.3554  lr:0.010000
[ Thu May 16 18:41:08 2024 ] 	Batch(300/2353) done. Loss: 0.3203  lr:0.010000
[ Thu May 16 18:41:46 2024 ] 	Batch(400/2353) done. Loss: 0.4457  lr:0.010000
[ Thu May 16 18:42:23 2024 ] 	Batch(500/2353) done. Loss: 0.3739  lr:0.010000
[ Thu May 16 18:43:01 2024 ] 	Batch(600/2353) done. Loss: 0.3373  lr:0.010000
[ Thu May 16 18:43:38 2024 ] 	Batch(700/2353) done. Loss: 0.4676  lr:0.010000
[ Thu May 16 18:44:16 2024 ] 	Batch(800/2353) done. Loss: 1.2878  lr:0.010000
[ Thu May 16 18:44:53 2024 ] 	Batch(900/2353) done. Loss: 0.5183  lr:0.010000
[ Thu May 16 18:45:31 2024 ] 	Batch(1000/2353) done. Loss: 0.4903  lr:0.010000
[ Thu May 16 18:46:08 2024 ] 	Batch(1100/2353) done. Loss: 0.4684  lr:0.010000
[ Thu May 16 18:46:45 2024 ] 	Batch(1200/2353) done. Loss: 0.8816  lr:0.010000
[ Thu May 16 18:47:23 2024 ] 	Batch(1300/2353) done. Loss: 0.3214  lr:0.010000
[ Thu May 16 18:48:00 2024 ] 	Batch(1400/2353) done. Loss: 0.5213  lr:0.010000
[ Thu May 16 18:48:37 2024 ] 	Batch(1500/2353) done. Loss: 0.7255  lr:0.010000
[ Thu May 16 18:49:15 2024 ] 	Batch(1600/2353) done. Loss: 0.3109  lr:0.010000
[ Thu May 16 18:49:52 2024 ] 	Batch(1700/2353) done. Loss: 0.4892  lr:0.010000
[ Thu May 16 18:50:30 2024 ] 	Batch(1800/2353) done. Loss: 0.4026  lr:0.010000
[ Thu May 16 18:51:07 2024 ] 	Batch(1900/2353) done. Loss: 0.3405  lr:0.010000
[ Thu May 16 18:51:44 2024 ] 	Batch(2000/2353) done. Loss: 0.3403  lr:0.010000
[ Thu May 16 18:52:23 2024 ] 	Batch(2100/2353) done. Loss: 0.4588  lr:0.010000
[ Thu May 16 18:53:01 2024 ] 	Batch(2200/2353) done. Loss: 0.4716  lr:0.010000
[ Thu May 16 18:53:39 2024 ] 	Batch(2300/2353) done. Loss: 0.3465  lr:0.010000
[ Thu May 16 18:53:59 2024 ] 	Mean training loss: 0.4978.
[ Thu May 16 18:53:59 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 18:53:59 2024 ] Training epoch: 19
[ Thu May 16 18:54:00 2024 ] 	Batch(0/2353) done. Loss: 0.4076  lr:0.010000
[ Thu May 16 18:54:37 2024 ] 	Batch(100/2353) done. Loss: 1.0611  lr:0.010000
[ Thu May 16 18:55:14 2024 ] 	Batch(200/2353) done. Loss: 0.4402  lr:0.010000
[ Thu May 16 18:55:52 2024 ] 	Batch(300/2353) done. Loss: 0.5705  lr:0.010000
[ Thu May 16 18:56:29 2024 ] 	Batch(400/2353) done. Loss: 0.5066  lr:0.010000
[ Thu May 16 18:57:07 2024 ] 	Batch(500/2353) done. Loss: 0.5234  lr:0.010000
[ Thu May 16 18:57:44 2024 ] 	Batch(600/2353) done. Loss: 0.0626  lr:0.010000
[ Thu May 16 18:58:21 2024 ] 	Batch(700/2353) done. Loss: 0.4313  lr:0.010000
[ Thu May 16 18:58:59 2024 ] 	Batch(800/2353) done. Loss: 1.1606  lr:0.010000
[ Thu May 16 18:59:36 2024 ] 	Batch(900/2353) done. Loss: 1.3169  lr:0.010000
[ Thu May 16 19:00:13 2024 ] 	Batch(1000/2353) done. Loss: 0.3295  lr:0.010000
[ Thu May 16 19:00:51 2024 ] 	Batch(1100/2353) done. Loss: 1.4972  lr:0.010000
[ Thu May 16 19:01:28 2024 ] 	Batch(1200/2353) done. Loss: 0.2241  lr:0.010000
[ Thu May 16 19:02:06 2024 ] 	Batch(1300/2353) done. Loss: 0.4320  lr:0.010000
[ Thu May 16 19:02:43 2024 ] 	Batch(1400/2353) done. Loss: 0.3920  lr:0.010000
[ Thu May 16 19:03:20 2024 ] 	Batch(1500/2353) done. Loss: 1.0527  lr:0.010000
[ Thu May 16 19:03:58 2024 ] 	Batch(1600/2353) done. Loss: 0.7354  lr:0.010000
[ Thu May 16 19:04:36 2024 ] 	Batch(1700/2353) done. Loss: 0.1031  lr:0.010000
[ Thu May 16 19:05:14 2024 ] 	Batch(1800/2353) done. Loss: 0.1222  lr:0.010000
[ Thu May 16 19:05:52 2024 ] 	Batch(1900/2353) done. Loss: 0.3395  lr:0.010000
[ Thu May 16 19:06:29 2024 ] 	Batch(2000/2353) done. Loss: 0.6327  lr:0.010000
[ Thu May 16 19:07:07 2024 ] 	Batch(2100/2353) done. Loss: 0.3112  lr:0.010000
[ Thu May 16 19:07:45 2024 ] 	Batch(2200/2353) done. Loss: 0.4068  lr:0.010000
[ Thu May 16 19:08:24 2024 ] 	Batch(2300/2353) done. Loss: 0.2418  lr:0.010000
[ Thu May 16 19:08:43 2024 ] 	Mean training loss: 0.4891.
[ Thu May 16 19:08:43 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 19:08:44 2024 ] Training epoch: 20
[ Thu May 16 19:08:44 2024 ] 	Batch(0/2353) done. Loss: 0.3780  lr:0.010000
[ Thu May 16 19:09:22 2024 ] 	Batch(100/2353) done. Loss: 0.2651  lr:0.010000
[ Thu May 16 19:09:59 2024 ] 	Batch(200/2353) done. Loss: 0.5210  lr:0.010000
[ Thu May 16 19:10:37 2024 ] 	Batch(300/2353) done. Loss: 0.7148  lr:0.010000
[ Thu May 16 19:11:14 2024 ] 	Batch(400/2353) done. Loss: 0.8023  lr:0.010000
[ Thu May 16 19:11:51 2024 ] 	Batch(500/2353) done. Loss: 0.2962  lr:0.010000
[ Thu May 16 19:12:29 2024 ] 	Batch(600/2353) done. Loss: 0.3701  lr:0.010000
[ Thu May 16 19:13:06 2024 ] 	Batch(700/2353) done. Loss: 0.8229  lr:0.010000
[ Thu May 16 19:13:44 2024 ] 	Batch(800/2353) done. Loss: 0.7172  lr:0.010000
[ Thu May 16 19:14:21 2024 ] 	Batch(900/2353) done. Loss: 0.2504  lr:0.010000
[ Thu May 16 19:14:58 2024 ] 	Batch(1000/2353) done. Loss: 0.3283  lr:0.010000
[ Thu May 16 19:15:36 2024 ] 	Batch(1100/2353) done. Loss: 0.6443  lr:0.010000
[ Thu May 16 19:16:13 2024 ] 	Batch(1200/2353) done. Loss: 0.2472  lr:0.010000
[ Thu May 16 19:16:50 2024 ] 	Batch(1300/2353) done. Loss: 0.1569  lr:0.010000
[ Thu May 16 19:17:28 2024 ] 	Batch(1400/2353) done. Loss: 0.4668  lr:0.010000
[ Thu May 16 19:18:05 2024 ] 	Batch(1500/2353) done. Loss: 0.1946  lr:0.010000
[ Thu May 16 19:18:43 2024 ] 	Batch(1600/2353) done. Loss: 0.5531  lr:0.010000
[ Thu May 16 19:19:20 2024 ] 	Batch(1700/2353) done. Loss: 0.6414  lr:0.010000
[ Thu May 16 19:19:57 2024 ] 	Batch(1800/2353) done. Loss: 0.5695  lr:0.010000
[ Thu May 16 19:20:35 2024 ] 	Batch(1900/2353) done. Loss: 0.4071  lr:0.010000
[ Thu May 16 19:21:12 2024 ] 	Batch(2000/2353) done. Loss: 0.7633  lr:0.010000
[ Thu May 16 19:21:50 2024 ] 	Batch(2100/2353) done. Loss: 0.4846  lr:0.010000
[ Thu May 16 19:22:28 2024 ] 	Batch(2200/2353) done. Loss: 0.6025  lr:0.010000
[ Thu May 16 19:23:06 2024 ] 	Batch(2300/2353) done. Loss: 0.4792  lr:0.010000
[ Thu May 16 19:23:26 2024 ] 	Mean training loss: 0.4637.
[ Thu May 16 19:23:26 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 19:23:26 2024 ] Eval epoch: 20
[ Thu May 16 19:25:29 2024 ] 	Mean val loss of 2367 batches: 0.3773038570781021.
[ Thu May 16 19:25:29 2024 ] Training epoch: 21
[ Thu May 16 19:25:30 2024 ] 	Batch(0/2353) done. Loss: 0.2548  lr:0.010000
[ Thu May 16 19:26:07 2024 ] 	Batch(100/2353) done. Loss: 0.3673  lr:0.010000
[ Thu May 16 19:26:45 2024 ] 	Batch(200/2353) done. Loss: 1.0827  lr:0.010000
[ Thu May 16 19:27:22 2024 ] 	Batch(300/2353) done. Loss: 0.9346  lr:0.010000
[ Thu May 16 19:28:00 2024 ] 	Batch(400/2353) done. Loss: 0.3714  lr:0.010000
[ Thu May 16 19:28:37 2024 ] 	Batch(500/2353) done. Loss: 0.2620  lr:0.010000
[ Thu May 16 19:29:14 2024 ] 	Batch(600/2353) done. Loss: 0.1778  lr:0.010000
[ Thu May 16 19:29:52 2024 ] 	Batch(700/2353) done. Loss: 0.5371  lr:0.010000
[ Thu May 16 19:30:29 2024 ] 	Batch(800/2353) done. Loss: 0.3522  lr:0.010000
[ Thu May 16 19:31:07 2024 ] 	Batch(900/2353) done. Loss: 0.4593  lr:0.010000
[ Thu May 16 19:31:44 2024 ] 	Batch(1000/2353) done. Loss: 0.4690  lr:0.010000
[ Thu May 16 19:32:21 2024 ] 	Batch(1100/2353) done. Loss: 0.4590  lr:0.010000
[ Thu May 16 19:32:59 2024 ] 	Batch(1200/2353) done. Loss: 0.1660  lr:0.010000
[ Thu May 16 19:33:36 2024 ] 	Batch(1300/2353) done. Loss: 0.3435  lr:0.010000
[ Thu May 16 19:34:14 2024 ] 	Batch(1400/2353) done. Loss: 0.4809  lr:0.010000
[ Thu May 16 19:34:51 2024 ] 	Batch(1500/2353) done. Loss: 0.3137  lr:0.010000
[ Thu May 16 19:35:29 2024 ] 	Batch(1600/2353) done. Loss: 0.4731  lr:0.010000
[ Thu May 16 19:36:06 2024 ] 	Batch(1700/2353) done. Loss: 0.2900  lr:0.010000
[ Thu May 16 19:36:43 2024 ] 	Batch(1800/2353) done. Loss: 0.2573  lr:0.010000
[ Thu May 16 19:37:21 2024 ] 	Batch(1900/2353) done. Loss: 0.1235  lr:0.010000
[ Thu May 16 19:37:58 2024 ] 	Batch(2000/2353) done. Loss: 0.1153  lr:0.010000
[ Thu May 16 19:38:36 2024 ] 	Batch(2100/2353) done. Loss: 0.6013  lr:0.010000
[ Thu May 16 19:39:15 2024 ] 	Batch(2200/2353) done. Loss: 0.4569  lr:0.010000
[ Thu May 16 19:39:53 2024 ] 	Batch(2300/2353) done. Loss: 0.0971  lr:0.010000
[ Thu May 16 19:40:12 2024 ] 	Mean training loss: 0.4524.
[ Thu May 16 19:40:12 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 19:40:12 2024 ] Training epoch: 22
[ Thu May 16 19:40:13 2024 ] 	Batch(0/2353) done. Loss: 0.0994  lr:0.010000
[ Thu May 16 19:40:51 2024 ] 	Batch(100/2353) done. Loss: 0.2419  lr:0.010000
[ Thu May 16 19:41:28 2024 ] 	Batch(200/2353) done. Loss: 0.4927  lr:0.010000
[ Thu May 16 19:42:05 2024 ] 	Batch(300/2353) done. Loss: 0.1068  lr:0.010000
[ Thu May 16 19:42:43 2024 ] 	Batch(400/2353) done. Loss: 0.9616  lr:0.010000
[ Thu May 16 19:43:20 2024 ] 	Batch(500/2353) done. Loss: 0.9645  lr:0.010000
[ Thu May 16 19:43:58 2024 ] 	Batch(600/2353) done. Loss: 0.1390  lr:0.010000
[ Thu May 16 19:44:35 2024 ] 	Batch(700/2353) done. Loss: 0.1606  lr:0.010000
[ Thu May 16 19:45:12 2024 ] 	Batch(800/2353) done. Loss: 0.4663  lr:0.010000
[ Thu May 16 19:45:50 2024 ] 	Batch(900/2353) done. Loss: 0.5644  lr:0.010000
[ Thu May 16 19:46:27 2024 ] 	Batch(1000/2353) done. Loss: 0.5498  lr:0.010000
[ Thu May 16 19:47:05 2024 ] 	Batch(1100/2353) done. Loss: 0.0471  lr:0.010000
[ Thu May 16 19:47:42 2024 ] 	Batch(1200/2353) done. Loss: 0.1382  lr:0.010000
[ Thu May 16 19:48:19 2024 ] 	Batch(1300/2353) done. Loss: 0.3665  lr:0.010000
[ Thu May 16 19:48:57 2024 ] 	Batch(1400/2353) done. Loss: 0.3248  lr:0.010000
[ Thu May 16 19:49:34 2024 ] 	Batch(1500/2353) done. Loss: 0.4233  lr:0.010000
[ Thu May 16 19:50:12 2024 ] 	Batch(1600/2353) done. Loss: 0.8492  lr:0.010000
[ Thu May 16 19:50:50 2024 ] 	Batch(1700/2353) done. Loss: 0.2940  lr:0.010000
[ Thu May 16 19:51:28 2024 ] 	Batch(1800/2353) done. Loss: 0.3675  lr:0.010000
[ Thu May 16 19:52:06 2024 ] 	Batch(1900/2353) done. Loss: 0.4235  lr:0.010000
[ Thu May 16 19:52:44 2024 ] 	Batch(2000/2353) done. Loss: 1.4857  lr:0.010000
[ Thu May 16 19:53:22 2024 ] 	Batch(2100/2353) done. Loss: 0.3967  lr:0.010000
[ Thu May 16 19:54:00 2024 ] 	Batch(2200/2353) done. Loss: 0.3207  lr:0.010000
[ Thu May 16 19:54:38 2024 ] 	Batch(2300/2353) done. Loss: 0.3945  lr:0.010000
[ Thu May 16 19:54:58 2024 ] 	Mean training loss: 0.4323.
[ Thu May 16 19:54:58 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 19:54:58 2024 ] Training epoch: 23
[ Thu May 16 19:54:59 2024 ] 	Batch(0/2353) done. Loss: 0.3429  lr:0.010000
[ Thu May 16 19:55:36 2024 ] 	Batch(100/2353) done. Loss: 0.2179  lr:0.010000
[ Thu May 16 19:56:14 2024 ] 	Batch(200/2353) done. Loss: 0.2197  lr:0.010000
[ Thu May 16 19:56:52 2024 ] 	Batch(300/2353) done. Loss: 0.4652  lr:0.010000
[ Thu May 16 19:57:29 2024 ] 	Batch(400/2353) done. Loss: 0.8629  lr:0.010000
[ Thu May 16 19:58:07 2024 ] 	Batch(500/2353) done. Loss: 0.1117  lr:0.010000
[ Thu May 16 19:58:45 2024 ] 	Batch(600/2353) done. Loss: 0.2348  lr:0.010000
[ Thu May 16 19:59:22 2024 ] 	Batch(700/2353) done. Loss: 0.5322  lr:0.010000
[ Thu May 16 20:00:00 2024 ] 	Batch(800/2353) done. Loss: 0.2842  lr:0.010000
[ Thu May 16 20:00:38 2024 ] 	Batch(900/2353) done. Loss: 0.1368  lr:0.010000
[ Thu May 16 20:01:16 2024 ] 	Batch(1000/2353) done. Loss: 0.0825  lr:0.010000
[ Thu May 16 20:01:53 2024 ] 	Batch(1100/2353) done. Loss: 0.1709  lr:0.010000
[ Thu May 16 20:02:31 2024 ] 	Batch(1200/2353) done. Loss: 0.2366  lr:0.010000
[ Thu May 16 20:03:09 2024 ] 	Batch(1300/2353) done. Loss: 0.7368  lr:0.010000
[ Thu May 16 20:03:46 2024 ] 	Batch(1400/2353) done. Loss: 0.3372  lr:0.010000
[ Thu May 16 20:04:24 2024 ] 	Batch(1500/2353) done. Loss: 0.0971  lr:0.010000
[ Thu May 16 20:05:01 2024 ] 	Batch(1600/2353) done. Loss: 0.7124  lr:0.010000
[ Thu May 16 20:05:39 2024 ] 	Batch(1700/2353) done. Loss: 0.1179  lr:0.010000
[ Thu May 16 20:06:16 2024 ] 	Batch(1800/2353) done. Loss: 0.2155  lr:0.010000
[ Thu May 16 20:06:54 2024 ] 	Batch(1900/2353) done. Loss: 0.2172  lr:0.010000
[ Thu May 16 20:07:31 2024 ] 	Batch(2000/2353) done. Loss: 0.2860  lr:0.010000
[ Thu May 16 20:08:09 2024 ] 	Batch(2100/2353) done. Loss: 0.3237  lr:0.010000
[ Thu May 16 20:08:46 2024 ] 	Batch(2200/2353) done. Loss: 0.2509  lr:0.010000
[ Thu May 16 20:09:23 2024 ] 	Batch(2300/2353) done. Loss: 0.5981  lr:0.010000
[ Thu May 16 20:09:43 2024 ] 	Mean training loss: 0.4025.
[ Thu May 16 20:09:43 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 20:09:43 2024 ] Training epoch: 24
[ Thu May 16 20:09:44 2024 ] 	Batch(0/2353) done. Loss: 0.2288  lr:0.010000
[ Thu May 16 20:10:22 2024 ] 	Batch(100/2353) done. Loss: 0.2335  lr:0.010000
[ Thu May 16 20:10:59 2024 ] 	Batch(200/2353) done. Loss: 0.5976  lr:0.010000
[ Thu May 16 20:11:36 2024 ] 	Batch(300/2353) done. Loss: 0.6431  lr:0.010000
[ Thu May 16 20:12:14 2024 ] 	Batch(400/2353) done. Loss: 0.2684  lr:0.010000
[ Thu May 16 20:12:51 2024 ] 	Batch(500/2353) done. Loss: 0.3965  lr:0.010000
[ Thu May 16 20:13:28 2024 ] 	Batch(600/2353) done. Loss: 0.2663  lr:0.010000
[ Thu May 16 20:14:06 2024 ] 	Batch(700/2353) done. Loss: 0.1119  lr:0.010000
[ Thu May 16 20:14:43 2024 ] 	Batch(800/2353) done. Loss: 0.5358  lr:0.010000
[ Thu May 16 20:15:21 2024 ] 	Batch(900/2353) done. Loss: 0.1818  lr:0.010000
[ Thu May 16 20:15:58 2024 ] 	Batch(1000/2353) done. Loss: 0.2085  lr:0.010000
[ Thu May 16 20:16:35 2024 ] 	Batch(1100/2353) done. Loss: 0.3238  lr:0.010000
[ Thu May 16 20:17:13 2024 ] 	Batch(1200/2353) done. Loss: 0.3344  lr:0.010000
[ Thu May 16 20:17:51 2024 ] 	Batch(1300/2353) done. Loss: 0.5081  lr:0.010000
[ Thu May 16 20:18:29 2024 ] 	Batch(1400/2353) done. Loss: 0.2123  lr:0.010000
[ Thu May 16 20:19:07 2024 ] 	Batch(1500/2353) done. Loss: 0.9230  lr:0.010000
[ Thu May 16 20:19:44 2024 ] 	Batch(1600/2353) done. Loss: 0.6150  lr:0.010000
[ Thu May 16 20:20:22 2024 ] 	Batch(1700/2353) done. Loss: 0.7391  lr:0.010000
[ Thu May 16 20:20:59 2024 ] 	Batch(1800/2353) done. Loss: 0.2164  lr:0.010000
[ Thu May 16 20:21:36 2024 ] 	Batch(1900/2353) done. Loss: 0.2212  lr:0.010000
[ Thu May 16 20:22:14 2024 ] 	Batch(2000/2353) done. Loss: 0.4339  lr:0.010000
[ Thu May 16 20:22:52 2024 ] 	Batch(2100/2353) done. Loss: 1.2299  lr:0.010000
[ Thu May 16 20:23:30 2024 ] 	Batch(2200/2353) done. Loss: 0.4607  lr:0.010000
[ Thu May 16 20:24:07 2024 ] 	Batch(2300/2353) done. Loss: 0.7277  lr:0.010000
[ Thu May 16 20:24:27 2024 ] 	Mean training loss: 0.4038.
[ Thu May 16 20:24:27 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 20:24:27 2024 ] Training epoch: 25
[ Thu May 16 20:24:28 2024 ] 	Batch(0/2353) done. Loss: 0.5367  lr:0.010000
[ Thu May 16 20:25:06 2024 ] 	Batch(100/2353) done. Loss: 0.4191  lr:0.010000
[ Thu May 16 20:25:43 2024 ] 	Batch(200/2353) done. Loss: 0.0429  lr:0.010000
[ Thu May 16 20:26:20 2024 ] 	Batch(300/2353) done. Loss: 0.5449  lr:0.010000
[ Thu May 16 20:26:58 2024 ] 	Batch(400/2353) done. Loss: 0.0723  lr:0.010000
[ Thu May 16 20:27:35 2024 ] 	Batch(500/2353) done. Loss: 0.4792  lr:0.010000
[ Thu May 16 20:28:13 2024 ] 	Batch(600/2353) done. Loss: 0.1080  lr:0.010000
[ Thu May 16 20:28:51 2024 ] 	Batch(700/2353) done. Loss: 0.3042  lr:0.010000
[ Thu May 16 20:29:28 2024 ] 	Batch(800/2353) done. Loss: 0.6348  lr:0.010000
[ Thu May 16 20:30:06 2024 ] 	Batch(900/2353) done. Loss: 0.2025  lr:0.010000
[ Thu May 16 20:30:43 2024 ] 	Batch(1000/2353) done. Loss: 0.5578  lr:0.010000
[ Thu May 16 20:31:20 2024 ] 	Batch(1100/2353) done. Loss: 0.2596  lr:0.010000
[ Thu May 16 20:31:58 2024 ] 	Batch(1200/2353) done. Loss: 0.1324  lr:0.010000
[ Thu May 16 20:32:35 2024 ] 	Batch(1300/2353) done. Loss: 0.3451  lr:0.010000
[ Thu May 16 20:33:13 2024 ] 	Batch(1400/2353) done. Loss: 0.5816  lr:0.010000
[ Thu May 16 20:33:50 2024 ] 	Batch(1500/2353) done. Loss: 0.3311  lr:0.010000
[ Thu May 16 20:34:28 2024 ] 	Batch(1600/2353) done. Loss: 0.5021  lr:0.010000
[ Thu May 16 20:35:05 2024 ] 	Batch(1700/2353) done. Loss: 0.2112  lr:0.010000
[ Thu May 16 20:35:42 2024 ] 	Batch(1800/2353) done. Loss: 0.4638  lr:0.010000
[ Thu May 16 20:36:20 2024 ] 	Batch(1900/2353) done. Loss: 0.3030  lr:0.010000
[ Thu May 16 20:36:57 2024 ] 	Batch(2000/2353) done. Loss: 0.1926  lr:0.010000
[ Thu May 16 20:37:35 2024 ] 	Batch(2100/2353) done. Loss: 0.5404  lr:0.010000
[ Thu May 16 20:38:12 2024 ] 	Batch(2200/2353) done. Loss: 0.1707  lr:0.010000
[ Thu May 16 20:38:49 2024 ] 	Batch(2300/2353) done. Loss: 0.2257  lr:0.010000
[ Thu May 16 20:39:09 2024 ] 	Mean training loss: 0.3913.
[ Thu May 16 20:39:09 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 20:39:09 2024 ] Training epoch: 26
[ Thu May 16 20:39:10 2024 ] 	Batch(0/2353) done. Loss: 0.3439  lr:0.010000
[ Thu May 16 20:39:47 2024 ] 	Batch(100/2353) done. Loss: 0.9730  lr:0.010000
[ Thu May 16 20:40:24 2024 ] 	Batch(200/2353) done. Loss: 0.2335  lr:0.010000
[ Thu May 16 20:41:02 2024 ] 	Batch(300/2353) done. Loss: 0.1423  lr:0.010000
[ Thu May 16 20:41:39 2024 ] 	Batch(400/2353) done. Loss: 0.3427  lr:0.010000
[ Thu May 16 20:42:17 2024 ] 	Batch(500/2353) done. Loss: 0.0515  lr:0.010000
[ Thu May 16 20:42:54 2024 ] 	Batch(600/2353) done. Loss: 0.2379  lr:0.010000
[ Thu May 16 20:43:31 2024 ] 	Batch(700/2353) done. Loss: 0.3589  lr:0.010000
[ Thu May 16 20:44:09 2024 ] 	Batch(800/2353) done. Loss: 0.0979  lr:0.010000
[ Thu May 16 20:44:46 2024 ] 	Batch(900/2353) done. Loss: 0.3120  lr:0.010000
[ Thu May 16 20:45:24 2024 ] 	Batch(1000/2353) done. Loss: 0.1762  lr:0.010000
[ Thu May 16 20:46:01 2024 ] 	Batch(1100/2353) done. Loss: 0.1446  lr:0.010000
[ Thu May 16 20:46:38 2024 ] 	Batch(1200/2353) done. Loss: 0.2412  lr:0.010000
[ Thu May 16 20:47:16 2024 ] 	Batch(1300/2353) done. Loss: 0.2456  lr:0.010000
[ Thu May 16 20:47:53 2024 ] 	Batch(1400/2353) done. Loss: 0.3322  lr:0.010000
[ Thu May 16 20:48:31 2024 ] 	Batch(1500/2353) done. Loss: 0.2704  lr:0.010000
[ Thu May 16 20:49:08 2024 ] 	Batch(1600/2353) done. Loss: 0.3095  lr:0.010000
[ Thu May 16 20:49:46 2024 ] 	Batch(1700/2353) done. Loss: 0.3717  lr:0.010000
[ Thu May 16 20:50:23 2024 ] 	Batch(1800/2353) done. Loss: 0.4197  lr:0.010000
[ Thu May 16 20:51:00 2024 ] 	Batch(1900/2353) done. Loss: 0.1404  lr:0.010000
[ Thu May 16 20:51:38 2024 ] 	Batch(2000/2353) done. Loss: 0.1671  lr:0.010000
[ Thu May 16 20:52:16 2024 ] 	Batch(2100/2353) done. Loss: 0.6393  lr:0.010000
[ Thu May 16 20:52:54 2024 ] 	Batch(2200/2353) done. Loss: 0.5909  lr:0.010000
[ Thu May 16 20:53:32 2024 ] 	Batch(2300/2353) done. Loss: 0.2660  lr:0.010000
[ Thu May 16 20:53:51 2024 ] 	Mean training loss: 0.3780.
[ Thu May 16 20:53:51 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 20:53:51 2024 ] Training epoch: 27
[ Thu May 16 20:53:52 2024 ] 	Batch(0/2353) done. Loss: 0.1380  lr:0.010000
[ Thu May 16 20:54:30 2024 ] 	Batch(100/2353) done. Loss: 0.1720  lr:0.010000
[ Thu May 16 20:55:07 2024 ] 	Batch(200/2353) done. Loss: 0.2118  lr:0.010000
[ Thu May 16 20:55:44 2024 ] 	Batch(300/2353) done. Loss: 0.1663  lr:0.010000
[ Thu May 16 20:56:22 2024 ] 	Batch(400/2353) done. Loss: 0.3882  lr:0.010000
[ Thu May 16 20:56:59 2024 ] 	Batch(500/2353) done. Loss: 0.3394  lr:0.010000
[ Thu May 16 20:57:36 2024 ] 	Batch(600/2353) done. Loss: 0.2309  lr:0.010000
[ Thu May 16 20:58:14 2024 ] 	Batch(700/2353) done. Loss: 0.1532  lr:0.010000
[ Thu May 16 20:58:51 2024 ] 	Batch(800/2353) done. Loss: 0.4545  lr:0.010000
[ Thu May 16 20:59:29 2024 ] 	Batch(900/2353) done. Loss: 0.0951  lr:0.010000
[ Thu May 16 21:00:06 2024 ] 	Batch(1000/2353) done. Loss: 0.4126  lr:0.010000
[ Thu May 16 21:00:43 2024 ] 	Batch(1100/2353) done. Loss: 0.9546  lr:0.010000
[ Thu May 16 21:01:21 2024 ] 	Batch(1200/2353) done. Loss: 0.3713  lr:0.010000
[ Thu May 16 21:01:58 2024 ] 	Batch(1300/2353) done. Loss: 0.2385  lr:0.010000
[ Thu May 16 21:02:36 2024 ] 	Batch(1400/2353) done. Loss: 0.4430  lr:0.010000
[ Thu May 16 21:03:13 2024 ] 	Batch(1500/2353) done. Loss: 0.5923  lr:0.010000
[ Thu May 16 21:03:51 2024 ] 	Batch(1600/2353) done. Loss: 0.2677  lr:0.010000
[ Thu May 16 21:04:28 2024 ] 	Batch(1700/2353) done. Loss: 0.0502  lr:0.010000
[ Thu May 16 21:05:05 2024 ] 	Batch(1800/2353) done. Loss: 0.2601  lr:0.010000
[ Thu May 16 21:05:43 2024 ] 	Batch(1900/2353) done. Loss: 0.4307  lr:0.010000
[ Thu May 16 21:06:20 2024 ] 	Batch(2000/2353) done. Loss: 1.1832  lr:0.010000
[ Thu May 16 21:06:57 2024 ] 	Batch(2100/2353) done. Loss: 0.2205  lr:0.010000
[ Thu May 16 21:07:35 2024 ] 	Batch(2200/2353) done. Loss: 0.2895  lr:0.010000
[ Thu May 16 21:08:12 2024 ] 	Batch(2300/2353) done. Loss: 0.1248  lr:0.010000
[ Thu May 16 21:08:32 2024 ] 	Mean training loss: 0.3647.
[ Thu May 16 21:08:32 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 21:08:32 2024 ] Training epoch: 28
[ Thu May 16 21:08:33 2024 ] 	Batch(0/2353) done. Loss: 0.2121  lr:0.010000
[ Thu May 16 21:09:10 2024 ] 	Batch(100/2353) done. Loss: 0.4777  lr:0.010000
[ Thu May 16 21:09:47 2024 ] 	Batch(200/2353) done. Loss: 0.2356  lr:0.010000
[ Thu May 16 21:10:25 2024 ] 	Batch(300/2353) done. Loss: 0.2011  lr:0.010000
[ Thu May 16 21:11:02 2024 ] 	Batch(400/2353) done. Loss: 0.2031  lr:0.010000
[ Thu May 16 21:11:39 2024 ] 	Batch(500/2353) done. Loss: 0.1631  lr:0.010000
[ Thu May 16 21:12:17 2024 ] 	Batch(600/2353) done. Loss: 0.2375  lr:0.010000
[ Thu May 16 21:12:55 2024 ] 	Batch(700/2353) done. Loss: 0.6928  lr:0.010000
[ Thu May 16 21:13:33 2024 ] 	Batch(800/2353) done. Loss: 0.1004  lr:0.010000
[ Thu May 16 21:14:12 2024 ] 	Batch(900/2353) done. Loss: 0.2882  lr:0.010000
[ Thu May 16 21:14:50 2024 ] 	Batch(1000/2353) done. Loss: 0.1075  lr:0.010000
[ Thu May 16 21:15:28 2024 ] 	Batch(1100/2353) done. Loss: 0.3249  lr:0.010000
[ Thu May 16 21:16:06 2024 ] 	Batch(1200/2353) done. Loss: 0.0392  lr:0.010000
[ Thu May 16 21:16:43 2024 ] 	Batch(1300/2353) done. Loss: 0.7854  lr:0.010000
[ Thu May 16 21:17:21 2024 ] 	Batch(1400/2353) done. Loss: 0.2756  lr:0.010000
[ Thu May 16 21:17:58 2024 ] 	Batch(1500/2353) done. Loss: 0.1942  lr:0.010000
[ Thu May 16 21:18:36 2024 ] 	Batch(1600/2353) done. Loss: 0.1470  lr:0.010000
[ Thu May 16 21:19:13 2024 ] 	Batch(1700/2353) done. Loss: 0.2269  lr:0.010000
[ Thu May 16 21:19:51 2024 ] 	Batch(1800/2353) done. Loss: 0.9176  lr:0.010000
[ Thu May 16 21:20:28 2024 ] 	Batch(1900/2353) done. Loss: 0.3533  lr:0.010000
[ Thu May 16 21:21:06 2024 ] 	Batch(2000/2353) done. Loss: 0.1338  lr:0.010000
[ Thu May 16 21:21:44 2024 ] 	Batch(2100/2353) done. Loss: 0.0273  lr:0.010000
[ Thu May 16 21:22:21 2024 ] 	Batch(2200/2353) done. Loss: 0.3342  lr:0.010000
[ Thu May 16 21:22:59 2024 ] 	Batch(2300/2353) done. Loss: 0.1516  lr:0.010000
[ Thu May 16 21:23:18 2024 ] 	Mean training loss: 0.3551.
[ Thu May 16 21:23:18 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 21:23:18 2024 ] Training epoch: 29
[ Thu May 16 21:23:19 2024 ] 	Batch(0/2353) done. Loss: 0.3839  lr:0.010000
[ Thu May 16 21:23:56 2024 ] 	Batch(100/2353) done. Loss: 0.2518  lr:0.010000
[ Thu May 16 21:24:34 2024 ] 	Batch(200/2353) done. Loss: 0.2213  lr:0.010000
[ Thu May 16 21:25:11 2024 ] 	Batch(300/2353) done. Loss: 0.3162  lr:0.010000
[ Thu May 16 21:25:48 2024 ] 	Batch(400/2353) done. Loss: 0.1435  lr:0.010000
[ Thu May 16 21:26:26 2024 ] 	Batch(500/2353) done. Loss: 0.4642  lr:0.010000
[ Thu May 16 21:27:03 2024 ] 	Batch(600/2353) done. Loss: 0.3035  lr:0.010000
[ Thu May 16 21:27:41 2024 ] 	Batch(700/2353) done. Loss: 0.5180  lr:0.010000
[ Thu May 16 21:28:18 2024 ] 	Batch(800/2353) done. Loss: 0.1221  lr:0.010000
[ Thu May 16 21:28:55 2024 ] 	Batch(900/2353) done. Loss: 0.1508  lr:0.010000
[ Thu May 16 21:29:33 2024 ] 	Batch(1000/2353) done. Loss: 0.2554  lr:0.010000
[ Thu May 16 21:30:10 2024 ] 	Batch(1100/2353) done. Loss: 0.4948  lr:0.010000
[ Thu May 16 21:30:47 2024 ] 	Batch(1200/2353) done. Loss: 0.1168  lr:0.010000
[ Thu May 16 21:31:25 2024 ] 	Batch(1300/2353) done. Loss: 0.1489  lr:0.010000
[ Thu May 16 21:32:02 2024 ] 	Batch(1400/2353) done. Loss: 0.0647  lr:0.010000
[ Thu May 16 21:32:39 2024 ] 	Batch(1500/2353) done. Loss: 0.4094  lr:0.010000
[ Thu May 16 21:33:17 2024 ] 	Batch(1600/2353) done. Loss: 0.3507  lr:0.010000
[ Thu May 16 21:33:54 2024 ] 	Batch(1700/2353) done. Loss: 0.4712  lr:0.010000
[ Thu May 16 21:34:32 2024 ] 	Batch(1800/2353) done. Loss: 0.6702  lr:0.010000
[ Thu May 16 21:35:09 2024 ] 	Batch(1900/2353) done. Loss: 0.3875  lr:0.010000
[ Thu May 16 21:35:46 2024 ] 	Batch(2000/2353) done. Loss: 0.0569  lr:0.010000
[ Thu May 16 21:36:24 2024 ] 	Batch(2100/2353) done. Loss: 0.0400  lr:0.010000
[ Thu May 16 21:37:01 2024 ] 	Batch(2200/2353) done. Loss: 0.6406  lr:0.010000
[ Thu May 16 21:37:38 2024 ] 	Batch(2300/2353) done. Loss: 0.3889  lr:0.010000
[ Thu May 16 21:37:58 2024 ] 	Mean training loss: 0.3430.
[ Thu May 16 21:37:58 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 21:37:58 2024 ] Training epoch: 30
[ Thu May 16 21:37:59 2024 ] 	Batch(0/2353) done. Loss: 0.4197  lr:0.010000
[ Thu May 16 21:38:36 2024 ] 	Batch(100/2353) done. Loss: 0.2939  lr:0.010000
[ Thu May 16 21:39:14 2024 ] 	Batch(200/2353) done. Loss: 0.3443  lr:0.010000
[ Thu May 16 21:39:51 2024 ] 	Batch(300/2353) done. Loss: 0.2175  lr:0.010000
[ Thu May 16 21:40:28 2024 ] 	Batch(400/2353) done. Loss: 0.6168  lr:0.010000
[ Thu May 16 21:41:06 2024 ] 	Batch(500/2353) done. Loss: 0.3801  lr:0.010000
[ Thu May 16 21:41:43 2024 ] 	Batch(600/2353) done. Loss: 0.2966  lr:0.010000
[ Thu May 16 21:42:21 2024 ] 	Batch(700/2353) done. Loss: 0.2669  lr:0.010000
[ Thu May 16 21:42:58 2024 ] 	Batch(800/2353) done. Loss: 0.4073  lr:0.010000
[ Thu May 16 21:43:35 2024 ] 	Batch(900/2353) done. Loss: 0.5025  lr:0.010000
[ Thu May 16 21:44:13 2024 ] 	Batch(1000/2353) done. Loss: 0.4762  lr:0.010000
[ Thu May 16 21:44:50 2024 ] 	Batch(1100/2353) done. Loss: 0.3965  lr:0.010000
[ Thu May 16 21:45:27 2024 ] 	Batch(1200/2353) done. Loss: 0.4234  lr:0.010000
[ Thu May 16 21:46:05 2024 ] 	Batch(1300/2353) done. Loss: 0.6269  lr:0.010000
[ Thu May 16 21:46:42 2024 ] 	Batch(1400/2353) done. Loss: 0.2131  lr:0.010000
[ Thu May 16 21:47:19 2024 ] 	Batch(1500/2353) done. Loss: 0.1514  lr:0.010000
[ Thu May 16 21:47:57 2024 ] 	Batch(1600/2353) done. Loss: 0.1800  lr:0.010000
[ Thu May 16 21:48:34 2024 ] 	Batch(1700/2353) done. Loss: 1.1362  lr:0.010000
[ Thu May 16 21:49:12 2024 ] 	Batch(1800/2353) done. Loss: 0.2038  lr:0.010000
[ Thu May 16 21:49:49 2024 ] 	Batch(1900/2353) done. Loss: 0.7650  lr:0.010000
[ Thu May 16 21:50:26 2024 ] 	Batch(2000/2353) done. Loss: 0.2600  lr:0.010000
[ Thu May 16 21:51:04 2024 ] 	Batch(2100/2353) done. Loss: 0.5192  lr:0.010000
[ Thu May 16 21:51:41 2024 ] 	Batch(2200/2353) done. Loss: 0.9358  lr:0.010000
[ Thu May 16 21:52:18 2024 ] 	Batch(2300/2353) done. Loss: 0.3099  lr:0.010000
[ Thu May 16 21:52:38 2024 ] 	Mean training loss: 0.3367.
[ Thu May 16 21:52:38 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 21:52:38 2024 ] Eval epoch: 30
[ Thu May 16 21:54:41 2024 ] 	Mean val loss of 2367 batches: 0.34793065871701057.
[ Thu May 16 21:54:41 2024 ] Training epoch: 31
[ Thu May 16 21:54:42 2024 ] 	Batch(0/2353) done. Loss: 0.0848  lr:0.010000
[ Thu May 16 21:55:19 2024 ] 	Batch(100/2353) done. Loss: 0.1466  lr:0.010000
[ Thu May 16 21:55:57 2024 ] 	Batch(200/2353) done. Loss: 0.6480  lr:0.010000
[ Thu May 16 21:56:34 2024 ] 	Batch(300/2353) done. Loss: 0.3284  lr:0.010000
[ Thu May 16 21:57:11 2024 ] 	Batch(400/2353) done. Loss: 0.6316  lr:0.010000
[ Thu May 16 21:57:49 2024 ] 	Batch(500/2353) done. Loss: 0.3519  lr:0.010000
[ Thu May 16 21:58:26 2024 ] 	Batch(600/2353) done. Loss: 0.1867  lr:0.010000
[ Thu May 16 21:59:03 2024 ] 	Batch(700/2353) done. Loss: 0.2455  lr:0.010000
[ Thu May 16 21:59:41 2024 ] 	Batch(800/2353) done. Loss: 0.3961  lr:0.010000
[ Thu May 16 22:00:18 2024 ] 	Batch(900/2353) done. Loss: 0.4513  lr:0.010000
[ Thu May 16 22:00:55 2024 ] 	Batch(1000/2353) done. Loss: 0.6513  lr:0.010000
[ Thu May 16 22:01:33 2024 ] 	Batch(1100/2353) done. Loss: 0.2104  lr:0.010000
[ Thu May 16 22:02:10 2024 ] 	Batch(1200/2353) done. Loss: 0.3392  lr:0.010000
[ Thu May 16 22:02:48 2024 ] 	Batch(1300/2353) done. Loss: 0.5834  lr:0.010000
[ Thu May 16 22:03:25 2024 ] 	Batch(1400/2353) done. Loss: 0.5351  lr:0.010000
[ Thu May 16 22:04:02 2024 ] 	Batch(1500/2353) done. Loss: 0.3756  lr:0.010000
[ Thu May 16 22:04:40 2024 ] 	Batch(1600/2353) done. Loss: 0.3518  lr:0.010000
[ Thu May 16 22:05:18 2024 ] 	Batch(1700/2353) done. Loss: 0.4505  lr:0.010000
[ Thu May 16 22:05:56 2024 ] 	Batch(1800/2353) done. Loss: 0.4962  lr:0.010000
[ Thu May 16 22:06:34 2024 ] 	Batch(1900/2353) done. Loss: 0.2329  lr:0.010000
[ Thu May 16 22:07:12 2024 ] 	Batch(2000/2353) done. Loss: 0.2683  lr:0.010000
[ Thu May 16 22:07:50 2024 ] 	Batch(2100/2353) done. Loss: 0.3174  lr:0.010000
[ Thu May 16 22:08:27 2024 ] 	Batch(2200/2353) done. Loss: 0.3847  lr:0.010000
[ Thu May 16 22:09:05 2024 ] 	Batch(2300/2353) done. Loss: 0.1361  lr:0.010000
[ Thu May 16 22:09:24 2024 ] 	Mean training loss: 0.3294.
[ Thu May 16 22:09:24 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 22:09:24 2024 ] Training epoch: 32
[ Thu May 16 22:09:25 2024 ] 	Batch(0/2353) done. Loss: 0.2018  lr:0.010000
[ Thu May 16 22:10:02 2024 ] 	Batch(100/2353) done. Loss: 0.3357  lr:0.010000
[ Thu May 16 22:10:40 2024 ] 	Batch(200/2353) done. Loss: 0.4449  lr:0.010000
[ Thu May 16 22:11:17 2024 ] 	Batch(300/2353) done. Loss: 0.1099  lr:0.010000
[ Thu May 16 22:11:55 2024 ] 	Batch(400/2353) done. Loss: 0.1646  lr:0.010000
[ Thu May 16 22:12:33 2024 ] 	Batch(500/2353) done. Loss: 0.2006  lr:0.010000
[ Thu May 16 22:13:11 2024 ] 	Batch(600/2353) done. Loss: 0.1296  lr:0.010000
[ Thu May 16 22:13:50 2024 ] 	Batch(700/2353) done. Loss: 0.1689  lr:0.010000
[ Thu May 16 22:14:28 2024 ] 	Batch(800/2353) done. Loss: 0.1026  lr:0.010000
[ Thu May 16 22:15:05 2024 ] 	Batch(900/2353) done. Loss: 0.3246  lr:0.010000
[ Thu May 16 22:15:42 2024 ] 	Batch(1000/2353) done. Loss: 0.8179  lr:0.010000
[ Thu May 16 22:16:20 2024 ] 	Batch(1100/2353) done. Loss: 0.3559  lr:0.010000
[ Thu May 16 22:16:57 2024 ] 	Batch(1200/2353) done. Loss: 0.3867  lr:0.010000
[ Thu May 16 22:17:35 2024 ] 	Batch(1300/2353) done. Loss: 0.7114  lr:0.010000
[ Thu May 16 22:18:12 2024 ] 	Batch(1400/2353) done. Loss: 0.2863  lr:0.010000
[ Thu May 16 22:18:49 2024 ] 	Batch(1500/2353) done. Loss: 0.6371  lr:0.010000
[ Thu May 16 22:19:27 2024 ] 	Batch(1600/2353) done. Loss: 0.4294  lr:0.010000
[ Thu May 16 22:20:04 2024 ] 	Batch(1700/2353) done. Loss: 0.0810  lr:0.010000
[ Thu May 16 22:20:42 2024 ] 	Batch(1800/2353) done. Loss: 0.1955  lr:0.010000
[ Thu May 16 22:21:19 2024 ] 	Batch(1900/2353) done. Loss: 0.3634  lr:0.010000
[ Thu May 16 22:21:56 2024 ] 	Batch(2000/2353) done. Loss: 0.0735  lr:0.010000
[ Thu May 16 22:22:34 2024 ] 	Batch(2100/2353) done. Loss: 0.4604  lr:0.010000
[ Thu May 16 22:23:12 2024 ] 	Batch(2200/2353) done. Loss: 0.2488  lr:0.010000
[ Thu May 16 22:23:49 2024 ] 	Batch(2300/2353) done. Loss: 0.4021  lr:0.010000
[ Thu May 16 22:24:09 2024 ] 	Mean training loss: 0.3015.
[ Thu May 16 22:24:09 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 22:24:09 2024 ] Training epoch: 33
[ Thu May 16 22:24:10 2024 ] 	Batch(0/2353) done. Loss: 0.1580  lr:0.010000
[ Thu May 16 22:24:47 2024 ] 	Batch(100/2353) done. Loss: 0.4598  lr:0.010000
[ Thu May 16 22:25:24 2024 ] 	Batch(200/2353) done. Loss: 0.2376  lr:0.010000
[ Thu May 16 22:26:02 2024 ] 	Batch(300/2353) done. Loss: 0.1509  lr:0.010000
[ Thu May 16 22:26:40 2024 ] 	Batch(400/2353) done. Loss: 0.2646  lr:0.010000
[ Thu May 16 22:27:18 2024 ] 	Batch(500/2353) done. Loss: 0.7261  lr:0.010000
[ Thu May 16 22:27:56 2024 ] 	Batch(600/2353) done. Loss: 0.8786  lr:0.010000
[ Thu May 16 22:28:34 2024 ] 	Batch(700/2353) done. Loss: 0.2094  lr:0.010000
[ Thu May 16 22:29:11 2024 ] 	Batch(800/2353) done. Loss: 0.1052  lr:0.010000
[ Thu May 16 22:29:49 2024 ] 	Batch(900/2353) done. Loss: 0.5275  lr:0.010000
[ Thu May 16 22:30:27 2024 ] 	Batch(1000/2353) done. Loss: 0.2003  lr:0.010000
[ Thu May 16 22:31:04 2024 ] 	Batch(1100/2353) done. Loss: 0.1988  lr:0.010000
[ Thu May 16 22:31:42 2024 ] 	Batch(1200/2353) done. Loss: 0.1149  lr:0.010000
[ Thu May 16 22:32:19 2024 ] 	Batch(1300/2353) done. Loss: 0.1492  lr:0.010000
[ Thu May 16 22:32:57 2024 ] 	Batch(1400/2353) done. Loss: 0.0941  lr:0.010000
[ Thu May 16 22:33:34 2024 ] 	Batch(1500/2353) done. Loss: 0.1666  lr:0.010000
[ Thu May 16 22:34:12 2024 ] 	Batch(1600/2353) done. Loss: 0.2915  lr:0.010000
[ Thu May 16 22:34:49 2024 ] 	Batch(1700/2353) done. Loss: 0.2165  lr:0.010000
[ Thu May 16 22:35:27 2024 ] 	Batch(1800/2353) done. Loss: 0.2535  lr:0.010000
[ Thu May 16 22:36:04 2024 ] 	Batch(1900/2353) done. Loss: 0.3757  lr:0.010000
[ Thu May 16 22:36:42 2024 ] 	Batch(2000/2353) done. Loss: 0.5505  lr:0.010000
[ Thu May 16 22:37:19 2024 ] 	Batch(2100/2353) done. Loss: 0.0448  lr:0.010000
[ Thu May 16 22:37:56 2024 ] 	Batch(2200/2353) done. Loss: 0.6089  lr:0.010000
[ Thu May 16 22:38:34 2024 ] 	Batch(2300/2353) done. Loss: 0.2710  lr:0.010000
[ Thu May 16 22:38:53 2024 ] 	Mean training loss: 0.2966.
[ Thu May 16 22:38:53 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 22:38:53 2024 ] Training epoch: 34
[ Thu May 16 22:38:54 2024 ] 	Batch(0/2353) done. Loss: 0.1078  lr:0.010000
[ Thu May 16 22:39:32 2024 ] 	Batch(100/2353) done. Loss: 0.1351  lr:0.010000
[ Thu May 16 22:40:09 2024 ] 	Batch(200/2353) done. Loss: 0.1947  lr:0.010000
[ Thu May 16 22:40:46 2024 ] 	Batch(300/2353) done. Loss: 0.2523  lr:0.010000
[ Thu May 16 22:41:24 2024 ] 	Batch(400/2353) done. Loss: 0.5592  lr:0.010000
[ Thu May 16 22:42:01 2024 ] 	Batch(500/2353) done. Loss: 0.1938  lr:0.010000
[ Thu May 16 22:42:38 2024 ] 	Batch(600/2353) done. Loss: 0.1319  lr:0.010000
[ Thu May 16 22:43:16 2024 ] 	Batch(700/2353) done. Loss: 0.0859  lr:0.010000
[ Thu May 16 22:43:53 2024 ] 	Batch(800/2353) done. Loss: 0.1628  lr:0.010000
[ Thu May 16 22:44:31 2024 ] 	Batch(900/2353) done. Loss: 0.1868  lr:0.010000
[ Thu May 16 22:45:09 2024 ] 	Batch(1000/2353) done. Loss: 0.2979  lr:0.010000
[ Thu May 16 22:45:46 2024 ] 	Batch(1100/2353) done. Loss: 0.6012  lr:0.010000
[ Thu May 16 22:46:24 2024 ] 	Batch(1200/2353) done. Loss: 0.2290  lr:0.010000
[ Thu May 16 22:47:01 2024 ] 	Batch(1300/2353) done. Loss: 0.1712  lr:0.010000
[ Thu May 16 22:47:38 2024 ] 	Batch(1400/2353) done. Loss: 0.2068  lr:0.010000
[ Thu May 16 22:48:16 2024 ] 	Batch(1500/2353) done. Loss: 0.1202  lr:0.010000
[ Thu May 16 22:48:53 2024 ] 	Batch(1600/2353) done. Loss: 0.3381  lr:0.010000
[ Thu May 16 22:49:31 2024 ] 	Batch(1700/2353) done. Loss: 0.4555  lr:0.010000
[ Thu May 16 22:50:08 2024 ] 	Batch(1800/2353) done. Loss: 0.5737  lr:0.010000
[ Thu May 16 22:50:45 2024 ] 	Batch(1900/2353) done. Loss: 0.9064  lr:0.010000
[ Thu May 16 22:51:23 2024 ] 	Batch(2000/2353) done. Loss: 0.1159  lr:0.010000
[ Thu May 16 22:52:00 2024 ] 	Batch(2100/2353) done. Loss: 0.2674  lr:0.010000
[ Thu May 16 22:52:38 2024 ] 	Batch(2200/2353) done. Loss: 0.2893  lr:0.010000
[ Thu May 16 22:53:15 2024 ] 	Batch(2300/2353) done. Loss: 0.3667  lr:0.010000
[ Thu May 16 22:53:34 2024 ] 	Mean training loss: 0.2930.
[ Thu May 16 22:53:34 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 22:53:34 2024 ] Training epoch: 35
[ Thu May 16 22:53:35 2024 ] 	Batch(0/2353) done. Loss: 0.4512  lr:0.010000
[ Thu May 16 22:54:13 2024 ] 	Batch(100/2353) done. Loss: 0.3561  lr:0.010000
[ Thu May 16 22:54:50 2024 ] 	Batch(200/2353) done. Loss: 0.2272  lr:0.010000
[ Thu May 16 22:55:28 2024 ] 	Batch(300/2353) done. Loss: 0.1499  lr:0.010000
[ Thu May 16 22:56:05 2024 ] 	Batch(400/2353) done. Loss: 0.1775  lr:0.010000
[ Thu May 16 22:56:43 2024 ] 	Batch(500/2353) done. Loss: 0.1055  lr:0.010000
[ Thu May 16 22:57:20 2024 ] 	Batch(600/2353) done. Loss: 0.0769  lr:0.010000
[ Thu May 16 22:57:57 2024 ] 	Batch(700/2353) done. Loss: 0.2390  lr:0.010000
[ Thu May 16 22:58:35 2024 ] 	Batch(800/2353) done. Loss: 0.1747  lr:0.010000
[ Thu May 16 22:59:12 2024 ] 	Batch(900/2353) done. Loss: 0.1545  lr:0.010000
[ Thu May 16 22:59:50 2024 ] 	Batch(1000/2353) done. Loss: 0.2131  lr:0.010000
[ Thu May 16 23:00:27 2024 ] 	Batch(1100/2353) done. Loss: 0.1042  lr:0.010000
[ Thu May 16 23:01:04 2024 ] 	Batch(1200/2353) done. Loss: 0.4277  lr:0.010000
[ Thu May 16 23:01:42 2024 ] 	Batch(1300/2353) done. Loss: 0.2227  lr:0.010000
[ Thu May 16 23:02:19 2024 ] 	Batch(1400/2353) done. Loss: 0.4467  lr:0.010000
[ Thu May 16 23:02:56 2024 ] 	Batch(1500/2353) done. Loss: 0.1890  lr:0.010000
[ Thu May 16 23:03:34 2024 ] 	Batch(1600/2353) done. Loss: 0.2329  lr:0.010000
[ Thu May 16 23:04:11 2024 ] 	Batch(1700/2353) done. Loss: 0.0682  lr:0.010000
[ Thu May 16 23:04:48 2024 ] 	Batch(1800/2353) done. Loss: 0.4833  lr:0.010000
[ Thu May 16 23:05:26 2024 ] 	Batch(1900/2353) done. Loss: 0.1104  lr:0.010000
[ Thu May 16 23:06:03 2024 ] 	Batch(2000/2353) done. Loss: 0.3028  lr:0.010000
[ Thu May 16 23:06:41 2024 ] 	Batch(2100/2353) done. Loss: 0.4830  lr:0.010000
[ Thu May 16 23:07:18 2024 ] 	Batch(2200/2353) done. Loss: 0.0997  lr:0.010000
[ Thu May 16 23:07:56 2024 ] 	Batch(2300/2353) done. Loss: 0.4003  lr:0.010000
[ Thu May 16 23:08:15 2024 ] 	Mean training loss: 0.2748.
[ Thu May 16 23:08:15 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 23:08:15 2024 ] Training epoch: 36
[ Thu May 16 23:08:16 2024 ] 	Batch(0/2353) done. Loss: 0.2381  lr:0.010000
[ Thu May 16 23:08:53 2024 ] 	Batch(100/2353) done. Loss: 0.1828  lr:0.010000
[ Thu May 16 23:09:31 2024 ] 	Batch(200/2353) done. Loss: 0.1618  lr:0.010000
[ Thu May 16 23:10:08 2024 ] 	Batch(300/2353) done. Loss: 0.1350  lr:0.010000
[ Thu May 16 23:10:45 2024 ] 	Batch(400/2353) done. Loss: 0.3867  lr:0.010000
[ Thu May 16 23:11:23 2024 ] 	Batch(500/2353) done. Loss: 0.1946  lr:0.010000
[ Thu May 16 23:12:00 2024 ] 	Batch(600/2353) done. Loss: 0.0651  lr:0.010000
[ Thu May 16 23:12:38 2024 ] 	Batch(700/2353) done. Loss: 0.2041  lr:0.010000
[ Thu May 16 23:13:15 2024 ] 	Batch(800/2353) done. Loss: 0.0397  lr:0.010000
[ Thu May 16 23:13:52 2024 ] 	Batch(900/2353) done. Loss: 0.3684  lr:0.010000
[ Thu May 16 23:14:30 2024 ] 	Batch(1000/2353) done. Loss: 0.0730  lr:0.010000
[ Thu May 16 23:15:07 2024 ] 	Batch(1100/2353) done. Loss: 0.0643  lr:0.010000
[ Thu May 16 23:15:45 2024 ] 	Batch(1200/2353) done. Loss: 0.5897  lr:0.010000
[ Thu May 16 23:16:22 2024 ] 	Batch(1300/2353) done. Loss: 0.2451  lr:0.010000
[ Thu May 16 23:16:59 2024 ] 	Batch(1400/2353) done. Loss: 0.2316  lr:0.010000
[ Thu May 16 23:17:37 2024 ] 	Batch(1500/2353) done. Loss: 0.3887  lr:0.010000
[ Thu May 16 23:18:14 2024 ] 	Batch(1600/2353) done. Loss: 0.1665  lr:0.010000
[ Thu May 16 23:18:52 2024 ] 	Batch(1700/2353) done. Loss: 0.2513  lr:0.010000
[ Thu May 16 23:19:30 2024 ] 	Batch(1800/2353) done. Loss: 0.2722  lr:0.010000
[ Thu May 16 23:20:08 2024 ] 	Batch(1900/2353) done. Loss: 0.8457  lr:0.010000
[ Thu May 16 23:20:47 2024 ] 	Batch(2000/2353) done. Loss: 0.4299  lr:0.010000
[ Thu May 16 23:21:24 2024 ] 	Batch(2100/2353) done. Loss: 0.6832  lr:0.010000
[ Thu May 16 23:22:01 2024 ] 	Batch(2200/2353) done. Loss: 0.0394  lr:0.010000
[ Thu May 16 23:22:39 2024 ] 	Batch(2300/2353) done. Loss: 0.5170  lr:0.010000
[ Thu May 16 23:22:58 2024 ] 	Mean training loss: 0.2817.
[ Thu May 16 23:22:58 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 23:22:58 2024 ] Training epoch: 37
[ Thu May 16 23:22:59 2024 ] 	Batch(0/2353) done. Loss: 0.1565  lr:0.010000
[ Thu May 16 23:23:36 2024 ] 	Batch(100/2353) done. Loss: 0.2640  lr:0.010000
[ Thu May 16 23:24:14 2024 ] 	Batch(200/2353) done. Loss: 0.4139  lr:0.010000
[ Thu May 16 23:24:51 2024 ] 	Batch(300/2353) done. Loss: 0.1387  lr:0.010000
[ Thu May 16 23:25:28 2024 ] 	Batch(400/2353) done. Loss: 0.6620  lr:0.010000
[ Thu May 16 23:26:06 2024 ] 	Batch(500/2353) done. Loss: 0.0857  lr:0.010000
[ Thu May 16 23:26:43 2024 ] 	Batch(600/2353) done. Loss: 0.1656  lr:0.010000
[ Thu May 16 23:27:20 2024 ] 	Batch(700/2353) done. Loss: 0.4203  lr:0.010000
[ Thu May 16 23:27:58 2024 ] 	Batch(800/2353) done. Loss: 0.1882  lr:0.010000
[ Thu May 16 23:28:36 2024 ] 	Batch(900/2353) done. Loss: 0.3146  lr:0.010000
[ Thu May 16 23:29:13 2024 ] 	Batch(1000/2353) done. Loss: 0.4082  lr:0.010000
[ Thu May 16 23:29:51 2024 ] 	Batch(1100/2353) done. Loss: 0.2113  lr:0.010000
[ Thu May 16 23:30:29 2024 ] 	Batch(1200/2353) done. Loss: 0.3894  lr:0.010000
[ Thu May 16 23:31:06 2024 ] 	Batch(1300/2353) done. Loss: 0.0208  lr:0.010000
[ Thu May 16 23:31:44 2024 ] 	Batch(1400/2353) done. Loss: 0.2392  lr:0.010000
[ Thu May 16 23:32:22 2024 ] 	Batch(1500/2353) done. Loss: 0.5355  lr:0.010000
[ Thu May 16 23:33:00 2024 ] 	Batch(1600/2353) done. Loss: 0.5180  lr:0.010000
[ Thu May 16 23:33:37 2024 ] 	Batch(1700/2353) done. Loss: 0.0179  lr:0.010000
[ Thu May 16 23:34:15 2024 ] 	Batch(1800/2353) done. Loss: 0.0861  lr:0.010000
[ Thu May 16 23:34:53 2024 ] 	Batch(1900/2353) done. Loss: 0.0408  lr:0.010000
[ Thu May 16 23:35:30 2024 ] 	Batch(2000/2353) done. Loss: 0.1547  lr:0.010000
[ Thu May 16 23:36:08 2024 ] 	Batch(2100/2353) done. Loss: 0.1986  lr:0.010000
[ Thu May 16 23:36:46 2024 ] 	Batch(2200/2353) done. Loss: 0.1296  lr:0.010000
[ Thu May 16 23:37:24 2024 ] 	Batch(2300/2353) done. Loss: 0.8340  lr:0.010000
[ Thu May 16 23:37:44 2024 ] 	Mean training loss: 0.2666.
[ Thu May 16 23:37:44 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 23:37:44 2024 ] Training epoch: 38
[ Thu May 16 23:37:44 2024 ] 	Batch(0/2353) done. Loss: 0.0866  lr:0.010000
[ Thu May 16 23:38:22 2024 ] 	Batch(100/2353) done. Loss: 0.4299  lr:0.010000
[ Thu May 16 23:39:00 2024 ] 	Batch(200/2353) done. Loss: 0.3602  lr:0.010000
[ Thu May 16 23:39:38 2024 ] 	Batch(300/2353) done. Loss: 0.2293  lr:0.010000
[ Thu May 16 23:40:15 2024 ] 	Batch(400/2353) done. Loss: 0.1565  lr:0.010000
[ Thu May 16 23:40:53 2024 ] 	Batch(500/2353) done. Loss: 0.1587  lr:0.010000
[ Thu May 16 23:41:31 2024 ] 	Batch(600/2353) done. Loss: 0.3665  lr:0.010000
[ Thu May 16 23:42:08 2024 ] 	Batch(700/2353) done. Loss: 0.2576  lr:0.010000
[ Thu May 16 23:42:46 2024 ] 	Batch(800/2353) done. Loss: 0.1692  lr:0.010000
[ Thu May 16 23:43:24 2024 ] 	Batch(900/2353) done. Loss: 0.1542  lr:0.010000
[ Thu May 16 23:44:01 2024 ] 	Batch(1000/2353) done. Loss: 0.3139  lr:0.010000
[ Thu May 16 23:44:38 2024 ] 	Batch(1100/2353) done. Loss: 0.3896  lr:0.010000
[ Thu May 16 23:45:16 2024 ] 	Batch(1200/2353) done. Loss: 0.1338  lr:0.010000
[ Thu May 16 23:45:53 2024 ] 	Batch(1300/2353) done. Loss: 0.3338  lr:0.010000
[ Thu May 16 23:46:31 2024 ] 	Batch(1400/2353) done. Loss: 0.1361  lr:0.010000
[ Thu May 16 23:47:08 2024 ] 	Batch(1500/2353) done. Loss: 0.4100  lr:0.010000
[ Thu May 16 23:47:45 2024 ] 	Batch(1600/2353) done. Loss: 0.4023  lr:0.010000
[ Thu May 16 23:48:23 2024 ] 	Batch(1700/2353) done. Loss: 0.0677  lr:0.010000
[ Thu May 16 23:49:00 2024 ] 	Batch(1800/2353) done. Loss: 0.3862  lr:0.010000
[ Thu May 16 23:49:37 2024 ] 	Batch(1900/2353) done. Loss: 0.0218  lr:0.010000
[ Thu May 16 23:50:15 2024 ] 	Batch(2000/2353) done. Loss: 0.5094  lr:0.010000
[ Thu May 16 23:50:53 2024 ] 	Batch(2100/2353) done. Loss: 0.3370  lr:0.010000
[ Thu May 16 23:51:31 2024 ] 	Batch(2200/2353) done. Loss: 0.3121  lr:0.010000
[ Thu May 16 23:52:09 2024 ] 	Batch(2300/2353) done. Loss: 0.2449  lr:0.010000
[ Thu May 16 23:52:29 2024 ] 	Mean training loss: 0.2547.
[ Thu May 16 23:52:29 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Thu May 16 23:52:29 2024 ] Training epoch: 39
[ Thu May 16 23:52:30 2024 ] 	Batch(0/2353) done. Loss: 0.0887  lr:0.010000
[ Thu May 16 23:53:07 2024 ] 	Batch(100/2353) done. Loss: 0.1859  lr:0.010000
[ Thu May 16 23:53:45 2024 ] 	Batch(200/2353) done. Loss: 0.2590  lr:0.010000
[ Thu May 16 23:54:22 2024 ] 	Batch(300/2353) done. Loss: 0.3216  lr:0.010000
[ Thu May 16 23:54:59 2024 ] 	Batch(400/2353) done. Loss: 0.0386  lr:0.010000
[ Thu May 16 23:55:37 2024 ] 	Batch(500/2353) done. Loss: 0.7233  lr:0.010000
[ Thu May 16 23:56:14 2024 ] 	Batch(600/2353) done. Loss: 0.1722  lr:0.010000
[ Thu May 16 23:56:51 2024 ] 	Batch(700/2353) done. Loss: 0.5668  lr:0.010000
[ Thu May 16 23:57:29 2024 ] 	Batch(800/2353) done. Loss: 0.4609  lr:0.010000
[ Thu May 16 23:58:06 2024 ] 	Batch(900/2353) done. Loss: 0.1335  lr:0.010000
[ Thu May 16 23:58:44 2024 ] 	Batch(1000/2353) done. Loss: 0.1035  lr:0.010000
[ Thu May 16 23:59:21 2024 ] 	Batch(1100/2353) done. Loss: 0.7719  lr:0.010000
[ Thu May 16 23:59:58 2024 ] 	Batch(1200/2353) done. Loss: 0.3491  lr:0.010000
[ Fri May 17 00:00:36 2024 ] 	Batch(1300/2353) done. Loss: 0.3770  lr:0.010000
[ Fri May 17 00:01:13 2024 ] 	Batch(1400/2353) done. Loss: 0.0768  lr:0.010000
[ Fri May 17 00:01:50 2024 ] 	Batch(1500/2353) done. Loss: 0.0410  lr:0.010000
[ Fri May 17 00:02:28 2024 ] 	Batch(1600/2353) done. Loss: 0.4088  lr:0.010000
[ Fri May 17 00:03:06 2024 ] 	Batch(1700/2353) done. Loss: 0.0790  lr:0.010000
[ Fri May 17 00:03:43 2024 ] 	Batch(1800/2353) done. Loss: 0.3571  lr:0.010000
[ Fri May 17 00:04:20 2024 ] 	Batch(1900/2353) done. Loss: 0.3676  lr:0.010000
[ Fri May 17 00:04:58 2024 ] 	Batch(2000/2353) done. Loss: 0.3194  lr:0.010000
[ Fri May 17 00:05:36 2024 ] 	Batch(2100/2353) done. Loss: 0.5614  lr:0.010000
[ Fri May 17 00:06:13 2024 ] 	Batch(2200/2353) done. Loss: 0.3092  lr:0.010000
[ Fri May 17 00:06:51 2024 ] 	Batch(2300/2353) done. Loss: 0.2003  lr:0.010000
[ Fri May 17 00:07:10 2024 ] 	Mean training loss: 0.2499.
[ Fri May 17 00:07:10 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 00:07:11 2024 ] Training epoch: 40
[ Fri May 17 00:07:11 2024 ] 	Batch(0/2353) done. Loss: 0.0430  lr:0.010000
[ Fri May 17 00:07:49 2024 ] 	Batch(100/2353) done. Loss: 0.3053  lr:0.010000
[ Fri May 17 00:08:26 2024 ] 	Batch(200/2353) done. Loss: 0.1954  lr:0.010000
[ Fri May 17 00:09:04 2024 ] 	Batch(300/2353) done. Loss: 0.4027  lr:0.010000
[ Fri May 17 00:09:42 2024 ] 	Batch(400/2353) done. Loss: 0.1773  lr:0.010000
[ Fri May 17 00:10:19 2024 ] 	Batch(500/2353) done. Loss: 0.1117  lr:0.010000
[ Fri May 17 00:10:57 2024 ] 	Batch(600/2353) done. Loss: 0.1681  lr:0.010000
[ Fri May 17 00:11:34 2024 ] 	Batch(700/2353) done. Loss: 0.6273  lr:0.010000
[ Fri May 17 00:12:12 2024 ] 	Batch(800/2353) done. Loss: 0.2189  lr:0.010000
[ Fri May 17 00:12:50 2024 ] 	Batch(900/2353) done. Loss: 0.5221  lr:0.010000
[ Fri May 17 00:13:28 2024 ] 	Batch(1000/2353) done. Loss: 0.1491  lr:0.010000
[ Fri May 17 00:14:06 2024 ] 	Batch(1100/2353) done. Loss: 0.2604  lr:0.010000
[ Fri May 17 00:14:44 2024 ] 	Batch(1200/2353) done. Loss: 0.6601  lr:0.010000
[ Fri May 17 00:15:22 2024 ] 	Batch(1300/2353) done. Loss: 0.0193  lr:0.010000
[ Fri May 17 00:16:00 2024 ] 	Batch(1400/2353) done. Loss: 0.0490  lr:0.010000
[ Fri May 17 00:16:38 2024 ] 	Batch(1500/2353) done. Loss: 0.1227  lr:0.010000
[ Fri May 17 00:17:17 2024 ] 	Batch(1600/2353) done. Loss: 0.2212  lr:0.010000
[ Fri May 17 00:17:55 2024 ] 	Batch(1700/2353) done. Loss: 0.1974  lr:0.010000
[ Fri May 17 00:18:32 2024 ] 	Batch(1800/2353) done. Loss: 0.1172  lr:0.010000
[ Fri May 17 00:19:10 2024 ] 	Batch(1900/2353) done. Loss: 0.2902  lr:0.010000
[ Fri May 17 00:19:47 2024 ] 	Batch(2000/2353) done. Loss: 0.1550  lr:0.010000
[ Fri May 17 00:20:24 2024 ] 	Batch(2100/2353) done. Loss: 0.5697  lr:0.010000
[ Fri May 17 00:21:02 2024 ] 	Batch(2200/2353) done. Loss: 0.0267  lr:0.010000
[ Fri May 17 00:21:39 2024 ] 	Batch(2300/2353) done. Loss: 0.2284  lr:0.010000
[ Fri May 17 00:21:59 2024 ] 	Mean training loss: 0.2419.
[ Fri May 17 00:21:59 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 00:21:59 2024 ] Eval epoch: 40
[ Fri May 17 00:24:02 2024 ] 	Mean val loss of 2367 batches: 0.33279519982241473.
[ Fri May 17 00:24:02 2024 ] Training epoch: 41
[ Fri May 17 00:24:03 2024 ] 	Batch(0/2353) done. Loss: 0.5938  lr:0.010000
[ Fri May 17 00:24:40 2024 ] 	Batch(100/2353) done. Loss: 0.3456  lr:0.010000
[ Fri May 17 00:25:17 2024 ] 	Batch(200/2353) done. Loss: 0.2669  lr:0.010000
[ Fri May 17 00:25:55 2024 ] 	Batch(300/2353) done. Loss: 0.3207  lr:0.010000
[ Fri May 17 00:26:32 2024 ] 	Batch(400/2353) done. Loss: 0.0819  lr:0.010000
[ Fri May 17 00:27:09 2024 ] 	Batch(500/2353) done. Loss: 0.3374  lr:0.010000
[ Fri May 17 00:27:47 2024 ] 	Batch(600/2353) done. Loss: 0.2446  lr:0.010000
[ Fri May 17 00:28:24 2024 ] 	Batch(700/2353) done. Loss: 0.5253  lr:0.010000
[ Fri May 17 00:29:01 2024 ] 	Batch(800/2353) done. Loss: 0.1141  lr:0.010000
[ Fri May 17 00:29:39 2024 ] 	Batch(900/2353) done. Loss: 0.6072  lr:0.010000
[ Fri May 17 00:30:17 2024 ] 	Batch(1000/2353) done. Loss: 0.3048  lr:0.010000
[ Fri May 17 00:30:55 2024 ] 	Batch(1100/2353) done. Loss: 0.3119  lr:0.010000
[ Fri May 17 00:31:33 2024 ] 	Batch(1200/2353) done. Loss: 0.0355  lr:0.010000
[ Fri May 17 00:32:10 2024 ] 	Batch(1300/2353) done. Loss: 0.0412  lr:0.010000
[ Fri May 17 00:32:48 2024 ] 	Batch(1400/2353) done. Loss: 0.2698  lr:0.010000
[ Fri May 17 00:33:25 2024 ] 	Batch(1500/2353) done. Loss: 0.1820  lr:0.010000
[ Fri May 17 00:34:02 2024 ] 	Batch(1600/2353) done. Loss: 0.1554  lr:0.010000
[ Fri May 17 00:34:41 2024 ] 	Batch(1700/2353) done. Loss: 0.3841  lr:0.010000
[ Fri May 17 00:35:19 2024 ] 	Batch(1800/2353) done. Loss: 0.4388  lr:0.010000
[ Fri May 17 00:35:56 2024 ] 	Batch(1900/2353) done. Loss: 0.0437  lr:0.010000
[ Fri May 17 00:36:34 2024 ] 	Batch(2000/2353) done. Loss: 0.3227  lr:0.010000
[ Fri May 17 00:37:11 2024 ] 	Batch(2100/2353) done. Loss: 0.4097  lr:0.010000
[ Fri May 17 00:37:48 2024 ] 	Batch(2200/2353) done. Loss: 0.4140  lr:0.010000
[ Fri May 17 00:38:26 2024 ] 	Batch(2300/2353) done. Loss: 0.0435  lr:0.010000
[ Fri May 17 00:38:45 2024 ] 	Mean training loss: 0.2271.
[ Fri May 17 00:38:45 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 00:38:45 2024 ] Training epoch: 42
[ Fri May 17 00:38:46 2024 ] 	Batch(0/2353) done. Loss: 0.1710  lr:0.010000
[ Fri May 17 00:39:24 2024 ] 	Batch(100/2353) done. Loss: 0.3403  lr:0.010000
[ Fri May 17 00:40:02 2024 ] 	Batch(200/2353) done. Loss: 0.3006  lr:0.010000
[ Fri May 17 00:40:40 2024 ] 	Batch(300/2353) done. Loss: 0.0184  lr:0.010000
[ Fri May 17 00:41:17 2024 ] 	Batch(400/2353) done. Loss: 0.0714  lr:0.010000
[ Fri May 17 00:41:55 2024 ] 	Batch(500/2353) done. Loss: 0.2459  lr:0.010000
[ Fri May 17 00:42:32 2024 ] 	Batch(600/2353) done. Loss: 0.5991  lr:0.010000
[ Fri May 17 00:43:09 2024 ] 	Batch(700/2353) done. Loss: 0.2193  lr:0.010000
[ Fri May 17 00:43:47 2024 ] 	Batch(800/2353) done. Loss: 0.4128  lr:0.010000
[ Fri May 17 00:44:24 2024 ] 	Batch(900/2353) done. Loss: 0.0791  lr:0.010000
[ Fri May 17 00:45:01 2024 ] 	Batch(1000/2353) done. Loss: 0.0903  lr:0.010000
[ Fri May 17 00:45:39 2024 ] 	Batch(1100/2353) done. Loss: 0.0790  lr:0.010000
[ Fri May 17 00:46:16 2024 ] 	Batch(1200/2353) done. Loss: 0.2545  lr:0.010000
[ Fri May 17 00:46:54 2024 ] 	Batch(1300/2353) done. Loss: 0.3207  lr:0.010000
[ Fri May 17 00:47:31 2024 ] 	Batch(1400/2353) done. Loss: 0.1245  lr:0.010000
[ Fri May 17 00:48:08 2024 ] 	Batch(1500/2353) done. Loss: 0.2926  lr:0.010000
[ Fri May 17 00:48:46 2024 ] 	Batch(1600/2353) done. Loss: 0.4598  lr:0.010000
[ Fri May 17 00:49:23 2024 ] 	Batch(1700/2353) done. Loss: 0.0530  lr:0.010000
[ Fri May 17 00:50:01 2024 ] 	Batch(1800/2353) done. Loss: 0.0352  lr:0.010000
[ Fri May 17 00:50:38 2024 ] 	Batch(1900/2353) done. Loss: 0.3483  lr:0.010000
[ Fri May 17 00:51:15 2024 ] 	Batch(2000/2353) done. Loss: 0.3003  lr:0.010000
[ Fri May 17 00:51:53 2024 ] 	Batch(2100/2353) done. Loss: 0.1555  lr:0.010000
[ Fri May 17 00:52:30 2024 ] 	Batch(2200/2353) done. Loss: 0.2316  lr:0.010000
[ Fri May 17 00:53:07 2024 ] 	Batch(2300/2353) done. Loss: 0.1166  lr:0.010000
[ Fri May 17 00:53:27 2024 ] 	Mean training loss: 0.2276.
[ Fri May 17 00:53:27 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 00:53:27 2024 ] Training epoch: 43
[ Fri May 17 00:53:28 2024 ] 	Batch(0/2353) done. Loss: 0.3308  lr:0.010000
[ Fri May 17 00:54:05 2024 ] 	Batch(100/2353) done. Loss: 0.3661  lr:0.010000
[ Fri May 17 00:54:42 2024 ] 	Batch(200/2353) done. Loss: 0.4044  lr:0.010000
[ Fri May 17 00:55:20 2024 ] 	Batch(300/2353) done. Loss: 0.1307  lr:0.010000
[ Fri May 17 00:55:57 2024 ] 	Batch(400/2353) done. Loss: 0.3893  lr:0.010000
[ Fri May 17 00:56:34 2024 ] 	Batch(500/2353) done. Loss: 0.3092  lr:0.010000
[ Fri May 17 00:57:12 2024 ] 	Batch(600/2353) done. Loss: 0.0540  lr:0.010000
[ Fri May 17 00:57:49 2024 ] 	Batch(700/2353) done. Loss: 0.1250  lr:0.010000
[ Fri May 17 00:58:27 2024 ] 	Batch(800/2353) done. Loss: 0.1230  lr:0.010000
[ Fri May 17 00:59:04 2024 ] 	Batch(900/2353) done. Loss: 0.1974  lr:0.010000
[ Fri May 17 00:59:42 2024 ] 	Batch(1000/2353) done. Loss: 0.3690  lr:0.010000
[ Fri May 17 01:00:20 2024 ] 	Batch(1100/2353) done. Loss: 0.2314  lr:0.010000
[ Fri May 17 01:00:58 2024 ] 	Batch(1200/2353) done. Loss: 0.1100  lr:0.010000
[ Fri May 17 01:01:36 2024 ] 	Batch(1300/2353) done. Loss: 0.0849  lr:0.010000
[ Fri May 17 01:02:14 2024 ] 	Batch(1400/2353) done. Loss: 0.3907  lr:0.010000
[ Fri May 17 01:02:52 2024 ] 	Batch(1500/2353) done. Loss: 0.2192  lr:0.010000
[ Fri May 17 01:03:29 2024 ] 	Batch(1600/2353) done. Loss: 0.2000  lr:0.010000
[ Fri May 17 01:04:07 2024 ] 	Batch(1700/2353) done. Loss: 0.5303  lr:0.010000
[ Fri May 17 01:04:44 2024 ] 	Batch(1800/2353) done. Loss: 0.0801  lr:0.010000
[ Fri May 17 01:05:21 2024 ] 	Batch(1900/2353) done. Loss: 0.0884  lr:0.010000
[ Fri May 17 01:05:59 2024 ] 	Batch(2000/2353) done. Loss: 0.7376  lr:0.010000
[ Fri May 17 01:06:37 2024 ] 	Batch(2100/2353) done. Loss: 0.4816  lr:0.010000
[ Fri May 17 01:07:14 2024 ] 	Batch(2200/2353) done. Loss: 0.1479  lr:0.010000
[ Fri May 17 01:07:51 2024 ] 	Batch(2300/2353) done. Loss: 0.1243  lr:0.010000
[ Fri May 17 01:08:11 2024 ] 	Mean training loss: 0.2294.
[ Fri May 17 01:08:11 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 01:08:11 2024 ] Training epoch: 44
[ Fri May 17 01:08:12 2024 ] 	Batch(0/2353) done. Loss: 0.1309  lr:0.010000
[ Fri May 17 01:08:50 2024 ] 	Batch(100/2353) done. Loss: 0.1327  lr:0.010000
[ Fri May 17 01:09:27 2024 ] 	Batch(200/2353) done. Loss: 0.2312  lr:0.010000
[ Fri May 17 01:10:05 2024 ] 	Batch(300/2353) done. Loss: 0.2410  lr:0.010000
[ Fri May 17 01:10:42 2024 ] 	Batch(400/2353) done. Loss: 0.2558  lr:0.010000
[ Fri May 17 01:11:19 2024 ] 	Batch(500/2353) done. Loss: 0.1368  lr:0.010000
[ Fri May 17 01:11:57 2024 ] 	Batch(600/2353) done. Loss: 0.0630  lr:0.010000
[ Fri May 17 01:12:34 2024 ] 	Batch(700/2353) done. Loss: 0.0691  lr:0.010000
[ Fri May 17 01:13:12 2024 ] 	Batch(800/2353) done. Loss: 0.0748  lr:0.010000
[ Fri May 17 01:13:49 2024 ] 	Batch(900/2353) done. Loss: 0.0670  lr:0.010000
[ Fri May 17 01:14:26 2024 ] 	Batch(1000/2353) done. Loss: 0.1700  lr:0.010000
[ Fri May 17 01:15:04 2024 ] 	Batch(1100/2353) done. Loss: 0.2722  lr:0.010000
[ Fri May 17 01:15:41 2024 ] 	Batch(1200/2353) done. Loss: 0.4105  lr:0.010000
[ Fri May 17 01:16:19 2024 ] 	Batch(1300/2353) done. Loss: 0.0291  lr:0.010000
[ Fri May 17 01:16:56 2024 ] 	Batch(1400/2353) done. Loss: 0.2540  lr:0.010000
[ Fri May 17 01:17:34 2024 ] 	Batch(1500/2353) done. Loss: 0.3092  lr:0.010000
[ Fri May 17 01:18:11 2024 ] 	Batch(1600/2353) done. Loss: 0.1732  lr:0.010000
[ Fri May 17 01:18:48 2024 ] 	Batch(1700/2353) done. Loss: 0.3493  lr:0.010000
[ Fri May 17 01:19:26 2024 ] 	Batch(1800/2353) done. Loss: 0.1019  lr:0.010000
[ Fri May 17 01:20:03 2024 ] 	Batch(1900/2353) done. Loss: 0.0989  lr:0.010000
[ Fri May 17 01:20:41 2024 ] 	Batch(2000/2353) done. Loss: 0.3084  lr:0.010000
[ Fri May 17 01:21:18 2024 ] 	Batch(2100/2353) done. Loss: 0.0157  lr:0.010000
[ Fri May 17 01:21:55 2024 ] 	Batch(2200/2353) done. Loss: 0.2761  lr:0.010000
[ Fri May 17 01:22:33 2024 ] 	Batch(2300/2353) done. Loss: 0.1963  lr:0.010000
[ Fri May 17 01:22:52 2024 ] 	Mean training loss: 0.2111.
[ Fri May 17 01:22:52 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 01:22:52 2024 ] Training epoch: 45
[ Fri May 17 01:22:53 2024 ] 	Batch(0/2353) done. Loss: 0.2079  lr:0.010000
[ Fri May 17 01:23:30 2024 ] 	Batch(100/2353) done. Loss: 0.2050  lr:0.010000
[ Fri May 17 01:24:08 2024 ] 	Batch(200/2353) done. Loss: 0.1967  lr:0.010000
[ Fri May 17 01:24:45 2024 ] 	Batch(300/2353) done. Loss: 0.1791  lr:0.010000
[ Fri May 17 01:25:23 2024 ] 	Batch(400/2353) done. Loss: 0.5215  lr:0.010000
[ Fri May 17 01:26:00 2024 ] 	Batch(500/2353) done. Loss: 0.1299  lr:0.010000
[ Fri May 17 01:26:37 2024 ] 	Batch(600/2353) done. Loss: 0.4303  lr:0.010000
[ Fri May 17 01:27:15 2024 ] 	Batch(700/2353) done. Loss: 0.0587  lr:0.010000
[ Fri May 17 01:27:52 2024 ] 	Batch(800/2353) done. Loss: 0.0760  lr:0.010000
[ Fri May 17 01:28:29 2024 ] 	Batch(900/2353) done. Loss: 0.1777  lr:0.010000
[ Fri May 17 01:29:07 2024 ] 	Batch(1000/2353) done. Loss: 0.1195  lr:0.010000
[ Fri May 17 01:29:44 2024 ] 	Batch(1100/2353) done. Loss: 0.1253  lr:0.010000
[ Fri May 17 01:30:22 2024 ] 	Batch(1200/2353) done. Loss: 0.2623  lr:0.010000
[ Fri May 17 01:30:59 2024 ] 	Batch(1300/2353) done. Loss: 0.1066  lr:0.010000
[ Fri May 17 01:31:36 2024 ] 	Batch(1400/2353) done. Loss: 0.4183  lr:0.010000
[ Fri May 17 01:32:14 2024 ] 	Batch(1500/2353) done. Loss: 0.0855  lr:0.010000
[ Fri May 17 01:32:51 2024 ] 	Batch(1600/2353) done. Loss: 0.1202  lr:0.010000
[ Fri May 17 01:33:28 2024 ] 	Batch(1700/2353) done. Loss: 0.4248  lr:0.010000
[ Fri May 17 01:34:06 2024 ] 	Batch(1800/2353) done. Loss: 0.0907  lr:0.010000
[ Fri May 17 01:34:43 2024 ] 	Batch(1900/2353) done. Loss: 0.0213  lr:0.010000
[ Fri May 17 01:35:20 2024 ] 	Batch(2000/2353) done. Loss: 0.1871  lr:0.010000
[ Fri May 17 01:35:59 2024 ] 	Batch(2100/2353) done. Loss: 0.1031  lr:0.010000
[ Fri May 17 01:36:37 2024 ] 	Batch(2200/2353) done. Loss: 0.2564  lr:0.010000
[ Fri May 17 01:37:15 2024 ] 	Batch(2300/2353) done. Loss: 0.2640  lr:0.010000
[ Fri May 17 01:37:35 2024 ] 	Mean training loss: 0.2095.
[ Fri May 17 01:37:35 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 01:37:35 2024 ] Training epoch: 46
[ Fri May 17 01:37:35 2024 ] 	Batch(0/2353) done. Loss: 0.2815  lr:0.010000
[ Fri May 17 01:38:13 2024 ] 	Batch(100/2353) done. Loss: 0.4905  lr:0.010000
[ Fri May 17 01:38:50 2024 ] 	Batch(200/2353) done. Loss: 0.3470  lr:0.010000
[ Fri May 17 01:39:27 2024 ] 	Batch(300/2353) done. Loss: 0.1362  lr:0.010000
[ Fri May 17 01:40:05 2024 ] 	Batch(400/2353) done. Loss: 0.0492  lr:0.010000
[ Fri May 17 01:40:42 2024 ] 	Batch(500/2353) done. Loss: 0.2023  lr:0.010000
[ Fri May 17 01:41:20 2024 ] 	Batch(600/2353) done. Loss: 0.0426  lr:0.010000
[ Fri May 17 01:41:57 2024 ] 	Batch(700/2353) done. Loss: 0.0774  lr:0.010000
[ Fri May 17 01:42:35 2024 ] 	Batch(800/2353) done. Loss: 0.1579  lr:0.010000
[ Fri May 17 01:43:13 2024 ] 	Batch(900/2353) done. Loss: 0.1887  lr:0.010000
[ Fri May 17 01:43:51 2024 ] 	Batch(1000/2353) done. Loss: 0.0771  lr:0.010000
[ Fri May 17 01:44:29 2024 ] 	Batch(1100/2353) done. Loss: 0.1748  lr:0.010000
[ Fri May 17 01:45:06 2024 ] 	Batch(1200/2353) done. Loss: 0.0577  lr:0.010000
[ Fri May 17 01:45:43 2024 ] 	Batch(1300/2353) done. Loss: 0.0295  lr:0.010000
[ Fri May 17 01:46:21 2024 ] 	Batch(1400/2353) done. Loss: 0.1103  lr:0.010000
[ Fri May 17 01:46:58 2024 ] 	Batch(1500/2353) done. Loss: 0.0279  lr:0.010000
[ Fri May 17 01:47:36 2024 ] 	Batch(1600/2353) done. Loss: 0.1183  lr:0.010000
[ Fri May 17 01:48:14 2024 ] 	Batch(1700/2353) done. Loss: 0.3911  lr:0.010000
[ Fri May 17 01:48:51 2024 ] 	Batch(1800/2353) done. Loss: 0.2193  lr:0.010000
[ Fri May 17 01:49:29 2024 ] 	Batch(1900/2353) done. Loss: 0.2684  lr:0.010000
[ Fri May 17 01:50:06 2024 ] 	Batch(2000/2353) done. Loss: 0.1867  lr:0.010000
[ Fri May 17 01:50:44 2024 ] 	Batch(2100/2353) done. Loss: 0.0750  lr:0.010000
[ Fri May 17 01:51:21 2024 ] 	Batch(2200/2353) done. Loss: 0.1470  lr:0.010000
[ Fri May 17 01:51:58 2024 ] 	Batch(2300/2353) done. Loss: 0.0901  lr:0.010000
[ Fri May 17 01:52:18 2024 ] 	Mean training loss: 0.2058.
[ Fri May 17 01:52:18 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 01:52:18 2024 ] Training epoch: 47
[ Fri May 17 01:52:19 2024 ] 	Batch(0/2353) done. Loss: 0.1390  lr:0.010000
[ Fri May 17 01:52:56 2024 ] 	Batch(100/2353) done. Loss: 0.2226  lr:0.010000
[ Fri May 17 01:53:33 2024 ] 	Batch(200/2353) done. Loss: 0.4131  lr:0.010000
[ Fri May 17 01:54:11 2024 ] 	Batch(300/2353) done. Loss: 0.0569  lr:0.010000
[ Fri May 17 01:54:48 2024 ] 	Batch(400/2353) done. Loss: 0.1435  lr:0.010000
[ Fri May 17 01:55:25 2024 ] 	Batch(500/2353) done. Loss: 0.1660  lr:0.010000
[ Fri May 17 01:56:03 2024 ] 	Batch(600/2353) done. Loss: 0.0907  lr:0.010000
[ Fri May 17 01:56:40 2024 ] 	Batch(700/2353) done. Loss: 0.1205  lr:0.010000
[ Fri May 17 01:57:17 2024 ] 	Batch(800/2353) done. Loss: 0.0815  lr:0.010000
[ Fri May 17 01:57:55 2024 ] 	Batch(900/2353) done. Loss: 0.2396  lr:0.010000
[ Fri May 17 01:58:33 2024 ] 	Batch(1000/2353) done. Loss: 0.0610  lr:0.010000
[ Fri May 17 01:59:11 2024 ] 	Batch(1100/2353) done. Loss: 0.1114  lr:0.010000
[ Fri May 17 01:59:50 2024 ] 	Batch(1200/2353) done. Loss: 0.1880  lr:0.010000
[ Fri May 17 02:00:27 2024 ] 	Batch(1300/2353) done. Loss: 0.2320  lr:0.010000
[ Fri May 17 02:01:04 2024 ] 	Batch(1400/2353) done. Loss: 0.4328  lr:0.010000
[ Fri May 17 02:01:42 2024 ] 	Batch(1500/2353) done. Loss: 0.1893  lr:0.010000
[ Fri May 17 02:02:19 2024 ] 	Batch(1600/2353) done. Loss: 0.2585  lr:0.010000
[ Fri May 17 02:02:57 2024 ] 	Batch(1700/2353) done. Loss: 0.2984  lr:0.010000
[ Fri May 17 02:03:34 2024 ] 	Batch(1800/2353) done. Loss: 0.0449  lr:0.010000
[ Fri May 17 02:04:11 2024 ] 	Batch(1900/2353) done. Loss: 0.2408  lr:0.010000
[ Fri May 17 02:04:49 2024 ] 	Batch(2000/2353) done. Loss: 0.2845  lr:0.010000
[ Fri May 17 02:05:26 2024 ] 	Batch(2100/2353) done. Loss: 0.5676  lr:0.010000
[ Fri May 17 02:06:04 2024 ] 	Batch(2200/2353) done. Loss: 0.4746  lr:0.010000
[ Fri May 17 02:06:41 2024 ] 	Batch(2300/2353) done. Loss: 0.0940  lr:0.010000
[ Fri May 17 02:07:01 2024 ] 	Mean training loss: 0.1935.
[ Fri May 17 02:07:01 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 02:07:01 2024 ] Training epoch: 48
[ Fri May 17 02:07:02 2024 ] 	Batch(0/2353) done. Loss: 0.0603  lr:0.010000
[ Fri May 17 02:07:39 2024 ] 	Batch(100/2353) done. Loss: 0.1240  lr:0.010000
[ Fri May 17 02:08:17 2024 ] 	Batch(200/2353) done. Loss: 0.2652  lr:0.010000
[ Fri May 17 02:08:54 2024 ] 	Batch(300/2353) done. Loss: 0.0517  lr:0.010000
[ Fri May 17 02:09:32 2024 ] 	Batch(400/2353) done. Loss: 0.1482  lr:0.010000
[ Fri May 17 02:10:09 2024 ] 	Batch(500/2353) done. Loss: 0.2359  lr:0.010000
[ Fri May 17 02:10:46 2024 ] 	Batch(600/2353) done. Loss: 0.2969  lr:0.010000
[ Fri May 17 02:11:24 2024 ] 	Batch(700/2353) done. Loss: 0.3696  lr:0.010000
[ Fri May 17 02:12:01 2024 ] 	Batch(800/2353) done. Loss: 0.1158  lr:0.010000
[ Fri May 17 02:12:38 2024 ] 	Batch(900/2353) done. Loss: 0.1892  lr:0.010000
[ Fri May 17 02:13:16 2024 ] 	Batch(1000/2353) done. Loss: 0.1930  lr:0.010000
[ Fri May 17 02:13:53 2024 ] 	Batch(1100/2353) done. Loss: 0.4770  lr:0.010000
[ Fri May 17 02:14:31 2024 ] 	Batch(1200/2353) done. Loss: 0.4332  lr:0.010000
[ Fri May 17 02:15:08 2024 ] 	Batch(1300/2353) done. Loss: 0.1817  lr:0.010000
[ Fri May 17 02:15:46 2024 ] 	Batch(1400/2353) done. Loss: 0.2992  lr:0.010000
[ Fri May 17 02:16:23 2024 ] 	Batch(1500/2353) done. Loss: 0.2218  lr:0.010000
[ Fri May 17 02:17:00 2024 ] 	Batch(1600/2353) done. Loss: 0.0589  lr:0.010000
[ Fri May 17 02:17:38 2024 ] 	Batch(1700/2353) done. Loss: 0.3934  lr:0.010000
[ Fri May 17 02:18:15 2024 ] 	Batch(1800/2353) done. Loss: 0.2218  lr:0.010000
[ Fri May 17 02:18:52 2024 ] 	Batch(1900/2353) done. Loss: 0.1621  lr:0.010000
[ Fri May 17 02:19:30 2024 ] 	Batch(2000/2353) done. Loss: 0.5747  lr:0.010000
[ Fri May 17 02:20:07 2024 ] 	Batch(2100/2353) done. Loss: 0.2867  lr:0.010000
[ Fri May 17 02:20:45 2024 ] 	Batch(2200/2353) done. Loss: 0.0171  lr:0.010000
[ Fri May 17 02:21:22 2024 ] 	Batch(2300/2353) done. Loss: 0.0385  lr:0.010000
[ Fri May 17 02:21:41 2024 ] 	Mean training loss: 0.1880.
[ Fri May 17 02:21:41 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 02:21:42 2024 ] Training epoch: 49
[ Fri May 17 02:21:42 2024 ] 	Batch(0/2353) done. Loss: 0.0932  lr:0.010000
[ Fri May 17 02:22:20 2024 ] 	Batch(100/2353) done. Loss: 0.2907  lr:0.010000
[ Fri May 17 02:22:57 2024 ] 	Batch(200/2353) done. Loss: 0.0298  lr:0.010000
[ Fri May 17 02:23:34 2024 ] 	Batch(300/2353) done. Loss: 0.2676  lr:0.010000
[ Fri May 17 02:24:12 2024 ] 	Batch(400/2353) done. Loss: 0.0324  lr:0.010000
[ Fri May 17 02:24:49 2024 ] 	Batch(500/2353) done. Loss: 0.3095  lr:0.010000
[ Fri May 17 02:25:26 2024 ] 	Batch(600/2353) done. Loss: 0.0680  lr:0.010000
[ Fri May 17 02:26:04 2024 ] 	Batch(700/2353) done. Loss: 0.1466  lr:0.010000
[ Fri May 17 02:26:41 2024 ] 	Batch(800/2353) done. Loss: 0.0315  lr:0.010000
[ Fri May 17 02:27:19 2024 ] 	Batch(900/2353) done. Loss: 0.3731  lr:0.010000
[ Fri May 17 02:27:57 2024 ] 	Batch(1000/2353) done. Loss: 0.0368  lr:0.010000
[ Fri May 17 02:28:35 2024 ] 	Batch(1100/2353) done. Loss: 0.0832  lr:0.010000
[ Fri May 17 02:29:14 2024 ] 	Batch(1200/2353) done. Loss: 0.2149  lr:0.010000
[ Fri May 17 02:29:52 2024 ] 	Batch(1300/2353) done. Loss: 0.1507  lr:0.010000
[ Fri May 17 02:30:30 2024 ] 	Batch(1400/2353) done. Loss: 0.1525  lr:0.010000
[ Fri May 17 02:31:08 2024 ] 	Batch(1500/2353) done. Loss: 0.0411  lr:0.010000
[ Fri May 17 02:31:46 2024 ] 	Batch(1600/2353) done. Loss: 0.1170  lr:0.010000
[ Fri May 17 02:32:24 2024 ] 	Batch(1700/2353) done. Loss: 0.5669  lr:0.010000
[ Fri May 17 02:33:02 2024 ] 	Batch(1800/2353) done. Loss: 0.2116  lr:0.010000
[ Fri May 17 02:33:40 2024 ] 	Batch(1900/2353) done. Loss: 0.2922  lr:0.010000
[ Fri May 17 02:34:19 2024 ] 	Batch(2000/2353) done. Loss: 0.0111  lr:0.010000
[ Fri May 17 02:34:57 2024 ] 	Batch(2100/2353) done. Loss: 0.2194  lr:0.010000
[ Fri May 17 02:35:34 2024 ] 	Batch(2200/2353) done. Loss: 0.2801  lr:0.010000
[ Fri May 17 02:36:11 2024 ] 	Batch(2300/2353) done. Loss: 0.0646  lr:0.010000
[ Fri May 17 02:36:31 2024 ] 	Mean training loss: 0.1807.
[ Fri May 17 02:36:31 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 02:36:31 2024 ] Training epoch: 50
[ Fri May 17 02:36:32 2024 ] 	Batch(0/2353) done. Loss: 0.0395  lr:0.010000
[ Fri May 17 02:37:09 2024 ] 	Batch(100/2353) done. Loss: 0.0386  lr:0.010000
[ Fri May 17 02:37:47 2024 ] 	Batch(200/2353) done. Loss: 0.0485  lr:0.010000
[ Fri May 17 02:38:24 2024 ] 	Batch(300/2353) done. Loss: 0.1322  lr:0.010000
[ Fri May 17 02:39:01 2024 ] 	Batch(400/2353) done. Loss: 0.3337  lr:0.010000
[ Fri May 17 02:39:39 2024 ] 	Batch(500/2353) done. Loss: 0.0779  lr:0.010000
[ Fri May 17 02:40:16 2024 ] 	Batch(600/2353) done. Loss: 0.0535  lr:0.010000
[ Fri May 17 02:40:53 2024 ] 	Batch(700/2353) done. Loss: 0.2578  lr:0.010000
[ Fri May 17 02:41:31 2024 ] 	Batch(800/2353) done. Loss: 0.0322  lr:0.010000
[ Fri May 17 02:42:08 2024 ] 	Batch(900/2353) done. Loss: 0.0697  lr:0.010000
[ Fri May 17 02:42:46 2024 ] 	Batch(1000/2353) done. Loss: 0.0282  lr:0.010000
[ Fri May 17 02:43:23 2024 ] 	Batch(1100/2353) done. Loss: 0.2208  lr:0.010000
[ Fri May 17 02:44:00 2024 ] 	Batch(1200/2353) done. Loss: 0.0565  lr:0.010000
[ Fri May 17 02:44:39 2024 ] 	Batch(1300/2353) done. Loss: 0.1210  lr:0.010000
[ Fri May 17 02:45:18 2024 ] 	Batch(1400/2353) done. Loss: 0.0849  lr:0.010000
[ Fri May 17 02:45:56 2024 ] 	Batch(1500/2353) done. Loss: 0.1217  lr:0.010000
[ Fri May 17 02:46:34 2024 ] 	Batch(1600/2353) done. Loss: 0.0754  lr:0.010000
[ Fri May 17 02:47:11 2024 ] 	Batch(1700/2353) done. Loss: 0.0880  lr:0.010000
[ Fri May 17 02:47:49 2024 ] 	Batch(1800/2353) done. Loss: 0.1537  lr:0.010000
[ Fri May 17 02:48:26 2024 ] 	Batch(1900/2353) done. Loss: 0.0852  lr:0.010000
[ Fri May 17 02:49:04 2024 ] 	Batch(2000/2353) done. Loss: 0.0428  lr:0.010000
[ Fri May 17 02:49:41 2024 ] 	Batch(2100/2353) done. Loss: 0.2237  lr:0.010000
[ Fri May 17 02:50:19 2024 ] 	Batch(2200/2353) done. Loss: 0.3114  lr:0.010000
[ Fri May 17 02:50:56 2024 ] 	Batch(2300/2353) done. Loss: 0.0995  lr:0.010000
[ Fri May 17 02:51:15 2024 ] 	Mean training loss: 0.1877.
[ Fri May 17 02:51:15 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 02:51:16 2024 ] Eval epoch: 50
[ Fri May 17 02:53:19 2024 ] 	Mean val loss of 2367 batches: 0.3005457570453923.
[ Fri May 17 02:53:19 2024 ] Training epoch: 51
[ Fri May 17 02:53:19 2024 ] 	Batch(0/2353) done. Loss: 0.1281  lr:0.010000
[ Fri May 17 02:53:57 2024 ] 	Batch(100/2353) done. Loss: 0.0624  lr:0.010000
[ Fri May 17 02:54:34 2024 ] 	Batch(200/2353) done. Loss: 0.2277  lr:0.010000
[ Fri May 17 02:55:11 2024 ] 	Batch(300/2353) done. Loss: 0.1615  lr:0.010000
[ Fri May 17 02:55:49 2024 ] 	Batch(400/2353) done. Loss: 0.1297  lr:0.010000
[ Fri May 17 02:56:26 2024 ] 	Batch(500/2353) done. Loss: 0.0604  lr:0.010000
[ Fri May 17 02:57:04 2024 ] 	Batch(600/2353) done. Loss: 0.2955  lr:0.010000
[ Fri May 17 02:57:41 2024 ] 	Batch(700/2353) done. Loss: 0.1303  lr:0.010000
[ Fri May 17 02:58:18 2024 ] 	Batch(800/2353) done. Loss: 0.2034  lr:0.010000
[ Fri May 17 02:58:56 2024 ] 	Batch(900/2353) done. Loss: 0.2164  lr:0.010000
[ Fri May 17 02:59:33 2024 ] 	Batch(1000/2353) done. Loss: 0.0395  lr:0.010000
[ Fri May 17 03:00:10 2024 ] 	Batch(1100/2353) done. Loss: 0.5002  lr:0.010000
[ Fri May 17 03:00:48 2024 ] 	Batch(1200/2353) done. Loss: 0.1363  lr:0.010000
[ Fri May 17 03:01:25 2024 ] 	Batch(1300/2353) done. Loss: 0.2202  lr:0.010000
[ Fri May 17 03:02:02 2024 ] 	Batch(1400/2353) done. Loss: 0.2673  lr:0.010000
[ Fri May 17 03:02:39 2024 ] 	Batch(1500/2353) done. Loss: 0.2539  lr:0.010000
[ Fri May 17 03:03:17 2024 ] 	Batch(1600/2353) done. Loss: 0.0279  lr:0.010000
[ Fri May 17 03:03:54 2024 ] 	Batch(1700/2353) done. Loss: 0.5946  lr:0.010000
[ Fri May 17 03:04:32 2024 ] 	Batch(1800/2353) done. Loss: 0.1667  lr:0.010000
[ Fri May 17 03:05:09 2024 ] 	Batch(1900/2353) done. Loss: 0.0369  lr:0.010000
[ Fri May 17 03:05:46 2024 ] 	Batch(2000/2353) done. Loss: 0.0479  lr:0.010000
[ Fri May 17 03:06:24 2024 ] 	Batch(2100/2353) done. Loss: 0.1824  lr:0.010000
[ Fri May 17 03:07:01 2024 ] 	Batch(2200/2353) done. Loss: 0.0784  lr:0.010000
[ Fri May 17 03:07:39 2024 ] 	Batch(2300/2353) done. Loss: 0.0533  lr:0.010000
[ Fri May 17 03:07:58 2024 ] 	Mean training loss: 0.1779.
[ Fri May 17 03:07:58 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 03:07:58 2024 ] Training epoch: 52
[ Fri May 17 03:07:59 2024 ] 	Batch(0/2353) done. Loss: 0.0814  lr:0.010000
[ Fri May 17 03:08:37 2024 ] 	Batch(100/2353) done. Loss: 0.1382  lr:0.010000
[ Fri May 17 03:09:15 2024 ] 	Batch(200/2353) done. Loss: 0.0765  lr:0.010000
[ Fri May 17 03:09:53 2024 ] 	Batch(300/2353) done. Loss: 0.0532  lr:0.010000
[ Fri May 17 03:10:32 2024 ] 	Batch(400/2353) done. Loss: 0.1065  lr:0.010000
[ Fri May 17 03:11:10 2024 ] 	Batch(500/2353) done. Loss: 0.0099  lr:0.010000
[ Fri May 17 03:11:47 2024 ] 	Batch(600/2353) done. Loss: 0.1083  lr:0.010000
[ Fri May 17 03:12:24 2024 ] 	Batch(700/2353) done. Loss: 0.1905  lr:0.010000
[ Fri May 17 03:13:02 2024 ] 	Batch(800/2353) done. Loss: 0.1410  lr:0.010000
[ Fri May 17 03:13:39 2024 ] 	Batch(900/2353) done. Loss: 0.0903  lr:0.010000
[ Fri May 17 03:14:16 2024 ] 	Batch(1000/2353) done. Loss: 0.1383  lr:0.010000
[ Fri May 17 03:14:54 2024 ] 	Batch(1100/2353) done. Loss: 0.3783  lr:0.010000
[ Fri May 17 03:15:31 2024 ] 	Batch(1200/2353) done. Loss: 0.2524  lr:0.010000
[ Fri May 17 03:16:09 2024 ] 	Batch(1300/2353) done. Loss: 0.0337  lr:0.010000
[ Fri May 17 03:16:46 2024 ] 	Batch(1400/2353) done. Loss: 0.5125  lr:0.010000
[ Fri May 17 03:17:23 2024 ] 	Batch(1500/2353) done. Loss: 0.1530  lr:0.010000
[ Fri May 17 03:18:01 2024 ] 	Batch(1600/2353) done. Loss: 0.0635  lr:0.010000
[ Fri May 17 03:18:38 2024 ] 	Batch(1700/2353) done. Loss: 0.0818  lr:0.010000
[ Fri May 17 03:19:16 2024 ] 	Batch(1800/2353) done. Loss: 0.0793  lr:0.010000
[ Fri May 17 03:19:53 2024 ] 	Batch(1900/2353) done. Loss: 0.0471  lr:0.010000
[ Fri May 17 03:20:31 2024 ] 	Batch(2000/2353) done. Loss: 0.0225  lr:0.010000
[ Fri May 17 03:21:08 2024 ] 	Batch(2100/2353) done. Loss: 0.0965  lr:0.010000
[ Fri May 17 03:21:46 2024 ] 	Batch(2200/2353) done. Loss: 0.1283  lr:0.010000
[ Fri May 17 03:22:23 2024 ] 	Batch(2300/2353) done. Loss: 0.4647  lr:0.010000
[ Fri May 17 03:22:43 2024 ] 	Mean training loss: 0.1633.
[ Fri May 17 03:22:43 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 03:22:43 2024 ] Training epoch: 53
[ Fri May 17 03:22:44 2024 ] 	Batch(0/2353) done. Loss: 0.6451  lr:0.010000
[ Fri May 17 03:23:21 2024 ] 	Batch(100/2353) done. Loss: 0.0100  lr:0.010000
[ Fri May 17 03:23:58 2024 ] 	Batch(200/2353) done. Loss: 0.0469  lr:0.010000
[ Fri May 17 03:24:36 2024 ] 	Batch(300/2353) done. Loss: 0.2559  lr:0.010000
[ Fri May 17 03:25:13 2024 ] 	Batch(400/2353) done. Loss: 0.1155  lr:0.010000
[ Fri May 17 03:25:50 2024 ] 	Batch(500/2353) done. Loss: 0.0955  lr:0.010000
[ Fri May 17 03:26:28 2024 ] 	Batch(600/2353) done. Loss: 0.0400  lr:0.010000
[ Fri May 17 03:27:05 2024 ] 	Batch(700/2353) done. Loss: 0.0706  lr:0.010000
[ Fri May 17 03:27:43 2024 ] 	Batch(800/2353) done. Loss: 0.2673  lr:0.010000
[ Fri May 17 03:28:21 2024 ] 	Batch(900/2353) done. Loss: 0.3712  lr:0.010000
[ Fri May 17 03:28:58 2024 ] 	Batch(1000/2353) done. Loss: 0.0630  lr:0.010000
[ Fri May 17 03:29:35 2024 ] 	Batch(1100/2353) done. Loss: 0.0615  lr:0.010000
[ Fri May 17 03:30:13 2024 ] 	Batch(1200/2353) done. Loss: 0.1885  lr:0.010000
[ Fri May 17 03:30:50 2024 ] 	Batch(1300/2353) done. Loss: 0.3464  lr:0.010000
[ Fri May 17 03:31:28 2024 ] 	Batch(1400/2353) done. Loss: 0.1598  lr:0.010000
[ Fri May 17 03:32:06 2024 ] 	Batch(1500/2353) done. Loss: 0.0999  lr:0.010000
[ Fri May 17 03:32:43 2024 ] 	Batch(1600/2353) done. Loss: 0.1216  lr:0.010000
[ Fri May 17 03:33:21 2024 ] 	Batch(1700/2353) done. Loss: 0.2520  lr:0.010000
[ Fri May 17 03:33:59 2024 ] 	Batch(1800/2353) done. Loss: 0.2249  lr:0.010000
[ Fri May 17 03:34:37 2024 ] 	Batch(1900/2353) done. Loss: 0.2612  lr:0.010000
[ Fri May 17 03:35:15 2024 ] 	Batch(2000/2353) done. Loss: 0.3244  lr:0.010000
[ Fri May 17 03:35:53 2024 ] 	Batch(2100/2353) done. Loss: 0.1082  lr:0.010000
[ Fri May 17 03:36:30 2024 ] 	Batch(2200/2353) done. Loss: 0.2178  lr:0.010000
[ Fri May 17 03:37:08 2024 ] 	Batch(2300/2353) done. Loss: 0.0275  lr:0.010000
[ Fri May 17 03:37:27 2024 ] 	Mean training loss: 0.1706.
[ Fri May 17 03:37:27 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 03:37:27 2024 ] Training epoch: 54
[ Fri May 17 03:37:28 2024 ] 	Batch(0/2353) done. Loss: 0.1911  lr:0.010000
[ Fri May 17 03:38:05 2024 ] 	Batch(100/2353) done. Loss: 0.0305  lr:0.010000
[ Fri May 17 03:38:43 2024 ] 	Batch(200/2353) done. Loss: 0.2033  lr:0.010000
[ Fri May 17 03:39:20 2024 ] 	Batch(300/2353) done. Loss: 0.0949  lr:0.010000
[ Fri May 17 03:39:57 2024 ] 	Batch(400/2353) done. Loss: 0.1406  lr:0.010000
[ Fri May 17 03:40:35 2024 ] 	Batch(500/2353) done. Loss: 0.1703  lr:0.010000
[ Fri May 17 03:41:12 2024 ] 	Batch(600/2353) done. Loss: 0.0884  lr:0.010000
[ Fri May 17 03:41:49 2024 ] 	Batch(700/2353) done. Loss: 0.0332  lr:0.010000
[ Fri May 17 03:42:27 2024 ] 	Batch(800/2353) done. Loss: 0.0910  lr:0.010000
[ Fri May 17 03:43:04 2024 ] 	Batch(900/2353) done. Loss: 0.1754  lr:0.010000
[ Fri May 17 03:43:41 2024 ] 	Batch(1000/2353) done. Loss: 0.0555  lr:0.010000
[ Fri May 17 03:44:19 2024 ] 	Batch(1100/2353) done. Loss: 0.4957  lr:0.010000
[ Fri May 17 03:44:56 2024 ] 	Batch(1200/2353) done. Loss: 0.2518  lr:0.010000
[ Fri May 17 03:45:34 2024 ] 	Batch(1300/2353) done. Loss: 0.0935  lr:0.010000
[ Fri May 17 03:46:11 2024 ] 	Batch(1400/2353) done. Loss: 0.1099  lr:0.010000
[ Fri May 17 03:46:48 2024 ] 	Batch(1500/2353) done. Loss: 0.1782  lr:0.010000
[ Fri May 17 03:47:26 2024 ] 	Batch(1600/2353) done. Loss: 0.2034  lr:0.010000
[ Fri May 17 03:48:04 2024 ] 	Batch(1700/2353) done. Loss: 0.2277  lr:0.010000
[ Fri May 17 03:48:42 2024 ] 	Batch(1800/2353) done. Loss: 0.0723  lr:0.010000
[ Fri May 17 03:49:20 2024 ] 	Batch(1900/2353) done. Loss: 0.3309  lr:0.010000
[ Fri May 17 03:49:58 2024 ] 	Batch(2000/2353) done. Loss: 0.2540  lr:0.010000
[ Fri May 17 03:50:36 2024 ] 	Batch(2100/2353) done. Loss: 0.3200  lr:0.010000
[ Fri May 17 03:51:14 2024 ] 	Batch(2200/2353) done. Loss: 0.2319  lr:0.010000
[ Fri May 17 03:51:53 2024 ] 	Batch(2300/2353) done. Loss: 0.1423  lr:0.010000
[ Fri May 17 03:52:12 2024 ] 	Mean training loss: 0.1608.
[ Fri May 17 03:52:12 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 03:52:13 2024 ] Training epoch: 55
[ Fri May 17 03:52:13 2024 ] 	Batch(0/2353) done. Loss: 0.0777  lr:0.010000
[ Fri May 17 03:52:51 2024 ] 	Batch(100/2353) done. Loss: 0.0266  lr:0.010000
[ Fri May 17 03:53:28 2024 ] 	Batch(200/2353) done. Loss: 0.0073  lr:0.010000
[ Fri May 17 03:54:06 2024 ] 	Batch(300/2353) done. Loss: 0.0349  lr:0.010000
[ Fri May 17 03:54:43 2024 ] 	Batch(400/2353) done. Loss: 0.0207  lr:0.010000
[ Fri May 17 03:55:21 2024 ] 	Batch(500/2353) done. Loss: 0.0935  lr:0.010000
[ Fri May 17 03:55:58 2024 ] 	Batch(600/2353) done. Loss: 0.1410  lr:0.010000
[ Fri May 17 03:56:35 2024 ] 	Batch(700/2353) done. Loss: 0.0767  lr:0.010000
[ Fri May 17 03:57:13 2024 ] 	Batch(800/2353) done. Loss: 0.2620  lr:0.010000
[ Fri May 17 03:57:50 2024 ] 	Batch(900/2353) done. Loss: 0.0437  lr:0.010000
[ Fri May 17 03:58:28 2024 ] 	Batch(1000/2353) done. Loss: 0.3527  lr:0.010000
[ Fri May 17 03:59:06 2024 ] 	Batch(1100/2353) done. Loss: 0.1865  lr:0.010000
[ Fri May 17 03:59:45 2024 ] 	Batch(1200/2353) done. Loss: 0.0320  lr:0.010000
[ Fri May 17 04:00:22 2024 ] 	Batch(1300/2353) done. Loss: 0.0543  lr:0.010000
[ Fri May 17 04:00:59 2024 ] 	Batch(1400/2353) done. Loss: 0.0208  lr:0.010000
[ Fri May 17 04:01:37 2024 ] 	Batch(1500/2353) done. Loss: 0.2038  lr:0.010000
[ Fri May 17 04:02:14 2024 ] 	Batch(1600/2353) done. Loss: 0.1265  lr:0.010000
[ Fri May 17 04:02:52 2024 ] 	Batch(1700/2353) done. Loss: 0.1562  lr:0.010000
[ Fri May 17 04:03:29 2024 ] 	Batch(1800/2353) done. Loss: 0.2192  lr:0.010000
[ Fri May 17 04:04:06 2024 ] 	Batch(1900/2353) done. Loss: 0.0044  lr:0.010000
[ Fri May 17 04:04:44 2024 ] 	Batch(2000/2353) done. Loss: 0.3376  lr:0.010000
[ Fri May 17 04:05:21 2024 ] 	Batch(2100/2353) done. Loss: 0.1018  lr:0.010000
[ Fri May 17 04:05:59 2024 ] 	Batch(2200/2353) done. Loss: 0.2458  lr:0.010000
[ Fri May 17 04:06:36 2024 ] 	Batch(2300/2353) done. Loss: 0.2361  lr:0.010000
[ Fri May 17 04:06:56 2024 ] 	Mean training loss: 0.1586.
[ Fri May 17 04:06:56 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 04:06:56 2024 ] Training epoch: 56
[ Fri May 17 04:06:56 2024 ] 	Batch(0/2353) done. Loss: 0.1014  lr:0.010000
[ Fri May 17 04:07:34 2024 ] 	Batch(100/2353) done. Loss: 0.4348  lr:0.010000
[ Fri May 17 04:08:11 2024 ] 	Batch(200/2353) done. Loss: 0.1974  lr:0.010000
[ Fri May 17 04:08:48 2024 ] 	Batch(300/2353) done. Loss: 0.0443  lr:0.010000
[ Fri May 17 04:09:26 2024 ] 	Batch(400/2353) done. Loss: 0.2052  lr:0.010000
[ Fri May 17 04:10:03 2024 ] 	Batch(500/2353) done. Loss: 0.0750  lr:0.010000
[ Fri May 17 04:10:40 2024 ] 	Batch(600/2353) done. Loss: 0.1498  lr:0.010000
[ Fri May 17 04:11:18 2024 ] 	Batch(700/2353) done. Loss: 0.0873  lr:0.010000
[ Fri May 17 04:11:55 2024 ] 	Batch(800/2353) done. Loss: 0.0655  lr:0.010000
[ Fri May 17 04:12:33 2024 ] 	Batch(900/2353) done. Loss: 0.0649  lr:0.010000
[ Fri May 17 04:13:10 2024 ] 	Batch(1000/2353) done. Loss: 0.1344  lr:0.010000
[ Fri May 17 04:13:47 2024 ] 	Batch(1100/2353) done. Loss: 0.4436  lr:0.010000
[ Fri May 17 04:14:25 2024 ] 	Batch(1200/2353) done. Loss: 0.1869  lr:0.010000
[ Fri May 17 04:15:02 2024 ] 	Batch(1300/2353) done. Loss: 0.1071  lr:0.010000
[ Fri May 17 04:15:39 2024 ] 	Batch(1400/2353) done. Loss: 0.1502  lr:0.010000
[ Fri May 17 04:16:17 2024 ] 	Batch(1500/2353) done. Loss: 0.2607  lr:0.010000
[ Fri May 17 04:16:54 2024 ] 	Batch(1600/2353) done. Loss: 0.2614  lr:0.010000
[ Fri May 17 04:17:31 2024 ] 	Batch(1700/2353) done. Loss: 0.0299  lr:0.010000
[ Fri May 17 04:18:09 2024 ] 	Batch(1800/2353) done. Loss: 0.0700  lr:0.010000
[ Fri May 17 04:18:46 2024 ] 	Batch(1900/2353) done. Loss: 0.1717  lr:0.010000
[ Fri May 17 04:19:23 2024 ] 	Batch(2000/2353) done. Loss: 0.0580  lr:0.010000
[ Fri May 17 04:20:01 2024 ] 	Batch(2100/2353) done. Loss: 0.0765  lr:0.010000
[ Fri May 17 04:20:38 2024 ] 	Batch(2200/2353) done. Loss: 0.1564  lr:0.010000
[ Fri May 17 04:21:15 2024 ] 	Batch(2300/2353) done. Loss: 0.2135  lr:0.010000
[ Fri May 17 04:21:35 2024 ] 	Mean training loss: 0.1540.
[ Fri May 17 04:21:35 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 04:21:35 2024 ] Training epoch: 57
[ Fri May 17 04:21:36 2024 ] 	Batch(0/2353) done. Loss: 0.2244  lr:0.010000
[ Fri May 17 04:22:13 2024 ] 	Batch(100/2353) done. Loss: 0.3275  lr:0.010000
[ Fri May 17 04:22:50 2024 ] 	Batch(200/2353) done. Loss: 0.1313  lr:0.010000
[ Fri May 17 04:23:28 2024 ] 	Batch(300/2353) done. Loss: 0.0469  lr:0.010000
[ Fri May 17 04:24:05 2024 ] 	Batch(400/2353) done. Loss: 0.1462  lr:0.010000
[ Fri May 17 04:24:43 2024 ] 	Batch(500/2353) done. Loss: 0.2553  lr:0.010000
[ Fri May 17 04:25:20 2024 ] 	Batch(600/2353) done. Loss: 0.4350  lr:0.010000
[ Fri May 17 04:25:57 2024 ] 	Batch(700/2353) done. Loss: 0.1259  lr:0.010000
[ Fri May 17 04:26:35 2024 ] 	Batch(800/2353) done. Loss: 0.0835  lr:0.010000
[ Fri May 17 04:27:12 2024 ] 	Batch(900/2353) done. Loss: 0.1027  lr:0.010000
[ Fri May 17 04:27:50 2024 ] 	Batch(1000/2353) done. Loss: 0.0983  lr:0.010000
[ Fri May 17 04:28:27 2024 ] 	Batch(1100/2353) done. Loss: 0.3873  lr:0.010000
[ Fri May 17 04:29:04 2024 ] 	Batch(1200/2353) done. Loss: 0.1051  lr:0.010000
[ Fri May 17 04:29:42 2024 ] 	Batch(1300/2353) done. Loss: 0.0856  lr:0.010000
[ Fri May 17 04:30:19 2024 ] 	Batch(1400/2353) done. Loss: 0.0918  lr:0.010000
[ Fri May 17 04:30:56 2024 ] 	Batch(1500/2353) done. Loss: 0.1032  lr:0.010000
[ Fri May 17 04:31:34 2024 ] 	Batch(1600/2353) done. Loss: 0.0073  lr:0.010000
[ Fri May 17 04:32:11 2024 ] 	Batch(1700/2353) done. Loss: 0.1249  lr:0.010000
[ Fri May 17 04:32:48 2024 ] 	Batch(1800/2353) done. Loss: 0.0438  lr:0.010000
[ Fri May 17 04:33:26 2024 ] 	Batch(1900/2353) done. Loss: 0.2587  lr:0.010000
[ Fri May 17 04:34:03 2024 ] 	Batch(2000/2353) done. Loss: 0.1014  lr:0.010000
[ Fri May 17 04:34:40 2024 ] 	Batch(2100/2353) done. Loss: 0.3194  lr:0.010000
[ Fri May 17 04:35:18 2024 ] 	Batch(2200/2353) done. Loss: 0.2251  lr:0.010000
[ Fri May 17 04:35:55 2024 ] 	Batch(2300/2353) done. Loss: 0.2205  lr:0.010000
[ Fri May 17 04:36:15 2024 ] 	Mean training loss: 0.1577.
[ Fri May 17 04:36:15 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 04:36:15 2024 ] Training epoch: 58
[ Fri May 17 04:36:16 2024 ] 	Batch(0/2353) done. Loss: 0.0226  lr:0.010000
[ Fri May 17 04:36:53 2024 ] 	Batch(100/2353) done. Loss: 0.2507  lr:0.010000
[ Fri May 17 04:37:30 2024 ] 	Batch(200/2353) done. Loss: 0.0532  lr:0.010000
[ Fri May 17 04:38:07 2024 ] 	Batch(300/2353) done. Loss: 0.0669  lr:0.010000
[ Fri May 17 04:38:45 2024 ] 	Batch(400/2353) done. Loss: 0.0740  lr:0.010000
[ Fri May 17 04:39:22 2024 ] 	Batch(500/2353) done. Loss: 0.0388  lr:0.010000
[ Fri May 17 04:40:00 2024 ] 	Batch(600/2353) done. Loss: 0.1239  lr:0.010000
[ Fri May 17 04:40:37 2024 ] 	Batch(700/2353) done. Loss: 0.0665  lr:0.010000
[ Fri May 17 04:41:14 2024 ] 	Batch(800/2353) done. Loss: 0.1055  lr:0.010000
[ Fri May 17 04:41:52 2024 ] 	Batch(900/2353) done. Loss: 0.4957  lr:0.010000
[ Fri May 17 04:42:31 2024 ] 	Batch(1000/2353) done. Loss: 0.4254  lr:0.010000
[ Fri May 17 04:43:09 2024 ] 	Batch(1100/2353) done. Loss: 0.0071  lr:0.010000
[ Fri May 17 04:43:47 2024 ] 	Batch(1200/2353) done. Loss: 0.0243  lr:0.010000
[ Fri May 17 04:44:25 2024 ] 	Batch(1300/2353) done. Loss: 0.0442  lr:0.010000
[ Fri May 17 04:45:03 2024 ] 	Batch(1400/2353) done. Loss: 0.0800  lr:0.010000
[ Fri May 17 04:45:41 2024 ] 	Batch(1500/2353) done. Loss: 0.0318  lr:0.010000
[ Fri May 17 04:46:19 2024 ] 	Batch(1600/2353) done. Loss: 0.1359  lr:0.010000
[ Fri May 17 04:46:57 2024 ] 	Batch(1700/2353) done. Loss: 0.1871  lr:0.010000
[ Fri May 17 04:47:36 2024 ] 	Batch(1800/2353) done. Loss: 0.0319  lr:0.010000
[ Fri May 17 04:48:14 2024 ] 	Batch(1900/2353) done. Loss: 0.1706  lr:0.010000
[ Fri May 17 04:48:52 2024 ] 	Batch(2000/2353) done. Loss: 0.6211  lr:0.010000
[ Fri May 17 04:49:31 2024 ] 	Batch(2100/2353) done. Loss: 0.2198  lr:0.010000
[ Fri May 17 04:50:09 2024 ] 	Batch(2200/2353) done. Loss: 0.0582  lr:0.010000
[ Fri May 17 04:50:47 2024 ] 	Batch(2300/2353) done. Loss: 0.0952  lr:0.010000
[ Fri May 17 04:51:06 2024 ] 	Mean training loss: 0.1480.
[ Fri May 17 04:51:06 2024 ] 	Time consumption: [Data]01%, [Network]92%
[ Fri May 17 04:51:06 2024 ] Training epoch: 59
[ Fri May 17 04:51:07 2024 ] 	Batch(0/2353) done. Loss: 0.0700  lr:0.010000
[ Fri May 17 04:51:44 2024 ] 	Batch(100/2353) done. Loss: 0.0123  lr:0.010000
[ Fri May 17 04:52:22 2024 ] 	Batch(200/2353) done. Loss: 0.0169  lr:0.010000
[ Fri May 17 04:52:59 2024 ] 	Batch(300/2353) done. Loss: 0.0599  lr:0.010000
[ Fri May 17 04:53:36 2024 ] 	Batch(400/2353) done. Loss: 0.1538  lr:0.010000
[ Fri May 17 04:54:14 2024 ] 	Batch(500/2353) done. Loss: 0.1681  lr:0.010000
[ Fri May 17 04:54:51 2024 ] 	Batch(600/2353) done. Loss: 0.0193  lr:0.010000
[ Fri May 17 04:55:28 2024 ] 	Batch(700/2353) done. Loss: 0.0762  lr:0.010000
[ Fri May 17 04:56:06 2024 ] 	Batch(800/2353) done. Loss: 0.1273  lr:0.010000
[ Fri May 17 04:56:43 2024 ] 	Batch(900/2353) done. Loss: 0.0730  lr:0.010000
[ Fri May 17 04:57:21 2024 ] 	Batch(1000/2353) done. Loss: 0.1062  lr:0.010000
[ Fri May 17 04:57:58 2024 ] 	Batch(1100/2353) done. Loss: 0.0104  lr:0.010000
[ Fri May 17 04:58:36 2024 ] 	Batch(1200/2353) done. Loss: 0.0267  lr:0.010000
[ Fri May 17 04:59:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0276  lr:0.010000
[ Fri May 17 04:59:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0114  lr:0.010000
[ Fri May 17 05:00:28 2024 ] 	Batch(1500/2353) done. Loss: 0.1617  lr:0.010000
[ Fri May 17 05:01:05 2024 ] 	Batch(1600/2353) done. Loss: 0.1291  lr:0.010000
[ Fri May 17 05:01:42 2024 ] 	Batch(1700/2353) done. Loss: 0.0687  lr:0.010000
[ Fri May 17 05:02:20 2024 ] 	Batch(1800/2353) done. Loss: 0.0873  lr:0.010000
[ Fri May 17 05:02:57 2024 ] 	Batch(1900/2353) done. Loss: 0.0651  lr:0.010000
[ Fri May 17 05:03:35 2024 ] 	Batch(2000/2353) done. Loss: 0.0783  lr:0.010000
[ Fri May 17 05:04:12 2024 ] 	Batch(2100/2353) done. Loss: 0.3650  lr:0.010000
[ Fri May 17 05:04:49 2024 ] 	Batch(2200/2353) done. Loss: 0.0608  lr:0.010000
[ Fri May 17 05:05:27 2024 ] 	Batch(2300/2353) done. Loss: 0.0760  lr:0.010000
[ Fri May 17 05:05:46 2024 ] 	Mean training loss: 0.1439.
[ Fri May 17 05:05:46 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 05:05:46 2024 ] Training epoch: 60
[ Fri May 17 05:05:47 2024 ] 	Batch(0/2353) done. Loss: 0.1228  lr:0.010000
[ Fri May 17 05:06:24 2024 ] 	Batch(100/2353) done. Loss: 0.0303  lr:0.010000
[ Fri May 17 05:07:02 2024 ] 	Batch(200/2353) done. Loss: 0.0345  lr:0.010000
[ Fri May 17 05:07:40 2024 ] 	Batch(300/2353) done. Loss: 0.0428  lr:0.010000
[ Fri May 17 05:08:17 2024 ] 	Batch(400/2353) done. Loss: 0.2433  lr:0.010000
[ Fri May 17 05:08:54 2024 ] 	Batch(500/2353) done. Loss: 0.1795  lr:0.010000
[ Fri May 17 05:09:32 2024 ] 	Batch(600/2353) done. Loss: 0.0300  lr:0.010000
[ Fri May 17 05:10:09 2024 ] 	Batch(700/2353) done. Loss: 0.0532  lr:0.010000
[ Fri May 17 05:10:47 2024 ] 	Batch(800/2353) done. Loss: 0.0580  lr:0.010000
[ Fri May 17 05:11:24 2024 ] 	Batch(900/2353) done. Loss: 0.0339  lr:0.010000
[ Fri May 17 05:12:01 2024 ] 	Batch(1000/2353) done. Loss: 0.0987  lr:0.010000
[ Fri May 17 05:12:39 2024 ] 	Batch(1100/2353) done. Loss: 0.0131  lr:0.010000
[ Fri May 17 05:13:16 2024 ] 	Batch(1200/2353) done. Loss: 0.2198  lr:0.010000
[ Fri May 17 05:13:54 2024 ] 	Batch(1300/2353) done. Loss: 0.1752  lr:0.010000
[ Fri May 17 05:14:31 2024 ] 	Batch(1400/2353) done. Loss: 0.6361  lr:0.010000
[ Fri May 17 05:15:08 2024 ] 	Batch(1500/2353) done. Loss: 0.1619  lr:0.010000
[ Fri May 17 05:15:46 2024 ] 	Batch(1600/2353) done. Loss: 0.0128  lr:0.010000
[ Fri May 17 05:16:23 2024 ] 	Batch(1700/2353) done. Loss: 0.0555  lr:0.010000
[ Fri May 17 05:17:01 2024 ] 	Batch(1800/2353) done. Loss: 0.0519  lr:0.010000
[ Fri May 17 05:17:38 2024 ] 	Batch(1900/2353) done. Loss: 0.1920  lr:0.010000
[ Fri May 17 05:18:16 2024 ] 	Batch(2000/2353) done. Loss: 0.2636  lr:0.010000
[ Fri May 17 05:18:53 2024 ] 	Batch(2100/2353) done. Loss: 0.0742  lr:0.010000
[ Fri May 17 05:19:30 2024 ] 	Batch(2200/2353) done. Loss: 0.1826  lr:0.010000
[ Fri May 17 05:20:08 2024 ] 	Batch(2300/2353) done. Loss: 0.0975  lr:0.010000
[ Fri May 17 05:20:27 2024 ] 	Mean training loss: 0.1432.
[ Fri May 17 05:20:27 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 05:20:27 2024 ] Eval epoch: 60
[ Fri May 17 05:22:30 2024 ] 	Mean val loss of 2367 batches: 0.3208012929731479.
[ Fri May 17 05:22:30 2024 ] Training epoch: 61
[ Fri May 17 05:22:31 2024 ] 	Batch(0/2353) done. Loss: 0.0170  lr:0.000100
[ Fri May 17 05:23:08 2024 ] 	Batch(100/2353) done. Loss: 0.0809  lr:0.000100
[ Fri May 17 05:23:45 2024 ] 	Batch(200/2353) done. Loss: 0.0363  lr:0.000100
[ Fri May 17 05:24:23 2024 ] 	Batch(300/2353) done. Loss: 0.1896  lr:0.000100
[ Fri May 17 05:25:00 2024 ] 	Batch(400/2353) done. Loss: 0.0321  lr:0.000100
[ Fri May 17 05:25:38 2024 ] 	Batch(500/2353) done. Loss: 0.0765  lr:0.000100
[ Fri May 17 05:26:15 2024 ] 	Batch(600/2353) done. Loss: 0.0579  lr:0.000100
[ Fri May 17 05:26:53 2024 ] 	Batch(700/2353) done. Loss: 0.1386  lr:0.000100
[ Fri May 17 05:27:30 2024 ] 	Batch(800/2353) done. Loss: 0.0175  lr:0.000100
[ Fri May 17 05:28:07 2024 ] 	Batch(900/2353) done. Loss: 0.0873  lr:0.000100
[ Fri May 17 05:28:45 2024 ] 	Batch(1000/2353) done. Loss: 0.0484  lr:0.000100
[ Fri May 17 05:29:23 2024 ] 	Batch(1100/2353) done. Loss: 0.1604  lr:0.000100
[ Fri May 17 05:30:00 2024 ] 	Batch(1200/2353) done. Loss: 0.0358  lr:0.000100
[ Fri May 17 05:30:38 2024 ] 	Batch(1300/2353) done. Loss: 0.0970  lr:0.000100
[ Fri May 17 05:31:15 2024 ] 	Batch(1400/2353) done. Loss: 0.1490  lr:0.000100
[ Fri May 17 05:31:52 2024 ] 	Batch(1500/2353) done. Loss: 0.0588  lr:0.000100
[ Fri May 17 05:32:30 2024 ] 	Batch(1600/2353) done. Loss: 0.0328  lr:0.000100
[ Fri May 17 05:33:07 2024 ] 	Batch(1700/2353) done. Loss: 0.0228  lr:0.000100
[ Fri May 17 05:33:45 2024 ] 	Batch(1800/2353) done. Loss: 0.0065  lr:0.000100
[ Fri May 17 05:34:22 2024 ] 	Batch(1900/2353) done. Loss: 0.2388  lr:0.000100
[ Fri May 17 05:34:59 2024 ] 	Batch(2000/2353) done. Loss: 0.1469  lr:0.000100
[ Fri May 17 05:35:37 2024 ] 	Batch(2100/2353) done. Loss: 0.0257  lr:0.000100
[ Fri May 17 05:36:14 2024 ] 	Batch(2200/2353) done. Loss: 0.0700  lr:0.000100
[ Fri May 17 05:36:51 2024 ] 	Batch(2300/2353) done. Loss: 0.0442  lr:0.000100
[ Fri May 17 05:37:11 2024 ] 	Mean training loss: 0.0945.
[ Fri May 17 05:37:11 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 05:37:11 2024 ] Training epoch: 62
[ Fri May 17 05:37:12 2024 ] 	Batch(0/2353) done. Loss: 0.0315  lr:0.000100
[ Fri May 17 05:37:49 2024 ] 	Batch(100/2353) done. Loss: 0.1535  lr:0.000100
[ Fri May 17 05:38:26 2024 ] 	Batch(200/2353) done. Loss: 0.0103  lr:0.000100
[ Fri May 17 05:39:04 2024 ] 	Batch(300/2353) done. Loss: 0.0245  lr:0.000100
[ Fri May 17 05:39:41 2024 ] 	Batch(400/2353) done. Loss: 0.0599  lr:0.000100
[ Fri May 17 05:40:19 2024 ] 	Batch(500/2353) done. Loss: 0.0099  lr:0.000100
[ Fri May 17 05:40:56 2024 ] 	Batch(600/2353) done. Loss: 0.0822  lr:0.000100
[ Fri May 17 05:41:34 2024 ] 	Batch(700/2353) done. Loss: 0.0643  lr:0.000100
[ Fri May 17 05:42:12 2024 ] 	Batch(800/2353) done. Loss: 0.0249  lr:0.000100
[ Fri May 17 05:42:49 2024 ] 	Batch(900/2353) done. Loss: 0.0384  lr:0.000100
[ Fri May 17 05:43:27 2024 ] 	Batch(1000/2353) done. Loss: 0.0629  lr:0.000100
[ Fri May 17 05:44:04 2024 ] 	Batch(1100/2353) done. Loss: 0.0301  lr:0.000100
[ Fri May 17 05:44:42 2024 ] 	Batch(1200/2353) done. Loss: 0.0197  lr:0.000100
[ Fri May 17 05:45:21 2024 ] 	Batch(1300/2353) done. Loss: 0.0906  lr:0.000100
[ Fri May 17 05:45:59 2024 ] 	Batch(1400/2353) done. Loss: 0.1313  lr:0.000100
[ Fri May 17 05:46:37 2024 ] 	Batch(1500/2353) done. Loss: 0.0874  lr:0.000100
[ Fri May 17 05:47:15 2024 ] 	Batch(1600/2353) done. Loss: 0.2098  lr:0.000100
[ Fri May 17 05:47:52 2024 ] 	Batch(1700/2353) done. Loss: 0.0341  lr:0.000100
[ Fri May 17 05:48:30 2024 ] 	Batch(1800/2353) done. Loss: 0.0110  lr:0.000100
[ Fri May 17 05:49:08 2024 ] 	Batch(1900/2353) done. Loss: 0.0061  lr:0.000100
[ Fri May 17 05:49:46 2024 ] 	Batch(2000/2353) done. Loss: 0.0414  lr:0.000100
[ Fri May 17 05:50:23 2024 ] 	Batch(2100/2353) done. Loss: 0.0194  lr:0.000100
[ Fri May 17 05:51:01 2024 ] 	Batch(2200/2353) done. Loss: 0.0538  lr:0.000100
[ Fri May 17 05:51:39 2024 ] 	Batch(2300/2353) done. Loss: 0.0380  lr:0.000100
[ Fri May 17 05:51:58 2024 ] 	Mean training loss: 0.0747.
[ Fri May 17 05:51:58 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 05:51:58 2024 ] Training epoch: 63
[ Fri May 17 05:51:59 2024 ] 	Batch(0/2353) done. Loss: 0.0244  lr:0.000100
[ Fri May 17 05:52:37 2024 ] 	Batch(100/2353) done. Loss: 0.0888  lr:0.000100
[ Fri May 17 05:53:14 2024 ] 	Batch(200/2353) done. Loss: 0.0228  lr:0.000100
[ Fri May 17 05:53:52 2024 ] 	Batch(300/2353) done. Loss: 0.0845  lr:0.000100
[ Fri May 17 05:54:30 2024 ] 	Batch(400/2353) done. Loss: 0.3103  lr:0.000100
[ Fri May 17 05:55:07 2024 ] 	Batch(500/2353) done. Loss: 0.2623  lr:0.000100
[ Fri May 17 05:55:45 2024 ] 	Batch(600/2353) done. Loss: 0.0183  lr:0.000100
[ Fri May 17 05:56:23 2024 ] 	Batch(700/2353) done. Loss: 0.0470  lr:0.000100
[ Fri May 17 05:57:00 2024 ] 	Batch(800/2353) done. Loss: 0.0023  lr:0.000100
[ Fri May 17 05:57:38 2024 ] 	Batch(900/2353) done. Loss: 0.0744  lr:0.000100
[ Fri May 17 05:58:16 2024 ] 	Batch(1000/2353) done. Loss: 0.0613  lr:0.000100
[ Fri May 17 05:58:53 2024 ] 	Batch(1100/2353) done. Loss: 0.1819  lr:0.000100
[ Fri May 17 05:59:31 2024 ] 	Batch(1200/2353) done. Loss: 0.0147  lr:0.000100
[ Fri May 17 06:00:09 2024 ] 	Batch(1300/2353) done. Loss: 0.0650  lr:0.000100
[ Fri May 17 06:00:46 2024 ] 	Batch(1400/2353) done. Loss: 0.1576  lr:0.000100
[ Fri May 17 06:01:24 2024 ] 	Batch(1500/2353) done. Loss: 0.0789  lr:0.000100
[ Fri May 17 06:02:02 2024 ] 	Batch(1600/2353) done. Loss: 0.0318  lr:0.000100
[ Fri May 17 06:02:40 2024 ] 	Batch(1700/2353) done. Loss: 0.0697  lr:0.000100
[ Fri May 17 06:03:17 2024 ] 	Batch(1800/2353) done. Loss: 0.0958  lr:0.000100
[ Fri May 17 06:03:55 2024 ] 	Batch(1900/2353) done. Loss: 0.0712  lr:0.000100
[ Fri May 17 06:04:33 2024 ] 	Batch(2000/2353) done. Loss: 0.0987  lr:0.000100
[ Fri May 17 06:05:11 2024 ] 	Batch(2100/2353) done. Loss: 0.1826  lr:0.000100
[ Fri May 17 06:05:48 2024 ] 	Batch(2200/2353) done. Loss: 0.0774  lr:0.000100
[ Fri May 17 06:06:26 2024 ] 	Batch(2300/2353) done. Loss: 0.0236  lr:0.000100
[ Fri May 17 06:06:46 2024 ] 	Mean training loss: 0.0693.
[ Fri May 17 06:06:46 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 06:06:46 2024 ] Training epoch: 64
[ Fri May 17 06:06:47 2024 ] 	Batch(0/2353) done. Loss: 0.0791  lr:0.000100
[ Fri May 17 06:07:24 2024 ] 	Batch(100/2353) done. Loss: 0.0080  lr:0.000100
[ Fri May 17 06:08:02 2024 ] 	Batch(200/2353) done. Loss: 0.0224  lr:0.000100
[ Fri May 17 06:08:40 2024 ] 	Batch(300/2353) done. Loss: 0.1546  lr:0.000100
[ Fri May 17 06:09:17 2024 ] 	Batch(400/2353) done. Loss: 0.0507  lr:0.000100
[ Fri May 17 06:09:55 2024 ] 	Batch(500/2353) done. Loss: 0.0477  lr:0.000100
[ Fri May 17 06:10:33 2024 ] 	Batch(600/2353) done. Loss: 0.0392  lr:0.000100
[ Fri May 17 06:11:10 2024 ] 	Batch(700/2353) done. Loss: 0.0782  lr:0.000100
[ Fri May 17 06:11:48 2024 ] 	Batch(800/2353) done. Loss: 0.0049  lr:0.000100
[ Fri May 17 06:12:26 2024 ] 	Batch(900/2353) done. Loss: 0.0595  lr:0.000100
[ Fri May 17 06:13:03 2024 ] 	Batch(1000/2353) done. Loss: 0.0299  lr:0.000100
[ Fri May 17 06:13:41 2024 ] 	Batch(1100/2353) done. Loss: 0.1870  lr:0.000100
[ Fri May 17 06:14:19 2024 ] 	Batch(1200/2353) done. Loss: 0.0229  lr:0.000100
[ Fri May 17 06:14:57 2024 ] 	Batch(1300/2353) done. Loss: 0.0204  lr:0.000100
[ Fri May 17 06:15:36 2024 ] 	Batch(1400/2353) done. Loss: 0.2250  lr:0.000100
[ Fri May 17 06:16:14 2024 ] 	Batch(1500/2353) done. Loss: 0.0251  lr:0.000100
[ Fri May 17 06:16:52 2024 ] 	Batch(1600/2353) done. Loss: 0.0103  lr:0.000100
[ Fri May 17 06:17:30 2024 ] 	Batch(1700/2353) done. Loss: 0.0115  lr:0.000100
[ Fri May 17 06:18:08 2024 ] 	Batch(1800/2353) done. Loss: 0.0456  lr:0.000100
[ Fri May 17 06:18:45 2024 ] 	Batch(1900/2353) done. Loss: 0.0202  lr:0.000100
[ Fri May 17 06:19:23 2024 ] 	Batch(2000/2353) done. Loss: 0.0614  lr:0.000100
[ Fri May 17 06:20:01 2024 ] 	Batch(2100/2353) done. Loss: 0.0973  lr:0.000100
[ Fri May 17 06:20:39 2024 ] 	Batch(2200/2353) done. Loss: 0.2089  lr:0.000100
[ Fri May 17 06:21:17 2024 ] 	Batch(2300/2353) done. Loss: 0.0217  lr:0.000100
[ Fri May 17 06:21:36 2024 ] 	Mean training loss: 0.0628.
[ Fri May 17 06:21:36 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 06:21:36 2024 ] Training epoch: 65
[ Fri May 17 06:21:37 2024 ] 	Batch(0/2353) done. Loss: 0.2061  lr:0.000100
[ Fri May 17 06:22:15 2024 ] 	Batch(100/2353) done. Loss: 0.0053  lr:0.000100
[ Fri May 17 06:22:54 2024 ] 	Batch(200/2353) done. Loss: 0.0755  lr:0.000100
[ Fri May 17 06:23:32 2024 ] 	Batch(300/2353) done. Loss: 0.0325  lr:0.000100
[ Fri May 17 06:24:11 2024 ] 	Batch(400/2353) done. Loss: 0.0228  lr:0.000100
[ Fri May 17 06:24:48 2024 ] 	Batch(500/2353) done. Loss: 0.0021  lr:0.000100
[ Fri May 17 06:25:26 2024 ] 	Batch(600/2353) done. Loss: 0.1258  lr:0.000100
[ Fri May 17 06:26:04 2024 ] 	Batch(700/2353) done. Loss: 0.0594  lr:0.000100
[ Fri May 17 06:26:42 2024 ] 	Batch(800/2353) done. Loss: 0.0587  lr:0.000100
[ Fri May 17 06:27:19 2024 ] 	Batch(900/2353) done. Loss: 0.0868  lr:0.000100
[ Fri May 17 06:27:57 2024 ] 	Batch(1000/2353) done. Loss: 0.1088  lr:0.000100
[ Fri May 17 06:28:35 2024 ] 	Batch(1100/2353) done. Loss: 0.0086  lr:0.000100
[ Fri May 17 06:29:12 2024 ] 	Batch(1200/2353) done. Loss: 0.0940  lr:0.000100
[ Fri May 17 06:29:50 2024 ] 	Batch(1300/2353) done. Loss: 0.0128  lr:0.000100
[ Fri May 17 06:30:27 2024 ] 	Batch(1400/2353) done. Loss: 0.0467  lr:0.000100
[ Fri May 17 06:31:04 2024 ] 	Batch(1500/2353) done. Loss: 0.1430  lr:0.000100
[ Fri May 17 06:31:42 2024 ] 	Batch(1600/2353) done. Loss: 0.0187  lr:0.000100
[ Fri May 17 06:32:19 2024 ] 	Batch(1700/2353) done. Loss: 0.0143  lr:0.000100
[ Fri May 17 06:32:57 2024 ] 	Batch(1800/2353) done. Loss: 0.0696  lr:0.000100
[ Fri May 17 06:33:34 2024 ] 	Batch(1900/2353) done. Loss: 0.0162  lr:0.000100
[ Fri May 17 06:34:12 2024 ] 	Batch(2000/2353) done. Loss: 0.0226  lr:0.000100
[ Fri May 17 06:34:49 2024 ] 	Batch(2100/2353) done. Loss: 0.1569  lr:0.000100
[ Fri May 17 06:35:27 2024 ] 	Batch(2200/2353) done. Loss: 0.0568  lr:0.000100
[ Fri May 17 06:36:04 2024 ] 	Batch(2300/2353) done. Loss: 0.1535  lr:0.000100
[ Fri May 17 06:36:24 2024 ] 	Mean training loss: 0.0612.
[ Fri May 17 06:36:24 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 06:36:24 2024 ] Training epoch: 66
[ Fri May 17 06:36:24 2024 ] 	Batch(0/2353) done. Loss: 0.0547  lr:0.000100
[ Fri May 17 06:37:02 2024 ] 	Batch(100/2353) done. Loss: 0.0201  lr:0.000100
[ Fri May 17 06:37:39 2024 ] 	Batch(200/2353) done. Loss: 0.0408  lr:0.000100
[ Fri May 17 06:38:16 2024 ] 	Batch(300/2353) done. Loss: 0.1317  lr:0.000100
[ Fri May 17 06:38:54 2024 ] 	Batch(400/2353) done. Loss: 0.0046  lr:0.000100
[ Fri May 17 06:39:31 2024 ] 	Batch(500/2353) done. Loss: 0.0048  lr:0.000100
[ Fri May 17 06:40:08 2024 ] 	Batch(600/2353) done. Loss: 0.0222  lr:0.000100
[ Fri May 17 06:40:46 2024 ] 	Batch(700/2353) done. Loss: 0.0037  lr:0.000100
[ Fri May 17 06:41:23 2024 ] 	Batch(800/2353) done. Loss: 0.1401  lr:0.000100
[ Fri May 17 06:42:01 2024 ] 	Batch(900/2353) done. Loss: 0.0437  lr:0.000100
[ Fri May 17 06:42:38 2024 ] 	Batch(1000/2353) done. Loss: 0.1655  lr:0.000100
[ Fri May 17 06:43:15 2024 ] 	Batch(1100/2353) done. Loss: 0.0400  lr:0.000100
[ Fri May 17 06:43:53 2024 ] 	Batch(1200/2353) done. Loss: 0.0235  lr:0.000100
[ Fri May 17 06:44:30 2024 ] 	Batch(1300/2353) done. Loss: 0.2037  lr:0.000100
[ Fri May 17 06:45:07 2024 ] 	Batch(1400/2353) done. Loss: 0.0371  lr:0.000100
[ Fri May 17 06:45:45 2024 ] 	Batch(1500/2353) done. Loss: 0.0512  lr:0.000100
[ Fri May 17 06:46:22 2024 ] 	Batch(1600/2353) done. Loss: 0.0394  lr:0.000100
[ Fri May 17 06:47:00 2024 ] 	Batch(1700/2353) done. Loss: 0.0241  lr:0.000100
[ Fri May 17 06:47:37 2024 ] 	Batch(1800/2353) done. Loss: 0.0346  lr:0.000100
[ Fri May 17 06:48:14 2024 ] 	Batch(1900/2353) done. Loss: 0.0466  lr:0.000100
[ Fri May 17 06:48:52 2024 ] 	Batch(2000/2353) done. Loss: 0.0675  lr:0.000100
[ Fri May 17 06:49:29 2024 ] 	Batch(2100/2353) done. Loss: 0.0448  lr:0.000100
[ Fri May 17 06:50:06 2024 ] 	Batch(2200/2353) done. Loss: 0.0453  lr:0.000100
[ Fri May 17 06:50:44 2024 ] 	Batch(2300/2353) done. Loss: 0.0336  lr:0.000100
[ Fri May 17 06:51:03 2024 ] 	Mean training loss: 0.0567.
[ Fri May 17 06:51:03 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 06:51:04 2024 ] Training epoch: 67
[ Fri May 17 06:51:04 2024 ] 	Batch(0/2353) done. Loss: 0.0478  lr:0.000100
[ Fri May 17 06:51:42 2024 ] 	Batch(100/2353) done. Loss: 0.0438  lr:0.000100
[ Fri May 17 06:52:19 2024 ] 	Batch(200/2353) done. Loss: 0.0186  lr:0.000100
[ Fri May 17 06:52:56 2024 ] 	Batch(300/2353) done. Loss: 0.0210  lr:0.000100
[ Fri May 17 06:53:34 2024 ] 	Batch(400/2353) done. Loss: 0.0159  lr:0.000100
[ Fri May 17 06:54:11 2024 ] 	Batch(500/2353) done. Loss: 0.0310  lr:0.000100
[ Fri May 17 06:54:48 2024 ] 	Batch(600/2353) done. Loss: 0.0897  lr:0.000100
[ Fri May 17 06:55:26 2024 ] 	Batch(700/2353) done. Loss: 0.0824  lr:0.000100
[ Fri May 17 06:56:03 2024 ] 	Batch(800/2353) done. Loss: 0.0496  lr:0.000100
[ Fri May 17 06:56:41 2024 ] 	Batch(900/2353) done. Loss: 0.0045  lr:0.000100
[ Fri May 17 06:57:18 2024 ] 	Batch(1000/2353) done. Loss: 0.0883  lr:0.000100
[ Fri May 17 06:57:56 2024 ] 	Batch(1100/2353) done. Loss: 0.0469  lr:0.000100
[ Fri May 17 06:58:33 2024 ] 	Batch(1200/2353) done. Loss: 0.0292  lr:0.000100
[ Fri May 17 06:59:11 2024 ] 	Batch(1300/2353) done. Loss: 0.0116  lr:0.000100
[ Fri May 17 06:59:48 2024 ] 	Batch(1400/2353) done. Loss: 0.0269  lr:0.000100
[ Fri May 17 07:00:25 2024 ] 	Batch(1500/2353) done. Loss: 0.0143  lr:0.000100
[ Fri May 17 07:01:03 2024 ] 	Batch(1600/2353) done. Loss: 0.0648  lr:0.000100
[ Fri May 17 07:01:41 2024 ] 	Batch(1700/2353) done. Loss: 0.0142  lr:0.000100
[ Fri May 17 07:02:18 2024 ] 	Batch(1800/2353) done. Loss: 0.1023  lr:0.000100
[ Fri May 17 07:02:56 2024 ] 	Batch(1900/2353) done. Loss: 0.0920  lr:0.000100
[ Fri May 17 07:03:33 2024 ] 	Batch(2000/2353) done. Loss: 0.0438  lr:0.000100
[ Fri May 17 07:04:10 2024 ] 	Batch(2100/2353) done. Loss: 0.0277  lr:0.000100
[ Fri May 17 07:04:48 2024 ] 	Batch(2200/2353) done. Loss: 0.0243  lr:0.000100
[ Fri May 17 07:05:25 2024 ] 	Batch(2300/2353) done. Loss: 0.0205  lr:0.000100
[ Fri May 17 07:05:45 2024 ] 	Mean training loss: 0.0539.
[ Fri May 17 07:05:45 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 07:05:45 2024 ] Training epoch: 68
[ Fri May 17 07:05:46 2024 ] 	Batch(0/2353) done. Loss: 0.0129  lr:0.000100
[ Fri May 17 07:06:24 2024 ] 	Batch(100/2353) done. Loss: 0.2061  lr:0.000100
[ Fri May 17 07:07:02 2024 ] 	Batch(200/2353) done. Loss: 0.1085  lr:0.000100
[ Fri May 17 07:07:39 2024 ] 	Batch(300/2353) done. Loss: 0.1743  lr:0.000100
[ Fri May 17 07:08:17 2024 ] 	Batch(400/2353) done. Loss: 0.0796  lr:0.000100
[ Fri May 17 07:08:54 2024 ] 	Batch(500/2353) done. Loss: 0.0067  lr:0.000100
[ Fri May 17 07:09:31 2024 ] 	Batch(600/2353) done. Loss: 0.0134  lr:0.000100
[ Fri May 17 07:10:09 2024 ] 	Batch(700/2353) done. Loss: 0.0062  lr:0.000100
[ Fri May 17 07:10:46 2024 ] 	Batch(800/2353) done. Loss: 0.0014  lr:0.000100
[ Fri May 17 07:11:24 2024 ] 	Batch(900/2353) done. Loss: 0.0592  lr:0.000100
[ Fri May 17 07:12:01 2024 ] 	Batch(1000/2353) done. Loss: 0.0036  lr:0.000100
[ Fri May 17 07:12:38 2024 ] 	Batch(1100/2353) done. Loss: 0.0201  lr:0.000100
[ Fri May 17 07:13:16 2024 ] 	Batch(1200/2353) done. Loss: 0.0542  lr:0.000100
[ Fri May 17 07:13:54 2024 ] 	Batch(1300/2353) done. Loss: 0.1052  lr:0.000100
[ Fri May 17 07:14:32 2024 ] 	Batch(1400/2353) done. Loss: 0.0098  lr:0.000100
[ Fri May 17 07:15:09 2024 ] 	Batch(1500/2353) done. Loss: 0.0513  lr:0.000100
[ Fri May 17 07:15:47 2024 ] 	Batch(1600/2353) done. Loss: 0.1436  lr:0.000100
[ Fri May 17 07:16:24 2024 ] 	Batch(1700/2353) done. Loss: 0.0109  lr:0.000100
[ Fri May 17 07:17:01 2024 ] 	Batch(1800/2353) done. Loss: 0.0107  lr:0.000100
[ Fri May 17 07:17:39 2024 ] 	Batch(1900/2353) done. Loss: 0.1179  lr:0.000100
[ Fri May 17 07:18:16 2024 ] 	Batch(2000/2353) done. Loss: 0.0477  lr:0.000100
[ Fri May 17 07:18:54 2024 ] 	Batch(2100/2353) done. Loss: 0.0093  lr:0.000100
[ Fri May 17 07:19:31 2024 ] 	Batch(2200/2353) done. Loss: 0.0703  lr:0.000100
[ Fri May 17 07:20:08 2024 ] 	Batch(2300/2353) done. Loss: 0.1142  lr:0.000100
[ Fri May 17 07:20:28 2024 ] 	Mean training loss: 0.0549.
[ Fri May 17 07:20:28 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 07:20:28 2024 ] Training epoch: 69
[ Fri May 17 07:20:29 2024 ] 	Batch(0/2353) done. Loss: 0.0962  lr:0.000100
[ Fri May 17 07:21:06 2024 ] 	Batch(100/2353) done. Loss: 0.0354  lr:0.000100
[ Fri May 17 07:21:43 2024 ] 	Batch(200/2353) done. Loss: 0.0564  lr:0.000100
[ Fri May 17 07:22:21 2024 ] 	Batch(300/2353) done. Loss: 0.0152  lr:0.000100
[ Fri May 17 07:22:58 2024 ] 	Batch(400/2353) done. Loss: 0.0140  lr:0.000100
[ Fri May 17 07:23:36 2024 ] 	Batch(500/2353) done. Loss: 0.4901  lr:0.000100
[ Fri May 17 07:24:13 2024 ] 	Batch(600/2353) done. Loss: 0.0109  lr:0.000100
[ Fri May 17 07:24:50 2024 ] 	Batch(700/2353) done. Loss: 0.0039  lr:0.000100
[ Fri May 17 07:25:28 2024 ] 	Batch(800/2353) done. Loss: 0.0861  lr:0.000100
[ Fri May 17 07:26:05 2024 ] 	Batch(900/2353) done. Loss: 0.0114  lr:0.000100
[ Fri May 17 07:26:42 2024 ] 	Batch(1000/2353) done. Loss: 0.0196  lr:0.000100
[ Fri May 17 07:27:20 2024 ] 	Batch(1100/2353) done. Loss: 0.0022  lr:0.000100
[ Fri May 17 07:27:57 2024 ] 	Batch(1200/2353) done. Loss: 0.0932  lr:0.000100
[ Fri May 17 07:28:35 2024 ] 	Batch(1300/2353) done. Loss: 0.0697  lr:0.000100
[ Fri May 17 07:29:12 2024 ] 	Batch(1400/2353) done. Loss: 0.0147  lr:0.000100
[ Fri May 17 07:29:49 2024 ] 	Batch(1500/2353) done. Loss: 0.0205  lr:0.000100
[ Fri May 17 07:30:27 2024 ] 	Batch(1600/2353) done. Loss: 0.0147  lr:0.000100
[ Fri May 17 07:31:04 2024 ] 	Batch(1700/2353) done. Loss: 0.0545  lr:0.000100
[ Fri May 17 07:31:42 2024 ] 	Batch(1800/2353) done. Loss: 0.0375  lr:0.000100
[ Fri May 17 07:32:19 2024 ] 	Batch(1900/2353) done. Loss: 0.0442  lr:0.000100
[ Fri May 17 07:32:57 2024 ] 	Batch(2000/2353) done. Loss: 0.0071  lr:0.000100
[ Fri May 17 07:33:34 2024 ] 	Batch(2100/2353) done. Loss: 0.0646  lr:0.000100
[ Fri May 17 07:34:11 2024 ] 	Batch(2200/2353) done. Loss: 0.0465  lr:0.000100
[ Fri May 17 07:34:49 2024 ] 	Batch(2300/2353) done. Loss: 0.0060  lr:0.000100
[ Fri May 17 07:35:08 2024 ] 	Mean training loss: 0.0509.
[ Fri May 17 07:35:08 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 07:35:08 2024 ] Training epoch: 70
[ Fri May 17 07:35:09 2024 ] 	Batch(0/2353) done. Loss: 0.1306  lr:0.000100
[ Fri May 17 07:35:46 2024 ] 	Batch(100/2353) done. Loss: 0.0184  lr:0.000100
[ Fri May 17 07:36:24 2024 ] 	Batch(200/2353) done. Loss: 0.1181  lr:0.000100
[ Fri May 17 07:37:01 2024 ] 	Batch(300/2353) done. Loss: 0.0073  lr:0.000100
[ Fri May 17 07:37:39 2024 ] 	Batch(400/2353) done. Loss: 0.0330  lr:0.000100
[ Fri May 17 07:38:17 2024 ] 	Batch(500/2353) done. Loss: 0.1276  lr:0.000100
[ Fri May 17 07:38:55 2024 ] 	Batch(600/2353) done. Loss: 0.0052  lr:0.000100
[ Fri May 17 07:39:33 2024 ] 	Batch(700/2353) done. Loss: 0.0195  lr:0.000100
[ Fri May 17 07:40:11 2024 ] 	Batch(800/2353) done. Loss: 0.2299  lr:0.000100
[ Fri May 17 07:40:49 2024 ] 	Batch(900/2353) done. Loss: 0.0839  lr:0.000100
[ Fri May 17 07:41:27 2024 ] 	Batch(1000/2353) done. Loss: 0.0307  lr:0.000100
[ Fri May 17 07:42:05 2024 ] 	Batch(1100/2353) done. Loss: 0.1312  lr:0.000100
[ Fri May 17 07:42:43 2024 ] 	Batch(1200/2353) done. Loss: 0.0577  lr:0.000100
[ Fri May 17 07:43:21 2024 ] 	Batch(1300/2353) done. Loss: 0.0065  lr:0.000100
[ Fri May 17 07:43:58 2024 ] 	Batch(1400/2353) done. Loss: 0.0690  lr:0.000100
[ Fri May 17 07:44:36 2024 ] 	Batch(1500/2353) done. Loss: 0.0200  lr:0.000100
[ Fri May 17 07:45:13 2024 ] 	Batch(1600/2353) done. Loss: 0.0136  lr:0.000100
[ Fri May 17 07:45:50 2024 ] 	Batch(1700/2353) done. Loss: 0.0526  lr:0.000100
[ Fri May 17 07:46:28 2024 ] 	Batch(1800/2353) done. Loss: 0.0209  lr:0.000100
[ Fri May 17 07:47:05 2024 ] 	Batch(1900/2353) done. Loss: 0.0061  lr:0.000100
[ Fri May 17 07:47:43 2024 ] 	Batch(2000/2353) done. Loss: 0.1540  lr:0.000100
[ Fri May 17 07:48:20 2024 ] 	Batch(2100/2353) done. Loss: 0.0178  lr:0.000100
[ Fri May 17 07:48:58 2024 ] 	Batch(2200/2353) done. Loss: 0.0156  lr:0.000100
[ Fri May 17 07:49:35 2024 ] 	Batch(2300/2353) done. Loss: 0.0116  lr:0.000100
[ Fri May 17 07:49:54 2024 ] 	Mean training loss: 0.0524.
[ Fri May 17 07:49:54 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 07:49:54 2024 ] Eval epoch: 70
[ Fri May 17 07:51:58 2024 ] 	Mean val loss of 2367 batches: 0.25092287795900503.
[ Fri May 17 07:51:58 2024 ] Training epoch: 71
[ Fri May 17 07:51:58 2024 ] 	Batch(0/2353) done. Loss: 0.0086  lr:0.000100
[ Fri May 17 07:52:36 2024 ] 	Batch(100/2353) done. Loss: 0.1108  lr:0.000100
[ Fri May 17 07:53:14 2024 ] 	Batch(200/2353) done. Loss: 0.1046  lr:0.000100
[ Fri May 17 07:53:52 2024 ] 	Batch(300/2353) done. Loss: 0.2266  lr:0.000100
[ Fri May 17 07:54:30 2024 ] 	Batch(400/2353) done. Loss: 0.0044  lr:0.000100
[ Fri May 17 07:55:07 2024 ] 	Batch(500/2353) done. Loss: 0.0052  lr:0.000100
[ Fri May 17 07:55:45 2024 ] 	Batch(600/2353) done. Loss: 0.0711  lr:0.000100
[ Fri May 17 07:56:23 2024 ] 	Batch(700/2353) done. Loss: 0.0334  lr:0.000100
[ Fri May 17 07:57:01 2024 ] 	Batch(800/2353) done. Loss: 0.1270  lr:0.000100
[ Fri May 17 07:57:39 2024 ] 	Batch(900/2353) done. Loss: 0.0248  lr:0.000100
[ Fri May 17 07:58:17 2024 ] 	Batch(1000/2353) done. Loss: 0.0079  lr:0.000100
[ Fri May 17 07:58:55 2024 ] 	Batch(1100/2353) done. Loss: 0.0021  lr:0.000100
[ Fri May 17 07:59:33 2024 ] 	Batch(1200/2353) done. Loss: 0.0163  lr:0.000100
[ Fri May 17 08:00:10 2024 ] 	Batch(1300/2353) done. Loss: 0.0034  lr:0.000100
[ Fri May 17 08:00:48 2024 ] 	Batch(1400/2353) done. Loss: 0.0222  lr:0.000100
[ Fri May 17 08:01:25 2024 ] 	Batch(1500/2353) done. Loss: 0.0178  lr:0.000100
[ Fri May 17 08:02:02 2024 ] 	Batch(1600/2353) done. Loss: 0.0037  lr:0.000100
[ Fri May 17 08:02:40 2024 ] 	Batch(1700/2353) done. Loss: 0.0283  lr:0.000100
[ Fri May 17 08:03:17 2024 ] 	Batch(1800/2353) done. Loss: 0.0285  lr:0.000100
[ Fri May 17 08:03:54 2024 ] 	Batch(1900/2353) done. Loss: 0.0211  lr:0.000100
[ Fri May 17 08:04:32 2024 ] 	Batch(2000/2353) done. Loss: 0.0044  lr:0.000100
[ Fri May 17 08:05:09 2024 ] 	Batch(2100/2353) done. Loss: 0.2514  lr:0.000100
[ Fri May 17 08:05:47 2024 ] 	Batch(2200/2353) done. Loss: 0.0587  lr:0.000100
[ Fri May 17 08:06:24 2024 ] 	Batch(2300/2353) done. Loss: 0.1952  lr:0.000100
[ Fri May 17 08:06:43 2024 ] 	Mean training loss: 0.0474.
[ Fri May 17 08:06:43 2024 ] 	Time consumption: [Data]01%, [Network]92%
[ Fri May 17 08:06:43 2024 ] Training epoch: 72
[ Fri May 17 08:06:44 2024 ] 	Batch(0/2353) done. Loss: 0.0268  lr:0.000100
[ Fri May 17 08:07:22 2024 ] 	Batch(100/2353) done. Loss: 0.0140  lr:0.000100
[ Fri May 17 08:07:59 2024 ] 	Batch(200/2353) done. Loss: 0.0296  lr:0.000100
[ Fri May 17 08:08:36 2024 ] 	Batch(300/2353) done. Loss: 0.0088  lr:0.000100
[ Fri May 17 08:09:14 2024 ] 	Batch(400/2353) done. Loss: 0.0486  lr:0.000100
[ Fri May 17 08:09:51 2024 ] 	Batch(500/2353) done. Loss: 0.0430  lr:0.000100
[ Fri May 17 08:10:28 2024 ] 	Batch(600/2353) done. Loss: 0.0181  lr:0.000100
[ Fri May 17 08:11:06 2024 ] 	Batch(700/2353) done. Loss: 0.0318  lr:0.000100
[ Fri May 17 08:11:43 2024 ] 	Batch(800/2353) done. Loss: 0.0339  lr:0.000100
[ Fri May 17 08:12:21 2024 ] 	Batch(900/2353) done. Loss: 0.0811  lr:0.000100
[ Fri May 17 08:12:58 2024 ] 	Batch(1000/2353) done. Loss: 0.0477  lr:0.000100
[ Fri May 17 08:13:35 2024 ] 	Batch(1100/2353) done. Loss: 0.0135  lr:0.000100
[ Fri May 17 08:14:13 2024 ] 	Batch(1200/2353) done. Loss: 0.0680  lr:0.000100
[ Fri May 17 08:14:50 2024 ] 	Batch(1300/2353) done. Loss: 0.0019  lr:0.000100
[ Fri May 17 08:15:27 2024 ] 	Batch(1400/2353) done. Loss: 0.0052  lr:0.000100
[ Fri May 17 08:16:05 2024 ] 	Batch(1500/2353) done. Loss: 0.1027  lr:0.000100
[ Fri May 17 08:16:42 2024 ] 	Batch(1600/2353) done. Loss: 0.0038  lr:0.000100
[ Fri May 17 08:17:20 2024 ] 	Batch(1700/2353) done. Loss: 0.1033  lr:0.000100
[ Fri May 17 08:17:57 2024 ] 	Batch(1800/2353) done. Loss: 0.0405  lr:0.000100
[ Fri May 17 08:18:34 2024 ] 	Batch(1900/2353) done. Loss: 0.0099  lr:0.000100
[ Fri May 17 08:19:12 2024 ] 	Batch(2000/2353) done. Loss: 0.0262  lr:0.000100
[ Fri May 17 08:19:49 2024 ] 	Batch(2100/2353) done. Loss: 0.0301  lr:0.000100
[ Fri May 17 08:20:27 2024 ] 	Batch(2200/2353) done. Loss: 0.0094  lr:0.000100
[ Fri May 17 08:21:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0425  lr:0.000100
[ Fri May 17 08:21:23 2024 ] 	Mean training loss: 0.0459.
[ Fri May 17 08:21:23 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 08:21:24 2024 ] Training epoch: 73
[ Fri May 17 08:21:24 2024 ] 	Batch(0/2353) done. Loss: 0.0072  lr:0.000100
[ Fri May 17 08:22:02 2024 ] 	Batch(100/2353) done. Loss: 0.0308  lr:0.000100
[ Fri May 17 08:22:39 2024 ] 	Batch(200/2353) done. Loss: 0.0135  lr:0.000100
[ Fri May 17 08:23:16 2024 ] 	Batch(300/2353) done. Loss: 0.0530  lr:0.000100
[ Fri May 17 08:23:54 2024 ] 	Batch(400/2353) done. Loss: 0.0100  lr:0.000100
[ Fri May 17 08:24:31 2024 ] 	Batch(500/2353) done. Loss: 0.0057  lr:0.000100
[ Fri May 17 08:25:08 2024 ] 	Batch(600/2353) done. Loss: 0.0447  lr:0.000100
[ Fri May 17 08:25:45 2024 ] 	Batch(700/2353) done. Loss: 0.0398  lr:0.000100
[ Fri May 17 08:26:23 2024 ] 	Batch(800/2353) done. Loss: 0.0606  lr:0.000100
[ Fri May 17 08:27:01 2024 ] 	Batch(900/2353) done. Loss: 0.0406  lr:0.000100
[ Fri May 17 08:27:39 2024 ] 	Batch(1000/2353) done. Loss: 0.1280  lr:0.000100
[ Fri May 17 08:28:17 2024 ] 	Batch(1100/2353) done. Loss: 0.0064  lr:0.000100
[ Fri May 17 08:28:55 2024 ] 	Batch(1200/2353) done. Loss: 0.0401  lr:0.000100
[ Fri May 17 08:29:33 2024 ] 	Batch(1300/2353) done. Loss: 0.0269  lr:0.000100
[ Fri May 17 08:30:10 2024 ] 	Batch(1400/2353) done. Loss: 0.0120  lr:0.000100
[ Fri May 17 08:30:47 2024 ] 	Batch(1500/2353) done. Loss: 0.0215  lr:0.000100
[ Fri May 17 08:31:25 2024 ] 	Batch(1600/2353) done. Loss: 0.0189  lr:0.000100
[ Fri May 17 08:32:02 2024 ] 	Batch(1700/2353) done. Loss: 0.0423  lr:0.000100
[ Fri May 17 08:32:40 2024 ] 	Batch(1800/2353) done. Loss: 0.1086  lr:0.000100
[ Fri May 17 08:33:17 2024 ] 	Batch(1900/2353) done. Loss: 0.0268  lr:0.000100
[ Fri May 17 08:33:54 2024 ] 	Batch(2000/2353) done. Loss: 0.0052  lr:0.000100
[ Fri May 17 08:34:32 2024 ] 	Batch(2100/2353) done. Loss: 0.0485  lr:0.000100
[ Fri May 17 08:35:09 2024 ] 	Batch(2200/2353) done. Loss: 0.0430  lr:0.000100
[ Fri May 17 08:35:46 2024 ] 	Batch(2300/2353) done. Loss: 0.0479  lr:0.000100
[ Fri May 17 08:36:06 2024 ] 	Mean training loss: 0.0493.
[ Fri May 17 08:36:06 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 08:36:06 2024 ] Training epoch: 74
[ Fri May 17 08:36:07 2024 ] 	Batch(0/2353) done. Loss: 0.0579  lr:0.000100
[ Fri May 17 08:36:44 2024 ] 	Batch(100/2353) done. Loss: 0.0027  lr:0.000100
[ Fri May 17 08:37:22 2024 ] 	Batch(200/2353) done. Loss: 0.0042  lr:0.000100
[ Fri May 17 08:37:59 2024 ] 	Batch(300/2353) done. Loss: 0.0417  lr:0.000100
[ Fri May 17 08:38:36 2024 ] 	Batch(400/2353) done. Loss: 0.0233  lr:0.000100
[ Fri May 17 08:39:14 2024 ] 	Batch(500/2353) done. Loss: 0.0053  lr:0.000100
[ Fri May 17 08:39:51 2024 ] 	Batch(600/2353) done. Loss: 0.2611  lr:0.000100
[ Fri May 17 08:40:28 2024 ] 	Batch(700/2353) done. Loss: 0.0073  lr:0.000100
[ Fri May 17 08:41:06 2024 ] 	Batch(800/2353) done. Loss: 0.0504  lr:0.000100
[ Fri May 17 08:41:43 2024 ] 	Batch(900/2353) done. Loss: 0.1719  lr:0.000100
[ Fri May 17 08:42:21 2024 ] 	Batch(1000/2353) done. Loss: 0.1363  lr:0.000100
[ Fri May 17 08:42:58 2024 ] 	Batch(1100/2353) done. Loss: 0.0184  lr:0.000100
[ Fri May 17 08:43:35 2024 ] 	Batch(1200/2353) done. Loss: 0.0415  lr:0.000100
[ Fri May 17 08:44:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0295  lr:0.000100
[ Fri May 17 08:44:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0069  lr:0.000100
[ Fri May 17 08:45:27 2024 ] 	Batch(1500/2353) done. Loss: 0.0044  lr:0.000100
[ Fri May 17 08:46:05 2024 ] 	Batch(1600/2353) done. Loss: 0.0089  lr:0.000100
[ Fri May 17 08:46:42 2024 ] 	Batch(1700/2353) done. Loss: 0.0155  lr:0.000100
[ Fri May 17 08:47:20 2024 ] 	Batch(1800/2353) done. Loss: 0.0505  lr:0.000100
[ Fri May 17 08:47:57 2024 ] 	Batch(1900/2353) done. Loss: 0.0086  lr:0.000100
[ Fri May 17 08:48:35 2024 ] 	Batch(2000/2353) done. Loss: 0.0038  lr:0.000100
[ Fri May 17 08:49:12 2024 ] 	Batch(2100/2353) done. Loss: 0.0233  lr:0.000100
[ Fri May 17 08:49:49 2024 ] 	Batch(2200/2353) done. Loss: 0.0111  lr:0.000100
[ Fri May 17 08:50:27 2024 ] 	Batch(2300/2353) done. Loss: 0.0010  lr:0.000100
[ Fri May 17 08:50:46 2024 ] 	Mean training loss: 0.0471.
[ Fri May 17 08:50:46 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 08:50:46 2024 ] Training epoch: 75
[ Fri May 17 08:50:47 2024 ] 	Batch(0/2353) done. Loss: 0.0101  lr:0.000100
[ Fri May 17 08:51:24 2024 ] 	Batch(100/2353) done. Loss: 0.0207  lr:0.000100
[ Fri May 17 08:52:01 2024 ] 	Batch(200/2353) done. Loss: 0.0398  lr:0.000100
[ Fri May 17 08:52:39 2024 ] 	Batch(300/2353) done. Loss: 0.0137  lr:0.000100
[ Fri May 17 08:53:16 2024 ] 	Batch(400/2353) done. Loss: 0.0115  lr:0.000100
[ Fri May 17 08:53:53 2024 ] 	Batch(500/2353) done. Loss: 0.0205  lr:0.000100
[ Fri May 17 08:54:31 2024 ] 	Batch(600/2353) done. Loss: 0.0360  lr:0.000100
[ Fri May 17 08:55:08 2024 ] 	Batch(700/2353) done. Loss: 0.0307  lr:0.000100
[ Fri May 17 08:55:46 2024 ] 	Batch(800/2353) done. Loss: 0.0257  lr:0.000100
[ Fri May 17 08:56:23 2024 ] 	Batch(900/2353) done. Loss: 0.0113  lr:0.000100
[ Fri May 17 08:57:00 2024 ] 	Batch(1000/2353) done. Loss: 0.0109  lr:0.000100
[ Fri May 17 08:57:38 2024 ] 	Batch(1100/2353) done. Loss: 0.0082  lr:0.000100
[ Fri May 17 08:58:15 2024 ] 	Batch(1200/2353) done. Loss: 0.0335  lr:0.000100
[ Fri May 17 08:58:53 2024 ] 	Batch(1300/2353) done. Loss: 0.0092  lr:0.000100
[ Fri May 17 08:59:30 2024 ] 	Batch(1400/2353) done. Loss: 0.0860  lr:0.000100
[ Fri May 17 09:00:07 2024 ] 	Batch(1500/2353) done. Loss: 0.0550  lr:0.000100
[ Fri May 17 09:00:45 2024 ] 	Batch(1600/2353) done. Loss: 0.0016  lr:0.000100
[ Fri May 17 09:01:22 2024 ] 	Batch(1700/2353) done. Loss: 0.0152  lr:0.000100
[ Fri May 17 09:02:00 2024 ] 	Batch(1800/2353) done. Loss: 0.0429  lr:0.000100
[ Fri May 17 09:02:37 2024 ] 	Batch(1900/2353) done. Loss: 0.0125  lr:0.000100
[ Fri May 17 09:03:15 2024 ] 	Batch(2000/2353) done. Loss: 0.0175  lr:0.000100
[ Fri May 17 09:03:53 2024 ] 	Batch(2100/2353) done. Loss: 0.0447  lr:0.000100
[ Fri May 17 09:04:30 2024 ] 	Batch(2200/2353) done. Loss: 0.0165  lr:0.000100
[ Fri May 17 09:05:07 2024 ] 	Batch(2300/2353) done. Loss: 0.0135  lr:0.000100
[ Fri May 17 09:05:27 2024 ] 	Mean training loss: 0.0441.
[ Fri May 17 09:05:27 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 09:05:27 2024 ] Training epoch: 76
[ Fri May 17 09:05:28 2024 ] 	Batch(0/2353) done. Loss: 0.0051  lr:0.000100
[ Fri May 17 09:06:05 2024 ] 	Batch(100/2353) done. Loss: 0.0132  lr:0.000100
[ Fri May 17 09:06:42 2024 ] 	Batch(200/2353) done. Loss: 0.0794  lr:0.000100
[ Fri May 17 09:07:20 2024 ] 	Batch(300/2353) done. Loss: 0.0072  lr:0.000100
[ Fri May 17 09:07:57 2024 ] 	Batch(400/2353) done. Loss: 0.0165  lr:0.000100
[ Fri May 17 09:08:35 2024 ] 	Batch(500/2353) done. Loss: 0.0180  lr:0.000100
[ Fri May 17 09:09:12 2024 ] 	Batch(600/2353) done. Loss: 0.0409  lr:0.000100
[ Fri May 17 09:09:50 2024 ] 	Batch(700/2353) done. Loss: 0.0220  lr:0.000100
[ Fri May 17 09:10:27 2024 ] 	Batch(800/2353) done. Loss: 0.0098  lr:0.000100
[ Fri May 17 09:11:05 2024 ] 	Batch(900/2353) done. Loss: 0.0295  lr:0.000100
[ Fri May 17 09:11:42 2024 ] 	Batch(1000/2353) done. Loss: 0.0110  lr:0.000100
[ Fri May 17 09:12:19 2024 ] 	Batch(1100/2353) done. Loss: 0.0469  lr:0.000100
[ Fri May 17 09:12:57 2024 ] 	Batch(1200/2353) done. Loss: 0.0884  lr:0.000100
[ Fri May 17 09:13:34 2024 ] 	Batch(1300/2353) done. Loss: 0.0751  lr:0.000100
[ Fri May 17 09:14:12 2024 ] 	Batch(1400/2353) done. Loss: 0.0030  lr:0.000100
[ Fri May 17 09:14:49 2024 ] 	Batch(1500/2353) done. Loss: 0.0276  lr:0.000100
[ Fri May 17 09:15:26 2024 ] 	Batch(1600/2353) done. Loss: 0.0049  lr:0.000100
[ Fri May 17 09:16:04 2024 ] 	Batch(1700/2353) done. Loss: 0.0254  lr:0.000100
[ Fri May 17 09:16:41 2024 ] 	Batch(1800/2353) done. Loss: 0.0179  lr:0.000100
[ Fri May 17 09:17:18 2024 ] 	Batch(1900/2353) done. Loss: 0.0165  lr:0.000100
[ Fri May 17 09:17:56 2024 ] 	Batch(2000/2353) done. Loss: 0.0383  lr:0.000100
[ Fri May 17 09:18:34 2024 ] 	Batch(2100/2353) done. Loss: 0.0075  lr:0.000100
[ Fri May 17 09:19:12 2024 ] 	Batch(2200/2353) done. Loss: 0.0713  lr:0.000100
[ Fri May 17 09:19:50 2024 ] 	Batch(2300/2353) done. Loss: 0.0829  lr:0.000100
[ Fri May 17 09:20:10 2024 ] 	Mean training loss: 0.0460.
[ Fri May 17 09:20:10 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 09:20:10 2024 ] Training epoch: 77
[ Fri May 17 09:20:11 2024 ] 	Batch(0/2353) done. Loss: 0.0014  lr:0.000100
[ Fri May 17 09:20:48 2024 ] 	Batch(100/2353) done. Loss: 0.0059  lr:0.000100
[ Fri May 17 09:21:26 2024 ] 	Batch(200/2353) done. Loss: 0.0495  lr:0.000100
[ Fri May 17 09:22:03 2024 ] 	Batch(300/2353) done. Loss: 0.0108  lr:0.000100
[ Fri May 17 09:22:41 2024 ] 	Batch(400/2353) done. Loss: 0.0731  lr:0.000100
[ Fri May 17 09:23:18 2024 ] 	Batch(500/2353) done. Loss: 0.3255  lr:0.000100
[ Fri May 17 09:23:55 2024 ] 	Batch(600/2353) done. Loss: 0.0023  lr:0.000100
[ Fri May 17 09:24:33 2024 ] 	Batch(700/2353) done. Loss: 0.0073  lr:0.000100
[ Fri May 17 09:25:10 2024 ] 	Batch(800/2353) done. Loss: 0.0178  lr:0.000100
[ Fri May 17 09:25:48 2024 ] 	Batch(900/2353) done. Loss: 0.0367  lr:0.000100
[ Fri May 17 09:26:25 2024 ] 	Batch(1000/2353) done. Loss: 0.0601  lr:0.000100
[ Fri May 17 09:27:02 2024 ] 	Batch(1100/2353) done. Loss: 0.0610  lr:0.000100
[ Fri May 17 09:27:40 2024 ] 	Batch(1200/2353) done. Loss: 0.0202  lr:0.000100
[ Fri May 17 09:28:17 2024 ] 	Batch(1300/2353) done. Loss: 0.0477  lr:0.000100
[ Fri May 17 09:28:54 2024 ] 	Batch(1400/2353) done. Loss: 0.0914  lr:0.000100
[ Fri May 17 09:29:32 2024 ] 	Batch(1500/2353) done. Loss: 0.0047  lr:0.000100
[ Fri May 17 09:30:09 2024 ] 	Batch(1600/2353) done. Loss: 0.0058  lr:0.000100
[ Fri May 17 09:30:47 2024 ] 	Batch(1700/2353) done. Loss: 0.0046  lr:0.000100
[ Fri May 17 09:31:24 2024 ] 	Batch(1800/2353) done. Loss: 0.0182  lr:0.000100
[ Fri May 17 09:32:01 2024 ] 	Batch(1900/2353) done. Loss: 0.0292  lr:0.000100
[ Fri May 17 09:32:39 2024 ] 	Batch(2000/2353) done. Loss: 0.0111  lr:0.000100
[ Fri May 17 09:33:16 2024 ] 	Batch(2100/2353) done. Loss: 0.0071  lr:0.000100
[ Fri May 17 09:33:54 2024 ] 	Batch(2200/2353) done. Loss: 0.0262  lr:0.000100
[ Fri May 17 09:34:31 2024 ] 	Batch(2300/2353) done. Loss: 0.0071  lr:0.000100
[ Fri May 17 09:34:50 2024 ] 	Mean training loss: 0.0429.
[ Fri May 17 09:34:50 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 09:34:51 2024 ] Training epoch: 78
[ Fri May 17 09:34:51 2024 ] 	Batch(0/2353) done. Loss: 0.0204  lr:0.000100
[ Fri May 17 09:35:29 2024 ] 	Batch(100/2353) done. Loss: 0.0939  lr:0.000100
[ Fri May 17 09:36:06 2024 ] 	Batch(200/2353) done. Loss: 0.0208  lr:0.000100
[ Fri May 17 09:36:43 2024 ] 	Batch(300/2353) done. Loss: 0.0082  lr:0.000100
[ Fri May 17 09:37:21 2024 ] 	Batch(400/2353) done. Loss: 0.0880  lr:0.000100
[ Fri May 17 09:37:58 2024 ] 	Batch(500/2353) done. Loss: 0.0280  lr:0.000100
[ Fri May 17 09:38:35 2024 ] 	Batch(600/2353) done. Loss: 0.0666  lr:0.000100
[ Fri May 17 09:39:13 2024 ] 	Batch(700/2353) done. Loss: 0.0231  lr:0.000100
[ Fri May 17 09:39:50 2024 ] 	Batch(800/2353) done. Loss: 0.0285  lr:0.000100
[ Fri May 17 09:40:27 2024 ] 	Batch(900/2353) done. Loss: 0.0104  lr:0.000100
[ Fri May 17 09:41:05 2024 ] 	Batch(1000/2353) done. Loss: 0.0381  lr:0.000100
[ Fri May 17 09:41:42 2024 ] 	Batch(1100/2353) done. Loss: 0.1476  lr:0.000100
[ Fri May 17 09:42:20 2024 ] 	Batch(1200/2353) done. Loss: 0.1603  lr:0.000100
[ Fri May 17 09:42:57 2024 ] 	Batch(1300/2353) done. Loss: 0.0027  lr:0.000100
[ Fri May 17 09:43:34 2024 ] 	Batch(1400/2353) done. Loss: 0.0339  lr:0.000100
[ Fri May 17 09:44:12 2024 ] 	Batch(1500/2353) done. Loss: 0.0142  lr:0.000100
[ Fri May 17 09:44:49 2024 ] 	Batch(1600/2353) done. Loss: 0.0178  lr:0.000100
[ Fri May 17 09:45:27 2024 ] 	Batch(1700/2353) done. Loss: 0.0591  lr:0.000100
[ Fri May 17 09:46:04 2024 ] 	Batch(1800/2353) done. Loss: 0.0624  lr:0.000100
[ Fri May 17 09:46:41 2024 ] 	Batch(1900/2353) done. Loss: 0.0473  lr:0.000100
[ Fri May 17 09:47:19 2024 ] 	Batch(2000/2353) done. Loss: 0.0321  lr:0.000100
[ Fri May 17 09:47:56 2024 ] 	Batch(2100/2353) done. Loss: 0.0079  lr:0.000100
[ Fri May 17 09:48:34 2024 ] 	Batch(2200/2353) done. Loss: 0.1181  lr:0.000100
[ Fri May 17 09:49:11 2024 ] 	Batch(2300/2353) done. Loss: 0.0109  lr:0.000100
[ Fri May 17 09:49:30 2024 ] 	Mean training loss: 0.0437.
[ Fri May 17 09:49:30 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 09:49:30 2024 ] Training epoch: 79
[ Fri May 17 09:49:31 2024 ] 	Batch(0/2353) done. Loss: 0.0116  lr:0.000100
[ Fri May 17 09:50:09 2024 ] 	Batch(100/2353) done. Loss: 0.0012  lr:0.000100
[ Fri May 17 09:50:46 2024 ] 	Batch(200/2353) done. Loss: 0.0270  lr:0.000100
[ Fri May 17 09:51:23 2024 ] 	Batch(300/2353) done. Loss: 0.0221  lr:0.000100
[ Fri May 17 09:52:01 2024 ] 	Batch(400/2353) done. Loss: 0.0055  lr:0.000100
[ Fri May 17 09:52:38 2024 ] 	Batch(500/2353) done. Loss: 0.0012  lr:0.000100
[ Fri May 17 09:53:15 2024 ] 	Batch(600/2353) done. Loss: 0.0078  lr:0.000100
[ Fri May 17 09:53:53 2024 ] 	Batch(700/2353) done. Loss: 0.0551  lr:0.000100
[ Fri May 17 09:54:30 2024 ] 	Batch(800/2353) done. Loss: 0.0020  lr:0.000100
[ Fri May 17 09:55:08 2024 ] 	Batch(900/2353) done. Loss: 0.0052  lr:0.000100
[ Fri May 17 09:55:46 2024 ] 	Batch(1000/2353) done. Loss: 0.0332  lr:0.000100
[ Fri May 17 09:56:24 2024 ] 	Batch(1100/2353) done. Loss: 0.0345  lr:0.000100
[ Fri May 17 09:57:02 2024 ] 	Batch(1200/2353) done. Loss: 0.0348  lr:0.000100
[ Fri May 17 09:57:39 2024 ] 	Batch(1300/2353) done. Loss: 0.0146  lr:0.000100
[ Fri May 17 09:58:17 2024 ] 	Batch(1400/2353) done. Loss: 0.0747  lr:0.000100
[ Fri May 17 09:58:54 2024 ] 	Batch(1500/2353) done. Loss: 0.0093  lr:0.000100
[ Fri May 17 09:59:32 2024 ] 	Batch(1600/2353) done. Loss: 0.1458  lr:0.000100
[ Fri May 17 10:00:09 2024 ] 	Batch(1700/2353) done. Loss: 0.0737  lr:0.000100
[ Fri May 17 10:00:47 2024 ] 	Batch(1800/2353) done. Loss: 0.0178  lr:0.000100
[ Fri May 17 10:01:24 2024 ] 	Batch(1900/2353) done. Loss: 0.0529  lr:0.000100
[ Fri May 17 10:02:02 2024 ] 	Batch(2000/2353) done. Loss: 0.0025  lr:0.000100
[ Fri May 17 10:02:39 2024 ] 	Batch(2100/2353) done. Loss: 0.0250  lr:0.000100
[ Fri May 17 10:03:17 2024 ] 	Batch(2200/2353) done. Loss: 0.0893  lr:0.000100
[ Fri May 17 10:03:54 2024 ] 	Batch(2300/2353) done. Loss: 0.0292  lr:0.000100
[ Fri May 17 10:04:14 2024 ] 	Mean training loss: 0.0444.
[ Fri May 17 10:04:14 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 10:04:14 2024 ] Training epoch: 80
[ Fri May 17 10:04:15 2024 ] 	Batch(0/2353) done. Loss: 0.0058  lr:0.000100
[ Fri May 17 10:04:52 2024 ] 	Batch(100/2353) done. Loss: 0.0581  lr:0.000100
[ Fri May 17 10:05:30 2024 ] 	Batch(200/2353) done. Loss: 0.0330  lr:0.000100
[ Fri May 17 10:06:08 2024 ] 	Batch(300/2353) done. Loss: 0.0102  lr:0.000100
[ Fri May 17 10:06:46 2024 ] 	Batch(400/2353) done. Loss: 0.1405  lr:0.000100
[ Fri May 17 10:07:23 2024 ] 	Batch(500/2353) done. Loss: 0.0040  lr:0.000100
[ Fri May 17 10:08:01 2024 ] 	Batch(600/2353) done. Loss: 0.0096  lr:0.000100
[ Fri May 17 10:08:38 2024 ] 	Batch(700/2353) done. Loss: 0.0387  lr:0.000100
[ Fri May 17 10:09:16 2024 ] 	Batch(800/2353) done. Loss: 0.0239  lr:0.000100
[ Fri May 17 10:09:53 2024 ] 	Batch(900/2353) done. Loss: 0.0423  lr:0.000100
[ Fri May 17 10:10:30 2024 ] 	Batch(1000/2353) done. Loss: 0.0746  lr:0.000100
[ Fri May 17 10:11:08 2024 ] 	Batch(1100/2353) done. Loss: 0.0061  lr:0.000100
[ Fri May 17 10:11:45 2024 ] 	Batch(1200/2353) done. Loss: 0.0042  lr:0.000100
[ Fri May 17 10:12:22 2024 ] 	Batch(1300/2353) done. Loss: 0.0418  lr:0.000100
[ Fri May 17 10:13:00 2024 ] 	Batch(1400/2353) done. Loss: 0.0332  lr:0.000100
[ Fri May 17 10:13:37 2024 ] 	Batch(1500/2353) done. Loss: 0.0497  lr:0.000100
[ Fri May 17 10:14:14 2024 ] 	Batch(1600/2353) done. Loss: 0.0092  lr:0.000100
[ Fri May 17 10:14:52 2024 ] 	Batch(1700/2353) done. Loss: 0.0265  lr:0.000100
[ Fri May 17 10:15:29 2024 ] 	Batch(1800/2353) done. Loss: 0.0116  lr:0.000100
[ Fri May 17 10:16:07 2024 ] 	Batch(1900/2353) done. Loss: 0.0072  lr:0.000100
[ Fri May 17 10:16:44 2024 ] 	Batch(2000/2353) done. Loss: 0.0132  lr:0.000100
[ Fri May 17 10:17:21 2024 ] 	Batch(2100/2353) done. Loss: 0.0065  lr:0.000100
[ Fri May 17 10:17:59 2024 ] 	Batch(2200/2353) done. Loss: 0.0157  lr:0.000100
[ Fri May 17 10:18:36 2024 ] 	Batch(2300/2353) done. Loss: 0.0413  lr:0.000100
[ Fri May 17 10:18:56 2024 ] 	Mean training loss: 0.0423.
[ Fri May 17 10:18:56 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 10:18:56 2024 ] Eval epoch: 80
[ Fri May 17 10:20:59 2024 ] 	Mean val loss of 2367 batches: 0.242798213116084.
[ Fri May 17 10:20:59 2024 ] Training epoch: 81
[ Fri May 17 10:20:59 2024 ] 	Batch(0/2353) done. Loss: 0.0046  lr:0.000100
[ Fri May 17 10:21:37 2024 ] 	Batch(100/2353) done. Loss: 0.1408  lr:0.000100
[ Fri May 17 10:22:14 2024 ] 	Batch(200/2353) done. Loss: 0.0109  lr:0.000100
[ Fri May 17 10:22:51 2024 ] 	Batch(300/2353) done. Loss: 0.0100  lr:0.000100
[ Fri May 17 10:23:29 2024 ] 	Batch(400/2353) done. Loss: 0.0200  lr:0.000100
[ Fri May 17 10:24:06 2024 ] 	Batch(500/2353) done. Loss: 0.0163  lr:0.000100
[ Fri May 17 10:24:44 2024 ] 	Batch(600/2353) done. Loss: 0.1594  lr:0.000100
[ Fri May 17 10:25:21 2024 ] 	Batch(700/2353) done. Loss: 0.0360  lr:0.000100
[ Fri May 17 10:25:58 2024 ] 	Batch(800/2353) done. Loss: 0.1488  lr:0.000100
[ Fri May 17 10:26:36 2024 ] 	Batch(900/2353) done. Loss: 0.0289  lr:0.000100
[ Fri May 17 10:27:13 2024 ] 	Batch(1000/2353) done. Loss: 0.0646  lr:0.000100
[ Fri May 17 10:27:50 2024 ] 	Batch(1100/2353) done. Loss: 0.0112  lr:0.000100
[ Fri May 17 10:28:28 2024 ] 	Batch(1200/2353) done. Loss: 0.0458  lr:0.000100
[ Fri May 17 10:29:05 2024 ] 	Batch(1300/2353) done. Loss: 0.0425  lr:0.000100
[ Fri May 17 10:29:43 2024 ] 	Batch(1400/2353) done. Loss: 0.0423  lr:0.000100
[ Fri May 17 10:30:20 2024 ] 	Batch(1500/2353) done. Loss: 0.0101  lr:0.000100
[ Fri May 17 10:30:58 2024 ] 	Batch(1600/2353) done. Loss: 0.1854  lr:0.000100
[ Fri May 17 10:31:35 2024 ] 	Batch(1700/2353) done. Loss: 0.0043  lr:0.000100
[ Fri May 17 10:32:12 2024 ] 	Batch(1800/2353) done. Loss: 0.0230  lr:0.000100
[ Fri May 17 10:32:50 2024 ] 	Batch(1900/2353) done. Loss: 0.0289  lr:0.000100
[ Fri May 17 10:33:28 2024 ] 	Batch(2000/2353) done. Loss: 0.1317  lr:0.000100
[ Fri May 17 10:34:05 2024 ] 	Batch(2100/2353) done. Loss: 0.0951  lr:0.000100
[ Fri May 17 10:34:43 2024 ] 	Batch(2200/2353) done. Loss: 0.1096  lr:0.000100
[ Fri May 17 10:35:20 2024 ] 	Batch(2300/2353) done. Loss: 0.0139  lr:0.000100
[ Fri May 17 10:35:40 2024 ] 	Mean training loss: 0.0424.
[ Fri May 17 10:35:40 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 10:35:40 2024 ] Training epoch: 82
[ Fri May 17 10:35:40 2024 ] 	Batch(0/2353) done. Loss: 0.0346  lr:0.000100
[ Fri May 17 10:36:18 2024 ] 	Batch(100/2353) done. Loss: 0.0089  lr:0.000100
[ Fri May 17 10:36:55 2024 ] 	Batch(200/2353) done. Loss: 0.0398  lr:0.000100
[ Fri May 17 10:37:33 2024 ] 	Batch(300/2353) done. Loss: 0.0215  lr:0.000100
[ Fri May 17 10:38:10 2024 ] 	Batch(400/2353) done. Loss: 0.0094  lr:0.000100
[ Fri May 17 10:38:47 2024 ] 	Batch(500/2353) done. Loss: 0.0499  lr:0.000100
[ Fri May 17 10:39:25 2024 ] 	Batch(600/2353) done. Loss: 0.0185  lr:0.000100
[ Fri May 17 10:40:02 2024 ] 	Batch(700/2353) done. Loss: 0.0650  lr:0.000100
[ Fri May 17 10:40:40 2024 ] 	Batch(800/2353) done. Loss: 0.0080  lr:0.000100
[ Fri May 17 10:41:17 2024 ] 	Batch(900/2353) done. Loss: 0.4807  lr:0.000100
[ Fri May 17 10:41:54 2024 ] 	Batch(1000/2353) done. Loss: 0.0110  lr:0.000100
[ Fri May 17 10:42:32 2024 ] 	Batch(1100/2353) done. Loss: 0.0348  lr:0.000100
[ Fri May 17 10:43:09 2024 ] 	Batch(1200/2353) done. Loss: 0.0276  lr:0.000100
[ Fri May 17 10:43:47 2024 ] 	Batch(1300/2353) done. Loss: 0.0093  lr:0.000100
[ Fri May 17 10:44:24 2024 ] 	Batch(1400/2353) done. Loss: 0.0661  lr:0.000100
[ Fri May 17 10:45:01 2024 ] 	Batch(1500/2353) done. Loss: 0.0270  lr:0.000100
[ Fri May 17 10:45:39 2024 ] 	Batch(1600/2353) done. Loss: 0.0051  lr:0.000100
[ Fri May 17 10:46:16 2024 ] 	Batch(1700/2353) done. Loss: 0.0149  lr:0.000100
[ Fri May 17 10:46:53 2024 ] 	Batch(1800/2353) done. Loss: 0.1585  lr:0.000100
[ Fri May 17 10:47:31 2024 ] 	Batch(1900/2353) done. Loss: 0.0166  lr:0.000100
[ Fri May 17 10:48:08 2024 ] 	Batch(2000/2353) done. Loss: 0.0490  lr:0.000100
[ Fri May 17 10:48:47 2024 ] 	Batch(2100/2353) done. Loss: 0.0069  lr:0.000100
[ Fri May 17 10:49:25 2024 ] 	Batch(2200/2353) done. Loss: 0.0336  lr:0.000100
[ Fri May 17 10:50:03 2024 ] 	Batch(2300/2353) done. Loss: 0.0075  lr:0.000100
[ Fri May 17 10:50:23 2024 ] 	Mean training loss: 0.0419.
[ Fri May 17 10:50:23 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 10:50:23 2024 ] Training epoch: 83
[ Fri May 17 10:50:23 2024 ] 	Batch(0/2353) done. Loss: 0.0036  lr:0.000100
[ Fri May 17 10:51:01 2024 ] 	Batch(100/2353) done. Loss: 0.0560  lr:0.000100
[ Fri May 17 10:51:38 2024 ] 	Batch(200/2353) done. Loss: 0.0022  lr:0.000100
[ Fri May 17 10:52:15 2024 ] 	Batch(300/2353) done. Loss: 0.0497  lr:0.000100
[ Fri May 17 10:52:53 2024 ] 	Batch(400/2353) done. Loss: 0.0319  lr:0.000100
[ Fri May 17 10:53:30 2024 ] 	Batch(500/2353) done. Loss: 0.0408  lr:0.000100
[ Fri May 17 10:54:07 2024 ] 	Batch(600/2353) done. Loss: 0.1037  lr:0.000100
[ Fri May 17 10:54:45 2024 ] 	Batch(700/2353) done. Loss: 0.0046  lr:0.000100
[ Fri May 17 10:55:22 2024 ] 	Batch(800/2353) done. Loss: 0.0120  lr:0.000100
[ Fri May 17 10:56:00 2024 ] 	Batch(900/2353) done. Loss: 0.0244  lr:0.000100
[ Fri May 17 10:56:38 2024 ] 	Batch(1000/2353) done. Loss: 0.0697  lr:0.000100
[ Fri May 17 10:57:16 2024 ] 	Batch(1100/2353) done. Loss: 0.0975  lr:0.000100
[ Fri May 17 10:57:54 2024 ] 	Batch(1200/2353) done. Loss: 0.0201  lr:0.000100
[ Fri May 17 10:58:32 2024 ] 	Batch(1300/2353) done. Loss: 0.0297  lr:0.000100
[ Fri May 17 10:59:09 2024 ] 	Batch(1400/2353) done. Loss: 0.0205  lr:0.000100
[ Fri May 17 10:59:46 2024 ] 	Batch(1500/2353) done. Loss: 0.0103  lr:0.000100
[ Fri May 17 11:00:24 2024 ] 	Batch(1600/2353) done. Loss: 0.0126  lr:0.000100
[ Fri May 17 11:01:01 2024 ] 	Batch(1700/2353) done. Loss: 0.0802  lr:0.000100
[ Fri May 17 11:01:38 2024 ] 	Batch(1800/2353) done. Loss: 0.0099  lr:0.000100
[ Fri May 17 11:02:16 2024 ] 	Batch(1900/2353) done. Loss: 0.0073  lr:0.000100
[ Fri May 17 11:02:53 2024 ] 	Batch(2000/2353) done. Loss: 0.0284  lr:0.000100
[ Fri May 17 11:03:31 2024 ] 	Batch(2100/2353) done. Loss: 0.0254  lr:0.000100
[ Fri May 17 11:04:08 2024 ] 	Batch(2200/2353) done. Loss: 0.0080  lr:0.000100
[ Fri May 17 11:04:45 2024 ] 	Batch(2300/2353) done. Loss: 0.0976  lr:0.000100
[ Fri May 17 11:05:05 2024 ] 	Mean training loss: 0.0406.
[ Fri May 17 11:05:05 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 11:05:05 2024 ] Training epoch: 84
[ Fri May 17 11:05:06 2024 ] 	Batch(0/2353) done. Loss: 0.0194  lr:0.000100
[ Fri May 17 11:05:43 2024 ] 	Batch(100/2353) done. Loss: 0.0164  lr:0.000100
[ Fri May 17 11:06:20 2024 ] 	Batch(200/2353) done. Loss: 0.0077  lr:0.000100
[ Fri May 17 11:06:58 2024 ] 	Batch(300/2353) done. Loss: 0.0139  lr:0.000100
[ Fri May 17 11:07:35 2024 ] 	Batch(400/2353) done. Loss: 0.0250  lr:0.000100
[ Fri May 17 11:08:12 2024 ] 	Batch(500/2353) done. Loss: 0.0280  lr:0.000100
[ Fri May 17 11:08:50 2024 ] 	Batch(600/2353) done. Loss: 0.0270  lr:0.000100
[ Fri May 17 11:09:27 2024 ] 	Batch(700/2353) done. Loss: 0.0063  lr:0.000100
[ Fri May 17 11:10:05 2024 ] 	Batch(800/2353) done. Loss: 0.0479  lr:0.000100
[ Fri May 17 11:10:42 2024 ] 	Batch(900/2353) done. Loss: 0.0267  lr:0.000100
[ Fri May 17 11:11:19 2024 ] 	Batch(1000/2353) done. Loss: 0.0165  lr:0.000100
[ Fri May 17 11:11:57 2024 ] 	Batch(1100/2353) done. Loss: 0.0085  lr:0.000100
[ Fri May 17 11:12:34 2024 ] 	Batch(1200/2353) done. Loss: 0.0179  lr:0.000100
[ Fri May 17 11:13:12 2024 ] 	Batch(1300/2353) done. Loss: 0.0212  lr:0.000100
[ Fri May 17 11:13:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0016  lr:0.000100
[ Fri May 17 11:14:28 2024 ] 	Batch(1500/2353) done. Loss: 0.0294  lr:0.000100
[ Fri May 17 11:15:07 2024 ] 	Batch(1600/2353) done. Loss: 0.0375  lr:0.000100
[ Fri May 17 11:15:44 2024 ] 	Batch(1700/2353) done. Loss: 0.0517  lr:0.000100
[ Fri May 17 11:16:21 2024 ] 	Batch(1800/2353) done. Loss: 0.0183  lr:0.000100
[ Fri May 17 11:16:59 2024 ] 	Batch(1900/2353) done. Loss: 0.0326  lr:0.000100
[ Fri May 17 11:17:36 2024 ] 	Batch(2000/2353) done. Loss: 0.0014  lr:0.000100
[ Fri May 17 11:18:14 2024 ] 	Batch(2100/2353) done. Loss: 0.0129  lr:0.000100
[ Fri May 17 11:18:51 2024 ] 	Batch(2200/2353) done. Loss: 0.0164  lr:0.000100
[ Fri May 17 11:19:28 2024 ] 	Batch(2300/2353) done. Loss: 0.0080  lr:0.000100
[ Fri May 17 11:19:48 2024 ] 	Mean training loss: 0.0392.
[ Fri May 17 11:19:48 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 11:19:48 2024 ] Training epoch: 85
[ Fri May 17 11:19:49 2024 ] 	Batch(0/2353) done. Loss: 0.0289  lr:0.000100
[ Fri May 17 11:20:26 2024 ] 	Batch(100/2353) done. Loss: 0.0458  lr:0.000100
[ Fri May 17 11:21:03 2024 ] 	Batch(200/2353) done. Loss: 0.0048  lr:0.000100
[ Fri May 17 11:21:41 2024 ] 	Batch(300/2353) done. Loss: 0.0108  lr:0.000100
[ Fri May 17 11:22:18 2024 ] 	Batch(400/2353) done. Loss: 0.0179  lr:0.000100
[ Fri May 17 11:22:56 2024 ] 	Batch(500/2353) done. Loss: 0.0168  lr:0.000100
[ Fri May 17 11:23:33 2024 ] 	Batch(600/2353) done. Loss: 0.0023  lr:0.000100
[ Fri May 17 11:24:10 2024 ] 	Batch(700/2353) done. Loss: 0.0495  lr:0.000100
[ Fri May 17 11:24:48 2024 ] 	Batch(800/2353) done. Loss: 0.0019  lr:0.000100
[ Fri May 17 11:25:25 2024 ] 	Batch(900/2353) done. Loss: 0.0401  lr:0.000100
[ Fri May 17 11:26:03 2024 ] 	Batch(1000/2353) done. Loss: 0.0470  lr:0.000100
[ Fri May 17 11:26:40 2024 ] 	Batch(1100/2353) done. Loss: 0.0093  lr:0.000100
[ Fri May 17 11:27:17 2024 ] 	Batch(1200/2353) done. Loss: 0.0150  lr:0.000100
[ Fri May 17 11:27:55 2024 ] 	Batch(1300/2353) done. Loss: 0.0410  lr:0.000100
[ Fri May 17 11:28:32 2024 ] 	Batch(1400/2353) done. Loss: 0.0081  lr:0.000100
[ Fri May 17 11:29:09 2024 ] 	Batch(1500/2353) done. Loss: 0.0014  lr:0.000100
[ Fri May 17 11:29:47 2024 ] 	Batch(1600/2353) done. Loss: 0.0330  lr:0.000100
[ Fri May 17 11:30:25 2024 ] 	Batch(1700/2353) done. Loss: 0.0081  lr:0.000100
[ Fri May 17 11:31:03 2024 ] 	Batch(1800/2353) done. Loss: 0.0667  lr:0.000100
[ Fri May 17 11:31:41 2024 ] 	Batch(1900/2353) done. Loss: 0.0147  lr:0.000100
[ Fri May 17 11:32:19 2024 ] 	Batch(2000/2353) done. Loss: 0.0276  lr:0.000100
[ Fri May 17 11:32:57 2024 ] 	Batch(2100/2353) done. Loss: 0.0145  lr:0.000100
[ Fri May 17 11:33:36 2024 ] 	Batch(2200/2353) done. Loss: 0.0161  lr:0.000100
[ Fri May 17 11:34:14 2024 ] 	Batch(2300/2353) done. Loss: 0.0039  lr:0.000100
[ Fri May 17 11:34:33 2024 ] 	Mean training loss: 0.0393.
[ Fri May 17 11:34:33 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 11:34:34 2024 ] Training epoch: 86
[ Fri May 17 11:34:34 2024 ] 	Batch(0/2353) done. Loss: 0.0212  lr:0.000100
[ Fri May 17 11:35:12 2024 ] 	Batch(100/2353) done. Loss: 0.0200  lr:0.000100
[ Fri May 17 11:35:49 2024 ] 	Batch(200/2353) done. Loss: 0.0184  lr:0.000100
[ Fri May 17 11:36:26 2024 ] 	Batch(300/2353) done. Loss: 0.0581  lr:0.000100
[ Fri May 17 11:37:04 2024 ] 	Batch(400/2353) done. Loss: 0.0168  lr:0.000100
[ Fri May 17 11:37:41 2024 ] 	Batch(500/2353) done. Loss: 0.0073  lr:0.000100
[ Fri May 17 11:38:19 2024 ] 	Batch(600/2353) done. Loss: 0.0148  lr:0.000100
[ Fri May 17 11:38:56 2024 ] 	Batch(700/2353) done. Loss: 0.0121  lr:0.000100
[ Fri May 17 11:39:34 2024 ] 	Batch(800/2353) done. Loss: 0.1130  lr:0.000100
[ Fri May 17 11:40:12 2024 ] 	Batch(900/2353) done. Loss: 0.0008  lr:0.000100
[ Fri May 17 11:40:50 2024 ] 	Batch(1000/2353) done. Loss: 0.0160  lr:0.000100
[ Fri May 17 11:41:27 2024 ] 	Batch(1100/2353) done. Loss: 0.0146  lr:0.000100
[ Fri May 17 11:42:04 2024 ] 	Batch(1200/2353) done. Loss: 0.0022  lr:0.000100
[ Fri May 17 11:42:42 2024 ] 	Batch(1300/2353) done. Loss: 0.0251  lr:0.000100
[ Fri May 17 11:43:19 2024 ] 	Batch(1400/2353) done. Loss: 0.0128  lr:0.000100
[ Fri May 17 11:43:57 2024 ] 	Batch(1500/2353) done. Loss: 0.0175  lr:0.000100
[ Fri May 17 11:44:34 2024 ] 	Batch(1600/2353) done. Loss: 0.0936  lr:0.000100
[ Fri May 17 11:45:11 2024 ] 	Batch(1700/2353) done. Loss: 0.0502  lr:0.000100
[ Fri May 17 11:45:49 2024 ] 	Batch(1800/2353) done. Loss: 0.0648  lr:0.000100
[ Fri May 17 11:46:26 2024 ] 	Batch(1900/2353) done. Loss: 0.0184  lr:0.000100
[ Fri May 17 11:47:04 2024 ] 	Batch(2000/2353) done. Loss: 0.1314  lr:0.000100
[ Fri May 17 11:47:42 2024 ] 	Batch(2100/2353) done. Loss: 0.0653  lr:0.000100
[ Fri May 17 11:48:20 2024 ] 	Batch(2200/2353) done. Loss: 0.0910  lr:0.000100
[ Fri May 17 11:48:58 2024 ] 	Batch(2300/2353) done. Loss: 0.0092  lr:0.000100
[ Fri May 17 11:49:18 2024 ] 	Mean training loss: 0.0390.
[ Fri May 17 11:49:18 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 11:49:18 2024 ] Training epoch: 87
[ Fri May 17 11:49:19 2024 ] 	Batch(0/2353) done. Loss: 0.0275  lr:0.000100
[ Fri May 17 11:49:56 2024 ] 	Batch(100/2353) done. Loss: 0.0100  lr:0.000100
[ Fri May 17 11:50:33 2024 ] 	Batch(200/2353) done. Loss: 0.0754  lr:0.000100
[ Fri May 17 11:51:11 2024 ] 	Batch(300/2353) done. Loss: 0.0935  lr:0.000100
[ Fri May 17 11:51:48 2024 ] 	Batch(400/2353) done. Loss: 0.0393  lr:0.000100
[ Fri May 17 11:52:25 2024 ] 	Batch(500/2353) done. Loss: 0.0117  lr:0.000100
[ Fri May 17 11:53:03 2024 ] 	Batch(600/2353) done. Loss: 0.0308  lr:0.000100
[ Fri May 17 11:53:40 2024 ] 	Batch(700/2353) done. Loss: 0.0863  lr:0.000100
[ Fri May 17 11:54:18 2024 ] 	Batch(800/2353) done. Loss: 0.0027  lr:0.000100
[ Fri May 17 11:54:55 2024 ] 	Batch(900/2353) done. Loss: 0.0286  lr:0.000100
[ Fri May 17 11:55:32 2024 ] 	Batch(1000/2353) done. Loss: 0.0276  lr:0.000100
[ Fri May 17 11:56:10 2024 ] 	Batch(1100/2353) done. Loss: 0.0095  lr:0.000100
[ Fri May 17 11:56:47 2024 ] 	Batch(1200/2353) done. Loss: 0.0040  lr:0.000100
[ Fri May 17 11:57:25 2024 ] 	Batch(1300/2353) done. Loss: 0.0373  lr:0.000100
[ Fri May 17 11:58:02 2024 ] 	Batch(1400/2353) done. Loss: 0.0169  lr:0.000100
[ Fri May 17 11:58:40 2024 ] 	Batch(1500/2353) done. Loss: 0.0624  lr:0.000100
[ Fri May 17 11:59:17 2024 ] 	Batch(1600/2353) done. Loss: 0.0142  lr:0.000100
[ Fri May 17 11:59:54 2024 ] 	Batch(1700/2353) done. Loss: 0.3329  lr:0.000100
[ Fri May 17 12:00:32 2024 ] 	Batch(1800/2353) done. Loss: 0.0085  lr:0.000100
[ Fri May 17 12:01:10 2024 ] 	Batch(1900/2353) done. Loss: 0.0345  lr:0.000100
[ Fri May 17 12:01:48 2024 ] 	Batch(2000/2353) done. Loss: 0.0121  lr:0.000100
[ Fri May 17 12:02:25 2024 ] 	Batch(2100/2353) done. Loss: 0.0356  lr:0.000100
[ Fri May 17 12:03:03 2024 ] 	Batch(2200/2353) done. Loss: 0.0383  lr:0.000100
[ Fri May 17 12:03:40 2024 ] 	Batch(2300/2353) done. Loss: 0.1547  lr:0.000100
[ Fri May 17 12:03:59 2024 ] 	Mean training loss: 0.0380.
[ Fri May 17 12:03:59 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 12:04:00 2024 ] Training epoch: 88
[ Fri May 17 12:04:00 2024 ] 	Batch(0/2353) done. Loss: 0.0148  lr:0.000100
[ Fri May 17 12:04:38 2024 ] 	Batch(100/2353) done. Loss: 0.1601  lr:0.000100
[ Fri May 17 12:05:15 2024 ] 	Batch(200/2353) done. Loss: 0.0246  lr:0.000100
[ Fri May 17 12:05:52 2024 ] 	Batch(300/2353) done. Loss: 0.0426  lr:0.000100
[ Fri May 17 12:06:30 2024 ] 	Batch(400/2353) done. Loss: 0.0208  lr:0.000100
[ Fri May 17 12:07:07 2024 ] 	Batch(500/2353) done. Loss: 0.0107  lr:0.000100
[ Fri May 17 12:07:44 2024 ] 	Batch(600/2353) done. Loss: 0.1508  lr:0.000100
[ Fri May 17 12:08:22 2024 ] 	Batch(700/2353) done. Loss: 0.0033  lr:0.000100
[ Fri May 17 12:08:59 2024 ] 	Batch(800/2353) done. Loss: 0.1044  lr:0.000100
[ Fri May 17 12:09:37 2024 ] 	Batch(900/2353) done. Loss: 0.0135  lr:0.000100
[ Fri May 17 12:10:14 2024 ] 	Batch(1000/2353) done. Loss: 0.0426  lr:0.000100
[ Fri May 17 12:10:51 2024 ] 	Batch(1100/2353) done. Loss: 0.0274  lr:0.000100
[ Fri May 17 12:11:29 2024 ] 	Batch(1200/2353) done. Loss: 0.0261  lr:0.000100
[ Fri May 17 12:12:06 2024 ] 	Batch(1300/2353) done. Loss: 0.0647  lr:0.000100
[ Fri May 17 12:12:43 2024 ] 	Batch(1400/2353) done. Loss: 0.0131  lr:0.000100
[ Fri May 17 12:13:21 2024 ] 	Batch(1500/2353) done. Loss: 0.0959  lr:0.000100
[ Fri May 17 12:13:58 2024 ] 	Batch(1600/2353) done. Loss: 0.0688  lr:0.000100
[ Fri May 17 12:14:36 2024 ] 	Batch(1700/2353) done. Loss: 0.0482  lr:0.000100
[ Fri May 17 12:15:13 2024 ] 	Batch(1800/2353) done. Loss: 0.0300  lr:0.000100
[ Fri May 17 12:15:50 2024 ] 	Batch(1900/2353) done. Loss: 0.0089  lr:0.000100
[ Fri May 17 12:16:28 2024 ] 	Batch(2000/2353) done. Loss: 0.0088  lr:0.000100
[ Fri May 17 12:17:05 2024 ] 	Batch(2100/2353) done. Loss: 0.1584  lr:0.000100
[ Fri May 17 12:17:42 2024 ] 	Batch(2200/2353) done. Loss: 0.0760  lr:0.000100
[ Fri May 17 12:18:20 2024 ] 	Batch(2300/2353) done. Loss: 0.0418  lr:0.000100
[ Fri May 17 12:18:39 2024 ] 	Mean training loss: 0.0369.
[ Fri May 17 12:18:39 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 12:18:39 2024 ] Training epoch: 89
[ Fri May 17 12:18:40 2024 ] 	Batch(0/2353) done. Loss: 0.0118  lr:0.000100
[ Fri May 17 12:19:18 2024 ] 	Batch(100/2353) done. Loss: 0.0303  lr:0.000100
[ Fri May 17 12:19:55 2024 ] 	Batch(200/2353) done. Loss: 0.0398  lr:0.000100
[ Fri May 17 12:20:32 2024 ] 	Batch(300/2353) done. Loss: 0.0065  lr:0.000100
[ Fri May 17 12:21:10 2024 ] 	Batch(400/2353) done. Loss: 0.0159  lr:0.000100
[ Fri May 17 12:21:47 2024 ] 	Batch(500/2353) done. Loss: 0.0529  lr:0.000100
[ Fri May 17 12:22:24 2024 ] 	Batch(600/2353) done. Loss: 0.0372  lr:0.000100
[ Fri May 17 12:23:02 2024 ] 	Batch(700/2353) done. Loss: 0.0146  lr:0.000100
[ Fri May 17 12:23:39 2024 ] 	Batch(800/2353) done. Loss: 0.0332  lr:0.000100
[ Fri May 17 12:24:17 2024 ] 	Batch(900/2353) done. Loss: 0.0201  lr:0.000100
[ Fri May 17 12:24:54 2024 ] 	Batch(1000/2353) done. Loss: 0.0732  lr:0.000100
[ Fri May 17 12:25:31 2024 ] 	Batch(1100/2353) done. Loss: 0.0297  lr:0.000100
[ Fri May 17 12:26:09 2024 ] 	Batch(1200/2353) done. Loss: 0.0156  lr:0.000100
[ Fri May 17 12:26:46 2024 ] 	Batch(1300/2353) done. Loss: 0.0481  lr:0.000100
[ Fri May 17 12:27:24 2024 ] 	Batch(1400/2353) done. Loss: 0.2481  lr:0.000100
[ Fri May 17 12:28:01 2024 ] 	Batch(1500/2353) done. Loss: 0.0293  lr:0.000100
[ Fri May 17 12:28:39 2024 ] 	Batch(1600/2353) done. Loss: 0.0071  lr:0.000100
[ Fri May 17 12:29:16 2024 ] 	Batch(1700/2353) done. Loss: 0.0276  lr:0.000100
[ Fri May 17 12:29:53 2024 ] 	Batch(1800/2353) done. Loss: 0.0661  lr:0.000100
[ Fri May 17 12:30:31 2024 ] 	Batch(1900/2353) done. Loss: 0.0425  lr:0.000100
[ Fri May 17 12:31:08 2024 ] 	Batch(2000/2353) done. Loss: 0.0124  lr:0.000100
[ Fri May 17 12:31:45 2024 ] 	Batch(2100/2353) done. Loss: 0.0071  lr:0.000100
[ Fri May 17 12:32:23 2024 ] 	Batch(2200/2353) done. Loss: 0.0173  lr:0.000100
[ Fri May 17 12:33:00 2024 ] 	Batch(2300/2353) done. Loss: 0.0164  lr:0.000100
[ Fri May 17 12:33:20 2024 ] 	Mean training loss: 0.0386.
[ Fri May 17 12:33:20 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 12:33:20 2024 ] Training epoch: 90
[ Fri May 17 12:33:20 2024 ] 	Batch(0/2353) done. Loss: 0.0074  lr:0.000100
[ Fri May 17 12:33:58 2024 ] 	Batch(100/2353) done. Loss: 0.0497  lr:0.000100
[ Fri May 17 12:34:35 2024 ] 	Batch(200/2353) done. Loss: 0.0392  lr:0.000100
[ Fri May 17 12:35:12 2024 ] 	Batch(300/2353) done. Loss: 0.0816  lr:0.000100
[ Fri May 17 12:35:50 2024 ] 	Batch(400/2353) done. Loss: 0.0140  lr:0.000100
[ Fri May 17 12:36:27 2024 ] 	Batch(500/2353) done. Loss: 0.0127  lr:0.000100
[ Fri May 17 12:37:04 2024 ] 	Batch(600/2353) done. Loss: 0.0391  lr:0.000100
[ Fri May 17 12:37:42 2024 ] 	Batch(700/2353) done. Loss: 0.0531  lr:0.000100
[ Fri May 17 12:38:19 2024 ] 	Batch(800/2353) done. Loss: 0.0263  lr:0.000100
[ Fri May 17 12:38:57 2024 ] 	Batch(900/2353) done. Loss: 0.0024  lr:0.000100
[ Fri May 17 12:39:34 2024 ] 	Batch(1000/2353) done. Loss: 0.0081  lr:0.000100
[ Fri May 17 12:40:11 2024 ] 	Batch(1100/2353) done. Loss: 0.1095  lr:0.000100
[ Fri May 17 12:40:49 2024 ] 	Batch(1200/2353) done. Loss: 0.0125  lr:0.000100
[ Fri May 17 12:41:27 2024 ] 	Batch(1300/2353) done. Loss: 0.0059  lr:0.000100
[ Fri May 17 12:42:06 2024 ] 	Batch(1400/2353) done. Loss: 0.0378  lr:0.000100
[ Fri May 17 12:42:44 2024 ] 	Batch(1500/2353) done. Loss: 0.0130  lr:0.000100
[ Fri May 17 12:43:23 2024 ] 	Batch(1600/2353) done. Loss: 0.0141  lr:0.000100
[ Fri May 17 12:44:01 2024 ] 	Batch(1700/2353) done. Loss: 0.0684  lr:0.000100
[ Fri May 17 12:44:39 2024 ] 	Batch(1800/2353) done. Loss: 0.0372  lr:0.000100
[ Fri May 17 12:45:17 2024 ] 	Batch(1900/2353) done. Loss: 0.0526  lr:0.000100
[ Fri May 17 12:45:55 2024 ] 	Batch(2000/2353) done. Loss: 0.0117  lr:0.000100
[ Fri May 17 12:46:33 2024 ] 	Batch(2100/2353) done. Loss: 0.0026  lr:0.000100
[ Fri May 17 12:47:11 2024 ] 	Batch(2200/2353) done. Loss: 0.0307  lr:0.000100
[ Fri May 17 12:47:49 2024 ] 	Batch(2300/2353) done. Loss: 0.0083  lr:0.000100
[ Fri May 17 12:48:09 2024 ] 	Mean training loss: 0.0374.
[ Fri May 17 12:48:09 2024 ] 	Time consumption: [Data]01%, [Network]92%
[ Fri May 17 12:48:10 2024 ] Eval epoch: 90
[ Fri May 17 12:50:13 2024 ] 	Mean val loss of 2367 batches: 0.2580354344640523.
[ Fri May 17 12:50:13 2024 ] Training epoch: 91
[ Fri May 17 12:50:13 2024 ] 	Batch(0/2353) done. Loss: 0.1481  lr:0.000001
[ Fri May 17 12:50:51 2024 ] 	Batch(100/2353) done. Loss: 0.0259  lr:0.000001
[ Fri May 17 12:51:29 2024 ] 	Batch(200/2353) done. Loss: 0.0172  lr:0.000001
[ Fri May 17 12:52:07 2024 ] 	Batch(300/2353) done. Loss: 0.0831  lr:0.000001
[ Fri May 17 12:52:44 2024 ] 	Batch(400/2353) done. Loss: 0.0085  lr:0.000001
[ Fri May 17 12:53:22 2024 ] 	Batch(500/2353) done. Loss: 0.0091  lr:0.000001
[ Fri May 17 12:53:59 2024 ] 	Batch(600/2353) done. Loss: 0.0089  lr:0.000001
[ Fri May 17 12:54:36 2024 ] 	Batch(700/2353) done. Loss: 0.0088  lr:0.000001
[ Fri May 17 12:55:14 2024 ] 	Batch(800/2353) done. Loss: 0.0658  lr:0.000001
[ Fri May 17 12:55:51 2024 ] 	Batch(900/2353) done. Loss: 0.0170  lr:0.000001
[ Fri May 17 12:56:28 2024 ] 	Batch(1000/2353) done. Loss: 0.0257  lr:0.000001
[ Fri May 17 12:57:06 2024 ] 	Batch(1100/2353) done. Loss: 0.0236  lr:0.000001
[ Fri May 17 12:57:43 2024 ] 	Batch(1200/2353) done. Loss: 0.0122  lr:0.000001
[ Fri May 17 12:58:20 2024 ] 	Batch(1300/2353) done. Loss: 0.0203  lr:0.000001
[ Fri May 17 12:58:58 2024 ] 	Batch(1400/2353) done. Loss: 0.1314  lr:0.000001
[ Fri May 17 12:59:35 2024 ] 	Batch(1500/2353) done. Loss: 0.0215  lr:0.000001
[ Fri May 17 13:00:12 2024 ] 	Batch(1600/2353) done. Loss: 0.0121  lr:0.000001
[ Fri May 17 13:00:50 2024 ] 	Batch(1700/2353) done. Loss: 0.0710  lr:0.000001
[ Fri May 17 13:01:28 2024 ] 	Batch(1800/2353) done. Loss: 0.1972  lr:0.000001
[ Fri May 17 13:02:05 2024 ] 	Batch(1900/2353) done. Loss: 0.0122  lr:0.000001
[ Fri May 17 13:02:43 2024 ] 	Batch(2000/2353) done. Loss: 0.0337  lr:0.000001
[ Fri May 17 13:03:20 2024 ] 	Batch(2100/2353) done. Loss: 0.0886  lr:0.000001
[ Fri May 17 13:03:57 2024 ] 	Batch(2200/2353) done. Loss: 0.0353  lr:0.000001
[ Fri May 17 13:04:35 2024 ] 	Batch(2300/2353) done. Loss: 0.0620  lr:0.000001
[ Fri May 17 13:04:54 2024 ] 	Mean training loss: 0.0357.
[ Fri May 17 13:04:54 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 13:04:54 2024 ] Training epoch: 92
[ Fri May 17 13:04:55 2024 ] 	Batch(0/2353) done. Loss: 0.0884  lr:0.000001
[ Fri May 17 13:05:33 2024 ] 	Batch(100/2353) done. Loss: 0.0038  lr:0.000001
[ Fri May 17 13:06:10 2024 ] 	Batch(200/2353) done. Loss: 0.0175  lr:0.000001
[ Fri May 17 13:06:47 2024 ] 	Batch(300/2353) done. Loss: 0.0043  lr:0.000001
[ Fri May 17 13:07:25 2024 ] 	Batch(400/2353) done. Loss: 0.0025  lr:0.000001
[ Fri May 17 13:08:02 2024 ] 	Batch(500/2353) done. Loss: 0.0217  lr:0.000001
[ Fri May 17 13:08:39 2024 ] 	Batch(600/2353) done. Loss: 0.0053  lr:0.000001
[ Fri May 17 13:09:17 2024 ] 	Batch(700/2353) done. Loss: 0.0093  lr:0.000001
[ Fri May 17 13:09:54 2024 ] 	Batch(800/2353) done. Loss: 0.0475  lr:0.000001
[ Fri May 17 13:10:31 2024 ] 	Batch(900/2353) done. Loss: 0.0263  lr:0.000001
[ Fri May 17 13:11:09 2024 ] 	Batch(1000/2353) done. Loss: 0.0378  lr:0.000001
[ Fri May 17 13:11:46 2024 ] 	Batch(1100/2353) done. Loss: 0.0212  lr:0.000001
[ Fri May 17 13:12:24 2024 ] 	Batch(1200/2353) done. Loss: 0.0459  lr:0.000001
[ Fri May 17 13:13:01 2024 ] 	Batch(1300/2353) done. Loss: 0.0247  lr:0.000001
[ Fri May 17 13:13:38 2024 ] 	Batch(1400/2353) done. Loss: 0.0458  lr:0.000001
[ Fri May 17 13:14:16 2024 ] 	Batch(1500/2353) done. Loss: 0.0206  lr:0.000001
[ Fri May 17 13:14:53 2024 ] 	Batch(1600/2353) done. Loss: 0.0278  lr:0.000001
[ Fri May 17 13:15:30 2024 ] 	Batch(1700/2353) done. Loss: 0.0371  lr:0.000001
[ Fri May 17 13:16:08 2024 ] 	Batch(1800/2353) done. Loss: 0.0060  lr:0.000001
[ Fri May 17 13:16:45 2024 ] 	Batch(1900/2353) done. Loss: 0.0289  lr:0.000001
[ Fri May 17 13:17:23 2024 ] 	Batch(2000/2353) done. Loss: 0.0136  lr:0.000001
[ Fri May 17 13:18:00 2024 ] 	Batch(2100/2353) done. Loss: 0.0633  lr:0.000001
[ Fri May 17 13:18:37 2024 ] 	Batch(2200/2353) done. Loss: 0.0092  lr:0.000001
[ Fri May 17 13:19:15 2024 ] 	Batch(2300/2353) done. Loss: 0.2718  lr:0.000001
[ Fri May 17 13:19:34 2024 ] 	Mean training loss: 0.0369.
[ Fri May 17 13:19:34 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 13:19:34 2024 ] Training epoch: 93
[ Fri May 17 13:19:35 2024 ] 	Batch(0/2353) done. Loss: 0.0064  lr:0.000001
[ Fri May 17 13:20:12 2024 ] 	Batch(100/2353) done. Loss: 0.0106  lr:0.000001
[ Fri May 17 13:20:50 2024 ] 	Batch(200/2353) done. Loss: 0.0027  lr:0.000001
[ Fri May 17 13:21:27 2024 ] 	Batch(300/2353) done. Loss: 0.0068  lr:0.000001
[ Fri May 17 13:22:04 2024 ] 	Batch(400/2353) done. Loss: 0.1140  lr:0.000001
[ Fri May 17 13:22:42 2024 ] 	Batch(500/2353) done. Loss: 0.0063  lr:0.000001
[ Fri May 17 13:23:19 2024 ] 	Batch(600/2353) done. Loss: 0.0030  lr:0.000001
[ Fri May 17 13:23:57 2024 ] 	Batch(700/2353) done. Loss: 0.0169  lr:0.000001
[ Fri May 17 13:24:34 2024 ] 	Batch(800/2353) done. Loss: 0.0069  lr:0.000001
[ Fri May 17 13:25:11 2024 ] 	Batch(900/2353) done. Loss: 0.0157  lr:0.000001
[ Fri May 17 13:25:49 2024 ] 	Batch(1000/2353) done. Loss: 0.0262  lr:0.000001
[ Fri May 17 13:26:26 2024 ] 	Batch(1100/2353) done. Loss: 0.0405  lr:0.000001
[ Fri May 17 13:27:03 2024 ] 	Batch(1200/2353) done. Loss: 0.0557  lr:0.000001
[ Fri May 17 13:27:41 2024 ] 	Batch(1300/2353) done. Loss: 0.0051  lr:0.000001
[ Fri May 17 13:28:18 2024 ] 	Batch(1400/2353) done. Loss: 0.0202  lr:0.000001
[ Fri May 17 13:28:56 2024 ] 	Batch(1500/2353) done. Loss: 0.0290  lr:0.000001
[ Fri May 17 13:29:33 2024 ] 	Batch(1600/2353) done. Loss: 0.0884  lr:0.000001
[ Fri May 17 13:30:10 2024 ] 	Batch(1700/2353) done. Loss: 0.0690  lr:0.000001
[ Fri May 17 13:30:48 2024 ] 	Batch(1800/2353) done. Loss: 0.0455  lr:0.000001
[ Fri May 17 13:31:25 2024 ] 	Batch(1900/2353) done. Loss: 0.0090  lr:0.000001
[ Fri May 17 13:32:03 2024 ] 	Batch(2000/2353) done. Loss: 0.0073  lr:0.000001
[ Fri May 17 13:32:41 2024 ] 	Batch(2100/2353) done. Loss: 0.1104  lr:0.000001
[ Fri May 17 13:33:19 2024 ] 	Batch(2200/2353) done. Loss: 0.0057  lr:0.000001
[ Fri May 17 13:33:57 2024 ] 	Batch(2300/2353) done. Loss: 0.0645  lr:0.000001
[ Fri May 17 13:34:17 2024 ] 	Mean training loss: 0.0379.
[ Fri May 17 13:34:17 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 13:34:17 2024 ] Training epoch: 94
[ Fri May 17 13:34:18 2024 ] 	Batch(0/2353) done. Loss: 0.0262  lr:0.000001
[ Fri May 17 13:34:55 2024 ] 	Batch(100/2353) done. Loss: 0.0056  lr:0.000001
[ Fri May 17 13:35:33 2024 ] 	Batch(200/2353) done. Loss: 0.0377  lr:0.000001
[ Fri May 17 13:36:10 2024 ] 	Batch(300/2353) done. Loss: 0.0019  lr:0.000001
[ Fri May 17 13:36:47 2024 ] 	Batch(400/2353) done. Loss: 0.0502  lr:0.000001
[ Fri May 17 13:37:25 2024 ] 	Batch(500/2353) done. Loss: 0.0026  lr:0.000001
[ Fri May 17 13:38:02 2024 ] 	Batch(600/2353) done. Loss: 0.0184  lr:0.000001
[ Fri May 17 13:38:40 2024 ] 	Batch(700/2353) done. Loss: 0.0096  lr:0.000001
[ Fri May 17 13:39:17 2024 ] 	Batch(800/2353) done. Loss: 0.0059  lr:0.000001
[ Fri May 17 13:39:55 2024 ] 	Batch(900/2353) done. Loss: 0.0133  lr:0.000001
[ Fri May 17 13:40:32 2024 ] 	Batch(1000/2353) done. Loss: 0.0134  lr:0.000001
[ Fri May 17 13:41:09 2024 ] 	Batch(1100/2353) done. Loss: 0.0061  lr:0.000001
[ Fri May 17 13:41:47 2024 ] 	Batch(1200/2353) done. Loss: 0.0277  lr:0.000001
[ Fri May 17 13:42:24 2024 ] 	Batch(1300/2353) done. Loss: 0.0165  lr:0.000001
[ Fri May 17 13:43:01 2024 ] 	Batch(1400/2353) done. Loss: 0.0041  lr:0.000001
[ Fri May 17 13:43:39 2024 ] 	Batch(1500/2353) done. Loss: 0.1699  lr:0.000001
[ Fri May 17 13:44:16 2024 ] 	Batch(1600/2353) done. Loss: 0.0251  lr:0.000001
[ Fri May 17 13:44:54 2024 ] 	Batch(1700/2353) done. Loss: 0.0875  lr:0.000001
[ Fri May 17 13:45:32 2024 ] 	Batch(1800/2353) done. Loss: 0.0686  lr:0.000001
[ Fri May 17 13:46:10 2024 ] 	Batch(1900/2353) done. Loss: 0.0855  lr:0.000001
[ Fri May 17 13:46:49 2024 ] 	Batch(2000/2353) done. Loss: 0.0071  lr:0.000001
[ Fri May 17 13:47:27 2024 ] 	Batch(2100/2353) done. Loss: 0.0526  lr:0.000001
[ Fri May 17 13:48:05 2024 ] 	Batch(2200/2353) done. Loss: 0.0072  lr:0.000001
[ Fri May 17 13:48:43 2024 ] 	Batch(2300/2353) done. Loss: 0.0095  lr:0.000001
[ Fri May 17 13:49:03 2024 ] 	Mean training loss: 0.0373.
[ Fri May 17 13:49:03 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 13:49:03 2024 ] Training epoch: 95
[ Fri May 17 13:49:04 2024 ] 	Batch(0/2353) done. Loss: 0.0054  lr:0.000001
[ Fri May 17 13:49:41 2024 ] 	Batch(100/2353) done. Loss: 0.0088  lr:0.000001
[ Fri May 17 13:50:18 2024 ] 	Batch(200/2353) done. Loss: 0.0226  lr:0.000001
[ Fri May 17 13:50:56 2024 ] 	Batch(300/2353) done. Loss: 0.0493  lr:0.000001
[ Fri May 17 13:51:33 2024 ] 	Batch(400/2353) done. Loss: 0.0106  lr:0.000001
[ Fri May 17 13:52:10 2024 ] 	Batch(500/2353) done. Loss: 0.1023  lr:0.000001
[ Fri May 17 13:52:48 2024 ] 	Batch(600/2353) done. Loss: 0.0043  lr:0.000001
[ Fri May 17 13:53:25 2024 ] 	Batch(700/2353) done. Loss: 0.0056  lr:0.000001
[ Fri May 17 13:54:03 2024 ] 	Batch(800/2353) done. Loss: 0.0626  lr:0.000001
[ Fri May 17 13:54:40 2024 ] 	Batch(900/2353) done. Loss: 0.0057  lr:0.000001
[ Fri May 17 13:55:17 2024 ] 	Batch(1000/2353) done. Loss: 0.0246  lr:0.000001
[ Fri May 17 13:55:55 2024 ] 	Batch(1100/2353) done. Loss: 0.0340  lr:0.000001
[ Fri May 17 13:56:32 2024 ] 	Batch(1200/2353) done. Loss: 0.0605  lr:0.000001
[ Fri May 17 13:57:09 2024 ] 	Batch(1300/2353) done. Loss: 0.0141  lr:0.000001
[ Fri May 17 13:57:47 2024 ] 	Batch(1400/2353) done. Loss: 0.0170  lr:0.000001
[ Fri May 17 13:58:24 2024 ] 	Batch(1500/2353) done. Loss: 0.0415  lr:0.000001
[ Fri May 17 13:59:02 2024 ] 	Batch(1600/2353) done. Loss: 0.0355  lr:0.000001
[ Fri May 17 13:59:39 2024 ] 	Batch(1700/2353) done. Loss: 0.0270  lr:0.000001
[ Fri May 17 14:00:16 2024 ] 	Batch(1800/2353) done. Loss: 0.0080  lr:0.000001
[ Fri May 17 14:00:54 2024 ] 	Batch(1900/2353) done. Loss: 0.0030  lr:0.000001
[ Fri May 17 14:01:31 2024 ] 	Batch(2000/2353) done. Loss: 0.0115  lr:0.000001
[ Fri May 17 14:02:08 2024 ] 	Batch(2100/2353) done. Loss: 0.0269  lr:0.000001
[ Fri May 17 14:02:46 2024 ] 	Batch(2200/2353) done. Loss: 0.0576  lr:0.000001
[ Fri May 17 14:03:23 2024 ] 	Batch(2300/2353) done. Loss: 0.0173  lr:0.000001
[ Fri May 17 14:03:43 2024 ] 	Mean training loss: 0.0349.
[ Fri May 17 14:03:43 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 14:03:43 2024 ] Training epoch: 96
[ Fri May 17 14:03:43 2024 ] 	Batch(0/2353) done. Loss: 0.1211  lr:0.000001
[ Fri May 17 14:04:21 2024 ] 	Batch(100/2353) done. Loss: 0.0211  lr:0.000001
[ Fri May 17 14:04:59 2024 ] 	Batch(200/2353) done. Loss: 0.0519  lr:0.000001
[ Fri May 17 14:05:37 2024 ] 	Batch(300/2353) done. Loss: 0.0125  lr:0.000001
[ Fri May 17 14:06:16 2024 ] 	Batch(400/2353) done. Loss: 0.0392  lr:0.000001
[ Fri May 17 14:06:54 2024 ] 	Batch(500/2353) done. Loss: 0.0014  lr:0.000001
[ Fri May 17 14:07:32 2024 ] 	Batch(600/2353) done. Loss: 0.0060  lr:0.000001
[ Fri May 17 14:08:10 2024 ] 	Batch(700/2353) done. Loss: 0.0055  lr:0.000001
[ Fri May 17 14:08:48 2024 ] 	Batch(800/2353) done. Loss: 0.0067  lr:0.000001
[ Fri May 17 14:09:25 2024 ] 	Batch(900/2353) done. Loss: 0.0173  lr:0.000001
[ Fri May 17 14:10:03 2024 ] 	Batch(1000/2353) done. Loss: 0.2472  lr:0.000001
[ Fri May 17 14:10:40 2024 ] 	Batch(1100/2353) done. Loss: 0.0064  lr:0.000001
[ Fri May 17 14:11:18 2024 ] 	Batch(1200/2353) done. Loss: 0.0714  lr:0.000001
[ Fri May 17 14:11:55 2024 ] 	Batch(1300/2353) done. Loss: 0.0295  lr:0.000001
[ Fri May 17 14:12:32 2024 ] 	Batch(1400/2353) done. Loss: 0.0023  lr:0.000001
[ Fri May 17 14:13:10 2024 ] 	Batch(1500/2353) done. Loss: 0.0280  lr:0.000001
[ Fri May 17 14:13:47 2024 ] 	Batch(1600/2353) done. Loss: 0.0114  lr:0.000001
[ Fri May 17 14:14:25 2024 ] 	Batch(1700/2353) done. Loss: 0.0158  lr:0.000001
[ Fri May 17 14:15:03 2024 ] 	Batch(1800/2353) done. Loss: 0.0071  lr:0.000001
[ Fri May 17 14:15:42 2024 ] 	Batch(1900/2353) done. Loss: 0.0095  lr:0.000001
[ Fri May 17 14:16:20 2024 ] 	Batch(2000/2353) done. Loss: 0.0140  lr:0.000001
[ Fri May 17 14:16:57 2024 ] 	Batch(2100/2353) done. Loss: 0.0118  lr:0.000001
[ Fri May 17 14:17:35 2024 ] 	Batch(2200/2353) done. Loss: 0.1058  lr:0.000001
[ Fri May 17 14:18:12 2024 ] 	Batch(2300/2353) done. Loss: 0.0033  lr:0.000001
[ Fri May 17 14:18:31 2024 ] 	Mean training loss: 0.0376.
[ Fri May 17 14:18:31 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 14:18:32 2024 ] Training epoch: 97
[ Fri May 17 14:18:32 2024 ] 	Batch(0/2353) done. Loss: 0.0218  lr:0.000001
[ Fri May 17 14:19:10 2024 ] 	Batch(100/2353) done. Loss: 0.1438  lr:0.000001
[ Fri May 17 14:19:47 2024 ] 	Batch(200/2353) done. Loss: 0.0322  lr:0.000001
[ Fri May 17 14:20:24 2024 ] 	Batch(300/2353) done. Loss: 0.0122  lr:0.000001
[ Fri May 17 14:21:02 2024 ] 	Batch(400/2353) done. Loss: 0.0496  lr:0.000001
[ Fri May 17 14:21:39 2024 ] 	Batch(500/2353) done. Loss: 0.0066  lr:0.000001
[ Fri May 17 14:22:16 2024 ] 	Batch(600/2353) done. Loss: 0.0565  lr:0.000001
[ Fri May 17 14:22:54 2024 ] 	Batch(700/2353) done. Loss: 0.0242  lr:0.000001
[ Fri May 17 14:23:31 2024 ] 	Batch(800/2353) done. Loss: 0.0390  lr:0.000001
[ Fri May 17 14:24:09 2024 ] 	Batch(900/2353) done. Loss: 0.0848  lr:0.000001
[ Fri May 17 14:24:46 2024 ] 	Batch(1000/2353) done. Loss: 0.0056  lr:0.000001
[ Fri May 17 14:25:23 2024 ] 	Batch(1100/2353) done. Loss: 0.0031  lr:0.000001
[ Fri May 17 14:26:01 2024 ] 	Batch(1200/2353) done. Loss: 0.0172  lr:0.000001
[ Fri May 17 14:26:38 2024 ] 	Batch(1300/2353) done. Loss: 0.0052  lr:0.000001
[ Fri May 17 14:27:16 2024 ] 	Batch(1400/2353) done. Loss: 0.0090  lr:0.000001
[ Fri May 17 14:27:54 2024 ] 	Batch(1500/2353) done. Loss: 0.0145  lr:0.000001
[ Fri May 17 14:28:31 2024 ] 	Batch(1600/2353) done. Loss: 0.0256  lr:0.000001
[ Fri May 17 14:29:09 2024 ] 	Batch(1700/2353) done. Loss: 0.0634  lr:0.000001
[ Fri May 17 14:29:46 2024 ] 	Batch(1800/2353) done. Loss: 0.0825  lr:0.000001
[ Fri May 17 14:30:24 2024 ] 	Batch(1900/2353) done. Loss: 0.0024  lr:0.000001
[ Fri May 17 14:31:01 2024 ] 	Batch(2000/2353) done. Loss: 0.0304  lr:0.000001
[ Fri May 17 14:31:39 2024 ] 	Batch(2100/2353) done. Loss: 0.0127  lr:0.000001
[ Fri May 17 14:32:16 2024 ] 	Batch(2200/2353) done. Loss: 0.0203  lr:0.000001
[ Fri May 17 14:32:53 2024 ] 	Batch(2300/2353) done. Loss: 0.0287  lr:0.000001
[ Fri May 17 14:33:13 2024 ] 	Mean training loss: 0.0353.
[ Fri May 17 14:33:13 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 14:33:13 2024 ] Training epoch: 98
[ Fri May 17 14:33:14 2024 ] 	Batch(0/2353) done. Loss: 0.0024  lr:0.000001
[ Fri May 17 14:33:51 2024 ] 	Batch(100/2353) done. Loss: 0.0111  lr:0.000001
[ Fri May 17 14:34:28 2024 ] 	Batch(200/2353) done. Loss: 0.0140  lr:0.000001
[ Fri May 17 14:35:06 2024 ] 	Batch(300/2353) done. Loss: 0.0375  lr:0.000001
[ Fri May 17 14:35:43 2024 ] 	Batch(400/2353) done. Loss: 0.0675  lr:0.000001
[ Fri May 17 14:36:21 2024 ] 	Batch(500/2353) done. Loss: 0.0337  lr:0.000001
[ Fri May 17 14:36:58 2024 ] 	Batch(600/2353) done. Loss: 0.0012  lr:0.000001
[ Fri May 17 14:37:35 2024 ] 	Batch(700/2353) done. Loss: 0.0151  lr:0.000001
[ Fri May 17 14:38:13 2024 ] 	Batch(800/2353) done. Loss: 0.0036  lr:0.000001
[ Fri May 17 14:38:50 2024 ] 	Batch(900/2353) done. Loss: 0.1180  lr:0.000001
[ Fri May 17 14:39:28 2024 ] 	Batch(1000/2353) done. Loss: 0.0119  lr:0.000001
[ Fri May 17 14:40:05 2024 ] 	Batch(1100/2353) done. Loss: 0.0091  lr:0.000001
[ Fri May 17 14:40:43 2024 ] 	Batch(1200/2353) done. Loss: 0.0173  lr:0.000001
[ Fri May 17 14:41:21 2024 ] 	Batch(1300/2353) done. Loss: 0.0028  lr:0.000001
[ Fri May 17 14:41:58 2024 ] 	Batch(1400/2353) done. Loss: 0.0146  lr:0.000001
[ Fri May 17 14:42:35 2024 ] 	Batch(1500/2353) done. Loss: 0.3192  lr:0.000001
[ Fri May 17 14:43:13 2024 ] 	Batch(1600/2353) done. Loss: 0.0115  lr:0.000001
[ Fri May 17 14:43:51 2024 ] 	Batch(1700/2353) done. Loss: 0.0038  lr:0.000001
[ Fri May 17 14:44:29 2024 ] 	Batch(1800/2353) done. Loss: 0.1055  lr:0.000001
[ Fri May 17 14:45:07 2024 ] 	Batch(1900/2353) done. Loss: 0.0323  lr:0.000001
[ Fri May 17 14:45:46 2024 ] 	Batch(2000/2353) done. Loss: 0.0222  lr:0.000001
[ Fri May 17 14:46:24 2024 ] 	Batch(2100/2353) done. Loss: 0.1147  lr:0.000001
[ Fri May 17 14:47:02 2024 ] 	Batch(2200/2353) done. Loss: 0.0169  lr:0.000001
[ Fri May 17 14:47:40 2024 ] 	Batch(2300/2353) done. Loss: 0.0374  lr:0.000001
[ Fri May 17 14:48:00 2024 ] 	Mean training loss: 0.0373.
[ Fri May 17 14:48:00 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 14:48:00 2024 ] Training epoch: 99
[ Fri May 17 14:48:00 2024 ] 	Batch(0/2353) done. Loss: 0.1151  lr:0.000001
[ Fri May 17 14:48:38 2024 ] 	Batch(100/2353) done. Loss: 0.0226  lr:0.000001
[ Fri May 17 14:49:15 2024 ] 	Batch(200/2353) done. Loss: 0.0208  lr:0.000001
[ Fri May 17 14:49:52 2024 ] 	Batch(300/2353) done. Loss: 0.0206  lr:0.000001
[ Fri May 17 14:50:30 2024 ] 	Batch(400/2353) done. Loss: 0.0388  lr:0.000001
[ Fri May 17 14:51:07 2024 ] 	Batch(500/2353) done. Loss: 0.0082  lr:0.000001
[ Fri May 17 14:51:45 2024 ] 	Batch(600/2353) done. Loss: 0.0018  lr:0.000001
[ Fri May 17 14:52:22 2024 ] 	Batch(700/2353) done. Loss: 0.0032  lr:0.000001
[ Fri May 17 14:52:59 2024 ] 	Batch(800/2353) done. Loss: 0.0376  lr:0.000001
[ Fri May 17 14:53:37 2024 ] 	Batch(900/2353) done. Loss: 0.0187  lr:0.000001
[ Fri May 17 14:54:14 2024 ] 	Batch(1000/2353) done. Loss: 0.0016  lr:0.000001
[ Fri May 17 14:54:52 2024 ] 	Batch(1100/2353) done. Loss: 0.0169  lr:0.000001
[ Fri May 17 14:55:29 2024 ] 	Batch(1200/2353) done. Loss: 0.0978  lr:0.000001
[ Fri May 17 14:56:07 2024 ] 	Batch(1300/2353) done. Loss: 0.0055  lr:0.000001
[ Fri May 17 14:56:44 2024 ] 	Batch(1400/2353) done. Loss: 0.0098  lr:0.000001
[ Fri May 17 14:57:21 2024 ] 	Batch(1500/2353) done. Loss: 0.0355  lr:0.000001
[ Fri May 17 14:57:59 2024 ] 	Batch(1600/2353) done. Loss: 0.0316  lr:0.000001
[ Fri May 17 14:58:36 2024 ] 	Batch(1700/2353) done. Loss: 0.0338  lr:0.000001
[ Fri May 17 14:59:13 2024 ] 	Batch(1800/2353) done. Loss: 0.0515  lr:0.000001
[ Fri May 17 14:59:51 2024 ] 	Batch(1900/2353) done. Loss: 0.1074  lr:0.000001
[ Fri May 17 15:00:28 2024 ] 	Batch(2000/2353) done. Loss: 0.0243  lr:0.000001
[ Fri May 17 15:01:06 2024 ] 	Batch(2100/2353) done. Loss: 0.0637  lr:0.000001
[ Fri May 17 15:01:43 2024 ] 	Batch(2200/2353) done. Loss: 0.0093  lr:0.000001
[ Fri May 17 15:02:21 2024 ] 	Batch(2300/2353) done. Loss: 0.0126  lr:0.000001
[ Fri May 17 15:02:40 2024 ] 	Mean training loss: 0.0351.
[ Fri May 17 15:02:40 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 15:02:40 2024 ] Training epoch: 100
[ Fri May 17 15:02:41 2024 ] 	Batch(0/2353) done. Loss: 0.0127  lr:0.000001
[ Fri May 17 15:03:19 2024 ] 	Batch(100/2353) done. Loss: 0.2807  lr:0.000001
[ Fri May 17 15:03:56 2024 ] 	Batch(200/2353) done. Loss: 0.0482  lr:0.000001
[ Fri May 17 15:04:33 2024 ] 	Batch(300/2353) done. Loss: 0.0069  lr:0.000001
[ Fri May 17 15:05:11 2024 ] 	Batch(400/2353) done. Loss: 0.0042  lr:0.000001
[ Fri May 17 15:05:48 2024 ] 	Batch(500/2353) done. Loss: 0.0058  lr:0.000001
[ Fri May 17 15:06:26 2024 ] 	Batch(600/2353) done. Loss: 0.0219  lr:0.000001
[ Fri May 17 15:07:03 2024 ] 	Batch(700/2353) done. Loss: 0.0034  lr:0.000001
[ Fri May 17 15:07:40 2024 ] 	Batch(800/2353) done. Loss: 0.0550  lr:0.000001
[ Fri May 17 15:08:18 2024 ] 	Batch(900/2353) done. Loss: 0.0495  lr:0.000001
[ Fri May 17 15:08:55 2024 ] 	Batch(1000/2353) done. Loss: 0.0086  lr:0.000001
[ Fri May 17 15:09:33 2024 ] 	Batch(1100/2353) done. Loss: 0.0147  lr:0.000001
[ Fri May 17 15:10:10 2024 ] 	Batch(1200/2353) done. Loss: 0.0107  lr:0.000001
[ Fri May 17 15:10:47 2024 ] 	Batch(1300/2353) done. Loss: 0.3015  lr:0.000001
[ Fri May 17 15:11:25 2024 ] 	Batch(1400/2353) done. Loss: 0.0418  lr:0.000001
[ Fri May 17 15:12:02 2024 ] 	Batch(1500/2353) done. Loss: 0.0828  lr:0.000001
[ Fri May 17 15:12:40 2024 ] 	Batch(1600/2353) done. Loss: 0.0075  lr:0.000001
[ Fri May 17 15:13:17 2024 ] 	Batch(1700/2353) done. Loss: 0.0098  lr:0.000001
[ Fri May 17 15:13:54 2024 ] 	Batch(1800/2353) done. Loss: 0.0285  lr:0.000001
[ Fri May 17 15:14:32 2024 ] 	Batch(1900/2353) done. Loss: 0.0119  lr:0.000001
[ Fri May 17 15:15:09 2024 ] 	Batch(2000/2353) done. Loss: 0.0171  lr:0.000001
[ Fri May 17 15:15:47 2024 ] 	Batch(2100/2353) done. Loss: 0.0165  lr:0.000001
[ Fri May 17 15:16:25 2024 ] 	Batch(2200/2353) done. Loss: 0.0926  lr:0.000001
[ Fri May 17 15:17:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0923  lr:0.000001
[ Fri May 17 15:17:23 2024 ] 	Mean training loss: 0.0366.
[ Fri May 17 15:17:23 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 15:17:24 2024 ] Eval epoch: 100
[ Fri May 17 15:19:27 2024 ] 	Mean val loss of 2367 batches: 0.24518129939785874.
[ Fri May 17 15:19:27 2024 ] Training epoch: 101
[ Fri May 17 15:19:27 2024 ] 	Batch(0/2353) done. Loss: 0.0012  lr:0.000001
[ Fri May 17 15:20:05 2024 ] 	Batch(100/2353) done. Loss: 0.0063  lr:0.000001
[ Fri May 17 15:20:43 2024 ] 	Batch(200/2353) done. Loss: 0.0026  lr:0.000001
[ Fri May 17 15:21:20 2024 ] 	Batch(300/2353) done. Loss: 0.0640  lr:0.000001
[ Fri May 17 15:21:58 2024 ] 	Batch(400/2353) done. Loss: 0.0224  lr:0.000001
[ Fri May 17 15:22:35 2024 ] 	Batch(500/2353) done. Loss: 0.0061  lr:0.000001
[ Fri May 17 15:23:12 2024 ] 	Batch(600/2353) done. Loss: 0.0172  lr:0.000001
[ Fri May 17 15:23:50 2024 ] 	Batch(700/2353) done. Loss: 0.0016  lr:0.000001
[ Fri May 17 15:24:27 2024 ] 	Batch(800/2353) done. Loss: 0.0093  lr:0.000001
[ Fri May 17 15:25:04 2024 ] 	Batch(900/2353) done. Loss: 0.0142  lr:0.000001
[ Fri May 17 15:25:42 2024 ] 	Batch(1000/2353) done. Loss: 0.0753  lr:0.000001
[ Fri May 17 15:26:19 2024 ] 	Batch(1100/2353) done. Loss: 0.0567  lr:0.000001
[ Fri May 17 15:26:57 2024 ] 	Batch(1200/2353) done. Loss: 0.0052  lr:0.000001
[ Fri May 17 15:27:34 2024 ] 	Batch(1300/2353) done. Loss: 0.0053  lr:0.000001
[ Fri May 17 15:28:11 2024 ] 	Batch(1400/2353) done. Loss: 0.0202  lr:0.000001
[ Fri May 17 15:28:49 2024 ] 	Batch(1500/2353) done. Loss: 0.0219  lr:0.000001
[ Fri May 17 15:29:26 2024 ] 	Batch(1600/2353) done. Loss: 0.0052  lr:0.000001
[ Fri May 17 15:30:04 2024 ] 	Batch(1700/2353) done. Loss: 0.0256  lr:0.000001
[ Fri May 17 15:30:41 2024 ] 	Batch(1800/2353) done. Loss: 0.0046  lr:0.000001
[ Fri May 17 15:31:18 2024 ] 	Batch(1900/2353) done. Loss: 0.0613  lr:0.000001
[ Fri May 17 15:31:56 2024 ] 	Batch(2000/2353) done. Loss: 0.0531  lr:0.000001
[ Fri May 17 15:32:33 2024 ] 	Batch(2100/2353) done. Loss: 0.0178  lr:0.000001
[ Fri May 17 15:33:11 2024 ] 	Batch(2200/2353) done. Loss: 0.0158  lr:0.000001
[ Fri May 17 15:33:48 2024 ] 	Batch(2300/2353) done. Loss: 0.0273  lr:0.000001
[ Fri May 17 15:34:07 2024 ] 	Mean training loss: 0.0367.
[ Fri May 17 15:34:07 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 15:34:08 2024 ] Training epoch: 102
[ Fri May 17 15:34:08 2024 ] 	Batch(0/2353) done. Loss: 0.0352  lr:0.000001
[ Fri May 17 15:34:46 2024 ] 	Batch(100/2353) done. Loss: 0.1071  lr:0.000001
[ Fri May 17 15:35:23 2024 ] 	Batch(200/2353) done. Loss: 0.0324  lr:0.000001
[ Fri May 17 15:36:00 2024 ] 	Batch(300/2353) done. Loss: 0.0581  lr:0.000001
[ Fri May 17 15:36:38 2024 ] 	Batch(400/2353) done. Loss: 0.0202  lr:0.000001
[ Fri May 17 15:37:15 2024 ] 	Batch(500/2353) done. Loss: 0.0316  lr:0.000001
[ Fri May 17 15:37:52 2024 ] 	Batch(600/2353) done. Loss: 0.0108  lr:0.000001
[ Fri May 17 15:38:30 2024 ] 	Batch(700/2353) done. Loss: 0.0319  lr:0.000001
[ Fri May 17 15:39:07 2024 ] 	Batch(800/2353) done. Loss: 0.0120  lr:0.000001
[ Fri May 17 15:39:44 2024 ] 	Batch(900/2353) done. Loss: 0.0381  lr:0.000001
[ Fri May 17 15:40:22 2024 ] 	Batch(1000/2353) done. Loss: 0.0079  lr:0.000001
[ Fri May 17 15:40:59 2024 ] 	Batch(1100/2353) done. Loss: 0.0114  lr:0.000001
[ Fri May 17 15:41:37 2024 ] 	Batch(1200/2353) done. Loss: 0.0231  lr:0.000001
[ Fri May 17 15:42:14 2024 ] 	Batch(1300/2353) done. Loss: 0.0025  lr:0.000001
[ Fri May 17 15:42:51 2024 ] 	Batch(1400/2353) done. Loss: 0.0040  lr:0.000001
[ Fri May 17 15:43:29 2024 ] 	Batch(1500/2353) done. Loss: 0.0441  lr:0.000001
[ Fri May 17 15:44:06 2024 ] 	Batch(1600/2353) done. Loss: 0.1787  lr:0.000001
[ Fri May 17 15:44:44 2024 ] 	Batch(1700/2353) done. Loss: 0.0148  lr:0.000001
[ Fri May 17 15:45:22 2024 ] 	Batch(1800/2353) done. Loss: 0.0132  lr:0.000001
[ Fri May 17 15:46:00 2024 ] 	Batch(1900/2353) done. Loss: 0.0703  lr:0.000001
[ Fri May 17 15:46:39 2024 ] 	Batch(2000/2353) done. Loss: 0.0113  lr:0.000001
[ Fri May 17 15:47:16 2024 ] 	Batch(2100/2353) done. Loss: 0.0068  lr:0.000001
[ Fri May 17 15:47:53 2024 ] 	Batch(2200/2353) done. Loss: 0.0641  lr:0.000001
[ Fri May 17 15:48:31 2024 ] 	Batch(2300/2353) done. Loss: 0.0050  lr:0.000001
[ Fri May 17 15:48:50 2024 ] 	Mean training loss: 0.0372.
[ Fri May 17 15:48:50 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 15:48:50 2024 ] Training epoch: 103
[ Fri May 17 15:48:51 2024 ] 	Batch(0/2353) done. Loss: 0.0080  lr:0.000001
[ Fri May 17 15:49:28 2024 ] 	Batch(100/2353) done. Loss: 0.0110  lr:0.000001
[ Fri May 17 15:50:06 2024 ] 	Batch(200/2353) done. Loss: 0.0038  lr:0.000001
[ Fri May 17 15:50:43 2024 ] 	Batch(300/2353) done. Loss: 0.0234  lr:0.000001
[ Fri May 17 15:51:20 2024 ] 	Batch(400/2353) done. Loss: 0.0291  lr:0.000001
[ Fri May 17 15:51:58 2024 ] 	Batch(500/2353) done. Loss: 0.1119  lr:0.000001
[ Fri May 17 15:52:35 2024 ] 	Batch(600/2353) done. Loss: 0.0274  lr:0.000001
[ Fri May 17 15:53:13 2024 ] 	Batch(700/2353) done. Loss: 0.0194  lr:0.000001
[ Fri May 17 15:53:50 2024 ] 	Batch(800/2353) done. Loss: 0.0217  lr:0.000001
[ Fri May 17 15:54:27 2024 ] 	Batch(900/2353) done. Loss: 0.0061  lr:0.000001
[ Fri May 17 15:55:05 2024 ] 	Batch(1000/2353) done. Loss: 0.0113  lr:0.000001
[ Fri May 17 15:55:42 2024 ] 	Batch(1100/2353) done. Loss: 0.0161  lr:0.000001
[ Fri May 17 15:56:19 2024 ] 	Batch(1200/2353) done. Loss: 0.1330  lr:0.000001
[ Fri May 17 15:56:57 2024 ] 	Batch(1300/2353) done. Loss: 0.0125  lr:0.000001
[ Fri May 17 15:57:34 2024 ] 	Batch(1400/2353) done. Loss: 0.0381  lr:0.000001
[ Fri May 17 15:58:11 2024 ] 	Batch(1500/2353) done. Loss: 0.0143  lr:0.000001
[ Fri May 17 15:58:49 2024 ] 	Batch(1600/2353) done. Loss: 0.0024  lr:0.000001
[ Fri May 17 15:59:26 2024 ] 	Batch(1700/2353) done. Loss: 0.0090  lr:0.000001
[ Fri May 17 16:00:03 2024 ] 	Batch(1800/2353) done. Loss: 0.0137  lr:0.000001
[ Fri May 17 16:00:41 2024 ] 	Batch(1900/2353) done. Loss: 0.0028  lr:0.000001
[ Fri May 17 16:01:18 2024 ] 	Batch(2000/2353) done. Loss: 0.0664  lr:0.000001
[ Fri May 17 16:01:56 2024 ] 	Batch(2100/2353) done. Loss: 0.0065  lr:0.000001
[ Fri May 17 16:02:34 2024 ] 	Batch(2200/2353) done. Loss: 0.0425  lr:0.000001
[ Fri May 17 16:03:12 2024 ] 	Batch(2300/2353) done. Loss: 0.0273  lr:0.000001
[ Fri May 17 16:03:32 2024 ] 	Mean training loss: 0.0381.
[ Fri May 17 16:03:32 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 16:03:32 2024 ] Training epoch: 104
[ Fri May 17 16:03:33 2024 ] 	Batch(0/2353) done. Loss: 0.0457  lr:0.000001
[ Fri May 17 16:04:10 2024 ] 	Batch(100/2353) done. Loss: 0.0084  lr:0.000001
[ Fri May 17 16:04:47 2024 ] 	Batch(200/2353) done. Loss: 0.0079  lr:0.000001
[ Fri May 17 16:05:25 2024 ] 	Batch(300/2353) done. Loss: 0.0558  lr:0.000001
[ Fri May 17 16:06:02 2024 ] 	Batch(400/2353) done. Loss: 0.0159  lr:0.000001
[ Fri May 17 16:06:39 2024 ] 	Batch(500/2353) done. Loss: 0.0340  lr:0.000001
[ Fri May 17 16:07:17 2024 ] 	Batch(600/2353) done. Loss: 0.0101  lr:0.000001
[ Fri May 17 16:07:54 2024 ] 	Batch(700/2353) done. Loss: 0.0427  lr:0.000001
[ Fri May 17 16:08:32 2024 ] 	Batch(800/2353) done. Loss: 0.0162  lr:0.000001
[ Fri May 17 16:09:09 2024 ] 	Batch(900/2353) done. Loss: 0.0058  lr:0.000001
[ Fri May 17 16:09:46 2024 ] 	Batch(1000/2353) done. Loss: 0.0019  lr:0.000001
[ Fri May 17 16:10:23 2024 ] 	Batch(1100/2353) done. Loss: 0.0688  lr:0.000001
[ Fri May 17 16:11:01 2024 ] 	Batch(1200/2353) done. Loss: 0.0017  lr:0.000001
[ Fri May 17 16:11:38 2024 ] 	Batch(1300/2353) done. Loss: 0.0176  lr:0.000001
[ Fri May 17 16:12:16 2024 ] 	Batch(1400/2353) done. Loss: 0.0888  lr:0.000001
[ Fri May 17 16:12:53 2024 ] 	Batch(1500/2353) done. Loss: 0.1143  lr:0.000001
[ Fri May 17 16:13:30 2024 ] 	Batch(1600/2353) done. Loss: 0.1002  lr:0.000001
[ Fri May 17 16:14:08 2024 ] 	Batch(1700/2353) done. Loss: 0.0642  lr:0.000001
[ Fri May 17 16:14:45 2024 ] 	Batch(1800/2353) done. Loss: 0.0103  lr:0.000001
[ Fri May 17 16:15:22 2024 ] 	Batch(1900/2353) done. Loss: 0.0099  lr:0.000001
[ Fri May 17 16:16:00 2024 ] 	Batch(2000/2353) done. Loss: 0.0035  lr:0.000001
[ Fri May 17 16:16:37 2024 ] 	Batch(2100/2353) done. Loss: 0.0033  lr:0.000001
[ Fri May 17 16:17:15 2024 ] 	Batch(2200/2353) done. Loss: 0.0051  lr:0.000001
[ Fri May 17 16:17:52 2024 ] 	Batch(2300/2353) done. Loss: 0.0256  lr:0.000001
[ Fri May 17 16:18:12 2024 ] 	Mean training loss: 0.0368.
[ Fri May 17 16:18:12 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 16:18:12 2024 ] Training epoch: 105
[ Fri May 17 16:18:12 2024 ] 	Batch(0/2353) done. Loss: 0.0743  lr:0.000001
[ Fri May 17 16:18:50 2024 ] 	Batch(100/2353) done. Loss: 0.0036  lr:0.000001
[ Fri May 17 16:19:27 2024 ] 	Batch(200/2353) done. Loss: 0.0053  lr:0.000001
[ Fri May 17 16:20:04 2024 ] 	Batch(300/2353) done. Loss: 0.0155  lr:0.000001
[ Fri May 17 16:20:42 2024 ] 	Batch(400/2353) done. Loss: 0.0769  lr:0.000001
[ Fri May 17 16:21:19 2024 ] 	Batch(500/2353) done. Loss: 0.0210  lr:0.000001
[ Fri May 17 16:21:56 2024 ] 	Batch(600/2353) done. Loss: 0.0142  lr:0.000001
[ Fri May 17 16:22:34 2024 ] 	Batch(700/2353) done. Loss: 0.0485  lr:0.000001
[ Fri May 17 16:23:11 2024 ] 	Batch(800/2353) done. Loss: 0.0060  lr:0.000001
[ Fri May 17 16:23:49 2024 ] 	Batch(900/2353) done. Loss: 0.0051  lr:0.000001
[ Fri May 17 16:24:26 2024 ] 	Batch(1000/2353) done. Loss: 0.0063  lr:0.000001
[ Fri May 17 16:25:03 2024 ] 	Batch(1100/2353) done. Loss: 0.0070  lr:0.000001
[ Fri May 17 16:25:41 2024 ] 	Batch(1200/2353) done. Loss: 0.1019  lr:0.000001
[ Fri May 17 16:26:18 2024 ] 	Batch(1300/2353) done. Loss: 0.0055  lr:0.000001
[ Fri May 17 16:26:56 2024 ] 	Batch(1400/2353) done. Loss: 0.0299  lr:0.000001
[ Fri May 17 16:27:34 2024 ] 	Batch(1500/2353) done. Loss: 0.0042  lr:0.000001
[ Fri May 17 16:28:12 2024 ] 	Batch(1600/2353) done. Loss: 0.0217  lr:0.000001
[ Fri May 17 16:28:50 2024 ] 	Batch(1700/2353) done. Loss: 0.0264  lr:0.000001
[ Fri May 17 16:29:27 2024 ] 	Batch(1800/2353) done. Loss: 0.0198  lr:0.000001
[ Fri May 17 16:30:04 2024 ] 	Batch(1900/2353) done. Loss: 0.1254  lr:0.000001
[ Fri May 17 16:30:42 2024 ] 	Batch(2000/2353) done. Loss: 0.0068  lr:0.000001
[ Fri May 17 16:31:19 2024 ] 	Batch(2100/2353) done. Loss: 0.0307  lr:0.000001
[ Fri May 17 16:31:57 2024 ] 	Batch(2200/2353) done. Loss: 0.0263  lr:0.000001
[ Fri May 17 16:32:34 2024 ] 	Batch(2300/2353) done. Loss: 0.0864  lr:0.000001
[ Fri May 17 16:32:53 2024 ] 	Mean training loss: 0.0367.
[ Fri May 17 16:32:53 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 16:32:53 2024 ] Training epoch: 106
[ Fri May 17 16:32:54 2024 ] 	Batch(0/2353) done. Loss: 0.0063  lr:0.000001
[ Fri May 17 16:33:32 2024 ] 	Batch(100/2353) done. Loss: 0.0174  lr:0.000001
[ Fri May 17 16:34:09 2024 ] 	Batch(200/2353) done. Loss: 0.1119  lr:0.000001
[ Fri May 17 16:34:46 2024 ] 	Batch(300/2353) done. Loss: 0.0763  lr:0.000001
[ Fri May 17 16:35:24 2024 ] 	Batch(400/2353) done. Loss: 0.0069  lr:0.000001
[ Fri May 17 16:36:01 2024 ] 	Batch(500/2353) done. Loss: 0.0826  lr:0.000001
[ Fri May 17 16:36:39 2024 ] 	Batch(600/2353) done. Loss: 0.0688  lr:0.000001
[ Fri May 17 16:37:16 2024 ] 	Batch(700/2353) done. Loss: 0.1016  lr:0.000001
[ Fri May 17 16:37:53 2024 ] 	Batch(800/2353) done. Loss: 0.0408  lr:0.000001
[ Fri May 17 16:38:31 2024 ] 	Batch(900/2353) done. Loss: 0.0088  lr:0.000001
[ Fri May 17 16:39:08 2024 ] 	Batch(1000/2353) done. Loss: 0.0372  lr:0.000001
[ Fri May 17 16:39:45 2024 ] 	Batch(1100/2353) done. Loss: 0.0134  lr:0.000001
[ Fri May 17 16:40:23 2024 ] 	Batch(1200/2353) done. Loss: 0.0315  lr:0.000001
[ Fri May 17 16:41:00 2024 ] 	Batch(1300/2353) done. Loss: 0.0277  lr:0.000001
[ Fri May 17 16:41:38 2024 ] 	Batch(1400/2353) done. Loss: 0.1056  lr:0.000001
[ Fri May 17 16:42:15 2024 ] 	Batch(1500/2353) done. Loss: 0.0062  lr:0.000001
[ Fri May 17 16:42:53 2024 ] 	Batch(1600/2353) done. Loss: 0.0120  lr:0.000001
[ Fri May 17 16:43:30 2024 ] 	Batch(1700/2353) done. Loss: 0.0397  lr:0.000001
[ Fri May 17 16:44:08 2024 ] 	Batch(1800/2353) done. Loss: 0.0183  lr:0.000001
[ Fri May 17 16:44:45 2024 ] 	Batch(1900/2353) done. Loss: 0.2010  lr:0.000001
[ Fri May 17 16:45:22 2024 ] 	Batch(2000/2353) done. Loss: 0.0402  lr:0.000001
[ Fri May 17 16:46:00 2024 ] 	Batch(2100/2353) done. Loss: 0.0363  lr:0.000001
[ Fri May 17 16:46:37 2024 ] 	Batch(2200/2353) done. Loss: 0.0191  lr:0.000001
[ Fri May 17 16:47:14 2024 ] 	Batch(2300/2353) done. Loss: 0.0234  lr:0.000001
[ Fri May 17 16:47:34 2024 ] 	Mean training loss: 0.0372.
[ Fri May 17 16:47:34 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 16:47:34 2024 ] Training epoch: 107
[ Fri May 17 16:47:35 2024 ] 	Batch(0/2353) done. Loss: 0.0314  lr:0.000001
[ Fri May 17 16:48:13 2024 ] 	Batch(100/2353) done. Loss: 0.0320  lr:0.000001
[ Fri May 17 16:48:50 2024 ] 	Batch(200/2353) done. Loss: 0.0090  lr:0.000001
[ Fri May 17 16:49:27 2024 ] 	Batch(300/2353) done. Loss: 0.0052  lr:0.000001
[ Fri May 17 16:50:05 2024 ] 	Batch(400/2353) done. Loss: 0.0386  lr:0.000001
[ Fri May 17 16:50:42 2024 ] 	Batch(500/2353) done. Loss: 0.0028  lr:0.000001
[ Fri May 17 16:51:19 2024 ] 	Batch(600/2353) done. Loss: 0.0816  lr:0.000001
[ Fri May 17 16:51:57 2024 ] 	Batch(700/2353) done. Loss: 0.0296  lr:0.000001
[ Fri May 17 16:52:34 2024 ] 	Batch(800/2353) done. Loss: 0.0126  lr:0.000001
[ Fri May 17 16:53:11 2024 ] 	Batch(900/2353) done. Loss: 0.0512  lr:0.000001
[ Fri May 17 16:53:49 2024 ] 	Batch(1000/2353) done. Loss: 0.0739  lr:0.000001
[ Fri May 17 16:54:26 2024 ] 	Batch(1100/2353) done. Loss: 0.0534  lr:0.000001
[ Fri May 17 16:55:03 2024 ] 	Batch(1200/2353) done. Loss: 0.2992  lr:0.000001
[ Fri May 17 16:55:42 2024 ] 	Batch(1300/2353) done. Loss: 0.0090  lr:0.000001
[ Fri May 17 16:56:20 2024 ] 	Batch(1400/2353) done. Loss: 0.0205  lr:0.000001
[ Fri May 17 16:56:58 2024 ] 	Batch(1500/2353) done. Loss: 0.0202  lr:0.000001
[ Fri May 17 16:57:36 2024 ] 	Batch(1600/2353) done. Loss: 0.0305  lr:0.000001
[ Fri May 17 16:58:14 2024 ] 	Batch(1700/2353) done. Loss: 0.0306  lr:0.000001
[ Fri May 17 16:58:52 2024 ] 	Batch(1800/2353) done. Loss: 0.0432  lr:0.000001
[ Fri May 17 16:59:30 2024 ] 	Batch(1900/2353) done. Loss: 0.0122  lr:0.000001
[ Fri May 17 17:00:08 2024 ] 	Batch(2000/2353) done. Loss: 0.1531  lr:0.000001
[ Fri May 17 17:00:46 2024 ] 	Batch(2100/2353) done. Loss: 0.0625  lr:0.000001
[ Fri May 17 17:01:23 2024 ] 	Batch(2200/2353) done. Loss: 0.0043  lr:0.000001
[ Fri May 17 17:02:00 2024 ] 	Batch(2300/2353) done. Loss: 0.0087  lr:0.000001
[ Fri May 17 17:02:20 2024 ] 	Mean training loss: 0.0372.
[ Fri May 17 17:02:20 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 17:02:20 2024 ] Training epoch: 108
[ Fri May 17 17:02:21 2024 ] 	Batch(0/2353) done. Loss: 0.0022  lr:0.000001
[ Fri May 17 17:02:58 2024 ] 	Batch(100/2353) done. Loss: 0.0201  lr:0.000001
[ Fri May 17 17:03:35 2024 ] 	Batch(200/2353) done. Loss: 0.0277  lr:0.000001
[ Fri May 17 17:04:13 2024 ] 	Batch(300/2353) done. Loss: 0.0810  lr:0.000001
[ Fri May 17 17:04:50 2024 ] 	Batch(400/2353) done. Loss: 0.0145  lr:0.000001
[ Fri May 17 17:05:27 2024 ] 	Batch(500/2353) done. Loss: 0.0231  lr:0.000001
[ Fri May 17 17:06:05 2024 ] 	Batch(600/2353) done. Loss: 0.0239  lr:0.000001
[ Fri May 17 17:06:42 2024 ] 	Batch(700/2353) done. Loss: 0.0693  lr:0.000001
[ Fri May 17 17:07:19 2024 ] 	Batch(800/2353) done. Loss: 0.0042  lr:0.000001
[ Fri May 17 17:07:57 2024 ] 	Batch(900/2353) done. Loss: 0.0062  lr:0.000001
[ Fri May 17 17:08:34 2024 ] 	Batch(1000/2353) done. Loss: 0.0358  lr:0.000001
[ Fri May 17 17:09:11 2024 ] 	Batch(1100/2353) done. Loss: 0.0855  lr:0.000001
[ Fri May 17 17:09:49 2024 ] 	Batch(1200/2353) done. Loss: 0.1207  lr:0.000001
[ Fri May 17 17:10:26 2024 ] 	Batch(1300/2353) done. Loss: 0.0160  lr:0.000001
[ Fri May 17 17:11:04 2024 ] 	Batch(1400/2353) done. Loss: 0.0031  lr:0.000001
[ Fri May 17 17:11:42 2024 ] 	Batch(1500/2353) done. Loss: 0.0396  lr:0.000001
[ Fri May 17 17:12:19 2024 ] 	Batch(1600/2353) done. Loss: 0.0069  lr:0.000001
[ Fri May 17 17:12:56 2024 ] 	Batch(1700/2353) done. Loss: 0.0021  lr:0.000001
[ Fri May 17 17:13:34 2024 ] 	Batch(1800/2353) done. Loss: 0.0291  lr:0.000001
[ Fri May 17 17:14:11 2024 ] 	Batch(1900/2353) done. Loss: 0.0407  lr:0.000001
[ Fri May 17 17:14:49 2024 ] 	Batch(2000/2353) done. Loss: 0.0969  lr:0.000001
[ Fri May 17 17:15:26 2024 ] 	Batch(2100/2353) done. Loss: 0.0027  lr:0.000001
[ Fri May 17 17:16:03 2024 ] 	Batch(2200/2353) done. Loss: 0.0119  lr:0.000001
[ Fri May 17 17:16:41 2024 ] 	Batch(2300/2353) done. Loss: 0.0605  lr:0.000001
[ Fri May 17 17:17:00 2024 ] 	Mean training loss: 0.0388.
[ Fri May 17 17:17:00 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 17:17:00 2024 ] Training epoch: 109
[ Fri May 17 17:17:01 2024 ] 	Batch(0/2353) done. Loss: 0.0376  lr:0.000001
[ Fri May 17 17:17:39 2024 ] 	Batch(100/2353) done. Loss: 0.2408  lr:0.000001
[ Fri May 17 17:18:17 2024 ] 	Batch(200/2353) done. Loss: 0.0089  lr:0.000001
[ Fri May 17 17:18:55 2024 ] 	Batch(300/2353) done. Loss: 0.0209  lr:0.000001
[ Fri May 17 17:19:33 2024 ] 	Batch(400/2353) done. Loss: 0.0157  lr:0.000001
[ Fri May 17 17:20:11 2024 ] 	Batch(500/2353) done. Loss: 0.0100  lr:0.000001
[ Fri May 17 17:20:48 2024 ] 	Batch(600/2353) done. Loss: 0.0428  lr:0.000001
[ Fri May 17 17:21:26 2024 ] 	Batch(700/2353) done. Loss: 0.0541  lr:0.000001
[ Fri May 17 17:22:03 2024 ] 	Batch(800/2353) done. Loss: 0.0149  lr:0.000001
[ Fri May 17 17:22:41 2024 ] 	Batch(900/2353) done. Loss: 0.0578  lr:0.000001
[ Fri May 17 17:23:19 2024 ] 	Batch(1000/2353) done. Loss: 0.1899  lr:0.000001
[ Fri May 17 17:23:57 2024 ] 	Batch(1100/2353) done. Loss: 0.0224  lr:0.000001
[ Fri May 17 17:24:36 2024 ] 	Batch(1200/2353) done. Loss: 0.0042  lr:0.000001
[ Fri May 17 17:25:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0177  lr:0.000001
[ Fri May 17 17:25:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0342  lr:0.000001
[ Fri May 17 17:26:28 2024 ] 	Batch(1500/2353) done. Loss: 0.1225  lr:0.000001
[ Fri May 17 17:27:05 2024 ] 	Batch(1600/2353) done. Loss: 0.0535  lr:0.000001
[ Fri May 17 17:27:42 2024 ] 	Batch(1700/2353) done. Loss: 0.0467  lr:0.000001
[ Fri May 17 17:28:20 2024 ] 	Batch(1800/2353) done. Loss: 0.0165  lr:0.000001
[ Fri May 17 17:28:57 2024 ] 	Batch(1900/2353) done. Loss: 0.0099  lr:0.000001
[ Fri May 17 17:29:35 2024 ] 	Batch(2000/2353) done. Loss: 0.0342  lr:0.000001
[ Fri May 17 17:30:12 2024 ] 	Batch(2100/2353) done. Loss: 0.0145  lr:0.000001
[ Fri May 17 17:30:49 2024 ] 	Batch(2200/2353) done. Loss: 0.0092  lr:0.000001
[ Fri May 17 17:31:27 2024 ] 	Batch(2300/2353) done. Loss: 0.0201  lr:0.000001
[ Fri May 17 17:31:46 2024 ] 	Mean training loss: 0.0381.
[ Fri May 17 17:31:46 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 17:31:46 2024 ] Training epoch: 110
[ Fri May 17 17:31:47 2024 ] 	Batch(0/2353) done. Loss: 0.0067  lr:0.000001
[ Fri May 17 17:32:26 2024 ] 	Batch(100/2353) done. Loss: 0.0960  lr:0.000001
[ Fri May 17 17:33:04 2024 ] 	Batch(200/2353) done. Loss: 0.0295  lr:0.000001
[ Fri May 17 17:33:42 2024 ] 	Batch(300/2353) done. Loss: 0.0053  lr:0.000001
[ Fri May 17 17:34:20 2024 ] 	Batch(400/2353) done. Loss: 0.0198  lr:0.000001
[ Fri May 17 17:34:59 2024 ] 	Batch(500/2353) done. Loss: 0.0450  lr:0.000001
[ Fri May 17 17:35:36 2024 ] 	Batch(600/2353) done. Loss: 0.0102  lr:0.000001
[ Fri May 17 17:36:13 2024 ] 	Batch(700/2353) done. Loss: 0.1003  lr:0.000001
[ Fri May 17 17:36:51 2024 ] 	Batch(800/2353) done. Loss: 0.0679  lr:0.000001
[ Fri May 17 17:37:28 2024 ] 	Batch(900/2353) done. Loss: 0.0051  lr:0.000001
[ Fri May 17 17:38:05 2024 ] 	Batch(1000/2353) done. Loss: 0.0147  lr:0.000001
[ Fri May 17 17:38:43 2024 ] 	Batch(1100/2353) done. Loss: 0.0249  lr:0.000001
[ Fri May 17 17:39:20 2024 ] 	Batch(1200/2353) done. Loss: 0.0136  lr:0.000001
[ Fri May 17 17:39:58 2024 ] 	Batch(1300/2353) done. Loss: 0.0141  lr:0.000001
[ Fri May 17 17:40:35 2024 ] 	Batch(1400/2353) done. Loss: 0.0104  lr:0.000001
[ Fri May 17 17:41:12 2024 ] 	Batch(1500/2353) done. Loss: 0.0923  lr:0.000001
[ Fri May 17 17:41:50 2024 ] 	Batch(1600/2353) done. Loss: 0.0318  lr:0.000001
[ Fri May 17 17:42:27 2024 ] 	Batch(1700/2353) done. Loss: 0.0089  lr:0.000001
[ Fri May 17 17:43:05 2024 ] 	Batch(1800/2353) done. Loss: 0.0128  lr:0.000001
[ Fri May 17 17:43:43 2024 ] 	Batch(1900/2353) done. Loss: 0.0137  lr:0.000001
[ Fri May 17 17:44:21 2024 ] 	Batch(2000/2353) done. Loss: 0.0159  lr:0.000001
[ Fri May 17 17:44:58 2024 ] 	Batch(2100/2353) done. Loss: 0.0692  lr:0.000001
[ Fri May 17 17:45:36 2024 ] 	Batch(2200/2353) done. Loss: 0.0110  lr:0.000001
[ Fri May 17 17:46:13 2024 ] 	Batch(2300/2353) done. Loss: 0.0075  lr:0.000001
[ Fri May 17 17:46:33 2024 ] 	Mean training loss: 0.0377.
[ Fri May 17 17:46:33 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 17:46:33 2024 ] Eval epoch: 110
[ Fri May 17 17:48:36 2024 ] 	Mean val loss of 2367 batches: 0.25268056372466163.
[ Fri May 17 17:48:36 2024 ] Training epoch: 111
[ Fri May 17 17:48:37 2024 ] 	Batch(0/2353) done. Loss: 0.0211  lr:0.000001
[ Fri May 17 17:49:15 2024 ] 	Batch(100/2353) done. Loss: 0.0294  lr:0.000001
[ Fri May 17 17:49:52 2024 ] 	Batch(200/2353) done. Loss: 0.0533  lr:0.000001
[ Fri May 17 17:50:29 2024 ] 	Batch(300/2353) done. Loss: 0.0356  lr:0.000001
[ Fri May 17 17:51:07 2024 ] 	Batch(400/2353) done. Loss: 0.0422  lr:0.000001
[ Fri May 17 17:51:44 2024 ] 	Batch(500/2353) done. Loss: 0.0261  lr:0.000001
[ Fri May 17 17:52:21 2024 ] 	Batch(600/2353) done. Loss: 0.0385  lr:0.000001
[ Fri May 17 17:52:59 2024 ] 	Batch(700/2353) done. Loss: 0.0048  lr:0.000001
[ Fri May 17 17:53:36 2024 ] 	Batch(800/2353) done. Loss: 0.1410  lr:0.000001
[ Fri May 17 17:54:13 2024 ] 	Batch(900/2353) done. Loss: 0.0426  lr:0.000001
[ Fri May 17 17:54:51 2024 ] 	Batch(1000/2353) done. Loss: 0.0134  lr:0.000001
[ Fri May 17 17:55:28 2024 ] 	Batch(1100/2353) done. Loss: 0.0567  lr:0.000001
[ Fri May 17 17:56:05 2024 ] 	Batch(1200/2353) done. Loss: 0.0081  lr:0.000001
[ Fri May 17 17:56:43 2024 ] 	Batch(1300/2353) done. Loss: 0.0367  lr:0.000001
[ Fri May 17 17:57:20 2024 ] 	Batch(1400/2353) done. Loss: 0.0246  lr:0.000001
[ Fri May 17 17:57:57 2024 ] 	Batch(1500/2353) done. Loss: 0.0036  lr:0.000001
[ Fri May 17 17:58:35 2024 ] 	Batch(1600/2353) done. Loss: 0.0359  lr:0.000001
[ Fri May 17 17:59:13 2024 ] 	Batch(1700/2353) done. Loss: 0.0162  lr:0.000001
[ Fri May 17 17:59:51 2024 ] 	Batch(1800/2353) done. Loss: 0.0190  lr:0.000001
[ Fri May 17 18:00:29 2024 ] 	Batch(1900/2353) done. Loss: 0.0032  lr:0.000001
[ Fri May 17 18:01:07 2024 ] 	Batch(2000/2353) done. Loss: 0.0220  lr:0.000001
[ Fri May 17 18:01:45 2024 ] 	Batch(2100/2353) done. Loss: 0.0108  lr:0.000001
[ Fri May 17 18:02:23 2024 ] 	Batch(2200/2353) done. Loss: 0.0263  lr:0.000001
[ Fri May 17 18:03:01 2024 ] 	Batch(2300/2353) done. Loss: 0.0041  lr:0.000001
[ Fri May 17 18:03:21 2024 ] 	Mean training loss: 0.0359.
[ Fri May 17 18:03:21 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 18:03:22 2024 ] Training epoch: 112
[ Fri May 17 18:03:22 2024 ] 	Batch(0/2353) done. Loss: 0.0129  lr:0.000001
[ Fri May 17 18:04:00 2024 ] 	Batch(100/2353) done. Loss: 0.0405  lr:0.000001
[ Fri May 17 18:04:38 2024 ] 	Batch(200/2353) done. Loss: 0.0665  lr:0.000001
[ Fri May 17 18:05:16 2024 ] 	Batch(300/2353) done. Loss: 0.0389  lr:0.000001
[ Fri May 17 18:05:55 2024 ] 	Batch(400/2353) done. Loss: 0.0404  lr:0.000001
[ Fri May 17 18:06:33 2024 ] 	Batch(500/2353) done. Loss: 0.0314  lr:0.000001
[ Fri May 17 18:07:11 2024 ] 	Batch(600/2353) done. Loss: 0.0085  lr:0.000001
[ Fri May 17 18:07:49 2024 ] 	Batch(700/2353) done. Loss: 0.0090  lr:0.000001
[ Fri May 17 18:08:27 2024 ] 	Batch(800/2353) done. Loss: 0.0128  lr:0.000001
[ Fri May 17 18:09:05 2024 ] 	Batch(900/2353) done. Loss: 0.0298  lr:0.000001
[ Fri May 17 18:09:43 2024 ] 	Batch(1000/2353) done. Loss: 0.0064  lr:0.000001
[ Fri May 17 18:10:21 2024 ] 	Batch(1100/2353) done. Loss: 0.0574  lr:0.000001
[ Fri May 17 18:10:59 2024 ] 	Batch(1200/2353) done. Loss: 0.0207  lr:0.000001
[ Fri May 17 18:11:37 2024 ] 	Batch(1300/2353) done. Loss: 0.0053  lr:0.000001
[ Fri May 17 18:12:14 2024 ] 	Batch(1400/2353) done. Loss: 0.0112  lr:0.000001
[ Fri May 17 18:12:51 2024 ] 	Batch(1500/2353) done. Loss: 0.0113  lr:0.000001
[ Fri May 17 18:13:29 2024 ] 	Batch(1600/2353) done. Loss: 0.0297  lr:0.000001
[ Fri May 17 18:14:06 2024 ] 	Batch(1700/2353) done. Loss: 0.0196  lr:0.000001
[ Fri May 17 18:14:44 2024 ] 	Batch(1800/2353) done. Loss: 0.0179  lr:0.000001
[ Fri May 17 18:15:21 2024 ] 	Batch(1900/2353) done. Loss: 0.0389  lr:0.000001
[ Fri May 17 18:15:58 2024 ] 	Batch(2000/2353) done. Loss: 0.0047  lr:0.000001
[ Fri May 17 18:16:36 2024 ] 	Batch(2100/2353) done. Loss: 0.0228  lr:0.000001
[ Fri May 17 18:17:13 2024 ] 	Batch(2200/2353) done. Loss: 0.0513  lr:0.000001
[ Fri May 17 18:17:50 2024 ] 	Batch(2300/2353) done. Loss: 0.0100  lr:0.000001
[ Fri May 17 18:18:10 2024 ] 	Mean training loss: 0.0383.
[ Fri May 17 18:18:10 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 18:18:10 2024 ] Training epoch: 113
[ Fri May 17 18:18:11 2024 ] 	Batch(0/2353) done. Loss: 0.0790  lr:0.000001
[ Fri May 17 18:18:48 2024 ] 	Batch(100/2353) done. Loss: 0.0337  lr:0.000001
[ Fri May 17 18:19:26 2024 ] 	Batch(200/2353) done. Loss: 0.0231  lr:0.000001
[ Fri May 17 18:20:03 2024 ] 	Batch(300/2353) done. Loss: 0.0075  lr:0.000001
[ Fri May 17 18:20:40 2024 ] 	Batch(400/2353) done. Loss: 0.0116  lr:0.000001
[ Fri May 17 18:21:18 2024 ] 	Batch(500/2353) done. Loss: 0.0092  lr:0.000001
[ Fri May 17 18:21:55 2024 ] 	Batch(600/2353) done. Loss: 0.0291  lr:0.000001
[ Fri May 17 18:22:32 2024 ] 	Batch(700/2353) done. Loss: 0.0104  lr:0.000001
[ Fri May 17 18:23:10 2024 ] 	Batch(800/2353) done. Loss: 0.0114  lr:0.000001
[ Fri May 17 18:23:47 2024 ] 	Batch(900/2353) done. Loss: 0.0254  lr:0.000001
[ Fri May 17 18:24:25 2024 ] 	Batch(1000/2353) done. Loss: 0.0444  lr:0.000001
[ Fri May 17 18:25:02 2024 ] 	Batch(1100/2353) done. Loss: 0.0162  lr:0.000001
[ Fri May 17 18:25:40 2024 ] 	Batch(1200/2353) done. Loss: 0.0111  lr:0.000001
[ Fri May 17 18:26:18 2024 ] 	Batch(1300/2353) done. Loss: 0.1241  lr:0.000001
[ Fri May 17 18:26:56 2024 ] 	Batch(1400/2353) done. Loss: 0.0481  lr:0.000001
[ Fri May 17 18:27:34 2024 ] 	Batch(1500/2353) done. Loss: 0.0114  lr:0.000001
[ Fri May 17 18:28:12 2024 ] 	Batch(1600/2353) done. Loss: 0.0314  lr:0.000001
[ Fri May 17 18:28:49 2024 ] 	Batch(1700/2353) done. Loss: 0.0521  lr:0.000001
[ Fri May 17 18:29:27 2024 ] 	Batch(1800/2353) done. Loss: 0.0038  lr:0.000001
[ Fri May 17 18:30:04 2024 ] 	Batch(1900/2353) done. Loss: 0.0605  lr:0.000001
[ Fri May 17 18:30:42 2024 ] 	Batch(2000/2353) done. Loss: 0.0128  lr:0.000001
[ Fri May 17 18:31:19 2024 ] 	Batch(2100/2353) done. Loss: 0.0026  lr:0.000001
[ Fri May 17 18:31:56 2024 ] 	Batch(2200/2353) done. Loss: 0.0187  lr:0.000001
[ Fri May 17 18:32:34 2024 ] 	Batch(2300/2353) done. Loss: 0.0119  lr:0.000001
[ Fri May 17 18:32:53 2024 ] 	Mean training loss: 0.0363.
[ Fri May 17 18:32:53 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 18:32:53 2024 ] Training epoch: 114
[ Fri May 17 18:32:54 2024 ] 	Batch(0/2353) done. Loss: 0.0100  lr:0.000001
[ Fri May 17 18:33:31 2024 ] 	Batch(100/2353) done. Loss: 0.2232  lr:0.000001
[ Fri May 17 18:34:09 2024 ] 	Batch(200/2353) done. Loss: 0.0054  lr:0.000001
[ Fri May 17 18:34:46 2024 ] 	Batch(300/2353) done. Loss: 0.0112  lr:0.000001
[ Fri May 17 18:35:23 2024 ] 	Batch(400/2353) done. Loss: 0.0068  lr:0.000001
[ Fri May 17 18:36:01 2024 ] 	Batch(500/2353) done. Loss: 0.0642  lr:0.000001
[ Fri May 17 18:36:38 2024 ] 	Batch(600/2353) done. Loss: 0.0094  lr:0.000001
[ Fri May 17 18:37:15 2024 ] 	Batch(700/2353) done. Loss: 0.1022  lr:0.000001
[ Fri May 17 18:37:53 2024 ] 	Batch(800/2353) done. Loss: 0.0055  lr:0.000001
[ Fri May 17 18:38:31 2024 ] 	Batch(900/2353) done. Loss: 0.0592  lr:0.000001
[ Fri May 17 18:39:08 2024 ] 	Batch(1000/2353) done. Loss: 0.0305  lr:0.000001
[ Fri May 17 18:39:45 2024 ] 	Batch(1100/2353) done. Loss: 0.0096  lr:0.000001
[ Fri May 17 18:40:23 2024 ] 	Batch(1200/2353) done. Loss: 0.0970  lr:0.000001
[ Fri May 17 18:41:00 2024 ] 	Batch(1300/2353) done. Loss: 0.0283  lr:0.000001
[ Fri May 17 18:41:37 2024 ] 	Batch(1400/2353) done. Loss: 0.0190  lr:0.000001
[ Fri May 17 18:42:15 2024 ] 	Batch(1500/2353) done. Loss: 0.0106  lr:0.000001
[ Fri May 17 18:42:52 2024 ] 	Batch(1600/2353) done. Loss: 0.0045  lr:0.000001
[ Fri May 17 18:43:30 2024 ] 	Batch(1700/2353) done. Loss: 0.0407  lr:0.000001
[ Fri May 17 18:44:07 2024 ] 	Batch(1800/2353) done. Loss: 0.0244  lr:0.000001
[ Fri May 17 18:44:44 2024 ] 	Batch(1900/2353) done. Loss: 0.0568  lr:0.000001
[ Fri May 17 18:45:22 2024 ] 	Batch(2000/2353) done. Loss: 0.0129  lr:0.000001
[ Fri May 17 18:46:00 2024 ] 	Batch(2100/2353) done. Loss: 0.0067  lr:0.000001
[ Fri May 17 18:46:37 2024 ] 	Batch(2200/2353) done. Loss: 0.0128  lr:0.000001
[ Fri May 17 18:47:15 2024 ] 	Batch(2300/2353) done. Loss: 0.0138  lr:0.000001
[ Fri May 17 18:47:34 2024 ] 	Mean training loss: 0.0363.
[ Fri May 17 18:47:34 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 18:47:34 2024 ] Training epoch: 115
[ Fri May 17 18:47:35 2024 ] 	Batch(0/2353) done. Loss: 0.0059  lr:0.000001
[ Fri May 17 18:48:12 2024 ] 	Batch(100/2353) done. Loss: 0.0412  lr:0.000001
[ Fri May 17 18:48:50 2024 ] 	Batch(200/2353) done. Loss: 0.0348  lr:0.000001
[ Fri May 17 18:49:27 2024 ] 	Batch(300/2353) done. Loss: 0.0348  lr:0.000001
[ Fri May 17 18:50:04 2024 ] 	Batch(400/2353) done. Loss: 0.0278  lr:0.000001
[ Fri May 17 18:50:42 2024 ] 	Batch(500/2353) done. Loss: 0.0307  lr:0.000001
[ Fri May 17 18:51:19 2024 ] 	Batch(600/2353) done. Loss: 0.0327  lr:0.000001
[ Fri May 17 18:51:56 2024 ] 	Batch(700/2353) done. Loss: 0.0065  lr:0.000001
[ Fri May 17 18:52:34 2024 ] 	Batch(800/2353) done. Loss: 0.0222  lr:0.000001
[ Fri May 17 18:53:12 2024 ] 	Batch(900/2353) done. Loss: 0.1250  lr:0.000001
[ Fri May 17 18:53:50 2024 ] 	Batch(1000/2353) done. Loss: 0.0209  lr:0.000001
[ Fri May 17 18:54:28 2024 ] 	Batch(1100/2353) done. Loss: 0.0069  lr:0.000001
[ Fri May 17 18:55:05 2024 ] 	Batch(1200/2353) done. Loss: 0.0194  lr:0.000001
[ Fri May 17 18:55:42 2024 ] 	Batch(1300/2353) done. Loss: 0.0114  lr:0.000001
[ Fri May 17 18:56:20 2024 ] 	Batch(1400/2353) done. Loss: 0.0032  lr:0.000001
[ Fri May 17 18:56:57 2024 ] 	Batch(1500/2353) done. Loss: 0.0175  lr:0.000001
[ Fri May 17 18:57:35 2024 ] 	Batch(1600/2353) done. Loss: 0.0285  lr:0.000001
[ Fri May 17 18:58:12 2024 ] 	Batch(1700/2353) done. Loss: 0.0168  lr:0.000001
[ Fri May 17 18:58:49 2024 ] 	Batch(1800/2353) done. Loss: 0.0071  lr:0.000001
[ Fri May 17 18:59:27 2024 ] 	Batch(1900/2353) done. Loss: 0.0914  lr:0.000001
[ Fri May 17 19:00:04 2024 ] 	Batch(2000/2353) done. Loss: 0.0431  lr:0.000001
[ Fri May 17 19:00:41 2024 ] 	Batch(2100/2353) done. Loss: 0.0062  lr:0.000001
[ Fri May 17 19:01:19 2024 ] 	Batch(2200/2353) done. Loss: 0.0089  lr:0.000001
[ Fri May 17 19:01:56 2024 ] 	Batch(2300/2353) done. Loss: 0.0042  lr:0.000001
[ Fri May 17 19:02:16 2024 ] 	Mean training loss: 0.0375.
[ Fri May 17 19:02:16 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 19:02:16 2024 ] Training epoch: 116
[ Fri May 17 19:02:17 2024 ] 	Batch(0/2353) done. Loss: 0.0445  lr:0.000001
[ Fri May 17 19:02:54 2024 ] 	Batch(100/2353) done. Loss: 0.0660  lr:0.000001
[ Fri May 17 19:03:31 2024 ] 	Batch(200/2353) done. Loss: 0.0754  lr:0.000001
[ Fri May 17 19:04:09 2024 ] 	Batch(300/2353) done. Loss: 0.0824  lr:0.000001
[ Fri May 17 19:04:46 2024 ] 	Batch(400/2353) done. Loss: 0.0123  lr:0.000001
[ Fri May 17 19:05:24 2024 ] 	Batch(500/2353) done. Loss: 0.0663  lr:0.000001
[ Fri May 17 19:06:01 2024 ] 	Batch(600/2353) done. Loss: 0.0240  lr:0.000001
[ Fri May 17 19:06:38 2024 ] 	Batch(700/2353) done. Loss: 0.0878  lr:0.000001
[ Fri May 17 19:07:16 2024 ] 	Batch(800/2353) done. Loss: 0.0026  lr:0.000001
[ Fri May 17 19:07:54 2024 ] 	Batch(900/2353) done. Loss: 0.0161  lr:0.000001
[ Fri May 17 19:08:32 2024 ] 	Batch(1000/2353) done. Loss: 0.0148  lr:0.000001
[ Fri May 17 19:09:09 2024 ] 	Batch(1100/2353) done. Loss: 0.0269  lr:0.000001
[ Fri May 17 19:09:47 2024 ] 	Batch(1200/2353) done. Loss: 0.0158  lr:0.000001
[ Fri May 17 19:10:25 2024 ] 	Batch(1300/2353) done. Loss: 0.1992  lr:0.000001
[ Fri May 17 19:11:03 2024 ] 	Batch(1400/2353) done. Loss: 0.0210  lr:0.000001
[ Fri May 17 19:11:40 2024 ] 	Batch(1500/2353) done. Loss: 0.0211  lr:0.000001
[ Fri May 17 19:12:18 2024 ] 	Batch(1600/2353) done. Loss: 0.0094  lr:0.000001
[ Fri May 17 19:12:55 2024 ] 	Batch(1700/2353) done. Loss: 0.0209  lr:0.000001
[ Fri May 17 19:13:32 2024 ] 	Batch(1800/2353) done. Loss: 0.0262  lr:0.000001
[ Fri May 17 19:14:10 2024 ] 	Batch(1900/2353) done. Loss: 0.0922  lr:0.000001
[ Fri May 17 19:14:47 2024 ] 	Batch(2000/2353) done. Loss: 0.0212  lr:0.000001
[ Fri May 17 19:15:25 2024 ] 	Batch(2100/2353) done. Loss: 0.0469  lr:0.000001
[ Fri May 17 19:16:02 2024 ] 	Batch(2200/2353) done. Loss: 0.0425  lr:0.000001
[ Fri May 17 19:16:39 2024 ] 	Batch(2300/2353) done. Loss: 0.0171  lr:0.000001
[ Fri May 17 19:16:59 2024 ] 	Mean training loss: 0.0353.
[ Fri May 17 19:16:59 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 19:16:59 2024 ] Training epoch: 117
[ Fri May 17 19:17:00 2024 ] 	Batch(0/2353) done. Loss: 0.0138  lr:0.000001
[ Fri May 17 19:17:38 2024 ] 	Batch(100/2353) done. Loss: 0.0124  lr:0.000001
[ Fri May 17 19:18:17 2024 ] 	Batch(200/2353) done. Loss: 0.0137  lr:0.000001
[ Fri May 17 19:18:55 2024 ] 	Batch(300/2353) done. Loss: 0.0405  lr:0.000001
[ Fri May 17 19:19:33 2024 ] 	Batch(400/2353) done. Loss: 0.0781  lr:0.000001
[ Fri May 17 19:20:11 2024 ] 	Batch(500/2353) done. Loss: 0.0119  lr:0.000001
[ Fri May 17 19:20:49 2024 ] 	Batch(600/2353) done. Loss: 0.0033  lr:0.000001
[ Fri May 17 19:21:27 2024 ] 	Batch(700/2353) done. Loss: 0.0812  lr:0.000001
[ Fri May 17 19:22:05 2024 ] 	Batch(800/2353) done. Loss: 0.0146  lr:0.000001
[ Fri May 17 19:22:43 2024 ] 	Batch(900/2353) done. Loss: 0.0228  lr:0.000001
[ Fri May 17 19:23:20 2024 ] 	Batch(1000/2353) done. Loss: 0.0405  lr:0.000001
[ Fri May 17 19:23:58 2024 ] 	Batch(1100/2353) done. Loss: 0.0296  lr:0.000001
[ Fri May 17 19:24:36 2024 ] 	Batch(1200/2353) done. Loss: 0.0217  lr:0.000001
[ Fri May 17 19:25:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0024  lr:0.000001
[ Fri May 17 19:25:51 2024 ] 	Batch(1400/2353) done. Loss: 0.0082  lr:0.000001
[ Fri May 17 19:26:28 2024 ] 	Batch(1500/2353) done. Loss: 0.0098  lr:0.000001
[ Fri May 17 19:27:05 2024 ] 	Batch(1600/2353) done. Loss: 0.0330  lr:0.000001
[ Fri May 17 19:27:43 2024 ] 	Batch(1700/2353) done. Loss: 0.0175  lr:0.000001
[ Fri May 17 19:28:20 2024 ] 	Batch(1800/2353) done. Loss: 0.0224  lr:0.000001
[ Fri May 17 19:28:57 2024 ] 	Batch(1900/2353) done. Loss: 0.0227  lr:0.000001
[ Fri May 17 19:29:35 2024 ] 	Batch(2000/2353) done. Loss: 0.0112  lr:0.000001
[ Fri May 17 19:30:13 2024 ] 	Batch(2100/2353) done. Loss: 0.0301  lr:0.000001
[ Fri May 17 19:30:51 2024 ] 	Batch(2200/2353) done. Loss: 0.0065  lr:0.000001
[ Fri May 17 19:31:29 2024 ] 	Batch(2300/2353) done. Loss: 0.0213  lr:0.000001
[ Fri May 17 19:31:49 2024 ] 	Mean training loss: 0.0372.
[ Fri May 17 19:31:49 2024 ] 	Time consumption: [Data]01%, [Network]92%
[ Fri May 17 19:31:49 2024 ] Training epoch: 118
[ Fri May 17 19:31:50 2024 ] 	Batch(0/2353) done. Loss: 0.0163  lr:0.000001
[ Fri May 17 19:32:28 2024 ] 	Batch(100/2353) done. Loss: 0.0121  lr:0.000001
[ Fri May 17 19:33:06 2024 ] 	Batch(200/2353) done. Loss: 0.0483  lr:0.000001
[ Fri May 17 19:33:44 2024 ] 	Batch(300/2353) done. Loss: 0.0233  lr:0.000001
[ Fri May 17 19:34:22 2024 ] 	Batch(400/2353) done. Loss: 0.0147  lr:0.000001
[ Fri May 17 19:35:00 2024 ] 	Batch(500/2353) done. Loss: 0.0056  lr:0.000001
[ Fri May 17 19:35:38 2024 ] 	Batch(600/2353) done. Loss: 0.0093  lr:0.000001
[ Fri May 17 19:36:15 2024 ] 	Batch(700/2353) done. Loss: 0.0116  lr:0.000001
[ Fri May 17 19:36:53 2024 ] 	Batch(800/2353) done. Loss: 0.0020  lr:0.000001
[ Fri May 17 19:37:30 2024 ] 	Batch(900/2353) done. Loss: 0.0565  lr:0.000001
[ Fri May 17 19:38:07 2024 ] 	Batch(1000/2353) done. Loss: 0.0076  lr:0.000001
[ Fri May 17 19:38:45 2024 ] 	Batch(1100/2353) done. Loss: 0.0354  lr:0.000001
[ Fri May 17 19:39:22 2024 ] 	Batch(1200/2353) done. Loss: 0.0098  lr:0.000001
[ Fri May 17 19:39:59 2024 ] 	Batch(1300/2353) done. Loss: 0.0061  lr:0.000001
[ Fri May 17 19:40:37 2024 ] 	Batch(1400/2353) done. Loss: 0.0158  lr:0.000001
[ Fri May 17 19:41:14 2024 ] 	Batch(1500/2353) done. Loss: 0.0525  lr:0.000001
[ Fri May 17 19:41:52 2024 ] 	Batch(1600/2353) done. Loss: 0.0179  lr:0.000001
[ Fri May 17 19:42:30 2024 ] 	Batch(1700/2353) done. Loss: 0.0696  lr:0.000001
[ Fri May 17 19:43:08 2024 ] 	Batch(1800/2353) done. Loss: 0.0809  lr:0.000001
[ Fri May 17 19:43:45 2024 ] 	Batch(1900/2353) done. Loss: 0.0145  lr:0.000001
[ Fri May 17 19:44:22 2024 ] 	Batch(2000/2353) done. Loss: 0.1337  lr:0.000001
[ Fri May 17 19:45:00 2024 ] 	Batch(2100/2353) done. Loss: 0.0086  lr:0.000001
[ Fri May 17 19:45:37 2024 ] 	Batch(2200/2353) done. Loss: 0.0087  lr:0.000001
[ Fri May 17 19:46:14 2024 ] 	Batch(2300/2353) done. Loss: 0.1033  lr:0.000001
[ Fri May 17 19:46:34 2024 ] 	Mean training loss: 0.0374.
[ Fri May 17 19:46:34 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 19:46:34 2024 ] Training epoch: 119
[ Fri May 17 19:46:35 2024 ] 	Batch(0/2353) done. Loss: 0.0501  lr:0.000001
[ Fri May 17 19:47:12 2024 ] 	Batch(100/2353) done. Loss: 0.0153  lr:0.000001
[ Fri May 17 19:47:50 2024 ] 	Batch(200/2353) done. Loss: 0.0448  lr:0.000001
[ Fri May 17 19:48:27 2024 ] 	Batch(300/2353) done. Loss: 0.0687  lr:0.000001
[ Fri May 17 19:49:04 2024 ] 	Batch(400/2353) done. Loss: 0.0300  lr:0.000001
[ Fri May 17 19:49:42 2024 ] 	Batch(500/2353) done. Loss: 0.0497  lr:0.000001
[ Fri May 17 19:50:19 2024 ] 	Batch(600/2353) done. Loss: 0.0107  lr:0.000001
[ Fri May 17 19:50:56 2024 ] 	Batch(700/2353) done. Loss: 0.0201  lr:0.000001
[ Fri May 17 19:51:34 2024 ] 	Batch(800/2353) done. Loss: 0.2272  lr:0.000001
[ Fri May 17 19:52:12 2024 ] 	Batch(900/2353) done. Loss: 0.0053  lr:0.000001
[ Fri May 17 19:52:50 2024 ] 	Batch(1000/2353) done. Loss: 0.0557  lr:0.000001
[ Fri May 17 19:53:28 2024 ] 	Batch(1100/2353) done. Loss: 0.0601  lr:0.000001
[ Fri May 17 19:54:06 2024 ] 	Batch(1200/2353) done. Loss: 0.0080  lr:0.000001
[ Fri May 17 19:54:43 2024 ] 	Batch(1300/2353) done. Loss: 0.0149  lr:0.000001
[ Fri May 17 19:55:20 2024 ] 	Batch(1400/2353) done. Loss: 0.0038  lr:0.000001
[ Fri May 17 19:55:58 2024 ] 	Batch(1500/2353) done. Loss: 0.0195  lr:0.000001
[ Fri May 17 19:56:35 2024 ] 	Batch(1600/2353) done. Loss: 0.0033  lr:0.000001
[ Fri May 17 19:57:12 2024 ] 	Batch(1700/2353) done. Loss: 0.0651  lr:0.000001
[ Fri May 17 19:57:50 2024 ] 	Batch(1800/2353) done. Loss: 0.0099  lr:0.000001
[ Fri May 17 19:58:27 2024 ] 	Batch(1900/2353) done. Loss: 0.0059  lr:0.000001
[ Fri May 17 19:59:05 2024 ] 	Batch(2000/2353) done. Loss: 0.0181  lr:0.000001
[ Fri May 17 19:59:43 2024 ] 	Batch(2100/2353) done. Loss: 0.0137  lr:0.000001
[ Fri May 17 20:00:21 2024 ] 	Batch(2200/2353) done. Loss: 0.0081  lr:0.000001
[ Fri May 17 20:00:58 2024 ] 	Batch(2300/2353) done. Loss: 0.0412  lr:0.000001
[ Fri May 17 20:01:18 2024 ] 	Mean training loss: 0.0360.
[ Fri May 17 20:01:18 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 20:01:18 2024 ] Training epoch: 120
[ Fri May 17 20:01:19 2024 ] 	Batch(0/2353) done. Loss: 0.0282  lr:0.000001
[ Fri May 17 20:01:56 2024 ] 	Batch(100/2353) done. Loss: 0.0029  lr:0.000001
[ Fri May 17 20:02:33 2024 ] 	Batch(200/2353) done. Loss: 0.0675  lr:0.000001
[ Fri May 17 20:03:11 2024 ] 	Batch(300/2353) done. Loss: 0.0197  lr:0.000001
[ Fri May 17 20:03:48 2024 ] 	Batch(400/2353) done. Loss: 0.0568  lr:0.000001
[ Fri May 17 20:04:26 2024 ] 	Batch(500/2353) done. Loss: 0.0361  lr:0.000001
[ Fri May 17 20:05:03 2024 ] 	Batch(600/2353) done. Loss: 0.0567  lr:0.000001
[ Fri May 17 20:05:40 2024 ] 	Batch(700/2353) done. Loss: 0.0771  lr:0.000001
[ Fri May 17 20:06:18 2024 ] 	Batch(800/2353) done. Loss: 0.0117  lr:0.000001
[ Fri May 17 20:06:56 2024 ] 	Batch(900/2353) done. Loss: 0.0025  lr:0.000001
[ Fri May 17 20:07:35 2024 ] 	Batch(1000/2353) done. Loss: 0.0450  lr:0.000001
[ Fri May 17 20:08:13 2024 ] 	Batch(1100/2353) done. Loss: 0.0240  lr:0.000001
[ Fri May 17 20:08:51 2024 ] 	Batch(1200/2353) done. Loss: 0.0071  lr:0.000001
[ Fri May 17 20:09:29 2024 ] 	Batch(1300/2353) done. Loss: 0.0098  lr:0.000001
[ Fri May 17 20:10:07 2024 ] 	Batch(1400/2353) done. Loss: 0.0335  lr:0.000001
[ Fri May 17 20:10:45 2024 ] 	Batch(1500/2353) done. Loss: 0.1372  lr:0.000001
[ Fri May 17 20:11:24 2024 ] 	Batch(1600/2353) done. Loss: 0.0141  lr:0.000001
[ Fri May 17 20:12:01 2024 ] 	Batch(1700/2353) done. Loss: 0.0034  lr:0.000001
[ Fri May 17 20:12:38 2024 ] 	Batch(1800/2353) done. Loss: 0.0115  lr:0.000001
[ Fri May 17 20:13:16 2024 ] 	Batch(1900/2353) done. Loss: 0.0422  lr:0.000001
[ Fri May 17 20:13:53 2024 ] 	Batch(2000/2353) done. Loss: 0.0022  lr:0.000001
[ Fri May 17 20:14:31 2024 ] 	Batch(2100/2353) done. Loss: 0.0499  lr:0.000001
[ Fri May 17 20:15:09 2024 ] 	Batch(2200/2353) done. Loss: 0.0257  lr:0.000001
[ Fri May 17 20:15:46 2024 ] 	Batch(2300/2353) done. Loss: 0.0075  lr:0.000001
[ Fri May 17 20:16:06 2024 ] 	Mean training loss: 0.0361.
[ Fri May 17 20:16:06 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 20:16:06 2024 ] Eval epoch: 120
[ Fri May 17 20:18:09 2024 ] 	Mean val loss of 2367 batches: 0.24361743139816075.
[ Fri May 17 20:18:09 2024 ] Load weights from ./prova20/epoch119_model.pt.
[ Fri May 17 20:55:45 2024 ] Load weights from prova20/epoch119_model.pt.
[ Fri May 17 20:55:45 2024 ] Eval epoch: 1
[ Fri May 17 20:57:52 2024 ] 	Mean test loss of 2367 batches: 0.244538912846753.
[ Fri May 17 20:57:52 2024 ] 	Top1: 93.53%
[ Fri May 17 20:57:52 2024 ] 	Top5: 98.98%

Here are the just predicted labels:  tensor([55, 37, 56, 30], device='cuda:0')
Here are the correct labels:  tensor([55, 37, 56, 30], device='cuda:0')
Total samples seen so far:  18932

Testing: Epoch [0/120], Samples [17708.0/18932], Loss: 0.244538912846753, Testing Accuracy: 93.53475596873018
Accuracy of 1 : 280 / 316 = 88 %
Accuracy of 2 : 299 / 316 = 94 %
Accuracy of 3 : 307 / 316 = 97 %
Accuracy of 4 : 304 / 316 = 96 %
Accuracy of 5 : 302 / 316 = 95 %
Accuracy of 6 : 308 / 316 = 97 %
Accuracy of 7 : 306 / 315 = 97 %
Accuracy of 8 : 311 / 316 = 98 %
Accuracy of 9 : 280 / 316 = 88 %
Accuracy of 10 : 236 / 315 = 74 %
Accuracy of 11 : 217 / 315 = 68 %
Accuracy of 12 : 295 / 316 = 93 %
Accuracy of 13 : 313 / 316 = 99 %
Accuracy of 14 : 311 / 316 = 98 %
Accuracy of 15 : 272 / 315 = 86 %
Accuracy of 16 : 274 / 316 = 86 %
Accuracy of 17 : 290 / 316 = 91 %
Accuracy of 18 : 297 / 316 = 93 %
Accuracy of 19 : 303 / 315 = 96 %
Accuracy of 20 : 305 / 316 = 96 %
Accuracy of 21 : 312 / 316 = 98 %
Accuracy of 22 : 307 / 316 = 97 %
Accuracy of 23 : 302 / 316 = 95 %
Accuracy of 24 : 297 / 316 = 93 %
Accuracy of 25 : 307 / 316 = 97 %
Accuracy of 26 : 314 / 316 = 99 %
Accuracy of 27 : 300 / 316 = 94 %
Accuracy of 28 : 274 / 316 = 86 %
Accuracy of 29 : 247 / 316 = 78 %
Accuracy of 30 : 295 / 315 = 93 %
Accuracy of 31 : 306 / 316 = 96 %
Accuracy of 32 : 302 / 316 = 95 %
Accuracy of 33 : 286 / 316 = 90 %
Accuracy of 34 : 310 / 316 = 98 %
Accuracy of 35 : 307 / 316 = 97 %
Accuracy of 36 : 279 / 316 = 88 %
Accuracy of 37 : 309 / 316 = 97 %
Accuracy of 38 : 311 / 316 = 98 %
Accuracy of 39 : 303 / 312 = 97 %
Accuracy of 40 : 282 / 316 = 89 %
Accuracy of 41 : 312 / 316 = 98 %
Accuracy of 42 : 316 / 316 = 100 %
Accuracy of 43 : 273 / 316 = 86 %
Accuracy of 44 : 294 / 316 = 93 %
Accuracy of 45 : 284 / 316 = 89 %
Accuracy of 46 : 287 / 316 = 90 %
Accuracy of 47 : 287 / 316 = 90 %
Accuracy of 48 : 306 / 316 = 96 %
Accuracy of 49 : 298 / 313 = 95 %
Accuracy of 50 : 300 / 314 = 95 %
Accuracy of 51 : 303 / 315 = 96 %
Accuracy of 52 : 292 / 316 = 92 %
Accuracy of 53 : 304 / 314 = 96 %
Accuracy of 54 : 305 / 309 = 98 %
Accuracy of 55 : 300 / 316 = 94 %
Accuracy of 56 : 294 / 316 = 93 %
Accuracy of 57 : 305 / 316 = 96 %
Accuracy of 58 : 302 / 316 = 95 %
Accuracy of 59 : 298 / 313 = 95 %
Accuracy of 60 : 288 / 316 = 91 %

Testing: Epoch [0/120], Samples [17708.0/18932], Loss: 0.244538912846753, Testing Accuracy: 93.53475596873018

