[ Fri May 17 21:25:09 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/train_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/train_label.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/val_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/val_label.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 60, 'channel': 6, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': False, 'only_attention': True, 'tcn_attention': True, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': True, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': False, 'scheduler': 1, 'base_lr': 0.01, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 16, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': False, 'display_recall_precision': False}

[ Fri May 17 21:25:09 2024 ] Training epoch: 1
[ Fri May 17 21:25:11 2024 ] 	Batch(0/2353) done. Loss: 8.0453  lr:0.010000
[ Fri May 17 21:25:47 2024 ] 	Batch(100/2353) done. Loss: 4.4853  lr:0.010000
[ Fri May 17 21:26:23 2024 ] 	Batch(200/2353) done. Loss: 3.7115  lr:0.010000
[ Fri May 17 21:26:59 2024 ] 	Batch(300/2353) done. Loss: 2.8383  lr:0.010000
[ Fri May 17 21:27:35 2024 ] 	Batch(400/2353) done. Loss: 3.0662  lr:0.010000
[ Fri May 17 21:28:11 2024 ] 	Batch(500/2353) done. Loss: 2.9126  lr:0.010000
[ Fri May 17 21:28:47 2024 ] 	Batch(600/2353) done. Loss: 3.0769  lr:0.010000
[ Fri May 17 21:29:23 2024 ] 	Batch(700/2353) done. Loss: 2.6514  lr:0.010000
[ Fri May 17 21:29:59 2024 ] 	Batch(800/2353) done. Loss: 2.0743  lr:0.010000
[ Fri May 17 21:30:35 2024 ] 	Batch(900/2353) done. Loss: 2.8712  lr:0.010000
[ Fri May 17 21:31:10 2024 ] 	Batch(1000/2353) done. Loss: 2.6279  lr:0.010000
[ Fri May 17 21:31:46 2024 ] 	Batch(1100/2353) done. Loss: 1.9052  lr:0.010000
[ Fri May 17 21:32:22 2024 ] 	Batch(1200/2353) done. Loss: 1.7884  lr:0.010000
[ Fri May 17 21:32:58 2024 ] 	Batch(1300/2353) done. Loss: 2.8522  lr:0.010000
[ Fri May 17 21:33:34 2024 ] 	Batch(1400/2353) done. Loss: 2.2641  lr:0.010000
[ Fri May 17 21:34:10 2024 ] 	Batch(1500/2353) done. Loss: 2.3589  lr:0.010000
[ Fri May 17 21:34:46 2024 ] 	Batch(1600/2353) done. Loss: 2.0841  lr:0.010000
[ Fri May 17 21:35:22 2024 ] 	Batch(1700/2353) done. Loss: 3.1084  lr:0.010000
[ Fri May 17 21:35:57 2024 ] 	Batch(1800/2353) done. Loss: 2.0426  lr:0.010000
[ Fri May 17 21:36:33 2024 ] 	Batch(1900/2353) done. Loss: 2.9659  lr:0.010000
[ Fri May 17 21:37:09 2024 ] 	Batch(2000/2353) done. Loss: 2.9738  lr:0.010000
[ Fri May 17 21:37:45 2024 ] 	Batch(2100/2353) done. Loss: 1.7659  lr:0.010000
[ Fri May 17 21:38:21 2024 ] 	Batch(2200/2353) done. Loss: 1.5865  lr:0.010000
[ Fri May 17 21:38:57 2024 ] 	Batch(2300/2353) done. Loss: 1.4080  lr:0.010000
[ Fri May 17 21:39:15 2024 ] 	Mean training loss: 2.5734.
[ Fri May 17 21:39:15 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Fri May 17 21:39:15 2024 ] Training epoch: 2
[ Fri May 17 21:39:16 2024 ] 	Batch(0/2353) done. Loss: 1.3572  lr:0.010000
[ Fri May 17 21:39:53 2024 ] 	Batch(100/2353) done. Loss: 1.8598  lr:0.010000
[ Fri May 17 21:40:30 2024 ] 	Batch(200/2353) done. Loss: 2.5793  lr:0.010000
[ Fri May 17 21:41:07 2024 ] 	Batch(300/2353) done. Loss: 1.7664  lr:0.010000
[ Fri May 17 21:41:43 2024 ] 	Batch(400/2353) done. Loss: 1.5117  lr:0.010000
[ Fri May 17 21:42:20 2024 ] 	Batch(500/2353) done. Loss: 1.7029  lr:0.010000
[ Fri May 17 21:42:57 2024 ] 	Batch(600/2353) done. Loss: 0.9820  lr:0.010000
[ Fri May 17 21:43:33 2024 ] 	Batch(700/2353) done. Loss: 1.5969  lr:0.010000
[ Fri May 17 21:44:10 2024 ] 	Batch(800/2353) done. Loss: 1.7377  lr:0.010000
[ Fri May 17 21:44:47 2024 ] 	Batch(900/2353) done. Loss: 1.3463  lr:0.010000
[ Fri May 17 21:45:23 2024 ] 	Batch(1000/2353) done. Loss: 1.4628  lr:0.010000
[ Fri May 17 21:46:00 2024 ] 	Batch(1100/2353) done. Loss: 1.2125  lr:0.010000
[ Fri May 17 21:46:37 2024 ] 	Batch(1200/2353) done. Loss: 1.9597  lr:0.010000
[ Fri May 17 21:47:13 2024 ] 	Batch(1300/2353) done. Loss: 1.4208  lr:0.010000
[ Fri May 17 21:47:50 2024 ] 	Batch(1400/2353) done. Loss: 1.7453  lr:0.010000
[ Fri May 17 21:48:27 2024 ] 	Batch(1500/2353) done. Loss: 1.0423  lr:0.010000
[ Fri May 17 21:49:03 2024 ] 	Batch(1600/2353) done. Loss: 2.3523  lr:0.010000
[ Fri May 17 21:49:41 2024 ] 	Batch(1700/2353) done. Loss: 1.4946  lr:0.010000
[ Fri May 17 21:50:18 2024 ] 	Batch(1800/2353) done. Loss: 1.6532  lr:0.010000
[ Fri May 17 21:50:55 2024 ] 	Batch(1900/2353) done. Loss: 1.4790  lr:0.010000
[ Fri May 17 21:51:32 2024 ] 	Batch(2000/2353) done. Loss: 1.2151  lr:0.010000
[ Fri May 17 21:52:09 2024 ] 	Batch(2100/2353) done. Loss: 1.8403  lr:0.010000
[ Fri May 17 21:52:45 2024 ] 	Batch(2200/2353) done. Loss: 1.5260  lr:0.010000
[ Fri May 17 21:53:22 2024 ] 	Batch(2300/2353) done. Loss: 1.4613  lr:0.010000
[ Fri May 17 21:53:41 2024 ] 	Mean training loss: 1.5949.
[ Fri May 17 21:53:41 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 21:53:41 2024 ] Training epoch: 3
[ Fri May 17 21:53:42 2024 ] 	Batch(0/2353) done. Loss: 0.9550  lr:0.010000
[ Fri May 17 21:54:18 2024 ] 	Batch(100/2353) done. Loss: 1.5120  lr:0.010000
[ Fri May 17 21:54:55 2024 ] 	Batch(200/2353) done. Loss: 1.1528  lr:0.010000
[ Fri May 17 21:55:32 2024 ] 	Batch(300/2353) done. Loss: 0.9822  lr:0.010000
[ Fri May 17 21:56:09 2024 ] 	Batch(400/2353) done. Loss: 1.2190  lr:0.010000
[ Fri May 17 21:56:46 2024 ] 	Batch(500/2353) done. Loss: 0.8745  lr:0.010000
[ Fri May 17 21:57:22 2024 ] 	Batch(600/2353) done. Loss: 1.6285  lr:0.010000
[ Fri May 17 21:57:59 2024 ] 	Batch(700/2353) done. Loss: 1.8462  lr:0.010000
[ Fri May 17 21:58:36 2024 ] 	Batch(800/2353) done. Loss: 1.3609  lr:0.010000
[ Fri May 17 21:59:12 2024 ] 	Batch(900/2353) done. Loss: 1.3762  lr:0.010000
[ Fri May 17 21:59:49 2024 ] 	Batch(1000/2353) done. Loss: 1.7890  lr:0.010000
[ Fri May 17 22:00:26 2024 ] 	Batch(1100/2353) done. Loss: 1.2687  lr:0.010000
[ Fri May 17 22:01:02 2024 ] 	Batch(1200/2353) done. Loss: 0.7387  lr:0.010000
[ Fri May 17 22:01:40 2024 ] 	Batch(1300/2353) done. Loss: 1.6324  lr:0.010000
[ Fri May 17 22:02:17 2024 ] 	Batch(1400/2353) done. Loss: 1.0639  lr:0.010000
[ Fri May 17 22:02:53 2024 ] 	Batch(1500/2353) done. Loss: 1.4461  lr:0.010000
[ Fri May 17 22:03:30 2024 ] 	Batch(1600/2353) done. Loss: 0.8354  lr:0.010000
[ Fri May 17 22:04:07 2024 ] 	Batch(1700/2353) done. Loss: 1.7957  lr:0.010000
[ Fri May 17 22:04:43 2024 ] 	Batch(1800/2353) done. Loss: 1.6569  lr:0.010000
[ Fri May 17 22:05:20 2024 ] 	Batch(1900/2353) done. Loss: 1.4465  lr:0.010000
[ Fri May 17 22:05:57 2024 ] 	Batch(2000/2353) done. Loss: 2.0062  lr:0.010000
[ Fri May 17 22:06:33 2024 ] 	Batch(2100/2353) done. Loss: 1.1920  lr:0.010000
[ Fri May 17 22:07:10 2024 ] 	Batch(2200/2353) done. Loss: 0.5282  lr:0.010000
[ Fri May 17 22:07:47 2024 ] 	Batch(2300/2353) done. Loss: 1.7369  lr:0.010000
[ Fri May 17 22:08:06 2024 ] 	Mean training loss: 1.2988.
[ Fri May 17 22:08:06 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 22:08:06 2024 ] Training epoch: 4
[ Fri May 17 22:08:07 2024 ] 	Batch(0/2353) done. Loss: 0.7343  lr:0.010000
[ Fri May 17 22:08:44 2024 ] 	Batch(100/2353) done. Loss: 0.9716  lr:0.010000
[ Fri May 17 22:09:20 2024 ] 	Batch(200/2353) done. Loss: 1.3424  lr:0.010000
[ Fri May 17 22:09:57 2024 ] 	Batch(300/2353) done. Loss: 1.5763  lr:0.010000
[ Fri May 17 22:10:34 2024 ] 	Batch(400/2353) done. Loss: 1.1183  lr:0.010000
[ Fri May 17 22:11:10 2024 ] 	Batch(500/2353) done. Loss: 0.5663  lr:0.010000
[ Fri May 17 22:11:47 2024 ] 	Batch(600/2353) done. Loss: 0.6677  lr:0.010000
[ Fri May 17 22:12:24 2024 ] 	Batch(700/2353) done. Loss: 1.4254  lr:0.010000
[ Fri May 17 22:13:01 2024 ] 	Batch(800/2353) done. Loss: 1.0455  lr:0.010000
[ Fri May 17 22:13:38 2024 ] 	Batch(900/2353) done. Loss: 1.3600  lr:0.010000
[ Fri May 17 22:14:15 2024 ] 	Batch(1000/2353) done. Loss: 1.5925  lr:0.010000
[ Fri May 17 22:14:52 2024 ] 	Batch(1100/2353) done. Loss: 0.8544  lr:0.010000
[ Fri May 17 22:15:30 2024 ] 	Batch(1200/2353) done. Loss: 1.0762  lr:0.010000
[ Fri May 17 22:16:07 2024 ] 	Batch(1300/2353) done. Loss: 0.6822  lr:0.010000
[ Fri May 17 22:16:44 2024 ] 	Batch(1400/2353) done. Loss: 2.0815  lr:0.010000
[ Fri May 17 22:17:21 2024 ] 	Batch(1500/2353) done. Loss: 0.9538  lr:0.010000
[ Fri May 17 22:17:58 2024 ] 	Batch(1600/2353) done. Loss: 0.9015  lr:0.010000
[ Fri May 17 22:18:35 2024 ] 	Batch(1700/2353) done. Loss: 1.5301  lr:0.010000
[ Fri May 17 22:19:12 2024 ] 	Batch(1800/2353) done. Loss: 1.0834  lr:0.010000
[ Fri May 17 22:19:48 2024 ] 	Batch(1900/2353) done. Loss: 2.1539  lr:0.010000
[ Fri May 17 22:20:25 2024 ] 	Batch(2000/2353) done. Loss: 0.7592  lr:0.010000
[ Fri May 17 22:21:02 2024 ] 	Batch(2100/2353) done. Loss: 0.8299  lr:0.010000
[ Fri May 17 22:21:40 2024 ] 	Batch(2200/2353) done. Loss: 1.1708  lr:0.010000
[ Fri May 17 22:22:17 2024 ] 	Batch(2300/2353) done. Loss: 1.0394  lr:0.010000
[ Fri May 17 22:22:37 2024 ] 	Mean training loss: 1.1249.
[ Fri May 17 22:22:37 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 22:22:37 2024 ] Training epoch: 5
[ Fri May 17 22:22:37 2024 ] 	Batch(0/2353) done. Loss: 1.1579  lr:0.010000
[ Fri May 17 22:23:14 2024 ] 	Batch(100/2353) done. Loss: 1.1115  lr:0.010000
[ Fri May 17 22:23:51 2024 ] 	Batch(200/2353) done. Loss: 0.4809  lr:0.010000
[ Fri May 17 22:24:27 2024 ] 	Batch(300/2353) done. Loss: 0.5472  lr:0.010000
[ Fri May 17 22:25:04 2024 ] 	Batch(400/2353) done. Loss: 1.3530  lr:0.010000
[ Fri May 17 22:25:41 2024 ] 	Batch(500/2353) done. Loss: 1.2827  lr:0.010000
[ Fri May 17 22:26:17 2024 ] 	Batch(600/2353) done. Loss: 1.5017  lr:0.010000
[ Fri May 17 22:26:54 2024 ] 	Batch(700/2353) done. Loss: 1.1962  lr:0.010000
[ Fri May 17 22:27:31 2024 ] 	Batch(800/2353) done. Loss: 1.0995  lr:0.010000
[ Fri May 17 22:28:07 2024 ] 	Batch(900/2353) done. Loss: 0.7461  lr:0.010000
[ Fri May 17 22:28:44 2024 ] 	Batch(1000/2353) done. Loss: 1.0736  lr:0.010000
[ Fri May 17 22:29:20 2024 ] 	Batch(1100/2353) done. Loss: 0.9685  lr:0.010000
[ Fri May 17 22:29:57 2024 ] 	Batch(1200/2353) done. Loss: 0.7481  lr:0.010000
[ Fri May 17 22:30:34 2024 ] 	Batch(1300/2353) done. Loss: 1.0624  lr:0.010000
[ Fri May 17 22:31:11 2024 ] 	Batch(1400/2353) done. Loss: 0.7763  lr:0.010000
[ Fri May 17 22:31:47 2024 ] 	Batch(1500/2353) done. Loss: 0.6318  lr:0.010000
[ Fri May 17 22:32:24 2024 ] 	Batch(1600/2353) done. Loss: 0.5489  lr:0.010000
[ Fri May 17 22:33:00 2024 ] 	Batch(1700/2353) done. Loss: 1.0577  lr:0.010000
[ Fri May 17 22:33:37 2024 ] 	Batch(1800/2353) done. Loss: 0.9261  lr:0.010000
[ Fri May 17 22:34:14 2024 ] 	Batch(1900/2353) done. Loss: 1.0434  lr:0.010000
[ Fri May 17 22:34:50 2024 ] 	Batch(2000/2353) done. Loss: 1.5547  lr:0.010000
[ Fri May 17 22:35:27 2024 ] 	Batch(2100/2353) done. Loss: 0.9716  lr:0.010000
[ Fri May 17 22:36:04 2024 ] 	Batch(2200/2353) done. Loss: 1.3466  lr:0.010000
[ Fri May 17 22:36:40 2024 ] 	Batch(2300/2353) done. Loss: 0.8086  lr:0.010000
[ Fri May 17 22:36:59 2024 ] 	Mean training loss: 1.0173.
[ Fri May 17 22:36:59 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Fri May 17 22:37:00 2024 ] Training epoch: 6
[ Fri May 17 22:37:00 2024 ] 	Batch(0/2353) done. Loss: 1.8143  lr:0.010000
[ Fri May 17 22:37:37 2024 ] 	Batch(100/2353) done. Loss: 0.8734  lr:0.010000
[ Fri May 17 22:38:14 2024 ] 	Batch(200/2353) done. Loss: 0.8508  lr:0.010000
[ Fri May 17 22:38:51 2024 ] 	Batch(300/2353) done. Loss: 0.6426  lr:0.010000
[ Fri May 17 22:39:28 2024 ] 	Batch(400/2353) done. Loss: 0.9777  lr:0.010000
[ Fri May 17 22:40:05 2024 ] 	Batch(500/2353) done. Loss: 0.5785  lr:0.010000
[ Fri May 17 22:40:42 2024 ] 	Batch(600/2353) done. Loss: 1.2777  lr:0.010000
[ Fri May 17 22:41:18 2024 ] 	Batch(700/2353) done. Loss: 0.5980  lr:0.010000
[ Fri May 17 22:41:55 2024 ] 	Batch(800/2353) done. Loss: 1.0771  lr:0.010000
[ Fri May 17 22:42:32 2024 ] 	Batch(900/2353) done. Loss: 0.9957  lr:0.010000
[ Fri May 17 22:43:08 2024 ] 	Batch(1000/2353) done. Loss: 0.9715  lr:0.010000
[ Fri May 17 22:43:45 2024 ] 	Batch(1100/2353) done. Loss: 0.6419  lr:0.010000
[ Fri May 17 22:44:22 2024 ] 	Batch(1200/2353) done. Loss: 0.9949  lr:0.010000
[ Fri May 17 22:44:58 2024 ] 	Batch(1300/2353) done. Loss: 0.7462  lr:0.010000
[ Fri May 17 22:45:35 2024 ] 	Batch(1400/2353) done. Loss: 0.7582  lr:0.010000
[ Fri May 17 22:46:12 2024 ] 	Batch(1500/2353) done. Loss: 0.8322  lr:0.010000
[ Fri May 17 22:46:48 2024 ] 	Batch(1600/2353) done. Loss: 0.6435  lr:0.010000
[ Fri May 17 22:47:26 2024 ] 	Batch(1700/2353) done. Loss: 0.5271  lr:0.010000
[ Fri May 17 22:48:03 2024 ] 	Batch(1800/2353) done. Loss: 1.1065  lr:0.010000
[ Fri May 17 22:48:40 2024 ] 	Batch(1900/2353) done. Loss: 1.0579  lr:0.010000
[ Fri May 17 22:49:18 2024 ] 	Batch(2000/2353) done. Loss: 0.5341  lr:0.010000
[ Fri May 17 22:49:54 2024 ] 	Batch(2100/2353) done. Loss: 0.9023  lr:0.010000
[ Fri May 17 22:50:31 2024 ] 	Batch(2200/2353) done. Loss: 0.5908  lr:0.010000
[ Fri May 17 22:51:08 2024 ] 	Batch(2300/2353) done. Loss: 0.6712  lr:0.010000
[ Fri May 17 22:51:27 2024 ] 	Mean training loss: 0.9219.
[ Fri May 17 22:51:27 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 22:51:27 2024 ] Training epoch: 7
[ Fri May 17 22:51:28 2024 ] 	Batch(0/2353) done. Loss: 0.2815  lr:0.010000
[ Fri May 17 22:52:05 2024 ] 	Batch(100/2353) done. Loss: 0.8108  lr:0.010000
[ Fri May 17 22:52:42 2024 ] 	Batch(200/2353) done. Loss: 0.3386  lr:0.010000
[ Fri May 17 22:53:20 2024 ] 	Batch(300/2353) done. Loss: 1.0186  lr:0.010000
[ Fri May 17 22:53:57 2024 ] 	Batch(400/2353) done. Loss: 0.7659  lr:0.010000
[ Fri May 17 22:54:34 2024 ] 	Batch(500/2353) done. Loss: 1.4322  lr:0.010000
[ Fri May 17 22:55:11 2024 ] 	Batch(600/2353) done. Loss: 0.9841  lr:0.010000
[ Fri May 17 22:55:47 2024 ] 	Batch(700/2353) done. Loss: 0.4209  lr:0.010000
[ Fri May 17 22:56:24 2024 ] 	Batch(800/2353) done. Loss: 0.9364  lr:0.010000
[ Fri May 17 22:57:01 2024 ] 	Batch(900/2353) done. Loss: 0.5073  lr:0.010000
[ Fri May 17 22:57:37 2024 ] 	Batch(1000/2353) done. Loss: 1.0103  lr:0.010000
[ Fri May 17 22:58:14 2024 ] 	Batch(1100/2353) done. Loss: 0.8844  lr:0.010000
[ Fri May 17 22:58:51 2024 ] 	Batch(1200/2353) done. Loss: 0.3312  lr:0.010000
[ Fri May 17 22:59:28 2024 ] 	Batch(1300/2353) done. Loss: 1.4752  lr:0.010000
[ Fri May 17 23:00:04 2024 ] 	Batch(1400/2353) done. Loss: 0.7924  lr:0.010000
[ Fri May 17 23:00:41 2024 ] 	Batch(1500/2353) done. Loss: 0.9664  lr:0.010000
[ Fri May 17 23:01:18 2024 ] 	Batch(1600/2353) done. Loss: 0.3406  lr:0.010000
[ Fri May 17 23:01:54 2024 ] 	Batch(1700/2353) done. Loss: 0.6300  lr:0.010000
[ Fri May 17 23:02:31 2024 ] 	Batch(1800/2353) done. Loss: 0.5202  lr:0.010000
[ Fri May 17 23:03:08 2024 ] 	Batch(1900/2353) done. Loss: 1.1208  lr:0.010000
[ Fri May 17 23:03:44 2024 ] 	Batch(2000/2353) done. Loss: 0.5432  lr:0.010000
[ Fri May 17 23:04:21 2024 ] 	Batch(2100/2353) done. Loss: 0.4945  lr:0.010000
[ Fri May 17 23:04:58 2024 ] 	Batch(2200/2353) done. Loss: 0.9796  lr:0.010000
[ Fri May 17 23:05:35 2024 ] 	Batch(2300/2353) done. Loss: 1.1504  lr:0.010000
[ Fri May 17 23:05:54 2024 ] 	Mean training loss: 0.8882.
[ Fri May 17 23:05:54 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 23:05:54 2024 ] Training epoch: 8
[ Fri May 17 23:05:55 2024 ] 	Batch(0/2353) done. Loss: 1.5273  lr:0.010000
[ Fri May 17 23:06:32 2024 ] 	Batch(100/2353) done. Loss: 0.9169  lr:0.010000
[ Fri May 17 23:07:09 2024 ] 	Batch(200/2353) done. Loss: 0.7305  lr:0.010000
[ Fri May 17 23:07:46 2024 ] 	Batch(300/2353) done. Loss: 0.5685  lr:0.010000
[ Fri May 17 23:08:23 2024 ] 	Batch(400/2353) done. Loss: 0.4985  lr:0.010000
[ Fri May 17 23:08:59 2024 ] 	Batch(500/2353) done. Loss: 0.5886  lr:0.010000
[ Fri May 17 23:09:36 2024 ] 	Batch(600/2353) done. Loss: 0.7039  lr:0.010000
[ Fri May 17 23:10:13 2024 ] 	Batch(700/2353) done. Loss: 0.8778  lr:0.010000
[ Fri May 17 23:10:50 2024 ] 	Batch(800/2353) done. Loss: 1.3622  lr:0.010000
[ Fri May 17 23:11:27 2024 ] 	Batch(900/2353) done. Loss: 1.2788  lr:0.010000
[ Fri May 17 23:12:04 2024 ] 	Batch(1000/2353) done. Loss: 0.4776  lr:0.010000
[ Fri May 17 23:12:41 2024 ] 	Batch(1100/2353) done. Loss: 1.0848  lr:0.010000
[ Fri May 17 23:13:17 2024 ] 	Batch(1200/2353) done. Loss: 0.7977  lr:0.010000
[ Fri May 17 23:13:54 2024 ] 	Batch(1300/2353) done. Loss: 0.6834  lr:0.010000
[ Fri May 17 23:14:31 2024 ] 	Batch(1400/2353) done. Loss: 0.9702  lr:0.010000
[ Fri May 17 23:15:07 2024 ] 	Batch(1500/2353) done. Loss: 1.1973  lr:0.010000
[ Fri May 17 23:15:44 2024 ] 	Batch(1600/2353) done. Loss: 0.7747  lr:0.010000
[ Fri May 17 23:16:21 2024 ] 	Batch(1700/2353) done. Loss: 0.7100  lr:0.010000
[ Fri May 17 23:16:57 2024 ] 	Batch(1800/2353) done. Loss: 1.0489  lr:0.010000
[ Fri May 17 23:17:34 2024 ] 	Batch(1900/2353) done. Loss: 0.9203  lr:0.010000
[ Fri May 17 23:18:11 2024 ] 	Batch(2000/2353) done. Loss: 0.4036  lr:0.010000
[ Fri May 17 23:18:47 2024 ] 	Batch(2100/2353) done. Loss: 0.3146  lr:0.010000
[ Fri May 17 23:19:24 2024 ] 	Batch(2200/2353) done. Loss: 0.4056  lr:0.010000
[ Fri May 17 23:20:01 2024 ] 	Batch(2300/2353) done. Loss: 0.1377  lr:0.010000
[ Fri May 17 23:20:20 2024 ] 	Mean training loss: 0.8141.
[ Fri May 17 23:20:20 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Fri May 17 23:20:20 2024 ] Training epoch: 9
[ Fri May 17 23:20:21 2024 ] 	Batch(0/2353) done. Loss: 1.1481  lr:0.010000
[ Fri May 17 23:20:57 2024 ] 	Batch(100/2353) done. Loss: 0.7685  lr:0.010000
[ Fri May 17 23:21:34 2024 ] 	Batch(200/2353) done. Loss: 0.7439  lr:0.010000
[ Fri May 17 23:22:10 2024 ] 	Batch(300/2353) done. Loss: 0.9965  lr:0.010000
[ Fri May 17 23:22:47 2024 ] 	Batch(400/2353) done. Loss: 0.4416  lr:0.010000
[ Fri May 17 23:23:24 2024 ] 	Batch(500/2353) done. Loss: 0.8107  lr:0.010000
[ Fri May 17 23:24:01 2024 ] 	Batch(600/2353) done. Loss: 0.9506  lr:0.010000
[ Fri May 17 23:24:37 2024 ] 	Batch(700/2353) done. Loss: 0.6183  lr:0.010000
[ Fri May 17 23:25:14 2024 ] 	Batch(800/2353) done. Loss: 0.6983  lr:0.010000
[ Fri May 17 23:25:51 2024 ] 	Batch(900/2353) done. Loss: 0.8669  lr:0.010000
[ Fri May 17 23:26:27 2024 ] 	Batch(1000/2353) done. Loss: 1.0190  lr:0.010000
[ Fri May 17 23:27:04 2024 ] 	Batch(1100/2353) done. Loss: 0.6790  lr:0.010000
[ Fri May 17 23:27:40 2024 ] 	Batch(1200/2353) done. Loss: 0.9645  lr:0.010000
[ Fri May 17 23:28:17 2024 ] 	Batch(1300/2353) done. Loss: 1.1306  lr:0.010000
[ Fri May 17 23:28:54 2024 ] 	Batch(1400/2353) done. Loss: 1.0124  lr:0.010000
[ Fri May 17 23:29:30 2024 ] 	Batch(1500/2353) done. Loss: 0.7971  lr:0.010000
[ Fri May 17 23:30:07 2024 ] 	Batch(1600/2353) done. Loss: 0.5486  lr:0.010000
[ Fri May 17 23:30:44 2024 ] 	Batch(1700/2353) done. Loss: 0.4231  lr:0.010000
[ Fri May 17 23:31:20 2024 ] 	Batch(1800/2353) done. Loss: 0.9347  lr:0.010000
[ Fri May 17 23:31:57 2024 ] 	Batch(1900/2353) done. Loss: 0.6517  lr:0.010000
[ Fri May 17 23:32:34 2024 ] 	Batch(2000/2353) done. Loss: 1.2155  lr:0.010000
[ Fri May 17 23:33:11 2024 ] 	Batch(2100/2353) done. Loss: 0.8426  lr:0.010000
[ Fri May 17 23:33:48 2024 ] 	Batch(2200/2353) done. Loss: 0.8825  lr:0.010000
[ Fri May 17 23:34:24 2024 ] 	Batch(2300/2353) done. Loss: 0.7894  lr:0.010000
[ Fri May 17 23:34:44 2024 ] 	Mean training loss: 0.7581.
[ Fri May 17 23:34:44 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Fri May 17 23:34:44 2024 ] Training epoch: 10
[ Fri May 17 23:34:44 2024 ] 	Batch(0/2353) done. Loss: 0.8585  lr:0.010000
[ Fri May 17 23:35:21 2024 ] 	Batch(100/2353) done. Loss: 0.4100  lr:0.010000
[ Fri May 17 23:35:58 2024 ] 	Batch(200/2353) done. Loss: 0.6691  lr:0.010000
[ Fri May 17 23:36:36 2024 ] 	Batch(300/2353) done. Loss: 0.3018  lr:0.010000
[ Fri May 17 23:37:13 2024 ] 	Batch(400/2353) done. Loss: 0.9707  lr:0.010000
[ Fri May 17 23:37:50 2024 ] 	Batch(500/2353) done. Loss: 0.6237  lr:0.010000
[ Fri May 17 23:38:27 2024 ] 	Batch(600/2353) done. Loss: 0.8021  lr:0.010000
[ Fri May 17 23:39:04 2024 ] 	Batch(700/2353) done. Loss: 1.4428  lr:0.010000
[ Fri May 17 23:39:41 2024 ] 	Batch(800/2353) done. Loss: 0.9358  lr:0.010000
[ Fri May 17 23:40:17 2024 ] 	Batch(900/2353) done. Loss: 0.4321  lr:0.010000
[ Fri May 17 23:40:54 2024 ] 	Batch(1000/2353) done. Loss: 0.4714  lr:0.010000
[ Fri May 17 23:41:30 2024 ] 	Batch(1100/2353) done. Loss: 0.7276  lr:0.010000
[ Fri May 17 23:42:07 2024 ] 	Batch(1200/2353) done. Loss: 0.6808  lr:0.010000
[ Fri May 17 23:42:44 2024 ] 	Batch(1300/2353) done. Loss: 1.3621  lr:0.010000
[ Fri May 17 23:43:20 2024 ] 	Batch(1400/2353) done. Loss: 0.4168  lr:0.010000
[ Fri May 17 23:43:57 2024 ] 	Batch(1500/2353) done. Loss: 0.9370  lr:0.010000
[ Fri May 17 23:44:34 2024 ] 	Batch(1600/2353) done. Loss: 0.9626  lr:0.010000
[ Fri May 17 23:45:10 2024 ] 	Batch(1700/2353) done. Loss: 0.5336  lr:0.010000
[ Fri May 17 23:45:47 2024 ] 	Batch(1800/2353) done. Loss: 0.6874  lr:0.010000
[ Fri May 17 23:46:23 2024 ] 	Batch(1900/2353) done. Loss: 0.4867  lr:0.010000
[ Fri May 17 23:47:01 2024 ] 	Batch(2000/2353) done. Loss: 0.5808  lr:0.010000
[ Fri May 17 23:47:38 2024 ] 	Batch(2100/2353) done. Loss: 0.4023  lr:0.010000
[ Fri May 17 23:48:16 2024 ] 	Batch(2200/2353) done. Loss: 0.4231  lr:0.010000
[ Fri May 17 23:48:53 2024 ] 	Batch(2300/2353) done. Loss: 1.4342  lr:0.010000
[ Fri May 17 23:49:12 2024 ] 	Mean training loss: 0.7226.
[ Fri May 17 23:49:12 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Fri May 17 23:49:13 2024 ] Eval epoch: 10
[ Fri May 17 23:51:16 2024 ] 	Mean val loss of 2367 batches: 0.5321848748429615.
[ Fri May 17 23:51:16 2024 ] Training epoch: 11
[ Fri May 17 23:51:17 2024 ] 	Batch(0/2353) done. Loss: 0.7358  lr:0.010000
[ Fri May 17 23:51:53 2024 ] 	Batch(100/2353) done. Loss: 0.7442  lr:0.010000
[ Fri May 17 23:52:30 2024 ] 	Batch(200/2353) done. Loss: 0.2910  lr:0.010000
[ Fri May 17 23:53:06 2024 ] 	Batch(300/2353) done. Loss: 0.9784  lr:0.010000
[ Fri May 17 23:53:43 2024 ] 	Batch(400/2353) done. Loss: 0.5694  lr:0.010000
[ Fri May 17 23:54:20 2024 ] 	Batch(500/2353) done. Loss: 0.4622  lr:0.010000
[ Fri May 17 23:54:56 2024 ] 	Batch(600/2353) done. Loss: 0.5456  lr:0.010000
[ Fri May 17 23:55:33 2024 ] 	Batch(700/2353) done. Loss: 0.4490  lr:0.010000
[ Fri May 17 23:56:10 2024 ] 	Batch(800/2353) done. Loss: 0.8373  lr:0.010000
[ Fri May 17 23:56:46 2024 ] 	Batch(900/2353) done. Loss: 0.4587  lr:0.010000
[ Fri May 17 23:57:23 2024 ] 	Batch(1000/2353) done. Loss: 0.7514  lr:0.010000
[ Fri May 17 23:58:00 2024 ] 	Batch(1100/2353) done. Loss: 0.2328  lr:0.010000
[ Fri May 17 23:58:36 2024 ] 	Batch(1200/2353) done. Loss: 0.6258  lr:0.010000
[ Fri May 17 23:59:13 2024 ] 	Batch(1300/2353) done. Loss: 0.4849  lr:0.010000
[ Fri May 17 23:59:49 2024 ] 	Batch(1400/2353) done. Loss: 0.2402  lr:0.010000
[ Sat May 18 00:00:26 2024 ] 	Batch(1500/2353) done. Loss: 0.5151  lr:0.010000
[ Sat May 18 00:01:03 2024 ] 	Batch(1600/2353) done. Loss: 0.5994  lr:0.010000
[ Sat May 18 00:01:40 2024 ] 	Batch(1700/2353) done. Loss: 0.3422  lr:0.010000
[ Sat May 18 00:02:18 2024 ] 	Batch(1800/2353) done. Loss: 0.9893  lr:0.010000
[ Sat May 18 00:02:55 2024 ] 	Batch(1900/2353) done. Loss: 1.3491  lr:0.010000
[ Sat May 18 00:03:32 2024 ] 	Batch(2000/2353) done. Loss: 0.5926  lr:0.010000
[ Sat May 18 00:04:08 2024 ] 	Batch(2100/2353) done. Loss: 0.5402  lr:0.010000
[ Sat May 18 00:04:45 2024 ] 	Batch(2200/2353) done. Loss: 0.7120  lr:0.010000
[ Sat May 18 00:05:22 2024 ] 	Batch(2300/2353) done. Loss: 0.6546  lr:0.010000
[ Sat May 18 00:05:41 2024 ] 	Mean training loss: 0.6960.
[ Sat May 18 00:05:41 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 00:05:41 2024 ] Training epoch: 12
[ Sat May 18 00:05:42 2024 ] 	Batch(0/2353) done. Loss: 0.2654  lr:0.010000
[ Sat May 18 00:06:18 2024 ] 	Batch(100/2353) done. Loss: 0.4383  lr:0.010000
[ Sat May 18 00:06:55 2024 ] 	Batch(200/2353) done. Loss: 0.7590  lr:0.010000
[ Sat May 18 00:07:32 2024 ] 	Batch(300/2353) done. Loss: 0.7469  lr:0.010000
[ Sat May 18 00:08:09 2024 ] 	Batch(400/2353) done. Loss: 0.7574  lr:0.010000
[ Sat May 18 00:08:46 2024 ] 	Batch(500/2353) done. Loss: 1.1877  lr:0.010000
[ Sat May 18 00:09:22 2024 ] 	Batch(600/2353) done. Loss: 0.5921  lr:0.010000
[ Sat May 18 00:09:59 2024 ] 	Batch(700/2353) done. Loss: 0.9262  lr:0.010000
[ Sat May 18 00:10:36 2024 ] 	Batch(800/2353) done. Loss: 0.4171  lr:0.010000
[ Sat May 18 00:11:12 2024 ] 	Batch(900/2353) done. Loss: 1.0121  lr:0.010000
[ Sat May 18 00:11:49 2024 ] 	Batch(1000/2353) done. Loss: 0.2997  lr:0.010000
[ Sat May 18 00:12:26 2024 ] 	Batch(1100/2353) done. Loss: 0.9581  lr:0.010000
[ Sat May 18 00:13:03 2024 ] 	Batch(1200/2353) done. Loss: 0.8284  lr:0.010000
[ Sat May 18 00:13:40 2024 ] 	Batch(1300/2353) done. Loss: 0.3968  lr:0.010000
[ Sat May 18 00:14:16 2024 ] 	Batch(1400/2353) done. Loss: 0.5032  lr:0.010000
[ Sat May 18 00:14:53 2024 ] 	Batch(1500/2353) done. Loss: 0.3734  lr:0.010000
[ Sat May 18 00:15:30 2024 ] 	Batch(1600/2353) done. Loss: 0.6951  lr:0.010000
[ Sat May 18 00:16:06 2024 ] 	Batch(1700/2353) done. Loss: 1.0190  lr:0.010000
[ Sat May 18 00:16:43 2024 ] 	Batch(1800/2353) done. Loss: 0.4002  lr:0.010000
[ Sat May 18 00:17:20 2024 ] 	Batch(1900/2353) done. Loss: 1.0378  lr:0.010000
[ Sat May 18 00:17:56 2024 ] 	Batch(2000/2353) done. Loss: 0.5559  lr:0.010000
[ Sat May 18 00:18:33 2024 ] 	Batch(2100/2353) done. Loss: 0.3401  lr:0.010000
[ Sat May 18 00:19:10 2024 ] 	Batch(2200/2353) done. Loss: 0.5414  lr:0.010000
[ Sat May 18 00:19:46 2024 ] 	Batch(2300/2353) done. Loss: 0.5015  lr:0.010000
[ Sat May 18 00:20:05 2024 ] 	Mean training loss: 0.6649.
[ Sat May 18 00:20:05 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 00:20:05 2024 ] Training epoch: 13
[ Sat May 18 00:20:06 2024 ] 	Batch(0/2353) done. Loss: 0.3523  lr:0.010000
[ Sat May 18 00:20:43 2024 ] 	Batch(100/2353) done. Loss: 0.6298  lr:0.010000
[ Sat May 18 00:21:19 2024 ] 	Batch(200/2353) done. Loss: 0.5421  lr:0.010000
[ Sat May 18 00:21:56 2024 ] 	Batch(300/2353) done. Loss: 0.6863  lr:0.010000
[ Sat May 18 00:22:33 2024 ] 	Batch(400/2353) done. Loss: 0.6838  lr:0.010000
[ Sat May 18 00:23:09 2024 ] 	Batch(500/2353) done. Loss: 0.9919  lr:0.010000
[ Sat May 18 00:23:46 2024 ] 	Batch(600/2353) done. Loss: 0.8958  lr:0.010000
[ Sat May 18 00:24:22 2024 ] 	Batch(700/2353) done. Loss: 0.6381  lr:0.010000
[ Sat May 18 00:24:59 2024 ] 	Batch(800/2353) done. Loss: 0.4971  lr:0.010000
[ Sat May 18 00:25:36 2024 ] 	Batch(900/2353) done. Loss: 1.7430  lr:0.010000
[ Sat May 18 00:26:12 2024 ] 	Batch(1000/2353) done. Loss: 0.5634  lr:0.010000
[ Sat May 18 00:26:49 2024 ] 	Batch(1100/2353) done. Loss: 0.8052  lr:0.010000
[ Sat May 18 00:27:26 2024 ] 	Batch(1200/2353) done. Loss: 0.3762  lr:0.010000
[ Sat May 18 00:28:02 2024 ] 	Batch(1300/2353) done. Loss: 0.4409  lr:0.010000
[ Sat May 18 00:28:39 2024 ] 	Batch(1400/2353) done. Loss: 1.0954  lr:0.010000
[ Sat May 18 00:29:16 2024 ] 	Batch(1500/2353) done. Loss: 0.9586  lr:0.010000
[ Sat May 18 00:29:52 2024 ] 	Batch(1600/2353) done. Loss: 0.8591  lr:0.010000
[ Sat May 18 00:30:29 2024 ] 	Batch(1700/2353) done. Loss: 0.6347  lr:0.010000
[ Sat May 18 00:31:06 2024 ] 	Batch(1800/2353) done. Loss: 0.6342  lr:0.010000
[ Sat May 18 00:31:42 2024 ] 	Batch(1900/2353) done. Loss: 0.5073  lr:0.010000
[ Sat May 18 00:32:19 2024 ] 	Batch(2000/2353) done. Loss: 0.4935  lr:0.010000
[ Sat May 18 00:32:55 2024 ] 	Batch(2100/2353) done. Loss: 0.3343  lr:0.010000
[ Sat May 18 00:33:32 2024 ] 	Batch(2200/2353) done. Loss: 1.0532  lr:0.010000
[ Sat May 18 00:34:09 2024 ] 	Batch(2300/2353) done. Loss: 0.6934  lr:0.010000
[ Sat May 18 00:34:28 2024 ] 	Mean training loss: 0.6206.
[ Sat May 18 00:34:28 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 00:34:28 2024 ] Training epoch: 14
[ Sat May 18 00:34:29 2024 ] 	Batch(0/2353) done. Loss: 0.4045  lr:0.010000
[ Sat May 18 00:35:05 2024 ] 	Batch(100/2353) done. Loss: 0.8619  lr:0.010000
[ Sat May 18 00:35:42 2024 ] 	Batch(200/2353) done. Loss: 0.4059  lr:0.010000
[ Sat May 18 00:36:18 2024 ] 	Batch(300/2353) done. Loss: 0.9348  lr:0.010000
[ Sat May 18 00:36:55 2024 ] 	Batch(400/2353) done. Loss: 0.5772  lr:0.010000
[ Sat May 18 00:37:32 2024 ] 	Batch(500/2353) done. Loss: 0.4887  lr:0.010000
[ Sat May 18 00:38:08 2024 ] 	Batch(600/2353) done. Loss: 1.2353  lr:0.010000
[ Sat May 18 00:38:45 2024 ] 	Batch(700/2353) done. Loss: 0.5536  lr:0.010000
[ Sat May 18 00:39:22 2024 ] 	Batch(800/2353) done. Loss: 0.3809  lr:0.010000
[ Sat May 18 00:39:58 2024 ] 	Batch(900/2353) done. Loss: 0.9830  lr:0.010000
[ Sat May 18 00:40:35 2024 ] 	Batch(1000/2353) done. Loss: 0.7975  lr:0.010000
[ Sat May 18 00:41:12 2024 ] 	Batch(1100/2353) done. Loss: 0.9423  lr:0.010000
[ Sat May 18 00:41:48 2024 ] 	Batch(1200/2353) done. Loss: 1.1000  lr:0.010000
[ Sat May 18 00:42:25 2024 ] 	Batch(1300/2353) done. Loss: 0.5764  lr:0.010000
[ Sat May 18 00:43:02 2024 ] 	Batch(1400/2353) done. Loss: 0.5378  lr:0.010000
[ Sat May 18 00:43:38 2024 ] 	Batch(1500/2353) done. Loss: 0.4242  lr:0.010000
[ Sat May 18 00:44:15 2024 ] 	Batch(1600/2353) done. Loss: 0.6439  lr:0.010000
[ Sat May 18 00:44:52 2024 ] 	Batch(1700/2353) done. Loss: 0.6334  lr:0.010000
[ Sat May 18 00:45:28 2024 ] 	Batch(1800/2353) done. Loss: 0.8528  lr:0.010000
[ Sat May 18 00:46:05 2024 ] 	Batch(1900/2353) done. Loss: 0.1754  lr:0.010000
[ Sat May 18 00:46:41 2024 ] 	Batch(2000/2353) done. Loss: 0.2157  lr:0.010000
[ Sat May 18 00:47:18 2024 ] 	Batch(2100/2353) done. Loss: 0.5109  lr:0.010000
[ Sat May 18 00:47:55 2024 ] 	Batch(2200/2353) done. Loss: 0.8534  lr:0.010000
[ Sat May 18 00:48:32 2024 ] 	Batch(2300/2353) done. Loss: 0.6420  lr:0.010000
[ Sat May 18 00:48:51 2024 ] 	Mean training loss: 0.5926.
[ Sat May 18 00:48:51 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 00:48:51 2024 ] Training epoch: 15
[ Sat May 18 00:48:51 2024 ] 	Batch(0/2353) done. Loss: 0.5707  lr:0.010000
[ Sat May 18 00:49:28 2024 ] 	Batch(100/2353) done. Loss: 0.5141  lr:0.010000
[ Sat May 18 00:50:05 2024 ] 	Batch(200/2353) done. Loss: 0.3468  lr:0.010000
[ Sat May 18 00:50:41 2024 ] 	Batch(300/2353) done. Loss: 0.2903  lr:0.010000
[ Sat May 18 00:51:18 2024 ] 	Batch(400/2353) done. Loss: 0.6586  lr:0.010000
[ Sat May 18 00:51:55 2024 ] 	Batch(500/2353) done. Loss: 0.6995  lr:0.010000
[ Sat May 18 00:52:31 2024 ] 	Batch(600/2353) done. Loss: 0.4823  lr:0.010000
[ Sat May 18 00:53:08 2024 ] 	Batch(700/2353) done. Loss: 0.5513  lr:0.010000
[ Sat May 18 00:53:45 2024 ] 	Batch(800/2353) done. Loss: 0.2115  lr:0.010000
[ Sat May 18 00:54:22 2024 ] 	Batch(900/2353) done. Loss: 0.1112  lr:0.010000
[ Sat May 18 00:54:59 2024 ] 	Batch(1000/2353) done. Loss: 0.8056  lr:0.010000
[ Sat May 18 00:55:36 2024 ] 	Batch(1100/2353) done. Loss: 0.5081  lr:0.010000
[ Sat May 18 00:56:13 2024 ] 	Batch(1200/2353) done. Loss: 0.4032  lr:0.010000
[ Sat May 18 00:56:49 2024 ] 	Batch(1300/2353) done. Loss: 0.5517  lr:0.010000
[ Sat May 18 00:57:26 2024 ] 	Batch(1400/2353) done. Loss: 0.4735  lr:0.010000
[ Sat May 18 00:58:03 2024 ] 	Batch(1500/2353) done. Loss: 0.8513  lr:0.010000
[ Sat May 18 00:58:40 2024 ] 	Batch(1600/2353) done. Loss: 0.4548  lr:0.010000
[ Sat May 18 00:59:17 2024 ] 	Batch(1700/2353) done. Loss: 0.4880  lr:0.010000
[ Sat May 18 00:59:54 2024 ] 	Batch(1800/2353) done. Loss: 0.6816  lr:0.010000
[ Sat May 18 01:00:30 2024 ] 	Batch(1900/2353) done. Loss: 0.9322  lr:0.010000
[ Sat May 18 01:01:07 2024 ] 	Batch(2000/2353) done. Loss: 0.9543  lr:0.010000
[ Sat May 18 01:01:44 2024 ] 	Batch(2100/2353) done. Loss: 0.3934  lr:0.010000
[ Sat May 18 01:02:20 2024 ] 	Batch(2200/2353) done. Loss: 0.5664  lr:0.010000
[ Sat May 18 01:02:57 2024 ] 	Batch(2300/2353) done. Loss: 0.5809  lr:0.010000
[ Sat May 18 01:03:16 2024 ] 	Mean training loss: 0.5859.
[ Sat May 18 01:03:16 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 01:03:16 2024 ] Training epoch: 16
[ Sat May 18 01:03:17 2024 ] 	Batch(0/2353) done. Loss: 0.8168  lr:0.010000
[ Sat May 18 01:03:53 2024 ] 	Batch(100/2353) done. Loss: 0.8415  lr:0.010000
[ Sat May 18 01:04:30 2024 ] 	Batch(200/2353) done. Loss: 0.3538  lr:0.010000
[ Sat May 18 01:05:07 2024 ] 	Batch(300/2353) done. Loss: 0.3016  lr:0.010000
[ Sat May 18 01:05:43 2024 ] 	Batch(400/2353) done. Loss: 0.4563  lr:0.010000
[ Sat May 18 01:06:20 2024 ] 	Batch(500/2353) done. Loss: 0.1604  lr:0.010000
[ Sat May 18 01:06:57 2024 ] 	Batch(600/2353) done. Loss: 0.8026  lr:0.010000
[ Sat May 18 01:07:33 2024 ] 	Batch(700/2353) done. Loss: 0.7841  lr:0.010000
[ Sat May 18 01:08:10 2024 ] 	Batch(800/2353) done. Loss: 1.0598  lr:0.010000
[ Sat May 18 01:08:47 2024 ] 	Batch(900/2353) done. Loss: 0.2158  lr:0.010000
[ Sat May 18 01:09:23 2024 ] 	Batch(1000/2353) done. Loss: 0.7095  lr:0.010000
[ Sat May 18 01:10:00 2024 ] 	Batch(1100/2353) done. Loss: 0.2489  lr:0.010000
[ Sat May 18 01:10:37 2024 ] 	Batch(1200/2353) done. Loss: 0.6523  lr:0.010000
[ Sat May 18 01:11:13 2024 ] 	Batch(1300/2353) done. Loss: 0.5001  lr:0.010000
[ Sat May 18 01:11:50 2024 ] 	Batch(1400/2353) done. Loss: 0.6154  lr:0.010000
[ Sat May 18 01:12:27 2024 ] 	Batch(1500/2353) done. Loss: 0.5619  lr:0.010000
[ Sat May 18 01:13:04 2024 ] 	Batch(1600/2353) done. Loss: 0.8594  lr:0.010000
[ Sat May 18 01:13:40 2024 ] 	Batch(1700/2353) done. Loss: 0.4199  lr:0.010000
[ Sat May 18 01:14:17 2024 ] 	Batch(1800/2353) done. Loss: 1.1969  lr:0.010000
[ Sat May 18 01:14:53 2024 ] 	Batch(1900/2353) done. Loss: 0.1325  lr:0.010000
[ Sat May 18 01:15:30 2024 ] 	Batch(2000/2353) done. Loss: 1.0432  lr:0.010000
[ Sat May 18 01:16:07 2024 ] 	Batch(2100/2353) done. Loss: 0.5097  lr:0.010000
[ Sat May 18 01:16:43 2024 ] 	Batch(2200/2353) done. Loss: 0.9811  lr:0.010000
[ Sat May 18 01:17:20 2024 ] 	Batch(2300/2353) done. Loss: 0.3494  lr:0.010000
[ Sat May 18 01:17:39 2024 ] 	Mean training loss: 0.5533.
[ Sat May 18 01:17:39 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 01:17:39 2024 ] Training epoch: 17
[ Sat May 18 01:17:40 2024 ] 	Batch(0/2353) done. Loss: 0.4705  lr:0.010000
[ Sat May 18 01:18:16 2024 ] 	Batch(100/2353) done. Loss: 0.2884  lr:0.010000
[ Sat May 18 01:18:53 2024 ] 	Batch(200/2353) done. Loss: 0.5879  lr:0.010000
[ Sat May 18 01:19:30 2024 ] 	Batch(300/2353) done. Loss: 0.2618  lr:0.010000
[ Sat May 18 01:20:06 2024 ] 	Batch(400/2353) done. Loss: 0.5427  lr:0.010000
[ Sat May 18 01:20:43 2024 ] 	Batch(500/2353) done. Loss: 0.7386  lr:0.010000
[ Sat May 18 01:21:20 2024 ] 	Batch(600/2353) done. Loss: 0.6936  lr:0.010000
[ Sat May 18 01:21:56 2024 ] 	Batch(700/2353) done. Loss: 0.6347  lr:0.010000
[ Sat May 18 01:22:33 2024 ] 	Batch(800/2353) done. Loss: 0.5354  lr:0.010000
[ Sat May 18 01:23:10 2024 ] 	Batch(900/2353) done. Loss: 0.6099  lr:0.010000
[ Sat May 18 01:23:47 2024 ] 	Batch(1000/2353) done. Loss: 0.5914  lr:0.010000
[ Sat May 18 01:24:23 2024 ] 	Batch(1100/2353) done. Loss: 0.5522  lr:0.010000
[ Sat May 18 01:25:00 2024 ] 	Batch(1200/2353) done. Loss: 0.4871  lr:0.010000
[ Sat May 18 01:25:37 2024 ] 	Batch(1300/2353) done. Loss: 0.2737  lr:0.010000
[ Sat May 18 01:26:13 2024 ] 	Batch(1400/2353) done. Loss: 1.0813  lr:0.010000
[ Sat May 18 01:26:50 2024 ] 	Batch(1500/2353) done. Loss: 0.2445  lr:0.010000
[ Sat May 18 01:27:27 2024 ] 	Batch(1600/2353) done. Loss: 0.2407  lr:0.010000
[ Sat May 18 01:28:04 2024 ] 	Batch(1700/2353) done. Loss: 0.5905  lr:0.010000
[ Sat May 18 01:28:41 2024 ] 	Batch(1800/2353) done. Loss: 0.4904  lr:0.010000
[ Sat May 18 01:29:17 2024 ] 	Batch(1900/2353) done. Loss: 0.8957  lr:0.010000
[ Sat May 18 01:29:54 2024 ] 	Batch(2000/2353) done. Loss: 0.4478  lr:0.010000
[ Sat May 18 01:30:31 2024 ] 	Batch(2100/2353) done. Loss: 0.3953  lr:0.010000
[ Sat May 18 01:31:07 2024 ] 	Batch(2200/2353) done. Loss: 0.2480  lr:0.010000
[ Sat May 18 01:31:44 2024 ] 	Batch(2300/2353) done. Loss: 0.4809  lr:0.010000
[ Sat May 18 01:32:03 2024 ] 	Mean training loss: 0.5357.
[ Sat May 18 01:32:03 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 01:32:03 2024 ] Training epoch: 18
[ Sat May 18 01:32:04 2024 ] 	Batch(0/2353) done. Loss: 0.3144  lr:0.010000
[ Sat May 18 01:32:41 2024 ] 	Batch(100/2353) done. Loss: 0.1280  lr:0.010000
[ Sat May 18 01:33:17 2024 ] 	Batch(200/2353) done. Loss: 0.4217  lr:0.010000
[ Sat May 18 01:33:54 2024 ] 	Batch(300/2353) done. Loss: 0.7704  lr:0.010000
[ Sat May 18 01:34:31 2024 ] 	Batch(400/2353) done. Loss: 0.5179  lr:0.010000
[ Sat May 18 01:35:08 2024 ] 	Batch(500/2353) done. Loss: 0.9545  lr:0.010000
[ Sat May 18 01:35:44 2024 ] 	Batch(600/2353) done. Loss: 0.3421  lr:0.010000
[ Sat May 18 01:36:21 2024 ] 	Batch(700/2353) done. Loss: 0.5443  lr:0.010000
[ Sat May 18 01:36:58 2024 ] 	Batch(800/2353) done. Loss: 1.2215  lr:0.010000
[ Sat May 18 01:37:34 2024 ] 	Batch(900/2353) done. Loss: 0.6761  lr:0.010000
[ Sat May 18 01:38:11 2024 ] 	Batch(1000/2353) done. Loss: 0.6663  lr:0.010000
[ Sat May 18 01:38:48 2024 ] 	Batch(1100/2353) done. Loss: 0.5882  lr:0.010000
[ Sat May 18 01:39:24 2024 ] 	Batch(1200/2353) done. Loss: 0.7189  lr:0.010000
[ Sat May 18 01:40:01 2024 ] 	Batch(1300/2353) done. Loss: 0.2741  lr:0.010000
[ Sat May 18 01:40:38 2024 ] 	Batch(1400/2353) done. Loss: 0.4834  lr:0.010000
[ Sat May 18 01:41:14 2024 ] 	Batch(1500/2353) done. Loss: 0.5089  lr:0.010000
[ Sat May 18 01:41:51 2024 ] 	Batch(1600/2353) done. Loss: 0.4132  lr:0.010000
[ Sat May 18 01:42:28 2024 ] 	Batch(1700/2353) done. Loss: 0.3979  lr:0.010000
[ Sat May 18 01:43:04 2024 ] 	Batch(1800/2353) done. Loss: 0.4279  lr:0.010000
[ Sat May 18 01:43:41 2024 ] 	Batch(1900/2353) done. Loss: 0.4053  lr:0.010000
[ Sat May 18 01:44:18 2024 ] 	Batch(2000/2353) done. Loss: 0.4626  lr:0.010000
[ Sat May 18 01:44:54 2024 ] 	Batch(2100/2353) done. Loss: 0.5853  lr:0.010000
[ Sat May 18 01:45:31 2024 ] 	Batch(2200/2353) done. Loss: 0.4684  lr:0.010000
[ Sat May 18 01:46:08 2024 ] 	Batch(2300/2353) done. Loss: 0.5783  lr:0.010000
[ Sat May 18 01:46:27 2024 ] 	Mean training loss: 0.5149.
[ Sat May 18 01:46:27 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 01:46:27 2024 ] Training epoch: 19
[ Sat May 18 01:46:28 2024 ] 	Batch(0/2353) done. Loss: 0.7095  lr:0.010000
[ Sat May 18 01:47:04 2024 ] 	Batch(100/2353) done. Loss: 0.9833  lr:0.010000
[ Sat May 18 01:47:41 2024 ] 	Batch(200/2353) done. Loss: 0.5997  lr:0.010000
[ Sat May 18 01:48:17 2024 ] 	Batch(300/2353) done. Loss: 0.7865  lr:0.010000
[ Sat May 18 01:48:54 2024 ] 	Batch(400/2353) done. Loss: 0.5835  lr:0.010000
[ Sat May 18 01:49:31 2024 ] 	Batch(500/2353) done. Loss: 0.3123  lr:0.010000
[ Sat May 18 01:50:07 2024 ] 	Batch(600/2353) done. Loss: 0.0886  lr:0.010000
[ Sat May 18 01:50:44 2024 ] 	Batch(700/2353) done. Loss: 0.3764  lr:0.010000
[ Sat May 18 01:51:21 2024 ] 	Batch(800/2353) done. Loss: 0.8285  lr:0.010000
[ Sat May 18 01:51:57 2024 ] 	Batch(900/2353) done. Loss: 1.4490  lr:0.010000
[ Sat May 18 01:52:34 2024 ] 	Batch(1000/2353) done. Loss: 0.2502  lr:0.010000
[ Sat May 18 01:53:11 2024 ] 	Batch(1100/2353) done. Loss: 1.1096  lr:0.010000
[ Sat May 18 01:53:47 2024 ] 	Batch(1200/2353) done. Loss: 0.2180  lr:0.010000
[ Sat May 18 01:54:24 2024 ] 	Batch(1300/2353) done. Loss: 0.4447  lr:0.010000
[ Sat May 18 01:55:01 2024 ] 	Batch(1400/2353) done. Loss: 0.5658  lr:0.010000
[ Sat May 18 01:55:37 2024 ] 	Batch(1500/2353) done. Loss: 1.2859  lr:0.010000
[ Sat May 18 01:56:14 2024 ] 	Batch(1600/2353) done. Loss: 0.8200  lr:0.010000
[ Sat May 18 01:56:51 2024 ] 	Batch(1700/2353) done. Loss: 0.3866  lr:0.010000
[ Sat May 18 01:57:27 2024 ] 	Batch(1800/2353) done. Loss: 0.1003  lr:0.010000
[ Sat May 18 01:58:04 2024 ] 	Batch(1900/2353) done. Loss: 0.4155  lr:0.010000
[ Sat May 18 01:58:41 2024 ] 	Batch(2000/2353) done. Loss: 0.7580  lr:0.010000
[ Sat May 18 01:59:17 2024 ] 	Batch(2100/2353) done. Loss: 0.3861  lr:0.010000
[ Sat May 18 01:59:54 2024 ] 	Batch(2200/2353) done. Loss: 0.3992  lr:0.010000
[ Sat May 18 02:00:31 2024 ] 	Batch(2300/2353) done. Loss: 0.4605  lr:0.010000
[ Sat May 18 02:00:50 2024 ] 	Mean training loss: 0.5025.
[ Sat May 18 02:00:50 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 02:00:50 2024 ] Training epoch: 20
[ Sat May 18 02:00:51 2024 ] 	Batch(0/2353) done. Loss: 0.4334  lr:0.010000
[ Sat May 18 02:01:27 2024 ] 	Batch(100/2353) done. Loss: 0.5354  lr:0.010000
[ Sat May 18 02:02:04 2024 ] 	Batch(200/2353) done. Loss: 0.2414  lr:0.010000
[ Sat May 18 02:02:40 2024 ] 	Batch(300/2353) done. Loss: 0.7677  lr:0.010000
[ Sat May 18 02:03:17 2024 ] 	Batch(400/2353) done. Loss: 0.3088  lr:0.010000
[ Sat May 18 02:03:54 2024 ] 	Batch(500/2353) done. Loss: 0.3433  lr:0.010000
[ Sat May 18 02:04:31 2024 ] 	Batch(600/2353) done. Loss: 0.4480  lr:0.010000
[ Sat May 18 02:05:07 2024 ] 	Batch(700/2353) done. Loss: 0.8957  lr:0.010000
[ Sat May 18 02:05:44 2024 ] 	Batch(800/2353) done. Loss: 0.6411  lr:0.010000
[ Sat May 18 02:06:21 2024 ] 	Batch(900/2353) done. Loss: 0.4818  lr:0.010000
[ Sat May 18 02:06:57 2024 ] 	Batch(1000/2353) done. Loss: 0.2592  lr:0.010000
[ Sat May 18 02:07:34 2024 ] 	Batch(1100/2353) done. Loss: 0.8895  lr:0.010000
[ Sat May 18 02:08:10 2024 ] 	Batch(1200/2353) done. Loss: 0.3986  lr:0.010000
[ Sat May 18 02:08:47 2024 ] 	Batch(1300/2353) done. Loss: 0.2297  lr:0.010000
[ Sat May 18 02:09:24 2024 ] 	Batch(1400/2353) done. Loss: 0.6706  lr:0.010000
[ Sat May 18 02:10:00 2024 ] 	Batch(1500/2353) done. Loss: 0.5040  lr:0.010000
[ Sat May 18 02:10:37 2024 ] 	Batch(1600/2353) done. Loss: 0.7146  lr:0.010000
[ Sat May 18 02:11:14 2024 ] 	Batch(1700/2353) done. Loss: 0.6353  lr:0.010000
[ Sat May 18 02:11:51 2024 ] 	Batch(1800/2353) done. Loss: 0.4436  lr:0.010000
[ Sat May 18 02:12:27 2024 ] 	Batch(1900/2353) done. Loss: 0.3416  lr:0.010000
[ Sat May 18 02:13:04 2024 ] 	Batch(2000/2353) done. Loss: 0.6475  lr:0.010000
[ Sat May 18 02:13:40 2024 ] 	Batch(2100/2353) done. Loss: 0.3130  lr:0.010000
[ Sat May 18 02:14:17 2024 ] 	Batch(2200/2353) done. Loss: 0.6035  lr:0.010000
[ Sat May 18 02:14:54 2024 ] 	Batch(2300/2353) done. Loss: 0.4024  lr:0.010000
[ Sat May 18 02:15:13 2024 ] 	Mean training loss: 0.4782.
[ Sat May 18 02:15:13 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 02:15:13 2024 ] Eval epoch: 20
[ Sat May 18 02:17:16 2024 ] 	Mean val loss of 2367 batches: 0.4501011419109705.
[ Sat May 18 02:17:16 2024 ] Training epoch: 21
[ Sat May 18 02:17:17 2024 ] 	Batch(0/2353) done. Loss: 0.5231  lr:0.010000
[ Sat May 18 02:17:54 2024 ] 	Batch(100/2353) done. Loss: 0.5142  lr:0.010000
[ Sat May 18 02:18:30 2024 ] 	Batch(200/2353) done. Loss: 1.1713  lr:0.010000
[ Sat May 18 02:19:07 2024 ] 	Batch(300/2353) done. Loss: 0.9372  lr:0.010000
[ Sat May 18 02:19:44 2024 ] 	Batch(400/2353) done. Loss: 0.3248  lr:0.010000
[ Sat May 18 02:20:22 2024 ] 	Batch(500/2353) done. Loss: 0.7160  lr:0.010000
[ Sat May 18 02:20:59 2024 ] 	Batch(600/2353) done. Loss: 0.2695  lr:0.010000
[ Sat May 18 02:21:36 2024 ] 	Batch(700/2353) done. Loss: 0.4238  lr:0.010000
[ Sat May 18 02:22:13 2024 ] 	Batch(800/2353) done. Loss: 0.1835  lr:0.010000
[ Sat May 18 02:22:50 2024 ] 	Batch(900/2353) done. Loss: 0.1925  lr:0.010000
[ Sat May 18 02:23:27 2024 ] 	Batch(1000/2353) done. Loss: 0.3271  lr:0.010000
[ Sat May 18 02:24:05 2024 ] 	Batch(1100/2353) done. Loss: 0.3182  lr:0.010000
[ Sat May 18 02:24:42 2024 ] 	Batch(1200/2353) done. Loss: 0.1599  lr:0.010000
[ Sat May 18 02:25:19 2024 ] 	Batch(1300/2353) done. Loss: 0.4941  lr:0.010000
[ Sat May 18 02:25:56 2024 ] 	Batch(1400/2353) done. Loss: 0.3995  lr:0.010000
[ Sat May 18 02:26:33 2024 ] 	Batch(1500/2353) done. Loss: 0.2357  lr:0.010000
[ Sat May 18 02:27:10 2024 ] 	Batch(1600/2353) done. Loss: 0.2230  lr:0.010000
[ Sat May 18 02:27:48 2024 ] 	Batch(1700/2353) done. Loss: 0.2568  lr:0.010000
[ Sat May 18 02:28:25 2024 ] 	Batch(1800/2353) done. Loss: 0.2769  lr:0.010000
[ Sat May 18 02:29:02 2024 ] 	Batch(1900/2353) done. Loss: 0.0777  lr:0.010000
[ Sat May 18 02:29:39 2024 ] 	Batch(2000/2353) done. Loss: 0.0668  lr:0.010000
[ Sat May 18 02:30:16 2024 ] 	Batch(2100/2353) done. Loss: 0.5470  lr:0.010000
[ Sat May 18 02:30:53 2024 ] 	Batch(2200/2353) done. Loss: 0.4905  lr:0.010000
[ Sat May 18 02:31:30 2024 ] 	Batch(2300/2353) done. Loss: 0.1270  lr:0.010000
[ Sat May 18 02:31:50 2024 ] 	Mean training loss: 0.4641.
[ Sat May 18 02:31:50 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 02:31:50 2024 ] Training epoch: 22
[ Sat May 18 02:31:51 2024 ] 	Batch(0/2353) done. Loss: 0.1302  lr:0.010000
[ Sat May 18 02:32:28 2024 ] 	Batch(100/2353) done. Loss: 0.2468  lr:0.010000
[ Sat May 18 02:33:05 2024 ] 	Batch(200/2353) done. Loss: 0.5568  lr:0.010000
[ Sat May 18 02:33:42 2024 ] 	Batch(300/2353) done. Loss: 0.1537  lr:0.010000
[ Sat May 18 02:34:19 2024 ] 	Batch(400/2353) done. Loss: 0.7220  lr:0.010000
[ Sat May 18 02:34:56 2024 ] 	Batch(500/2353) done. Loss: 0.7924  lr:0.010000
[ Sat May 18 02:35:33 2024 ] 	Batch(600/2353) done. Loss: 0.1849  lr:0.010000
[ Sat May 18 02:36:10 2024 ] 	Batch(700/2353) done. Loss: 0.2342  lr:0.010000
[ Sat May 18 02:36:48 2024 ] 	Batch(800/2353) done. Loss: 0.5259  lr:0.010000
[ Sat May 18 02:37:25 2024 ] 	Batch(900/2353) done. Loss: 0.4632  lr:0.010000
[ Sat May 18 02:38:02 2024 ] 	Batch(1000/2353) done. Loss: 0.6534  lr:0.010000
[ Sat May 18 02:38:39 2024 ] 	Batch(1100/2353) done. Loss: 0.0371  lr:0.010000
[ Sat May 18 02:39:16 2024 ] 	Batch(1200/2353) done. Loss: 0.0816  lr:0.010000
[ Sat May 18 02:39:52 2024 ] 	Batch(1300/2353) done. Loss: 0.3852  lr:0.010000
[ Sat May 18 02:40:29 2024 ] 	Batch(1400/2353) done. Loss: 0.1830  lr:0.010000
[ Sat May 18 02:41:06 2024 ] 	Batch(1500/2353) done. Loss: 0.4065  lr:0.010000
[ Sat May 18 02:41:43 2024 ] 	Batch(1600/2353) done. Loss: 0.4939  lr:0.010000
[ Sat May 18 02:42:19 2024 ] 	Batch(1700/2353) done. Loss: 0.1876  lr:0.010000
[ Sat May 18 02:42:56 2024 ] 	Batch(1800/2353) done. Loss: 0.4246  lr:0.010000
[ Sat May 18 02:43:33 2024 ] 	Batch(1900/2353) done. Loss: 0.4120  lr:0.010000
[ Sat May 18 02:44:10 2024 ] 	Batch(2000/2353) done. Loss: 1.1739  lr:0.010000
[ Sat May 18 02:44:46 2024 ] 	Batch(2100/2353) done. Loss: 0.7660  lr:0.010000
[ Sat May 18 02:45:23 2024 ] 	Batch(2200/2353) done. Loss: 0.3433  lr:0.010000
[ Sat May 18 02:45:59 2024 ] 	Batch(2300/2353) done. Loss: 0.4739  lr:0.010000
[ Sat May 18 02:46:19 2024 ] 	Mean training loss: 0.4465.
[ Sat May 18 02:46:19 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 02:46:19 2024 ] Training epoch: 23
[ Sat May 18 02:46:19 2024 ] 	Batch(0/2353) done. Loss: 0.2135  lr:0.010000
[ Sat May 18 02:46:57 2024 ] 	Batch(100/2353) done. Loss: 0.2306  lr:0.010000
[ Sat May 18 02:47:34 2024 ] 	Batch(200/2353) done. Loss: 0.1556  lr:0.010000
[ Sat May 18 02:48:11 2024 ] 	Batch(300/2353) done. Loss: 0.5163  lr:0.010000
[ Sat May 18 02:48:49 2024 ] 	Batch(400/2353) done. Loss: 0.8033  lr:0.010000
[ Sat May 18 02:49:26 2024 ] 	Batch(500/2353) done. Loss: 0.0653  lr:0.010000
[ Sat May 18 02:50:03 2024 ] 	Batch(600/2353) done. Loss: 0.2941  lr:0.010000
[ Sat May 18 02:50:41 2024 ] 	Batch(700/2353) done. Loss: 0.3883  lr:0.010000
[ Sat May 18 02:51:18 2024 ] 	Batch(800/2353) done. Loss: 0.2926  lr:0.010000
[ Sat May 18 02:51:55 2024 ] 	Batch(900/2353) done. Loss: 0.0825  lr:0.010000
[ Sat May 18 02:52:31 2024 ] 	Batch(1000/2353) done. Loss: 0.2944  lr:0.010000
[ Sat May 18 02:53:08 2024 ] 	Batch(1100/2353) done. Loss: 0.2146  lr:0.010000
[ Sat May 18 02:53:45 2024 ] 	Batch(1200/2353) done. Loss: 0.4721  lr:0.010000
[ Sat May 18 02:54:21 2024 ] 	Batch(1300/2353) done. Loss: 0.5185  lr:0.010000
[ Sat May 18 02:54:58 2024 ] 	Batch(1400/2353) done. Loss: 0.5005  lr:0.010000
[ Sat May 18 02:55:35 2024 ] 	Batch(1500/2353) done. Loss: 0.1062  lr:0.010000
[ Sat May 18 02:56:12 2024 ] 	Batch(1600/2353) done. Loss: 0.7407  lr:0.010000
[ Sat May 18 02:56:48 2024 ] 	Batch(1700/2353) done. Loss: 0.1184  lr:0.010000
[ Sat May 18 02:57:25 2024 ] 	Batch(1800/2353) done. Loss: 0.1559  lr:0.010000
[ Sat May 18 02:58:02 2024 ] 	Batch(1900/2353) done. Loss: 0.3238  lr:0.010000
[ Sat May 18 02:58:38 2024 ] 	Batch(2000/2353) done. Loss: 0.3721  lr:0.010000
[ Sat May 18 02:59:15 2024 ] 	Batch(2100/2353) done. Loss: 0.5039  lr:0.010000
[ Sat May 18 02:59:52 2024 ] 	Batch(2200/2353) done. Loss: 0.2459  lr:0.010000
[ Sat May 18 03:00:29 2024 ] 	Batch(2300/2353) done. Loss: 0.4097  lr:0.010000
[ Sat May 18 03:00:48 2024 ] 	Mean training loss: 0.4254.
[ Sat May 18 03:00:48 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 03:00:48 2024 ] Training epoch: 24
[ Sat May 18 03:00:49 2024 ] 	Batch(0/2353) done. Loss: 0.4804  lr:0.010000
[ Sat May 18 03:01:26 2024 ] 	Batch(100/2353) done. Loss: 0.3373  lr:0.010000
[ Sat May 18 03:02:03 2024 ] 	Batch(200/2353) done. Loss: 0.4146  lr:0.010000
[ Sat May 18 03:02:40 2024 ] 	Batch(300/2353) done. Loss: 0.4798  lr:0.010000
[ Sat May 18 03:03:16 2024 ] 	Batch(400/2353) done. Loss: 0.2088  lr:0.010000
[ Sat May 18 03:03:53 2024 ] 	Batch(500/2353) done. Loss: 0.3753  lr:0.010000
[ Sat May 18 03:04:30 2024 ] 	Batch(600/2353) done. Loss: 0.2972  lr:0.010000
[ Sat May 18 03:05:06 2024 ] 	Batch(700/2353) done. Loss: 0.0846  lr:0.010000
[ Sat May 18 03:05:43 2024 ] 	Batch(800/2353) done. Loss: 0.4509  lr:0.010000
[ Sat May 18 03:06:20 2024 ] 	Batch(900/2353) done. Loss: 0.1682  lr:0.010000
[ Sat May 18 03:06:56 2024 ] 	Batch(1000/2353) done. Loss: 0.1750  lr:0.010000
[ Sat May 18 03:07:33 2024 ] 	Batch(1100/2353) done. Loss: 0.2919  lr:0.010000
[ Sat May 18 03:08:10 2024 ] 	Batch(1200/2353) done. Loss: 0.3088  lr:0.010000
[ Sat May 18 03:08:46 2024 ] 	Batch(1300/2353) done. Loss: 0.3830  lr:0.010000
[ Sat May 18 03:09:23 2024 ] 	Batch(1400/2353) done. Loss: 0.3147  lr:0.010000
[ Sat May 18 03:10:00 2024 ] 	Batch(1500/2353) done. Loss: 0.8853  lr:0.010000
[ Sat May 18 03:10:36 2024 ] 	Batch(1600/2353) done. Loss: 0.4838  lr:0.010000
[ Sat May 18 03:11:13 2024 ] 	Batch(1700/2353) done. Loss: 0.7171  lr:0.010000
[ Sat May 18 03:11:50 2024 ] 	Batch(1800/2353) done. Loss: 0.2699  lr:0.010000
[ Sat May 18 03:12:26 2024 ] 	Batch(1900/2353) done. Loss: 0.2720  lr:0.010000
[ Sat May 18 03:13:03 2024 ] 	Batch(2000/2353) done. Loss: 0.1991  lr:0.010000
[ Sat May 18 03:13:40 2024 ] 	Batch(2100/2353) done. Loss: 0.9909  lr:0.010000
[ Sat May 18 03:14:16 2024 ] 	Batch(2200/2353) done. Loss: 0.5146  lr:0.010000
[ Sat May 18 03:14:53 2024 ] 	Batch(2300/2353) done. Loss: 0.7961  lr:0.010000
[ Sat May 18 03:15:12 2024 ] 	Mean training loss: 0.4180.
[ Sat May 18 03:15:12 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 03:15:12 2024 ] Training epoch: 25
[ Sat May 18 03:15:13 2024 ] 	Batch(0/2353) done. Loss: 0.4065  lr:0.010000
[ Sat May 18 03:15:49 2024 ] 	Batch(100/2353) done. Loss: 0.2211  lr:0.010000
[ Sat May 18 03:16:26 2024 ] 	Batch(200/2353) done. Loss: 0.1758  lr:0.010000
[ Sat May 18 03:17:03 2024 ] 	Batch(300/2353) done. Loss: 0.4734  lr:0.010000
[ Sat May 18 03:17:39 2024 ] 	Batch(400/2353) done. Loss: 0.1237  lr:0.010000
[ Sat May 18 03:18:16 2024 ] 	Batch(500/2353) done. Loss: 0.3723  lr:0.010000
[ Sat May 18 03:18:53 2024 ] 	Batch(600/2353) done. Loss: 0.2248  lr:0.010000
[ Sat May 18 03:19:29 2024 ] 	Batch(700/2353) done. Loss: 0.2576  lr:0.010000
[ Sat May 18 03:20:06 2024 ] 	Batch(800/2353) done. Loss: 0.9565  lr:0.010000
[ Sat May 18 03:20:43 2024 ] 	Batch(900/2353) done. Loss: 0.4744  lr:0.010000
[ Sat May 18 03:21:19 2024 ] 	Batch(1000/2353) done. Loss: 0.3757  lr:0.010000
[ Sat May 18 03:21:56 2024 ] 	Batch(1100/2353) done. Loss: 0.3184  lr:0.010000
[ Sat May 18 03:22:33 2024 ] 	Batch(1200/2353) done. Loss: 0.1828  lr:0.010000
[ Sat May 18 03:23:09 2024 ] 	Batch(1300/2353) done. Loss: 0.2755  lr:0.010000
[ Sat May 18 03:23:46 2024 ] 	Batch(1400/2353) done. Loss: 0.5421  lr:0.010000
[ Sat May 18 03:24:23 2024 ] 	Batch(1500/2353) done. Loss: 0.2884  lr:0.010000
[ Sat May 18 03:24:59 2024 ] 	Batch(1600/2353) done. Loss: 0.4237  lr:0.010000
[ Sat May 18 03:25:36 2024 ] 	Batch(1700/2353) done. Loss: 0.4631  lr:0.010000
[ Sat May 18 03:26:13 2024 ] 	Batch(1800/2353) done. Loss: 0.4045  lr:0.010000
[ Sat May 18 03:26:49 2024 ] 	Batch(1900/2353) done. Loss: 0.4394  lr:0.010000
[ Sat May 18 03:27:26 2024 ] 	Batch(2000/2353) done. Loss: 0.3391  lr:0.010000
[ Sat May 18 03:28:03 2024 ] 	Batch(2100/2353) done. Loss: 0.6364  lr:0.010000
[ Sat May 18 03:28:39 2024 ] 	Batch(2200/2353) done. Loss: 0.2219  lr:0.010000
[ Sat May 18 03:29:16 2024 ] 	Batch(2300/2353) done. Loss: 0.3135  lr:0.010000
[ Sat May 18 03:29:35 2024 ] 	Mean training loss: 0.4059.
[ Sat May 18 03:29:35 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 03:29:35 2024 ] Training epoch: 26
[ Sat May 18 03:29:36 2024 ] 	Batch(0/2353) done. Loss: 0.3910  lr:0.010000
[ Sat May 18 03:30:12 2024 ] 	Batch(100/2353) done. Loss: 0.7764  lr:0.010000
[ Sat May 18 03:30:49 2024 ] 	Batch(200/2353) done. Loss: 0.0567  lr:0.010000
[ Sat May 18 03:31:26 2024 ] 	Batch(300/2353) done. Loss: 0.1297  lr:0.010000
[ Sat May 18 03:32:03 2024 ] 	Batch(400/2353) done. Loss: 0.3356  lr:0.010000
[ Sat May 18 03:32:39 2024 ] 	Batch(500/2353) done. Loss: 0.1030  lr:0.010000
[ Sat May 18 03:33:16 2024 ] 	Batch(600/2353) done. Loss: 0.2719  lr:0.010000
[ Sat May 18 03:33:52 2024 ] 	Batch(700/2353) done. Loss: 0.2091  lr:0.010000
[ Sat May 18 03:34:29 2024 ] 	Batch(800/2353) done. Loss: 0.1465  lr:0.010000
[ Sat May 18 03:35:06 2024 ] 	Batch(900/2353) done. Loss: 0.5457  lr:0.010000
[ Sat May 18 03:35:42 2024 ] 	Batch(1000/2353) done. Loss: 0.1743  lr:0.010000
[ Sat May 18 03:36:19 2024 ] 	Batch(1100/2353) done. Loss: 0.1548  lr:0.010000
[ Sat May 18 03:36:56 2024 ] 	Batch(1200/2353) done. Loss: 0.2024  lr:0.010000
[ Sat May 18 03:37:32 2024 ] 	Batch(1300/2353) done. Loss: 0.1455  lr:0.010000
[ Sat May 18 03:38:09 2024 ] 	Batch(1400/2353) done. Loss: 0.3969  lr:0.010000
[ Sat May 18 03:38:46 2024 ] 	Batch(1500/2353) done. Loss: 0.2930  lr:0.010000
[ Sat May 18 03:39:22 2024 ] 	Batch(1600/2353) done. Loss: 0.3816  lr:0.010000
[ Sat May 18 03:39:59 2024 ] 	Batch(1700/2353) done. Loss: 0.3758  lr:0.010000
[ Sat May 18 03:40:36 2024 ] 	Batch(1800/2353) done. Loss: 0.6175  lr:0.010000
[ Sat May 18 03:41:12 2024 ] 	Batch(1900/2353) done. Loss: 0.4314  lr:0.010000
[ Sat May 18 03:41:49 2024 ] 	Batch(2000/2353) done. Loss: 0.1282  lr:0.010000
[ Sat May 18 03:42:26 2024 ] 	Batch(2100/2353) done. Loss: 0.9494  lr:0.010000
[ Sat May 18 03:43:02 2024 ] 	Batch(2200/2353) done. Loss: 0.5310  lr:0.010000
[ Sat May 18 03:43:39 2024 ] 	Batch(2300/2353) done. Loss: 0.3969  lr:0.010000
[ Sat May 18 03:43:58 2024 ] 	Mean training loss: 0.3922.
[ Sat May 18 03:43:58 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 03:43:58 2024 ] Training epoch: 27
[ Sat May 18 03:43:59 2024 ] 	Batch(0/2353) done. Loss: 0.0473  lr:0.010000
[ Sat May 18 03:44:35 2024 ] 	Batch(100/2353) done. Loss: 0.2027  lr:0.010000
[ Sat May 18 03:45:12 2024 ] 	Batch(200/2353) done. Loss: 0.1798  lr:0.010000
[ Sat May 18 03:45:49 2024 ] 	Batch(300/2353) done. Loss: 0.1788  lr:0.010000
[ Sat May 18 03:46:25 2024 ] 	Batch(400/2353) done. Loss: 0.7035  lr:0.010000
[ Sat May 18 03:47:02 2024 ] 	Batch(500/2353) done. Loss: 0.3713  lr:0.010000
[ Sat May 18 03:47:38 2024 ] 	Batch(600/2353) done. Loss: 0.3466  lr:0.010000
[ Sat May 18 03:48:15 2024 ] 	Batch(700/2353) done. Loss: 0.1893  lr:0.010000
[ Sat May 18 03:48:52 2024 ] 	Batch(800/2353) done. Loss: 0.2832  lr:0.010000
[ Sat May 18 03:49:29 2024 ] 	Batch(900/2353) done. Loss: 0.2385  lr:0.010000
[ Sat May 18 03:50:05 2024 ] 	Batch(1000/2353) done. Loss: 0.3135  lr:0.010000
[ Sat May 18 03:50:42 2024 ] 	Batch(1100/2353) done. Loss: 0.8941  lr:0.010000
[ Sat May 18 03:51:19 2024 ] 	Batch(1200/2353) done. Loss: 0.3495  lr:0.010000
[ Sat May 18 03:51:55 2024 ] 	Batch(1300/2353) done. Loss: 0.2316  lr:0.010000
[ Sat May 18 03:52:32 2024 ] 	Batch(1400/2353) done. Loss: 0.5356  lr:0.010000
[ Sat May 18 03:53:08 2024 ] 	Batch(1500/2353) done. Loss: 0.4327  lr:0.010000
[ Sat May 18 03:53:45 2024 ] 	Batch(1600/2353) done. Loss: 0.2142  lr:0.010000
[ Sat May 18 03:54:22 2024 ] 	Batch(1700/2353) done. Loss: 0.0712  lr:0.010000
[ Sat May 18 03:54:58 2024 ] 	Batch(1800/2353) done. Loss: 0.2023  lr:0.010000
[ Sat May 18 03:55:35 2024 ] 	Batch(1900/2353) done. Loss: 0.5869  lr:0.010000
[ Sat May 18 03:56:12 2024 ] 	Batch(2000/2353) done. Loss: 0.9205  lr:0.010000
[ Sat May 18 03:56:49 2024 ] 	Batch(2100/2353) done. Loss: 0.3779  lr:0.010000
[ Sat May 18 03:57:25 2024 ] 	Batch(2200/2353) done. Loss: 0.0591  lr:0.010000
[ Sat May 18 03:58:02 2024 ] 	Batch(2300/2353) done. Loss: 0.1686  lr:0.010000
[ Sat May 18 03:58:21 2024 ] 	Mean training loss: 0.3787.
[ Sat May 18 03:58:21 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 03:58:21 2024 ] Training epoch: 28
[ Sat May 18 03:58:22 2024 ] 	Batch(0/2353) done. Loss: 0.1067  lr:0.010000
[ Sat May 18 03:58:58 2024 ] 	Batch(100/2353) done. Loss: 0.3565  lr:0.010000
[ Sat May 18 03:59:35 2024 ] 	Batch(200/2353) done. Loss: 0.3165  lr:0.010000
[ Sat May 18 04:00:12 2024 ] 	Batch(300/2353) done. Loss: 0.1188  lr:0.010000
[ Sat May 18 04:00:48 2024 ] 	Batch(400/2353) done. Loss: 0.2885  lr:0.010000
[ Sat May 18 04:01:25 2024 ] 	Batch(500/2353) done. Loss: 0.0921  lr:0.010000
[ Sat May 18 04:02:02 2024 ] 	Batch(600/2353) done. Loss: 0.1643  lr:0.010000
[ Sat May 18 04:02:39 2024 ] 	Batch(700/2353) done. Loss: 0.8214  lr:0.010000
[ Sat May 18 04:03:16 2024 ] 	Batch(800/2353) done. Loss: 0.0444  lr:0.010000
[ Sat May 18 04:03:53 2024 ] 	Batch(900/2353) done. Loss: 0.6373  lr:0.010000
[ Sat May 18 04:04:29 2024 ] 	Batch(1000/2353) done. Loss: 0.1311  lr:0.010000
[ Sat May 18 04:05:06 2024 ] 	Batch(1100/2353) done. Loss: 0.1827  lr:0.010000
[ Sat May 18 04:05:43 2024 ] 	Batch(1200/2353) done. Loss: 0.0532  lr:0.010000
[ Sat May 18 04:06:19 2024 ] 	Batch(1300/2353) done. Loss: 0.5859  lr:0.010000
[ Sat May 18 04:06:56 2024 ] 	Batch(1400/2353) done. Loss: 0.4451  lr:0.010000
[ Sat May 18 04:07:33 2024 ] 	Batch(1500/2353) done. Loss: 0.2029  lr:0.010000
[ Sat May 18 04:08:09 2024 ] 	Batch(1600/2353) done. Loss: 0.1044  lr:0.010000
[ Sat May 18 04:08:46 2024 ] 	Batch(1700/2353) done. Loss: 0.2303  lr:0.010000
[ Sat May 18 04:09:23 2024 ] 	Batch(1800/2353) done. Loss: 0.9743  lr:0.010000
[ Sat May 18 04:09:59 2024 ] 	Batch(1900/2353) done. Loss: 0.4033  lr:0.010000
[ Sat May 18 04:10:36 2024 ] 	Batch(2000/2353) done. Loss: 0.3929  lr:0.010000
[ Sat May 18 04:11:13 2024 ] 	Batch(2100/2353) done. Loss: 0.0517  lr:0.010000
[ Sat May 18 04:11:49 2024 ] 	Batch(2200/2353) done. Loss: 0.1580  lr:0.010000
[ Sat May 18 04:12:26 2024 ] 	Batch(2300/2353) done. Loss: 0.1649  lr:0.010000
[ Sat May 18 04:12:45 2024 ] 	Mean training loss: 0.3654.
[ Sat May 18 04:12:45 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 04:12:45 2024 ] Training epoch: 29
[ Sat May 18 04:12:46 2024 ] 	Batch(0/2353) done. Loss: 0.3692  lr:0.010000
[ Sat May 18 04:13:23 2024 ] 	Batch(100/2353) done. Loss: 0.0962  lr:0.010000
[ Sat May 18 04:13:59 2024 ] 	Batch(200/2353) done. Loss: 0.2055  lr:0.010000
[ Sat May 18 04:14:36 2024 ] 	Batch(300/2353) done. Loss: 0.3637  lr:0.010000
[ Sat May 18 04:15:13 2024 ] 	Batch(400/2353) done. Loss: 0.2614  lr:0.010000
[ Sat May 18 04:15:50 2024 ] 	Batch(500/2353) done. Loss: 0.3573  lr:0.010000
[ Sat May 18 04:16:26 2024 ] 	Batch(600/2353) done. Loss: 0.3622  lr:0.010000
[ Sat May 18 04:17:03 2024 ] 	Batch(700/2353) done. Loss: 0.5018  lr:0.010000
[ Sat May 18 04:17:40 2024 ] 	Batch(800/2353) done. Loss: 0.1873  lr:0.010000
[ Sat May 18 04:18:16 2024 ] 	Batch(900/2353) done. Loss: 0.0866  lr:0.010000
[ Sat May 18 04:18:53 2024 ] 	Batch(1000/2353) done. Loss: 0.2299  lr:0.010000
[ Sat May 18 04:19:30 2024 ] 	Batch(1100/2353) done. Loss: 0.4553  lr:0.010000
[ Sat May 18 04:20:07 2024 ] 	Batch(1200/2353) done. Loss: 0.0531  lr:0.010000
[ Sat May 18 04:20:43 2024 ] 	Batch(1300/2353) done. Loss: 0.1662  lr:0.010000
[ Sat May 18 04:21:20 2024 ] 	Batch(1400/2353) done. Loss: 0.1448  lr:0.010000
[ Sat May 18 04:21:58 2024 ] 	Batch(1500/2353) done. Loss: 0.3474  lr:0.010000
[ Sat May 18 04:22:35 2024 ] 	Batch(1600/2353) done. Loss: 0.5897  lr:0.010000
[ Sat May 18 04:23:12 2024 ] 	Batch(1700/2353) done. Loss: 0.6779  lr:0.010000
[ Sat May 18 04:23:50 2024 ] 	Batch(1800/2353) done. Loss: 0.3513  lr:0.010000
[ Sat May 18 04:24:27 2024 ] 	Batch(1900/2353) done. Loss: 0.6619  lr:0.010000
[ Sat May 18 04:25:04 2024 ] 	Batch(2000/2353) done. Loss: 0.1312  lr:0.010000
[ Sat May 18 04:25:40 2024 ] 	Batch(2100/2353) done. Loss: 0.0440  lr:0.010000
[ Sat May 18 04:26:17 2024 ] 	Batch(2200/2353) done. Loss: 0.3358  lr:0.010000
[ Sat May 18 04:26:54 2024 ] 	Batch(2300/2353) done. Loss: 0.4582  lr:0.010000
[ Sat May 18 04:27:13 2024 ] 	Mean training loss: 0.3556.
[ Sat May 18 04:27:13 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 04:27:13 2024 ] Training epoch: 30
[ Sat May 18 04:27:13 2024 ] 	Batch(0/2353) done. Loss: 0.3949  lr:0.010000
[ Sat May 18 04:27:50 2024 ] 	Batch(100/2353) done. Loss: 0.4054  lr:0.010000
[ Sat May 18 04:28:27 2024 ] 	Batch(200/2353) done. Loss: 0.2798  lr:0.010000
[ Sat May 18 04:29:03 2024 ] 	Batch(300/2353) done. Loss: 0.1924  lr:0.010000
[ Sat May 18 04:29:40 2024 ] 	Batch(400/2353) done. Loss: 0.4386  lr:0.010000
[ Sat May 18 04:30:17 2024 ] 	Batch(500/2353) done. Loss: 0.2327  lr:0.010000
[ Sat May 18 04:30:53 2024 ] 	Batch(600/2353) done. Loss: 0.2136  lr:0.010000
[ Sat May 18 04:31:30 2024 ] 	Batch(700/2353) done. Loss: 0.4621  lr:0.010000
[ Sat May 18 04:32:07 2024 ] 	Batch(800/2353) done. Loss: 0.3509  lr:0.010000
[ Sat May 18 04:32:43 2024 ] 	Batch(900/2353) done. Loss: 0.7366  lr:0.010000
[ Sat May 18 04:33:20 2024 ] 	Batch(1000/2353) done. Loss: 0.1943  lr:0.010000
[ Sat May 18 04:33:57 2024 ] 	Batch(1100/2353) done. Loss: 0.2633  lr:0.010000
[ Sat May 18 04:34:33 2024 ] 	Batch(1200/2353) done. Loss: 0.7129  lr:0.010000
[ Sat May 18 04:35:10 2024 ] 	Batch(1300/2353) done. Loss: 0.4468  lr:0.010000
[ Sat May 18 04:35:48 2024 ] 	Batch(1400/2353) done. Loss: 0.0697  lr:0.010000
[ Sat May 18 04:36:25 2024 ] 	Batch(1500/2353) done. Loss: 0.1463  lr:0.010000
[ Sat May 18 04:37:02 2024 ] 	Batch(1600/2353) done. Loss: 0.2206  lr:0.010000
[ Sat May 18 04:37:40 2024 ] 	Batch(1700/2353) done. Loss: 0.9273  lr:0.010000
[ Sat May 18 04:38:17 2024 ] 	Batch(1800/2353) done. Loss: 0.4515  lr:0.010000
[ Sat May 18 04:38:53 2024 ] 	Batch(1900/2353) done. Loss: 0.9223  lr:0.010000
[ Sat May 18 04:39:30 2024 ] 	Batch(2000/2353) done. Loss: 0.1005  lr:0.010000
[ Sat May 18 04:40:07 2024 ] 	Batch(2100/2353) done. Loss: 0.5963  lr:0.010000
[ Sat May 18 04:40:43 2024 ] 	Batch(2200/2353) done. Loss: 0.5939  lr:0.010000
[ Sat May 18 04:41:20 2024 ] 	Batch(2300/2353) done. Loss: 0.2800  lr:0.010000
[ Sat May 18 04:41:39 2024 ] 	Mean training loss: 0.3478.
[ Sat May 18 04:41:39 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 04:41:39 2024 ] Eval epoch: 30
[ Sat May 18 04:43:42 2024 ] 	Mean val loss of 2367 batches: 0.44935010239104584.
[ Sat May 18 04:43:42 2024 ] Training epoch: 31
[ Sat May 18 04:43:43 2024 ] 	Batch(0/2353) done. Loss: 0.0453  lr:0.010000
[ Sat May 18 04:44:20 2024 ] 	Batch(100/2353) done. Loss: 0.3610  lr:0.010000
[ Sat May 18 04:44:56 2024 ] 	Batch(200/2353) done. Loss: 0.5523  lr:0.010000
[ Sat May 18 04:45:33 2024 ] 	Batch(300/2353) done. Loss: 0.2398  lr:0.010000
[ Sat May 18 04:46:09 2024 ] 	Batch(400/2353) done. Loss: 0.3238  lr:0.010000
[ Sat May 18 04:46:46 2024 ] 	Batch(500/2353) done. Loss: 0.3420  lr:0.010000
[ Sat May 18 04:47:23 2024 ] 	Batch(600/2353) done. Loss: 0.2386  lr:0.010000
[ Sat May 18 04:47:59 2024 ] 	Batch(700/2353) done. Loss: 0.2512  lr:0.010000
[ Sat May 18 04:48:36 2024 ] 	Batch(800/2353) done. Loss: 0.5828  lr:0.010000
[ Sat May 18 04:49:13 2024 ] 	Batch(900/2353) done. Loss: 0.7046  lr:0.010000
[ Sat May 18 04:49:49 2024 ] 	Batch(1000/2353) done. Loss: 0.3182  lr:0.010000
[ Sat May 18 04:50:26 2024 ] 	Batch(1100/2353) done. Loss: 0.1978  lr:0.010000
[ Sat May 18 04:51:02 2024 ] 	Batch(1200/2353) done. Loss: 0.1744  lr:0.010000
[ Sat May 18 04:51:39 2024 ] 	Batch(1300/2353) done. Loss: 0.5308  lr:0.010000
[ Sat May 18 04:52:16 2024 ] 	Batch(1400/2353) done. Loss: 0.3376  lr:0.010000
[ Sat May 18 04:52:52 2024 ] 	Batch(1500/2353) done. Loss: 0.5343  lr:0.010000
[ Sat May 18 04:53:29 2024 ] 	Batch(1600/2353) done. Loss: 0.3718  lr:0.010000
[ Sat May 18 04:54:06 2024 ] 	Batch(1700/2353) done. Loss: 0.3570  lr:0.010000
[ Sat May 18 04:54:42 2024 ] 	Batch(1800/2353) done. Loss: 0.7057  lr:0.010000
[ Sat May 18 04:55:19 2024 ] 	Batch(1900/2353) done. Loss: 0.2433  lr:0.010000
[ Sat May 18 04:55:56 2024 ] 	Batch(2000/2353) done. Loss: 0.1867  lr:0.010000
[ Sat May 18 04:56:32 2024 ] 	Batch(2100/2353) done. Loss: 0.2193  lr:0.010000
[ Sat May 18 04:57:09 2024 ] 	Batch(2200/2353) done. Loss: 0.2552  lr:0.010000
[ Sat May 18 04:57:45 2024 ] 	Batch(2300/2353) done. Loss: 0.1636  lr:0.010000
[ Sat May 18 04:58:05 2024 ] 	Mean training loss: 0.3429.
[ Sat May 18 04:58:05 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 04:58:05 2024 ] Training epoch: 32
[ Sat May 18 04:58:05 2024 ] 	Batch(0/2353) done. Loss: 0.3806  lr:0.010000
[ Sat May 18 04:58:42 2024 ] 	Batch(100/2353) done. Loss: 0.6902  lr:0.010000
[ Sat May 18 04:59:19 2024 ] 	Batch(200/2353) done. Loss: 0.5235  lr:0.010000
[ Sat May 18 04:59:55 2024 ] 	Batch(300/2353) done. Loss: 0.3396  lr:0.010000
[ Sat May 18 05:00:32 2024 ] 	Batch(400/2353) done. Loss: 0.1082  lr:0.010000
[ Sat May 18 05:01:09 2024 ] 	Batch(500/2353) done. Loss: 0.2145  lr:0.010000
[ Sat May 18 05:01:45 2024 ] 	Batch(600/2353) done. Loss: 0.1222  lr:0.010000
[ Sat May 18 05:02:22 2024 ] 	Batch(700/2353) done. Loss: 0.1798  lr:0.010000
[ Sat May 18 05:02:59 2024 ] 	Batch(800/2353) done. Loss: 0.2218  lr:0.010000
[ Sat May 18 05:03:35 2024 ] 	Batch(900/2353) done. Loss: 0.1874  lr:0.010000
[ Sat May 18 05:04:12 2024 ] 	Batch(1000/2353) done. Loss: 0.5514  lr:0.010000
[ Sat May 18 05:04:48 2024 ] 	Batch(1100/2353) done. Loss: 0.5161  lr:0.010000
[ Sat May 18 05:05:25 2024 ] 	Batch(1200/2353) done. Loss: 0.3352  lr:0.010000
[ Sat May 18 05:06:02 2024 ] 	Batch(1300/2353) done. Loss: 0.3654  lr:0.010000
[ Sat May 18 05:06:38 2024 ] 	Batch(1400/2353) done. Loss: 0.1530  lr:0.010000
[ Sat May 18 05:07:15 2024 ] 	Batch(1500/2353) done. Loss: 0.5203  lr:0.010000
[ Sat May 18 05:07:51 2024 ] 	Batch(1600/2353) done. Loss: 0.4739  lr:0.010000
[ Sat May 18 05:08:28 2024 ] 	Batch(1700/2353) done. Loss: 0.2715  lr:0.010000
[ Sat May 18 05:09:05 2024 ] 	Batch(1800/2353) done. Loss: 0.2023  lr:0.010000
[ Sat May 18 05:09:41 2024 ] 	Batch(1900/2353) done. Loss: 0.4238  lr:0.010000
[ Sat May 18 05:10:18 2024 ] 	Batch(2000/2353) done. Loss: 0.1339  lr:0.010000
[ Sat May 18 05:10:55 2024 ] 	Batch(2100/2353) done. Loss: 0.4407  lr:0.010000
[ Sat May 18 05:11:31 2024 ] 	Batch(2200/2353) done. Loss: 0.2985  lr:0.010000
[ Sat May 18 05:12:08 2024 ] 	Batch(2300/2353) done. Loss: 0.4763  lr:0.010000
[ Sat May 18 05:12:27 2024 ] 	Mean training loss: 0.3149.
[ Sat May 18 05:12:27 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 05:12:27 2024 ] Training epoch: 33
[ Sat May 18 05:12:28 2024 ] 	Batch(0/2353) done. Loss: 0.5514  lr:0.010000
[ Sat May 18 05:13:04 2024 ] 	Batch(100/2353) done. Loss: 0.6155  lr:0.010000
[ Sat May 18 05:13:41 2024 ] 	Batch(200/2353) done. Loss: 0.1235  lr:0.010000
[ Sat May 18 05:14:18 2024 ] 	Batch(300/2353) done. Loss: 0.2201  lr:0.010000
[ Sat May 18 05:14:54 2024 ] 	Batch(400/2353) done. Loss: 0.1799  lr:0.010000
[ Sat May 18 05:15:31 2024 ] 	Batch(500/2353) done. Loss: 0.7005  lr:0.010000
[ Sat May 18 05:16:07 2024 ] 	Batch(600/2353) done. Loss: 0.4736  lr:0.010000
[ Sat May 18 05:16:44 2024 ] 	Batch(700/2353) done. Loss: 0.1774  lr:0.010000
[ Sat May 18 05:17:21 2024 ] 	Batch(800/2353) done. Loss: 0.0392  lr:0.010000
[ Sat May 18 05:17:57 2024 ] 	Batch(900/2353) done. Loss: 0.4285  lr:0.010000
[ Sat May 18 05:18:34 2024 ] 	Batch(1000/2353) done. Loss: 0.2378  lr:0.010000
[ Sat May 18 05:19:10 2024 ] 	Batch(1100/2353) done. Loss: 0.2718  lr:0.010000
[ Sat May 18 05:19:47 2024 ] 	Batch(1200/2353) done. Loss: 0.1805  lr:0.010000
[ Sat May 18 05:20:24 2024 ] 	Batch(1300/2353) done. Loss: 0.0620  lr:0.010000
[ Sat May 18 05:21:00 2024 ] 	Batch(1400/2353) done. Loss: 0.1334  lr:0.010000
[ Sat May 18 05:21:37 2024 ] 	Batch(1500/2353) done. Loss: 0.0414  lr:0.010000
[ Sat May 18 05:22:14 2024 ] 	Batch(1600/2353) done. Loss: 0.1486  lr:0.010000
[ Sat May 18 05:22:50 2024 ] 	Batch(1700/2353) done. Loss: 0.2285  lr:0.010000
[ Sat May 18 05:23:27 2024 ] 	Batch(1800/2353) done. Loss: 0.1649  lr:0.010000
[ Sat May 18 05:24:04 2024 ] 	Batch(1900/2353) done. Loss: 0.1566  lr:0.010000
[ Sat May 18 05:24:40 2024 ] 	Batch(2000/2353) done. Loss: 0.7425  lr:0.010000
[ Sat May 18 05:25:17 2024 ] 	Batch(2100/2353) done. Loss: 0.2308  lr:0.010000
[ Sat May 18 05:25:54 2024 ] 	Batch(2200/2353) done. Loss: 0.4193  lr:0.010000
[ Sat May 18 05:26:30 2024 ] 	Batch(2300/2353) done. Loss: 0.2865  lr:0.010000
[ Sat May 18 05:26:49 2024 ] 	Mean training loss: 0.3093.
[ Sat May 18 05:26:49 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 05:26:49 2024 ] Training epoch: 34
[ Sat May 18 05:26:50 2024 ] 	Batch(0/2353) done. Loss: 0.2011  lr:0.010000
[ Sat May 18 05:27:27 2024 ] 	Batch(100/2353) done. Loss: 0.2382  lr:0.010000
[ Sat May 18 05:28:03 2024 ] 	Batch(200/2353) done. Loss: 0.1515  lr:0.010000
[ Sat May 18 05:28:40 2024 ] 	Batch(300/2353) done. Loss: 0.2407  lr:0.010000
[ Sat May 18 05:29:17 2024 ] 	Batch(400/2353) done. Loss: 0.4312  lr:0.010000
[ Sat May 18 05:29:53 2024 ] 	Batch(500/2353) done. Loss: 0.2036  lr:0.010000
[ Sat May 18 05:30:30 2024 ] 	Batch(600/2353) done. Loss: 0.2466  lr:0.010000
[ Sat May 18 05:31:07 2024 ] 	Batch(700/2353) done. Loss: 0.1119  lr:0.010000
[ Sat May 18 05:31:43 2024 ] 	Batch(800/2353) done. Loss: 0.3216  lr:0.010000
[ Sat May 18 05:32:20 2024 ] 	Batch(900/2353) done. Loss: 0.1424  lr:0.010000
[ Sat May 18 05:32:57 2024 ] 	Batch(1000/2353) done. Loss: 0.2737  lr:0.010000
[ Sat May 18 05:33:33 2024 ] 	Batch(1100/2353) done. Loss: 0.5606  lr:0.010000
[ Sat May 18 05:34:10 2024 ] 	Batch(1200/2353) done. Loss: 0.1028  lr:0.010000
[ Sat May 18 05:34:47 2024 ] 	Batch(1300/2353) done. Loss: 0.1035  lr:0.010000
[ Sat May 18 05:35:23 2024 ] 	Batch(1400/2353) done. Loss: 0.2912  lr:0.010000
[ Sat May 18 05:36:00 2024 ] 	Batch(1500/2353) done. Loss: 0.0980  lr:0.010000
[ Sat May 18 05:36:36 2024 ] 	Batch(1600/2353) done. Loss: 0.2947  lr:0.010000
[ Sat May 18 05:37:13 2024 ] 	Batch(1700/2353) done. Loss: 0.4652  lr:0.010000
[ Sat May 18 05:37:50 2024 ] 	Batch(1800/2353) done. Loss: 0.4268  lr:0.010000
[ Sat May 18 05:38:26 2024 ] 	Batch(1900/2353) done. Loss: 0.9475  lr:0.010000
[ Sat May 18 05:39:03 2024 ] 	Batch(2000/2353) done. Loss: 0.1215  lr:0.010000
[ Sat May 18 05:39:40 2024 ] 	Batch(2100/2353) done. Loss: 0.1872  lr:0.010000
[ Sat May 18 05:40:16 2024 ] 	Batch(2200/2353) done. Loss: 0.2605  lr:0.010000
[ Sat May 18 05:40:53 2024 ] 	Batch(2300/2353) done. Loss: 0.2576  lr:0.010000
[ Sat May 18 05:41:12 2024 ] 	Mean training loss: 0.3042.
[ Sat May 18 05:41:12 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 05:41:12 2024 ] Training epoch: 35
[ Sat May 18 05:41:13 2024 ] 	Batch(0/2353) done. Loss: 0.6697  lr:0.010000
[ Sat May 18 05:41:50 2024 ] 	Batch(100/2353) done. Loss: 0.1200  lr:0.010000
[ Sat May 18 05:42:26 2024 ] 	Batch(200/2353) done. Loss: 0.1641  lr:0.010000
[ Sat May 18 05:43:03 2024 ] 	Batch(300/2353) done. Loss: 0.2033  lr:0.010000
[ Sat May 18 05:43:39 2024 ] 	Batch(400/2353) done. Loss: 0.3069  lr:0.010000
[ Sat May 18 05:44:16 2024 ] 	Batch(500/2353) done. Loss: 0.1100  lr:0.010000
[ Sat May 18 05:44:53 2024 ] 	Batch(600/2353) done. Loss: 0.1988  lr:0.010000
[ Sat May 18 05:45:29 2024 ] 	Batch(700/2353) done. Loss: 0.0965  lr:0.010000
[ Sat May 18 05:46:06 2024 ] 	Batch(800/2353) done. Loss: 0.2952  lr:0.010000
[ Sat May 18 05:46:43 2024 ] 	Batch(900/2353) done. Loss: 0.1821  lr:0.010000
[ Sat May 18 05:47:19 2024 ] 	Batch(1000/2353) done. Loss: 0.3708  lr:0.010000
[ Sat May 18 05:47:56 2024 ] 	Batch(1100/2353) done. Loss: 0.1330  lr:0.010000
[ Sat May 18 05:48:33 2024 ] 	Batch(1200/2353) done. Loss: 0.4751  lr:0.010000
[ Sat May 18 05:49:09 2024 ] 	Batch(1300/2353) done. Loss: 0.3486  lr:0.010000
[ Sat May 18 05:49:46 2024 ] 	Batch(1400/2353) done. Loss: 0.6062  lr:0.010000
[ Sat May 18 05:50:23 2024 ] 	Batch(1500/2353) done. Loss: 0.2147  lr:0.010000
[ Sat May 18 05:50:59 2024 ] 	Batch(1600/2353) done. Loss: 0.3052  lr:0.010000
[ Sat May 18 05:51:36 2024 ] 	Batch(1700/2353) done. Loss: 0.1344  lr:0.010000
[ Sat May 18 05:52:13 2024 ] 	Batch(1800/2353) done. Loss: 0.4302  lr:0.010000
[ Sat May 18 05:52:49 2024 ] 	Batch(1900/2353) done. Loss: 0.0537  lr:0.010000
[ Sat May 18 05:53:26 2024 ] 	Batch(2000/2353) done. Loss: 0.3555  lr:0.010000
[ Sat May 18 05:54:03 2024 ] 	Batch(2100/2353) done. Loss: 0.1842  lr:0.010000
[ Sat May 18 05:54:40 2024 ] 	Batch(2200/2353) done. Loss: 0.1576  lr:0.010000
[ Sat May 18 05:55:18 2024 ] 	Batch(2300/2353) done. Loss: 0.1447  lr:0.010000
[ Sat May 18 05:55:37 2024 ] 	Mean training loss: 0.2951.
[ Sat May 18 05:55:37 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 05:55:37 2024 ] Training epoch: 36
[ Sat May 18 05:55:38 2024 ] 	Batch(0/2353) done. Loss: 0.1272  lr:0.010000
[ Sat May 18 05:56:14 2024 ] 	Batch(100/2353) done. Loss: 0.3581  lr:0.010000
[ Sat May 18 05:56:51 2024 ] 	Batch(200/2353) done. Loss: 0.1789  lr:0.010000
[ Sat May 18 05:57:28 2024 ] 	Batch(300/2353) done. Loss: 0.1204  lr:0.010000
[ Sat May 18 05:58:04 2024 ] 	Batch(400/2353) done. Loss: 0.2629  lr:0.010000
[ Sat May 18 05:58:41 2024 ] 	Batch(500/2353) done. Loss: 0.0979  lr:0.010000
[ Sat May 18 05:59:18 2024 ] 	Batch(600/2353) done. Loss: 0.2754  lr:0.010000
[ Sat May 18 05:59:54 2024 ] 	Batch(700/2353) done. Loss: 0.1830  lr:0.010000
[ Sat May 18 06:00:31 2024 ] 	Batch(800/2353) done. Loss: 0.1517  lr:0.010000
[ Sat May 18 06:01:08 2024 ] 	Batch(900/2353) done. Loss: 0.3205  lr:0.010000
[ Sat May 18 06:01:44 2024 ] 	Batch(1000/2353) done. Loss: 0.0862  lr:0.010000
[ Sat May 18 06:02:21 2024 ] 	Batch(1100/2353) done. Loss: 0.2050  lr:0.010000
[ Sat May 18 06:02:57 2024 ] 	Batch(1200/2353) done. Loss: 0.8850  lr:0.010000
[ Sat May 18 06:03:34 2024 ] 	Batch(1300/2353) done. Loss: 0.2786  lr:0.010000
[ Sat May 18 06:04:11 2024 ] 	Batch(1400/2353) done. Loss: 0.1304  lr:0.010000
[ Sat May 18 06:04:47 2024 ] 	Batch(1500/2353) done. Loss: 0.3707  lr:0.010000
[ Sat May 18 06:05:24 2024 ] 	Batch(1600/2353) done. Loss: 0.1963  lr:0.010000
[ Sat May 18 06:06:01 2024 ] 	Batch(1700/2353) done. Loss: 0.1563  lr:0.010000
[ Sat May 18 06:06:38 2024 ] 	Batch(1800/2353) done. Loss: 0.2115  lr:0.010000
[ Sat May 18 06:07:15 2024 ] 	Batch(1900/2353) done. Loss: 1.1103  lr:0.010000
[ Sat May 18 06:07:51 2024 ] 	Batch(2000/2353) done. Loss: 0.5335  lr:0.010000
[ Sat May 18 06:08:28 2024 ] 	Batch(2100/2353) done. Loss: 0.8245  lr:0.010000
[ Sat May 18 06:09:05 2024 ] 	Batch(2200/2353) done. Loss: 0.0503  lr:0.010000
[ Sat May 18 06:09:41 2024 ] 	Batch(2300/2353) done. Loss: 0.3501  lr:0.010000
[ Sat May 18 06:10:00 2024 ] 	Mean training loss: 0.2909.
[ Sat May 18 06:10:00 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 06:10:01 2024 ] Training epoch: 37
[ Sat May 18 06:10:01 2024 ] 	Batch(0/2353) done. Loss: 0.5412  lr:0.010000
[ Sat May 18 06:10:38 2024 ] 	Batch(100/2353) done. Loss: 0.4338  lr:0.010000
[ Sat May 18 06:11:14 2024 ] 	Batch(200/2353) done. Loss: 0.6268  lr:0.010000
[ Sat May 18 06:11:51 2024 ] 	Batch(300/2353) done. Loss: 0.1489  lr:0.010000
[ Sat May 18 06:12:28 2024 ] 	Batch(400/2353) done. Loss: 0.3356  lr:0.010000
[ Sat May 18 06:13:05 2024 ] 	Batch(500/2353) done. Loss: 0.1198  lr:0.010000
[ Sat May 18 06:13:41 2024 ] 	Batch(600/2353) done. Loss: 0.1258  lr:0.010000
[ Sat May 18 06:14:18 2024 ] 	Batch(700/2353) done. Loss: 0.3457  lr:0.010000
[ Sat May 18 06:14:55 2024 ] 	Batch(800/2353) done. Loss: 0.0916  lr:0.010000
[ Sat May 18 06:15:31 2024 ] 	Batch(900/2353) done. Loss: 0.2745  lr:0.010000
[ Sat May 18 06:16:08 2024 ] 	Batch(1000/2353) done. Loss: 0.0513  lr:0.010000
[ Sat May 18 06:16:45 2024 ] 	Batch(1100/2353) done. Loss: 0.3355  lr:0.010000
[ Sat May 18 06:17:22 2024 ] 	Batch(1200/2353) done. Loss: 0.0799  lr:0.010000
[ Sat May 18 06:17:59 2024 ] 	Batch(1300/2353) done. Loss: 0.0136  lr:0.010000
[ Sat May 18 06:18:36 2024 ] 	Batch(1400/2353) done. Loss: 0.2210  lr:0.010000
[ Sat May 18 06:19:13 2024 ] 	Batch(1500/2353) done. Loss: 0.3524  lr:0.010000
[ Sat May 18 06:19:50 2024 ] 	Batch(1600/2353) done. Loss: 0.6168  lr:0.010000
[ Sat May 18 06:20:27 2024 ] 	Batch(1700/2353) done. Loss: 0.2718  lr:0.010000
[ Sat May 18 06:21:04 2024 ] 	Batch(1800/2353) done. Loss: 0.1848  lr:0.010000
[ Sat May 18 06:21:40 2024 ] 	Batch(1900/2353) done. Loss: 0.1922  lr:0.010000
[ Sat May 18 06:22:17 2024 ] 	Batch(2000/2353) done. Loss: 0.1367  lr:0.010000
[ Sat May 18 06:22:54 2024 ] 	Batch(2100/2353) done. Loss: 0.5050  lr:0.010000
[ Sat May 18 06:23:30 2024 ] 	Batch(2200/2353) done. Loss: 0.1999  lr:0.010000
[ Sat May 18 06:24:07 2024 ] 	Batch(2300/2353) done. Loss: 0.9029  lr:0.010000
[ Sat May 18 06:24:26 2024 ] 	Mean training loss: 0.2878.
[ Sat May 18 06:24:26 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 06:24:26 2024 ] Training epoch: 38
[ Sat May 18 06:24:27 2024 ] 	Batch(0/2353) done. Loss: 0.1397  lr:0.010000
[ Sat May 18 06:25:04 2024 ] 	Batch(100/2353) done. Loss: 0.1801  lr:0.010000
[ Sat May 18 06:25:41 2024 ] 	Batch(200/2353) done. Loss: 0.1005  lr:0.010000
[ Sat May 18 06:26:17 2024 ] 	Batch(300/2353) done. Loss: 0.2317  lr:0.010000
[ Sat May 18 06:26:54 2024 ] 	Batch(400/2353) done. Loss: 0.2444  lr:0.010000
[ Sat May 18 06:27:31 2024 ] 	Batch(500/2353) done. Loss: 0.1804  lr:0.010000
[ Sat May 18 06:28:07 2024 ] 	Batch(600/2353) done. Loss: 0.4302  lr:0.010000
[ Sat May 18 06:28:44 2024 ] 	Batch(700/2353) done. Loss: 0.4953  lr:0.010000
[ Sat May 18 06:29:21 2024 ] 	Batch(800/2353) done. Loss: 0.2261  lr:0.010000
[ Sat May 18 06:29:57 2024 ] 	Batch(900/2353) done. Loss: 0.3020  lr:0.010000
[ Sat May 18 06:30:34 2024 ] 	Batch(1000/2353) done. Loss: 0.2525  lr:0.010000
[ Sat May 18 06:31:11 2024 ] 	Batch(1100/2353) done. Loss: 0.3404  lr:0.010000
[ Sat May 18 06:31:47 2024 ] 	Batch(1200/2353) done. Loss: 0.1033  lr:0.010000
[ Sat May 18 06:32:24 2024 ] 	Batch(1300/2353) done. Loss: 0.2041  lr:0.010000
[ Sat May 18 06:33:01 2024 ] 	Batch(1400/2353) done. Loss: 0.0395  lr:0.010000
[ Sat May 18 06:33:37 2024 ] 	Batch(1500/2353) done. Loss: 0.4479  lr:0.010000
[ Sat May 18 06:34:14 2024 ] 	Batch(1600/2353) done. Loss: 0.1665  lr:0.010000
[ Sat May 18 06:34:51 2024 ] 	Batch(1700/2353) done. Loss: 0.1827  lr:0.010000
[ Sat May 18 06:35:27 2024 ] 	Batch(1800/2353) done. Loss: 0.4748  lr:0.010000
[ Sat May 18 06:36:04 2024 ] 	Batch(1900/2353) done. Loss: 0.1127  lr:0.010000
[ Sat May 18 06:36:40 2024 ] 	Batch(2000/2353) done. Loss: 0.6370  lr:0.010000
[ Sat May 18 06:37:17 2024 ] 	Batch(2100/2353) done. Loss: 0.2677  lr:0.010000
[ Sat May 18 06:37:54 2024 ] 	Batch(2200/2353) done. Loss: 0.3320  lr:0.010000
[ Sat May 18 06:38:30 2024 ] 	Batch(2300/2353) done. Loss: 0.1598  lr:0.010000
[ Sat May 18 06:38:49 2024 ] 	Mean training loss: 0.2706.
[ Sat May 18 06:38:49 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 06:38:50 2024 ] Training epoch: 39
[ Sat May 18 06:38:50 2024 ] 	Batch(0/2353) done. Loss: 0.0652  lr:0.010000
[ Sat May 18 06:39:27 2024 ] 	Batch(100/2353) done. Loss: 0.1456  lr:0.010000
[ Sat May 18 06:40:04 2024 ] 	Batch(200/2353) done. Loss: 0.1894  lr:0.010000
[ Sat May 18 06:40:40 2024 ] 	Batch(300/2353) done. Loss: 0.3208  lr:0.010000
[ Sat May 18 06:41:17 2024 ] 	Batch(400/2353) done. Loss: 0.0288  lr:0.010000
[ Sat May 18 06:41:53 2024 ] 	Batch(500/2353) done. Loss: 0.4114  lr:0.010000
[ Sat May 18 06:42:30 2024 ] 	Batch(600/2353) done. Loss: 0.2386  lr:0.010000
[ Sat May 18 06:43:07 2024 ] 	Batch(700/2353) done. Loss: 0.4357  lr:0.010000
[ Sat May 18 06:43:43 2024 ] 	Batch(800/2353) done. Loss: 0.4052  lr:0.010000
[ Sat May 18 06:44:20 2024 ] 	Batch(900/2353) done. Loss: 0.0666  lr:0.010000
[ Sat May 18 06:44:57 2024 ] 	Batch(1000/2353) done. Loss: 0.1428  lr:0.010000
[ Sat May 18 06:45:34 2024 ] 	Batch(1100/2353) done. Loss: 0.7057  lr:0.010000
[ Sat May 18 06:46:10 2024 ] 	Batch(1200/2353) done. Loss: 0.1766  lr:0.010000
[ Sat May 18 06:46:47 2024 ] 	Batch(1300/2353) done. Loss: 0.2820  lr:0.010000
[ Sat May 18 06:47:24 2024 ] 	Batch(1400/2353) done. Loss: 0.0986  lr:0.010000
[ Sat May 18 06:48:00 2024 ] 	Batch(1500/2353) done. Loss: 0.0419  lr:0.010000
[ Sat May 18 06:48:37 2024 ] 	Batch(1600/2353) done. Loss: 0.1979  lr:0.010000
[ Sat May 18 06:49:13 2024 ] 	Batch(1700/2353) done. Loss: 0.0693  lr:0.010000
[ Sat May 18 06:49:50 2024 ] 	Batch(1800/2353) done. Loss: 0.1918  lr:0.010000
[ Sat May 18 06:50:27 2024 ] 	Batch(1900/2353) done. Loss: 0.2450  lr:0.010000
[ Sat May 18 06:51:03 2024 ] 	Batch(2000/2353) done. Loss: 0.5961  lr:0.010000
[ Sat May 18 06:51:40 2024 ] 	Batch(2100/2353) done. Loss: 0.3712  lr:0.010000
[ Sat May 18 06:52:17 2024 ] 	Batch(2200/2353) done. Loss: 0.2092  lr:0.010000
[ Sat May 18 06:52:53 2024 ] 	Batch(2300/2353) done. Loss: 0.2877  lr:0.010000
[ Sat May 18 06:53:12 2024 ] 	Mean training loss: 0.2576.
[ Sat May 18 06:53:12 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 06:53:13 2024 ] Training epoch: 40
[ Sat May 18 06:53:13 2024 ] 	Batch(0/2353) done. Loss: 0.0462  lr:0.010000
[ Sat May 18 06:53:51 2024 ] 	Batch(100/2353) done. Loss: 0.5161  lr:0.010000
[ Sat May 18 06:54:28 2024 ] 	Batch(200/2353) done. Loss: 0.1738  lr:0.010000
[ Sat May 18 06:55:05 2024 ] 	Batch(300/2353) done. Loss: 0.1283  lr:0.010000
[ Sat May 18 06:55:43 2024 ] 	Batch(400/2353) done. Loss: 0.1973  lr:0.010000
[ Sat May 18 06:56:20 2024 ] 	Batch(500/2353) done. Loss: 0.1446  lr:0.010000
[ Sat May 18 06:56:57 2024 ] 	Batch(600/2353) done. Loss: 0.2805  lr:0.010000
[ Sat May 18 06:57:33 2024 ] 	Batch(700/2353) done. Loss: 0.2872  lr:0.010000
[ Sat May 18 06:58:10 2024 ] 	Batch(800/2353) done. Loss: 0.6573  lr:0.010000
[ Sat May 18 06:58:47 2024 ] 	Batch(900/2353) done. Loss: 0.6536  lr:0.010000
[ Sat May 18 06:59:23 2024 ] 	Batch(1000/2353) done. Loss: 0.2129  lr:0.010000
[ Sat May 18 07:00:00 2024 ] 	Batch(1100/2353) done. Loss: 0.4478  lr:0.010000
[ Sat May 18 07:00:37 2024 ] 	Batch(1200/2353) done. Loss: 0.5334  lr:0.010000
[ Sat May 18 07:01:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0519  lr:0.010000
[ Sat May 18 07:01:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0443  lr:0.010000
[ Sat May 18 07:02:26 2024 ] 	Batch(1500/2353) done. Loss: 0.4810  lr:0.010000
[ Sat May 18 07:03:03 2024 ] 	Batch(1600/2353) done. Loss: 0.2178  lr:0.010000
[ Sat May 18 07:03:40 2024 ] 	Batch(1700/2353) done. Loss: 0.4689  lr:0.010000
[ Sat May 18 07:04:16 2024 ] 	Batch(1800/2353) done. Loss: 0.2349  lr:0.010000
[ Sat May 18 07:04:53 2024 ] 	Batch(1900/2353) done. Loss: 0.3894  lr:0.010000
[ Sat May 18 07:05:30 2024 ] 	Batch(2000/2353) done. Loss: 0.1308  lr:0.010000
[ Sat May 18 07:06:06 2024 ] 	Batch(2100/2353) done. Loss: 0.5379  lr:0.010000
[ Sat May 18 07:06:43 2024 ] 	Batch(2200/2353) done. Loss: 0.0735  lr:0.010000
[ Sat May 18 07:07:19 2024 ] 	Batch(2300/2353) done. Loss: 0.2152  lr:0.010000
[ Sat May 18 07:07:39 2024 ] 	Mean training loss: 0.2571.
[ Sat May 18 07:07:39 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 07:07:39 2024 ] Eval epoch: 40
[ Sat May 18 07:09:42 2024 ] 	Mean val loss of 2367 batches: 0.3533112808302003.
[ Sat May 18 07:09:42 2024 ] Training epoch: 41
[ Sat May 18 07:09:43 2024 ] 	Batch(0/2353) done. Loss: 0.4776  lr:0.010000
[ Sat May 18 07:10:19 2024 ] 	Batch(100/2353) done. Loss: 0.2276  lr:0.010000
[ Sat May 18 07:10:56 2024 ] 	Batch(200/2353) done. Loss: 0.2769  lr:0.010000
[ Sat May 18 07:11:33 2024 ] 	Batch(300/2353) done. Loss: 0.5276  lr:0.010000
[ Sat May 18 07:12:09 2024 ] 	Batch(400/2353) done. Loss: 0.1691  lr:0.010000
[ Sat May 18 07:12:46 2024 ] 	Batch(500/2353) done. Loss: 0.1888  lr:0.010000
[ Sat May 18 07:13:22 2024 ] 	Batch(600/2353) done. Loss: 0.0681  lr:0.010000
[ Sat May 18 07:13:59 2024 ] 	Batch(700/2353) done. Loss: 0.0724  lr:0.010000
[ Sat May 18 07:14:36 2024 ] 	Batch(800/2353) done. Loss: 0.1037  lr:0.010000
[ Sat May 18 07:15:13 2024 ] 	Batch(900/2353) done. Loss: 0.5231  lr:0.010000
[ Sat May 18 07:15:50 2024 ] 	Batch(1000/2353) done. Loss: 0.5532  lr:0.010000
[ Sat May 18 07:16:27 2024 ] 	Batch(1100/2353) done. Loss: 0.2719  lr:0.010000
[ Sat May 18 07:17:04 2024 ] 	Batch(1200/2353) done. Loss: 0.0706  lr:0.010000
[ Sat May 18 07:17:41 2024 ] 	Batch(1300/2353) done. Loss: 0.0840  lr:0.010000
[ Sat May 18 07:18:19 2024 ] 	Batch(1400/2353) done. Loss: 0.1513  lr:0.010000
[ Sat May 18 07:18:56 2024 ] 	Batch(1500/2353) done. Loss: 0.2040  lr:0.010000
[ Sat May 18 07:19:33 2024 ] 	Batch(1600/2353) done. Loss: 0.2006  lr:0.010000
[ Sat May 18 07:20:10 2024 ] 	Batch(1700/2353) done. Loss: 0.1952  lr:0.010000
[ Sat May 18 07:20:48 2024 ] 	Batch(1800/2353) done. Loss: 0.2153  lr:0.010000
[ Sat May 18 07:21:25 2024 ] 	Batch(1900/2353) done. Loss: 0.1816  lr:0.010000
[ Sat May 18 07:22:02 2024 ] 	Batch(2000/2353) done. Loss: 0.3950  lr:0.010000
[ Sat May 18 07:22:39 2024 ] 	Batch(2100/2353) done. Loss: 0.2442  lr:0.010000
[ Sat May 18 07:23:17 2024 ] 	Batch(2200/2353) done. Loss: 0.4222  lr:0.010000
[ Sat May 18 07:23:54 2024 ] 	Batch(2300/2353) done. Loss: 0.0748  lr:0.010000
[ Sat May 18 07:24:13 2024 ] 	Mean training loss: 0.2431.
[ Sat May 18 07:24:13 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 07:24:13 2024 ] Training epoch: 42
[ Sat May 18 07:24:14 2024 ] 	Batch(0/2353) done. Loss: 0.5205  lr:0.010000
[ Sat May 18 07:24:51 2024 ] 	Batch(100/2353) done. Loss: 0.2420  lr:0.010000
[ Sat May 18 07:25:27 2024 ] 	Batch(200/2353) done. Loss: 0.3727  lr:0.010000
[ Sat May 18 07:26:04 2024 ] 	Batch(300/2353) done. Loss: 0.0105  lr:0.010000
[ Sat May 18 07:26:41 2024 ] 	Batch(400/2353) done. Loss: 0.2287  lr:0.010000
[ Sat May 18 07:27:17 2024 ] 	Batch(500/2353) done. Loss: 0.1981  lr:0.010000
[ Sat May 18 07:27:54 2024 ] 	Batch(600/2353) done. Loss: 0.4114  lr:0.010000
[ Sat May 18 07:28:30 2024 ] 	Batch(700/2353) done. Loss: 0.0666  lr:0.010000
[ Sat May 18 07:29:07 2024 ] 	Batch(800/2353) done. Loss: 0.2604  lr:0.010000
[ Sat May 18 07:29:44 2024 ] 	Batch(900/2353) done. Loss: 0.2215  lr:0.010000
[ Sat May 18 07:30:21 2024 ] 	Batch(1000/2353) done. Loss: 0.2617  lr:0.010000
[ Sat May 18 07:30:57 2024 ] 	Batch(1100/2353) done. Loss: 0.2223  lr:0.010000
[ Sat May 18 07:31:34 2024 ] 	Batch(1200/2353) done. Loss: 0.3419  lr:0.010000
[ Sat May 18 07:32:10 2024 ] 	Batch(1300/2353) done. Loss: 0.1765  lr:0.010000
[ Sat May 18 07:32:47 2024 ] 	Batch(1400/2353) done. Loss: 0.1688  lr:0.010000
[ Sat May 18 07:33:24 2024 ] 	Batch(1500/2353) done. Loss: 0.2867  lr:0.010000
[ Sat May 18 07:34:00 2024 ] 	Batch(1600/2353) done. Loss: 0.2172  lr:0.010000
[ Sat May 18 07:34:37 2024 ] 	Batch(1700/2353) done. Loss: 0.0484  lr:0.010000
[ Sat May 18 07:35:13 2024 ] 	Batch(1800/2353) done. Loss: 0.0917  lr:0.010000
[ Sat May 18 07:35:50 2024 ] 	Batch(1900/2353) done. Loss: 0.3400  lr:0.010000
[ Sat May 18 07:36:27 2024 ] 	Batch(2000/2353) done. Loss: 0.5916  lr:0.010000
[ Sat May 18 07:37:03 2024 ] 	Batch(2100/2353) done. Loss: 0.0566  lr:0.010000
[ Sat May 18 07:37:40 2024 ] 	Batch(2200/2353) done. Loss: 0.2533  lr:0.010000
[ Sat May 18 07:38:16 2024 ] 	Batch(2300/2353) done. Loss: 0.2415  lr:0.010000
[ Sat May 18 07:38:36 2024 ] 	Mean training loss: 0.2375.
[ Sat May 18 07:38:36 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 07:38:36 2024 ] Training epoch: 43
[ Sat May 18 07:38:36 2024 ] 	Batch(0/2353) done. Loss: 0.3518  lr:0.010000
[ Sat May 18 07:39:13 2024 ] 	Batch(100/2353) done. Loss: 0.5601  lr:0.010000
[ Sat May 18 07:39:50 2024 ] 	Batch(200/2353) done. Loss: 0.4657  lr:0.010000
[ Sat May 18 07:40:28 2024 ] 	Batch(300/2353) done. Loss: 0.1555  lr:0.010000
[ Sat May 18 07:41:05 2024 ] 	Batch(400/2353) done. Loss: 0.5076  lr:0.010000
[ Sat May 18 07:41:42 2024 ] 	Batch(500/2353) done. Loss: 0.0687  lr:0.010000
[ Sat May 18 07:42:19 2024 ] 	Batch(600/2353) done. Loss: 0.1042  lr:0.010000
[ Sat May 18 07:42:56 2024 ] 	Batch(700/2353) done. Loss: 0.1460  lr:0.010000
[ Sat May 18 07:43:33 2024 ] 	Batch(800/2353) done. Loss: 0.3849  lr:0.010000
[ Sat May 18 07:44:10 2024 ] 	Batch(900/2353) done. Loss: 0.2476  lr:0.010000
[ Sat May 18 07:44:46 2024 ] 	Batch(1000/2353) done. Loss: 0.4281  lr:0.010000
[ Sat May 18 07:45:23 2024 ] 	Batch(1100/2353) done. Loss: 0.1587  lr:0.010000
[ Sat May 18 07:46:00 2024 ] 	Batch(1200/2353) done. Loss: 0.2392  lr:0.010000
[ Sat May 18 07:46:36 2024 ] 	Batch(1300/2353) done. Loss: 0.1639  lr:0.010000
[ Sat May 18 07:47:13 2024 ] 	Batch(1400/2353) done. Loss: 0.2201  lr:0.010000
[ Sat May 18 07:47:50 2024 ] 	Batch(1500/2353) done. Loss: 0.1752  lr:0.010000
[ Sat May 18 07:48:27 2024 ] 	Batch(1600/2353) done. Loss: 0.1071  lr:0.010000
[ Sat May 18 07:49:03 2024 ] 	Batch(1700/2353) done. Loss: 1.0446  lr:0.010000
[ Sat May 18 07:49:40 2024 ] 	Batch(1800/2353) done. Loss: 0.0816  lr:0.010000
[ Sat May 18 07:50:16 2024 ] 	Batch(1900/2353) done. Loss: 0.1120  lr:0.010000
[ Sat May 18 07:50:53 2024 ] 	Batch(2000/2353) done. Loss: 0.6836  lr:0.010000
[ Sat May 18 07:51:30 2024 ] 	Batch(2100/2353) done. Loss: 0.2312  lr:0.010000
[ Sat May 18 07:52:06 2024 ] 	Batch(2200/2353) done. Loss: 0.2385  lr:0.010000
[ Sat May 18 07:52:43 2024 ] 	Batch(2300/2353) done. Loss: 0.1899  lr:0.010000
[ Sat May 18 07:53:02 2024 ] 	Mean training loss: 0.2446.
[ Sat May 18 07:53:02 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 07:53:02 2024 ] Training epoch: 44
[ Sat May 18 07:53:03 2024 ] 	Batch(0/2353) done. Loss: 0.2644  lr:0.010000
[ Sat May 18 07:53:39 2024 ] 	Batch(100/2353) done. Loss: 0.2519  lr:0.010000
[ Sat May 18 07:54:16 2024 ] 	Batch(200/2353) done. Loss: 0.6769  lr:0.010000
[ Sat May 18 07:54:52 2024 ] 	Batch(300/2353) done. Loss: 0.3969  lr:0.010000
[ Sat May 18 07:55:29 2024 ] 	Batch(400/2353) done. Loss: 0.1019  lr:0.010000
[ Sat May 18 07:56:06 2024 ] 	Batch(500/2353) done. Loss: 0.0250  lr:0.010000
[ Sat May 18 07:56:42 2024 ] 	Batch(600/2353) done. Loss: 0.1616  lr:0.010000
[ Sat May 18 07:57:19 2024 ] 	Batch(700/2353) done. Loss: 0.0188  lr:0.010000
[ Sat May 18 07:57:56 2024 ] 	Batch(800/2353) done. Loss: 0.1740  lr:0.010000
[ Sat May 18 07:58:32 2024 ] 	Batch(900/2353) done. Loss: 0.2120  lr:0.010000
[ Sat May 18 07:59:09 2024 ] 	Batch(1000/2353) done. Loss: 0.3886  lr:0.010000
[ Sat May 18 07:59:46 2024 ] 	Batch(1100/2353) done. Loss: 0.2004  lr:0.010000
[ Sat May 18 08:00:23 2024 ] 	Batch(1200/2353) done. Loss: 0.1946  lr:0.010000
[ Sat May 18 08:01:01 2024 ] 	Batch(1300/2353) done. Loss: 0.0535  lr:0.010000
[ Sat May 18 08:01:38 2024 ] 	Batch(1400/2353) done. Loss: 0.1619  lr:0.010000
[ Sat May 18 08:02:15 2024 ] 	Batch(1500/2353) done. Loss: 0.2253  lr:0.010000
[ Sat May 18 08:02:52 2024 ] 	Batch(1600/2353) done. Loss: 0.0977  lr:0.010000
[ Sat May 18 08:03:28 2024 ] 	Batch(1700/2353) done. Loss: 0.2374  lr:0.010000
[ Sat May 18 08:04:05 2024 ] 	Batch(1800/2353) done. Loss: 0.0695  lr:0.010000
[ Sat May 18 08:04:41 2024 ] 	Batch(1900/2353) done. Loss: 0.1368  lr:0.010000
[ Sat May 18 08:05:18 2024 ] 	Batch(2000/2353) done. Loss: 0.1600  lr:0.010000
[ Sat May 18 08:05:55 2024 ] 	Batch(2100/2353) done. Loss: 0.1672  lr:0.010000
[ Sat May 18 08:06:32 2024 ] 	Batch(2200/2353) done. Loss: 0.3417  lr:0.010000
[ Sat May 18 08:07:08 2024 ] 	Batch(2300/2353) done. Loss: 0.1169  lr:0.010000
[ Sat May 18 08:07:27 2024 ] 	Mean training loss: 0.2252.
[ Sat May 18 08:07:27 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 08:07:28 2024 ] Training epoch: 45
[ Sat May 18 08:07:28 2024 ] 	Batch(0/2353) done. Loss: 0.2472  lr:0.010000
[ Sat May 18 08:08:05 2024 ] 	Batch(100/2353) done. Loss: 0.2151  lr:0.010000
[ Sat May 18 08:08:41 2024 ] 	Batch(200/2353) done. Loss: 0.2538  lr:0.010000
[ Sat May 18 08:09:18 2024 ] 	Batch(300/2353) done. Loss: 0.2021  lr:0.010000
[ Sat May 18 08:09:55 2024 ] 	Batch(400/2353) done. Loss: 0.3795  lr:0.010000
[ Sat May 18 08:10:31 2024 ] 	Batch(500/2353) done. Loss: 0.3281  lr:0.010000
[ Sat May 18 08:11:08 2024 ] 	Batch(600/2353) done. Loss: 0.1151  lr:0.010000
[ Sat May 18 08:11:45 2024 ] 	Batch(700/2353) done. Loss: 0.0292  lr:0.010000
[ Sat May 18 08:12:21 2024 ] 	Batch(800/2353) done. Loss: 0.0384  lr:0.010000
[ Sat May 18 08:12:58 2024 ] 	Batch(900/2353) done. Loss: 0.2700  lr:0.010000
[ Sat May 18 08:13:35 2024 ] 	Batch(1000/2353) done. Loss: 0.2547  lr:0.010000
[ Sat May 18 08:14:11 2024 ] 	Batch(1100/2353) done. Loss: 0.1699  lr:0.010000
[ Sat May 18 08:14:48 2024 ] 	Batch(1200/2353) done. Loss: 0.2017  lr:0.010000
[ Sat May 18 08:15:25 2024 ] 	Batch(1300/2353) done. Loss: 0.1823  lr:0.010000
[ Sat May 18 08:16:02 2024 ] 	Batch(1400/2353) done. Loss: 0.5763  lr:0.010000
[ Sat May 18 08:16:39 2024 ] 	Batch(1500/2353) done. Loss: 0.3074  lr:0.010000
[ Sat May 18 08:17:16 2024 ] 	Batch(1600/2353) done. Loss: 0.1887  lr:0.010000
[ Sat May 18 08:17:52 2024 ] 	Batch(1700/2353) done. Loss: 0.2343  lr:0.010000
[ Sat May 18 08:18:29 2024 ] 	Batch(1800/2353) done. Loss: 0.1089  lr:0.010000
[ Sat May 18 08:19:05 2024 ] 	Batch(1900/2353) done. Loss: 0.2214  lr:0.010000
[ Sat May 18 08:19:42 2024 ] 	Batch(2000/2353) done. Loss: 0.2437  lr:0.010000
[ Sat May 18 08:20:19 2024 ] 	Batch(2100/2353) done. Loss: 0.0811  lr:0.010000
[ Sat May 18 08:20:56 2024 ] 	Batch(2200/2353) done. Loss: 0.1888  lr:0.010000
[ Sat May 18 08:21:32 2024 ] 	Batch(2300/2353) done. Loss: 0.1997  lr:0.010000
[ Sat May 18 08:21:51 2024 ] 	Mean training loss: 0.2296.
[ Sat May 18 08:21:51 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 08:21:52 2024 ] Training epoch: 46
[ Sat May 18 08:21:52 2024 ] 	Batch(0/2353) done. Loss: 0.1369  lr:0.010000
[ Sat May 18 08:22:29 2024 ] 	Batch(100/2353) done. Loss: 0.3018  lr:0.010000
[ Sat May 18 08:23:07 2024 ] 	Batch(200/2353) done. Loss: 0.3025  lr:0.010000
[ Sat May 18 08:23:44 2024 ] 	Batch(300/2353) done. Loss: 0.2757  lr:0.010000
[ Sat May 18 08:24:21 2024 ] 	Batch(400/2353) done. Loss: 0.0106  lr:0.010000
[ Sat May 18 08:24:59 2024 ] 	Batch(500/2353) done. Loss: 0.2378  lr:0.010000
[ Sat May 18 08:25:36 2024 ] 	Batch(600/2353) done. Loss: 0.1177  lr:0.010000
[ Sat May 18 08:26:13 2024 ] 	Batch(700/2353) done. Loss: 0.0549  lr:0.010000
[ Sat May 18 08:26:51 2024 ] 	Batch(800/2353) done. Loss: 0.1817  lr:0.010000
[ Sat May 18 08:27:28 2024 ] 	Batch(900/2353) done. Loss: 0.0494  lr:0.010000
[ Sat May 18 08:28:04 2024 ] 	Batch(1000/2353) done. Loss: 0.0956  lr:0.010000
[ Sat May 18 08:28:41 2024 ] 	Batch(1100/2353) done. Loss: 0.0581  lr:0.010000
[ Sat May 18 08:29:18 2024 ] 	Batch(1200/2353) done. Loss: 0.1190  lr:0.010000
[ Sat May 18 08:29:54 2024 ] 	Batch(1300/2353) done. Loss: 0.1423  lr:0.010000
[ Sat May 18 08:30:31 2024 ] 	Batch(1400/2353) done. Loss: 0.0811  lr:0.010000
[ Sat May 18 08:31:08 2024 ] 	Batch(1500/2353) done. Loss: 0.0598  lr:0.010000
[ Sat May 18 08:31:44 2024 ] 	Batch(1600/2353) done. Loss: 0.0697  lr:0.010000
[ Sat May 18 08:32:21 2024 ] 	Batch(1700/2353) done. Loss: 0.3329  lr:0.010000
[ Sat May 18 08:32:58 2024 ] 	Batch(1800/2353) done. Loss: 0.3018  lr:0.010000
[ Sat May 18 08:33:34 2024 ] 	Batch(1900/2353) done. Loss: 0.1469  lr:0.010000
[ Sat May 18 08:34:11 2024 ] 	Batch(2000/2353) done. Loss: 0.2460  lr:0.010000
[ Sat May 18 08:34:49 2024 ] 	Batch(2100/2353) done. Loss: 0.0640  lr:0.010000
[ Sat May 18 08:35:27 2024 ] 	Batch(2200/2353) done. Loss: 0.2339  lr:0.010000
[ Sat May 18 08:36:04 2024 ] 	Batch(2300/2353) done. Loss: 0.1555  lr:0.010000
[ Sat May 18 08:36:23 2024 ] 	Mean training loss: 0.2187.
[ Sat May 18 08:36:23 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 08:36:23 2024 ] Training epoch: 47
[ Sat May 18 08:36:24 2024 ] 	Batch(0/2353) done. Loss: 0.1636  lr:0.010000
[ Sat May 18 08:37:01 2024 ] 	Batch(100/2353) done. Loss: 0.1663  lr:0.010000
[ Sat May 18 08:37:37 2024 ] 	Batch(200/2353) done. Loss: 0.1802  lr:0.010000
[ Sat May 18 08:38:14 2024 ] 	Batch(300/2353) done. Loss: 0.0315  lr:0.010000
[ Sat May 18 08:38:51 2024 ] 	Batch(400/2353) done. Loss: 0.2102  lr:0.010000
[ Sat May 18 08:39:27 2024 ] 	Batch(500/2353) done. Loss: 0.0627  lr:0.010000
[ Sat May 18 08:40:04 2024 ] 	Batch(600/2353) done. Loss: 0.1286  lr:0.010000
[ Sat May 18 08:40:40 2024 ] 	Batch(700/2353) done. Loss: 0.2304  lr:0.010000
[ Sat May 18 08:41:17 2024 ] 	Batch(800/2353) done. Loss: 0.0340  lr:0.010000
[ Sat May 18 08:41:54 2024 ] 	Batch(900/2353) done. Loss: 0.2216  lr:0.010000
[ Sat May 18 08:42:30 2024 ] 	Batch(1000/2353) done. Loss: 0.0497  lr:0.010000
[ Sat May 18 08:43:07 2024 ] 	Batch(1100/2353) done. Loss: 0.1915  lr:0.010000
[ Sat May 18 08:43:44 2024 ] 	Batch(1200/2353) done. Loss: 0.1062  lr:0.010000
[ Sat May 18 08:44:20 2024 ] 	Batch(1300/2353) done. Loss: 0.3113  lr:0.010000
[ Sat May 18 08:44:57 2024 ] 	Batch(1400/2353) done. Loss: 0.6273  lr:0.010000
[ Sat May 18 08:45:34 2024 ] 	Batch(1500/2353) done. Loss: 0.3002  lr:0.010000
[ Sat May 18 08:46:10 2024 ] 	Batch(1600/2353) done. Loss: 0.2217  lr:0.010000
[ Sat May 18 08:46:47 2024 ] 	Batch(1700/2353) done. Loss: 0.3946  lr:0.010000
[ Sat May 18 08:47:24 2024 ] 	Batch(1800/2353) done. Loss: 0.0196  lr:0.010000
[ Sat May 18 08:48:00 2024 ] 	Batch(1900/2353) done. Loss: 0.3150  lr:0.010000
[ Sat May 18 08:48:37 2024 ] 	Batch(2000/2353) done. Loss: 0.3033  lr:0.010000
[ Sat May 18 08:49:14 2024 ] 	Batch(2100/2353) done. Loss: 0.5133  lr:0.010000
[ Sat May 18 08:49:50 2024 ] 	Batch(2200/2353) done. Loss: 0.4214  lr:0.010000
[ Sat May 18 08:50:27 2024 ] 	Batch(2300/2353) done. Loss: 0.1263  lr:0.010000
[ Sat May 18 08:50:46 2024 ] 	Mean training loss: 0.2119.
[ Sat May 18 08:50:46 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 08:50:46 2024 ] Training epoch: 48
[ Sat May 18 08:50:47 2024 ] 	Batch(0/2353) done. Loss: 0.1126  lr:0.010000
[ Sat May 18 08:51:23 2024 ] 	Batch(100/2353) done. Loss: 0.1345  lr:0.010000
[ Sat May 18 08:52:00 2024 ] 	Batch(200/2353) done. Loss: 0.2933  lr:0.010000
[ Sat May 18 08:52:37 2024 ] 	Batch(300/2353) done. Loss: 0.1757  lr:0.010000
[ Sat May 18 08:53:13 2024 ] 	Batch(400/2353) done. Loss: 0.0953  lr:0.010000
[ Sat May 18 08:53:50 2024 ] 	Batch(500/2353) done. Loss: 0.2118  lr:0.010000
[ Sat May 18 08:54:27 2024 ] 	Batch(600/2353) done. Loss: 0.1796  lr:0.010000
[ Sat May 18 08:55:03 2024 ] 	Batch(700/2353) done. Loss: 0.2067  lr:0.010000
[ Sat May 18 08:55:40 2024 ] 	Batch(800/2353) done. Loss: 0.2911  lr:0.010000
[ Sat May 18 08:56:16 2024 ] 	Batch(900/2353) done. Loss: 0.0646  lr:0.010000
[ Sat May 18 08:56:53 2024 ] 	Batch(1000/2353) done. Loss: 0.1062  lr:0.010000
[ Sat May 18 08:57:30 2024 ] 	Batch(1100/2353) done. Loss: 0.4633  lr:0.010000
[ Sat May 18 08:58:06 2024 ] 	Batch(1200/2353) done. Loss: 0.2468  lr:0.010000
[ Sat May 18 08:58:43 2024 ] 	Batch(1300/2353) done. Loss: 0.3179  lr:0.010000
[ Sat May 18 08:59:20 2024 ] 	Batch(1400/2353) done. Loss: 0.2027  lr:0.010000
[ Sat May 18 08:59:57 2024 ] 	Batch(1500/2353) done. Loss: 0.0610  lr:0.010000
[ Sat May 18 09:00:33 2024 ] 	Batch(1600/2353) done. Loss: 0.2488  lr:0.010000
[ Sat May 18 09:01:10 2024 ] 	Batch(1700/2353) done. Loss: 0.3183  lr:0.010000
[ Sat May 18 09:01:47 2024 ] 	Batch(1800/2353) done. Loss: 0.2643  lr:0.010000
[ Sat May 18 09:02:23 2024 ] 	Batch(1900/2353) done. Loss: 0.0526  lr:0.010000
[ Sat May 18 09:03:00 2024 ] 	Batch(2000/2353) done. Loss: 0.3549  lr:0.010000
[ Sat May 18 09:03:37 2024 ] 	Batch(2100/2353) done. Loss: 0.4219  lr:0.010000
[ Sat May 18 09:04:13 2024 ] 	Batch(2200/2353) done. Loss: 0.1426  lr:0.010000
[ Sat May 18 09:04:50 2024 ] 	Batch(2300/2353) done. Loss: 0.0304  lr:0.010000
[ Sat May 18 09:05:09 2024 ] 	Mean training loss: 0.2067.
[ Sat May 18 09:05:09 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 09:05:09 2024 ] Training epoch: 49
[ Sat May 18 09:05:10 2024 ] 	Batch(0/2353) done. Loss: 0.0418  lr:0.010000
[ Sat May 18 09:05:47 2024 ] 	Batch(100/2353) done. Loss: 0.2774  lr:0.010000
[ Sat May 18 09:06:23 2024 ] 	Batch(200/2353) done. Loss: 0.0410  lr:0.010000
[ Sat May 18 09:07:00 2024 ] 	Batch(300/2353) done. Loss: 0.2331  lr:0.010000
[ Sat May 18 09:07:37 2024 ] 	Batch(400/2353) done. Loss: 0.0243  lr:0.010000
[ Sat May 18 09:08:13 2024 ] 	Batch(500/2353) done. Loss: 0.5044  lr:0.010000
[ Sat May 18 09:08:50 2024 ] 	Batch(600/2353) done. Loss: 0.1237  lr:0.010000
[ Sat May 18 09:09:26 2024 ] 	Batch(700/2353) done. Loss: 0.2049  lr:0.010000
[ Sat May 18 09:10:03 2024 ] 	Batch(800/2353) done. Loss: 0.2460  lr:0.010000
[ Sat May 18 09:10:40 2024 ] 	Batch(900/2353) done. Loss: 0.0667  lr:0.010000
[ Sat May 18 09:11:16 2024 ] 	Batch(1000/2353) done. Loss: 0.1118  lr:0.010000
[ Sat May 18 09:11:53 2024 ] 	Batch(1100/2353) done. Loss: 0.1030  lr:0.010000
[ Sat May 18 09:12:30 2024 ] 	Batch(1200/2353) done. Loss: 0.1992  lr:0.010000
[ Sat May 18 09:13:06 2024 ] 	Batch(1300/2353) done. Loss: 0.1805  lr:0.010000
[ Sat May 18 09:13:43 2024 ] 	Batch(1400/2353) done. Loss: 0.1867  lr:0.010000
[ Sat May 18 09:14:20 2024 ] 	Batch(1500/2353) done. Loss: 0.0999  lr:0.010000
[ Sat May 18 09:14:56 2024 ] 	Batch(1600/2353) done. Loss: 0.0455  lr:0.010000
[ Sat May 18 09:15:33 2024 ] 	Batch(1700/2353) done. Loss: 0.6035  lr:0.010000
[ Sat May 18 09:16:10 2024 ] 	Batch(1800/2353) done. Loss: 0.1439  lr:0.010000
[ Sat May 18 09:16:46 2024 ] 	Batch(1900/2353) done. Loss: 0.2336  lr:0.010000
[ Sat May 18 09:17:23 2024 ] 	Batch(2000/2353) done. Loss: 0.0538  lr:0.010000
[ Sat May 18 09:18:00 2024 ] 	Batch(2100/2353) done. Loss: 0.2612  lr:0.010000
[ Sat May 18 09:18:36 2024 ] 	Batch(2200/2353) done. Loss: 0.2191  lr:0.010000
[ Sat May 18 09:19:13 2024 ] 	Batch(2300/2353) done. Loss: 0.2086  lr:0.010000
[ Sat May 18 09:19:32 2024 ] 	Mean training loss: 0.1956.
[ Sat May 18 09:19:32 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 09:19:32 2024 ] Training epoch: 50
[ Sat May 18 09:19:33 2024 ] 	Batch(0/2353) done. Loss: 0.0083  lr:0.010000
[ Sat May 18 09:20:09 2024 ] 	Batch(100/2353) done. Loss: 0.1297  lr:0.010000
[ Sat May 18 09:20:46 2024 ] 	Batch(200/2353) done. Loss: 0.1505  lr:0.010000
[ Sat May 18 09:21:23 2024 ] 	Batch(300/2353) done. Loss: 0.2150  lr:0.010000
[ Sat May 18 09:22:00 2024 ] 	Batch(400/2353) done. Loss: 0.4404  lr:0.010000
[ Sat May 18 09:22:37 2024 ] 	Batch(500/2353) done. Loss: 0.1429  lr:0.010000
[ Sat May 18 09:23:14 2024 ] 	Batch(600/2353) done. Loss: 0.0833  lr:0.010000
[ Sat May 18 09:23:51 2024 ] 	Batch(700/2353) done. Loss: 0.5245  lr:0.010000
[ Sat May 18 09:24:27 2024 ] 	Batch(800/2353) done. Loss: 0.0956  lr:0.010000
[ Sat May 18 09:25:04 2024 ] 	Batch(900/2353) done. Loss: 0.1248  lr:0.010000
[ Sat May 18 09:25:41 2024 ] 	Batch(1000/2353) done. Loss: 0.1309  lr:0.010000
[ Sat May 18 09:26:18 2024 ] 	Batch(1100/2353) done. Loss: 0.2399  lr:0.010000
[ Sat May 18 09:26:55 2024 ] 	Batch(1200/2353) done. Loss: 0.1348  lr:0.010000
[ Sat May 18 09:27:31 2024 ] 	Batch(1300/2353) done. Loss: 0.0886  lr:0.010000
[ Sat May 18 09:28:08 2024 ] 	Batch(1400/2353) done. Loss: 0.0740  lr:0.010000
[ Sat May 18 09:28:45 2024 ] 	Batch(1500/2353) done. Loss: 0.4070  lr:0.010000
[ Sat May 18 09:29:22 2024 ] 	Batch(1600/2353) done. Loss: 0.0865  lr:0.010000
[ Sat May 18 09:29:59 2024 ] 	Batch(1700/2353) done. Loss: 0.3493  lr:0.010000
[ Sat May 18 09:30:36 2024 ] 	Batch(1800/2353) done. Loss: 0.0222  lr:0.010000
[ Sat May 18 09:31:12 2024 ] 	Batch(1900/2353) done. Loss: 0.1777  lr:0.010000
[ Sat May 18 09:31:49 2024 ] 	Batch(2000/2353) done. Loss: 0.2012  lr:0.010000
[ Sat May 18 09:32:26 2024 ] 	Batch(2100/2353) done. Loss: 0.1043  lr:0.010000
[ Sat May 18 09:33:03 2024 ] 	Batch(2200/2353) done. Loss: 0.1890  lr:0.010000
[ Sat May 18 09:33:40 2024 ] 	Batch(2300/2353) done. Loss: 0.0738  lr:0.010000
[ Sat May 18 09:33:59 2024 ] 	Mean training loss: 0.2004.
[ Sat May 18 09:33:59 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 09:33:59 2024 ] Eval epoch: 50
[ Sat May 18 09:36:02 2024 ] 	Mean val loss of 2367 batches: 0.3347318800637772.
[ Sat May 18 09:36:02 2024 ] Training epoch: 51
[ Sat May 18 09:36:03 2024 ] 	Batch(0/2353) done. Loss: 0.1639  lr:0.010000
[ Sat May 18 09:36:40 2024 ] 	Batch(100/2353) done. Loss: 0.0801  lr:0.010000
[ Sat May 18 09:37:17 2024 ] 	Batch(200/2353) done. Loss: 0.1392  lr:0.010000
[ Sat May 18 09:37:53 2024 ] 	Batch(300/2353) done. Loss: 0.0950  lr:0.010000
[ Sat May 18 09:38:30 2024 ] 	Batch(400/2353) done. Loss: 0.0445  lr:0.010000
[ Sat May 18 09:39:07 2024 ] 	Batch(500/2353) done. Loss: 0.3264  lr:0.010000
[ Sat May 18 09:39:44 2024 ] 	Batch(600/2353) done. Loss: 0.1505  lr:0.010000
[ Sat May 18 09:40:21 2024 ] 	Batch(700/2353) done. Loss: 0.1591  lr:0.010000
[ Sat May 18 09:40:57 2024 ] 	Batch(800/2353) done. Loss: 0.1885  lr:0.010000
[ Sat May 18 09:41:34 2024 ] 	Batch(900/2353) done. Loss: 0.4311  lr:0.010000
[ Sat May 18 09:42:11 2024 ] 	Batch(1000/2353) done. Loss: 0.1222  lr:0.010000
[ Sat May 18 09:42:48 2024 ] 	Batch(1100/2353) done. Loss: 0.3865  lr:0.010000
[ Sat May 18 09:43:25 2024 ] 	Batch(1200/2353) done. Loss: 0.0981  lr:0.010000
[ Sat May 18 09:44:01 2024 ] 	Batch(1300/2353) done. Loss: 0.0492  lr:0.010000
[ Sat May 18 09:44:38 2024 ] 	Batch(1400/2353) done. Loss: 0.1109  lr:0.010000
[ Sat May 18 09:45:15 2024 ] 	Batch(1500/2353) done. Loss: 0.3046  lr:0.010000
[ Sat May 18 09:45:52 2024 ] 	Batch(1600/2353) done. Loss: 0.0366  lr:0.010000
[ Sat May 18 09:46:29 2024 ] 	Batch(1700/2353) done. Loss: 0.4852  lr:0.010000
[ Sat May 18 09:47:05 2024 ] 	Batch(1800/2353) done. Loss: 0.3416  lr:0.010000
[ Sat May 18 09:47:42 2024 ] 	Batch(1900/2353) done. Loss: 0.2219  lr:0.010000
[ Sat May 18 09:48:19 2024 ] 	Batch(2000/2353) done. Loss: 0.0474  lr:0.010000
[ Sat May 18 09:48:56 2024 ] 	Batch(2100/2353) done. Loss: 0.1947  lr:0.010000
[ Sat May 18 09:49:33 2024 ] 	Batch(2200/2353) done. Loss: 0.0453  lr:0.010000
[ Sat May 18 09:50:09 2024 ] 	Batch(2300/2353) done. Loss: 0.0148  lr:0.010000
[ Sat May 18 09:50:29 2024 ] 	Mean training loss: 0.1832.
[ Sat May 18 09:50:29 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 09:50:29 2024 ] Training epoch: 52
[ Sat May 18 09:50:29 2024 ] 	Batch(0/2353) done. Loss: 0.0629  lr:0.010000
[ Sat May 18 09:51:06 2024 ] 	Batch(100/2353) done. Loss: 0.1493  lr:0.010000
[ Sat May 18 09:51:43 2024 ] 	Batch(200/2353) done. Loss: 0.0107  lr:0.010000
[ Sat May 18 09:52:20 2024 ] 	Batch(300/2353) done. Loss: 0.0604  lr:0.010000
[ Sat May 18 09:52:57 2024 ] 	Batch(400/2353) done. Loss: 0.1190  lr:0.010000
[ Sat May 18 09:53:34 2024 ] 	Batch(500/2353) done. Loss: 0.0649  lr:0.010000
[ Sat May 18 09:54:10 2024 ] 	Batch(600/2353) done. Loss: 0.2498  lr:0.010000
[ Sat May 18 09:54:47 2024 ] 	Batch(700/2353) done. Loss: 0.1253  lr:0.010000
[ Sat May 18 09:55:24 2024 ] 	Batch(800/2353) done. Loss: 0.2408  lr:0.010000
[ Sat May 18 09:56:00 2024 ] 	Batch(900/2353) done. Loss: 0.1228  lr:0.010000
[ Sat May 18 09:56:37 2024 ] 	Batch(1000/2353) done. Loss: 0.1703  lr:0.010000
[ Sat May 18 09:57:13 2024 ] 	Batch(1100/2353) done. Loss: 0.4574  lr:0.010000
[ Sat May 18 09:57:50 2024 ] 	Batch(1200/2353) done. Loss: 0.3141  lr:0.010000
[ Sat May 18 09:58:27 2024 ] 	Batch(1300/2353) done. Loss: 0.1357  lr:0.010000
[ Sat May 18 09:59:05 2024 ] 	Batch(1400/2353) done. Loss: 0.5031  lr:0.010000
[ Sat May 18 09:59:42 2024 ] 	Batch(1500/2353) done. Loss: 0.3577  lr:0.010000
[ Sat May 18 10:00:19 2024 ] 	Batch(1600/2353) done. Loss: 0.1332  lr:0.010000
[ Sat May 18 10:00:56 2024 ] 	Batch(1700/2353) done. Loss: 0.0783  lr:0.010000
[ Sat May 18 10:01:33 2024 ] 	Batch(1800/2353) done. Loss: 0.0823  lr:0.010000
[ Sat May 18 10:02:09 2024 ] 	Batch(1900/2353) done. Loss: 0.1026  lr:0.010000
[ Sat May 18 10:02:46 2024 ] 	Batch(2000/2353) done. Loss: 0.1629  lr:0.010000
[ Sat May 18 10:03:23 2024 ] 	Batch(2100/2353) done. Loss: 0.0972  lr:0.010000
[ Sat May 18 10:03:59 2024 ] 	Batch(2200/2353) done. Loss: 0.3645  lr:0.010000
[ Sat May 18 10:04:36 2024 ] 	Batch(2300/2353) done. Loss: 0.2824  lr:0.010000
[ Sat May 18 10:04:55 2024 ] 	Mean training loss: 0.1858.
[ Sat May 18 10:04:55 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 10:04:55 2024 ] Training epoch: 53
[ Sat May 18 10:04:56 2024 ] 	Batch(0/2353) done. Loss: 0.2836  lr:0.010000
[ Sat May 18 10:05:33 2024 ] 	Batch(100/2353) done. Loss: 0.0228  lr:0.010000
[ Sat May 18 10:06:09 2024 ] 	Batch(200/2353) done. Loss: 0.6899  lr:0.010000
[ Sat May 18 10:06:46 2024 ] 	Batch(300/2353) done. Loss: 0.1278  lr:0.010000
[ Sat May 18 10:07:22 2024 ] 	Batch(400/2353) done. Loss: 0.1102  lr:0.010000
[ Sat May 18 10:07:59 2024 ] 	Batch(500/2353) done. Loss: 0.2351  lr:0.010000
[ Sat May 18 10:08:36 2024 ] 	Batch(600/2353) done. Loss: 0.1886  lr:0.010000
[ Sat May 18 10:09:12 2024 ] 	Batch(700/2353) done. Loss: 0.0321  lr:0.010000
[ Sat May 18 10:09:49 2024 ] 	Batch(800/2353) done. Loss: 0.1806  lr:0.010000
[ Sat May 18 10:10:25 2024 ] 	Batch(900/2353) done. Loss: 0.1022  lr:0.010000
[ Sat May 18 10:11:02 2024 ] 	Batch(1000/2353) done. Loss: 0.0754  lr:0.010000
[ Sat May 18 10:11:39 2024 ] 	Batch(1100/2353) done. Loss: 0.1358  lr:0.010000
[ Sat May 18 10:12:15 2024 ] 	Batch(1200/2353) done. Loss: 0.1243  lr:0.010000
[ Sat May 18 10:12:52 2024 ] 	Batch(1300/2353) done. Loss: 0.1560  lr:0.010000
[ Sat May 18 10:13:29 2024 ] 	Batch(1400/2353) done. Loss: 0.1874  lr:0.010000
[ Sat May 18 10:14:05 2024 ] 	Batch(1500/2353) done. Loss: 0.0441  lr:0.010000
[ Sat May 18 10:14:42 2024 ] 	Batch(1600/2353) done. Loss: 0.1035  lr:0.010000
[ Sat May 18 10:15:19 2024 ] 	Batch(1700/2353) done. Loss: 0.1753  lr:0.010000
[ Sat May 18 10:15:55 2024 ] 	Batch(1800/2353) done. Loss: 0.1647  lr:0.010000
[ Sat May 18 10:16:32 2024 ] 	Batch(1900/2353) done. Loss: 0.2686  lr:0.010000
[ Sat May 18 10:17:09 2024 ] 	Batch(2000/2353) done. Loss: 0.4101  lr:0.010000
[ Sat May 18 10:17:45 2024 ] 	Batch(2100/2353) done. Loss: 0.4048  lr:0.010000
[ Sat May 18 10:18:22 2024 ] 	Batch(2200/2353) done. Loss: 0.4560  lr:0.010000
[ Sat May 18 10:18:58 2024 ] 	Batch(2300/2353) done. Loss: 0.1432  lr:0.010000
[ Sat May 18 10:19:18 2024 ] 	Mean training loss: 0.1849.
[ Sat May 18 10:19:18 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 10:19:18 2024 ] Training epoch: 54
[ Sat May 18 10:19:18 2024 ] 	Batch(0/2353) done. Loss: 0.1141  lr:0.010000
[ Sat May 18 10:19:55 2024 ] 	Batch(100/2353) done. Loss: 0.1833  lr:0.010000
[ Sat May 18 10:20:32 2024 ] 	Batch(200/2353) done. Loss: 0.4262  lr:0.010000
[ Sat May 18 10:21:08 2024 ] 	Batch(300/2353) done. Loss: 0.0720  lr:0.010000
[ Sat May 18 10:21:45 2024 ] 	Batch(400/2353) done. Loss: 0.3088  lr:0.010000
[ Sat May 18 10:22:22 2024 ] 	Batch(500/2353) done. Loss: 0.3339  lr:0.010000
[ Sat May 18 10:22:58 2024 ] 	Batch(600/2353) done. Loss: 0.1315  lr:0.010000
[ Sat May 18 10:23:35 2024 ] 	Batch(700/2353) done. Loss: 0.1519  lr:0.010000
[ Sat May 18 10:24:12 2024 ] 	Batch(800/2353) done. Loss: 0.1722  lr:0.010000
[ Sat May 18 10:24:48 2024 ] 	Batch(900/2353) done. Loss: 0.1188  lr:0.010000
[ Sat May 18 10:25:25 2024 ] 	Batch(1000/2353) done. Loss: 0.1400  lr:0.010000
[ Sat May 18 10:26:01 2024 ] 	Batch(1100/2353) done. Loss: 0.4744  lr:0.010000
[ Sat May 18 10:26:38 2024 ] 	Batch(1200/2353) done. Loss: 0.1860  lr:0.010000
[ Sat May 18 10:27:15 2024 ] 	Batch(1300/2353) done. Loss: 0.1191  lr:0.010000
[ Sat May 18 10:27:51 2024 ] 	Batch(1400/2353) done. Loss: 0.1501  lr:0.010000
[ Sat May 18 10:28:28 2024 ] 	Batch(1500/2353) done. Loss: 0.0466  lr:0.010000
[ Sat May 18 10:29:05 2024 ] 	Batch(1600/2353) done. Loss: 0.2295  lr:0.010000
[ Sat May 18 10:29:41 2024 ] 	Batch(1700/2353) done. Loss: 0.1102  lr:0.010000
[ Sat May 18 10:30:18 2024 ] 	Batch(1800/2353) done. Loss: 0.0113  lr:0.010000
[ Sat May 18 10:30:55 2024 ] 	Batch(1900/2353) done. Loss: 0.1255  lr:0.010000
[ Sat May 18 10:31:31 2024 ] 	Batch(2000/2353) done. Loss: 0.0900  lr:0.010000
[ Sat May 18 10:32:08 2024 ] 	Batch(2100/2353) done. Loss: 0.1492  lr:0.010000
[ Sat May 18 10:32:45 2024 ] 	Batch(2200/2353) done. Loss: 0.1959  lr:0.010000
[ Sat May 18 10:33:21 2024 ] 	Batch(2300/2353) done. Loss: 0.1085  lr:0.010000
[ Sat May 18 10:33:41 2024 ] 	Mean training loss: 0.1741.
[ Sat May 18 10:33:41 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 10:33:41 2024 ] Training epoch: 55
[ Sat May 18 10:33:41 2024 ] 	Batch(0/2353) done. Loss: 0.3837  lr:0.010000
[ Sat May 18 10:34:18 2024 ] 	Batch(100/2353) done. Loss: 0.0831  lr:0.010000
[ Sat May 18 10:34:55 2024 ] 	Batch(200/2353) done. Loss: 0.0033  lr:0.010000
[ Sat May 18 10:35:32 2024 ] 	Batch(300/2353) done. Loss: 0.1165  lr:0.010000
[ Sat May 18 10:36:09 2024 ] 	Batch(400/2353) done. Loss: 0.0164  lr:0.010000
[ Sat May 18 10:36:45 2024 ] 	Batch(500/2353) done. Loss: 0.0482  lr:0.010000
[ Sat May 18 10:37:22 2024 ] 	Batch(600/2353) done. Loss: 0.3315  lr:0.010000
[ Sat May 18 10:37:58 2024 ] 	Batch(700/2353) done. Loss: 0.0997  lr:0.010000
[ Sat May 18 10:38:35 2024 ] 	Batch(800/2353) done. Loss: 0.3351  lr:0.010000
[ Sat May 18 10:39:12 2024 ] 	Batch(900/2353) done. Loss: 0.2606  lr:0.010000
[ Sat May 18 10:39:48 2024 ] 	Batch(1000/2353) done. Loss: 0.2799  lr:0.010000
[ Sat May 18 10:40:25 2024 ] 	Batch(1100/2353) done. Loss: 0.2099  lr:0.010000
[ Sat May 18 10:41:02 2024 ] 	Batch(1200/2353) done. Loss: 0.0910  lr:0.010000
[ Sat May 18 10:41:38 2024 ] 	Batch(1300/2353) done. Loss: 0.0184  lr:0.010000
[ Sat May 18 10:42:15 2024 ] 	Batch(1400/2353) done. Loss: 0.0187  lr:0.010000
[ Sat May 18 10:42:52 2024 ] 	Batch(1500/2353) done. Loss: 0.1446  lr:0.010000
[ Sat May 18 10:43:28 2024 ] 	Batch(1600/2353) done. Loss: 0.1696  lr:0.010000
[ Sat May 18 10:44:05 2024 ] 	Batch(1700/2353) done. Loss: 0.0987  lr:0.010000
[ Sat May 18 10:44:42 2024 ] 	Batch(1800/2353) done. Loss: 0.2113  lr:0.010000
[ Sat May 18 10:45:18 2024 ] 	Batch(1900/2353) done. Loss: 0.1450  lr:0.010000
[ Sat May 18 10:45:55 2024 ] 	Batch(2000/2353) done. Loss: 0.3244  lr:0.010000
[ Sat May 18 10:46:32 2024 ] 	Batch(2100/2353) done. Loss: 0.1231  lr:0.010000
[ Sat May 18 10:47:08 2024 ] 	Batch(2200/2353) done. Loss: 0.1694  lr:0.010000
[ Sat May 18 10:47:45 2024 ] 	Batch(2300/2353) done. Loss: 0.2979  lr:0.010000
[ Sat May 18 10:48:04 2024 ] 	Mean training loss: 0.1724.
[ Sat May 18 10:48:04 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 10:48:04 2024 ] Training epoch: 56
[ Sat May 18 10:48:05 2024 ] 	Batch(0/2353) done. Loss: 0.1471  lr:0.010000
[ Sat May 18 10:48:42 2024 ] 	Batch(100/2353) done. Loss: 0.0683  lr:0.010000
[ Sat May 18 10:49:18 2024 ] 	Batch(200/2353) done. Loss: 0.2078  lr:0.010000
[ Sat May 18 10:49:56 2024 ] 	Batch(300/2353) done. Loss: 0.1810  lr:0.010000
[ Sat May 18 10:50:33 2024 ] 	Batch(400/2353) done. Loss: 0.5147  lr:0.010000
[ Sat May 18 10:51:10 2024 ] 	Batch(500/2353) done. Loss: 0.1907  lr:0.010000
[ Sat May 18 10:51:46 2024 ] 	Batch(600/2353) done. Loss: 0.0299  lr:0.010000
[ Sat May 18 10:52:23 2024 ] 	Batch(700/2353) done. Loss: 0.0704  lr:0.010000
[ Sat May 18 10:53:00 2024 ] 	Batch(800/2353) done. Loss: 0.1074  lr:0.010000
[ Sat May 18 10:53:37 2024 ] 	Batch(900/2353) done. Loss: 0.1143  lr:0.010000
[ Sat May 18 10:54:13 2024 ] 	Batch(1000/2353) done. Loss: 0.2783  lr:0.010000
[ Sat May 18 10:54:50 2024 ] 	Batch(1100/2353) done. Loss: 0.3036  lr:0.010000
[ Sat May 18 10:55:27 2024 ] 	Batch(1200/2353) done. Loss: 0.2072  lr:0.010000
[ Sat May 18 10:56:03 2024 ] 	Batch(1300/2353) done. Loss: 0.1236  lr:0.010000
[ Sat May 18 10:56:40 2024 ] 	Batch(1400/2353) done. Loss: 0.0738  lr:0.010000
[ Sat May 18 10:57:17 2024 ] 	Batch(1500/2353) done. Loss: 0.3943  lr:0.010000
[ Sat May 18 10:57:53 2024 ] 	Batch(1600/2353) done. Loss: 0.3630  lr:0.010000
[ Sat May 18 10:58:30 2024 ] 	Batch(1700/2353) done. Loss: 0.0214  lr:0.010000
[ Sat May 18 10:59:07 2024 ] 	Batch(1800/2353) done. Loss: 0.1651  lr:0.010000
[ Sat May 18 10:59:43 2024 ] 	Batch(1900/2353) done. Loss: 0.0762  lr:0.010000
[ Sat May 18 11:00:20 2024 ] 	Batch(2000/2353) done. Loss: 0.0920  lr:0.010000
[ Sat May 18 11:00:57 2024 ] 	Batch(2100/2353) done. Loss: 0.3589  lr:0.010000
[ Sat May 18 11:01:34 2024 ] 	Batch(2200/2353) done. Loss: 0.3639  lr:0.010000
[ Sat May 18 11:02:10 2024 ] 	Batch(2300/2353) done. Loss: 0.3875  lr:0.010000
[ Sat May 18 11:02:29 2024 ] 	Mean training loss: 0.1691.
[ Sat May 18 11:02:29 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 11:02:30 2024 ] Training epoch: 57
[ Sat May 18 11:02:30 2024 ] 	Batch(0/2353) done. Loss: 0.2933  lr:0.010000
[ Sat May 18 11:03:07 2024 ] 	Batch(100/2353) done. Loss: 0.2712  lr:0.010000
[ Sat May 18 11:03:43 2024 ] 	Batch(200/2353) done. Loss: 0.0348  lr:0.010000
[ Sat May 18 11:04:20 2024 ] 	Batch(300/2353) done. Loss: 0.1024  lr:0.010000
[ Sat May 18 11:04:57 2024 ] 	Batch(400/2353) done. Loss: 0.0465  lr:0.010000
[ Sat May 18 11:05:33 2024 ] 	Batch(500/2353) done. Loss: 0.1524  lr:0.010000
[ Sat May 18 11:06:10 2024 ] 	Batch(600/2353) done. Loss: 0.1720  lr:0.010000
[ Sat May 18 11:06:47 2024 ] 	Batch(700/2353) done. Loss: 0.1667  lr:0.010000
[ Sat May 18 11:07:23 2024 ] 	Batch(800/2353) done. Loss: 0.2253  lr:0.010000
[ Sat May 18 11:08:00 2024 ] 	Batch(900/2353) done. Loss: 0.2799  lr:0.010000
[ Sat May 18 11:08:37 2024 ] 	Batch(1000/2353) done. Loss: 0.2109  lr:0.010000
[ Sat May 18 11:09:13 2024 ] 	Batch(1100/2353) done. Loss: 0.2808  lr:0.010000
[ Sat May 18 11:09:50 2024 ] 	Batch(1200/2353) done. Loss: 0.0504  lr:0.010000
[ Sat May 18 11:10:27 2024 ] 	Batch(1300/2353) done. Loss: 0.2843  lr:0.010000
[ Sat May 18 11:11:04 2024 ] 	Batch(1400/2353) done. Loss: 0.0768  lr:0.010000
[ Sat May 18 11:11:40 2024 ] 	Batch(1500/2353) done. Loss: 0.0444  lr:0.010000
[ Sat May 18 11:12:17 2024 ] 	Batch(1600/2353) done. Loss: 0.0959  lr:0.010000
[ Sat May 18 11:12:54 2024 ] 	Batch(1700/2353) done. Loss: 0.0848  lr:0.010000
[ Sat May 18 11:13:30 2024 ] 	Batch(1800/2353) done. Loss: 0.1066  lr:0.010000
[ Sat May 18 11:14:07 2024 ] 	Batch(1900/2353) done. Loss: 0.0300  lr:0.010000
[ Sat May 18 11:14:44 2024 ] 	Batch(2000/2353) done. Loss: 0.0846  lr:0.010000
[ Sat May 18 11:15:20 2024 ] 	Batch(2100/2353) done. Loss: 0.4636  lr:0.010000
[ Sat May 18 11:15:57 2024 ] 	Batch(2200/2353) done. Loss: 0.0734  lr:0.010000
[ Sat May 18 11:16:34 2024 ] 	Batch(2300/2353) done. Loss: 0.2975  lr:0.010000
[ Sat May 18 11:16:53 2024 ] 	Mean training loss: 0.1634.
[ Sat May 18 11:16:53 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 11:16:53 2024 ] Training epoch: 58
[ Sat May 18 11:16:54 2024 ] 	Batch(0/2353) done. Loss: 0.0450  lr:0.010000
[ Sat May 18 11:17:30 2024 ] 	Batch(100/2353) done. Loss: 0.2962  lr:0.010000
[ Sat May 18 11:18:07 2024 ] 	Batch(200/2353) done. Loss: 0.2264  lr:0.010000
[ Sat May 18 11:18:44 2024 ] 	Batch(300/2353) done. Loss: 0.0623  lr:0.010000
[ Sat May 18 11:19:22 2024 ] 	Batch(400/2353) done. Loss: 0.0422  lr:0.010000
[ Sat May 18 11:19:59 2024 ] 	Batch(500/2353) done. Loss: 0.1564  lr:0.010000
[ Sat May 18 11:20:37 2024 ] 	Batch(600/2353) done. Loss: 0.2227  lr:0.010000
[ Sat May 18 11:21:14 2024 ] 	Batch(700/2353) done. Loss: 0.0534  lr:0.010000
[ Sat May 18 11:21:51 2024 ] 	Batch(800/2353) done. Loss: 0.2504  lr:0.010000
[ Sat May 18 11:22:29 2024 ] 	Batch(900/2353) done. Loss: 0.1642  lr:0.010000
[ Sat May 18 11:23:06 2024 ] 	Batch(1000/2353) done. Loss: 0.1134  lr:0.010000
[ Sat May 18 11:23:43 2024 ] 	Batch(1100/2353) done. Loss: 0.0479  lr:0.010000
[ Sat May 18 11:24:21 2024 ] 	Batch(1200/2353) done. Loss: 0.0214  lr:0.010000
[ Sat May 18 11:24:57 2024 ] 	Batch(1300/2353) done. Loss: 0.2511  lr:0.010000
[ Sat May 18 11:25:34 2024 ] 	Batch(1400/2353) done. Loss: 0.1182  lr:0.010000
[ Sat May 18 11:26:11 2024 ] 	Batch(1500/2353) done. Loss: 0.0566  lr:0.010000
[ Sat May 18 11:26:48 2024 ] 	Batch(1600/2353) done. Loss: 0.3950  lr:0.010000
[ Sat May 18 11:27:25 2024 ] 	Batch(1700/2353) done. Loss: 0.3868  lr:0.010000
[ Sat May 18 11:28:02 2024 ] 	Batch(1800/2353) done. Loss: 0.0102  lr:0.010000
[ Sat May 18 11:28:39 2024 ] 	Batch(1900/2353) done. Loss: 0.1116  lr:0.010000
[ Sat May 18 11:29:16 2024 ] 	Batch(2000/2353) done. Loss: 0.5171  lr:0.010000
[ Sat May 18 11:29:52 2024 ] 	Batch(2100/2353) done. Loss: 0.6491  lr:0.010000
[ Sat May 18 11:30:29 2024 ] 	Batch(2200/2353) done. Loss: 0.1115  lr:0.010000
[ Sat May 18 11:31:06 2024 ] 	Batch(2300/2353) done. Loss: 0.0141  lr:0.010000
[ Sat May 18 11:31:25 2024 ] 	Mean training loss: 0.1554.
[ Sat May 18 11:31:25 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 11:31:25 2024 ] Training epoch: 59
[ Sat May 18 11:31:26 2024 ] 	Batch(0/2353) done. Loss: 0.0204  lr:0.010000
[ Sat May 18 11:32:03 2024 ] 	Batch(100/2353) done. Loss: 0.0121  lr:0.010000
[ Sat May 18 11:32:40 2024 ] 	Batch(200/2353) done. Loss: 0.0258  lr:0.010000
[ Sat May 18 11:33:17 2024 ] 	Batch(300/2353) done. Loss: 0.0146  lr:0.010000
[ Sat May 18 11:33:55 2024 ] 	Batch(400/2353) done. Loss: 0.0599  lr:0.010000
[ Sat May 18 11:34:32 2024 ] 	Batch(500/2353) done. Loss: 0.0164  lr:0.010000
[ Sat May 18 11:35:09 2024 ] 	Batch(600/2353) done. Loss: 0.2336  lr:0.010000
[ Sat May 18 11:35:47 2024 ] 	Batch(700/2353) done. Loss: 0.0293  lr:0.010000
[ Sat May 18 11:36:24 2024 ] 	Batch(800/2353) done. Loss: 0.1213  lr:0.010000
[ Sat May 18 11:37:02 2024 ] 	Batch(900/2353) done. Loss: 0.0627  lr:0.010000
[ Sat May 18 11:37:39 2024 ] 	Batch(1000/2353) done. Loss: 0.0599  lr:0.010000
[ Sat May 18 11:38:15 2024 ] 	Batch(1100/2353) done. Loss: 0.1424  lr:0.010000
[ Sat May 18 11:38:52 2024 ] 	Batch(1200/2353) done. Loss: 0.0435  lr:0.010000
[ Sat May 18 11:39:29 2024 ] 	Batch(1300/2353) done. Loss: 0.0193  lr:0.010000
[ Sat May 18 11:40:05 2024 ] 	Batch(1400/2353) done. Loss: 0.0632  lr:0.010000
[ Sat May 18 11:40:42 2024 ] 	Batch(1500/2353) done. Loss: 0.2174  lr:0.010000
[ Sat May 18 11:41:19 2024 ] 	Batch(1600/2353) done. Loss: 0.1165  lr:0.010000
[ Sat May 18 11:41:56 2024 ] 	Batch(1700/2353) done. Loss: 0.0936  lr:0.010000
[ Sat May 18 11:42:32 2024 ] 	Batch(1800/2353) done. Loss: 0.0226  lr:0.010000
[ Sat May 18 11:43:09 2024 ] 	Batch(1900/2353) done. Loss: 0.0305  lr:0.010000
[ Sat May 18 11:43:46 2024 ] 	Batch(2000/2353) done. Loss: 0.0636  lr:0.010000
[ Sat May 18 11:44:22 2024 ] 	Batch(2100/2353) done. Loss: 0.1766  lr:0.010000
[ Sat May 18 11:44:59 2024 ] 	Batch(2200/2353) done. Loss: 0.1333  lr:0.010000
[ Sat May 18 11:45:36 2024 ] 	Batch(2300/2353) done. Loss: 0.0654  lr:0.010000
[ Sat May 18 11:45:55 2024 ] 	Mean training loss: 0.1474.
[ Sat May 18 11:45:55 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 11:45:55 2024 ] Training epoch: 60
[ Sat May 18 11:45:56 2024 ] 	Batch(0/2353) done. Loss: 0.5225  lr:0.010000
[ Sat May 18 11:46:32 2024 ] 	Batch(100/2353) done. Loss: 0.1059  lr:0.010000
[ Sat May 18 11:47:09 2024 ] 	Batch(200/2353) done. Loss: 0.0269  lr:0.010000
[ Sat May 18 11:47:46 2024 ] 	Batch(300/2353) done. Loss: 0.1364  lr:0.010000
[ Sat May 18 11:48:22 2024 ] 	Batch(400/2353) done. Loss: 0.1831  lr:0.010000
[ Sat May 18 11:48:59 2024 ] 	Batch(500/2353) done. Loss: 0.1048  lr:0.010000
[ Sat May 18 11:49:36 2024 ] 	Batch(600/2353) done. Loss: 0.1079  lr:0.010000
[ Sat May 18 11:50:13 2024 ] 	Batch(700/2353) done. Loss: 0.0397  lr:0.010000
[ Sat May 18 11:50:49 2024 ] 	Batch(800/2353) done. Loss: 0.0550  lr:0.010000
[ Sat May 18 11:51:26 2024 ] 	Batch(900/2353) done. Loss: 0.0872  lr:0.010000
[ Sat May 18 11:52:03 2024 ] 	Batch(1000/2353) done. Loss: 0.2153  lr:0.010000
[ Sat May 18 11:52:39 2024 ] 	Batch(1100/2353) done. Loss: 0.0419  lr:0.010000
[ Sat May 18 11:53:16 2024 ] 	Batch(1200/2353) done. Loss: 0.4868  lr:0.010000
[ Sat May 18 11:53:53 2024 ] 	Batch(1300/2353) done. Loss: 0.1070  lr:0.010000
[ Sat May 18 11:54:30 2024 ] 	Batch(1400/2353) done. Loss: 0.3156  lr:0.010000
[ Sat May 18 11:55:06 2024 ] 	Batch(1500/2353) done. Loss: 0.2032  lr:0.010000
[ Sat May 18 11:55:43 2024 ] 	Batch(1600/2353) done. Loss: 0.0509  lr:0.010000
[ Sat May 18 11:56:20 2024 ] 	Batch(1700/2353) done. Loss: 0.0653  lr:0.010000
[ Sat May 18 11:56:56 2024 ] 	Batch(1800/2353) done. Loss: 0.0903  lr:0.010000
[ Sat May 18 11:57:33 2024 ] 	Batch(1900/2353) done. Loss: 0.1382  lr:0.010000
[ Sat May 18 11:58:10 2024 ] 	Batch(2000/2353) done. Loss: 0.0502  lr:0.010000
[ Sat May 18 11:58:46 2024 ] 	Batch(2100/2353) done. Loss: 0.1301  lr:0.010000
[ Sat May 18 11:59:23 2024 ] 	Batch(2200/2353) done. Loss: 0.4271  lr:0.010000
[ Sat May 18 12:00:00 2024 ] 	Batch(2300/2353) done. Loss: 0.1560  lr:0.010000
[ Sat May 18 12:00:19 2024 ] 	Mean training loss: 0.1502.
[ Sat May 18 12:00:19 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 12:00:19 2024 ] Eval epoch: 60
[ Sat May 18 12:02:22 2024 ] 	Mean val loss of 2367 batches: 0.3099725496458312.
[ Sat May 18 12:02:22 2024 ] Training epoch: 61
[ Sat May 18 12:02:23 2024 ] 	Batch(0/2353) done. Loss: 0.0161  lr:0.000100
[ Sat May 18 12:03:00 2024 ] 	Batch(100/2353) done. Loss: 0.0510  lr:0.000100
[ Sat May 18 12:03:36 2024 ] 	Batch(200/2353) done. Loss: 0.0668  lr:0.000100
[ Sat May 18 12:04:13 2024 ] 	Batch(300/2353) done. Loss: 0.0108  lr:0.000100
[ Sat May 18 12:04:49 2024 ] 	Batch(400/2353) done. Loss: 0.1038  lr:0.000100
[ Sat May 18 12:05:26 2024 ] 	Batch(500/2353) done. Loss: 0.1329  lr:0.000100
[ Sat May 18 12:06:03 2024 ] 	Batch(600/2353) done. Loss: 0.0674  lr:0.000100
[ Sat May 18 12:06:39 2024 ] 	Batch(700/2353) done. Loss: 0.0512  lr:0.000100
[ Sat May 18 12:07:16 2024 ] 	Batch(800/2353) done. Loss: 0.1168  lr:0.000100
[ Sat May 18 12:07:52 2024 ] 	Batch(900/2353) done. Loss: 0.0307  lr:0.000100
[ Sat May 18 12:08:29 2024 ] 	Batch(1000/2353) done. Loss: 0.0223  lr:0.000100
[ Sat May 18 12:09:06 2024 ] 	Batch(1100/2353) done. Loss: 0.1666  lr:0.000100
[ Sat May 18 12:09:42 2024 ] 	Batch(1200/2353) done. Loss: 0.0617  lr:0.000100
[ Sat May 18 12:10:19 2024 ] 	Batch(1300/2353) done. Loss: 0.0101  lr:0.000100
[ Sat May 18 12:10:56 2024 ] 	Batch(1400/2353) done. Loss: 0.2208  lr:0.000100
[ Sat May 18 12:11:32 2024 ] 	Batch(1500/2353) done. Loss: 0.0262  lr:0.000100
[ Sat May 18 12:12:09 2024 ] 	Batch(1600/2353) done. Loss: 0.1084  lr:0.000100
[ Sat May 18 12:12:46 2024 ] 	Batch(1700/2353) done. Loss: 0.0242  lr:0.000100
[ Sat May 18 12:13:22 2024 ] 	Batch(1800/2353) done. Loss: 0.0025  lr:0.000100
[ Sat May 18 12:13:59 2024 ] 	Batch(1900/2353) done. Loss: 0.3521  lr:0.000100
[ Sat May 18 12:14:35 2024 ] 	Batch(2000/2353) done. Loss: 0.3582  lr:0.000100
[ Sat May 18 12:15:12 2024 ] 	Batch(2100/2353) done. Loss: 0.0405  lr:0.000100
[ Sat May 18 12:15:49 2024 ] 	Batch(2200/2353) done. Loss: 0.2103  lr:0.000100
[ Sat May 18 12:16:26 2024 ] 	Batch(2300/2353) done. Loss: 0.0277  lr:0.000100
[ Sat May 18 12:16:46 2024 ] 	Mean training loss: 0.1038.
[ Sat May 18 12:16:46 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 12:16:46 2024 ] Training epoch: 62
[ Sat May 18 12:16:46 2024 ] 	Batch(0/2353) done. Loss: 0.0208  lr:0.000100
[ Sat May 18 12:17:23 2024 ] 	Batch(100/2353) done. Loss: 0.1730  lr:0.000100
[ Sat May 18 12:18:00 2024 ] 	Batch(200/2353) done. Loss: 0.0111  lr:0.000100
[ Sat May 18 12:18:36 2024 ] 	Batch(300/2353) done. Loss: 0.0381  lr:0.000100
[ Sat May 18 12:19:13 2024 ] 	Batch(400/2353) done. Loss: 0.0607  lr:0.000100
[ Sat May 18 12:19:50 2024 ] 	Batch(500/2353) done. Loss: 0.0552  lr:0.000100
[ Sat May 18 12:20:26 2024 ] 	Batch(600/2353) done. Loss: 0.0138  lr:0.000100
[ Sat May 18 12:21:03 2024 ] 	Batch(700/2353) done. Loss: 0.0222  lr:0.000100
[ Sat May 18 12:21:40 2024 ] 	Batch(800/2353) done. Loss: 0.0239  lr:0.000100
[ Sat May 18 12:22:16 2024 ] 	Batch(900/2353) done. Loss: 0.0687  lr:0.000100
[ Sat May 18 12:22:53 2024 ] 	Batch(1000/2353) done. Loss: 0.0194  lr:0.000100
[ Sat May 18 12:23:29 2024 ] 	Batch(1100/2353) done. Loss: 0.0581  lr:0.000100
[ Sat May 18 12:24:06 2024 ] 	Batch(1200/2353) done. Loss: 0.0859  lr:0.000100
[ Sat May 18 12:24:43 2024 ] 	Batch(1300/2353) done. Loss: 0.0266  lr:0.000100
[ Sat May 18 12:25:19 2024 ] 	Batch(1400/2353) done. Loss: 0.1040  lr:0.000100
[ Sat May 18 12:25:56 2024 ] 	Batch(1500/2353) done. Loss: 0.0403  lr:0.000100
[ Sat May 18 12:26:33 2024 ] 	Batch(1600/2353) done. Loss: 0.1628  lr:0.000100
[ Sat May 18 12:27:10 2024 ] 	Batch(1700/2353) done. Loss: 0.0831  lr:0.000100
[ Sat May 18 12:27:46 2024 ] 	Batch(1800/2353) done. Loss: 0.0425  lr:0.000100
[ Sat May 18 12:28:23 2024 ] 	Batch(1900/2353) done. Loss: 0.0137  lr:0.000100
[ Sat May 18 12:29:00 2024 ] 	Batch(2000/2353) done. Loss: 0.0136  lr:0.000100
[ Sat May 18 12:29:36 2024 ] 	Batch(2100/2353) done. Loss: 0.0706  lr:0.000100
[ Sat May 18 12:30:13 2024 ] 	Batch(2200/2353) done. Loss: 0.0736  lr:0.000100
[ Sat May 18 12:30:49 2024 ] 	Batch(2300/2353) done. Loss: 0.0338  lr:0.000100
[ Sat May 18 12:31:09 2024 ] 	Mean training loss: 0.0853.
[ Sat May 18 12:31:09 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 12:31:09 2024 ] Training epoch: 63
[ Sat May 18 12:31:09 2024 ] 	Batch(0/2353) done. Loss: 0.0580  lr:0.000100
[ Sat May 18 12:31:46 2024 ] 	Batch(100/2353) done. Loss: 0.1081  lr:0.000100
[ Sat May 18 12:32:23 2024 ] 	Batch(200/2353) done. Loss: 0.0123  lr:0.000100
[ Sat May 18 12:32:59 2024 ] 	Batch(300/2353) done. Loss: 0.2018  lr:0.000100
[ Sat May 18 12:33:36 2024 ] 	Batch(400/2353) done. Loss: 0.0329  lr:0.000100
[ Sat May 18 12:34:12 2024 ] 	Batch(500/2353) done. Loss: 0.1072  lr:0.000100
[ Sat May 18 12:34:49 2024 ] 	Batch(600/2353) done. Loss: 0.0386  lr:0.000100
[ Sat May 18 12:35:26 2024 ] 	Batch(700/2353) done. Loss: 0.0401  lr:0.000100
[ Sat May 18 12:36:02 2024 ] 	Batch(800/2353) done. Loss: 0.0061  lr:0.000100
[ Sat May 18 12:36:39 2024 ] 	Batch(900/2353) done. Loss: 0.0252  lr:0.000100
[ Sat May 18 12:37:16 2024 ] 	Batch(1000/2353) done. Loss: 0.0341  lr:0.000100
[ Sat May 18 12:37:52 2024 ] 	Batch(1100/2353) done. Loss: 0.1567  lr:0.000100
[ Sat May 18 12:38:29 2024 ] 	Batch(1200/2353) done. Loss: 0.0109  lr:0.000100
[ Sat May 18 12:39:06 2024 ] 	Batch(1300/2353) done. Loss: 0.0476  lr:0.000100
[ Sat May 18 12:39:42 2024 ] 	Batch(1400/2353) done. Loss: 0.1654  lr:0.000100
[ Sat May 18 12:40:19 2024 ] 	Batch(1500/2353) done. Loss: 0.0299  lr:0.000100
[ Sat May 18 12:40:55 2024 ] 	Batch(1600/2353) done. Loss: 0.0242  lr:0.000100
[ Sat May 18 12:41:32 2024 ] 	Batch(1700/2353) done. Loss: 0.4862  lr:0.000100
[ Sat May 18 12:42:09 2024 ] 	Batch(1800/2353) done. Loss: 0.0158  lr:0.000100
[ Sat May 18 12:42:45 2024 ] 	Batch(1900/2353) done. Loss: 0.0477  lr:0.000100
[ Sat May 18 12:43:22 2024 ] 	Batch(2000/2353) done. Loss: 0.2069  lr:0.000100
[ Sat May 18 12:43:59 2024 ] 	Batch(2100/2353) done. Loss: 0.1887  lr:0.000100
[ Sat May 18 12:44:35 2024 ] 	Batch(2200/2353) done. Loss: 0.0113  lr:0.000100
[ Sat May 18 12:45:12 2024 ] 	Batch(2300/2353) done. Loss: 0.0212  lr:0.000100
[ Sat May 18 12:45:31 2024 ] 	Mean training loss: 0.0756.
[ Sat May 18 12:45:31 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 12:45:31 2024 ] Training epoch: 64
[ Sat May 18 12:45:32 2024 ] 	Batch(0/2353) done. Loss: 0.1256  lr:0.000100
[ Sat May 18 12:46:09 2024 ] 	Batch(100/2353) done. Loss: 0.0278  lr:0.000100
[ Sat May 18 12:46:46 2024 ] 	Batch(200/2353) done. Loss: 0.0236  lr:0.000100
[ Sat May 18 12:47:22 2024 ] 	Batch(300/2353) done. Loss: 0.0844  lr:0.000100
[ Sat May 18 12:47:59 2024 ] 	Batch(400/2353) done. Loss: 0.0332  lr:0.000100
[ Sat May 18 12:48:36 2024 ] 	Batch(500/2353) done. Loss: 0.0687  lr:0.000100
[ Sat May 18 12:49:13 2024 ] 	Batch(600/2353) done. Loss: 0.0530  lr:0.000100
[ Sat May 18 12:49:50 2024 ] 	Batch(700/2353) done. Loss: 0.0621  lr:0.000100
[ Sat May 18 12:50:27 2024 ] 	Batch(800/2353) done. Loss: 0.0505  lr:0.000100
[ Sat May 18 12:51:03 2024 ] 	Batch(900/2353) done. Loss: 0.1124  lr:0.000100
[ Sat May 18 12:51:40 2024 ] 	Batch(1000/2353) done. Loss: 0.0212  lr:0.000100
[ Sat May 18 12:52:17 2024 ] 	Batch(1100/2353) done. Loss: 0.1128  lr:0.000100
[ Sat May 18 12:52:54 2024 ] 	Batch(1200/2353) done. Loss: 0.0175  lr:0.000100
[ Sat May 18 12:53:31 2024 ] 	Batch(1300/2353) done. Loss: 0.1369  lr:0.000100
[ Sat May 18 12:54:07 2024 ] 	Batch(1400/2353) done. Loss: 0.0256  lr:0.000100
[ Sat May 18 12:54:44 2024 ] 	Batch(1500/2353) done. Loss: 0.0355  lr:0.000100
[ Sat May 18 12:55:21 2024 ] 	Batch(1600/2353) done. Loss: 0.0209  lr:0.000100
[ Sat May 18 12:55:58 2024 ] 	Batch(1700/2353) done. Loss: 0.0133  lr:0.000100
[ Sat May 18 12:56:35 2024 ] 	Batch(1800/2353) done. Loss: 0.1136  lr:0.000100
[ Sat May 18 12:57:12 2024 ] 	Batch(1900/2353) done. Loss: 0.0668  lr:0.000100
[ Sat May 18 12:57:48 2024 ] 	Batch(2000/2353) done. Loss: 0.1886  lr:0.000100
[ Sat May 18 12:58:25 2024 ] 	Batch(2100/2353) done. Loss: 0.1110  lr:0.000100
[ Sat May 18 12:59:02 2024 ] 	Batch(2200/2353) done. Loss: 0.1264  lr:0.000100
[ Sat May 18 12:59:39 2024 ] 	Batch(2300/2353) done. Loss: 0.0068  lr:0.000100
[ Sat May 18 12:59:58 2024 ] 	Mean training loss: 0.0711.
[ Sat May 18 12:59:58 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 12:59:58 2024 ] Training epoch: 65
[ Sat May 18 12:59:59 2024 ] 	Batch(0/2353) done. Loss: 0.2555  lr:0.000100
[ Sat May 18 13:00:36 2024 ] 	Batch(100/2353) done. Loss: 0.0151  lr:0.000100
[ Sat May 18 13:01:12 2024 ] 	Batch(200/2353) done. Loss: 0.0154  lr:0.000100
[ Sat May 18 13:01:49 2024 ] 	Batch(300/2353) done. Loss: 0.0439  lr:0.000100
[ Sat May 18 13:02:26 2024 ] 	Batch(400/2353) done. Loss: 0.0815  lr:0.000100
[ Sat May 18 13:03:03 2024 ] 	Batch(500/2353) done. Loss: 0.0023  lr:0.000100
[ Sat May 18 13:03:39 2024 ] 	Batch(600/2353) done. Loss: 0.1296  lr:0.000100
[ Sat May 18 13:04:16 2024 ] 	Batch(700/2353) done. Loss: 0.1894  lr:0.000100
[ Sat May 18 13:04:53 2024 ] 	Batch(800/2353) done. Loss: 0.1096  lr:0.000100
[ Sat May 18 13:05:30 2024 ] 	Batch(900/2353) done. Loss: 0.0523  lr:0.000100
[ Sat May 18 13:06:07 2024 ] 	Batch(1000/2353) done. Loss: 0.0474  lr:0.000100
[ Sat May 18 13:06:43 2024 ] 	Batch(1100/2353) done. Loss: 0.0102  lr:0.000100
[ Sat May 18 13:07:20 2024 ] 	Batch(1200/2353) done. Loss: 0.2204  lr:0.000100
[ Sat May 18 13:07:58 2024 ] 	Batch(1300/2353) done. Loss: 0.0133  lr:0.000100
[ Sat May 18 13:08:35 2024 ] 	Batch(1400/2353) done. Loss: 0.0187  lr:0.000100
[ Sat May 18 13:09:13 2024 ] 	Batch(1500/2353) done. Loss: 0.0202  lr:0.000100
[ Sat May 18 13:09:50 2024 ] 	Batch(1600/2353) done. Loss: 0.0728  lr:0.000100
[ Sat May 18 13:10:27 2024 ] 	Batch(1700/2353) done. Loss: 0.0063  lr:0.000100
[ Sat May 18 13:11:04 2024 ] 	Batch(1800/2353) done. Loss: 0.0588  lr:0.000100
[ Sat May 18 13:11:41 2024 ] 	Batch(1900/2353) done. Loss: 0.0421  lr:0.000100
[ Sat May 18 13:12:18 2024 ] 	Batch(2000/2353) done. Loss: 0.0290  lr:0.000100
[ Sat May 18 13:12:54 2024 ] 	Batch(2100/2353) done. Loss: 0.0405  lr:0.000100
[ Sat May 18 13:13:31 2024 ] 	Batch(2200/2353) done. Loss: 0.0657  lr:0.000100
[ Sat May 18 13:14:08 2024 ] 	Batch(2300/2353) done. Loss: 0.0676  lr:0.000100
[ Sat May 18 13:14:27 2024 ] 	Mean training loss: 0.0713.
[ Sat May 18 13:14:27 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 13:14:27 2024 ] Training epoch: 66
[ Sat May 18 13:14:28 2024 ] 	Batch(0/2353) done. Loss: 0.0247  lr:0.000100
[ Sat May 18 13:15:05 2024 ] 	Batch(100/2353) done. Loss: 0.0401  lr:0.000100
[ Sat May 18 13:15:41 2024 ] 	Batch(200/2353) done. Loss: 0.0726  lr:0.000100
[ Sat May 18 13:16:18 2024 ] 	Batch(300/2353) done. Loss: 0.0583  lr:0.000100
[ Sat May 18 13:16:55 2024 ] 	Batch(400/2353) done. Loss: 0.0142  lr:0.000100
[ Sat May 18 13:17:32 2024 ] 	Batch(500/2353) done. Loss: 0.0259  lr:0.000100
[ Sat May 18 13:18:09 2024 ] 	Batch(600/2353) done. Loss: 0.0271  lr:0.000100
[ Sat May 18 13:18:46 2024 ] 	Batch(700/2353) done. Loss: 0.0040  lr:0.000100
[ Sat May 18 13:19:23 2024 ] 	Batch(800/2353) done. Loss: 0.1268  lr:0.000100
[ Sat May 18 13:19:59 2024 ] 	Batch(900/2353) done. Loss: 0.0858  lr:0.000100
[ Sat May 18 13:20:36 2024 ] 	Batch(1000/2353) done. Loss: 0.2312  lr:0.000100
[ Sat May 18 13:21:13 2024 ] 	Batch(1100/2353) done. Loss: 0.1264  lr:0.000100
[ Sat May 18 13:21:50 2024 ] 	Batch(1200/2353) done. Loss: 0.0159  lr:0.000100
[ Sat May 18 13:22:27 2024 ] 	Batch(1300/2353) done. Loss: 0.0232  lr:0.000100
[ Sat May 18 13:23:04 2024 ] 	Batch(1400/2353) done. Loss: 0.1136  lr:0.000100
[ Sat May 18 13:23:40 2024 ] 	Batch(1500/2353) done. Loss: 0.0221  lr:0.000100
[ Sat May 18 13:24:17 2024 ] 	Batch(1600/2353) done. Loss: 0.2196  lr:0.000100
[ Sat May 18 13:24:54 2024 ] 	Batch(1700/2353) done. Loss: 0.0542  lr:0.000100
[ Sat May 18 13:25:30 2024 ] 	Batch(1800/2353) done. Loss: 0.0461  lr:0.000100
[ Sat May 18 13:26:07 2024 ] 	Batch(1900/2353) done. Loss: 0.0378  lr:0.000100
[ Sat May 18 13:26:44 2024 ] 	Batch(2000/2353) done. Loss: 0.0256  lr:0.000100
[ Sat May 18 13:27:21 2024 ] 	Batch(2100/2353) done. Loss: 0.0274  lr:0.000100
[ Sat May 18 13:27:57 2024 ] 	Batch(2200/2353) done. Loss: 0.1177  lr:0.000100
[ Sat May 18 13:28:34 2024 ] 	Batch(2300/2353) done. Loss: 0.0278  lr:0.000100
[ Sat May 18 13:28:53 2024 ] 	Mean training loss: 0.0638.
[ Sat May 18 13:28:53 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 13:28:53 2024 ] Training epoch: 67
[ Sat May 18 13:28:54 2024 ] 	Batch(0/2353) done. Loss: 0.0269  lr:0.000100
[ Sat May 18 13:29:32 2024 ] 	Batch(100/2353) done. Loss: 0.0053  lr:0.000100
[ Sat May 18 13:30:09 2024 ] 	Batch(200/2353) done. Loss: 0.0285  lr:0.000100
[ Sat May 18 13:30:47 2024 ] 	Batch(300/2353) done. Loss: 0.0082  lr:0.000100
[ Sat May 18 13:31:25 2024 ] 	Batch(400/2353) done. Loss: 0.0742  lr:0.000100
[ Sat May 18 13:32:02 2024 ] 	Batch(500/2353) done. Loss: 0.0580  lr:0.000100
[ Sat May 18 13:32:39 2024 ] 	Batch(600/2353) done. Loss: 0.1908  lr:0.000100
[ Sat May 18 13:33:17 2024 ] 	Batch(700/2353) done. Loss: 0.0406  lr:0.000100
[ Sat May 18 13:33:54 2024 ] 	Batch(800/2353) done. Loss: 0.0280  lr:0.000100
[ Sat May 18 13:34:31 2024 ] 	Batch(900/2353) done. Loss: 0.0181  lr:0.000100
[ Sat May 18 13:35:08 2024 ] 	Batch(1000/2353) done. Loss: 0.1276  lr:0.000100
[ Sat May 18 13:35:44 2024 ] 	Batch(1100/2353) done. Loss: 0.0440  lr:0.000100
[ Sat May 18 13:36:21 2024 ] 	Batch(1200/2353) done. Loss: 0.0063  lr:0.000100
[ Sat May 18 13:36:58 2024 ] 	Batch(1300/2353) done. Loss: 0.0136  lr:0.000100
[ Sat May 18 13:37:34 2024 ] 	Batch(1400/2353) done. Loss: 0.0745  lr:0.000100
[ Sat May 18 13:38:11 2024 ] 	Batch(1500/2353) done. Loss: 0.0158  lr:0.000100
[ Sat May 18 13:38:48 2024 ] 	Batch(1600/2353) done. Loss: 0.0553  lr:0.000100
[ Sat May 18 13:39:24 2024 ] 	Batch(1700/2353) done. Loss: 0.0694  lr:0.000100
[ Sat May 18 13:40:01 2024 ] 	Batch(1800/2353) done. Loss: 0.0496  lr:0.000100
[ Sat May 18 13:40:38 2024 ] 	Batch(1900/2353) done. Loss: 0.0482  lr:0.000100
[ Sat May 18 13:41:14 2024 ] 	Batch(2000/2353) done. Loss: 0.0208  lr:0.000100
[ Sat May 18 13:41:51 2024 ] 	Batch(2100/2353) done. Loss: 0.0497  lr:0.000100
[ Sat May 18 13:42:28 2024 ] 	Batch(2200/2353) done. Loss: 0.0716  lr:0.000100
[ Sat May 18 13:43:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0209  lr:0.000100
[ Sat May 18 13:43:23 2024 ] 	Mean training loss: 0.0623.
[ Sat May 18 13:43:23 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 13:43:24 2024 ] Training epoch: 68
[ Sat May 18 13:43:24 2024 ] 	Batch(0/2353) done. Loss: 0.0090  lr:0.000100
[ Sat May 18 13:44:01 2024 ] 	Batch(100/2353) done. Loss: 0.1632  lr:0.000100
[ Sat May 18 13:44:37 2024 ] 	Batch(200/2353) done. Loss: 0.1852  lr:0.000100
[ Sat May 18 13:45:14 2024 ] 	Batch(300/2353) done. Loss: 0.0326  lr:0.000100
[ Sat May 18 13:45:51 2024 ] 	Batch(400/2353) done. Loss: 0.0737  lr:0.000100
[ Sat May 18 13:46:27 2024 ] 	Batch(500/2353) done. Loss: 0.0341  lr:0.000100
[ Sat May 18 13:47:04 2024 ] 	Batch(600/2353) done. Loss: 0.0080  lr:0.000100
[ Sat May 18 13:47:41 2024 ] 	Batch(700/2353) done. Loss: 0.0214  lr:0.000100
[ Sat May 18 13:48:18 2024 ] 	Batch(800/2353) done. Loss: 0.0142  lr:0.000100
[ Sat May 18 13:48:54 2024 ] 	Batch(900/2353) done. Loss: 0.0528  lr:0.000100
[ Sat May 18 13:49:31 2024 ] 	Batch(1000/2353) done. Loss: 0.0124  lr:0.000100
[ Sat May 18 13:50:07 2024 ] 	Batch(1100/2353) done. Loss: 0.0285  lr:0.000100
[ Sat May 18 13:50:44 2024 ] 	Batch(1200/2353) done. Loss: 0.1466  lr:0.000100
[ Sat May 18 13:51:21 2024 ] 	Batch(1300/2353) done. Loss: 0.0463  lr:0.000100
[ Sat May 18 13:51:57 2024 ] 	Batch(1400/2353) done. Loss: 0.0230  lr:0.000100
[ Sat May 18 13:52:34 2024 ] 	Batch(1500/2353) done. Loss: 0.1661  lr:0.000100
[ Sat May 18 13:53:11 2024 ] 	Batch(1600/2353) done. Loss: 0.0297  lr:0.000100
[ Sat May 18 13:53:47 2024 ] 	Batch(1700/2353) done. Loss: 0.0829  lr:0.000100
[ Sat May 18 13:54:24 2024 ] 	Batch(1800/2353) done. Loss: 0.1538  lr:0.000100
[ Sat May 18 13:55:01 2024 ] 	Batch(1900/2353) done. Loss: 0.0353  lr:0.000100
[ Sat May 18 13:55:37 2024 ] 	Batch(2000/2353) done. Loss: 0.0494  lr:0.000100
[ Sat May 18 13:56:14 2024 ] 	Batch(2100/2353) done. Loss: 0.0864  lr:0.000100
[ Sat May 18 13:56:50 2024 ] 	Batch(2200/2353) done. Loss: 0.0366  lr:0.000100
[ Sat May 18 13:57:27 2024 ] 	Batch(2300/2353) done. Loss: 0.1450  lr:0.000100
[ Sat May 18 13:57:46 2024 ] 	Mean training loss: 0.0616.
[ Sat May 18 13:57:46 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 13:57:46 2024 ] Training epoch: 69
[ Sat May 18 13:57:47 2024 ] 	Batch(0/2353) done. Loss: 0.0405  lr:0.000100
[ Sat May 18 13:58:24 2024 ] 	Batch(100/2353) done. Loss: 0.0065  lr:0.000100
[ Sat May 18 13:59:00 2024 ] 	Batch(200/2353) done. Loss: 0.0429  lr:0.000100
[ Sat May 18 13:59:37 2024 ] 	Batch(300/2353) done. Loss: 0.0508  lr:0.000100
[ Sat May 18 14:00:14 2024 ] 	Batch(400/2353) done. Loss: 0.0065  lr:0.000100
[ Sat May 18 14:00:51 2024 ] 	Batch(500/2353) done. Loss: 0.0669  lr:0.000100
[ Sat May 18 14:01:27 2024 ] 	Batch(600/2353) done. Loss: 0.0605  lr:0.000100
[ Sat May 18 14:02:04 2024 ] 	Batch(700/2353) done. Loss: 0.0195  lr:0.000100
[ Sat May 18 14:02:41 2024 ] 	Batch(800/2353) done. Loss: 0.2316  lr:0.000100
[ Sat May 18 14:03:17 2024 ] 	Batch(900/2353) done. Loss: 0.0312  lr:0.000100
[ Sat May 18 14:03:54 2024 ] 	Batch(1000/2353) done. Loss: 0.0438  lr:0.000100
[ Sat May 18 14:04:31 2024 ] 	Batch(1100/2353) done. Loss: 0.0110  lr:0.000100
[ Sat May 18 14:05:08 2024 ] 	Batch(1200/2353) done. Loss: 0.0402  lr:0.000100
[ Sat May 18 14:05:45 2024 ] 	Batch(1300/2353) done. Loss: 0.0940  lr:0.000100
[ Sat May 18 14:06:21 2024 ] 	Batch(1400/2353) done. Loss: 0.0069  lr:0.000100
[ Sat May 18 14:06:58 2024 ] 	Batch(1500/2353) done. Loss: 0.0043  lr:0.000100
[ Sat May 18 14:07:36 2024 ] 	Batch(1600/2353) done. Loss: 0.0289  lr:0.000100
[ Sat May 18 14:08:13 2024 ] 	Batch(1700/2353) done. Loss: 0.0363  lr:0.000100
[ Sat May 18 14:08:49 2024 ] 	Batch(1800/2353) done. Loss: 0.0360  lr:0.000100
[ Sat May 18 14:09:27 2024 ] 	Batch(1900/2353) done. Loss: 0.0169  lr:0.000100
[ Sat May 18 14:10:04 2024 ] 	Batch(2000/2353) done. Loss: 0.0137  lr:0.000100
[ Sat May 18 14:10:40 2024 ] 	Batch(2100/2353) done. Loss: 0.0597  lr:0.000100
[ Sat May 18 14:11:17 2024 ] 	Batch(2200/2353) done. Loss: 0.1022  lr:0.000100
[ Sat May 18 14:11:54 2024 ] 	Batch(2300/2353) done. Loss: 0.0581  lr:0.000100
[ Sat May 18 14:12:13 2024 ] 	Mean training loss: 0.0555.
[ Sat May 18 14:12:13 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 14:12:13 2024 ] Training epoch: 70
[ Sat May 18 14:12:14 2024 ] 	Batch(0/2353) done. Loss: 0.1652  lr:0.000100
[ Sat May 18 14:12:50 2024 ] 	Batch(100/2353) done. Loss: 0.0060  lr:0.000100
[ Sat May 18 14:13:27 2024 ] 	Batch(200/2353) done. Loss: 0.0528  lr:0.000100
[ Sat May 18 14:14:03 2024 ] 	Batch(300/2353) done. Loss: 0.1112  lr:0.000100
[ Sat May 18 14:14:40 2024 ] 	Batch(400/2353) done. Loss: 0.0408  lr:0.000100
[ Sat May 18 14:15:17 2024 ] 	Batch(500/2353) done. Loss: 0.0305  lr:0.000100
[ Sat May 18 14:15:53 2024 ] 	Batch(600/2353) done. Loss: 0.0248  lr:0.000100
[ Sat May 18 14:16:30 2024 ] 	Batch(700/2353) done. Loss: 0.0599  lr:0.000100
[ Sat May 18 14:17:07 2024 ] 	Batch(800/2353) done. Loss: 0.0568  lr:0.000100
[ Sat May 18 14:17:43 2024 ] 	Batch(900/2353) done. Loss: 0.0836  lr:0.000100
[ Sat May 18 14:18:20 2024 ] 	Batch(1000/2353) done. Loss: 0.1011  lr:0.000100
[ Sat May 18 14:18:57 2024 ] 	Batch(1100/2353) done. Loss: 0.0623  lr:0.000100
[ Sat May 18 14:19:33 2024 ] 	Batch(1200/2353) done. Loss: 0.1719  lr:0.000100
[ Sat May 18 14:20:10 2024 ] 	Batch(1300/2353) done. Loss: 0.0209  lr:0.000100
[ Sat May 18 14:20:47 2024 ] 	Batch(1400/2353) done. Loss: 0.0133  lr:0.000100
[ Sat May 18 14:21:23 2024 ] 	Batch(1500/2353) done. Loss: 0.0114  lr:0.000100
[ Sat May 18 14:22:00 2024 ] 	Batch(1600/2353) done. Loss: 0.0167  lr:0.000100
[ Sat May 18 14:22:37 2024 ] 	Batch(1700/2353) done. Loss: 0.0277  lr:0.000100
[ Sat May 18 14:23:14 2024 ] 	Batch(1800/2353) done. Loss: 0.0118  lr:0.000100
[ Sat May 18 14:23:50 2024 ] 	Batch(1900/2353) done. Loss: 0.0211  lr:0.000100
[ Sat May 18 14:24:27 2024 ] 	Batch(2000/2353) done. Loss: 0.1630  lr:0.000100
[ Sat May 18 14:25:04 2024 ] 	Batch(2100/2353) done. Loss: 0.0294  lr:0.000100
[ Sat May 18 14:25:40 2024 ] 	Batch(2200/2353) done. Loss: 0.0317  lr:0.000100
[ Sat May 18 14:26:17 2024 ] 	Batch(2300/2353) done. Loss: 0.0748  lr:0.000100
[ Sat May 18 14:26:36 2024 ] 	Mean training loss: 0.0610.
[ Sat May 18 14:26:36 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 14:26:36 2024 ] Eval epoch: 70
[ Sat May 18 14:28:39 2024 ] 	Mean val loss of 2367 batches: 0.24364829132567462.
[ Sat May 18 14:28:39 2024 ] Training epoch: 71
[ Sat May 18 14:28:40 2024 ] 	Batch(0/2353) done. Loss: 0.0053  lr:0.000100
[ Sat May 18 14:29:17 2024 ] 	Batch(100/2353) done. Loss: 0.0532  lr:0.000100
[ Sat May 18 14:29:53 2024 ] 	Batch(200/2353) done. Loss: 0.0415  lr:0.000100
[ Sat May 18 14:30:30 2024 ] 	Batch(300/2353) done. Loss: 0.0494  lr:0.000100
[ Sat May 18 14:31:07 2024 ] 	Batch(400/2353) done. Loss: 0.0038  lr:0.000100
[ Sat May 18 14:31:44 2024 ] 	Batch(500/2353) done. Loss: 0.0170  lr:0.000100
[ Sat May 18 14:32:21 2024 ] 	Batch(600/2353) done. Loss: 0.1521  lr:0.000100
[ Sat May 18 14:32:57 2024 ] 	Batch(700/2353) done. Loss: 0.1590  lr:0.000100
[ Sat May 18 14:33:34 2024 ] 	Batch(800/2353) done. Loss: 0.0946  lr:0.000100
[ Sat May 18 14:34:11 2024 ] 	Batch(900/2353) done. Loss: 0.0215  lr:0.000100
[ Sat May 18 14:34:48 2024 ] 	Batch(1000/2353) done. Loss: 0.0101  lr:0.000100
[ Sat May 18 14:35:24 2024 ] 	Batch(1100/2353) done. Loss: 0.0039  lr:0.000100
[ Sat May 18 14:36:01 2024 ] 	Batch(1200/2353) done. Loss: 0.0251  lr:0.000100
[ Sat May 18 14:36:38 2024 ] 	Batch(1300/2353) done. Loss: 0.1152  lr:0.000100
[ Sat May 18 14:37:14 2024 ] 	Batch(1400/2353) done. Loss: 0.1179  lr:0.000100
[ Sat May 18 14:37:51 2024 ] 	Batch(1500/2353) done. Loss: 0.0436  lr:0.000100
[ Sat May 18 14:38:28 2024 ] 	Batch(1600/2353) done. Loss: 0.0370  lr:0.000100
[ Sat May 18 14:39:04 2024 ] 	Batch(1700/2353) done. Loss: 0.0269  lr:0.000100
[ Sat May 18 14:39:41 2024 ] 	Batch(1800/2353) done. Loss: 0.0569  lr:0.000100
[ Sat May 18 14:40:18 2024 ] 	Batch(1900/2353) done. Loss: 0.0034  lr:0.000100
[ Sat May 18 14:40:55 2024 ] 	Batch(2000/2353) done. Loss: 0.0361  lr:0.000100
[ Sat May 18 14:41:31 2024 ] 	Batch(2100/2353) done. Loss: 0.2913  lr:0.000100
[ Sat May 18 14:42:08 2024 ] 	Batch(2200/2353) done. Loss: 0.1316  lr:0.000100
[ Sat May 18 14:42:45 2024 ] 	Batch(2300/2353) done. Loss: 0.0436  lr:0.000100
[ Sat May 18 14:43:04 2024 ] 	Mean training loss: 0.0551.
[ Sat May 18 14:43:04 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 14:43:04 2024 ] Training epoch: 72
[ Sat May 18 14:43:05 2024 ] 	Batch(0/2353) done. Loss: 0.0053  lr:0.000100
[ Sat May 18 14:43:41 2024 ] 	Batch(100/2353) done. Loss: 0.0326  lr:0.000100
[ Sat May 18 14:44:18 2024 ] 	Batch(200/2353) done. Loss: 0.0968  lr:0.000100
[ Sat May 18 14:44:55 2024 ] 	Batch(300/2353) done. Loss: 0.0102  lr:0.000100
[ Sat May 18 14:45:31 2024 ] 	Batch(400/2353) done. Loss: 0.3383  lr:0.000100
[ Sat May 18 14:46:08 2024 ] 	Batch(500/2353) done. Loss: 0.1402  lr:0.000100
[ Sat May 18 14:46:45 2024 ] 	Batch(600/2353) done. Loss: 0.0465  lr:0.000100
[ Sat May 18 14:47:21 2024 ] 	Batch(700/2353) done. Loss: 0.0456  lr:0.000100
[ Sat May 18 14:47:58 2024 ] 	Batch(800/2353) done. Loss: 0.0370  lr:0.000100
[ Sat May 18 14:48:35 2024 ] 	Batch(900/2353) done. Loss: 0.3825  lr:0.000100
[ Sat May 18 14:49:11 2024 ] 	Batch(1000/2353) done. Loss: 0.0219  lr:0.000100
[ Sat May 18 14:49:48 2024 ] 	Batch(1100/2353) done. Loss: 0.0233  lr:0.000100
[ Sat May 18 14:50:25 2024 ] 	Batch(1200/2353) done. Loss: 0.0866  lr:0.000100
[ Sat May 18 14:51:01 2024 ] 	Batch(1300/2353) done. Loss: 0.0049  lr:0.000100
[ Sat May 18 14:51:38 2024 ] 	Batch(1400/2353) done. Loss: 0.0361  lr:0.000100
[ Sat May 18 14:52:15 2024 ] 	Batch(1500/2353) done. Loss: 0.0327  lr:0.000100
[ Sat May 18 14:52:52 2024 ] 	Batch(1600/2353) done. Loss: 0.0744  lr:0.000100
[ Sat May 18 14:53:28 2024 ] 	Batch(1700/2353) done. Loss: 0.0702  lr:0.000100
[ Sat May 18 14:54:05 2024 ] 	Batch(1800/2353) done. Loss: 0.0218  lr:0.000100
[ Sat May 18 14:54:42 2024 ] 	Batch(1900/2353) done. Loss: 0.0375  lr:0.000100
[ Sat May 18 14:55:18 2024 ] 	Batch(2000/2353) done. Loss: 0.0574  lr:0.000100
[ Sat May 18 14:55:55 2024 ] 	Batch(2100/2353) done. Loss: 0.0367  lr:0.000100
[ Sat May 18 14:56:32 2024 ] 	Batch(2200/2353) done. Loss: 0.0064  lr:0.000100
[ Sat May 18 14:57:08 2024 ] 	Batch(2300/2353) done. Loss: 0.0076  lr:0.000100
[ Sat May 18 14:57:28 2024 ] 	Mean training loss: 0.0505.
[ Sat May 18 14:57:28 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 14:57:28 2024 ] Training epoch: 73
[ Sat May 18 14:57:29 2024 ] 	Batch(0/2353) done. Loss: 0.0153  lr:0.000100
[ Sat May 18 14:58:06 2024 ] 	Batch(100/2353) done. Loss: 0.0229  lr:0.000100
[ Sat May 18 14:58:43 2024 ] 	Batch(200/2353) done. Loss: 0.0051  lr:0.000100
[ Sat May 18 14:59:20 2024 ] 	Batch(300/2353) done. Loss: 0.0537  lr:0.000100
[ Sat May 18 14:59:58 2024 ] 	Batch(400/2353) done. Loss: 0.0347  lr:0.000100
[ Sat May 18 15:00:35 2024 ] 	Batch(500/2353) done. Loss: 0.0061  lr:0.000100
[ Sat May 18 15:01:13 2024 ] 	Batch(600/2353) done. Loss: 0.0256  lr:0.000100
[ Sat May 18 15:01:50 2024 ] 	Batch(700/2353) done. Loss: 0.0709  lr:0.000100
[ Sat May 18 15:02:26 2024 ] 	Batch(800/2353) done. Loss: 0.0177  lr:0.000100
[ Sat May 18 15:03:03 2024 ] 	Batch(900/2353) done. Loss: 0.0146  lr:0.000100
[ Sat May 18 15:03:40 2024 ] 	Batch(1000/2353) done. Loss: 0.1205  lr:0.000100
[ Sat May 18 15:04:16 2024 ] 	Batch(1100/2353) done. Loss: 0.0097  lr:0.000100
[ Sat May 18 15:04:53 2024 ] 	Batch(1200/2353) done. Loss: 0.1672  lr:0.000100
[ Sat May 18 15:05:30 2024 ] 	Batch(1300/2353) done. Loss: 0.0304  lr:0.000100
[ Sat May 18 15:06:06 2024 ] 	Batch(1400/2353) done. Loss: 0.0172  lr:0.000100
[ Sat May 18 15:06:43 2024 ] 	Batch(1500/2353) done. Loss: 0.0148  lr:0.000100
[ Sat May 18 15:07:20 2024 ] 	Batch(1600/2353) done. Loss: 0.0220  lr:0.000100
[ Sat May 18 15:07:57 2024 ] 	Batch(1700/2353) done. Loss: 0.0323  lr:0.000100
[ Sat May 18 15:08:34 2024 ] 	Batch(1800/2353) done. Loss: 0.0386  lr:0.000100
[ Sat May 18 15:09:10 2024 ] 	Batch(1900/2353) done. Loss: 0.0721  lr:0.000100
[ Sat May 18 15:09:47 2024 ] 	Batch(2000/2353) done. Loss: 0.0129  lr:0.000100
[ Sat May 18 15:10:24 2024 ] 	Batch(2100/2353) done. Loss: 0.0283  lr:0.000100
[ Sat May 18 15:11:00 2024 ] 	Batch(2200/2353) done. Loss: 0.0124  lr:0.000100
[ Sat May 18 15:11:37 2024 ] 	Batch(2300/2353) done. Loss: 0.0114  lr:0.000100
[ Sat May 18 15:11:57 2024 ] 	Mean training loss: 0.0532.
[ Sat May 18 15:11:57 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 15:11:57 2024 ] Training epoch: 74
[ Sat May 18 15:11:57 2024 ] 	Batch(0/2353) done. Loss: 0.0300  lr:0.000100
[ Sat May 18 15:12:34 2024 ] 	Batch(100/2353) done. Loss: 0.0279  lr:0.000100
[ Sat May 18 15:13:11 2024 ] 	Batch(200/2353) done. Loss: 0.0065  lr:0.000100
[ Sat May 18 15:13:47 2024 ] 	Batch(300/2353) done. Loss: 0.1134  lr:0.000100
[ Sat May 18 15:14:24 2024 ] 	Batch(400/2353) done. Loss: 0.0124  lr:0.000100
[ Sat May 18 15:15:01 2024 ] 	Batch(500/2353) done. Loss: 0.0072  lr:0.000100
[ Sat May 18 15:15:38 2024 ] 	Batch(600/2353) done. Loss: 0.2438  lr:0.000100
[ Sat May 18 15:16:14 2024 ] 	Batch(700/2353) done. Loss: 0.0471  lr:0.000100
[ Sat May 18 15:16:51 2024 ] 	Batch(800/2353) done. Loss: 0.0056  lr:0.000100
[ Sat May 18 15:17:28 2024 ] 	Batch(900/2353) done. Loss: 0.2580  lr:0.000100
[ Sat May 18 15:18:05 2024 ] 	Batch(1000/2353) done. Loss: 0.0786  lr:0.000100
[ Sat May 18 15:18:41 2024 ] 	Batch(1100/2353) done. Loss: 0.0097  lr:0.000100
[ Sat May 18 15:19:18 2024 ] 	Batch(1200/2353) done. Loss: 0.0778  lr:0.000100
[ Sat May 18 15:19:55 2024 ] 	Batch(1300/2353) done. Loss: 0.0316  lr:0.000100
[ Sat May 18 15:20:32 2024 ] 	Batch(1400/2353) done. Loss: 0.0063  lr:0.000100
[ Sat May 18 15:21:09 2024 ] 	Batch(1500/2353) done. Loss: 0.0059  lr:0.000100
[ Sat May 18 15:21:45 2024 ] 	Batch(1600/2353) done. Loss: 0.0742  lr:0.000100
[ Sat May 18 15:22:22 2024 ] 	Batch(1700/2353) done. Loss: 0.0118  lr:0.000100
[ Sat May 18 15:22:59 2024 ] 	Batch(1800/2353) done. Loss: 0.0214  lr:0.000100
[ Sat May 18 15:23:36 2024 ] 	Batch(1900/2353) done. Loss: 0.0174  lr:0.000100
[ Sat May 18 15:24:13 2024 ] 	Batch(2000/2353) done. Loss: 0.0058  lr:0.000100
[ Sat May 18 15:24:49 2024 ] 	Batch(2100/2353) done. Loss: 0.0181  lr:0.000100
[ Sat May 18 15:25:26 2024 ] 	Batch(2200/2353) done. Loss: 0.1463  lr:0.000100
[ Sat May 18 15:26:03 2024 ] 	Batch(2300/2353) done. Loss: 0.0339  lr:0.000100
[ Sat May 18 15:26:23 2024 ] 	Mean training loss: 0.0532.
[ Sat May 18 15:26:23 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 15:26:23 2024 ] Training epoch: 75
[ Sat May 18 15:26:23 2024 ] 	Batch(0/2353) done. Loss: 0.0119  lr:0.000100
[ Sat May 18 15:27:00 2024 ] 	Batch(100/2353) done. Loss: 0.0286  lr:0.000100
[ Sat May 18 15:27:37 2024 ] 	Batch(200/2353) done. Loss: 0.1629  lr:0.000100
[ Sat May 18 15:28:14 2024 ] 	Batch(300/2353) done. Loss: 0.0660  lr:0.000100
[ Sat May 18 15:28:51 2024 ] 	Batch(400/2353) done. Loss: 0.1418  lr:0.000100
[ Sat May 18 15:29:27 2024 ] 	Batch(500/2353) done. Loss: 0.0129  lr:0.000100
[ Sat May 18 15:30:04 2024 ] 	Batch(600/2353) done. Loss: 0.0415  lr:0.000100
[ Sat May 18 15:30:41 2024 ] 	Batch(700/2353) done. Loss: 0.1247  lr:0.000100
[ Sat May 18 15:31:19 2024 ] 	Batch(800/2353) done. Loss: 0.0338  lr:0.000100
[ Sat May 18 15:31:55 2024 ] 	Batch(900/2353) done. Loss: 0.0060  lr:0.000100
[ Sat May 18 15:32:32 2024 ] 	Batch(1000/2353) done. Loss: 0.0078  lr:0.000100
[ Sat May 18 15:33:09 2024 ] 	Batch(1100/2353) done. Loss: 0.0206  lr:0.000100
[ Sat May 18 15:33:46 2024 ] 	Batch(1200/2353) done. Loss: 0.0496  lr:0.000100
[ Sat May 18 15:34:23 2024 ] 	Batch(1300/2353) done. Loss: 0.0134  lr:0.000100
[ Sat May 18 15:34:59 2024 ] 	Batch(1400/2353) done. Loss: 0.0663  lr:0.000100
[ Sat May 18 15:35:36 2024 ] 	Batch(1500/2353) done. Loss: 0.0477  lr:0.000100
[ Sat May 18 15:36:13 2024 ] 	Batch(1600/2353) done. Loss: 0.0058  lr:0.000100
[ Sat May 18 15:36:50 2024 ] 	Batch(1700/2353) done. Loss: 0.0903  lr:0.000100
[ Sat May 18 15:37:27 2024 ] 	Batch(1800/2353) done. Loss: 0.2433  lr:0.000100
[ Sat May 18 15:38:03 2024 ] 	Batch(1900/2353) done. Loss: 0.0125  lr:0.000100
[ Sat May 18 15:38:40 2024 ] 	Batch(2000/2353) done. Loss: 0.0534  lr:0.000100
[ Sat May 18 15:39:17 2024 ] 	Batch(2100/2353) done. Loss: 0.0101  lr:0.000100
[ Sat May 18 15:39:54 2024 ] 	Batch(2200/2353) done. Loss: 0.0256  lr:0.000100
[ Sat May 18 15:40:30 2024 ] 	Batch(2300/2353) done. Loss: 0.0300  lr:0.000100
[ Sat May 18 15:40:50 2024 ] 	Mean training loss: 0.0490.
[ Sat May 18 15:40:50 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 15:40:50 2024 ] Training epoch: 76
[ Sat May 18 15:40:51 2024 ] 	Batch(0/2353) done. Loss: 0.0810  lr:0.000100
[ Sat May 18 15:41:28 2024 ] 	Batch(100/2353) done. Loss: 0.0326  lr:0.000100
[ Sat May 18 15:42:04 2024 ] 	Batch(200/2353) done. Loss: 0.0065  lr:0.000100
[ Sat May 18 15:42:41 2024 ] 	Batch(300/2353) done. Loss: 0.0290  lr:0.000100
[ Sat May 18 15:43:18 2024 ] 	Batch(400/2353) done. Loss: 0.0067  lr:0.000100
[ Sat May 18 15:43:55 2024 ] 	Batch(500/2353) done. Loss: 0.0512  lr:0.000100
[ Sat May 18 15:44:32 2024 ] 	Batch(600/2353) done. Loss: 0.0156  lr:0.000100
[ Sat May 18 15:45:09 2024 ] 	Batch(700/2353) done. Loss: 0.0484  lr:0.000100
[ Sat May 18 15:45:45 2024 ] 	Batch(800/2353) done. Loss: 0.0049  lr:0.000100
[ Sat May 18 15:46:22 2024 ] 	Batch(900/2353) done. Loss: 0.0253  lr:0.000100
[ Sat May 18 15:46:59 2024 ] 	Batch(1000/2353) done. Loss: 0.0038  lr:0.000100
[ Sat May 18 15:47:36 2024 ] 	Batch(1100/2353) done. Loss: 0.0345  lr:0.000100
[ Sat May 18 15:48:14 2024 ] 	Batch(1200/2353) done. Loss: 0.0108  lr:0.000100
[ Sat May 18 15:48:51 2024 ] 	Batch(1300/2353) done. Loss: 0.0256  lr:0.000100
[ Sat May 18 15:49:28 2024 ] 	Batch(1400/2353) done. Loss: 0.0024  lr:0.000100
[ Sat May 18 15:50:04 2024 ] 	Batch(1500/2353) done. Loss: 0.1791  lr:0.000100
[ Sat May 18 15:50:41 2024 ] 	Batch(1600/2353) done. Loss: 0.0186  lr:0.000100
[ Sat May 18 15:51:18 2024 ] 	Batch(1700/2353) done. Loss: 0.0158  lr:0.000100
[ Sat May 18 15:51:55 2024 ] 	Batch(1800/2353) done. Loss: 0.0083  lr:0.000100
[ Sat May 18 15:52:32 2024 ] 	Batch(1900/2353) done. Loss: 0.0066  lr:0.000100
[ Sat May 18 15:53:09 2024 ] 	Batch(2000/2353) done. Loss: 0.0373  lr:0.000100
[ Sat May 18 15:53:45 2024 ] 	Batch(2100/2353) done. Loss: 0.0242  lr:0.000100
[ Sat May 18 15:54:22 2024 ] 	Batch(2200/2353) done. Loss: 0.0936  lr:0.000100
[ Sat May 18 15:54:59 2024 ] 	Batch(2300/2353) done. Loss: 0.0172  lr:0.000100
[ Sat May 18 15:55:18 2024 ] 	Mean training loss: 0.0513.
[ Sat May 18 15:55:18 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 15:55:19 2024 ] Training epoch: 77
[ Sat May 18 15:55:19 2024 ] 	Batch(0/2353) done. Loss: 0.0274  lr:0.000100
[ Sat May 18 15:55:56 2024 ] 	Batch(100/2353) done. Loss: 0.0428  lr:0.000100
[ Sat May 18 15:56:33 2024 ] 	Batch(200/2353) done. Loss: 0.0028  lr:0.000100
[ Sat May 18 15:57:09 2024 ] 	Batch(300/2353) done. Loss: 0.0160  lr:0.000100
[ Sat May 18 15:57:46 2024 ] 	Batch(400/2353) done. Loss: 0.0127  lr:0.000100
[ Sat May 18 15:58:23 2024 ] 	Batch(500/2353) done. Loss: 0.0509  lr:0.000100
[ Sat May 18 15:59:00 2024 ] 	Batch(600/2353) done. Loss: 0.0080  lr:0.000100
[ Sat May 18 15:59:36 2024 ] 	Batch(700/2353) done. Loss: 0.0136  lr:0.000100
[ Sat May 18 16:00:13 2024 ] 	Batch(800/2353) done. Loss: 0.0157  lr:0.000100
[ Sat May 18 16:00:50 2024 ] 	Batch(900/2353) done. Loss: 0.0366  lr:0.000100
[ Sat May 18 16:01:27 2024 ] 	Batch(1000/2353) done. Loss: 0.0125  lr:0.000100
[ Sat May 18 16:02:03 2024 ] 	Batch(1100/2353) done. Loss: 0.0485  lr:0.000100
[ Sat May 18 16:02:40 2024 ] 	Batch(1200/2353) done. Loss: 0.0348  lr:0.000100
[ Sat May 18 16:03:17 2024 ] 	Batch(1300/2353) done. Loss: 0.1235  lr:0.000100
[ Sat May 18 16:03:54 2024 ] 	Batch(1400/2353) done. Loss: 0.0169  lr:0.000100
[ Sat May 18 16:04:30 2024 ] 	Batch(1500/2353) done. Loss: 0.0127  lr:0.000100
[ Sat May 18 16:05:07 2024 ] 	Batch(1600/2353) done. Loss: 0.0081  lr:0.000100
[ Sat May 18 16:05:44 2024 ] 	Batch(1700/2353) done. Loss: 0.0104  lr:0.000100
[ Sat May 18 16:06:21 2024 ] 	Batch(1800/2353) done. Loss: 0.0040  lr:0.000100
[ Sat May 18 16:06:57 2024 ] 	Batch(1900/2353) done. Loss: 0.0508  lr:0.000100
[ Sat May 18 16:07:34 2024 ] 	Batch(2000/2353) done. Loss: 0.0166  lr:0.000100
[ Sat May 18 16:08:11 2024 ] 	Batch(2100/2353) done. Loss: 0.0335  lr:0.000100
[ Sat May 18 16:08:48 2024 ] 	Batch(2200/2353) done. Loss: 0.0016  lr:0.000100
[ Sat May 18 16:09:25 2024 ] 	Batch(2300/2353) done. Loss: 0.0172  lr:0.000100
[ Sat May 18 16:09:44 2024 ] 	Mean training loss: 0.0495.
[ Sat May 18 16:09:44 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 16:09:44 2024 ] Training epoch: 78
[ Sat May 18 16:09:45 2024 ] 	Batch(0/2353) done. Loss: 0.1099  lr:0.000100
[ Sat May 18 16:10:21 2024 ] 	Batch(100/2353) done. Loss: 0.0225  lr:0.000100
[ Sat May 18 16:10:58 2024 ] 	Batch(200/2353) done. Loss: 0.0048  lr:0.000100
[ Sat May 18 16:11:35 2024 ] 	Batch(300/2353) done. Loss: 0.0296  lr:0.000100
[ Sat May 18 16:12:11 2024 ] 	Batch(400/2353) done. Loss: 0.1447  lr:0.000100
[ Sat May 18 16:12:48 2024 ] 	Batch(500/2353) done. Loss: 0.0046  lr:0.000100
[ Sat May 18 16:13:25 2024 ] 	Batch(600/2353) done. Loss: 0.0313  lr:0.000100
[ Sat May 18 16:14:01 2024 ] 	Batch(700/2353) done. Loss: 0.0325  lr:0.000100
[ Sat May 18 16:14:38 2024 ] 	Batch(800/2353) done. Loss: 0.0554  lr:0.000100
[ Sat May 18 16:15:15 2024 ] 	Batch(900/2353) done. Loss: 0.0390  lr:0.000100
[ Sat May 18 16:15:51 2024 ] 	Batch(1000/2353) done. Loss: 0.0245  lr:0.000100
[ Sat May 18 16:16:28 2024 ] 	Batch(1100/2353) done. Loss: 0.1395  lr:0.000100
[ Sat May 18 16:17:05 2024 ] 	Batch(1200/2353) done. Loss: 0.2815  lr:0.000100
[ Sat May 18 16:17:42 2024 ] 	Batch(1300/2353) done. Loss: 0.0077  lr:0.000100
[ Sat May 18 16:18:18 2024 ] 	Batch(1400/2353) done. Loss: 0.1679  lr:0.000100
[ Sat May 18 16:18:55 2024 ] 	Batch(1500/2353) done. Loss: 0.1363  lr:0.000100
[ Sat May 18 16:19:32 2024 ] 	Batch(1600/2353) done. Loss: 0.0125  lr:0.000100
[ Sat May 18 16:20:08 2024 ] 	Batch(1700/2353) done. Loss: 0.1186  lr:0.000100
[ Sat May 18 16:20:45 2024 ] 	Batch(1800/2353) done. Loss: 0.0333  lr:0.000100
[ Sat May 18 16:21:22 2024 ] 	Batch(1900/2353) done. Loss: 0.0961  lr:0.000100
[ Sat May 18 16:21:58 2024 ] 	Batch(2000/2353) done. Loss: 0.0182  lr:0.000100
[ Sat May 18 16:22:35 2024 ] 	Batch(2100/2353) done. Loss: 0.0271  lr:0.000100
[ Sat May 18 16:23:12 2024 ] 	Batch(2200/2353) done. Loss: 0.0254  lr:0.000100
[ Sat May 18 16:23:48 2024 ] 	Batch(2300/2353) done. Loss: 0.0353  lr:0.000100
[ Sat May 18 16:24:08 2024 ] 	Mean training loss: 0.0509.
[ Sat May 18 16:24:08 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 16:24:08 2024 ] Training epoch: 79
[ Sat May 18 16:24:09 2024 ] 	Batch(0/2353) done. Loss: 0.0065  lr:0.000100
[ Sat May 18 16:24:45 2024 ] 	Batch(100/2353) done. Loss: 0.0111  lr:0.000100
[ Sat May 18 16:25:22 2024 ] 	Batch(200/2353) done. Loss: 0.0077  lr:0.000100
[ Sat May 18 16:25:59 2024 ] 	Batch(300/2353) done. Loss: 0.0236  lr:0.000100
[ Sat May 18 16:26:35 2024 ] 	Batch(400/2353) done. Loss: 0.0056  lr:0.000100
[ Sat May 18 16:27:12 2024 ] 	Batch(500/2353) done. Loss: 0.0224  lr:0.000100
[ Sat May 18 16:27:49 2024 ] 	Batch(600/2353) done. Loss: 0.0177  lr:0.000100
[ Sat May 18 16:28:25 2024 ] 	Batch(700/2353) done. Loss: 0.0658  lr:0.000100
[ Sat May 18 16:29:02 2024 ] 	Batch(800/2353) done. Loss: 0.0042  lr:0.000100
[ Sat May 18 16:29:39 2024 ] 	Batch(900/2353) done. Loss: 0.0011  lr:0.000100
[ Sat May 18 16:30:16 2024 ] 	Batch(1000/2353) done. Loss: 0.0494  lr:0.000100
[ Sat May 18 16:30:52 2024 ] 	Batch(1100/2353) done. Loss: 0.0513  lr:0.000100
[ Sat May 18 16:31:29 2024 ] 	Batch(1200/2353) done. Loss: 0.0639  lr:0.000100
[ Sat May 18 16:32:07 2024 ] 	Batch(1300/2353) done. Loss: 0.0327  lr:0.000100
[ Sat May 18 16:32:44 2024 ] 	Batch(1400/2353) done. Loss: 0.1079  lr:0.000100
[ Sat May 18 16:33:21 2024 ] 	Batch(1500/2353) done. Loss: 0.0061  lr:0.000100
[ Sat May 18 16:33:59 2024 ] 	Batch(1600/2353) done. Loss: 0.1686  lr:0.000100
[ Sat May 18 16:34:35 2024 ] 	Batch(1700/2353) done. Loss: 0.0600  lr:0.000100
[ Sat May 18 16:35:12 2024 ] 	Batch(1800/2353) done. Loss: 0.0243  lr:0.000100
[ Sat May 18 16:35:49 2024 ] 	Batch(1900/2353) done. Loss: 0.0222  lr:0.000100
[ Sat May 18 16:36:25 2024 ] 	Batch(2000/2353) done. Loss: 0.0019  lr:0.000100
[ Sat May 18 16:37:02 2024 ] 	Batch(2100/2353) done. Loss: 0.0218  lr:0.000100
[ Sat May 18 16:37:39 2024 ] 	Batch(2200/2353) done. Loss: 0.0369  lr:0.000100
[ Sat May 18 16:38:15 2024 ] 	Batch(2300/2353) done. Loss: 0.0360  lr:0.000100
[ Sat May 18 16:38:35 2024 ] 	Mean training loss: 0.0504.
[ Sat May 18 16:38:35 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 16:38:35 2024 ] Training epoch: 80
[ Sat May 18 16:38:35 2024 ] 	Batch(0/2353) done. Loss: 0.0076  lr:0.000100
[ Sat May 18 16:39:12 2024 ] 	Batch(100/2353) done. Loss: 0.0486  lr:0.000100
[ Sat May 18 16:39:49 2024 ] 	Batch(200/2353) done. Loss: 0.0381  lr:0.000100
[ Sat May 18 16:40:25 2024 ] 	Batch(300/2353) done. Loss: 0.0154  lr:0.000100
[ Sat May 18 16:41:03 2024 ] 	Batch(400/2353) done. Loss: 0.0865  lr:0.000100
[ Sat May 18 16:41:40 2024 ] 	Batch(500/2353) done. Loss: 0.0080  lr:0.000100
[ Sat May 18 16:42:17 2024 ] 	Batch(600/2353) done. Loss: 0.0062  lr:0.000100
[ Sat May 18 16:42:55 2024 ] 	Batch(700/2353) done. Loss: 0.0509  lr:0.000100
[ Sat May 18 16:43:32 2024 ] 	Batch(800/2353) done. Loss: 0.0075  lr:0.000100
[ Sat May 18 16:44:09 2024 ] 	Batch(900/2353) done. Loss: 0.0340  lr:0.000100
[ Sat May 18 16:44:45 2024 ] 	Batch(1000/2353) done. Loss: 0.0208  lr:0.000100
[ Sat May 18 16:45:22 2024 ] 	Batch(1100/2353) done. Loss: 0.0143  lr:0.000100
[ Sat May 18 16:45:59 2024 ] 	Batch(1200/2353) done. Loss: 0.0039  lr:0.000100
[ Sat May 18 16:46:35 2024 ] 	Batch(1300/2353) done. Loss: 0.0832  lr:0.000100
[ Sat May 18 16:47:12 2024 ] 	Batch(1400/2353) done. Loss: 0.0329  lr:0.000100
[ Sat May 18 16:47:49 2024 ] 	Batch(1500/2353) done. Loss: 0.0439  lr:0.000100
[ Sat May 18 16:48:25 2024 ] 	Batch(1600/2353) done. Loss: 0.0155  lr:0.000100
[ Sat May 18 16:49:02 2024 ] 	Batch(1700/2353) done. Loss: 0.0019  lr:0.000100
[ Sat May 18 16:49:39 2024 ] 	Batch(1800/2353) done. Loss: 0.0272  lr:0.000100
[ Sat May 18 16:50:15 2024 ] 	Batch(1900/2353) done. Loss: 0.0484  lr:0.000100
[ Sat May 18 16:50:52 2024 ] 	Batch(2000/2353) done. Loss: 0.0208  lr:0.000100
[ Sat May 18 16:51:29 2024 ] 	Batch(2100/2353) done. Loss: 0.0581  lr:0.000100
[ Sat May 18 16:52:05 2024 ] 	Batch(2200/2353) done. Loss: 0.0405  lr:0.000100
[ Sat May 18 16:52:42 2024 ] 	Batch(2300/2353) done. Loss: 0.1153  lr:0.000100
[ Sat May 18 16:53:01 2024 ] 	Mean training loss: 0.0480.
[ Sat May 18 16:53:01 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 16:53:02 2024 ] Eval epoch: 80
[ Sat May 18 16:55:06 2024 ] 	Mean val loss of 2367 batches: 0.23926240863583853.
[ Sat May 18 16:55:06 2024 ] Training epoch: 81
[ Sat May 18 16:55:06 2024 ] 	Batch(0/2353) done. Loss: 0.0031  lr:0.000100
[ Sat May 18 16:55:43 2024 ] 	Batch(100/2353) done. Loss: 0.0904  lr:0.000100
[ Sat May 18 16:56:19 2024 ] 	Batch(200/2353) done. Loss: 0.0581  lr:0.000100
[ Sat May 18 16:56:56 2024 ] 	Batch(300/2353) done. Loss: 0.0308  lr:0.000100
[ Sat May 18 16:57:33 2024 ] 	Batch(400/2353) done. Loss: 0.0033  lr:0.000100
[ Sat May 18 16:58:09 2024 ] 	Batch(500/2353) done. Loss: 0.0321  lr:0.000100
[ Sat May 18 16:58:46 2024 ] 	Batch(600/2353) done. Loss: 0.1609  lr:0.000100
[ Sat May 18 16:59:22 2024 ] 	Batch(700/2353) done. Loss: 0.0254  lr:0.000100
[ Sat May 18 16:59:59 2024 ] 	Batch(800/2353) done. Loss: 0.1292  lr:0.000100
[ Sat May 18 17:00:36 2024 ] 	Batch(900/2353) done. Loss: 0.0459  lr:0.000100
[ Sat May 18 17:01:12 2024 ] 	Batch(1000/2353) done. Loss: 0.0102  lr:0.000100
[ Sat May 18 17:01:49 2024 ] 	Batch(1100/2353) done. Loss: 0.0077  lr:0.000100
[ Sat May 18 17:02:26 2024 ] 	Batch(1200/2353) done. Loss: 0.0121  lr:0.000100
[ Sat May 18 17:03:03 2024 ] 	Batch(1300/2353) done. Loss: 0.0321  lr:0.000100
[ Sat May 18 17:03:40 2024 ] 	Batch(1400/2353) done. Loss: 0.0709  lr:0.000100
[ Sat May 18 17:04:17 2024 ] 	Batch(1500/2353) done. Loss: 0.0109  lr:0.000100
[ Sat May 18 17:04:54 2024 ] 	Batch(1600/2353) done. Loss: 0.1638  lr:0.000100
[ Sat May 18 17:05:31 2024 ] 	Batch(1700/2353) done. Loss: 0.0099  lr:0.000100
[ Sat May 18 17:06:08 2024 ] 	Batch(1800/2353) done. Loss: 0.0333  lr:0.000100
[ Sat May 18 17:06:44 2024 ] 	Batch(1900/2353) done. Loss: 0.0053  lr:0.000100
[ Sat May 18 17:07:21 2024 ] 	Batch(2000/2353) done. Loss: 0.0759  lr:0.000100
[ Sat May 18 17:07:58 2024 ] 	Batch(2100/2353) done. Loss: 0.1550  lr:0.000100
[ Sat May 18 17:08:35 2024 ] 	Batch(2200/2353) done. Loss: 0.0187  lr:0.000100
[ Sat May 18 17:09:11 2024 ] 	Batch(2300/2353) done. Loss: 0.0445  lr:0.000100
[ Sat May 18 17:09:31 2024 ] 	Mean training loss: 0.0478.
[ Sat May 18 17:09:31 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 17:09:31 2024 ] Training epoch: 82
[ Sat May 18 17:09:32 2024 ] 	Batch(0/2353) done. Loss: 0.0778  lr:0.000100
[ Sat May 18 17:10:09 2024 ] 	Batch(100/2353) done. Loss: 0.0328  lr:0.000100
[ Sat May 18 17:10:45 2024 ] 	Batch(200/2353) done. Loss: 0.0159  lr:0.000100
[ Sat May 18 17:11:22 2024 ] 	Batch(300/2353) done. Loss: 0.0474  lr:0.000100
[ Sat May 18 17:11:59 2024 ] 	Batch(400/2353) done. Loss: 0.0058  lr:0.000100
[ Sat May 18 17:12:35 2024 ] 	Batch(500/2353) done. Loss: 0.0533  lr:0.000100
[ Sat May 18 17:13:12 2024 ] 	Batch(600/2353) done. Loss: 0.0281  lr:0.000100
[ Sat May 18 17:13:48 2024 ] 	Batch(700/2353) done. Loss: 0.0339  lr:0.000100
[ Sat May 18 17:14:25 2024 ] 	Batch(800/2353) done. Loss: 0.0045  lr:0.000100
[ Sat May 18 17:15:02 2024 ] 	Batch(900/2353) done. Loss: 0.4255  lr:0.000100
[ Sat May 18 17:15:38 2024 ] 	Batch(1000/2353) done. Loss: 0.0493  lr:0.000100
[ Sat May 18 17:16:15 2024 ] 	Batch(1100/2353) done. Loss: 0.0609  lr:0.000100
[ Sat May 18 17:16:51 2024 ] 	Batch(1200/2353) done. Loss: 0.0048  lr:0.000100
[ Sat May 18 17:17:28 2024 ] 	Batch(1300/2353) done. Loss: 0.0433  lr:0.000100
[ Sat May 18 17:18:04 2024 ] 	Batch(1400/2353) done. Loss: 0.0755  lr:0.000100
[ Sat May 18 17:18:41 2024 ] 	Batch(1500/2353) done. Loss: 0.0398  lr:0.000100
[ Sat May 18 17:19:18 2024 ] 	Batch(1600/2353) done. Loss: 0.0316  lr:0.000100
[ Sat May 18 17:19:54 2024 ] 	Batch(1700/2353) done. Loss: 0.0169  lr:0.000100
[ Sat May 18 17:20:31 2024 ] 	Batch(1800/2353) done. Loss: 0.1205  lr:0.000100
[ Sat May 18 17:21:07 2024 ] 	Batch(1900/2353) done. Loss: 0.0364  lr:0.000100
[ Sat May 18 17:21:44 2024 ] 	Batch(2000/2353) done. Loss: 0.0299  lr:0.000100
[ Sat May 18 17:22:21 2024 ] 	Batch(2100/2353) done. Loss: 0.0014  lr:0.000100
[ Sat May 18 17:22:59 2024 ] 	Batch(2200/2353) done. Loss: 0.0185  lr:0.000100
[ Sat May 18 17:23:36 2024 ] 	Batch(2300/2353) done. Loss: 0.0153  lr:0.000100
[ Sat May 18 17:23:56 2024 ] 	Mean training loss: 0.0469.
[ Sat May 18 17:23:56 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 17:23:56 2024 ] Training epoch: 83
[ Sat May 18 17:23:57 2024 ] 	Batch(0/2353) done. Loss: 0.0248  lr:0.000100
[ Sat May 18 17:24:33 2024 ] 	Batch(100/2353) done. Loss: 0.0576  lr:0.000100
[ Sat May 18 17:25:10 2024 ] 	Batch(200/2353) done. Loss: 0.0070  lr:0.000100
[ Sat May 18 17:25:46 2024 ] 	Batch(300/2353) done. Loss: 0.0082  lr:0.000100
[ Sat May 18 17:26:23 2024 ] 	Batch(400/2353) done. Loss: 0.0466  lr:0.000100
[ Sat May 18 17:27:00 2024 ] 	Batch(500/2353) done. Loss: 0.0394  lr:0.000100
[ Sat May 18 17:27:36 2024 ] 	Batch(600/2353) done. Loss: 0.0032  lr:0.000100
[ Sat May 18 17:28:13 2024 ] 	Batch(700/2353) done. Loss: 0.0199  lr:0.000100
[ Sat May 18 17:28:49 2024 ] 	Batch(800/2353) done. Loss: 0.0065  lr:0.000100
[ Sat May 18 17:29:26 2024 ] 	Batch(900/2353) done. Loss: 0.1052  lr:0.000100
[ Sat May 18 17:30:03 2024 ] 	Batch(1000/2353) done. Loss: 0.0197  lr:0.000100
[ Sat May 18 17:30:39 2024 ] 	Batch(1100/2353) done. Loss: 0.0903  lr:0.000100
[ Sat May 18 17:31:16 2024 ] 	Batch(1200/2353) done. Loss: 0.0056  lr:0.000100
[ Sat May 18 17:31:53 2024 ] 	Batch(1300/2353) done. Loss: 0.0118  lr:0.000100
[ Sat May 18 17:32:29 2024 ] 	Batch(1400/2353) done. Loss: 0.0378  lr:0.000100
[ Sat May 18 17:33:06 2024 ] 	Batch(1500/2353) done. Loss: 0.0173  lr:0.000100
[ Sat May 18 17:33:43 2024 ] 	Batch(1600/2353) done. Loss: 0.0185  lr:0.000100
[ Sat May 18 17:34:20 2024 ] 	Batch(1700/2353) done. Loss: 0.0680  lr:0.000100
[ Sat May 18 17:34:57 2024 ] 	Batch(1800/2353) done. Loss: 0.0287  lr:0.000100
[ Sat May 18 17:35:34 2024 ] 	Batch(1900/2353) done. Loss: 0.0033  lr:0.000100
[ Sat May 18 17:36:10 2024 ] 	Batch(2000/2353) done. Loss: 0.0308  lr:0.000100
[ Sat May 18 17:36:47 2024 ] 	Batch(2100/2353) done. Loss: 0.0179  lr:0.000100
[ Sat May 18 17:37:24 2024 ] 	Batch(2200/2353) done. Loss: 0.0036  lr:0.000100
[ Sat May 18 17:38:00 2024 ] 	Batch(2300/2353) done. Loss: 0.0208  lr:0.000100
[ Sat May 18 17:38:20 2024 ] 	Mean training loss: 0.0452.
[ Sat May 18 17:38:20 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 17:38:20 2024 ] Training epoch: 84
[ Sat May 18 17:38:21 2024 ] 	Batch(0/2353) done. Loss: 0.0087  lr:0.000100
[ Sat May 18 17:38:57 2024 ] 	Batch(100/2353) done. Loss: 0.0212  lr:0.000100
[ Sat May 18 17:39:34 2024 ] 	Batch(200/2353) done. Loss: 0.0144  lr:0.000100
[ Sat May 18 17:40:10 2024 ] 	Batch(300/2353) done. Loss: 0.0169  lr:0.000100
[ Sat May 18 17:40:47 2024 ] 	Batch(400/2353) done. Loss: 0.0171  lr:0.000100
[ Sat May 18 17:41:23 2024 ] 	Batch(500/2353) done. Loss: 0.0211  lr:0.000100
[ Sat May 18 17:42:00 2024 ] 	Batch(600/2353) done. Loss: 0.0402  lr:0.000100
[ Sat May 18 17:42:37 2024 ] 	Batch(700/2353) done. Loss: 0.0123  lr:0.000100
[ Sat May 18 17:43:13 2024 ] 	Batch(800/2353) done. Loss: 0.0176  lr:0.000100
[ Sat May 18 17:43:50 2024 ] 	Batch(900/2353) done. Loss: 0.0636  lr:0.000100
[ Sat May 18 17:44:26 2024 ] 	Batch(1000/2353) done. Loss: 0.0318  lr:0.000100
[ Sat May 18 17:45:03 2024 ] 	Batch(1100/2353) done. Loss: 0.0372  lr:0.000100
[ Sat May 18 17:45:39 2024 ] 	Batch(1200/2353) done. Loss: 0.0134  lr:0.000100
[ Sat May 18 17:46:16 2024 ] 	Batch(1300/2353) done. Loss: 0.0151  lr:0.000100
[ Sat May 18 17:46:52 2024 ] 	Batch(1400/2353) done. Loss: 0.0060  lr:0.000100
[ Sat May 18 17:47:29 2024 ] 	Batch(1500/2353) done. Loss: 0.0154  lr:0.000100
[ Sat May 18 17:48:07 2024 ] 	Batch(1600/2353) done. Loss: 0.0744  lr:0.000100
[ Sat May 18 17:48:44 2024 ] 	Batch(1700/2353) done. Loss: 0.1859  lr:0.000100
[ Sat May 18 17:49:21 2024 ] 	Batch(1800/2353) done. Loss: 0.0085  lr:0.000100
[ Sat May 18 17:49:58 2024 ] 	Batch(1900/2353) done. Loss: 0.0755  lr:0.000100
[ Sat May 18 17:50:35 2024 ] 	Batch(2000/2353) done. Loss: 0.0011  lr:0.000100
[ Sat May 18 17:51:12 2024 ] 	Batch(2100/2353) done. Loss: 0.0324  lr:0.000100
[ Sat May 18 17:51:48 2024 ] 	Batch(2200/2353) done. Loss: 0.0585  lr:0.000100
[ Sat May 18 17:52:25 2024 ] 	Batch(2300/2353) done. Loss: 0.0136  lr:0.000100
[ Sat May 18 17:52:45 2024 ] 	Mean training loss: 0.0443.
[ Sat May 18 17:52:45 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 17:52:45 2024 ] Training epoch: 85
[ Sat May 18 17:52:46 2024 ] 	Batch(0/2353) done. Loss: 0.0145  lr:0.000100
[ Sat May 18 17:53:22 2024 ] 	Batch(100/2353) done. Loss: 0.0395  lr:0.000100
[ Sat May 18 17:53:59 2024 ] 	Batch(200/2353) done. Loss: 0.0032  lr:0.000100
[ Sat May 18 17:54:35 2024 ] 	Batch(300/2353) done. Loss: 0.0763  lr:0.000100
[ Sat May 18 17:55:12 2024 ] 	Batch(400/2353) done. Loss: 0.0480  lr:0.000100
[ Sat May 18 17:55:49 2024 ] 	Batch(500/2353) done. Loss: 0.0225  lr:0.000100
[ Sat May 18 17:56:25 2024 ] 	Batch(600/2353) done. Loss: 0.0257  lr:0.000100
[ Sat May 18 17:57:02 2024 ] 	Batch(700/2353) done. Loss: 0.0166  lr:0.000100
[ Sat May 18 17:57:39 2024 ] 	Batch(800/2353) done. Loss: 0.0093  lr:0.000100
[ Sat May 18 17:58:15 2024 ] 	Batch(900/2353) done. Loss: 0.2150  lr:0.000100
[ Sat May 18 17:58:52 2024 ] 	Batch(1000/2353) done. Loss: 0.0093  lr:0.000100
[ Sat May 18 17:59:29 2024 ] 	Batch(1100/2353) done. Loss: 0.0442  lr:0.000100
[ Sat May 18 18:00:05 2024 ] 	Batch(1200/2353) done. Loss: 0.0537  lr:0.000100
[ Sat May 18 18:00:42 2024 ] 	Batch(1300/2353) done. Loss: 0.0866  lr:0.000100
[ Sat May 18 18:01:18 2024 ] 	Batch(1400/2353) done. Loss: 0.0384  lr:0.000100
[ Sat May 18 18:01:55 2024 ] 	Batch(1500/2353) done. Loss: 0.0025  lr:0.000100
[ Sat May 18 18:02:32 2024 ] 	Batch(1600/2353) done. Loss: 0.0511  lr:0.000100
[ Sat May 18 18:03:08 2024 ] 	Batch(1700/2353) done. Loss: 0.0310  lr:0.000100
[ Sat May 18 18:03:45 2024 ] 	Batch(1800/2353) done. Loss: 0.0572  lr:0.000100
[ Sat May 18 18:04:21 2024 ] 	Batch(1900/2353) done. Loss: 0.0594  lr:0.000100
[ Sat May 18 18:04:58 2024 ] 	Batch(2000/2353) done. Loss: 0.0389  lr:0.000100
[ Sat May 18 18:05:35 2024 ] 	Batch(2100/2353) done. Loss: 0.0165  lr:0.000100
[ Sat May 18 18:06:11 2024 ] 	Batch(2200/2353) done. Loss: 0.0520  lr:0.000100
[ Sat May 18 18:06:48 2024 ] 	Batch(2300/2353) done. Loss: 0.0100  lr:0.000100
[ Sat May 18 18:07:08 2024 ] 	Mean training loss: 0.0434.
[ Sat May 18 18:07:08 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 18:07:08 2024 ] Training epoch: 86
[ Sat May 18 18:07:09 2024 ] 	Batch(0/2353) done. Loss: 0.0143  lr:0.000100
[ Sat May 18 18:07:45 2024 ] 	Batch(100/2353) done. Loss: 0.0684  lr:0.000100
[ Sat May 18 18:08:22 2024 ] 	Batch(200/2353) done. Loss: 0.0221  lr:0.000100
[ Sat May 18 18:08:58 2024 ] 	Batch(300/2353) done. Loss: 0.0242  lr:0.000100
[ Sat May 18 18:09:35 2024 ] 	Batch(400/2353) done. Loss: 0.0083  lr:0.000100
[ Sat May 18 18:10:12 2024 ] 	Batch(500/2353) done. Loss: 0.0316  lr:0.000100
[ Sat May 18 18:10:49 2024 ] 	Batch(600/2353) done. Loss: 0.2401  lr:0.000100
[ Sat May 18 18:11:25 2024 ] 	Batch(700/2353) done. Loss: 0.0124  lr:0.000100
[ Sat May 18 18:12:02 2024 ] 	Batch(800/2353) done. Loss: 0.0962  lr:0.000100
[ Sat May 18 18:12:38 2024 ] 	Batch(900/2353) done. Loss: 0.0278  lr:0.000100
[ Sat May 18 18:13:15 2024 ] 	Batch(1000/2353) done. Loss: 0.0588  lr:0.000100
[ Sat May 18 18:13:52 2024 ] 	Batch(1100/2353) done. Loss: 0.0262  lr:0.000100
[ Sat May 18 18:14:28 2024 ] 	Batch(1200/2353) done. Loss: 0.0025  lr:0.000100
[ Sat May 18 18:15:05 2024 ] 	Batch(1300/2353) done. Loss: 0.0072  lr:0.000100
[ Sat May 18 18:15:42 2024 ] 	Batch(1400/2353) done. Loss: 0.0317  lr:0.000100
[ Sat May 18 18:16:18 2024 ] 	Batch(1500/2353) done. Loss: 0.0123  lr:0.000100
[ Sat May 18 18:16:55 2024 ] 	Batch(1600/2353) done. Loss: 0.0410  lr:0.000100
[ Sat May 18 18:17:32 2024 ] 	Batch(1700/2353) done. Loss: 0.0835  lr:0.000100
[ Sat May 18 18:18:08 2024 ] 	Batch(1800/2353) done. Loss: 0.0459  lr:0.000100
[ Sat May 18 18:18:45 2024 ] 	Batch(1900/2353) done. Loss: 0.0388  lr:0.000100
[ Sat May 18 18:19:22 2024 ] 	Batch(2000/2353) done. Loss: 0.1214  lr:0.000100
[ Sat May 18 18:19:58 2024 ] 	Batch(2100/2353) done. Loss: 0.0401  lr:0.000100
[ Sat May 18 18:20:35 2024 ] 	Batch(2200/2353) done. Loss: 0.0932  lr:0.000100
[ Sat May 18 18:21:11 2024 ] 	Batch(2300/2353) done. Loss: 0.0060  lr:0.000100
[ Sat May 18 18:21:31 2024 ] 	Mean training loss: 0.0432.
[ Sat May 18 18:21:31 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 18:21:32 2024 ] Training epoch: 87
[ Sat May 18 18:21:32 2024 ] 	Batch(0/2353) done. Loss: 0.0662  lr:0.000100
[ Sat May 18 18:22:09 2024 ] 	Batch(100/2353) done. Loss: 0.0079  lr:0.000100
[ Sat May 18 18:22:45 2024 ] 	Batch(200/2353) done. Loss: 0.0397  lr:0.000100
[ Sat May 18 18:23:22 2024 ] 	Batch(300/2353) done. Loss: 0.0085  lr:0.000100
[ Sat May 18 18:23:59 2024 ] 	Batch(400/2353) done. Loss: 0.0113  lr:0.000100
[ Sat May 18 18:24:35 2024 ] 	Batch(500/2353) done. Loss: 0.0110  lr:0.000100
[ Sat May 18 18:25:12 2024 ] 	Batch(600/2353) done. Loss: 0.0456  lr:0.000100
[ Sat May 18 18:25:49 2024 ] 	Batch(700/2353) done. Loss: 0.0699  lr:0.000100
[ Sat May 18 18:26:26 2024 ] 	Batch(800/2353) done. Loss: 0.0548  lr:0.000100
[ Sat May 18 18:27:02 2024 ] 	Batch(900/2353) done. Loss: 0.0666  lr:0.000100
[ Sat May 18 18:27:39 2024 ] 	Batch(1000/2353) done. Loss: 0.0593  lr:0.000100
[ Sat May 18 18:28:15 2024 ] 	Batch(1100/2353) done. Loss: 0.0077  lr:0.000100
[ Sat May 18 18:28:52 2024 ] 	Batch(1200/2353) done. Loss: 0.0336  lr:0.000100
[ Sat May 18 18:29:29 2024 ] 	Batch(1300/2353) done. Loss: 0.0248  lr:0.000100
[ Sat May 18 18:30:06 2024 ] 	Batch(1400/2353) done. Loss: 0.0047  lr:0.000100
[ Sat May 18 18:30:42 2024 ] 	Batch(1500/2353) done. Loss: 0.1826  lr:0.000100
[ Sat May 18 18:31:19 2024 ] 	Batch(1600/2353) done. Loss: 0.0587  lr:0.000100
[ Sat May 18 18:31:56 2024 ] 	Batch(1700/2353) done. Loss: 0.2108  lr:0.000100
[ Sat May 18 18:32:32 2024 ] 	Batch(1800/2353) done. Loss: 0.0048  lr:0.000100
[ Sat May 18 18:33:09 2024 ] 	Batch(1900/2353) done. Loss: 0.0503  lr:0.000100
[ Sat May 18 18:33:46 2024 ] 	Batch(2000/2353) done. Loss: 0.0367  lr:0.000100
[ Sat May 18 18:34:23 2024 ] 	Batch(2100/2353) done. Loss: 0.0581  lr:0.000100
[ Sat May 18 18:34:59 2024 ] 	Batch(2200/2353) done. Loss: 0.1896  lr:0.000100
[ Sat May 18 18:35:36 2024 ] 	Batch(2300/2353) done. Loss: 0.1260  lr:0.000100
[ Sat May 18 18:35:56 2024 ] 	Mean training loss: 0.0420.
[ Sat May 18 18:35:56 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 18:35:56 2024 ] Training epoch: 88
[ Sat May 18 18:35:56 2024 ] 	Batch(0/2353) done. Loss: 0.0762  lr:0.000100
[ Sat May 18 18:36:33 2024 ] 	Batch(100/2353) done. Loss: 0.1439  lr:0.000100
[ Sat May 18 18:37:10 2024 ] 	Batch(200/2353) done. Loss: 0.0555  lr:0.000100
[ Sat May 18 18:37:46 2024 ] 	Batch(300/2353) done. Loss: 0.0436  lr:0.000100
[ Sat May 18 18:38:23 2024 ] 	Batch(400/2353) done. Loss: 0.0257  lr:0.000100
[ Sat May 18 18:38:59 2024 ] 	Batch(500/2353) done. Loss: 0.1293  lr:0.000100
[ Sat May 18 18:39:36 2024 ] 	Batch(600/2353) done. Loss: 0.1561  lr:0.000100
[ Sat May 18 18:40:13 2024 ] 	Batch(700/2353) done. Loss: 0.0486  lr:0.000100
[ Sat May 18 18:40:49 2024 ] 	Batch(800/2353) done. Loss: 0.0589  lr:0.000100
[ Sat May 18 18:41:26 2024 ] 	Batch(900/2353) done. Loss: 0.0367  lr:0.000100
[ Sat May 18 18:42:03 2024 ] 	Batch(1000/2353) done. Loss: 0.0816  lr:0.000100
[ Sat May 18 18:42:39 2024 ] 	Batch(1100/2353) done. Loss: 0.0150  lr:0.000100
[ Sat May 18 18:43:16 2024 ] 	Batch(1200/2353) done. Loss: 0.1179  lr:0.000100
[ Sat May 18 18:43:53 2024 ] 	Batch(1300/2353) done. Loss: 0.0336  lr:0.000100
[ Sat May 18 18:44:29 2024 ] 	Batch(1400/2353) done. Loss: 0.0076  lr:0.000100
[ Sat May 18 18:45:06 2024 ] 	Batch(1500/2353) done. Loss: 0.0831  lr:0.000100
[ Sat May 18 18:45:43 2024 ] 	Batch(1600/2353) done. Loss: 0.0198  lr:0.000100
[ Sat May 18 18:46:20 2024 ] 	Batch(1700/2353) done. Loss: 0.0573  lr:0.000100
[ Sat May 18 18:46:56 2024 ] 	Batch(1800/2353) done. Loss: 0.0992  lr:0.000100
[ Sat May 18 18:47:33 2024 ] 	Batch(1900/2353) done. Loss: 0.0658  lr:0.000100
[ Sat May 18 18:48:10 2024 ] 	Batch(2000/2353) done. Loss: 0.0095  lr:0.000100
[ Sat May 18 18:48:47 2024 ] 	Batch(2100/2353) done. Loss: 0.1324  lr:0.000100
[ Sat May 18 18:49:23 2024 ] 	Batch(2200/2353) done. Loss: 0.0416  lr:0.000100
[ Sat May 18 18:50:00 2024 ] 	Batch(2300/2353) done. Loss: 0.0311  lr:0.000100
[ Sat May 18 18:50:20 2024 ] 	Mean training loss: 0.0431.
[ Sat May 18 18:50:20 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 18:50:20 2024 ] Training epoch: 89
[ Sat May 18 18:50:21 2024 ] 	Batch(0/2353) done. Loss: 0.0052  lr:0.000100
[ Sat May 18 18:50:57 2024 ] 	Batch(100/2353) done. Loss: 0.0224  lr:0.000100
[ Sat May 18 18:51:34 2024 ] 	Batch(200/2353) done. Loss: 0.0072  lr:0.000100
[ Sat May 18 18:52:11 2024 ] 	Batch(300/2353) done. Loss: 0.0020  lr:0.000100
[ Sat May 18 18:52:47 2024 ] 	Batch(400/2353) done. Loss: 0.0094  lr:0.000100
[ Sat May 18 18:53:24 2024 ] 	Batch(500/2353) done. Loss: 0.0115  lr:0.000100
[ Sat May 18 18:54:01 2024 ] 	Batch(600/2353) done. Loss: 0.0069  lr:0.000100
[ Sat May 18 18:54:37 2024 ] 	Batch(700/2353) done. Loss: 0.0081  lr:0.000100
[ Sat May 18 18:55:14 2024 ] 	Batch(800/2353) done. Loss: 0.1457  lr:0.000100
[ Sat May 18 18:55:51 2024 ] 	Batch(900/2353) done. Loss: 0.0375  lr:0.000100
[ Sat May 18 18:56:28 2024 ] 	Batch(1000/2353) done. Loss: 0.0150  lr:0.000100
[ Sat May 18 18:57:04 2024 ] 	Batch(1100/2353) done. Loss: 0.0146  lr:0.000100
[ Sat May 18 18:57:41 2024 ] 	Batch(1200/2353) done. Loss: 0.0122  lr:0.000100
[ Sat May 18 18:58:18 2024 ] 	Batch(1300/2353) done. Loss: 0.0189  lr:0.000100
[ Sat May 18 18:58:54 2024 ] 	Batch(1400/2353) done. Loss: 0.2658  lr:0.000100
[ Sat May 18 18:59:31 2024 ] 	Batch(1500/2353) done. Loss: 0.0870  lr:0.000100
[ Sat May 18 19:00:08 2024 ] 	Batch(1600/2353) done. Loss: 0.0252  lr:0.000100
[ Sat May 18 19:00:44 2024 ] 	Batch(1700/2353) done. Loss: 0.0736  lr:0.000100
[ Sat May 18 19:01:21 2024 ] 	Batch(1800/2353) done. Loss: 0.0570  lr:0.000100
[ Sat May 18 19:01:57 2024 ] 	Batch(1900/2353) done. Loss: 0.0610  lr:0.000100
[ Sat May 18 19:02:34 2024 ] 	Batch(2000/2353) done. Loss: 0.0220  lr:0.000100
[ Sat May 18 19:03:11 2024 ] 	Batch(2100/2353) done. Loss: 0.0213  lr:0.000100
[ Sat May 18 19:03:49 2024 ] 	Batch(2200/2353) done. Loss: 0.0144  lr:0.000100
[ Sat May 18 19:04:26 2024 ] 	Batch(2300/2353) done. Loss: 0.0060  lr:0.000100
[ Sat May 18 19:04:46 2024 ] 	Mean training loss: 0.0417.
[ Sat May 18 19:04:46 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 19:04:46 2024 ] Training epoch: 90
[ Sat May 18 19:04:47 2024 ] 	Batch(0/2353) done. Loss: 0.0063  lr:0.000100
[ Sat May 18 19:05:24 2024 ] 	Batch(100/2353) done. Loss: 0.0163  lr:0.000100
[ Sat May 18 19:06:01 2024 ] 	Batch(200/2353) done. Loss: 0.0369  lr:0.000100
[ Sat May 18 19:06:37 2024 ] 	Batch(300/2353) done. Loss: 0.0503  lr:0.000100
[ Sat May 18 19:07:14 2024 ] 	Batch(400/2353) done. Loss: 0.0434  lr:0.000100
[ Sat May 18 19:07:51 2024 ] 	Batch(500/2353) done. Loss: 0.0089  lr:0.000100
[ Sat May 18 19:08:28 2024 ] 	Batch(600/2353) done. Loss: 0.0331  lr:0.000100
[ Sat May 18 19:09:05 2024 ] 	Batch(700/2353) done. Loss: 0.0473  lr:0.000100
[ Sat May 18 19:09:41 2024 ] 	Batch(800/2353) done. Loss: 0.0294  lr:0.000100
[ Sat May 18 19:10:18 2024 ] 	Batch(900/2353) done. Loss: 0.1498  lr:0.000100
[ Sat May 18 19:10:55 2024 ] 	Batch(1000/2353) done. Loss: 0.0028  lr:0.000100
[ Sat May 18 19:11:31 2024 ] 	Batch(1100/2353) done. Loss: 0.0481  lr:0.000100
[ Sat May 18 19:12:08 2024 ] 	Batch(1200/2353) done. Loss: 0.0161  lr:0.000100
[ Sat May 18 19:12:44 2024 ] 	Batch(1300/2353) done. Loss: 0.0055  lr:0.000100
[ Sat May 18 19:13:21 2024 ] 	Batch(1400/2353) done. Loss: 0.0108  lr:0.000100
[ Sat May 18 19:13:58 2024 ] 	Batch(1500/2353) done. Loss: 0.0079  lr:0.000100
[ Sat May 18 19:14:35 2024 ] 	Batch(1600/2353) done. Loss: 0.0306  lr:0.000100
[ Sat May 18 19:15:11 2024 ] 	Batch(1700/2353) done. Loss: 0.1480  lr:0.000100
[ Sat May 18 19:15:48 2024 ] 	Batch(1800/2353) done. Loss: 0.0247  lr:0.000100
[ Sat May 18 19:16:24 2024 ] 	Batch(1900/2353) done. Loss: 0.0244  lr:0.000100
[ Sat May 18 19:17:01 2024 ] 	Batch(2000/2353) done. Loss: 0.0063  lr:0.000100
[ Sat May 18 19:17:38 2024 ] 	Batch(2100/2353) done. Loss: 0.0036  lr:0.000100
[ Sat May 18 19:18:14 2024 ] 	Batch(2200/2353) done. Loss: 0.0267  lr:0.000100
[ Sat May 18 19:18:51 2024 ] 	Batch(2300/2353) done. Loss: 0.0037  lr:0.000100
[ Sat May 18 19:19:11 2024 ] 	Mean training loss: 0.0431.
[ Sat May 18 19:19:11 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 19:19:11 2024 ] Eval epoch: 90
[ Sat May 18 19:21:14 2024 ] 	Mean val loss of 2367 batches: 0.24761399716535176.
[ Sat May 18 19:21:14 2024 ] Training epoch: 91
[ Sat May 18 19:21:15 2024 ] 	Batch(0/2353) done. Loss: 0.1518  lr:0.000001
[ Sat May 18 19:21:52 2024 ] 	Batch(100/2353) done. Loss: 0.0134  lr:0.000001
[ Sat May 18 19:22:28 2024 ] 	Batch(200/2353) done. Loss: 0.0134  lr:0.000001
[ Sat May 18 19:23:05 2024 ] 	Batch(300/2353) done. Loss: 0.1071  lr:0.000001
[ Sat May 18 19:23:42 2024 ] 	Batch(400/2353) done. Loss: 0.0262  lr:0.000001
[ Sat May 18 19:24:18 2024 ] 	Batch(500/2353) done. Loss: 0.0242  lr:0.000001
[ Sat May 18 19:24:55 2024 ] 	Batch(600/2353) done. Loss: 0.0109  lr:0.000001
[ Sat May 18 19:25:32 2024 ] 	Batch(700/2353) done. Loss: 0.0157  lr:0.000001
[ Sat May 18 19:26:08 2024 ] 	Batch(800/2353) done. Loss: 0.0703  lr:0.000001
[ Sat May 18 19:26:45 2024 ] 	Batch(900/2353) done. Loss: 0.0135  lr:0.000001
[ Sat May 18 19:27:22 2024 ] 	Batch(1000/2353) done. Loss: 0.0727  lr:0.000001
[ Sat May 18 19:27:58 2024 ] 	Batch(1100/2353) done. Loss: 0.0249  lr:0.000001
[ Sat May 18 19:28:35 2024 ] 	Batch(1200/2353) done. Loss: 0.0883  lr:0.000001
[ Sat May 18 19:29:12 2024 ] 	Batch(1300/2353) done. Loss: 0.0588  lr:0.000001
[ Sat May 18 19:29:48 2024 ] 	Batch(1400/2353) done. Loss: 0.0524  lr:0.000001
[ Sat May 18 19:30:25 2024 ] 	Batch(1500/2353) done. Loss: 0.0418  lr:0.000001
[ Sat May 18 19:31:02 2024 ] 	Batch(1600/2353) done. Loss: 0.0107  lr:0.000001
[ Sat May 18 19:31:39 2024 ] 	Batch(1700/2353) done. Loss: 0.0095  lr:0.000001
[ Sat May 18 19:32:15 2024 ] 	Batch(1800/2353) done. Loss: 0.0617  lr:0.000001
[ Sat May 18 19:32:52 2024 ] 	Batch(1900/2353) done. Loss: 0.0233  lr:0.000001
[ Sat May 18 19:33:29 2024 ] 	Batch(2000/2353) done. Loss: 0.0084  lr:0.000001
[ Sat May 18 19:34:05 2024 ] 	Batch(2100/2353) done. Loss: 0.0541  lr:0.000001
[ Sat May 18 19:34:42 2024 ] 	Batch(2200/2353) done. Loss: 0.0068  lr:0.000001
[ Sat May 18 19:35:19 2024 ] 	Batch(2300/2353) done. Loss: 0.0119  lr:0.000001
[ Sat May 18 19:35:39 2024 ] 	Mean training loss: 0.0418.
[ Sat May 18 19:35:39 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 19:35:39 2024 ] Training epoch: 92
[ Sat May 18 19:35:40 2024 ] 	Batch(0/2353) done. Loss: 0.0176  lr:0.000001
[ Sat May 18 19:36:16 2024 ] 	Batch(100/2353) done. Loss: 0.0063  lr:0.000001
[ Sat May 18 19:36:53 2024 ] 	Batch(200/2353) done. Loss: 0.0126  lr:0.000001
[ Sat May 18 19:37:29 2024 ] 	Batch(300/2353) done. Loss: 0.0117  lr:0.000001
[ Sat May 18 19:38:06 2024 ] 	Batch(400/2353) done. Loss: 0.0032  lr:0.000001
[ Sat May 18 19:38:43 2024 ] 	Batch(500/2353) done. Loss: 0.0485  lr:0.000001
[ Sat May 18 19:39:19 2024 ] 	Batch(600/2353) done. Loss: 0.0038  lr:0.000001
[ Sat May 18 19:39:56 2024 ] 	Batch(700/2353) done. Loss: 0.0809  lr:0.000001
[ Sat May 18 19:40:33 2024 ] 	Batch(800/2353) done. Loss: 0.0474  lr:0.000001
[ Sat May 18 19:41:10 2024 ] 	Batch(900/2353) done. Loss: 0.0228  lr:0.000001
[ Sat May 18 19:41:47 2024 ] 	Batch(1000/2353) done. Loss: 0.0256  lr:0.000001
[ Sat May 18 19:42:24 2024 ] 	Batch(1100/2353) done. Loss: 0.0604  lr:0.000001
[ Sat May 18 19:43:02 2024 ] 	Batch(1200/2353) done. Loss: 0.0673  lr:0.000001
[ Sat May 18 19:43:38 2024 ] 	Batch(1300/2353) done. Loss: 0.0101  lr:0.000001
[ Sat May 18 19:44:15 2024 ] 	Batch(1400/2353) done. Loss: 0.0322  lr:0.000001
[ Sat May 18 19:44:52 2024 ] 	Batch(1500/2353) done. Loss: 0.0070  lr:0.000001
[ Sat May 18 19:45:28 2024 ] 	Batch(1600/2353) done. Loss: 0.0041  lr:0.000001
[ Sat May 18 19:46:05 2024 ] 	Batch(1700/2353) done. Loss: 0.0109  lr:0.000001
[ Sat May 18 19:46:41 2024 ] 	Batch(1800/2353) done. Loss: 0.0192  lr:0.000001
[ Sat May 18 19:47:18 2024 ] 	Batch(1900/2353) done. Loss: 0.0781  lr:0.000001
[ Sat May 18 19:47:55 2024 ] 	Batch(2000/2353) done. Loss: 0.0256  lr:0.000001
[ Sat May 18 19:48:31 2024 ] 	Batch(2100/2353) done. Loss: 0.0993  lr:0.000001
[ Sat May 18 19:49:08 2024 ] 	Batch(2200/2353) done. Loss: 0.0163  lr:0.000001
[ Sat May 18 19:49:44 2024 ] 	Batch(2300/2353) done. Loss: 0.1889  lr:0.000001
[ Sat May 18 19:50:04 2024 ] 	Mean training loss: 0.0413.
[ Sat May 18 19:50:04 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 19:50:04 2024 ] Training epoch: 93
[ Sat May 18 19:50:05 2024 ] 	Batch(0/2353) done. Loss: 0.0527  lr:0.000001
[ Sat May 18 19:50:42 2024 ] 	Batch(100/2353) done. Loss: 0.0175  lr:0.000001
[ Sat May 18 19:51:18 2024 ] 	Batch(200/2353) done. Loss: 0.0053  lr:0.000001
[ Sat May 18 19:51:55 2024 ] 	Batch(300/2353) done. Loss: 0.0073  lr:0.000001
[ Sat May 18 19:52:31 2024 ] 	Batch(400/2353) done. Loss: 0.0390  lr:0.000001
[ Sat May 18 19:53:08 2024 ] 	Batch(500/2353) done. Loss: 0.0721  lr:0.000001
[ Sat May 18 19:53:44 2024 ] 	Batch(600/2353) done. Loss: 0.0027  lr:0.000001
[ Sat May 18 19:54:21 2024 ] 	Batch(700/2353) done. Loss: 0.0098  lr:0.000001
[ Sat May 18 19:54:58 2024 ] 	Batch(800/2353) done. Loss: 0.0115  lr:0.000001
[ Sat May 18 19:55:34 2024 ] 	Batch(900/2353) done. Loss: 0.0128  lr:0.000001
[ Sat May 18 19:56:11 2024 ] 	Batch(1000/2353) done. Loss: 0.0030  lr:0.000001
[ Sat May 18 19:56:47 2024 ] 	Batch(1100/2353) done. Loss: 0.0255  lr:0.000001
[ Sat May 18 19:57:24 2024 ] 	Batch(1200/2353) done. Loss: 0.0907  lr:0.000001
[ Sat May 18 19:58:00 2024 ] 	Batch(1300/2353) done. Loss: 0.0061  lr:0.000001
[ Sat May 18 19:58:37 2024 ] 	Batch(1400/2353) done. Loss: 0.0092  lr:0.000001
[ Sat May 18 19:59:14 2024 ] 	Batch(1500/2353) done. Loss: 0.0158  lr:0.000001
[ Sat May 18 19:59:50 2024 ] 	Batch(1600/2353) done. Loss: 0.1000  lr:0.000001
[ Sat May 18 20:00:27 2024 ] 	Batch(1700/2353) done. Loss: 0.0492  lr:0.000001
[ Sat May 18 20:01:04 2024 ] 	Batch(1800/2353) done. Loss: 0.0681  lr:0.000001
[ Sat May 18 20:01:40 2024 ] 	Batch(1900/2353) done. Loss: 0.0144  lr:0.000001
[ Sat May 18 20:02:17 2024 ] 	Batch(2000/2353) done. Loss: 0.0043  lr:0.000001
[ Sat May 18 20:02:53 2024 ] 	Batch(2100/2353) done. Loss: 0.0214  lr:0.000001
[ Sat May 18 20:03:30 2024 ] 	Batch(2200/2353) done. Loss: 0.0078  lr:0.000001
[ Sat May 18 20:04:06 2024 ] 	Batch(2300/2353) done. Loss: 0.0643  lr:0.000001
[ Sat May 18 20:04:27 2024 ] 	Mean training loss: 0.0422.
[ Sat May 18 20:04:27 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 20:04:27 2024 ] Training epoch: 94
[ Sat May 18 20:04:27 2024 ] 	Batch(0/2353) done. Loss: 0.0040  lr:0.000001
[ Sat May 18 20:05:04 2024 ] 	Batch(100/2353) done. Loss: 0.0033  lr:0.000001
[ Sat May 18 20:05:41 2024 ] 	Batch(200/2353) done. Loss: 0.0857  lr:0.000001
[ Sat May 18 20:06:17 2024 ] 	Batch(300/2353) done. Loss: 0.0022  lr:0.000001
[ Sat May 18 20:06:54 2024 ] 	Batch(400/2353) done. Loss: 0.0345  lr:0.000001
[ Sat May 18 20:07:31 2024 ] 	Batch(500/2353) done. Loss: 0.0064  lr:0.000001
[ Sat May 18 20:08:07 2024 ] 	Batch(600/2353) done. Loss: 0.0072  lr:0.000001
[ Sat May 18 20:08:44 2024 ] 	Batch(700/2353) done. Loss: 0.0102  lr:0.000001
[ Sat May 18 20:09:21 2024 ] 	Batch(800/2353) done. Loss: 0.0115  lr:0.000001
[ Sat May 18 20:09:57 2024 ] 	Batch(900/2353) done. Loss: 0.0058  lr:0.000001
[ Sat May 18 20:10:34 2024 ] 	Batch(1000/2353) done. Loss: 0.0047  lr:0.000001
[ Sat May 18 20:11:11 2024 ] 	Batch(1100/2353) done. Loss: 0.0014  lr:0.000001
[ Sat May 18 20:11:48 2024 ] 	Batch(1200/2353) done. Loss: 0.1314  lr:0.000001
[ Sat May 18 20:12:25 2024 ] 	Batch(1300/2353) done. Loss: 0.0132  lr:0.000001
[ Sat May 18 20:13:02 2024 ] 	Batch(1400/2353) done. Loss: 0.0032  lr:0.000001
[ Sat May 18 20:13:39 2024 ] 	Batch(1500/2353) done. Loss: 0.2454  lr:0.000001
[ Sat May 18 20:14:16 2024 ] 	Batch(1600/2353) done. Loss: 0.0181  lr:0.000001
[ Sat May 18 20:14:53 2024 ] 	Batch(1700/2353) done. Loss: 0.0287  lr:0.000001
[ Sat May 18 20:15:30 2024 ] 	Batch(1800/2353) done. Loss: 0.1634  lr:0.000001
[ Sat May 18 20:16:08 2024 ] 	Batch(1900/2353) done. Loss: 0.1647  lr:0.000001
[ Sat May 18 20:16:45 2024 ] 	Batch(2000/2353) done. Loss: 0.0019  lr:0.000001
[ Sat May 18 20:17:22 2024 ] 	Batch(2100/2353) done. Loss: 0.0330  lr:0.000001
[ Sat May 18 20:18:00 2024 ] 	Batch(2200/2353) done. Loss: 0.0101  lr:0.000001
[ Sat May 18 20:18:37 2024 ] 	Batch(2300/2353) done. Loss: 0.0146  lr:0.000001
[ Sat May 18 20:18:57 2024 ] 	Mean training loss: 0.0414.
[ Sat May 18 20:18:57 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 20:18:58 2024 ] Training epoch: 95
[ Sat May 18 20:18:58 2024 ] 	Batch(0/2353) done. Loss: 0.0088  lr:0.000001
[ Sat May 18 20:19:35 2024 ] 	Batch(100/2353) done. Loss: 0.0038  lr:0.000001
[ Sat May 18 20:20:11 2024 ] 	Batch(200/2353) done. Loss: 0.0193  lr:0.000001
[ Sat May 18 20:20:48 2024 ] 	Batch(300/2353) done. Loss: 0.0212  lr:0.000001
[ Sat May 18 20:21:25 2024 ] 	Batch(400/2353) done. Loss: 0.0855  lr:0.000001
[ Sat May 18 20:22:01 2024 ] 	Batch(500/2353) done. Loss: 0.0449  lr:0.000001
[ Sat May 18 20:22:38 2024 ] 	Batch(600/2353) done. Loss: 0.0046  lr:0.000001
[ Sat May 18 20:23:14 2024 ] 	Batch(700/2353) done. Loss: 0.0277  lr:0.000001
[ Sat May 18 20:23:51 2024 ] 	Batch(800/2353) done. Loss: 0.1322  lr:0.000001
[ Sat May 18 20:24:28 2024 ] 	Batch(900/2353) done. Loss: 0.0044  lr:0.000001
[ Sat May 18 20:25:04 2024 ] 	Batch(1000/2353) done. Loss: 0.0255  lr:0.000001
[ Sat May 18 20:25:41 2024 ] 	Batch(1100/2353) done. Loss: 0.0178  lr:0.000001
[ Sat May 18 20:26:18 2024 ] 	Batch(1200/2353) done. Loss: 0.1134  lr:0.000001
[ Sat May 18 20:26:54 2024 ] 	Batch(1300/2353) done. Loss: 0.0251  lr:0.000001
[ Sat May 18 20:27:31 2024 ] 	Batch(1400/2353) done. Loss: 0.0053  lr:0.000001
[ Sat May 18 20:28:08 2024 ] 	Batch(1500/2353) done. Loss: 0.0228  lr:0.000001
[ Sat May 18 20:28:44 2024 ] 	Batch(1600/2353) done. Loss: 0.0124  lr:0.000001
[ Sat May 18 20:29:21 2024 ] 	Batch(1700/2353) done. Loss: 0.0567  lr:0.000001
[ Sat May 18 20:29:57 2024 ] 	Batch(1800/2353) done. Loss: 0.0360  lr:0.000001
[ Sat May 18 20:30:34 2024 ] 	Batch(1900/2353) done. Loss: 0.0169  lr:0.000001
[ Sat May 18 20:31:11 2024 ] 	Batch(2000/2353) done. Loss: 0.0658  lr:0.000001
[ Sat May 18 20:31:47 2024 ] 	Batch(2100/2353) done. Loss: 0.1578  lr:0.000001
[ Sat May 18 20:32:24 2024 ] 	Batch(2200/2353) done. Loss: 0.1189  lr:0.000001
[ Sat May 18 20:33:00 2024 ] 	Batch(2300/2353) done. Loss: 0.0509  lr:0.000001
[ Sat May 18 20:33:20 2024 ] 	Mean training loss: 0.0407.
[ Sat May 18 20:33:20 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 20:33:20 2024 ] Training epoch: 96
[ Sat May 18 20:33:21 2024 ] 	Batch(0/2353) done. Loss: 0.0377  lr:0.000001
[ Sat May 18 20:33:58 2024 ] 	Batch(100/2353) done. Loss: 0.0050  lr:0.000001
[ Sat May 18 20:34:36 2024 ] 	Batch(200/2353) done. Loss: 0.0298  lr:0.000001
[ Sat May 18 20:35:13 2024 ] 	Batch(300/2353) done. Loss: 0.0059  lr:0.000001
[ Sat May 18 20:35:50 2024 ] 	Batch(400/2353) done. Loss: 0.0768  lr:0.000001
[ Sat May 18 20:36:27 2024 ] 	Batch(500/2353) done. Loss: 0.0016  lr:0.000001
[ Sat May 18 20:37:04 2024 ] 	Batch(600/2353) done. Loss: 0.0158  lr:0.000001
[ Sat May 18 20:37:40 2024 ] 	Batch(700/2353) done. Loss: 0.0051  lr:0.000001
[ Sat May 18 20:38:17 2024 ] 	Batch(800/2353) done. Loss: 0.0063  lr:0.000001
[ Sat May 18 20:38:54 2024 ] 	Batch(900/2353) done. Loss: 0.0267  lr:0.000001
[ Sat May 18 20:39:30 2024 ] 	Batch(1000/2353) done. Loss: 0.1049  lr:0.000001
[ Sat May 18 20:40:07 2024 ] 	Batch(1100/2353) done. Loss: 0.0295  lr:0.000001
[ Sat May 18 20:40:44 2024 ] 	Batch(1200/2353) done. Loss: 0.0777  lr:0.000001
[ Sat May 18 20:41:20 2024 ] 	Batch(1300/2353) done. Loss: 0.1557  lr:0.000001
[ Sat May 18 20:41:57 2024 ] 	Batch(1400/2353) done. Loss: 0.0017  lr:0.000001
[ Sat May 18 20:42:33 2024 ] 	Batch(1500/2353) done. Loss: 0.0104  lr:0.000001
[ Sat May 18 20:43:10 2024 ] 	Batch(1600/2353) done. Loss: 0.0357  lr:0.000001
[ Sat May 18 20:43:47 2024 ] 	Batch(1700/2353) done. Loss: 0.0870  lr:0.000001
[ Sat May 18 20:44:23 2024 ] 	Batch(1800/2353) done. Loss: 0.0587  lr:0.000001
[ Sat May 18 20:45:00 2024 ] 	Batch(1900/2353) done. Loss: 0.0222  lr:0.000001
[ Sat May 18 20:45:37 2024 ] 	Batch(2000/2353) done. Loss: 0.0304  lr:0.000001
[ Sat May 18 20:46:13 2024 ] 	Batch(2100/2353) done. Loss: 0.0297  lr:0.000001
[ Sat May 18 20:46:50 2024 ] 	Batch(2200/2353) done. Loss: 0.0294  lr:0.000001
[ Sat May 18 20:47:26 2024 ] 	Batch(2300/2353) done. Loss: 0.0026  lr:0.000001
[ Sat May 18 20:47:47 2024 ] 	Mean training loss: 0.0422.
[ Sat May 18 20:47:47 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 20:47:47 2024 ] Training epoch: 97
[ Sat May 18 20:47:47 2024 ] 	Batch(0/2353) done. Loss: 0.0134  lr:0.000001
[ Sat May 18 20:48:24 2024 ] 	Batch(100/2353) done. Loss: 0.1862  lr:0.000001
[ Sat May 18 20:49:00 2024 ] 	Batch(200/2353) done. Loss: 0.0293  lr:0.000001
[ Sat May 18 20:49:37 2024 ] 	Batch(300/2353) done. Loss: 0.0682  lr:0.000001
[ Sat May 18 20:50:14 2024 ] 	Batch(400/2353) done. Loss: 0.0047  lr:0.000001
[ Sat May 18 20:50:50 2024 ] 	Batch(500/2353) done. Loss: 0.0179  lr:0.000001
[ Sat May 18 20:51:27 2024 ] 	Batch(600/2353) done. Loss: 0.0121  lr:0.000001
[ Sat May 18 20:52:04 2024 ] 	Batch(700/2353) done. Loss: 0.0257  lr:0.000001
[ Sat May 18 20:52:40 2024 ] 	Batch(800/2353) done. Loss: 0.0497  lr:0.000001
[ Sat May 18 20:53:17 2024 ] 	Batch(900/2353) done. Loss: 0.0537  lr:0.000001
[ Sat May 18 20:53:54 2024 ] 	Batch(1000/2353) done. Loss: 0.0261  lr:0.000001
[ Sat May 18 20:54:31 2024 ] 	Batch(1100/2353) done. Loss: 0.0035  lr:0.000001
[ Sat May 18 20:55:08 2024 ] 	Batch(1200/2353) done. Loss: 0.0178  lr:0.000001
[ Sat May 18 20:55:44 2024 ] 	Batch(1300/2353) done. Loss: 0.0198  lr:0.000001
[ Sat May 18 20:56:21 2024 ] 	Batch(1400/2353) done. Loss: 0.0025  lr:0.000001
[ Sat May 18 20:56:58 2024 ] 	Batch(1500/2353) done. Loss: 0.0659  lr:0.000001
[ Sat May 18 20:57:34 2024 ] 	Batch(1600/2353) done. Loss: 0.0234  lr:0.000001
[ Sat May 18 20:58:11 2024 ] 	Batch(1700/2353) done. Loss: 0.0129  lr:0.000001
[ Sat May 18 20:58:48 2024 ] 	Batch(1800/2353) done. Loss: 0.0213  lr:0.000001
[ Sat May 18 20:59:24 2024 ] 	Batch(1900/2353) done. Loss: 0.0014  lr:0.000001
[ Sat May 18 21:00:01 2024 ] 	Batch(2000/2353) done. Loss: 0.0882  lr:0.000001
[ Sat May 18 21:00:38 2024 ] 	Batch(2100/2353) done. Loss: 0.0251  lr:0.000001
[ Sat May 18 21:01:16 2024 ] 	Batch(2200/2353) done. Loss: 0.0222  lr:0.000001
[ Sat May 18 21:01:53 2024 ] 	Batch(2300/2353) done. Loss: 0.0491  lr:0.000001
[ Sat May 18 21:02:13 2024 ] 	Mean training loss: 0.0406.
[ Sat May 18 21:02:13 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 21:02:13 2024 ] Training epoch: 98
[ Sat May 18 21:02:14 2024 ] 	Batch(0/2353) done. Loss: 0.0150  lr:0.000001
[ Sat May 18 21:02:51 2024 ] 	Batch(100/2353) done. Loss: 0.0064  lr:0.000001
[ Sat May 18 21:03:28 2024 ] 	Batch(200/2353) done. Loss: 0.0187  lr:0.000001
[ Sat May 18 21:04:06 2024 ] 	Batch(300/2353) done. Loss: 0.0243  lr:0.000001
[ Sat May 18 21:04:43 2024 ] 	Batch(400/2353) done. Loss: 0.0120  lr:0.000001
[ Sat May 18 21:05:20 2024 ] 	Batch(500/2353) done. Loss: 0.0162  lr:0.000001
[ Sat May 18 21:05:57 2024 ] 	Batch(600/2353) done. Loss: 0.0025  lr:0.000001
[ Sat May 18 21:06:33 2024 ] 	Batch(700/2353) done. Loss: 0.0101  lr:0.000001
[ Sat May 18 21:07:10 2024 ] 	Batch(800/2353) done. Loss: 0.0015  lr:0.000001
[ Sat May 18 21:07:46 2024 ] 	Batch(900/2353) done. Loss: 0.0638  lr:0.000001
[ Sat May 18 21:08:23 2024 ] 	Batch(1000/2353) done. Loss: 0.0879  lr:0.000001
[ Sat May 18 21:09:00 2024 ] 	Batch(1100/2353) done. Loss: 0.0142  lr:0.000001
[ Sat May 18 21:09:37 2024 ] 	Batch(1200/2353) done. Loss: 0.0103  lr:0.000001
[ Sat May 18 21:10:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0104  lr:0.000001
[ Sat May 18 21:10:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0115  lr:0.000001
[ Sat May 18 21:11:26 2024 ] 	Batch(1500/2353) done. Loss: 0.3781  lr:0.000001
[ Sat May 18 21:12:03 2024 ] 	Batch(1600/2353) done. Loss: 0.0282  lr:0.000001
[ Sat May 18 21:12:40 2024 ] 	Batch(1700/2353) done. Loss: 0.0070  lr:0.000001
[ Sat May 18 21:13:16 2024 ] 	Batch(1800/2353) done. Loss: 0.0231  lr:0.000001
[ Sat May 18 21:13:53 2024 ] 	Batch(1900/2353) done. Loss: 0.0479  lr:0.000001
[ Sat May 18 21:14:30 2024 ] 	Batch(2000/2353) done. Loss: 0.0205  lr:0.000001
[ Sat May 18 21:15:07 2024 ] 	Batch(2100/2353) done. Loss: 0.0216  lr:0.000001
[ Sat May 18 21:15:44 2024 ] 	Batch(2200/2353) done. Loss: 0.0809  lr:0.000001
[ Sat May 18 21:16:21 2024 ] 	Batch(2300/2353) done. Loss: 0.0256  lr:0.000001
[ Sat May 18 21:16:40 2024 ] 	Mean training loss: 0.0421.
[ Sat May 18 21:16:40 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 21:16:41 2024 ] Training epoch: 99
[ Sat May 18 21:16:41 2024 ] 	Batch(0/2353) done. Loss: 0.0493  lr:0.000001
[ Sat May 18 21:17:18 2024 ] 	Batch(100/2353) done. Loss: 0.0383  lr:0.000001
[ Sat May 18 21:17:54 2024 ] 	Batch(200/2353) done. Loss: 0.0407  lr:0.000001
[ Sat May 18 21:18:31 2024 ] 	Batch(300/2353) done. Loss: 0.0187  lr:0.000001
[ Sat May 18 21:19:08 2024 ] 	Batch(400/2353) done. Loss: 0.0697  lr:0.000001
[ Sat May 18 21:19:44 2024 ] 	Batch(500/2353) done. Loss: 0.0309  lr:0.000001
[ Sat May 18 21:20:21 2024 ] 	Batch(600/2353) done. Loss: 0.0063  lr:0.000001
[ Sat May 18 21:20:57 2024 ] 	Batch(700/2353) done. Loss: 0.0141  lr:0.000001
[ Sat May 18 21:21:34 2024 ] 	Batch(800/2353) done. Loss: 0.0831  lr:0.000001
[ Sat May 18 21:22:11 2024 ] 	Batch(900/2353) done. Loss: 0.0266  lr:0.000001
[ Sat May 18 21:22:47 2024 ] 	Batch(1000/2353) done. Loss: 0.0009  lr:0.000001
[ Sat May 18 21:23:24 2024 ] 	Batch(1100/2353) done. Loss: 0.0310  lr:0.000001
[ Sat May 18 21:24:00 2024 ] 	Batch(1200/2353) done. Loss: 0.1326  lr:0.000001
[ Sat May 18 21:24:37 2024 ] 	Batch(1300/2353) done. Loss: 0.0019  lr:0.000001
[ Sat May 18 21:25:14 2024 ] 	Batch(1400/2353) done. Loss: 0.0041  lr:0.000001
[ Sat May 18 21:25:50 2024 ] 	Batch(1500/2353) done. Loss: 0.0193  lr:0.000001
[ Sat May 18 21:26:27 2024 ] 	Batch(1600/2353) done. Loss: 0.0204  lr:0.000001
[ Sat May 18 21:27:04 2024 ] 	Batch(1700/2353) done. Loss: 0.0920  lr:0.000001
[ Sat May 18 21:27:40 2024 ] 	Batch(1800/2353) done. Loss: 0.0507  lr:0.000001
[ Sat May 18 21:28:17 2024 ] 	Batch(1900/2353) done. Loss: 0.0299  lr:0.000001
[ Sat May 18 21:28:54 2024 ] 	Batch(2000/2353) done. Loss: 0.1517  lr:0.000001
[ Sat May 18 21:29:30 2024 ] 	Batch(2100/2353) done. Loss: 0.0108  lr:0.000001
[ Sat May 18 21:30:07 2024 ] 	Batch(2200/2353) done. Loss: 0.1223  lr:0.000001
[ Sat May 18 21:30:43 2024 ] 	Batch(2300/2353) done. Loss: 0.0042  lr:0.000001
[ Sat May 18 21:31:03 2024 ] 	Mean training loss: 0.0416.
[ Sat May 18 21:31:03 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 21:31:03 2024 ] Training epoch: 100
[ Sat May 18 21:31:04 2024 ] 	Batch(0/2353) done. Loss: 0.0165  lr:0.000001
[ Sat May 18 21:31:41 2024 ] 	Batch(100/2353) done. Loss: 0.1307  lr:0.000001
[ Sat May 18 21:32:17 2024 ] 	Batch(200/2353) done. Loss: 0.0857  lr:0.000001
[ Sat May 18 21:32:54 2024 ] 	Batch(300/2353) done. Loss: 0.0109  lr:0.000001
[ Sat May 18 21:33:31 2024 ] 	Batch(400/2353) done. Loss: 0.1035  lr:0.000001
[ Sat May 18 21:34:07 2024 ] 	Batch(500/2353) done. Loss: 0.0077  lr:0.000001
[ Sat May 18 21:34:44 2024 ] 	Batch(600/2353) done. Loss: 0.0222  lr:0.000001
[ Sat May 18 21:35:20 2024 ] 	Batch(700/2353) done. Loss: 0.0067  lr:0.000001
[ Sat May 18 21:35:58 2024 ] 	Batch(800/2353) done. Loss: 0.0981  lr:0.000001
[ Sat May 18 21:36:35 2024 ] 	Batch(900/2353) done. Loss: 0.0501  lr:0.000001
[ Sat May 18 21:37:13 2024 ] 	Batch(1000/2353) done. Loss: 0.0271  lr:0.000001
[ Sat May 18 21:37:50 2024 ] 	Batch(1100/2353) done. Loss: 0.0076  lr:0.000001
[ Sat May 18 21:38:27 2024 ] 	Batch(1200/2353) done. Loss: 0.0110  lr:0.000001
[ Sat May 18 21:39:05 2024 ] 	Batch(1300/2353) done. Loss: 0.1015  lr:0.000001
[ Sat May 18 21:39:42 2024 ] 	Batch(1400/2353) done. Loss: 0.0249  lr:0.000001
[ Sat May 18 21:40:18 2024 ] 	Batch(1500/2353) done. Loss: 0.0318  lr:0.000001
[ Sat May 18 21:40:55 2024 ] 	Batch(1600/2353) done. Loss: 0.0265  lr:0.000001
[ Sat May 18 21:41:32 2024 ] 	Batch(1700/2353) done. Loss: 0.0061  lr:0.000001
[ Sat May 18 21:42:08 2024 ] 	Batch(1800/2353) done. Loss: 0.0796  lr:0.000001
[ Sat May 18 21:42:45 2024 ] 	Batch(1900/2353) done. Loss: 0.0065  lr:0.000001
[ Sat May 18 21:43:22 2024 ] 	Batch(2000/2353) done. Loss: 0.1618  lr:0.000001
[ Sat May 18 21:43:58 2024 ] 	Batch(2100/2353) done. Loss: 0.0091  lr:0.000001
[ Sat May 18 21:44:35 2024 ] 	Batch(2200/2353) done. Loss: 0.0538  lr:0.000001
[ Sat May 18 21:45:11 2024 ] 	Batch(2300/2353) done. Loss: 0.1471  lr:0.000001
[ Sat May 18 21:45:31 2024 ] 	Mean training loss: 0.0418.
[ Sat May 18 21:45:31 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 21:45:31 2024 ] Eval epoch: 100
[ Sat May 18 21:47:35 2024 ] 	Mean val loss of 2367 batches: 0.2468508650588744.
[ Sat May 18 21:47:35 2024 ] Training epoch: 101
[ Sat May 18 21:47:36 2024 ] 	Batch(0/2353) done. Loss: 0.0038  lr:0.000001
[ Sat May 18 21:48:12 2024 ] 	Batch(100/2353) done. Loss: 0.0036  lr:0.000001
[ Sat May 18 21:48:49 2024 ] 	Batch(200/2353) done. Loss: 0.0110  lr:0.000001
[ Sat May 18 21:49:25 2024 ] 	Batch(300/2353) done. Loss: 0.0261  lr:0.000001
[ Sat May 18 21:50:02 2024 ] 	Batch(400/2353) done. Loss: 0.0249  lr:0.000001
[ Sat May 18 21:50:39 2024 ] 	Batch(500/2353) done. Loss: 0.0027  lr:0.000001
[ Sat May 18 21:51:15 2024 ] 	Batch(600/2353) done. Loss: 0.0558  lr:0.000001
[ Sat May 18 21:51:52 2024 ] 	Batch(700/2353) done. Loss: 0.0020  lr:0.000001
[ Sat May 18 21:52:28 2024 ] 	Batch(800/2353) done. Loss: 0.0207  lr:0.000001
[ Sat May 18 21:53:05 2024 ] 	Batch(900/2353) done. Loss: 0.1925  lr:0.000001
[ Sat May 18 21:53:41 2024 ] 	Batch(1000/2353) done. Loss: 0.0292  lr:0.000001
[ Sat May 18 21:54:18 2024 ] 	Batch(1100/2353) done. Loss: 0.0392  lr:0.000001
[ Sat May 18 21:54:55 2024 ] 	Batch(1200/2353) done. Loss: 0.0018  lr:0.000001
[ Sat May 18 21:55:31 2024 ] 	Batch(1300/2353) done. Loss: 0.0064  lr:0.000001
[ Sat May 18 21:56:08 2024 ] 	Batch(1400/2353) done. Loss: 0.0130  lr:0.000001
[ Sat May 18 21:56:44 2024 ] 	Batch(1500/2353) done. Loss: 0.0278  lr:0.000001
[ Sat May 18 21:57:21 2024 ] 	Batch(1600/2353) done. Loss: 0.0421  lr:0.000001
[ Sat May 18 21:57:58 2024 ] 	Batch(1700/2353) done. Loss: 0.0224  lr:0.000001
[ Sat May 18 21:58:34 2024 ] 	Batch(1800/2353) done. Loss: 0.0052  lr:0.000001
[ Sat May 18 21:59:11 2024 ] 	Batch(1900/2353) done. Loss: 0.0565  lr:0.000001
[ Sat May 18 21:59:47 2024 ] 	Batch(2000/2353) done. Loss: 0.0249  lr:0.000001
[ Sat May 18 22:00:25 2024 ] 	Batch(2100/2353) done. Loss: 0.0106  lr:0.000001
[ Sat May 18 22:01:02 2024 ] 	Batch(2200/2353) done. Loss: 0.0048  lr:0.000001
[ Sat May 18 22:01:39 2024 ] 	Batch(2300/2353) done. Loss: 0.1611  lr:0.000001
[ Sat May 18 22:01:59 2024 ] 	Mean training loss: 0.0409.
[ Sat May 18 22:01:59 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 22:02:00 2024 ] Training epoch: 102
[ Sat May 18 22:02:00 2024 ] 	Batch(0/2353) done. Loss: 0.0189  lr:0.000001
[ Sat May 18 22:02:37 2024 ] 	Batch(100/2353) done. Loss: 0.0668  lr:0.000001
[ Sat May 18 22:03:13 2024 ] 	Batch(200/2353) done. Loss: 0.0456  lr:0.000001
[ Sat May 18 22:03:50 2024 ] 	Batch(300/2353) done. Loss: 0.0123  lr:0.000001
[ Sat May 18 22:04:27 2024 ] 	Batch(400/2353) done. Loss: 0.0050  lr:0.000001
[ Sat May 18 22:05:03 2024 ] 	Batch(500/2353) done. Loss: 0.0187  lr:0.000001
[ Sat May 18 22:05:40 2024 ] 	Batch(600/2353) done. Loss: 0.0134  lr:0.000001
[ Sat May 18 22:06:16 2024 ] 	Batch(700/2353) done. Loss: 0.2177  lr:0.000001
[ Sat May 18 22:06:53 2024 ] 	Batch(800/2353) done. Loss: 0.0012  lr:0.000001
[ Sat May 18 22:07:30 2024 ] 	Batch(900/2353) done. Loss: 0.0205  lr:0.000001
[ Sat May 18 22:08:06 2024 ] 	Batch(1000/2353) done. Loss: 0.0074  lr:0.000001
[ Sat May 18 22:08:43 2024 ] 	Batch(1100/2353) done. Loss: 0.0143  lr:0.000001
[ Sat May 18 22:09:20 2024 ] 	Batch(1200/2353) done. Loss: 0.0435  lr:0.000001
[ Sat May 18 22:09:56 2024 ] 	Batch(1300/2353) done. Loss: 0.0057  lr:0.000001
[ Sat May 18 22:10:33 2024 ] 	Batch(1400/2353) done. Loss: 0.0072  lr:0.000001
[ Sat May 18 22:11:09 2024 ] 	Batch(1500/2353) done. Loss: 0.0643  lr:0.000001
[ Sat May 18 22:11:46 2024 ] 	Batch(1600/2353) done. Loss: 0.0419  lr:0.000001
[ Sat May 18 22:12:23 2024 ] 	Batch(1700/2353) done. Loss: 0.0073  lr:0.000001
[ Sat May 18 22:12:59 2024 ] 	Batch(1800/2353) done. Loss: 0.0101  lr:0.000001
[ Sat May 18 22:13:36 2024 ] 	Batch(1900/2353) done. Loss: 0.0285  lr:0.000001
[ Sat May 18 22:14:13 2024 ] 	Batch(2000/2353) done. Loss: 0.0077  lr:0.000001
[ Sat May 18 22:14:49 2024 ] 	Batch(2100/2353) done. Loss: 0.0039  lr:0.000001
[ Sat May 18 22:15:26 2024 ] 	Batch(2200/2353) done. Loss: 0.0320  lr:0.000001
[ Sat May 18 22:16:02 2024 ] 	Batch(2300/2353) done. Loss: 0.0023  lr:0.000001
[ Sat May 18 22:16:22 2024 ] 	Mean training loss: 0.0421.
[ Sat May 18 22:16:22 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 22:16:22 2024 ] Training epoch: 103
[ Sat May 18 22:16:23 2024 ] 	Batch(0/2353) done. Loss: 0.0173  lr:0.000001
[ Sat May 18 22:17:00 2024 ] 	Batch(100/2353) done. Loss: 0.0187  lr:0.000001
[ Sat May 18 22:17:36 2024 ] 	Batch(200/2353) done. Loss: 0.0138  lr:0.000001
[ Sat May 18 22:18:13 2024 ] 	Batch(300/2353) done. Loss: 0.0472  lr:0.000001
[ Sat May 18 22:18:50 2024 ] 	Batch(400/2353) done. Loss: 0.0571  lr:0.000001
[ Sat May 18 22:19:26 2024 ] 	Batch(500/2353) done. Loss: 0.0544  lr:0.000001
[ Sat May 18 22:20:03 2024 ] 	Batch(600/2353) done. Loss: 0.0153  lr:0.000001
[ Sat May 18 22:20:39 2024 ] 	Batch(700/2353) done. Loss: 0.0363  lr:0.000001
[ Sat May 18 22:21:16 2024 ] 	Batch(800/2353) done. Loss: 0.0821  lr:0.000001
[ Sat May 18 22:21:53 2024 ] 	Batch(900/2353) done. Loss: 0.0096  lr:0.000001
[ Sat May 18 22:22:31 2024 ] 	Batch(1000/2353) done. Loss: 0.0279  lr:0.000001
[ Sat May 18 22:23:08 2024 ] 	Batch(1100/2353) done. Loss: 0.0219  lr:0.000001
[ Sat May 18 22:23:46 2024 ] 	Batch(1200/2353) done. Loss: 0.0382  lr:0.000001
[ Sat May 18 22:24:23 2024 ] 	Batch(1300/2353) done. Loss: 0.0420  lr:0.000001
[ Sat May 18 22:25:00 2024 ] 	Batch(1400/2353) done. Loss: 0.0372  lr:0.000001
[ Sat May 18 22:25:37 2024 ] 	Batch(1500/2353) done. Loss: 0.0210  lr:0.000001
[ Sat May 18 22:26:15 2024 ] 	Batch(1600/2353) done. Loss: 0.0040  lr:0.000001
[ Sat May 18 22:26:52 2024 ] 	Batch(1700/2353) done. Loss: 0.0043  lr:0.000001
[ Sat May 18 22:27:29 2024 ] 	Batch(1800/2353) done. Loss: 0.0219  lr:0.000001
[ Sat May 18 22:28:07 2024 ] 	Batch(1900/2353) done. Loss: 0.0127  lr:0.000001
[ Sat May 18 22:28:44 2024 ] 	Batch(2000/2353) done. Loss: 0.0196  lr:0.000001
[ Sat May 18 22:29:21 2024 ] 	Batch(2100/2353) done. Loss: 0.0173  lr:0.000001
[ Sat May 18 22:29:59 2024 ] 	Batch(2200/2353) done. Loss: 0.0617  lr:0.000001
[ Sat May 18 22:30:36 2024 ] 	Batch(2300/2353) done. Loss: 0.0090  lr:0.000001
[ Sat May 18 22:30:56 2024 ] 	Mean training loss: 0.0431.
[ Sat May 18 22:30:56 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 22:30:56 2024 ] Training epoch: 104
[ Sat May 18 22:30:57 2024 ] 	Batch(0/2353) done. Loss: 0.0898  lr:0.000001
[ Sat May 18 22:31:34 2024 ] 	Batch(100/2353) done. Loss: 0.0220  lr:0.000001
[ Sat May 18 22:32:10 2024 ] 	Batch(200/2353) done. Loss: 0.0697  lr:0.000001
[ Sat May 18 22:32:47 2024 ] 	Batch(300/2353) done. Loss: 0.1698  lr:0.000001
[ Sat May 18 22:33:24 2024 ] 	Batch(400/2353) done. Loss: 0.0311  lr:0.000001
[ Sat May 18 22:34:00 2024 ] 	Batch(500/2353) done. Loss: 0.0117  lr:0.000001
[ Sat May 18 22:34:37 2024 ] 	Batch(600/2353) done. Loss: 0.0200  lr:0.000001
[ Sat May 18 22:35:13 2024 ] 	Batch(700/2353) done. Loss: 0.0277  lr:0.000001
[ Sat May 18 22:35:50 2024 ] 	Batch(800/2353) done. Loss: 0.0113  lr:0.000001
[ Sat May 18 22:36:27 2024 ] 	Batch(900/2353) done. Loss: 0.0309  lr:0.000001
[ Sat May 18 22:37:04 2024 ] 	Batch(1000/2353) done. Loss: 0.0105  lr:0.000001
[ Sat May 18 22:37:40 2024 ] 	Batch(1100/2353) done. Loss: 0.0510  lr:0.000001
[ Sat May 18 22:38:17 2024 ] 	Batch(1200/2353) done. Loss: 0.0159  lr:0.000001
[ Sat May 18 22:38:54 2024 ] 	Batch(1300/2353) done. Loss: 0.0091  lr:0.000001
[ Sat May 18 22:39:30 2024 ] 	Batch(1400/2353) done. Loss: 0.0415  lr:0.000001
[ Sat May 18 22:40:07 2024 ] 	Batch(1500/2353) done. Loss: 0.0743  lr:0.000001
[ Sat May 18 22:40:44 2024 ] 	Batch(1600/2353) done. Loss: 0.0396  lr:0.000001
[ Sat May 18 22:41:20 2024 ] 	Batch(1700/2353) done. Loss: 0.0338  lr:0.000001
[ Sat May 18 22:41:57 2024 ] 	Batch(1800/2353) done. Loss: 0.0058  lr:0.000001
[ Sat May 18 22:42:34 2024 ] 	Batch(1900/2353) done. Loss: 0.0555  lr:0.000001
[ Sat May 18 22:43:10 2024 ] 	Batch(2000/2353) done. Loss: 0.0024  lr:0.000001
[ Sat May 18 22:43:48 2024 ] 	Batch(2100/2353) done. Loss: 0.0025  lr:0.000001
[ Sat May 18 22:44:25 2024 ] 	Batch(2200/2353) done. Loss: 0.0208  lr:0.000001
[ Sat May 18 22:45:02 2024 ] 	Batch(2300/2353) done. Loss: 0.0324  lr:0.000001
[ Sat May 18 22:45:23 2024 ] 	Mean training loss: 0.0411.
[ Sat May 18 22:45:23 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 22:45:23 2024 ] Training epoch: 105
[ Sat May 18 22:45:23 2024 ] 	Batch(0/2353) done. Loss: 0.0098  lr:0.000001
[ Sat May 18 22:46:00 2024 ] 	Batch(100/2353) done. Loss: 0.0035  lr:0.000001
[ Sat May 18 22:46:37 2024 ] 	Batch(200/2353) done. Loss: 0.0094  lr:0.000001
[ Sat May 18 22:47:15 2024 ] 	Batch(300/2353) done. Loss: 0.0216  lr:0.000001
[ Sat May 18 22:47:52 2024 ] 	Batch(400/2353) done. Loss: 0.0379  lr:0.000001
[ Sat May 18 22:48:29 2024 ] 	Batch(500/2353) done. Loss: 0.0326  lr:0.000001
[ Sat May 18 22:49:07 2024 ] 	Batch(600/2353) done. Loss: 0.0540  lr:0.000001
[ Sat May 18 22:49:44 2024 ] 	Batch(700/2353) done. Loss: 0.2587  lr:0.000001
[ Sat May 18 22:50:22 2024 ] 	Batch(800/2353) done. Loss: 0.0126  lr:0.000001
[ Sat May 18 22:50:58 2024 ] 	Batch(900/2353) done. Loss: 0.0175  lr:0.000001
[ Sat May 18 22:51:35 2024 ] 	Batch(1000/2353) done. Loss: 0.0357  lr:0.000001
[ Sat May 18 22:52:11 2024 ] 	Batch(1100/2353) done. Loss: 0.0123  lr:0.000001
[ Sat May 18 22:52:48 2024 ] 	Batch(1200/2353) done. Loss: 0.0179  lr:0.000001
[ Sat May 18 22:53:25 2024 ] 	Batch(1300/2353) done. Loss: 0.0595  lr:0.000001
[ Sat May 18 22:54:01 2024 ] 	Batch(1400/2353) done. Loss: 0.0968  lr:0.000001
[ Sat May 18 22:54:38 2024 ] 	Batch(1500/2353) done. Loss: 0.0025  lr:0.000001
[ Sat May 18 22:55:14 2024 ] 	Batch(1600/2353) done. Loss: 0.0541  lr:0.000001
[ Sat May 18 22:55:51 2024 ] 	Batch(1700/2353) done. Loss: 0.0253  lr:0.000001
[ Sat May 18 22:56:28 2024 ] 	Batch(1800/2353) done. Loss: 0.0859  lr:0.000001
[ Sat May 18 22:57:04 2024 ] 	Batch(1900/2353) done. Loss: 0.0548  lr:0.000001
[ Sat May 18 22:57:41 2024 ] 	Batch(2000/2353) done. Loss: 0.0074  lr:0.000001
[ Sat May 18 22:58:17 2024 ] 	Batch(2100/2353) done. Loss: 0.0531  lr:0.000001
[ Sat May 18 22:58:54 2024 ] 	Batch(2200/2353) done. Loss: 0.0116  lr:0.000001
[ Sat May 18 22:59:31 2024 ] 	Batch(2300/2353) done. Loss: 0.1104  lr:0.000001
[ Sat May 18 22:59:51 2024 ] 	Mean training loss: 0.0418.
[ Sat May 18 22:59:51 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 22:59:51 2024 ] Training epoch: 106
[ Sat May 18 22:59:51 2024 ] 	Batch(0/2353) done. Loss: 0.0332  lr:0.000001
[ Sat May 18 23:00:28 2024 ] 	Batch(100/2353) done. Loss: 0.0094  lr:0.000001
[ Sat May 18 23:01:05 2024 ] 	Batch(200/2353) done. Loss: 0.0492  lr:0.000001
[ Sat May 18 23:01:41 2024 ] 	Batch(300/2353) done. Loss: 0.0431  lr:0.000001
[ Sat May 18 23:02:18 2024 ] 	Batch(400/2353) done. Loss: 0.0039  lr:0.000001
[ Sat May 18 23:02:55 2024 ] 	Batch(500/2353) done. Loss: 0.0252  lr:0.000001
[ Sat May 18 23:03:32 2024 ] 	Batch(600/2353) done. Loss: 0.0469  lr:0.000001
[ Sat May 18 23:04:09 2024 ] 	Batch(700/2353) done. Loss: 0.1228  lr:0.000001
[ Sat May 18 23:04:45 2024 ] 	Batch(800/2353) done. Loss: 0.0223  lr:0.000001
[ Sat May 18 23:05:22 2024 ] 	Batch(900/2353) done. Loss: 0.0211  lr:0.000001
[ Sat May 18 23:05:59 2024 ] 	Batch(1000/2353) done. Loss: 0.0125  lr:0.000001
[ Sat May 18 23:06:35 2024 ] 	Batch(1100/2353) done. Loss: 0.0057  lr:0.000001
[ Sat May 18 23:07:12 2024 ] 	Batch(1200/2353) done. Loss: 0.0355  lr:0.000001
[ Sat May 18 23:07:49 2024 ] 	Batch(1300/2353) done. Loss: 0.0212  lr:0.000001
[ Sat May 18 23:08:25 2024 ] 	Batch(1400/2353) done. Loss: 0.1406  lr:0.000001
[ Sat May 18 23:09:02 2024 ] 	Batch(1500/2353) done. Loss: 0.0066  lr:0.000001
[ Sat May 18 23:09:39 2024 ] 	Batch(1600/2353) done. Loss: 0.0065  lr:0.000001
[ Sat May 18 23:10:15 2024 ] 	Batch(1700/2353) done. Loss: 0.1353  lr:0.000001
[ Sat May 18 23:10:52 2024 ] 	Batch(1800/2353) done. Loss: 0.0118  lr:0.000001
[ Sat May 18 23:11:28 2024 ] 	Batch(1900/2353) done. Loss: 0.1329  lr:0.000001
[ Sat May 18 23:12:05 2024 ] 	Batch(2000/2353) done. Loss: 0.0237  lr:0.000001
[ Sat May 18 23:12:42 2024 ] 	Batch(2100/2353) done. Loss: 0.0939  lr:0.000001
[ Sat May 18 23:13:18 2024 ] 	Batch(2200/2353) done. Loss: 0.1288  lr:0.000001
[ Sat May 18 23:13:55 2024 ] 	Batch(2300/2353) done. Loss: 0.0347  lr:0.000001
[ Sat May 18 23:14:15 2024 ] 	Mean training loss: 0.0417.
[ Sat May 18 23:14:15 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 23:14:15 2024 ] Training epoch: 107
[ Sat May 18 23:14:16 2024 ] 	Batch(0/2353) done. Loss: 0.0293  lr:0.000001
[ Sat May 18 23:14:53 2024 ] 	Batch(100/2353) done. Loss: 0.0487  lr:0.000001
[ Sat May 18 23:15:31 2024 ] 	Batch(200/2353) done. Loss: 0.1307  lr:0.000001
[ Sat May 18 23:16:08 2024 ] 	Batch(300/2353) done. Loss: 0.0057  lr:0.000001
[ Sat May 18 23:16:45 2024 ] 	Batch(400/2353) done. Loss: 0.0531  lr:0.000001
[ Sat May 18 23:17:23 2024 ] 	Batch(500/2353) done. Loss: 0.0126  lr:0.000001
[ Sat May 18 23:18:00 2024 ] 	Batch(600/2353) done. Loss: 0.1893  lr:0.000001
[ Sat May 18 23:18:36 2024 ] 	Batch(700/2353) done. Loss: 0.0121  lr:0.000001
[ Sat May 18 23:19:13 2024 ] 	Batch(800/2353) done. Loss: 0.1012  lr:0.000001
[ Sat May 18 23:19:50 2024 ] 	Batch(900/2353) done. Loss: 0.0136  lr:0.000001
[ Sat May 18 23:20:26 2024 ] 	Batch(1000/2353) done. Loss: 0.1302  lr:0.000001
[ Sat May 18 23:21:03 2024 ] 	Batch(1100/2353) done. Loss: 0.0574  lr:0.000001
[ Sat May 18 23:21:40 2024 ] 	Batch(1200/2353) done. Loss: 0.0733  lr:0.000001
[ Sat May 18 23:22:16 2024 ] 	Batch(1300/2353) done. Loss: 0.0109  lr:0.000001
[ Sat May 18 23:22:53 2024 ] 	Batch(1400/2353) done. Loss: 0.0192  lr:0.000001
[ Sat May 18 23:23:29 2024 ] 	Batch(1500/2353) done. Loss: 0.0362  lr:0.000001
[ Sat May 18 23:24:06 2024 ] 	Batch(1600/2353) done. Loss: 0.0686  lr:0.000001
[ Sat May 18 23:24:43 2024 ] 	Batch(1700/2353) done. Loss: 0.0146  lr:0.000001
[ Sat May 18 23:25:19 2024 ] 	Batch(1800/2353) done. Loss: 0.0049  lr:0.000001
[ Sat May 18 23:25:56 2024 ] 	Batch(1900/2353) done. Loss: 0.0179  lr:0.000001
[ Sat May 18 23:26:33 2024 ] 	Batch(2000/2353) done. Loss: 0.1854  lr:0.000001
[ Sat May 18 23:27:09 2024 ] 	Batch(2100/2353) done. Loss: 0.0504  lr:0.000001
[ Sat May 18 23:27:46 2024 ] 	Batch(2200/2353) done. Loss: 0.0019  lr:0.000001
[ Sat May 18 23:28:22 2024 ] 	Batch(2300/2353) done. Loss: 0.0459  lr:0.000001
[ Sat May 18 23:28:42 2024 ] 	Mean training loss: 0.0398.
[ Sat May 18 23:28:42 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sat May 18 23:28:42 2024 ] Training epoch: 108
[ Sat May 18 23:28:43 2024 ] 	Batch(0/2353) done. Loss: 0.0068  lr:0.000001
[ Sat May 18 23:29:20 2024 ] 	Batch(100/2353) done. Loss: 0.1205  lr:0.000001
[ Sat May 18 23:29:56 2024 ] 	Batch(200/2353) done. Loss: 0.0312  lr:0.000001
[ Sat May 18 23:30:33 2024 ] 	Batch(300/2353) done. Loss: 0.0616  lr:0.000001
[ Sat May 18 23:31:10 2024 ] 	Batch(400/2353) done. Loss: 0.0147  lr:0.000001
[ Sat May 18 23:31:46 2024 ] 	Batch(500/2353) done. Loss: 0.0263  lr:0.000001
[ Sat May 18 23:32:23 2024 ] 	Batch(600/2353) done. Loss: 0.0346  lr:0.000001
[ Sat May 18 23:33:00 2024 ] 	Batch(700/2353) done. Loss: 0.0099  lr:0.000001
[ Sat May 18 23:33:36 2024 ] 	Batch(800/2353) done. Loss: 0.0043  lr:0.000001
[ Sat May 18 23:34:13 2024 ] 	Batch(900/2353) done. Loss: 0.0272  lr:0.000001
[ Sat May 18 23:34:50 2024 ] 	Batch(1000/2353) done. Loss: 0.0235  lr:0.000001
[ Sat May 18 23:35:26 2024 ] 	Batch(1100/2353) done. Loss: 0.0149  lr:0.000001
[ Sat May 18 23:36:03 2024 ] 	Batch(1200/2353) done. Loss: 0.1777  lr:0.000001
[ Sat May 18 23:36:40 2024 ] 	Batch(1300/2353) done. Loss: 0.0255  lr:0.000001
[ Sat May 18 23:37:16 2024 ] 	Batch(1400/2353) done. Loss: 0.0063  lr:0.000001
[ Sat May 18 23:37:53 2024 ] 	Batch(1500/2353) done. Loss: 0.0688  lr:0.000001
[ Sat May 18 23:38:30 2024 ] 	Batch(1600/2353) done. Loss: 0.0645  lr:0.000001
[ Sat May 18 23:39:06 2024 ] 	Batch(1700/2353) done. Loss: 0.0026  lr:0.000001
[ Sat May 18 23:39:43 2024 ] 	Batch(1800/2353) done. Loss: 0.0183  lr:0.000001
[ Sat May 18 23:40:20 2024 ] 	Batch(1900/2353) done. Loss: 0.0267  lr:0.000001
[ Sat May 18 23:40:56 2024 ] 	Batch(2000/2353) done. Loss: 0.0244  lr:0.000001
[ Sat May 18 23:41:33 2024 ] 	Batch(2100/2353) done. Loss: 0.0023  lr:0.000001
[ Sat May 18 23:42:10 2024 ] 	Batch(2200/2353) done. Loss: 0.0215  lr:0.000001
[ Sat May 18 23:42:46 2024 ] 	Batch(2300/2353) done. Loss: 0.0331  lr:0.000001
[ Sat May 18 23:43:06 2024 ] 	Mean training loss: 0.0435.
[ Sat May 18 23:43:06 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 23:43:06 2024 ] Training epoch: 109
[ Sat May 18 23:43:07 2024 ] 	Batch(0/2353) done. Loss: 0.0206  lr:0.000001
[ Sat May 18 23:43:43 2024 ] 	Batch(100/2353) done. Loss: 0.0266  lr:0.000001
[ Sat May 18 23:44:20 2024 ] 	Batch(200/2353) done. Loss: 0.0559  lr:0.000001
[ Sat May 18 23:44:57 2024 ] 	Batch(300/2353) done. Loss: 0.2219  lr:0.000001
[ Sat May 18 23:45:33 2024 ] 	Batch(400/2353) done. Loss: 0.0120  lr:0.000001
[ Sat May 18 23:46:10 2024 ] 	Batch(500/2353) done. Loss: 0.0518  lr:0.000001
[ Sat May 18 23:46:47 2024 ] 	Batch(600/2353) done. Loss: 0.0501  lr:0.000001
[ Sat May 18 23:47:23 2024 ] 	Batch(700/2353) done. Loss: 0.0558  lr:0.000001
[ Sat May 18 23:48:00 2024 ] 	Batch(800/2353) done. Loss: 0.0190  lr:0.000001
[ Sat May 18 23:48:36 2024 ] 	Batch(900/2353) done. Loss: 0.0441  lr:0.000001
[ Sat May 18 23:49:13 2024 ] 	Batch(1000/2353) done. Loss: 0.1201  lr:0.000001
[ Sat May 18 23:49:50 2024 ] 	Batch(1100/2353) done. Loss: 0.0381  lr:0.000001
[ Sat May 18 23:50:26 2024 ] 	Batch(1200/2353) done. Loss: 0.0148  lr:0.000001
[ Sat May 18 23:51:03 2024 ] 	Batch(1300/2353) done. Loss: 0.0140  lr:0.000001
[ Sat May 18 23:51:40 2024 ] 	Batch(1400/2353) done. Loss: 0.0638  lr:0.000001
[ Sat May 18 23:52:16 2024 ] 	Batch(1500/2353) done. Loss: 0.0488  lr:0.000001
[ Sat May 18 23:52:53 2024 ] 	Batch(1600/2353) done. Loss: 0.1214  lr:0.000001
[ Sat May 18 23:53:30 2024 ] 	Batch(1700/2353) done. Loss: 0.0402  lr:0.000001
[ Sat May 18 23:54:06 2024 ] 	Batch(1800/2353) done. Loss: 0.0377  lr:0.000001
[ Sat May 18 23:54:43 2024 ] 	Batch(1900/2353) done. Loss: 0.0146  lr:0.000001
[ Sat May 18 23:55:20 2024 ] 	Batch(2000/2353) done. Loss: 0.1127  lr:0.000001
[ Sat May 18 23:55:56 2024 ] 	Batch(2100/2353) done. Loss: 0.0078  lr:0.000001
[ Sat May 18 23:56:33 2024 ] 	Batch(2200/2353) done. Loss: 0.0282  lr:0.000001
[ Sat May 18 23:57:09 2024 ] 	Batch(2300/2353) done. Loss: 0.0287  lr:0.000001
[ Sat May 18 23:57:29 2024 ] 	Mean training loss: 0.0416.
[ Sat May 18 23:57:29 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sat May 18 23:57:30 2024 ] Training epoch: 110
[ Sat May 18 23:57:30 2024 ] 	Batch(0/2353) done. Loss: 0.0368  lr:0.000001
[ Sat May 18 23:58:07 2024 ] 	Batch(100/2353) done. Loss: 0.0176  lr:0.000001
[ Sat May 18 23:58:43 2024 ] 	Batch(200/2353) done. Loss: 0.0180  lr:0.000001
[ Sat May 18 23:59:20 2024 ] 	Batch(300/2353) done. Loss: 0.0097  lr:0.000001
[ Sat May 18 23:59:57 2024 ] 	Batch(400/2353) done. Loss: 0.0127  lr:0.000001
[ Sun May 19 00:00:33 2024 ] 	Batch(500/2353) done. Loss: 0.0497  lr:0.000001
[ Sun May 19 00:01:10 2024 ] 	Batch(600/2353) done. Loss: 0.0316  lr:0.000001
[ Sun May 19 00:01:46 2024 ] 	Batch(700/2353) done. Loss: 0.1570  lr:0.000001
[ Sun May 19 00:02:23 2024 ] 	Batch(800/2353) done. Loss: 0.0136  lr:0.000001
[ Sun May 19 00:03:00 2024 ] 	Batch(900/2353) done. Loss: 0.0023  lr:0.000001
[ Sun May 19 00:03:36 2024 ] 	Batch(1000/2353) done. Loss: 0.0326  lr:0.000001
[ Sun May 19 00:04:13 2024 ] 	Batch(1100/2353) done. Loss: 0.0091  lr:0.000001
[ Sun May 19 00:04:50 2024 ] 	Batch(1200/2353) done. Loss: 0.0192  lr:0.000001
[ Sun May 19 00:05:26 2024 ] 	Batch(1300/2353) done. Loss: 0.0223  lr:0.000001
[ Sun May 19 00:06:03 2024 ] 	Batch(1400/2353) done. Loss: 0.0066  lr:0.000001
[ Sun May 19 00:06:39 2024 ] 	Batch(1500/2353) done. Loss: 0.0165  lr:0.000001
[ Sun May 19 00:07:16 2024 ] 	Batch(1600/2353) done. Loss: 0.0270  lr:0.000001
[ Sun May 19 00:07:53 2024 ] 	Batch(1700/2353) done. Loss: 0.0177  lr:0.000001
[ Sun May 19 00:08:30 2024 ] 	Batch(1800/2353) done. Loss: 0.0082  lr:0.000001
[ Sun May 19 00:09:06 2024 ] 	Batch(1900/2353) done. Loss: 0.0271  lr:0.000001
[ Sun May 19 00:09:43 2024 ] 	Batch(2000/2353) done. Loss: 0.0085  lr:0.000001
[ Sun May 19 00:10:20 2024 ] 	Batch(2100/2353) done. Loss: 0.0795  lr:0.000001
[ Sun May 19 00:10:57 2024 ] 	Batch(2200/2353) done. Loss: 0.0231  lr:0.000001
[ Sun May 19 00:11:33 2024 ] 	Batch(2300/2353) done. Loss: 0.0230  lr:0.000001
[ Sun May 19 00:11:53 2024 ] 	Mean training loss: 0.0407.
[ Sun May 19 00:11:53 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 00:11:54 2024 ] Eval epoch: 110
[ Sun May 19 00:13:57 2024 ] 	Mean val loss of 2367 batches: 0.24476032599614944.
[ Sun May 19 00:13:57 2024 ] Training epoch: 111
[ Sun May 19 00:13:58 2024 ] 	Batch(0/2353) done. Loss: 0.1289  lr:0.000001
[ Sun May 19 00:14:35 2024 ] 	Batch(100/2353) done. Loss: 0.0229  lr:0.000001
[ Sun May 19 00:15:12 2024 ] 	Batch(200/2353) done. Loss: 0.0071  lr:0.000001
[ Sun May 19 00:15:48 2024 ] 	Batch(300/2353) done. Loss: 0.0253  lr:0.000001
[ Sun May 19 00:16:25 2024 ] 	Batch(400/2353) done. Loss: 0.0671  lr:0.000001
[ Sun May 19 00:17:02 2024 ] 	Batch(500/2353) done. Loss: 0.0237  lr:0.000001
[ Sun May 19 00:17:39 2024 ] 	Batch(600/2353) done. Loss: 0.0637  lr:0.000001
[ Sun May 19 00:18:15 2024 ] 	Batch(700/2353) done. Loss: 0.0026  lr:0.000001
[ Sun May 19 00:18:52 2024 ] 	Batch(800/2353) done. Loss: 0.0723  lr:0.000001
[ Sun May 19 00:19:29 2024 ] 	Batch(900/2353) done. Loss: 0.0631  lr:0.000001
[ Sun May 19 00:20:06 2024 ] 	Batch(1000/2353) done. Loss: 0.0029  lr:0.000001
[ Sun May 19 00:20:42 2024 ] 	Batch(1100/2353) done. Loss: 0.0687  lr:0.000001
[ Sun May 19 00:21:19 2024 ] 	Batch(1200/2353) done. Loss: 0.0150  lr:0.000001
[ Sun May 19 00:21:55 2024 ] 	Batch(1300/2353) done. Loss: 0.0418  lr:0.000001
[ Sun May 19 00:22:32 2024 ] 	Batch(1400/2353) done. Loss: 0.0019  lr:0.000001
[ Sun May 19 00:23:09 2024 ] 	Batch(1500/2353) done. Loss: 0.0029  lr:0.000001
[ Sun May 19 00:23:45 2024 ] 	Batch(1600/2353) done. Loss: 0.0160  lr:0.000001
[ Sun May 19 00:24:22 2024 ] 	Batch(1700/2353) done. Loss: 0.0075  lr:0.000001
[ Sun May 19 00:24:58 2024 ] 	Batch(1800/2353) done. Loss: 0.0045  lr:0.000001
[ Sun May 19 00:25:35 2024 ] 	Batch(1900/2353) done. Loss: 0.0086  lr:0.000001
[ Sun May 19 00:26:12 2024 ] 	Batch(2000/2353) done. Loss: 0.0265  lr:0.000001
[ Sun May 19 00:26:49 2024 ] 	Batch(2100/2353) done. Loss: 0.0276  lr:0.000001
[ Sun May 19 00:27:26 2024 ] 	Batch(2200/2353) done. Loss: 0.0328  lr:0.000001
[ Sun May 19 00:28:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0057  lr:0.000001
[ Sun May 19 00:28:23 2024 ] 	Mean training loss: 0.0385.
[ Sun May 19 00:28:23 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 00:28:24 2024 ] Training epoch: 112
[ Sun May 19 00:28:24 2024 ] 	Batch(0/2353) done. Loss: 0.0213  lr:0.000001
[ Sun May 19 00:29:01 2024 ] 	Batch(100/2353) done. Loss: 0.0308  lr:0.000001
[ Sun May 19 00:29:37 2024 ] 	Batch(200/2353) done. Loss: 0.1541  lr:0.000001
[ Sun May 19 00:30:14 2024 ] 	Batch(300/2353) done. Loss: 0.0347  lr:0.000001
[ Sun May 19 00:30:51 2024 ] 	Batch(400/2353) done. Loss: 0.0078  lr:0.000001
[ Sun May 19 00:31:27 2024 ] 	Batch(500/2353) done. Loss: 0.0074  lr:0.000001
[ Sun May 19 00:32:04 2024 ] 	Batch(600/2353) done. Loss: 0.0243  lr:0.000001
[ Sun May 19 00:32:40 2024 ] 	Batch(700/2353) done. Loss: 0.0060  lr:0.000001
[ Sun May 19 00:33:17 2024 ] 	Batch(800/2353) done. Loss: 0.0263  lr:0.000001
[ Sun May 19 00:33:54 2024 ] 	Batch(900/2353) done. Loss: 0.0206  lr:0.000001
[ Sun May 19 00:34:30 2024 ] 	Batch(1000/2353) done. Loss: 0.0301  lr:0.000001
[ Sun May 19 00:35:07 2024 ] 	Batch(1100/2353) done. Loss: 0.0215  lr:0.000001
[ Sun May 19 00:35:44 2024 ] 	Batch(1200/2353) done. Loss: 0.0401  lr:0.000001
[ Sun May 19 00:36:20 2024 ] 	Batch(1300/2353) done. Loss: 0.0099  lr:0.000001
[ Sun May 19 00:36:57 2024 ] 	Batch(1400/2353) done. Loss: 0.0314  lr:0.000001
[ Sun May 19 00:37:33 2024 ] 	Batch(1500/2353) done. Loss: 0.0685  lr:0.000001
[ Sun May 19 00:38:10 2024 ] 	Batch(1600/2353) done. Loss: 0.0110  lr:0.000001
[ Sun May 19 00:38:46 2024 ] 	Batch(1700/2353) done. Loss: 0.0055  lr:0.000001
[ Sun May 19 00:39:23 2024 ] 	Batch(1800/2353) done. Loss: 0.0110  lr:0.000001
[ Sun May 19 00:40:00 2024 ] 	Batch(1900/2353) done. Loss: 0.1023  lr:0.000001
[ Sun May 19 00:40:36 2024 ] 	Batch(2000/2353) done. Loss: 0.0089  lr:0.000001
[ Sun May 19 00:41:13 2024 ] 	Batch(2100/2353) done. Loss: 0.0248  lr:0.000001
[ Sun May 19 00:41:50 2024 ] 	Batch(2200/2353) done. Loss: 0.0544  lr:0.000001
[ Sun May 19 00:42:26 2024 ] 	Batch(2300/2353) done. Loss: 0.0184  lr:0.000001
[ Sun May 19 00:42:46 2024 ] 	Mean training loss: 0.0432.
[ Sun May 19 00:42:46 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 00:42:46 2024 ] Training epoch: 113
[ Sun May 19 00:42:47 2024 ] 	Batch(0/2353) done. Loss: 0.1275  lr:0.000001
[ Sun May 19 00:43:23 2024 ] 	Batch(100/2353) done. Loss: 0.0469  lr:0.000001
[ Sun May 19 00:44:00 2024 ] 	Batch(200/2353) done. Loss: 0.0387  lr:0.000001
[ Sun May 19 00:44:36 2024 ] 	Batch(300/2353) done. Loss: 0.0025  lr:0.000001
[ Sun May 19 00:45:14 2024 ] 	Batch(400/2353) done. Loss: 0.0052  lr:0.000001
[ Sun May 19 00:45:51 2024 ] 	Batch(500/2353) done. Loss: 0.0370  lr:0.000001
[ Sun May 19 00:46:27 2024 ] 	Batch(600/2353) done. Loss: 0.0313  lr:0.000001
[ Sun May 19 00:47:04 2024 ] 	Batch(700/2353) done. Loss: 0.0396  lr:0.000001
[ Sun May 19 00:47:41 2024 ] 	Batch(800/2353) done. Loss: 0.0139  lr:0.000001
[ Sun May 19 00:48:17 2024 ] 	Batch(900/2353) done. Loss: 0.0719  lr:0.000001
[ Sun May 19 00:48:54 2024 ] 	Batch(1000/2353) done. Loss: 0.0383  lr:0.000001
[ Sun May 19 00:49:31 2024 ] 	Batch(1100/2353) done. Loss: 0.0203  lr:0.000001
[ Sun May 19 00:50:07 2024 ] 	Batch(1200/2353) done. Loss: 0.0074  lr:0.000001
[ Sun May 19 00:50:44 2024 ] 	Batch(1300/2353) done. Loss: 0.1133  lr:0.000001
[ Sun May 19 00:51:21 2024 ] 	Batch(1400/2353) done. Loss: 0.0482  lr:0.000001
[ Sun May 19 00:51:57 2024 ] 	Batch(1500/2353) done. Loss: 0.0405  lr:0.000001
[ Sun May 19 00:52:34 2024 ] 	Batch(1600/2353) done. Loss: 0.0071  lr:0.000001
[ Sun May 19 00:53:11 2024 ] 	Batch(1700/2353) done. Loss: 0.0057  lr:0.000001
[ Sun May 19 00:53:47 2024 ] 	Batch(1800/2353) done. Loss: 0.0153  lr:0.000001
[ Sun May 19 00:54:24 2024 ] 	Batch(1900/2353) done. Loss: 0.0294  lr:0.000001
[ Sun May 19 00:55:00 2024 ] 	Batch(2000/2353) done. Loss: 0.0035  lr:0.000001
[ Sun May 19 00:55:37 2024 ] 	Batch(2100/2353) done. Loss: 0.0014  lr:0.000001
[ Sun May 19 00:56:14 2024 ] 	Batch(2200/2353) done. Loss: 0.0093  lr:0.000001
[ Sun May 19 00:56:50 2024 ] 	Batch(2300/2353) done. Loss: 0.0246  lr:0.000001
[ Sun May 19 00:57:10 2024 ] 	Mean training loss: 0.0410.
[ Sun May 19 00:57:10 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 00:57:11 2024 ] Training epoch: 114
[ Sun May 19 00:57:11 2024 ] 	Batch(0/2353) done. Loss: 0.0334  lr:0.000001
[ Sun May 19 00:57:48 2024 ] 	Batch(100/2353) done. Loss: 0.1445  lr:0.000001
[ Sun May 19 00:58:24 2024 ] 	Batch(200/2353) done. Loss: 0.0058  lr:0.000001
[ Sun May 19 00:59:01 2024 ] 	Batch(300/2353) done. Loss: 0.0104  lr:0.000001
[ Sun May 19 00:59:38 2024 ] 	Batch(400/2353) done. Loss: 0.0047  lr:0.000001
[ Sun May 19 01:00:14 2024 ] 	Batch(500/2353) done. Loss: 0.1362  lr:0.000001
[ Sun May 19 01:00:51 2024 ] 	Batch(600/2353) done. Loss: 0.0027  lr:0.000001
[ Sun May 19 01:01:28 2024 ] 	Batch(700/2353) done. Loss: 0.0265  lr:0.000001
[ Sun May 19 01:02:04 2024 ] 	Batch(800/2353) done. Loss: 0.0668  lr:0.000001
[ Sun May 19 01:02:41 2024 ] 	Batch(900/2353) done. Loss: 0.0130  lr:0.000001
[ Sun May 19 01:03:18 2024 ] 	Batch(1000/2353) done. Loss: 0.0556  lr:0.000001
[ Sun May 19 01:03:55 2024 ] 	Batch(1100/2353) done. Loss: 0.0086  lr:0.000001
[ Sun May 19 01:04:32 2024 ] 	Batch(1200/2353) done. Loss: 0.0172  lr:0.000001
[ Sun May 19 01:05:08 2024 ] 	Batch(1300/2353) done. Loss: 0.0837  lr:0.000001
[ Sun May 19 01:05:45 2024 ] 	Batch(1400/2353) done. Loss: 0.0176  lr:0.000001
[ Sun May 19 01:06:21 2024 ] 	Batch(1500/2353) done. Loss: 0.0342  lr:0.000001
[ Sun May 19 01:06:58 2024 ] 	Batch(1600/2353) done. Loss: 0.0290  lr:0.000001
[ Sun May 19 01:07:35 2024 ] 	Batch(1700/2353) done. Loss: 0.0250  lr:0.000001
[ Sun May 19 01:08:12 2024 ] 	Batch(1800/2353) done. Loss: 0.0164  lr:0.000001
[ Sun May 19 01:08:48 2024 ] 	Batch(1900/2353) done. Loss: 0.1509  lr:0.000001
[ Sun May 19 01:09:25 2024 ] 	Batch(2000/2353) done. Loss: 0.0217  lr:0.000001
[ Sun May 19 01:10:02 2024 ] 	Batch(2100/2353) done. Loss: 0.0093  lr:0.000001
[ Sun May 19 01:10:38 2024 ] 	Batch(2200/2353) done. Loss: 0.0548  lr:0.000001
[ Sun May 19 01:11:15 2024 ] 	Batch(2300/2353) done. Loss: 0.0513  lr:0.000001
[ Sun May 19 01:11:35 2024 ] 	Mean training loss: 0.0402.
[ Sun May 19 01:11:35 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 01:11:35 2024 ] Training epoch: 115
[ Sun May 19 01:11:36 2024 ] 	Batch(0/2353) done. Loss: 0.0428  lr:0.000001
[ Sun May 19 01:12:13 2024 ] 	Batch(100/2353) done. Loss: 0.0256  lr:0.000001
[ Sun May 19 01:12:49 2024 ] 	Batch(200/2353) done. Loss: 0.0373  lr:0.000001
[ Sun May 19 01:13:26 2024 ] 	Batch(300/2353) done. Loss: 0.0182  lr:0.000001
[ Sun May 19 01:14:02 2024 ] 	Batch(400/2353) done. Loss: 0.0138  lr:0.000001
[ Sun May 19 01:14:39 2024 ] 	Batch(500/2353) done. Loss: 0.0288  lr:0.000001
[ Sun May 19 01:15:16 2024 ] 	Batch(600/2353) done. Loss: 0.0861  lr:0.000001
[ Sun May 19 01:15:52 2024 ] 	Batch(700/2353) done. Loss: 0.0075  lr:0.000001
[ Sun May 19 01:16:29 2024 ] 	Batch(800/2353) done. Loss: 0.0155  lr:0.000001
[ Sun May 19 01:17:06 2024 ] 	Batch(900/2353) done. Loss: 0.2012  lr:0.000001
[ Sun May 19 01:17:42 2024 ] 	Batch(1000/2353) done. Loss: 0.0189  lr:0.000001
[ Sun May 19 01:18:19 2024 ] 	Batch(1100/2353) done. Loss: 0.0317  lr:0.000001
[ Sun May 19 01:18:56 2024 ] 	Batch(1200/2353) done. Loss: 0.0089  lr:0.000001
[ Sun May 19 01:19:32 2024 ] 	Batch(1300/2353) done. Loss: 0.0087  lr:0.000001
[ Sun May 19 01:20:09 2024 ] 	Batch(1400/2353) done. Loss: 0.0025  lr:0.000001
[ Sun May 19 01:20:46 2024 ] 	Batch(1500/2353) done. Loss: 0.0243  lr:0.000001
[ Sun May 19 01:21:23 2024 ] 	Batch(1600/2353) done. Loss: 0.0338  lr:0.000001
[ Sun May 19 01:21:59 2024 ] 	Batch(1700/2353) done. Loss: 0.0232  lr:0.000001
[ Sun May 19 01:22:36 2024 ] 	Batch(1800/2353) done. Loss: 0.0144  lr:0.000001
[ Sun May 19 01:23:12 2024 ] 	Batch(1900/2353) done. Loss: 0.0406  lr:0.000001
[ Sun May 19 01:23:49 2024 ] 	Batch(2000/2353) done. Loss: 0.0115  lr:0.000001
[ Sun May 19 01:24:26 2024 ] 	Batch(2100/2353) done. Loss: 0.0051  lr:0.000001
[ Sun May 19 01:25:02 2024 ] 	Batch(2200/2353) done. Loss: 0.0176  lr:0.000001
[ Sun May 19 01:25:39 2024 ] 	Batch(2300/2353) done. Loss: 0.0018  lr:0.000001
[ Sun May 19 01:25:59 2024 ] 	Mean training loss: 0.0431.
[ Sun May 19 01:25:59 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 01:25:59 2024 ] Training epoch: 116
[ Sun May 19 01:25:59 2024 ] 	Batch(0/2353) done. Loss: 0.0194  lr:0.000001
[ Sun May 19 01:26:36 2024 ] 	Batch(100/2353) done. Loss: 0.0242  lr:0.000001
[ Sun May 19 01:27:13 2024 ] 	Batch(200/2353) done. Loss: 0.0492  lr:0.000001
[ Sun May 19 01:27:49 2024 ] 	Batch(300/2353) done. Loss: 0.0857  lr:0.000001
[ Sun May 19 01:28:26 2024 ] 	Batch(400/2353) done. Loss: 0.0070  lr:0.000001
[ Sun May 19 01:29:02 2024 ] 	Batch(500/2353) done. Loss: 0.0132  lr:0.000001
[ Sun May 19 01:29:39 2024 ] 	Batch(600/2353) done. Loss: 0.0392  lr:0.000001
[ Sun May 19 01:30:16 2024 ] 	Batch(700/2353) done. Loss: 0.0041  lr:0.000001
[ Sun May 19 01:30:52 2024 ] 	Batch(800/2353) done. Loss: 0.0863  lr:0.000001
[ Sun May 19 01:31:29 2024 ] 	Batch(900/2353) done. Loss: 0.0536  lr:0.000001
[ Sun May 19 01:32:06 2024 ] 	Batch(1000/2353) done. Loss: 0.0098  lr:0.000001
[ Sun May 19 01:32:42 2024 ] 	Batch(1100/2353) done. Loss: 0.0951  lr:0.000001
[ Sun May 19 01:33:19 2024 ] 	Batch(1200/2353) done. Loss: 0.0243  lr:0.000001
[ Sun May 19 01:33:56 2024 ] 	Batch(1300/2353) done. Loss: 0.1238  lr:0.000001
[ Sun May 19 01:34:32 2024 ] 	Batch(1400/2353) done. Loss: 0.0933  lr:0.000001
[ Sun May 19 01:35:09 2024 ] 	Batch(1500/2353) done. Loss: 0.0368  lr:0.000001
[ Sun May 19 01:35:46 2024 ] 	Batch(1600/2353) done. Loss: 0.0075  lr:0.000001
[ Sun May 19 01:36:22 2024 ] 	Batch(1700/2353) done. Loss: 0.0330  lr:0.000001
[ Sun May 19 01:36:59 2024 ] 	Batch(1800/2353) done. Loss: 0.1350  lr:0.000001
[ Sun May 19 01:37:36 2024 ] 	Batch(1900/2353) done. Loss: 0.0392  lr:0.000001
[ Sun May 19 01:38:13 2024 ] 	Batch(2000/2353) done. Loss: 0.0439  lr:0.000001
[ Sun May 19 01:38:49 2024 ] 	Batch(2100/2353) done. Loss: 0.0224  lr:0.000001
[ Sun May 19 01:39:26 2024 ] 	Batch(2200/2353) done. Loss: 0.0236  lr:0.000001
[ Sun May 19 01:40:03 2024 ] 	Batch(2300/2353) done. Loss: 0.0130  lr:0.000001
[ Sun May 19 01:40:23 2024 ] 	Mean training loss: 0.0412.
[ Sun May 19 01:40:23 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 01:40:23 2024 ] Training epoch: 117
[ Sun May 19 01:40:23 2024 ] 	Batch(0/2353) done. Loss: 0.0686  lr:0.000001
[ Sun May 19 01:41:01 2024 ] 	Batch(100/2353) done. Loss: 0.0065  lr:0.000001
[ Sun May 19 01:41:38 2024 ] 	Batch(200/2353) done. Loss: 0.0288  lr:0.000001
[ Sun May 19 01:42:15 2024 ] 	Batch(300/2353) done. Loss: 0.0194  lr:0.000001
[ Sun May 19 01:42:52 2024 ] 	Batch(400/2353) done. Loss: 0.0967  lr:0.000001
[ Sun May 19 01:43:28 2024 ] 	Batch(500/2353) done. Loss: 0.0691  lr:0.000001
[ Sun May 19 01:44:05 2024 ] 	Batch(600/2353) done. Loss: 0.0069  lr:0.000001
[ Sun May 19 01:44:42 2024 ] 	Batch(700/2353) done. Loss: 0.0226  lr:0.000001
[ Sun May 19 01:45:18 2024 ] 	Batch(800/2353) done. Loss: 0.0049  lr:0.000001
[ Sun May 19 01:45:55 2024 ] 	Batch(900/2353) done. Loss: 0.0082  lr:0.000001
[ Sun May 19 01:46:32 2024 ] 	Batch(1000/2353) done. Loss: 0.0161  lr:0.000001
[ Sun May 19 01:47:08 2024 ] 	Batch(1100/2353) done. Loss: 0.0225  lr:0.000001
[ Sun May 19 01:47:45 2024 ] 	Batch(1200/2353) done. Loss: 0.0211  lr:0.000001
[ Sun May 19 01:48:22 2024 ] 	Batch(1300/2353) done. Loss: 0.0140  lr:0.000001
[ Sun May 19 01:48:58 2024 ] 	Batch(1400/2353) done. Loss: 0.0459  lr:0.000001
[ Sun May 19 01:49:35 2024 ] 	Batch(1500/2353) done. Loss: 0.0546  lr:0.000001
[ Sun May 19 01:50:12 2024 ] 	Batch(1600/2353) done. Loss: 0.0259  lr:0.000001
[ Sun May 19 01:50:48 2024 ] 	Batch(1700/2353) done. Loss: 0.0203  lr:0.000001
[ Sun May 19 01:51:25 2024 ] 	Batch(1800/2353) done. Loss: 0.0828  lr:0.000001
[ Sun May 19 01:52:01 2024 ] 	Batch(1900/2353) done. Loss: 0.0112  lr:0.000001
[ Sun May 19 01:52:38 2024 ] 	Batch(2000/2353) done. Loss: 0.0409  lr:0.000001
[ Sun May 19 01:53:15 2024 ] 	Batch(2100/2353) done. Loss: 0.0753  lr:0.000001
[ Sun May 19 01:53:51 2024 ] 	Batch(2200/2353) done. Loss: 0.0342  lr:0.000001
[ Sun May 19 01:54:28 2024 ] 	Batch(2300/2353) done. Loss: 0.0085  lr:0.000001
[ Sun May 19 01:54:48 2024 ] 	Mean training loss: 0.0408.
[ Sun May 19 01:54:48 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 01:54:48 2024 ] Training epoch: 118
[ Sun May 19 01:54:49 2024 ] 	Batch(0/2353) done. Loss: 0.0177  lr:0.000001
[ Sun May 19 01:55:25 2024 ] 	Batch(100/2353) done. Loss: 0.0288  lr:0.000001
[ Sun May 19 01:56:02 2024 ] 	Batch(200/2353) done. Loss: 0.0639  lr:0.000001
[ Sun May 19 01:56:39 2024 ] 	Batch(300/2353) done. Loss: 0.0139  lr:0.000001
[ Sun May 19 01:57:15 2024 ] 	Batch(400/2353) done. Loss: 0.0052  lr:0.000001
[ Sun May 19 01:57:52 2024 ] 	Batch(500/2353) done. Loss: 0.0524  lr:0.000001
[ Sun May 19 01:58:29 2024 ] 	Batch(600/2353) done. Loss: 0.0064  lr:0.000001
[ Sun May 19 01:59:05 2024 ] 	Batch(700/2353) done. Loss: 0.0076  lr:0.000001
[ Sun May 19 01:59:42 2024 ] 	Batch(800/2353) done. Loss: 0.0138  lr:0.000001
[ Sun May 19 02:00:19 2024 ] 	Batch(900/2353) done. Loss: 0.1934  lr:0.000001
[ Sun May 19 02:00:55 2024 ] 	Batch(1000/2353) done. Loss: 0.0097  lr:0.000001
[ Sun May 19 02:01:32 2024 ] 	Batch(1100/2353) done. Loss: 0.0228  lr:0.000001
[ Sun May 19 02:02:09 2024 ] 	Batch(1200/2353) done. Loss: 0.0050  lr:0.000001
[ Sun May 19 02:02:45 2024 ] 	Batch(1300/2353) done. Loss: 0.0112  lr:0.000001
[ Sun May 19 02:03:22 2024 ] 	Batch(1400/2353) done. Loss: 0.0044  lr:0.000001
[ Sun May 19 02:03:59 2024 ] 	Batch(1500/2353) done. Loss: 0.1090  lr:0.000001
[ Sun May 19 02:04:35 2024 ] 	Batch(1600/2353) done. Loss: 0.0579  lr:0.000001
[ Sun May 19 02:05:12 2024 ] 	Batch(1700/2353) done. Loss: 0.0419  lr:0.000001
[ Sun May 19 02:05:49 2024 ] 	Batch(1800/2353) done. Loss: 0.0870  lr:0.000001
[ Sun May 19 02:06:25 2024 ] 	Batch(1900/2353) done. Loss: 0.0194  lr:0.000001
[ Sun May 19 02:07:02 2024 ] 	Batch(2000/2353) done. Loss: 0.0333  lr:0.000001
[ Sun May 19 02:07:39 2024 ] 	Batch(2100/2353) done. Loss: 0.0083  lr:0.000001
[ Sun May 19 02:08:15 2024 ] 	Batch(2200/2353) done. Loss: 0.0117  lr:0.000001
[ Sun May 19 02:08:52 2024 ] 	Batch(2300/2353) done. Loss: 0.1391  lr:0.000001
[ Sun May 19 02:09:12 2024 ] 	Mean training loss: 0.0417.
[ Sun May 19 02:09:12 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 02:09:12 2024 ] Training epoch: 119
[ Sun May 19 02:09:13 2024 ] 	Batch(0/2353) done. Loss: 0.0399  lr:0.000001
[ Sun May 19 02:09:49 2024 ] 	Batch(100/2353) done. Loss: 0.0082  lr:0.000001
[ Sun May 19 02:10:26 2024 ] 	Batch(200/2353) done. Loss: 0.0960  lr:0.000001
[ Sun May 19 02:11:03 2024 ] 	Batch(300/2353) done. Loss: 0.2407  lr:0.000001
[ Sun May 19 02:11:40 2024 ] 	Batch(400/2353) done. Loss: 0.0405  lr:0.000001
[ Sun May 19 02:12:16 2024 ] 	Batch(500/2353) done. Loss: 0.0717  lr:0.000001
[ Sun May 19 02:12:53 2024 ] 	Batch(600/2353) done. Loss: 0.0043  lr:0.000001
[ Sun May 19 02:13:30 2024 ] 	Batch(700/2353) done. Loss: 0.0389  lr:0.000001
[ Sun May 19 02:14:07 2024 ] 	Batch(800/2353) done. Loss: 0.1094  lr:0.000001
[ Sun May 19 02:14:44 2024 ] 	Batch(900/2353) done. Loss: 0.0399  lr:0.000001
[ Sun May 19 02:15:21 2024 ] 	Batch(1000/2353) done. Loss: 0.0135  lr:0.000001
[ Sun May 19 02:15:57 2024 ] 	Batch(1100/2353) done. Loss: 0.0122  lr:0.000001
[ Sun May 19 02:16:34 2024 ] 	Batch(1200/2353) done. Loss: 0.0328  lr:0.000001
[ Sun May 19 02:17:11 2024 ] 	Batch(1300/2353) done. Loss: 0.0569  lr:0.000001
[ Sun May 19 02:17:47 2024 ] 	Batch(1400/2353) done. Loss: 0.0126  lr:0.000001
[ Sun May 19 02:18:24 2024 ] 	Batch(1500/2353) done. Loss: 0.0762  lr:0.000001
[ Sun May 19 02:19:00 2024 ] 	Batch(1600/2353) done. Loss: 0.0460  lr:0.000001
[ Sun May 19 02:19:37 2024 ] 	Batch(1700/2353) done. Loss: 0.0570  lr:0.000001
[ Sun May 19 02:20:14 2024 ] 	Batch(1800/2353) done. Loss: 0.0368  lr:0.000001
[ Sun May 19 02:20:50 2024 ] 	Batch(1900/2353) done. Loss: 0.0087  lr:0.000001
[ Sun May 19 02:21:27 2024 ] 	Batch(2000/2353) done. Loss: 0.0590  lr:0.000001
[ Sun May 19 02:22:03 2024 ] 	Batch(2100/2353) done. Loss: 0.0279  lr:0.000001
[ Sun May 19 02:22:40 2024 ] 	Batch(2200/2353) done. Loss: 0.0145  lr:0.000001
[ Sun May 19 02:23:17 2024 ] 	Batch(2300/2353) done. Loss: 0.0127  lr:0.000001
[ Sun May 19 02:23:36 2024 ] 	Mean training loss: 0.0391.
[ Sun May 19 02:23:36 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 02:23:37 2024 ] Training epoch: 120
[ Sun May 19 02:23:37 2024 ] 	Batch(0/2353) done. Loss: 0.0202  lr:0.000001
[ Sun May 19 02:24:14 2024 ] 	Batch(100/2353) done. Loss: 0.0223  lr:0.000001
[ Sun May 19 02:24:50 2024 ] 	Batch(200/2353) done. Loss: 0.1167  lr:0.000001
[ Sun May 19 02:25:27 2024 ] 	Batch(300/2353) done. Loss: 0.0042  lr:0.000001
[ Sun May 19 02:26:03 2024 ] 	Batch(400/2353) done. Loss: 0.0100  lr:0.000001
[ Sun May 19 02:26:40 2024 ] 	Batch(500/2353) done. Loss: 0.0595  lr:0.000001
[ Sun May 19 02:27:17 2024 ] 	Batch(600/2353) done. Loss: 0.0124  lr:0.000001
[ Sun May 19 02:27:53 2024 ] 	Batch(700/2353) done. Loss: 0.1866  lr:0.000001
[ Sun May 19 02:28:30 2024 ] 	Batch(800/2353) done. Loss: 0.0215  lr:0.000001
[ Sun May 19 02:29:06 2024 ] 	Batch(900/2353) done. Loss: 0.1471  lr:0.000001
[ Sun May 19 02:29:43 2024 ] 	Batch(1000/2353) done. Loss: 0.1228  lr:0.000001
[ Sun May 19 02:30:20 2024 ] 	Batch(1100/2353) done. Loss: 0.0186  lr:0.000001
[ Sun May 19 02:30:56 2024 ] 	Batch(1200/2353) done. Loss: 0.0489  lr:0.000001
[ Sun May 19 02:31:34 2024 ] 	Batch(1300/2353) done. Loss: 0.0055  lr:0.000001
[ Sun May 19 02:32:11 2024 ] 	Batch(1400/2353) done. Loss: 0.0113  lr:0.000001
[ Sun May 19 02:32:48 2024 ] 	Batch(1500/2353) done. Loss: 0.0625  lr:0.000001
[ Sun May 19 02:33:25 2024 ] 	Batch(1600/2353) done. Loss: 0.0121  lr:0.000001
[ Sun May 19 02:34:03 2024 ] 	Batch(1700/2353) done. Loss: 0.0033  lr:0.000001
[ Sun May 19 02:34:39 2024 ] 	Batch(1800/2353) done. Loss: 0.0113  lr:0.000001
[ Sun May 19 02:35:16 2024 ] 	Batch(1900/2353) done. Loss: 0.0481  lr:0.000001
[ Sun May 19 02:35:54 2024 ] 	Batch(2000/2353) done. Loss: 0.0292  lr:0.000001
[ Sun May 19 02:36:31 2024 ] 	Batch(2100/2353) done. Loss: 0.0168  lr:0.000001
[ Sun May 19 02:37:08 2024 ] 	Batch(2200/2353) done. Loss: 0.0794  lr:0.000001
[ Sun May 19 02:37:46 2024 ] 	Batch(2300/2353) done. Loss: 0.0530  lr:0.000001
[ Sun May 19 02:38:06 2024 ] 	Mean training loss: 0.0418.
[ Sun May 19 02:38:06 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 02:38:06 2024 ] Eval epoch: 120
[ Sun May 19 02:40:09 2024 ] 	Mean val loss of 2367 batches: 0.24323486931473787.
[ Sun May 19 02:40:09 2024 ] Load weights from ./prova20/epoch119_model.pt.
[ Sun May 19 06:45:48 2024 ] Load weights from prova20/epoch118_model.pt.
[ Sun May 19 06:45:48 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/train_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/train_label.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/val_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/val_label.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 60, 'channel': 6, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': False, 'only_attention': True, 'tcn_attention': True, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': True, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': 'prova20/epoch118_model.pt', 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': False, 'scheduler': 1, 'base_lr': 0.01, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 16, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': False, 'display_recall_precision': False}

[ Sun May 19 06:45:48 2024 ] Training epoch: 1
[ Sun May 19 06:45:52 2024 ] 	Batch(0/2353) done. Loss: 0.0416  lr:0.010000
[ Sun May 19 06:46:28 2024 ] 	Batch(100/2353) done. Loss: 0.0095  lr:0.010000
[ Sun May 19 06:47:04 2024 ] 	Batch(200/2353) done. Loss: 0.2610  lr:0.010000
[ Sun May 19 06:47:40 2024 ] 	Batch(300/2353) done. Loss: 0.0074  lr:0.010000
[ Sun May 19 06:48:16 2024 ] 	Batch(400/2353) done. Loss: 0.0312  lr:0.010000
[ Sun May 19 06:48:52 2024 ] 	Batch(500/2353) done. Loss: 0.0062  lr:0.010000
[ Sun May 19 06:49:28 2024 ] 	Batch(600/2353) done. Loss: 0.0789  lr:0.010000
[ Sun May 19 06:50:04 2024 ] 	Batch(700/2353) done. Loss: 0.1444  lr:0.010000
[ Sun May 19 06:50:40 2024 ] 	Batch(800/2353) done. Loss: 0.0801  lr:0.010000
[ Sun May 19 06:51:16 2024 ] 	Batch(900/2353) done. Loss: 0.0402  lr:0.010000
[ Sun May 19 06:51:52 2024 ] 	Batch(1000/2353) done. Loss: 0.0665  lr:0.010000
[ Sun May 19 06:52:28 2024 ] 	Batch(1100/2353) done. Loss: 0.2353  lr:0.010000
[ Sun May 19 06:53:04 2024 ] 	Batch(1200/2353) done. Loss: 0.1317  lr:0.010000
[ Sun May 19 06:53:39 2024 ] 	Batch(1300/2353) done. Loss: 0.2036  lr:0.010000
[ Sun May 19 06:54:15 2024 ] 	Batch(1400/2353) done. Loss: 0.0622  lr:0.010000
[ Sun May 19 06:54:51 2024 ] 	Batch(1500/2353) done. Loss: 0.0233  lr:0.010000
[ Sun May 19 06:55:27 2024 ] 	Batch(1600/2353) done. Loss: 0.0535  lr:0.010000
[ Sun May 19 06:56:03 2024 ] 	Batch(1700/2353) done. Loss: 0.2896  lr:0.010000
[ Sun May 19 06:56:39 2024 ] 	Batch(1800/2353) done. Loss: 0.1887  lr:0.010000
[ Sun May 19 06:57:15 2024 ] 	Batch(1900/2353) done. Loss: 0.0127  lr:0.010000
[ Sun May 19 06:57:51 2024 ] 	Batch(2000/2353) done. Loss: 0.3068  lr:0.010000
[ Sun May 19 06:58:27 2024 ] 	Batch(2100/2353) done. Loss: 0.0249  lr:0.010000
[ Sun May 19 06:59:02 2024 ] 	Batch(2200/2353) done. Loss: 0.1525  lr:0.010000
[ Sun May 19 06:59:38 2024 ] 	Batch(2300/2353) done. Loss: 0.0297  lr:0.010000
[ Sun May 19 06:59:57 2024 ] 	Mean training loss: 0.1227.
[ Sun May 19 06:59:57 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 06:59:57 2024 ] Training epoch: 2
[ Sun May 19 06:59:58 2024 ] 	Batch(0/2353) done. Loss: 0.0289  lr:0.010000
[ Sun May 19 07:00:35 2024 ] 	Batch(100/2353) done. Loss: 0.0792  lr:0.010000
[ Sun May 19 07:01:12 2024 ] 	Batch(200/2353) done. Loss: 0.3686  lr:0.010000
[ Sun May 19 07:01:48 2024 ] 	Batch(300/2353) done. Loss: 0.0707  lr:0.010000
[ Sun May 19 07:02:25 2024 ] 	Batch(400/2353) done. Loss: 0.0644  lr:0.010000
[ Sun May 19 07:03:02 2024 ] 	Batch(500/2353) done. Loss: 0.1854  lr:0.010000
[ Sun May 19 07:03:38 2024 ] 	Batch(600/2353) done. Loss: 0.0401  lr:0.010000
[ Sun May 19 07:04:15 2024 ] 	Batch(700/2353) done. Loss: 0.2278  lr:0.010000
[ Sun May 19 07:04:52 2024 ] 	Batch(800/2353) done. Loss: 0.1536  lr:0.010000
[ Sun May 19 07:05:28 2024 ] 	Batch(900/2353) done. Loss: 0.0984  lr:0.010000
[ Sun May 19 07:06:05 2024 ] 	Batch(1000/2353) done. Loss: 0.0471  lr:0.010000
[ Sun May 19 07:06:41 2024 ] 	Batch(1100/2353) done. Loss: 0.1873  lr:0.010000
[ Sun May 19 07:07:18 2024 ] 	Batch(1200/2353) done. Loss: 0.0752  lr:0.010000
[ Sun May 19 07:07:55 2024 ] 	Batch(1300/2353) done. Loss: 0.4617  lr:0.010000
[ Sun May 19 07:08:31 2024 ] 	Batch(1400/2353) done. Loss: 0.0452  lr:0.010000
[ Sun May 19 07:09:08 2024 ] 	Batch(1500/2353) done. Loss: 0.0940  lr:0.010000
[ Sun May 19 07:09:44 2024 ] 	Batch(1600/2353) done. Loss: 0.2107  lr:0.010000
[ Sun May 19 07:10:21 2024 ] 	Batch(1700/2353) done. Loss: 0.2154  lr:0.010000
[ Sun May 19 07:10:58 2024 ] 	Batch(1800/2353) done. Loss: 0.3983  lr:0.010000
[ Sun May 19 07:11:34 2024 ] 	Batch(1900/2353) done. Loss: 0.1859  lr:0.010000
[ Sun May 19 07:12:11 2024 ] 	Batch(2000/2353) done. Loss: 0.0831  lr:0.010000
[ Sun May 19 07:12:48 2024 ] 	Batch(2100/2353) done. Loss: 0.1604  lr:0.010000
[ Sun May 19 07:13:24 2024 ] 	Batch(2200/2353) done. Loss: 0.5224  lr:0.010000
[ Sun May 19 07:14:01 2024 ] 	Batch(2300/2353) done. Loss: 0.2126  lr:0.010000
[ Sun May 19 07:14:20 2024 ] 	Mean training loss: 0.1416.
[ Sun May 19 07:14:20 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 07:14:20 2024 ] Training epoch: 3
[ Sun May 19 07:14:21 2024 ] 	Batch(0/2353) done. Loss: 0.0753  lr:0.010000
[ Sun May 19 07:14:57 2024 ] 	Batch(100/2353) done. Loss: 0.0818  lr:0.010000
[ Sun May 19 07:15:34 2024 ] 	Batch(200/2353) done. Loss: 0.0480  lr:0.010000
[ Sun May 19 07:16:11 2024 ] 	Batch(300/2353) done. Loss: 0.0850  lr:0.010000
[ Sun May 19 07:16:47 2024 ] 	Batch(400/2353) done. Loss: 0.0150  lr:0.010000
[ Sun May 19 07:17:24 2024 ] 	Batch(500/2353) done. Loss: 0.2314  lr:0.010000
[ Sun May 19 07:18:01 2024 ] 	Batch(600/2353) done. Loss: 0.0481  lr:0.010000
[ Sun May 19 07:18:37 2024 ] 	Batch(700/2353) done. Loss: 0.2092  lr:0.010000
[ Sun May 19 07:19:14 2024 ] 	Batch(800/2353) done. Loss: 0.0936  lr:0.010000
[ Sun May 19 07:19:51 2024 ] 	Batch(900/2353) done. Loss: 0.0483  lr:0.010000
[ Sun May 19 07:20:27 2024 ] 	Batch(1000/2353) done. Loss: 0.1120  lr:0.010000
[ Sun May 19 07:21:04 2024 ] 	Batch(1100/2353) done. Loss: 0.0793  lr:0.010000
[ Sun May 19 07:21:40 2024 ] 	Batch(1200/2353) done. Loss: 0.0410  lr:0.010000
[ Sun May 19 07:22:17 2024 ] 	Batch(1300/2353) done. Loss: 0.1162  lr:0.010000
[ Sun May 19 07:22:54 2024 ] 	Batch(1400/2353) done. Loss: 0.0299  lr:0.010000
[ Sun May 19 07:23:30 2024 ] 	Batch(1500/2353) done. Loss: 0.0156  lr:0.010000
[ Sun May 19 07:24:07 2024 ] 	Batch(1600/2353) done. Loss: 0.1120  lr:0.010000
[ Sun May 19 07:24:44 2024 ] 	Batch(1700/2353) done. Loss: 0.4144  lr:0.010000
[ Sun May 19 07:25:20 2024 ] 	Batch(1800/2353) done. Loss: 0.6492  lr:0.010000
[ Sun May 19 07:25:57 2024 ] 	Batch(1900/2353) done. Loss: 0.0127  lr:0.010000
[ Sun May 19 07:26:34 2024 ] 	Batch(2000/2353) done. Loss: 0.1219  lr:0.010000
[ Sun May 19 07:27:10 2024 ] 	Batch(2100/2353) done. Loss: 0.2071  lr:0.010000
[ Sun May 19 07:27:47 2024 ] 	Batch(2200/2353) done. Loss: 0.0909  lr:0.010000
[ Sun May 19 07:28:23 2024 ] 	Batch(2300/2353) done. Loss: 0.1396  lr:0.010000
[ Sun May 19 07:28:42 2024 ] 	Mean training loss: 0.1499.
[ Sun May 19 07:28:42 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 07:28:43 2024 ] Training epoch: 4
[ Sun May 19 07:28:43 2024 ] 	Batch(0/2353) done. Loss: 0.0342  lr:0.010000
[ Sun May 19 07:29:20 2024 ] 	Batch(100/2353) done. Loss: 0.3454  lr:0.010000
[ Sun May 19 07:29:56 2024 ] 	Batch(200/2353) done. Loss: 0.0333  lr:0.010000
[ Sun May 19 07:30:33 2024 ] 	Batch(300/2353) done. Loss: 0.0289  lr:0.010000
[ Sun May 19 07:31:10 2024 ] 	Batch(400/2353) done. Loss: 0.0219  lr:0.010000
[ Sun May 19 07:31:46 2024 ] 	Batch(500/2353) done. Loss: 0.0951  lr:0.010000
[ Sun May 19 07:32:23 2024 ] 	Batch(600/2353) done. Loss: 0.0222  lr:0.010000
[ Sun May 19 07:32:59 2024 ] 	Batch(700/2353) done. Loss: 0.0929  lr:0.010000
[ Sun May 19 07:33:36 2024 ] 	Batch(800/2353) done. Loss: 0.2414  lr:0.010000
[ Sun May 19 07:34:13 2024 ] 	Batch(900/2353) done. Loss: 0.1607  lr:0.010000
[ Sun May 19 07:34:49 2024 ] 	Batch(1000/2353) done. Loss: 0.2752  lr:0.010000
[ Sun May 19 07:35:26 2024 ] 	Batch(1100/2353) done. Loss: 0.1419  lr:0.010000
[ Sun May 19 07:36:03 2024 ] 	Batch(1200/2353) done. Loss: 0.2477  lr:0.010000
[ Sun May 19 07:36:39 2024 ] 	Batch(1300/2353) done. Loss: 0.0105  lr:0.010000
[ Sun May 19 07:37:16 2024 ] 	Batch(1400/2353) done. Loss: 0.2966  lr:0.010000
[ Sun May 19 07:37:53 2024 ] 	Batch(1500/2353) done. Loss: 0.3986  lr:0.010000
[ Sun May 19 07:38:29 2024 ] 	Batch(1600/2353) done. Loss: 0.0540  lr:0.010000
[ Sun May 19 07:39:06 2024 ] 	Batch(1700/2353) done. Loss: 0.4600  lr:0.010000
[ Sun May 19 07:39:43 2024 ] 	Batch(1800/2353) done. Loss: 0.0921  lr:0.010000
[ Sun May 19 07:40:19 2024 ] 	Batch(1900/2353) done. Loss: 0.2236  lr:0.010000
[ Sun May 19 07:40:56 2024 ] 	Batch(2000/2353) done. Loss: 0.1717  lr:0.010000
[ Sun May 19 07:41:33 2024 ] 	Batch(2100/2353) done. Loss: 0.0135  lr:0.010000
[ Sun May 19 07:42:09 2024 ] 	Batch(2200/2353) done. Loss: 0.1123  lr:0.010000
[ Sun May 19 07:42:46 2024 ] 	Batch(2300/2353) done. Loss: 0.0534  lr:0.010000
[ Sun May 19 07:43:05 2024 ] 	Mean training loss: 0.1414.
[ Sun May 19 07:43:05 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 07:43:05 2024 ] Training epoch: 5
[ Sun May 19 07:43:06 2024 ] 	Batch(0/2353) done. Loss: 0.1936  lr:0.010000
[ Sun May 19 07:43:43 2024 ] 	Batch(100/2353) done. Loss: 0.1540  lr:0.010000
[ Sun May 19 07:44:20 2024 ] 	Batch(200/2353) done. Loss: 0.0461  lr:0.010000
[ Sun May 19 07:44:58 2024 ] 	Batch(300/2353) done. Loss: 0.0256  lr:0.010000
[ Sun May 19 07:45:35 2024 ] 	Batch(400/2353) done. Loss: 0.0415  lr:0.010000
[ Sun May 19 07:46:13 2024 ] 	Batch(500/2353) done. Loss: 0.1281  lr:0.010000
[ Sun May 19 07:46:50 2024 ] 	Batch(600/2353) done. Loss: 0.3213  lr:0.010000
[ Sun May 19 07:47:27 2024 ] 	Batch(700/2353) done. Loss: 0.0182  lr:0.010000
[ Sun May 19 07:48:03 2024 ] 	Batch(800/2353) done. Loss: 0.0667  lr:0.010000
[ Sun May 19 07:48:40 2024 ] 	Batch(900/2353) done. Loss: 0.0533  lr:0.010000
[ Sun May 19 07:49:17 2024 ] 	Batch(1000/2353) done. Loss: 0.0210  lr:0.010000
[ Sun May 19 07:49:53 2024 ] 	Batch(1100/2353) done. Loss: 0.5510  lr:0.010000
[ Sun May 19 07:50:30 2024 ] 	Batch(1200/2353) done. Loss: 0.0286  lr:0.010000
[ Sun May 19 07:51:06 2024 ] 	Batch(1300/2353) done. Loss: 0.0293  lr:0.010000
[ Sun May 19 07:51:43 2024 ] 	Batch(1400/2353) done. Loss: 0.0740  lr:0.010000
[ Sun May 19 07:52:20 2024 ] 	Batch(1500/2353) done. Loss: 0.3095  lr:0.010000
[ Sun May 19 07:52:57 2024 ] 	Batch(1600/2353) done. Loss: 0.0082  lr:0.010000
[ Sun May 19 07:53:33 2024 ] 	Batch(1700/2353) done. Loss: 0.0312  lr:0.010000
[ Sun May 19 07:54:10 2024 ] 	Batch(1800/2353) done. Loss: 0.4724  lr:0.010000
[ Sun May 19 07:54:47 2024 ] 	Batch(1900/2353) done. Loss: 0.0600  lr:0.010000
[ Sun May 19 07:55:23 2024 ] 	Batch(2000/2353) done. Loss: 0.2221  lr:0.010000
[ Sun May 19 07:56:00 2024 ] 	Batch(2100/2353) done. Loss: 0.1761  lr:0.010000
[ Sun May 19 07:56:37 2024 ] 	Batch(2200/2353) done. Loss: 0.1298  lr:0.010000
[ Sun May 19 07:57:13 2024 ] 	Batch(2300/2353) done. Loss: 0.2485  lr:0.010000
[ Sun May 19 07:57:32 2024 ] 	Mean training loss: 0.1370.
[ Sun May 19 07:57:32 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 07:57:32 2024 ] Training epoch: 6
[ Sun May 19 07:57:33 2024 ] 	Batch(0/2353) done. Loss: 0.0787  lr:0.010000
[ Sun May 19 07:58:10 2024 ] 	Batch(100/2353) done. Loss: 0.0627  lr:0.010000
[ Sun May 19 07:58:46 2024 ] 	Batch(200/2353) done. Loss: 0.0523  lr:0.010000
[ Sun May 19 07:59:23 2024 ] 	Batch(300/2353) done. Loss: 0.0753  lr:0.010000
[ Sun May 19 08:00:00 2024 ] 	Batch(400/2353) done. Loss: 0.0243  lr:0.010000
[ Sun May 19 08:00:37 2024 ] 	Batch(500/2353) done. Loss: 0.0921  lr:0.010000
[ Sun May 19 08:01:13 2024 ] 	Batch(600/2353) done. Loss: 0.3337  lr:0.010000
[ Sun May 19 08:01:50 2024 ] 	Batch(700/2353) done. Loss: 0.0898  lr:0.010000
[ Sun May 19 08:02:27 2024 ] 	Batch(800/2353) done. Loss: 0.3063  lr:0.010000
[ Sun May 19 08:03:03 2024 ] 	Batch(900/2353) done. Loss: 0.0349  lr:0.010000
[ Sun May 19 08:03:40 2024 ] 	Batch(1000/2353) done. Loss: 0.2609  lr:0.010000
[ Sun May 19 08:04:16 2024 ] 	Batch(1100/2353) done. Loss: 0.3172  lr:0.010000
[ Sun May 19 08:04:53 2024 ] 	Batch(1200/2353) done. Loss: 0.0804  lr:0.010000
[ Sun May 19 08:05:30 2024 ] 	Batch(1300/2353) done. Loss: 0.1275  lr:0.010000
[ Sun May 19 08:06:07 2024 ] 	Batch(1400/2353) done. Loss: 0.2704  lr:0.010000
[ Sun May 19 08:06:43 2024 ] 	Batch(1500/2353) done. Loss: 0.1389  lr:0.010000
[ Sun May 19 08:07:20 2024 ] 	Batch(1600/2353) done. Loss: 0.0673  lr:0.010000
[ Sun May 19 08:07:57 2024 ] 	Batch(1700/2353) done. Loss: 0.1355  lr:0.010000
[ Sun May 19 08:08:33 2024 ] 	Batch(1800/2353) done. Loss: 0.3754  lr:0.010000
[ Sun May 19 08:09:10 2024 ] 	Batch(1900/2353) done. Loss: 0.3348  lr:0.010000
[ Sun May 19 08:09:47 2024 ] 	Batch(2000/2353) done. Loss: 0.1662  lr:0.010000
[ Sun May 19 08:10:23 2024 ] 	Batch(2100/2353) done. Loss: 0.1093  lr:0.010000
[ Sun May 19 08:11:00 2024 ] 	Batch(2200/2353) done. Loss: 0.1169  lr:0.010000
[ Sun May 19 08:11:37 2024 ] 	Batch(2300/2353) done. Loss: 0.2570  lr:0.010000
[ Sun May 19 08:11:56 2024 ] 	Mean training loss: 0.1395.
[ Sun May 19 08:11:56 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 08:11:56 2024 ] Training epoch: 7
[ Sun May 19 08:11:57 2024 ] 	Batch(0/2353) done. Loss: 0.0305  lr:0.010000
[ Sun May 19 08:12:33 2024 ] 	Batch(100/2353) done. Loss: 0.0412  lr:0.010000
[ Sun May 19 08:13:10 2024 ] 	Batch(200/2353) done. Loss: 0.0504  lr:0.010000
[ Sun May 19 08:13:47 2024 ] 	Batch(300/2353) done. Loss: 0.0229  lr:0.010000
[ Sun May 19 08:14:24 2024 ] 	Batch(400/2353) done. Loss: 0.0254  lr:0.010000
[ Sun May 19 08:15:00 2024 ] 	Batch(500/2353) done. Loss: 0.0565  lr:0.010000
[ Sun May 19 08:15:37 2024 ] 	Batch(600/2353) done. Loss: 0.0264  lr:0.010000
[ Sun May 19 08:16:13 2024 ] 	Batch(700/2353) done. Loss: 0.0993  lr:0.010000
[ Sun May 19 08:16:50 2024 ] 	Batch(800/2353) done. Loss: 0.1650  lr:0.010000
[ Sun May 19 08:17:27 2024 ] 	Batch(900/2353) done. Loss: 0.2283  lr:0.010000
[ Sun May 19 08:18:03 2024 ] 	Batch(1000/2353) done. Loss: 0.3498  lr:0.010000
[ Sun May 19 08:18:40 2024 ] 	Batch(1100/2353) done. Loss: 0.0358  lr:0.010000
[ Sun May 19 08:19:17 2024 ] 	Batch(1200/2353) done. Loss: 0.1992  lr:0.010000
[ Sun May 19 08:19:53 2024 ] 	Batch(1300/2353) done. Loss: 0.2865  lr:0.010000
[ Sun May 19 08:20:30 2024 ] 	Batch(1400/2353) done. Loss: 0.2655  lr:0.010000
[ Sun May 19 08:21:07 2024 ] 	Batch(1500/2353) done. Loss: 0.0357  lr:0.010000
[ Sun May 19 08:21:43 2024 ] 	Batch(1600/2353) done. Loss: 0.0820  lr:0.010000
[ Sun May 19 08:22:20 2024 ] 	Batch(1700/2353) done. Loss: 0.0733  lr:0.010000
[ Sun May 19 08:22:57 2024 ] 	Batch(1800/2353) done. Loss: 0.2797  lr:0.010000
[ Sun May 19 08:23:33 2024 ] 	Batch(1900/2353) done. Loss: 0.1864  lr:0.010000
[ Sun May 19 08:24:10 2024 ] 	Batch(2000/2353) done. Loss: 0.0412  lr:0.010000
[ Sun May 19 08:24:47 2024 ] 	Batch(2100/2353) done. Loss: 0.0164  lr:0.010000
[ Sun May 19 08:25:23 2024 ] 	Batch(2200/2353) done. Loss: 0.2496  lr:0.010000
[ Sun May 19 08:26:00 2024 ] 	Batch(2300/2353) done. Loss: 0.0657  lr:0.010000
[ Sun May 19 08:26:19 2024 ] 	Mean training loss: 0.1414.
[ Sun May 19 08:26:19 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 08:26:19 2024 ] Training epoch: 8
[ Sun May 19 08:26:20 2024 ] 	Batch(0/2353) done. Loss: 0.2284  lr:0.010000
[ Sun May 19 08:26:56 2024 ] 	Batch(100/2353) done. Loss: 0.4279  lr:0.010000
[ Sun May 19 08:27:33 2024 ] 	Batch(200/2353) done. Loss: 0.0607  lr:0.010000
[ Sun May 19 08:28:09 2024 ] 	Batch(300/2353) done. Loss: 0.0254  lr:0.010000
[ Sun May 19 08:28:46 2024 ] 	Batch(400/2353) done. Loss: 0.0342  lr:0.010000
[ Sun May 19 08:29:23 2024 ] 	Batch(500/2353) done. Loss: 0.0317  lr:0.010000
[ Sun May 19 08:29:59 2024 ] 	Batch(600/2353) done. Loss: 0.0227  lr:0.010000
[ Sun May 19 08:30:36 2024 ] 	Batch(700/2353) done. Loss: 0.0260  lr:0.010000
[ Sun May 19 08:31:13 2024 ] 	Batch(800/2353) done. Loss: 0.1640  lr:0.010000
[ Sun May 19 08:31:50 2024 ] 	Batch(900/2353) done. Loss: 0.0583  lr:0.010000
[ Sun May 19 08:32:28 2024 ] 	Batch(1000/2353) done. Loss: 0.0358  lr:0.010000
[ Sun May 19 08:33:05 2024 ] 	Batch(1100/2353) done. Loss: 0.1486  lr:0.010000
[ Sun May 19 08:33:42 2024 ] 	Batch(1200/2353) done. Loss: 0.1473  lr:0.010000
[ Sun May 19 08:34:19 2024 ] 	Batch(1300/2353) done. Loss: 0.0568  lr:0.010000
[ Sun May 19 08:34:56 2024 ] 	Batch(1400/2353) done. Loss: 0.1753  lr:0.010000
[ Sun May 19 08:35:32 2024 ] 	Batch(1500/2353) done. Loss: 0.1984  lr:0.010000
[ Sun May 19 08:36:09 2024 ] 	Batch(1600/2353) done. Loss: 0.0835  lr:0.010000
[ Sun May 19 08:36:46 2024 ] 	Batch(1700/2353) done. Loss: 0.0400  lr:0.010000
[ Sun May 19 08:37:22 2024 ] 	Batch(1800/2353) done. Loss: 0.1459  lr:0.010000
[ Sun May 19 08:37:59 2024 ] 	Batch(1900/2353) done. Loss: 0.0857  lr:0.010000
[ Sun May 19 08:38:36 2024 ] 	Batch(2000/2353) done. Loss: 0.0150  lr:0.010000
[ Sun May 19 08:39:12 2024 ] 	Batch(2100/2353) done. Loss: 0.0651  lr:0.010000
[ Sun May 19 08:39:49 2024 ] 	Batch(2200/2353) done. Loss: 0.0253  lr:0.010000
[ Sun May 19 08:40:26 2024 ] 	Batch(2300/2353) done. Loss: 0.0133  lr:0.010000
[ Sun May 19 08:40:45 2024 ] 	Mean training loss: 0.1293.
[ Sun May 19 08:40:45 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 08:40:45 2024 ] Training epoch: 9
[ Sun May 19 08:40:46 2024 ] 	Batch(0/2353) done. Loss: 0.1006  lr:0.010000
[ Sun May 19 08:41:22 2024 ] 	Batch(100/2353) done. Loss: 0.0935  lr:0.010000
[ Sun May 19 08:41:59 2024 ] 	Batch(200/2353) done. Loss: 0.1635  lr:0.010000
[ Sun May 19 08:42:35 2024 ] 	Batch(300/2353) done. Loss: 0.1391  lr:0.010000
[ Sun May 19 08:43:12 2024 ] 	Batch(400/2353) done. Loss: 0.0942  lr:0.010000
[ Sun May 19 08:43:49 2024 ] 	Batch(500/2353) done. Loss: 0.0357  lr:0.010000
[ Sun May 19 08:44:25 2024 ] 	Batch(600/2353) done. Loss: 0.0251  lr:0.010000
[ Sun May 19 08:45:02 2024 ] 	Batch(700/2353) done. Loss: 0.3685  lr:0.010000
[ Sun May 19 08:45:39 2024 ] 	Batch(800/2353) done. Loss: 0.0323  lr:0.010000
[ Sun May 19 08:46:15 2024 ] 	Batch(900/2353) done. Loss: 0.3096  lr:0.010000
[ Sun May 19 08:46:52 2024 ] 	Batch(1000/2353) done. Loss: 0.0294  lr:0.010000
[ Sun May 19 08:47:28 2024 ] 	Batch(1100/2353) done. Loss: 0.3586  lr:0.010000
[ Sun May 19 08:48:05 2024 ] 	Batch(1200/2353) done. Loss: 0.1246  lr:0.010000
[ Sun May 19 08:48:42 2024 ] 	Batch(1300/2353) done. Loss: 0.2799  lr:0.010000
[ Sun May 19 08:49:19 2024 ] 	Batch(1400/2353) done. Loss: 0.0303  lr:0.010000
[ Sun May 19 08:49:55 2024 ] 	Batch(1500/2353) done. Loss: 0.1539  lr:0.010000
[ Sun May 19 08:50:32 2024 ] 	Batch(1600/2353) done. Loss: 0.0217  lr:0.010000
[ Sun May 19 08:51:09 2024 ] 	Batch(1700/2353) done. Loss: 0.0059  lr:0.010000
[ Sun May 19 08:51:45 2024 ] 	Batch(1800/2353) done. Loss: 0.2075  lr:0.010000
[ Sun May 19 08:52:22 2024 ] 	Batch(1900/2353) done. Loss: 0.1388  lr:0.010000
[ Sun May 19 08:52:59 2024 ] 	Batch(2000/2353) done. Loss: 0.0961  lr:0.010000
[ Sun May 19 08:53:35 2024 ] 	Batch(2100/2353) done. Loss: 0.2718  lr:0.010000
[ Sun May 19 08:54:12 2024 ] 	Batch(2200/2353) done. Loss: 0.1739  lr:0.010000
[ Sun May 19 08:54:49 2024 ] 	Batch(2300/2353) done. Loss: 0.3614  lr:0.010000
[ Sun May 19 08:55:09 2024 ] 	Mean training loss: 0.1366.
[ Sun May 19 08:55:09 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 08:55:09 2024 ] Training epoch: 10
[ Sun May 19 08:55:09 2024 ] 	Batch(0/2353) done. Loss: 0.0558  lr:0.010000
[ Sun May 19 08:55:46 2024 ] 	Batch(100/2353) done. Loss: 0.0156  lr:0.010000
[ Sun May 19 08:56:23 2024 ] 	Batch(200/2353) done. Loss: 0.2450  lr:0.010000
[ Sun May 19 08:57:00 2024 ] 	Batch(300/2353) done. Loss: 0.0186  lr:0.010000
[ Sun May 19 08:57:38 2024 ] 	Batch(400/2353) done. Loss: 0.0141  lr:0.010000
[ Sun May 19 08:58:15 2024 ] 	Batch(500/2353) done. Loss: 0.0419  lr:0.010000
[ Sun May 19 08:58:51 2024 ] 	Batch(600/2353) done. Loss: 0.0139  lr:0.010000
[ Sun May 19 08:59:28 2024 ] 	Batch(700/2353) done. Loss: 0.1124  lr:0.010000
[ Sun May 19 09:00:05 2024 ] 	Batch(800/2353) done. Loss: 0.1677  lr:0.010000
[ Sun May 19 09:00:41 2024 ] 	Batch(900/2353) done. Loss: 0.1226  lr:0.010000
[ Sun May 19 09:01:18 2024 ] 	Batch(1000/2353) done. Loss: 0.0244  lr:0.010000
[ Sun May 19 09:01:55 2024 ] 	Batch(1100/2353) done. Loss: 0.4939  lr:0.010000
[ Sun May 19 09:02:31 2024 ] 	Batch(1200/2353) done. Loss: 0.0648  lr:0.010000
[ Sun May 19 09:03:08 2024 ] 	Batch(1300/2353) done. Loss: 0.2748  lr:0.010000
[ Sun May 19 09:03:45 2024 ] 	Batch(1400/2353) done. Loss: 0.1195  lr:0.010000
[ Sun May 19 09:04:21 2024 ] 	Batch(1500/2353) done. Loss: 0.1869  lr:0.010000
[ Sun May 19 09:04:58 2024 ] 	Batch(1600/2353) done. Loss: 0.2981  lr:0.010000
[ Sun May 19 09:05:35 2024 ] 	Batch(1700/2353) done. Loss: 0.0690  lr:0.010000
[ Sun May 19 09:06:11 2024 ] 	Batch(1800/2353) done. Loss: 0.0573  lr:0.010000
[ Sun May 19 09:06:48 2024 ] 	Batch(1900/2353) done. Loss: 0.2382  lr:0.010000
[ Sun May 19 09:07:25 2024 ] 	Batch(2000/2353) done. Loss: 0.0236  lr:0.010000
[ Sun May 19 09:08:01 2024 ] 	Batch(2100/2353) done. Loss: 0.0548  lr:0.010000
[ Sun May 19 09:08:38 2024 ] 	Batch(2200/2353) done. Loss: 0.0610  lr:0.010000
[ Sun May 19 09:09:15 2024 ] 	Batch(2300/2353) done. Loss: 0.7759  lr:0.010000
[ Sun May 19 09:09:34 2024 ] 	Mean training loss: 0.1209.
[ Sun May 19 09:09:34 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 09:09:34 2024 ] Eval epoch: 10
[ Sun May 19 09:11:37 2024 ] 	Mean val loss of 2367 batches: 0.3235538215224562.
[ Sun May 19 09:11:37 2024 ] Training epoch: 11
[ Sun May 19 09:11:38 2024 ] 	Batch(0/2353) done. Loss: 0.2762  lr:0.010000
[ Sun May 19 09:12:19 2024 ] Load weights from prova20/epoch118_model.pt.
[ Sun May 19 09:12:19 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/train_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/train_label.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/val_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/val_label.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 60, 'channel': 6, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': False, 'only_attention': True, 'tcn_attention': True, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': True, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': 'prova20/epoch118_model.pt', 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': False, 'scheduler': 1, 'base_lr': 1e-05, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 16, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': False, 'display_recall_precision': False}

[ Sun May 19 09:12:19 2024 ] Training epoch: 1
[ Sun May 19 09:12:20 2024 ] 	Batch(0/2353) done. Loss: 0.0416  lr:0.000010
[ Sun May 19 09:12:56 2024 ] 	Batch(100/2353) done. Loss: 0.0033  lr:0.000010
[ Sun May 19 09:13:32 2024 ] 	Batch(200/2353) done. Loss: 0.1811  lr:0.000010
[ Sun May 19 09:14:07 2024 ] 	Batch(300/2353) done. Loss: 0.0095  lr:0.000010
[ Sun May 19 09:14:43 2024 ] 	Batch(400/2353) done. Loss: 0.0160  lr:0.000010
[ Sun May 19 09:15:19 2024 ] 	Batch(500/2353) done. Loss: 0.0119  lr:0.000010
[ Sun May 19 09:15:55 2024 ] 	Batch(600/2353) done. Loss: 0.0214  lr:0.000010
[ Sun May 19 09:16:31 2024 ] 	Batch(700/2353) done. Loss: 0.0473  lr:0.000010
[ Sun May 19 09:17:07 2024 ] 	Batch(800/2353) done. Loss: 0.0095  lr:0.000010
[ Sun May 19 09:17:42 2024 ] 	Batch(900/2353) done. Loss: 0.0092  lr:0.000010
[ Sun May 19 09:18:18 2024 ] 	Batch(1000/2353) done. Loss: 0.0355  lr:0.000010
[ Sun May 19 09:18:54 2024 ] 	Batch(1100/2353) done. Loss: 0.1357  lr:0.000010
[ Sun May 19 09:19:30 2024 ] 	Batch(1200/2353) done. Loss: 0.0595  lr:0.000010
[ Sun May 19 09:20:05 2024 ] 	Batch(1300/2353) done. Loss: 0.0326  lr:0.000010
[ Sun May 19 09:20:41 2024 ] 	Batch(1400/2353) done. Loss: 0.0062  lr:0.000010
[ Sun May 19 09:21:17 2024 ] 	Batch(1500/2353) done. Loss: 0.0106  lr:0.000010
[ Sun May 19 09:21:54 2024 ] 	Batch(1600/2353) done. Loss: 0.0245  lr:0.000010
[ Sun May 19 09:22:30 2024 ] 	Batch(1700/2353) done. Loss: 0.1597  lr:0.000010
[ Sun May 19 09:23:05 2024 ] 	Batch(1800/2353) done. Loss: 0.0732  lr:0.000010
[ Sun May 19 09:23:41 2024 ] 	Batch(1900/2353) done. Loss: 0.0023  lr:0.000010
[ Sun May 19 09:24:17 2024 ] 	Batch(2000/2353) done. Loss: 0.0662  lr:0.000010
[ Sun May 19 09:24:53 2024 ] 	Batch(2100/2353) done. Loss: 0.0071  lr:0.000010
[ Sun May 19 09:25:29 2024 ] 	Batch(2200/2353) done. Loss: 0.0225  lr:0.000010
[ Sun May 19 09:26:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0104  lr:0.000010
[ Sun May 19 09:26:23 2024 ] 	Mean training loss: 0.0400.
[ Sun May 19 09:26:23 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 09:26:23 2024 ] Training epoch: 2
[ Sun May 19 09:26:24 2024 ] 	Batch(0/2353) done. Loss: 0.0043  lr:0.000010
[ Sun May 19 09:27:01 2024 ] 	Batch(100/2353) done. Loss: 0.0060  lr:0.000010
[ Sun May 19 09:27:38 2024 ] 	Batch(200/2353) done. Loss: 0.0857  lr:0.000010
[ Sun May 19 09:28:14 2024 ] 	Batch(300/2353) done. Loss: 0.0383  lr:0.000010
[ Sun May 19 09:28:51 2024 ] 	Batch(400/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 09:29:28 2024 ] 	Batch(500/2353) done. Loss: 0.1361  lr:0.000010
[ Sun May 19 09:30:04 2024 ] 	Batch(600/2353) done. Loss: 0.0085  lr:0.000010
[ Sun May 19 09:30:41 2024 ] 	Batch(700/2353) done. Loss: 0.0324  lr:0.000010
[ Sun May 19 09:31:18 2024 ] 	Batch(800/2353) done. Loss: 0.0205  lr:0.000010
[ Sun May 19 09:31:55 2024 ] 	Batch(900/2353) done. Loss: 0.0135  lr:0.000010
[ Sun May 19 09:32:32 2024 ] 	Batch(1000/2353) done. Loss: 0.0084  lr:0.000010
[ Sun May 19 09:33:10 2024 ] 	Batch(1100/2353) done. Loss: 0.0748  lr:0.000010
[ Sun May 19 09:33:47 2024 ] 	Batch(1200/2353) done. Loss: 0.0135  lr:0.000010
[ Sun May 19 09:34:25 2024 ] 	Batch(1300/2353) done. Loss: 0.3158  lr:0.000010
[ Sun May 19 09:35:02 2024 ] 	Batch(1400/2353) done. Loss: 0.0097  lr:0.000010
[ Sun May 19 09:35:39 2024 ] 	Batch(1500/2353) done. Loss: 0.0221  lr:0.000010
[ Sun May 19 09:36:17 2024 ] 	Batch(1600/2353) done. Loss: 0.0409  lr:0.000010
[ Sun May 19 09:36:54 2024 ] 	Batch(1700/2353) done. Loss: 0.0084  lr:0.000010
[ Sun May 19 09:37:31 2024 ] 	Batch(1800/2353) done. Loss: 0.0567  lr:0.000010
[ Sun May 19 09:38:09 2024 ] 	Batch(1900/2353) done. Loss: 0.0430  lr:0.000010
[ Sun May 19 09:38:46 2024 ] 	Batch(2000/2353) done. Loss: 0.0032  lr:0.000010
[ Sun May 19 09:39:23 2024 ] 	Batch(2100/2353) done. Loss: 0.1016  lr:0.000010
[ Sun May 19 09:39:59 2024 ] 	Batch(2200/2353) done. Loss: 0.2211  lr:0.000010
[ Sun May 19 09:40:36 2024 ] 	Batch(2300/2353) done. Loss: 0.0384  lr:0.000010
[ Sun May 19 09:40:55 2024 ] 	Mean training loss: 0.0404.
[ Sun May 19 09:40:55 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 09:40:55 2024 ] Training epoch: 3
[ Sun May 19 09:40:56 2024 ] 	Batch(0/2353) done. Loss: 0.0138  lr:0.000010
[ Sun May 19 09:41:34 2024 ] 	Batch(100/2353) done. Loss: 0.0191  lr:0.000010
[ Sun May 19 09:42:11 2024 ] 	Batch(200/2353) done. Loss: 0.0059  lr:0.000010
[ Sun May 19 09:42:47 2024 ] 	Batch(300/2353) done. Loss: 0.0142  lr:0.000010
[ Sun May 19 09:43:24 2024 ] 	Batch(400/2353) done. Loss: 0.0029  lr:0.000010
[ Sun May 19 09:44:01 2024 ] 	Batch(500/2353) done. Loss: 0.0164  lr:0.000010
[ Sun May 19 09:44:38 2024 ] 	Batch(600/2353) done. Loss: 0.1628  lr:0.000010
[ Sun May 19 09:45:14 2024 ] 	Batch(700/2353) done. Loss: 0.0525  lr:0.000010
[ Sun May 19 09:45:51 2024 ] 	Batch(800/2353) done. Loss: 0.0323  lr:0.000010
[ Sun May 19 09:46:28 2024 ] 	Batch(900/2353) done. Loss: 0.0090  lr:0.000010
[ Sun May 19 09:47:04 2024 ] 	Batch(1000/2353) done. Loss: 0.0289  lr:0.000010
[ Sun May 19 09:47:41 2024 ] 	Batch(1100/2353) done. Loss: 0.0019  lr:0.000010
[ Sun May 19 09:48:18 2024 ] 	Batch(1200/2353) done. Loss: 0.0239  lr:0.000010
[ Sun May 19 09:48:55 2024 ] 	Batch(1300/2353) done. Loss: 0.0439  lr:0.000010
[ Sun May 19 09:49:31 2024 ] 	Batch(1400/2353) done. Loss: 0.0044  lr:0.000010
[ Sun May 19 09:50:08 2024 ] 	Batch(1500/2353) done. Loss: 0.0155  lr:0.000010
[ Sun May 19 09:50:45 2024 ] 	Batch(1600/2353) done. Loss: 0.0214  lr:0.000010
[ Sun May 19 09:51:21 2024 ] 	Batch(1700/2353) done. Loss: 0.0407  lr:0.000010
[ Sun May 19 09:51:59 2024 ] 	Batch(1800/2353) done. Loss: 0.1543  lr:0.000010
[ Sun May 19 09:52:36 2024 ] 	Batch(1900/2353) done. Loss: 0.0090  lr:0.000010
[ Sun May 19 09:53:14 2024 ] 	Batch(2000/2353) done. Loss: 0.0423  lr:0.000010
[ Sun May 19 09:53:51 2024 ] 	Batch(2100/2353) done. Loss: 0.0149  lr:0.000010
[ Sun May 19 09:54:27 2024 ] 	Batch(2200/2353) done. Loss: 0.0268  lr:0.000010
[ Sun May 19 09:55:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0338  lr:0.000010
[ Sun May 19 09:55:23 2024 ] 	Mean training loss: 0.0420.
[ Sun May 19 09:55:23 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 09:55:23 2024 ] Training epoch: 4
[ Sun May 19 09:55:24 2024 ] 	Batch(0/2353) done. Loss: 0.0092  lr:0.000010
[ Sun May 19 09:56:01 2024 ] 	Batch(100/2353) done. Loss: 0.0228  lr:0.000010
[ Sun May 19 09:56:39 2024 ] 	Batch(200/2353) done. Loss: 0.0155  lr:0.000010
[ Sun May 19 09:57:16 2024 ] 	Batch(300/2353) done. Loss: 0.0439  lr:0.000010
[ Sun May 19 09:57:54 2024 ] 	Batch(400/2353) done. Loss: 0.0024  lr:0.000010
[ Sun May 19 09:58:31 2024 ] 	Batch(500/2353) done. Loss: 0.0294  lr:0.000010
[ Sun May 19 09:59:07 2024 ] 	Batch(600/2353) done. Loss: 0.0138  lr:0.000010
[ Sun May 19 09:59:44 2024 ] 	Batch(700/2353) done. Loss: 0.0457  lr:0.000010
[ Sun May 19 10:00:21 2024 ] 	Batch(800/2353) done. Loss: 0.0145  lr:0.000010
[ Sun May 19 10:00:58 2024 ] 	Batch(900/2353) done. Loss: 0.0159  lr:0.000010
[ Sun May 19 10:01:34 2024 ] 	Batch(1000/2353) done. Loss: 0.0282  lr:0.000010
[ Sun May 19 10:02:11 2024 ] 	Batch(1100/2353) done. Loss: 0.0131  lr:0.000010
[ Sun May 19 10:02:48 2024 ] 	Batch(1200/2353) done. Loss: 0.0946  lr:0.000010
[ Sun May 19 10:03:25 2024 ] 	Batch(1300/2353) done. Loss: 0.0020  lr:0.000010
[ Sun May 19 10:04:01 2024 ] 	Batch(1400/2353) done. Loss: 0.0651  lr:0.000010
[ Sun May 19 10:04:38 2024 ] 	Batch(1500/2353) done. Loss: 0.0727  lr:0.000010
[ Sun May 19 10:05:15 2024 ] 	Batch(1600/2353) done. Loss: 0.0037  lr:0.000010
[ Sun May 19 10:05:52 2024 ] 	Batch(1700/2353) done. Loss: 0.0629  lr:0.000010
[ Sun May 19 10:06:28 2024 ] 	Batch(1800/2353) done. Loss: 0.0189  lr:0.000010
[ Sun May 19 10:07:05 2024 ] 	Batch(1900/2353) done. Loss: 0.0779  lr:0.000010
[ Sun May 19 10:07:42 2024 ] 	Batch(2000/2353) done. Loss: 0.0520  lr:0.000010
[ Sun May 19 10:08:19 2024 ] 	Batch(2100/2353) done. Loss: 0.0510  lr:0.000010
[ Sun May 19 10:08:57 2024 ] 	Batch(2200/2353) done. Loss: 0.0138  lr:0.000010
[ Sun May 19 10:09:34 2024 ] 	Batch(2300/2353) done. Loss: 0.0777  lr:0.000010
[ Sun May 19 10:09:54 2024 ] 	Mean training loss: 0.0415.
[ Sun May 19 10:09:54 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 10:09:54 2024 ] Training epoch: 5
[ Sun May 19 10:09:55 2024 ] 	Batch(0/2353) done. Loss: 0.0382  lr:0.000010
[ Sun May 19 10:10:32 2024 ] 	Batch(100/2353) done. Loss: 0.0120  lr:0.000010
[ Sun May 19 10:11:10 2024 ] 	Batch(200/2353) done. Loss: 0.0341  lr:0.000010
[ Sun May 19 10:11:47 2024 ] 	Batch(300/2353) done. Loss: 0.0062  lr:0.000010
[ Sun May 19 10:12:25 2024 ] 	Batch(400/2353) done. Loss: 0.0160  lr:0.000010
[ Sun May 19 10:13:02 2024 ] 	Batch(500/2353) done. Loss: 0.0927  lr:0.000010
[ Sun May 19 10:13:39 2024 ] 	Batch(600/2353) done. Loss: 0.0578  lr:0.000010
[ Sun May 19 10:14:17 2024 ] 	Batch(700/2353) done. Loss: 0.0058  lr:0.000010
[ Sun May 19 10:14:54 2024 ] 	Batch(800/2353) done. Loss: 0.0121  lr:0.000010
[ Sun May 19 10:15:31 2024 ] 	Batch(900/2353) done. Loss: 0.0094  lr:0.000010
[ Sun May 19 10:16:08 2024 ] 	Batch(1000/2353) done. Loss: 0.0100  lr:0.000010
[ Sun May 19 10:16:44 2024 ] 	Batch(1100/2353) done. Loss: 0.0395  lr:0.000010
[ Sun May 19 10:17:21 2024 ] 	Batch(1200/2353) done. Loss: 0.0040  lr:0.000010
[ Sun May 19 10:17:58 2024 ] 	Batch(1300/2353) done. Loss: 0.0062  lr:0.000010
[ Sun May 19 10:18:35 2024 ] 	Batch(1400/2353) done. Loss: 0.0618  lr:0.000010
[ Sun May 19 10:19:11 2024 ] 	Batch(1500/2353) done. Loss: 0.0718  lr:0.000010
[ Sun May 19 10:19:48 2024 ] 	Batch(1600/2353) done. Loss: 0.0089  lr:0.000010
[ Sun May 19 10:20:25 2024 ] 	Batch(1700/2353) done. Loss: 0.0181  lr:0.000010
[ Sun May 19 10:21:02 2024 ] 	Batch(1800/2353) done. Loss: 0.0186  lr:0.000010
[ Sun May 19 10:21:39 2024 ] 	Batch(1900/2353) done. Loss: 0.0250  lr:0.000010
[ Sun May 19 10:22:15 2024 ] 	Batch(2000/2353) done. Loss: 0.0097  lr:0.000010
[ Sun May 19 10:22:52 2024 ] 	Batch(2100/2353) done. Loss: 0.0697  lr:0.000010
[ Sun May 19 10:23:29 2024 ] 	Batch(2200/2353) done. Loss: 0.1064  lr:0.000010
[ Sun May 19 10:24:06 2024 ] 	Batch(2300/2353) done. Loss: 0.0281  lr:0.000010
[ Sun May 19 10:24:25 2024 ] 	Mean training loss: 0.0407.
[ Sun May 19 10:24:25 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 10:24:25 2024 ] Training epoch: 6
[ Sun May 19 10:24:26 2024 ] 	Batch(0/2353) done. Loss: 0.0893  lr:0.000010
[ Sun May 19 10:25:03 2024 ] 	Batch(100/2353) done. Loss: 0.0334  lr:0.000010
[ Sun May 19 10:25:39 2024 ] 	Batch(200/2353) done. Loss: 0.0767  lr:0.000010
[ Sun May 19 10:26:16 2024 ] 	Batch(300/2353) done. Loss: 0.0572  lr:0.000010
[ Sun May 19 10:26:53 2024 ] 	Batch(400/2353) done. Loss: 0.0355  lr:0.000010
[ Sun May 19 10:27:30 2024 ] 	Batch(500/2353) done. Loss: 0.0688  lr:0.000010
[ Sun May 19 10:28:06 2024 ] 	Batch(600/2353) done. Loss: 0.2363  lr:0.000010
[ Sun May 19 10:28:43 2024 ] 	Batch(700/2353) done. Loss: 0.0066  lr:0.000010
[ Sun May 19 10:29:20 2024 ] 	Batch(800/2353) done. Loss: 0.0723  lr:0.000010
[ Sun May 19 10:29:57 2024 ] 	Batch(900/2353) done. Loss: 0.0711  lr:0.000010
[ Sun May 19 10:30:33 2024 ] 	Batch(1000/2353) done. Loss: 0.0689  lr:0.000010
[ Sun May 19 10:31:10 2024 ] 	Batch(1100/2353) done. Loss: 0.0345  lr:0.000010
[ Sun May 19 10:31:47 2024 ] 	Batch(1200/2353) done. Loss: 0.0280  lr:0.000010
[ Sun May 19 10:32:24 2024 ] 	Batch(1300/2353) done. Loss: 0.0054  lr:0.000010
[ Sun May 19 10:33:00 2024 ] 	Batch(1400/2353) done. Loss: 0.0280  lr:0.000010
[ Sun May 19 10:33:37 2024 ] 	Batch(1500/2353) done. Loss: 0.0323  lr:0.000010
[ Sun May 19 10:34:14 2024 ] 	Batch(1600/2353) done. Loss: 0.0149  lr:0.000010
[ Sun May 19 10:34:51 2024 ] 	Batch(1700/2353) done. Loss: 0.0079  lr:0.000010
[ Sun May 19 10:35:27 2024 ] 	Batch(1800/2353) done. Loss: 0.0275  lr:0.000010
[ Sun May 19 10:36:04 2024 ] 	Batch(1900/2353) done. Loss: 0.0222  lr:0.000010
[ Sun May 19 10:36:41 2024 ] 	Batch(2000/2353) done. Loss: 0.0219  lr:0.000010
[ Sun May 19 10:37:17 2024 ] 	Batch(2100/2353) done. Loss: 0.0181  lr:0.000010
[ Sun May 19 10:37:54 2024 ] 	Batch(2200/2353) done. Loss: 0.0017  lr:0.000010
[ Sun May 19 10:38:31 2024 ] 	Batch(2300/2353) done. Loss: 0.0706  lr:0.000010
[ Sun May 19 10:38:50 2024 ] 	Mean training loss: 0.0417.
[ Sun May 19 10:38:50 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 10:38:50 2024 ] Training epoch: 7
[ Sun May 19 10:38:51 2024 ] 	Batch(0/2353) done. Loss: 0.0033  lr:0.000010
[ Sun May 19 10:39:27 2024 ] 	Batch(100/2353) done. Loss: 0.0492  lr:0.000010
[ Sun May 19 10:40:04 2024 ] 	Batch(200/2353) done. Loss: 0.1135  lr:0.000010
[ Sun May 19 10:40:41 2024 ] 	Batch(300/2353) done. Loss: 0.0092  lr:0.000010
[ Sun May 19 10:41:18 2024 ] 	Batch(400/2353) done. Loss: 0.0087  lr:0.000010
[ Sun May 19 10:41:54 2024 ] 	Batch(500/2353) done. Loss: 0.0675  lr:0.000010
[ Sun May 19 10:42:31 2024 ] 	Batch(600/2353) done. Loss: 0.0064  lr:0.000010
[ Sun May 19 10:43:08 2024 ] 	Batch(700/2353) done. Loss: 0.0043  lr:0.000010
[ Sun May 19 10:43:44 2024 ] 	Batch(800/2353) done. Loss: 0.1537  lr:0.000010
[ Sun May 19 10:44:21 2024 ] 	Batch(900/2353) done. Loss: 0.0322  lr:0.000010
[ Sun May 19 10:44:58 2024 ] 	Batch(1000/2353) done. Loss: 0.0194  lr:0.000010
[ Sun May 19 10:45:34 2024 ] 	Batch(1100/2353) done. Loss: 0.0023  lr:0.000010
[ Sun May 19 10:46:11 2024 ] 	Batch(1200/2353) done. Loss: 0.1072  lr:0.000010
[ Sun May 19 10:46:48 2024 ] 	Batch(1300/2353) done. Loss: 0.1262  lr:0.000010
[ Sun May 19 10:47:25 2024 ] 	Batch(1400/2353) done. Loss: 0.0313  lr:0.000010
[ Sun May 19 10:48:01 2024 ] 	Batch(1500/2353) done. Loss: 0.0357  lr:0.000010
[ Sun May 19 10:48:38 2024 ] 	Batch(1600/2353) done. Loss: 0.0068  lr:0.000010
[ Sun May 19 10:49:15 2024 ] 	Batch(1700/2353) done. Loss: 0.0684  lr:0.000010
[ Sun May 19 10:49:52 2024 ] 	Batch(1800/2353) done. Loss: 0.1196  lr:0.000010
[ Sun May 19 10:50:28 2024 ] 	Batch(1900/2353) done. Loss: 0.0164  lr:0.000010
[ Sun May 19 10:51:05 2024 ] 	Batch(2000/2353) done. Loss: 0.0113  lr:0.000010
[ Sun May 19 10:51:42 2024 ] 	Batch(2100/2353) done. Loss: 0.0059  lr:0.000010
[ Sun May 19 10:52:19 2024 ] 	Batch(2200/2353) done. Loss: 0.1260  lr:0.000010
[ Sun May 19 10:52:56 2024 ] 	Batch(2300/2353) done. Loss: 0.0154  lr:0.000010
[ Sun May 19 10:53:15 2024 ] 	Mean training loss: 0.0400.
[ Sun May 19 10:53:15 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 10:53:15 2024 ] Training epoch: 8
[ Sun May 19 10:53:16 2024 ] 	Batch(0/2353) done. Loss: 0.0327  lr:0.000010
[ Sun May 19 10:53:52 2024 ] 	Batch(100/2353) done. Loss: 0.0461  lr:0.000010
[ Sun May 19 10:54:29 2024 ] 	Batch(200/2353) done. Loss: 0.0318  lr:0.000010
[ Sun May 19 10:55:06 2024 ] 	Batch(300/2353) done. Loss: 0.0093  lr:0.000010
[ Sun May 19 10:55:43 2024 ] 	Batch(400/2353) done. Loss: 0.0076  lr:0.000010
[ Sun May 19 10:56:20 2024 ] 	Batch(500/2353) done. Loss: 0.0064  lr:0.000010
[ Sun May 19 10:56:57 2024 ] 	Batch(600/2353) done. Loss: 0.0030  lr:0.000010
[ Sun May 19 10:57:34 2024 ] 	Batch(700/2353) done. Loss: 0.0103  lr:0.000010
[ Sun May 19 10:58:11 2024 ] 	Batch(800/2353) done. Loss: 0.0269  lr:0.000010
[ Sun May 19 10:58:48 2024 ] 	Batch(900/2353) done. Loss: 0.0664  lr:0.000010
[ Sun May 19 10:59:24 2024 ] 	Batch(1000/2353) done. Loss: 0.0066  lr:0.000010
[ Sun May 19 11:00:01 2024 ] 	Batch(1100/2353) done. Loss: 0.0561  lr:0.000010
[ Sun May 19 11:00:38 2024 ] 	Batch(1200/2353) done. Loss: 0.0369  lr:0.000010
[ Sun May 19 11:01:15 2024 ] 	Batch(1300/2353) done. Loss: 0.0317  lr:0.000010
[ Sun May 19 11:01:52 2024 ] 	Batch(1400/2353) done. Loss: 0.0126  lr:0.000010
[ Sun May 19 11:02:29 2024 ] 	Batch(1500/2353) done. Loss: 0.0729  lr:0.000010
[ Sun May 19 11:03:06 2024 ] 	Batch(1600/2353) done. Loss: 0.0133  lr:0.000010
[ Sun May 19 11:03:43 2024 ] 	Batch(1700/2353) done. Loss: 0.0159  lr:0.000010
[ Sun May 19 11:04:20 2024 ] 	Batch(1800/2353) done. Loss: 0.0050  lr:0.000010
[ Sun May 19 11:04:57 2024 ] 	Batch(1900/2353) done. Loss: 0.0594  lr:0.000010
[ Sun May 19 11:05:34 2024 ] 	Batch(2000/2353) done. Loss: 0.0131  lr:0.000010
[ Sun May 19 11:06:11 2024 ] 	Batch(2100/2353) done. Loss: 0.0195  lr:0.000010
[ Sun May 19 11:06:48 2024 ] 	Batch(2200/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 11:07:25 2024 ] 	Batch(2300/2353) done. Loss: 0.0012  lr:0.000010
[ Sun May 19 11:07:44 2024 ] 	Mean training loss: 0.0407.
[ Sun May 19 11:07:44 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 11:07:44 2024 ] Training epoch: 9
[ Sun May 19 11:07:45 2024 ] 	Batch(0/2353) done. Loss: 0.0242  lr:0.000010
[ Sun May 19 11:08:22 2024 ] 	Batch(100/2353) done. Loss: 0.0383  lr:0.000010
[ Sun May 19 11:08:59 2024 ] 	Batch(200/2353) done. Loss: 0.0778  lr:0.000010
[ Sun May 19 11:09:36 2024 ] 	Batch(300/2353) done. Loss: 0.1165  lr:0.000010
[ Sun May 19 11:10:13 2024 ] 	Batch(400/2353) done. Loss: 0.0132  lr:0.000010
[ Sun May 19 11:10:50 2024 ] 	Batch(500/2353) done. Loss: 0.0149  lr:0.000010
[ Sun May 19 11:11:27 2024 ] 	Batch(600/2353) done. Loss: 0.0081  lr:0.000010
[ Sun May 19 11:12:04 2024 ] 	Batch(700/2353) done. Loss: 0.0680  lr:0.000010
[ Sun May 19 11:12:41 2024 ] 	Batch(800/2353) done. Loss: 0.0274  lr:0.000010
[ Sun May 19 11:13:18 2024 ] 	Batch(900/2353) done. Loss: 0.0919  lr:0.000010
[ Sun May 19 11:13:55 2024 ] 	Batch(1000/2353) done. Loss: 0.0079  lr:0.000010
[ Sun May 19 11:14:32 2024 ] 	Batch(1100/2353) done. Loss: 0.2149  lr:0.000010
[ Sun May 19 11:15:09 2024 ] 	Batch(1200/2353) done. Loss: 0.0996  lr:0.000010
[ Sun May 19 11:15:46 2024 ] 	Batch(1300/2353) done. Loss: 0.0526  lr:0.000010
[ Sun May 19 11:16:23 2024 ] 	Batch(1400/2353) done. Loss: 0.0268  lr:0.000010
[ Sun May 19 11:17:00 2024 ] 	Batch(1500/2353) done. Loss: 0.0693  lr:0.000010
[ Sun May 19 11:17:37 2024 ] 	Batch(1600/2353) done. Loss: 0.0393  lr:0.000010
[ Sun May 19 11:18:14 2024 ] 	Batch(1700/2353) done. Loss: 0.0029  lr:0.000010
[ Sun May 19 11:18:51 2024 ] 	Batch(1800/2353) done. Loss: 0.0120  lr:0.000010
[ Sun May 19 11:19:28 2024 ] 	Batch(1900/2353) done. Loss: 0.0052  lr:0.000010
[ Sun May 19 11:20:05 2024 ] 	Batch(2000/2353) done. Loss: 0.0151  lr:0.000010
[ Sun May 19 11:20:42 2024 ] 	Batch(2100/2353) done. Loss: 0.0162  lr:0.000010
[ Sun May 19 11:21:19 2024 ] 	Batch(2200/2353) done. Loss: 0.0235  lr:0.000010
[ Sun May 19 11:21:56 2024 ] 	Batch(2300/2353) done. Loss: 0.0983  lr:0.000010
[ Sun May 19 11:22:15 2024 ] 	Mean training loss: 0.0410.
[ Sun May 19 11:22:15 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 11:22:15 2024 ] Training epoch: 10
[ Sun May 19 11:22:16 2024 ] 	Batch(0/2353) done. Loss: 0.0405  lr:0.000010
[ Sun May 19 11:22:53 2024 ] 	Batch(100/2353) done. Loss: 0.0103  lr:0.000010
[ Sun May 19 11:23:30 2024 ] 	Batch(200/2353) done. Loss: 0.0451  lr:0.000010
[ Sun May 19 11:24:07 2024 ] 	Batch(300/2353) done. Loss: 0.0063  lr:0.000010
[ Sun May 19 11:24:44 2024 ] 	Batch(400/2353) done. Loss: 0.0237  lr:0.000010
[ Sun May 19 11:25:21 2024 ] 	Batch(500/2353) done. Loss: 0.0253  lr:0.000010
[ Sun May 19 11:25:58 2024 ] 	Batch(600/2353) done. Loss: 0.0039  lr:0.000010
[ Sun May 19 11:26:35 2024 ] 	Batch(700/2353) done. Loss: 0.0691  lr:0.000010
[ Sun May 19 11:27:13 2024 ] 	Batch(800/2353) done. Loss: 0.0145  lr:0.000010
[ Sun May 19 11:27:50 2024 ] 	Batch(900/2353) done. Loss: 0.0060  lr:0.000010
[ Sun May 19 11:28:27 2024 ] 	Batch(1000/2353) done. Loss: 0.0483  lr:0.000010
[ Sun May 19 11:29:03 2024 ] 	Batch(1100/2353) done. Loss: 0.0653  lr:0.000010
[ Sun May 19 11:29:40 2024 ] 	Batch(1200/2353) done. Loss: 0.0069  lr:0.000010
[ Sun May 19 11:30:17 2024 ] 	Batch(1300/2353) done. Loss: 0.0707  lr:0.000010
[ Sun May 19 11:30:54 2024 ] 	Batch(1400/2353) done. Loss: 0.0175  lr:0.000010
[ Sun May 19 11:31:31 2024 ] 	Batch(1500/2353) done. Loss: 0.0407  lr:0.000010
[ Sun May 19 11:32:08 2024 ] 	Batch(1600/2353) done. Loss: 0.0127  lr:0.000010
[ Sun May 19 11:32:45 2024 ] 	Batch(1700/2353) done. Loss: 0.0466  lr:0.000010
[ Sun May 19 11:33:22 2024 ] 	Batch(1800/2353) done. Loss: 0.0091  lr:0.000010
[ Sun May 19 11:33:59 2024 ] 	Batch(1900/2353) done. Loss: 0.0317  lr:0.000010
[ Sun May 19 11:34:36 2024 ] 	Batch(2000/2353) done. Loss: 0.0175  lr:0.000010
[ Sun May 19 11:35:13 2024 ] 	Batch(2100/2353) done. Loss: 0.0666  lr:0.000010
[ Sun May 19 11:35:50 2024 ] 	Batch(2200/2353) done. Loss: 0.0647  lr:0.000010
[ Sun May 19 11:36:27 2024 ] 	Batch(2300/2353) done. Loss: 0.1185  lr:0.000010
[ Sun May 19 11:36:46 2024 ] 	Mean training loss: 0.0387.
[ Sun May 19 11:36:46 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 11:36:46 2024 ] Eval epoch: 10
[ Sun May 19 11:38:50 2024 ] 	Mean val loss of 2367 batches: 0.2459708900296413.
[ Sun May 19 11:38:50 2024 ] Training epoch: 11
[ Sun May 19 11:38:51 2024 ] 	Batch(0/2353) done. Loss: 0.1588  lr:0.000010
[ Sun May 19 11:39:28 2024 ] 	Batch(100/2353) done. Loss: 0.0092  lr:0.000010
[ Sun May 19 11:40:04 2024 ] 	Batch(200/2353) done. Loss: 0.0084  lr:0.000010
[ Sun May 19 11:40:41 2024 ] 	Batch(300/2353) done. Loss: 0.1119  lr:0.000010
[ Sun May 19 11:41:18 2024 ] 	Batch(400/2353) done. Loss: 0.0089  lr:0.000010
[ Sun May 19 11:41:55 2024 ] 	Batch(500/2353) done. Loss: 0.0570  lr:0.000010
[ Sun May 19 11:42:32 2024 ] 	Batch(600/2353) done. Loss: 0.0020  lr:0.000010
[ Sun May 19 11:43:09 2024 ] 	Batch(700/2353) done. Loss: 0.0105  lr:0.000010
[ Sun May 19 11:43:46 2024 ] 	Batch(800/2353) done. Loss: 0.0217  lr:0.000010
[ Sun May 19 11:44:23 2024 ] 	Batch(900/2353) done. Loss: 0.0120  lr:0.000010
[ Sun May 19 11:44:59 2024 ] 	Batch(1000/2353) done. Loss: 0.0118  lr:0.000010
[ Sun May 19 11:45:36 2024 ] 	Batch(1100/2353) done. Loss: 0.0143  lr:0.000010
[ Sun May 19 11:46:13 2024 ] 	Batch(1200/2353) done. Loss: 0.0395  lr:0.000010
[ Sun May 19 11:46:51 2024 ] 	Batch(1300/2353) done. Loss: 0.0301  lr:0.000010
[ Sun May 19 11:47:28 2024 ] 	Batch(1400/2353) done. Loss: 0.0348  lr:0.000010
[ Sun May 19 11:48:06 2024 ] 	Batch(1500/2353) done. Loss: 0.0962  lr:0.000010
[ Sun May 19 11:48:43 2024 ] 	Batch(1600/2353) done. Loss: 0.0151  lr:0.000010
[ Sun May 19 11:49:20 2024 ] 	Batch(1700/2353) done. Loss: 0.0075  lr:0.000010
[ Sun May 19 11:49:57 2024 ] 	Batch(1800/2353) done. Loss: 0.0767  lr:0.000010
[ Sun May 19 11:50:34 2024 ] 	Batch(1900/2353) done. Loss: 0.0916  lr:0.000010
[ Sun May 19 11:51:11 2024 ] 	Batch(2000/2353) done. Loss: 0.0191  lr:0.000010
[ Sun May 19 11:51:47 2024 ] 	Batch(2100/2353) done. Loss: 0.0194  lr:0.000010
[ Sun May 19 11:52:24 2024 ] 	Batch(2200/2353) done. Loss: 0.0200  lr:0.000010
[ Sun May 19 11:53:01 2024 ] 	Batch(2300/2353) done. Loss: 0.0465  lr:0.000010
[ Sun May 19 11:53:20 2024 ] 	Mean training loss: 0.0414.
[ Sun May 19 11:53:20 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 11:53:20 2024 ] Training epoch: 12
[ Sun May 19 11:53:21 2024 ] 	Batch(0/2353) done. Loss: 0.0123  lr:0.000010
[ Sun May 19 11:53:58 2024 ] 	Batch(100/2353) done. Loss: 0.0273  lr:0.000010
[ Sun May 19 11:54:34 2024 ] 	Batch(200/2353) done. Loss: 0.0173  lr:0.000010
[ Sun May 19 11:55:11 2024 ] 	Batch(300/2353) done. Loss: 0.0124  lr:0.000010
[ Sun May 19 11:55:48 2024 ] 	Batch(400/2353) done. Loss: 0.1018  lr:0.000010
[ Sun May 19 11:56:25 2024 ] 	Batch(500/2353) done. Loss: 0.1701  lr:0.000010
[ Sun May 19 11:57:02 2024 ] 	Batch(600/2353) done. Loss: 0.0262  lr:0.000010
[ Sun May 19 11:57:39 2024 ] 	Batch(700/2353) done. Loss: 0.0510  lr:0.000010
[ Sun May 19 11:58:17 2024 ] 	Batch(800/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 11:58:53 2024 ] 	Batch(900/2353) done. Loss: 0.0369  lr:0.000010
[ Sun May 19 11:59:30 2024 ] 	Batch(1000/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 12:00:07 2024 ] 	Batch(1100/2353) done. Loss: 0.0066  lr:0.000010
[ Sun May 19 12:00:44 2024 ] 	Batch(1200/2353) done. Loss: 0.0237  lr:0.000010
[ Sun May 19 12:01:21 2024 ] 	Batch(1300/2353) done. Loss: 0.0424  lr:0.000010
[ Sun May 19 12:01:58 2024 ] 	Batch(1400/2353) done. Loss: 0.0249  lr:0.000010
[ Sun May 19 12:02:34 2024 ] 	Batch(1500/2353) done. Loss: 0.0093  lr:0.000010
[ Sun May 19 12:03:11 2024 ] 	Batch(1600/2353) done. Loss: 0.1353  lr:0.000010
[ Sun May 19 12:03:48 2024 ] 	Batch(1700/2353) done. Loss: 0.0912  lr:0.000010
[ Sun May 19 12:04:25 2024 ] 	Batch(1800/2353) done. Loss: 0.0619  lr:0.000010
[ Sun May 19 12:05:02 2024 ] 	Batch(1900/2353) done. Loss: 0.0666  lr:0.000010
[ Sun May 19 12:05:39 2024 ] 	Batch(2000/2353) done. Loss: 0.0436  lr:0.000010
[ Sun May 19 12:06:15 2024 ] 	Batch(2100/2353) done. Loss: 0.0545  lr:0.000010
[ Sun May 19 12:06:52 2024 ] 	Batch(2200/2353) done. Loss: 0.0038  lr:0.000010
[ Sun May 19 12:07:29 2024 ] 	Batch(2300/2353) done. Loss: 0.0593  lr:0.000010
[ Sun May 19 12:07:48 2024 ] 	Mean training loss: 0.0403.
[ Sun May 19 12:07:48 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 12:07:48 2024 ] Training epoch: 13
[ Sun May 19 12:07:49 2024 ] 	Batch(0/2353) done. Loss: 0.0202  lr:0.000010
[ Sun May 19 12:08:26 2024 ] 	Batch(100/2353) done. Loss: 0.0431  lr:0.000010
[ Sun May 19 12:09:03 2024 ] 	Batch(200/2353) done. Loss: 0.0042  lr:0.000010
[ Sun May 19 12:09:39 2024 ] 	Batch(300/2353) done. Loss: 0.0702  lr:0.000010
[ Sun May 19 12:10:16 2024 ] 	Batch(400/2353) done. Loss: 0.0457  lr:0.000010
[ Sun May 19 12:10:53 2024 ] 	Batch(500/2353) done. Loss: 0.0527  lr:0.000010
[ Sun May 19 12:11:31 2024 ] 	Batch(600/2353) done. Loss: 0.1883  lr:0.000010
[ Sun May 19 12:12:08 2024 ] 	Batch(700/2353) done. Loss: 0.0089  lr:0.000010
[ Sun May 19 12:12:45 2024 ] 	Batch(800/2353) done. Loss: 0.0140  lr:0.000010
[ Sun May 19 12:13:22 2024 ] 	Batch(900/2353) done. Loss: 0.0340  lr:0.000010
[ Sun May 19 12:13:59 2024 ] 	Batch(1000/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 12:14:36 2024 ] 	Batch(1100/2353) done. Loss: 0.0421  lr:0.000010
[ Sun May 19 12:15:13 2024 ] 	Batch(1200/2353) done. Loss: 0.0073  lr:0.000010
[ Sun May 19 12:15:50 2024 ] 	Batch(1300/2353) done. Loss: 0.0107  lr:0.000010
[ Sun May 19 12:16:26 2024 ] 	Batch(1400/2353) done. Loss: 0.1454  lr:0.000010
[ Sun May 19 12:17:03 2024 ] 	Batch(1500/2353) done. Loss: 0.0979  lr:0.000010
[ Sun May 19 12:17:40 2024 ] 	Batch(1600/2353) done. Loss: 0.0230  lr:0.000010
[ Sun May 19 12:18:17 2024 ] 	Batch(1700/2353) done. Loss: 0.0113  lr:0.000010
[ Sun May 19 12:18:54 2024 ] 	Batch(1800/2353) done. Loss: 0.0602  lr:0.000010
[ Sun May 19 12:19:31 2024 ] 	Batch(1900/2353) done. Loss: 0.0282  lr:0.000010
[ Sun May 19 12:20:08 2024 ] 	Batch(2000/2353) done. Loss: 0.0308  lr:0.000010
[ Sun May 19 12:20:45 2024 ] 	Batch(2100/2353) done. Loss: 0.0268  lr:0.000010
[ Sun May 19 12:21:21 2024 ] 	Batch(2200/2353) done. Loss: 0.0221  lr:0.000010
[ Sun May 19 12:21:58 2024 ] 	Batch(2300/2353) done. Loss: 0.0405  lr:0.000010
[ Sun May 19 12:22:18 2024 ] 	Mean training loss: 0.0387.
[ Sun May 19 12:22:18 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 12:22:18 2024 ] Training epoch: 14
[ Sun May 19 12:22:18 2024 ] 	Batch(0/2353) done. Loss: 0.0059  lr:0.000010
[ Sun May 19 12:22:55 2024 ] 	Batch(100/2353) done. Loss: 0.0260  lr:0.000010
[ Sun May 19 12:23:32 2024 ] 	Batch(200/2353) done. Loss: 0.1057  lr:0.000010
[ Sun May 19 12:24:09 2024 ] 	Batch(300/2353) done. Loss: 0.1701  lr:0.000010
[ Sun May 19 12:24:46 2024 ] 	Batch(400/2353) done. Loss: 0.0205  lr:0.000010
[ Sun May 19 12:25:23 2024 ] 	Batch(500/2353) done. Loss: 0.0421  lr:0.000010
[ Sun May 19 12:26:00 2024 ] 	Batch(600/2353) done. Loss: 0.2016  lr:0.000010
[ Sun May 19 12:26:37 2024 ] 	Batch(700/2353) done. Loss: 0.0146  lr:0.000010
[ Sun May 19 12:27:14 2024 ] 	Batch(800/2353) done. Loss: 0.0607  lr:0.000010
[ Sun May 19 12:27:51 2024 ] 	Batch(900/2353) done. Loss: 0.0095  lr:0.000010
[ Sun May 19 12:28:28 2024 ] 	Batch(1000/2353) done. Loss: 0.0444  lr:0.000010
[ Sun May 19 12:29:04 2024 ] 	Batch(1100/2353) done. Loss: 0.0579  lr:0.000010
[ Sun May 19 12:29:41 2024 ] 	Batch(1200/2353) done. Loss: 0.0790  lr:0.000010
[ Sun May 19 12:30:18 2024 ] 	Batch(1300/2353) done. Loss: 0.1083  lr:0.000010
[ Sun May 19 12:30:55 2024 ] 	Batch(1400/2353) done. Loss: 0.0513  lr:0.000010
[ Sun May 19 12:31:32 2024 ] 	Batch(1500/2353) done. Loss: 0.0077  lr:0.000010
[ Sun May 19 12:32:09 2024 ] 	Batch(1600/2353) done. Loss: 0.0449  lr:0.000010
[ Sun May 19 12:32:46 2024 ] 	Batch(1700/2353) done. Loss: 0.0296  lr:0.000010
[ Sun May 19 12:33:23 2024 ] 	Batch(1800/2353) done. Loss: 0.0290  lr:0.000010
[ Sun May 19 12:34:00 2024 ] 	Batch(1900/2353) done. Loss: 0.0084  lr:0.000010
[ Sun May 19 12:34:37 2024 ] 	Batch(2000/2353) done. Loss: 0.0256  lr:0.000010
[ Sun May 19 12:35:13 2024 ] 	Batch(2100/2353) done. Loss: 0.0039  lr:0.000010
[ Sun May 19 12:35:50 2024 ] 	Batch(2200/2353) done. Loss: 0.0457  lr:0.000010
[ Sun May 19 12:36:27 2024 ] 	Batch(2300/2353) done. Loss: 0.0796  lr:0.000010
[ Sun May 19 12:36:46 2024 ] 	Mean training loss: 0.0395.
[ Sun May 19 12:36:46 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 12:36:46 2024 ] Training epoch: 15
[ Sun May 19 12:36:47 2024 ] 	Batch(0/2353) done. Loss: 0.0448  lr:0.000010
[ Sun May 19 12:37:24 2024 ] 	Batch(100/2353) done. Loss: 0.0170  lr:0.000010
[ Sun May 19 12:38:01 2024 ] 	Batch(200/2353) done. Loss: 0.0095  lr:0.000010
[ Sun May 19 12:38:38 2024 ] 	Batch(300/2353) done. Loss: 0.0128  lr:0.000010
[ Sun May 19 12:39:15 2024 ] 	Batch(400/2353) done. Loss: 0.0244  lr:0.000010
[ Sun May 19 12:39:51 2024 ] 	Batch(500/2353) done. Loss: 0.0957  lr:0.000010
[ Sun May 19 12:40:28 2024 ] 	Batch(600/2353) done. Loss: 0.0198  lr:0.000010
[ Sun May 19 12:41:05 2024 ] 	Batch(700/2353) done. Loss: 0.0116  lr:0.000010
[ Sun May 19 12:41:42 2024 ] 	Batch(800/2353) done. Loss: 0.0189  lr:0.000010
[ Sun May 19 12:42:19 2024 ] 	Batch(900/2353) done. Loss: 0.0144  lr:0.000010
[ Sun May 19 12:42:56 2024 ] 	Batch(1000/2353) done. Loss: 0.0425  lr:0.000010
[ Sun May 19 12:43:33 2024 ] 	Batch(1100/2353) done. Loss: 0.0235  lr:0.000010
[ Sun May 19 12:44:10 2024 ] 	Batch(1200/2353) done. Loss: 0.0148  lr:0.000010
[ Sun May 19 12:44:47 2024 ] 	Batch(1300/2353) done. Loss: 0.0323  lr:0.000010
[ Sun May 19 12:45:24 2024 ] 	Batch(1400/2353) done. Loss: 0.0200  lr:0.000010
[ Sun May 19 12:46:01 2024 ] 	Batch(1500/2353) done. Loss: 0.0087  lr:0.000010
[ Sun May 19 12:46:38 2024 ] 	Batch(1600/2353) done. Loss: 0.0107  lr:0.000010
[ Sun May 19 12:47:15 2024 ] 	Batch(1700/2353) done. Loss: 0.0044  lr:0.000010
[ Sun May 19 12:47:52 2024 ] 	Batch(1800/2353) done. Loss: 0.0361  lr:0.000010
[ Sun May 19 12:48:29 2024 ] 	Batch(1900/2353) done. Loss: 0.0896  lr:0.000010
[ Sun May 19 12:49:06 2024 ] 	Batch(2000/2353) done. Loss: 0.0295  lr:0.000010
[ Sun May 19 12:49:43 2024 ] 	Batch(2100/2353) done. Loss: 0.0274  lr:0.000010
[ Sun May 19 12:50:20 2024 ] 	Batch(2200/2353) done. Loss: 0.0279  lr:0.000010
[ Sun May 19 12:50:57 2024 ] 	Batch(2300/2353) done. Loss: 0.0120  lr:0.000010
[ Sun May 19 12:51:16 2024 ] 	Mean training loss: 0.0401.
[ Sun May 19 12:51:16 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 12:51:16 2024 ] Training epoch: 16
[ Sun May 19 12:51:17 2024 ] 	Batch(0/2353) done. Loss: 0.1195  lr:0.000010
[ Sun May 19 12:51:54 2024 ] 	Batch(100/2353) done. Loss: 0.0922  lr:0.000010
[ Sun May 19 12:52:31 2024 ] 	Batch(200/2353) done. Loss: 0.0170  lr:0.000010
[ Sun May 19 12:53:08 2024 ] 	Batch(300/2353) done. Loss: 0.0204  lr:0.000010
[ Sun May 19 12:53:45 2024 ] 	Batch(400/2353) done. Loss: 0.0752  lr:0.000010
[ Sun May 19 12:54:21 2024 ] 	Batch(500/2353) done. Loss: 0.0025  lr:0.000010
[ Sun May 19 12:54:59 2024 ] 	Batch(600/2353) done. Loss: 0.1074  lr:0.000010
[ Sun May 19 12:55:36 2024 ] 	Batch(700/2353) done. Loss: 0.0312  lr:0.000010
[ Sun May 19 12:56:13 2024 ] 	Batch(800/2353) done. Loss: 0.0578  lr:0.000010
[ Sun May 19 12:56:50 2024 ] 	Batch(900/2353) done. Loss: 0.0788  lr:0.000010
[ Sun May 19 12:57:27 2024 ] 	Batch(1000/2353) done. Loss: 0.0746  lr:0.000010
[ Sun May 19 12:58:04 2024 ] 	Batch(1100/2353) done. Loss: 0.0048  lr:0.000010
[ Sun May 19 12:58:41 2024 ] 	Batch(1200/2353) done. Loss: 0.0590  lr:0.000010
[ Sun May 19 12:59:18 2024 ] 	Batch(1300/2353) done. Loss: 0.0156  lr:0.000010
[ Sun May 19 12:59:54 2024 ] 	Batch(1400/2353) done. Loss: 0.0244  lr:0.000010
[ Sun May 19 13:00:31 2024 ] 	Batch(1500/2353) done. Loss: 0.0407  lr:0.000010
[ Sun May 19 13:01:08 2024 ] 	Batch(1600/2353) done. Loss: 0.0507  lr:0.000010
[ Sun May 19 13:01:45 2024 ] 	Batch(1700/2353) done. Loss: 0.0226  lr:0.000010
[ Sun May 19 13:02:22 2024 ] 	Batch(1800/2353) done. Loss: 0.1078  lr:0.000010
[ Sun May 19 13:02:59 2024 ] 	Batch(1900/2353) done. Loss: 0.0063  lr:0.000010
[ Sun May 19 13:03:36 2024 ] 	Batch(2000/2353) done. Loss: 0.0611  lr:0.000010
[ Sun May 19 13:04:13 2024 ] 	Batch(2100/2353) done. Loss: 0.0199  lr:0.000010
[ Sun May 19 13:04:50 2024 ] 	Batch(2200/2353) done. Loss: 0.1108  lr:0.000010
[ Sun May 19 13:05:27 2024 ] 	Batch(2300/2353) done. Loss: 0.0027  lr:0.000010
[ Sun May 19 13:05:46 2024 ] 	Mean training loss: 0.0368.
[ Sun May 19 13:05:46 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 13:05:46 2024 ] Training epoch: 17
[ Sun May 19 13:05:47 2024 ] 	Batch(0/2353) done. Loss: 0.0565  lr:0.000010
[ Sun May 19 13:06:23 2024 ] 	Batch(100/2353) done. Loss: 0.0129  lr:0.000010
[ Sun May 19 13:07:00 2024 ] 	Batch(200/2353) done. Loss: 0.0418  lr:0.000010
[ Sun May 19 13:07:37 2024 ] 	Batch(300/2353) done. Loss: 0.0354  lr:0.000010
[ Sun May 19 13:08:14 2024 ] 	Batch(400/2353) done. Loss: 0.0598  lr:0.000010
[ Sun May 19 13:08:51 2024 ] 	Batch(500/2353) done. Loss: 0.1055  lr:0.000010
[ Sun May 19 13:09:28 2024 ] 	Batch(600/2353) done. Loss: 0.0474  lr:0.000010
[ Sun May 19 13:10:05 2024 ] 	Batch(700/2353) done. Loss: 0.0073  lr:0.000010
[ Sun May 19 13:10:42 2024 ] 	Batch(800/2353) done. Loss: 0.0399  lr:0.000010
[ Sun May 19 13:11:19 2024 ] 	Batch(900/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 13:11:56 2024 ] 	Batch(1000/2353) done. Loss: 0.1297  lr:0.000010
[ Sun May 19 13:12:33 2024 ] 	Batch(1100/2353) done. Loss: 0.0388  lr:0.000010
[ Sun May 19 13:13:10 2024 ] 	Batch(1200/2353) done. Loss: 0.0383  lr:0.000010
[ Sun May 19 13:13:47 2024 ] 	Batch(1300/2353) done. Loss: 0.0058  lr:0.000010
[ Sun May 19 13:14:24 2024 ] 	Batch(1400/2353) done. Loss: 0.0614  lr:0.000010
[ Sun May 19 13:15:00 2024 ] 	Batch(1500/2353) done. Loss: 0.0142  lr:0.000010
[ Sun May 19 13:15:37 2024 ] 	Batch(1600/2353) done. Loss: 0.0166  lr:0.000010
[ Sun May 19 13:16:14 2024 ] 	Batch(1700/2353) done. Loss: 0.0083  lr:0.000010
[ Sun May 19 13:16:51 2024 ] 	Batch(1800/2353) done. Loss: 0.0576  lr:0.000010
[ Sun May 19 13:17:28 2024 ] 	Batch(1900/2353) done. Loss: 0.3574  lr:0.000010
[ Sun May 19 13:18:05 2024 ] 	Batch(2000/2353) done. Loss: 0.0497  lr:0.000010
[ Sun May 19 13:18:42 2024 ] 	Batch(2100/2353) done. Loss: 0.0244  lr:0.000010
[ Sun May 19 13:19:19 2024 ] 	Batch(2200/2353) done. Loss: 0.0080  lr:0.000010
[ Sun May 19 13:19:56 2024 ] 	Batch(2300/2353) done. Loss: 0.0130  lr:0.000010
[ Sun May 19 13:20:15 2024 ] 	Mean training loss: 0.0409.
[ Sun May 19 13:20:15 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 13:20:15 2024 ] Training epoch: 18
[ Sun May 19 13:20:16 2024 ] 	Batch(0/2353) done. Loss: 0.0075  lr:0.000010
[ Sun May 19 13:20:53 2024 ] 	Batch(100/2353) done. Loss: 0.0173  lr:0.000010
[ Sun May 19 13:21:30 2024 ] 	Batch(200/2353) done. Loss: 0.0304  lr:0.000010
[ Sun May 19 13:22:07 2024 ] 	Batch(300/2353) done. Loss: 0.0965  lr:0.000010
[ Sun May 19 13:22:44 2024 ] 	Batch(400/2353) done. Loss: 0.0138  lr:0.000010
[ Sun May 19 13:23:20 2024 ] 	Batch(500/2353) done. Loss: 0.0111  lr:0.000010
[ Sun May 19 13:23:57 2024 ] 	Batch(600/2353) done. Loss: 0.0148  lr:0.000010
[ Sun May 19 13:24:34 2024 ] 	Batch(700/2353) done. Loss: 0.0212  lr:0.000010
[ Sun May 19 13:25:11 2024 ] 	Batch(800/2353) done. Loss: 0.1130  lr:0.000010
[ Sun May 19 13:25:48 2024 ] 	Batch(900/2353) done. Loss: 0.0488  lr:0.000010
[ Sun May 19 13:26:25 2024 ] 	Batch(1000/2353) done. Loss: 0.0162  lr:0.000010
[ Sun May 19 13:27:02 2024 ] 	Batch(1100/2353) done. Loss: 0.0065  lr:0.000010
[ Sun May 19 13:27:39 2024 ] 	Batch(1200/2353) done. Loss: 0.0674  lr:0.000010
[ Sun May 19 13:28:16 2024 ] 	Batch(1300/2353) done. Loss: 0.0383  lr:0.000010
[ Sun May 19 13:28:53 2024 ] 	Batch(1400/2353) done. Loss: 0.0109  lr:0.000010
[ Sun May 19 13:29:30 2024 ] 	Batch(1500/2353) done. Loss: 0.0337  lr:0.000010
[ Sun May 19 13:30:07 2024 ] 	Batch(1600/2353) done. Loss: 0.0390  lr:0.000010
[ Sun May 19 13:30:43 2024 ] 	Batch(1700/2353) done. Loss: 0.0321  lr:0.000010
[ Sun May 19 13:31:20 2024 ] 	Batch(1800/2353) done. Loss: 0.0173  lr:0.000010
[ Sun May 19 13:31:57 2024 ] 	Batch(1900/2353) done. Loss: 0.0173  lr:0.000010
[ Sun May 19 13:32:34 2024 ] 	Batch(2000/2353) done. Loss: 0.0321  lr:0.000010
[ Sun May 19 13:33:11 2024 ] 	Batch(2100/2353) done. Loss: 0.0310  lr:0.000010
[ Sun May 19 13:33:48 2024 ] 	Batch(2200/2353) done. Loss: 0.0271  lr:0.000010
[ Sun May 19 13:34:25 2024 ] 	Batch(2300/2353) done. Loss: 0.0091  lr:0.000010
[ Sun May 19 13:34:44 2024 ] 	Mean training loss: 0.0381.
[ Sun May 19 13:34:44 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 13:34:44 2024 ] Training epoch: 19
[ Sun May 19 13:34:45 2024 ] 	Batch(0/2353) done. Loss: 0.0626  lr:0.000010
[ Sun May 19 13:35:22 2024 ] 	Batch(100/2353) done. Loss: 0.0336  lr:0.000010
[ Sun May 19 13:35:59 2024 ] 	Batch(200/2353) done. Loss: 0.0106  lr:0.000010
[ Sun May 19 13:36:36 2024 ] 	Batch(300/2353) done. Loss: 0.0086  lr:0.000010
[ Sun May 19 13:37:13 2024 ] 	Batch(400/2353) done. Loss: 0.0123  lr:0.000010
[ Sun May 19 13:37:50 2024 ] 	Batch(500/2353) done. Loss: 0.0278  lr:0.000010
[ Sun May 19 13:38:26 2024 ] 	Batch(600/2353) done. Loss: 0.0070  lr:0.000010
[ Sun May 19 13:39:03 2024 ] 	Batch(700/2353) done. Loss: 0.0153  lr:0.000010
[ Sun May 19 13:39:40 2024 ] 	Batch(800/2353) done. Loss: 0.0359  lr:0.000010
[ Sun May 19 13:40:17 2024 ] 	Batch(900/2353) done. Loss: 0.0582  lr:0.000010
[ Sun May 19 13:40:54 2024 ] 	Batch(1000/2353) done. Loss: 0.0054  lr:0.000010
[ Sun May 19 13:41:31 2024 ] 	Batch(1100/2353) done. Loss: 0.0775  lr:0.000010
[ Sun May 19 13:42:08 2024 ] 	Batch(1200/2353) done. Loss: 0.0040  lr:0.000010
[ Sun May 19 13:42:45 2024 ] 	Batch(1300/2353) done. Loss: 0.0083  lr:0.000010
[ Sun May 19 13:43:22 2024 ] 	Batch(1400/2353) done. Loss: 0.0532  lr:0.000010
[ Sun May 19 13:43:58 2024 ] 	Batch(1500/2353) done. Loss: 0.1456  lr:0.000010
[ Sun May 19 13:44:35 2024 ] 	Batch(1600/2353) done. Loss: 0.0268  lr:0.000010
[ Sun May 19 13:45:12 2024 ] 	Batch(1700/2353) done. Loss: 0.0140  lr:0.000010
[ Sun May 19 13:45:49 2024 ] 	Batch(1800/2353) done. Loss: 0.0035  lr:0.000010
[ Sun May 19 13:46:26 2024 ] 	Batch(1900/2353) done. Loss: 0.0147  lr:0.000010
[ Sun May 19 13:47:03 2024 ] 	Batch(2000/2353) done. Loss: 0.0123  lr:0.000010
[ Sun May 19 13:47:40 2024 ] 	Batch(2100/2353) done. Loss: 0.0566  lr:0.000010
[ Sun May 19 13:48:17 2024 ] 	Batch(2200/2353) done. Loss: 0.0664  lr:0.000010
[ Sun May 19 13:48:54 2024 ] 	Batch(2300/2353) done. Loss: 0.0132  lr:0.000010
[ Sun May 19 13:49:13 2024 ] 	Mean training loss: 0.0377.
[ Sun May 19 13:49:13 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 13:49:13 2024 ] Training epoch: 20
[ Sun May 19 13:49:14 2024 ] 	Batch(0/2353) done. Loss: 0.0108  lr:0.000010
[ Sun May 19 13:49:50 2024 ] 	Batch(100/2353) done. Loss: 0.0369  lr:0.000010
[ Sun May 19 13:50:27 2024 ] 	Batch(200/2353) done. Loss: 0.0071  lr:0.000010
[ Sun May 19 13:51:04 2024 ] 	Batch(300/2353) done. Loss: 0.0098  lr:0.000010
[ Sun May 19 13:51:41 2024 ] 	Batch(400/2353) done. Loss: 0.0111  lr:0.000010
[ Sun May 19 13:52:18 2024 ] 	Batch(500/2353) done. Loss: 0.0255  lr:0.000010
[ Sun May 19 13:52:55 2024 ] 	Batch(600/2353) done. Loss: 0.0464  lr:0.000010
[ Sun May 19 13:53:32 2024 ] 	Batch(700/2353) done. Loss: 0.0479  lr:0.000010
[ Sun May 19 13:54:09 2024 ] 	Batch(800/2353) done. Loss: 0.0449  lr:0.000010
[ Sun May 19 13:54:45 2024 ] 	Batch(900/2353) done. Loss: 0.0108  lr:0.000010
[ Sun May 19 13:55:22 2024 ] 	Batch(1000/2353) done. Loss: 0.0332  lr:0.000010
[ Sun May 19 13:55:59 2024 ] 	Batch(1100/2353) done. Loss: 0.0409  lr:0.000010
[ Sun May 19 13:56:36 2024 ] 	Batch(1200/2353) done. Loss: 0.1327  lr:0.000010
[ Sun May 19 13:57:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0123  lr:0.000010
[ Sun May 19 13:57:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0538  lr:0.000010
[ Sun May 19 13:58:27 2024 ] 	Batch(1500/2353) done. Loss: 0.0260  lr:0.000010
[ Sun May 19 13:59:04 2024 ] 	Batch(1600/2353) done. Loss: 0.0084  lr:0.000010
[ Sun May 19 13:59:41 2024 ] 	Batch(1700/2353) done. Loss: 0.0615  lr:0.000010
[ Sun May 19 14:00:17 2024 ] 	Batch(1800/2353) done. Loss: 0.0047  lr:0.000010
[ Sun May 19 14:00:54 2024 ] 	Batch(1900/2353) done. Loss: 0.0100  lr:0.000010
[ Sun May 19 14:01:31 2024 ] 	Batch(2000/2353) done. Loss: 0.0252  lr:0.000010
[ Sun May 19 14:02:08 2024 ] 	Batch(2100/2353) done. Loss: 0.0224  lr:0.000010
[ Sun May 19 14:02:45 2024 ] 	Batch(2200/2353) done. Loss: 0.0096  lr:0.000010
[ Sun May 19 14:03:22 2024 ] 	Batch(2300/2353) done. Loss: 0.0204  lr:0.000010
[ Sun May 19 14:03:41 2024 ] 	Mean training loss: 0.0410.
[ Sun May 19 14:03:41 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 14:03:41 2024 ] Eval epoch: 20
[ Sun May 19 14:05:45 2024 ] 	Mean val loss of 2367 batches: 0.24808922795920316.
[ Sun May 19 14:05:45 2024 ] Training epoch: 21
[ Sun May 19 14:05:46 2024 ] 	Batch(0/2353) done. Loss: 0.0170  lr:0.000010
[ Sun May 19 14:06:23 2024 ] 	Batch(100/2353) done. Loss: 0.1360  lr:0.000010
[ Sun May 19 14:07:00 2024 ] 	Batch(200/2353) done. Loss: 0.0503  lr:0.000010
[ Sun May 19 14:07:37 2024 ] 	Batch(300/2353) done. Loss: 0.0912  lr:0.000010
[ Sun May 19 14:08:13 2024 ] 	Batch(400/2353) done. Loss: 0.0261  lr:0.000010
[ Sun May 19 14:08:50 2024 ] 	Batch(500/2353) done. Loss: 0.0221  lr:0.000010
[ Sun May 19 14:09:27 2024 ] 	Batch(600/2353) done. Loss: 0.0197  lr:0.000010
[ Sun May 19 14:10:05 2024 ] 	Batch(700/2353) done. Loss: 0.0370  lr:0.000010
[ Sun May 19 14:10:42 2024 ] 	Batch(800/2353) done. Loss: 0.0075  lr:0.000010
[ Sun May 19 14:11:19 2024 ] 	Batch(900/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 14:11:57 2024 ] 	Batch(1000/2353) done. Loss: 0.0630  lr:0.000010
[ Sun May 19 14:12:33 2024 ] 	Batch(1100/2353) done. Loss: 0.0423  lr:0.000010
[ Sun May 19 14:13:10 2024 ] 	Batch(1200/2353) done. Loss: 0.0085  lr:0.000010
[ Sun May 19 14:13:47 2024 ] 	Batch(1300/2353) done. Loss: 0.0123  lr:0.000010
[ Sun May 19 14:14:24 2024 ] 	Batch(1400/2353) done. Loss: 0.0444  lr:0.000010
[ Sun May 19 14:15:01 2024 ] 	Batch(1500/2353) done. Loss: 0.0098  lr:0.000010
[ Sun May 19 14:15:38 2024 ] 	Batch(1600/2353) done. Loss: 0.0560  lr:0.000010
[ Sun May 19 14:16:15 2024 ] 	Batch(1700/2353) done. Loss: 0.0171  lr:0.000010
[ Sun May 19 14:16:52 2024 ] 	Batch(1800/2353) done. Loss: 0.0044  lr:0.000010
[ Sun May 19 14:17:29 2024 ] 	Batch(1900/2353) done. Loss: 0.0030  lr:0.000010
[ Sun May 19 14:18:07 2024 ] 	Batch(2000/2353) done. Loss: 0.0028  lr:0.000010
[ Sun May 19 14:18:44 2024 ] 	Batch(2100/2353) done. Loss: 0.0314  lr:0.000010
[ Sun May 19 14:19:21 2024 ] 	Batch(2200/2353) done. Loss: 0.1090  lr:0.000010
[ Sun May 19 14:19:58 2024 ] 	Batch(2300/2353) done. Loss: 0.0369  lr:0.000010
[ Sun May 19 14:20:17 2024 ] 	Mean training loss: 0.0375.
[ Sun May 19 14:20:17 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 14:20:17 2024 ] Training epoch: 22
[ Sun May 19 14:20:18 2024 ] 	Batch(0/2353) done. Loss: 0.0024  lr:0.000010
[ Sun May 19 14:20:55 2024 ] 	Batch(100/2353) done. Loss: 0.0052  lr:0.000010
[ Sun May 19 14:21:32 2024 ] 	Batch(200/2353) done. Loss: 0.1967  lr:0.000010
[ Sun May 19 14:22:09 2024 ] 	Batch(300/2353) done. Loss: 0.0440  lr:0.000010
[ Sun May 19 14:22:46 2024 ] 	Batch(400/2353) done. Loss: 0.2691  lr:0.000010
[ Sun May 19 14:23:23 2024 ] 	Batch(500/2353) done. Loss: 0.0139  lr:0.000010
[ Sun May 19 14:24:00 2024 ] 	Batch(600/2353) done. Loss: 0.0033  lr:0.000010
[ Sun May 19 14:24:37 2024 ] 	Batch(700/2353) done. Loss: 0.0092  lr:0.000010
[ Sun May 19 14:25:14 2024 ] 	Batch(800/2353) done. Loss: 0.0541  lr:0.000010
[ Sun May 19 14:25:51 2024 ] 	Batch(900/2353) done. Loss: 0.0138  lr:0.000010
[ Sun May 19 14:26:28 2024 ] 	Batch(1000/2353) done. Loss: 0.1657  lr:0.000010
[ Sun May 19 14:27:05 2024 ] 	Batch(1100/2353) done. Loss: 0.0071  lr:0.000010
[ Sun May 19 14:27:42 2024 ] 	Batch(1200/2353) done. Loss: 0.0023  lr:0.000010
[ Sun May 19 14:28:19 2024 ] 	Batch(1300/2353) done. Loss: 0.1735  lr:0.000010
[ Sun May 19 14:28:56 2024 ] 	Batch(1400/2353) done. Loss: 0.0023  lr:0.000010
[ Sun May 19 14:29:33 2024 ] 	Batch(1500/2353) done. Loss: 0.0379  lr:0.000010
[ Sun May 19 14:30:10 2024 ] 	Batch(1600/2353) done. Loss: 0.0645  lr:0.000010
[ Sun May 19 14:30:47 2024 ] 	Batch(1700/2353) done. Loss: 0.0030  lr:0.000010
[ Sun May 19 14:31:24 2024 ] 	Batch(1800/2353) done. Loss: 0.0241  lr:0.000010
[ Sun May 19 14:32:01 2024 ] 	Batch(1900/2353) done. Loss: 0.0195  lr:0.000010
[ Sun May 19 14:32:39 2024 ] 	Batch(2000/2353) done. Loss: 0.3578  lr:0.000010
[ Sun May 19 14:33:16 2024 ] 	Batch(2100/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 14:33:54 2024 ] 	Batch(2200/2353) done. Loss: 0.0102  lr:0.000010
[ Sun May 19 14:34:31 2024 ] 	Batch(2300/2353) done. Loss: 0.0114  lr:0.000010
[ Sun May 19 14:34:51 2024 ] 	Mean training loss: 0.0390.
[ Sun May 19 14:34:51 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 14:34:51 2024 ] Training epoch: 23
[ Sun May 19 14:34:52 2024 ] 	Batch(0/2353) done. Loss: 0.0171  lr:0.000010
[ Sun May 19 14:35:29 2024 ] 	Batch(100/2353) done. Loss: 0.0030  lr:0.000010
[ Sun May 19 14:36:05 2024 ] 	Batch(200/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 14:36:42 2024 ] 	Batch(300/2353) done. Loss: 0.1186  lr:0.000010
[ Sun May 19 14:37:20 2024 ] 	Batch(400/2353) done. Loss: 0.0109  lr:0.000010
[ Sun May 19 14:37:58 2024 ] 	Batch(500/2353) done. Loss: 0.0078  lr:0.000010
[ Sun May 19 14:38:35 2024 ] 	Batch(600/2353) done. Loss: 0.0258  lr:0.000010
[ Sun May 19 14:39:13 2024 ] 	Batch(700/2353) done. Loss: 0.0059  lr:0.000010
[ Sun May 19 14:39:50 2024 ] 	Batch(800/2353) done. Loss: 0.0204  lr:0.000010
[ Sun May 19 14:40:27 2024 ] 	Batch(900/2353) done. Loss: 0.0049  lr:0.000010
[ Sun May 19 14:41:04 2024 ] 	Batch(1000/2353) done. Loss: 0.0058  lr:0.000010
[ Sun May 19 14:41:41 2024 ] 	Batch(1100/2353) done. Loss: 0.0543  lr:0.000010
[ Sun May 19 14:42:18 2024 ] 	Batch(1200/2353) done. Loss: 0.0098  lr:0.000010
[ Sun May 19 14:42:55 2024 ] 	Batch(1300/2353) done. Loss: 0.1738  lr:0.000010
[ Sun May 19 14:43:31 2024 ] 	Batch(1400/2353) done. Loss: 0.0209  lr:0.000010
[ Sun May 19 14:44:08 2024 ] 	Batch(1500/2353) done. Loss: 0.0037  lr:0.000010
[ Sun May 19 14:44:45 2024 ] 	Batch(1600/2353) done. Loss: 0.0195  lr:0.000010
[ Sun May 19 14:45:22 2024 ] 	Batch(1700/2353) done. Loss: 0.0146  lr:0.000010
[ Sun May 19 14:45:59 2024 ] 	Batch(1800/2353) done. Loss: 0.0163  lr:0.000010
[ Sun May 19 14:46:36 2024 ] 	Batch(1900/2353) done. Loss: 0.0243  lr:0.000010
[ Sun May 19 14:47:13 2024 ] 	Batch(2000/2353) done. Loss: 0.0421  lr:0.000010
[ Sun May 19 14:47:50 2024 ] 	Batch(2100/2353) done. Loss: 0.0181  lr:0.000010
[ Sun May 19 14:48:27 2024 ] 	Batch(2200/2353) done. Loss: 0.0141  lr:0.000010
[ Sun May 19 14:49:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0169  lr:0.000010
[ Sun May 19 14:49:23 2024 ] 	Mean training loss: 0.0371.
[ Sun May 19 14:49:23 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 14:49:23 2024 ] Training epoch: 24
[ Sun May 19 14:49:24 2024 ] 	Batch(0/2353) done. Loss: 0.1242  lr:0.000010
[ Sun May 19 14:50:00 2024 ] 	Batch(100/2353) done. Loss: 0.0646  lr:0.000010
[ Sun May 19 14:50:37 2024 ] 	Batch(200/2353) done. Loss: 0.0585  lr:0.000010
[ Sun May 19 14:51:14 2024 ] 	Batch(300/2353) done. Loss: 0.0402  lr:0.000010
[ Sun May 19 14:51:51 2024 ] 	Batch(400/2353) done. Loss: 0.0436  lr:0.000010
[ Sun May 19 14:52:28 2024 ] 	Batch(500/2353) done. Loss: 0.0189  lr:0.000010
[ Sun May 19 14:53:05 2024 ] 	Batch(600/2353) done. Loss: 0.0181  lr:0.000010
[ Sun May 19 14:53:42 2024 ] 	Batch(700/2353) done. Loss: 0.0006  lr:0.000010
[ Sun May 19 14:54:19 2024 ] 	Batch(800/2353) done. Loss: 0.0204  lr:0.000010
[ Sun May 19 14:54:55 2024 ] 	Batch(900/2353) done. Loss: 0.0068  lr:0.000010
[ Sun May 19 14:55:32 2024 ] 	Batch(1000/2353) done. Loss: 0.0173  lr:0.000010
[ Sun May 19 14:56:09 2024 ] 	Batch(1100/2353) done. Loss: 0.0050  lr:0.000010
[ Sun May 19 14:56:46 2024 ] 	Batch(1200/2353) done. Loss: 0.0177  lr:0.000010
[ Sun May 19 14:57:23 2024 ] 	Batch(1300/2353) done. Loss: 0.0113  lr:0.000010
[ Sun May 19 14:58:00 2024 ] 	Batch(1400/2353) done. Loss: 0.0158  lr:0.000010
[ Sun May 19 14:58:37 2024 ] 	Batch(1500/2353) done. Loss: 0.2052  lr:0.000010
[ Sun May 19 14:59:14 2024 ] 	Batch(1600/2353) done. Loss: 0.0757  lr:0.000010
[ Sun May 19 14:59:51 2024 ] 	Batch(1700/2353) done. Loss: 0.0738  lr:0.000010
[ Sun May 19 15:00:27 2024 ] 	Batch(1800/2353) done. Loss: 0.0058  lr:0.000010
[ Sun May 19 15:01:04 2024 ] 	Batch(1900/2353) done. Loss: 0.0478  lr:0.000010
[ Sun May 19 15:01:41 2024 ] 	Batch(2000/2353) done. Loss: 0.0063  lr:0.000010
[ Sun May 19 15:02:18 2024 ] 	Batch(2100/2353) done. Loss: 0.2080  lr:0.000010
[ Sun May 19 15:02:55 2024 ] 	Batch(2200/2353) done. Loss: 0.0241  lr:0.000010
[ Sun May 19 15:03:32 2024 ] 	Batch(2300/2353) done. Loss: 0.0157  lr:0.000010
[ Sun May 19 15:03:51 2024 ] 	Mean training loss: 0.0399.
[ Sun May 19 15:03:51 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 15:03:52 2024 ] Training epoch: 25
[ Sun May 19 15:03:52 2024 ] 	Batch(0/2353) done. Loss: 0.0246  lr:0.000010
[ Sun May 19 15:04:29 2024 ] 	Batch(100/2353) done. Loss: 0.0514  lr:0.000010
[ Sun May 19 15:05:06 2024 ] 	Batch(200/2353) done. Loss: 0.0102  lr:0.000010
[ Sun May 19 15:05:43 2024 ] 	Batch(300/2353) done. Loss: 0.0205  lr:0.000010
[ Sun May 19 15:06:20 2024 ] 	Batch(400/2353) done. Loss: 0.0046  lr:0.000010
[ Sun May 19 15:06:57 2024 ] 	Batch(500/2353) done. Loss: 0.0137  lr:0.000010
[ Sun May 19 15:07:34 2024 ] 	Batch(600/2353) done. Loss: 0.0312  lr:0.000010
[ Sun May 19 15:08:11 2024 ] 	Batch(700/2353) done. Loss: 0.0342  lr:0.000010
[ Sun May 19 15:08:48 2024 ] 	Batch(800/2353) done. Loss: 0.2210  lr:0.000010
[ Sun May 19 15:09:25 2024 ] 	Batch(900/2353) done. Loss: 0.0198  lr:0.000010
[ Sun May 19 15:10:02 2024 ] 	Batch(1000/2353) done. Loss: 0.0564  lr:0.000010
[ Sun May 19 15:10:38 2024 ] 	Batch(1100/2353) done. Loss: 0.0077  lr:0.000010
[ Sun May 19 15:11:15 2024 ] 	Batch(1200/2353) done. Loss: 0.0110  lr:0.000010
[ Sun May 19 15:11:52 2024 ] 	Batch(1300/2353) done. Loss: 0.0129  lr:0.000010
[ Sun May 19 15:12:29 2024 ] 	Batch(1400/2353) done. Loss: 0.0371  lr:0.000010
[ Sun May 19 15:13:06 2024 ] 	Batch(1500/2353) done. Loss: 0.0181  lr:0.000010
[ Sun May 19 15:13:43 2024 ] 	Batch(1600/2353) done. Loss: 0.0338  lr:0.000010
[ Sun May 19 15:14:20 2024 ] 	Batch(1700/2353) done. Loss: 0.0147  lr:0.000010
[ Sun May 19 15:14:56 2024 ] 	Batch(1800/2353) done. Loss: 0.0481  lr:0.000010
[ Sun May 19 15:15:33 2024 ] 	Batch(1900/2353) done. Loss: 0.0139  lr:0.000010
[ Sun May 19 15:16:10 2024 ] 	Batch(2000/2353) done. Loss: 0.0160  lr:0.000010
[ Sun May 19 15:16:47 2024 ] 	Batch(2100/2353) done. Loss: 0.0111  lr:0.000010
[ Sun May 19 15:17:24 2024 ] 	Batch(2200/2353) done. Loss: 0.0021  lr:0.000010
[ Sun May 19 15:18:01 2024 ] 	Batch(2300/2353) done. Loss: 0.0216  lr:0.000010
[ Sun May 19 15:18:20 2024 ] 	Mean training loss: 0.0393.
[ Sun May 19 15:18:20 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 15:18:20 2024 ] Training epoch: 26
[ Sun May 19 15:18:21 2024 ] 	Batch(0/2353) done. Loss: 0.0201  lr:0.000010
[ Sun May 19 15:18:58 2024 ] 	Batch(100/2353) done. Loss: 0.0957  lr:0.000010
[ Sun May 19 15:19:35 2024 ] 	Batch(200/2353) done. Loss: 0.0201  lr:0.000010
[ Sun May 19 15:20:11 2024 ] 	Batch(300/2353) done. Loss: 0.0372  lr:0.000010
[ Sun May 19 15:20:48 2024 ] 	Batch(400/2353) done. Loss: 0.0127  lr:0.000010
[ Sun May 19 15:21:25 2024 ] 	Batch(500/2353) done. Loss: 0.0087  lr:0.000010
[ Sun May 19 15:22:02 2024 ] 	Batch(600/2353) done. Loss: 0.0100  lr:0.000010
[ Sun May 19 15:22:39 2024 ] 	Batch(700/2353) done. Loss: 0.0032  lr:0.000010
[ Sun May 19 15:23:16 2024 ] 	Batch(800/2353) done. Loss: 0.0195  lr:0.000010
[ Sun May 19 15:23:53 2024 ] 	Batch(900/2353) done. Loss: 0.0578  lr:0.000010
[ Sun May 19 15:24:29 2024 ] 	Batch(1000/2353) done. Loss: 0.0081  lr:0.000010
[ Sun May 19 15:25:06 2024 ] 	Batch(1100/2353) done. Loss: 0.0264  lr:0.000010
[ Sun May 19 15:25:43 2024 ] 	Batch(1200/2353) done. Loss: 0.0159  lr:0.000010
[ Sun May 19 15:26:20 2024 ] 	Batch(1300/2353) done. Loss: 0.0089  lr:0.000010
[ Sun May 19 15:26:57 2024 ] 	Batch(1400/2353) done. Loss: 0.0479  lr:0.000010
[ Sun May 19 15:27:34 2024 ] 	Batch(1500/2353) done. Loss: 0.0184  lr:0.000010
[ Sun May 19 15:28:11 2024 ] 	Batch(1600/2353) done. Loss: 0.0236  lr:0.000010
[ Sun May 19 15:28:48 2024 ] 	Batch(1700/2353) done. Loss: 0.0095  lr:0.000010
[ Sun May 19 15:29:25 2024 ] 	Batch(1800/2353) done. Loss: 0.0310  lr:0.000010
[ Sun May 19 15:30:02 2024 ] 	Batch(1900/2353) done. Loss: 0.0217  lr:0.000010
[ Sun May 19 15:30:39 2024 ] 	Batch(2000/2353) done. Loss: 0.0375  lr:0.000010
[ Sun May 19 15:31:16 2024 ] 	Batch(2100/2353) done. Loss: 0.0528  lr:0.000010
[ Sun May 19 15:31:53 2024 ] 	Batch(2200/2353) done. Loss: 0.0253  lr:0.000010
[ Sun May 19 15:32:29 2024 ] 	Batch(2300/2353) done. Loss: 0.0320  lr:0.000010
[ Sun May 19 15:32:49 2024 ] 	Mean training loss: 0.0377.
[ Sun May 19 15:32:49 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 15:32:49 2024 ] Training epoch: 27
[ Sun May 19 15:32:49 2024 ] 	Batch(0/2353) done. Loss: 0.0067  lr:0.000010
[ Sun May 19 15:33:26 2024 ] 	Batch(100/2353) done. Loss: 0.0299  lr:0.000010
[ Sun May 19 15:34:03 2024 ] 	Batch(200/2353) done. Loss: 0.0074  lr:0.000010
[ Sun May 19 15:34:40 2024 ] 	Batch(300/2353) done. Loss: 0.0260  lr:0.000010
[ Sun May 19 15:35:17 2024 ] 	Batch(400/2353) done. Loss: 0.1769  lr:0.000010
[ Sun May 19 15:35:54 2024 ] 	Batch(500/2353) done. Loss: 0.0207  lr:0.000010
[ Sun May 19 15:36:32 2024 ] 	Batch(600/2353) done. Loss: 0.0165  lr:0.000010
[ Sun May 19 15:37:10 2024 ] 	Batch(700/2353) done. Loss: 0.0045  lr:0.000010
[ Sun May 19 15:37:48 2024 ] 	Batch(800/2353) done. Loss: 0.0099  lr:0.000010
[ Sun May 19 15:38:25 2024 ] 	Batch(900/2353) done. Loss: 0.0183  lr:0.000010
[ Sun May 19 15:39:01 2024 ] 	Batch(1000/2353) done. Loss: 0.0069  lr:0.000010
[ Sun May 19 15:39:38 2024 ] 	Batch(1100/2353) done. Loss: 0.1405  lr:0.000010
[ Sun May 19 15:40:15 2024 ] 	Batch(1200/2353) done. Loss: 0.0106  lr:0.000010
[ Sun May 19 15:40:52 2024 ] 	Batch(1300/2353) done. Loss: 0.0104  lr:0.000010
[ Sun May 19 15:41:29 2024 ] 	Batch(1400/2353) done. Loss: 0.0169  lr:0.000010
[ Sun May 19 15:42:06 2024 ] 	Batch(1500/2353) done. Loss: 0.1073  lr:0.000010
[ Sun May 19 15:42:43 2024 ] 	Batch(1600/2353) done. Loss: 0.0093  lr:0.000010
[ Sun May 19 15:43:19 2024 ] 	Batch(1700/2353) done. Loss: 0.0099  lr:0.000010
[ Sun May 19 15:43:56 2024 ] 	Batch(1800/2353) done. Loss: 0.0130  lr:0.000010
[ Sun May 19 15:44:33 2024 ] 	Batch(1900/2353) done. Loss: 0.0179  lr:0.000010
[ Sun May 19 15:45:10 2024 ] 	Batch(2000/2353) done. Loss: 0.1102  lr:0.000010
[ Sun May 19 15:45:47 2024 ] 	Batch(2100/2353) done. Loss: 0.0186  lr:0.000010
[ Sun May 19 15:46:24 2024 ] 	Batch(2200/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 15:47:01 2024 ] 	Batch(2300/2353) done. Loss: 0.0012  lr:0.000010
[ Sun May 19 15:47:20 2024 ] 	Mean training loss: 0.0362.
[ Sun May 19 15:47:20 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 15:47:20 2024 ] Training epoch: 28
[ Sun May 19 15:47:21 2024 ] 	Batch(0/2353) done. Loss: 0.0043  lr:0.000010
[ Sun May 19 15:47:58 2024 ] 	Batch(100/2353) done. Loss: 0.0668  lr:0.000010
[ Sun May 19 15:48:35 2024 ] 	Batch(200/2353) done. Loss: 0.0551  lr:0.000010
[ Sun May 19 15:49:13 2024 ] 	Batch(300/2353) done. Loss: 0.0049  lr:0.000010
[ Sun May 19 15:49:50 2024 ] 	Batch(400/2353) done. Loss: 0.0135  lr:0.000010
[ Sun May 19 15:50:27 2024 ] 	Batch(500/2353) done. Loss: 0.0191  lr:0.000010
[ Sun May 19 15:51:04 2024 ] 	Batch(600/2353) done. Loss: 0.0106  lr:0.000010
[ Sun May 19 15:51:41 2024 ] 	Batch(700/2353) done. Loss: 0.0312  lr:0.000010
[ Sun May 19 15:52:18 2024 ] 	Batch(800/2353) done. Loss: 0.0059  lr:0.000010
[ Sun May 19 15:52:55 2024 ] 	Batch(900/2353) done. Loss: 0.0577  lr:0.000010
[ Sun May 19 15:53:32 2024 ] 	Batch(1000/2353) done. Loss: 0.0052  lr:0.000010
[ Sun May 19 15:54:08 2024 ] 	Batch(1100/2353) done. Loss: 0.0487  lr:0.000010
[ Sun May 19 15:54:45 2024 ] 	Batch(1200/2353) done. Loss: 0.0047  lr:0.000010
[ Sun May 19 15:55:22 2024 ] 	Batch(1300/2353) done. Loss: 0.0488  lr:0.000010
[ Sun May 19 15:55:59 2024 ] 	Batch(1400/2353) done. Loss: 0.0754  lr:0.000010
[ Sun May 19 15:56:36 2024 ] 	Batch(1500/2353) done. Loss: 0.0004  lr:0.000010
[ Sun May 19 15:57:13 2024 ] 	Batch(1600/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 15:57:50 2024 ] 	Batch(1700/2353) done. Loss: 0.0348  lr:0.000010
[ Sun May 19 15:58:28 2024 ] 	Batch(1800/2353) done. Loss: 0.0354  lr:0.000010
[ Sun May 19 15:59:05 2024 ] 	Batch(1900/2353) done. Loss: 0.0045  lr:0.000010
[ Sun May 19 15:59:43 2024 ] 	Batch(2000/2353) done. Loss: 0.0213  lr:0.000010
[ Sun May 19 16:00:20 2024 ] 	Batch(2100/2353) done. Loss: 0.0353  lr:0.000010
[ Sun May 19 16:00:57 2024 ] 	Batch(2200/2353) done. Loss: 0.0046  lr:0.000010
[ Sun May 19 16:01:34 2024 ] 	Batch(2300/2353) done. Loss: 0.0109  lr:0.000010
[ Sun May 19 16:01:53 2024 ] 	Mean training loss: 0.0363.
[ Sun May 19 16:01:53 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 16:01:53 2024 ] Training epoch: 29
[ Sun May 19 16:01:54 2024 ] 	Batch(0/2353) done. Loss: 0.1150  lr:0.000010
[ Sun May 19 16:02:31 2024 ] 	Batch(100/2353) done. Loss: 0.0941  lr:0.000010
[ Sun May 19 16:03:08 2024 ] 	Batch(200/2353) done. Loss: 0.0305  lr:0.000010
[ Sun May 19 16:03:46 2024 ] 	Batch(300/2353) done. Loss: 0.1071  lr:0.000010
[ Sun May 19 16:04:23 2024 ] 	Batch(400/2353) done. Loss: 0.0230  lr:0.000010
[ Sun May 19 16:05:00 2024 ] 	Batch(500/2353) done. Loss: 0.0473  lr:0.000010
[ Sun May 19 16:05:37 2024 ] 	Batch(600/2353) done. Loss: 0.0085  lr:0.000010
[ Sun May 19 16:06:14 2024 ] 	Batch(700/2353) done. Loss: 0.0531  lr:0.000010
[ Sun May 19 16:06:51 2024 ] 	Batch(800/2353) done. Loss: 0.0181  lr:0.000010
[ Sun May 19 16:07:28 2024 ] 	Batch(900/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 16:08:05 2024 ] 	Batch(1000/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 16:08:42 2024 ] 	Batch(1100/2353) done. Loss: 0.1354  lr:0.000010
[ Sun May 19 16:09:19 2024 ] 	Batch(1200/2353) done. Loss: 0.0211  lr:0.000010
[ Sun May 19 16:09:55 2024 ] 	Batch(1300/2353) done. Loss: 0.0198  lr:0.000010
[ Sun May 19 16:10:32 2024 ] 	Batch(1400/2353) done. Loss: 0.0266  lr:0.000010
[ Sun May 19 16:11:09 2024 ] 	Batch(1500/2353) done. Loss: 0.0304  lr:0.000010
[ Sun May 19 16:11:46 2024 ] 	Batch(1600/2353) done. Loss: 0.0464  lr:0.000010
[ Sun May 19 16:12:23 2024 ] 	Batch(1700/2353) done. Loss: 0.0639  lr:0.000010
[ Sun May 19 16:13:00 2024 ] 	Batch(1800/2353) done. Loss: 0.0258  lr:0.000010
[ Sun May 19 16:13:37 2024 ] 	Batch(1900/2353) done. Loss: 0.0148  lr:0.000010
[ Sun May 19 16:14:14 2024 ] 	Batch(2000/2353) done. Loss: 0.0059  lr:0.000010
[ Sun May 19 16:14:50 2024 ] 	Batch(2100/2353) done. Loss: 0.0105  lr:0.000010
[ Sun May 19 16:15:27 2024 ] 	Batch(2200/2353) done. Loss: 0.0441  lr:0.000010
[ Sun May 19 16:16:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0634  lr:0.000010
[ Sun May 19 16:16:24 2024 ] 	Mean training loss: 0.0379.
[ Sun May 19 16:16:24 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 16:16:24 2024 ] Training epoch: 30
[ Sun May 19 16:16:24 2024 ] 	Batch(0/2353) done. Loss: 0.0198  lr:0.000010
[ Sun May 19 16:17:01 2024 ] 	Batch(100/2353) done. Loss: 0.0571  lr:0.000010
[ Sun May 19 16:17:38 2024 ] 	Batch(200/2353) done. Loss: 0.0283  lr:0.000010
[ Sun May 19 16:18:15 2024 ] 	Batch(300/2353) done. Loss: 0.0570  lr:0.000010
[ Sun May 19 16:18:52 2024 ] 	Batch(400/2353) done. Loss: 0.0469  lr:0.000010
[ Sun May 19 16:19:29 2024 ] 	Batch(500/2353) done. Loss: 0.0405  lr:0.000010
[ Sun May 19 16:20:05 2024 ] 	Batch(600/2353) done. Loss: 0.0571  lr:0.000010
[ Sun May 19 16:20:42 2024 ] 	Batch(700/2353) done. Loss: 0.0522  lr:0.000010
[ Sun May 19 16:21:19 2024 ] 	Batch(800/2353) done. Loss: 0.0341  lr:0.000010
[ Sun May 19 16:21:56 2024 ] 	Batch(900/2353) done. Loss: 0.0303  lr:0.000010
[ Sun May 19 16:22:33 2024 ] 	Batch(1000/2353) done. Loss: 0.0197  lr:0.000010
[ Sun May 19 16:23:10 2024 ] 	Batch(1100/2353) done. Loss: 0.0588  lr:0.000010
[ Sun May 19 16:23:47 2024 ] 	Batch(1200/2353) done. Loss: 0.1258  lr:0.000010
[ Sun May 19 16:24:24 2024 ] 	Batch(1300/2353) done. Loss: 0.0088  lr:0.000010
[ Sun May 19 16:25:00 2024 ] 	Batch(1400/2353) done. Loss: 0.0125  lr:0.000010
[ Sun May 19 16:25:37 2024 ] 	Batch(1500/2353) done. Loss: 0.0056  lr:0.000010
[ Sun May 19 16:26:14 2024 ] 	Batch(1600/2353) done. Loss: 0.0042  lr:0.000010
[ Sun May 19 16:26:51 2024 ] 	Batch(1700/2353) done. Loss: 0.0511  lr:0.000010
[ Sun May 19 16:27:28 2024 ] 	Batch(1800/2353) done. Loss: 0.0129  lr:0.000010
[ Sun May 19 16:28:05 2024 ] 	Batch(1900/2353) done. Loss: 0.0812  lr:0.000010
[ Sun May 19 16:28:42 2024 ] 	Batch(2000/2353) done. Loss: 0.0035  lr:0.000010
[ Sun May 19 16:29:19 2024 ] 	Batch(2100/2353) done. Loss: 0.1467  lr:0.000010
[ Sun May 19 16:29:56 2024 ] 	Batch(2200/2353) done. Loss: 0.0349  lr:0.000010
[ Sun May 19 16:30:32 2024 ] 	Batch(2300/2353) done. Loss: 0.0661  lr:0.000010
[ Sun May 19 16:30:52 2024 ] 	Mean training loss: 0.0364.
[ Sun May 19 16:30:52 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 16:30:52 2024 ] Eval epoch: 30
[ Sun May 19 16:32:56 2024 ] 	Mean val loss of 2367 batches: 0.24770461973103733.
[ Sun May 19 16:32:56 2024 ] Training epoch: 31
[ Sun May 19 16:32:57 2024 ] 	Batch(0/2353) done. Loss: 0.0109  lr:0.000010
[ Sun May 19 16:33:34 2024 ] 	Batch(100/2353) done. Loss: 0.0096  lr:0.000010
[ Sun May 19 16:34:11 2024 ] 	Batch(200/2353) done. Loss: 0.0531  lr:0.000010
[ Sun May 19 16:34:48 2024 ] 	Batch(300/2353) done. Loss: 0.0080  lr:0.000010
[ Sun May 19 16:35:25 2024 ] 	Batch(400/2353) done. Loss: 0.0373  lr:0.000010
[ Sun May 19 16:36:02 2024 ] 	Batch(500/2353) done. Loss: 0.0308  lr:0.000010
[ Sun May 19 16:36:39 2024 ] 	Batch(600/2353) done. Loss: 0.0499  lr:0.000010
[ Sun May 19 16:37:16 2024 ] 	Batch(700/2353) done. Loss: 0.0238  lr:0.000010
[ Sun May 19 16:37:53 2024 ] 	Batch(800/2353) done. Loss: 0.0159  lr:0.000010
[ Sun May 19 16:38:29 2024 ] 	Batch(900/2353) done. Loss: 0.0718  lr:0.000010
[ Sun May 19 16:39:06 2024 ] 	Batch(1000/2353) done. Loss: 0.0265  lr:0.000010
[ Sun May 19 16:39:43 2024 ] 	Batch(1100/2353) done. Loss: 0.1062  lr:0.000010
[ Sun May 19 16:40:20 2024 ] 	Batch(1200/2353) done. Loss: 0.0039  lr:0.000010
[ Sun May 19 16:40:57 2024 ] 	Batch(1300/2353) done. Loss: 0.0330  lr:0.000010
[ Sun May 19 16:41:34 2024 ] 	Batch(1400/2353) done. Loss: 0.0249  lr:0.000010
[ Sun May 19 16:42:11 2024 ] 	Batch(1500/2353) done. Loss: 0.0500  lr:0.000010
[ Sun May 19 16:42:48 2024 ] 	Batch(1600/2353) done. Loss: 0.0401  lr:0.000010
[ Sun May 19 16:43:25 2024 ] 	Batch(1700/2353) done. Loss: 0.0619  lr:0.000010
[ Sun May 19 16:44:02 2024 ] 	Batch(1800/2353) done. Loss: 0.1781  lr:0.000010
[ Sun May 19 16:44:38 2024 ] 	Batch(1900/2353) done. Loss: 0.0116  lr:0.000010
[ Sun May 19 16:45:15 2024 ] 	Batch(2000/2353) done. Loss: 0.0107  lr:0.000010
[ Sun May 19 16:45:52 2024 ] 	Batch(2100/2353) done. Loss: 0.0195  lr:0.000010
[ Sun May 19 16:46:29 2024 ] 	Batch(2200/2353) done. Loss: 0.0185  lr:0.000010
[ Sun May 19 16:47:06 2024 ] 	Batch(2300/2353) done. Loss: 0.0449  lr:0.000010
[ Sun May 19 16:47:25 2024 ] 	Mean training loss: 0.0392.
[ Sun May 19 16:47:25 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 16:47:25 2024 ] Training epoch: 32
[ Sun May 19 16:47:26 2024 ] 	Batch(0/2353) done. Loss: 0.0190  lr:0.000010
[ Sun May 19 16:48:03 2024 ] 	Batch(100/2353) done. Loss: 0.1013  lr:0.000010
[ Sun May 19 16:48:40 2024 ] 	Batch(200/2353) done. Loss: 0.0301  lr:0.000010
[ Sun May 19 16:49:17 2024 ] 	Batch(300/2353) done. Loss: 0.0755  lr:0.000010
[ Sun May 19 16:49:53 2024 ] 	Batch(400/2353) done. Loss: 0.0068  lr:0.000010
[ Sun May 19 16:50:30 2024 ] 	Batch(500/2353) done. Loss: 0.0155  lr:0.000010
[ Sun May 19 16:51:07 2024 ] 	Batch(600/2353) done. Loss: 0.0426  lr:0.000010
[ Sun May 19 16:51:44 2024 ] 	Batch(700/2353) done. Loss: 0.0321  lr:0.000010
[ Sun May 19 16:52:21 2024 ] 	Batch(800/2353) done. Loss: 0.0122  lr:0.000010
[ Sun May 19 16:52:58 2024 ] 	Batch(900/2353) done. Loss: 0.0076  lr:0.000010
[ Sun May 19 16:53:35 2024 ] 	Batch(1000/2353) done. Loss: 0.0194  lr:0.000010
[ Sun May 19 16:54:12 2024 ] 	Batch(1100/2353) done. Loss: 0.1449  lr:0.000010
[ Sun May 19 16:54:49 2024 ] 	Batch(1200/2353) done. Loss: 0.1329  lr:0.000010
[ Sun May 19 16:55:26 2024 ] 	Batch(1300/2353) done. Loss: 0.0201  lr:0.000010
[ Sun May 19 16:56:04 2024 ] 	Batch(1400/2353) done. Loss: 0.0705  lr:0.000010
[ Sun May 19 16:56:42 2024 ] 	Batch(1500/2353) done. Loss: 0.0945  lr:0.000010
[ Sun May 19 16:57:20 2024 ] 	Batch(1600/2353) done. Loss: 0.0319  lr:0.000010
[ Sun May 19 16:57:57 2024 ] 	Batch(1700/2353) done. Loss: 0.0692  lr:0.000010
[ Sun May 19 16:58:35 2024 ] 	Batch(1800/2353) done. Loss: 0.0123  lr:0.000010
[ Sun May 19 16:59:12 2024 ] 	Batch(1900/2353) done. Loss: 0.0577  lr:0.000010
[ Sun May 19 16:59:49 2024 ] 	Batch(2000/2353) done. Loss: 0.0322  lr:0.000010
[ Sun May 19 17:00:26 2024 ] 	Batch(2100/2353) done. Loss: 0.0449  lr:0.000010
[ Sun May 19 17:01:03 2024 ] 	Batch(2200/2353) done. Loss: 0.0033  lr:0.000010
[ Sun May 19 17:01:40 2024 ] 	Batch(2300/2353) done. Loss: 0.0060  lr:0.000010
[ Sun May 19 17:01:59 2024 ] 	Mean training loss: 0.0374.
[ Sun May 19 17:01:59 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 17:01:59 2024 ] Training epoch: 33
[ Sun May 19 17:02:00 2024 ] 	Batch(0/2353) done. Loss: 0.0481  lr:0.000010
[ Sun May 19 17:02:36 2024 ] 	Batch(100/2353) done. Loss: 0.0818  lr:0.000010
[ Sun May 19 17:03:13 2024 ] 	Batch(200/2353) done. Loss: 0.0024  lr:0.000010
[ Sun May 19 17:03:50 2024 ] 	Batch(300/2353) done. Loss: 0.0054  lr:0.000010
[ Sun May 19 17:04:27 2024 ] 	Batch(400/2353) done. Loss: 0.0123  lr:0.000010
[ Sun May 19 17:05:04 2024 ] 	Batch(500/2353) done. Loss: 0.1169  lr:0.000010
[ Sun May 19 17:05:41 2024 ] 	Batch(600/2353) done. Loss: 0.1173  lr:0.000010
[ Sun May 19 17:06:18 2024 ] 	Batch(700/2353) done. Loss: 0.0053  lr:0.000010
[ Sun May 19 17:06:55 2024 ] 	Batch(800/2353) done. Loss: 0.0037  lr:0.000010
[ Sun May 19 17:07:31 2024 ] 	Batch(900/2353) done. Loss: 0.0362  lr:0.000010
[ Sun May 19 17:08:08 2024 ] 	Batch(1000/2353) done. Loss: 0.0201  lr:0.000010
[ Sun May 19 17:08:45 2024 ] 	Batch(1100/2353) done. Loss: 0.0014  lr:0.000010
[ Sun May 19 17:09:22 2024 ] 	Batch(1200/2353) done. Loss: 0.0085  lr:0.000010
[ Sun May 19 17:09:59 2024 ] 	Batch(1300/2353) done. Loss: 0.0079  lr:0.000010
[ Sun May 19 17:10:36 2024 ] 	Batch(1400/2353) done. Loss: 0.0047  lr:0.000010
[ Sun May 19 17:11:13 2024 ] 	Batch(1500/2353) done. Loss: 0.0197  lr:0.000010
[ Sun May 19 17:11:50 2024 ] 	Batch(1600/2353) done. Loss: 0.0347  lr:0.000010
[ Sun May 19 17:12:26 2024 ] 	Batch(1700/2353) done. Loss: 0.0223  lr:0.000010
[ Sun May 19 17:13:03 2024 ] 	Batch(1800/2353) done. Loss: 0.0030  lr:0.000010
[ Sun May 19 17:13:41 2024 ] 	Batch(1900/2353) done. Loss: 0.0278  lr:0.000010
[ Sun May 19 17:14:19 2024 ] 	Batch(2000/2353) done. Loss: 0.0507  lr:0.000010
[ Sun May 19 17:14:56 2024 ] 	Batch(2100/2353) done. Loss: 0.0104  lr:0.000010
[ Sun May 19 17:15:33 2024 ] 	Batch(2200/2353) done. Loss: 0.0264  lr:0.000010
[ Sun May 19 17:16:11 2024 ] 	Batch(2300/2353) done. Loss: 0.0307  lr:0.000010
[ Sun May 19 17:16:30 2024 ] 	Mean training loss: 0.0362.
[ Sun May 19 17:16:30 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 17:16:30 2024 ] Training epoch: 34
[ Sun May 19 17:16:31 2024 ] 	Batch(0/2353) done. Loss: 0.0100  lr:0.000010
[ Sun May 19 17:17:08 2024 ] 	Batch(100/2353) done. Loss: 0.0701  lr:0.000010
[ Sun May 19 17:17:44 2024 ] 	Batch(200/2353) done. Loss: 0.0344  lr:0.000010
[ Sun May 19 17:18:21 2024 ] 	Batch(300/2353) done. Loss: 0.0314  lr:0.000010
[ Sun May 19 17:18:58 2024 ] 	Batch(400/2353) done. Loss: 0.0503  lr:0.000010
[ Sun May 19 17:19:35 2024 ] 	Batch(500/2353) done. Loss: 0.0053  lr:0.000010
[ Sun May 19 17:20:12 2024 ] 	Batch(600/2353) done. Loss: 0.0472  lr:0.000010
[ Sun May 19 17:20:49 2024 ] 	Batch(700/2353) done. Loss: 0.0104  lr:0.000010
[ Sun May 19 17:21:26 2024 ] 	Batch(800/2353) done. Loss: 0.0090  lr:0.000010
[ Sun May 19 17:22:03 2024 ] 	Batch(900/2353) done. Loss: 0.0464  lr:0.000010
[ Sun May 19 17:22:40 2024 ] 	Batch(1000/2353) done. Loss: 0.0045  lr:0.000010
[ Sun May 19 17:23:17 2024 ] 	Batch(1100/2353) done. Loss: 0.0097  lr:0.000010
[ Sun May 19 17:23:55 2024 ] 	Batch(1200/2353) done. Loss: 0.0364  lr:0.000010
[ Sun May 19 17:24:32 2024 ] 	Batch(1300/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 17:25:09 2024 ] 	Batch(1400/2353) done. Loss: 0.0165  lr:0.000010
[ Sun May 19 17:25:46 2024 ] 	Batch(1500/2353) done. Loss: 0.0021  lr:0.000010
[ Sun May 19 17:26:23 2024 ] 	Batch(1600/2353) done. Loss: 0.0198  lr:0.000010
[ Sun May 19 17:27:00 2024 ] 	Batch(1700/2353) done. Loss: 0.0743  lr:0.000010
[ Sun May 19 17:27:38 2024 ] 	Batch(1800/2353) done. Loss: 0.0386  lr:0.000010
[ Sun May 19 17:28:15 2024 ] 	Batch(1900/2353) done. Loss: 0.0470  lr:0.000010
[ Sun May 19 17:28:52 2024 ] 	Batch(2000/2353) done. Loss: 0.0029  lr:0.000010
[ Sun May 19 17:29:29 2024 ] 	Batch(2100/2353) done. Loss: 0.0205  lr:0.000010
[ Sun May 19 17:30:06 2024 ] 	Batch(2200/2353) done. Loss: 0.0139  lr:0.000010
[ Sun May 19 17:30:42 2024 ] 	Batch(2300/2353) done. Loss: 0.0165  lr:0.000010
[ Sun May 19 17:31:02 2024 ] 	Mean training loss: 0.0377.
[ Sun May 19 17:31:02 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 17:31:02 2024 ] Training epoch: 35
[ Sun May 19 17:31:03 2024 ] 	Batch(0/2353) done. Loss: 0.0216  lr:0.000010
[ Sun May 19 17:31:39 2024 ] 	Batch(100/2353) done. Loss: 0.0076  lr:0.000010
[ Sun May 19 17:32:16 2024 ] 	Batch(200/2353) done. Loss: 0.0659  lr:0.000010
[ Sun May 19 17:32:53 2024 ] 	Batch(300/2353) done. Loss: 0.0179  lr:0.000010
[ Sun May 19 17:33:30 2024 ] 	Batch(400/2353) done. Loss: 0.0705  lr:0.000010
[ Sun May 19 17:34:07 2024 ] 	Batch(500/2353) done. Loss: 0.0064  lr:0.000010
[ Sun May 19 17:34:44 2024 ] 	Batch(600/2353) done. Loss: 0.0227  lr:0.000010
[ Sun May 19 17:35:20 2024 ] 	Batch(700/2353) done. Loss: 0.0272  lr:0.000010
[ Sun May 19 17:35:57 2024 ] 	Batch(800/2353) done. Loss: 0.0223  lr:0.000010
[ Sun May 19 17:36:34 2024 ] 	Batch(900/2353) done. Loss: 0.0049  lr:0.000010
[ Sun May 19 17:37:11 2024 ] 	Batch(1000/2353) done. Loss: 0.0342  lr:0.000010
[ Sun May 19 17:37:48 2024 ] 	Batch(1100/2353) done. Loss: 0.0040  lr:0.000010
[ Sun May 19 17:38:25 2024 ] 	Batch(1200/2353) done. Loss: 0.0126  lr:0.000010
[ Sun May 19 17:39:02 2024 ] 	Batch(1300/2353) done. Loss: 0.0127  lr:0.000010
[ Sun May 19 17:39:39 2024 ] 	Batch(1400/2353) done. Loss: 0.0196  lr:0.000010
[ Sun May 19 17:40:16 2024 ] 	Batch(1500/2353) done. Loss: 0.0406  lr:0.000010
[ Sun May 19 17:40:53 2024 ] 	Batch(1600/2353) done. Loss: 0.0186  lr:0.000010
[ Sun May 19 17:41:29 2024 ] 	Batch(1700/2353) done. Loss: 0.0118  lr:0.000010
[ Sun May 19 17:42:06 2024 ] 	Batch(1800/2353) done. Loss: 0.1300  lr:0.000010
[ Sun May 19 17:42:43 2024 ] 	Batch(1900/2353) done. Loss: 0.0051  lr:0.000010
[ Sun May 19 17:43:20 2024 ] 	Batch(2000/2353) done. Loss: 0.0129  lr:0.000010
[ Sun May 19 17:43:57 2024 ] 	Batch(2100/2353) done. Loss: 0.0417  lr:0.000010
[ Sun May 19 17:44:34 2024 ] 	Batch(2200/2353) done. Loss: 0.0152  lr:0.000010
[ Sun May 19 17:45:11 2024 ] 	Batch(2300/2353) done. Loss: 0.0175  lr:0.000010
[ Sun May 19 17:45:30 2024 ] 	Mean training loss: 0.0355.
[ Sun May 19 17:45:30 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 17:45:30 2024 ] Training epoch: 36
[ Sun May 19 17:45:31 2024 ] 	Batch(0/2353) done. Loss: 0.0125  lr:0.000010
[ Sun May 19 17:46:09 2024 ] 	Batch(100/2353) done. Loss: 0.0227  lr:0.000010
[ Sun May 19 17:46:46 2024 ] 	Batch(200/2353) done. Loss: 0.0194  lr:0.000010
[ Sun May 19 17:47:23 2024 ] 	Batch(300/2353) done. Loss: 0.0187  lr:0.000010
[ Sun May 19 17:48:00 2024 ] 	Batch(400/2353) done. Loss: 0.0425  lr:0.000010
[ Sun May 19 17:48:37 2024 ] 	Batch(500/2353) done. Loss: 0.0207  lr:0.000010
[ Sun May 19 17:49:13 2024 ] 	Batch(600/2353) done. Loss: 0.0092  lr:0.000010
[ Sun May 19 17:49:50 2024 ] 	Batch(700/2353) done. Loss: 0.0064  lr:0.000010
[ Sun May 19 17:50:27 2024 ] 	Batch(800/2353) done. Loss: 0.0329  lr:0.000010
[ Sun May 19 17:51:04 2024 ] 	Batch(900/2353) done. Loss: 0.1202  lr:0.000010
[ Sun May 19 17:51:41 2024 ] 	Batch(1000/2353) done. Loss: 0.0336  lr:0.000010
[ Sun May 19 17:52:18 2024 ] 	Batch(1100/2353) done. Loss: 0.0019  lr:0.000010
[ Sun May 19 17:52:55 2024 ] 	Batch(1200/2353) done. Loss: 0.0558  lr:0.000010
[ Sun May 19 17:53:32 2024 ] 	Batch(1300/2353) done. Loss: 0.0353  lr:0.000010
[ Sun May 19 17:54:10 2024 ] 	Batch(1400/2353) done. Loss: 0.0113  lr:0.000010
[ Sun May 19 17:54:47 2024 ] 	Batch(1500/2353) done. Loss: 0.0024  lr:0.000010
[ Sun May 19 17:55:24 2024 ] 	Batch(1600/2353) done. Loss: 0.0243  lr:0.000010
[ Sun May 19 17:56:00 2024 ] 	Batch(1700/2353) done. Loss: 0.2142  lr:0.000010
[ Sun May 19 17:56:37 2024 ] 	Batch(1800/2353) done. Loss: 0.0673  lr:0.000010
[ Sun May 19 17:57:14 2024 ] 	Batch(1900/2353) done. Loss: 0.0680  lr:0.000010
[ Sun May 19 17:57:51 2024 ] 	Batch(2000/2353) done. Loss: 0.0136  lr:0.000010
[ Sun May 19 17:58:28 2024 ] 	Batch(2100/2353) done. Loss: 0.0436  lr:0.000010
[ Sun May 19 17:59:05 2024 ] 	Batch(2200/2353) done. Loss: 0.0086  lr:0.000010
[ Sun May 19 17:59:42 2024 ] 	Batch(2300/2353) done. Loss: 0.0265  lr:0.000010
[ Sun May 19 18:00:01 2024 ] 	Mean training loss: 0.0390.
[ Sun May 19 18:00:01 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 18:00:01 2024 ] Training epoch: 37
[ Sun May 19 18:00:02 2024 ] 	Batch(0/2353) done. Loss: 0.0638  lr:0.000010
[ Sun May 19 18:00:38 2024 ] 	Batch(100/2353) done. Loss: 0.0490  lr:0.000010
[ Sun May 19 18:01:15 2024 ] 	Batch(200/2353) done. Loss: 0.1573  lr:0.000010
[ Sun May 19 18:01:52 2024 ] 	Batch(300/2353) done. Loss: 0.0258  lr:0.000010
[ Sun May 19 18:02:29 2024 ] 	Batch(400/2353) done. Loss: 0.0716  lr:0.000010
[ Sun May 19 18:03:06 2024 ] 	Batch(500/2353) done. Loss: 0.0148  lr:0.000010
[ Sun May 19 18:03:43 2024 ] 	Batch(600/2353) done. Loss: 0.0151  lr:0.000010
[ Sun May 19 18:04:20 2024 ] 	Batch(700/2353) done. Loss: 0.0087  lr:0.000010
[ Sun May 19 18:04:57 2024 ] 	Batch(800/2353) done. Loss: 0.0018  lr:0.000010
[ Sun May 19 18:05:34 2024 ] 	Batch(900/2353) done. Loss: 0.0316  lr:0.000010
[ Sun May 19 18:06:11 2024 ] 	Batch(1000/2353) done. Loss: 0.0950  lr:0.000010
[ Sun May 19 18:06:47 2024 ] 	Batch(1100/2353) done. Loss: 0.0329  lr:0.000010
[ Sun May 19 18:07:24 2024 ] 	Batch(1200/2353) done. Loss: 0.0638  lr:0.000010
[ Sun May 19 18:08:01 2024 ] 	Batch(1300/2353) done. Loss: 0.0050  lr:0.000010
[ Sun May 19 18:08:38 2024 ] 	Batch(1400/2353) done. Loss: 0.0279  lr:0.000010
[ Sun May 19 18:09:15 2024 ] 	Batch(1500/2353) done. Loss: 0.0740  lr:0.000010
[ Sun May 19 18:09:52 2024 ] 	Batch(1600/2353) done. Loss: 0.1122  lr:0.000010
[ Sun May 19 18:10:28 2024 ] 	Batch(1700/2353) done. Loss: 0.0139  lr:0.000010
[ Sun May 19 18:11:05 2024 ] 	Batch(1800/2353) done. Loss: 0.0125  lr:0.000010
[ Sun May 19 18:11:42 2024 ] 	Batch(1900/2353) done. Loss: 0.0990  lr:0.000010
[ Sun May 19 18:12:19 2024 ] 	Batch(2000/2353) done. Loss: 0.0049  lr:0.000010
[ Sun May 19 18:12:57 2024 ] 	Batch(2100/2353) done. Loss: 0.0054  lr:0.000010
[ Sun May 19 18:13:34 2024 ] 	Batch(2200/2353) done. Loss: 0.0301  lr:0.000010
[ Sun May 19 18:14:12 2024 ] 	Batch(2300/2353) done. Loss: 0.0872  lr:0.000010
[ Sun May 19 18:14:31 2024 ] 	Mean training loss: 0.0382.
[ Sun May 19 18:14:31 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 18:14:31 2024 ] Training epoch: 38
[ Sun May 19 18:14:32 2024 ] 	Batch(0/2353) done. Loss: 0.0068  lr:0.000010
[ Sun May 19 18:15:08 2024 ] 	Batch(100/2353) done. Loss: 0.0359  lr:0.000010
[ Sun May 19 18:15:45 2024 ] 	Batch(200/2353) done. Loss: 0.0151  lr:0.000010
[ Sun May 19 18:16:22 2024 ] 	Batch(300/2353) done. Loss: 0.0114  lr:0.000010
[ Sun May 19 18:16:59 2024 ] 	Batch(400/2353) done. Loss: 0.0081  lr:0.000010
[ Sun May 19 18:17:36 2024 ] 	Batch(500/2353) done. Loss: 0.0496  lr:0.000010
[ Sun May 19 18:18:13 2024 ] 	Batch(600/2353) done. Loss: 0.1616  lr:0.000010
[ Sun May 19 18:18:50 2024 ] 	Batch(700/2353) done. Loss: 0.0458  lr:0.000010
[ Sun May 19 18:19:27 2024 ] 	Batch(800/2353) done. Loss: 0.0314  lr:0.000010
[ Sun May 19 18:20:04 2024 ] 	Batch(900/2353) done. Loss: 0.0089  lr:0.000010
[ Sun May 19 18:20:41 2024 ] 	Batch(1000/2353) done. Loss: 0.0100  lr:0.000010
[ Sun May 19 18:21:18 2024 ] 	Batch(1100/2353) done. Loss: 0.1229  lr:0.000010
[ Sun May 19 18:21:55 2024 ] 	Batch(1200/2353) done. Loss: 0.0056  lr:0.000010
[ Sun May 19 18:22:31 2024 ] 	Batch(1300/2353) done. Loss: 0.0063  lr:0.000010
[ Sun May 19 18:23:08 2024 ] 	Batch(1400/2353) done. Loss: 0.0073  lr:0.000010
[ Sun May 19 18:23:45 2024 ] 	Batch(1500/2353) done. Loss: 0.1111  lr:0.000010
[ Sun May 19 18:24:22 2024 ] 	Batch(1600/2353) done. Loss: 0.0094  lr:0.000010
[ Sun May 19 18:24:59 2024 ] 	Batch(1700/2353) done. Loss: 0.0526  lr:0.000010
[ Sun May 19 18:25:36 2024 ] 	Batch(1800/2353) done. Loss: 0.0254  lr:0.000010
[ Sun May 19 18:26:13 2024 ] 	Batch(1900/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 18:26:50 2024 ] 	Batch(2000/2353) done. Loss: 0.0370  lr:0.000010
[ Sun May 19 18:27:27 2024 ] 	Batch(2100/2353) done. Loss: 0.0224  lr:0.000010
[ Sun May 19 18:28:03 2024 ] 	Batch(2200/2353) done. Loss: 0.0811  lr:0.000010
[ Sun May 19 18:28:40 2024 ] 	Batch(2300/2353) done. Loss: 0.0221  lr:0.000010
[ Sun May 19 18:28:59 2024 ] 	Mean training loss: 0.0376.
[ Sun May 19 18:28:59 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 18:29:00 2024 ] Training epoch: 39
[ Sun May 19 18:29:00 2024 ] 	Batch(0/2353) done. Loss: 0.0121  lr:0.000010
[ Sun May 19 18:29:37 2024 ] 	Batch(100/2353) done. Loss: 0.0212  lr:0.000010
[ Sun May 19 18:30:14 2024 ] 	Batch(200/2353) done. Loss: 0.0336  lr:0.000010
[ Sun May 19 18:30:51 2024 ] 	Batch(300/2353) done. Loss: 0.0135  lr:0.000010
[ Sun May 19 18:31:28 2024 ] 	Batch(400/2353) done. Loss: 0.0057  lr:0.000010
[ Sun May 19 18:32:05 2024 ] 	Batch(500/2353) done. Loss: 0.0241  lr:0.000010
[ Sun May 19 18:32:41 2024 ] 	Batch(600/2353) done. Loss: 0.0030  lr:0.000010
[ Sun May 19 18:33:18 2024 ] 	Batch(700/2353) done. Loss: 0.0474  lr:0.000010
[ Sun May 19 18:33:55 2024 ] 	Batch(800/2353) done. Loss: 0.0742  lr:0.000010
[ Sun May 19 18:34:32 2024 ] 	Batch(900/2353) done. Loss: 0.0091  lr:0.000010
[ Sun May 19 18:35:09 2024 ] 	Batch(1000/2353) done. Loss: 0.0111  lr:0.000010
[ Sun May 19 18:35:46 2024 ] 	Batch(1100/2353) done. Loss: 0.1093  lr:0.000010
[ Sun May 19 18:36:23 2024 ] 	Batch(1200/2353) done. Loss: 0.0372  lr:0.000010
[ Sun May 19 18:37:00 2024 ] 	Batch(1300/2353) done. Loss: 0.0834  lr:0.000010
[ Sun May 19 18:37:37 2024 ] 	Batch(1400/2353) done. Loss: 0.0060  lr:0.000010
[ Sun May 19 18:38:13 2024 ] 	Batch(1500/2353) done. Loss: 0.0158  lr:0.000010
[ Sun May 19 18:38:50 2024 ] 	Batch(1600/2353) done. Loss: 0.0399  lr:0.000010
[ Sun May 19 18:39:27 2024 ] 	Batch(1700/2353) done. Loss: 0.0096  lr:0.000010
[ Sun May 19 18:40:04 2024 ] 	Batch(1800/2353) done. Loss: 0.0136  lr:0.000010
[ Sun May 19 18:40:41 2024 ] 	Batch(1900/2353) done. Loss: 0.0048  lr:0.000010
[ Sun May 19 18:41:18 2024 ] 	Batch(2000/2353) done. Loss: 0.0274  lr:0.000010
[ Sun May 19 18:41:55 2024 ] 	Batch(2100/2353) done. Loss: 0.0542  lr:0.000010
[ Sun May 19 18:42:32 2024 ] 	Batch(2200/2353) done. Loss: 0.0194  lr:0.000010
[ Sun May 19 18:43:08 2024 ] 	Batch(2300/2353) done. Loss: 0.0296  lr:0.000010
[ Sun May 19 18:43:28 2024 ] 	Mean training loss: 0.0342.
[ Sun May 19 18:43:28 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 18:43:28 2024 ] Training epoch: 40
[ Sun May 19 18:43:29 2024 ] 	Batch(0/2353) done. Loss: 0.0756  lr:0.000010
[ Sun May 19 18:44:05 2024 ] 	Batch(100/2353) done. Loss: 0.0164  lr:0.000010
[ Sun May 19 18:44:42 2024 ] 	Batch(200/2353) done. Loss: 0.0049  lr:0.000010
[ Sun May 19 18:45:19 2024 ] 	Batch(300/2353) done. Loss: 0.0405  lr:0.000010
[ Sun May 19 18:45:56 2024 ] 	Batch(400/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 18:46:33 2024 ] 	Batch(500/2353) done. Loss: 0.0066  lr:0.000010
[ Sun May 19 18:47:10 2024 ] 	Batch(600/2353) done. Loss: 0.0264  lr:0.000010
[ Sun May 19 18:47:47 2024 ] 	Batch(700/2353) done. Loss: 0.0695  lr:0.000010
[ Sun May 19 18:48:24 2024 ] 	Batch(800/2353) done. Loss: 0.0253  lr:0.000010
[ Sun May 19 18:49:00 2024 ] 	Batch(900/2353) done. Loss: 0.0557  lr:0.000010
[ Sun May 19 18:49:37 2024 ] 	Batch(1000/2353) done. Loss: 0.0427  lr:0.000010
[ Sun May 19 18:50:14 2024 ] 	Batch(1100/2353) done. Loss: 0.0658  lr:0.000010
[ Sun May 19 18:50:51 2024 ] 	Batch(1200/2353) done. Loss: 0.1467  lr:0.000010
[ Sun May 19 18:51:28 2024 ] 	Batch(1300/2353) done. Loss: 0.0111  lr:0.000010
[ Sun May 19 18:52:05 2024 ] 	Batch(1400/2353) done. Loss: 0.0268  lr:0.000010
[ Sun May 19 18:52:41 2024 ] 	Batch(1500/2353) done. Loss: 0.0083  lr:0.000010
[ Sun May 19 18:53:18 2024 ] 	Batch(1600/2353) done. Loss: 0.0607  lr:0.000010
[ Sun May 19 18:53:55 2024 ] 	Batch(1700/2353) done. Loss: 0.0183  lr:0.000010
[ Sun May 19 18:54:32 2024 ] 	Batch(1800/2353) done. Loss: 0.0500  lr:0.000010
[ Sun May 19 18:55:09 2024 ] 	Batch(1900/2353) done. Loss: 0.0508  lr:0.000010
[ Sun May 19 18:55:46 2024 ] 	Batch(2000/2353) done. Loss: 0.0142  lr:0.000010
[ Sun May 19 18:56:23 2024 ] 	Batch(2100/2353) done. Loss: 0.0456  lr:0.000010
[ Sun May 19 18:56:59 2024 ] 	Batch(2200/2353) done. Loss: 0.0090  lr:0.000010
[ Sun May 19 18:57:36 2024 ] 	Batch(2300/2353) done. Loss: 0.0169  lr:0.000010
[ Sun May 19 18:57:55 2024 ] 	Mean training loss: 0.0348.
[ Sun May 19 18:57:55 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 18:57:55 2024 ] Eval epoch: 40
[ Sun May 19 18:59:59 2024 ] 	Mean val loss of 2367 batches: 0.2468764457021181.
[ Sun May 19 18:59:59 2024 ] Training epoch: 41
[ Sun May 19 19:00:00 2024 ] 	Batch(0/2353) done. Loss: 0.1321  lr:0.000010
[ Sun May 19 19:00:37 2024 ] 	Batch(100/2353) done. Loss: 0.0402  lr:0.000010
[ Sun May 19 19:01:15 2024 ] 	Batch(200/2353) done. Loss: 0.0098  lr:0.000010
[ Sun May 19 19:01:52 2024 ] 	Batch(300/2353) done. Loss: 0.0618  lr:0.000010
[ Sun May 19 19:02:30 2024 ] 	Batch(400/2353) done. Loss: 0.0462  lr:0.000010
[ Sun May 19 19:03:07 2024 ] 	Batch(500/2353) done. Loss: 0.0341  lr:0.000010
[ Sun May 19 19:03:44 2024 ] 	Batch(600/2353) done. Loss: 0.0370  lr:0.000010
[ Sun May 19 19:04:21 2024 ] 	Batch(700/2353) done. Loss: 0.0169  lr:0.000010
[ Sun May 19 19:04:58 2024 ] 	Batch(800/2353) done. Loss: 0.0069  lr:0.000010
[ Sun May 19 19:05:35 2024 ] 	Batch(900/2353) done. Loss: 0.1494  lr:0.000010
[ Sun May 19 19:06:12 2024 ] 	Batch(1000/2353) done. Loss: 0.0161  lr:0.000010
[ Sun May 19 19:06:49 2024 ] 	Batch(1100/2353) done. Loss: 0.0533  lr:0.000010
[ Sun May 19 19:07:26 2024 ] 	Batch(1200/2353) done. Loss: 0.0121  lr:0.000010
[ Sun May 19 19:08:03 2024 ] 	Batch(1300/2353) done. Loss: 0.0054  lr:0.000010
[ Sun May 19 19:08:40 2024 ] 	Batch(1400/2353) done. Loss: 0.0131  lr:0.000010
[ Sun May 19 19:09:16 2024 ] 	Batch(1500/2353) done. Loss: 0.0239  lr:0.000010
[ Sun May 19 19:09:53 2024 ] 	Batch(1600/2353) done. Loss: 0.0328  lr:0.000010
[ Sun May 19 19:10:30 2024 ] 	Batch(1700/2353) done. Loss: 0.0269  lr:0.000010
[ Sun May 19 19:11:07 2024 ] 	Batch(1800/2353) done. Loss: 0.0044  lr:0.000010
[ Sun May 19 19:11:44 2024 ] 	Batch(1900/2353) done. Loss: 0.0036  lr:0.000010
[ Sun May 19 19:12:20 2024 ] 	Batch(2000/2353) done. Loss: 0.0741  lr:0.000010
[ Sun May 19 19:12:57 2024 ] 	Batch(2100/2353) done. Loss: 0.0272  lr:0.000010
[ Sun May 19 19:13:34 2024 ] 	Batch(2200/2353) done. Loss: 0.0312  lr:0.000010
[ Sun May 19 19:14:10 2024 ] 	Batch(2300/2353) done. Loss: 0.0122  lr:0.000010
[ Sun May 19 19:14:30 2024 ] 	Mean training loss: 0.0347.
[ Sun May 19 19:14:30 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 19:14:30 2024 ] Training epoch: 42
[ Sun May 19 19:14:30 2024 ] 	Batch(0/2353) done. Loss: 0.1147  lr:0.000010
[ Sun May 19 19:15:07 2024 ] 	Batch(100/2353) done. Loss: 0.0333  lr:0.000010
[ Sun May 19 19:15:44 2024 ] 	Batch(200/2353) done. Loss: 0.0457  lr:0.000010
[ Sun May 19 19:16:20 2024 ] 	Batch(300/2353) done. Loss: 0.0077  lr:0.000010
[ Sun May 19 19:16:57 2024 ] 	Batch(400/2353) done. Loss: 0.1508  lr:0.000010
[ Sun May 19 19:17:34 2024 ] 	Batch(500/2353) done. Loss: 0.0130  lr:0.000010
[ Sun May 19 19:18:10 2024 ] 	Batch(600/2353) done. Loss: 0.0117  lr:0.000010
[ Sun May 19 19:18:47 2024 ] 	Batch(700/2353) done. Loss: 0.0259  lr:0.000010
[ Sun May 19 19:19:23 2024 ] 	Batch(800/2353) done. Loss: 0.0473  lr:0.000010
[ Sun May 19 19:20:00 2024 ] 	Batch(900/2353) done. Loss: 0.0876  lr:0.000010
[ Sun May 19 19:20:37 2024 ] 	Batch(1000/2353) done. Loss: 0.1093  lr:0.000010
[ Sun May 19 19:21:13 2024 ] 	Batch(1100/2353) done. Loss: 0.0083  lr:0.000010
[ Sun May 19 19:21:50 2024 ] 	Batch(1200/2353) done. Loss: 0.0232  lr:0.000010
[ Sun May 19 19:22:27 2024 ] 	Batch(1300/2353) done. Loss: 0.0151  lr:0.000010
[ Sun May 19 19:23:03 2024 ] 	Batch(1400/2353) done. Loss: 0.0190  lr:0.000010
[ Sun May 19 19:23:40 2024 ] 	Batch(1500/2353) done. Loss: 0.0112  lr:0.000010
[ Sun May 19 19:24:16 2024 ] 	Batch(1600/2353) done. Loss: 0.0338  lr:0.000010
[ Sun May 19 19:24:53 2024 ] 	Batch(1700/2353) done. Loss: 0.0095  lr:0.000010
[ Sun May 19 19:25:30 2024 ] 	Batch(1800/2353) done. Loss: 0.0143  lr:0.000010
[ Sun May 19 19:26:06 2024 ] 	Batch(1900/2353) done. Loss: 0.1191  lr:0.000010
[ Sun May 19 19:26:43 2024 ] 	Batch(2000/2353) done. Loss: 0.0648  lr:0.000010
[ Sun May 19 19:27:20 2024 ] 	Batch(2100/2353) done. Loss: 0.0067  lr:0.000010
[ Sun May 19 19:27:57 2024 ] 	Batch(2200/2353) done. Loss: 0.0445  lr:0.000010
[ Sun May 19 19:28:33 2024 ] 	Batch(2300/2353) done. Loss: 0.0174  lr:0.000010
[ Sun May 19 19:28:52 2024 ] 	Mean training loss: 0.0361.
[ Sun May 19 19:28:52 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 19:28:52 2024 ] Training epoch: 43
[ Sun May 19 19:28:53 2024 ] 	Batch(0/2353) done. Loss: 0.0534  lr:0.000010
[ Sun May 19 19:29:30 2024 ] 	Batch(100/2353) done. Loss: 0.0599  lr:0.000010
[ Sun May 19 19:30:07 2024 ] 	Batch(200/2353) done. Loss: 0.0307  lr:0.000010
[ Sun May 19 19:30:43 2024 ] 	Batch(300/2353) done. Loss: 0.0566  lr:0.000010
[ Sun May 19 19:31:20 2024 ] 	Batch(400/2353) done. Loss: 0.0575  lr:0.000010
[ Sun May 19 19:31:57 2024 ] 	Batch(500/2353) done. Loss: 0.1229  lr:0.000010
[ Sun May 19 19:32:33 2024 ] 	Batch(600/2353) done. Loss: 0.0058  lr:0.000010
[ Sun May 19 19:33:10 2024 ] 	Batch(700/2353) done. Loss: 0.0077  lr:0.000010
[ Sun May 19 19:33:47 2024 ] 	Batch(800/2353) done. Loss: 0.0414  lr:0.000010
[ Sun May 19 19:34:23 2024 ] 	Batch(900/2353) done. Loss: 0.0351  lr:0.000010
[ Sun May 19 19:35:00 2024 ] 	Batch(1000/2353) done. Loss: 0.1155  lr:0.000010
[ Sun May 19 19:35:37 2024 ] 	Batch(1100/2353) done. Loss: 0.0075  lr:0.000010
[ Sun May 19 19:36:13 2024 ] 	Batch(1200/2353) done. Loss: 0.0066  lr:0.000010
[ Sun May 19 19:36:50 2024 ] 	Batch(1300/2353) done. Loss: 0.0289  lr:0.000010
[ Sun May 19 19:37:27 2024 ] 	Batch(1400/2353) done. Loss: 0.0218  lr:0.000010
[ Sun May 19 19:38:03 2024 ] 	Batch(1500/2353) done. Loss: 0.0440  lr:0.000010
[ Sun May 19 19:38:40 2024 ] 	Batch(1600/2353) done. Loss: 0.0287  lr:0.000010
[ Sun May 19 19:39:17 2024 ] 	Batch(1700/2353) done. Loss: 0.1707  lr:0.000010
[ Sun May 19 19:39:54 2024 ] 	Batch(1800/2353) done. Loss: 0.0086  lr:0.000010
[ Sun May 19 19:40:30 2024 ] 	Batch(1900/2353) done. Loss: 0.0308  lr:0.000010
[ Sun May 19 19:41:07 2024 ] 	Batch(2000/2353) done. Loss: 0.2264  lr:0.000010
[ Sun May 19 19:41:44 2024 ] 	Batch(2100/2353) done. Loss: 0.0426  lr:0.000010
[ Sun May 19 19:42:20 2024 ] 	Batch(2200/2353) done. Loss: 0.0163  lr:0.000010
[ Sun May 19 19:42:57 2024 ] 	Batch(2300/2353) done. Loss: 0.0855  lr:0.000010
[ Sun May 19 19:43:16 2024 ] 	Mean training loss: 0.0361.
[ Sun May 19 19:43:16 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 19:43:16 2024 ] Training epoch: 44
[ Sun May 19 19:43:17 2024 ] 	Batch(0/2353) done. Loss: 0.0405  lr:0.000010
[ Sun May 19 19:43:53 2024 ] 	Batch(100/2353) done. Loss: 0.0278  lr:0.000010
[ Sun May 19 19:44:30 2024 ] 	Batch(200/2353) done. Loss: 0.0325  lr:0.000010
[ Sun May 19 19:45:06 2024 ] 	Batch(300/2353) done. Loss: 0.0734  lr:0.000010
[ Sun May 19 19:45:43 2024 ] 	Batch(400/2353) done. Loss: 0.0425  lr:0.000010
[ Sun May 19 19:46:20 2024 ] 	Batch(500/2353) done. Loss: 0.0187  lr:0.000010
[ Sun May 19 19:46:56 2024 ] 	Batch(600/2353) done. Loss: 0.0262  lr:0.000010
[ Sun May 19 19:47:33 2024 ] 	Batch(700/2353) done. Loss: 0.0186  lr:0.000010
[ Sun May 19 19:48:10 2024 ] 	Batch(800/2353) done. Loss: 0.0336  lr:0.000010
[ Sun May 19 19:48:46 2024 ] 	Batch(900/2353) done. Loss: 0.0191  lr:0.000010
[ Sun May 19 19:49:23 2024 ] 	Batch(1000/2353) done. Loss: 0.0464  lr:0.000010
[ Sun May 19 19:50:00 2024 ] 	Batch(1100/2353) done. Loss: 0.0057  lr:0.000010
[ Sun May 19 19:50:36 2024 ] 	Batch(1200/2353) done. Loss: 0.0091  lr:0.000010
[ Sun May 19 19:51:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0074  lr:0.000010
[ Sun May 19 19:51:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0198  lr:0.000010
[ Sun May 19 19:52:26 2024 ] 	Batch(1500/2353) done. Loss: 0.0171  lr:0.000010
[ Sun May 19 19:53:03 2024 ] 	Batch(1600/2353) done. Loss: 0.0022  lr:0.000010
[ Sun May 19 19:53:40 2024 ] 	Batch(1700/2353) done. Loss: 0.0392  lr:0.000010
[ Sun May 19 19:54:16 2024 ] 	Batch(1800/2353) done. Loss: 0.0030  lr:0.000010
[ Sun May 19 19:54:53 2024 ] 	Batch(1900/2353) done. Loss: 0.0014  lr:0.000010
[ Sun May 19 19:55:29 2024 ] 	Batch(2000/2353) done. Loss: 0.0216  lr:0.000010
[ Sun May 19 19:56:06 2024 ] 	Batch(2100/2353) done. Loss: 0.0157  lr:0.000010
[ Sun May 19 19:56:43 2024 ] 	Batch(2200/2353) done. Loss: 0.0164  lr:0.000010
[ Sun May 19 19:57:19 2024 ] 	Batch(2300/2353) done. Loss: 0.0251  lr:0.000010
[ Sun May 19 19:57:38 2024 ] 	Mean training loss: 0.0344.
[ Sun May 19 19:57:38 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 19:57:39 2024 ] Training epoch: 45
[ Sun May 19 19:57:39 2024 ] 	Batch(0/2353) done. Loss: 0.0677  lr:0.000010
[ Sun May 19 19:58:16 2024 ] 	Batch(100/2353) done. Loss: 0.0508  lr:0.000010
[ Sun May 19 19:58:53 2024 ] 	Batch(200/2353) done. Loss: 0.0654  lr:0.000010
[ Sun May 19 19:59:29 2024 ] 	Batch(300/2353) done. Loss: 0.0442  lr:0.000010
[ Sun May 19 20:00:06 2024 ] 	Batch(400/2353) done. Loss: 0.0572  lr:0.000010
[ Sun May 19 20:00:43 2024 ] 	Batch(500/2353) done. Loss: 0.0236  lr:0.000010
[ Sun May 19 20:01:19 2024 ] 	Batch(600/2353) done. Loss: 0.0067  lr:0.000010
[ Sun May 19 20:01:56 2024 ] 	Batch(700/2353) done. Loss: 0.0030  lr:0.000010
[ Sun May 19 20:02:33 2024 ] 	Batch(800/2353) done. Loss: 0.0045  lr:0.000010
[ Sun May 19 20:03:09 2024 ] 	Batch(900/2353) done. Loss: 0.1194  lr:0.000010
[ Sun May 19 20:03:46 2024 ] 	Batch(1000/2353) done. Loss: 0.0237  lr:0.000010
[ Sun May 19 20:04:22 2024 ] 	Batch(1100/2353) done. Loss: 0.0278  lr:0.000010
[ Sun May 19 20:04:59 2024 ] 	Batch(1200/2353) done. Loss: 0.0073  lr:0.000010
[ Sun May 19 20:05:36 2024 ] 	Batch(1300/2353) done. Loss: 0.0104  lr:0.000010
[ Sun May 19 20:06:14 2024 ] 	Batch(1400/2353) done. Loss: 0.0399  lr:0.000010
[ Sun May 19 20:06:51 2024 ] 	Batch(1500/2353) done. Loss: 0.0365  lr:0.000010
[ Sun May 19 20:07:28 2024 ] 	Batch(1600/2353) done. Loss: 0.0185  lr:0.000010
[ Sun May 19 20:08:05 2024 ] 	Batch(1700/2353) done. Loss: 0.1442  lr:0.000010
[ Sun May 19 20:08:43 2024 ] 	Batch(1800/2353) done. Loss: 0.0075  lr:0.000010
[ Sun May 19 20:09:20 2024 ] 	Batch(1900/2353) done. Loss: 0.0134  lr:0.000010
[ Sun May 19 20:09:57 2024 ] 	Batch(2000/2353) done. Loss: 0.0236  lr:0.000010
[ Sun May 19 20:10:34 2024 ] 	Batch(2100/2353) done. Loss: 0.0078  lr:0.000010
[ Sun May 19 20:11:11 2024 ] 	Batch(2200/2353) done. Loss: 0.0192  lr:0.000010
[ Sun May 19 20:11:47 2024 ] 	Batch(2300/2353) done. Loss: 0.0073  lr:0.000010
[ Sun May 19 20:12:06 2024 ] 	Mean training loss: 0.0339.
[ Sun May 19 20:12:06 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 20:12:07 2024 ] Training epoch: 46
[ Sun May 19 20:12:07 2024 ] 	Batch(0/2353) done. Loss: 0.0074  lr:0.000010
[ Sun May 19 20:12:44 2024 ] 	Batch(100/2353) done. Loss: 0.0079  lr:0.000010
[ Sun May 19 20:13:20 2024 ] 	Batch(200/2353) done. Loss: 0.0194  lr:0.000010
[ Sun May 19 20:13:57 2024 ] 	Batch(300/2353) done. Loss: 0.0190  lr:0.000010
[ Sun May 19 20:14:34 2024 ] 	Batch(400/2353) done. Loss: 0.0017  lr:0.000010
[ Sun May 19 20:15:10 2024 ] 	Batch(500/2353) done. Loss: 0.2237  lr:0.000010
[ Sun May 19 20:15:47 2024 ] 	Batch(600/2353) done. Loss: 0.0062  lr:0.000010
[ Sun May 19 20:16:24 2024 ] 	Batch(700/2353) done. Loss: 0.0013  lr:0.000010
[ Sun May 19 20:17:01 2024 ] 	Batch(800/2353) done. Loss: 0.0091  lr:0.000010
[ Sun May 19 20:17:37 2024 ] 	Batch(900/2353) done. Loss: 0.0110  lr:0.000010
[ Sun May 19 20:18:14 2024 ] 	Batch(1000/2353) done. Loss: 0.0029  lr:0.000010
[ Sun May 19 20:18:51 2024 ] 	Batch(1100/2353) done. Loss: 0.0162  lr:0.000010
[ Sun May 19 20:19:28 2024 ] 	Batch(1200/2353) done. Loss: 0.0103  lr:0.000010
[ Sun May 19 20:20:05 2024 ] 	Batch(1300/2353) done. Loss: 0.0053  lr:0.000010
[ Sun May 19 20:20:41 2024 ] 	Batch(1400/2353) done. Loss: 0.1032  lr:0.000010
[ Sun May 19 20:21:18 2024 ] 	Batch(1500/2353) done. Loss: 0.0074  lr:0.000010
[ Sun May 19 20:21:55 2024 ] 	Batch(1600/2353) done. Loss: 0.0257  lr:0.000010
[ Sun May 19 20:22:31 2024 ] 	Batch(1700/2353) done. Loss: 0.0225  lr:0.000010
[ Sun May 19 20:23:08 2024 ] 	Batch(1800/2353) done. Loss: 0.0109  lr:0.000010
[ Sun May 19 20:23:44 2024 ] 	Batch(1900/2353) done. Loss: 0.0052  lr:0.000010
[ Sun May 19 20:24:21 2024 ] 	Batch(2000/2353) done. Loss: 0.0499  lr:0.000010
[ Sun May 19 20:24:58 2024 ] 	Batch(2100/2353) done. Loss: 0.0059  lr:0.000010
[ Sun May 19 20:25:34 2024 ] 	Batch(2200/2353) done. Loss: 0.0180  lr:0.000010
[ Sun May 19 20:26:11 2024 ] 	Batch(2300/2353) done. Loss: 0.0269  lr:0.000010
[ Sun May 19 20:26:30 2024 ] 	Mean training loss: 0.0331.
[ Sun May 19 20:26:30 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 20:26:31 2024 ] Training epoch: 47
[ Sun May 19 20:26:31 2024 ] 	Batch(0/2353) done. Loss: 0.0358  lr:0.000010
[ Sun May 19 20:27:08 2024 ] 	Batch(100/2353) done. Loss: 0.0183  lr:0.000010
[ Sun May 19 20:27:44 2024 ] 	Batch(200/2353) done. Loss: 0.0136  lr:0.000010
[ Sun May 19 20:28:21 2024 ] 	Batch(300/2353) done. Loss: 0.0031  lr:0.000010
[ Sun May 19 20:28:58 2024 ] 	Batch(400/2353) done. Loss: 0.0272  lr:0.000010
[ Sun May 19 20:29:34 2024 ] 	Batch(500/2353) done. Loss: 0.0165  lr:0.000010
[ Sun May 19 20:30:11 2024 ] 	Batch(600/2353) done. Loss: 0.0236  lr:0.000010
[ Sun May 19 20:30:47 2024 ] 	Batch(700/2353) done. Loss: 0.0304  lr:0.000010
[ Sun May 19 20:31:24 2024 ] 	Batch(800/2353) done. Loss: 0.0118  lr:0.000010
[ Sun May 19 20:32:01 2024 ] 	Batch(900/2353) done. Loss: 0.0298  lr:0.000010
[ Sun May 19 20:32:38 2024 ] 	Batch(1000/2353) done. Loss: 0.0016  lr:0.000010
[ Sun May 19 20:33:15 2024 ] 	Batch(1100/2353) done. Loss: 0.0255  lr:0.000010
[ Sun May 19 20:33:51 2024 ] 	Batch(1200/2353) done. Loss: 0.0125  lr:0.000010
[ Sun May 19 20:34:28 2024 ] 	Batch(1300/2353) done. Loss: 0.0475  lr:0.000010
[ Sun May 19 20:35:05 2024 ] 	Batch(1400/2353) done. Loss: 0.0230  lr:0.000010
[ Sun May 19 20:35:41 2024 ] 	Batch(1500/2353) done. Loss: 0.0392  lr:0.000010
[ Sun May 19 20:36:18 2024 ] 	Batch(1600/2353) done. Loss: 0.0810  lr:0.000010
[ Sun May 19 20:36:55 2024 ] 	Batch(1700/2353) done. Loss: 0.0513  lr:0.000010
[ Sun May 19 20:37:31 2024 ] 	Batch(1800/2353) done. Loss: 0.0016  lr:0.000010
[ Sun May 19 20:38:08 2024 ] 	Batch(1900/2353) done. Loss: 0.0096  lr:0.000010
[ Sun May 19 20:38:45 2024 ] 	Batch(2000/2353) done. Loss: 0.0321  lr:0.000010
[ Sun May 19 20:39:21 2024 ] 	Batch(2100/2353) done. Loss: 0.1402  lr:0.000010
[ Sun May 19 20:39:58 2024 ] 	Batch(2200/2353) done. Loss: 0.0130  lr:0.000010
[ Sun May 19 20:40:34 2024 ] 	Batch(2300/2353) done. Loss: 0.0116  lr:0.000010
[ Sun May 19 20:40:54 2024 ] 	Mean training loss: 0.0353.
[ Sun May 19 20:40:54 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 20:40:54 2024 ] Training epoch: 48
[ Sun May 19 20:40:55 2024 ] 	Batch(0/2353) done. Loss: 0.0033  lr:0.000010
[ Sun May 19 20:41:31 2024 ] 	Batch(100/2353) done. Loss: 0.0145  lr:0.000010
[ Sun May 19 20:42:08 2024 ] 	Batch(200/2353) done. Loss: 0.0413  lr:0.000010
[ Sun May 19 20:42:44 2024 ] 	Batch(300/2353) done. Loss: 0.0222  lr:0.000010
[ Sun May 19 20:43:21 2024 ] 	Batch(400/2353) done. Loss: 0.0089  lr:0.000010
[ Sun May 19 20:43:58 2024 ] 	Batch(500/2353) done. Loss: 0.0666  lr:0.000010
[ Sun May 19 20:44:34 2024 ] 	Batch(600/2353) done. Loss: 0.0851  lr:0.000010
[ Sun May 19 20:45:11 2024 ] 	Batch(700/2353) done. Loss: 0.0942  lr:0.000010
[ Sun May 19 20:45:47 2024 ] 	Batch(800/2353) done. Loss: 0.0071  lr:0.000010
[ Sun May 19 20:46:24 2024 ] 	Batch(900/2353) done. Loss: 0.0069  lr:0.000010
[ Sun May 19 20:47:00 2024 ] 	Batch(1000/2353) done. Loss: 0.0311  lr:0.000010
[ Sun May 19 20:47:37 2024 ] 	Batch(1100/2353) done. Loss: 0.0652  lr:0.000010
[ Sun May 19 20:48:14 2024 ] 	Batch(1200/2353) done. Loss: 0.0106  lr:0.000010
[ Sun May 19 20:48:50 2024 ] 	Batch(1300/2353) done. Loss: 0.0273  lr:0.000010
[ Sun May 19 20:49:27 2024 ] 	Batch(1400/2353) done. Loss: 0.0363  lr:0.000010
[ Sun May 19 20:50:03 2024 ] 	Batch(1500/2353) done. Loss: 0.0288  lr:0.000010
[ Sun May 19 20:50:40 2024 ] 	Batch(1600/2353) done. Loss: 0.0088  lr:0.000010
[ Sun May 19 20:51:17 2024 ] 	Batch(1700/2353) done. Loss: 0.0480  lr:0.000010
[ Sun May 19 20:51:53 2024 ] 	Batch(1800/2353) done. Loss: 0.0298  lr:0.000010
[ Sun May 19 20:52:30 2024 ] 	Batch(1900/2353) done. Loss: 0.0130  lr:0.000010
[ Sun May 19 20:53:07 2024 ] 	Batch(2000/2353) done. Loss: 0.0513  lr:0.000010
[ Sun May 19 20:53:43 2024 ] 	Batch(2100/2353) done. Loss: 0.0130  lr:0.000010
[ Sun May 19 20:54:20 2024 ] 	Batch(2200/2353) done. Loss: 0.0303  lr:0.000010
[ Sun May 19 20:54:56 2024 ] 	Batch(2300/2353) done. Loss: 0.0050  lr:0.000010
[ Sun May 19 20:55:16 2024 ] 	Mean training loss: 0.0357.
[ Sun May 19 20:55:16 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 20:55:16 2024 ] Training epoch: 49
[ Sun May 19 20:55:16 2024 ] 	Batch(0/2353) done. Loss: 0.0063  lr:0.000010
[ Sun May 19 20:55:53 2024 ] 	Batch(100/2353) done. Loss: 0.0070  lr:0.000010
[ Sun May 19 20:56:29 2024 ] 	Batch(200/2353) done. Loss: 0.0227  lr:0.000010
[ Sun May 19 20:57:06 2024 ] 	Batch(300/2353) done. Loss: 0.0573  lr:0.000010
[ Sun May 19 20:57:43 2024 ] 	Batch(400/2353) done. Loss: 0.0020  lr:0.000010
[ Sun May 19 20:58:19 2024 ] 	Batch(500/2353) done. Loss: 0.1263  lr:0.000010
[ Sun May 19 20:58:56 2024 ] 	Batch(600/2353) done. Loss: 0.0120  lr:0.000010
[ Sun May 19 20:59:33 2024 ] 	Batch(700/2353) done. Loss: 0.0216  lr:0.000010
[ Sun May 19 21:00:09 2024 ] 	Batch(800/2353) done. Loss: 0.0157  lr:0.000010
[ Sun May 19 21:00:46 2024 ] 	Batch(900/2353) done. Loss: 0.0144  lr:0.000010
[ Sun May 19 21:01:23 2024 ] 	Batch(1000/2353) done. Loss: 0.0028  lr:0.000010
[ Sun May 19 21:01:59 2024 ] 	Batch(1100/2353) done. Loss: 0.0067  lr:0.000010
[ Sun May 19 21:02:36 2024 ] 	Batch(1200/2353) done. Loss: 0.1087  lr:0.000010
[ Sun May 19 21:03:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0194  lr:0.000010
[ Sun May 19 21:03:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0319  lr:0.000010
[ Sun May 19 21:04:26 2024 ] 	Batch(1500/2353) done. Loss: 0.0038  lr:0.000010
[ Sun May 19 21:05:03 2024 ] 	Batch(1600/2353) done. Loss: 0.0055  lr:0.000010
[ Sun May 19 21:05:39 2024 ] 	Batch(1700/2353) done. Loss: 0.0525  lr:0.000010
[ Sun May 19 21:06:16 2024 ] 	Batch(1800/2353) done. Loss: 0.0259  lr:0.000010
[ Sun May 19 21:06:52 2024 ] 	Batch(1900/2353) done. Loss: 0.0258  lr:0.000010
[ Sun May 19 21:07:29 2024 ] 	Batch(2000/2353) done. Loss: 0.0044  lr:0.000010
[ Sun May 19 21:08:06 2024 ] 	Batch(2100/2353) done. Loss: 0.0076  lr:0.000010
[ Sun May 19 21:08:42 2024 ] 	Batch(2200/2353) done. Loss: 0.0175  lr:0.000010
[ Sun May 19 21:09:19 2024 ] 	Batch(2300/2353) done. Loss: 0.0259  lr:0.000010
[ Sun May 19 21:09:38 2024 ] 	Mean training loss: 0.0334.
[ Sun May 19 21:09:38 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 21:09:38 2024 ] Training epoch: 50
[ Sun May 19 21:09:39 2024 ] 	Batch(0/2353) done. Loss: 0.0016  lr:0.000010
[ Sun May 19 21:10:15 2024 ] 	Batch(100/2353) done. Loss: 0.0144  lr:0.000010
[ Sun May 19 21:10:52 2024 ] 	Batch(200/2353) done. Loss: 0.0036  lr:0.000010
[ Sun May 19 21:11:28 2024 ] 	Batch(300/2353) done. Loss: 0.0499  lr:0.000010
[ Sun May 19 21:12:05 2024 ] 	Batch(400/2353) done. Loss: 0.1769  lr:0.000010
[ Sun May 19 21:12:41 2024 ] 	Batch(500/2353) done. Loss: 0.0142  lr:0.000010
[ Sun May 19 21:13:18 2024 ] 	Batch(600/2353) done. Loss: 0.0159  lr:0.000010
[ Sun May 19 21:13:55 2024 ] 	Batch(700/2353) done. Loss: 0.0221  lr:0.000010
[ Sun May 19 21:14:32 2024 ] 	Batch(800/2353) done. Loss: 0.0078  lr:0.000010
[ Sun May 19 21:15:08 2024 ] 	Batch(900/2353) done. Loss: 0.0172  lr:0.000010
[ Sun May 19 21:15:45 2024 ] 	Batch(1000/2353) done. Loss: 0.0266  lr:0.000010
[ Sun May 19 21:16:21 2024 ] 	Batch(1100/2353) done. Loss: 0.0170  lr:0.000010
[ Sun May 19 21:16:58 2024 ] 	Batch(1200/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 21:17:35 2024 ] 	Batch(1300/2353) done. Loss: 0.0122  lr:0.000010
[ Sun May 19 21:18:11 2024 ] 	Batch(1400/2353) done. Loss: 0.0184  lr:0.000010
[ Sun May 19 21:18:48 2024 ] 	Batch(1500/2353) done. Loss: 0.0047  lr:0.000010
[ Sun May 19 21:19:25 2024 ] 	Batch(1600/2353) done. Loss: 0.0083  lr:0.000010
[ Sun May 19 21:20:01 2024 ] 	Batch(1700/2353) done. Loss: 0.0094  lr:0.000010
[ Sun May 19 21:20:38 2024 ] 	Batch(1800/2353) done. Loss: 0.0049  lr:0.000010
[ Sun May 19 21:21:15 2024 ] 	Batch(1900/2353) done. Loss: 0.0178  lr:0.000010
[ Sun May 19 21:21:52 2024 ] 	Batch(2000/2353) done. Loss: 0.0239  lr:0.000010
[ Sun May 19 21:22:28 2024 ] 	Batch(2100/2353) done. Loss: 0.0265  lr:0.000010
[ Sun May 19 21:23:05 2024 ] 	Batch(2200/2353) done. Loss: 0.0046  lr:0.000010
[ Sun May 19 21:23:41 2024 ] 	Batch(2300/2353) done. Loss: 0.0193  lr:0.000010
[ Sun May 19 21:24:00 2024 ] 	Mean training loss: 0.0341.
[ Sun May 19 21:24:00 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 21:24:01 2024 ] Eval epoch: 50
[ Sun May 19 21:26:03 2024 ] 	Mean val loss of 2367 batches: 0.24273280180953397.
[ Sun May 19 21:26:03 2024 ] Training epoch: 51
[ Sun May 19 21:26:04 2024 ] 	Batch(0/2353) done. Loss: 0.0085  lr:0.000010
[ Sun May 19 21:26:41 2024 ] 	Batch(100/2353) done. Loss: 0.0063  lr:0.000010
[ Sun May 19 21:27:17 2024 ] 	Batch(200/2353) done. Loss: 0.0157  lr:0.000010
[ Sun May 19 21:27:54 2024 ] 	Batch(300/2353) done. Loss: 0.0085  lr:0.000010
[ Sun May 19 21:28:30 2024 ] 	Batch(400/2353) done. Loss: 0.0071  lr:0.000010
[ Sun May 19 21:29:07 2024 ] 	Batch(500/2353) done. Loss: 0.0159  lr:0.000010
[ Sun May 19 21:29:43 2024 ] 	Batch(600/2353) done. Loss: 0.0208  lr:0.000010
[ Sun May 19 21:30:20 2024 ] 	Batch(700/2353) done. Loss: 0.0446  lr:0.000010
[ Sun May 19 21:30:57 2024 ] 	Batch(800/2353) done. Loss: 0.0322  lr:0.000010
[ Sun May 19 21:31:33 2024 ] 	Batch(900/2353) done. Loss: 0.0205  lr:0.000010
[ Sun May 19 21:32:10 2024 ] 	Batch(1000/2353) done. Loss: 0.0512  lr:0.000010
[ Sun May 19 21:32:46 2024 ] 	Batch(1100/2353) done. Loss: 0.0645  lr:0.000010
[ Sun May 19 21:33:23 2024 ] 	Batch(1200/2353) done. Loss: 0.1342  lr:0.000010
[ Sun May 19 21:34:00 2024 ] 	Batch(1300/2353) done. Loss: 0.0141  lr:0.000010
[ Sun May 19 21:34:36 2024 ] 	Batch(1400/2353) done. Loss: 0.0148  lr:0.000010
[ Sun May 19 21:35:13 2024 ] 	Batch(1500/2353) done. Loss: 0.0151  lr:0.000010
[ Sun May 19 21:35:49 2024 ] 	Batch(1600/2353) done. Loss: 0.0021  lr:0.000010
[ Sun May 19 21:36:26 2024 ] 	Batch(1700/2353) done. Loss: 0.0599  lr:0.000010
[ Sun May 19 21:37:03 2024 ] 	Batch(1800/2353) done. Loss: 0.0148  lr:0.000010
[ Sun May 19 21:37:39 2024 ] 	Batch(1900/2353) done. Loss: 0.0679  lr:0.000010
[ Sun May 19 21:38:16 2024 ] 	Batch(2000/2353) done. Loss: 0.0089  lr:0.000010
[ Sun May 19 21:38:52 2024 ] 	Batch(2100/2353) done. Loss: 0.0153  lr:0.000010
[ Sun May 19 21:39:29 2024 ] 	Batch(2200/2353) done. Loss: 0.0655  lr:0.000010
[ Sun May 19 21:40:06 2024 ] 	Batch(2300/2353) done. Loss: 0.0077  lr:0.000010
[ Sun May 19 21:40:25 2024 ] 	Mean training loss: 0.0323.
[ Sun May 19 21:40:25 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 21:40:25 2024 ] Training epoch: 52
[ Sun May 19 21:40:25 2024 ] 	Batch(0/2353) done. Loss: 0.0153  lr:0.000010
[ Sun May 19 21:41:02 2024 ] 	Batch(100/2353) done. Loss: 0.0269  lr:0.000010
[ Sun May 19 21:41:39 2024 ] 	Batch(200/2353) done. Loss: 0.0054  lr:0.000010
[ Sun May 19 21:42:15 2024 ] 	Batch(300/2353) done. Loss: 0.0056  lr:0.000010
[ Sun May 19 21:42:52 2024 ] 	Batch(400/2353) done. Loss: 0.0048  lr:0.000010
[ Sun May 19 21:43:28 2024 ] 	Batch(500/2353) done. Loss: 0.0043  lr:0.000010
[ Sun May 19 21:44:05 2024 ] 	Batch(600/2353) done. Loss: 0.0200  lr:0.000010
[ Sun May 19 21:44:42 2024 ] 	Batch(700/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 21:45:18 2024 ] 	Batch(800/2353) done. Loss: 0.0261  lr:0.000010
[ Sun May 19 21:45:55 2024 ] 	Batch(900/2353) done. Loss: 0.0106  lr:0.000010
[ Sun May 19 21:46:31 2024 ] 	Batch(1000/2353) done. Loss: 0.0017  lr:0.000010
[ Sun May 19 21:47:08 2024 ] 	Batch(1100/2353) done. Loss: 0.0215  lr:0.000010
[ Sun May 19 21:47:44 2024 ] 	Batch(1200/2353) done. Loss: 0.0178  lr:0.000010
[ Sun May 19 21:48:21 2024 ] 	Batch(1300/2353) done. Loss: 0.0135  lr:0.000010
[ Sun May 19 21:48:58 2024 ] 	Batch(1400/2353) done. Loss: 0.0161  lr:0.000010
[ Sun May 19 21:49:34 2024 ] 	Batch(1500/2353) done. Loss: 0.0080  lr:0.000010
[ Sun May 19 21:50:11 2024 ] 	Batch(1600/2353) done. Loss: 0.0136  lr:0.000010
[ Sun May 19 21:50:47 2024 ] 	Batch(1700/2353) done. Loss: 0.0326  lr:0.000010
[ Sun May 19 21:51:24 2024 ] 	Batch(1800/2353) done. Loss: 0.0228  lr:0.000010
[ Sun May 19 21:52:01 2024 ] 	Batch(1900/2353) done. Loss: 0.0131  lr:0.000010
[ Sun May 19 21:52:37 2024 ] 	Batch(2000/2353) done. Loss: 0.0115  lr:0.000010
[ Sun May 19 21:53:14 2024 ] 	Batch(2100/2353) done. Loss: 0.0110  lr:0.000010
[ Sun May 19 21:53:50 2024 ] 	Batch(2200/2353) done. Loss: 0.0289  lr:0.000010
[ Sun May 19 21:54:27 2024 ] 	Batch(2300/2353) done. Loss: 0.0289  lr:0.000010
[ Sun May 19 21:54:46 2024 ] 	Mean training loss: 0.0316.
[ Sun May 19 21:54:46 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 21:54:46 2024 ] Training epoch: 53
[ Sun May 19 21:54:47 2024 ] 	Batch(0/2353) done. Loss: 0.1269  lr:0.000010
[ Sun May 19 21:55:23 2024 ] 	Batch(100/2353) done. Loss: 0.0060  lr:0.000010
[ Sun May 19 21:56:00 2024 ] 	Batch(200/2353) done. Loss: 0.0760  lr:0.000010
[ Sun May 19 21:56:37 2024 ] 	Batch(300/2353) done. Loss: 0.0257  lr:0.000010
[ Sun May 19 21:57:13 2024 ] 	Batch(400/2353) done. Loss: 0.0344  lr:0.000010
[ Sun May 19 21:57:50 2024 ] 	Batch(500/2353) done. Loss: 0.0256  lr:0.000010
[ Sun May 19 21:58:26 2024 ] 	Batch(600/2353) done. Loss: 0.0197  lr:0.000010
[ Sun May 19 21:59:03 2024 ] 	Batch(700/2353) done. Loss: 0.0056  lr:0.000010
[ Sun May 19 21:59:40 2024 ] 	Batch(800/2353) done. Loss: 0.0264  lr:0.000010
[ Sun May 19 22:00:16 2024 ] 	Batch(900/2353) done. Loss: 0.0087  lr:0.000010
[ Sun May 19 22:00:53 2024 ] 	Batch(1000/2353) done. Loss: 0.0154  lr:0.000010
[ Sun May 19 22:01:31 2024 ] 	Batch(1100/2353) done. Loss: 0.0207  lr:0.000010
[ Sun May 19 22:02:08 2024 ] 	Batch(1200/2353) done. Loss: 0.0185  lr:0.000010
[ Sun May 19 22:02:45 2024 ] 	Batch(1300/2353) done. Loss: 0.0419  lr:0.000010
[ Sun May 19 22:03:22 2024 ] 	Batch(1400/2353) done. Loss: 0.0266  lr:0.000010
[ Sun May 19 22:04:00 2024 ] 	Batch(1500/2353) done. Loss: 0.0053  lr:0.000010
[ Sun May 19 22:04:37 2024 ] 	Batch(1600/2353) done. Loss: 0.0334  lr:0.000010
[ Sun May 19 22:05:14 2024 ] 	Batch(1700/2353) done. Loss: 0.0072  lr:0.000010
[ Sun May 19 22:05:52 2024 ] 	Batch(1800/2353) done. Loss: 0.0394  lr:0.000010
[ Sun May 19 22:06:29 2024 ] 	Batch(1900/2353) done. Loss: 0.0477  lr:0.000010
[ Sun May 19 22:07:06 2024 ] 	Batch(2000/2353) done. Loss: 0.0345  lr:0.000010
[ Sun May 19 22:07:43 2024 ] 	Batch(2100/2353) done. Loss: 0.0747  lr:0.000010
[ Sun May 19 22:08:21 2024 ] 	Batch(2200/2353) done. Loss: 0.0139  lr:0.000010
[ Sun May 19 22:08:58 2024 ] 	Batch(2300/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 22:09:17 2024 ] 	Mean training loss: 0.0305.
[ Sun May 19 22:09:17 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 22:09:17 2024 ] Training epoch: 54
[ Sun May 19 22:09:18 2024 ] 	Batch(0/2353) done. Loss: 0.0159  lr:0.000010
[ Sun May 19 22:09:54 2024 ] 	Batch(100/2353) done. Loss: 0.0105  lr:0.000010
[ Sun May 19 22:10:31 2024 ] 	Batch(200/2353) done. Loss: 0.0807  lr:0.000010
[ Sun May 19 22:11:07 2024 ] 	Batch(300/2353) done. Loss: 0.0327  lr:0.000010
[ Sun May 19 22:11:44 2024 ] 	Batch(400/2353) done. Loss: 0.0123  lr:0.000010
[ Sun May 19 22:12:21 2024 ] 	Batch(500/2353) done. Loss: 0.0225  lr:0.000010
[ Sun May 19 22:12:57 2024 ] 	Batch(600/2353) done. Loss: 0.0157  lr:0.000010
[ Sun May 19 22:13:34 2024 ] 	Batch(700/2353) done. Loss: 0.0217  lr:0.000010
[ Sun May 19 22:14:10 2024 ] 	Batch(800/2353) done. Loss: 0.0093  lr:0.000010
[ Sun May 19 22:14:47 2024 ] 	Batch(900/2353) done. Loss: 0.0067  lr:0.000010
[ Sun May 19 22:15:24 2024 ] 	Batch(1000/2353) done. Loss: 0.0210  lr:0.000010
[ Sun May 19 22:16:00 2024 ] 	Batch(1100/2353) done. Loss: 0.0795  lr:0.000010
[ Sun May 19 22:16:37 2024 ] 	Batch(1200/2353) done. Loss: 0.0349  lr:0.000010
[ Sun May 19 22:17:13 2024 ] 	Batch(1300/2353) done. Loss: 0.0057  lr:0.000010
[ Sun May 19 22:17:50 2024 ] 	Batch(1400/2353) done. Loss: 0.0109  lr:0.000010
[ Sun May 19 22:18:27 2024 ] 	Batch(1500/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 22:19:03 2024 ] 	Batch(1600/2353) done. Loss: 0.0118  lr:0.000010
[ Sun May 19 22:19:40 2024 ] 	Batch(1700/2353) done. Loss: 0.0061  lr:0.000010
[ Sun May 19 22:20:16 2024 ] 	Batch(1800/2353) done. Loss: 0.0054  lr:0.000010
[ Sun May 19 22:20:53 2024 ] 	Batch(1900/2353) done. Loss: 0.0276  lr:0.000010
[ Sun May 19 22:21:30 2024 ] 	Batch(2000/2353) done. Loss: 0.0181  lr:0.000010
[ Sun May 19 22:22:06 2024 ] 	Batch(2100/2353) done. Loss: 0.0169  lr:0.000010
[ Sun May 19 22:22:43 2024 ] 	Batch(2200/2353) done. Loss: 0.0078  lr:0.000010
[ Sun May 19 22:23:19 2024 ] 	Batch(2300/2353) done. Loss: 0.0016  lr:0.000010
[ Sun May 19 22:23:39 2024 ] 	Mean training loss: 0.0294.
[ Sun May 19 22:23:39 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 22:23:39 2024 ] Training epoch: 55
[ Sun May 19 22:23:39 2024 ] 	Batch(0/2353) done. Loss: 0.0320  lr:0.000010
[ Sun May 19 22:24:16 2024 ] 	Batch(100/2353) done. Loss: 0.1018  lr:0.000010
[ Sun May 19 22:24:53 2024 ] 	Batch(200/2353) done. Loss: 0.0093  lr:0.000010
[ Sun May 19 22:25:30 2024 ] 	Batch(300/2353) done. Loss: 0.0119  lr:0.000010
[ Sun May 19 22:26:07 2024 ] 	Batch(400/2353) done. Loss: 0.0166  lr:0.000010
[ Sun May 19 22:26:43 2024 ] 	Batch(500/2353) done. Loss: 0.0226  lr:0.000010
[ Sun May 19 22:27:20 2024 ] 	Batch(600/2353) done. Loss: 0.0279  lr:0.000010
[ Sun May 19 22:27:57 2024 ] 	Batch(700/2353) done. Loss: 0.0185  lr:0.000010
[ Sun May 19 22:28:33 2024 ] 	Batch(800/2353) done. Loss: 0.0323  lr:0.000010
[ Sun May 19 22:29:10 2024 ] 	Batch(900/2353) done. Loss: 0.0057  lr:0.000010
[ Sun May 19 22:29:46 2024 ] 	Batch(1000/2353) done. Loss: 0.0600  lr:0.000010
[ Sun May 19 22:30:23 2024 ] 	Batch(1100/2353) done. Loss: 0.1020  lr:0.000010
[ Sun May 19 22:31:00 2024 ] 	Batch(1200/2353) done. Loss: 0.0327  lr:0.000010
[ Sun May 19 22:31:36 2024 ] 	Batch(1300/2353) done. Loss: 0.0071  lr:0.000010
[ Sun May 19 22:32:13 2024 ] 	Batch(1400/2353) done. Loss: 0.0133  lr:0.000010
[ Sun May 19 22:32:49 2024 ] 	Batch(1500/2353) done. Loss: 0.0157  lr:0.000010
[ Sun May 19 22:33:26 2024 ] 	Batch(1600/2353) done. Loss: 0.0474  lr:0.000010
[ Sun May 19 22:34:03 2024 ] 	Batch(1700/2353) done. Loss: 0.0532  lr:0.000010
[ Sun May 19 22:34:40 2024 ] 	Batch(1800/2353) done. Loss: 0.0245  lr:0.000010
[ Sun May 19 22:35:16 2024 ] 	Batch(1900/2353) done. Loss: 0.0062  lr:0.000010
[ Sun May 19 22:35:53 2024 ] 	Batch(2000/2353) done. Loss: 0.0644  lr:0.000010
[ Sun May 19 22:36:30 2024 ] 	Batch(2100/2353) done. Loss: 0.0166  lr:0.000010
[ Sun May 19 22:37:06 2024 ] 	Batch(2200/2353) done. Loss: 0.0112  lr:0.000010
[ Sun May 19 22:37:43 2024 ] 	Batch(2300/2353) done. Loss: 0.0067  lr:0.000010
[ Sun May 19 22:38:02 2024 ] 	Mean training loss: 0.0293.
[ Sun May 19 22:38:02 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 22:38:02 2024 ] Training epoch: 56
[ Sun May 19 22:38:03 2024 ] 	Batch(0/2353) done. Loss: 0.0096  lr:0.000010
[ Sun May 19 22:38:40 2024 ] 	Batch(100/2353) done. Loss: 0.0417  lr:0.000010
[ Sun May 19 22:39:16 2024 ] 	Batch(200/2353) done. Loss: 0.0107  lr:0.000010
[ Sun May 19 22:39:53 2024 ] 	Batch(300/2353) done. Loss: 0.0148  lr:0.000010
[ Sun May 19 22:40:30 2024 ] 	Batch(400/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 22:41:06 2024 ] 	Batch(500/2353) done. Loss: 0.0083  lr:0.000010
[ Sun May 19 22:41:43 2024 ] 	Batch(600/2353) done. Loss: 0.0221  lr:0.000010
[ Sun May 19 22:42:19 2024 ] 	Batch(700/2353) done. Loss: 0.0180  lr:0.000010
[ Sun May 19 22:42:56 2024 ] 	Batch(800/2353) done. Loss: 0.0188  lr:0.000010
[ Sun May 19 22:43:33 2024 ] 	Batch(900/2353) done. Loss: 0.0184  lr:0.000010
[ Sun May 19 22:44:09 2024 ] 	Batch(1000/2353) done. Loss: 0.0419  lr:0.000010
[ Sun May 19 22:44:46 2024 ] 	Batch(1100/2353) done. Loss: 0.0174  lr:0.000010
[ Sun May 19 22:45:23 2024 ] 	Batch(1200/2353) done. Loss: 0.0318  lr:0.000010
[ Sun May 19 22:46:00 2024 ] 	Batch(1300/2353) done. Loss: 0.0116  lr:0.000010
[ Sun May 19 22:46:36 2024 ] 	Batch(1400/2353) done. Loss: 0.0143  lr:0.000010
[ Sun May 19 22:47:13 2024 ] 	Batch(1500/2353) done. Loss: 0.0249  lr:0.000010
[ Sun May 19 22:47:49 2024 ] 	Batch(1600/2353) done. Loss: 0.0925  lr:0.000010
[ Sun May 19 22:48:26 2024 ] 	Batch(1700/2353) done. Loss: 0.0025  lr:0.000010
[ Sun May 19 22:49:03 2024 ] 	Batch(1800/2353) done. Loss: 0.0572  lr:0.000010
[ Sun May 19 22:49:39 2024 ] 	Batch(1900/2353) done. Loss: 0.0052  lr:0.000010
[ Sun May 19 22:50:16 2024 ] 	Batch(2000/2353) done. Loss: 0.0062  lr:0.000010
[ Sun May 19 22:50:53 2024 ] 	Batch(2100/2353) done. Loss: 0.0282  lr:0.000010
[ Sun May 19 22:51:29 2024 ] 	Batch(2200/2353) done. Loss: 0.0399  lr:0.000010
[ Sun May 19 22:52:06 2024 ] 	Batch(2300/2353) done. Loss: 0.0183  lr:0.000010
[ Sun May 19 22:52:25 2024 ] 	Mean training loss: 0.0287.
[ Sun May 19 22:52:25 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 22:52:25 2024 ] Training epoch: 57
[ Sun May 19 22:52:25 2024 ] 	Batch(0/2353) done. Loss: 0.0253  lr:0.000010
[ Sun May 19 22:53:02 2024 ] 	Batch(100/2353) done. Loss: 0.0579  lr:0.000010
[ Sun May 19 22:53:38 2024 ] 	Batch(200/2353) done. Loss: 0.0028  lr:0.000010
[ Sun May 19 22:54:15 2024 ] 	Batch(300/2353) done. Loss: 0.0608  lr:0.000010
[ Sun May 19 22:54:52 2024 ] 	Batch(400/2353) done. Loss: 0.0108  lr:0.000010
[ Sun May 19 22:55:28 2024 ] 	Batch(500/2353) done. Loss: 0.0513  lr:0.000010
[ Sun May 19 22:56:05 2024 ] 	Batch(600/2353) done. Loss: 0.0348  lr:0.000010
[ Sun May 19 22:56:41 2024 ] 	Batch(700/2353) done. Loss: 0.0128  lr:0.000010
[ Sun May 19 22:57:18 2024 ] 	Batch(800/2353) done. Loss: 0.0870  lr:0.000010
[ Sun May 19 22:57:54 2024 ] 	Batch(900/2353) done. Loss: 0.0290  lr:0.000010
[ Sun May 19 22:58:31 2024 ] 	Batch(1000/2353) done. Loss: 0.0103  lr:0.000010
[ Sun May 19 22:59:08 2024 ] 	Batch(1100/2353) done. Loss: 0.0132  lr:0.000010
[ Sun May 19 22:59:44 2024 ] 	Batch(1200/2353) done. Loss: 0.0101  lr:0.000010
[ Sun May 19 23:00:21 2024 ] 	Batch(1300/2353) done. Loss: 0.0217  lr:0.000010
[ Sun May 19 23:00:58 2024 ] 	Batch(1400/2353) done. Loss: 0.0068  lr:0.000010
[ Sun May 19 23:01:34 2024 ] 	Batch(1500/2353) done. Loss: 0.0066  lr:0.000010
[ Sun May 19 23:02:11 2024 ] 	Batch(1600/2353) done. Loss: 0.0133  lr:0.000010
[ Sun May 19 23:02:47 2024 ] 	Batch(1700/2353) done. Loss: 0.0138  lr:0.000010
[ Sun May 19 23:03:24 2024 ] 	Batch(1800/2353) done. Loss: 0.0135  lr:0.000010
[ Sun May 19 23:04:01 2024 ] 	Batch(1900/2353) done. Loss: 0.0257  lr:0.000010
[ Sun May 19 23:04:37 2024 ] 	Batch(2000/2353) done. Loss: 0.0359  lr:0.000010
[ Sun May 19 23:05:14 2024 ] 	Batch(2100/2353) done. Loss: 0.0591  lr:0.000010
[ Sun May 19 23:05:51 2024 ] 	Batch(2200/2353) done. Loss: 0.1034  lr:0.000010
[ Sun May 19 23:06:28 2024 ] 	Batch(2300/2353) done. Loss: 0.0131  lr:0.000010
[ Sun May 19 23:06:48 2024 ] 	Mean training loss: 0.0263.
[ Sun May 19 23:06:48 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 23:06:48 2024 ] Training epoch: 58
[ Sun May 19 23:06:48 2024 ] 	Batch(0/2353) done. Loss: 0.0052  lr:0.000010
[ Sun May 19 23:07:25 2024 ] 	Batch(100/2353) done. Loss: 0.0800  lr:0.000010
[ Sun May 19 23:08:02 2024 ] 	Batch(200/2353) done. Loss: 0.0100  lr:0.000010
[ Sun May 19 23:08:38 2024 ] 	Batch(300/2353) done. Loss: 0.0404  lr:0.000010
[ Sun May 19 23:09:15 2024 ] 	Batch(400/2353) done. Loss: 0.0024  lr:0.000010
[ Sun May 19 23:09:51 2024 ] 	Batch(500/2353) done. Loss: 0.0025  lr:0.000010
[ Sun May 19 23:10:28 2024 ] 	Batch(600/2353) done. Loss: 0.0245  lr:0.000010
[ Sun May 19 23:11:05 2024 ] 	Batch(700/2353) done. Loss: 0.0217  lr:0.000010
[ Sun May 19 23:11:41 2024 ] 	Batch(800/2353) done. Loss: 0.0063  lr:0.000010
[ Sun May 19 23:12:18 2024 ] 	Batch(900/2353) done. Loss: 0.0060  lr:0.000010
[ Sun May 19 23:12:54 2024 ] 	Batch(1000/2353) done. Loss: 0.0818  lr:0.000010
[ Sun May 19 23:13:31 2024 ] 	Batch(1100/2353) done. Loss: 0.0314  lr:0.000010
[ Sun May 19 23:14:08 2024 ] 	Batch(1200/2353) done. Loss: 0.0193  lr:0.000010
[ Sun May 19 23:14:44 2024 ] 	Batch(1300/2353) done. Loss: 0.0111  lr:0.000010
[ Sun May 19 23:15:21 2024 ] 	Batch(1400/2353) done. Loss: 0.0211  lr:0.000010
[ Sun May 19 23:15:57 2024 ] 	Batch(1500/2353) done. Loss: 0.0142  lr:0.000010
[ Sun May 19 23:16:34 2024 ] 	Batch(1600/2353) done. Loss: 0.0142  lr:0.000010
[ Sun May 19 23:17:11 2024 ] 	Batch(1700/2353) done. Loss: 0.0178  lr:0.000010
[ Sun May 19 23:17:48 2024 ] 	Batch(1800/2353) done. Loss: 0.0368  lr:0.000010
[ Sun May 19 23:18:24 2024 ] 	Batch(1900/2353) done. Loss: 0.0587  lr:0.000010
[ Sun May 19 23:19:01 2024 ] 	Batch(2000/2353) done. Loss: 0.0482  lr:0.000010
[ Sun May 19 23:19:37 2024 ] 	Batch(2100/2353) done. Loss: 0.0531  lr:0.000010
[ Sun May 19 23:20:14 2024 ] 	Batch(2200/2353) done. Loss: 0.0064  lr:0.000010
[ Sun May 19 23:20:51 2024 ] 	Batch(2300/2353) done. Loss: 0.0092  lr:0.000010
[ Sun May 19 23:21:11 2024 ] 	Mean training loss: 0.0230.
[ Sun May 19 23:21:11 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 23:21:11 2024 ] Training epoch: 59
[ Sun May 19 23:21:11 2024 ] 	Batch(0/2353) done. Loss: 0.0049  lr:0.000010
[ Sun May 19 23:21:48 2024 ] 	Batch(100/2353) done. Loss: 0.0012  lr:0.000010
[ Sun May 19 23:22:24 2024 ] 	Batch(200/2353) done. Loss: 0.0104  lr:0.000010
[ Sun May 19 23:23:01 2024 ] 	Batch(300/2353) done. Loss: 0.0032  lr:0.000010
[ Sun May 19 23:23:38 2024 ] 	Batch(400/2353) done. Loss: 0.0059  lr:0.000010
[ Sun May 19 23:24:14 2024 ] 	Batch(500/2353) done. Loss: 0.0040  lr:0.000010
[ Sun May 19 23:24:51 2024 ] 	Batch(600/2353) done. Loss: 0.0162  lr:0.000010
[ Sun May 19 23:25:27 2024 ] 	Batch(700/2353) done. Loss: 0.0236  lr:0.000010
[ Sun May 19 23:26:04 2024 ] 	Batch(800/2353) done. Loss: 0.0712  lr:0.000010
[ Sun May 19 23:26:40 2024 ] 	Batch(900/2353) done. Loss: 0.0285  lr:0.000010
[ Sun May 19 23:27:17 2024 ] 	Batch(1000/2353) done. Loss: 0.0092  lr:0.000010
[ Sun May 19 23:27:54 2024 ] 	Batch(1100/2353) done. Loss: 0.0056  lr:0.000010
[ Sun May 19 23:28:31 2024 ] 	Batch(1200/2353) done. Loss: 0.0038  lr:0.000010
[ Sun May 19 23:29:07 2024 ] 	Batch(1300/2353) done. Loss: 0.0048  lr:0.000010
[ Sun May 19 23:29:44 2024 ] 	Batch(1400/2353) done. Loss: 0.0178  lr:0.000010
[ Sun May 19 23:30:21 2024 ] 	Batch(1500/2353) done. Loss: 0.0441  lr:0.000010
[ Sun May 19 23:30:58 2024 ] 	Batch(1600/2353) done. Loss: 0.0184  lr:0.000010
[ Sun May 19 23:31:35 2024 ] 	Batch(1700/2353) done. Loss: 0.0100  lr:0.000010
[ Sun May 19 23:32:11 2024 ] 	Batch(1800/2353) done. Loss: 0.0057  lr:0.000010
[ Sun May 19 23:32:48 2024 ] 	Batch(1900/2353) done. Loss: 0.0183  lr:0.000010
[ Sun May 19 23:33:25 2024 ] 	Batch(2000/2353) done. Loss: 0.0021  lr:0.000010
[ Sun May 19 23:34:02 2024 ] 	Batch(2100/2353) done. Loss: 0.0130  lr:0.000010
[ Sun May 19 23:34:38 2024 ] 	Batch(2200/2353) done. Loss: 0.0099  lr:0.000010
[ Sun May 19 23:35:15 2024 ] 	Batch(2300/2353) done. Loss: 0.0046  lr:0.000010
[ Sun May 19 23:35:34 2024 ] 	Mean training loss: 0.0217.
[ Sun May 19 23:35:34 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Sun May 19 23:35:34 2024 ] Training epoch: 60
[ Sun May 19 23:35:35 2024 ] 	Batch(0/2353) done. Loss: 0.0186  lr:0.000010
[ Sun May 19 23:36:11 2024 ] 	Batch(100/2353) done. Loss: 0.0068  lr:0.000010
[ Sun May 19 23:36:48 2024 ] 	Batch(200/2353) done. Loss: 0.0028  lr:0.000010
[ Sun May 19 23:37:25 2024 ] 	Batch(300/2353) done. Loss: 0.0315  lr:0.000010
[ Sun May 19 23:38:01 2024 ] 	Batch(400/2353) done. Loss: 0.0508  lr:0.000010
[ Sun May 19 23:38:38 2024 ] 	Batch(500/2353) done. Loss: 0.0322  lr:0.000010
[ Sun May 19 23:39:14 2024 ] 	Batch(600/2353) done. Loss: 0.0338  lr:0.000010
[ Sun May 19 23:39:51 2024 ] 	Batch(700/2353) done. Loss: 0.0119  lr:0.000010
[ Sun May 19 23:40:28 2024 ] 	Batch(800/2353) done. Loss: 0.0087  lr:0.000010
[ Sun May 19 23:41:04 2024 ] 	Batch(900/2353) done. Loss: 0.0114  lr:0.000010
[ Sun May 19 23:41:41 2024 ] 	Batch(1000/2353) done. Loss: 0.0131  lr:0.000010
[ Sun May 19 23:42:17 2024 ] 	Batch(1100/2353) done. Loss: 0.0029  lr:0.000010
[ Sun May 19 23:42:54 2024 ] 	Batch(1200/2353) done. Loss: 0.1357  lr:0.000010
[ Sun May 19 23:43:32 2024 ] 	Batch(1300/2353) done. Loss: 0.0168  lr:0.000010
[ Sun May 19 23:44:09 2024 ] 	Batch(1400/2353) done. Loss: 0.0531  lr:0.000010
[ Sun May 19 23:44:46 2024 ] 	Batch(1500/2353) done. Loss: 0.0237  lr:0.000010
[ Sun May 19 23:45:23 2024 ] 	Batch(1600/2353) done. Loss: 0.0098  lr:0.000010
[ Sun May 19 23:46:00 2024 ] 	Batch(1700/2353) done. Loss: 0.0106  lr:0.000010
[ Sun May 19 23:46:36 2024 ] 	Batch(1800/2353) done. Loss: 0.0025  lr:0.000010
[ Sun May 19 23:47:13 2024 ] 	Batch(1900/2353) done. Loss: 0.0121  lr:0.000010
[ Sun May 19 23:47:50 2024 ] 	Batch(2000/2353) done. Loss: 0.0067  lr:0.000010
[ Sun May 19 23:48:26 2024 ] 	Batch(2100/2353) done. Loss: 0.0143  lr:0.000010
[ Sun May 19 23:49:03 2024 ] 	Batch(2200/2353) done. Loss: 0.0040  lr:0.000010
[ Sun May 19 23:49:40 2024 ] 	Batch(2300/2353) done. Loss: 0.0068  lr:0.000010
[ Sun May 19 23:49:59 2024 ] 	Mean training loss: 0.0168.
[ Sun May 19 23:49:59 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Sun May 19 23:49:59 2024 ] Eval epoch: 60
[ Sun May 19 23:52:02 2024 ] 	Mean val loss of 2367 batches: 0.24531329347290726.
[ Sun May 19 23:52:02 2024 ] Training epoch: 61
[ Sun May 19 23:52:03 2024 ] 	Batch(0/2353) done. Loss: 0.0031  lr:0.000000
[ Sun May 19 23:52:39 2024 ] 	Batch(100/2353) done. Loss: 0.0044  lr:0.000000
[ Sun May 19 23:53:16 2024 ] 	Batch(200/2353) done. Loss: 0.0109  lr:0.000000
[ Sun May 19 23:53:52 2024 ] 	Batch(300/2353) done. Loss: 0.0114  lr:0.000000
[ Sun May 19 23:54:29 2024 ] 	Batch(400/2353) done. Loss: 0.0984  lr:0.000000
[ Sun May 19 23:55:05 2024 ] 	Batch(500/2353) done. Loss: 0.0164  lr:0.000000
[ Sun May 19 23:55:42 2024 ] 	Batch(600/2353) done. Loss: 0.0091  lr:0.000000
[ Sun May 19 23:56:18 2024 ] 	Batch(700/2353) done. Loss: 0.0207  lr:0.000000
[ Sun May 19 23:56:55 2024 ] 	Batch(800/2353) done. Loss: 0.0141  lr:0.000000
[ Sun May 19 23:57:32 2024 ] 	Batch(900/2353) done. Loss: 0.0095  lr:0.000000
[ Sun May 19 23:58:08 2024 ] 	Batch(1000/2353) done. Loss: 0.0072  lr:0.000000
[ Sun May 19 23:58:45 2024 ] 	Batch(1100/2353) done. Loss: 0.0716  lr:0.000000
[ Sun May 19 23:59:21 2024 ] 	Batch(1200/2353) done. Loss: 0.0199  lr:0.000000
[ Sun May 19 23:59:58 2024 ] 	Batch(1300/2353) done. Loss: 0.0054  lr:0.000000
[ Mon May 20 00:00:35 2024 ] 	Batch(1400/2353) done. Loss: 0.0499  lr:0.000000
[ Mon May 20 00:01:11 2024 ] 	Batch(1500/2353) done. Loss: 0.0138  lr:0.000000
[ Mon May 20 00:01:48 2024 ] 	Batch(1600/2353) done. Loss: 0.0629  lr:0.000000
[ Mon May 20 00:02:24 2024 ] 	Batch(1700/2353) done. Loss: 0.0136  lr:0.000000
[ Mon May 20 00:03:01 2024 ] 	Batch(1800/2353) done. Loss: 0.0007  lr:0.000000
[ Mon May 20 00:03:37 2024 ] 	Batch(1900/2353) done. Loss: 0.0511  lr:0.000000
[ Mon May 20 00:04:14 2024 ] 	Batch(2000/2353) done. Loss: 0.0435  lr:0.000000
[ Mon May 20 00:04:51 2024 ] 	Batch(2100/2353) done. Loss: 0.0146  lr:0.000000
[ Mon May 20 00:05:27 2024 ] 	Batch(2200/2353) done. Loss: 0.1056  lr:0.000000
[ Mon May 20 00:06:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0059  lr:0.000000
[ Mon May 20 00:06:23 2024 ] 	Mean training loss: 0.0370.
[ Mon May 20 00:06:23 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 00:06:23 2024 ] Training epoch: 62
[ Mon May 20 00:06:24 2024 ] 	Batch(0/2353) done. Loss: 0.0146  lr:0.000000
[ Mon May 20 00:07:00 2024 ] 	Batch(100/2353) done. Loss: 0.0679  lr:0.000000
[ Mon May 20 00:07:37 2024 ] 	Batch(200/2353) done. Loss: 0.0042  lr:0.000000
[ Mon May 20 00:08:14 2024 ] 	Batch(300/2353) done. Loss: 0.0137  lr:0.000000
[ Mon May 20 00:08:50 2024 ] 	Batch(400/2353) done. Loss: 0.0241  lr:0.000000
[ Mon May 20 00:09:27 2024 ] 	Batch(500/2353) done. Loss: 0.0184  lr:0.000000
[ Mon May 20 00:10:03 2024 ] 	Batch(600/2353) done. Loss: 0.0093  lr:0.000000
[ Mon May 20 00:10:40 2024 ] 	Batch(700/2353) done. Loss: 0.0069  lr:0.000000
[ Mon May 20 00:11:17 2024 ] 	Batch(800/2353) done. Loss: 0.0078  lr:0.000000
[ Mon May 20 00:11:53 2024 ] 	Batch(900/2353) done. Loss: 0.0193  lr:0.000000
[ Mon May 20 00:12:30 2024 ] 	Batch(1000/2353) done. Loss: 0.0019  lr:0.000000
[ Mon May 20 00:13:06 2024 ] 	Batch(1100/2353) done. Loss: 0.0532  lr:0.000000
[ Mon May 20 00:13:43 2024 ] 	Batch(1200/2353) done. Loss: 0.0168  lr:0.000000
[ Mon May 20 00:14:20 2024 ] 	Batch(1300/2353) done. Loss: 0.0191  lr:0.000000
[ Mon May 20 00:14:56 2024 ] 	Batch(1400/2353) done. Loss: 0.0700  lr:0.000000
[ Mon May 20 00:15:33 2024 ] 	Batch(1500/2353) done. Loss: 0.0189  lr:0.000000
[ Mon May 20 00:16:09 2024 ] 	Batch(1600/2353) done. Loss: 0.0710  lr:0.000000
[ Mon May 20 00:16:46 2024 ] 	Batch(1700/2353) done. Loss: 0.0364  lr:0.000000
[ Mon May 20 00:17:23 2024 ] 	Batch(1800/2353) done. Loss: 0.0116  lr:0.000000
[ Mon May 20 00:17:59 2024 ] 	Batch(1900/2353) done. Loss: 0.0084  lr:0.000000
[ Mon May 20 00:18:36 2024 ] 	Batch(2000/2353) done. Loss: 0.0035  lr:0.000000
[ Mon May 20 00:19:12 2024 ] 	Batch(2100/2353) done. Loss: 0.0227  lr:0.000000
[ Mon May 20 00:19:49 2024 ] 	Batch(2200/2353) done. Loss: 0.0369  lr:0.000000
[ Mon May 20 00:20:26 2024 ] 	Batch(2300/2353) done. Loss: 0.0100  lr:0.000000
[ Mon May 20 00:20:45 2024 ] 	Mean training loss: 0.0370.
[ Mon May 20 00:20:45 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 00:20:45 2024 ] Training epoch: 63
[ Mon May 20 00:20:46 2024 ] 	Batch(0/2353) done. Loss: 0.0260  lr:0.000000
[ Mon May 20 00:21:23 2024 ] 	Batch(100/2353) done. Loss: 0.0410  lr:0.000000
[ Mon May 20 00:21:59 2024 ] 	Batch(200/2353) done. Loss: 0.0054  lr:0.000000
[ Mon May 20 00:22:36 2024 ] 	Batch(300/2353) done. Loss: 0.0443  lr:0.000000
[ Mon May 20 00:23:13 2024 ] 	Batch(400/2353) done. Loss: 0.0251  lr:0.000000
[ Mon May 20 00:23:49 2024 ] 	Batch(500/2353) done. Loss: 0.0491  lr:0.000000
[ Mon May 20 00:24:26 2024 ] 	Batch(600/2353) done. Loss: 0.0109  lr:0.000000
[ Mon May 20 00:25:02 2024 ] 	Batch(700/2353) done. Loss: 0.0310  lr:0.000000
[ Mon May 20 00:25:39 2024 ] 	Batch(800/2353) done. Loss: 0.0032  lr:0.000000
[ Mon May 20 00:26:15 2024 ] 	Batch(900/2353) done. Loss: 0.0125  lr:0.000000
[ Mon May 20 00:26:52 2024 ] 	Batch(1000/2353) done. Loss: 0.0078  lr:0.000000
[ Mon May 20 00:27:28 2024 ] 	Batch(1100/2353) done. Loss: 0.1599  lr:0.000000
[ Mon May 20 00:28:05 2024 ] 	Batch(1200/2353) done. Loss: 0.0073  lr:0.000000
[ Mon May 20 00:28:42 2024 ] 	Batch(1300/2353) done. Loss: 0.0214  lr:0.000000
[ Mon May 20 00:29:18 2024 ] 	Batch(1400/2353) done. Loss: 0.0422  lr:0.000000
[ Mon May 20 00:29:55 2024 ] 	Batch(1500/2353) done. Loss: 0.0114  lr:0.000000
[ Mon May 20 00:30:32 2024 ] 	Batch(1600/2353) done. Loss: 0.0097  lr:0.000000
[ Mon May 20 00:31:08 2024 ] 	Batch(1700/2353) done. Loss: 0.2316  lr:0.000000
[ Mon May 20 00:31:45 2024 ] 	Batch(1800/2353) done. Loss: 0.0077  lr:0.000000
[ Mon May 20 00:32:21 2024 ] 	Batch(1900/2353) done. Loss: 0.0188  lr:0.000000
[ Mon May 20 00:32:58 2024 ] 	Batch(2000/2353) done. Loss: 0.0481  lr:0.000000
[ Mon May 20 00:33:35 2024 ] 	Batch(2100/2353) done. Loss: 0.0614  lr:0.000000
[ Mon May 20 00:34:11 2024 ] 	Batch(2200/2353) done. Loss: 0.0018  lr:0.000000
[ Mon May 20 00:34:48 2024 ] 	Batch(2300/2353) done. Loss: 0.0076  lr:0.000000
[ Mon May 20 00:35:07 2024 ] 	Mean training loss: 0.0383.
[ Mon May 20 00:35:07 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 00:35:07 2024 ] Training epoch: 64
[ Mon May 20 00:35:08 2024 ] 	Batch(0/2353) done. Loss: 0.0257  lr:0.000000
[ Mon May 20 00:35:44 2024 ] 	Batch(100/2353) done. Loss: 0.0118  lr:0.000000
[ Mon May 20 00:36:21 2024 ] 	Batch(200/2353) done. Loss: 0.0151  lr:0.000000
[ Mon May 20 00:36:57 2024 ] 	Batch(300/2353) done. Loss: 0.0747  lr:0.000000
[ Mon May 20 00:37:34 2024 ] 	Batch(400/2353) done. Loss: 0.0241  lr:0.000000
[ Mon May 20 00:38:11 2024 ] 	Batch(500/2353) done. Loss: 0.0427  lr:0.000000
[ Mon May 20 00:38:47 2024 ] 	Batch(600/2353) done. Loss: 0.0403  lr:0.000000
[ Mon May 20 00:39:24 2024 ] 	Batch(700/2353) done. Loss: 0.0149  lr:0.000000
[ Mon May 20 00:40:01 2024 ] 	Batch(800/2353) done. Loss: 0.0123  lr:0.000000
[ Mon May 20 00:40:38 2024 ] 	Batch(900/2353) done. Loss: 0.0463  lr:0.000000
[ Mon May 20 00:41:14 2024 ] 	Batch(1000/2353) done. Loss: 0.0113  lr:0.000000
[ Mon May 20 00:41:51 2024 ] 	Batch(1100/2353) done. Loss: 0.0578  lr:0.000000
[ Mon May 20 00:42:28 2024 ] 	Batch(1200/2353) done. Loss: 0.0102  lr:0.000000
[ Mon May 20 00:43:04 2024 ] 	Batch(1300/2353) done. Loss: 0.0892  lr:0.000000
[ Mon May 20 00:43:41 2024 ] 	Batch(1400/2353) done. Loss: 0.0112  lr:0.000000
[ Mon May 20 00:44:18 2024 ] 	Batch(1500/2353) done. Loss: 0.0201  lr:0.000000
[ Mon May 20 00:44:54 2024 ] 	Batch(1600/2353) done. Loss: 0.0142  lr:0.000000
[ Mon May 20 00:45:31 2024 ] 	Batch(1700/2353) done. Loss: 0.0115  lr:0.000000
[ Mon May 20 00:46:07 2024 ] 	Batch(1800/2353) done. Loss: 0.0310  lr:0.000000
[ Mon May 20 00:46:44 2024 ] 	Batch(1900/2353) done. Loss: 0.0181  lr:0.000000
[ Mon May 20 00:47:21 2024 ] 	Batch(2000/2353) done. Loss: 0.1204  lr:0.000000
[ Mon May 20 00:47:58 2024 ] 	Batch(2100/2353) done. Loss: 0.1281  lr:0.000000
[ Mon May 20 00:48:35 2024 ] 	Batch(2200/2353) done. Loss: 0.1003  lr:0.000000
[ Mon May 20 00:49:11 2024 ] 	Batch(2300/2353) done. Loss: 0.0049  lr:0.000000
[ Mon May 20 00:49:30 2024 ] 	Mean training loss: 0.0364.
[ Mon May 20 00:49:30 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Mon May 20 00:49:30 2024 ] Training epoch: 65
[ Mon May 20 00:49:31 2024 ] 	Batch(0/2353) done. Loss: 0.1304  lr:0.000000
[ Mon May 20 00:50:08 2024 ] 	Batch(100/2353) done. Loss: 0.0157  lr:0.000000
[ Mon May 20 00:50:44 2024 ] 	Batch(200/2353) done. Loss: 0.0097  lr:0.000000
[ Mon May 20 00:51:21 2024 ] 	Batch(300/2353) done. Loss: 0.0215  lr:0.000000
[ Mon May 20 00:51:58 2024 ] 	Batch(400/2353) done. Loss: 0.0610  lr:0.000000
[ Mon May 20 00:52:34 2024 ] 	Batch(500/2353) done. Loss: 0.0009  lr:0.000000
[ Mon May 20 00:53:11 2024 ] 	Batch(600/2353) done. Loss: 0.0639  lr:0.000000
[ Mon May 20 00:53:47 2024 ] 	Batch(700/2353) done. Loss: 0.0747  lr:0.000000
[ Mon May 20 00:54:24 2024 ] 	Batch(800/2353) done. Loss: 0.0735  lr:0.000000
[ Mon May 20 00:55:01 2024 ] 	Batch(900/2353) done. Loss: 0.0357  lr:0.000000
[ Mon May 20 00:55:37 2024 ] 	Batch(1000/2353) done. Loss: 0.0165  lr:0.000000
[ Mon May 20 00:56:14 2024 ] 	Batch(1100/2353) done. Loss: 0.0046  lr:0.000000
[ Mon May 20 00:56:51 2024 ] 	Batch(1200/2353) done. Loss: 0.1170  lr:0.000000
[ Mon May 20 00:57:27 2024 ] 	Batch(1300/2353) done. Loss: 0.0082  lr:0.000000
[ Mon May 20 00:58:04 2024 ] 	Batch(1400/2353) done. Loss: 0.0078  lr:0.000000
[ Mon May 20 00:58:41 2024 ] 	Batch(1500/2353) done. Loss: 0.0118  lr:0.000000
[ Mon May 20 00:59:18 2024 ] 	Batch(1600/2353) done. Loss: 0.0391  lr:0.000000
[ Mon May 20 00:59:54 2024 ] 	Batch(1700/2353) done. Loss: 0.0035  lr:0.000000
[ Mon May 20 01:00:31 2024 ] 	Batch(1800/2353) done. Loss: 0.0525  lr:0.000000
[ Mon May 20 01:01:08 2024 ] 	Batch(1900/2353) done. Loss: 0.0338  lr:0.000000
[ Mon May 20 01:01:44 2024 ] 	Batch(2000/2353) done. Loss: 0.0221  lr:0.000000
[ Mon May 20 01:02:21 2024 ] 	Batch(2100/2353) done. Loss: 0.0174  lr:0.000000
[ Mon May 20 01:02:57 2024 ] 	Batch(2200/2353) done. Loss: 0.0316  lr:0.000000
[ Mon May 20 01:03:34 2024 ] 	Batch(2300/2353) done. Loss: 0.0266  lr:0.000000
[ Mon May 20 01:03:53 2024 ] 	Mean training loss: 0.0391.
[ Mon May 20 01:03:53 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Mon May 20 01:03:53 2024 ] Training epoch: 66
[ Mon May 20 01:03:54 2024 ] 	Batch(0/2353) done. Loss: 0.0229  lr:0.000000
[ Mon May 20 01:04:30 2024 ] 	Batch(100/2353) done. Loss: 0.0224  lr:0.000000
[ Mon May 20 01:05:07 2024 ] 	Batch(200/2353) done. Loss: 0.0319  lr:0.000000
[ Mon May 20 01:05:44 2024 ] 	Batch(300/2353) done. Loss: 0.0420  lr:0.000000
[ Mon May 20 01:06:20 2024 ] 	Batch(400/2353) done. Loss: 0.0082  lr:0.000000
[ Mon May 20 01:06:57 2024 ] 	Batch(500/2353) done. Loss: 0.0094  lr:0.000000
[ Mon May 20 01:07:33 2024 ] 	Batch(600/2353) done. Loss: 0.0235  lr:0.000000
[ Mon May 20 01:08:10 2024 ] 	Batch(700/2353) done. Loss: 0.0022  lr:0.000000
[ Mon May 20 01:08:47 2024 ] 	Batch(800/2353) done. Loss: 0.0907  lr:0.000000
[ Mon May 20 01:09:23 2024 ] 	Batch(900/2353) done. Loss: 0.0283  lr:0.000000
[ Mon May 20 01:10:00 2024 ] 	Batch(1000/2353) done. Loss: 0.0942  lr:0.000000
[ Mon May 20 01:10:36 2024 ] 	Batch(1100/2353) done. Loss: 0.0629  lr:0.000000
[ Mon May 20 01:11:13 2024 ] 	Batch(1200/2353) done. Loss: 0.0126  lr:0.000000
[ Mon May 20 01:11:50 2024 ] 	Batch(1300/2353) done. Loss: 0.0086  lr:0.000000
[ Mon May 20 01:12:27 2024 ] 	Batch(1400/2353) done. Loss: 0.0494  lr:0.000000
[ Mon May 20 01:13:04 2024 ] 	Batch(1500/2353) done. Loss: 0.0095  lr:0.000000
[ Mon May 20 01:13:41 2024 ] 	Batch(1600/2353) done. Loss: 0.1815  lr:0.000000
[ Mon May 20 01:14:17 2024 ] 	Batch(1700/2353) done. Loss: 0.0186  lr:0.000000
[ Mon May 20 01:14:54 2024 ] 	Batch(1800/2353) done. Loss: 0.0131  lr:0.000000
[ Mon May 20 01:15:31 2024 ] 	Batch(1900/2353) done. Loss: 0.0294  lr:0.000000
[ Mon May 20 01:16:07 2024 ] 	Batch(2000/2353) done. Loss: 0.0232  lr:0.000000
[ Mon May 20 01:16:44 2024 ] 	Batch(2100/2353) done. Loss: 0.0295  lr:0.000000
[ Mon May 20 01:17:21 2024 ] 	Batch(2200/2353) done. Loss: 0.0843  lr:0.000000
[ Mon May 20 01:17:58 2024 ] 	Batch(2300/2353) done. Loss: 0.0139  lr:0.000000
[ Mon May 20 01:18:17 2024 ] 	Mean training loss: 0.0367.
[ Mon May 20 01:18:17 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 01:18:17 2024 ] Training epoch: 67
[ Mon May 20 01:18:18 2024 ] 	Batch(0/2353) done. Loss: 0.0083  lr:0.000000
[ Mon May 20 01:18:54 2024 ] 	Batch(100/2353) done. Loss: 0.0032  lr:0.000000
[ Mon May 20 01:19:31 2024 ] 	Batch(200/2353) done. Loss: 0.0082  lr:0.000000
[ Mon May 20 01:20:08 2024 ] 	Batch(300/2353) done. Loss: 0.0044  lr:0.000000
[ Mon May 20 01:20:45 2024 ] 	Batch(400/2353) done. Loss: 0.0299  lr:0.000000
[ Mon May 20 01:21:21 2024 ] 	Batch(500/2353) done. Loss: 0.0340  lr:0.000000
[ Mon May 20 01:21:58 2024 ] 	Batch(600/2353) done. Loss: 0.1106  lr:0.000000
[ Mon May 20 01:22:35 2024 ] 	Batch(700/2353) done. Loss: 0.0313  lr:0.000000
[ Mon May 20 01:23:12 2024 ] 	Batch(800/2353) done. Loss: 0.0150  lr:0.000000
[ Mon May 20 01:23:48 2024 ] 	Batch(900/2353) done. Loss: 0.0067  lr:0.000000
[ Mon May 20 01:24:25 2024 ] 	Batch(1000/2353) done. Loss: 0.0969  lr:0.000000
[ Mon May 20 01:25:02 2024 ] 	Batch(1100/2353) done. Loss: 0.0299  lr:0.000000
[ Mon May 20 01:25:39 2024 ] 	Batch(1200/2353) done. Loss: 0.0030  lr:0.000000
[ Mon May 20 01:26:15 2024 ] 	Batch(1300/2353) done. Loss: 0.0143  lr:0.000000
[ Mon May 20 01:26:52 2024 ] 	Batch(1400/2353) done. Loss: 0.0226  lr:0.000000
[ Mon May 20 01:27:29 2024 ] 	Batch(1500/2353) done. Loss: 0.0314  lr:0.000000
[ Mon May 20 01:28:06 2024 ] 	Batch(1600/2353) done. Loss: 0.0410  lr:0.000000
[ Mon May 20 01:28:43 2024 ] 	Batch(1700/2353) done. Loss: 0.0357  lr:0.000000
[ Mon May 20 01:29:19 2024 ] 	Batch(1800/2353) done. Loss: 0.0142  lr:0.000000
[ Mon May 20 01:29:56 2024 ] 	Batch(1900/2353) done. Loss: 0.0281  lr:0.000000
[ Mon May 20 01:30:33 2024 ] 	Batch(2000/2353) done. Loss: 0.0115  lr:0.000000
[ Mon May 20 01:31:10 2024 ] 	Batch(2100/2353) done. Loss: 0.0318  lr:0.000000
[ Mon May 20 01:31:47 2024 ] 	Batch(2200/2353) done. Loss: 0.0407  lr:0.000000
[ Mon May 20 01:32:23 2024 ] 	Batch(2300/2353) done. Loss: 0.0153  lr:0.000000
[ Mon May 20 01:32:42 2024 ] 	Mean training loss: 0.0375.
[ Mon May 20 01:32:42 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 01:32:43 2024 ] Training epoch: 68
[ Mon May 20 01:32:43 2024 ] 	Batch(0/2353) done. Loss: 0.0085  lr:0.000000
[ Mon May 20 01:33:20 2024 ] 	Batch(100/2353) done. Loss: 0.0349  lr:0.000000
[ Mon May 20 01:33:57 2024 ] 	Batch(200/2353) done. Loss: 0.1784  lr:0.000000
[ Mon May 20 01:34:33 2024 ] 	Batch(300/2353) done. Loss: 0.0180  lr:0.000000
[ Mon May 20 01:35:10 2024 ] 	Batch(400/2353) done. Loss: 0.0547  lr:0.000000
[ Mon May 20 01:35:47 2024 ] 	Batch(500/2353) done. Loss: 0.0105  lr:0.000000
[ Mon May 20 01:36:24 2024 ] 	Batch(600/2353) done. Loss: 0.0115  lr:0.000000
[ Mon May 20 01:37:00 2024 ] 	Batch(700/2353) done. Loss: 0.0186  lr:0.000000
[ Mon May 20 01:37:37 2024 ] 	Batch(800/2353) done. Loss: 0.0054  lr:0.000000
[ Mon May 20 01:38:14 2024 ] 	Batch(900/2353) done. Loss: 0.0469  lr:0.000000
[ Mon May 20 01:38:51 2024 ] 	Batch(1000/2353) done. Loss: 0.0081  lr:0.000000
[ Mon May 20 01:39:27 2024 ] 	Batch(1100/2353) done. Loss: 0.0129  lr:0.000000
[ Mon May 20 01:40:04 2024 ] 	Batch(1200/2353) done. Loss: 0.0871  lr:0.000000
[ Mon May 20 01:40:41 2024 ] 	Batch(1300/2353) done. Loss: 0.0379  lr:0.000000
[ Mon May 20 01:41:18 2024 ] 	Batch(1400/2353) done. Loss: 0.0157  lr:0.000000
[ Mon May 20 01:41:54 2024 ] 	Batch(1500/2353) done. Loss: 0.1453  lr:0.000000
[ Mon May 20 01:42:31 2024 ] 	Batch(1600/2353) done. Loss: 0.0161  lr:0.000000
[ Mon May 20 01:43:08 2024 ] 	Batch(1700/2353) done. Loss: 0.0159  lr:0.000000
[ Mon May 20 01:43:45 2024 ] 	Batch(1800/2353) done. Loss: 0.0577  lr:0.000000
[ Mon May 20 01:44:22 2024 ] 	Batch(1900/2353) done. Loss: 0.0298  lr:0.000000
[ Mon May 20 01:44:58 2024 ] 	Batch(2000/2353) done. Loss: 0.0322  lr:0.000000
[ Mon May 20 01:45:35 2024 ] 	Batch(2100/2353) done. Loss: 0.0471  lr:0.000000
[ Mon May 20 01:46:12 2024 ] 	Batch(2200/2353) done. Loss: 0.0079  lr:0.000000
[ Mon May 20 01:46:49 2024 ] 	Batch(2300/2353) done. Loss: 0.0654  lr:0.000000
[ Mon May 20 01:47:08 2024 ] 	Mean training loss: 0.0385.
[ Mon May 20 01:47:08 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 01:47:08 2024 ] Training epoch: 69
[ Mon May 20 01:47:09 2024 ] 	Batch(0/2353) done. Loss: 0.0182  lr:0.000000
[ Mon May 20 01:47:45 2024 ] 	Batch(100/2353) done. Loss: 0.0053  lr:0.000000
[ Mon May 20 01:48:22 2024 ] 	Batch(200/2353) done. Loss: 0.0158  lr:0.000000
[ Mon May 20 01:48:59 2024 ] 	Batch(300/2353) done. Loss: 0.0315  lr:0.000000
[ Mon May 20 01:49:36 2024 ] 	Batch(400/2353) done. Loss: 0.0039  lr:0.000000
[ Mon May 20 01:50:13 2024 ] 	Batch(500/2353) done. Loss: 0.0449  lr:0.000000
[ Mon May 20 01:50:49 2024 ] 	Batch(600/2353) done. Loss: 0.0306  lr:0.000000
[ Mon May 20 01:51:26 2024 ] 	Batch(700/2353) done. Loss: 0.0077  lr:0.000000
[ Mon May 20 01:52:03 2024 ] 	Batch(800/2353) done. Loss: 0.1176  lr:0.000000
[ Mon May 20 01:52:40 2024 ] 	Batch(900/2353) done. Loss: 0.0110  lr:0.000000
[ Mon May 20 01:53:16 2024 ] 	Batch(1000/2353) done. Loss: 0.0464  lr:0.000000
[ Mon May 20 01:53:53 2024 ] 	Batch(1100/2353) done. Loss: 0.0070  lr:0.000000
[ Mon May 20 01:54:30 2024 ] 	Batch(1200/2353) done. Loss: 0.0251  lr:0.000000
[ Mon May 20 01:55:06 2024 ] 	Batch(1300/2353) done. Loss: 0.0858  lr:0.000000
[ Mon May 20 01:55:43 2024 ] 	Batch(1400/2353) done. Loss: 0.0032  lr:0.000000
[ Mon May 20 01:56:19 2024 ] 	Batch(1500/2353) done. Loss: 0.0026  lr:0.000000
[ Mon May 20 01:56:56 2024 ] 	Batch(1600/2353) done. Loss: 0.0188  lr:0.000000
[ Mon May 20 01:57:33 2024 ] 	Batch(1700/2353) done. Loss: 0.0183  lr:0.000000
[ Mon May 20 01:58:09 2024 ] 	Batch(1800/2353) done. Loss: 0.0330  lr:0.000000
[ Mon May 20 01:58:46 2024 ] 	Batch(1900/2353) done. Loss: 0.0277  lr:0.000000
[ Mon May 20 01:59:23 2024 ] 	Batch(2000/2353) done. Loss: 0.0078  lr:0.000000
[ Mon May 20 01:59:59 2024 ] 	Batch(2100/2353) done. Loss: 0.0339  lr:0.000000
[ Mon May 20 02:00:36 2024 ] 	Batch(2200/2353) done. Loss: 0.0437  lr:0.000000
[ Mon May 20 02:01:12 2024 ] 	Batch(2300/2353) done. Loss: 0.0535  lr:0.000000
[ Mon May 20 02:01:31 2024 ] 	Mean training loss: 0.0349.
[ Mon May 20 02:01:31 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 02:01:32 2024 ] Training epoch: 70
[ Mon May 20 02:01:32 2024 ] 	Batch(0/2353) done. Loss: 0.0832  lr:0.000000
[ Mon May 20 02:02:10 2024 ] 	Batch(100/2353) done. Loss: 0.0026  lr:0.000000
[ Mon May 20 02:02:47 2024 ] 	Batch(200/2353) done. Loss: 0.0340  lr:0.000000
[ Mon May 20 02:03:24 2024 ] 	Batch(300/2353) done. Loss: 0.1354  lr:0.000000
[ Mon May 20 02:04:01 2024 ] 	Batch(400/2353) done. Loss: 0.0336  lr:0.000000
[ Mon May 20 02:04:39 2024 ] 	Batch(500/2353) done. Loss: 0.0245  lr:0.000000
[ Mon May 20 02:05:16 2024 ] 	Batch(600/2353) done. Loss: 0.0080  lr:0.000000
[ Mon May 20 02:05:53 2024 ] 	Batch(700/2353) done. Loss: 0.0449  lr:0.000000
[ Mon May 20 02:06:30 2024 ] 	Batch(800/2353) done. Loss: 0.0304  lr:0.000000
[ Mon May 20 02:07:06 2024 ] 	Batch(900/2353) done. Loss: 0.0378  lr:0.000000
[ Mon May 20 02:07:43 2024 ] 	Batch(1000/2353) done. Loss: 0.0502  lr:0.000000
[ Mon May 20 02:08:19 2024 ] 	Batch(1100/2353) done. Loss: 0.0304  lr:0.000000
[ Mon May 20 02:08:56 2024 ] 	Batch(1200/2353) done. Loss: 0.1256  lr:0.000000
[ Mon May 20 02:09:33 2024 ] 	Batch(1300/2353) done. Loss: 0.0086  lr:0.000000
[ Mon May 20 02:10:09 2024 ] 	Batch(1400/2353) done. Loss: 0.0075  lr:0.000000
[ Mon May 20 02:10:46 2024 ] 	Batch(1500/2353) done. Loss: 0.0047  lr:0.000000
[ Mon May 20 02:11:23 2024 ] 	Batch(1600/2353) done. Loss: 0.0137  lr:0.000000
[ Mon May 20 02:11:59 2024 ] 	Batch(1700/2353) done. Loss: 0.0128  lr:0.000000
[ Mon May 20 02:12:36 2024 ] 	Batch(1800/2353) done. Loss: 0.0075  lr:0.000000
[ Mon May 20 02:13:12 2024 ] 	Batch(1900/2353) done. Loss: 0.0131  lr:0.000000
[ Mon May 20 02:13:49 2024 ] 	Batch(2000/2353) done. Loss: 0.1021  lr:0.000000
[ Mon May 20 02:14:26 2024 ] 	Batch(2100/2353) done. Loss: 0.0159  lr:0.000000
[ Mon May 20 02:15:02 2024 ] 	Batch(2200/2353) done. Loss: 0.0213  lr:0.000000
[ Mon May 20 02:15:39 2024 ] 	Batch(2300/2353) done. Loss: 0.0360  lr:0.000000
[ Mon May 20 02:15:58 2024 ] 	Mean training loss: 0.0405.
[ Mon May 20 02:15:58 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Mon May 20 02:15:58 2024 ] Eval epoch: 70
[ Mon May 20 02:18:01 2024 ] 	Mean val loss of 2367 batches: 0.24198487075881667.
[ Mon May 20 02:18:01 2024 ] Training epoch: 71
[ Mon May 20 02:18:02 2024 ] 	Batch(0/2353) done. Loss: 0.0053  lr:0.000000
[ Mon May 20 02:18:39 2024 ] 	Batch(100/2353) done. Loss: 0.0273  lr:0.000000
[ Mon May 20 02:19:15 2024 ] 	Batch(200/2353) done. Loss: 0.0233  lr:0.000000
[ Mon May 20 02:19:52 2024 ] 	Batch(300/2353) done. Loss: 0.0344  lr:0.000000
[ Mon May 20 02:20:29 2024 ] 	Batch(400/2353) done. Loss: 0.0034  lr:0.000000
[ Mon May 20 02:21:05 2024 ] 	Batch(500/2353) done. Loss: 0.0219  lr:0.000000
[ Mon May 20 02:21:42 2024 ] 	Batch(600/2353) done. Loss: 0.0886  lr:0.000000
[ Mon May 20 02:22:18 2024 ] 	Batch(700/2353) done. Loss: 0.1582  lr:0.000000
[ Mon May 20 02:22:55 2024 ] 	Batch(800/2353) done. Loss: 0.0827  lr:0.000000
[ Mon May 20 02:23:32 2024 ] 	Batch(900/2353) done. Loss: 0.0164  lr:0.000000
[ Mon May 20 02:24:08 2024 ] 	Batch(1000/2353) done. Loss: 0.0119  lr:0.000000
[ Mon May 20 02:24:45 2024 ] 	Batch(1100/2353) done. Loss: 0.0025  lr:0.000000
[ Mon May 20 02:25:21 2024 ] 	Batch(1200/2353) done. Loss: 0.0134  lr:0.000000
[ Mon May 20 02:25:58 2024 ] 	Batch(1300/2353) done. Loss: 0.0715  lr:0.000000
[ Mon May 20 02:26:35 2024 ] 	Batch(1400/2353) done. Loss: 0.0899  lr:0.000000
[ Mon May 20 02:27:11 2024 ] 	Batch(1500/2353) done. Loss: 0.0468  lr:0.000000
[ Mon May 20 02:27:48 2024 ] 	Batch(1600/2353) done. Loss: 0.0187  lr:0.000000
[ Mon May 20 02:28:24 2024 ] 	Batch(1700/2353) done. Loss: 0.0195  lr:0.000000
[ Mon May 20 02:29:01 2024 ] 	Batch(1800/2353) done. Loss: 0.0187  lr:0.000000
[ Mon May 20 02:29:38 2024 ] 	Batch(1900/2353) done. Loss: 0.0026  lr:0.000000
[ Mon May 20 02:30:14 2024 ] 	Batch(2000/2353) done. Loss: 0.0245  lr:0.000000
[ Mon May 20 02:30:51 2024 ] 	Batch(2100/2353) done. Loss: 0.1920  lr:0.000000
[ Mon May 20 02:31:28 2024 ] 	Batch(2200/2353) done. Loss: 0.0672  lr:0.000000
[ Mon May 20 02:32:04 2024 ] 	Batch(2300/2353) done. Loss: 0.0216  lr:0.000000
[ Mon May 20 02:32:23 2024 ] 	Mean training loss: 0.0365.
[ Mon May 20 02:32:23 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 02:32:23 2024 ] Training epoch: 72
[ Mon May 20 02:32:24 2024 ] 	Batch(0/2353) done. Loss: 0.0032  lr:0.000000
[ Mon May 20 02:33:01 2024 ] 	Batch(100/2353) done. Loss: 0.0176  lr:0.000000
[ Mon May 20 02:33:39 2024 ] 	Batch(200/2353) done. Loss: 0.0512  lr:0.000000
[ Mon May 20 02:34:16 2024 ] 	Batch(300/2353) done. Loss: 0.0079  lr:0.000000
[ Mon May 20 02:34:53 2024 ] 	Batch(400/2353) done. Loss: 0.2486  lr:0.000000
[ Mon May 20 02:35:30 2024 ] 	Batch(500/2353) done. Loss: 0.0916  lr:0.000000
[ Mon May 20 02:36:08 2024 ] 	Batch(600/2353) done. Loss: 0.0402  lr:0.000000
[ Mon May 20 02:36:44 2024 ] 	Batch(700/2353) done. Loss: 0.0253  lr:0.000000
[ Mon May 20 02:37:21 2024 ] 	Batch(800/2353) done. Loss: 0.0494  lr:0.000000
[ Mon May 20 02:37:58 2024 ] 	Batch(900/2353) done. Loss: 0.2855  lr:0.000000
[ Mon May 20 02:38:34 2024 ] 	Batch(1000/2353) done. Loss: 0.0110  lr:0.000000
[ Mon May 20 02:39:11 2024 ] 	Batch(1100/2353) done. Loss: 0.0135  lr:0.000000
[ Mon May 20 02:39:48 2024 ] 	Batch(1200/2353) done. Loss: 0.0452  lr:0.000000
[ Mon May 20 02:40:24 2024 ] 	Batch(1300/2353) done. Loss: 0.0040  lr:0.000000
[ Mon May 20 02:41:01 2024 ] 	Batch(1400/2353) done. Loss: 0.0274  lr:0.000000
[ Mon May 20 02:41:37 2024 ] 	Batch(1500/2353) done. Loss: 0.0233  lr:0.000000
[ Mon May 20 02:42:14 2024 ] 	Batch(1600/2353) done. Loss: 0.0475  lr:0.000000
[ Mon May 20 02:42:51 2024 ] 	Batch(1700/2353) done. Loss: 0.0355  lr:0.000000
[ Mon May 20 02:43:27 2024 ] 	Batch(1800/2353) done. Loss: 0.0161  lr:0.000000
[ Mon May 20 02:44:04 2024 ] 	Batch(1900/2353) done. Loss: 0.0233  lr:0.000000
[ Mon May 20 02:44:41 2024 ] 	Batch(2000/2353) done. Loss: 0.0315  lr:0.000000
[ Mon May 20 02:45:18 2024 ] 	Batch(2100/2353) done. Loss: 0.0285  lr:0.000000
[ Mon May 20 02:45:54 2024 ] 	Batch(2200/2353) done. Loss: 0.0047  lr:0.000000
[ Mon May 20 02:46:31 2024 ] 	Batch(2300/2353) done. Loss: 0.0061  lr:0.000000
[ Mon May 20 02:46:50 2024 ] 	Mean training loss: 0.0349.
[ Mon May 20 02:46:50 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Mon May 20 02:46:50 2024 ] Training epoch: 73
[ Mon May 20 02:46:51 2024 ] 	Batch(0/2353) done. Loss: 0.0091  lr:0.000000
[ Mon May 20 02:47:28 2024 ] 	Batch(100/2353) done. Loss: 0.0124  lr:0.000000
[ Mon May 20 02:48:04 2024 ] 	Batch(200/2353) done. Loss: 0.0039  lr:0.000000
[ Mon May 20 02:48:41 2024 ] 	Batch(300/2353) done. Loss: 0.0429  lr:0.000000
[ Mon May 20 02:49:18 2024 ] 	Batch(400/2353) done. Loss: 0.0342  lr:0.000000
[ Mon May 20 02:49:54 2024 ] 	Batch(500/2353) done. Loss: 0.0035  lr:0.000000
[ Mon May 20 02:50:31 2024 ] 	Batch(600/2353) done. Loss: 0.0318  lr:0.000000
[ Mon May 20 02:51:07 2024 ] 	Batch(700/2353) done. Loss: 0.0423  lr:0.000000
[ Mon May 20 02:51:44 2024 ] 	Batch(800/2353) done. Loss: 0.0119  lr:0.000000
[ Mon May 20 02:52:21 2024 ] 	Batch(900/2353) done. Loss: 0.0069  lr:0.000000
[ Mon May 20 02:52:57 2024 ] 	Batch(1000/2353) done. Loss: 0.0746  lr:0.000000
[ Mon May 20 02:53:34 2024 ] 	Batch(1100/2353) done. Loss: 0.0057  lr:0.000000
[ Mon May 20 02:54:10 2024 ] 	Batch(1200/2353) done. Loss: 0.1052  lr:0.000000
[ Mon May 20 02:54:47 2024 ] 	Batch(1300/2353) done. Loss: 0.0247  lr:0.000000
[ Mon May 20 02:55:24 2024 ] 	Batch(1400/2353) done. Loss: 0.0123  lr:0.000000
[ Mon May 20 02:56:01 2024 ] 	Batch(1500/2353) done. Loss: 0.0090  lr:0.000000
[ Mon May 20 02:56:38 2024 ] 	Batch(1600/2353) done. Loss: 0.0157  lr:0.000000
[ Mon May 20 02:57:14 2024 ] 	Batch(1700/2353) done. Loss: 0.0268  lr:0.000000
[ Mon May 20 02:57:51 2024 ] 	Batch(1800/2353) done. Loss: 0.0295  lr:0.000000
[ Mon May 20 02:58:27 2024 ] 	Batch(1900/2353) done. Loss: 0.0405  lr:0.000000
[ Mon May 20 02:59:04 2024 ] 	Batch(2000/2353) done. Loss: 0.0079  lr:0.000000
[ Mon May 20 02:59:41 2024 ] 	Batch(2100/2353) done. Loss: 0.0108  lr:0.000000
[ Mon May 20 03:00:17 2024 ] 	Batch(2200/2353) done. Loss: 0.0088  lr:0.000000
[ Mon May 20 03:00:54 2024 ] 	Batch(2300/2353) done. Loss: 0.0078  lr:0.000000
[ Mon May 20 03:01:13 2024 ] 	Mean training loss: 0.0374.
[ Mon May 20 03:01:13 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 03:01:13 2024 ] Training epoch: 74
[ Mon May 20 03:01:14 2024 ] 	Batch(0/2353) done. Loss: 0.0194  lr:0.000000
[ Mon May 20 03:01:50 2024 ] 	Batch(100/2353) done. Loss: 0.0214  lr:0.000000
[ Mon May 20 03:02:27 2024 ] 	Batch(200/2353) done. Loss: 0.0051  lr:0.000000
[ Mon May 20 03:03:03 2024 ] 	Batch(300/2353) done. Loss: 0.0505  lr:0.000000
[ Mon May 20 03:03:40 2024 ] 	Batch(400/2353) done. Loss: 0.0098  lr:0.000000
[ Mon May 20 03:04:17 2024 ] 	Batch(500/2353) done. Loss: 0.0037  lr:0.000000
[ Mon May 20 03:04:53 2024 ] 	Batch(600/2353) done. Loss: 0.1933  lr:0.000000
[ Mon May 20 03:05:30 2024 ] 	Batch(700/2353) done. Loss: 0.0237  lr:0.000000
[ Mon May 20 03:06:06 2024 ] 	Batch(800/2353) done. Loss: 0.0058  lr:0.000000
[ Mon May 20 03:06:43 2024 ] 	Batch(900/2353) done. Loss: 0.1922  lr:0.000000
[ Mon May 20 03:07:20 2024 ] 	Batch(1000/2353) done. Loss: 0.0413  lr:0.000000
[ Mon May 20 03:07:56 2024 ] 	Batch(1100/2353) done. Loss: 0.0041  lr:0.000000
[ Mon May 20 03:08:33 2024 ] 	Batch(1200/2353) done. Loss: 0.0431  lr:0.000000
[ Mon May 20 03:09:09 2024 ] 	Batch(1300/2353) done. Loss: 0.0184  lr:0.000000
[ Mon May 20 03:09:46 2024 ] 	Batch(1400/2353) done. Loss: 0.0051  lr:0.000000
[ Mon May 20 03:10:22 2024 ] 	Batch(1500/2353) done. Loss: 0.0040  lr:0.000000
[ Mon May 20 03:10:59 2024 ] 	Batch(1600/2353) done. Loss: 0.0557  lr:0.000000
[ Mon May 20 03:11:36 2024 ] 	Batch(1700/2353) done. Loss: 0.0045  lr:0.000000
[ Mon May 20 03:12:12 2024 ] 	Batch(1800/2353) done. Loss: 0.0116  lr:0.000000
[ Mon May 20 03:12:49 2024 ] 	Batch(1900/2353) done. Loss: 0.0074  lr:0.000000
[ Mon May 20 03:13:26 2024 ] 	Batch(2000/2353) done. Loss: 0.0042  lr:0.000000
[ Mon May 20 03:14:02 2024 ] 	Batch(2100/2353) done. Loss: 0.0112  lr:0.000000
[ Mon May 20 03:14:39 2024 ] 	Batch(2200/2353) done. Loss: 0.0898  lr:0.000000
[ Mon May 20 03:15:15 2024 ] 	Batch(2300/2353) done. Loss: 0.0194  lr:0.000000
[ Mon May 20 03:15:35 2024 ] 	Mean training loss: 0.0380.
[ Mon May 20 03:15:35 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 03:15:35 2024 ] Training epoch: 75
[ Mon May 20 03:15:35 2024 ] 	Batch(0/2353) done. Loss: 0.0107  lr:0.000000
[ Mon May 20 03:16:12 2024 ] 	Batch(100/2353) done. Loss: 0.0164  lr:0.000000
[ Mon May 20 03:16:48 2024 ] 	Batch(200/2353) done. Loss: 0.1119  lr:0.000000
[ Mon May 20 03:17:25 2024 ] 	Batch(300/2353) done. Loss: 0.0358  lr:0.000000
[ Mon May 20 03:18:02 2024 ] 	Batch(400/2353) done. Loss: 0.0797  lr:0.000000
[ Mon May 20 03:18:38 2024 ] 	Batch(500/2353) done. Loss: 0.0081  lr:0.000000
[ Mon May 20 03:19:15 2024 ] 	Batch(600/2353) done. Loss: 0.0310  lr:0.000000
[ Mon May 20 03:19:51 2024 ] 	Batch(700/2353) done. Loss: 0.0473  lr:0.000000
[ Mon May 20 03:20:28 2024 ] 	Batch(800/2353) done. Loss: 0.0194  lr:0.000000
[ Mon May 20 03:21:05 2024 ] 	Batch(900/2353) done. Loss: 0.0039  lr:0.000000
[ Mon May 20 03:21:41 2024 ] 	Batch(1000/2353) done. Loss: 0.0051  lr:0.000000
[ Mon May 20 03:22:18 2024 ] 	Batch(1100/2353) done. Loss: 0.0124  lr:0.000000
[ Mon May 20 03:22:55 2024 ] 	Batch(1200/2353) done. Loss: 0.0307  lr:0.000000
[ Mon May 20 03:23:31 2024 ] 	Batch(1300/2353) done. Loss: 0.0133  lr:0.000000
[ Mon May 20 03:24:08 2024 ] 	Batch(1400/2353) done. Loss: 0.0614  lr:0.000000
[ Mon May 20 03:24:45 2024 ] 	Batch(1500/2353) done. Loss: 0.0308  lr:0.000000
[ Mon May 20 03:25:21 2024 ] 	Batch(1600/2353) done. Loss: 0.0028  lr:0.000000
[ Mon May 20 03:25:58 2024 ] 	Batch(1700/2353) done. Loss: 0.0629  lr:0.000000
[ Mon May 20 03:26:35 2024 ] 	Batch(1800/2353) done. Loss: 0.2335  lr:0.000000
[ Mon May 20 03:27:11 2024 ] 	Batch(1900/2353) done. Loss: 0.0077  lr:0.000000
[ Mon May 20 03:27:48 2024 ] 	Batch(2000/2353) done. Loss: 0.0344  lr:0.000000
[ Mon May 20 03:28:24 2024 ] 	Batch(2100/2353) done. Loss: 0.0071  lr:0.000000
[ Mon May 20 03:29:01 2024 ] 	Batch(2200/2353) done. Loss: 0.0166  lr:0.000000
[ Mon May 20 03:29:38 2024 ] 	Batch(2300/2353) done. Loss: 0.0183  lr:0.000000
[ Mon May 20 03:29:57 2024 ] 	Mean training loss: 0.0362.
[ Mon May 20 03:29:57 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 03:29:57 2024 ] Training epoch: 76
[ Mon May 20 03:29:58 2024 ] 	Batch(0/2353) done. Loss: 0.0443  lr:0.000000
[ Mon May 20 03:30:35 2024 ] 	Batch(100/2353) done. Loss: 0.0192  lr:0.000000
[ Mon May 20 03:31:12 2024 ] 	Batch(200/2353) done. Loss: 0.0044  lr:0.000000
[ Mon May 20 03:31:49 2024 ] 	Batch(300/2353) done. Loss: 0.0225  lr:0.000000
[ Mon May 20 03:32:25 2024 ] 	Batch(400/2353) done. Loss: 0.0065  lr:0.000000
[ Mon May 20 03:33:02 2024 ] 	Batch(500/2353) done. Loss: 0.0247  lr:0.000000
[ Mon May 20 03:33:39 2024 ] 	Batch(600/2353) done. Loss: 0.0139  lr:0.000000
[ Mon May 20 03:34:16 2024 ] 	Batch(700/2353) done. Loss: 0.0328  lr:0.000000
[ Mon May 20 03:34:53 2024 ] 	Batch(800/2353) done. Loss: 0.0039  lr:0.000000
[ Mon May 20 03:35:29 2024 ] 	Batch(900/2353) done. Loss: 0.0192  lr:0.000000
[ Mon May 20 03:36:06 2024 ] 	Batch(1000/2353) done. Loss: 0.0038  lr:0.000000
[ Mon May 20 03:36:43 2024 ] 	Batch(1100/2353) done. Loss: 0.0331  lr:0.000000
[ Mon May 20 03:37:19 2024 ] 	Batch(1200/2353) done. Loss: 0.0084  lr:0.000000
[ Mon May 20 03:37:56 2024 ] 	Batch(1300/2353) done. Loss: 0.0148  lr:0.000000
[ Mon May 20 03:38:33 2024 ] 	Batch(1400/2353) done. Loss: 0.0020  lr:0.000000
[ Mon May 20 03:39:10 2024 ] 	Batch(1500/2353) done. Loss: 0.0771  lr:0.000000
[ Mon May 20 03:39:47 2024 ] 	Batch(1600/2353) done. Loss: 0.0088  lr:0.000000
[ Mon May 20 03:40:25 2024 ] 	Batch(1700/2353) done. Loss: 0.0110  lr:0.000000
[ Mon May 20 03:41:02 2024 ] 	Batch(1800/2353) done. Loss: 0.0059  lr:0.000000
[ Mon May 20 03:41:38 2024 ] 	Batch(1900/2353) done. Loss: 0.0046  lr:0.000000
[ Mon May 20 03:42:15 2024 ] 	Batch(2000/2353) done. Loss: 0.0246  lr:0.000000
[ Mon May 20 03:42:52 2024 ] 	Batch(2100/2353) done. Loss: 0.0165  lr:0.000000
[ Mon May 20 03:43:29 2024 ] 	Batch(2200/2353) done. Loss: 0.0736  lr:0.000000
[ Mon May 20 03:44:06 2024 ] 	Batch(2300/2353) done. Loss: 0.0152  lr:0.000000
[ Mon May 20 03:44:25 2024 ] 	Mean training loss: 0.0387.
[ Mon May 20 03:44:25 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Mon May 20 03:44:25 2024 ] Training epoch: 77
[ Mon May 20 03:44:26 2024 ] 	Batch(0/2353) done. Loss: 0.0165  lr:0.000000
[ Mon May 20 03:45:02 2024 ] 	Batch(100/2353) done. Loss: 0.0500  lr:0.000000
[ Mon May 20 03:45:39 2024 ] 	Batch(200/2353) done. Loss: 0.0029  lr:0.000000
[ Mon May 20 03:46:16 2024 ] 	Batch(300/2353) done. Loss: 0.0106  lr:0.000000
[ Mon May 20 03:46:52 2024 ] 	Batch(400/2353) done. Loss: 0.0088  lr:0.000000
[ Mon May 20 03:47:29 2024 ] 	Batch(500/2353) done. Loss: 0.0368  lr:0.000000
[ Mon May 20 03:48:06 2024 ] 	Batch(600/2353) done. Loss: 0.0064  lr:0.000000
[ Mon May 20 03:48:43 2024 ] 	Batch(700/2353) done. Loss: 0.0093  lr:0.000000
[ Mon May 20 03:49:20 2024 ] 	Batch(800/2353) done. Loss: 0.0112  lr:0.000000
[ Mon May 20 03:49:57 2024 ] 	Batch(900/2353) done. Loss: 0.0186  lr:0.000000
[ Mon May 20 03:50:33 2024 ] 	Batch(1000/2353) done. Loss: 0.0095  lr:0.000000
[ Mon May 20 03:51:10 2024 ] 	Batch(1100/2353) done. Loss: 0.0450  lr:0.000000
[ Mon May 20 03:51:47 2024 ] 	Batch(1200/2353) done. Loss: 0.0292  lr:0.000000
[ Mon May 20 03:52:24 2024 ] 	Batch(1300/2353) done. Loss: 0.0706  lr:0.000000
[ Mon May 20 03:53:01 2024 ] 	Batch(1400/2353) done. Loss: 0.0109  lr:0.000000
[ Mon May 20 03:53:37 2024 ] 	Batch(1500/2353) done. Loss: 0.0093  lr:0.000000
[ Mon May 20 03:54:14 2024 ] 	Batch(1600/2353) done. Loss: 0.0075  lr:0.000000
[ Mon May 20 03:54:51 2024 ] 	Batch(1700/2353) done. Loss: 0.0109  lr:0.000000
[ Mon May 20 03:55:27 2024 ] 	Batch(1800/2353) done. Loss: 0.0034  lr:0.000000
[ Mon May 20 03:56:04 2024 ] 	Batch(1900/2353) done. Loss: 0.0384  lr:0.000000
[ Mon May 20 03:56:41 2024 ] 	Batch(2000/2353) done. Loss: 0.0116  lr:0.000000
[ Mon May 20 03:57:17 2024 ] 	Batch(2100/2353) done. Loss: 0.0230  lr:0.000000
[ Mon May 20 03:57:55 2024 ] 	Batch(2200/2353) done. Loss: 0.0013  lr:0.000000
[ Mon May 20 03:58:32 2024 ] 	Batch(2300/2353) done. Loss: 0.0179  lr:0.000000
[ Mon May 20 03:58:51 2024 ] 	Mean training loss: 0.0372.
[ Mon May 20 03:58:51 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 03:58:51 2024 ] Training epoch: 78
[ Mon May 20 03:58:52 2024 ] 	Batch(0/2353) done. Loss: 0.0909  lr:0.000000
[ Mon May 20 03:59:29 2024 ] 	Batch(100/2353) done. Loss: 0.0205  lr:0.000000
[ Mon May 20 04:00:06 2024 ] 	Batch(200/2353) done. Loss: 0.0035  lr:0.000000
[ Mon May 20 04:00:43 2024 ] 	Batch(300/2353) done. Loss: 0.0211  lr:0.000000
[ Mon May 20 04:01:20 2024 ] 	Batch(400/2353) done. Loss: 0.0819  lr:0.000000
[ Mon May 20 04:01:57 2024 ] 	Batch(500/2353) done. Loss: 0.0043  lr:0.000000
[ Mon May 20 04:02:34 2024 ] 	Batch(600/2353) done. Loss: 0.0188  lr:0.000000
[ Mon May 20 04:03:12 2024 ] 	Batch(700/2353) done. Loss: 0.0288  lr:0.000000
[ Mon May 20 04:03:49 2024 ] 	Batch(800/2353) done. Loss: 0.0394  lr:0.000000
[ Mon May 20 04:04:25 2024 ] 	Batch(900/2353) done. Loss: 0.0244  lr:0.000000
[ Mon May 20 04:05:02 2024 ] 	Batch(1000/2353) done. Loss: 0.0159  lr:0.000000
[ Mon May 20 04:05:39 2024 ] 	Batch(1100/2353) done. Loss: 0.1379  lr:0.000000
[ Mon May 20 04:06:15 2024 ] 	Batch(1200/2353) done. Loss: 0.2534  lr:0.000000
[ Mon May 20 04:06:52 2024 ] 	Batch(1300/2353) done. Loss: 0.0057  lr:0.000000
[ Mon May 20 04:07:29 2024 ] 	Batch(1400/2353) done. Loss: 0.1336  lr:0.000000
[ Mon May 20 04:08:05 2024 ] 	Batch(1500/2353) done. Loss: 0.0879  lr:0.000000
[ Mon May 20 04:08:42 2024 ] 	Batch(1600/2353) done. Loss: 0.0063  lr:0.000000
[ Mon May 20 04:09:18 2024 ] 	Batch(1700/2353) done. Loss: 0.1047  lr:0.000000
[ Mon May 20 04:09:55 2024 ] 	Batch(1800/2353) done. Loss: 0.0271  lr:0.000000
[ Mon May 20 04:10:32 2024 ] 	Batch(1900/2353) done. Loss: 0.0700  lr:0.000000
[ Mon May 20 04:11:08 2024 ] 	Batch(2000/2353) done. Loss: 0.0165  lr:0.000000
[ Mon May 20 04:11:45 2024 ] 	Batch(2100/2353) done. Loss: 0.0222  lr:0.000000
[ Mon May 20 04:12:22 2024 ] 	Batch(2200/2353) done. Loss: 0.0149  lr:0.000000
[ Mon May 20 04:12:58 2024 ] 	Batch(2300/2353) done. Loss: 0.0260  lr:0.000000
[ Mon May 20 04:13:17 2024 ] 	Mean training loss: 0.0395.
[ Mon May 20 04:13:17 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Mon May 20 04:13:17 2024 ] Training epoch: 79
[ Mon May 20 04:13:18 2024 ] 	Batch(0/2353) done. Loss: 0.0037  lr:0.000000
[ Mon May 20 04:13:55 2024 ] 	Batch(100/2353) done. Loss: 0.0070  lr:0.000000
[ Mon May 20 04:14:31 2024 ] 	Batch(200/2353) done. Loss: 0.0050  lr:0.000000
[ Mon May 20 04:15:08 2024 ] 	Batch(300/2353) done. Loss: 0.0209  lr:0.000000
[ Mon May 20 04:15:45 2024 ] 	Batch(400/2353) done. Loss: 0.0052  lr:0.000000
[ Mon May 20 04:16:21 2024 ] 	Batch(500/2353) done. Loss: 0.0167  lr:0.000000
[ Mon May 20 04:16:58 2024 ] 	Batch(600/2353) done. Loss: 0.0177  lr:0.000000
[ Mon May 20 04:17:34 2024 ] 	Batch(700/2353) done. Loss: 0.0341  lr:0.000000
[ Mon May 20 04:18:11 2024 ] 	Batch(800/2353) done. Loss: 0.0030  lr:0.000000
[ Mon May 20 04:18:48 2024 ] 	Batch(900/2353) done. Loss: 0.0009  lr:0.000000
[ Mon May 20 04:19:24 2024 ] 	Batch(1000/2353) done. Loss: 0.0460  lr:0.000000
[ Mon May 20 04:20:01 2024 ] 	Batch(1100/2353) done. Loss: 0.0405  lr:0.000000
[ Mon May 20 04:20:38 2024 ] 	Batch(1200/2353) done. Loss: 0.0614  lr:0.000000
[ Mon May 20 04:21:14 2024 ] 	Batch(1300/2353) done. Loss: 0.0307  lr:0.000000
[ Mon May 20 04:21:51 2024 ] 	Batch(1400/2353) done. Loss: 0.0655  lr:0.000000
[ Mon May 20 04:22:28 2024 ] 	Batch(1500/2353) done. Loss: 0.0044  lr:0.000000
[ Mon May 20 04:23:04 2024 ] 	Batch(1600/2353) done. Loss: 0.1459  lr:0.000000
[ Mon May 20 04:23:41 2024 ] 	Batch(1700/2353) done. Loss: 0.0485  lr:0.000000
[ Mon May 20 04:24:17 2024 ] 	Batch(1800/2353) done. Loss: 0.0176  lr:0.000000
[ Mon May 20 04:24:54 2024 ] 	Batch(1900/2353) done. Loss: 0.0123  lr:0.000000
[ Mon May 20 04:25:31 2024 ] 	Batch(2000/2353) done. Loss: 0.0015  lr:0.000000
[ Mon May 20 04:26:07 2024 ] 	Batch(2100/2353) done. Loss: 0.0158  lr:0.000000
[ Mon May 20 04:26:44 2024 ] 	Batch(2200/2353) done. Loss: 0.0301  lr:0.000000
[ Mon May 20 04:27:21 2024 ] 	Batch(2300/2353) done. Loss: 0.0294  lr:0.000000
[ Mon May 20 04:27:40 2024 ] 	Mean training loss: 0.0395.
[ Mon May 20 04:27:40 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 04:27:40 2024 ] Training epoch: 80
[ Mon May 20 04:27:41 2024 ] 	Batch(0/2353) done. Loss: 0.0069  lr:0.000000
[ Mon May 20 04:28:17 2024 ] 	Batch(100/2353) done. Loss: 0.0418  lr:0.000000
[ Mon May 20 04:28:54 2024 ] 	Batch(200/2353) done. Loss: 0.0261  lr:0.000000
[ Mon May 20 04:29:30 2024 ] 	Batch(300/2353) done. Loss: 0.0124  lr:0.000000
[ Mon May 20 04:30:07 2024 ] 	Batch(400/2353) done. Loss: 0.0656  lr:0.000000
[ Mon May 20 04:30:43 2024 ] 	Batch(500/2353) done. Loss: 0.0069  lr:0.000000
[ Mon May 20 04:31:20 2024 ] 	Batch(600/2353) done. Loss: 0.0043  lr:0.000000
[ Mon May 20 04:31:57 2024 ] 	Batch(700/2353) done. Loss: 0.0301  lr:0.000000
[ Mon May 20 04:32:33 2024 ] 	Batch(800/2353) done. Loss: 0.0063  lr:0.000000
[ Mon May 20 04:33:10 2024 ] 	Batch(900/2353) done. Loss: 0.0313  lr:0.000000
[ Mon May 20 04:33:47 2024 ] 	Batch(1000/2353) done. Loss: 0.0154  lr:0.000000
[ Mon May 20 04:34:23 2024 ] 	Batch(1100/2353) done. Loss: 0.0119  lr:0.000000
[ Mon May 20 04:35:00 2024 ] 	Batch(1200/2353) done. Loss: 0.0032  lr:0.000000
[ Mon May 20 04:35:37 2024 ] 	Batch(1300/2353) done. Loss: 0.0737  lr:0.000000
[ Mon May 20 04:36:13 2024 ] 	Batch(1400/2353) done. Loss: 0.0223  lr:0.000000
[ Mon May 20 04:36:50 2024 ] 	Batch(1500/2353) done. Loss: 0.0327  lr:0.000000
[ Mon May 20 04:37:27 2024 ] 	Batch(1600/2353) done. Loss: 0.0100  lr:0.000000
[ Mon May 20 04:38:03 2024 ] 	Batch(1700/2353) done. Loss: 0.0016  lr:0.000000
[ Mon May 20 04:38:40 2024 ] 	Batch(1800/2353) done. Loss: 0.0203  lr:0.000000
[ Mon May 20 04:39:16 2024 ] 	Batch(1900/2353) done. Loss: 0.0342  lr:0.000000
[ Mon May 20 04:39:53 2024 ] 	Batch(2000/2353) done. Loss: 0.0190  lr:0.000000
[ Mon May 20 04:40:30 2024 ] 	Batch(2100/2353) done. Loss: 0.0318  lr:0.000000
[ Mon May 20 04:41:06 2024 ] 	Batch(2200/2353) done. Loss: 0.0291  lr:0.000000
[ Mon May 20 04:41:43 2024 ] 	Batch(2300/2353) done. Loss: 0.1037  lr:0.000000
[ Mon May 20 04:42:02 2024 ] 	Mean training loss: 0.0383.
[ Mon May 20 04:42:02 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 04:42:02 2024 ] Eval epoch: 80
[ Mon May 20 04:44:05 2024 ] 	Mean val loss of 2367 batches: 0.23884499075292426.
[ Mon May 20 04:44:05 2024 ] Training epoch: 81
[ Mon May 20 04:44:06 2024 ] 	Batch(0/2353) done. Loss: 0.0031  lr:0.000000
[ Mon May 20 04:44:43 2024 ] 	Batch(100/2353) done. Loss: 0.0745  lr:0.000000
[ Mon May 20 04:45:19 2024 ] 	Batch(200/2353) done. Loss: 0.0489  lr:0.000000
[ Mon May 20 04:45:56 2024 ] 	Batch(300/2353) done. Loss: 0.0317  lr:0.000000
[ Mon May 20 04:46:33 2024 ] 	Batch(400/2353) done. Loss: 0.0024  lr:0.000000
[ Mon May 20 04:47:09 2024 ] 	Batch(500/2353) done. Loss: 0.0223  lr:0.000000
[ Mon May 20 04:47:46 2024 ] 	Batch(600/2353) done. Loss: 0.1276  lr:0.000000
[ Mon May 20 04:48:23 2024 ] 	Batch(700/2353) done. Loss: 0.0198  lr:0.000000
[ Mon May 20 04:49:00 2024 ] 	Batch(800/2353) done. Loss: 0.0838  lr:0.000000
[ Mon May 20 04:49:37 2024 ] 	Batch(900/2353) done. Loss: 0.0328  lr:0.000000
[ Mon May 20 04:50:14 2024 ] 	Batch(1000/2353) done. Loss: 0.0086  lr:0.000000
[ Mon May 20 04:50:50 2024 ] 	Batch(1100/2353) done. Loss: 0.0057  lr:0.000000
[ Mon May 20 04:51:27 2024 ] 	Batch(1200/2353) done. Loss: 0.0111  lr:0.000000
[ Mon May 20 04:52:03 2024 ] 	Batch(1300/2353) done. Loss: 0.0263  lr:0.000000
[ Mon May 20 04:52:40 2024 ] 	Batch(1400/2353) done. Loss: 0.0561  lr:0.000000
[ Mon May 20 04:53:16 2024 ] 	Batch(1500/2353) done. Loss: 0.0090  lr:0.000000
[ Mon May 20 04:53:53 2024 ] 	Batch(1600/2353) done. Loss: 0.1361  lr:0.000000
[ Mon May 20 04:54:30 2024 ] 	Batch(1700/2353) done. Loss: 0.0078  lr:0.000000
[ Mon May 20 04:55:06 2024 ] 	Batch(1800/2353) done. Loss: 0.0302  lr:0.000000
[ Mon May 20 04:55:43 2024 ] 	Batch(1900/2353) done. Loss: 0.0037  lr:0.000000
[ Mon May 20 04:56:19 2024 ] 	Batch(2000/2353) done. Loss: 0.0600  lr:0.000000
[ Mon May 20 04:56:56 2024 ] 	Batch(2100/2353) done. Loss: 0.1284  lr:0.000000
[ Mon May 20 04:57:32 2024 ] 	Batch(2200/2353) done. Loss: 0.0166  lr:0.000000
[ Mon May 20 04:58:09 2024 ] 	Batch(2300/2353) done. Loss: 0.0342  lr:0.000000
[ Mon May 20 04:58:28 2024 ] 	Mean training loss: 0.0384.
[ Mon May 20 04:58:28 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 04:58:28 2024 ] Training epoch: 82
[ Mon May 20 04:58:29 2024 ] 	Batch(0/2353) done. Loss: 0.0797  lr:0.000000
[ Mon May 20 04:59:05 2024 ] 	Batch(100/2353) done. Loss: 0.0265  lr:0.000000
[ Mon May 20 04:59:42 2024 ] 	Batch(200/2353) done. Loss: 0.0131  lr:0.000000
[ Mon May 20 05:00:19 2024 ] 	Batch(300/2353) done. Loss: 0.0425  lr:0.000000
[ Mon May 20 05:00:55 2024 ] 	Batch(400/2353) done. Loss: 0.0049  lr:0.000000
[ Mon May 20 05:01:32 2024 ] 	Batch(500/2353) done. Loss: 0.0435  lr:0.000000
[ Mon May 20 05:02:08 2024 ] 	Batch(600/2353) done. Loss: 0.0235  lr:0.000000
[ Mon May 20 05:02:45 2024 ] 	Batch(700/2353) done. Loss: 0.0193  lr:0.000000
[ Mon May 20 05:03:22 2024 ] 	Batch(800/2353) done. Loss: 0.0035  lr:0.000000
[ Mon May 20 05:03:58 2024 ] 	Batch(900/2353) done. Loss: 0.3793  lr:0.000000
[ Mon May 20 05:04:35 2024 ] 	Batch(1000/2353) done. Loss: 0.0392  lr:0.000000
[ Mon May 20 05:05:12 2024 ] 	Batch(1100/2353) done. Loss: 0.0510  lr:0.000000
[ Mon May 20 05:05:49 2024 ] 	Batch(1200/2353) done. Loss: 0.0033  lr:0.000000
[ Mon May 20 05:06:25 2024 ] 	Batch(1300/2353) done. Loss: 0.0261  lr:0.000000
[ Mon May 20 05:07:02 2024 ] 	Batch(1400/2353) done. Loss: 0.0649  lr:0.000000
[ Mon May 20 05:07:39 2024 ] 	Batch(1500/2353) done. Loss: 0.0329  lr:0.000000
[ Mon May 20 05:08:16 2024 ] 	Batch(1600/2353) done. Loss: 0.0244  lr:0.000000
[ Mon May 20 05:08:53 2024 ] 	Batch(1700/2353) done. Loss: 0.0130  lr:0.000000
[ Mon May 20 05:09:31 2024 ] 	Batch(1800/2353) done. Loss: 0.1038  lr:0.000000
[ Mon May 20 05:10:07 2024 ] 	Batch(1900/2353) done. Loss: 0.0296  lr:0.000000
[ Mon May 20 05:10:44 2024 ] 	Batch(2000/2353) done. Loss: 0.0234  lr:0.000000
[ Mon May 20 05:11:21 2024 ] 	Batch(2100/2353) done. Loss: 0.0012  lr:0.000000
[ Mon May 20 05:11:57 2024 ] 	Batch(2200/2353) done. Loss: 0.0160  lr:0.000000
[ Mon May 20 05:12:34 2024 ] 	Batch(2300/2353) done. Loss: 0.0142  lr:0.000000
[ Mon May 20 05:12:53 2024 ] 	Mean training loss: 0.0383.
[ Mon May 20 05:12:53 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Mon May 20 05:12:53 2024 ] Training epoch: 83
[ Mon May 20 05:12:54 2024 ] 	Batch(0/2353) done. Loss: 0.0247  lr:0.000000
[ Mon May 20 05:13:30 2024 ] 	Batch(100/2353) done. Loss: 0.0384  lr:0.000000
[ Mon May 20 05:14:07 2024 ] 	Batch(200/2353) done. Loss: 0.0054  lr:0.000000
[ Mon May 20 05:14:43 2024 ] 	Batch(300/2353) done. Loss: 0.0069  lr:0.000000
[ Mon May 20 05:15:20 2024 ] 	Batch(400/2353) done. Loss: 0.0351  lr:0.000000
[ Mon May 20 05:15:57 2024 ] 	Batch(500/2353) done. Loss: 0.0484  lr:0.000000
[ Mon May 20 05:16:33 2024 ] 	Batch(600/2353) done. Loss: 0.0028  lr:0.000000
[ Mon May 20 05:17:10 2024 ] 	Batch(700/2353) done. Loss: 0.0143  lr:0.000000
[ Mon May 20 05:17:47 2024 ] 	Batch(800/2353) done. Loss: 0.0051  lr:0.000000
[ Mon May 20 05:18:23 2024 ] 	Batch(900/2353) done. Loss: 0.0726  lr:0.000000
[ Mon May 20 05:19:00 2024 ] 	Batch(1000/2353) done. Loss: 0.0205  lr:0.000000
[ Mon May 20 05:19:36 2024 ] 	Batch(1100/2353) done. Loss: 0.0750  lr:0.000000
[ Mon May 20 05:20:13 2024 ] 	Batch(1200/2353) done. Loss: 0.0052  lr:0.000000
[ Mon May 20 05:20:50 2024 ] 	Batch(1300/2353) done. Loss: 0.0082  lr:0.000000
[ Mon May 20 05:21:27 2024 ] 	Batch(1400/2353) done. Loss: 0.0315  lr:0.000000
[ Mon May 20 05:22:03 2024 ] 	Batch(1500/2353) done. Loss: 0.0135  lr:0.000000
[ Mon May 20 05:22:40 2024 ] 	Batch(1600/2353) done. Loss: 0.0181  lr:0.000000
[ Mon May 20 05:23:16 2024 ] 	Batch(1700/2353) done. Loss: 0.0580  lr:0.000000
[ Mon May 20 05:23:53 2024 ] 	Batch(1800/2353) done. Loss: 0.0291  lr:0.000000
[ Mon May 20 05:24:30 2024 ] 	Batch(1900/2353) done. Loss: 0.0027  lr:0.000000
[ Mon May 20 05:25:06 2024 ] 	Batch(2000/2353) done. Loss: 0.0263  lr:0.000000
[ Mon May 20 05:25:43 2024 ] 	Batch(2100/2353) done. Loss: 0.0133  lr:0.000000
[ Mon May 20 05:26:20 2024 ] 	Batch(2200/2353) done. Loss: 0.0033  lr:0.000000
[ Mon May 20 05:26:56 2024 ] 	Batch(2300/2353) done. Loss: 0.0155  lr:0.000000
[ Mon May 20 05:27:15 2024 ] 	Mean training loss: 0.0374.
[ Mon May 20 05:27:15 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 05:27:16 2024 ] Training epoch: 84
[ Mon May 20 05:27:16 2024 ] 	Batch(0/2353) done. Loss: 0.0090  lr:0.000000
[ Mon May 20 05:27:53 2024 ] 	Batch(100/2353) done. Loss: 0.0210  lr:0.000000
[ Mon May 20 05:28:29 2024 ] 	Batch(200/2353) done. Loss: 0.0108  lr:0.000000
[ Mon May 20 05:29:06 2024 ] 	Batch(300/2353) done. Loss: 0.0143  lr:0.000000
[ Mon May 20 05:29:43 2024 ] 	Batch(400/2353) done. Loss: 0.0139  lr:0.000000
[ Mon May 20 05:30:19 2024 ] 	Batch(500/2353) done. Loss: 0.0170  lr:0.000000
[ Mon May 20 05:30:56 2024 ] 	Batch(600/2353) done. Loss: 0.0371  lr:0.000000
[ Mon May 20 05:31:32 2024 ] 	Batch(700/2353) done. Loss: 0.0103  lr:0.000000
[ Mon May 20 05:32:09 2024 ] 	Batch(800/2353) done. Loss: 0.0154  lr:0.000000
[ Mon May 20 05:32:46 2024 ] 	Batch(900/2353) done. Loss: 0.0529  lr:0.000000
[ Mon May 20 05:33:22 2024 ] 	Batch(1000/2353) done. Loss: 0.0267  lr:0.000000
[ Mon May 20 05:33:59 2024 ] 	Batch(1100/2353) done. Loss: 0.0277  lr:0.000000
[ Mon May 20 05:34:36 2024 ] 	Batch(1200/2353) done. Loss: 0.0104  lr:0.000000
[ Mon May 20 05:35:12 2024 ] 	Batch(1300/2353) done. Loss: 0.0131  lr:0.000000
[ Mon May 20 05:35:49 2024 ] 	Batch(1400/2353) done. Loss: 0.0052  lr:0.000000
[ Mon May 20 05:36:26 2024 ] 	Batch(1500/2353) done. Loss: 0.0166  lr:0.000000
[ Mon May 20 05:37:02 2024 ] 	Batch(1600/2353) done. Loss: 0.0628  lr:0.000000
[ Mon May 20 05:37:39 2024 ] 	Batch(1700/2353) done. Loss: 0.1402  lr:0.000000
[ Mon May 20 05:38:16 2024 ] 	Batch(1800/2353) done. Loss: 0.0064  lr:0.000000
[ Mon May 20 05:38:52 2024 ] 	Batch(1900/2353) done. Loss: 0.0650  lr:0.000000
[ Mon May 20 05:39:29 2024 ] 	Batch(2000/2353) done. Loss: 0.0009  lr:0.000000
[ Mon May 20 05:40:06 2024 ] 	Batch(2100/2353) done. Loss: 0.0229  lr:0.000000
[ Mon May 20 05:40:43 2024 ] 	Batch(2200/2353) done. Loss: 0.0509  lr:0.000000
[ Mon May 20 05:41:19 2024 ] 	Batch(2300/2353) done. Loss: 0.0129  lr:0.000000
[ Mon May 20 05:41:38 2024 ] 	Mean training loss: 0.0372.
[ Mon May 20 05:41:38 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 05:41:38 2024 ] Training epoch: 85
[ Mon May 20 05:41:39 2024 ] 	Batch(0/2353) done. Loss: 0.0142  lr:0.000000
[ Mon May 20 05:42:16 2024 ] 	Batch(100/2353) done. Loss: 0.0318  lr:0.000000
[ Mon May 20 05:42:52 2024 ] 	Batch(200/2353) done. Loss: 0.0031  lr:0.000000
[ Mon May 20 05:43:29 2024 ] 	Batch(300/2353) done. Loss: 0.0529  lr:0.000000
[ Mon May 20 05:44:05 2024 ] 	Batch(400/2353) done. Loss: 0.0406  lr:0.000000
[ Mon May 20 05:44:42 2024 ] 	Batch(500/2353) done. Loss: 0.0182  lr:0.000000
[ Mon May 20 05:45:19 2024 ] 	Batch(600/2353) done. Loss: 0.0229  lr:0.000000
[ Mon May 20 05:45:55 2024 ] 	Batch(700/2353) done. Loss: 0.0139  lr:0.000000
[ Mon May 20 05:46:32 2024 ] 	Batch(800/2353) done. Loss: 0.0086  lr:0.000000
[ Mon May 20 05:47:09 2024 ] 	Batch(900/2353) done. Loss: 0.1949  lr:0.000000
[ Mon May 20 05:47:45 2024 ] 	Batch(1000/2353) done. Loss: 0.0089  lr:0.000000
[ Mon May 20 05:48:22 2024 ] 	Batch(1100/2353) done. Loss: 0.0353  lr:0.000000
[ Mon May 20 05:48:59 2024 ] 	Batch(1200/2353) done. Loss: 0.0488  lr:0.000000
[ Mon May 20 05:49:35 2024 ] 	Batch(1300/2353) done. Loss: 0.0821  lr:0.000000
[ Mon May 20 05:50:12 2024 ] 	Batch(1400/2353) done. Loss: 0.0273  lr:0.000000
[ Mon May 20 05:50:48 2024 ] 	Batch(1500/2353) done. Loss: 0.0019  lr:0.000000
[ Mon May 20 05:51:25 2024 ] 	Batch(1600/2353) done. Loss: 0.0473  lr:0.000000
[ Mon May 20 05:52:02 2024 ] 	Batch(1700/2353) done. Loss: 0.0245  lr:0.000000
[ Mon May 20 05:52:38 2024 ] 	Batch(1800/2353) done. Loss: 0.0439  lr:0.000000
[ Mon May 20 05:53:15 2024 ] 	Batch(1900/2353) done. Loss: 0.0400  lr:0.000000
[ Mon May 20 05:53:52 2024 ] 	Batch(2000/2353) done. Loss: 0.0355  lr:0.000000
[ Mon May 20 05:54:28 2024 ] 	Batch(2100/2353) done. Loss: 0.0176  lr:0.000000
[ Mon May 20 05:55:05 2024 ] 	Batch(2200/2353) done. Loss: 0.0424  lr:0.000000
[ Mon May 20 05:55:41 2024 ] 	Batch(2300/2353) done. Loss: 0.0099  lr:0.000000
[ Mon May 20 05:56:01 2024 ] 	Mean training loss: 0.0367.
[ Mon May 20 05:56:01 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 05:56:01 2024 ] Training epoch: 86
[ Mon May 20 05:56:01 2024 ] 	Batch(0/2353) done. Loss: 0.0141  lr:0.000000
[ Mon May 20 05:56:38 2024 ] 	Batch(100/2353) done. Loss: 0.0580  lr:0.000000
[ Mon May 20 05:57:16 2024 ] 	Batch(200/2353) done. Loss: 0.0178  lr:0.000000
[ Mon May 20 05:57:53 2024 ] 	Batch(300/2353) done. Loss: 0.0215  lr:0.000000
[ Mon May 20 05:58:30 2024 ] 	Batch(400/2353) done. Loss: 0.0066  lr:0.000000
[ Mon May 20 05:59:08 2024 ] 	Batch(500/2353) done. Loss: 0.0272  lr:0.000000
[ Mon May 20 05:59:45 2024 ] 	Batch(600/2353) done. Loss: 0.2095  lr:0.000000
[ Mon May 20 06:00:22 2024 ] 	Batch(700/2353) done. Loss: 0.0111  lr:0.000000
[ Mon May 20 06:01:00 2024 ] 	Batch(800/2353) done. Loss: 0.0851  lr:0.000000
[ Mon May 20 06:01:36 2024 ] 	Batch(900/2353) done. Loss: 0.0228  lr:0.000000
[ Mon May 20 06:02:13 2024 ] 	Batch(1000/2353) done. Loss: 0.0459  lr:0.000000
[ Mon May 20 06:02:49 2024 ] 	Batch(1100/2353) done. Loss: 0.0199  lr:0.000000
[ Mon May 20 06:03:26 2024 ] 	Batch(1200/2353) done. Loss: 0.0021  lr:0.000000
[ Mon May 20 06:04:03 2024 ] 	Batch(1300/2353) done. Loss: 0.0062  lr:0.000000
[ Mon May 20 06:04:39 2024 ] 	Batch(1400/2353) done. Loss: 0.0289  lr:0.000000
[ Mon May 20 06:05:16 2024 ] 	Batch(1500/2353) done. Loss: 0.0106  lr:0.000000
[ Mon May 20 06:05:53 2024 ] 	Batch(1600/2353) done. Loss: 0.0360  lr:0.000000
[ Mon May 20 06:06:29 2024 ] 	Batch(1700/2353) done. Loss: 0.0729  lr:0.000000
[ Mon May 20 06:07:06 2024 ] 	Batch(1800/2353) done. Loss: 0.0386  lr:0.000000
[ Mon May 20 06:07:42 2024 ] 	Batch(1900/2353) done. Loss: 0.0274  lr:0.000000
[ Mon May 20 06:08:19 2024 ] 	Batch(2000/2353) done. Loss: 0.1039  lr:0.000000
[ Mon May 20 06:08:56 2024 ] 	Batch(2100/2353) done. Loss: 0.0356  lr:0.000000
[ Mon May 20 06:09:32 2024 ] 	Batch(2200/2353) done. Loss: 0.0742  lr:0.000000
[ Mon May 20 06:10:09 2024 ] 	Batch(2300/2353) done. Loss: 0.0051  lr:0.000000
[ Mon May 20 06:10:28 2024 ] 	Mean training loss: 0.0373.
[ Mon May 20 06:10:28 2024 ] 	Time consumption: [Data]01%, [Network]93%
[ Mon May 20 06:10:28 2024 ] Training epoch: 87
[ Mon May 20 06:10:29 2024 ] 	Batch(0/2353) done. Loss: 0.0650  lr:0.000000
[ Mon May 20 06:11:05 2024 ] 	Batch(100/2353) done. Loss: 0.0068  lr:0.000000
[ Mon May 20 06:11:42 2024 ] 	Batch(200/2353) done. Loss: 0.0382  lr:0.000000
[ Mon May 20 06:12:19 2024 ] 	Batch(300/2353) done. Loss: 0.0084  lr:0.000000
[ Mon May 20 06:12:55 2024 ] 	Batch(400/2353) done. Loss: 0.0113  lr:0.000000
[ Mon May 20 06:13:32 2024 ] 	Batch(500/2353) done. Loss: 0.0091  lr:0.000000
[ Mon May 20 06:14:09 2024 ] 	Batch(600/2353) done. Loss: 0.0384  lr:0.000000
[ Mon May 20 06:14:45 2024 ] 	Batch(700/2353) done. Loss: 0.0577  lr:0.000000
[ Mon May 20 06:15:22 2024 ] 	Batch(800/2353) done. Loss: 0.0540  lr:0.000000
[ Mon May 20 06:15:58 2024 ] 	Batch(900/2353) done. Loss: 0.0567  lr:0.000000
[ Mon May 20 06:16:35 2024 ] 	Batch(1000/2353) done. Loss: 0.0504  lr:0.000000
[ Mon May 20 06:17:12 2024 ] 	Batch(1100/2353) done. Loss: 0.0067  lr:0.000000
[ Mon May 20 06:17:48 2024 ] 	Batch(1200/2353) done. Loss: 0.0290  lr:0.000000
[ Mon May 20 06:18:25 2024 ] 	Batch(1300/2353) done. Loss: 0.0198  lr:0.000000
[ Mon May 20 06:19:02 2024 ] 	Batch(1400/2353) done. Loss: 0.0045  lr:0.000000
[ Mon May 20 06:19:38 2024 ] 	Batch(1500/2353) done. Loss: 0.1717  lr:0.000000
[ Mon May 20 06:20:15 2024 ] 	Batch(1600/2353) done. Loss: 0.0507  lr:0.000000
[ Mon May 20 06:20:52 2024 ] 	Batch(1700/2353) done. Loss: 0.1979  lr:0.000000
[ Mon May 20 06:21:28 2024 ] 	Batch(1800/2353) done. Loss: 0.0043  lr:0.000000
[ Mon May 20 06:22:05 2024 ] 	Batch(1900/2353) done. Loss: 0.0460  lr:0.000000
[ Mon May 20 06:22:42 2024 ] 	Batch(2000/2353) done. Loss: 0.0357  lr:0.000000
[ Mon May 20 06:23:18 2024 ] 	Batch(2100/2353) done. Loss: 0.0517  lr:0.000000
[ Mon May 20 06:23:55 2024 ] 	Batch(2200/2353) done. Loss: 0.1652  lr:0.000000
[ Mon May 20 06:24:31 2024 ] 	Batch(2300/2353) done. Loss: 0.1117  lr:0.000000
[ Mon May 20 06:24:51 2024 ] 	Mean training loss: 0.0367.
[ Mon May 20 06:24:51 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 06:24:51 2024 ] Training epoch: 88
[ Mon May 20 06:24:51 2024 ] 	Batch(0/2353) done. Loss: 0.0673  lr:0.000000
[ Mon May 20 06:25:28 2024 ] 	Batch(100/2353) done. Loss: 0.1251  lr:0.000000
[ Mon May 20 06:26:05 2024 ] 	Batch(200/2353) done. Loss: 0.0610  lr:0.000000
[ Mon May 20 06:26:42 2024 ] 	Batch(300/2353) done. Loss: 0.0385  lr:0.000000
[ Mon May 20 06:27:19 2024 ] 	Batch(400/2353) done. Loss: 0.0214  lr:0.000000
[ Mon May 20 06:27:55 2024 ] 	Batch(500/2353) done. Loss: 0.1049  lr:0.000000
[ Mon May 20 06:28:32 2024 ] 	Batch(600/2353) done. Loss: 0.1326  lr:0.000000
[ Mon May 20 06:29:09 2024 ] 	Batch(700/2353) done. Loss: 0.0342  lr:0.000000
[ Mon May 20 06:29:46 2024 ] 	Batch(800/2353) done. Loss: 0.0558  lr:0.000000
[ Mon May 20 06:30:23 2024 ] 	Batch(900/2353) done. Loss: 0.0312  lr:0.000000
[ Mon May 20 06:30:59 2024 ] 	Batch(1000/2353) done. Loss: 0.0810  lr:0.000000
[ Mon May 20 06:31:36 2024 ] 	Batch(1100/2353) done. Loss: 0.0112  lr:0.000000
[ Mon May 20 06:32:13 2024 ] 	Batch(1200/2353) done. Loss: 0.0910  lr:0.000000
[ Mon May 20 06:32:50 2024 ] 	Batch(1300/2353) done. Loss: 0.0286  lr:0.000000
[ Mon May 20 06:33:27 2024 ] 	Batch(1400/2353) done. Loss: 0.0074  lr:0.000000
[ Mon May 20 06:34:04 2024 ] 	Batch(1500/2353) done. Loss: 0.0704  lr:0.000000
[ Mon May 20 06:34:40 2024 ] 	Batch(1600/2353) done. Loss: 0.0182  lr:0.000000
[ Mon May 20 06:35:17 2024 ] 	Batch(1700/2353) done. Loss: 0.0420  lr:0.000000
[ Mon May 20 06:35:54 2024 ] 	Batch(1800/2353) done. Loss: 0.0864  lr:0.000000
[ Mon May 20 06:36:31 2024 ] 	Batch(1900/2353) done. Loss: 0.0594  lr:0.000000
[ Mon May 20 06:37:08 2024 ] 	Batch(2000/2353) done. Loss: 0.0088  lr:0.000000
[ Mon May 20 06:37:44 2024 ] 	Batch(2100/2353) done. Loss: 0.1275  lr:0.000000
[ Mon May 20 06:38:21 2024 ] 	Batch(2200/2353) done. Loss: 0.0330  lr:0.000000
[ Mon May 20 06:38:58 2024 ] 	Batch(2300/2353) done. Loss: 0.0268  lr:0.000000
[ Mon May 20 06:39:17 2024 ] 	Mean training loss: 0.0378.
[ Mon May 20 06:39:17 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 06:39:17 2024 ] Training epoch: 89
[ Mon May 20 06:39:18 2024 ] 	Batch(0/2353) done. Loss: 0.0051  lr:0.000000
[ Mon May 20 06:39:54 2024 ] 	Batch(100/2353) done. Loss: 0.0223  lr:0.000000
[ Mon May 20 06:40:31 2024 ] 	Batch(200/2353) done. Loss: 0.0065  lr:0.000000
[ Mon May 20 06:41:08 2024 ] 	Batch(300/2353) done. Loss: 0.0018  lr:0.000000
[ Mon May 20 06:41:44 2024 ] 	Batch(400/2353) done. Loss: 0.0080  lr:0.000000
[ Mon May 20 06:42:21 2024 ] 	Batch(500/2353) done. Loss: 0.0107  lr:0.000000
[ Mon May 20 06:42:58 2024 ] 	Batch(600/2353) done. Loss: 0.0066  lr:0.000000
[ Mon May 20 06:43:34 2024 ] 	Batch(700/2353) done. Loss: 0.0073  lr:0.000000
[ Mon May 20 06:44:11 2024 ] 	Batch(800/2353) done. Loss: 0.1311  lr:0.000000
[ Mon May 20 06:44:48 2024 ] 	Batch(900/2353) done. Loss: 0.0291  lr:0.000000
[ Mon May 20 06:45:24 2024 ] 	Batch(1000/2353) done. Loss: 0.0129  lr:0.000000
[ Mon May 20 06:46:01 2024 ] 	Batch(1100/2353) done. Loss: 0.0126  lr:0.000000
[ Mon May 20 06:46:38 2024 ] 	Batch(1200/2353) done. Loss: 0.0115  lr:0.000000
[ Mon May 20 06:47:14 2024 ] 	Batch(1300/2353) done. Loss: 0.0165  lr:0.000000
[ Mon May 20 06:47:51 2024 ] 	Batch(1400/2353) done. Loss: 0.2462  lr:0.000000
[ Mon May 20 06:48:28 2024 ] 	Batch(1500/2353) done. Loss: 0.0714  lr:0.000000
[ Mon May 20 06:49:04 2024 ] 	Batch(1600/2353) done. Loss: 0.0193  lr:0.000000
[ Mon May 20 06:49:41 2024 ] 	Batch(1700/2353) done. Loss: 0.0697  lr:0.000000
[ Mon May 20 06:50:18 2024 ] 	Batch(1800/2353) done. Loss: 0.0577  lr:0.000000
[ Mon May 20 06:50:54 2024 ] 	Batch(1900/2353) done. Loss: 0.0566  lr:0.000000
[ Mon May 20 06:51:31 2024 ] 	Batch(2000/2353) done. Loss: 0.0207  lr:0.000000
[ Mon May 20 06:52:08 2024 ] 	Batch(2100/2353) done. Loss: 0.0178  lr:0.000000
[ Mon May 20 06:52:44 2024 ] 	Batch(2200/2353) done. Loss: 0.0113  lr:0.000000
[ Mon May 20 06:53:21 2024 ] 	Batch(2300/2353) done. Loss: 0.0056  lr:0.000000
[ Mon May 20 06:53:40 2024 ] 	Mean training loss: 0.0374.
[ Mon May 20 06:53:40 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 06:53:40 2024 ] Training epoch: 90
[ Mon May 20 06:53:41 2024 ] 	Batch(0/2353) done. Loss: 0.0062  lr:0.000000
[ Mon May 20 06:54:18 2024 ] 	Batch(100/2353) done. Loss: 0.0148  lr:0.000000
[ Mon May 20 06:54:55 2024 ] 	Batch(200/2353) done. Loss: 0.0323  lr:0.000000
[ Mon May 20 06:55:31 2024 ] 	Batch(300/2353) done. Loss: 0.0465  lr:0.000000
[ Mon May 20 06:56:08 2024 ] 	Batch(400/2353) done. Loss: 0.0380  lr:0.000000
[ Mon May 20 06:56:45 2024 ] 	Batch(500/2353) done. Loss: 0.0085  lr:0.000000
[ Mon May 20 06:57:21 2024 ] 	Batch(600/2353) done. Loss: 0.0263  lr:0.000000
[ Mon May 20 06:57:58 2024 ] 	Batch(700/2353) done. Loss: 0.0430  lr:0.000000
[ Mon May 20 06:58:35 2024 ] 	Batch(800/2353) done. Loss: 0.0274  lr:0.000000
[ Mon May 20 06:59:11 2024 ] 	Batch(900/2353) done. Loss: 0.0978  lr:0.000000
[ Mon May 20 06:59:48 2024 ] 	Batch(1000/2353) done. Loss: 0.0026  lr:0.000000
[ Mon May 20 07:00:24 2024 ] 	Batch(1100/2353) done. Loss: 0.0456  lr:0.000000
[ Mon May 20 07:01:01 2024 ] 	Batch(1200/2353) done. Loss: 0.0144  lr:0.000000
[ Mon May 20 07:01:38 2024 ] 	Batch(1300/2353) done. Loss: 0.0045  lr:0.000000
[ Mon May 20 07:02:14 2024 ] 	Batch(1400/2353) done. Loss: 0.0099  lr:0.000000
[ Mon May 20 07:02:51 2024 ] 	Batch(1500/2353) done. Loss: 0.0072  lr:0.000000
[ Mon May 20 07:03:27 2024 ] 	Batch(1600/2353) done. Loss: 0.0302  lr:0.000000
[ Mon May 20 07:04:04 2024 ] 	Batch(1700/2353) done. Loss: 0.1350  lr:0.000000
[ Mon May 20 07:04:41 2024 ] 	Batch(1800/2353) done. Loss: 0.0203  lr:0.000000
[ Mon May 20 07:05:17 2024 ] 	Batch(1900/2353) done. Loss: 0.0232  lr:0.000000
[ Mon May 20 07:05:54 2024 ] 	Batch(2000/2353) done. Loss: 0.0057  lr:0.000000
[ Mon May 20 07:06:31 2024 ] 	Batch(2100/2353) done. Loss: 0.0032  lr:0.000000
[ Mon May 20 07:07:07 2024 ] 	Batch(2200/2353) done. Loss: 0.0238  lr:0.000000
[ Mon May 20 07:07:44 2024 ] 	Batch(2300/2353) done. Loss: 0.0036  lr:0.000000
[ Mon May 20 07:08:03 2024 ] 	Mean training loss: 0.0387.
[ Mon May 20 07:08:03 2024 ] 	Time consumption: [Data]01%, [Network]94%
[ Mon May 20 07:08:03 2024 ] Eval epoch: 90
[ Mon May 20 07:10:06 2024 ] 	Mean val loss of 2367 batches: 0.2472419485109101.
[ Mon May 20 07:10:06 2024 ] Training epoch: 91
[ Mon May 20 07:10:07 2024 ] 	Batch(0/2353) done. Loss: 0.1406  lr:0.000000
[ Mon May 20 07:10:44 2024 ] 	Batch(100/2353) done. Loss: 0.0139  lr:0.000000
[ Mon May 20 07:11:21 2024 ] 	Batch(200/2353) done. Loss: 0.0121  lr:0.000000
[ Mon May 20 07:11:58 2024 ] 	Batch(300/2353) done. Loss: 0.0968  lr:0.000000
[ Mon May 20 07:12:35 2024 ] 	Batch(400/2353) done. Loss: 0.0203  lr:0.000000
[ Mon May 20 07:13:11 2024 ] 	Batch(500/2353) done. Loss: 0.0220  lr:0.000000
[ Mon May 20 07:13:48 2024 ] 	Batch(600/2353) done. Loss: 0.0103  lr:0.000000
[ Mon May 20 07:14:24 2024 ] 	Batch(700/2353) done. Loss: 0.0142  lr:0.000000
[ Mon May 20 07:15:01 2024 ] 	Batch(800/2353) done. Loss: 0.0617  lr:0.000000
[ Mon May 20 07:15:38 2024 ] 	Batch(900/2353) done. Loss: 0.0120  lr:0.000000
[ Mon May 20 07:16:14 2024 ] 	Batch(1000/2353) done. Loss: 0.0636  lr:0.000000
[ Mon May 20 07:16:51 2024 ] 	Batch(1100/2353) done. Loss: 0.0211  lr:0.000000
[ Mon May 20 07:17:28 2024 ] 	Batch(1200/2353) done. Loss: 0.0733  lr:0.000000
[ Mon May 20 07:18:13 2024 ] Load weights from prova20/epoch79_model.pt.
[ Mon May 20 07:18:13 2024 ] Eval epoch: 1
[ Mon May 20 07:20:19 2024 ] 	Mean test loss of 2367 batches: 0.2427752734055162.
[ Mon May 20 07:20:19 2024 ] 	Top1: 93.36%
[ Mon May 20 07:20:19 2024 ] 	Top5: 98.91%
[ Mon May 20 07:20:19 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': False, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/train_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/train_label.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': '../Output_skeletons_without_missing_skeletons/xview/val_data_joint_bones.npy', 'label_path': '../Output_skeletons_without_missing_skeletons/xview/val_label.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 60, 'channel': 6, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': False, 'only_attention': True, 'tcn_attention': True, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': True, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': 'prova20/epoch79_model.pt', 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': False, 'scheduler': 1, 'base_lr': 0.01, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 16, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': False, 'display_recall_precision': False}

Testing: Epoch [0/120], Samples [17674.0/18932], Loss: 0.2427752734055162, Testing Accuracy: 93.35516585675047
Accuracy of 1 : 271 / 316 = 85 %
Accuracy of 2 : 296 / 316 = 93 %
Accuracy of 3 : 305 / 316 = 96 %
Accuracy of 4 : 302 / 316 = 95 %
Accuracy of 5 : 302 / 316 = 95 %
Accuracy of 6 : 307 / 316 = 97 %
Accuracy of 7 : 304 / 315 = 96 %
Accuracy of 8 : 311 / 316 = 98 %
Accuracy of 9 : 293 / 316 = 92 %
Accuracy of 10 : 235 / 315 = 74 %
Accuracy of 11 : 205 / 315 = 65 %
Accuracy of 12 : 303 / 316 = 95 %
Accuracy of 13 : 310 / 316 = 98 %
Accuracy of 14 : 310 / 316 = 98 %
Accuracy of 15 : 279 / 315 = 88 %
Accuracy of 16 : 263 / 316 = 83 %
Accuracy of 17 : 292 / 316 = 92 %
Accuracy of 18 : 295 / 316 = 93 %
Accuracy of 19 : 310 / 315 = 98 %
Accuracy of 20 : 305 / 316 = 96 %
Accuracy of 21 : 311 / 316 = 98 %
Accuracy of 22 : 310 / 316 = 98 %
Accuracy of 23 : 303 / 316 = 95 %
Accuracy of 24 : 297 / 316 = 93 %
Accuracy of 25 : 309 / 316 = 97 %
Accuracy of 26 : 312 / 316 = 98 %
Accuracy of 27 : 292 / 316 = 92 %
Accuracy of 28 : 271 / 316 = 85 %
Accuracy of 29 : 249 / 316 = 78 %
Accuracy of 30 : 306 / 315 = 97 %
Accuracy of 31 : 300 / 316 = 94 %
Accuracy of 32 : 303 / 316 = 95 %
Accuracy of 33 : 275 / 316 = 87 %
Accuracy of 34 : 310 / 316 = 98 %
Accuracy of 35 : 302 / 316 = 95 %
Accuracy of 36 : 282 / 316 = 89 %
Accuracy of 37 : 309 / 316 = 97 %
Accuracy of 38 : 306 / 316 = 96 %
Accuracy of 39 : 300 / 312 = 96 %
Accuracy of 40 : 273 / 316 = 86 %
Accuracy of 41 : 310 / 316 = 98 %
Accuracy of 42 : 316 / 316 = 100 %
Accuracy of 43 : 273 / 316 = 86 %
Accuracy of 44 : 294 / 316 = 93 %
Accuracy of 45 : 290 / 316 = 91 %
Accuracy of 46 : 294 / 316 = 93 %
Accuracy of 47 : 289 / 316 = 91 %
Accuracy of 48 : 305 / 316 = 96 %
Accuracy of 49 : 295 / 313 = 94 %
Accuracy of 50 : 300 / 314 = 95 %
Accuracy of 51 : 303 / 315 = 96 %
Accuracy of 52 : 298 / 316 = 94 %
Accuracy of 53 : 305 / 314 = 97 %
Accuracy of 54 : 299 / 309 = 96 %
Accuracy of 55 : 299 / 316 = 94 %
Accuracy of 56 : 289 / 316 = 91 %
Accuracy of 57 : 301 / 316 = 95 %
Accuracy of 58 : 302 / 316 = 95 %
Accuracy of 59 : 297 / 313 = 94 %
Accuracy of 60 : 297 / 316 = 93 %

Testing: Epoch [0/120], Samples [17674.0/18932], Loss: 0.2427752734055162, Testing Accuracy: 93.35516585675047

