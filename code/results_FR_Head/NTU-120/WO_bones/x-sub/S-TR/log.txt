[ Tue Jul  2 21:17:54 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xsub/train_joint_120.npy', 'label_path': 'new_data_processed/xsub/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xsub/val_joint_120.npy', 'label_path': 'new_data_processed/xsub/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': True, 'only_attention': True, 'tcn_attention': False, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': True, 'scheduler': 1, 'base_lr': 0.01, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Tue Jul  2 21:17:54 2024 ] Training epoch: 1
[ Tue Jul  2 21:17:56 2024 ] 	Batch(0/7879) done. Loss: 6.2611  lr:0.010000
[ Tue Jul  2 21:18:13 2024 ] 	Batch(100/7879) done. Loss: 5.2991  lr:0.010000
[ Tue Jul  2 21:18:31 2024 ] 	Batch(200/7879) done. Loss: 5.1975  lr:0.010000
[ Tue Jul  2 21:18:48 2024 ] 	Batch(300/7879) done. Loss: 5.1574  lr:0.010000
[ Tue Jul  2 21:19:06 2024 ] 	Batch(400/7879) done. Loss: 5.0035  lr:0.010000
[ Tue Jul  2 21:19:23 2024 ] 
Training: Epoch [0/120], Step [499], Loss: 4.370076656341553, Training Accuracy: 2.7
[ Tue Jul  2 21:19:24 2024 ] 	Batch(500/7879) done. Loss: 4.8937  lr:0.010000
[ Tue Jul  2 21:19:41 2024 ] 	Batch(600/7879) done. Loss: 4.2608  lr:0.010000
[ Tue Jul  2 21:19:59 2024 ] 	Batch(700/7879) done. Loss: 4.4455  lr:0.010000
[ Tue Jul  2 21:20:17 2024 ] 	Batch(800/7879) done. Loss: 4.3352  lr:0.010000
[ Tue Jul  2 21:20:34 2024 ] 	Batch(900/7879) done. Loss: 3.8248  lr:0.010000
[ Tue Jul  2 21:20:52 2024 ] 
Training: Epoch [0/120], Step [999], Loss: 3.867431640625, Training Accuracy: 3.5374999999999996
[ Tue Jul  2 21:20:52 2024 ] 	Batch(1000/7879) done. Loss: 4.4700  lr:0.010000
[ Tue Jul  2 21:21:09 2024 ] 	Batch(1100/7879) done. Loss: 4.1018  lr:0.010000
[ Tue Jul  2 21:21:27 2024 ] 	Batch(1200/7879) done. Loss: 4.0350  lr:0.010000
[ Tue Jul  2 21:21:45 2024 ] 	Batch(1300/7879) done. Loss: 3.8675  lr:0.010000
[ Tue Jul  2 21:22:02 2024 ] 	Batch(1400/7879) done. Loss: 4.7532  lr:0.010000
[ Tue Jul  2 21:22:20 2024 ] 
Training: Epoch [0/120], Step [1499], Loss: 4.2077460289001465, Training Accuracy: 4.458333333333334
[ Tue Jul  2 21:22:20 2024 ] 	Batch(1500/7879) done. Loss: 3.6091  lr:0.010000
[ Tue Jul  2 21:22:38 2024 ] 	Batch(1600/7879) done. Loss: 3.9830  lr:0.010000
[ Tue Jul  2 21:22:55 2024 ] 	Batch(1700/7879) done. Loss: 4.2146  lr:0.010000
[ Tue Jul  2 21:23:13 2024 ] 	Batch(1800/7879) done. Loss: 3.7542  lr:0.010000
[ Tue Jul  2 21:23:31 2024 ] 	Batch(1900/7879) done. Loss: 3.7113  lr:0.010000
[ Tue Jul  2 21:23:48 2024 ] 
Training: Epoch [0/120], Step [1999], Loss: 3.4816651344299316, Training Accuracy: 5.41875
[ Tue Jul  2 21:23:48 2024 ] 	Batch(2000/7879) done. Loss: 4.7053  lr:0.010000
[ Tue Jul  2 21:24:06 2024 ] 	Batch(2100/7879) done. Loss: 3.5871  lr:0.010000
[ Tue Jul  2 21:24:23 2024 ] 	Batch(2200/7879) done. Loss: 3.6490  lr:0.010000
[ Tue Jul  2 21:24:41 2024 ] 	Batch(2300/7879) done. Loss: 3.9997  lr:0.010000
[ Tue Jul  2 21:24:58 2024 ] 	Batch(2400/7879) done. Loss: 3.9117  lr:0.010000
[ Tue Jul  2 21:25:15 2024 ] 
Training: Epoch [0/120], Step [2499], Loss: 3.84177827835083, Training Accuracy: 6.115
[ Tue Jul  2 21:25:15 2024 ] 	Batch(2500/7879) done. Loss: 3.4185  lr:0.010000
[ Tue Jul  2 21:25:33 2024 ] 	Batch(2600/7879) done. Loss: 3.4965  lr:0.010000
[ Tue Jul  2 21:25:50 2024 ] 	Batch(2700/7879) done. Loss: 4.3902  lr:0.010000
[ Tue Jul  2 21:26:08 2024 ] 	Batch(2800/7879) done. Loss: 3.5826  lr:0.010000
[ Tue Jul  2 21:26:26 2024 ] 	Batch(2900/7879) done. Loss: 3.6571  lr:0.010000
[ Tue Jul  2 21:26:44 2024 ] 
Training: Epoch [0/120], Step [2999], Loss: 3.7677221298217773, Training Accuracy: 6.729166666666667
[ Tue Jul  2 21:26:44 2024 ] 	Batch(3000/7879) done. Loss: 3.0094  lr:0.010000
[ Tue Jul  2 21:27:02 2024 ] 	Batch(3100/7879) done. Loss: 3.2816  lr:0.010000
[ Tue Jul  2 21:27:20 2024 ] 	Batch(3200/7879) done. Loss: 3.8537  lr:0.010000
[ Tue Jul  2 21:27:39 2024 ] 	Batch(3300/7879) done. Loss: 3.2454  lr:0.010000
[ Tue Jul  2 21:27:57 2024 ] 	Batch(3400/7879) done. Loss: 3.5071  lr:0.010000
[ Tue Jul  2 21:28:14 2024 ] 
Training: Epoch [0/120], Step [3499], Loss: 3.1130759716033936, Training Accuracy: 7.492857142857143
[ Tue Jul  2 21:28:15 2024 ] 	Batch(3500/7879) done. Loss: 4.5561  lr:0.010000
[ Tue Jul  2 21:28:33 2024 ] 	Batch(3600/7879) done. Loss: 2.7208  lr:0.010000
[ Tue Jul  2 21:28:50 2024 ] 	Batch(3700/7879) done. Loss: 3.3378  lr:0.010000
[ Tue Jul  2 21:29:08 2024 ] 	Batch(3800/7879) done. Loss: 3.8143  lr:0.010000
[ Tue Jul  2 21:29:25 2024 ] 	Batch(3900/7879) done. Loss: 3.0640  lr:0.010000
[ Tue Jul  2 21:29:42 2024 ] 
Training: Epoch [0/120], Step [3999], Loss: 3.086566686630249, Training Accuracy: 8.259374999999999
[ Tue Jul  2 21:29:43 2024 ] 	Batch(4000/7879) done. Loss: 3.5163  lr:0.010000
[ Tue Jul  2 21:30:00 2024 ] 	Batch(4100/7879) done. Loss: 2.6597  lr:0.010000
[ Tue Jul  2 21:30:18 2024 ] 	Batch(4200/7879) done. Loss: 3.4226  lr:0.010000
[ Tue Jul  2 21:30:35 2024 ] 	Batch(4300/7879) done. Loss: 2.7974  lr:0.010000
[ Tue Jul  2 21:30:53 2024 ] 	Batch(4400/7879) done. Loss: 3.8712  lr:0.010000
[ Tue Jul  2 21:31:10 2024 ] 
Training: Epoch [0/120], Step [4499], Loss: 3.5730793476104736, Training Accuracy: 9.177777777777777
[ Tue Jul  2 21:31:10 2024 ] 	Batch(4500/7879) done. Loss: 2.5777  lr:0.010000
[ Tue Jul  2 21:31:28 2024 ] 	Batch(4600/7879) done. Loss: 3.1873  lr:0.010000
[ Tue Jul  2 21:31:45 2024 ] 	Batch(4700/7879) done. Loss: 3.9192  lr:0.010000
[ Tue Jul  2 21:32:03 2024 ] 	Batch(4800/7879) done. Loss: 3.6468  lr:0.010000
[ Tue Jul  2 21:32:20 2024 ] 	Batch(4900/7879) done. Loss: 4.8208  lr:0.010000
[ Tue Jul  2 21:32:37 2024 ] 
Training: Epoch [0/120], Step [4999], Loss: 2.5625784397125244, Training Accuracy: 10.115
[ Tue Jul  2 21:32:37 2024 ] 	Batch(5000/7879) done. Loss: 3.3103  lr:0.010000
[ Tue Jul  2 21:32:54 2024 ] 	Batch(5100/7879) done. Loss: 4.2918  lr:0.010000
[ Tue Jul  2 21:33:12 2024 ] 	Batch(5200/7879) done. Loss: 3.2000  lr:0.010000
[ Tue Jul  2 21:33:29 2024 ] 	Batch(5300/7879) done. Loss: 3.4721  lr:0.010000
[ Tue Jul  2 21:33:46 2024 ] 	Batch(5400/7879) done. Loss: 3.4629  lr:0.010000
[ Tue Jul  2 21:34:03 2024 ] 
Training: Epoch [0/120], Step [5499], Loss: 2.6761927604675293, Training Accuracy: 11.056818181818182
[ Tue Jul  2 21:34:03 2024 ] 	Batch(5500/7879) done. Loss: 2.7374  lr:0.010000
[ Tue Jul  2 21:34:21 2024 ] 	Batch(5600/7879) done. Loss: 3.2324  lr:0.010000
[ Tue Jul  2 21:34:38 2024 ] 	Batch(5700/7879) done. Loss: 2.6583  lr:0.010000
[ Tue Jul  2 21:34:55 2024 ] 	Batch(5800/7879) done. Loss: 3.4286  lr:0.010000
[ Tue Jul  2 21:35:12 2024 ] 	Batch(5900/7879) done. Loss: 2.2844  lr:0.010000
[ Tue Jul  2 21:35:29 2024 ] 
Training: Epoch [0/120], Step [5999], Loss: 3.3301801681518555, Training Accuracy: 11.897916666666667
[ Tue Jul  2 21:35:29 2024 ] 	Batch(6000/7879) done. Loss: 3.2035  lr:0.010000
[ Tue Jul  2 21:35:46 2024 ] 	Batch(6100/7879) done. Loss: 2.5628  lr:0.010000
[ Tue Jul  2 21:36:04 2024 ] 	Batch(6200/7879) done. Loss: 2.2821  lr:0.010000
[ Tue Jul  2 21:36:21 2024 ] 	Batch(6300/7879) done. Loss: 3.9747  lr:0.010000
[ Tue Jul  2 21:36:38 2024 ] 	Batch(6400/7879) done. Loss: 1.9775  lr:0.010000
[ Tue Jul  2 21:36:55 2024 ] 
Training: Epoch [0/120], Step [6499], Loss: 3.157782793045044, Training Accuracy: 12.763461538461538
[ Tue Jul  2 21:36:55 2024 ] 	Batch(6500/7879) done. Loss: 3.5551  lr:0.010000
[ Tue Jul  2 21:37:12 2024 ] 	Batch(6600/7879) done. Loss: 2.7985  lr:0.010000
[ Tue Jul  2 21:37:29 2024 ] 	Batch(6700/7879) done. Loss: 3.3237  lr:0.010000
[ Tue Jul  2 21:37:47 2024 ] 	Batch(6800/7879) done. Loss: 2.3723  lr:0.010000
[ Tue Jul  2 21:38:04 2024 ] 	Batch(6900/7879) done. Loss: 2.6576  lr:0.010000
[ Tue Jul  2 21:38:21 2024 ] 
Training: Epoch [0/120], Step [6999], Loss: 3.070988893508911, Training Accuracy: 13.62142857142857
[ Tue Jul  2 21:38:21 2024 ] 	Batch(7000/7879) done. Loss: 2.7236  lr:0.010000
[ Tue Jul  2 21:38:38 2024 ] 	Batch(7100/7879) done. Loss: 2.4834  lr:0.010000
[ Tue Jul  2 21:38:55 2024 ] 	Batch(7200/7879) done. Loss: 3.0900  lr:0.010000
[ Tue Jul  2 21:39:13 2024 ] 	Batch(7300/7879) done. Loss: 3.6431  lr:0.010000
[ Tue Jul  2 21:39:30 2024 ] 	Batch(7400/7879) done. Loss: 2.7398  lr:0.010000
[ Tue Jul  2 21:39:47 2024 ] 
Training: Epoch [0/120], Step [7499], Loss: 2.5184714794158936, Training Accuracy: 14.475
[ Tue Jul  2 21:39:47 2024 ] 	Batch(7500/7879) done. Loss: 2.9820  lr:0.010000
[ Tue Jul  2 21:40:04 2024 ] 	Batch(7600/7879) done. Loss: 2.7635  lr:0.010000
[ Tue Jul  2 21:40:21 2024 ] 	Batch(7700/7879) done. Loss: 2.8944  lr:0.010000
[ Tue Jul  2 21:40:38 2024 ] 	Batch(7800/7879) done. Loss: 1.5322  lr:0.010000
[ Tue Jul  2 21:40:52 2024 ] 	Mean training loss: 3.5012.
[ Tue Jul  2 21:40:52 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Tue Jul  2 21:40:52 2024 ] Training epoch: 2
[ Tue Jul  2 21:40:53 2024 ] 	Batch(0/7879) done. Loss: 3.4496  lr:0.010000
[ Tue Jul  2 21:41:10 2024 ] 	Batch(100/7879) done. Loss: 2.9034  lr:0.010000
[ Tue Jul  2 21:41:28 2024 ] 	Batch(200/7879) done. Loss: 2.4489  lr:0.010000
[ Tue Jul  2 21:41:46 2024 ] 	Batch(300/7879) done. Loss: 2.0621  lr:0.010000
[ Tue Jul  2 21:42:04 2024 ] 	Batch(400/7879) done. Loss: 2.5408  lr:0.010000
[ Tue Jul  2 21:42:22 2024 ] 
Training: Epoch [1/120], Step [499], Loss: 2.661007881164551, Training Accuracy: 27.725
[ Tue Jul  2 21:42:22 2024 ] 	Batch(500/7879) done. Loss: 1.7895  lr:0.010000
[ Tue Jul  2 21:42:40 2024 ] 	Batch(600/7879) done. Loss: 2.5759  lr:0.010000
[ Tue Jul  2 21:42:58 2024 ] 	Batch(700/7879) done. Loss: 2.2277  lr:0.010000
[ Tue Jul  2 21:43:16 2024 ] 	Batch(800/7879) done. Loss: 2.6564  lr:0.010000
[ Tue Jul  2 21:43:34 2024 ] 	Batch(900/7879) done. Loss: 2.3660  lr:0.010000
[ Tue Jul  2 21:43:52 2024 ] 
Training: Epoch [1/120], Step [999], Loss: 2.551020622253418, Training Accuracy: 28.8625
[ Tue Jul  2 21:43:52 2024 ] 	Batch(1000/7879) done. Loss: 2.9393  lr:0.010000
[ Tue Jul  2 21:44:10 2024 ] 	Batch(1100/7879) done. Loss: 2.7992  lr:0.010000
[ Tue Jul  2 21:44:28 2024 ] 	Batch(1200/7879) done. Loss: 2.9452  lr:0.010000
[ Tue Jul  2 21:44:46 2024 ] 	Batch(1300/7879) done. Loss: 1.9850  lr:0.010000
[ Tue Jul  2 21:45:03 2024 ] 	Batch(1400/7879) done. Loss: 2.4888  lr:0.010000
[ Tue Jul  2 21:45:21 2024 ] 
Training: Epoch [1/120], Step [1499], Loss: 1.773829460144043, Training Accuracy: 29.5
[ Tue Jul  2 21:45:21 2024 ] 	Batch(1500/7879) done. Loss: 1.9272  lr:0.010000
[ Tue Jul  2 21:45:39 2024 ] 	Batch(1600/7879) done. Loss: 3.3045  lr:0.010000
[ Tue Jul  2 21:45:57 2024 ] 	Batch(1700/7879) done. Loss: 2.5259  lr:0.010000
[ Tue Jul  2 21:46:15 2024 ] 	Batch(1800/7879) done. Loss: 2.3301  lr:0.010000
[ Tue Jul  2 21:46:33 2024 ] 	Batch(1900/7879) done. Loss: 2.3373  lr:0.010000
[ Tue Jul  2 21:46:51 2024 ] 
Training: Epoch [1/120], Step [1999], Loss: 2.718728542327881, Training Accuracy: 30.23125
[ Tue Jul  2 21:46:51 2024 ] 	Batch(2000/7879) done. Loss: 2.1177  lr:0.010000
[ Tue Jul  2 21:47:09 2024 ] 	Batch(2100/7879) done. Loss: 2.9751  lr:0.010000
[ Tue Jul  2 21:47:27 2024 ] 	Batch(2200/7879) done. Loss: 2.2129  lr:0.010000
[ Tue Jul  2 21:47:44 2024 ] 	Batch(2300/7879) done. Loss: 2.5326  lr:0.010000
[ Tue Jul  2 21:48:02 2024 ] 	Batch(2400/7879) done. Loss: 2.9159  lr:0.010000
[ Tue Jul  2 21:48:20 2024 ] 
Training: Epoch [1/120], Step [2499], Loss: 1.888113021850586, Training Accuracy: 30.915
[ Tue Jul  2 21:48:20 2024 ] 	Batch(2500/7879) done. Loss: 2.1476  lr:0.010000
[ Tue Jul  2 21:48:38 2024 ] 	Batch(2600/7879) done. Loss: 2.4120  lr:0.010000
[ Tue Jul  2 21:48:56 2024 ] 	Batch(2700/7879) done. Loss: 2.3016  lr:0.010000
[ Tue Jul  2 21:49:15 2024 ] 	Batch(2800/7879) done. Loss: 2.5556  lr:0.010000
[ Tue Jul  2 21:49:32 2024 ] 	Batch(2900/7879) done. Loss: 2.2486  lr:0.010000
[ Tue Jul  2 21:49:50 2024 ] 
Training: Epoch [1/120], Step [2999], Loss: 1.6011351346969604, Training Accuracy: 31.645833333333336
[ Tue Jul  2 21:49:50 2024 ] 	Batch(3000/7879) done. Loss: 1.8358  lr:0.010000
[ Tue Jul  2 21:50:08 2024 ] 	Batch(3100/7879) done. Loss: 2.6081  lr:0.010000
[ Tue Jul  2 21:50:26 2024 ] 	Batch(3200/7879) done. Loss: 2.3463  lr:0.010000
[ Tue Jul  2 21:50:45 2024 ] 	Batch(3300/7879) done. Loss: 1.7888  lr:0.010000
[ Tue Jul  2 21:51:03 2024 ] 	Batch(3400/7879) done. Loss: 2.0157  lr:0.010000
[ Tue Jul  2 21:51:22 2024 ] 
Training: Epoch [1/120], Step [3499], Loss: 2.657519578933716, Training Accuracy: 32.41428571428571
[ Tue Jul  2 21:51:22 2024 ] 	Batch(3500/7879) done. Loss: 1.7479  lr:0.010000
[ Tue Jul  2 21:51:41 2024 ] 	Batch(3600/7879) done. Loss: 3.1089  lr:0.010000
[ Tue Jul  2 21:51:59 2024 ] 	Batch(3700/7879) done. Loss: 2.0111  lr:0.010000
[ Tue Jul  2 21:52:18 2024 ] 	Batch(3800/7879) done. Loss: 3.5518  lr:0.010000
[ Tue Jul  2 21:52:36 2024 ] 	Batch(3900/7879) done. Loss: 1.5843  lr:0.010000
[ Tue Jul  2 21:52:53 2024 ] 
Training: Epoch [1/120], Step [3999], Loss: 2.213578939437866, Training Accuracy: 33.00625
[ Tue Jul  2 21:52:54 2024 ] 	Batch(4000/7879) done. Loss: 2.3012  lr:0.010000
[ Tue Jul  2 21:53:12 2024 ] 	Batch(4100/7879) done. Loss: 1.7616  lr:0.010000
[ Tue Jul  2 21:53:31 2024 ] 	Batch(4200/7879) done. Loss: 1.3949  lr:0.010000
[ Tue Jul  2 21:53:49 2024 ] 	Batch(4300/7879) done. Loss: 2.2202  lr:0.010000
[ Tue Jul  2 21:54:08 2024 ] 	Batch(4400/7879) done. Loss: 2.9816  lr:0.010000
[ Tue Jul  2 21:54:26 2024 ] 
Training: Epoch [1/120], Step [4499], Loss: 3.0667171478271484, Training Accuracy: 33.38888888888889
[ Tue Jul  2 21:54:26 2024 ] 	Batch(4500/7879) done. Loss: 2.2882  lr:0.010000
[ Tue Jul  2 21:54:44 2024 ] 	Batch(4600/7879) done. Loss: 1.7395  lr:0.010000
[ Tue Jul  2 21:55:02 2024 ] 	Batch(4700/7879) done. Loss: 2.1270  lr:0.010000
[ Tue Jul  2 21:55:20 2024 ] 	Batch(4800/7879) done. Loss: 1.8936  lr:0.010000
[ Tue Jul  2 21:55:38 2024 ] 	Batch(4900/7879) done. Loss: 1.5471  lr:0.010000
[ Tue Jul  2 21:55:55 2024 ] 
Training: Epoch [1/120], Step [4999], Loss: 2.754041910171509, Training Accuracy: 33.9325
[ Tue Jul  2 21:55:55 2024 ] 	Batch(5000/7879) done. Loss: 2.2589  lr:0.010000
[ Tue Jul  2 21:56:13 2024 ] 	Batch(5100/7879) done. Loss: 2.3424  lr:0.010000
[ Tue Jul  2 21:56:31 2024 ] 	Batch(5200/7879) done. Loss: 1.8416  lr:0.010000
[ Tue Jul  2 21:56:49 2024 ] 	Batch(5300/7879) done. Loss: 1.7207  lr:0.010000
[ Tue Jul  2 21:57:07 2024 ] 	Batch(5400/7879) done. Loss: 1.8365  lr:0.010000
[ Tue Jul  2 21:57:25 2024 ] 
Training: Epoch [1/120], Step [5499], Loss: 3.652113199234009, Training Accuracy: 34.33409090909091
[ Tue Jul  2 21:57:25 2024 ] 	Batch(5500/7879) done. Loss: 2.3751  lr:0.010000
[ Tue Jul  2 21:57:43 2024 ] 	Batch(5600/7879) done. Loss: 2.6281  lr:0.010000
[ Tue Jul  2 21:58:01 2024 ] 	Batch(5700/7879) done. Loss: 2.1656  lr:0.010000
[ Tue Jul  2 21:58:20 2024 ] 	Batch(5800/7879) done. Loss: 2.6951  lr:0.010000
[ Tue Jul  2 21:58:38 2024 ] 	Batch(5900/7879) done. Loss: 3.4133  lr:0.010000
[ Tue Jul  2 21:58:57 2024 ] 
Training: Epoch [1/120], Step [5999], Loss: 1.6154870986938477, Training Accuracy: 34.91458333333333
[ Tue Jul  2 21:58:57 2024 ] 	Batch(6000/7879) done. Loss: 1.8047  lr:0.010000
[ Tue Jul  2 21:59:15 2024 ] 	Batch(6100/7879) done. Loss: 1.7080  lr:0.010000
[ Tue Jul  2 21:59:33 2024 ] 	Batch(6200/7879) done. Loss: 2.1250  lr:0.010000
[ Tue Jul  2 21:59:51 2024 ] 	Batch(6300/7879) done. Loss: 1.4792  lr:0.010000
[ Tue Jul  2 22:00:09 2024 ] 	Batch(6400/7879) done. Loss: 3.0165  lr:0.010000
[ Tue Jul  2 22:00:26 2024 ] 
Training: Epoch [1/120], Step [6499], Loss: 2.688443899154663, Training Accuracy: 35.375
[ Tue Jul  2 22:00:27 2024 ] 	Batch(6500/7879) done. Loss: 2.6756  lr:0.010000
[ Tue Jul  2 22:00:44 2024 ] 	Batch(6600/7879) done. Loss: 2.9010  lr:0.010000
[ Tue Jul  2 22:01:02 2024 ] 	Batch(6700/7879) done. Loss: 3.5108  lr:0.010000
[ Tue Jul  2 22:01:20 2024 ] 	Batch(6800/7879) done. Loss: 1.7668  lr:0.010000
[ Tue Jul  2 22:01:39 2024 ] 	Batch(6900/7879) done. Loss: 1.9837  lr:0.010000
[ Tue Jul  2 22:01:57 2024 ] 
Training: Epoch [1/120], Step [6999], Loss: 3.1361887454986572, Training Accuracy: 35.86607142857143
[ Tue Jul  2 22:01:57 2024 ] 	Batch(7000/7879) done. Loss: 1.9093  lr:0.010000
[ Tue Jul  2 22:02:16 2024 ] 	Batch(7100/7879) done. Loss: 1.2925  lr:0.010000
[ Tue Jul  2 22:02:34 2024 ] 	Batch(7200/7879) done. Loss: 2.3082  lr:0.010000
[ Tue Jul  2 22:02:52 2024 ] 	Batch(7300/7879) done. Loss: 2.1258  lr:0.010000
[ Tue Jul  2 22:03:10 2024 ] 	Batch(7400/7879) done. Loss: 2.2735  lr:0.010000
[ Tue Jul  2 22:03:28 2024 ] 
Training: Epoch [1/120], Step [7499], Loss: 1.9660558700561523, Training Accuracy: 36.40333333333333
[ Tue Jul  2 22:03:28 2024 ] 	Batch(7500/7879) done. Loss: 1.4498  lr:0.010000
[ Tue Jul  2 22:03:46 2024 ] 	Batch(7600/7879) done. Loss: 2.1919  lr:0.010000
[ Tue Jul  2 22:04:04 2024 ] 	Batch(7700/7879) done. Loss: 1.9539  lr:0.010000
[ Tue Jul  2 22:04:22 2024 ] 	Batch(7800/7879) done. Loss: 2.8916  lr:0.010000
[ Tue Jul  2 22:04:36 2024 ] 	Mean training loss: 2.3416.
[ Tue Jul  2 22:04:36 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Tue Jul  2 22:04:36 2024 ] Training epoch: 3
[ Tue Jul  2 22:04:37 2024 ] 	Batch(0/7879) done. Loss: 2.0922  lr:0.010000
[ Tue Jul  2 22:04:55 2024 ] 	Batch(100/7879) done. Loss: 2.2622  lr:0.010000
[ Tue Jul  2 22:05:12 2024 ] 	Batch(200/7879) done. Loss: 1.5110  lr:0.010000
[ Tue Jul  2 22:05:30 2024 ] 	Batch(300/7879) done. Loss: 1.1941  lr:0.010000
[ Tue Jul  2 22:05:48 2024 ] 	Batch(400/7879) done. Loss: 2.2039  lr:0.010000
[ Tue Jul  2 22:06:06 2024 ] 
Training: Epoch [2/120], Step [499], Loss: 1.572126865386963, Training Accuracy: 43.675000000000004
[ Tue Jul  2 22:06:06 2024 ] 	Batch(500/7879) done. Loss: 1.9895  lr:0.010000
[ Tue Jul  2 22:06:24 2024 ] 	Batch(600/7879) done. Loss: 2.6254  lr:0.010000
[ Tue Jul  2 22:06:42 2024 ] 	Batch(700/7879) done. Loss: 1.7373  lr:0.010000
[ Tue Jul  2 22:07:00 2024 ] 	Batch(800/7879) done. Loss: 1.0464  lr:0.010000
[ Tue Jul  2 22:07:18 2024 ] 	Batch(900/7879) done. Loss: 0.4803  lr:0.010000
[ Tue Jul  2 22:07:36 2024 ] 
Training: Epoch [2/120], Step [999], Loss: 1.5470939874649048, Training Accuracy: 44.6375
[ Tue Jul  2 22:07:36 2024 ] 	Batch(1000/7879) done. Loss: 1.3658  lr:0.010000
[ Tue Jul  2 22:07:54 2024 ] 	Batch(1100/7879) done. Loss: 2.1625  lr:0.010000
[ Tue Jul  2 22:08:12 2024 ] 	Batch(1200/7879) done. Loss: 1.8921  lr:0.010000
[ Tue Jul  2 22:08:30 2024 ] 	Batch(1300/7879) done. Loss: 1.6075  lr:0.010000
[ Tue Jul  2 22:08:48 2024 ] 	Batch(1400/7879) done. Loss: 2.5987  lr:0.010000
[ Tue Jul  2 22:09:05 2024 ] 
Training: Epoch [2/120], Step [1499], Loss: 1.7018847465515137, Training Accuracy: 45.49166666666667
[ Tue Jul  2 22:09:06 2024 ] 	Batch(1500/7879) done. Loss: 1.0878  lr:0.010000
[ Tue Jul  2 22:09:24 2024 ] 	Batch(1600/7879) done. Loss: 0.6941  lr:0.010000
[ Tue Jul  2 22:09:41 2024 ] 	Batch(1700/7879) done. Loss: 1.4389  lr:0.010000
[ Tue Jul  2 22:09:59 2024 ] 	Batch(1800/7879) done. Loss: 2.7414  lr:0.010000
[ Tue Jul  2 22:10:17 2024 ] 	Batch(1900/7879) done. Loss: 1.6226  lr:0.010000
[ Tue Jul  2 22:10:35 2024 ] 
Training: Epoch [2/120], Step [1999], Loss: 1.9860798120498657, Training Accuracy: 46.2375
[ Tue Jul  2 22:10:35 2024 ] 	Batch(2000/7879) done. Loss: 1.5599  lr:0.010000
[ Tue Jul  2 22:10:54 2024 ] 	Batch(2100/7879) done. Loss: 1.6684  lr:0.010000
[ Tue Jul  2 22:11:12 2024 ] 	Batch(2200/7879) done. Loss: 2.1359  lr:0.010000
[ Tue Jul  2 22:11:31 2024 ] 	Batch(2300/7879) done. Loss: 1.3482  lr:0.010000
[ Tue Jul  2 22:11:49 2024 ] 	Batch(2400/7879) done. Loss: 2.1454  lr:0.010000
[ Tue Jul  2 22:12:07 2024 ] 
Training: Epoch [2/120], Step [2499], Loss: 1.7532752752304077, Training Accuracy: 46.595
[ Tue Jul  2 22:12:07 2024 ] 	Batch(2500/7879) done. Loss: 1.7489  lr:0.010000
[ Tue Jul  2 22:12:25 2024 ] 	Batch(2600/7879) done. Loss: 2.1205  lr:0.010000
[ Tue Jul  2 22:12:43 2024 ] 	Batch(2700/7879) done. Loss: 1.4823  lr:0.010000
[ Tue Jul  2 22:13:01 2024 ] 	Batch(2800/7879) done. Loss: 2.1863  lr:0.010000
[ Tue Jul  2 22:13:19 2024 ] 	Batch(2900/7879) done. Loss: 1.0572  lr:0.010000
[ Tue Jul  2 22:13:37 2024 ] 
Training: Epoch [2/120], Step [2999], Loss: 1.7865930795669556, Training Accuracy: 47.05
[ Tue Jul  2 22:13:37 2024 ] 	Batch(3000/7879) done. Loss: 1.8124  lr:0.010000
[ Tue Jul  2 22:13:55 2024 ] 	Batch(3100/7879) done. Loss: 1.1287  lr:0.010000
[ Tue Jul  2 22:14:13 2024 ] 	Batch(3200/7879) done. Loss: 2.8482  lr:0.010000
[ Tue Jul  2 22:14:31 2024 ] 	Batch(3300/7879) done. Loss: 1.4606  lr:0.010000
[ Tue Jul  2 22:14:49 2024 ] 	Batch(3400/7879) done. Loss: 1.2980  lr:0.010000
[ Tue Jul  2 22:15:06 2024 ] 
Training: Epoch [2/120], Step [3499], Loss: 1.5210394859313965, Training Accuracy: 47.346428571428575
[ Tue Jul  2 22:15:07 2024 ] 	Batch(3500/7879) done. Loss: 1.5682  lr:0.010000
[ Tue Jul  2 22:15:25 2024 ] 	Batch(3600/7879) done. Loss: 2.0596  lr:0.010000
[ Tue Jul  2 22:15:42 2024 ] 	Batch(3700/7879) done. Loss: 1.4390  lr:0.010000
[ Tue Jul  2 22:16:00 2024 ] 	Batch(3800/7879) done. Loss: 1.2352  lr:0.010000
[ Tue Jul  2 22:16:18 2024 ] 	Batch(3900/7879) done. Loss: 2.4376  lr:0.010000
[ Tue Jul  2 22:16:36 2024 ] 
Training: Epoch [2/120], Step [3999], Loss: 1.4795211553573608, Training Accuracy: 47.5125
[ Tue Jul  2 22:16:36 2024 ] 	Batch(4000/7879) done. Loss: 1.7860  lr:0.010000
[ Tue Jul  2 22:16:54 2024 ] 	Batch(4100/7879) done. Loss: 2.6867  lr:0.010000
[ Tue Jul  2 22:17:12 2024 ] 	Batch(4200/7879) done. Loss: 1.0726  lr:0.010000
[ Tue Jul  2 22:17:30 2024 ] 	Batch(4300/7879) done. Loss: 2.4238  lr:0.010000
[ Tue Jul  2 22:17:48 2024 ] 	Batch(4400/7879) done. Loss: 1.8198  lr:0.010000
[ Tue Jul  2 22:18:07 2024 ] 
Training: Epoch [2/120], Step [4499], Loss: 1.9233406782150269, Training Accuracy: 47.825
[ Tue Jul  2 22:18:07 2024 ] 	Batch(4500/7879) done. Loss: 2.2124  lr:0.010000
[ Tue Jul  2 22:18:26 2024 ] 	Batch(4600/7879) done. Loss: 3.1881  lr:0.010000
[ Tue Jul  2 22:18:44 2024 ] 	Batch(4700/7879) done. Loss: 1.7800  lr:0.010000
[ Tue Jul  2 22:19:03 2024 ] 	Batch(4800/7879) done. Loss: 1.8704  lr:0.010000
[ Tue Jul  2 22:19:21 2024 ] 	Batch(4900/7879) done. Loss: 2.5927  lr:0.010000
[ Tue Jul  2 22:19:38 2024 ] 
Training: Epoch [2/120], Step [4999], Loss: 2.913867712020874, Training Accuracy: 48.055
[ Tue Jul  2 22:19:39 2024 ] 	Batch(5000/7879) done. Loss: 2.1686  lr:0.010000
[ Tue Jul  2 22:19:57 2024 ] 	Batch(5100/7879) done. Loss: 2.2048  lr:0.010000
[ Tue Jul  2 22:20:15 2024 ] 	Batch(5200/7879) done. Loss: 1.8739  lr:0.010000
[ Tue Jul  2 22:20:33 2024 ] 	Batch(5300/7879) done. Loss: 2.8573  lr:0.010000
[ Tue Jul  2 22:20:52 2024 ] 	Batch(5400/7879) done. Loss: 1.5272  lr:0.010000
[ Tue Jul  2 22:21:09 2024 ] 
Training: Epoch [2/120], Step [5499], Loss: 1.587641954421997, Training Accuracy: 48.32727272727273
[ Tue Jul  2 22:21:10 2024 ] 	Batch(5500/7879) done. Loss: 1.2388  lr:0.010000
[ Tue Jul  2 22:21:28 2024 ] 	Batch(5600/7879) done. Loss: 2.2067  lr:0.010000
[ Tue Jul  2 22:21:46 2024 ] 	Batch(5700/7879) done. Loss: 1.9997  lr:0.010000
[ Tue Jul  2 22:22:04 2024 ] 	Batch(5800/7879) done. Loss: 2.0146  lr:0.010000
[ Tue Jul  2 22:22:21 2024 ] 	Batch(5900/7879) done. Loss: 1.7710  lr:0.010000
[ Tue Jul  2 22:22:39 2024 ] 
Training: Epoch [2/120], Step [5999], Loss: 1.4325518608093262, Training Accuracy: 48.66875
[ Tue Jul  2 22:22:40 2024 ] 	Batch(6000/7879) done. Loss: 2.9026  lr:0.010000
[ Tue Jul  2 22:22:57 2024 ] 	Batch(6100/7879) done. Loss: 1.3631  lr:0.010000
[ Tue Jul  2 22:23:15 2024 ] 	Batch(6200/7879) done. Loss: 2.9844  lr:0.010000
[ Tue Jul  2 22:23:33 2024 ] 	Batch(6300/7879) done. Loss: 2.1386  lr:0.010000
[ Tue Jul  2 22:23:51 2024 ] 	Batch(6400/7879) done. Loss: 1.5109  lr:0.010000
[ Tue Jul  2 22:24:09 2024 ] 
Training: Epoch [2/120], Step [6499], Loss: 1.9922751188278198, Training Accuracy: 49.02884615384615
[ Tue Jul  2 22:24:10 2024 ] 	Batch(6500/7879) done. Loss: 1.4400  lr:0.010000
[ Tue Jul  2 22:24:28 2024 ] 	Batch(6600/7879) done. Loss: 1.4983  lr:0.010000
[ Tue Jul  2 22:24:45 2024 ] 	Batch(6700/7879) done. Loss: 0.8226  lr:0.010000
[ Tue Jul  2 22:25:04 2024 ] 	Batch(6800/7879) done. Loss: 2.5587  lr:0.010000
[ Tue Jul  2 22:25:21 2024 ] 	Batch(6900/7879) done. Loss: 1.6409  lr:0.010000
[ Tue Jul  2 22:25:39 2024 ] 
Training: Epoch [2/120], Step [6999], Loss: 2.78163480758667, Training Accuracy: 49.25178571428571
[ Tue Jul  2 22:25:39 2024 ] 	Batch(7000/7879) done. Loss: 2.9842  lr:0.010000
[ Tue Jul  2 22:25:57 2024 ] 	Batch(7100/7879) done. Loss: 1.5063  lr:0.010000
[ Tue Jul  2 22:26:15 2024 ] 	Batch(7200/7879) done. Loss: 2.9858  lr:0.010000
[ Tue Jul  2 22:26:33 2024 ] 	Batch(7300/7879) done. Loss: 2.1284  lr:0.010000
[ Tue Jul  2 22:26:51 2024 ] 	Batch(7400/7879) done. Loss: 2.3383  lr:0.010000
[ Tue Jul  2 22:27:09 2024 ] 
Training: Epoch [2/120], Step [7499], Loss: 1.2292561531066895, Training Accuracy: 49.49
[ Tue Jul  2 22:27:09 2024 ] 	Batch(7500/7879) done. Loss: 1.0377  lr:0.010000
[ Tue Jul  2 22:27:27 2024 ] 	Batch(7600/7879) done. Loss: 0.3699  lr:0.010000
[ Tue Jul  2 22:27:45 2024 ] 	Batch(7700/7879) done. Loss: 1.5021  lr:0.010000
[ Tue Jul  2 22:28:04 2024 ] 	Batch(7800/7879) done. Loss: 1.3745  lr:0.010000
[ Tue Jul  2 22:28:19 2024 ] 	Mean training loss: 1.7932.
[ Tue Jul  2 22:28:19 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Tue Jul  2 22:28:19 2024 ] Training epoch: 4
[ Tue Jul  2 22:28:19 2024 ] 	Batch(0/7879) done. Loss: 0.7304  lr:0.010000
[ Tue Jul  2 22:28:37 2024 ] 	Batch(100/7879) done. Loss: 1.4846  lr:0.010000
[ Tue Jul  2 22:28:55 2024 ] 	Batch(200/7879) done. Loss: 1.8142  lr:0.010000
[ Tue Jul  2 22:29:13 2024 ] 	Batch(300/7879) done. Loss: 1.1733  lr:0.010000
[ Tue Jul  2 22:29:31 2024 ] 	Batch(400/7879) done. Loss: 2.2220  lr:0.010000
[ Tue Jul  2 22:29:49 2024 ] 
Training: Epoch [3/120], Step [499], Loss: 1.2718011140823364, Training Accuracy: 54.50000000000001
[ Tue Jul  2 22:29:49 2024 ] 	Batch(500/7879) done. Loss: 1.2615  lr:0.010000
[ Tue Jul  2 22:30:07 2024 ] 	Batch(600/7879) done. Loss: 1.5273  lr:0.010000
[ Tue Jul  2 22:30:25 2024 ] 	Batch(700/7879) done. Loss: 1.2894  lr:0.010000
[ Tue Jul  2 22:30:43 2024 ] 	Batch(800/7879) done. Loss: 1.0823  lr:0.010000
[ Tue Jul  2 22:31:01 2024 ] 	Batch(900/7879) done. Loss: 1.4496  lr:0.010000
[ Tue Jul  2 22:31:18 2024 ] 
Training: Epoch [3/120], Step [999], Loss: 0.9252812266349792, Training Accuracy: 55.2125
[ Tue Jul  2 22:31:18 2024 ] 	Batch(1000/7879) done. Loss: 2.1150  lr:0.010000
[ Tue Jul  2 22:31:36 2024 ] 	Batch(1100/7879) done. Loss: 1.3652  lr:0.010000
[ Tue Jul  2 22:31:54 2024 ] 	Batch(1200/7879) done. Loss: 1.7707  lr:0.010000
[ Tue Jul  2 22:32:12 2024 ] 	Batch(1300/7879) done. Loss: 0.9113  lr:0.010000
[ Tue Jul  2 22:32:30 2024 ] 	Batch(1400/7879) done. Loss: 0.9060  lr:0.010000
[ Tue Jul  2 22:32:48 2024 ] 
Training: Epoch [3/120], Step [1499], Loss: 0.7340706586837769, Training Accuracy: 55.49166666666666
[ Tue Jul  2 22:32:48 2024 ] 	Batch(1500/7879) done. Loss: 1.1906  lr:0.010000
[ Tue Jul  2 22:33:06 2024 ] 	Batch(1600/7879) done. Loss: 1.2212  lr:0.010000
[ Tue Jul  2 22:33:24 2024 ] 	Batch(1700/7879) done. Loss: 1.4314  lr:0.010000
[ Tue Jul  2 22:33:42 2024 ] 	Batch(1800/7879) done. Loss: 2.1443  lr:0.010000
[ Tue Jul  2 22:34:00 2024 ] 	Batch(1900/7879) done. Loss: 2.4218  lr:0.010000
[ Tue Jul  2 22:34:18 2024 ] 
Training: Epoch [3/120], Step [1999], Loss: 2.155649423599243, Training Accuracy: 55.512499999999996
[ Tue Jul  2 22:34:18 2024 ] 	Batch(2000/7879) done. Loss: 0.5667  lr:0.010000
[ Tue Jul  2 22:34:36 2024 ] 	Batch(2100/7879) done. Loss: 1.2379  lr:0.010000
[ Tue Jul  2 22:34:54 2024 ] 	Batch(2200/7879) done. Loss: 0.9837  lr:0.010000
[ Tue Jul  2 22:35:12 2024 ] 	Batch(2300/7879) done. Loss: 1.1967  lr:0.010000
[ Tue Jul  2 22:35:30 2024 ] 	Batch(2400/7879) done. Loss: 0.8603  lr:0.010000
[ Tue Jul  2 22:35:48 2024 ] 
Training: Epoch [3/120], Step [2499], Loss: 2.4651806354522705, Training Accuracy: 55.645
[ Tue Jul  2 22:35:48 2024 ] 	Batch(2500/7879) done. Loss: 2.5621  lr:0.010000
[ Tue Jul  2 22:36:06 2024 ] 	Batch(2600/7879) done. Loss: 0.6826  lr:0.010000
[ Tue Jul  2 22:36:24 2024 ] 	Batch(2700/7879) done. Loss: 2.6227  lr:0.010000
[ Tue Jul  2 22:36:42 2024 ] 	Batch(2800/7879) done. Loss: 1.7325  lr:0.010000
[ Tue Jul  2 22:37:00 2024 ] 	Batch(2900/7879) done. Loss: 1.6302  lr:0.010000
[ Tue Jul  2 22:37:17 2024 ] 
Training: Epoch [3/120], Step [2999], Loss: 1.8329334259033203, Training Accuracy: 55.7
[ Tue Jul  2 22:37:17 2024 ] 	Batch(3000/7879) done. Loss: 2.3897  lr:0.010000
[ Tue Jul  2 22:37:35 2024 ] 	Batch(3100/7879) done. Loss: 3.0580  lr:0.010000
[ Tue Jul  2 22:37:53 2024 ] 	Batch(3200/7879) done. Loss: 1.8927  lr:0.010000
[ Tue Jul  2 22:38:11 2024 ] 	Batch(3300/7879) done. Loss: 2.6201  lr:0.010000
[ Tue Jul  2 22:38:29 2024 ] 	Batch(3400/7879) done. Loss: 2.1831  lr:0.010000
[ Tue Jul  2 22:38:47 2024 ] 
Training: Epoch [3/120], Step [3499], Loss: 1.6008622646331787, Training Accuracy: 55.82142857142857
[ Tue Jul  2 22:38:47 2024 ] 	Batch(3500/7879) done. Loss: 1.5103  lr:0.010000
[ Tue Jul  2 22:39:05 2024 ] 	Batch(3600/7879) done. Loss: 0.9625  lr:0.010000
[ Tue Jul  2 22:39:23 2024 ] 	Batch(3700/7879) done. Loss: 0.6259  lr:0.010000
[ Tue Jul  2 22:39:41 2024 ] 	Batch(3800/7879) done. Loss: 2.2171  lr:0.010000
[ Tue Jul  2 22:39:58 2024 ] 	Batch(3900/7879) done. Loss: 1.3907  lr:0.010000
[ Tue Jul  2 22:40:16 2024 ] 
Training: Epoch [3/120], Step [3999], Loss: 1.9246866703033447, Training Accuracy: 55.90625
[ Tue Jul  2 22:40:17 2024 ] 	Batch(4000/7879) done. Loss: 1.8013  lr:0.010000
[ Tue Jul  2 22:40:35 2024 ] 	Batch(4100/7879) done. Loss: 1.1450  lr:0.010000
[ Tue Jul  2 22:40:53 2024 ] 	Batch(4200/7879) done. Loss: 1.9606  lr:0.010000
[ Tue Jul  2 22:41:11 2024 ] 	Batch(4300/7879) done. Loss: 1.7287  lr:0.010000
[ Tue Jul  2 22:41:29 2024 ] 	Batch(4400/7879) done. Loss: 1.6842  lr:0.010000
[ Tue Jul  2 22:41:48 2024 ] 
Training: Epoch [3/120], Step [4499], Loss: 1.1996347904205322, Training Accuracy: 56.03055555555555
[ Tue Jul  2 22:41:48 2024 ] 	Batch(4500/7879) done. Loss: 1.3444  lr:0.010000
[ Tue Jul  2 22:42:07 2024 ] 	Batch(4600/7879) done. Loss: 0.6436  lr:0.010000
[ Tue Jul  2 22:42:25 2024 ] 	Batch(4700/7879) done. Loss: 1.7053  lr:0.010000
[ Tue Jul  2 22:42:43 2024 ] 	Batch(4800/7879) done. Loss: 1.6591  lr:0.010000
[ Tue Jul  2 22:43:01 2024 ] 	Batch(4900/7879) done. Loss: 1.5721  lr:0.010000
[ Tue Jul  2 22:43:19 2024 ] 
Training: Epoch [3/120], Step [4999], Loss: 1.464028000831604, Training Accuracy: 56.2175
[ Tue Jul  2 22:43:19 2024 ] 	Batch(5000/7879) done. Loss: 0.9108  lr:0.010000
[ Tue Jul  2 22:43:37 2024 ] 	Batch(5100/7879) done. Loss: 1.2643  lr:0.010000
[ Tue Jul  2 22:43:55 2024 ] 	Batch(5200/7879) done. Loss: 0.9308  lr:0.010000
[ Tue Jul  2 22:44:13 2024 ] 	Batch(5300/7879) done. Loss: 0.9296  lr:0.010000
[ Tue Jul  2 22:44:31 2024 ] 	Batch(5400/7879) done. Loss: 1.1349  lr:0.010000
[ Tue Jul  2 22:44:49 2024 ] 
Training: Epoch [3/120], Step [5499], Loss: 3.071241617202759, Training Accuracy: 56.42045454545455
[ Tue Jul  2 22:44:49 2024 ] 	Batch(5500/7879) done. Loss: 1.9642  lr:0.010000
[ Tue Jul  2 22:45:07 2024 ] 	Batch(5600/7879) done. Loss: 0.8340  lr:0.010000
[ Tue Jul  2 22:45:25 2024 ] 	Batch(5700/7879) done. Loss: 0.7294  lr:0.010000
[ Tue Jul  2 22:45:43 2024 ] 	Batch(5800/7879) done. Loss: 1.0014  lr:0.010000
[ Tue Jul  2 22:46:00 2024 ] 	Batch(5900/7879) done. Loss: 1.3925  lr:0.010000
[ Tue Jul  2 22:46:18 2024 ] 
Training: Epoch [3/120], Step [5999], Loss: 2.2909669876098633, Training Accuracy: 56.599999999999994
[ Tue Jul  2 22:46:19 2024 ] 	Batch(6000/7879) done. Loss: 1.9097  lr:0.010000
[ Tue Jul  2 22:46:36 2024 ] 	Batch(6100/7879) done. Loss: 1.1575  lr:0.010000
[ Tue Jul  2 22:46:54 2024 ] 	Batch(6200/7879) done. Loss: 0.6724  lr:0.010000
[ Tue Jul  2 22:47:12 2024 ] 	Batch(6300/7879) done. Loss: 1.1562  lr:0.010000
[ Tue Jul  2 22:47:30 2024 ] 	Batch(6400/7879) done. Loss: 1.1969  lr:0.010000
[ Tue Jul  2 22:47:48 2024 ] 
Training: Epoch [3/120], Step [6499], Loss: 1.8371347188949585, Training Accuracy: 56.753846153846155
[ Tue Jul  2 22:47:48 2024 ] 	Batch(6500/7879) done. Loss: 1.3462  lr:0.010000
[ Tue Jul  2 22:48:06 2024 ] 	Batch(6600/7879) done. Loss: 2.0787  lr:0.010000
[ Tue Jul  2 22:48:24 2024 ] 	Batch(6700/7879) done. Loss: 1.5408  lr:0.010000
[ Tue Jul  2 22:48:42 2024 ] 	Batch(6800/7879) done. Loss: 2.8009  lr:0.010000
[ Tue Jul  2 22:49:00 2024 ] 	Batch(6900/7879) done. Loss: 1.5292  lr:0.010000
[ Tue Jul  2 22:49:18 2024 ] 
Training: Epoch [3/120], Step [6999], Loss: 2.3150315284729004, Training Accuracy: 56.90357142857143
[ Tue Jul  2 22:49:18 2024 ] 	Batch(7000/7879) done. Loss: 1.2179  lr:0.010000
[ Tue Jul  2 22:49:36 2024 ] 	Batch(7100/7879) done. Loss: 1.3571  lr:0.010000
[ Tue Jul  2 22:49:54 2024 ] 	Batch(7200/7879) done. Loss: 1.6383  lr:0.010000
[ Tue Jul  2 22:50:12 2024 ] 	Batch(7300/7879) done. Loss: 1.3448  lr:0.010000
[ Tue Jul  2 22:50:29 2024 ] 	Batch(7400/7879) done. Loss: 1.1460  lr:0.010000
[ Tue Jul  2 22:50:47 2024 ] 
Training: Epoch [3/120], Step [7499], Loss: 0.8074362277984619, Training Accuracy: 57.17166666666667
[ Tue Jul  2 22:50:47 2024 ] 	Batch(7500/7879) done. Loss: 1.3862  lr:0.010000
[ Tue Jul  2 22:51:05 2024 ] 	Batch(7600/7879) done. Loss: 1.8747  lr:0.010000
[ Tue Jul  2 22:51:23 2024 ] 	Batch(7700/7879) done. Loss: 2.2415  lr:0.010000
[ Tue Jul  2 22:51:41 2024 ] 	Batch(7800/7879) done. Loss: 1.3053  lr:0.010000
[ Tue Jul  2 22:51:55 2024 ] 	Mean training loss: 1.5152.
[ Tue Jul  2 22:51:55 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Tue Jul  2 22:51:55 2024 ] Training epoch: 5
[ Tue Jul  2 22:51:56 2024 ] 	Batch(0/7879) done. Loss: 0.5553  lr:0.010000
[ Tue Jul  2 22:52:14 2024 ] 	Batch(100/7879) done. Loss: 0.9384  lr:0.010000
[ Tue Jul  2 22:52:32 2024 ] 	Batch(200/7879) done. Loss: 1.5607  lr:0.010000
[ Tue Jul  2 22:52:49 2024 ] 	Batch(300/7879) done. Loss: 0.9613  lr:0.010000
[ Tue Jul  2 22:53:07 2024 ] 	Batch(400/7879) done. Loss: 2.2022  lr:0.010000
[ Tue Jul  2 22:53:25 2024 ] 
Training: Epoch [4/120], Step [499], Loss: 1.3735640048980713, Training Accuracy: 59.9
[ Tue Jul  2 22:53:25 2024 ] 	Batch(500/7879) done. Loss: 1.2568  lr:0.010000
[ Tue Jul  2 22:53:43 2024 ] 	Batch(600/7879) done. Loss: 1.6306  lr:0.010000
[ Tue Jul  2 22:54:01 2024 ] 	Batch(700/7879) done. Loss: 1.5138  lr:0.010000
[ Tue Jul  2 22:54:19 2024 ] 	Batch(800/7879) done. Loss: 1.1688  lr:0.010000
[ Tue Jul  2 22:54:38 2024 ] 	Batch(900/7879) done. Loss: 1.6651  lr:0.010000
[ Tue Jul  2 22:54:56 2024 ] 
Training: Epoch [4/120], Step [999], Loss: 1.1615437269210815, Training Accuracy: 60.075
[ Tue Jul  2 22:54:56 2024 ] 	Batch(1000/7879) done. Loss: 0.7761  lr:0.010000
[ Tue Jul  2 22:55:15 2024 ] 	Batch(1100/7879) done. Loss: 1.0202  lr:0.010000
[ Tue Jul  2 22:55:33 2024 ] 	Batch(1200/7879) done. Loss: 1.0453  lr:0.010000
[ Tue Jul  2 22:55:51 2024 ] 	Batch(1300/7879) done. Loss: 2.0724  lr:0.010000
[ Tue Jul  2 22:56:09 2024 ] 	Batch(1400/7879) done. Loss: 1.0369  lr:0.010000
[ Tue Jul  2 22:56:27 2024 ] 
Training: Epoch [4/120], Step [1499], Loss: 1.681869626045227, Training Accuracy: 60.175
[ Tue Jul  2 22:56:27 2024 ] 	Batch(1500/7879) done. Loss: 1.0642  lr:0.010000
[ Tue Jul  2 22:56:45 2024 ] 	Batch(1600/7879) done. Loss: 1.1860  lr:0.010000
[ Tue Jul  2 22:57:03 2024 ] 	Batch(1700/7879) done. Loss: 1.8312  lr:0.010000
[ Tue Jul  2 22:57:21 2024 ] 	Batch(1800/7879) done. Loss: 1.4216  lr:0.010000
[ Tue Jul  2 22:57:39 2024 ] 	Batch(1900/7879) done. Loss: 0.7674  lr:0.010000
[ Tue Jul  2 22:57:57 2024 ] 
Training: Epoch [4/120], Step [1999], Loss: 2.6441328525543213, Training Accuracy: 60.650000000000006
[ Tue Jul  2 22:57:57 2024 ] 	Batch(2000/7879) done. Loss: 0.9808  lr:0.010000
[ Tue Jul  2 22:58:15 2024 ] 	Batch(2100/7879) done. Loss: 1.7612  lr:0.010000
[ Tue Jul  2 22:58:33 2024 ] 	Batch(2200/7879) done. Loss: 1.4192  lr:0.010000
[ Tue Jul  2 22:58:51 2024 ] 	Batch(2300/7879) done. Loss: 1.5507  lr:0.010000
[ Tue Jul  2 22:59:09 2024 ] 	Batch(2400/7879) done. Loss: 1.8678  lr:0.010000
[ Tue Jul  2 22:59:26 2024 ] 
Training: Epoch [4/120], Step [2499], Loss: 0.5646815299987793, Training Accuracy: 60.62
[ Tue Jul  2 22:59:26 2024 ] 	Batch(2500/7879) done. Loss: 1.2334  lr:0.010000
[ Tue Jul  2 22:59:44 2024 ] 	Batch(2600/7879) done. Loss: 0.7234  lr:0.010000
[ Tue Jul  2 23:00:02 2024 ] 	Batch(2700/7879) done. Loss: 1.3697  lr:0.010000
[ Tue Jul  2 23:00:20 2024 ] 	Batch(2800/7879) done. Loss: 0.6263  lr:0.010000
[ Tue Jul  2 23:00:38 2024 ] 	Batch(2900/7879) done. Loss: 1.3624  lr:0.010000
[ Tue Jul  2 23:00:56 2024 ] 
Training: Epoch [4/120], Step [2999], Loss: 1.5986701250076294, Training Accuracy: 60.8125
[ Tue Jul  2 23:00:56 2024 ] 	Batch(3000/7879) done. Loss: 2.5830  lr:0.010000
[ Tue Jul  2 23:01:14 2024 ] 	Batch(3100/7879) done. Loss: 1.1379  lr:0.010000
[ Tue Jul  2 23:01:32 2024 ] 	Batch(3200/7879) done. Loss: 0.6732  lr:0.010000
[ Tue Jul  2 23:01:50 2024 ] 	Batch(3300/7879) done. Loss: 0.8167  lr:0.010000
[ Tue Jul  2 23:02:08 2024 ] 	Batch(3400/7879) done. Loss: 1.4068  lr:0.010000
[ Tue Jul  2 23:02:26 2024 ] 
Training: Epoch [4/120], Step [3499], Loss: 0.5386999845504761, Training Accuracy: 60.878571428571426
[ Tue Jul  2 23:02:26 2024 ] 	Batch(3500/7879) done. Loss: 0.8296  lr:0.010000
[ Tue Jul  2 23:02:44 2024 ] 	Batch(3600/7879) done. Loss: 1.2989  lr:0.010000
[ Tue Jul  2 23:03:02 2024 ] 	Batch(3700/7879) done. Loss: 0.9286  lr:0.010000
[ Tue Jul  2 23:03:20 2024 ] 	Batch(3800/7879) done. Loss: 0.9982  lr:0.010000
[ Tue Jul  2 23:03:38 2024 ] 	Batch(3900/7879) done. Loss: 0.5025  lr:0.010000
[ Tue Jul  2 23:03:56 2024 ] 
Training: Epoch [4/120], Step [3999], Loss: 1.5113637447357178, Training Accuracy: 61.0125
[ Tue Jul  2 23:03:56 2024 ] 	Batch(4000/7879) done. Loss: 1.6562  lr:0.010000
[ Tue Jul  2 23:04:14 2024 ] 	Batch(4100/7879) done. Loss: 3.0096  lr:0.010000
[ Tue Jul  2 23:04:32 2024 ] 	Batch(4200/7879) done. Loss: 1.9544  lr:0.010000
[ Tue Jul  2 23:04:50 2024 ] 	Batch(4300/7879) done. Loss: 1.1107  lr:0.010000
[ Tue Jul  2 23:05:08 2024 ] 	Batch(4400/7879) done. Loss: 1.3736  lr:0.010000
[ Tue Jul  2 23:05:26 2024 ] 
Training: Epoch [4/120], Step [4499], Loss: 1.6176822185516357, Training Accuracy: 61.15277777777778
[ Tue Jul  2 23:05:26 2024 ] 	Batch(4500/7879) done. Loss: 1.6227  lr:0.010000
[ Tue Jul  2 23:05:44 2024 ] 	Batch(4600/7879) done. Loss: 0.6890  lr:0.010000
[ Tue Jul  2 23:06:02 2024 ] 	Batch(4700/7879) done. Loss: 0.6111  lr:0.010000
[ Tue Jul  2 23:06:20 2024 ] 	Batch(4800/7879) done. Loss: 2.1194  lr:0.010000
[ Tue Jul  2 23:06:38 2024 ] 	Batch(4900/7879) done. Loss: 1.0574  lr:0.010000
[ Tue Jul  2 23:06:56 2024 ] 
Training: Epoch [4/120], Step [4999], Loss: 1.0011706352233887, Training Accuracy: 61.2575
[ Tue Jul  2 23:06:56 2024 ] 	Batch(5000/7879) done. Loss: 2.4232  lr:0.010000
[ Tue Jul  2 23:07:14 2024 ] 	Batch(5100/7879) done. Loss: 1.2352  lr:0.010000
[ Tue Jul  2 23:07:32 2024 ] 	Batch(5200/7879) done. Loss: 0.9426  lr:0.010000
[ Tue Jul  2 23:07:50 2024 ] 	Batch(5300/7879) done. Loss: 0.7005  lr:0.010000
[ Tue Jul  2 23:08:08 2024 ] 	Batch(5400/7879) done. Loss: 1.3031  lr:0.010000
[ Tue Jul  2 23:08:26 2024 ] 
Training: Epoch [4/120], Step [5499], Loss: 1.314064383506775, Training Accuracy: 61.28181818181818
[ Tue Jul  2 23:08:26 2024 ] 	Batch(5500/7879) done. Loss: 0.2530  lr:0.010000
[ Tue Jul  2 23:08:44 2024 ] 	Batch(5600/7879) done. Loss: 1.6182  lr:0.010000
[ Tue Jul  2 23:09:02 2024 ] 	Batch(5700/7879) done. Loss: 0.7922  lr:0.010000
[ Tue Jul  2 23:09:20 2024 ] 	Batch(5800/7879) done. Loss: 2.0855  lr:0.010000
[ Tue Jul  2 23:09:38 2024 ] 	Batch(5900/7879) done. Loss: 0.4995  lr:0.010000
[ Tue Jul  2 23:09:56 2024 ] 
Training: Epoch [4/120], Step [5999], Loss: 1.0027512311935425, Training Accuracy: 61.55208333333333
[ Tue Jul  2 23:09:56 2024 ] 	Batch(6000/7879) done. Loss: 1.1556  lr:0.010000
[ Tue Jul  2 23:10:15 2024 ] 	Batch(6100/7879) done. Loss: 3.1972  lr:0.010000
[ Tue Jul  2 23:10:33 2024 ] 	Batch(6200/7879) done. Loss: 0.5705  lr:0.010000
[ Tue Jul  2 23:10:51 2024 ] 	Batch(6300/7879) done. Loss: 2.3636  lr:0.010000
[ Tue Jul  2 23:11:09 2024 ] 	Batch(6400/7879) done. Loss: 1.7651  lr:0.010000
[ Tue Jul  2 23:11:27 2024 ] 
Training: Epoch [4/120], Step [6499], Loss: 1.2919905185699463, Training Accuracy: 61.65384615384616
[ Tue Jul  2 23:11:27 2024 ] 	Batch(6500/7879) done. Loss: 1.8290  lr:0.010000
[ Tue Jul  2 23:11:45 2024 ] 	Batch(6600/7879) done. Loss: 1.6560  lr:0.010000
[ Tue Jul  2 23:12:03 2024 ] 	Batch(6700/7879) done. Loss: 0.5820  lr:0.010000
[ Tue Jul  2 23:12:21 2024 ] 	Batch(6800/7879) done. Loss: 1.4163  lr:0.010000
[ Tue Jul  2 23:12:39 2024 ] 	Batch(6900/7879) done. Loss: 2.0163  lr:0.010000
[ Tue Jul  2 23:12:56 2024 ] 
Training: Epoch [4/120], Step [6999], Loss: 1.7974138259887695, Training Accuracy: 61.74821428571428
[ Tue Jul  2 23:12:57 2024 ] 	Batch(7000/7879) done. Loss: 0.6990  lr:0.010000
[ Tue Jul  2 23:13:14 2024 ] 	Batch(7100/7879) done. Loss: 0.6445  lr:0.010000
[ Tue Jul  2 23:13:32 2024 ] 	Batch(7200/7879) done. Loss: 0.6968  lr:0.010000
[ Tue Jul  2 23:13:50 2024 ] 	Batch(7300/7879) done. Loss: 0.2547  lr:0.010000
[ Tue Jul  2 23:14:08 2024 ] 	Batch(7400/7879) done. Loss: 1.5766  lr:0.010000
[ Tue Jul  2 23:14:26 2024 ] 
Training: Epoch [4/120], Step [7499], Loss: 3.51314640045166, Training Accuracy: 61.815
[ Tue Jul  2 23:14:26 2024 ] 	Batch(7500/7879) done. Loss: 2.0050  lr:0.010000
[ Tue Jul  2 23:14:44 2024 ] 	Batch(7600/7879) done. Loss: 0.8513  lr:0.010000
[ Tue Jul  2 23:15:02 2024 ] 	Batch(7700/7879) done. Loss: 1.6276  lr:0.010000
[ Tue Jul  2 23:15:20 2024 ] 	Batch(7800/7879) done. Loss: 0.7135  lr:0.010000
[ Tue Jul  2 23:15:34 2024 ] 	Mean training loss: 1.3335.
[ Tue Jul  2 23:15:34 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Tue Jul  2 23:15:34 2024 ] Training epoch: 6
[ Tue Jul  2 23:15:34 2024 ] 	Batch(0/7879) done. Loss: 0.7583  lr:0.010000
[ Tue Jul  2 23:15:52 2024 ] 	Batch(100/7879) done. Loss: 0.8602  lr:0.010000
[ Tue Jul  2 23:16:10 2024 ] 	Batch(200/7879) done. Loss: 1.7164  lr:0.010000
[ Tue Jul  2 23:16:28 2024 ] 	Batch(300/7879) done. Loss: 1.2708  lr:0.010000
[ Tue Jul  2 23:16:46 2024 ] 	Batch(400/7879) done. Loss: 1.8343  lr:0.010000
[ Tue Jul  2 23:17:04 2024 ] 
Training: Epoch [5/120], Step [499], Loss: 2.2591779232025146, Training Accuracy: 64.67500000000001
[ Tue Jul  2 23:17:04 2024 ] 	Batch(500/7879) done. Loss: 0.3872  lr:0.010000
[ Tue Jul  2 23:17:22 2024 ] 	Batch(600/7879) done. Loss: 2.1062  lr:0.010000
[ Tue Jul  2 23:17:40 2024 ] 	Batch(700/7879) done. Loss: 1.4859  lr:0.010000
[ Tue Jul  2 23:17:58 2024 ] 	Batch(800/7879) done. Loss: 0.9793  lr:0.010000
[ Tue Jul  2 23:18:16 2024 ] 	Batch(900/7879) done. Loss: 0.7599  lr:0.010000
[ Tue Jul  2 23:18:33 2024 ] 
Training: Epoch [5/120], Step [999], Loss: 1.388388991355896, Training Accuracy: 65.225
[ Tue Jul  2 23:18:33 2024 ] 	Batch(1000/7879) done. Loss: 0.9647  lr:0.010000
[ Tue Jul  2 23:18:51 2024 ] 	Batch(1100/7879) done. Loss: 1.7752  lr:0.010000
[ Tue Jul  2 23:19:09 2024 ] 	Batch(1200/7879) done. Loss: 0.5891  lr:0.010000
[ Tue Jul  2 23:19:27 2024 ] 	Batch(1300/7879) done. Loss: 1.1442  lr:0.010000
[ Tue Jul  2 23:19:45 2024 ] 	Batch(1400/7879) done. Loss: 1.1794  lr:0.010000
[ Tue Jul  2 23:20:03 2024 ] 
Training: Epoch [5/120], Step [1499], Loss: 1.3217695951461792, Training Accuracy: 65.275
[ Tue Jul  2 23:20:03 2024 ] 	Batch(1500/7879) done. Loss: 0.4317  lr:0.010000
[ Tue Jul  2 23:20:21 2024 ] 	Batch(1600/7879) done. Loss: 0.3316  lr:0.010000
[ Tue Jul  2 23:20:40 2024 ] 	Batch(1700/7879) done. Loss: 1.4906  lr:0.010000
[ Tue Jul  2 23:20:58 2024 ] 	Batch(1800/7879) done. Loss: 0.6301  lr:0.010000
[ Tue Jul  2 23:21:17 2024 ] 	Batch(1900/7879) done. Loss: 1.0340  lr:0.010000
[ Tue Jul  2 23:21:35 2024 ] 
Training: Epoch [5/120], Step [1999], Loss: 0.6344016790390015, Training Accuracy: 65.26875
[ Tue Jul  2 23:21:35 2024 ] 	Batch(2000/7879) done. Loss: 1.7956  lr:0.010000
[ Tue Jul  2 23:21:54 2024 ] 	Batch(2100/7879) done. Loss: 0.6113  lr:0.010000
[ Tue Jul  2 23:22:12 2024 ] 	Batch(2200/7879) done. Loss: 1.6532  lr:0.010000
[ Tue Jul  2 23:22:30 2024 ] 	Batch(2300/7879) done. Loss: 1.6026  lr:0.010000
[ Tue Jul  2 23:22:48 2024 ] 	Batch(2400/7879) done. Loss: 0.7681  lr:0.010000
[ Tue Jul  2 23:23:06 2024 ] 
Training: Epoch [5/120], Step [2499], Loss: 1.6223829984664917, Training Accuracy: 65.115
[ Tue Jul  2 23:23:06 2024 ] 	Batch(2500/7879) done. Loss: 1.8263  lr:0.010000
[ Tue Jul  2 23:23:24 2024 ] 	Batch(2600/7879) done. Loss: 1.9080  lr:0.010000
[ Tue Jul  2 23:23:42 2024 ] 	Batch(2700/7879) done. Loss: 1.5955  lr:0.010000
[ Tue Jul  2 23:24:00 2024 ] 	Batch(2800/7879) done. Loss: 0.8805  lr:0.010000
[ Tue Jul  2 23:24:19 2024 ] 	Batch(2900/7879) done. Loss: 0.7533  lr:0.010000
[ Tue Jul  2 23:24:37 2024 ] 
Training: Epoch [5/120], Step [2999], Loss: 0.5112853050231934, Training Accuracy: 65.05833333333332
[ Tue Jul  2 23:24:37 2024 ] 	Batch(3000/7879) done. Loss: 0.7905  lr:0.010000
[ Tue Jul  2 23:24:56 2024 ] 	Batch(3100/7879) done. Loss: 1.0818  lr:0.010000
[ Tue Jul  2 23:25:15 2024 ] 	Batch(3200/7879) done. Loss: 1.7281  lr:0.010000
[ Tue Jul  2 23:25:32 2024 ] 	Batch(3300/7879) done. Loss: 1.9324  lr:0.010000
[ Tue Jul  2 23:25:51 2024 ] 	Batch(3400/7879) done. Loss: 1.3401  lr:0.010000
[ Tue Jul  2 23:26:09 2024 ] 
Training: Epoch [5/120], Step [3499], Loss: 2.053865909576416, Training Accuracy: 65.00357142857143
[ Tue Jul  2 23:26:09 2024 ] 	Batch(3500/7879) done. Loss: 0.3016  lr:0.010000
[ Tue Jul  2 23:26:28 2024 ] 	Batch(3600/7879) done. Loss: 0.3587  lr:0.010000
[ Tue Jul  2 23:26:47 2024 ] 	Batch(3700/7879) done. Loss: 1.2995  lr:0.010000
[ Tue Jul  2 23:27:05 2024 ] 	Batch(3800/7879) done. Loss: 1.8485  lr:0.010000
[ Tue Jul  2 23:27:24 2024 ] 	Batch(3900/7879) done. Loss: 0.8426  lr:0.010000
[ Tue Jul  2 23:27:42 2024 ] 
Training: Epoch [5/120], Step [3999], Loss: 2.6570591926574707, Training Accuracy: 65.14687500000001
[ Tue Jul  2 23:27:42 2024 ] 	Batch(4000/7879) done. Loss: 0.8317  lr:0.010000
[ Tue Jul  2 23:28:01 2024 ] 	Batch(4100/7879) done. Loss: 1.7389  lr:0.010000
[ Tue Jul  2 23:28:19 2024 ] 	Batch(4200/7879) done. Loss: 2.0474  lr:0.010000
[ Tue Jul  2 23:28:38 2024 ] 	Batch(4300/7879) done. Loss: 0.5726  lr:0.010000
[ Tue Jul  2 23:28:57 2024 ] 	Batch(4400/7879) done. Loss: 0.3102  lr:0.010000
[ Tue Jul  2 23:29:15 2024 ] 
Training: Epoch [5/120], Step [4499], Loss: 2.1598610877990723, Training Accuracy: 65.14166666666667
[ Tue Jul  2 23:29:15 2024 ] 	Batch(4500/7879) done. Loss: 0.9885  lr:0.010000
[ Tue Jul  2 23:29:34 2024 ] 	Batch(4600/7879) done. Loss: 1.4427  lr:0.010000
[ Tue Jul  2 23:29:52 2024 ] 	Batch(4700/7879) done. Loss: 1.0825  lr:0.010000
[ Tue Jul  2 23:30:11 2024 ] 	Batch(4800/7879) done. Loss: 2.0841  lr:0.010000
[ Tue Jul  2 23:30:30 2024 ] 	Batch(4900/7879) done. Loss: 1.0384  lr:0.010000
[ Tue Jul  2 23:30:48 2024 ] 
Training: Epoch [5/120], Step [4999], Loss: 0.745179295539856, Training Accuracy: 65.13250000000001
[ Tue Jul  2 23:30:48 2024 ] 	Batch(5000/7879) done. Loss: 1.5816  lr:0.010000
[ Tue Jul  2 23:31:07 2024 ] 	Batch(5100/7879) done. Loss: 1.8641  lr:0.010000
[ Tue Jul  2 23:31:25 2024 ] 	Batch(5200/7879) done. Loss: 1.3573  lr:0.010000
[ Tue Jul  2 23:31:44 2024 ] 	Batch(5300/7879) done. Loss: 1.3367  lr:0.010000
[ Tue Jul  2 23:32:02 2024 ] 	Batch(5400/7879) done. Loss: 0.4274  lr:0.010000
[ Tue Jul  2 23:32:21 2024 ] 
Training: Epoch [5/120], Step [5499], Loss: 1.1725255250930786, Training Accuracy: 65.17045454545455
[ Tue Jul  2 23:32:21 2024 ] 	Batch(5500/7879) done. Loss: 1.5486  lr:0.010000
[ Tue Jul  2 23:32:40 2024 ] 	Batch(5600/7879) done. Loss: 1.8348  lr:0.010000
[ Tue Jul  2 23:32:58 2024 ] 	Batch(5700/7879) done. Loss: 0.9998  lr:0.010000
[ Tue Jul  2 23:33:17 2024 ] 	Batch(5800/7879) done. Loss: 2.0758  lr:0.010000
[ Tue Jul  2 23:33:35 2024 ] 	Batch(5900/7879) done. Loss: 1.6417  lr:0.010000
[ Tue Jul  2 23:33:53 2024 ] 
Training: Epoch [5/120], Step [5999], Loss: 0.9180020093917847, Training Accuracy: 65.27916666666667
[ Tue Jul  2 23:33:54 2024 ] 	Batch(6000/7879) done. Loss: 0.6292  lr:0.010000
[ Tue Jul  2 23:34:11 2024 ] 	Batch(6100/7879) done. Loss: 2.1443  lr:0.010000
[ Tue Jul  2 23:34:29 2024 ] 	Batch(6200/7879) done. Loss: 0.7606  lr:0.010000
[ Tue Jul  2 23:34:47 2024 ] 	Batch(6300/7879) done. Loss: 2.4331  lr:0.010000
[ Tue Jul  2 23:35:05 2024 ] 	Batch(6400/7879) done. Loss: 1.2717  lr:0.010000
[ Tue Jul  2 23:35:23 2024 ] 
Training: Epoch [5/120], Step [6499], Loss: 0.8612666726112366, Training Accuracy: 65.21153846153847
[ Tue Jul  2 23:35:23 2024 ] 	Batch(6500/7879) done. Loss: 0.7631  lr:0.010000
[ Tue Jul  2 23:35:41 2024 ] 	Batch(6600/7879) done. Loss: 1.3552  lr:0.010000
[ Tue Jul  2 23:35:59 2024 ] 	Batch(6700/7879) done. Loss: 1.8243  lr:0.010000
[ Tue Jul  2 23:36:17 2024 ] 	Batch(6800/7879) done. Loss: 1.4210  lr:0.010000
[ Tue Jul  2 23:36:35 2024 ] 	Batch(6900/7879) done. Loss: 1.1103  lr:0.010000
[ Tue Jul  2 23:36:53 2024 ] 
Training: Epoch [5/120], Step [6999], Loss: 2.1142311096191406, Training Accuracy: 65.35178571428571
[ Tue Jul  2 23:36:53 2024 ] 	Batch(7000/7879) done. Loss: 0.9121  lr:0.010000
[ Tue Jul  2 23:37:11 2024 ] 	Batch(7100/7879) done. Loss: 1.1024  lr:0.010000
[ Tue Jul  2 23:37:29 2024 ] 	Batch(7200/7879) done. Loss: 0.5285  lr:0.010000
[ Tue Jul  2 23:37:47 2024 ] 	Batch(7300/7879) done. Loss: 0.5806  lr:0.010000
[ Tue Jul  2 23:38:05 2024 ] 	Batch(7400/7879) done. Loss: 0.2354  lr:0.010000
[ Tue Jul  2 23:38:22 2024 ] 
Training: Epoch [5/120], Step [7499], Loss: 1.206272840499878, Training Accuracy: 65.42833333333333
[ Tue Jul  2 23:38:23 2024 ] 	Batch(7500/7879) done. Loss: 1.0332  lr:0.010000
[ Tue Jul  2 23:38:41 2024 ] 	Batch(7600/7879) done. Loss: 1.9101  lr:0.010000
[ Tue Jul  2 23:38:58 2024 ] 	Batch(7700/7879) done. Loss: 1.8434  lr:0.010000
[ Tue Jul  2 23:39:16 2024 ] 	Batch(7800/7879) done. Loss: 1.2774  lr:0.010000
[ Tue Jul  2 23:39:30 2024 ] 	Mean training loss: 1.1957.
[ Tue Jul  2 23:39:30 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  2 23:39:31 2024 ] Training epoch: 7
[ Tue Jul  2 23:39:31 2024 ] 	Batch(0/7879) done. Loss: 1.5577  lr:0.010000
[ Tue Jul  2 23:39:49 2024 ] 	Batch(100/7879) done. Loss: 0.8828  lr:0.010000
[ Tue Jul  2 23:40:07 2024 ] 	Batch(200/7879) done. Loss: 0.4603  lr:0.010000
[ Tue Jul  2 23:40:25 2024 ] 	Batch(300/7879) done. Loss: 1.6603  lr:0.010000
[ Tue Jul  2 23:40:43 2024 ] 	Batch(400/7879) done. Loss: 0.8906  lr:0.010000
[ Tue Jul  2 23:41:01 2024 ] 
Training: Epoch [6/120], Step [499], Loss: 1.2846813201904297, Training Accuracy: 68.325
[ Tue Jul  2 23:41:01 2024 ] 	Batch(500/7879) done. Loss: 1.4493  lr:0.010000
[ Tue Jul  2 23:41:19 2024 ] 	Batch(600/7879) done. Loss: 0.7861  lr:0.010000
[ Tue Jul  2 23:41:37 2024 ] 	Batch(700/7879) done. Loss: 1.6434  lr:0.010000
[ Tue Jul  2 23:41:55 2024 ] 	Batch(800/7879) done. Loss: 0.6105  lr:0.010000
[ Tue Jul  2 23:42:13 2024 ] 	Batch(900/7879) done. Loss: 1.2189  lr:0.010000
[ Tue Jul  2 23:42:31 2024 ] 
Training: Epoch [6/120], Step [999], Loss: 1.338288426399231, Training Accuracy: 67.30000000000001
[ Tue Jul  2 23:42:31 2024 ] 	Batch(1000/7879) done. Loss: 1.2304  lr:0.010000
[ Tue Jul  2 23:42:49 2024 ] 	Batch(1100/7879) done. Loss: 1.0433  lr:0.010000
[ Tue Jul  2 23:43:07 2024 ] 	Batch(1200/7879) done. Loss: 0.9710  lr:0.010000
[ Tue Jul  2 23:43:25 2024 ] 	Batch(1300/7879) done. Loss: 0.5913  lr:0.010000
[ Tue Jul  2 23:43:43 2024 ] 	Batch(1400/7879) done. Loss: 1.7530  lr:0.010000
[ Tue Jul  2 23:44:01 2024 ] 
Training: Epoch [6/120], Step [1499], Loss: 1.0830729007720947, Training Accuracy: 67.05
[ Tue Jul  2 23:44:01 2024 ] 	Batch(1500/7879) done. Loss: 0.5021  lr:0.010000
[ Tue Jul  2 23:44:19 2024 ] 	Batch(1600/7879) done. Loss: 1.4141  lr:0.010000
[ Tue Jul  2 23:44:37 2024 ] 	Batch(1700/7879) done. Loss: 1.3704  lr:0.010000
[ Tue Jul  2 23:44:55 2024 ] 	Batch(1800/7879) done. Loss: 2.1168  lr:0.010000
[ Tue Jul  2 23:45:13 2024 ] 	Batch(1900/7879) done. Loss: 0.6597  lr:0.010000
[ Tue Jul  2 23:45:30 2024 ] 
Training: Epoch [6/120], Step [1999], Loss: 1.535567283630371, Training Accuracy: 67.475
[ Tue Jul  2 23:45:31 2024 ] 	Batch(2000/7879) done. Loss: 0.3869  lr:0.010000
[ Tue Jul  2 23:45:49 2024 ] 	Batch(2100/7879) done. Loss: 1.8930  lr:0.010000
[ Tue Jul  2 23:46:08 2024 ] 	Batch(2200/7879) done. Loss: 0.6295  lr:0.010000
[ Tue Jul  2 23:46:26 2024 ] 	Batch(2300/7879) done. Loss: 1.3754  lr:0.010000
[ Tue Jul  2 23:46:44 2024 ] 	Batch(2400/7879) done. Loss: 0.7230  lr:0.010000
[ Tue Jul  2 23:47:02 2024 ] 
Training: Epoch [6/120], Step [2499], Loss: 2.8732919692993164, Training Accuracy: 67.67999999999999
[ Tue Jul  2 23:47:02 2024 ] 	Batch(2500/7879) done. Loss: 1.0375  lr:0.010000
[ Tue Jul  2 23:47:20 2024 ] 	Batch(2600/7879) done. Loss: 0.8505  lr:0.010000
[ Tue Jul  2 23:47:38 2024 ] 	Batch(2700/7879) done. Loss: 0.6078  lr:0.010000
[ Tue Jul  2 23:47:56 2024 ] 	Batch(2800/7879) done. Loss: 0.8532  lr:0.010000
[ Tue Jul  2 23:48:15 2024 ] 	Batch(2900/7879) done. Loss: 1.6621  lr:0.010000
[ Tue Jul  2 23:48:33 2024 ] 
Training: Epoch [6/120], Step [2999], Loss: 0.5993848443031311, Training Accuracy: 67.675
[ Tue Jul  2 23:48:33 2024 ] 	Batch(3000/7879) done. Loss: 1.1315  lr:0.010000
[ Tue Jul  2 23:48:52 2024 ] 	Batch(3100/7879) done. Loss: 1.4722  lr:0.010000
[ Tue Jul  2 23:49:11 2024 ] 	Batch(3200/7879) done. Loss: 1.1147  lr:0.010000
[ Tue Jul  2 23:49:28 2024 ] 	Batch(3300/7879) done. Loss: 0.3134  lr:0.010000
[ Tue Jul  2 23:49:46 2024 ] 	Batch(3400/7879) done. Loss: 1.5688  lr:0.010000
[ Tue Jul  2 23:50:04 2024 ] 
Training: Epoch [6/120], Step [3499], Loss: 1.6340208053588867, Training Accuracy: 67.65714285714286
[ Tue Jul  2 23:50:04 2024 ] 	Batch(3500/7879) done. Loss: 1.2104  lr:0.010000
[ Tue Jul  2 23:50:22 2024 ] 	Batch(3600/7879) done. Loss: 1.5337  lr:0.010000
[ Tue Jul  2 23:50:40 2024 ] 	Batch(3700/7879) done. Loss: 2.3395  lr:0.010000
[ Tue Jul  2 23:50:58 2024 ] 	Batch(3800/7879) done. Loss: 0.8398  lr:0.010000
[ Tue Jul  2 23:51:16 2024 ] 	Batch(3900/7879) done. Loss: 0.8888  lr:0.010000
[ Tue Jul  2 23:51:34 2024 ] 
Training: Epoch [6/120], Step [3999], Loss: 1.0487993955612183, Training Accuracy: 67.684375
[ Tue Jul  2 23:51:34 2024 ] 	Batch(4000/7879) done. Loss: 1.8069  lr:0.010000
[ Tue Jul  2 23:51:52 2024 ] 	Batch(4100/7879) done. Loss: 0.9656  lr:0.010000
[ Tue Jul  2 23:52:10 2024 ] 	Batch(4200/7879) done. Loss: 0.2652  lr:0.010000
[ Tue Jul  2 23:52:28 2024 ] 	Batch(4300/7879) done. Loss: 1.5860  lr:0.010000
[ Tue Jul  2 23:52:46 2024 ] 	Batch(4400/7879) done. Loss: 0.8478  lr:0.010000
[ Tue Jul  2 23:53:04 2024 ] 
Training: Epoch [6/120], Step [4499], Loss: 1.6983097791671753, Training Accuracy: 67.85
[ Tue Jul  2 23:53:04 2024 ] 	Batch(4500/7879) done. Loss: 0.9788  lr:0.010000
[ Tue Jul  2 23:53:22 2024 ] 	Batch(4600/7879) done. Loss: 0.4061  lr:0.010000
[ Tue Jul  2 23:53:40 2024 ] 	Batch(4700/7879) done. Loss: 1.7100  lr:0.010000
[ Tue Jul  2 23:53:58 2024 ] 	Batch(4800/7879) done. Loss: 1.3089  lr:0.010000
[ Tue Jul  2 23:54:16 2024 ] 	Batch(4900/7879) done. Loss: 0.9771  lr:0.010000
[ Tue Jul  2 23:54:33 2024 ] 
Training: Epoch [6/120], Step [4999], Loss: 2.0442872047424316, Training Accuracy: 67.89500000000001
[ Tue Jul  2 23:54:33 2024 ] 	Batch(5000/7879) done. Loss: 0.5694  lr:0.010000
[ Tue Jul  2 23:54:51 2024 ] 	Batch(5100/7879) done. Loss: 1.8472  lr:0.010000
[ Tue Jul  2 23:55:09 2024 ] 	Batch(5200/7879) done. Loss: 0.6016  lr:0.010000
[ Tue Jul  2 23:55:27 2024 ] 	Batch(5300/7879) done. Loss: 1.1521  lr:0.010000
[ Tue Jul  2 23:55:45 2024 ] 	Batch(5400/7879) done. Loss: 0.6198  lr:0.010000
[ Tue Jul  2 23:56:03 2024 ] 
Training: Epoch [6/120], Step [5499], Loss: 2.233393669128418, Training Accuracy: 67.85
[ Tue Jul  2 23:56:03 2024 ] 	Batch(5500/7879) done. Loss: 1.3007  lr:0.010000
[ Tue Jul  2 23:56:21 2024 ] 	Batch(5600/7879) done. Loss: 0.6155  lr:0.010000
[ Tue Jul  2 23:56:40 2024 ] 	Batch(5700/7879) done. Loss: 1.8414  lr:0.010000
[ Tue Jul  2 23:56:58 2024 ] 	Batch(5800/7879) done. Loss: 1.7374  lr:0.010000
[ Tue Jul  2 23:57:17 2024 ] 	Batch(5900/7879) done. Loss: 1.1884  lr:0.010000
[ Tue Jul  2 23:57:35 2024 ] 
Training: Epoch [6/120], Step [5999], Loss: 1.7059046030044556, Training Accuracy: 67.82083333333333
[ Tue Jul  2 23:57:35 2024 ] 	Batch(6000/7879) done. Loss: 0.5854  lr:0.010000
[ Tue Jul  2 23:57:54 2024 ] 	Batch(6100/7879) done. Loss: 0.9235  lr:0.010000
[ Tue Jul  2 23:58:12 2024 ] 	Batch(6200/7879) done. Loss: 0.9886  lr:0.010000
[ Tue Jul  2 23:58:31 2024 ] 	Batch(6300/7879) done. Loss: 1.1131  lr:0.010000
[ Tue Jul  2 23:58:50 2024 ] 	Batch(6400/7879) done. Loss: 0.5930  lr:0.010000
[ Tue Jul  2 23:59:08 2024 ] 
Training: Epoch [6/120], Step [6499], Loss: 0.9650460481643677, Training Accuracy: 67.86346153846154
[ Tue Jul  2 23:59:08 2024 ] 	Batch(6500/7879) done. Loss: 0.9429  lr:0.010000
[ Tue Jul  2 23:59:27 2024 ] 	Batch(6600/7879) done. Loss: 2.0141  lr:0.010000
[ Tue Jul  2 23:59:45 2024 ] 	Batch(6700/7879) done. Loss: 1.4374  lr:0.010000
[ Wed Jul  3 00:00:04 2024 ] 	Batch(6800/7879) done. Loss: 0.7263  lr:0.010000
[ Wed Jul  3 00:00:22 2024 ] 	Batch(6900/7879) done. Loss: 1.3803  lr:0.010000
[ Wed Jul  3 00:00:41 2024 ] 
Training: Epoch [6/120], Step [6999], Loss: 0.7572680711746216, Training Accuracy: 67.90892857142858
[ Wed Jul  3 00:00:41 2024 ] 	Batch(7000/7879) done. Loss: 1.1050  lr:0.010000
[ Wed Jul  3 00:00:59 2024 ] 	Batch(7100/7879) done. Loss: 0.5147  lr:0.010000
[ Wed Jul  3 00:01:18 2024 ] 	Batch(7200/7879) done. Loss: 0.4169  lr:0.010000
[ Wed Jul  3 00:01:37 2024 ] 	Batch(7300/7879) done. Loss: 1.6841  lr:0.010000
[ Wed Jul  3 00:01:55 2024 ] 	Batch(7400/7879) done. Loss: 1.3195  lr:0.010000
[ Wed Jul  3 00:02:13 2024 ] 
Training: Epoch [6/120], Step [7499], Loss: 0.7471072673797607, Training Accuracy: 68.02333333333334
[ Wed Jul  3 00:02:13 2024 ] 	Batch(7500/7879) done. Loss: 0.5404  lr:0.010000
[ Wed Jul  3 00:02:31 2024 ] 	Batch(7600/7879) done. Loss: 0.2589  lr:0.010000
[ Wed Jul  3 00:02:49 2024 ] 	Batch(7700/7879) done. Loss: 1.5688  lr:0.010000
[ Wed Jul  3 00:03:07 2024 ] 	Batch(7800/7879) done. Loss: 1.0797  lr:0.010000
[ Wed Jul  3 00:03:21 2024 ] 	Mean training loss: 1.0988.
[ Wed Jul  3 00:03:21 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 00:03:21 2024 ] Training epoch: 8
[ Wed Jul  3 00:03:21 2024 ] 	Batch(0/7879) done. Loss: 0.3663  lr:0.010000
[ Wed Jul  3 00:03:39 2024 ] 	Batch(100/7879) done. Loss: 0.9724  lr:0.010000
[ Wed Jul  3 00:03:57 2024 ] 	Batch(200/7879) done. Loss: 0.8592  lr:0.010000
[ Wed Jul  3 00:04:15 2024 ] 	Batch(300/7879) done. Loss: 0.2124  lr:0.010000
[ Wed Jul  3 00:04:33 2024 ] 	Batch(400/7879) done. Loss: 1.5292  lr:0.010000
[ Wed Jul  3 00:04:51 2024 ] 
Training: Epoch [7/120], Step [499], Loss: 0.6450564861297607, Training Accuracy: 69.675
[ Wed Jul  3 00:04:51 2024 ] 	Batch(500/7879) done. Loss: 1.1355  lr:0.010000
[ Wed Jul  3 00:05:10 2024 ] 	Batch(600/7879) done. Loss: 0.8390  lr:0.010000
[ Wed Jul  3 00:05:28 2024 ] 	Batch(700/7879) done. Loss: 0.7505  lr:0.010000
[ Wed Jul  3 00:05:47 2024 ] 	Batch(800/7879) done. Loss: 0.2796  lr:0.010000
[ Wed Jul  3 00:06:05 2024 ] 	Batch(900/7879) done. Loss: 1.6338  lr:0.010000
[ Wed Jul  3 00:06:23 2024 ] 
Training: Epoch [7/120], Step [999], Loss: 1.0785750150680542, Training Accuracy: 69.77499999999999
[ Wed Jul  3 00:06:23 2024 ] 	Batch(1000/7879) done. Loss: 1.7336  lr:0.010000
[ Wed Jul  3 00:06:41 2024 ] 	Batch(1100/7879) done. Loss: 0.5068  lr:0.010000
[ Wed Jul  3 00:06:59 2024 ] 	Batch(1200/7879) done. Loss: 1.4847  lr:0.010000
[ Wed Jul  3 00:07:17 2024 ] 	Batch(1300/7879) done. Loss: 0.3516  lr:0.010000
[ Wed Jul  3 00:07:34 2024 ] 	Batch(1400/7879) done. Loss: 1.1952  lr:0.010000
[ Wed Jul  3 00:07:52 2024 ] 
Training: Epoch [7/120], Step [1499], Loss: 1.6426141262054443, Training Accuracy: 69.95
[ Wed Jul  3 00:07:52 2024 ] 	Batch(1500/7879) done. Loss: 1.0731  lr:0.010000
[ Wed Jul  3 00:08:10 2024 ] 	Batch(1600/7879) done. Loss: 1.7435  lr:0.010000
[ Wed Jul  3 00:08:29 2024 ] 	Batch(1700/7879) done. Loss: 0.9684  lr:0.010000
[ Wed Jul  3 00:08:47 2024 ] 	Batch(1800/7879) done. Loss: 0.9262  lr:0.010000
[ Wed Jul  3 00:09:06 2024 ] 	Batch(1900/7879) done. Loss: 0.6532  lr:0.010000
[ Wed Jul  3 00:09:24 2024 ] 
Training: Epoch [7/120], Step [1999], Loss: 0.6408892273902893, Training Accuracy: 69.825
[ Wed Jul  3 00:09:24 2024 ] 	Batch(2000/7879) done. Loss: 0.8949  lr:0.010000
[ Wed Jul  3 00:09:42 2024 ] 	Batch(2100/7879) done. Loss: 0.3308  lr:0.010000
[ Wed Jul  3 00:10:00 2024 ] 	Batch(2200/7879) done. Loss: 2.0997  lr:0.010000
[ Wed Jul  3 00:10:18 2024 ] 	Batch(2300/7879) done. Loss: 0.3250  lr:0.010000
[ Wed Jul  3 00:10:36 2024 ] 	Batch(2400/7879) done. Loss: 1.3159  lr:0.010000
[ Wed Jul  3 00:10:54 2024 ] 
Training: Epoch [7/120], Step [2499], Loss: 0.7310315370559692, Training Accuracy: 70.21
[ Wed Jul  3 00:10:54 2024 ] 	Batch(2500/7879) done. Loss: 0.1926  lr:0.010000
[ Wed Jul  3 00:11:12 2024 ] 	Batch(2600/7879) done. Loss: 0.3527  lr:0.010000
[ Wed Jul  3 00:11:30 2024 ] 	Batch(2700/7879) done. Loss: 1.3624  lr:0.010000
[ Wed Jul  3 00:11:48 2024 ] 	Batch(2800/7879) done. Loss: 0.3457  lr:0.010000
[ Wed Jul  3 00:12:06 2024 ] 	Batch(2900/7879) done. Loss: 0.7010  lr:0.010000
[ Wed Jul  3 00:12:24 2024 ] 
Training: Epoch [7/120], Step [2999], Loss: 1.0486044883728027, Training Accuracy: 70.19583333333334
[ Wed Jul  3 00:12:25 2024 ] 	Batch(3000/7879) done. Loss: 2.3328  lr:0.010000
[ Wed Jul  3 00:12:43 2024 ] 	Batch(3100/7879) done. Loss: 1.0051  lr:0.010000
[ Wed Jul  3 00:13:02 2024 ] 	Batch(3200/7879) done. Loss: 0.7467  lr:0.010000
[ Wed Jul  3 00:13:20 2024 ] 	Batch(3300/7879) done. Loss: 0.9768  lr:0.010000
[ Wed Jul  3 00:13:39 2024 ] 	Batch(3400/7879) done. Loss: 1.1004  lr:0.010000
[ Wed Jul  3 00:13:57 2024 ] 
Training: Epoch [7/120], Step [3499], Loss: 1.632012128829956, Training Accuracy: 70.1
[ Wed Jul  3 00:13:57 2024 ] 	Batch(3500/7879) done. Loss: 0.7699  lr:0.010000
[ Wed Jul  3 00:14:16 2024 ] 	Batch(3600/7879) done. Loss: 0.4163  lr:0.010000
[ Wed Jul  3 00:14:35 2024 ] 	Batch(3700/7879) done. Loss: 0.7615  lr:0.010000
[ Wed Jul  3 00:14:53 2024 ] 	Batch(3800/7879) done. Loss: 1.4146  lr:0.010000
[ Wed Jul  3 00:15:12 2024 ] 	Batch(3900/7879) done. Loss: 1.3615  lr:0.010000
[ Wed Jul  3 00:15:30 2024 ] 
Training: Epoch [7/120], Step [3999], Loss: 0.7392239570617676, Training Accuracy: 70.175
[ Wed Jul  3 00:15:30 2024 ] 	Batch(4000/7879) done. Loss: 0.8025  lr:0.010000
[ Wed Jul  3 00:15:49 2024 ] 	Batch(4100/7879) done. Loss: 1.0644  lr:0.010000
[ Wed Jul  3 00:16:07 2024 ] 	Batch(4200/7879) done. Loss: 1.3292  lr:0.010000
[ Wed Jul  3 00:16:26 2024 ] 	Batch(4300/7879) done. Loss: 1.0917  lr:0.010000
[ Wed Jul  3 00:16:45 2024 ] 	Batch(4400/7879) done. Loss: 0.4726  lr:0.010000
[ Wed Jul  3 00:17:03 2024 ] 
Training: Epoch [7/120], Step [4499], Loss: 1.6603097915649414, Training Accuracy: 70.175
[ Wed Jul  3 00:17:03 2024 ] 	Batch(4500/7879) done. Loss: 1.0643  lr:0.010000
[ Wed Jul  3 00:17:22 2024 ] 	Batch(4600/7879) done. Loss: 0.8297  lr:0.010000
[ Wed Jul  3 00:17:40 2024 ] 	Batch(4700/7879) done. Loss: 0.5103  lr:0.010000
[ Wed Jul  3 00:17:59 2024 ] 	Batch(4800/7879) done. Loss: 1.0138  lr:0.010000
[ Wed Jul  3 00:18:17 2024 ] 	Batch(4900/7879) done. Loss: 1.2964  lr:0.010000
[ Wed Jul  3 00:18:36 2024 ] 
Training: Epoch [7/120], Step [4999], Loss: 0.27315330505371094, Training Accuracy: 70.35
[ Wed Jul  3 00:18:36 2024 ] 	Batch(5000/7879) done. Loss: 1.1762  lr:0.010000
[ Wed Jul  3 00:18:54 2024 ] 	Batch(5100/7879) done. Loss: 1.6297  lr:0.010000
[ Wed Jul  3 00:19:13 2024 ] 	Batch(5200/7879) done. Loss: 1.2542  lr:0.010000
[ Wed Jul  3 00:19:32 2024 ] 	Batch(5300/7879) done. Loss: 0.4730  lr:0.010000
[ Wed Jul  3 00:19:50 2024 ] 	Batch(5400/7879) done. Loss: 1.3260  lr:0.010000
[ Wed Jul  3 00:20:08 2024 ] 
Training: Epoch [7/120], Step [5499], Loss: 0.7621116638183594, Training Accuracy: 70.21818181818182
[ Wed Jul  3 00:20:09 2024 ] 	Batch(5500/7879) done. Loss: 0.4551  lr:0.010000
[ Wed Jul  3 00:20:27 2024 ] 	Batch(5600/7879) done. Loss: 0.6266  lr:0.010000
[ Wed Jul  3 00:20:45 2024 ] 	Batch(5700/7879) done. Loss: 0.6915  lr:0.010000
[ Wed Jul  3 00:21:03 2024 ] 	Batch(5800/7879) done. Loss: 0.6528  lr:0.010000
[ Wed Jul  3 00:21:21 2024 ] 	Batch(5900/7879) done. Loss: 0.6820  lr:0.010000
[ Wed Jul  3 00:21:39 2024 ] 
Training: Epoch [7/120], Step [5999], Loss: 0.7490975856781006, Training Accuracy: 70.17708333333333
[ Wed Jul  3 00:21:39 2024 ] 	Batch(6000/7879) done. Loss: 1.5865  lr:0.010000
[ Wed Jul  3 00:21:57 2024 ] 	Batch(6100/7879) done. Loss: 1.7342  lr:0.010000
[ Wed Jul  3 00:22:15 2024 ] 	Batch(6200/7879) done. Loss: 0.7995  lr:0.010000
[ Wed Jul  3 00:22:33 2024 ] 	Batch(6300/7879) done. Loss: 0.8716  lr:0.010000
[ Wed Jul  3 00:22:51 2024 ] 	Batch(6400/7879) done. Loss: 1.6683  lr:0.010000
[ Wed Jul  3 00:23:09 2024 ] 
Training: Epoch [7/120], Step [6499], Loss: 1.6793543100357056, Training Accuracy: 70.23846153846154
[ Wed Jul  3 00:23:09 2024 ] 	Batch(6500/7879) done. Loss: 1.0303  lr:0.010000
[ Wed Jul  3 00:23:27 2024 ] 	Batch(6600/7879) done. Loss: 1.9545  lr:0.010000
[ Wed Jul  3 00:23:45 2024 ] 	Batch(6700/7879) done. Loss: 2.7677  lr:0.010000
[ Wed Jul  3 00:24:03 2024 ] 	Batch(6800/7879) done. Loss: 1.6089  lr:0.010000
[ Wed Jul  3 00:24:21 2024 ] 	Batch(6900/7879) done. Loss: 0.5001  lr:0.010000
[ Wed Jul  3 00:24:39 2024 ] 
Training: Epoch [7/120], Step [6999], Loss: 1.4313268661499023, Training Accuracy: 70.26607142857144
[ Wed Jul  3 00:24:39 2024 ] 	Batch(7000/7879) done. Loss: 1.1780  lr:0.010000
[ Wed Jul  3 00:24:57 2024 ] 	Batch(7100/7879) done. Loss: 1.3260  lr:0.010000
[ Wed Jul  3 00:25:15 2024 ] 	Batch(7200/7879) done. Loss: 0.8728  lr:0.010000
[ Wed Jul  3 00:25:33 2024 ] 	Batch(7300/7879) done. Loss: 1.1499  lr:0.010000
[ Wed Jul  3 00:25:51 2024 ] 	Batch(7400/7879) done. Loss: 1.4924  lr:0.010000
[ Wed Jul  3 00:26:08 2024 ] 
Training: Epoch [7/120], Step [7499], Loss: 0.738728404045105, Training Accuracy: 70.23333333333333
[ Wed Jul  3 00:26:08 2024 ] 	Batch(7500/7879) done. Loss: 0.6717  lr:0.010000
[ Wed Jul  3 00:26:26 2024 ] 	Batch(7600/7879) done. Loss: 0.2537  lr:0.010000
[ Wed Jul  3 00:26:44 2024 ] 	Batch(7700/7879) done. Loss: 0.9047  lr:0.010000
[ Wed Jul  3 00:27:02 2024 ] 	Batch(7800/7879) done. Loss: 0.7678  lr:0.010000
[ Wed Jul  3 00:27:16 2024 ] 	Mean training loss: 1.0187.
[ Wed Jul  3 00:27:16 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 00:27:16 2024 ] Training epoch: 9
[ Wed Jul  3 00:27:17 2024 ] 	Batch(0/7879) done. Loss: 1.9810  lr:0.010000
[ Wed Jul  3 00:27:35 2024 ] 	Batch(100/7879) done. Loss: 0.6859  lr:0.010000
[ Wed Jul  3 00:27:53 2024 ] 	Batch(200/7879) done. Loss: 0.4616  lr:0.010000
[ Wed Jul  3 00:28:11 2024 ] 	Batch(300/7879) done. Loss: 0.4671  lr:0.010000
[ Wed Jul  3 00:28:29 2024 ] 	Batch(400/7879) done. Loss: 1.7688  lr:0.010000
[ Wed Jul  3 00:28:46 2024 ] 
Training: Epoch [8/120], Step [499], Loss: 1.1146907806396484, Training Accuracy: 73.175
[ Wed Jul  3 00:28:46 2024 ] 	Batch(500/7879) done. Loss: 1.0729  lr:0.010000
[ Wed Jul  3 00:29:04 2024 ] 	Batch(600/7879) done. Loss: 1.9427  lr:0.010000
[ Wed Jul  3 00:29:22 2024 ] 	Batch(700/7879) done. Loss: 1.4614  lr:0.010000
[ Wed Jul  3 00:29:40 2024 ] 	Batch(800/7879) done. Loss: 1.1638  lr:0.010000
[ Wed Jul  3 00:29:59 2024 ] 	Batch(900/7879) done. Loss: 0.2993  lr:0.010000
[ Wed Jul  3 00:30:18 2024 ] 
Training: Epoch [8/120], Step [999], Loss: 1.4720866680145264, Training Accuracy: 72.1
[ Wed Jul  3 00:30:18 2024 ] 	Batch(1000/7879) done. Loss: 1.0959  lr:0.010000
[ Wed Jul  3 00:30:36 2024 ] 	Batch(1100/7879) done. Loss: 1.4210  lr:0.010000
[ Wed Jul  3 00:30:55 2024 ] 	Batch(1200/7879) done. Loss: 0.4126  lr:0.010000
[ Wed Jul  3 00:31:14 2024 ] 	Batch(1300/7879) done. Loss: 0.7934  lr:0.010000
[ Wed Jul  3 00:31:32 2024 ] 	Batch(1400/7879) done. Loss: 0.5032  lr:0.010000
[ Wed Jul  3 00:31:51 2024 ] 
Training: Epoch [8/120], Step [1499], Loss: 1.280548334121704, Training Accuracy: 72.15
[ Wed Jul  3 00:31:51 2024 ] 	Batch(1500/7879) done. Loss: 2.4423  lr:0.010000
[ Wed Jul  3 00:32:10 2024 ] 	Batch(1600/7879) done. Loss: 0.5881  lr:0.010000
[ Wed Jul  3 00:32:28 2024 ] 	Batch(1700/7879) done. Loss: 1.4500  lr:0.010000
[ Wed Jul  3 00:32:47 2024 ] 	Batch(1800/7879) done. Loss: 0.3354  lr:0.010000
[ Wed Jul  3 00:33:05 2024 ] 	Batch(1900/7879) done. Loss: 0.2195  lr:0.010000
[ Wed Jul  3 00:33:24 2024 ] 
Training: Epoch [8/120], Step [1999], Loss: 1.313299298286438, Training Accuracy: 71.925
[ Wed Jul  3 00:33:24 2024 ] 	Batch(2000/7879) done. Loss: 0.5461  lr:0.010000
[ Wed Jul  3 00:33:42 2024 ] 	Batch(2100/7879) done. Loss: 1.2688  lr:0.010000
[ Wed Jul  3 00:34:01 2024 ] 	Batch(2200/7879) done. Loss: 0.4226  lr:0.010000
[ Wed Jul  3 00:34:19 2024 ] 	Batch(2300/7879) done. Loss: 1.5352  lr:0.010000
[ Wed Jul  3 00:34:38 2024 ] 	Batch(2400/7879) done. Loss: 1.2740  lr:0.010000
[ Wed Jul  3 00:34:57 2024 ] 
Training: Epoch [8/120], Step [2499], Loss: 0.6366978287696838, Training Accuracy: 71.735
[ Wed Jul  3 00:34:57 2024 ] 	Batch(2500/7879) done. Loss: 0.9999  lr:0.010000
[ Wed Jul  3 00:35:15 2024 ] 	Batch(2600/7879) done. Loss: 1.4979  lr:0.010000
[ Wed Jul  3 00:35:33 2024 ] 	Batch(2700/7879) done. Loss: 0.2220  lr:0.010000
[ Wed Jul  3 00:35:51 2024 ] 	Batch(2800/7879) done. Loss: 0.2361  lr:0.010000
[ Wed Jul  3 00:36:09 2024 ] 	Batch(2900/7879) done. Loss: 1.2140  lr:0.010000
[ Wed Jul  3 00:36:27 2024 ] 
Training: Epoch [8/120], Step [2999], Loss: 0.8815083503723145, Training Accuracy: 71.925
[ Wed Jul  3 00:36:27 2024 ] 	Batch(3000/7879) done. Loss: 1.3883  lr:0.010000
[ Wed Jul  3 00:36:45 2024 ] 	Batch(3100/7879) done. Loss: 0.6972  lr:0.010000
[ Wed Jul  3 00:37:03 2024 ] 	Batch(3200/7879) done. Loss: 0.7121  lr:0.010000
[ Wed Jul  3 00:37:21 2024 ] 	Batch(3300/7879) done. Loss: 1.6771  lr:0.010000
[ Wed Jul  3 00:37:39 2024 ] 	Batch(3400/7879) done. Loss: 0.9841  lr:0.010000
[ Wed Jul  3 00:37:56 2024 ] 
Training: Epoch [8/120], Step [3499], Loss: 0.913108766078949, Training Accuracy: 72.03928571428571
[ Wed Jul  3 00:37:57 2024 ] 	Batch(3500/7879) done. Loss: 1.0676  lr:0.010000
[ Wed Jul  3 00:38:15 2024 ] 	Batch(3600/7879) done. Loss: 2.2024  lr:0.010000
[ Wed Jul  3 00:38:32 2024 ] 	Batch(3700/7879) done. Loss: 1.3397  lr:0.010000
[ Wed Jul  3 00:38:50 2024 ] 	Batch(3800/7879) done. Loss: 0.5916  lr:0.010000
[ Wed Jul  3 00:39:08 2024 ] 	Batch(3900/7879) done. Loss: 0.4475  lr:0.010000
[ Wed Jul  3 00:39:26 2024 ] 
Training: Epoch [8/120], Step [3999], Loss: 0.8793175220489502, Training Accuracy: 72.11562500000001
[ Wed Jul  3 00:39:26 2024 ] 	Batch(4000/7879) done. Loss: 0.9713  lr:0.010000
[ Wed Jul  3 00:39:44 2024 ] 	Batch(4100/7879) done. Loss: 0.9484  lr:0.010000
[ Wed Jul  3 00:40:02 2024 ] 	Batch(4200/7879) done. Loss: 1.5195  lr:0.010000
[ Wed Jul  3 00:40:20 2024 ] 	Batch(4300/7879) done. Loss: 0.9933  lr:0.010000
[ Wed Jul  3 00:40:38 2024 ] 	Batch(4400/7879) done. Loss: 0.9786  lr:0.010000
[ Wed Jul  3 00:40:56 2024 ] 
Training: Epoch [8/120], Step [4499], Loss: 1.5279127359390259, Training Accuracy: 72.20555555555556
[ Wed Jul  3 00:40:56 2024 ] 	Batch(4500/7879) done. Loss: 0.2870  lr:0.010000
[ Wed Jul  3 00:41:14 2024 ] 	Batch(4600/7879) done. Loss: 0.5866  lr:0.010000
[ Wed Jul  3 00:41:32 2024 ] 	Batch(4700/7879) done. Loss: 0.3312  lr:0.010000
[ Wed Jul  3 00:41:50 2024 ] 	Batch(4800/7879) done. Loss: 0.6324  lr:0.010000
[ Wed Jul  3 00:42:08 2024 ] 	Batch(4900/7879) done. Loss: 0.8802  lr:0.010000
[ Wed Jul  3 00:42:27 2024 ] 
Training: Epoch [8/120], Step [4999], Loss: 0.5008156895637512, Training Accuracy: 72.08
[ Wed Jul  3 00:42:27 2024 ] 	Batch(5000/7879) done. Loss: 1.0310  lr:0.010000
[ Wed Jul  3 00:42:45 2024 ] 	Batch(5100/7879) done. Loss: 1.7768  lr:0.010000
[ Wed Jul  3 00:43:04 2024 ] 	Batch(5200/7879) done. Loss: 0.4322  lr:0.010000
[ Wed Jul  3 00:43:22 2024 ] 	Batch(5300/7879) done. Loss: 0.4628  lr:0.010000
[ Wed Jul  3 00:43:40 2024 ] 	Batch(5400/7879) done. Loss: 0.9504  lr:0.010000
[ Wed Jul  3 00:43:57 2024 ] 
Training: Epoch [8/120], Step [5499], Loss: 0.6743736267089844, Training Accuracy: 72.14545454545454
[ Wed Jul  3 00:43:58 2024 ] 	Batch(5500/7879) done. Loss: 0.7005  lr:0.010000
[ Wed Jul  3 00:44:16 2024 ] 	Batch(5600/7879) done. Loss: 0.5072  lr:0.010000
[ Wed Jul  3 00:44:33 2024 ] 	Batch(5700/7879) done. Loss: 1.4107  lr:0.010000
[ Wed Jul  3 00:44:51 2024 ] 	Batch(5800/7879) done. Loss: 0.5345  lr:0.010000
[ Wed Jul  3 00:45:09 2024 ] 	Batch(5900/7879) done. Loss: 1.4216  lr:0.010000
[ Wed Jul  3 00:45:27 2024 ] 
Training: Epoch [8/120], Step [5999], Loss: 0.5940620303153992, Training Accuracy: 72.14375
[ Wed Jul  3 00:45:27 2024 ] 	Batch(6000/7879) done. Loss: 1.5731  lr:0.010000
[ Wed Jul  3 00:45:45 2024 ] 	Batch(6100/7879) done. Loss: 0.3872  lr:0.010000
[ Wed Jul  3 00:46:03 2024 ] 	Batch(6200/7879) done. Loss: 1.3753  lr:0.010000
[ Wed Jul  3 00:46:21 2024 ] 	Batch(6300/7879) done. Loss: 1.0744  lr:0.010000
[ Wed Jul  3 00:46:39 2024 ] 	Batch(6400/7879) done. Loss: 1.3481  lr:0.010000
[ Wed Jul  3 00:46:57 2024 ] 
Training: Epoch [8/120], Step [6499], Loss: 0.9472550749778748, Training Accuracy: 72.16153846153847
[ Wed Jul  3 00:46:57 2024 ] 	Batch(6500/7879) done. Loss: 1.3427  lr:0.010000
[ Wed Jul  3 00:47:15 2024 ] 	Batch(6600/7879) done. Loss: 1.4709  lr:0.010000
[ Wed Jul  3 00:47:33 2024 ] 	Batch(6700/7879) done. Loss: 1.1491  lr:0.010000
[ Wed Jul  3 00:47:51 2024 ] 	Batch(6800/7879) done. Loss: 0.9556  lr:0.010000
[ Wed Jul  3 00:48:09 2024 ] 	Batch(6900/7879) done. Loss: 1.7024  lr:0.010000
[ Wed Jul  3 00:48:27 2024 ] 
Training: Epoch [8/120], Step [6999], Loss: 0.6737926006317139, Training Accuracy: 72.10892857142858
[ Wed Jul  3 00:48:27 2024 ] 	Batch(7000/7879) done. Loss: 0.5487  lr:0.010000
[ Wed Jul  3 00:48:45 2024 ] 	Batch(7100/7879) done. Loss: 1.4233  lr:0.010000
[ Wed Jul  3 00:49:03 2024 ] 	Batch(7200/7879) done. Loss: 0.4096  lr:0.010000
[ Wed Jul  3 00:49:21 2024 ] 	Batch(7300/7879) done. Loss: 0.8012  lr:0.010000
[ Wed Jul  3 00:49:39 2024 ] 	Batch(7400/7879) done. Loss: 1.6454  lr:0.010000
[ Wed Jul  3 00:49:56 2024 ] 
Training: Epoch [8/120], Step [7499], Loss: 0.4611085057258606, Training Accuracy: 72.07833333333333
[ Wed Jul  3 00:49:56 2024 ] 	Batch(7500/7879) done. Loss: 0.5050  lr:0.010000
[ Wed Jul  3 00:50:14 2024 ] 	Batch(7600/7879) done. Loss: 0.3313  lr:0.010000
[ Wed Jul  3 00:50:33 2024 ] 	Batch(7700/7879) done. Loss: 0.9662  lr:0.010000
[ Wed Jul  3 00:50:51 2024 ] 	Batch(7800/7879) done. Loss: 1.2111  lr:0.010000
[ Wed Jul  3 00:51:05 2024 ] 	Mean training loss: 0.9513.
[ Wed Jul  3 00:51:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 00:51:05 2024 ] Training epoch: 10
[ Wed Jul  3 00:51:05 2024 ] 	Batch(0/7879) done. Loss: 0.5679  lr:0.010000
[ Wed Jul  3 00:51:23 2024 ] 	Batch(100/7879) done. Loss: 0.9265  lr:0.010000
[ Wed Jul  3 00:51:41 2024 ] 	Batch(200/7879) done. Loss: 2.6382  lr:0.010000
[ Wed Jul  3 00:51:59 2024 ] 	Batch(300/7879) done. Loss: 0.4453  lr:0.010000
[ Wed Jul  3 00:52:17 2024 ] 	Batch(400/7879) done. Loss: 1.4935  lr:0.010000
[ Wed Jul  3 00:52:35 2024 ] 
Training: Epoch [9/120], Step [499], Loss: 0.44167816638946533, Training Accuracy: 73.55000000000001
[ Wed Jul  3 00:52:35 2024 ] 	Batch(500/7879) done. Loss: 1.1241  lr:0.010000
[ Wed Jul  3 00:52:53 2024 ] 	Batch(600/7879) done. Loss: 0.8162  lr:0.010000
[ Wed Jul  3 00:53:11 2024 ] 	Batch(700/7879) done. Loss: 1.1445  lr:0.010000
[ Wed Jul  3 00:53:29 2024 ] 	Batch(800/7879) done. Loss: 0.1987  lr:0.010000
[ Wed Jul  3 00:53:47 2024 ] 	Batch(900/7879) done. Loss: 0.5139  lr:0.010000
[ Wed Jul  3 00:54:04 2024 ] 
Training: Epoch [9/120], Step [999], Loss: 0.16758716106414795, Training Accuracy: 73.825
[ Wed Jul  3 00:54:04 2024 ] 	Batch(1000/7879) done. Loss: 0.2970  lr:0.010000
[ Wed Jul  3 00:54:22 2024 ] 	Batch(1100/7879) done. Loss: 0.8709  lr:0.010000
[ Wed Jul  3 00:54:40 2024 ] 	Batch(1200/7879) done. Loss: 0.8030  lr:0.010000
[ Wed Jul  3 00:54:59 2024 ] 	Batch(1300/7879) done. Loss: 1.3285  lr:0.010000
[ Wed Jul  3 00:55:17 2024 ] 	Batch(1400/7879) done. Loss: 0.9199  lr:0.010000
[ Wed Jul  3 00:55:36 2024 ] 
Training: Epoch [9/120], Step [1499], Loss: 0.6271519660949707, Training Accuracy: 73.52499999999999
[ Wed Jul  3 00:55:36 2024 ] 	Batch(1500/7879) done. Loss: 1.2909  lr:0.010000
[ Wed Jul  3 00:55:55 2024 ] 	Batch(1600/7879) done. Loss: 1.0523  lr:0.010000
[ Wed Jul  3 00:56:13 2024 ] 	Batch(1700/7879) done. Loss: 0.5940  lr:0.010000
[ Wed Jul  3 00:56:32 2024 ] 	Batch(1800/7879) done. Loss: 0.6525  lr:0.010000
[ Wed Jul  3 00:56:50 2024 ] 	Batch(1900/7879) done. Loss: 0.2829  lr:0.010000
[ Wed Jul  3 00:57:07 2024 ] 
Training: Epoch [9/120], Step [1999], Loss: 0.5130281448364258, Training Accuracy: 73.59375
[ Wed Jul  3 00:57:08 2024 ] 	Batch(2000/7879) done. Loss: 1.1341  lr:0.010000
[ Wed Jul  3 00:57:25 2024 ] 	Batch(2100/7879) done. Loss: 0.4167  lr:0.010000
[ Wed Jul  3 00:57:43 2024 ] 	Batch(2200/7879) done. Loss: 1.1004  lr:0.010000
[ Wed Jul  3 00:58:01 2024 ] 	Batch(2300/7879) done. Loss: 0.2606  lr:0.010000
[ Wed Jul  3 00:58:19 2024 ] 	Batch(2400/7879) done. Loss: 0.3167  lr:0.010000
[ Wed Jul  3 00:58:37 2024 ] 
Training: Epoch [9/120], Step [2499], Loss: 0.5485273003578186, Training Accuracy: 73.47
[ Wed Jul  3 00:58:37 2024 ] 	Batch(2500/7879) done. Loss: 0.7380  lr:0.010000
[ Wed Jul  3 00:58:55 2024 ] 	Batch(2600/7879) done. Loss: 0.7832  lr:0.010000
[ Wed Jul  3 00:59:13 2024 ] 	Batch(2700/7879) done. Loss: 0.8727  lr:0.010000
[ Wed Jul  3 00:59:31 2024 ] 	Batch(2800/7879) done. Loss: 1.4641  lr:0.010000
[ Wed Jul  3 00:59:50 2024 ] 	Batch(2900/7879) done. Loss: 0.4699  lr:0.010000
[ Wed Jul  3 01:00:08 2024 ] 
Training: Epoch [9/120], Step [2999], Loss: 1.322887897491455, Training Accuracy: 73.26666666666667
[ Wed Jul  3 01:00:08 2024 ] 	Batch(3000/7879) done. Loss: 0.5418  lr:0.010000
[ Wed Jul  3 01:00:27 2024 ] 	Batch(3100/7879) done. Loss: 1.5190  lr:0.010000
[ Wed Jul  3 01:00:45 2024 ] 	Batch(3200/7879) done. Loss: 1.3342  lr:0.010000
[ Wed Jul  3 01:01:03 2024 ] 	Batch(3300/7879) done. Loss: 0.9298  lr:0.010000
[ Wed Jul  3 01:01:21 2024 ] 	Batch(3400/7879) done. Loss: 1.0074  lr:0.010000
[ Wed Jul  3 01:01:39 2024 ] 
Training: Epoch [9/120], Step [3499], Loss: 0.7982226014137268, Training Accuracy: 73.27142857142857
[ Wed Jul  3 01:01:39 2024 ] 	Batch(3500/7879) done. Loss: 0.5284  lr:0.010000
[ Wed Jul  3 01:01:57 2024 ] 	Batch(3600/7879) done. Loss: 1.6738  lr:0.010000
[ Wed Jul  3 01:02:16 2024 ] 	Batch(3700/7879) done. Loss: 0.9082  lr:0.010000
[ Wed Jul  3 01:02:34 2024 ] 	Batch(3800/7879) done. Loss: 1.0585  lr:0.010000
[ Wed Jul  3 01:02:53 2024 ] 	Batch(3900/7879) done. Loss: 2.4216  lr:0.010000
[ Wed Jul  3 01:03:11 2024 ] 
Training: Epoch [9/120], Step [3999], Loss: 1.179869294166565, Training Accuracy: 73.15937500000001
[ Wed Jul  3 01:03:11 2024 ] 	Batch(4000/7879) done. Loss: 0.6723  lr:0.010000
[ Wed Jul  3 01:03:30 2024 ] 	Batch(4100/7879) done. Loss: 0.7406  lr:0.010000
[ Wed Jul  3 01:03:48 2024 ] 	Batch(4200/7879) done. Loss: 1.1186  lr:0.010000
[ Wed Jul  3 01:04:06 2024 ] 	Batch(4300/7879) done. Loss: 1.0347  lr:0.010000
[ Wed Jul  3 01:04:24 2024 ] 	Batch(4400/7879) done. Loss: 0.4719  lr:0.010000
[ Wed Jul  3 01:04:42 2024 ] 
Training: Epoch [9/120], Step [4499], Loss: 1.6065342426300049, Training Accuracy: 73.21944444444445
[ Wed Jul  3 01:04:42 2024 ] 	Batch(4500/7879) done. Loss: 0.8349  lr:0.010000
[ Wed Jul  3 01:05:00 2024 ] 	Batch(4600/7879) done. Loss: 1.5470  lr:0.010000
[ Wed Jul  3 01:05:18 2024 ] 	Batch(4700/7879) done. Loss: 1.1346  lr:0.010000
[ Wed Jul  3 01:05:36 2024 ] 	Batch(4800/7879) done. Loss: 0.4651  lr:0.010000
[ Wed Jul  3 01:05:54 2024 ] 	Batch(4900/7879) done. Loss: 0.6506  lr:0.010000
[ Wed Jul  3 01:06:12 2024 ] 
Training: Epoch [9/120], Step [4999], Loss: 0.44729453325271606, Training Accuracy: 73.19250000000001
[ Wed Jul  3 01:06:12 2024 ] 	Batch(5000/7879) done. Loss: 0.3832  lr:0.010000
[ Wed Jul  3 01:06:30 2024 ] 	Batch(5100/7879) done. Loss: 1.3346  lr:0.010000
[ Wed Jul  3 01:06:48 2024 ] 	Batch(5200/7879) done. Loss: 0.9587  lr:0.010000
[ Wed Jul  3 01:07:06 2024 ] 	Batch(5300/7879) done. Loss: 1.2335  lr:0.010000
[ Wed Jul  3 01:07:23 2024 ] 	Batch(5400/7879) done. Loss: 0.8798  lr:0.010000
[ Wed Jul  3 01:07:41 2024 ] 
Training: Epoch [9/120], Step [5499], Loss: 1.129744052886963, Training Accuracy: 73.175
[ Wed Jul  3 01:07:41 2024 ] 	Batch(5500/7879) done. Loss: 1.7219  lr:0.010000
[ Wed Jul  3 01:07:59 2024 ] 	Batch(5600/7879) done. Loss: 0.6297  lr:0.010000
[ Wed Jul  3 01:08:17 2024 ] 	Batch(5700/7879) done. Loss: 1.3454  lr:0.010000
[ Wed Jul  3 01:08:35 2024 ] 	Batch(5800/7879) done. Loss: 0.4901  lr:0.010000
[ Wed Jul  3 01:08:53 2024 ] 	Batch(5900/7879) done. Loss: 1.2246  lr:0.010000
[ Wed Jul  3 01:09:11 2024 ] 
Training: Epoch [9/120], Step [5999], Loss: 0.2974626421928406, Training Accuracy: 73.19375000000001
[ Wed Jul  3 01:09:11 2024 ] 	Batch(6000/7879) done. Loss: 0.2554  lr:0.010000
[ Wed Jul  3 01:09:30 2024 ] 	Batch(6100/7879) done. Loss: 0.8656  lr:0.010000
[ Wed Jul  3 01:09:48 2024 ] 	Batch(6200/7879) done. Loss: 0.7850  lr:0.010000
[ Wed Jul  3 01:10:07 2024 ] 	Batch(6300/7879) done. Loss: 0.9640  lr:0.010000
[ Wed Jul  3 01:10:25 2024 ] 	Batch(6400/7879) done. Loss: 0.3035  lr:0.010000
[ Wed Jul  3 01:10:44 2024 ] 
Training: Epoch [9/120], Step [6499], Loss: 1.0968111753463745, Training Accuracy: 73.26346153846154
[ Wed Jul  3 01:10:44 2024 ] 	Batch(6500/7879) done. Loss: 0.4463  lr:0.010000
[ Wed Jul  3 01:11:02 2024 ] 	Batch(6600/7879) done. Loss: 2.2208  lr:0.010000
[ Wed Jul  3 01:11:21 2024 ] 	Batch(6700/7879) done. Loss: 0.8585  lr:0.010000
[ Wed Jul  3 01:11:40 2024 ] 	Batch(6800/7879) done. Loss: 0.7514  lr:0.010000
[ Wed Jul  3 01:11:58 2024 ] 	Batch(6900/7879) done. Loss: 0.3477  lr:0.010000
[ Wed Jul  3 01:12:17 2024 ] 
Training: Epoch [9/120], Step [6999], Loss: 0.6744521260261536, Training Accuracy: 73.40357142857142
[ Wed Jul  3 01:12:17 2024 ] 	Batch(7000/7879) done. Loss: 0.8736  lr:0.010000
[ Wed Jul  3 01:12:35 2024 ] 	Batch(7100/7879) done. Loss: 0.3866  lr:0.010000
[ Wed Jul  3 01:12:53 2024 ] 	Batch(7200/7879) done. Loss: 0.8347  lr:0.010000
[ Wed Jul  3 01:13:11 2024 ] 	Batch(7300/7879) done. Loss: 0.8268  lr:0.010000
[ Wed Jul  3 01:13:29 2024 ] 	Batch(7400/7879) done. Loss: 0.8236  lr:0.010000
[ Wed Jul  3 01:13:47 2024 ] 
Training: Epoch [9/120], Step [7499], Loss: 0.9103221893310547, Training Accuracy: 73.41333333333333
[ Wed Jul  3 01:13:47 2024 ] 	Batch(7500/7879) done. Loss: 0.9959  lr:0.010000
[ Wed Jul  3 01:14:05 2024 ] 	Batch(7600/7879) done. Loss: 1.6842  lr:0.010000
[ Wed Jul  3 01:14:23 2024 ] 	Batch(7700/7879) done. Loss: 0.9514  lr:0.010000
[ Wed Jul  3 01:14:41 2024 ] 	Batch(7800/7879) done. Loss: 0.3846  lr:0.010000
[ Wed Jul  3 01:14:55 2024 ] 	Mean training loss: 0.9091.
[ Wed Jul  3 01:14:55 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 01:14:55 2024 ] Eval epoch: 10
[ Wed Jul  3 01:19:41 2024 ] 	Mean val loss of 6365 batches: 2.6033881356548725.
[ Wed Jul  3 01:19:42 2024 ] Training epoch: 11
[ Wed Jul  3 01:19:42 2024 ] 	Batch(0/7879) done. Loss: 1.4385  lr:0.010000
[ Wed Jul  3 01:20:00 2024 ] 	Batch(100/7879) done. Loss: 1.1478  lr:0.010000
[ Wed Jul  3 01:20:18 2024 ] 	Batch(200/7879) done. Loss: 0.3707  lr:0.010000
[ Wed Jul  3 01:20:37 2024 ] 	Batch(300/7879) done. Loss: 0.6968  lr:0.010000
[ Wed Jul  3 01:20:55 2024 ] 	Batch(400/7879) done. Loss: 0.9576  lr:0.010000
[ Wed Jul  3 01:21:13 2024 ] 
Training: Epoch [10/120], Step [499], Loss: 0.7102037072181702, Training Accuracy: 74.95
[ Wed Jul  3 01:21:13 2024 ] 	Batch(500/7879) done. Loss: 0.8688  lr:0.010000
[ Wed Jul  3 01:21:31 2024 ] 	Batch(600/7879) done. Loss: 0.9562  lr:0.010000
[ Wed Jul  3 01:21:49 2024 ] 	Batch(700/7879) done. Loss: 0.1738  lr:0.010000
[ Wed Jul  3 01:22:07 2024 ] 	Batch(800/7879) done. Loss: 0.1485  lr:0.010000
[ Wed Jul  3 01:22:24 2024 ] 	Batch(900/7879) done. Loss: 1.0588  lr:0.010000
[ Wed Jul  3 01:22:42 2024 ] 
Training: Epoch [10/120], Step [999], Loss: 0.596008837223053, Training Accuracy: 75.275
[ Wed Jul  3 01:22:42 2024 ] 	Batch(1000/7879) done. Loss: 0.9556  lr:0.010000
[ Wed Jul  3 01:23:00 2024 ] 	Batch(1100/7879) done. Loss: 0.5489  lr:0.010000
[ Wed Jul  3 01:23:18 2024 ] 	Batch(1200/7879) done. Loss: 0.1269  lr:0.010000
[ Wed Jul  3 01:23:37 2024 ] 	Batch(1300/7879) done. Loss: 0.8588  lr:0.010000
[ Wed Jul  3 01:23:54 2024 ] 	Batch(1400/7879) done. Loss: 0.9786  lr:0.010000
[ Wed Jul  3 01:24:12 2024 ] 
Training: Epoch [10/120], Step [1499], Loss: 0.3118836283683777, Training Accuracy: 75.28333333333333
[ Wed Jul  3 01:24:12 2024 ] 	Batch(1500/7879) done. Loss: 0.5875  lr:0.010000
[ Wed Jul  3 01:24:30 2024 ] 	Batch(1600/7879) done. Loss: 1.0533  lr:0.010000
[ Wed Jul  3 01:24:48 2024 ] 	Batch(1700/7879) done. Loss: 0.3284  lr:0.010000
[ Wed Jul  3 01:25:06 2024 ] 	Batch(1800/7879) done. Loss: 0.4338  lr:0.010000
[ Wed Jul  3 01:25:24 2024 ] 	Batch(1900/7879) done. Loss: 0.8186  lr:0.010000
[ Wed Jul  3 01:25:42 2024 ] 
Training: Epoch [10/120], Step [1999], Loss: 1.0398104190826416, Training Accuracy: 75.18125
[ Wed Jul  3 01:25:42 2024 ] 	Batch(2000/7879) done. Loss: 1.3331  lr:0.010000
[ Wed Jul  3 01:26:00 2024 ] 	Batch(2100/7879) done. Loss: 0.6005  lr:0.010000
[ Wed Jul  3 01:26:18 2024 ] 	Batch(2200/7879) done. Loss: 0.7444  lr:0.010000
[ Wed Jul  3 01:26:35 2024 ] 	Batch(2300/7879) done. Loss: 0.8805  lr:0.010000
[ Wed Jul  3 01:26:53 2024 ] 	Batch(2400/7879) done. Loss: 0.7564  lr:0.010000
[ Wed Jul  3 01:27:12 2024 ] 
Training: Epoch [10/120], Step [2499], Loss: 0.9162399768829346, Training Accuracy: 75.14
[ Wed Jul  3 01:27:12 2024 ] 	Batch(2500/7879) done. Loss: 0.1943  lr:0.010000
[ Wed Jul  3 01:27:30 2024 ] 	Batch(2600/7879) done. Loss: 0.2678  lr:0.010000
[ Wed Jul  3 01:27:49 2024 ] 	Batch(2700/7879) done. Loss: 2.0589  lr:0.010000
[ Wed Jul  3 01:28:08 2024 ] 	Batch(2800/7879) done. Loss: 0.7275  lr:0.010000
[ Wed Jul  3 01:28:26 2024 ] 	Batch(2900/7879) done. Loss: 1.0024  lr:0.010000
[ Wed Jul  3 01:28:44 2024 ] 
Training: Epoch [10/120], Step [2999], Loss: 0.7976024746894836, Training Accuracy: 75.0125
[ Wed Jul  3 01:28:45 2024 ] 	Batch(3000/7879) done. Loss: 0.9183  lr:0.010000
[ Wed Jul  3 01:29:03 2024 ] 	Batch(3100/7879) done. Loss: 0.6460  lr:0.010000
[ Wed Jul  3 01:29:22 2024 ] 	Batch(3200/7879) done. Loss: 0.6609  lr:0.010000
[ Wed Jul  3 01:29:40 2024 ] 	Batch(3300/7879) done. Loss: 1.0793  lr:0.010000
[ Wed Jul  3 01:29:57 2024 ] 	Batch(3400/7879) done. Loss: 0.5467  lr:0.010000
[ Wed Jul  3 01:30:15 2024 ] 
Training: Epoch [10/120], Step [3499], Loss: 0.8513944745063782, Training Accuracy: 74.80714285714286
[ Wed Jul  3 01:30:15 2024 ] 	Batch(3500/7879) done. Loss: 1.5197  lr:0.010000
[ Wed Jul  3 01:30:33 2024 ] 	Batch(3600/7879) done. Loss: 0.4921  lr:0.010000
[ Wed Jul  3 01:30:52 2024 ] 	Batch(3700/7879) done. Loss: 0.1050  lr:0.010000
[ Wed Jul  3 01:31:10 2024 ] 	Batch(3800/7879) done. Loss: 1.0446  lr:0.010000
[ Wed Jul  3 01:31:29 2024 ] 	Batch(3900/7879) done. Loss: 1.0317  lr:0.010000
[ Wed Jul  3 01:31:47 2024 ] 
Training: Epoch [10/120], Step [3999], Loss: 0.37210261821746826, Training Accuracy: 74.575
[ Wed Jul  3 01:31:48 2024 ] 	Batch(4000/7879) done. Loss: 0.9855  lr:0.010000
[ Wed Jul  3 01:32:05 2024 ] 	Batch(4100/7879) done. Loss: 0.8379  lr:0.010000
[ Wed Jul  3 01:32:23 2024 ] 	Batch(4200/7879) done. Loss: 0.2980  lr:0.010000
[ Wed Jul  3 01:32:42 2024 ] 	Batch(4300/7879) done. Loss: 0.6178  lr:0.010000
[ Wed Jul  3 01:33:01 2024 ] 	Batch(4400/7879) done. Loss: 0.3234  lr:0.010000
[ Wed Jul  3 01:33:19 2024 ] 
Training: Epoch [10/120], Step [4499], Loss: 0.7713140249252319, Training Accuracy: 74.67222222222222
[ Wed Jul  3 01:33:19 2024 ] 	Batch(4500/7879) done. Loss: 1.2413  lr:0.010000
[ Wed Jul  3 01:33:38 2024 ] 	Batch(4600/7879) done. Loss: 0.5762  lr:0.010000
[ Wed Jul  3 01:33:56 2024 ] 	Batch(4700/7879) done. Loss: 0.5123  lr:0.010000
[ Wed Jul  3 01:34:15 2024 ] 	Batch(4800/7879) done. Loss: 0.1385  lr:0.010000
[ Wed Jul  3 01:34:33 2024 ] 	Batch(4900/7879) done. Loss: 0.7311  lr:0.010000
[ Wed Jul  3 01:34:52 2024 ] 
Training: Epoch [10/120], Step [4999], Loss: 1.1214135885238647, Training Accuracy: 74.65
[ Wed Jul  3 01:34:52 2024 ] 	Batch(5000/7879) done. Loss: 0.3998  lr:0.010000
[ Wed Jul  3 01:35:11 2024 ] 	Batch(5100/7879) done. Loss: 0.6310  lr:0.010000
[ Wed Jul  3 01:35:29 2024 ] 	Batch(5200/7879) done. Loss: 0.6793  lr:0.010000
[ Wed Jul  3 01:35:48 2024 ] 	Batch(5300/7879) done. Loss: 1.9136  lr:0.010000
[ Wed Jul  3 01:36:06 2024 ] 	Batch(5400/7879) done. Loss: 0.7541  lr:0.010000
[ Wed Jul  3 01:36:25 2024 ] 
Training: Epoch [10/120], Step [5499], Loss: 0.9847073554992676, Training Accuracy: 74.68181818181819
[ Wed Jul  3 01:36:25 2024 ] 	Batch(5500/7879) done. Loss: 0.9743  lr:0.010000
[ Wed Jul  3 01:36:43 2024 ] 	Batch(5600/7879) done. Loss: 0.8359  lr:0.010000
[ Wed Jul  3 01:37:01 2024 ] 	Batch(5700/7879) done. Loss: 0.9697  lr:0.010000
[ Wed Jul  3 01:37:19 2024 ] 	Batch(5800/7879) done. Loss: 0.8231  lr:0.010000
[ Wed Jul  3 01:37:37 2024 ] 	Batch(5900/7879) done. Loss: 0.3374  lr:0.010000
[ Wed Jul  3 01:37:55 2024 ] 
Training: Epoch [10/120], Step [5999], Loss: 0.20726290345191956, Training Accuracy: 74.64791666666667
[ Wed Jul  3 01:37:55 2024 ] 	Batch(6000/7879) done. Loss: 1.5540  lr:0.010000
[ Wed Jul  3 01:38:13 2024 ] 	Batch(6100/7879) done. Loss: 0.8821  lr:0.010000
[ Wed Jul  3 01:38:31 2024 ] 	Batch(6200/7879) done. Loss: 0.4611  lr:0.010000
[ Wed Jul  3 01:38:49 2024 ] 	Batch(6300/7879) done. Loss: 0.9807  lr:0.010000
[ Wed Jul  3 01:39:08 2024 ] 	Batch(6400/7879) done. Loss: 1.4868  lr:0.010000
[ Wed Jul  3 01:39:26 2024 ] 
Training: Epoch [10/120], Step [6499], Loss: 1.1969029903411865, Training Accuracy: 74.64615384615385
[ Wed Jul  3 01:39:26 2024 ] 	Batch(6500/7879) done. Loss: 0.9590  lr:0.010000
[ Wed Jul  3 01:39:44 2024 ] 	Batch(6600/7879) done. Loss: 0.5890  lr:0.010000
[ Wed Jul  3 01:40:02 2024 ] 	Batch(6700/7879) done. Loss: 0.3133  lr:0.010000
[ Wed Jul  3 01:40:20 2024 ] 	Batch(6800/7879) done. Loss: 0.3873  lr:0.010000
[ Wed Jul  3 01:40:38 2024 ] 	Batch(6900/7879) done. Loss: 0.5191  lr:0.010000
[ Wed Jul  3 01:40:56 2024 ] 
Training: Epoch [10/120], Step [6999], Loss: 0.26038891077041626, Training Accuracy: 74.6375
[ Wed Jul  3 01:40:57 2024 ] 	Batch(7000/7879) done. Loss: 0.2184  lr:0.010000
[ Wed Jul  3 01:41:15 2024 ] 	Batch(7100/7879) done. Loss: 0.1834  lr:0.010000
[ Wed Jul  3 01:41:34 2024 ] 	Batch(7200/7879) done. Loss: 1.2698  lr:0.010000
[ Wed Jul  3 01:41:52 2024 ] 	Batch(7300/7879) done. Loss: 0.8089  lr:0.010000
[ Wed Jul  3 01:42:11 2024 ] 	Batch(7400/7879) done. Loss: 1.9205  lr:0.010000
[ Wed Jul  3 01:42:29 2024 ] 
Training: Epoch [10/120], Step [7499], Loss: 0.9061441421508789, Training Accuracy: 74.69333333333333
[ Wed Jul  3 01:42:29 2024 ] 	Batch(7500/7879) done. Loss: 0.9245  lr:0.010000
[ Wed Jul  3 01:42:47 2024 ] 	Batch(7600/7879) done. Loss: 1.0753  lr:0.010000
[ Wed Jul  3 01:43:05 2024 ] 	Batch(7700/7879) done. Loss: 0.2514  lr:0.010000
[ Wed Jul  3 01:43:23 2024 ] 	Batch(7800/7879) done. Loss: 0.4611  lr:0.010000
[ Wed Jul  3 01:43:37 2024 ] 	Mean training loss: 0.8585.
[ Wed Jul  3 01:43:37 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 01:43:37 2024 ] Training epoch: 12
[ Wed Jul  3 01:43:38 2024 ] 	Batch(0/7879) done. Loss: 0.5989  lr:0.010000
[ Wed Jul  3 01:43:56 2024 ] 	Batch(100/7879) done. Loss: 0.5040  lr:0.010000
[ Wed Jul  3 01:44:13 2024 ] 	Batch(200/7879) done. Loss: 0.5271  lr:0.010000
[ Wed Jul  3 01:44:31 2024 ] 	Batch(300/7879) done. Loss: 0.2735  lr:0.010000
[ Wed Jul  3 01:44:49 2024 ] 	Batch(400/7879) done. Loss: 0.2693  lr:0.010000
[ Wed Jul  3 01:45:07 2024 ] 
Training: Epoch [11/120], Step [499], Loss: 1.4250848293304443, Training Accuracy: 75.3
[ Wed Jul  3 01:45:07 2024 ] 	Batch(500/7879) done. Loss: 0.2372  lr:0.010000
[ Wed Jul  3 01:45:25 2024 ] 	Batch(600/7879) done. Loss: 0.9979  lr:0.010000
[ Wed Jul  3 01:45:43 2024 ] 	Batch(700/7879) done. Loss: 0.4323  lr:0.010000
[ Wed Jul  3 01:46:01 2024 ] 	Batch(800/7879) done. Loss: 0.8991  lr:0.010000
[ Wed Jul  3 01:46:20 2024 ] 	Batch(900/7879) done. Loss: 0.4332  lr:0.010000
[ Wed Jul  3 01:46:38 2024 ] 
Training: Epoch [11/120], Step [999], Loss: 1.0948641300201416, Training Accuracy: 75.94999999999999
[ Wed Jul  3 01:46:38 2024 ] 	Batch(1000/7879) done. Loss: 0.7008  lr:0.010000
[ Wed Jul  3 01:46:57 2024 ] 	Batch(1100/7879) done. Loss: 0.2243  lr:0.010000
[ Wed Jul  3 01:47:15 2024 ] 	Batch(1200/7879) done. Loss: 0.7533  lr:0.010000
[ Wed Jul  3 01:47:34 2024 ] 	Batch(1300/7879) done. Loss: 0.5721  lr:0.010000
[ Wed Jul  3 01:47:51 2024 ] 	Batch(1400/7879) done. Loss: 0.4601  lr:0.010000
[ Wed Jul  3 01:48:09 2024 ] 
Training: Epoch [11/120], Step [1499], Loss: 0.636689305305481, Training Accuracy: 76.10833333333333
[ Wed Jul  3 01:48:09 2024 ] 	Batch(1500/7879) done. Loss: 0.5053  lr:0.010000
[ Wed Jul  3 01:48:27 2024 ] 	Batch(1600/7879) done. Loss: 0.5054  lr:0.010000
[ Wed Jul  3 01:48:45 2024 ] 	Batch(1700/7879) done. Loss: 0.3763  lr:0.010000
[ Wed Jul  3 01:49:03 2024 ] 	Batch(1800/7879) done. Loss: 0.8960  lr:0.010000
[ Wed Jul  3 01:49:21 2024 ] 	Batch(1900/7879) done. Loss: 1.1734  lr:0.010000
[ Wed Jul  3 01:49:39 2024 ] 
Training: Epoch [11/120], Step [1999], Loss: 0.9493945837020874, Training Accuracy: 76.03750000000001
[ Wed Jul  3 01:49:39 2024 ] 	Batch(2000/7879) done. Loss: 1.1781  lr:0.010000
[ Wed Jul  3 01:49:57 2024 ] 	Batch(2100/7879) done. Loss: 0.4174  lr:0.010000
[ Wed Jul  3 01:50:15 2024 ] 	Batch(2200/7879) done. Loss: 0.5980  lr:0.010000
[ Wed Jul  3 01:50:33 2024 ] 	Batch(2300/7879) done. Loss: 0.6587  lr:0.010000
[ Wed Jul  3 01:50:51 2024 ] 	Batch(2400/7879) done. Loss: 0.6817  lr:0.010000
[ Wed Jul  3 01:51:09 2024 ] 
Training: Epoch [11/120], Step [2499], Loss: 0.4810553789138794, Training Accuracy: 76.08
[ Wed Jul  3 01:51:09 2024 ] 	Batch(2500/7879) done. Loss: 0.8260  lr:0.010000
[ Wed Jul  3 01:51:27 2024 ] 	Batch(2600/7879) done. Loss: 0.2112  lr:0.010000
[ Wed Jul  3 01:51:45 2024 ] 	Batch(2700/7879) done. Loss: 0.5464  lr:0.010000
[ Wed Jul  3 01:52:03 2024 ] 	Batch(2800/7879) done. Loss: 1.1702  lr:0.010000
[ Wed Jul  3 01:52:21 2024 ] 	Batch(2900/7879) done. Loss: 0.6136  lr:0.010000
[ Wed Jul  3 01:52:38 2024 ] 
Training: Epoch [11/120], Step [2999], Loss: 0.6969687938690186, Training Accuracy: 75.97916666666666
[ Wed Jul  3 01:52:39 2024 ] 	Batch(3000/7879) done. Loss: 1.4476  lr:0.010000
[ Wed Jul  3 01:52:57 2024 ] 	Batch(3100/7879) done. Loss: 0.1338  lr:0.010000
[ Wed Jul  3 01:53:15 2024 ] 	Batch(3200/7879) done. Loss: 1.0630  lr:0.010000
[ Wed Jul  3 01:53:32 2024 ] 	Batch(3300/7879) done. Loss: 0.4621  lr:0.010000
[ Wed Jul  3 01:53:50 2024 ] 	Batch(3400/7879) done. Loss: 0.1316  lr:0.010000
[ Wed Jul  3 01:54:08 2024 ] 
Training: Epoch [11/120], Step [3499], Loss: 0.20220771431922913, Training Accuracy: 76.04285714285714
[ Wed Jul  3 01:54:08 2024 ] 	Batch(3500/7879) done. Loss: 1.0756  lr:0.010000
[ Wed Jul  3 01:54:26 2024 ] 	Batch(3600/7879) done. Loss: 1.0837  lr:0.010000
[ Wed Jul  3 01:54:44 2024 ] 	Batch(3700/7879) done. Loss: 0.6663  lr:0.010000
[ Wed Jul  3 01:55:02 2024 ] 	Batch(3800/7879) done. Loss: 1.0748  lr:0.010000
[ Wed Jul  3 01:55:20 2024 ] 	Batch(3900/7879) done. Loss: 0.5393  lr:0.010000
[ Wed Jul  3 01:55:37 2024 ] 
Training: Epoch [11/120], Step [3999], Loss: 0.40992483496665955, Training Accuracy: 76.009375
[ Wed Jul  3 01:55:38 2024 ] 	Batch(4000/7879) done. Loss: 0.8748  lr:0.010000
[ Wed Jul  3 01:55:56 2024 ] 	Batch(4100/7879) done. Loss: 0.2264  lr:0.010000
[ Wed Jul  3 01:56:14 2024 ] 	Batch(4200/7879) done. Loss: 1.6493  lr:0.010000
[ Wed Jul  3 01:56:32 2024 ] 	Batch(4300/7879) done. Loss: 0.9568  lr:0.010000
[ Wed Jul  3 01:56:50 2024 ] 	Batch(4400/7879) done. Loss: 1.5721  lr:0.010000
[ Wed Jul  3 01:57:07 2024 ] 
Training: Epoch [11/120], Step [4499], Loss: 1.3589556217193604, Training Accuracy: 76.05555555555556
[ Wed Jul  3 01:57:07 2024 ] 	Batch(4500/7879) done. Loss: 0.2668  lr:0.010000
[ Wed Jul  3 01:57:25 2024 ] 	Batch(4600/7879) done. Loss: 0.2887  lr:0.010000
[ Wed Jul  3 01:57:43 2024 ] 	Batch(4700/7879) done. Loss: 1.1982  lr:0.010000
[ Wed Jul  3 01:58:01 2024 ] 	Batch(4800/7879) done. Loss: 0.4603  lr:0.010000
[ Wed Jul  3 01:58:19 2024 ] 	Batch(4900/7879) done. Loss: 0.6200  lr:0.010000
[ Wed Jul  3 01:58:37 2024 ] 
Training: Epoch [11/120], Step [4999], Loss: 0.6547950506210327, Training Accuracy: 76.125
[ Wed Jul  3 01:58:37 2024 ] 	Batch(5000/7879) done. Loss: 1.0211  lr:0.010000
[ Wed Jul  3 01:58:55 2024 ] 	Batch(5100/7879) done. Loss: 0.4491  lr:0.010000
[ Wed Jul  3 01:59:13 2024 ] 	Batch(5200/7879) done. Loss: 0.1436  lr:0.010000
[ Wed Jul  3 01:59:31 2024 ] 	Batch(5300/7879) done. Loss: 0.9719  lr:0.010000
[ Wed Jul  3 01:59:49 2024 ] 	Batch(5400/7879) done. Loss: 0.7970  lr:0.010000
[ Wed Jul  3 02:00:06 2024 ] 
Training: Epoch [11/120], Step [5499], Loss: 1.3330882787704468, Training Accuracy: 76.02499999999999
[ Wed Jul  3 02:00:07 2024 ] 	Batch(5500/7879) done. Loss: 1.4404  lr:0.010000
[ Wed Jul  3 02:00:25 2024 ] 	Batch(5600/7879) done. Loss: 0.5893  lr:0.010000
[ Wed Jul  3 02:00:43 2024 ] 	Batch(5700/7879) done. Loss: 0.6127  lr:0.010000
[ Wed Jul  3 02:01:01 2024 ] 	Batch(5800/7879) done. Loss: 0.2226  lr:0.010000
[ Wed Jul  3 02:01:19 2024 ] 	Batch(5900/7879) done. Loss: 0.8381  lr:0.010000
[ Wed Jul  3 02:01:37 2024 ] 
Training: Epoch [11/120], Step [5999], Loss: 0.9921226501464844, Training Accuracy: 75.93333333333334
[ Wed Jul  3 02:01:37 2024 ] 	Batch(6000/7879) done. Loss: 1.5550  lr:0.010000
[ Wed Jul  3 02:01:55 2024 ] 	Batch(6100/7879) done. Loss: 1.6446  lr:0.010000
[ Wed Jul  3 02:02:13 2024 ] 	Batch(6200/7879) done. Loss: 0.3822  lr:0.010000
[ Wed Jul  3 02:02:31 2024 ] 	Batch(6300/7879) done. Loss: 0.4926  lr:0.010000
[ Wed Jul  3 02:02:49 2024 ] 	Batch(6400/7879) done. Loss: 0.1594  lr:0.010000
[ Wed Jul  3 02:03:06 2024 ] 
Training: Epoch [11/120], Step [6499], Loss: 0.2904887795448303, Training Accuracy: 75.925
[ Wed Jul  3 02:03:07 2024 ] 	Batch(6500/7879) done. Loss: 0.3753  lr:0.010000
[ Wed Jul  3 02:03:24 2024 ] 	Batch(6600/7879) done. Loss: 2.0422  lr:0.010000
[ Wed Jul  3 02:03:42 2024 ] 	Batch(6700/7879) done. Loss: 0.3431  lr:0.010000
[ Wed Jul  3 02:04:00 2024 ] 	Batch(6800/7879) done. Loss: 2.0931  lr:0.010000
[ Wed Jul  3 02:04:18 2024 ] 	Batch(6900/7879) done. Loss: 0.4219  lr:0.010000
[ Wed Jul  3 02:04:36 2024 ] 
Training: Epoch [11/120], Step [6999], Loss: 0.5111369490623474, Training Accuracy: 75.87321428571428
[ Wed Jul  3 02:04:36 2024 ] 	Batch(7000/7879) done. Loss: 0.4498  lr:0.010000
[ Wed Jul  3 02:04:54 2024 ] 	Batch(7100/7879) done. Loss: 0.8483  lr:0.010000
[ Wed Jul  3 02:05:12 2024 ] 	Batch(7200/7879) done. Loss: 1.2145  lr:0.010000
[ Wed Jul  3 02:05:30 2024 ] 	Batch(7300/7879) done. Loss: 1.1912  lr:0.010000
[ Wed Jul  3 02:05:48 2024 ] 	Batch(7400/7879) done. Loss: 0.6292  lr:0.010000
[ Wed Jul  3 02:06:06 2024 ] 
Training: Epoch [11/120], Step [7499], Loss: 2.1644904613494873, Training Accuracy: 75.91333333333333
[ Wed Jul  3 02:06:06 2024 ] 	Batch(7500/7879) done. Loss: 0.4493  lr:0.010000
[ Wed Jul  3 02:06:24 2024 ] 	Batch(7600/7879) done. Loss: 1.0808  lr:0.010000
[ Wed Jul  3 02:06:42 2024 ] 	Batch(7700/7879) done. Loss: 1.0879  lr:0.010000
[ Wed Jul  3 02:07:00 2024 ] 	Batch(7800/7879) done. Loss: 0.5535  lr:0.010000
[ Wed Jul  3 02:07:14 2024 ] 	Mean training loss: 0.8104.
[ Wed Jul  3 02:07:14 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 02:07:14 2024 ] Training epoch: 13
[ Wed Jul  3 02:07:15 2024 ] 	Batch(0/7879) done. Loss: 0.7540  lr:0.010000
[ Wed Jul  3 02:07:33 2024 ] 	Batch(100/7879) done. Loss: 0.6645  lr:0.010000
[ Wed Jul  3 02:07:50 2024 ] 	Batch(200/7879) done. Loss: 0.0687  lr:0.010000
[ Wed Jul  3 02:08:08 2024 ] 	Batch(300/7879) done. Loss: 0.3180  lr:0.010000
[ Wed Jul  3 02:08:26 2024 ] 	Batch(400/7879) done. Loss: 0.8032  lr:0.010000
[ Wed Jul  3 02:08:44 2024 ] 
Training: Epoch [12/120], Step [499], Loss: 0.6072484850883484, Training Accuracy: 76.97500000000001
[ Wed Jul  3 02:08:44 2024 ] 	Batch(500/7879) done. Loss: 0.7284  lr:0.010000
[ Wed Jul  3 02:09:02 2024 ] 	Batch(600/7879) done. Loss: 0.8043  lr:0.010000
[ Wed Jul  3 02:09:20 2024 ] 	Batch(700/7879) done. Loss: 0.5976  lr:0.010000
[ Wed Jul  3 02:09:38 2024 ] 	Batch(800/7879) done. Loss: 0.3737  lr:0.010000
[ Wed Jul  3 02:09:56 2024 ] 	Batch(900/7879) done. Loss: 1.5973  lr:0.010000
[ Wed Jul  3 02:10:14 2024 ] 
Training: Epoch [12/120], Step [999], Loss: 0.5600886940956116, Training Accuracy: 76.875
[ Wed Jul  3 02:10:14 2024 ] 	Batch(1000/7879) done. Loss: 0.4920  lr:0.010000
[ Wed Jul  3 02:10:32 2024 ] 	Batch(1100/7879) done. Loss: 0.3973  lr:0.010000
[ Wed Jul  3 02:10:50 2024 ] 	Batch(1200/7879) done. Loss: 0.2543  lr:0.010000
[ Wed Jul  3 02:11:08 2024 ] 	Batch(1300/7879) done. Loss: 0.8717  lr:0.010000
[ Wed Jul  3 02:11:26 2024 ] 	Batch(1400/7879) done. Loss: 0.6353  lr:0.010000
[ Wed Jul  3 02:11:43 2024 ] 
Training: Epoch [12/120], Step [1499], Loss: 0.3808761239051819, Training Accuracy: 76.69166666666666
[ Wed Jul  3 02:11:43 2024 ] 	Batch(1500/7879) done. Loss: 1.0373  lr:0.010000
[ Wed Jul  3 02:12:01 2024 ] 	Batch(1600/7879) done. Loss: 0.9232  lr:0.010000
[ Wed Jul  3 02:12:19 2024 ] 	Batch(1700/7879) done. Loss: 0.9542  lr:0.010000
[ Wed Jul  3 02:12:37 2024 ] 	Batch(1800/7879) done. Loss: 0.4383  lr:0.010000
[ Wed Jul  3 02:12:55 2024 ] 	Batch(1900/7879) done. Loss: 0.6007  lr:0.010000
[ Wed Jul  3 02:13:13 2024 ] 
Training: Epoch [12/120], Step [1999], Loss: 0.7798614501953125, Training Accuracy: 76.625
[ Wed Jul  3 02:13:13 2024 ] 	Batch(2000/7879) done. Loss: 0.4491  lr:0.010000
[ Wed Jul  3 02:13:31 2024 ] 	Batch(2100/7879) done. Loss: 0.3394  lr:0.010000
[ Wed Jul  3 02:13:50 2024 ] 	Batch(2200/7879) done. Loss: 0.3949  lr:0.010000
[ Wed Jul  3 02:14:09 2024 ] 	Batch(2300/7879) done. Loss: 0.6810  lr:0.010000
[ Wed Jul  3 02:14:27 2024 ] 	Batch(2400/7879) done. Loss: 0.4739  lr:0.010000
[ Wed Jul  3 02:14:45 2024 ] 
Training: Epoch [12/120], Step [2499], Loss: 0.3769955039024353, Training Accuracy: 76.85499999999999
[ Wed Jul  3 02:14:45 2024 ] 	Batch(2500/7879) done. Loss: 1.6416  lr:0.010000
[ Wed Jul  3 02:15:03 2024 ] 	Batch(2600/7879) done. Loss: 0.6067  lr:0.010000
[ Wed Jul  3 02:15:21 2024 ] 	Batch(2700/7879) done. Loss: 0.9448  lr:0.010000
[ Wed Jul  3 02:15:39 2024 ] 	Batch(2800/7879) done. Loss: 0.4103  lr:0.010000
[ Wed Jul  3 02:15:57 2024 ] 	Batch(2900/7879) done. Loss: 0.9668  lr:0.010000
[ Wed Jul  3 02:16:15 2024 ] 
Training: Epoch [12/120], Step [2999], Loss: 0.3896533250808716, Training Accuracy: 76.79166666666667
[ Wed Jul  3 02:16:15 2024 ] 	Batch(3000/7879) done. Loss: 1.0090  lr:0.010000
[ Wed Jul  3 02:16:33 2024 ] 	Batch(3100/7879) done. Loss: 1.8652  lr:0.010000
[ Wed Jul  3 02:16:51 2024 ] 	Batch(3200/7879) done. Loss: 1.2642  lr:0.010000
[ Wed Jul  3 02:17:09 2024 ] 	Batch(3300/7879) done. Loss: 0.3176  lr:0.010000
[ Wed Jul  3 02:17:27 2024 ] 	Batch(3400/7879) done. Loss: 1.5617  lr:0.010000
[ Wed Jul  3 02:17:45 2024 ] 
Training: Epoch [12/120], Step [3499], Loss: 1.0951883792877197, Training Accuracy: 76.62857142857142
[ Wed Jul  3 02:17:45 2024 ] 	Batch(3500/7879) done. Loss: 1.5344  lr:0.010000
[ Wed Jul  3 02:18:03 2024 ] 	Batch(3600/7879) done. Loss: 0.7714  lr:0.010000
[ Wed Jul  3 02:18:21 2024 ] 	Batch(3700/7879) done. Loss: 0.9338  lr:0.010000
[ Wed Jul  3 02:18:39 2024 ] 	Batch(3800/7879) done. Loss: 1.0145  lr:0.010000
[ Wed Jul  3 02:18:57 2024 ] 	Batch(3900/7879) done. Loss: 0.3724  lr:0.010000
[ Wed Jul  3 02:19:15 2024 ] 
Training: Epoch [12/120], Step [3999], Loss: 0.45698997378349304, Training Accuracy: 76.6125
[ Wed Jul  3 02:19:15 2024 ] 	Batch(4000/7879) done. Loss: 0.1014  lr:0.010000
[ Wed Jul  3 02:19:33 2024 ] 	Batch(4100/7879) done. Loss: 1.4966  lr:0.010000
[ Wed Jul  3 02:19:51 2024 ] 	Batch(4200/7879) done. Loss: 0.8294  lr:0.010000
[ Wed Jul  3 02:20:09 2024 ] 	Batch(4300/7879) done. Loss: 0.6664  lr:0.010000
[ Wed Jul  3 02:20:27 2024 ] 	Batch(4400/7879) done. Loss: 0.8650  lr:0.010000
[ Wed Jul  3 02:20:46 2024 ] 
Training: Epoch [12/120], Step [4499], Loss: 0.2193417102098465, Training Accuracy: 76.70555555555556
[ Wed Jul  3 02:20:46 2024 ] 	Batch(4500/7879) done. Loss: 1.4561  lr:0.010000
[ Wed Jul  3 02:21:04 2024 ] 	Batch(4600/7879) done. Loss: 0.5444  lr:0.010000
[ Wed Jul  3 02:21:23 2024 ] 	Batch(4700/7879) done. Loss: 1.1240  lr:0.010000
[ Wed Jul  3 02:21:41 2024 ] 	Batch(4800/7879) done. Loss: 0.4546  lr:0.010000
[ Wed Jul  3 02:22:00 2024 ] 	Batch(4900/7879) done. Loss: 0.6633  lr:0.010000
[ Wed Jul  3 02:22:18 2024 ] 
Training: Epoch [12/120], Step [4999], Loss: 1.809225082397461, Training Accuracy: 76.69
[ Wed Jul  3 02:22:18 2024 ] 	Batch(5000/7879) done. Loss: 0.5421  lr:0.010000
[ Wed Jul  3 02:22:37 2024 ] 	Batch(5100/7879) done. Loss: 1.2799  lr:0.010000
[ Wed Jul  3 02:22:56 2024 ] 	Batch(5200/7879) done. Loss: 0.9314  lr:0.010000
[ Wed Jul  3 02:23:14 2024 ] 	Batch(5300/7879) done. Loss: 0.4999  lr:0.010000
[ Wed Jul  3 02:23:32 2024 ] 	Batch(5400/7879) done. Loss: 0.6006  lr:0.010000
[ Wed Jul  3 02:23:49 2024 ] 
Training: Epoch [12/120], Step [5499], Loss: 0.30281946063041687, Training Accuracy: 76.71818181818182
[ Wed Jul  3 02:23:50 2024 ] 	Batch(5500/7879) done. Loss: 1.0026  lr:0.010000
[ Wed Jul  3 02:24:08 2024 ] 	Batch(5600/7879) done. Loss: 0.4266  lr:0.010000
[ Wed Jul  3 02:24:26 2024 ] 	Batch(5700/7879) done. Loss: 1.6138  lr:0.010000
[ Wed Jul  3 02:24:45 2024 ] 	Batch(5800/7879) done. Loss: 0.6670  lr:0.010000
[ Wed Jul  3 02:25:03 2024 ] 	Batch(5900/7879) done. Loss: 0.6015  lr:0.010000
[ Wed Jul  3 02:25:21 2024 ] 
Training: Epoch [12/120], Step [5999], Loss: 2.0512466430664062, Training Accuracy: 76.58958333333334
[ Wed Jul  3 02:25:22 2024 ] 	Batch(6000/7879) done. Loss: 0.8859  lr:0.010000
[ Wed Jul  3 02:25:40 2024 ] 	Batch(6100/7879) done. Loss: 0.8036  lr:0.010000
[ Wed Jul  3 02:25:57 2024 ] 	Batch(6200/7879) done. Loss: 0.8716  lr:0.010000
[ Wed Jul  3 02:26:15 2024 ] 	Batch(6300/7879) done. Loss: 1.4458  lr:0.010000
[ Wed Jul  3 02:26:33 2024 ] 	Batch(6400/7879) done. Loss: 0.6944  lr:0.010000
[ Wed Jul  3 02:26:51 2024 ] 
Training: Epoch [12/120], Step [6499], Loss: 0.741936206817627, Training Accuracy: 76.67115384615386
[ Wed Jul  3 02:26:51 2024 ] 	Batch(6500/7879) done. Loss: 0.7218  lr:0.010000
[ Wed Jul  3 02:27:09 2024 ] 	Batch(6600/7879) done. Loss: 0.7950  lr:0.010000
[ Wed Jul  3 02:27:27 2024 ] 	Batch(6700/7879) done. Loss: 0.3751  lr:0.010000
[ Wed Jul  3 02:27:45 2024 ] 	Batch(6800/7879) done. Loss: 0.7760  lr:0.010000
[ Wed Jul  3 02:28:04 2024 ] 	Batch(6900/7879) done. Loss: 0.9104  lr:0.010000
[ Wed Jul  3 02:28:22 2024 ] 
Training: Epoch [12/120], Step [6999], Loss: 0.5744186639785767, Training Accuracy: 76.7
[ Wed Jul  3 02:28:22 2024 ] 	Batch(7000/7879) done. Loss: 0.4953  lr:0.010000
[ Wed Jul  3 02:28:41 2024 ] 	Batch(7100/7879) done. Loss: 1.0944  lr:0.010000
[ Wed Jul  3 02:28:59 2024 ] 	Batch(7200/7879) done. Loss: 0.2494  lr:0.010000
[ Wed Jul  3 02:29:18 2024 ] 	Batch(7300/7879) done. Loss: 1.0528  lr:0.010000
[ Wed Jul  3 02:29:37 2024 ] 	Batch(7400/7879) done. Loss: 0.4507  lr:0.010000
[ Wed Jul  3 02:29:55 2024 ] 
Training: Epoch [12/120], Step [7499], Loss: 1.0056946277618408, Training Accuracy: 76.68
[ Wed Jul  3 02:29:55 2024 ] 	Batch(7500/7879) done. Loss: 0.6276  lr:0.010000
[ Wed Jul  3 02:30:14 2024 ] 	Batch(7600/7879) done. Loss: 0.2886  lr:0.010000
[ Wed Jul  3 02:30:32 2024 ] 	Batch(7700/7879) done. Loss: 0.6471  lr:0.010000
[ Wed Jul  3 02:30:50 2024 ] 	Batch(7800/7879) done. Loss: 0.9450  lr:0.010000
[ Wed Jul  3 02:31:04 2024 ] 	Mean training loss: 0.7804.
[ Wed Jul  3 02:31:04 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 02:31:04 2024 ] Training epoch: 14
[ Wed Jul  3 02:31:04 2024 ] 	Batch(0/7879) done. Loss: 0.3176  lr:0.010000
[ Wed Jul  3 02:31:23 2024 ] 	Batch(100/7879) done. Loss: 0.7555  lr:0.010000
[ Wed Jul  3 02:31:41 2024 ] 	Batch(200/7879) done. Loss: 0.2588  lr:0.010000
[ Wed Jul  3 02:31:59 2024 ] 	Batch(300/7879) done. Loss: 0.3414  lr:0.010000
[ Wed Jul  3 02:32:17 2024 ] 	Batch(400/7879) done. Loss: 0.3136  lr:0.010000
[ Wed Jul  3 02:32:35 2024 ] 
Training: Epoch [13/120], Step [499], Loss: 0.888627290725708, Training Accuracy: 77.05
[ Wed Jul  3 02:32:35 2024 ] 	Batch(500/7879) done. Loss: 1.2080  lr:0.010000
[ Wed Jul  3 02:32:53 2024 ] 	Batch(600/7879) done. Loss: 0.7942  lr:0.010000
[ Wed Jul  3 02:33:11 2024 ] 	Batch(700/7879) done. Loss: 0.4840  lr:0.010000
[ Wed Jul  3 02:33:29 2024 ] 	Batch(800/7879) done. Loss: 0.8486  lr:0.010000
[ Wed Jul  3 02:33:47 2024 ] 	Batch(900/7879) done. Loss: 0.7621  lr:0.010000
[ Wed Jul  3 02:34:06 2024 ] 
Training: Epoch [13/120], Step [999], Loss: 1.1965409517288208, Training Accuracy: 77.55
[ Wed Jul  3 02:34:06 2024 ] 	Batch(1000/7879) done. Loss: 0.2587  lr:0.010000
[ Wed Jul  3 02:34:24 2024 ] 	Batch(1100/7879) done. Loss: 0.7738  lr:0.010000
[ Wed Jul  3 02:34:43 2024 ] 	Batch(1200/7879) done. Loss: 0.6250  lr:0.010000
[ Wed Jul  3 02:35:01 2024 ] 	Batch(1300/7879) done. Loss: 1.0672  lr:0.010000
[ Wed Jul  3 02:35:19 2024 ] 	Batch(1400/7879) done. Loss: 1.0886  lr:0.010000
[ Wed Jul  3 02:35:37 2024 ] 
Training: Epoch [13/120], Step [1499], Loss: 1.1724584102630615, Training Accuracy: 77.74166666666666
[ Wed Jul  3 02:35:37 2024 ] 	Batch(1500/7879) done. Loss: 1.2068  lr:0.010000
[ Wed Jul  3 02:35:55 2024 ] 	Batch(1600/7879) done. Loss: 0.9965  lr:0.010000
[ Wed Jul  3 02:36:13 2024 ] 	Batch(1700/7879) done. Loss: 0.5944  lr:0.010000
[ Wed Jul  3 02:36:31 2024 ] 	Batch(1800/7879) done. Loss: 1.1375  lr:0.010000
[ Wed Jul  3 02:36:49 2024 ] 	Batch(1900/7879) done. Loss: 0.1940  lr:0.010000
[ Wed Jul  3 02:37:06 2024 ] 
Training: Epoch [13/120], Step [1999], Loss: 1.0020787715911865, Training Accuracy: 77.66875
[ Wed Jul  3 02:37:07 2024 ] 	Batch(2000/7879) done. Loss: 0.3101  lr:0.010000
[ Wed Jul  3 02:37:25 2024 ] 	Batch(2100/7879) done. Loss: 0.4068  lr:0.010000
[ Wed Jul  3 02:37:42 2024 ] 	Batch(2200/7879) done. Loss: 0.4444  lr:0.010000
[ Wed Jul  3 02:38:00 2024 ] 	Batch(2300/7879) done. Loss: 0.5984  lr:0.010000
[ Wed Jul  3 02:38:18 2024 ] 	Batch(2400/7879) done. Loss: 0.3340  lr:0.010000
[ Wed Jul  3 02:38:37 2024 ] 
Training: Epoch [13/120], Step [2499], Loss: 0.8001744151115417, Training Accuracy: 77.61
[ Wed Jul  3 02:38:37 2024 ] 	Batch(2500/7879) done. Loss: 0.4265  lr:0.010000
[ Wed Jul  3 02:38:55 2024 ] 	Batch(2600/7879) done. Loss: 0.4501  lr:0.010000
[ Wed Jul  3 02:39:14 2024 ] 	Batch(2700/7879) done. Loss: 0.9314  lr:0.010000
[ Wed Jul  3 02:39:33 2024 ] 	Batch(2800/7879) done. Loss: 1.3676  lr:0.010000
[ Wed Jul  3 02:39:51 2024 ] 	Batch(2900/7879) done. Loss: 1.7369  lr:0.010000
[ Wed Jul  3 02:40:10 2024 ] 
Training: Epoch [13/120], Step [2999], Loss: 0.25289538502693176, Training Accuracy: 77.56666666666666
[ Wed Jul  3 02:40:10 2024 ] 	Batch(3000/7879) done. Loss: 0.1418  lr:0.010000
[ Wed Jul  3 02:40:28 2024 ] 	Batch(3100/7879) done. Loss: 1.7438  lr:0.010000
[ Wed Jul  3 02:40:47 2024 ] 	Batch(3200/7879) done. Loss: 0.9690  lr:0.010000
[ Wed Jul  3 02:41:05 2024 ] 	Batch(3300/7879) done. Loss: 0.1524  lr:0.010000
[ Wed Jul  3 02:41:23 2024 ] 	Batch(3400/7879) done. Loss: 1.0563  lr:0.010000
[ Wed Jul  3 02:41:40 2024 ] 
Training: Epoch [13/120], Step [3499], Loss: 0.31159961223602295, Training Accuracy: 77.42857142857143
[ Wed Jul  3 02:41:40 2024 ] 	Batch(3500/7879) done. Loss: 1.0536  lr:0.010000
[ Wed Jul  3 02:41:58 2024 ] 	Batch(3600/7879) done. Loss: 0.4598  lr:0.010000
[ Wed Jul  3 02:42:17 2024 ] 	Batch(3700/7879) done. Loss: 0.4226  lr:0.010000
[ Wed Jul  3 02:42:36 2024 ] 	Batch(3800/7879) done. Loss: 0.3636  lr:0.010000
[ Wed Jul  3 02:42:54 2024 ] 	Batch(3900/7879) done. Loss: 1.3663  lr:0.010000
[ Wed Jul  3 02:43:11 2024 ] 
Training: Epoch [13/120], Step [3999], Loss: 0.9374468326568604, Training Accuracy: 77.34375
[ Wed Jul  3 02:43:12 2024 ] 	Batch(4000/7879) done. Loss: 0.4645  lr:0.010000
[ Wed Jul  3 02:43:30 2024 ] 	Batch(4100/7879) done. Loss: 0.3663  lr:0.010000
[ Wed Jul  3 02:43:47 2024 ] 	Batch(4200/7879) done. Loss: 1.1896  lr:0.010000
[ Wed Jul  3 02:44:05 2024 ] 	Batch(4300/7879) done. Loss: 0.2949  lr:0.010000
[ Wed Jul  3 02:44:23 2024 ] 	Batch(4400/7879) done. Loss: 1.4157  lr:0.010000
[ Wed Jul  3 02:44:42 2024 ] 
Training: Epoch [13/120], Step [4499], Loss: 0.2804069221019745, Training Accuracy: 77.275
[ Wed Jul  3 02:44:42 2024 ] 	Batch(4500/7879) done. Loss: 0.6589  lr:0.010000
[ Wed Jul  3 02:45:00 2024 ] 	Batch(4600/7879) done. Loss: 1.1987  lr:0.010000
[ Wed Jul  3 02:45:19 2024 ] 	Batch(4700/7879) done. Loss: 0.6004  lr:0.010000
[ Wed Jul  3 02:45:38 2024 ] 	Batch(4800/7879) done. Loss: 0.7801  lr:0.010000
[ Wed Jul  3 02:45:56 2024 ] 	Batch(4900/7879) done. Loss: 0.4708  lr:0.010000
[ Wed Jul  3 02:46:13 2024 ] 
Training: Epoch [13/120], Step [4999], Loss: 0.5676178336143494, Training Accuracy: 77.235
[ Wed Jul  3 02:46:13 2024 ] 	Batch(5000/7879) done. Loss: 1.1532  lr:0.010000
[ Wed Jul  3 02:46:31 2024 ] 	Batch(5100/7879) done. Loss: 0.5240  lr:0.010000
[ Wed Jul  3 02:46:49 2024 ] 	Batch(5200/7879) done. Loss: 0.1958  lr:0.010000
[ Wed Jul  3 02:47:08 2024 ] 	Batch(5300/7879) done. Loss: 0.3508  lr:0.010000
[ Wed Jul  3 02:47:26 2024 ] 	Batch(5400/7879) done. Loss: 0.7649  lr:0.010000
[ Wed Jul  3 02:47:44 2024 ] 
Training: Epoch [13/120], Step [5499], Loss: 0.42965325713157654, Training Accuracy: 77.34545454545454
[ Wed Jul  3 02:47:45 2024 ] 	Batch(5500/7879) done. Loss: 0.1620  lr:0.010000
[ Wed Jul  3 02:48:03 2024 ] 	Batch(5600/7879) done. Loss: 0.2740  lr:0.010000
[ Wed Jul  3 02:48:20 2024 ] 	Batch(5700/7879) done. Loss: 0.3447  lr:0.010000
[ Wed Jul  3 02:48:38 2024 ] 	Batch(5800/7879) done. Loss: 0.6728  lr:0.010000
[ Wed Jul  3 02:48:56 2024 ] 	Batch(5900/7879) done. Loss: 0.8327  lr:0.010000
[ Wed Jul  3 02:49:14 2024 ] 
Training: Epoch [13/120], Step [5999], Loss: 0.965243399143219, Training Accuracy: 77.44375
[ Wed Jul  3 02:49:14 2024 ] 	Batch(6000/7879) done. Loss: 0.4188  lr:0.010000
[ Wed Jul  3 02:49:32 2024 ] 	Batch(6100/7879) done. Loss: 1.5489  lr:0.010000
[ Wed Jul  3 02:49:50 2024 ] 	Batch(6200/7879) done. Loss: 1.1278  lr:0.010000
[ Wed Jul  3 02:50:08 2024 ] 	Batch(6300/7879) done. Loss: 0.5078  lr:0.010000
[ Wed Jul  3 02:50:26 2024 ] 	Batch(6400/7879) done. Loss: 0.7292  lr:0.010000
[ Wed Jul  3 02:50:44 2024 ] 
Training: Epoch [13/120], Step [6499], Loss: 0.9613063931465149, Training Accuracy: 77.46923076923076
[ Wed Jul  3 02:50:44 2024 ] 	Batch(6500/7879) done. Loss: 0.1509  lr:0.010000
[ Wed Jul  3 02:51:02 2024 ] 	Batch(6600/7879) done. Loss: 1.2667  lr:0.010000
[ Wed Jul  3 02:51:20 2024 ] 	Batch(6700/7879) done. Loss: 1.2864  lr:0.010000
[ Wed Jul  3 02:51:38 2024 ] 	Batch(6800/7879) done. Loss: 0.8832  lr:0.010000
[ Wed Jul  3 02:51:56 2024 ] 	Batch(6900/7879) done. Loss: 0.5472  lr:0.010000
[ Wed Jul  3 02:52:14 2024 ] 
Training: Epoch [13/120], Step [6999], Loss: 1.4387036561965942, Training Accuracy: 77.40357142857142
[ Wed Jul  3 02:52:14 2024 ] 	Batch(7000/7879) done. Loss: 0.7183  lr:0.010000
[ Wed Jul  3 02:52:32 2024 ] 	Batch(7100/7879) done. Loss: 0.9456  lr:0.010000
[ Wed Jul  3 02:52:50 2024 ] 	Batch(7200/7879) done. Loss: 0.5010  lr:0.010000
[ Wed Jul  3 02:53:08 2024 ] 	Batch(7300/7879) done. Loss: 0.6598  lr:0.010000
[ Wed Jul  3 02:53:27 2024 ] 	Batch(7400/7879) done. Loss: 0.8433  lr:0.010000
[ Wed Jul  3 02:53:45 2024 ] 
Training: Epoch [13/120], Step [7499], Loss: 0.6806962490081787, Training Accuracy: 77.34666666666666
[ Wed Jul  3 02:53:45 2024 ] 	Batch(7500/7879) done. Loss: 0.8778  lr:0.010000
[ Wed Jul  3 02:54:04 2024 ] 	Batch(7600/7879) done. Loss: 0.8573  lr:0.010000
[ Wed Jul  3 02:54:22 2024 ] 	Batch(7700/7879) done. Loss: 0.5258  lr:0.010000
[ Wed Jul  3 02:54:40 2024 ] 	Batch(7800/7879) done. Loss: 0.6372  lr:0.010000
[ Wed Jul  3 02:54:54 2024 ] 	Mean training loss: 0.7577.
[ Wed Jul  3 02:54:54 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 02:54:54 2024 ] Training epoch: 15
[ Wed Jul  3 02:54:54 2024 ] 	Batch(0/7879) done. Loss: 1.1315  lr:0.010000
[ Wed Jul  3 02:55:12 2024 ] 	Batch(100/7879) done. Loss: 0.4577  lr:0.010000
[ Wed Jul  3 02:55:30 2024 ] 	Batch(200/7879) done. Loss: 0.5296  lr:0.010000
[ Wed Jul  3 02:55:48 2024 ] 	Batch(300/7879) done. Loss: 1.3010  lr:0.010000
[ Wed Jul  3 02:56:06 2024 ] 	Batch(400/7879) done. Loss: 0.7164  lr:0.010000
[ Wed Jul  3 02:56:24 2024 ] 
Training: Epoch [14/120], Step [499], Loss: 0.13806098699569702, Training Accuracy: 78.125
[ Wed Jul  3 02:56:24 2024 ] 	Batch(500/7879) done. Loss: 1.3786  lr:0.010000
[ Wed Jul  3 02:56:42 2024 ] 	Batch(600/7879) done. Loss: 0.2297  lr:0.010000
[ Wed Jul  3 02:57:00 2024 ] 	Batch(700/7879) done. Loss: 0.6252  lr:0.010000
[ Wed Jul  3 02:57:18 2024 ] 	Batch(800/7879) done. Loss: 0.2298  lr:0.010000
[ Wed Jul  3 02:57:36 2024 ] 	Batch(900/7879) done. Loss: 0.4755  lr:0.010000
[ Wed Jul  3 02:57:53 2024 ] 
Training: Epoch [14/120], Step [999], Loss: 1.1540100574493408, Training Accuracy: 78.525
[ Wed Jul  3 02:57:54 2024 ] 	Batch(1000/7879) done. Loss: 0.6331  lr:0.010000
[ Wed Jul  3 02:58:12 2024 ] 	Batch(1100/7879) done. Loss: 0.3910  lr:0.010000
[ Wed Jul  3 02:58:30 2024 ] 	Batch(1200/7879) done. Loss: 0.1927  lr:0.010000
[ Wed Jul  3 02:58:47 2024 ] 	Batch(1300/7879) done. Loss: 1.3509  lr:0.010000
[ Wed Jul  3 02:59:05 2024 ] 	Batch(1400/7879) done. Loss: 0.2735  lr:0.010000
[ Wed Jul  3 02:59:23 2024 ] 
Training: Epoch [14/120], Step [1499], Loss: 0.4038175940513611, Training Accuracy: 79.04166666666667
[ Wed Jul  3 02:59:23 2024 ] 	Batch(1500/7879) done. Loss: 0.1580  lr:0.010000
[ Wed Jul  3 02:59:41 2024 ] 	Batch(1600/7879) done. Loss: 0.6416  lr:0.010000
[ Wed Jul  3 03:00:00 2024 ] 	Batch(1700/7879) done. Loss: 1.2449  lr:0.010000
[ Wed Jul  3 03:00:17 2024 ] 	Batch(1800/7879) done. Loss: 0.8699  lr:0.010000
[ Wed Jul  3 03:00:35 2024 ] 	Batch(1900/7879) done. Loss: 0.7861  lr:0.010000
[ Wed Jul  3 03:00:53 2024 ] 
Training: Epoch [14/120], Step [1999], Loss: 0.34403109550476074, Training Accuracy: 78.71875
[ Wed Jul  3 03:00:53 2024 ] 	Batch(2000/7879) done. Loss: 0.1971  lr:0.010000
[ Wed Jul  3 03:01:11 2024 ] 	Batch(2100/7879) done. Loss: 1.1258  lr:0.010000
[ Wed Jul  3 03:01:29 2024 ] 	Batch(2200/7879) done. Loss: 0.2537  lr:0.010000
[ Wed Jul  3 03:01:47 2024 ] 	Batch(2300/7879) done. Loss: 0.6357  lr:0.010000
[ Wed Jul  3 03:02:05 2024 ] 	Batch(2400/7879) done. Loss: 0.6000  lr:0.010000
[ Wed Jul  3 03:02:23 2024 ] 
Training: Epoch [14/120], Step [2499], Loss: 1.8758896589279175, Training Accuracy: 78.69500000000001
[ Wed Jul  3 03:02:23 2024 ] 	Batch(2500/7879) done. Loss: 0.4625  lr:0.010000
[ Wed Jul  3 03:02:41 2024 ] 	Batch(2600/7879) done. Loss: 0.2047  lr:0.010000
[ Wed Jul  3 03:02:59 2024 ] 	Batch(2700/7879) done. Loss: 1.3474  lr:0.010000
[ Wed Jul  3 03:03:17 2024 ] 	Batch(2800/7879) done. Loss: 0.8017  lr:0.010000
[ Wed Jul  3 03:03:35 2024 ] 	Batch(2900/7879) done. Loss: 1.3346  lr:0.010000
[ Wed Jul  3 03:03:52 2024 ] 
Training: Epoch [14/120], Step [2999], Loss: 0.32190823554992676, Training Accuracy: 78.49166666666667
[ Wed Jul  3 03:03:53 2024 ] 	Batch(3000/7879) done. Loss: 0.4596  lr:0.010000
[ Wed Jul  3 03:04:10 2024 ] 	Batch(3100/7879) done. Loss: 0.8811  lr:0.010000
[ Wed Jul  3 03:04:28 2024 ] 	Batch(3200/7879) done. Loss: 1.7823  lr:0.010000
[ Wed Jul  3 03:04:46 2024 ] 	Batch(3300/7879) done. Loss: 0.8481  lr:0.010000
[ Wed Jul  3 03:05:04 2024 ] 	Batch(3400/7879) done. Loss: 0.2961  lr:0.010000
[ Wed Jul  3 03:05:22 2024 ] 
Training: Epoch [14/120], Step [3499], Loss: 1.3002674579620361, Training Accuracy: 78.39285714285714
[ Wed Jul  3 03:05:22 2024 ] 	Batch(3500/7879) done. Loss: 0.8887  lr:0.010000
[ Wed Jul  3 03:05:40 2024 ] 	Batch(3600/7879) done. Loss: 0.7795  lr:0.010000
[ Wed Jul  3 03:05:59 2024 ] 	Batch(3700/7879) done. Loss: 0.3927  lr:0.010000
[ Wed Jul  3 03:06:17 2024 ] 	Batch(3800/7879) done. Loss: 0.8699  lr:0.010000
[ Wed Jul  3 03:06:35 2024 ] 	Batch(3900/7879) done. Loss: 1.1229  lr:0.010000
[ Wed Jul  3 03:06:53 2024 ] 
Training: Epoch [14/120], Step [3999], Loss: 0.9067946672439575, Training Accuracy: 78.31875000000001
[ Wed Jul  3 03:06:53 2024 ] 	Batch(4000/7879) done. Loss: 1.8267  lr:0.010000
[ Wed Jul  3 03:07:11 2024 ] 	Batch(4100/7879) done. Loss: 0.0413  lr:0.010000
[ Wed Jul  3 03:07:29 2024 ] 	Batch(4200/7879) done. Loss: 0.9413  lr:0.010000
[ Wed Jul  3 03:07:47 2024 ] 	Batch(4300/7879) done. Loss: 0.4668  lr:0.010000
[ Wed Jul  3 03:08:05 2024 ] 	Batch(4400/7879) done. Loss: 0.0905  lr:0.010000
[ Wed Jul  3 03:08:22 2024 ] 
Training: Epoch [14/120], Step [4499], Loss: 0.2876013219356537, Training Accuracy: 78.35
[ Wed Jul  3 03:08:22 2024 ] 	Batch(4500/7879) done. Loss: 0.7127  lr:0.010000
[ Wed Jul  3 03:08:40 2024 ] 	Batch(4600/7879) done. Loss: 0.2015  lr:0.010000
[ Wed Jul  3 03:08:58 2024 ] 	Batch(4700/7879) done. Loss: 1.0131  lr:0.010000
[ Wed Jul  3 03:09:16 2024 ] 	Batch(4800/7879) done. Loss: 0.1543  lr:0.010000
[ Wed Jul  3 03:09:34 2024 ] 	Batch(4900/7879) done. Loss: 0.3148  lr:0.010000
[ Wed Jul  3 03:09:52 2024 ] 
Training: Epoch [14/120], Step [4999], Loss: 0.6693496108055115, Training Accuracy: 78.3675
[ Wed Jul  3 03:09:52 2024 ] 	Batch(5000/7879) done. Loss: 0.9648  lr:0.010000
[ Wed Jul  3 03:10:10 2024 ] 	Batch(5100/7879) done. Loss: 0.5578  lr:0.010000
[ Wed Jul  3 03:10:28 2024 ] 	Batch(5200/7879) done. Loss: 0.2839  lr:0.010000
[ Wed Jul  3 03:10:46 2024 ] 	Batch(5300/7879) done. Loss: 1.3070  lr:0.010000
[ Wed Jul  3 03:11:04 2024 ] 	Batch(5400/7879) done. Loss: 0.6060  lr:0.010000
[ Wed Jul  3 03:11:21 2024 ] 
Training: Epoch [14/120], Step [5499], Loss: 0.5793605446815491, Training Accuracy: 78.31136363636364
[ Wed Jul  3 03:11:22 2024 ] 	Batch(5500/7879) done. Loss: 0.6023  lr:0.010000
[ Wed Jul  3 03:11:40 2024 ] 	Batch(5600/7879) done. Loss: 0.2491  lr:0.010000
[ Wed Jul  3 03:11:58 2024 ] 	Batch(5700/7879) done. Loss: 0.9807  lr:0.010000
[ Wed Jul  3 03:12:15 2024 ] 	Batch(5800/7879) done. Loss: 0.3892  lr:0.010000
[ Wed Jul  3 03:12:33 2024 ] 	Batch(5900/7879) done. Loss: 0.2146  lr:0.010000
[ Wed Jul  3 03:12:51 2024 ] 
Training: Epoch [14/120], Step [5999], Loss: 0.3281813859939575, Training Accuracy: 78.29166666666667
[ Wed Jul  3 03:12:51 2024 ] 	Batch(6000/7879) done. Loss: 1.3194  lr:0.010000
[ Wed Jul  3 03:13:09 2024 ] 	Batch(6100/7879) done. Loss: 0.6465  lr:0.010000
[ Wed Jul  3 03:13:27 2024 ] 	Batch(6200/7879) done. Loss: 1.1475  lr:0.010000
[ Wed Jul  3 03:13:45 2024 ] 	Batch(6300/7879) done. Loss: 0.4872  lr:0.010000
[ Wed Jul  3 03:14:03 2024 ] 	Batch(6400/7879) done. Loss: 0.8411  lr:0.010000
[ Wed Jul  3 03:14:21 2024 ] 
Training: Epoch [14/120], Step [6499], Loss: 0.7566656470298767, Training Accuracy: 78.26923076923077
[ Wed Jul  3 03:14:21 2024 ] 	Batch(6500/7879) done. Loss: 0.9137  lr:0.010000
[ Wed Jul  3 03:14:39 2024 ] 	Batch(6600/7879) done. Loss: 1.1118  lr:0.010000
[ Wed Jul  3 03:14:57 2024 ] 	Batch(6700/7879) done. Loss: 0.8280  lr:0.010000
[ Wed Jul  3 03:15:15 2024 ] 	Batch(6800/7879) done. Loss: 0.5469  lr:0.010000
[ Wed Jul  3 03:15:34 2024 ] 	Batch(6900/7879) done. Loss: 0.9332  lr:0.010000
[ Wed Jul  3 03:15:53 2024 ] 
Training: Epoch [14/120], Step [6999], Loss: 0.040002234280109406, Training Accuracy: 78.19464285714287
[ Wed Jul  3 03:15:53 2024 ] 	Batch(7000/7879) done. Loss: 0.0890  lr:0.010000
[ Wed Jul  3 03:16:11 2024 ] 	Batch(7100/7879) done. Loss: 0.7177  lr:0.010000
[ Wed Jul  3 03:16:30 2024 ] 	Batch(7200/7879) done. Loss: 0.9182  lr:0.010000
[ Wed Jul  3 03:16:49 2024 ] 	Batch(7300/7879) done. Loss: 0.3099  lr:0.010000
[ Wed Jul  3 03:17:07 2024 ] 	Batch(7400/7879) done. Loss: 0.8206  lr:0.010000
[ Wed Jul  3 03:17:25 2024 ] 
Training: Epoch [14/120], Step [7499], Loss: 0.5638876557350159, Training Accuracy: 78.19500000000001
[ Wed Jul  3 03:17:26 2024 ] 	Batch(7500/7879) done. Loss: 1.4751  lr:0.010000
[ Wed Jul  3 03:17:44 2024 ] 	Batch(7600/7879) done. Loss: 1.0678  lr:0.010000
[ Wed Jul  3 03:18:02 2024 ] 	Batch(7700/7879) done. Loss: 0.7638  lr:0.010000
[ Wed Jul  3 03:18:20 2024 ] 	Batch(7800/7879) done. Loss: 0.3924  lr:0.010000
[ Wed Jul  3 03:18:34 2024 ] 	Mean training loss: 0.7295.
[ Wed Jul  3 03:18:34 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 03:18:34 2024 ] Training epoch: 16
[ Wed Jul  3 03:18:35 2024 ] 	Batch(0/7879) done. Loss: 0.8109  lr:0.010000
[ Wed Jul  3 03:18:53 2024 ] 	Batch(100/7879) done. Loss: 0.2042  lr:0.010000
[ Wed Jul  3 03:19:11 2024 ] 	Batch(200/7879) done. Loss: 0.2557  lr:0.010000
[ Wed Jul  3 03:19:28 2024 ] 	Batch(300/7879) done. Loss: 0.5901  lr:0.010000
[ Wed Jul  3 03:19:46 2024 ] 	Batch(400/7879) done. Loss: 0.5153  lr:0.010000
[ Wed Jul  3 03:20:04 2024 ] 
Training: Epoch [15/120], Step [499], Loss: 0.8401879668235779, Training Accuracy: 78.325
[ Wed Jul  3 03:20:04 2024 ] 	Batch(500/7879) done. Loss: 1.7294  lr:0.010000
[ Wed Jul  3 03:20:22 2024 ] 	Batch(600/7879) done. Loss: 0.9207  lr:0.010000
[ Wed Jul  3 03:20:40 2024 ] 	Batch(700/7879) done. Loss: 0.4545  lr:0.010000
[ Wed Jul  3 03:20:58 2024 ] 	Batch(800/7879) done. Loss: 0.6343  lr:0.010000
[ Wed Jul  3 03:21:16 2024 ] 	Batch(900/7879) done. Loss: 0.9015  lr:0.010000
[ Wed Jul  3 03:21:34 2024 ] 
Training: Epoch [15/120], Step [999], Loss: 0.760320782661438, Training Accuracy: 78.925
[ Wed Jul  3 03:21:34 2024 ] 	Batch(1000/7879) done. Loss: 0.0870  lr:0.010000
[ Wed Jul  3 03:21:52 2024 ] 	Batch(1100/7879) done. Loss: 0.5162  lr:0.010000
[ Wed Jul  3 03:22:10 2024 ] 	Batch(1200/7879) done. Loss: 1.1140  lr:0.010000
[ Wed Jul  3 03:22:28 2024 ] 	Batch(1300/7879) done. Loss: 0.0365  lr:0.010000
[ Wed Jul  3 03:22:46 2024 ] 	Batch(1400/7879) done. Loss: 1.5471  lr:0.010000
[ Wed Jul  3 03:23:03 2024 ] 
Training: Epoch [15/120], Step [1499], Loss: 0.6291227340698242, Training Accuracy: 79.57499999999999
[ Wed Jul  3 03:23:04 2024 ] 	Batch(1500/7879) done. Loss: 0.9328  lr:0.010000
[ Wed Jul  3 03:23:22 2024 ] 	Batch(1600/7879) done. Loss: 0.2927  lr:0.010000
[ Wed Jul  3 03:23:40 2024 ] 	Batch(1700/7879) done. Loss: 0.3588  lr:0.010000
[ Wed Jul  3 03:23:59 2024 ] 	Batch(1800/7879) done. Loss: 0.6454  lr:0.010000
[ Wed Jul  3 03:24:17 2024 ] 	Batch(1900/7879) done. Loss: 0.5493  lr:0.010000
[ Wed Jul  3 03:24:36 2024 ] 
Training: Epoch [15/120], Step [1999], Loss: 1.5865038633346558, Training Accuracy: 79.65
[ Wed Jul  3 03:24:36 2024 ] 	Batch(2000/7879) done. Loss: 0.6178  lr:0.010000
[ Wed Jul  3 03:24:54 2024 ] 	Batch(2100/7879) done. Loss: 0.6271  lr:0.010000
[ Wed Jul  3 03:25:12 2024 ] 	Batch(2200/7879) done. Loss: 0.9104  lr:0.010000
[ Wed Jul  3 03:25:30 2024 ] 	Batch(2300/7879) done. Loss: 0.6839  lr:0.010000
[ Wed Jul  3 03:25:48 2024 ] 	Batch(2400/7879) done. Loss: 1.1394  lr:0.010000
[ Wed Jul  3 03:26:05 2024 ] 
Training: Epoch [15/120], Step [2499], Loss: 0.36022651195526123, Training Accuracy: 79.27
[ Wed Jul  3 03:26:06 2024 ] 	Batch(2500/7879) done. Loss: 0.2612  lr:0.010000
[ Wed Jul  3 03:26:24 2024 ] 	Batch(2600/7879) done. Loss: 0.3557  lr:0.010000
[ Wed Jul  3 03:26:41 2024 ] 	Batch(2700/7879) done. Loss: 1.4054  lr:0.010000
[ Wed Jul  3 03:26:59 2024 ] 	Batch(2800/7879) done. Loss: 0.4118  lr:0.010000
[ Wed Jul  3 03:27:17 2024 ] 	Batch(2900/7879) done. Loss: 0.5988  lr:0.010000
[ Wed Jul  3 03:27:35 2024 ] 
Training: Epoch [15/120], Step [2999], Loss: 0.6862383484840393, Training Accuracy: 79.44166666666666
[ Wed Jul  3 03:27:35 2024 ] 	Batch(3000/7879) done. Loss: 0.2893  lr:0.010000
[ Wed Jul  3 03:27:53 2024 ] 	Batch(3100/7879) done. Loss: 0.6913  lr:0.010000
[ Wed Jul  3 03:28:11 2024 ] 	Batch(3200/7879) done. Loss: 0.4126  lr:0.010000
[ Wed Jul  3 03:28:30 2024 ] 	Batch(3300/7879) done. Loss: 0.3783  lr:0.010000
[ Wed Jul  3 03:28:48 2024 ] 	Batch(3400/7879) done. Loss: 0.4739  lr:0.010000
[ Wed Jul  3 03:29:07 2024 ] 
Training: Epoch [15/120], Step [3499], Loss: 0.34253406524658203, Training Accuracy: 79.29285714285714
[ Wed Jul  3 03:29:07 2024 ] 	Batch(3500/7879) done. Loss: 1.5262  lr:0.010000
[ Wed Jul  3 03:29:26 2024 ] 	Batch(3600/7879) done. Loss: 0.2964  lr:0.010000
[ Wed Jul  3 03:29:45 2024 ] 	Batch(3700/7879) done. Loss: 0.3262  lr:0.010000
[ Wed Jul  3 03:30:03 2024 ] 	Batch(3800/7879) done. Loss: 0.7632  lr:0.010000
[ Wed Jul  3 03:30:22 2024 ] 	Batch(3900/7879) done. Loss: 1.4113  lr:0.010000
[ Wed Jul  3 03:30:41 2024 ] 
Training: Epoch [15/120], Step [3999], Loss: 0.5037003755569458, Training Accuracy: 79.29375
[ Wed Jul  3 03:30:41 2024 ] 	Batch(4000/7879) done. Loss: 0.4374  lr:0.010000
[ Wed Jul  3 03:31:00 2024 ] 	Batch(4100/7879) done. Loss: 1.5121  lr:0.010000
[ Wed Jul  3 03:31:18 2024 ] 	Batch(4200/7879) done. Loss: 0.8291  lr:0.010000
[ Wed Jul  3 03:31:37 2024 ] 	Batch(4300/7879) done. Loss: 0.1711  lr:0.010000
[ Wed Jul  3 03:31:55 2024 ] 	Batch(4400/7879) done. Loss: 1.2415  lr:0.010000
[ Wed Jul  3 03:32:14 2024 ] 
Training: Epoch [15/120], Step [4499], Loss: 1.0933750867843628, Training Accuracy: 79.22222222222223
[ Wed Jul  3 03:32:14 2024 ] 	Batch(4500/7879) done. Loss: 1.0704  lr:0.010000
[ Wed Jul  3 03:32:32 2024 ] 	Batch(4600/7879) done. Loss: 1.1342  lr:0.010000
[ Wed Jul  3 03:32:51 2024 ] 	Batch(4700/7879) done. Loss: 0.7416  lr:0.010000
[ Wed Jul  3 03:33:10 2024 ] 	Batch(4800/7879) done. Loss: 0.8092  lr:0.010000
[ Wed Jul  3 03:33:28 2024 ] 	Batch(4900/7879) done. Loss: 1.1411  lr:0.010000
[ Wed Jul  3 03:33:46 2024 ] 
Training: Epoch [15/120], Step [4999], Loss: 0.7739705443382263, Training Accuracy: 79.355
[ Wed Jul  3 03:33:47 2024 ] 	Batch(5000/7879) done. Loss: 0.2905  lr:0.010000
[ Wed Jul  3 03:34:05 2024 ] 	Batch(5100/7879) done. Loss: 0.4595  lr:0.010000
[ Wed Jul  3 03:34:24 2024 ] 	Batch(5200/7879) done. Loss: 1.3508  lr:0.010000
[ Wed Jul  3 03:34:42 2024 ] 	Batch(5300/7879) done. Loss: 0.8203  lr:0.010000
[ Wed Jul  3 03:34:59 2024 ] 	Batch(5400/7879) done. Loss: 0.5046  lr:0.010000
[ Wed Jul  3 03:35:17 2024 ] 
Training: Epoch [15/120], Step [5499], Loss: 0.7817203402519226, Training Accuracy: 79.33636363636364
[ Wed Jul  3 03:35:17 2024 ] 	Batch(5500/7879) done. Loss: 0.5559  lr:0.010000
[ Wed Jul  3 03:35:35 2024 ] 	Batch(5600/7879) done. Loss: 1.2176  lr:0.010000
[ Wed Jul  3 03:35:54 2024 ] 	Batch(5700/7879) done. Loss: 1.4483  lr:0.010000
[ Wed Jul  3 03:36:12 2024 ] 	Batch(5800/7879) done. Loss: 0.3432  lr:0.010000
[ Wed Jul  3 03:36:31 2024 ] 	Batch(5900/7879) done. Loss: 0.1818  lr:0.010000
[ Wed Jul  3 03:36:49 2024 ] 
Training: Epoch [15/120], Step [5999], Loss: 0.7757761478424072, Training Accuracy: 79.25208333333333
[ Wed Jul  3 03:36:50 2024 ] 	Batch(6000/7879) done. Loss: 1.4506  lr:0.010000
[ Wed Jul  3 03:37:08 2024 ] 	Batch(6100/7879) done. Loss: 0.6323  lr:0.010000
[ Wed Jul  3 03:37:27 2024 ] 	Batch(6200/7879) done. Loss: 1.4007  lr:0.010000
[ Wed Jul  3 03:37:45 2024 ] 	Batch(6300/7879) done. Loss: 0.8324  lr:0.010000
[ Wed Jul  3 03:38:04 2024 ] 	Batch(6400/7879) done. Loss: 1.5161  lr:0.010000
[ Wed Jul  3 03:38:22 2024 ] 
Training: Epoch [15/120], Step [6499], Loss: 0.7488241791725159, Training Accuracy: 79.20384615384616
[ Wed Jul  3 03:38:22 2024 ] 	Batch(6500/7879) done. Loss: 0.0946  lr:0.010000
[ Wed Jul  3 03:38:40 2024 ] 	Batch(6600/7879) done. Loss: 1.0049  lr:0.010000
[ Wed Jul  3 03:38:58 2024 ] 	Batch(6700/7879) done. Loss: 1.1907  lr:0.010000
[ Wed Jul  3 03:39:16 2024 ] 	Batch(6800/7879) done. Loss: 0.4446  lr:0.010000
[ Wed Jul  3 03:39:34 2024 ] 	Batch(6900/7879) done. Loss: 0.3981  lr:0.010000
[ Wed Jul  3 03:39:52 2024 ] 
Training: Epoch [15/120], Step [6999], Loss: 1.5268305540084839, Training Accuracy: 79.15535714285714
[ Wed Jul  3 03:39:53 2024 ] 	Batch(7000/7879) done. Loss: 0.9080  lr:0.010000
[ Wed Jul  3 03:40:11 2024 ] 	Batch(7100/7879) done. Loss: 0.4769  lr:0.010000
[ Wed Jul  3 03:40:30 2024 ] 	Batch(7200/7879) done. Loss: 1.4371  lr:0.010000
[ Wed Jul  3 03:40:48 2024 ] 	Batch(7300/7879) done. Loss: 0.1746  lr:0.010000
[ Wed Jul  3 03:41:07 2024 ] 	Batch(7400/7879) done. Loss: 0.7989  lr:0.010000
[ Wed Jul  3 03:41:25 2024 ] 
Training: Epoch [15/120], Step [7499], Loss: 0.098277248442173, Training Accuracy: 79.13833333333334
[ Wed Jul  3 03:41:25 2024 ] 	Batch(7500/7879) done. Loss: 1.6698  lr:0.010000
[ Wed Jul  3 03:41:44 2024 ] 	Batch(7600/7879) done. Loss: 1.9901  lr:0.010000
[ Wed Jul  3 03:42:02 2024 ] 	Batch(7700/7879) done. Loss: 0.4229  lr:0.010000
[ Wed Jul  3 03:42:21 2024 ] 	Batch(7800/7879) done. Loss: 0.1935  lr:0.010000
[ Wed Jul  3 03:42:35 2024 ] 	Mean training loss: 0.6962.
[ Wed Jul  3 03:42:35 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 03:42:35 2024 ] Training epoch: 17
[ Wed Jul  3 03:42:36 2024 ] 	Batch(0/7879) done. Loss: 0.2659  lr:0.010000
[ Wed Jul  3 03:42:54 2024 ] 	Batch(100/7879) done. Loss: 0.7933  lr:0.010000
[ Wed Jul  3 03:43:12 2024 ] 	Batch(200/7879) done. Loss: 0.2125  lr:0.010000
[ Wed Jul  3 03:43:30 2024 ] 	Batch(300/7879) done. Loss: 0.7832  lr:0.010000
[ Wed Jul  3 03:43:48 2024 ] 	Batch(400/7879) done. Loss: 0.9279  lr:0.010000
[ Wed Jul  3 03:44:05 2024 ] 
Training: Epoch [16/120], Step [499], Loss: 0.14614245295524597, Training Accuracy: 80.85
[ Wed Jul  3 03:44:05 2024 ] 	Batch(500/7879) done. Loss: 0.1403  lr:0.010000
[ Wed Jul  3 03:44:23 2024 ] 	Batch(600/7879) done. Loss: 0.9532  lr:0.010000
[ Wed Jul  3 03:44:41 2024 ] 	Batch(700/7879) done. Loss: 0.0630  lr:0.010000
[ Wed Jul  3 03:44:59 2024 ] 	Batch(800/7879) done. Loss: 0.9193  lr:0.010000
[ Wed Jul  3 03:45:17 2024 ] 	Batch(900/7879) done. Loss: 1.0521  lr:0.010000
[ Wed Jul  3 03:45:35 2024 ] 
Training: Epoch [16/120], Step [999], Loss: 0.3440571129322052, Training Accuracy: 80.5
[ Wed Jul  3 03:45:35 2024 ] 	Batch(1000/7879) done. Loss: 1.0263  lr:0.010000
[ Wed Jul  3 03:45:53 2024 ] 	Batch(1100/7879) done. Loss: 0.7130  lr:0.010000
[ Wed Jul  3 03:46:11 2024 ] 	Batch(1200/7879) done. Loss: 0.5012  lr:0.010000
[ Wed Jul  3 03:46:29 2024 ] 	Batch(1300/7879) done. Loss: 1.1985  lr:0.010000
[ Wed Jul  3 03:46:47 2024 ] 	Batch(1400/7879) done. Loss: 1.0916  lr:0.010000
[ Wed Jul  3 03:47:04 2024 ] 
Training: Epoch [16/120], Step [1499], Loss: 0.6922568082809448, Training Accuracy: 79.99166666666667
[ Wed Jul  3 03:47:04 2024 ] 	Batch(1500/7879) done. Loss: 0.7461  lr:0.010000
[ Wed Jul  3 03:47:22 2024 ] 	Batch(1600/7879) done. Loss: 0.9843  lr:0.010000
[ Wed Jul  3 03:47:41 2024 ] 	Batch(1700/7879) done. Loss: 0.2533  lr:0.010000
[ Wed Jul  3 03:47:59 2024 ] 	Batch(1800/7879) done. Loss: 1.8920  lr:0.010000
[ Wed Jul  3 03:48:17 2024 ] 	Batch(1900/7879) done. Loss: 1.5380  lr:0.010000
[ Wed Jul  3 03:48:35 2024 ] 
Training: Epoch [16/120], Step [1999], Loss: 0.1324884593486786, Training Accuracy: 80.04375
[ Wed Jul  3 03:48:35 2024 ] 	Batch(2000/7879) done. Loss: 0.5600  lr:0.010000
[ Wed Jul  3 03:48:53 2024 ] 	Batch(2100/7879) done. Loss: 0.2202  lr:0.010000
[ Wed Jul  3 03:49:11 2024 ] 	Batch(2200/7879) done. Loss: 0.8699  lr:0.010000
[ Wed Jul  3 03:49:29 2024 ] 	Batch(2300/7879) done. Loss: 1.4180  lr:0.010000
[ Wed Jul  3 03:49:47 2024 ] 	Batch(2400/7879) done. Loss: 0.3287  lr:0.010000
[ Wed Jul  3 03:50:05 2024 ] 
Training: Epoch [16/120], Step [2499], Loss: 0.8244035840034485, Training Accuracy: 79.81
[ Wed Jul  3 03:50:05 2024 ] 	Batch(2500/7879) done. Loss: 0.2473  lr:0.010000
[ Wed Jul  3 03:50:24 2024 ] 	Batch(2600/7879) done. Loss: 1.0877  lr:0.010000
[ Wed Jul  3 03:50:42 2024 ] 	Batch(2700/7879) done. Loss: 0.4004  lr:0.010000
[ Wed Jul  3 03:51:01 2024 ] 	Batch(2800/7879) done. Loss: 0.3425  lr:0.010000
[ Wed Jul  3 03:51:19 2024 ] 	Batch(2900/7879) done. Loss: 0.8718  lr:0.010000
[ Wed Jul  3 03:51:37 2024 ] 
Training: Epoch [16/120], Step [2999], Loss: 0.09259478747844696, Training Accuracy: 79.84583333333333
[ Wed Jul  3 03:51:37 2024 ] 	Batch(3000/7879) done. Loss: 0.4458  lr:0.010000
[ Wed Jul  3 03:51:55 2024 ] 	Batch(3100/7879) done. Loss: 0.1196  lr:0.010000
[ Wed Jul  3 03:52:13 2024 ] 	Batch(3200/7879) done. Loss: 0.6871  lr:0.010000
[ Wed Jul  3 03:52:31 2024 ] 	Batch(3300/7879) done. Loss: 0.7452  lr:0.010000
[ Wed Jul  3 03:52:49 2024 ] 	Batch(3400/7879) done. Loss: 0.0692  lr:0.010000
[ Wed Jul  3 03:53:06 2024 ] 
Training: Epoch [16/120], Step [3499], Loss: 1.6106624603271484, Training Accuracy: 79.73928571428571
[ Wed Jul  3 03:53:06 2024 ] 	Batch(3500/7879) done. Loss: 1.7453  lr:0.010000
[ Wed Jul  3 03:53:24 2024 ] 	Batch(3600/7879) done. Loss: 0.7158  lr:0.010000
[ Wed Jul  3 03:53:42 2024 ] 	Batch(3700/7879) done. Loss: 1.6286  lr:0.010000
[ Wed Jul  3 03:54:00 2024 ] 	Batch(3800/7879) done. Loss: 0.6128  lr:0.010000
[ Wed Jul  3 03:54:18 2024 ] 	Batch(3900/7879) done. Loss: 0.6496  lr:0.010000
[ Wed Jul  3 03:54:36 2024 ] 
Training: Epoch [16/120], Step [3999], Loss: 0.040415916591882706, Training Accuracy: 79.83125
[ Wed Jul  3 03:54:36 2024 ] 	Batch(4000/7879) done. Loss: 0.3828  lr:0.010000
[ Wed Jul  3 03:54:54 2024 ] 	Batch(4100/7879) done. Loss: 0.7863  lr:0.010000
[ Wed Jul  3 03:55:12 2024 ] 	Batch(4200/7879) done. Loss: 0.6480  lr:0.010000
[ Wed Jul  3 03:55:30 2024 ] 	Batch(4300/7879) done. Loss: 1.4046  lr:0.010000
[ Wed Jul  3 03:55:48 2024 ] 	Batch(4400/7879) done. Loss: 0.5855  lr:0.010000
[ Wed Jul  3 03:56:05 2024 ] 
Training: Epoch [16/120], Step [4499], Loss: 0.8185821771621704, Training Accuracy: 79.80000000000001
[ Wed Jul  3 03:56:06 2024 ] 	Batch(4500/7879) done. Loss: 0.1223  lr:0.010000
[ Wed Jul  3 03:56:24 2024 ] 	Batch(4600/7879) done. Loss: 0.8320  lr:0.010000
[ Wed Jul  3 03:56:41 2024 ] 	Batch(4700/7879) done. Loss: 1.6708  lr:0.010000
[ Wed Jul  3 03:56:59 2024 ] 	Batch(4800/7879) done. Loss: 0.6963  lr:0.010000
[ Wed Jul  3 03:57:17 2024 ] 	Batch(4900/7879) done. Loss: 0.6303  lr:0.010000
[ Wed Jul  3 03:57:35 2024 ] 
Training: Epoch [16/120], Step [4999], Loss: 0.21077492833137512, Training Accuracy: 79.6925
[ Wed Jul  3 03:57:35 2024 ] 	Batch(5000/7879) done. Loss: 0.6925  lr:0.010000
[ Wed Jul  3 03:57:53 2024 ] 	Batch(5100/7879) done. Loss: 0.4630  lr:0.010000
[ Wed Jul  3 03:58:11 2024 ] 	Batch(5200/7879) done. Loss: 0.0817  lr:0.010000
[ Wed Jul  3 03:58:30 2024 ] 	Batch(5300/7879) done. Loss: 0.4603  lr:0.010000
[ Wed Jul  3 03:58:48 2024 ] 	Batch(5400/7879) done. Loss: 0.4455  lr:0.010000
[ Wed Jul  3 03:59:07 2024 ] 
Training: Epoch [16/120], Step [5499], Loss: 0.8029191493988037, Training Accuracy: 79.6340909090909
[ Wed Jul  3 03:59:07 2024 ] 	Batch(5500/7879) done. Loss: 0.5998  lr:0.010000
[ Wed Jul  3 03:59:25 2024 ] 	Batch(5600/7879) done. Loss: 0.7159  lr:0.010000
[ Wed Jul  3 03:59:44 2024 ] 	Batch(5700/7879) done. Loss: 0.4573  lr:0.010000
[ Wed Jul  3 04:00:02 2024 ] 	Batch(5800/7879) done. Loss: 0.6801  lr:0.010000
[ Wed Jul  3 04:00:21 2024 ] 	Batch(5900/7879) done. Loss: 0.5122  lr:0.010000
[ Wed Jul  3 04:00:39 2024 ] 
Training: Epoch [16/120], Step [5999], Loss: 0.26797008514404297, Training Accuracy: 79.69583333333333
[ Wed Jul  3 04:00:40 2024 ] 	Batch(6000/7879) done. Loss: 0.7454  lr:0.010000
[ Wed Jul  3 04:00:57 2024 ] 	Batch(6100/7879) done. Loss: 1.0773  lr:0.010000
[ Wed Jul  3 04:01:15 2024 ] 	Batch(6200/7879) done. Loss: 0.0429  lr:0.010000
[ Wed Jul  3 04:01:33 2024 ] 	Batch(6300/7879) done. Loss: 0.5713  lr:0.010000
[ Wed Jul  3 04:01:51 2024 ] 	Batch(6400/7879) done. Loss: 0.1821  lr:0.010000
[ Wed Jul  3 04:02:09 2024 ] 
Training: Epoch [16/120], Step [6499], Loss: 0.9810699224472046, Training Accuracy: 79.71538461538461
[ Wed Jul  3 04:02:09 2024 ] 	Batch(6500/7879) done. Loss: 0.6388  lr:0.010000
[ Wed Jul  3 04:02:27 2024 ] 	Batch(6600/7879) done. Loss: 1.4245  lr:0.010000
[ Wed Jul  3 04:02:45 2024 ] 	Batch(6700/7879) done. Loss: 0.1507  lr:0.010000
[ Wed Jul  3 04:03:03 2024 ] 	Batch(6800/7879) done. Loss: 1.0378  lr:0.010000
[ Wed Jul  3 04:03:21 2024 ] 	Batch(6900/7879) done. Loss: 0.0108  lr:0.010000
[ Wed Jul  3 04:03:38 2024 ] 
Training: Epoch [16/120], Step [6999], Loss: 0.35809454321861267, Training Accuracy: 79.69642857142857
[ Wed Jul  3 04:03:39 2024 ] 	Batch(7000/7879) done. Loss: 0.8451  lr:0.010000
[ Wed Jul  3 04:03:56 2024 ] 	Batch(7100/7879) done. Loss: 0.3248  lr:0.010000
[ Wed Jul  3 04:04:15 2024 ] 	Batch(7200/7879) done. Loss: 0.1632  lr:0.010000
[ Wed Jul  3 04:04:32 2024 ] 	Batch(7300/7879) done. Loss: 0.6974  lr:0.010000
[ Wed Jul  3 04:04:50 2024 ] 	Batch(7400/7879) done. Loss: 0.3364  lr:0.010000
[ Wed Jul  3 04:05:08 2024 ] 
Training: Epoch [16/120], Step [7499], Loss: 0.47415101528167725, Training Accuracy: 79.62166666666667
[ Wed Jul  3 04:05:08 2024 ] 	Batch(7500/7879) done. Loss: 1.1046  lr:0.010000
[ Wed Jul  3 04:05:26 2024 ] 	Batch(7600/7879) done. Loss: 0.4749  lr:0.010000
[ Wed Jul  3 04:05:44 2024 ] 	Batch(7700/7879) done. Loss: 0.2842  lr:0.010000
[ Wed Jul  3 04:06:02 2024 ] 	Batch(7800/7879) done. Loss: 1.4240  lr:0.010000
[ Wed Jul  3 04:06:16 2024 ] 	Mean training loss: 0.6709.
[ Wed Jul  3 04:06:16 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 04:06:16 2024 ] Training epoch: 18
[ Wed Jul  3 04:06:17 2024 ] 	Batch(0/7879) done. Loss: 0.3773  lr:0.010000
[ Wed Jul  3 04:06:34 2024 ] 	Batch(100/7879) done. Loss: 0.1922  lr:0.010000
[ Wed Jul  3 04:06:52 2024 ] 	Batch(200/7879) done. Loss: 0.1633  lr:0.010000
[ Wed Jul  3 04:07:10 2024 ] 	Batch(300/7879) done. Loss: 0.5620  lr:0.010000
[ Wed Jul  3 04:07:28 2024 ] 	Batch(400/7879) done. Loss: 0.6985  lr:0.010000
[ Wed Jul  3 04:07:46 2024 ] 
Training: Epoch [17/120], Step [499], Loss: 0.8386412858963013, Training Accuracy: 80.925
[ Wed Jul  3 04:07:46 2024 ] 	Batch(500/7879) done. Loss: 0.8269  lr:0.010000
[ Wed Jul  3 04:08:04 2024 ] 	Batch(600/7879) done. Loss: 0.5132  lr:0.010000
[ Wed Jul  3 04:08:22 2024 ] 	Batch(700/7879) done. Loss: 0.2564  lr:0.010000
[ Wed Jul  3 04:08:40 2024 ] 	Batch(800/7879) done. Loss: 2.2934  lr:0.010000
[ Wed Jul  3 04:08:58 2024 ] 	Batch(900/7879) done. Loss: 0.0658  lr:0.010000
[ Wed Jul  3 04:09:15 2024 ] 
Training: Epoch [17/120], Step [999], Loss: 0.4831113815307617, Training Accuracy: 80.5625
[ Wed Jul  3 04:09:16 2024 ] 	Batch(1000/7879) done. Loss: 0.8451  lr:0.010000
[ Wed Jul  3 04:09:33 2024 ] 	Batch(1100/7879) done. Loss: 0.4234  lr:0.010000
[ Wed Jul  3 04:09:52 2024 ] 	Batch(1200/7879) done. Loss: 0.3124  lr:0.010000
[ Wed Jul  3 04:10:10 2024 ] 	Batch(1300/7879) done. Loss: 0.5031  lr:0.010000
[ Wed Jul  3 04:10:29 2024 ] 	Batch(1400/7879) done. Loss: 0.4893  lr:0.010000
[ Wed Jul  3 04:10:47 2024 ] 
Training: Epoch [17/120], Step [1499], Loss: 0.8905140161514282, Training Accuracy: 80.7
[ Wed Jul  3 04:10:47 2024 ] 	Batch(1500/7879) done. Loss: 0.4406  lr:0.010000
[ Wed Jul  3 04:11:06 2024 ] 	Batch(1600/7879) done. Loss: 0.2409  lr:0.010000
[ Wed Jul  3 04:11:24 2024 ] 	Batch(1700/7879) done. Loss: 0.1574  lr:0.010000
[ Wed Jul  3 04:11:42 2024 ] 	Batch(1800/7879) done. Loss: 0.5667  lr:0.010000
[ Wed Jul  3 04:11:59 2024 ] 	Batch(1900/7879) done. Loss: 0.5148  lr:0.010000
[ Wed Jul  3 04:12:17 2024 ] 
Training: Epoch [17/120], Step [1999], Loss: 0.2704868018627167, Training Accuracy: 80.575
[ Wed Jul  3 04:12:17 2024 ] 	Batch(2000/7879) done. Loss: 0.3414  lr:0.010000
[ Wed Jul  3 04:12:35 2024 ] 	Batch(2100/7879) done. Loss: 0.4541  lr:0.010000
[ Wed Jul  3 04:12:53 2024 ] 	Batch(2200/7879) done. Loss: 0.0961  lr:0.010000
[ Wed Jul  3 04:13:11 2024 ] 	Batch(2300/7879) done. Loss: 0.0476  lr:0.010000
[ Wed Jul  3 04:13:29 2024 ] 	Batch(2400/7879) done. Loss: 0.4114  lr:0.010000
[ Wed Jul  3 04:13:47 2024 ] 
Training: Epoch [17/120], Step [2499], Loss: 0.1927756518125534, Training Accuracy: 80.67
[ Wed Jul  3 04:13:47 2024 ] 	Batch(2500/7879) done. Loss: 0.2427  lr:0.010000
[ Wed Jul  3 04:14:05 2024 ] 	Batch(2600/7879) done. Loss: 0.9025  lr:0.010000
[ Wed Jul  3 04:14:23 2024 ] 	Batch(2700/7879) done. Loss: 0.2139  lr:0.010000
[ Wed Jul  3 04:14:41 2024 ] 	Batch(2800/7879) done. Loss: 0.5808  lr:0.010000
[ Wed Jul  3 04:14:59 2024 ] 	Batch(2900/7879) done. Loss: 0.3658  lr:0.010000
[ Wed Jul  3 04:15:16 2024 ] 
Training: Epoch [17/120], Step [2999], Loss: 1.2311068773269653, Training Accuracy: 80.37083333333334
[ Wed Jul  3 04:15:16 2024 ] 	Batch(3000/7879) done. Loss: 0.2209  lr:0.010000
[ Wed Jul  3 04:15:34 2024 ] 	Batch(3100/7879) done. Loss: 0.1278  lr:0.010000
[ Wed Jul  3 04:15:52 2024 ] 	Batch(3200/7879) done. Loss: 0.6669  lr:0.010000
[ Wed Jul  3 04:16:10 2024 ] 	Batch(3300/7879) done. Loss: 0.2954  lr:0.010000
[ Wed Jul  3 04:16:28 2024 ] 	Batch(3400/7879) done. Loss: 0.4518  lr:0.010000
[ Wed Jul  3 04:16:46 2024 ] 
Training: Epoch [17/120], Step [3499], Loss: 0.6027344465255737, Training Accuracy: 80.46428571428571
[ Wed Jul  3 04:16:46 2024 ] 	Batch(3500/7879) done. Loss: 0.4069  lr:0.010000
[ Wed Jul  3 04:17:04 2024 ] 	Batch(3600/7879) done. Loss: 0.0502  lr:0.010000
[ Wed Jul  3 04:17:22 2024 ] 	Batch(3700/7879) done. Loss: 0.3257  lr:0.010000
[ Wed Jul  3 04:17:40 2024 ] 	Batch(3800/7879) done. Loss: 0.8361  lr:0.010000
[ Wed Jul  3 04:17:58 2024 ] 	Batch(3900/7879) done. Loss: 0.6206  lr:0.010000
[ Wed Jul  3 04:18:16 2024 ] 
Training: Epoch [17/120], Step [3999], Loss: 0.5089025497436523, Training Accuracy: 80.390625
[ Wed Jul  3 04:18:16 2024 ] 	Batch(4000/7879) done. Loss: 1.0963  lr:0.010000
[ Wed Jul  3 04:18:34 2024 ] 	Batch(4100/7879) done. Loss: 1.1240  lr:0.010000
[ Wed Jul  3 04:18:52 2024 ] 	Batch(4200/7879) done. Loss: 0.4832  lr:0.010000
[ Wed Jul  3 04:19:09 2024 ] 	Batch(4300/7879) done. Loss: 0.0863  lr:0.010000
[ Wed Jul  3 04:19:28 2024 ] 	Batch(4400/7879) done. Loss: 0.6762  lr:0.010000
[ Wed Jul  3 04:19:45 2024 ] 
Training: Epoch [17/120], Step [4499], Loss: 0.4726753532886505, Training Accuracy: 80.35555555555555
[ Wed Jul  3 04:19:45 2024 ] 	Batch(4500/7879) done. Loss: 0.4482  lr:0.010000
[ Wed Jul  3 04:20:03 2024 ] 	Batch(4600/7879) done. Loss: 0.6780  lr:0.010000
[ Wed Jul  3 04:20:21 2024 ] 	Batch(4700/7879) done. Loss: 0.5905  lr:0.010000
[ Wed Jul  3 04:20:39 2024 ] 	Batch(4800/7879) done. Loss: 0.3585  lr:0.010000
[ Wed Jul  3 04:20:58 2024 ] 	Batch(4900/7879) done. Loss: 0.3384  lr:0.010000
[ Wed Jul  3 04:21:16 2024 ] 
Training: Epoch [17/120], Step [4999], Loss: 1.3625091314315796, Training Accuracy: 80.345
[ Wed Jul  3 04:21:16 2024 ] 	Batch(5000/7879) done. Loss: 0.7935  lr:0.010000
[ Wed Jul  3 04:21:35 2024 ] 	Batch(5100/7879) done. Loss: 0.2923  lr:0.010000
[ Wed Jul  3 04:21:54 2024 ] 	Batch(5200/7879) done. Loss: 0.7763  lr:0.010000
[ Wed Jul  3 04:22:12 2024 ] 	Batch(5300/7879) done. Loss: 1.7214  lr:0.010000
[ Wed Jul  3 04:22:31 2024 ] 	Batch(5400/7879) done. Loss: 0.7513  lr:0.010000
[ Wed Jul  3 04:22:49 2024 ] 
Training: Epoch [17/120], Step [5499], Loss: 0.5135037302970886, Training Accuracy: 80.30454545454545
[ Wed Jul  3 04:22:49 2024 ] 	Batch(5500/7879) done. Loss: 0.4828  lr:0.010000
[ Wed Jul  3 04:23:08 2024 ] 	Batch(5600/7879) done. Loss: 0.2024  lr:0.010000
[ Wed Jul  3 04:23:26 2024 ] 	Batch(5700/7879) done. Loss: 0.5146  lr:0.010000
[ Wed Jul  3 04:23:44 2024 ] 	Batch(5800/7879) done. Loss: 0.5948  lr:0.010000
[ Wed Jul  3 04:24:01 2024 ] 	Batch(5900/7879) done. Loss: 0.8929  lr:0.010000
[ Wed Jul  3 04:24:19 2024 ] 
Training: Epoch [17/120], Step [5999], Loss: 0.3347228169441223, Training Accuracy: 80.26666666666667
[ Wed Jul  3 04:24:19 2024 ] 	Batch(6000/7879) done. Loss: 0.6356  lr:0.010000
[ Wed Jul  3 04:24:38 2024 ] 	Batch(6100/7879) done. Loss: 1.2112  lr:0.010000
[ Wed Jul  3 04:24:57 2024 ] 	Batch(6200/7879) done. Loss: 0.7210  lr:0.010000
[ Wed Jul  3 04:25:15 2024 ] 	Batch(6300/7879) done. Loss: 0.6677  lr:0.010000
[ Wed Jul  3 04:25:34 2024 ] 	Batch(6400/7879) done. Loss: 0.6421  lr:0.010000
[ Wed Jul  3 04:25:52 2024 ] 
Training: Epoch [17/120], Step [6499], Loss: 0.545066237449646, Training Accuracy: 80.24615384615385
[ Wed Jul  3 04:25:52 2024 ] 	Batch(6500/7879) done. Loss: 0.2412  lr:0.010000
[ Wed Jul  3 04:26:11 2024 ] 	Batch(6600/7879) done. Loss: 1.6769  lr:0.010000
[ Wed Jul  3 04:26:29 2024 ] 	Batch(6700/7879) done. Loss: 0.1303  lr:0.010000
[ Wed Jul  3 04:26:48 2024 ] 	Batch(6800/7879) done. Loss: 0.8301  lr:0.010000
[ Wed Jul  3 04:27:06 2024 ] 	Batch(6900/7879) done. Loss: 0.6460  lr:0.010000
[ Wed Jul  3 04:27:24 2024 ] 
Training: Epoch [17/120], Step [6999], Loss: 0.06441435217857361, Training Accuracy: 80.19821428571429
[ Wed Jul  3 04:27:24 2024 ] 	Batch(7000/7879) done. Loss: 1.6106  lr:0.010000
[ Wed Jul  3 04:27:42 2024 ] 	Batch(7100/7879) done. Loss: 0.9469  lr:0.010000
[ Wed Jul  3 04:28:00 2024 ] 	Batch(7200/7879) done. Loss: 0.5354  lr:0.010000
[ Wed Jul  3 04:28:18 2024 ] 	Batch(7300/7879) done. Loss: 0.6833  lr:0.010000
[ Wed Jul  3 04:28:37 2024 ] 	Batch(7400/7879) done. Loss: 0.5359  lr:0.010000
[ Wed Jul  3 04:28:55 2024 ] 
Training: Epoch [17/120], Step [7499], Loss: 1.248092532157898, Training Accuracy: 80.18333333333332
[ Wed Jul  3 04:28:55 2024 ] 	Batch(7500/7879) done. Loss: 0.2073  lr:0.010000
[ Wed Jul  3 04:29:14 2024 ] 	Batch(7600/7879) done. Loss: 0.6748  lr:0.010000
[ Wed Jul  3 04:29:32 2024 ] 	Batch(7700/7879) done. Loss: 0.2966  lr:0.010000
[ Wed Jul  3 04:29:51 2024 ] 	Batch(7800/7879) done. Loss: 0.2183  lr:0.010000
[ Wed Jul  3 04:30:05 2024 ] 	Mean training loss: 0.6511.
[ Wed Jul  3 04:30:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 04:30:06 2024 ] Training epoch: 19
[ Wed Jul  3 04:30:06 2024 ] 	Batch(0/7879) done. Loss: 0.6149  lr:0.010000
[ Wed Jul  3 04:30:24 2024 ] 	Batch(100/7879) done. Loss: 0.3391  lr:0.010000
[ Wed Jul  3 04:30:42 2024 ] 	Batch(200/7879) done. Loss: 0.6188  lr:0.010000
[ Wed Jul  3 04:31:00 2024 ] 	Batch(300/7879) done. Loss: 0.7549  lr:0.010000
[ Wed Jul  3 04:31:18 2024 ] 	Batch(400/7879) done. Loss: 0.2161  lr:0.010000
[ Wed Jul  3 04:31:35 2024 ] 
Training: Epoch [18/120], Step [499], Loss: 0.22805547714233398, Training Accuracy: 81.125
[ Wed Jul  3 04:31:36 2024 ] 	Batch(500/7879) done. Loss: 0.0714  lr:0.010000
[ Wed Jul  3 04:31:53 2024 ] 	Batch(600/7879) done. Loss: 1.0978  lr:0.010000
[ Wed Jul  3 04:32:11 2024 ] 	Batch(700/7879) done. Loss: 0.6566  lr:0.010000
[ Wed Jul  3 04:32:29 2024 ] 	Batch(800/7879) done. Loss: 0.4716  lr:0.010000
[ Wed Jul  3 04:32:47 2024 ] 	Batch(900/7879) done. Loss: 0.7697  lr:0.010000
[ Wed Jul  3 04:33:05 2024 ] 
Training: Epoch [18/120], Step [999], Loss: 0.048843223601579666, Training Accuracy: 81.45
[ Wed Jul  3 04:33:05 2024 ] 	Batch(1000/7879) done. Loss: 0.0812  lr:0.010000
[ Wed Jul  3 04:33:23 2024 ] 	Batch(1100/7879) done. Loss: 0.1083  lr:0.010000
[ Wed Jul  3 04:33:41 2024 ] 	Batch(1200/7879) done. Loss: 0.6831  lr:0.010000
[ Wed Jul  3 04:33:59 2024 ] 	Batch(1300/7879) done. Loss: 0.4992  lr:0.010000
[ Wed Jul  3 04:34:17 2024 ] 	Batch(1400/7879) done. Loss: 0.3244  lr:0.010000
[ Wed Jul  3 04:34:34 2024 ] 
Training: Epoch [18/120], Step [1499], Loss: 0.7407493591308594, Training Accuracy: 81.10000000000001
[ Wed Jul  3 04:34:34 2024 ] 	Batch(1500/7879) done. Loss: 0.3586  lr:0.010000
[ Wed Jul  3 04:34:52 2024 ] 	Batch(1600/7879) done. Loss: 0.6167  lr:0.010000
[ Wed Jul  3 04:35:11 2024 ] 	Batch(1700/7879) done. Loss: 0.3117  lr:0.010000
[ Wed Jul  3 04:35:29 2024 ] 	Batch(1800/7879) done. Loss: 0.8702  lr:0.010000
[ Wed Jul  3 04:35:48 2024 ] 	Batch(1900/7879) done. Loss: 1.0957  lr:0.010000
[ Wed Jul  3 04:36:06 2024 ] 
Training: Epoch [18/120], Step [1999], Loss: 0.181500643491745, Training Accuracy: 81.08125000000001
[ Wed Jul  3 04:36:07 2024 ] 	Batch(2000/7879) done. Loss: 0.5872  lr:0.010000
[ Wed Jul  3 04:36:25 2024 ] 	Batch(2100/7879) done. Loss: 0.2817  lr:0.010000
[ Wed Jul  3 04:36:44 2024 ] 	Batch(2200/7879) done. Loss: 0.1340  lr:0.010000
[ Wed Jul  3 04:37:02 2024 ] 	Batch(2300/7879) done. Loss: 0.5743  lr:0.010000
[ Wed Jul  3 04:37:20 2024 ] 	Batch(2400/7879) done. Loss: 0.5023  lr:0.010000
[ Wed Jul  3 04:37:39 2024 ] 
Training: Epoch [18/120], Step [2499], Loss: 0.720702588558197, Training Accuracy: 81.285
[ Wed Jul  3 04:37:39 2024 ] 	Batch(2500/7879) done. Loss: 0.5052  lr:0.010000
[ Wed Jul  3 04:37:57 2024 ] 	Batch(2600/7879) done. Loss: 0.3466  lr:0.010000
[ Wed Jul  3 04:38:16 2024 ] 	Batch(2700/7879) done. Loss: 1.4423  lr:0.010000
[ Wed Jul  3 04:38:35 2024 ] 	Batch(2800/7879) done. Loss: 0.3509  lr:0.010000
[ Wed Jul  3 04:38:53 2024 ] 	Batch(2900/7879) done. Loss: 0.6802  lr:0.010000
[ Wed Jul  3 04:39:10 2024 ] 
Training: Epoch [18/120], Step [2999], Loss: 0.7065033316612244, Training Accuracy: 81.05416666666667
[ Wed Jul  3 04:39:10 2024 ] 	Batch(3000/7879) done. Loss: 0.2627  lr:0.010000
[ Wed Jul  3 04:39:29 2024 ] 	Batch(3100/7879) done. Loss: 0.6729  lr:0.010000
[ Wed Jul  3 04:39:48 2024 ] 	Batch(3200/7879) done. Loss: 0.3918  lr:0.010000
[ Wed Jul  3 04:40:07 2024 ] 	Batch(3300/7879) done. Loss: 0.8226  lr:0.010000
[ Wed Jul  3 04:40:25 2024 ] 	Batch(3400/7879) done. Loss: 0.7731  lr:0.010000
[ Wed Jul  3 04:40:44 2024 ] 
Training: Epoch [18/120], Step [3499], Loss: 0.8680216670036316, Training Accuracy: 80.90357142857142
[ Wed Jul  3 04:40:44 2024 ] 	Batch(3500/7879) done. Loss: 0.2794  lr:0.010000
[ Wed Jul  3 04:41:02 2024 ] 	Batch(3600/7879) done. Loss: 0.2923  lr:0.010000
[ Wed Jul  3 04:41:20 2024 ] 	Batch(3700/7879) done. Loss: 0.7530  lr:0.010000
[ Wed Jul  3 04:41:38 2024 ] 	Batch(3800/7879) done. Loss: 0.6634  lr:0.010000
[ Wed Jul  3 04:41:56 2024 ] 	Batch(3900/7879) done. Loss: 0.2742  lr:0.010000
[ Wed Jul  3 04:42:14 2024 ] 
Training: Epoch [18/120], Step [3999], Loss: 0.6339119076728821, Training Accuracy: 80.925
[ Wed Jul  3 04:42:14 2024 ] 	Batch(4000/7879) done. Loss: 0.5899  lr:0.010000
[ Wed Jul  3 04:42:33 2024 ] 	Batch(4100/7879) done. Loss: 0.8004  lr:0.010000
[ Wed Jul  3 04:42:51 2024 ] 	Batch(4200/7879) done. Loss: 0.5522  lr:0.010000
[ Wed Jul  3 04:43:10 2024 ] 	Batch(4300/7879) done. Loss: 0.0470  lr:0.010000
[ Wed Jul  3 04:43:29 2024 ] 	Batch(4400/7879) done. Loss: 1.1861  lr:0.010000
[ Wed Jul  3 04:43:47 2024 ] 
Training: Epoch [18/120], Step [4499], Loss: 0.5562102794647217, Training Accuracy: 80.91666666666667
[ Wed Jul  3 04:43:47 2024 ] 	Batch(4500/7879) done. Loss: 1.3199  lr:0.010000
[ Wed Jul  3 04:44:05 2024 ] 	Batch(4600/7879) done. Loss: 0.7532  lr:0.010000
[ Wed Jul  3 04:44:23 2024 ] 	Batch(4700/7879) done. Loss: 0.8296  lr:0.010000
[ Wed Jul  3 04:44:41 2024 ] 	Batch(4800/7879) done. Loss: 0.3393  lr:0.010000
[ Wed Jul  3 04:44:59 2024 ] 	Batch(4900/7879) done. Loss: 0.3812  lr:0.010000
[ Wed Jul  3 04:45:17 2024 ] 
Training: Epoch [18/120], Step [4999], Loss: 0.05252327769994736, Training Accuracy: 80.84750000000001
[ Wed Jul  3 04:45:17 2024 ] 	Batch(5000/7879) done. Loss: 0.7554  lr:0.010000
[ Wed Jul  3 04:45:35 2024 ] 	Batch(5100/7879) done. Loss: 0.5639  lr:0.010000
[ Wed Jul  3 04:45:53 2024 ] 	Batch(5200/7879) done. Loss: 0.3998  lr:0.010000
[ Wed Jul  3 04:46:11 2024 ] 	Batch(5300/7879) done. Loss: 1.2316  lr:0.010000
[ Wed Jul  3 04:46:30 2024 ] 	Batch(5400/7879) done. Loss: 1.0115  lr:0.010000
[ Wed Jul  3 04:46:48 2024 ] 
Training: Epoch [18/120], Step [5499], Loss: 0.7115239500999451, Training Accuracy: 80.88863636363637
[ Wed Jul  3 04:46:48 2024 ] 	Batch(5500/7879) done. Loss: 1.0618  lr:0.010000
[ Wed Jul  3 04:47:07 2024 ] 	Batch(5600/7879) done. Loss: 1.0711  lr:0.010000
[ Wed Jul  3 04:47:26 2024 ] 	Batch(5700/7879) done. Loss: 1.0867  lr:0.010000
[ Wed Jul  3 04:47:44 2024 ] 	Batch(5800/7879) done. Loss: 0.6688  lr:0.010000
[ Wed Jul  3 04:48:03 2024 ] 	Batch(5900/7879) done. Loss: 0.4404  lr:0.010000
[ Wed Jul  3 04:48:21 2024 ] 
Training: Epoch [18/120], Step [5999], Loss: 1.520938754081726, Training Accuracy: 80.84583333333333
[ Wed Jul  3 04:48:21 2024 ] 	Batch(6000/7879) done. Loss: 0.4111  lr:0.010000
[ Wed Jul  3 04:48:40 2024 ] 	Batch(6100/7879) done. Loss: 0.2091  lr:0.010000
[ Wed Jul  3 04:48:58 2024 ] 	Batch(6200/7879) done. Loss: 0.4314  lr:0.010000
[ Wed Jul  3 04:49:17 2024 ] 	Batch(6300/7879) done. Loss: 0.3074  lr:0.010000
[ Wed Jul  3 04:49:35 2024 ] 	Batch(6400/7879) done. Loss: 1.6451  lr:0.010000
[ Wed Jul  3 04:49:53 2024 ] 
Training: Epoch [18/120], Step [6499], Loss: 1.18462336063385, Training Accuracy: 80.83461538461538
[ Wed Jul  3 04:49:53 2024 ] 	Batch(6500/7879) done. Loss: 0.9363  lr:0.010000
[ Wed Jul  3 04:50:11 2024 ] 	Batch(6600/7879) done. Loss: 0.2131  lr:0.010000
[ Wed Jul  3 04:50:29 2024 ] 	Batch(6700/7879) done. Loss: 1.4346  lr:0.010000
[ Wed Jul  3 04:50:47 2024 ] 	Batch(6800/7879) done. Loss: 0.9388  lr:0.010000
[ Wed Jul  3 04:51:05 2024 ] 	Batch(6900/7879) done. Loss: 1.3939  lr:0.010000
[ Wed Jul  3 04:51:23 2024 ] 
Training: Epoch [18/120], Step [6999], Loss: 1.4765280485153198, Training Accuracy: 80.74464285714285
[ Wed Jul  3 04:51:23 2024 ] 	Batch(7000/7879) done. Loss: 1.9564  lr:0.010000
[ Wed Jul  3 04:51:41 2024 ] 	Batch(7100/7879) done. Loss: 0.3044  lr:0.010000
[ Wed Jul  3 04:51:59 2024 ] 	Batch(7200/7879) done. Loss: 0.9454  lr:0.010000
[ Wed Jul  3 04:52:17 2024 ] 	Batch(7300/7879) done. Loss: 0.5116  lr:0.010000
[ Wed Jul  3 04:52:34 2024 ] 	Batch(7400/7879) done. Loss: 0.6512  lr:0.010000
[ Wed Jul  3 04:52:52 2024 ] 
Training: Epoch [18/120], Step [7499], Loss: 1.9966866970062256, Training Accuracy: 80.66
[ Wed Jul  3 04:52:52 2024 ] 	Batch(7500/7879) done. Loss: 0.8498  lr:0.010000
[ Wed Jul  3 04:53:10 2024 ] 	Batch(7600/7879) done. Loss: 0.6104  lr:0.010000
[ Wed Jul  3 04:53:29 2024 ] 	Batch(7700/7879) done. Loss: 1.4228  lr:0.010000
[ Wed Jul  3 04:53:47 2024 ] 	Batch(7800/7879) done. Loss: 1.6114  lr:0.010000
[ Wed Jul  3 04:54:02 2024 ] 	Mean training loss: 0.6342.
[ Wed Jul  3 04:54:02 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 04:54:02 2024 ] Training epoch: 20
[ Wed Jul  3 04:54:02 2024 ] 	Batch(0/7879) done. Loss: 0.6540  lr:0.010000
[ Wed Jul  3 04:54:20 2024 ] 	Batch(100/7879) done. Loss: 0.3096  lr:0.010000
[ Wed Jul  3 04:54:38 2024 ] 	Batch(200/7879) done. Loss: 0.5586  lr:0.010000
[ Wed Jul  3 04:54:56 2024 ] 	Batch(300/7879) done. Loss: 0.4586  lr:0.010000
[ Wed Jul  3 04:55:14 2024 ] 	Batch(400/7879) done. Loss: 0.7150  lr:0.010000
[ Wed Jul  3 04:55:32 2024 ] 
Training: Epoch [19/120], Step [499], Loss: 0.3899139165878296, Training Accuracy: 82.75
[ Wed Jul  3 04:55:32 2024 ] 	Batch(500/7879) done. Loss: 0.1561  lr:0.010000
[ Wed Jul  3 04:55:50 2024 ] 	Batch(600/7879) done. Loss: 0.8564  lr:0.010000
[ Wed Jul  3 04:56:08 2024 ] 	Batch(700/7879) done. Loss: 0.7792  lr:0.010000
[ Wed Jul  3 04:56:26 2024 ] 	Batch(800/7879) done. Loss: 0.5997  lr:0.010000
[ Wed Jul  3 04:56:44 2024 ] 	Batch(900/7879) done. Loss: 0.3466  lr:0.010000
[ Wed Jul  3 04:57:01 2024 ] 
Training: Epoch [19/120], Step [999], Loss: 0.48625412583351135, Training Accuracy: 82.075
[ Wed Jul  3 04:57:01 2024 ] 	Batch(1000/7879) done. Loss: 0.7313  lr:0.010000
[ Wed Jul  3 04:57:19 2024 ] 	Batch(1100/7879) done. Loss: 0.1036  lr:0.010000
[ Wed Jul  3 04:57:37 2024 ] 	Batch(1200/7879) done. Loss: 0.5551  lr:0.010000
[ Wed Jul  3 04:57:55 2024 ] 	Batch(1300/7879) done. Loss: 0.4180  lr:0.010000
[ Wed Jul  3 04:58:14 2024 ] 	Batch(1400/7879) done. Loss: 0.4473  lr:0.010000
[ Wed Jul  3 04:58:32 2024 ] 
Training: Epoch [19/120], Step [1499], Loss: 0.12660129368305206, Training Accuracy: 81.91666666666667
[ Wed Jul  3 04:58:32 2024 ] 	Batch(1500/7879) done. Loss: 0.8600  lr:0.010000
[ Wed Jul  3 04:58:51 2024 ] 	Batch(1600/7879) done. Loss: 0.7765  lr:0.010000
[ Wed Jul  3 04:59:10 2024 ] 	Batch(1700/7879) done. Loss: 0.5450  lr:0.010000
[ Wed Jul  3 04:59:28 2024 ] 	Batch(1800/7879) done. Loss: 0.4326  lr:0.010000
[ Wed Jul  3 04:59:47 2024 ] 	Batch(1900/7879) done. Loss: 0.8168  lr:0.010000
[ Wed Jul  3 05:00:04 2024 ] 
Training: Epoch [19/120], Step [1999], Loss: 0.7844271659851074, Training Accuracy: 81.88125000000001
[ Wed Jul  3 05:00:05 2024 ] 	Batch(2000/7879) done. Loss: 1.5365  lr:0.010000
[ Wed Jul  3 05:00:22 2024 ] 	Batch(2100/7879) done. Loss: 1.0923  lr:0.010000
[ Wed Jul  3 05:00:40 2024 ] 	Batch(2200/7879) done. Loss: 1.3956  lr:0.010000
[ Wed Jul  3 05:00:58 2024 ] 	Batch(2300/7879) done. Loss: 0.1006  lr:0.010000
[ Wed Jul  3 05:01:16 2024 ] 	Batch(2400/7879) done. Loss: 0.6600  lr:0.010000
[ Wed Jul  3 05:01:35 2024 ] 
Training: Epoch [19/120], Step [2499], Loss: 0.4145936965942383, Training Accuracy: 81.925
[ Wed Jul  3 05:01:35 2024 ] 	Batch(2500/7879) done. Loss: 0.8032  lr:0.010000
[ Wed Jul  3 05:01:53 2024 ] 	Batch(2600/7879) done. Loss: 0.3239  lr:0.010000
[ Wed Jul  3 05:02:12 2024 ] 	Batch(2700/7879) done. Loss: 0.1975  lr:0.010000
[ Wed Jul  3 05:02:30 2024 ] 	Batch(2800/7879) done. Loss: 0.3691  lr:0.010000
[ Wed Jul  3 05:02:48 2024 ] 	Batch(2900/7879) done. Loss: 0.0730  lr:0.010000
[ Wed Jul  3 05:03:05 2024 ] 
Training: Epoch [19/120], Step [2999], Loss: 1.0786503553390503, Training Accuracy: 81.8875
[ Wed Jul  3 05:03:06 2024 ] 	Batch(3000/7879) done. Loss: 1.2493  lr:0.010000
[ Wed Jul  3 05:03:23 2024 ] 	Batch(3100/7879) done. Loss: 0.0263  lr:0.010000
[ Wed Jul  3 05:03:41 2024 ] 	Batch(3200/7879) done. Loss: 0.1967  lr:0.010000
[ Wed Jul  3 05:04:00 2024 ] 	Batch(3300/7879) done. Loss: 0.8115  lr:0.010000
[ Wed Jul  3 05:04:18 2024 ] 	Batch(3400/7879) done. Loss: 0.4749  lr:0.010000
[ Wed Jul  3 05:04:37 2024 ] 
Training: Epoch [19/120], Step [3499], Loss: 0.23060035705566406, Training Accuracy: 81.8
[ Wed Jul  3 05:04:37 2024 ] 	Batch(3500/7879) done. Loss: 0.9874  lr:0.010000
[ Wed Jul  3 05:04:56 2024 ] 	Batch(3600/7879) done. Loss: 2.8288  lr:0.010000
[ Wed Jul  3 05:05:14 2024 ] 	Batch(3700/7879) done. Loss: 0.2391  lr:0.010000
[ Wed Jul  3 05:05:32 2024 ] 	Batch(3800/7879) done. Loss: 0.3567  lr:0.010000
[ Wed Jul  3 05:05:49 2024 ] 	Batch(3900/7879) done. Loss: 0.7077  lr:0.010000
[ Wed Jul  3 05:06:07 2024 ] 
Training: Epoch [19/120], Step [3999], Loss: 0.6368703246116638, Training Accuracy: 81.640625
[ Wed Jul  3 05:06:07 2024 ] 	Batch(4000/7879) done. Loss: 0.0729  lr:0.010000
[ Wed Jul  3 05:06:26 2024 ] 	Batch(4100/7879) done. Loss: 0.1774  lr:0.010000
[ Wed Jul  3 05:06:44 2024 ] 	Batch(4200/7879) done. Loss: 0.1598  lr:0.010000
[ Wed Jul  3 05:07:03 2024 ] 	Batch(4300/7879) done. Loss: 0.6612  lr:0.010000
[ Wed Jul  3 05:07:22 2024 ] 	Batch(4400/7879) done. Loss: 0.3166  lr:0.010000
[ Wed Jul  3 05:07:39 2024 ] 
Training: Epoch [19/120], Step [4499], Loss: 0.48498478531837463, Training Accuracy: 81.66388888888889
[ Wed Jul  3 05:07:39 2024 ] 	Batch(4500/7879) done. Loss: 0.3877  lr:0.010000
[ Wed Jul  3 05:07:57 2024 ] 	Batch(4600/7879) done. Loss: 0.4822  lr:0.010000
[ Wed Jul  3 05:08:15 2024 ] 	Batch(4700/7879) done. Loss: 0.6045  lr:0.010000
[ Wed Jul  3 05:08:33 2024 ] 	Batch(4800/7879) done. Loss: 0.2341  lr:0.010000
[ Wed Jul  3 05:08:51 2024 ] 	Batch(4900/7879) done. Loss: 1.3041  lr:0.010000
[ Wed Jul  3 05:09:09 2024 ] 
Training: Epoch [19/120], Step [4999], Loss: 0.9191126823425293, Training Accuracy: 81.605
[ Wed Jul  3 05:09:09 2024 ] 	Batch(5000/7879) done. Loss: 0.9570  lr:0.010000
[ Wed Jul  3 05:09:27 2024 ] 	Batch(5100/7879) done. Loss: 0.5052  lr:0.010000
[ Wed Jul  3 05:09:45 2024 ] 	Batch(5200/7879) done. Loss: 1.0005  lr:0.010000
[ Wed Jul  3 05:10:03 2024 ] 	Batch(5300/7879) done. Loss: 0.2539  lr:0.010000
[ Wed Jul  3 05:10:22 2024 ] 	Batch(5400/7879) done. Loss: 0.1897  lr:0.010000
[ Wed Jul  3 05:10:40 2024 ] 
Training: Epoch [19/120], Step [5499], Loss: 1.7987014055252075, Training Accuracy: 81.6
[ Wed Jul  3 05:10:40 2024 ] 	Batch(5500/7879) done. Loss: 0.0782  lr:0.010000
[ Wed Jul  3 05:10:59 2024 ] 	Batch(5600/7879) done. Loss: 0.1845  lr:0.010000
[ Wed Jul  3 05:11:18 2024 ] 	Batch(5700/7879) done. Loss: 0.2613  lr:0.010000
[ Wed Jul  3 05:11:36 2024 ] 	Batch(5800/7879) done. Loss: 0.9650  lr:0.010000
[ Wed Jul  3 05:11:55 2024 ] 	Batch(5900/7879) done. Loss: 0.9389  lr:0.010000
[ Wed Jul  3 05:12:13 2024 ] 
Training: Epoch [19/120], Step [5999], Loss: 0.43221515417099, Training Accuracy: 81.4875
[ Wed Jul  3 05:12:13 2024 ] 	Batch(6000/7879) done. Loss: 1.1145  lr:0.010000
[ Wed Jul  3 05:12:32 2024 ] 	Batch(6100/7879) done. Loss: 1.0142  lr:0.010000
[ Wed Jul  3 05:12:50 2024 ] 	Batch(6200/7879) done. Loss: 0.1985  lr:0.010000
[ Wed Jul  3 05:13:08 2024 ] 	Batch(6300/7879) done. Loss: 0.8816  lr:0.010000
[ Wed Jul  3 05:13:26 2024 ] 	Batch(6400/7879) done. Loss: 0.3741  lr:0.010000
[ Wed Jul  3 05:13:44 2024 ] 
Training: Epoch [19/120], Step [6499], Loss: 0.24943459033966064, Training Accuracy: 81.425
[ Wed Jul  3 05:13:45 2024 ] 	Batch(6500/7879) done. Loss: 0.6216  lr:0.010000
[ Wed Jul  3 05:14:03 2024 ] 	Batch(6600/7879) done. Loss: 0.5404  lr:0.010000
[ Wed Jul  3 05:14:22 2024 ] 	Batch(6700/7879) done. Loss: 0.3607  lr:0.010000
[ Wed Jul  3 05:14:40 2024 ] 	Batch(6800/7879) done. Loss: 0.4974  lr:0.010000
[ Wed Jul  3 05:14:58 2024 ] 	Batch(6900/7879) done. Loss: 0.3995  lr:0.010000
[ Wed Jul  3 05:15:16 2024 ] 
Training: Epoch [19/120], Step [6999], Loss: 0.20794393122196198, Training Accuracy: 81.50714285714285
[ Wed Jul  3 05:15:16 2024 ] 	Batch(7000/7879) done. Loss: 0.4814  lr:0.010000
[ Wed Jul  3 05:15:34 2024 ] 	Batch(7100/7879) done. Loss: 0.5662  lr:0.010000
[ Wed Jul  3 05:15:52 2024 ] 	Batch(7200/7879) done. Loss: 0.5601  lr:0.010000
[ Wed Jul  3 05:16:10 2024 ] 	Batch(7300/7879) done. Loss: 0.5334  lr:0.010000
[ Wed Jul  3 05:16:27 2024 ] 	Batch(7400/7879) done. Loss: 0.4855  lr:0.010000
[ Wed Jul  3 05:16:45 2024 ] 
Training: Epoch [19/120], Step [7499], Loss: 0.2515966594219208, Training Accuracy: 81.42666666666668
[ Wed Jul  3 05:16:45 2024 ] 	Batch(7500/7879) done. Loss: 0.4388  lr:0.010000
[ Wed Jul  3 05:17:03 2024 ] 	Batch(7600/7879) done. Loss: 0.4143  lr:0.010000
[ Wed Jul  3 05:17:21 2024 ] 	Batch(7700/7879) done. Loss: 0.7129  lr:0.010000
[ Wed Jul  3 05:17:39 2024 ] 	Batch(7800/7879) done. Loss: 0.4208  lr:0.010000
[ Wed Jul  3 05:17:53 2024 ] 	Mean training loss: 0.6081.
[ Wed Jul  3 05:17:53 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 05:17:53 2024 ] Eval epoch: 20
[ Wed Jul  3 05:22:40 2024 ] 	Mean val loss of 6365 batches: 2.25858091951104.
[ Wed Jul  3 05:22:40 2024 ] Training epoch: 21
[ Wed Jul  3 05:22:41 2024 ] 	Batch(0/7879) done. Loss: 0.2197  lr:0.010000
[ Wed Jul  3 05:22:59 2024 ] 	Batch(100/7879) done. Loss: 1.7383  lr:0.010000
[ Wed Jul  3 05:23:16 2024 ] 	Batch(200/7879) done. Loss: 0.5319  lr:0.010000
[ Wed Jul  3 05:23:34 2024 ] 	Batch(300/7879) done. Loss: 0.2747  lr:0.010000
[ Wed Jul  3 05:23:52 2024 ] 	Batch(400/7879) done. Loss: 0.7341  lr:0.010000
[ Wed Jul  3 05:24:10 2024 ] 
Training: Epoch [20/120], Step [499], Loss: 0.2612884044647217, Training Accuracy: 82.15
[ Wed Jul  3 05:24:10 2024 ] 	Batch(500/7879) done. Loss: 0.4355  lr:0.010000
[ Wed Jul  3 05:24:28 2024 ] 	Batch(600/7879) done. Loss: 0.3768  lr:0.010000
[ Wed Jul  3 05:24:46 2024 ] 	Batch(700/7879) done. Loss: 0.7144  lr:0.010000
[ Wed Jul  3 05:25:04 2024 ] 	Batch(800/7879) done. Loss: 0.4698  lr:0.010000
[ Wed Jul  3 05:25:22 2024 ] 	Batch(900/7879) done. Loss: 0.0872  lr:0.010000
[ Wed Jul  3 05:25:40 2024 ] 
Training: Epoch [20/120], Step [999], Loss: 1.1293648481369019, Training Accuracy: 82.9375
[ Wed Jul  3 05:25:40 2024 ] 	Batch(1000/7879) done. Loss: 0.1580  lr:0.010000
[ Wed Jul  3 05:25:58 2024 ] 	Batch(1100/7879) done. Loss: 0.6549  lr:0.010000
[ Wed Jul  3 05:26:16 2024 ] 	Batch(1200/7879) done. Loss: 0.4711  lr:0.010000
[ Wed Jul  3 05:26:35 2024 ] 	Batch(1300/7879) done. Loss: 1.0383  lr:0.010000
[ Wed Jul  3 05:26:53 2024 ] 	Batch(1400/7879) done. Loss: 1.0967  lr:0.010000
[ Wed Jul  3 05:27:12 2024 ] 
Training: Epoch [20/120], Step [1499], Loss: 0.4308980405330658, Training Accuracy: 82.45833333333333
[ Wed Jul  3 05:27:12 2024 ] 	Batch(1500/7879) done. Loss: 1.2423  lr:0.010000
[ Wed Jul  3 05:27:31 2024 ] 	Batch(1600/7879) done. Loss: 0.8353  lr:0.010000
[ Wed Jul  3 05:27:48 2024 ] 	Batch(1700/7879) done. Loss: 1.1488  lr:0.010000
[ Wed Jul  3 05:28:06 2024 ] 	Batch(1800/7879) done. Loss: 0.5030  lr:0.010000
[ Wed Jul  3 05:28:24 2024 ] 	Batch(1900/7879) done. Loss: 1.0812  lr:0.010000
[ Wed Jul  3 05:28:42 2024 ] 
Training: Epoch [20/120], Step [1999], Loss: 0.78730708360672, Training Accuracy: 82.56875000000001
[ Wed Jul  3 05:28:43 2024 ] 	Batch(2000/7879) done. Loss: 0.4158  lr:0.010000
[ Wed Jul  3 05:29:01 2024 ] 	Batch(2100/7879) done. Loss: 0.5109  lr:0.010000
[ Wed Jul  3 05:29:20 2024 ] 	Batch(2200/7879) done. Loss: 1.1457  lr:0.010000
[ Wed Jul  3 05:29:38 2024 ] 	Batch(2300/7879) done. Loss: 0.2682  lr:0.010000
[ Wed Jul  3 05:29:57 2024 ] 	Batch(2400/7879) done. Loss: 1.2434  lr:0.010000
[ Wed Jul  3 05:30:15 2024 ] 
Training: Epoch [20/120], Step [2499], Loss: 0.16026997566223145, Training Accuracy: 82.43
[ Wed Jul  3 05:30:15 2024 ] 	Batch(2500/7879) done. Loss: 0.2692  lr:0.010000
[ Wed Jul  3 05:30:34 2024 ] 	Batch(2600/7879) done. Loss: 0.2263  lr:0.010000
[ Wed Jul  3 05:30:52 2024 ] 	Batch(2700/7879) done. Loss: 0.5629  lr:0.010000
[ Wed Jul  3 05:31:11 2024 ] 	Batch(2800/7879) done. Loss: 0.1941  lr:0.010000
[ Wed Jul  3 05:31:29 2024 ] 	Batch(2900/7879) done. Loss: 1.5133  lr:0.010000
[ Wed Jul  3 05:31:46 2024 ] 
Training: Epoch [20/120], Step [2999], Loss: 0.8557952642440796, Training Accuracy: 82.40833333333333
[ Wed Jul  3 05:31:46 2024 ] 	Batch(3000/7879) done. Loss: 1.5539  lr:0.010000
[ Wed Jul  3 05:32:04 2024 ] 	Batch(3100/7879) done. Loss: 0.3132  lr:0.010000
[ Wed Jul  3 05:32:22 2024 ] 	Batch(3200/7879) done. Loss: 1.1890  lr:0.010000
[ Wed Jul  3 05:32:40 2024 ] 	Batch(3300/7879) done. Loss: 0.8754  lr:0.010000
[ Wed Jul  3 05:32:58 2024 ] 	Batch(3400/7879) done. Loss: 0.8773  lr:0.010000
[ Wed Jul  3 05:33:16 2024 ] 
Training: Epoch [20/120], Step [3499], Loss: 1.3378044366836548, Training Accuracy: 82.19642857142857
[ Wed Jul  3 05:33:16 2024 ] 	Batch(3500/7879) done. Loss: 1.4821  lr:0.010000
[ Wed Jul  3 05:33:34 2024 ] 	Batch(3600/7879) done. Loss: 0.4203  lr:0.010000
[ Wed Jul  3 05:33:52 2024 ] 	Batch(3700/7879) done. Loss: 0.6633  lr:0.010000
[ Wed Jul  3 05:34:10 2024 ] 	Batch(3800/7879) done. Loss: 0.1655  lr:0.010000
[ Wed Jul  3 05:34:28 2024 ] 	Batch(3900/7879) done. Loss: 0.7604  lr:0.010000
[ Wed Jul  3 05:34:45 2024 ] 
Training: Epoch [20/120], Step [3999], Loss: 0.1733943223953247, Training Accuracy: 82.05624999999999
[ Wed Jul  3 05:34:46 2024 ] 	Batch(4000/7879) done. Loss: 0.4772  lr:0.010000
[ Wed Jul  3 05:35:04 2024 ] 	Batch(4100/7879) done. Loss: 0.3787  lr:0.010000
[ Wed Jul  3 05:35:21 2024 ] 	Batch(4200/7879) done. Loss: 1.7328  lr:0.010000
[ Wed Jul  3 05:35:39 2024 ] 	Batch(4300/7879) done. Loss: 1.0353  lr:0.010000
[ Wed Jul  3 05:35:57 2024 ] 	Batch(4400/7879) done. Loss: 0.6325  lr:0.010000
[ Wed Jul  3 05:36:15 2024 ] 
Training: Epoch [20/120], Step [4499], Loss: 0.9016227126121521, Training Accuracy: 81.89722222222223
[ Wed Jul  3 05:36:15 2024 ] 	Batch(4500/7879) done. Loss: 1.0920  lr:0.010000
[ Wed Jul  3 05:36:33 2024 ] 	Batch(4600/7879) done. Loss: 0.6868  lr:0.010000
[ Wed Jul  3 05:36:51 2024 ] 	Batch(4700/7879) done. Loss: 0.0857  lr:0.010000
[ Wed Jul  3 05:37:09 2024 ] 	Batch(4800/7879) done. Loss: 0.3897  lr:0.010000
[ Wed Jul  3 05:37:27 2024 ] 	Batch(4900/7879) done. Loss: 0.9104  lr:0.010000
[ Wed Jul  3 05:37:46 2024 ] 
Training: Epoch [20/120], Step [4999], Loss: 0.24307912588119507, Training Accuracy: 81.83
[ Wed Jul  3 05:37:46 2024 ] 	Batch(5000/7879) done. Loss: 0.6286  lr:0.010000
[ Wed Jul  3 05:38:04 2024 ] 	Batch(5100/7879) done. Loss: 0.2613  lr:0.010000
[ Wed Jul  3 05:38:22 2024 ] 	Batch(5200/7879) done. Loss: 0.7647  lr:0.010000
[ Wed Jul  3 05:38:40 2024 ] 	Batch(5300/7879) done. Loss: 0.4675  lr:0.010000
[ Wed Jul  3 05:38:58 2024 ] 	Batch(5400/7879) done. Loss: 0.8700  lr:0.010000
[ Wed Jul  3 05:39:15 2024 ] 
Training: Epoch [20/120], Step [5499], Loss: 0.4260190427303314, Training Accuracy: 81.73636363636363
[ Wed Jul  3 05:39:16 2024 ] 	Batch(5500/7879) done. Loss: 0.1706  lr:0.010000
[ Wed Jul  3 05:39:34 2024 ] 	Batch(5600/7879) done. Loss: 0.4397  lr:0.010000
[ Wed Jul  3 05:39:51 2024 ] 	Batch(5700/7879) done. Loss: 0.3056  lr:0.010000
[ Wed Jul  3 05:40:09 2024 ] 	Batch(5800/7879) done. Loss: 0.4448  lr:0.010000
[ Wed Jul  3 05:40:27 2024 ] 	Batch(5900/7879) done. Loss: 0.3293  lr:0.010000
[ Wed Jul  3 05:40:45 2024 ] 
Training: Epoch [20/120], Step [5999], Loss: 0.12289723008871078, Training Accuracy: 81.67916666666667
[ Wed Jul  3 05:40:45 2024 ] 	Batch(6000/7879) done. Loss: 0.3925  lr:0.010000
[ Wed Jul  3 05:41:03 2024 ] 	Batch(6100/7879) done. Loss: 1.1294  lr:0.010000
[ Wed Jul  3 05:41:21 2024 ] 	Batch(6200/7879) done. Loss: 0.3038  lr:0.010000
[ Wed Jul  3 05:41:39 2024 ] 	Batch(6300/7879) done. Loss: 1.0868  lr:0.010000
[ Wed Jul  3 05:41:57 2024 ] 	Batch(6400/7879) done. Loss: 0.3104  lr:0.010000
[ Wed Jul  3 05:42:15 2024 ] 
Training: Epoch [20/120], Step [6499], Loss: 0.18775422871112823, Training Accuracy: 81.71538461538461
[ Wed Jul  3 05:42:15 2024 ] 	Batch(6500/7879) done. Loss: 0.0864  lr:0.010000
[ Wed Jul  3 05:42:33 2024 ] 	Batch(6600/7879) done. Loss: 0.4834  lr:0.010000
[ Wed Jul  3 05:42:51 2024 ] 	Batch(6700/7879) done. Loss: 0.3192  lr:0.010000
[ Wed Jul  3 05:43:09 2024 ] 	Batch(6800/7879) done. Loss: 0.7924  lr:0.010000
[ Wed Jul  3 05:43:27 2024 ] 	Batch(6900/7879) done. Loss: 0.5375  lr:0.010000
[ Wed Jul  3 05:43:45 2024 ] 
Training: Epoch [20/120], Step [6999], Loss: 0.6502304077148438, Training Accuracy: 81.73214285714285
[ Wed Jul  3 05:43:46 2024 ] 	Batch(7000/7879) done. Loss: 0.9236  lr:0.010000
[ Wed Jul  3 05:44:04 2024 ] 	Batch(7100/7879) done. Loss: 0.4932  lr:0.010000
[ Wed Jul  3 05:44:22 2024 ] 	Batch(7200/7879) done. Loss: 0.1876  lr:0.010000
[ Wed Jul  3 05:44:40 2024 ] 	Batch(7300/7879) done. Loss: 0.8047  lr:0.010000
[ Wed Jul  3 05:44:57 2024 ] 	Batch(7400/7879) done. Loss: 1.0577  lr:0.010000
[ Wed Jul  3 05:45:15 2024 ] 
Training: Epoch [20/120], Step [7499], Loss: 0.17223283648490906, Training Accuracy: 81.71166666666667
[ Wed Jul  3 05:45:15 2024 ] 	Batch(7500/7879) done. Loss: 0.2797  lr:0.010000
[ Wed Jul  3 05:45:33 2024 ] 	Batch(7600/7879) done. Loss: 0.7894  lr:0.010000
[ Wed Jul  3 05:45:52 2024 ] 	Batch(7700/7879) done. Loss: 1.1021  lr:0.010000
[ Wed Jul  3 05:46:10 2024 ] 	Batch(7800/7879) done. Loss: 0.1392  lr:0.010000
[ Wed Jul  3 05:46:25 2024 ] 	Mean training loss: 0.6105.
[ Wed Jul  3 05:46:25 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 05:46:25 2024 ] Training epoch: 22
[ Wed Jul  3 05:46:25 2024 ] 	Batch(0/7879) done. Loss: 0.1563  lr:0.010000
[ Wed Jul  3 05:46:43 2024 ] 	Batch(100/7879) done. Loss: 0.7544  lr:0.010000
[ Wed Jul  3 05:47:01 2024 ] 	Batch(200/7879) done. Loss: 0.7521  lr:0.010000
[ Wed Jul  3 05:47:19 2024 ] 	Batch(300/7879) done. Loss: 0.3764  lr:0.010000
[ Wed Jul  3 05:47:37 2024 ] 	Batch(400/7879) done. Loss: 0.2868  lr:0.010000
[ Wed Jul  3 05:47:55 2024 ] 
Training: Epoch [21/120], Step [499], Loss: 1.3960953950881958, Training Accuracy: 83.1
[ Wed Jul  3 05:47:55 2024 ] 	Batch(500/7879) done. Loss: 0.6275  lr:0.010000
[ Wed Jul  3 05:48:13 2024 ] 	Batch(600/7879) done. Loss: 1.0857  lr:0.010000
[ Wed Jul  3 05:48:31 2024 ] 	Batch(700/7879) done. Loss: 1.1971  lr:0.010000
[ Wed Jul  3 05:48:49 2024 ] 	Batch(800/7879) done. Loss: 0.2345  lr:0.010000
[ Wed Jul  3 05:49:07 2024 ] 	Batch(900/7879) done. Loss: 0.7579  lr:0.010000
[ Wed Jul  3 05:49:24 2024 ] 
Training: Epoch [21/120], Step [999], Loss: 0.5779711008071899, Training Accuracy: 83.05
[ Wed Jul  3 05:49:24 2024 ] 	Batch(1000/7879) done. Loss: 0.7523  lr:0.010000
[ Wed Jul  3 05:49:42 2024 ] 	Batch(1100/7879) done. Loss: 0.5019  lr:0.010000
[ Wed Jul  3 05:50:00 2024 ] 	Batch(1200/7879) done. Loss: 0.4287  lr:0.010000
[ Wed Jul  3 05:50:19 2024 ] 	Batch(1300/7879) done. Loss: 0.2506  lr:0.010000
[ Wed Jul  3 05:50:37 2024 ] 	Batch(1400/7879) done. Loss: 0.4765  lr:0.010000
[ Wed Jul  3 05:50:56 2024 ] 
Training: Epoch [21/120], Step [1499], Loss: 0.08610443025827408, Training Accuracy: 82.91666666666667
[ Wed Jul  3 05:50:56 2024 ] 	Batch(1500/7879) done. Loss: 0.2135  lr:0.010000
[ Wed Jul  3 05:51:15 2024 ] 	Batch(1600/7879) done. Loss: 0.1477  lr:0.010000
[ Wed Jul  3 05:51:33 2024 ] 	Batch(1700/7879) done. Loss: 0.0915  lr:0.010000
[ Wed Jul  3 05:51:52 2024 ] 	Batch(1800/7879) done. Loss: 0.0505  lr:0.010000
[ Wed Jul  3 05:52:10 2024 ] 	Batch(1900/7879) done. Loss: 0.2432  lr:0.010000
[ Wed Jul  3 05:52:29 2024 ] 
Training: Epoch [21/120], Step [1999], Loss: 0.44162970781326294, Training Accuracy: 82.75625
[ Wed Jul  3 05:52:29 2024 ] 	Batch(2000/7879) done. Loss: 0.3912  lr:0.010000
[ Wed Jul  3 05:52:47 2024 ] 	Batch(2100/7879) done. Loss: 0.2830  lr:0.010000
[ Wed Jul  3 05:53:05 2024 ] 	Batch(2200/7879) done. Loss: 0.4437  lr:0.010000
[ Wed Jul  3 05:53:23 2024 ] 	Batch(2300/7879) done. Loss: 1.0263  lr:0.010000
[ Wed Jul  3 05:53:41 2024 ] 	Batch(2400/7879) done. Loss: 1.2940  lr:0.010000
[ Wed Jul  3 05:53:59 2024 ] 
Training: Epoch [21/120], Step [2499], Loss: 0.9967620968818665, Training Accuracy: 82.78
[ Wed Jul  3 05:53:59 2024 ] 	Batch(2500/7879) done. Loss: 1.2380  lr:0.010000
[ Wed Jul  3 05:54:17 2024 ] 	Batch(2600/7879) done. Loss: 0.7028  lr:0.010000
[ Wed Jul  3 05:54:36 2024 ] 	Batch(2700/7879) done. Loss: 0.9850  lr:0.010000
[ Wed Jul  3 05:54:54 2024 ] 	Batch(2800/7879) done. Loss: 0.3355  lr:0.010000
[ Wed Jul  3 05:55:12 2024 ] 	Batch(2900/7879) done. Loss: 0.8621  lr:0.010000
[ Wed Jul  3 05:55:29 2024 ] 
Training: Epoch [21/120], Step [2999], Loss: 0.7995244264602661, Training Accuracy: 82.44583333333333
[ Wed Jul  3 05:55:30 2024 ] 	Batch(3000/7879) done. Loss: 0.8693  lr:0.010000
[ Wed Jul  3 05:55:47 2024 ] 	Batch(3100/7879) done. Loss: 0.6868  lr:0.010000
[ Wed Jul  3 05:56:05 2024 ] 	Batch(3200/7879) done. Loss: 0.4596  lr:0.010000
[ Wed Jul  3 05:56:23 2024 ] 	Batch(3300/7879) done. Loss: 0.6656  lr:0.010000
[ Wed Jul  3 05:56:41 2024 ] 	Batch(3400/7879) done. Loss: 0.7170  lr:0.010000
[ Wed Jul  3 05:56:59 2024 ] 
Training: Epoch [21/120], Step [3499], Loss: 1.3577748537063599, Training Accuracy: 82.39999999999999
[ Wed Jul  3 05:56:59 2024 ] 	Batch(3500/7879) done. Loss: 1.4704  lr:0.010000
[ Wed Jul  3 05:57:17 2024 ] 	Batch(3600/7879) done. Loss: 1.4698  lr:0.010000
[ Wed Jul  3 05:57:36 2024 ] 	Batch(3700/7879) done. Loss: 0.6439  lr:0.010000
[ Wed Jul  3 05:57:54 2024 ] 	Batch(3800/7879) done. Loss: 0.9991  lr:0.010000
[ Wed Jul  3 05:58:13 2024 ] 	Batch(3900/7879) done. Loss: 0.1377  lr:0.010000
[ Wed Jul  3 05:58:31 2024 ] 
Training: Epoch [21/120], Step [3999], Loss: 0.3199712038040161, Training Accuracy: 82.31875
[ Wed Jul  3 05:58:31 2024 ] 	Batch(4000/7879) done. Loss: 0.4646  lr:0.010000
[ Wed Jul  3 05:58:49 2024 ] 	Batch(4100/7879) done. Loss: 0.9274  lr:0.010000
[ Wed Jul  3 05:59:07 2024 ] 	Batch(4200/7879) done. Loss: 0.1690  lr:0.010000
[ Wed Jul  3 05:59:25 2024 ] 	Batch(4300/7879) done. Loss: 1.0832  lr:0.010000
[ Wed Jul  3 05:59:43 2024 ] 	Batch(4400/7879) done. Loss: 0.4165  lr:0.010000
[ Wed Jul  3 06:00:01 2024 ] 
Training: Epoch [21/120], Step [4499], Loss: 0.6251183152198792, Training Accuracy: 82.3
[ Wed Jul  3 06:00:02 2024 ] 	Batch(4500/7879) done. Loss: 0.3508  lr:0.010000
[ Wed Jul  3 06:00:20 2024 ] 	Batch(4600/7879) done. Loss: 1.1416  lr:0.010000
[ Wed Jul  3 06:00:39 2024 ] 	Batch(4700/7879) done. Loss: 0.5364  lr:0.010000
[ Wed Jul  3 06:00:57 2024 ] 	Batch(4800/7879) done. Loss: 0.8057  lr:0.010000
[ Wed Jul  3 06:01:15 2024 ] 	Batch(4900/7879) done. Loss: 0.7020  lr:0.010000
[ Wed Jul  3 06:01:33 2024 ] 
Training: Epoch [21/120], Step [4999], Loss: 0.13006529211997986, Training Accuracy: 82.235
[ Wed Jul  3 06:01:33 2024 ] 	Batch(5000/7879) done. Loss: 1.4813  lr:0.010000
[ Wed Jul  3 06:01:51 2024 ] 	Batch(5100/7879) done. Loss: 1.5727  lr:0.010000
[ Wed Jul  3 06:02:09 2024 ] 	Batch(5200/7879) done. Loss: 0.5361  lr:0.010000
[ Wed Jul  3 06:02:27 2024 ] 	Batch(5300/7879) done. Loss: 0.7346  lr:0.010000
[ Wed Jul  3 06:02:44 2024 ] 	Batch(5400/7879) done. Loss: 1.2666  lr:0.010000
[ Wed Jul  3 06:03:02 2024 ] 
Training: Epoch [21/120], Step [5499], Loss: 0.12297660112380981, Training Accuracy: 82.22500000000001
[ Wed Jul  3 06:03:02 2024 ] 	Batch(5500/7879) done. Loss: 0.4793  lr:0.010000
[ Wed Jul  3 06:03:20 2024 ] 	Batch(5600/7879) done. Loss: 0.3648  lr:0.010000
[ Wed Jul  3 06:03:38 2024 ] 	Batch(5700/7879) done. Loss: 0.2139  lr:0.010000
[ Wed Jul  3 06:03:56 2024 ] 	Batch(5800/7879) done. Loss: 0.1322  lr:0.010000
[ Wed Jul  3 06:04:14 2024 ] 	Batch(5900/7879) done. Loss: 0.6659  lr:0.010000
[ Wed Jul  3 06:04:32 2024 ] 
Training: Epoch [21/120], Step [5999], Loss: 0.1363472193479538, Training Accuracy: 82.17708333333333
[ Wed Jul  3 06:04:32 2024 ] 	Batch(6000/7879) done. Loss: 1.1240  lr:0.010000
[ Wed Jul  3 06:04:50 2024 ] 	Batch(6100/7879) done. Loss: 0.8154  lr:0.010000
[ Wed Jul  3 06:05:08 2024 ] 	Batch(6200/7879) done. Loss: 0.3572  lr:0.010000
[ Wed Jul  3 06:05:25 2024 ] 	Batch(6300/7879) done. Loss: 0.7100  lr:0.010000
[ Wed Jul  3 06:05:43 2024 ] 	Batch(6400/7879) done. Loss: 0.5782  lr:0.010000
[ Wed Jul  3 06:06:02 2024 ] 
Training: Epoch [21/120], Step [6499], Loss: 0.5781209468841553, Training Accuracy: 82.17307692307692
[ Wed Jul  3 06:06:02 2024 ] 	Batch(6500/7879) done. Loss: 0.3242  lr:0.010000
[ Wed Jul  3 06:06:20 2024 ] 	Batch(6600/7879) done. Loss: 0.2281  lr:0.010000
[ Wed Jul  3 06:06:39 2024 ] 	Batch(6700/7879) done. Loss: 1.2068  lr:0.010000
[ Wed Jul  3 06:06:57 2024 ] 	Batch(6800/7879) done. Loss: 0.5761  lr:0.010000
[ Wed Jul  3 06:07:15 2024 ] 	Batch(6900/7879) done. Loss: 0.3583  lr:0.010000
[ Wed Jul  3 06:07:33 2024 ] 
Training: Epoch [21/120], Step [6999], Loss: 0.5208405256271362, Training Accuracy: 82.2125
[ Wed Jul  3 06:07:33 2024 ] 	Batch(7000/7879) done. Loss: 0.2116  lr:0.010000
[ Wed Jul  3 06:07:51 2024 ] 	Batch(7100/7879) done. Loss: 0.4187  lr:0.010000
[ Wed Jul  3 06:08:09 2024 ] 	Batch(7200/7879) done. Loss: 0.7585  lr:0.010000
[ Wed Jul  3 06:08:28 2024 ] 	Batch(7300/7879) done. Loss: 1.7312  lr:0.010000
[ Wed Jul  3 06:08:46 2024 ] 	Batch(7400/7879) done. Loss: 0.8857  lr:0.010000
[ Wed Jul  3 06:09:05 2024 ] 
Training: Epoch [21/120], Step [7499], Loss: 0.254370778799057, Training Accuracy: 82.155
[ Wed Jul  3 06:09:05 2024 ] 	Batch(7500/7879) done. Loss: 0.4152  lr:0.010000
[ Wed Jul  3 06:09:23 2024 ] 	Batch(7600/7879) done. Loss: 0.4119  lr:0.010000
[ Wed Jul  3 06:09:41 2024 ] 	Batch(7700/7879) done. Loss: 0.8419  lr:0.010000
[ Wed Jul  3 06:09:59 2024 ] 	Batch(7800/7879) done. Loss: 0.4545  lr:0.010000
[ Wed Jul  3 06:10:13 2024 ] 	Mean training loss: 0.5920.
[ Wed Jul  3 06:10:13 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 06:10:13 2024 ] Training epoch: 23
[ Wed Jul  3 06:10:14 2024 ] 	Batch(0/7879) done. Loss: 0.7302  lr:0.010000
[ Wed Jul  3 06:10:32 2024 ] 	Batch(100/7879) done. Loss: 0.1022  lr:0.010000
[ Wed Jul  3 06:10:50 2024 ] 	Batch(200/7879) done. Loss: 0.4344  lr:0.010000
[ Wed Jul  3 06:11:07 2024 ] 	Batch(300/7879) done. Loss: 0.2051  lr:0.010000
[ Wed Jul  3 06:11:25 2024 ] 	Batch(400/7879) done. Loss: 0.1335  lr:0.010000
[ Wed Jul  3 06:11:43 2024 ] 
Training: Epoch [22/120], Step [499], Loss: 0.7470718026161194, Training Accuracy: 84.52499999999999
[ Wed Jul  3 06:11:43 2024 ] 	Batch(500/7879) done. Loss: 0.1297  lr:0.010000
[ Wed Jul  3 06:12:01 2024 ] 	Batch(600/7879) done. Loss: 0.4264  lr:0.010000
[ Wed Jul  3 06:12:19 2024 ] 	Batch(700/7879) done. Loss: 0.5157  lr:0.010000
[ Wed Jul  3 06:12:37 2024 ] 	Batch(800/7879) done. Loss: 0.5577  lr:0.010000
[ Wed Jul  3 06:12:55 2024 ] 	Batch(900/7879) done. Loss: 0.6174  lr:0.010000
[ Wed Jul  3 06:13:13 2024 ] 
Training: Epoch [22/120], Step [999], Loss: 0.7621374130249023, Training Accuracy: 84.1
[ Wed Jul  3 06:13:13 2024 ] 	Batch(1000/7879) done. Loss: 0.0867  lr:0.010000
[ Wed Jul  3 06:13:31 2024 ] 	Batch(1100/7879) done. Loss: 0.8807  lr:0.010000
[ Wed Jul  3 06:13:49 2024 ] 	Batch(1200/7879) done. Loss: 0.5333  lr:0.010000
[ Wed Jul  3 06:14:07 2024 ] 	Batch(1300/7879) done. Loss: 0.2284  lr:0.010000
[ Wed Jul  3 06:14:25 2024 ] 	Batch(1400/7879) done. Loss: 0.3964  lr:0.010000
[ Wed Jul  3 06:14:43 2024 ] 
Training: Epoch [22/120], Step [1499], Loss: 1.3539443016052246, Training Accuracy: 83.75
[ Wed Jul  3 06:14:43 2024 ] 	Batch(1500/7879) done. Loss: 0.1711  lr:0.010000
[ Wed Jul  3 06:15:02 2024 ] 	Batch(1600/7879) done. Loss: 0.9067  lr:0.010000
[ Wed Jul  3 06:15:19 2024 ] 	Batch(1700/7879) done. Loss: 0.9259  lr:0.010000
[ Wed Jul  3 06:15:37 2024 ] 	Batch(1800/7879) done. Loss: 0.0407  lr:0.010000
[ Wed Jul  3 06:15:55 2024 ] 	Batch(1900/7879) done. Loss: 1.1858  lr:0.010000
[ Wed Jul  3 06:16:13 2024 ] 
Training: Epoch [22/120], Step [1999], Loss: 0.32400715351104736, Training Accuracy: 83.43125
[ Wed Jul  3 06:16:13 2024 ] 	Batch(2000/7879) done. Loss: 0.1948  lr:0.010000
[ Wed Jul  3 06:16:31 2024 ] 	Batch(2100/7879) done. Loss: 0.4412  lr:0.010000
[ Wed Jul  3 06:16:49 2024 ] 	Batch(2200/7879) done. Loss: 0.8606  lr:0.010000
[ Wed Jul  3 06:17:07 2024 ] 	Batch(2300/7879) done. Loss: 1.5254  lr:0.010000
[ Wed Jul  3 06:17:25 2024 ] 	Batch(2400/7879) done. Loss: 0.6153  lr:0.010000
[ Wed Jul  3 06:17:43 2024 ] 
Training: Epoch [22/120], Step [2499], Loss: 0.15338954329490662, Training Accuracy: 83.24000000000001
[ Wed Jul  3 06:17:43 2024 ] 	Batch(2500/7879) done. Loss: 0.9447  lr:0.010000
[ Wed Jul  3 06:18:01 2024 ] 	Batch(2600/7879) done. Loss: 0.5446  lr:0.010000
[ Wed Jul  3 06:18:19 2024 ] 	Batch(2700/7879) done. Loss: 0.7696  lr:0.010000
[ Wed Jul  3 06:18:37 2024 ] 	Batch(2800/7879) done. Loss: 0.2028  lr:0.010000
[ Wed Jul  3 06:18:55 2024 ] 	Batch(2900/7879) done. Loss: 0.5911  lr:0.010000
[ Wed Jul  3 06:19:12 2024 ] 
Training: Epoch [22/120], Step [2999], Loss: 0.5008239150047302, Training Accuracy: 83.1375
[ Wed Jul  3 06:19:13 2024 ] 	Batch(3000/7879) done. Loss: 0.1131  lr:0.010000
[ Wed Jul  3 06:19:30 2024 ] 	Batch(3100/7879) done. Loss: 0.3195  lr:0.010000
[ Wed Jul  3 06:19:49 2024 ] 	Batch(3200/7879) done. Loss: 0.3175  lr:0.010000
[ Wed Jul  3 06:20:07 2024 ] 	Batch(3300/7879) done. Loss: 0.1862  lr:0.010000
[ Wed Jul  3 06:20:26 2024 ] 	Batch(3400/7879) done. Loss: 0.8065  lr:0.010000
[ Wed Jul  3 06:20:44 2024 ] 
Training: Epoch [22/120], Step [3499], Loss: 0.3053877353668213, Training Accuracy: 83.03571428571429
[ Wed Jul  3 06:20:44 2024 ] 	Batch(3500/7879) done. Loss: 1.1461  lr:0.010000
[ Wed Jul  3 06:21:03 2024 ] 	Batch(3600/7879) done. Loss: 1.1367  lr:0.010000
[ Wed Jul  3 06:21:21 2024 ] 	Batch(3700/7879) done. Loss: 0.5040  lr:0.010000
[ Wed Jul  3 06:21:38 2024 ] 	Batch(3800/7879) done. Loss: 1.4370  lr:0.010000
[ Wed Jul  3 06:21:56 2024 ] 	Batch(3900/7879) done. Loss: 1.1495  lr:0.010000
[ Wed Jul  3 06:22:14 2024 ] 
Training: Epoch [22/120], Step [3999], Loss: 0.648756742477417, Training Accuracy: 82.87187499999999
[ Wed Jul  3 06:22:14 2024 ] 	Batch(4000/7879) done. Loss: 0.2382  lr:0.010000
[ Wed Jul  3 06:22:32 2024 ] 	Batch(4100/7879) done. Loss: 0.8384  lr:0.010000
[ Wed Jul  3 06:22:50 2024 ] 	Batch(4200/7879) done. Loss: 0.9195  lr:0.010000
[ Wed Jul  3 06:23:08 2024 ] 	Batch(4300/7879) done. Loss: 0.3195  lr:0.010000
[ Wed Jul  3 06:23:26 2024 ] 	Batch(4400/7879) done. Loss: 0.7433  lr:0.010000
[ Wed Jul  3 06:23:44 2024 ] 
Training: Epoch [22/120], Step [4499], Loss: 0.4374498426914215, Training Accuracy: 82.85277777777777
[ Wed Jul  3 06:23:45 2024 ] 	Batch(4500/7879) done. Loss: 0.0459  lr:0.010000
[ Wed Jul  3 06:24:03 2024 ] 	Batch(4600/7879) done. Loss: 0.9197  lr:0.010000
[ Wed Jul  3 06:24:21 2024 ] 	Batch(4700/7879) done. Loss: 0.5334  lr:0.010000
[ Wed Jul  3 06:24:39 2024 ] 	Batch(4800/7879) done. Loss: 0.5722  lr:0.010000
[ Wed Jul  3 06:24:58 2024 ] 	Batch(4900/7879) done. Loss: 0.9201  lr:0.010000
[ Wed Jul  3 06:25:16 2024 ] 
Training: Epoch [22/120], Step [4999], Loss: 0.40225809812545776, Training Accuracy: 82.83
[ Wed Jul  3 06:25:16 2024 ] 	Batch(5000/7879) done. Loss: 1.0685  lr:0.010000
[ Wed Jul  3 06:25:34 2024 ] 	Batch(5100/7879) done. Loss: 0.0292  lr:0.010000
[ Wed Jul  3 06:25:52 2024 ] 	Batch(5200/7879) done. Loss: 0.0623  lr:0.010000
[ Wed Jul  3 06:26:11 2024 ] 	Batch(5300/7879) done. Loss: 0.7406  lr:0.010000
[ Wed Jul  3 06:26:29 2024 ] 	Batch(5400/7879) done. Loss: 1.3990  lr:0.010000
[ Wed Jul  3 06:26:47 2024 ] 
Training: Epoch [22/120], Step [5499], Loss: 0.29064059257507324, Training Accuracy: 82.72727272727273
[ Wed Jul  3 06:26:47 2024 ] 	Batch(5500/7879) done. Loss: 0.2633  lr:0.010000
[ Wed Jul  3 06:27:06 2024 ] 	Batch(5600/7879) done. Loss: 0.8217  lr:0.010000
[ Wed Jul  3 06:27:24 2024 ] 	Batch(5700/7879) done. Loss: 0.5068  lr:0.010000
[ Wed Jul  3 06:27:42 2024 ] 	Batch(5800/7879) done. Loss: 1.0938  lr:0.010000
[ Wed Jul  3 06:28:00 2024 ] 	Batch(5900/7879) done. Loss: 0.8163  lr:0.010000
[ Wed Jul  3 06:28:18 2024 ] 
Training: Epoch [22/120], Step [5999], Loss: 0.20025932788848877, Training Accuracy: 82.69999999999999
[ Wed Jul  3 06:28:19 2024 ] 	Batch(6000/7879) done. Loss: 0.3573  lr:0.010000
[ Wed Jul  3 06:28:37 2024 ] 	Batch(6100/7879) done. Loss: 0.3152  lr:0.010000
[ Wed Jul  3 06:28:55 2024 ] 	Batch(6200/7879) done. Loss: 0.2155  lr:0.010000
[ Wed Jul  3 06:29:13 2024 ] 	Batch(6300/7879) done. Loss: 0.9988  lr:0.010000
[ Wed Jul  3 06:29:32 2024 ] 	Batch(6400/7879) done. Loss: 0.7796  lr:0.010000
[ Wed Jul  3 06:29:50 2024 ] 
Training: Epoch [22/120], Step [6499], Loss: 1.1615687608718872, Training Accuracy: 82.66923076923078
[ Wed Jul  3 06:29:50 2024 ] 	Batch(6500/7879) done. Loss: 0.2747  lr:0.010000
[ Wed Jul  3 06:30:08 2024 ] 	Batch(6600/7879) done. Loss: 0.4717  lr:0.010000
[ Wed Jul  3 06:30:27 2024 ] 	Batch(6700/7879) done. Loss: 0.8082  lr:0.010000
[ Wed Jul  3 06:30:45 2024 ] 	Batch(6800/7879) done. Loss: 0.4373  lr:0.010000
[ Wed Jul  3 06:31:03 2024 ] 	Batch(6900/7879) done. Loss: 0.4726  lr:0.010000
[ Wed Jul  3 06:31:21 2024 ] 
Training: Epoch [22/120], Step [6999], Loss: 0.958340048789978, Training Accuracy: 82.59285714285714
[ Wed Jul  3 06:31:22 2024 ] 	Batch(7000/7879) done. Loss: 1.5471  lr:0.010000
[ Wed Jul  3 06:31:40 2024 ] 	Batch(7100/7879) done. Loss: 0.3780  lr:0.010000
[ Wed Jul  3 06:31:58 2024 ] 	Batch(7200/7879) done. Loss: 0.1710  lr:0.010000
[ Wed Jul  3 06:32:17 2024 ] 	Batch(7300/7879) done. Loss: 0.8465  lr:0.010000
[ Wed Jul  3 06:32:35 2024 ] 	Batch(7400/7879) done. Loss: 0.6002  lr:0.010000
[ Wed Jul  3 06:32:53 2024 ] 
Training: Epoch [22/120], Step [7499], Loss: 0.20647674798965454, Training Accuracy: 82.56833333333333
[ Wed Jul  3 06:32:53 2024 ] 	Batch(7500/7879) done. Loss: 0.0740  lr:0.010000
[ Wed Jul  3 06:33:12 2024 ] 	Batch(7600/7879) done. Loss: 0.9925  lr:0.010000
[ Wed Jul  3 06:33:29 2024 ] 	Batch(7700/7879) done. Loss: 1.4981  lr:0.010000
[ Wed Jul  3 06:33:47 2024 ] 	Batch(7800/7879) done. Loss: 0.9108  lr:0.010000
[ Wed Jul  3 06:34:01 2024 ] 	Mean training loss: 0.5771.
[ Wed Jul  3 06:34:01 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 06:34:02 2024 ] Training epoch: 24
[ Wed Jul  3 06:34:02 2024 ] 	Batch(0/7879) done. Loss: 0.2089  lr:0.010000
[ Wed Jul  3 06:34:20 2024 ] 	Batch(100/7879) done. Loss: 0.2147  lr:0.010000
[ Wed Jul  3 06:34:38 2024 ] 	Batch(200/7879) done. Loss: 0.4730  lr:0.010000
[ Wed Jul  3 06:34:56 2024 ] 	Batch(300/7879) done. Loss: 0.4956  lr:0.010000
[ Wed Jul  3 06:35:15 2024 ] 	Batch(400/7879) done. Loss: 0.4185  lr:0.010000
[ Wed Jul  3 06:35:32 2024 ] 
Training: Epoch [23/120], Step [499], Loss: 1.6339261531829834, Training Accuracy: 83.45
[ Wed Jul  3 06:35:32 2024 ] 	Batch(500/7879) done. Loss: 0.4356  lr:0.010000
[ Wed Jul  3 06:35:50 2024 ] 	Batch(600/7879) done. Loss: 0.1037  lr:0.010000
[ Wed Jul  3 06:36:08 2024 ] 	Batch(700/7879) done. Loss: 0.2934  lr:0.010000
[ Wed Jul  3 06:36:26 2024 ] 	Batch(800/7879) done. Loss: 1.0936  lr:0.010000
[ Wed Jul  3 06:36:45 2024 ] 	Batch(900/7879) done. Loss: 0.7853  lr:0.010000
[ Wed Jul  3 06:37:03 2024 ] 
Training: Epoch [23/120], Step [999], Loss: 0.6027109622955322, Training Accuracy: 83.5375
[ Wed Jul  3 06:37:03 2024 ] 	Batch(1000/7879) done. Loss: 0.6286  lr:0.010000
[ Wed Jul  3 06:37:21 2024 ] 	Batch(1100/7879) done. Loss: 0.7103  lr:0.010000
[ Wed Jul  3 06:37:39 2024 ] 	Batch(1200/7879) done. Loss: 0.6381  lr:0.010000
[ Wed Jul  3 06:37:57 2024 ] 	Batch(1300/7879) done. Loss: 0.4191  lr:0.010000
[ Wed Jul  3 06:38:15 2024 ] 	Batch(1400/7879) done. Loss: 1.6624  lr:0.010000
[ Wed Jul  3 06:38:32 2024 ] 
Training: Epoch [23/120], Step [1499], Loss: 0.0966346487402916, Training Accuracy: 83.25833333333334
[ Wed Jul  3 06:38:33 2024 ] 	Batch(1500/7879) done. Loss: 0.6778  lr:0.010000
[ Wed Jul  3 06:38:51 2024 ] 	Batch(1600/7879) done. Loss: 0.3926  lr:0.010000
[ Wed Jul  3 06:39:09 2024 ] 	Batch(1700/7879) done. Loss: 0.6720  lr:0.010000
[ Wed Jul  3 06:39:28 2024 ] 	Batch(1800/7879) done. Loss: 1.1522  lr:0.010000
[ Wed Jul  3 06:39:46 2024 ] 	Batch(1900/7879) done. Loss: 0.7495  lr:0.010000
[ Wed Jul  3 06:40:04 2024 ] 
Training: Epoch [23/120], Step [1999], Loss: 0.09395966678857803, Training Accuracy: 83.38125
[ Wed Jul  3 06:40:04 2024 ] 	Batch(2000/7879) done. Loss: 0.4819  lr:0.010000
[ Wed Jul  3 06:40:23 2024 ] 	Batch(2100/7879) done. Loss: 0.3112  lr:0.010000
[ Wed Jul  3 06:40:41 2024 ] 	Batch(2200/7879) done. Loss: 0.1070  lr:0.010000
[ Wed Jul  3 06:40:58 2024 ] 	Batch(2300/7879) done. Loss: 1.0501  lr:0.010000
[ Wed Jul  3 06:41:16 2024 ] 	Batch(2400/7879) done. Loss: 0.3557  lr:0.010000
[ Wed Jul  3 06:41:34 2024 ] 
Training: Epoch [23/120], Step [2499], Loss: 0.2990175485610962, Training Accuracy: 83.31
[ Wed Jul  3 06:41:34 2024 ] 	Batch(2500/7879) done. Loss: 0.1716  lr:0.010000
[ Wed Jul  3 06:41:52 2024 ] 	Batch(2600/7879) done. Loss: 0.3691  lr:0.010000
[ Wed Jul  3 06:42:10 2024 ] 	Batch(2700/7879) done. Loss: 1.3259  lr:0.010000
[ Wed Jul  3 06:42:28 2024 ] 	Batch(2800/7879) done. Loss: 0.2648  lr:0.010000
[ Wed Jul  3 06:42:46 2024 ] 	Batch(2900/7879) done. Loss: 0.3975  lr:0.010000
[ Wed Jul  3 06:43:04 2024 ] 
Training: Epoch [23/120], Step [2999], Loss: 0.5837455987930298, Training Accuracy: 83.19166666666666
[ Wed Jul  3 06:43:04 2024 ] 	Batch(3000/7879) done. Loss: 0.5970  lr:0.010000
[ Wed Jul  3 06:43:22 2024 ] 	Batch(3100/7879) done. Loss: 0.6609  lr:0.010000
[ Wed Jul  3 06:43:40 2024 ] 	Batch(3200/7879) done. Loss: 0.5184  lr:0.010000
[ Wed Jul  3 06:43:58 2024 ] 	Batch(3300/7879) done. Loss: 0.4088  lr:0.010000
[ Wed Jul  3 06:44:16 2024 ] 	Batch(3400/7879) done. Loss: 0.3414  lr:0.010000
[ Wed Jul  3 06:44:33 2024 ] 
Training: Epoch [23/120], Step [3499], Loss: 0.5019499063491821, Training Accuracy: 82.975
[ Wed Jul  3 06:44:33 2024 ] 	Batch(3500/7879) done. Loss: 0.3048  lr:0.010000
[ Wed Jul  3 06:44:51 2024 ] 	Batch(3600/7879) done. Loss: 0.0521  lr:0.010000
[ Wed Jul  3 06:45:10 2024 ] 	Batch(3700/7879) done. Loss: 0.3857  lr:0.010000
[ Wed Jul  3 06:45:28 2024 ] 	Batch(3800/7879) done. Loss: 0.7643  lr:0.010000
[ Wed Jul  3 06:45:47 2024 ] 	Batch(3900/7879) done. Loss: 0.2999  lr:0.010000
[ Wed Jul  3 06:46:05 2024 ] 
Training: Epoch [23/120], Step [3999], Loss: 0.42958998680114746, Training Accuracy: 82.83437500000001
[ Wed Jul  3 06:46:06 2024 ] 	Batch(4000/7879) done. Loss: 0.8906  lr:0.010000
[ Wed Jul  3 06:46:24 2024 ] 	Batch(4100/7879) done. Loss: 0.1980  lr:0.010000
[ Wed Jul  3 06:46:41 2024 ] 	Batch(4200/7879) done. Loss: 0.4149  lr:0.010000
[ Wed Jul  3 06:46:59 2024 ] 	Batch(4300/7879) done. Loss: 0.8177  lr:0.010000
[ Wed Jul  3 06:47:17 2024 ] 	Batch(4400/7879) done. Loss: 0.6265  lr:0.010000
[ Wed Jul  3 06:47:36 2024 ] 
Training: Epoch [23/120], Step [4499], Loss: 0.11628454923629761, Training Accuracy: 82.88055555555556
[ Wed Jul  3 06:47:36 2024 ] 	Batch(4500/7879) done. Loss: 0.3953  lr:0.010000
[ Wed Jul  3 06:47:54 2024 ] 	Batch(4600/7879) done. Loss: 0.6207  lr:0.010000
[ Wed Jul  3 06:48:13 2024 ] 	Batch(4700/7879) done. Loss: 0.6053  lr:0.010000
[ Wed Jul  3 06:48:31 2024 ] 	Batch(4800/7879) done. Loss: 0.1331  lr:0.010000
[ Wed Jul  3 06:48:49 2024 ] 	Batch(4900/7879) done. Loss: 0.4287  lr:0.010000
[ Wed Jul  3 06:49:07 2024 ] 
Training: Epoch [23/120], Step [4999], Loss: 0.5233932733535767, Training Accuracy: 82.7675
[ Wed Jul  3 06:49:07 2024 ] 	Batch(5000/7879) done. Loss: 0.2818  lr:0.010000
[ Wed Jul  3 06:49:25 2024 ] 	Batch(5100/7879) done. Loss: 1.1609  lr:0.010000
[ Wed Jul  3 06:49:43 2024 ] 	Batch(5200/7879) done. Loss: 1.2895  lr:0.010000
[ Wed Jul  3 06:50:01 2024 ] 	Batch(5300/7879) done. Loss: 0.3881  lr:0.010000
[ Wed Jul  3 06:50:19 2024 ] 	Batch(5400/7879) done. Loss: 0.6547  lr:0.010000
[ Wed Jul  3 06:50:37 2024 ] 
Training: Epoch [23/120], Step [5499], Loss: 0.20651863515377045, Training Accuracy: 82.75227272727273
[ Wed Jul  3 06:50:37 2024 ] 	Batch(5500/7879) done. Loss: 0.9783  lr:0.010000
[ Wed Jul  3 06:50:55 2024 ] 	Batch(5600/7879) done. Loss: 0.4155  lr:0.010000
[ Wed Jul  3 06:51:13 2024 ] 	Batch(5700/7879) done. Loss: 0.1922  lr:0.010000
[ Wed Jul  3 06:51:31 2024 ] 	Batch(5800/7879) done. Loss: 0.9034  lr:0.010000
[ Wed Jul  3 06:51:48 2024 ] 	Batch(5900/7879) done. Loss: 0.8434  lr:0.010000
[ Wed Jul  3 06:52:06 2024 ] 
Training: Epoch [23/120], Step [5999], Loss: 0.5126978158950806, Training Accuracy: 82.74375
[ Wed Jul  3 06:52:06 2024 ] 	Batch(6000/7879) done. Loss: 0.4710  lr:0.010000
[ Wed Jul  3 06:52:24 2024 ] 	Batch(6100/7879) done. Loss: 0.5949  lr:0.010000
[ Wed Jul  3 06:52:42 2024 ] 	Batch(6200/7879) done. Loss: 1.2185  lr:0.010000
[ Wed Jul  3 06:53:00 2024 ] 	Batch(6300/7879) done. Loss: 0.5022  lr:0.010000
[ Wed Jul  3 06:53:18 2024 ] 	Batch(6400/7879) done. Loss: 0.5032  lr:0.010000
[ Wed Jul  3 06:53:36 2024 ] 
Training: Epoch [23/120], Step [6499], Loss: 1.1491029262542725, Training Accuracy: 82.77884615384615
[ Wed Jul  3 06:53:36 2024 ] 	Batch(6500/7879) done. Loss: 0.9008  lr:0.010000
[ Wed Jul  3 06:53:54 2024 ] 	Batch(6600/7879) done. Loss: 0.4305  lr:0.010000
[ Wed Jul  3 06:54:12 2024 ] 	Batch(6700/7879) done. Loss: 0.1606  lr:0.010000
[ Wed Jul  3 06:54:30 2024 ] 	Batch(6800/7879) done. Loss: 0.0800  lr:0.010000
[ Wed Jul  3 06:54:48 2024 ] 	Batch(6900/7879) done. Loss: 0.8752  lr:0.010000
[ Wed Jul  3 06:55:07 2024 ] 
Training: Epoch [23/120], Step [6999], Loss: 0.8650931119918823, Training Accuracy: 82.82321428571429
[ Wed Jul  3 06:55:07 2024 ] 	Batch(7000/7879) done. Loss: 0.3378  lr:0.010000
[ Wed Jul  3 06:55:25 2024 ] 	Batch(7100/7879) done. Loss: 0.2351  lr:0.010000
[ Wed Jul  3 06:55:44 2024 ] 	Batch(7200/7879) done. Loss: 0.1407  lr:0.010000
[ Wed Jul  3 06:56:03 2024 ] 	Batch(7300/7879) done. Loss: 0.1428  lr:0.010000
[ Wed Jul  3 06:56:21 2024 ] 	Batch(7400/7879) done. Loss: 0.4782  lr:0.010000
[ Wed Jul  3 06:56:39 2024 ] 
Training: Epoch [23/120], Step [7499], Loss: 0.4900476038455963, Training Accuracy: 82.855
[ Wed Jul  3 06:56:40 2024 ] 	Batch(7500/7879) done. Loss: 0.8845  lr:0.010000
[ Wed Jul  3 06:56:58 2024 ] 	Batch(7600/7879) done. Loss: 0.0820  lr:0.010000
[ Wed Jul  3 06:57:16 2024 ] 	Batch(7700/7879) done. Loss: 0.2729  lr:0.010000
[ Wed Jul  3 06:57:34 2024 ] 	Batch(7800/7879) done. Loss: 0.4140  lr:0.010000
[ Wed Jul  3 06:57:48 2024 ] 	Mean training loss: 0.5607.
[ Wed Jul  3 06:57:48 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 06:57:48 2024 ] Training epoch: 25
[ Wed Jul  3 06:57:49 2024 ] 	Batch(0/7879) done. Loss: 0.7666  lr:0.010000
[ Wed Jul  3 06:58:06 2024 ] 	Batch(100/7879) done. Loss: 0.8927  lr:0.010000
[ Wed Jul  3 06:58:24 2024 ] 	Batch(200/7879) done. Loss: 1.0853  lr:0.010000
[ Wed Jul  3 06:58:42 2024 ] 	Batch(300/7879) done. Loss: 0.1830  lr:0.010000
[ Wed Jul  3 06:59:00 2024 ] 	Batch(400/7879) done. Loss: 0.0644  lr:0.010000
[ Wed Jul  3 06:59:18 2024 ] 
Training: Epoch [24/120], Step [499], Loss: 0.08238071948289871, Training Accuracy: 83.55
[ Wed Jul  3 06:59:18 2024 ] 	Batch(500/7879) done. Loss: 0.2437  lr:0.010000
[ Wed Jul  3 06:59:36 2024 ] 	Batch(600/7879) done. Loss: 0.0569  lr:0.010000
[ Wed Jul  3 06:59:54 2024 ] 	Batch(700/7879) done. Loss: 0.4115  lr:0.010000
[ Wed Jul  3 07:00:12 2024 ] 	Batch(800/7879) done. Loss: 0.2556  lr:0.010000
[ Wed Jul  3 07:00:30 2024 ] 	Batch(900/7879) done. Loss: 0.3692  lr:0.010000
[ Wed Jul  3 07:00:47 2024 ] 
Training: Epoch [24/120], Step [999], Loss: 0.2523393929004669, Training Accuracy: 83.85000000000001
[ Wed Jul  3 07:00:47 2024 ] 	Batch(1000/7879) done. Loss: 0.5423  lr:0.010000
[ Wed Jul  3 07:01:05 2024 ] 	Batch(1100/7879) done. Loss: 0.3418  lr:0.010000
[ Wed Jul  3 07:01:23 2024 ] 	Batch(1200/7879) done. Loss: 0.0870  lr:0.010000
[ Wed Jul  3 07:01:42 2024 ] 	Batch(1300/7879) done. Loss: 0.3846  lr:0.010000
[ Wed Jul  3 07:02:00 2024 ] 	Batch(1400/7879) done. Loss: 1.4512  lr:0.010000
[ Wed Jul  3 07:02:18 2024 ] 
Training: Epoch [24/120], Step [1499], Loss: 0.4881078898906708, Training Accuracy: 83.99166666666666
[ Wed Jul  3 07:02:19 2024 ] 	Batch(1500/7879) done. Loss: 0.6448  lr:0.010000
[ Wed Jul  3 07:02:37 2024 ] 	Batch(1600/7879) done. Loss: 0.5108  lr:0.010000
[ Wed Jul  3 07:02:54 2024 ] 	Batch(1700/7879) done. Loss: 0.2273  lr:0.010000
[ Wed Jul  3 07:03:12 2024 ] 	Batch(1800/7879) done. Loss: 0.9402  lr:0.010000
[ Wed Jul  3 07:03:30 2024 ] 	Batch(1900/7879) done. Loss: 0.6206  lr:0.010000
[ Wed Jul  3 07:03:48 2024 ] 
Training: Epoch [24/120], Step [1999], Loss: 0.46812596917152405, Training Accuracy: 84.1625
[ Wed Jul  3 07:03:48 2024 ] 	Batch(2000/7879) done. Loss: 0.3193  lr:0.010000
[ Wed Jul  3 07:04:06 2024 ] 	Batch(2100/7879) done. Loss: 0.3077  lr:0.010000
[ Wed Jul  3 07:04:24 2024 ] 	Batch(2200/7879) done. Loss: 0.6620  lr:0.010000
[ Wed Jul  3 07:04:42 2024 ] 	Batch(2300/7879) done. Loss: 0.5265  lr:0.010000
[ Wed Jul  3 07:05:00 2024 ] 	Batch(2400/7879) done. Loss: 0.5592  lr:0.010000
[ Wed Jul  3 07:05:18 2024 ] 
Training: Epoch [24/120], Step [2499], Loss: 0.6881526112556458, Training Accuracy: 84.08
[ Wed Jul  3 07:05:18 2024 ] 	Batch(2500/7879) done. Loss: 1.2635  lr:0.010000
[ Wed Jul  3 07:05:36 2024 ] 	Batch(2600/7879) done. Loss: 0.9398  lr:0.010000
[ Wed Jul  3 07:05:54 2024 ] 	Batch(2700/7879) done. Loss: 1.0327  lr:0.010000
[ Wed Jul  3 07:06:12 2024 ] 	Batch(2800/7879) done. Loss: 0.7172  lr:0.010000
[ Wed Jul  3 07:06:30 2024 ] 	Batch(2900/7879) done. Loss: 0.0697  lr:0.010000
[ Wed Jul  3 07:06:49 2024 ] 
Training: Epoch [24/120], Step [2999], Loss: 0.7704848647117615, Training Accuracy: 83.9875
[ Wed Jul  3 07:06:49 2024 ] 	Batch(3000/7879) done. Loss: 0.2219  lr:0.010000
[ Wed Jul  3 07:07:07 2024 ] 	Batch(3100/7879) done. Loss: 0.5332  lr:0.010000
[ Wed Jul  3 07:07:26 2024 ] 	Batch(3200/7879) done. Loss: 0.2376  lr:0.010000
[ Wed Jul  3 07:07:44 2024 ] 	Batch(3300/7879) done. Loss: 0.7316  lr:0.010000
[ Wed Jul  3 07:08:03 2024 ] 	Batch(3400/7879) done. Loss: 0.6714  lr:0.010000
[ Wed Jul  3 07:08:21 2024 ] 
Training: Epoch [24/120], Step [3499], Loss: 1.707311749458313, Training Accuracy: 84.09642857142858
[ Wed Jul  3 07:08:21 2024 ] 	Batch(3500/7879) done. Loss: 0.2856  lr:0.010000
[ Wed Jul  3 07:08:40 2024 ] 	Batch(3600/7879) done. Loss: 0.9192  lr:0.010000
[ Wed Jul  3 07:08:59 2024 ] 	Batch(3700/7879) done. Loss: 0.3534  lr:0.010000
[ Wed Jul  3 07:09:17 2024 ] 	Batch(3800/7879) done. Loss: 0.9845  lr:0.010000
[ Wed Jul  3 07:09:36 2024 ] 	Batch(3900/7879) done. Loss: 0.7600  lr:0.010000
[ Wed Jul  3 07:09:54 2024 ] 
Training: Epoch [24/120], Step [3999], Loss: 0.03832545503973961, Training Accuracy: 83.921875
[ Wed Jul  3 07:09:54 2024 ] 	Batch(4000/7879) done. Loss: 0.6164  lr:0.010000
[ Wed Jul  3 07:10:12 2024 ] 	Batch(4100/7879) done. Loss: 0.6442  lr:0.010000
[ Wed Jul  3 07:10:31 2024 ] 	Batch(4200/7879) done. Loss: 0.0605  lr:0.010000
[ Wed Jul  3 07:10:49 2024 ] 	Batch(4300/7879) done. Loss: 0.0636  lr:0.010000
[ Wed Jul  3 07:11:08 2024 ] 	Batch(4400/7879) done. Loss: 0.5950  lr:0.010000
[ Wed Jul  3 07:11:26 2024 ] 
Training: Epoch [24/120], Step [4499], Loss: 0.7505983114242554, Training Accuracy: 83.83333333333334
[ Wed Jul  3 07:11:26 2024 ] 	Batch(4500/7879) done. Loss: 0.3197  lr:0.010000
[ Wed Jul  3 07:11:44 2024 ] 	Batch(4600/7879) done. Loss: 0.0372  lr:0.010000
[ Wed Jul  3 07:12:02 2024 ] 	Batch(4700/7879) done. Loss: 0.6674  lr:0.010000
[ Wed Jul  3 07:12:20 2024 ] 	Batch(4800/7879) done. Loss: 0.0415  lr:0.010000
[ Wed Jul  3 07:12:38 2024 ] 	Batch(4900/7879) done. Loss: 0.9415  lr:0.010000
[ Wed Jul  3 07:12:55 2024 ] 
Training: Epoch [24/120], Step [4999], Loss: 0.5166102647781372, Training Accuracy: 83.7175
[ Wed Jul  3 07:12:55 2024 ] 	Batch(5000/7879) done. Loss: 0.8695  lr:0.010000
[ Wed Jul  3 07:13:13 2024 ] 	Batch(5100/7879) done. Loss: 1.4136  lr:0.010000
[ Wed Jul  3 07:13:31 2024 ] 	Batch(5200/7879) done. Loss: 0.6155  lr:0.010000
[ Wed Jul  3 07:13:49 2024 ] 	Batch(5300/7879) done. Loss: 0.7233  lr:0.010000
[ Wed Jul  3 07:14:07 2024 ] 	Batch(5400/7879) done. Loss: 0.2322  lr:0.010000
[ Wed Jul  3 07:14:25 2024 ] 
Training: Epoch [24/120], Step [5499], Loss: 0.26556336879730225, Training Accuracy: 83.55681818181819
[ Wed Jul  3 07:14:25 2024 ] 	Batch(5500/7879) done. Loss: 0.2501  lr:0.010000
[ Wed Jul  3 07:14:43 2024 ] 	Batch(5600/7879) done. Loss: 1.4175  lr:0.010000
[ Wed Jul  3 07:15:02 2024 ] 	Batch(5700/7879) done. Loss: 0.2381  lr:0.010000
[ Wed Jul  3 07:15:20 2024 ] 	Batch(5800/7879) done. Loss: 0.1845  lr:0.010000
[ Wed Jul  3 07:15:39 2024 ] 	Batch(5900/7879) done. Loss: 1.0587  lr:0.010000
[ Wed Jul  3 07:15:57 2024 ] 
Training: Epoch [24/120], Step [5999], Loss: 1.107309103012085, Training Accuracy: 83.55833333333334
[ Wed Jul  3 07:15:57 2024 ] 	Batch(6000/7879) done. Loss: 0.7070  lr:0.010000
[ Wed Jul  3 07:16:15 2024 ] 	Batch(6100/7879) done. Loss: 0.0378  lr:0.010000
[ Wed Jul  3 07:16:33 2024 ] 	Batch(6200/7879) done. Loss: 0.1323  lr:0.010000
[ Wed Jul  3 07:16:51 2024 ] 	Batch(6300/7879) done. Loss: 0.5642  lr:0.010000
[ Wed Jul  3 07:17:09 2024 ] 	Batch(6400/7879) done. Loss: 0.1809  lr:0.010000
[ Wed Jul  3 07:17:26 2024 ] 
Training: Epoch [24/120], Step [6499], Loss: 0.3515043258666992, Training Accuracy: 83.52884615384616
[ Wed Jul  3 07:17:27 2024 ] 	Batch(6500/7879) done. Loss: 1.2040  lr:0.010000
[ Wed Jul  3 07:17:45 2024 ] 	Batch(6600/7879) done. Loss: 0.2858  lr:0.010000
[ Wed Jul  3 07:18:02 2024 ] 	Batch(6700/7879) done. Loss: 0.6842  lr:0.010000
[ Wed Jul  3 07:18:20 2024 ] 	Batch(6800/7879) done. Loss: 1.2217  lr:0.010000
[ Wed Jul  3 07:18:38 2024 ] 	Batch(6900/7879) done. Loss: 0.1143  lr:0.010000
[ Wed Jul  3 07:18:56 2024 ] 
Training: Epoch [24/120], Step [6999], Loss: 1.211473822593689, Training Accuracy: 83.46249999999999
[ Wed Jul  3 07:18:56 2024 ] 	Batch(7000/7879) done. Loss: 0.3743  lr:0.010000
[ Wed Jul  3 07:19:14 2024 ] 	Batch(7100/7879) done. Loss: 1.1830  lr:0.010000
[ Wed Jul  3 07:19:32 2024 ] 	Batch(7200/7879) done. Loss: 0.3776  lr:0.010000
[ Wed Jul  3 07:19:51 2024 ] 	Batch(7300/7879) done. Loss: 1.1063  lr:0.010000
[ Wed Jul  3 07:20:09 2024 ] 	Batch(7400/7879) done. Loss: 0.9759  lr:0.010000
[ Wed Jul  3 07:20:27 2024 ] 
Training: Epoch [24/120], Step [7499], Loss: 0.32567185163497925, Training Accuracy: 83.38499999999999
[ Wed Jul  3 07:20:28 2024 ] 	Batch(7500/7879) done. Loss: 0.3072  lr:0.010000
[ Wed Jul  3 07:20:46 2024 ] 	Batch(7600/7879) done. Loss: 0.2132  lr:0.010000
[ Wed Jul  3 07:21:04 2024 ] 	Batch(7700/7879) done. Loss: 0.1527  lr:0.010000
[ Wed Jul  3 07:21:22 2024 ] 	Batch(7800/7879) done. Loss: 0.6958  lr:0.010000
[ Wed Jul  3 07:21:36 2024 ] 	Mean training loss: 0.5462.
[ Wed Jul  3 07:21:36 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 07:21:36 2024 ] Training epoch: 26
[ Wed Jul  3 07:21:37 2024 ] 	Batch(0/7879) done. Loss: 0.3051  lr:0.010000
[ Wed Jul  3 07:21:55 2024 ] 	Batch(100/7879) done. Loss: 0.1473  lr:0.010000
[ Wed Jul  3 07:22:12 2024 ] 	Batch(200/7879) done. Loss: 0.2022  lr:0.010000
[ Wed Jul  3 07:22:30 2024 ] 	Batch(300/7879) done. Loss: 0.8344  lr:0.010000
[ Wed Jul  3 07:22:48 2024 ] 	Batch(400/7879) done. Loss: 0.6637  lr:0.010000
[ Wed Jul  3 07:23:06 2024 ] 
Training: Epoch [25/120], Step [499], Loss: 0.5690182447433472, Training Accuracy: 83.85000000000001
[ Wed Jul  3 07:23:06 2024 ] 	Batch(500/7879) done. Loss: 0.0188  lr:0.010000
[ Wed Jul  3 07:23:24 2024 ] 	Batch(600/7879) done. Loss: 0.5094  lr:0.010000
[ Wed Jul  3 07:23:42 2024 ] 	Batch(700/7879) done. Loss: 0.3354  lr:0.010000
[ Wed Jul  3 07:24:00 2024 ] 	Batch(800/7879) done. Loss: 0.2106  lr:0.010000
[ Wed Jul  3 07:24:18 2024 ] 	Batch(900/7879) done. Loss: 0.1060  lr:0.010000
[ Wed Jul  3 07:24:36 2024 ] 
Training: Epoch [25/120], Step [999], Loss: 0.44804254174232483, Training Accuracy: 83.6875
[ Wed Jul  3 07:24:37 2024 ] 	Batch(1000/7879) done. Loss: 0.3478  lr:0.010000
[ Wed Jul  3 07:24:54 2024 ] 	Batch(1100/7879) done. Loss: 0.8924  lr:0.010000
[ Wed Jul  3 07:25:12 2024 ] 	Batch(1200/7879) done. Loss: 0.2298  lr:0.010000
[ Wed Jul  3 07:25:30 2024 ] 	Batch(1300/7879) done. Loss: 0.2916  lr:0.010000
[ Wed Jul  3 07:25:48 2024 ] 	Batch(1400/7879) done. Loss: 1.4800  lr:0.010000
[ Wed Jul  3 07:26:06 2024 ] 
Training: Epoch [25/120], Step [1499], Loss: 1.050171971321106, Training Accuracy: 83.90833333333333
[ Wed Jul  3 07:26:06 2024 ] 	Batch(1500/7879) done. Loss: 1.3650  lr:0.010000
[ Wed Jul  3 07:26:24 2024 ] 	Batch(1600/7879) done. Loss: 1.0937  lr:0.010000
[ Wed Jul  3 07:26:43 2024 ] 	Batch(1700/7879) done. Loss: 0.5924  lr:0.010000
[ Wed Jul  3 07:27:01 2024 ] 	Batch(1800/7879) done. Loss: 0.4587  lr:0.010000
[ Wed Jul  3 07:27:19 2024 ] 	Batch(1900/7879) done. Loss: 0.0979  lr:0.010000
[ Wed Jul  3 07:27:37 2024 ] 
Training: Epoch [25/120], Step [1999], Loss: 0.8570403456687927, Training Accuracy: 83.95
[ Wed Jul  3 07:27:37 2024 ] 	Batch(2000/7879) done. Loss: 0.1673  lr:0.010000
[ Wed Jul  3 07:27:56 2024 ] 	Batch(2100/7879) done. Loss: 0.2650  lr:0.010000
[ Wed Jul  3 07:28:14 2024 ] 	Batch(2200/7879) done. Loss: 0.1700  lr:0.010000
[ Wed Jul  3 07:28:33 2024 ] 	Batch(2300/7879) done. Loss: 0.8534  lr:0.010000
[ Wed Jul  3 07:28:51 2024 ] 	Batch(2400/7879) done. Loss: 0.1222  lr:0.010000
[ Wed Jul  3 07:29:09 2024 ] 
Training: Epoch [25/120], Step [2499], Loss: 0.5309298634529114, Training Accuracy: 83.985
[ Wed Jul  3 07:29:09 2024 ] 	Batch(2500/7879) done. Loss: 0.2022  lr:0.010000
[ Wed Jul  3 07:29:27 2024 ] 	Batch(2600/7879) done. Loss: 1.5528  lr:0.010000
[ Wed Jul  3 07:29:45 2024 ] 	Batch(2700/7879) done. Loss: 0.7051  lr:0.010000
[ Wed Jul  3 07:30:03 2024 ] 	Batch(2800/7879) done. Loss: 0.4441  lr:0.010000
[ Wed Jul  3 07:30:21 2024 ] 	Batch(2900/7879) done. Loss: 1.0951  lr:0.010000
[ Wed Jul  3 07:30:39 2024 ] 
Training: Epoch [25/120], Step [2999], Loss: 0.28364235162734985, Training Accuracy: 83.97916666666667
[ Wed Jul  3 07:30:39 2024 ] 	Batch(3000/7879) done. Loss: 0.9731  lr:0.010000
[ Wed Jul  3 07:30:57 2024 ] 	Batch(3100/7879) done. Loss: 0.4084  lr:0.010000
[ Wed Jul  3 07:31:15 2024 ] 	Batch(3200/7879) done. Loss: 0.8223  lr:0.010000
[ Wed Jul  3 07:31:33 2024 ] 	Batch(3300/7879) done. Loss: 1.6736  lr:0.010000
[ Wed Jul  3 07:31:52 2024 ] 	Batch(3400/7879) done. Loss: 0.4909  lr:0.010000
[ Wed Jul  3 07:32:10 2024 ] 
Training: Epoch [25/120], Step [3499], Loss: 0.06419964134693146, Training Accuracy: 83.93214285714285
[ Wed Jul  3 07:32:10 2024 ] 	Batch(3500/7879) done. Loss: 0.8529  lr:0.010000
[ Wed Jul  3 07:32:29 2024 ] 	Batch(3600/7879) done. Loss: 0.0633  lr:0.010000
[ Wed Jul  3 07:32:47 2024 ] 	Batch(3700/7879) done. Loss: 0.4397  lr:0.010000
[ Wed Jul  3 07:33:05 2024 ] 	Batch(3800/7879) done. Loss: 0.3806  lr:0.010000
[ Wed Jul  3 07:33:23 2024 ] 	Batch(3900/7879) done. Loss: 0.2339  lr:0.010000
[ Wed Jul  3 07:33:41 2024 ] 
Training: Epoch [25/120], Step [3999], Loss: 0.36620017886161804, Training Accuracy: 83.909375
[ Wed Jul  3 07:33:41 2024 ] 	Batch(4000/7879) done. Loss: 0.9577  lr:0.010000
[ Wed Jul  3 07:33:59 2024 ] 	Batch(4100/7879) done. Loss: 0.3429  lr:0.010000
[ Wed Jul  3 07:34:17 2024 ] 	Batch(4200/7879) done. Loss: 0.8289  lr:0.010000
[ Wed Jul  3 07:34:35 2024 ] 	Batch(4300/7879) done. Loss: 0.3775  lr:0.010000
[ Wed Jul  3 07:34:53 2024 ] 	Batch(4400/7879) done. Loss: 0.7687  lr:0.010000
[ Wed Jul  3 07:35:11 2024 ] 
Training: Epoch [25/120], Step [4499], Loss: 1.1784213781356812, Training Accuracy: 83.76666666666667
[ Wed Jul  3 07:35:11 2024 ] 	Batch(4500/7879) done. Loss: 0.0993  lr:0.010000
[ Wed Jul  3 07:35:30 2024 ] 	Batch(4600/7879) done. Loss: 0.5446  lr:0.010000
[ Wed Jul  3 07:35:48 2024 ] 	Batch(4700/7879) done. Loss: 0.7353  lr:0.010000
[ Wed Jul  3 07:36:07 2024 ] 	Batch(4800/7879) done. Loss: 0.6978  lr:0.010000
[ Wed Jul  3 07:36:25 2024 ] 	Batch(4900/7879) done. Loss: 0.5395  lr:0.010000
[ Wed Jul  3 07:36:43 2024 ] 
Training: Epoch [25/120], Step [4999], Loss: 1.6270228624343872, Training Accuracy: 83.7125
[ Wed Jul  3 07:36:43 2024 ] 	Batch(5000/7879) done. Loss: 0.5498  lr:0.010000
[ Wed Jul  3 07:37:01 2024 ] 	Batch(5100/7879) done. Loss: 0.3435  lr:0.010000
[ Wed Jul  3 07:37:19 2024 ] 	Batch(5200/7879) done. Loss: 0.4168  lr:0.010000
[ Wed Jul  3 07:37:37 2024 ] 	Batch(5300/7879) done. Loss: 0.3635  lr:0.010000
[ Wed Jul  3 07:37:55 2024 ] 	Batch(5400/7879) done. Loss: 0.1007  lr:0.010000
[ Wed Jul  3 07:38:12 2024 ] 
Training: Epoch [25/120], Step [5499], Loss: 0.12602251768112183, Training Accuracy: 83.73863636363636
[ Wed Jul  3 07:38:12 2024 ] 	Batch(5500/7879) done. Loss: 1.6044  lr:0.010000
[ Wed Jul  3 07:38:30 2024 ] 	Batch(5600/7879) done. Loss: 0.5162  lr:0.010000
[ Wed Jul  3 07:38:49 2024 ] 	Batch(5700/7879) done. Loss: 0.7663  lr:0.010000
[ Wed Jul  3 07:39:08 2024 ] 	Batch(5800/7879) done. Loss: 0.5835  lr:0.010000
[ Wed Jul  3 07:39:26 2024 ] 	Batch(5900/7879) done. Loss: 0.8205  lr:0.010000
[ Wed Jul  3 07:39:44 2024 ] 
Training: Epoch [25/120], Step [5999], Loss: 0.5542706847190857, Training Accuracy: 83.7
[ Wed Jul  3 07:39:45 2024 ] 	Batch(6000/7879) done. Loss: 0.3672  lr:0.010000
[ Wed Jul  3 07:40:03 2024 ] 	Batch(6100/7879) done. Loss: 0.8144  lr:0.010000
[ Wed Jul  3 07:40:22 2024 ] 	Batch(6200/7879) done. Loss: 0.2582  lr:0.010000
[ Wed Jul  3 07:40:40 2024 ] 	Batch(6300/7879) done. Loss: 0.1334  lr:0.010000
[ Wed Jul  3 07:40:59 2024 ] 	Batch(6400/7879) done. Loss: 0.5217  lr:0.010000
[ Wed Jul  3 07:41:17 2024 ] 
Training: Epoch [25/120], Step [6499], Loss: 1.3888598680496216, Training Accuracy: 83.675
[ Wed Jul  3 07:41:17 2024 ] 	Batch(6500/7879) done. Loss: 0.4002  lr:0.010000
[ Wed Jul  3 07:41:36 2024 ] 	Batch(6600/7879) done. Loss: 0.0364  lr:0.010000
[ Wed Jul  3 07:41:55 2024 ] 	Batch(6700/7879) done. Loss: 0.2810  lr:0.010000
[ Wed Jul  3 07:42:13 2024 ] 	Batch(6800/7879) done. Loss: 0.7494  lr:0.010000
[ Wed Jul  3 07:42:31 2024 ] 	Batch(6900/7879) done. Loss: 0.2435  lr:0.010000
[ Wed Jul  3 07:42:49 2024 ] 
Training: Epoch [25/120], Step [6999], Loss: 0.8425420522689819, Training Accuracy: 83.63571428571429
[ Wed Jul  3 07:42:49 2024 ] 	Batch(7000/7879) done. Loss: 0.9098  lr:0.010000
[ Wed Jul  3 07:43:07 2024 ] 	Batch(7100/7879) done. Loss: 0.2536  lr:0.010000
[ Wed Jul  3 07:43:25 2024 ] 	Batch(7200/7879) done. Loss: 0.1661  lr:0.010000
[ Wed Jul  3 07:43:43 2024 ] 	Batch(7300/7879) done. Loss: 0.6255  lr:0.010000
[ Wed Jul  3 07:44:01 2024 ] 	Batch(7400/7879) done. Loss: 0.7435  lr:0.010000
[ Wed Jul  3 07:44:18 2024 ] 
Training: Epoch [25/120], Step [7499], Loss: 0.38669270277023315, Training Accuracy: 83.66333333333333
[ Wed Jul  3 07:44:18 2024 ] 	Batch(7500/7879) done. Loss: 0.3997  lr:0.010000
[ Wed Jul  3 07:44:36 2024 ] 	Batch(7600/7879) done. Loss: 0.1819  lr:0.010000
[ Wed Jul  3 07:44:54 2024 ] 	Batch(7700/7879) done. Loss: 0.4793  lr:0.010000
[ Wed Jul  3 07:45:12 2024 ] 	Batch(7800/7879) done. Loss: 0.1446  lr:0.010000
[ Wed Jul  3 07:45:26 2024 ] 	Mean training loss: 0.5305.
[ Wed Jul  3 07:45:26 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 07:45:26 2024 ] Training epoch: 27
[ Wed Jul  3 07:45:27 2024 ] 	Batch(0/7879) done. Loss: 0.0707  lr:0.010000
[ Wed Jul  3 07:45:45 2024 ] 	Batch(100/7879) done. Loss: 0.5268  lr:0.010000
[ Wed Jul  3 07:46:04 2024 ] 	Batch(200/7879) done. Loss: 0.3087  lr:0.010000
[ Wed Jul  3 07:46:22 2024 ] 	Batch(300/7879) done. Loss: 0.3300  lr:0.010000
[ Wed Jul  3 07:46:41 2024 ] 	Batch(400/7879) done. Loss: 0.2809  lr:0.010000
[ Wed Jul  3 07:46:59 2024 ] 
Training: Epoch [26/120], Step [499], Loss: 1.5923510789871216, Training Accuracy: 85.5
[ Wed Jul  3 07:46:59 2024 ] 	Batch(500/7879) done. Loss: 0.1947  lr:0.010000
[ Wed Jul  3 07:47:17 2024 ] 	Batch(600/7879) done. Loss: 0.3492  lr:0.010000
[ Wed Jul  3 07:47:35 2024 ] 	Batch(700/7879) done. Loss: 0.9811  lr:0.010000
[ Wed Jul  3 07:47:53 2024 ] 	Batch(800/7879) done. Loss: 0.0606  lr:0.010000
[ Wed Jul  3 07:48:11 2024 ] 	Batch(900/7879) done. Loss: 0.2374  lr:0.010000
[ Wed Jul  3 07:48:28 2024 ] 
Training: Epoch [26/120], Step [999], Loss: 0.4065471589565277, Training Accuracy: 85.425
[ Wed Jul  3 07:48:29 2024 ] 	Batch(1000/7879) done. Loss: 0.3794  lr:0.010000
[ Wed Jul  3 07:48:46 2024 ] 	Batch(1100/7879) done. Loss: 0.6590  lr:0.010000
[ Wed Jul  3 07:49:04 2024 ] 	Batch(1200/7879) done. Loss: 0.3754  lr:0.010000
[ Wed Jul  3 07:49:22 2024 ] 	Batch(1300/7879) done. Loss: 0.3808  lr:0.010000
[ Wed Jul  3 07:49:40 2024 ] 	Batch(1400/7879) done. Loss: 0.5606  lr:0.010000
[ Wed Jul  3 07:49:58 2024 ] 
Training: Epoch [26/120], Step [1499], Loss: 0.39637628197669983, Training Accuracy: 85.26666666666667
[ Wed Jul  3 07:49:58 2024 ] 	Batch(1500/7879) done. Loss: 0.3041  lr:0.010000
[ Wed Jul  3 07:50:16 2024 ] 	Batch(1600/7879) done. Loss: 0.2600  lr:0.010000
[ Wed Jul  3 07:50:34 2024 ] 	Batch(1700/7879) done. Loss: 0.8580  lr:0.010000
[ Wed Jul  3 07:50:52 2024 ] 	Batch(1800/7879) done. Loss: 0.6902  lr:0.010000
[ Wed Jul  3 07:51:10 2024 ] 	Batch(1900/7879) done. Loss: 0.0483  lr:0.010000
[ Wed Jul  3 07:51:28 2024 ] 
Training: Epoch [26/120], Step [1999], Loss: 0.3246753513813019, Training Accuracy: 85.0125
[ Wed Jul  3 07:51:28 2024 ] 	Batch(2000/7879) done. Loss: 0.1665  lr:0.010000
[ Wed Jul  3 07:51:46 2024 ] 	Batch(2100/7879) done. Loss: 0.0629  lr:0.010000
[ Wed Jul  3 07:52:04 2024 ] 	Batch(2200/7879) done. Loss: 0.1522  lr:0.010000
[ Wed Jul  3 07:52:22 2024 ] 	Batch(2300/7879) done. Loss: 1.3720  lr:0.010000
[ Wed Jul  3 07:52:40 2024 ] 	Batch(2400/7879) done. Loss: 0.9180  lr:0.010000
[ Wed Jul  3 07:52:58 2024 ] 
Training: Epoch [26/120], Step [2499], Loss: 0.2843296527862549, Training Accuracy: 84.905
[ Wed Jul  3 07:52:58 2024 ] 	Batch(2500/7879) done. Loss: 0.8779  lr:0.010000
[ Wed Jul  3 07:53:16 2024 ] 	Batch(2600/7879) done. Loss: 0.9806  lr:0.010000
[ Wed Jul  3 07:53:34 2024 ] 	Batch(2700/7879) done. Loss: 0.1999  lr:0.010000
[ Wed Jul  3 07:53:52 2024 ] 	Batch(2800/7879) done. Loss: 0.7767  lr:0.010000
[ Wed Jul  3 07:54:10 2024 ] 	Batch(2900/7879) done. Loss: 0.0747  lr:0.010000
[ Wed Jul  3 07:54:28 2024 ] 
Training: Epoch [26/120], Step [2999], Loss: 0.04134912043809891, Training Accuracy: 84.81666666666666
[ Wed Jul  3 07:54:29 2024 ] 	Batch(3000/7879) done. Loss: 0.8049  lr:0.010000
[ Wed Jul  3 07:54:47 2024 ] 	Batch(3100/7879) done. Loss: 0.7436  lr:0.010000
[ Wed Jul  3 07:55:06 2024 ] 	Batch(3200/7879) done. Loss: 0.7259  lr:0.010000
[ Wed Jul  3 07:55:24 2024 ] 	Batch(3300/7879) done. Loss: 1.2908  lr:0.010000
[ Wed Jul  3 07:55:42 2024 ] 	Batch(3400/7879) done. Loss: 0.4688  lr:0.010000
[ Wed Jul  3 07:55:59 2024 ] 
Training: Epoch [26/120], Step [3499], Loss: 0.11283855140209198, Training Accuracy: 84.73571428571428
[ Wed Jul  3 07:55:59 2024 ] 	Batch(3500/7879) done. Loss: 0.1669  lr:0.010000
[ Wed Jul  3 07:56:17 2024 ] 	Batch(3600/7879) done. Loss: 0.1246  lr:0.010000
[ Wed Jul  3 07:56:35 2024 ] 	Batch(3700/7879) done. Loss: 0.3048  lr:0.010000
[ Wed Jul  3 07:56:53 2024 ] 	Batch(3800/7879) done. Loss: 1.0130  lr:0.010000
[ Wed Jul  3 07:57:11 2024 ] 	Batch(3900/7879) done. Loss: 1.3392  lr:0.010000
[ Wed Jul  3 07:57:29 2024 ] 
Training: Epoch [26/120], Step [3999], Loss: 0.6186701059341431, Training Accuracy: 84.67812500000001
[ Wed Jul  3 07:57:29 2024 ] 	Batch(4000/7879) done. Loss: 0.5847  lr:0.010000
[ Wed Jul  3 07:57:47 2024 ] 	Batch(4100/7879) done. Loss: 0.5760  lr:0.010000
[ Wed Jul  3 07:58:05 2024 ] 	Batch(4200/7879) done. Loss: 0.0627  lr:0.010000
[ Wed Jul  3 07:58:23 2024 ] 	Batch(4300/7879) done. Loss: 0.4570  lr:0.010000
[ Wed Jul  3 07:58:41 2024 ] 	Batch(4400/7879) done. Loss: 0.5665  lr:0.010000
[ Wed Jul  3 07:58:59 2024 ] 
Training: Epoch [26/120], Step [4499], Loss: 0.21254298090934753, Training Accuracy: 84.48333333333333
[ Wed Jul  3 07:58:59 2024 ] 	Batch(4500/7879) done. Loss: 0.1526  lr:0.010000
[ Wed Jul  3 07:59:17 2024 ] 	Batch(4600/7879) done. Loss: 0.0562  lr:0.010000
[ Wed Jul  3 07:59:35 2024 ] 	Batch(4700/7879) done. Loss: 0.9514  lr:0.010000
[ Wed Jul  3 07:59:53 2024 ] 	Batch(4800/7879) done. Loss: 0.1107  lr:0.010000
[ Wed Jul  3 08:00:11 2024 ] 	Batch(4900/7879) done. Loss: 0.6423  lr:0.010000
[ Wed Jul  3 08:00:29 2024 ] 
Training: Epoch [26/120], Step [4999], Loss: 0.3126804828643799, Training Accuracy: 84.3925
[ Wed Jul  3 08:00:30 2024 ] 	Batch(5000/7879) done. Loss: 0.1477  lr:0.010000
[ Wed Jul  3 08:00:48 2024 ] 	Batch(5100/7879) done. Loss: 0.3237  lr:0.010000
[ Wed Jul  3 08:01:06 2024 ] 	Batch(5200/7879) done. Loss: 0.9572  lr:0.010000
[ Wed Jul  3 08:01:24 2024 ] 	Batch(5300/7879) done. Loss: 0.5846  lr:0.010000
[ Wed Jul  3 08:01:42 2024 ] 	Batch(5400/7879) done. Loss: 0.0160  lr:0.010000
[ Wed Jul  3 08:02:00 2024 ] 
Training: Epoch [26/120], Step [5499], Loss: 0.1519717127084732, Training Accuracy: 84.35454545454546
[ Wed Jul  3 08:02:00 2024 ] 	Batch(5500/7879) done. Loss: 0.6136  lr:0.010000
[ Wed Jul  3 08:02:18 2024 ] 	Batch(5600/7879) done. Loss: 1.2341  lr:0.010000
[ Wed Jul  3 08:02:36 2024 ] 	Batch(5700/7879) done. Loss: 0.7591  lr:0.010000
[ Wed Jul  3 08:02:54 2024 ] 	Batch(5800/7879) done. Loss: 0.7245  lr:0.010000
[ Wed Jul  3 08:03:12 2024 ] 	Batch(5900/7879) done. Loss: 0.2593  lr:0.010000
[ Wed Jul  3 08:03:30 2024 ] 
Training: Epoch [26/120], Step [5999], Loss: 0.7712333798408508, Training Accuracy: 84.30625
[ Wed Jul  3 08:03:30 2024 ] 	Batch(6000/7879) done. Loss: 0.5655  lr:0.010000
[ Wed Jul  3 08:03:49 2024 ] 	Batch(6100/7879) done. Loss: 0.0222  lr:0.010000
[ Wed Jul  3 08:04:07 2024 ] 	Batch(6200/7879) done. Loss: 1.3168  lr:0.010000
[ Wed Jul  3 08:04:26 2024 ] 	Batch(6300/7879) done. Loss: 0.5479  lr:0.010000
[ Wed Jul  3 08:04:44 2024 ] 	Batch(6400/7879) done. Loss: 0.8346  lr:0.010000
[ Wed Jul  3 08:05:02 2024 ] 
Training: Epoch [26/120], Step [6499], Loss: 0.06924698501825333, Training Accuracy: 84.13461538461539
[ Wed Jul  3 08:05:02 2024 ] 	Batch(6500/7879) done. Loss: 0.7688  lr:0.010000
[ Wed Jul  3 08:05:20 2024 ] 	Batch(6600/7879) done. Loss: 0.7488  lr:0.010000
[ Wed Jul  3 08:05:38 2024 ] 	Batch(6700/7879) done. Loss: 0.5920  lr:0.010000
[ Wed Jul  3 08:05:56 2024 ] 	Batch(6800/7879) done. Loss: 0.5504  lr:0.010000
[ Wed Jul  3 08:06:14 2024 ] 	Batch(6900/7879) done. Loss: 0.1066  lr:0.010000
[ Wed Jul  3 08:06:32 2024 ] 
Training: Epoch [26/120], Step [6999], Loss: 1.1396303176879883, Training Accuracy: 84.10892857142858
[ Wed Jul  3 08:06:32 2024 ] 	Batch(7000/7879) done. Loss: 0.4387  lr:0.010000
[ Wed Jul  3 08:06:50 2024 ] 	Batch(7100/7879) done. Loss: 0.4980  lr:0.010000
[ Wed Jul  3 08:07:08 2024 ] 	Batch(7200/7879) done. Loss: 0.1936  lr:0.010000
[ Wed Jul  3 08:07:25 2024 ] 	Batch(7300/7879) done. Loss: 0.6316  lr:0.010000
[ Wed Jul  3 08:07:43 2024 ] 	Batch(7400/7879) done. Loss: 1.0399  lr:0.010000
[ Wed Jul  3 08:08:01 2024 ] 
Training: Epoch [26/120], Step [7499], Loss: 0.031015822663903236, Training Accuracy: 84.125
[ Wed Jul  3 08:08:01 2024 ] 	Batch(7500/7879) done. Loss: 0.6632  lr:0.010000
[ Wed Jul  3 08:08:19 2024 ] 	Batch(7600/7879) done. Loss: 0.4863  lr:0.010000
[ Wed Jul  3 08:08:37 2024 ] 	Batch(7700/7879) done. Loss: 0.1195  lr:0.010000
[ Wed Jul  3 08:08:55 2024 ] 	Batch(7800/7879) done. Loss: 1.2425  lr:0.010000
[ Wed Jul  3 08:09:09 2024 ] 	Mean training loss: 0.5150.
[ Wed Jul  3 08:09:09 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 08:09:09 2024 ] Training epoch: 28
[ Wed Jul  3 08:09:10 2024 ] 	Batch(0/7879) done. Loss: 0.0233  lr:0.010000
[ Wed Jul  3 08:09:28 2024 ] 	Batch(100/7879) done. Loss: 0.7059  lr:0.010000
[ Wed Jul  3 08:09:45 2024 ] 	Batch(200/7879) done. Loss: 0.4520  lr:0.010000
[ Wed Jul  3 08:10:03 2024 ] 	Batch(300/7879) done. Loss: 0.7669  lr:0.010000
[ Wed Jul  3 08:10:21 2024 ] 	Batch(400/7879) done. Loss: 0.4798  lr:0.010000
[ Wed Jul  3 08:10:39 2024 ] 
Training: Epoch [27/120], Step [499], Loss: 0.2779274582862854, Training Accuracy: 86.075
[ Wed Jul  3 08:10:39 2024 ] 	Batch(500/7879) done. Loss: 0.1190  lr:0.010000
[ Wed Jul  3 08:10:57 2024 ] 	Batch(600/7879) done. Loss: 0.9622  lr:0.010000
[ Wed Jul  3 08:11:15 2024 ] 	Batch(700/7879) done. Loss: 0.2544  lr:0.010000
[ Wed Jul  3 08:11:33 2024 ] 	Batch(800/7879) done. Loss: 0.1147  lr:0.010000
[ Wed Jul  3 08:11:52 2024 ] 	Batch(900/7879) done. Loss: 0.2695  lr:0.010000
[ Wed Jul  3 08:12:10 2024 ] 
Training: Epoch [27/120], Step [999], Loss: 0.08156787604093552, Training Accuracy: 85.05
[ Wed Jul  3 08:12:10 2024 ] 	Batch(1000/7879) done. Loss: 0.6686  lr:0.010000
[ Wed Jul  3 08:12:28 2024 ] 	Batch(1100/7879) done. Loss: 0.5765  lr:0.010000
[ Wed Jul  3 08:12:46 2024 ] 	Batch(1200/7879) done. Loss: 0.4701  lr:0.010000
[ Wed Jul  3 08:13:04 2024 ] 	Batch(1300/7879) done. Loss: 0.0607  lr:0.010000
[ Wed Jul  3 08:13:22 2024 ] 	Batch(1400/7879) done. Loss: 0.2745  lr:0.010000
[ Wed Jul  3 08:13:40 2024 ] 
Training: Epoch [27/120], Step [1499], Loss: 1.1228030920028687, Training Accuracy: 84.93333333333334
[ Wed Jul  3 08:13:40 2024 ] 	Batch(1500/7879) done. Loss: 0.1692  lr:0.010000
[ Wed Jul  3 08:13:58 2024 ] 	Batch(1600/7879) done. Loss: 1.0058  lr:0.010000
[ Wed Jul  3 08:14:16 2024 ] 	Batch(1700/7879) done. Loss: 0.6683  lr:0.010000
[ Wed Jul  3 08:14:35 2024 ] 	Batch(1800/7879) done. Loss: 0.6309  lr:0.010000
[ Wed Jul  3 08:14:53 2024 ] 	Batch(1900/7879) done. Loss: 0.1532  lr:0.010000
[ Wed Jul  3 08:15:12 2024 ] 
Training: Epoch [27/120], Step [1999], Loss: 0.571678638458252, Training Accuracy: 84.96875
[ Wed Jul  3 08:15:12 2024 ] 	Batch(2000/7879) done. Loss: 0.1809  lr:0.010000
[ Wed Jul  3 08:15:30 2024 ] 	Batch(2100/7879) done. Loss: 0.0081  lr:0.010000
[ Wed Jul  3 08:15:49 2024 ] 	Batch(2200/7879) done. Loss: 0.4100  lr:0.010000
[ Wed Jul  3 08:16:07 2024 ] 	Batch(2300/7879) done. Loss: 0.3159  lr:0.010000
[ Wed Jul  3 08:16:26 2024 ] 	Batch(2400/7879) done. Loss: 0.8229  lr:0.010000
[ Wed Jul  3 08:16:44 2024 ] 
Training: Epoch [27/120], Step [2499], Loss: 0.2532348036766052, Training Accuracy: 84.905
[ Wed Jul  3 08:16:44 2024 ] 	Batch(2500/7879) done. Loss: 0.4737  lr:0.010000
[ Wed Jul  3 08:17:02 2024 ] 	Batch(2600/7879) done. Loss: 0.0431  lr:0.010000
[ Wed Jul  3 08:17:20 2024 ] 	Batch(2700/7879) done. Loss: 0.8246  lr:0.010000
[ Wed Jul  3 08:17:38 2024 ] 	Batch(2800/7879) done. Loss: 0.6916  lr:0.010000
[ Wed Jul  3 08:17:56 2024 ] 	Batch(2900/7879) done. Loss: 0.3011  lr:0.010000
[ Wed Jul  3 08:18:14 2024 ] 
Training: Epoch [27/120], Step [2999], Loss: 0.9795544743537903, Training Accuracy: 84.775
[ Wed Jul  3 08:18:14 2024 ] 	Batch(3000/7879) done. Loss: 0.8312  lr:0.010000
[ Wed Jul  3 08:18:32 2024 ] 	Batch(3100/7879) done. Loss: 0.4857  lr:0.010000
[ Wed Jul  3 08:18:50 2024 ] 	Batch(3200/7879) done. Loss: 0.3729  lr:0.010000
[ Wed Jul  3 08:19:08 2024 ] 	Batch(3300/7879) done. Loss: 1.7343  lr:0.010000
[ Wed Jul  3 08:19:25 2024 ] 	Batch(3400/7879) done. Loss: 0.2150  lr:0.010000
[ Wed Jul  3 08:19:43 2024 ] 
Training: Epoch [27/120], Step [3499], Loss: 0.7475383281707764, Training Accuracy: 84.81428571428572
[ Wed Jul  3 08:19:43 2024 ] 	Batch(3500/7879) done. Loss: 0.2321  lr:0.010000
[ Wed Jul  3 08:20:01 2024 ] 	Batch(3600/7879) done. Loss: 0.6368  lr:0.010000
[ Wed Jul  3 08:20:19 2024 ] 	Batch(3700/7879) done. Loss: 0.2024  lr:0.010000
[ Wed Jul  3 08:20:37 2024 ] 	Batch(3800/7879) done. Loss: 0.7545  lr:0.010000
[ Wed Jul  3 08:20:55 2024 ] 	Batch(3900/7879) done. Loss: 0.9808  lr:0.010000
[ Wed Jul  3 08:21:12 2024 ] 
Training: Epoch [27/120], Step [3999], Loss: 0.22954176366329193, Training Accuracy: 84.8
[ Wed Jul  3 08:21:13 2024 ] 	Batch(4000/7879) done. Loss: 0.1805  lr:0.010000
[ Wed Jul  3 08:21:31 2024 ] 	Batch(4100/7879) done. Loss: 0.1125  lr:0.010000
[ Wed Jul  3 08:21:49 2024 ] 	Batch(4200/7879) done. Loss: 0.3643  lr:0.010000
[ Wed Jul  3 08:22:06 2024 ] 	Batch(4300/7879) done. Loss: 0.1354  lr:0.010000
[ Wed Jul  3 08:22:24 2024 ] 	Batch(4400/7879) done. Loss: 0.5926  lr:0.010000
[ Wed Jul  3 08:22:42 2024 ] 
Training: Epoch [27/120], Step [4499], Loss: 0.15586715936660767, Training Accuracy: 84.70833333333333
[ Wed Jul  3 08:22:42 2024 ] 	Batch(4500/7879) done. Loss: 0.1302  lr:0.010000
[ Wed Jul  3 08:23:00 2024 ] 	Batch(4600/7879) done. Loss: 0.8060  lr:0.010000
[ Wed Jul  3 08:23:18 2024 ] 	Batch(4700/7879) done. Loss: 1.1350  lr:0.010000
[ Wed Jul  3 08:23:36 2024 ] 	Batch(4800/7879) done. Loss: 0.9010  lr:0.010000
[ Wed Jul  3 08:23:54 2024 ] 	Batch(4900/7879) done. Loss: 0.1637  lr:0.010000
[ Wed Jul  3 08:24:11 2024 ] 
Training: Epoch [27/120], Step [4999], Loss: 0.2716941237449646, Training Accuracy: 84.55
[ Wed Jul  3 08:24:12 2024 ] 	Batch(5000/7879) done. Loss: 1.3399  lr:0.010000
[ Wed Jul  3 08:24:30 2024 ] 	Batch(5100/7879) done. Loss: 1.0041  lr:0.010000
[ Wed Jul  3 08:24:48 2024 ] 	Batch(5200/7879) done. Loss: 0.5795  lr:0.010000
[ Wed Jul  3 08:25:06 2024 ] 	Batch(5300/7879) done. Loss: 0.6480  lr:0.010000
[ Wed Jul  3 08:25:25 2024 ] 	Batch(5400/7879) done. Loss: 0.1086  lr:0.010000
[ Wed Jul  3 08:25:43 2024 ] 
Training: Epoch [27/120], Step [5499], Loss: 0.7424602508544922, Training Accuracy: 84.58409090909092
[ Wed Jul  3 08:25:43 2024 ] 	Batch(5500/7879) done. Loss: 0.5952  lr:0.010000
[ Wed Jul  3 08:26:02 2024 ] 	Batch(5600/7879) done. Loss: 1.0907  lr:0.010000
[ Wed Jul  3 08:26:20 2024 ] 	Batch(5700/7879) done. Loss: 0.3393  lr:0.010000
[ Wed Jul  3 08:26:39 2024 ] 	Batch(5800/7879) done. Loss: 0.3479  lr:0.010000
[ Wed Jul  3 08:26:57 2024 ] 	Batch(5900/7879) done. Loss: 0.2764  lr:0.010000
[ Wed Jul  3 08:27:14 2024 ] 
Training: Epoch [27/120], Step [5999], Loss: 1.4624983072280884, Training Accuracy: 84.44375
[ Wed Jul  3 08:27:15 2024 ] 	Batch(6000/7879) done. Loss: 0.7809  lr:0.010000
[ Wed Jul  3 08:27:33 2024 ] 	Batch(6100/7879) done. Loss: 0.2541  lr:0.010000
[ Wed Jul  3 08:27:51 2024 ] 	Batch(6200/7879) done. Loss: 0.4567  lr:0.010000
[ Wed Jul  3 08:28:08 2024 ] 	Batch(6300/7879) done. Loss: 1.2561  lr:0.010000
[ Wed Jul  3 08:28:26 2024 ] 	Batch(6400/7879) done. Loss: 0.7424  lr:0.010000
[ Wed Jul  3 08:28:44 2024 ] 
Training: Epoch [27/120], Step [6499], Loss: 0.11578210443258286, Training Accuracy: 84.41538461538461
[ Wed Jul  3 08:28:44 2024 ] 	Batch(6500/7879) done. Loss: 0.2852  lr:0.010000
[ Wed Jul  3 08:29:02 2024 ] 	Batch(6600/7879) done. Loss: 0.1139  lr:0.010000
[ Wed Jul  3 08:29:20 2024 ] 	Batch(6700/7879) done. Loss: 0.5982  lr:0.010000
[ Wed Jul  3 08:29:38 2024 ] 	Batch(6800/7879) done. Loss: 0.1782  lr:0.010000
[ Wed Jul  3 08:29:56 2024 ] 	Batch(6900/7879) done. Loss: 0.7389  lr:0.010000
[ Wed Jul  3 08:30:14 2024 ] 
Training: Epoch [27/120], Step [6999], Loss: 0.2222827970981598, Training Accuracy: 84.39999999999999
[ Wed Jul  3 08:30:14 2024 ] 	Batch(7000/7879) done. Loss: 0.1945  lr:0.010000
[ Wed Jul  3 08:30:32 2024 ] 	Batch(7100/7879) done. Loss: 0.3875  lr:0.010000
[ Wed Jul  3 08:30:50 2024 ] 	Batch(7200/7879) done. Loss: 0.9505  lr:0.010000
[ Wed Jul  3 08:31:07 2024 ] 	Batch(7300/7879) done. Loss: 0.3238  lr:0.010000
[ Wed Jul  3 08:31:25 2024 ] 	Batch(7400/7879) done. Loss: 0.6326  lr:0.010000
[ Wed Jul  3 08:31:43 2024 ] 
Training: Epoch [27/120], Step [7499], Loss: 1.0991897583007812, Training Accuracy: 84.40833333333333
[ Wed Jul  3 08:31:43 2024 ] 	Batch(7500/7879) done. Loss: 0.7850  lr:0.010000
[ Wed Jul  3 08:32:01 2024 ] 	Batch(7600/7879) done. Loss: 1.2795  lr:0.010000
[ Wed Jul  3 08:32:19 2024 ] 	Batch(7700/7879) done. Loss: 1.2506  lr:0.010000
[ Wed Jul  3 08:32:37 2024 ] 	Batch(7800/7879) done. Loss: 0.1548  lr:0.010000
[ Wed Jul  3 08:32:51 2024 ] 	Mean training loss: 0.5078.
[ Wed Jul  3 08:32:51 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 08:32:51 2024 ] Training epoch: 29
[ Wed Jul  3 08:32:51 2024 ] 	Batch(0/7879) done. Loss: 0.3361  lr:0.010000
[ Wed Jul  3 08:33:10 2024 ] 	Batch(100/7879) done. Loss: 0.3263  lr:0.010000
[ Wed Jul  3 08:33:28 2024 ] 	Batch(200/7879) done. Loss: 1.8739  lr:0.010000
[ Wed Jul  3 08:33:47 2024 ] 	Batch(300/7879) done. Loss: 0.1271  lr:0.010000
[ Wed Jul  3 08:34:05 2024 ] 	Batch(400/7879) done. Loss: 0.5911  lr:0.010000
[ Wed Jul  3 08:34:23 2024 ] 
Training: Epoch [28/120], Step [499], Loss: 0.47601401805877686, Training Accuracy: 85.175
[ Wed Jul  3 08:34:23 2024 ] 	Batch(500/7879) done. Loss: 0.4087  lr:0.010000
[ Wed Jul  3 08:34:41 2024 ] 	Batch(600/7879) done. Loss: 0.6150  lr:0.010000
[ Wed Jul  3 08:34:59 2024 ] 	Batch(700/7879) done. Loss: 0.4314  lr:0.010000
[ Wed Jul  3 08:35:17 2024 ] 	Batch(800/7879) done. Loss: 0.5354  lr:0.010000
[ Wed Jul  3 08:35:34 2024 ] 	Batch(900/7879) done. Loss: 1.1542  lr:0.010000
[ Wed Jul  3 08:35:52 2024 ] 
Training: Epoch [28/120], Step [999], Loss: 0.30234217643737793, Training Accuracy: 85.35000000000001
[ Wed Jul  3 08:35:52 2024 ] 	Batch(1000/7879) done. Loss: 0.3168  lr:0.010000
[ Wed Jul  3 08:36:10 2024 ] 	Batch(1100/7879) done. Loss: 0.2324  lr:0.010000
[ Wed Jul  3 08:36:28 2024 ] 	Batch(1200/7879) done. Loss: 0.3184  lr:0.010000
[ Wed Jul  3 08:36:47 2024 ] 	Batch(1300/7879) done. Loss: 0.6923  lr:0.010000
[ Wed Jul  3 08:37:05 2024 ] 	Batch(1400/7879) done. Loss: 0.7481  lr:0.010000
[ Wed Jul  3 08:37:24 2024 ] 
Training: Epoch [28/120], Step [1499], Loss: 1.3256340026855469, Training Accuracy: 85.28333333333333
[ Wed Jul  3 08:37:24 2024 ] 	Batch(1500/7879) done. Loss: 0.6246  lr:0.010000
[ Wed Jul  3 08:37:42 2024 ] 	Batch(1600/7879) done. Loss: 0.2013  lr:0.010000
[ Wed Jul  3 08:38:00 2024 ] 	Batch(1700/7879) done. Loss: 0.6600  lr:0.010000
[ Wed Jul  3 08:38:18 2024 ] 	Batch(1800/7879) done. Loss: 0.8170  lr:0.010000
[ Wed Jul  3 08:38:36 2024 ] 	Batch(1900/7879) done. Loss: 0.2748  lr:0.010000
[ Wed Jul  3 08:38:54 2024 ] 
Training: Epoch [28/120], Step [1999], Loss: 0.16296349465847015, Training Accuracy: 85.1625
[ Wed Jul  3 08:38:54 2024 ] 	Batch(2000/7879) done. Loss: 0.5459  lr:0.010000
[ Wed Jul  3 08:39:12 2024 ] 	Batch(2100/7879) done. Loss: 0.1288  lr:0.010000
[ Wed Jul  3 08:39:30 2024 ] 	Batch(2200/7879) done. Loss: 0.4656  lr:0.010000
[ Wed Jul  3 08:39:48 2024 ] 	Batch(2300/7879) done. Loss: 1.0610  lr:0.010000
[ Wed Jul  3 08:40:06 2024 ] 	Batch(2400/7879) done. Loss: 0.9468  lr:0.010000
[ Wed Jul  3 08:40:24 2024 ] 
Training: Epoch [28/120], Step [2499], Loss: 0.11936813592910767, Training Accuracy: 84.92
[ Wed Jul  3 08:40:24 2024 ] 	Batch(2500/7879) done. Loss: 0.0380  lr:0.010000
[ Wed Jul  3 08:40:42 2024 ] 	Batch(2600/7879) done. Loss: 0.3495  lr:0.010000
[ Wed Jul  3 08:41:00 2024 ] 	Batch(2700/7879) done. Loss: 0.2314  lr:0.010000
[ Wed Jul  3 08:41:18 2024 ] 	Batch(2800/7879) done. Loss: 0.3086  lr:0.010000
[ Wed Jul  3 08:41:35 2024 ] 	Batch(2900/7879) done. Loss: 0.8977  lr:0.010000
[ Wed Jul  3 08:41:53 2024 ] 
Training: Epoch [28/120], Step [2999], Loss: 0.34418606758117676, Training Accuracy: 84.84166666666667
[ Wed Jul  3 08:41:53 2024 ] 	Batch(3000/7879) done. Loss: 0.5717  lr:0.010000
[ Wed Jul  3 08:42:11 2024 ] 	Batch(3100/7879) done. Loss: 0.3881  lr:0.010000
[ Wed Jul  3 08:42:29 2024 ] 	Batch(3200/7879) done. Loss: 0.2121  lr:0.010000
[ Wed Jul  3 08:42:47 2024 ] 	Batch(3300/7879) done. Loss: 0.0091  lr:0.010000
[ Wed Jul  3 08:43:05 2024 ] 	Batch(3400/7879) done. Loss: 0.0035  lr:0.010000
[ Wed Jul  3 08:43:23 2024 ] 
Training: Epoch [28/120], Step [3499], Loss: 0.10947875678539276, Training Accuracy: 84.94285714285714
[ Wed Jul  3 08:43:23 2024 ] 	Batch(3500/7879) done. Loss: 0.3820  lr:0.010000
[ Wed Jul  3 08:43:41 2024 ] 	Batch(3600/7879) done. Loss: 0.8350  lr:0.010000
[ Wed Jul  3 08:43:59 2024 ] 	Batch(3700/7879) done. Loss: 0.0795  lr:0.010000
[ Wed Jul  3 08:44:16 2024 ] 	Batch(3800/7879) done. Loss: 0.1271  lr:0.010000
[ Wed Jul  3 08:44:34 2024 ] 	Batch(3900/7879) done. Loss: 0.5434  lr:0.010000
[ Wed Jul  3 08:44:52 2024 ] 
Training: Epoch [28/120], Step [3999], Loss: 0.3045141398906708, Training Accuracy: 84.990625
[ Wed Jul  3 08:44:52 2024 ] 	Batch(4000/7879) done. Loss: 1.3007  lr:0.010000
[ Wed Jul  3 08:45:10 2024 ] 	Batch(4100/7879) done. Loss: 0.2728  lr:0.010000
[ Wed Jul  3 08:45:28 2024 ] 	Batch(4200/7879) done. Loss: 1.1077  lr:0.010000
[ Wed Jul  3 08:45:46 2024 ] 	Batch(4300/7879) done. Loss: 0.5787  lr:0.010000
[ Wed Jul  3 08:46:04 2024 ] 	Batch(4400/7879) done. Loss: 0.8756  lr:0.010000
[ Wed Jul  3 08:46:22 2024 ] 
Training: Epoch [28/120], Step [4499], Loss: 0.4387180805206299, Training Accuracy: 84.80277777777778
[ Wed Jul  3 08:46:22 2024 ] 	Batch(4500/7879) done. Loss: 0.1785  lr:0.010000
[ Wed Jul  3 08:46:40 2024 ] 	Batch(4600/7879) done. Loss: 0.5808  lr:0.010000
[ Wed Jul  3 08:46:58 2024 ] 	Batch(4700/7879) done. Loss: 0.5737  lr:0.010000
[ Wed Jul  3 08:47:16 2024 ] 	Batch(4800/7879) done. Loss: 0.4828  lr:0.010000
[ Wed Jul  3 08:47:34 2024 ] 	Batch(4900/7879) done. Loss: 0.6363  lr:0.010000
[ Wed Jul  3 08:47:51 2024 ] 
Training: Epoch [28/120], Step [4999], Loss: 0.06361015141010284, Training Accuracy: 84.725
[ Wed Jul  3 08:47:51 2024 ] 	Batch(5000/7879) done. Loss: 0.3332  lr:0.010000
[ Wed Jul  3 08:48:09 2024 ] 	Batch(5100/7879) done. Loss: 1.1063  lr:0.010000
[ Wed Jul  3 08:48:27 2024 ] 	Batch(5200/7879) done. Loss: 0.0311  lr:0.010000
[ Wed Jul  3 08:48:45 2024 ] 	Batch(5300/7879) done. Loss: 0.1439  lr:0.010000
[ Wed Jul  3 08:49:03 2024 ] 	Batch(5400/7879) done. Loss: 0.7505  lr:0.010000
[ Wed Jul  3 08:49:21 2024 ] 
Training: Epoch [28/120], Step [5499], Loss: 0.02145381085574627, Training Accuracy: 84.65454545454546
[ Wed Jul  3 08:49:21 2024 ] 	Batch(5500/7879) done. Loss: 0.4179  lr:0.010000
[ Wed Jul  3 08:49:39 2024 ] 	Batch(5600/7879) done. Loss: 0.1520  lr:0.010000
[ Wed Jul  3 08:49:58 2024 ] 	Batch(5700/7879) done. Loss: 0.4603  lr:0.010000
[ Wed Jul  3 08:50:16 2024 ] 	Batch(5800/7879) done. Loss: 0.1726  lr:0.010000
[ Wed Jul  3 08:50:35 2024 ] 	Batch(5900/7879) done. Loss: 0.1159  lr:0.010000
[ Wed Jul  3 08:50:53 2024 ] 
Training: Epoch [28/120], Step [5999], Loss: 0.1510336995124817, Training Accuracy: 84.63958333333333
[ Wed Jul  3 08:50:53 2024 ] 	Batch(6000/7879) done. Loss: 0.9407  lr:0.010000
[ Wed Jul  3 08:51:12 2024 ] 	Batch(6100/7879) done. Loss: 0.4968  lr:0.010000
[ Wed Jul  3 08:51:30 2024 ] 	Batch(6200/7879) done. Loss: 0.4278  lr:0.010000
[ Wed Jul  3 08:51:49 2024 ] 	Batch(6300/7879) done. Loss: 0.5100  lr:0.010000
[ Wed Jul  3 08:52:08 2024 ] 	Batch(6400/7879) done. Loss: 1.0233  lr:0.010000
[ Wed Jul  3 08:52:26 2024 ] 
Training: Epoch [28/120], Step [6499], Loss: 0.9551565051078796, Training Accuracy: 84.6096153846154
[ Wed Jul  3 08:52:26 2024 ] 	Batch(6500/7879) done. Loss: 0.5526  lr:0.010000
[ Wed Jul  3 08:52:44 2024 ] 	Batch(6600/7879) done. Loss: 0.6321  lr:0.010000
[ Wed Jul  3 08:53:02 2024 ] 	Batch(6700/7879) done. Loss: 0.7837  lr:0.010000
[ Wed Jul  3 08:53:21 2024 ] 	Batch(6800/7879) done. Loss: 0.6051  lr:0.010000
[ Wed Jul  3 08:53:39 2024 ] 	Batch(6900/7879) done. Loss: 0.4221  lr:0.010000
[ Wed Jul  3 08:53:56 2024 ] 
Training: Epoch [28/120], Step [6999], Loss: 0.3347246050834656, Training Accuracy: 84.55535714285715
[ Wed Jul  3 08:53:56 2024 ] 	Batch(7000/7879) done. Loss: 1.1848  lr:0.010000
[ Wed Jul  3 08:54:14 2024 ] 	Batch(7100/7879) done. Loss: 0.2689  lr:0.010000
[ Wed Jul  3 08:54:32 2024 ] 	Batch(7200/7879) done. Loss: 0.0734  lr:0.010000
[ Wed Jul  3 08:54:50 2024 ] 	Batch(7300/7879) done. Loss: 0.2208  lr:0.010000
[ Wed Jul  3 08:55:08 2024 ] 	Batch(7400/7879) done. Loss: 0.2514  lr:0.010000
[ Wed Jul  3 08:55:26 2024 ] 
Training: Epoch [28/120], Step [7499], Loss: 0.024166643619537354, Training Accuracy: 84.52499999999999
[ Wed Jul  3 08:55:26 2024 ] 	Batch(7500/7879) done. Loss: 0.6495  lr:0.010000
[ Wed Jul  3 08:55:44 2024 ] 	Batch(7600/7879) done. Loss: 0.7230  lr:0.010000
[ Wed Jul  3 08:56:02 2024 ] 	Batch(7700/7879) done. Loss: 0.1257  lr:0.010000
[ Wed Jul  3 08:56:20 2024 ] 	Batch(7800/7879) done. Loss: 0.1823  lr:0.010000
[ Wed Jul  3 08:56:34 2024 ] 	Mean training loss: 0.5063.
[ Wed Jul  3 08:56:34 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 08:56:34 2024 ] Training epoch: 30
[ Wed Jul  3 08:56:34 2024 ] 	Batch(0/7879) done. Loss: 1.9408  lr:0.010000
[ Wed Jul  3 08:56:52 2024 ] 	Batch(100/7879) done. Loss: 0.2393  lr:0.010000
[ Wed Jul  3 08:57:11 2024 ] 	Batch(200/7879) done. Loss: 0.3950  lr:0.010000
[ Wed Jul  3 08:57:29 2024 ] 	Batch(300/7879) done. Loss: 0.1620  lr:0.010000
[ Wed Jul  3 08:57:48 2024 ] 	Batch(400/7879) done. Loss: 0.2456  lr:0.010000
[ Wed Jul  3 08:58:06 2024 ] 
Training: Epoch [29/120], Step [499], Loss: 0.786403477191925, Training Accuracy: 87.9
[ Wed Jul  3 08:58:06 2024 ] 	Batch(500/7879) done. Loss: 0.3831  lr:0.010000
[ Wed Jul  3 08:58:25 2024 ] 	Batch(600/7879) done. Loss: 0.9389  lr:0.010000
[ Wed Jul  3 08:58:43 2024 ] 	Batch(700/7879) done. Loss: 0.1567  lr:0.010000
[ Wed Jul  3 08:59:02 2024 ] 	Batch(800/7879) done. Loss: 0.7849  lr:0.010000
[ Wed Jul  3 08:59:20 2024 ] 	Batch(900/7879) done. Loss: 0.3695  lr:0.010000
[ Wed Jul  3 08:59:38 2024 ] 
Training: Epoch [29/120], Step [999], Loss: 0.710301399230957, Training Accuracy: 87.4
[ Wed Jul  3 08:59:38 2024 ] 	Batch(1000/7879) done. Loss: 0.0997  lr:0.010000
[ Wed Jul  3 08:59:56 2024 ] 	Batch(1100/7879) done. Loss: 0.4367  lr:0.010000
[ Wed Jul  3 09:00:14 2024 ] 	Batch(1200/7879) done. Loss: 0.7426  lr:0.010000
[ Wed Jul  3 09:00:32 2024 ] 	Batch(1300/7879) done. Loss: 1.0921  lr:0.010000
[ Wed Jul  3 09:00:49 2024 ] 	Batch(1400/7879) done. Loss: 0.2260  lr:0.010000
[ Wed Jul  3 09:01:07 2024 ] 
Training: Epoch [29/120], Step [1499], Loss: 0.670677661895752, Training Accuracy: 86.89166666666667
[ Wed Jul  3 09:01:07 2024 ] 	Batch(1500/7879) done. Loss: 1.3410  lr:0.010000
[ Wed Jul  3 09:01:25 2024 ] 	Batch(1600/7879) done. Loss: 0.1178  lr:0.010000
[ Wed Jul  3 09:01:43 2024 ] 	Batch(1700/7879) done. Loss: 0.1592  lr:0.010000
[ Wed Jul  3 09:02:01 2024 ] 	Batch(1800/7879) done. Loss: 0.4237  lr:0.010000
[ Wed Jul  3 09:02:19 2024 ] 	Batch(1900/7879) done. Loss: 0.0712  lr:0.010000
[ Wed Jul  3 09:02:37 2024 ] 
Training: Epoch [29/120], Step [1999], Loss: 0.7188940048217773, Training Accuracy: 86.51875
[ Wed Jul  3 09:02:37 2024 ] 	Batch(2000/7879) done. Loss: 0.4686  lr:0.010000
[ Wed Jul  3 09:02:55 2024 ] 	Batch(2100/7879) done. Loss: 0.4422  lr:0.010000
[ Wed Jul  3 09:03:13 2024 ] 	Batch(2200/7879) done. Loss: 0.3033  lr:0.010000
[ Wed Jul  3 09:03:31 2024 ] 	Batch(2300/7879) done. Loss: 0.8308  lr:0.010000
[ Wed Jul  3 09:03:49 2024 ] 	Batch(2400/7879) done. Loss: 0.4149  lr:0.010000
[ Wed Jul  3 09:04:06 2024 ] 
Training: Epoch [29/120], Step [2499], Loss: 0.16143321990966797, Training Accuracy: 86.41499999999999
[ Wed Jul  3 09:04:06 2024 ] 	Batch(2500/7879) done. Loss: 0.0760  lr:0.010000
[ Wed Jul  3 09:04:24 2024 ] 	Batch(2600/7879) done. Loss: 0.6675  lr:0.010000
[ Wed Jul  3 09:04:42 2024 ] 	Batch(2700/7879) done. Loss: 1.1000  lr:0.010000
[ Wed Jul  3 09:05:00 2024 ] 	Batch(2800/7879) done. Loss: 0.0407  lr:0.010000
[ Wed Jul  3 09:05:19 2024 ] 	Batch(2900/7879) done. Loss: 0.2730  lr:0.010000
[ Wed Jul  3 09:05:37 2024 ] 
Training: Epoch [29/120], Step [2999], Loss: 1.061329960823059, Training Accuracy: 86.08749999999999
[ Wed Jul  3 09:05:37 2024 ] 	Batch(3000/7879) done. Loss: 0.8972  lr:0.010000
[ Wed Jul  3 09:05:56 2024 ] 	Batch(3100/7879) done. Loss: 0.2812  lr:0.010000
[ Wed Jul  3 09:06:14 2024 ] 	Batch(3200/7879) done. Loss: 0.2943  lr:0.010000
[ Wed Jul  3 09:06:32 2024 ] 	Batch(3300/7879) done. Loss: 0.2955  lr:0.010000
[ Wed Jul  3 09:06:50 2024 ] 	Batch(3400/7879) done. Loss: 0.2625  lr:0.010000
[ Wed Jul  3 09:07:07 2024 ] 
Training: Epoch [29/120], Step [3499], Loss: 0.5301746726036072, Training Accuracy: 86.0142857142857
[ Wed Jul  3 09:07:08 2024 ] 	Batch(3500/7879) done. Loss: 0.5202  lr:0.010000
[ Wed Jul  3 09:07:26 2024 ] 	Batch(3600/7879) done. Loss: 0.6339  lr:0.010000
[ Wed Jul  3 09:07:44 2024 ] 	Batch(3700/7879) done. Loss: 0.3022  lr:0.010000
[ Wed Jul  3 09:08:03 2024 ] 	Batch(3800/7879) done. Loss: 0.1834  lr:0.010000
[ Wed Jul  3 09:08:21 2024 ] 	Batch(3900/7879) done. Loss: 0.2155  lr:0.010000
[ Wed Jul  3 09:08:39 2024 ] 
Training: Epoch [29/120], Step [3999], Loss: 0.20146864652633667, Training Accuracy: 85.86875
[ Wed Jul  3 09:08:40 2024 ] 	Batch(4000/7879) done. Loss: 0.0985  lr:0.010000
[ Wed Jul  3 09:08:58 2024 ] 	Batch(4100/7879) done. Loss: 0.0487  lr:0.010000
[ Wed Jul  3 09:09:15 2024 ] 	Batch(4200/7879) done. Loss: 1.0447  lr:0.010000
[ Wed Jul  3 09:09:33 2024 ] 	Batch(4300/7879) done. Loss: 0.3274  lr:0.010000
[ Wed Jul  3 09:09:51 2024 ] 	Batch(4400/7879) done. Loss: 0.8453  lr:0.010000
[ Wed Jul  3 09:10:09 2024 ] 
Training: Epoch [29/120], Step [4499], Loss: 0.3634413480758667, Training Accuracy: 85.82777777777778
[ Wed Jul  3 09:10:09 2024 ] 	Batch(4500/7879) done. Loss: 0.3206  lr:0.010000
[ Wed Jul  3 09:10:27 2024 ] 	Batch(4600/7879) done. Loss: 0.1123  lr:0.010000
[ Wed Jul  3 09:10:45 2024 ] 	Batch(4700/7879) done. Loss: 0.3238  lr:0.010000
[ Wed Jul  3 09:11:03 2024 ] 	Batch(4800/7879) done. Loss: 0.5789  lr:0.010000
[ Wed Jul  3 09:11:21 2024 ] 	Batch(4900/7879) done. Loss: 0.2127  lr:0.010000
[ Wed Jul  3 09:11:39 2024 ] 
Training: Epoch [29/120], Step [4999], Loss: 1.27375328540802, Training Accuracy: 85.835
[ Wed Jul  3 09:11:39 2024 ] 	Batch(5000/7879) done. Loss: 0.4297  lr:0.010000
[ Wed Jul  3 09:11:58 2024 ] 	Batch(5100/7879) done. Loss: 0.2388  lr:0.010000
[ Wed Jul  3 09:12:16 2024 ] 	Batch(5200/7879) done. Loss: 0.3286  lr:0.010000
[ Wed Jul  3 09:12:34 2024 ] 	Batch(5300/7879) done. Loss: 0.7590  lr:0.010000
[ Wed Jul  3 09:12:52 2024 ] 	Batch(5400/7879) done. Loss: 0.6902  lr:0.010000
[ Wed Jul  3 09:13:10 2024 ] 
Training: Epoch [29/120], Step [5499], Loss: 1.1185389757156372, Training Accuracy: 85.6909090909091
[ Wed Jul  3 09:13:10 2024 ] 	Batch(5500/7879) done. Loss: 0.8171  lr:0.010000
[ Wed Jul  3 09:13:28 2024 ] 	Batch(5600/7879) done. Loss: 0.5131  lr:0.010000
[ Wed Jul  3 09:13:46 2024 ] 	Batch(5700/7879) done. Loss: 0.7181  lr:0.010000
[ Wed Jul  3 09:14:04 2024 ] 	Batch(5800/7879) done. Loss: 0.5742  lr:0.010000
[ Wed Jul  3 09:14:22 2024 ] 	Batch(5900/7879) done. Loss: 0.0800  lr:0.010000
[ Wed Jul  3 09:14:40 2024 ] 
Training: Epoch [29/120], Step [5999], Loss: 1.076978325843811, Training Accuracy: 85.44375
[ Wed Jul  3 09:14:40 2024 ] 	Batch(6000/7879) done. Loss: 0.3480  lr:0.010000
[ Wed Jul  3 09:14:58 2024 ] 	Batch(6100/7879) done. Loss: 0.0797  lr:0.010000
[ Wed Jul  3 09:15:17 2024 ] 	Batch(6200/7879) done. Loss: 0.4881  lr:0.010000
[ Wed Jul  3 09:15:35 2024 ] 	Batch(6300/7879) done. Loss: 0.7486  lr:0.010000
[ Wed Jul  3 09:15:54 2024 ] 	Batch(6400/7879) done. Loss: 0.1958  lr:0.010000
[ Wed Jul  3 09:16:12 2024 ] 
Training: Epoch [29/120], Step [6499], Loss: 0.4074781537055969, Training Accuracy: 85.30961538461538
[ Wed Jul  3 09:16:13 2024 ] 	Batch(6500/7879) done. Loss: 0.7055  lr:0.010000
[ Wed Jul  3 09:16:30 2024 ] 	Batch(6600/7879) done. Loss: 0.6033  lr:0.010000
[ Wed Jul  3 09:16:48 2024 ] 	Batch(6700/7879) done. Loss: 0.3282  lr:0.010000
[ Wed Jul  3 09:17:06 2024 ] 	Batch(6800/7879) done. Loss: 1.1494  lr:0.010000
[ Wed Jul  3 09:17:24 2024 ] 	Batch(6900/7879) done. Loss: 0.3035  lr:0.010000
[ Wed Jul  3 09:17:42 2024 ] 
Training: Epoch [29/120], Step [6999], Loss: 0.3554442524909973, Training Accuracy: 85.29464285714286
[ Wed Jul  3 09:17:42 2024 ] 	Batch(7000/7879) done. Loss: 0.8422  lr:0.010000
[ Wed Jul  3 09:18:00 2024 ] 	Batch(7100/7879) done. Loss: 0.9494  lr:0.010000
[ Wed Jul  3 09:18:18 2024 ] 	Batch(7200/7879) done. Loss: 0.5085  lr:0.010000
[ Wed Jul  3 09:18:36 2024 ] 	Batch(7300/7879) done. Loss: 0.9380  lr:0.010000
[ Wed Jul  3 09:18:53 2024 ] 	Batch(7400/7879) done. Loss: 1.0089  lr:0.010000
[ Wed Jul  3 09:19:11 2024 ] 
Training: Epoch [29/120], Step [7499], Loss: 1.1867743730545044, Training Accuracy: 85.20166666666667
[ Wed Jul  3 09:19:11 2024 ] 	Batch(7500/7879) done. Loss: 0.3536  lr:0.010000
[ Wed Jul  3 09:19:29 2024 ] 	Batch(7600/7879) done. Loss: 0.4289  lr:0.010000
[ Wed Jul  3 09:19:47 2024 ] 	Batch(7700/7879) done. Loss: 0.6358  lr:0.010000
[ Wed Jul  3 09:20:05 2024 ] 	Batch(7800/7879) done. Loss: 0.7673  lr:0.010000
[ Wed Jul  3 09:20:19 2024 ] 	Mean training loss: 0.4881.
[ Wed Jul  3 09:20:19 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 09:20:19 2024 ] Eval epoch: 30
[ Wed Jul  3 09:25:06 2024 ] 	Mean val loss of 6365 batches: 2.2220457734119505.
[ Wed Jul  3 09:25:06 2024 ] Training epoch: 31
[ Wed Jul  3 09:25:07 2024 ] 	Batch(0/7879) done. Loss: 0.9075  lr:0.010000
[ Wed Jul  3 09:25:24 2024 ] 	Batch(100/7879) done. Loss: 0.1973  lr:0.010000
[ Wed Jul  3 09:25:42 2024 ] 	Batch(200/7879) done. Loss: 0.4891  lr:0.010000
[ Wed Jul  3 09:26:00 2024 ] 	Batch(300/7879) done. Loss: 0.1267  lr:0.010000
[ Wed Jul  3 09:26:18 2024 ] 	Batch(400/7879) done. Loss: 0.2631  lr:0.010000
[ Wed Jul  3 09:26:36 2024 ] 
Training: Epoch [30/120], Step [499], Loss: 0.7390821576118469, Training Accuracy: 85.55
[ Wed Jul  3 09:26:36 2024 ] 	Batch(500/7879) done. Loss: 0.2383  lr:0.010000
[ Wed Jul  3 09:26:54 2024 ] 	Batch(600/7879) done. Loss: 1.0068  lr:0.010000
[ Wed Jul  3 09:27:12 2024 ] 	Batch(700/7879) done. Loss: 0.4300  lr:0.010000
[ Wed Jul  3 09:27:30 2024 ] 	Batch(800/7879) done. Loss: 0.0686  lr:0.010000
[ Wed Jul  3 09:27:48 2024 ] 	Batch(900/7879) done. Loss: 0.3516  lr:0.010000
[ Wed Jul  3 09:28:05 2024 ] 
Training: Epoch [30/120], Step [999], Loss: 0.1290409117937088, Training Accuracy: 85.78750000000001
[ Wed Jul  3 09:28:05 2024 ] 	Batch(1000/7879) done. Loss: 0.2214  lr:0.010000
[ Wed Jul  3 09:28:23 2024 ] 	Batch(1100/7879) done. Loss: 0.1446  lr:0.010000
[ Wed Jul  3 09:28:41 2024 ] 	Batch(1200/7879) done. Loss: 0.1419  lr:0.010000
[ Wed Jul  3 09:29:00 2024 ] 	Batch(1300/7879) done. Loss: 0.2103  lr:0.010000
[ Wed Jul  3 09:29:18 2024 ] 	Batch(1400/7879) done. Loss: 0.0017  lr:0.010000
[ Wed Jul  3 09:29:36 2024 ] 
Training: Epoch [30/120], Step [1499], Loss: 0.46972209215164185, Training Accuracy: 85.79166666666667
[ Wed Jul  3 09:29:36 2024 ] 	Batch(1500/7879) done. Loss: 0.6478  lr:0.010000
[ Wed Jul  3 09:29:54 2024 ] 	Batch(1600/7879) done. Loss: 0.1256  lr:0.010000
[ Wed Jul  3 09:30:12 2024 ] 	Batch(1700/7879) done. Loss: 0.3513  lr:0.010000
[ Wed Jul  3 09:30:30 2024 ] 	Batch(1800/7879) done. Loss: 0.4240  lr:0.010000
[ Wed Jul  3 09:30:48 2024 ] 	Batch(1900/7879) done. Loss: 0.6305  lr:0.010000
[ Wed Jul  3 09:31:05 2024 ] 
Training: Epoch [30/120], Step [1999], Loss: 0.6859627962112427, Training Accuracy: 85.78750000000001
[ Wed Jul  3 09:31:06 2024 ] 	Batch(2000/7879) done. Loss: 0.4448  lr:0.010000
[ Wed Jul  3 09:31:24 2024 ] 	Batch(2100/7879) done. Loss: 0.7381  lr:0.010000
[ Wed Jul  3 09:31:42 2024 ] 	Batch(2200/7879) done. Loss: 0.4731  lr:0.010000
[ Wed Jul  3 09:31:59 2024 ] 	Batch(2300/7879) done. Loss: 0.4100  lr:0.010000
[ Wed Jul  3 09:32:17 2024 ] 	Batch(2400/7879) done. Loss: 0.1530  lr:0.010000
[ Wed Jul  3 09:32:35 2024 ] 
Training: Epoch [30/120], Step [2499], Loss: 0.628862202167511, Training Accuracy: 85.795
[ Wed Jul  3 09:32:35 2024 ] 	Batch(2500/7879) done. Loss: 0.4465  lr:0.010000
[ Wed Jul  3 09:32:53 2024 ] 	Batch(2600/7879) done. Loss: 0.8205  lr:0.010000
[ Wed Jul  3 09:33:11 2024 ] 	Batch(2700/7879) done. Loss: 0.8534  lr:0.010000
[ Wed Jul  3 09:33:29 2024 ] 	Batch(2800/7879) done. Loss: 0.5118  lr:0.010000
[ Wed Jul  3 09:33:47 2024 ] 	Batch(2900/7879) done. Loss: 0.4100  lr:0.010000
[ Wed Jul  3 09:34:05 2024 ] 
Training: Epoch [30/120], Step [2999], Loss: 0.3913508653640747, Training Accuracy: 85.77916666666667
[ Wed Jul  3 09:34:05 2024 ] 	Batch(3000/7879) done. Loss: 0.2315  lr:0.010000
[ Wed Jul  3 09:34:23 2024 ] 	Batch(3100/7879) done. Loss: 0.2501  lr:0.010000
[ Wed Jul  3 09:34:41 2024 ] 	Batch(3200/7879) done. Loss: 0.4820  lr:0.010000
[ Wed Jul  3 09:34:59 2024 ] 	Batch(3300/7879) done. Loss: 0.6330  lr:0.010000
[ Wed Jul  3 09:35:17 2024 ] 	Batch(3400/7879) done. Loss: 0.2889  lr:0.010000
[ Wed Jul  3 09:35:34 2024 ] 
Training: Epoch [30/120], Step [3499], Loss: 0.12869864702224731, Training Accuracy: 85.78571428571429
[ Wed Jul  3 09:35:35 2024 ] 	Batch(3500/7879) done. Loss: 1.2140  lr:0.010000
[ Wed Jul  3 09:35:53 2024 ] 	Batch(3600/7879) done. Loss: 0.4080  lr:0.010000
[ Wed Jul  3 09:36:11 2024 ] 	Batch(3700/7879) done. Loss: 0.2061  lr:0.010000
[ Wed Jul  3 09:36:29 2024 ] 	Batch(3800/7879) done. Loss: 0.0377  lr:0.010000
[ Wed Jul  3 09:36:46 2024 ] 	Batch(3900/7879) done. Loss: 0.2881  lr:0.010000
[ Wed Jul  3 09:37:04 2024 ] 
Training: Epoch [30/120], Step [3999], Loss: 0.686665952205658, Training Accuracy: 85.7375
[ Wed Jul  3 09:37:05 2024 ] 	Batch(4000/7879) done. Loss: 0.8040  lr:0.010000
[ Wed Jul  3 09:37:23 2024 ] 	Batch(4100/7879) done. Loss: 0.5716  lr:0.010000
[ Wed Jul  3 09:37:42 2024 ] 	Batch(4200/7879) done. Loss: 0.6667  lr:0.010000
[ Wed Jul  3 09:38:00 2024 ] 	Batch(4300/7879) done. Loss: 0.5571  lr:0.010000
[ Wed Jul  3 09:38:19 2024 ] 	Batch(4400/7879) done. Loss: 0.3645  lr:0.010000
[ Wed Jul  3 09:38:37 2024 ] 
Training: Epoch [30/120], Step [4499], Loss: 0.21708089113235474, Training Accuracy: 85.73055555555555
[ Wed Jul  3 09:38:37 2024 ] 	Batch(4500/7879) done. Loss: 0.1938  lr:0.010000
[ Wed Jul  3 09:38:55 2024 ] 	Batch(4600/7879) done. Loss: 0.1216  lr:0.010000
[ Wed Jul  3 09:39:13 2024 ] 	Batch(4700/7879) done. Loss: 0.2288  lr:0.010000
[ Wed Jul  3 09:39:31 2024 ] 	Batch(4800/7879) done. Loss: 0.3653  lr:0.010000
[ Wed Jul  3 09:39:49 2024 ] 	Batch(4900/7879) done. Loss: 0.3018  lr:0.010000
[ Wed Jul  3 09:40:08 2024 ] 
Training: Epoch [30/120], Step [4999], Loss: 0.056720130145549774, Training Accuracy: 85.875
[ Wed Jul  3 09:40:08 2024 ] 	Batch(5000/7879) done. Loss: 0.0236  lr:0.010000
[ Wed Jul  3 09:40:26 2024 ] 	Batch(5100/7879) done. Loss: 0.2087  lr:0.010000
[ Wed Jul  3 09:40:45 2024 ] 	Batch(5200/7879) done. Loss: 0.6613  lr:0.010000
[ Wed Jul  3 09:41:03 2024 ] 	Batch(5300/7879) done. Loss: 0.1679  lr:0.010000
[ Wed Jul  3 09:41:22 2024 ] 	Batch(5400/7879) done. Loss: 0.4468  lr:0.010000
[ Wed Jul  3 09:41:40 2024 ] 
Training: Epoch [30/120], Step [5499], Loss: 0.18603524565696716, Training Accuracy: 85.84318181818182
[ Wed Jul  3 09:41:41 2024 ] 	Batch(5500/7879) done. Loss: 0.0683  lr:0.010000
[ Wed Jul  3 09:41:59 2024 ] 	Batch(5600/7879) done. Loss: 0.8091  lr:0.010000
[ Wed Jul  3 09:42:18 2024 ] 	Batch(5700/7879) done. Loss: 0.1943  lr:0.010000
[ Wed Jul  3 09:42:36 2024 ] 	Batch(5800/7879) done. Loss: 1.0281  lr:0.010000
[ Wed Jul  3 09:42:54 2024 ] 	Batch(5900/7879) done. Loss: 0.6729  lr:0.010000
[ Wed Jul  3 09:43:11 2024 ] 
Training: Epoch [30/120], Step [5999], Loss: 0.12026908248662949, Training Accuracy: 85.81666666666666
[ Wed Jul  3 09:43:12 2024 ] 	Batch(6000/7879) done. Loss: 0.8359  lr:0.010000
[ Wed Jul  3 09:43:30 2024 ] 	Batch(6100/7879) done. Loss: 0.1379  lr:0.010000
[ Wed Jul  3 09:43:47 2024 ] 	Batch(6200/7879) done. Loss: 0.6823  lr:0.010000
[ Wed Jul  3 09:44:05 2024 ] 	Batch(6300/7879) done. Loss: 0.2734  lr:0.010000
[ Wed Jul  3 09:44:23 2024 ] 	Batch(6400/7879) done. Loss: 0.0533  lr:0.010000
[ Wed Jul  3 09:44:41 2024 ] 
Training: Epoch [30/120], Step [6499], Loss: 0.7770369648933411, Training Accuracy: 85.70384615384616
[ Wed Jul  3 09:44:41 2024 ] 	Batch(6500/7879) done. Loss: 0.5258  lr:0.010000
[ Wed Jul  3 09:44:59 2024 ] 	Batch(6600/7879) done. Loss: 0.3847  lr:0.010000
[ Wed Jul  3 09:45:17 2024 ] 	Batch(6700/7879) done. Loss: 0.4968  lr:0.010000
[ Wed Jul  3 09:45:35 2024 ] 	Batch(6800/7879) done. Loss: 0.9250  lr:0.010000
[ Wed Jul  3 09:45:53 2024 ] 	Batch(6900/7879) done. Loss: 0.0881  lr:0.010000
[ Wed Jul  3 09:46:10 2024 ] 
Training: Epoch [30/120], Step [6999], Loss: 0.12216287851333618, Training Accuracy: 85.60714285714286
[ Wed Jul  3 09:46:11 2024 ] 	Batch(7000/7879) done. Loss: 0.2201  lr:0.010000
[ Wed Jul  3 09:46:28 2024 ] 	Batch(7100/7879) done. Loss: 0.8393  lr:0.010000
[ Wed Jul  3 09:46:47 2024 ] 	Batch(7200/7879) done. Loss: 0.3091  lr:0.010000
[ Wed Jul  3 09:47:04 2024 ] 	Batch(7300/7879) done. Loss: 1.0793  lr:0.010000
[ Wed Jul  3 09:47:22 2024 ] 	Batch(7400/7879) done. Loss: 0.4619  lr:0.010000
[ Wed Jul  3 09:47:40 2024 ] 
Training: Epoch [30/120], Step [7499], Loss: 0.7893695831298828, Training Accuracy: 85.55166666666668
[ Wed Jul  3 09:47:41 2024 ] 	Batch(7500/7879) done. Loss: 0.0375  lr:0.010000
[ Wed Jul  3 09:47:59 2024 ] 	Batch(7600/7879) done. Loss: 0.5815  lr:0.010000
[ Wed Jul  3 09:48:18 2024 ] 	Batch(7700/7879) done. Loss: 0.1158  lr:0.010000
[ Wed Jul  3 09:48:36 2024 ] 	Batch(7800/7879) done. Loss: 0.2093  lr:0.010000
[ Wed Jul  3 09:48:51 2024 ] 	Mean training loss: 0.4778.
[ Wed Jul  3 09:48:51 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 09:48:51 2024 ] Training epoch: 32
[ Wed Jul  3 09:48:52 2024 ] 	Batch(0/7879) done. Loss: 0.3741  lr:0.010000
[ Wed Jul  3 09:49:10 2024 ] 	Batch(100/7879) done. Loss: 0.3524  lr:0.010000
[ Wed Jul  3 09:49:28 2024 ] 	Batch(200/7879) done. Loss: 0.1717  lr:0.010000
[ Wed Jul  3 09:49:45 2024 ] 	Batch(300/7879) done. Loss: 0.1221  lr:0.010000
[ Wed Jul  3 09:50:04 2024 ] 	Batch(400/7879) done. Loss: 0.6915  lr:0.010000
[ Wed Jul  3 09:50:21 2024 ] 
Training: Epoch [31/120], Step [499], Loss: 0.5069356560707092, Training Accuracy: 85.5
[ Wed Jul  3 09:50:21 2024 ] 	Batch(500/7879) done. Loss: 0.5063  lr:0.010000
[ Wed Jul  3 09:50:39 2024 ] 	Batch(600/7879) done. Loss: 0.2313  lr:0.010000
[ Wed Jul  3 09:50:57 2024 ] 	Batch(700/7879) done. Loss: 0.5739  lr:0.010000
[ Wed Jul  3 09:51:15 2024 ] 	Batch(800/7879) done. Loss: 0.2938  lr:0.010000
[ Wed Jul  3 09:51:34 2024 ] 	Batch(900/7879) done. Loss: 0.3868  lr:0.010000
[ Wed Jul  3 09:51:51 2024 ] 
Training: Epoch [31/120], Step [999], Loss: 0.15978898108005524, Training Accuracy: 86.1875
[ Wed Jul  3 09:51:52 2024 ] 	Batch(1000/7879) done. Loss: 0.3012  lr:0.010000
[ Wed Jul  3 09:52:09 2024 ] 	Batch(1100/7879) done. Loss: 0.1896  lr:0.010000
[ Wed Jul  3 09:52:28 2024 ] 	Batch(1200/7879) done. Loss: 0.6989  lr:0.010000
[ Wed Jul  3 09:52:45 2024 ] 	Batch(1300/7879) done. Loss: 0.8199  lr:0.010000
[ Wed Jul  3 09:53:03 2024 ] 	Batch(1400/7879) done. Loss: 0.0319  lr:0.010000
[ Wed Jul  3 09:53:21 2024 ] 
Training: Epoch [31/120], Step [1499], Loss: 0.16317400336265564, Training Accuracy: 86.30833333333334
[ Wed Jul  3 09:53:21 2024 ] 	Batch(1500/7879) done. Loss: 0.3161  lr:0.010000
[ Wed Jul  3 09:53:39 2024 ] 	Batch(1600/7879) done. Loss: 0.2044  lr:0.010000
[ Wed Jul  3 09:53:57 2024 ] 	Batch(1700/7879) done. Loss: 0.8543  lr:0.010000
[ Wed Jul  3 09:54:15 2024 ] 	Batch(1800/7879) done. Loss: 0.5833  lr:0.010000
[ Wed Jul  3 09:54:33 2024 ] 	Batch(1900/7879) done. Loss: 0.8822  lr:0.010000
[ Wed Jul  3 09:54:51 2024 ] 
Training: Epoch [31/120], Step [1999], Loss: 0.20011337101459503, Training Accuracy: 86.25625
[ Wed Jul  3 09:54:51 2024 ] 	Batch(2000/7879) done. Loss: 0.4818  lr:0.010000
[ Wed Jul  3 09:55:10 2024 ] 	Batch(2100/7879) done. Loss: 0.6880  lr:0.010000
[ Wed Jul  3 09:55:28 2024 ] 	Batch(2200/7879) done. Loss: 0.1356  lr:0.010000
[ Wed Jul  3 09:55:47 2024 ] 	Batch(2300/7879) done. Loss: 0.5024  lr:0.010000
[ Wed Jul  3 09:56:05 2024 ] 	Batch(2400/7879) done. Loss: 0.1619  lr:0.010000
[ Wed Jul  3 09:56:23 2024 ] 
Training: Epoch [31/120], Step [2499], Loss: 0.32441556453704834, Training Accuracy: 86.315
[ Wed Jul  3 09:56:23 2024 ] 	Batch(2500/7879) done. Loss: 0.4815  lr:0.010000
[ Wed Jul  3 09:56:41 2024 ] 	Batch(2600/7879) done. Loss: 0.0427  lr:0.010000
[ Wed Jul  3 09:56:59 2024 ] 	Batch(2700/7879) done. Loss: 0.5082  lr:0.010000
[ Wed Jul  3 09:57:17 2024 ] 	Batch(2800/7879) done. Loss: 0.0481  lr:0.010000
[ Wed Jul  3 09:57:34 2024 ] 	Batch(2900/7879) done. Loss: 0.0474  lr:0.010000
[ Wed Jul  3 09:57:52 2024 ] 
Training: Epoch [31/120], Step [2999], Loss: 0.21548442542552948, Training Accuracy: 86.23333333333333
[ Wed Jul  3 09:57:52 2024 ] 	Batch(3000/7879) done. Loss: 0.5668  lr:0.010000
[ Wed Jul  3 09:58:10 2024 ] 	Batch(3100/7879) done. Loss: 0.4082  lr:0.010000
[ Wed Jul  3 09:58:28 2024 ] 	Batch(3200/7879) done. Loss: 1.6650  lr:0.010000
[ Wed Jul  3 09:58:46 2024 ] 	Batch(3300/7879) done. Loss: 0.5787  lr:0.010000
[ Wed Jul  3 09:59:04 2024 ] 	Batch(3400/7879) done. Loss: 0.3612  lr:0.010000
[ Wed Jul  3 09:59:22 2024 ] 
Training: Epoch [31/120], Step [3499], Loss: 0.2956741452217102, Training Accuracy: 86.03928571428571
[ Wed Jul  3 09:59:22 2024 ] 	Batch(3500/7879) done. Loss: 0.3733  lr:0.010000
[ Wed Jul  3 09:59:40 2024 ] 	Batch(3600/7879) done. Loss: 0.0170  lr:0.010000
[ Wed Jul  3 09:59:58 2024 ] 	Batch(3700/7879) done. Loss: 0.1662  lr:0.010000
[ Wed Jul  3 10:00:17 2024 ] 	Batch(3800/7879) done. Loss: 0.0740  lr:0.010000
[ Wed Jul  3 10:00:35 2024 ] 	Batch(3900/7879) done. Loss: 0.2594  lr:0.010000
[ Wed Jul  3 10:00:53 2024 ] 
Training: Epoch [31/120], Step [3999], Loss: 0.21426260471343994, Training Accuracy: 86.05000000000001
[ Wed Jul  3 10:00:54 2024 ] 	Batch(4000/7879) done. Loss: 0.5889  lr:0.010000
[ Wed Jul  3 10:01:12 2024 ] 	Batch(4100/7879) done. Loss: 0.1948  lr:0.010000
[ Wed Jul  3 10:01:29 2024 ] 	Batch(4200/7879) done. Loss: 0.0201  lr:0.010000
[ Wed Jul  3 10:01:47 2024 ] 	Batch(4300/7879) done. Loss: 0.4157  lr:0.010000
[ Wed Jul  3 10:02:05 2024 ] 	Batch(4400/7879) done. Loss: 0.5515  lr:0.010000
[ Wed Jul  3 10:02:23 2024 ] 
Training: Epoch [31/120], Step [4499], Loss: 0.09039431065320969, Training Accuracy: 85.94166666666668
[ Wed Jul  3 10:02:23 2024 ] 	Batch(4500/7879) done. Loss: 0.1252  lr:0.010000
[ Wed Jul  3 10:02:41 2024 ] 	Batch(4600/7879) done. Loss: 0.0444  lr:0.010000
[ Wed Jul  3 10:02:59 2024 ] 	Batch(4700/7879) done. Loss: 0.8324  lr:0.010000
[ Wed Jul  3 10:03:17 2024 ] 	Batch(4800/7879) done. Loss: 0.2782  lr:0.010000
[ Wed Jul  3 10:03:35 2024 ] 	Batch(4900/7879) done. Loss: 0.8432  lr:0.010000
[ Wed Jul  3 10:03:54 2024 ] 
Training: Epoch [31/120], Step [4999], Loss: 1.2427088022232056, Training Accuracy: 85.82499999999999
[ Wed Jul  3 10:03:54 2024 ] 	Batch(5000/7879) done. Loss: 0.1593  lr:0.010000
[ Wed Jul  3 10:04:12 2024 ] 	Batch(5100/7879) done. Loss: 0.2762  lr:0.010000
[ Wed Jul  3 10:04:31 2024 ] 	Batch(5200/7879) done. Loss: 0.9757  lr:0.010000
[ Wed Jul  3 10:04:50 2024 ] 	Batch(5300/7879) done. Loss: 0.1040  lr:0.010000
[ Wed Jul  3 10:05:08 2024 ] 	Batch(5400/7879) done. Loss: 0.4719  lr:0.010000
[ Wed Jul  3 10:05:26 2024 ] 
Training: Epoch [31/120], Step [5499], Loss: 0.07214212417602539, Training Accuracy: 85.78636363636363
[ Wed Jul  3 10:05:27 2024 ] 	Batch(5500/7879) done. Loss: 0.5299  lr:0.010000
[ Wed Jul  3 10:05:45 2024 ] 	Batch(5600/7879) done. Loss: 0.0208  lr:0.010000
[ Wed Jul  3 10:06:04 2024 ] 	Batch(5700/7879) done. Loss: 0.0981  lr:0.010000
[ Wed Jul  3 10:06:22 2024 ] 	Batch(5800/7879) done. Loss: 0.7443  lr:0.010000
[ Wed Jul  3 10:06:40 2024 ] 	Batch(5900/7879) done. Loss: 0.2916  lr:0.010000
[ Wed Jul  3 10:06:58 2024 ] 
Training: Epoch [31/120], Step [5999], Loss: 0.4545521140098572, Training Accuracy: 85.7
[ Wed Jul  3 10:06:58 2024 ] 	Batch(6000/7879) done. Loss: 0.4955  lr:0.010000
[ Wed Jul  3 10:07:16 2024 ] 	Batch(6100/7879) done. Loss: 0.1852  lr:0.010000
[ Wed Jul  3 10:07:34 2024 ] 	Batch(6200/7879) done. Loss: 0.2907  lr:0.010000
[ Wed Jul  3 10:07:52 2024 ] 	Batch(6300/7879) done. Loss: 0.0269  lr:0.010000
[ Wed Jul  3 10:08:10 2024 ] 	Batch(6400/7879) done. Loss: 0.5449  lr:0.010000
[ Wed Jul  3 10:08:27 2024 ] 
Training: Epoch [31/120], Step [6499], Loss: 0.25569814443588257, Training Accuracy: 85.61538461538461
[ Wed Jul  3 10:08:27 2024 ] 	Batch(6500/7879) done. Loss: 0.9641  lr:0.010000
[ Wed Jul  3 10:08:45 2024 ] 	Batch(6600/7879) done. Loss: 0.4108  lr:0.010000
[ Wed Jul  3 10:09:03 2024 ] 	Batch(6700/7879) done. Loss: 0.3791  lr:0.010000
[ Wed Jul  3 10:09:21 2024 ] 	Batch(6800/7879) done. Loss: 0.7832  lr:0.010000
[ Wed Jul  3 10:09:39 2024 ] 	Batch(6900/7879) done. Loss: 0.5607  lr:0.010000
[ Wed Jul  3 10:09:57 2024 ] 
Training: Epoch [31/120], Step [6999], Loss: 0.025779858231544495, Training Accuracy: 85.61071428571428
[ Wed Jul  3 10:09:57 2024 ] 	Batch(7000/7879) done. Loss: 1.1771  lr:0.010000
[ Wed Jul  3 10:10:15 2024 ] 	Batch(7100/7879) done. Loss: 0.2898  lr:0.010000
[ Wed Jul  3 10:10:33 2024 ] 	Batch(7200/7879) done. Loss: 0.3667  lr:0.010000
[ Wed Jul  3 10:10:51 2024 ] 	Batch(7300/7879) done. Loss: 0.0511  lr:0.010000
[ Wed Jul  3 10:11:09 2024 ] 	Batch(7400/7879) done. Loss: 0.6880  lr:0.010000
[ Wed Jul  3 10:11:26 2024 ] 
Training: Epoch [31/120], Step [7499], Loss: 0.7443627715110779, Training Accuracy: 85.57666666666667
[ Wed Jul  3 10:11:26 2024 ] 	Batch(7500/7879) done. Loss: 0.1444  lr:0.010000
[ Wed Jul  3 10:11:44 2024 ] 	Batch(7600/7879) done. Loss: 0.4093  lr:0.010000
[ Wed Jul  3 10:12:03 2024 ] 	Batch(7700/7879) done. Loss: 0.3092  lr:0.010000
[ Wed Jul  3 10:12:21 2024 ] 	Batch(7800/7879) done. Loss: 0.6811  lr:0.010000
[ Wed Jul  3 10:12:35 2024 ] 	Mean training loss: 0.4684.
[ Wed Jul  3 10:12:35 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 10:12:35 2024 ] Training epoch: 33
[ Wed Jul  3 10:12:36 2024 ] 	Batch(0/7879) done. Loss: 0.0392  lr:0.010000
[ Wed Jul  3 10:12:54 2024 ] 	Batch(100/7879) done. Loss: 0.1107  lr:0.010000
[ Wed Jul  3 10:13:11 2024 ] 	Batch(200/7879) done. Loss: 0.5679  lr:0.010000
[ Wed Jul  3 10:13:29 2024 ] 	Batch(300/7879) done. Loss: 0.0458  lr:0.010000
[ Wed Jul  3 10:13:47 2024 ] 	Batch(400/7879) done. Loss: 0.6396  lr:0.010000
[ Wed Jul  3 10:14:05 2024 ] 
Training: Epoch [32/120], Step [499], Loss: 0.24605298042297363, Training Accuracy: 86.275
[ Wed Jul  3 10:14:05 2024 ] 	Batch(500/7879) done. Loss: 0.1564  lr:0.010000
[ Wed Jul  3 10:14:23 2024 ] 	Batch(600/7879) done. Loss: 0.3891  lr:0.010000
[ Wed Jul  3 10:14:41 2024 ] 	Batch(700/7879) done. Loss: 0.2678  lr:0.010000
[ Wed Jul  3 10:14:59 2024 ] 	Batch(800/7879) done. Loss: 0.1040  lr:0.010000
[ Wed Jul  3 10:15:17 2024 ] 	Batch(900/7879) done. Loss: 0.5330  lr:0.010000
[ Wed Jul  3 10:15:36 2024 ] 
Training: Epoch [32/120], Step [999], Loss: 0.11373386532068253, Training Accuracy: 86.6
[ Wed Jul  3 10:15:36 2024 ] 	Batch(1000/7879) done. Loss: 0.5379  lr:0.010000
[ Wed Jul  3 10:15:54 2024 ] 	Batch(1100/7879) done. Loss: 0.9391  lr:0.010000
[ Wed Jul  3 10:16:13 2024 ] 	Batch(1200/7879) done. Loss: 0.3889  lr:0.010000
[ Wed Jul  3 10:16:31 2024 ] 	Batch(1300/7879) done. Loss: 0.3068  lr:0.010000
[ Wed Jul  3 10:16:49 2024 ] 	Batch(1400/7879) done. Loss: 0.9249  lr:0.010000
[ Wed Jul  3 10:17:06 2024 ] 
Training: Epoch [32/120], Step [1499], Loss: 0.9898686408996582, Training Accuracy: 86.44166666666668
[ Wed Jul  3 10:17:06 2024 ] 	Batch(1500/7879) done. Loss: 0.6517  lr:0.010000
[ Wed Jul  3 10:17:25 2024 ] 	Batch(1600/7879) done. Loss: 0.3205  lr:0.010000
[ Wed Jul  3 10:17:42 2024 ] 	Batch(1700/7879) done. Loss: 0.1689  lr:0.010000
[ Wed Jul  3 10:18:00 2024 ] 	Batch(1800/7879) done. Loss: 0.2076  lr:0.010000
[ Wed Jul  3 10:18:18 2024 ] 	Batch(1900/7879) done. Loss: 0.1097  lr:0.010000
[ Wed Jul  3 10:18:36 2024 ] 
Training: Epoch [32/120], Step [1999], Loss: 0.3057752847671509, Training Accuracy: 86.2375
[ Wed Jul  3 10:18:36 2024 ] 	Batch(2000/7879) done. Loss: 0.4742  lr:0.010000
[ Wed Jul  3 10:18:55 2024 ] 	Batch(2100/7879) done. Loss: 0.8649  lr:0.010000
[ Wed Jul  3 10:19:14 2024 ] 	Batch(2200/7879) done. Loss: 0.4562  lr:0.010000
[ Wed Jul  3 10:19:33 2024 ] 	Batch(2300/7879) done. Loss: 0.7995  lr:0.010000
[ Wed Jul  3 10:19:51 2024 ] 	Batch(2400/7879) done. Loss: 0.1132  lr:0.010000
[ Wed Jul  3 10:20:10 2024 ] 
Training: Epoch [32/120], Step [2499], Loss: 0.5595847368240356, Training Accuracy: 85.985
[ Wed Jul  3 10:20:10 2024 ] 	Batch(2500/7879) done. Loss: 0.0163  lr:0.010000
[ Wed Jul  3 10:20:28 2024 ] 	Batch(2600/7879) done. Loss: 0.7621  lr:0.010000
[ Wed Jul  3 10:20:47 2024 ] 	Batch(2700/7879) done. Loss: 0.2501  lr:0.010000
[ Wed Jul  3 10:21:06 2024 ] 	Batch(2800/7879) done. Loss: 0.6623  lr:0.010000
[ Wed Jul  3 10:21:24 2024 ] 	Batch(2900/7879) done. Loss: 0.1811  lr:0.010000
[ Wed Jul  3 10:21:43 2024 ] 
Training: Epoch [32/120], Step [2999], Loss: 0.0076588173396885395, Training Accuracy: 85.9875
[ Wed Jul  3 10:21:43 2024 ] 	Batch(3000/7879) done. Loss: 0.0334  lr:0.010000
[ Wed Jul  3 10:22:01 2024 ] 	Batch(3100/7879) done. Loss: 0.7982  lr:0.010000
[ Wed Jul  3 10:22:19 2024 ] 	Batch(3200/7879) done. Loss: 0.5797  lr:0.010000
[ Wed Jul  3 10:22:37 2024 ] 	Batch(3300/7879) done. Loss: 0.5255  lr:0.010000
[ Wed Jul  3 10:22:55 2024 ] 	Batch(3400/7879) done. Loss: 1.4239  lr:0.010000
[ Wed Jul  3 10:23:13 2024 ] 
Training: Epoch [32/120], Step [3499], Loss: 0.6964793801307678, Training Accuracy: 86.08571428571429
[ Wed Jul  3 10:23:13 2024 ] 	Batch(3500/7879) done. Loss: 0.3661  lr:0.010000
[ Wed Jul  3 10:23:31 2024 ] 	Batch(3600/7879) done. Loss: 0.5236  lr:0.010000
[ Wed Jul  3 10:23:49 2024 ] 	Batch(3700/7879) done. Loss: 0.1671  lr:0.010000
[ Wed Jul  3 10:24:07 2024 ] 	Batch(3800/7879) done. Loss: 0.2136  lr:0.010000
[ Wed Jul  3 10:24:25 2024 ] 	Batch(3900/7879) done. Loss: 0.0552  lr:0.010000
[ Wed Jul  3 10:24:43 2024 ] 
Training: Epoch [32/120], Step [3999], Loss: 1.2908352613449097, Training Accuracy: 86.09375
[ Wed Jul  3 10:24:43 2024 ] 	Batch(4000/7879) done. Loss: 0.5334  lr:0.010000
[ Wed Jul  3 10:25:01 2024 ] 	Batch(4100/7879) done. Loss: 0.0495  lr:0.010000
[ Wed Jul  3 10:25:19 2024 ] 	Batch(4200/7879) done. Loss: 0.5293  lr:0.010000
[ Wed Jul  3 10:25:37 2024 ] 	Batch(4300/7879) done. Loss: 0.1004  lr:0.010000
[ Wed Jul  3 10:25:55 2024 ] 	Batch(4400/7879) done. Loss: 0.3403  lr:0.010000
[ Wed Jul  3 10:26:13 2024 ] 
Training: Epoch [32/120], Step [4499], Loss: 0.07627201825380325, Training Accuracy: 85.96388888888889
[ Wed Jul  3 10:26:13 2024 ] 	Batch(4500/7879) done. Loss: 0.7119  lr:0.010000
[ Wed Jul  3 10:26:31 2024 ] 	Batch(4600/7879) done. Loss: 0.1116  lr:0.010000
[ Wed Jul  3 10:26:48 2024 ] 	Batch(4700/7879) done. Loss: 0.0638  lr:0.010000
[ Wed Jul  3 10:27:07 2024 ] 	Batch(4800/7879) done. Loss: 0.3723  lr:0.010000
[ Wed Jul  3 10:27:25 2024 ] 	Batch(4900/7879) done. Loss: 0.5261  lr:0.010000
[ Wed Jul  3 10:27:43 2024 ] 
Training: Epoch [32/120], Step [4999], Loss: 0.07725397497415543, Training Accuracy: 85.895
[ Wed Jul  3 10:27:43 2024 ] 	Batch(5000/7879) done. Loss: 0.2988  lr:0.010000
[ Wed Jul  3 10:28:01 2024 ] 	Batch(5100/7879) done. Loss: 0.4277  lr:0.010000
[ Wed Jul  3 10:28:19 2024 ] 	Batch(5200/7879) done. Loss: 0.6907  lr:0.010000
[ Wed Jul  3 10:28:37 2024 ] 	Batch(5300/7879) done. Loss: 0.8756  lr:0.010000
[ Wed Jul  3 10:28:55 2024 ] 	Batch(5400/7879) done. Loss: 0.2499  lr:0.010000
[ Wed Jul  3 10:29:13 2024 ] 
Training: Epoch [32/120], Step [5499], Loss: 0.751572847366333, Training Accuracy: 85.90681818181818
[ Wed Jul  3 10:29:13 2024 ] 	Batch(5500/7879) done. Loss: 0.0589  lr:0.010000
[ Wed Jul  3 10:29:31 2024 ] 	Batch(5600/7879) done. Loss: 0.4369  lr:0.010000
[ Wed Jul  3 10:29:49 2024 ] 	Batch(5700/7879) done. Loss: 0.5349  lr:0.010000
[ Wed Jul  3 10:30:07 2024 ] 	Batch(5800/7879) done. Loss: 0.8637  lr:0.010000
[ Wed Jul  3 10:30:24 2024 ] 	Batch(5900/7879) done. Loss: 0.3076  lr:0.010000
[ Wed Jul  3 10:30:42 2024 ] 
Training: Epoch [32/120], Step [5999], Loss: 0.6266918182373047, Training Accuracy: 85.85833333333333
[ Wed Jul  3 10:30:42 2024 ] 	Batch(6000/7879) done. Loss: 0.6704  lr:0.010000
[ Wed Jul  3 10:31:00 2024 ] 	Batch(6100/7879) done. Loss: 0.3895  lr:0.010000
[ Wed Jul  3 10:31:18 2024 ] 	Batch(6200/7879) done. Loss: 0.0952  lr:0.010000
[ Wed Jul  3 10:31:36 2024 ] 	Batch(6300/7879) done. Loss: 0.0341  lr:0.010000
[ Wed Jul  3 10:31:54 2024 ] 	Batch(6400/7879) done. Loss: 0.1035  lr:0.010000
[ Wed Jul  3 10:32:12 2024 ] 
Training: Epoch [32/120], Step [6499], Loss: 0.012003451585769653, Training Accuracy: 85.91538461538461
[ Wed Jul  3 10:32:13 2024 ] 	Batch(6500/7879) done. Loss: 0.1693  lr:0.010000
[ Wed Jul  3 10:32:31 2024 ] 	Batch(6600/7879) done. Loss: 0.4807  lr:0.010000
[ Wed Jul  3 10:32:50 2024 ] 	Batch(6700/7879) done. Loss: 0.0678  lr:0.010000
[ Wed Jul  3 10:33:08 2024 ] 	Batch(6800/7879) done. Loss: 0.2604  lr:0.010000
[ Wed Jul  3 10:33:27 2024 ] 	Batch(6900/7879) done. Loss: 0.1756  lr:0.010000
[ Wed Jul  3 10:33:45 2024 ] 
Training: Epoch [32/120], Step [6999], Loss: 0.5897488594055176, Training Accuracy: 85.87857142857143
[ Wed Jul  3 10:33:45 2024 ] 	Batch(7000/7879) done. Loss: 1.0283  lr:0.010000
[ Wed Jul  3 10:34:04 2024 ] 	Batch(7100/7879) done. Loss: 0.1069  lr:0.010000
[ Wed Jul  3 10:34:22 2024 ] 	Batch(7200/7879) done. Loss: 0.1903  lr:0.010000
[ Wed Jul  3 10:34:41 2024 ] 	Batch(7300/7879) done. Loss: 0.9699  lr:0.010000
[ Wed Jul  3 10:34:59 2024 ] 	Batch(7400/7879) done. Loss: 0.3848  lr:0.010000
[ Wed Jul  3 10:35:18 2024 ] 
Training: Epoch [32/120], Step [7499], Loss: 1.0511225461959839, Training Accuracy: 85.84166666666667
[ Wed Jul  3 10:35:18 2024 ] 	Batch(7500/7879) done. Loss: 0.8305  lr:0.010000
[ Wed Jul  3 10:35:36 2024 ] 	Batch(7600/7879) done. Loss: 0.1329  lr:0.010000
[ Wed Jul  3 10:35:54 2024 ] 	Batch(7700/7879) done. Loss: 1.0864  lr:0.010000
[ Wed Jul  3 10:36:12 2024 ] 	Batch(7800/7879) done. Loss: 0.9255  lr:0.010000
[ Wed Jul  3 10:36:26 2024 ] 	Mean training loss: 0.4528.
[ Wed Jul  3 10:36:26 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 10:36:26 2024 ] Training epoch: 34
[ Wed Jul  3 10:36:27 2024 ] 	Batch(0/7879) done. Loss: 0.9138  lr:0.010000
[ Wed Jul  3 10:36:45 2024 ] 	Batch(100/7879) done. Loss: 0.3952  lr:0.010000
[ Wed Jul  3 10:37:04 2024 ] 	Batch(200/7879) done. Loss: 0.2607  lr:0.010000
[ Wed Jul  3 10:37:22 2024 ] 	Batch(300/7879) done. Loss: 0.2422  lr:0.010000
[ Wed Jul  3 10:37:41 2024 ] 	Batch(400/7879) done. Loss: 1.1875  lr:0.010000
[ Wed Jul  3 10:37:59 2024 ] 
Training: Epoch [33/120], Step [499], Loss: 0.5347861051559448, Training Accuracy: 86.55000000000001
[ Wed Jul  3 10:37:59 2024 ] 	Batch(500/7879) done. Loss: 0.5776  lr:0.010000
[ Wed Jul  3 10:38:18 2024 ] 	Batch(600/7879) done. Loss: 0.2932  lr:0.010000
[ Wed Jul  3 10:38:36 2024 ] 	Batch(700/7879) done. Loss: 0.6468  lr:0.010000
[ Wed Jul  3 10:38:55 2024 ] 	Batch(800/7879) done. Loss: 0.0499  lr:0.010000
[ Wed Jul  3 10:39:13 2024 ] 	Batch(900/7879) done. Loss: 0.4529  lr:0.010000
[ Wed Jul  3 10:39:31 2024 ] 
Training: Epoch [33/120], Step [999], Loss: 0.49164697527885437, Training Accuracy: 86.5375
[ Wed Jul  3 10:39:31 2024 ] 	Batch(1000/7879) done. Loss: 0.3479  lr:0.010000
[ Wed Jul  3 10:39:49 2024 ] 	Batch(1100/7879) done. Loss: 0.4173  lr:0.010000
[ Wed Jul  3 10:40:07 2024 ] 	Batch(1200/7879) done. Loss: 0.1571  lr:0.010000
[ Wed Jul  3 10:40:25 2024 ] 	Batch(1300/7879) done. Loss: 0.4237  lr:0.010000
[ Wed Jul  3 10:40:43 2024 ] 	Batch(1400/7879) done. Loss: 0.0391  lr:0.010000
[ Wed Jul  3 10:41:00 2024 ] 
Training: Epoch [33/120], Step [1499], Loss: 0.46766597032546997, Training Accuracy: 86.55833333333334
[ Wed Jul  3 10:41:01 2024 ] 	Batch(1500/7879) done. Loss: 0.4521  lr:0.010000
[ Wed Jul  3 10:41:19 2024 ] 	Batch(1600/7879) done. Loss: 0.2007  lr:0.010000
[ Wed Jul  3 10:41:37 2024 ] 	Batch(1700/7879) done. Loss: 0.4371  lr:0.010000
[ Wed Jul  3 10:41:56 2024 ] 	Batch(1800/7879) done. Loss: 0.1817  lr:0.010000
[ Wed Jul  3 10:42:14 2024 ] 	Batch(1900/7879) done. Loss: 0.1076  lr:0.010000
[ Wed Jul  3 10:42:32 2024 ] 
Training: Epoch [33/120], Step [1999], Loss: 0.3175975978374481, Training Accuracy: 86.55625
[ Wed Jul  3 10:42:33 2024 ] 	Batch(2000/7879) done. Loss: 1.1386  lr:0.010000
[ Wed Jul  3 10:42:51 2024 ] 	Batch(2100/7879) done. Loss: 0.2739  lr:0.010000
[ Wed Jul  3 10:43:10 2024 ] 	Batch(2200/7879) done. Loss: 0.5895  lr:0.010000
[ Wed Jul  3 10:43:29 2024 ] 	Batch(2300/7879) done. Loss: 0.4517  lr:0.010000
[ Wed Jul  3 10:43:47 2024 ] 	Batch(2400/7879) done. Loss: 0.1344  lr:0.010000
[ Wed Jul  3 10:44:06 2024 ] 
Training: Epoch [33/120], Step [2499], Loss: 0.6574828028678894, Training Accuracy: 86.52499999999999
[ Wed Jul  3 10:44:06 2024 ] 	Batch(2500/7879) done. Loss: 0.3750  lr:0.010000
[ Wed Jul  3 10:44:24 2024 ] 	Batch(2600/7879) done. Loss: 0.1443  lr:0.010000
[ Wed Jul  3 10:44:43 2024 ] 	Batch(2700/7879) done. Loss: 0.5478  lr:0.010000
[ Wed Jul  3 10:45:01 2024 ] 	Batch(2800/7879) done. Loss: 0.8288  lr:0.010000
[ Wed Jul  3 10:45:20 2024 ] 	Batch(2900/7879) done. Loss: 0.5185  lr:0.010000
[ Wed Jul  3 10:45:38 2024 ] 
Training: Epoch [33/120], Step [2999], Loss: 0.46570664644241333, Training Accuracy: 86.45416666666667
[ Wed Jul  3 10:45:38 2024 ] 	Batch(3000/7879) done. Loss: 0.6251  lr:0.010000
[ Wed Jul  3 10:45:57 2024 ] 	Batch(3100/7879) done. Loss: 0.5539  lr:0.010000
[ Wed Jul  3 10:46:16 2024 ] 	Batch(3200/7879) done. Loss: 0.2209  lr:0.010000
[ Wed Jul  3 10:46:34 2024 ] 	Batch(3300/7879) done. Loss: 0.4210  lr:0.010000
[ Wed Jul  3 10:46:53 2024 ] 	Batch(3400/7879) done. Loss: 0.4771  lr:0.010000
[ Wed Jul  3 10:47:11 2024 ] 
Training: Epoch [33/120], Step [3499], Loss: 0.12785477936267853, Training Accuracy: 86.34285714285714
[ Wed Jul  3 10:47:11 2024 ] 	Batch(3500/7879) done. Loss: 0.7020  lr:0.010000
[ Wed Jul  3 10:47:30 2024 ] 	Batch(3600/7879) done. Loss: 0.5609  lr:0.010000
[ Wed Jul  3 10:47:48 2024 ] 	Batch(3700/7879) done. Loss: 0.7258  lr:0.010000
[ Wed Jul  3 10:48:07 2024 ] 	Batch(3800/7879) done. Loss: 0.1936  lr:0.010000
[ Wed Jul  3 10:48:25 2024 ] 	Batch(3900/7879) done. Loss: 0.1222  lr:0.010000
[ Wed Jul  3 10:48:42 2024 ] 
Training: Epoch [33/120], Step [3999], Loss: 1.0859249830245972, Training Accuracy: 86.1875
[ Wed Jul  3 10:48:43 2024 ] 	Batch(4000/7879) done. Loss: 1.1182  lr:0.010000
[ Wed Jul  3 10:49:01 2024 ] 	Batch(4100/7879) done. Loss: 0.4279  lr:0.010000
[ Wed Jul  3 10:49:18 2024 ] 	Batch(4200/7879) done. Loss: 0.8088  lr:0.010000
[ Wed Jul  3 10:49:36 2024 ] 	Batch(4300/7879) done. Loss: 0.1409  lr:0.010000
[ Wed Jul  3 10:49:54 2024 ] 	Batch(4400/7879) done. Loss: 0.0845  lr:0.010000
[ Wed Jul  3 10:50:12 2024 ] 
Training: Epoch [33/120], Step [4499], Loss: 0.010624006390571594, Training Accuracy: 86.03888888888889
[ Wed Jul  3 10:50:12 2024 ] 	Batch(4500/7879) done. Loss: 0.2676  lr:0.010000
[ Wed Jul  3 10:50:30 2024 ] 	Batch(4600/7879) done. Loss: 0.1869  lr:0.010000
[ Wed Jul  3 10:50:48 2024 ] 	Batch(4700/7879) done. Loss: 0.6495  lr:0.010000
[ Wed Jul  3 10:51:06 2024 ] 	Batch(4800/7879) done. Loss: 0.4811  lr:0.010000
[ Wed Jul  3 10:51:24 2024 ] 	Batch(4900/7879) done. Loss: 0.4499  lr:0.010000
[ Wed Jul  3 10:51:43 2024 ] 
Training: Epoch [33/120], Step [4999], Loss: 0.5182872414588928, Training Accuracy: 86.0325
[ Wed Jul  3 10:51:43 2024 ] 	Batch(5000/7879) done. Loss: 0.7943  lr:0.010000
[ Wed Jul  3 10:52:01 2024 ] 	Batch(5100/7879) done. Loss: 0.0230  lr:0.010000
[ Wed Jul  3 10:52:20 2024 ] 	Batch(5200/7879) done. Loss: 0.2402  lr:0.010000
[ Wed Jul  3 10:52:39 2024 ] 	Batch(5300/7879) done. Loss: 0.3818  lr:0.010000
[ Wed Jul  3 10:52:57 2024 ] 	Batch(5400/7879) done. Loss: 0.1186  lr:0.010000
[ Wed Jul  3 10:53:15 2024 ] 
Training: Epoch [33/120], Step [5499], Loss: 0.3861371874809265, Training Accuracy: 86.02045454545456
[ Wed Jul  3 10:53:16 2024 ] 	Batch(5500/7879) done. Loss: 0.1607  lr:0.010000
[ Wed Jul  3 10:53:34 2024 ] 	Batch(5600/7879) done. Loss: 0.2421  lr:0.010000
[ Wed Jul  3 10:53:52 2024 ] 	Batch(5700/7879) done. Loss: 0.5586  lr:0.010000
[ Wed Jul  3 10:54:10 2024 ] 	Batch(5800/7879) done. Loss: 1.2792  lr:0.010000
[ Wed Jul  3 10:54:28 2024 ] 	Batch(5900/7879) done. Loss: 1.0170  lr:0.010000
[ Wed Jul  3 10:54:45 2024 ] 
Training: Epoch [33/120], Step [5999], Loss: 0.11887966841459274, Training Accuracy: 85.96875
[ Wed Jul  3 10:54:46 2024 ] 	Batch(6000/7879) done. Loss: 0.2263  lr:0.010000
[ Wed Jul  3 10:55:04 2024 ] 	Batch(6100/7879) done. Loss: 0.4960  lr:0.010000
[ Wed Jul  3 10:55:21 2024 ] 	Batch(6200/7879) done. Loss: 0.3842  lr:0.010000
[ Wed Jul  3 10:55:39 2024 ] 	Batch(6300/7879) done. Loss: 0.6275  lr:0.010000
[ Wed Jul  3 10:55:57 2024 ] 	Batch(6400/7879) done. Loss: 0.6684  lr:0.010000
[ Wed Jul  3 10:56:15 2024 ] 
Training: Epoch [33/120], Step [6499], Loss: 0.17300276458263397, Training Accuracy: 85.93076923076923
[ Wed Jul  3 10:56:15 2024 ] 	Batch(6500/7879) done. Loss: 0.3041  lr:0.010000
[ Wed Jul  3 10:56:33 2024 ] 	Batch(6600/7879) done. Loss: 1.3437  lr:0.010000
[ Wed Jul  3 10:56:51 2024 ] 	Batch(6700/7879) done. Loss: 0.0962  lr:0.010000
[ Wed Jul  3 10:57:09 2024 ] 	Batch(6800/7879) done. Loss: 0.6404  lr:0.010000
[ Wed Jul  3 10:57:27 2024 ] 	Batch(6900/7879) done. Loss: 0.7541  lr:0.010000
[ Wed Jul  3 10:57:45 2024 ] 
Training: Epoch [33/120], Step [6999], Loss: 0.2978019118309021, Training Accuracy: 85.91964285714285
[ Wed Jul  3 10:57:45 2024 ] 	Batch(7000/7879) done. Loss: 0.9600  lr:0.010000
[ Wed Jul  3 10:58:03 2024 ] 	Batch(7100/7879) done. Loss: 0.3372  lr:0.010000
[ Wed Jul  3 10:58:21 2024 ] 	Batch(7200/7879) done. Loss: 0.3744  lr:0.010000
[ Wed Jul  3 10:58:39 2024 ] 	Batch(7300/7879) done. Loss: 0.6307  lr:0.010000
[ Wed Jul  3 10:58:58 2024 ] 	Batch(7400/7879) done. Loss: 0.2195  lr:0.010000
[ Wed Jul  3 10:59:16 2024 ] 
Training: Epoch [33/120], Step [7499], Loss: 0.4385504126548767, Training Accuracy: 85.94000000000001
[ Wed Jul  3 10:59:16 2024 ] 	Batch(7500/7879) done. Loss: 0.2921  lr:0.010000
[ Wed Jul  3 10:59:35 2024 ] 	Batch(7600/7879) done. Loss: 0.4699  lr:0.010000
[ Wed Jul  3 10:59:53 2024 ] 	Batch(7700/7879) done. Loss: 0.0358  lr:0.010000
[ Wed Jul  3 11:00:12 2024 ] 	Batch(7800/7879) done. Loss: 0.5108  lr:0.010000
[ Wed Jul  3 11:00:26 2024 ] 	Mean training loss: 0.4591.
[ Wed Jul  3 11:00:26 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 11:00:26 2024 ] Training epoch: 35
[ Wed Jul  3 11:00:27 2024 ] 	Batch(0/7879) done. Loss: 1.0206  lr:0.010000
[ Wed Jul  3 11:00:45 2024 ] 	Batch(100/7879) done. Loss: 0.1374  lr:0.010000
[ Wed Jul  3 11:01:03 2024 ] 	Batch(200/7879) done. Loss: 0.3746  lr:0.010000
[ Wed Jul  3 11:01:22 2024 ] 	Batch(300/7879) done. Loss: 0.6265  lr:0.010000
[ Wed Jul  3 11:01:40 2024 ] 	Batch(400/7879) done. Loss: 0.3169  lr:0.010000
[ Wed Jul  3 11:01:58 2024 ] 
Training: Epoch [34/120], Step [499], Loss: 0.12787994742393494, Training Accuracy: 86.925
[ Wed Jul  3 11:01:58 2024 ] 	Batch(500/7879) done. Loss: 0.6205  lr:0.010000
[ Wed Jul  3 11:02:17 2024 ] 	Batch(600/7879) done. Loss: 0.0347  lr:0.010000
[ Wed Jul  3 11:02:35 2024 ] 	Batch(700/7879) done. Loss: 0.5973  lr:0.010000
[ Wed Jul  3 11:02:53 2024 ] 	Batch(800/7879) done. Loss: 0.5798  lr:0.010000
[ Wed Jul  3 11:03:12 2024 ] 	Batch(900/7879) done. Loss: 0.1287  lr:0.010000
[ Wed Jul  3 11:03:30 2024 ] 
Training: Epoch [34/120], Step [999], Loss: 0.5355578660964966, Training Accuracy: 87.3
[ Wed Jul  3 11:03:30 2024 ] 	Batch(1000/7879) done. Loss: 0.1624  lr:0.010000
[ Wed Jul  3 11:03:48 2024 ] 	Batch(1100/7879) done. Loss: 0.2600  lr:0.010000
[ Wed Jul  3 11:04:07 2024 ] 	Batch(1200/7879) done. Loss: 1.0949  lr:0.010000
[ Wed Jul  3 11:04:25 2024 ] 	Batch(1300/7879) done. Loss: 0.5520  lr:0.010000
[ Wed Jul  3 11:04:44 2024 ] 	Batch(1400/7879) done. Loss: 0.1861  lr:0.010000
[ Wed Jul  3 11:05:02 2024 ] 
Training: Epoch [34/120], Step [1499], Loss: 0.42317822575569153, Training Accuracy: 86.98333333333333
[ Wed Jul  3 11:05:02 2024 ] 	Batch(1500/7879) done. Loss: 0.2864  lr:0.010000
[ Wed Jul  3 11:05:21 2024 ] 	Batch(1600/7879) done. Loss: 0.6418  lr:0.010000
[ Wed Jul  3 11:05:40 2024 ] 	Batch(1700/7879) done. Loss: 0.3114  lr:0.010000
[ Wed Jul  3 11:05:58 2024 ] 	Batch(1800/7879) done. Loss: 0.6704  lr:0.010000
[ Wed Jul  3 11:06:17 2024 ] 	Batch(1900/7879) done. Loss: 0.9836  lr:0.010000
[ Wed Jul  3 11:06:35 2024 ] 
Training: Epoch [34/120], Step [1999], Loss: 0.23421531915664673, Training Accuracy: 86.56875000000001
[ Wed Jul  3 11:06:35 2024 ] 	Batch(2000/7879) done. Loss: 0.0871  lr:0.010000
[ Wed Jul  3 11:06:54 2024 ] 	Batch(2100/7879) done. Loss: 0.2548  lr:0.010000
[ Wed Jul  3 11:07:12 2024 ] 	Batch(2200/7879) done. Loss: 0.0500  lr:0.010000
[ Wed Jul  3 11:07:31 2024 ] 	Batch(2300/7879) done. Loss: 0.1476  lr:0.010000
[ Wed Jul  3 11:07:49 2024 ] 	Batch(2400/7879) done. Loss: 0.4418  lr:0.010000
[ Wed Jul  3 11:08:08 2024 ] 
Training: Epoch [34/120], Step [2499], Loss: 0.04227781295776367, Training Accuracy: 86.565
[ Wed Jul  3 11:08:08 2024 ] 	Batch(2500/7879) done. Loss: 0.0553  lr:0.010000
[ Wed Jul  3 11:08:27 2024 ] 	Batch(2600/7879) done. Loss: 0.2527  lr:0.010000
[ Wed Jul  3 11:08:45 2024 ] 	Batch(2700/7879) done. Loss: 0.6036  lr:0.010000
[ Wed Jul  3 11:09:04 2024 ] 	Batch(2800/7879) done. Loss: 0.0646  lr:0.010000
[ Wed Jul  3 11:09:22 2024 ] 	Batch(2900/7879) done. Loss: 0.0876  lr:0.010000
[ Wed Jul  3 11:09:39 2024 ] 
Training: Epoch [34/120], Step [2999], Loss: 0.057558573782444, Training Accuracy: 86.56666666666666
[ Wed Jul  3 11:09:39 2024 ] 	Batch(3000/7879) done. Loss: 0.2760  lr:0.010000
[ Wed Jul  3 11:09:57 2024 ] 	Batch(3100/7879) done. Loss: 0.0641  lr:0.010000
[ Wed Jul  3 11:10:15 2024 ] 	Batch(3200/7879) done. Loss: 0.3164  lr:0.010000
[ Wed Jul  3 11:10:33 2024 ] 	Batch(3300/7879) done. Loss: 0.2785  lr:0.010000
[ Wed Jul  3 11:10:51 2024 ] 	Batch(3400/7879) done. Loss: 1.3256  lr:0.010000
[ Wed Jul  3 11:11:09 2024 ] 
Training: Epoch [34/120], Step [3499], Loss: 0.5378943681716919, Training Accuracy: 86.4
[ Wed Jul  3 11:11:09 2024 ] 	Batch(3500/7879) done. Loss: 0.3531  lr:0.010000
[ Wed Jul  3 11:11:27 2024 ] 	Batch(3600/7879) done. Loss: 0.4263  lr:0.010000
[ Wed Jul  3 11:11:46 2024 ] 	Batch(3700/7879) done. Loss: 0.2798  lr:0.010000
[ Wed Jul  3 11:12:04 2024 ] 	Batch(3800/7879) done. Loss: 0.3682  lr:0.010000
[ Wed Jul  3 11:12:23 2024 ] 	Batch(3900/7879) done. Loss: 0.7447  lr:0.010000
[ Wed Jul  3 11:12:41 2024 ] 
Training: Epoch [34/120], Step [3999], Loss: 0.14678926765918732, Training Accuracy: 86.375
[ Wed Jul  3 11:12:42 2024 ] 	Batch(4000/7879) done. Loss: 0.0493  lr:0.010000
[ Wed Jul  3 11:13:00 2024 ] 	Batch(4100/7879) done. Loss: 0.3633  lr:0.010000
[ Wed Jul  3 11:13:19 2024 ] 	Batch(4200/7879) done. Loss: 1.1929  lr:0.010000
[ Wed Jul  3 11:13:36 2024 ] 	Batch(4300/7879) done. Loss: 0.1572  lr:0.010000
[ Wed Jul  3 11:13:54 2024 ] 	Batch(4400/7879) done. Loss: 0.3289  lr:0.010000
[ Wed Jul  3 11:14:12 2024 ] 
Training: Epoch [34/120], Step [4499], Loss: 0.3355662524700165, Training Accuracy: 86.33611111111111
[ Wed Jul  3 11:14:12 2024 ] 	Batch(4500/7879) done. Loss: 0.2475  lr:0.010000
[ Wed Jul  3 11:14:30 2024 ] 	Batch(4600/7879) done. Loss: 0.5509  lr:0.010000
[ Wed Jul  3 11:14:48 2024 ] 	Batch(4700/7879) done. Loss: 0.7526  lr:0.010000
[ Wed Jul  3 11:15:06 2024 ] 	Batch(4800/7879) done. Loss: 1.2115  lr:0.010000
[ Wed Jul  3 11:15:24 2024 ] 	Batch(4900/7879) done. Loss: 0.4334  lr:0.010000
[ Wed Jul  3 11:15:42 2024 ] 
Training: Epoch [34/120], Step [4999], Loss: 1.8867003917694092, Training Accuracy: 86.33
[ Wed Jul  3 11:15:43 2024 ] 	Batch(5000/7879) done. Loss: 0.4661  lr:0.010000
[ Wed Jul  3 11:16:01 2024 ] 	Batch(5100/7879) done. Loss: 2.3576  lr:0.010000
[ Wed Jul  3 11:16:19 2024 ] 	Batch(5200/7879) done. Loss: 0.6157  lr:0.010000
[ Wed Jul  3 11:16:36 2024 ] 	Batch(5300/7879) done. Loss: 0.3743  lr:0.010000
[ Wed Jul  3 11:16:54 2024 ] 	Batch(5400/7879) done. Loss: 1.9334  lr:0.010000
[ Wed Jul  3 11:17:12 2024 ] 
Training: Epoch [34/120], Step [5499], Loss: 0.3678776025772095, Training Accuracy: 86.2909090909091
[ Wed Jul  3 11:17:12 2024 ] 	Batch(5500/7879) done. Loss: 0.0397  lr:0.010000
[ Wed Jul  3 11:17:30 2024 ] 	Batch(5600/7879) done. Loss: 1.2092  lr:0.010000
[ Wed Jul  3 11:17:49 2024 ] 	Batch(5700/7879) done. Loss: 0.6864  lr:0.010000
[ Wed Jul  3 11:18:08 2024 ] 	Batch(5800/7879) done. Loss: 0.2924  lr:0.010000
[ Wed Jul  3 11:18:26 2024 ] 	Batch(5900/7879) done. Loss: 0.4257  lr:0.010000
[ Wed Jul  3 11:18:44 2024 ] 
Training: Epoch [34/120], Step [5999], Loss: 0.521963357925415, Training Accuracy: 86.22708333333333
[ Wed Jul  3 11:18:44 2024 ] 	Batch(6000/7879) done. Loss: 0.1399  lr:0.010000
[ Wed Jul  3 11:19:02 2024 ] 	Batch(6100/7879) done. Loss: 0.6004  lr:0.010000
[ Wed Jul  3 11:19:20 2024 ] 	Batch(6200/7879) done. Loss: 0.1946  lr:0.010000
[ Wed Jul  3 11:19:38 2024 ] 	Batch(6300/7879) done. Loss: 0.0757  lr:0.010000
[ Wed Jul  3 11:19:56 2024 ] 	Batch(6400/7879) done. Loss: 0.4937  lr:0.010000
[ Wed Jul  3 11:20:14 2024 ] 
Training: Epoch [34/120], Step [6499], Loss: 1.1771119832992554, Training Accuracy: 86.20576923076923
[ Wed Jul  3 11:20:14 2024 ] 	Batch(6500/7879) done. Loss: 0.1534  lr:0.010000
[ Wed Jul  3 11:20:32 2024 ] 	Batch(6600/7879) done. Loss: 0.1477  lr:0.010000
[ Wed Jul  3 11:20:50 2024 ] 	Batch(6700/7879) done. Loss: 0.5971  lr:0.010000
[ Wed Jul  3 11:21:08 2024 ] 	Batch(6800/7879) done. Loss: 0.2476  lr:0.010000
[ Wed Jul  3 11:21:26 2024 ] 	Batch(6900/7879) done. Loss: 0.0305  lr:0.010000
[ Wed Jul  3 11:21:43 2024 ] 
Training: Epoch [34/120], Step [6999], Loss: 0.15754416584968567, Training Accuracy: 86.2625
[ Wed Jul  3 11:21:44 2024 ] 	Batch(7000/7879) done. Loss: 0.5429  lr:0.010000
[ Wed Jul  3 11:22:01 2024 ] 	Batch(7100/7879) done. Loss: 0.4752  lr:0.010000
[ Wed Jul  3 11:22:19 2024 ] 	Batch(7200/7879) done. Loss: 0.2558  lr:0.010000
[ Wed Jul  3 11:22:37 2024 ] 	Batch(7300/7879) done. Loss: 0.2302  lr:0.010000
[ Wed Jul  3 11:22:55 2024 ] 	Batch(7400/7879) done. Loss: 0.9351  lr:0.010000
[ Wed Jul  3 11:23:13 2024 ] 
Training: Epoch [34/120], Step [7499], Loss: 0.3557911217212677, Training Accuracy: 86.30333333333333
[ Wed Jul  3 11:23:13 2024 ] 	Batch(7500/7879) done. Loss: 0.4988  lr:0.010000
[ Wed Jul  3 11:23:31 2024 ] 	Batch(7600/7879) done. Loss: 0.3684  lr:0.010000
[ Wed Jul  3 11:23:49 2024 ] 	Batch(7700/7879) done. Loss: 0.0305  lr:0.010000
[ Wed Jul  3 11:24:07 2024 ] 	Batch(7800/7879) done. Loss: 0.3817  lr:0.010000
[ Wed Jul  3 11:24:21 2024 ] 	Mean training loss: 0.4386.
[ Wed Jul  3 11:24:21 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 11:24:21 2024 ] Training epoch: 36
[ Wed Jul  3 11:24:22 2024 ] 	Batch(0/7879) done. Loss: 0.1409  lr:0.010000
[ Wed Jul  3 11:24:40 2024 ] 	Batch(100/7879) done. Loss: 0.2896  lr:0.010000
[ Wed Jul  3 11:24:58 2024 ] 	Batch(200/7879) done. Loss: 0.1902  lr:0.010000
[ Wed Jul  3 11:25:16 2024 ] 	Batch(300/7879) done. Loss: 1.2464  lr:0.010000
[ Wed Jul  3 11:25:35 2024 ] 	Batch(400/7879) done. Loss: 0.0614  lr:0.010000
[ Wed Jul  3 11:25:53 2024 ] 
Training: Epoch [35/120], Step [499], Loss: 0.9066947102546692, Training Accuracy: 87.275
[ Wed Jul  3 11:25:53 2024 ] 	Batch(500/7879) done. Loss: 0.2220  lr:0.010000
[ Wed Jul  3 11:26:11 2024 ] 	Batch(600/7879) done. Loss: 0.2012  lr:0.010000
[ Wed Jul  3 11:26:30 2024 ] 	Batch(700/7879) done. Loss: 0.7055  lr:0.010000
[ Wed Jul  3 11:26:48 2024 ] 	Batch(800/7879) done. Loss: 0.3225  lr:0.010000
[ Wed Jul  3 11:27:06 2024 ] 	Batch(900/7879) done. Loss: 0.0293  lr:0.010000
[ Wed Jul  3 11:27:24 2024 ] 
Training: Epoch [35/120], Step [999], Loss: 0.4837453365325928, Training Accuracy: 87.375
[ Wed Jul  3 11:27:25 2024 ] 	Batch(1000/7879) done. Loss: 0.6197  lr:0.010000
[ Wed Jul  3 11:27:43 2024 ] 	Batch(1100/7879) done. Loss: 0.0190  lr:0.010000
[ Wed Jul  3 11:28:01 2024 ] 	Batch(1200/7879) done. Loss: 0.2241  lr:0.010000
[ Wed Jul  3 11:28:20 2024 ] 	Batch(1300/7879) done. Loss: 0.5980  lr:0.010000
[ Wed Jul  3 11:28:38 2024 ] 	Batch(1400/7879) done. Loss: 1.1062  lr:0.010000
[ Wed Jul  3 11:28:56 2024 ] 
Training: Epoch [35/120], Step [1499], Loss: 0.08266855031251907, Training Accuracy: 87.35000000000001
[ Wed Jul  3 11:28:57 2024 ] 	Batch(1500/7879) done. Loss: 0.3105  lr:0.010000
[ Wed Jul  3 11:29:15 2024 ] 	Batch(1600/7879) done. Loss: 0.5540  lr:0.010000
[ Wed Jul  3 11:29:34 2024 ] 	Batch(1700/7879) done. Loss: 0.2959  lr:0.010000
[ Wed Jul  3 11:29:52 2024 ] 	Batch(1800/7879) done. Loss: 0.4027  lr:0.010000
[ Wed Jul  3 11:30:11 2024 ] 	Batch(1900/7879) done. Loss: 0.1896  lr:0.010000
[ Wed Jul  3 11:30:29 2024 ] 
Training: Epoch [35/120], Step [1999], Loss: 0.010935455560684204, Training Accuracy: 87.225
[ Wed Jul  3 11:30:29 2024 ] 	Batch(2000/7879) done. Loss: 0.3114  lr:0.010000
[ Wed Jul  3 11:30:48 2024 ] 	Batch(2100/7879) done. Loss: 0.0262  lr:0.010000
[ Wed Jul  3 11:31:06 2024 ] 	Batch(2200/7879) done. Loss: 0.3407  lr:0.010000
[ Wed Jul  3 11:31:24 2024 ] 	Batch(2300/7879) done. Loss: 0.0394  lr:0.010000
[ Wed Jul  3 11:31:43 2024 ] 	Batch(2400/7879) done. Loss: 0.3850  lr:0.010000
[ Wed Jul  3 11:32:01 2024 ] 
Training: Epoch [35/120], Step [2499], Loss: 0.3010014593601227, Training Accuracy: 87.25500000000001
[ Wed Jul  3 11:32:01 2024 ] 	Batch(2500/7879) done. Loss: 0.0720  lr:0.010000
[ Wed Jul  3 11:32:19 2024 ] 	Batch(2600/7879) done. Loss: 0.6681  lr:0.010000
[ Wed Jul  3 11:32:38 2024 ] 	Batch(2700/7879) done. Loss: 1.1355  lr:0.010000
[ Wed Jul  3 11:32:56 2024 ] 	Batch(2800/7879) done. Loss: 0.1479  lr:0.010000
[ Wed Jul  3 11:33:14 2024 ] 	Batch(2900/7879) done. Loss: 0.2468  lr:0.010000
[ Wed Jul  3 11:33:32 2024 ] 
Training: Epoch [35/120], Step [2999], Loss: 0.11793604493141174, Training Accuracy: 86.96666666666667
[ Wed Jul  3 11:33:32 2024 ] 	Batch(3000/7879) done. Loss: 0.1996  lr:0.010000
[ Wed Jul  3 11:33:51 2024 ] 	Batch(3100/7879) done. Loss: 0.3037  lr:0.010000
[ Wed Jul  3 11:34:09 2024 ] 	Batch(3200/7879) done. Loss: 0.7186  lr:0.010000
[ Wed Jul  3 11:34:28 2024 ] 	Batch(3300/7879) done. Loss: 0.9012  lr:0.010000
[ Wed Jul  3 11:34:46 2024 ] 	Batch(3400/7879) done. Loss: 0.8351  lr:0.010000
[ Wed Jul  3 11:35:05 2024 ] 
Training: Epoch [35/120], Step [3499], Loss: 0.5128228068351746, Training Accuracy: 86.85357142857143
[ Wed Jul  3 11:35:05 2024 ] 	Batch(3500/7879) done. Loss: 0.8774  lr:0.010000
[ Wed Jul  3 11:35:23 2024 ] 	Batch(3600/7879) done. Loss: 0.2440  lr:0.010000
[ Wed Jul  3 11:35:41 2024 ] 	Batch(3700/7879) done. Loss: 0.9472  lr:0.010000
[ Wed Jul  3 11:35:59 2024 ] 	Batch(3800/7879) done. Loss: 0.6066  lr:0.010000
[ Wed Jul  3 11:36:17 2024 ] 	Batch(3900/7879) done. Loss: 1.2280  lr:0.010000
[ Wed Jul  3 11:36:35 2024 ] 
Training: Epoch [35/120], Step [3999], Loss: 0.8309835195541382, Training Accuracy: 86.784375
[ Wed Jul  3 11:36:35 2024 ] 	Batch(4000/7879) done. Loss: 0.0647  lr:0.010000
[ Wed Jul  3 11:36:53 2024 ] 	Batch(4100/7879) done. Loss: 0.4929  lr:0.010000
[ Wed Jul  3 11:37:11 2024 ] 	Batch(4200/7879) done. Loss: 0.3533  lr:0.010000
[ Wed Jul  3 11:37:29 2024 ] 	Batch(4300/7879) done. Loss: 0.1280  lr:0.010000
[ Wed Jul  3 11:37:47 2024 ] 	Batch(4400/7879) done. Loss: 0.2073  lr:0.010000
[ Wed Jul  3 11:38:05 2024 ] 
Training: Epoch [35/120], Step [4499], Loss: 0.5699102878570557, Training Accuracy: 86.76666666666667
[ Wed Jul  3 11:38:05 2024 ] 	Batch(4500/7879) done. Loss: 0.0160  lr:0.010000
[ Wed Jul  3 11:38:23 2024 ] 	Batch(4600/7879) done. Loss: 1.0818  lr:0.010000
[ Wed Jul  3 11:38:41 2024 ] 	Batch(4700/7879) done. Loss: 0.1069  lr:0.010000
[ Wed Jul  3 11:38:59 2024 ] 	Batch(4800/7879) done. Loss: 0.6195  lr:0.010000
[ Wed Jul  3 11:39:16 2024 ] 	Batch(4900/7879) done. Loss: 0.3974  lr:0.010000
[ Wed Jul  3 11:39:34 2024 ] 
Training: Epoch [35/120], Step [4999], Loss: 0.1360751986503601, Training Accuracy: 86.71
[ Wed Jul  3 11:39:34 2024 ] 	Batch(5000/7879) done. Loss: 0.1140  lr:0.010000
[ Wed Jul  3 11:39:52 2024 ] 	Batch(5100/7879) done. Loss: 0.1798  lr:0.010000
[ Wed Jul  3 11:40:10 2024 ] 	Batch(5200/7879) done. Loss: 0.3826  lr:0.010000
[ Wed Jul  3 11:40:28 2024 ] 	Batch(5300/7879) done. Loss: 0.1567  lr:0.010000
[ Wed Jul  3 11:40:46 2024 ] 	Batch(5400/7879) done. Loss: 0.3558  lr:0.010000
[ Wed Jul  3 11:41:04 2024 ] 
Training: Epoch [35/120], Step [5499], Loss: 0.8750444650650024, Training Accuracy: 86.6590909090909
[ Wed Jul  3 11:41:04 2024 ] 	Batch(5500/7879) done. Loss: 0.0589  lr:0.010000
[ Wed Jul  3 11:41:22 2024 ] 	Batch(5600/7879) done. Loss: 0.4322  lr:0.010000
[ Wed Jul  3 11:41:40 2024 ] 	Batch(5700/7879) done. Loss: 0.8840  lr:0.010000
[ Wed Jul  3 11:41:58 2024 ] 	Batch(5800/7879) done. Loss: 0.4795  lr:0.010000
[ Wed Jul  3 11:42:15 2024 ] 	Batch(5900/7879) done. Loss: 0.5752  lr:0.010000
[ Wed Jul  3 11:42:33 2024 ] 
Training: Epoch [35/120], Step [5999], Loss: 0.29492107033729553, Training Accuracy: 86.6625
[ Wed Jul  3 11:42:33 2024 ] 	Batch(6000/7879) done. Loss: 0.5873  lr:0.010000
[ Wed Jul  3 11:42:51 2024 ] 	Batch(6100/7879) done. Loss: 0.4794  lr:0.010000
[ Wed Jul  3 11:43:09 2024 ] 	Batch(6200/7879) done. Loss: 0.7830  lr:0.010000
[ Wed Jul  3 11:43:27 2024 ] 	Batch(6300/7879) done. Loss: 0.2256  lr:0.010000
[ Wed Jul  3 11:43:45 2024 ] 	Batch(6400/7879) done. Loss: 0.9803  lr:0.010000
[ Wed Jul  3 11:44:03 2024 ] 
Training: Epoch [35/120], Step [6499], Loss: 0.43150800466537476, Training Accuracy: 86.61923076923077
[ Wed Jul  3 11:44:03 2024 ] 	Batch(6500/7879) done. Loss: 1.6451  lr:0.010000
[ Wed Jul  3 11:44:21 2024 ] 	Batch(6600/7879) done. Loss: 0.1631  lr:0.010000
[ Wed Jul  3 11:44:39 2024 ] 	Batch(6700/7879) done. Loss: 0.2790  lr:0.010000
[ Wed Jul  3 11:44:57 2024 ] 	Batch(6800/7879) done. Loss: 0.2433  lr:0.010000
[ Wed Jul  3 11:45:15 2024 ] 	Batch(6900/7879) done. Loss: 0.2122  lr:0.010000
[ Wed Jul  3 11:45:32 2024 ] 
Training: Epoch [35/120], Step [6999], Loss: 0.6854984164237976, Training Accuracy: 86.51785714285715
[ Wed Jul  3 11:45:32 2024 ] 	Batch(7000/7879) done. Loss: 0.4265  lr:0.010000
[ Wed Jul  3 11:45:50 2024 ] 	Batch(7100/7879) done. Loss: 0.0591  lr:0.010000
[ Wed Jul  3 11:46:08 2024 ] 	Batch(7200/7879) done. Loss: 0.2255  lr:0.010000
[ Wed Jul  3 11:46:26 2024 ] 	Batch(7300/7879) done. Loss: 0.6059  lr:0.010000
[ Wed Jul  3 11:46:44 2024 ] 	Batch(7400/7879) done. Loss: 0.3833  lr:0.010000
[ Wed Jul  3 11:47:02 2024 ] 
Training: Epoch [35/120], Step [7499], Loss: 0.3976459801197052, Training Accuracy: 86.43333333333332
[ Wed Jul  3 11:47:02 2024 ] 	Batch(7500/7879) done. Loss: 0.2894  lr:0.010000
[ Wed Jul  3 11:47:20 2024 ] 	Batch(7600/7879) done. Loss: 0.6372  lr:0.010000
[ Wed Jul  3 11:47:38 2024 ] 	Batch(7700/7879) done. Loss: 0.0259  lr:0.010000
[ Wed Jul  3 11:47:56 2024 ] 	Batch(7800/7879) done. Loss: 0.3723  lr:0.010000
[ Wed Jul  3 11:48:10 2024 ] 	Mean training loss: 0.4411.
[ Wed Jul  3 11:48:10 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 11:48:10 2024 ] Training epoch: 37
[ Wed Jul  3 11:48:11 2024 ] 	Batch(0/7879) done. Loss: 0.6417  lr:0.010000
[ Wed Jul  3 11:48:29 2024 ] 	Batch(100/7879) done. Loss: 0.2334  lr:0.010000
[ Wed Jul  3 11:48:47 2024 ] 	Batch(200/7879) done. Loss: 0.1185  lr:0.010000
[ Wed Jul  3 11:49:04 2024 ] 	Batch(300/7879) done. Loss: 0.8769  lr:0.010000
[ Wed Jul  3 11:49:22 2024 ] 	Batch(400/7879) done. Loss: 1.1291  lr:0.010000
[ Wed Jul  3 11:49:40 2024 ] 
Training: Epoch [36/120], Step [499], Loss: 0.06687390804290771, Training Accuracy: 87.85
[ Wed Jul  3 11:49:40 2024 ] 	Batch(500/7879) done. Loss: 0.3344  lr:0.010000
[ Wed Jul  3 11:49:58 2024 ] 	Batch(600/7879) done. Loss: 0.0098  lr:0.010000
[ Wed Jul  3 11:50:16 2024 ] 	Batch(700/7879) done. Loss: 0.1763  lr:0.010000
[ Wed Jul  3 11:50:34 2024 ] 	Batch(800/7879) done. Loss: 0.0719  lr:0.010000
[ Wed Jul  3 11:50:52 2024 ] 	Batch(900/7879) done. Loss: 0.0359  lr:0.010000
[ Wed Jul  3 11:51:10 2024 ] 
Training: Epoch [36/120], Step [999], Loss: 0.09713335335254669, Training Accuracy: 87.75
[ Wed Jul  3 11:51:10 2024 ] 	Batch(1000/7879) done. Loss: 0.5987  lr:0.010000
[ Wed Jul  3 11:51:28 2024 ] 	Batch(1100/7879) done. Loss: 0.5879  lr:0.010000
[ Wed Jul  3 11:51:46 2024 ] 	Batch(1200/7879) done. Loss: 0.3614  lr:0.010000
[ Wed Jul  3 11:52:03 2024 ] 	Batch(1300/7879) done. Loss: 0.9129  lr:0.010000
[ Wed Jul  3 11:52:21 2024 ] 	Batch(1400/7879) done. Loss: 0.0369  lr:0.010000
[ Wed Jul  3 11:52:39 2024 ] 
Training: Epoch [36/120], Step [1499], Loss: 0.22146710753440857, Training Accuracy: 88.00833333333333
[ Wed Jul  3 11:52:39 2024 ] 	Batch(1500/7879) done. Loss: 0.2997  lr:0.010000
[ Wed Jul  3 11:52:57 2024 ] 	Batch(1600/7879) done. Loss: 0.3481  lr:0.010000
[ Wed Jul  3 11:53:15 2024 ] 	Batch(1700/7879) done. Loss: 0.0153  lr:0.010000
[ Wed Jul  3 11:53:33 2024 ] 	Batch(1800/7879) done. Loss: 0.4337  lr:0.010000
[ Wed Jul  3 11:53:51 2024 ] 	Batch(1900/7879) done. Loss: 0.9050  lr:0.010000
[ Wed Jul  3 11:54:08 2024 ] 
Training: Epoch [36/120], Step [1999], Loss: 1.0459439754486084, Training Accuracy: 87.97500000000001
[ Wed Jul  3 11:54:09 2024 ] 	Batch(2000/7879) done. Loss: 0.0716  lr:0.010000
[ Wed Jul  3 11:54:26 2024 ] 	Batch(2100/7879) done. Loss: 0.1735  lr:0.010000
[ Wed Jul  3 11:54:44 2024 ] 	Batch(2200/7879) done. Loss: 0.2886  lr:0.010000
[ Wed Jul  3 11:55:02 2024 ] 	Batch(2300/7879) done. Loss: 0.3961  lr:0.010000
[ Wed Jul  3 11:55:20 2024 ] 	Batch(2400/7879) done. Loss: 0.4348  lr:0.010000
[ Wed Jul  3 11:55:38 2024 ] 
Training: Epoch [36/120], Step [2499], Loss: 0.08651246130466461, Training Accuracy: 88.09
[ Wed Jul  3 11:55:38 2024 ] 	Batch(2500/7879) done. Loss: 0.1256  lr:0.010000
[ Wed Jul  3 11:55:56 2024 ] 	Batch(2600/7879) done. Loss: 1.0261  lr:0.010000
[ Wed Jul  3 11:56:14 2024 ] 	Batch(2700/7879) done. Loss: 0.0331  lr:0.010000
[ Wed Jul  3 11:56:32 2024 ] 	Batch(2800/7879) done. Loss: 0.2340  lr:0.010000
[ Wed Jul  3 11:56:50 2024 ] 	Batch(2900/7879) done. Loss: 0.5005  lr:0.010000
[ Wed Jul  3 11:57:08 2024 ] 
Training: Epoch [36/120], Step [2999], Loss: 0.16635707020759583, Training Accuracy: 87.77916666666667
[ Wed Jul  3 11:57:08 2024 ] 	Batch(3000/7879) done. Loss: 0.4762  lr:0.010000
[ Wed Jul  3 11:57:26 2024 ] 	Batch(3100/7879) done. Loss: 0.1016  lr:0.010000
[ Wed Jul  3 11:57:44 2024 ] 	Batch(3200/7879) done. Loss: 0.5204  lr:0.010000
[ Wed Jul  3 11:58:02 2024 ] 	Batch(3300/7879) done. Loss: 0.2737  lr:0.010000
[ Wed Jul  3 11:58:20 2024 ] 	Batch(3400/7879) done. Loss: 0.6694  lr:0.010000
[ Wed Jul  3 11:58:38 2024 ] 
Training: Epoch [36/120], Step [3499], Loss: 0.16364966332912445, Training Accuracy: 87.73214285714286
[ Wed Jul  3 11:58:38 2024 ] 	Batch(3500/7879) done. Loss: 0.1765  lr:0.010000
[ Wed Jul  3 11:58:56 2024 ] 	Batch(3600/7879) done. Loss: 0.0974  lr:0.010000
[ Wed Jul  3 11:59:14 2024 ] 	Batch(3700/7879) done. Loss: 0.1372  lr:0.010000
[ Wed Jul  3 11:59:33 2024 ] 	Batch(3800/7879) done. Loss: 0.2752  lr:0.010000
[ Wed Jul  3 11:59:51 2024 ] 	Batch(3900/7879) done. Loss: 0.2590  lr:0.010000
[ Wed Jul  3 12:00:10 2024 ] 
Training: Epoch [36/120], Step [3999], Loss: 0.8494145274162292, Training Accuracy: 87.54375
[ Wed Jul  3 12:00:10 2024 ] 	Batch(4000/7879) done. Loss: 0.0949  lr:0.010000
[ Wed Jul  3 12:00:28 2024 ] 	Batch(4100/7879) done. Loss: 0.3765  lr:0.010000
[ Wed Jul  3 12:00:46 2024 ] 	Batch(4200/7879) done. Loss: 0.9265  lr:0.010000
[ Wed Jul  3 12:01:04 2024 ] 	Batch(4300/7879) done. Loss: 0.8661  lr:0.010000
[ Wed Jul  3 12:01:22 2024 ] 	Batch(4400/7879) done. Loss: 0.3887  lr:0.010000
[ Wed Jul  3 12:01:40 2024 ] 
Training: Epoch [36/120], Step [4499], Loss: 1.172967553138733, Training Accuracy: 87.3111111111111
[ Wed Jul  3 12:01:40 2024 ] 	Batch(4500/7879) done. Loss: 0.5740  lr:0.010000
[ Wed Jul  3 12:01:59 2024 ] 	Batch(4600/7879) done. Loss: 0.1880  lr:0.010000
[ Wed Jul  3 12:02:17 2024 ] 	Batch(4700/7879) done. Loss: 0.3301  lr:0.010000
[ Wed Jul  3 12:02:35 2024 ] 	Batch(4800/7879) done. Loss: 0.5253  lr:0.010000
[ Wed Jul  3 12:02:53 2024 ] 	Batch(4900/7879) done. Loss: 0.3859  lr:0.010000
[ Wed Jul  3 12:03:11 2024 ] 
Training: Epoch [36/120], Step [4999], Loss: 0.1259327381849289, Training Accuracy: 87.1425
[ Wed Jul  3 12:03:11 2024 ] 	Batch(5000/7879) done. Loss: 0.1213  lr:0.010000
[ Wed Jul  3 12:03:29 2024 ] 	Batch(5100/7879) done. Loss: 1.0940  lr:0.010000
[ Wed Jul  3 12:03:47 2024 ] 	Batch(5200/7879) done. Loss: 0.7040  lr:0.010000
[ Wed Jul  3 12:04:05 2024 ] 	Batch(5300/7879) done. Loss: 0.7959  lr:0.010000
[ Wed Jul  3 12:04:24 2024 ] 	Batch(5400/7879) done. Loss: 0.0127  lr:0.010000
[ Wed Jul  3 12:04:42 2024 ] 
Training: Epoch [36/120], Step [5499], Loss: 0.9315022826194763, Training Accuracy: 87.05909090909091
[ Wed Jul  3 12:04:42 2024 ] 	Batch(5500/7879) done. Loss: 0.3267  lr:0.010000
[ Wed Jul  3 12:05:01 2024 ] 	Batch(5600/7879) done. Loss: 0.3265  lr:0.010000
[ Wed Jul  3 12:05:19 2024 ] 	Batch(5700/7879) done. Loss: 0.6358  lr:0.010000
[ Wed Jul  3 12:05:38 2024 ] 	Batch(5800/7879) done. Loss: 0.3125  lr:0.010000
[ Wed Jul  3 12:05:56 2024 ] 	Batch(5900/7879) done. Loss: 0.1822  lr:0.010000
[ Wed Jul  3 12:06:15 2024 ] 
Training: Epoch [36/120], Step [5999], Loss: 0.8125725388526917, Training Accuracy: 87.00833333333333
[ Wed Jul  3 12:06:15 2024 ] 	Batch(6000/7879) done. Loss: 1.3584  lr:0.010000
[ Wed Jul  3 12:06:34 2024 ] 	Batch(6100/7879) done. Loss: 0.4105  lr:0.010000
[ Wed Jul  3 12:06:52 2024 ] 	Batch(6200/7879) done. Loss: 1.0077  lr:0.010000
[ Wed Jul  3 12:07:10 2024 ] 	Batch(6300/7879) done. Loss: 0.0673  lr:0.010000
[ Wed Jul  3 12:07:28 2024 ] 	Batch(6400/7879) done. Loss: 0.2455  lr:0.010000
[ Wed Jul  3 12:07:47 2024 ] 
Training: Epoch [36/120], Step [6499], Loss: 0.16798558831214905, Training Accuracy: 86.90961538461538
[ Wed Jul  3 12:07:47 2024 ] 	Batch(6500/7879) done. Loss: 0.0741  lr:0.010000
[ Wed Jul  3 12:08:05 2024 ] 	Batch(6600/7879) done. Loss: 0.5707  lr:0.010000
[ Wed Jul  3 12:08:24 2024 ] 	Batch(6700/7879) done. Loss: 0.3190  lr:0.010000
[ Wed Jul  3 12:08:43 2024 ] 	Batch(6800/7879) done. Loss: 0.0284  lr:0.010000
[ Wed Jul  3 12:09:01 2024 ] 	Batch(6900/7879) done. Loss: 0.5305  lr:0.010000
[ Wed Jul  3 12:09:19 2024 ] 
Training: Epoch [36/120], Step [6999], Loss: 0.7715215682983398, Training Accuracy: 86.81785714285715
[ Wed Jul  3 12:09:20 2024 ] 	Batch(7000/7879) done. Loss: 1.2698  lr:0.010000
[ Wed Jul  3 12:09:38 2024 ] 	Batch(7100/7879) done. Loss: 0.9874  lr:0.010000
[ Wed Jul  3 12:09:57 2024 ] 	Batch(7200/7879) done. Loss: 0.0048  lr:0.010000
[ Wed Jul  3 12:10:15 2024 ] 	Batch(7300/7879) done. Loss: 0.0978  lr:0.010000
[ Wed Jul  3 12:10:33 2024 ] 	Batch(7400/7879) done. Loss: 1.1230  lr:0.010000
[ Wed Jul  3 12:10:51 2024 ] 
Training: Epoch [36/120], Step [7499], Loss: 0.5502486824989319, Training Accuracy: 86.82166666666666
[ Wed Jul  3 12:10:51 2024 ] 	Batch(7500/7879) done. Loss: 0.8739  lr:0.010000
[ Wed Jul  3 12:11:09 2024 ] 	Batch(7600/7879) done. Loss: 0.5703  lr:0.010000
[ Wed Jul  3 12:11:27 2024 ] 	Batch(7700/7879) done. Loss: 0.3377  lr:0.010000
[ Wed Jul  3 12:11:45 2024 ] 	Batch(7800/7879) done. Loss: 0.2235  lr:0.010000
[ Wed Jul  3 12:11:59 2024 ] 	Mean training loss: 0.4276.
[ Wed Jul  3 12:11:59 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 12:11:59 2024 ] Training epoch: 38
[ Wed Jul  3 12:11:59 2024 ] 	Batch(0/7879) done. Loss: 0.3864  lr:0.010000
[ Wed Jul  3 12:12:18 2024 ] 	Batch(100/7879) done. Loss: 0.0989  lr:0.010000
[ Wed Jul  3 12:12:36 2024 ] 	Batch(200/7879) done. Loss: 0.0681  lr:0.010000
[ Wed Jul  3 12:12:55 2024 ] 	Batch(300/7879) done. Loss: 0.5479  lr:0.010000
[ Wed Jul  3 12:13:14 2024 ] 	Batch(400/7879) done. Loss: 0.4168  lr:0.010000
[ Wed Jul  3 12:13:32 2024 ] 
Training: Epoch [37/120], Step [499], Loss: 0.6442573070526123, Training Accuracy: 88.3
[ Wed Jul  3 12:13:32 2024 ] 	Batch(500/7879) done. Loss: 0.0149  lr:0.010000
[ Wed Jul  3 12:13:51 2024 ] 	Batch(600/7879) done. Loss: 0.2874  lr:0.010000
[ Wed Jul  3 12:14:08 2024 ] 	Batch(700/7879) done. Loss: 0.0833  lr:0.010000
[ Wed Jul  3 12:14:26 2024 ] 	Batch(800/7879) done. Loss: 0.3404  lr:0.010000
[ Wed Jul  3 12:14:44 2024 ] 	Batch(900/7879) done. Loss: 0.9932  lr:0.010000
[ Wed Jul  3 12:15:02 2024 ] 
Training: Epoch [37/120], Step [999], Loss: 0.3373520076274872, Training Accuracy: 88.3
[ Wed Jul  3 12:15:02 2024 ] 	Batch(1000/7879) done. Loss: 0.3242  lr:0.010000
[ Wed Jul  3 12:15:20 2024 ] 	Batch(1100/7879) done. Loss: 0.4643  lr:0.010000
[ Wed Jul  3 12:15:38 2024 ] 	Batch(1200/7879) done. Loss: 0.8782  lr:0.010000
[ Wed Jul  3 12:15:56 2024 ] 	Batch(1300/7879) done. Loss: 0.7742  lr:0.010000
[ Wed Jul  3 12:16:14 2024 ] 	Batch(1400/7879) done. Loss: 0.1949  lr:0.010000
[ Wed Jul  3 12:16:31 2024 ] 
Training: Epoch [37/120], Step [1499], Loss: 0.4066678285598755, Training Accuracy: 88.16666666666667
[ Wed Jul  3 12:16:32 2024 ] 	Batch(1500/7879) done. Loss: 0.1134  lr:0.010000
[ Wed Jul  3 12:16:50 2024 ] 	Batch(1600/7879) done. Loss: 0.4595  lr:0.010000
[ Wed Jul  3 12:17:07 2024 ] 	Batch(1700/7879) done. Loss: 0.3828  lr:0.010000
[ Wed Jul  3 12:17:25 2024 ] 	Batch(1800/7879) done. Loss: 0.0473  lr:0.010000
[ Wed Jul  3 12:17:43 2024 ] 	Batch(1900/7879) done. Loss: 0.1519  lr:0.010000
[ Wed Jul  3 12:18:01 2024 ] 
Training: Epoch [37/120], Step [1999], Loss: 0.31717750430107117, Training Accuracy: 88.0
[ Wed Jul  3 12:18:01 2024 ] 	Batch(2000/7879) done. Loss: 0.4453  lr:0.010000
[ Wed Jul  3 12:18:19 2024 ] 	Batch(2100/7879) done. Loss: 0.3544  lr:0.010000
[ Wed Jul  3 12:18:37 2024 ] 	Batch(2200/7879) done. Loss: 0.2749  lr:0.010000
[ Wed Jul  3 12:18:55 2024 ] 	Batch(2300/7879) done. Loss: 0.3504  lr:0.010000
[ Wed Jul  3 12:19:13 2024 ] 	Batch(2400/7879) done. Loss: 0.6082  lr:0.010000
[ Wed Jul  3 12:19:30 2024 ] 
Training: Epoch [37/120], Step [2499], Loss: 0.2714844048023224, Training Accuracy: 87.715
[ Wed Jul  3 12:19:31 2024 ] 	Batch(2500/7879) done. Loss: 0.3890  lr:0.010000
[ Wed Jul  3 12:19:48 2024 ] 	Batch(2600/7879) done. Loss: 0.5347  lr:0.010000
[ Wed Jul  3 12:20:06 2024 ] 	Batch(2700/7879) done. Loss: 0.6917  lr:0.010000
[ Wed Jul  3 12:20:24 2024 ] 	Batch(2800/7879) done. Loss: 0.6047  lr:0.010000
[ Wed Jul  3 12:20:43 2024 ] 	Batch(2900/7879) done. Loss: 0.0760  lr:0.010000
[ Wed Jul  3 12:21:01 2024 ] 
Training: Epoch [37/120], Step [2999], Loss: 0.2488134354352951, Training Accuracy: 87.79583333333333
[ Wed Jul  3 12:21:01 2024 ] 	Batch(3000/7879) done. Loss: 0.0840  lr:0.010000
[ Wed Jul  3 12:21:20 2024 ] 	Batch(3100/7879) done. Loss: 0.1498  lr:0.010000
[ Wed Jul  3 12:21:38 2024 ] 	Batch(3200/7879) done. Loss: 0.6249  lr:0.010000
[ Wed Jul  3 12:21:56 2024 ] 	Batch(3300/7879) done. Loss: 0.0312  lr:0.010000
[ Wed Jul  3 12:22:14 2024 ] 	Batch(3400/7879) done. Loss: 0.0232  lr:0.010000
[ Wed Jul  3 12:22:32 2024 ] 
Training: Epoch [37/120], Step [3499], Loss: 0.6449720859527588, Training Accuracy: 87.75357142857143
[ Wed Jul  3 12:22:32 2024 ] 	Batch(3500/7879) done. Loss: 0.4430  lr:0.010000
[ Wed Jul  3 12:22:50 2024 ] 	Batch(3600/7879) done. Loss: 0.1781  lr:0.010000
[ Wed Jul  3 12:23:09 2024 ] 	Batch(3700/7879) done. Loss: 0.3501  lr:0.010000
[ Wed Jul  3 12:23:27 2024 ] 	Batch(3800/7879) done. Loss: 0.4758  lr:0.010000
[ Wed Jul  3 12:23:46 2024 ] 	Batch(3900/7879) done. Loss: 0.3852  lr:0.010000
[ Wed Jul  3 12:24:04 2024 ] 
Training: Epoch [37/120], Step [3999], Loss: 0.31807687878608704, Training Accuracy: 87.675
[ Wed Jul  3 12:24:05 2024 ] 	Batch(4000/7879) done. Loss: 0.5519  lr:0.010000
[ Wed Jul  3 12:24:23 2024 ] 	Batch(4100/7879) done. Loss: 0.4602  lr:0.010000
[ Wed Jul  3 12:24:40 2024 ] 	Batch(4200/7879) done. Loss: 0.5208  lr:0.010000
[ Wed Jul  3 12:24:58 2024 ] 	Batch(4300/7879) done. Loss: 0.6680  lr:0.010000
[ Wed Jul  3 12:25:16 2024 ] 	Batch(4400/7879) done. Loss: 0.1121  lr:0.010000
[ Wed Jul  3 12:25:34 2024 ] 
Training: Epoch [37/120], Step [4499], Loss: 0.020758679136633873, Training Accuracy: 87.6138888888889
[ Wed Jul  3 12:25:34 2024 ] 	Batch(4500/7879) done. Loss: 0.6334  lr:0.010000
[ Wed Jul  3 12:25:52 2024 ] 	Batch(4600/7879) done. Loss: 0.5135  lr:0.010000
[ Wed Jul  3 12:26:10 2024 ] 	Batch(4700/7879) done. Loss: 0.2339  lr:0.010000
[ Wed Jul  3 12:26:28 2024 ] 	Batch(4800/7879) done. Loss: 0.2042  lr:0.010000
[ Wed Jul  3 12:26:46 2024 ] 	Batch(4900/7879) done. Loss: 0.6710  lr:0.010000
[ Wed Jul  3 12:27:04 2024 ] 
Training: Epoch [37/120], Step [4999], Loss: 0.3488343358039856, Training Accuracy: 87.5425
[ Wed Jul  3 12:27:04 2024 ] 	Batch(5000/7879) done. Loss: 0.1115  lr:0.010000
[ Wed Jul  3 12:27:22 2024 ] 	Batch(5100/7879) done. Loss: 0.1562  lr:0.010000
[ Wed Jul  3 12:27:40 2024 ] 	Batch(5200/7879) done. Loss: 0.1260  lr:0.010000
[ Wed Jul  3 12:27:58 2024 ] 	Batch(5300/7879) done. Loss: 0.1463  lr:0.010000
[ Wed Jul  3 12:28:16 2024 ] 	Batch(5400/7879) done. Loss: 0.0913  lr:0.010000
[ Wed Jul  3 12:28:34 2024 ] 
Training: Epoch [37/120], Step [5499], Loss: 0.4927222728729248, Training Accuracy: 87.38181818181818
[ Wed Jul  3 12:28:34 2024 ] 	Batch(5500/7879) done. Loss: 0.0790  lr:0.010000
[ Wed Jul  3 12:28:52 2024 ] 	Batch(5600/7879) done. Loss: 0.2094  lr:0.010000
[ Wed Jul  3 12:29:10 2024 ] 	Batch(5700/7879) done. Loss: 1.4645  lr:0.010000
[ Wed Jul  3 12:29:28 2024 ] 	Batch(5800/7879) done. Loss: 0.0439  lr:0.010000
[ Wed Jul  3 12:29:46 2024 ] 	Batch(5900/7879) done. Loss: 0.1500  lr:0.010000
[ Wed Jul  3 12:30:03 2024 ] 
Training: Epoch [37/120], Step [5999], Loss: 0.7656773924827576, Training Accuracy: 87.34375
[ Wed Jul  3 12:30:04 2024 ] 	Batch(6000/7879) done. Loss: 0.0766  lr:0.010000
[ Wed Jul  3 12:30:22 2024 ] 	Batch(6100/7879) done. Loss: 0.2687  lr:0.010000
[ Wed Jul  3 12:30:40 2024 ] 	Batch(6200/7879) done. Loss: 0.4798  lr:0.010000
[ Wed Jul  3 12:30:58 2024 ] 	Batch(6300/7879) done. Loss: 1.0824  lr:0.010000
[ Wed Jul  3 12:31:16 2024 ] 	Batch(6400/7879) done. Loss: 0.2772  lr:0.010000
[ Wed Jul  3 12:31:34 2024 ] 
Training: Epoch [37/120], Step [6499], Loss: 0.31723278760910034, Training Accuracy: 87.23076923076924
[ Wed Jul  3 12:31:34 2024 ] 	Batch(6500/7879) done. Loss: 0.1756  lr:0.010000
[ Wed Jul  3 12:31:52 2024 ] 	Batch(6600/7879) done. Loss: 0.2847  lr:0.010000
[ Wed Jul  3 12:32:10 2024 ] 	Batch(6700/7879) done. Loss: 0.6009  lr:0.010000
[ Wed Jul  3 12:32:28 2024 ] 	Batch(6800/7879) done. Loss: 0.3021  lr:0.010000
[ Wed Jul  3 12:32:46 2024 ] 	Batch(6900/7879) done. Loss: 0.1194  lr:0.010000
[ Wed Jul  3 12:33:03 2024 ] 
Training: Epoch [37/120], Step [6999], Loss: 0.02941444143652916, Training Accuracy: 87.20892857142857
[ Wed Jul  3 12:33:03 2024 ] 	Batch(7000/7879) done. Loss: 0.6779  lr:0.010000
[ Wed Jul  3 12:33:21 2024 ] 	Batch(7100/7879) done. Loss: 1.1994  lr:0.010000
[ Wed Jul  3 12:33:39 2024 ] 	Batch(7200/7879) done. Loss: 0.7908  lr:0.010000
[ Wed Jul  3 12:33:57 2024 ] 	Batch(7300/7879) done. Loss: 0.3114  lr:0.010000
[ Wed Jul  3 12:34:15 2024 ] 	Batch(7400/7879) done. Loss: 0.0852  lr:0.010000
[ Wed Jul  3 12:34:33 2024 ] 
Training: Epoch [37/120], Step [7499], Loss: 0.5434876084327698, Training Accuracy: 87.11333333333333
[ Wed Jul  3 12:34:33 2024 ] 	Batch(7500/7879) done. Loss: 0.5471  lr:0.010000
[ Wed Jul  3 12:34:51 2024 ] 	Batch(7600/7879) done. Loss: 0.0419  lr:0.010000
[ Wed Jul  3 12:35:09 2024 ] 	Batch(7700/7879) done. Loss: 0.3338  lr:0.010000
[ Wed Jul  3 12:35:27 2024 ] 	Batch(7800/7879) done. Loss: 0.5439  lr:0.010000
[ Wed Jul  3 12:35:41 2024 ] 	Mean training loss: 0.4208.
[ Wed Jul  3 12:35:41 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 12:35:41 2024 ] Training epoch: 39
[ Wed Jul  3 12:35:41 2024 ] 	Batch(0/7879) done. Loss: 0.2818  lr:0.010000
[ Wed Jul  3 12:36:00 2024 ] 	Batch(100/7879) done. Loss: 0.5044  lr:0.010000
[ Wed Jul  3 12:36:18 2024 ] 	Batch(200/7879) done. Loss: 0.2035  lr:0.010000
[ Wed Jul  3 12:36:37 2024 ] 	Batch(300/7879) done. Loss: 0.3645  lr:0.010000
[ Wed Jul  3 12:36:55 2024 ] 	Batch(400/7879) done. Loss: 0.1676  lr:0.010000
[ Wed Jul  3 12:37:14 2024 ] 
Training: Epoch [38/120], Step [499], Loss: 0.17975836992263794, Training Accuracy: 87.8
[ Wed Jul  3 12:37:14 2024 ] 	Batch(500/7879) done. Loss: 0.3042  lr:0.010000
[ Wed Jul  3 12:37:32 2024 ] 	Batch(600/7879) done. Loss: 0.0253  lr:0.010000
[ Wed Jul  3 12:37:51 2024 ] 	Batch(700/7879) done. Loss: 0.1826  lr:0.010000
[ Wed Jul  3 12:38:09 2024 ] 	Batch(800/7879) done. Loss: 0.5691  lr:0.010000
[ Wed Jul  3 12:38:27 2024 ] 	Batch(900/7879) done. Loss: 0.6857  lr:0.010000
[ Wed Jul  3 12:38:45 2024 ] 
Training: Epoch [38/120], Step [999], Loss: 0.3077431619167328, Training Accuracy: 87.91250000000001
[ Wed Jul  3 12:38:45 2024 ] 	Batch(1000/7879) done. Loss: 0.0860  lr:0.010000
[ Wed Jul  3 12:39:03 2024 ] 	Batch(1100/7879) done. Loss: 1.3901  lr:0.010000
[ Wed Jul  3 12:39:21 2024 ] 	Batch(1200/7879) done. Loss: 0.4781  lr:0.010000
[ Wed Jul  3 12:39:39 2024 ] 	Batch(1300/7879) done. Loss: 0.1927  lr:0.010000
[ Wed Jul  3 12:39:57 2024 ] 	Batch(1400/7879) done. Loss: 0.0661  lr:0.010000
[ Wed Jul  3 12:40:15 2024 ] 
Training: Epoch [38/120], Step [1499], Loss: 0.3044033944606781, Training Accuracy: 88.03333333333333
[ Wed Jul  3 12:40:15 2024 ] 	Batch(1500/7879) done. Loss: 0.3007  lr:0.010000
[ Wed Jul  3 12:40:33 2024 ] 	Batch(1600/7879) done. Loss: 0.2028  lr:0.010000
[ Wed Jul  3 12:40:51 2024 ] 	Batch(1700/7879) done. Loss: 0.3628  lr:0.010000
[ Wed Jul  3 12:41:09 2024 ] 	Batch(1800/7879) done. Loss: 0.1475  lr:0.010000
[ Wed Jul  3 12:41:27 2024 ] 	Batch(1900/7879) done. Loss: 0.7178  lr:0.010000
[ Wed Jul  3 12:41:44 2024 ] 
Training: Epoch [38/120], Step [1999], Loss: 0.16397899389266968, Training Accuracy: 87.775
[ Wed Jul  3 12:41:45 2024 ] 	Batch(2000/7879) done. Loss: 0.0972  lr:0.010000
[ Wed Jul  3 12:42:03 2024 ] 	Batch(2100/7879) done. Loss: 0.1473  lr:0.010000
[ Wed Jul  3 12:42:20 2024 ] 	Batch(2200/7879) done. Loss: 0.7496  lr:0.010000
[ Wed Jul  3 12:42:38 2024 ] 	Batch(2300/7879) done. Loss: 0.9152  lr:0.010000
[ Wed Jul  3 12:42:56 2024 ] 	Batch(2400/7879) done. Loss: 0.0948  lr:0.010000
[ Wed Jul  3 12:43:14 2024 ] 
Training: Epoch [38/120], Step [2499], Loss: 0.12803621590137482, Training Accuracy: 87.63499999999999
[ Wed Jul  3 12:43:15 2024 ] 	Batch(2500/7879) done. Loss: 0.9764  lr:0.010000
[ Wed Jul  3 12:43:32 2024 ] 	Batch(2600/7879) done. Loss: 0.5012  lr:0.010000
[ Wed Jul  3 12:43:50 2024 ] 	Batch(2700/7879) done. Loss: 0.6725  lr:0.010000
[ Wed Jul  3 12:44:08 2024 ] 	Batch(2800/7879) done. Loss: 0.3563  lr:0.010000
[ Wed Jul  3 12:44:27 2024 ] 	Batch(2900/7879) done. Loss: 0.4162  lr:0.010000
[ Wed Jul  3 12:44:45 2024 ] 
Training: Epoch [38/120], Step [2999], Loss: 0.3326166570186615, Training Accuracy: 87.58749999999999
[ Wed Jul  3 12:44:45 2024 ] 	Batch(3000/7879) done. Loss: 0.2176  lr:0.010000
[ Wed Jul  3 12:45:03 2024 ] 	Batch(3100/7879) done. Loss: 0.3444  lr:0.010000
[ Wed Jul  3 12:45:21 2024 ] 	Batch(3200/7879) done. Loss: 0.4396  lr:0.010000
[ Wed Jul  3 12:45:39 2024 ] 	Batch(3300/7879) done. Loss: 0.5255  lr:0.010000
[ Wed Jul  3 12:45:57 2024 ] 	Batch(3400/7879) done. Loss: 0.4417  lr:0.010000
[ Wed Jul  3 12:46:14 2024 ] 
Training: Epoch [38/120], Step [3499], Loss: 0.3298478126525879, Training Accuracy: 87.6
[ Wed Jul  3 12:46:14 2024 ] 	Batch(3500/7879) done. Loss: 0.1520  lr:0.010000
[ Wed Jul  3 12:46:32 2024 ] 	Batch(3600/7879) done. Loss: 0.0226  lr:0.010000
[ Wed Jul  3 12:46:50 2024 ] 	Batch(3700/7879) done. Loss: 0.2265  lr:0.010000
[ Wed Jul  3 12:47:08 2024 ] 	Batch(3800/7879) done. Loss: 0.6038  lr:0.010000
[ Wed Jul  3 12:47:26 2024 ] 	Batch(3900/7879) done. Loss: 0.1396  lr:0.010000
[ Wed Jul  3 12:47:44 2024 ] 
Training: Epoch [38/120], Step [3999], Loss: 0.04465833678841591, Training Accuracy: 87.56875
[ Wed Jul  3 12:47:44 2024 ] 	Batch(4000/7879) done. Loss: 0.8261  lr:0.010000
[ Wed Jul  3 12:48:03 2024 ] 	Batch(4100/7879) done. Loss: 0.7190  lr:0.010000
[ Wed Jul  3 12:48:21 2024 ] 	Batch(4200/7879) done. Loss: 0.4200  lr:0.010000
[ Wed Jul  3 12:48:40 2024 ] 	Batch(4300/7879) done. Loss: 0.2930  lr:0.010000
[ Wed Jul  3 12:48:58 2024 ] 	Batch(4400/7879) done. Loss: 0.2097  lr:0.010000
[ Wed Jul  3 12:49:15 2024 ] 
Training: Epoch [38/120], Step [4499], Loss: 0.0975089967250824, Training Accuracy: 87.43055555555556
[ Wed Jul  3 12:49:16 2024 ] 	Batch(4500/7879) done. Loss: 0.1266  lr:0.010000
[ Wed Jul  3 12:49:33 2024 ] 	Batch(4600/7879) done. Loss: 0.8455  lr:0.010000
[ Wed Jul  3 12:49:51 2024 ] 	Batch(4700/7879) done. Loss: 0.7210  lr:0.010000
[ Wed Jul  3 12:50:09 2024 ] 	Batch(4800/7879) done. Loss: 0.1551  lr:0.010000
[ Wed Jul  3 12:50:27 2024 ] 	Batch(4900/7879) done. Loss: 1.0499  lr:0.010000
[ Wed Jul  3 12:50:45 2024 ] 
Training: Epoch [38/120], Step [4999], Loss: 0.3503316044807434, Training Accuracy: 87.42750000000001
[ Wed Jul  3 12:50:45 2024 ] 	Batch(5000/7879) done. Loss: 0.2621  lr:0.010000
[ Wed Jul  3 12:51:03 2024 ] 	Batch(5100/7879) done. Loss: 0.5123  lr:0.010000
[ Wed Jul  3 12:51:21 2024 ] 	Batch(5200/7879) done. Loss: 0.2238  lr:0.010000
[ Wed Jul  3 12:51:39 2024 ] 	Batch(5300/7879) done. Loss: 0.3528  lr:0.010000
[ Wed Jul  3 12:51:57 2024 ] 	Batch(5400/7879) done. Loss: 0.2023  lr:0.010000
[ Wed Jul  3 12:52:14 2024 ] 
Training: Epoch [38/120], Step [5499], Loss: 0.14606769382953644, Training Accuracy: 87.40681818181818
[ Wed Jul  3 12:52:15 2024 ] 	Batch(5500/7879) done. Loss: 0.1083  lr:0.010000
[ Wed Jul  3 12:52:33 2024 ] 	Batch(5600/7879) done. Loss: 0.0820  lr:0.010000
[ Wed Jul  3 12:52:51 2024 ] 	Batch(5700/7879) done. Loss: 0.1729  lr:0.010000
[ Wed Jul  3 12:53:10 2024 ] 	Batch(5800/7879) done. Loss: 0.2053  lr:0.010000
[ Wed Jul  3 12:53:28 2024 ] 	Batch(5900/7879) done. Loss: 0.0897  lr:0.010000
[ Wed Jul  3 12:53:47 2024 ] 
Training: Epoch [38/120], Step [5999], Loss: 0.1980479210615158, Training Accuracy: 87.33541666666666
[ Wed Jul  3 12:53:47 2024 ] 	Batch(6000/7879) done. Loss: 0.2732  lr:0.010000
[ Wed Jul  3 12:54:05 2024 ] 	Batch(6100/7879) done. Loss: 0.6028  lr:0.010000
[ Wed Jul  3 12:54:23 2024 ] 	Batch(6200/7879) done. Loss: 0.2431  lr:0.010000
[ Wed Jul  3 12:54:41 2024 ] 	Batch(6300/7879) done. Loss: 0.2477  lr:0.010000
[ Wed Jul  3 12:54:59 2024 ] 	Batch(6400/7879) done. Loss: 0.1792  lr:0.010000
[ Wed Jul  3 12:55:17 2024 ] 
Training: Epoch [38/120], Step [6499], Loss: 0.11781466007232666, Training Accuracy: 87.18461538461538
[ Wed Jul  3 12:55:17 2024 ] 	Batch(6500/7879) done. Loss: 0.6560  lr:0.010000
[ Wed Jul  3 12:55:36 2024 ] 	Batch(6600/7879) done. Loss: 1.0261  lr:0.010000
[ Wed Jul  3 12:55:54 2024 ] 	Batch(6700/7879) done. Loss: 0.4485  lr:0.010000
[ Wed Jul  3 12:56:12 2024 ] 	Batch(6800/7879) done. Loss: 0.8670  lr:0.010000
[ Wed Jul  3 12:56:30 2024 ] 	Batch(6900/7879) done. Loss: 0.6308  lr:0.010000
[ Wed Jul  3 12:56:48 2024 ] 
Training: Epoch [38/120], Step [6999], Loss: 0.36644890904426575, Training Accuracy: 87.11428571428571
[ Wed Jul  3 12:56:48 2024 ] 	Batch(7000/7879) done. Loss: 0.0910  lr:0.010000
[ Wed Jul  3 12:57:06 2024 ] 	Batch(7100/7879) done. Loss: 1.4334  lr:0.010000
[ Wed Jul  3 12:57:24 2024 ] 	Batch(7200/7879) done. Loss: 0.1336  lr:0.010000
[ Wed Jul  3 12:57:42 2024 ] 	Batch(7300/7879) done. Loss: 0.2427  lr:0.010000
[ Wed Jul  3 12:58:00 2024 ] 	Batch(7400/7879) done. Loss: 1.3179  lr:0.010000
[ Wed Jul  3 12:58:17 2024 ] 
Training: Epoch [38/120], Step [7499], Loss: 0.05491116642951965, Training Accuracy: 87.07833333333333
[ Wed Jul  3 12:58:18 2024 ] 	Batch(7500/7879) done. Loss: 0.0133  lr:0.010000
[ Wed Jul  3 12:58:36 2024 ] 	Batch(7600/7879) done. Loss: 0.3157  lr:0.010000
[ Wed Jul  3 12:58:54 2024 ] 	Batch(7700/7879) done. Loss: 0.8650  lr:0.010000
[ Wed Jul  3 12:59:13 2024 ] 	Batch(7800/7879) done. Loss: 0.3757  lr:0.010000
[ Wed Jul  3 12:59:27 2024 ] 	Mean training loss: 0.4171.
[ Wed Jul  3 12:59:27 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 12:59:27 2024 ] Training epoch: 40
[ Wed Jul  3 12:59:28 2024 ] 	Batch(0/7879) done. Loss: 0.2305  lr:0.010000
[ Wed Jul  3 12:59:46 2024 ] 	Batch(100/7879) done. Loss: 0.0973  lr:0.010000
[ Wed Jul  3 13:00:03 2024 ] 	Batch(200/7879) done. Loss: 0.2830  lr:0.010000
[ Wed Jul  3 13:00:21 2024 ] 	Batch(300/7879) done. Loss: 0.7271  lr:0.010000
[ Wed Jul  3 13:00:39 2024 ] 	Batch(400/7879) done. Loss: 0.0253  lr:0.010000
[ Wed Jul  3 13:00:57 2024 ] 
Training: Epoch [39/120], Step [499], Loss: 0.2695538401603699, Training Accuracy: 89.14999999999999
[ Wed Jul  3 13:00:57 2024 ] 	Batch(500/7879) done. Loss: 0.2086  lr:0.010000
[ Wed Jul  3 13:01:15 2024 ] 	Batch(600/7879) done. Loss: 0.2823  lr:0.010000
[ Wed Jul  3 13:01:33 2024 ] 	Batch(700/7879) done. Loss: 0.1503  lr:0.010000
[ Wed Jul  3 13:01:51 2024 ] 	Batch(800/7879) done. Loss: 0.1963  lr:0.010000
[ Wed Jul  3 13:02:09 2024 ] 	Batch(900/7879) done. Loss: 0.7476  lr:0.010000
[ Wed Jul  3 13:02:27 2024 ] 
Training: Epoch [39/120], Step [999], Loss: 0.9746059775352478, Training Accuracy: 88.3875
[ Wed Jul  3 13:02:27 2024 ] 	Batch(1000/7879) done. Loss: 0.5064  lr:0.010000
[ Wed Jul  3 13:02:45 2024 ] 	Batch(1100/7879) done. Loss: 0.5404  lr:0.010000
[ Wed Jul  3 13:03:03 2024 ] 	Batch(1200/7879) done. Loss: 0.4623  lr:0.010000
[ Wed Jul  3 13:03:21 2024 ] 	Batch(1300/7879) done. Loss: 0.0173  lr:0.010000
[ Wed Jul  3 13:03:40 2024 ] 	Batch(1400/7879) done. Loss: 0.2161  lr:0.010000
[ Wed Jul  3 13:03:57 2024 ] 
Training: Epoch [39/120], Step [1499], Loss: 1.1693754196166992, Training Accuracy: 88.56666666666668
[ Wed Jul  3 13:03:58 2024 ] 	Batch(1500/7879) done. Loss: 0.2812  lr:0.010000
[ Wed Jul  3 13:04:16 2024 ] 	Batch(1600/7879) done. Loss: 0.1727  lr:0.010000
[ Wed Jul  3 13:04:34 2024 ] 	Batch(1700/7879) done. Loss: 0.2970  lr:0.010000
[ Wed Jul  3 13:04:52 2024 ] 	Batch(1800/7879) done. Loss: 0.9512  lr:0.010000
[ Wed Jul  3 13:05:09 2024 ] 	Batch(1900/7879) done. Loss: 0.1176  lr:0.010000
[ Wed Jul  3 13:05:27 2024 ] 
Training: Epoch [39/120], Step [1999], Loss: 0.2701287567615509, Training Accuracy: 88.35625
[ Wed Jul  3 13:05:27 2024 ] 	Batch(2000/7879) done. Loss: 0.4031  lr:0.010000
[ Wed Jul  3 13:05:45 2024 ] 	Batch(2100/7879) done. Loss: 0.6960  lr:0.010000
[ Wed Jul  3 13:06:03 2024 ] 	Batch(2200/7879) done. Loss: 0.1563  lr:0.010000
[ Wed Jul  3 13:06:21 2024 ] 	Batch(2300/7879) done. Loss: 0.1317  lr:0.010000
[ Wed Jul  3 13:06:39 2024 ] 	Batch(2400/7879) done. Loss: 0.1559  lr:0.010000
[ Wed Jul  3 13:06:57 2024 ] 
Training: Epoch [39/120], Step [2499], Loss: 0.3065025210380554, Training Accuracy: 88.34
[ Wed Jul  3 13:06:57 2024 ] 	Batch(2500/7879) done. Loss: 0.5772  lr:0.010000
[ Wed Jul  3 13:07:15 2024 ] 	Batch(2600/7879) done. Loss: 1.0652  lr:0.010000
[ Wed Jul  3 13:07:33 2024 ] 	Batch(2700/7879) done. Loss: 0.3816  lr:0.010000
[ Wed Jul  3 13:07:51 2024 ] 	Batch(2800/7879) done. Loss: 0.2838  lr:0.010000
[ Wed Jul  3 13:08:09 2024 ] 	Batch(2900/7879) done. Loss: 0.4119  lr:0.010000
[ Wed Jul  3 13:08:28 2024 ] 
Training: Epoch [39/120], Step [2999], Loss: 0.238927960395813, Training Accuracy: 88.36666666666667
[ Wed Jul  3 13:08:28 2024 ] 	Batch(3000/7879) done. Loss: 1.3528  lr:0.010000
[ Wed Jul  3 13:08:46 2024 ] 	Batch(3100/7879) done. Loss: 0.5710  lr:0.010000
[ Wed Jul  3 13:09:05 2024 ] 	Batch(3200/7879) done. Loss: 0.6546  lr:0.010000
[ Wed Jul  3 13:09:24 2024 ] 	Batch(3300/7879) done. Loss: 0.3382  lr:0.010000
[ Wed Jul  3 13:09:42 2024 ] 	Batch(3400/7879) done. Loss: 0.4405  lr:0.010000
[ Wed Jul  3 13:10:00 2024 ] 
Training: Epoch [39/120], Step [3499], Loss: 0.17289194464683533, Training Accuracy: 88.125
[ Wed Jul  3 13:10:01 2024 ] 	Batch(3500/7879) done. Loss: 0.9050  lr:0.010000
[ Wed Jul  3 13:10:19 2024 ] 	Batch(3600/7879) done. Loss: 0.4510  lr:0.010000
[ Wed Jul  3 13:10:38 2024 ] 	Batch(3700/7879) done. Loss: 0.3356  lr:0.010000
[ Wed Jul  3 13:10:56 2024 ] 	Batch(3800/7879) done. Loss: 0.7563  lr:0.010000
[ Wed Jul  3 13:11:15 2024 ] 	Batch(3900/7879) done. Loss: 0.8213  lr:0.010000
[ Wed Jul  3 13:11:33 2024 ] 
Training: Epoch [39/120], Step [3999], Loss: 0.2696461081504822, Training Accuracy: 87.9875
[ Wed Jul  3 13:11:33 2024 ] 	Batch(4000/7879) done. Loss: 0.0607  lr:0.010000
[ Wed Jul  3 13:11:51 2024 ] 	Batch(4100/7879) done. Loss: 0.8777  lr:0.010000
[ Wed Jul  3 13:12:09 2024 ] 	Batch(4200/7879) done. Loss: 0.3019  lr:0.010000
[ Wed Jul  3 13:12:27 2024 ] 	Batch(4300/7879) done. Loss: 0.5458  lr:0.010000
[ Wed Jul  3 13:12:45 2024 ] 	Batch(4400/7879) done. Loss: 0.0924  lr:0.010000
[ Wed Jul  3 13:13:03 2024 ] 
Training: Epoch [39/120], Step [4499], Loss: 0.22344040870666504, Training Accuracy: 87.77777777777777
[ Wed Jul  3 13:13:03 2024 ] 	Batch(4500/7879) done. Loss: 0.5285  lr:0.010000
[ Wed Jul  3 13:13:22 2024 ] 	Batch(4600/7879) done. Loss: 0.3638  lr:0.010000
[ Wed Jul  3 13:13:40 2024 ] 	Batch(4700/7879) done. Loss: 0.4078  lr:0.010000
[ Wed Jul  3 13:13:59 2024 ] 	Batch(4800/7879) done. Loss: 0.5944  lr:0.010000
[ Wed Jul  3 13:14:17 2024 ] 	Batch(4900/7879) done. Loss: 0.1964  lr:0.010000
[ Wed Jul  3 13:14:35 2024 ] 
Training: Epoch [39/120], Step [4999], Loss: 0.6196492314338684, Training Accuracy: 87.6575
[ Wed Jul  3 13:14:35 2024 ] 	Batch(5000/7879) done. Loss: 0.0664  lr:0.010000
[ Wed Jul  3 13:14:53 2024 ] 	Batch(5100/7879) done. Loss: 0.7275  lr:0.010000
[ Wed Jul  3 13:15:11 2024 ] 	Batch(5200/7879) done. Loss: 0.3443  lr:0.010000
[ Wed Jul  3 13:15:28 2024 ] 	Batch(5300/7879) done. Loss: 0.7216  lr:0.010000
[ Wed Jul  3 13:15:46 2024 ] 	Batch(5400/7879) done. Loss: 0.7673  lr:0.010000
[ Wed Jul  3 13:16:04 2024 ] 
Training: Epoch [39/120], Step [5499], Loss: 0.4271407425403595, Training Accuracy: 87.52272727272728
[ Wed Jul  3 13:16:04 2024 ] 	Batch(5500/7879) done. Loss: 0.7964  lr:0.010000
[ Wed Jul  3 13:16:22 2024 ] 	Batch(5600/7879) done. Loss: 0.6060  lr:0.010000
[ Wed Jul  3 13:16:40 2024 ] 	Batch(5700/7879) done. Loss: 0.0042  lr:0.010000
[ Wed Jul  3 13:16:58 2024 ] 	Batch(5800/7879) done. Loss: 0.2334  lr:0.010000
[ Wed Jul  3 13:17:16 2024 ] 	Batch(5900/7879) done. Loss: 0.1876  lr:0.010000
[ Wed Jul  3 13:17:34 2024 ] 
Training: Epoch [39/120], Step [5999], Loss: 0.13051488995552063, Training Accuracy: 87.44166666666666
[ Wed Jul  3 13:17:34 2024 ] 	Batch(6000/7879) done. Loss: 0.8052  lr:0.010000
[ Wed Jul  3 13:17:52 2024 ] 	Batch(6100/7879) done. Loss: 1.2822  lr:0.010000
[ Wed Jul  3 13:18:11 2024 ] 	Batch(6200/7879) done. Loss: 0.5452  lr:0.010000
[ Wed Jul  3 13:18:29 2024 ] 	Batch(6300/7879) done. Loss: 0.7491  lr:0.010000
[ Wed Jul  3 13:18:47 2024 ] 	Batch(6400/7879) done. Loss: 0.4581  lr:0.010000
[ Wed Jul  3 13:19:05 2024 ] 
Training: Epoch [39/120], Step [6499], Loss: 0.20062918961048126, Training Accuracy: 87.42307692307692
[ Wed Jul  3 13:19:05 2024 ] 	Batch(6500/7879) done. Loss: 0.0116  lr:0.010000
[ Wed Jul  3 13:19:23 2024 ] 	Batch(6600/7879) done. Loss: 0.6128  lr:0.010000
[ Wed Jul  3 13:19:41 2024 ] 	Batch(6700/7879) done. Loss: 0.8533  lr:0.010000
[ Wed Jul  3 13:19:59 2024 ] 	Batch(6800/7879) done. Loss: 0.2290  lr:0.010000
[ Wed Jul  3 13:20:18 2024 ] 	Batch(6900/7879) done. Loss: 0.1475  lr:0.010000
[ Wed Jul  3 13:20:36 2024 ] 
Training: Epoch [39/120], Step [6999], Loss: 0.9727745652198792, Training Accuracy: 87.43035714285713
[ Wed Jul  3 13:20:36 2024 ] 	Batch(7000/7879) done. Loss: 0.2770  lr:0.010000
[ Wed Jul  3 13:20:55 2024 ] 	Batch(7100/7879) done. Loss: 0.6490  lr:0.010000
[ Wed Jul  3 13:21:13 2024 ] 	Batch(7200/7879) done. Loss: 0.0233  lr:0.010000
[ Wed Jul  3 13:21:32 2024 ] 	Batch(7300/7879) done. Loss: 0.2343  lr:0.010000
[ Wed Jul  3 13:21:51 2024 ] 	Batch(7400/7879) done. Loss: 0.1027  lr:0.010000
[ Wed Jul  3 13:22:09 2024 ] 
Training: Epoch [39/120], Step [7499], Loss: 0.11913567781448364, Training Accuracy: 87.36333333333334
[ Wed Jul  3 13:22:09 2024 ] 	Batch(7500/7879) done. Loss: 0.6541  lr:0.010000
[ Wed Jul  3 13:22:28 2024 ] 	Batch(7600/7879) done. Loss: 0.6337  lr:0.010000
[ Wed Jul  3 13:22:46 2024 ] 	Batch(7700/7879) done. Loss: 0.5467  lr:0.010000
[ Wed Jul  3 13:23:04 2024 ] 	Batch(7800/7879) done. Loss: 0.8833  lr:0.010000
[ Wed Jul  3 13:23:18 2024 ] 	Mean training loss: 0.4169.
[ Wed Jul  3 13:23:18 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 13:23:18 2024 ] Eval epoch: 40
[ Wed Jul  3 13:28:04 2024 ] 	Mean val loss of 6365 batches: 1.8191098846534801.
[ Wed Jul  3 13:28:04 2024 ] Training epoch: 41
[ Wed Jul  3 13:28:04 2024 ] 	Batch(0/7879) done. Loss: 0.8424  lr:0.010000
[ Wed Jul  3 13:28:22 2024 ] 	Batch(100/7879) done. Loss: 0.1372  lr:0.010000
[ Wed Jul  3 13:28:40 2024 ] 	Batch(200/7879) done. Loss: 0.9912  lr:0.010000
[ Wed Jul  3 13:28:58 2024 ] 	Batch(300/7879) done. Loss: 0.2266  lr:0.010000
[ Wed Jul  3 13:29:16 2024 ] 	Batch(400/7879) done. Loss: 0.1751  lr:0.010000
[ Wed Jul  3 13:29:34 2024 ] 
Training: Epoch [40/120], Step [499], Loss: 0.13041982054710388, Training Accuracy: 88.35
[ Wed Jul  3 13:29:34 2024 ] 	Batch(500/7879) done. Loss: 0.3224  lr:0.010000
[ Wed Jul  3 13:29:52 2024 ] 	Batch(600/7879) done. Loss: 0.0815  lr:0.010000
[ Wed Jul  3 13:30:10 2024 ] 	Batch(700/7879) done. Loss: 0.2734  lr:0.010000
[ Wed Jul  3 13:30:28 2024 ] 	Batch(800/7879) done. Loss: 0.7465  lr:0.010000
[ Wed Jul  3 13:30:46 2024 ] 	Batch(900/7879) done. Loss: 0.1306  lr:0.010000
[ Wed Jul  3 13:31:03 2024 ] 
Training: Epoch [40/120], Step [999], Loss: 0.20668341219425201, Training Accuracy: 88.4375
[ Wed Jul  3 13:31:04 2024 ] 	Batch(1000/7879) done. Loss: 0.9744  lr:0.010000
[ Wed Jul  3 13:31:21 2024 ] 	Batch(1100/7879) done. Loss: 0.3185  lr:0.010000
[ Wed Jul  3 13:31:39 2024 ] 	Batch(1200/7879) done. Loss: 0.4877  lr:0.010000
[ Wed Jul  3 13:31:57 2024 ] 	Batch(1300/7879) done. Loss: 0.2017  lr:0.010000
[ Wed Jul  3 13:32:15 2024 ] 	Batch(1400/7879) done. Loss: 0.3705  lr:0.010000
[ Wed Jul  3 13:32:33 2024 ] 
Training: Epoch [40/120], Step [1499], Loss: 0.3708864152431488, Training Accuracy: 88.2
[ Wed Jul  3 13:32:33 2024 ] 	Batch(1500/7879) done. Loss: 0.1341  lr:0.010000
[ Wed Jul  3 13:32:51 2024 ] 	Batch(1600/7879) done. Loss: 0.5506  lr:0.010000
[ Wed Jul  3 13:33:09 2024 ] 	Batch(1700/7879) done. Loss: 0.2329  lr:0.010000
[ Wed Jul  3 13:33:27 2024 ] 	Batch(1800/7879) done. Loss: 0.5502  lr:0.010000
[ Wed Jul  3 13:33:45 2024 ] 	Batch(1900/7879) done. Loss: 0.4001  lr:0.010000
[ Wed Jul  3 13:34:02 2024 ] 
Training: Epoch [40/120], Step [1999], Loss: 0.037880174815654755, Training Accuracy: 88.08749999999999
[ Wed Jul  3 13:34:03 2024 ] 	Batch(2000/7879) done. Loss: 0.0147  lr:0.010000
[ Wed Jul  3 13:34:20 2024 ] 	Batch(2100/7879) done. Loss: 1.1777  lr:0.010000
[ Wed Jul  3 13:34:38 2024 ] 	Batch(2200/7879) done. Loss: 0.2854  lr:0.010000
[ Wed Jul  3 13:34:56 2024 ] 	Batch(2300/7879) done. Loss: 0.0940  lr:0.010000
[ Wed Jul  3 13:35:14 2024 ] 	Batch(2400/7879) done. Loss: 0.4173  lr:0.010000
[ Wed Jul  3 13:35:32 2024 ] 
Training: Epoch [40/120], Step [2499], Loss: 0.5860921144485474, Training Accuracy: 88.015
[ Wed Jul  3 13:35:32 2024 ] 	Batch(2500/7879) done. Loss: 0.3640  lr:0.010000
[ Wed Jul  3 13:35:50 2024 ] 	Batch(2600/7879) done. Loss: 0.3590  lr:0.010000
[ Wed Jul  3 13:36:07 2024 ] 	Batch(2700/7879) done. Loss: 0.4755  lr:0.010000
[ Wed Jul  3 13:36:25 2024 ] 	Batch(2800/7879) done. Loss: 1.0572  lr:0.010000
[ Wed Jul  3 13:36:43 2024 ] 	Batch(2900/7879) done. Loss: 0.3588  lr:0.010000
[ Wed Jul  3 13:37:01 2024 ] 
Training: Epoch [40/120], Step [2999], Loss: 0.32980889081954956, Training Accuracy: 87.9875
[ Wed Jul  3 13:37:01 2024 ] 	Batch(3000/7879) done. Loss: 0.1553  lr:0.010000
[ Wed Jul  3 13:37:19 2024 ] 	Batch(3100/7879) done. Loss: 0.2340  lr:0.010000
[ Wed Jul  3 13:37:37 2024 ] 	Batch(3200/7879) done. Loss: 0.4460  lr:0.010000
[ Wed Jul  3 13:37:55 2024 ] 	Batch(3300/7879) done. Loss: 0.8850  lr:0.010000
[ Wed Jul  3 13:38:13 2024 ] 	Batch(3400/7879) done. Loss: 1.1568  lr:0.010000
[ Wed Jul  3 13:38:30 2024 ] 
Training: Epoch [40/120], Step [3499], Loss: 0.6084975004196167, Training Accuracy: 87.87142857142857
[ Wed Jul  3 13:38:30 2024 ] 	Batch(3500/7879) done. Loss: 0.1412  lr:0.010000
[ Wed Jul  3 13:38:48 2024 ] 	Batch(3600/7879) done. Loss: 0.2751  lr:0.010000
[ Wed Jul  3 13:39:07 2024 ] 	Batch(3700/7879) done. Loss: 0.1440  lr:0.010000
[ Wed Jul  3 13:39:25 2024 ] 	Batch(3800/7879) done. Loss: 1.1957  lr:0.010000
[ Wed Jul  3 13:39:43 2024 ] 	Batch(3900/7879) done. Loss: 1.2614  lr:0.010000
[ Wed Jul  3 13:40:01 2024 ] 
Training: Epoch [40/120], Step [3999], Loss: 0.10582525283098221, Training Accuracy: 87.88125
[ Wed Jul  3 13:40:01 2024 ] 	Batch(4000/7879) done. Loss: 0.4372  lr:0.010000
[ Wed Jul  3 13:40:19 2024 ] 	Batch(4100/7879) done. Loss: 0.0266  lr:0.010000
[ Wed Jul  3 13:40:37 2024 ] 	Batch(4200/7879) done. Loss: 0.0112  lr:0.010000
[ Wed Jul  3 13:40:54 2024 ] 	Batch(4300/7879) done. Loss: 0.0908  lr:0.010000
[ Wed Jul  3 13:41:12 2024 ] 	Batch(4400/7879) done. Loss: 0.5009  lr:0.010000
[ Wed Jul  3 13:41:30 2024 ] 
Training: Epoch [40/120], Step [4499], Loss: 0.2950699031352997, Training Accuracy: 87.675
[ Wed Jul  3 13:41:30 2024 ] 	Batch(4500/7879) done. Loss: 0.2340  lr:0.010000
[ Wed Jul  3 13:41:48 2024 ] 	Batch(4600/7879) done. Loss: 0.0110  lr:0.010000
[ Wed Jul  3 13:42:06 2024 ] 	Batch(4700/7879) done. Loss: 0.2222  lr:0.010000
[ Wed Jul  3 13:42:24 2024 ] 	Batch(4800/7879) done. Loss: 0.5441  lr:0.010000
[ Wed Jul  3 13:42:42 2024 ] 	Batch(4900/7879) done. Loss: 0.7622  lr:0.010000
[ Wed Jul  3 13:43:00 2024 ] 
Training: Epoch [40/120], Step [4999], Loss: 0.5718401670455933, Training Accuracy: 87.72
[ Wed Jul  3 13:43:00 2024 ] 	Batch(5000/7879) done. Loss: 0.9928  lr:0.010000
[ Wed Jul  3 13:43:18 2024 ] 	Batch(5100/7879) done. Loss: 0.6489  lr:0.010000
[ Wed Jul  3 13:43:36 2024 ] 	Batch(5200/7879) done. Loss: 1.0304  lr:0.010000
[ Wed Jul  3 13:43:54 2024 ] 	Batch(5300/7879) done. Loss: 0.7070  lr:0.010000
[ Wed Jul  3 13:44:13 2024 ] 	Batch(5400/7879) done. Loss: 0.5177  lr:0.010000
[ Wed Jul  3 13:44:31 2024 ] 
Training: Epoch [40/120], Step [5499], Loss: 0.5587878823280334, Training Accuracy: 87.60227272727272
[ Wed Jul  3 13:44:31 2024 ] 	Batch(5500/7879) done. Loss: 0.1938  lr:0.010000
[ Wed Jul  3 13:44:50 2024 ] 	Batch(5600/7879) done. Loss: 0.1994  lr:0.010000
[ Wed Jul  3 13:45:08 2024 ] 	Batch(5700/7879) done. Loss: 0.5699  lr:0.010000
[ Wed Jul  3 13:45:27 2024 ] 	Batch(5800/7879) done. Loss: 0.5595  lr:0.010000
[ Wed Jul  3 13:45:45 2024 ] 	Batch(5900/7879) done. Loss: 0.0049  lr:0.010000
[ Wed Jul  3 13:46:04 2024 ] 
Training: Epoch [40/120], Step [5999], Loss: 0.1131831482052803, Training Accuracy: 87.52708333333334
[ Wed Jul  3 13:46:04 2024 ] 	Batch(6000/7879) done. Loss: 0.2109  lr:0.010000
[ Wed Jul  3 13:46:22 2024 ] 	Batch(6100/7879) done. Loss: 0.1660  lr:0.010000
[ Wed Jul  3 13:46:41 2024 ] 	Batch(6200/7879) done. Loss: 0.0217  lr:0.010000
[ Wed Jul  3 13:46:59 2024 ] 	Batch(6300/7879) done. Loss: 0.6190  lr:0.010000
[ Wed Jul  3 13:47:18 2024 ] 	Batch(6400/7879) done. Loss: 0.3117  lr:0.010000
[ Wed Jul  3 13:47:36 2024 ] 
Training: Epoch [40/120], Step [6499], Loss: 0.13558650016784668, Training Accuracy: 87.50192307692308
[ Wed Jul  3 13:47:36 2024 ] 	Batch(6500/7879) done. Loss: 0.3632  lr:0.010000
[ Wed Jul  3 13:47:54 2024 ] 	Batch(6600/7879) done. Loss: 0.5276  lr:0.010000
[ Wed Jul  3 13:48:11 2024 ] 	Batch(6700/7879) done. Loss: 0.3375  lr:0.010000
[ Wed Jul  3 13:48:29 2024 ] 	Batch(6800/7879) done. Loss: 0.0692  lr:0.010000
[ Wed Jul  3 13:48:47 2024 ] 	Batch(6900/7879) done. Loss: 0.0776  lr:0.010000
[ Wed Jul  3 13:49:05 2024 ] 
Training: Epoch [40/120], Step [6999], Loss: 0.24594658613204956, Training Accuracy: 87.51964285714286
[ Wed Jul  3 13:49:05 2024 ] 	Batch(7000/7879) done. Loss: 0.1713  lr:0.010000
[ Wed Jul  3 13:49:23 2024 ] 	Batch(7100/7879) done. Loss: 0.2622  lr:0.010000
[ Wed Jul  3 13:49:41 2024 ] 	Batch(7200/7879) done. Loss: 0.2130  lr:0.010000
[ Wed Jul  3 13:49:59 2024 ] 	Batch(7300/7879) done. Loss: 0.4642  lr:0.010000
[ Wed Jul  3 13:50:18 2024 ] 	Batch(7400/7879) done. Loss: 0.3808  lr:0.010000
[ Wed Jul  3 13:50:36 2024 ] 
Training: Epoch [40/120], Step [7499], Loss: 0.29466724395751953, Training Accuracy: 87.53166666666667
[ Wed Jul  3 13:50:36 2024 ] 	Batch(7500/7879) done. Loss: 0.2350  lr:0.010000
[ Wed Jul  3 13:50:55 2024 ] 	Batch(7600/7879) done. Loss: 0.0846  lr:0.010000
[ Wed Jul  3 13:51:14 2024 ] 	Batch(7700/7879) done. Loss: 1.4163  lr:0.010000
[ Wed Jul  3 13:51:32 2024 ] 	Batch(7800/7879) done. Loss: 0.1155  lr:0.010000
[ Wed Jul  3 13:51:47 2024 ] 	Mean training loss: 0.4099.
[ Wed Jul  3 13:51:47 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 13:51:47 2024 ] Training epoch: 42
[ Wed Jul  3 13:51:47 2024 ] 	Batch(0/7879) done. Loss: 0.3861  lr:0.010000
[ Wed Jul  3 13:52:05 2024 ] 	Batch(100/7879) done. Loss: 0.4220  lr:0.010000
[ Wed Jul  3 13:52:23 2024 ] 	Batch(200/7879) done. Loss: 0.0269  lr:0.010000
[ Wed Jul  3 13:52:41 2024 ] 	Batch(300/7879) done. Loss: 0.2072  lr:0.010000
[ Wed Jul  3 13:52:59 2024 ] 	Batch(400/7879) done. Loss: 0.0873  lr:0.010000
[ Wed Jul  3 13:53:17 2024 ] 
Training: Epoch [41/120], Step [499], Loss: 0.24793966114521027, Training Accuracy: 88.125
[ Wed Jul  3 13:53:17 2024 ] 	Batch(500/7879) done. Loss: 0.1967  lr:0.010000
[ Wed Jul  3 13:53:35 2024 ] 	Batch(600/7879) done. Loss: 0.1514  lr:0.010000
[ Wed Jul  3 13:53:53 2024 ] 	Batch(700/7879) done. Loss: 0.2133  lr:0.010000
[ Wed Jul  3 13:54:11 2024 ] 	Batch(800/7879) done. Loss: 0.4024  lr:0.010000
[ Wed Jul  3 13:54:29 2024 ] 	Batch(900/7879) done. Loss: 0.0161  lr:0.010000
[ Wed Jul  3 13:54:47 2024 ] 
Training: Epoch [41/120], Step [999], Loss: 0.5051752328872681, Training Accuracy: 88.7
[ Wed Jul  3 13:54:47 2024 ] 	Batch(1000/7879) done. Loss: 0.5887  lr:0.010000
[ Wed Jul  3 13:55:06 2024 ] 	Batch(1100/7879) done. Loss: 0.6611  lr:0.010000
[ Wed Jul  3 13:55:25 2024 ] 	Batch(1200/7879) done. Loss: 0.8578  lr:0.010000
[ Wed Jul  3 13:55:43 2024 ] 	Batch(1300/7879) done. Loss: 0.2380  lr:0.010000
[ Wed Jul  3 13:56:02 2024 ] 	Batch(1400/7879) done. Loss: 0.1379  lr:0.010000
[ Wed Jul  3 13:56:20 2024 ] 
Training: Epoch [41/120], Step [1499], Loss: 0.046606920659542084, Training Accuracy: 88.46666666666667
[ Wed Jul  3 13:56:20 2024 ] 	Batch(1500/7879) done. Loss: 0.6414  lr:0.010000
[ Wed Jul  3 13:56:39 2024 ] 	Batch(1600/7879) done. Loss: 0.1271  lr:0.010000
[ Wed Jul  3 13:56:57 2024 ] 	Batch(1700/7879) done. Loss: 0.1687  lr:0.010000
[ Wed Jul  3 13:57:14 2024 ] 	Batch(1800/7879) done. Loss: 0.3515  lr:0.010000
[ Wed Jul  3 13:57:32 2024 ] 	Batch(1900/7879) done. Loss: 0.1705  lr:0.010000
[ Wed Jul  3 13:57:50 2024 ] 
Training: Epoch [41/120], Step [1999], Loss: 0.024823293089866638, Training Accuracy: 88.5625
[ Wed Jul  3 13:57:50 2024 ] 	Batch(2000/7879) done. Loss: 0.2126  lr:0.010000
[ Wed Jul  3 13:58:08 2024 ] 	Batch(2100/7879) done. Loss: 0.3733  lr:0.010000
[ Wed Jul  3 13:58:26 2024 ] 	Batch(2200/7879) done. Loss: 0.0242  lr:0.010000
[ Wed Jul  3 13:58:44 2024 ] 	Batch(2300/7879) done. Loss: 0.3351  lr:0.010000
[ Wed Jul  3 13:59:02 2024 ] 	Batch(2400/7879) done. Loss: 0.2066  lr:0.010000
[ Wed Jul  3 13:59:19 2024 ] 
Training: Epoch [41/120], Step [2499], Loss: 0.4333864748477936, Training Accuracy: 88.44
[ Wed Jul  3 13:59:20 2024 ] 	Batch(2500/7879) done. Loss: 0.5997  lr:0.010000
[ Wed Jul  3 13:59:37 2024 ] 	Batch(2600/7879) done. Loss: 0.2220  lr:0.010000
[ Wed Jul  3 13:59:55 2024 ] 	Batch(2700/7879) done. Loss: 0.6345  lr:0.010000
[ Wed Jul  3 14:00:14 2024 ] 	Batch(2800/7879) done. Loss: 0.4195  lr:0.010000
[ Wed Jul  3 14:00:31 2024 ] 	Batch(2900/7879) done. Loss: 0.1288  lr:0.010000
[ Wed Jul  3 14:00:49 2024 ] 
Training: Epoch [41/120], Step [2999], Loss: 0.2609880566596985, Training Accuracy: 88.41666666666667
[ Wed Jul  3 14:00:49 2024 ] 	Batch(3000/7879) done. Loss: 0.4310  lr:0.010000
[ Wed Jul  3 14:01:07 2024 ] 	Batch(3100/7879) done. Loss: 0.1620  lr:0.010000
[ Wed Jul  3 14:01:25 2024 ] 	Batch(3200/7879) done. Loss: 0.1375  lr:0.010000
[ Wed Jul  3 14:01:43 2024 ] 	Batch(3300/7879) done. Loss: 0.4706  lr:0.010000
[ Wed Jul  3 14:02:01 2024 ] 	Batch(3400/7879) done. Loss: 0.0758  lr:0.010000
[ Wed Jul  3 14:02:19 2024 ] 
Training: Epoch [41/120], Step [3499], Loss: 0.32639098167419434, Training Accuracy: 88.26071428571429
[ Wed Jul  3 14:02:19 2024 ] 	Batch(3500/7879) done. Loss: 0.5619  lr:0.010000
[ Wed Jul  3 14:02:37 2024 ] 	Batch(3600/7879) done. Loss: 0.2359  lr:0.010000
[ Wed Jul  3 14:02:55 2024 ] 	Batch(3700/7879) done. Loss: 0.2524  lr:0.010000
[ Wed Jul  3 14:03:13 2024 ] 	Batch(3800/7879) done. Loss: 0.4636  lr:0.010000
[ Wed Jul  3 14:03:31 2024 ] 	Batch(3900/7879) done. Loss: 0.5403  lr:0.010000
[ Wed Jul  3 14:03:49 2024 ] 
Training: Epoch [41/120], Step [3999], Loss: 0.10097718983888626, Training Accuracy: 88.16874999999999
[ Wed Jul  3 14:03:49 2024 ] 	Batch(4000/7879) done. Loss: 0.1913  lr:0.010000
[ Wed Jul  3 14:04:08 2024 ] 	Batch(4100/7879) done. Loss: 0.2843  lr:0.010000
[ Wed Jul  3 14:04:26 2024 ] 	Batch(4200/7879) done. Loss: 0.3713  lr:0.010000
[ Wed Jul  3 14:04:44 2024 ] 	Batch(4300/7879) done. Loss: 0.7462  lr:0.010000
[ Wed Jul  3 14:05:02 2024 ] 	Batch(4400/7879) done. Loss: 0.3786  lr:0.010000
[ Wed Jul  3 14:05:20 2024 ] 
Training: Epoch [41/120], Step [4499], Loss: 0.0013237674720585346, Training Accuracy: 88.06388888888888
[ Wed Jul  3 14:05:20 2024 ] 	Batch(4500/7879) done. Loss: 0.4493  lr:0.010000
[ Wed Jul  3 14:05:38 2024 ] 	Batch(4600/7879) done. Loss: 0.0088  lr:0.010000
[ Wed Jul  3 14:05:56 2024 ] 	Batch(4700/7879) done. Loss: 0.4613  lr:0.010000
[ Wed Jul  3 14:06:14 2024 ] 	Batch(4800/7879) done. Loss: 0.1945  lr:0.010000
[ Wed Jul  3 14:06:33 2024 ] 	Batch(4900/7879) done. Loss: 0.3040  lr:0.010000
[ Wed Jul  3 14:06:51 2024 ] 
Training: Epoch [41/120], Step [4999], Loss: 0.10228325426578522, Training Accuracy: 87.9325
[ Wed Jul  3 14:06:51 2024 ] 	Batch(5000/7879) done. Loss: 0.7645  lr:0.010000
[ Wed Jul  3 14:07:09 2024 ] 	Batch(5100/7879) done. Loss: 0.0728  lr:0.010000
[ Wed Jul  3 14:07:27 2024 ] 	Batch(5200/7879) done. Loss: 0.3453  lr:0.010000
[ Wed Jul  3 14:07:45 2024 ] 	Batch(5300/7879) done. Loss: 0.5176  lr:0.010000
[ Wed Jul  3 14:08:03 2024 ] 	Batch(5400/7879) done. Loss: 0.0487  lr:0.010000
[ Wed Jul  3 14:08:22 2024 ] 
Training: Epoch [41/120], Step [5499], Loss: 0.6305162906646729, Training Accuracy: 87.94090909090909
[ Wed Jul  3 14:08:22 2024 ] 	Batch(5500/7879) done. Loss: 0.7417  lr:0.010000
[ Wed Jul  3 14:08:40 2024 ] 	Batch(5600/7879) done. Loss: 0.2942  lr:0.010000
[ Wed Jul  3 14:08:58 2024 ] 	Batch(5700/7879) done. Loss: 0.3843  lr:0.010000
[ Wed Jul  3 14:09:16 2024 ] 	Batch(5800/7879) done. Loss: 0.9370  lr:0.010000
[ Wed Jul  3 14:09:34 2024 ] 	Batch(5900/7879) done. Loss: 0.4712  lr:0.010000
[ Wed Jul  3 14:09:52 2024 ] 
Training: Epoch [41/120], Step [5999], Loss: 0.1135874018073082, Training Accuracy: 87.88333333333334
[ Wed Jul  3 14:09:52 2024 ] 	Batch(6000/7879) done. Loss: 0.0326  lr:0.010000
[ Wed Jul  3 14:10:10 2024 ] 	Batch(6100/7879) done. Loss: 0.0113  lr:0.010000
[ Wed Jul  3 14:10:28 2024 ] 	Batch(6200/7879) done. Loss: 0.4249  lr:0.010000
[ Wed Jul  3 14:10:46 2024 ] 	Batch(6300/7879) done. Loss: 0.7191  lr:0.010000
[ Wed Jul  3 14:11:05 2024 ] 	Batch(6400/7879) done. Loss: 0.3633  lr:0.010000
[ Wed Jul  3 14:11:22 2024 ] 
Training: Epoch [41/120], Step [6499], Loss: 0.040787871927022934, Training Accuracy: 87.80192307692307
[ Wed Jul  3 14:11:23 2024 ] 	Batch(6500/7879) done. Loss: 0.7326  lr:0.010000
[ Wed Jul  3 14:11:41 2024 ] 	Batch(6600/7879) done. Loss: 0.6411  lr:0.010000
[ Wed Jul  3 14:11:59 2024 ] 	Batch(6700/7879) done. Loss: 0.2420  lr:0.010000
[ Wed Jul  3 14:12:18 2024 ] 	Batch(6800/7879) done. Loss: 0.5844  lr:0.010000
[ Wed Jul  3 14:12:36 2024 ] 	Batch(6900/7879) done. Loss: 0.3192  lr:0.010000
[ Wed Jul  3 14:12:54 2024 ] 
Training: Epoch [41/120], Step [6999], Loss: 1.3324124813079834, Training Accuracy: 87.73571428571428
[ Wed Jul  3 14:12:54 2024 ] 	Batch(7000/7879) done. Loss: 0.6184  lr:0.010000
[ Wed Jul  3 14:13:12 2024 ] 	Batch(7100/7879) done. Loss: 0.4749  lr:0.010000
[ Wed Jul  3 14:13:30 2024 ] 	Batch(7200/7879) done. Loss: 0.9081  lr:0.010000
[ Wed Jul  3 14:13:48 2024 ] 	Batch(7300/7879) done. Loss: 0.1810  lr:0.010000
[ Wed Jul  3 14:14:07 2024 ] 	Batch(7400/7879) done. Loss: 0.6500  lr:0.010000
[ Wed Jul  3 14:14:25 2024 ] 
Training: Epoch [41/120], Step [7499], Loss: 0.3350425660610199, Training Accuracy: 87.69666666666667
[ Wed Jul  3 14:14:25 2024 ] 	Batch(7500/7879) done. Loss: 0.0412  lr:0.010000
[ Wed Jul  3 14:14:43 2024 ] 	Batch(7600/7879) done. Loss: 0.4540  lr:0.010000
[ Wed Jul  3 14:15:01 2024 ] 	Batch(7700/7879) done. Loss: 0.2790  lr:0.010000
[ Wed Jul  3 14:15:19 2024 ] 	Batch(7800/7879) done. Loss: 0.4925  lr:0.010000
[ Wed Jul  3 14:15:34 2024 ] 	Mean training loss: 0.4001.
[ Wed Jul  3 14:15:34 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 14:15:34 2024 ] Training epoch: 43
[ Wed Jul  3 14:15:34 2024 ] 	Batch(0/7879) done. Loss: 0.0645  lr:0.010000
[ Wed Jul  3 14:15:52 2024 ] 	Batch(100/7879) done. Loss: 0.5145  lr:0.010000
[ Wed Jul  3 14:16:10 2024 ] 	Batch(200/7879) done. Loss: 0.0276  lr:0.010000
[ Wed Jul  3 14:16:28 2024 ] 	Batch(300/7879) done. Loss: 0.0227  lr:0.010000
[ Wed Jul  3 14:16:46 2024 ] 	Batch(400/7879) done. Loss: 0.0407  lr:0.010000
[ Wed Jul  3 14:17:04 2024 ] 
Training: Epoch [42/120], Step [499], Loss: 0.6564003229141235, Training Accuracy: 89.2
[ Wed Jul  3 14:17:04 2024 ] 	Batch(500/7879) done. Loss: 0.1207  lr:0.010000
[ Wed Jul  3 14:17:22 2024 ] 	Batch(600/7879) done. Loss: 0.7424  lr:0.010000
[ Wed Jul  3 14:17:40 2024 ] 	Batch(700/7879) done. Loss: 0.0193  lr:0.010000
[ Wed Jul  3 14:17:59 2024 ] 	Batch(800/7879) done. Loss: 0.2073  lr:0.010000
[ Wed Jul  3 14:18:17 2024 ] 	Batch(900/7879) done. Loss: 0.3817  lr:0.010000
[ Wed Jul  3 14:18:34 2024 ] 
Training: Epoch [42/120], Step [999], Loss: 0.0918184220790863, Training Accuracy: 89.4125
[ Wed Jul  3 14:18:34 2024 ] 	Batch(1000/7879) done. Loss: 0.4973  lr:0.010000
[ Wed Jul  3 14:18:52 2024 ] 	Batch(1100/7879) done. Loss: 0.0422  lr:0.010000
[ Wed Jul  3 14:19:10 2024 ] 	Batch(1200/7879) done. Loss: 0.0103  lr:0.010000
[ Wed Jul  3 14:19:28 2024 ] 	Batch(1300/7879) done. Loss: 1.4810  lr:0.010000
[ Wed Jul  3 14:19:46 2024 ] 	Batch(1400/7879) done. Loss: 0.1086  lr:0.010000
[ Wed Jul  3 14:20:04 2024 ] 
Training: Epoch [42/120], Step [1499], Loss: 0.240401491522789, Training Accuracy: 89.48333333333333
[ Wed Jul  3 14:20:04 2024 ] 	Batch(1500/7879) done. Loss: 0.3881  lr:0.010000
[ Wed Jul  3 14:20:22 2024 ] 	Batch(1600/7879) done. Loss: 0.3332  lr:0.010000
[ Wed Jul  3 14:20:41 2024 ] 	Batch(1700/7879) done. Loss: 0.0530  lr:0.010000
[ Wed Jul  3 14:20:59 2024 ] 	Batch(1800/7879) done. Loss: 0.3628  lr:0.010000
[ Wed Jul  3 14:21:17 2024 ] 	Batch(1900/7879) done. Loss: 0.0916  lr:0.010000
[ Wed Jul  3 14:21:35 2024 ] 
Training: Epoch [42/120], Step [1999], Loss: 0.05083687603473663, Training Accuracy: 89.375
[ Wed Jul  3 14:21:35 2024 ] 	Batch(2000/7879) done. Loss: 0.0960  lr:0.010000
[ Wed Jul  3 14:21:53 2024 ] 	Batch(2100/7879) done. Loss: 0.3993  lr:0.010000
[ Wed Jul  3 14:22:11 2024 ] 	Batch(2200/7879) done. Loss: 0.1877  lr:0.010000
[ Wed Jul  3 14:22:29 2024 ] 	Batch(2300/7879) done. Loss: 0.6360  lr:0.010000
[ Wed Jul  3 14:22:47 2024 ] 	Batch(2400/7879) done. Loss: 0.8180  lr:0.010000
[ Wed Jul  3 14:23:05 2024 ] 
Training: Epoch [42/120], Step [2499], Loss: 0.36096665263175964, Training Accuracy: 89.21
[ Wed Jul  3 14:23:05 2024 ] 	Batch(2500/7879) done. Loss: 0.4349  lr:0.010000
[ Wed Jul  3 14:23:23 2024 ] 	Batch(2600/7879) done. Loss: 0.1910  lr:0.010000
[ Wed Jul  3 14:23:40 2024 ] 	Batch(2700/7879) done. Loss: 0.0661  lr:0.010000
[ Wed Jul  3 14:23:58 2024 ] 	Batch(2800/7879) done. Loss: 1.1453  lr:0.010000
[ Wed Jul  3 14:24:16 2024 ] 	Batch(2900/7879) done. Loss: 0.3330  lr:0.010000
[ Wed Jul  3 14:24:34 2024 ] 
Training: Epoch [42/120], Step [2999], Loss: 0.3062511086463928, Training Accuracy: 89.04166666666666
[ Wed Jul  3 14:24:34 2024 ] 	Batch(3000/7879) done. Loss: 0.4223  lr:0.010000
[ Wed Jul  3 14:24:52 2024 ] 	Batch(3100/7879) done. Loss: 0.1549  lr:0.010000
[ Wed Jul  3 14:25:10 2024 ] 	Batch(3200/7879) done. Loss: 0.1708  lr:0.010000
[ Wed Jul  3 14:25:28 2024 ] 	Batch(3300/7879) done. Loss: 0.0588  lr:0.010000
[ Wed Jul  3 14:25:46 2024 ] 	Batch(3400/7879) done. Loss: 0.4972  lr:0.010000
[ Wed Jul  3 14:26:04 2024 ] 
Training: Epoch [42/120], Step [3499], Loss: 1.4598501920700073, Training Accuracy: 88.77857142857142
[ Wed Jul  3 14:26:04 2024 ] 	Batch(3500/7879) done. Loss: 0.5631  lr:0.010000
[ Wed Jul  3 14:26:22 2024 ] 	Batch(3600/7879) done. Loss: 0.7050  lr:0.010000
[ Wed Jul  3 14:26:40 2024 ] 	Batch(3700/7879) done. Loss: 0.8299  lr:0.010000
[ Wed Jul  3 14:26:57 2024 ] 	Batch(3800/7879) done. Loss: 0.4883  lr:0.010000
[ Wed Jul  3 14:27:15 2024 ] 	Batch(3900/7879) done. Loss: 0.6219  lr:0.010000
[ Wed Jul  3 14:27:33 2024 ] 
Training: Epoch [42/120], Step [3999], Loss: 1.020186185836792, Training Accuracy: 88.459375
[ Wed Jul  3 14:27:33 2024 ] 	Batch(4000/7879) done. Loss: 0.3189  lr:0.010000
[ Wed Jul  3 14:27:52 2024 ] 	Batch(4100/7879) done. Loss: 0.8277  lr:0.010000
[ Wed Jul  3 14:28:10 2024 ] 	Batch(4200/7879) done. Loss: 0.3797  lr:0.010000
[ Wed Jul  3 14:28:29 2024 ] 	Batch(4300/7879) done. Loss: 0.5693  lr:0.010000
[ Wed Jul  3 14:28:48 2024 ] 	Batch(4400/7879) done. Loss: 0.3305  lr:0.010000
[ Wed Jul  3 14:29:05 2024 ] 
Training: Epoch [42/120], Step [4499], Loss: 0.20188511908054352, Training Accuracy: 88.3861111111111
[ Wed Jul  3 14:29:05 2024 ] 	Batch(4500/7879) done. Loss: 0.2526  lr:0.010000
[ Wed Jul  3 14:29:23 2024 ] 	Batch(4600/7879) done. Loss: 0.0364  lr:0.010000
[ Wed Jul  3 14:29:41 2024 ] 	Batch(4700/7879) done. Loss: 0.4655  lr:0.010000
[ Wed Jul  3 14:29:59 2024 ] 	Batch(4800/7879) done. Loss: 0.3473  lr:0.010000
[ Wed Jul  3 14:30:17 2024 ] 	Batch(4900/7879) done. Loss: 0.2535  lr:0.010000
[ Wed Jul  3 14:30:35 2024 ] 
Training: Epoch [42/120], Step [4999], Loss: 0.03587732091546059, Training Accuracy: 88.3425
[ Wed Jul  3 14:30:35 2024 ] 	Batch(5000/7879) done. Loss: 0.3656  lr:0.010000
[ Wed Jul  3 14:30:53 2024 ] 	Batch(5100/7879) done. Loss: 0.3498  lr:0.010000
[ Wed Jul  3 14:31:11 2024 ] 	Batch(5200/7879) done. Loss: 0.0874  lr:0.010000
[ Wed Jul  3 14:31:29 2024 ] 	Batch(5300/7879) done. Loss: 0.0980  lr:0.010000
[ Wed Jul  3 14:31:48 2024 ] 	Batch(5400/7879) done. Loss: 0.6855  lr:0.010000
[ Wed Jul  3 14:32:06 2024 ] 
Training: Epoch [42/120], Step [5499], Loss: 0.16071447730064392, Training Accuracy: 88.23409090909091
[ Wed Jul  3 14:32:06 2024 ] 	Batch(5500/7879) done. Loss: 0.1084  lr:0.010000
[ Wed Jul  3 14:32:25 2024 ] 	Batch(5600/7879) done. Loss: 0.4082  lr:0.010000
[ Wed Jul  3 14:32:43 2024 ] 	Batch(5700/7879) done. Loss: 0.1967  lr:0.010000
[ Wed Jul  3 14:33:02 2024 ] 	Batch(5800/7879) done. Loss: 0.5125  lr:0.010000
[ Wed Jul  3 14:33:20 2024 ] 	Batch(5900/7879) done. Loss: 0.3361  lr:0.010000
[ Wed Jul  3 14:33:38 2024 ] 
Training: Epoch [42/120], Step [5999], Loss: 0.19407927989959717, Training Accuracy: 88.20625
[ Wed Jul  3 14:33:38 2024 ] 	Batch(6000/7879) done. Loss: 0.5396  lr:0.010000
[ Wed Jul  3 14:33:56 2024 ] 	Batch(6100/7879) done. Loss: 0.7020  lr:0.010000
[ Wed Jul  3 14:34:14 2024 ] 	Batch(6200/7879) done. Loss: 0.0564  lr:0.010000
[ Wed Jul  3 14:34:32 2024 ] 	Batch(6300/7879) done. Loss: 0.9617  lr:0.010000
[ Wed Jul  3 14:34:50 2024 ] 	Batch(6400/7879) done. Loss: 0.2012  lr:0.010000
[ Wed Jul  3 14:35:07 2024 ] 
Training: Epoch [42/120], Step [6499], Loss: 0.045507144182920456, Training Accuracy: 88.12884615384615
[ Wed Jul  3 14:35:08 2024 ] 	Batch(6500/7879) done. Loss: 0.9256  lr:0.010000
[ Wed Jul  3 14:35:26 2024 ] 	Batch(6600/7879) done. Loss: 0.1654  lr:0.010000
[ Wed Jul  3 14:35:43 2024 ] 	Batch(6700/7879) done. Loss: 0.1077  lr:0.010000
[ Wed Jul  3 14:36:01 2024 ] 	Batch(6800/7879) done. Loss: 0.1598  lr:0.010000
[ Wed Jul  3 14:36:20 2024 ] 	Batch(6900/7879) done. Loss: 1.3995  lr:0.010000
[ Wed Jul  3 14:36:38 2024 ] 
Training: Epoch [42/120], Step [6999], Loss: 0.2490309625864029, Training Accuracy: 88.0
[ Wed Jul  3 14:36:38 2024 ] 	Batch(7000/7879) done. Loss: 1.5245  lr:0.010000
[ Wed Jul  3 14:36:56 2024 ] 	Batch(7100/7879) done. Loss: 0.2607  lr:0.010000
[ Wed Jul  3 14:37:14 2024 ] 	Batch(7200/7879) done. Loss: 0.3733  lr:0.010000
[ Wed Jul  3 14:37:32 2024 ] 	Batch(7300/7879) done. Loss: 0.0094  lr:0.010000
[ Wed Jul  3 14:37:51 2024 ] 	Batch(7400/7879) done. Loss: 0.2372  lr:0.010000
[ Wed Jul  3 14:38:09 2024 ] 
Training: Epoch [42/120], Step [7499], Loss: 0.11286018788814545, Training Accuracy: 87.935
[ Wed Jul  3 14:38:09 2024 ] 	Batch(7500/7879) done. Loss: 0.2997  lr:0.010000
[ Wed Jul  3 14:38:28 2024 ] 	Batch(7600/7879) done. Loss: 0.0502  lr:0.010000
[ Wed Jul  3 14:38:46 2024 ] 	Batch(7700/7879) done. Loss: 0.3715  lr:0.010000
[ Wed Jul  3 14:39:04 2024 ] 	Batch(7800/7879) done. Loss: 0.0958  lr:0.010000
[ Wed Jul  3 14:39:18 2024 ] 	Mean training loss: 0.3935.
[ Wed Jul  3 14:39:18 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 14:39:18 2024 ] Training epoch: 44
[ Wed Jul  3 14:39:19 2024 ] 	Batch(0/7879) done. Loss: 0.0301  lr:0.010000
[ Wed Jul  3 14:39:37 2024 ] 	Batch(100/7879) done. Loss: 0.6410  lr:0.010000
[ Wed Jul  3 14:39:55 2024 ] 	Batch(200/7879) done. Loss: 0.9193  lr:0.010000
[ Wed Jul  3 14:40:14 2024 ] 	Batch(300/7879) done. Loss: 0.0296  lr:0.010000
[ Wed Jul  3 14:40:32 2024 ] 	Batch(400/7879) done. Loss: 0.5202  lr:0.010000
[ Wed Jul  3 14:40:50 2024 ] 
Training: Epoch [43/120], Step [499], Loss: 0.7164604067802429, Training Accuracy: 89.225
[ Wed Jul  3 14:40:50 2024 ] 	Batch(500/7879) done. Loss: 0.2956  lr:0.010000
[ Wed Jul  3 14:41:08 2024 ] 	Batch(600/7879) done. Loss: 0.1490  lr:0.010000
[ Wed Jul  3 14:41:26 2024 ] 	Batch(700/7879) done. Loss: 0.2595  lr:0.010000
[ Wed Jul  3 14:41:44 2024 ] 	Batch(800/7879) done. Loss: 0.8597  lr:0.010000
[ Wed Jul  3 14:42:02 2024 ] 	Batch(900/7879) done. Loss: 0.5778  lr:0.010000
[ Wed Jul  3 14:42:19 2024 ] 
Training: Epoch [43/120], Step [999], Loss: 0.36358126997947693, Training Accuracy: 88.53750000000001
[ Wed Jul  3 14:42:19 2024 ] 	Batch(1000/7879) done. Loss: 0.0515  lr:0.010000
[ Wed Jul  3 14:42:37 2024 ] 	Batch(1100/7879) done. Loss: 0.2255  lr:0.010000
[ Wed Jul  3 14:42:55 2024 ] 	Batch(1200/7879) done. Loss: 0.5231  lr:0.010000
[ Wed Jul  3 14:43:14 2024 ] 	Batch(1300/7879) done. Loss: 0.0855  lr:0.010000
[ Wed Jul  3 14:43:32 2024 ] 	Batch(1400/7879) done. Loss: 0.1984  lr:0.010000
[ Wed Jul  3 14:43:51 2024 ] 
Training: Epoch [43/120], Step [1499], Loss: 0.1249413713812828, Training Accuracy: 88.43333333333334
[ Wed Jul  3 14:43:51 2024 ] 	Batch(1500/7879) done. Loss: 0.6332  lr:0.010000
[ Wed Jul  3 14:44:10 2024 ] 	Batch(1600/7879) done. Loss: 0.5755  lr:0.010000
[ Wed Jul  3 14:44:27 2024 ] 	Batch(1700/7879) done. Loss: 0.4720  lr:0.010000
[ Wed Jul  3 14:44:45 2024 ] 	Batch(1800/7879) done. Loss: 0.8315  lr:0.010000
[ Wed Jul  3 14:45:03 2024 ] 	Batch(1900/7879) done. Loss: 0.1355  lr:0.010000
[ Wed Jul  3 14:45:21 2024 ] 
Training: Epoch [43/120], Step [1999], Loss: 0.5082107782363892, Training Accuracy: 88.35
[ Wed Jul  3 14:45:21 2024 ] 	Batch(2000/7879) done. Loss: 0.3188  lr:0.010000
[ Wed Jul  3 14:45:39 2024 ] 	Batch(2100/7879) done. Loss: 0.0417  lr:0.010000
[ Wed Jul  3 14:45:57 2024 ] 	Batch(2200/7879) done. Loss: 0.4895  lr:0.010000
[ Wed Jul  3 14:46:15 2024 ] 	Batch(2300/7879) done. Loss: 0.0607  lr:0.010000
[ Wed Jul  3 14:46:33 2024 ] 	Batch(2400/7879) done. Loss: 1.0178  lr:0.010000
[ Wed Jul  3 14:46:51 2024 ] 
Training: Epoch [43/120], Step [2499], Loss: 1.0859532356262207, Training Accuracy: 88.28
[ Wed Jul  3 14:46:51 2024 ] 	Batch(2500/7879) done. Loss: 0.4725  lr:0.010000
[ Wed Jul  3 14:47:09 2024 ] 	Batch(2600/7879) done. Loss: 0.2103  lr:0.010000
[ Wed Jul  3 14:47:27 2024 ] 	Batch(2700/7879) done. Loss: 0.1669  lr:0.010000
[ Wed Jul  3 14:47:45 2024 ] 	Batch(2800/7879) done. Loss: 0.5628  lr:0.010000
[ Wed Jul  3 14:48:02 2024 ] 	Batch(2900/7879) done. Loss: 0.3318  lr:0.010000
[ Wed Jul  3 14:48:20 2024 ] 
Training: Epoch [43/120], Step [2999], Loss: 0.5020509362220764, Training Accuracy: 88.25833333333334
[ Wed Jul  3 14:48:20 2024 ] 	Batch(3000/7879) done. Loss: 0.3417  lr:0.010000
[ Wed Jul  3 14:48:38 2024 ] 	Batch(3100/7879) done. Loss: 0.0362  lr:0.010000
[ Wed Jul  3 14:48:56 2024 ] 	Batch(3200/7879) done. Loss: 0.0435  lr:0.010000
[ Wed Jul  3 14:49:14 2024 ] 	Batch(3300/7879) done. Loss: 0.0869  lr:0.010000
[ Wed Jul  3 14:49:32 2024 ] 	Batch(3400/7879) done. Loss: 0.3689  lr:0.010000
[ Wed Jul  3 14:49:50 2024 ] 
Training: Epoch [43/120], Step [3499], Loss: 0.08291713893413544, Training Accuracy: 88.35357142857143
[ Wed Jul  3 14:49:50 2024 ] 	Batch(3500/7879) done. Loss: 0.3240  lr:0.010000
[ Wed Jul  3 14:50:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0396  lr:0.010000
[ Wed Jul  3 14:50:26 2024 ] 	Batch(3700/7879) done. Loss: 0.6705  lr:0.010000
[ Wed Jul  3 14:50:45 2024 ] 	Batch(3800/7879) done. Loss: 1.1220  lr:0.010000
[ Wed Jul  3 14:51:03 2024 ] 	Batch(3900/7879) done. Loss: 0.2386  lr:0.010000
[ Wed Jul  3 14:51:22 2024 ] 
Training: Epoch [43/120], Step [3999], Loss: 0.35254526138305664, Training Accuracy: 88.234375
[ Wed Jul  3 14:51:22 2024 ] 	Batch(4000/7879) done. Loss: 0.9510  lr:0.010000
[ Wed Jul  3 14:51:40 2024 ] 	Batch(4100/7879) done. Loss: 0.1817  lr:0.010000
[ Wed Jul  3 14:51:58 2024 ] 	Batch(4200/7879) done. Loss: 0.0503  lr:0.010000
[ Wed Jul  3 14:52:16 2024 ] 	Batch(4300/7879) done. Loss: 0.2376  lr:0.010000
[ Wed Jul  3 14:52:34 2024 ] 	Batch(4400/7879) done. Loss: 0.3441  lr:0.010000
[ Wed Jul  3 14:52:51 2024 ] 
Training: Epoch [43/120], Step [4499], Loss: 1.1364288330078125, Training Accuracy: 88.28055555555555
[ Wed Jul  3 14:52:52 2024 ] 	Batch(4500/7879) done. Loss: 0.9550  lr:0.010000
[ Wed Jul  3 14:53:09 2024 ] 	Batch(4600/7879) done. Loss: 0.0897  lr:0.010000
[ Wed Jul  3 14:53:27 2024 ] 	Batch(4700/7879) done. Loss: 0.9594  lr:0.010000
[ Wed Jul  3 14:53:45 2024 ] 	Batch(4800/7879) done. Loss: 0.7341  lr:0.010000
[ Wed Jul  3 14:54:03 2024 ] 	Batch(4900/7879) done. Loss: 0.5010  lr:0.010000
[ Wed Jul  3 14:54:21 2024 ] 
Training: Epoch [43/120], Step [4999], Loss: 1.2339977025985718, Training Accuracy: 88.235
[ Wed Jul  3 14:54:21 2024 ] 	Batch(5000/7879) done. Loss: 0.0226  lr:0.010000
[ Wed Jul  3 14:54:39 2024 ] 	Batch(5100/7879) done. Loss: 0.1572  lr:0.010000
[ Wed Jul  3 14:54:57 2024 ] 	Batch(5200/7879) done. Loss: 0.3184  lr:0.010000
[ Wed Jul  3 14:55:15 2024 ] 	Batch(5300/7879) done. Loss: 0.4779  lr:0.010000
[ Wed Jul  3 14:55:33 2024 ] 	Batch(5400/7879) done. Loss: 0.2615  lr:0.010000
[ Wed Jul  3 14:55:51 2024 ] 
Training: Epoch [43/120], Step [5499], Loss: 1.5573461055755615, Training Accuracy: 88.04318181818182
[ Wed Jul  3 14:55:51 2024 ] 	Batch(5500/7879) done. Loss: 0.4253  lr:0.010000
[ Wed Jul  3 14:56:10 2024 ] 	Batch(5600/7879) done. Loss: 0.3275  lr:0.010000
[ Wed Jul  3 14:56:28 2024 ] 	Batch(5700/7879) done. Loss: 0.0542  lr:0.010000
[ Wed Jul  3 14:56:47 2024 ] 	Batch(5800/7879) done. Loss: 0.0158  lr:0.010000
[ Wed Jul  3 14:57:05 2024 ] 	Batch(5900/7879) done. Loss: 0.1277  lr:0.010000
[ Wed Jul  3 14:57:23 2024 ] 
Training: Epoch [43/120], Step [5999], Loss: 0.23366859555244446, Training Accuracy: 88.075
[ Wed Jul  3 14:57:24 2024 ] 	Batch(6000/7879) done. Loss: 0.3736  lr:0.010000
[ Wed Jul  3 14:57:42 2024 ] 	Batch(6100/7879) done. Loss: 0.0600  lr:0.010000
[ Wed Jul  3 14:58:01 2024 ] 	Batch(6200/7879) done. Loss: 0.1595  lr:0.010000
[ Wed Jul  3 14:58:19 2024 ] 	Batch(6300/7879) done. Loss: 0.0391  lr:0.010000
[ Wed Jul  3 14:58:38 2024 ] 	Batch(6400/7879) done. Loss: 0.5018  lr:0.010000
[ Wed Jul  3 14:58:56 2024 ] 
Training: Epoch [43/120], Step [6499], Loss: 1.0643727779388428, Training Accuracy: 88.00769230769231
[ Wed Jul  3 14:58:56 2024 ] 	Batch(6500/7879) done. Loss: 0.7080  lr:0.010000
[ Wed Jul  3 14:59:14 2024 ] 	Batch(6600/7879) done. Loss: 0.2009  lr:0.010000
[ Wed Jul  3 14:59:31 2024 ] 	Batch(6700/7879) done. Loss: 0.3730  lr:0.010000
[ Wed Jul  3 14:59:49 2024 ] 	Batch(6800/7879) done. Loss: 1.3643  lr:0.010000
[ Wed Jul  3 15:00:08 2024 ] 	Batch(6900/7879) done. Loss: 0.1926  lr:0.010000
[ Wed Jul  3 15:00:26 2024 ] 
Training: Epoch [43/120], Step [6999], Loss: 0.342921644449234, Training Accuracy: 87.86607142857143
[ Wed Jul  3 15:00:26 2024 ] 	Batch(7000/7879) done. Loss: 0.0501  lr:0.010000
[ Wed Jul  3 15:00:45 2024 ] 	Batch(7100/7879) done. Loss: 0.4766  lr:0.010000
[ Wed Jul  3 15:01:04 2024 ] 	Batch(7200/7879) done. Loss: 0.3730  lr:0.010000
[ Wed Jul  3 15:01:22 2024 ] 	Batch(7300/7879) done. Loss: 0.2410  lr:0.010000
[ Wed Jul  3 15:01:41 2024 ] 	Batch(7400/7879) done. Loss: 0.0685  lr:0.010000
[ Wed Jul  3 15:01:59 2024 ] 
Training: Epoch [43/120], Step [7499], Loss: 0.11903634667396545, Training Accuracy: 87.86666666666667
[ Wed Jul  3 15:01:59 2024 ] 	Batch(7500/7879) done. Loss: 0.4912  lr:0.010000
[ Wed Jul  3 15:02:18 2024 ] 	Batch(7600/7879) done. Loss: 0.6820  lr:0.010000
[ Wed Jul  3 15:02:36 2024 ] 	Batch(7700/7879) done. Loss: 0.2489  lr:0.010000
[ Wed Jul  3 15:02:55 2024 ] 	Batch(7800/7879) done. Loss: 1.0307  lr:0.010000
[ Wed Jul  3 15:03:09 2024 ] 	Mean training loss: 0.3910.
[ Wed Jul  3 15:03:09 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 15:03:10 2024 ] Training epoch: 45
[ Wed Jul  3 15:03:10 2024 ] 	Batch(0/7879) done. Loss: 0.3833  lr:0.010000
[ Wed Jul  3 15:03:28 2024 ] 	Batch(100/7879) done. Loss: 0.1924  lr:0.010000
[ Wed Jul  3 15:03:46 2024 ] 	Batch(200/7879) done. Loss: 0.8146  lr:0.010000
[ Wed Jul  3 15:04:04 2024 ] 	Batch(300/7879) done. Loss: 0.0945  lr:0.010000
[ Wed Jul  3 15:04:22 2024 ] 	Batch(400/7879) done. Loss: 0.0368  lr:0.010000
[ Wed Jul  3 15:04:40 2024 ] 
Training: Epoch [44/120], Step [499], Loss: 0.5890061259269714, Training Accuracy: 89.35
[ Wed Jul  3 15:04:40 2024 ] 	Batch(500/7879) done. Loss: 0.1269  lr:0.010000
[ Wed Jul  3 15:04:58 2024 ] 	Batch(600/7879) done. Loss: 0.3689  lr:0.010000
[ Wed Jul  3 15:05:17 2024 ] 	Batch(700/7879) done. Loss: 0.1146  lr:0.010000
[ Wed Jul  3 15:05:35 2024 ] 	Batch(800/7879) done. Loss: 0.1641  lr:0.010000
[ Wed Jul  3 15:05:52 2024 ] 	Batch(900/7879) done. Loss: 0.7808  lr:0.010000
[ Wed Jul  3 15:06:10 2024 ] 
Training: Epoch [44/120], Step [999], Loss: 0.10424789041280746, Training Accuracy: 89.4375
[ Wed Jul  3 15:06:10 2024 ] 	Batch(1000/7879) done. Loss: 0.4745  lr:0.010000
[ Wed Jul  3 15:06:28 2024 ] 	Batch(1100/7879) done. Loss: 0.9052  lr:0.010000
[ Wed Jul  3 15:06:46 2024 ] 	Batch(1200/7879) done. Loss: 0.4225  lr:0.010000
[ Wed Jul  3 15:07:04 2024 ] 	Batch(1300/7879) done. Loss: 0.1553  lr:0.010000
[ Wed Jul  3 15:07:22 2024 ] 	Batch(1400/7879) done. Loss: 0.0462  lr:0.010000
[ Wed Jul  3 15:07:40 2024 ] 
Training: Epoch [44/120], Step [1499], Loss: 0.15252986550331116, Training Accuracy: 89.575
[ Wed Jul  3 15:07:40 2024 ] 	Batch(1500/7879) done. Loss: 0.0482  lr:0.010000
[ Wed Jul  3 15:07:58 2024 ] 	Batch(1600/7879) done. Loss: 0.6271  lr:0.010000
[ Wed Jul  3 15:08:16 2024 ] 	Batch(1700/7879) done. Loss: 0.2309  lr:0.010000
[ Wed Jul  3 15:08:34 2024 ] 	Batch(1800/7879) done. Loss: 0.0923  lr:0.010000
[ Wed Jul  3 15:08:52 2024 ] 	Batch(1900/7879) done. Loss: 0.0395  lr:0.010000
[ Wed Jul  3 15:09:09 2024 ] 
Training: Epoch [44/120], Step [1999], Loss: 0.3117542862892151, Training Accuracy: 89.575
[ Wed Jul  3 15:09:10 2024 ] 	Batch(2000/7879) done. Loss: 0.2117  lr:0.010000
[ Wed Jul  3 15:09:27 2024 ] 	Batch(2100/7879) done. Loss: 0.1839  lr:0.010000
[ Wed Jul  3 15:09:45 2024 ] 	Batch(2200/7879) done. Loss: 0.0377  lr:0.010000
[ Wed Jul  3 15:10:03 2024 ] 	Batch(2300/7879) done. Loss: 0.4516  lr:0.010000
[ Wed Jul  3 15:10:21 2024 ] 	Batch(2400/7879) done. Loss: 0.3496  lr:0.010000
[ Wed Jul  3 15:10:39 2024 ] 
Training: Epoch [44/120], Step [2499], Loss: 0.09424867480993271, Training Accuracy: 89.38000000000001
[ Wed Jul  3 15:10:39 2024 ] 	Batch(2500/7879) done. Loss: 0.0821  lr:0.010000
[ Wed Jul  3 15:10:57 2024 ] 	Batch(2600/7879) done. Loss: 0.3834  lr:0.010000
[ Wed Jul  3 15:11:15 2024 ] 	Batch(2700/7879) done. Loss: 0.0461  lr:0.010000
[ Wed Jul  3 15:11:33 2024 ] 	Batch(2800/7879) done. Loss: 0.5126  lr:0.010000
[ Wed Jul  3 15:11:51 2024 ] 	Batch(2900/7879) done. Loss: 0.1506  lr:0.010000
[ Wed Jul  3 15:12:09 2024 ] 
Training: Epoch [44/120], Step [2999], Loss: 0.3329649567604065, Training Accuracy: 89.3
[ Wed Jul  3 15:12:10 2024 ] 	Batch(3000/7879) done. Loss: 0.1224  lr:0.010000
[ Wed Jul  3 15:12:28 2024 ] 	Batch(3100/7879) done. Loss: 1.0943  lr:0.010000
[ Wed Jul  3 15:12:47 2024 ] 	Batch(3200/7879) done. Loss: 0.2275  lr:0.010000
[ Wed Jul  3 15:13:05 2024 ] 	Batch(3300/7879) done. Loss: 0.3566  lr:0.010000
[ Wed Jul  3 15:13:22 2024 ] 	Batch(3400/7879) done. Loss: 0.0645  lr:0.010000
[ Wed Jul  3 15:13:40 2024 ] 
Training: Epoch [44/120], Step [3499], Loss: 0.3107827305793762, Training Accuracy: 89.16785714285714
[ Wed Jul  3 15:13:40 2024 ] 	Batch(3500/7879) done. Loss: 0.8435  lr:0.010000
[ Wed Jul  3 15:13:58 2024 ] 	Batch(3600/7879) done. Loss: 0.2500  lr:0.010000
[ Wed Jul  3 15:14:16 2024 ] 	Batch(3700/7879) done. Loss: 0.6178  lr:0.010000
[ Wed Jul  3 15:14:34 2024 ] 	Batch(3800/7879) done. Loss: 0.4033  lr:0.010000
[ Wed Jul  3 15:14:52 2024 ] 	Batch(3900/7879) done. Loss: 0.6897  lr:0.010000
[ Wed Jul  3 15:15:10 2024 ] 
Training: Epoch [44/120], Step [3999], Loss: 0.2705188989639282, Training Accuracy: 88.925
[ Wed Jul  3 15:15:10 2024 ] 	Batch(4000/7879) done. Loss: 0.8590  lr:0.010000
[ Wed Jul  3 15:15:28 2024 ] 	Batch(4100/7879) done. Loss: 0.0922  lr:0.010000
[ Wed Jul  3 15:15:46 2024 ] 	Batch(4200/7879) done. Loss: 0.3805  lr:0.010000
[ Wed Jul  3 15:16:04 2024 ] 	Batch(4300/7879) done. Loss: 0.0778  lr:0.010000
[ Wed Jul  3 15:16:22 2024 ] 	Batch(4400/7879) done. Loss: 0.2722  lr:0.010000
[ Wed Jul  3 15:16:39 2024 ] 
Training: Epoch [44/120], Step [4499], Loss: 0.2067156285047531, Training Accuracy: 88.9
[ Wed Jul  3 15:16:40 2024 ] 	Batch(4500/7879) done. Loss: 0.8818  lr:0.010000
[ Wed Jul  3 15:16:57 2024 ] 	Batch(4600/7879) done. Loss: 0.1648  lr:0.010000
[ Wed Jul  3 15:17:15 2024 ] 	Batch(4700/7879) done. Loss: 0.0681  lr:0.010000
[ Wed Jul  3 15:17:33 2024 ] 	Batch(4800/7879) done. Loss: 0.9256  lr:0.010000
[ Wed Jul  3 15:17:51 2024 ] 	Batch(4900/7879) done. Loss: 0.6181  lr:0.010000
[ Wed Jul  3 15:18:09 2024 ] 
Training: Epoch [44/120], Step [4999], Loss: 0.4827236831188202, Training Accuracy: 88.715
[ Wed Jul  3 15:18:09 2024 ] 	Batch(5000/7879) done. Loss: 0.3594  lr:0.010000
[ Wed Jul  3 15:18:27 2024 ] 	Batch(5100/7879) done. Loss: 0.1287  lr:0.010000
[ Wed Jul  3 15:18:45 2024 ] 	Batch(5200/7879) done. Loss: 0.2719  lr:0.010000
[ Wed Jul  3 15:19:04 2024 ] 	Batch(5300/7879) done. Loss: 0.0163  lr:0.010000
[ Wed Jul  3 15:19:22 2024 ] 	Batch(5400/7879) done. Loss: 0.0886  lr:0.010000
[ Wed Jul  3 15:19:40 2024 ] 
Training: Epoch [44/120], Step [5499], Loss: 0.7526695132255554, Training Accuracy: 88.58636363636364
[ Wed Jul  3 15:19:41 2024 ] 	Batch(5500/7879) done. Loss: 0.8696  lr:0.010000
[ Wed Jul  3 15:19:59 2024 ] 	Batch(5600/7879) done. Loss: 1.0412  lr:0.010000
[ Wed Jul  3 15:20:17 2024 ] 	Batch(5700/7879) done. Loss: 0.6663  lr:0.010000
[ Wed Jul  3 15:20:35 2024 ] 	Batch(5800/7879) done. Loss: 0.0971  lr:0.010000
[ Wed Jul  3 15:20:53 2024 ] 	Batch(5900/7879) done. Loss: 0.2892  lr:0.010000
[ Wed Jul  3 15:21:11 2024 ] 
Training: Epoch [44/120], Step [5999], Loss: 0.10925301164388657, Training Accuracy: 88.47500000000001
[ Wed Jul  3 15:21:11 2024 ] 	Batch(6000/7879) done. Loss: 0.0626  lr:0.010000
[ Wed Jul  3 15:21:29 2024 ] 	Batch(6100/7879) done. Loss: 0.4531  lr:0.010000
[ Wed Jul  3 15:21:47 2024 ] 	Batch(6200/7879) done. Loss: 0.4194  lr:0.010000
[ Wed Jul  3 15:22:05 2024 ] 	Batch(6300/7879) done. Loss: 0.3600  lr:0.010000
[ Wed Jul  3 15:22:23 2024 ] 	Batch(6400/7879) done. Loss: 0.2990  lr:0.010000
[ Wed Jul  3 15:22:40 2024 ] 
Training: Epoch [44/120], Step [6499], Loss: 0.08739975094795227, Training Accuracy: 88.42692307692307
[ Wed Jul  3 15:22:41 2024 ] 	Batch(6500/7879) done. Loss: 0.7902  lr:0.010000
[ Wed Jul  3 15:22:58 2024 ] 	Batch(6600/7879) done. Loss: 0.4315  lr:0.010000
[ Wed Jul  3 15:23:16 2024 ] 	Batch(6700/7879) done. Loss: 0.1679  lr:0.010000
[ Wed Jul  3 15:23:34 2024 ] 	Batch(6800/7879) done. Loss: 0.1695  lr:0.010000
[ Wed Jul  3 15:23:52 2024 ] 	Batch(6900/7879) done. Loss: 0.3040  lr:0.010000
[ Wed Jul  3 15:24:10 2024 ] 
Training: Epoch [44/120], Step [6999], Loss: 0.06517580896615982, Training Accuracy: 88.40714285714286
[ Wed Jul  3 15:24:10 2024 ] 	Batch(7000/7879) done. Loss: 0.3639  lr:0.010000
[ Wed Jul  3 15:24:29 2024 ] 	Batch(7100/7879) done. Loss: 0.2809  lr:0.010000
[ Wed Jul  3 15:24:47 2024 ] 	Batch(7200/7879) done. Loss: 0.1408  lr:0.010000
[ Wed Jul  3 15:25:06 2024 ] 	Batch(7300/7879) done. Loss: 0.3302  lr:0.010000
[ Wed Jul  3 15:25:24 2024 ] 	Batch(7400/7879) done. Loss: 0.4161  lr:0.010000
[ Wed Jul  3 15:25:43 2024 ] 
Training: Epoch [44/120], Step [7499], Loss: 0.2436842918395996, Training Accuracy: 88.33166666666666
[ Wed Jul  3 15:25:43 2024 ] 	Batch(7500/7879) done. Loss: 0.0370  lr:0.010000
[ Wed Jul  3 15:26:02 2024 ] 	Batch(7600/7879) done. Loss: 0.0618  lr:0.010000
[ Wed Jul  3 15:26:20 2024 ] 	Batch(7700/7879) done. Loss: 0.1941  lr:0.010000
[ Wed Jul  3 15:26:38 2024 ] 	Batch(7800/7879) done. Loss: 0.2660  lr:0.010000
[ Wed Jul  3 15:26:52 2024 ] 	Mean training loss: 0.3777.
[ Wed Jul  3 15:26:52 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 15:26:52 2024 ] Training epoch: 46
[ Wed Jul  3 15:26:52 2024 ] 	Batch(0/7879) done. Loss: 0.5463  lr:0.010000
[ Wed Jul  3 15:27:10 2024 ] 	Batch(100/7879) done. Loss: 1.2409  lr:0.010000
[ Wed Jul  3 15:27:28 2024 ] 	Batch(200/7879) done. Loss: 0.1721  lr:0.010000
[ Wed Jul  3 15:27:46 2024 ] 	Batch(300/7879) done. Loss: 0.0247  lr:0.010000
[ Wed Jul  3 15:28:04 2024 ] 	Batch(400/7879) done. Loss: 0.1399  lr:0.010000
[ Wed Jul  3 15:28:21 2024 ] 
Training: Epoch [45/120], Step [499], Loss: 0.20890183746814728, Training Accuracy: 89.275
[ Wed Jul  3 15:28:22 2024 ] 	Batch(500/7879) done. Loss: 0.1732  lr:0.010000
[ Wed Jul  3 15:28:40 2024 ] 	Batch(600/7879) done. Loss: 0.1816  lr:0.010000
[ Wed Jul  3 15:28:57 2024 ] 	Batch(700/7879) done. Loss: 1.2281  lr:0.010000
[ Wed Jul  3 15:29:15 2024 ] 	Batch(800/7879) done. Loss: 0.0158  lr:0.010000
[ Wed Jul  3 15:29:33 2024 ] 	Batch(900/7879) done. Loss: 0.1677  lr:0.010000
[ Wed Jul  3 15:29:51 2024 ] 
Training: Epoch [45/120], Step [999], Loss: 0.4050793945789337, Training Accuracy: 89.2
[ Wed Jul  3 15:29:51 2024 ] 	Batch(1000/7879) done. Loss: 0.0496  lr:0.010000
[ Wed Jul  3 15:30:09 2024 ] 	Batch(1100/7879) done. Loss: 0.2712  lr:0.010000
[ Wed Jul  3 15:30:27 2024 ] 	Batch(1200/7879) done. Loss: 0.6649  lr:0.010000
[ Wed Jul  3 15:30:45 2024 ] 	Batch(1300/7879) done. Loss: 0.4451  lr:0.010000
[ Wed Jul  3 15:31:03 2024 ] 	Batch(1400/7879) done. Loss: 0.4136  lr:0.010000
[ Wed Jul  3 15:31:21 2024 ] 
Training: Epoch [45/120], Step [1499], Loss: 0.13868236541748047, Training Accuracy: 89.11666666666666
[ Wed Jul  3 15:31:21 2024 ] 	Batch(1500/7879) done. Loss: 0.3783  lr:0.010000
[ Wed Jul  3 15:31:39 2024 ] 	Batch(1600/7879) done. Loss: 0.0935  lr:0.010000
[ Wed Jul  3 15:31:57 2024 ] 	Batch(1700/7879) done. Loss: 1.5319  lr:0.010000
[ Wed Jul  3 15:32:14 2024 ] 	Batch(1800/7879) done. Loss: 0.4484  lr:0.010000
[ Wed Jul  3 15:32:32 2024 ] 	Batch(1900/7879) done. Loss: 0.0766  lr:0.010000
[ Wed Jul  3 15:32:50 2024 ] 
Training: Epoch [45/120], Step [1999], Loss: 0.4323505163192749, Training Accuracy: 88.96875
[ Wed Jul  3 15:32:50 2024 ] 	Batch(2000/7879) done. Loss: 0.2905  lr:0.010000
[ Wed Jul  3 15:33:08 2024 ] 	Batch(2100/7879) done. Loss: 0.2708  lr:0.010000
[ Wed Jul  3 15:33:26 2024 ] 	Batch(2200/7879) done. Loss: 0.1091  lr:0.010000
[ Wed Jul  3 15:33:44 2024 ] 	Batch(2300/7879) done. Loss: 0.1737  lr:0.010000
[ Wed Jul  3 15:34:02 2024 ] 	Batch(2400/7879) done. Loss: 0.3530  lr:0.010000
[ Wed Jul  3 15:34:20 2024 ] 
Training: Epoch [45/120], Step [2499], Loss: 0.07123764604330063, Training Accuracy: 88.99000000000001
[ Wed Jul  3 15:34:20 2024 ] 	Batch(2500/7879) done. Loss: 0.0678  lr:0.010000
[ Wed Jul  3 15:34:38 2024 ] 	Batch(2600/7879) done. Loss: 0.3661  lr:0.010000
[ Wed Jul  3 15:34:56 2024 ] 	Batch(2700/7879) done. Loss: 0.2817  lr:0.010000
[ Wed Jul  3 15:35:14 2024 ] 	Batch(2800/7879) done. Loss: 0.6769  lr:0.010000
[ Wed Jul  3 15:35:31 2024 ] 	Batch(2900/7879) done. Loss: 0.0635  lr:0.010000
[ Wed Jul  3 15:35:49 2024 ] 
Training: Epoch [45/120], Step [2999], Loss: 0.29847511649131775, Training Accuracy: 88.95833333333333
[ Wed Jul  3 15:35:49 2024 ] 	Batch(3000/7879) done. Loss: 0.3527  lr:0.010000
[ Wed Jul  3 15:36:07 2024 ] 	Batch(3100/7879) done. Loss: 0.1901  lr:0.010000
[ Wed Jul  3 15:36:25 2024 ] 	Batch(3200/7879) done. Loss: 0.5132  lr:0.010000
[ Wed Jul  3 15:36:43 2024 ] 	Batch(3300/7879) done. Loss: 1.3029  lr:0.010000
[ Wed Jul  3 15:37:01 2024 ] 	Batch(3400/7879) done. Loss: 1.0611  lr:0.010000
[ Wed Jul  3 15:37:19 2024 ] 
Training: Epoch [45/120], Step [3499], Loss: 0.3877585232257843, Training Accuracy: 88.95357142857144
[ Wed Jul  3 15:37:19 2024 ] 	Batch(3500/7879) done. Loss: 0.3009  lr:0.010000
[ Wed Jul  3 15:37:37 2024 ] 	Batch(3600/7879) done. Loss: 0.7824  lr:0.010000
[ Wed Jul  3 15:37:55 2024 ] 	Batch(3700/7879) done. Loss: 0.4142  lr:0.010000
[ Wed Jul  3 15:38:12 2024 ] 	Batch(3800/7879) done. Loss: 0.0709  lr:0.010000
[ Wed Jul  3 15:38:30 2024 ] 	Batch(3900/7879) done. Loss: 0.0264  lr:0.010000
[ Wed Jul  3 15:38:48 2024 ] 
Training: Epoch [45/120], Step [3999], Loss: 0.9571744203567505, Training Accuracy: 88.934375
[ Wed Jul  3 15:38:48 2024 ] 	Batch(4000/7879) done. Loss: 0.3223  lr:0.010000
[ Wed Jul  3 15:39:06 2024 ] 	Batch(4100/7879) done. Loss: 0.2281  lr:0.010000
[ Wed Jul  3 15:39:24 2024 ] 	Batch(4200/7879) done. Loss: 0.6135  lr:0.010000
[ Wed Jul  3 15:39:42 2024 ] 	Batch(4300/7879) done. Loss: 1.0781  lr:0.010000
[ Wed Jul  3 15:40:00 2024 ] 	Batch(4400/7879) done. Loss: 0.0848  lr:0.010000
[ Wed Jul  3 15:40:18 2024 ] 
Training: Epoch [45/120], Step [4499], Loss: 0.35513776540756226, Training Accuracy: 88.84444444444445
[ Wed Jul  3 15:40:18 2024 ] 	Batch(4500/7879) done. Loss: 0.1165  lr:0.010000
[ Wed Jul  3 15:40:36 2024 ] 	Batch(4600/7879) done. Loss: 0.1476  lr:0.010000
[ Wed Jul  3 15:40:54 2024 ] 	Batch(4700/7879) done. Loss: 0.1416  lr:0.010000
[ Wed Jul  3 15:41:12 2024 ] 	Batch(4800/7879) done. Loss: 0.2963  lr:0.010000
[ Wed Jul  3 15:41:30 2024 ] 	Batch(4900/7879) done. Loss: 0.5597  lr:0.010000
[ Wed Jul  3 15:41:48 2024 ] 
Training: Epoch [45/120], Step [4999], Loss: 0.06601493805646896, Training Accuracy: 88.8775
[ Wed Jul  3 15:41:48 2024 ] 	Batch(5000/7879) done. Loss: 0.2586  lr:0.010000
[ Wed Jul  3 15:42:06 2024 ] 	Batch(5100/7879) done. Loss: 0.4295  lr:0.010000
[ Wed Jul  3 15:42:24 2024 ] 	Batch(5200/7879) done. Loss: 0.4274  lr:0.010000
[ Wed Jul  3 15:42:42 2024 ] 	Batch(5300/7879) done. Loss: 0.0159  lr:0.010000
[ Wed Jul  3 15:42:59 2024 ] 	Batch(5400/7879) done. Loss: 0.7386  lr:0.010000
[ Wed Jul  3 15:43:17 2024 ] 
Training: Epoch [45/120], Step [5499], Loss: 0.4415859580039978, Training Accuracy: 88.72727272727273
[ Wed Jul  3 15:43:17 2024 ] 	Batch(5500/7879) done. Loss: 1.3325  lr:0.010000
[ Wed Jul  3 15:43:35 2024 ] 	Batch(5600/7879) done. Loss: 0.1041  lr:0.010000
[ Wed Jul  3 15:43:53 2024 ] 	Batch(5700/7879) done. Loss: 0.0511  lr:0.010000
[ Wed Jul  3 15:44:12 2024 ] 	Batch(5800/7879) done. Loss: 0.5886  lr:0.010000
[ Wed Jul  3 15:44:29 2024 ] 	Batch(5900/7879) done. Loss: 0.0499  lr:0.010000
[ Wed Jul  3 15:44:47 2024 ] 
Training: Epoch [45/120], Step [5999], Loss: 1.0673571825027466, Training Accuracy: 88.71458333333334
[ Wed Jul  3 15:44:47 2024 ] 	Batch(6000/7879) done. Loss: 0.3074  lr:0.010000
[ Wed Jul  3 15:45:05 2024 ] 	Batch(6100/7879) done. Loss: 0.0958  lr:0.010000
[ Wed Jul  3 15:45:23 2024 ] 	Batch(6200/7879) done. Loss: 0.2394  lr:0.010000
[ Wed Jul  3 15:45:41 2024 ] 	Batch(6300/7879) done. Loss: 0.0235  lr:0.010000
[ Wed Jul  3 15:45:59 2024 ] 	Batch(6400/7879) done. Loss: 0.1129  lr:0.010000
[ Wed Jul  3 15:46:17 2024 ] 
Training: Epoch [45/120], Step [6499], Loss: 0.9110509753227234, Training Accuracy: 88.71153846153847
[ Wed Jul  3 15:46:17 2024 ] 	Batch(6500/7879) done. Loss: 1.0504  lr:0.010000
[ Wed Jul  3 15:46:35 2024 ] 	Batch(6600/7879) done. Loss: 0.4449  lr:0.010000
[ Wed Jul  3 15:46:53 2024 ] 	Batch(6700/7879) done. Loss: 0.3784  lr:0.010000
[ Wed Jul  3 15:47:11 2024 ] 	Batch(6800/7879) done. Loss: 0.4031  lr:0.010000
[ Wed Jul  3 15:47:29 2024 ] 	Batch(6900/7879) done. Loss: 0.2431  lr:0.010000
[ Wed Jul  3 15:47:46 2024 ] 
Training: Epoch [45/120], Step [6999], Loss: 0.1446341872215271, Training Accuracy: 88.61964285714285
[ Wed Jul  3 15:47:46 2024 ] 	Batch(7000/7879) done. Loss: 0.2918  lr:0.010000
[ Wed Jul  3 15:48:04 2024 ] 	Batch(7100/7879) done. Loss: 0.0163  lr:0.010000
[ Wed Jul  3 15:48:22 2024 ] 	Batch(7200/7879) done. Loss: 0.0549  lr:0.010000
[ Wed Jul  3 15:48:40 2024 ] 	Batch(7300/7879) done. Loss: 0.3262  lr:0.010000
[ Wed Jul  3 15:48:58 2024 ] 	Batch(7400/7879) done. Loss: 0.2384  lr:0.010000
[ Wed Jul  3 15:49:16 2024 ] 
Training: Epoch [45/120], Step [7499], Loss: 1.2055119276046753, Training Accuracy: 88.50666666666666
[ Wed Jul  3 15:49:16 2024 ] 	Batch(7500/7879) done. Loss: 0.4334  lr:0.010000
[ Wed Jul  3 15:49:34 2024 ] 	Batch(7600/7879) done. Loss: 0.0395  lr:0.010000
[ Wed Jul  3 15:49:52 2024 ] 	Batch(7700/7879) done. Loss: 0.5095  lr:0.010000
[ Wed Jul  3 15:50:10 2024 ] 	Batch(7800/7879) done. Loss: 0.3010  lr:0.010000
[ Wed Jul  3 15:50:24 2024 ] 	Mean training loss: 0.3778.
[ Wed Jul  3 15:50:24 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 15:50:24 2024 ] Training epoch: 47
[ Wed Jul  3 15:50:24 2024 ] 	Batch(0/7879) done. Loss: 0.5792  lr:0.010000
[ Wed Jul  3 15:50:42 2024 ] 	Batch(100/7879) done. Loss: 0.0106  lr:0.010000
[ Wed Jul  3 15:51:00 2024 ] 	Batch(200/7879) done. Loss: 0.2756  lr:0.010000
[ Wed Jul  3 15:51:18 2024 ] 	Batch(300/7879) done. Loss: 0.0142  lr:0.010000
[ Wed Jul  3 15:51:36 2024 ] 	Batch(400/7879) done. Loss: 0.1317  lr:0.010000
[ Wed Jul  3 15:51:54 2024 ] 
Training: Epoch [46/120], Step [499], Loss: 1.2011299133300781, Training Accuracy: 89.0
[ Wed Jul  3 15:51:54 2024 ] 	Batch(500/7879) done. Loss: 0.1421  lr:0.010000
[ Wed Jul  3 15:52:12 2024 ] 	Batch(600/7879) done. Loss: 0.2564  lr:0.010000
[ Wed Jul  3 15:52:30 2024 ] 	Batch(700/7879) done. Loss: 0.0111  lr:0.010000
[ Wed Jul  3 15:52:48 2024 ] 	Batch(800/7879) done. Loss: 0.1500  lr:0.010000
[ Wed Jul  3 15:53:06 2024 ] 	Batch(900/7879) done. Loss: 0.3044  lr:0.010000
[ Wed Jul  3 15:53:23 2024 ] 
Training: Epoch [46/120], Step [999], Loss: 0.2312271147966385, Training Accuracy: 89.625
[ Wed Jul  3 15:53:23 2024 ] 	Batch(1000/7879) done. Loss: 0.2910  lr:0.010000
[ Wed Jul  3 15:53:41 2024 ] 	Batch(1100/7879) done. Loss: 0.2333  lr:0.010000
[ Wed Jul  3 15:53:59 2024 ] 	Batch(1200/7879) done. Loss: 0.4041  lr:0.010000
[ Wed Jul  3 15:54:17 2024 ] 	Batch(1300/7879) done. Loss: 0.4630  lr:0.010000
[ Wed Jul  3 15:54:35 2024 ] 	Batch(1400/7879) done. Loss: 0.4201  lr:0.010000
[ Wed Jul  3 15:54:53 2024 ] 
Training: Epoch [46/120], Step [1499], Loss: 0.2649635076522827, Training Accuracy: 89.59166666666667
[ Wed Jul  3 15:54:53 2024 ] 	Batch(1500/7879) done. Loss: 0.7797  lr:0.010000
[ Wed Jul  3 15:55:11 2024 ] 	Batch(1600/7879) done. Loss: 0.2834  lr:0.010000
[ Wed Jul  3 15:55:29 2024 ] 	Batch(1700/7879) done. Loss: 0.2919  lr:0.010000
[ Wed Jul  3 15:55:48 2024 ] 	Batch(1800/7879) done. Loss: 0.2514  lr:0.010000
[ Wed Jul  3 15:56:06 2024 ] 	Batch(1900/7879) done. Loss: 0.3883  lr:0.010000
[ Wed Jul  3 15:56:25 2024 ] 
Training: Epoch [46/120], Step [1999], Loss: 0.007593902293592691, Training Accuracy: 89.45625
[ Wed Jul  3 15:56:25 2024 ] 	Batch(2000/7879) done. Loss: 0.0562  lr:0.010000
[ Wed Jul  3 15:56:44 2024 ] 	Batch(2100/7879) done. Loss: 0.0216  lr:0.010000
[ Wed Jul  3 15:57:02 2024 ] 	Batch(2200/7879) done. Loss: 0.7280  lr:0.010000
[ Wed Jul  3 15:57:21 2024 ] 	Batch(2300/7879) done. Loss: 0.6006  lr:0.010000
[ Wed Jul  3 15:57:39 2024 ] 	Batch(2400/7879) done. Loss: 0.3138  lr:0.010000
[ Wed Jul  3 15:57:58 2024 ] 
Training: Epoch [46/120], Step [2499], Loss: 0.31505629420280457, Training Accuracy: 89.47
[ Wed Jul  3 15:57:58 2024 ] 	Batch(2500/7879) done. Loss: 0.0333  lr:0.010000
[ Wed Jul  3 15:58:16 2024 ] 	Batch(2600/7879) done. Loss: 0.6275  lr:0.010000
[ Wed Jul  3 15:58:35 2024 ] 	Batch(2700/7879) done. Loss: 0.4068  lr:0.010000
[ Wed Jul  3 15:58:54 2024 ] 	Batch(2800/7879) done. Loss: 0.2458  lr:0.010000
[ Wed Jul  3 15:59:12 2024 ] 	Batch(2900/7879) done. Loss: 0.6241  lr:0.010000
[ Wed Jul  3 15:59:30 2024 ] 
Training: Epoch [46/120], Step [2999], Loss: 0.10786416381597519, Training Accuracy: 89.51666666666667
[ Wed Jul  3 15:59:31 2024 ] 	Batch(3000/7879) done. Loss: 0.3053  lr:0.010000
[ Wed Jul  3 15:59:48 2024 ] 	Batch(3100/7879) done. Loss: 0.0964  lr:0.010000
[ Wed Jul  3 16:00:07 2024 ] 	Batch(3200/7879) done. Loss: 0.7997  lr:0.010000
[ Wed Jul  3 16:00:24 2024 ] 	Batch(3300/7879) done. Loss: 0.0148  lr:0.010000
[ Wed Jul  3 16:00:42 2024 ] 	Batch(3400/7879) done. Loss: 0.2470  lr:0.010000
[ Wed Jul  3 16:01:00 2024 ] 
Training: Epoch [46/120], Step [3499], Loss: 1.1754779815673828, Training Accuracy: 89.43214285714286
[ Wed Jul  3 16:01:00 2024 ] 	Batch(3500/7879) done. Loss: 0.0684  lr:0.010000
[ Wed Jul  3 16:01:18 2024 ] 	Batch(3600/7879) done. Loss: 0.1640  lr:0.010000
[ Wed Jul  3 16:01:36 2024 ] 	Batch(3700/7879) done. Loss: 0.4511  lr:0.010000
[ Wed Jul  3 16:01:54 2024 ] 	Batch(3800/7879) done. Loss: 0.2142  lr:0.010000
[ Wed Jul  3 16:02:12 2024 ] 	Batch(3900/7879) done. Loss: 0.4466  lr:0.010000
[ Wed Jul  3 16:02:30 2024 ] 
Training: Epoch [46/120], Step [3999], Loss: 0.2713906764984131, Training Accuracy: 89.090625
[ Wed Jul  3 16:02:30 2024 ] 	Batch(4000/7879) done. Loss: 0.2812  lr:0.010000
[ Wed Jul  3 16:02:48 2024 ] 	Batch(4100/7879) done. Loss: 0.0098  lr:0.010000
[ Wed Jul  3 16:03:06 2024 ] 	Batch(4200/7879) done. Loss: 0.7933  lr:0.010000
[ Wed Jul  3 16:03:24 2024 ] 	Batch(4300/7879) done. Loss: 0.3526  lr:0.010000
[ Wed Jul  3 16:03:42 2024 ] 	Batch(4400/7879) done. Loss: 0.3912  lr:0.010000
[ Wed Jul  3 16:04:00 2024 ] 
Training: Epoch [46/120], Step [4499], Loss: 0.23459558188915253, Training Accuracy: 88.85
[ Wed Jul  3 16:04:00 2024 ] 	Batch(4500/7879) done. Loss: 0.1131  lr:0.010000
[ Wed Jul  3 16:04:18 2024 ] 	Batch(4600/7879) done. Loss: 0.2074  lr:0.010000
[ Wed Jul  3 16:04:36 2024 ] 	Batch(4700/7879) done. Loss: 0.2565  lr:0.010000
[ Wed Jul  3 16:04:54 2024 ] 	Batch(4800/7879) done. Loss: 0.1600  lr:0.010000
[ Wed Jul  3 16:05:11 2024 ] 	Batch(4900/7879) done. Loss: 0.2735  lr:0.010000
[ Wed Jul  3 16:05:29 2024 ] 
Training: Epoch [46/120], Step [4999], Loss: 0.32417577505111694, Training Accuracy: 88.83250000000001
[ Wed Jul  3 16:05:29 2024 ] 	Batch(5000/7879) done. Loss: 0.3206  lr:0.010000
[ Wed Jul  3 16:05:47 2024 ] 	Batch(5100/7879) done. Loss: 0.1182  lr:0.010000
[ Wed Jul  3 16:06:05 2024 ] 	Batch(5200/7879) done. Loss: 0.2758  lr:0.010000
[ Wed Jul  3 16:06:23 2024 ] 	Batch(5300/7879) done. Loss: 0.0946  lr:0.010000
[ Wed Jul  3 16:06:41 2024 ] 	Batch(5400/7879) done. Loss: 0.4107  lr:0.010000
[ Wed Jul  3 16:06:59 2024 ] 
Training: Epoch [46/120], Step [5499], Loss: 1.2869454622268677, Training Accuracy: 88.83636363636363
[ Wed Jul  3 16:06:59 2024 ] 	Batch(5500/7879) done. Loss: 0.8797  lr:0.010000
[ Wed Jul  3 16:07:17 2024 ] 	Batch(5600/7879) done. Loss: 0.0122  lr:0.010000
[ Wed Jul  3 16:07:35 2024 ] 	Batch(5700/7879) done. Loss: 0.3877  lr:0.010000
[ Wed Jul  3 16:07:53 2024 ] 	Batch(5800/7879) done. Loss: 0.6478  lr:0.010000
[ Wed Jul  3 16:08:11 2024 ] 	Batch(5900/7879) done. Loss: 0.1131  lr:0.010000
[ Wed Jul  3 16:08:28 2024 ] 
Training: Epoch [46/120], Step [5999], Loss: 0.6089802980422974, Training Accuracy: 88.72916666666667
[ Wed Jul  3 16:08:29 2024 ] 	Batch(6000/7879) done. Loss: 0.4631  lr:0.010000
[ Wed Jul  3 16:08:47 2024 ] 	Batch(6100/7879) done. Loss: 1.0106  lr:0.010000
[ Wed Jul  3 16:09:04 2024 ] 	Batch(6200/7879) done. Loss: 0.1684  lr:0.010000
[ Wed Jul  3 16:09:22 2024 ] 	Batch(6300/7879) done. Loss: 0.2510  lr:0.010000
[ Wed Jul  3 16:09:40 2024 ] 	Batch(6400/7879) done. Loss: 0.2200  lr:0.010000
[ Wed Jul  3 16:09:59 2024 ] 
Training: Epoch [46/120], Step [6499], Loss: 0.19854703545570374, Training Accuracy: 88.68846153846154
[ Wed Jul  3 16:09:59 2024 ] 	Batch(6500/7879) done. Loss: 0.2863  lr:0.010000
[ Wed Jul  3 16:10:17 2024 ] 	Batch(6600/7879) done. Loss: 0.3951  lr:0.010000
[ Wed Jul  3 16:10:36 2024 ] 	Batch(6700/7879) done. Loss: 0.6341  lr:0.010000
[ Wed Jul  3 16:10:54 2024 ] 	Batch(6800/7879) done. Loss: 0.2342  lr:0.010000
[ Wed Jul  3 16:11:12 2024 ] 	Batch(6900/7879) done. Loss: 0.1896  lr:0.010000
[ Wed Jul  3 16:11:30 2024 ] 
Training: Epoch [46/120], Step [6999], Loss: 0.22082607448101044, Training Accuracy: 88.7375
[ Wed Jul  3 16:11:30 2024 ] 	Batch(7000/7879) done. Loss: 0.2560  lr:0.010000
[ Wed Jul  3 16:11:48 2024 ] 	Batch(7100/7879) done. Loss: 0.1608  lr:0.010000
[ Wed Jul  3 16:12:06 2024 ] 	Batch(7200/7879) done. Loss: 0.1183  lr:0.010000
[ Wed Jul  3 16:12:24 2024 ] 	Batch(7300/7879) done. Loss: 0.6306  lr:0.010000
[ Wed Jul  3 16:12:42 2024 ] 	Batch(7400/7879) done. Loss: 0.3716  lr:0.010000
[ Wed Jul  3 16:12:59 2024 ] 
Training: Epoch [46/120], Step [7499], Loss: 0.7852221727371216, Training Accuracy: 88.62166666666667
[ Wed Jul  3 16:13:00 2024 ] 	Batch(7500/7879) done. Loss: 0.5997  lr:0.010000
[ Wed Jul  3 16:13:18 2024 ] 	Batch(7600/7879) done. Loss: 0.1739  lr:0.010000
[ Wed Jul  3 16:13:36 2024 ] 	Batch(7700/7879) done. Loss: 1.5971  lr:0.010000
[ Wed Jul  3 16:13:53 2024 ] 	Batch(7800/7879) done. Loss: 0.2416  lr:0.010000
[ Wed Jul  3 16:14:07 2024 ] 	Mean training loss: 0.3667.
[ Wed Jul  3 16:14:07 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 16:14:08 2024 ] Training epoch: 48
[ Wed Jul  3 16:14:08 2024 ] 	Batch(0/7879) done. Loss: 0.0203  lr:0.010000
[ Wed Jul  3 16:14:26 2024 ] 	Batch(100/7879) done. Loss: 0.0151  lr:0.010000
[ Wed Jul  3 16:14:45 2024 ] 	Batch(200/7879) done. Loss: 0.2143  lr:0.010000
[ Wed Jul  3 16:15:03 2024 ] 	Batch(300/7879) done. Loss: 0.0950  lr:0.010000
[ Wed Jul  3 16:15:21 2024 ] 	Batch(400/7879) done. Loss: 0.3947  lr:0.010000
[ Wed Jul  3 16:15:39 2024 ] 
Training: Epoch [47/120], Step [499], Loss: 0.4545738697052002, Training Accuracy: 90.17500000000001
[ Wed Jul  3 16:15:39 2024 ] 	Batch(500/7879) done. Loss: 0.6230  lr:0.010000
[ Wed Jul  3 16:15:58 2024 ] 	Batch(600/7879) done. Loss: 0.0659  lr:0.010000
[ Wed Jul  3 16:16:16 2024 ] 	Batch(700/7879) done. Loss: 0.2622  lr:0.010000
[ Wed Jul  3 16:16:34 2024 ] 	Batch(800/7879) done. Loss: 0.2696  lr:0.010000
[ Wed Jul  3 16:16:52 2024 ] 	Batch(900/7879) done. Loss: 0.1006  lr:0.010000
[ Wed Jul  3 16:17:10 2024 ] 
Training: Epoch [47/120], Step [999], Loss: 0.2574074864387512, Training Accuracy: 90.05
[ Wed Jul  3 16:17:10 2024 ] 	Batch(1000/7879) done. Loss: 0.1922  lr:0.010000
[ Wed Jul  3 16:17:28 2024 ] 	Batch(1100/7879) done. Loss: 0.2448  lr:0.010000
[ Wed Jul  3 16:17:46 2024 ] 	Batch(1200/7879) done. Loss: 0.2721  lr:0.010000
[ Wed Jul  3 16:18:04 2024 ] 	Batch(1300/7879) done. Loss: 0.3494  lr:0.010000
[ Wed Jul  3 16:18:22 2024 ] 	Batch(1400/7879) done. Loss: 0.1231  lr:0.010000
[ Wed Jul  3 16:18:40 2024 ] 
Training: Epoch [47/120], Step [1499], Loss: 0.21073608100414276, Training Accuracy: 90.04166666666666
[ Wed Jul  3 16:18:40 2024 ] 	Batch(1500/7879) done. Loss: 0.0036  lr:0.010000
[ Wed Jul  3 16:18:58 2024 ] 	Batch(1600/7879) done. Loss: 0.0195  lr:0.010000
[ Wed Jul  3 16:19:16 2024 ] 	Batch(1700/7879) done. Loss: 0.0630  lr:0.010000
[ Wed Jul  3 16:19:34 2024 ] 	Batch(1800/7879) done. Loss: 0.1421  lr:0.010000
[ Wed Jul  3 16:19:52 2024 ] 	Batch(1900/7879) done. Loss: 0.3872  lr:0.010000
[ Wed Jul  3 16:20:09 2024 ] 
Training: Epoch [47/120], Step [1999], Loss: 0.015270931646227837, Training Accuracy: 90.025
[ Wed Jul  3 16:20:10 2024 ] 	Batch(2000/7879) done. Loss: 0.4230  lr:0.010000
[ Wed Jul  3 16:20:28 2024 ] 	Batch(2100/7879) done. Loss: 0.0508  lr:0.010000
[ Wed Jul  3 16:20:46 2024 ] 	Batch(2200/7879) done. Loss: 0.0306  lr:0.010000
[ Wed Jul  3 16:21:04 2024 ] 	Batch(2300/7879) done. Loss: 0.2791  lr:0.010000
[ Wed Jul  3 16:21:22 2024 ] 	Batch(2400/7879) done. Loss: 0.0109  lr:0.010000
[ Wed Jul  3 16:21:40 2024 ] 
Training: Epoch [47/120], Step [2499], Loss: 0.019400743767619133, Training Accuracy: 89.895
[ Wed Jul  3 16:21:40 2024 ] 	Batch(2500/7879) done. Loss: 1.0403  lr:0.010000
[ Wed Jul  3 16:21:59 2024 ] 	Batch(2600/7879) done. Loss: 0.6667  lr:0.010000
[ Wed Jul  3 16:22:17 2024 ] 	Batch(2700/7879) done. Loss: 0.1440  lr:0.010000
[ Wed Jul  3 16:22:36 2024 ] 	Batch(2800/7879) done. Loss: 0.6630  lr:0.010000
[ Wed Jul  3 16:22:54 2024 ] 	Batch(2900/7879) done. Loss: 0.0713  lr:0.010000
[ Wed Jul  3 16:23:12 2024 ] 
Training: Epoch [47/120], Step [2999], Loss: 0.6407000422477722, Training Accuracy: 89.61666666666666
[ Wed Jul  3 16:23:12 2024 ] 	Batch(3000/7879) done. Loss: 0.3272  lr:0.010000
[ Wed Jul  3 16:23:30 2024 ] 	Batch(3100/7879) done. Loss: 0.0750  lr:0.010000
[ Wed Jul  3 16:23:48 2024 ] 	Batch(3200/7879) done. Loss: 1.2141  lr:0.010000
[ Wed Jul  3 16:24:06 2024 ] 	Batch(3300/7879) done. Loss: 0.1342  lr:0.010000
[ Wed Jul  3 16:24:25 2024 ] 	Batch(3400/7879) done. Loss: 0.6477  lr:0.010000
[ Wed Jul  3 16:24:43 2024 ] 
Training: Epoch [47/120], Step [3499], Loss: 0.10649044066667557, Training Accuracy: 89.58928571428572
[ Wed Jul  3 16:24:43 2024 ] 	Batch(3500/7879) done. Loss: 0.0901  lr:0.010000
[ Wed Jul  3 16:25:02 2024 ] 	Batch(3600/7879) done. Loss: 0.1187  lr:0.010000
[ Wed Jul  3 16:25:20 2024 ] 	Batch(3700/7879) done. Loss: 0.1835  lr:0.010000
[ Wed Jul  3 16:25:38 2024 ] 	Batch(3800/7879) done. Loss: 0.1981  lr:0.010000
[ Wed Jul  3 16:25:56 2024 ] 	Batch(3900/7879) done. Loss: 0.2350  lr:0.010000
[ Wed Jul  3 16:26:13 2024 ] 
Training: Epoch [47/120], Step [3999], Loss: 0.20414219796657562, Training Accuracy: 89.44375
[ Wed Jul  3 16:26:14 2024 ] 	Batch(4000/7879) done. Loss: 0.1250  lr:0.010000
[ Wed Jul  3 16:26:32 2024 ] 	Batch(4100/7879) done. Loss: 0.0099  lr:0.010000
[ Wed Jul  3 16:26:51 2024 ] 	Batch(4200/7879) done. Loss: 0.0796  lr:0.010000
[ Wed Jul  3 16:27:09 2024 ] 	Batch(4300/7879) done. Loss: 0.0629  lr:0.010000
[ Wed Jul  3 16:27:28 2024 ] 	Batch(4400/7879) done. Loss: 1.5771  lr:0.010000
[ Wed Jul  3 16:27:46 2024 ] 
Training: Epoch [47/120], Step [4499], Loss: 0.337783545255661, Training Accuracy: 89.32777777777777
[ Wed Jul  3 16:27:46 2024 ] 	Batch(4500/7879) done. Loss: 0.1009  lr:0.010000
[ Wed Jul  3 16:28:04 2024 ] 	Batch(4600/7879) done. Loss: 1.0041  lr:0.010000
[ Wed Jul  3 16:28:22 2024 ] 	Batch(4700/7879) done. Loss: 0.4497  lr:0.010000
[ Wed Jul  3 16:28:40 2024 ] 	Batch(4800/7879) done. Loss: 0.0144  lr:0.010000
[ Wed Jul  3 16:28:58 2024 ] 	Batch(4900/7879) done. Loss: 0.1137  lr:0.010000
[ Wed Jul  3 16:29:15 2024 ] 
Training: Epoch [47/120], Step [4999], Loss: 0.01241286564618349, Training Accuracy: 89.19749999999999
[ Wed Jul  3 16:29:16 2024 ] 	Batch(5000/7879) done. Loss: 0.5580  lr:0.010000
[ Wed Jul  3 16:29:34 2024 ] 	Batch(5100/7879) done. Loss: 1.0694  lr:0.010000
[ Wed Jul  3 16:29:51 2024 ] 	Batch(5200/7879) done. Loss: 0.1732  lr:0.010000
[ Wed Jul  3 16:30:09 2024 ] 	Batch(5300/7879) done. Loss: 0.5593  lr:0.010000
[ Wed Jul  3 16:30:27 2024 ] 	Batch(5400/7879) done. Loss: 0.3062  lr:0.010000
[ Wed Jul  3 16:30:45 2024 ] 
Training: Epoch [47/120], Step [5499], Loss: 1.0151360034942627, Training Accuracy: 89.13409090909092
[ Wed Jul  3 16:30:45 2024 ] 	Batch(5500/7879) done. Loss: 0.9962  lr:0.010000
[ Wed Jul  3 16:31:03 2024 ] 	Batch(5600/7879) done. Loss: 0.0543  lr:0.010000
[ Wed Jul  3 16:31:21 2024 ] 	Batch(5700/7879) done. Loss: 0.5725  lr:0.010000
[ Wed Jul  3 16:31:39 2024 ] 	Batch(5800/7879) done. Loss: 0.4112  lr:0.010000
[ Wed Jul  3 16:31:57 2024 ] 	Batch(5900/7879) done. Loss: 0.2777  lr:0.010000
[ Wed Jul  3 16:32:14 2024 ] 
Training: Epoch [47/120], Step [5999], Loss: 0.6358720064163208, Training Accuracy: 89.03958333333334
[ Wed Jul  3 16:32:15 2024 ] 	Batch(6000/7879) done. Loss: 0.0565  lr:0.010000
[ Wed Jul  3 16:32:33 2024 ] 	Batch(6100/7879) done. Loss: 0.1072  lr:0.010000
[ Wed Jul  3 16:32:52 2024 ] 	Batch(6200/7879) done. Loss: 0.0313  lr:0.010000
[ Wed Jul  3 16:33:10 2024 ] 	Batch(6300/7879) done. Loss: 0.9957  lr:0.010000
[ Wed Jul  3 16:33:29 2024 ] 	Batch(6400/7879) done. Loss: 0.1191  lr:0.010000
[ Wed Jul  3 16:33:47 2024 ] 
Training: Epoch [47/120], Step [6499], Loss: 0.06835821270942688, Training Accuracy: 88.93653846153846
[ Wed Jul  3 16:33:47 2024 ] 	Batch(6500/7879) done. Loss: 0.1714  lr:0.010000
[ Wed Jul  3 16:34:05 2024 ] 	Batch(6600/7879) done. Loss: 0.6468  lr:0.010000
[ Wed Jul  3 16:34:22 2024 ] 	Batch(6700/7879) done. Loss: 0.1867  lr:0.010000
[ Wed Jul  3 16:34:40 2024 ] 	Batch(6800/7879) done. Loss: 0.1813  lr:0.010000
[ Wed Jul  3 16:34:58 2024 ] 	Batch(6900/7879) done. Loss: 0.0234  lr:0.010000
[ Wed Jul  3 16:35:16 2024 ] 
Training: Epoch [47/120], Step [6999], Loss: 0.0694873183965683, Training Accuracy: 88.93214285714286
[ Wed Jul  3 16:35:16 2024 ] 	Batch(7000/7879) done. Loss: 0.0261  lr:0.010000
[ Wed Jul  3 16:35:34 2024 ] 	Batch(7100/7879) done. Loss: 0.3532  lr:0.010000
[ Wed Jul  3 16:35:52 2024 ] 	Batch(7200/7879) done. Loss: 0.3130  lr:0.010000
[ Wed Jul  3 16:36:10 2024 ] 	Batch(7300/7879) done. Loss: 0.9564  lr:0.010000
[ Wed Jul  3 16:36:28 2024 ] 	Batch(7400/7879) done. Loss: 0.3700  lr:0.010000
[ Wed Jul  3 16:36:45 2024 ] 
Training: Epoch [47/120], Step [7499], Loss: 0.3063761293888092, Training Accuracy: 88.805
[ Wed Jul  3 16:36:45 2024 ] 	Batch(7500/7879) done. Loss: 0.8756  lr:0.010000
[ Wed Jul  3 16:37:03 2024 ] 	Batch(7600/7879) done. Loss: 0.1613  lr:0.010000
[ Wed Jul  3 16:37:22 2024 ] 	Batch(7700/7879) done. Loss: 0.1811  lr:0.010000
[ Wed Jul  3 16:37:40 2024 ] 	Batch(7800/7879) done. Loss: 0.0641  lr:0.010000
[ Wed Jul  3 16:37:55 2024 ] 	Mean training loss: 0.3606.
[ Wed Jul  3 16:37:55 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 16:37:55 2024 ] Training epoch: 49
[ Wed Jul  3 16:37:56 2024 ] 	Batch(0/7879) done. Loss: 1.2463  lr:0.010000
[ Wed Jul  3 16:38:13 2024 ] 	Batch(100/7879) done. Loss: 0.6459  lr:0.010000
[ Wed Jul  3 16:38:31 2024 ] 	Batch(200/7879) done. Loss: 0.0917  lr:0.010000
[ Wed Jul  3 16:38:49 2024 ] 	Batch(300/7879) done. Loss: 0.0378  lr:0.010000
[ Wed Jul  3 16:39:07 2024 ] 	Batch(400/7879) done. Loss: 0.1346  lr:0.010000
[ Wed Jul  3 16:39:25 2024 ] 
Training: Epoch [48/120], Step [499], Loss: 0.3457549810409546, Training Accuracy: 89.4
[ Wed Jul  3 16:39:25 2024 ] 	Batch(500/7879) done. Loss: 0.2525  lr:0.010000
[ Wed Jul  3 16:39:43 2024 ] 	Batch(600/7879) done. Loss: 0.2514  lr:0.010000
[ Wed Jul  3 16:40:01 2024 ] 	Batch(700/7879) done. Loss: 0.3437  lr:0.010000
[ Wed Jul  3 16:40:19 2024 ] 	Batch(800/7879) done. Loss: 0.1532  lr:0.010000
[ Wed Jul  3 16:40:37 2024 ] 	Batch(900/7879) done. Loss: 0.4379  lr:0.010000
[ Wed Jul  3 16:40:55 2024 ] 
Training: Epoch [48/120], Step [999], Loss: 0.04727398231625557, Training Accuracy: 89.875
[ Wed Jul  3 16:40:56 2024 ] 	Batch(1000/7879) done. Loss: 1.1449  lr:0.010000
[ Wed Jul  3 16:41:14 2024 ] 	Batch(1100/7879) done. Loss: 0.1584  lr:0.010000
[ Wed Jul  3 16:41:33 2024 ] 	Batch(1200/7879) done. Loss: 0.4734  lr:0.010000
[ Wed Jul  3 16:41:51 2024 ] 	Batch(1300/7879) done. Loss: 0.0465  lr:0.010000
[ Wed Jul  3 16:42:09 2024 ] 	Batch(1400/7879) done. Loss: 0.2572  lr:0.010000
[ Wed Jul  3 16:42:27 2024 ] 
Training: Epoch [48/120], Step [1499], Loss: 0.03744322806596756, Training Accuracy: 89.79166666666667
[ Wed Jul  3 16:42:27 2024 ] 	Batch(1500/7879) done. Loss: 0.4635  lr:0.010000
[ Wed Jul  3 16:42:45 2024 ] 	Batch(1600/7879) done. Loss: 0.0169  lr:0.010000
[ Wed Jul  3 16:43:04 2024 ] 	Batch(1700/7879) done. Loss: 0.6531  lr:0.010000
[ Wed Jul  3 16:43:22 2024 ] 	Batch(1800/7879) done. Loss: 0.3045  lr:0.010000
[ Wed Jul  3 16:43:41 2024 ] 	Batch(1900/7879) done. Loss: 0.1215  lr:0.010000
[ Wed Jul  3 16:43:59 2024 ] 
Training: Epoch [48/120], Step [1999], Loss: 0.3901269733905792, Training Accuracy: 89.6125
[ Wed Jul  3 16:43:59 2024 ] 	Batch(2000/7879) done. Loss: 0.1525  lr:0.010000
[ Wed Jul  3 16:44:17 2024 ] 	Batch(2100/7879) done. Loss: 0.0774  lr:0.010000
[ Wed Jul  3 16:44:35 2024 ] 	Batch(2200/7879) done. Loss: 0.1927  lr:0.010000
[ Wed Jul  3 16:44:53 2024 ] 	Batch(2300/7879) done. Loss: 0.7095  lr:0.010000
[ Wed Jul  3 16:45:11 2024 ] 	Batch(2400/7879) done. Loss: 0.2106  lr:0.010000
[ Wed Jul  3 16:45:29 2024 ] 
Training: Epoch [48/120], Step [2499], Loss: 0.40001827478408813, Training Accuracy: 89.46
[ Wed Jul  3 16:45:29 2024 ] 	Batch(2500/7879) done. Loss: 0.3603  lr:0.010000
[ Wed Jul  3 16:45:47 2024 ] 	Batch(2600/7879) done. Loss: 0.7103  lr:0.010000
[ Wed Jul  3 16:46:04 2024 ] 	Batch(2700/7879) done. Loss: 0.2886  lr:0.010000
[ Wed Jul  3 16:46:22 2024 ] 	Batch(2800/7879) done. Loss: 0.1991  lr:0.010000
[ Wed Jul  3 16:46:40 2024 ] 	Batch(2900/7879) done. Loss: 0.4549  lr:0.010000
[ Wed Jul  3 16:46:58 2024 ] 
Training: Epoch [48/120], Step [2999], Loss: 0.6097009181976318, Training Accuracy: 89.43333333333334
[ Wed Jul  3 16:46:58 2024 ] 	Batch(3000/7879) done. Loss: 0.2128  lr:0.010000
[ Wed Jul  3 16:47:16 2024 ] 	Batch(3100/7879) done. Loss: 0.0105  lr:0.010000
[ Wed Jul  3 16:47:34 2024 ] 	Batch(3200/7879) done. Loss: 0.3529  lr:0.010000
[ Wed Jul  3 16:47:52 2024 ] 	Batch(3300/7879) done. Loss: 0.3601  lr:0.010000
[ Wed Jul  3 16:48:10 2024 ] 	Batch(3400/7879) done. Loss: 1.0345  lr:0.010000
[ Wed Jul  3 16:48:27 2024 ] 
Training: Epoch [48/120], Step [3499], Loss: 0.19567030668258667, Training Accuracy: 89.56428571428572
[ Wed Jul  3 16:48:28 2024 ] 	Batch(3500/7879) done. Loss: 0.7153  lr:0.010000
[ Wed Jul  3 16:48:46 2024 ] 	Batch(3600/7879) done. Loss: 0.4301  lr:0.010000
[ Wed Jul  3 16:49:04 2024 ] 	Batch(3700/7879) done. Loss: 0.2290  lr:0.010000
[ Wed Jul  3 16:49:22 2024 ] 	Batch(3800/7879) done. Loss: 0.0309  lr:0.010000
[ Wed Jul  3 16:49:40 2024 ] 	Batch(3900/7879) done. Loss: 0.7436  lr:0.010000
[ Wed Jul  3 16:49:59 2024 ] 
Training: Epoch [48/120], Step [3999], Loss: 1.1563477516174316, Training Accuracy: 89.43124999999999
[ Wed Jul  3 16:49:59 2024 ] 	Batch(4000/7879) done. Loss: 0.4465  lr:0.010000
[ Wed Jul  3 16:50:17 2024 ] 	Batch(4100/7879) done. Loss: 0.0261  lr:0.010000
[ Wed Jul  3 16:50:35 2024 ] 	Batch(4200/7879) done. Loss: 0.6007  lr:0.010000
[ Wed Jul  3 16:50:53 2024 ] 	Batch(4300/7879) done. Loss: 0.0562  lr:0.010000
[ Wed Jul  3 16:51:11 2024 ] 	Batch(4400/7879) done. Loss: 0.1927  lr:0.010000
[ Wed Jul  3 16:51:29 2024 ] 
Training: Epoch [48/120], Step [4499], Loss: 0.3186905086040497, Training Accuracy: 89.39166666666667
[ Wed Jul  3 16:51:29 2024 ] 	Batch(4500/7879) done. Loss: 0.3979  lr:0.010000
[ Wed Jul  3 16:51:47 2024 ] 	Batch(4600/7879) done. Loss: 0.2206  lr:0.010000
[ Wed Jul  3 16:52:05 2024 ] 	Batch(4700/7879) done. Loss: 0.0378  lr:0.010000
[ Wed Jul  3 16:52:23 2024 ] 	Batch(4800/7879) done. Loss: 0.3157  lr:0.010000
[ Wed Jul  3 16:52:42 2024 ] 	Batch(4900/7879) done. Loss: 0.3127  lr:0.010000
[ Wed Jul  3 16:53:00 2024 ] 
Training: Epoch [48/120], Step [4999], Loss: 1.326203465461731, Training Accuracy: 89.3625
[ Wed Jul  3 16:53:00 2024 ] 	Batch(5000/7879) done. Loss: 0.5777  lr:0.010000
[ Wed Jul  3 16:53:18 2024 ] 	Batch(5100/7879) done. Loss: 0.1752  lr:0.010000
[ Wed Jul  3 16:53:36 2024 ] 	Batch(5200/7879) done. Loss: 0.3816  lr:0.010000
[ Wed Jul  3 16:53:54 2024 ] 	Batch(5300/7879) done. Loss: 0.3449  lr:0.010000
[ Wed Jul  3 16:54:12 2024 ] 	Batch(5400/7879) done. Loss: 0.0113  lr:0.010000
[ Wed Jul  3 16:54:30 2024 ] 
Training: Epoch [48/120], Step [5499], Loss: 0.414157897233963, Training Accuracy: 89.28636363636365
[ Wed Jul  3 16:54:30 2024 ] 	Batch(5500/7879) done. Loss: 0.6110  lr:0.010000
[ Wed Jul  3 16:54:48 2024 ] 	Batch(5600/7879) done. Loss: 0.0701  lr:0.010000
[ Wed Jul  3 16:55:06 2024 ] 	Batch(5700/7879) done. Loss: 0.4933  lr:0.010000
[ Wed Jul  3 16:55:24 2024 ] 	Batch(5800/7879) done. Loss: 0.1033  lr:0.010000
[ Wed Jul  3 16:55:42 2024 ] 	Batch(5900/7879) done. Loss: 0.1642  lr:0.010000
[ Wed Jul  3 16:56:00 2024 ] 
Training: Epoch [48/120], Step [5999], Loss: 0.5163174867630005, Training Accuracy: 89.23124999999999
[ Wed Jul  3 16:56:01 2024 ] 	Batch(6000/7879) done. Loss: 0.2417  lr:0.010000
[ Wed Jul  3 16:56:19 2024 ] 	Batch(6100/7879) done. Loss: 0.8048  lr:0.010000
[ Wed Jul  3 16:56:36 2024 ] 	Batch(6200/7879) done. Loss: 1.0981  lr:0.010000
[ Wed Jul  3 16:56:54 2024 ] 	Batch(6300/7879) done. Loss: 0.7313  lr:0.010000
[ Wed Jul  3 16:57:13 2024 ] 	Batch(6400/7879) done. Loss: 0.1194  lr:0.010000
[ Wed Jul  3 16:57:31 2024 ] 
Training: Epoch [48/120], Step [6499], Loss: 0.04579365998506546, Training Accuracy: 89.14038461538462
[ Wed Jul  3 16:57:31 2024 ] 	Batch(6500/7879) done. Loss: 0.4150  lr:0.010000
[ Wed Jul  3 16:57:50 2024 ] 	Batch(6600/7879) done. Loss: 0.2485  lr:0.010000
[ Wed Jul  3 16:58:09 2024 ] 	Batch(6700/7879) done. Loss: 0.5000  lr:0.010000
[ Wed Jul  3 16:58:27 2024 ] 	Batch(6800/7879) done. Loss: 0.8077  lr:0.010000
[ Wed Jul  3 16:58:45 2024 ] 	Batch(6900/7879) done. Loss: 0.3201  lr:0.010000
[ Wed Jul  3 16:59:03 2024 ] 
Training: Epoch [48/120], Step [6999], Loss: 0.31353268027305603, Training Accuracy: 89.02321428571427
[ Wed Jul  3 16:59:03 2024 ] 	Batch(7000/7879) done. Loss: 0.1987  lr:0.010000
[ Wed Jul  3 16:59:21 2024 ] 	Batch(7100/7879) done. Loss: 1.4988  lr:0.010000
[ Wed Jul  3 16:59:39 2024 ] 	Batch(7200/7879) done. Loss: 0.3124  lr:0.010000
[ Wed Jul  3 16:59:58 2024 ] 	Batch(7300/7879) done. Loss: 0.2600  lr:0.010000
[ Wed Jul  3 17:00:17 2024 ] 	Batch(7400/7879) done. Loss: 0.7043  lr:0.010000
[ Wed Jul  3 17:00:35 2024 ] 
Training: Epoch [48/120], Step [7499], Loss: 1.7229841947555542, Training Accuracy: 88.96
[ Wed Jul  3 17:00:35 2024 ] 	Batch(7500/7879) done. Loss: 0.3613  lr:0.010000
[ Wed Jul  3 17:00:54 2024 ] 	Batch(7600/7879) done. Loss: 0.6959  lr:0.010000
[ Wed Jul  3 17:01:12 2024 ] 	Batch(7700/7879) done. Loss: 0.0447  lr:0.010000
[ Wed Jul  3 17:01:30 2024 ] 	Batch(7800/7879) done. Loss: 0.2841  lr:0.010000
[ Wed Jul  3 17:01:44 2024 ] 	Mean training loss: 0.3552.
[ Wed Jul  3 17:01:44 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 17:01:44 2024 ] Training epoch: 50
[ Wed Jul  3 17:01:45 2024 ] 	Batch(0/7879) done. Loss: 0.1838  lr:0.010000
[ Wed Jul  3 17:02:03 2024 ] 	Batch(100/7879) done. Loss: 0.2091  lr:0.010000
[ Wed Jul  3 17:02:22 2024 ] 	Batch(200/7879) done. Loss: 1.1002  lr:0.010000
[ Wed Jul  3 17:02:41 2024 ] 	Batch(300/7879) done. Loss: 0.2248  lr:0.010000
[ Wed Jul  3 17:02:59 2024 ] 	Batch(400/7879) done. Loss: 0.4306  lr:0.010000
[ Wed Jul  3 17:03:18 2024 ] 
Training: Epoch [49/120], Step [499], Loss: 1.096264362335205, Training Accuracy: 88.275
[ Wed Jul  3 17:03:18 2024 ] 	Batch(500/7879) done. Loss: 0.2652  lr:0.010000
[ Wed Jul  3 17:03:37 2024 ] 	Batch(600/7879) done. Loss: 0.2206  lr:0.010000
[ Wed Jul  3 17:03:55 2024 ] 	Batch(700/7879) done. Loss: 0.0483  lr:0.010000
[ Wed Jul  3 17:04:13 2024 ] 	Batch(800/7879) done. Loss: 0.2355  lr:0.010000
[ Wed Jul  3 17:04:31 2024 ] 	Batch(900/7879) done. Loss: 0.0776  lr:0.010000
[ Wed Jul  3 17:04:48 2024 ] 
Training: Epoch [49/120], Step [999], Loss: 0.5607805848121643, Training Accuracy: 88.825
[ Wed Jul  3 17:04:49 2024 ] 	Batch(1000/7879) done. Loss: 0.0476  lr:0.010000
[ Wed Jul  3 17:05:07 2024 ] 	Batch(1100/7879) done. Loss: 0.7799  lr:0.010000
[ Wed Jul  3 17:05:25 2024 ] 	Batch(1200/7879) done. Loss: 0.7093  lr:0.010000
[ Wed Jul  3 17:05:43 2024 ] 	Batch(1300/7879) done. Loss: 0.4755  lr:0.010000
[ Wed Jul  3 17:06:02 2024 ] 	Batch(1400/7879) done. Loss: 0.9473  lr:0.010000
[ Wed Jul  3 17:06:21 2024 ] 
Training: Epoch [49/120], Step [1499], Loss: 0.2383131980895996, Training Accuracy: 89.25
[ Wed Jul  3 17:06:21 2024 ] 	Batch(1500/7879) done. Loss: 0.6435  lr:0.010000
[ Wed Jul  3 17:06:39 2024 ] 	Batch(1600/7879) done. Loss: 0.2467  lr:0.010000
[ Wed Jul  3 17:06:57 2024 ] 	Batch(1700/7879) done. Loss: 0.4978  lr:0.010000
[ Wed Jul  3 17:07:15 2024 ] 	Batch(1800/7879) done. Loss: 0.0195  lr:0.010000
[ Wed Jul  3 17:07:33 2024 ] 	Batch(1900/7879) done. Loss: 0.0551  lr:0.010000
[ Wed Jul  3 17:07:51 2024 ] 
Training: Epoch [49/120], Step [1999], Loss: 0.624143660068512, Training Accuracy: 89.3375
[ Wed Jul  3 17:07:52 2024 ] 	Batch(2000/7879) done. Loss: 0.4457  lr:0.010000
[ Wed Jul  3 17:08:10 2024 ] 	Batch(2100/7879) done. Loss: 0.2323  lr:0.010000
[ Wed Jul  3 17:08:28 2024 ] 	Batch(2200/7879) done. Loss: 0.1922  lr:0.010000
[ Wed Jul  3 17:08:45 2024 ] 	Batch(2300/7879) done. Loss: 0.0500  lr:0.010000
[ Wed Jul  3 17:09:04 2024 ] 	Batch(2400/7879) done. Loss: 0.4688  lr:0.010000
[ Wed Jul  3 17:09:21 2024 ] 
Training: Epoch [49/120], Step [2499], Loss: 0.07678539305925369, Training Accuracy: 89.33
[ Wed Jul  3 17:09:22 2024 ] 	Batch(2500/7879) done. Loss: 0.0642  lr:0.010000
[ Wed Jul  3 17:09:40 2024 ] 	Batch(2600/7879) done. Loss: 0.3313  lr:0.010000
[ Wed Jul  3 17:09:58 2024 ] 	Batch(2700/7879) done. Loss: 0.3532  lr:0.010000
[ Wed Jul  3 17:10:16 2024 ] 	Batch(2800/7879) done. Loss: 0.1324  lr:0.010000
[ Wed Jul  3 17:10:34 2024 ] 	Batch(2900/7879) done. Loss: 0.4137  lr:0.010000
[ Wed Jul  3 17:10:52 2024 ] 
Training: Epoch [49/120], Step [2999], Loss: 0.17105555534362793, Training Accuracy: 89.28333333333333
[ Wed Jul  3 17:10:52 2024 ] 	Batch(3000/7879) done. Loss: 0.7644  lr:0.010000
[ Wed Jul  3 17:11:10 2024 ] 	Batch(3100/7879) done. Loss: 0.3240  lr:0.010000
[ Wed Jul  3 17:11:28 2024 ] 	Batch(3200/7879) done. Loss: 0.2289  lr:0.010000
[ Wed Jul  3 17:11:46 2024 ] 	Batch(3300/7879) done. Loss: 0.4129  lr:0.010000
[ Wed Jul  3 17:12:04 2024 ] 	Batch(3400/7879) done. Loss: 0.3124  lr:0.010000
[ Wed Jul  3 17:12:22 2024 ] 
Training: Epoch [49/120], Step [3499], Loss: 0.2816803455352783, Training Accuracy: 89.16071428571428
[ Wed Jul  3 17:12:22 2024 ] 	Batch(3500/7879) done. Loss: 0.5605  lr:0.010000
[ Wed Jul  3 17:12:40 2024 ] 	Batch(3600/7879) done. Loss: 0.5337  lr:0.010000
[ Wed Jul  3 17:12:58 2024 ] 	Batch(3700/7879) done. Loss: 0.5080  lr:0.010000
[ Wed Jul  3 17:13:16 2024 ] 	Batch(3800/7879) done. Loss: 0.0397  lr:0.010000
[ Wed Jul  3 17:13:34 2024 ] 	Batch(3900/7879) done. Loss: 0.0356  lr:0.010000
[ Wed Jul  3 17:13:51 2024 ] 
Training: Epoch [49/120], Step [3999], Loss: 0.6951905488967896, Training Accuracy: 89.096875
[ Wed Jul  3 17:13:52 2024 ] 	Batch(4000/7879) done. Loss: 0.3468  lr:0.010000
[ Wed Jul  3 17:14:09 2024 ] 	Batch(4100/7879) done. Loss: 0.2866  lr:0.010000
[ Wed Jul  3 17:14:27 2024 ] 	Batch(4200/7879) done. Loss: 0.0158  lr:0.010000
[ Wed Jul  3 17:14:45 2024 ] 	Batch(4300/7879) done. Loss: 0.2461  lr:0.010000
[ Wed Jul  3 17:15:03 2024 ] 	Batch(4400/7879) done. Loss: 0.0596  lr:0.010000
[ Wed Jul  3 17:15:21 2024 ] 
Training: Epoch [49/120], Step [4499], Loss: 0.16497032344341278, Training Accuracy: 89.14444444444445
[ Wed Jul  3 17:15:21 2024 ] 	Batch(4500/7879) done. Loss: 0.6259  lr:0.010000
[ Wed Jul  3 17:15:39 2024 ] 	Batch(4600/7879) done. Loss: 0.8446  lr:0.010000
[ Wed Jul  3 17:15:57 2024 ] 	Batch(4700/7879) done. Loss: 0.6069  lr:0.010000
[ Wed Jul  3 17:16:15 2024 ] 	Batch(4800/7879) done. Loss: 0.1101  lr:0.010000
[ Wed Jul  3 17:16:33 2024 ] 	Batch(4900/7879) done. Loss: 0.2210  lr:0.010000
[ Wed Jul  3 17:16:50 2024 ] 
Training: Epoch [49/120], Step [4999], Loss: 0.5326879024505615, Training Accuracy: 89.0725
[ Wed Jul  3 17:16:51 2024 ] 	Batch(5000/7879) done. Loss: 0.4095  lr:0.010000
[ Wed Jul  3 17:17:09 2024 ] 	Batch(5100/7879) done. Loss: 0.8919  lr:0.010000
[ Wed Jul  3 17:17:27 2024 ] 	Batch(5200/7879) done. Loss: 0.0932  lr:0.010000
[ Wed Jul  3 17:17:45 2024 ] 	Batch(5300/7879) done. Loss: 0.4629  lr:0.010000
[ Wed Jul  3 17:18:04 2024 ] 	Batch(5400/7879) done. Loss: 0.8579  lr:0.010000
[ Wed Jul  3 17:18:22 2024 ] 
Training: Epoch [49/120], Step [5499], Loss: 0.5387840867042542, Training Accuracy: 89.075
[ Wed Jul  3 17:18:22 2024 ] 	Batch(5500/7879) done. Loss: 0.0584  lr:0.010000
[ Wed Jul  3 17:18:41 2024 ] 	Batch(5600/7879) done. Loss: 0.2574  lr:0.010000
[ Wed Jul  3 17:18:59 2024 ] 	Batch(5700/7879) done. Loss: 0.3280  lr:0.010000
[ Wed Jul  3 17:19:18 2024 ] 	Batch(5800/7879) done. Loss: 0.1802  lr:0.010000
[ Wed Jul  3 17:19:36 2024 ] 	Batch(5900/7879) done. Loss: 0.4380  lr:0.010000
[ Wed Jul  3 17:19:55 2024 ] 
Training: Epoch [49/120], Step [5999], Loss: 0.2851220369338989, Training Accuracy: 89.0125
[ Wed Jul  3 17:19:55 2024 ] 	Batch(6000/7879) done. Loss: 0.2482  lr:0.010000
[ Wed Jul  3 17:20:13 2024 ] 	Batch(6100/7879) done. Loss: 0.4150  lr:0.010000
[ Wed Jul  3 17:20:32 2024 ] 	Batch(6200/7879) done. Loss: 0.6355  lr:0.010000
[ Wed Jul  3 17:20:50 2024 ] 	Batch(6300/7879) done. Loss: 0.4099  lr:0.010000
[ Wed Jul  3 17:21:09 2024 ] 	Batch(6400/7879) done. Loss: 0.6178  lr:0.010000
[ Wed Jul  3 17:21:27 2024 ] 
Training: Epoch [49/120], Step [6499], Loss: 0.0881061777472496, Training Accuracy: 88.98076923076923
[ Wed Jul  3 17:21:28 2024 ] 	Batch(6500/7879) done. Loss: 0.0449  lr:0.010000
[ Wed Jul  3 17:21:46 2024 ] 	Batch(6600/7879) done. Loss: 0.3237  lr:0.010000
[ Wed Jul  3 17:22:05 2024 ] 	Batch(6700/7879) done. Loss: 1.0047  lr:0.010000
[ Wed Jul  3 17:22:23 2024 ] 	Batch(6800/7879) done. Loss: 0.2314  lr:0.010000
[ Wed Jul  3 17:22:41 2024 ] 	Batch(6900/7879) done. Loss: 0.0682  lr:0.010000
[ Wed Jul  3 17:22:59 2024 ] 
Training: Epoch [49/120], Step [6999], Loss: 0.135358065366745, Training Accuracy: 88.92678571428571
[ Wed Jul  3 17:22:59 2024 ] 	Batch(7000/7879) done. Loss: 0.2764  lr:0.010000
[ Wed Jul  3 17:23:17 2024 ] 	Batch(7100/7879) done. Loss: 0.4937  lr:0.010000
[ Wed Jul  3 17:23:35 2024 ] 	Batch(7200/7879) done. Loss: 0.5568  lr:0.010000
[ Wed Jul  3 17:23:53 2024 ] 	Batch(7300/7879) done. Loss: 0.1637  lr:0.010000
[ Wed Jul  3 17:24:10 2024 ] 	Batch(7400/7879) done. Loss: 0.8307  lr:0.010000
[ Wed Jul  3 17:24:28 2024 ] 
Training: Epoch [49/120], Step [7499], Loss: 0.19715829193592072, Training Accuracy: 88.84666666666666
[ Wed Jul  3 17:24:28 2024 ] 	Batch(7500/7879) done. Loss: 0.0252  lr:0.010000
[ Wed Jul  3 17:24:46 2024 ] 	Batch(7600/7879) done. Loss: 0.9166  lr:0.010000
[ Wed Jul  3 17:25:04 2024 ] 	Batch(7700/7879) done. Loss: 0.5199  lr:0.010000
[ Wed Jul  3 17:25:22 2024 ] 	Batch(7800/7879) done. Loss: 0.3962  lr:0.010000
[ Wed Jul  3 17:25:36 2024 ] 	Mean training loss: 0.3559.
[ Wed Jul  3 17:25:36 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 17:25:36 2024 ] Eval epoch: 50
[ Wed Jul  3 17:30:23 2024 ] 	Mean val loss of 6365 batches: 2.247397641520416.
[ Wed Jul  3 17:30:23 2024 ] Training epoch: 51
[ Wed Jul  3 17:30:24 2024 ] 	Batch(0/7879) done. Loss: 0.1794  lr:0.010000
[ Wed Jul  3 17:30:41 2024 ] 	Batch(100/7879) done. Loss: 1.1573  lr:0.010000
[ Wed Jul  3 17:30:59 2024 ] 	Batch(200/7879) done. Loss: 0.1999  lr:0.010000
[ Wed Jul  3 17:31:17 2024 ] 	Batch(300/7879) done. Loss: 0.5099  lr:0.010000
[ Wed Jul  3 17:31:35 2024 ] 	Batch(400/7879) done. Loss: 0.2030  lr:0.010000
[ Wed Jul  3 17:31:53 2024 ] 
Training: Epoch [50/120], Step [499], Loss: 0.03329399973154068, Training Accuracy: 89.9
[ Wed Jul  3 17:31:53 2024 ] 	Batch(500/7879) done. Loss: 0.5328  lr:0.010000
[ Wed Jul  3 17:32:11 2024 ] 	Batch(600/7879) done. Loss: 0.1261  lr:0.010000
[ Wed Jul  3 17:32:29 2024 ] 	Batch(700/7879) done. Loss: 0.0740  lr:0.010000
[ Wed Jul  3 17:32:47 2024 ] 	Batch(800/7879) done. Loss: 0.4482  lr:0.010000
[ Wed Jul  3 17:33:05 2024 ] 	Batch(900/7879) done. Loss: 0.1912  lr:0.010000
[ Wed Jul  3 17:33:22 2024 ] 
Training: Epoch [50/120], Step [999], Loss: 0.3663710653781891, Training Accuracy: 90.1625
[ Wed Jul  3 17:33:23 2024 ] 	Batch(1000/7879) done. Loss: 0.3218  lr:0.010000
[ Wed Jul  3 17:33:40 2024 ] 	Batch(1100/7879) done. Loss: 0.0809  lr:0.010000
[ Wed Jul  3 17:33:59 2024 ] 	Batch(1200/7879) done. Loss: 0.4548  lr:0.010000
[ Wed Jul  3 17:34:17 2024 ] 	Batch(1300/7879) done. Loss: 1.0942  lr:0.010000
[ Wed Jul  3 17:34:34 2024 ] 	Batch(1400/7879) done. Loss: 0.4331  lr:0.010000
[ Wed Jul  3 17:34:52 2024 ] 
Training: Epoch [50/120], Step [1499], Loss: 0.32934582233428955, Training Accuracy: 89.88333333333334
[ Wed Jul  3 17:34:52 2024 ] 	Batch(1500/7879) done. Loss: 0.5178  lr:0.010000
[ Wed Jul  3 17:35:10 2024 ] 	Batch(1600/7879) done. Loss: 0.0322  lr:0.010000
[ Wed Jul  3 17:35:28 2024 ] 	Batch(1700/7879) done. Loss: 0.2538  lr:0.010000
[ Wed Jul  3 17:35:46 2024 ] 	Batch(1800/7879) done. Loss: 0.3005  lr:0.010000
[ Wed Jul  3 17:36:04 2024 ] 	Batch(1900/7879) done. Loss: 0.5650  lr:0.010000
[ Wed Jul  3 17:36:22 2024 ] 
Training: Epoch [50/120], Step [1999], Loss: 0.3505646288394928, Training Accuracy: 89.8
[ Wed Jul  3 17:36:22 2024 ] 	Batch(2000/7879) done. Loss: 0.1442  lr:0.010000
[ Wed Jul  3 17:36:40 2024 ] 	Batch(2100/7879) done. Loss: 0.1077  lr:0.010000
[ Wed Jul  3 17:36:58 2024 ] 	Batch(2200/7879) done. Loss: 0.0672  lr:0.010000
[ Wed Jul  3 17:37:17 2024 ] 	Batch(2300/7879) done. Loss: 0.4396  lr:0.010000
[ Wed Jul  3 17:37:35 2024 ] 	Batch(2400/7879) done. Loss: 0.5226  lr:0.010000
[ Wed Jul  3 17:37:54 2024 ] 
Training: Epoch [50/120], Step [2499], Loss: 0.40518805384635925, Training Accuracy: 89.515
[ Wed Jul  3 17:37:54 2024 ] 	Batch(2500/7879) done. Loss: 0.3029  lr:0.010000
[ Wed Jul  3 17:38:12 2024 ] 	Batch(2600/7879) done. Loss: 0.6998  lr:0.010000
[ Wed Jul  3 17:38:31 2024 ] 	Batch(2700/7879) done. Loss: 0.2144  lr:0.010000
[ Wed Jul  3 17:38:49 2024 ] 	Batch(2800/7879) done. Loss: 0.8442  lr:0.010000
[ Wed Jul  3 17:39:08 2024 ] 	Batch(2900/7879) done. Loss: 0.1289  lr:0.010000
[ Wed Jul  3 17:39:26 2024 ] 
Training: Epoch [50/120], Step [2999], Loss: 0.25903433561325073, Training Accuracy: 89.4375
[ Wed Jul  3 17:39:26 2024 ] 	Batch(3000/7879) done. Loss: 0.2436  lr:0.010000
[ Wed Jul  3 17:39:44 2024 ] 	Batch(3100/7879) done. Loss: 0.0343  lr:0.010000
[ Wed Jul  3 17:40:02 2024 ] 	Batch(3200/7879) done. Loss: 0.6120  lr:0.010000
[ Wed Jul  3 17:40:20 2024 ] 	Batch(3300/7879) done. Loss: 0.0544  lr:0.010000
[ Wed Jul  3 17:40:38 2024 ] 	Batch(3400/7879) done. Loss: 0.8102  lr:0.010000
[ Wed Jul  3 17:40:55 2024 ] 
Training: Epoch [50/120], Step [3499], Loss: 0.2097802609205246, Training Accuracy: 89.32142857142857
[ Wed Jul  3 17:40:56 2024 ] 	Batch(3500/7879) done. Loss: 0.2329  lr:0.010000
[ Wed Jul  3 17:41:14 2024 ] 	Batch(3600/7879) done. Loss: 0.5080  lr:0.010000
[ Wed Jul  3 17:41:32 2024 ] 	Batch(3700/7879) done. Loss: 0.2962  lr:0.010000
[ Wed Jul  3 17:41:49 2024 ] 	Batch(3800/7879) done. Loss: 0.6158  lr:0.010000
[ Wed Jul  3 17:42:08 2024 ] 	Batch(3900/7879) done. Loss: 0.3349  lr:0.010000
[ Wed Jul  3 17:42:26 2024 ] 
Training: Epoch [50/120], Step [3999], Loss: 0.06433569639921188, Training Accuracy: 89.20937500000001
[ Wed Jul  3 17:42:26 2024 ] 	Batch(4000/7879) done. Loss: 0.5807  lr:0.010000
[ Wed Jul  3 17:42:44 2024 ] 	Batch(4100/7879) done. Loss: 0.0329  lr:0.010000
[ Wed Jul  3 17:43:02 2024 ] 	Batch(4200/7879) done. Loss: 0.6361  lr:0.010000
[ Wed Jul  3 17:43:20 2024 ] 	Batch(4300/7879) done. Loss: 0.2390  lr:0.010000
[ Wed Jul  3 17:43:39 2024 ] 	Batch(4400/7879) done. Loss: 0.5541  lr:0.010000
[ Wed Jul  3 17:43:57 2024 ] 
Training: Epoch [50/120], Step [4499], Loss: 0.5246289372444153, Training Accuracy: 89.01944444444445
[ Wed Jul  3 17:43:57 2024 ] 	Batch(4500/7879) done. Loss: 0.4148  lr:0.010000
[ Wed Jul  3 17:44:15 2024 ] 	Batch(4600/7879) done. Loss: 1.0473  lr:0.010000
[ Wed Jul  3 17:44:33 2024 ] 	Batch(4700/7879) done. Loss: 1.2859  lr:0.010000
[ Wed Jul  3 17:44:52 2024 ] 	Batch(4800/7879) done. Loss: 0.0392  lr:0.010000
[ Wed Jul  3 17:45:10 2024 ] 	Batch(4900/7879) done. Loss: 0.9829  lr:0.010000
[ Wed Jul  3 17:45:28 2024 ] 
Training: Epoch [50/120], Step [4999], Loss: 0.3189394772052765, Training Accuracy: 88.98
[ Wed Jul  3 17:45:28 2024 ] 	Batch(5000/7879) done. Loss: 0.0063  lr:0.010000
[ Wed Jul  3 17:45:46 2024 ] 	Batch(5100/7879) done. Loss: 0.0646  lr:0.010000
[ Wed Jul  3 17:46:04 2024 ] 	Batch(5200/7879) done. Loss: 0.5443  lr:0.010000
[ Wed Jul  3 17:46:23 2024 ] 	Batch(5300/7879) done. Loss: 0.1448  lr:0.010000
[ Wed Jul  3 17:46:41 2024 ] 	Batch(5400/7879) done. Loss: 1.0741  lr:0.010000
[ Wed Jul  3 17:46:59 2024 ] 
Training: Epoch [50/120], Step [5499], Loss: 0.15375524759292603, Training Accuracy: 89.0090909090909
[ Wed Jul  3 17:46:59 2024 ] 	Batch(5500/7879) done. Loss: 0.2756  lr:0.010000
[ Wed Jul  3 17:47:17 2024 ] 	Batch(5600/7879) done. Loss: 0.1895  lr:0.010000
[ Wed Jul  3 17:47:35 2024 ] 	Batch(5700/7879) done. Loss: 0.4501  lr:0.010000
[ Wed Jul  3 17:47:54 2024 ] 	Batch(5800/7879) done. Loss: 0.1081  lr:0.010000
[ Wed Jul  3 17:48:12 2024 ] 	Batch(5900/7879) done. Loss: 0.3117  lr:0.010000
[ Wed Jul  3 17:48:30 2024 ] 
Training: Epoch [50/120], Step [5999], Loss: 0.34306395053863525, Training Accuracy: 89.01666666666667
[ Wed Jul  3 17:48:30 2024 ] 	Batch(6000/7879) done. Loss: 0.0698  lr:0.010000
[ Wed Jul  3 17:48:49 2024 ] 	Batch(6100/7879) done. Loss: 0.5818  lr:0.010000
[ Wed Jul  3 17:49:08 2024 ] 	Batch(6200/7879) done. Loss: 0.4849  lr:0.010000
[ Wed Jul  3 17:49:27 2024 ] 	Batch(6300/7879) done. Loss: 0.0197  lr:0.010000
[ Wed Jul  3 17:49:46 2024 ] 	Batch(6400/7879) done. Loss: 0.3539  lr:0.010000
[ Wed Jul  3 17:50:04 2024 ] 
Training: Epoch [50/120], Step [6499], Loss: 0.03879552707076073, Training Accuracy: 89.05192307692307
[ Wed Jul  3 17:50:04 2024 ] 	Batch(6500/7879) done. Loss: 0.6648  lr:0.010000
[ Wed Jul  3 17:50:23 2024 ] 	Batch(6600/7879) done. Loss: 0.0827  lr:0.010000
[ Wed Jul  3 17:50:42 2024 ] 	Batch(6700/7879) done. Loss: 1.2890  lr:0.010000
[ Wed Jul  3 17:51:01 2024 ] 	Batch(6800/7879) done. Loss: 0.3971  lr:0.010000
[ Wed Jul  3 17:51:19 2024 ] 	Batch(6900/7879) done. Loss: 0.1346  lr:0.010000
[ Wed Jul  3 17:51:37 2024 ] 
Training: Epoch [50/120], Step [6999], Loss: 0.2778075337409973, Training Accuracy: 88.90892857142858
[ Wed Jul  3 17:51:37 2024 ] 	Batch(7000/7879) done. Loss: 0.0112  lr:0.010000
[ Wed Jul  3 17:51:56 2024 ] 	Batch(7100/7879) done. Loss: 0.6653  lr:0.010000
[ Wed Jul  3 17:52:14 2024 ] 	Batch(7200/7879) done. Loss: 0.6718  lr:0.010000
[ Wed Jul  3 17:52:32 2024 ] 	Batch(7300/7879) done. Loss: 0.5135  lr:0.010000
[ Wed Jul  3 17:52:51 2024 ] 	Batch(7400/7879) done. Loss: 0.1458  lr:0.010000
[ Wed Jul  3 17:53:09 2024 ] 
Training: Epoch [50/120], Step [7499], Loss: 0.14892800152301788, Training Accuracy: 88.895
[ Wed Jul  3 17:53:09 2024 ] 	Batch(7500/7879) done. Loss: 1.7874  lr:0.010000
[ Wed Jul  3 17:53:28 2024 ] 	Batch(7600/7879) done. Loss: 0.2143  lr:0.010000
[ Wed Jul  3 17:53:46 2024 ] 	Batch(7700/7879) done. Loss: 0.2147  lr:0.010000
[ Wed Jul  3 17:54:05 2024 ] 	Batch(7800/7879) done. Loss: 0.4043  lr:0.010000
[ Wed Jul  3 17:54:19 2024 ] 	Mean training loss: 0.3494.
[ Wed Jul  3 17:54:19 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 17:54:20 2024 ] Training epoch: 52
[ Wed Jul  3 17:54:20 2024 ] 	Batch(0/7879) done. Loss: 0.0965  lr:0.010000
[ Wed Jul  3 17:54:38 2024 ] 	Batch(100/7879) done. Loss: 0.5840  lr:0.010000
[ Wed Jul  3 17:54:56 2024 ] 	Batch(200/7879) done. Loss: 0.4047  lr:0.010000
[ Wed Jul  3 17:55:14 2024 ] 	Batch(300/7879) done. Loss: 0.5010  lr:0.010000
[ Wed Jul  3 17:55:32 2024 ] 	Batch(400/7879) done. Loss: 1.0805  lr:0.010000
[ Wed Jul  3 17:55:49 2024 ] 
Training: Epoch [51/120], Step [499], Loss: 0.004161117132753134, Training Accuracy: 90.17500000000001
[ Wed Jul  3 17:55:50 2024 ] 	Batch(500/7879) done. Loss: 0.7540  lr:0.010000
[ Wed Jul  3 17:56:08 2024 ] 	Batch(600/7879) done. Loss: 0.7564  lr:0.010000
[ Wed Jul  3 17:56:26 2024 ] 	Batch(700/7879) done. Loss: 0.0944  lr:0.010000
[ Wed Jul  3 17:56:44 2024 ] 	Batch(800/7879) done. Loss: 0.3747  lr:0.010000
[ Wed Jul  3 17:57:02 2024 ] 	Batch(900/7879) done. Loss: 0.2688  lr:0.010000
[ Wed Jul  3 17:57:20 2024 ] 
Training: Epoch [51/120], Step [999], Loss: 0.028621943667531013, Training Accuracy: 89.9
[ Wed Jul  3 17:57:20 2024 ] 	Batch(1000/7879) done. Loss: 0.0754  lr:0.010000
[ Wed Jul  3 17:57:38 2024 ] 	Batch(1100/7879) done. Loss: 0.2846  lr:0.010000
[ Wed Jul  3 17:57:56 2024 ] 	Batch(1200/7879) done. Loss: 0.7368  lr:0.010000
[ Wed Jul  3 17:58:14 2024 ] 	Batch(1300/7879) done. Loss: 0.1341  lr:0.010000
[ Wed Jul  3 17:58:32 2024 ] 	Batch(1400/7879) done. Loss: 0.6165  lr:0.010000
[ Wed Jul  3 17:58:50 2024 ] 
Training: Epoch [51/120], Step [1499], Loss: 0.602755606174469, Training Accuracy: 90.21666666666667
[ Wed Jul  3 17:58:50 2024 ] 	Batch(1500/7879) done. Loss: 0.1405  lr:0.010000
[ Wed Jul  3 17:59:08 2024 ] 	Batch(1600/7879) done. Loss: 0.2280  lr:0.010000
[ Wed Jul  3 17:59:27 2024 ] 	Batch(1700/7879) done. Loss: 0.2266  lr:0.010000
[ Wed Jul  3 17:59:45 2024 ] 	Batch(1800/7879) done. Loss: 0.1883  lr:0.010000
[ Wed Jul  3 18:00:04 2024 ] 	Batch(1900/7879) done. Loss: 0.0068  lr:0.010000
[ Wed Jul  3 18:00:22 2024 ] 
Training: Epoch [51/120], Step [1999], Loss: 0.2540834844112396, Training Accuracy: 90.11875
[ Wed Jul  3 18:00:22 2024 ] 	Batch(2000/7879) done. Loss: 0.7049  lr:0.010000
[ Wed Jul  3 18:00:41 2024 ] 	Batch(2100/7879) done. Loss: 0.0880  lr:0.010000
[ Wed Jul  3 18:00:59 2024 ] 	Batch(2200/7879) done. Loss: 0.3689  lr:0.010000
[ Wed Jul  3 18:01:17 2024 ] 	Batch(2300/7879) done. Loss: 0.2049  lr:0.010000
[ Wed Jul  3 18:01:35 2024 ] 	Batch(2400/7879) done. Loss: 1.4677  lr:0.010000
[ Wed Jul  3 18:01:53 2024 ] 
Training: Epoch [51/120], Step [2499], Loss: 0.08170939236879349, Training Accuracy: 90.03
[ Wed Jul  3 18:01:53 2024 ] 	Batch(2500/7879) done. Loss: 0.0758  lr:0.010000
[ Wed Jul  3 18:02:11 2024 ] 	Batch(2600/7879) done. Loss: 0.0769  lr:0.010000
[ Wed Jul  3 18:02:29 2024 ] 	Batch(2700/7879) done. Loss: 0.6317  lr:0.010000
[ Wed Jul  3 18:02:47 2024 ] 	Batch(2800/7879) done. Loss: 0.1392  lr:0.010000
[ Wed Jul  3 18:03:05 2024 ] 	Batch(2900/7879) done. Loss: 0.6438  lr:0.010000
[ Wed Jul  3 18:03:22 2024 ] 
Training: Epoch [51/120], Step [2999], Loss: 1.0483856201171875, Training Accuracy: 89.8625
[ Wed Jul  3 18:03:23 2024 ] 	Batch(3000/7879) done. Loss: 0.0768  lr:0.010000
[ Wed Jul  3 18:03:40 2024 ] 	Batch(3100/7879) done. Loss: 0.3014  lr:0.010000
[ Wed Jul  3 18:03:58 2024 ] 	Batch(3200/7879) done. Loss: 0.9296  lr:0.010000
[ Wed Jul  3 18:04:16 2024 ] 	Batch(3300/7879) done. Loss: 0.5787  lr:0.010000
[ Wed Jul  3 18:04:34 2024 ] 	Batch(3400/7879) done. Loss: 0.4473  lr:0.010000
[ Wed Jul  3 18:04:52 2024 ] 
Training: Epoch [51/120], Step [3499], Loss: 0.2747114300727844, Training Accuracy: 89.79642857142856
[ Wed Jul  3 18:04:52 2024 ] 	Batch(3500/7879) done. Loss: 0.6802  lr:0.010000
[ Wed Jul  3 18:05:10 2024 ] 	Batch(3600/7879) done. Loss: 0.0378  lr:0.010000
[ Wed Jul  3 18:05:28 2024 ] 	Batch(3700/7879) done. Loss: 0.1039  lr:0.010000
[ Wed Jul  3 18:05:47 2024 ] 	Batch(3800/7879) done. Loss: 0.2814  lr:0.010000
[ Wed Jul  3 18:06:05 2024 ] 	Batch(3900/7879) done. Loss: 0.0385  lr:0.010000
[ Wed Jul  3 18:06:22 2024 ] 
Training: Epoch [51/120], Step [3999], Loss: 0.6387479901313782, Training Accuracy: 89.70937500000001
[ Wed Jul  3 18:06:23 2024 ] 	Batch(4000/7879) done. Loss: 0.2015  lr:0.010000
[ Wed Jul  3 18:06:41 2024 ] 	Batch(4100/7879) done. Loss: 0.0164  lr:0.010000
[ Wed Jul  3 18:06:59 2024 ] 	Batch(4200/7879) done. Loss: 0.7784  lr:0.010000
[ Wed Jul  3 18:07:17 2024 ] 	Batch(4300/7879) done. Loss: 0.1888  lr:0.010000
[ Wed Jul  3 18:07:35 2024 ] 	Batch(4400/7879) done. Loss: 0.0486  lr:0.010000
[ Wed Jul  3 18:07:53 2024 ] 
Training: Epoch [51/120], Step [4499], Loss: 0.7330934405326843, Training Accuracy: 89.75277777777778
[ Wed Jul  3 18:07:53 2024 ] 	Batch(4500/7879) done. Loss: 0.5420  lr:0.010000
[ Wed Jul  3 18:08:11 2024 ] 	Batch(4600/7879) done. Loss: 0.1541  lr:0.010000
[ Wed Jul  3 18:08:29 2024 ] 	Batch(4700/7879) done. Loss: 0.2560  lr:0.010000
[ Wed Jul  3 18:08:48 2024 ] 	Batch(4800/7879) done. Loss: 0.3752  lr:0.010000
[ Wed Jul  3 18:09:06 2024 ] 	Batch(4900/7879) done. Loss: 0.2475  lr:0.010000
[ Wed Jul  3 18:09:24 2024 ] 
Training: Epoch [51/120], Step [4999], Loss: 0.020205816254019737, Training Accuracy: 89.71000000000001
[ Wed Jul  3 18:09:24 2024 ] 	Batch(5000/7879) done. Loss: 0.0281  lr:0.010000
[ Wed Jul  3 18:09:42 2024 ] 	Batch(5100/7879) done. Loss: 0.5872  lr:0.010000
[ Wed Jul  3 18:10:00 2024 ] 	Batch(5200/7879) done. Loss: 0.1793  lr:0.010000
[ Wed Jul  3 18:10:19 2024 ] 	Batch(5300/7879) done. Loss: 0.5814  lr:0.010000
[ Wed Jul  3 18:10:37 2024 ] 	Batch(5400/7879) done. Loss: 0.6114  lr:0.010000
[ Wed Jul  3 18:10:56 2024 ] 
Training: Epoch [51/120], Step [5499], Loss: 0.3449936807155609, Training Accuracy: 89.64772727272728
[ Wed Jul  3 18:10:56 2024 ] 	Batch(5500/7879) done. Loss: 0.4583  lr:0.010000
[ Wed Jul  3 18:11:15 2024 ] 	Batch(5600/7879) done. Loss: 0.0354  lr:0.010000
[ Wed Jul  3 18:11:33 2024 ] 	Batch(5700/7879) done. Loss: 0.0476  lr:0.010000
[ Wed Jul  3 18:11:51 2024 ] 	Batch(5800/7879) done. Loss: 0.1301  lr:0.010000
[ Wed Jul  3 18:12:09 2024 ] 	Batch(5900/7879) done. Loss: 0.2043  lr:0.010000
[ Wed Jul  3 18:12:27 2024 ] 
Training: Epoch [51/120], Step [5999], Loss: 0.14657622575759888, Training Accuracy: 89.55416666666667
[ Wed Jul  3 18:12:27 2024 ] 	Batch(6000/7879) done. Loss: 0.4406  lr:0.010000
[ Wed Jul  3 18:12:45 2024 ] 	Batch(6100/7879) done. Loss: 0.8075  lr:0.010000
[ Wed Jul  3 18:13:03 2024 ] 	Batch(6200/7879) done. Loss: 0.0567  lr:0.010000
[ Wed Jul  3 18:13:21 2024 ] 	Batch(6300/7879) done. Loss: 0.9596  lr:0.010000
[ Wed Jul  3 18:13:39 2024 ] 	Batch(6400/7879) done. Loss: 0.1460  lr:0.010000
[ Wed Jul  3 18:13:57 2024 ] 
Training: Epoch [51/120], Step [6499], Loss: 0.02144560217857361, Training Accuracy: 89.52884615384616
[ Wed Jul  3 18:13:57 2024 ] 	Batch(6500/7879) done. Loss: 0.3487  lr:0.010000
[ Wed Jul  3 18:14:15 2024 ] 	Batch(6600/7879) done. Loss: 0.5304  lr:0.010000
[ Wed Jul  3 18:14:33 2024 ] 	Batch(6700/7879) done. Loss: 0.5128  lr:0.010000
[ Wed Jul  3 18:14:52 2024 ] 	Batch(6800/7879) done. Loss: 0.4729  lr:0.010000
[ Wed Jul  3 18:15:10 2024 ] 	Batch(6900/7879) done. Loss: 0.1303  lr:0.010000
[ Wed Jul  3 18:15:27 2024 ] 
Training: Epoch [51/120], Step [6999], Loss: 0.005177176091820002, Training Accuracy: 89.47678571428571
[ Wed Jul  3 18:15:27 2024 ] 	Batch(7000/7879) done. Loss: 0.1502  lr:0.010000
[ Wed Jul  3 18:15:45 2024 ] 	Batch(7100/7879) done. Loss: 0.3660  lr:0.010000
[ Wed Jul  3 18:16:03 2024 ] 	Batch(7200/7879) done. Loss: 0.4683  lr:0.010000
[ Wed Jul  3 18:16:22 2024 ] 	Batch(7300/7879) done. Loss: 0.2162  lr:0.010000
[ Wed Jul  3 18:16:40 2024 ] 	Batch(7400/7879) done. Loss: 0.0161  lr:0.010000
[ Wed Jul  3 18:16:59 2024 ] 
Training: Epoch [51/120], Step [7499], Loss: 0.17539581656455994, Training Accuracy: 89.47500000000001
[ Wed Jul  3 18:16:59 2024 ] 	Batch(7500/7879) done. Loss: 0.8851  lr:0.010000
[ Wed Jul  3 18:17:17 2024 ] 	Batch(7600/7879) done. Loss: 0.7533  lr:0.010000
[ Wed Jul  3 18:17:35 2024 ] 	Batch(7700/7879) done. Loss: 0.1871  lr:0.010000
[ Wed Jul  3 18:17:53 2024 ] 	Batch(7800/7879) done. Loss: 0.0510  lr:0.010000
[ Wed Jul  3 18:18:07 2024 ] 	Mean training loss: 0.3393.
[ Wed Jul  3 18:18:07 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 18:18:07 2024 ] Training epoch: 53
[ Wed Jul  3 18:18:08 2024 ] 	Batch(0/7879) done. Loss: 0.0478  lr:0.010000
[ Wed Jul  3 18:18:26 2024 ] 	Batch(100/7879) done. Loss: 0.1404  lr:0.010000
[ Wed Jul  3 18:18:45 2024 ] 	Batch(200/7879) done. Loss: 0.0214  lr:0.010000
[ Wed Jul  3 18:19:03 2024 ] 	Batch(300/7879) done. Loss: 0.2333  lr:0.010000
[ Wed Jul  3 18:19:22 2024 ] 	Batch(400/7879) done. Loss: 0.1769  lr:0.010000
[ Wed Jul  3 18:19:40 2024 ] 
Training: Epoch [52/120], Step [499], Loss: 0.14177021384239197, Training Accuracy: 89.95
[ Wed Jul  3 18:19:40 2024 ] 	Batch(500/7879) done. Loss: 0.1151  lr:0.010000
[ Wed Jul  3 18:19:59 2024 ] 	Batch(600/7879) done. Loss: 0.3549  lr:0.010000
[ Wed Jul  3 18:20:17 2024 ] 	Batch(700/7879) done. Loss: 0.4268  lr:0.010000
[ Wed Jul  3 18:20:36 2024 ] 	Batch(800/7879) done. Loss: 0.3028  lr:0.010000
[ Wed Jul  3 18:20:54 2024 ] 	Batch(900/7879) done. Loss: 0.0927  lr:0.010000
[ Wed Jul  3 18:21:12 2024 ] 
Training: Epoch [52/120], Step [999], Loss: 0.5867589116096497, Training Accuracy: 90.4875
[ Wed Jul  3 18:21:12 2024 ] 	Batch(1000/7879) done. Loss: 0.0484  lr:0.010000
[ Wed Jul  3 18:21:30 2024 ] 	Batch(1100/7879) done. Loss: 0.3541  lr:0.010000
[ Wed Jul  3 18:21:48 2024 ] 	Batch(1200/7879) done. Loss: 1.3997  lr:0.010000
[ Wed Jul  3 18:22:06 2024 ] 	Batch(1300/7879) done. Loss: 0.7844  lr:0.010000
[ Wed Jul  3 18:22:24 2024 ] 	Batch(1400/7879) done. Loss: 0.2420  lr:0.010000
[ Wed Jul  3 18:22:42 2024 ] 
Training: Epoch [52/120], Step [1499], Loss: 0.09507633745670319, Training Accuracy: 90.15833333333333
[ Wed Jul  3 18:22:42 2024 ] 	Batch(1500/7879) done. Loss: 0.1905  lr:0.010000
[ Wed Jul  3 18:23:00 2024 ] 	Batch(1600/7879) done. Loss: 0.0957  lr:0.010000
[ Wed Jul  3 18:23:18 2024 ] 	Batch(1700/7879) done. Loss: 0.2659  lr:0.010000
[ Wed Jul  3 18:23:36 2024 ] 	Batch(1800/7879) done. Loss: 0.1591  lr:0.010000
[ Wed Jul  3 18:23:54 2024 ] 	Batch(1900/7879) done. Loss: 0.3895  lr:0.010000
[ Wed Jul  3 18:24:11 2024 ] 
Training: Epoch [52/120], Step [1999], Loss: 0.12972234189510345, Training Accuracy: 90.15625
[ Wed Jul  3 18:24:12 2024 ] 	Batch(2000/7879) done. Loss: 0.2521  lr:0.010000
[ Wed Jul  3 18:24:30 2024 ] 	Batch(2100/7879) done. Loss: 0.0471  lr:0.010000
[ Wed Jul  3 18:24:47 2024 ] 	Batch(2200/7879) done. Loss: 0.7403  lr:0.010000
[ Wed Jul  3 18:25:05 2024 ] 	Batch(2300/7879) done. Loss: 0.7407  lr:0.010000
[ Wed Jul  3 18:25:23 2024 ] 	Batch(2400/7879) done. Loss: 0.0730  lr:0.010000
[ Wed Jul  3 18:25:41 2024 ] 
Training: Epoch [52/120], Step [2499], Loss: 0.5623674988746643, Training Accuracy: 90.025
[ Wed Jul  3 18:25:41 2024 ] 	Batch(2500/7879) done. Loss: 0.2965  lr:0.010000
[ Wed Jul  3 18:25:59 2024 ] 	Batch(2600/7879) done. Loss: 0.0071  lr:0.010000
[ Wed Jul  3 18:26:17 2024 ] 	Batch(2700/7879) done. Loss: 0.2204  lr:0.010000
[ Wed Jul  3 18:26:35 2024 ] 	Batch(2800/7879) done. Loss: 0.0608  lr:0.010000
[ Wed Jul  3 18:26:53 2024 ] 	Batch(2900/7879) done. Loss: 0.3431  lr:0.010000
[ Wed Jul  3 18:27:11 2024 ] 
Training: Epoch [52/120], Step [2999], Loss: 0.09061774611473083, Training Accuracy: 90.02916666666667
[ Wed Jul  3 18:27:11 2024 ] 	Batch(3000/7879) done. Loss: 0.1376  lr:0.010000
[ Wed Jul  3 18:27:29 2024 ] 	Batch(3100/7879) done. Loss: 0.0948  lr:0.010000
[ Wed Jul  3 18:27:47 2024 ] 	Batch(3200/7879) done. Loss: 0.1348  lr:0.010000
[ Wed Jul  3 18:28:05 2024 ] 	Batch(3300/7879) done. Loss: 0.0400  lr:0.010000
[ Wed Jul  3 18:28:24 2024 ] 	Batch(3400/7879) done. Loss: 0.5775  lr:0.010000
[ Wed Jul  3 18:28:42 2024 ] 
Training: Epoch [52/120], Step [3499], Loss: 0.12606944143772125, Training Accuracy: 89.97857142857143
[ Wed Jul  3 18:28:42 2024 ] 	Batch(3500/7879) done. Loss: 0.2039  lr:0.010000
[ Wed Jul  3 18:29:00 2024 ] 	Batch(3600/7879) done. Loss: 0.4985  lr:0.010000
[ Wed Jul  3 18:29:18 2024 ] 	Batch(3700/7879) done. Loss: 0.2087  lr:0.010000
[ Wed Jul  3 18:29:36 2024 ] 	Batch(3800/7879) done. Loss: 1.7938  lr:0.010000
[ Wed Jul  3 18:29:54 2024 ] 	Batch(3900/7879) done. Loss: 0.1581  lr:0.010000
[ Wed Jul  3 18:30:11 2024 ] 
Training: Epoch [52/120], Step [3999], Loss: 0.3368081748485565, Training Accuracy: 89.90625
[ Wed Jul  3 18:30:12 2024 ] 	Batch(4000/7879) done. Loss: 0.9674  lr:0.010000
[ Wed Jul  3 18:30:30 2024 ] 	Batch(4100/7879) done. Loss: 0.1906  lr:0.010000
[ Wed Jul  3 18:30:47 2024 ] 	Batch(4200/7879) done. Loss: 0.1431  lr:0.010000
[ Wed Jul  3 18:31:05 2024 ] 	Batch(4300/7879) done. Loss: 0.2503  lr:0.010000
[ Wed Jul  3 18:31:23 2024 ] 	Batch(4400/7879) done. Loss: 0.8737  lr:0.010000
[ Wed Jul  3 18:31:41 2024 ] 
Training: Epoch [52/120], Step [4499], Loss: 0.4889461100101471, Training Accuracy: 89.68055555555556
[ Wed Jul  3 18:31:41 2024 ] 	Batch(4500/7879) done. Loss: 0.6174  lr:0.010000
[ Wed Jul  3 18:31:59 2024 ] 	Batch(4600/7879) done. Loss: 0.6819  lr:0.010000
[ Wed Jul  3 18:32:17 2024 ] 	Batch(4700/7879) done. Loss: 0.1165  lr:0.010000
[ Wed Jul  3 18:32:35 2024 ] 	Batch(4800/7879) done. Loss: 0.0105  lr:0.010000
[ Wed Jul  3 18:32:53 2024 ] 	Batch(4900/7879) done. Loss: 0.0927  lr:0.010000
[ Wed Jul  3 18:33:10 2024 ] 
Training: Epoch [52/120], Step [4999], Loss: 0.5183568000793457, Training Accuracy: 89.655
[ Wed Jul  3 18:33:11 2024 ] 	Batch(5000/7879) done. Loss: 0.1031  lr:0.010000
[ Wed Jul  3 18:33:28 2024 ] 	Batch(5100/7879) done. Loss: 0.0806  lr:0.010000
[ Wed Jul  3 18:33:46 2024 ] 	Batch(5200/7879) done. Loss: 0.0328  lr:0.010000
[ Wed Jul  3 18:34:04 2024 ] 	Batch(5300/7879) done. Loss: 0.8508  lr:0.010000
[ Wed Jul  3 18:34:22 2024 ] 	Batch(5400/7879) done. Loss: 0.1736  lr:0.010000
[ Wed Jul  3 18:34:40 2024 ] 
Training: Epoch [52/120], Step [5499], Loss: 0.15913380682468414, Training Accuracy: 89.58636363636364
[ Wed Jul  3 18:34:40 2024 ] 	Batch(5500/7879) done. Loss: 0.4262  lr:0.010000
[ Wed Jul  3 18:34:58 2024 ] 	Batch(5600/7879) done. Loss: 0.3231  lr:0.010000
[ Wed Jul  3 18:35:16 2024 ] 	Batch(5700/7879) done. Loss: 0.0270  lr:0.010000
[ Wed Jul  3 18:35:34 2024 ] 	Batch(5800/7879) done. Loss: 0.1814  lr:0.010000
[ Wed Jul  3 18:35:52 2024 ] 	Batch(5900/7879) done. Loss: 0.3374  lr:0.010000
[ Wed Jul  3 18:36:10 2024 ] 
Training: Epoch [52/120], Step [5999], Loss: 0.3756428062915802, Training Accuracy: 89.55625
[ Wed Jul  3 18:36:10 2024 ] 	Batch(6000/7879) done. Loss: 0.0355  lr:0.010000
[ Wed Jul  3 18:36:28 2024 ] 	Batch(6100/7879) done. Loss: 0.0964  lr:0.010000
[ Wed Jul  3 18:36:46 2024 ] 	Batch(6200/7879) done. Loss: 1.0524  lr:0.010000
[ Wed Jul  3 18:37:04 2024 ] 	Batch(6300/7879) done. Loss: 0.7555  lr:0.010000
[ Wed Jul  3 18:37:22 2024 ] 	Batch(6400/7879) done. Loss: 0.2670  lr:0.010000
[ Wed Jul  3 18:37:39 2024 ] 
Training: Epoch [52/120], Step [6499], Loss: 0.09343142807483673, Training Accuracy: 89.45384615384614
[ Wed Jul  3 18:37:40 2024 ] 	Batch(6500/7879) done. Loss: 0.4440  lr:0.010000
[ Wed Jul  3 18:37:57 2024 ] 	Batch(6600/7879) done. Loss: 0.8618  lr:0.010000
[ Wed Jul  3 18:38:15 2024 ] 	Batch(6700/7879) done. Loss: 0.5539  lr:0.010000
[ Wed Jul  3 18:38:33 2024 ] 	Batch(6800/7879) done. Loss: 0.0810  lr:0.010000
[ Wed Jul  3 18:38:51 2024 ] 	Batch(6900/7879) done. Loss: 0.8864  lr:0.010000
[ Wed Jul  3 18:39:09 2024 ] 
Training: Epoch [52/120], Step [6999], Loss: 0.1841520518064499, Training Accuracy: 89.32678571428572
[ Wed Jul  3 18:39:09 2024 ] 	Batch(7000/7879) done. Loss: 0.0371  lr:0.010000
[ Wed Jul  3 18:39:27 2024 ] 	Batch(7100/7879) done. Loss: 0.6515  lr:0.010000
[ Wed Jul  3 18:39:45 2024 ] 	Batch(7200/7879) done. Loss: 0.2469  lr:0.010000
[ Wed Jul  3 18:40:03 2024 ] 	Batch(7300/7879) done. Loss: 0.4231  lr:0.010000
[ Wed Jul  3 18:40:20 2024 ] 	Batch(7400/7879) done. Loss: 0.5046  lr:0.010000
[ Wed Jul  3 18:40:38 2024 ] 
Training: Epoch [52/120], Step [7499], Loss: 0.15954722464084625, Training Accuracy: 89.265
[ Wed Jul  3 18:40:38 2024 ] 	Batch(7500/7879) done. Loss: 0.2876  lr:0.010000
[ Wed Jul  3 18:40:56 2024 ] 	Batch(7600/7879) done. Loss: 0.0090  lr:0.010000
[ Wed Jul  3 18:41:15 2024 ] 	Batch(7700/7879) done. Loss: 0.3365  lr:0.010000
[ Wed Jul  3 18:41:33 2024 ] 	Batch(7800/7879) done. Loss: 0.3200  lr:0.010000
[ Wed Jul  3 18:41:48 2024 ] 	Mean training loss: 0.3446.
[ Wed Jul  3 18:41:48 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 18:41:48 2024 ] Training epoch: 54
[ Wed Jul  3 18:41:48 2024 ] 	Batch(0/7879) done. Loss: 0.1749  lr:0.010000
[ Wed Jul  3 18:42:06 2024 ] 	Batch(100/7879) done. Loss: 0.0611  lr:0.010000
[ Wed Jul  3 18:42:24 2024 ] 	Batch(200/7879) done. Loss: 0.4255  lr:0.010000
[ Wed Jul  3 18:42:42 2024 ] 	Batch(300/7879) done. Loss: 0.3113  lr:0.010000
[ Wed Jul  3 18:43:00 2024 ] 	Batch(400/7879) done. Loss: 0.1596  lr:0.010000
[ Wed Jul  3 18:43:18 2024 ] 
Training: Epoch [53/120], Step [499], Loss: 0.11667612940073013, Training Accuracy: 89.75
[ Wed Jul  3 18:43:18 2024 ] 	Batch(500/7879) done. Loss: 0.5370  lr:0.010000
[ Wed Jul  3 18:43:36 2024 ] 	Batch(600/7879) done. Loss: 0.5044  lr:0.010000
[ Wed Jul  3 18:43:54 2024 ] 	Batch(700/7879) done. Loss: 0.1957  lr:0.010000
[ Wed Jul  3 18:44:12 2024 ] 	Batch(800/7879) done. Loss: 0.2879  lr:0.010000
[ Wed Jul  3 18:44:30 2024 ] 	Batch(900/7879) done. Loss: 0.1387  lr:0.010000
[ Wed Jul  3 18:44:47 2024 ] 
Training: Epoch [53/120], Step [999], Loss: 0.05151890963315964, Training Accuracy: 90.4875
[ Wed Jul  3 18:44:47 2024 ] 	Batch(1000/7879) done. Loss: 0.7873  lr:0.010000
[ Wed Jul  3 18:45:05 2024 ] 	Batch(1100/7879) done. Loss: 0.2468  lr:0.010000
[ Wed Jul  3 18:45:23 2024 ] 	Batch(1200/7879) done. Loss: 0.2241  lr:0.010000
[ Wed Jul  3 18:45:41 2024 ] 	Batch(1300/7879) done. Loss: 0.0213  lr:0.010000
[ Wed Jul  3 18:45:59 2024 ] 	Batch(1400/7879) done. Loss: 0.6187  lr:0.010000
[ Wed Jul  3 18:46:17 2024 ] 
Training: Epoch [53/120], Step [1499], Loss: 0.4358619153499603, Training Accuracy: 90.43333333333334
[ Wed Jul  3 18:46:17 2024 ] 	Batch(1500/7879) done. Loss: 0.1051  lr:0.010000
[ Wed Jul  3 18:46:35 2024 ] 	Batch(1600/7879) done. Loss: 0.0619  lr:0.010000
[ Wed Jul  3 18:46:53 2024 ] 	Batch(1700/7879) done. Loss: 0.2231  lr:0.010000
[ Wed Jul  3 18:47:11 2024 ] 	Batch(1800/7879) done. Loss: 0.4925  lr:0.010000
[ Wed Jul  3 18:47:29 2024 ] 	Batch(1900/7879) done. Loss: 0.2243  lr:0.010000
[ Wed Jul  3 18:47:46 2024 ] 
Training: Epoch [53/120], Step [1999], Loss: 0.1834678202867508, Training Accuracy: 90.46875
[ Wed Jul  3 18:47:47 2024 ] 	Batch(2000/7879) done. Loss: 0.3746  lr:0.010000
[ Wed Jul  3 18:48:05 2024 ] 	Batch(2100/7879) done. Loss: 1.2670  lr:0.010000
[ Wed Jul  3 18:48:24 2024 ] 	Batch(2200/7879) done. Loss: 0.0609  lr:0.010000
[ Wed Jul  3 18:48:42 2024 ] 	Batch(2300/7879) done. Loss: 0.1152  lr:0.010000
[ Wed Jul  3 18:49:01 2024 ] 	Batch(2400/7879) done. Loss: 0.3892  lr:0.010000
[ Wed Jul  3 18:49:19 2024 ] 
Training: Epoch [53/120], Step [2499], Loss: 0.5357672572135925, Training Accuracy: 90.335
[ Wed Jul  3 18:49:19 2024 ] 	Batch(2500/7879) done. Loss: 0.0990  lr:0.010000
[ Wed Jul  3 18:49:37 2024 ] 	Batch(2600/7879) done. Loss: 0.3951  lr:0.010000
[ Wed Jul  3 18:49:55 2024 ] 	Batch(2700/7879) done. Loss: 0.3583  lr:0.010000
[ Wed Jul  3 18:50:13 2024 ] 	Batch(2800/7879) done. Loss: 0.0494  lr:0.010000
[ Wed Jul  3 18:50:30 2024 ] 	Batch(2900/7879) done. Loss: 0.1191  lr:0.010000
[ Wed Jul  3 18:50:48 2024 ] 
Training: Epoch [53/120], Step [2999], Loss: 0.08589665591716766, Training Accuracy: 90.20416666666667
[ Wed Jul  3 18:50:48 2024 ] 	Batch(3000/7879) done. Loss: 0.0325  lr:0.010000
[ Wed Jul  3 18:51:06 2024 ] 	Batch(3100/7879) done. Loss: 0.3014  lr:0.010000
[ Wed Jul  3 18:51:24 2024 ] 	Batch(3200/7879) done. Loss: 0.5524  lr:0.010000
[ Wed Jul  3 18:51:42 2024 ] 	Batch(3300/7879) done. Loss: 0.7172  lr:0.010000
[ Wed Jul  3 18:52:00 2024 ] 	Batch(3400/7879) done. Loss: 0.1330  lr:0.010000
[ Wed Jul  3 18:52:18 2024 ] 
Training: Epoch [53/120], Step [3499], Loss: 0.16425952315330505, Training Accuracy: 90.19285714285714
[ Wed Jul  3 18:52:18 2024 ] 	Batch(3500/7879) done. Loss: 0.8409  lr:0.010000
[ Wed Jul  3 18:52:36 2024 ] 	Batch(3600/7879) done. Loss: 0.3373  lr:0.010000
[ Wed Jul  3 18:52:54 2024 ] 	Batch(3700/7879) done. Loss: 0.3290  lr:0.010000
[ Wed Jul  3 18:53:12 2024 ] 	Batch(3800/7879) done. Loss: 0.0062  lr:0.010000
[ Wed Jul  3 18:53:30 2024 ] 	Batch(3900/7879) done. Loss: 0.1402  lr:0.010000
[ Wed Jul  3 18:53:47 2024 ] 
Training: Epoch [53/120], Step [3999], Loss: 0.1292627900838852, Training Accuracy: 90.1625
[ Wed Jul  3 18:53:48 2024 ] 	Batch(4000/7879) done. Loss: 0.4875  lr:0.010000
[ Wed Jul  3 18:54:06 2024 ] 	Batch(4100/7879) done. Loss: 0.6897  lr:0.010000
[ Wed Jul  3 18:54:23 2024 ] 	Batch(4200/7879) done. Loss: 0.1012  lr:0.010000
[ Wed Jul  3 18:54:41 2024 ] 	Batch(4300/7879) done. Loss: 1.0021  lr:0.010000
[ Wed Jul  3 18:54:59 2024 ] 	Batch(4400/7879) done. Loss: 0.2542  lr:0.010000
[ Wed Jul  3 18:55:18 2024 ] 
Training: Epoch [53/120], Step [4499], Loss: 0.46418190002441406, Training Accuracy: 90.09722222222221
[ Wed Jul  3 18:55:18 2024 ] 	Batch(4500/7879) done. Loss: 0.0383  lr:0.010000
[ Wed Jul  3 18:55:36 2024 ] 	Batch(4600/7879) done. Loss: 0.7601  lr:0.010000
[ Wed Jul  3 18:55:55 2024 ] 	Batch(4700/7879) done. Loss: 0.0953  lr:0.010000
[ Wed Jul  3 18:56:14 2024 ] 	Batch(4800/7879) done. Loss: 0.2033  lr:0.010000
[ Wed Jul  3 18:56:32 2024 ] 	Batch(4900/7879) done. Loss: 0.1991  lr:0.010000
[ Wed Jul  3 18:56:50 2024 ] 
Training: Epoch [53/120], Step [4999], Loss: 0.29141682386398315, Training Accuracy: 90.01
[ Wed Jul  3 18:56:51 2024 ] 	Batch(5000/7879) done. Loss: 0.4624  lr:0.010000
[ Wed Jul  3 18:57:09 2024 ] 	Batch(5100/7879) done. Loss: 1.0307  lr:0.010000
[ Wed Jul  3 18:57:28 2024 ] 	Batch(5200/7879) done. Loss: 0.0217  lr:0.010000
[ Wed Jul  3 18:57:46 2024 ] 	Batch(5300/7879) done. Loss: 0.1988  lr:0.010000
[ Wed Jul  3 18:58:04 2024 ] 	Batch(5400/7879) done. Loss: 0.6325  lr:0.010000
[ Wed Jul  3 18:58:21 2024 ] 
Training: Epoch [53/120], Step [5499], Loss: 0.24988462030887604, Training Accuracy: 89.86363636363637
[ Wed Jul  3 18:58:21 2024 ] 	Batch(5500/7879) done. Loss: 0.3457  lr:0.010000
[ Wed Jul  3 18:58:39 2024 ] 	Batch(5600/7879) done. Loss: 0.0974  lr:0.010000
[ Wed Jul  3 18:58:57 2024 ] 	Batch(5700/7879) done. Loss: 0.1199  lr:0.010000
[ Wed Jul  3 18:59:15 2024 ] 	Batch(5800/7879) done. Loss: 0.1879  lr:0.010000
[ Wed Jul  3 18:59:33 2024 ] 	Batch(5900/7879) done. Loss: 0.3809  lr:0.010000
[ Wed Jul  3 18:59:51 2024 ] 
Training: Epoch [53/120], Step [5999], Loss: 0.22510530054569244, Training Accuracy: 89.83125
[ Wed Jul  3 18:59:51 2024 ] 	Batch(6000/7879) done. Loss: 0.1079  lr:0.010000
[ Wed Jul  3 19:00:09 2024 ] 	Batch(6100/7879) done. Loss: 0.2183  lr:0.010000
[ Wed Jul  3 19:00:27 2024 ] 	Batch(6200/7879) done. Loss: 0.2681  lr:0.010000
[ Wed Jul  3 19:00:45 2024 ] 	Batch(6300/7879) done. Loss: 0.1043  lr:0.010000
[ Wed Jul  3 19:01:03 2024 ] 	Batch(6400/7879) done. Loss: 0.2003  lr:0.010000
[ Wed Jul  3 19:01:21 2024 ] 
Training: Epoch [53/120], Step [6499], Loss: 0.12061116099357605, Training Accuracy: 89.73461538461538
[ Wed Jul  3 19:01:21 2024 ] 	Batch(6500/7879) done. Loss: 0.4856  lr:0.010000
[ Wed Jul  3 19:01:39 2024 ] 	Batch(6600/7879) done. Loss: 0.5190  lr:0.010000
[ Wed Jul  3 19:01:57 2024 ] 	Batch(6700/7879) done. Loss: 0.1328  lr:0.010000
[ Wed Jul  3 19:02:15 2024 ] 	Batch(6800/7879) done. Loss: 0.6963  lr:0.010000
[ Wed Jul  3 19:02:33 2024 ] 	Batch(6900/7879) done. Loss: 0.1626  lr:0.010000
[ Wed Jul  3 19:02:50 2024 ] 
Training: Epoch [53/120], Step [6999], Loss: 0.10492610931396484, Training Accuracy: 89.72321428571428
[ Wed Jul  3 19:02:51 2024 ] 	Batch(7000/7879) done. Loss: 0.1722  lr:0.010000
[ Wed Jul  3 19:03:08 2024 ] 	Batch(7100/7879) done. Loss: 0.4004  lr:0.010000
[ Wed Jul  3 19:03:26 2024 ] 	Batch(7200/7879) done. Loss: 0.0795  lr:0.010000
[ Wed Jul  3 19:03:44 2024 ] 	Batch(7300/7879) done. Loss: 0.6519  lr:0.010000
[ Wed Jul  3 19:04:02 2024 ] 	Batch(7400/7879) done. Loss: 0.0293  lr:0.010000
[ Wed Jul  3 19:04:20 2024 ] 
Training: Epoch [53/120], Step [7499], Loss: 0.2457927167415619, Training Accuracy: 89.655
[ Wed Jul  3 19:04:20 2024 ] 	Batch(7500/7879) done. Loss: 0.3737  lr:0.010000
[ Wed Jul  3 19:04:38 2024 ] 	Batch(7600/7879) done. Loss: 0.0902  lr:0.010000
[ Wed Jul  3 19:04:56 2024 ] 	Batch(7700/7879) done. Loss: 0.0692  lr:0.010000
[ Wed Jul  3 19:05:14 2024 ] 	Batch(7800/7879) done. Loss: 0.0197  lr:0.010000
[ Wed Jul  3 19:05:28 2024 ] 	Mean training loss: 0.3316.
[ Wed Jul  3 19:05:28 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 19:05:28 2024 ] Training epoch: 55
[ Wed Jul  3 19:05:29 2024 ] 	Batch(0/7879) done. Loss: 0.0723  lr:0.010000
[ Wed Jul  3 19:05:46 2024 ] 	Batch(100/7879) done. Loss: 0.2390  lr:0.010000
[ Wed Jul  3 19:06:04 2024 ] 	Batch(200/7879) done. Loss: 0.0483  lr:0.010000
[ Wed Jul  3 19:06:22 2024 ] 	Batch(300/7879) done. Loss: 0.2887  lr:0.010000
[ Wed Jul  3 19:06:40 2024 ] 	Batch(400/7879) done. Loss: 0.3156  lr:0.010000
[ Wed Jul  3 19:06:58 2024 ] 
Training: Epoch [54/120], Step [499], Loss: 0.06099212169647217, Training Accuracy: 91.07499999999999
[ Wed Jul  3 19:06:58 2024 ] 	Batch(500/7879) done. Loss: 0.3891  lr:0.010000
[ Wed Jul  3 19:07:16 2024 ] 	Batch(600/7879) done. Loss: 0.5902  lr:0.010000
[ Wed Jul  3 19:07:34 2024 ] 	Batch(700/7879) done. Loss: 0.3566  lr:0.010000
[ Wed Jul  3 19:07:52 2024 ] 	Batch(800/7879) done. Loss: 0.2717  lr:0.010000
[ Wed Jul  3 19:08:10 2024 ] 	Batch(900/7879) done. Loss: 0.9775  lr:0.010000
[ Wed Jul  3 19:08:28 2024 ] 
Training: Epoch [54/120], Step [999], Loss: 0.5949586629867554, Training Accuracy: 90.575
[ Wed Jul  3 19:08:28 2024 ] 	Batch(1000/7879) done. Loss: 0.2651  lr:0.010000
[ Wed Jul  3 19:08:46 2024 ] 	Batch(1100/7879) done. Loss: 0.5006  lr:0.010000
[ Wed Jul  3 19:09:04 2024 ] 	Batch(1200/7879) done. Loss: 0.1297  lr:0.010000
[ Wed Jul  3 19:09:22 2024 ] 	Batch(1300/7879) done. Loss: 0.9142  lr:0.010000
[ Wed Jul  3 19:09:41 2024 ] 	Batch(1400/7879) done. Loss: 0.5478  lr:0.010000
[ Wed Jul  3 19:09:59 2024 ] 
Training: Epoch [54/120], Step [1499], Loss: 0.03092108480632305, Training Accuracy: 90.9
[ Wed Jul  3 19:09:59 2024 ] 	Batch(1500/7879) done. Loss: 0.4807  lr:0.010000
[ Wed Jul  3 19:10:18 2024 ] 	Batch(1600/7879) done. Loss: 0.1256  lr:0.010000
[ Wed Jul  3 19:10:37 2024 ] 	Batch(1700/7879) done. Loss: 0.2634  lr:0.010000
[ Wed Jul  3 19:10:55 2024 ] 	Batch(1800/7879) done. Loss: 0.2703  lr:0.010000
[ Wed Jul  3 19:11:14 2024 ] 	Batch(1900/7879) done. Loss: 0.0419  lr:0.010000
[ Wed Jul  3 19:11:31 2024 ] 
Training: Epoch [54/120], Step [1999], Loss: 1.7353739738464355, Training Accuracy: 90.88125000000001
[ Wed Jul  3 19:11:32 2024 ] 	Batch(2000/7879) done. Loss: 0.8460  lr:0.010000
[ Wed Jul  3 19:11:49 2024 ] 	Batch(2100/7879) done. Loss: 0.0888  lr:0.010000
[ Wed Jul  3 19:12:07 2024 ] 	Batch(2200/7879) done. Loss: 0.0799  lr:0.010000
[ Wed Jul  3 19:12:25 2024 ] 	Batch(2300/7879) done. Loss: 0.2647  lr:0.010000
[ Wed Jul  3 19:12:43 2024 ] 	Batch(2400/7879) done. Loss: 1.1049  lr:0.010000
[ Wed Jul  3 19:13:01 2024 ] 
Training: Epoch [54/120], Step [2499], Loss: 0.07201280444860458, Training Accuracy: 90.805
[ Wed Jul  3 19:13:01 2024 ] 	Batch(2500/7879) done. Loss: 0.5174  lr:0.010000
[ Wed Jul  3 19:13:19 2024 ] 	Batch(2600/7879) done. Loss: 0.4855  lr:0.010000
[ Wed Jul  3 19:13:37 2024 ] 	Batch(2700/7879) done. Loss: 0.4709  lr:0.010000
[ Wed Jul  3 19:13:55 2024 ] 	Batch(2800/7879) done. Loss: 0.0418  lr:0.010000
[ Wed Jul  3 19:14:13 2024 ] 	Batch(2900/7879) done. Loss: 0.1409  lr:0.010000
[ Wed Jul  3 19:14:31 2024 ] 
Training: Epoch [54/120], Step [2999], Loss: 0.1603398323059082, Training Accuracy: 90.4875
[ Wed Jul  3 19:14:31 2024 ] 	Batch(3000/7879) done. Loss: 0.0156  lr:0.010000
[ Wed Jul  3 19:14:49 2024 ] 	Batch(3100/7879) done. Loss: 0.1859  lr:0.010000
[ Wed Jul  3 19:15:07 2024 ] 	Batch(3200/7879) done. Loss: 0.0490  lr:0.010000
[ Wed Jul  3 19:15:25 2024 ] 	Batch(3300/7879) done. Loss: 0.2832  lr:0.010000
[ Wed Jul  3 19:15:43 2024 ] 	Batch(3400/7879) done. Loss: 0.2022  lr:0.010000
[ Wed Jul  3 19:16:02 2024 ] 
Training: Epoch [54/120], Step [3499], Loss: 0.5416011214256287, Training Accuracy: 90.53571428571429
[ Wed Jul  3 19:16:02 2024 ] 	Batch(3500/7879) done. Loss: 0.2581  lr:0.010000
[ Wed Jul  3 19:16:21 2024 ] 	Batch(3600/7879) done. Loss: 0.0977  lr:0.010000
[ Wed Jul  3 19:16:39 2024 ] 	Batch(3700/7879) done. Loss: 0.9859  lr:0.010000
[ Wed Jul  3 19:16:57 2024 ] 	Batch(3800/7879) done. Loss: 0.5303  lr:0.010000
[ Wed Jul  3 19:17:16 2024 ] 	Batch(3900/7879) done. Loss: 0.0128  lr:0.010000
[ Wed Jul  3 19:17:34 2024 ] 
Training: Epoch [54/120], Step [3999], Loss: 0.4077044725418091, Training Accuracy: 90.41875
[ Wed Jul  3 19:17:35 2024 ] 	Batch(4000/7879) done. Loss: 0.0323  lr:0.010000
[ Wed Jul  3 19:17:53 2024 ] 	Batch(4100/7879) done. Loss: 0.3561  lr:0.010000
[ Wed Jul  3 19:18:12 2024 ] 	Batch(4200/7879) done. Loss: 0.1484  lr:0.010000
[ Wed Jul  3 19:18:30 2024 ] 	Batch(4300/7879) done. Loss: 0.3720  lr:0.010000
[ Wed Jul  3 19:18:48 2024 ] 	Batch(4400/7879) done. Loss: 0.4289  lr:0.010000
[ Wed Jul  3 19:19:06 2024 ] 
Training: Epoch [54/120], Step [4499], Loss: 0.14651335775852203, Training Accuracy: 90.30277777777778
[ Wed Jul  3 19:19:06 2024 ] 	Batch(4500/7879) done. Loss: 0.5982  lr:0.010000
[ Wed Jul  3 19:19:25 2024 ] 	Batch(4600/7879) done. Loss: 0.2284  lr:0.010000
[ Wed Jul  3 19:19:43 2024 ] 	Batch(4700/7879) done. Loss: 0.0349  lr:0.010000
[ Wed Jul  3 19:20:02 2024 ] 	Batch(4800/7879) done. Loss: 0.4622  lr:0.010000
[ Wed Jul  3 19:20:20 2024 ] 	Batch(4900/7879) done. Loss: 0.3193  lr:0.010000
[ Wed Jul  3 19:20:38 2024 ] 
Training: Epoch [54/120], Step [4999], Loss: 0.1413775235414505, Training Accuracy: 90.2675
[ Wed Jul  3 19:20:38 2024 ] 	Batch(5000/7879) done. Loss: 0.8155  lr:0.010000
[ Wed Jul  3 19:20:56 2024 ] 	Batch(5100/7879) done. Loss: 0.0463  lr:0.010000
[ Wed Jul  3 19:21:14 2024 ] 	Batch(5200/7879) done. Loss: 0.5572  lr:0.010000
[ Wed Jul  3 19:21:33 2024 ] 	Batch(5300/7879) done. Loss: 0.1979  lr:0.010000
[ Wed Jul  3 19:21:51 2024 ] 	Batch(5400/7879) done. Loss: 0.4229  lr:0.010000
[ Wed Jul  3 19:22:08 2024 ] 
Training: Epoch [54/120], Step [5499], Loss: 0.04295196011662483, Training Accuracy: 90.10681818181818
[ Wed Jul  3 19:22:09 2024 ] 	Batch(5500/7879) done. Loss: 0.0170  lr:0.010000
[ Wed Jul  3 19:22:27 2024 ] 	Batch(5600/7879) done. Loss: 0.1850  lr:0.010000
[ Wed Jul  3 19:22:45 2024 ] 	Batch(5700/7879) done. Loss: 0.2024  lr:0.010000
[ Wed Jul  3 19:23:04 2024 ] 	Batch(5800/7879) done. Loss: 0.2167  lr:0.010000
[ Wed Jul  3 19:23:22 2024 ] 	Batch(5900/7879) done. Loss: 0.1182  lr:0.010000
[ Wed Jul  3 19:23:40 2024 ] 
Training: Epoch [54/120], Step [5999], Loss: 0.29293519258499146, Training Accuracy: 89.90625
[ Wed Jul  3 19:23:41 2024 ] 	Batch(6000/7879) done. Loss: 0.1026  lr:0.010000
[ Wed Jul  3 19:23:59 2024 ] 	Batch(6100/7879) done. Loss: 0.0690  lr:0.010000
[ Wed Jul  3 19:24:17 2024 ] 	Batch(6200/7879) done. Loss: 0.3003  lr:0.010000
[ Wed Jul  3 19:24:35 2024 ] 	Batch(6300/7879) done. Loss: 0.0973  lr:0.010000
[ Wed Jul  3 19:24:53 2024 ] 	Batch(6400/7879) done. Loss: 0.6029  lr:0.010000
[ Wed Jul  3 19:25:11 2024 ] 
Training: Epoch [54/120], Step [6499], Loss: 0.27018943428993225, Training Accuracy: 89.83461538461538
[ Wed Jul  3 19:25:11 2024 ] 	Batch(6500/7879) done. Loss: 0.0236  lr:0.010000
[ Wed Jul  3 19:25:29 2024 ] 	Batch(6600/7879) done. Loss: 1.0790  lr:0.010000
[ Wed Jul  3 19:25:46 2024 ] 	Batch(6700/7879) done. Loss: 0.4582  lr:0.010000
[ Wed Jul  3 19:26:04 2024 ] 	Batch(6800/7879) done. Loss: 0.4625  lr:0.010000
[ Wed Jul  3 19:26:22 2024 ] 	Batch(6900/7879) done. Loss: 0.1390  lr:0.010000
[ Wed Jul  3 19:26:40 2024 ] 
Training: Epoch [54/120], Step [6999], Loss: 0.21684367954730988, Training Accuracy: 89.84285714285714
[ Wed Jul  3 19:26:40 2024 ] 	Batch(7000/7879) done. Loss: 0.2211  lr:0.010000
[ Wed Jul  3 19:26:58 2024 ] 	Batch(7100/7879) done. Loss: 0.3678  lr:0.010000
[ Wed Jul  3 19:27:16 2024 ] 	Batch(7200/7879) done. Loss: 0.0356  lr:0.010000
[ Wed Jul  3 19:27:34 2024 ] 	Batch(7300/7879) done. Loss: 0.0722  lr:0.010000
[ Wed Jul  3 19:27:52 2024 ] 	Batch(7400/7879) done. Loss: 0.1406  lr:0.010000
[ Wed Jul  3 19:28:10 2024 ] 
Training: Epoch [54/120], Step [7499], Loss: 0.07243779301643372, Training Accuracy: 89.81166666666667
[ Wed Jul  3 19:28:10 2024 ] 	Batch(7500/7879) done. Loss: 0.2821  lr:0.010000
[ Wed Jul  3 19:28:28 2024 ] 	Batch(7600/7879) done. Loss: 0.2352  lr:0.010000
[ Wed Jul  3 19:28:46 2024 ] 	Batch(7700/7879) done. Loss: 0.6858  lr:0.010000
[ Wed Jul  3 19:29:04 2024 ] 	Batch(7800/7879) done. Loss: 0.2622  lr:0.010000
[ Wed Jul  3 19:29:18 2024 ] 	Mean training loss: 0.3299.
[ Wed Jul  3 19:29:18 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 19:29:18 2024 ] Training epoch: 56
[ Wed Jul  3 19:29:18 2024 ] 	Batch(0/7879) done. Loss: 0.1754  lr:0.010000
[ Wed Jul  3 19:29:37 2024 ] 	Batch(100/7879) done. Loss: 0.1297  lr:0.010000
[ Wed Jul  3 19:29:55 2024 ] 	Batch(200/7879) done. Loss: 0.3852  lr:0.010000
[ Wed Jul  3 19:30:14 2024 ] 	Batch(300/7879) done. Loss: 0.2726  lr:0.010000
[ Wed Jul  3 19:30:33 2024 ] 	Batch(400/7879) done. Loss: 0.0205  lr:0.010000
[ Wed Jul  3 19:30:51 2024 ] 
Training: Epoch [55/120], Step [499], Loss: 0.7815577387809753, Training Accuracy: 90.25
[ Wed Jul  3 19:30:51 2024 ] 	Batch(500/7879) done. Loss: 0.1258  lr:0.010000
[ Wed Jul  3 19:31:10 2024 ] 	Batch(600/7879) done. Loss: 0.0869  lr:0.010000
[ Wed Jul  3 19:31:28 2024 ] 	Batch(700/7879) done. Loss: 0.7207  lr:0.010000
[ Wed Jul  3 19:31:47 2024 ] 	Batch(800/7879) done. Loss: 0.9246  lr:0.010000
[ Wed Jul  3 19:32:05 2024 ] 	Batch(900/7879) done. Loss: 0.0911  lr:0.010000
[ Wed Jul  3 19:32:24 2024 ] 
Training: Epoch [55/120], Step [999], Loss: 0.091731958091259, Training Accuracy: 90.1375
[ Wed Jul  3 19:32:24 2024 ] 	Batch(1000/7879) done. Loss: 0.2457  lr:0.010000
[ Wed Jul  3 19:32:42 2024 ] 	Batch(1100/7879) done. Loss: 0.0727  lr:0.010000
[ Wed Jul  3 19:33:01 2024 ] 	Batch(1200/7879) done. Loss: 0.3160  lr:0.010000
[ Wed Jul  3 19:33:19 2024 ] 	Batch(1300/7879) done. Loss: 0.7598  lr:0.010000
[ Wed Jul  3 19:33:38 2024 ] 	Batch(1400/7879) done. Loss: 0.4243  lr:0.010000
[ Wed Jul  3 19:33:56 2024 ] 
Training: Epoch [55/120], Step [1499], Loss: 0.3888399600982666, Training Accuracy: 90.21666666666667
[ Wed Jul  3 19:33:56 2024 ] 	Batch(1500/7879) done. Loss: 1.5775  lr:0.010000
[ Wed Jul  3 19:34:15 2024 ] 	Batch(1600/7879) done. Loss: 0.5940  lr:0.010000
[ Wed Jul  3 19:34:34 2024 ] 	Batch(1700/7879) done. Loss: 0.2474  lr:0.010000
[ Wed Jul  3 19:34:51 2024 ] 	Batch(1800/7879) done. Loss: 0.3651  lr:0.010000
[ Wed Jul  3 19:35:09 2024 ] 	Batch(1900/7879) done. Loss: 0.0536  lr:0.010000
[ Wed Jul  3 19:35:27 2024 ] 
Training: Epoch [55/120], Step [1999], Loss: 0.1711738407611847, Training Accuracy: 90.14999999999999
[ Wed Jul  3 19:35:28 2024 ] 	Batch(2000/7879) done. Loss: 0.2736  lr:0.010000
[ Wed Jul  3 19:35:46 2024 ] 	Batch(2100/7879) done. Loss: 0.1956  lr:0.010000
[ Wed Jul  3 19:36:03 2024 ] 	Batch(2200/7879) done. Loss: 0.2776  lr:0.010000
[ Wed Jul  3 19:36:21 2024 ] 	Batch(2300/7879) done. Loss: 0.3978  lr:0.010000
[ Wed Jul  3 19:36:39 2024 ] 	Batch(2400/7879) done. Loss: 0.9118  lr:0.010000
[ Wed Jul  3 19:36:57 2024 ] 
Training: Epoch [55/120], Step [2499], Loss: 0.19040189683437347, Training Accuracy: 90.03
[ Wed Jul  3 19:36:57 2024 ] 	Batch(2500/7879) done. Loss: 0.7731  lr:0.010000
[ Wed Jul  3 19:37:15 2024 ] 	Batch(2600/7879) done. Loss: 0.2033  lr:0.010000
[ Wed Jul  3 19:37:33 2024 ] 	Batch(2700/7879) done. Loss: 0.2269  lr:0.010000
[ Wed Jul  3 19:37:51 2024 ] 	Batch(2800/7879) done. Loss: 0.3064  lr:0.010000
[ Wed Jul  3 19:38:09 2024 ] 	Batch(2900/7879) done. Loss: 0.0640  lr:0.010000
[ Wed Jul  3 19:38:26 2024 ] 
Training: Epoch [55/120], Step [2999], Loss: 0.4664301872253418, Training Accuracy: 89.95
[ Wed Jul  3 19:38:27 2024 ] 	Batch(3000/7879) done. Loss: 0.1870  lr:0.010000
[ Wed Jul  3 19:38:44 2024 ] 	Batch(3100/7879) done. Loss: 0.0119  lr:0.010000
[ Wed Jul  3 19:39:02 2024 ] 	Batch(3200/7879) done. Loss: 0.3415  lr:0.010000
[ Wed Jul  3 19:39:20 2024 ] 	Batch(3300/7879) done. Loss: 0.4319  lr:0.010000
[ Wed Jul  3 19:39:38 2024 ] 	Batch(3400/7879) done. Loss: 0.6302  lr:0.010000
[ Wed Jul  3 19:39:56 2024 ] 
Training: Epoch [55/120], Step [3499], Loss: 0.4588565230369568, Training Accuracy: 89.96785714285714
[ Wed Jul  3 19:39:56 2024 ] 	Batch(3500/7879) done. Loss: 0.0119  lr:0.010000
[ Wed Jul  3 19:40:14 2024 ] 	Batch(3600/7879) done. Loss: 0.0415  lr:0.010000
[ Wed Jul  3 19:40:32 2024 ] 	Batch(3700/7879) done. Loss: 0.7464  lr:0.010000
[ Wed Jul  3 19:40:50 2024 ] 	Batch(3800/7879) done. Loss: 0.4280  lr:0.010000
[ Wed Jul  3 19:41:08 2024 ] 	Batch(3900/7879) done. Loss: 0.4190  lr:0.010000
[ Wed Jul  3 19:41:26 2024 ] 
Training: Epoch [55/120], Step [3999], Loss: 0.31769606471061707, Training Accuracy: 89.828125
[ Wed Jul  3 19:41:26 2024 ] 	Batch(4000/7879) done. Loss: 0.8966  lr:0.010000
[ Wed Jul  3 19:41:44 2024 ] 	Batch(4100/7879) done. Loss: 0.0929  lr:0.010000
[ Wed Jul  3 19:42:02 2024 ] 	Batch(4200/7879) done. Loss: 0.4144  lr:0.010000
[ Wed Jul  3 19:42:20 2024 ] 	Batch(4300/7879) done. Loss: 0.2554  lr:0.010000
[ Wed Jul  3 19:42:38 2024 ] 	Batch(4400/7879) done. Loss: 0.9264  lr:0.010000
[ Wed Jul  3 19:42:56 2024 ] 
Training: Epoch [55/120], Step [4499], Loss: 0.5328456163406372, Training Accuracy: 89.68611111111112
[ Wed Jul  3 19:42:56 2024 ] 	Batch(4500/7879) done. Loss: 0.4338  lr:0.010000
[ Wed Jul  3 19:43:14 2024 ] 	Batch(4600/7879) done. Loss: 0.0590  lr:0.010000
[ Wed Jul  3 19:43:32 2024 ] 	Batch(4700/7879) done. Loss: 0.2445  lr:0.010000
[ Wed Jul  3 19:43:50 2024 ] 	Batch(4800/7879) done. Loss: 0.5231  lr:0.010000
[ Wed Jul  3 19:44:08 2024 ] 	Batch(4900/7879) done. Loss: 0.1468  lr:0.010000
[ Wed Jul  3 19:44:25 2024 ] 
Training: Epoch [55/120], Step [4999], Loss: 0.052434228360652924, Training Accuracy: 89.64999999999999
[ Wed Jul  3 19:44:25 2024 ] 	Batch(5000/7879) done. Loss: 0.1243  lr:0.010000
[ Wed Jul  3 19:44:43 2024 ] 	Batch(5100/7879) done. Loss: 1.1918  lr:0.010000
[ Wed Jul  3 19:45:01 2024 ] 	Batch(5200/7879) done. Loss: 0.0377  lr:0.010000
[ Wed Jul  3 19:45:20 2024 ] 	Batch(5300/7879) done. Loss: 0.3049  lr:0.010000
[ Wed Jul  3 19:45:38 2024 ] 	Batch(5400/7879) done. Loss: 0.1041  lr:0.010000
[ Wed Jul  3 19:45:56 2024 ] 
Training: Epoch [55/120], Step [5499], Loss: 0.16637156903743744, Training Accuracy: 89.62272727272727
[ Wed Jul  3 19:45:56 2024 ] 	Batch(5500/7879) done. Loss: 0.0121  lr:0.010000
[ Wed Jul  3 19:46:14 2024 ] 	Batch(5600/7879) done. Loss: 0.1135  lr:0.010000
[ Wed Jul  3 19:46:32 2024 ] 	Batch(5700/7879) done. Loss: 0.5936  lr:0.010000
[ Wed Jul  3 19:46:51 2024 ] 	Batch(5800/7879) done. Loss: 0.5829  lr:0.010000
[ Wed Jul  3 19:47:09 2024 ] 	Batch(5900/7879) done. Loss: 0.0167  lr:0.010000
[ Wed Jul  3 19:47:27 2024 ] 
Training: Epoch [55/120], Step [5999], Loss: 0.1979159563779831, Training Accuracy: 89.66666666666666
[ Wed Jul  3 19:47:27 2024 ] 	Batch(6000/7879) done. Loss: 0.0023  lr:0.010000
[ Wed Jul  3 19:47:45 2024 ] 	Batch(6100/7879) done. Loss: 0.2242  lr:0.010000
[ Wed Jul  3 19:48:04 2024 ] 	Batch(6200/7879) done. Loss: 0.3040  lr:0.010000
[ Wed Jul  3 19:48:23 2024 ] 	Batch(6300/7879) done. Loss: 0.0397  lr:0.010000
[ Wed Jul  3 19:48:41 2024 ] 	Batch(6400/7879) done. Loss: 0.0368  lr:0.010000
[ Wed Jul  3 19:49:00 2024 ] 
Training: Epoch [55/120], Step [6499], Loss: 1.1081377267837524, Training Accuracy: 89.64038461538462
[ Wed Jul  3 19:49:00 2024 ] 	Batch(6500/7879) done. Loss: 0.7863  lr:0.010000
[ Wed Jul  3 19:49:18 2024 ] 	Batch(6600/7879) done. Loss: 0.6285  lr:0.010000
[ Wed Jul  3 19:49:37 2024 ] 	Batch(6700/7879) done. Loss: 0.0795  lr:0.010000
[ Wed Jul  3 19:49:55 2024 ] 	Batch(6800/7879) done. Loss: 0.3157  lr:0.010000
[ Wed Jul  3 19:50:13 2024 ] 	Batch(6900/7879) done. Loss: 0.4056  lr:0.010000
[ Wed Jul  3 19:50:31 2024 ] 
Training: Epoch [55/120], Step [6999], Loss: 0.3186820447444916, Training Accuracy: 89.61785714285713
[ Wed Jul  3 19:50:32 2024 ] 	Batch(7000/7879) done. Loss: 0.2226  lr:0.010000
[ Wed Jul  3 19:50:49 2024 ] 	Batch(7100/7879) done. Loss: 0.2160  lr:0.010000
[ Wed Jul  3 19:51:07 2024 ] 	Batch(7200/7879) done. Loss: 0.1756  lr:0.010000
[ Wed Jul  3 19:51:26 2024 ] 	Batch(7300/7879) done. Loss: 0.5475  lr:0.010000
[ Wed Jul  3 19:51:44 2024 ] 	Batch(7400/7879) done. Loss: 0.3287  lr:0.010000
[ Wed Jul  3 19:52:03 2024 ] 
Training: Epoch [55/120], Step [7499], Loss: 0.1408122181892395, Training Accuracy: 89.53166666666667
[ Wed Jul  3 19:52:03 2024 ] 	Batch(7500/7879) done. Loss: 1.0766  lr:0.010000
[ Wed Jul  3 19:52:22 2024 ] 	Batch(7600/7879) done. Loss: 0.1948  lr:0.010000
[ Wed Jul  3 19:52:39 2024 ] 	Batch(7700/7879) done. Loss: 0.1183  lr:0.010000
[ Wed Jul  3 19:52:57 2024 ] 	Batch(7800/7879) done. Loss: 0.0130  lr:0.010000
[ Wed Jul  3 19:53:11 2024 ] 	Mean training loss: 0.3375.
[ Wed Jul  3 19:53:11 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 19:53:12 2024 ] Training epoch: 57
[ Wed Jul  3 19:53:12 2024 ] 	Batch(0/7879) done. Loss: 0.1433  lr:0.010000
[ Wed Jul  3 19:53:30 2024 ] 	Batch(100/7879) done. Loss: 0.4563  lr:0.010000
[ Wed Jul  3 19:53:48 2024 ] 	Batch(200/7879) done. Loss: 0.3363  lr:0.010000
[ Wed Jul  3 19:54:06 2024 ] 	Batch(300/7879) done. Loss: 0.1516  lr:0.010000
[ Wed Jul  3 19:54:23 2024 ] 	Batch(400/7879) done. Loss: 0.2718  lr:0.010000
[ Wed Jul  3 19:54:41 2024 ] 
Training: Epoch [56/120], Step [499], Loss: 0.2919510006904602, Training Accuracy: 91.025
[ Wed Jul  3 19:54:41 2024 ] 	Batch(500/7879) done. Loss: 0.4039  lr:0.010000
[ Wed Jul  3 19:54:59 2024 ] 	Batch(600/7879) done. Loss: 0.1883  lr:0.010000
[ Wed Jul  3 19:55:17 2024 ] 	Batch(700/7879) done. Loss: 0.3600  lr:0.010000
[ Wed Jul  3 19:55:35 2024 ] 	Batch(800/7879) done. Loss: 0.0749  lr:0.010000
[ Wed Jul  3 19:55:53 2024 ] 	Batch(900/7879) done. Loss: 0.0724  lr:0.010000
[ Wed Jul  3 19:56:11 2024 ] 
Training: Epoch [56/120], Step [999], Loss: 0.009394612163305283, Training Accuracy: 91.325
[ Wed Jul  3 19:56:11 2024 ] 	Batch(1000/7879) done. Loss: 1.0906  lr:0.010000
[ Wed Jul  3 19:56:29 2024 ] 	Batch(1100/7879) done. Loss: 1.5474  lr:0.010000
[ Wed Jul  3 19:56:47 2024 ] 	Batch(1200/7879) done. Loss: 0.7283  lr:0.010000
[ Wed Jul  3 19:57:04 2024 ] 	Batch(1300/7879) done. Loss: 0.3657  lr:0.010000
[ Wed Jul  3 19:57:22 2024 ] 	Batch(1400/7879) done. Loss: 0.1976  lr:0.010000
[ Wed Jul  3 19:57:40 2024 ] 
Training: Epoch [56/120], Step [1499], Loss: 0.1701379418373108, Training Accuracy: 90.94166666666666
[ Wed Jul  3 19:57:40 2024 ] 	Batch(1500/7879) done. Loss: 0.5341  lr:0.010000
[ Wed Jul  3 19:57:58 2024 ] 	Batch(1600/7879) done. Loss: 0.0599  lr:0.010000
[ Wed Jul  3 19:58:16 2024 ] 	Batch(1700/7879) done. Loss: 0.8596  lr:0.010000
[ Wed Jul  3 19:58:34 2024 ] 	Batch(1800/7879) done. Loss: 0.1599  lr:0.010000
[ Wed Jul  3 19:58:52 2024 ] 	Batch(1900/7879) done. Loss: 0.8701  lr:0.010000
[ Wed Jul  3 19:59:10 2024 ] 
Training: Epoch [56/120], Step [1999], Loss: 0.23288361728191376, Training Accuracy: 91.09375
[ Wed Jul  3 19:59:10 2024 ] 	Batch(2000/7879) done. Loss: 0.2031  lr:0.010000
[ Wed Jul  3 19:59:29 2024 ] 	Batch(2100/7879) done. Loss: 0.1459  lr:0.010000
[ Wed Jul  3 19:59:47 2024 ] 	Batch(2200/7879) done. Loss: 0.1710  lr:0.010000
[ Wed Jul  3 20:00:06 2024 ] 	Batch(2300/7879) done. Loss: 0.4228  lr:0.010000
[ Wed Jul  3 20:00:24 2024 ] 	Batch(2400/7879) done. Loss: 0.0831  lr:0.010000
[ Wed Jul  3 20:00:42 2024 ] 
Training: Epoch [56/120], Step [2499], Loss: 0.09972690790891647, Training Accuracy: 90.95
[ Wed Jul  3 20:00:42 2024 ] 	Batch(2500/7879) done. Loss: 0.3868  lr:0.010000
[ Wed Jul  3 20:01:00 2024 ] 	Batch(2600/7879) done. Loss: 0.0803  lr:0.010000
[ Wed Jul  3 20:01:18 2024 ] 	Batch(2700/7879) done. Loss: 0.4241  lr:0.010000
[ Wed Jul  3 20:01:36 2024 ] 	Batch(2800/7879) done. Loss: 0.1321  lr:0.010000
[ Wed Jul  3 20:01:54 2024 ] 	Batch(2900/7879) done. Loss: 0.0068  lr:0.010000
[ Wed Jul  3 20:02:11 2024 ] 
Training: Epoch [56/120], Step [2999], Loss: 0.06877151131629944, Training Accuracy: 90.84166666666667
[ Wed Jul  3 20:02:12 2024 ] 	Batch(3000/7879) done. Loss: 0.8762  lr:0.010000
[ Wed Jul  3 20:02:29 2024 ] 	Batch(3100/7879) done. Loss: 0.1325  lr:0.010000
[ Wed Jul  3 20:02:47 2024 ] 	Batch(3200/7879) done. Loss: 0.2726  lr:0.010000
[ Wed Jul  3 20:03:05 2024 ] 	Batch(3300/7879) done. Loss: 0.0936  lr:0.010000
[ Wed Jul  3 20:03:23 2024 ] 	Batch(3400/7879) done. Loss: 0.1721  lr:0.010000
[ Wed Jul  3 20:03:41 2024 ] 
Training: Epoch [56/120], Step [3499], Loss: 0.09599308669567108, Training Accuracy: 90.71785714285714
[ Wed Jul  3 20:03:41 2024 ] 	Batch(3500/7879) done. Loss: 0.0917  lr:0.010000
[ Wed Jul  3 20:03:59 2024 ] 	Batch(3600/7879) done. Loss: 0.3484  lr:0.010000
[ Wed Jul  3 20:04:17 2024 ] 	Batch(3700/7879) done. Loss: 0.2751  lr:0.010000
[ Wed Jul  3 20:04:35 2024 ] 	Batch(3800/7879) done. Loss: 0.5987  lr:0.010000
[ Wed Jul  3 20:04:53 2024 ] 	Batch(3900/7879) done. Loss: 0.1360  lr:0.010000
[ Wed Jul  3 20:05:10 2024 ] 
Training: Epoch [56/120], Step [3999], Loss: 0.12587708234786987, Training Accuracy: 90.60312499999999
[ Wed Jul  3 20:05:11 2024 ] 	Batch(4000/7879) done. Loss: 0.3278  lr:0.010000
[ Wed Jul  3 20:05:29 2024 ] 	Batch(4100/7879) done. Loss: 0.0082  lr:0.010000
[ Wed Jul  3 20:05:48 2024 ] 	Batch(4200/7879) done. Loss: 0.2504  lr:0.010000
[ Wed Jul  3 20:06:06 2024 ] 	Batch(4300/7879) done. Loss: 0.4342  lr:0.010000
[ Wed Jul  3 20:06:25 2024 ] 	Batch(4400/7879) done. Loss: 0.3963  lr:0.010000
[ Wed Jul  3 20:06:43 2024 ] 
Training: Epoch [56/120], Step [4499], Loss: 0.20456953346729279, Training Accuracy: 90.59444444444445
[ Wed Jul  3 20:06:43 2024 ] 	Batch(4500/7879) done. Loss: 0.6986  lr:0.010000
[ Wed Jul  3 20:07:02 2024 ] 	Batch(4600/7879) done. Loss: 0.7302  lr:0.010000
[ Wed Jul  3 20:07:21 2024 ] 	Batch(4700/7879) done. Loss: 0.2016  lr:0.010000
[ Wed Jul  3 20:07:39 2024 ] 	Batch(4800/7879) done. Loss: 0.3787  lr:0.010000
[ Wed Jul  3 20:07:58 2024 ] 	Batch(4900/7879) done. Loss: 0.6725  lr:0.010000
[ Wed Jul  3 20:08:16 2024 ] 
Training: Epoch [56/120], Step [4999], Loss: 0.30016422271728516, Training Accuracy: 90.3975
[ Wed Jul  3 20:08:16 2024 ] 	Batch(5000/7879) done. Loss: 0.2415  lr:0.010000
[ Wed Jul  3 20:08:35 2024 ] 	Batch(5100/7879) done. Loss: 0.6479  lr:0.010000
[ Wed Jul  3 20:08:53 2024 ] 	Batch(5200/7879) done. Loss: 0.3310  lr:0.010000
[ Wed Jul  3 20:09:12 2024 ] 	Batch(5300/7879) done. Loss: 0.1456  lr:0.010000
[ Wed Jul  3 20:09:30 2024 ] 	Batch(5400/7879) done. Loss: 0.0232  lr:0.010000
[ Wed Jul  3 20:09:49 2024 ] 
Training: Epoch [56/120], Step [5499], Loss: 0.35296598076820374, Training Accuracy: 90.35681818181817
[ Wed Jul  3 20:09:49 2024 ] 	Batch(5500/7879) done. Loss: 0.2472  lr:0.010000
[ Wed Jul  3 20:10:07 2024 ] 	Batch(5600/7879) done. Loss: 0.6084  lr:0.010000
[ Wed Jul  3 20:10:26 2024 ] 	Batch(5700/7879) done. Loss: 0.6169  lr:0.010000
[ Wed Jul  3 20:10:44 2024 ] 	Batch(5800/7879) done. Loss: 0.2946  lr:0.010000
[ Wed Jul  3 20:11:03 2024 ] 	Batch(5900/7879) done. Loss: 0.3501  lr:0.010000
[ Wed Jul  3 20:11:21 2024 ] 
Training: Epoch [56/120], Step [5999], Loss: 0.29473641514778137, Training Accuracy: 90.32708333333333
[ Wed Jul  3 20:11:22 2024 ] 	Batch(6000/7879) done. Loss: 0.0091  lr:0.010000
[ Wed Jul  3 20:11:39 2024 ] 	Batch(6100/7879) done. Loss: 0.1721  lr:0.010000
[ Wed Jul  3 20:11:57 2024 ] 	Batch(6200/7879) done. Loss: 0.2276  lr:0.010000
[ Wed Jul  3 20:12:15 2024 ] 	Batch(6300/7879) done. Loss: 0.0543  lr:0.010000
[ Wed Jul  3 20:12:33 2024 ] 	Batch(6400/7879) done. Loss: 0.2597  lr:0.010000
[ Wed Jul  3 20:12:51 2024 ] 
Training: Epoch [56/120], Step [6499], Loss: 0.19857656955718994, Training Accuracy: 90.25769230769231
[ Wed Jul  3 20:12:51 2024 ] 	Batch(6500/7879) done. Loss: 0.3793  lr:0.010000
[ Wed Jul  3 20:13:09 2024 ] 	Batch(6600/7879) done. Loss: 0.2569  lr:0.010000
[ Wed Jul  3 20:13:27 2024 ] 	Batch(6700/7879) done. Loss: 0.0405  lr:0.010000
[ Wed Jul  3 20:13:45 2024 ] 	Batch(6800/7879) done. Loss: 0.6545  lr:0.010000
[ Wed Jul  3 20:14:04 2024 ] 	Batch(6900/7879) done. Loss: 0.2712  lr:0.010000
[ Wed Jul  3 20:14:22 2024 ] 
Training: Epoch [56/120], Step [6999], Loss: 0.24902531504631042, Training Accuracy: 90.18392857142857
[ Wed Jul  3 20:14:23 2024 ] 	Batch(7000/7879) done. Loss: 0.1435  lr:0.010000
[ Wed Jul  3 20:14:41 2024 ] 	Batch(7100/7879) done. Loss: 0.4983  lr:0.010000
[ Wed Jul  3 20:15:00 2024 ] 	Batch(7200/7879) done. Loss: 0.8222  lr:0.010000
[ Wed Jul  3 20:15:18 2024 ] 	Batch(7300/7879) done. Loss: 0.1834  lr:0.010000
[ Wed Jul  3 20:15:37 2024 ] 	Batch(7400/7879) done. Loss: 0.1959  lr:0.010000
[ Wed Jul  3 20:15:55 2024 ] 
Training: Epoch [56/120], Step [7499], Loss: 0.24067525565624237, Training Accuracy: 90.155
[ Wed Jul  3 20:15:55 2024 ] 	Batch(7500/7879) done. Loss: 0.0734  lr:0.010000
[ Wed Jul  3 20:16:14 2024 ] 	Batch(7600/7879) done. Loss: 0.1349  lr:0.010000
[ Wed Jul  3 20:16:33 2024 ] 	Batch(7700/7879) done. Loss: 0.2241  lr:0.010000
[ Wed Jul  3 20:16:51 2024 ] 	Batch(7800/7879) done. Loss: 0.5282  lr:0.010000
[ Wed Jul  3 20:17:06 2024 ] 	Mean training loss: 0.3172.
[ Wed Jul  3 20:17:06 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 20:17:06 2024 ] Training epoch: 58
[ Wed Jul  3 20:17:06 2024 ] 	Batch(0/7879) done. Loss: 0.4947  lr:0.010000
[ Wed Jul  3 20:17:25 2024 ] 	Batch(100/7879) done. Loss: 0.3986  lr:0.010000
[ Wed Jul  3 20:17:43 2024 ] 	Batch(200/7879) done. Loss: 0.0760  lr:0.010000
[ Wed Jul  3 20:18:02 2024 ] 	Batch(300/7879) done. Loss: 0.3624  lr:0.010000
[ Wed Jul  3 20:18:20 2024 ] 	Batch(400/7879) done. Loss: 0.1645  lr:0.010000
[ Wed Jul  3 20:18:39 2024 ] 
Training: Epoch [57/120], Step [499], Loss: 0.45436954498291016, Training Accuracy: 90.725
[ Wed Jul  3 20:18:39 2024 ] 	Batch(500/7879) done. Loss: 0.2796  lr:0.010000
[ Wed Jul  3 20:18:57 2024 ] 	Batch(600/7879) done. Loss: 0.3245  lr:0.010000
[ Wed Jul  3 20:19:16 2024 ] 	Batch(700/7879) done. Loss: 0.4904  lr:0.010000
[ Wed Jul  3 20:19:35 2024 ] 	Batch(800/7879) done. Loss: 0.0061  lr:0.010000
[ Wed Jul  3 20:19:52 2024 ] 	Batch(900/7879) done. Loss: 0.3409  lr:0.010000
[ Wed Jul  3 20:20:10 2024 ] 
Training: Epoch [57/120], Step [999], Loss: 0.3405308723449707, Training Accuracy: 91.03750000000001
[ Wed Jul  3 20:20:10 2024 ] 	Batch(1000/7879) done. Loss: 0.1385  lr:0.010000
[ Wed Jul  3 20:20:28 2024 ] 	Batch(1100/7879) done. Loss: 0.2396  lr:0.010000
[ Wed Jul  3 20:20:46 2024 ] 	Batch(1200/7879) done. Loss: 0.0313  lr:0.010000
[ Wed Jul  3 20:21:05 2024 ] 	Batch(1300/7879) done. Loss: 0.0087  lr:0.010000
[ Wed Jul  3 20:21:24 2024 ] 	Batch(1400/7879) done. Loss: 0.5817  lr:0.010000
[ Wed Jul  3 20:21:43 2024 ] 
Training: Epoch [57/120], Step [1499], Loss: 0.9257069826126099, Training Accuracy: 90.76666666666667
[ Wed Jul  3 20:21:43 2024 ] 	Batch(1500/7879) done. Loss: 0.5793  lr:0.010000
[ Wed Jul  3 20:22:02 2024 ] 	Batch(1600/7879) done. Loss: 0.1941  lr:0.010000
[ Wed Jul  3 20:22:20 2024 ] 	Batch(1700/7879) done. Loss: 0.0663  lr:0.010000
[ Wed Jul  3 20:22:39 2024 ] 	Batch(1800/7879) done. Loss: 0.6022  lr:0.010000
[ Wed Jul  3 20:22:57 2024 ] 	Batch(1900/7879) done. Loss: 0.2508  lr:0.010000
[ Wed Jul  3 20:23:15 2024 ] 
Training: Epoch [57/120], Step [1999], Loss: 0.31293416023254395, Training Accuracy: 90.26875
[ Wed Jul  3 20:23:15 2024 ] 	Batch(2000/7879) done. Loss: 0.1830  lr:0.010000
[ Wed Jul  3 20:23:33 2024 ] 	Batch(2100/7879) done. Loss: 0.2231  lr:0.010000
[ Wed Jul  3 20:23:51 2024 ] 	Batch(2200/7879) done. Loss: 0.8986  lr:0.010000
[ Wed Jul  3 20:24:09 2024 ] 	Batch(2300/7879) done. Loss: 0.0775  lr:0.010000
[ Wed Jul  3 20:24:27 2024 ] 	Batch(2400/7879) done. Loss: 0.6740  lr:0.010000
[ Wed Jul  3 20:24:45 2024 ] 
Training: Epoch [57/120], Step [2499], Loss: 0.04212053492665291, Training Accuracy: 90.41
[ Wed Jul  3 20:24:45 2024 ] 	Batch(2500/7879) done. Loss: 0.3826  lr:0.010000
[ Wed Jul  3 20:25:03 2024 ] 	Batch(2600/7879) done. Loss: 0.2728  lr:0.010000
[ Wed Jul  3 20:25:21 2024 ] 	Batch(2700/7879) done. Loss: 0.4991  lr:0.010000
[ Wed Jul  3 20:25:39 2024 ] 	Batch(2800/7879) done. Loss: 0.2164  lr:0.010000
[ Wed Jul  3 20:25:57 2024 ] 	Batch(2900/7879) done. Loss: 0.5859  lr:0.010000
[ Wed Jul  3 20:26:14 2024 ] 
Training: Epoch [57/120], Step [2999], Loss: 0.8482578992843628, Training Accuracy: 90.39583333333333
[ Wed Jul  3 20:26:15 2024 ] 	Batch(3000/7879) done. Loss: 0.3524  lr:0.010000
[ Wed Jul  3 20:26:32 2024 ] 	Batch(3100/7879) done. Loss: 0.3875  lr:0.010000
[ Wed Jul  3 20:26:50 2024 ] 	Batch(3200/7879) done. Loss: 0.5391  lr:0.010000
[ Wed Jul  3 20:27:09 2024 ] 	Batch(3300/7879) done. Loss: 0.2965  lr:0.010000
[ Wed Jul  3 20:27:27 2024 ] 	Batch(3400/7879) done. Loss: 0.9511  lr:0.010000
[ Wed Jul  3 20:27:44 2024 ] 
Training: Epoch [57/120], Step [3499], Loss: 0.07863686233758926, Training Accuracy: 90.05
[ Wed Jul  3 20:27:44 2024 ] 	Batch(3500/7879) done. Loss: 0.1341  lr:0.010000
[ Wed Jul  3 20:28:02 2024 ] 	Batch(3600/7879) done. Loss: 0.3996  lr:0.010000
[ Wed Jul  3 20:28:21 2024 ] 	Batch(3700/7879) done. Loss: 0.5609  lr:0.010000
[ Wed Jul  3 20:28:39 2024 ] 	Batch(3800/7879) done. Loss: 0.9206  lr:0.010000
[ Wed Jul  3 20:28:58 2024 ] 	Batch(3900/7879) done. Loss: 0.1397  lr:0.010000
[ Wed Jul  3 20:29:16 2024 ] 
Training: Epoch [57/120], Step [3999], Loss: 0.42292219400405884, Training Accuracy: 89.940625
[ Wed Jul  3 20:29:17 2024 ] 	Batch(4000/7879) done. Loss: 0.4126  lr:0.010000
[ Wed Jul  3 20:29:35 2024 ] 	Batch(4100/7879) done. Loss: 0.3350  lr:0.010000
[ Wed Jul  3 20:29:54 2024 ] 	Batch(4200/7879) done. Loss: 0.7403  lr:0.010000
[ Wed Jul  3 20:30:12 2024 ] 	Batch(4300/7879) done. Loss: 0.5297  lr:0.010000
[ Wed Jul  3 20:30:31 2024 ] 	Batch(4400/7879) done. Loss: 0.0750  lr:0.010000
[ Wed Jul  3 20:30:49 2024 ] 
Training: Epoch [57/120], Step [4499], Loss: 0.4990676939487457, Training Accuracy: 89.89444444444445
[ Wed Jul  3 20:30:49 2024 ] 	Batch(4500/7879) done. Loss: 0.0358  lr:0.010000
[ Wed Jul  3 20:31:08 2024 ] 	Batch(4600/7879) done. Loss: 0.1267  lr:0.010000
[ Wed Jul  3 20:31:26 2024 ] 	Batch(4700/7879) done. Loss: 0.2506  lr:0.010000
[ Wed Jul  3 20:31:45 2024 ] 	Batch(4800/7879) done. Loss: 0.0539  lr:0.010000
[ Wed Jul  3 20:32:03 2024 ] 	Batch(4900/7879) done. Loss: 0.0130  lr:0.010000
[ Wed Jul  3 20:32:21 2024 ] 
Training: Epoch [57/120], Step [4999], Loss: 0.024407275021076202, Training Accuracy: 89.9225
[ Wed Jul  3 20:32:21 2024 ] 	Batch(5000/7879) done. Loss: 0.1899  lr:0.010000
[ Wed Jul  3 20:32:39 2024 ] 	Batch(5100/7879) done. Loss: 0.0319  lr:0.010000
[ Wed Jul  3 20:32:57 2024 ] 	Batch(5200/7879) done. Loss: 0.4448  lr:0.010000
[ Wed Jul  3 20:33:15 2024 ] 	Batch(5300/7879) done. Loss: 0.6882  lr:0.010000
[ Wed Jul  3 20:33:33 2024 ] 	Batch(5400/7879) done. Loss: 0.0577  lr:0.010000
[ Wed Jul  3 20:33:50 2024 ] 
Training: Epoch [57/120], Step [5499], Loss: 0.011229380965232849, Training Accuracy: 89.82272727272728
[ Wed Jul  3 20:33:50 2024 ] 	Batch(5500/7879) done. Loss: 0.1890  lr:0.010000
[ Wed Jul  3 20:34:08 2024 ] 	Batch(5600/7879) done. Loss: 0.0308  lr:0.010000
[ Wed Jul  3 20:34:26 2024 ] 	Batch(5700/7879) done. Loss: 0.7599  lr:0.010000
[ Wed Jul  3 20:34:44 2024 ] 	Batch(5800/7879) done. Loss: 0.0111  lr:0.010000
[ Wed Jul  3 20:35:02 2024 ] 	Batch(5900/7879) done. Loss: 0.2264  lr:0.010000
[ Wed Jul  3 20:35:20 2024 ] 
Training: Epoch [57/120], Step [5999], Loss: 0.5085508823394775, Training Accuracy: 89.7875
[ Wed Jul  3 20:35:20 2024 ] 	Batch(6000/7879) done. Loss: 0.5511  lr:0.010000
[ Wed Jul  3 20:35:38 2024 ] 	Batch(6100/7879) done. Loss: 0.0894  lr:0.010000
[ Wed Jul  3 20:35:56 2024 ] 	Batch(6200/7879) done. Loss: 0.7568  lr:0.010000
[ Wed Jul  3 20:36:14 2024 ] 	Batch(6300/7879) done. Loss: 0.0511  lr:0.010000
[ Wed Jul  3 20:36:32 2024 ] 	Batch(6400/7879) done. Loss: 0.5002  lr:0.010000
[ Wed Jul  3 20:36:50 2024 ] 
Training: Epoch [57/120], Step [6499], Loss: 0.9170798063278198, Training Accuracy: 89.7
[ Wed Jul  3 20:36:50 2024 ] 	Batch(6500/7879) done. Loss: 0.0351  lr:0.010000
[ Wed Jul  3 20:37:09 2024 ] 	Batch(6600/7879) done. Loss: 0.0945  lr:0.010000
[ Wed Jul  3 20:37:27 2024 ] 	Batch(6700/7879) done. Loss: 0.5484  lr:0.010000
[ Wed Jul  3 20:37:46 2024 ] 	Batch(6800/7879) done. Loss: 1.6458  lr:0.010000
[ Wed Jul  3 20:38:05 2024 ] 	Batch(6900/7879) done. Loss: 0.7657  lr:0.010000
[ Wed Jul  3 20:38:22 2024 ] 
Training: Epoch [57/120], Step [6999], Loss: 0.4760073721408844, Training Accuracy: 89.68928571428572
[ Wed Jul  3 20:38:23 2024 ] 	Batch(7000/7879) done. Loss: 0.1591  lr:0.010000
[ Wed Jul  3 20:38:41 2024 ] 	Batch(7100/7879) done. Loss: 0.5487  lr:0.010000
[ Wed Jul  3 20:38:59 2024 ] 	Batch(7200/7879) done. Loss: 0.0637  lr:0.010000
[ Wed Jul  3 20:39:17 2024 ] 	Batch(7300/7879) done. Loss: 0.1905  lr:0.010000
[ Wed Jul  3 20:39:35 2024 ] 	Batch(7400/7879) done. Loss: 0.4684  lr:0.010000
[ Wed Jul  3 20:39:52 2024 ] 
Training: Epoch [57/120], Step [7499], Loss: 0.6231853365898132, Training Accuracy: 89.74499999999999
[ Wed Jul  3 20:39:53 2024 ] 	Batch(7500/7879) done. Loss: 0.6906  lr:0.010000
[ Wed Jul  3 20:40:11 2024 ] 	Batch(7600/7879) done. Loss: 0.3737  lr:0.010000
[ Wed Jul  3 20:40:29 2024 ] 	Batch(7700/7879) done. Loss: 0.4345  lr:0.010000
[ Wed Jul  3 20:40:48 2024 ] 	Batch(7800/7879) done. Loss: 0.1334  lr:0.010000
[ Wed Jul  3 20:41:02 2024 ] 	Mean training loss: 0.3278.
[ Wed Jul  3 20:41:02 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul  3 20:41:02 2024 ] Training epoch: 59
[ Wed Jul  3 20:41:03 2024 ] 	Batch(0/7879) done. Loss: 0.0232  lr:0.010000
[ Wed Jul  3 20:41:21 2024 ] 	Batch(100/7879) done. Loss: 0.2507  lr:0.010000
[ Wed Jul  3 20:41:39 2024 ] 	Batch(200/7879) done. Loss: 0.2359  lr:0.010000
[ Wed Jul  3 20:41:57 2024 ] 	Batch(300/7879) done. Loss: 0.0523  lr:0.010000
[ Wed Jul  3 20:42:15 2024 ] 	Batch(400/7879) done. Loss: 0.3014  lr:0.010000
[ Wed Jul  3 20:42:32 2024 ] 
Training: Epoch [58/120], Step [499], Loss: 0.1656457632780075, Training Accuracy: 90.25
[ Wed Jul  3 20:42:33 2024 ] 	Batch(500/7879) done. Loss: 0.5006  lr:0.010000
[ Wed Jul  3 20:42:50 2024 ] 	Batch(600/7879) done. Loss: 0.5998  lr:0.010000
[ Wed Jul  3 20:43:08 2024 ] 	Batch(700/7879) done. Loss: 0.2529  lr:0.010000
[ Wed Jul  3 20:43:26 2024 ] 	Batch(800/7879) done. Loss: 0.7428  lr:0.010000
[ Wed Jul  3 20:43:44 2024 ] 	Batch(900/7879) done. Loss: 0.0082  lr:0.010000
[ Wed Jul  3 20:44:02 2024 ] 
Training: Epoch [58/120], Step [999], Loss: 0.17322713136672974, Training Accuracy: 90.3375
[ Wed Jul  3 20:44:02 2024 ] 	Batch(1000/7879) done. Loss: 0.1450  lr:0.010000
[ Wed Jul  3 20:44:20 2024 ] 	Batch(1100/7879) done. Loss: 0.4101  lr:0.010000
[ Wed Jul  3 20:44:38 2024 ] 	Batch(1200/7879) done. Loss: 0.0462  lr:0.010000
[ Wed Jul  3 20:44:56 2024 ] 	Batch(1300/7879) done. Loss: 0.0681  lr:0.010000
[ Wed Jul  3 20:45:14 2024 ] 	Batch(1400/7879) done. Loss: 0.2260  lr:0.010000
[ Wed Jul  3 20:45:32 2024 ] 
Training: Epoch [58/120], Step [1499], Loss: 0.012801209464669228, Training Accuracy: 91.04166666666667
[ Wed Jul  3 20:45:33 2024 ] 	Batch(1500/7879) done. Loss: 0.1047  lr:0.010000
[ Wed Jul  3 20:45:51 2024 ] 	Batch(1600/7879) done. Loss: 0.2844  lr:0.010000
[ Wed Jul  3 20:46:09 2024 ] 	Batch(1700/7879) done. Loss: 0.4855  lr:0.010000
[ Wed Jul  3 20:46:28 2024 ] 	Batch(1800/7879) done. Loss: 0.0811  lr:0.010000
[ Wed Jul  3 20:46:46 2024 ] 	Batch(1900/7879) done. Loss: 0.2483  lr:0.010000
[ Wed Jul  3 20:47:05 2024 ] 
Training: Epoch [58/120], Step [1999], Loss: 0.2923200726509094, Training Accuracy: 91.1125
[ Wed Jul  3 20:47:05 2024 ] 	Batch(2000/7879) done. Loss: 0.0286  lr:0.010000
[ Wed Jul  3 20:47:23 2024 ] 	Batch(2100/7879) done. Loss: 0.0880  lr:0.010000
[ Wed Jul  3 20:47:42 2024 ] 	Batch(2200/7879) done. Loss: 0.0246  lr:0.010000
[ Wed Jul  3 20:48:00 2024 ] 	Batch(2300/7879) done. Loss: 0.6783  lr:0.010000
[ Wed Jul  3 20:48:19 2024 ] 	Batch(2400/7879) done. Loss: 0.0468  lr:0.010000
[ Wed Jul  3 20:48:38 2024 ] 
Training: Epoch [58/120], Step [2499], Loss: 0.21590858697891235, Training Accuracy: 90.995
[ Wed Jul  3 20:48:38 2024 ] 	Batch(2500/7879) done. Loss: 0.3590  lr:0.010000
[ Wed Jul  3 20:48:56 2024 ] 	Batch(2600/7879) done. Loss: 0.2200  lr:0.010000
[ Wed Jul  3 20:49:15 2024 ] 	Batch(2700/7879) done. Loss: 0.4298  lr:0.010000
[ Wed Jul  3 20:49:33 2024 ] 	Batch(2800/7879) done. Loss: 1.3132  lr:0.010000
[ Wed Jul  3 20:49:52 2024 ] 	Batch(2900/7879) done. Loss: 0.3422  lr:0.010000
[ Wed Jul  3 20:50:10 2024 ] 
Training: Epoch [58/120], Step [2999], Loss: 0.23099090158939362, Training Accuracy: 90.85416666666667
[ Wed Jul  3 20:50:10 2024 ] 	Batch(3000/7879) done. Loss: 0.3962  lr:0.010000
[ Wed Jul  3 20:50:29 2024 ] 	Batch(3100/7879) done. Loss: 1.0646  lr:0.010000
[ Wed Jul  3 20:50:48 2024 ] 	Batch(3200/7879) done. Loss: 0.3126  lr:0.010000
[ Wed Jul  3 20:51:06 2024 ] 	Batch(3300/7879) done. Loss: 0.1604  lr:0.010000
[ Wed Jul  3 20:51:25 2024 ] 	Batch(3400/7879) done. Loss: 0.1966  lr:0.010000
[ Wed Jul  3 20:51:43 2024 ] 
Training: Epoch [58/120], Step [3499], Loss: 0.40010905265808105, Training Accuracy: 90.80357142857143
[ Wed Jul  3 20:51:43 2024 ] 	Batch(3500/7879) done. Loss: 0.4373  lr:0.010000
[ Wed Jul  3 20:52:02 2024 ] 	Batch(3600/7879) done. Loss: 0.8219  lr:0.010000
[ Wed Jul  3 20:52:19 2024 ] 	Batch(3700/7879) done. Loss: 0.1407  lr:0.010000
[ Wed Jul  3 20:52:37 2024 ] 	Batch(3800/7879) done. Loss: 0.0364  lr:0.010000
[ Wed Jul  3 20:52:55 2024 ] 	Batch(3900/7879) done. Loss: 1.2310  lr:0.010000
[ Wed Jul  3 20:53:13 2024 ] 
Training: Epoch [58/120], Step [3999], Loss: 0.5798405408859253, Training Accuracy: 90.559375
[ Wed Jul  3 20:53:13 2024 ] 	Batch(4000/7879) done. Loss: 0.9614  lr:0.010000
[ Wed Jul  3 20:53:31 2024 ] 	Batch(4100/7879) done. Loss: 0.3682  lr:0.010000
[ Wed Jul  3 20:53:49 2024 ] 	Batch(4200/7879) done. Loss: 0.1414  lr:0.010000
[ Wed Jul  3 20:54:07 2024 ] 	Batch(4300/7879) done. Loss: 0.5826  lr:0.010000
[ Wed Jul  3 20:54:25 2024 ] 	Batch(4400/7879) done. Loss: 0.3480  lr:0.010000
[ Wed Jul  3 20:54:43 2024 ] 
Training: Epoch [58/120], Step [4499], Loss: 0.12447507679462433, Training Accuracy: 90.42777777777778
[ Wed Jul  3 20:54:43 2024 ] 	Batch(4500/7879) done. Loss: 0.1081  lr:0.010000
[ Wed Jul  3 20:55:01 2024 ] 	Batch(4600/7879) done. Loss: 0.2957  lr:0.010000
[ Wed Jul  3 20:55:19 2024 ] 	Batch(4700/7879) done. Loss: 1.0364  lr:0.010000
[ Wed Jul  3 20:55:37 2024 ] 	Batch(4800/7879) done. Loss: 0.8007  lr:0.010000
[ Wed Jul  3 20:55:55 2024 ] 	Batch(4900/7879) done. Loss: 0.1234  lr:0.010000
[ Wed Jul  3 20:56:12 2024 ] 
Training: Epoch [58/120], Step [4999], Loss: 0.5830601453781128, Training Accuracy: 90.4225
[ Wed Jul  3 20:56:12 2024 ] 	Batch(5000/7879) done. Loss: 0.0560  lr:0.010000
[ Wed Jul  3 20:56:30 2024 ] 	Batch(5100/7879) done. Loss: 0.1383  lr:0.010000
[ Wed Jul  3 20:56:48 2024 ] 	Batch(5200/7879) done. Loss: 0.1333  lr:0.010000
[ Wed Jul  3 20:57:07 2024 ] 	Batch(5300/7879) done. Loss: 0.0501  lr:0.010000
[ Wed Jul  3 20:57:25 2024 ] 	Batch(5400/7879) done. Loss: 0.1201  lr:0.010000
[ Wed Jul  3 20:57:44 2024 ] 
Training: Epoch [58/120], Step [5499], Loss: 0.0031938436441123486, Training Accuracy: 90.325
[ Wed Jul  3 20:57:44 2024 ] 	Batch(5500/7879) done. Loss: 0.0522  lr:0.010000
[ Wed Jul  3 20:58:03 2024 ] 	Batch(5600/7879) done. Loss: 0.4794  lr:0.010000
[ Wed Jul  3 20:58:21 2024 ] 	Batch(5700/7879) done. Loss: 0.7636  lr:0.010000
[ Wed Jul  3 20:58:40 2024 ] 	Batch(5800/7879) done. Loss: 0.3710  lr:0.010000
[ Wed Jul  3 20:58:58 2024 ] 	Batch(5900/7879) done. Loss: 0.0919  lr:0.010000
[ Wed Jul  3 20:59:17 2024 ] 
Training: Epoch [58/120], Step [5999], Loss: 0.25312498211860657, Training Accuracy: 90.35625
[ Wed Jul  3 20:59:17 2024 ] 	Batch(6000/7879) done. Loss: 0.2470  lr:0.010000
[ Wed Jul  3 20:59:35 2024 ] 	Batch(6100/7879) done. Loss: 0.5043  lr:0.010000
[ Wed Jul  3 20:59:53 2024 ] 	Batch(6200/7879) done. Loss: 0.5242  lr:0.010000
[ Wed Jul  3 21:00:11 2024 ] 	Batch(6300/7879) done. Loss: 0.3423  lr:0.010000
[ Wed Jul  3 21:00:29 2024 ] 	Batch(6400/7879) done. Loss: 2.0729  lr:0.010000
[ Wed Jul  3 21:00:46 2024 ] 
Training: Epoch [58/120], Step [6499], Loss: 0.49780768156051636, Training Accuracy: 90.25384615384615
[ Wed Jul  3 21:00:47 2024 ] 	Batch(6500/7879) done. Loss: 0.0953  lr:0.010000
[ Wed Jul  3 21:01:04 2024 ] 	Batch(6600/7879) done. Loss: 0.7947  lr:0.010000
[ Wed Jul  3 21:01:22 2024 ] 	Batch(6700/7879) done. Loss: 0.7038  lr:0.010000
[ Wed Jul  3 21:01:40 2024 ] 	Batch(6800/7879) done. Loss: 0.3601  lr:0.010000
[ Wed Jul  3 21:01:58 2024 ] 	Batch(6900/7879) done. Loss: 0.2463  lr:0.010000
[ Wed Jul  3 21:02:16 2024 ] 
Training: Epoch [58/120], Step [6999], Loss: 0.14098761975765228, Training Accuracy: 90.17321428571428
[ Wed Jul  3 21:02:16 2024 ] 	Batch(7000/7879) done. Loss: 0.1599  lr:0.010000
[ Wed Jul  3 21:02:34 2024 ] 	Batch(7100/7879) done. Loss: 0.3695  lr:0.010000
[ Wed Jul  3 21:02:52 2024 ] 	Batch(7200/7879) done. Loss: 1.1327  lr:0.010000
[ Wed Jul  3 21:03:10 2024 ] 	Batch(7300/7879) done. Loss: 0.0552  lr:0.010000
[ Wed Jul  3 21:03:28 2024 ] 	Batch(7400/7879) done. Loss: 0.0854  lr:0.010000
[ Wed Jul  3 21:03:45 2024 ] 
Training: Epoch [58/120], Step [7499], Loss: 0.38159865140914917, Training Accuracy: 90.06333333333333
[ Wed Jul  3 21:03:46 2024 ] 	Batch(7500/7879) done. Loss: 0.2888  lr:0.010000
[ Wed Jul  3 21:04:04 2024 ] 	Batch(7600/7879) done. Loss: 0.1574  lr:0.010000
[ Wed Jul  3 21:04:22 2024 ] 	Batch(7700/7879) done. Loss: 0.5708  lr:0.010000
[ Wed Jul  3 21:04:41 2024 ] 	Batch(7800/7879) done. Loss: 0.2048  lr:0.010000
[ Wed Jul  3 21:04:56 2024 ] 	Mean training loss: 0.3109.
[ Wed Jul  3 21:04:56 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 21:04:56 2024 ] Training epoch: 60
[ Wed Jul  3 21:04:57 2024 ] 	Batch(0/7879) done. Loss: 0.2431  lr:0.010000
[ Wed Jul  3 21:05:14 2024 ] 	Batch(100/7879) done. Loss: 0.3221  lr:0.010000
[ Wed Jul  3 21:05:32 2024 ] 	Batch(200/7879) done. Loss: 0.2060  lr:0.010000
[ Wed Jul  3 21:05:50 2024 ] 	Batch(300/7879) done. Loss: 0.0663  lr:0.010000
[ Wed Jul  3 21:06:08 2024 ] 	Batch(400/7879) done. Loss: 0.4473  lr:0.010000
[ Wed Jul  3 21:06:26 2024 ] 
Training: Epoch [59/120], Step [499], Loss: 0.04511238634586334, Training Accuracy: 91.025
[ Wed Jul  3 21:06:26 2024 ] 	Batch(500/7879) done. Loss: 0.2802  lr:0.010000
[ Wed Jul  3 21:06:44 2024 ] 	Batch(600/7879) done. Loss: 0.0189  lr:0.010000
[ Wed Jul  3 21:07:02 2024 ] 	Batch(700/7879) done. Loss: 0.0012  lr:0.010000
[ Wed Jul  3 21:07:20 2024 ] 	Batch(800/7879) done. Loss: 0.0074  lr:0.010000
[ Wed Jul  3 21:07:37 2024 ] 	Batch(900/7879) done. Loss: 0.0637  lr:0.010000
[ Wed Jul  3 21:07:55 2024 ] 
Training: Epoch [59/120], Step [999], Loss: 0.010754460468888283, Training Accuracy: 91.27499999999999
[ Wed Jul  3 21:07:55 2024 ] 	Batch(1000/7879) done. Loss: 0.4180  lr:0.010000
[ Wed Jul  3 21:08:13 2024 ] 	Batch(1100/7879) done. Loss: 0.1547  lr:0.010000
[ Wed Jul  3 21:08:31 2024 ] 	Batch(1200/7879) done. Loss: 0.0345  lr:0.010000
[ Wed Jul  3 21:08:49 2024 ] 	Batch(1300/7879) done. Loss: 0.3657  lr:0.010000
[ Wed Jul  3 21:09:07 2024 ] 	Batch(1400/7879) done. Loss: 0.4043  lr:0.010000
[ Wed Jul  3 21:09:25 2024 ] 
Training: Epoch [59/120], Step [1499], Loss: 0.5843834280967712, Training Accuracy: 90.93333333333334
[ Wed Jul  3 21:09:25 2024 ] 	Batch(1500/7879) done. Loss: 0.2275  lr:0.010000
[ Wed Jul  3 21:09:43 2024 ] 	Batch(1600/7879) done. Loss: 0.1097  lr:0.010000
[ Wed Jul  3 21:10:01 2024 ] 	Batch(1700/7879) done. Loss: 0.2708  lr:0.010000
[ Wed Jul  3 21:10:19 2024 ] 	Batch(1800/7879) done. Loss: 0.1413  lr:0.010000
[ Wed Jul  3 21:10:36 2024 ] 	Batch(1900/7879) done. Loss: 0.9983  lr:0.010000
[ Wed Jul  3 21:10:54 2024 ] 
Training: Epoch [59/120], Step [1999], Loss: 0.021045565605163574, Training Accuracy: 90.93124999999999
[ Wed Jul  3 21:10:55 2024 ] 	Batch(2000/7879) done. Loss: 0.1560  lr:0.010000
[ Wed Jul  3 21:11:13 2024 ] 	Batch(2100/7879) done. Loss: 0.0831  lr:0.010000
[ Wed Jul  3 21:11:32 2024 ] 	Batch(2200/7879) done. Loss: 0.2109  lr:0.010000
[ Wed Jul  3 21:11:50 2024 ] 	Batch(2300/7879) done. Loss: 0.3451  lr:0.010000
[ Wed Jul  3 21:12:08 2024 ] 	Batch(2400/7879) done. Loss: 0.2035  lr:0.010000
[ Wed Jul  3 21:12:25 2024 ] 
Training: Epoch [59/120], Step [2499], Loss: 0.1457400918006897, Training Accuracy: 90.825
[ Wed Jul  3 21:12:26 2024 ] 	Batch(2500/7879) done. Loss: 0.1038  lr:0.010000
[ Wed Jul  3 21:12:43 2024 ] 	Batch(2600/7879) done. Loss: 0.0273  lr:0.010000
[ Wed Jul  3 21:13:01 2024 ] 	Batch(2700/7879) done. Loss: 0.0142  lr:0.010000
[ Wed Jul  3 21:13:19 2024 ] 	Batch(2800/7879) done. Loss: 0.5030  lr:0.010000
[ Wed Jul  3 21:13:38 2024 ] 	Batch(2900/7879) done. Loss: 0.2023  lr:0.010000
[ Wed Jul  3 21:13:56 2024 ] 
Training: Epoch [59/120], Step [2999], Loss: 0.3746899962425232, Training Accuracy: 90.8375
[ Wed Jul  3 21:13:56 2024 ] 	Batch(3000/7879) done. Loss: 0.0552  lr:0.010000
[ Wed Jul  3 21:14:14 2024 ] 	Batch(3100/7879) done. Loss: 0.4113  lr:0.010000
[ Wed Jul  3 21:14:32 2024 ] 	Batch(3200/7879) done. Loss: 0.1852  lr:0.010000
[ Wed Jul  3 21:14:50 2024 ] 	Batch(3300/7879) done. Loss: 0.1468  lr:0.010000
[ Wed Jul  3 21:15:08 2024 ] 	Batch(3400/7879) done. Loss: 0.1278  lr:0.010000
[ Wed Jul  3 21:15:26 2024 ] 
Training: Epoch [59/120], Step [3499], Loss: 0.016705669462680817, Training Accuracy: 90.825
[ Wed Jul  3 21:15:26 2024 ] 	Batch(3500/7879) done. Loss: 0.0738  lr:0.010000
[ Wed Jul  3 21:15:44 2024 ] 	Batch(3600/7879) done. Loss: 0.3034  lr:0.010000
[ Wed Jul  3 21:16:02 2024 ] 	Batch(3700/7879) done. Loss: 0.1091  lr:0.010000
[ Wed Jul  3 21:16:19 2024 ] 	Batch(3800/7879) done. Loss: 0.1484  lr:0.010000
[ Wed Jul  3 21:16:37 2024 ] 	Batch(3900/7879) done. Loss: 0.2835  lr:0.010000
[ Wed Jul  3 21:16:55 2024 ] 
Training: Epoch [59/120], Step [3999], Loss: 0.5426809191703796, Training Accuracy: 90.7875
[ Wed Jul  3 21:16:55 2024 ] 	Batch(4000/7879) done. Loss: 0.8481  lr:0.010000
[ Wed Jul  3 21:17:13 2024 ] 	Batch(4100/7879) done. Loss: 0.3840  lr:0.010000
[ Wed Jul  3 21:17:31 2024 ] 	Batch(4200/7879) done. Loss: 0.5325  lr:0.010000
[ Wed Jul  3 21:17:49 2024 ] 	Batch(4300/7879) done. Loss: 0.0669  lr:0.010000
[ Wed Jul  3 21:18:07 2024 ] 	Batch(4400/7879) done. Loss: 0.0740  lr:0.010000
[ Wed Jul  3 21:18:25 2024 ] 
Training: Epoch [59/120], Step [4499], Loss: 0.08515580743551254, Training Accuracy: 90.64166666666667
[ Wed Jul  3 21:18:25 2024 ] 	Batch(4500/7879) done. Loss: 0.1393  lr:0.010000
[ Wed Jul  3 21:18:43 2024 ] 	Batch(4600/7879) done. Loss: 0.1368  lr:0.010000
[ Wed Jul  3 21:19:01 2024 ] 	Batch(4700/7879) done. Loss: 0.1833  lr:0.010000
[ Wed Jul  3 21:19:19 2024 ] 	Batch(4800/7879) done. Loss: 0.0316  lr:0.010000
[ Wed Jul  3 21:19:37 2024 ] 	Batch(4900/7879) done. Loss: 0.1373  lr:0.010000
[ Wed Jul  3 21:19:54 2024 ] 
Training: Epoch [59/120], Step [4999], Loss: 0.07374389469623566, Training Accuracy: 90.53999999999999
[ Wed Jul  3 21:19:55 2024 ] 	Batch(5000/7879) done. Loss: 0.1432  lr:0.010000
[ Wed Jul  3 21:20:12 2024 ] 	Batch(5100/7879) done. Loss: 0.4262  lr:0.010000
[ Wed Jul  3 21:20:31 2024 ] 	Batch(5200/7879) done. Loss: 0.1143  lr:0.010000
[ Wed Jul  3 21:20:48 2024 ] 	Batch(5300/7879) done. Loss: 0.0100  lr:0.010000
[ Wed Jul  3 21:21:06 2024 ] 	Batch(5400/7879) done. Loss: 0.0073  lr:0.010000
[ Wed Jul  3 21:21:24 2024 ] 
Training: Epoch [59/120], Step [5499], Loss: 0.7143470048904419, Training Accuracy: 90.39090909090909
[ Wed Jul  3 21:21:24 2024 ] 	Batch(5500/7879) done. Loss: 0.3358  lr:0.010000
[ Wed Jul  3 21:21:42 2024 ] 	Batch(5600/7879) done. Loss: 0.0709  lr:0.010000
[ Wed Jul  3 21:22:01 2024 ] 	Batch(5700/7879) done. Loss: 0.5240  lr:0.010000
[ Wed Jul  3 21:22:19 2024 ] 	Batch(5800/7879) done. Loss: 0.2603  lr:0.010000
[ Wed Jul  3 21:22:38 2024 ] 	Batch(5900/7879) done. Loss: 0.6561  lr:0.010000
[ Wed Jul  3 21:22:56 2024 ] 
Training: Epoch [59/120], Step [5999], Loss: 0.052883539348840714, Training Accuracy: 90.35416666666667
[ Wed Jul  3 21:22:57 2024 ] 	Batch(6000/7879) done. Loss: 0.0808  lr:0.010000
[ Wed Jul  3 21:23:15 2024 ] 	Batch(6100/7879) done. Loss: 0.0519  lr:0.010000
[ Wed Jul  3 21:23:33 2024 ] 	Batch(6200/7879) done. Loss: 1.0618  lr:0.010000
[ Wed Jul  3 21:23:51 2024 ] 	Batch(6300/7879) done. Loss: 0.5190  lr:0.010000
[ Wed Jul  3 21:24:09 2024 ] 	Batch(6400/7879) done. Loss: 0.1616  lr:0.010000
[ Wed Jul  3 21:24:26 2024 ] 
Training: Epoch [59/120], Step [6499], Loss: 0.2304254025220871, Training Accuracy: 90.24423076923077
[ Wed Jul  3 21:24:27 2024 ] 	Batch(6500/7879) done. Loss: 0.8523  lr:0.010000
[ Wed Jul  3 21:24:44 2024 ] 	Batch(6600/7879) done. Loss: 0.4177  lr:0.010000
[ Wed Jul  3 21:25:02 2024 ] 	Batch(6700/7879) done. Loss: 0.4828  lr:0.010000
[ Wed Jul  3 21:25:20 2024 ] 	Batch(6800/7879) done. Loss: 0.7080  lr:0.010000
[ Wed Jul  3 21:25:38 2024 ] 	Batch(6900/7879) done. Loss: 0.1978  lr:0.010000
[ Wed Jul  3 21:25:56 2024 ] 
Training: Epoch [59/120], Step [6999], Loss: 0.13330310583114624, Training Accuracy: 90.20714285714286
[ Wed Jul  3 21:25:56 2024 ] 	Batch(7000/7879) done. Loss: 0.8933  lr:0.010000
[ Wed Jul  3 21:26:14 2024 ] 	Batch(7100/7879) done. Loss: 0.9321  lr:0.010000
[ Wed Jul  3 21:26:32 2024 ] 	Batch(7200/7879) done. Loss: 0.1343  lr:0.010000
[ Wed Jul  3 21:26:50 2024 ] 	Batch(7300/7879) done. Loss: 0.5150  lr:0.010000
[ Wed Jul  3 21:27:08 2024 ] 	Batch(7400/7879) done. Loss: 0.7524  lr:0.010000
[ Wed Jul  3 21:27:26 2024 ] 
Training: Epoch [59/120], Step [7499], Loss: 0.23348368704319, Training Accuracy: 90.18666666666667
[ Wed Jul  3 21:27:26 2024 ] 	Batch(7500/7879) done. Loss: 0.3340  lr:0.010000
[ Wed Jul  3 21:27:44 2024 ] 	Batch(7600/7879) done. Loss: 0.1575  lr:0.010000
[ Wed Jul  3 21:28:02 2024 ] 	Batch(7700/7879) done. Loss: 0.8655  lr:0.010000
[ Wed Jul  3 21:28:19 2024 ] 	Batch(7800/7879) done. Loss: 0.0661  lr:0.010000
[ Wed Jul  3 21:28:33 2024 ] 	Mean training loss: 0.3165.
[ Wed Jul  3 21:28:33 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 21:28:34 2024 ] Eval epoch: 60
[ Wed Jul  3 21:33:19 2024 ] 	Mean val loss of 6365 batches: 1.7749002437584462.
[ Wed Jul  3 21:33:19 2024 ] Training epoch: 61
[ Wed Jul  3 21:33:20 2024 ] 	Batch(0/7879) done. Loss: 0.8248  lr:0.000100
[ Wed Jul  3 21:33:38 2024 ] 	Batch(100/7879) done. Loss: 0.5116  lr:0.000100
[ Wed Jul  3 21:33:56 2024 ] 	Batch(200/7879) done. Loss: 0.0629  lr:0.000100
[ Wed Jul  3 21:34:14 2024 ] 	Batch(300/7879) done. Loss: 0.0253  lr:0.000100
[ Wed Jul  3 21:34:32 2024 ] 	Batch(400/7879) done. Loss: 0.9500  lr:0.000100
[ Wed Jul  3 21:34:49 2024 ] 
Training: Epoch [60/120], Step [499], Loss: 0.4761766791343689, Training Accuracy: 91.375
[ Wed Jul  3 21:34:49 2024 ] 	Batch(500/7879) done. Loss: 0.1356  lr:0.000100
[ Wed Jul  3 21:35:07 2024 ] 	Batch(600/7879) done. Loss: 0.1406  lr:0.000100
[ Wed Jul  3 21:35:25 2024 ] 	Batch(700/7879) done. Loss: 0.4324  lr:0.000100
[ Wed Jul  3 21:35:43 2024 ] 	Batch(800/7879) done. Loss: 0.1553  lr:0.000100
[ Wed Jul  3 21:36:01 2024 ] 	Batch(900/7879) done. Loss: 0.0019  lr:0.000100
[ Wed Jul  3 21:36:19 2024 ] 
Training: Epoch [60/120], Step [999], Loss: 0.2113889455795288, Training Accuracy: 91.925
[ Wed Jul  3 21:36:19 2024 ] 	Batch(1000/7879) done. Loss: 0.0412  lr:0.000100
[ Wed Jul  3 21:36:37 2024 ] 	Batch(1100/7879) done. Loss: 0.4173  lr:0.000100
[ Wed Jul  3 21:36:55 2024 ] 	Batch(1200/7879) done. Loss: 0.6172  lr:0.000100
[ Wed Jul  3 21:37:13 2024 ] 	Batch(1300/7879) done. Loss: 0.4385  lr:0.000100
[ Wed Jul  3 21:37:31 2024 ] 	Batch(1400/7879) done. Loss: 0.0652  lr:0.000100
[ Wed Jul  3 21:37:48 2024 ] 
Training: Epoch [60/120], Step [1499], Loss: 0.3355252742767334, Training Accuracy: 92.14166666666667
[ Wed Jul  3 21:37:48 2024 ] 	Batch(1500/7879) done. Loss: 0.2166  lr:0.000100
[ Wed Jul  3 21:38:06 2024 ] 	Batch(1600/7879) done. Loss: 0.4798  lr:0.000100
[ Wed Jul  3 21:38:24 2024 ] 	Batch(1700/7879) done. Loss: 0.0052  lr:0.000100
[ Wed Jul  3 21:38:42 2024 ] 	Batch(1800/7879) done. Loss: 0.0499  lr:0.000100
[ Wed Jul  3 21:39:00 2024 ] 	Batch(1900/7879) done. Loss: 0.2177  lr:0.000100
[ Wed Jul  3 21:39:18 2024 ] 
Training: Epoch [60/120], Step [1999], Loss: 0.5695825219154358, Training Accuracy: 92.45625
[ Wed Jul  3 21:39:18 2024 ] 	Batch(2000/7879) done. Loss: 0.3050  lr:0.000100
[ Wed Jul  3 21:39:36 2024 ] 	Batch(2100/7879) done. Loss: 0.1352  lr:0.000100
[ Wed Jul  3 21:39:54 2024 ] 	Batch(2200/7879) done. Loss: 0.0836  lr:0.000100
[ Wed Jul  3 21:40:12 2024 ] 	Batch(2300/7879) done. Loss: 0.3798  lr:0.000100
[ Wed Jul  3 21:40:30 2024 ] 	Batch(2400/7879) done. Loss: 0.4520  lr:0.000100
[ Wed Jul  3 21:40:48 2024 ] 
Training: Epoch [60/120], Step [2499], Loss: 0.8668926358222961, Training Accuracy: 92.595
[ Wed Jul  3 21:40:48 2024 ] 	Batch(2500/7879) done. Loss: 0.0874  lr:0.000100
[ Wed Jul  3 21:41:06 2024 ] 	Batch(2600/7879) done. Loss: 0.1448  lr:0.000100
[ Wed Jul  3 21:41:24 2024 ] 	Batch(2700/7879) done. Loss: 0.2352  lr:0.000100
[ Wed Jul  3 21:41:42 2024 ] 	Batch(2800/7879) done. Loss: 0.1819  lr:0.000100
[ Wed Jul  3 21:42:00 2024 ] 	Batch(2900/7879) done. Loss: 0.2026  lr:0.000100
[ Wed Jul  3 21:42:18 2024 ] 
Training: Epoch [60/120], Step [2999], Loss: 0.2927670478820801, Training Accuracy: 92.83749999999999
[ Wed Jul  3 21:42:18 2024 ] 	Batch(3000/7879) done. Loss: 0.2002  lr:0.000100
[ Wed Jul  3 21:42:36 2024 ] 	Batch(3100/7879) done. Loss: 0.0861  lr:0.000100
[ Wed Jul  3 21:42:54 2024 ] 	Batch(3200/7879) done. Loss: 0.1698  lr:0.000100
[ Wed Jul  3 21:43:12 2024 ] 	Batch(3300/7879) done. Loss: 0.5432  lr:0.000100
[ Wed Jul  3 21:43:31 2024 ] 	Batch(3400/7879) done. Loss: 0.0660  lr:0.000100
[ Wed Jul  3 21:43:49 2024 ] 
Training: Epoch [60/120], Step [3499], Loss: 0.079278863966465, Training Accuracy: 92.96428571428571
[ Wed Jul  3 21:43:49 2024 ] 	Batch(3500/7879) done. Loss: 0.0975  lr:0.000100
[ Wed Jul  3 21:44:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0873  lr:0.000100
[ Wed Jul  3 21:44:26 2024 ] 	Batch(3700/7879) done. Loss: 0.1191  lr:0.000100
[ Wed Jul  3 21:44:45 2024 ] 	Batch(3800/7879) done. Loss: 0.0419  lr:0.000100
[ Wed Jul  3 21:45:03 2024 ] 	Batch(3900/7879) done. Loss: 0.2986  lr:0.000100
[ Wed Jul  3 21:45:22 2024 ] 
Training: Epoch [60/120], Step [3999], Loss: 0.11924263834953308, Training Accuracy: 93.153125
[ Wed Jul  3 21:45:22 2024 ] 	Batch(4000/7879) done. Loss: 0.0838  lr:0.000100
[ Wed Jul  3 21:45:40 2024 ] 	Batch(4100/7879) done. Loss: 0.1020  lr:0.000100
[ Wed Jul  3 21:45:58 2024 ] 	Batch(4200/7879) done. Loss: 0.0599  lr:0.000100
[ Wed Jul  3 21:46:16 2024 ] 	Batch(4300/7879) done. Loss: 0.1871  lr:0.000100
[ Wed Jul  3 21:46:34 2024 ] 	Batch(4400/7879) done. Loss: 0.4483  lr:0.000100
[ Wed Jul  3 21:46:51 2024 ] 
Training: Epoch [60/120], Step [4499], Loss: 0.07646522670984268, Training Accuracy: 93.31388888888888
[ Wed Jul  3 21:46:52 2024 ] 	Batch(4500/7879) done. Loss: 0.4762  lr:0.000100
[ Wed Jul  3 21:47:10 2024 ] 	Batch(4600/7879) done. Loss: 0.0813  lr:0.000100
[ Wed Jul  3 21:47:27 2024 ] 	Batch(4700/7879) done. Loss: 0.1409  lr:0.000100
[ Wed Jul  3 21:47:45 2024 ] 	Batch(4800/7879) done. Loss: 0.3020  lr:0.000100
[ Wed Jul  3 21:48:03 2024 ] 	Batch(4900/7879) done. Loss: 0.0424  lr:0.000100
[ Wed Jul  3 21:48:21 2024 ] 
Training: Epoch [60/120], Step [4999], Loss: 0.20436562597751617, Training Accuracy: 93.31
[ Wed Jul  3 21:48:21 2024 ] 	Batch(5000/7879) done. Loss: 0.0441  lr:0.000100
[ Wed Jul  3 21:48:39 2024 ] 	Batch(5100/7879) done. Loss: 0.1250  lr:0.000100
[ Wed Jul  3 21:48:57 2024 ] 	Batch(5200/7879) done. Loss: 0.0307  lr:0.000100
[ Wed Jul  3 21:49:15 2024 ] 	Batch(5300/7879) done. Loss: 0.1115  lr:0.000100
[ Wed Jul  3 21:49:33 2024 ] 	Batch(5400/7879) done. Loss: 0.3364  lr:0.000100
[ Wed Jul  3 21:49:51 2024 ] 
Training: Epoch [60/120], Step [5499], Loss: 0.13735543191432953, Training Accuracy: 93.40227272727273
[ Wed Jul  3 21:49:51 2024 ] 	Batch(5500/7879) done. Loss: 0.1778  lr:0.000100
[ Wed Jul  3 21:50:09 2024 ] 	Batch(5600/7879) done. Loss: 0.1591  lr:0.000100
[ Wed Jul  3 21:50:27 2024 ] 	Batch(5700/7879) done. Loss: 0.5358  lr:0.000100
[ Wed Jul  3 21:50:45 2024 ] 	Batch(5800/7879) done. Loss: 0.8324  lr:0.000100
[ Wed Jul  3 21:51:04 2024 ] 	Batch(5900/7879) done. Loss: 0.0968  lr:0.000100
[ Wed Jul  3 21:51:22 2024 ] 
Training: Epoch [60/120], Step [5999], Loss: 0.14200448989868164, Training Accuracy: 93.46458333333332
[ Wed Jul  3 21:51:23 2024 ] 	Batch(6000/7879) done. Loss: 0.0278  lr:0.000100
[ Wed Jul  3 21:51:41 2024 ] 	Batch(6100/7879) done. Loss: 0.2156  lr:0.000100
[ Wed Jul  3 21:52:00 2024 ] 	Batch(6200/7879) done. Loss: 0.0057  lr:0.000100
[ Wed Jul  3 21:52:19 2024 ] 	Batch(6300/7879) done. Loss: 0.0236  lr:0.000100
[ Wed Jul  3 21:52:38 2024 ] 	Batch(6400/7879) done. Loss: 0.0219  lr:0.000100
[ Wed Jul  3 21:52:56 2024 ] 
Training: Epoch [60/120], Step [6499], Loss: 0.2137116640806198, Training Accuracy: 93.5173076923077
[ Wed Jul  3 21:52:56 2024 ] 	Batch(6500/7879) done. Loss: 0.3627  lr:0.000100
[ Wed Jul  3 21:53:15 2024 ] 	Batch(6600/7879) done. Loss: 0.3596  lr:0.000100
[ Wed Jul  3 21:53:33 2024 ] 	Batch(6700/7879) done. Loss: 0.1099  lr:0.000100
[ Wed Jul  3 21:53:52 2024 ] 	Batch(6800/7879) done. Loss: 0.0372  lr:0.000100
[ Wed Jul  3 21:54:10 2024 ] 	Batch(6900/7879) done. Loss: 0.0730  lr:0.000100
[ Wed Jul  3 21:54:29 2024 ] 
Training: Epoch [60/120], Step [6999], Loss: 0.09149615466594696, Training Accuracy: 93.61785714285715
[ Wed Jul  3 21:54:29 2024 ] 	Batch(7000/7879) done. Loss: 0.2786  lr:0.000100
[ Wed Jul  3 21:54:47 2024 ] 	Batch(7100/7879) done. Loss: 0.0862  lr:0.000100
[ Wed Jul  3 21:55:05 2024 ] 	Batch(7200/7879) done. Loss: 0.0875  lr:0.000100
[ Wed Jul  3 21:55:23 2024 ] 	Batch(7300/7879) done. Loss: 0.2468  lr:0.000100
[ Wed Jul  3 21:55:41 2024 ] 	Batch(7400/7879) done. Loss: 0.1161  lr:0.000100
[ Wed Jul  3 21:55:58 2024 ] 
Training: Epoch [60/120], Step [7499], Loss: 0.7500040531158447, Training Accuracy: 93.69666666666666
[ Wed Jul  3 21:55:59 2024 ] 	Batch(7500/7879) done. Loss: 0.4660  lr:0.000100
[ Wed Jul  3 21:56:17 2024 ] 	Batch(7600/7879) done. Loss: 0.2389  lr:0.000100
[ Wed Jul  3 21:56:34 2024 ] 	Batch(7700/7879) done. Loss: 0.1665  lr:0.000100
[ Wed Jul  3 21:56:52 2024 ] 	Batch(7800/7879) done. Loss: 0.1084  lr:0.000100
[ Wed Jul  3 21:57:06 2024 ] 	Mean training loss: 0.2143.
[ Wed Jul  3 21:57:06 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 21:57:06 2024 ] Training epoch: 62
[ Wed Jul  3 21:57:07 2024 ] 	Batch(0/7879) done. Loss: 0.1779  lr:0.000100
[ Wed Jul  3 21:57:25 2024 ] 	Batch(100/7879) done. Loss: 0.3296  lr:0.000100
[ Wed Jul  3 21:57:43 2024 ] 	Batch(200/7879) done. Loss: 0.1061  lr:0.000100
[ Wed Jul  3 21:58:00 2024 ] 	Batch(300/7879) done. Loss: 0.2160  lr:0.000100
[ Wed Jul  3 21:58:18 2024 ] 	Batch(400/7879) done. Loss: 0.0990  lr:0.000100
[ Wed Jul  3 21:58:36 2024 ] 
Training: Epoch [61/120], Step [499], Loss: 0.02013360895216465, Training Accuracy: 94.675
[ Wed Jul  3 21:58:36 2024 ] 	Batch(500/7879) done. Loss: 0.1163  lr:0.000100
[ Wed Jul  3 21:58:54 2024 ] 	Batch(600/7879) done. Loss: 0.3807  lr:0.000100
[ Wed Jul  3 21:59:12 2024 ] 	Batch(700/7879) done. Loss: 0.4120  lr:0.000100
[ Wed Jul  3 21:59:30 2024 ] 	Batch(800/7879) done. Loss: 0.0381  lr:0.000100
[ Wed Jul  3 21:59:48 2024 ] 	Batch(900/7879) done. Loss: 0.0677  lr:0.000100
[ Wed Jul  3 22:00:06 2024 ] 
Training: Epoch [61/120], Step [999], Loss: 0.3598722517490387, Training Accuracy: 94.875
[ Wed Jul  3 22:00:06 2024 ] 	Batch(1000/7879) done. Loss: 0.0390  lr:0.000100
[ Wed Jul  3 22:00:24 2024 ] 	Batch(1100/7879) done. Loss: 0.0522  lr:0.000100
[ Wed Jul  3 22:00:42 2024 ] 	Batch(1200/7879) done. Loss: 0.4674  lr:0.000100
[ Wed Jul  3 22:01:00 2024 ] 	Batch(1300/7879) done. Loss: 0.2335  lr:0.000100
[ Wed Jul  3 22:01:19 2024 ] 	Batch(1400/7879) done. Loss: 0.0706  lr:0.000100
[ Wed Jul  3 22:01:37 2024 ] 
Training: Epoch [61/120], Step [1499], Loss: 0.6023310422897339, Training Accuracy: 94.94166666666666
[ Wed Jul  3 22:01:37 2024 ] 	Batch(1500/7879) done. Loss: 0.0935  lr:0.000100
[ Wed Jul  3 22:01:56 2024 ] 	Batch(1600/7879) done. Loss: 0.0031  lr:0.000100
[ Wed Jul  3 22:02:14 2024 ] 	Batch(1700/7879) done. Loss: 0.0319  lr:0.000100
[ Wed Jul  3 22:02:33 2024 ] 	Batch(1800/7879) done. Loss: 0.2711  lr:0.000100
[ Wed Jul  3 22:02:51 2024 ] 	Batch(1900/7879) done. Loss: 0.5708  lr:0.000100
[ Wed Jul  3 22:03:09 2024 ] 
Training: Epoch [61/120], Step [1999], Loss: 0.26125386357307434, Training Accuracy: 94.8875
[ Wed Jul  3 22:03:09 2024 ] 	Batch(2000/7879) done. Loss: 0.1963  lr:0.000100
[ Wed Jul  3 22:03:27 2024 ] 	Batch(2100/7879) done. Loss: 0.2130  lr:0.000100
[ Wed Jul  3 22:03:45 2024 ] 	Batch(2200/7879) done. Loss: 0.2197  lr:0.000100
[ Wed Jul  3 22:04:02 2024 ] 	Batch(2300/7879) done. Loss: 0.1122  lr:0.000100
[ Wed Jul  3 22:04:21 2024 ] 	Batch(2400/7879) done. Loss: 0.3360  lr:0.000100
[ Wed Jul  3 22:04:38 2024 ] 
Training: Epoch [61/120], Step [2499], Loss: 0.08892799913883209, Training Accuracy: 94.87
[ Wed Jul  3 22:04:38 2024 ] 	Batch(2500/7879) done. Loss: 0.0660  lr:0.000100
[ Wed Jul  3 22:04:56 2024 ] 	Batch(2600/7879) done. Loss: 0.1631  lr:0.000100
[ Wed Jul  3 22:05:14 2024 ] 	Batch(2700/7879) done. Loss: 0.0423  lr:0.000100
[ Wed Jul  3 22:05:32 2024 ] 	Batch(2800/7879) done. Loss: 0.0130  lr:0.000100
[ Wed Jul  3 22:05:50 2024 ] 	Batch(2900/7879) done. Loss: 0.0658  lr:0.000100
[ Wed Jul  3 22:06:08 2024 ] 
Training: Epoch [61/120], Step [2999], Loss: 0.016899028792977333, Training Accuracy: 94.8875
[ Wed Jul  3 22:06:08 2024 ] 	Batch(3000/7879) done. Loss: 0.0106  lr:0.000100
[ Wed Jul  3 22:06:26 2024 ] 	Batch(3100/7879) done. Loss: 0.0416  lr:0.000100
[ Wed Jul  3 22:06:44 2024 ] 	Batch(3200/7879) done. Loss: 0.1794  lr:0.000100
[ Wed Jul  3 22:07:02 2024 ] 	Batch(3300/7879) done. Loss: 0.0321  lr:0.000100
[ Wed Jul  3 22:07:21 2024 ] 	Batch(3400/7879) done. Loss: 0.0535  lr:0.000100
[ Wed Jul  3 22:07:39 2024 ] 
Training: Epoch [61/120], Step [3499], Loss: 0.006741172168403864, Training Accuracy: 94.91785714285714
[ Wed Jul  3 22:07:39 2024 ] 	Batch(3500/7879) done. Loss: 0.1773  lr:0.000100
[ Wed Jul  3 22:07:57 2024 ] 	Batch(3600/7879) done. Loss: 0.1532  lr:0.000100
[ Wed Jul  3 22:08:15 2024 ] 	Batch(3700/7879) done. Loss: 0.0643  lr:0.000100
[ Wed Jul  3 22:08:33 2024 ] 	Batch(3800/7879) done. Loss: 0.1850  lr:0.000100
[ Wed Jul  3 22:08:50 2024 ] 	Batch(3900/7879) done. Loss: 0.0345  lr:0.000100
[ Wed Jul  3 22:09:08 2024 ] 
Training: Epoch [61/120], Step [3999], Loss: 0.015146493911743164, Training Accuracy: 94.971875
[ Wed Jul  3 22:09:08 2024 ] 	Batch(4000/7879) done. Loss: 0.2864  lr:0.000100
[ Wed Jul  3 22:09:26 2024 ] 	Batch(4100/7879) done. Loss: 0.1429  lr:0.000100
[ Wed Jul  3 22:09:44 2024 ] 	Batch(4200/7879) done. Loss: 0.2528  lr:0.000100
[ Wed Jul  3 22:10:02 2024 ] 	Batch(4300/7879) done. Loss: 0.4082  lr:0.000100
[ Wed Jul  3 22:10:20 2024 ] 	Batch(4400/7879) done. Loss: 0.0312  lr:0.000100
[ Wed Jul  3 22:10:38 2024 ] 
Training: Epoch [61/120], Step [4499], Loss: 0.043097566813230515, Training Accuracy: 95.01111111111112
[ Wed Jul  3 22:10:38 2024 ] 	Batch(4500/7879) done. Loss: 0.0174  lr:0.000100
[ Wed Jul  3 22:10:56 2024 ] 	Batch(4600/7879) done. Loss: 0.1280  lr:0.000100
[ Wed Jul  3 22:11:14 2024 ] 	Batch(4700/7879) done. Loss: 0.0237  lr:0.000100
[ Wed Jul  3 22:11:32 2024 ] 	Batch(4800/7879) done. Loss: 0.0880  lr:0.000100
[ Wed Jul  3 22:11:50 2024 ] 	Batch(4900/7879) done. Loss: 0.3251  lr:0.000100
[ Wed Jul  3 22:12:07 2024 ] 
Training: Epoch [61/120], Step [4999], Loss: 0.13209974765777588, Training Accuracy: 95.075
[ Wed Jul  3 22:12:07 2024 ] 	Batch(5000/7879) done. Loss: 0.1614  lr:0.000100
[ Wed Jul  3 22:12:25 2024 ] 	Batch(5100/7879) done. Loss: 0.0749  lr:0.000100
[ Wed Jul  3 22:12:43 2024 ] 	Batch(5200/7879) done. Loss: 0.1755  lr:0.000100
[ Wed Jul  3 22:13:01 2024 ] 	Batch(5300/7879) done. Loss: 0.0170  lr:0.000100
[ Wed Jul  3 22:13:19 2024 ] 	Batch(5400/7879) done. Loss: 0.0360  lr:0.000100
[ Wed Jul  3 22:13:37 2024 ] 
Training: Epoch [61/120], Step [5499], Loss: 0.0037031290121376514, Training Accuracy: 95.05454545454546
[ Wed Jul  3 22:13:37 2024 ] 	Batch(5500/7879) done. Loss: 0.0346  lr:0.000100
[ Wed Jul  3 22:13:55 2024 ] 	Batch(5600/7879) done. Loss: 0.1129  lr:0.000100
[ Wed Jul  3 22:14:13 2024 ] 	Batch(5700/7879) done. Loss: 0.0294  lr:0.000100
[ Wed Jul  3 22:14:31 2024 ] 	Batch(5800/7879) done. Loss: 0.0830  lr:0.000100
[ Wed Jul  3 22:14:49 2024 ] 	Batch(5900/7879) done. Loss: 0.1786  lr:0.000100
[ Wed Jul  3 22:15:07 2024 ] 
Training: Epoch [61/120], Step [5999], Loss: 0.014051681384444237, Training Accuracy: 95.1
[ Wed Jul  3 22:15:07 2024 ] 	Batch(6000/7879) done. Loss: 0.6114  lr:0.000100
[ Wed Jul  3 22:15:25 2024 ] 	Batch(6100/7879) done. Loss: 0.1757  lr:0.000100
[ Wed Jul  3 22:15:43 2024 ] 	Batch(6200/7879) done. Loss: 0.2971  lr:0.000100
[ Wed Jul  3 22:16:01 2024 ] 	Batch(6300/7879) done. Loss: 0.5742  lr:0.000100
[ Wed Jul  3 22:16:19 2024 ] 	Batch(6400/7879) done. Loss: 0.0702  lr:0.000100
[ Wed Jul  3 22:16:36 2024 ] 
Training: Epoch [61/120], Step [6499], Loss: 1.132429838180542, Training Accuracy: 95.11730769230769
[ Wed Jul  3 22:16:36 2024 ] 	Batch(6500/7879) done. Loss: 0.2362  lr:0.000100
[ Wed Jul  3 22:16:54 2024 ] 	Batch(6600/7879) done. Loss: 0.0143  lr:0.000100
[ Wed Jul  3 22:17:12 2024 ] 	Batch(6700/7879) done. Loss: 0.0099  lr:0.000100
[ Wed Jul  3 22:17:30 2024 ] 	Batch(6800/7879) done. Loss: 0.1630  lr:0.000100
[ Wed Jul  3 22:17:48 2024 ] 	Batch(6900/7879) done. Loss: 0.3555  lr:0.000100
[ Wed Jul  3 22:18:06 2024 ] 
Training: Epoch [61/120], Step [6999], Loss: 0.010698501951992512, Training Accuracy: 95.14821428571429
[ Wed Jul  3 22:18:06 2024 ] 	Batch(7000/7879) done. Loss: 0.1036  lr:0.000100
[ Wed Jul  3 22:18:24 2024 ] 	Batch(7100/7879) done. Loss: 0.1553  lr:0.000100
[ Wed Jul  3 22:18:42 2024 ] 	Batch(7200/7879) done. Loss: 0.1791  lr:0.000100
[ Wed Jul  3 22:19:00 2024 ] 	Batch(7300/7879) done. Loss: 0.0766  lr:0.000100
[ Wed Jul  3 22:19:19 2024 ] 	Batch(7400/7879) done. Loss: 0.0463  lr:0.000100
[ Wed Jul  3 22:19:36 2024 ] 
Training: Epoch [61/120], Step [7499], Loss: 0.018194355070590973, Training Accuracy: 95.16166666666666
[ Wed Jul  3 22:19:36 2024 ] 	Batch(7500/7879) done. Loss: 0.0175  lr:0.000100
[ Wed Jul  3 22:19:54 2024 ] 	Batch(7600/7879) done. Loss: 0.0829  lr:0.000100
[ Wed Jul  3 22:20:12 2024 ] 	Batch(7700/7879) done. Loss: 0.0501  lr:0.000100
[ Wed Jul  3 22:20:30 2024 ] 	Batch(7800/7879) done. Loss: 0.1832  lr:0.000100
[ Wed Jul  3 22:20:44 2024 ] 	Mean training loss: 0.1741.
[ Wed Jul  3 22:20:44 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 22:20:44 2024 ] Training epoch: 63
[ Wed Jul  3 22:20:45 2024 ] 	Batch(0/7879) done. Loss: 0.0312  lr:0.000100
[ Wed Jul  3 22:21:03 2024 ] 	Batch(100/7879) done. Loss: 0.0408  lr:0.000100
[ Wed Jul  3 22:21:21 2024 ] 	Batch(200/7879) done. Loss: 0.2341  lr:0.000100
[ Wed Jul  3 22:21:39 2024 ] 	Batch(300/7879) done. Loss: 0.0652  lr:0.000100
[ Wed Jul  3 22:21:57 2024 ] 	Batch(400/7879) done. Loss: 0.1602  lr:0.000100
[ Wed Jul  3 22:22:14 2024 ] 
Training: Epoch [62/120], Step [499], Loss: 0.09225212782621384, Training Accuracy: 95.25
[ Wed Jul  3 22:22:15 2024 ] 	Batch(500/7879) done. Loss: 0.1210  lr:0.000100
[ Wed Jul  3 22:22:32 2024 ] 	Batch(600/7879) done. Loss: 0.2522  lr:0.000100
[ Wed Jul  3 22:22:50 2024 ] 	Batch(700/7879) done. Loss: 0.2136  lr:0.000100
[ Wed Jul  3 22:23:08 2024 ] 	Batch(800/7879) done. Loss: 0.0211  lr:0.000100
[ Wed Jul  3 22:23:26 2024 ] 	Batch(900/7879) done. Loss: 0.1506  lr:0.000100
[ Wed Jul  3 22:23:44 2024 ] 
Training: Epoch [62/120], Step [999], Loss: 0.06411071121692657, Training Accuracy: 95.35
[ Wed Jul  3 22:23:44 2024 ] 	Batch(1000/7879) done. Loss: 0.0459  lr:0.000100
[ Wed Jul  3 22:24:02 2024 ] 	Batch(1100/7879) done. Loss: 0.1293  lr:0.000100
[ Wed Jul  3 22:24:20 2024 ] 	Batch(1200/7879) done. Loss: 0.0298  lr:0.000100
[ Wed Jul  3 22:24:38 2024 ] 	Batch(1300/7879) done. Loss: 0.0323  lr:0.000100
[ Wed Jul  3 22:24:56 2024 ] 	Batch(1400/7879) done. Loss: 0.0332  lr:0.000100
[ Wed Jul  3 22:25:14 2024 ] 
Training: Epoch [62/120], Step [1499], Loss: 0.03717080131173134, Training Accuracy: 95.48333333333333
[ Wed Jul  3 22:25:14 2024 ] 	Batch(1500/7879) done. Loss: 0.3259  lr:0.000100
[ Wed Jul  3 22:25:32 2024 ] 	Batch(1600/7879) done. Loss: 0.0599  lr:0.000100
[ Wed Jul  3 22:25:50 2024 ] 	Batch(1700/7879) done. Loss: 0.0061  lr:0.000100
[ Wed Jul  3 22:26:08 2024 ] 	Batch(1800/7879) done. Loss: 0.0765  lr:0.000100
[ Wed Jul  3 22:26:25 2024 ] 	Batch(1900/7879) done. Loss: 0.1006  lr:0.000100
[ Wed Jul  3 22:26:43 2024 ] 
Training: Epoch [62/120], Step [1999], Loss: 0.0211701150983572, Training Accuracy: 95.48124999999999
[ Wed Jul  3 22:26:43 2024 ] 	Batch(2000/7879) done. Loss: 0.0576  lr:0.000100
[ Wed Jul  3 22:27:01 2024 ] 	Batch(2100/7879) done. Loss: 0.0924  lr:0.000100
[ Wed Jul  3 22:27:19 2024 ] 	Batch(2200/7879) done. Loss: 0.0462  lr:0.000100
[ Wed Jul  3 22:27:37 2024 ] 	Batch(2300/7879) done. Loss: 0.0117  lr:0.000100
[ Wed Jul  3 22:27:55 2024 ] 	Batch(2400/7879) done. Loss: 0.0724  lr:0.000100
[ Wed Jul  3 22:28:14 2024 ] 
Training: Epoch [62/120], Step [2499], Loss: 0.0534396767616272, Training Accuracy: 95.53
[ Wed Jul  3 22:28:14 2024 ] 	Batch(2500/7879) done. Loss: 0.0406  lr:0.000100
[ Wed Jul  3 22:28:32 2024 ] 	Batch(2600/7879) done. Loss: 0.2820  lr:0.000100
[ Wed Jul  3 22:28:50 2024 ] 	Batch(2700/7879) done. Loss: 0.0489  lr:0.000100
[ Wed Jul  3 22:29:08 2024 ] 	Batch(2800/7879) done. Loss: 0.0167  lr:0.000100
[ Wed Jul  3 22:29:27 2024 ] 	Batch(2900/7879) done. Loss: 0.2013  lr:0.000100
[ Wed Jul  3 22:29:45 2024 ] 
Training: Epoch [62/120], Step [2999], Loss: 0.3144959807395935, Training Accuracy: 95.57916666666667
[ Wed Jul  3 22:29:45 2024 ] 	Batch(3000/7879) done. Loss: 0.4394  lr:0.000100
[ Wed Jul  3 22:30:03 2024 ] 	Batch(3100/7879) done. Loss: 0.0267  lr:0.000100
[ Wed Jul  3 22:30:21 2024 ] 	Batch(3200/7879) done. Loss: 0.0090  lr:0.000100
[ Wed Jul  3 22:30:39 2024 ] 	Batch(3300/7879) done. Loss: 0.2003  lr:0.000100
[ Wed Jul  3 22:30:57 2024 ] 	Batch(3400/7879) done. Loss: 0.0051  lr:0.000100
[ Wed Jul  3 22:31:15 2024 ] 
Training: Epoch [62/120], Step [3499], Loss: 0.05281547084450722, Training Accuracy: 95.575
[ Wed Jul  3 22:31:15 2024 ] 	Batch(3500/7879) done. Loss: 0.0382  lr:0.000100
[ Wed Jul  3 22:31:33 2024 ] 	Batch(3600/7879) done. Loss: 0.1704  lr:0.000100
[ Wed Jul  3 22:31:51 2024 ] 	Batch(3700/7879) done. Loss: 0.2207  lr:0.000100
[ Wed Jul  3 22:32:09 2024 ] 	Batch(3800/7879) done. Loss: 0.0378  lr:0.000100
[ Wed Jul  3 22:32:27 2024 ] 	Batch(3900/7879) done. Loss: 0.2322  lr:0.000100
[ Wed Jul  3 22:32:45 2024 ] 
Training: Epoch [62/120], Step [3999], Loss: 0.13297531008720398, Training Accuracy: 95.61562500000001
[ Wed Jul  3 22:32:45 2024 ] 	Batch(4000/7879) done. Loss: 0.0446  lr:0.000100
[ Wed Jul  3 22:33:03 2024 ] 	Batch(4100/7879) done. Loss: 0.0400  lr:0.000100
[ Wed Jul  3 22:33:22 2024 ] 	Batch(4200/7879) done. Loss: 0.1082  lr:0.000100
[ Wed Jul  3 22:33:40 2024 ] 	Batch(4300/7879) done. Loss: 0.0346  lr:0.000100
[ Wed Jul  3 22:33:59 2024 ] 	Batch(4400/7879) done. Loss: 0.3942  lr:0.000100
[ Wed Jul  3 22:34:17 2024 ] 
Training: Epoch [62/120], Step [4499], Loss: 0.10466084629297256, Training Accuracy: 95.61111111111111
[ Wed Jul  3 22:34:17 2024 ] 	Batch(4500/7879) done. Loss: 0.2945  lr:0.000100
[ Wed Jul  3 22:34:35 2024 ] 	Batch(4600/7879) done. Loss: 0.0389  lr:0.000100
[ Wed Jul  3 22:34:53 2024 ] 	Batch(4700/7879) done. Loss: 0.0780  lr:0.000100
[ Wed Jul  3 22:35:11 2024 ] 	Batch(4800/7879) done. Loss: 0.0333  lr:0.000100
[ Wed Jul  3 22:35:29 2024 ] 	Batch(4900/7879) done. Loss: 0.0294  lr:0.000100
[ Wed Jul  3 22:35:48 2024 ] 
Training: Epoch [62/120], Step [4999], Loss: 0.05812269076704979, Training Accuracy: 95.5775
[ Wed Jul  3 22:35:48 2024 ] 	Batch(5000/7879) done. Loss: 0.0416  lr:0.000100
[ Wed Jul  3 22:36:06 2024 ] 	Batch(5100/7879) done. Loss: 0.1233  lr:0.000100
[ Wed Jul  3 22:36:25 2024 ] 	Batch(5200/7879) done. Loss: 0.2458  lr:0.000100
[ Wed Jul  3 22:36:43 2024 ] 	Batch(5300/7879) done. Loss: 0.1194  lr:0.000100
[ Wed Jul  3 22:37:01 2024 ] 	Batch(5400/7879) done. Loss: 0.1542  lr:0.000100
[ Wed Jul  3 22:37:18 2024 ] 
Training: Epoch [62/120], Step [5499], Loss: 0.16302701830863953, Training Accuracy: 95.56136363636364
[ Wed Jul  3 22:37:19 2024 ] 	Batch(5500/7879) done. Loss: 0.4734  lr:0.000100
[ Wed Jul  3 22:37:37 2024 ] 	Batch(5600/7879) done. Loss: 0.0633  lr:0.000100
[ Wed Jul  3 22:37:54 2024 ] 	Batch(5700/7879) done. Loss: 0.2880  lr:0.000100
[ Wed Jul  3 22:38:12 2024 ] 	Batch(5800/7879) done. Loss: 0.0202  lr:0.000100
[ Wed Jul  3 22:38:30 2024 ] 	Batch(5900/7879) done. Loss: 0.4498  lr:0.000100
[ Wed Jul  3 22:38:48 2024 ] 
Training: Epoch [62/120], Step [5999], Loss: 0.06068527698516846, Training Accuracy: 95.52083333333333
[ Wed Jul  3 22:38:48 2024 ] 	Batch(6000/7879) done. Loss: 0.0105  lr:0.000100
[ Wed Jul  3 22:39:06 2024 ] 	Batch(6100/7879) done. Loss: 0.0783  lr:0.000100
[ Wed Jul  3 22:39:24 2024 ] 	Batch(6200/7879) done. Loss: 0.0655  lr:0.000100
[ Wed Jul  3 22:39:42 2024 ] 	Batch(6300/7879) done. Loss: 0.2580  lr:0.000100
[ Wed Jul  3 22:40:00 2024 ] 	Batch(6400/7879) done. Loss: 0.1251  lr:0.000100
[ Wed Jul  3 22:40:18 2024 ] 
Training: Epoch [62/120], Step [6499], Loss: 0.27993547916412354, Training Accuracy: 95.5
[ Wed Jul  3 22:40:18 2024 ] 	Batch(6500/7879) done. Loss: 0.2244  lr:0.000100
[ Wed Jul  3 22:40:36 2024 ] 	Batch(6600/7879) done. Loss: 0.0385  lr:0.000100
[ Wed Jul  3 22:40:53 2024 ] 	Batch(6700/7879) done. Loss: 0.0096  lr:0.000100
[ Wed Jul  3 22:41:11 2024 ] 	Batch(6800/7879) done. Loss: 0.0176  lr:0.000100
[ Wed Jul  3 22:41:30 2024 ] 	Batch(6900/7879) done. Loss: 0.1995  lr:0.000100
[ Wed Jul  3 22:41:48 2024 ] 
Training: Epoch [62/120], Step [6999], Loss: 0.18995647132396698, Training Accuracy: 95.53928571428571
[ Wed Jul  3 22:41:49 2024 ] 	Batch(7000/7879) done. Loss: 0.0637  lr:0.000100
[ Wed Jul  3 22:42:07 2024 ] 	Batch(7100/7879) done. Loss: 0.0504  lr:0.000100
[ Wed Jul  3 22:42:26 2024 ] 	Batch(7200/7879) done. Loss: 0.3904  lr:0.000100
[ Wed Jul  3 22:42:44 2024 ] 	Batch(7300/7879) done. Loss: 0.2499  lr:0.000100
[ Wed Jul  3 22:43:01 2024 ] 	Batch(7400/7879) done. Loss: 0.0163  lr:0.000100
[ Wed Jul  3 22:43:19 2024 ] 
Training: Epoch [62/120], Step [7499], Loss: 0.05378161743283272, Training Accuracy: 95.52666666666667
[ Wed Jul  3 22:43:19 2024 ] 	Batch(7500/7879) done. Loss: 0.0062  lr:0.000100
[ Wed Jul  3 22:43:37 2024 ] 	Batch(7600/7879) done. Loss: 0.1576  lr:0.000100
[ Wed Jul  3 22:43:55 2024 ] 	Batch(7700/7879) done. Loss: 0.2748  lr:0.000100
[ Wed Jul  3 22:44:13 2024 ] 	Batch(7800/7879) done. Loss: 0.0052  lr:0.000100
[ Wed Jul  3 22:44:27 2024 ] 	Mean training loss: 0.1574.
[ Wed Jul  3 22:44:27 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 22:44:28 2024 ] Training epoch: 64
[ Wed Jul  3 22:44:28 2024 ] 	Batch(0/7879) done. Loss: 0.0415  lr:0.000100
[ Wed Jul  3 22:44:46 2024 ] 	Batch(100/7879) done. Loss: 0.0771  lr:0.000100
[ Wed Jul  3 22:45:04 2024 ] 	Batch(200/7879) done. Loss: 0.2448  lr:0.000100
[ Wed Jul  3 22:45:22 2024 ] 	Batch(300/7879) done. Loss: 0.0142  lr:0.000100
[ Wed Jul  3 22:45:40 2024 ] 	Batch(400/7879) done. Loss: 0.2261  lr:0.000100
[ Wed Jul  3 22:45:57 2024 ] 
Training: Epoch [63/120], Step [499], Loss: 0.22654026746749878, Training Accuracy: 95.7
[ Wed Jul  3 22:45:58 2024 ] 	Batch(500/7879) done. Loss: 0.2126  lr:0.000100
[ Wed Jul  3 22:46:15 2024 ] 	Batch(600/7879) done. Loss: 0.4676  lr:0.000100
[ Wed Jul  3 22:46:33 2024 ] 	Batch(700/7879) done. Loss: 0.0538  lr:0.000100
[ Wed Jul  3 22:46:51 2024 ] 	Batch(800/7879) done. Loss: 0.1336  lr:0.000100
[ Wed Jul  3 22:47:09 2024 ] 	Batch(900/7879) done. Loss: 0.6443  lr:0.000100
[ Wed Jul  3 22:47:27 2024 ] 
Training: Epoch [63/120], Step [999], Loss: 0.17698565125465393, Training Accuracy: 95.72500000000001
[ Wed Jul  3 22:47:27 2024 ] 	Batch(1000/7879) done. Loss: 0.0613  lr:0.000100
[ Wed Jul  3 22:47:45 2024 ] 	Batch(1100/7879) done. Loss: 0.0072  lr:0.000100
[ Wed Jul  3 22:48:03 2024 ] 	Batch(1200/7879) done. Loss: 0.1564  lr:0.000100
[ Wed Jul  3 22:48:21 2024 ] 	Batch(1300/7879) done. Loss: 0.0298  lr:0.000100
[ Wed Jul  3 22:48:39 2024 ] 	Batch(1400/7879) done. Loss: 0.0927  lr:0.000100
[ Wed Jul  3 22:48:57 2024 ] 
Training: Epoch [63/120], Step [1499], Loss: 0.29035520553588867, Training Accuracy: 95.65
[ Wed Jul  3 22:48:57 2024 ] 	Batch(1500/7879) done. Loss: 0.1953  lr:0.000100
[ Wed Jul  3 22:49:15 2024 ] 	Batch(1600/7879) done. Loss: 0.4844  lr:0.000100
[ Wed Jul  3 22:49:33 2024 ] 	Batch(1700/7879) done. Loss: 0.0300  lr:0.000100
[ Wed Jul  3 22:49:52 2024 ] 	Batch(1800/7879) done. Loss: 0.1491  lr:0.000100
[ Wed Jul  3 22:50:10 2024 ] 	Batch(1900/7879) done. Loss: 0.1629  lr:0.000100
[ Wed Jul  3 22:50:28 2024 ] 
Training: Epoch [63/120], Step [1999], Loss: 0.10228331387042999, Training Accuracy: 95.66875
[ Wed Jul  3 22:50:28 2024 ] 	Batch(2000/7879) done. Loss: 0.1018  lr:0.000100
[ Wed Jul  3 22:50:46 2024 ] 	Batch(2100/7879) done. Loss: 0.0347  lr:0.000100
[ Wed Jul  3 22:51:04 2024 ] 	Batch(2200/7879) done. Loss: 0.0464  lr:0.000100
[ Wed Jul  3 22:51:22 2024 ] 	Batch(2300/7879) done. Loss: 0.1388  lr:0.000100
[ Wed Jul  3 22:51:40 2024 ] 	Batch(2400/7879) done. Loss: 0.2584  lr:0.000100
[ Wed Jul  3 22:51:57 2024 ] 
Training: Epoch [63/120], Step [2499], Loss: 0.05771086737513542, Training Accuracy: 95.72500000000001
[ Wed Jul  3 22:51:58 2024 ] 	Batch(2500/7879) done. Loss: 0.4074  lr:0.000100
[ Wed Jul  3 22:52:15 2024 ] 	Batch(2600/7879) done. Loss: 0.2475  lr:0.000100
[ Wed Jul  3 22:52:33 2024 ] 	Batch(2700/7879) done. Loss: 0.1851  lr:0.000100
[ Wed Jul  3 22:52:52 2024 ] 	Batch(2800/7879) done. Loss: 0.0412  lr:0.000100
[ Wed Jul  3 22:53:10 2024 ] 	Batch(2900/7879) done. Loss: 0.0202  lr:0.000100
[ Wed Jul  3 22:53:28 2024 ] 
Training: Epoch [63/120], Step [2999], Loss: 0.3420799672603607, Training Accuracy: 95.75
[ Wed Jul  3 22:53:29 2024 ] 	Batch(3000/7879) done. Loss: 0.2315  lr:0.000100
[ Wed Jul  3 22:53:47 2024 ] 	Batch(3100/7879) done. Loss: 0.2360  lr:0.000100
[ Wed Jul  3 22:54:06 2024 ] 	Batch(3200/7879) done. Loss: 0.0278  lr:0.000100
[ Wed Jul  3 22:54:24 2024 ] 	Batch(3300/7879) done. Loss: 0.1413  lr:0.000100
[ Wed Jul  3 22:54:42 2024 ] 	Batch(3400/7879) done. Loss: 0.0247  lr:0.000100
[ Wed Jul  3 22:55:00 2024 ] 
Training: Epoch [63/120], Step [3499], Loss: 0.05429837852716446, Training Accuracy: 95.79642857142858
[ Wed Jul  3 22:55:00 2024 ] 	Batch(3500/7879) done. Loss: 0.1870  lr:0.000100
[ Wed Jul  3 22:55:18 2024 ] 	Batch(3600/7879) done. Loss: 0.0657  lr:0.000100
[ Wed Jul  3 22:55:36 2024 ] 	Batch(3700/7879) done. Loss: 0.1423  lr:0.000100
[ Wed Jul  3 22:55:54 2024 ] 	Batch(3800/7879) done. Loss: 0.1882  lr:0.000100
[ Wed Jul  3 22:56:12 2024 ] 	Batch(3900/7879) done. Loss: 0.0342  lr:0.000100
[ Wed Jul  3 22:56:29 2024 ] 
Training: Epoch [63/120], Step [3999], Loss: 0.029731284826993942, Training Accuracy: 95.79062499999999
[ Wed Jul  3 22:56:30 2024 ] 	Batch(4000/7879) done. Loss: 0.2394  lr:0.000100
[ Wed Jul  3 22:56:48 2024 ] 	Batch(4100/7879) done. Loss: 0.0035  lr:0.000100
[ Wed Jul  3 22:57:07 2024 ] 	Batch(4200/7879) done. Loss: 0.1533  lr:0.000100
[ Wed Jul  3 22:57:25 2024 ] 	Batch(4300/7879) done. Loss: 0.0085  lr:0.000100
[ Wed Jul  3 22:57:44 2024 ] 	Batch(4400/7879) done. Loss: 0.1574  lr:0.000100
[ Wed Jul  3 22:58:02 2024 ] 
Training: Epoch [63/120], Step [4499], Loss: 0.7717826962471008, Training Accuracy: 95.79444444444445
[ Wed Jul  3 22:58:02 2024 ] 	Batch(4500/7879) done. Loss: 0.0997  lr:0.000100
[ Wed Jul  3 22:58:21 2024 ] 	Batch(4600/7879) done. Loss: 0.1199  lr:0.000100
[ Wed Jul  3 22:58:39 2024 ] 	Batch(4700/7879) done. Loss: 0.0693  lr:0.000100
[ Wed Jul  3 22:58:58 2024 ] 	Batch(4800/7879) done. Loss: 0.2296  lr:0.000100
[ Wed Jul  3 22:59:17 2024 ] 	Batch(4900/7879) done. Loss: 0.2848  lr:0.000100
[ Wed Jul  3 22:59:35 2024 ] 
Training: Epoch [63/120], Step [4999], Loss: 0.017779210582375526, Training Accuracy: 95.785
[ Wed Jul  3 22:59:35 2024 ] 	Batch(5000/7879) done. Loss: 0.0317  lr:0.000100
[ Wed Jul  3 22:59:54 2024 ] 	Batch(5100/7879) done. Loss: 0.2376  lr:0.000100
[ Wed Jul  3 23:00:12 2024 ] 	Batch(5200/7879) done. Loss: 0.3806  lr:0.000100
[ Wed Jul  3 23:00:31 2024 ] 	Batch(5300/7879) done. Loss: 0.1348  lr:0.000100
[ Wed Jul  3 23:00:49 2024 ] 	Batch(5400/7879) done. Loss: 0.2315  lr:0.000100
[ Wed Jul  3 23:01:08 2024 ] 
Training: Epoch [63/120], Step [5499], Loss: 0.18737034499645233, Training Accuracy: 95.76590909090909
[ Wed Jul  3 23:01:08 2024 ] 	Batch(5500/7879) done. Loss: 0.0382  lr:0.000100
[ Wed Jul  3 23:01:26 2024 ] 	Batch(5600/7879) done. Loss: 0.1639  lr:0.000100
[ Wed Jul  3 23:01:44 2024 ] 	Batch(5700/7879) done. Loss: 0.0796  lr:0.000100
[ Wed Jul  3 23:02:02 2024 ] 	Batch(5800/7879) done. Loss: 0.0367  lr:0.000100
[ Wed Jul  3 23:02:20 2024 ] 	Batch(5900/7879) done. Loss: 0.2929  lr:0.000100
[ Wed Jul  3 23:02:38 2024 ] 
Training: Epoch [63/120], Step [5999], Loss: 0.18558256328105927, Training Accuracy: 95.78541666666666
[ Wed Jul  3 23:02:38 2024 ] 	Batch(6000/7879) done. Loss: 0.0894  lr:0.000100
[ Wed Jul  3 23:02:56 2024 ] 	Batch(6100/7879) done. Loss: 0.0085  lr:0.000100
[ Wed Jul  3 23:03:14 2024 ] 	Batch(6200/7879) done. Loss: 0.0606  lr:0.000100
[ Wed Jul  3 23:03:32 2024 ] 	Batch(6300/7879) done. Loss: 0.0283  lr:0.000100
[ Wed Jul  3 23:03:50 2024 ] 	Batch(6400/7879) done. Loss: 0.0697  lr:0.000100
[ Wed Jul  3 23:04:07 2024 ] 
Training: Epoch [63/120], Step [6499], Loss: 0.1393079310655594, Training Accuracy: 95.81153846153846
[ Wed Jul  3 23:04:08 2024 ] 	Batch(6500/7879) done. Loss: 0.1225  lr:0.000100
[ Wed Jul  3 23:04:25 2024 ] 	Batch(6600/7879) done. Loss: 0.0120  lr:0.000100
[ Wed Jul  3 23:04:43 2024 ] 	Batch(6700/7879) done. Loss: 0.0155  lr:0.000100
[ Wed Jul  3 23:05:01 2024 ] 	Batch(6800/7879) done. Loss: 0.3821  lr:0.000100
[ Wed Jul  3 23:05:19 2024 ] 	Batch(6900/7879) done. Loss: 0.1128  lr:0.000100
[ Wed Jul  3 23:05:37 2024 ] 
Training: Epoch [63/120], Step [6999], Loss: 0.02074623666703701, Training Accuracy: 95.83928571428572
[ Wed Jul  3 23:05:37 2024 ] 	Batch(7000/7879) done. Loss: 0.1017  lr:0.000100
[ Wed Jul  3 23:05:55 2024 ] 	Batch(7100/7879) done. Loss: 0.2823  lr:0.000100
[ Wed Jul  3 23:06:13 2024 ] 	Batch(7200/7879) done. Loss: 0.3306  lr:0.000100
[ Wed Jul  3 23:06:31 2024 ] 	Batch(7300/7879) done. Loss: 0.0390  lr:0.000100
[ Wed Jul  3 23:06:49 2024 ] 	Batch(7400/7879) done. Loss: 0.0173  lr:0.000100
[ Wed Jul  3 23:07:07 2024 ] 
Training: Epoch [63/120], Step [7499], Loss: 0.057327911257743835, Training Accuracy: 95.84
[ Wed Jul  3 23:07:07 2024 ] 	Batch(7500/7879) done. Loss: 0.0980  lr:0.000100
[ Wed Jul  3 23:07:25 2024 ] 	Batch(7600/7879) done. Loss: 0.0280  lr:0.000100
[ Wed Jul  3 23:07:43 2024 ] 	Batch(7700/7879) done. Loss: 0.0162  lr:0.000100
[ Wed Jul  3 23:08:02 2024 ] 	Batch(7800/7879) done. Loss: 0.0344  lr:0.000100
[ Wed Jul  3 23:08:16 2024 ] 	Mean training loss: 0.1490.
[ Wed Jul  3 23:08:16 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 23:08:17 2024 ] Training epoch: 65
[ Wed Jul  3 23:08:17 2024 ] 	Batch(0/7879) done. Loss: 0.0125  lr:0.000100
[ Wed Jul  3 23:08:35 2024 ] 	Batch(100/7879) done. Loss: 0.0867  lr:0.000100
[ Wed Jul  3 23:08:53 2024 ] 	Batch(200/7879) done. Loss: 0.2670  lr:0.000100
[ Wed Jul  3 23:09:11 2024 ] 	Batch(300/7879) done. Loss: 0.1466  lr:0.000100
[ Wed Jul  3 23:09:29 2024 ] 	Batch(400/7879) done. Loss: 0.0243  lr:0.000100
[ Wed Jul  3 23:09:46 2024 ] 
Training: Epoch [64/120], Step [499], Loss: 0.021169157698750496, Training Accuracy: 96.325
[ Wed Jul  3 23:09:47 2024 ] 	Batch(500/7879) done. Loss: 0.2150  lr:0.000100
[ Wed Jul  3 23:10:04 2024 ] 	Batch(600/7879) done. Loss: 0.5109  lr:0.000100
[ Wed Jul  3 23:10:22 2024 ] 	Batch(700/7879) done. Loss: 0.0395  lr:0.000100
[ Wed Jul  3 23:10:40 2024 ] 	Batch(800/7879) done. Loss: 0.0229  lr:0.000100
[ Wed Jul  3 23:10:58 2024 ] 	Batch(900/7879) done. Loss: 0.0651  lr:0.000100
[ Wed Jul  3 23:11:16 2024 ] 
Training: Epoch [64/120], Step [999], Loss: 0.2199123203754425, Training Accuracy: 96.3625
[ Wed Jul  3 23:11:16 2024 ] 	Batch(1000/7879) done. Loss: 0.0033  lr:0.000100
[ Wed Jul  3 23:11:34 2024 ] 	Batch(1100/7879) done. Loss: 0.3333  lr:0.000100
[ Wed Jul  3 23:11:52 2024 ] 	Batch(1200/7879) done. Loss: 0.0657  lr:0.000100
[ Wed Jul  3 23:12:10 2024 ] 	Batch(1300/7879) done. Loss: 0.7500  lr:0.000100
[ Wed Jul  3 23:12:29 2024 ] 	Batch(1400/7879) done. Loss: 0.0877  lr:0.000100
[ Wed Jul  3 23:12:47 2024 ] 
Training: Epoch [64/120], Step [1499], Loss: 0.15702112019062042, Training Accuracy: 96.24166666666667
[ Wed Jul  3 23:12:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0103  lr:0.000100
[ Wed Jul  3 23:13:05 2024 ] 	Batch(1600/7879) done. Loss: 0.0332  lr:0.000100
[ Wed Jul  3 23:13:24 2024 ] 	Batch(1700/7879) done. Loss: 0.0809  lr:0.000100
[ Wed Jul  3 23:13:42 2024 ] 	Batch(1800/7879) done. Loss: 0.1120  lr:0.000100
[ Wed Jul  3 23:14:01 2024 ] 	Batch(1900/7879) done. Loss: 0.1646  lr:0.000100
[ Wed Jul  3 23:14:19 2024 ] 
Training: Epoch [64/120], Step [1999], Loss: 0.026216546073555946, Training Accuracy: 96.2
[ Wed Jul  3 23:14:20 2024 ] 	Batch(2000/7879) done. Loss: 0.0154  lr:0.000100
[ Wed Jul  3 23:14:37 2024 ] 	Batch(2100/7879) done. Loss: 0.0225  lr:0.000100
[ Wed Jul  3 23:14:55 2024 ] 	Batch(2200/7879) done. Loss: 0.3018  lr:0.000100
[ Wed Jul  3 23:15:13 2024 ] 	Batch(2300/7879) done. Loss: 0.0142  lr:0.000100
[ Wed Jul  3 23:15:31 2024 ] 	Batch(2400/7879) done. Loss: 0.1146  lr:0.000100
[ Wed Jul  3 23:15:50 2024 ] 
Training: Epoch [64/120], Step [2499], Loss: 0.3798125982284546, Training Accuracy: 96.12
[ Wed Jul  3 23:15:50 2024 ] 	Batch(2500/7879) done. Loss: 0.0168  lr:0.000100
[ Wed Jul  3 23:16:08 2024 ] 	Batch(2600/7879) done. Loss: 0.3796  lr:0.000100
[ Wed Jul  3 23:16:27 2024 ] 	Batch(2700/7879) done. Loss: 0.1360  lr:0.000100
[ Wed Jul  3 23:16:45 2024 ] 	Batch(2800/7879) done. Loss: 0.0392  lr:0.000100
[ Wed Jul  3 23:17:03 2024 ] 	Batch(2900/7879) done. Loss: 0.2966  lr:0.000100
[ Wed Jul  3 23:17:21 2024 ] 
Training: Epoch [64/120], Step [2999], Loss: 0.029780374839901924, Training Accuracy: 96.10416666666667
[ Wed Jul  3 23:17:21 2024 ] 	Batch(3000/7879) done. Loss: 0.0124  lr:0.000100
[ Wed Jul  3 23:17:39 2024 ] 	Batch(3100/7879) done. Loss: 0.0225  lr:0.000100
[ Wed Jul  3 23:17:57 2024 ] 	Batch(3200/7879) done. Loss: 0.1987  lr:0.000100
[ Wed Jul  3 23:18:15 2024 ] 	Batch(3300/7879) done. Loss: 0.1886  lr:0.000100
[ Wed Jul  3 23:18:33 2024 ] 	Batch(3400/7879) done. Loss: 0.0766  lr:0.000100
[ Wed Jul  3 23:18:51 2024 ] 
Training: Epoch [64/120], Step [3499], Loss: 0.28026944398880005, Training Accuracy: 96.10714285714286
[ Wed Jul  3 23:18:51 2024 ] 	Batch(3500/7879) done. Loss: 0.1014  lr:0.000100
[ Wed Jul  3 23:19:09 2024 ] 	Batch(3600/7879) done. Loss: 0.3358  lr:0.000100
[ Wed Jul  3 23:19:27 2024 ] 	Batch(3700/7879) done. Loss: 0.0979  lr:0.000100
[ Wed Jul  3 23:19:45 2024 ] 	Batch(3800/7879) done. Loss: 0.0993  lr:0.000100
[ Wed Jul  3 23:20:02 2024 ] 	Batch(3900/7879) done. Loss: 0.1890  lr:0.000100
[ Wed Jul  3 23:20:20 2024 ] 
Training: Epoch [64/120], Step [3999], Loss: 0.07017872482538223, Training Accuracy: 96.109375
[ Wed Jul  3 23:20:20 2024 ] 	Batch(4000/7879) done. Loss: 0.1373  lr:0.000100
[ Wed Jul  3 23:20:39 2024 ] 	Batch(4100/7879) done. Loss: 0.0421  lr:0.000100
[ Wed Jul  3 23:20:58 2024 ] 	Batch(4200/7879) done. Loss: 0.1494  lr:0.000100
[ Wed Jul  3 23:21:16 2024 ] 	Batch(4300/7879) done. Loss: 0.1211  lr:0.000100
[ Wed Jul  3 23:21:35 2024 ] 	Batch(4400/7879) done. Loss: 0.2598  lr:0.000100
[ Wed Jul  3 23:21:52 2024 ] 
Training: Epoch [64/120], Step [4499], Loss: 0.1323455423116684, Training Accuracy: 96.10277777777779
[ Wed Jul  3 23:21:53 2024 ] 	Batch(4500/7879) done. Loss: 0.0679  lr:0.000100
[ Wed Jul  3 23:22:10 2024 ] 	Batch(4600/7879) done. Loss: 0.0460  lr:0.000100
[ Wed Jul  3 23:22:28 2024 ] 	Batch(4700/7879) done. Loss: 0.0097  lr:0.000100
[ Wed Jul  3 23:22:46 2024 ] 	Batch(4800/7879) done. Loss: 0.4029  lr:0.000100
[ Wed Jul  3 23:23:04 2024 ] 	Batch(4900/7879) done. Loss: 0.1610  lr:0.000100
[ Wed Jul  3 23:23:22 2024 ] 
Training: Epoch [64/120], Step [4999], Loss: 0.051039841026067734, Training Accuracy: 96.1375
[ Wed Jul  3 23:23:22 2024 ] 	Batch(5000/7879) done. Loss: 0.0974  lr:0.000100
[ Wed Jul  3 23:23:40 2024 ] 	Batch(5100/7879) done. Loss: 0.0824  lr:0.000100
[ Wed Jul  3 23:23:58 2024 ] 	Batch(5200/7879) done. Loss: 0.0165  lr:0.000100
[ Wed Jul  3 23:24:16 2024 ] 	Batch(5300/7879) done. Loss: 0.3765  lr:0.000100
[ Wed Jul  3 23:24:34 2024 ] 	Batch(5400/7879) done. Loss: 0.0314  lr:0.000100
[ Wed Jul  3 23:24:52 2024 ] 
Training: Epoch [64/120], Step [5499], Loss: 0.005749625153839588, Training Accuracy: 96.15227272727273
[ Wed Jul  3 23:24:53 2024 ] 	Batch(5500/7879) done. Loss: 0.0016  lr:0.000100
[ Wed Jul  3 23:25:11 2024 ] 	Batch(5600/7879) done. Loss: 0.0169  lr:0.000100
[ Wed Jul  3 23:25:29 2024 ] 	Batch(5700/7879) done. Loss: 0.0739  lr:0.000100
[ Wed Jul  3 23:25:47 2024 ] 	Batch(5800/7879) done. Loss: 0.3464  lr:0.000100
[ Wed Jul  3 23:26:05 2024 ] 	Batch(5900/7879) done. Loss: 0.0709  lr:0.000100
[ Wed Jul  3 23:26:23 2024 ] 
Training: Epoch [64/120], Step [5999], Loss: 0.006393318064510822, Training Accuracy: 96.12708333333333
[ Wed Jul  3 23:26:23 2024 ] 	Batch(6000/7879) done. Loss: 0.3714  lr:0.000100
[ Wed Jul  3 23:26:41 2024 ] 	Batch(6100/7879) done. Loss: 0.0636  lr:0.000100
[ Wed Jul  3 23:27:00 2024 ] 	Batch(6200/7879) done. Loss: 0.0298  lr:0.000100
[ Wed Jul  3 23:27:19 2024 ] 	Batch(6300/7879) done. Loss: 0.1419  lr:0.000100
[ Wed Jul  3 23:27:37 2024 ] 	Batch(6400/7879) done. Loss: 0.0545  lr:0.000100
[ Wed Jul  3 23:27:56 2024 ] 
Training: Epoch [64/120], Step [6499], Loss: 0.03482783958315849, Training Accuracy: 96.13461538461539
[ Wed Jul  3 23:27:56 2024 ] 	Batch(6500/7879) done. Loss: 0.0284  lr:0.000100
[ Wed Jul  3 23:28:14 2024 ] 	Batch(6600/7879) done. Loss: 0.0652  lr:0.000100
[ Wed Jul  3 23:28:33 2024 ] 	Batch(6700/7879) done. Loss: 0.2038  lr:0.000100
[ Wed Jul  3 23:28:52 2024 ] 	Batch(6800/7879) done. Loss: 0.0239  lr:0.000100
[ Wed Jul  3 23:29:09 2024 ] 	Batch(6900/7879) done. Loss: 0.0214  lr:0.000100
[ Wed Jul  3 23:29:27 2024 ] 
Training: Epoch [64/120], Step [6999], Loss: 0.06848528981208801, Training Accuracy: 96.1375
[ Wed Jul  3 23:29:27 2024 ] 	Batch(7000/7879) done. Loss: 0.5456  lr:0.000100
[ Wed Jul  3 23:29:45 2024 ] 	Batch(7100/7879) done. Loss: 0.2351  lr:0.000100
[ Wed Jul  3 23:30:03 2024 ] 	Batch(7200/7879) done. Loss: 0.1213  lr:0.000100
[ Wed Jul  3 23:30:21 2024 ] 	Batch(7300/7879) done. Loss: 0.0354  lr:0.000100
[ Wed Jul  3 23:30:39 2024 ] 	Batch(7400/7879) done. Loss: 0.2707  lr:0.000100
[ Wed Jul  3 23:30:57 2024 ] 
Training: Epoch [64/120], Step [7499], Loss: 0.12281498312950134, Training Accuracy: 96.14
[ Wed Jul  3 23:30:57 2024 ] 	Batch(7500/7879) done. Loss: 0.0198  lr:0.000100
[ Wed Jul  3 23:31:15 2024 ] 	Batch(7600/7879) done. Loss: 0.0800  lr:0.000100
[ Wed Jul  3 23:31:34 2024 ] 	Batch(7700/7879) done. Loss: 0.0244  lr:0.000100
[ Wed Jul  3 23:31:52 2024 ] 	Batch(7800/7879) done. Loss: 0.1245  lr:0.000100
[ Wed Jul  3 23:32:07 2024 ] 	Mean training loss: 0.1413.
[ Wed Jul  3 23:32:07 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 23:32:07 2024 ] Training epoch: 66
[ Wed Jul  3 23:32:07 2024 ] 	Batch(0/7879) done. Loss: 0.0644  lr:0.000100
[ Wed Jul  3 23:32:26 2024 ] 	Batch(100/7879) done. Loss: 0.2582  lr:0.000100
[ Wed Jul  3 23:32:44 2024 ] 	Batch(200/7879) done. Loss: 0.0528  lr:0.000100
[ Wed Jul  3 23:33:02 2024 ] 	Batch(300/7879) done. Loss: 0.0086  lr:0.000100
[ Wed Jul  3 23:33:21 2024 ] 	Batch(400/7879) done. Loss: 0.0320  lr:0.000100
[ Wed Jul  3 23:33:39 2024 ] 
Training: Epoch [65/120], Step [499], Loss: 0.016468465328216553, Training Accuracy: 96.1
[ Wed Jul  3 23:33:39 2024 ] 	Batch(500/7879) done. Loss: 0.0123  lr:0.000100
[ Wed Jul  3 23:33:57 2024 ] 	Batch(600/7879) done. Loss: 0.1637  lr:0.000100
[ Wed Jul  3 23:34:16 2024 ] 	Batch(700/7879) done. Loss: 0.0177  lr:0.000100
[ Wed Jul  3 23:34:34 2024 ] 	Batch(800/7879) done. Loss: 0.2519  lr:0.000100
[ Wed Jul  3 23:34:52 2024 ] 	Batch(900/7879) done. Loss: 0.0204  lr:0.000100
[ Wed Jul  3 23:35:10 2024 ] 
Training: Epoch [65/120], Step [999], Loss: 0.2169376015663147, Training Accuracy: 95.6875
[ Wed Jul  3 23:35:10 2024 ] 	Batch(1000/7879) done. Loss: 0.4306  lr:0.000100
[ Wed Jul  3 23:35:28 2024 ] 	Batch(1100/7879) done. Loss: 0.0507  lr:0.000100
[ Wed Jul  3 23:35:46 2024 ] 	Batch(1200/7879) done. Loss: 0.0806  lr:0.000100
[ Wed Jul  3 23:36:04 2024 ] 	Batch(1300/7879) done. Loss: 0.6685  lr:0.000100
[ Wed Jul  3 23:36:22 2024 ] 	Batch(1400/7879) done. Loss: 0.1641  lr:0.000100
[ Wed Jul  3 23:36:40 2024 ] 
Training: Epoch [65/120], Step [1499], Loss: 0.18371233344078064, Training Accuracy: 96.00833333333333
[ Wed Jul  3 23:36:40 2024 ] 	Batch(1500/7879) done. Loss: 0.1314  lr:0.000100
[ Wed Jul  3 23:36:58 2024 ] 	Batch(1600/7879) done. Loss: 0.0463  lr:0.000100
[ Wed Jul  3 23:37:16 2024 ] 	Batch(1700/7879) done. Loss: 0.1692  lr:0.000100
[ Wed Jul  3 23:37:34 2024 ] 	Batch(1800/7879) done. Loss: 0.0564  lr:0.000100
[ Wed Jul  3 23:37:52 2024 ] 	Batch(1900/7879) done. Loss: 0.1188  lr:0.000100
[ Wed Jul  3 23:38:09 2024 ] 
Training: Epoch [65/120], Step [1999], Loss: 0.0863177478313446, Training Accuracy: 96.0625
[ Wed Jul  3 23:38:10 2024 ] 	Batch(2000/7879) done. Loss: 0.1231  lr:0.000100
[ Wed Jul  3 23:38:28 2024 ] 	Batch(2100/7879) done. Loss: 0.0587  lr:0.000100
[ Wed Jul  3 23:38:47 2024 ] 	Batch(2200/7879) done. Loss: 0.0285  lr:0.000100
[ Wed Jul  3 23:39:05 2024 ] 	Batch(2300/7879) done. Loss: 0.1016  lr:0.000100
[ Wed Jul  3 23:39:24 2024 ] 	Batch(2400/7879) done. Loss: 0.0421  lr:0.000100
[ Wed Jul  3 23:39:42 2024 ] 
Training: Epoch [65/120], Step [2499], Loss: 0.02477404661476612, Training Accuracy: 96.055
[ Wed Jul  3 23:39:43 2024 ] 	Batch(2500/7879) done. Loss: 0.0662  lr:0.000100
[ Wed Jul  3 23:40:01 2024 ] 	Batch(2600/7879) done. Loss: 0.0324  lr:0.000100
[ Wed Jul  3 23:40:20 2024 ] 	Batch(2700/7879) done. Loss: 0.0709  lr:0.000100
[ Wed Jul  3 23:40:38 2024 ] 	Batch(2800/7879) done. Loss: 0.2951  lr:0.000100
[ Wed Jul  3 23:40:56 2024 ] 	Batch(2900/7879) done. Loss: 0.0636  lr:0.000100
[ Wed Jul  3 23:41:14 2024 ] 
Training: Epoch [65/120], Step [2999], Loss: 0.6279668807983398, Training Accuracy: 96.10416666666667
[ Wed Jul  3 23:41:14 2024 ] 	Batch(3000/7879) done. Loss: 0.1255  lr:0.000100
[ Wed Jul  3 23:41:32 2024 ] 	Batch(3100/7879) done. Loss: 0.1800  lr:0.000100
[ Wed Jul  3 23:41:50 2024 ] 	Batch(3200/7879) done. Loss: 0.0303  lr:0.000100
[ Wed Jul  3 23:42:08 2024 ] 	Batch(3300/7879) done. Loss: 0.0180  lr:0.000100
[ Wed Jul  3 23:42:26 2024 ] 	Batch(3400/7879) done. Loss: 0.0967  lr:0.000100
[ Wed Jul  3 23:42:44 2024 ] 
Training: Epoch [65/120], Step [3499], Loss: 0.19528600573539734, Training Accuracy: 96.10714285714286
[ Wed Jul  3 23:42:44 2024 ] 	Batch(3500/7879) done. Loss: 0.5892  lr:0.000100
[ Wed Jul  3 23:43:02 2024 ] 	Batch(3600/7879) done. Loss: 0.2520  lr:0.000100
[ Wed Jul  3 23:43:20 2024 ] 	Batch(3700/7879) done. Loss: 0.1192  lr:0.000100
[ Wed Jul  3 23:43:38 2024 ] 	Batch(3800/7879) done. Loss: 0.5829  lr:0.000100
[ Wed Jul  3 23:43:55 2024 ] 	Batch(3900/7879) done. Loss: 0.0105  lr:0.000100
[ Wed Jul  3 23:44:13 2024 ] 
Training: Epoch [65/120], Step [3999], Loss: 0.04379327595233917, Training Accuracy: 96.125
[ Wed Jul  3 23:44:13 2024 ] 	Batch(4000/7879) done. Loss: 0.3803  lr:0.000100
[ Wed Jul  3 23:44:31 2024 ] 	Batch(4100/7879) done. Loss: 0.2120  lr:0.000100
[ Wed Jul  3 23:44:49 2024 ] 	Batch(4200/7879) done. Loss: 0.2102  lr:0.000100
[ Wed Jul  3 23:45:07 2024 ] 	Batch(4300/7879) done. Loss: 0.0557  lr:0.000100
[ Wed Jul  3 23:45:25 2024 ] 	Batch(4400/7879) done. Loss: 0.3624  lr:0.000100
[ Wed Jul  3 23:45:43 2024 ] 
Training: Epoch [65/120], Step [4499], Loss: 0.007273183669894934, Training Accuracy: 96.1861111111111
[ Wed Jul  3 23:45:43 2024 ] 	Batch(4500/7879) done. Loss: 0.0149  lr:0.000100
[ Wed Jul  3 23:46:01 2024 ] 	Batch(4600/7879) done. Loss: 0.3189  lr:0.000100
[ Wed Jul  3 23:46:19 2024 ] 	Batch(4700/7879) done. Loss: 0.1117  lr:0.000100
[ Wed Jul  3 23:46:37 2024 ] 	Batch(4800/7879) done. Loss: 0.0452  lr:0.000100
[ Wed Jul  3 23:46:55 2024 ] 	Batch(4900/7879) done. Loss: 0.0883  lr:0.000100
[ Wed Jul  3 23:47:12 2024 ] 
Training: Epoch [65/120], Step [4999], Loss: 0.35480767488479614, Training Accuracy: 96.17
[ Wed Jul  3 23:47:12 2024 ] 	Batch(5000/7879) done. Loss: 0.0664  lr:0.000100
[ Wed Jul  3 23:47:30 2024 ] 	Batch(5100/7879) done. Loss: 0.0542  lr:0.000100
[ Wed Jul  3 23:47:48 2024 ] 	Batch(5200/7879) done. Loss: 0.0066  lr:0.000100
[ Wed Jul  3 23:48:06 2024 ] 	Batch(5300/7879) done. Loss: 0.1097  lr:0.000100
[ Wed Jul  3 23:48:24 2024 ] 	Batch(5400/7879) done. Loss: 0.0721  lr:0.000100
[ Wed Jul  3 23:48:42 2024 ] 
Training: Epoch [65/120], Step [5499], Loss: 0.0018781907856464386, Training Accuracy: 96.19545454545455
[ Wed Jul  3 23:48:42 2024 ] 	Batch(5500/7879) done. Loss: 0.0291  lr:0.000100
[ Wed Jul  3 23:49:00 2024 ] 	Batch(5600/7879) done. Loss: 0.0116  lr:0.000100
[ Wed Jul  3 23:49:18 2024 ] 	Batch(5700/7879) done. Loss: 0.1386  lr:0.000100
[ Wed Jul  3 23:49:36 2024 ] 	Batch(5800/7879) done. Loss: 0.0086  lr:0.000100
[ Wed Jul  3 23:49:54 2024 ] 	Batch(5900/7879) done. Loss: 0.3804  lr:0.000100
[ Wed Jul  3 23:50:12 2024 ] 
Training: Epoch [65/120], Step [5999], Loss: 0.04617706686258316, Training Accuracy: 96.19166666666666
[ Wed Jul  3 23:50:12 2024 ] 	Batch(6000/7879) done. Loss: 0.0588  lr:0.000100
[ Wed Jul  3 23:50:30 2024 ] 	Batch(6100/7879) done. Loss: 0.5024  lr:0.000100
[ Wed Jul  3 23:50:48 2024 ] 	Batch(6200/7879) done. Loss: 0.0671  lr:0.000100
[ Wed Jul  3 23:51:06 2024 ] 	Batch(6300/7879) done. Loss: 0.0424  lr:0.000100
[ Wed Jul  3 23:51:24 2024 ] 	Batch(6400/7879) done. Loss: 0.0951  lr:0.000100
[ Wed Jul  3 23:51:41 2024 ] 
Training: Epoch [65/120], Step [6499], Loss: 0.0892423689365387, Training Accuracy: 96.20384615384616
[ Wed Jul  3 23:51:42 2024 ] 	Batch(6500/7879) done. Loss: 0.0143  lr:0.000100
[ Wed Jul  3 23:51:59 2024 ] 	Batch(6600/7879) done. Loss: 0.0526  lr:0.000100
[ Wed Jul  3 23:52:17 2024 ] 	Batch(6700/7879) done. Loss: 0.0779  lr:0.000100
[ Wed Jul  3 23:52:35 2024 ] 	Batch(6800/7879) done. Loss: 0.1599  lr:0.000100
[ Wed Jul  3 23:52:53 2024 ] 	Batch(6900/7879) done. Loss: 0.0556  lr:0.000100
[ Wed Jul  3 23:53:11 2024 ] 
Training: Epoch [65/120], Step [6999], Loss: 0.2486225962638855, Training Accuracy: 96.23035714285714
[ Wed Jul  3 23:53:11 2024 ] 	Batch(7000/7879) done. Loss: 0.0301  lr:0.000100
[ Wed Jul  3 23:53:29 2024 ] 	Batch(7100/7879) done. Loss: 0.4358  lr:0.000100
[ Wed Jul  3 23:53:47 2024 ] 	Batch(7200/7879) done. Loss: 0.0438  lr:0.000100
[ Wed Jul  3 23:54:05 2024 ] 	Batch(7300/7879) done. Loss: 0.0419  lr:0.000100
[ Wed Jul  3 23:54:23 2024 ] 	Batch(7400/7879) done. Loss: 0.2982  lr:0.000100
[ Wed Jul  3 23:54:41 2024 ] 
Training: Epoch [65/120], Step [7499], Loss: 0.49964743852615356, Training Accuracy: 96.245
[ Wed Jul  3 23:54:41 2024 ] 	Batch(7500/7879) done. Loss: 0.0271  lr:0.000100
[ Wed Jul  3 23:54:59 2024 ] 	Batch(7600/7879) done. Loss: 0.0184  lr:0.000100
[ Wed Jul  3 23:55:17 2024 ] 	Batch(7700/7879) done. Loss: 0.0219  lr:0.000100
[ Wed Jul  3 23:55:35 2024 ] 	Batch(7800/7879) done. Loss: 1.1111  lr:0.000100
[ Wed Jul  3 23:55:49 2024 ] 	Mean training loss: 0.1384.
[ Wed Jul  3 23:55:49 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Wed Jul  3 23:55:49 2024 ] Training epoch: 67
[ Wed Jul  3 23:55:49 2024 ] 	Batch(0/7879) done. Loss: 0.0111  lr:0.000100
[ Wed Jul  3 23:56:07 2024 ] 	Batch(100/7879) done. Loss: 0.1282  lr:0.000100
[ Wed Jul  3 23:56:25 2024 ] 	Batch(200/7879) done. Loss: 0.2093  lr:0.000100
[ Wed Jul  3 23:56:43 2024 ] 	Batch(300/7879) done. Loss: 0.1022  lr:0.000100
[ Wed Jul  3 23:57:01 2024 ] 	Batch(400/7879) done. Loss: 0.0692  lr:0.000100
[ Wed Jul  3 23:57:19 2024 ] 
Training: Epoch [66/120], Step [499], Loss: 0.11717569082975388, Training Accuracy: 96.375
[ Wed Jul  3 23:57:19 2024 ] 	Batch(500/7879) done. Loss: 0.1259  lr:0.000100
[ Wed Jul  3 23:57:37 2024 ] 	Batch(600/7879) done. Loss: 0.0735  lr:0.000100
[ Wed Jul  3 23:57:55 2024 ] 	Batch(700/7879) done. Loss: 0.2398  lr:0.000100
[ Wed Jul  3 23:58:13 2024 ] 	Batch(800/7879) done. Loss: 0.0508  lr:0.000100
[ Wed Jul  3 23:58:32 2024 ] 	Batch(900/7879) done. Loss: 0.0111  lr:0.000100
[ Wed Jul  3 23:58:50 2024 ] 
Training: Epoch [66/120], Step [999], Loss: 0.14396867156028748, Training Accuracy: 96.475
[ Wed Jul  3 23:58:50 2024 ] 	Batch(1000/7879) done. Loss: 0.1622  lr:0.000100
[ Wed Jul  3 23:59:09 2024 ] 	Batch(1100/7879) done. Loss: 0.5409  lr:0.000100
[ Wed Jul  3 23:59:28 2024 ] 	Batch(1200/7879) done. Loss: 0.0411  lr:0.000100
[ Wed Jul  3 23:59:46 2024 ] 	Batch(1300/7879) done. Loss: 0.1110  lr:0.000100
[ Thu Jul  4 00:00:04 2024 ] 	Batch(1400/7879) done. Loss: 0.0510  lr:0.000100
[ Thu Jul  4 00:00:22 2024 ] 
Training: Epoch [66/120], Step [1499], Loss: 0.8361172676086426, Training Accuracy: 96.45
[ Thu Jul  4 00:00:22 2024 ] 	Batch(1500/7879) done. Loss: 0.1768  lr:0.000100
[ Thu Jul  4 00:00:40 2024 ] 	Batch(1600/7879) done. Loss: 0.0163  lr:0.000100
[ Thu Jul  4 00:00:58 2024 ] 	Batch(1700/7879) done. Loss: 0.1272  lr:0.000100
[ Thu Jul  4 00:01:16 2024 ] 	Batch(1800/7879) done. Loss: 0.0605  lr:0.000100
[ Thu Jul  4 00:01:34 2024 ] 	Batch(1900/7879) done. Loss: 0.1038  lr:0.000100
[ Thu Jul  4 00:01:51 2024 ] 
Training: Epoch [66/120], Step [1999], Loss: 0.014660132117569447, Training Accuracy: 96.3625
[ Thu Jul  4 00:01:52 2024 ] 	Batch(2000/7879) done. Loss: 0.3460  lr:0.000100
[ Thu Jul  4 00:02:10 2024 ] 	Batch(2100/7879) done. Loss: 0.0521  lr:0.000100
[ Thu Jul  4 00:02:28 2024 ] 	Batch(2200/7879) done. Loss: 0.0973  lr:0.000100
[ Thu Jul  4 00:02:46 2024 ] 	Batch(2300/7879) done. Loss: 0.2727  lr:0.000100
[ Thu Jul  4 00:03:04 2024 ] 	Batch(2400/7879) done. Loss: 0.1726  lr:0.000100
[ Thu Jul  4 00:03:22 2024 ] 
Training: Epoch [66/120], Step [2499], Loss: 0.056630250066518784, Training Accuracy: 96.35000000000001
[ Thu Jul  4 00:03:22 2024 ] 	Batch(2500/7879) done. Loss: 0.0222  lr:0.000100
[ Thu Jul  4 00:03:41 2024 ] 	Batch(2600/7879) done. Loss: 0.0398  lr:0.000100
[ Thu Jul  4 00:03:59 2024 ] 	Batch(2700/7879) done. Loss: 0.5241  lr:0.000100
[ Thu Jul  4 00:04:17 2024 ] 	Batch(2800/7879) done. Loss: 0.0081  lr:0.000100
[ Thu Jul  4 00:04:35 2024 ] 	Batch(2900/7879) done. Loss: 0.0332  lr:0.000100
[ Thu Jul  4 00:04:53 2024 ] 
Training: Epoch [66/120], Step [2999], Loss: 0.3370799720287323, Training Accuracy: 96.39583333333334
[ Thu Jul  4 00:04:54 2024 ] 	Batch(3000/7879) done. Loss: 0.0221  lr:0.000100
[ Thu Jul  4 00:05:12 2024 ] 	Batch(3100/7879) done. Loss: 0.0272  lr:0.000100
[ Thu Jul  4 00:05:31 2024 ] 	Batch(3200/7879) done. Loss: 0.0533  lr:0.000100
[ Thu Jul  4 00:05:49 2024 ] 	Batch(3300/7879) done. Loss: 0.0580  lr:0.000100
[ Thu Jul  4 00:06:06 2024 ] 	Batch(3400/7879) done. Loss: 0.0164  lr:0.000100
[ Thu Jul  4 00:06:24 2024 ] 
Training: Epoch [66/120], Step [3499], Loss: 0.004475535824894905, Training Accuracy: 96.37857142857142
[ Thu Jul  4 00:06:24 2024 ] 	Batch(3500/7879) done. Loss: 0.2676  lr:0.000100
[ Thu Jul  4 00:06:42 2024 ] 	Batch(3600/7879) done. Loss: 0.0670  lr:0.000100
[ Thu Jul  4 00:07:00 2024 ] 	Batch(3700/7879) done. Loss: 0.0093  lr:0.000100
[ Thu Jul  4 00:07:18 2024 ] 	Batch(3800/7879) done. Loss: 0.2782  lr:0.000100
[ Thu Jul  4 00:07:36 2024 ] 	Batch(3900/7879) done. Loss: 0.1032  lr:0.000100
[ Thu Jul  4 00:07:54 2024 ] 
Training: Epoch [66/120], Step [3999], Loss: 0.0764756128191948, Training Accuracy: 96.434375
[ Thu Jul  4 00:07:54 2024 ] 	Batch(4000/7879) done. Loss: 0.3490  lr:0.000100
[ Thu Jul  4 00:08:12 2024 ] 	Batch(4100/7879) done. Loss: 0.4075  lr:0.000100
[ Thu Jul  4 00:08:30 2024 ] 	Batch(4200/7879) done. Loss: 0.0637  lr:0.000100
[ Thu Jul  4 00:08:48 2024 ] 	Batch(4300/7879) done. Loss: 0.1021  lr:0.000100
[ Thu Jul  4 00:09:06 2024 ] 	Batch(4400/7879) done. Loss: 0.0154  lr:0.000100
[ Thu Jul  4 00:09:24 2024 ] 
Training: Epoch [66/120], Step [4499], Loss: 0.1274791657924652, Training Accuracy: 96.41666666666666
[ Thu Jul  4 00:09:24 2024 ] 	Batch(4500/7879) done. Loss: 0.0802  lr:0.000100
[ Thu Jul  4 00:09:42 2024 ] 	Batch(4600/7879) done. Loss: 0.1174  lr:0.000100
[ Thu Jul  4 00:10:00 2024 ] 	Batch(4700/7879) done. Loss: 0.0123  lr:0.000100
[ Thu Jul  4 00:10:18 2024 ] 	Batch(4800/7879) done. Loss: 0.0044  lr:0.000100
[ Thu Jul  4 00:10:36 2024 ] 	Batch(4900/7879) done. Loss: 0.0570  lr:0.000100
[ Thu Jul  4 00:10:55 2024 ] 
Training: Epoch [66/120], Step [4999], Loss: 0.017356693744659424, Training Accuracy: 96.3925
[ Thu Jul  4 00:10:55 2024 ] 	Batch(5000/7879) done. Loss: 0.0608  lr:0.000100
[ Thu Jul  4 00:11:13 2024 ] 	Batch(5100/7879) done. Loss: 0.1381  lr:0.000100
[ Thu Jul  4 00:11:32 2024 ] 	Batch(5200/7879) done. Loss: 0.4032  lr:0.000100
[ Thu Jul  4 00:11:50 2024 ] 	Batch(5300/7879) done. Loss: 0.0014  lr:0.000100
[ Thu Jul  4 00:12:09 2024 ] 	Batch(5400/7879) done. Loss: 0.0525  lr:0.000100
[ Thu Jul  4 00:12:27 2024 ] 
Training: Epoch [66/120], Step [5499], Loss: 0.018824925646185875, Training Accuracy: 96.41363636363637
[ Thu Jul  4 00:12:28 2024 ] 	Batch(5500/7879) done. Loss: 0.5170  lr:0.000100
[ Thu Jul  4 00:12:46 2024 ] 	Batch(5600/7879) done. Loss: 0.0155  lr:0.000100
[ Thu Jul  4 00:13:05 2024 ] 	Batch(5700/7879) done. Loss: 0.1049  lr:0.000100
[ Thu Jul  4 00:13:23 2024 ] 	Batch(5800/7879) done. Loss: 0.0486  lr:0.000100
[ Thu Jul  4 00:13:42 2024 ] 	Batch(5900/7879) done. Loss: 0.0361  lr:0.000100
[ Thu Jul  4 00:14:00 2024 ] 
Training: Epoch [66/120], Step [5999], Loss: 0.021693293005228043, Training Accuracy: 96.38541666666667
[ Thu Jul  4 00:14:00 2024 ] 	Batch(6000/7879) done. Loss: 0.1861  lr:0.000100
[ Thu Jul  4 00:14:18 2024 ] 	Batch(6100/7879) done. Loss: 0.0266  lr:0.000100
[ Thu Jul  4 00:14:36 2024 ] 	Batch(6200/7879) done. Loss: 0.2826  lr:0.000100
[ Thu Jul  4 00:14:54 2024 ] 	Batch(6300/7879) done. Loss: 0.3360  lr:0.000100
[ Thu Jul  4 00:15:12 2024 ] 	Batch(6400/7879) done. Loss: 0.0247  lr:0.000100
[ Thu Jul  4 00:15:30 2024 ] 
Training: Epoch [66/120], Step [6499], Loss: 0.08694328367710114, Training Accuracy: 96.39423076923077
[ Thu Jul  4 00:15:30 2024 ] 	Batch(6500/7879) done. Loss: 0.1278  lr:0.000100
[ Thu Jul  4 00:15:48 2024 ] 	Batch(6600/7879) done. Loss: 0.1896  lr:0.000100
[ Thu Jul  4 00:16:06 2024 ] 	Batch(6700/7879) done. Loss: 0.0625  lr:0.000100
[ Thu Jul  4 00:16:24 2024 ] 	Batch(6800/7879) done. Loss: 0.3308  lr:0.000100
[ Thu Jul  4 00:16:42 2024 ] 	Batch(6900/7879) done. Loss: 0.9238  lr:0.000100
[ Thu Jul  4 00:16:59 2024 ] 
Training: Epoch [66/120], Step [6999], Loss: 0.24475830793380737, Training Accuracy: 96.39999999999999
[ Thu Jul  4 00:17:00 2024 ] 	Batch(7000/7879) done. Loss: 0.2069  lr:0.000100
[ Thu Jul  4 00:17:17 2024 ] 	Batch(7100/7879) done. Loss: 0.0131  lr:0.000100
[ Thu Jul  4 00:17:35 2024 ] 	Batch(7200/7879) done. Loss: 0.1373  lr:0.000100
[ Thu Jul  4 00:17:54 2024 ] 	Batch(7300/7879) done. Loss: 0.0037  lr:0.000100
[ Thu Jul  4 00:18:12 2024 ] 	Batch(7400/7879) done. Loss: 0.5873  lr:0.000100
[ Thu Jul  4 00:18:29 2024 ] 
Training: Epoch [66/120], Step [7499], Loss: 0.23357953131198883, Training Accuracy: 96.40166666666666
[ Thu Jul  4 00:18:30 2024 ] 	Batch(7500/7879) done. Loss: 0.0226  lr:0.000100
[ Thu Jul  4 00:18:48 2024 ] 	Batch(7600/7879) done. Loss: 0.0975  lr:0.000100
[ Thu Jul  4 00:19:06 2024 ] 	Batch(7700/7879) done. Loss: 0.2087  lr:0.000100
[ Thu Jul  4 00:19:25 2024 ] 	Batch(7800/7879) done. Loss: 0.1089  lr:0.000100
[ Thu Jul  4 00:19:39 2024 ] 	Mean training loss: 0.1298.
[ Thu Jul  4 00:19:39 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 00:19:39 2024 ] Training epoch: 68
[ Thu Jul  4 00:19:40 2024 ] 	Batch(0/7879) done. Loss: 0.0484  lr:0.000100
[ Thu Jul  4 00:19:58 2024 ] 	Batch(100/7879) done. Loss: 0.1390  lr:0.000100
[ Thu Jul  4 00:20:16 2024 ] 	Batch(200/7879) done. Loss: 0.2276  lr:0.000100
[ Thu Jul  4 00:20:34 2024 ] 	Batch(300/7879) done. Loss: 0.0148  lr:0.000100
[ Thu Jul  4 00:20:52 2024 ] 	Batch(400/7879) done. Loss: 0.0710  lr:0.000100
[ Thu Jul  4 00:21:09 2024 ] 
Training: Epoch [67/120], Step [499], Loss: 0.09676714241504669, Training Accuracy: 96.1
[ Thu Jul  4 00:21:09 2024 ] 	Batch(500/7879) done. Loss: 0.0549  lr:0.000100
[ Thu Jul  4 00:21:27 2024 ] 	Batch(600/7879) done. Loss: 0.1132  lr:0.000100
[ Thu Jul  4 00:21:45 2024 ] 	Batch(700/7879) done. Loss: 0.1277  lr:0.000100
[ Thu Jul  4 00:22:03 2024 ] 	Batch(800/7879) done. Loss: 0.1381  lr:0.000100
[ Thu Jul  4 00:22:21 2024 ] 	Batch(900/7879) done. Loss: 0.0038  lr:0.000100
[ Thu Jul  4 00:22:39 2024 ] 
Training: Epoch [67/120], Step [999], Loss: 0.12824107706546783, Training Accuracy: 96.2
[ Thu Jul  4 00:22:39 2024 ] 	Batch(1000/7879) done. Loss: 0.0303  lr:0.000100
[ Thu Jul  4 00:22:57 2024 ] 	Batch(1100/7879) done. Loss: 0.0823  lr:0.000100
[ Thu Jul  4 00:23:15 2024 ] 	Batch(1200/7879) done. Loss: 0.0852  lr:0.000100
[ Thu Jul  4 00:23:33 2024 ] 	Batch(1300/7879) done. Loss: 0.0298  lr:0.000100
[ Thu Jul  4 00:23:52 2024 ] 	Batch(1400/7879) done. Loss: 0.0854  lr:0.000100
[ Thu Jul  4 00:24:10 2024 ] 
Training: Epoch [67/120], Step [1499], Loss: 0.18958058953285217, Training Accuracy: 96.13333333333334
[ Thu Jul  4 00:24:11 2024 ] 	Batch(1500/7879) done. Loss: 0.1588  lr:0.000100
[ Thu Jul  4 00:24:29 2024 ] 	Batch(1600/7879) done. Loss: 0.1372  lr:0.000100
[ Thu Jul  4 00:24:47 2024 ] 	Batch(1700/7879) done. Loss: 0.2908  lr:0.000100
[ Thu Jul  4 00:25:05 2024 ] 	Batch(1800/7879) done. Loss: 0.0115  lr:0.000100
[ Thu Jul  4 00:25:23 2024 ] 	Batch(1900/7879) done. Loss: 0.3572  lr:0.000100
[ Thu Jul  4 00:25:41 2024 ] 
Training: Epoch [67/120], Step [1999], Loss: 0.267551988363266, Training Accuracy: 96.18124999999999
[ Thu Jul  4 00:25:41 2024 ] 	Batch(2000/7879) done. Loss: 0.0708  lr:0.000100
[ Thu Jul  4 00:25:59 2024 ] 	Batch(2100/7879) done. Loss: 0.0411  lr:0.000100
[ Thu Jul  4 00:26:17 2024 ] 	Batch(2200/7879) done. Loss: 0.0233  lr:0.000100
[ Thu Jul  4 00:26:34 2024 ] 	Batch(2300/7879) done. Loss: 0.1593  lr:0.000100
[ Thu Jul  4 00:26:52 2024 ] 	Batch(2400/7879) done. Loss: 0.2138  lr:0.000100
[ Thu Jul  4 00:27:10 2024 ] 
Training: Epoch [67/120], Step [2499], Loss: 0.050707824528217316, Training Accuracy: 96.35000000000001
[ Thu Jul  4 00:27:11 2024 ] 	Batch(2500/7879) done. Loss: 0.0046  lr:0.000100
[ Thu Jul  4 00:27:29 2024 ] 	Batch(2600/7879) done. Loss: 0.3492  lr:0.000100
[ Thu Jul  4 00:27:46 2024 ] 	Batch(2700/7879) done. Loss: 0.0910  lr:0.000100
[ Thu Jul  4 00:28:04 2024 ] 	Batch(2800/7879) done. Loss: 0.0151  lr:0.000100
[ Thu Jul  4 00:28:22 2024 ] 	Batch(2900/7879) done. Loss: 0.0378  lr:0.000100
[ Thu Jul  4 00:28:40 2024 ] 
Training: Epoch [67/120], Step [2999], Loss: 0.05095602571964264, Training Accuracy: 96.33749999999999
[ Thu Jul  4 00:28:40 2024 ] 	Batch(3000/7879) done. Loss: 0.0085  lr:0.000100
[ Thu Jul  4 00:28:58 2024 ] 	Batch(3100/7879) done. Loss: 0.0980  lr:0.000100
[ Thu Jul  4 00:29:16 2024 ] 	Batch(3200/7879) done. Loss: 0.0190  lr:0.000100
[ Thu Jul  4 00:29:34 2024 ] 	Batch(3300/7879) done. Loss: 0.1226  lr:0.000100
[ Thu Jul  4 00:29:52 2024 ] 	Batch(3400/7879) done. Loss: 0.0515  lr:0.000100
[ Thu Jul  4 00:30:10 2024 ] 
Training: Epoch [67/120], Step [3499], Loss: 0.014093819074332714, Training Accuracy: 96.35357142857143
[ Thu Jul  4 00:30:10 2024 ] 	Batch(3500/7879) done. Loss: 0.2142  lr:0.000100
[ Thu Jul  4 00:30:28 2024 ] 	Batch(3600/7879) done. Loss: 0.1058  lr:0.000100
[ Thu Jul  4 00:30:46 2024 ] 	Batch(3700/7879) done. Loss: 0.0174  lr:0.000100
[ Thu Jul  4 00:31:04 2024 ] 	Batch(3800/7879) done. Loss: 0.1058  lr:0.000100
[ Thu Jul  4 00:31:22 2024 ] 	Batch(3900/7879) done. Loss: 0.0518  lr:0.000100
[ Thu Jul  4 00:31:39 2024 ] 
Training: Epoch [67/120], Step [3999], Loss: 0.01311340183019638, Training Accuracy: 96.403125
[ Thu Jul  4 00:31:40 2024 ] 	Batch(4000/7879) done. Loss: 0.0369  lr:0.000100
[ Thu Jul  4 00:31:58 2024 ] 	Batch(4100/7879) done. Loss: 0.1019  lr:0.000100
[ Thu Jul  4 00:32:17 2024 ] 	Batch(4200/7879) done. Loss: 0.0159  lr:0.000100
[ Thu Jul  4 00:32:35 2024 ] 	Batch(4300/7879) done. Loss: 0.3386  lr:0.000100
[ Thu Jul  4 00:32:54 2024 ] 	Batch(4400/7879) done. Loss: 0.0355  lr:0.000100
[ Thu Jul  4 00:33:12 2024 ] 
Training: Epoch [67/120], Step [4499], Loss: 0.022273879498243332, Training Accuracy: 96.37777777777778
[ Thu Jul  4 00:33:12 2024 ] 	Batch(4500/7879) done. Loss: 0.3191  lr:0.000100
[ Thu Jul  4 00:33:31 2024 ] 	Batch(4600/7879) done. Loss: 0.4528  lr:0.000100
[ Thu Jul  4 00:33:49 2024 ] 	Batch(4700/7879) done. Loss: 0.1172  lr:0.000100
[ Thu Jul  4 00:34:08 2024 ] 	Batch(4800/7879) done. Loss: 0.0139  lr:0.000100
[ Thu Jul  4 00:34:26 2024 ] 	Batch(4900/7879) done. Loss: 0.2438  lr:0.000100
[ Thu Jul  4 00:34:45 2024 ] 
Training: Epoch [67/120], Step [4999], Loss: 0.01564575545489788, Training Accuracy: 96.385
[ Thu Jul  4 00:34:45 2024 ] 	Batch(5000/7879) done. Loss: 0.2780  lr:0.000100
[ Thu Jul  4 00:35:04 2024 ] 	Batch(5100/7879) done. Loss: 0.0938  lr:0.000100
[ Thu Jul  4 00:35:22 2024 ] 	Batch(5200/7879) done. Loss: 0.0114  lr:0.000100
[ Thu Jul  4 00:35:41 2024 ] 	Batch(5300/7879) done. Loss: 0.9062  lr:0.000100
[ Thu Jul  4 00:35:59 2024 ] 	Batch(5400/7879) done. Loss: 0.0031  lr:0.000100
[ Thu Jul  4 00:36:18 2024 ] 
Training: Epoch [67/120], Step [5499], Loss: 0.02666022628545761, Training Accuracy: 96.38636363636364
[ Thu Jul  4 00:36:18 2024 ] 	Batch(5500/7879) done. Loss: 0.2083  lr:0.000100
[ Thu Jul  4 00:36:36 2024 ] 	Batch(5600/7879) done. Loss: 0.1034  lr:0.000100
[ Thu Jul  4 00:36:55 2024 ] 	Batch(5700/7879) done. Loss: 0.0391  lr:0.000100
[ Thu Jul  4 00:37:13 2024 ] 	Batch(5800/7879) done. Loss: 0.0446  lr:0.000100
[ Thu Jul  4 00:37:32 2024 ] 	Batch(5900/7879) done. Loss: 0.0194  lr:0.000100
[ Thu Jul  4 00:37:50 2024 ] 
Training: Epoch [67/120], Step [5999], Loss: 0.04883953928947449, Training Accuracy: 96.42708333333333
[ Thu Jul  4 00:37:50 2024 ] 	Batch(6000/7879) done. Loss: 0.0628  lr:0.000100
[ Thu Jul  4 00:38:08 2024 ] 	Batch(6100/7879) done. Loss: 0.0120  lr:0.000100
[ Thu Jul  4 00:38:26 2024 ] 	Batch(6200/7879) done. Loss: 0.0503  lr:0.000100
[ Thu Jul  4 00:38:44 2024 ] 	Batch(6300/7879) done. Loss: 0.1833  lr:0.000100
[ Thu Jul  4 00:39:02 2024 ] 	Batch(6400/7879) done. Loss: 0.0422  lr:0.000100
[ Thu Jul  4 00:39:20 2024 ] 
Training: Epoch [67/120], Step [6499], Loss: 0.017577191814780235, Training Accuracy: 96.42884615384615
[ Thu Jul  4 00:39:20 2024 ] 	Batch(6500/7879) done. Loss: 0.0054  lr:0.000100
[ Thu Jul  4 00:39:38 2024 ] 	Batch(6600/7879) done. Loss: 0.1337  lr:0.000100
[ Thu Jul  4 00:39:56 2024 ] 	Batch(6700/7879) done. Loss: 0.0894  lr:0.000100
[ Thu Jul  4 00:40:14 2024 ] 	Batch(6800/7879) done. Loss: 0.0311  lr:0.000100
[ Thu Jul  4 00:40:32 2024 ] 	Batch(6900/7879) done. Loss: 0.1096  lr:0.000100
[ Thu Jul  4 00:40:49 2024 ] 
Training: Epoch [67/120], Step [6999], Loss: 0.5339581966400146, Training Accuracy: 96.44107142857142
[ Thu Jul  4 00:40:49 2024 ] 	Batch(7000/7879) done. Loss: 0.0635  lr:0.000100
[ Thu Jul  4 00:41:07 2024 ] 	Batch(7100/7879) done. Loss: 0.2433  lr:0.000100
[ Thu Jul  4 00:41:25 2024 ] 	Batch(7200/7879) done. Loss: 0.1259  lr:0.000100
[ Thu Jul  4 00:41:43 2024 ] 	Batch(7300/7879) done. Loss: 0.0043  lr:0.000100
[ Thu Jul  4 00:42:01 2024 ] 	Batch(7400/7879) done. Loss: 0.2767  lr:0.000100
[ Thu Jul  4 00:42:19 2024 ] 
Training: Epoch [67/120], Step [7499], Loss: 0.048360131680965424, Training Accuracy: 96.46000000000001
[ Thu Jul  4 00:42:19 2024 ] 	Batch(7500/7879) done. Loss: 0.0253  lr:0.000100
[ Thu Jul  4 00:42:37 2024 ] 	Batch(7600/7879) done. Loss: 0.1625  lr:0.000100
[ Thu Jul  4 00:42:55 2024 ] 	Batch(7700/7879) done. Loss: 0.1740  lr:0.000100
[ Thu Jul  4 00:43:13 2024 ] 	Batch(7800/7879) done. Loss: 0.0082  lr:0.000100
[ Thu Jul  4 00:43:27 2024 ] 	Mean training loss: 0.1262.
[ Thu Jul  4 00:43:27 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 00:43:27 2024 ] Training epoch: 69
[ Thu Jul  4 00:43:27 2024 ] 	Batch(0/7879) done. Loss: 0.4559  lr:0.000100
[ Thu Jul  4 00:43:45 2024 ] 	Batch(100/7879) done. Loss: 0.3247  lr:0.000100
[ Thu Jul  4 00:44:03 2024 ] 	Batch(200/7879) done. Loss: 0.0400  lr:0.000100
[ Thu Jul  4 00:44:21 2024 ] 	Batch(300/7879) done. Loss: 0.0843  lr:0.000100
[ Thu Jul  4 00:44:39 2024 ] 	Batch(400/7879) done. Loss: 0.2184  lr:0.000100
[ Thu Jul  4 00:44:57 2024 ] 
Training: Epoch [68/120], Step [499], Loss: 0.029534120112657547, Training Accuracy: 96.925
[ Thu Jul  4 00:44:58 2024 ] 	Batch(500/7879) done. Loss: 0.1997  lr:0.000100
[ Thu Jul  4 00:45:16 2024 ] 	Batch(600/7879) done. Loss: 0.0078  lr:0.000100
[ Thu Jul  4 00:45:33 2024 ] 	Batch(700/7879) done. Loss: 0.0200  lr:0.000100
[ Thu Jul  4 00:45:51 2024 ] 	Batch(800/7879) done. Loss: 0.3157  lr:0.000100
[ Thu Jul  4 00:46:10 2024 ] 	Batch(900/7879) done. Loss: 0.2009  lr:0.000100
[ Thu Jul  4 00:46:28 2024 ] 
Training: Epoch [68/120], Step [999], Loss: 0.019863324239850044, Training Accuracy: 97.0625
[ Thu Jul  4 00:46:28 2024 ] 	Batch(1000/7879) done. Loss: 0.1790  lr:0.000100
[ Thu Jul  4 00:46:47 2024 ] 	Batch(1100/7879) done. Loss: 0.5724  lr:0.000100
[ Thu Jul  4 00:47:06 2024 ] 	Batch(1200/7879) done. Loss: 0.0630  lr:0.000100
[ Thu Jul  4 00:47:24 2024 ] 	Batch(1300/7879) done. Loss: 0.0296  lr:0.000100
[ Thu Jul  4 00:47:43 2024 ] 	Batch(1400/7879) done. Loss: 0.0208  lr:0.000100
[ Thu Jul  4 00:48:01 2024 ] 
Training: Epoch [68/120], Step [1499], Loss: 0.07006053626537323, Training Accuracy: 96.95
[ Thu Jul  4 00:48:01 2024 ] 	Batch(1500/7879) done. Loss: 0.1084  lr:0.000100
[ Thu Jul  4 00:48:20 2024 ] 	Batch(1600/7879) done. Loss: 0.0060  lr:0.000100
[ Thu Jul  4 00:48:39 2024 ] 	Batch(1700/7879) done. Loss: 0.0829  lr:0.000100
[ Thu Jul  4 00:48:57 2024 ] 	Batch(1800/7879) done. Loss: 0.1368  lr:0.000100
[ Thu Jul  4 00:49:16 2024 ] 	Batch(1900/7879) done. Loss: 0.0576  lr:0.000100
[ Thu Jul  4 00:49:33 2024 ] 
Training: Epoch [68/120], Step [1999], Loss: 0.03857293725013733, Training Accuracy: 96.8625
[ Thu Jul  4 00:49:34 2024 ] 	Batch(2000/7879) done. Loss: 0.2134  lr:0.000100
[ Thu Jul  4 00:49:51 2024 ] 	Batch(2100/7879) done. Loss: 0.1036  lr:0.000100
[ Thu Jul  4 00:50:09 2024 ] 	Batch(2200/7879) done. Loss: 0.0497  lr:0.000100
[ Thu Jul  4 00:50:27 2024 ] 	Batch(2300/7879) done. Loss: 0.3637  lr:0.000100
[ Thu Jul  4 00:50:45 2024 ] 	Batch(2400/7879) done. Loss: 0.0098  lr:0.000100
[ Thu Jul  4 00:51:03 2024 ] 
Training: Epoch [68/120], Step [2499], Loss: 0.2960629165172577, Training Accuracy: 96.685
[ Thu Jul  4 00:51:03 2024 ] 	Batch(2500/7879) done. Loss: 0.1681  lr:0.000100
[ Thu Jul  4 00:51:21 2024 ] 	Batch(2600/7879) done. Loss: 0.0273  lr:0.000100
[ Thu Jul  4 00:51:39 2024 ] 	Batch(2700/7879) done. Loss: 0.0009  lr:0.000100
[ Thu Jul  4 00:51:57 2024 ] 	Batch(2800/7879) done. Loss: 0.3962  lr:0.000100
[ Thu Jul  4 00:52:15 2024 ] 	Batch(2900/7879) done. Loss: 0.1481  lr:0.000100
[ Thu Jul  4 00:52:32 2024 ] 
Training: Epoch [68/120], Step [2999], Loss: 0.07160302251577377, Training Accuracy: 96.625
[ Thu Jul  4 00:52:33 2024 ] 	Batch(3000/7879) done. Loss: 0.0411  lr:0.000100
[ Thu Jul  4 00:52:50 2024 ] 	Batch(3100/7879) done. Loss: 0.2184  lr:0.000100
[ Thu Jul  4 00:53:08 2024 ] 	Batch(3200/7879) done. Loss: 0.0424  lr:0.000100
[ Thu Jul  4 00:53:26 2024 ] 	Batch(3300/7879) done. Loss: 0.0393  lr:0.000100
[ Thu Jul  4 00:53:44 2024 ] 	Batch(3400/7879) done. Loss: 0.0440  lr:0.000100
[ Thu Jul  4 00:54:02 2024 ] 
Training: Epoch [68/120], Step [3499], Loss: 0.6277994513511658, Training Accuracy: 96.64642857142857
[ Thu Jul  4 00:54:02 2024 ] 	Batch(3500/7879) done. Loss: 0.0371  lr:0.000100
[ Thu Jul  4 00:54:20 2024 ] 	Batch(3600/7879) done. Loss: 0.0683  lr:0.000100
[ Thu Jul  4 00:54:38 2024 ] 	Batch(3700/7879) done. Loss: 0.2389  lr:0.000100
[ Thu Jul  4 00:54:56 2024 ] 	Batch(3800/7879) done. Loss: 0.0976  lr:0.000100
[ Thu Jul  4 00:55:14 2024 ] 	Batch(3900/7879) done. Loss: 0.0020  lr:0.000100
[ Thu Jul  4 00:55:32 2024 ] 
Training: Epoch [68/120], Step [3999], Loss: 0.021316252648830414, Training Accuracy: 96.659375
[ Thu Jul  4 00:55:32 2024 ] 	Batch(4000/7879) done. Loss: 0.0301  lr:0.000100
[ Thu Jul  4 00:55:50 2024 ] 	Batch(4100/7879) done. Loss: 0.0282  lr:0.000100
[ Thu Jul  4 00:56:08 2024 ] 	Batch(4200/7879) done. Loss: 0.0843  lr:0.000100
[ Thu Jul  4 00:56:25 2024 ] 	Batch(4300/7879) done. Loss: 0.2935  lr:0.000100
[ Thu Jul  4 00:56:43 2024 ] 	Batch(4400/7879) done. Loss: 0.0275  lr:0.000100
[ Thu Jul  4 00:57:01 2024 ] 
Training: Epoch [68/120], Step [4499], Loss: 0.3086131811141968, Training Accuracy: 96.63888888888889
[ Thu Jul  4 00:57:01 2024 ] 	Batch(4500/7879) done. Loss: 0.0567  lr:0.000100
[ Thu Jul  4 00:57:19 2024 ] 	Batch(4600/7879) done. Loss: 0.2036  lr:0.000100
[ Thu Jul  4 00:57:37 2024 ] 	Batch(4700/7879) done. Loss: 0.2823  lr:0.000100
[ Thu Jul  4 00:57:55 2024 ] 	Batch(4800/7879) done. Loss: 0.2191  lr:0.000100
[ Thu Jul  4 00:58:14 2024 ] 	Batch(4900/7879) done. Loss: 0.0135  lr:0.000100
[ Thu Jul  4 00:58:32 2024 ] 
Training: Epoch [68/120], Step [4999], Loss: 0.05578697472810745, Training Accuracy: 96.6325
[ Thu Jul  4 00:58:32 2024 ] 	Batch(5000/7879) done. Loss: 0.0428  lr:0.000100
[ Thu Jul  4 00:58:51 2024 ] 	Batch(5100/7879) done. Loss: 0.3741  lr:0.000100
[ Thu Jul  4 00:59:09 2024 ] 	Batch(5200/7879) done. Loss: 0.1302  lr:0.000100
[ Thu Jul  4 00:59:28 2024 ] 	Batch(5300/7879) done. Loss: 0.2179  lr:0.000100
[ Thu Jul  4 00:59:46 2024 ] 	Batch(5400/7879) done. Loss: 0.0075  lr:0.000100
[ Thu Jul  4 01:00:05 2024 ] 
Training: Epoch [68/120], Step [5499], Loss: 0.09561268240213394, Training Accuracy: 96.64772727272727
[ Thu Jul  4 01:00:05 2024 ] 	Batch(5500/7879) done. Loss: 0.0117  lr:0.000100
[ Thu Jul  4 01:00:23 2024 ] 	Batch(5600/7879) done. Loss: 0.0039  lr:0.000100
[ Thu Jul  4 01:00:41 2024 ] 	Batch(5700/7879) done. Loss: 0.2939  lr:0.000100
[ Thu Jul  4 01:00:59 2024 ] 	Batch(5800/7879) done. Loss: 0.3121  lr:0.000100
[ Thu Jul  4 01:01:17 2024 ] 	Batch(5900/7879) done. Loss: 0.0389  lr:0.000100
[ Thu Jul  4 01:01:35 2024 ] 
Training: Epoch [68/120], Step [5999], Loss: 0.1127040907740593, Training Accuracy: 96.69583333333334
[ Thu Jul  4 01:01:35 2024 ] 	Batch(6000/7879) done. Loss: 0.0136  lr:0.000100
[ Thu Jul  4 01:01:53 2024 ] 	Batch(6100/7879) done. Loss: 0.0127  lr:0.000100
[ Thu Jul  4 01:02:11 2024 ] 	Batch(6200/7879) done. Loss: 0.0159  lr:0.000100
[ Thu Jul  4 01:02:28 2024 ] 	Batch(6300/7879) done. Loss: 0.0164  lr:0.000100
[ Thu Jul  4 01:02:46 2024 ] 	Batch(6400/7879) done. Loss: 0.0443  lr:0.000100
[ Thu Jul  4 01:03:05 2024 ] 
Training: Epoch [68/120], Step [6499], Loss: 0.03633950650691986, Training Accuracy: 96.65769230769232
[ Thu Jul  4 01:03:05 2024 ] 	Batch(6500/7879) done. Loss: 0.0541  lr:0.000100
[ Thu Jul  4 01:03:24 2024 ] 	Batch(6600/7879) done. Loss: 0.1748  lr:0.000100
[ Thu Jul  4 01:03:42 2024 ] 	Batch(6700/7879) done. Loss: 0.0067  lr:0.000100
[ Thu Jul  4 01:04:01 2024 ] 	Batch(6800/7879) done. Loss: 0.0039  lr:0.000100
[ Thu Jul  4 01:04:19 2024 ] 	Batch(6900/7879) done. Loss: 0.0191  lr:0.000100
[ Thu Jul  4 01:04:37 2024 ] 
Training: Epoch [68/120], Step [6999], Loss: 0.1871608942747116, Training Accuracy: 96.675
[ Thu Jul  4 01:04:37 2024 ] 	Batch(7000/7879) done. Loss: 0.0625  lr:0.000100
[ Thu Jul  4 01:04:55 2024 ] 	Batch(7100/7879) done. Loss: 0.0282  lr:0.000100
[ Thu Jul  4 01:05:13 2024 ] 	Batch(7200/7879) done. Loss: 0.0179  lr:0.000100
[ Thu Jul  4 01:05:31 2024 ] 	Batch(7300/7879) done. Loss: 0.7635  lr:0.000100
[ Thu Jul  4 01:05:49 2024 ] 	Batch(7400/7879) done. Loss: 0.0373  lr:0.000100
[ Thu Jul  4 01:06:07 2024 ] 
Training: Epoch [68/120], Step [7499], Loss: 0.04161028563976288, Training Accuracy: 96.64333333333333
[ Thu Jul  4 01:06:07 2024 ] 	Batch(7500/7879) done. Loss: 0.1197  lr:0.000100
[ Thu Jul  4 01:06:25 2024 ] 	Batch(7600/7879) done. Loss: 0.0109  lr:0.000100
[ Thu Jul  4 01:06:43 2024 ] 	Batch(7700/7879) done. Loss: 0.0229  lr:0.000100
[ Thu Jul  4 01:07:02 2024 ] 	Batch(7800/7879) done. Loss: 0.0029  lr:0.000100
[ Thu Jul  4 01:07:16 2024 ] 	Mean training loss: 0.1257.
[ Thu Jul  4 01:07:16 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 01:07:16 2024 ] Training epoch: 70
[ Thu Jul  4 01:07:16 2024 ] 	Batch(0/7879) done. Loss: 0.0856  lr:0.000100
[ Thu Jul  4 01:07:34 2024 ] 	Batch(100/7879) done. Loss: 0.1385  lr:0.000100
[ Thu Jul  4 01:07:52 2024 ] 	Batch(200/7879) done. Loss: 0.3860  lr:0.000100
[ Thu Jul  4 01:08:10 2024 ] 	Batch(300/7879) done. Loss: 0.0071  lr:0.000100
[ Thu Jul  4 01:08:28 2024 ] 	Batch(400/7879) done. Loss: 0.0127  lr:0.000100
[ Thu Jul  4 01:08:46 2024 ] 
Training: Epoch [69/120], Step [499], Loss: 0.0689878985285759, Training Accuracy: 97.05
[ Thu Jul  4 01:08:46 2024 ] 	Batch(500/7879) done. Loss: 0.1600  lr:0.000100
[ Thu Jul  4 01:09:04 2024 ] 	Batch(600/7879) done. Loss: 0.0841  lr:0.000100
[ Thu Jul  4 01:09:22 2024 ] 	Batch(700/7879) done. Loss: 0.3766  lr:0.000100
[ Thu Jul  4 01:09:40 2024 ] 	Batch(800/7879) done. Loss: 0.0045  lr:0.000100
[ Thu Jul  4 01:09:57 2024 ] 	Batch(900/7879) done. Loss: 0.0683  lr:0.000100
[ Thu Jul  4 01:10:15 2024 ] 
Training: Epoch [69/120], Step [999], Loss: 0.1599847376346588, Training Accuracy: 96.8125
[ Thu Jul  4 01:10:15 2024 ] 	Batch(1000/7879) done. Loss: 0.1512  lr:0.000100
[ Thu Jul  4 01:10:33 2024 ] 	Batch(1100/7879) done. Loss: 0.1114  lr:0.000100
[ Thu Jul  4 01:10:51 2024 ] 	Batch(1200/7879) done. Loss: 0.3829  lr:0.000100
[ Thu Jul  4 01:11:10 2024 ] 	Batch(1300/7879) done. Loss: 0.2582  lr:0.000100
[ Thu Jul  4 01:11:28 2024 ] 	Batch(1400/7879) done. Loss: 0.1081  lr:0.000100
[ Thu Jul  4 01:11:47 2024 ] 
Training: Epoch [69/120], Step [1499], Loss: 0.17994225025177002, Training Accuracy: 96.825
[ Thu Jul  4 01:11:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0935  lr:0.000100
[ Thu Jul  4 01:12:06 2024 ] 	Batch(1600/7879) done. Loss: 0.1595  lr:0.000100
[ Thu Jul  4 01:12:24 2024 ] 	Batch(1700/7879) done. Loss: 0.0417  lr:0.000100
[ Thu Jul  4 01:12:43 2024 ] 	Batch(1800/7879) done. Loss: 0.1995  lr:0.000100
[ Thu Jul  4 01:13:01 2024 ] 	Batch(1900/7879) done. Loss: 0.0344  lr:0.000100
[ Thu Jul  4 01:13:19 2024 ] 
Training: Epoch [69/120], Step [1999], Loss: 0.0180773064494133, Training Accuracy: 96.825
[ Thu Jul  4 01:13:20 2024 ] 	Batch(2000/7879) done. Loss: 0.0175  lr:0.000100
[ Thu Jul  4 01:13:38 2024 ] 	Batch(2100/7879) done. Loss: 0.0531  lr:0.000100
[ Thu Jul  4 01:13:56 2024 ] 	Batch(2200/7879) done. Loss: 0.0131  lr:0.000100
[ Thu Jul  4 01:14:14 2024 ] 	Batch(2300/7879) done. Loss: 0.0172  lr:0.000100
[ Thu Jul  4 01:14:32 2024 ] 	Batch(2400/7879) done. Loss: 0.2581  lr:0.000100
[ Thu Jul  4 01:14:51 2024 ] 
Training: Epoch [69/120], Step [2499], Loss: 0.14718198776245117, Training Accuracy: 96.85000000000001
[ Thu Jul  4 01:14:51 2024 ] 	Batch(2500/7879) done. Loss: 0.1449  lr:0.000100
[ Thu Jul  4 01:15:09 2024 ] 	Batch(2600/7879) done. Loss: 0.0332  lr:0.000100
[ Thu Jul  4 01:15:28 2024 ] 	Batch(2700/7879) done. Loss: 0.0034  lr:0.000100
[ Thu Jul  4 01:15:47 2024 ] 	Batch(2800/7879) done. Loss: 0.0821  lr:0.000100
[ Thu Jul  4 01:16:06 2024 ] 	Batch(2900/7879) done. Loss: 0.1738  lr:0.000100
[ Thu Jul  4 01:16:24 2024 ] 
Training: Epoch [69/120], Step [2999], Loss: 0.03310012072324753, Training Accuracy: 96.80416666666667
[ Thu Jul  4 01:16:24 2024 ] 	Batch(3000/7879) done. Loss: 0.0272  lr:0.000100
[ Thu Jul  4 01:16:42 2024 ] 	Batch(3100/7879) done. Loss: 0.0457  lr:0.000100
[ Thu Jul  4 01:17:00 2024 ] 	Batch(3200/7879) done. Loss: 0.1504  lr:0.000100
[ Thu Jul  4 01:17:18 2024 ] 	Batch(3300/7879) done. Loss: 0.5196  lr:0.000100
[ Thu Jul  4 01:17:36 2024 ] 	Batch(3400/7879) done. Loss: 0.0265  lr:0.000100
[ Thu Jul  4 01:17:54 2024 ] 
Training: Epoch [69/120], Step [3499], Loss: 0.07960619032382965, Training Accuracy: 96.80714285714286
[ Thu Jul  4 01:17:54 2024 ] 	Batch(3500/7879) done. Loss: 0.0026  lr:0.000100
[ Thu Jul  4 01:18:13 2024 ] 	Batch(3600/7879) done. Loss: 0.1555  lr:0.000100
[ Thu Jul  4 01:18:31 2024 ] 	Batch(3700/7879) done. Loss: 0.4738  lr:0.000100
[ Thu Jul  4 01:18:49 2024 ] 	Batch(3800/7879) done. Loss: 0.0212  lr:0.000100
[ Thu Jul  4 01:19:07 2024 ] 	Batch(3900/7879) done. Loss: 0.0573  lr:0.000100
[ Thu Jul  4 01:19:25 2024 ] 
Training: Epoch [69/120], Step [3999], Loss: 0.1594237983226776, Training Accuracy: 96.88125
[ Thu Jul  4 01:19:25 2024 ] 	Batch(4000/7879) done. Loss: 0.1747  lr:0.000100
[ Thu Jul  4 01:19:43 2024 ] 	Batch(4100/7879) done. Loss: 0.1513  lr:0.000100
[ Thu Jul  4 01:20:01 2024 ] 	Batch(4200/7879) done. Loss: 0.1229  lr:0.000100
[ Thu Jul  4 01:20:19 2024 ] 	Batch(4300/7879) done. Loss: 0.0791  lr:0.000100
[ Thu Jul  4 01:20:38 2024 ] 	Batch(4400/7879) done. Loss: 0.4764  lr:0.000100
[ Thu Jul  4 01:20:56 2024 ] 
Training: Epoch [69/120], Step [4499], Loss: 0.011003018356859684, Training Accuracy: 96.85555555555555
[ Thu Jul  4 01:20:56 2024 ] 	Batch(4500/7879) done. Loss: 0.0274  lr:0.000100
[ Thu Jul  4 01:21:15 2024 ] 	Batch(4600/7879) done. Loss: 0.1062  lr:0.000100
[ Thu Jul  4 01:21:33 2024 ] 	Batch(4700/7879) done. Loss: 0.0106  lr:0.000100
[ Thu Jul  4 01:21:52 2024 ] 	Batch(4800/7879) done. Loss: 0.0640  lr:0.000100
[ Thu Jul  4 01:22:10 2024 ] 	Batch(4900/7879) done. Loss: 0.0129  lr:0.000100
[ Thu Jul  4 01:22:28 2024 ] 
Training: Epoch [69/120], Step [4999], Loss: 0.06283222883939743, Training Accuracy: 96.83
[ Thu Jul  4 01:22:28 2024 ] 	Batch(5000/7879) done. Loss: 0.0123  lr:0.000100
[ Thu Jul  4 01:22:46 2024 ] 	Batch(5100/7879) done. Loss: 0.0339  lr:0.000100
[ Thu Jul  4 01:23:04 2024 ] 	Batch(5200/7879) done. Loss: 0.1481  lr:0.000100
[ Thu Jul  4 01:23:22 2024 ] 	Batch(5300/7879) done. Loss: 0.0582  lr:0.000100
[ Thu Jul  4 01:23:39 2024 ] 	Batch(5400/7879) done. Loss: 0.2542  lr:0.000100
[ Thu Jul  4 01:23:57 2024 ] 
Training: Epoch [69/120], Step [5499], Loss: 0.08906841278076172, Training Accuracy: 96.83636363636363
[ Thu Jul  4 01:23:57 2024 ] 	Batch(5500/7879) done. Loss: 0.0022  lr:0.000100
[ Thu Jul  4 01:24:15 2024 ] 	Batch(5600/7879) done. Loss: 0.1672  lr:0.000100
[ Thu Jul  4 01:24:33 2024 ] 	Batch(5700/7879) done. Loss: 0.0309  lr:0.000100
[ Thu Jul  4 01:24:51 2024 ] 	Batch(5800/7879) done. Loss: 0.1432  lr:0.000100
[ Thu Jul  4 01:25:09 2024 ] 	Batch(5900/7879) done. Loss: 0.0055  lr:0.000100
[ Thu Jul  4 01:25:27 2024 ] 
Training: Epoch [69/120], Step [5999], Loss: 0.03787701949477196, Training Accuracy: 96.84375
[ Thu Jul  4 01:25:27 2024 ] 	Batch(6000/7879) done. Loss: 0.1918  lr:0.000100
[ Thu Jul  4 01:25:45 2024 ] 	Batch(6100/7879) done. Loss: 0.2108  lr:0.000100
[ Thu Jul  4 01:26:03 2024 ] 	Batch(6200/7879) done. Loss: 0.6731  lr:0.000100
[ Thu Jul  4 01:26:21 2024 ] 	Batch(6300/7879) done. Loss: 0.0381  lr:0.000100
[ Thu Jul  4 01:26:39 2024 ] 	Batch(6400/7879) done. Loss: 0.0799  lr:0.000100
[ Thu Jul  4 01:26:56 2024 ] 
Training: Epoch [69/120], Step [6499], Loss: 0.014268778264522552, Training Accuracy: 96.80384615384615
[ Thu Jul  4 01:26:56 2024 ] 	Batch(6500/7879) done. Loss: 0.1783  lr:0.000100
[ Thu Jul  4 01:27:14 2024 ] 	Batch(6600/7879) done. Loss: 0.1370  lr:0.000100
[ Thu Jul  4 01:27:32 2024 ] 	Batch(6700/7879) done. Loss: 0.3621  lr:0.000100
[ Thu Jul  4 01:27:50 2024 ] 	Batch(6800/7879) done. Loss: 0.0316  lr:0.000100
[ Thu Jul  4 01:28:08 2024 ] 	Batch(6900/7879) done. Loss: 0.1113  lr:0.000100
[ Thu Jul  4 01:28:26 2024 ] 
Training: Epoch [69/120], Step [6999], Loss: 0.05348782241344452, Training Accuracy: 96.76428571428572
[ Thu Jul  4 01:28:26 2024 ] 	Batch(7000/7879) done. Loss: 0.0530  lr:0.000100
[ Thu Jul  4 01:28:44 2024 ] 	Batch(7100/7879) done. Loss: 0.0304  lr:0.000100
[ Thu Jul  4 01:29:02 2024 ] 	Batch(7200/7879) done. Loss: 0.0548  lr:0.000100
[ Thu Jul  4 01:29:20 2024 ] 	Batch(7300/7879) done. Loss: 0.1404  lr:0.000100
[ Thu Jul  4 01:29:39 2024 ] 	Batch(7400/7879) done. Loss: 0.0873  lr:0.000100
[ Thu Jul  4 01:29:57 2024 ] 
Training: Epoch [69/120], Step [7499], Loss: 0.5238546133041382, Training Accuracy: 96.75333333333333
[ Thu Jul  4 01:29:57 2024 ] 	Batch(7500/7879) done. Loss: 0.0433  lr:0.000100
[ Thu Jul  4 01:30:15 2024 ] 	Batch(7600/7879) done. Loss: 0.2364  lr:0.000100
[ Thu Jul  4 01:30:33 2024 ] 	Batch(7700/7879) done. Loss: 0.0333  lr:0.000100
[ Thu Jul  4 01:30:51 2024 ] 	Batch(7800/7879) done. Loss: 0.1939  lr:0.000100
[ Thu Jul  4 01:31:05 2024 ] 	Mean training loss: 0.1245.
[ Thu Jul  4 01:31:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 01:31:05 2024 ] Eval epoch: 70
[ Thu Jul  4 01:35:52 2024 ] 	Mean val loss of 6365 batches: 1.7801443175564426.
[ Thu Jul  4 01:35:52 2024 ] Training epoch: 71
[ Thu Jul  4 01:35:52 2024 ] 	Batch(0/7879) done. Loss: 0.0980  lr:0.000100
[ Thu Jul  4 01:36:10 2024 ] 	Batch(100/7879) done. Loss: 0.2461  lr:0.000100
[ Thu Jul  4 01:36:28 2024 ] 	Batch(200/7879) done. Loss: 0.3660  lr:0.000100
[ Thu Jul  4 01:36:46 2024 ] 	Batch(300/7879) done. Loss: 0.0125  lr:0.000100
[ Thu Jul  4 01:37:04 2024 ] 	Batch(400/7879) done. Loss: 0.0979  lr:0.000100
[ Thu Jul  4 01:37:22 2024 ] 
Training: Epoch [70/120], Step [499], Loss: 0.1750480681657791, Training Accuracy: 96.675
[ Thu Jul  4 01:37:22 2024 ] 	Batch(500/7879) done. Loss: 0.0966  lr:0.000100
[ Thu Jul  4 01:37:40 2024 ] 	Batch(600/7879) done. Loss: 0.0402  lr:0.000100
[ Thu Jul  4 01:37:58 2024 ] 	Batch(700/7879) done. Loss: 0.0298  lr:0.000100
[ Thu Jul  4 01:38:16 2024 ] 	Batch(800/7879) done. Loss: 0.0539  lr:0.000100
[ Thu Jul  4 01:38:34 2024 ] 	Batch(900/7879) done. Loss: 0.1192  lr:0.000100
[ Thu Jul  4 01:38:51 2024 ] 
Training: Epoch [70/120], Step [999], Loss: 0.08544278144836426, Training Accuracy: 96.7
[ Thu Jul  4 01:38:51 2024 ] 	Batch(1000/7879) done. Loss: 0.2661  lr:0.000100
[ Thu Jul  4 01:39:09 2024 ] 	Batch(1100/7879) done. Loss: 0.1260  lr:0.000100
[ Thu Jul  4 01:39:27 2024 ] 	Batch(1200/7879) done. Loss: 0.0467  lr:0.000100
[ Thu Jul  4 01:39:45 2024 ] 	Batch(1300/7879) done. Loss: 0.8100  lr:0.000100
[ Thu Jul  4 01:40:03 2024 ] 	Batch(1400/7879) done. Loss: 0.1235  lr:0.000100
[ Thu Jul  4 01:40:21 2024 ] 
Training: Epoch [70/120], Step [1499], Loss: 0.146056666970253, Training Accuracy: 96.675
[ Thu Jul  4 01:40:21 2024 ] 	Batch(1500/7879) done. Loss: 0.0190  lr:0.000100
[ Thu Jul  4 01:40:39 2024 ] 	Batch(1600/7879) done. Loss: 0.1399  lr:0.000100
[ Thu Jul  4 01:40:57 2024 ] 	Batch(1700/7879) done. Loss: 0.0225  lr:0.000100
[ Thu Jul  4 01:41:15 2024 ] 	Batch(1800/7879) done. Loss: 0.0038  lr:0.000100
[ Thu Jul  4 01:41:32 2024 ] 	Batch(1900/7879) done. Loss: 0.6090  lr:0.000100
[ Thu Jul  4 01:41:50 2024 ] 
Training: Epoch [70/120], Step [1999], Loss: 0.03774449974298477, Training Accuracy: 96.7375
[ Thu Jul  4 01:41:51 2024 ] 	Batch(2000/7879) done. Loss: 0.0168  lr:0.000100
[ Thu Jul  4 01:42:08 2024 ] 	Batch(2100/7879) done. Loss: 0.0559  lr:0.000100
[ Thu Jul  4 01:42:26 2024 ] 	Batch(2200/7879) done. Loss: 0.0131  lr:0.000100
[ Thu Jul  4 01:42:44 2024 ] 	Batch(2300/7879) done. Loss: 0.2337  lr:0.000100
[ Thu Jul  4 01:43:02 2024 ] 	Batch(2400/7879) done. Loss: 0.1449  lr:0.000100
[ Thu Jul  4 01:43:20 2024 ] 
Training: Epoch [70/120], Step [2499], Loss: 0.02898503839969635, Training Accuracy: 96.7
[ Thu Jul  4 01:43:21 2024 ] 	Batch(2500/7879) done. Loss: 0.0502  lr:0.000100
[ Thu Jul  4 01:43:39 2024 ] 	Batch(2600/7879) done. Loss: 0.0367  lr:0.000100
[ Thu Jul  4 01:43:58 2024 ] 	Batch(2700/7879) done. Loss: 0.1054  lr:0.000100
[ Thu Jul  4 01:44:16 2024 ] 	Batch(2800/7879) done. Loss: 0.0242  lr:0.000100
[ Thu Jul  4 01:44:34 2024 ] 	Batch(2900/7879) done. Loss: 0.0027  lr:0.000100
[ Thu Jul  4 01:44:52 2024 ] 
Training: Epoch [70/120], Step [2999], Loss: 0.021647175773978233, Training Accuracy: 96.69583333333334
[ Thu Jul  4 01:44:52 2024 ] 	Batch(3000/7879) done. Loss: 0.1981  lr:0.000100
[ Thu Jul  4 01:45:10 2024 ] 	Batch(3100/7879) done. Loss: 0.1552  lr:0.000100
[ Thu Jul  4 01:45:28 2024 ] 	Batch(3200/7879) done. Loss: 0.0138  lr:0.000100
[ Thu Jul  4 01:45:46 2024 ] 	Batch(3300/7879) done. Loss: 0.0126  lr:0.000100
[ Thu Jul  4 01:46:04 2024 ] 	Batch(3400/7879) done. Loss: 0.1656  lr:0.000100
[ Thu Jul  4 01:46:21 2024 ] 
Training: Epoch [70/120], Step [3499], Loss: 0.09959933906793594, Training Accuracy: 96.74642857142857
[ Thu Jul  4 01:46:21 2024 ] 	Batch(3500/7879) done. Loss: 0.1587  lr:0.000100
[ Thu Jul  4 01:46:39 2024 ] 	Batch(3600/7879) done. Loss: 0.2660  lr:0.000100
[ Thu Jul  4 01:46:58 2024 ] 	Batch(3700/7879) done. Loss: 0.0543  lr:0.000100
[ Thu Jul  4 01:47:17 2024 ] 	Batch(3800/7879) done. Loss: 0.0091  lr:0.000100
[ Thu Jul  4 01:47:35 2024 ] 	Batch(3900/7879) done. Loss: 0.0420  lr:0.000100
[ Thu Jul  4 01:47:53 2024 ] 
Training: Epoch [70/120], Step [3999], Loss: 0.023031694814562798, Training Accuracy: 96.76875
[ Thu Jul  4 01:47:54 2024 ] 	Batch(4000/7879) done. Loss: 0.0689  lr:0.000100
[ Thu Jul  4 01:48:12 2024 ] 	Batch(4100/7879) done. Loss: 0.0101  lr:0.000100
[ Thu Jul  4 01:48:29 2024 ] 	Batch(4200/7879) done. Loss: 0.1365  lr:0.000100
[ Thu Jul  4 01:48:47 2024 ] 	Batch(4300/7879) done. Loss: 0.1140  lr:0.000100
[ Thu Jul  4 01:49:05 2024 ] 	Batch(4400/7879) done. Loss: 0.0409  lr:0.000100
[ Thu Jul  4 01:49:24 2024 ] 
Training: Epoch [70/120], Step [4499], Loss: 0.0696929469704628, Training Accuracy: 96.81111111111112
[ Thu Jul  4 01:49:24 2024 ] 	Batch(4500/7879) done. Loss: 0.0937  lr:0.000100
[ Thu Jul  4 01:49:42 2024 ] 	Batch(4600/7879) done. Loss: 0.1011  lr:0.000100
[ Thu Jul  4 01:50:01 2024 ] 	Batch(4700/7879) done. Loss: 0.0375  lr:0.000100
[ Thu Jul  4 01:50:19 2024 ] 	Batch(4800/7879) done. Loss: 0.0070  lr:0.000100
[ Thu Jul  4 01:50:38 2024 ] 	Batch(4900/7879) done. Loss: 0.0090  lr:0.000100
[ Thu Jul  4 01:50:56 2024 ] 
Training: Epoch [70/120], Step [4999], Loss: 0.008642381057143211, Training Accuracy: 96.7775
[ Thu Jul  4 01:50:56 2024 ] 	Batch(5000/7879) done. Loss: 0.0726  lr:0.000100
[ Thu Jul  4 01:51:15 2024 ] 	Batch(5100/7879) done. Loss: 0.0353  lr:0.000100
[ Thu Jul  4 01:51:34 2024 ] 	Batch(5200/7879) done. Loss: 0.0027  lr:0.000100
[ Thu Jul  4 01:51:52 2024 ] 	Batch(5300/7879) done. Loss: 0.0070  lr:0.000100
[ Thu Jul  4 01:52:11 2024 ] 	Batch(5400/7879) done. Loss: 0.0813  lr:0.000100
[ Thu Jul  4 01:52:29 2024 ] 
Training: Epoch [70/120], Step [5499], Loss: 0.022426147013902664, Training Accuracy: 96.75681818181818
[ Thu Jul  4 01:52:29 2024 ] 	Batch(5500/7879) done. Loss: 0.0276  lr:0.000100
[ Thu Jul  4 01:52:48 2024 ] 	Batch(5600/7879) done. Loss: 0.0295  lr:0.000100
[ Thu Jul  4 01:53:06 2024 ] 	Batch(5700/7879) done. Loss: 0.0175  lr:0.000100
[ Thu Jul  4 01:53:25 2024 ] 	Batch(5800/7879) done. Loss: 0.0074  lr:0.000100
[ Thu Jul  4 01:53:43 2024 ] 	Batch(5900/7879) done. Loss: 0.1796  lr:0.000100
[ Thu Jul  4 01:54:01 2024 ] 
Training: Epoch [70/120], Step [5999], Loss: 0.012200684286653996, Training Accuracy: 96.74374999999999
[ Thu Jul  4 01:54:01 2024 ] 	Batch(6000/7879) done. Loss: 0.0917  lr:0.000100
[ Thu Jul  4 01:54:20 2024 ] 	Batch(6100/7879) done. Loss: 0.0212  lr:0.000100
[ Thu Jul  4 01:54:38 2024 ] 	Batch(6200/7879) done. Loss: 0.0191  lr:0.000100
[ Thu Jul  4 01:54:57 2024 ] 	Batch(6300/7879) done. Loss: 0.0076  lr:0.000100
[ Thu Jul  4 01:55:15 2024 ] 	Batch(6400/7879) done. Loss: 0.1386  lr:0.000100
[ Thu Jul  4 01:55:34 2024 ] 
Training: Epoch [70/120], Step [6499], Loss: 0.12836402654647827, Training Accuracy: 96.73461538461538
[ Thu Jul  4 01:55:34 2024 ] 	Batch(6500/7879) done. Loss: 0.5667  lr:0.000100
[ Thu Jul  4 01:55:52 2024 ] 	Batch(6600/7879) done. Loss: 0.0449  lr:0.000100
[ Thu Jul  4 01:56:10 2024 ] 	Batch(6700/7879) done. Loss: 0.0398  lr:0.000100
[ Thu Jul  4 01:56:28 2024 ] 	Batch(6800/7879) done. Loss: 0.1153  lr:0.000100
[ Thu Jul  4 01:56:46 2024 ] 	Batch(6900/7879) done. Loss: 0.0225  lr:0.000100
[ Thu Jul  4 01:57:03 2024 ] 
Training: Epoch [70/120], Step [6999], Loss: 0.0664186030626297, Training Accuracy: 96.75178571428572
[ Thu Jul  4 01:57:03 2024 ] 	Batch(7000/7879) done. Loss: 0.3074  lr:0.000100
[ Thu Jul  4 01:57:21 2024 ] 	Batch(7100/7879) done. Loss: 0.0311  lr:0.000100
[ Thu Jul  4 01:57:39 2024 ] 	Batch(7200/7879) done. Loss: 0.2521  lr:0.000100
[ Thu Jul  4 01:57:58 2024 ] 	Batch(7300/7879) done. Loss: 0.0399  lr:0.000100
[ Thu Jul  4 01:58:16 2024 ] 	Batch(7400/7879) done. Loss: 0.1386  lr:0.000100
[ Thu Jul  4 01:58:35 2024 ] 
Training: Epoch [70/120], Step [7499], Loss: 0.008848432451486588, Training Accuracy: 96.74666666666667
[ Thu Jul  4 01:58:35 2024 ] 	Batch(7500/7879) done. Loss: 0.0182  lr:0.000100
[ Thu Jul  4 01:58:54 2024 ] 	Batch(7600/7879) done. Loss: 0.0353  lr:0.000100
[ Thu Jul  4 01:59:11 2024 ] 	Batch(7700/7879) done. Loss: 0.0313  lr:0.000100
[ Thu Jul  4 01:59:29 2024 ] 	Batch(7800/7879) done. Loss: 0.0216  lr:0.000100
[ Thu Jul  4 01:59:43 2024 ] 	Mean training loss: 0.1224.
[ Thu Jul  4 01:59:43 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 01:59:43 2024 ] Training epoch: 72
[ Thu Jul  4 01:59:44 2024 ] 	Batch(0/7879) done. Loss: 0.0024  lr:0.000100
[ Thu Jul  4 02:00:02 2024 ] 	Batch(100/7879) done. Loss: 0.0282  lr:0.000100
[ Thu Jul  4 02:00:20 2024 ] 	Batch(200/7879) done. Loss: 0.0145  lr:0.000100
[ Thu Jul  4 02:00:37 2024 ] 	Batch(300/7879) done. Loss: 0.0193  lr:0.000100
[ Thu Jul  4 02:00:55 2024 ] 	Batch(400/7879) done. Loss: 0.0269  lr:0.000100
[ Thu Jul  4 02:01:13 2024 ] 
Training: Epoch [71/120], Step [499], Loss: 0.14982688426971436, Training Accuracy: 96.775
[ Thu Jul  4 02:01:13 2024 ] 	Batch(500/7879) done. Loss: 0.1348  lr:0.000100
[ Thu Jul  4 02:01:31 2024 ] 	Batch(600/7879) done. Loss: 0.0118  lr:0.000100
[ Thu Jul  4 02:01:49 2024 ] 	Batch(700/7879) done. Loss: 0.3533  lr:0.000100
[ Thu Jul  4 02:02:07 2024 ] 	Batch(800/7879) done. Loss: 0.0774  lr:0.000100
[ Thu Jul  4 02:02:25 2024 ] 	Batch(900/7879) done. Loss: 0.0902  lr:0.000100
[ Thu Jul  4 02:02:44 2024 ] 
Training: Epoch [71/120], Step [999], Loss: 0.009757725521922112, Training Accuracy: 96.95
[ Thu Jul  4 02:02:44 2024 ] 	Batch(1000/7879) done. Loss: 0.0529  lr:0.000100
[ Thu Jul  4 02:03:03 2024 ] 	Batch(1100/7879) done. Loss: 0.0374  lr:0.000100
[ Thu Jul  4 02:03:21 2024 ] 	Batch(1200/7879) done. Loss: 0.0253  lr:0.000100
[ Thu Jul  4 02:03:40 2024 ] 	Batch(1300/7879) done. Loss: 0.0549  lr:0.000100
[ Thu Jul  4 02:03:58 2024 ] 	Batch(1400/7879) done. Loss: 0.0148  lr:0.000100
[ Thu Jul  4 02:04:16 2024 ] 
Training: Epoch [71/120], Step [1499], Loss: 0.5136098861694336, Training Accuracy: 96.88333333333333
[ Thu Jul  4 02:04:17 2024 ] 	Batch(1500/7879) done. Loss: 0.1786  lr:0.000100
[ Thu Jul  4 02:04:35 2024 ] 	Batch(1600/7879) done. Loss: 0.1242  lr:0.000100
[ Thu Jul  4 02:04:53 2024 ] 	Batch(1700/7879) done. Loss: 0.5062  lr:0.000100
[ Thu Jul  4 02:05:11 2024 ] 	Batch(1800/7879) done. Loss: 0.0235  lr:0.000100
[ Thu Jul  4 02:05:29 2024 ] 	Batch(1900/7879) done. Loss: 0.0505  lr:0.000100
[ Thu Jul  4 02:05:47 2024 ] 
Training: Epoch [71/120], Step [1999], Loss: 0.18699736893177032, Training Accuracy: 96.83125
[ Thu Jul  4 02:05:47 2024 ] 	Batch(2000/7879) done. Loss: 0.0890  lr:0.000100
[ Thu Jul  4 02:06:05 2024 ] 	Batch(2100/7879) done. Loss: 0.0125  lr:0.000100
[ Thu Jul  4 02:06:23 2024 ] 	Batch(2200/7879) done. Loss: 0.0107  lr:0.000100
[ Thu Jul  4 02:06:40 2024 ] 	Batch(2300/7879) done. Loss: 0.0175  lr:0.000100
[ Thu Jul  4 02:06:58 2024 ] 	Batch(2400/7879) done. Loss: 0.0246  lr:0.000100
[ Thu Jul  4 02:07:16 2024 ] 
Training: Epoch [71/120], Step [2499], Loss: 0.14699168503284454, Training Accuracy: 96.89
[ Thu Jul  4 02:07:17 2024 ] 	Batch(2500/7879) done. Loss: 0.0152  lr:0.000100
[ Thu Jul  4 02:07:34 2024 ] 	Batch(2600/7879) done. Loss: 0.0308  lr:0.000100
[ Thu Jul  4 02:07:52 2024 ] 	Batch(2700/7879) done. Loss: 0.0093  lr:0.000100
[ Thu Jul  4 02:08:10 2024 ] 	Batch(2800/7879) done. Loss: 0.0520  lr:0.000100
[ Thu Jul  4 02:08:28 2024 ] 	Batch(2900/7879) done. Loss: 0.4141  lr:0.000100
[ Thu Jul  4 02:08:46 2024 ] 
Training: Epoch [71/120], Step [2999], Loss: 0.03544911742210388, Training Accuracy: 96.9125
[ Thu Jul  4 02:08:46 2024 ] 	Batch(3000/7879) done. Loss: 0.2850  lr:0.000100
[ Thu Jul  4 02:09:04 2024 ] 	Batch(3100/7879) done. Loss: 0.0067  lr:0.000100
[ Thu Jul  4 02:09:22 2024 ] 	Batch(3200/7879) done. Loss: 0.0074  lr:0.000100
[ Thu Jul  4 02:09:40 2024 ] 	Batch(3300/7879) done. Loss: 0.1559  lr:0.000100
[ Thu Jul  4 02:09:59 2024 ] 	Batch(3400/7879) done. Loss: 0.0584  lr:0.000100
[ Thu Jul  4 02:10:17 2024 ] 
Training: Epoch [71/120], Step [3499], Loss: 0.15686261653900146, Training Accuracy: 96.93571428571428
[ Thu Jul  4 02:10:17 2024 ] 	Batch(3500/7879) done. Loss: 0.0719  lr:0.000100
[ Thu Jul  4 02:10:36 2024 ] 	Batch(3600/7879) done. Loss: 0.1128  lr:0.000100
[ Thu Jul  4 02:10:54 2024 ] 	Batch(3700/7879) done. Loss: 0.0351  lr:0.000100
[ Thu Jul  4 02:11:12 2024 ] 	Batch(3800/7879) done. Loss: 0.0084  lr:0.000100
[ Thu Jul  4 02:11:30 2024 ] 	Batch(3900/7879) done. Loss: 0.0468  lr:0.000100
[ Thu Jul  4 02:11:47 2024 ] 
Training: Epoch [71/120], Step [3999], Loss: 0.0030679674819111824, Training Accuracy: 97.0
[ Thu Jul  4 02:11:48 2024 ] 	Batch(4000/7879) done. Loss: 0.6523  lr:0.000100
[ Thu Jul  4 02:12:06 2024 ] 	Batch(4100/7879) done. Loss: 0.2347  lr:0.000100
[ Thu Jul  4 02:12:25 2024 ] 	Batch(4200/7879) done. Loss: 0.0391  lr:0.000100
[ Thu Jul  4 02:12:43 2024 ] 	Batch(4300/7879) done. Loss: 0.0450  lr:0.000100
[ Thu Jul  4 02:13:02 2024 ] 	Batch(4400/7879) done. Loss: 0.0123  lr:0.000100
[ Thu Jul  4 02:13:20 2024 ] 
Training: Epoch [71/120], Step [4499], Loss: 0.030102722346782684, Training Accuracy: 96.95555555555555
[ Thu Jul  4 02:13:20 2024 ] 	Batch(4500/7879) done. Loss: 0.0623  lr:0.000100
[ Thu Jul  4 02:13:38 2024 ] 	Batch(4600/7879) done. Loss: 0.1984  lr:0.000100
[ Thu Jul  4 02:13:56 2024 ] 	Batch(4700/7879) done. Loss: 0.1770  lr:0.000100
[ Thu Jul  4 02:14:14 2024 ] 	Batch(4800/7879) done. Loss: 0.6173  lr:0.000100
[ Thu Jul  4 02:14:32 2024 ] 	Batch(4900/7879) done. Loss: 0.0112  lr:0.000100
[ Thu Jul  4 02:14:51 2024 ] 
Training: Epoch [71/120], Step [4999], Loss: 0.21690526604652405, Training Accuracy: 96.89
[ Thu Jul  4 02:14:51 2024 ] 	Batch(5000/7879) done. Loss: 0.0390  lr:0.000100
[ Thu Jul  4 02:15:09 2024 ] 	Batch(5100/7879) done. Loss: 0.2527  lr:0.000100
[ Thu Jul  4 02:15:28 2024 ] 	Batch(5200/7879) done. Loss: 0.0201  lr:0.000100
[ Thu Jul  4 02:15:46 2024 ] 	Batch(5300/7879) done. Loss: 0.3471  lr:0.000100
[ Thu Jul  4 02:16:05 2024 ] 	Batch(5400/7879) done. Loss: 0.4548  lr:0.000100
[ Thu Jul  4 02:16:23 2024 ] 
Training: Epoch [71/120], Step [5499], Loss: 0.26759573817253113, Training Accuracy: 96.89545454545454
[ Thu Jul  4 02:16:23 2024 ] 	Batch(5500/7879) done. Loss: 0.1549  lr:0.000100
[ Thu Jul  4 02:16:42 2024 ] 	Batch(5600/7879) done. Loss: 0.1179  lr:0.000100
[ Thu Jul  4 02:17:00 2024 ] 	Batch(5700/7879) done. Loss: 0.2872  lr:0.000100
[ Thu Jul  4 02:17:18 2024 ] 	Batch(5800/7879) done. Loss: 0.0947  lr:0.000100
[ Thu Jul  4 02:17:36 2024 ] 	Batch(5900/7879) done. Loss: 0.1088  lr:0.000100
[ Thu Jul  4 02:17:53 2024 ] 
Training: Epoch [71/120], Step [5999], Loss: 0.015337459743022919, Training Accuracy: 96.88958333333333
[ Thu Jul  4 02:17:54 2024 ] 	Batch(6000/7879) done. Loss: 0.0187  lr:0.000100
[ Thu Jul  4 02:18:11 2024 ] 	Batch(6100/7879) done. Loss: 0.0853  lr:0.000100
[ Thu Jul  4 02:18:29 2024 ] 	Batch(6200/7879) done. Loss: 0.2177  lr:0.000100
[ Thu Jul  4 02:18:47 2024 ] 	Batch(6300/7879) done. Loss: 0.2412  lr:0.000100
[ Thu Jul  4 02:19:05 2024 ] 	Batch(6400/7879) done. Loss: 0.5330  lr:0.000100
[ Thu Jul  4 02:19:23 2024 ] 
Training: Epoch [71/120], Step [6499], Loss: 0.050163667649030685, Training Accuracy: 96.89038461538462
[ Thu Jul  4 02:19:23 2024 ] 	Batch(6500/7879) done. Loss: 0.3175  lr:0.000100
[ Thu Jul  4 02:19:41 2024 ] 	Batch(6600/7879) done. Loss: 0.0406  lr:0.000100
[ Thu Jul  4 02:19:59 2024 ] 	Batch(6700/7879) done. Loss: 0.0431  lr:0.000100
[ Thu Jul  4 02:20:17 2024 ] 	Batch(6800/7879) done. Loss: 0.0292  lr:0.000100
[ Thu Jul  4 02:20:35 2024 ] 	Batch(6900/7879) done. Loss: 0.1358  lr:0.000100
[ Thu Jul  4 02:20:52 2024 ] 
Training: Epoch [71/120], Step [6999], Loss: 0.022357773035764694, Training Accuracy: 96.88392857142857
[ Thu Jul  4 02:20:52 2024 ] 	Batch(7000/7879) done. Loss: 0.0013  lr:0.000100
[ Thu Jul  4 02:21:10 2024 ] 	Batch(7100/7879) done. Loss: 0.0425  lr:0.000100
[ Thu Jul  4 02:21:28 2024 ] 	Batch(7200/7879) done. Loss: 0.0062  lr:0.000100
[ Thu Jul  4 02:21:46 2024 ] 	Batch(7300/7879) done. Loss: 0.6851  lr:0.000100
[ Thu Jul  4 02:22:04 2024 ] 	Batch(7400/7879) done. Loss: 0.1031  lr:0.000100
[ Thu Jul  4 02:22:22 2024 ] 
Training: Epoch [71/120], Step [7499], Loss: 0.10146597772836685, Training Accuracy: 96.875
[ Thu Jul  4 02:22:22 2024 ] 	Batch(7500/7879) done. Loss: 0.0294  lr:0.000100
[ Thu Jul  4 02:22:40 2024 ] 	Batch(7600/7879) done. Loss: 0.0171  lr:0.000100
[ Thu Jul  4 02:22:59 2024 ] 	Batch(7700/7879) done. Loss: 0.0332  lr:0.000100
[ Thu Jul  4 02:23:17 2024 ] 	Batch(7800/7879) done. Loss: 0.1641  lr:0.000100
[ Thu Jul  4 02:23:31 2024 ] 	Mean training loss: 0.1195.
[ Thu Jul  4 02:23:31 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 02:23:31 2024 ] Training epoch: 73
[ Thu Jul  4 02:23:31 2024 ] 	Batch(0/7879) done. Loss: 0.1207  lr:0.000100
[ Thu Jul  4 02:23:49 2024 ] 	Batch(100/7879) done. Loss: 0.0122  lr:0.000100
[ Thu Jul  4 02:24:07 2024 ] 	Batch(200/7879) done. Loss: 0.0091  lr:0.000100
[ Thu Jul  4 02:24:25 2024 ] 	Batch(300/7879) done. Loss: 0.0841  lr:0.000100
[ Thu Jul  4 02:24:43 2024 ] 	Batch(400/7879) done. Loss: 0.0587  lr:0.000100
[ Thu Jul  4 02:25:01 2024 ] 
Training: Epoch [72/120], Step [499], Loss: 0.02877393737435341, Training Accuracy: 97.5
[ Thu Jul  4 02:25:01 2024 ] 	Batch(500/7879) done. Loss: 0.0102  lr:0.000100
[ Thu Jul  4 02:25:19 2024 ] 	Batch(600/7879) done. Loss: 0.1454  lr:0.000100
[ Thu Jul  4 02:25:36 2024 ] 	Batch(700/7879) done. Loss: 0.0737  lr:0.000100
[ Thu Jul  4 02:25:55 2024 ] 	Batch(800/7879) done. Loss: 0.0362  lr:0.000100
[ Thu Jul  4 02:26:13 2024 ] 	Batch(900/7879) done. Loss: 0.0204  lr:0.000100
[ Thu Jul  4 02:26:31 2024 ] 
Training: Epoch [72/120], Step [999], Loss: 0.3992233872413635, Training Accuracy: 97.25
[ Thu Jul  4 02:26:32 2024 ] 	Batch(1000/7879) done. Loss: 0.0504  lr:0.000100
[ Thu Jul  4 02:26:50 2024 ] 	Batch(1100/7879) done. Loss: 0.0450  lr:0.000100
[ Thu Jul  4 02:27:09 2024 ] 	Batch(1200/7879) done. Loss: 0.4170  lr:0.000100
[ Thu Jul  4 02:27:27 2024 ] 	Batch(1300/7879) done. Loss: 0.0269  lr:0.000100
[ Thu Jul  4 02:27:45 2024 ] 	Batch(1400/7879) done. Loss: 0.0163  lr:0.000100
[ Thu Jul  4 02:28:03 2024 ] 
Training: Epoch [72/120], Step [1499], Loss: 0.0801253467798233, Training Accuracy: 97.14166666666667
[ Thu Jul  4 02:28:03 2024 ] 	Batch(1500/7879) done. Loss: 0.0064  lr:0.000100
[ Thu Jul  4 02:28:21 2024 ] 	Batch(1600/7879) done. Loss: 0.1140  lr:0.000100
[ Thu Jul  4 02:28:39 2024 ] 	Batch(1700/7879) done. Loss: 0.0047  lr:0.000100
[ Thu Jul  4 02:28:57 2024 ] 	Batch(1800/7879) done. Loss: 0.1522  lr:0.000100
[ Thu Jul  4 02:29:16 2024 ] 	Batch(1900/7879) done. Loss: 0.1998  lr:0.000100
[ Thu Jul  4 02:29:33 2024 ] 
Training: Epoch [72/120], Step [1999], Loss: 0.09343373775482178, Training Accuracy: 97.05625
[ Thu Jul  4 02:29:34 2024 ] 	Batch(2000/7879) done. Loss: 0.1243  lr:0.000100
[ Thu Jul  4 02:29:52 2024 ] 	Batch(2100/7879) done. Loss: 0.3293  lr:0.000100
[ Thu Jul  4 02:30:09 2024 ] 	Batch(2200/7879) done. Loss: 0.2647  lr:0.000100
[ Thu Jul  4 02:30:27 2024 ] 	Batch(2300/7879) done. Loss: 0.0253  lr:0.000100
[ Thu Jul  4 02:30:45 2024 ] 	Batch(2400/7879) done. Loss: 0.0453  lr:0.000100
[ Thu Jul  4 02:31:03 2024 ] 
Training: Epoch [72/120], Step [2499], Loss: 0.06638864427804947, Training Accuracy: 97.085
[ Thu Jul  4 02:31:03 2024 ] 	Batch(2500/7879) done. Loss: 0.1073  lr:0.000100
[ Thu Jul  4 02:31:21 2024 ] 	Batch(2600/7879) done. Loss: 0.2200  lr:0.000100
[ Thu Jul  4 02:31:39 2024 ] 	Batch(2700/7879) done. Loss: 0.2030  lr:0.000100
[ Thu Jul  4 02:31:57 2024 ] 	Batch(2800/7879) done. Loss: 0.2163  lr:0.000100
[ Thu Jul  4 02:32:15 2024 ] 	Batch(2900/7879) done. Loss: 0.0071  lr:0.000100
[ Thu Jul  4 02:32:32 2024 ] 
Training: Epoch [72/120], Step [2999], Loss: 0.04514656215906143, Training Accuracy: 97.06666666666666
[ Thu Jul  4 02:32:33 2024 ] 	Batch(3000/7879) done. Loss: 0.0970  lr:0.000100
[ Thu Jul  4 02:32:50 2024 ] 	Batch(3100/7879) done. Loss: 0.0232  lr:0.000100
[ Thu Jul  4 02:33:08 2024 ] 	Batch(3200/7879) done. Loss: 0.1708  lr:0.000100
[ Thu Jul  4 02:33:26 2024 ] 	Batch(3300/7879) done. Loss: 0.2548  lr:0.000100
[ Thu Jul  4 02:33:44 2024 ] 	Batch(3400/7879) done. Loss: 0.0507  lr:0.000100
[ Thu Jul  4 02:34:02 2024 ] 
Training: Epoch [72/120], Step [3499], Loss: 0.02005438320338726, Training Accuracy: 97.04642857142856
[ Thu Jul  4 02:34:02 2024 ] 	Batch(3500/7879) done. Loss: 0.1195  lr:0.000100
[ Thu Jul  4 02:34:20 2024 ] 	Batch(3600/7879) done. Loss: 0.0822  lr:0.000100
[ Thu Jul  4 02:34:38 2024 ] 	Batch(3700/7879) done. Loss: 0.0470  lr:0.000100
[ Thu Jul  4 02:34:56 2024 ] 	Batch(3800/7879) done. Loss: 0.1044  lr:0.000100
[ Thu Jul  4 02:35:14 2024 ] 	Batch(3900/7879) done. Loss: 0.0380  lr:0.000100
[ Thu Jul  4 02:35:32 2024 ] 
Training: Epoch [72/120], Step [3999], Loss: 0.05795446038246155, Training Accuracy: 97.04374999999999
[ Thu Jul  4 02:35:32 2024 ] 	Batch(4000/7879) done. Loss: 0.0386  lr:0.000100
[ Thu Jul  4 02:35:51 2024 ] 	Batch(4100/7879) done. Loss: 0.0798  lr:0.000100
[ Thu Jul  4 02:36:09 2024 ] 	Batch(4200/7879) done. Loss: 0.0234  lr:0.000100
[ Thu Jul  4 02:36:28 2024 ] 	Batch(4300/7879) done. Loss: 0.1178  lr:0.000100
[ Thu Jul  4 02:36:46 2024 ] 	Batch(4400/7879) done. Loss: 0.2429  lr:0.000100
[ Thu Jul  4 02:37:05 2024 ] 
Training: Epoch [72/120], Step [4499], Loss: 0.055239781737327576, Training Accuracy: 97.05555555555556
[ Thu Jul  4 02:37:05 2024 ] 	Batch(4500/7879) done. Loss: 0.0377  lr:0.000100
[ Thu Jul  4 02:37:23 2024 ] 	Batch(4600/7879) done. Loss: 0.0772  lr:0.000100
[ Thu Jul  4 02:37:42 2024 ] 	Batch(4700/7879) done. Loss: 0.0044  lr:0.000100
[ Thu Jul  4 02:38:00 2024 ] 	Batch(4800/7879) done. Loss: 0.0211  lr:0.000100
[ Thu Jul  4 02:38:18 2024 ] 	Batch(4900/7879) done. Loss: 0.0317  lr:0.000100
[ Thu Jul  4 02:38:36 2024 ] 
Training: Epoch [72/120], Step [4999], Loss: 0.00834638811647892, Training Accuracy: 97.0425
[ Thu Jul  4 02:38:36 2024 ] 	Batch(5000/7879) done. Loss: 0.0774  lr:0.000100
[ Thu Jul  4 02:38:55 2024 ] 	Batch(5100/7879) done. Loss: 0.1341  lr:0.000100
[ Thu Jul  4 02:39:14 2024 ] 	Batch(5200/7879) done. Loss: 0.1651  lr:0.000100
[ Thu Jul  4 02:39:32 2024 ] 	Batch(5300/7879) done. Loss: 0.0148  lr:0.000100
[ Thu Jul  4 02:39:51 2024 ] 	Batch(5400/7879) done. Loss: 0.0394  lr:0.000100
[ Thu Jul  4 02:40:09 2024 ] 
Training: Epoch [72/120], Step [5499], Loss: 0.00573747931048274, Training Accuracy: 97.03863636363637
[ Thu Jul  4 02:40:09 2024 ] 	Batch(5500/7879) done. Loss: 0.0255  lr:0.000100
[ Thu Jul  4 02:40:27 2024 ] 	Batch(5600/7879) done. Loss: 0.1984  lr:0.000100
[ Thu Jul  4 02:40:45 2024 ] 	Batch(5700/7879) done. Loss: 0.1035  lr:0.000100
[ Thu Jul  4 02:41:03 2024 ] 	Batch(5800/7879) done. Loss: 0.0264  lr:0.000100
[ Thu Jul  4 02:41:21 2024 ] 	Batch(5900/7879) done. Loss: 0.2575  lr:0.000100
[ Thu Jul  4 02:41:39 2024 ] 
Training: Epoch [72/120], Step [5999], Loss: 0.3982235789299011, Training Accuracy: 97.01875
[ Thu Jul  4 02:41:39 2024 ] 	Batch(6000/7879) done. Loss: 0.2550  lr:0.000100
[ Thu Jul  4 02:41:57 2024 ] 	Batch(6100/7879) done. Loss: 0.2339  lr:0.000100
[ Thu Jul  4 02:42:16 2024 ] 	Batch(6200/7879) done. Loss: 0.0647  lr:0.000100
[ Thu Jul  4 02:42:34 2024 ] 	Batch(6300/7879) done. Loss: 0.0660  lr:0.000100
[ Thu Jul  4 02:42:53 2024 ] 	Batch(6400/7879) done. Loss: 0.1871  lr:0.000100
[ Thu Jul  4 02:43:11 2024 ] 
Training: Epoch [72/120], Step [6499], Loss: 0.12633034586906433, Training Accuracy: 97.01923076923077
[ Thu Jul  4 02:43:11 2024 ] 	Batch(6500/7879) done. Loss: 0.0218  lr:0.000100
[ Thu Jul  4 02:43:29 2024 ] 	Batch(6600/7879) done. Loss: 0.0303  lr:0.000100
[ Thu Jul  4 02:43:47 2024 ] 	Batch(6700/7879) done. Loss: 0.1479  lr:0.000100
[ Thu Jul  4 02:44:05 2024 ] 	Batch(6800/7879) done. Loss: 0.0024  lr:0.000100
[ Thu Jul  4 02:44:23 2024 ] 	Batch(6900/7879) done. Loss: 0.0154  lr:0.000100
[ Thu Jul  4 02:44:40 2024 ] 
Training: Epoch [72/120], Step [6999], Loss: 0.0509478785097599, Training Accuracy: 97.01785714285714
[ Thu Jul  4 02:44:40 2024 ] 	Batch(7000/7879) done. Loss: 0.0529  lr:0.000100
[ Thu Jul  4 02:44:58 2024 ] 	Batch(7100/7879) done. Loss: 0.0041  lr:0.000100
[ Thu Jul  4 02:45:16 2024 ] 	Batch(7200/7879) done. Loss: 0.0135  lr:0.000100
[ Thu Jul  4 02:45:34 2024 ] 	Batch(7300/7879) done. Loss: 0.1366  lr:0.000100
[ Thu Jul  4 02:45:52 2024 ] 	Batch(7400/7879) done. Loss: 0.1980  lr:0.000100
[ Thu Jul  4 02:46:10 2024 ] 
Training: Epoch [72/120], Step [7499], Loss: 0.014566810801625252, Training Accuracy: 97.015
[ Thu Jul  4 02:46:10 2024 ] 	Batch(7500/7879) done. Loss: 0.0340  lr:0.000100
[ Thu Jul  4 02:46:28 2024 ] 	Batch(7600/7879) done. Loss: 0.0124  lr:0.000100
[ Thu Jul  4 02:46:46 2024 ] 	Batch(7700/7879) done. Loss: 0.0833  lr:0.000100
[ Thu Jul  4 02:47:03 2024 ] 	Batch(7800/7879) done. Loss: 0.0265  lr:0.000100
[ Thu Jul  4 02:47:18 2024 ] 	Mean training loss: 0.1143.
[ Thu Jul  4 02:47:18 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 02:47:18 2024 ] Training epoch: 74
[ Thu Jul  4 02:47:18 2024 ] 	Batch(0/7879) done. Loss: 0.2922  lr:0.000100
[ Thu Jul  4 02:47:36 2024 ] 	Batch(100/7879) done. Loss: 0.2528  lr:0.000100
[ Thu Jul  4 02:47:54 2024 ] 	Batch(200/7879) done. Loss: 0.0066  lr:0.000100
[ Thu Jul  4 02:48:12 2024 ] 	Batch(300/7879) done. Loss: 0.0994  lr:0.000100
[ Thu Jul  4 02:48:30 2024 ] 	Batch(400/7879) done. Loss: 0.0389  lr:0.000100
[ Thu Jul  4 02:48:47 2024 ] 
Training: Epoch [73/120], Step [499], Loss: 0.0036763870157301426, Training Accuracy: 97.075
[ Thu Jul  4 02:48:48 2024 ] 	Batch(500/7879) done. Loss: 0.0377  lr:0.000100
[ Thu Jul  4 02:49:05 2024 ] 	Batch(600/7879) done. Loss: 0.0406  lr:0.000100
[ Thu Jul  4 02:49:23 2024 ] 	Batch(700/7879) done. Loss: 0.1297  lr:0.000100
[ Thu Jul  4 02:49:41 2024 ] 	Batch(800/7879) done. Loss: 0.1889  lr:0.000100
[ Thu Jul  4 02:49:59 2024 ] 	Batch(900/7879) done. Loss: 0.0194  lr:0.000100
[ Thu Jul  4 02:50:17 2024 ] 
Training: Epoch [73/120], Step [999], Loss: 0.006296523846685886, Training Accuracy: 96.925
[ Thu Jul  4 02:50:17 2024 ] 	Batch(1000/7879) done. Loss: 0.1066  lr:0.000100
[ Thu Jul  4 02:50:35 2024 ] 	Batch(1100/7879) done. Loss: 0.0067  lr:0.000100
[ Thu Jul  4 02:50:53 2024 ] 	Batch(1200/7879) done. Loss: 0.1078  lr:0.000100
[ Thu Jul  4 02:51:11 2024 ] 	Batch(1300/7879) done. Loss: 0.0846  lr:0.000100
[ Thu Jul  4 02:51:29 2024 ] 	Batch(1400/7879) done. Loss: 0.1071  lr:0.000100
[ Thu Jul  4 02:51:46 2024 ] 
Training: Epoch [73/120], Step [1499], Loss: 0.016620676964521408, Training Accuracy: 97.09166666666667
[ Thu Jul  4 02:51:47 2024 ] 	Batch(1500/7879) done. Loss: 0.2004  lr:0.000100
[ Thu Jul  4 02:52:04 2024 ] 	Batch(1600/7879) done. Loss: 0.0430  lr:0.000100
[ Thu Jul  4 02:52:23 2024 ] 	Batch(1700/7879) done. Loss: 0.1135  lr:0.000100
[ Thu Jul  4 02:52:42 2024 ] 	Batch(1800/7879) done. Loss: 0.1951  lr:0.000100
[ Thu Jul  4 02:53:00 2024 ] 	Batch(1900/7879) done. Loss: 0.0943  lr:0.000100
[ Thu Jul  4 02:53:18 2024 ] 
Training: Epoch [73/120], Step [1999], Loss: 0.0019690035842359066, Training Accuracy: 97.1125
[ Thu Jul  4 02:53:18 2024 ] 	Batch(2000/7879) done. Loss: 0.2471  lr:0.000100
[ Thu Jul  4 02:53:36 2024 ] 	Batch(2100/7879) done. Loss: 0.0204  lr:0.000100
[ Thu Jul  4 02:53:54 2024 ] 	Batch(2200/7879) done. Loss: 0.3031  lr:0.000100
[ Thu Jul  4 02:54:12 2024 ] 	Batch(2300/7879) done. Loss: 0.0554  lr:0.000100
[ Thu Jul  4 02:54:30 2024 ] 	Batch(2400/7879) done. Loss: 0.2837  lr:0.000100
[ Thu Jul  4 02:54:47 2024 ] 
Training: Epoch [73/120], Step [2499], Loss: 0.24631470441818237, Training Accuracy: 97.095
[ Thu Jul  4 02:54:48 2024 ] 	Batch(2500/7879) done. Loss: 0.2166  lr:0.000100
[ Thu Jul  4 02:55:05 2024 ] 	Batch(2600/7879) done. Loss: 0.2600  lr:0.000100
[ Thu Jul  4 02:55:23 2024 ] 	Batch(2700/7879) done. Loss: 0.0020  lr:0.000100
[ Thu Jul  4 02:55:41 2024 ] 	Batch(2800/7879) done. Loss: 0.0608  lr:0.000100
[ Thu Jul  4 02:55:59 2024 ] 	Batch(2900/7879) done. Loss: 0.0876  lr:0.000100
[ Thu Jul  4 02:56:17 2024 ] 
Training: Epoch [73/120], Step [2999], Loss: 0.1838584691286087, Training Accuracy: 97.07916666666667
[ Thu Jul  4 02:56:17 2024 ] 	Batch(3000/7879) done. Loss: 0.0171  lr:0.000100
[ Thu Jul  4 02:56:35 2024 ] 	Batch(3100/7879) done. Loss: 0.0074  lr:0.000100
[ Thu Jul  4 02:56:53 2024 ] 	Batch(3200/7879) done. Loss: 0.0185  lr:0.000100
[ Thu Jul  4 02:57:11 2024 ] 	Batch(3300/7879) done. Loss: 0.0193  lr:0.000100
[ Thu Jul  4 02:57:29 2024 ] 	Batch(3400/7879) done. Loss: 0.2979  lr:0.000100
[ Thu Jul  4 02:57:46 2024 ] 
Training: Epoch [73/120], Step [3499], Loss: 0.1797332763671875, Training Accuracy: 97.08928571428571
[ Thu Jul  4 02:57:47 2024 ] 	Batch(3500/7879) done. Loss: 0.0498  lr:0.000100
[ Thu Jul  4 02:58:05 2024 ] 	Batch(3600/7879) done. Loss: 0.0142  lr:0.000100
[ Thu Jul  4 02:58:23 2024 ] 	Batch(3700/7879) done. Loss: 0.1993  lr:0.000100
[ Thu Jul  4 02:58:42 2024 ] 	Batch(3800/7879) done. Loss: 0.0379  lr:0.000100
[ Thu Jul  4 02:59:00 2024 ] 	Batch(3900/7879) done. Loss: 0.1368  lr:0.000100
[ Thu Jul  4 02:59:18 2024 ] 
Training: Epoch [73/120], Step [3999], Loss: 0.2553851306438446, Training Accuracy: 97.05
[ Thu Jul  4 02:59:19 2024 ] 	Batch(4000/7879) done. Loss: 0.4669  lr:0.000100
[ Thu Jul  4 02:59:37 2024 ] 	Batch(4100/7879) done. Loss: 0.0508  lr:0.000100
[ Thu Jul  4 02:59:55 2024 ] 	Batch(4200/7879) done. Loss: 0.3115  lr:0.000100
[ Thu Jul  4 03:00:12 2024 ] 	Batch(4300/7879) done. Loss: 0.0319  lr:0.000100
[ Thu Jul  4 03:00:30 2024 ] 	Batch(4400/7879) done. Loss: 0.0062  lr:0.000100
[ Thu Jul  4 03:00:48 2024 ] 
Training: Epoch [73/120], Step [4499], Loss: 0.048986081033945084, Training Accuracy: 97.0611111111111
[ Thu Jul  4 03:00:48 2024 ] 	Batch(4500/7879) done. Loss: 0.0926  lr:0.000100
[ Thu Jul  4 03:01:06 2024 ] 	Batch(4600/7879) done. Loss: 0.0183  lr:0.000100
[ Thu Jul  4 03:01:24 2024 ] 	Batch(4700/7879) done. Loss: 0.0022  lr:0.000100
[ Thu Jul  4 03:01:42 2024 ] 	Batch(4800/7879) done. Loss: 0.0885  lr:0.000100
[ Thu Jul  4 03:02:01 2024 ] 	Batch(4900/7879) done. Loss: 0.0072  lr:0.000100
[ Thu Jul  4 03:02:19 2024 ] 
Training: Epoch [73/120], Step [4999], Loss: 0.021636417135596275, Training Accuracy: 97.07249999999999
[ Thu Jul  4 03:02:19 2024 ] 	Batch(5000/7879) done. Loss: 0.0705  lr:0.000100
[ Thu Jul  4 03:02:38 2024 ] 	Batch(5100/7879) done. Loss: 0.1772  lr:0.000100
[ Thu Jul  4 03:02:56 2024 ] 	Batch(5200/7879) done. Loss: 0.2814  lr:0.000100
[ Thu Jul  4 03:03:14 2024 ] 	Batch(5300/7879) done. Loss: 0.1429  lr:0.000100
[ Thu Jul  4 03:03:32 2024 ] 	Batch(5400/7879) done. Loss: 0.0357  lr:0.000100
[ Thu Jul  4 03:03:50 2024 ] 
Training: Epoch [73/120], Step [5499], Loss: 0.0844980850815773, Training Accuracy: 97.07727272727273
[ Thu Jul  4 03:03:50 2024 ] 	Batch(5500/7879) done. Loss: 0.0283  lr:0.000100
[ Thu Jul  4 03:04:08 2024 ] 	Batch(5600/7879) done. Loss: 0.0056  lr:0.000100
[ Thu Jul  4 03:04:26 2024 ] 	Batch(5700/7879) done. Loss: 0.1140  lr:0.000100
[ Thu Jul  4 03:04:43 2024 ] 	Batch(5800/7879) done. Loss: 0.0382  lr:0.000100
[ Thu Jul  4 03:05:01 2024 ] 	Batch(5900/7879) done. Loss: 0.0243  lr:0.000100
[ Thu Jul  4 03:05:19 2024 ] 
Training: Epoch [73/120], Step [5999], Loss: 0.049038905650377274, Training Accuracy: 97.06875000000001
[ Thu Jul  4 03:05:19 2024 ] 	Batch(6000/7879) done. Loss: 0.0782  lr:0.000100
[ Thu Jul  4 03:05:37 2024 ] 	Batch(6100/7879) done. Loss: 0.0133  lr:0.000100
[ Thu Jul  4 03:05:55 2024 ] 	Batch(6200/7879) done. Loss: 0.1061  lr:0.000100
[ Thu Jul  4 03:06:13 2024 ] 	Batch(6300/7879) done. Loss: 0.2537  lr:0.000100
[ Thu Jul  4 03:06:31 2024 ] 	Batch(6400/7879) done. Loss: 0.0205  lr:0.000100
[ Thu Jul  4 03:06:49 2024 ] 
Training: Epoch [73/120], Step [6499], Loss: 0.04632296413183212, Training Accuracy: 97.02499999999999
[ Thu Jul  4 03:06:49 2024 ] 	Batch(6500/7879) done. Loss: 0.1426  lr:0.000100
[ Thu Jul  4 03:07:07 2024 ] 	Batch(6600/7879) done. Loss: 0.1674  lr:0.000100
[ Thu Jul  4 03:07:25 2024 ] 	Batch(6700/7879) done. Loss: 0.0307  lr:0.000100
[ Thu Jul  4 03:07:43 2024 ] 	Batch(6800/7879) done. Loss: 0.0238  lr:0.000100
[ Thu Jul  4 03:08:00 2024 ] 	Batch(6900/7879) done. Loss: 0.0034  lr:0.000100
[ Thu Jul  4 03:08:19 2024 ] 
Training: Epoch [73/120], Step [6999], Loss: 0.07379191368818283, Training Accuracy: 97.05178571428571
[ Thu Jul  4 03:08:19 2024 ] 	Batch(7000/7879) done. Loss: 0.0274  lr:0.000100
[ Thu Jul  4 03:08:37 2024 ] 	Batch(7100/7879) done. Loss: 0.0069  lr:0.000100
[ Thu Jul  4 03:08:55 2024 ] 	Batch(7200/7879) done. Loss: 0.1486  lr:0.000100
[ Thu Jul  4 03:09:13 2024 ] 	Batch(7300/7879) done. Loss: 0.1238  lr:0.000100
[ Thu Jul  4 03:09:31 2024 ] 	Batch(7400/7879) done. Loss: 0.1157  lr:0.000100
[ Thu Jul  4 03:09:49 2024 ] 
Training: Epoch [73/120], Step [7499], Loss: 0.0121467811986804, Training Accuracy: 97.07166666666667
[ Thu Jul  4 03:09:49 2024 ] 	Batch(7500/7879) done. Loss: 0.0083  lr:0.000100
[ Thu Jul  4 03:10:07 2024 ] 	Batch(7600/7879) done. Loss: 0.6513  lr:0.000100
[ Thu Jul  4 03:10:26 2024 ] 	Batch(7700/7879) done. Loss: 0.1386  lr:0.000100
[ Thu Jul  4 03:10:44 2024 ] 	Batch(7800/7879) done. Loss: 0.0651  lr:0.000100
[ Thu Jul  4 03:10:59 2024 ] 	Mean training loss: 0.1138.
[ Thu Jul  4 03:10:59 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 03:10:59 2024 ] Training epoch: 75
[ Thu Jul  4 03:10:59 2024 ] 	Batch(0/7879) done. Loss: 0.1231  lr:0.000100
[ Thu Jul  4 03:11:18 2024 ] 	Batch(100/7879) done. Loss: 0.0072  lr:0.000100
[ Thu Jul  4 03:11:36 2024 ] 	Batch(200/7879) done. Loss: 0.1244  lr:0.000100
[ Thu Jul  4 03:11:54 2024 ] 	Batch(300/7879) done. Loss: 0.5728  lr:0.000100
[ Thu Jul  4 03:12:12 2024 ] 	Batch(400/7879) done. Loss: 0.0442  lr:0.000100
[ Thu Jul  4 03:12:31 2024 ] 
Training: Epoch [74/120], Step [499], Loss: 0.031422074884176254, Training Accuracy: 97.0
[ Thu Jul  4 03:12:31 2024 ] 	Batch(500/7879) done. Loss: 0.0819  lr:0.000100
[ Thu Jul  4 03:12:49 2024 ] 	Batch(600/7879) done. Loss: 0.0204  lr:0.000100
[ Thu Jul  4 03:13:07 2024 ] 	Batch(700/7879) done. Loss: 0.0068  lr:0.000100
[ Thu Jul  4 03:13:26 2024 ] 	Batch(800/7879) done. Loss: 0.0253  lr:0.000100
[ Thu Jul  4 03:13:44 2024 ] 	Batch(900/7879) done. Loss: 0.0655  lr:0.000100
[ Thu Jul  4 03:14:02 2024 ] 
Training: Epoch [74/120], Step [999], Loss: 0.05351937934756279, Training Accuracy: 97.0125
[ Thu Jul  4 03:14:02 2024 ] 	Batch(1000/7879) done. Loss: 0.0351  lr:0.000100
[ Thu Jul  4 03:14:21 2024 ] 	Batch(1100/7879) done. Loss: 0.0218  lr:0.000100
[ Thu Jul  4 03:14:39 2024 ] 	Batch(1200/7879) done. Loss: 0.0210  lr:0.000100
[ Thu Jul  4 03:14:57 2024 ] 	Batch(1300/7879) done. Loss: 0.0878  lr:0.000100
[ Thu Jul  4 03:15:16 2024 ] 	Batch(1400/7879) done. Loss: 0.0023  lr:0.000100
[ Thu Jul  4 03:15:34 2024 ] 
Training: Epoch [74/120], Step [1499], Loss: 0.0423663929104805, Training Accuracy: 96.75833333333334
[ Thu Jul  4 03:15:34 2024 ] 	Batch(1500/7879) done. Loss: 0.0154  lr:0.000100
[ Thu Jul  4 03:15:52 2024 ] 	Batch(1600/7879) done. Loss: 0.0046  lr:0.000100
[ Thu Jul  4 03:16:10 2024 ] 	Batch(1700/7879) done. Loss: 0.0220  lr:0.000100
[ Thu Jul  4 03:16:28 2024 ] 	Batch(1800/7879) done. Loss: 0.0299  lr:0.000100
[ Thu Jul  4 03:16:46 2024 ] 	Batch(1900/7879) done. Loss: 0.0747  lr:0.000100
[ Thu Jul  4 03:17:04 2024 ] 
Training: Epoch [74/120], Step [1999], Loss: 0.23377323150634766, Training Accuracy: 96.75
[ Thu Jul  4 03:17:04 2024 ] 	Batch(2000/7879) done. Loss: 0.0055  lr:0.000100
[ Thu Jul  4 03:17:22 2024 ] 	Batch(2100/7879) done. Loss: 0.3288  lr:0.000100
[ Thu Jul  4 03:17:40 2024 ] 	Batch(2200/7879) done. Loss: 0.0390  lr:0.000100
[ Thu Jul  4 03:17:58 2024 ] 	Batch(2300/7879) done. Loss: 0.0178  lr:0.000100
[ Thu Jul  4 03:18:16 2024 ] 	Batch(2400/7879) done. Loss: 0.0082  lr:0.000100
[ Thu Jul  4 03:18:33 2024 ] 
Training: Epoch [74/120], Step [2499], Loss: 0.11934927850961685, Training Accuracy: 96.895
[ Thu Jul  4 03:18:33 2024 ] 	Batch(2500/7879) done. Loss: 0.0125  lr:0.000100
[ Thu Jul  4 03:18:51 2024 ] 	Batch(2600/7879) done. Loss: 0.0904  lr:0.000100
[ Thu Jul  4 03:19:09 2024 ] 	Batch(2700/7879) done. Loss: 0.0952  lr:0.000100
[ Thu Jul  4 03:19:27 2024 ] 	Batch(2800/7879) done. Loss: 0.1694  lr:0.000100
[ Thu Jul  4 03:19:45 2024 ] 	Batch(2900/7879) done. Loss: 0.0074  lr:0.000100
[ Thu Jul  4 03:20:03 2024 ] 
Training: Epoch [74/120], Step [2999], Loss: 0.11810950189828873, Training Accuracy: 97.02916666666667
[ Thu Jul  4 03:20:03 2024 ] 	Batch(3000/7879) done. Loss: 0.0312  lr:0.000100
[ Thu Jul  4 03:20:21 2024 ] 	Batch(3100/7879) done. Loss: 0.1272  lr:0.000100
[ Thu Jul  4 03:20:39 2024 ] 	Batch(3200/7879) done. Loss: 0.0393  lr:0.000100
[ Thu Jul  4 03:20:57 2024 ] 	Batch(3300/7879) done. Loss: 0.0303  lr:0.000100
[ Thu Jul  4 03:21:14 2024 ] 	Batch(3400/7879) done. Loss: 0.0075  lr:0.000100
[ Thu Jul  4 03:21:32 2024 ] 
Training: Epoch [74/120], Step [3499], Loss: 0.008435514755547047, Training Accuracy: 97.02142857142857
[ Thu Jul  4 03:21:32 2024 ] 	Batch(3500/7879) done. Loss: 0.0168  lr:0.000100
[ Thu Jul  4 03:21:50 2024 ] 	Batch(3600/7879) done. Loss: 0.3267  lr:0.000100
[ Thu Jul  4 03:22:08 2024 ] 	Batch(3700/7879) done. Loss: 0.1412  lr:0.000100
[ Thu Jul  4 03:22:26 2024 ] 	Batch(3800/7879) done. Loss: 0.2153  lr:0.000100
[ Thu Jul  4 03:22:44 2024 ] 	Batch(3900/7879) done. Loss: 0.0871  lr:0.000100
[ Thu Jul  4 03:23:02 2024 ] 
Training: Epoch [74/120], Step [3999], Loss: 0.32301270961761475, Training Accuracy: 97.05625
[ Thu Jul  4 03:23:02 2024 ] 	Batch(4000/7879) done. Loss: 0.0432  lr:0.000100
[ Thu Jul  4 03:23:20 2024 ] 	Batch(4100/7879) done. Loss: 0.0500  lr:0.000100
[ Thu Jul  4 03:23:38 2024 ] 	Batch(4200/7879) done. Loss: 0.0071  lr:0.000100
[ Thu Jul  4 03:23:55 2024 ] 	Batch(4300/7879) done. Loss: 0.0215  lr:0.000100
[ Thu Jul  4 03:24:13 2024 ] 	Batch(4400/7879) done. Loss: 0.0783  lr:0.000100
[ Thu Jul  4 03:24:32 2024 ] 
Training: Epoch [74/120], Step [4499], Loss: 0.010326574556529522, Training Accuracy: 97.05277777777778
[ Thu Jul  4 03:24:32 2024 ] 	Batch(4500/7879) done. Loss: 0.1076  lr:0.000100
[ Thu Jul  4 03:24:50 2024 ] 	Batch(4600/7879) done. Loss: 0.0391  lr:0.000100
[ Thu Jul  4 03:25:08 2024 ] 	Batch(4700/7879) done. Loss: 0.3326  lr:0.000100
[ Thu Jul  4 03:25:26 2024 ] 	Batch(4800/7879) done. Loss: 0.1172  lr:0.000100
[ Thu Jul  4 03:25:44 2024 ] 	Batch(4900/7879) done. Loss: 0.2197  lr:0.000100
[ Thu Jul  4 03:26:02 2024 ] 
Training: Epoch [74/120], Step [4999], Loss: 0.09874005615711212, Training Accuracy: 97.08250000000001
[ Thu Jul  4 03:26:02 2024 ] 	Batch(5000/7879) done. Loss: 0.0142  lr:0.000100
[ Thu Jul  4 03:26:20 2024 ] 	Batch(5100/7879) done. Loss: 0.0191  lr:0.000100
[ Thu Jul  4 03:26:38 2024 ] 	Batch(5200/7879) done. Loss: 0.2692  lr:0.000100
[ Thu Jul  4 03:26:57 2024 ] 	Batch(5300/7879) done. Loss: 0.0021  lr:0.000100
[ Thu Jul  4 03:27:16 2024 ] 	Batch(5400/7879) done. Loss: 0.0779  lr:0.000100
[ Thu Jul  4 03:27:34 2024 ] 
Training: Epoch [74/120], Step [5499], Loss: 0.007337309420108795, Training Accuracy: 97.0909090909091
[ Thu Jul  4 03:27:34 2024 ] 	Batch(5500/7879) done. Loss: 0.0343  lr:0.000100
[ Thu Jul  4 03:27:53 2024 ] 	Batch(5600/7879) done. Loss: 0.2211  lr:0.000100
[ Thu Jul  4 03:28:11 2024 ] 	Batch(5700/7879) done. Loss: 0.1124  lr:0.000100
[ Thu Jul  4 03:28:30 2024 ] 	Batch(5800/7879) done. Loss: 0.0294  lr:0.000100
[ Thu Jul  4 03:28:48 2024 ] 	Batch(5900/7879) done. Loss: 0.0719  lr:0.000100
[ Thu Jul  4 03:29:07 2024 ] 
Training: Epoch [74/120], Step [5999], Loss: 0.026278359815478325, Training Accuracy: 97.12291666666667
[ Thu Jul  4 03:29:07 2024 ] 	Batch(6000/7879) done. Loss: 0.0190  lr:0.000100
[ Thu Jul  4 03:29:25 2024 ] 	Batch(6100/7879) done. Loss: 0.0582  lr:0.000100
[ Thu Jul  4 03:29:44 2024 ] 	Batch(6200/7879) done. Loss: 0.1153  lr:0.000100
[ Thu Jul  4 03:30:02 2024 ] 	Batch(6300/7879) done. Loss: 0.3330  lr:0.000100
[ Thu Jul  4 03:30:20 2024 ] 	Batch(6400/7879) done. Loss: 0.0498  lr:0.000100
[ Thu Jul  4 03:30:38 2024 ] 
Training: Epoch [74/120], Step [6499], Loss: 0.13086652755737305, Training Accuracy: 97.16538461538462
[ Thu Jul  4 03:30:38 2024 ] 	Batch(6500/7879) done. Loss: 0.0081  lr:0.000100
[ Thu Jul  4 03:30:56 2024 ] 	Batch(6600/7879) done. Loss: 0.0132  lr:0.000100
[ Thu Jul  4 03:31:13 2024 ] 	Batch(6700/7879) done. Loss: 0.0085  lr:0.000100
[ Thu Jul  4 03:31:31 2024 ] 	Batch(6800/7879) done. Loss: 0.0262  lr:0.000100
[ Thu Jul  4 03:31:50 2024 ] 	Batch(6900/7879) done. Loss: 0.1004  lr:0.000100
[ Thu Jul  4 03:32:08 2024 ] 
Training: Epoch [74/120], Step [6999], Loss: 0.04096893593668938, Training Accuracy: 97.17142857142858
[ Thu Jul  4 03:32:08 2024 ] 	Batch(7000/7879) done. Loss: 0.0204  lr:0.000100
[ Thu Jul  4 03:32:27 2024 ] 	Batch(7100/7879) done. Loss: 0.0933  lr:0.000100
[ Thu Jul  4 03:32:46 2024 ] 	Batch(7200/7879) done. Loss: 0.3154  lr:0.000100
[ Thu Jul  4 03:33:04 2024 ] 	Batch(7300/7879) done. Loss: 0.0105  lr:0.000100
[ Thu Jul  4 03:33:23 2024 ] 	Batch(7400/7879) done. Loss: 0.3042  lr:0.000100
[ Thu Jul  4 03:33:41 2024 ] 
Training: Epoch [74/120], Step [7499], Loss: 0.04121646657586098, Training Accuracy: 97.13666666666667
[ Thu Jul  4 03:33:41 2024 ] 	Batch(7500/7879) done. Loss: 0.1372  lr:0.000100
[ Thu Jul  4 03:34:00 2024 ] 	Batch(7600/7879) done. Loss: 0.0318  lr:0.000100
[ Thu Jul  4 03:34:18 2024 ] 	Batch(7700/7879) done. Loss: 0.1041  lr:0.000100
[ Thu Jul  4 03:34:37 2024 ] 	Batch(7800/7879) done. Loss: 0.0156  lr:0.000100
[ Thu Jul  4 03:34:51 2024 ] 	Mean training loss: 0.1136.
[ Thu Jul  4 03:34:51 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul  4 03:34:52 2024 ] Training epoch: 76
[ Thu Jul  4 03:34:52 2024 ] 	Batch(0/7879) done. Loss: 0.1575  lr:0.000100
[ Thu Jul  4 03:35:10 2024 ] 	Batch(100/7879) done. Loss: 0.0421  lr:0.000100
[ Thu Jul  4 03:35:28 2024 ] 	Batch(200/7879) done. Loss: 0.9039  lr:0.000100
[ Thu Jul  4 03:35:46 2024 ] 	Batch(300/7879) done. Loss: 0.0460  lr:0.000100
[ Thu Jul  4 03:36:04 2024 ] 	Batch(400/7879) done. Loss: 0.0181  lr:0.000100
[ Thu Jul  4 03:36:21 2024 ] 
Training: Epoch [75/120], Step [499], Loss: 0.018425945192575455, Training Accuracy: 97.55
[ Thu Jul  4 03:36:22 2024 ] 	Batch(500/7879) done. Loss: 0.0190  lr:0.000100
[ Thu Jul  4 03:36:39 2024 ] 	Batch(600/7879) done. Loss: 0.0902  lr:0.000100
[ Thu Jul  4 03:36:57 2024 ] 	Batch(700/7879) done. Loss: 0.3427  lr:0.000100
[ Thu Jul  4 03:37:15 2024 ] 	Batch(800/7879) done. Loss: 0.0238  lr:0.000100
[ Thu Jul  4 03:37:33 2024 ] 	Batch(900/7879) done. Loss: 0.0143  lr:0.000100
[ Thu Jul  4 03:37:51 2024 ] 
Training: Epoch [75/120], Step [999], Loss: 0.16269046068191528, Training Accuracy: 97.475
[ Thu Jul  4 03:37:51 2024 ] 	Batch(1000/7879) done. Loss: 0.0391  lr:0.000100
[ Thu Jul  4 03:38:09 2024 ] 	Batch(1100/7879) done. Loss: 0.1585  lr:0.000100
[ Thu Jul  4 03:38:27 2024 ] 	Batch(1200/7879) done. Loss: 0.0086  lr:0.000100
[ Thu Jul  4 03:38:45 2024 ] 	Batch(1300/7879) done. Loss: 0.0275  lr:0.000100
[ Thu Jul  4 03:39:03 2024 ] 	Batch(1400/7879) done. Loss: 0.0513  lr:0.000100
[ Thu Jul  4 03:39:20 2024 ] 
Training: Epoch [75/120], Step [1499], Loss: 0.04775507003068924, Training Accuracy: 97.40833333333333
[ Thu Jul  4 03:39:20 2024 ] 	Batch(1500/7879) done. Loss: 0.4047  lr:0.000100
[ Thu Jul  4 03:39:39 2024 ] 	Batch(1600/7879) done. Loss: 0.0914  lr:0.000100
[ Thu Jul  4 03:39:56 2024 ] 	Batch(1700/7879) done. Loss: 0.4256  lr:0.000100
[ Thu Jul  4 03:40:14 2024 ] 	Batch(1800/7879) done. Loss: 0.0064  lr:0.000100
[ Thu Jul  4 03:40:32 2024 ] 	Batch(1900/7879) done. Loss: 0.0517  lr:0.000100
[ Thu Jul  4 03:40:50 2024 ] 
Training: Epoch [75/120], Step [1999], Loss: 0.08150720596313477, Training Accuracy: 97.20625000000001
[ Thu Jul  4 03:40:50 2024 ] 	Batch(2000/7879) done. Loss: 0.0175  lr:0.000100
[ Thu Jul  4 03:41:08 2024 ] 	Batch(2100/7879) done. Loss: 0.1062  lr:0.000100
[ Thu Jul  4 03:41:26 2024 ] 	Batch(2200/7879) done. Loss: 0.0296  lr:0.000100
[ Thu Jul  4 03:41:44 2024 ] 	Batch(2300/7879) done. Loss: 0.6778  lr:0.000100
[ Thu Jul  4 03:42:02 2024 ] 	Batch(2400/7879) done. Loss: 0.3271  lr:0.000100
[ Thu Jul  4 03:42:19 2024 ] 
Training: Epoch [75/120], Step [2499], Loss: 0.03601774573326111, Training Accuracy: 97.125
[ Thu Jul  4 03:42:20 2024 ] 	Batch(2500/7879) done. Loss: 0.2290  lr:0.000100
[ Thu Jul  4 03:42:38 2024 ] 	Batch(2600/7879) done. Loss: 0.0766  lr:0.000100
[ Thu Jul  4 03:42:55 2024 ] 	Batch(2700/7879) done. Loss: 0.0738  lr:0.000100
[ Thu Jul  4 03:43:14 2024 ] 	Batch(2800/7879) done. Loss: 0.0381  lr:0.000100
[ Thu Jul  4 03:43:32 2024 ] 	Batch(2900/7879) done. Loss: 0.0083  lr:0.000100
[ Thu Jul  4 03:43:50 2024 ] 
Training: Epoch [75/120], Step [2999], Loss: 0.01353791169822216, Training Accuracy: 97.12083333333334
[ Thu Jul  4 03:43:51 2024 ] 	Batch(3000/7879) done. Loss: 0.0542  lr:0.000100
[ Thu Jul  4 03:44:09 2024 ] 	Batch(3100/7879) done. Loss: 0.0845  lr:0.000100
[ Thu Jul  4 03:44:28 2024 ] 	Batch(3200/7879) done. Loss: 0.1752  lr:0.000100
[ Thu Jul  4 03:44:46 2024 ] 	Batch(3300/7879) done. Loss: 0.0683  lr:0.000100
[ Thu Jul  4 03:45:03 2024 ] 	Batch(3400/7879) done. Loss: 0.0030  lr:0.000100
[ Thu Jul  4 03:45:21 2024 ] 
Training: Epoch [75/120], Step [3499], Loss: 0.05347045883536339, Training Accuracy: 97.15714285714286
[ Thu Jul  4 03:45:21 2024 ] 	Batch(3500/7879) done. Loss: 0.0107  lr:0.000100
[ Thu Jul  4 03:45:39 2024 ] 	Batch(3600/7879) done. Loss: 0.0255  lr:0.000100
[ Thu Jul  4 03:45:58 2024 ] 	Batch(3700/7879) done. Loss: 0.4229  lr:0.000100
[ Thu Jul  4 03:46:17 2024 ] 	Batch(3800/7879) done. Loss: 0.0633  lr:0.000100
[ Thu Jul  4 03:46:36 2024 ] 	Batch(3900/7879) done. Loss: 0.0481  lr:0.000100
[ Thu Jul  4 03:46:54 2024 ] 
Training: Epoch [75/120], Step [3999], Loss: 0.008607326075434685, Training Accuracy: 97.2125
[ Thu Jul  4 03:46:54 2024 ] 	Batch(4000/7879) done. Loss: 0.0506  lr:0.000100
[ Thu Jul  4 03:47:13 2024 ] 	Batch(4100/7879) done. Loss: 0.1684  lr:0.000100
[ Thu Jul  4 03:47:31 2024 ] 	Batch(4200/7879) done. Loss: 0.3469  lr:0.000100
[ Thu Jul  4 03:47:49 2024 ] 	Batch(4300/7879) done. Loss: 0.1715  lr:0.000100
[ Thu Jul  4 03:48:07 2024 ] 	Batch(4400/7879) done. Loss: 0.0388  lr:0.000100
[ Thu Jul  4 03:48:24 2024 ] 
Training: Epoch [75/120], Step [4499], Loss: 0.33116263151168823, Training Accuracy: 97.2
[ Thu Jul  4 03:48:24 2024 ] 	Batch(4500/7879) done. Loss: 0.0198  lr:0.000100
[ Thu Jul  4 03:48:42 2024 ] 	Batch(4600/7879) done. Loss: 0.0659  lr:0.000100
[ Thu Jul  4 03:49:00 2024 ] 	Batch(4700/7879) done. Loss: 0.4431  lr:0.000100
[ Thu Jul  4 03:49:18 2024 ] 	Batch(4800/7879) done. Loss: 0.0185  lr:0.000100
[ Thu Jul  4 03:49:36 2024 ] 	Batch(4900/7879) done. Loss: 0.0110  lr:0.000100
[ Thu Jul  4 03:49:54 2024 ] 
Training: Epoch [75/120], Step [4999], Loss: 0.014162198640406132, Training Accuracy: 97.2125
[ Thu Jul  4 03:49:54 2024 ] 	Batch(5000/7879) done. Loss: 0.0319  lr:0.000100
[ Thu Jul  4 03:50:12 2024 ] 	Batch(5100/7879) done. Loss: 0.0055  lr:0.000100
[ Thu Jul  4 03:50:30 2024 ] 	Batch(5200/7879) done. Loss: 0.0414  lr:0.000100
[ Thu Jul  4 03:50:48 2024 ] 	Batch(5300/7879) done. Loss: 0.0870  lr:0.000100
[ Thu Jul  4 03:51:06 2024 ] 	Batch(5400/7879) done. Loss: 0.0377  lr:0.000100
[ Thu Jul  4 03:51:23 2024 ] 
Training: Epoch [75/120], Step [5499], Loss: 0.03493879362940788, Training Accuracy: 97.18636363636364
[ Thu Jul  4 03:51:24 2024 ] 	Batch(5500/7879) done. Loss: 0.0038  lr:0.000100
[ Thu Jul  4 03:51:42 2024 ] 	Batch(5600/7879) done. Loss: 0.0488  lr:0.000100
[ Thu Jul  4 03:51:59 2024 ] 	Batch(5700/7879) done. Loss: 0.0247  lr:0.000100
[ Thu Jul  4 03:52:17 2024 ] 	Batch(5800/7879) done. Loss: 0.0465  lr:0.000100
[ Thu Jul  4 03:52:35 2024 ] 	Batch(5900/7879) done. Loss: 0.0294  lr:0.000100
[ Thu Jul  4 03:52:53 2024 ] 
Training: Epoch [75/120], Step [5999], Loss: 0.06913848221302032, Training Accuracy: 97.17708333333334
[ Thu Jul  4 03:52:53 2024 ] 	Batch(6000/7879) done. Loss: 0.0176  lr:0.000100
[ Thu Jul  4 03:53:12 2024 ] 	Batch(6100/7879) done. Loss: 0.0302  lr:0.000100
[ Thu Jul  4 03:53:30 2024 ] 	Batch(6200/7879) done. Loss: 0.1158  lr:0.000100
[ Thu Jul  4 03:53:49 2024 ] 	Batch(6300/7879) done. Loss: 0.0229  lr:0.000100
[ Thu Jul  4 03:54:07 2024 ] 	Batch(6400/7879) done. Loss: 0.0580  lr:0.000100
[ Thu Jul  4 03:54:26 2024 ] 
Training: Epoch [75/120], Step [6499], Loss: 0.13585184514522552, Training Accuracy: 97.20384615384616
[ Thu Jul  4 03:54:26 2024 ] 	Batch(6500/7879) done. Loss: 0.3649  lr:0.000100
[ Thu Jul  4 03:54:44 2024 ] 	Batch(6600/7879) done. Loss: 0.1393  lr:0.000100
[ Thu Jul  4 03:55:03 2024 ] 	Batch(6700/7879) done. Loss: 0.1029  lr:0.000100
[ Thu Jul  4 03:55:22 2024 ] 	Batch(6800/7879) done. Loss: 0.0089  lr:0.000100
[ Thu Jul  4 03:55:40 2024 ] 	Batch(6900/7879) done. Loss: 0.0318  lr:0.000100
[ Thu Jul  4 03:55:58 2024 ] 
Training: Epoch [75/120], Step [6999], Loss: 0.051628779619932175, Training Accuracy: 97.20178571428572
[ Thu Jul  4 03:55:59 2024 ] 	Batch(7000/7879) done. Loss: 0.0803  lr:0.000100
[ Thu Jul  4 03:56:17 2024 ] 	Batch(7100/7879) done. Loss: 0.0752  lr:0.000100
[ Thu Jul  4 03:56:36 2024 ] 	Batch(7200/7879) done. Loss: 0.0801  lr:0.000100
[ Thu Jul  4 03:56:54 2024 ] 	Batch(7300/7879) done. Loss: 0.3639  lr:0.000100
[ Thu Jul  4 03:57:13 2024 ] 	Batch(7400/7879) done. Loss: 0.1340  lr:0.000100
[ Thu Jul  4 03:57:31 2024 ] 
Training: Epoch [75/120], Step [7499], Loss: 0.3045699894428253, Training Accuracy: 97.21333333333332
[ Thu Jul  4 03:57:31 2024 ] 	Batch(7500/7879) done. Loss: 0.2155  lr:0.000100
[ Thu Jul  4 03:57:50 2024 ] 	Batch(7600/7879) done. Loss: 0.4328  lr:0.000100
[ Thu Jul  4 03:58:08 2024 ] 	Batch(7700/7879) done. Loss: 0.0299  lr:0.000100
[ Thu Jul  4 03:58:26 2024 ] 	Batch(7800/7879) done. Loss: 0.1572  lr:0.000100
[ Thu Jul  4 03:58:40 2024 ] 	Mean training loss: 0.1075.
[ Thu Jul  4 03:58:40 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 03:58:40 2024 ] Training epoch: 77
[ Thu Jul  4 03:58:40 2024 ] 	Batch(0/7879) done. Loss: 0.0529  lr:0.000100
[ Thu Jul  4 03:58:59 2024 ] 	Batch(100/7879) done. Loss: 0.3427  lr:0.000100
[ Thu Jul  4 03:59:17 2024 ] 	Batch(200/7879) done. Loss: 0.2475  lr:0.000100
[ Thu Jul  4 03:59:35 2024 ] 	Batch(300/7879) done. Loss: 0.1638  lr:0.000100
[ Thu Jul  4 03:59:54 2024 ] 	Batch(400/7879) done. Loss: 0.0425  lr:0.000100
[ Thu Jul  4 04:00:11 2024 ] 
Training: Epoch [76/120], Step [499], Loss: 0.03502320870757103, Training Accuracy: 97.5
[ Thu Jul  4 04:00:11 2024 ] 	Batch(500/7879) done. Loss: 0.0962  lr:0.000100
[ Thu Jul  4 04:00:29 2024 ] 	Batch(600/7879) done. Loss: 0.0249  lr:0.000100
[ Thu Jul  4 04:00:47 2024 ] 	Batch(700/7879) done. Loss: 0.0187  lr:0.000100
[ Thu Jul  4 04:01:05 2024 ] 	Batch(800/7879) done. Loss: 0.2145  lr:0.000100
[ Thu Jul  4 04:01:23 2024 ] 	Batch(900/7879) done. Loss: 0.1115  lr:0.000100
[ Thu Jul  4 04:01:41 2024 ] 
Training: Epoch [76/120], Step [999], Loss: 0.09219037741422653, Training Accuracy: 97.0625
[ Thu Jul  4 04:01:41 2024 ] 	Batch(1000/7879) done. Loss: 0.0114  lr:0.000100
[ Thu Jul  4 04:01:59 2024 ] 	Batch(1100/7879) done. Loss: 0.0766  lr:0.000100
[ Thu Jul  4 04:02:17 2024 ] 	Batch(1200/7879) done. Loss: 0.2264  lr:0.000100
[ Thu Jul  4 04:02:35 2024 ] 	Batch(1300/7879) done. Loss: 0.0237  lr:0.000100
[ Thu Jul  4 04:02:53 2024 ] 	Batch(1400/7879) done. Loss: 0.0508  lr:0.000100
[ Thu Jul  4 04:03:10 2024 ] 
Training: Epoch [76/120], Step [1499], Loss: 0.0067857177928090096, Training Accuracy: 97.21666666666667
[ Thu Jul  4 04:03:11 2024 ] 	Batch(1500/7879) done. Loss: 0.2557  lr:0.000100
[ Thu Jul  4 04:03:29 2024 ] 	Batch(1600/7879) done. Loss: 0.1083  lr:0.000100
[ Thu Jul  4 04:03:46 2024 ] 	Batch(1700/7879) done. Loss: 0.0322  lr:0.000100
[ Thu Jul  4 04:04:04 2024 ] 	Batch(1800/7879) done. Loss: 0.0053  lr:0.000100
[ Thu Jul  4 04:04:22 2024 ] 	Batch(1900/7879) done. Loss: 0.0381  lr:0.000100
[ Thu Jul  4 04:04:40 2024 ] 
Training: Epoch [76/120], Step [1999], Loss: 0.088463693857193, Training Accuracy: 97.175
[ Thu Jul  4 04:04:40 2024 ] 	Batch(2000/7879) done. Loss: 0.0687  lr:0.000100
[ Thu Jul  4 04:04:58 2024 ] 	Batch(2100/7879) done. Loss: 0.0241  lr:0.000100
[ Thu Jul  4 04:05:16 2024 ] 	Batch(2200/7879) done. Loss: 0.0022  lr:0.000100
[ Thu Jul  4 04:05:34 2024 ] 	Batch(2300/7879) done. Loss: 0.0035  lr:0.000100
[ Thu Jul  4 04:05:52 2024 ] 	Batch(2400/7879) done. Loss: 0.2173  lr:0.000100
[ Thu Jul  4 04:06:10 2024 ] 
Training: Epoch [76/120], Step [2499], Loss: 0.00786004588007927, Training Accuracy: 97.08
[ Thu Jul  4 04:06:10 2024 ] 	Batch(2500/7879) done. Loss: 0.0373  lr:0.000100
[ Thu Jul  4 04:06:28 2024 ] 	Batch(2600/7879) done. Loss: 0.4185  lr:0.000100
[ Thu Jul  4 04:06:46 2024 ] 	Batch(2700/7879) done. Loss: 0.1998  lr:0.000100
[ Thu Jul  4 04:07:04 2024 ] 	Batch(2800/7879) done. Loss: 0.0350  lr:0.000100
[ Thu Jul  4 04:07:22 2024 ] 	Batch(2900/7879) done. Loss: 0.3108  lr:0.000100
[ Thu Jul  4 04:07:39 2024 ] 
Training: Epoch [76/120], Step [2999], Loss: 0.0850718542933464, Training Accuracy: 97.08333333333333
[ Thu Jul  4 04:07:40 2024 ] 	Batch(3000/7879) done. Loss: 0.0457  lr:0.000100
[ Thu Jul  4 04:07:57 2024 ] 	Batch(3100/7879) done. Loss: 0.0142  lr:0.000100
[ Thu Jul  4 04:08:15 2024 ] 	Batch(3200/7879) done. Loss: 0.0138  lr:0.000100
[ Thu Jul  4 04:08:33 2024 ] 	Batch(3300/7879) done. Loss: 0.0015  lr:0.000100
[ Thu Jul  4 04:08:51 2024 ] 	Batch(3400/7879) done. Loss: 0.0227  lr:0.000100
[ Thu Jul  4 04:09:09 2024 ] 
Training: Epoch [76/120], Step [3499], Loss: 0.05704904720187187, Training Accuracy: 97.19285714285715
[ Thu Jul  4 04:09:09 2024 ] 	Batch(3500/7879) done. Loss: 0.3175  lr:0.000100
[ Thu Jul  4 04:09:27 2024 ] 	Batch(3600/7879) done. Loss: 0.0636  lr:0.000100
[ Thu Jul  4 04:09:46 2024 ] 	Batch(3700/7879) done. Loss: 0.0042  lr:0.000100
[ Thu Jul  4 04:10:04 2024 ] 	Batch(3800/7879) done. Loss: 0.0227  lr:0.000100
[ Thu Jul  4 04:10:23 2024 ] 	Batch(3900/7879) done. Loss: 0.1998  lr:0.000100
[ Thu Jul  4 04:10:41 2024 ] 
Training: Epoch [76/120], Step [3999], Loss: 0.1015794575214386, Training Accuracy: 97.271875
[ Thu Jul  4 04:10:41 2024 ] 	Batch(4000/7879) done. Loss: 0.0091  lr:0.000100
[ Thu Jul  4 04:11:00 2024 ] 	Batch(4100/7879) done. Loss: 0.0384  lr:0.000100
[ Thu Jul  4 04:11:18 2024 ] 	Batch(4200/7879) done. Loss: 0.1984  lr:0.000100
[ Thu Jul  4 04:11:37 2024 ] 	Batch(4300/7879) done. Loss: 0.1350  lr:0.000100
[ Thu Jul  4 04:11:56 2024 ] 	Batch(4400/7879) done. Loss: 0.0644  lr:0.000100
[ Thu Jul  4 04:12:13 2024 ] 
Training: Epoch [76/120], Step [4499], Loss: 0.009837467223405838, Training Accuracy: 97.23055555555555
[ Thu Jul  4 04:12:13 2024 ] 	Batch(4500/7879) done. Loss: 0.0231  lr:0.000100
[ Thu Jul  4 04:12:31 2024 ] 	Batch(4600/7879) done. Loss: 0.0846  lr:0.000100
[ Thu Jul  4 04:12:49 2024 ] 	Batch(4700/7879) done. Loss: 0.0058  lr:0.000100
[ Thu Jul  4 04:13:07 2024 ] 	Batch(4800/7879) done. Loss: 0.1802  lr:0.000100
[ Thu Jul  4 04:13:26 2024 ] 	Batch(4900/7879) done. Loss: 0.2475  lr:0.000100
[ Thu Jul  4 04:13:44 2024 ] 
Training: Epoch [76/120], Step [4999], Loss: 0.011627805419266224, Training Accuracy: 97.225
[ Thu Jul  4 04:13:44 2024 ] 	Batch(5000/7879) done. Loss: 0.3640  lr:0.000100
[ Thu Jul  4 04:14:03 2024 ] 	Batch(5100/7879) done. Loss: 0.1066  lr:0.000100
[ Thu Jul  4 04:14:21 2024 ] 	Batch(5200/7879) done. Loss: 0.0625  lr:0.000100
[ Thu Jul  4 04:14:40 2024 ] 	Batch(5300/7879) done. Loss: 0.0443  lr:0.000100
[ Thu Jul  4 04:14:58 2024 ] 	Batch(5400/7879) done. Loss: 0.2434  lr:0.000100
[ Thu Jul  4 04:15:17 2024 ] 
Training: Epoch [76/120], Step [5499], Loss: 0.0051839351654052734, Training Accuracy: 97.30454545454546
[ Thu Jul  4 04:15:17 2024 ] 	Batch(5500/7879) done. Loss: 0.3964  lr:0.000100
[ Thu Jul  4 04:15:36 2024 ] 	Batch(5600/7879) done. Loss: 0.0723  lr:0.000100
[ Thu Jul  4 04:15:54 2024 ] 	Batch(5700/7879) done. Loss: 0.0128  lr:0.000100
[ Thu Jul  4 04:16:13 2024 ] 	Batch(5800/7879) done. Loss: 0.0198  lr:0.000100
[ Thu Jul  4 04:16:31 2024 ] 	Batch(5900/7879) done. Loss: 0.2325  lr:0.000100
[ Thu Jul  4 04:16:50 2024 ] 
Training: Epoch [76/120], Step [5999], Loss: 0.06262610852718353, Training Accuracy: 97.29375
[ Thu Jul  4 04:16:50 2024 ] 	Batch(6000/7879) done. Loss: 0.0395  lr:0.000100
[ Thu Jul  4 04:17:08 2024 ] 	Batch(6100/7879) done. Loss: 0.5110  lr:0.000100
[ Thu Jul  4 04:17:26 2024 ] 	Batch(6200/7879) done. Loss: 0.0043  lr:0.000100
[ Thu Jul  4 04:17:44 2024 ] 	Batch(6300/7879) done. Loss: 0.0228  lr:0.000100
[ Thu Jul  4 04:18:02 2024 ] 	Batch(6400/7879) done. Loss: 0.0519  lr:0.000100
[ Thu Jul  4 04:18:20 2024 ] 
Training: Epoch [76/120], Step [6499], Loss: 0.4583590626716614, Training Accuracy: 97.26538461538462
[ Thu Jul  4 04:18:20 2024 ] 	Batch(6500/7879) done. Loss: 0.1672  lr:0.000100
[ Thu Jul  4 04:18:38 2024 ] 	Batch(6600/7879) done. Loss: 0.0450  lr:0.000100
[ Thu Jul  4 04:18:56 2024 ] 	Batch(6700/7879) done. Loss: 0.1637  lr:0.000100
[ Thu Jul  4 04:19:14 2024 ] 	Batch(6800/7879) done. Loss: 0.0918  lr:0.000100
[ Thu Jul  4 04:19:33 2024 ] 	Batch(6900/7879) done. Loss: 0.1241  lr:0.000100
[ Thu Jul  4 04:19:51 2024 ] 
Training: Epoch [76/120], Step [6999], Loss: 0.15976887941360474, Training Accuracy: 97.25714285714285
[ Thu Jul  4 04:19:51 2024 ] 	Batch(7000/7879) done. Loss: 0.6829  lr:0.000100
[ Thu Jul  4 04:20:10 2024 ] 	Batch(7100/7879) done. Loss: 0.1094  lr:0.000100
[ Thu Jul  4 04:20:29 2024 ] 	Batch(7200/7879) done. Loss: 0.0505  lr:0.000100
[ Thu Jul  4 04:20:47 2024 ] 	Batch(7300/7879) done. Loss: 0.2092  lr:0.000100
[ Thu Jul  4 04:21:05 2024 ] 	Batch(7400/7879) done. Loss: 0.0162  lr:0.000100
[ Thu Jul  4 04:21:23 2024 ] 
Training: Epoch [76/120], Step [7499], Loss: 0.054943379014730453, Training Accuracy: 97.23666666666666
[ Thu Jul  4 04:21:23 2024 ] 	Batch(7500/7879) done. Loss: 0.1575  lr:0.000100
[ Thu Jul  4 04:21:41 2024 ] 	Batch(7600/7879) done. Loss: 0.1638  lr:0.000100
[ Thu Jul  4 04:21:59 2024 ] 	Batch(7700/7879) done. Loss: 0.1432  lr:0.000100
[ Thu Jul  4 04:22:17 2024 ] 	Batch(7800/7879) done. Loss: 0.0090  lr:0.000100
[ Thu Jul  4 04:22:31 2024 ] 	Mean training loss: 0.1082.
[ Thu Jul  4 04:22:31 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul  4 04:22:31 2024 ] Training epoch: 78
[ Thu Jul  4 04:22:32 2024 ] 	Batch(0/7879) done. Loss: 0.0180  lr:0.000100
[ Thu Jul  4 04:22:50 2024 ] 	Batch(100/7879) done. Loss: 0.0031  lr:0.000100
[ Thu Jul  4 04:23:07 2024 ] 	Batch(200/7879) done. Loss: 0.0217  lr:0.000100
[ Thu Jul  4 04:23:25 2024 ] 	Batch(300/7879) done. Loss: 0.0715  lr:0.000100
[ Thu Jul  4 04:23:43 2024 ] 	Batch(400/7879) done. Loss: 0.2999  lr:0.000100
[ Thu Jul  4 04:24:01 2024 ] 
Training: Epoch [77/120], Step [499], Loss: 0.00784698873758316, Training Accuracy: 97.15
[ Thu Jul  4 04:24:01 2024 ] 	Batch(500/7879) done. Loss: 0.1853  lr:0.000100
[ Thu Jul  4 04:24:19 2024 ] 	Batch(600/7879) done. Loss: 0.0045  lr:0.000100
[ Thu Jul  4 04:24:37 2024 ] 	Batch(700/7879) done. Loss: 0.0473  lr:0.000100
[ Thu Jul  4 04:24:55 2024 ] 	Batch(800/7879) done. Loss: 0.2301  lr:0.000100
[ Thu Jul  4 04:25:13 2024 ] 	Batch(900/7879) done. Loss: 0.0515  lr:0.000100
[ Thu Jul  4 04:25:31 2024 ] 
Training: Epoch [77/120], Step [999], Loss: 0.04420624300837517, Training Accuracy: 97.2625
[ Thu Jul  4 04:25:32 2024 ] 	Batch(1000/7879) done. Loss: 0.0920  lr:0.000100
[ Thu Jul  4 04:25:50 2024 ] 	Batch(1100/7879) done. Loss: 0.2302  lr:0.000100
[ Thu Jul  4 04:26:08 2024 ] 	Batch(1200/7879) done. Loss: 0.0323  lr:0.000100
[ Thu Jul  4 04:26:26 2024 ] 	Batch(1300/7879) done. Loss: 0.0087  lr:0.000100
[ Thu Jul  4 04:26:43 2024 ] 	Batch(1400/7879) done. Loss: 0.1029  lr:0.000100
[ Thu Jul  4 04:27:01 2024 ] 
Training: Epoch [77/120], Step [1499], Loss: 0.04036043956875801, Training Accuracy: 97.30833333333334
[ Thu Jul  4 04:27:01 2024 ] 	Batch(1500/7879) done. Loss: 0.1175  lr:0.000100
[ Thu Jul  4 04:27:19 2024 ] 	Batch(1600/7879) done. Loss: 0.0462  lr:0.000100
[ Thu Jul  4 04:27:37 2024 ] 	Batch(1700/7879) done. Loss: 0.0170  lr:0.000100
[ Thu Jul  4 04:27:55 2024 ] 	Batch(1800/7879) done. Loss: 0.0138  lr:0.000100
[ Thu Jul  4 04:28:13 2024 ] 	Batch(1900/7879) done. Loss: 0.0221  lr:0.000100
[ Thu Jul  4 04:28:31 2024 ] 
Training: Epoch [77/120], Step [1999], Loss: 0.05624082684516907, Training Accuracy: 97.38125
[ Thu Jul  4 04:28:31 2024 ] 	Batch(2000/7879) done. Loss: 0.0802  lr:0.000100
[ Thu Jul  4 04:28:49 2024 ] 	Batch(2100/7879) done. Loss: 0.0347  lr:0.000100
[ Thu Jul  4 04:29:07 2024 ] 	Batch(2200/7879) done. Loss: 0.0885  lr:0.000100
[ Thu Jul  4 04:29:25 2024 ] 	Batch(2300/7879) done. Loss: 0.0734  lr:0.000100
[ Thu Jul  4 04:29:43 2024 ] 	Batch(2400/7879) done. Loss: 0.0086  lr:0.000100
[ Thu Jul  4 04:30:01 2024 ] 
Training: Epoch [77/120], Step [2499], Loss: 0.034952618181705475, Training Accuracy: 97.32499999999999
[ Thu Jul  4 04:30:01 2024 ] 	Batch(2500/7879) done. Loss: 0.0052  lr:0.000100
[ Thu Jul  4 04:30:19 2024 ] 	Batch(2600/7879) done. Loss: 0.0064  lr:0.000100
[ Thu Jul  4 04:30:37 2024 ] 	Batch(2700/7879) done. Loss: 0.0629  lr:0.000100
[ Thu Jul  4 04:30:55 2024 ] 	Batch(2800/7879) done. Loss: 0.3873  lr:0.000100
[ Thu Jul  4 04:31:13 2024 ] 	Batch(2900/7879) done. Loss: 0.3725  lr:0.000100
[ Thu Jul  4 04:31:30 2024 ] 
Training: Epoch [77/120], Step [2999], Loss: 0.030358459800481796, Training Accuracy: 97.32499999999999
[ Thu Jul  4 04:31:31 2024 ] 	Batch(3000/7879) done. Loss: 0.0302  lr:0.000100
[ Thu Jul  4 04:31:48 2024 ] 	Batch(3100/7879) done. Loss: 0.1628  lr:0.000100
[ Thu Jul  4 04:32:06 2024 ] 	Batch(3200/7879) done. Loss: 0.2109  lr:0.000100
[ Thu Jul  4 04:32:24 2024 ] 	Batch(3300/7879) done. Loss: 0.0385  lr:0.000100
[ Thu Jul  4 04:32:42 2024 ] 	Batch(3400/7879) done. Loss: 0.0235  lr:0.000100
[ Thu Jul  4 04:33:00 2024 ] 
Training: Epoch [77/120], Step [3499], Loss: 0.005556482821702957, Training Accuracy: 97.30357142857143
[ Thu Jul  4 04:33:00 2024 ] 	Batch(3500/7879) done. Loss: 0.0225  lr:0.000100
[ Thu Jul  4 04:33:18 2024 ] 	Batch(3600/7879) done. Loss: 0.2814  lr:0.000100
[ Thu Jul  4 04:33:36 2024 ] 	Batch(3700/7879) done. Loss: 0.2412  lr:0.000100
[ Thu Jul  4 04:33:54 2024 ] 	Batch(3800/7879) done. Loss: 0.0126  lr:0.000100
[ Thu Jul  4 04:34:12 2024 ] 	Batch(3900/7879) done. Loss: 0.0764  lr:0.000100
[ Thu Jul  4 04:34:29 2024 ] 
Training: Epoch [77/120], Step [3999], Loss: 0.029638275504112244, Training Accuracy: 97.278125
[ Thu Jul  4 04:34:30 2024 ] 	Batch(4000/7879) done. Loss: 0.2119  lr:0.000100
[ Thu Jul  4 04:34:48 2024 ] 	Batch(4100/7879) done. Loss: 0.0047  lr:0.000100
[ Thu Jul  4 04:35:07 2024 ] 	Batch(4200/7879) done. Loss: 0.0098  lr:0.000100
[ Thu Jul  4 04:35:25 2024 ] 	Batch(4300/7879) done. Loss: 0.0828  lr:0.000100
[ Thu Jul  4 04:35:44 2024 ] 	Batch(4400/7879) done. Loss: 0.5513  lr:0.000100
[ Thu Jul  4 04:36:02 2024 ] 
Training: Epoch [77/120], Step [4499], Loss: 0.48342961072921753, Training Accuracy: 97.29166666666667
[ Thu Jul  4 04:36:02 2024 ] 	Batch(4500/7879) done. Loss: 0.1384  lr:0.000100
[ Thu Jul  4 04:36:20 2024 ] 	Batch(4600/7879) done. Loss: 0.0808  lr:0.000100
[ Thu Jul  4 04:36:38 2024 ] 	Batch(4700/7879) done. Loss: 0.0693  lr:0.000100
[ Thu Jul  4 04:36:56 2024 ] 	Batch(4800/7879) done. Loss: 0.0637  lr:0.000100
[ Thu Jul  4 04:37:13 2024 ] 	Batch(4900/7879) done. Loss: 0.0274  lr:0.000100
[ Thu Jul  4 04:37:31 2024 ] 
Training: Epoch [77/120], Step [4999], Loss: 0.009956365451216698, Training Accuracy: 97.30250000000001
[ Thu Jul  4 04:37:31 2024 ] 	Batch(5000/7879) done. Loss: 0.0693  lr:0.000100
[ Thu Jul  4 04:37:49 2024 ] 	Batch(5100/7879) done. Loss: 0.5118  lr:0.000100
[ Thu Jul  4 04:38:07 2024 ] 	Batch(5200/7879) done. Loss: 0.2774  lr:0.000100
[ Thu Jul  4 04:38:26 2024 ] 	Batch(5300/7879) done. Loss: 0.0025  lr:0.000100
[ Thu Jul  4 04:38:44 2024 ] 	Batch(5400/7879) done. Loss: 0.0393  lr:0.000100
[ Thu Jul  4 04:39:02 2024 ] 
Training: Epoch [77/120], Step [5499], Loss: 0.04020564630627632, Training Accuracy: 97.29772727272727
[ Thu Jul  4 04:39:02 2024 ] 	Batch(5500/7879) done. Loss: 0.0568  lr:0.000100
[ Thu Jul  4 04:39:20 2024 ] 	Batch(5600/7879) done. Loss: 0.0095  lr:0.000100
[ Thu Jul  4 04:39:38 2024 ] 	Batch(5700/7879) done. Loss: 0.0083  lr:0.000100
[ Thu Jul  4 04:39:56 2024 ] 	Batch(5800/7879) done. Loss: 0.1779  lr:0.000100
[ Thu Jul  4 04:40:14 2024 ] 	Batch(5900/7879) done. Loss: 0.1331  lr:0.000100
[ Thu Jul  4 04:40:32 2024 ] 
Training: Epoch [77/120], Step [5999], Loss: 0.04566366598010063, Training Accuracy: 97.25416666666666
[ Thu Jul  4 04:40:32 2024 ] 	Batch(6000/7879) done. Loss: 0.2203  lr:0.000100
[ Thu Jul  4 04:40:50 2024 ] 	Batch(6100/7879) done. Loss: 0.0430  lr:0.000100
[ Thu Jul  4 04:41:09 2024 ] 	Batch(6200/7879) done. Loss: 0.1005  lr:0.000100
[ Thu Jul  4 04:41:28 2024 ] 	Batch(6300/7879) done. Loss: 0.4904  lr:0.000100
[ Thu Jul  4 04:41:46 2024 ] 	Batch(6400/7879) done. Loss: 0.2622  lr:0.000100
[ Thu Jul  4 04:42:05 2024 ] 
Training: Epoch [77/120], Step [6499], Loss: 0.11301545053720474, Training Accuracy: 97.21346153846154
[ Thu Jul  4 04:42:05 2024 ] 	Batch(6500/7879) done. Loss: 0.0651  lr:0.000100
[ Thu Jul  4 04:42:23 2024 ] 	Batch(6600/7879) done. Loss: 0.1029  lr:0.000100
[ Thu Jul  4 04:42:42 2024 ] 	Batch(6700/7879) done. Loss: 0.0523  lr:0.000100
[ Thu Jul  4 04:43:00 2024 ] 	Batch(6800/7879) done. Loss: 0.0147  lr:0.000100
[ Thu Jul  4 04:43:18 2024 ] 	Batch(6900/7879) done. Loss: 0.0170  lr:0.000100
[ Thu Jul  4 04:43:36 2024 ] 
Training: Epoch [77/120], Step [6999], Loss: 0.053827859461307526, Training Accuracy: 97.23035714285714
[ Thu Jul  4 04:43:36 2024 ] 	Batch(7000/7879) done. Loss: 0.1647  lr:0.000100
[ Thu Jul  4 04:43:54 2024 ] 	Batch(7100/7879) done. Loss: 0.3883  lr:0.000100
[ Thu Jul  4 04:44:12 2024 ] 	Batch(7200/7879) done. Loss: 0.3461  lr:0.000100
[ Thu Jul  4 04:44:30 2024 ] 	Batch(7300/7879) done. Loss: 0.0013  lr:0.000100
[ Thu Jul  4 04:44:48 2024 ] 	Batch(7400/7879) done. Loss: 0.0352  lr:0.000100
[ Thu Jul  4 04:45:05 2024 ] 
Training: Epoch [77/120], Step [7499], Loss: 0.0337032675743103, Training Accuracy: 97.20833333333333
[ Thu Jul  4 04:45:06 2024 ] 	Batch(7500/7879) done. Loss: 0.0943  lr:0.000100
[ Thu Jul  4 04:45:24 2024 ] 	Batch(7600/7879) done. Loss: 0.1594  lr:0.000100
[ Thu Jul  4 04:45:41 2024 ] 	Batch(7700/7879) done. Loss: 0.0323  lr:0.000100
[ Thu Jul  4 04:45:59 2024 ] 	Batch(7800/7879) done. Loss: 0.2603  lr:0.000100
[ Thu Jul  4 04:46:13 2024 ] 	Mean training loss: 0.1031.
[ Thu Jul  4 04:46:13 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 04:46:13 2024 ] Training epoch: 79
[ Thu Jul  4 04:46:14 2024 ] 	Batch(0/7879) done. Loss: 0.0057  lr:0.000100
[ Thu Jul  4 04:46:32 2024 ] 	Batch(100/7879) done. Loss: 0.0161  lr:0.000100
[ Thu Jul  4 04:46:50 2024 ] 	Batch(200/7879) done. Loss: 0.5308  lr:0.000100
[ Thu Jul  4 04:47:07 2024 ] 	Batch(300/7879) done. Loss: 0.0151  lr:0.000100
[ Thu Jul  4 04:47:25 2024 ] 	Batch(400/7879) done. Loss: 0.0186  lr:0.000100
[ Thu Jul  4 04:47:43 2024 ] 
Training: Epoch [78/120], Step [499], Loss: 0.009379776194691658, Training Accuracy: 97.45
[ Thu Jul  4 04:47:43 2024 ] 	Batch(500/7879) done. Loss: 0.0794  lr:0.000100
[ Thu Jul  4 04:48:01 2024 ] 	Batch(600/7879) done. Loss: 0.0992  lr:0.000100
[ Thu Jul  4 04:48:19 2024 ] 	Batch(700/7879) done. Loss: 0.0388  lr:0.000100
[ Thu Jul  4 04:48:37 2024 ] 	Batch(800/7879) done. Loss: 0.0667  lr:0.000100
[ Thu Jul  4 04:48:56 2024 ] 	Batch(900/7879) done. Loss: 0.0034  lr:0.000100
[ Thu Jul  4 04:49:14 2024 ] 
Training: Epoch [78/120], Step [999], Loss: 0.08675247430801392, Training Accuracy: 97.475
[ Thu Jul  4 04:49:14 2024 ] 	Batch(1000/7879) done. Loss: 0.0890  lr:0.000100
[ Thu Jul  4 04:49:33 2024 ] 	Batch(1100/7879) done. Loss: 0.0080  lr:0.000100
[ Thu Jul  4 04:49:51 2024 ] 	Batch(1200/7879) done. Loss: 0.0562  lr:0.000100
[ Thu Jul  4 04:50:10 2024 ] 	Batch(1300/7879) done. Loss: 0.3393  lr:0.000100
[ Thu Jul  4 04:50:28 2024 ] 	Batch(1400/7879) done. Loss: 0.1680  lr:0.000100
[ Thu Jul  4 04:50:47 2024 ] 
Training: Epoch [78/120], Step [1499], Loss: 0.46039634943008423, Training Accuracy: 97.56666666666666
[ Thu Jul  4 04:50:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0080  lr:0.000100
[ Thu Jul  4 04:51:05 2024 ] 	Batch(1600/7879) done. Loss: 0.1242  lr:0.000100
[ Thu Jul  4 04:51:23 2024 ] 	Batch(1700/7879) done. Loss: 0.0201  lr:0.000100
[ Thu Jul  4 04:51:41 2024 ] 	Batch(1800/7879) done. Loss: 0.0030  lr:0.000100
[ Thu Jul  4 04:51:59 2024 ] 	Batch(1900/7879) done. Loss: 0.0254  lr:0.000100
[ Thu Jul  4 04:52:17 2024 ] 
Training: Epoch [78/120], Step [1999], Loss: 0.17168083786964417, Training Accuracy: 97.55
[ Thu Jul  4 04:52:17 2024 ] 	Batch(2000/7879) done. Loss: 0.0041  lr:0.000100
[ Thu Jul  4 04:52:35 2024 ] 	Batch(2100/7879) done. Loss: 0.0247  lr:0.000100
[ Thu Jul  4 04:52:53 2024 ] 	Batch(2200/7879) done. Loss: 0.0271  lr:0.000100
[ Thu Jul  4 04:53:11 2024 ] 	Batch(2300/7879) done. Loss: 0.1452  lr:0.000100
[ Thu Jul  4 04:53:29 2024 ] 	Batch(2400/7879) done. Loss: 0.0503  lr:0.000100
[ Thu Jul  4 04:53:46 2024 ] 
Training: Epoch [78/120], Step [2499], Loss: 0.12150786817073822, Training Accuracy: 97.46000000000001
[ Thu Jul  4 04:53:46 2024 ] 	Batch(2500/7879) done. Loss: 0.3913  lr:0.000100
[ Thu Jul  4 04:54:04 2024 ] 	Batch(2600/7879) done. Loss: 0.0065  lr:0.000100
[ Thu Jul  4 04:54:22 2024 ] 	Batch(2700/7879) done. Loss: 0.1107  lr:0.000100
[ Thu Jul  4 04:54:40 2024 ] 	Batch(2800/7879) done. Loss: 0.0282  lr:0.000100
[ Thu Jul  4 04:54:58 2024 ] 	Batch(2900/7879) done. Loss: 0.0246  lr:0.000100
[ Thu Jul  4 04:55:17 2024 ] 
Training: Epoch [78/120], Step [2999], Loss: 0.04618644714355469, Training Accuracy: 97.45
[ Thu Jul  4 04:55:17 2024 ] 	Batch(3000/7879) done. Loss: 0.0477  lr:0.000100
[ Thu Jul  4 04:55:35 2024 ] 	Batch(3100/7879) done. Loss: 0.0075  lr:0.000100
[ Thu Jul  4 04:55:54 2024 ] 	Batch(3200/7879) done. Loss: 0.1738  lr:0.000100
[ Thu Jul  4 04:56:12 2024 ] 	Batch(3300/7879) done. Loss: 0.2806  lr:0.000100
[ Thu Jul  4 04:56:31 2024 ] 	Batch(3400/7879) done. Loss: 0.0610  lr:0.000100
[ Thu Jul  4 04:56:49 2024 ] 
Training: Epoch [78/120], Step [3499], Loss: 0.2234995812177658, Training Accuracy: 97.40357142857144
[ Thu Jul  4 04:56:49 2024 ] 	Batch(3500/7879) done. Loss: 0.0114  lr:0.000100
[ Thu Jul  4 04:57:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0471  lr:0.000100
[ Thu Jul  4 04:57:26 2024 ] 	Batch(3700/7879) done. Loss: 0.0859  lr:0.000100
[ Thu Jul  4 04:57:44 2024 ] 	Batch(3800/7879) done. Loss: 0.0859  lr:0.000100
[ Thu Jul  4 04:58:02 2024 ] 	Batch(3900/7879) done. Loss: 0.0870  lr:0.000100
[ Thu Jul  4 04:58:19 2024 ] 
Training: Epoch [78/120], Step [3999], Loss: 0.06104619428515434, Training Accuracy: 97.403125
[ Thu Jul  4 04:58:20 2024 ] 	Batch(4000/7879) done. Loss: 0.1287  lr:0.000100
[ Thu Jul  4 04:58:38 2024 ] 	Batch(4100/7879) done. Loss: 0.0331  lr:0.000100
[ Thu Jul  4 04:58:57 2024 ] 	Batch(4200/7879) done. Loss: 0.0348  lr:0.000100
[ Thu Jul  4 04:59:15 2024 ] 	Batch(4300/7879) done. Loss: 0.3966  lr:0.000100
[ Thu Jul  4 04:59:34 2024 ] 	Batch(4400/7879) done. Loss: 0.0590  lr:0.000100
[ Thu Jul  4 04:59:52 2024 ] 
Training: Epoch [78/120], Step [4499], Loss: 0.020758677273988724, Training Accuracy: 97.43333333333334
[ Thu Jul  4 04:59:53 2024 ] 	Batch(4500/7879) done. Loss: 0.0244  lr:0.000100
[ Thu Jul  4 05:00:11 2024 ] 	Batch(4600/7879) done. Loss: 0.0708  lr:0.000100
[ Thu Jul  4 05:00:30 2024 ] 	Batch(4700/7879) done. Loss: 0.0872  lr:0.000100
[ Thu Jul  4 05:00:48 2024 ] 	Batch(4800/7879) done. Loss: 0.1042  lr:0.000100
[ Thu Jul  4 05:01:07 2024 ] 	Batch(4900/7879) done. Loss: 0.0010  lr:0.000100
[ Thu Jul  4 05:01:25 2024 ] 
Training: Epoch [78/120], Step [4999], Loss: 0.30240368843078613, Training Accuracy: 97.4325
[ Thu Jul  4 05:01:25 2024 ] 	Batch(5000/7879) done. Loss: 0.0178  lr:0.000100
[ Thu Jul  4 05:01:44 2024 ] 	Batch(5100/7879) done. Loss: 0.0488  lr:0.000100
[ Thu Jul  4 05:02:02 2024 ] 	Batch(5200/7879) done. Loss: 0.0055  lr:0.000100
[ Thu Jul  4 05:02:21 2024 ] 	Batch(5300/7879) done. Loss: 0.0479  lr:0.000100
[ Thu Jul  4 05:02:39 2024 ] 	Batch(5400/7879) done. Loss: 0.1498  lr:0.000100
[ Thu Jul  4 05:02:56 2024 ] 
Training: Epoch [78/120], Step [5499], Loss: 0.3498649299144745, Training Accuracy: 97.38863636363637
[ Thu Jul  4 05:02:56 2024 ] 	Batch(5500/7879) done. Loss: 0.0202  lr:0.000100
[ Thu Jul  4 05:03:14 2024 ] 	Batch(5600/7879) done. Loss: 0.1529  lr:0.000100
[ Thu Jul  4 05:03:33 2024 ] 	Batch(5700/7879) done. Loss: 0.1208  lr:0.000100
[ Thu Jul  4 05:03:52 2024 ] 	Batch(5800/7879) done. Loss: 0.1082  lr:0.000100
[ Thu Jul  4 05:04:10 2024 ] 	Batch(5900/7879) done. Loss: 0.0690  lr:0.000100
[ Thu Jul  4 05:04:28 2024 ] 
Training: Epoch [78/120], Step [5999], Loss: 0.008160608820617199, Training Accuracy: 97.32291666666667
[ Thu Jul  4 05:04:29 2024 ] 	Batch(6000/7879) done. Loss: 0.0200  lr:0.000100
[ Thu Jul  4 05:04:47 2024 ] 	Batch(6100/7879) done. Loss: 0.0275  lr:0.000100
[ Thu Jul  4 05:05:04 2024 ] 	Batch(6200/7879) done. Loss: 0.0417  lr:0.000100
[ Thu Jul  4 05:05:22 2024 ] 	Batch(6300/7879) done. Loss: 0.1100  lr:0.000100
[ Thu Jul  4 05:05:40 2024 ] 	Batch(6400/7879) done. Loss: 0.0294  lr:0.000100
[ Thu Jul  4 05:05:58 2024 ] 
Training: Epoch [78/120], Step [6499], Loss: 0.022333482280373573, Training Accuracy: 97.28269230769232
[ Thu Jul  4 05:05:58 2024 ] 	Batch(6500/7879) done. Loss: 0.1138  lr:0.000100
[ Thu Jul  4 05:06:16 2024 ] 	Batch(6600/7879) done. Loss: 0.0055  lr:0.000100
[ Thu Jul  4 05:06:34 2024 ] 	Batch(6700/7879) done. Loss: 0.0114  lr:0.000100
[ Thu Jul  4 05:06:52 2024 ] 	Batch(6800/7879) done. Loss: 0.2055  lr:0.000100
[ Thu Jul  4 05:07:10 2024 ] 	Batch(6900/7879) done. Loss: 0.0182  lr:0.000100
[ Thu Jul  4 05:07:27 2024 ] 
Training: Epoch [78/120], Step [6999], Loss: 0.4050397276878357, Training Accuracy: 97.26785714285714
[ Thu Jul  4 05:07:28 2024 ] 	Batch(7000/7879) done. Loss: 0.2282  lr:0.000100
[ Thu Jul  4 05:07:45 2024 ] 	Batch(7100/7879) done. Loss: 0.0504  lr:0.000100
[ Thu Jul  4 05:08:03 2024 ] 	Batch(7200/7879) done. Loss: 0.0017  lr:0.000100
[ Thu Jul  4 05:08:21 2024 ] 	Batch(7300/7879) done. Loss: 0.0066  lr:0.000100
[ Thu Jul  4 05:08:39 2024 ] 	Batch(7400/7879) done. Loss: 0.0094  lr:0.000100
[ Thu Jul  4 05:08:57 2024 ] 
Training: Epoch [78/120], Step [7499], Loss: 0.06298395991325378, Training Accuracy: 97.27
[ Thu Jul  4 05:08:57 2024 ] 	Batch(7500/7879) done. Loss: 0.1485  lr:0.000100
[ Thu Jul  4 05:09:15 2024 ] 	Batch(7600/7879) done. Loss: 0.0082  lr:0.000100
[ Thu Jul  4 05:09:33 2024 ] 	Batch(7700/7879) done. Loss: 0.0652  lr:0.000100
[ Thu Jul  4 05:09:51 2024 ] 	Batch(7800/7879) done. Loss: 0.0870  lr:0.000100
[ Thu Jul  4 05:10:05 2024 ] 	Mean training loss: 0.1031.
[ Thu Jul  4 05:10:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 05:10:05 2024 ] Training epoch: 80
[ Thu Jul  4 05:10:05 2024 ] 	Batch(0/7879) done. Loss: 0.0484  lr:0.000100
[ Thu Jul  4 05:10:24 2024 ] 	Batch(100/7879) done. Loss: 0.0435  lr:0.000100
[ Thu Jul  4 05:10:42 2024 ] 	Batch(200/7879) done. Loss: 0.0195  lr:0.000100
[ Thu Jul  4 05:11:01 2024 ] 	Batch(300/7879) done. Loss: 0.0426  lr:0.000100
[ Thu Jul  4 05:11:19 2024 ] 	Batch(400/7879) done. Loss: 0.1788  lr:0.000100
[ Thu Jul  4 05:11:38 2024 ] 
Training: Epoch [79/120], Step [499], Loss: 0.5472553968429565, Training Accuracy: 97.05
[ Thu Jul  4 05:11:38 2024 ] 	Batch(500/7879) done. Loss: 0.0114  lr:0.000100
[ Thu Jul  4 05:11:56 2024 ] 	Batch(600/7879) done. Loss: 0.0457  lr:0.000100
[ Thu Jul  4 05:12:15 2024 ] 	Batch(700/7879) done. Loss: 0.6526  lr:0.000100
[ Thu Jul  4 05:12:34 2024 ] 	Batch(800/7879) done. Loss: 0.0662  lr:0.000100
[ Thu Jul  4 05:12:52 2024 ] 	Batch(900/7879) done. Loss: 0.0094  lr:0.000100
[ Thu Jul  4 05:13:10 2024 ] 
Training: Epoch [79/120], Step [999], Loss: 0.03773823752999306, Training Accuracy: 97.25
[ Thu Jul  4 05:13:11 2024 ] 	Batch(1000/7879) done. Loss: 0.2403  lr:0.000100
[ Thu Jul  4 05:13:29 2024 ] 	Batch(1100/7879) done. Loss: 0.0368  lr:0.000100
[ Thu Jul  4 05:13:48 2024 ] 	Batch(1200/7879) done. Loss: 0.0724  lr:0.000100
[ Thu Jul  4 05:14:06 2024 ] 	Batch(1300/7879) done. Loss: 0.1985  lr:0.000100
[ Thu Jul  4 05:14:24 2024 ] 	Batch(1400/7879) done. Loss: 0.0689  lr:0.000100
[ Thu Jul  4 05:14:41 2024 ] 
Training: Epoch [79/120], Step [1499], Loss: 0.0049926964566111565, Training Accuracy: 97.34166666666667
[ Thu Jul  4 05:14:41 2024 ] 	Batch(1500/7879) done. Loss: 0.2213  lr:0.000100
[ Thu Jul  4 05:14:59 2024 ] 	Batch(1600/7879) done. Loss: 0.0852  lr:0.000100
[ Thu Jul  4 05:15:18 2024 ] 	Batch(1700/7879) done. Loss: 0.0973  lr:0.000100
[ Thu Jul  4 05:15:36 2024 ] 	Batch(1800/7879) done. Loss: 0.0191  lr:0.000100
[ Thu Jul  4 05:15:55 2024 ] 	Batch(1900/7879) done. Loss: 0.2034  lr:0.000100
[ Thu Jul  4 05:16:13 2024 ] 
Training: Epoch [79/120], Step [1999], Loss: 0.07669766247272491, Training Accuracy: 97.36875
[ Thu Jul  4 05:16:13 2024 ] 	Batch(2000/7879) done. Loss: 0.0343  lr:0.000100
[ Thu Jul  4 05:16:31 2024 ] 	Batch(2100/7879) done. Loss: 0.0118  lr:0.000100
[ Thu Jul  4 05:16:49 2024 ] 	Batch(2200/7879) done. Loss: 0.0111  lr:0.000100
[ Thu Jul  4 05:17:06 2024 ] 	Batch(2300/7879) done. Loss: 0.2825  lr:0.000100
[ Thu Jul  4 05:17:24 2024 ] 	Batch(2400/7879) done. Loss: 0.0129  lr:0.000100
[ Thu Jul  4 05:17:42 2024 ] 
Training: Epoch [79/120], Step [2499], Loss: 0.008515428751707077, Training Accuracy: 97.30499999999999
[ Thu Jul  4 05:17:42 2024 ] 	Batch(2500/7879) done. Loss: 0.1924  lr:0.000100
[ Thu Jul  4 05:18:00 2024 ] 	Batch(2600/7879) done. Loss: 0.0362  lr:0.000100
[ Thu Jul  4 05:18:18 2024 ] 	Batch(2700/7879) done. Loss: 0.0537  lr:0.000100
[ Thu Jul  4 05:18:36 2024 ] 	Batch(2800/7879) done. Loss: 0.2472  lr:0.000100
[ Thu Jul  4 05:18:54 2024 ] 	Batch(2900/7879) done. Loss: 0.0193  lr:0.000100
[ Thu Jul  4 05:19:12 2024 ] 
Training: Epoch [79/120], Step [2999], Loss: 0.03933334723114967, Training Accuracy: 97.29166666666667
[ Thu Jul  4 05:19:12 2024 ] 	Batch(3000/7879) done. Loss: 0.0874  lr:0.000100
[ Thu Jul  4 05:19:30 2024 ] 	Batch(3100/7879) done. Loss: 0.3172  lr:0.000100
[ Thu Jul  4 05:19:48 2024 ] 	Batch(3200/7879) done. Loss: 0.0389  lr:0.000100
[ Thu Jul  4 05:20:06 2024 ] 	Batch(3300/7879) done. Loss: 0.0063  lr:0.000100
[ Thu Jul  4 05:20:23 2024 ] 	Batch(3400/7879) done. Loss: 0.5864  lr:0.000100
[ Thu Jul  4 05:20:41 2024 ] 
Training: Epoch [79/120], Step [3499], Loss: 0.024252193048596382, Training Accuracy: 97.27857142857142
[ Thu Jul  4 05:20:41 2024 ] 	Batch(3500/7879) done. Loss: 0.0462  lr:0.000100
[ Thu Jul  4 05:20:59 2024 ] 	Batch(3600/7879) done. Loss: 0.0606  lr:0.000100
[ Thu Jul  4 05:21:17 2024 ] 	Batch(3700/7879) done. Loss: 0.1839  lr:0.000100
[ Thu Jul  4 05:21:35 2024 ] 	Batch(3800/7879) done. Loss: 0.0377  lr:0.000100
[ Thu Jul  4 05:21:53 2024 ] 	Batch(3900/7879) done. Loss: 0.0385  lr:0.000100
[ Thu Jul  4 05:22:11 2024 ] 
Training: Epoch [79/120], Step [3999], Loss: 0.012181246653199196, Training Accuracy: 97.26875
[ Thu Jul  4 05:22:11 2024 ] 	Batch(4000/7879) done. Loss: 0.0402  lr:0.000100
[ Thu Jul  4 05:22:29 2024 ] 	Batch(4100/7879) done. Loss: 0.1517  lr:0.000100
[ Thu Jul  4 05:22:48 2024 ] 	Batch(4200/7879) done. Loss: 0.0071  lr:0.000100
[ Thu Jul  4 05:23:06 2024 ] 	Batch(4300/7879) done. Loss: 0.1295  lr:0.000100
[ Thu Jul  4 05:23:24 2024 ] 	Batch(4400/7879) done. Loss: 0.0051  lr:0.000100
[ Thu Jul  4 05:23:42 2024 ] 
Training: Epoch [79/120], Step [4499], Loss: 0.21806533634662628, Training Accuracy: 97.3
[ Thu Jul  4 05:23:42 2024 ] 	Batch(4500/7879) done. Loss: 0.0915  lr:0.000100
[ Thu Jul  4 05:24:00 2024 ] 	Batch(4600/7879) done. Loss: 0.0175  lr:0.000100
[ Thu Jul  4 05:24:18 2024 ] 	Batch(4700/7879) done. Loss: 0.0073  lr:0.000100
[ Thu Jul  4 05:24:36 2024 ] 	Batch(4800/7879) done. Loss: 0.5861  lr:0.000100
[ Thu Jul  4 05:24:53 2024 ] 	Batch(4900/7879) done. Loss: 0.0730  lr:0.000100
[ Thu Jul  4 05:25:11 2024 ] 
Training: Epoch [79/120], Step [4999], Loss: 0.11496452242136002, Training Accuracy: 97.30250000000001
[ Thu Jul  4 05:25:11 2024 ] 	Batch(5000/7879) done. Loss: 0.5650  lr:0.000100
[ Thu Jul  4 05:25:29 2024 ] 	Batch(5100/7879) done. Loss: 0.0044  lr:0.000100
[ Thu Jul  4 05:25:47 2024 ] 	Batch(5200/7879) done. Loss: 0.2494  lr:0.000100
[ Thu Jul  4 05:26:05 2024 ] 	Batch(5300/7879) done. Loss: 0.0082  lr:0.000100
[ Thu Jul  4 05:26:23 2024 ] 	Batch(5400/7879) done. Loss: 0.0233  lr:0.000100
[ Thu Jul  4 05:26:41 2024 ] 
Training: Epoch [79/120], Step [5499], Loss: 0.0037467465735971928, Training Accuracy: 97.3340909090909
[ Thu Jul  4 05:26:41 2024 ] 	Batch(5500/7879) done. Loss: 0.0608  lr:0.000100
[ Thu Jul  4 05:26:59 2024 ] 	Batch(5600/7879) done. Loss: 0.0896  lr:0.000100
[ Thu Jul  4 05:27:17 2024 ] 	Batch(5700/7879) done. Loss: 0.0765  lr:0.000100
[ Thu Jul  4 05:27:35 2024 ] 	Batch(5800/7879) done. Loss: 0.4558  lr:0.000100
[ Thu Jul  4 05:27:53 2024 ] 	Batch(5900/7879) done. Loss: 0.0085  lr:0.000100
[ Thu Jul  4 05:28:11 2024 ] 
Training: Epoch [79/120], Step [5999], Loss: 0.04675380513072014, Training Accuracy: 97.32291666666667
[ Thu Jul  4 05:28:11 2024 ] 	Batch(6000/7879) done. Loss: 0.0153  lr:0.000100
[ Thu Jul  4 05:28:29 2024 ] 	Batch(6100/7879) done. Loss: 0.0801  lr:0.000100
[ Thu Jul  4 05:28:47 2024 ] 	Batch(6200/7879) done. Loss: 0.0255  lr:0.000100
[ Thu Jul  4 05:29:04 2024 ] 	Batch(6300/7879) done. Loss: 0.0094  lr:0.000100
[ Thu Jul  4 05:29:23 2024 ] 	Batch(6400/7879) done. Loss: 0.1144  lr:0.000100
[ Thu Jul  4 05:29:40 2024 ] 
Training: Epoch [79/120], Step [6499], Loss: 0.032881010323762894, Training Accuracy: 97.34807692307692
[ Thu Jul  4 05:29:40 2024 ] 	Batch(6500/7879) done. Loss: 0.0180  lr:0.000100
[ Thu Jul  4 05:29:58 2024 ] 	Batch(6600/7879) done. Loss: 0.0107  lr:0.000100
[ Thu Jul  4 05:30:16 2024 ] 	Batch(6700/7879) done. Loss: 0.0281  lr:0.000100
[ Thu Jul  4 05:30:34 2024 ] 	Batch(6800/7879) done. Loss: 0.0506  lr:0.000100
[ Thu Jul  4 05:30:52 2024 ] 	Batch(6900/7879) done. Loss: 0.0729  lr:0.000100
[ Thu Jul  4 05:31:10 2024 ] 
Training: Epoch [79/120], Step [6999], Loss: 0.023842133581638336, Training Accuracy: 97.35714285714285
[ Thu Jul  4 05:31:10 2024 ] 	Batch(7000/7879) done. Loss: 0.0125  lr:0.000100
[ Thu Jul  4 05:31:28 2024 ] 	Batch(7100/7879) done. Loss: 0.0374  lr:0.000100
[ Thu Jul  4 05:31:46 2024 ] 	Batch(7200/7879) done. Loss: 0.1373  lr:0.000100
[ Thu Jul  4 05:32:04 2024 ] 	Batch(7300/7879) done. Loss: 0.1220  lr:0.000100
[ Thu Jul  4 05:32:22 2024 ] 	Batch(7400/7879) done. Loss: 0.0559  lr:0.000100
[ Thu Jul  4 05:32:39 2024 ] 
Training: Epoch [79/120], Step [7499], Loss: 0.0032316846773028374, Training Accuracy: 97.355
[ Thu Jul  4 05:32:39 2024 ] 	Batch(7500/7879) done. Loss: 0.0388  lr:0.000100
[ Thu Jul  4 05:32:57 2024 ] 	Batch(7600/7879) done. Loss: 0.0945  lr:0.000100
[ Thu Jul  4 05:33:16 2024 ] 	Batch(7700/7879) done. Loss: 0.5630  lr:0.000100
[ Thu Jul  4 05:33:34 2024 ] 	Batch(7800/7879) done. Loss: 0.1682  lr:0.000100
[ Thu Jul  4 05:33:49 2024 ] 	Mean training loss: 0.1025.
[ Thu Jul  4 05:33:49 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 05:33:49 2024 ] Eval epoch: 80
[ Thu Jul  4 05:38:35 2024 ] 	Mean val loss of 6365 batches: 1.750559840593437.
[ Thu Jul  4 05:38:35 2024 ] Training epoch: 81
[ Thu Jul  4 05:38:36 2024 ] 	Batch(0/7879) done. Loss: 0.0041  lr:0.000100
[ Thu Jul  4 05:38:54 2024 ] 	Batch(100/7879) done. Loss: 0.0110  lr:0.000100
[ Thu Jul  4 05:39:11 2024 ] 	Batch(200/7879) done. Loss: 0.0149  lr:0.000100
[ Thu Jul  4 05:39:29 2024 ] 	Batch(300/7879) done. Loss: 0.1545  lr:0.000100
[ Thu Jul  4 05:39:47 2024 ] 	Batch(400/7879) done. Loss: 0.0014  lr:0.000100
[ Thu Jul  4 05:40:05 2024 ] 
Training: Epoch [80/120], Step [499], Loss: 0.20266301929950714, Training Accuracy: 97.075
[ Thu Jul  4 05:40:05 2024 ] 	Batch(500/7879) done. Loss: 0.0768  lr:0.000100
[ Thu Jul  4 05:40:23 2024 ] 	Batch(600/7879) done. Loss: 0.0081  lr:0.000100
[ Thu Jul  4 05:40:41 2024 ] 	Batch(700/7879) done. Loss: 0.0778  lr:0.000100
[ Thu Jul  4 05:40:59 2024 ] 	Batch(800/7879) done. Loss: 0.0316  lr:0.000100
[ Thu Jul  4 05:41:17 2024 ] 	Batch(900/7879) done. Loss: 0.3967  lr:0.000100
[ Thu Jul  4 05:41:34 2024 ] 
Training: Epoch [80/120], Step [999], Loss: 0.0894046202301979, Training Accuracy: 97.28750000000001
[ Thu Jul  4 05:41:34 2024 ] 	Batch(1000/7879) done. Loss: 0.1523  lr:0.000100
[ Thu Jul  4 05:41:52 2024 ] 	Batch(1100/7879) done. Loss: 0.0088  lr:0.000100
[ Thu Jul  4 05:42:10 2024 ] 	Batch(1200/7879) done. Loss: 0.0517  lr:0.000100
[ Thu Jul  4 05:42:28 2024 ] 	Batch(1300/7879) done. Loss: 0.0968  lr:0.000100
[ Thu Jul  4 05:42:46 2024 ] 	Batch(1400/7879) done. Loss: 0.0129  lr:0.000100
[ Thu Jul  4 05:43:04 2024 ] 
Training: Epoch [80/120], Step [1499], Loss: 0.1363474428653717, Training Accuracy: 97.30833333333334
[ Thu Jul  4 05:43:04 2024 ] 	Batch(1500/7879) done. Loss: 0.1380  lr:0.000100
[ Thu Jul  4 05:43:22 2024 ] 	Batch(1600/7879) done. Loss: 0.0170  lr:0.000100
[ Thu Jul  4 05:43:40 2024 ] 	Batch(1700/7879) done. Loss: 0.1571  lr:0.000100
[ Thu Jul  4 05:43:58 2024 ] 	Batch(1800/7879) done. Loss: 0.0074  lr:0.000100
[ Thu Jul  4 05:44:16 2024 ] 	Batch(1900/7879) done. Loss: 0.1218  lr:0.000100
[ Thu Jul  4 05:44:33 2024 ] 
Training: Epoch [80/120], Step [1999], Loss: 0.583839476108551, Training Accuracy: 97.3125
[ Thu Jul  4 05:44:34 2024 ] 	Batch(2000/7879) done. Loss: 0.0587  lr:0.000100
[ Thu Jul  4 05:44:51 2024 ] 	Batch(2100/7879) done. Loss: 0.0108  lr:0.000100
[ Thu Jul  4 05:45:09 2024 ] 	Batch(2200/7879) done. Loss: 0.0861  lr:0.000100
[ Thu Jul  4 05:45:27 2024 ] 	Batch(2300/7879) done. Loss: 0.0184  lr:0.000100
[ Thu Jul  4 05:45:45 2024 ] 	Batch(2400/7879) done. Loss: 0.0245  lr:0.000100
[ Thu Jul  4 05:46:03 2024 ] 
Training: Epoch [80/120], Step [2499], Loss: 0.0378592424094677, Training Accuracy: 97.285
[ Thu Jul  4 05:46:04 2024 ] 	Batch(2500/7879) done. Loss: 0.1180  lr:0.000100
[ Thu Jul  4 05:46:22 2024 ] 	Batch(2600/7879) done. Loss: 0.0756  lr:0.000100
[ Thu Jul  4 05:46:41 2024 ] 	Batch(2700/7879) done. Loss: 0.0270  lr:0.000100
[ Thu Jul  4 05:46:59 2024 ] 	Batch(2800/7879) done. Loss: 0.0180  lr:0.000100
[ Thu Jul  4 05:47:18 2024 ] 	Batch(2900/7879) done. Loss: 0.0380  lr:0.000100
[ Thu Jul  4 05:47:36 2024 ] 
Training: Epoch [80/120], Step [2999], Loss: 0.024076111614704132, Training Accuracy: 97.35000000000001
[ Thu Jul  4 05:47:36 2024 ] 	Batch(3000/7879) done. Loss: 0.2946  lr:0.000100
[ Thu Jul  4 05:47:55 2024 ] 	Batch(3100/7879) done. Loss: 0.1088  lr:0.000100
[ Thu Jul  4 05:48:13 2024 ] 	Batch(3200/7879) done. Loss: 0.1169  lr:0.000100
[ Thu Jul  4 05:48:32 2024 ] 	Batch(3300/7879) done. Loss: 0.0390  lr:0.000100
[ Thu Jul  4 05:48:50 2024 ] 	Batch(3400/7879) done. Loss: 0.0793  lr:0.000100
[ Thu Jul  4 05:49:09 2024 ] 
Training: Epoch [80/120], Step [3499], Loss: 0.02358989045023918, Training Accuracy: 97.37142857142858
[ Thu Jul  4 05:49:09 2024 ] 	Batch(3500/7879) done. Loss: 0.1096  lr:0.000100
[ Thu Jul  4 05:49:28 2024 ] 	Batch(3600/7879) done. Loss: 0.0154  lr:0.000100
[ Thu Jul  4 05:49:46 2024 ] 	Batch(3700/7879) done. Loss: 0.0027  lr:0.000100
[ Thu Jul  4 05:50:04 2024 ] 	Batch(3800/7879) done. Loss: 0.1667  lr:0.000100
[ Thu Jul  4 05:50:22 2024 ] 	Batch(3900/7879) done. Loss: 0.1269  lr:0.000100
[ Thu Jul  4 05:50:39 2024 ] 
Training: Epoch [80/120], Step [3999], Loss: 0.01309138722717762, Training Accuracy: 97.36875
[ Thu Jul  4 05:50:40 2024 ] 	Batch(4000/7879) done. Loss: 0.0098  lr:0.000100
[ Thu Jul  4 05:50:58 2024 ] 	Batch(4100/7879) done. Loss: 0.0632  lr:0.000100
[ Thu Jul  4 05:51:15 2024 ] 	Batch(4200/7879) done. Loss: 0.3614  lr:0.000100
[ Thu Jul  4 05:51:33 2024 ] 	Batch(4300/7879) done. Loss: 0.0463  lr:0.000100
[ Thu Jul  4 05:51:51 2024 ] 	Batch(4400/7879) done. Loss: 0.0302  lr:0.000100
[ Thu Jul  4 05:52:09 2024 ] 
Training: Epoch [80/120], Step [4499], Loss: 0.12275221198797226, Training Accuracy: 97.36666666666667
[ Thu Jul  4 05:52:09 2024 ] 	Batch(4500/7879) done. Loss: 0.3157  lr:0.000100
[ Thu Jul  4 05:52:27 2024 ] 	Batch(4600/7879) done. Loss: 0.0116  lr:0.000100
[ Thu Jul  4 05:52:45 2024 ] 	Batch(4700/7879) done. Loss: 0.1773  lr:0.000100
[ Thu Jul  4 05:53:03 2024 ] 	Batch(4800/7879) done. Loss: 0.1317  lr:0.000100
[ Thu Jul  4 05:53:21 2024 ] 	Batch(4900/7879) done. Loss: 0.0615  lr:0.000100
[ Thu Jul  4 05:53:38 2024 ] 
Training: Epoch [80/120], Step [4999], Loss: 0.35125717520713806, Training Accuracy: 97.35249999999999
[ Thu Jul  4 05:53:39 2024 ] 	Batch(5000/7879) done. Loss: 0.0087  lr:0.000100
[ Thu Jul  4 05:53:56 2024 ] 	Batch(5100/7879) done. Loss: 0.0292  lr:0.000100
[ Thu Jul  4 05:54:14 2024 ] 	Batch(5200/7879) done. Loss: 0.0210  lr:0.000100
[ Thu Jul  4 05:54:32 2024 ] 	Batch(5300/7879) done. Loss: 0.1431  lr:0.000100
[ Thu Jul  4 05:54:50 2024 ] 	Batch(5400/7879) done. Loss: 0.0106  lr:0.000100
[ Thu Jul  4 05:55:08 2024 ] 
Training: Epoch [80/120], Step [5499], Loss: 0.22579452395439148, Training Accuracy: 97.36363636363636
[ Thu Jul  4 05:55:08 2024 ] 	Batch(5500/7879) done. Loss: 0.0223  lr:0.000100
[ Thu Jul  4 05:55:26 2024 ] 	Batch(5600/7879) done. Loss: 0.0194  lr:0.000100
[ Thu Jul  4 05:55:44 2024 ] 	Batch(5700/7879) done. Loss: 0.1734  lr:0.000100
[ Thu Jul  4 05:56:02 2024 ] 	Batch(5800/7879) done. Loss: 0.0062  lr:0.000100
[ Thu Jul  4 05:56:20 2024 ] 	Batch(5900/7879) done. Loss: 0.2016  lr:0.000100
[ Thu Jul  4 05:56:38 2024 ] 
Training: Epoch [80/120], Step [5999], Loss: 0.007791974116116762, Training Accuracy: 97.33333333333334
[ Thu Jul  4 05:56:38 2024 ] 	Batch(6000/7879) done. Loss: 0.0613  lr:0.000100
[ Thu Jul  4 05:56:57 2024 ] 	Batch(6100/7879) done. Loss: 0.1266  lr:0.000100
[ Thu Jul  4 05:57:15 2024 ] 	Batch(6200/7879) done. Loss: 0.0068  lr:0.000100
[ Thu Jul  4 05:57:34 2024 ] 	Batch(6300/7879) done. Loss: 0.0364  lr:0.000100
[ Thu Jul  4 05:57:52 2024 ] 	Batch(6400/7879) done. Loss: 0.0863  lr:0.000100
[ Thu Jul  4 05:58:11 2024 ] 
Training: Epoch [80/120], Step [6499], Loss: 0.01808810979127884, Training Accuracy: 97.33653846153845
[ Thu Jul  4 05:58:11 2024 ] 	Batch(6500/7879) done. Loss: 0.3275  lr:0.000100
[ Thu Jul  4 05:58:29 2024 ] 	Batch(6600/7879) done. Loss: 0.0375  lr:0.000100
[ Thu Jul  4 05:58:48 2024 ] 	Batch(6700/7879) done. Loss: 0.0254  lr:0.000100
[ Thu Jul  4 05:59:06 2024 ] 	Batch(6800/7879) done. Loss: 0.1391  lr:0.000100
[ Thu Jul  4 05:59:24 2024 ] 	Batch(6900/7879) done. Loss: 0.0232  lr:0.000100
[ Thu Jul  4 05:59:42 2024 ] 
Training: Epoch [80/120], Step [6999], Loss: 0.3427222967147827, Training Accuracy: 97.3607142857143
[ Thu Jul  4 05:59:42 2024 ] 	Batch(7000/7879) done. Loss: 0.1483  lr:0.000100
[ Thu Jul  4 06:00:00 2024 ] 	Batch(7100/7879) done. Loss: 0.0030  lr:0.000100
[ Thu Jul  4 06:00:18 2024 ] 	Batch(7200/7879) done. Loss: 0.0119  lr:0.000100
[ Thu Jul  4 06:00:36 2024 ] 	Batch(7300/7879) done. Loss: 0.0408  lr:0.000100
[ Thu Jul  4 06:00:54 2024 ] 	Batch(7400/7879) done. Loss: 0.0273  lr:0.000100
[ Thu Jul  4 06:01:11 2024 ] 
Training: Epoch [80/120], Step [7499], Loss: 0.015266348607838154, Training Accuracy: 97.35333333333334
[ Thu Jul  4 06:01:12 2024 ] 	Batch(7500/7879) done. Loss: 0.0071  lr:0.000100
[ Thu Jul  4 06:01:30 2024 ] 	Batch(7600/7879) done. Loss: 0.0194  lr:0.000100
[ Thu Jul  4 06:01:47 2024 ] 	Batch(7700/7879) done. Loss: 0.4071  lr:0.000100
[ Thu Jul  4 06:02:05 2024 ] 	Batch(7800/7879) done. Loss: 0.0047  lr:0.000100
[ Thu Jul  4 06:02:19 2024 ] 	Mean training loss: 0.1021.
[ Thu Jul  4 06:02:19 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 06:02:20 2024 ] Training epoch: 82
[ Thu Jul  4 06:02:20 2024 ] 	Batch(0/7879) done. Loss: 0.6192  lr:0.000100
[ Thu Jul  4 06:02:38 2024 ] 	Batch(100/7879) done. Loss: 0.0409  lr:0.000100
[ Thu Jul  4 06:02:56 2024 ] 	Batch(200/7879) done. Loss: 0.2418  lr:0.000100
[ Thu Jul  4 06:03:14 2024 ] 	Batch(300/7879) done. Loss: 0.2547  lr:0.000100
[ Thu Jul  4 06:03:32 2024 ] 	Batch(400/7879) done. Loss: 0.2845  lr:0.000100
[ Thu Jul  4 06:03:50 2024 ] 
Training: Epoch [81/120], Step [499], Loss: 0.05495903268456459, Training Accuracy: 97.55
[ Thu Jul  4 06:03:50 2024 ] 	Batch(500/7879) done. Loss: 0.0160  lr:0.000100
[ Thu Jul  4 06:04:08 2024 ] 	Batch(600/7879) done. Loss: 0.0058  lr:0.000100
[ Thu Jul  4 06:04:26 2024 ] 	Batch(700/7879) done. Loss: 0.0290  lr:0.000100
[ Thu Jul  4 06:04:44 2024 ] 	Batch(800/7879) done. Loss: 0.1332  lr:0.000100
[ Thu Jul  4 06:05:02 2024 ] 	Batch(900/7879) done. Loss: 0.0946  lr:0.000100
[ Thu Jul  4 06:05:20 2024 ] 
Training: Epoch [81/120], Step [999], Loss: 0.22747336328029633, Training Accuracy: 97.7125
[ Thu Jul  4 06:05:20 2024 ] 	Batch(1000/7879) done. Loss: 0.0611  lr:0.000100
[ Thu Jul  4 06:05:38 2024 ] 	Batch(1100/7879) done. Loss: 0.2653  lr:0.000100
[ Thu Jul  4 06:05:56 2024 ] 	Batch(1200/7879) done. Loss: 0.0569  lr:0.000100
[ Thu Jul  4 06:06:14 2024 ] 	Batch(1300/7879) done. Loss: 0.0327  lr:0.000100
[ Thu Jul  4 06:06:32 2024 ] 	Batch(1400/7879) done. Loss: 0.1703  lr:0.000100
[ Thu Jul  4 06:06:49 2024 ] 
Training: Epoch [81/120], Step [1499], Loss: 0.02216021530330181, Training Accuracy: 97.6
[ Thu Jul  4 06:06:50 2024 ] 	Batch(1500/7879) done. Loss: 0.0328  lr:0.000100
[ Thu Jul  4 06:07:08 2024 ] 	Batch(1600/7879) done. Loss: 0.0781  lr:0.000100
[ Thu Jul  4 06:07:25 2024 ] 	Batch(1700/7879) done. Loss: 0.0714  lr:0.000100
[ Thu Jul  4 06:07:43 2024 ] 	Batch(1800/7879) done. Loss: 0.0517  lr:0.000100
[ Thu Jul  4 06:08:01 2024 ] 	Batch(1900/7879) done. Loss: 0.0181  lr:0.000100
[ Thu Jul  4 06:08:19 2024 ] 
Training: Epoch [81/120], Step [1999], Loss: 0.13918715715408325, Training Accuracy: 97.6125
[ Thu Jul  4 06:08:19 2024 ] 	Batch(2000/7879) done. Loss: 0.1051  lr:0.000100
[ Thu Jul  4 06:08:37 2024 ] 	Batch(2100/7879) done. Loss: 0.0221  lr:0.000100
[ Thu Jul  4 06:08:55 2024 ] 	Batch(2200/7879) done. Loss: 0.0345  lr:0.000100
[ Thu Jul  4 06:09:13 2024 ] 	Batch(2300/7879) done. Loss: 0.0657  lr:0.000100
[ Thu Jul  4 06:09:31 2024 ] 	Batch(2400/7879) done. Loss: 0.0684  lr:0.000100
[ Thu Jul  4 06:09:48 2024 ] 
Training: Epoch [81/120], Step [2499], Loss: 0.15662986040115356, Training Accuracy: 97.605
[ Thu Jul  4 06:09:49 2024 ] 	Batch(2500/7879) done. Loss: 0.0988  lr:0.000100
[ Thu Jul  4 06:10:06 2024 ] 	Batch(2600/7879) done. Loss: 0.0835  lr:0.000100
[ Thu Jul  4 06:10:24 2024 ] 	Batch(2700/7879) done. Loss: 0.6592  lr:0.000100
[ Thu Jul  4 06:10:42 2024 ] 	Batch(2800/7879) done. Loss: 0.1304  lr:0.000100
[ Thu Jul  4 06:11:01 2024 ] 	Batch(2900/7879) done. Loss: 0.0557  lr:0.000100
[ Thu Jul  4 06:11:19 2024 ] 
Training: Epoch [81/120], Step [2999], Loss: 0.014439321123063564, Training Accuracy: 97.45
[ Thu Jul  4 06:11:19 2024 ] 	Batch(3000/7879) done. Loss: 0.0677  lr:0.000100
[ Thu Jul  4 06:11:38 2024 ] 	Batch(3100/7879) done. Loss: 0.0287  lr:0.000100
[ Thu Jul  4 06:11:56 2024 ] 	Batch(3200/7879) done. Loss: 0.1420  lr:0.000100
[ Thu Jul  4 06:12:15 2024 ] 	Batch(3300/7879) done. Loss: 0.1820  lr:0.000100
[ Thu Jul  4 06:12:32 2024 ] 	Batch(3400/7879) done. Loss: 0.0305  lr:0.000100
[ Thu Jul  4 06:12:50 2024 ] 
Training: Epoch [81/120], Step [3499], Loss: 0.1800290197134018, Training Accuracy: 97.44285714285714
[ Thu Jul  4 06:12:50 2024 ] 	Batch(3500/7879) done. Loss: 0.1448  lr:0.000100
[ Thu Jul  4 06:13:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0260  lr:0.000100
[ Thu Jul  4 06:13:26 2024 ] 	Batch(3700/7879) done. Loss: 0.1589  lr:0.000100
[ Thu Jul  4 06:13:44 2024 ] 	Batch(3800/7879) done. Loss: 0.1338  lr:0.000100
[ Thu Jul  4 06:14:02 2024 ] 	Batch(3900/7879) done. Loss: 0.0533  lr:0.000100
[ Thu Jul  4 06:14:20 2024 ] 
Training: Epoch [81/120], Step [3999], Loss: 0.07131199538707733, Training Accuracy: 97.44375
[ Thu Jul  4 06:14:20 2024 ] 	Batch(4000/7879) done. Loss: 0.0788  lr:0.000100
[ Thu Jul  4 06:14:38 2024 ] 	Batch(4100/7879) done. Loss: 0.0022  lr:0.000100
[ Thu Jul  4 06:14:56 2024 ] 	Batch(4200/7879) done. Loss: 0.0525  lr:0.000100
[ Thu Jul  4 06:15:13 2024 ] 	Batch(4300/7879) done. Loss: 0.4087  lr:0.000100
[ Thu Jul  4 06:15:31 2024 ] 	Batch(4400/7879) done. Loss: 0.0085  lr:0.000100
[ Thu Jul  4 06:15:49 2024 ] 
Training: Epoch [81/120], Step [4499], Loss: 0.011968649923801422, Training Accuracy: 97.47777777777777
[ Thu Jul  4 06:15:49 2024 ] 	Batch(4500/7879) done. Loss: 0.4002  lr:0.000100
[ Thu Jul  4 06:16:07 2024 ] 	Batch(4600/7879) done. Loss: 0.0258  lr:0.000100
[ Thu Jul  4 06:16:25 2024 ] 	Batch(4700/7879) done. Loss: 0.0595  lr:0.000100
[ Thu Jul  4 06:16:43 2024 ] 	Batch(4800/7879) done. Loss: 0.0292  lr:0.000100
[ Thu Jul  4 06:17:01 2024 ] 	Batch(4900/7879) done. Loss: 0.2720  lr:0.000100
[ Thu Jul  4 06:17:19 2024 ] 
Training: Epoch [81/120], Step [4999], Loss: 0.16516084969043732, Training Accuracy: 97.5
[ Thu Jul  4 06:17:19 2024 ] 	Batch(5000/7879) done. Loss: 0.2877  lr:0.000100
[ Thu Jul  4 06:17:37 2024 ] 	Batch(5100/7879) done. Loss: 0.0142  lr:0.000100
[ Thu Jul  4 06:17:55 2024 ] 	Batch(5200/7879) done. Loss: 0.0805  lr:0.000100
[ Thu Jul  4 06:18:13 2024 ] 	Batch(5300/7879) done. Loss: 0.4519  lr:0.000100
[ Thu Jul  4 06:18:30 2024 ] 	Batch(5400/7879) done. Loss: 0.0808  lr:0.000100
[ Thu Jul  4 06:18:48 2024 ] 
Training: Epoch [81/120], Step [5499], Loss: 0.0063463966362178326, Training Accuracy: 97.48409090909091
[ Thu Jul  4 06:18:48 2024 ] 	Batch(5500/7879) done. Loss: 0.3820  lr:0.000100
[ Thu Jul  4 06:19:06 2024 ] 	Batch(5600/7879) done. Loss: 0.0510  lr:0.000100
[ Thu Jul  4 06:19:25 2024 ] 	Batch(5700/7879) done. Loss: 0.0539  lr:0.000100
[ Thu Jul  4 06:19:43 2024 ] 	Batch(5800/7879) done. Loss: 0.2269  lr:0.000100
[ Thu Jul  4 06:20:02 2024 ] 	Batch(5900/7879) done. Loss: 0.0135  lr:0.000100
[ Thu Jul  4 06:20:20 2024 ] 
Training: Epoch [81/120], Step [5999], Loss: 0.786266028881073, Training Accuracy: 97.48541666666667
[ Thu Jul  4 06:20:21 2024 ] 	Batch(6000/7879) done. Loss: 0.2605  lr:0.000100
[ Thu Jul  4 06:20:39 2024 ] 	Batch(6100/7879) done. Loss: 0.0394  lr:0.000100
[ Thu Jul  4 06:20:58 2024 ] 	Batch(6200/7879) done. Loss: 0.0413  lr:0.000100
[ Thu Jul  4 06:21:16 2024 ] 	Batch(6300/7879) done. Loss: 0.0324  lr:0.000100
[ Thu Jul  4 06:21:35 2024 ] 	Batch(6400/7879) done. Loss: 0.0509  lr:0.000100
[ Thu Jul  4 06:21:53 2024 ] 
Training: Epoch [81/120], Step [6499], Loss: 0.20451845228672028, Training Accuracy: 97.46538461538462
[ Thu Jul  4 06:21:53 2024 ] 	Batch(6500/7879) done. Loss: 0.0932  lr:0.000100
[ Thu Jul  4 06:22:11 2024 ] 	Batch(6600/7879) done. Loss: 0.0121  lr:0.000100
[ Thu Jul  4 06:22:29 2024 ] 	Batch(6700/7879) done. Loss: 0.0258  lr:0.000100
[ Thu Jul  4 06:22:47 2024 ] 	Batch(6800/7879) done. Loss: 0.3425  lr:0.000100
[ Thu Jul  4 06:23:05 2024 ] 	Batch(6900/7879) done. Loss: 0.1152  lr:0.000100
[ Thu Jul  4 06:23:24 2024 ] 
Training: Epoch [81/120], Step [6999], Loss: 0.017794450744986534, Training Accuracy: 97.44642857142857
[ Thu Jul  4 06:23:24 2024 ] 	Batch(7000/7879) done. Loss: 0.2106  lr:0.000100
[ Thu Jul  4 06:23:42 2024 ] 	Batch(7100/7879) done. Loss: 0.0100  lr:0.000100
[ Thu Jul  4 06:24:01 2024 ] 	Batch(7200/7879) done. Loss: 0.0046  lr:0.000100
[ Thu Jul  4 06:24:19 2024 ] 	Batch(7300/7879) done. Loss: 0.0231  lr:0.000100
[ Thu Jul  4 06:24:37 2024 ] 	Batch(7400/7879) done. Loss: 0.0105  lr:0.000100
[ Thu Jul  4 06:24:55 2024 ] 
Training: Epoch [81/120], Step [7499], Loss: 0.09837079048156738, Training Accuracy: 97.44666666666667
[ Thu Jul  4 06:24:55 2024 ] 	Batch(7500/7879) done. Loss: 0.0180  lr:0.000100
[ Thu Jul  4 06:25:13 2024 ] 	Batch(7600/7879) done. Loss: 0.0209  lr:0.000100
[ Thu Jul  4 06:25:31 2024 ] 	Batch(7700/7879) done. Loss: 0.0527  lr:0.000100
[ Thu Jul  4 06:25:49 2024 ] 	Batch(7800/7879) done. Loss: 0.0479  lr:0.000100
[ Thu Jul  4 06:26:03 2024 ] 	Mean training loss: 0.1002.
[ Thu Jul  4 06:26:03 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 06:26:03 2024 ] Training epoch: 83
[ Thu Jul  4 06:26:03 2024 ] 	Batch(0/7879) done. Loss: 0.0909  lr:0.000100
[ Thu Jul  4 06:26:21 2024 ] 	Batch(100/7879) done. Loss: 0.0591  lr:0.000100
[ Thu Jul  4 06:26:39 2024 ] 	Batch(200/7879) done. Loss: 0.0504  lr:0.000100
[ Thu Jul  4 06:26:57 2024 ] 	Batch(300/7879) done. Loss: 0.0728  lr:0.000100
[ Thu Jul  4 06:27:15 2024 ] 	Batch(400/7879) done. Loss: 0.0347  lr:0.000100
[ Thu Jul  4 06:27:33 2024 ] 
Training: Epoch [82/120], Step [499], Loss: 0.3458479642868042, Training Accuracy: 97.5
[ Thu Jul  4 06:27:33 2024 ] 	Batch(500/7879) done. Loss: 0.0518  lr:0.000100
[ Thu Jul  4 06:27:51 2024 ] 	Batch(600/7879) done. Loss: 0.0073  lr:0.000100
[ Thu Jul  4 06:28:10 2024 ] 	Batch(700/7879) done. Loss: 0.2813  lr:0.000100
[ Thu Jul  4 06:28:29 2024 ] 	Batch(800/7879) done. Loss: 0.0874  lr:0.000100
[ Thu Jul  4 06:28:46 2024 ] 	Batch(900/7879) done. Loss: 0.0122  lr:0.000100
[ Thu Jul  4 06:29:04 2024 ] 
Training: Epoch [82/120], Step [999], Loss: 0.21499210596084595, Training Accuracy: 97.3875
[ Thu Jul  4 06:29:04 2024 ] 	Batch(1000/7879) done. Loss: 0.0295  lr:0.000100
[ Thu Jul  4 06:29:22 2024 ] 	Batch(1100/7879) done. Loss: 0.0240  lr:0.000100
[ Thu Jul  4 06:29:40 2024 ] 	Batch(1200/7879) done. Loss: 0.0170  lr:0.000100
[ Thu Jul  4 06:29:58 2024 ] 	Batch(1300/7879) done. Loss: 0.0425  lr:0.000100
[ Thu Jul  4 06:30:16 2024 ] 	Batch(1400/7879) done. Loss: 0.0117  lr:0.000100
[ Thu Jul  4 06:30:34 2024 ] 
Training: Epoch [82/120], Step [1499], Loss: 0.008134094998240471, Training Accuracy: 97.38333333333333
[ Thu Jul  4 06:30:34 2024 ] 	Batch(1500/7879) done. Loss: 0.2731  lr:0.000100
[ Thu Jul  4 06:30:52 2024 ] 	Batch(1600/7879) done. Loss: 0.2394  lr:0.000100
[ Thu Jul  4 06:31:10 2024 ] 	Batch(1700/7879) done. Loss: 0.1411  lr:0.000100
[ Thu Jul  4 06:31:27 2024 ] 	Batch(1800/7879) done. Loss: 0.1356  lr:0.000100
[ Thu Jul  4 06:31:45 2024 ] 	Batch(1900/7879) done. Loss: 0.0260  lr:0.000100
[ Thu Jul  4 06:32:03 2024 ] 
Training: Epoch [82/120], Step [1999], Loss: 0.031636644154787064, Training Accuracy: 97.45
[ Thu Jul  4 06:32:03 2024 ] 	Batch(2000/7879) done. Loss: 0.0495  lr:0.000100
[ Thu Jul  4 06:32:21 2024 ] 	Batch(2100/7879) done. Loss: 0.0563  lr:0.000100
[ Thu Jul  4 06:32:39 2024 ] 	Batch(2200/7879) done. Loss: 0.1298  lr:0.000100
[ Thu Jul  4 06:32:57 2024 ] 	Batch(2300/7879) done. Loss: 0.0084  lr:0.000100
[ Thu Jul  4 06:33:15 2024 ] 	Batch(2400/7879) done. Loss: 0.2787  lr:0.000100
[ Thu Jul  4 06:33:33 2024 ] 
Training: Epoch [82/120], Step [2499], Loss: 0.08885815739631653, Training Accuracy: 97.425
[ Thu Jul  4 06:33:33 2024 ] 	Batch(2500/7879) done. Loss: 0.0429  lr:0.000100
[ Thu Jul  4 06:33:51 2024 ] 	Batch(2600/7879) done. Loss: 0.0230  lr:0.000100
[ Thu Jul  4 06:34:08 2024 ] 	Batch(2700/7879) done. Loss: 0.3223  lr:0.000100
[ Thu Jul  4 06:34:27 2024 ] 	Batch(2800/7879) done. Loss: 0.0557  lr:0.000100
[ Thu Jul  4 06:34:45 2024 ] 	Batch(2900/7879) done. Loss: 0.2783  lr:0.000100
[ Thu Jul  4 06:35:03 2024 ] 
Training: Epoch [82/120], Step [2999], Loss: 0.1358473151922226, Training Accuracy: 97.39583333333334
[ Thu Jul  4 06:35:04 2024 ] 	Batch(3000/7879) done. Loss: 0.0037  lr:0.000100
[ Thu Jul  4 06:35:22 2024 ] 	Batch(3100/7879) done. Loss: 0.0138  lr:0.000100
[ Thu Jul  4 06:35:41 2024 ] 	Batch(3200/7879) done. Loss: 0.0247  lr:0.000100
[ Thu Jul  4 06:35:59 2024 ] 	Batch(3300/7879) done. Loss: 0.0110  lr:0.000100
[ Thu Jul  4 06:36:18 2024 ] 	Batch(3400/7879) done. Loss: 0.0118  lr:0.000100
[ Thu Jul  4 06:36:36 2024 ] 
Training: Epoch [82/120], Step [3499], Loss: 0.2973400056362152, Training Accuracy: 97.34642857142856
[ Thu Jul  4 06:36:36 2024 ] 	Batch(3500/7879) done. Loss: 0.0084  lr:0.000100
[ Thu Jul  4 06:36:55 2024 ] 	Batch(3600/7879) done. Loss: 0.0077  lr:0.000100
[ Thu Jul  4 06:37:14 2024 ] 	Batch(3700/7879) done. Loss: 0.0245  lr:0.000100
[ Thu Jul  4 06:37:32 2024 ] 	Batch(3800/7879) done. Loss: 0.0160  lr:0.000100
[ Thu Jul  4 06:37:51 2024 ] 	Batch(3900/7879) done. Loss: 0.0117  lr:0.000100
[ Thu Jul  4 06:38:09 2024 ] 
Training: Epoch [82/120], Step [3999], Loss: 0.0730929970741272, Training Accuracy: 97.32499999999999
[ Thu Jul  4 06:38:09 2024 ] 	Batch(4000/7879) done. Loss: 0.0493  lr:0.000100
[ Thu Jul  4 06:38:27 2024 ] 	Batch(4100/7879) done. Loss: 0.0208  lr:0.000100
[ Thu Jul  4 06:38:45 2024 ] 	Batch(4200/7879) done. Loss: 0.0011  lr:0.000100
[ Thu Jul  4 06:39:03 2024 ] 	Batch(4300/7879) done. Loss: 0.2313  lr:0.000100
[ Thu Jul  4 06:39:21 2024 ] 	Batch(4400/7879) done. Loss: 0.6325  lr:0.000100
[ Thu Jul  4 06:39:39 2024 ] 
Training: Epoch [82/120], Step [4499], Loss: 0.031081410124897957, Training Accuracy: 97.34444444444445
[ Thu Jul  4 06:39:39 2024 ] 	Batch(4500/7879) done. Loss: 0.0478  lr:0.000100
[ Thu Jul  4 06:39:57 2024 ] 	Batch(4600/7879) done. Loss: 0.1798  lr:0.000100
[ Thu Jul  4 06:40:15 2024 ] 	Batch(4700/7879) done. Loss: 0.0906  lr:0.000100
[ Thu Jul  4 06:40:33 2024 ] 	Batch(4800/7879) done. Loss: 0.2430  lr:0.000100
[ Thu Jul  4 06:40:50 2024 ] 	Batch(4900/7879) done. Loss: 0.0153  lr:0.000100
[ Thu Jul  4 06:41:08 2024 ] 
Training: Epoch [82/120], Step [4999], Loss: 0.06714362651109695, Training Accuracy: 97.36749999999999
[ Thu Jul  4 06:41:08 2024 ] 	Batch(5000/7879) done. Loss: 0.0152  lr:0.000100
[ Thu Jul  4 06:41:26 2024 ] 	Batch(5100/7879) done. Loss: 0.0306  lr:0.000100
[ Thu Jul  4 06:41:44 2024 ] 	Batch(5200/7879) done. Loss: 0.0858  lr:0.000100
[ Thu Jul  4 06:42:03 2024 ] 	Batch(5300/7879) done. Loss: 0.0185  lr:0.000100
[ Thu Jul  4 06:42:22 2024 ] 	Batch(5400/7879) done. Loss: 0.1696  lr:0.000100
[ Thu Jul  4 06:42:41 2024 ] 
Training: Epoch [82/120], Step [5499], Loss: 0.024555083364248276, Training Accuracy: 97.39999999999999
[ Thu Jul  4 06:42:41 2024 ] 	Batch(5500/7879) done. Loss: 0.0286  lr:0.000100
[ Thu Jul  4 06:43:00 2024 ] 	Batch(5600/7879) done. Loss: 0.0215  lr:0.000100
[ Thu Jul  4 06:43:19 2024 ] 	Batch(5700/7879) done. Loss: 0.1522  lr:0.000100
[ Thu Jul  4 06:43:37 2024 ] 	Batch(5800/7879) done. Loss: 0.1508  lr:0.000100
[ Thu Jul  4 06:43:56 2024 ] 	Batch(5900/7879) done. Loss: 0.2199  lr:0.000100
[ Thu Jul  4 06:44:13 2024 ] 
Training: Epoch [82/120], Step [5999], Loss: 0.002695442410185933, Training Accuracy: 97.40625
[ Thu Jul  4 06:44:14 2024 ] 	Batch(6000/7879) done. Loss: 0.0458  lr:0.000100
[ Thu Jul  4 06:44:32 2024 ] 	Batch(6100/7879) done. Loss: 0.5996  lr:0.000100
[ Thu Jul  4 06:44:50 2024 ] 	Batch(6200/7879) done. Loss: 0.1615  lr:0.000100
[ Thu Jul  4 06:45:07 2024 ] 	Batch(6300/7879) done. Loss: 0.1282  lr:0.000100
[ Thu Jul  4 06:45:25 2024 ] 	Batch(6400/7879) done. Loss: 0.0347  lr:0.000100
[ Thu Jul  4 06:45:43 2024 ] 
Training: Epoch [82/120], Step [6499], Loss: 0.09930519014596939, Training Accuracy: 97.41923076923077
[ Thu Jul  4 06:45:43 2024 ] 	Batch(6500/7879) done. Loss: 0.0787  lr:0.000100
[ Thu Jul  4 06:46:01 2024 ] 	Batch(6600/7879) done. Loss: 0.0938  lr:0.000100
[ Thu Jul  4 06:46:19 2024 ] 	Batch(6700/7879) done. Loss: 0.0814  lr:0.000100
[ Thu Jul  4 06:46:37 2024 ] 	Batch(6800/7879) done. Loss: 0.0107  lr:0.000100
[ Thu Jul  4 06:46:55 2024 ] 	Batch(6900/7879) done. Loss: 0.1188  lr:0.000100
[ Thu Jul  4 06:47:13 2024 ] 
Training: Epoch [82/120], Step [6999], Loss: 0.03671812638640404, Training Accuracy: 97.41785714285714
[ Thu Jul  4 06:47:13 2024 ] 	Batch(7000/7879) done. Loss: 0.0488  lr:0.000100
[ Thu Jul  4 06:47:31 2024 ] 	Batch(7100/7879) done. Loss: 0.0087  lr:0.000100
[ Thu Jul  4 06:47:49 2024 ] 	Batch(7200/7879) done. Loss: 0.0415  lr:0.000100
[ Thu Jul  4 06:48:07 2024 ] 	Batch(7300/7879) done. Loss: 0.1343  lr:0.000100
[ Thu Jul  4 06:48:24 2024 ] 	Batch(7400/7879) done. Loss: 0.2689  lr:0.000100
[ Thu Jul  4 06:48:42 2024 ] 
Training: Epoch [82/120], Step [7499], Loss: 0.0062159886583685875, Training Accuracy: 97.42833333333333
[ Thu Jul  4 06:48:42 2024 ] 	Batch(7500/7879) done. Loss: 0.1063  lr:0.000100
[ Thu Jul  4 06:49:00 2024 ] 	Batch(7600/7879) done. Loss: 0.3069  lr:0.000100
[ Thu Jul  4 06:49:18 2024 ] 	Batch(7700/7879) done. Loss: 0.0044  lr:0.000100
[ Thu Jul  4 06:49:36 2024 ] 	Batch(7800/7879) done. Loss: 0.0051  lr:0.000100
[ Thu Jul  4 06:49:50 2024 ] 	Mean training loss: 0.1005.
[ Thu Jul  4 06:49:50 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 06:49:50 2024 ] Training epoch: 84
[ Thu Jul  4 06:49:51 2024 ] 	Batch(0/7879) done. Loss: 0.2876  lr:0.000100
[ Thu Jul  4 06:50:09 2024 ] 	Batch(100/7879) done. Loss: 0.0010  lr:0.000100
[ Thu Jul  4 06:50:27 2024 ] 	Batch(200/7879) done. Loss: 0.0690  lr:0.000100
[ Thu Jul  4 06:50:45 2024 ] 	Batch(300/7879) done. Loss: 0.0358  lr:0.000100
[ Thu Jul  4 06:51:03 2024 ] 	Batch(400/7879) done. Loss: 0.3396  lr:0.000100
[ Thu Jul  4 06:51:20 2024 ] 
Training: Epoch [83/120], Step [499], Loss: 0.014721274375915527, Training Accuracy: 97.625
[ Thu Jul  4 06:51:20 2024 ] 	Batch(500/7879) done. Loss: 0.0862  lr:0.000100
[ Thu Jul  4 06:51:38 2024 ] 	Batch(600/7879) done. Loss: 0.1078  lr:0.000100
[ Thu Jul  4 06:51:56 2024 ] 	Batch(700/7879) done. Loss: 0.1145  lr:0.000100
[ Thu Jul  4 06:52:14 2024 ] 	Batch(800/7879) done. Loss: 0.0279  lr:0.000100
[ Thu Jul  4 06:52:32 2024 ] 	Batch(900/7879) done. Loss: 0.5785  lr:0.000100
[ Thu Jul  4 06:52:50 2024 ] 
Training: Epoch [83/120], Step [999], Loss: 0.10098686814308167, Training Accuracy: 97.45
[ Thu Jul  4 06:52:50 2024 ] 	Batch(1000/7879) done. Loss: 0.0093  lr:0.000100
[ Thu Jul  4 06:53:08 2024 ] 	Batch(1100/7879) done. Loss: 0.0726  lr:0.000100
[ Thu Jul  4 06:53:26 2024 ] 	Batch(1200/7879) done. Loss: 0.0870  lr:0.000100
[ Thu Jul  4 06:53:44 2024 ] 	Batch(1300/7879) done. Loss: 0.0725  lr:0.000100
[ Thu Jul  4 06:54:02 2024 ] 	Batch(1400/7879) done. Loss: 0.0236  lr:0.000100
[ Thu Jul  4 06:54:20 2024 ] 
Training: Epoch [83/120], Step [1499], Loss: 0.003970960155129433, Training Accuracy: 97.56666666666666
[ Thu Jul  4 06:54:20 2024 ] 	Batch(1500/7879) done. Loss: 0.0205  lr:0.000100
[ Thu Jul  4 06:54:38 2024 ] 	Batch(1600/7879) done. Loss: 0.0028  lr:0.000100
[ Thu Jul  4 06:54:56 2024 ] 	Batch(1700/7879) done. Loss: 0.3464  lr:0.000100
[ Thu Jul  4 06:55:14 2024 ] 	Batch(1800/7879) done. Loss: 0.1547  lr:0.000100
[ Thu Jul  4 06:55:31 2024 ] 	Batch(1900/7879) done. Loss: 0.0879  lr:0.000100
[ Thu Jul  4 06:55:49 2024 ] 
Training: Epoch [83/120], Step [1999], Loss: 0.16966356337070465, Training Accuracy: 97.51875
[ Thu Jul  4 06:55:49 2024 ] 	Batch(2000/7879) done. Loss: 0.1395  lr:0.000100
[ Thu Jul  4 06:56:08 2024 ] 	Batch(2100/7879) done. Loss: 0.0123  lr:0.000100
[ Thu Jul  4 06:56:26 2024 ] 	Batch(2200/7879) done. Loss: 0.0442  lr:0.000100
[ Thu Jul  4 06:56:44 2024 ] 	Batch(2300/7879) done. Loss: 0.0475  lr:0.000100
[ Thu Jul  4 06:57:02 2024 ] 	Batch(2400/7879) done. Loss: 0.0213  lr:0.000100
[ Thu Jul  4 06:57:20 2024 ] 
Training: Epoch [83/120], Step [2499], Loss: 0.06476610153913498, Training Accuracy: 97.56
[ Thu Jul  4 06:57:21 2024 ] 	Batch(2500/7879) done. Loss: 0.2863  lr:0.000100
[ Thu Jul  4 06:57:39 2024 ] 	Batch(2600/7879) done. Loss: 0.0276  lr:0.000100
[ Thu Jul  4 06:57:58 2024 ] 	Batch(2700/7879) done. Loss: 0.0810  lr:0.000100
[ Thu Jul  4 06:58:16 2024 ] 	Batch(2800/7879) done. Loss: 0.0116  lr:0.000100
[ Thu Jul  4 06:58:34 2024 ] 	Batch(2900/7879) done. Loss: 0.0519  lr:0.000100
[ Thu Jul  4 06:58:52 2024 ] 
Training: Epoch [83/120], Step [2999], Loss: 0.08333548903465271, Training Accuracy: 97.5375
[ Thu Jul  4 06:58:52 2024 ] 	Batch(3000/7879) done. Loss: 0.1021  lr:0.000100
[ Thu Jul  4 06:59:10 2024 ] 	Batch(3100/7879) done. Loss: 0.0216  lr:0.000100
[ Thu Jul  4 06:59:28 2024 ] 	Batch(3200/7879) done. Loss: 0.3615  lr:0.000100
[ Thu Jul  4 06:59:46 2024 ] 	Batch(3300/7879) done. Loss: 0.2978  lr:0.000100
[ Thu Jul  4 07:00:04 2024 ] 	Batch(3400/7879) done. Loss: 0.0400  lr:0.000100
[ Thu Jul  4 07:00:22 2024 ] 
Training: Epoch [83/120], Step [3499], Loss: 0.11639869958162308, Training Accuracy: 97.50357142857143
[ Thu Jul  4 07:00:22 2024 ] 	Batch(3500/7879) done. Loss: 0.2664  lr:0.000100
[ Thu Jul  4 07:00:40 2024 ] 	Batch(3600/7879) done. Loss: 0.0260  lr:0.000100
[ Thu Jul  4 07:00:58 2024 ] 	Batch(3700/7879) done. Loss: 0.2549  lr:0.000100
[ Thu Jul  4 07:01:17 2024 ] 	Batch(3800/7879) done. Loss: 0.0147  lr:0.000100
[ Thu Jul  4 07:01:35 2024 ] 	Batch(3900/7879) done. Loss: 0.0367  lr:0.000100
[ Thu Jul  4 07:01:54 2024 ] 
Training: Epoch [83/120], Step [3999], Loss: 0.03575268015265465, Training Accuracy: 97.484375
[ Thu Jul  4 07:01:54 2024 ] 	Batch(4000/7879) done. Loss: 0.1443  lr:0.000100
[ Thu Jul  4 07:02:13 2024 ] 	Batch(4100/7879) done. Loss: 0.4986  lr:0.000100
[ Thu Jul  4 07:02:31 2024 ] 	Batch(4200/7879) done. Loss: 0.0373  lr:0.000100
[ Thu Jul  4 07:02:49 2024 ] 	Batch(4300/7879) done. Loss: 0.1256  lr:0.000100
[ Thu Jul  4 07:03:08 2024 ] 	Batch(4400/7879) done. Loss: 0.0509  lr:0.000100
[ Thu Jul  4 07:03:26 2024 ] 
Training: Epoch [83/120], Step [4499], Loss: 0.07191222906112671, Training Accuracy: 97.52499999999999
[ Thu Jul  4 07:03:26 2024 ] 	Batch(4500/7879) done. Loss: 0.1216  lr:0.000100
[ Thu Jul  4 07:03:44 2024 ] 	Batch(4600/7879) done. Loss: 0.0357  lr:0.000100
[ Thu Jul  4 07:04:02 2024 ] 	Batch(4700/7879) done. Loss: 0.0061  lr:0.000100
[ Thu Jul  4 07:04:20 2024 ] 	Batch(4800/7879) done. Loss: 0.0191  lr:0.000100
[ Thu Jul  4 07:04:38 2024 ] 	Batch(4900/7879) done. Loss: 0.0820  lr:0.000100
[ Thu Jul  4 07:04:55 2024 ] 
Training: Epoch [83/120], Step [4999], Loss: 0.2568950653076172, Training Accuracy: 97.52499999999999
[ Thu Jul  4 07:04:55 2024 ] 	Batch(5000/7879) done. Loss: 0.1909  lr:0.000100
[ Thu Jul  4 07:05:13 2024 ] 	Batch(5100/7879) done. Loss: 0.0367  lr:0.000100
[ Thu Jul  4 07:05:31 2024 ] 	Batch(5200/7879) done. Loss: 0.2546  lr:0.000100
[ Thu Jul  4 07:05:49 2024 ] 	Batch(5300/7879) done. Loss: 0.0018  lr:0.000100
[ Thu Jul  4 07:06:07 2024 ] 	Batch(5400/7879) done. Loss: 0.0167  lr:0.000100
[ Thu Jul  4 07:06:25 2024 ] 
Training: Epoch [83/120], Step [5499], Loss: 0.005224025808274746, Training Accuracy: 97.50681818181818
[ Thu Jul  4 07:06:25 2024 ] 	Batch(5500/7879) done. Loss: 0.0560  lr:0.000100
[ Thu Jul  4 07:06:44 2024 ] 	Batch(5600/7879) done. Loss: 0.0055  lr:0.000100
[ Thu Jul  4 07:07:02 2024 ] 	Batch(5700/7879) done. Loss: 0.0257  lr:0.000100
[ Thu Jul  4 07:07:21 2024 ] 	Batch(5800/7879) done. Loss: 0.0029  lr:0.000100
[ Thu Jul  4 07:07:39 2024 ] 	Batch(5900/7879) done. Loss: 0.0106  lr:0.000100
[ Thu Jul  4 07:07:58 2024 ] 
Training: Epoch [83/120], Step [5999], Loss: 0.04513153061270714, Training Accuracy: 97.5125
[ Thu Jul  4 07:07:58 2024 ] 	Batch(6000/7879) done. Loss: 0.0687  lr:0.000100
[ Thu Jul  4 07:08:16 2024 ] 	Batch(6100/7879) done. Loss: 0.4233  lr:0.000100
[ Thu Jul  4 07:08:34 2024 ] 	Batch(6200/7879) done. Loss: 0.1141  lr:0.000100
[ Thu Jul  4 07:08:52 2024 ] 	Batch(6300/7879) done. Loss: 0.2270  lr:0.000100
[ Thu Jul  4 07:09:10 2024 ] 	Batch(6400/7879) done. Loss: 0.0963  lr:0.000100
[ Thu Jul  4 07:09:28 2024 ] 
Training: Epoch [83/120], Step [6499], Loss: 0.0070973956026136875, Training Accuracy: 97.46730769230768
[ Thu Jul  4 07:09:28 2024 ] 	Batch(6500/7879) done. Loss: 0.0156  lr:0.000100
[ Thu Jul  4 07:09:46 2024 ] 	Batch(6600/7879) done. Loss: 0.3579  lr:0.000100
[ Thu Jul  4 07:10:03 2024 ] 	Batch(6700/7879) done. Loss: 0.1535  lr:0.000100
[ Thu Jul  4 07:10:21 2024 ] 	Batch(6800/7879) done. Loss: 0.0096  lr:0.000100
[ Thu Jul  4 07:10:39 2024 ] 	Batch(6900/7879) done. Loss: 0.0084  lr:0.000100
[ Thu Jul  4 07:10:57 2024 ] 
Training: Epoch [83/120], Step [6999], Loss: 0.14738383889198303, Training Accuracy: 97.48750000000001
[ Thu Jul  4 07:10:57 2024 ] 	Batch(7000/7879) done. Loss: 0.1111  lr:0.000100
[ Thu Jul  4 07:11:15 2024 ] 	Batch(7100/7879) done. Loss: 0.0800  lr:0.000100
[ Thu Jul  4 07:11:33 2024 ] 	Batch(7200/7879) done. Loss: 0.3657  lr:0.000100
[ Thu Jul  4 07:11:52 2024 ] 	Batch(7300/7879) done. Loss: 0.0070  lr:0.000100
[ Thu Jul  4 07:12:10 2024 ] 	Batch(7400/7879) done. Loss: 0.2676  lr:0.000100
[ Thu Jul  4 07:12:28 2024 ] 
Training: Epoch [83/120], Step [7499], Loss: 0.2628454267978668, Training Accuracy: 97.49
[ Thu Jul  4 07:12:28 2024 ] 	Batch(7500/7879) done. Loss: 0.0069  lr:0.000100
[ Thu Jul  4 07:12:46 2024 ] 	Batch(7600/7879) done. Loss: 0.2121  lr:0.000100
[ Thu Jul  4 07:13:04 2024 ] 	Batch(7700/7879) done. Loss: 0.0446  lr:0.000100
[ Thu Jul  4 07:13:22 2024 ] 	Batch(7800/7879) done. Loss: 0.0735  lr:0.000100
[ Thu Jul  4 07:13:35 2024 ] 	Mean training loss: 0.0988.
[ Thu Jul  4 07:13:35 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 07:13:36 2024 ] Training epoch: 85
[ Thu Jul  4 07:13:36 2024 ] 	Batch(0/7879) done. Loss: 0.1561  lr:0.000100
[ Thu Jul  4 07:13:54 2024 ] 	Batch(100/7879) done. Loss: 0.0141  lr:0.000100
[ Thu Jul  4 07:14:12 2024 ] 	Batch(200/7879) done. Loss: 0.0036  lr:0.000100
[ Thu Jul  4 07:14:30 2024 ] 	Batch(300/7879) done. Loss: 0.0087  lr:0.000100
[ Thu Jul  4 07:14:48 2024 ] 	Batch(400/7879) done. Loss: 0.1624  lr:0.000100
[ Thu Jul  4 07:15:06 2024 ] 
Training: Epoch [84/120], Step [499], Loss: 0.010242270305752754, Training Accuracy: 97.45
[ Thu Jul  4 07:15:06 2024 ] 	Batch(500/7879) done. Loss: 0.1121  lr:0.000100
[ Thu Jul  4 07:15:24 2024 ] 	Batch(600/7879) done. Loss: 0.0162  lr:0.000100
[ Thu Jul  4 07:15:42 2024 ] 	Batch(700/7879) done. Loss: 0.0054  lr:0.000100
[ Thu Jul  4 07:16:00 2024 ] 	Batch(800/7879) done. Loss: 0.0711  lr:0.000100
[ Thu Jul  4 07:16:18 2024 ] 	Batch(900/7879) done. Loss: 0.0227  lr:0.000100
[ Thu Jul  4 07:16:35 2024 ] 
Training: Epoch [84/120], Step [999], Loss: 0.04980118200182915, Training Accuracy: 97.8125
[ Thu Jul  4 07:16:35 2024 ] 	Batch(1000/7879) done. Loss: 0.1014  lr:0.000100
[ Thu Jul  4 07:16:53 2024 ] 	Batch(1100/7879) done. Loss: 0.0266  lr:0.000100
[ Thu Jul  4 07:17:11 2024 ] 	Batch(1200/7879) done. Loss: 0.0033  lr:0.000100
[ Thu Jul  4 07:17:30 2024 ] 	Batch(1300/7879) done. Loss: 0.0276  lr:0.000100
[ Thu Jul  4 07:17:48 2024 ] 	Batch(1400/7879) done. Loss: 0.0186  lr:0.000100
[ Thu Jul  4 07:18:06 2024 ] 
Training: Epoch [84/120], Step [1499], Loss: 0.06475645303726196, Training Accuracy: 97.86666666666667
[ Thu Jul  4 07:18:06 2024 ] 	Batch(1500/7879) done. Loss: 0.1044  lr:0.000100
[ Thu Jul  4 07:18:24 2024 ] 	Batch(1600/7879) done. Loss: 0.0140  lr:0.000100
[ Thu Jul  4 07:18:42 2024 ] 	Batch(1700/7879) done. Loss: 0.0926  lr:0.000100
[ Thu Jul  4 07:19:01 2024 ] 	Batch(1800/7879) done. Loss: 0.1251  lr:0.000100
[ Thu Jul  4 07:19:19 2024 ] 	Batch(1900/7879) done. Loss: 0.0224  lr:0.000100
[ Thu Jul  4 07:19:37 2024 ] 
Training: Epoch [84/120], Step [1999], Loss: 0.027343016117811203, Training Accuracy: 97.78750000000001
[ Thu Jul  4 07:19:37 2024 ] 	Batch(2000/7879) done. Loss: 0.1456  lr:0.000100
[ Thu Jul  4 07:19:55 2024 ] 	Batch(2100/7879) done. Loss: 0.2558  lr:0.000100
[ Thu Jul  4 07:20:13 2024 ] 	Batch(2200/7879) done. Loss: 0.3120  lr:0.000100
[ Thu Jul  4 07:20:31 2024 ] 	Batch(2300/7879) done. Loss: 0.1743  lr:0.000100
[ Thu Jul  4 07:20:49 2024 ] 	Batch(2400/7879) done. Loss: 0.1897  lr:0.000100
[ Thu Jul  4 07:21:07 2024 ] 
Training: Epoch [84/120], Step [2499], Loss: 0.1538475900888443, Training Accuracy: 97.775
[ Thu Jul  4 07:21:07 2024 ] 	Batch(2500/7879) done. Loss: 0.0808  lr:0.000100
[ Thu Jul  4 07:21:25 2024 ] 	Batch(2600/7879) done. Loss: 0.0027  lr:0.000100
[ Thu Jul  4 07:21:43 2024 ] 	Batch(2700/7879) done. Loss: 0.0032  lr:0.000100
[ Thu Jul  4 07:22:01 2024 ] 	Batch(2800/7879) done. Loss: 0.0639  lr:0.000100
[ Thu Jul  4 07:22:19 2024 ] 	Batch(2900/7879) done. Loss: 0.2591  lr:0.000100
[ Thu Jul  4 07:22:37 2024 ] 
Training: Epoch [84/120], Step [2999], Loss: 0.00982758216559887, Training Accuracy: 97.75
[ Thu Jul  4 07:22:38 2024 ] 	Batch(3000/7879) done. Loss: 0.2973  lr:0.000100
[ Thu Jul  4 07:22:56 2024 ] 	Batch(3100/7879) done. Loss: 0.0168  lr:0.000100
[ Thu Jul  4 07:23:15 2024 ] 	Batch(3200/7879) done. Loss: 0.0201  lr:0.000100
[ Thu Jul  4 07:23:32 2024 ] 	Batch(3300/7879) done. Loss: 0.0094  lr:0.000100
[ Thu Jul  4 07:23:50 2024 ] 	Batch(3400/7879) done. Loss: 0.0371  lr:0.000100
[ Thu Jul  4 07:24:08 2024 ] 
Training: Epoch [84/120], Step [3499], Loss: 0.013359101489186287, Training Accuracy: 97.73571428571428
[ Thu Jul  4 07:24:08 2024 ] 	Batch(3500/7879) done. Loss: 0.4226  lr:0.000100
[ Thu Jul  4 07:24:26 2024 ] 	Batch(3600/7879) done. Loss: 0.3802  lr:0.000100
[ Thu Jul  4 07:24:45 2024 ] 	Batch(3700/7879) done. Loss: 0.0746  lr:0.000100
[ Thu Jul  4 07:25:03 2024 ] 	Batch(3800/7879) done. Loss: 0.5505  lr:0.000100
[ Thu Jul  4 07:25:21 2024 ] 	Batch(3900/7879) done. Loss: 0.0103  lr:0.000100
[ Thu Jul  4 07:25:39 2024 ] 
Training: Epoch [84/120], Step [3999], Loss: 0.04351827874779701, Training Accuracy: 97.75
[ Thu Jul  4 07:25:39 2024 ] 	Batch(4000/7879) done. Loss: 0.0563  lr:0.000100
[ Thu Jul  4 07:25:57 2024 ] 	Batch(4100/7879) done. Loss: 0.2684  lr:0.000100
[ Thu Jul  4 07:26:15 2024 ] 	Batch(4200/7879) done. Loss: 0.0304  lr:0.000100
[ Thu Jul  4 07:26:33 2024 ] 	Batch(4300/7879) done. Loss: 0.1560  lr:0.000100
[ Thu Jul  4 07:26:51 2024 ] 	Batch(4400/7879) done. Loss: 0.0158  lr:0.000100
[ Thu Jul  4 07:27:09 2024 ] 
Training: Epoch [84/120], Step [4499], Loss: 0.01053602248430252, Training Accuracy: 97.71944444444445
[ Thu Jul  4 07:27:09 2024 ] 	Batch(4500/7879) done. Loss: 0.0301  lr:0.000100
[ Thu Jul  4 07:27:28 2024 ] 	Batch(4600/7879) done. Loss: 0.0110  lr:0.000100
[ Thu Jul  4 07:27:46 2024 ] 	Batch(4700/7879) done. Loss: 0.1998  lr:0.000100
[ Thu Jul  4 07:28:05 2024 ] 	Batch(4800/7879) done. Loss: 0.4578  lr:0.000100
[ Thu Jul  4 07:28:23 2024 ] 	Batch(4900/7879) done. Loss: 0.0233  lr:0.000100
[ Thu Jul  4 07:28:42 2024 ] 
Training: Epoch [84/120], Step [4999], Loss: 0.053894735872745514, Training Accuracy: 97.68
[ Thu Jul  4 07:28:42 2024 ] 	Batch(5000/7879) done. Loss: 0.6488  lr:0.000100
[ Thu Jul  4 07:29:00 2024 ] 	Batch(5100/7879) done. Loss: 0.0812  lr:0.000100
[ Thu Jul  4 07:29:19 2024 ] 	Batch(5200/7879) done. Loss: 0.0221  lr:0.000100
[ Thu Jul  4 07:29:37 2024 ] 	Batch(5300/7879) done. Loss: 0.0135  lr:0.000100
[ Thu Jul  4 07:29:55 2024 ] 	Batch(5400/7879) done. Loss: 0.0614  lr:0.000100
[ Thu Jul  4 07:30:12 2024 ] 
Training: Epoch [84/120], Step [5499], Loss: 0.03273681923747063, Training Accuracy: 97.67272727272727
[ Thu Jul  4 07:30:13 2024 ] 	Batch(5500/7879) done. Loss: 0.0039  lr:0.000100
[ Thu Jul  4 07:30:31 2024 ] 	Batch(5600/7879) done. Loss: 0.0385  lr:0.000100
[ Thu Jul  4 07:30:49 2024 ] 	Batch(5700/7879) done. Loss: 0.0656  lr:0.000100
[ Thu Jul  4 07:31:08 2024 ] 	Batch(5800/7879) done. Loss: 0.6910  lr:0.000100
[ Thu Jul  4 07:31:27 2024 ] 	Batch(5900/7879) done. Loss: 0.0057  lr:0.000100
[ Thu Jul  4 07:31:45 2024 ] 
Training: Epoch [84/120], Step [5999], Loss: 0.11571593582630157, Training Accuracy: 97.62708333333333
[ Thu Jul  4 07:31:45 2024 ] 	Batch(6000/7879) done. Loss: 0.1093  lr:0.000100
[ Thu Jul  4 07:32:03 2024 ] 	Batch(6100/7879) done. Loss: 0.0438  lr:0.000100
[ Thu Jul  4 07:32:21 2024 ] 	Batch(6200/7879) done. Loss: 0.1985  lr:0.000100
[ Thu Jul  4 07:32:39 2024 ] 	Batch(6300/7879) done. Loss: 0.0445  lr:0.000100
[ Thu Jul  4 07:32:57 2024 ] 	Batch(6400/7879) done. Loss: 0.3527  lr:0.000100
[ Thu Jul  4 07:33:15 2024 ] 
Training: Epoch [84/120], Step [6499], Loss: 0.0673530101776123, Training Accuracy: 97.65384615384616
[ Thu Jul  4 07:33:15 2024 ] 	Batch(6500/7879) done. Loss: 0.0354  lr:0.000100
[ Thu Jul  4 07:33:33 2024 ] 	Batch(6600/7879) done. Loss: 0.0269  lr:0.000100
[ Thu Jul  4 07:33:51 2024 ] 	Batch(6700/7879) done. Loss: 0.0113  lr:0.000100
[ Thu Jul  4 07:34:09 2024 ] 	Batch(6800/7879) done. Loss: 0.0161  lr:0.000100
[ Thu Jul  4 07:34:28 2024 ] 	Batch(6900/7879) done. Loss: 0.0675  lr:0.000100
[ Thu Jul  4 07:34:46 2024 ] 
Training: Epoch [84/120], Step [6999], Loss: 0.10631420463323593, Training Accuracy: 97.66607142857143
[ Thu Jul  4 07:34:46 2024 ] 	Batch(7000/7879) done. Loss: 0.0092  lr:0.000100
[ Thu Jul  4 07:35:05 2024 ] 	Batch(7100/7879) done. Loss: 0.0128  lr:0.000100
[ Thu Jul  4 07:35:23 2024 ] 	Batch(7200/7879) done. Loss: 0.0063  lr:0.000100
[ Thu Jul  4 07:35:41 2024 ] 	Batch(7300/7879) done. Loss: 0.0238  lr:0.000100
[ Thu Jul  4 07:35:59 2024 ] 	Batch(7400/7879) done. Loss: 0.1380  lr:0.000100
[ Thu Jul  4 07:36:17 2024 ] 
Training: Epoch [84/120], Step [7499], Loss: 0.14004340767860413, Training Accuracy: 97.65
[ Thu Jul  4 07:36:17 2024 ] 	Batch(7500/7879) done. Loss: 0.0495  lr:0.000100
[ Thu Jul  4 07:36:35 2024 ] 	Batch(7600/7879) done. Loss: 0.0093  lr:0.000100
[ Thu Jul  4 07:36:54 2024 ] 	Batch(7700/7879) done. Loss: 0.0029  lr:0.000100
[ Thu Jul  4 07:37:12 2024 ] 	Batch(7800/7879) done. Loss: 0.2250  lr:0.000100
[ Thu Jul  4 07:37:27 2024 ] 	Mean training loss: 0.0966.
[ Thu Jul  4 07:37:27 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 07:37:27 2024 ] Training epoch: 86
[ Thu Jul  4 07:37:27 2024 ] 	Batch(0/7879) done. Loss: 0.0102  lr:0.000100
[ Thu Jul  4 07:37:46 2024 ] 	Batch(100/7879) done. Loss: 0.0315  lr:0.000100
[ Thu Jul  4 07:38:04 2024 ] 	Batch(200/7879) done. Loss: 0.0484  lr:0.000100
[ Thu Jul  4 07:38:22 2024 ] 	Batch(300/7879) done. Loss: 0.1886  lr:0.000100
[ Thu Jul  4 07:38:41 2024 ] 	Batch(400/7879) done. Loss: 0.2867  lr:0.000100
[ Thu Jul  4 07:38:59 2024 ] 
Training: Epoch [85/120], Step [499], Loss: 0.5506379008293152, Training Accuracy: 97.175
[ Thu Jul  4 07:38:59 2024 ] 	Batch(500/7879) done. Loss: 0.0843  lr:0.000100
[ Thu Jul  4 07:39:17 2024 ] 	Batch(600/7879) done. Loss: 0.1476  lr:0.000100
[ Thu Jul  4 07:39:36 2024 ] 	Batch(700/7879) done. Loss: 0.0054  lr:0.000100
[ Thu Jul  4 07:39:54 2024 ] 	Batch(800/7879) done. Loss: 0.2089  lr:0.000100
[ Thu Jul  4 07:40:12 2024 ] 	Batch(900/7879) done. Loss: 0.0862  lr:0.000100
[ Thu Jul  4 07:40:30 2024 ] 
Training: Epoch [85/120], Step [999], Loss: 0.0011484893038868904, Training Accuracy: 97.425
[ Thu Jul  4 07:40:30 2024 ] 	Batch(1000/7879) done. Loss: 0.2222  lr:0.000100
[ Thu Jul  4 07:40:48 2024 ] 	Batch(1100/7879) done. Loss: 0.1961  lr:0.000100
[ Thu Jul  4 07:41:06 2024 ] 	Batch(1200/7879) done. Loss: 0.0224  lr:0.000100
[ Thu Jul  4 07:41:25 2024 ] 	Batch(1300/7879) done. Loss: 0.1258  lr:0.000100
[ Thu Jul  4 07:41:43 2024 ] 	Batch(1400/7879) done. Loss: 0.0700  lr:0.000100
[ Thu Jul  4 07:42:00 2024 ] 
Training: Epoch [85/120], Step [1499], Loss: 0.010750532150268555, Training Accuracy: 97.49166666666666
[ Thu Jul  4 07:42:01 2024 ] 	Batch(1500/7879) done. Loss: 0.0599  lr:0.000100
[ Thu Jul  4 07:42:19 2024 ] 	Batch(1600/7879) done. Loss: 0.1758  lr:0.000100
[ Thu Jul  4 07:42:37 2024 ] 	Batch(1700/7879) done. Loss: 0.0736  lr:0.000100
[ Thu Jul  4 07:42:55 2024 ] 	Batch(1800/7879) done. Loss: 0.0334  lr:0.000100
[ Thu Jul  4 07:43:13 2024 ] 	Batch(1900/7879) done. Loss: 0.0278  lr:0.000100
[ Thu Jul  4 07:43:31 2024 ] 
Training: Epoch [85/120], Step [1999], Loss: 0.019559159874916077, Training Accuracy: 97.52499999999999
[ Thu Jul  4 07:43:31 2024 ] 	Batch(2000/7879) done. Loss: 0.0834  lr:0.000100
[ Thu Jul  4 07:43:49 2024 ] 	Batch(2100/7879) done. Loss: 0.1928  lr:0.000100
[ Thu Jul  4 07:44:07 2024 ] 	Batch(2200/7879) done. Loss: 0.0549  lr:0.000100
[ Thu Jul  4 07:44:25 2024 ] 	Batch(2300/7879) done. Loss: 0.0417  lr:0.000100
[ Thu Jul  4 07:44:44 2024 ] 	Batch(2400/7879) done. Loss: 0.0094  lr:0.000100
[ Thu Jul  4 07:45:02 2024 ] 
Training: Epoch [85/120], Step [2499], Loss: 0.05338739976286888, Training Accuracy: 97.45
[ Thu Jul  4 07:45:02 2024 ] 	Batch(2500/7879) done. Loss: 0.0512  lr:0.000100
[ Thu Jul  4 07:45:20 2024 ] 	Batch(2600/7879) done. Loss: 0.0326  lr:0.000100
[ Thu Jul  4 07:45:38 2024 ] 	Batch(2700/7879) done. Loss: 0.0047  lr:0.000100
[ Thu Jul  4 07:45:56 2024 ] 	Batch(2800/7879) done. Loss: 0.1330  lr:0.000100
[ Thu Jul  4 07:46:15 2024 ] 	Batch(2900/7879) done. Loss: 0.2670  lr:0.000100
[ Thu Jul  4 07:46:32 2024 ] 
Training: Epoch [85/120], Step [2999], Loss: 0.6081713438034058, Training Accuracy: 97.475
[ Thu Jul  4 07:46:33 2024 ] 	Batch(3000/7879) done. Loss: 0.2224  lr:0.000100
[ Thu Jul  4 07:46:51 2024 ] 	Batch(3100/7879) done. Loss: 0.1399  lr:0.000100
[ Thu Jul  4 07:47:09 2024 ] 	Batch(3200/7879) done. Loss: 0.0804  lr:0.000100
[ Thu Jul  4 07:47:27 2024 ] 	Batch(3300/7879) done. Loss: 0.4126  lr:0.000100
[ Thu Jul  4 07:47:45 2024 ] 	Batch(3400/7879) done. Loss: 0.0233  lr:0.000100
[ Thu Jul  4 07:48:03 2024 ] 
Training: Epoch [85/120], Step [3499], Loss: 0.023098766803741455, Training Accuracy: 97.46428571428571
[ Thu Jul  4 07:48:03 2024 ] 	Batch(3500/7879) done. Loss: 0.1260  lr:0.000100
[ Thu Jul  4 07:48:22 2024 ] 	Batch(3600/7879) done. Loss: 0.0104  lr:0.000100
[ Thu Jul  4 07:48:40 2024 ] 	Batch(3700/7879) done. Loss: 0.0082  lr:0.000100
[ Thu Jul  4 07:48:59 2024 ] 	Batch(3800/7879) done. Loss: 0.1915  lr:0.000100
[ Thu Jul  4 07:49:17 2024 ] 	Batch(3900/7879) done. Loss: 0.0058  lr:0.000100
[ Thu Jul  4 07:49:35 2024 ] 
Training: Epoch [85/120], Step [3999], Loss: 0.03536928445100784, Training Accuracy: 97.465625
[ Thu Jul  4 07:49:36 2024 ] 	Batch(4000/7879) done. Loss: 0.1640  lr:0.000100
[ Thu Jul  4 07:49:54 2024 ] 	Batch(4100/7879) done. Loss: 0.0038  lr:0.000100
[ Thu Jul  4 07:50:13 2024 ] 	Batch(4200/7879) done. Loss: 0.0049  lr:0.000100
[ Thu Jul  4 07:50:31 2024 ] 	Batch(4300/7879) done. Loss: 0.0204  lr:0.000100
[ Thu Jul  4 07:50:50 2024 ] 	Batch(4400/7879) done. Loss: 0.2030  lr:0.000100
[ Thu Jul  4 07:51:08 2024 ] 
Training: Epoch [85/120], Step [4499], Loss: 0.023127999156713486, Training Accuracy: 97.50555555555556
[ Thu Jul  4 07:51:08 2024 ] 	Batch(4500/7879) done. Loss: 0.3074  lr:0.000100
[ Thu Jul  4 07:51:26 2024 ] 	Batch(4600/7879) done. Loss: 0.0766  lr:0.000100
[ Thu Jul  4 07:51:44 2024 ] 	Batch(4700/7879) done. Loss: 0.1018  lr:0.000100
[ Thu Jul  4 07:52:02 2024 ] 	Batch(4800/7879) done. Loss: 0.0104  lr:0.000100
[ Thu Jul  4 07:52:20 2024 ] 	Batch(4900/7879) done. Loss: 0.0733  lr:0.000100
[ Thu Jul  4 07:52:37 2024 ] 
Training: Epoch [85/120], Step [4999], Loss: 0.0598779022693634, Training Accuracy: 97.55749999999999
[ Thu Jul  4 07:52:38 2024 ] 	Batch(5000/7879) done. Loss: 0.2240  lr:0.000100
[ Thu Jul  4 07:52:55 2024 ] 	Batch(5100/7879) done. Loss: 0.1526  lr:0.000100
[ Thu Jul  4 07:53:13 2024 ] 	Batch(5200/7879) done. Loss: 0.1917  lr:0.000100
[ Thu Jul  4 07:53:32 2024 ] 	Batch(5300/7879) done. Loss: 0.0317  lr:0.000100
[ Thu Jul  4 07:53:50 2024 ] 	Batch(5400/7879) done. Loss: 0.0507  lr:0.000100
[ Thu Jul  4 07:54:08 2024 ] 
Training: Epoch [85/120], Step [5499], Loss: 0.017423229292035103, Training Accuracy: 97.53181818181818
[ Thu Jul  4 07:54:08 2024 ] 	Batch(5500/7879) done. Loss: 0.0200  lr:0.000100
[ Thu Jul  4 07:54:26 2024 ] 	Batch(5600/7879) done. Loss: 0.0159  lr:0.000100
[ Thu Jul  4 07:54:44 2024 ] 	Batch(5700/7879) done. Loss: 0.1464  lr:0.000100
[ Thu Jul  4 07:55:01 2024 ] 	Batch(5800/7879) done. Loss: 0.0024  lr:0.000100
[ Thu Jul  4 07:55:19 2024 ] 	Batch(5900/7879) done. Loss: 0.0375  lr:0.000100
[ Thu Jul  4 07:55:37 2024 ] 
Training: Epoch [85/120], Step [5999], Loss: 0.016896016895771027, Training Accuracy: 97.54374999999999
[ Thu Jul  4 07:55:37 2024 ] 	Batch(6000/7879) done. Loss: 0.1997  lr:0.000100
[ Thu Jul  4 07:55:56 2024 ] 	Batch(6100/7879) done. Loss: 0.0033  lr:0.000100
[ Thu Jul  4 07:56:14 2024 ] 	Batch(6200/7879) done. Loss: 0.0037  lr:0.000100
[ Thu Jul  4 07:56:33 2024 ] 	Batch(6300/7879) done. Loss: 0.0555  lr:0.000100
[ Thu Jul  4 07:56:51 2024 ] 	Batch(6400/7879) done. Loss: 0.0043  lr:0.000100
[ Thu Jul  4 07:57:09 2024 ] 
Training: Epoch [85/120], Step [6499], Loss: 0.012054557912051678, Training Accuracy: 97.5576923076923
[ Thu Jul  4 07:57:09 2024 ] 	Batch(6500/7879) done. Loss: 0.0460  lr:0.000100
[ Thu Jul  4 07:57:28 2024 ] 	Batch(6600/7879) done. Loss: 0.1023  lr:0.000100
[ Thu Jul  4 07:57:46 2024 ] 	Batch(6700/7879) done. Loss: 0.0189  lr:0.000100
[ Thu Jul  4 07:58:04 2024 ] 	Batch(6800/7879) done. Loss: 0.0925  lr:0.000100
[ Thu Jul  4 07:58:22 2024 ] 	Batch(6900/7879) done. Loss: 0.2272  lr:0.000100
[ Thu Jul  4 07:58:41 2024 ] 
Training: Epoch [85/120], Step [6999], Loss: 0.5283323526382446, Training Accuracy: 97.5625
[ Thu Jul  4 07:58:41 2024 ] 	Batch(7000/7879) done. Loss: 0.0387  lr:0.000100
[ Thu Jul  4 07:58:59 2024 ] 	Batch(7100/7879) done. Loss: 0.0242  lr:0.000100
[ Thu Jul  4 07:59:18 2024 ] 	Batch(7200/7879) done. Loss: 0.2988  lr:0.000100
[ Thu Jul  4 07:59:37 2024 ] 	Batch(7300/7879) done. Loss: 0.0335  lr:0.000100
[ Thu Jul  4 07:59:55 2024 ] 	Batch(7400/7879) done. Loss: 0.0659  lr:0.000100
[ Thu Jul  4 08:00:13 2024 ] 
Training: Epoch [85/120], Step [7499], Loss: 0.3829047977924347, Training Accuracy: 97.57166666666667
[ Thu Jul  4 08:00:14 2024 ] 	Batch(7500/7879) done. Loss: 0.0819  lr:0.000100
[ Thu Jul  4 08:00:32 2024 ] 	Batch(7600/7879) done. Loss: 0.0104  lr:0.000100
[ Thu Jul  4 08:00:51 2024 ] 	Batch(7700/7879) done. Loss: 0.0249  lr:0.000100
[ Thu Jul  4 08:01:09 2024 ] 	Batch(7800/7879) done. Loss: 0.2827  lr:0.000100
[ Thu Jul  4 08:01:24 2024 ] 	Mean training loss: 0.1006.
[ Thu Jul  4 08:01:24 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul  4 08:01:24 2024 ] Training epoch: 87
[ Thu Jul  4 08:01:24 2024 ] 	Batch(0/7879) done. Loss: 0.0159  lr:0.000100
[ Thu Jul  4 08:01:42 2024 ] 	Batch(100/7879) done. Loss: 0.0075  lr:0.000100
[ Thu Jul  4 08:02:00 2024 ] 	Batch(200/7879) done. Loss: 0.2724  lr:0.000100
[ Thu Jul  4 08:02:18 2024 ] 	Batch(300/7879) done. Loss: 0.0209  lr:0.000100
[ Thu Jul  4 08:02:36 2024 ] 	Batch(400/7879) done. Loss: 0.1219  lr:0.000100
[ Thu Jul  4 08:02:53 2024 ] 
Training: Epoch [86/120], Step [499], Loss: 0.005590840708464384, Training Accuracy: 97.625
[ Thu Jul  4 08:02:53 2024 ] 	Batch(500/7879) done. Loss: 0.1672  lr:0.000100
[ Thu Jul  4 08:03:11 2024 ] 	Batch(600/7879) done. Loss: 0.0253  lr:0.000100
[ Thu Jul  4 08:03:29 2024 ] 	Batch(700/7879) done. Loss: 0.0067  lr:0.000100
[ Thu Jul  4 08:03:47 2024 ] 	Batch(800/7879) done. Loss: 0.0091  lr:0.000100
[ Thu Jul  4 08:04:05 2024 ] 	Batch(900/7879) done. Loss: 0.3696  lr:0.000100
[ Thu Jul  4 08:04:23 2024 ] 
Training: Epoch [86/120], Step [999], Loss: 0.01619012840092182, Training Accuracy: 97.775
[ Thu Jul  4 08:04:23 2024 ] 	Batch(1000/7879) done. Loss: 0.0816  lr:0.000100
[ Thu Jul  4 08:04:41 2024 ] 	Batch(1100/7879) done. Loss: 0.0034  lr:0.000100
[ Thu Jul  4 08:04:59 2024 ] 	Batch(1200/7879) done. Loss: 0.3356  lr:0.000100
[ Thu Jul  4 08:05:17 2024 ] 	Batch(1300/7879) done. Loss: 0.0242  lr:0.000100
[ Thu Jul  4 08:05:36 2024 ] 	Batch(1400/7879) done. Loss: 0.1943  lr:0.000100
[ Thu Jul  4 08:05:54 2024 ] 
Training: Epoch [86/120], Step [1499], Loss: 0.2576872408390045, Training Accuracy: 97.60833333333333
[ Thu Jul  4 08:05:54 2024 ] 	Batch(1500/7879) done. Loss: 0.7184  lr:0.000100
[ Thu Jul  4 08:06:13 2024 ] 	Batch(1600/7879) done. Loss: 0.0049  lr:0.000100
[ Thu Jul  4 08:06:31 2024 ] 	Batch(1700/7879) done. Loss: 0.0063  lr:0.000100
[ Thu Jul  4 08:06:50 2024 ] 	Batch(1800/7879) done. Loss: 0.1530  lr:0.000100
[ Thu Jul  4 08:07:08 2024 ] 	Batch(1900/7879) done. Loss: 0.0249  lr:0.000100
[ Thu Jul  4 08:07:27 2024 ] 
Training: Epoch [86/120], Step [1999], Loss: 0.31720831990242004, Training Accuracy: 97.53125
[ Thu Jul  4 08:07:27 2024 ] 	Batch(2000/7879) done. Loss: 0.0500  lr:0.000100
[ Thu Jul  4 08:07:45 2024 ] 	Batch(2100/7879) done. Loss: 0.0142  lr:0.000100
[ Thu Jul  4 08:08:03 2024 ] 	Batch(2200/7879) done. Loss: 0.0139  lr:0.000100
[ Thu Jul  4 08:08:21 2024 ] 	Batch(2300/7879) done. Loss: 0.3369  lr:0.000100
[ Thu Jul  4 08:08:39 2024 ] 	Batch(2400/7879) done. Loss: 0.0807  lr:0.000100
[ Thu Jul  4 08:08:56 2024 ] 
Training: Epoch [86/120], Step [2499], Loss: 0.028702277690172195, Training Accuracy: 97.52
[ Thu Jul  4 08:08:57 2024 ] 	Batch(2500/7879) done. Loss: 0.0188  lr:0.000100
[ Thu Jul  4 08:09:15 2024 ] 	Batch(2600/7879) done. Loss: 0.0582  lr:0.000100
[ Thu Jul  4 08:09:32 2024 ] 	Batch(2700/7879) done. Loss: 0.3048  lr:0.000100
[ Thu Jul  4 08:09:50 2024 ] 	Batch(2800/7879) done. Loss: 0.0326  lr:0.000100
[ Thu Jul  4 08:10:08 2024 ] 	Batch(2900/7879) done. Loss: 0.2903  lr:0.000100
[ Thu Jul  4 08:10:26 2024 ] 
Training: Epoch [86/120], Step [2999], Loss: 0.01924949884414673, Training Accuracy: 97.5625
[ Thu Jul  4 08:10:26 2024 ] 	Batch(3000/7879) done. Loss: 0.1561  lr:0.000100
[ Thu Jul  4 08:10:44 2024 ] 	Batch(3100/7879) done. Loss: 0.1004  lr:0.000100
[ Thu Jul  4 08:11:02 2024 ] 	Batch(3200/7879) done. Loss: 0.0404  lr:0.000100
[ Thu Jul  4 08:11:20 2024 ] 	Batch(3300/7879) done. Loss: 0.0059  lr:0.000100
[ Thu Jul  4 08:11:38 2024 ] 	Batch(3400/7879) done. Loss: 0.0029  lr:0.000100
[ Thu Jul  4 08:11:56 2024 ] 
Training: Epoch [86/120], Step [3499], Loss: 0.0014482253463938832, Training Accuracy: 97.55
[ Thu Jul  4 08:11:56 2024 ] 	Batch(3500/7879) done. Loss: 0.1657  lr:0.000100
[ Thu Jul  4 08:12:14 2024 ] 	Batch(3600/7879) done. Loss: 0.8331  lr:0.000100
[ Thu Jul  4 08:12:32 2024 ] 	Batch(3700/7879) done. Loss: 0.3242  lr:0.000100
[ Thu Jul  4 08:12:49 2024 ] 	Batch(3800/7879) done. Loss: 0.0226  lr:0.000100
[ Thu Jul  4 08:13:07 2024 ] 	Batch(3900/7879) done. Loss: 0.0346  lr:0.000100
[ Thu Jul  4 08:13:25 2024 ] 
Training: Epoch [86/120], Step [3999], Loss: 0.04915497452020645, Training Accuracy: 97.559375
[ Thu Jul  4 08:13:25 2024 ] 	Batch(4000/7879) done. Loss: 0.0159  lr:0.000100
[ Thu Jul  4 08:13:44 2024 ] 	Batch(4100/7879) done. Loss: 0.0066  lr:0.000100
[ Thu Jul  4 08:14:03 2024 ] 	Batch(4200/7879) done. Loss: 0.9755  lr:0.000100
[ Thu Jul  4 08:14:21 2024 ] 	Batch(4300/7879) done. Loss: 0.0139  lr:0.000100
[ Thu Jul  4 08:14:39 2024 ] 	Batch(4400/7879) done. Loss: 0.0182  lr:0.000100
[ Thu Jul  4 08:14:57 2024 ] 
Training: Epoch [86/120], Step [4499], Loss: 0.1273297667503357, Training Accuracy: 97.4861111111111
[ Thu Jul  4 08:14:57 2024 ] 	Batch(4500/7879) done. Loss: 0.0190  lr:0.000100
[ Thu Jul  4 08:15:16 2024 ] 	Batch(4600/7879) done. Loss: 0.1249  lr:0.000100
[ Thu Jul  4 08:15:34 2024 ] 	Batch(4700/7879) done. Loss: 0.2376  lr:0.000100
[ Thu Jul  4 08:15:53 2024 ] 	Batch(4800/7879) done. Loss: 0.2820  lr:0.000100
[ Thu Jul  4 08:16:11 2024 ] 	Batch(4900/7879) done. Loss: 0.0450  lr:0.000100
[ Thu Jul  4 08:16:28 2024 ] 
Training: Epoch [86/120], Step [4999], Loss: 0.054347146302461624, Training Accuracy: 97.4725
[ Thu Jul  4 08:16:29 2024 ] 	Batch(5000/7879) done. Loss: 0.0389  lr:0.000100
[ Thu Jul  4 08:16:46 2024 ] 	Batch(5100/7879) done. Loss: 0.0380  lr:0.000100
[ Thu Jul  4 08:17:04 2024 ] 	Batch(5200/7879) done. Loss: 0.0558  lr:0.000100
[ Thu Jul  4 08:17:22 2024 ] 	Batch(5300/7879) done. Loss: 0.0307  lr:0.000100
[ Thu Jul  4 08:17:40 2024 ] 	Batch(5400/7879) done. Loss: 0.4053  lr:0.000100
[ Thu Jul  4 08:17:58 2024 ] 
Training: Epoch [86/120], Step [5499], Loss: 0.13500842452049255, Training Accuracy: 97.49545454545454
[ Thu Jul  4 08:17:58 2024 ] 	Batch(5500/7879) done. Loss: 0.1073  lr:0.000100
[ Thu Jul  4 08:18:16 2024 ] 	Batch(5600/7879) done. Loss: 0.0256  lr:0.000100
[ Thu Jul  4 08:18:34 2024 ] 	Batch(5700/7879) done. Loss: 0.4386  lr:0.000100
[ Thu Jul  4 08:18:52 2024 ] 	Batch(5800/7879) done. Loss: 0.0688  lr:0.000100
[ Thu Jul  4 08:19:10 2024 ] 	Batch(5900/7879) done. Loss: 0.0756  lr:0.000100
[ Thu Jul  4 08:19:27 2024 ] 
Training: Epoch [86/120], Step [5999], Loss: 0.19648273289203644, Training Accuracy: 97.50416666666666
[ Thu Jul  4 08:19:27 2024 ] 	Batch(6000/7879) done. Loss: 0.2398  lr:0.000100
[ Thu Jul  4 08:19:46 2024 ] 	Batch(6100/7879) done. Loss: 0.0347  lr:0.000100
[ Thu Jul  4 08:20:04 2024 ] 	Batch(6200/7879) done. Loss: 0.0055  lr:0.000100
[ Thu Jul  4 08:20:23 2024 ] 	Batch(6300/7879) done. Loss: 0.3124  lr:0.000100
[ Thu Jul  4 08:20:42 2024 ] 	Batch(6400/7879) done. Loss: 0.0259  lr:0.000100
[ Thu Jul  4 08:20:59 2024 ] 
Training: Epoch [86/120], Step [6499], Loss: 0.05470613017678261, Training Accuracy: 97.5
[ Thu Jul  4 08:20:59 2024 ] 	Batch(6500/7879) done. Loss: 0.1866  lr:0.000100
[ Thu Jul  4 08:21:17 2024 ] 	Batch(6600/7879) done. Loss: 0.0113  lr:0.000100
[ Thu Jul  4 08:21:35 2024 ] 	Batch(6700/7879) done. Loss: 0.0265  lr:0.000100
[ Thu Jul  4 08:21:53 2024 ] 	Batch(6800/7879) done. Loss: 0.2032  lr:0.000100
[ Thu Jul  4 08:22:11 2024 ] 	Batch(6900/7879) done. Loss: 0.2050  lr:0.000100
[ Thu Jul  4 08:22:29 2024 ] 
Training: Epoch [86/120], Step [6999], Loss: 0.09696019440889359, Training Accuracy: 97.49642857142857
[ Thu Jul  4 08:22:29 2024 ] 	Batch(7000/7879) done. Loss: 0.1559  lr:0.000100
[ Thu Jul  4 08:22:47 2024 ] 	Batch(7100/7879) done. Loss: 0.0072  lr:0.000100
[ Thu Jul  4 08:23:05 2024 ] 	Batch(7200/7879) done. Loss: 0.2593  lr:0.000100
[ Thu Jul  4 08:23:23 2024 ] 	Batch(7300/7879) done. Loss: 0.0126  lr:0.000100
[ Thu Jul  4 08:23:42 2024 ] 	Batch(7400/7879) done. Loss: 0.3141  lr:0.000100
[ Thu Jul  4 08:24:00 2024 ] 
Training: Epoch [86/120], Step [7499], Loss: 0.08032136410474777, Training Accuracy: 97.48833333333333
[ Thu Jul  4 08:24:00 2024 ] 	Batch(7500/7879) done. Loss: 0.1577  lr:0.000100
[ Thu Jul  4 08:24:19 2024 ] 	Batch(7600/7879) done. Loss: 0.0058  lr:0.000100
[ Thu Jul  4 08:24:38 2024 ] 	Batch(7700/7879) done. Loss: 0.2201  lr:0.000100
[ Thu Jul  4 08:24:56 2024 ] 	Batch(7800/7879) done. Loss: 0.0226  lr:0.000100
[ Thu Jul  4 08:25:11 2024 ] 	Mean training loss: 0.0941.
[ Thu Jul  4 08:25:11 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 08:25:11 2024 ] Training epoch: 88
[ Thu Jul  4 08:25:11 2024 ] 	Batch(0/7879) done. Loss: 0.0327  lr:0.000100
[ Thu Jul  4 08:25:29 2024 ] 	Batch(100/7879) done. Loss: 0.0388  lr:0.000100
[ Thu Jul  4 08:25:48 2024 ] 	Batch(200/7879) done. Loss: 0.0066  lr:0.000100
[ Thu Jul  4 08:26:06 2024 ] 	Batch(300/7879) done. Loss: 0.0389  lr:0.000100
[ Thu Jul  4 08:26:24 2024 ] 	Batch(400/7879) done. Loss: 0.0733  lr:0.000100
[ Thu Jul  4 08:26:41 2024 ] 
Training: Epoch [87/120], Step [499], Loss: 0.117125503718853, Training Accuracy: 97.775
[ Thu Jul  4 08:26:41 2024 ] 	Batch(500/7879) done. Loss: 0.0356  lr:0.000100
[ Thu Jul  4 08:26:59 2024 ] 	Batch(600/7879) done. Loss: 0.0156  lr:0.000100
[ Thu Jul  4 08:27:17 2024 ] 	Batch(700/7879) done. Loss: 0.0028  lr:0.000100
[ Thu Jul  4 08:27:35 2024 ] 	Batch(800/7879) done. Loss: 0.0216  lr:0.000100
[ Thu Jul  4 08:27:53 2024 ] 	Batch(900/7879) done. Loss: 0.0693  lr:0.000100
[ Thu Jul  4 08:28:11 2024 ] 
Training: Epoch [87/120], Step [999], Loss: 0.26975300908088684, Training Accuracy: 97.6375
[ Thu Jul  4 08:28:11 2024 ] 	Batch(1000/7879) done. Loss: 0.2397  lr:0.000100
[ Thu Jul  4 08:28:29 2024 ] 	Batch(1100/7879) done. Loss: 0.0176  lr:0.000100
[ Thu Jul  4 08:28:47 2024 ] 	Batch(1200/7879) done. Loss: 0.0372  lr:0.000100
[ Thu Jul  4 08:29:06 2024 ] 	Batch(1300/7879) done. Loss: 0.0414  lr:0.000100
[ Thu Jul  4 08:29:24 2024 ] 	Batch(1400/7879) done. Loss: 0.1171  lr:0.000100
[ Thu Jul  4 08:29:43 2024 ] 
Training: Epoch [87/120], Step [1499], Loss: 0.09214932471513748, Training Accuracy: 97.52499999999999
[ Thu Jul  4 08:29:43 2024 ] 	Batch(1500/7879) done. Loss: 0.0777  lr:0.000100
[ Thu Jul  4 08:30:01 2024 ] 	Batch(1600/7879) done. Loss: 0.0544  lr:0.000100
[ Thu Jul  4 08:30:19 2024 ] 	Batch(1700/7879) done. Loss: 0.0023  lr:0.000100
[ Thu Jul  4 08:30:37 2024 ] 	Batch(1800/7879) done. Loss: 0.0107  lr:0.000100
[ Thu Jul  4 08:30:55 2024 ] 	Batch(1900/7879) done. Loss: 0.3933  lr:0.000100
[ Thu Jul  4 08:31:13 2024 ] 
Training: Epoch [87/120], Step [1999], Loss: 0.017294632270932198, Training Accuracy: 97.45625
[ Thu Jul  4 08:31:13 2024 ] 	Batch(2000/7879) done. Loss: 0.0050  lr:0.000100
[ Thu Jul  4 08:31:32 2024 ] 	Batch(2100/7879) done. Loss: 0.1684  lr:0.000100
[ Thu Jul  4 08:31:50 2024 ] 	Batch(2200/7879) done. Loss: 0.0019  lr:0.000100
[ Thu Jul  4 08:32:09 2024 ] 	Batch(2300/7879) done. Loss: 0.0668  lr:0.000100
[ Thu Jul  4 08:32:27 2024 ] 	Batch(2400/7879) done. Loss: 0.1767  lr:0.000100
[ Thu Jul  4 08:32:45 2024 ] 
Training: Epoch [87/120], Step [2499], Loss: 0.003840162418782711, Training Accuracy: 97.45
[ Thu Jul  4 08:32:45 2024 ] 	Batch(2500/7879) done. Loss: 0.0205  lr:0.000100
[ Thu Jul  4 08:33:03 2024 ] 	Batch(2600/7879) done. Loss: 0.0964  lr:0.000100
[ Thu Jul  4 08:33:20 2024 ] 	Batch(2700/7879) done. Loss: 0.0401  lr:0.000100
[ Thu Jul  4 08:33:38 2024 ] 	Batch(2800/7879) done. Loss: 0.0257  lr:0.000100
[ Thu Jul  4 08:33:56 2024 ] 	Batch(2900/7879) done. Loss: 0.0095  lr:0.000100
[ Thu Jul  4 08:34:14 2024 ] 
Training: Epoch [87/120], Step [2999], Loss: 0.013943196274340153, Training Accuracy: 97.35416666666666
[ Thu Jul  4 08:34:14 2024 ] 	Batch(3000/7879) done. Loss: 0.0078  lr:0.000100
[ Thu Jul  4 08:34:32 2024 ] 	Batch(3100/7879) done. Loss: 0.0199  lr:0.000100
[ Thu Jul  4 08:34:50 2024 ] 	Batch(3200/7879) done. Loss: 0.0137  lr:0.000100
[ Thu Jul  4 08:35:08 2024 ] 	Batch(3300/7879) done. Loss: 0.0225  lr:0.000100
[ Thu Jul  4 08:35:26 2024 ] 	Batch(3400/7879) done. Loss: 0.0105  lr:0.000100
[ Thu Jul  4 08:35:43 2024 ] 
Training: Epoch [87/120], Step [3499], Loss: 0.016329368576407433, Training Accuracy: 97.35357142857143
[ Thu Jul  4 08:35:44 2024 ] 	Batch(3500/7879) done. Loss: 0.0643  lr:0.000100
[ Thu Jul  4 08:36:02 2024 ] 	Batch(3600/7879) done. Loss: 0.0523  lr:0.000100
[ Thu Jul  4 08:36:20 2024 ] 	Batch(3700/7879) done. Loss: 0.0157  lr:0.000100
[ Thu Jul  4 08:36:37 2024 ] 	Batch(3800/7879) done. Loss: 0.0635  lr:0.000100
[ Thu Jul  4 08:36:55 2024 ] 	Batch(3900/7879) done. Loss: 0.0647  lr:0.000100
[ Thu Jul  4 08:37:13 2024 ] 
Training: Epoch [87/120], Step [3999], Loss: 0.08667261153459549, Training Accuracy: 97.403125
[ Thu Jul  4 08:37:13 2024 ] 	Batch(4000/7879) done. Loss: 0.0245  lr:0.000100
[ Thu Jul  4 08:37:31 2024 ] 	Batch(4100/7879) done. Loss: 0.0558  lr:0.000100
[ Thu Jul  4 08:37:49 2024 ] 	Batch(4200/7879) done. Loss: 0.1128  lr:0.000100
[ Thu Jul  4 08:38:07 2024 ] 	Batch(4300/7879) done. Loss: 0.3741  lr:0.000100
[ Thu Jul  4 08:38:25 2024 ] 	Batch(4400/7879) done. Loss: 0.0060  lr:0.000100
[ Thu Jul  4 08:38:43 2024 ] 
Training: Epoch [87/120], Step [4499], Loss: 0.023903358727693558, Training Accuracy: 97.42777777777778
[ Thu Jul  4 08:38:43 2024 ] 	Batch(4500/7879) done. Loss: 0.2139  lr:0.000100
[ Thu Jul  4 08:39:01 2024 ] 	Batch(4600/7879) done. Loss: 0.0388  lr:0.000100
[ Thu Jul  4 08:39:19 2024 ] 	Batch(4700/7879) done. Loss: 0.1243  lr:0.000100
[ Thu Jul  4 08:39:37 2024 ] 	Batch(4800/7879) done. Loss: 0.0512  lr:0.000100
[ Thu Jul  4 08:39:54 2024 ] 	Batch(4900/7879) done. Loss: 0.0052  lr:0.000100
[ Thu Jul  4 08:40:12 2024 ] 
Training: Epoch [87/120], Step [4999], Loss: 0.0005064124125055969, Training Accuracy: 97.44749999999999
[ Thu Jul  4 08:40:12 2024 ] 	Batch(5000/7879) done. Loss: 0.1399  lr:0.000100
[ Thu Jul  4 08:40:30 2024 ] 	Batch(5100/7879) done. Loss: 0.0034  lr:0.000100
[ Thu Jul  4 08:40:48 2024 ] 	Batch(5200/7879) done. Loss: 0.2403  lr:0.000100
[ Thu Jul  4 08:41:06 2024 ] 	Batch(5300/7879) done. Loss: 0.0130  lr:0.000100
[ Thu Jul  4 08:41:24 2024 ] 	Batch(5400/7879) done. Loss: 0.0911  lr:0.000100
[ Thu Jul  4 08:41:42 2024 ] 
Training: Epoch [87/120], Step [5499], Loss: 0.5094954967498779, Training Accuracy: 97.425
[ Thu Jul  4 08:41:42 2024 ] 	Batch(5500/7879) done. Loss: 0.2516  lr:0.000100
[ Thu Jul  4 08:42:00 2024 ] 	Batch(5600/7879) done. Loss: 0.0268  lr:0.000100
[ Thu Jul  4 08:42:18 2024 ] 	Batch(5700/7879) done. Loss: 0.2573  lr:0.000100
[ Thu Jul  4 08:42:36 2024 ] 	Batch(5800/7879) done. Loss: 0.1024  lr:0.000100
[ Thu Jul  4 08:42:54 2024 ] 	Batch(5900/7879) done. Loss: 0.0030  lr:0.000100
[ Thu Jul  4 08:43:12 2024 ] 
Training: Epoch [87/120], Step [5999], Loss: 0.049578264355659485, Training Accuracy: 97.43958333333333
[ Thu Jul  4 08:43:12 2024 ] 	Batch(6000/7879) done. Loss: 0.0489  lr:0.000100
[ Thu Jul  4 08:43:30 2024 ] 	Batch(6100/7879) done. Loss: 0.0878  lr:0.000100
[ Thu Jul  4 08:43:48 2024 ] 	Batch(6200/7879) done. Loss: 0.0700  lr:0.000100
[ Thu Jul  4 08:44:06 2024 ] 	Batch(6300/7879) done. Loss: 0.0294  lr:0.000100
[ Thu Jul  4 08:44:24 2024 ] 	Batch(6400/7879) done. Loss: 0.2708  lr:0.000100
[ Thu Jul  4 08:44:41 2024 ] 
Training: Epoch [87/120], Step [6499], Loss: 0.06307555735111237, Training Accuracy: 97.45384615384616
[ Thu Jul  4 08:44:41 2024 ] 	Batch(6500/7879) done. Loss: 0.0523  lr:0.000100
[ Thu Jul  4 08:44:59 2024 ] 	Batch(6600/7879) done. Loss: 0.3707  lr:0.000100
[ Thu Jul  4 08:45:17 2024 ] 	Batch(6700/7879) done. Loss: 0.2050  lr:0.000100
[ Thu Jul  4 08:45:35 2024 ] 	Batch(6800/7879) done. Loss: 0.0934  lr:0.000100
[ Thu Jul  4 08:45:53 2024 ] 	Batch(6900/7879) done. Loss: 0.3881  lr:0.000100
[ Thu Jul  4 08:46:11 2024 ] 
Training: Epoch [87/120], Step [6999], Loss: 0.005316055379807949, Training Accuracy: 97.48214285714286
[ Thu Jul  4 08:46:11 2024 ] 	Batch(7000/7879) done. Loss: 0.0268  lr:0.000100
[ Thu Jul  4 08:46:29 2024 ] 	Batch(7100/7879) done. Loss: 0.5673  lr:0.000100
[ Thu Jul  4 08:46:47 2024 ] 	Batch(7200/7879) done. Loss: 0.3897  lr:0.000100
[ Thu Jul  4 08:47:05 2024 ] 	Batch(7300/7879) done. Loss: 0.0430  lr:0.000100
[ Thu Jul  4 08:47:23 2024 ] 	Batch(7400/7879) done. Loss: 0.1264  lr:0.000100
[ Thu Jul  4 08:47:40 2024 ] 
Training: Epoch [87/120], Step [7499], Loss: 0.28082892298698425, Training Accuracy: 97.465
[ Thu Jul  4 08:47:40 2024 ] 	Batch(7500/7879) done. Loss: 0.0142  lr:0.000100
[ Thu Jul  4 08:47:58 2024 ] 	Batch(7600/7879) done. Loss: 0.3473  lr:0.000100
[ Thu Jul  4 08:48:17 2024 ] 	Batch(7700/7879) done. Loss: 0.0458  lr:0.000100
[ Thu Jul  4 08:48:35 2024 ] 	Batch(7800/7879) done. Loss: 0.0828  lr:0.000100
[ Thu Jul  4 08:48:50 2024 ] 	Mean training loss: 0.0963.
[ Thu Jul  4 08:48:50 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 08:48:50 2024 ] Training epoch: 89
[ Thu Jul  4 08:48:51 2024 ] 	Batch(0/7879) done. Loss: 0.3545  lr:0.000100
[ Thu Jul  4 08:49:09 2024 ] 	Batch(100/7879) done. Loss: 0.0069  lr:0.000100
[ Thu Jul  4 08:49:26 2024 ] 	Batch(200/7879) done. Loss: 0.1487  lr:0.000100
[ Thu Jul  4 08:49:44 2024 ] 	Batch(300/7879) done. Loss: 0.0200  lr:0.000100
[ Thu Jul  4 08:50:02 2024 ] 	Batch(400/7879) done. Loss: 0.0441  lr:0.000100
[ Thu Jul  4 08:50:20 2024 ] 
Training: Epoch [88/120], Step [499], Loss: 0.04440547525882721, Training Accuracy: 97.8
[ Thu Jul  4 08:50:20 2024 ] 	Batch(500/7879) done. Loss: 0.0087  lr:0.000100
[ Thu Jul  4 08:50:38 2024 ] 	Batch(600/7879) done. Loss: 0.0496  lr:0.000100
[ Thu Jul  4 08:50:56 2024 ] 	Batch(700/7879) done. Loss: 0.0368  lr:0.000100
[ Thu Jul  4 08:51:14 2024 ] 	Batch(800/7879) done. Loss: 0.0088  lr:0.000100
[ Thu Jul  4 08:51:32 2024 ] 	Batch(900/7879) done. Loss: 0.0139  lr:0.000100
[ Thu Jul  4 08:51:50 2024 ] 
Training: Epoch [88/120], Step [999], Loss: 0.011780083179473877, Training Accuracy: 97.5875
[ Thu Jul  4 08:51:50 2024 ] 	Batch(1000/7879) done. Loss: 0.1154  lr:0.000100
[ Thu Jul  4 08:52:08 2024 ] 	Batch(1100/7879) done. Loss: 0.1008  lr:0.000100
[ Thu Jul  4 08:52:26 2024 ] 	Batch(1200/7879) done. Loss: 0.2004  lr:0.000100
[ Thu Jul  4 08:52:44 2024 ] 	Batch(1300/7879) done. Loss: 0.0120  lr:0.000100
[ Thu Jul  4 08:53:02 2024 ] 	Batch(1400/7879) done. Loss: 0.0524  lr:0.000100
[ Thu Jul  4 08:53:19 2024 ] 
Training: Epoch [88/120], Step [1499], Loss: 0.006345248315483332, Training Accuracy: 97.60833333333333
[ Thu Jul  4 08:53:20 2024 ] 	Batch(1500/7879) done. Loss: 0.0071  lr:0.000100
[ Thu Jul  4 08:53:38 2024 ] 	Batch(1600/7879) done. Loss: 0.5379  lr:0.000100
[ Thu Jul  4 08:53:56 2024 ] 	Batch(1700/7879) done. Loss: 0.0184  lr:0.000100
[ Thu Jul  4 08:54:15 2024 ] 	Batch(1800/7879) done. Loss: 0.0105  lr:0.000100
[ Thu Jul  4 08:54:33 2024 ] 	Batch(1900/7879) done. Loss: 0.1279  lr:0.000100
[ Thu Jul  4 08:54:52 2024 ] 
Training: Epoch [88/120], Step [1999], Loss: 0.004304748494178057, Training Accuracy: 97.6375
[ Thu Jul  4 08:54:52 2024 ] 	Batch(2000/7879) done. Loss: 0.0873  lr:0.000100
[ Thu Jul  4 08:55:11 2024 ] 	Batch(2100/7879) done. Loss: 0.0299  lr:0.000100
[ Thu Jul  4 08:55:29 2024 ] 	Batch(2200/7879) done. Loss: 0.0080  lr:0.000100
[ Thu Jul  4 08:55:47 2024 ] 	Batch(2300/7879) done. Loss: 0.2085  lr:0.000100
[ Thu Jul  4 08:56:05 2024 ] 	Batch(2400/7879) done. Loss: 0.0467  lr:0.000100
[ Thu Jul  4 08:56:23 2024 ] 
Training: Epoch [88/120], Step [2499], Loss: 0.08758628368377686, Training Accuracy: 97.61999999999999
[ Thu Jul  4 08:56:23 2024 ] 	Batch(2500/7879) done. Loss: 0.2339  lr:0.000100
[ Thu Jul  4 08:56:41 2024 ] 	Batch(2600/7879) done. Loss: 0.1362  lr:0.000100
[ Thu Jul  4 08:56:59 2024 ] 	Batch(2700/7879) done. Loss: 0.0012  lr:0.000100
[ Thu Jul  4 08:57:17 2024 ] 	Batch(2800/7879) done. Loss: 0.1002  lr:0.000100
[ Thu Jul  4 08:57:35 2024 ] 	Batch(2900/7879) done. Loss: 0.0206  lr:0.000100
[ Thu Jul  4 08:57:52 2024 ] 
Training: Epoch [88/120], Step [2999], Loss: 0.009666354395449162, Training Accuracy: 97.67083333333333
[ Thu Jul  4 08:57:53 2024 ] 	Batch(3000/7879) done. Loss: 0.0200  lr:0.000100
[ Thu Jul  4 08:58:10 2024 ] 	Batch(3100/7879) done. Loss: 0.0555  lr:0.000100
[ Thu Jul  4 08:58:28 2024 ] 	Batch(3200/7879) done. Loss: 0.0813  lr:0.000100
[ Thu Jul  4 08:58:46 2024 ] 	Batch(3300/7879) done. Loss: 0.0339  lr:0.000100
[ Thu Jul  4 08:59:04 2024 ] 	Batch(3400/7879) done. Loss: 0.0588  lr:0.000100
[ Thu Jul  4 08:59:22 2024 ] 
Training: Epoch [88/120], Step [3499], Loss: 0.0164682324975729, Training Accuracy: 97.71071428571429
[ Thu Jul  4 08:59:22 2024 ] 	Batch(3500/7879) done. Loss: 0.0509  lr:0.000100
[ Thu Jul  4 08:59:40 2024 ] 	Batch(3600/7879) done. Loss: 0.0536  lr:0.000100
[ Thu Jul  4 08:59:58 2024 ] 	Batch(3700/7879) done. Loss: 0.0283  lr:0.000100
[ Thu Jul  4 09:00:16 2024 ] 	Batch(3800/7879) done. Loss: 0.0431  lr:0.000100
[ Thu Jul  4 09:00:34 2024 ] 	Batch(3900/7879) done. Loss: 0.0542  lr:0.000100
[ Thu Jul  4 09:00:51 2024 ] 
Training: Epoch [88/120], Step [3999], Loss: 0.21272489428520203, Training Accuracy: 97.665625
[ Thu Jul  4 09:00:52 2024 ] 	Batch(4000/7879) done. Loss: 0.0192  lr:0.000100
[ Thu Jul  4 09:01:10 2024 ] 	Batch(4100/7879) done. Loss: 0.0289  lr:0.000100
[ Thu Jul  4 09:01:27 2024 ] 	Batch(4200/7879) done. Loss: 0.3363  lr:0.000100
[ Thu Jul  4 09:01:45 2024 ] 	Batch(4300/7879) done. Loss: 0.0220  lr:0.000100
[ Thu Jul  4 09:02:03 2024 ] 	Batch(4400/7879) done. Loss: 0.0794  lr:0.000100
[ Thu Jul  4 09:02:21 2024 ] 
Training: Epoch [88/120], Step [4499], Loss: 0.006621857639402151, Training Accuracy: 97.66111111111111
[ Thu Jul  4 09:02:21 2024 ] 	Batch(4500/7879) done. Loss: 0.0079  lr:0.000100
[ Thu Jul  4 09:02:39 2024 ] 	Batch(4600/7879) done. Loss: 0.1763  lr:0.000100
[ Thu Jul  4 09:02:57 2024 ] 	Batch(4700/7879) done. Loss: 0.3112  lr:0.000100
[ Thu Jul  4 09:03:15 2024 ] 	Batch(4800/7879) done. Loss: 0.0028  lr:0.000100
[ Thu Jul  4 09:03:33 2024 ] 	Batch(4900/7879) done. Loss: 0.0128  lr:0.000100
[ Thu Jul  4 09:03:51 2024 ] 
Training: Epoch [88/120], Step [4999], Loss: 0.12562298774719238, Training Accuracy: 97.64500000000001
[ Thu Jul  4 09:03:51 2024 ] 	Batch(5000/7879) done. Loss: 0.1152  lr:0.000100
[ Thu Jul  4 09:04:09 2024 ] 	Batch(5100/7879) done. Loss: 0.0171  lr:0.000100
[ Thu Jul  4 09:04:27 2024 ] 	Batch(5200/7879) done. Loss: 0.0933  lr:0.000100
[ Thu Jul  4 09:04:45 2024 ] 	Batch(5300/7879) done. Loss: 0.1924  lr:0.000100
[ Thu Jul  4 09:05:02 2024 ] 	Batch(5400/7879) done. Loss: 0.0034  lr:0.000100
[ Thu Jul  4 09:05:20 2024 ] 
Training: Epoch [88/120], Step [5499], Loss: 0.018662964925169945, Training Accuracy: 97.65454545454546
[ Thu Jul  4 09:05:20 2024 ] 	Batch(5500/7879) done. Loss: 0.2911  lr:0.000100
[ Thu Jul  4 09:05:39 2024 ] 	Batch(5600/7879) done. Loss: 0.0105  lr:0.000100
[ Thu Jul  4 09:05:57 2024 ] 	Batch(5700/7879) done. Loss: 0.0861  lr:0.000100
[ Thu Jul  4 09:06:15 2024 ] 	Batch(5800/7879) done. Loss: 0.0053  lr:0.000100
[ Thu Jul  4 09:06:32 2024 ] 	Batch(5900/7879) done. Loss: 0.0194  lr:0.000100
[ Thu Jul  4 09:06:50 2024 ] 
Training: Epoch [88/120], Step [5999], Loss: 0.005919404793530703, Training Accuracy: 97.63333333333334
[ Thu Jul  4 09:06:50 2024 ] 	Batch(6000/7879) done. Loss: 0.1530  lr:0.000100
[ Thu Jul  4 09:07:08 2024 ] 	Batch(6100/7879) done. Loss: 0.2238  lr:0.000100
[ Thu Jul  4 09:07:26 2024 ] 	Batch(6200/7879) done. Loss: 0.0111  lr:0.000100
[ Thu Jul  4 09:07:44 2024 ] 	Batch(6300/7879) done. Loss: 0.0565  lr:0.000100
[ Thu Jul  4 09:08:02 2024 ] 	Batch(6400/7879) done. Loss: 0.0684  lr:0.000100
[ Thu Jul  4 09:08:20 2024 ] 
Training: Epoch [88/120], Step [6499], Loss: 0.28272998332977295, Training Accuracy: 97.63461538461539
[ Thu Jul  4 09:08:20 2024 ] 	Batch(6500/7879) done. Loss: 0.0057  lr:0.000100
[ Thu Jul  4 09:08:38 2024 ] 	Batch(6600/7879) done. Loss: 0.2555  lr:0.000100
[ Thu Jul  4 09:08:56 2024 ] 	Batch(6700/7879) done. Loss: 0.1519  lr:0.000100
[ Thu Jul  4 09:09:14 2024 ] 	Batch(6800/7879) done. Loss: 0.0778  lr:0.000100
[ Thu Jul  4 09:09:31 2024 ] 	Batch(6900/7879) done. Loss: 0.0157  lr:0.000100
[ Thu Jul  4 09:09:49 2024 ] 
Training: Epoch [88/120], Step [6999], Loss: 0.013513081707060337, Training Accuracy: 97.65178571428571
[ Thu Jul  4 09:09:49 2024 ] 	Batch(7000/7879) done. Loss: 0.0439  lr:0.000100
[ Thu Jul  4 09:10:07 2024 ] 	Batch(7100/7879) done. Loss: 0.3276  lr:0.000100
[ Thu Jul  4 09:10:26 2024 ] 	Batch(7200/7879) done. Loss: 0.0552  lr:0.000100
[ Thu Jul  4 09:10:44 2024 ] 	Batch(7300/7879) done. Loss: 0.0060  lr:0.000100
[ Thu Jul  4 09:11:02 2024 ] 	Batch(7400/7879) done. Loss: 0.0951  lr:0.000100
[ Thu Jul  4 09:11:20 2024 ] 
Training: Epoch [88/120], Step [7499], Loss: 0.054659053683280945, Training Accuracy: 97.67833333333333
[ Thu Jul  4 09:11:20 2024 ] 	Batch(7500/7879) done. Loss: 0.1011  lr:0.000100
[ Thu Jul  4 09:11:38 2024 ] 	Batch(7600/7879) done. Loss: 0.0602  lr:0.000100
[ Thu Jul  4 09:11:56 2024 ] 	Batch(7700/7879) done. Loss: 0.0106  lr:0.000100
[ Thu Jul  4 09:12:14 2024 ] 	Batch(7800/7879) done. Loss: 0.1612  lr:0.000100
[ Thu Jul  4 09:12:28 2024 ] 	Mean training loss: 0.0965.
[ Thu Jul  4 09:12:28 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 09:12:28 2024 ] Training epoch: 90
[ Thu Jul  4 09:12:28 2024 ] 	Batch(0/7879) done. Loss: 0.0244  lr:0.000100
[ Thu Jul  4 09:12:46 2024 ] 	Batch(100/7879) done. Loss: 0.1437  lr:0.000100
[ Thu Jul  4 09:13:04 2024 ] 	Batch(200/7879) done. Loss: 0.0356  lr:0.000100
[ Thu Jul  4 09:13:22 2024 ] 	Batch(300/7879) done. Loss: 0.0172  lr:0.000100
[ Thu Jul  4 09:13:40 2024 ] 	Batch(400/7879) done. Loss: 0.2462  lr:0.000100
[ Thu Jul  4 09:13:58 2024 ] 
Training: Epoch [89/120], Step [499], Loss: 0.024274267256259918, Training Accuracy: 97.39999999999999
[ Thu Jul  4 09:13:58 2024 ] 	Batch(500/7879) done. Loss: 0.1974  lr:0.000100
[ Thu Jul  4 09:14:16 2024 ] 	Batch(600/7879) done. Loss: 0.0628  lr:0.000100
[ Thu Jul  4 09:14:33 2024 ] 	Batch(700/7879) done. Loss: 0.2692  lr:0.000100
[ Thu Jul  4 09:14:51 2024 ] 	Batch(800/7879) done. Loss: 0.0314  lr:0.000100
[ Thu Jul  4 09:15:09 2024 ] 	Batch(900/7879) done. Loss: 0.0515  lr:0.000100
[ Thu Jul  4 09:15:27 2024 ] 
Training: Epoch [89/120], Step [999], Loss: 0.11994858831167221, Training Accuracy: 97.39999999999999
[ Thu Jul  4 09:15:27 2024 ] 	Batch(1000/7879) done. Loss: 0.0840  lr:0.000100
[ Thu Jul  4 09:15:45 2024 ] 	Batch(1100/7879) done. Loss: 0.0411  lr:0.000100
[ Thu Jul  4 09:16:03 2024 ] 	Batch(1200/7879) done. Loss: 0.0432  lr:0.000100
[ Thu Jul  4 09:16:21 2024 ] 	Batch(1300/7879) done. Loss: 0.0120  lr:0.000100
[ Thu Jul  4 09:16:39 2024 ] 	Batch(1400/7879) done. Loss: 0.3879  lr:0.000100
[ Thu Jul  4 09:16:56 2024 ] 
Training: Epoch [89/120], Step [1499], Loss: 0.028282247483730316, Training Accuracy: 97.375
[ Thu Jul  4 09:16:57 2024 ] 	Batch(1500/7879) done. Loss: 0.1196  lr:0.000100
[ Thu Jul  4 09:17:15 2024 ] 	Batch(1600/7879) done. Loss: 0.0890  lr:0.000100
[ Thu Jul  4 09:17:33 2024 ] 	Batch(1700/7879) done. Loss: 0.0053  lr:0.000100
[ Thu Jul  4 09:17:50 2024 ] 	Batch(1800/7879) done. Loss: 0.0895  lr:0.000100
[ Thu Jul  4 09:18:08 2024 ] 	Batch(1900/7879) done. Loss: 0.0054  lr:0.000100
[ Thu Jul  4 09:18:26 2024 ] 
Training: Epoch [89/120], Step [1999], Loss: 0.2567671537399292, Training Accuracy: 97.35000000000001
[ Thu Jul  4 09:18:26 2024 ] 	Batch(2000/7879) done. Loss: 0.1871  lr:0.000100
[ Thu Jul  4 09:18:44 2024 ] 	Batch(2100/7879) done. Loss: 0.4056  lr:0.000100
[ Thu Jul  4 09:19:02 2024 ] 	Batch(2200/7879) done. Loss: 0.4268  lr:0.000100
[ Thu Jul  4 09:19:20 2024 ] 	Batch(2300/7879) done. Loss: 0.0264  lr:0.000100
[ Thu Jul  4 09:19:38 2024 ] 	Batch(2400/7879) done. Loss: 0.0243  lr:0.000100
[ Thu Jul  4 09:19:56 2024 ] 
Training: Epoch [89/120], Step [2499], Loss: 0.007816880941390991, Training Accuracy: 97.375
[ Thu Jul  4 09:19:56 2024 ] 	Batch(2500/7879) done. Loss: 0.0573  lr:0.000100
[ Thu Jul  4 09:20:14 2024 ] 	Batch(2600/7879) done. Loss: 0.0173  lr:0.000100
[ Thu Jul  4 09:20:32 2024 ] 	Batch(2700/7879) done. Loss: 0.0072  lr:0.000100
[ Thu Jul  4 09:20:50 2024 ] 	Batch(2800/7879) done. Loss: 0.0196  lr:0.000100
[ Thu Jul  4 09:21:08 2024 ] 	Batch(2900/7879) done. Loss: 0.0169  lr:0.000100
[ Thu Jul  4 09:21:25 2024 ] 
Training: Epoch [89/120], Step [2999], Loss: 0.2144506871700287, Training Accuracy: 97.49583333333334
[ Thu Jul  4 09:21:26 2024 ] 	Batch(3000/7879) done. Loss: 0.0075  lr:0.000100
[ Thu Jul  4 09:21:43 2024 ] 	Batch(3100/7879) done. Loss: 0.0038  lr:0.000100
[ Thu Jul  4 09:22:02 2024 ] 	Batch(3200/7879) done. Loss: 0.1059  lr:0.000100
[ Thu Jul  4 09:22:19 2024 ] 	Batch(3300/7879) done. Loss: 0.1545  lr:0.000100
[ Thu Jul  4 09:22:37 2024 ] 	Batch(3400/7879) done. Loss: 0.0038  lr:0.000100
[ Thu Jul  4 09:22:55 2024 ] 
Training: Epoch [89/120], Step [3499], Loss: 0.003676682012155652, Training Accuracy: 97.51785714285714
[ Thu Jul  4 09:22:55 2024 ] 	Batch(3500/7879) done. Loss: 0.0537  lr:0.000100
[ Thu Jul  4 09:23:13 2024 ] 	Batch(3600/7879) done. Loss: 0.0219  lr:0.000100
[ Thu Jul  4 09:23:31 2024 ] 	Batch(3700/7879) done. Loss: 0.0780  lr:0.000100
[ Thu Jul  4 09:23:49 2024 ] 	Batch(3800/7879) done. Loss: 0.0225  lr:0.000100
[ Thu Jul  4 09:24:07 2024 ] 	Batch(3900/7879) done. Loss: 0.0368  lr:0.000100
[ Thu Jul  4 09:24:25 2024 ] 
Training: Epoch [89/120], Step [3999], Loss: 0.029748018831014633, Training Accuracy: 97.490625
[ Thu Jul  4 09:24:25 2024 ] 	Batch(4000/7879) done. Loss: 0.0704  lr:0.000100
[ Thu Jul  4 09:24:43 2024 ] 	Batch(4100/7879) done. Loss: 0.0005  lr:0.000100
[ Thu Jul  4 09:25:01 2024 ] 	Batch(4200/7879) done. Loss: 0.0265  lr:0.000100
[ Thu Jul  4 09:25:19 2024 ] 	Batch(4300/7879) done. Loss: 0.3268  lr:0.000100
[ Thu Jul  4 09:25:37 2024 ] 	Batch(4400/7879) done. Loss: 0.0093  lr:0.000100
[ Thu Jul  4 09:25:55 2024 ] 
Training: Epoch [89/120], Step [4499], Loss: 0.04221557080745697, Training Accuracy: 97.48333333333333
[ Thu Jul  4 09:25:55 2024 ] 	Batch(4500/7879) done. Loss: 0.0783  lr:0.000100
[ Thu Jul  4 09:26:14 2024 ] 	Batch(4600/7879) done. Loss: 0.0225  lr:0.000100
[ Thu Jul  4 09:26:32 2024 ] 	Batch(4700/7879) done. Loss: 0.0706  lr:0.000100
[ Thu Jul  4 09:26:50 2024 ] 	Batch(4800/7879) done. Loss: 0.1640  lr:0.000100
[ Thu Jul  4 09:27:08 2024 ] 	Batch(4900/7879) done. Loss: 0.0009  lr:0.000100
[ Thu Jul  4 09:27:26 2024 ] 
Training: Epoch [89/120], Step [4999], Loss: 0.024088267236948013, Training Accuracy: 97.5325
[ Thu Jul  4 09:27:26 2024 ] 	Batch(5000/7879) done. Loss: 0.0268  lr:0.000100
[ Thu Jul  4 09:27:44 2024 ] 	Batch(5100/7879) done. Loss: 0.0439  lr:0.000100
[ Thu Jul  4 09:28:02 2024 ] 	Batch(5200/7879) done. Loss: 0.0657  lr:0.000100
[ Thu Jul  4 09:28:20 2024 ] 	Batch(5300/7879) done. Loss: 0.0442  lr:0.000100
[ Thu Jul  4 09:28:38 2024 ] 	Batch(5400/7879) done. Loss: 0.1720  lr:0.000100
[ Thu Jul  4 09:28:55 2024 ] 
Training: Epoch [89/120], Step [5499], Loss: 0.009377846494317055, Training Accuracy: 97.50227272727273
[ Thu Jul  4 09:28:56 2024 ] 	Batch(5500/7879) done. Loss: 0.3648  lr:0.000100
[ Thu Jul  4 09:29:14 2024 ] 	Batch(5600/7879) done. Loss: 0.0046  lr:0.000100
[ Thu Jul  4 09:29:32 2024 ] 	Batch(5700/7879) done. Loss: 0.0394  lr:0.000100
[ Thu Jul  4 09:29:49 2024 ] 	Batch(5800/7879) done. Loss: 0.0148  lr:0.000100
[ Thu Jul  4 09:30:07 2024 ] 	Batch(5900/7879) done. Loss: 0.1126  lr:0.000100
[ Thu Jul  4 09:30:25 2024 ] 
Training: Epoch [89/120], Step [5999], Loss: 0.08750319480895996, Training Accuracy: 97.5
[ Thu Jul  4 09:30:25 2024 ] 	Batch(6000/7879) done. Loss: 0.0617  lr:0.000100
[ Thu Jul  4 09:30:43 2024 ] 	Batch(6100/7879) done. Loss: 0.0320  lr:0.000100
[ Thu Jul  4 09:31:01 2024 ] 	Batch(6200/7879) done. Loss: 0.0624  lr:0.000100
[ Thu Jul  4 09:31:19 2024 ] 	Batch(6300/7879) done. Loss: 0.0142  lr:0.000100
[ Thu Jul  4 09:31:37 2024 ] 	Batch(6400/7879) done. Loss: 0.0024  lr:0.000100
[ Thu Jul  4 09:31:54 2024 ] 
Training: Epoch [89/120], Step [6499], Loss: 0.11069013178348541, Training Accuracy: 97.52499999999999
[ Thu Jul  4 09:31:55 2024 ] 	Batch(6500/7879) done. Loss: 0.4086  lr:0.000100
[ Thu Jul  4 09:32:13 2024 ] 	Batch(6600/7879) done. Loss: 0.1105  lr:0.000100
[ Thu Jul  4 09:32:30 2024 ] 	Batch(6700/7879) done. Loss: 0.0552  lr:0.000100
[ Thu Jul  4 09:32:48 2024 ] 	Batch(6800/7879) done. Loss: 0.6572  lr:0.000100
[ Thu Jul  4 09:33:06 2024 ] 	Batch(6900/7879) done. Loss: 0.0761  lr:0.000100
[ Thu Jul  4 09:33:24 2024 ] 
Training: Epoch [89/120], Step [6999], Loss: 0.328696608543396, Training Accuracy: 97.52499999999999
[ Thu Jul  4 09:33:24 2024 ] 	Batch(7000/7879) done. Loss: 0.1361  lr:0.000100
[ Thu Jul  4 09:33:42 2024 ] 	Batch(7100/7879) done. Loss: 0.0113  lr:0.000100
[ Thu Jul  4 09:34:00 2024 ] 	Batch(7200/7879) done. Loss: 0.0154  lr:0.000100
[ Thu Jul  4 09:34:18 2024 ] 	Batch(7300/7879) done. Loss: 0.0735  lr:0.000100
[ Thu Jul  4 09:34:36 2024 ] 	Batch(7400/7879) done. Loss: 0.0221  lr:0.000100
[ Thu Jul  4 09:34:53 2024 ] 
Training: Epoch [89/120], Step [7499], Loss: 0.06479454785585403, Training Accuracy: 97.53500000000001
[ Thu Jul  4 09:34:53 2024 ] 	Batch(7500/7879) done. Loss: 0.1819  lr:0.000100
[ Thu Jul  4 09:35:11 2024 ] 	Batch(7600/7879) done. Loss: 0.0064  lr:0.000100
[ Thu Jul  4 09:35:29 2024 ] 	Batch(7700/7879) done. Loss: 0.2304  lr:0.000100
[ Thu Jul  4 09:35:47 2024 ] 	Batch(7800/7879) done. Loss: 0.0911  lr:0.000100
[ Thu Jul  4 09:36:01 2024 ] 	Mean training loss: 0.0962.
[ Thu Jul  4 09:36:01 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 09:36:01 2024 ] Eval epoch: 90
[ Thu Jul  4 09:40:47 2024 ] 	Mean val loss of 6365 batches: 1.686296413545518.
[ Thu Jul  4 09:40:47 2024 ] Training epoch: 91
[ Thu Jul  4 09:40:48 2024 ] 	Batch(0/7879) done. Loss: 0.0218  lr:0.000001
[ Thu Jul  4 09:41:06 2024 ] 	Batch(100/7879) done. Loss: 0.0133  lr:0.000001
[ Thu Jul  4 09:41:24 2024 ] 	Batch(200/7879) done. Loss: 0.1110  lr:0.000001
[ Thu Jul  4 09:41:43 2024 ] 	Batch(300/7879) done. Loss: 0.0445  lr:0.000001
[ Thu Jul  4 09:42:01 2024 ] 	Batch(400/7879) done. Loss: 0.0263  lr:0.000001
[ Thu Jul  4 09:42:18 2024 ] 
Training: Epoch [90/120], Step [499], Loss: 0.04909798130393028, Training Accuracy: 97.75
[ Thu Jul  4 09:42:18 2024 ] 	Batch(500/7879) done. Loss: 0.7613  lr:0.000001
[ Thu Jul  4 09:42:36 2024 ] 	Batch(600/7879) done. Loss: 0.0324  lr:0.000001
[ Thu Jul  4 09:42:54 2024 ] 	Batch(700/7879) done. Loss: 0.0354  lr:0.000001
[ Thu Jul  4 09:43:12 2024 ] 	Batch(800/7879) done. Loss: 0.0050  lr:0.000001
[ Thu Jul  4 09:43:30 2024 ] 	Batch(900/7879) done. Loss: 0.0289  lr:0.000001
[ Thu Jul  4 09:43:48 2024 ] 
Training: Epoch [90/120], Step [999], Loss: 0.00323487282730639, Training Accuracy: 97.6375
[ Thu Jul  4 09:43:48 2024 ] 	Batch(1000/7879) done. Loss: 0.0020  lr:0.000001
[ Thu Jul  4 09:44:06 2024 ] 	Batch(1100/7879) done. Loss: 0.0324  lr:0.000001
[ Thu Jul  4 09:44:24 2024 ] 	Batch(1200/7879) done. Loss: 0.0268  lr:0.000001
[ Thu Jul  4 09:44:42 2024 ] 	Batch(1300/7879) done. Loss: 0.0739  lr:0.000001
[ Thu Jul  4 09:45:01 2024 ] 	Batch(1400/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul  4 09:45:19 2024 ] 
Training: Epoch [90/120], Step [1499], Loss: 0.008187955245375633, Training Accuracy: 97.55833333333334
[ Thu Jul  4 09:45:19 2024 ] 	Batch(1500/7879) done. Loss: 0.0670  lr:0.000001
[ Thu Jul  4 09:45:38 2024 ] 	Batch(1600/7879) done. Loss: 0.2034  lr:0.000001
[ Thu Jul  4 09:45:56 2024 ] 	Batch(1700/7879) done. Loss: 0.0632  lr:0.000001
[ Thu Jul  4 09:46:14 2024 ] 	Batch(1800/7879) done. Loss: 0.0783  lr:0.000001
[ Thu Jul  4 09:46:31 2024 ] 	Batch(1900/7879) done. Loss: 0.0265  lr:0.000001
[ Thu Jul  4 09:46:49 2024 ] 
Training: Epoch [90/120], Step [1999], Loss: 0.18181787431240082, Training Accuracy: 97.53125
[ Thu Jul  4 09:46:49 2024 ] 	Batch(2000/7879) done. Loss: 0.0412  lr:0.000001
[ Thu Jul  4 09:47:08 2024 ] 	Batch(2100/7879) done. Loss: 0.0426  lr:0.000001
[ Thu Jul  4 09:47:26 2024 ] 	Batch(2200/7879) done. Loss: 0.0203  lr:0.000001
[ Thu Jul  4 09:47:45 2024 ] 	Batch(2300/7879) done. Loss: 0.0323  lr:0.000001
[ Thu Jul  4 09:48:04 2024 ] 	Batch(2400/7879) done. Loss: 0.0186  lr:0.000001
[ Thu Jul  4 09:48:21 2024 ] 
Training: Epoch [90/120], Step [2499], Loss: 0.02885974757373333, Training Accuracy: 97.54
[ Thu Jul  4 09:48:21 2024 ] 	Batch(2500/7879) done. Loss: 0.2955  lr:0.000001
[ Thu Jul  4 09:48:39 2024 ] 	Batch(2600/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul  4 09:48:57 2024 ] 	Batch(2700/7879) done. Loss: 0.0685  lr:0.000001
[ Thu Jul  4 09:49:15 2024 ] 	Batch(2800/7879) done. Loss: 0.0082  lr:0.000001
[ Thu Jul  4 09:49:34 2024 ] 	Batch(2900/7879) done. Loss: 0.2083  lr:0.000001
[ Thu Jul  4 09:49:52 2024 ] 
Training: Epoch [90/120], Step [2999], Loss: 0.2039998471736908, Training Accuracy: 97.59166666666667
[ Thu Jul  4 09:49:52 2024 ] 	Batch(3000/7879) done. Loss: 0.2240  lr:0.000001
[ Thu Jul  4 09:50:11 2024 ] 	Batch(3100/7879) done. Loss: 0.0315  lr:0.000001
[ Thu Jul  4 09:50:29 2024 ] 	Batch(3200/7879) done. Loss: 0.1140  lr:0.000001
[ Thu Jul  4 09:50:48 2024 ] 	Batch(3300/7879) done. Loss: 0.0075  lr:0.000001
[ Thu Jul  4 09:51:06 2024 ] 	Batch(3400/7879) done. Loss: 0.4509  lr:0.000001
[ Thu Jul  4 09:51:24 2024 ] 
Training: Epoch [90/120], Step [3499], Loss: 0.007039803080260754, Training Accuracy: 97.53928571428571
[ Thu Jul  4 09:51:25 2024 ] 	Batch(3500/7879) done. Loss: 0.5537  lr:0.000001
[ Thu Jul  4 09:51:43 2024 ] 	Batch(3600/7879) done. Loss: 0.0181  lr:0.000001
[ Thu Jul  4 09:52:01 2024 ] 	Batch(3700/7879) done. Loss: 0.0094  lr:0.000001
[ Thu Jul  4 09:52:18 2024 ] 	Batch(3800/7879) done. Loss: 0.0890  lr:0.000001
[ Thu Jul  4 09:52:36 2024 ] 	Batch(3900/7879) done. Loss: 0.0780  lr:0.000001
[ Thu Jul  4 09:52:54 2024 ] 
Training: Epoch [90/120], Step [3999], Loss: 0.14472129940986633, Training Accuracy: 97.54374999999999
[ Thu Jul  4 09:52:54 2024 ] 	Batch(4000/7879) done. Loss: 0.0225  lr:0.000001
[ Thu Jul  4 09:53:12 2024 ] 	Batch(4100/7879) done. Loss: 0.0958  lr:0.000001
[ Thu Jul  4 09:53:30 2024 ] 	Batch(4200/7879) done. Loss: 0.4731  lr:0.000001
[ Thu Jul  4 09:53:48 2024 ] 	Batch(4300/7879) done. Loss: 0.0116  lr:0.000001
[ Thu Jul  4 09:54:06 2024 ] 	Batch(4400/7879) done. Loss: 0.0951  lr:0.000001
[ Thu Jul  4 09:54:24 2024 ] 
Training: Epoch [90/120], Step [4499], Loss: 0.10188446938991547, Training Accuracy: 97.55555555555556
[ Thu Jul  4 09:54:24 2024 ] 	Batch(4500/7879) done. Loss: 0.0339  lr:0.000001
[ Thu Jul  4 09:54:42 2024 ] 	Batch(4600/7879) done. Loss: 0.1349  lr:0.000001
[ Thu Jul  4 09:54:59 2024 ] 	Batch(4700/7879) done. Loss: 0.3445  lr:0.000001
[ Thu Jul  4 09:55:18 2024 ] 	Batch(4800/7879) done. Loss: 0.0097  lr:0.000001
[ Thu Jul  4 09:55:35 2024 ] 	Batch(4900/7879) done. Loss: 0.3524  lr:0.000001
[ Thu Jul  4 09:55:53 2024 ] 
Training: Epoch [90/120], Step [4999], Loss: 0.17669248580932617, Training Accuracy: 97.48750000000001
[ Thu Jul  4 09:55:53 2024 ] 	Batch(5000/7879) done. Loss: 0.0569  lr:0.000001
[ Thu Jul  4 09:56:11 2024 ] 	Batch(5100/7879) done. Loss: 0.0073  lr:0.000001
[ Thu Jul  4 09:56:29 2024 ] 	Batch(5200/7879) done. Loss: 0.0719  lr:0.000001
[ Thu Jul  4 09:56:47 2024 ] 	Batch(5300/7879) done. Loss: 0.0394  lr:0.000001
[ Thu Jul  4 09:57:05 2024 ] 	Batch(5400/7879) done. Loss: 0.0107  lr:0.000001
[ Thu Jul  4 09:57:22 2024 ] 
Training: Epoch [90/120], Step [5499], Loss: 0.6457635164260864, Training Accuracy: 97.5340909090909
[ Thu Jul  4 09:57:23 2024 ] 	Batch(5500/7879) done. Loss: 0.0034  lr:0.000001
[ Thu Jul  4 09:57:41 2024 ] 	Batch(5600/7879) done. Loss: 0.0184  lr:0.000001
[ Thu Jul  4 09:57:58 2024 ] 	Batch(5700/7879) done. Loss: 0.0169  lr:0.000001
[ Thu Jul  4 09:58:16 2024 ] 	Batch(5800/7879) done. Loss: 0.0186  lr:0.000001
[ Thu Jul  4 09:58:34 2024 ] 	Batch(5900/7879) done. Loss: 0.0939  lr:0.000001
[ Thu Jul  4 09:58:52 2024 ] 
Training: Epoch [90/120], Step [5999], Loss: 0.06056016683578491, Training Accuracy: 97.51875
[ Thu Jul  4 09:58:52 2024 ] 	Batch(6000/7879) done. Loss: 0.1886  lr:0.000001
[ Thu Jul  4 09:59:10 2024 ] 	Batch(6100/7879) done. Loss: 0.1424  lr:0.000001
[ Thu Jul  4 09:59:28 2024 ] 	Batch(6200/7879) done. Loss: 0.0232  lr:0.000001
[ Thu Jul  4 09:59:46 2024 ] 	Batch(6300/7879) done. Loss: 0.0088  lr:0.000001
[ Thu Jul  4 10:00:04 2024 ] 	Batch(6400/7879) done. Loss: 0.0616  lr:0.000001
[ Thu Jul  4 10:00:21 2024 ] 
Training: Epoch [90/120], Step [6499], Loss: 0.00687845004722476, Training Accuracy: 97.53076923076924
[ Thu Jul  4 10:00:21 2024 ] 	Batch(6500/7879) done. Loss: 0.0013  lr:0.000001
[ Thu Jul  4 10:00:39 2024 ] 	Batch(6600/7879) done. Loss: 0.2018  lr:0.000001
[ Thu Jul  4 10:00:57 2024 ] 	Batch(6700/7879) done. Loss: 0.0353  lr:0.000001
[ Thu Jul  4 10:01:15 2024 ] 	Batch(6800/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul  4 10:01:33 2024 ] 	Batch(6900/7879) done. Loss: 0.0403  lr:0.000001
[ Thu Jul  4 10:01:51 2024 ] 
Training: Epoch [90/120], Step [6999], Loss: 0.29566633701324463, Training Accuracy: 97.54642857142856
[ Thu Jul  4 10:01:51 2024 ] 	Batch(7000/7879) done. Loss: 0.0546  lr:0.000001
[ Thu Jul  4 10:02:09 2024 ] 	Batch(7100/7879) done. Loss: 0.0057  lr:0.000001
[ Thu Jul  4 10:02:27 2024 ] 	Batch(7200/7879) done. Loss: 0.2203  lr:0.000001
[ Thu Jul  4 10:02:45 2024 ] 	Batch(7300/7879) done. Loss: 0.0225  lr:0.000001
[ Thu Jul  4 10:03:03 2024 ] 	Batch(7400/7879) done. Loss: 0.0737  lr:0.000001
[ Thu Jul  4 10:03:20 2024 ] 
Training: Epoch [90/120], Step [7499], Loss: 0.06935682892799377, Training Accuracy: 97.545
[ Thu Jul  4 10:03:21 2024 ] 	Batch(7500/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul  4 10:03:39 2024 ] 	Batch(7600/7879) done. Loss: 0.0121  lr:0.000001
[ Thu Jul  4 10:03:57 2024 ] 	Batch(7700/7879) done. Loss: 0.0197  lr:0.000001
[ Thu Jul  4 10:04:14 2024 ] 	Batch(7800/7879) done. Loss: 0.2995  lr:0.000001
[ Thu Jul  4 10:04:28 2024 ] 	Mean training loss: 0.0934.
[ Thu Jul  4 10:04:28 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 10:04:29 2024 ] Training epoch: 92
[ Thu Jul  4 10:04:29 2024 ] 	Batch(0/7879) done. Loss: 0.0481  lr:0.000001
[ Thu Jul  4 10:04:48 2024 ] 	Batch(100/7879) done. Loss: 0.0241  lr:0.000001
[ Thu Jul  4 10:05:06 2024 ] 	Batch(200/7879) done. Loss: 0.0311  lr:0.000001
[ Thu Jul  4 10:05:25 2024 ] 	Batch(300/7879) done. Loss: 0.0187  lr:0.000001
[ Thu Jul  4 10:05:44 2024 ] 	Batch(400/7879) done. Loss: 0.0310  lr:0.000001
[ Thu Jul  4 10:06:02 2024 ] 
Training: Epoch [91/120], Step [499], Loss: 0.03196761757135391, Training Accuracy: 97.89999999999999
[ Thu Jul  4 10:06:02 2024 ] 	Batch(500/7879) done. Loss: 0.0396  lr:0.000001
[ Thu Jul  4 10:06:21 2024 ] 	Batch(600/7879) done. Loss: 0.0606  lr:0.000001
[ Thu Jul  4 10:06:39 2024 ] 	Batch(700/7879) done. Loss: 0.4766  lr:0.000001
[ Thu Jul  4 10:06:58 2024 ] 	Batch(800/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul  4 10:07:16 2024 ] 	Batch(900/7879) done. Loss: 0.0170  lr:0.000001
[ Thu Jul  4 10:07:34 2024 ] 
Training: Epoch [91/120], Step [999], Loss: 0.09031198173761368, Training Accuracy: 97.8375
[ Thu Jul  4 10:07:34 2024 ] 	Batch(1000/7879) done. Loss: 0.2472  lr:0.000001
[ Thu Jul  4 10:07:52 2024 ] 	Batch(1100/7879) done. Loss: 0.0091  lr:0.000001
[ Thu Jul  4 10:08:10 2024 ] 	Batch(1200/7879) done. Loss: 0.0084  lr:0.000001
[ Thu Jul  4 10:08:28 2024 ] 	Batch(1300/7879) done. Loss: 0.0270  lr:0.000001
[ Thu Jul  4 10:08:46 2024 ] 	Batch(1400/7879) done. Loss: 0.0156  lr:0.000001
[ Thu Jul  4 10:09:03 2024 ] 
Training: Epoch [91/120], Step [1499], Loss: 0.0240929014980793, Training Accuracy: 97.78333333333333
[ Thu Jul  4 10:09:04 2024 ] 	Batch(1500/7879) done. Loss: 0.0096  lr:0.000001
[ Thu Jul  4 10:09:21 2024 ] 	Batch(1600/7879) done. Loss: 0.0432  lr:0.000001
[ Thu Jul  4 10:09:39 2024 ] 	Batch(1700/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul  4 10:09:57 2024 ] 	Batch(1800/7879) done. Loss: 0.0841  lr:0.000001
[ Thu Jul  4 10:10:15 2024 ] 	Batch(1900/7879) done. Loss: 0.0336  lr:0.000001
[ Thu Jul  4 10:10:33 2024 ] 
Training: Epoch [91/120], Step [1999], Loss: 0.04858740046620369, Training Accuracy: 97.7
[ Thu Jul  4 10:10:33 2024 ] 	Batch(2000/7879) done. Loss: 0.0141  lr:0.000001
[ Thu Jul  4 10:10:51 2024 ] 	Batch(2100/7879) done. Loss: 0.0312  lr:0.000001
[ Thu Jul  4 10:11:09 2024 ] 	Batch(2200/7879) done. Loss: 0.0926  lr:0.000001
[ Thu Jul  4 10:11:27 2024 ] 	Batch(2300/7879) done. Loss: 0.2385  lr:0.000001
[ Thu Jul  4 10:11:45 2024 ] 	Batch(2400/7879) done. Loss: 0.0009  lr:0.000001
[ Thu Jul  4 10:12:02 2024 ] 
Training: Epoch [91/120], Step [2499], Loss: 0.26931148767471313, Training Accuracy: 97.64500000000001
[ Thu Jul  4 10:12:02 2024 ] 	Batch(2500/7879) done. Loss: 0.0019  lr:0.000001
[ Thu Jul  4 10:12:20 2024 ] 	Batch(2600/7879) done. Loss: 0.2116  lr:0.000001
[ Thu Jul  4 10:12:38 2024 ] 	Batch(2700/7879) done. Loss: 0.0601  lr:0.000001
[ Thu Jul  4 10:12:56 2024 ] 	Batch(2800/7879) done. Loss: 0.0159  lr:0.000001
[ Thu Jul  4 10:13:14 2024 ] 	Batch(2900/7879) done. Loss: 0.0203  lr:0.000001
[ Thu Jul  4 10:13:32 2024 ] 
Training: Epoch [91/120], Step [2999], Loss: 0.007986686192452908, Training Accuracy: 97.59166666666667
[ Thu Jul  4 10:13:32 2024 ] 	Batch(3000/7879) done. Loss: 0.1811  lr:0.000001
[ Thu Jul  4 10:13:50 2024 ] 	Batch(3100/7879) done. Loss: 0.0059  lr:0.000001
[ Thu Jul  4 10:14:08 2024 ] 	Batch(3200/7879) done. Loss: 0.0656  lr:0.000001
[ Thu Jul  4 10:14:26 2024 ] 	Batch(3300/7879) done. Loss: 0.0309  lr:0.000001
[ Thu Jul  4 10:14:44 2024 ] 	Batch(3400/7879) done. Loss: 0.1206  lr:0.000001
[ Thu Jul  4 10:15:02 2024 ] 
Training: Epoch [91/120], Step [3499], Loss: 0.022171197459101677, Training Accuracy: 97.61071428571428
[ Thu Jul  4 10:15:02 2024 ] 	Batch(3500/7879) done. Loss: 0.0504  lr:0.000001
[ Thu Jul  4 10:15:20 2024 ] 	Batch(3600/7879) done. Loss: 0.1356  lr:0.000001
[ Thu Jul  4 10:15:39 2024 ] 	Batch(3700/7879) done. Loss: 0.0441  lr:0.000001
[ Thu Jul  4 10:15:57 2024 ] 	Batch(3800/7879) done. Loss: 0.1042  lr:0.000001
[ Thu Jul  4 10:16:16 2024 ] 	Batch(3900/7879) done. Loss: 0.0075  lr:0.000001
[ Thu Jul  4 10:16:34 2024 ] 
Training: Epoch [91/120], Step [3999], Loss: 0.04692947119474411, Training Accuracy: 97.640625
[ Thu Jul  4 10:16:34 2024 ] 	Batch(4000/7879) done. Loss: 0.0759  lr:0.000001
[ Thu Jul  4 10:16:52 2024 ] 	Batch(4100/7879) done. Loss: 0.0364  lr:0.000001
[ Thu Jul  4 10:17:10 2024 ] 	Batch(4200/7879) done. Loss: 0.7021  lr:0.000001
[ Thu Jul  4 10:17:28 2024 ] 	Batch(4300/7879) done. Loss: 0.0155  lr:0.000001
[ Thu Jul  4 10:17:46 2024 ] 	Batch(4400/7879) done. Loss: 0.5387  lr:0.000001
[ Thu Jul  4 10:18:03 2024 ] 
Training: Epoch [91/120], Step [4499], Loss: 0.16824322938919067, Training Accuracy: 97.60555555555555
[ Thu Jul  4 10:18:04 2024 ] 	Batch(4500/7879) done. Loss: 0.0065  lr:0.000001
[ Thu Jul  4 10:18:22 2024 ] 	Batch(4600/7879) done. Loss: 0.0193  lr:0.000001
[ Thu Jul  4 10:18:39 2024 ] 	Batch(4700/7879) done. Loss: 0.1806  lr:0.000001
[ Thu Jul  4 10:18:57 2024 ] 	Batch(4800/7879) done. Loss: 0.0785  lr:0.000001
[ Thu Jul  4 10:19:15 2024 ] 	Batch(4900/7879) done. Loss: 0.0355  lr:0.000001
[ Thu Jul  4 10:19:33 2024 ] 
Training: Epoch [91/120], Step [4999], Loss: 0.23314061760902405, Training Accuracy: 97.6025
[ Thu Jul  4 10:19:33 2024 ] 	Batch(5000/7879) done. Loss: 0.0109  lr:0.000001
[ Thu Jul  4 10:19:51 2024 ] 	Batch(5100/7879) done. Loss: 0.0217  lr:0.000001
[ Thu Jul  4 10:20:09 2024 ] 	Batch(5200/7879) done. Loss: 0.2387  lr:0.000001
[ Thu Jul  4 10:20:27 2024 ] 	Batch(5300/7879) done. Loss: 0.1515  lr:0.000001
[ Thu Jul  4 10:20:46 2024 ] 	Batch(5400/7879) done. Loss: 0.1105  lr:0.000001
[ Thu Jul  4 10:21:04 2024 ] 
Training: Epoch [91/120], Step [5499], Loss: 0.010966314002871513, Training Accuracy: 97.63636363636363
[ Thu Jul  4 10:21:04 2024 ] 	Batch(5500/7879) done. Loss: 0.0464  lr:0.000001
[ Thu Jul  4 10:21:23 2024 ] 	Batch(5600/7879) done. Loss: 0.4038  lr:0.000001
[ Thu Jul  4 10:21:41 2024 ] 	Batch(5700/7879) done. Loss: 0.1530  lr:0.000001
[ Thu Jul  4 10:22:00 2024 ] 	Batch(5800/7879) done. Loss: 0.0022  lr:0.000001
[ Thu Jul  4 10:22:18 2024 ] 	Batch(5900/7879) done. Loss: 0.0053  lr:0.000001
[ Thu Jul  4 10:22:37 2024 ] 
Training: Epoch [91/120], Step [5999], Loss: 0.0037539927288889885, Training Accuracy: 97.61875
[ Thu Jul  4 10:22:37 2024 ] 	Batch(6000/7879) done. Loss: 0.0879  lr:0.000001
[ Thu Jul  4 10:22:55 2024 ] 	Batch(6100/7879) done. Loss: 0.0935  lr:0.000001
[ Thu Jul  4 10:23:13 2024 ] 	Batch(6200/7879) done. Loss: 0.1816  lr:0.000001
[ Thu Jul  4 10:23:30 2024 ] 	Batch(6300/7879) done. Loss: 0.0724  lr:0.000001
[ Thu Jul  4 10:23:48 2024 ] 	Batch(6400/7879) done. Loss: 0.7324  lr:0.000001
[ Thu Jul  4 10:24:06 2024 ] 
Training: Epoch [91/120], Step [6499], Loss: 0.15527275204658508, Training Accuracy: 97.61730769230769
[ Thu Jul  4 10:24:06 2024 ] 	Batch(6500/7879) done. Loss: 0.0042  lr:0.000001
[ Thu Jul  4 10:24:24 2024 ] 	Batch(6600/7879) done. Loss: 0.4099  lr:0.000001
[ Thu Jul  4 10:24:42 2024 ] 	Batch(6700/7879) done. Loss: 0.0996  lr:0.000001
[ Thu Jul  4 10:25:00 2024 ] 	Batch(6800/7879) done. Loss: 0.1359  lr:0.000001
[ Thu Jul  4 10:25:19 2024 ] 	Batch(6900/7879) done. Loss: 0.0526  lr:0.000001
[ Thu Jul  4 10:25:37 2024 ] 
Training: Epoch [91/120], Step [6999], Loss: 0.03887240216135979, Training Accuracy: 97.62857142857143
[ Thu Jul  4 10:25:37 2024 ] 	Batch(7000/7879) done. Loss: 0.0746  lr:0.000001
[ Thu Jul  4 10:25:56 2024 ] 	Batch(7100/7879) done. Loss: 0.0967  lr:0.000001
[ Thu Jul  4 10:26:14 2024 ] 	Batch(7200/7879) done. Loss: 0.0256  lr:0.000001
[ Thu Jul  4 10:26:32 2024 ] 	Batch(7300/7879) done. Loss: 0.0143  lr:0.000001
[ Thu Jul  4 10:26:50 2024 ] 	Batch(7400/7879) done. Loss: 0.2056  lr:0.000001
[ Thu Jul  4 10:27:08 2024 ] 
Training: Epoch [91/120], Step [7499], Loss: 0.016518495976924896, Training Accuracy: 97.655
[ Thu Jul  4 10:27:08 2024 ] 	Batch(7500/7879) done. Loss: 0.0144  lr:0.000001
[ Thu Jul  4 10:27:26 2024 ] 	Batch(7600/7879) done. Loss: 0.0523  lr:0.000001
[ Thu Jul  4 10:27:44 2024 ] 	Batch(7700/7879) done. Loss: 0.1369  lr:0.000001
[ Thu Jul  4 10:28:02 2024 ] 	Batch(7800/7879) done. Loss: 0.0491  lr:0.000001
[ Thu Jul  4 10:28:16 2024 ] 	Mean training loss: 0.0946.
[ Thu Jul  4 10:28:16 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 10:28:16 2024 ] Training epoch: 93
[ Thu Jul  4 10:28:16 2024 ] 	Batch(0/7879) done. Loss: 0.0319  lr:0.000001
[ Thu Jul  4 10:28:35 2024 ] 	Batch(100/7879) done. Loss: 0.0346  lr:0.000001
[ Thu Jul  4 10:28:53 2024 ] 	Batch(200/7879) done. Loss: 0.5405  lr:0.000001
[ Thu Jul  4 10:29:11 2024 ] 	Batch(300/7879) done. Loss: 0.0199  lr:0.000001
[ Thu Jul  4 10:29:29 2024 ] 	Batch(400/7879) done. Loss: 0.0385  lr:0.000001
[ Thu Jul  4 10:29:48 2024 ] 
Training: Epoch [92/120], Step [499], Loss: 0.060813188552856445, Training Accuracy: 97.8
[ Thu Jul  4 10:29:48 2024 ] 	Batch(500/7879) done. Loss: 0.0827  lr:0.000001
[ Thu Jul  4 10:30:06 2024 ] 	Batch(600/7879) done. Loss: 0.0435  lr:0.000001
[ Thu Jul  4 10:30:24 2024 ] 	Batch(700/7879) done. Loss: 0.1125  lr:0.000001
[ Thu Jul  4 10:30:43 2024 ] 	Batch(800/7879) done. Loss: 0.0559  lr:0.000001
[ Thu Jul  4 10:31:01 2024 ] 	Batch(900/7879) done. Loss: 0.0431  lr:0.000001
[ Thu Jul  4 10:31:19 2024 ] 
Training: Epoch [92/120], Step [999], Loss: 0.08027524501085281, Training Accuracy: 97.55
[ Thu Jul  4 10:31:19 2024 ] 	Batch(1000/7879) done. Loss: 0.0295  lr:0.000001
[ Thu Jul  4 10:31:38 2024 ] 	Batch(1100/7879) done. Loss: 0.0396  lr:0.000001
[ Thu Jul  4 10:31:56 2024 ] 	Batch(1200/7879) done. Loss: 0.1251  lr:0.000001
[ Thu Jul  4 10:32:14 2024 ] 	Batch(1300/7879) done. Loss: 0.0394  lr:0.000001
[ Thu Jul  4 10:32:33 2024 ] 	Batch(1400/7879) done. Loss: 0.0022  lr:0.000001
[ Thu Jul  4 10:32:51 2024 ] 
Training: Epoch [92/120], Step [1499], Loss: 0.04877225682139397, Training Accuracy: 97.56666666666666
[ Thu Jul  4 10:32:51 2024 ] 	Batch(1500/7879) done. Loss: 0.0231  lr:0.000001
[ Thu Jul  4 10:33:09 2024 ] 	Batch(1600/7879) done. Loss: 0.3789  lr:0.000001
[ Thu Jul  4 10:33:28 2024 ] 	Batch(1700/7879) done. Loss: 0.4645  lr:0.000001
[ Thu Jul  4 10:33:46 2024 ] 	Batch(1800/7879) done. Loss: 0.0310  lr:0.000001
[ Thu Jul  4 10:34:05 2024 ] 	Batch(1900/7879) done. Loss: 0.0079  lr:0.000001
[ Thu Jul  4 10:34:23 2024 ] 
Training: Epoch [92/120], Step [1999], Loss: 0.30696526169776917, Training Accuracy: 97.6375
[ Thu Jul  4 10:34:24 2024 ] 	Batch(2000/7879) done. Loss: 0.0554  lr:0.000001
[ Thu Jul  4 10:34:42 2024 ] 	Batch(2100/7879) done. Loss: 0.0384  lr:0.000001
[ Thu Jul  4 10:35:01 2024 ] 	Batch(2200/7879) done. Loss: 0.1698  lr:0.000001
[ Thu Jul  4 10:35:19 2024 ] 	Batch(2300/7879) done. Loss: 0.0355  lr:0.000001
[ Thu Jul  4 10:35:38 2024 ] 	Batch(2400/7879) done. Loss: 0.0093  lr:0.000001
[ Thu Jul  4 10:35:56 2024 ] 
Training: Epoch [92/120], Step [2499], Loss: 0.12572386860847473, Training Accuracy: 97.63
[ Thu Jul  4 10:35:56 2024 ] 	Batch(2500/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul  4 10:36:15 2024 ] 	Batch(2600/7879) done. Loss: 0.0080  lr:0.000001
[ Thu Jul  4 10:36:33 2024 ] 	Batch(2700/7879) done. Loss: 0.0438  lr:0.000001
[ Thu Jul  4 10:36:52 2024 ] 	Batch(2800/7879) done. Loss: 0.0129  lr:0.000001
[ Thu Jul  4 10:37:10 2024 ] 	Batch(2900/7879) done. Loss: 0.0255  lr:0.000001
[ Thu Jul  4 10:37:29 2024 ] 
Training: Epoch [92/120], Step [2999], Loss: 0.002857424784451723, Training Accuracy: 97.59583333333333
[ Thu Jul  4 10:37:29 2024 ] 	Batch(3000/7879) done. Loss: 0.0044  lr:0.000001
[ Thu Jul  4 10:37:47 2024 ] 	Batch(3100/7879) done. Loss: 0.0349  lr:0.000001
[ Thu Jul  4 10:38:06 2024 ] 	Batch(3200/7879) done. Loss: 0.0714  lr:0.000001
[ Thu Jul  4 10:38:24 2024 ] 	Batch(3300/7879) done. Loss: 0.0545  lr:0.000001
[ Thu Jul  4 10:38:42 2024 ] 	Batch(3400/7879) done. Loss: 0.5800  lr:0.000001
[ Thu Jul  4 10:38:59 2024 ] 
Training: Epoch [92/120], Step [3499], Loss: 0.0025863549672067165, Training Accuracy: 97.55357142857143
[ Thu Jul  4 10:39:00 2024 ] 	Batch(3500/7879) done. Loss: 0.2403  lr:0.000001
[ Thu Jul  4 10:39:18 2024 ] 	Batch(3600/7879) done. Loss: 0.0467  lr:0.000001
[ Thu Jul  4 10:39:35 2024 ] 	Batch(3700/7879) done. Loss: 0.0500  lr:0.000001
[ Thu Jul  4 10:39:53 2024 ] 	Batch(3800/7879) done. Loss: 0.0247  lr:0.000001
[ Thu Jul  4 10:40:11 2024 ] 	Batch(3900/7879) done. Loss: 0.0195  lr:0.000001
[ Thu Jul  4 10:40:29 2024 ] 
Training: Epoch [92/120], Step [3999], Loss: 0.0073332637548446655, Training Accuracy: 97.534375
[ Thu Jul  4 10:40:29 2024 ] 	Batch(4000/7879) done. Loss: 0.0489  lr:0.000001
[ Thu Jul  4 10:40:47 2024 ] 	Batch(4100/7879) done. Loss: 0.3854  lr:0.000001
[ Thu Jul  4 10:41:05 2024 ] 	Batch(4200/7879) done. Loss: 0.1154  lr:0.000001
[ Thu Jul  4 10:41:23 2024 ] 	Batch(4300/7879) done. Loss: 0.0029  lr:0.000001
[ Thu Jul  4 10:41:41 2024 ] 	Batch(4400/7879) done. Loss: 0.0052  lr:0.000001
[ Thu Jul  4 10:41:59 2024 ] 
Training: Epoch [92/120], Step [4499], Loss: 0.23969928920269012, Training Accuracy: 97.53055555555555
[ Thu Jul  4 10:41:59 2024 ] 	Batch(4500/7879) done. Loss: 0.0444  lr:0.000001
[ Thu Jul  4 10:42:17 2024 ] 	Batch(4600/7879) done. Loss: 0.0084  lr:0.000001
[ Thu Jul  4 10:42:35 2024 ] 	Batch(4700/7879) done. Loss: 0.0222  lr:0.000001
[ Thu Jul  4 10:42:53 2024 ] 	Batch(4800/7879) done. Loss: 0.0079  lr:0.000001
[ Thu Jul  4 10:43:11 2024 ] 	Batch(4900/7879) done. Loss: 0.2929  lr:0.000001
[ Thu Jul  4 10:43:29 2024 ] 
Training: Epoch [92/120], Step [4999], Loss: 0.14876651763916016, Training Accuracy: 97.5125
[ Thu Jul  4 10:43:30 2024 ] 	Batch(5000/7879) done. Loss: 0.1359  lr:0.000001
[ Thu Jul  4 10:43:48 2024 ] 	Batch(5100/7879) done. Loss: 0.0195  lr:0.000001
[ Thu Jul  4 10:44:07 2024 ] 	Batch(5200/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul  4 10:44:25 2024 ] 	Batch(5300/7879) done. Loss: 0.0978  lr:0.000001
[ Thu Jul  4 10:44:44 2024 ] 	Batch(5400/7879) done. Loss: 0.0673  lr:0.000001
[ Thu Jul  4 10:45:02 2024 ] 
Training: Epoch [92/120], Step [5499], Loss: 0.09013041108846664, Training Accuracy: 97.55909090909091
[ Thu Jul  4 10:45:02 2024 ] 	Batch(5500/7879) done. Loss: 0.0457  lr:0.000001
[ Thu Jul  4 10:45:21 2024 ] 	Batch(5600/7879) done. Loss: 0.2634  lr:0.000001
[ Thu Jul  4 10:45:40 2024 ] 	Batch(5700/7879) done. Loss: 0.0602  lr:0.000001
[ Thu Jul  4 10:45:58 2024 ] 	Batch(5800/7879) done. Loss: 0.0024  lr:0.000001
[ Thu Jul  4 10:46:17 2024 ] 	Batch(5900/7879) done. Loss: 0.0254  lr:0.000001
[ Thu Jul  4 10:46:35 2024 ] 
Training: Epoch [92/120], Step [5999], Loss: 0.0036457485985010862, Training Accuracy: 97.58541666666667
[ Thu Jul  4 10:46:35 2024 ] 	Batch(6000/7879) done. Loss: 0.0237  lr:0.000001
[ Thu Jul  4 10:46:53 2024 ] 	Batch(6100/7879) done. Loss: 0.0036  lr:0.000001
[ Thu Jul  4 10:47:11 2024 ] 	Batch(6200/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul  4 10:47:29 2024 ] 	Batch(6300/7879) done. Loss: 0.0545  lr:0.000001
[ Thu Jul  4 10:47:47 2024 ] 	Batch(6400/7879) done. Loss: 0.4597  lr:0.000001
[ Thu Jul  4 10:48:05 2024 ] 
Training: Epoch [92/120], Step [6499], Loss: 0.02851026877760887, Training Accuracy: 97.61730769230769
[ Thu Jul  4 10:48:05 2024 ] 	Batch(6500/7879) done. Loss: 0.1354  lr:0.000001
[ Thu Jul  4 10:48:23 2024 ] 	Batch(6600/7879) done. Loss: 0.1440  lr:0.000001
[ Thu Jul  4 10:48:41 2024 ] 	Batch(6700/7879) done. Loss: 0.0417  lr:0.000001
[ Thu Jul  4 10:48:59 2024 ] 	Batch(6800/7879) done. Loss: 0.0643  lr:0.000001
[ Thu Jul  4 10:49:17 2024 ] 	Batch(6900/7879) done. Loss: 0.0497  lr:0.000001
[ Thu Jul  4 10:49:34 2024 ] 
Training: Epoch [92/120], Step [6999], Loss: 0.03818075358867645, Training Accuracy: 97.59642857142858
[ Thu Jul  4 10:49:35 2024 ] 	Batch(7000/7879) done. Loss: 0.0064  lr:0.000001
[ Thu Jul  4 10:49:52 2024 ] 	Batch(7100/7879) done. Loss: 0.0108  lr:0.000001
[ Thu Jul  4 10:50:10 2024 ] 	Batch(7200/7879) done. Loss: 0.0273  lr:0.000001
[ Thu Jul  4 10:50:29 2024 ] 	Batch(7300/7879) done. Loss: 0.0143  lr:0.000001
[ Thu Jul  4 10:50:47 2024 ] 	Batch(7400/7879) done. Loss: 0.5285  lr:0.000001
[ Thu Jul  4 10:51:06 2024 ] 
Training: Epoch [92/120], Step [7499], Loss: 0.05659903585910797, Training Accuracy: 97.59833333333333
[ Thu Jul  4 10:51:06 2024 ] 	Batch(7500/7879) done. Loss: 0.0022  lr:0.000001
[ Thu Jul  4 10:51:25 2024 ] 	Batch(7600/7879) done. Loss: 0.0314  lr:0.000001
[ Thu Jul  4 10:51:43 2024 ] 	Batch(7700/7879) done. Loss: 0.1400  lr:0.000001
[ Thu Jul  4 10:52:02 2024 ] 	Batch(7800/7879) done. Loss: 0.0191  lr:0.000001
[ Thu Jul  4 10:52:16 2024 ] 	Mean training loss: 0.0917.
[ Thu Jul  4 10:52:16 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul  4 10:52:16 2024 ] Training epoch: 94
[ Thu Jul  4 10:52:17 2024 ] 	Batch(0/7879) done. Loss: 0.0288  lr:0.000001
[ Thu Jul  4 10:52:35 2024 ] 	Batch(100/7879) done. Loss: 0.2428  lr:0.000001
[ Thu Jul  4 10:52:53 2024 ] 	Batch(200/7879) done. Loss: 0.0201  lr:0.000001
[ Thu Jul  4 10:53:11 2024 ] 	Batch(300/7879) done. Loss: 0.0023  lr:0.000001
[ Thu Jul  4 10:53:29 2024 ] 	Batch(400/7879) done. Loss: 0.1303  lr:0.000001
[ Thu Jul  4 10:53:47 2024 ] 
Training: Epoch [93/120], Step [499], Loss: 0.21135768294334412, Training Accuracy: 97.39999999999999
[ Thu Jul  4 10:53:47 2024 ] 	Batch(500/7879) done. Loss: 0.0946  lr:0.000001
[ Thu Jul  4 10:54:05 2024 ] 	Batch(600/7879) done. Loss: 0.2162  lr:0.000001
[ Thu Jul  4 10:54:22 2024 ] 	Batch(700/7879) done. Loss: 0.0100  lr:0.000001
[ Thu Jul  4 10:54:40 2024 ] 	Batch(800/7879) done. Loss: 0.0228  lr:0.000001
[ Thu Jul  4 10:54:58 2024 ] 	Batch(900/7879) done. Loss: 0.1048  lr:0.000001
[ Thu Jul  4 10:55:16 2024 ] 
Training: Epoch [93/120], Step [999], Loss: 0.009349836967885494, Training Accuracy: 97.6125
[ Thu Jul  4 10:55:16 2024 ] 	Batch(1000/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul  4 10:55:34 2024 ] 	Batch(1100/7879) done. Loss: 0.1201  lr:0.000001
[ Thu Jul  4 10:55:52 2024 ] 	Batch(1200/7879) done. Loss: 0.0144  lr:0.000001
[ Thu Jul  4 10:56:11 2024 ] 	Batch(1300/7879) done. Loss: 0.0642  lr:0.000001
[ Thu Jul  4 10:56:29 2024 ] 	Batch(1400/7879) done. Loss: 0.0571  lr:0.000001
[ Thu Jul  4 10:56:48 2024 ] 
Training: Epoch [93/120], Step [1499], Loss: 0.027555029839277267, Training Accuracy: 97.6
[ Thu Jul  4 10:56:48 2024 ] 	Batch(1500/7879) done. Loss: 0.0531  lr:0.000001
[ Thu Jul  4 10:57:07 2024 ] 	Batch(1600/7879) done. Loss: 0.0197  lr:0.000001
[ Thu Jul  4 10:57:25 2024 ] 	Batch(1700/7879) done. Loss: 0.0437  lr:0.000001
[ Thu Jul  4 10:57:44 2024 ] 	Batch(1800/7879) done. Loss: 0.0986  lr:0.000001
[ Thu Jul  4 10:58:02 2024 ] 	Batch(1900/7879) done. Loss: 0.0430  lr:0.000001
[ Thu Jul  4 10:58:20 2024 ] 
Training: Epoch [93/120], Step [1999], Loss: 0.06539475917816162, Training Accuracy: 97.61875
[ Thu Jul  4 10:58:21 2024 ] 	Batch(2000/7879) done. Loss: 0.0158  lr:0.000001
[ Thu Jul  4 10:58:39 2024 ] 	Batch(2100/7879) done. Loss: 0.0726  lr:0.000001
[ Thu Jul  4 10:58:58 2024 ] 	Batch(2200/7879) done. Loss: 0.1292  lr:0.000001
[ Thu Jul  4 10:59:16 2024 ] 	Batch(2300/7879) done. Loss: 0.3307  lr:0.000001
[ Thu Jul  4 10:59:34 2024 ] 	Batch(2400/7879) done. Loss: 0.0665  lr:0.000001
[ Thu Jul  4 10:59:52 2024 ] 
Training: Epoch [93/120], Step [2499], Loss: 0.01572626829147339, Training Accuracy: 97.64
[ Thu Jul  4 10:59:52 2024 ] 	Batch(2500/7879) done. Loss: 0.1673  lr:0.000001
[ Thu Jul  4 11:00:10 2024 ] 	Batch(2600/7879) done. Loss: 0.0605  lr:0.000001
[ Thu Jul  4 11:00:28 2024 ] 	Batch(2700/7879) done. Loss: 0.0149  lr:0.000001
[ Thu Jul  4 11:00:46 2024 ] 	Batch(2800/7879) done. Loss: 0.0107  lr:0.000001
[ Thu Jul  4 11:01:03 2024 ] 	Batch(2900/7879) done. Loss: 0.0311  lr:0.000001
[ Thu Jul  4 11:01:21 2024 ] 
Training: Epoch [93/120], Step [2999], Loss: 0.19094601273536682, Training Accuracy: 97.64166666666667
[ Thu Jul  4 11:01:21 2024 ] 	Batch(3000/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul  4 11:01:39 2024 ] 	Batch(3100/7879) done. Loss: 0.1579  lr:0.000001
[ Thu Jul  4 11:01:57 2024 ] 	Batch(3200/7879) done. Loss: 0.0644  lr:0.000001
[ Thu Jul  4 11:02:15 2024 ] 	Batch(3300/7879) done. Loss: 0.0062  lr:0.000001
[ Thu Jul  4 11:02:33 2024 ] 	Batch(3400/7879) done. Loss: 0.0399  lr:0.000001
[ Thu Jul  4 11:02:51 2024 ] 
Training: Epoch [93/120], Step [3499], Loss: 0.00735272653400898, Training Accuracy: 97.62857142857143
[ Thu Jul  4 11:02:51 2024 ] 	Batch(3500/7879) done. Loss: 0.1762  lr:0.000001
[ Thu Jul  4 11:03:09 2024 ] 	Batch(3600/7879) done. Loss: 0.2662  lr:0.000001
[ Thu Jul  4 11:03:27 2024 ] 	Batch(3700/7879) done. Loss: 0.0135  lr:0.000001
[ Thu Jul  4 11:03:45 2024 ] 	Batch(3800/7879) done. Loss: 0.0027  lr:0.000001
[ Thu Jul  4 11:04:03 2024 ] 	Batch(3900/7879) done. Loss: 0.4300  lr:0.000001
[ Thu Jul  4 11:04:20 2024 ] 
Training: Epoch [93/120], Step [3999], Loss: 0.025794530287384987, Training Accuracy: 97.6375
[ Thu Jul  4 11:04:21 2024 ] 	Batch(4000/7879) done. Loss: 0.0342  lr:0.000001
[ Thu Jul  4 11:04:39 2024 ] 	Batch(4100/7879) done. Loss: 0.0864  lr:0.000001
[ Thu Jul  4 11:04:56 2024 ] 	Batch(4200/7879) done. Loss: 0.4204  lr:0.000001
[ Thu Jul  4 11:05:14 2024 ] 	Batch(4300/7879) done. Loss: 0.0586  lr:0.000001
[ Thu Jul  4 11:05:32 2024 ] 	Batch(4400/7879) done. Loss: 0.0273  lr:0.000001
[ Thu Jul  4 11:05:50 2024 ] 
Training: Epoch [93/120], Step [4499], Loss: 0.07404951751232147, Training Accuracy: 97.66111111111111
[ Thu Jul  4 11:05:50 2024 ] 	Batch(4500/7879) done. Loss: 0.0218  lr:0.000001
[ Thu Jul  4 11:06:08 2024 ] 	Batch(4600/7879) done. Loss: 0.0029  lr:0.000001
[ Thu Jul  4 11:06:26 2024 ] 	Batch(4700/7879) done. Loss: 0.0059  lr:0.000001
[ Thu Jul  4 11:06:44 2024 ] 	Batch(4800/7879) done. Loss: 0.0022  lr:0.000001
[ Thu Jul  4 11:07:03 2024 ] 	Batch(4900/7879) done. Loss: 0.0056  lr:0.000001
[ Thu Jul  4 11:07:21 2024 ] 
Training: Epoch [93/120], Step [4999], Loss: 0.0649995505809784, Training Accuracy: 97.6475
[ Thu Jul  4 11:07:21 2024 ] 	Batch(5000/7879) done. Loss: 0.4899  lr:0.000001
[ Thu Jul  4 11:07:40 2024 ] 	Batch(5100/7879) done. Loss: 0.0239  lr:0.000001
[ Thu Jul  4 11:07:58 2024 ] 	Batch(5200/7879) done. Loss: 0.0069  lr:0.000001
[ Thu Jul  4 11:08:17 2024 ] 	Batch(5300/7879) done. Loss: 0.0066  lr:0.000001
[ Thu Jul  4 11:08:35 2024 ] 	Batch(5400/7879) done. Loss: 0.2052  lr:0.000001
[ Thu Jul  4 11:08:54 2024 ] 
Training: Epoch [93/120], Step [5499], Loss: 0.022946227341890335, Training Accuracy: 97.65
[ Thu Jul  4 11:08:54 2024 ] 	Batch(5500/7879) done. Loss: 0.1289  lr:0.000001
[ Thu Jul  4 11:09:12 2024 ] 	Batch(5600/7879) done. Loss: 0.0025  lr:0.000001
[ Thu Jul  4 11:09:30 2024 ] 	Batch(5700/7879) done. Loss: 0.0209  lr:0.000001
[ Thu Jul  4 11:09:47 2024 ] 	Batch(5800/7879) done. Loss: 0.0616  lr:0.000001
[ Thu Jul  4 11:10:05 2024 ] 	Batch(5900/7879) done. Loss: 0.0355  lr:0.000001
[ Thu Jul  4 11:10:23 2024 ] 
Training: Epoch [93/120], Step [5999], Loss: 0.08637934178113937, Training Accuracy: 97.63541666666666
[ Thu Jul  4 11:10:23 2024 ] 	Batch(6000/7879) done. Loss: 0.0215  lr:0.000001
[ Thu Jul  4 11:10:41 2024 ] 	Batch(6100/7879) done. Loss: 0.0675  lr:0.000001
[ Thu Jul  4 11:10:59 2024 ] 	Batch(6200/7879) done. Loss: 0.0216  lr:0.000001
[ Thu Jul  4 11:11:17 2024 ] 	Batch(6300/7879) done. Loss: 0.0745  lr:0.000001
[ Thu Jul  4 11:11:35 2024 ] 	Batch(6400/7879) done. Loss: 0.0402  lr:0.000001
[ Thu Jul  4 11:11:53 2024 ] 
Training: Epoch [93/120], Step [6499], Loss: 0.3725781738758087, Training Accuracy: 97.64038461538462
[ Thu Jul  4 11:11:53 2024 ] 	Batch(6500/7879) done. Loss: 0.1394  lr:0.000001
[ Thu Jul  4 11:12:11 2024 ] 	Batch(6600/7879) done. Loss: 0.0134  lr:0.000001
[ Thu Jul  4 11:12:29 2024 ] 	Batch(6700/7879) done. Loss: 0.0743  lr:0.000001
[ Thu Jul  4 11:12:48 2024 ] 	Batch(6800/7879) done. Loss: 0.0510  lr:0.000001
[ Thu Jul  4 11:13:06 2024 ] 	Batch(6900/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul  4 11:13:25 2024 ] 
Training: Epoch [93/120], Step [6999], Loss: 0.0023845145478844643, Training Accuracy: 97.64821428571429
[ Thu Jul  4 11:13:25 2024 ] 	Batch(7000/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul  4 11:13:43 2024 ] 	Batch(7100/7879) done. Loss: 0.0173  lr:0.000001
[ Thu Jul  4 11:14:01 2024 ] 	Batch(7200/7879) done. Loss: 0.0184  lr:0.000001
[ Thu Jul  4 11:14:19 2024 ] 	Batch(7300/7879) done. Loss: 0.1395  lr:0.000001
[ Thu Jul  4 11:14:37 2024 ] 	Batch(7400/7879) done. Loss: 0.0282  lr:0.000001
[ Thu Jul  4 11:14:55 2024 ] 
Training: Epoch [93/120], Step [7499], Loss: 0.08762195706367493, Training Accuracy: 97.69
[ Thu Jul  4 11:14:55 2024 ] 	Batch(7500/7879) done. Loss: 0.0207  lr:0.000001
[ Thu Jul  4 11:15:13 2024 ] 	Batch(7600/7879) done. Loss: 0.0095  lr:0.000001
[ Thu Jul  4 11:15:32 2024 ] 	Batch(7700/7879) done. Loss: 0.0254  lr:0.000001
[ Thu Jul  4 11:15:50 2024 ] 	Batch(7800/7879) done. Loss: 0.0903  lr:0.000001
[ Thu Jul  4 11:16:05 2024 ] 	Mean training loss: 0.0947.
[ Thu Jul  4 11:16:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 11:16:05 2024 ] Training epoch: 95
[ Thu Jul  4 11:16:05 2024 ] 	Batch(0/7879) done. Loss: 0.0257  lr:0.000001
[ Thu Jul  4 11:16:24 2024 ] 	Batch(100/7879) done. Loss: 0.0916  lr:0.000001
[ Thu Jul  4 11:16:42 2024 ] 	Batch(200/7879) done. Loss: 0.0057  lr:0.000001
[ Thu Jul  4 11:17:01 2024 ] 	Batch(300/7879) done. Loss: 0.0241  lr:0.000001
[ Thu Jul  4 11:17:19 2024 ] 	Batch(400/7879) done. Loss: 0.0280  lr:0.000001
[ Thu Jul  4 11:17:38 2024 ] 
Training: Epoch [94/120], Step [499], Loss: 0.3313893675804138, Training Accuracy: 97.375
[ Thu Jul  4 11:17:38 2024 ] 	Batch(500/7879) done. Loss: 0.0231  lr:0.000001
[ Thu Jul  4 11:17:56 2024 ] 	Batch(600/7879) done. Loss: 0.1062  lr:0.000001
[ Thu Jul  4 11:18:15 2024 ] 	Batch(700/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul  4 11:18:33 2024 ] 	Batch(800/7879) done. Loss: 0.0438  lr:0.000001
[ Thu Jul  4 11:18:51 2024 ] 	Batch(900/7879) done. Loss: 0.0745  lr:0.000001
[ Thu Jul  4 11:19:10 2024 ] 
Training: Epoch [94/120], Step [999], Loss: 0.3013409674167633, Training Accuracy: 97.5625
[ Thu Jul  4 11:19:10 2024 ] 	Batch(1000/7879) done. Loss: 0.0057  lr:0.000001
[ Thu Jul  4 11:19:28 2024 ] 	Batch(1100/7879) done. Loss: 0.2058  lr:0.000001
[ Thu Jul  4 11:19:47 2024 ] 	Batch(1200/7879) done. Loss: 0.0811  lr:0.000001
[ Thu Jul  4 11:20:05 2024 ] 	Batch(1300/7879) done. Loss: 0.0941  lr:0.000001
[ Thu Jul  4 11:20:23 2024 ] 	Batch(1400/7879) done. Loss: 0.1414  lr:0.000001
[ Thu Jul  4 11:20:40 2024 ] 
Training: Epoch [94/120], Step [1499], Loss: 0.0006010818760842085, Training Accuracy: 97.45833333333334
[ Thu Jul  4 11:20:40 2024 ] 	Batch(1500/7879) done. Loss: 0.0085  lr:0.000001
[ Thu Jul  4 11:20:59 2024 ] 	Batch(1600/7879) done. Loss: 0.2250  lr:0.000001
[ Thu Jul  4 11:21:16 2024 ] 	Batch(1700/7879) done. Loss: 0.4282  lr:0.000001
[ Thu Jul  4 11:21:34 2024 ] 	Batch(1800/7879) done. Loss: 0.0343  lr:0.000001
[ Thu Jul  4 11:21:52 2024 ] 	Batch(1900/7879) done. Loss: 0.1881  lr:0.000001
[ Thu Jul  4 11:22:10 2024 ] 
Training: Epoch [94/120], Step [1999], Loss: 0.08844847232103348, Training Accuracy: 97.52499999999999
[ Thu Jul  4 11:22:10 2024 ] 	Batch(2000/7879) done. Loss: 0.0798  lr:0.000001
[ Thu Jul  4 11:22:28 2024 ] 	Batch(2100/7879) done. Loss: 0.3500  lr:0.000001
[ Thu Jul  4 11:22:46 2024 ] 	Batch(2200/7879) done. Loss: 0.0441  lr:0.000001
[ Thu Jul  4 11:23:04 2024 ] 	Batch(2300/7879) done. Loss: 0.1264  lr:0.000001
[ Thu Jul  4 11:23:22 2024 ] 	Batch(2400/7879) done. Loss: 0.0228  lr:0.000001
[ Thu Jul  4 11:23:39 2024 ] 
Training: Epoch [94/120], Step [2499], Loss: 0.0024762277025729418, Training Accuracy: 97.53
[ Thu Jul  4 11:23:40 2024 ] 	Batch(2500/7879) done. Loss: 0.3029  lr:0.000001
[ Thu Jul  4 11:23:57 2024 ] 	Batch(2600/7879) done. Loss: 0.7606  lr:0.000001
[ Thu Jul  4 11:24:15 2024 ] 	Batch(2700/7879) done. Loss: 0.1718  lr:0.000001
[ Thu Jul  4 11:24:33 2024 ] 	Batch(2800/7879) done. Loss: 0.0227  lr:0.000001
[ Thu Jul  4 11:24:51 2024 ] 	Batch(2900/7879) done. Loss: 0.8927  lr:0.000001
[ Thu Jul  4 11:25:09 2024 ] 
Training: Epoch [94/120], Step [2999], Loss: 0.013309999369084835, Training Accuracy: 97.52499999999999
[ Thu Jul  4 11:25:09 2024 ] 	Batch(3000/7879) done. Loss: 0.0034  lr:0.000001
[ Thu Jul  4 11:25:27 2024 ] 	Batch(3100/7879) done. Loss: 0.0267  lr:0.000001
[ Thu Jul  4 11:25:45 2024 ] 	Batch(3200/7879) done. Loss: 0.0034  lr:0.000001
[ Thu Jul  4 11:26:03 2024 ] 	Batch(3300/7879) done. Loss: 0.0414  lr:0.000001
[ Thu Jul  4 11:26:21 2024 ] 	Batch(3400/7879) done. Loss: 0.0272  lr:0.000001
[ Thu Jul  4 11:26:38 2024 ] 
Training: Epoch [94/120], Step [3499], Loss: 0.01663968898355961, Training Accuracy: 97.55
[ Thu Jul  4 11:26:39 2024 ] 	Batch(3500/7879) done. Loss: 0.0922  lr:0.000001
[ Thu Jul  4 11:26:57 2024 ] 	Batch(3600/7879) done. Loss: 0.0512  lr:0.000001
[ Thu Jul  4 11:27:14 2024 ] 	Batch(3700/7879) done. Loss: 0.0771  lr:0.000001
[ Thu Jul  4 11:27:32 2024 ] 	Batch(3800/7879) done. Loss: 0.1370  lr:0.000001
[ Thu Jul  4 11:27:50 2024 ] 	Batch(3900/7879) done. Loss: 0.0155  lr:0.000001
[ Thu Jul  4 11:28:08 2024 ] 
Training: Epoch [94/120], Step [3999], Loss: 0.015300889499485493, Training Accuracy: 97.575
[ Thu Jul  4 11:28:08 2024 ] 	Batch(4000/7879) done. Loss: 0.0311  lr:0.000001
[ Thu Jul  4 11:28:26 2024 ] 	Batch(4100/7879) done. Loss: 0.0060  lr:0.000001
[ Thu Jul  4 11:28:44 2024 ] 	Batch(4200/7879) done. Loss: 0.0260  lr:0.000001
[ Thu Jul  4 11:29:02 2024 ] 	Batch(4300/7879) done. Loss: 0.0332  lr:0.000001
[ Thu Jul  4 11:29:20 2024 ] 	Batch(4400/7879) done. Loss: 0.0654  lr:0.000001
[ Thu Jul  4 11:29:38 2024 ] 
Training: Epoch [94/120], Step [4499], Loss: 0.005168254021555185, Training Accuracy: 97.6138888888889
[ Thu Jul  4 11:29:38 2024 ] 	Batch(4500/7879) done. Loss: 0.0735  lr:0.000001
[ Thu Jul  4 11:29:56 2024 ] 	Batch(4600/7879) done. Loss: 0.1053  lr:0.000001
[ Thu Jul  4 11:30:15 2024 ] 	Batch(4700/7879) done. Loss: 0.0527  lr:0.000001
[ Thu Jul  4 11:30:33 2024 ] 	Batch(4800/7879) done. Loss: 0.0500  lr:0.000001
[ Thu Jul  4 11:30:52 2024 ] 	Batch(4900/7879) done. Loss: 0.0648  lr:0.000001
[ Thu Jul  4 11:31:10 2024 ] 
Training: Epoch [94/120], Step [4999], Loss: 0.15967905521392822, Training Accuracy: 97.635
[ Thu Jul  4 11:31:10 2024 ] 	Batch(5000/7879) done. Loss: 0.0549  lr:0.000001
[ Thu Jul  4 11:31:29 2024 ] 	Batch(5100/7879) done. Loss: 0.0095  lr:0.000001
[ Thu Jul  4 11:31:47 2024 ] 	Batch(5200/7879) done. Loss: 0.0923  lr:0.000001
[ Thu Jul  4 11:32:05 2024 ] 	Batch(5300/7879) done. Loss: 0.2009  lr:0.000001
[ Thu Jul  4 11:32:23 2024 ] 	Batch(5400/7879) done. Loss: 0.2147  lr:0.000001
[ Thu Jul  4 11:32:41 2024 ] 
Training: Epoch [94/120], Step [5499], Loss: 0.06782719492912292, Training Accuracy: 97.61136363636363
[ Thu Jul  4 11:32:41 2024 ] 	Batch(5500/7879) done. Loss: 0.0376  lr:0.000001
[ Thu Jul  4 11:32:59 2024 ] 	Batch(5600/7879) done. Loss: 0.0720  lr:0.000001
[ Thu Jul  4 11:33:17 2024 ] 	Batch(5700/7879) done. Loss: 0.0129  lr:0.000001
[ Thu Jul  4 11:33:35 2024 ] 	Batch(5800/7879) done. Loss: 0.0315  lr:0.000001
[ Thu Jul  4 11:33:53 2024 ] 	Batch(5900/7879) done. Loss: 0.1316  lr:0.000001
[ Thu Jul  4 11:34:10 2024 ] 
Training: Epoch [94/120], Step [5999], Loss: 0.1605534553527832, Training Accuracy: 97.6375
[ Thu Jul  4 11:34:11 2024 ] 	Batch(6000/7879) done. Loss: 0.1953  lr:0.000001
[ Thu Jul  4 11:34:29 2024 ] 	Batch(6100/7879) done. Loss: 0.0846  lr:0.000001
[ Thu Jul  4 11:34:46 2024 ] 	Batch(6200/7879) done. Loss: 0.1597  lr:0.000001
[ Thu Jul  4 11:35:04 2024 ] 	Batch(6300/7879) done. Loss: 0.0283  lr:0.000001
[ Thu Jul  4 11:35:22 2024 ] 	Batch(6400/7879) done. Loss: 0.4496  lr:0.000001
[ Thu Jul  4 11:35:40 2024 ] 
Training: Epoch [94/120], Step [6499], Loss: 0.08423350751399994, Training Accuracy: 97.6423076923077
[ Thu Jul  4 11:35:40 2024 ] 	Batch(6500/7879) done. Loss: 0.0834  lr:0.000001
[ Thu Jul  4 11:35:58 2024 ] 	Batch(6600/7879) done. Loss: 0.0094  lr:0.000001
[ Thu Jul  4 11:36:16 2024 ] 	Batch(6700/7879) done. Loss: 0.1243  lr:0.000001
[ Thu Jul  4 11:36:34 2024 ] 	Batch(6800/7879) done. Loss: 0.3185  lr:0.000001
[ Thu Jul  4 11:36:52 2024 ] 	Batch(6900/7879) done. Loss: 0.0066  lr:0.000001
[ Thu Jul  4 11:37:09 2024 ] 
Training: Epoch [94/120], Step [6999], Loss: 0.3225843906402588, Training Accuracy: 97.6625
[ Thu Jul  4 11:37:09 2024 ] 	Batch(7000/7879) done. Loss: 0.0346  lr:0.000001
[ Thu Jul  4 11:37:27 2024 ] 	Batch(7100/7879) done. Loss: 0.0230  lr:0.000001
[ Thu Jul  4 11:37:45 2024 ] 	Batch(7200/7879) done. Loss: 0.0957  lr:0.000001
[ Thu Jul  4 11:38:03 2024 ] 	Batch(7300/7879) done. Loss: 0.0775  lr:0.000001
[ Thu Jul  4 11:38:21 2024 ] 	Batch(7400/7879) done. Loss: 0.1723  lr:0.000001
[ Thu Jul  4 11:38:39 2024 ] 
Training: Epoch [94/120], Step [7499], Loss: 0.036928799003362656, Training Accuracy: 97.64500000000001
[ Thu Jul  4 11:38:39 2024 ] 	Batch(7500/7879) done. Loss: 0.0030  lr:0.000001
[ Thu Jul  4 11:38:57 2024 ] 	Batch(7600/7879) done. Loss: 0.1099  lr:0.000001
[ Thu Jul  4 11:39:15 2024 ] 	Batch(7700/7879) done. Loss: 0.1857  lr:0.000001
[ Thu Jul  4 11:39:33 2024 ] 	Batch(7800/7879) done. Loss: 0.0032  lr:0.000001
[ Thu Jul  4 11:39:47 2024 ] 	Mean training loss: 0.0935.
[ Thu Jul  4 11:39:47 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 11:39:47 2024 ] Training epoch: 96
[ Thu Jul  4 11:39:47 2024 ] 	Batch(0/7879) done. Loss: 0.2634  lr:0.000001
[ Thu Jul  4 11:40:05 2024 ] 	Batch(100/7879) done. Loss: 0.0820  lr:0.000001
[ Thu Jul  4 11:40:23 2024 ] 	Batch(200/7879) done. Loss: 0.3249  lr:0.000001
[ Thu Jul  4 11:40:41 2024 ] 	Batch(300/7879) done. Loss: 0.4215  lr:0.000001
[ Thu Jul  4 11:40:59 2024 ] 	Batch(400/7879) done. Loss: 0.0227  lr:0.000001
[ Thu Jul  4 11:41:17 2024 ] 
Training: Epoch [95/120], Step [499], Loss: 0.035545069724321365, Training Accuracy: 97.425
[ Thu Jul  4 11:41:17 2024 ] 	Batch(500/7879) done. Loss: 0.0892  lr:0.000001
[ Thu Jul  4 11:41:35 2024 ] 	Batch(600/7879) done. Loss: 0.0550  lr:0.000001
[ Thu Jul  4 11:41:53 2024 ] 	Batch(700/7879) done. Loss: 0.0178  lr:0.000001
[ Thu Jul  4 11:42:11 2024 ] 	Batch(800/7879) done. Loss: 0.0207  lr:0.000001
[ Thu Jul  4 11:42:28 2024 ] 	Batch(900/7879) done. Loss: 0.1365  lr:0.000001
[ Thu Jul  4 11:42:46 2024 ] 
Training: Epoch [95/120], Step [999], Loss: 0.0011975797824561596, Training Accuracy: 97.5625
[ Thu Jul  4 11:42:46 2024 ] 	Batch(1000/7879) done. Loss: 0.0274  lr:0.000001
[ Thu Jul  4 11:43:04 2024 ] 	Batch(1100/7879) done. Loss: 0.0509  lr:0.000001
[ Thu Jul  4 11:43:22 2024 ] 	Batch(1200/7879) done. Loss: 0.0042  lr:0.000001
[ Thu Jul  4 11:43:40 2024 ] 	Batch(1300/7879) done. Loss: 0.1084  lr:0.000001
[ Thu Jul  4 11:43:58 2024 ] 	Batch(1400/7879) done. Loss: 0.0268  lr:0.000001
[ Thu Jul  4 11:44:16 2024 ] 
Training: Epoch [95/120], Step [1499], Loss: 0.060106124728918076, Training Accuracy: 97.60833333333333
[ Thu Jul  4 11:44:16 2024 ] 	Batch(1500/7879) done. Loss: 0.2399  lr:0.000001
[ Thu Jul  4 11:44:34 2024 ] 	Batch(1600/7879) done. Loss: 0.1930  lr:0.000001
[ Thu Jul  4 11:44:52 2024 ] 	Batch(1700/7879) done. Loss: 0.1202  lr:0.000001
[ Thu Jul  4 11:45:10 2024 ] 	Batch(1800/7879) done. Loss: 0.0781  lr:0.000001
[ Thu Jul  4 11:45:28 2024 ] 	Batch(1900/7879) done. Loss: 0.0540  lr:0.000001
[ Thu Jul  4 11:45:45 2024 ] 
Training: Epoch [95/120], Step [1999], Loss: 0.4651181399822235, Training Accuracy: 97.6
[ Thu Jul  4 11:45:46 2024 ] 	Batch(2000/7879) done. Loss: 0.0452  lr:0.000001
[ Thu Jul  4 11:46:03 2024 ] 	Batch(2100/7879) done. Loss: 0.1774  lr:0.000001
[ Thu Jul  4 11:46:21 2024 ] 	Batch(2200/7879) done. Loss: 0.1236  lr:0.000001
[ Thu Jul  4 11:46:39 2024 ] 	Batch(2300/7879) done. Loss: 0.0257  lr:0.000001
[ Thu Jul  4 11:46:57 2024 ] 	Batch(2400/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul  4 11:47:15 2024 ] 
Training: Epoch [95/120], Step [2499], Loss: 0.2787346839904785, Training Accuracy: 97.59
[ Thu Jul  4 11:47:15 2024 ] 	Batch(2500/7879) done. Loss: 0.3732  lr:0.000001
[ Thu Jul  4 11:47:33 2024 ] 	Batch(2600/7879) done. Loss: 0.1391  lr:0.000001
[ Thu Jul  4 11:47:51 2024 ] 	Batch(2700/7879) done. Loss: 0.1579  lr:0.000001
[ Thu Jul  4 11:48:09 2024 ] 	Batch(2800/7879) done. Loss: 0.0143  lr:0.000001
[ Thu Jul  4 11:48:27 2024 ] 	Batch(2900/7879) done. Loss: 0.0159  lr:0.000001
[ Thu Jul  4 11:48:45 2024 ] 
Training: Epoch [95/120], Step [2999], Loss: 0.023934166878461838, Training Accuracy: 97.5875
[ Thu Jul  4 11:48:46 2024 ] 	Batch(3000/7879) done. Loss: 0.1618  lr:0.000001
[ Thu Jul  4 11:49:03 2024 ] 	Batch(3100/7879) done. Loss: 0.0033  lr:0.000001
[ Thu Jul  4 11:49:21 2024 ] 	Batch(3200/7879) done. Loss: 0.0022  lr:0.000001
[ Thu Jul  4 11:49:39 2024 ] 	Batch(3300/7879) done. Loss: 0.0409  lr:0.000001
[ Thu Jul  4 11:49:57 2024 ] 	Batch(3400/7879) done. Loss: 0.0191  lr:0.000001
[ Thu Jul  4 11:50:15 2024 ] 
Training: Epoch [95/120], Step [3499], Loss: 0.01636895164847374, Training Accuracy: 97.61428571428571
[ Thu Jul  4 11:50:15 2024 ] 	Batch(3500/7879) done. Loss: 0.1235  lr:0.000001
[ Thu Jul  4 11:50:33 2024 ] 	Batch(3600/7879) done. Loss: 0.0234  lr:0.000001
[ Thu Jul  4 11:50:51 2024 ] 	Batch(3700/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul  4 11:51:08 2024 ] 	Batch(3800/7879) done. Loss: 0.0741  lr:0.000001
[ Thu Jul  4 11:51:26 2024 ] 	Batch(3900/7879) done. Loss: 0.0277  lr:0.000001
[ Thu Jul  4 11:51:44 2024 ] 
Training: Epoch [95/120], Step [3999], Loss: 0.04558967798948288, Training Accuracy: 97.584375
[ Thu Jul  4 11:51:44 2024 ] 	Batch(4000/7879) done. Loss: 0.1078  lr:0.000001
[ Thu Jul  4 11:52:03 2024 ] 	Batch(4100/7879) done. Loss: 0.0289  lr:0.000001
[ Thu Jul  4 11:52:21 2024 ] 	Batch(4200/7879) done. Loss: 0.0265  lr:0.000001
[ Thu Jul  4 11:52:40 2024 ] 	Batch(4300/7879) done. Loss: 0.0395  lr:0.000001
[ Thu Jul  4 11:52:58 2024 ] 	Batch(4400/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul  4 11:53:16 2024 ] 
Training: Epoch [95/120], Step [4499], Loss: 0.014968243427574635, Training Accuracy: 97.6138888888889
[ Thu Jul  4 11:53:16 2024 ] 	Batch(4500/7879) done. Loss: 0.0588  lr:0.000001
[ Thu Jul  4 11:53:34 2024 ] 	Batch(4600/7879) done. Loss: 0.0129  lr:0.000001
[ Thu Jul  4 11:53:52 2024 ] 	Batch(4700/7879) done. Loss: 0.0276  lr:0.000001
[ Thu Jul  4 11:54:10 2024 ] 	Batch(4800/7879) done. Loss: 0.1646  lr:0.000001
[ Thu Jul  4 11:54:28 2024 ] 	Batch(4900/7879) done. Loss: 0.0056  lr:0.000001
[ Thu Jul  4 11:54:45 2024 ] 
Training: Epoch [95/120], Step [4999], Loss: 0.06901716440916061, Training Accuracy: 97.5925
[ Thu Jul  4 11:54:46 2024 ] 	Batch(5000/7879) done. Loss: 0.0321  lr:0.000001
[ Thu Jul  4 11:55:04 2024 ] 	Batch(5100/7879) done. Loss: 0.0328  lr:0.000001
[ Thu Jul  4 11:55:21 2024 ] 	Batch(5200/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul  4 11:55:39 2024 ] 	Batch(5300/7879) done. Loss: 0.0152  lr:0.000001
[ Thu Jul  4 11:55:57 2024 ] 	Batch(5400/7879) done. Loss: 0.0681  lr:0.000001
[ Thu Jul  4 11:56:15 2024 ] 
Training: Epoch [95/120], Step [5499], Loss: 0.015210902318358421, Training Accuracy: 97.60909090909091
[ Thu Jul  4 11:56:15 2024 ] 	Batch(5500/7879) done. Loss: 0.1525  lr:0.000001
[ Thu Jul  4 11:56:33 2024 ] 	Batch(5600/7879) done. Loss: 0.0199  lr:0.000001
[ Thu Jul  4 11:56:51 2024 ] 	Batch(5700/7879) done. Loss: 0.0249  lr:0.000001
[ Thu Jul  4 11:57:09 2024 ] 	Batch(5800/7879) done. Loss: 0.0057  lr:0.000001
[ Thu Jul  4 11:57:27 2024 ] 	Batch(5900/7879) done. Loss: 0.1754  lr:0.000001
[ Thu Jul  4 11:57:44 2024 ] 
Training: Epoch [95/120], Step [5999], Loss: 0.1441427767276764, Training Accuracy: 97.64583333333333
[ Thu Jul  4 11:57:45 2024 ] 	Batch(6000/7879) done. Loss: 0.0228  lr:0.000001
[ Thu Jul  4 11:58:03 2024 ] 	Batch(6100/7879) done. Loss: 0.0150  lr:0.000001
[ Thu Jul  4 11:58:22 2024 ] 	Batch(6200/7879) done. Loss: 0.0346  lr:0.000001
[ Thu Jul  4 11:58:40 2024 ] 	Batch(6300/7879) done. Loss: 0.2214  lr:0.000001
[ Thu Jul  4 11:58:59 2024 ] 	Batch(6400/7879) done. Loss: 0.4354  lr:0.000001
[ Thu Jul  4 11:59:16 2024 ] 
Training: Epoch [95/120], Step [6499], Loss: 0.09836878627538681, Training Accuracy: 97.64038461538462
[ Thu Jul  4 11:59:17 2024 ] 	Batch(6500/7879) done. Loss: 0.0368  lr:0.000001
[ Thu Jul  4 11:59:34 2024 ] 	Batch(6600/7879) done. Loss: 0.2010  lr:0.000001
[ Thu Jul  4 11:59:52 2024 ] 	Batch(6700/7879) done. Loss: 0.1672  lr:0.000001
[ Thu Jul  4 12:00:10 2024 ] 	Batch(6800/7879) done. Loss: 0.0497  lr:0.000001
[ Thu Jul  4 12:00:28 2024 ] 	Batch(6900/7879) done. Loss: 0.1076  lr:0.000001
[ Thu Jul  4 12:00:46 2024 ] 
Training: Epoch [95/120], Step [6999], Loss: 0.013092583045363426, Training Accuracy: 97.65535714285714
[ Thu Jul  4 12:00:46 2024 ] 	Batch(7000/7879) done. Loss: 0.0131  lr:0.000001
[ Thu Jul  4 12:01:04 2024 ] 	Batch(7100/7879) done. Loss: 0.0207  lr:0.000001
[ Thu Jul  4 12:01:22 2024 ] 	Batch(7200/7879) done. Loss: 0.1912  lr:0.000001
[ Thu Jul  4 12:01:40 2024 ] 	Batch(7300/7879) done. Loss: 0.0769  lr:0.000001
[ Thu Jul  4 12:01:58 2024 ] 	Batch(7400/7879) done. Loss: 0.0037  lr:0.000001
[ Thu Jul  4 12:02:15 2024 ] 
Training: Epoch [95/120], Step [7499], Loss: 0.04794666916131973, Training Accuracy: 97.655
[ Thu Jul  4 12:02:15 2024 ] 	Batch(7500/7879) done. Loss: 0.0355  lr:0.000001
[ Thu Jul  4 12:02:33 2024 ] 	Batch(7600/7879) done. Loss: 0.0834  lr:0.000001
[ Thu Jul  4 12:02:51 2024 ] 	Batch(7700/7879) done. Loss: 0.0942  lr:0.000001
[ Thu Jul  4 12:03:09 2024 ] 	Batch(7800/7879) done. Loss: 0.1899  lr:0.000001
[ Thu Jul  4 12:03:23 2024 ] 	Mean training loss: 0.0929.
[ Thu Jul  4 12:03:23 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 12:03:23 2024 ] Training epoch: 97
[ Thu Jul  4 12:03:24 2024 ] 	Batch(0/7879) done. Loss: 0.0272  lr:0.000001
[ Thu Jul  4 12:03:42 2024 ] 	Batch(100/7879) done. Loss: 0.0386  lr:0.000001
[ Thu Jul  4 12:04:00 2024 ] 	Batch(200/7879) done. Loss: 0.0552  lr:0.000001
[ Thu Jul  4 12:04:18 2024 ] 	Batch(300/7879) done. Loss: 0.2142  lr:0.000001
[ Thu Jul  4 12:04:36 2024 ] 	Batch(400/7879) done. Loss: 0.0159  lr:0.000001
[ Thu Jul  4 12:04:54 2024 ] 
Training: Epoch [96/120], Step [499], Loss: 0.13795731961727142, Training Accuracy: 97.425
[ Thu Jul  4 12:04:54 2024 ] 	Batch(500/7879) done. Loss: 0.1406  lr:0.000001
[ Thu Jul  4 12:05:12 2024 ] 	Batch(600/7879) done. Loss: 0.1787  lr:0.000001
[ Thu Jul  4 12:05:30 2024 ] 	Batch(700/7879) done. Loss: 0.1595  lr:0.000001
[ Thu Jul  4 12:05:48 2024 ] 	Batch(800/7879) done. Loss: 0.0256  lr:0.000001
[ Thu Jul  4 12:06:07 2024 ] 	Batch(900/7879) done. Loss: 0.0343  lr:0.000001
[ Thu Jul  4 12:06:25 2024 ] 
Training: Epoch [96/120], Step [999], Loss: 0.5000829696655273, Training Accuracy: 97.475
[ Thu Jul  4 12:06:25 2024 ] 	Batch(1000/7879) done. Loss: 0.0293  lr:0.000001
[ Thu Jul  4 12:06:43 2024 ] 	Batch(1100/7879) done. Loss: 0.0298  lr:0.000001
[ Thu Jul  4 12:07:01 2024 ] 	Batch(1200/7879) done. Loss: 0.1842  lr:0.000001
[ Thu Jul  4 12:07:19 2024 ] 	Batch(1300/7879) done. Loss: 0.0097  lr:0.000001
[ Thu Jul  4 12:07:37 2024 ] 	Batch(1400/7879) done. Loss: 0.1549  lr:0.000001
[ Thu Jul  4 12:07:55 2024 ] 
Training: Epoch [96/120], Step [1499], Loss: 0.18077021837234497, Training Accuracy: 97.46666666666667
[ Thu Jul  4 12:07:55 2024 ] 	Batch(1500/7879) done. Loss: 0.0225  lr:0.000001
[ Thu Jul  4 12:08:13 2024 ] 	Batch(1600/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul  4 12:08:31 2024 ] 	Batch(1700/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul  4 12:08:49 2024 ] 	Batch(1800/7879) done. Loss: 0.0165  lr:0.000001
[ Thu Jul  4 12:09:07 2024 ] 	Batch(1900/7879) done. Loss: 0.1543  lr:0.000001
[ Thu Jul  4 12:09:25 2024 ] 
Training: Epoch [96/120], Step [1999], Loss: 0.0344395637512207, Training Accuracy: 97.4375
[ Thu Jul  4 12:09:25 2024 ] 	Batch(2000/7879) done. Loss: 0.1680  lr:0.000001
[ Thu Jul  4 12:09:43 2024 ] 	Batch(2100/7879) done. Loss: 0.0138  lr:0.000001
[ Thu Jul  4 12:10:01 2024 ] 	Batch(2200/7879) done. Loss: 0.0591  lr:0.000001
[ Thu Jul  4 12:10:19 2024 ] 	Batch(2300/7879) done. Loss: 0.0279  lr:0.000001
[ Thu Jul  4 12:10:37 2024 ] 	Batch(2400/7879) done. Loss: 0.0813  lr:0.000001
[ Thu Jul  4 12:10:55 2024 ] 
Training: Epoch [96/120], Step [2499], Loss: 0.06468462944030762, Training Accuracy: 97.41
[ Thu Jul  4 12:10:55 2024 ] 	Batch(2500/7879) done. Loss: 0.2559  lr:0.000001
[ Thu Jul  4 12:11:13 2024 ] 	Batch(2600/7879) done. Loss: 0.0739  lr:0.000001
[ Thu Jul  4 12:11:31 2024 ] 	Batch(2700/7879) done. Loss: 0.2897  lr:0.000001
[ Thu Jul  4 12:11:50 2024 ] 	Batch(2800/7879) done. Loss: 0.0337  lr:0.000001
[ Thu Jul  4 12:12:07 2024 ] 	Batch(2900/7879) done. Loss: 0.1309  lr:0.000001
[ Thu Jul  4 12:12:25 2024 ] 
Training: Epoch [96/120], Step [2999], Loss: 0.12913201749324799, Training Accuracy: 97.425
[ Thu Jul  4 12:12:25 2024 ] 	Batch(3000/7879) done. Loss: 0.0957  lr:0.000001
[ Thu Jul  4 12:12:43 2024 ] 	Batch(3100/7879) done. Loss: 0.0087  lr:0.000001
[ Thu Jul  4 12:13:02 2024 ] 	Batch(3200/7879) done. Loss: 0.0214  lr:0.000001
[ Thu Jul  4 12:13:20 2024 ] 	Batch(3300/7879) done. Loss: 0.0180  lr:0.000001
[ Thu Jul  4 12:13:38 2024 ] 	Batch(3400/7879) done. Loss: 0.0195  lr:0.000001
[ Thu Jul  4 12:13:55 2024 ] 
Training: Epoch [96/120], Step [3499], Loss: 0.06229652836918831, Training Accuracy: 97.51785714285714
[ Thu Jul  4 12:13:56 2024 ] 	Batch(3500/7879) done. Loss: 0.0025  lr:0.000001
[ Thu Jul  4 12:14:14 2024 ] 	Batch(3600/7879) done. Loss: 0.1172  lr:0.000001
[ Thu Jul  4 12:14:32 2024 ] 	Batch(3700/7879) done. Loss: 0.1538  lr:0.000001
[ Thu Jul  4 12:14:50 2024 ] 	Batch(3800/7879) done. Loss: 0.1091  lr:0.000001
[ Thu Jul  4 12:15:08 2024 ] 	Batch(3900/7879) done. Loss: 0.0408  lr:0.000001
[ Thu Jul  4 12:15:26 2024 ] 
Training: Epoch [96/120], Step [3999], Loss: 0.03983118757605553, Training Accuracy: 97.515625
[ Thu Jul  4 12:15:26 2024 ] 	Batch(4000/7879) done. Loss: 0.4885  lr:0.000001
[ Thu Jul  4 12:15:44 2024 ] 	Batch(4100/7879) done. Loss: 0.0137  lr:0.000001
[ Thu Jul  4 12:16:02 2024 ] 	Batch(4200/7879) done. Loss: 0.1098  lr:0.000001
[ Thu Jul  4 12:16:20 2024 ] 	Batch(4300/7879) done. Loss: 0.1536  lr:0.000001
[ Thu Jul  4 12:16:38 2024 ] 	Batch(4400/7879) done. Loss: 0.2654  lr:0.000001
[ Thu Jul  4 12:16:56 2024 ] 
Training: Epoch [96/120], Step [4499], Loss: 0.01405517291277647, Training Accuracy: 97.52777777777779
[ Thu Jul  4 12:16:57 2024 ] 	Batch(4500/7879) done. Loss: 0.0682  lr:0.000001
[ Thu Jul  4 12:17:15 2024 ] 	Batch(4600/7879) done. Loss: 0.0297  lr:0.000001
[ Thu Jul  4 12:17:34 2024 ] 	Batch(4700/7879) done. Loss: 0.1625  lr:0.000001
[ Thu Jul  4 12:17:53 2024 ] 	Batch(4800/7879) done. Loss: 0.0032  lr:0.000001
[ Thu Jul  4 12:18:11 2024 ] 	Batch(4900/7879) done. Loss: 0.0298  lr:0.000001
[ Thu Jul  4 12:18:30 2024 ] 
Training: Epoch [96/120], Step [4999], Loss: 0.09212547540664673, Training Accuracy: 97.5775
[ Thu Jul  4 12:18:30 2024 ] 	Batch(5000/7879) done. Loss: 0.4008  lr:0.000001
[ Thu Jul  4 12:18:49 2024 ] 	Batch(5100/7879) done. Loss: 0.0609  lr:0.000001
[ Thu Jul  4 12:19:07 2024 ] 	Batch(5200/7879) done. Loss: 0.2756  lr:0.000001
[ Thu Jul  4 12:19:26 2024 ] 	Batch(5300/7879) done. Loss: 0.0547  lr:0.000001
[ Thu Jul  4 12:19:44 2024 ] 	Batch(5400/7879) done. Loss: 0.0044  lr:0.000001
[ Thu Jul  4 12:20:02 2024 ] 
Training: Epoch [96/120], Step [5499], Loss: 0.04263094812631607, Training Accuracy: 97.57954545454545
[ Thu Jul  4 12:20:02 2024 ] 	Batch(5500/7879) done. Loss: 0.0468  lr:0.000001
[ Thu Jul  4 12:20:20 2024 ] 	Batch(5600/7879) done. Loss: 0.0253  lr:0.000001
[ Thu Jul  4 12:20:38 2024 ] 	Batch(5700/7879) done. Loss: 0.3147  lr:0.000001
[ Thu Jul  4 12:20:56 2024 ] 	Batch(5800/7879) done. Loss: 0.0336  lr:0.000001
[ Thu Jul  4 12:21:14 2024 ] 	Batch(5900/7879) done. Loss: 0.0229  lr:0.000001
[ Thu Jul  4 12:21:32 2024 ] 
Training: Epoch [96/120], Step [5999], Loss: 0.1620749980211258, Training Accuracy: 97.56458333333333
[ Thu Jul  4 12:21:32 2024 ] 	Batch(6000/7879) done. Loss: 0.1766  lr:0.000001
[ Thu Jul  4 12:21:50 2024 ] 	Batch(6100/7879) done. Loss: 0.0232  lr:0.000001
[ Thu Jul  4 12:22:08 2024 ] 	Batch(6200/7879) done. Loss: 0.0281  lr:0.000001
[ Thu Jul  4 12:22:26 2024 ] 	Batch(6300/7879) done. Loss: 0.0643  lr:0.000001
[ Thu Jul  4 12:22:44 2024 ] 	Batch(6400/7879) done. Loss: 0.0080  lr:0.000001
[ Thu Jul  4 12:23:03 2024 ] 
Training: Epoch [96/120], Step [6499], Loss: 0.009148371405899525, Training Accuracy: 97.58846153846153
[ Thu Jul  4 12:23:03 2024 ] 	Batch(6500/7879) done. Loss: 0.0701  lr:0.000001
[ Thu Jul  4 12:23:22 2024 ] 	Batch(6600/7879) done. Loss: 0.0335  lr:0.000001
[ Thu Jul  4 12:23:40 2024 ] 	Batch(6700/7879) done. Loss: 0.0671  lr:0.000001
[ Thu Jul  4 12:23:59 2024 ] 	Batch(6800/7879) done. Loss: 0.2677  lr:0.000001
[ Thu Jul  4 12:24:17 2024 ] 	Batch(6900/7879) done. Loss: 0.0092  lr:0.000001
[ Thu Jul  4 12:24:35 2024 ] 
Training: Epoch [96/120], Step [6999], Loss: 0.050606951117515564, Training Accuracy: 97.57857142857142
[ Thu Jul  4 12:24:35 2024 ] 	Batch(7000/7879) done. Loss: 0.1959  lr:0.000001
[ Thu Jul  4 12:24:53 2024 ] 	Batch(7100/7879) done. Loss: 0.2131  lr:0.000001
[ Thu Jul  4 12:25:11 2024 ] 	Batch(7200/7879) done. Loss: 0.1520  lr:0.000001
[ Thu Jul  4 12:25:30 2024 ] 	Batch(7300/7879) done. Loss: 0.1293  lr:0.000001
[ Thu Jul  4 12:25:49 2024 ] 	Batch(7400/7879) done. Loss: 0.0120  lr:0.000001
[ Thu Jul  4 12:26:07 2024 ] 
Training: Epoch [96/120], Step [7499], Loss: 0.015519151464104652, Training Accuracy: 97.58333333333333
[ Thu Jul  4 12:26:07 2024 ] 	Batch(7500/7879) done. Loss: 0.2718  lr:0.000001
[ Thu Jul  4 12:26:26 2024 ] 	Batch(7600/7879) done. Loss: 0.0302  lr:0.000001
[ Thu Jul  4 12:26:44 2024 ] 	Batch(7700/7879) done. Loss: 0.0982  lr:0.000001
[ Thu Jul  4 12:27:02 2024 ] 	Batch(7800/7879) done. Loss: 0.0559  lr:0.000001
[ Thu Jul  4 12:27:16 2024 ] 	Mean training loss: 0.0948.
[ Thu Jul  4 12:27:16 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 12:27:16 2024 ] Training epoch: 98
[ Thu Jul  4 12:27:17 2024 ] 	Batch(0/7879) done. Loss: 0.2255  lr:0.000001
[ Thu Jul  4 12:27:35 2024 ] 	Batch(100/7879) done. Loss: 0.0337  lr:0.000001
[ Thu Jul  4 12:27:53 2024 ] 	Batch(200/7879) done. Loss: 0.2523  lr:0.000001
[ Thu Jul  4 12:28:11 2024 ] 	Batch(300/7879) done. Loss: 0.0587  lr:0.000001
[ Thu Jul  4 12:28:29 2024 ] 	Batch(400/7879) done. Loss: 0.1560  lr:0.000001
[ Thu Jul  4 12:28:47 2024 ] 
Training: Epoch [97/120], Step [499], Loss: 0.4994457960128784, Training Accuracy: 97.625
[ Thu Jul  4 12:28:47 2024 ] 	Batch(500/7879) done. Loss: 0.0498  lr:0.000001
[ Thu Jul  4 12:29:05 2024 ] 	Batch(600/7879) done. Loss: 0.1030  lr:0.000001
[ Thu Jul  4 12:29:23 2024 ] 	Batch(700/7879) done. Loss: 0.0510  lr:0.000001
[ Thu Jul  4 12:29:41 2024 ] 	Batch(800/7879) done. Loss: 0.2733  lr:0.000001
[ Thu Jul  4 12:29:59 2024 ] 	Batch(900/7879) done. Loss: 0.0012  lr:0.000001
[ Thu Jul  4 12:30:17 2024 ] 
Training: Epoch [97/120], Step [999], Loss: 0.16168738901615143, Training Accuracy: 97.625
[ Thu Jul  4 12:30:17 2024 ] 	Batch(1000/7879) done. Loss: 0.0770  lr:0.000001
[ Thu Jul  4 12:30:35 2024 ] 	Batch(1100/7879) done. Loss: 0.2128  lr:0.000001
[ Thu Jul  4 12:30:53 2024 ] 	Batch(1200/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul  4 12:31:11 2024 ] 	Batch(1300/7879) done. Loss: 0.0474  lr:0.000001
[ Thu Jul  4 12:31:29 2024 ] 	Batch(1400/7879) done. Loss: 0.0147  lr:0.000001
[ Thu Jul  4 12:31:47 2024 ] 
Training: Epoch [97/120], Step [1499], Loss: 0.022979868575930595, Training Accuracy: 97.59166666666667
[ Thu Jul  4 12:31:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0314  lr:0.000001
[ Thu Jul  4 12:32:05 2024 ] 	Batch(1600/7879) done. Loss: 0.5236  lr:0.000001
[ Thu Jul  4 12:32:24 2024 ] 	Batch(1700/7879) done. Loss: 0.0014  lr:0.000001
[ Thu Jul  4 12:32:42 2024 ] 	Batch(1800/7879) done. Loss: 0.0931  lr:0.000001
[ Thu Jul  4 12:33:00 2024 ] 	Batch(1900/7879) done. Loss: 0.0517  lr:0.000001
[ Thu Jul  4 12:33:18 2024 ] 
Training: Epoch [97/120], Step [1999], Loss: 0.01206199824810028, Training Accuracy: 97.55625
[ Thu Jul  4 12:33:18 2024 ] 	Batch(2000/7879) done. Loss: 0.1854  lr:0.000001
[ Thu Jul  4 12:33:36 2024 ] 	Batch(2100/7879) done. Loss: 0.1239  lr:0.000001
[ Thu Jul  4 12:33:54 2024 ] 	Batch(2200/7879) done. Loss: 0.0285  lr:0.000001
[ Thu Jul  4 12:34:12 2024 ] 	Batch(2300/7879) done. Loss: 0.0094  lr:0.000001
[ Thu Jul  4 12:34:31 2024 ] 	Batch(2400/7879) done. Loss: 0.0205  lr:0.000001
[ Thu Jul  4 12:34:49 2024 ] 
Training: Epoch [97/120], Step [2499], Loss: 0.2293063849210739, Training Accuracy: 97.515
[ Thu Jul  4 12:34:49 2024 ] 	Batch(2500/7879) done. Loss: 0.0067  lr:0.000001
[ Thu Jul  4 12:35:07 2024 ] 	Batch(2600/7879) done. Loss: 0.0287  lr:0.000001
[ Thu Jul  4 12:35:25 2024 ] 	Batch(2700/7879) done. Loss: 0.0247  lr:0.000001
[ Thu Jul  4 12:35:43 2024 ] 	Batch(2800/7879) done. Loss: 0.0250  lr:0.000001
[ Thu Jul  4 12:36:01 2024 ] 	Batch(2900/7879) done. Loss: 0.1038  lr:0.000001
[ Thu Jul  4 12:36:19 2024 ] 
Training: Epoch [97/120], Step [2999], Loss: 0.3433228135108948, Training Accuracy: 97.54583333333333
[ Thu Jul  4 12:36:19 2024 ] 	Batch(3000/7879) done. Loss: 0.0204  lr:0.000001
[ Thu Jul  4 12:36:37 2024 ] 	Batch(3100/7879) done. Loss: 0.0371  lr:0.000001
[ Thu Jul  4 12:36:55 2024 ] 	Batch(3200/7879) done. Loss: 0.0756  lr:0.000001
[ Thu Jul  4 12:37:13 2024 ] 	Batch(3300/7879) done. Loss: 0.0682  lr:0.000001
[ Thu Jul  4 12:37:31 2024 ] 	Batch(3400/7879) done. Loss: 0.6835  lr:0.000001
[ Thu Jul  4 12:37:49 2024 ] 
Training: Epoch [97/120], Step [3499], Loss: 0.01249684114009142, Training Accuracy: 97.54285714285714
[ Thu Jul  4 12:37:50 2024 ] 	Batch(3500/7879) done. Loss: 0.0037  lr:0.000001
[ Thu Jul  4 12:38:08 2024 ] 	Batch(3600/7879) done. Loss: 0.2259  lr:0.000001
[ Thu Jul  4 12:38:27 2024 ] 	Batch(3700/7879) done. Loss: 0.0047  lr:0.000001
[ Thu Jul  4 12:38:45 2024 ] 	Batch(3800/7879) done. Loss: 0.0076  lr:0.000001
[ Thu Jul  4 12:39:04 2024 ] 	Batch(3900/7879) done. Loss: 0.1055  lr:0.000001
[ Thu Jul  4 12:39:21 2024 ] 
Training: Epoch [97/120], Step [3999], Loss: 0.017858633771538734, Training Accuracy: 97.51875
[ Thu Jul  4 12:39:22 2024 ] 	Batch(4000/7879) done. Loss: 0.0066  lr:0.000001
[ Thu Jul  4 12:39:40 2024 ] 	Batch(4100/7879) done. Loss: 0.0356  lr:0.000001
[ Thu Jul  4 12:39:58 2024 ] 	Batch(4200/7879) done. Loss: 0.0333  lr:0.000001
[ Thu Jul  4 12:40:15 2024 ] 	Batch(4300/7879) done. Loss: 0.0168  lr:0.000001
[ Thu Jul  4 12:40:33 2024 ] 	Batch(4400/7879) done. Loss: 0.0554  lr:0.000001
[ Thu Jul  4 12:40:51 2024 ] 
Training: Epoch [97/120], Step [4499], Loss: 0.03993167728185654, Training Accuracy: 97.51388888888889
[ Thu Jul  4 12:40:51 2024 ] 	Batch(4500/7879) done. Loss: 0.0173  lr:0.000001
[ Thu Jul  4 12:41:09 2024 ] 	Batch(4600/7879) done. Loss: 0.5140  lr:0.000001
[ Thu Jul  4 12:41:27 2024 ] 	Batch(4700/7879) done. Loss: 0.0021  lr:0.000001
[ Thu Jul  4 12:41:45 2024 ] 	Batch(4800/7879) done. Loss: 0.0052  lr:0.000001
[ Thu Jul  4 12:42:04 2024 ] 	Batch(4900/7879) done. Loss: 0.2653  lr:0.000001
[ Thu Jul  4 12:42:22 2024 ] 
Training: Epoch [97/120], Step [4999], Loss: 0.03355870395898819, Training Accuracy: 97.4925
[ Thu Jul  4 12:42:22 2024 ] 	Batch(5000/7879) done. Loss: 0.0354  lr:0.000001
[ Thu Jul  4 12:42:41 2024 ] 	Batch(5100/7879) done. Loss: 0.0154  lr:0.000001
[ Thu Jul  4 12:42:59 2024 ] 	Batch(5200/7879) done. Loss: 0.1215  lr:0.000001
[ Thu Jul  4 12:43:17 2024 ] 	Batch(5300/7879) done. Loss: 0.0142  lr:0.000001
[ Thu Jul  4 12:43:35 2024 ] 	Batch(5400/7879) done. Loss: 0.0876  lr:0.000001
[ Thu Jul  4 12:43:52 2024 ] 
Training: Epoch [97/120], Step [5499], Loss: 0.0475492887198925, Training Accuracy: 97.55909090909091
[ Thu Jul  4 12:43:52 2024 ] 	Batch(5500/7879) done. Loss: 0.0624  lr:0.000001
[ Thu Jul  4 12:44:11 2024 ] 	Batch(5600/7879) done. Loss: 0.0330  lr:0.000001
[ Thu Jul  4 12:44:29 2024 ] 	Batch(5700/7879) done. Loss: 0.0052  lr:0.000001
[ Thu Jul  4 12:44:48 2024 ] 	Batch(5800/7879) done. Loss: 0.0677  lr:0.000001
[ Thu Jul  4 12:45:06 2024 ] 	Batch(5900/7879) done. Loss: 0.0966  lr:0.000001
[ Thu Jul  4 12:45:24 2024 ] 
Training: Epoch [97/120], Step [5999], Loss: 0.0023518013767898083, Training Accuracy: 97.56875000000001
[ Thu Jul  4 12:45:25 2024 ] 	Batch(6000/7879) done. Loss: 0.3319  lr:0.000001
[ Thu Jul  4 12:45:43 2024 ] 	Batch(6100/7879) done. Loss: 0.0838  lr:0.000001
[ Thu Jul  4 12:46:02 2024 ] 	Batch(6200/7879) done. Loss: 0.0444  lr:0.000001
[ Thu Jul  4 12:46:20 2024 ] 	Batch(6300/7879) done. Loss: 0.0935  lr:0.000001
[ Thu Jul  4 12:46:39 2024 ] 	Batch(6400/7879) done. Loss: 0.1344  lr:0.000001
[ Thu Jul  4 12:46:57 2024 ] 
Training: Epoch [97/120], Step [6499], Loss: 0.053609490394592285, Training Accuracy: 97.58653846153847
[ Thu Jul  4 12:46:57 2024 ] 	Batch(6500/7879) done. Loss: 0.0620  lr:0.000001
[ Thu Jul  4 12:47:16 2024 ] 	Batch(6600/7879) done. Loss: 0.0210  lr:0.000001
[ Thu Jul  4 12:47:34 2024 ] 	Batch(6700/7879) done. Loss: 0.0144  lr:0.000001
[ Thu Jul  4 12:47:53 2024 ] 	Batch(6800/7879) done. Loss: 0.0207  lr:0.000001
[ Thu Jul  4 12:48:11 2024 ] 	Batch(6900/7879) done. Loss: 0.0466  lr:0.000001
[ Thu Jul  4 12:48:29 2024 ] 
Training: Epoch [97/120], Step [6999], Loss: 0.1935851126909256, Training Accuracy: 97.59285714285714
[ Thu Jul  4 12:48:29 2024 ] 	Batch(7000/7879) done. Loss: 0.0379  lr:0.000001
[ Thu Jul  4 12:48:47 2024 ] 	Batch(7100/7879) done. Loss: 0.0716  lr:0.000001
[ Thu Jul  4 12:49:05 2024 ] 	Batch(7200/7879) done. Loss: 0.0175  lr:0.000001
[ Thu Jul  4 12:49:23 2024 ] 	Batch(7300/7879) done. Loss: 0.0559  lr:0.000001
[ Thu Jul  4 12:49:40 2024 ] 	Batch(7400/7879) done. Loss: 0.3639  lr:0.000001
[ Thu Jul  4 12:49:58 2024 ] 
Training: Epoch [97/120], Step [7499], Loss: 0.023005854338407516, Training Accuracy: 97.605
[ Thu Jul  4 12:49:58 2024 ] 	Batch(7500/7879) done. Loss: 0.1127  lr:0.000001
[ Thu Jul  4 12:50:16 2024 ] 	Batch(7600/7879) done. Loss: 0.1805  lr:0.000001
[ Thu Jul  4 12:50:35 2024 ] 	Batch(7700/7879) done. Loss: 0.0607  lr:0.000001
[ Thu Jul  4 12:50:53 2024 ] 	Batch(7800/7879) done. Loss: 0.0035  lr:0.000001
[ Thu Jul  4 12:51:08 2024 ] 	Mean training loss: 0.0924.
[ Thu Jul  4 12:51:08 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 12:51:08 2024 ] Training epoch: 99
[ Thu Jul  4 12:51:08 2024 ] 	Batch(0/7879) done. Loss: 0.0984  lr:0.000001
[ Thu Jul  4 12:51:26 2024 ] 	Batch(100/7879) done. Loss: 0.0331  lr:0.000001
[ Thu Jul  4 12:51:44 2024 ] 	Batch(200/7879) done. Loss: 0.0080  lr:0.000001
[ Thu Jul  4 12:52:02 2024 ] 	Batch(300/7879) done. Loss: 0.0890  lr:0.000001
[ Thu Jul  4 12:52:20 2024 ] 	Batch(400/7879) done. Loss: 0.1485  lr:0.000001
[ Thu Jul  4 12:52:38 2024 ] 
Training: Epoch [98/120], Step [499], Loss: 0.05445069074630737, Training Accuracy: 97.875
[ Thu Jul  4 12:52:38 2024 ] 	Batch(500/7879) done. Loss: 0.0079  lr:0.000001
[ Thu Jul  4 12:52:56 2024 ] 	Batch(600/7879) done. Loss: 0.0201  lr:0.000001
[ Thu Jul  4 12:53:15 2024 ] 	Batch(700/7879) done. Loss: 0.3655  lr:0.000001
[ Thu Jul  4 12:53:33 2024 ] 	Batch(800/7879) done. Loss: 0.0327  lr:0.000001
[ Thu Jul  4 12:53:51 2024 ] 	Batch(900/7879) done. Loss: 0.0499  lr:0.000001
[ Thu Jul  4 12:54:08 2024 ] 
Training: Epoch [98/120], Step [999], Loss: 0.012997861951589584, Training Accuracy: 98.0125
[ Thu Jul  4 12:54:08 2024 ] 	Batch(1000/7879) done. Loss: 0.0220  lr:0.000001
[ Thu Jul  4 12:54:26 2024 ] 	Batch(1100/7879) done. Loss: 0.0849  lr:0.000001
[ Thu Jul  4 12:54:44 2024 ] 	Batch(1200/7879) done. Loss: 0.0069  lr:0.000001
[ Thu Jul  4 12:55:03 2024 ] 	Batch(1300/7879) done. Loss: 0.0487  lr:0.000001
[ Thu Jul  4 12:55:21 2024 ] 	Batch(1400/7879) done. Loss: 0.3091  lr:0.000001
[ Thu Jul  4 12:55:40 2024 ] 
Training: Epoch [98/120], Step [1499], Loss: 0.00046305320574902, Training Accuracy: 97.88333333333334
[ Thu Jul  4 12:55:40 2024 ] 	Batch(1500/7879) done. Loss: 0.3706  lr:0.000001
[ Thu Jul  4 12:55:59 2024 ] 	Batch(1600/7879) done. Loss: 0.0684  lr:0.000001
[ Thu Jul  4 12:56:17 2024 ] 	Batch(1700/7879) done. Loss: 0.0184  lr:0.000001
[ Thu Jul  4 12:56:35 2024 ] 	Batch(1800/7879) done. Loss: 0.0132  lr:0.000001
[ Thu Jul  4 12:56:53 2024 ] 	Batch(1900/7879) done. Loss: 0.0518  lr:0.000001
[ Thu Jul  4 12:57:11 2024 ] 
Training: Epoch [98/120], Step [1999], Loss: 0.024000734090805054, Training Accuracy: 97.82499999999999
[ Thu Jul  4 12:57:11 2024 ] 	Batch(2000/7879) done. Loss: 0.1029  lr:0.000001
[ Thu Jul  4 12:57:29 2024 ] 	Batch(2100/7879) done. Loss: 0.0088  lr:0.000001
[ Thu Jul  4 12:57:48 2024 ] 	Batch(2200/7879) done. Loss: 0.0518  lr:0.000001
[ Thu Jul  4 12:58:06 2024 ] 	Batch(2300/7879) done. Loss: 0.0520  lr:0.000001
[ Thu Jul  4 12:58:25 2024 ] 	Batch(2400/7879) done. Loss: 0.1353  lr:0.000001
[ Thu Jul  4 12:58:43 2024 ] 
Training: Epoch [98/120], Step [2499], Loss: 0.09560100734233856, Training Accuracy: 97.75
[ Thu Jul  4 12:58:43 2024 ] 	Batch(2500/7879) done. Loss: 0.0567  lr:0.000001
[ Thu Jul  4 12:59:01 2024 ] 	Batch(2600/7879) done. Loss: 0.4275  lr:0.000001
[ Thu Jul  4 12:59:19 2024 ] 	Batch(2700/7879) done. Loss: 0.1475  lr:0.000001
[ Thu Jul  4 12:59:37 2024 ] 	Batch(2800/7879) done. Loss: 0.0041  lr:0.000001
[ Thu Jul  4 12:59:55 2024 ] 	Batch(2900/7879) done. Loss: 0.1312  lr:0.000001
[ Thu Jul  4 13:00:12 2024 ] 
Training: Epoch [98/120], Step [2999], Loss: 0.30279314517974854, Training Accuracy: 97.6625
[ Thu Jul  4 13:00:13 2024 ] 	Batch(3000/7879) done. Loss: 0.5431  lr:0.000001
[ Thu Jul  4 13:00:31 2024 ] 	Batch(3100/7879) done. Loss: 0.0357  lr:0.000001
[ Thu Jul  4 13:00:49 2024 ] 	Batch(3200/7879) done. Loss: 0.0346  lr:0.000001
[ Thu Jul  4 13:01:06 2024 ] 	Batch(3300/7879) done. Loss: 0.1599  lr:0.000001
[ Thu Jul  4 13:01:24 2024 ] 	Batch(3400/7879) done. Loss: 0.0593  lr:0.000001
[ Thu Jul  4 13:01:42 2024 ] 
Training: Epoch [98/120], Step [3499], Loss: 0.14112290740013123, Training Accuracy: 97.68214285714286
[ Thu Jul  4 13:01:42 2024 ] 	Batch(3500/7879) done. Loss: 0.0586  lr:0.000001
[ Thu Jul  4 13:02:00 2024 ] 	Batch(3600/7879) done. Loss: 0.0156  lr:0.000001
[ Thu Jul  4 13:02:18 2024 ] 	Batch(3700/7879) done. Loss: 0.0279  lr:0.000001
[ Thu Jul  4 13:02:36 2024 ] 	Batch(3800/7879) done. Loss: 0.0470  lr:0.000001
[ Thu Jul  4 13:02:54 2024 ] 	Batch(3900/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul  4 13:03:12 2024 ] 
Training: Epoch [98/120], Step [3999], Loss: 0.016718531027436256, Training Accuracy: 97.709375
[ Thu Jul  4 13:03:12 2024 ] 	Batch(4000/7879) done. Loss: 0.0432  lr:0.000001
[ Thu Jul  4 13:03:30 2024 ] 	Batch(4100/7879) done. Loss: 0.2165  lr:0.000001
[ Thu Jul  4 13:03:48 2024 ] 	Batch(4200/7879) done. Loss: 0.0453  lr:0.000001
[ Thu Jul  4 13:04:06 2024 ] 	Batch(4300/7879) done. Loss: 0.1362  lr:0.000001
[ Thu Jul  4 13:04:24 2024 ] 	Batch(4400/7879) done. Loss: 0.0900  lr:0.000001
[ Thu Jul  4 13:04:42 2024 ] 
Training: Epoch [98/120], Step [4499], Loss: 0.005733458325266838, Training Accuracy: 97.66666666666667
[ Thu Jul  4 13:04:42 2024 ] 	Batch(4500/7879) done. Loss: 0.0191  lr:0.000001
[ Thu Jul  4 13:05:00 2024 ] 	Batch(4600/7879) done. Loss: 0.1761  lr:0.000001
[ Thu Jul  4 13:05:17 2024 ] 	Batch(4700/7879) done. Loss: 0.0379  lr:0.000001
[ Thu Jul  4 13:05:36 2024 ] 	Batch(4800/7879) done. Loss: 0.0040  lr:0.000001
[ Thu Jul  4 13:05:54 2024 ] 	Batch(4900/7879) done. Loss: 0.0046  lr:0.000001
[ Thu Jul  4 13:06:12 2024 ] 
Training: Epoch [98/120], Step [4999], Loss: 0.012706094421446323, Training Accuracy: 97.69250000000001
[ Thu Jul  4 13:06:12 2024 ] 	Batch(5000/7879) done. Loss: 0.0126  lr:0.000001
[ Thu Jul  4 13:06:30 2024 ] 	Batch(5100/7879) done. Loss: 0.0150  lr:0.000001
[ Thu Jul  4 13:06:48 2024 ] 	Batch(5200/7879) done. Loss: 0.0637  lr:0.000001
[ Thu Jul  4 13:07:06 2024 ] 	Batch(5300/7879) done. Loss: 0.0543  lr:0.000001
[ Thu Jul  4 13:07:23 2024 ] 	Batch(5400/7879) done. Loss: 0.2232  lr:0.000001
[ Thu Jul  4 13:07:41 2024 ] 
Training: Epoch [98/120], Step [5499], Loss: 0.005607543513178825, Training Accuracy: 97.75
[ Thu Jul  4 13:07:41 2024 ] 	Batch(5500/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul  4 13:07:59 2024 ] 	Batch(5600/7879) done. Loss: 0.3240  lr:0.000001
[ Thu Jul  4 13:08:17 2024 ] 	Batch(5700/7879) done. Loss: 0.0158  lr:0.000001
[ Thu Jul  4 13:08:35 2024 ] 	Batch(5800/7879) done. Loss: 0.0398  lr:0.000001
[ Thu Jul  4 13:08:53 2024 ] 	Batch(5900/7879) done. Loss: 0.0468  lr:0.000001
[ Thu Jul  4 13:09:11 2024 ] 
Training: Epoch [98/120], Step [5999], Loss: 0.005481474567204714, Training Accuracy: 97.74583333333334
[ Thu Jul  4 13:09:11 2024 ] 	Batch(6000/7879) done. Loss: 0.0142  lr:0.000001
[ Thu Jul  4 13:09:29 2024 ] 	Batch(6100/7879) done. Loss: 0.0494  lr:0.000001
[ Thu Jul  4 13:09:47 2024 ] 	Batch(6200/7879) done. Loss: 0.4912  lr:0.000001
[ Thu Jul  4 13:10:05 2024 ] 	Batch(6300/7879) done. Loss: 0.0196  lr:0.000001
[ Thu Jul  4 13:10:23 2024 ] 	Batch(6400/7879) done. Loss: 0.0135  lr:0.000001
[ Thu Jul  4 13:10:42 2024 ] 
Training: Epoch [98/120], Step [6499], Loss: 0.008583209477365017, Training Accuracy: 97.74615384615385
[ Thu Jul  4 13:10:42 2024 ] 	Batch(6500/7879) done. Loss: 0.0604  lr:0.000001
[ Thu Jul  4 13:11:00 2024 ] 	Batch(6600/7879) done. Loss: 0.0507  lr:0.000001
[ Thu Jul  4 13:11:19 2024 ] 	Batch(6700/7879) done. Loss: 0.0522  lr:0.000001
[ Thu Jul  4 13:11:37 2024 ] 	Batch(6800/7879) done. Loss: 0.1125  lr:0.000001
[ Thu Jul  4 13:11:55 2024 ] 	Batch(6900/7879) done. Loss: 0.0076  lr:0.000001
[ Thu Jul  4 13:12:13 2024 ] 
Training: Epoch [98/120], Step [6999], Loss: 0.06843756139278412, Training Accuracy: 97.74107142857143
[ Thu Jul  4 13:12:13 2024 ] 	Batch(7000/7879) done. Loss: 0.0371  lr:0.000001
[ Thu Jul  4 13:12:31 2024 ] 	Batch(7100/7879) done. Loss: 0.0536  lr:0.000001
[ Thu Jul  4 13:12:49 2024 ] 	Batch(7200/7879) done. Loss: 0.0147  lr:0.000001
[ Thu Jul  4 13:13:07 2024 ] 	Batch(7300/7879) done. Loss: 0.0240  lr:0.000001
[ Thu Jul  4 13:13:25 2024 ] 	Batch(7400/7879) done. Loss: 0.0119  lr:0.000001
[ Thu Jul  4 13:13:43 2024 ] 
Training: Epoch [98/120], Step [7499], Loss: 0.01228987518697977, Training Accuracy: 97.75333333333333
[ Thu Jul  4 13:13:43 2024 ] 	Batch(7500/7879) done. Loss: 0.0148  lr:0.000001
[ Thu Jul  4 13:14:01 2024 ] 	Batch(7600/7879) done. Loss: 0.0120  lr:0.000001
[ Thu Jul  4 13:14:19 2024 ] 	Batch(7700/7879) done. Loss: 0.0031  lr:0.000001
[ Thu Jul  4 13:14:36 2024 ] 	Batch(7800/7879) done. Loss: 0.0760  lr:0.000001
[ Thu Jul  4 13:14:51 2024 ] 	Mean training loss: 0.0912.
[ Thu Jul  4 13:14:51 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 13:14:51 2024 ] Training epoch: 100
[ Thu Jul  4 13:14:51 2024 ] 	Batch(0/7879) done. Loss: 0.0263  lr:0.000001
[ Thu Jul  4 13:15:09 2024 ] 	Batch(100/7879) done. Loss: 0.0094  lr:0.000001
[ Thu Jul  4 13:15:27 2024 ] 	Batch(200/7879) done. Loss: 0.0321  lr:0.000001
[ Thu Jul  4 13:15:45 2024 ] 	Batch(300/7879) done. Loss: 0.0534  lr:0.000001
[ Thu Jul  4 13:16:03 2024 ] 	Batch(400/7879) done. Loss: 0.0196  lr:0.000001
[ Thu Jul  4 13:16:21 2024 ] 
Training: Epoch [99/120], Step [499], Loss: 0.015349739231169224, Training Accuracy: 97.82499999999999
[ Thu Jul  4 13:16:21 2024 ] 	Batch(500/7879) done. Loss: 0.0037  lr:0.000001
[ Thu Jul  4 13:16:39 2024 ] 	Batch(600/7879) done. Loss: 0.0114  lr:0.000001
[ Thu Jul  4 13:16:56 2024 ] 	Batch(700/7879) done. Loss: 0.0194  lr:0.000001
[ Thu Jul  4 13:17:14 2024 ] 	Batch(800/7879) done. Loss: 0.0972  lr:0.000001
[ Thu Jul  4 13:17:33 2024 ] 	Batch(900/7879) done. Loss: 0.0514  lr:0.000001
[ Thu Jul  4 13:17:50 2024 ] 
Training: Epoch [99/120], Step [999], Loss: 0.04736959934234619, Training Accuracy: 97.8125
[ Thu Jul  4 13:17:50 2024 ] 	Batch(1000/7879) done. Loss: 0.0747  lr:0.000001
[ Thu Jul  4 13:18:08 2024 ] 	Batch(1100/7879) done. Loss: 0.0886  lr:0.000001
[ Thu Jul  4 13:18:26 2024 ] 	Batch(1200/7879) done. Loss: 0.0451  lr:0.000001
[ Thu Jul  4 13:18:44 2024 ] 	Batch(1300/7879) done. Loss: 0.0277  lr:0.000001
[ Thu Jul  4 13:19:02 2024 ] 	Batch(1400/7879) done. Loss: 0.3718  lr:0.000001
[ Thu Jul  4 13:19:20 2024 ] 
Training: Epoch [99/120], Step [1499], Loss: 0.3681594729423523, Training Accuracy: 97.79166666666667
[ Thu Jul  4 13:19:20 2024 ] 	Batch(1500/7879) done. Loss: 0.2373  lr:0.000001
[ Thu Jul  4 13:19:38 2024 ] 	Batch(1600/7879) done. Loss: 0.0658  lr:0.000001
[ Thu Jul  4 13:19:56 2024 ] 	Batch(1700/7879) done. Loss: 0.1168  lr:0.000001
[ Thu Jul  4 13:20:14 2024 ] 	Batch(1800/7879) done. Loss: 0.0100  lr:0.000001
[ Thu Jul  4 13:20:32 2024 ] 	Batch(1900/7879) done. Loss: 0.0279  lr:0.000001
[ Thu Jul  4 13:20:49 2024 ] 
Training: Epoch [99/120], Step [1999], Loss: 0.021383045241236687, Training Accuracy: 97.75625
[ Thu Jul  4 13:20:50 2024 ] 	Batch(2000/7879) done. Loss: 0.2888  lr:0.000001
[ Thu Jul  4 13:21:08 2024 ] 	Batch(2100/7879) done. Loss: 0.0865  lr:0.000001
[ Thu Jul  4 13:21:27 2024 ] 	Batch(2200/7879) done. Loss: 0.1148  lr:0.000001
[ Thu Jul  4 13:21:45 2024 ] 	Batch(2300/7879) done. Loss: 0.0153  lr:0.000001
[ Thu Jul  4 13:22:04 2024 ] 	Batch(2400/7879) done. Loss: 0.0764  lr:0.000001
[ Thu Jul  4 13:22:22 2024 ] 
Training: Epoch [99/120], Step [2499], Loss: 0.011885669082403183, Training Accuracy: 97.615
[ Thu Jul  4 13:22:22 2024 ] 	Batch(2500/7879) done. Loss: 0.1832  lr:0.000001
[ Thu Jul  4 13:22:40 2024 ] 	Batch(2600/7879) done. Loss: 0.0416  lr:0.000001
[ Thu Jul  4 13:22:58 2024 ] 	Batch(2700/7879) done. Loss: 0.0044  lr:0.000001
[ Thu Jul  4 13:23:16 2024 ] 	Batch(2800/7879) done. Loss: 0.0157  lr:0.000001
[ Thu Jul  4 13:23:33 2024 ] 	Batch(2900/7879) done. Loss: 0.3825  lr:0.000001
[ Thu Jul  4 13:23:51 2024 ] 
Training: Epoch [99/120], Step [2999], Loss: 0.013101962395012379, Training Accuracy: 97.6
[ Thu Jul  4 13:23:51 2024 ] 	Batch(3000/7879) done. Loss: 0.0100  lr:0.000001
[ Thu Jul  4 13:24:09 2024 ] 	Batch(3100/7879) done. Loss: 0.0801  lr:0.000001
[ Thu Jul  4 13:24:27 2024 ] 	Batch(3200/7879) done. Loss: 0.0107  lr:0.000001
[ Thu Jul  4 13:24:45 2024 ] 	Batch(3300/7879) done. Loss: 0.2227  lr:0.000001
[ Thu Jul  4 13:25:03 2024 ] 	Batch(3400/7879) done. Loss: 0.2072  lr:0.000001
[ Thu Jul  4 13:25:20 2024 ] 
Training: Epoch [99/120], Step [3499], Loss: 0.16284947097301483, Training Accuracy: 97.59642857142858
[ Thu Jul  4 13:25:21 2024 ] 	Batch(3500/7879) done. Loss: 0.1340  lr:0.000001
[ Thu Jul  4 13:25:39 2024 ] 	Batch(3600/7879) done. Loss: 0.0137  lr:0.000001
[ Thu Jul  4 13:25:56 2024 ] 	Batch(3700/7879) done. Loss: 0.0328  lr:0.000001
[ Thu Jul  4 13:26:14 2024 ] 	Batch(3800/7879) done. Loss: 0.0141  lr:0.000001
[ Thu Jul  4 13:26:32 2024 ] 	Batch(3900/7879) done. Loss: 0.1954  lr:0.000001
[ Thu Jul  4 13:26:50 2024 ] 
Training: Epoch [99/120], Step [3999], Loss: 0.2636212110519409, Training Accuracy: 97.60625
[ Thu Jul  4 13:26:50 2024 ] 	Batch(4000/7879) done. Loss: 0.2333  lr:0.000001
[ Thu Jul  4 13:27:08 2024 ] 	Batch(4100/7879) done. Loss: 0.0018  lr:0.000001
[ Thu Jul  4 13:27:26 2024 ] 	Batch(4200/7879) done. Loss: 0.0740  lr:0.000001
[ Thu Jul  4 13:27:44 2024 ] 	Batch(4300/7879) done. Loss: 0.1017  lr:0.000001
[ Thu Jul  4 13:28:02 2024 ] 	Batch(4400/7879) done. Loss: 0.0141  lr:0.000001
[ Thu Jul  4 13:28:20 2024 ] 
Training: Epoch [99/120], Step [4499], Loss: 0.01976345106959343, Training Accuracy: 97.65555555555555
[ Thu Jul  4 13:28:20 2024 ] 	Batch(4500/7879) done. Loss: 0.0246  lr:0.000001
[ Thu Jul  4 13:28:38 2024 ] 	Batch(4600/7879) done. Loss: 0.1670  lr:0.000001
[ Thu Jul  4 13:28:55 2024 ] 	Batch(4700/7879) done. Loss: 0.0035  lr:0.000001
[ Thu Jul  4 13:29:13 2024 ] 	Batch(4800/7879) done. Loss: 0.2390  lr:0.000001
[ Thu Jul  4 13:29:31 2024 ] 	Batch(4900/7879) done. Loss: 0.0193  lr:0.000001
[ Thu Jul  4 13:29:49 2024 ] 
Training: Epoch [99/120], Step [4999], Loss: 0.006594858132302761, Training Accuracy: 97.67
[ Thu Jul  4 13:29:49 2024 ] 	Batch(5000/7879) done. Loss: 0.0159  lr:0.000001
[ Thu Jul  4 13:30:07 2024 ] 	Batch(5100/7879) done. Loss: 0.0458  lr:0.000001
[ Thu Jul  4 13:30:25 2024 ] 	Batch(5200/7879) done. Loss: 0.0040  lr:0.000001
[ Thu Jul  4 13:30:43 2024 ] 	Batch(5300/7879) done. Loss: 0.0459  lr:0.000001
[ Thu Jul  4 13:31:01 2024 ] 	Batch(5400/7879) done. Loss: 0.0154  lr:0.000001
[ Thu Jul  4 13:31:19 2024 ] 
Training: Epoch [99/120], Step [5499], Loss: 0.11537527292966843, Training Accuracy: 97.69090909090909
[ Thu Jul  4 13:31:19 2024 ] 	Batch(5500/7879) done. Loss: 0.0052  lr:0.000001
[ Thu Jul  4 13:31:37 2024 ] 	Batch(5600/7879) done. Loss: 0.2639  lr:0.000001
[ Thu Jul  4 13:31:55 2024 ] 	Batch(5700/7879) done. Loss: 0.0792  lr:0.000001
[ Thu Jul  4 13:32:13 2024 ] 	Batch(5800/7879) done. Loss: 0.0520  lr:0.000001
[ Thu Jul  4 13:32:31 2024 ] 	Batch(5900/7879) done. Loss: 0.0030  lr:0.000001
[ Thu Jul  4 13:32:48 2024 ] 
Training: Epoch [99/120], Step [5999], Loss: 0.16836468875408173, Training Accuracy: 97.68124999999999
[ Thu Jul  4 13:32:49 2024 ] 	Batch(6000/7879) done. Loss: 0.1522  lr:0.000001
[ Thu Jul  4 13:33:06 2024 ] 	Batch(6100/7879) done. Loss: 0.1176  lr:0.000001
[ Thu Jul  4 13:33:24 2024 ] 	Batch(6200/7879) done. Loss: 0.0325  lr:0.000001
[ Thu Jul  4 13:33:42 2024 ] 	Batch(6300/7879) done. Loss: 0.0218  lr:0.000001
[ Thu Jul  4 13:34:00 2024 ] 	Batch(6400/7879) done. Loss: 0.0218  lr:0.000001
[ Thu Jul  4 13:34:18 2024 ] 
Training: Epoch [99/120], Step [6499], Loss: 0.048314955085515976, Training Accuracy: 97.70576923076923
[ Thu Jul  4 13:34:18 2024 ] 	Batch(6500/7879) done. Loss: 0.0522  lr:0.000001
[ Thu Jul  4 13:34:36 2024 ] 	Batch(6600/7879) done. Loss: 0.1013  lr:0.000001
[ Thu Jul  4 13:34:54 2024 ] 	Batch(6700/7879) done. Loss: 0.1157  lr:0.000001
[ Thu Jul  4 13:35:12 2024 ] 	Batch(6800/7879) done. Loss: 0.0223  lr:0.000001
[ Thu Jul  4 13:35:30 2024 ] 	Batch(6900/7879) done. Loss: 0.0029  lr:0.000001
[ Thu Jul  4 13:35:47 2024 ] 
Training: Epoch [99/120], Step [6999], Loss: 0.055071987211704254, Training Accuracy: 97.71071428571429
[ Thu Jul  4 13:35:48 2024 ] 	Batch(7000/7879) done. Loss: 0.1881  lr:0.000001
[ Thu Jul  4 13:36:05 2024 ] 	Batch(7100/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul  4 13:36:24 2024 ] 	Batch(7200/7879) done. Loss: 0.0438  lr:0.000001
[ Thu Jul  4 13:36:42 2024 ] 	Batch(7300/7879) done. Loss: 0.0589  lr:0.000001
[ Thu Jul  4 13:36:59 2024 ] 	Batch(7400/7879) done. Loss: 0.3200  lr:0.000001
[ Thu Jul  4 13:37:17 2024 ] 
Training: Epoch [99/120], Step [7499], Loss: 0.012943947687745094, Training Accuracy: 97.72
[ Thu Jul  4 13:37:18 2024 ] 	Batch(7500/7879) done. Loss: 0.3151  lr:0.000001
[ Thu Jul  4 13:37:36 2024 ] 	Batch(7600/7879) done. Loss: 0.0179  lr:0.000001
[ Thu Jul  4 13:37:54 2024 ] 	Batch(7700/7879) done. Loss: 0.0430  lr:0.000001
[ Thu Jul  4 13:38:13 2024 ] 	Batch(7800/7879) done. Loss: 0.0097  lr:0.000001
[ Thu Jul  4 13:38:27 2024 ] 	Mean training loss: 0.0926.
[ Thu Jul  4 13:38:27 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 13:38:27 2024 ] Eval epoch: 100
[ Thu Jul  4 13:43:13 2024 ] 	Mean val loss of 6365 batches: 1.5558648661223182.
[ Thu Jul  4 13:43:13 2024 ] Training epoch: 101
[ Thu Jul  4 13:43:14 2024 ] 	Batch(0/7879) done. Loss: 0.0523  lr:0.000001
[ Thu Jul  4 13:43:32 2024 ] 	Batch(100/7879) done. Loss: 0.0120  lr:0.000001
[ Thu Jul  4 13:43:50 2024 ] 	Batch(200/7879) done. Loss: 0.0304  lr:0.000001
[ Thu Jul  4 13:44:08 2024 ] 	Batch(300/7879) done. Loss: 0.0389  lr:0.000001
[ Thu Jul  4 13:44:26 2024 ] 	Batch(400/7879) done. Loss: 0.0222  lr:0.000001
[ Thu Jul  4 13:44:43 2024 ] 
Training: Epoch [100/120], Step [499], Loss: 0.13086405396461487, Training Accuracy: 97.675
[ Thu Jul  4 13:44:43 2024 ] 	Batch(500/7879) done. Loss: 0.0094  lr:0.000001
[ Thu Jul  4 13:45:01 2024 ] 	Batch(600/7879) done. Loss: 0.0130  lr:0.000001
[ Thu Jul  4 13:45:19 2024 ] 	Batch(700/7879) done. Loss: 0.1424  lr:0.000001
[ Thu Jul  4 13:45:37 2024 ] 	Batch(800/7879) done. Loss: 0.1218  lr:0.000001
[ Thu Jul  4 13:45:55 2024 ] 	Batch(900/7879) done. Loss: 0.0871  lr:0.000001
[ Thu Jul  4 13:46:13 2024 ] 
Training: Epoch [100/120], Step [999], Loss: 0.10992059111595154, Training Accuracy: 97.575
[ Thu Jul  4 13:46:13 2024 ] 	Batch(1000/7879) done. Loss: 0.1778  lr:0.000001
[ Thu Jul  4 13:46:32 2024 ] 	Batch(1100/7879) done. Loss: 0.1696  lr:0.000001
[ Thu Jul  4 13:46:50 2024 ] 	Batch(1200/7879) done. Loss: 0.2121  lr:0.000001
[ Thu Jul  4 13:47:08 2024 ] 	Batch(1300/7879) done. Loss: 0.0019  lr:0.000001
[ Thu Jul  4 13:47:26 2024 ] 	Batch(1400/7879) done. Loss: 0.0002  lr:0.000001
[ Thu Jul  4 13:47:44 2024 ] 
Training: Epoch [100/120], Step [1499], Loss: 0.15436406433582306, Training Accuracy: 97.54166666666667
[ Thu Jul  4 13:47:44 2024 ] 	Batch(1500/7879) done. Loss: 0.0396  lr:0.000001
[ Thu Jul  4 13:48:02 2024 ] 	Batch(1600/7879) done. Loss: 0.0235  lr:0.000001
[ Thu Jul  4 13:48:20 2024 ] 	Batch(1700/7879) done. Loss: 0.0141  lr:0.000001
[ Thu Jul  4 13:48:38 2024 ] 	Batch(1800/7879) done. Loss: 0.0097  lr:0.000001
[ Thu Jul  4 13:48:56 2024 ] 	Batch(1900/7879) done. Loss: 0.0097  lr:0.000001
[ Thu Jul  4 13:49:13 2024 ] 
Training: Epoch [100/120], Step [1999], Loss: 0.0686478391289711, Training Accuracy: 97.6875
[ Thu Jul  4 13:49:14 2024 ] 	Batch(2000/7879) done. Loss: 0.0127  lr:0.000001
[ Thu Jul  4 13:49:32 2024 ] 	Batch(2100/7879) done. Loss: 0.1578  lr:0.000001
[ Thu Jul  4 13:49:49 2024 ] 	Batch(2200/7879) done. Loss: 0.0432  lr:0.000001
[ Thu Jul  4 13:50:07 2024 ] 	Batch(2300/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul  4 13:50:25 2024 ] 	Batch(2400/7879) done. Loss: 0.0806  lr:0.000001
[ Thu Jul  4 13:50:43 2024 ] 
Training: Epoch [100/120], Step [2499], Loss: 0.03906363993883133, Training Accuracy: 97.675
[ Thu Jul  4 13:50:43 2024 ] 	Batch(2500/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul  4 13:51:01 2024 ] 	Batch(2600/7879) done. Loss: 0.0869  lr:0.000001
[ Thu Jul  4 13:51:19 2024 ] 	Batch(2700/7879) done. Loss: 0.1212  lr:0.000001
[ Thu Jul  4 13:51:37 2024 ] 	Batch(2800/7879) done. Loss: 0.0435  lr:0.000001
[ Thu Jul  4 13:51:55 2024 ] 	Batch(2900/7879) done. Loss: 0.0090  lr:0.000001
[ Thu Jul  4 13:52:13 2024 ] 
Training: Epoch [100/120], Step [2999], Loss: 0.008009913377463818, Training Accuracy: 97.75
[ Thu Jul  4 13:52:13 2024 ] 	Batch(3000/7879) done. Loss: 0.0287  lr:0.000001
[ Thu Jul  4 13:52:31 2024 ] 	Batch(3100/7879) done. Loss: 0.0072  lr:0.000001
[ Thu Jul  4 13:52:49 2024 ] 	Batch(3200/7879) done. Loss: 0.0156  lr:0.000001
[ Thu Jul  4 13:53:07 2024 ] 	Batch(3300/7879) done. Loss: 0.0164  lr:0.000001
[ Thu Jul  4 13:53:25 2024 ] 	Batch(3400/7879) done. Loss: 0.1901  lr:0.000001
[ Thu Jul  4 13:53:42 2024 ] 
Training: Epoch [100/120], Step [3499], Loss: 0.013058966025710106, Training Accuracy: 97.71071428571429
[ Thu Jul  4 13:53:42 2024 ] 	Batch(3500/7879) done. Loss: 0.1159  lr:0.000001
[ Thu Jul  4 13:54:00 2024 ] 	Batch(3600/7879) done. Loss: 0.1385  lr:0.000001
[ Thu Jul  4 13:54:18 2024 ] 	Batch(3700/7879) done. Loss: 0.1348  lr:0.000001
[ Thu Jul  4 13:54:36 2024 ] 	Batch(3800/7879) done. Loss: 0.0663  lr:0.000001
[ Thu Jul  4 13:54:54 2024 ] 	Batch(3900/7879) done. Loss: 0.0088  lr:0.000001
[ Thu Jul  4 13:55:12 2024 ] 
Training: Epoch [100/120], Step [3999], Loss: 0.1211770623922348, Training Accuracy: 97.6875
[ Thu Jul  4 13:55:12 2024 ] 	Batch(4000/7879) done. Loss: 0.0130  lr:0.000001
[ Thu Jul  4 13:55:30 2024 ] 	Batch(4100/7879) done. Loss: 0.0767  lr:0.000001
[ Thu Jul  4 13:55:48 2024 ] 	Batch(4200/7879) done. Loss: 0.1969  lr:0.000001
[ Thu Jul  4 13:56:06 2024 ] 	Batch(4300/7879) done. Loss: 0.3588  lr:0.000001
[ Thu Jul  4 13:56:24 2024 ] 	Batch(4400/7879) done. Loss: 0.1011  lr:0.000001
[ Thu Jul  4 13:56:42 2024 ] 
Training: Epoch [100/120], Step [4499], Loss: 0.012638632208108902, Training Accuracy: 97.68333333333334
[ Thu Jul  4 13:56:42 2024 ] 	Batch(4500/7879) done. Loss: 0.1535  lr:0.000001
[ Thu Jul  4 13:57:00 2024 ] 	Batch(4600/7879) done. Loss: 0.0354  lr:0.000001
[ Thu Jul  4 13:57:18 2024 ] 	Batch(4700/7879) done. Loss: 0.0166  lr:0.000001
[ Thu Jul  4 13:57:36 2024 ] 	Batch(4800/7879) done. Loss: 0.0418  lr:0.000001
[ Thu Jul  4 13:57:54 2024 ] 	Batch(4900/7879) done. Loss: 0.0075  lr:0.000001
[ Thu Jul  4 13:58:11 2024 ] 
Training: Epoch [100/120], Step [4999], Loss: 0.048886749893426895, Training Accuracy: 97.7
[ Thu Jul  4 13:58:12 2024 ] 	Batch(5000/7879) done. Loss: 0.0349  lr:0.000001
[ Thu Jul  4 13:58:29 2024 ] 	Batch(5100/7879) done. Loss: 0.4825  lr:0.000001
[ Thu Jul  4 13:58:47 2024 ] 	Batch(5200/7879) done. Loss: 0.0307  lr:0.000001
[ Thu Jul  4 13:59:05 2024 ] 	Batch(5300/7879) done. Loss: 0.0331  lr:0.000001
[ Thu Jul  4 13:59:23 2024 ] 	Batch(5400/7879) done. Loss: 0.0215  lr:0.000001
[ Thu Jul  4 13:59:41 2024 ] 
Training: Epoch [100/120], Step [5499], Loss: 0.0270327590405941, Training Accuracy: 97.67954545454546
[ Thu Jul  4 13:59:41 2024 ] 	Batch(5500/7879) done. Loss: 0.0574  lr:0.000001
[ Thu Jul  4 13:59:59 2024 ] 	Batch(5600/7879) done. Loss: 0.0071  lr:0.000001
[ Thu Jul  4 14:00:17 2024 ] 	Batch(5700/7879) done. Loss: 0.0240  lr:0.000001
[ Thu Jul  4 14:00:35 2024 ] 	Batch(5800/7879) done. Loss: 0.0169  lr:0.000001
[ Thu Jul  4 14:00:53 2024 ] 	Batch(5900/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul  4 14:01:10 2024 ] 
Training: Epoch [100/120], Step [5999], Loss: 0.070731021463871, Training Accuracy: 97.65416666666667
[ Thu Jul  4 14:01:11 2024 ] 	Batch(6000/7879) done. Loss: 0.0140  lr:0.000001
[ Thu Jul  4 14:01:29 2024 ] 	Batch(6100/7879) done. Loss: 0.0351  lr:0.000001
[ Thu Jul  4 14:01:47 2024 ] 	Batch(6200/7879) done. Loss: 0.0251  lr:0.000001
[ Thu Jul  4 14:02:04 2024 ] 	Batch(6300/7879) done. Loss: 0.0059  lr:0.000001
[ Thu Jul  4 14:02:22 2024 ] 	Batch(6400/7879) done. Loss: 0.2553  lr:0.000001
[ Thu Jul  4 14:02:40 2024 ] 
Training: Epoch [100/120], Step [6499], Loss: 0.08875823765993118, Training Accuracy: 97.6423076923077
[ Thu Jul  4 14:02:40 2024 ] 	Batch(6500/7879) done. Loss: 0.2611  lr:0.000001
[ Thu Jul  4 14:02:58 2024 ] 	Batch(6600/7879) done. Loss: 0.0793  lr:0.000001
[ Thu Jul  4 14:03:16 2024 ] 	Batch(6700/7879) done. Loss: 0.0248  lr:0.000001
[ Thu Jul  4 14:03:34 2024 ] 	Batch(6800/7879) done. Loss: 0.0576  lr:0.000001
[ Thu Jul  4 14:03:52 2024 ] 	Batch(6900/7879) done. Loss: 0.0938  lr:0.000001
[ Thu Jul  4 14:04:10 2024 ] 
Training: Epoch [100/120], Step [6999], Loss: 0.015423913486301899, Training Accuracy: 97.63214285714285
[ Thu Jul  4 14:04:10 2024 ] 	Batch(7000/7879) done. Loss: 0.1425  lr:0.000001
[ Thu Jul  4 14:04:28 2024 ] 	Batch(7100/7879) done. Loss: 0.0124  lr:0.000001
[ Thu Jul  4 14:04:46 2024 ] 	Batch(7200/7879) done. Loss: 0.2079  lr:0.000001
[ Thu Jul  4 14:05:04 2024 ] 	Batch(7300/7879) done. Loss: 0.0530  lr:0.000001
[ Thu Jul  4 14:05:22 2024 ] 	Batch(7400/7879) done. Loss: 0.0433  lr:0.000001
[ Thu Jul  4 14:05:40 2024 ] 
Training: Epoch [100/120], Step [7499], Loss: 0.20292508602142334, Training Accuracy: 97.59833333333333
[ Thu Jul  4 14:05:40 2024 ] 	Batch(7500/7879) done. Loss: 0.0230  lr:0.000001
[ Thu Jul  4 14:05:58 2024 ] 	Batch(7600/7879) done. Loss: 0.1241  lr:0.000001
[ Thu Jul  4 14:06:16 2024 ] 	Batch(7700/7879) done. Loss: 0.0417  lr:0.000001
[ Thu Jul  4 14:06:33 2024 ] 	Batch(7800/7879) done. Loss: 0.0579  lr:0.000001
[ Thu Jul  4 14:06:47 2024 ] 	Mean training loss: 0.0952.
[ Thu Jul  4 14:06:47 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 14:06:48 2024 ] Training epoch: 102
[ Thu Jul  4 14:06:48 2024 ] 	Batch(0/7879) done. Loss: 0.2512  lr:0.000001
[ Thu Jul  4 14:07:06 2024 ] 	Batch(100/7879) done. Loss: 0.2170  lr:0.000001
[ Thu Jul  4 14:07:24 2024 ] 	Batch(200/7879) done. Loss: 0.0215  lr:0.000001
[ Thu Jul  4 14:07:42 2024 ] 	Batch(300/7879) done. Loss: 0.1398  lr:0.000001
[ Thu Jul  4 14:08:00 2024 ] 	Batch(400/7879) done. Loss: 0.3360  lr:0.000001
[ Thu Jul  4 14:08:17 2024 ] 
Training: Epoch [101/120], Step [499], Loss: 0.3290267586708069, Training Accuracy: 97.89999999999999
[ Thu Jul  4 14:08:17 2024 ] 	Batch(500/7879) done. Loss: 0.0769  lr:0.000001
[ Thu Jul  4 14:08:35 2024 ] 	Batch(600/7879) done. Loss: 0.0418  lr:0.000001
[ Thu Jul  4 14:08:53 2024 ] 	Batch(700/7879) done. Loss: 0.0876  lr:0.000001
[ Thu Jul  4 14:09:11 2024 ] 	Batch(800/7879) done. Loss: 0.0991  lr:0.000001
[ Thu Jul  4 14:09:29 2024 ] 	Batch(900/7879) done. Loss: 0.0095  lr:0.000001
[ Thu Jul  4 14:09:47 2024 ] 
Training: Epoch [101/120], Step [999], Loss: 0.07421202957630157, Training Accuracy: 97.78750000000001
[ Thu Jul  4 14:09:47 2024 ] 	Batch(1000/7879) done. Loss: 0.0758  lr:0.000001
[ Thu Jul  4 14:10:05 2024 ] 	Batch(1100/7879) done. Loss: 0.3575  lr:0.000001
[ Thu Jul  4 14:10:23 2024 ] 	Batch(1200/7879) done. Loss: 0.0254  lr:0.000001
[ Thu Jul  4 14:10:41 2024 ] 	Batch(1300/7879) done. Loss: 0.0112  lr:0.000001
[ Thu Jul  4 14:10:58 2024 ] 	Batch(1400/7879) done. Loss: 0.0126  lr:0.000001
[ Thu Jul  4 14:11:16 2024 ] 
Training: Epoch [101/120], Step [1499], Loss: 0.012067131698131561, Training Accuracy: 97.74166666666667
[ Thu Jul  4 14:11:16 2024 ] 	Batch(1500/7879) done. Loss: 0.0894  lr:0.000001
[ Thu Jul  4 14:11:34 2024 ] 	Batch(1600/7879) done. Loss: 0.1329  lr:0.000001
[ Thu Jul  4 14:11:52 2024 ] 	Batch(1700/7879) done. Loss: 0.0644  lr:0.000001
[ Thu Jul  4 14:12:10 2024 ] 	Batch(1800/7879) done. Loss: 0.1034  lr:0.000001
[ Thu Jul  4 14:12:28 2024 ] 	Batch(1900/7879) done. Loss: 0.0856  lr:0.000001
[ Thu Jul  4 14:12:45 2024 ] 
Training: Epoch [101/120], Step [1999], Loss: 0.14538425207138062, Training Accuracy: 97.74374999999999
[ Thu Jul  4 14:12:46 2024 ] 	Batch(2000/7879) done. Loss: 0.0337  lr:0.000001
[ Thu Jul  4 14:13:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0554  lr:0.000001
[ Thu Jul  4 14:13:21 2024 ] 	Batch(2200/7879) done. Loss: 0.2921  lr:0.000001
[ Thu Jul  4 14:13:39 2024 ] 	Batch(2300/7879) done. Loss: 0.2951  lr:0.000001
[ Thu Jul  4 14:13:57 2024 ] 	Batch(2400/7879) done. Loss: 0.0085  lr:0.000001
[ Thu Jul  4 14:14:15 2024 ] 
Training: Epoch [101/120], Step [2499], Loss: 0.11640828102827072, Training Accuracy: 97.66
[ Thu Jul  4 14:14:15 2024 ] 	Batch(2500/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul  4 14:14:33 2024 ] 	Batch(2600/7879) done. Loss: 0.1067  lr:0.000001
[ Thu Jul  4 14:14:52 2024 ] 	Batch(2700/7879) done. Loss: 0.0567  lr:0.000001
[ Thu Jul  4 14:15:10 2024 ] 	Batch(2800/7879) done. Loss: 0.0831  lr:0.000001
[ Thu Jul  4 14:15:28 2024 ] 	Batch(2900/7879) done. Loss: 0.3452  lr:0.000001
[ Thu Jul  4 14:15:46 2024 ] 
Training: Epoch [101/120], Step [2999], Loss: 0.1943332403898239, Training Accuracy: 97.6625
[ Thu Jul  4 14:15:46 2024 ] 	Batch(3000/7879) done. Loss: 0.0426  lr:0.000001
[ Thu Jul  4 14:16:04 2024 ] 	Batch(3100/7879) done. Loss: 0.2138  lr:0.000001
[ Thu Jul  4 14:16:22 2024 ] 	Batch(3200/7879) done. Loss: 0.2131  lr:0.000001
[ Thu Jul  4 14:16:40 2024 ] 	Batch(3300/7879) done. Loss: 0.2139  lr:0.000001
[ Thu Jul  4 14:16:57 2024 ] 	Batch(3400/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul  4 14:17:15 2024 ] 
Training: Epoch [101/120], Step [3499], Loss: 0.3323993682861328, Training Accuracy: 97.64642857142857
[ Thu Jul  4 14:17:15 2024 ] 	Batch(3500/7879) done. Loss: 0.1259  lr:0.000001
[ Thu Jul  4 14:17:33 2024 ] 	Batch(3600/7879) done. Loss: 0.0233  lr:0.000001
[ Thu Jul  4 14:17:52 2024 ] 	Batch(3700/7879) done. Loss: 0.0209  lr:0.000001
[ Thu Jul  4 14:18:10 2024 ] 	Batch(3800/7879) done. Loss: 0.0312  lr:0.000001
[ Thu Jul  4 14:18:29 2024 ] 	Batch(3900/7879) done. Loss: 0.2958  lr:0.000001
[ Thu Jul  4 14:18:47 2024 ] 
Training: Epoch [101/120], Step [3999], Loss: 0.07147180289030075, Training Accuracy: 97.66875
[ Thu Jul  4 14:18:47 2024 ] 	Batch(4000/7879) done. Loss: 0.0841  lr:0.000001
[ Thu Jul  4 14:19:05 2024 ] 	Batch(4100/7879) done. Loss: 0.0689  lr:0.000001
[ Thu Jul  4 14:19:23 2024 ] 	Batch(4200/7879) done. Loss: 0.0397  lr:0.000001
[ Thu Jul  4 14:19:41 2024 ] 	Batch(4300/7879) done. Loss: 0.0889  lr:0.000001
[ Thu Jul  4 14:19:59 2024 ] 	Batch(4400/7879) done. Loss: 0.0949  lr:0.000001
[ Thu Jul  4 14:20:17 2024 ] 
Training: Epoch [101/120], Step [4499], Loss: 0.01568686217069626, Training Accuracy: 97.66666666666667
[ Thu Jul  4 14:20:17 2024 ] 	Batch(4500/7879) done. Loss: 0.2761  lr:0.000001
[ Thu Jul  4 14:20:35 2024 ] 	Batch(4600/7879) done. Loss: 0.0698  lr:0.000001
[ Thu Jul  4 14:20:53 2024 ] 	Batch(4700/7879) done. Loss: 0.0026  lr:0.000001
[ Thu Jul  4 14:21:11 2024 ] 	Batch(4800/7879) done. Loss: 0.0664  lr:0.000001
[ Thu Jul  4 14:21:28 2024 ] 	Batch(4900/7879) done. Loss: 0.0460  lr:0.000001
[ Thu Jul  4 14:21:46 2024 ] 
Training: Epoch [101/120], Step [4999], Loss: 0.003930268809199333, Training Accuracy: 97.64500000000001
[ Thu Jul  4 14:21:46 2024 ] 	Batch(5000/7879) done. Loss: 0.1467  lr:0.000001
[ Thu Jul  4 14:22:04 2024 ] 	Batch(5100/7879) done. Loss: 0.1060  lr:0.000001
[ Thu Jul  4 14:22:22 2024 ] 	Batch(5200/7879) done. Loss: 0.2792  lr:0.000001
[ Thu Jul  4 14:22:40 2024 ] 	Batch(5300/7879) done. Loss: 0.0274  lr:0.000001
[ Thu Jul  4 14:22:58 2024 ] 	Batch(5400/7879) done. Loss: 0.0131  lr:0.000001
[ Thu Jul  4 14:23:16 2024 ] 
Training: Epoch [101/120], Step [5499], Loss: 0.28303152322769165, Training Accuracy: 97.66590909090908
[ Thu Jul  4 14:23:16 2024 ] 	Batch(5500/7879) done. Loss: 0.2130  lr:0.000001
[ Thu Jul  4 14:23:34 2024 ] 	Batch(5600/7879) done. Loss: 0.1571  lr:0.000001
[ Thu Jul  4 14:23:52 2024 ] 	Batch(5700/7879) done. Loss: 0.0200  lr:0.000001
[ Thu Jul  4 14:24:10 2024 ] 	Batch(5800/7879) done. Loss: 0.0296  lr:0.000001
[ Thu Jul  4 14:24:28 2024 ] 	Batch(5900/7879) done. Loss: 0.0226  lr:0.000001
[ Thu Jul  4 14:24:45 2024 ] 
Training: Epoch [101/120], Step [5999], Loss: 0.010316378436982632, Training Accuracy: 97.70625
[ Thu Jul  4 14:24:46 2024 ] 	Batch(6000/7879) done. Loss: 0.0060  lr:0.000001
[ Thu Jul  4 14:25:04 2024 ] 	Batch(6100/7879) done. Loss: 0.0149  lr:0.000001
[ Thu Jul  4 14:25:22 2024 ] 	Batch(6200/7879) done. Loss: 0.1170  lr:0.000001
[ Thu Jul  4 14:25:39 2024 ] 	Batch(6300/7879) done. Loss: 0.0085  lr:0.000001
[ Thu Jul  4 14:25:57 2024 ] 	Batch(6400/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul  4 14:26:15 2024 ] 
Training: Epoch [101/120], Step [6499], Loss: 0.03897278383374214, Training Accuracy: 97.69038461538462
[ Thu Jul  4 14:26:16 2024 ] 	Batch(6500/7879) done. Loss: 0.1108  lr:0.000001
[ Thu Jul  4 14:26:34 2024 ] 	Batch(6600/7879) done. Loss: 0.0525  lr:0.000001
[ Thu Jul  4 14:26:52 2024 ] 	Batch(6700/7879) done. Loss: 0.0301  lr:0.000001
[ Thu Jul  4 14:27:10 2024 ] 	Batch(6800/7879) done. Loss: 0.0823  lr:0.000001
[ Thu Jul  4 14:27:28 2024 ] 	Batch(6900/7879) done. Loss: 0.1139  lr:0.000001
[ Thu Jul  4 14:27:46 2024 ] 
Training: Epoch [101/120], Step [6999], Loss: 0.018406707793474197, Training Accuracy: 97.69821428571429
[ Thu Jul  4 14:27:46 2024 ] 	Batch(7000/7879) done. Loss: 0.0506  lr:0.000001
[ Thu Jul  4 14:28:04 2024 ] 	Batch(7100/7879) done. Loss: 0.0041  lr:0.000001
[ Thu Jul  4 14:28:22 2024 ] 	Batch(7200/7879) done. Loss: 0.1881  lr:0.000001
[ Thu Jul  4 14:28:40 2024 ] 	Batch(7300/7879) done. Loss: 0.1089  lr:0.000001
[ Thu Jul  4 14:28:58 2024 ] 	Batch(7400/7879) done. Loss: 0.3411  lr:0.000001
[ Thu Jul  4 14:29:16 2024 ] 
Training: Epoch [101/120], Step [7499], Loss: 0.036500234156847, Training Accuracy: 97.7
[ Thu Jul  4 14:29:16 2024 ] 	Batch(7500/7879) done. Loss: 0.0265  lr:0.000001
[ Thu Jul  4 14:29:34 2024 ] 	Batch(7600/7879) done. Loss: 0.0025  lr:0.000001
[ Thu Jul  4 14:29:52 2024 ] 	Batch(7700/7879) done. Loss: 0.0614  lr:0.000001
[ Thu Jul  4 14:30:10 2024 ] 	Batch(7800/7879) done. Loss: 0.0099  lr:0.000001
[ Thu Jul  4 14:30:24 2024 ] 	Mean training loss: 0.0938.
[ Thu Jul  4 14:30:24 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 14:30:24 2024 ] Training epoch: 103
[ Thu Jul  4 14:30:24 2024 ] 	Batch(0/7879) done. Loss: 0.0532  lr:0.000001
[ Thu Jul  4 14:30:43 2024 ] 	Batch(100/7879) done. Loss: 0.0153  lr:0.000001
[ Thu Jul  4 14:31:01 2024 ] 	Batch(200/7879) done. Loss: 0.0287  lr:0.000001
[ Thu Jul  4 14:31:20 2024 ] 	Batch(300/7879) done. Loss: 0.0203  lr:0.000001
[ Thu Jul  4 14:31:38 2024 ] 	Batch(400/7879) done. Loss: 0.3306  lr:0.000001
[ Thu Jul  4 14:31:56 2024 ] 
Training: Epoch [102/120], Step [499], Loss: 0.24336764216423035, Training Accuracy: 97.375
[ Thu Jul  4 14:31:56 2024 ] 	Batch(500/7879) done. Loss: 0.0031  lr:0.000001
[ Thu Jul  4 14:32:14 2024 ] 	Batch(600/7879) done. Loss: 0.1002  lr:0.000001
[ Thu Jul  4 14:32:32 2024 ] 	Batch(700/7879) done. Loss: 0.0438  lr:0.000001
[ Thu Jul  4 14:32:50 2024 ] 	Batch(800/7879) done. Loss: 0.1015  lr:0.000001
[ Thu Jul  4 14:33:08 2024 ] 	Batch(900/7879) done. Loss: 0.0128  lr:0.000001
[ Thu Jul  4 14:33:25 2024 ] 
Training: Epoch [102/120], Step [999], Loss: 0.10940185189247131, Training Accuracy: 97.775
[ Thu Jul  4 14:33:26 2024 ] 	Batch(1000/7879) done. Loss: 0.0885  lr:0.000001
[ Thu Jul  4 14:33:43 2024 ] 	Batch(1100/7879) done. Loss: 0.0040  lr:0.000001
[ Thu Jul  4 14:34:01 2024 ] 	Batch(1200/7879) done. Loss: 0.0021  lr:0.000001
[ Thu Jul  4 14:34:19 2024 ] 	Batch(1300/7879) done. Loss: 0.0145  lr:0.000001
[ Thu Jul  4 14:34:37 2024 ] 	Batch(1400/7879) done. Loss: 0.0065  lr:0.000001
[ Thu Jul  4 14:34:55 2024 ] 
Training: Epoch [102/120], Step [1499], Loss: 0.006659252103418112, Training Accuracy: 97.83333333333334
[ Thu Jul  4 14:34:55 2024 ] 	Batch(1500/7879) done. Loss: 0.0601  lr:0.000001
[ Thu Jul  4 14:35:13 2024 ] 	Batch(1600/7879) done. Loss: 0.0768  lr:0.000001
[ Thu Jul  4 14:35:31 2024 ] 	Batch(1700/7879) done. Loss: 0.0196  lr:0.000001
[ Thu Jul  4 14:35:50 2024 ] 	Batch(1800/7879) done. Loss: 0.0120  lr:0.000001
[ Thu Jul  4 14:36:08 2024 ] 	Batch(1900/7879) done. Loss: 0.3825  lr:0.000001
[ Thu Jul  4 14:36:27 2024 ] 
Training: Epoch [102/120], Step [1999], Loss: 0.015658657997846603, Training Accuracy: 97.89375
[ Thu Jul  4 14:36:27 2024 ] 	Batch(2000/7879) done. Loss: 0.1021  lr:0.000001
[ Thu Jul  4 14:36:46 2024 ] 	Batch(2100/7879) done. Loss: 0.0569  lr:0.000001
[ Thu Jul  4 14:37:04 2024 ] 	Batch(2200/7879) done. Loss: 0.0653  lr:0.000001
[ Thu Jul  4 14:37:23 2024 ] 	Batch(2300/7879) done. Loss: 0.0112  lr:0.000001
[ Thu Jul  4 14:37:41 2024 ] 	Batch(2400/7879) done. Loss: 0.0074  lr:0.000001
[ Thu Jul  4 14:37:59 2024 ] 
Training: Epoch [102/120], Step [2499], Loss: 0.03967482969164848, Training Accuracy: 97.87
[ Thu Jul  4 14:37:59 2024 ] 	Batch(2500/7879) done. Loss: 0.2596  lr:0.000001
[ Thu Jul  4 14:38:17 2024 ] 	Batch(2600/7879) done. Loss: 0.0768  lr:0.000001
[ Thu Jul  4 14:38:35 2024 ] 	Batch(2700/7879) done. Loss: 0.0023  lr:0.000001
[ Thu Jul  4 14:38:53 2024 ] 	Batch(2800/7879) done. Loss: 0.0058  lr:0.000001
[ Thu Jul  4 14:39:11 2024 ] 	Batch(2900/7879) done. Loss: 0.0111  lr:0.000001
[ Thu Jul  4 14:39:29 2024 ] 
Training: Epoch [102/120], Step [2999], Loss: 0.07654916495084763, Training Accuracy: 97.79583333333333
[ Thu Jul  4 14:39:29 2024 ] 	Batch(3000/7879) done. Loss: 0.0185  lr:0.000001
[ Thu Jul  4 14:39:47 2024 ] 	Batch(3100/7879) done. Loss: 0.0272  lr:0.000001
[ Thu Jul  4 14:40:05 2024 ] 	Batch(3200/7879) done. Loss: 0.1461  lr:0.000001
[ Thu Jul  4 14:40:23 2024 ] 	Batch(3300/7879) done. Loss: 0.2326  lr:0.000001
[ Thu Jul  4 14:40:41 2024 ] 	Batch(3400/7879) done. Loss: 0.1049  lr:0.000001
[ Thu Jul  4 14:40:59 2024 ] 
Training: Epoch [102/120], Step [3499], Loss: 0.0674339160323143, Training Accuracy: 97.75357142857143
[ Thu Jul  4 14:40:59 2024 ] 	Batch(3500/7879) done. Loss: 0.1620  lr:0.000001
[ Thu Jul  4 14:41:17 2024 ] 	Batch(3600/7879) done. Loss: 0.0289  lr:0.000001
[ Thu Jul  4 14:41:36 2024 ] 	Batch(3700/7879) done. Loss: 0.0075  lr:0.000001
[ Thu Jul  4 14:41:54 2024 ] 	Batch(3800/7879) done. Loss: 0.0488  lr:0.000001
[ Thu Jul  4 14:42:13 2024 ] 	Batch(3900/7879) done. Loss: 0.0373  lr:0.000001
[ Thu Jul  4 14:42:31 2024 ] 
Training: Epoch [102/120], Step [3999], Loss: 0.13039450347423553, Training Accuracy: 97.746875
[ Thu Jul  4 14:42:31 2024 ] 	Batch(4000/7879) done. Loss: 0.0766  lr:0.000001
[ Thu Jul  4 14:42:49 2024 ] 	Batch(4100/7879) done. Loss: 0.0153  lr:0.000001
[ Thu Jul  4 14:43:07 2024 ] 	Batch(4200/7879) done. Loss: 0.0758  lr:0.000001
[ Thu Jul  4 14:43:25 2024 ] 	Batch(4300/7879) done. Loss: 0.0083  lr:0.000001
[ Thu Jul  4 14:43:43 2024 ] 	Batch(4400/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul  4 14:44:01 2024 ] 
Training: Epoch [102/120], Step [4499], Loss: 0.08996079117059708, Training Accuracy: 97.75833333333334
[ Thu Jul  4 14:44:01 2024 ] 	Batch(4500/7879) done. Loss: 0.0601  lr:0.000001
[ Thu Jul  4 14:44:19 2024 ] 	Batch(4600/7879) done. Loss: 0.0579  lr:0.000001
[ Thu Jul  4 14:44:37 2024 ] 	Batch(4700/7879) done. Loss: 0.1280  lr:0.000001
[ Thu Jul  4 14:44:55 2024 ] 	Batch(4800/7879) done. Loss: 0.0112  lr:0.000001
[ Thu Jul  4 14:45:12 2024 ] 	Batch(4900/7879) done. Loss: 0.0425  lr:0.000001
[ Thu Jul  4 14:45:30 2024 ] 
Training: Epoch [102/120], Step [4999], Loss: 0.002261224901303649, Training Accuracy: 97.7175
[ Thu Jul  4 14:45:30 2024 ] 	Batch(5000/7879) done. Loss: 0.0404  lr:0.000001
[ Thu Jul  4 14:45:48 2024 ] 	Batch(5100/7879) done. Loss: 0.0698  lr:0.000001
[ Thu Jul  4 14:46:06 2024 ] 	Batch(5200/7879) done. Loss: 0.0916  lr:0.000001
[ Thu Jul  4 14:46:24 2024 ] 	Batch(5300/7879) done. Loss: 0.0049  lr:0.000001
[ Thu Jul  4 14:46:42 2024 ] 	Batch(5400/7879) done. Loss: 0.0374  lr:0.000001
[ Thu Jul  4 14:46:59 2024 ] 
Training: Epoch [102/120], Step [5499], Loss: 0.006663878448307514, Training Accuracy: 97.71818181818182
[ Thu Jul  4 14:47:00 2024 ] 	Batch(5500/7879) done. Loss: 0.0199  lr:0.000001
[ Thu Jul  4 14:47:18 2024 ] 	Batch(5600/7879) done. Loss: 0.0049  lr:0.000001
[ Thu Jul  4 14:47:35 2024 ] 	Batch(5700/7879) done. Loss: 0.0132  lr:0.000001
[ Thu Jul  4 14:47:53 2024 ] 	Batch(5800/7879) done. Loss: 0.2956  lr:0.000001
[ Thu Jul  4 14:48:11 2024 ] 	Batch(5900/7879) done. Loss: 0.0743  lr:0.000001
[ Thu Jul  4 14:48:29 2024 ] 
Training: Epoch [102/120], Step [5999], Loss: 0.03598302975296974, Training Accuracy: 97.68541666666667
[ Thu Jul  4 14:48:29 2024 ] 	Batch(6000/7879) done. Loss: 0.4749  lr:0.000001
[ Thu Jul  4 14:48:47 2024 ] 	Batch(6100/7879) done. Loss: 0.0060  lr:0.000001
[ Thu Jul  4 14:49:05 2024 ] 	Batch(6200/7879) done. Loss: 0.0200  lr:0.000001
[ Thu Jul  4 14:49:23 2024 ] 	Batch(6300/7879) done. Loss: 0.0626  lr:0.000001
[ Thu Jul  4 14:49:41 2024 ] 	Batch(6400/7879) done. Loss: 0.0766  lr:0.000001
[ Thu Jul  4 14:49:58 2024 ] 
Training: Epoch [102/120], Step [6499], Loss: 0.047574762254953384, Training Accuracy: 97.675
[ Thu Jul  4 14:49:59 2024 ] 	Batch(6500/7879) done. Loss: 0.0013  lr:0.000001
[ Thu Jul  4 14:50:16 2024 ] 	Batch(6600/7879) done. Loss: 0.0823  lr:0.000001
[ Thu Jul  4 14:50:34 2024 ] 	Batch(6700/7879) done. Loss: 0.0186  lr:0.000001
[ Thu Jul  4 14:50:52 2024 ] 	Batch(6800/7879) done. Loss: 0.0150  lr:0.000001
[ Thu Jul  4 14:51:11 2024 ] 	Batch(6900/7879) done. Loss: 0.0240  lr:0.000001
[ Thu Jul  4 14:51:29 2024 ] 
Training: Epoch [102/120], Step [6999], Loss: 0.006753898225724697, Training Accuracy: 97.6875
[ Thu Jul  4 14:51:29 2024 ] 	Batch(7000/7879) done. Loss: 0.0162  lr:0.000001
[ Thu Jul  4 14:51:48 2024 ] 	Batch(7100/7879) done. Loss: 0.0932  lr:0.000001
[ Thu Jul  4 14:52:06 2024 ] 	Batch(7200/7879) done. Loss: 0.0326  lr:0.000001
[ Thu Jul  4 14:52:25 2024 ] 	Batch(7300/7879) done. Loss: 0.0935  lr:0.000001
[ Thu Jul  4 14:52:43 2024 ] 	Batch(7400/7879) done. Loss: 0.0263  lr:0.000001
[ Thu Jul  4 14:53:00 2024 ] 
Training: Epoch [102/120], Step [7499], Loss: 0.02060331217944622, Training Accuracy: 97.72166666666666
[ Thu Jul  4 14:53:01 2024 ] 	Batch(7500/7879) done. Loss: 0.0404  lr:0.000001
[ Thu Jul  4 14:53:19 2024 ] 	Batch(7600/7879) done. Loss: 0.0037  lr:0.000001
[ Thu Jul  4 14:53:36 2024 ] 	Batch(7700/7879) done. Loss: 0.1684  lr:0.000001
[ Thu Jul  4 14:53:54 2024 ] 	Batch(7800/7879) done. Loss: 0.0069  lr:0.000001
[ Thu Jul  4 14:54:08 2024 ] 	Mean training loss: 0.0936.
[ Thu Jul  4 14:54:08 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 14:54:09 2024 ] Training epoch: 104
[ Thu Jul  4 14:54:09 2024 ] 	Batch(0/7879) done. Loss: 0.0291  lr:0.000001
[ Thu Jul  4 14:54:27 2024 ] 	Batch(100/7879) done. Loss: 0.0357  lr:0.000001
[ Thu Jul  4 14:54:45 2024 ] 	Batch(200/7879) done. Loss: 0.0259  lr:0.000001
[ Thu Jul  4 14:55:03 2024 ] 	Batch(300/7879) done. Loss: 0.2901  lr:0.000001
[ Thu Jul  4 14:55:21 2024 ] 	Batch(400/7879) done. Loss: 0.0572  lr:0.000001
[ Thu Jul  4 14:55:38 2024 ] 
Training: Epoch [103/120], Step [499], Loss: 0.019905414432287216, Training Accuracy: 97.675
[ Thu Jul  4 14:55:39 2024 ] 	Batch(500/7879) done. Loss: 0.0211  lr:0.000001
[ Thu Jul  4 14:55:56 2024 ] 	Batch(600/7879) done. Loss: 0.0352  lr:0.000001
[ Thu Jul  4 14:56:14 2024 ] 	Batch(700/7879) done. Loss: 0.0427  lr:0.000001
[ Thu Jul  4 14:56:32 2024 ] 	Batch(800/7879) done. Loss: 0.0159  lr:0.000001
[ Thu Jul  4 14:56:50 2024 ] 	Batch(900/7879) done. Loss: 0.0232  lr:0.000001
[ Thu Jul  4 14:57:08 2024 ] 
Training: Epoch [103/120], Step [999], Loss: 0.07801748067140579, Training Accuracy: 97.85000000000001
[ Thu Jul  4 14:57:08 2024 ] 	Batch(1000/7879) done. Loss: 0.2262  lr:0.000001
[ Thu Jul  4 14:57:26 2024 ] 	Batch(1100/7879) done. Loss: 0.0397  lr:0.000001
[ Thu Jul  4 14:57:44 2024 ] 	Batch(1200/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul  4 14:58:02 2024 ] 	Batch(1300/7879) done. Loss: 0.0672  lr:0.000001
[ Thu Jul  4 14:58:20 2024 ] 	Batch(1400/7879) done. Loss: 0.0529  lr:0.000001
[ Thu Jul  4 14:58:37 2024 ] 
Training: Epoch [103/120], Step [1499], Loss: 0.3627760708332062, Training Accuracy: 97.65833333333333
[ Thu Jul  4 14:58:38 2024 ] 	Batch(1500/7879) done. Loss: 0.1043  lr:0.000001
[ Thu Jul  4 14:58:56 2024 ] 	Batch(1600/7879) done. Loss: 0.1603  lr:0.000001
[ Thu Jul  4 14:59:14 2024 ] 	Batch(1700/7879) done. Loss: 0.0184  lr:0.000001
[ Thu Jul  4 14:59:32 2024 ] 	Batch(1800/7879) done. Loss: 0.0097  lr:0.000001
[ Thu Jul  4 14:59:49 2024 ] 	Batch(1900/7879) done. Loss: 0.1124  lr:0.000001
[ Thu Jul  4 15:00:07 2024 ] 
Training: Epoch [103/120], Step [1999], Loss: 0.006456274073570967, Training Accuracy: 97.68124999999999
[ Thu Jul  4 15:00:07 2024 ] 	Batch(2000/7879) done. Loss: 0.0902  lr:0.000001
[ Thu Jul  4 15:00:26 2024 ] 	Batch(2100/7879) done. Loss: 0.0040  lr:0.000001
[ Thu Jul  4 15:00:44 2024 ] 	Batch(2200/7879) done. Loss: 0.0267  lr:0.000001
[ Thu Jul  4 15:01:03 2024 ] 	Batch(2300/7879) done. Loss: 0.0530  lr:0.000001
[ Thu Jul  4 15:01:22 2024 ] 	Batch(2400/7879) done. Loss: 0.0185  lr:0.000001
[ Thu Jul  4 15:01:40 2024 ] 
Training: Epoch [103/120], Step [2499], Loss: 0.010753730311989784, Training Accuracy: 97.68
[ Thu Jul  4 15:01:40 2024 ] 	Batch(2500/7879) done. Loss: 0.2834  lr:0.000001
[ Thu Jul  4 15:01:58 2024 ] 	Batch(2600/7879) done. Loss: 0.5211  lr:0.000001
[ Thu Jul  4 15:02:16 2024 ] 	Batch(2700/7879) done. Loss: 0.0034  lr:0.000001
[ Thu Jul  4 15:02:34 2024 ] 	Batch(2800/7879) done. Loss: 0.0599  lr:0.000001
[ Thu Jul  4 15:02:52 2024 ] 	Batch(2900/7879) done. Loss: 0.0153  lr:0.000001
[ Thu Jul  4 15:03:11 2024 ] 
Training: Epoch [103/120], Step [2999], Loss: 0.2681567966938019, Training Accuracy: 97.70833333333333
[ Thu Jul  4 15:03:11 2024 ] 	Batch(3000/7879) done. Loss: 0.0301  lr:0.000001
[ Thu Jul  4 15:03:29 2024 ] 	Batch(3100/7879) done. Loss: 0.0475  lr:0.000001
[ Thu Jul  4 15:03:48 2024 ] 	Batch(3200/7879) done. Loss: 0.0132  lr:0.000001
[ Thu Jul  4 15:04:07 2024 ] 	Batch(3300/7879) done. Loss: 0.2067  lr:0.000001
[ Thu Jul  4 15:04:25 2024 ] 	Batch(3400/7879) done. Loss: 0.1361  lr:0.000001
[ Thu Jul  4 15:04:42 2024 ] 
Training: Epoch [103/120], Step [3499], Loss: 0.2556914687156677, Training Accuracy: 97.68928571428572
[ Thu Jul  4 15:04:42 2024 ] 	Batch(3500/7879) done. Loss: 0.0763  lr:0.000001
[ Thu Jul  4 15:05:00 2024 ] 	Batch(3600/7879) done. Loss: 0.0017  lr:0.000001
[ Thu Jul  4 15:05:18 2024 ] 	Batch(3700/7879) done. Loss: 0.0057  lr:0.000001
[ Thu Jul  4 15:05:36 2024 ] 	Batch(3800/7879) done. Loss: 0.0203  lr:0.000001
[ Thu Jul  4 15:05:54 2024 ] 	Batch(3900/7879) done. Loss: 0.0023  lr:0.000001
[ Thu Jul  4 15:06:12 2024 ] 
Training: Epoch [103/120], Step [3999], Loss: 0.04345283657312393, Training Accuracy: 97.68124999999999
[ Thu Jul  4 15:06:12 2024 ] 	Batch(4000/7879) done. Loss: 0.0059  lr:0.000001
[ Thu Jul  4 15:06:30 2024 ] 	Batch(4100/7879) done. Loss: 0.0145  lr:0.000001
[ Thu Jul  4 15:06:48 2024 ] 	Batch(4200/7879) done. Loss: 0.0728  lr:0.000001
[ Thu Jul  4 15:07:06 2024 ] 	Batch(4300/7879) done. Loss: 0.0036  lr:0.000001
[ Thu Jul  4 15:07:24 2024 ] 	Batch(4400/7879) done. Loss: 0.1821  lr:0.000001
[ Thu Jul  4 15:07:41 2024 ] 
Training: Epoch [103/120], Step [4499], Loss: 0.06742984056472778, Training Accuracy: 97.6861111111111
[ Thu Jul  4 15:07:42 2024 ] 	Batch(4500/7879) done. Loss: 0.1181  lr:0.000001
[ Thu Jul  4 15:07:59 2024 ] 	Batch(4600/7879) done. Loss: 0.0827  lr:0.000001
[ Thu Jul  4 15:08:17 2024 ] 	Batch(4700/7879) done. Loss: 0.0341  lr:0.000001
[ Thu Jul  4 15:08:35 2024 ] 	Batch(4800/7879) done. Loss: 0.0411  lr:0.000001
[ Thu Jul  4 15:08:53 2024 ] 	Batch(4900/7879) done. Loss: 0.0180  lr:0.000001
[ Thu Jul  4 15:09:11 2024 ] 
Training: Epoch [103/120], Step [4999], Loss: 0.09482928365468979, Training Accuracy: 97.7075
[ Thu Jul  4 15:09:11 2024 ] 	Batch(5000/7879) done. Loss: 0.0303  lr:0.000001
[ Thu Jul  4 15:09:29 2024 ] 	Batch(5100/7879) done. Loss: 0.0282  lr:0.000001
[ Thu Jul  4 15:09:47 2024 ] 	Batch(5200/7879) done. Loss: 0.0660  lr:0.000001
[ Thu Jul  4 15:10:05 2024 ] 	Batch(5300/7879) done. Loss: 0.0471  lr:0.000001
[ Thu Jul  4 15:10:23 2024 ] 	Batch(5400/7879) done. Loss: 0.0257  lr:0.000001
[ Thu Jul  4 15:10:40 2024 ] 
Training: Epoch [103/120], Step [5499], Loss: 0.07127765566110611, Training Accuracy: 97.7
[ Thu Jul  4 15:10:41 2024 ] 	Batch(5500/7879) done. Loss: 0.0975  lr:0.000001
[ Thu Jul  4 15:10:59 2024 ] 	Batch(5600/7879) done. Loss: 0.0986  lr:0.000001
[ Thu Jul  4 15:11:16 2024 ] 	Batch(5700/7879) done. Loss: 0.0264  lr:0.000001
[ Thu Jul  4 15:11:34 2024 ] 	Batch(5800/7879) done. Loss: 0.0158  lr:0.000001
[ Thu Jul  4 15:11:52 2024 ] 	Batch(5900/7879) done. Loss: 0.0147  lr:0.000001
[ Thu Jul  4 15:12:10 2024 ] 
Training: Epoch [103/120], Step [5999], Loss: 0.006425740197300911, Training Accuracy: 97.71041666666666
[ Thu Jul  4 15:12:10 2024 ] 	Batch(6000/7879) done. Loss: 0.1010  lr:0.000001
[ Thu Jul  4 15:12:28 2024 ] 	Batch(6100/7879) done. Loss: 0.1573  lr:0.000001
[ Thu Jul  4 15:12:46 2024 ] 	Batch(6200/7879) done. Loss: 0.1958  lr:0.000001
[ Thu Jul  4 15:13:04 2024 ] 	Batch(6300/7879) done. Loss: 0.0677  lr:0.000001
[ Thu Jul  4 15:13:22 2024 ] 	Batch(6400/7879) done. Loss: 0.0184  lr:0.000001
[ Thu Jul  4 15:13:40 2024 ] 
Training: Epoch [103/120], Step [6499], Loss: 0.006147462874650955, Training Accuracy: 97.70769230769231
[ Thu Jul  4 15:13:40 2024 ] 	Batch(6500/7879) done. Loss: 0.2187  lr:0.000001
[ Thu Jul  4 15:13:58 2024 ] 	Batch(6600/7879) done. Loss: 0.0216  lr:0.000001
[ Thu Jul  4 15:14:15 2024 ] 	Batch(6700/7879) done. Loss: 0.1024  lr:0.000001
[ Thu Jul  4 15:14:33 2024 ] 	Batch(6800/7879) done. Loss: 0.0094  lr:0.000001
[ Thu Jul  4 15:14:51 2024 ] 	Batch(6900/7879) done. Loss: 0.0247  lr:0.000001
[ Thu Jul  4 15:15:09 2024 ] 
Training: Epoch [103/120], Step [6999], Loss: 0.02395104244351387, Training Accuracy: 97.70714285714286
[ Thu Jul  4 15:15:09 2024 ] 	Batch(7000/7879) done. Loss: 0.0723  lr:0.000001
[ Thu Jul  4 15:15:27 2024 ] 	Batch(7100/7879) done. Loss: 0.0156  lr:0.000001
[ Thu Jul  4 15:15:45 2024 ] 	Batch(7200/7879) done. Loss: 0.0776  lr:0.000001
[ Thu Jul  4 15:16:03 2024 ] 	Batch(7300/7879) done. Loss: 0.0663  lr:0.000001
[ Thu Jul  4 15:16:21 2024 ] 	Batch(7400/7879) done. Loss: 0.0167  lr:0.000001
[ Thu Jul  4 15:16:39 2024 ] 
Training: Epoch [103/120], Step [7499], Loss: 0.1172637864947319, Training Accuracy: 97.71166666666666
[ Thu Jul  4 15:16:39 2024 ] 	Batch(7500/7879) done. Loss: 0.0837  lr:0.000001
[ Thu Jul  4 15:16:57 2024 ] 	Batch(7600/7879) done. Loss: 0.0696  lr:0.000001
[ Thu Jul  4 15:17:15 2024 ] 	Batch(7700/7879) done. Loss: 0.1003  lr:0.000001
[ Thu Jul  4 15:17:34 2024 ] 	Batch(7800/7879) done. Loss: 0.0916  lr:0.000001
[ Thu Jul  4 15:17:48 2024 ] 	Mean training loss: 0.0931.
[ Thu Jul  4 15:17:48 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 15:17:49 2024 ] Training epoch: 105
[ Thu Jul  4 15:17:49 2024 ] 	Batch(0/7879) done. Loss: 0.0912  lr:0.000001
[ Thu Jul  4 15:18:08 2024 ] 	Batch(100/7879) done. Loss: 0.0818  lr:0.000001
[ Thu Jul  4 15:18:26 2024 ] 	Batch(200/7879) done. Loss: 0.0070  lr:0.000001
[ Thu Jul  4 15:18:45 2024 ] 	Batch(300/7879) done. Loss: 0.4904  lr:0.000001
[ Thu Jul  4 15:19:03 2024 ] 	Batch(400/7879) done. Loss: 0.0723  lr:0.000001
[ Thu Jul  4 15:19:22 2024 ] 
Training: Epoch [104/120], Step [499], Loss: 0.038136646151542664, Training Accuracy: 97.8
[ Thu Jul  4 15:19:22 2024 ] 	Batch(500/7879) done. Loss: 0.0370  lr:0.000001
[ Thu Jul  4 15:19:40 2024 ] 	Batch(600/7879) done. Loss: 0.3810  lr:0.000001
[ Thu Jul  4 15:19:59 2024 ] 	Batch(700/7879) done. Loss: 0.1268  lr:0.000001
[ Thu Jul  4 15:20:17 2024 ] 	Batch(800/7879) done. Loss: 0.0607  lr:0.000001
[ Thu Jul  4 15:20:36 2024 ] 	Batch(900/7879) done. Loss: 0.0377  lr:0.000001
[ Thu Jul  4 15:20:54 2024 ] 
Training: Epoch [104/120], Step [999], Loss: 0.19743254780769348, Training Accuracy: 97.5875
[ Thu Jul  4 15:20:54 2024 ] 	Batch(1000/7879) done. Loss: 0.2060  lr:0.000001
[ Thu Jul  4 15:21:13 2024 ] 	Batch(1100/7879) done. Loss: 0.0959  lr:0.000001
[ Thu Jul  4 15:21:31 2024 ] 	Batch(1200/7879) done. Loss: 0.0487  lr:0.000001
[ Thu Jul  4 15:21:50 2024 ] 	Batch(1300/7879) done. Loss: 0.0097  lr:0.000001
[ Thu Jul  4 15:22:08 2024 ] 	Batch(1400/7879) done. Loss: 0.2345  lr:0.000001
[ Thu Jul  4 15:22:27 2024 ] 
Training: Epoch [104/120], Step [1499], Loss: 0.012823871336877346, Training Accuracy: 97.63333333333334
[ Thu Jul  4 15:22:27 2024 ] 	Batch(1500/7879) done. Loss: 0.1162  lr:0.000001
[ Thu Jul  4 15:22:45 2024 ] 	Batch(1600/7879) done. Loss: 0.0145  lr:0.000001
[ Thu Jul  4 15:23:03 2024 ] 	Batch(1700/7879) done. Loss: 0.0761  lr:0.000001
[ Thu Jul  4 15:23:21 2024 ] 	Batch(1800/7879) done. Loss: 0.0481  lr:0.000001
[ Thu Jul  4 15:23:39 2024 ] 	Batch(1900/7879) done. Loss: 0.3806  lr:0.000001
[ Thu Jul  4 15:23:57 2024 ] 
Training: Epoch [104/120], Step [1999], Loss: 0.0509134940803051, Training Accuracy: 97.625
[ Thu Jul  4 15:23:57 2024 ] 	Batch(2000/7879) done. Loss: 0.1797  lr:0.000001
[ Thu Jul  4 15:24:16 2024 ] 	Batch(2100/7879) done. Loss: 0.0147  lr:0.000001
[ Thu Jul  4 15:24:35 2024 ] 	Batch(2200/7879) done. Loss: 0.0539  lr:0.000001
[ Thu Jul  4 15:24:53 2024 ] 	Batch(2300/7879) done. Loss: 0.1294  lr:0.000001
[ Thu Jul  4 15:25:12 2024 ] 	Batch(2400/7879) done. Loss: 0.1064  lr:0.000001
[ Thu Jul  4 15:25:31 2024 ] 
Training: Epoch [104/120], Step [2499], Loss: 0.07486973702907562, Training Accuracy: 97.65
[ Thu Jul  4 15:25:31 2024 ] 	Batch(2500/7879) done. Loss: 0.0120  lr:0.000001
[ Thu Jul  4 15:25:49 2024 ] 	Batch(2600/7879) done. Loss: 0.4015  lr:0.000001
[ Thu Jul  4 15:26:08 2024 ] 	Batch(2700/7879) done. Loss: 0.0877  lr:0.000001
[ Thu Jul  4 15:26:26 2024 ] 	Batch(2800/7879) done. Loss: 0.0239  lr:0.000001
[ Thu Jul  4 15:26:44 2024 ] 	Batch(2900/7879) done. Loss: 0.1080  lr:0.000001
[ Thu Jul  4 15:27:02 2024 ] 
Training: Epoch [104/120], Step [2999], Loss: 0.051249824464321136, Training Accuracy: 97.6
[ Thu Jul  4 15:27:02 2024 ] 	Batch(3000/7879) done. Loss: 0.0272  lr:0.000001
[ Thu Jul  4 15:27:20 2024 ] 	Batch(3100/7879) done. Loss: 0.0475  lr:0.000001
[ Thu Jul  4 15:27:38 2024 ] 	Batch(3200/7879) done. Loss: 0.2272  lr:0.000001
[ Thu Jul  4 15:27:57 2024 ] 	Batch(3300/7879) done. Loss: 0.0929  lr:0.000001
[ Thu Jul  4 15:28:15 2024 ] 	Batch(3400/7879) done. Loss: 0.0170  lr:0.000001
[ Thu Jul  4 15:28:34 2024 ] 
Training: Epoch [104/120], Step [3499], Loss: 0.01128468383103609, Training Accuracy: 97.68214285714286
[ Thu Jul  4 15:28:34 2024 ] 	Batch(3500/7879) done. Loss: 0.0271  lr:0.000001
[ Thu Jul  4 15:28:53 2024 ] 	Batch(3600/7879) done. Loss: 0.0106  lr:0.000001
[ Thu Jul  4 15:29:11 2024 ] 	Batch(3700/7879) done. Loss: 0.0049  lr:0.000001
[ Thu Jul  4 15:29:29 2024 ] 	Batch(3800/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul  4 15:29:48 2024 ] 	Batch(3900/7879) done. Loss: 0.1047  lr:0.000001
[ Thu Jul  4 15:30:05 2024 ] 
Training: Epoch [104/120], Step [3999], Loss: 0.004119563847780228, Training Accuracy: 97.675
[ Thu Jul  4 15:30:06 2024 ] 	Batch(4000/7879) done. Loss: 0.4396  lr:0.000001
[ Thu Jul  4 15:30:24 2024 ] 	Batch(4100/7879) done. Loss: 0.0321  lr:0.000001
[ Thu Jul  4 15:30:43 2024 ] 	Batch(4200/7879) done. Loss: 0.0162  lr:0.000001
[ Thu Jul  4 15:31:02 2024 ] 	Batch(4300/7879) done. Loss: 0.1009  lr:0.000001
[ Thu Jul  4 15:31:21 2024 ] 	Batch(4400/7879) done. Loss: 0.3545  lr:0.000001
[ Thu Jul  4 15:31:38 2024 ] 
Training: Epoch [104/120], Step [4499], Loss: 0.3349367380142212, Training Accuracy: 97.72777777777777
[ Thu Jul  4 15:31:39 2024 ] 	Batch(4500/7879) done. Loss: 0.0479  lr:0.000001
[ Thu Jul  4 15:31:57 2024 ] 	Batch(4600/7879) done. Loss: 0.8873  lr:0.000001
[ Thu Jul  4 15:32:15 2024 ] 	Batch(4700/7879) done. Loss: 0.0585  lr:0.000001
[ Thu Jul  4 15:32:33 2024 ] 	Batch(4800/7879) done. Loss: 0.0180  lr:0.000001
[ Thu Jul  4 15:32:51 2024 ] 	Batch(4900/7879) done. Loss: 0.1388  lr:0.000001
[ Thu Jul  4 15:33:10 2024 ] 
Training: Epoch [104/120], Step [4999], Loss: 0.051856186240911484, Training Accuracy: 97.735
[ Thu Jul  4 15:33:10 2024 ] 	Batch(5000/7879) done. Loss: 0.1928  lr:0.000001
[ Thu Jul  4 15:33:28 2024 ] 	Batch(5100/7879) done. Loss: 0.1331  lr:0.000001
[ Thu Jul  4 15:33:46 2024 ] 	Batch(5200/7879) done. Loss: 0.0925  lr:0.000001
[ Thu Jul  4 15:34:05 2024 ] 	Batch(5300/7879) done. Loss: 0.0237  lr:0.000001
[ Thu Jul  4 15:34:23 2024 ] 	Batch(5400/7879) done. Loss: 0.3524  lr:0.000001
[ Thu Jul  4 15:34:41 2024 ] 
Training: Epoch [104/120], Step [5499], Loss: 0.1669631451368332, Training Accuracy: 97.74545454545455
[ Thu Jul  4 15:34:41 2024 ] 	Batch(5500/7879) done. Loss: 0.0046  lr:0.000001
[ Thu Jul  4 15:34:59 2024 ] 	Batch(5600/7879) done. Loss: 0.0255  lr:0.000001
[ Thu Jul  4 15:35:17 2024 ] 	Batch(5700/7879) done. Loss: 0.0991  lr:0.000001
[ Thu Jul  4 15:35:35 2024 ] 	Batch(5800/7879) done. Loss: 0.1271  lr:0.000001
[ Thu Jul  4 15:35:53 2024 ] 	Batch(5900/7879) done. Loss: 0.0268  lr:0.000001
[ Thu Jul  4 15:36:10 2024 ] 
Training: Epoch [104/120], Step [5999], Loss: 0.020775627344846725, Training Accuracy: 97.77291666666666
[ Thu Jul  4 15:36:11 2024 ] 	Batch(6000/7879) done. Loss: 0.0040  lr:0.000001
[ Thu Jul  4 15:36:29 2024 ] 	Batch(6100/7879) done. Loss: 0.0186  lr:0.000001
[ Thu Jul  4 15:36:46 2024 ] 	Batch(6200/7879) done. Loss: 0.0081  lr:0.000001
[ Thu Jul  4 15:37:04 2024 ] 	Batch(6300/7879) done. Loss: 0.1537  lr:0.000001
[ Thu Jul  4 15:37:22 2024 ] 	Batch(6400/7879) done. Loss: 0.0834  lr:0.000001
[ Thu Jul  4 15:37:40 2024 ] 
Training: Epoch [104/120], Step [6499], Loss: 0.07173088192939758, Training Accuracy: 97.76923076923076
[ Thu Jul  4 15:37:40 2024 ] 	Batch(6500/7879) done. Loss: 0.0522  lr:0.000001
[ Thu Jul  4 15:37:59 2024 ] 	Batch(6600/7879) done. Loss: 0.0088  lr:0.000001
[ Thu Jul  4 15:38:17 2024 ] 	Batch(6700/7879) done. Loss: 0.0206  lr:0.000001
[ Thu Jul  4 15:38:36 2024 ] 	Batch(6800/7879) done. Loss: 0.0094  lr:0.000001
[ Thu Jul  4 15:38:54 2024 ] 	Batch(6900/7879) done. Loss: 0.1100  lr:0.000001
[ Thu Jul  4 15:39:13 2024 ] 
Training: Epoch [104/120], Step [6999], Loss: 0.013872102834284306, Training Accuracy: 97.77857142857142
[ Thu Jul  4 15:39:13 2024 ] 	Batch(7000/7879) done. Loss: 0.0428  lr:0.000001
[ Thu Jul  4 15:39:31 2024 ] 	Batch(7100/7879) done. Loss: 0.1436  lr:0.000001
[ Thu Jul  4 15:39:50 2024 ] 	Batch(7200/7879) done. Loss: 0.0096  lr:0.000001
[ Thu Jul  4 15:40:09 2024 ] 	Batch(7300/7879) done. Loss: 0.0158  lr:0.000001
[ Thu Jul  4 15:40:27 2024 ] 	Batch(7400/7879) done. Loss: 0.0502  lr:0.000001
[ Thu Jul  4 15:40:45 2024 ] 
Training: Epoch [104/120], Step [7499], Loss: 0.03230496868491173, Training Accuracy: 97.77666666666667
[ Thu Jul  4 15:40:46 2024 ] 	Batch(7500/7879) done. Loss: 0.1192  lr:0.000001
[ Thu Jul  4 15:41:04 2024 ] 	Batch(7600/7879) done. Loss: 0.0250  lr:0.000001
[ Thu Jul  4 15:41:22 2024 ] 	Batch(7700/7879) done. Loss: 0.1231  lr:0.000001
[ Thu Jul  4 15:41:40 2024 ] 	Batch(7800/7879) done. Loss: 0.0050  lr:0.000001
[ Thu Jul  4 15:41:54 2024 ] 	Mean training loss: 0.0920.
[ Thu Jul  4 15:41:54 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul  4 15:41:54 2024 ] Training epoch: 106
[ Thu Jul  4 15:41:55 2024 ] 	Batch(0/7879) done. Loss: 0.1386  lr:0.000001
[ Thu Jul  4 15:42:13 2024 ] 	Batch(100/7879) done. Loss: 0.1376  lr:0.000001
[ Thu Jul  4 15:42:31 2024 ] 	Batch(200/7879) done. Loss: 0.0317  lr:0.000001
[ Thu Jul  4 15:42:49 2024 ] 	Batch(300/7879) done. Loss: 0.1105  lr:0.000001
[ Thu Jul  4 15:43:08 2024 ] 	Batch(400/7879) done. Loss: 0.1899  lr:0.000001
[ Thu Jul  4 15:43:26 2024 ] 
Training: Epoch [105/120], Step [499], Loss: 0.025073178112506866, Training Accuracy: 97.45
[ Thu Jul  4 15:43:26 2024 ] 	Batch(500/7879) done. Loss: 0.0915  lr:0.000001
[ Thu Jul  4 15:43:44 2024 ] 	Batch(600/7879) done. Loss: 0.0835  lr:0.000001
[ Thu Jul  4 15:44:02 2024 ] 	Batch(700/7879) done. Loss: 0.0043  lr:0.000001
[ Thu Jul  4 15:44:20 2024 ] 	Batch(800/7879) done. Loss: 0.0974  lr:0.000001
[ Thu Jul  4 15:44:38 2024 ] 	Batch(900/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul  4 15:44:56 2024 ] 
Training: Epoch [105/120], Step [999], Loss: 0.011820937506854534, Training Accuracy: 97.48750000000001
[ Thu Jul  4 15:44:56 2024 ] 	Batch(1000/7879) done. Loss: 0.0055  lr:0.000001
[ Thu Jul  4 15:45:14 2024 ] 	Batch(1100/7879) done. Loss: 0.0232  lr:0.000001
[ Thu Jul  4 15:45:32 2024 ] 	Batch(1200/7879) done. Loss: 0.0509  lr:0.000001
[ Thu Jul  4 15:45:49 2024 ] 	Batch(1300/7879) done. Loss: 0.0254  lr:0.000001
[ Thu Jul  4 15:46:07 2024 ] 	Batch(1400/7879) done. Loss: 0.0938  lr:0.000001
[ Thu Jul  4 15:46:25 2024 ] 
Training: Epoch [105/120], Step [1499], Loss: 0.05049258843064308, Training Accuracy: 97.50833333333333
[ Thu Jul  4 15:46:25 2024 ] 	Batch(1500/7879) done. Loss: 0.1026  lr:0.000001
[ Thu Jul  4 15:46:43 2024 ] 	Batch(1600/7879) done. Loss: 0.2716  lr:0.000001
[ Thu Jul  4 15:47:02 2024 ] 	Batch(1700/7879) done. Loss: 0.0051  lr:0.000001
[ Thu Jul  4 15:47:20 2024 ] 	Batch(1800/7879) done. Loss: 0.0357  lr:0.000001
[ Thu Jul  4 15:47:38 2024 ] 	Batch(1900/7879) done. Loss: 0.1119  lr:0.000001
[ Thu Jul  4 15:47:56 2024 ] 
Training: Epoch [105/120], Step [1999], Loss: 0.019703427329659462, Training Accuracy: 97.675
[ Thu Jul  4 15:47:56 2024 ] 	Batch(2000/7879) done. Loss: 0.0224  lr:0.000001
[ Thu Jul  4 15:48:14 2024 ] 	Batch(2100/7879) done. Loss: 0.0752  lr:0.000001
[ Thu Jul  4 15:48:32 2024 ] 	Batch(2200/7879) done. Loss: 0.0275  lr:0.000001
[ Thu Jul  4 15:48:50 2024 ] 	Batch(2300/7879) done. Loss: 0.1521  lr:0.000001
[ Thu Jul  4 15:49:08 2024 ] 	Batch(2400/7879) done. Loss: 0.1786  lr:0.000001
[ Thu Jul  4 15:49:26 2024 ] 
Training: Epoch [105/120], Step [2499], Loss: 0.19938157498836517, Training Accuracy: 97.735
[ Thu Jul  4 15:49:26 2024 ] 	Batch(2500/7879) done. Loss: 0.1076  lr:0.000001
[ Thu Jul  4 15:49:44 2024 ] 	Batch(2600/7879) done. Loss: 0.1308  lr:0.000001
[ Thu Jul  4 15:50:02 2024 ] 	Batch(2700/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul  4 15:50:20 2024 ] 	Batch(2800/7879) done. Loss: 0.0556  lr:0.000001
[ Thu Jul  4 15:50:38 2024 ] 	Batch(2900/7879) done. Loss: 0.0636  lr:0.000001
[ Thu Jul  4 15:50:56 2024 ] 
Training: Epoch [105/120], Step [2999], Loss: 0.013774659484624863, Training Accuracy: 97.79166666666667
[ Thu Jul  4 15:50:57 2024 ] 	Batch(3000/7879) done. Loss: 0.0272  lr:0.000001
[ Thu Jul  4 15:51:15 2024 ] 	Batch(3100/7879) done. Loss: 0.1156  lr:0.000001
[ Thu Jul  4 15:51:34 2024 ] 	Batch(3200/7879) done. Loss: 0.0100  lr:0.000001
[ Thu Jul  4 15:51:52 2024 ] 	Batch(3300/7879) done. Loss: 0.0174  lr:0.000001
[ Thu Jul  4 15:52:10 2024 ] 	Batch(3400/7879) done. Loss: 0.0851  lr:0.000001
[ Thu Jul  4 15:52:27 2024 ] 
Training: Epoch [105/120], Step [3499], Loss: 0.11667094379663467, Training Accuracy: 97.82499999999999
[ Thu Jul  4 15:52:27 2024 ] 	Batch(3500/7879) done. Loss: 0.0243  lr:0.000001
[ Thu Jul  4 15:52:46 2024 ] 	Batch(3600/7879) done. Loss: 0.0336  lr:0.000001
[ Thu Jul  4 15:53:03 2024 ] 	Batch(3700/7879) done. Loss: 0.1293  lr:0.000001
[ Thu Jul  4 15:53:21 2024 ] 	Batch(3800/7879) done. Loss: 0.4587  lr:0.000001
[ Thu Jul  4 15:53:39 2024 ] 	Batch(3900/7879) done. Loss: 0.2238  lr:0.000001
[ Thu Jul  4 15:53:57 2024 ] 
Training: Epoch [105/120], Step [3999], Loss: 0.02145278826355934, Training Accuracy: 97.809375
[ Thu Jul  4 15:53:57 2024 ] 	Batch(4000/7879) done. Loss: 0.0719  lr:0.000001
[ Thu Jul  4 15:54:15 2024 ] 	Batch(4100/7879) done. Loss: 0.1119  lr:0.000001
[ Thu Jul  4 15:54:33 2024 ] 	Batch(4200/7879) done. Loss: 0.0970  lr:0.000001
[ Thu Jul  4 15:54:51 2024 ] 	Batch(4300/7879) done. Loss: 0.1368  lr:0.000001
[ Thu Jul  4 15:55:09 2024 ] 	Batch(4400/7879) done. Loss: 0.0561  lr:0.000001
[ Thu Jul  4 15:55:27 2024 ] 
Training: Epoch [105/120], Step [4499], Loss: 0.03338603302836418, Training Accuracy: 97.85555555555555
[ Thu Jul  4 15:55:27 2024 ] 	Batch(4500/7879) done. Loss: 0.0117  lr:0.000001
[ Thu Jul  4 15:55:45 2024 ] 	Batch(4600/7879) done. Loss: 0.0686  lr:0.000001
[ Thu Jul  4 15:56:03 2024 ] 	Batch(4700/7879) done. Loss: 0.0061  lr:0.000001
[ Thu Jul  4 15:56:21 2024 ] 	Batch(4800/7879) done. Loss: 0.0026  lr:0.000001
[ Thu Jul  4 15:56:39 2024 ] 	Batch(4900/7879) done. Loss: 0.0189  lr:0.000001
[ Thu Jul  4 15:56:57 2024 ] 
Training: Epoch [105/120], Step [4999], Loss: 0.041574712842702866, Training Accuracy: 97.85249999999999
[ Thu Jul  4 15:56:57 2024 ] 	Batch(5000/7879) done. Loss: 0.0056  lr:0.000001
[ Thu Jul  4 15:57:15 2024 ] 	Batch(5100/7879) done. Loss: 0.0715  lr:0.000001
[ Thu Jul  4 15:57:33 2024 ] 	Batch(5200/7879) done. Loss: 0.0216  lr:0.000001
[ Thu Jul  4 15:57:51 2024 ] 	Batch(5300/7879) done. Loss: 0.0487  lr:0.000001
[ Thu Jul  4 15:58:08 2024 ] 	Batch(5400/7879) done. Loss: 0.0753  lr:0.000001
[ Thu Jul  4 15:58:26 2024 ] 
Training: Epoch [105/120], Step [5499], Loss: 0.011135212145745754, Training Accuracy: 97.8409090909091
[ Thu Jul  4 15:58:26 2024 ] 	Batch(5500/7879) done. Loss: 0.0267  lr:0.000001
[ Thu Jul  4 15:58:44 2024 ] 	Batch(5600/7879) done. Loss: 0.1048  lr:0.000001
[ Thu Jul  4 15:59:02 2024 ] 	Batch(5700/7879) done. Loss: 0.0762  lr:0.000001
[ Thu Jul  4 15:59:20 2024 ] 	Batch(5800/7879) done. Loss: 0.1497  lr:0.000001
[ Thu Jul  4 15:59:38 2024 ] 	Batch(5900/7879) done. Loss: 0.0088  lr:0.000001
[ Thu Jul  4 15:59:56 2024 ] 
Training: Epoch [105/120], Step [5999], Loss: 0.011643544770777225, Training Accuracy: 97.83333333333334
[ Thu Jul  4 15:59:56 2024 ] 	Batch(6000/7879) done. Loss: 0.0078  lr:0.000001
[ Thu Jul  4 16:00:14 2024 ] 	Batch(6100/7879) done. Loss: 0.0118  lr:0.000001
[ Thu Jul  4 16:00:32 2024 ] 	Batch(6200/7879) done. Loss: 0.0744  lr:0.000001
[ Thu Jul  4 16:00:50 2024 ] 	Batch(6300/7879) done. Loss: 0.0463  lr:0.000001
[ Thu Jul  4 16:01:08 2024 ] 	Batch(6400/7879) done. Loss: 0.0075  lr:0.000001
[ Thu Jul  4 16:01:26 2024 ] 
Training: Epoch [105/120], Step [6499], Loss: 0.1673121452331543, Training Accuracy: 97.81538461538462
[ Thu Jul  4 16:01:26 2024 ] 	Batch(6500/7879) done. Loss: 0.1048  lr:0.000001
[ Thu Jul  4 16:01:45 2024 ] 	Batch(6600/7879) done. Loss: 0.0647  lr:0.000001
[ Thu Jul  4 16:02:03 2024 ] 	Batch(6700/7879) done. Loss: 0.1251  lr:0.000001
[ Thu Jul  4 16:02:21 2024 ] 	Batch(6800/7879) done. Loss: 0.0263  lr:0.000001
[ Thu Jul  4 16:02:40 2024 ] 	Batch(6900/7879) done. Loss: 0.0101  lr:0.000001
[ Thu Jul  4 16:02:58 2024 ] 
Training: Epoch [105/120], Step [6999], Loss: 0.0019366181222721934, Training Accuracy: 97.80357142857142
[ Thu Jul  4 16:02:58 2024 ] 	Batch(7000/7879) done. Loss: 0.0711  lr:0.000001
[ Thu Jul  4 16:03:17 2024 ] 	Batch(7100/7879) done. Loss: 0.0066  lr:0.000001
[ Thu Jul  4 16:03:35 2024 ] 	Batch(7200/7879) done. Loss: 0.0186  lr:0.000001
[ Thu Jul  4 16:03:53 2024 ] 	Batch(7300/7879) done. Loss: 0.0104  lr:0.000001
[ Thu Jul  4 16:04:11 2024 ] 	Batch(7400/7879) done. Loss: 0.0361  lr:0.000001
[ Thu Jul  4 16:04:28 2024 ] 
Training: Epoch [105/120], Step [7499], Loss: 0.007669518701732159, Training Accuracy: 97.79833333333333
[ Thu Jul  4 16:04:28 2024 ] 	Batch(7500/7879) done. Loss: 0.0147  lr:0.000001
[ Thu Jul  4 16:04:46 2024 ] 	Batch(7600/7879) done. Loss: 0.0275  lr:0.000001
[ Thu Jul  4 16:05:05 2024 ] 	Batch(7700/7879) done. Loss: 0.0775  lr:0.000001
[ Thu Jul  4 16:05:24 2024 ] 	Batch(7800/7879) done. Loss: 0.4512  lr:0.000001
[ Thu Jul  4 16:05:38 2024 ] 	Mean training loss: 0.0904.
[ Thu Jul  4 16:05:38 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 16:05:38 2024 ] Training epoch: 107
[ Thu Jul  4 16:05:39 2024 ] 	Batch(0/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul  4 16:05:57 2024 ] 	Batch(100/7879) done. Loss: 0.0669  lr:0.000001
[ Thu Jul  4 16:06:15 2024 ] 	Batch(200/7879) done. Loss: 0.0153  lr:0.000001
[ Thu Jul  4 16:06:33 2024 ] 	Batch(300/7879) done. Loss: 0.0107  lr:0.000001
[ Thu Jul  4 16:06:52 2024 ] 	Batch(400/7879) done. Loss: 0.0600  lr:0.000001
[ Thu Jul  4 16:07:10 2024 ] 
Training: Epoch [106/120], Step [499], Loss: 0.025485243648290634, Training Accuracy: 97.675
[ Thu Jul  4 16:07:10 2024 ] 	Batch(500/7879) done. Loss: 0.1299  lr:0.000001
[ Thu Jul  4 16:07:28 2024 ] 	Batch(600/7879) done. Loss: 0.0425  lr:0.000001
[ Thu Jul  4 16:07:46 2024 ] 	Batch(700/7879) done. Loss: 0.0343  lr:0.000001
[ Thu Jul  4 16:08:05 2024 ] 	Batch(800/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul  4 16:08:23 2024 ] 	Batch(900/7879) done. Loss: 0.0655  lr:0.000001
[ Thu Jul  4 16:08:41 2024 ] 
Training: Epoch [106/120], Step [999], Loss: 0.07873419672250748, Training Accuracy: 97.7625
[ Thu Jul  4 16:08:42 2024 ] 	Batch(1000/7879) done. Loss: 0.0336  lr:0.000001
[ Thu Jul  4 16:09:00 2024 ] 	Batch(1100/7879) done. Loss: 0.0474  lr:0.000001
[ Thu Jul  4 16:09:18 2024 ] 	Batch(1200/7879) done. Loss: 0.0366  lr:0.000001
[ Thu Jul  4 16:09:36 2024 ] 	Batch(1300/7879) done. Loss: 0.0884  lr:0.000001
[ Thu Jul  4 16:09:54 2024 ] 	Batch(1400/7879) done. Loss: 0.0364  lr:0.000001
[ Thu Jul  4 16:10:12 2024 ] 
Training: Epoch [106/120], Step [1499], Loss: 0.005979847628623247, Training Accuracy: 97.80833333333334
[ Thu Jul  4 16:10:12 2024 ] 	Batch(1500/7879) done. Loss: 0.0547  lr:0.000001
[ Thu Jul  4 16:10:30 2024 ] 	Batch(1600/7879) done. Loss: 0.0158  lr:0.000001
[ Thu Jul  4 16:10:48 2024 ] 	Batch(1700/7879) done. Loss: 0.0664  lr:0.000001
[ Thu Jul  4 16:11:06 2024 ] 	Batch(1800/7879) done. Loss: 0.0062  lr:0.000001
[ Thu Jul  4 16:11:24 2024 ] 	Batch(1900/7879) done. Loss: 0.0060  lr:0.000001
[ Thu Jul  4 16:11:41 2024 ] 
Training: Epoch [106/120], Step [1999], Loss: 0.10486585646867752, Training Accuracy: 97.8625
[ Thu Jul  4 16:11:42 2024 ] 	Batch(2000/7879) done. Loss: 0.3378  lr:0.000001
[ Thu Jul  4 16:12:00 2024 ] 	Batch(2100/7879) done. Loss: 0.0258  lr:0.000001
[ Thu Jul  4 16:12:19 2024 ] 	Batch(2200/7879) done. Loss: 0.0159  lr:0.000001
[ Thu Jul  4 16:12:37 2024 ] 	Batch(2300/7879) done. Loss: 0.0318  lr:0.000001
[ Thu Jul  4 16:12:56 2024 ] 	Batch(2400/7879) done. Loss: 0.1155  lr:0.000001
[ Thu Jul  4 16:13:14 2024 ] 
Training: Epoch [106/120], Step [2499], Loss: 0.00798068381845951, Training Accuracy: 97.815
[ Thu Jul  4 16:13:14 2024 ] 	Batch(2500/7879) done. Loss: 0.1333  lr:0.000001
[ Thu Jul  4 16:13:32 2024 ] 	Batch(2600/7879) done. Loss: 0.0097  lr:0.000001
[ Thu Jul  4 16:13:49 2024 ] 	Batch(2700/7879) done. Loss: 0.3782  lr:0.000001
[ Thu Jul  4 16:14:07 2024 ] 	Batch(2800/7879) done. Loss: 0.0012  lr:0.000001
[ Thu Jul  4 16:14:26 2024 ] 	Batch(2900/7879) done. Loss: 0.1614  lr:0.000001
[ Thu Jul  4 16:14:44 2024 ] 
Training: Epoch [106/120], Step [2999], Loss: 0.3887818455696106, Training Accuracy: 97.75416666666666
[ Thu Jul  4 16:14:44 2024 ] 	Batch(3000/7879) done. Loss: 0.3258  lr:0.000001
[ Thu Jul  4 16:15:02 2024 ] 	Batch(3100/7879) done. Loss: 0.0281  lr:0.000001
[ Thu Jul  4 16:15:20 2024 ] 	Batch(3200/7879) done. Loss: 0.0608  lr:0.000001
[ Thu Jul  4 16:15:38 2024 ] 	Batch(3300/7879) done. Loss: 0.3654  lr:0.000001
[ Thu Jul  4 16:15:57 2024 ] 	Batch(3400/7879) done. Loss: 0.0024  lr:0.000001
[ Thu Jul  4 16:16:15 2024 ] 
Training: Epoch [106/120], Step [3499], Loss: 0.007398142479360104, Training Accuracy: 97.72857142857143
[ Thu Jul  4 16:16:15 2024 ] 	Batch(3500/7879) done. Loss: 0.0897  lr:0.000001
[ Thu Jul  4 16:16:34 2024 ] 	Batch(3600/7879) done. Loss: 0.1044  lr:0.000001
[ Thu Jul  4 16:16:51 2024 ] 	Batch(3700/7879) done. Loss: 0.0274  lr:0.000001
[ Thu Jul  4 16:17:09 2024 ] 	Batch(3800/7879) done. Loss: 0.0183  lr:0.000001
[ Thu Jul  4 16:17:27 2024 ] 	Batch(3900/7879) done. Loss: 0.0277  lr:0.000001
[ Thu Jul  4 16:17:45 2024 ] 
Training: Epoch [106/120], Step [3999], Loss: 0.04394806548953056, Training Accuracy: 97.69375
[ Thu Jul  4 16:17:45 2024 ] 	Batch(4000/7879) done. Loss: 0.0909  lr:0.000001
[ Thu Jul  4 16:18:04 2024 ] 	Batch(4100/7879) done. Loss: 0.0197  lr:0.000001
[ Thu Jul  4 16:18:22 2024 ] 	Batch(4200/7879) done. Loss: 0.0066  lr:0.000001
[ Thu Jul  4 16:18:41 2024 ] 	Batch(4300/7879) done. Loss: 0.3585  lr:0.000001
[ Thu Jul  4 16:18:59 2024 ] 	Batch(4400/7879) done. Loss: 0.4117  lr:0.000001
[ Thu Jul  4 16:19:16 2024 ] 
Training: Epoch [106/120], Step [4499], Loss: 0.01116788387298584, Training Accuracy: 97.63055555555556
[ Thu Jul  4 16:19:16 2024 ] 	Batch(4500/7879) done. Loss: 0.0068  lr:0.000001
[ Thu Jul  4 16:19:34 2024 ] 	Batch(4600/7879) done. Loss: 0.1065  lr:0.000001
[ Thu Jul  4 16:19:52 2024 ] 	Batch(4700/7879) done. Loss: 0.2115  lr:0.000001
[ Thu Jul  4 16:20:11 2024 ] 	Batch(4800/7879) done. Loss: 0.1697  lr:0.000001
[ Thu Jul  4 16:20:28 2024 ] 	Batch(4900/7879) done. Loss: 0.0402  lr:0.000001
[ Thu Jul  4 16:20:46 2024 ] 
Training: Epoch [106/120], Step [4999], Loss: 0.5650846362113953, Training Accuracy: 97.61999999999999
[ Thu Jul  4 16:20:46 2024 ] 	Batch(5000/7879) done. Loss: 0.0334  lr:0.000001
[ Thu Jul  4 16:21:04 2024 ] 	Batch(5100/7879) done. Loss: 0.0894  lr:0.000001
[ Thu Jul  4 16:21:22 2024 ] 	Batch(5200/7879) done. Loss: 0.0368  lr:0.000001
[ Thu Jul  4 16:21:41 2024 ] 	Batch(5300/7879) done. Loss: 0.2537  lr:0.000001
[ Thu Jul  4 16:21:59 2024 ] 	Batch(5400/7879) done. Loss: 0.0918  lr:0.000001
[ Thu Jul  4 16:22:17 2024 ] 
Training: Epoch [106/120], Step [5499], Loss: 0.17581643164157867, Training Accuracy: 97.63409090909092
[ Thu Jul  4 16:22:17 2024 ] 	Batch(5500/7879) done. Loss: 0.0260  lr:0.000001
[ Thu Jul  4 16:22:35 2024 ] 	Batch(5600/7879) done. Loss: 0.1783  lr:0.000001
[ Thu Jul  4 16:22:54 2024 ] 	Batch(5700/7879) done. Loss: 0.0152  lr:0.000001
[ Thu Jul  4 16:23:12 2024 ] 	Batch(5800/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul  4 16:23:31 2024 ] 	Batch(5900/7879) done. Loss: 0.2971  lr:0.000001
[ Thu Jul  4 16:23:49 2024 ] 
Training: Epoch [106/120], Step [5999], Loss: 0.0050436463207006454, Training Accuracy: 97.67291666666667
[ Thu Jul  4 16:23:49 2024 ] 	Batch(6000/7879) done. Loss: 0.0507  lr:0.000001
[ Thu Jul  4 16:24:07 2024 ] 	Batch(6100/7879) done. Loss: 0.0039  lr:0.000001
[ Thu Jul  4 16:24:25 2024 ] 	Batch(6200/7879) done. Loss: 0.1663  lr:0.000001
[ Thu Jul  4 16:24:43 2024 ] 	Batch(6300/7879) done. Loss: 0.1143  lr:0.000001
[ Thu Jul  4 16:25:01 2024 ] 	Batch(6400/7879) done. Loss: 0.0196  lr:0.000001
[ Thu Jul  4 16:25:19 2024 ] 
Training: Epoch [106/120], Step [6499], Loss: 0.0076893894001841545, Training Accuracy: 97.63269230769231
[ Thu Jul  4 16:25:19 2024 ] 	Batch(6500/7879) done. Loss: 0.1079  lr:0.000001
[ Thu Jul  4 16:25:37 2024 ] 	Batch(6600/7879) done. Loss: 0.0272  lr:0.000001
[ Thu Jul  4 16:25:55 2024 ] 	Batch(6700/7879) done. Loss: 0.1686  lr:0.000001
[ Thu Jul  4 16:26:13 2024 ] 	Batch(6800/7879) done. Loss: 0.1071  lr:0.000001
[ Thu Jul  4 16:26:31 2024 ] 	Batch(6900/7879) done. Loss: 0.4309  lr:0.000001
[ Thu Jul  4 16:26:50 2024 ] 
Training: Epoch [106/120], Step [6999], Loss: 0.029696650803089142, Training Accuracy: 97.64464285714286
[ Thu Jul  4 16:26:50 2024 ] 	Batch(7000/7879) done. Loss: 0.0149  lr:0.000001
[ Thu Jul  4 16:27:08 2024 ] 	Batch(7100/7879) done. Loss: 0.0724  lr:0.000001
[ Thu Jul  4 16:27:27 2024 ] 	Batch(7200/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul  4 16:27:45 2024 ] 	Batch(7300/7879) done. Loss: 0.0145  lr:0.000001
[ Thu Jul  4 16:28:03 2024 ] 	Batch(7400/7879) done. Loss: 0.0190  lr:0.000001
[ Thu Jul  4 16:28:21 2024 ] 
Training: Epoch [106/120], Step [7499], Loss: 0.384129136800766, Training Accuracy: 97.67833333333333
[ Thu Jul  4 16:28:21 2024 ] 	Batch(7500/7879) done. Loss: 0.0448  lr:0.000001
[ Thu Jul  4 16:28:39 2024 ] 	Batch(7600/7879) done. Loss: 0.0691  lr:0.000001
[ Thu Jul  4 16:28:57 2024 ] 	Batch(7700/7879) done. Loss: 0.0558  lr:0.000001
[ Thu Jul  4 16:29:14 2024 ] 	Batch(7800/7879) done. Loss: 0.0170  lr:0.000001
[ Thu Jul  4 16:29:29 2024 ] 	Mean training loss: 0.0929.
[ Thu Jul  4 16:29:29 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul  4 16:29:29 2024 ] Training epoch: 108
[ Thu Jul  4 16:29:29 2024 ] 	Batch(0/7879) done. Loss: 0.0541  lr:0.000001
[ Thu Jul  4 16:29:48 2024 ] 	Batch(100/7879) done. Loss: 0.0529  lr:0.000001
[ Thu Jul  4 16:30:06 2024 ] 	Batch(200/7879) done. Loss: 0.0939  lr:0.000001
[ Thu Jul  4 16:30:24 2024 ] 	Batch(300/7879) done. Loss: 0.0323  lr:0.000001
[ Thu Jul  4 16:30:42 2024 ] 	Batch(400/7879) done. Loss: 0.0377  lr:0.000001
[ Thu Jul  4 16:31:00 2024 ] 
Training: Epoch [107/120], Step [499], Loss: 0.11274941265583038, Training Accuracy: 97.775
[ Thu Jul  4 16:31:00 2024 ] 	Batch(500/7879) done. Loss: 0.0953  lr:0.000001
[ Thu Jul  4 16:31:19 2024 ] 	Batch(600/7879) done. Loss: 0.0449  lr:0.000001
[ Thu Jul  4 16:31:37 2024 ] 	Batch(700/7879) done. Loss: 0.1721  lr:0.000001
[ Thu Jul  4 16:31:55 2024 ] 	Batch(800/7879) done. Loss: 0.1229  lr:0.000001
[ Thu Jul  4 16:32:14 2024 ] 	Batch(900/7879) done. Loss: 0.0063  lr:0.000001
[ Thu Jul  4 16:32:32 2024 ] 
Training: Epoch [107/120], Step [999], Loss: 0.026793353259563446, Training Accuracy: 97.8375
[ Thu Jul  4 16:32:32 2024 ] 	Batch(1000/7879) done. Loss: 0.0086  lr:0.000001
[ Thu Jul  4 16:32:51 2024 ] 	Batch(1100/7879) done. Loss: 0.0404  lr:0.000001
[ Thu Jul  4 16:33:09 2024 ] 	Batch(1200/7879) done. Loss: 0.0864  lr:0.000001
[ Thu Jul  4 16:33:27 2024 ] 	Batch(1300/7879) done. Loss: 0.4233  lr:0.000001
[ Thu Jul  4 16:33:45 2024 ] 	Batch(1400/7879) done. Loss: 0.0137  lr:0.000001
[ Thu Jul  4 16:34:02 2024 ] 
Training: Epoch [107/120], Step [1499], Loss: 0.08528878539800644, Training Accuracy: 97.78333333333333
[ Thu Jul  4 16:34:03 2024 ] 	Batch(1500/7879) done. Loss: 0.3500  lr:0.000001
[ Thu Jul  4 16:34:21 2024 ] 	Batch(1600/7879) done. Loss: 0.0813  lr:0.000001
[ Thu Jul  4 16:34:38 2024 ] 	Batch(1700/7879) done. Loss: 0.0602  lr:0.000001
[ Thu Jul  4 16:34:56 2024 ] 	Batch(1800/7879) done. Loss: 0.0709  lr:0.000001
[ Thu Jul  4 16:35:14 2024 ] 	Batch(1900/7879) done. Loss: 0.1783  lr:0.000001
[ Thu Jul  4 16:35:32 2024 ] 
Training: Epoch [107/120], Step [1999], Loss: 0.03149352967739105, Training Accuracy: 97.8375
[ Thu Jul  4 16:35:32 2024 ] 	Batch(2000/7879) done. Loss: 0.0068  lr:0.000001
[ Thu Jul  4 16:35:50 2024 ] 	Batch(2100/7879) done. Loss: 0.0248  lr:0.000001
[ Thu Jul  4 16:36:08 2024 ] 	Batch(2200/7879) done. Loss: 0.0266  lr:0.000001
[ Thu Jul  4 16:36:26 2024 ] 	Batch(2300/7879) done. Loss: 0.3995  lr:0.000001
[ Thu Jul  4 16:36:44 2024 ] 	Batch(2400/7879) done. Loss: 0.0759  lr:0.000001
[ Thu Jul  4 16:37:02 2024 ] 
Training: Epoch [107/120], Step [2499], Loss: 0.16308078169822693, Training Accuracy: 97.76
[ Thu Jul  4 16:37:02 2024 ] 	Batch(2500/7879) done. Loss: 0.0060  lr:0.000001
[ Thu Jul  4 16:37:21 2024 ] 	Batch(2600/7879) done. Loss: 0.0027  lr:0.000001
[ Thu Jul  4 16:37:39 2024 ] 	Batch(2700/7879) done. Loss: 0.0604  lr:0.000001
[ Thu Jul  4 16:37:58 2024 ] 	Batch(2800/7879) done. Loss: 0.0977  lr:0.000001
[ Thu Jul  4 16:38:17 2024 ] 	Batch(2900/7879) done. Loss: 0.2670  lr:0.000001
[ Thu Jul  4 16:38:35 2024 ] 
Training: Epoch [107/120], Step [2999], Loss: 0.03722633793950081, Training Accuracy: 97.8
[ Thu Jul  4 16:38:35 2024 ] 	Batch(3000/7879) done. Loss: 0.0379  lr:0.000001
[ Thu Jul  4 16:38:54 2024 ] 	Batch(3100/7879) done. Loss: 0.0315  lr:0.000001
[ Thu Jul  4 16:39:12 2024 ] 	Batch(3200/7879) done. Loss: 0.0128  lr:0.000001
[ Thu Jul  4 16:39:30 2024 ] 	Batch(3300/7879) done. Loss: 0.0395  lr:0.000001
[ Thu Jul  4 16:39:48 2024 ] 	Batch(3400/7879) done. Loss: 0.0060  lr:0.000001
[ Thu Jul  4 16:40:06 2024 ] 
Training: Epoch [107/120], Step [3499], Loss: 0.2992272973060608, Training Accuracy: 97.79642857142858
[ Thu Jul  4 16:40:06 2024 ] 	Batch(3500/7879) done. Loss: 0.0868  lr:0.000001
[ Thu Jul  4 16:40:24 2024 ] 	Batch(3600/7879) done. Loss: 0.4013  lr:0.000001
[ Thu Jul  4 16:40:42 2024 ] 	Batch(3700/7879) done. Loss: 0.3408  lr:0.000001
[ Thu Jul  4 16:41:00 2024 ] 	Batch(3800/7879) done. Loss: 0.0472  lr:0.000001
[ Thu Jul  4 16:41:18 2024 ] 	Batch(3900/7879) done. Loss: 0.0337  lr:0.000001
[ Thu Jul  4 16:41:35 2024 ] 
Training: Epoch [107/120], Step [3999], Loss: 0.26514384150505066, Training Accuracy: 97.74374999999999
[ Thu Jul  4 16:41:36 2024 ] 	Batch(4000/7879) done. Loss: 0.0128  lr:0.000001
[ Thu Jul  4 16:41:54 2024 ] 	Batch(4100/7879) done. Loss: 0.1196  lr:0.000001
[ Thu Jul  4 16:42:12 2024 ] 	Batch(4200/7879) done. Loss: 0.0507  lr:0.000001
[ Thu Jul  4 16:42:30 2024 ] 	Batch(4300/7879) done. Loss: 0.1212  lr:0.000001
[ Thu Jul  4 16:42:48 2024 ] 	Batch(4400/7879) done. Loss: 0.0134  lr:0.000001
[ Thu Jul  4 16:43:06 2024 ] 
Training: Epoch [107/120], Step [4499], Loss: 0.10321365296840668, Training Accuracy: 97.73611111111111
[ Thu Jul  4 16:43:06 2024 ] 	Batch(4500/7879) done. Loss: 0.0294  lr:0.000001
[ Thu Jul  4 16:43:25 2024 ] 	Batch(4600/7879) done. Loss: 0.0313  lr:0.000001
[ Thu Jul  4 16:43:43 2024 ] 	Batch(4700/7879) done. Loss: 0.0676  lr:0.000001
[ Thu Jul  4 16:44:02 2024 ] 	Batch(4800/7879) done. Loss: 0.1033  lr:0.000001
[ Thu Jul  4 16:44:20 2024 ] 	Batch(4900/7879) done. Loss: 0.0280  lr:0.000001
[ Thu Jul  4 16:44:38 2024 ] 
Training: Epoch [107/120], Step [4999], Loss: 0.14974305033683777, Training Accuracy: 97.735
[ Thu Jul  4 16:44:38 2024 ] 	Batch(5000/7879) done. Loss: 0.0326  lr:0.000001
[ Thu Jul  4 16:44:56 2024 ] 	Batch(5100/7879) done. Loss: 0.0925  lr:0.000001
[ Thu Jul  4 16:45:14 2024 ] 	Batch(5200/7879) done. Loss: 0.0387  lr:0.000001
[ Thu Jul  4 16:45:33 2024 ] 	Batch(5300/7879) done. Loss: 0.0320  lr:0.000001
[ Thu Jul  4 16:45:52 2024 ] 	Batch(5400/7879) done. Loss: 0.0433  lr:0.000001
[ Thu Jul  4 16:46:10 2024 ] 
Training: Epoch [107/120], Step [5499], Loss: 0.09779136627912521, Training Accuracy: 97.75
[ Thu Jul  4 16:46:11 2024 ] 	Batch(5500/7879) done. Loss: 0.1279  lr:0.000001
[ Thu Jul  4 16:46:29 2024 ] 	Batch(5600/7879) done. Loss: 0.0199  lr:0.000001
[ Thu Jul  4 16:46:46 2024 ] 	Batch(5700/7879) done. Loss: 0.1092  lr:0.000001
[ Thu Jul  4 16:47:04 2024 ] 	Batch(5800/7879) done. Loss: 0.1112  lr:0.000001
[ Thu Jul  4 16:47:22 2024 ] 	Batch(5900/7879) done. Loss: 0.3431  lr:0.000001
[ Thu Jul  4 16:47:40 2024 ] 
Training: Epoch [107/120], Step [5999], Loss: 0.14442117512226105, Training Accuracy: 97.76875000000001
[ Thu Jul  4 16:47:40 2024 ] 	Batch(6000/7879) done. Loss: 0.0082  lr:0.000001
[ Thu Jul  4 16:47:59 2024 ] 	Batch(6100/7879) done. Loss: 0.0370  lr:0.000001
[ Thu Jul  4 16:48:17 2024 ] 	Batch(6200/7879) done. Loss: 0.0917  lr:0.000001
[ Thu Jul  4 16:48:36 2024 ] 	Batch(6300/7879) done. Loss: 0.0031  lr:0.000001
[ Thu Jul  4 16:48:54 2024 ] 	Batch(6400/7879) done. Loss: 0.0628  lr:0.000001
[ Thu Jul  4 16:49:12 2024 ] 
Training: Epoch [107/120], Step [6499], Loss: 0.03380316123366356, Training Accuracy: 97.75384615384615
[ Thu Jul  4 16:49:12 2024 ] 	Batch(6500/7879) done. Loss: 0.0043  lr:0.000001
[ Thu Jul  4 16:49:30 2024 ] 	Batch(6600/7879) done. Loss: 0.1405  lr:0.000001
[ Thu Jul  4 16:49:48 2024 ] 	Batch(6700/7879) done. Loss: 0.2640  lr:0.000001
[ Thu Jul  4 16:50:06 2024 ] 	Batch(6800/7879) done. Loss: 0.0131  lr:0.000001
[ Thu Jul  4 16:50:24 2024 ] 	Batch(6900/7879) done. Loss: 0.0077  lr:0.000001
[ Thu Jul  4 16:50:42 2024 ] 
Training: Epoch [107/120], Step [6999], Loss: 0.1596507430076599, Training Accuracy: 97.77142857142857
[ Thu Jul  4 16:50:42 2024 ] 	Batch(7000/7879) done. Loss: 0.0073  lr:0.000001
[ Thu Jul  4 16:51:00 2024 ] 	Batch(7100/7879) done. Loss: 0.0383  lr:0.000001
[ Thu Jul  4 16:51:18 2024 ] 	Batch(7200/7879) done. Loss: 0.0409  lr:0.000001
[ Thu Jul  4 16:51:36 2024 ] 	Batch(7300/7879) done. Loss: 0.4088  lr:0.000001
[ Thu Jul  4 16:51:55 2024 ] 	Batch(7400/7879) done. Loss: 0.0118  lr:0.000001
[ Thu Jul  4 16:52:13 2024 ] 
Training: Epoch [107/120], Step [7499], Loss: 0.19903476536273956, Training Accuracy: 97.75833333333334
[ Thu Jul  4 16:52:13 2024 ] 	Batch(7500/7879) done. Loss: 0.3634  lr:0.000001
[ Thu Jul  4 16:52:31 2024 ] 	Batch(7600/7879) done. Loss: 0.3014  lr:0.000001
[ Thu Jul  4 16:52:49 2024 ] 	Batch(7700/7879) done. Loss: 0.0114  lr:0.000001
[ Thu Jul  4 16:53:08 2024 ] 	Batch(7800/7879) done. Loss: 0.0903  lr:0.000001
[ Thu Jul  4 16:53:22 2024 ] 	Mean training loss: 0.0921.
[ Thu Jul  4 16:53:22 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul  4 16:53:23 2024 ] Training epoch: 109
[ Thu Jul  4 16:53:23 2024 ] 	Batch(0/7879) done. Loss: 0.0873  lr:0.000001
[ Thu Jul  4 16:53:41 2024 ] 	Batch(100/7879) done. Loss: 0.0357  lr:0.000001
[ Thu Jul  4 16:53:59 2024 ] 	Batch(200/7879) done. Loss: 0.1909  lr:0.000001
[ Thu Jul  4 16:54:17 2024 ] 	Batch(300/7879) done. Loss: 0.0262  lr:0.000001
[ Thu Jul  4 16:54:35 2024 ] 	Batch(400/7879) done. Loss: 0.0703  lr:0.000001
[ Thu Jul  4 16:54:52 2024 ] 
Training: Epoch [108/120], Step [499], Loss: 0.08844994008541107, Training Accuracy: 97.82499999999999
[ Thu Jul  4 16:54:52 2024 ] 	Batch(500/7879) done. Loss: 0.0019  lr:0.000001
[ Thu Jul  4 16:55:10 2024 ] 	Batch(600/7879) done. Loss: 0.0770  lr:0.000001
[ Thu Jul  4 16:55:28 2024 ] 	Batch(700/7879) done. Loss: 0.1968  lr:0.000001
[ Thu Jul  4 16:55:46 2024 ] 	Batch(800/7879) done. Loss: 0.4297  lr:0.000001
[ Thu Jul  4 16:56:04 2024 ] 	Batch(900/7879) done. Loss: 0.0831  lr:0.000001
[ Thu Jul  4 16:56:22 2024 ] 
Training: Epoch [108/120], Step [999], Loss: 0.02848241850733757, Training Accuracy: 97.8125
[ Thu Jul  4 16:56:22 2024 ] 	Batch(1000/7879) done. Loss: 0.0210  lr:0.000001
[ Thu Jul  4 16:56:40 2024 ] 	Batch(1100/7879) done. Loss: 0.0033  lr:0.000001
[ Thu Jul  4 16:56:58 2024 ] 	Batch(1200/7879) done. Loss: 0.0822  lr:0.000001
[ Thu Jul  4 16:57:16 2024 ] 	Batch(1300/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul  4 16:57:33 2024 ] 	Batch(1400/7879) done. Loss: 0.0590  lr:0.000001
[ Thu Jul  4 16:57:51 2024 ] 
Training: Epoch [108/120], Step [1499], Loss: 0.0298110730946064, Training Accuracy: 97.74166666666667
[ Thu Jul  4 16:57:51 2024 ] 	Batch(1500/7879) done. Loss: 0.1694  lr:0.000001
[ Thu Jul  4 16:58:09 2024 ] 	Batch(1600/7879) done. Loss: 0.1893  lr:0.000001
[ Thu Jul  4 16:58:28 2024 ] 	Batch(1700/7879) done. Loss: 0.0597  lr:0.000001
[ Thu Jul  4 16:58:46 2024 ] 	Batch(1800/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul  4 16:59:05 2024 ] 	Batch(1900/7879) done. Loss: 0.0412  lr:0.000001
[ Thu Jul  4 16:59:23 2024 ] 
Training: Epoch [108/120], Step [1999], Loss: 0.00783516839146614, Training Accuracy: 97.6875
[ Thu Jul  4 16:59:23 2024 ] 	Batch(2000/7879) done. Loss: 0.0062  lr:0.000001
[ Thu Jul  4 16:59:41 2024 ] 	Batch(2100/7879) done. Loss: 0.0986  lr:0.000001
[ Thu Jul  4 16:59:59 2024 ] 	Batch(2200/7879) done. Loss: 0.1074  lr:0.000001
[ Thu Jul  4 17:00:17 2024 ] 	Batch(2300/7879) done. Loss: 0.0254  lr:0.000001
[ Thu Jul  4 17:00:35 2024 ] 	Batch(2400/7879) done. Loss: 0.1290  lr:0.000001
[ Thu Jul  4 17:00:52 2024 ] 
Training: Epoch [108/120], Step [2499], Loss: 0.06722099334001541, Training Accuracy: 97.685
[ Thu Jul  4 17:00:53 2024 ] 	Batch(2500/7879) done. Loss: 0.0579  lr:0.000001
[ Thu Jul  4 17:01:10 2024 ] 	Batch(2600/7879) done. Loss: 0.1320  lr:0.000001
[ Thu Jul  4 17:01:28 2024 ] 	Batch(2700/7879) done. Loss: 0.0126  lr:0.000001
[ Thu Jul  4 17:01:46 2024 ] 	Batch(2800/7879) done. Loss: 0.0155  lr:0.000001
[ Thu Jul  4 17:02:04 2024 ] 	Batch(2900/7879) done. Loss: 0.0198  lr:0.000001
[ Thu Jul  4 17:02:22 2024 ] 
Training: Epoch [108/120], Step [2999], Loss: 0.024866603314876556, Training Accuracy: 97.6375
[ Thu Jul  4 17:02:22 2024 ] 	Batch(3000/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul  4 17:02:40 2024 ] 	Batch(3100/7879) done. Loss: 0.0013  lr:0.000001
[ Thu Jul  4 17:02:58 2024 ] 	Batch(3200/7879) done. Loss: 0.1293  lr:0.000001
[ Thu Jul  4 17:03:16 2024 ] 	Batch(3300/7879) done. Loss: 0.3123  lr:0.000001
[ Thu Jul  4 17:03:34 2024 ] 	Batch(3400/7879) done. Loss: 0.0165  lr:0.000001
[ Thu Jul  4 17:03:52 2024 ] 
Training: Epoch [108/120], Step [3499], Loss: 0.08593114465475082, Training Accuracy: 97.68571428571428
[ Thu Jul  4 17:03:52 2024 ] 	Batch(3500/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul  4 17:04:10 2024 ] 	Batch(3600/7879) done. Loss: 0.0052  lr:0.000001
[ Thu Jul  4 17:04:28 2024 ] 	Batch(3700/7879) done. Loss: 0.0607  lr:0.000001
[ Thu Jul  4 17:04:46 2024 ] 	Batch(3800/7879) done. Loss: 0.0151  lr:0.000001
[ Thu Jul  4 17:05:03 2024 ] 	Batch(3900/7879) done. Loss: 0.0486  lr:0.000001
[ Thu Jul  4 17:05:21 2024 ] 
Training: Epoch [108/120], Step [3999], Loss: 0.042665597051382065, Training Accuracy: 97.696875
[ Thu Jul  4 17:05:21 2024 ] 	Batch(4000/7879) done. Loss: 0.0763  lr:0.000001
[ Thu Jul  4 17:05:39 2024 ] 	Batch(4100/7879) done. Loss: 0.0687  lr:0.000001
[ Thu Jul  4 17:05:57 2024 ] 	Batch(4200/7879) done. Loss: 0.2021  lr:0.000001
[ Thu Jul  4 17:06:15 2024 ] 	Batch(4300/7879) done. Loss: 0.0774  lr:0.000001
[ Thu Jul  4 17:06:33 2024 ] 	Batch(4400/7879) done. Loss: 0.0777  lr:0.000001
[ Thu Jul  4 17:06:51 2024 ] 
Training: Epoch [108/120], Step [4499], Loss: 0.21892641484737396, Training Accuracy: 97.71944444444445
[ Thu Jul  4 17:06:52 2024 ] 	Batch(4500/7879) done. Loss: 0.0022  lr:0.000001
[ Thu Jul  4 17:07:09 2024 ] 	Batch(4600/7879) done. Loss: 0.1309  lr:0.000001
[ Thu Jul  4 17:07:27 2024 ] 	Batch(4700/7879) done. Loss: 0.0059  lr:0.000001
[ Thu Jul  4 17:07:45 2024 ] 	Batch(4800/7879) done. Loss: 0.0467  lr:0.000001
[ Thu Jul  4 17:08:03 2024 ] 	Batch(4900/7879) done. Loss: 0.0535  lr:0.000001
[ Thu Jul  4 17:08:21 2024 ] 
Training: Epoch [108/120], Step [4999], Loss: 0.030175717547535896, Training Accuracy: 97.735
[ Thu Jul  4 17:08:21 2024 ] 	Batch(5000/7879) done. Loss: 0.3040  lr:0.000001
[ Thu Jul  4 17:08:39 2024 ] 	Batch(5100/7879) done. Loss: 0.0263  lr:0.000001
[ Thu Jul  4 17:08:57 2024 ] 	Batch(5200/7879) done. Loss: 0.0874  lr:0.000001
[ Thu Jul  4 17:09:15 2024 ] 	Batch(5300/7879) done. Loss: 0.3587  lr:0.000001
[ Thu Jul  4 17:09:33 2024 ] 	Batch(5400/7879) done. Loss: 0.2925  lr:0.000001
[ Thu Jul  4 17:09:51 2024 ] 
Training: Epoch [108/120], Step [5499], Loss: 0.008757615461945534, Training Accuracy: 97.70454545454545
[ Thu Jul  4 17:09:51 2024 ] 	Batch(5500/7879) done. Loss: 0.0060  lr:0.000001
[ Thu Jul  4 21:52:17 2024 ] Load weights from prova20/epoch107_model.pt.
[ Thu Jul  4 21:52:17 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xsub/train_joint_120.npy', 'label_path': 'new_data_processed/xsub/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xsub/val_joint_120.npy', 'label_path': 'new_data_processed/xsub/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': True, 'only_attention': True, 'tcn_attention': False, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': 'prova20/epoch107_model.pt', 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': True, 'scheduler': 1, 'base_lr': 1e-06, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Thu Jul  4 21:52:17 2024 ] Training epoch: 1
[ Thu Jul  4 21:52:20 2024 ] 	Batch(0/7879) done. Loss: 0.0168  lr:0.000001
[ Thu Jul  4 21:52:37 2024 ] 	Batch(100/7879) done. Loss: 0.0247  lr:0.000001
[ Thu Jul  4 21:52:54 2024 ] 	Batch(200/7879) done. Loss: 0.1065  lr:0.000001
[ Thu Jul  4 21:53:12 2024 ] 	Batch(300/7879) done. Loss: 0.0268  lr:0.000001
[ Thu Jul  4 21:53:29 2024 ] 	Batch(400/7879) done. Loss: 0.5518  lr:0.000001
[ Thu Jul  4 21:53:47 2024 ] 
Training: Epoch [0/120], Step [499], Loss: 0.01707024872303009, Training Accuracy: 97.625
[ Thu Jul  4 21:53:47 2024 ] 	Batch(500/7879) done. Loss: 0.0815  lr:0.000001
[ Thu Jul  4 21:54:04 2024 ] 	Batch(600/7879) done. Loss: 0.0049  lr:0.000001
[ Thu Jul  4 21:54:22 2024 ] 	Batch(700/7879) done. Loss: 0.0426  lr:0.000001
[ Thu Jul  4 21:54:40 2024 ] 	Batch(800/7879) done. Loss: 0.0343  lr:0.000001
[ Thu Jul  4 21:54:58 2024 ] 	Batch(900/7879) done. Loss: 0.0866  lr:0.000001
[ Thu Jul  4 21:55:15 2024 ] 
Training: Epoch [0/120], Step [999], Loss: 0.03313727676868439, Training Accuracy: 97.75
[ Thu Jul  4 21:55:15 2024 ] 	Batch(1000/7879) done. Loss: 0.0854  lr:0.000001
[ Thu Jul  4 21:55:33 2024 ] 	Batch(1100/7879) done. Loss: 0.1748  lr:0.000001
[ Thu Jul  4 21:55:50 2024 ] 	Batch(1200/7879) done. Loss: 0.0208  lr:0.000001
[ Thu Jul  4 21:56:08 2024 ] 	Batch(1300/7879) done. Loss: 0.1010  lr:0.000001
[ Thu Jul  4 21:56:26 2024 ] 	Batch(1400/7879) done. Loss: 0.0413  lr:0.000001
[ Thu Jul  4 21:56:44 2024 ] 
Training: Epoch [0/120], Step [1499], Loss: 0.05800902470946312, Training Accuracy: 97.70833333333333
[ Thu Jul  4 21:56:44 2024 ] 	Batch(1500/7879) done. Loss: 0.0205  lr:0.000001
[ Thu Jul  4 21:57:02 2024 ] 	Batch(1600/7879) done. Loss: 0.0595  lr:0.000001
[ Thu Jul  4 21:57:20 2024 ] 	Batch(1700/7879) done. Loss: 0.0767  lr:0.000001
[ Thu Jul  4 21:57:38 2024 ] 	Batch(1800/7879) done. Loss: 0.0116  lr:0.000001
[ Thu Jul  4 21:57:56 2024 ] 	Batch(1900/7879) done. Loss: 0.0213  lr:0.000001
[ Thu Jul  4 21:58:13 2024 ] 
Training: Epoch [0/120], Step [1999], Loss: 0.004264410585165024, Training Accuracy: 97.775
[ Thu Jul  4 21:58:14 2024 ] 	Batch(2000/7879) done. Loss: 0.1387  lr:0.000001
[ Thu Jul  4 21:58:31 2024 ] 	Batch(2100/7879) done. Loss: 0.1047  lr:0.000001
[ Thu Jul  4 21:58:48 2024 ] 	Batch(2200/7879) done. Loss: 0.0746  lr:0.000001
[ Thu Jul  4 21:59:05 2024 ] 	Batch(2300/7879) done. Loss: 0.0566  lr:0.000001
[ Thu Jul  4 21:59:23 2024 ] 	Batch(2400/7879) done. Loss: 0.4373  lr:0.000001
[ Thu Jul  4 21:59:41 2024 ] 
Training: Epoch [0/120], Step [2499], Loss: 0.10513263940811157, Training Accuracy: 97.75500000000001
[ Thu Jul  4 21:59:41 2024 ] 	Batch(2500/7879) done. Loss: 0.0812  lr:0.000001
[ Thu Jul  4 21:59:59 2024 ] 	Batch(2600/7879) done. Loss: 0.0405  lr:0.000001
[ Thu Jul  4 22:00:16 2024 ] 	Batch(2700/7879) done. Loss: 0.1086  lr:0.000001
[ Thu Jul  4 22:00:34 2024 ] 	Batch(2800/7879) done. Loss: 0.7128  lr:0.000001
[ Thu Jul  4 22:00:52 2024 ] 	Batch(2900/7879) done. Loss: 0.3789  lr:0.000001
[ Thu Jul  4 22:01:10 2024 ] 
Training: Epoch [0/120], Step [2999], Loss: 0.24998335540294647, Training Accuracy: 97.7125
[ Thu Jul  4 22:01:10 2024 ] 	Batch(3000/7879) done. Loss: 0.0568  lr:0.000001
[ Thu Jul  4 22:01:28 2024 ] 	Batch(3100/7879) done. Loss: 0.0579  lr:0.000001
[ Thu Jul  4 22:01:46 2024 ] 	Batch(3200/7879) done. Loss: 0.1840  lr:0.000001
[ Thu Jul  4 22:02:04 2024 ] 	Batch(3300/7879) done. Loss: 0.0296  lr:0.000001
[ Thu Jul  4 22:02:21 2024 ] 	Batch(3400/7879) done. Loss: 0.2558  lr:0.000001
[ Thu Jul  4 22:02:39 2024 ] 
Training: Epoch [0/120], Step [3499], Loss: 0.02002560719847679, Training Accuracy: 97.77142857142857
[ Thu Jul  4 22:02:39 2024 ] 	Batch(3500/7879) done. Loss: 0.1916  lr:0.000001
[ Thu Jul  4 22:02:57 2024 ] 	Batch(3600/7879) done. Loss: 0.1656  lr:0.000001
[ Thu Jul  4 22:03:14 2024 ] 	Batch(3700/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul  4 22:03:32 2024 ] 	Batch(3800/7879) done. Loss: 0.0471  lr:0.000001
[ Thu Jul  4 22:03:49 2024 ] 	Batch(3900/7879) done. Loss: 0.0116  lr:0.000001
[ Thu Jul  4 22:04:06 2024 ] 
Training: Epoch [0/120], Step [3999], Loss: 0.005401480942964554, Training Accuracy: 97.775
[ Thu Jul  4 22:04:06 2024 ] 	Batch(4000/7879) done. Loss: 0.0366  lr:0.000001
[ Thu Jul  4 22:04:24 2024 ] 	Batch(4100/7879) done. Loss: 0.0658  lr:0.000001
[ Thu Jul  4 22:04:42 2024 ] 	Batch(4200/7879) done. Loss: 0.0137  lr:0.000001
[ Thu Jul  4 22:05:00 2024 ] 	Batch(4300/7879) done. Loss: 0.0204  lr:0.000001
[ Thu Jul  4 22:05:18 2024 ] 	Batch(4400/7879) done. Loss: 0.0264  lr:0.000001
[ Thu Jul  4 22:05:35 2024 ] 
Training: Epoch [0/120], Step [4499], Loss: 0.08472003042697906, Training Accuracy: 97.81388888888888
[ Thu Jul  4 22:05:35 2024 ] 	Batch(4500/7879) done. Loss: 0.0081  lr:0.000001
[ Thu Jul  4 22:05:52 2024 ] 	Batch(4600/7879) done. Loss: 0.0017  lr:0.000001
[ Thu Jul  4 22:06:09 2024 ] 	Batch(4700/7879) done. Loss: 0.0075  lr:0.000001
[ Thu Jul  4 22:06:27 2024 ] 	Batch(4800/7879) done. Loss: 0.2208  lr:0.000001
[ Thu Jul  4 22:06:44 2024 ] 	Batch(4900/7879) done. Loss: 0.2945  lr:0.000001
[ Thu Jul  4 22:07:02 2024 ] 
Training: Epoch [0/120], Step [4999], Loss: 0.07794429361820221, Training Accuracy: 97.815
[ Thu Jul  4 22:07:02 2024 ] 	Batch(5000/7879) done. Loss: 0.2215  lr:0.000001
[ Thu Jul  4 22:07:20 2024 ] 	Batch(5100/7879) done. Loss: 0.3518  lr:0.000001
[ Thu Jul  4 22:07:38 2024 ] 	Batch(5200/7879) done. Loss: 0.0355  lr:0.000001
[ Thu Jul  4 22:07:55 2024 ] 	Batch(5300/7879) done. Loss: 0.0886  lr:0.000001
[ Thu Jul  4 22:08:13 2024 ] 	Batch(5400/7879) done. Loss: 0.1206  lr:0.000001
[ Thu Jul  4 22:08:30 2024 ] 
Training: Epoch [0/120], Step [5499], Loss: 0.004713968839496374, Training Accuracy: 97.82954545454545
[ Thu Jul  4 22:08:30 2024 ] 	Batch(5500/7879) done. Loss: 0.0461  lr:0.000001
[ Thu Jul  4 22:08:47 2024 ] 	Batch(5600/7879) done. Loss: 0.0054  lr:0.000001
[ Thu Jul  4 22:09:05 2024 ] 	Batch(5700/7879) done. Loss: 0.1358  lr:0.000001
[ Thu Jul  4 22:09:23 2024 ] 	Batch(5800/7879) done. Loss: 0.0372  lr:0.000001
[ Thu Jul  4 22:09:41 2024 ] 	Batch(5900/7879) done. Loss: 0.0943  lr:0.000001
[ Thu Jul  4 22:09:59 2024 ] 
Training: Epoch [0/120], Step [5999], Loss: 0.01258148718625307, Training Accuracy: 97.78541666666666
[ Thu Jul  4 22:09:59 2024 ] 	Batch(6000/7879) done. Loss: 0.0045  lr:0.000001
[ Thu Jul  4 22:10:17 2024 ] 	Batch(6100/7879) done. Loss: 0.0189  lr:0.000001
[ Thu Jul  4 22:10:35 2024 ] 	Batch(6200/7879) done. Loss: 0.0289  lr:0.000001
[ Thu Jul  4 22:10:52 2024 ] 	Batch(6300/7879) done. Loss: 0.0379  lr:0.000001
[ Thu Jul  4 22:11:10 2024 ] 	Batch(6400/7879) done. Loss: 0.0346  lr:0.000001
[ Thu Jul  4 22:11:27 2024 ] 
Training: Epoch [0/120], Step [6499], Loss: 0.07747440040111542, Training Accuracy: 97.76346153846154
[ Thu Jul  4 22:11:28 2024 ] 	Batch(6500/7879) done. Loss: 0.1245  lr:0.000001
[ Thu Jul  4 22:11:45 2024 ] 	Batch(6600/7879) done. Loss: 0.1369  lr:0.000001
[ Thu Jul  4 22:12:02 2024 ] 	Batch(6700/7879) done. Loss: 0.1077  lr:0.000001
[ Thu Jul  4 22:12:19 2024 ] 	Batch(6800/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul  4 22:12:37 2024 ] 	Batch(6900/7879) done. Loss: 0.0670  lr:0.000001
[ Thu Jul  4 22:12:54 2024 ] 
Training: Epoch [0/120], Step [6999], Loss: 0.009144963696599007, Training Accuracy: 97.74821428571428
[ Thu Jul  4 22:12:54 2024 ] 	Batch(7000/7879) done. Loss: 0.1750  lr:0.000001
[ Thu Jul  4 22:13:11 2024 ] 	Batch(7100/7879) done. Loss: 0.0282  lr:0.000001
[ Thu Jul  4 22:13:28 2024 ] 	Batch(7200/7879) done. Loss: 0.1300  lr:0.000001
[ Thu Jul  4 22:13:46 2024 ] 	Batch(7300/7879) done. Loss: 0.1202  lr:0.000001
[ Thu Jul  4 22:14:04 2024 ] 	Batch(7400/7879) done. Loss: 0.0904  lr:0.000001
[ Thu Jul  4 22:14:22 2024 ] 
Training: Epoch [0/120], Step [7499], Loss: 0.09006580710411072, Training Accuracy: 97.73666666666668
[ Thu Jul  4 22:14:22 2024 ] 	Batch(7500/7879) done. Loss: 0.0216  lr:0.000001
[ Thu Jul  4 22:14:40 2024 ] 	Batch(7600/7879) done. Loss: 0.1912  lr:0.000001
[ Thu Jul  4 22:14:58 2024 ] 	Batch(7700/7879) done. Loss: 0.4047  lr:0.000001
[ Thu Jul  4 22:15:15 2024 ] 	Batch(7800/7879) done. Loss: 0.0347  lr:0.000001
[ Thu Jul  4 22:15:29 2024 ] 	Mean training loss: 0.0935.
[ Thu Jul  4 22:15:29 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 22:15:30 2024 ] Training epoch: 2
[ Thu Jul  4 22:15:30 2024 ] 	Batch(0/7879) done. Loss: 0.0243  lr:0.000001
[ Thu Jul  4 22:15:48 2024 ] 	Batch(100/7879) done. Loss: 0.0367  lr:0.000001
[ Thu Jul  4 22:16:06 2024 ] 	Batch(200/7879) done. Loss: 0.0242  lr:0.000001
[ Thu Jul  4 22:16:24 2024 ] 	Batch(300/7879) done. Loss: 0.0333  lr:0.000001
[ Thu Jul  4 22:16:42 2024 ] 	Batch(400/7879) done. Loss: 0.0129  lr:0.000001
[ Thu Jul  4 22:17:00 2024 ] 
Training: Epoch [1/120], Step [499], Loss: 0.003178671235218644, Training Accuracy: 97.55
[ Thu Jul  4 22:17:00 2024 ] 	Batch(500/7879) done. Loss: 0.0666  lr:0.000001
[ Thu Jul  4 22:17:18 2024 ] 	Batch(600/7879) done. Loss: 0.1284  lr:0.000001
[ Thu Jul  4 22:17:36 2024 ] 	Batch(700/7879) done. Loss: 0.0155  lr:0.000001
[ Thu Jul  4 22:17:54 2024 ] 	Batch(800/7879) done. Loss: 0.0617  lr:0.000001
[ Thu Jul  4 22:18:12 2024 ] 	Batch(900/7879) done. Loss: 0.1282  lr:0.000001
[ Thu Jul  4 22:18:30 2024 ] 
Training: Epoch [1/120], Step [999], Loss: 0.03275316581130028, Training Accuracy: 97.675
[ Thu Jul  4 22:18:30 2024 ] 	Batch(1000/7879) done. Loss: 0.0596  lr:0.000001
[ Thu Jul  4 22:18:48 2024 ] 	Batch(1100/7879) done. Loss: 0.0244  lr:0.000001
[ Thu Jul  4 22:19:06 2024 ] 	Batch(1200/7879) done. Loss: 0.0026  lr:0.000001
[ Thu Jul  4 22:19:25 2024 ] 	Batch(1300/7879) done. Loss: 0.1898  lr:0.000001
[ Thu Jul  4 22:19:44 2024 ] 	Batch(1400/7879) done. Loss: 0.0523  lr:0.000001
[ Thu Jul  4 22:20:02 2024 ] 
Training: Epoch [1/120], Step [1499], Loss: 0.02169596403837204, Training Accuracy: 97.71666666666667
[ Thu Jul  4 22:20:02 2024 ] 	Batch(1500/7879) done. Loss: 0.0064  lr:0.000001
[ Thu Jul  4 22:20:21 2024 ] 	Batch(1600/7879) done. Loss: 0.1062  lr:0.000001
[ Thu Jul  4 22:20:39 2024 ] 	Batch(1700/7879) done. Loss: 0.2102  lr:0.000001
[ Thu Jul  4 22:20:57 2024 ] 	Batch(1800/7879) done. Loss: 0.0260  lr:0.000001
[ Thu Jul  4 22:21:15 2024 ] 	Batch(1900/7879) done. Loss: 0.0174  lr:0.000001
[ Thu Jul  4 22:21:33 2024 ] 
Training: Epoch [1/120], Step [1999], Loss: 0.021728400141000748, Training Accuracy: 97.6625
[ Thu Jul  4 22:21:33 2024 ] 	Batch(2000/7879) done. Loss: 0.0261  lr:0.000001
[ Thu Jul  4 22:21:52 2024 ] 	Batch(2100/7879) done. Loss: 0.0516  lr:0.000001
[ Thu Jul  4 22:22:11 2024 ] 	Batch(2200/7879) done. Loss: 0.0074  lr:0.000001
[ Thu Jul  4 22:22:29 2024 ] 	Batch(2300/7879) done. Loss: 0.1065  lr:0.000001
[ Thu Jul  4 22:22:48 2024 ] 	Batch(2400/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul  4 22:23:06 2024 ] 
Training: Epoch [1/120], Step [2499], Loss: 0.20658710598945618, Training Accuracy: 97.68
[ Thu Jul  4 22:23:06 2024 ] 	Batch(2500/7879) done. Loss: 0.0638  lr:0.000001
[ Thu Jul  4 22:23:24 2024 ] 	Batch(2600/7879) done. Loss: 0.0351  lr:0.000001
[ Thu Jul  4 22:23:42 2024 ] 	Batch(2700/7879) done. Loss: 0.1284  lr:0.000001
[ Thu Jul  4 22:24:00 2024 ] 	Batch(2800/7879) done. Loss: 0.1934  lr:0.000001
[ Thu Jul  4 22:24:19 2024 ] 	Batch(2900/7879) done. Loss: 0.3898  lr:0.000001
[ Thu Jul  4 22:24:37 2024 ] 
Training: Epoch [1/120], Step [2999], Loss: 0.03540650010108948, Training Accuracy: 97.67916666666666
[ Thu Jul  4 22:24:37 2024 ] 	Batch(3000/7879) done. Loss: 0.0709  lr:0.000001
[ Thu Jul  4 22:24:56 2024 ] 	Batch(3100/7879) done. Loss: 0.0113  lr:0.000001
[ Thu Jul  4 22:25:15 2024 ] 	Batch(3200/7879) done. Loss: 0.0154  lr:0.000001
[ Thu Jul  4 22:25:33 2024 ] 	Batch(3300/7879) done. Loss: 0.0127  lr:0.000001
[ Thu Jul  4 22:25:51 2024 ] 	Batch(3400/7879) done. Loss: 0.1286  lr:0.000001
[ Thu Jul  4 22:26:09 2024 ] 
Training: Epoch [1/120], Step [3499], Loss: 0.1136481836438179, Training Accuracy: 97.63928571428572
[ Thu Jul  4 22:26:09 2024 ] 	Batch(3500/7879) done. Loss: 0.0144  lr:0.000001
[ Thu Jul  4 22:26:28 2024 ] 	Batch(3600/7879) done. Loss: 0.0732  lr:0.000001
[ Thu Jul  4 22:26:46 2024 ] 	Batch(3700/7879) done. Loss: 0.0092  lr:0.000001
[ Thu Jul  4 22:27:05 2024 ] 	Batch(3800/7879) done. Loss: 0.0526  lr:0.000001
[ Thu Jul  4 22:27:24 2024 ] 	Batch(3900/7879) done. Loss: 0.0982  lr:0.000001
[ Thu Jul  4 22:27:42 2024 ] 
Training: Epoch [1/120], Step [3999], Loss: 0.030481282621622086, Training Accuracy: 97.621875
[ Thu Jul  4 22:27:42 2024 ] 	Batch(4000/7879) done. Loss: 0.0183  lr:0.000001
[ Thu Jul  4 22:28:01 2024 ] 	Batch(4100/7879) done. Loss: 0.0050  lr:0.000001
[ Thu Jul  4 22:28:19 2024 ] 	Batch(4200/7879) done. Loss: 0.0078  lr:0.000001
[ Thu Jul  4 22:28:38 2024 ] 	Batch(4300/7879) done. Loss: 0.0572  lr:0.000001
[ Thu Jul  4 22:28:57 2024 ] 	Batch(4400/7879) done. Loss: 0.0155  lr:0.000001
[ Thu Jul  4 22:29:15 2024 ] 
Training: Epoch [1/120], Step [4499], Loss: 0.036956992000341415, Training Accuracy: 97.66388888888888
[ Thu Jul  4 22:29:16 2024 ] 	Batch(4500/7879) done. Loss: 0.0773  lr:0.000001
[ Thu Jul  4 22:29:34 2024 ] 	Batch(4600/7879) done. Loss: 0.0059  lr:0.000001
[ Thu Jul  4 22:29:53 2024 ] 	Batch(4700/7879) done. Loss: 0.2787  lr:0.000001
[ Thu Jul  4 22:30:11 2024 ] 	Batch(4800/7879) done. Loss: 0.0629  lr:0.000001
[ Thu Jul  4 22:30:30 2024 ] 	Batch(4900/7879) done. Loss: 0.1064  lr:0.000001
[ Thu Jul  4 22:30:47 2024 ] 
Training: Epoch [1/120], Step [4999], Loss: 0.14701639115810394, Training Accuracy: 97.66749999999999
[ Thu Jul  4 22:30:48 2024 ] 	Batch(5000/7879) done. Loss: 0.0067  lr:0.000001
[ Thu Jul  4 22:31:06 2024 ] 	Batch(5100/7879) done. Loss: 0.0022  lr:0.000001
[ Thu Jul  4 22:31:24 2024 ] 	Batch(5200/7879) done. Loss: 0.1024  lr:0.000001
[ Thu Jul  4 22:31:42 2024 ] 	Batch(5300/7879) done. Loss: 0.1637  lr:0.000001
[ Thu Jul  4 22:32:01 2024 ] 	Batch(5400/7879) done. Loss: 0.0134  lr:0.000001
[ Thu Jul  4 22:32:19 2024 ] 
Training: Epoch [1/120], Step [5499], Loss: 0.21542713046073914, Training Accuracy: 97.7
[ Thu Jul  4 22:32:20 2024 ] 	Batch(5500/7879) done. Loss: 0.2288  lr:0.000001
[ Thu Jul  4 22:32:38 2024 ] 	Batch(5600/7879) done. Loss: 0.0023  lr:0.000001
[ Thu Jul  4 22:32:56 2024 ] 	Batch(5700/7879) done. Loss: 0.1238  lr:0.000001
[ Thu Jul  4 22:33:14 2024 ] 	Batch(5800/7879) done. Loss: 0.1051  lr:0.000001
[ Thu Jul  4 22:33:33 2024 ] 	Batch(5900/7879) done. Loss: 0.3820  lr:0.000001
[ Thu Jul  4 22:33:50 2024 ] 
Training: Epoch [1/120], Step [5999], Loss: 0.019138606265187263, Training Accuracy: 97.675
[ Thu Jul  4 22:33:51 2024 ] 	Batch(6000/7879) done. Loss: 0.0095  lr:0.000001
[ Thu Jul  4 22:34:09 2024 ] 	Batch(6100/7879) done. Loss: 0.0836  lr:0.000001
[ Thu Jul  4 22:34:27 2024 ] 	Batch(6200/7879) done. Loss: 0.0447  lr:0.000001
[ Thu Jul  4 22:34:45 2024 ] 	Batch(6300/7879) done. Loss: 0.0213  lr:0.000001
[ Thu Jul  4 22:35:03 2024 ] 	Batch(6400/7879) done. Loss: 0.1817  lr:0.000001
[ Thu Jul  4 22:35:21 2024 ] 
Training: Epoch [1/120], Step [6499], Loss: 0.09170223027467728, Training Accuracy: 97.68461538461538
[ Thu Jul  4 22:35:21 2024 ] 	Batch(6500/7879) done. Loss: 0.0149  lr:0.000001
[ Thu Jul  4 22:35:39 2024 ] 	Batch(6600/7879) done. Loss: 0.0204  lr:0.000001
[ Thu Jul  4 22:35:57 2024 ] 	Batch(6700/7879) done. Loss: 0.2525  lr:0.000001
[ Thu Jul  4 22:36:15 2024 ] 	Batch(6800/7879) done. Loss: 0.1333  lr:0.000001
[ Thu Jul  4 22:36:33 2024 ] 	Batch(6900/7879) done. Loss: 0.0305  lr:0.000001
[ Thu Jul  4 22:36:51 2024 ] 
Training: Epoch [1/120], Step [6999], Loss: 0.07737500220537186, Training Accuracy: 97.66964285714286
[ Thu Jul  4 22:36:51 2024 ] 	Batch(7000/7879) done. Loss: 0.0352  lr:0.000001
[ Thu Jul  4 22:37:09 2024 ] 	Batch(7100/7879) done. Loss: 0.0244  lr:0.000001
[ Thu Jul  4 22:37:27 2024 ] 	Batch(7200/7879) done. Loss: 0.0108  lr:0.000001
[ Thu Jul  4 22:37:46 2024 ] 	Batch(7300/7879) done. Loss: 0.0323  lr:0.000001
[ Thu Jul  4 22:38:05 2024 ] 	Batch(7400/7879) done. Loss: 0.0663  lr:0.000001
[ Thu Jul  4 22:38:23 2024 ] 
Training: Epoch [1/120], Step [7499], Loss: 0.12135819345712662, Training Accuracy: 97.69
[ Thu Jul  4 22:38:23 2024 ] 	Batch(7500/7879) done. Loss: 0.0195  lr:0.000001
[ Thu Jul  4 22:38:42 2024 ] 	Batch(7600/7879) done. Loss: 0.0195  lr:0.000001
[ Thu Jul  4 22:39:00 2024 ] 	Batch(7700/7879) done. Loss: 0.1019  lr:0.000001
[ Thu Jul  4 22:39:18 2024 ] 	Batch(7800/7879) done. Loss: 0.5255  lr:0.000001
[ Thu Jul  4 22:39:32 2024 ] 	Mean training loss: 0.0940.
[ Thu Jul  4 22:39:32 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul  4 22:39:32 2024 ] Training epoch: 3
[ Thu Jul  4 22:39:32 2024 ] 	Batch(0/7879) done. Loss: 0.0054  lr:0.000001
[ Thu Jul  4 22:39:50 2024 ] 	Batch(100/7879) done. Loss: 0.1929  lr:0.000001
[ Thu Jul  4 22:40:08 2024 ] 	Batch(200/7879) done. Loss: 0.0020  lr:0.000001
[ Thu Jul  4 22:40:26 2024 ] 	Batch(300/7879) done. Loss: 0.2077  lr:0.000001
[ Thu Jul  4 22:40:44 2024 ] 	Batch(400/7879) done. Loss: 0.2196  lr:0.000001
[ Thu Jul  4 22:41:02 2024 ] 
Training: Epoch [2/120], Step [499], Loss: 0.004588389303535223, Training Accuracy: 97.575
[ Thu Jul  4 22:41:02 2024 ] 	Batch(500/7879) done. Loss: 0.0766  lr:0.000001
[ Thu Jul  4 22:41:20 2024 ] 	Batch(600/7879) done. Loss: 0.1304  lr:0.000001
[ Thu Jul  4 22:41:38 2024 ] 	Batch(700/7879) done. Loss: 0.0081  lr:0.000001
[ Thu Jul  4 22:41:56 2024 ] 	Batch(800/7879) done. Loss: 0.0260  lr:0.000001
[ Thu Jul  4 22:42:15 2024 ] 	Batch(900/7879) done. Loss: 0.0107  lr:0.000001
[ Thu Jul  4 22:42:33 2024 ] 
Training: Epoch [2/120], Step [999], Loss: 0.011424493975937366, Training Accuracy: 97.5875
[ Thu Jul  4 22:42:33 2024 ] 	Batch(1000/7879) done. Loss: 0.0458  lr:0.000001
[ Thu Jul  4 22:42:51 2024 ] 	Batch(1100/7879) done. Loss: 0.0154  lr:0.000001
[ Thu Jul  4 22:43:10 2024 ] 	Batch(1200/7879) done. Loss: 0.0991  lr:0.000001
[ Thu Jul  4 22:43:27 2024 ] 	Batch(1300/7879) done. Loss: 0.0381  lr:0.000001
[ Thu Jul  4 22:43:45 2024 ] 	Batch(1400/7879) done. Loss: 0.0408  lr:0.000001
[ Thu Jul  4 22:44:03 2024 ] 
Training: Epoch [2/120], Step [1499], Loss: 0.06278041750192642, Training Accuracy: 97.53333333333333
[ Thu Jul  4 22:44:03 2024 ] 	Batch(1500/7879) done. Loss: 0.0043  lr:0.000001
[ Thu Jul  4 22:44:21 2024 ] 	Batch(1600/7879) done. Loss: 0.0007  lr:0.000001
[ Thu Jul  4 22:44:39 2024 ] 	Batch(1700/7879) done. Loss: 0.1996  lr:0.000001
[ Thu Jul  4 22:44:57 2024 ] 	Batch(1800/7879) done. Loss: 0.0279  lr:0.000001
[ Thu Jul  4 22:45:15 2024 ] 	Batch(1900/7879) done. Loss: 0.0369  lr:0.000001
[ Thu Jul  4 22:45:32 2024 ] 
Training: Epoch [2/120], Step [1999], Loss: 0.050447940826416016, Training Accuracy: 97.5375
[ Thu Jul  4 22:45:33 2024 ] 	Batch(2000/7879) done. Loss: 0.0415  lr:0.000001
[ Thu Jul  4 22:45:51 2024 ] 	Batch(2100/7879) done. Loss: 0.0304  lr:0.000001
[ Thu Jul  4 22:46:09 2024 ] 	Batch(2200/7879) done. Loss: 0.0244  lr:0.000001
[ Thu Jul  4 22:46:26 2024 ] 	Batch(2300/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul  4 22:46:44 2024 ] 	Batch(2400/7879) done. Loss: 0.0389  lr:0.000001
[ Thu Jul  4 22:47:02 2024 ] 
Training: Epoch [2/120], Step [2499], Loss: 0.16048961877822876, Training Accuracy: 97.50999999999999
[ Thu Jul  4 22:47:02 2024 ] 	Batch(2500/7879) done. Loss: 0.0199  lr:0.000001
[ Thu Jul  4 22:47:20 2024 ] 	Batch(2600/7879) done. Loss: 0.0501  lr:0.000001
[ Thu Jul  4 22:47:38 2024 ] 	Batch(2700/7879) done. Loss: 0.0330  lr:0.000001
[ Thu Jul  4 22:47:56 2024 ] 	Batch(2800/7879) done. Loss: 0.0408  lr:0.000001
[ Thu Jul  4 22:48:14 2024 ] 	Batch(2900/7879) done. Loss: 0.0026  lr:0.000001
[ Thu Jul  4 22:48:32 2024 ] 
Training: Epoch [2/120], Step [2999], Loss: 0.06968249380588531, Training Accuracy: 97.5625
[ Thu Jul  4 22:48:32 2024 ] 	Batch(3000/7879) done. Loss: 0.0909  lr:0.000001
[ Thu Jul  4 22:48:50 2024 ] 	Batch(3100/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul  4 22:49:08 2024 ] 	Batch(3200/7879) done. Loss: 0.2421  lr:0.000001
[ Thu Jul  4 22:49:26 2024 ] 	Batch(3300/7879) done. Loss: 0.8614  lr:0.000001
[ Thu Jul  4 22:49:45 2024 ] 	Batch(3400/7879) done. Loss: 0.1116  lr:0.000001
[ Thu Jul  4 22:50:03 2024 ] 
Training: Epoch [2/120], Step [3499], Loss: 0.05073244869709015, Training Accuracy: 97.56428571428572
[ Thu Jul  4 22:50:03 2024 ] 	Batch(3500/7879) done. Loss: 0.0256  lr:0.000001
[ Thu Jul  4 22:50:22 2024 ] 	Batch(3600/7879) done. Loss: 0.0494  lr:0.000001
[ Thu Jul  4 22:50:40 2024 ] 	Batch(3700/7879) done. Loss: 0.1372  lr:0.000001
[ Thu Jul  4 22:50:58 2024 ] 	Batch(3800/7879) done. Loss: 0.0116  lr:0.000001
[ Thu Jul  4 22:51:16 2024 ] 	Batch(3900/7879) done. Loss: 0.0757  lr:0.000001
[ Thu Jul  4 22:51:33 2024 ] 
Training: Epoch [2/120], Step [3999], Loss: 0.04502904415130615, Training Accuracy: 97.57187499999999
[ Thu Jul  4 22:51:34 2024 ] 	Batch(4000/7879) done. Loss: 0.0131  lr:0.000001
[ Thu Jul  4 22:51:52 2024 ] 	Batch(4100/7879) done. Loss: 0.1128  lr:0.000001
[ Thu Jul  4 22:52:10 2024 ] 	Batch(4200/7879) done. Loss: 0.0519  lr:0.000001
[ Thu Jul  4 22:52:27 2024 ] 	Batch(4300/7879) done. Loss: 0.1823  lr:0.000001
[ Thu Jul  4 22:52:46 2024 ] 	Batch(4400/7879) done. Loss: 0.0371  lr:0.000001
[ Thu Jul  4 22:53:03 2024 ] 
Training: Epoch [2/120], Step [4499], Loss: 0.03254523128271103, Training Accuracy: 97.61944444444445
[ Thu Jul  4 22:53:03 2024 ] 	Batch(4500/7879) done. Loss: 0.0188  lr:0.000001
[ Thu Jul  4 22:53:21 2024 ] 	Batch(4600/7879) done. Loss: 0.2113  lr:0.000001
[ Thu Jul  4 22:53:39 2024 ] 	Batch(4700/7879) done. Loss: 0.0723  lr:0.000001
[ Thu Jul  4 22:53:57 2024 ] 	Batch(4800/7879) done. Loss: 0.3406  lr:0.000001
[ Thu Jul  4 22:54:15 2024 ] 	Batch(4900/7879) done. Loss: 0.1015  lr:0.000001
[ Thu Jul  4 22:54:33 2024 ] 
Training: Epoch [2/120], Step [4999], Loss: 0.22562630474567413, Training Accuracy: 97.63
[ Thu Jul  4 22:54:33 2024 ] 	Batch(5000/7879) done. Loss: 0.1392  lr:0.000001
[ Thu Jul  4 22:54:51 2024 ] 	Batch(5100/7879) done. Loss: 0.1586  lr:0.000001
[ Thu Jul  4 22:55:09 2024 ] 	Batch(5200/7879) done. Loss: 0.1027  lr:0.000001
[ Thu Jul  4 22:55:28 2024 ] 	Batch(5300/7879) done. Loss: 0.1814  lr:0.000001
[ Thu Jul  4 22:55:46 2024 ] 	Batch(5400/7879) done. Loss: 0.0388  lr:0.000001
[ Thu Jul  4 22:56:04 2024 ] 
Training: Epoch [2/120], Step [5499], Loss: 0.06525476276874542, Training Accuracy: 97.625
[ Thu Jul  4 22:56:05 2024 ] 	Batch(5500/7879) done. Loss: 0.0366  lr:0.000001
[ Thu Jul  4 22:56:23 2024 ] 	Batch(5600/7879) done. Loss: 0.1462  lr:0.000001
[ Thu Jul  4 22:56:42 2024 ] 	Batch(5700/7879) done. Loss: 0.1508  lr:0.000001
[ Thu Jul  4 22:57:00 2024 ] 	Batch(5800/7879) done. Loss: 0.2305  lr:0.000001
[ Thu Jul  4 22:57:18 2024 ] 	Batch(5900/7879) done. Loss: 0.3016  lr:0.000001
[ Thu Jul  4 22:57:35 2024 ] 
Training: Epoch [2/120], Step [5999], Loss: 0.006140140816569328, Training Accuracy: 97.68333333333334
[ Thu Jul  4 22:57:36 2024 ] 	Batch(6000/7879) done. Loss: 0.1824  lr:0.000001
[ Thu Jul  4 22:57:54 2024 ] 	Batch(6100/7879) done. Loss: 0.0191  lr:0.000001
[ Thu Jul  4 22:58:13 2024 ] 	Batch(6200/7879) done. Loss: 0.0527  lr:0.000001
[ Thu Jul  4 22:58:31 2024 ] 	Batch(6300/7879) done. Loss: 0.0838  lr:0.000001
[ Thu Jul  4 22:58:50 2024 ] 	Batch(6400/7879) done. Loss: 0.0643  lr:0.000001
[ Thu Jul  4 22:59:08 2024 ] 
Training: Epoch [2/120], Step [6499], Loss: 0.17523197829723358, Training Accuracy: 97.70384615384616
[ Thu Jul  4 22:59:08 2024 ] 	Batch(6500/7879) done. Loss: 0.0652  lr:0.000001
[ Thu Jul  4 22:59:26 2024 ] 	Batch(6600/7879) done. Loss: 0.0081  lr:0.000001
[ Thu Jul  4 22:59:44 2024 ] 	Batch(6700/7879) done. Loss: 0.0134  lr:0.000001
[ Thu Jul  4 23:00:02 2024 ] 	Batch(6800/7879) done. Loss: 0.2516  lr:0.000001
[ Thu Jul  4 23:00:19 2024 ] 	Batch(6900/7879) done. Loss: 0.0041  lr:0.000001
[ Thu Jul  4 23:00:37 2024 ] 
Training: Epoch [2/120], Step [6999], Loss: 0.17523552477359772, Training Accuracy: 97.66785714285714
[ Thu Jul  4 23:00:37 2024 ] 	Batch(7000/7879) done. Loss: 0.5837  lr:0.000001
[ Thu Jul  4 23:00:55 2024 ] 	Batch(7100/7879) done. Loss: 0.1315  lr:0.000001
[ Thu Jul  4 23:01:13 2024 ] 	Batch(7200/7879) done. Loss: 0.5092  lr:0.000001
[ Thu Jul  4 23:01:31 2024 ] 	Batch(7300/7879) done. Loss: 0.0407  lr:0.000001
[ Thu Jul  4 23:01:49 2024 ] 	Batch(7400/7879) done. Loss: 0.3228  lr:0.000001
[ Thu Jul  4 23:02:07 2024 ] 
Training: Epoch [2/120], Step [7499], Loss: 0.005846305284649134, Training Accuracy: 97.65333333333334
[ Thu Jul  4 23:02:07 2024 ] 	Batch(7500/7879) done. Loss: 0.0992  lr:0.000001
[ Thu Jul  4 23:02:25 2024 ] 	Batch(7600/7879) done. Loss: 0.1358  lr:0.000001
[ Thu Jul  4 23:02:44 2024 ] 	Batch(7700/7879) done. Loss: 0.0281  lr:0.000001
[ Thu Jul  4 23:03:02 2024 ] 	Batch(7800/7879) done. Loss: 0.0520  lr:0.000001
[ Thu Jul  4 23:03:17 2024 ] 	Mean training loss: 0.0934.
[ Thu Jul  4 23:03:17 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 23:03:17 2024 ] Training epoch: 4
[ Thu Jul  4 23:03:17 2024 ] 	Batch(0/7879) done. Loss: 0.0065  lr:0.000001
[ Thu Jul  4 23:03:35 2024 ] 	Batch(100/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul  4 23:03:53 2024 ] 	Batch(200/7879) done. Loss: 0.0336  lr:0.000001
[ Thu Jul  4 23:04:11 2024 ] 	Batch(300/7879) done. Loss: 0.0076  lr:0.000001
[ Thu Jul  4 23:04:29 2024 ] 	Batch(400/7879) done. Loss: 0.0857  lr:0.000001
[ Thu Jul  4 23:04:47 2024 ] 
Training: Epoch [3/120], Step [499], Loss: 0.005980988964438438, Training Accuracy: 97.5
[ Thu Jul  4 23:04:47 2024 ] 	Batch(500/7879) done. Loss: 0.0438  lr:0.000001
[ Thu Jul  4 23:05:05 2024 ] 	Batch(600/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul  4 23:05:23 2024 ] 	Batch(700/7879) done. Loss: 0.0448  lr:0.000001
[ Thu Jul  4 23:05:41 2024 ] 	Batch(800/7879) done. Loss: 0.0199  lr:0.000001
[ Thu Jul  4 23:05:59 2024 ] 	Batch(900/7879) done. Loss: 0.3241  lr:0.000001
[ Thu Jul  4 23:06:16 2024 ] 
Training: Epoch [3/120], Step [999], Loss: 0.044843558222055435, Training Accuracy: 97.7625
[ Thu Jul  4 23:06:17 2024 ] 	Batch(1000/7879) done. Loss: 0.1535  lr:0.000001
[ Thu Jul  4 23:06:34 2024 ] 	Batch(1100/7879) done. Loss: 0.0050  lr:0.000001
[ Thu Jul  4 23:06:53 2024 ] 	Batch(1200/7879) done. Loss: 0.0456  lr:0.000001
[ Thu Jul  4 23:07:11 2024 ] 	Batch(1300/7879) done. Loss: 0.0013  lr:0.000001
[ Thu Jul  4 23:07:29 2024 ] 	Batch(1400/7879) done. Loss: 0.0442  lr:0.000001
[ Thu Jul  4 23:07:47 2024 ] 
Training: Epoch [3/120], Step [1499], Loss: 0.026858678087592125, Training Accuracy: 97.82499999999999
[ Thu Jul  4 23:07:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0798  lr:0.000001
[ Thu Jul  4 23:08:05 2024 ] 	Batch(1600/7879) done. Loss: 0.0524  lr:0.000001
[ Thu Jul  4 23:08:23 2024 ] 	Batch(1700/7879) done. Loss: 0.0889  lr:0.000001
[ Thu Jul  4 23:08:42 2024 ] 	Batch(1800/7879) done. Loss: 0.0512  lr:0.000001
[ Thu Jul  4 23:09:01 2024 ] 	Batch(1900/7879) done. Loss: 0.0063  lr:0.000001
[ Thu Jul  4 23:09:19 2024 ] 
Training: Epoch [3/120], Step [1999], Loss: 0.18270379304885864, Training Accuracy: 97.8625
[ Thu Jul  4 23:09:19 2024 ] 	Batch(2000/7879) done. Loss: 0.0151  lr:0.000001
[ Thu Jul  4 23:09:37 2024 ] 	Batch(2100/7879) done. Loss: 0.0464  lr:0.000001
[ Thu Jul  4 23:09:55 2024 ] 	Batch(2200/7879) done. Loss: 0.0295  lr:0.000001
[ Thu Jul  4 23:10:13 2024 ] 	Batch(2300/7879) done. Loss: 0.4008  lr:0.000001
[ Thu Jul  4 23:10:31 2024 ] 	Batch(2400/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul  4 23:10:49 2024 ] 
Training: Epoch [3/120], Step [2499], Loss: 0.024372119456529617, Training Accuracy: 97.78999999999999
[ Thu Jul  4 23:10:49 2024 ] 	Batch(2500/7879) done. Loss: 0.2104  lr:0.000001
[ Thu Jul  4 23:11:07 2024 ] 	Batch(2600/7879) done. Loss: 0.0847  lr:0.000001
[ Thu Jul  4 23:11:25 2024 ] 	Batch(2700/7879) done. Loss: 0.0197  lr:0.000001
[ Thu Jul  4 23:11:43 2024 ] 	Batch(2800/7879) done. Loss: 0.0494  lr:0.000001
[ Thu Jul  4 23:12:01 2024 ] 	Batch(2900/7879) done. Loss: 0.0853  lr:0.000001
[ Thu Jul  4 23:12:19 2024 ] 
Training: Epoch [3/120], Step [2999], Loss: 0.06754106283187866, Training Accuracy: 97.79583333333333
[ Thu Jul  4 23:12:19 2024 ] 	Batch(3000/7879) done. Loss: 0.2735  lr:0.000001
[ Thu Jul  4 23:12:37 2024 ] 	Batch(3100/7879) done. Loss: 0.1767  lr:0.000001
[ Thu Jul  4 23:12:55 2024 ] 	Batch(3200/7879) done. Loss: 0.1354  lr:0.000001
[ Thu Jul  4 23:13:13 2024 ] 	Batch(3300/7879) done. Loss: 0.6101  lr:0.000001
[ Thu Jul  4 23:13:32 2024 ] 	Batch(3400/7879) done. Loss: 0.1063  lr:0.000001
[ Thu Jul  4 23:13:50 2024 ] 
Training: Epoch [3/120], Step [3499], Loss: 0.23923751711845398, Training Accuracy: 97.68928571428572
[ Thu Jul  4 23:13:50 2024 ] 	Batch(3500/7879) done. Loss: 0.1968  lr:0.000001
[ Thu Jul  4 23:14:09 2024 ] 	Batch(3600/7879) done. Loss: 0.0084  lr:0.000001
[ Thu Jul  4 23:14:28 2024 ] 	Batch(3700/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul  4 23:14:45 2024 ] 	Batch(3800/7879) done. Loss: 0.2184  lr:0.000001
[ Thu Jul  4 23:15:03 2024 ] 	Batch(3900/7879) done. Loss: 0.1684  lr:0.000001
[ Thu Jul  4 23:15:21 2024 ] 
Training: Epoch [3/120], Step [3999], Loss: 0.18398237228393555, Training Accuracy: 97.64375
[ Thu Jul  4 23:15:21 2024 ] 	Batch(4000/7879) done. Loss: 0.0728  lr:0.000001
[ Thu Jul  4 23:15:40 2024 ] 	Batch(4100/7879) done. Loss: 0.0323  lr:0.000001
[ Thu Jul  4 23:15:58 2024 ] 	Batch(4200/7879) done. Loss: 0.0520  lr:0.000001
[ Thu Jul  4 23:16:17 2024 ] 	Batch(4300/7879) done. Loss: 0.1679  lr:0.000001
[ Thu Jul  4 23:16:35 2024 ] 	Batch(4400/7879) done. Loss: 0.0141  lr:0.000001
[ Thu Jul  4 23:16:53 2024 ] 
Training: Epoch [3/120], Step [4499], Loss: 0.003394986502826214, Training Accuracy: 97.68055555555556
[ Thu Jul  4 23:16:53 2024 ] 	Batch(4500/7879) done. Loss: 0.0332  lr:0.000001
[ Thu Jul  4 23:17:11 2024 ] 	Batch(4600/7879) done. Loss: 0.0134  lr:0.000001
[ Thu Jul  4 23:17:29 2024 ] 	Batch(4700/7879) done. Loss: 0.0672  lr:0.000001
[ Thu Jul  4 23:17:47 2024 ] 	Batch(4800/7879) done. Loss: 0.0107  lr:0.000001
[ Thu Jul  4 23:18:05 2024 ] 	Batch(4900/7879) done. Loss: 0.2800  lr:0.000001
[ Thu Jul  4 23:18:23 2024 ] 
Training: Epoch [3/120], Step [4999], Loss: 0.22610104084014893, Training Accuracy: 97.68
[ Thu Jul  4 23:18:23 2024 ] 	Batch(5000/7879) done. Loss: 0.0038  lr:0.000001
[ Thu Jul  4 23:18:41 2024 ] 	Batch(5100/7879) done. Loss: 0.0141  lr:0.000001
[ Thu Jul  4 23:18:59 2024 ] 	Batch(5200/7879) done. Loss: 0.0297  lr:0.000001
[ Thu Jul  4 23:19:17 2024 ] 	Batch(5300/7879) done. Loss: 0.0588  lr:0.000001
[ Thu Jul  4 23:19:34 2024 ] 	Batch(5400/7879) done. Loss: 0.1076  lr:0.000001
[ Thu Jul  4 23:19:52 2024 ] 
Training: Epoch [3/120], Step [5499], Loss: 0.306213915348053, Training Accuracy: 97.66818181818182
[ Thu Jul  4 23:19:52 2024 ] 	Batch(5500/7879) done. Loss: 0.1986  lr:0.000001
[ Thu Jul  4 23:20:11 2024 ] 	Batch(5600/7879) done. Loss: 0.0029  lr:0.000001
[ Thu Jul  4 23:20:29 2024 ] 	Batch(5700/7879) done. Loss: 0.1624  lr:0.000001
[ Thu Jul  4 23:20:48 2024 ] 	Batch(5800/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul  4 23:21:06 2024 ] 	Batch(5900/7879) done. Loss: 0.0868  lr:0.000001
[ Thu Jul  4 23:21:23 2024 ] 
Training: Epoch [3/120], Step [5999], Loss: 0.12013432383537292, Training Accuracy: 97.67708333333334
[ Thu Jul  4 23:21:24 2024 ] 	Batch(6000/7879) done. Loss: 0.1802  lr:0.000001
[ Thu Jul  4 23:21:42 2024 ] 	Batch(6100/7879) done. Loss: 0.0421  lr:0.000001
[ Thu Jul  4 23:22:01 2024 ] 	Batch(6200/7879) done. Loss: 0.0394  lr:0.000001
[ Thu Jul  4 23:22:19 2024 ] 	Batch(6300/7879) done. Loss: 0.0108  lr:0.000001
[ Thu Jul  4 23:22:38 2024 ] 	Batch(6400/7879) done. Loss: 0.0242  lr:0.000001
[ Thu Jul  4 23:22:56 2024 ] 
Training: Epoch [3/120], Step [6499], Loss: 0.04121186211705208, Training Accuracy: 97.6923076923077
[ Thu Jul  4 23:22:56 2024 ] 	Batch(6500/7879) done. Loss: 0.2861  lr:0.000001
[ Thu Jul  4 23:23:14 2024 ] 	Batch(6600/7879) done. Loss: 0.0376  lr:0.000001
[ Thu Jul  4 23:23:32 2024 ] 	Batch(6700/7879) done. Loss: 0.0035  lr:0.000001
[ Thu Jul  4 23:23:50 2024 ] 	Batch(6800/7879) done. Loss: 0.4947  lr:0.000001
[ Thu Jul  4 23:24:08 2024 ] 	Batch(6900/7879) done. Loss: 0.0409  lr:0.000001
[ Thu Jul  4 23:24:25 2024 ] 
Training: Epoch [3/120], Step [6999], Loss: 0.24473784863948822, Training Accuracy: 97.70357142857144
[ Thu Jul  4 23:24:26 2024 ] 	Batch(7000/7879) done. Loss: 0.0592  lr:0.000001
[ Thu Jul  4 23:24:43 2024 ] 	Batch(7100/7879) done. Loss: 0.0784  lr:0.000001
[ Thu Jul  4 23:25:02 2024 ] 	Batch(7200/7879) done. Loss: 0.0468  lr:0.000001
[ Thu Jul  4 23:25:20 2024 ] 	Batch(7300/7879) done. Loss: 0.0197  lr:0.000001
[ Thu Jul  4 23:25:38 2024 ] 	Batch(7400/7879) done. Loss: 0.0784  lr:0.000001
[ Thu Jul  4 23:25:55 2024 ] 
Training: Epoch [3/120], Step [7499], Loss: 0.007968408055603504, Training Accuracy: 97.69
[ Thu Jul  4 23:25:55 2024 ] 	Batch(7500/7879) done. Loss: 0.0405  lr:0.000001
[ Thu Jul  4 23:26:13 2024 ] 	Batch(7600/7879) done. Loss: 0.4118  lr:0.000001
[ Thu Jul  4 23:26:32 2024 ] 	Batch(7700/7879) done. Loss: 0.2060  lr:0.000001
[ Thu Jul  4 23:26:51 2024 ] 	Batch(7800/7879) done. Loss: 0.0031  lr:0.000001
[ Thu Jul  4 23:27:05 2024 ] 	Mean training loss: 0.0922.
[ Thu Jul  4 23:27:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul  4 23:27:05 2024 ] Training epoch: 5
[ Thu Jul  4 23:27:06 2024 ] 	Batch(0/7879) done. Loss: 0.0147  lr:0.000001
[ Thu Jul  4 23:27:24 2024 ] 	Batch(100/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul  4 23:27:42 2024 ] 	Batch(200/7879) done. Loss: 0.1159  lr:0.000001
[ Thu Jul  4 23:28:01 2024 ] 	Batch(300/7879) done. Loss: 0.0732  lr:0.000001
[ Thu Jul  4 23:28:19 2024 ] 	Batch(400/7879) done. Loss: 0.0112  lr:0.000001
[ Thu Jul  4 23:28:37 2024 ] 
Training: Epoch [4/120], Step [499], Loss: 0.5534201264381409, Training Accuracy: 97.75
[ Thu Jul  4 23:28:37 2024 ] 	Batch(500/7879) done. Loss: 0.0494  lr:0.000001
[ Thu Jul  4 23:28:56 2024 ] 	Batch(600/7879) done. Loss: 0.0735  lr:0.000001
[ Thu Jul  4 23:29:14 2024 ] 	Batch(700/7879) done. Loss: 0.0491  lr:0.000001
[ Thu Jul  4 23:29:32 2024 ] 	Batch(800/7879) done. Loss: 0.0036  lr:0.000001
[ Thu Jul  4 23:29:50 2024 ] 	Batch(900/7879) done. Loss: 0.0965  lr:0.000001
[ Thu Jul  4 23:30:08 2024 ] 
Training: Epoch [4/120], Step [999], Loss: 0.022442303597927094, Training Accuracy: 97.8625
[ Thu Jul  4 23:30:08 2024 ] 	Batch(1000/7879) done. Loss: 0.0347  lr:0.000001
[ Thu Jul  4 23:30:26 2024 ] 	Batch(1100/7879) done. Loss: 0.0548  lr:0.000001
[ Thu Jul  4 23:30:45 2024 ] 	Batch(1200/7879) done. Loss: 0.0752  lr:0.000001
[ Thu Jul  4 23:31:03 2024 ] 	Batch(1300/7879) done. Loss: 0.2278  lr:0.000001
[ Thu Jul  4 23:31:21 2024 ] 	Batch(1400/7879) done. Loss: 0.1840  lr:0.000001
[ Thu Jul  4 23:31:38 2024 ] 
Training: Epoch [4/120], Step [1499], Loss: 0.02770080603659153, Training Accuracy: 97.75
[ Thu Jul  4 23:31:39 2024 ] 	Batch(1500/7879) done. Loss: 0.0078  lr:0.000001
[ Thu Jul  4 23:31:57 2024 ] 	Batch(1600/7879) done. Loss: 0.0173  lr:0.000001
[ Thu Jul  4 23:32:15 2024 ] 	Batch(1700/7879) done. Loss: 0.2278  lr:0.000001
[ Thu Jul  4 23:32:34 2024 ] 	Batch(1800/7879) done. Loss: 0.3157  lr:0.000001
[ Thu Jul  4 23:32:52 2024 ] 	Batch(1900/7879) done. Loss: 0.0645  lr:0.000001
[ Thu Jul  4 23:33:10 2024 ] 
Training: Epoch [4/120], Step [1999], Loss: 0.22198227047920227, Training Accuracy: 97.79375
[ Thu Jul  4 23:33:10 2024 ] 	Batch(2000/7879) done. Loss: 0.1751  lr:0.000001
[ Thu Jul  4 23:33:29 2024 ] 	Batch(2100/7879) done. Loss: 0.0135  lr:0.000001
[ Thu Jul  4 23:33:47 2024 ] 	Batch(2200/7879) done. Loss: 0.1397  lr:0.000001
[ Thu Jul  4 23:34:05 2024 ] 	Batch(2300/7879) done. Loss: 0.0577  lr:0.000001
[ Thu Jul  4 23:34:23 2024 ] 	Batch(2400/7879) done. Loss: 0.1678  lr:0.000001
[ Thu Jul  4 23:34:41 2024 ] 
Training: Epoch [4/120], Step [2499], Loss: 0.00807097740471363, Training Accuracy: 97.695
[ Thu Jul  4 23:34:41 2024 ] 	Batch(2500/7879) done. Loss: 0.0554  lr:0.000001
[ Thu Jul  4 23:34:59 2024 ] 	Batch(2600/7879) done. Loss: 0.1042  lr:0.000001
[ Thu Jul  4 23:35:17 2024 ] 	Batch(2700/7879) done. Loss: 0.0457  lr:0.000001
[ Thu Jul  4 23:35:35 2024 ] 	Batch(2800/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul  4 23:35:53 2024 ] 	Batch(2900/7879) done. Loss: 0.0413  lr:0.000001
[ Thu Jul  4 23:36:11 2024 ] 
Training: Epoch [4/120], Step [2999], Loss: 0.013724848628044128, Training Accuracy: 97.70416666666667
[ Thu Jul  4 23:36:11 2024 ] 	Batch(3000/7879) done. Loss: 0.1267  lr:0.000001
[ Thu Jul  4 23:36:29 2024 ] 	Batch(3100/7879) done. Loss: 0.1229  lr:0.000001
[ Thu Jul  4 23:36:47 2024 ] 	Batch(3200/7879) done. Loss: 0.0021  lr:0.000001
[ Thu Jul  4 23:37:05 2024 ] 	Batch(3300/7879) done. Loss: 0.0707  lr:0.000001
[ Thu Jul  4 23:37:23 2024 ] 	Batch(3400/7879) done. Loss: 0.1993  lr:0.000001
[ Thu Jul  4 23:37:41 2024 ] 
Training: Epoch [4/120], Step [3499], Loss: 0.0070797353982925415, Training Accuracy: 97.69642857142857
[ Thu Jul  4 23:37:41 2024 ] 	Batch(3500/7879) done. Loss: 0.1871  lr:0.000001
[ Thu Jul  4 23:37:59 2024 ] 	Batch(3600/7879) done. Loss: 0.1622  lr:0.000001
[ Thu Jul  4 23:38:17 2024 ] 	Batch(3700/7879) done. Loss: 0.0292  lr:0.000001
[ Thu Jul  4 23:38:35 2024 ] 	Batch(3800/7879) done. Loss: 0.0617  lr:0.000001
[ Thu Jul  4 23:38:53 2024 ] 	Batch(3900/7879) done. Loss: 0.0227  lr:0.000001
[ Thu Jul  4 23:39:11 2024 ] 
Training: Epoch [4/120], Step [3999], Loss: 0.34596872329711914, Training Accuracy: 97.746875
[ Thu Jul  4 23:39:11 2024 ] 	Batch(4000/7879) done. Loss: 0.0355  lr:0.000001
[ Thu Jul  4 23:39:29 2024 ] 	Batch(4100/7879) done. Loss: 0.0271  lr:0.000001
[ Thu Jul  4 23:39:47 2024 ] 	Batch(4200/7879) done. Loss: 0.0547  lr:0.000001
[ Thu Jul  4 23:40:05 2024 ] 	Batch(4300/7879) done. Loss: 0.0121  lr:0.000001
[ Thu Jul  4 23:40:23 2024 ] 	Batch(4400/7879) done. Loss: 0.0138  lr:0.000001
[ Thu Jul  4 23:40:41 2024 ] 
Training: Epoch [4/120], Step [4499], Loss: 0.06857418268918991, Training Accuracy: 97.775
[ Thu Jul  4 23:40:41 2024 ] 	Batch(4500/7879) done. Loss: 0.0268  lr:0.000001
[ Thu Jul  4 23:40:59 2024 ] 	Batch(4600/7879) done. Loss: 0.0086  lr:0.000001
[ Thu Jul  4 23:41:17 2024 ] 	Batch(4700/7879) done. Loss: 0.1770  lr:0.000001
[ Thu Jul  4 23:41:36 2024 ] 	Batch(4800/7879) done. Loss: 0.4309  lr:0.000001
[ Thu Jul  4 23:41:54 2024 ] 	Batch(4900/7879) done. Loss: 0.0178  lr:0.000001
[ Thu Jul  4 23:42:12 2024 ] 
Training: Epoch [4/120], Step [4999], Loss: 0.027512378990650177, Training Accuracy: 97.765
[ Thu Jul  4 23:42:12 2024 ] 	Batch(5000/7879) done. Loss: 0.0816  lr:0.000001
[ Thu Jul  4 23:42:30 2024 ] 	Batch(5100/7879) done. Loss: 0.2537  lr:0.000001
[ Thu Jul  4 23:42:48 2024 ] 	Batch(5200/7879) done. Loss: 0.0059  lr:0.000001
[ Thu Jul  4 23:43:06 2024 ] 	Batch(5300/7879) done. Loss: 0.0056  lr:0.000001
[ Thu Jul  4 23:43:24 2024 ] 	Batch(5400/7879) done. Loss: 0.2386  lr:0.000001
[ Thu Jul  4 23:43:42 2024 ] 
Training: Epoch [4/120], Step [5499], Loss: 0.008532763458788395, Training Accuracy: 97.7590909090909
[ Thu Jul  4 23:43:42 2024 ] 	Batch(5500/7879) done. Loss: 0.0058  lr:0.000001
[ Thu Jul  4 23:44:00 2024 ] 	Batch(5600/7879) done. Loss: 0.0347  lr:0.000001
[ Thu Jul  4 23:44:18 2024 ] 	Batch(5700/7879) done. Loss: 0.0747  lr:0.000001
[ Thu Jul  4 23:44:36 2024 ] 	Batch(5800/7879) done. Loss: 1.0087  lr:0.000001
[ Thu Jul  4 23:44:54 2024 ] 	Batch(5900/7879) done. Loss: 0.1419  lr:0.000001
[ Thu Jul  4 23:45:12 2024 ] 
Training: Epoch [4/120], Step [5999], Loss: 0.10950083285570145, Training Accuracy: 97.75833333333334
[ Thu Jul  4 23:45:12 2024 ] 	Batch(6000/7879) done. Loss: 0.3512  lr:0.000001
[ Thu Jul  4 23:45:30 2024 ] 	Batch(6100/7879) done. Loss: 0.1121  lr:0.000001
[ Thu Jul  4 23:45:48 2024 ] 	Batch(6200/7879) done. Loss: 0.0719  lr:0.000001
[ Thu Jul  4 23:46:06 2024 ] 	Batch(6300/7879) done. Loss: 0.0479  lr:0.000001
[ Thu Jul  4 23:46:25 2024 ] 	Batch(6400/7879) done. Loss: 0.0374  lr:0.000001
[ Thu Jul  4 23:46:42 2024 ] 
Training: Epoch [4/120], Step [6499], Loss: 0.14820143580436707, Training Accuracy: 97.7673076923077
[ Thu Jul  4 23:46:43 2024 ] 	Batch(6500/7879) done. Loss: 0.1596  lr:0.000001
[ Thu Jul  4 23:47:01 2024 ] 	Batch(6600/7879) done. Loss: 0.0321  lr:0.000001
[ Thu Jul  4 23:47:18 2024 ] 	Batch(6700/7879) done. Loss: 0.0093  lr:0.000001
[ Thu Jul  4 23:47:37 2024 ] 	Batch(6800/7879) done. Loss: 0.5112  lr:0.000001
[ Thu Jul  4 23:47:55 2024 ] 	Batch(6900/7879) done. Loss: 0.0540  lr:0.000001
[ Thu Jul  4 23:48:13 2024 ] 
Training: Epoch [4/120], Step [6999], Loss: 0.05302286520600319, Training Accuracy: 97.76785714285714
[ Thu Jul  4 23:48:13 2024 ] 	Batch(7000/7879) done. Loss: 0.0070  lr:0.000001
[ Thu Jul  4 23:48:31 2024 ] 	Batch(7100/7879) done. Loss: 0.0124  lr:0.000001
[ Thu Jul  4 23:48:49 2024 ] 	Batch(7200/7879) done. Loss: 0.0090  lr:0.000001
[ Thu Jul  4 23:49:07 2024 ] 	Batch(7300/7879) done. Loss: 0.2693  lr:0.000001
[ Thu Jul  4 23:49:25 2024 ] 	Batch(7400/7879) done. Loss: 0.0783  lr:0.000001
[ Thu Jul  4 23:49:43 2024 ] 
Training: Epoch [4/120], Step [7499], Loss: 0.3178238868713379, Training Accuracy: 97.75666666666667
[ Thu Jul  4 23:49:43 2024 ] 	Batch(7500/7879) done. Loss: 0.0165  lr:0.000001
[ Thu Jul  4 23:50:02 2024 ] 	Batch(7600/7879) done. Loss: 0.5098  lr:0.000001
[ Thu Jul  4 23:50:20 2024 ] 	Batch(7700/7879) done. Loss: 0.0227  lr:0.000001
[ Thu Jul  4 23:50:38 2024 ] 	Batch(7800/7879) done. Loss: 0.0054  lr:0.000001
[ Thu Jul  4 23:50:53 2024 ] 	Mean training loss: 0.0906.
[ Thu Jul  4 23:50:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul  4 23:50:53 2024 ] Training epoch: 6
[ Thu Jul  4 23:50:54 2024 ] 	Batch(0/7879) done. Loss: 0.0850  lr:0.000001
[ Thu Jul  4 23:51:12 2024 ] 	Batch(100/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul  4 23:51:30 2024 ] 	Batch(200/7879) done. Loss: 0.0851  lr:0.000001
[ Thu Jul  4 23:51:48 2024 ] 	Batch(300/7879) done. Loss: 0.0325  lr:0.000001
[ Thu Jul  4 23:52:06 2024 ] 	Batch(400/7879) done. Loss: 0.1931  lr:0.000001
[ Thu Jul  4 23:52:24 2024 ] 
Training: Epoch [5/120], Step [499], Loss: 0.4045940935611725, Training Accuracy: 98.175
[ Thu Jul  4 23:52:24 2024 ] 	Batch(500/7879) done. Loss: 0.0487  lr:0.000001
[ Thu Jul  4 23:52:43 2024 ] 	Batch(600/7879) done. Loss: 0.0979  lr:0.000001
[ Thu Jul  4 23:53:01 2024 ] 	Batch(700/7879) done. Loss: 0.2356  lr:0.000001
[ Thu Jul  4 23:53:19 2024 ] 	Batch(800/7879) done. Loss: 0.0078  lr:0.000001
[ Thu Jul  4 23:53:38 2024 ] 	Batch(900/7879) done. Loss: 0.0044  lr:0.000001
[ Thu Jul  4 23:53:55 2024 ] 
Training: Epoch [5/120], Step [999], Loss: 0.2151370346546173, Training Accuracy: 97.875
[ Thu Jul  4 23:53:56 2024 ] 	Batch(1000/7879) done. Loss: 0.0104  lr:0.000001
[ Thu Jul  4 23:54:13 2024 ] 	Batch(1100/7879) done. Loss: 0.1743  lr:0.000001
[ Thu Jul  4 23:54:32 2024 ] 	Batch(1200/7879) done. Loss: 0.0802  lr:0.000001
[ Thu Jul  4 23:54:50 2024 ] 	Batch(1300/7879) done. Loss: 0.0224  lr:0.000001
[ Thu Jul  4 23:55:08 2024 ] 	Batch(1400/7879) done. Loss: 0.1370  lr:0.000001
[ Thu Jul  4 23:55:26 2024 ] 
Training: Epoch [5/120], Step [1499], Loss: 0.045092836022377014, Training Accuracy: 97.82499999999999
[ Thu Jul  4 23:55:26 2024 ] 	Batch(1500/7879) done. Loss: 0.0179  lr:0.000001
[ Thu Jul  4 23:55:44 2024 ] 	Batch(1600/7879) done. Loss: 0.0051  lr:0.000001
[ Thu Jul  4 23:56:02 2024 ] 	Batch(1700/7879) done. Loss: 0.1555  lr:0.000001
[ Thu Jul  4 23:56:20 2024 ] 	Batch(1800/7879) done. Loss: 0.0023  lr:0.000001
[ Thu Jul  4 23:56:38 2024 ] 	Batch(1900/7879) done. Loss: 0.0083  lr:0.000001
[ Thu Jul  4 23:56:56 2024 ] 
Training: Epoch [5/120], Step [1999], Loss: 0.01005280576646328, Training Accuracy: 97.75625
[ Thu Jul  4 23:56:57 2024 ] 	Batch(2000/7879) done. Loss: 0.1747  lr:0.000001
[ Thu Jul  4 23:57:15 2024 ] 	Batch(2100/7879) done. Loss: 0.0091  lr:0.000001
[ Thu Jul  4 23:57:33 2024 ] 	Batch(2200/7879) done. Loss: 0.1897  lr:0.000001
[ Thu Jul  4 23:57:51 2024 ] 	Batch(2300/7879) done. Loss: 0.1185  lr:0.000001
[ Thu Jul  4 23:58:09 2024 ] 	Batch(2400/7879) done. Loss: 0.0130  lr:0.000001
[ Thu Jul  4 23:58:27 2024 ] 
Training: Epoch [5/120], Step [2499], Loss: 0.09659746289253235, Training Accuracy: 97.8
[ Thu Jul  4 23:58:27 2024 ] 	Batch(2500/7879) done. Loss: 0.1427  lr:0.000001
[ Thu Jul  4 23:58:46 2024 ] 	Batch(2600/7879) done. Loss: 0.3285  lr:0.000001
[ Thu Jul  4 23:59:04 2024 ] 	Batch(2700/7879) done. Loss: 0.3266  lr:0.000001
[ Thu Jul  4 23:59:22 2024 ] 	Batch(2800/7879) done. Loss: 0.1515  lr:0.000001
[ Thu Jul  4 23:59:41 2024 ] 	Batch(2900/7879) done. Loss: 0.0421  lr:0.000001
[ Thu Jul  4 23:59:59 2024 ] 
Training: Epoch [5/120], Step [2999], Loss: 0.038238849490880966, Training Accuracy: 97.7375
[ Thu Jul  4 23:59:59 2024 ] 	Batch(3000/7879) done. Loss: 0.0222  lr:0.000001
[ Fri Jul  5 00:00:18 2024 ] 	Batch(3100/7879) done. Loss: 0.1788  lr:0.000001
[ Fri Jul  5 00:00:36 2024 ] 	Batch(3200/7879) done. Loss: 0.0866  lr:0.000001
[ Fri Jul  5 00:00:54 2024 ] 	Batch(3300/7879) done. Loss: 0.0707  lr:0.000001
[ Fri Jul  5 00:01:12 2024 ] 	Batch(3400/7879) done. Loss: 0.0323  lr:0.000001
[ Fri Jul  5 00:01:30 2024 ] 
Training: Epoch [5/120], Step [3499], Loss: 0.2157067060470581, Training Accuracy: 97.73928571428571
[ Fri Jul  5 00:01:30 2024 ] 	Batch(3500/7879) done. Loss: 0.0047  lr:0.000001
[ Fri Jul  5 00:01:48 2024 ] 	Batch(3600/7879) done. Loss: 0.0019  lr:0.000001
[ Fri Jul  5 00:02:07 2024 ] 	Batch(3700/7879) done. Loss: 0.2268  lr:0.000001
[ Fri Jul  5 00:02:26 2024 ] 	Batch(3800/7879) done. Loss: 0.0785  lr:0.000001
[ Fri Jul  5 00:02:45 2024 ] 	Batch(3900/7879) done. Loss: 0.0360  lr:0.000001
[ Fri Jul  5 00:03:04 2024 ] 
Training: Epoch [5/120], Step [3999], Loss: 0.03253409266471863, Training Accuracy: 97.7625
[ Fri Jul  5 00:03:04 2024 ] 	Batch(4000/7879) done. Loss: 0.0113  lr:0.000001
[ Fri Jul  5 00:03:22 2024 ] 	Batch(4100/7879) done. Loss: 0.0142  lr:0.000001
[ Fri Jul  5 00:03:40 2024 ] 	Batch(4200/7879) done. Loss: 0.1664  lr:0.000001
[ Fri Jul  5 00:03:58 2024 ] 	Batch(4300/7879) done. Loss: 0.0054  lr:0.000001
[ Fri Jul  5 00:04:16 2024 ] 	Batch(4400/7879) done. Loss: 0.0017  lr:0.000001
[ Fri Jul  5 00:04:34 2024 ] 
Training: Epoch [5/120], Step [4499], Loss: 0.13695348799228668, Training Accuracy: 97.72777777777777
[ Fri Jul  5 00:04:34 2024 ] 	Batch(4500/7879) done. Loss: 0.2053  lr:0.000001
[ Fri Jul  5 00:04:52 2024 ] 	Batch(4600/7879) done. Loss: 0.0272  lr:0.000001
[ Fri Jul  5 00:05:10 2024 ] 	Batch(4700/7879) done. Loss: 0.0752  lr:0.000001
[ Fri Jul  5 00:05:28 2024 ] 	Batch(4800/7879) done. Loss: 0.0221  lr:0.000001
[ Fri Jul  5 00:05:45 2024 ] 	Batch(4900/7879) done. Loss: 0.0029  lr:0.000001
[ Fri Jul  5 00:06:03 2024 ] 
Training: Epoch [5/120], Step [4999], Loss: 0.10752042382955551, Training Accuracy: 97.67
[ Fri Jul  5 00:06:03 2024 ] 	Batch(5000/7879) done. Loss: 0.0722  lr:0.000001
[ Fri Jul  5 00:06:21 2024 ] 	Batch(5100/7879) done. Loss: 0.0553  lr:0.000001
[ Fri Jul  5 00:06:39 2024 ] 	Batch(5200/7879) done. Loss: 0.0134  lr:0.000001
[ Fri Jul  5 00:06:57 2024 ] 	Batch(5300/7879) done. Loss: 0.0129  lr:0.000001
[ Fri Jul  5 00:07:15 2024 ] 	Batch(5400/7879) done. Loss: 0.0683  lr:0.000001
[ Fri Jul  5 00:07:33 2024 ] 
Training: Epoch [5/120], Step [5499], Loss: 0.10278584808111191, Training Accuracy: 97.675
[ Fri Jul  5 00:07:33 2024 ] 	Batch(5500/7879) done. Loss: 0.0301  lr:0.000001
[ Fri Jul  5 00:07:51 2024 ] 	Batch(5600/7879) done. Loss: 0.1286  lr:0.000001
[ Fri Jul  5 00:08:09 2024 ] 	Batch(5700/7879) done. Loss: 0.0698  lr:0.000001
[ Fri Jul  5 00:08:27 2024 ] 	Batch(5800/7879) done. Loss: 0.2545  lr:0.000001
[ Fri Jul  5 00:08:45 2024 ] 	Batch(5900/7879) done. Loss: 0.0380  lr:0.000001
[ Fri Jul  5 00:09:03 2024 ] 
Training: Epoch [5/120], Step [5999], Loss: 0.2600347399711609, Training Accuracy: 97.66875
[ Fri Jul  5 00:09:03 2024 ] 	Batch(6000/7879) done. Loss: 0.0112  lr:0.000001
[ Fri Jul  5 00:09:21 2024 ] 	Batch(6100/7879) done. Loss: 0.0409  lr:0.000001
[ Fri Jul  5 00:09:40 2024 ] 	Batch(6200/7879) done. Loss: 0.1302  lr:0.000001
[ Fri Jul  5 00:09:59 2024 ] 	Batch(6300/7879) done. Loss: 0.0731  lr:0.000001
[ Fri Jul  5 00:10:17 2024 ] 	Batch(6400/7879) done. Loss: 0.1487  lr:0.000001
[ Fri Jul  5 00:10:35 2024 ] 
Training: Epoch [5/120], Step [6499], Loss: 0.012592230923473835, Training Accuracy: 97.68846153846154
[ Fri Jul  5 00:10:35 2024 ] 	Batch(6500/7879) done. Loss: 0.0465  lr:0.000001
[ Fri Jul  5 00:10:53 2024 ] 	Batch(6600/7879) done. Loss: 0.0220  lr:0.000001
[ Fri Jul  5 00:11:11 2024 ] 	Batch(6700/7879) done. Loss: 0.1353  lr:0.000001
[ Fri Jul  5 00:11:29 2024 ] 	Batch(6800/7879) done. Loss: 0.0114  lr:0.000001
[ Fri Jul  5 00:11:47 2024 ] 	Batch(6900/7879) done. Loss: 0.0322  lr:0.000001
[ Fri Jul  5 00:12:05 2024 ] 
Training: Epoch [5/120], Step [6999], Loss: 0.24564874172210693, Training Accuracy: 97.69464285714285
[ Fri Jul  5 00:12:05 2024 ] 	Batch(7000/7879) done. Loss: 0.0083  lr:0.000001
[ Fri Jul  5 00:12:23 2024 ] 	Batch(7100/7879) done. Loss: 0.1457  lr:0.000001
[ Fri Jul  5 00:12:41 2024 ] 	Batch(7200/7879) done. Loss: 0.0145  lr:0.000001
[ Fri Jul  5 00:12:59 2024 ] 	Batch(7300/7879) done. Loss: 0.0062  lr:0.000001
[ Fri Jul  5 00:13:17 2024 ] 	Batch(7400/7879) done. Loss: 0.0466  lr:0.000001
[ Fri Jul  5 00:13:35 2024 ] 
Training: Epoch [5/120], Step [7499], Loss: 0.19342397153377533, Training Accuracy: 97.69833333333334
[ Fri Jul  5 00:13:35 2024 ] 	Batch(7500/7879) done. Loss: 0.0884  lr:0.000001
[ Fri Jul  5 00:13:53 2024 ] 	Batch(7600/7879) done. Loss: 0.0167  lr:0.000001
[ Fri Jul  5 00:14:11 2024 ] 	Batch(7700/7879) done. Loss: 0.1135  lr:0.000001
[ Fri Jul  5 00:14:29 2024 ] 	Batch(7800/7879) done. Loss: 0.0916  lr:0.000001
[ Fri Jul  5 00:14:43 2024 ] 	Mean training loss: 0.0922.
[ Fri Jul  5 00:14:43 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul  5 00:14:43 2024 ] Training epoch: 7
[ Fri Jul  5 00:14:44 2024 ] 	Batch(0/7879) done. Loss: 0.2065  lr:0.000001
[ Fri Jul  5 00:15:01 2024 ] 	Batch(100/7879) done. Loss: 0.0780  lr:0.000001
[ Fri Jul  5 00:15:19 2024 ] 	Batch(200/7879) done. Loss: 0.0746  lr:0.000001
[ Fri Jul  5 00:15:37 2024 ] 	Batch(300/7879) done. Loss: 0.0340  lr:0.000001
[ Fri Jul  5 00:15:55 2024 ] 	Batch(400/7879) done. Loss: 0.2383  lr:0.000001
[ Fri Jul  5 00:16:13 2024 ] 
Training: Epoch [6/120], Step [499], Loss: 0.05687067657709122, Training Accuracy: 97.52499999999999
[ Fri Jul  5 00:16:13 2024 ] 	Batch(500/7879) done. Loss: 0.2853  lr:0.000001
[ Fri Jul  5 00:16:32 2024 ] 	Batch(600/7879) done. Loss: 0.0902  lr:0.000001
[ Fri Jul  5 00:16:50 2024 ] 	Batch(700/7879) done. Loss: 0.1764  lr:0.000001
[ Fri Jul  5 00:17:08 2024 ] 	Batch(800/7879) done. Loss: 0.0462  lr:0.000001
[ Fri Jul  5 00:17:27 2024 ] 	Batch(900/7879) done. Loss: 0.0282  lr:0.000001
[ Fri Jul  5 00:17:45 2024 ] 
Training: Epoch [6/120], Step [999], Loss: 0.06183551251888275, Training Accuracy: 97.6125
[ Fri Jul  5 00:17:45 2024 ] 	Batch(1000/7879) done. Loss: 0.0461  lr:0.000001
[ Fri Jul  5 00:18:03 2024 ] 	Batch(1100/7879) done. Loss: 0.0596  lr:0.000001
[ Fri Jul  5 00:18:21 2024 ] 	Batch(1200/7879) done. Loss: 0.4987  lr:0.000001
[ Fri Jul  5 00:18:39 2024 ] 	Batch(1300/7879) done. Loss: 0.0141  lr:0.000001
[ Fri Jul  5 00:18:57 2024 ] 	Batch(1400/7879) done. Loss: 0.0891  lr:0.000001
[ Fri Jul  5 00:19:15 2024 ] 
Training: Epoch [6/120], Step [1499], Loss: 0.034005872905254364, Training Accuracy: 97.68333333333334
[ Fri Jul  5 00:19:15 2024 ] 	Batch(1500/7879) done. Loss: 0.0264  lr:0.000001
[ Fri Jul  5 00:19:33 2024 ] 	Batch(1600/7879) done. Loss: 0.0416  lr:0.000001
[ Fri Jul  5 00:19:52 2024 ] 	Batch(1700/7879) done. Loss: 0.3626  lr:0.000001
[ Fri Jul  5 00:20:10 2024 ] 	Batch(1800/7879) done. Loss: 0.4539  lr:0.000001
[ Fri Jul  5 00:20:28 2024 ] 	Batch(1900/7879) done. Loss: 0.1636  lr:0.000001
[ Fri Jul  5 00:20:46 2024 ] 
Training: Epoch [6/120], Step [1999], Loss: 0.011503538116812706, Training Accuracy: 97.5625
[ Fri Jul  5 00:20:46 2024 ] 	Batch(2000/7879) done. Loss: 0.0097  lr:0.000001
[ Fri Jul  5 00:21:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0258  lr:0.000001
[ Fri Jul  5 00:21:22 2024 ] 	Batch(2200/7879) done. Loss: 0.0037  lr:0.000001
[ Fri Jul  5 00:21:40 2024 ] 	Batch(2300/7879) done. Loss: 0.2137  lr:0.000001
[ Fri Jul  5 00:21:58 2024 ] 	Batch(2400/7879) done. Loss: 0.0191  lr:0.000001
[ Fri Jul  5 00:22:16 2024 ] 
Training: Epoch [6/120], Step [2499], Loss: 0.5315025448799133, Training Accuracy: 97.455
[ Fri Jul  5 00:22:16 2024 ] 	Batch(2500/7879) done. Loss: 0.5792  lr:0.000001
[ Fri Jul  5 00:22:34 2024 ] 	Batch(2600/7879) done. Loss: 0.1205  lr:0.000001
[ Fri Jul  5 00:22:52 2024 ] 	Batch(2700/7879) done. Loss: 0.0627  lr:0.000001
[ Fri Jul  5 00:23:11 2024 ] 	Batch(2800/7879) done. Loss: 0.0227  lr:0.000001
[ Fri Jul  5 00:23:29 2024 ] 	Batch(2900/7879) done. Loss: 0.1269  lr:0.000001
[ Fri Jul  5 00:23:46 2024 ] 
Training: Epoch [6/120], Step [2999], Loss: 0.008452976122498512, Training Accuracy: 97.5
[ Fri Jul  5 00:23:47 2024 ] 	Batch(3000/7879) done. Loss: 0.1934  lr:0.000001
[ Fri Jul  5 00:24:05 2024 ] 	Batch(3100/7879) done. Loss: 0.0419  lr:0.000001
[ Fri Jul  5 00:24:23 2024 ] 	Batch(3200/7879) done. Loss: 0.0088  lr:0.000001
[ Fri Jul  5 00:24:41 2024 ] 	Batch(3300/7879) done. Loss: 0.0149  lr:0.000001
[ Fri Jul  5 00:24:59 2024 ] 	Batch(3400/7879) done. Loss: 0.0066  lr:0.000001
[ Fri Jul  5 00:25:17 2024 ] 
Training: Epoch [6/120], Step [3499], Loss: 0.03140871226787567, Training Accuracy: 97.51785714285714
[ Fri Jul  5 00:25:17 2024 ] 	Batch(3500/7879) done. Loss: 0.0606  lr:0.000001
[ Fri Jul  5 00:25:35 2024 ] 	Batch(3600/7879) done. Loss: 0.0232  lr:0.000001
[ Fri Jul  5 00:25:53 2024 ] 	Batch(3700/7879) done. Loss: 0.3587  lr:0.000001
[ Fri Jul  5 00:26:11 2024 ] 	Batch(3800/7879) done. Loss: 0.0133  lr:0.000001
[ Fri Jul  5 00:26:29 2024 ] 	Batch(3900/7879) done. Loss: 0.0076  lr:0.000001
[ Fri Jul  5 00:26:47 2024 ] 
Training: Epoch [6/120], Step [3999], Loss: 0.006535331252962351, Training Accuracy: 97.534375
[ Fri Jul  5 00:26:47 2024 ] 	Batch(4000/7879) done. Loss: 0.1439  lr:0.000001
[ Fri Jul  5 00:27:05 2024 ] 	Batch(4100/7879) done. Loss: 0.1778  lr:0.000001
[ Fri Jul  5 00:27:23 2024 ] 	Batch(4200/7879) done. Loss: 0.0040  lr:0.000001
[ Fri Jul  5 00:27:41 2024 ] 	Batch(4300/7879) done. Loss: 0.0166  lr:0.000001
[ Fri Jul  5 00:27:59 2024 ] 	Batch(4400/7879) done. Loss: 0.0097  lr:0.000001
[ Fri Jul  5 00:28:17 2024 ] 
Training: Epoch [6/120], Step [4499], Loss: 0.14465369284152985, Training Accuracy: 97.55277777777778
[ Fri Jul  5 00:28:17 2024 ] 	Batch(4500/7879) done. Loss: 0.0296  lr:0.000001
[ Fri Jul  5 00:28:36 2024 ] 	Batch(4600/7879) done. Loss: 0.0353  lr:0.000001
[ Fri Jul  5 00:28:54 2024 ] 	Batch(4700/7879) done. Loss: 0.0233  lr:0.000001
[ Fri Jul  5 00:29:12 2024 ] 	Batch(4800/7879) done. Loss: 0.0226  lr:0.000001
[ Fri Jul  5 00:29:30 2024 ] 	Batch(4900/7879) done. Loss: 0.0784  lr:0.000001
[ Fri Jul  5 00:29:48 2024 ] 
Training: Epoch [6/120], Step [4999], Loss: 0.27739253640174866, Training Accuracy: 97.57249999999999
[ Fri Jul  5 00:29:48 2024 ] 	Batch(5000/7879) done. Loss: 0.0101  lr:0.000001
[ Fri Jul  5 00:30:06 2024 ] 	Batch(5100/7879) done. Loss: 0.2539  lr:0.000001
[ Fri Jul  5 00:30:24 2024 ] 	Batch(5200/7879) done. Loss: 0.0188  lr:0.000001
[ Fri Jul  5 00:30:42 2024 ] 	Batch(5300/7879) done. Loss: 0.0427  lr:0.000001
[ Fri Jul  5 00:31:00 2024 ] 	Batch(5400/7879) done. Loss: 0.0119  lr:0.000001
[ Fri Jul  5 00:31:18 2024 ] 
Training: Epoch [6/120], Step [5499], Loss: 0.13984987139701843, Training Accuracy: 97.52727272727273
[ Fri Jul  5 00:31:18 2024 ] 	Batch(5500/7879) done. Loss: 0.1889  lr:0.000001
[ Fri Jul  5 00:31:36 2024 ] 	Batch(5600/7879) done. Loss: 0.0549  lr:0.000001
[ Fri Jul  5 00:31:54 2024 ] 	Batch(5700/7879) done. Loss: 0.3549  lr:0.000001
[ Fri Jul  5 00:32:13 2024 ] 	Batch(5800/7879) done. Loss: 0.0299  lr:0.000001
[ Fri Jul  5 00:32:31 2024 ] 	Batch(5900/7879) done. Loss: 0.1430  lr:0.000001
[ Fri Jul  5 00:32:49 2024 ] 
Training: Epoch [6/120], Step [5999], Loss: 0.09924787282943726, Training Accuracy: 97.55625
[ Fri Jul  5 00:32:49 2024 ] 	Batch(6000/7879) done. Loss: 0.0125  lr:0.000001
[ Fri Jul  5 00:33:08 2024 ] 	Batch(6100/7879) done. Loss: 0.2277  lr:0.000001
[ Fri Jul  5 00:33:26 2024 ] 	Batch(6200/7879) done. Loss: 0.0287  lr:0.000001
[ Fri Jul  5 00:33:44 2024 ] 	Batch(6300/7879) done. Loss: 0.0367  lr:0.000001
[ Fri Jul  5 00:34:03 2024 ] 	Batch(6400/7879) done. Loss: 0.0058  lr:0.000001
[ Fri Jul  5 00:34:21 2024 ] 
Training: Epoch [6/120], Step [6499], Loss: 0.06796709448099136, Training Accuracy: 97.58076923076922
[ Fri Jul  5 00:34:21 2024 ] 	Batch(6500/7879) done. Loss: 0.0810  lr:0.000001
[ Fri Jul  5 00:34:39 2024 ] 	Batch(6600/7879) done. Loss: 0.1051  lr:0.000001
[ Fri Jul  5 00:34:58 2024 ] 	Batch(6700/7879) done. Loss: 0.0079  lr:0.000001
[ Fri Jul  5 00:35:16 2024 ] 	Batch(6800/7879) done. Loss: 0.1240  lr:0.000001
[ Fri Jul  5 00:35:34 2024 ] 	Batch(6900/7879) done. Loss: 0.0766  lr:0.000001
[ Fri Jul  5 00:35:52 2024 ] 
Training: Epoch [6/120], Step [6999], Loss: 0.12339328974485397, Training Accuracy: 97.58928571428571
[ Fri Jul  5 00:35:52 2024 ] 	Batch(7000/7879) done. Loss: 0.0148  lr:0.000001
[ Fri Jul  5 00:36:10 2024 ] 	Batch(7100/7879) done. Loss: 0.0153  lr:0.000001
[ Fri Jul  5 00:36:28 2024 ] 	Batch(7200/7879) done. Loss: 0.0408  lr:0.000001
[ Fri Jul  5 00:36:46 2024 ] 	Batch(7300/7879) done. Loss: 0.0754  lr:0.000001
[ Fri Jul  5 00:37:04 2024 ] 	Batch(7400/7879) done. Loss: 0.0256  lr:0.000001
[ Fri Jul  5 00:37:22 2024 ] 
Training: Epoch [6/120], Step [7499], Loss: 0.02814626693725586, Training Accuracy: 97.6
[ Fri Jul  5 00:37:22 2024 ] 	Batch(7500/7879) done. Loss: 0.0159  lr:0.000001
[ Fri Jul  5 00:37:40 2024 ] 	Batch(7600/7879) done. Loss: 0.0177  lr:0.000001
[ Fri Jul  5 00:37:58 2024 ] 	Batch(7700/7879) done. Loss: 0.0251  lr:0.000001
[ Fri Jul  5 00:38:16 2024 ] 	Batch(7800/7879) done. Loss: 0.0075  lr:0.000001
[ Fri Jul  5 00:38:30 2024 ] 	Mean training loss: 0.0936.
[ Fri Jul  5 00:38:30 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul  5 00:38:30 2024 ] Training epoch: 8
[ Fri Jul  5 00:38:31 2024 ] 	Batch(0/7879) done. Loss: 0.0037  lr:0.000001
[ Fri Jul  5 00:38:49 2024 ] 	Batch(100/7879) done. Loss: 0.0312  lr:0.000001
[ Fri Jul  5 00:39:07 2024 ] 	Batch(200/7879) done. Loss: 0.1150  lr:0.000001
[ Fri Jul  5 00:39:25 2024 ] 	Batch(300/7879) done. Loss: 0.0334  lr:0.000001
[ Fri Jul  5 00:39:43 2024 ] 	Batch(400/7879) done. Loss: 0.3184  lr:0.000001
[ Fri Jul  5 00:40:01 2024 ] 
Training: Epoch [7/120], Step [499], Loss: 0.1798965036869049, Training Accuracy: 97.475
[ Fri Jul  5 00:40:01 2024 ] 	Batch(500/7879) done. Loss: 0.3044  lr:0.000001
[ Fri Jul  5 00:40:19 2024 ] 	Batch(600/7879) done. Loss: 0.0138  lr:0.000001
[ Fri Jul  5 00:40:38 2024 ] 	Batch(700/7879) done. Loss: 0.0123  lr:0.000001
[ Fri Jul  5 00:40:56 2024 ] 	Batch(800/7879) done. Loss: 0.0082  lr:0.000001
[ Fri Jul  5 00:41:15 2024 ] 	Batch(900/7879) done. Loss: 0.1390  lr:0.000001
[ Fri Jul  5 00:41:33 2024 ] 
Training: Epoch [7/120], Step [999], Loss: 0.16326171159744263, Training Accuracy: 97.75
[ Fri Jul  5 00:41:33 2024 ] 	Batch(1000/7879) done. Loss: 0.0017  lr:0.000001
[ Fri Jul  5 00:41:51 2024 ] 	Batch(1100/7879) done. Loss: 0.0041  lr:0.000001
[ Fri Jul  5 00:42:10 2024 ] 	Batch(1200/7879) done. Loss: 0.0653  lr:0.000001
[ Fri Jul  5 00:42:28 2024 ] 	Batch(1300/7879) done. Loss: 0.0010  lr:0.000001
[ Fri Jul  5 00:42:46 2024 ] 	Batch(1400/7879) done. Loss: 0.0310  lr:0.000001
[ Fri Jul  5 00:43:04 2024 ] 
Training: Epoch [7/120], Step [1499], Loss: 0.12968367338180542, Training Accuracy: 97.73333333333333
[ Fri Jul  5 00:43:04 2024 ] 	Batch(1500/7879) done. Loss: 0.1236  lr:0.000001
[ Fri Jul  5 00:43:23 2024 ] 	Batch(1600/7879) done. Loss: 0.0212  lr:0.000001
[ Fri Jul  5 00:43:41 2024 ] 	Batch(1700/7879) done. Loss: 0.1269  lr:0.000001
[ Fri Jul  5 00:43:59 2024 ] 	Batch(1800/7879) done. Loss: 0.1895  lr:0.000001
[ Fri Jul  5 00:44:17 2024 ] 	Batch(1900/7879) done. Loss: 0.0096  lr:0.000001
[ Fri Jul  5 00:44:35 2024 ] 
Training: Epoch [7/120], Step [1999], Loss: 0.015378464013338089, Training Accuracy: 97.6875
[ Fri Jul  5 00:44:35 2024 ] 	Batch(2000/7879) done. Loss: 0.0362  lr:0.000001
[ Fri Jul  5 00:44:53 2024 ] 	Batch(2100/7879) done. Loss: 0.0437  lr:0.000001
[ Fri Jul  5 00:45:11 2024 ] 	Batch(2200/7879) done. Loss: 0.0970  lr:0.000001
[ Fri Jul  5 00:45:29 2024 ] 	Batch(2300/7879) done. Loss: 0.0034  lr:0.000001
[ Fri Jul  5 00:45:48 2024 ] 	Batch(2400/7879) done. Loss: 0.0106  lr:0.000001
[ Fri Jul  5 00:46:06 2024 ] 
Training: Epoch [7/120], Step [2499], Loss: 0.052069250494241714, Training Accuracy: 97.715
[ Fri Jul  5 00:46:06 2024 ] 	Batch(2500/7879) done. Loss: 0.0204  lr:0.000001
[ Fri Jul  5 00:46:24 2024 ] 	Batch(2600/7879) done. Loss: 0.0030  lr:0.000001
[ Fri Jul  5 00:46:42 2024 ] 	Batch(2700/7879) done. Loss: 0.1849  lr:0.000001
[ Fri Jul  5 00:47:00 2024 ] 	Batch(2800/7879) done. Loss: 0.1293  lr:0.000001
[ Fri Jul  5 00:47:19 2024 ] 	Batch(2900/7879) done. Loss: 0.2138  lr:0.000001
[ Fri Jul  5 00:47:37 2024 ] 
Training: Epoch [7/120], Step [2999], Loss: 0.08422461897134781, Training Accuracy: 97.65833333333333
[ Fri Jul  5 00:47:37 2024 ] 	Batch(3000/7879) done. Loss: 0.3679  lr:0.000001
[ Fri Jul  5 00:47:55 2024 ] 	Batch(3100/7879) done. Loss: 0.0752  lr:0.000001
[ Fri Jul  5 00:48:13 2024 ] 	Batch(3200/7879) done. Loss: 0.0127  lr:0.000001
[ Fri Jul  5 00:48:32 2024 ] 	Batch(3300/7879) done. Loss: 0.0884  lr:0.000001
[ Fri Jul  5 00:48:50 2024 ] 	Batch(3400/7879) done. Loss: 0.0537  lr:0.000001
[ Fri Jul  5 00:49:08 2024 ] 
Training: Epoch [7/120], Step [3499], Loss: 0.1321083903312683, Training Accuracy: 97.6
[ Fri Jul  5 00:49:08 2024 ] 	Batch(3500/7879) done. Loss: 0.0305  lr:0.000001
[ Fri Jul  5 00:49:26 2024 ] 	Batch(3600/7879) done. Loss: 0.0114  lr:0.000001
[ Fri Jul  5 00:49:44 2024 ] 	Batch(3700/7879) done. Loss: 0.0611  lr:0.000001
[ Fri Jul  5 00:50:02 2024 ] 	Batch(3800/7879) done. Loss: 0.1553  lr:0.000001
[ Fri Jul  5 00:50:20 2024 ] 	Batch(3900/7879) done. Loss: 0.0036  lr:0.000001
[ Fri Jul  5 00:50:38 2024 ] 
Training: Epoch [7/120], Step [3999], Loss: 0.039083197712898254, Training Accuracy: 97.6125
[ Fri Jul  5 00:50:38 2024 ] 	Batch(4000/7879) done. Loss: 0.0755  lr:0.000001
[ Fri Jul  5 00:50:57 2024 ] 	Batch(4100/7879) done. Loss: 0.0486  lr:0.000001
[ Fri Jul  5 00:51:15 2024 ] 	Batch(4200/7879) done. Loss: 0.0267  lr:0.000001
[ Fri Jul  5 00:51:33 2024 ] 	Batch(4300/7879) done. Loss: 0.0192  lr:0.000001
[ Fri Jul  5 00:51:51 2024 ] 	Batch(4400/7879) done. Loss: 0.0574  lr:0.000001
[ Fri Jul  5 00:52:09 2024 ] 
Training: Epoch [7/120], Step [4499], Loss: 0.02987661585211754, Training Accuracy: 97.61666666666666
[ Fri Jul  5 00:52:09 2024 ] 	Batch(4500/7879) done. Loss: 0.0406  lr:0.000001
[ Fri Jul  5 00:52:27 2024 ] 	Batch(4600/7879) done. Loss: 0.0083  lr:0.000001
[ Fri Jul  5 00:52:45 2024 ] 	Batch(4700/7879) done. Loss: 0.0359  lr:0.000001
[ Fri Jul  5 00:53:03 2024 ] 	Batch(4800/7879) done. Loss: 0.0064  lr:0.000001
[ Fri Jul  5 00:53:21 2024 ] 	Batch(4900/7879) done. Loss: 0.1714  lr:0.000001
[ Fri Jul  5 00:53:39 2024 ] 
Training: Epoch [7/120], Step [4999], Loss: 0.010529098100960255, Training Accuracy: 97.63
[ Fri Jul  5 00:53:39 2024 ] 	Batch(5000/7879) done. Loss: 0.1250  lr:0.000001
[ Fri Jul  5 00:53:57 2024 ] 	Batch(5100/7879) done. Loss: 0.4170  lr:0.000001
[ Fri Jul  5 00:54:16 2024 ] 	Batch(5200/7879) done. Loss: 0.1333  lr:0.000001
[ Fri Jul  5 00:54:34 2024 ] 	Batch(5300/7879) done. Loss: 0.0290  lr:0.000001
[ Fri Jul  5 00:54:52 2024 ] 	Batch(5400/7879) done. Loss: 0.1137  lr:0.000001
[ Fri Jul  5 00:55:10 2024 ] 
Training: Epoch [7/120], Step [5499], Loss: 0.046243395656347275, Training Accuracy: 97.61363636363637
[ Fri Jul  5 00:55:10 2024 ] 	Batch(5500/7879) done. Loss: 0.1136  lr:0.000001
[ Fri Jul  5 00:55:28 2024 ] 	Batch(5600/7879) done. Loss: 0.0257  lr:0.000001
[ Fri Jul  5 00:55:46 2024 ] 	Batch(5700/7879) done. Loss: 0.0895  lr:0.000001
[ Fri Jul  5 00:56:04 2024 ] 	Batch(5800/7879) done. Loss: 0.0741  lr:0.000001
[ Fri Jul  5 00:56:22 2024 ] 	Batch(5900/7879) done. Loss: 0.0388  lr:0.000001
[ Fri Jul  5 00:56:40 2024 ] 
Training: Epoch [7/120], Step [5999], Loss: 0.16280795633792877, Training Accuracy: 97.64791666666667
[ Fri Jul  5 00:56:41 2024 ] 	Batch(6000/7879) done. Loss: 0.1611  lr:0.000001
[ Fri Jul  5 00:56:59 2024 ] 	Batch(6100/7879) done. Loss: 0.0309  lr:0.000001
[ Fri Jul  5 00:57:17 2024 ] 	Batch(6200/7879) done. Loss: 0.0331  lr:0.000001
[ Fri Jul  5 00:57:35 2024 ] 	Batch(6300/7879) done. Loss: 0.0179  lr:0.000001
[ Fri Jul  5 00:57:53 2024 ] 	Batch(6400/7879) done. Loss: 0.0508  lr:0.000001
[ Fri Jul  5 00:58:11 2024 ] 
Training: Epoch [7/120], Step [6499], Loss: 0.3118913471698761, Training Accuracy: 97.66346153846153
[ Fri Jul  5 00:58:11 2024 ] 	Batch(6500/7879) done. Loss: 0.2335  lr:0.000001
[ Fri Jul  5 00:58:29 2024 ] 	Batch(6600/7879) done. Loss: 0.1166  lr:0.000001
[ Fri Jul  5 00:58:47 2024 ] 	Batch(6700/7879) done. Loss: 0.2307  lr:0.000001
[ Fri Jul  5 00:59:06 2024 ] 	Batch(6800/7879) done. Loss: 0.0558  lr:0.000001
[ Fri Jul  5 00:59:24 2024 ] 	Batch(6900/7879) done. Loss: 0.0136  lr:0.000001
[ Fri Jul  5 00:59:42 2024 ] 
Training: Epoch [7/120], Step [6999], Loss: 0.12948337197303772, Training Accuracy: 97.66964285714286
[ Fri Jul  5 00:59:42 2024 ] 	Batch(7000/7879) done. Loss: 0.0832  lr:0.000001
[ Fri Jul  5 01:00:00 2024 ] 	Batch(7100/7879) done. Loss: 0.0937  lr:0.000001
[ Fri Jul  5 01:00:18 2024 ] 	Batch(7200/7879) done. Loss: 0.0080  lr:0.000001
[ Fri Jul  5 01:00:36 2024 ] 	Batch(7300/7879) done. Loss: 0.0713  lr:0.000001
[ Fri Jul  5 01:00:55 2024 ] 	Batch(7400/7879) done. Loss: 0.0670  lr:0.000001
[ Fri Jul  5 01:01:13 2024 ] 
Training: Epoch [7/120], Step [7499], Loss: 0.004703800193965435, Training Accuracy: 97.65833333333333
[ Fri Jul  5 01:01:14 2024 ] 	Batch(7500/7879) done. Loss: 0.0230  lr:0.000001
[ Fri Jul  5 01:01:32 2024 ] 	Batch(7600/7879) done. Loss: 0.0602  lr:0.000001
[ Fri Jul  5 01:01:50 2024 ] 	Batch(7700/7879) done. Loss: 0.0796  lr:0.000001
[ Fri Jul  5 01:02:08 2024 ] 	Batch(7800/7879) done. Loss: 0.0153  lr:0.000001
[ Fri Jul  5 01:02:22 2024 ] 	Mean training loss: 0.0952.
[ Fri Jul  5 01:02:22 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul  5 01:02:22 2024 ] Training epoch: 9
[ Fri Jul  5 01:02:23 2024 ] 	Batch(0/7879) done. Loss: 0.1218  lr:0.000001
[ Fri Jul  5 01:02:41 2024 ] 	Batch(100/7879) done. Loss: 0.1761  lr:0.000001
[ Fri Jul  5 01:02:59 2024 ] 	Batch(200/7879) done. Loss: 0.0394  lr:0.000001
[ Fri Jul  5 01:03:18 2024 ] 	Batch(300/7879) done. Loss: 0.0423  lr:0.000001
[ Fri Jul  5 01:03:36 2024 ] 	Batch(400/7879) done. Loss: 0.0438  lr:0.000001
[ Fri Jul  5 01:03:54 2024 ] 
Training: Epoch [8/120], Step [499], Loss: 0.03349842503666878, Training Accuracy: 97.5
[ Fri Jul  5 01:03:54 2024 ] 	Batch(500/7879) done. Loss: 0.0496  lr:0.000001
[ Fri Jul  5 01:04:12 2024 ] 	Batch(600/7879) done. Loss: 0.0535  lr:0.000001
[ Fri Jul  5 01:04:30 2024 ] 	Batch(700/7879) done. Loss: 0.3929  lr:0.000001
[ Fri Jul  5 01:04:48 2024 ] 	Batch(800/7879) done. Loss: 0.0814  lr:0.000001
[ Fri Jul  5 01:05:06 2024 ] 	Batch(900/7879) done. Loss: 0.0374  lr:0.000001
[ Fri Jul  5 01:05:24 2024 ] 
Training: Epoch [8/120], Step [999], Loss: 0.03848951682448387, Training Accuracy: 97.7
[ Fri Jul  5 01:05:24 2024 ] 	Batch(1000/7879) done. Loss: 0.0630  lr:0.000001
[ Fri Jul  5 01:05:42 2024 ] 	Batch(1100/7879) done. Loss: 0.0178  lr:0.000001
[ Fri Jul  5 01:06:00 2024 ] 	Batch(1200/7879) done. Loss: 0.0042  lr:0.000001
[ Fri Jul  5 01:06:18 2024 ] 	Batch(1300/7879) done. Loss: 0.0125  lr:0.000001
[ Fri Jul  5 01:06:36 2024 ] 	Batch(1400/7879) done. Loss: 0.0044  lr:0.000001
[ Fri Jul  5 01:06:54 2024 ] 
Training: Epoch [8/120], Step [1499], Loss: 0.13027243316173553, Training Accuracy: 97.575
[ Fri Jul  5 01:06:54 2024 ] 	Batch(1500/7879) done. Loss: 0.2703  lr:0.000001
[ Fri Jul  5 01:07:13 2024 ] 	Batch(1600/7879) done. Loss: 0.0300  lr:0.000001
[ Fri Jul  5 01:07:31 2024 ] 	Batch(1700/7879) done. Loss: 0.3752  lr:0.000001
[ Fri Jul  5 01:07:49 2024 ] 	Batch(1800/7879) done. Loss: 0.0402  lr:0.000001
[ Fri Jul  5 01:08:07 2024 ] 	Batch(1900/7879) done. Loss: 0.0098  lr:0.000001
[ Fri Jul  5 01:08:24 2024 ] 
Training: Epoch [8/120], Step [1999], Loss: 0.1593659669160843, Training Accuracy: 97.69375
[ Fri Jul  5 01:08:25 2024 ] 	Batch(2000/7879) done. Loss: 0.1139  lr:0.000001
[ Fri Jul  5 01:08:43 2024 ] 	Batch(2100/7879) done. Loss: 0.1398  lr:0.000001
[ Fri Jul  5 01:09:01 2024 ] 	Batch(2200/7879) done. Loss: 0.0240  lr:0.000001
[ Fri Jul  5 01:09:19 2024 ] 	Batch(2300/7879) done. Loss: 0.1619  lr:0.000001
[ Fri Jul  5 01:09:37 2024 ] 	Batch(2400/7879) done. Loss: 0.4887  lr:0.000001
[ Fri Jul  5 01:09:55 2024 ] 
Training: Epoch [8/120], Step [2499], Loss: 0.03508656844496727, Training Accuracy: 97.695
[ Fri Jul  5 01:09:55 2024 ] 	Batch(2500/7879) done. Loss: 0.0111  lr:0.000001
[ Fri Jul  5 01:10:14 2024 ] 	Batch(2600/7879) done. Loss: 0.0674  lr:0.000001
[ Fri Jul  5 01:10:32 2024 ] 	Batch(2700/7879) done. Loss: 0.0279  lr:0.000001
[ Fri Jul  5 01:10:51 2024 ] 	Batch(2800/7879) done. Loss: 0.0105  lr:0.000001
[ Fri Jul  5 01:11:09 2024 ] 	Batch(2900/7879) done. Loss: 0.0648  lr:0.000001
[ Fri Jul  5 01:11:27 2024 ] 
Training: Epoch [8/120], Step [2999], Loss: 0.012670404277741909, Training Accuracy: 97.7625
[ Fri Jul  5 01:11:27 2024 ] 	Batch(3000/7879) done. Loss: 0.5089  lr:0.000001
[ Fri Jul  5 01:11:45 2024 ] 	Batch(3100/7879) done. Loss: 0.0063  lr:0.000001
[ Fri Jul  5 01:12:03 2024 ] 	Batch(3200/7879) done. Loss: 0.0367  lr:0.000001
[ Fri Jul  5 01:12:21 2024 ] 	Batch(3300/7879) done. Loss: 0.0484  lr:0.000001
[ Fri Jul  5 01:12:39 2024 ] 	Batch(3400/7879) done. Loss: 0.0579  lr:0.000001
[ Fri Jul  5 01:12:57 2024 ] 
Training: Epoch [8/120], Step [3499], Loss: 0.10801074653863907, Training Accuracy: 97.76785714285714
[ Fri Jul  5 01:12:57 2024 ] 	Batch(3500/7879) done. Loss: 0.0143  lr:0.000001
[ Fri Jul  5 01:13:15 2024 ] 	Batch(3600/7879) done. Loss: 0.2086  lr:0.000001
[ Fri Jul  5 01:13:34 2024 ] 	Batch(3700/7879) done. Loss: 0.1742  lr:0.000001
[ Fri Jul  5 01:13:52 2024 ] 	Batch(3800/7879) done. Loss: 0.1429  lr:0.000001
[ Fri Jul  5 01:14:11 2024 ] 	Batch(3900/7879) done. Loss: 0.0329  lr:0.000001
[ Fri Jul  5 01:14:29 2024 ] 
Training: Epoch [8/120], Step [3999], Loss: 0.02701721340417862, Training Accuracy: 97.78750000000001
[ Fri Jul  5 01:14:29 2024 ] 	Batch(4000/7879) done. Loss: 0.1584  lr:0.000001
[ Fri Jul  5 01:14:48 2024 ] 	Batch(4100/7879) done. Loss: 0.0470  lr:0.000001
[ Fri Jul  5 01:15:06 2024 ] 	Batch(4200/7879) done. Loss: 0.0947  lr:0.000001
[ Fri Jul  5 01:15:25 2024 ] 	Batch(4300/7879) done. Loss: 0.1125  lr:0.000001
[ Fri Jul  5 01:15:44 2024 ] 	Batch(4400/7879) done. Loss: 0.1304  lr:0.000001
[ Fri Jul  5 01:16:02 2024 ] 
Training: Epoch [8/120], Step [4499], Loss: 0.03730608895421028, Training Accuracy: 97.75277777777778
[ Fri Jul  5 01:16:02 2024 ] 	Batch(4500/7879) done. Loss: 0.0545  lr:0.000001
[ Fri Jul  5 01:16:20 2024 ] 	Batch(4600/7879) done. Loss: 0.0463  lr:0.000001
[ Fri Jul  5 01:16:38 2024 ] 	Batch(4700/7879) done. Loss: 0.0132  lr:0.000001
[ Fri Jul  5 01:16:56 2024 ] 	Batch(4800/7879) done. Loss: 0.0531  lr:0.000001
[ Fri Jul  5 01:17:14 2024 ] 	Batch(4900/7879) done. Loss: 0.1221  lr:0.000001
[ Fri Jul  5 01:17:32 2024 ] 
Training: Epoch [8/120], Step [4999], Loss: 0.01862454041838646, Training Accuracy: 97.7375
[ Fri Jul  5 01:17:32 2024 ] 	Batch(5000/7879) done. Loss: 0.0577  lr:0.000001
[ Fri Jul  5 01:17:50 2024 ] 	Batch(5100/7879) done. Loss: 0.2605  lr:0.000001
[ Fri Jul  5 01:18:08 2024 ] 	Batch(5200/7879) done. Loss: 0.1088  lr:0.000001
[ Fri Jul  5 01:18:26 2024 ] 	Batch(5300/7879) done. Loss: 0.0206  lr:0.000001
[ Fri Jul  5 01:18:44 2024 ] 	Batch(5400/7879) done. Loss: 0.1859  lr:0.000001
[ Fri Jul  5 01:19:01 2024 ] 
Training: Epoch [8/120], Step [5499], Loss: 0.003483121981844306, Training Accuracy: 97.74090909090908
[ Fri Jul  5 01:19:02 2024 ] 	Batch(5500/7879) done. Loss: 0.0049  lr:0.000001
[ Fri Jul  5 01:19:20 2024 ] 	Batch(5600/7879) done. Loss: 0.0070  lr:0.000001
[ Fri Jul  5 01:19:38 2024 ] 	Batch(5700/7879) done. Loss: 0.0442  lr:0.000001
[ Fri Jul  5 01:19:56 2024 ] 	Batch(5800/7879) done. Loss: 0.0123  lr:0.000001
[ Fri Jul  5 01:20:13 2024 ] 	Batch(5900/7879) done. Loss: 0.1654  lr:0.000001
[ Fri Jul  5 01:20:31 2024 ] 
Training: Epoch [8/120], Step [5999], Loss: 0.03660590201616287, Training Accuracy: 97.73125
[ Fri Jul  5 01:20:32 2024 ] 	Batch(6000/7879) done. Loss: 0.0359  lr:0.000001
[ Fri Jul  5 01:20:49 2024 ] 	Batch(6100/7879) done. Loss: 0.0120  lr:0.000001
[ Fri Jul  5 01:21:07 2024 ] 	Batch(6200/7879) done. Loss: 0.4505  lr:0.000001
[ Fri Jul  5 01:21:25 2024 ] 	Batch(6300/7879) done. Loss: 0.0295  lr:0.000001
[ Fri Jul  5 01:21:43 2024 ] 	Batch(6400/7879) done. Loss: 0.0471  lr:0.000001
[ Fri Jul  5 01:22:01 2024 ] 
Training: Epoch [8/120], Step [6499], Loss: 0.1711570918560028, Training Accuracy: 97.71538461538462
[ Fri Jul  5 01:22:01 2024 ] 	Batch(6500/7879) done. Loss: 0.2658  lr:0.000001
[ Fri Jul  5 01:22:19 2024 ] 	Batch(6600/7879) done. Loss: 0.1178  lr:0.000001
[ Fri Jul  5 01:22:37 2024 ] 	Batch(6700/7879) done. Loss: 0.0046  lr:0.000001
[ Fri Jul  5 01:22:55 2024 ] 	Batch(6800/7879) done. Loss: 0.0725  lr:0.000001
[ Fri Jul  5 01:23:13 2024 ] 	Batch(6900/7879) done. Loss: 0.1969  lr:0.000001
[ Fri Jul  5 01:23:31 2024 ] 
Training: Epoch [8/120], Step [6999], Loss: 0.20247206091880798, Training Accuracy: 97.70892857142857
[ Fri Jul  5 01:23:31 2024 ] 	Batch(7000/7879) done. Loss: 0.0117  lr:0.000001
[ Fri Jul  5 01:23:49 2024 ] 	Batch(7100/7879) done. Loss: 0.0271  lr:0.000001
[ Fri Jul  5 01:24:07 2024 ] 	Batch(7200/7879) done. Loss: 0.0104  lr:0.000001
[ Fri Jul  5 01:24:25 2024 ] 	Batch(7300/7879) done. Loss: 0.0669  lr:0.000001
[ Fri Jul  5 01:24:43 2024 ] 	Batch(7400/7879) done. Loss: 0.7041  lr:0.000001
[ Fri Jul  5 01:25:00 2024 ] 
Training: Epoch [8/120], Step [7499], Loss: 0.004455920774489641, Training Accuracy: 97.70666666666666
[ Fri Jul  5 01:25:01 2024 ] 	Batch(7500/7879) done. Loss: 0.0931  lr:0.000001
[ Fri Jul  5 01:25:19 2024 ] 	Batch(7600/7879) done. Loss: 0.0129  lr:0.000001
[ Fri Jul  5 01:25:37 2024 ] 	Batch(7700/7879) done. Loss: 0.0605  lr:0.000001
[ Fri Jul  5 01:25:56 2024 ] 	Batch(7800/7879) done. Loss: 0.6091  lr:0.000001
[ Fri Jul  5 01:26:10 2024 ] 	Mean training loss: 0.0911.
[ Fri Jul  5 01:26:10 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul  5 01:26:11 2024 ] Training epoch: 10
[ Fri Jul  5 01:26:11 2024 ] 	Batch(0/7879) done. Loss: 0.0141  lr:0.000001
[ Fri Jul  5 01:26:29 2024 ] 	Batch(100/7879) done. Loss: 0.0206  lr:0.000001
[ Fri Jul  5 01:26:47 2024 ] 	Batch(200/7879) done. Loss: 0.5386  lr:0.000001
[ Fri Jul  5 01:27:05 2024 ] 	Batch(300/7879) done. Loss: 0.0124  lr:0.000001
[ Fri Jul  5 01:27:23 2024 ] 	Batch(400/7879) done. Loss: 0.2908  lr:0.000001
[ Fri Jul  5 01:27:41 2024 ] 
Training: Epoch [9/120], Step [499], Loss: 0.01756027527153492, Training Accuracy: 98.275
[ Fri Jul  5 01:27:41 2024 ] 	Batch(500/7879) done. Loss: 0.0182  lr:0.000001
[ Fri Jul  5 01:27:59 2024 ] 	Batch(600/7879) done. Loss: 0.0830  lr:0.000001
[ Fri Jul  5 01:28:17 2024 ] 	Batch(700/7879) done. Loss: 0.0388  lr:0.000001
[ Fri Jul  5 01:28:36 2024 ] 	Batch(800/7879) done. Loss: 0.0064  lr:0.000001
[ Fri Jul  5 01:28:54 2024 ] 	Batch(900/7879) done. Loss: 0.0285  lr:0.000001
[ Fri Jul  5 01:29:12 2024 ] 
Training: Epoch [9/120], Step [999], Loss: 0.0014923085691407323, Training Accuracy: 97.875
[ Fri Jul  5 01:29:12 2024 ] 	Batch(1000/7879) done. Loss: 0.0102  lr:0.000001
[ Fri Jul  5 01:29:30 2024 ] 	Batch(1100/7879) done. Loss: 0.1192  lr:0.000001
[ Fri Jul  5 01:29:49 2024 ] 	Batch(1200/7879) done. Loss: 0.0388  lr:0.000001
[ Fri Jul  5 01:30:07 2024 ] 	Batch(1300/7879) done. Loss: 0.0346  lr:0.000001
[ Fri Jul  5 01:30:25 2024 ] 	Batch(1400/7879) done. Loss: 0.0330  lr:0.000001
[ Fri Jul  5 01:30:43 2024 ] 
Training: Epoch [9/120], Step [1499], Loss: 0.008800978772342205, Training Accuracy: 97.84166666666667
[ Fri Jul  5 01:30:43 2024 ] 	Batch(1500/7879) done. Loss: 0.1078  lr:0.000001
[ Fri Jul  5 01:31:01 2024 ] 	Batch(1600/7879) done. Loss: 0.0808  lr:0.000001
[ Fri Jul  5 01:31:19 2024 ] 	Batch(1700/7879) done. Loss: 0.0182  lr:0.000001
[ Fri Jul  5 01:31:38 2024 ] 	Batch(1800/7879) done. Loss: 0.0139  lr:0.000001
[ Fri Jul  5 01:31:56 2024 ] 	Batch(1900/7879) done. Loss: 0.0048  lr:0.000001
[ Fri Jul  5 01:32:14 2024 ] 
Training: Epoch [9/120], Step [1999], Loss: 0.03704860061407089, Training Accuracy: 97.8
[ Fri Jul  5 01:32:14 2024 ] 	Batch(2000/7879) done. Loss: 0.0494  lr:0.000001
[ Fri Jul  5 01:32:33 2024 ] 	Batch(2100/7879) done. Loss: 0.0916  lr:0.000001
[ Fri Jul  5 01:32:51 2024 ] 	Batch(2200/7879) done. Loss: 0.0286  lr:0.000001
[ Fri Jul  5 01:33:09 2024 ] 	Batch(2300/7879) done. Loss: 0.0011  lr:0.000001
[ Fri Jul  5 01:33:28 2024 ] 	Batch(2400/7879) done. Loss: 0.0685  lr:0.000001
[ Fri Jul  5 01:33:46 2024 ] 
Training: Epoch [9/120], Step [2499], Loss: 0.12359242886304855, Training Accuracy: 97.725
[ Fri Jul  5 01:33:46 2024 ] 	Batch(2500/7879) done. Loss: 0.0353  lr:0.000001
[ Fri Jul  5 01:34:05 2024 ] 	Batch(2600/7879) done. Loss: 0.0033  lr:0.000001
[ Fri Jul  5 01:34:23 2024 ] 	Batch(2700/7879) done. Loss: 0.0756  lr:0.000001
[ Fri Jul  5 01:34:41 2024 ] 	Batch(2800/7879) done. Loss: 0.0383  lr:0.000001
[ Fri Jul  5 01:35:00 2024 ] 	Batch(2900/7879) done. Loss: 0.0022  lr:0.000001
[ Fri Jul  5 01:35:18 2024 ] 
Training: Epoch [9/120], Step [2999], Loss: 0.04242554306983948, Training Accuracy: 97.6875
[ Fri Jul  5 01:35:18 2024 ] 	Batch(3000/7879) done. Loss: 0.0326  lr:0.000001
[ Fri Jul  5 01:35:36 2024 ] 	Batch(3100/7879) done. Loss: 0.2214  lr:0.000001
[ Fri Jul  5 01:35:55 2024 ] 	Batch(3200/7879) done. Loss: 0.0088  lr:0.000001
[ Fri Jul  5 01:36:13 2024 ] 	Batch(3300/7879) done. Loss: 0.1131  lr:0.000001
[ Fri Jul  5 01:36:31 2024 ] 	Batch(3400/7879) done. Loss: 0.5099  lr:0.000001
[ Fri Jul  5 01:36:49 2024 ] 
Training: Epoch [9/120], Step [3499], Loss: 0.00827989261597395, Training Accuracy: 97.70714285714286
[ Fri Jul  5 01:36:49 2024 ] 	Batch(3500/7879) done. Loss: 0.0078  lr:0.000001
[ Fri Jul  5 01:37:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0937  lr:0.000001
[ Fri Jul  5 01:37:26 2024 ] 	Batch(3700/7879) done. Loss: 0.0857  lr:0.000001
[ Fri Jul  5 01:37:44 2024 ] 	Batch(3800/7879) done. Loss: 0.1423  lr:0.000001
[ Fri Jul  5 01:38:02 2024 ] 	Batch(3900/7879) done. Loss: 0.3394  lr:0.000001
[ Fri Jul  5 01:38:20 2024 ] 
Training: Epoch [9/120], Step [3999], Loss: 0.06208719313144684, Training Accuracy: 97.690625
[ Fri Jul  5 01:38:20 2024 ] 	Batch(4000/7879) done. Loss: 0.0356  lr:0.000001
[ Fri Jul  5 01:38:38 2024 ] 	Batch(4100/7879) done. Loss: 0.0210  lr:0.000001
[ Fri Jul  5 01:38:57 2024 ] 	Batch(4200/7879) done. Loss: 0.0245  lr:0.000001
[ Fri Jul  5 01:39:15 2024 ] 	Batch(4300/7879) done. Loss: 0.1250  lr:0.000001
[ Fri Jul  5 01:39:33 2024 ] 	Batch(4400/7879) done. Loss: 0.0127  lr:0.000001
[ Fri Jul  5 01:39:51 2024 ] 
Training: Epoch [9/120], Step [4499], Loss: 0.11601308733224869, Training Accuracy: 97.64166666666667
[ Fri Jul  5 01:39:52 2024 ] 	Batch(4500/7879) done. Loss: 0.0080  lr:0.000001
[ Fri Jul  5 01:40:10 2024 ] 	Batch(4600/7879) done. Loss: 0.1044  lr:0.000001
[ Fri Jul  5 01:40:28 2024 ] 	Batch(4700/7879) done. Loss: 0.3388  lr:0.000001
[ Fri Jul  5 01:40:46 2024 ] 	Batch(4800/7879) done. Loss: 0.2702  lr:0.000001
[ Fri Jul  5 01:41:04 2024 ] 	Batch(4900/7879) done. Loss: 0.0107  lr:0.000001
[ Fri Jul  5 01:41:22 2024 ] 
Training: Epoch [9/120], Step [4999], Loss: 0.09098353981971741, Training Accuracy: 97.6025
[ Fri Jul  5 01:41:22 2024 ] 	Batch(5000/7879) done. Loss: 0.0080  lr:0.000001
[ Fri Jul  5 01:41:40 2024 ] 	Batch(5100/7879) done. Loss: 0.0686  lr:0.000001
[ Fri Jul  5 01:41:58 2024 ] 	Batch(5200/7879) done. Loss: 0.1270  lr:0.000001
[ Fri Jul  5 01:42:16 2024 ] 	Batch(5300/7879) done. Loss: 0.2075  lr:0.000001
[ Fri Jul  5 01:42:34 2024 ] 	Batch(5400/7879) done. Loss: 0.0098  lr:0.000001
[ Fri Jul  5 01:42:52 2024 ] 
Training: Epoch [9/120], Step [5499], Loss: 0.022481674328446388, Training Accuracy: 97.58181818181818
[ Fri Jul  5 01:42:52 2024 ] 	Batch(5500/7879) done. Loss: 0.0956  lr:0.000001
[ Fri Jul  5 01:43:10 2024 ] 	Batch(5600/7879) done. Loss: 0.0173  lr:0.000001
[ Fri Jul  5 01:43:28 2024 ] 	Batch(5700/7879) done. Loss: 0.1287  lr:0.000001
[ Fri Jul  5 01:43:46 2024 ] 	Batch(5800/7879) done. Loss: 0.0149  lr:0.000001
[ Fri Jul  5 01:44:04 2024 ] 	Batch(5900/7879) done. Loss: 0.1462  lr:0.000001
[ Fri Jul  5 01:44:22 2024 ] 
Training: Epoch [9/120], Step [5999], Loss: 0.0025979275815188885, Training Accuracy: 97.61875
[ Fri Jul  5 01:44:22 2024 ] 	Batch(6000/7879) done. Loss: 0.0017  lr:0.000001
[ Fri Jul  5 01:44:40 2024 ] 	Batch(6100/7879) done. Loss: 0.0436  lr:0.000001
[ Fri Jul  5 01:44:58 2024 ] 	Batch(6200/7879) done. Loss: 0.0159  lr:0.000001
[ Fri Jul  5 01:45:16 2024 ] 	Batch(6300/7879) done. Loss: 0.0049  lr:0.000001
[ Fri Jul  5 01:45:34 2024 ] 	Batch(6400/7879) done. Loss: 0.0264  lr:0.000001
[ Fri Jul  5 01:45:52 2024 ] 
Training: Epoch [9/120], Step [6499], Loss: 0.02214263193309307, Training Accuracy: 97.61730769230769
[ Fri Jul  5 01:45:53 2024 ] 	Batch(6500/7879) done. Loss: 0.0321  lr:0.000001
[ Fri Jul  5 01:46:11 2024 ] 	Batch(6600/7879) done. Loss: 0.2725  lr:0.000001
[ Fri Jul  5 01:46:29 2024 ] 	Batch(6700/7879) done. Loss: 0.1828  lr:0.000001
[ Fri Jul  5 01:46:47 2024 ] 	Batch(6800/7879) done. Loss: 0.1319  lr:0.000001
[ Fri Jul  5 01:47:05 2024 ] 	Batch(6900/7879) done. Loss: 0.0448  lr:0.000001
[ Fri Jul  5 01:47:23 2024 ] 
Training: Epoch [9/120], Step [6999], Loss: 0.53679358959198, Training Accuracy: 97.63928571428572
[ Fri Jul  5 01:47:23 2024 ] 	Batch(7000/7879) done. Loss: 0.0024  lr:0.000001
[ Fri Jul  5 01:47:41 2024 ] 	Batch(7100/7879) done. Loss: 0.0029  lr:0.000001
[ Fri Jul  5 01:48:00 2024 ] 	Batch(7200/7879) done. Loss: 0.0372  lr:0.000001
[ Fri Jul  5 01:48:18 2024 ] 	Batch(7300/7879) done. Loss: 0.0016  lr:0.000001
[ Fri Jul  5 01:48:36 2024 ] 	Batch(7400/7879) done. Loss: 0.1732  lr:0.000001
[ Fri Jul  5 01:48:54 2024 ] 
Training: Epoch [9/120], Step [7499], Loss: 0.14042869210243225, Training Accuracy: 97.65166666666667
[ Fri Jul  5 01:48:54 2024 ] 	Batch(7500/7879) done. Loss: 0.1783  lr:0.000001
[ Fri Jul  5 01:49:13 2024 ] 	Batch(7600/7879) done. Loss: 0.0068  lr:0.000001
[ Fri Jul  5 01:49:31 2024 ] 	Batch(7700/7879) done. Loss: 0.0156  lr:0.000001
[ Fri Jul  5 01:49:49 2024 ] 	Batch(7800/7879) done. Loss: 0.0098  lr:0.000001
[ Fri Jul  5 01:50:03 2024 ] 	Mean training loss: 0.0917.
[ Fri Jul  5 01:50:03 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul  5 01:50:03 2024 ] Eval epoch: 10
[ Fri Jul  5 01:54:50 2024 ] 	Mean val loss of 6365 batches: 1.5151873108523335.
[ Fri Jul  5 01:54:50 2024 ] 
Validation: Epoch [9/120], Samples [39436.0/50919], Loss: 0.03870629519224167, Validation Accuracy: 77.44849663190557
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 1 : 201 / 275 = 73 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 2 : 220 / 273 = 80 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 3 : 226 / 273 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 4 : 218 / 275 = 79 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 5 : 245 / 275 = 89 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 6 : 227 / 275 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 7 : 249 / 273 = 91 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 8 : 264 / 273 = 96 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 9 : 194 / 273 = 71 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 10 : 115 / 273 = 42 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 11 : 127 / 272 = 46 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 12 : 228 / 271 = 84 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 13 : 266 / 275 = 96 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 14 : 267 / 276 = 96 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 15 : 223 / 273 = 81 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 16 : 222 / 274 = 81 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 17 : 234 / 273 = 85 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 18 : 248 / 274 = 90 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 19 : 256 / 272 = 94 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 20 : 246 / 273 = 90 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 21 : 225 / 274 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 22 : 233 / 274 = 85 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 23 : 259 / 276 = 93 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 24 : 227 / 274 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 25 : 258 / 275 = 93 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 26 : 270 / 276 = 97 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 27 : 220 / 275 = 80 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 28 : 173 / 275 = 62 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 29 : 143 / 275 = 52 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 30 : 169 / 276 = 61 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 31 : 240 / 276 = 86 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 32 : 245 / 276 = 88 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 33 : 238 / 276 = 86 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 34 : 240 / 276 = 86 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 35 : 239 / 275 = 86 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 36 : 233 / 276 = 84 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 37 : 248 / 276 = 89 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 38 : 251 / 276 = 90 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 39 : 252 / 276 = 91 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 40 : 188 / 276 = 68 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 41 : 257 / 276 = 93 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 42 : 262 / 275 = 95 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 43 : 179 / 276 = 64 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 44 : 246 / 276 = 89 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 45 : 246 / 276 = 89 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 46 : 222 / 276 = 80 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 47 : 220 / 275 = 80 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 48 : 220 / 275 = 80 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 49 : 226 / 274 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 50 : 241 / 276 = 87 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 51 : 251 / 276 = 90 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 52 : 238 / 276 = 86 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 53 : 236 / 276 = 85 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 54 : 269 / 274 = 98 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 55 : 241 / 276 = 87 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 56 : 242 / 275 = 88 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 57 : 267 / 276 = 96 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 58 : 269 / 273 = 98 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 59 : 267 / 276 = 96 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 60 : 465 / 561 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 61 : 442 / 566 = 78 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 62 : 461 / 572 = 80 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 63 : 520 / 570 = 91 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 64 : 431 / 574 = 75 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 65 : 503 / 573 = 87 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 66 : 412 / 573 = 71 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 67 : 415 / 575 = 72 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 68 : 326 / 575 = 56 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 69 : 469 / 575 = 81 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 70 : 229 / 575 = 39 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 71 : 217 / 575 = 37 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 72 : 142 / 571 = 24 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 73 : 212 / 570 = 37 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 74 : 382 / 569 = 67 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 75 : 247 / 573 = 43 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 76 : 352 / 574 = 61 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 77 : 396 / 573 = 69 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 78 : 409 / 575 = 71 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 79 : 542 / 574 = 94 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 80 : 499 / 573 = 87 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 81 : 324 / 575 = 56 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 82 : 391 / 575 = 68 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 83 : 327 / 572 = 57 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 84 : 430 / 574 = 74 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 85 : 401 / 574 = 69 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 86 : 515 / 575 = 89 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 87 : 490 / 576 = 85 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 88 : 406 / 575 = 70 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 89 : 478 / 576 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 90 : 245 / 574 = 42 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 91 : 471 / 568 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 92 : 352 / 576 = 61 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 93 : 416 / 573 = 72 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 94 : 506 / 574 = 88 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 95 : 519 / 575 = 90 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 96 : 562 / 575 = 97 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 97 : 558 / 574 = 97 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 98 : 541 / 575 = 94 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 99 : 544 / 574 = 94 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 100 : 507 / 574 = 88 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 101 : 509 / 574 = 88 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 102 : 338 / 575 = 58 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 103 : 493 / 576 = 85 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 104 : 272 / 575 = 47 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 105 : 248 / 575 = 43 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 106 : 332 / 576 = 57 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 107 : 482 / 576 = 83 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 108 : 485 / 575 = 84 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 109 : 335 / 575 = 58 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 110 : 473 / 575 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 111 : 534 / 576 = 92 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 112 : 552 / 575 = 96 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 113 : 512 / 576 = 88 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 114 : 488 / 576 = 84 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 115 : 516 / 576 = 89 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 116 : 477 / 575 = 82 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 117 : 453 / 575 = 78 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 118 : 491 / 575 = 85 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 119 : 531 / 576 = 92 %
[ Fri Jul  5 01:54:50 2024 ] Accuracy of 120 : 235 / 274 = 85 %

[ Sat Jul  6 08:00:55 2024 ] Load weights from prova20/epoch10_model_old.
[ Sat Jul  6 08:01:23 2024 ] Load weights from prova20/epoch10_model_old.pt.
[ Sat Jul  6 08:01:23 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xsub/train_joint_120.npy', 'label_path': 'new_data_processed/xsub/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xsub/val_joint_120.npy', 'label_path': 'new_data_processed/xsub/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': True, 'only_attention': True, 'tcn_attention': False, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': 'prova20/epoch10_model_old.pt', 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': True, 'scheduler': 1, 'base_lr': 1e-05, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Sat Jul  6 08:01:23 2024 ] Training epoch: 1
[ Sat Jul  6 08:01:26 2024 ] 	Batch(0/7879) done. Loss: 0.0170  lr:0.000010
[ Sat Jul  6 08:01:43 2024 ] 	Batch(100/7879) done. Loss: 0.0248  lr:0.000010
[ Sat Jul  6 08:02:01 2024 ] 	Batch(200/7879) done. Loss: 0.1048  lr:0.000010
[ Sat Jul  6 08:02:18 2024 ] 	Batch(300/7879) done. Loss: 0.0266  lr:0.000010
[ Sat Jul  6 08:02:36 2024 ] 	Batch(400/7879) done. Loss: 0.5514  lr:0.000010
[ Sat Jul  6 08:02:53 2024 ] 
Training: Epoch [0/120], Step [499], Loss: 0.017301088199019432, Training Accuracy: 97.625
[ Sat Jul  6 08:02:54 2024 ] 	Batch(500/7879) done. Loss: 0.0814  lr:0.000010
[ Sat Jul  6 08:03:11 2024 ] 	Batch(600/7879) done. Loss: 0.0049  lr:0.000010
[ Sat Jul  6 08:03:29 2024 ] 	Batch(700/7879) done. Loss: 0.0422  lr:0.000010
[ Sat Jul  6 08:03:46 2024 ] 	Batch(800/7879) done. Loss: 0.0346  lr:0.000010
[ Sat Jul  6 08:04:04 2024 ] 	Batch(900/7879) done. Loss: 0.0860  lr:0.000010
[ Sat Jul  6 08:04:22 2024 ] 
Training: Epoch [0/120], Step [999], Loss: 0.03277081996202469, Training Accuracy: 97.75
[ Sat Jul  6 08:04:22 2024 ] 	Batch(1000/7879) done. Loss: 0.0856  lr:0.000010
[ Sat Jul  6 08:04:40 2024 ] 	Batch(1100/7879) done. Loss: 0.1732  lr:0.000010
[ Sat Jul  6 08:04:58 2024 ] 	Batch(1200/7879) done. Loss: 0.0211  lr:0.000010
[ Sat Jul  6 08:05:15 2024 ] 	Batch(1300/7879) done. Loss: 0.1017  lr:0.000010
[ Sat Jul  6 08:05:32 2024 ] 	Batch(1400/7879) done. Loss: 0.0412  lr:0.000010
[ Sat Jul  6 08:05:49 2024 ] 
Training: Epoch [0/120], Step [1499], Loss: 0.05761212110519409, Training Accuracy: 97.71666666666667
[ Sat Jul  6 08:05:49 2024 ] 	Batch(1500/7879) done. Loss: 0.0208  lr:0.000010
[ Sat Jul  6 08:06:07 2024 ] 	Batch(1600/7879) done. Loss: 0.0590  lr:0.000010
[ Sat Jul  6 08:06:25 2024 ] 	Batch(1700/7879) done. Loss: 0.0767  lr:0.000010
[ Sat Jul  6 08:06:42 2024 ] 	Batch(1800/7879) done. Loss: 0.0120  lr:0.000010
[ Sat Jul  6 08:07:00 2024 ] 	Batch(1900/7879) done. Loss: 0.0213  lr:0.000010
[ Sat Jul  6 08:07:18 2024 ] 
Training: Epoch [0/120], Step [1999], Loss: 0.004317156504839659, Training Accuracy: 97.79375
[ Sat Jul  6 08:07:18 2024 ] 	Batch(2000/7879) done. Loss: 0.1384  lr:0.000010
[ Sat Jul  6 08:07:36 2024 ] 	Batch(2100/7879) done. Loss: 0.1050  lr:0.000010
[ Sat Jul  6 08:07:54 2024 ] 	Batch(2200/7879) done. Loss: 0.0743  lr:0.000010
[ Sat Jul  6 08:08:12 2024 ] 	Batch(2300/7879) done. Loss: 0.0568  lr:0.000010
[ Sat Jul  6 08:08:29 2024 ] 	Batch(2400/7879) done. Loss: 0.4373  lr:0.000010
[ Sat Jul  6 08:08:47 2024 ] 
Training: Epoch [0/120], Step [2499], Loss: 0.10486353933811188, Training Accuracy: 97.775
[ Sat Jul  6 08:08:47 2024 ] 	Batch(2500/7879) done. Loss: 0.0815  lr:0.000010
[ Sat Jul  6 08:09:04 2024 ] 	Batch(2600/7879) done. Loss: 0.0405  lr:0.000010
[ Sat Jul  6 08:09:21 2024 ] 	Batch(2700/7879) done. Loss: 0.1079  lr:0.000010
[ Sat Jul  6 08:09:39 2024 ] 	Batch(2800/7879) done. Loss: 0.7119  lr:0.000010
[ Sat Jul  6 08:09:56 2024 ] 	Batch(2900/7879) done. Loss: 0.3753  lr:0.000010
[ Sat Jul  6 08:10:13 2024 ] 
Training: Epoch [0/120], Step [2999], Loss: 0.24998442828655243, Training Accuracy: 97.725
[ Sat Jul  6 08:10:13 2024 ] 	Batch(3000/7879) done. Loss: 0.0560  lr:0.000010
[ Sat Jul  6 08:10:30 2024 ] 	Batch(3100/7879) done. Loss: 0.0585  lr:0.000010
[ Sat Jul  6 08:10:48 2024 ] 	Batch(3200/7879) done. Loss: 0.1833  lr:0.000010
[ Sat Jul  6 08:11:05 2024 ] 	Batch(3300/7879) done. Loss: 0.0294  lr:0.000010
[ Sat Jul  6 08:11:23 2024 ] 	Batch(3400/7879) done. Loss: 0.2563  lr:0.000010
[ Sat Jul  6 08:11:40 2024 ] 
Training: Epoch [0/120], Step [3499], Loss: 0.02016538754105568, Training Accuracy: 97.78571428571429
[ Sat Jul  6 08:11:41 2024 ] 	Batch(3500/7879) done. Loss: 0.1895  lr:0.000010
[ Sat Jul  6 08:11:58 2024 ] 	Batch(3600/7879) done. Loss: 0.1656  lr:0.000010
[ Sat Jul  6 08:12:15 2024 ] 	Batch(3700/7879) done. Loss: 0.0048  lr:0.000010
[ Sat Jul  6 08:12:32 2024 ] 	Batch(3800/7879) done. Loss: 0.0473  lr:0.000010
[ Sat Jul  6 08:12:50 2024 ] 	Batch(3900/7879) done. Loss: 0.0116  lr:0.000010
[ Sat Jul  6 08:13:07 2024 ] 
Training: Epoch [0/120], Step [3999], Loss: 0.005420173984020948, Training Accuracy: 97.78750000000001
[ Sat Jul  6 08:13:07 2024 ] 	Batch(4000/7879) done. Loss: 0.0365  lr:0.000010
[ Sat Jul  6 08:13:24 2024 ] 	Batch(4100/7879) done. Loss: 0.0651  lr:0.000010
[ Sat Jul  6 08:13:42 2024 ] 	Batch(4200/7879) done. Loss: 0.0138  lr:0.000010
[ Sat Jul  6 08:13:59 2024 ] 	Batch(4300/7879) done. Loss: 0.0203  lr:0.000010
[ Sat Jul  6 08:14:16 2024 ] 	Batch(4400/7879) done. Loss: 0.0262  lr:0.000010
[ Sat Jul  6 08:14:34 2024 ] 
Training: Epoch [0/120], Step [4499], Loss: 0.08390948176383972, Training Accuracy: 97.82499999999999
[ Sat Jul  6 08:14:34 2024 ] 	Batch(4500/7879) done. Loss: 0.0080  lr:0.000010
[ Sat Jul  6 08:14:52 2024 ] 	Batch(4600/7879) done. Loss: 0.0017  lr:0.000010
[ Sat Jul  6 08:15:09 2024 ] 	Batch(4700/7879) done. Loss: 0.0073  lr:0.000010
[ Sat Jul  6 08:15:27 2024 ] 	Batch(4800/7879) done. Loss: 0.2201  lr:0.000010
[ Sat Jul  6 08:15:45 2024 ] 	Batch(4900/7879) done. Loss: 0.2929  lr:0.000010
[ Sat Jul  6 08:16:03 2024 ] 
Training: Epoch [0/120], Step [4999], Loss: 0.07788695394992828, Training Accuracy: 97.8225
[ Sat Jul  6 08:16:03 2024 ] 	Batch(5000/7879) done. Loss: 0.2204  lr:0.000010
[ Sat Jul  6 08:16:21 2024 ] 	Batch(5100/7879) done. Loss: 0.3492  lr:0.000010
[ Sat Jul  6 08:16:39 2024 ] 	Batch(5200/7879) done. Loss: 0.0349  lr:0.000010
[ Sat Jul  6 08:16:56 2024 ] 	Batch(5300/7879) done. Loss: 0.0885  lr:0.000010
[ Sat Jul  6 08:17:13 2024 ] 	Batch(5400/7879) done. Loss: 0.1209  lr:0.000010
[ Sat Jul  6 08:17:30 2024 ] 
Training: Epoch [0/120], Step [5499], Loss: 0.00466461107134819, Training Accuracy: 97.83636363636363
[ Sat Jul  6 08:17:30 2024 ] 	Batch(5500/7879) done. Loss: 0.0463  lr:0.000010
[ Sat Jul  6 08:17:48 2024 ] 	Batch(5600/7879) done. Loss: 0.0054  lr:0.000010
[ Sat Jul  6 08:18:05 2024 ] 	Batch(5700/7879) done. Loss: 0.1335  lr:0.000010
[ Sat Jul  6 08:18:23 2024 ] 	Batch(5800/7879) done. Loss: 0.0367  lr:0.000010
[ Sat Jul  6 08:18:41 2024 ] 	Batch(5900/7879) done. Loss: 0.0937  lr:0.000010
[ Sat Jul  6 08:18:59 2024 ] 
Training: Epoch [0/120], Step [5999], Loss: 0.012622189708054066, Training Accuracy: 97.79166666666667
[ Sat Jul  6 08:18:59 2024 ] 	Batch(6000/7879) done. Loss: 0.0045  lr:0.000010
[ Sat Jul  6 08:19:16 2024 ] 	Batch(6100/7879) done. Loss: 0.0191  lr:0.000010
[ Sat Jul  6 08:19:33 2024 ] 	Batch(6200/7879) done. Loss: 0.0295  lr:0.000010
[ Sat Jul  6 08:19:50 2024 ] 	Batch(6300/7879) done. Loss: 0.0381  lr:0.000010
[ Sat Jul  6 08:20:08 2024 ] 	Batch(6400/7879) done. Loss: 0.0338  lr:0.000010
[ Sat Jul  6 08:20:25 2024 ] 
Training: Epoch [0/120], Step [6499], Loss: 0.07803648710250854, Training Accuracy: 97.76923076923076
[ Sat Jul  6 08:20:25 2024 ] 	Batch(6500/7879) done. Loss: 0.1233  lr:0.000010
[ Sat Jul  6 08:20:42 2024 ] 	Batch(6600/7879) done. Loss: 0.1382  lr:0.000010
[ Sat Jul  6 08:20:59 2024 ] 	Batch(6700/7879) done. Loss: 0.1079  lr:0.000010
[ Sat Jul  6 08:21:17 2024 ] 	Batch(6800/7879) done. Loss: 0.0049  lr:0.000010
[ Sat Jul  6 08:21:34 2024 ] 	Batch(6900/7879) done. Loss: 0.0665  lr:0.000010
[ Sat Jul  6 08:21:52 2024 ] 
Training: Epoch [0/120], Step [6999], Loss: 0.00883970595896244, Training Accuracy: 97.75535714285715
[ Sat Jul  6 08:21:52 2024 ] 	Batch(7000/7879) done. Loss: 0.1725  lr:0.000010
[ Sat Jul  6 08:22:09 2024 ] 	Batch(7100/7879) done. Loss: 0.0276  lr:0.000010
[ Sat Jul  6 08:22:26 2024 ] 	Batch(7200/7879) done. Loss: 0.1298  lr:0.000010
[ Sat Jul  6 08:22:44 2024 ] 	Batch(7300/7879) done. Loss: 0.1205  lr:0.000010
[ Sat Jul  6 08:23:01 2024 ] 	Batch(7400/7879) done. Loss: 0.0906  lr:0.000010
[ Sat Jul  6 08:23:18 2024 ] 
Training: Epoch [0/120], Step [7499], Loss: 0.08808448165655136, Training Accuracy: 97.74833333333333
[ Sat Jul  6 08:23:18 2024 ] 	Batch(7500/7879) done. Loss: 0.0217  lr:0.000010
[ Sat Jul  6 08:23:36 2024 ] 	Batch(7600/7879) done. Loss: 0.1897  lr:0.000010
[ Sat Jul  6 08:23:53 2024 ] 	Batch(7700/7879) done. Loss: 0.4023  lr:0.000010
[ Sat Jul  6 08:24:11 2024 ] 	Batch(7800/7879) done. Loss: 0.0339  lr:0.000010
[ Sat Jul  6 08:24:24 2024 ] 	Mean training loss: 0.0934.
[ Sat Jul  6 08:24:24 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 08:24:24 2024 ] Training epoch: 2
[ Sat Jul  6 08:24:25 2024 ] 	Batch(0/7879) done. Loss: 0.0241  lr:0.000010
[ Sat Jul  6 08:24:43 2024 ] 	Batch(100/7879) done. Loss: 0.0370  lr:0.000010
[ Sat Jul  6 08:25:01 2024 ] 	Batch(200/7879) done. Loss: 0.0234  lr:0.000010
[ Sat Jul  6 08:25:19 2024 ] 	Batch(300/7879) done. Loss: 0.0332  lr:0.000010
[ Sat Jul  6 08:25:37 2024 ] 	Batch(400/7879) done. Loss: 0.0129  lr:0.000010
[ Sat Jul  6 08:25:54 2024 ] 
Training: Epoch [1/120], Step [499], Loss: 0.003129506716504693, Training Accuracy: 97.575
[ Sat Jul  6 08:25:55 2024 ] 	Batch(500/7879) done. Loss: 0.0665  lr:0.000010
[ Sat Jul  6 08:26:13 2024 ] 	Batch(600/7879) done. Loss: 0.1283  lr:0.000010
[ Sat Jul  6 08:26:30 2024 ] 	Batch(700/7879) done. Loss: 0.0159  lr:0.000010
[ Sat Jul  6 08:26:48 2024 ] 	Batch(800/7879) done. Loss: 0.0603  lr:0.000010
[ Sat Jul  6 08:27:07 2024 ] 	Batch(900/7879) done. Loss: 0.1271  lr:0.000010
[ Sat Jul  6 08:27:25 2024 ] 
Training: Epoch [1/120], Step [999], Loss: 0.032667726278305054, Training Accuracy: 97.6875
[ Sat Jul  6 08:27:26 2024 ] 	Batch(1000/7879) done. Loss: 0.0583  lr:0.000010
[ Sat Jul  6 08:27:44 2024 ] 	Batch(1100/7879) done. Loss: 0.0240  lr:0.000010
[ Sat Jul  6 08:28:02 2024 ] 	Batch(1200/7879) done. Loss: 0.0026  lr:0.000010
[ Sat Jul  6 08:28:19 2024 ] 	Batch(1300/7879) done. Loss: 0.1892  lr:0.000010
[ Sat Jul  6 08:28:37 2024 ] 	Batch(1400/7879) done. Loss: 0.0519  lr:0.000010
[ Sat Jul  6 08:28:55 2024 ] 
Training: Epoch [1/120], Step [1499], Loss: 0.02128785476088524, Training Accuracy: 97.73333333333333
[ Sat Jul  6 08:28:55 2024 ] 	Batch(1500/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 08:29:13 2024 ] 	Batch(1600/7879) done. Loss: 0.1057  lr:0.000010
[ Sat Jul  6 08:29:31 2024 ] 	Batch(1700/7879) done. Loss: 0.2107  lr:0.000010
[ Sat Jul  6 08:29:49 2024 ] 	Batch(1800/7879) done. Loss: 0.0254  lr:0.000010
[ Sat Jul  6 08:30:07 2024 ] 	Batch(1900/7879) done. Loss: 0.0164  lr:0.000010
[ Sat Jul  6 08:30:25 2024 ] 
Training: Epoch [1/120], Step [1999], Loss: 0.02169272117316723, Training Accuracy: 97.675
[ Sat Jul  6 08:30:25 2024 ] 	Batch(2000/7879) done. Loss: 0.0259  lr:0.000010
[ Sat Jul  6 08:30:43 2024 ] 	Batch(2100/7879) done. Loss: 0.0517  lr:0.000010
[ Sat Jul  6 08:31:01 2024 ] 	Batch(2200/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 08:31:19 2024 ] 	Batch(2300/7879) done. Loss: 0.1068  lr:0.000010
[ Sat Jul  6 08:31:37 2024 ] 	Batch(2400/7879) done. Loss: 0.0176  lr:0.000010
[ Sat Jul  6 08:31:55 2024 ] 
Training: Epoch [1/120], Step [2499], Loss: 0.20634038746356964, Training Accuracy: 97.685
[ Sat Jul  6 08:31:55 2024 ] 	Batch(2500/7879) done. Loss: 0.0620  lr:0.000010
[ Sat Jul  6 08:32:13 2024 ] 	Batch(2600/7879) done. Loss: 0.0350  lr:0.000010
[ Sat Jul  6 08:32:31 2024 ] 	Batch(2700/7879) done. Loss: 0.1277  lr:0.000010
[ Sat Jul  6 08:32:49 2024 ] 	Batch(2800/7879) done. Loss: 0.1949  lr:0.000010
[ Sat Jul  6 08:33:07 2024 ] 	Batch(2900/7879) done. Loss: 0.3956  lr:0.000010
[ Sat Jul  6 08:33:24 2024 ] 
Training: Epoch [1/120], Step [2999], Loss: 0.03548752889037132, Training Accuracy: 97.68333333333334
[ Sat Jul  6 08:33:25 2024 ] 	Batch(3000/7879) done. Loss: 0.0706  lr:0.000010
[ Sat Jul  6 08:33:42 2024 ] 	Batch(3100/7879) done. Loss: 0.0113  lr:0.000010
[ Sat Jul  6 08:34:01 2024 ] 	Batch(3200/7879) done. Loss: 0.0154  lr:0.000010
[ Sat Jul  6 08:34:18 2024 ] 	Batch(3300/7879) done. Loss: 0.0129  lr:0.000010
[ Sat Jul  6 08:34:36 2024 ] 	Batch(3400/7879) done. Loss: 0.1272  lr:0.000010
[ Sat Jul  6 08:34:54 2024 ] 
Training: Epoch [1/120], Step [3499], Loss: 0.11237458139657974, Training Accuracy: 97.63928571428572
[ Sat Jul  6 08:34:54 2024 ] 	Batch(3500/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 08:35:12 2024 ] 	Batch(3600/7879) done. Loss: 0.0721  lr:0.000010
[ Sat Jul  6 08:35:30 2024 ] 	Batch(3700/7879) done. Loss: 0.0093  lr:0.000010
[ Sat Jul  6 08:35:48 2024 ] 	Batch(3800/7879) done. Loss: 0.0523  lr:0.000010
[ Sat Jul  6 08:36:06 2024 ] 	Batch(3900/7879) done. Loss: 0.0981  lr:0.000010
[ Sat Jul  6 08:36:24 2024 ] 
Training: Epoch [1/120], Step [3999], Loss: 0.030239757150411606, Training Accuracy: 97.615625
[ Sat Jul  6 08:36:24 2024 ] 	Batch(4000/7879) done. Loss: 0.0182  lr:0.000010
[ Sat Jul  6 08:36:42 2024 ] 	Batch(4100/7879) done. Loss: 0.0051  lr:0.000010
[ Sat Jul  6 08:37:00 2024 ] 	Batch(4200/7879) done. Loss: 0.0077  lr:0.000010
[ Sat Jul  6 08:37:18 2024 ] 	Batch(4300/7879) done. Loss: 0.0564  lr:0.000010
[ Sat Jul  6 08:37:36 2024 ] 	Batch(4400/7879) done. Loss: 0.0154  lr:0.000010
[ Sat Jul  6 08:37:54 2024 ] 
Training: Epoch [1/120], Step [4499], Loss: 0.037483904510736465, Training Accuracy: 97.66111111111111
[ Sat Jul  6 08:37:54 2024 ] 	Batch(4500/7879) done. Loss: 0.0773  lr:0.000010
[ Sat Jul  6 08:38:12 2024 ] 	Batch(4600/7879) done. Loss: 0.0059  lr:0.000010
[ Sat Jul  6 08:38:30 2024 ] 	Batch(4700/7879) done. Loss: 0.2740  lr:0.000010
[ Sat Jul  6 08:38:48 2024 ] 	Batch(4800/7879) done. Loss: 0.0611  lr:0.000010
[ Sat Jul  6 08:39:06 2024 ] 	Batch(4900/7879) done. Loss: 0.1067  lr:0.000010
[ Sat Jul  6 08:39:24 2024 ] 
Training: Epoch [1/120], Step [4999], Loss: 0.1459188312292099, Training Accuracy: 97.665
[ Sat Jul  6 08:39:25 2024 ] 	Batch(5000/7879) done. Loss: 0.0067  lr:0.000010
[ Sat Jul  6 08:39:43 2024 ] 	Batch(5100/7879) done. Loss: 0.0022  lr:0.000010
[ Sat Jul  6 08:40:01 2024 ] 	Batch(5200/7879) done. Loss: 0.1004  lr:0.000010
[ Sat Jul  6 08:40:18 2024 ] 	Batch(5300/7879) done. Loss: 0.1626  lr:0.000010
[ Sat Jul  6 08:40:36 2024 ] 	Batch(5400/7879) done. Loss: 0.0131  lr:0.000010
[ Sat Jul  6 08:40:54 2024 ] 
Training: Epoch [1/120], Step [5499], Loss: 0.21099905669689178, Training Accuracy: 97.7
[ Sat Jul  6 08:40:54 2024 ] 	Batch(5500/7879) done. Loss: 0.2296  lr:0.000010
[ Sat Jul  6 08:41:12 2024 ] 	Batch(5600/7879) done. Loss: 0.0023  lr:0.000010
[ Sat Jul  6 08:41:30 2024 ] 	Batch(5700/7879) done. Loss: 0.1237  lr:0.000010
[ Sat Jul  6 08:41:48 2024 ] 	Batch(5800/7879) done. Loss: 0.1047  lr:0.000010
[ Sat Jul  6 08:42:06 2024 ] 	Batch(5900/7879) done. Loss: 0.3789  lr:0.000010
[ Sat Jul  6 08:42:24 2024 ] 
Training: Epoch [1/120], Step [5999], Loss: 0.018800407648086548, Training Accuracy: 97.67708333333334
[ Sat Jul  6 08:42:24 2024 ] 	Batch(6000/7879) done. Loss: 0.0094  lr:0.000010
[ Sat Jul  6 08:42:42 2024 ] 	Batch(6100/7879) done. Loss: 0.0838  lr:0.000010
[ Sat Jul  6 08:43:00 2024 ] 	Batch(6200/7879) done. Loss: 0.0444  lr:0.000010
[ Sat Jul  6 08:43:18 2024 ] 	Batch(6300/7879) done. Loss: 0.0217  lr:0.000010
[ Sat Jul  6 08:43:36 2024 ] 	Batch(6400/7879) done. Loss: 0.1842  lr:0.000010
[ Sat Jul  6 08:43:54 2024 ] 
Training: Epoch [1/120], Step [6499], Loss: 0.09313268959522247, Training Accuracy: 97.68846153846154
[ Sat Jul  6 08:43:54 2024 ] 	Batch(6500/7879) done. Loss: 0.0152  lr:0.000010
[ Sat Jul  6 08:44:12 2024 ] 	Batch(6600/7879) done. Loss: 0.0201  lr:0.000010
[ Sat Jul  6 08:44:30 2024 ] 	Batch(6700/7879) done. Loss: 0.2539  lr:0.000010
[ Sat Jul  6 08:44:48 2024 ] 	Batch(6800/7879) done. Loss: 0.1311  lr:0.000010
[ Sat Jul  6 08:45:06 2024 ] 	Batch(6900/7879) done. Loss: 0.0301  lr:0.000010
[ Sat Jul  6 08:45:25 2024 ] 
Training: Epoch [1/120], Step [6999], Loss: 0.07654843479394913, Training Accuracy: 97.675
[ Sat Jul  6 08:45:25 2024 ] 	Batch(7000/7879) done. Loss: 0.0351  lr:0.000010
[ Sat Jul  6 08:45:43 2024 ] 	Batch(7100/7879) done. Loss: 0.0245  lr:0.000010
[ Sat Jul  6 08:46:02 2024 ] 	Batch(7200/7879) done. Loss: 0.0108  lr:0.000010
[ Sat Jul  6 08:46:21 2024 ] 	Batch(7300/7879) done. Loss: 0.0328  lr:0.000010
[ Sat Jul  6 08:46:39 2024 ] 	Batch(7400/7879) done. Loss: 0.0666  lr:0.000010
[ Sat Jul  6 08:46:58 2024 ] 
Training: Epoch [1/120], Step [7499], Loss: 0.11981607973575592, Training Accuracy: 97.695
[ Sat Jul  6 08:46:58 2024 ] 	Batch(7500/7879) done. Loss: 0.0191  lr:0.000010
[ Sat Jul  6 08:47:17 2024 ] 	Batch(7600/7879) done. Loss: 0.0195  lr:0.000010
[ Sat Jul  6 08:47:35 2024 ] 	Batch(7700/7879) done. Loss: 0.1011  lr:0.000010
[ Sat Jul  6 08:47:54 2024 ] 	Batch(7800/7879) done. Loss: 0.5286  lr:0.000010
[ Sat Jul  6 08:48:08 2024 ] 	Mean training loss: 0.0938.
[ Sat Jul  6 08:48:08 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 08:48:08 2024 ] Training epoch: 3
[ Sat Jul  6 08:48:09 2024 ] 	Batch(0/7879) done. Loss: 0.0054  lr:0.000010
[ Sat Jul  6 08:48:27 2024 ] 	Batch(100/7879) done. Loss: 0.1907  lr:0.000010
[ Sat Jul  6 08:48:45 2024 ] 	Batch(200/7879) done. Loss: 0.0020  lr:0.000010
[ Sat Jul  6 08:49:03 2024 ] 	Batch(300/7879) done. Loss: 0.2075  lr:0.000010
[ Sat Jul  6 08:49:21 2024 ] 	Batch(400/7879) done. Loss: 0.2209  lr:0.000010
[ Sat Jul  6 08:49:38 2024 ] 
Training: Epoch [2/120], Step [499], Loss: 0.0045838807709515095, Training Accuracy: 97.5
[ Sat Jul  6 08:49:39 2024 ] 	Batch(500/7879) done. Loss: 0.0746  lr:0.000010
[ Sat Jul  6 08:49:57 2024 ] 	Batch(600/7879) done. Loss: 0.1270  lr:0.000010
[ Sat Jul  6 08:50:14 2024 ] 	Batch(700/7879) done. Loss: 0.0079  lr:0.000010
[ Sat Jul  6 08:50:32 2024 ] 	Batch(800/7879) done. Loss: 0.0261  lr:0.000010
[ Sat Jul  6 08:50:50 2024 ] 	Batch(900/7879) done. Loss: 0.0107  lr:0.000010
[ Sat Jul  6 08:51:08 2024 ] 
Training: Epoch [2/120], Step [999], Loss: 0.011166252195835114, Training Accuracy: 97.575
[ Sat Jul  6 08:51:08 2024 ] 	Batch(1000/7879) done. Loss: 0.0456  lr:0.000010
[ Sat Jul  6 08:51:26 2024 ] 	Batch(1100/7879) done. Loss: 0.0152  lr:0.000010
[ Sat Jul  6 08:51:44 2024 ] 	Batch(1200/7879) done. Loss: 0.1015  lr:0.000010
[ Sat Jul  6 08:52:02 2024 ] 	Batch(1300/7879) done. Loss: 0.0383  lr:0.000010
[ Sat Jul  6 08:52:20 2024 ] 	Batch(1400/7879) done. Loss: 0.0401  lr:0.000010
[ Sat Jul  6 08:52:38 2024 ] 
Training: Epoch [2/120], Step [1499], Loss: 0.06359902024269104, Training Accuracy: 97.52499999999999
[ Sat Jul  6 08:52:38 2024 ] 	Batch(1500/7879) done. Loss: 0.0042  lr:0.000010
[ Sat Jul  6 08:52:56 2024 ] 	Batch(1600/7879) done. Loss: 0.0007  lr:0.000010
[ Sat Jul  6 08:53:15 2024 ] 	Batch(1700/7879) done. Loss: 0.2003  lr:0.000010
[ Sat Jul  6 08:53:33 2024 ] 	Batch(1800/7879) done. Loss: 0.0275  lr:0.000010
[ Sat Jul  6 08:53:52 2024 ] 	Batch(1900/7879) done. Loss: 0.0374  lr:0.000010
[ Sat Jul  6 08:54:10 2024 ] 
Training: Epoch [2/120], Step [1999], Loss: 0.0511469766497612, Training Accuracy: 97.50625000000001
[ Sat Jul  6 08:54:11 2024 ] 	Batch(2000/7879) done. Loss: 0.0403  lr:0.000010
[ Sat Jul  6 08:54:29 2024 ] 	Batch(2100/7879) done. Loss: 0.0301  lr:0.000010
[ Sat Jul  6 08:54:47 2024 ] 	Batch(2200/7879) done. Loss: 0.0235  lr:0.000010
[ Sat Jul  6 08:55:05 2024 ] 	Batch(2300/7879) done. Loss: 0.0106  lr:0.000010
[ Sat Jul  6 08:55:23 2024 ] 	Batch(2400/7879) done. Loss: 0.0390  lr:0.000010
[ Sat Jul  6 08:55:41 2024 ] 
Training: Epoch [2/120], Step [2499], Loss: 0.16585935652256012, Training Accuracy: 97.495
[ Sat Jul  6 08:55:41 2024 ] 	Batch(2500/7879) done. Loss: 0.0192  lr:0.000010
[ Sat Jul  6 08:56:00 2024 ] 	Batch(2600/7879) done. Loss: 0.0503  lr:0.000010
[ Sat Jul  6 08:56:18 2024 ] 	Batch(2700/7879) done. Loss: 0.0327  lr:0.000010
[ Sat Jul  6 08:56:37 2024 ] 	Batch(2800/7879) done. Loss: 0.0414  lr:0.000010
[ Sat Jul  6 08:56:55 2024 ] 	Batch(2900/7879) done. Loss: 0.0026  lr:0.000010
[ Sat Jul  6 08:57:12 2024 ] 
Training: Epoch [2/120], Step [2999], Loss: 0.06739474833011627, Training Accuracy: 97.5625
[ Sat Jul  6 08:57:12 2024 ] 	Batch(3000/7879) done. Loss: 0.0908  lr:0.000010
[ Sat Jul  6 08:57:30 2024 ] 	Batch(3100/7879) done. Loss: 0.0158  lr:0.000010
[ Sat Jul  6 08:57:48 2024 ] 	Batch(3200/7879) done. Loss: 0.2424  lr:0.000010
[ Sat Jul  6 08:58:06 2024 ] 	Batch(3300/7879) done. Loss: 0.8610  lr:0.000010
[ Sat Jul  6 08:58:24 2024 ] 	Batch(3400/7879) done. Loss: 0.1115  lr:0.000010
[ Sat Jul  6 08:58:42 2024 ] 
Training: Epoch [2/120], Step [3499], Loss: 0.05108802020549774, Training Accuracy: 97.57142857142857
[ Sat Jul  6 08:58:42 2024 ] 	Batch(3500/7879) done. Loss: 0.0259  lr:0.000010
[ Sat Jul  6 08:59:00 2024 ] 	Batch(3600/7879) done. Loss: 0.0496  lr:0.000010
[ Sat Jul  6 08:59:18 2024 ] 	Batch(3700/7879) done. Loss: 0.1392  lr:0.000010
[ Sat Jul  6 08:59:36 2024 ] 	Batch(3800/7879) done. Loss: 0.0117  lr:0.000010
[ Sat Jul  6 08:59:54 2024 ] 	Batch(3900/7879) done. Loss: 0.0719  lr:0.000010
[ Sat Jul  6 09:00:12 2024 ] 
Training: Epoch [2/120], Step [3999], Loss: 0.04286252707242966, Training Accuracy: 97.578125
[ Sat Jul  6 09:00:12 2024 ] 	Batch(4000/7879) done. Loss: 0.0131  lr:0.000010
[ Sat Jul  6 09:00:30 2024 ] 	Batch(4100/7879) done. Loss: 0.1133  lr:0.000010
[ Sat Jul  6 09:00:48 2024 ] 	Batch(4200/7879) done. Loss: 0.0517  lr:0.000010
[ Sat Jul  6 09:01:06 2024 ] 	Batch(4300/7879) done. Loss: 0.1795  lr:0.000010
[ Sat Jul  6 09:01:25 2024 ] 	Batch(4400/7879) done. Loss: 0.0369  lr:0.000010
[ Sat Jul  6 09:01:43 2024 ] 
Training: Epoch [2/120], Step [4499], Loss: 0.032256171107292175, Training Accuracy: 97.62222222222222
[ Sat Jul  6 09:01:43 2024 ] 	Batch(4500/7879) done. Loss: 0.0188  lr:0.000010
[ Sat Jul  6 09:02:01 2024 ] 	Batch(4600/7879) done. Loss: 0.2116  lr:0.000010
[ Sat Jul  6 09:02:19 2024 ] 	Batch(4700/7879) done. Loss: 0.0699  lr:0.000010
[ Sat Jul  6 09:02:37 2024 ] 	Batch(4800/7879) done. Loss: 0.3341  lr:0.000010
[ Sat Jul  6 09:02:55 2024 ] 	Batch(4900/7879) done. Loss: 0.1026  lr:0.000010
[ Sat Jul  6 09:03:13 2024 ] 
Training: Epoch [2/120], Step [4999], Loss: 0.21978367865085602, Training Accuracy: 97.6325
[ Sat Jul  6 09:03:14 2024 ] 	Batch(5000/7879) done. Loss: 0.1381  lr:0.000010
[ Sat Jul  6 09:03:32 2024 ] 	Batch(5100/7879) done. Loss: 0.1606  lr:0.000010
[ Sat Jul  6 09:03:50 2024 ] 	Batch(5200/7879) done. Loss: 0.1046  lr:0.000010
[ Sat Jul  6 09:04:09 2024 ] 	Batch(5300/7879) done. Loss: 0.1795  lr:0.000010
[ Sat Jul  6 09:04:27 2024 ] 	Batch(5400/7879) done. Loss: 0.0386  lr:0.000010
[ Sat Jul  6 09:04:45 2024 ] 
Training: Epoch [2/120], Step [5499], Loss: 0.06525851786136627, Training Accuracy: 97.625
[ Sat Jul  6 09:04:45 2024 ] 	Batch(5500/7879) done. Loss: 0.0361  lr:0.000010
[ Sat Jul  6 09:05:04 2024 ] 	Batch(5600/7879) done. Loss: 0.1423  lr:0.000010
[ Sat Jul  6 09:05:22 2024 ] 	Batch(5700/7879) done. Loss: 0.1501  lr:0.000010
[ Sat Jul  6 09:05:41 2024 ] 	Batch(5800/7879) done. Loss: 0.2306  lr:0.000010
[ Sat Jul  6 09:05:59 2024 ] 	Batch(5900/7879) done. Loss: 0.3003  lr:0.000010
[ Sat Jul  6 09:06:17 2024 ] 
Training: Epoch [2/120], Step [5999], Loss: 0.006049443036317825, Training Accuracy: 97.67916666666666
[ Sat Jul  6 09:06:17 2024 ] 	Batch(6000/7879) done. Loss: 0.1839  lr:0.000010
[ Sat Jul  6 09:06:35 2024 ] 	Batch(6100/7879) done. Loss: 0.0183  lr:0.000010
[ Sat Jul  6 09:06:53 2024 ] 	Batch(6200/7879) done. Loss: 0.0533  lr:0.000010
[ Sat Jul  6 09:07:11 2024 ] 	Batch(6300/7879) done. Loss: 0.0825  lr:0.000010
[ Sat Jul  6 09:07:29 2024 ] 	Batch(6400/7879) done. Loss: 0.0652  lr:0.000010
[ Sat Jul  6 09:07:47 2024 ] 
Training: Epoch [2/120], Step [6499], Loss: 0.1763288378715515, Training Accuracy: 97.7
[ Sat Jul  6 09:07:47 2024 ] 	Batch(6500/7879) done. Loss: 0.0658  lr:0.000010
[ Sat Jul  6 09:08:05 2024 ] 	Batch(6600/7879) done. Loss: 0.0082  lr:0.000010
[ Sat Jul  6 09:08:23 2024 ] 	Batch(6700/7879) done. Loss: 0.0136  lr:0.000010
[ Sat Jul  6 09:08:41 2024 ] 	Batch(6800/7879) done. Loss: 0.2551  lr:0.000010
[ Sat Jul  6 09:08:59 2024 ] 	Batch(6900/7879) done. Loss: 0.0042  lr:0.000010
[ Sat Jul  6 09:09:17 2024 ] 
Training: Epoch [2/120], Step [6999], Loss: 0.1729854792356491, Training Accuracy: 97.66428571428571
[ Sat Jul  6 09:09:17 2024 ] 	Batch(7000/7879) done. Loss: 0.5829  lr:0.000010
[ Sat Jul  6 09:09:35 2024 ] 	Batch(7100/7879) done. Loss: 0.1321  lr:0.000010
[ Sat Jul  6 09:09:53 2024 ] 	Batch(7200/7879) done. Loss: 0.5058  lr:0.000010
[ Sat Jul  6 09:10:11 2024 ] 	Batch(7300/7879) done. Loss: 0.0418  lr:0.000010
[ Sat Jul  6 09:10:29 2024 ] 	Batch(7400/7879) done. Loss: 0.3250  lr:0.000010
[ Sat Jul  6 09:10:47 2024 ] 
Training: Epoch [2/120], Step [7499], Loss: 0.005840254947543144, Training Accuracy: 97.64833333333334
[ Sat Jul  6 09:10:47 2024 ] 	Batch(7500/7879) done. Loss: 0.0975  lr:0.000010
[ Sat Jul  6 09:11:05 2024 ] 	Batch(7600/7879) done. Loss: 0.1354  lr:0.000010
[ Sat Jul  6 09:11:24 2024 ] 	Batch(7700/7879) done. Loss: 0.0290  lr:0.000010
[ Sat Jul  6 09:11:42 2024 ] 	Batch(7800/7879) done. Loss: 0.0533  lr:0.000010
[ Sat Jul  6 09:11:57 2024 ] 	Mean training loss: 0.0931.
[ Sat Jul  6 09:11:57 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 09:11:57 2024 ] Training epoch: 4
[ Sat Jul  6 09:11:57 2024 ] 	Batch(0/7879) done. Loss: 0.0066  lr:0.000010
[ Sat Jul  6 09:12:15 2024 ] 	Batch(100/7879) done. Loss: 0.0097  lr:0.000010
[ Sat Jul  6 09:12:33 2024 ] 	Batch(200/7879) done. Loss: 0.0326  lr:0.000010
[ Sat Jul  6 09:12:51 2024 ] 	Batch(300/7879) done. Loss: 0.0076  lr:0.000010
[ Sat Jul  6 09:13:09 2024 ] 	Batch(400/7879) done. Loss: 0.0863  lr:0.000010
[ Sat Jul  6 09:13:27 2024 ] 
Training: Epoch [3/120], Step [499], Loss: 0.005924978293478489, Training Accuracy: 97.475
[ Sat Jul  6 09:13:27 2024 ] 	Batch(500/7879) done. Loss: 0.0423  lr:0.000010
[ Sat Jul  6 09:13:45 2024 ] 	Batch(600/7879) done. Loss: 0.0183  lr:0.000010
[ Sat Jul  6 09:14:03 2024 ] 	Batch(700/7879) done. Loss: 0.0448  lr:0.000010
[ Sat Jul  6 09:14:22 2024 ] 	Batch(800/7879) done. Loss: 0.0196  lr:0.000010
[ Sat Jul  6 09:14:40 2024 ] 	Batch(900/7879) done. Loss: 0.3215  lr:0.000010
[ Sat Jul  6 09:14:58 2024 ] 
Training: Epoch [3/120], Step [999], Loss: 0.04475075751543045, Training Accuracy: 97.7625
[ Sat Jul  6 09:14:58 2024 ] 	Batch(1000/7879) done. Loss: 0.1526  lr:0.000010
[ Sat Jul  6 09:15:16 2024 ] 	Batch(1100/7879) done. Loss: 0.0048  lr:0.000010
[ Sat Jul  6 09:15:34 2024 ] 	Batch(1200/7879) done. Loss: 0.0456  lr:0.000010
[ Sat Jul  6 09:15:52 2024 ] 	Batch(1300/7879) done. Loss: 0.0013  lr:0.000010
[ Sat Jul  6 09:16:10 2024 ] 	Batch(1400/7879) done. Loss: 0.0424  lr:0.000010
[ Sat Jul  6 09:16:28 2024 ] 
Training: Epoch [3/120], Step [1499], Loss: 0.027215493842959404, Training Accuracy: 97.81666666666666
[ Sat Jul  6 09:16:28 2024 ] 	Batch(1500/7879) done. Loss: 0.0796  lr:0.000010
[ Sat Jul  6 09:16:46 2024 ] 	Batch(1600/7879) done. Loss: 0.0524  lr:0.000010
[ Sat Jul  6 09:17:05 2024 ] 	Batch(1700/7879) done. Loss: 0.0885  lr:0.000010
[ Sat Jul  6 09:17:23 2024 ] 	Batch(1800/7879) done. Loss: 0.0500  lr:0.000010
[ Sat Jul  6 09:17:41 2024 ] 	Batch(1900/7879) done. Loss: 0.0063  lr:0.000010
[ Sat Jul  6 09:17:59 2024 ] 
Training: Epoch [3/120], Step [1999], Loss: 0.18334665894508362, Training Accuracy: 97.84375
[ Sat Jul  6 09:17:59 2024 ] 	Batch(2000/7879) done. Loss: 0.0152  lr:0.000010
[ Sat Jul  6 09:18:17 2024 ] 	Batch(2100/7879) done. Loss: 0.0447  lr:0.000010
[ Sat Jul  6 09:18:35 2024 ] 	Batch(2200/7879) done. Loss: 0.0297  lr:0.000010
[ Sat Jul  6 09:18:53 2024 ] 	Batch(2300/7879) done. Loss: 0.3999  lr:0.000010
[ Sat Jul  6 09:19:11 2024 ] 	Batch(2400/7879) done. Loss: 0.0098  lr:0.000010
[ Sat Jul  6 09:19:29 2024 ] 
Training: Epoch [3/120], Step [2499], Loss: 0.02513197436928749, Training Accuracy: 97.775
[ Sat Jul  6 09:19:29 2024 ] 	Batch(2500/7879) done. Loss: 0.2111  lr:0.000010
[ Sat Jul  6 09:19:47 2024 ] 	Batch(2600/7879) done. Loss: 0.0851  lr:0.000010
[ Sat Jul  6 09:20:05 2024 ] 	Batch(2700/7879) done. Loss: 0.0196  lr:0.000010
[ Sat Jul  6 09:20:23 2024 ] 	Batch(2800/7879) done. Loss: 0.0493  lr:0.000010
[ Sat Jul  6 09:20:41 2024 ] 	Batch(2900/7879) done. Loss: 0.0853  lr:0.000010
[ Sat Jul  6 09:20:59 2024 ] 
Training: Epoch [3/120], Step [2999], Loss: 0.06662260740995407, Training Accuracy: 97.77916666666667
[ Sat Jul  6 09:20:59 2024 ] 	Batch(3000/7879) done. Loss: 0.2724  lr:0.000010
[ Sat Jul  6 09:21:17 2024 ] 	Batch(3100/7879) done. Loss: 0.1758  lr:0.000010
[ Sat Jul  6 09:21:35 2024 ] 	Batch(3200/7879) done. Loss: 0.1330  lr:0.000010
[ Sat Jul  6 09:21:54 2024 ] 	Batch(3300/7879) done. Loss: 0.5987  lr:0.000010
[ Sat Jul  6 09:22:12 2024 ] 	Batch(3400/7879) done. Loss: 0.1064  lr:0.000010
[ Sat Jul  6 09:22:30 2024 ] 
Training: Epoch [3/120], Step [3499], Loss: 0.23917531967163086, Training Accuracy: 97.68571428571428
[ Sat Jul  6 09:22:30 2024 ] 	Batch(3500/7879) done. Loss: 0.1913  lr:0.000010
[ Sat Jul  6 09:22:48 2024 ] 	Batch(3600/7879) done. Loss: 0.0083  lr:0.000010
[ Sat Jul  6 09:23:06 2024 ] 	Batch(3700/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 09:23:24 2024 ] 	Batch(3800/7879) done. Loss: 0.2222  lr:0.000010
[ Sat Jul  6 09:23:42 2024 ] 	Batch(3900/7879) done. Loss: 0.1685  lr:0.000010
[ Sat Jul  6 09:24:00 2024 ] 
Training: Epoch [3/120], Step [3999], Loss: 0.1835116595029831, Training Accuracy: 97.6375
[ Sat Jul  6 09:24:00 2024 ] 	Batch(4000/7879) done. Loss: 0.0704  lr:0.000010
[ Sat Jul  6 09:24:18 2024 ] 	Batch(4100/7879) done. Loss: 0.0309  lr:0.000010
[ Sat Jul  6 09:24:36 2024 ] 	Batch(4200/7879) done. Loss: 0.0495  lr:0.000010
[ Sat Jul  6 09:24:54 2024 ] 	Batch(4300/7879) done. Loss: 0.1664  lr:0.000010
[ Sat Jul  6 09:25:12 2024 ] 	Batch(4400/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 09:25:30 2024 ] 
Training: Epoch [3/120], Step [4499], Loss: 0.003336101770401001, Training Accuracy: 97.675
[ Sat Jul  6 09:25:30 2024 ] 	Batch(4500/7879) done. Loss: 0.0323  lr:0.000010
[ Sat Jul  6 09:25:48 2024 ] 	Batch(4600/7879) done. Loss: 0.0133  lr:0.000010
[ Sat Jul  6 09:26:06 2024 ] 	Batch(4700/7879) done. Loss: 0.0663  lr:0.000010
[ Sat Jul  6 09:26:24 2024 ] 	Batch(4800/7879) done. Loss: 0.0106  lr:0.000010
[ Sat Jul  6 09:26:42 2024 ] 	Batch(4900/7879) done. Loss: 0.2710  lr:0.000010
[ Sat Jul  6 09:27:00 2024 ] 
Training: Epoch [3/120], Step [4999], Loss: 0.22712747752666473, Training Accuracy: 97.67
[ Sat Jul  6 09:27:00 2024 ] 	Batch(5000/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 09:27:18 2024 ] 	Batch(5100/7879) done. Loss: 0.0139  lr:0.000010
[ Sat Jul  6 09:27:36 2024 ] 	Batch(5200/7879) done. Loss: 0.0296  lr:0.000010
[ Sat Jul  6 09:27:54 2024 ] 	Batch(5300/7879) done. Loss: 0.0589  lr:0.000010
[ Sat Jul  6 09:28:12 2024 ] 	Batch(5400/7879) done. Loss: 0.1072  lr:0.000010
[ Sat Jul  6 09:28:30 2024 ] 
Training: Epoch [3/120], Step [5499], Loss: 0.3083994388580322, Training Accuracy: 97.6590909090909
[ Sat Jul  6 09:28:30 2024 ] 	Batch(5500/7879) done. Loss: 0.2002  lr:0.000010
[ Sat Jul  6 09:28:48 2024 ] 	Batch(5600/7879) done. Loss: 0.0029  lr:0.000010
[ Sat Jul  6 09:29:06 2024 ] 	Batch(5700/7879) done. Loss: 0.1638  lr:0.000010
[ Sat Jul  6 09:29:24 2024 ] 	Batch(5800/7879) done. Loss: 0.0093  lr:0.000010
[ Sat Jul  6 09:29:42 2024 ] 	Batch(5900/7879) done. Loss: 0.0863  lr:0.000010
[ Sat Jul  6 09:30:00 2024 ] 
Training: Epoch [3/120], Step [5999], Loss: 0.12103188782930374, Training Accuracy: 97.66875
[ Sat Jul  6 09:30:00 2024 ] 	Batch(6000/7879) done. Loss: 0.1736  lr:0.000010
[ Sat Jul  6 09:30:18 2024 ] 	Batch(6100/7879) done. Loss: 0.0430  lr:0.000010
[ Sat Jul  6 09:30:36 2024 ] 	Batch(6200/7879) done. Loss: 0.0403  lr:0.000010
[ Sat Jul  6 09:30:54 2024 ] 	Batch(6300/7879) done. Loss: 0.0109  lr:0.000010
[ Sat Jul  6 09:31:12 2024 ] 	Batch(6400/7879) done. Loss: 0.0241  lr:0.000010
[ Sat Jul  6 09:31:30 2024 ] 
Training: Epoch [3/120], Step [6499], Loss: 0.04110674560070038, Training Accuracy: 97.68846153846154
[ Sat Jul  6 09:31:30 2024 ] 	Batch(6500/7879) done. Loss: 0.2852  lr:0.000010
[ Sat Jul  6 09:31:48 2024 ] 	Batch(6600/7879) done. Loss: 0.0378  lr:0.000010
[ Sat Jul  6 09:32:06 2024 ] 	Batch(6700/7879) done. Loss: 0.0034  lr:0.000010
[ Sat Jul  6 09:32:24 2024 ] 	Batch(6800/7879) done. Loss: 0.4896  lr:0.000010
[ Sat Jul  6 09:32:42 2024 ] 	Batch(6900/7879) done. Loss: 0.0393  lr:0.000010
[ Sat Jul  6 09:33:00 2024 ] 
Training: Epoch [3/120], Step [6999], Loss: 0.23930540680885315, Training Accuracy: 97.70535714285714
[ Sat Jul  6 09:33:00 2024 ] 	Batch(7000/7879) done. Loss: 0.0608  lr:0.000010
[ Sat Jul  6 09:33:18 2024 ] 	Batch(7100/7879) done. Loss: 0.0798  lr:0.000010
[ Sat Jul  6 09:33:36 2024 ] 	Batch(7200/7879) done. Loss: 0.0458  lr:0.000010
[ Sat Jul  6 09:33:55 2024 ] 	Batch(7300/7879) done. Loss: 0.0193  lr:0.000010
[ Sat Jul  6 09:34:13 2024 ] 	Batch(7400/7879) done. Loss: 0.0783  lr:0.000010
[ Sat Jul  6 09:34:31 2024 ] 
Training: Epoch [3/120], Step [7499], Loss: 0.008051548153162003, Training Accuracy: 97.69
[ Sat Jul  6 09:34:31 2024 ] 	Batch(7500/7879) done. Loss: 0.0407  lr:0.000010
[ Sat Jul  6 09:34:49 2024 ] 	Batch(7600/7879) done. Loss: 0.4138  lr:0.000010
[ Sat Jul  6 09:35:07 2024 ] 	Batch(7700/7879) done. Loss: 0.2095  lr:0.000010
[ Sat Jul  6 09:35:25 2024 ] 	Batch(7800/7879) done. Loss: 0.0031  lr:0.000010
[ Sat Jul  6 09:35:39 2024 ] 	Mean training loss: 0.0918.
[ Sat Jul  6 09:35:39 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 09:35:39 2024 ] Training epoch: 5
[ Sat Jul  6 09:35:40 2024 ] 	Batch(0/7879) done. Loss: 0.0146  lr:0.000010
[ Sat Jul  6 09:35:58 2024 ] 	Batch(100/7879) done. Loss: 0.0107  lr:0.000010
[ Sat Jul  6 09:36:16 2024 ] 	Batch(200/7879) done. Loss: 0.1130  lr:0.000010
[ Sat Jul  6 09:36:34 2024 ] 	Batch(300/7879) done. Loss: 0.0737  lr:0.000010
[ Sat Jul  6 09:36:52 2024 ] 	Batch(400/7879) done. Loss: 0.0116  lr:0.000010
[ Sat Jul  6 09:37:10 2024 ] 
Training: Epoch [4/120], Step [499], Loss: 0.5507930517196655, Training Accuracy: 97.775
[ Sat Jul  6 09:37:10 2024 ] 	Batch(500/7879) done. Loss: 0.0470  lr:0.000010
[ Sat Jul  6 09:37:28 2024 ] 	Batch(600/7879) done. Loss: 0.0760  lr:0.000010
[ Sat Jul  6 09:37:46 2024 ] 	Batch(700/7879) done. Loss: 0.0488  lr:0.000010
[ Sat Jul  6 09:38:04 2024 ] 	Batch(800/7879) done. Loss: 0.0036  lr:0.000010
[ Sat Jul  6 09:38:22 2024 ] 	Batch(900/7879) done. Loss: 0.0935  lr:0.000010
[ Sat Jul  6 09:38:41 2024 ] 
Training: Epoch [4/120], Step [999], Loss: 0.022169340401887894, Training Accuracy: 97.95
[ Sat Jul  6 09:38:41 2024 ] 	Batch(1000/7879) done. Loss: 0.0349  lr:0.000010
[ Sat Jul  6 09:39:00 2024 ] 	Batch(1100/7879) done. Loss: 0.0561  lr:0.000010
[ Sat Jul  6 09:39:18 2024 ] 	Batch(1200/7879) done. Loss: 0.0762  lr:0.000010
[ Sat Jul  6 09:39:36 2024 ] 	Batch(1300/7879) done. Loss: 0.2215  lr:0.000010
[ Sat Jul  6 09:39:54 2024 ] 	Batch(1400/7879) done. Loss: 0.1864  lr:0.000010
[ Sat Jul  6 09:40:12 2024 ] 
Training: Epoch [4/120], Step [1499], Loss: 0.027553528547286987, Training Accuracy: 97.80833333333334
[ Sat Jul  6 09:40:12 2024 ] 	Batch(1500/7879) done. Loss: 0.0076  lr:0.000010
[ Sat Jul  6 09:40:30 2024 ] 	Batch(1600/7879) done. Loss: 0.0176  lr:0.000010
[ Sat Jul  6 09:40:48 2024 ] 	Batch(1700/7879) done. Loss: 0.2224  lr:0.000010
[ Sat Jul  6 09:41:06 2024 ] 	Batch(1800/7879) done. Loss: 0.3187  lr:0.000010
[ Sat Jul  6 09:41:24 2024 ] 	Batch(1900/7879) done. Loss: 0.0642  lr:0.000010
[ Sat Jul  6 09:41:42 2024 ] 
Training: Epoch [4/120], Step [1999], Loss: 0.22127565741539001, Training Accuracy: 97.8375
[ Sat Jul  6 09:41:42 2024 ] 	Batch(2000/7879) done. Loss: 0.1711  lr:0.000010
[ Sat Jul  6 09:42:00 2024 ] 	Batch(2100/7879) done. Loss: 0.0140  lr:0.000010
[ Sat Jul  6 09:42:18 2024 ] 	Batch(2200/7879) done. Loss: 0.1319  lr:0.000010
[ Sat Jul  6 09:42:36 2024 ] 	Batch(2300/7879) done. Loss: 0.0574  lr:0.000010
[ Sat Jul  6 09:42:54 2024 ] 	Batch(2400/7879) done. Loss: 0.1657  lr:0.000010
[ Sat Jul  6 09:43:12 2024 ] 
Training: Epoch [4/120], Step [2499], Loss: 0.008128118701279163, Training Accuracy: 97.735
[ Sat Jul  6 09:43:12 2024 ] 	Batch(2500/7879) done. Loss: 0.0548  lr:0.000010
[ Sat Jul  6 09:43:30 2024 ] 	Batch(2600/7879) done. Loss: 0.1058  lr:0.000010
[ Sat Jul  6 09:43:48 2024 ] 	Batch(2700/7879) done. Loss: 0.0445  lr:0.000010
[ Sat Jul  6 09:44:06 2024 ] 	Batch(2800/7879) done. Loss: 0.0049  lr:0.000010
[ Sat Jul  6 09:44:24 2024 ] 	Batch(2900/7879) done. Loss: 0.0402  lr:0.000010
[ Sat Jul  6 09:44:42 2024 ] 
Training: Epoch [4/120], Step [2999], Loss: 0.01350895781069994, Training Accuracy: 97.7375
[ Sat Jul  6 09:44:42 2024 ] 	Batch(3000/7879) done. Loss: 0.1277  lr:0.000010
[ Sat Jul  6 09:45:00 2024 ] 	Batch(3100/7879) done. Loss: 0.1229  lr:0.000010
[ Sat Jul  6 09:45:18 2024 ] 	Batch(3200/7879) done. Loss: 0.0021  lr:0.000010
[ Sat Jul  6 09:45:37 2024 ] 	Batch(3300/7879) done. Loss: 0.0717  lr:0.000010
[ Sat Jul  6 09:45:55 2024 ] 	Batch(3400/7879) done. Loss: 0.1945  lr:0.000010
[ Sat Jul  6 09:46:14 2024 ] 
Training: Epoch [4/120], Step [3499], Loss: 0.007040871772915125, Training Accuracy: 97.73214285714286
[ Sat Jul  6 09:46:14 2024 ] 	Batch(3500/7879) done. Loss: 0.1835  lr:0.000010
[ Sat Jul  6 09:46:33 2024 ] 	Batch(3600/7879) done. Loss: 0.1642  lr:0.000010
[ Sat Jul  6 09:46:51 2024 ] 	Batch(3700/7879) done. Loss: 0.0310  lr:0.000010
[ Sat Jul  6 09:47:09 2024 ] 	Batch(3800/7879) done. Loss: 0.0600  lr:0.000010
[ Sat Jul  6 09:47:27 2024 ] 	Batch(3900/7879) done. Loss: 0.0222  lr:0.000010
[ Sat Jul  6 09:47:44 2024 ] 
Training: Epoch [4/120], Step [3999], Loss: 0.3406575322151184, Training Accuracy: 97.778125
[ Sat Jul  6 09:47:45 2024 ] 	Batch(4000/7879) done. Loss: 0.0344  lr:0.000010
[ Sat Jul  6 09:48:03 2024 ] 	Batch(4100/7879) done. Loss: 0.0267  lr:0.000010
[ Sat Jul  6 09:48:22 2024 ] 	Batch(4200/7879) done. Loss: 0.0538  lr:0.000010
[ Sat Jul  6 09:48:40 2024 ] 	Batch(4300/7879) done. Loss: 0.0122  lr:0.000010
[ Sat Jul  6 09:48:59 2024 ] 	Batch(4400/7879) done. Loss: 0.0139  lr:0.000010
[ Sat Jul  6 09:49:17 2024 ] 
Training: Epoch [4/120], Step [4499], Loss: 0.06758616864681244, Training Accuracy: 97.80555555555556
[ Sat Jul  6 09:49:17 2024 ] 	Batch(4500/7879) done. Loss: 0.0272  lr:0.000010
[ Sat Jul  6 09:49:35 2024 ] 	Batch(4600/7879) done. Loss: 0.0085  lr:0.000010
[ Sat Jul  6 09:49:53 2024 ] 	Batch(4700/7879) done. Loss: 0.1834  lr:0.000010
[ Sat Jul  6 09:50:11 2024 ] 	Batch(4800/7879) done. Loss: 0.4291  lr:0.000010
[ Sat Jul  6 09:50:30 2024 ] 	Batch(4900/7879) done. Loss: 0.0177  lr:0.000010
[ Sat Jul  6 09:50:47 2024 ] 
Training: Epoch [4/120], Step [4999], Loss: 0.027482520788908005, Training Accuracy: 97.78999999999999
[ Sat Jul  6 09:50:48 2024 ] 	Batch(5000/7879) done. Loss: 0.0801  lr:0.000010
[ Sat Jul  6 09:51:06 2024 ] 	Batch(5100/7879) done. Loss: 0.2545  lr:0.000010
[ Sat Jul  6 09:51:24 2024 ] 	Batch(5200/7879) done. Loss: 0.0059  lr:0.000010
[ Sat Jul  6 09:51:42 2024 ] 	Batch(5300/7879) done. Loss: 0.0055  lr:0.000010
[ Sat Jul  6 09:52:01 2024 ] 	Batch(5400/7879) done. Loss: 0.2411  lr:0.000010
[ Sat Jul  6 09:52:19 2024 ] 
Training: Epoch [4/120], Step [5499], Loss: 0.008927363902330399, Training Accuracy: 97.7840909090909
[ Sat Jul  6 09:52:19 2024 ] 	Batch(5500/7879) done. Loss: 0.0058  lr:0.000010
[ Sat Jul  6 09:52:38 2024 ] 	Batch(5600/7879) done. Loss: 0.0337  lr:0.000010
[ Sat Jul  6 09:52:56 2024 ] 	Batch(5700/7879) done. Loss: 0.0757  lr:0.000010
[ Sat Jul  6 09:53:14 2024 ] 	Batch(5800/7879) done. Loss: 1.0035  lr:0.000010
[ Sat Jul  6 09:53:32 2024 ] 	Batch(5900/7879) done. Loss: 0.1449  lr:0.000010
[ Sat Jul  6 09:53:50 2024 ] 
Training: Epoch [4/120], Step [5999], Loss: 0.11005282402038574, Training Accuracy: 97.78541666666666
[ Sat Jul  6 09:53:50 2024 ] 	Batch(6000/7879) done. Loss: 0.3544  lr:0.000010
[ Sat Jul  6 09:54:08 2024 ] 	Batch(6100/7879) done. Loss: 0.1114  lr:0.000010
[ Sat Jul  6 09:54:26 2024 ] 	Batch(6200/7879) done. Loss: 0.0715  lr:0.000010
[ Sat Jul  6 09:54:44 2024 ] 	Batch(6300/7879) done. Loss: 0.0473  lr:0.000010
[ Sat Jul  6 09:55:02 2024 ] 	Batch(6400/7879) done. Loss: 0.0357  lr:0.000010
[ Sat Jul  6 09:55:20 2024 ] 
Training: Epoch [4/120], Step [6499], Loss: 0.14720362424850464, Training Accuracy: 97.79038461538462
[ Sat Jul  6 09:55:20 2024 ] 	Batch(6500/7879) done. Loss: 0.1565  lr:0.000010
[ Sat Jul  6 09:55:38 2024 ] 	Batch(6600/7879) done. Loss: 0.0315  lr:0.000010
[ Sat Jul  6 09:55:56 2024 ] 	Batch(6700/7879) done. Loss: 0.0095  lr:0.000010
[ Sat Jul  6 09:56:14 2024 ] 	Batch(6800/7879) done. Loss: 0.5160  lr:0.000010
[ Sat Jul  6 09:56:32 2024 ] 	Batch(6900/7879) done. Loss: 0.0547  lr:0.000010
[ Sat Jul  6 09:56:50 2024 ] 
Training: Epoch [4/120], Step [6999], Loss: 0.05198460817337036, Training Accuracy: 97.78750000000001
[ Sat Jul  6 09:56:50 2024 ] 	Batch(7000/7879) done. Loss: 0.0068  lr:0.000010
[ Sat Jul  6 09:57:08 2024 ] 	Batch(7100/7879) done. Loss: 0.0122  lr:0.000010
[ Sat Jul  6 09:57:26 2024 ] 	Batch(7200/7879) done. Loss: 0.0087  lr:0.000010
[ Sat Jul  6 09:57:44 2024 ] 	Batch(7300/7879) done. Loss: 0.2680  lr:0.000010
[ Sat Jul  6 09:58:02 2024 ] 	Batch(7400/7879) done. Loss: 0.0731  lr:0.000010
[ Sat Jul  6 09:58:20 2024 ] 
Training: Epoch [4/120], Step [7499], Loss: 0.31981396675109863, Training Accuracy: 97.77833333333334
[ Sat Jul  6 09:58:20 2024 ] 	Batch(7500/7879) done. Loss: 0.0164  lr:0.000010
[ Sat Jul  6 09:58:38 2024 ] 	Batch(7600/7879) done. Loss: 0.5108  lr:0.000010
[ Sat Jul  6 09:58:56 2024 ] 	Batch(7700/7879) done. Loss: 0.0222  lr:0.000010
[ Sat Jul  6 09:59:15 2024 ] 	Batch(7800/7879) done. Loss: 0.0052  lr:0.000010
[ Sat Jul  6 09:59:30 2024 ] 	Mean training loss: 0.0901.
[ Sat Jul  6 09:59:30 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 09:59:30 2024 ] Training epoch: 6
[ Sat Jul  6 09:59:30 2024 ] 	Batch(0/7879) done. Loss: 0.0863  lr:0.000010
[ Sat Jul  6 09:59:48 2024 ] 	Batch(100/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 10:00:06 2024 ] 	Batch(200/7879) done. Loss: 0.0829  lr:0.000010
[ Sat Jul  6 10:00:24 2024 ] 	Batch(300/7879) done. Loss: 0.0312  lr:0.000010
[ Sat Jul  6 10:00:42 2024 ] 	Batch(400/7879) done. Loss: 0.1912  lr:0.000010
[ Sat Jul  6 10:01:00 2024 ] 
Training: Epoch [5/120], Step [499], Loss: 0.4049142897129059, Training Accuracy: 98.225
[ Sat Jul  6 10:01:00 2024 ] 	Batch(500/7879) done. Loss: 0.0504  lr:0.000010
[ Sat Jul  6 10:01:18 2024 ] 	Batch(600/7879) done. Loss: 0.0976  lr:0.000010
[ Sat Jul  6 10:01:36 2024 ] 	Batch(700/7879) done. Loss: 0.2273  lr:0.000010
[ Sat Jul  6 10:01:54 2024 ] 	Batch(800/7879) done. Loss: 0.0078  lr:0.000010
[ Sat Jul  6 10:02:12 2024 ] 	Batch(900/7879) done. Loss: 0.0048  lr:0.000010
[ Sat Jul  6 10:02:30 2024 ] 
Training: Epoch [5/120], Step [999], Loss: 0.21431750059127808, Training Accuracy: 97.8875
[ Sat Jul  6 10:02:30 2024 ] 	Batch(1000/7879) done. Loss: 0.0101  lr:0.000010
[ Sat Jul  6 10:02:48 2024 ] 	Batch(1100/7879) done. Loss: 0.1708  lr:0.000010
[ Sat Jul  6 10:03:06 2024 ] 	Batch(1200/7879) done. Loss: 0.0785  lr:0.000010
[ Sat Jul  6 10:03:24 2024 ] 	Batch(1300/7879) done. Loss: 0.0223  lr:0.000010
[ Sat Jul  6 10:03:42 2024 ] 	Batch(1400/7879) done. Loss: 0.1305  lr:0.000010
[ Sat Jul  6 10:03:59 2024 ] 
Training: Epoch [5/120], Step [1499], Loss: 0.04478643089532852, Training Accuracy: 97.84166666666667
[ Sat Jul  6 10:04:00 2024 ] 	Batch(1500/7879) done. Loss: 0.0172  lr:0.000010
[ Sat Jul  6 10:04:18 2024 ] 	Batch(1600/7879) done. Loss: 0.0052  lr:0.000010
[ Sat Jul  6 10:04:36 2024 ] 	Batch(1700/7879) done. Loss: 0.1503  lr:0.000010
[ Sat Jul  6 10:04:53 2024 ] 	Batch(1800/7879) done. Loss: 0.0023  lr:0.000010
[ Sat Jul  6 10:05:11 2024 ] 	Batch(1900/7879) done. Loss: 0.0082  lr:0.000010
[ Sat Jul  6 10:05:29 2024 ] 
Training: Epoch [5/120], Step [1999], Loss: 0.009767865762114525, Training Accuracy: 97.775
[ Sat Jul  6 10:05:29 2024 ] 	Batch(2000/7879) done. Loss: 0.1723  lr:0.000010
[ Sat Jul  6 10:05:47 2024 ] 	Batch(2100/7879) done. Loss: 0.0090  lr:0.000010
[ Sat Jul  6 10:06:05 2024 ] 	Batch(2200/7879) done. Loss: 0.1913  lr:0.000010
[ Sat Jul  6 10:06:23 2024 ] 	Batch(2300/7879) done. Loss: 0.1180  lr:0.000010
[ Sat Jul  6 10:06:41 2024 ] 	Batch(2400/7879) done. Loss: 0.0132  lr:0.000010
[ Sat Jul  6 10:06:59 2024 ] 
Training: Epoch [5/120], Step [2499], Loss: 0.09737780690193176, Training Accuracy: 97.815
[ Sat Jul  6 10:07:00 2024 ] 	Batch(2500/7879) done. Loss: 0.1432  lr:0.000010
[ Sat Jul  6 10:07:18 2024 ] 	Batch(2600/7879) done. Loss: 0.3270  lr:0.000010
[ Sat Jul  6 10:07:36 2024 ] 	Batch(2700/7879) done. Loss: 0.3252  lr:0.000010
[ Sat Jul  6 10:07:54 2024 ] 	Batch(2800/7879) done. Loss: 0.1556  lr:0.000010
[ Sat Jul  6 10:08:11 2024 ] 	Batch(2900/7879) done. Loss: 0.0415  lr:0.000010
[ Sat Jul  6 10:08:29 2024 ] 
Training: Epoch [5/120], Step [2999], Loss: 0.037951499223709106, Training Accuracy: 97.75416666666666
[ Sat Jul  6 10:08:29 2024 ] 	Batch(3000/7879) done. Loss: 0.0221  lr:0.000010
[ Sat Jul  6 10:08:47 2024 ] 	Batch(3100/7879) done. Loss: 0.1736  lr:0.000010
[ Sat Jul  6 10:09:05 2024 ] 	Batch(3200/7879) done. Loss: 0.0853  lr:0.000010
[ Sat Jul  6 10:09:24 2024 ] 	Batch(3300/7879) done. Loss: 0.0696  lr:0.000010
[ Sat Jul  6 10:09:43 2024 ] 	Batch(3400/7879) done. Loss: 0.0318  lr:0.000010
[ Sat Jul  6 10:10:01 2024 ] 
Training: Epoch [5/120], Step [3499], Loss: 0.2115098387002945, Training Accuracy: 97.76071428571429
[ Sat Jul  6 10:10:01 2024 ] 	Batch(3500/7879) done. Loss: 0.0044  lr:0.000010
[ Sat Jul  6 10:10:20 2024 ] 	Batch(3600/7879) done. Loss: 0.0019  lr:0.000010
[ Sat Jul  6 10:10:38 2024 ] 	Batch(3700/7879) done. Loss: 0.2270  lr:0.000010
[ Sat Jul  6 10:10:56 2024 ] 	Batch(3800/7879) done. Loss: 0.0767  lr:0.000010
[ Sat Jul  6 10:11:13 2024 ] 	Batch(3900/7879) done. Loss: 0.0358  lr:0.000010
[ Sat Jul  6 10:11:31 2024 ] 
Training: Epoch [5/120], Step [3999], Loss: 0.032335683703422546, Training Accuracy: 97.775
[ Sat Jul  6 10:11:32 2024 ] 	Batch(4000/7879) done. Loss: 0.0113  lr:0.000010
[ Sat Jul  6 10:11:50 2024 ] 	Batch(4100/7879) done. Loss: 0.0139  lr:0.000010
[ Sat Jul  6 10:12:08 2024 ] 	Batch(4200/7879) done. Loss: 0.1642  lr:0.000010
[ Sat Jul  6 10:12:26 2024 ] 	Batch(4300/7879) done. Loss: 0.0053  lr:0.000010
[ Sat Jul  6 10:12:44 2024 ] 	Batch(4400/7879) done. Loss: 0.0017  lr:0.000010
[ Sat Jul  6 10:13:03 2024 ] 
Training: Epoch [5/120], Step [4499], Loss: 0.13401135802268982, Training Accuracy: 97.73888888888888
[ Sat Jul  6 10:13:03 2024 ] 	Batch(4500/7879) done. Loss: 0.2012  lr:0.000010
[ Sat Jul  6 10:13:21 2024 ] 	Batch(4600/7879) done. Loss: 0.0264  lr:0.000010
[ Sat Jul  6 10:13:39 2024 ] 	Batch(4700/7879) done. Loss: 0.0773  lr:0.000010
[ Sat Jul  6 10:13:57 2024 ] 	Batch(4800/7879) done. Loss: 0.0226  lr:0.000010
[ Sat Jul  6 10:14:16 2024 ] 	Batch(4900/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 10:14:34 2024 ] 
Training: Epoch [5/120], Step [4999], Loss: 0.10845935344696045, Training Accuracy: 97.6825
[ Sat Jul  6 10:14:35 2024 ] 	Batch(5000/7879) done. Loss: 0.0717  lr:0.000010
[ Sat Jul  6 10:14:53 2024 ] 	Batch(5100/7879) done. Loss: 0.0529  lr:0.000010
[ Sat Jul  6 10:15:12 2024 ] 	Batch(5200/7879) done. Loss: 0.0135  lr:0.000010
[ Sat Jul  6 10:15:30 2024 ] 	Batch(5300/7879) done. Loss: 0.0129  lr:0.000010
[ Sat Jul  6 10:15:48 2024 ] 	Batch(5400/7879) done. Loss: 0.0679  lr:0.000010
[ Sat Jul  6 10:16:06 2024 ] 
Training: Epoch [5/120], Step [5499], Loss: 0.09949488937854767, Training Accuracy: 97.69090909090909
[ Sat Jul  6 10:16:06 2024 ] 	Batch(5500/7879) done. Loss: 0.0300  lr:0.000010
[ Sat Jul  6 10:16:24 2024 ] 	Batch(5600/7879) done. Loss: 0.1270  lr:0.000010
[ Sat Jul  6 10:16:43 2024 ] 	Batch(5700/7879) done. Loss: 0.0671  lr:0.000010
[ Sat Jul  6 10:17:01 2024 ] 	Batch(5800/7879) done. Loss: 0.2543  lr:0.000010
[ Sat Jul  6 10:17:20 2024 ] 	Batch(5900/7879) done. Loss: 0.0380  lr:0.000010
[ Sat Jul  6 10:17:38 2024 ] 
Training: Epoch [5/120], Step [5999], Loss: 0.2556384801864624, Training Accuracy: 97.69166666666666
[ Sat Jul  6 10:17:39 2024 ] 	Batch(6000/7879) done. Loss: 0.0110  lr:0.000010
[ Sat Jul  6 10:17:56 2024 ] 	Batch(6100/7879) done. Loss: 0.0432  lr:0.000010
[ Sat Jul  6 10:18:14 2024 ] 	Batch(6200/7879) done. Loss: 0.1310  lr:0.000010
[ Sat Jul  6 10:18:32 2024 ] 	Batch(6300/7879) done. Loss: 0.0719  lr:0.000010
[ Sat Jul  6 10:18:50 2024 ] 	Batch(6400/7879) done. Loss: 0.1471  lr:0.000010
[ Sat Jul  6 10:19:08 2024 ] 
Training: Epoch [5/120], Step [6499], Loss: 0.012082021683454514, Training Accuracy: 97.70769230769231
[ Sat Jul  6 10:19:08 2024 ] 	Batch(6500/7879) done. Loss: 0.0473  lr:0.000010
[ Sat Jul  6 10:19:26 2024 ] 	Batch(6600/7879) done. Loss: 0.0215  lr:0.000010
[ Sat Jul  6 10:19:44 2024 ] 	Batch(6700/7879) done. Loss: 0.1368  lr:0.000010
[ Sat Jul  6 10:20:02 2024 ] 	Batch(6800/7879) done. Loss: 0.0113  lr:0.000010
[ Sat Jul  6 10:20:20 2024 ] 	Batch(6900/7879) done. Loss: 0.0315  lr:0.000010
[ Sat Jul  6 10:20:38 2024 ] 
Training: Epoch [5/120], Step [6999], Loss: 0.24578021466732025, Training Accuracy: 97.70892857142857
[ Sat Jul  6 10:20:38 2024 ] 	Batch(7000/7879) done. Loss: 0.0082  lr:0.000010
[ Sat Jul  6 10:20:56 2024 ] 	Batch(7100/7879) done. Loss: 0.1443  lr:0.000010
[ Sat Jul  6 10:21:14 2024 ] 	Batch(7200/7879) done. Loss: 0.0150  lr:0.000010
[ Sat Jul  6 10:21:32 2024 ] 	Batch(7300/7879) done. Loss: 0.0062  lr:0.000010
[ Sat Jul  6 10:21:50 2024 ] 	Batch(7400/7879) done. Loss: 0.0454  lr:0.000010
[ Sat Jul  6 10:22:08 2024 ] 
Training: Epoch [5/120], Step [7499], Loss: 0.19987817108631134, Training Accuracy: 97.70833333333333
[ Sat Jul  6 10:22:08 2024 ] 	Batch(7500/7879) done. Loss: 0.0845  lr:0.000010
[ Sat Jul  6 10:22:26 2024 ] 	Batch(7600/7879) done. Loss: 0.0164  lr:0.000010
[ Sat Jul  6 10:22:44 2024 ] 	Batch(7700/7879) done. Loss: 0.1121  lr:0.000010
[ Sat Jul  6 10:23:02 2024 ] 	Batch(7800/7879) done. Loss: 0.0947  lr:0.000010
[ Sat Jul  6 10:23:16 2024 ] 	Mean training loss: 0.0917.
[ Sat Jul  6 10:23:16 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 10:23:16 2024 ] Training epoch: 7
[ Sat Jul  6 10:23:16 2024 ] 	Batch(0/7879) done. Loss: 0.2031  lr:0.000010
[ Sat Jul  6 10:23:35 2024 ] 	Batch(100/7879) done. Loss: 0.0760  lr:0.000010
[ Sat Jul  6 10:23:54 2024 ] 	Batch(200/7879) done. Loss: 0.0723  lr:0.000010
[ Sat Jul  6 10:24:12 2024 ] 	Batch(300/7879) done. Loss: 0.0340  lr:0.000010
[ Sat Jul  6 10:24:31 2024 ] 	Batch(400/7879) done. Loss: 0.2428  lr:0.000010
[ Sat Jul  6 10:24:49 2024 ] 
Training: Epoch [6/120], Step [499], Loss: 0.05677500367164612, Training Accuracy: 97.55
[ Sat Jul  6 10:24:49 2024 ] 	Batch(500/7879) done. Loss: 0.2853  lr:0.000010
[ Sat Jul  6 10:25:08 2024 ] 	Batch(600/7879) done. Loss: 0.0839  lr:0.000010
[ Sat Jul  6 10:25:26 2024 ] 	Batch(700/7879) done. Loss: 0.1738  lr:0.000010
[ Sat Jul  6 10:25:45 2024 ] 	Batch(800/7879) done. Loss: 0.0433  lr:0.000010
[ Sat Jul  6 10:26:03 2024 ] 	Batch(900/7879) done. Loss: 0.0284  lr:0.000010
[ Sat Jul  6 10:26:21 2024 ] 
Training: Epoch [6/120], Step [999], Loss: 0.05859912186861038, Training Accuracy: 97.625
[ Sat Jul  6 10:26:21 2024 ] 	Batch(1000/7879) done. Loss: 0.0462  lr:0.000010
[ Sat Jul  6 10:26:39 2024 ] 	Batch(1100/7879) done. Loss: 0.0591  lr:0.000010
[ Sat Jul  6 10:26:57 2024 ] 	Batch(1200/7879) done. Loss: 0.4989  lr:0.000010
[ Sat Jul  6 10:27:15 2024 ] 	Batch(1300/7879) done. Loss: 0.0135  lr:0.000010
[ Sat Jul  6 10:27:33 2024 ] 	Batch(1400/7879) done. Loss: 0.0867  lr:0.000010
[ Sat Jul  6 10:27:50 2024 ] 
Training: Epoch [6/120], Step [1499], Loss: 0.0340370312333107, Training Accuracy: 97.7
[ Sat Jul  6 10:27:51 2024 ] 	Batch(1500/7879) done. Loss: 0.0269  lr:0.000010
[ Sat Jul  6 10:28:09 2024 ] 	Batch(1600/7879) done. Loss: 0.0400  lr:0.000010
[ Sat Jul  6 10:28:27 2024 ] 	Batch(1700/7879) done. Loss: 0.3609  lr:0.000010
[ Sat Jul  6 10:28:45 2024 ] 	Batch(1800/7879) done. Loss: 0.4518  lr:0.000010
[ Sat Jul  6 10:29:03 2024 ] 	Batch(1900/7879) done. Loss: 0.1665  lr:0.000010
[ Sat Jul  6 10:29:20 2024 ] 
Training: Epoch [6/120], Step [1999], Loss: 0.011584986932575703, Training Accuracy: 97.6
[ Sat Jul  6 10:29:21 2024 ] 	Batch(2000/7879) done. Loss: 0.0094  lr:0.000010
[ Sat Jul  6 10:29:39 2024 ] 	Batch(2100/7879) done. Loss: 0.0254  lr:0.000010
[ Sat Jul  6 10:29:57 2024 ] 	Batch(2200/7879) done. Loss: 0.0034  lr:0.000010
[ Sat Jul  6 10:30:14 2024 ] 	Batch(2300/7879) done. Loss: 0.2043  lr:0.000010
[ Sat Jul  6 10:30:33 2024 ] 	Batch(2400/7879) done. Loss: 0.0191  lr:0.000010
[ Sat Jul  6 10:30:50 2024 ] 
Training: Epoch [6/120], Step [2499], Loss: 0.5269092321395874, Training Accuracy: 97.49
[ Sat Jul  6 10:30:50 2024 ] 	Batch(2500/7879) done. Loss: 0.5771  lr:0.000010
[ Sat Jul  6 10:31:08 2024 ] 	Batch(2600/7879) done. Loss: 0.1223  lr:0.000010
[ Sat Jul  6 10:31:26 2024 ] 	Batch(2700/7879) done. Loss: 0.0639  lr:0.000010
[ Sat Jul  6 10:31:44 2024 ] 	Batch(2800/7879) done. Loss: 0.0222  lr:0.000010
[ Sat Jul  6 10:32:02 2024 ] 	Batch(2900/7879) done. Loss: 0.1256  lr:0.000010
[ Sat Jul  6 10:32:20 2024 ] 
Training: Epoch [6/120], Step [2999], Loss: 0.008088593371212482, Training Accuracy: 97.53333333333333
[ Sat Jul  6 10:32:20 2024 ] 	Batch(3000/7879) done. Loss: 0.1913  lr:0.000010
[ Sat Jul  6 10:32:38 2024 ] 	Batch(3100/7879) done. Loss: 0.0409  lr:0.000010
[ Sat Jul  6 10:32:56 2024 ] 	Batch(3200/7879) done. Loss: 0.0086  lr:0.000010
[ Sat Jul  6 10:33:14 2024 ] 	Batch(3300/7879) done. Loss: 0.0149  lr:0.000010
[ Sat Jul  6 10:33:32 2024 ] 	Batch(3400/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 10:33:50 2024 ] 
Training: Epoch [6/120], Step [3499], Loss: 0.03149733319878578, Training Accuracy: 97.54285714285714
[ Sat Jul  6 10:33:50 2024 ] 	Batch(3500/7879) done. Loss: 0.0595  lr:0.000010
[ Sat Jul  6 10:34:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0233  lr:0.000010
[ Sat Jul  6 10:34:27 2024 ] 	Batch(3700/7879) done. Loss: 0.3568  lr:0.000010
[ Sat Jul  6 10:34:45 2024 ] 	Batch(3800/7879) done. Loss: 0.0138  lr:0.000010
[ Sat Jul  6 10:35:04 2024 ] 	Batch(3900/7879) done. Loss: 0.0075  lr:0.000010
[ Sat Jul  6 10:35:22 2024 ] 
Training: Epoch [6/120], Step [3999], Loss: 0.006531219929456711, Training Accuracy: 97.565625
[ Sat Jul  6 10:35:23 2024 ] 	Batch(4000/7879) done. Loss: 0.1426  lr:0.000010
[ Sat Jul  6 10:35:41 2024 ] 	Batch(4100/7879) done. Loss: 0.1800  lr:0.000010
[ Sat Jul  6 10:35:58 2024 ] 	Batch(4200/7879) done. Loss: 0.0040  lr:0.000010
[ Sat Jul  6 10:36:16 2024 ] 	Batch(4300/7879) done. Loss: 0.0158  lr:0.000010
[ Sat Jul  6 10:36:35 2024 ] 	Batch(4400/7879) done. Loss: 0.0097  lr:0.000010
[ Sat Jul  6 10:36:52 2024 ] 
Training: Epoch [6/120], Step [4499], Loss: 0.1431768387556076, Training Accuracy: 97.58333333333333
[ Sat Jul  6 10:36:52 2024 ] 	Batch(4500/7879) done. Loss: 0.0277  lr:0.000010
[ Sat Jul  6 10:37:10 2024 ] 	Batch(4600/7879) done. Loss: 0.0344  lr:0.000010
[ Sat Jul  6 10:37:28 2024 ] 	Batch(4700/7879) done. Loss: 0.0229  lr:0.000010
[ Sat Jul  6 10:37:46 2024 ] 	Batch(4800/7879) done. Loss: 0.0226  lr:0.000010
[ Sat Jul  6 10:38:05 2024 ] 	Batch(4900/7879) done. Loss: 0.0778  lr:0.000010
[ Sat Jul  6 10:38:23 2024 ] 
Training: Epoch [6/120], Step [4999], Loss: 0.2794250249862671, Training Accuracy: 97.5975
[ Sat Jul  6 10:38:23 2024 ] 	Batch(5000/7879) done. Loss: 0.0101  lr:0.000010
[ Sat Jul  6 10:38:42 2024 ] 	Batch(5100/7879) done. Loss: 0.2525  lr:0.000010
[ Sat Jul  6 10:39:01 2024 ] 	Batch(5200/7879) done. Loss: 0.0167  lr:0.000010
[ Sat Jul  6 10:39:19 2024 ] 	Batch(5300/7879) done. Loss: 0.0456  lr:0.000010
[ Sat Jul  6 10:39:37 2024 ] 	Batch(5400/7879) done. Loss: 0.0118  lr:0.000010
[ Sat Jul  6 10:39:54 2024 ] 
Training: Epoch [6/120], Step [5499], Loss: 0.14022278785705566, Training Accuracy: 97.55
[ Sat Jul  6 10:39:55 2024 ] 	Batch(5500/7879) done. Loss: 0.1923  lr:0.000010
[ Sat Jul  6 10:40:13 2024 ] 	Batch(5600/7879) done. Loss: 0.0545  lr:0.000010
[ Sat Jul  6 10:40:30 2024 ] 	Batch(5700/7879) done. Loss: 0.3538  lr:0.000010
[ Sat Jul  6 10:40:48 2024 ] 	Batch(5800/7879) done. Loss: 0.0300  lr:0.000010
[ Sat Jul  6 10:41:06 2024 ] 	Batch(5900/7879) done. Loss: 0.1447  lr:0.000010
[ Sat Jul  6 10:41:24 2024 ] 
Training: Epoch [6/120], Step [5999], Loss: 0.09846943616867065, Training Accuracy: 97.57916666666667
[ Sat Jul  6 10:41:24 2024 ] 	Batch(6000/7879) done. Loss: 0.0132  lr:0.000010
[ Sat Jul  6 10:41:43 2024 ] 	Batch(6100/7879) done. Loss: 0.2310  lr:0.000010
[ Sat Jul  6 10:42:01 2024 ] 	Batch(6200/7879) done. Loss: 0.0293  lr:0.000010
[ Sat Jul  6 10:42:19 2024 ] 	Batch(6300/7879) done. Loss: 0.0373  lr:0.000010
[ Sat Jul  6 10:42:37 2024 ] 	Batch(6400/7879) done. Loss: 0.0060  lr:0.000010
[ Sat Jul  6 10:42:55 2024 ] 
Training: Epoch [6/120], Step [6499], Loss: 0.06964430958032608, Training Accuracy: 97.60192307692309
[ Sat Jul  6 10:42:55 2024 ] 	Batch(6500/7879) done. Loss: 0.0828  lr:0.000010
[ Sat Jul  6 10:43:13 2024 ] 	Batch(6600/7879) done. Loss: 0.1088  lr:0.000010
[ Sat Jul  6 10:43:31 2024 ] 	Batch(6700/7879) done. Loss: 0.0076  lr:0.000010
[ Sat Jul  6 10:43:49 2024 ] 	Batch(6800/7879) done. Loss: 0.1233  lr:0.000010
[ Sat Jul  6 10:44:06 2024 ] 	Batch(6900/7879) done. Loss: 0.0788  lr:0.000010
[ Sat Jul  6 10:44:24 2024 ] 
Training: Epoch [6/120], Step [6999], Loss: 0.12045601010322571, Training Accuracy: 97.60714285714286
[ Sat Jul  6 10:44:24 2024 ] 	Batch(7000/7879) done. Loss: 0.0146  lr:0.000010
[ Sat Jul  6 10:44:42 2024 ] 	Batch(7100/7879) done. Loss: 0.0155  lr:0.000010
[ Sat Jul  6 10:45:00 2024 ] 	Batch(7200/7879) done. Loss: 0.0426  lr:0.000010
[ Sat Jul  6 10:45:19 2024 ] 	Batch(7300/7879) done. Loss: 0.0753  lr:0.000010
[ Sat Jul  6 10:45:38 2024 ] 	Batch(7400/7879) done. Loss: 0.0249  lr:0.000010
[ Sat Jul  6 10:45:55 2024 ] 
Training: Epoch [6/120], Step [7499], Loss: 0.027241645380854607, Training Accuracy: 97.61999999999999
[ Sat Jul  6 10:45:55 2024 ] 	Batch(7500/7879) done. Loss: 0.0158  lr:0.000010
[ Sat Jul  6 10:46:13 2024 ] 	Batch(7600/7879) done. Loss: 0.0163  lr:0.000010
[ Sat Jul  6 10:46:31 2024 ] 	Batch(7700/7879) done. Loss: 0.0254  lr:0.000010
[ Sat Jul  6 10:46:49 2024 ] 	Batch(7800/7879) done. Loss: 0.0070  lr:0.000010
[ Sat Jul  6 10:47:03 2024 ] 	Mean training loss: 0.0931.
[ Sat Jul  6 10:47:03 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 10:47:03 2024 ] Training epoch: 8
[ Sat Jul  6 10:47:04 2024 ] 	Batch(0/7879) done. Loss: 0.0035  lr:0.000010
[ Sat Jul  6 10:47:22 2024 ] 	Batch(100/7879) done. Loss: 0.0300  lr:0.000010
[ Sat Jul  6 10:47:40 2024 ] 	Batch(200/7879) done. Loss: 0.1178  lr:0.000010
[ Sat Jul  6 10:47:58 2024 ] 	Batch(300/7879) done. Loss: 0.0320  lr:0.000010
[ Sat Jul  6 10:48:16 2024 ] 	Batch(400/7879) done. Loss: 0.3161  lr:0.000010
[ Sat Jul  6 10:48:34 2024 ] 
Training: Epoch [7/120], Step [499], Loss: 0.18151910603046417, Training Accuracy: 97.52499999999999
[ Sat Jul  6 10:48:34 2024 ] 	Batch(500/7879) done. Loss: 0.2988  lr:0.000010
[ Sat Jul  6 10:48:52 2024 ] 	Batch(600/7879) done. Loss: 0.0135  lr:0.000010
[ Sat Jul  6 10:49:10 2024 ] 	Batch(700/7879) done. Loss: 0.0123  lr:0.000010
[ Sat Jul  6 10:49:28 2024 ] 	Batch(800/7879) done. Loss: 0.0073  lr:0.000010
[ Sat Jul  6 10:49:46 2024 ] 	Batch(900/7879) done. Loss: 0.1379  lr:0.000010
[ Sat Jul  6 10:50:04 2024 ] 
Training: Epoch [7/120], Step [999], Loss: 0.1637967973947525, Training Accuracy: 97.8
[ Sat Jul  6 10:50:04 2024 ] 	Batch(1000/7879) done. Loss: 0.0017  lr:0.000010
[ Sat Jul  6 10:50:22 2024 ] 	Batch(1100/7879) done. Loss: 0.0041  lr:0.000010
[ Sat Jul  6 10:50:40 2024 ] 	Batch(1200/7879) done. Loss: 0.0642  lr:0.000010
[ Sat Jul  6 10:50:58 2024 ] 	Batch(1300/7879) done. Loss: 0.0010  lr:0.000010
[ Sat Jul  6 10:51:16 2024 ] 	Batch(1400/7879) done. Loss: 0.0303  lr:0.000010
[ Sat Jul  6 10:51:34 2024 ] 
Training: Epoch [7/120], Step [1499], Loss: 0.1343201845884323, Training Accuracy: 97.775
[ Sat Jul  6 10:51:34 2024 ] 	Batch(1500/7879) done. Loss: 0.1173  lr:0.000010
[ Sat Jul  6 10:51:52 2024 ] 	Batch(1600/7879) done. Loss: 0.0213  lr:0.000010
[ Sat Jul  6 10:52:10 2024 ] 	Batch(1700/7879) done. Loss: 0.1298  lr:0.000010
[ Sat Jul  6 10:52:28 2024 ] 	Batch(1800/7879) done. Loss: 0.1831  lr:0.000010
[ Sat Jul  6 10:52:46 2024 ] 	Batch(1900/7879) done. Loss: 0.0091  lr:0.000010
[ Sat Jul  6 10:53:03 2024 ] 
Training: Epoch [7/120], Step [1999], Loss: 0.015429566614329815, Training Accuracy: 97.71875
[ Sat Jul  6 10:53:04 2024 ] 	Batch(2000/7879) done. Loss: 0.0369  lr:0.000010
[ Sat Jul  6 10:53:22 2024 ] 	Batch(2100/7879) done. Loss: 0.0401  lr:0.000010
[ Sat Jul  6 10:53:40 2024 ] 	Batch(2200/7879) done. Loss: 0.0988  lr:0.000010
[ Sat Jul  6 10:53:57 2024 ] 	Batch(2300/7879) done. Loss: 0.0032  lr:0.000010
[ Sat Jul  6 10:54:15 2024 ] 	Batch(2400/7879) done. Loss: 0.0103  lr:0.000010
[ Sat Jul  6 10:54:33 2024 ] 
Training: Epoch [7/120], Step [2499], Loss: 0.054303303360939026, Training Accuracy: 97.74000000000001
[ Sat Jul  6 10:54:33 2024 ] 	Batch(2500/7879) done. Loss: 0.0211  lr:0.000010
[ Sat Jul  6 10:54:51 2024 ] 	Batch(2600/7879) done. Loss: 0.0030  lr:0.000010
[ Sat Jul  6 10:55:09 2024 ] 	Batch(2700/7879) done. Loss: 0.1870  lr:0.000010
[ Sat Jul  6 10:55:27 2024 ] 	Batch(2800/7879) done. Loss: 0.1262  lr:0.000010
[ Sat Jul  6 10:55:46 2024 ] 	Batch(2900/7879) done. Loss: 0.2027  lr:0.000010
[ Sat Jul  6 10:56:04 2024 ] 
Training: Epoch [7/120], Step [2999], Loss: 0.08531500399112701, Training Accuracy: 97.68333333333334
[ Sat Jul  6 10:56:04 2024 ] 	Batch(3000/7879) done. Loss: 0.3647  lr:0.000010
[ Sat Jul  6 10:56:23 2024 ] 	Batch(3100/7879) done. Loss: 0.0720  lr:0.000010
[ Sat Jul  6 10:56:42 2024 ] 	Batch(3200/7879) done. Loss: 0.0131  lr:0.000010
[ Sat Jul  6 10:57:00 2024 ] 	Batch(3300/7879) done. Loss: 0.0901  lr:0.000010
[ Sat Jul  6 10:57:19 2024 ] 	Batch(3400/7879) done. Loss: 0.0531  lr:0.000010
[ Sat Jul  6 10:57:37 2024 ] 
Training: Epoch [7/120], Step [3499], Loss: 0.1306677758693695, Training Accuracy: 97.61071428571428
[ Sat Jul  6 10:57:37 2024 ] 	Batch(3500/7879) done. Loss: 0.0308  lr:0.000010
[ Sat Jul  6 10:57:56 2024 ] 	Batch(3600/7879) done. Loss: 0.0115  lr:0.000010
[ Sat Jul  6 10:58:14 2024 ] 	Batch(3700/7879) done. Loss: 0.0647  lr:0.000010
[ Sat Jul  6 10:58:32 2024 ] 	Batch(3800/7879) done. Loss: 0.1483  lr:0.000010
[ Sat Jul  6 10:58:50 2024 ] 	Batch(3900/7879) done. Loss: 0.0036  lr:0.000010
[ Sat Jul  6 10:59:07 2024 ] 
Training: Epoch [7/120], Step [3999], Loss: 0.03724953532218933, Training Accuracy: 97.625
[ Sat Jul  6 10:59:08 2024 ] 	Batch(4000/7879) done. Loss: 0.0791  lr:0.000010
[ Sat Jul  6 10:59:26 2024 ] 	Batch(4100/7879) done. Loss: 0.0479  lr:0.000010
[ Sat Jul  6 10:59:44 2024 ] 	Batch(4200/7879) done. Loss: 0.0258  lr:0.000010
[ Sat Jul  6 11:00:01 2024 ] 	Batch(4300/7879) done. Loss: 0.0190  lr:0.000010
[ Sat Jul  6 11:00:20 2024 ] 	Batch(4400/7879) done. Loss: 0.0600  lr:0.000010
[ Sat Jul  6 11:00:37 2024 ] 
Training: Epoch [7/120], Step [4499], Loss: 0.029625970870256424, Training Accuracy: 97.62777777777778
[ Sat Jul  6 11:00:37 2024 ] 	Batch(4500/7879) done. Loss: 0.0404  lr:0.000010
[ Sat Jul  6 11:00:55 2024 ] 	Batch(4600/7879) done. Loss: 0.0080  lr:0.000010
[ Sat Jul  6 11:01:13 2024 ] 	Batch(4700/7879) done. Loss: 0.0348  lr:0.000010
[ Sat Jul  6 11:01:31 2024 ] 	Batch(4800/7879) done. Loss: 0.0061  lr:0.000010
[ Sat Jul  6 11:01:49 2024 ] 	Batch(4900/7879) done. Loss: 0.1759  lr:0.000010
[ Sat Jul  6 11:02:07 2024 ] 
Training: Epoch [7/120], Step [4999], Loss: 0.010675943456590176, Training Accuracy: 97.64
[ Sat Jul  6 11:02:07 2024 ] 	Batch(5000/7879) done. Loss: 0.1302  lr:0.000010
[ Sat Jul  6 11:02:25 2024 ] 	Batch(5100/7879) done. Loss: 0.4115  lr:0.000010
[ Sat Jul  6 11:02:43 2024 ] 	Batch(5200/7879) done. Loss: 0.1369  lr:0.000010
[ Sat Jul  6 11:03:02 2024 ] 	Batch(5300/7879) done. Loss: 0.0278  lr:0.000010
[ Sat Jul  6 11:03:21 2024 ] 	Batch(5400/7879) done. Loss: 0.1146  lr:0.000010
[ Sat Jul  6 11:03:39 2024 ] 
Training: Epoch [7/120], Step [5499], Loss: 0.047256916761398315, Training Accuracy: 97.625
[ Sat Jul  6 11:03:39 2024 ] 	Batch(5500/7879) done. Loss: 0.1092  lr:0.000010
[ Sat Jul  6 11:03:58 2024 ] 	Batch(5600/7879) done. Loss: 0.0256  lr:0.000010
[ Sat Jul  6 11:04:16 2024 ] 	Batch(5700/7879) done. Loss: 0.0880  lr:0.000010
[ Sat Jul  6 11:04:35 2024 ] 	Batch(5800/7879) done. Loss: 0.0733  lr:0.000010
[ Sat Jul  6 11:04:54 2024 ] 	Batch(5900/7879) done. Loss: 0.0400  lr:0.000010
[ Sat Jul  6 11:05:12 2024 ] 
Training: Epoch [7/120], Step [5999], Loss: 0.16106878221035004, Training Accuracy: 97.66041666666668
[ Sat Jul  6 11:05:12 2024 ] 	Batch(6000/7879) done. Loss: 0.1579  lr:0.000010
[ Sat Jul  6 11:05:31 2024 ] 	Batch(6100/7879) done. Loss: 0.0295  lr:0.000010
[ Sat Jul  6 11:05:49 2024 ] 	Batch(6200/7879) done. Loss: 0.0346  lr:0.000010
[ Sat Jul  6 11:06:07 2024 ] 	Batch(6300/7879) done. Loss: 0.0166  lr:0.000010
[ Sat Jul  6 11:06:25 2024 ] 	Batch(6400/7879) done. Loss: 0.0505  lr:0.000010
[ Sat Jul  6 11:06:43 2024 ] 
Training: Epoch [7/120], Step [6499], Loss: 0.31264108419418335, Training Accuracy: 97.67307692307692
[ Sat Jul  6 11:06:43 2024 ] 	Batch(6500/7879) done. Loss: 0.2364  lr:0.000010
[ Sat Jul  6 11:07:01 2024 ] 	Batch(6600/7879) done. Loss: 0.1167  lr:0.000010
[ Sat Jul  6 11:07:19 2024 ] 	Batch(6700/7879) done. Loss: 0.2349  lr:0.000010
[ Sat Jul  6 11:07:37 2024 ] 	Batch(6800/7879) done. Loss: 0.0554  lr:0.000010
[ Sat Jul  6 11:07:55 2024 ] 	Batch(6900/7879) done. Loss: 0.0137  lr:0.000010
[ Sat Jul  6 11:08:13 2024 ] 
Training: Epoch [7/120], Step [6999], Loss: 0.1327609419822693, Training Accuracy: 97.68214285714286
[ Sat Jul  6 11:08:13 2024 ] 	Batch(7000/7879) done. Loss: 0.0962  lr:0.000010
[ Sat Jul  6 11:08:31 2024 ] 	Batch(7100/7879) done. Loss: 0.0888  lr:0.000010
[ Sat Jul  6 11:08:49 2024 ] 	Batch(7200/7879) done. Loss: 0.0079  lr:0.000010
[ Sat Jul  6 11:09:07 2024 ] 	Batch(7300/7879) done. Loss: 0.0705  lr:0.000010
[ Sat Jul  6 11:09:26 2024 ] 	Batch(7400/7879) done. Loss: 0.0673  lr:0.000010
[ Sat Jul  6 11:09:44 2024 ] 
Training: Epoch [7/120], Step [7499], Loss: 0.004492218140512705, Training Accuracy: 97.67333333333333
[ Sat Jul  6 11:09:44 2024 ] 	Batch(7500/7879) done. Loss: 0.0225  lr:0.000010
[ Sat Jul  6 11:10:02 2024 ] 	Batch(7600/7879) done. Loss: 0.0581  lr:0.000010
[ Sat Jul  6 11:10:21 2024 ] 	Batch(7700/7879) done. Loss: 0.0779  lr:0.000010
[ Sat Jul  6 11:10:40 2024 ] 	Batch(7800/7879) done. Loss: 0.0142  lr:0.000010
[ Sat Jul  6 11:10:54 2024 ] 	Mean training loss: 0.0946.
[ Sat Jul  6 11:10:54 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 11:10:54 2024 ] Training epoch: 9
[ Sat Jul  6 11:10:55 2024 ] 	Batch(0/7879) done. Loss: 0.1187  lr:0.000010
[ Sat Jul  6 11:11:13 2024 ] 	Batch(100/7879) done. Loss: 0.1769  lr:0.000010
[ Sat Jul  6 11:11:31 2024 ] 	Batch(200/7879) done. Loss: 0.0383  lr:0.000010
[ Sat Jul  6 11:11:49 2024 ] 	Batch(300/7879) done. Loss: 0.0415  lr:0.000010
[ Sat Jul  6 11:12:07 2024 ] 	Batch(400/7879) done. Loss: 0.0444  lr:0.000010
[ Sat Jul  6 11:12:24 2024 ] 
Training: Epoch [8/120], Step [499], Loss: 0.033496543765068054, Training Accuracy: 97.55
[ Sat Jul  6 11:12:25 2024 ] 	Batch(500/7879) done. Loss: 0.0485  lr:0.000010
[ Sat Jul  6 11:12:42 2024 ] 	Batch(600/7879) done. Loss: 0.0544  lr:0.000010
[ Sat Jul  6 11:13:00 2024 ] 	Batch(700/7879) done. Loss: 0.3991  lr:0.000010
[ Sat Jul  6 11:13:18 2024 ] 	Batch(800/7879) done. Loss: 0.0820  lr:0.000010
[ Sat Jul  6 11:13:37 2024 ] 	Batch(900/7879) done. Loss: 0.0371  lr:0.000010
[ Sat Jul  6 11:13:56 2024 ] 
Training: Epoch [8/120], Step [999], Loss: 0.03792879730463028, Training Accuracy: 97.7375
[ Sat Jul  6 11:13:56 2024 ] 	Batch(1000/7879) done. Loss: 0.0628  lr:0.000010
[ Sat Jul  6 11:14:14 2024 ] 	Batch(1100/7879) done. Loss: 0.0173  lr:0.000010
[ Sat Jul  6 11:14:32 2024 ] 	Batch(1200/7879) done. Loss: 0.0041  lr:0.000010
[ Sat Jul  6 11:14:51 2024 ] 	Batch(1300/7879) done. Loss: 0.0123  lr:0.000010
[ Sat Jul  6 11:15:09 2024 ] 	Batch(1400/7879) done. Loss: 0.0042  lr:0.000010
[ Sat Jul  6 11:15:27 2024 ] 
Training: Epoch [8/120], Step [1499], Loss: 0.1291726678609848, Training Accuracy: 97.61666666666666
[ Sat Jul  6 11:15:28 2024 ] 	Batch(1500/7879) done. Loss: 0.2733  lr:0.000010
[ Sat Jul  6 11:15:46 2024 ] 	Batch(1600/7879) done. Loss: 0.0300  lr:0.000010
[ Sat Jul  6 11:16:04 2024 ] 	Batch(1700/7879) done. Loss: 0.3781  lr:0.000010
[ Sat Jul  6 11:16:22 2024 ] 	Batch(1800/7879) done. Loss: 0.0413  lr:0.000010
[ Sat Jul  6 11:16:40 2024 ] 	Batch(1900/7879) done. Loss: 0.0100  lr:0.000010
[ Sat Jul  6 11:16:57 2024 ] 
Training: Epoch [8/120], Step [1999], Loss: 0.1601589173078537, Training Accuracy: 97.73125
[ Sat Jul  6 11:16:58 2024 ] 	Batch(2000/7879) done. Loss: 0.1154  lr:0.000010
[ Sat Jul  6 11:17:16 2024 ] 	Batch(2100/7879) done. Loss: 0.1391  lr:0.000010
[ Sat Jul  6 11:17:34 2024 ] 	Batch(2200/7879) done. Loss: 0.0234  lr:0.000010
[ Sat Jul  6 11:17:52 2024 ] 	Batch(2300/7879) done. Loss: 0.1565  lr:0.000010
[ Sat Jul  6 11:18:10 2024 ] 	Batch(2400/7879) done. Loss: 0.4834  lr:0.000010
[ Sat Jul  6 11:18:28 2024 ] 
Training: Epoch [8/120], Step [2499], Loss: 0.033586982637643814, Training Accuracy: 97.72999999999999
[ Sat Jul  6 11:18:28 2024 ] 	Batch(2500/7879) done. Loss: 0.0108  lr:0.000010
[ Sat Jul  6 11:18:47 2024 ] 	Batch(2600/7879) done. Loss: 0.0679  lr:0.000010
[ Sat Jul  6 11:19:05 2024 ] 	Batch(2700/7879) done. Loss: 0.0284  lr:0.000010
[ Sat Jul  6 11:19:24 2024 ] 	Batch(2800/7879) done. Loss: 0.0101  lr:0.000010
[ Sat Jul  6 11:19:43 2024 ] 	Batch(2900/7879) done. Loss: 0.0676  lr:0.000010
[ Sat Jul  6 11:20:01 2024 ] 
Training: Epoch [8/120], Step [2999], Loss: 0.011943801306188107, Training Accuracy: 97.78750000000001
[ Sat Jul  6 11:20:01 2024 ] 	Batch(3000/7879) done. Loss: 0.5129  lr:0.000010
[ Sat Jul  6 11:20:20 2024 ] 	Batch(3100/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 11:20:38 2024 ] 	Batch(3200/7879) done. Loss: 0.0374  lr:0.000010
[ Sat Jul  6 11:20:57 2024 ] 	Batch(3300/7879) done. Loss: 0.0470  lr:0.000010
[ Sat Jul  6 11:21:16 2024 ] 	Batch(3400/7879) done. Loss: 0.0541  lr:0.000010
[ Sat Jul  6 11:21:34 2024 ] 
Training: Epoch [8/120], Step [3499], Loss: 0.11540215462446213, Training Accuracy: 97.78928571428571
[ Sat Jul  6 11:21:34 2024 ] 	Batch(3500/7879) done. Loss: 0.0147  lr:0.000010
[ Sat Jul  6 11:21:52 2024 ] 	Batch(3600/7879) done. Loss: 0.2039  lr:0.000010
[ Sat Jul  6 11:22:11 2024 ] 	Batch(3700/7879) done. Loss: 0.1757  lr:0.000010
[ Sat Jul  6 11:22:30 2024 ] 	Batch(3800/7879) done. Loss: 0.1492  lr:0.000010
[ Sat Jul  6 11:22:48 2024 ] 	Batch(3900/7879) done. Loss: 0.0335  lr:0.000010
[ Sat Jul  6 11:23:06 2024 ] 
Training: Epoch [8/120], Step [3999], Loss: 0.026397086679935455, Training Accuracy: 97.80312500000001
[ Sat Jul  6 11:23:06 2024 ] 	Batch(4000/7879) done. Loss: 0.1589  lr:0.000010
[ Sat Jul  6 11:23:24 2024 ] 	Batch(4100/7879) done. Loss: 0.0471  lr:0.000010
[ Sat Jul  6 11:23:42 2024 ] 	Batch(4200/7879) done. Loss: 0.0953  lr:0.000010
[ Sat Jul  6 11:24:00 2024 ] 	Batch(4300/7879) done. Loss: 0.1136  lr:0.000010
[ Sat Jul  6 11:24:18 2024 ] 	Batch(4400/7879) done. Loss: 0.1263  lr:0.000010
[ Sat Jul  6 11:24:36 2024 ] 
Training: Epoch [8/120], Step [4499], Loss: 0.03682457655668259, Training Accuracy: 97.76666666666667
[ Sat Jul  6 11:24:36 2024 ] 	Batch(4500/7879) done. Loss: 0.0538  lr:0.000010
[ Sat Jul  6 11:24:54 2024 ] 	Batch(4600/7879) done. Loss: 0.0457  lr:0.000010
[ Sat Jul  6 11:25:12 2024 ] 	Batch(4700/7879) done. Loss: 0.0129  lr:0.000010
[ Sat Jul  6 11:25:30 2024 ] 	Batch(4800/7879) done. Loss: 0.0518  lr:0.000010
[ Sat Jul  6 11:25:48 2024 ] 	Batch(4900/7879) done. Loss: 0.1157  lr:0.000010
[ Sat Jul  6 11:26:06 2024 ] 
Training: Epoch [8/120], Step [4999], Loss: 0.019748998805880547, Training Accuracy: 97.75
[ Sat Jul  6 11:26:06 2024 ] 	Batch(5000/7879) done. Loss: 0.0575  lr:0.000010
[ Sat Jul  6 11:26:24 2024 ] 	Batch(5100/7879) done. Loss: 0.2490  lr:0.000010
[ Sat Jul  6 11:26:42 2024 ] 	Batch(5200/7879) done. Loss: 0.1113  lr:0.000010
[ Sat Jul  6 11:27:00 2024 ] 	Batch(5300/7879) done. Loss: 0.0195  lr:0.000010
[ Sat Jul  6 11:27:19 2024 ] 	Batch(5400/7879) done. Loss: 0.1871  lr:0.000010
[ Sat Jul  6 11:27:37 2024 ] 
Training: Epoch [8/120], Step [5499], Loss: 0.0032958260271698236, Training Accuracy: 97.75454545454545
[ Sat Jul  6 11:27:38 2024 ] 	Batch(5500/7879) done. Loss: 0.0048  lr:0.000010
[ Sat Jul  6 11:27:56 2024 ] 	Batch(5600/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 11:28:14 2024 ] 	Batch(5700/7879) done. Loss: 0.0432  lr:0.000010
[ Sat Jul  6 11:28:32 2024 ] 	Batch(5800/7879) done. Loss: 0.0119  lr:0.000010
[ Sat Jul  6 11:28:50 2024 ] 	Batch(5900/7879) done. Loss: 0.1649  lr:0.000010
[ Sat Jul  6 11:29:08 2024 ] 
Training: Epoch [8/120], Step [5999], Loss: 0.0374942421913147, Training Accuracy: 97.75
[ Sat Jul  6 11:29:08 2024 ] 	Batch(6000/7879) done. Loss: 0.0348  lr:0.000010
[ Sat Jul  6 11:29:27 2024 ] 	Batch(6100/7879) done. Loss: 0.0121  lr:0.000010
[ Sat Jul  6 11:29:45 2024 ] 	Batch(6200/7879) done. Loss: 0.4490  lr:0.000010
[ Sat Jul  6 11:30:04 2024 ] 	Batch(6300/7879) done. Loss: 0.0280  lr:0.000010
[ Sat Jul  6 11:30:23 2024 ] 	Batch(6400/7879) done. Loss: 0.0456  lr:0.000010
[ Sat Jul  6 11:30:41 2024 ] 
Training: Epoch [8/120], Step [6499], Loss: 0.1734286993741989, Training Accuracy: 97.73653846153846
[ Sat Jul  6 11:30:41 2024 ] 	Batch(6500/7879) done. Loss: 0.2662  lr:0.000010
[ Sat Jul  6 11:31:00 2024 ] 	Batch(6600/7879) done. Loss: 0.1101  lr:0.000010
[ Sat Jul  6 11:31:18 2024 ] 	Batch(6700/7879) done. Loss: 0.0043  lr:0.000010
[ Sat Jul  6 11:31:37 2024 ] 	Batch(6800/7879) done. Loss: 0.0701  lr:0.000010
[ Sat Jul  6 11:31:55 2024 ] 	Batch(6900/7879) done. Loss: 0.1962  lr:0.000010
[ Sat Jul  6 11:32:13 2024 ] 
Training: Epoch [8/120], Step [6999], Loss: 0.2002958059310913, Training Accuracy: 97.72857142857143
[ Sat Jul  6 11:32:13 2024 ] 	Batch(7000/7879) done. Loss: 0.0114  lr:0.000010
[ Sat Jul  6 11:32:31 2024 ] 	Batch(7100/7879) done. Loss: 0.0260  lr:0.000010
[ Sat Jul  6 11:32:49 2024 ] 	Batch(7200/7879) done. Loss: 0.0102  lr:0.000010
[ Sat Jul  6 11:33:07 2024 ] 	Batch(7300/7879) done. Loss: 0.0642  lr:0.000010
[ Sat Jul  6 11:33:25 2024 ] 	Batch(7400/7879) done. Loss: 0.7064  lr:0.000010
[ Sat Jul  6 11:33:42 2024 ] 
Training: Epoch [8/120], Step [7499], Loss: 0.004431169014424086, Training Accuracy: 97.72833333333332
[ Sat Jul  6 11:33:43 2024 ] 	Batch(7500/7879) done. Loss: 0.0965  lr:0.000010
[ Sat Jul  6 11:34:01 2024 ] 	Batch(7600/7879) done. Loss: 0.0131  lr:0.000010
[ Sat Jul  6 11:34:19 2024 ] 	Batch(7700/7879) done. Loss: 0.0610  lr:0.000010
[ Sat Jul  6 11:34:37 2024 ] 	Batch(7800/7879) done. Loss: 0.6134  lr:0.000010
[ Sat Jul  6 11:34:51 2024 ] 	Mean training loss: 0.0904.
[ Sat Jul  6 11:34:51 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 11:34:51 2024 ] Training epoch: 10
[ Sat Jul  6 11:34:51 2024 ] 	Batch(0/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 11:35:10 2024 ] 	Batch(100/7879) done. Loss: 0.0198  lr:0.000010
[ Sat Jul  6 11:35:29 2024 ] 	Batch(200/7879) done. Loss: 0.5376  lr:0.000010
[ Sat Jul  6 11:35:47 2024 ] 	Batch(300/7879) done. Loss: 0.0129  lr:0.000010
[ Sat Jul  6 11:36:06 2024 ] 	Batch(400/7879) done. Loss: 0.2850  lr:0.000010
[ Sat Jul  6 11:36:24 2024 ] 
Training: Epoch [9/120], Step [499], Loss: 0.01751825399696827, Training Accuracy: 98.3
[ Sat Jul  6 11:36:24 2024 ] 	Batch(500/7879) done. Loss: 0.0175  lr:0.000010
[ Sat Jul  6 11:36:43 2024 ] 	Batch(600/7879) done. Loss: 0.0817  lr:0.000010
[ Sat Jul  6 11:37:01 2024 ] 	Batch(700/7879) done. Loss: 0.0395  lr:0.000010
[ Sat Jul  6 11:37:19 2024 ] 	Batch(800/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 11:37:37 2024 ] 	Batch(900/7879) done. Loss: 0.0278  lr:0.000010
[ Sat Jul  6 11:37:56 2024 ] 
Training: Epoch [9/120], Step [999], Loss: 0.0014125365996733308, Training Accuracy: 97.875
[ Sat Jul  6 11:37:56 2024 ] 	Batch(1000/7879) done. Loss: 0.0105  lr:0.000010
[ Sat Jul  6 11:38:14 2024 ] 	Batch(1100/7879) done. Loss: 0.1176  lr:0.000010
[ Sat Jul  6 11:38:33 2024 ] 	Batch(1200/7879) done. Loss: 0.0371  lr:0.000010
[ Sat Jul  6 11:38:51 2024 ] 	Batch(1300/7879) done. Loss: 0.0353  lr:0.000010
[ Sat Jul  6 11:39:09 2024 ] 	Batch(1400/7879) done. Loss: 0.0328  lr:0.000010
[ Sat Jul  6 11:39:27 2024 ] 
Training: Epoch [9/120], Step [1499], Loss: 0.009028078988194466, Training Accuracy: 97.84166666666667
[ Sat Jul  6 11:39:27 2024 ] 	Batch(1500/7879) done. Loss: 0.1088  lr:0.000010
[ Sat Jul  6 11:39:45 2024 ] 	Batch(1600/7879) done. Loss: 0.0813  lr:0.000010
[ Sat Jul  6 11:40:03 2024 ] 	Batch(1700/7879) done. Loss: 0.0183  lr:0.000010
[ Sat Jul  6 11:40:21 2024 ] 	Batch(1800/7879) done. Loss: 0.0137  lr:0.000010
[ Sat Jul  6 11:40:39 2024 ] 	Batch(1900/7879) done. Loss: 0.0045  lr:0.000010
[ Sat Jul  6 11:40:57 2024 ] 
Training: Epoch [9/120], Step [1999], Loss: 0.03604404628276825, Training Accuracy: 97.8
[ Sat Jul  6 11:40:57 2024 ] 	Batch(2000/7879) done. Loss: 0.0467  lr:0.000010
[ Sat Jul  6 11:41:15 2024 ] 	Batch(2100/7879) done. Loss: 0.0887  lr:0.000010
[ Sat Jul  6 11:41:33 2024 ] 	Batch(2200/7879) done. Loss: 0.0280  lr:0.000010
[ Sat Jul  6 11:41:51 2024 ] 	Batch(2300/7879) done. Loss: 0.0011  lr:0.000010
[ Sat Jul  6 11:42:09 2024 ] 	Batch(2400/7879) done. Loss: 0.0695  lr:0.000010
[ Sat Jul  6 11:42:26 2024 ] 
Training: Epoch [9/120], Step [2499], Loss: 0.12240851670503616, Training Accuracy: 97.71
[ Sat Jul  6 11:42:27 2024 ] 	Batch(2500/7879) done. Loss: 0.0367  lr:0.000010
[ Sat Jul  6 11:42:44 2024 ] 	Batch(2600/7879) done. Loss: 0.0032  lr:0.000010
[ Sat Jul  6 11:43:02 2024 ] 	Batch(2700/7879) done. Loss: 0.0738  lr:0.000010
[ Sat Jul  6 11:43:20 2024 ] 	Batch(2800/7879) done. Loss: 0.0377  lr:0.000010
[ Sat Jul  6 11:43:38 2024 ] 	Batch(2900/7879) done. Loss: 0.0021  lr:0.000010
[ Sat Jul  6 11:43:56 2024 ] 
Training: Epoch [9/120], Step [2999], Loss: 0.04160472750663757, Training Accuracy: 97.675
[ Sat Jul  6 11:43:56 2024 ] 	Batch(3000/7879) done. Loss: 0.0345  lr:0.000010
[ Sat Jul  6 11:44:14 2024 ] 	Batch(3100/7879) done. Loss: 0.2150  lr:0.000010
[ Sat Jul  6 11:44:32 2024 ] 	Batch(3200/7879) done. Loss: 0.0087  lr:0.000010
[ Sat Jul  6 11:44:50 2024 ] 	Batch(3300/7879) done. Loss: 0.1127  lr:0.000010
[ Sat Jul  6 11:45:08 2024 ] 	Batch(3400/7879) done. Loss: 0.4975  lr:0.000010
[ Sat Jul  6 11:45:26 2024 ] 
Training: Epoch [9/120], Step [3499], Loss: 0.007964258082211018, Training Accuracy: 97.70714285714286
[ Sat Jul  6 11:45:26 2024 ] 	Batch(3500/7879) done. Loss: 0.0079  lr:0.000010
[ Sat Jul  6 11:45:44 2024 ] 	Batch(3600/7879) done. Loss: 0.0874  lr:0.000010
[ Sat Jul  6 11:46:02 2024 ] 	Batch(3700/7879) done. Loss: 0.0827  lr:0.000010
[ Sat Jul  6 11:46:20 2024 ] 	Batch(3800/7879) done. Loss: 0.1395  lr:0.000010
[ Sat Jul  6 11:46:38 2024 ] 	Batch(3900/7879) done. Loss: 0.3356  lr:0.000010
[ Sat Jul  6 11:46:56 2024 ] 
Training: Epoch [9/120], Step [3999], Loss: 0.05895363539457321, Training Accuracy: 97.696875
[ Sat Jul  6 11:46:56 2024 ] 	Batch(4000/7879) done. Loss: 0.0351  lr:0.000010
[ Sat Jul  6 11:47:15 2024 ] 	Batch(4100/7879) done. Loss: 0.0203  lr:0.000010
[ Sat Jul  6 11:47:33 2024 ] 	Batch(4200/7879) done. Loss: 0.0248  lr:0.000010
[ Sat Jul  6 11:47:52 2024 ] 	Batch(4300/7879) done. Loss: 0.1247  lr:0.000010
[ Sat Jul  6 11:48:11 2024 ] 	Batch(4400/7879) done. Loss: 0.0126  lr:0.000010
[ Sat Jul  6 11:48:29 2024 ] 
Training: Epoch [9/120], Step [4499], Loss: 0.11437665671110153, Training Accuracy: 97.64722222222221
[ Sat Jul  6 11:48:29 2024 ] 	Batch(4500/7879) done. Loss: 0.0081  lr:0.000010
[ Sat Jul  6 11:48:47 2024 ] 	Batch(4600/7879) done. Loss: 0.1026  lr:0.000010
[ Sat Jul  6 11:49:05 2024 ] 	Batch(4700/7879) done. Loss: 0.3327  lr:0.000010
[ Sat Jul  6 11:49:23 2024 ] 	Batch(4800/7879) done. Loss: 0.2689  lr:0.000010
[ Sat Jul  6 11:49:42 2024 ] 	Batch(4900/7879) done. Loss: 0.0106  lr:0.000010
[ Sat Jul  6 11:50:00 2024 ] 
Training: Epoch [9/120], Step [4999], Loss: 0.08942028880119324, Training Accuracy: 97.615
[ Sat Jul  6 11:50:00 2024 ] 	Batch(5000/7879) done. Loss: 0.0077  lr:0.000010
[ Sat Jul  6 11:50:19 2024 ] 	Batch(5100/7879) done. Loss: 0.0707  lr:0.000010
[ Sat Jul  6 11:50:38 2024 ] 	Batch(5200/7879) done. Loss: 0.1271  lr:0.000010
[ Sat Jul  6 11:50:56 2024 ] 	Batch(5300/7879) done. Loss: 0.2061  lr:0.000010
[ Sat Jul  6 11:51:14 2024 ] 	Batch(5400/7879) done. Loss: 0.0096  lr:0.000010
[ Sat Jul  6 11:51:31 2024 ] 
Training: Epoch [9/120], Step [5499], Loss: 0.024291379377245903, Training Accuracy: 97.59772727272727
[ Sat Jul  6 11:51:31 2024 ] 	Batch(5500/7879) done. Loss: 0.1007  lr:0.000010
[ Sat Jul  6 11:51:50 2024 ] 	Batch(5600/7879) done. Loss: 0.0168  lr:0.000010
[ Sat Jul  6 11:52:08 2024 ] 	Batch(5700/7879) done. Loss: 0.1259  lr:0.000010
[ Sat Jul  6 11:52:27 2024 ] 	Batch(5800/7879) done. Loss: 0.0142  lr:0.000010
[ Sat Jul  6 11:52:45 2024 ] 	Batch(5900/7879) done. Loss: 0.1453  lr:0.000010
[ Sat Jul  6 11:53:04 2024 ] 
Training: Epoch [9/120], Step [5999], Loss: 0.002656692173331976, Training Accuracy: 97.63958333333333
[ Sat Jul  6 11:53:04 2024 ] 	Batch(6000/7879) done. Loss: 0.0016  lr:0.000010
[ Sat Jul  6 11:53:22 2024 ] 	Batch(6100/7879) done. Loss: 0.0431  lr:0.000010
[ Sat Jul  6 11:53:40 2024 ] 	Batch(6200/7879) done. Loss: 0.0157  lr:0.000010
[ Sat Jul  6 11:53:58 2024 ] 	Batch(6300/7879) done. Loss: 0.0047  lr:0.000010
[ Sat Jul  6 11:54:16 2024 ] 	Batch(6400/7879) done. Loss: 0.0261  lr:0.000010
[ Sat Jul  6 11:54:34 2024 ] 
Training: Epoch [9/120], Step [6499], Loss: 0.021968672052025795, Training Accuracy: 97.63653846153846
[ Sat Jul  6 11:54:34 2024 ] 	Batch(6500/7879) done. Loss: 0.0308  lr:0.000010
[ Sat Jul  6 11:54:52 2024 ] 	Batch(6600/7879) done. Loss: 0.2739  lr:0.000010
[ Sat Jul  6 11:55:10 2024 ] 	Batch(6700/7879) done. Loss: 0.1868  lr:0.000010
[ Sat Jul  6 11:55:28 2024 ] 	Batch(6800/7879) done. Loss: 0.1304  lr:0.000010
[ Sat Jul  6 11:55:46 2024 ] 	Batch(6900/7879) done. Loss: 0.0415  lr:0.000010
[ Sat Jul  6 11:56:04 2024 ] 
Training: Epoch [9/120], Step [6999], Loss: 0.5329809784889221, Training Accuracy: 97.66071428571429
[ Sat Jul  6 11:56:04 2024 ] 	Batch(7000/7879) done. Loss: 0.0025  lr:0.000010
[ Sat Jul  6 11:56:22 2024 ] 	Batch(7100/7879) done. Loss: 0.0029  lr:0.000010
[ Sat Jul  6 11:56:40 2024 ] 	Batch(7200/7879) done. Loss: 0.0371  lr:0.000010
[ Sat Jul  6 11:56:58 2024 ] 	Batch(7300/7879) done. Loss: 0.0016  lr:0.000010
[ Sat Jul  6 11:57:16 2024 ] 	Batch(7400/7879) done. Loss: 0.1704  lr:0.000010
[ Sat Jul  6 11:57:33 2024 ] 
Training: Epoch [9/120], Step [7499], Loss: 0.13525226712226868, Training Accuracy: 97.67333333333333
[ Sat Jul  6 11:57:33 2024 ] 	Batch(7500/7879) done. Loss: 0.1802  lr:0.000010
[ Sat Jul  6 11:57:52 2024 ] 	Batch(7600/7879) done. Loss: 0.0065  lr:0.000010
[ Sat Jul  6 11:58:10 2024 ] 	Batch(7700/7879) done. Loss: 0.0157  lr:0.000010
[ Sat Jul  6 11:58:28 2024 ] 	Batch(7800/7879) done. Loss: 0.0094  lr:0.000010
[ Sat Jul  6 11:58:42 2024 ] 	Mean training loss: 0.0909.
[ Sat Jul  6 11:58:42 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 11:58:42 2024 ] Eval epoch: 10
[ Sat Jul  6 12:03:29 2024 ] 	Mean val loss of 6365 batches: 1.5191544129400985.
[ Sat Jul  6 12:03:29 2024 ] 
Validation: Epoch [9/120], Samples [39430.0/50919], Loss: 0.03783806785941124, Validation Accuracy: 77.43671321117854
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 1 : 201 / 275 = 73 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 2 : 220 / 273 = 80 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 3 : 225 / 273 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 4 : 218 / 275 = 79 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 5 : 245 / 275 = 89 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 6 : 227 / 275 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 7 : 249 / 273 = 91 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 8 : 264 / 273 = 96 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 9 : 194 / 273 = 71 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 10 : 113 / 273 = 41 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 11 : 127 / 272 = 46 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 12 : 228 / 271 = 84 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 13 : 265 / 275 = 96 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 14 : 268 / 276 = 97 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 15 : 223 / 273 = 81 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 16 : 222 / 274 = 81 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 17 : 234 / 273 = 85 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 18 : 248 / 274 = 90 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 19 : 256 / 272 = 94 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 20 : 246 / 273 = 90 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 21 : 225 / 274 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 22 : 233 / 274 = 85 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 23 : 260 / 276 = 94 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 24 : 227 / 274 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 25 : 258 / 275 = 93 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 26 : 270 / 276 = 97 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 27 : 221 / 275 = 80 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 28 : 174 / 275 = 63 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 29 : 143 / 275 = 52 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 30 : 170 / 276 = 61 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 31 : 241 / 276 = 87 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 32 : 245 / 276 = 88 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 33 : 238 / 276 = 86 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 34 : 239 / 276 = 86 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 35 : 239 / 275 = 86 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 36 : 233 / 276 = 84 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 37 : 248 / 276 = 89 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 38 : 252 / 276 = 91 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 39 : 252 / 276 = 91 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 40 : 189 / 276 = 68 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 41 : 257 / 276 = 93 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 42 : 262 / 275 = 95 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 43 : 179 / 276 = 64 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 44 : 246 / 276 = 89 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 45 : 248 / 276 = 89 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 46 : 222 / 276 = 80 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 47 : 221 / 275 = 80 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 48 : 220 / 275 = 80 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 49 : 226 / 274 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 50 : 241 / 276 = 87 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 51 : 252 / 276 = 91 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 52 : 238 / 276 = 86 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 53 : 236 / 276 = 85 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 54 : 269 / 274 = 98 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 55 : 240 / 276 = 86 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 56 : 243 / 275 = 88 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 57 : 267 / 276 = 96 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 58 : 268 / 273 = 98 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 59 : 267 / 276 = 96 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 60 : 465 / 561 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 61 : 443 / 566 = 78 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 62 : 462 / 572 = 80 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 63 : 519 / 570 = 91 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 64 : 431 / 574 = 75 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 65 : 502 / 573 = 87 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 66 : 412 / 573 = 71 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 67 : 418 / 575 = 72 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 68 : 325 / 575 = 56 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 69 : 469 / 575 = 81 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 70 : 228 / 575 = 39 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 71 : 215 / 575 = 37 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 72 : 141 / 571 = 24 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 73 : 211 / 570 = 37 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 74 : 382 / 569 = 67 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 75 : 247 / 573 = 43 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 76 : 352 / 574 = 61 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 77 : 394 / 573 = 68 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 78 : 408 / 575 = 70 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 79 : 542 / 574 = 94 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 80 : 499 / 573 = 87 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 81 : 323 / 575 = 56 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 82 : 390 / 575 = 67 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 83 : 326 / 572 = 56 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 84 : 431 / 574 = 75 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 85 : 400 / 574 = 69 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 86 : 515 / 575 = 89 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 87 : 490 / 576 = 85 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 88 : 407 / 575 = 70 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 89 : 478 / 576 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 90 : 245 / 574 = 42 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 91 : 471 / 568 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 92 : 353 / 576 = 61 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 93 : 416 / 573 = 72 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 94 : 504 / 574 = 87 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 95 : 520 / 575 = 90 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 96 : 562 / 575 = 97 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 97 : 558 / 574 = 97 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 98 : 543 / 575 = 94 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 99 : 544 / 574 = 94 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 100 : 508 / 574 = 88 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 101 : 509 / 574 = 88 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 102 : 332 / 575 = 57 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 103 : 494 / 576 = 85 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 104 : 272 / 575 = 47 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 105 : 248 / 575 = 43 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 106 : 330 / 576 = 57 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 107 : 483 / 576 = 83 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 108 : 484 / 575 = 84 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 109 : 334 / 575 = 58 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 110 : 475 / 575 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 111 : 534 / 576 = 92 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 112 : 552 / 575 = 96 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 113 : 512 / 576 = 88 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 114 : 489 / 576 = 84 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 115 : 516 / 576 = 89 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 116 : 477 / 575 = 82 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 117 : 452 / 575 = 78 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 118 : 492 / 575 = 85 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 119 : 530 / 576 = 92 %
[ Sat Jul  6 12:03:29 2024 ] Accuracy of 120 : 234 / 274 = 85 %
[ Sat Jul  6 12:03:29 2024 ] Training epoch: 11
[ Sat Jul  6 12:03:29 2024 ] 	Batch(0/7879) done. Loss: 0.1254  lr:0.000010
[ Sat Jul  6 12:03:47 2024 ] 	Batch(100/7879) done. Loss: 0.3923  lr:0.000010
[ Sat Jul  6 12:04:05 2024 ] 	Batch(200/7879) done. Loss: 0.0545  lr:0.000010
[ Sat Jul  6 12:04:23 2024 ] 	Batch(300/7879) done. Loss: 0.6620  lr:0.000010
[ Sat Jul  6 12:04:41 2024 ] 	Batch(400/7879) done. Loss: 0.0148  lr:0.000010
[ Sat Jul  6 12:04:59 2024 ] 
Training: Epoch [10/120], Step [499], Loss: 0.060620199888944626, Training Accuracy: 97.95
[ Sat Jul  6 12:04:59 2024 ] 	Batch(500/7879) done. Loss: 0.0555  lr:0.000010
[ Sat Jul  6 12:05:17 2024 ] 	Batch(600/7879) done. Loss: 0.0986  lr:0.000010
[ Sat Jul  6 12:05:35 2024 ] 	Batch(700/7879) done. Loss: 0.0042  lr:0.000010
[ Sat Jul  6 12:05:53 2024 ] 	Batch(800/7879) done. Loss: 0.0037  lr:0.000010
[ Sat Jul  6 12:06:11 2024 ] 	Batch(900/7879) done. Loss: 0.0938  lr:0.000010
[ Sat Jul  6 12:06:29 2024 ] 
Training: Epoch [10/120], Step [999], Loss: 0.061314165592193604, Training Accuracy: 97.8875
[ Sat Jul  6 12:06:29 2024 ] 	Batch(1000/7879) done. Loss: 0.0301  lr:0.000010
[ Sat Jul  6 12:06:47 2024 ] 	Batch(1100/7879) done. Loss: 0.2254  lr:0.000010
[ Sat Jul  6 12:07:05 2024 ] 	Batch(1200/7879) done. Loss: 0.0275  lr:0.000010
[ Sat Jul  6 12:07:23 2024 ] 	Batch(1300/7879) done. Loss: 0.2290  lr:0.000010
[ Sat Jul  6 12:07:41 2024 ] 	Batch(1400/7879) done. Loss: 0.1171  lr:0.000010
[ Sat Jul  6 12:07:59 2024 ] 
Training: Epoch [10/120], Step [1499], Loss: 0.009741183370351791, Training Accuracy: 97.775
[ Sat Jul  6 12:07:59 2024 ] 	Batch(1500/7879) done. Loss: 0.0300  lr:0.000010
[ Sat Jul  6 12:08:17 2024 ] 	Batch(1600/7879) done. Loss: 0.0844  lr:0.000010
[ Sat Jul  6 12:08:35 2024 ] 	Batch(1700/7879) done. Loss: 0.0247  lr:0.000010
[ Sat Jul  6 12:08:53 2024 ] 	Batch(1800/7879) done. Loss: 0.0082  lr:0.000010
[ Sat Jul  6 12:09:11 2024 ] 	Batch(1900/7879) done. Loss: 0.3235  lr:0.000010
[ Sat Jul  6 12:09:28 2024 ] 
Training: Epoch [10/120], Step [1999], Loss: 0.02271134965121746, Training Accuracy: 97.7125
[ Sat Jul  6 12:09:29 2024 ] 	Batch(2000/7879) done. Loss: 0.0759  lr:0.000010
[ Sat Jul  6 12:09:47 2024 ] 	Batch(2100/7879) done. Loss: 0.1470  lr:0.000010
[ Sat Jul  6 12:10:05 2024 ] 	Batch(2200/7879) done. Loss: 0.0621  lr:0.000010
[ Sat Jul  6 12:10:22 2024 ] 	Batch(2300/7879) done. Loss: 0.0398  lr:0.000010
[ Sat Jul  6 12:10:40 2024 ] 	Batch(2400/7879) done. Loss: 0.0384  lr:0.000010
[ Sat Jul  6 12:10:58 2024 ] 
Training: Epoch [10/120], Step [2499], Loss: 0.17040382325649261, Training Accuracy: 97.705
[ Sat Jul  6 12:10:58 2024 ] 	Batch(2500/7879) done. Loss: 0.0034  lr:0.000010
[ Sat Jul  6 12:11:16 2024 ] 	Batch(2600/7879) done. Loss: 0.0607  lr:0.000010
[ Sat Jul  6 12:11:34 2024 ] 	Batch(2700/7879) done. Loss: 0.1670  lr:0.000010
[ Sat Jul  6 12:11:52 2024 ] 	Batch(2800/7879) done. Loss: 0.0084  lr:0.000010
[ Sat Jul  6 12:12:10 2024 ] 	Batch(2900/7879) done. Loss: 0.5772  lr:0.000010
[ Sat Jul  6 12:12:28 2024 ] 
Training: Epoch [10/120], Step [2999], Loss: 0.07626760005950928, Training Accuracy: 97.6375
[ Sat Jul  6 12:12:28 2024 ] 	Batch(3000/7879) done. Loss: 0.0090  lr:0.000010
[ Sat Jul  6 12:12:46 2024 ] 	Batch(3100/7879) done. Loss: 0.0050  lr:0.000010
[ Sat Jul  6 12:13:04 2024 ] 	Batch(3200/7879) done. Loss: 0.0089  lr:0.000010
[ Sat Jul  6 12:13:22 2024 ] 	Batch(3300/7879) done. Loss: 0.1236  lr:0.000010
[ Sat Jul  6 12:13:40 2024 ] 	Batch(3400/7879) done. Loss: 0.0011  lr:0.000010
[ Sat Jul  6 12:13:57 2024 ] 
Training: Epoch [10/120], Step [3499], Loss: 0.11982449889183044, Training Accuracy: 97.65357142857142
[ Sat Jul  6 12:13:58 2024 ] 	Batch(3500/7879) done. Loss: 0.0730  lr:0.000010
[ Sat Jul  6 12:14:16 2024 ] 	Batch(3600/7879) done. Loss: 0.0121  lr:0.000010
[ Sat Jul  6 12:14:35 2024 ] 	Batch(3700/7879) done. Loss: 0.0031  lr:0.000010
[ Sat Jul  6 12:14:53 2024 ] 	Batch(3800/7879) done. Loss: 0.0323  lr:0.000010
[ Sat Jul  6 12:15:12 2024 ] 	Batch(3900/7879) done. Loss: 0.1358  lr:0.000010
[ Sat Jul  6 12:15:31 2024 ] 
Training: Epoch [10/120], Step [3999], Loss: 0.006252960301935673, Training Accuracy: 97.725
[ Sat Jul  6 12:15:31 2024 ] 	Batch(4000/7879) done. Loss: 0.0875  lr:0.000010
[ Sat Jul  6 12:15:50 2024 ] 	Batch(4100/7879) done. Loss: 0.0647  lr:0.000010
[ Sat Jul  6 12:16:09 2024 ] 	Batch(4200/7879) done. Loss: 0.0128  lr:0.000010
[ Sat Jul  6 12:16:27 2024 ] 	Batch(4300/7879) done. Loss: 0.0311  lr:0.000010
[ Sat Jul  6 12:16:46 2024 ] 	Batch(4400/7879) done. Loss: 0.0210  lr:0.000010
[ Sat Jul  6 12:17:04 2024 ] 
Training: Epoch [10/120], Step [4499], Loss: 0.04080897197127342, Training Accuracy: 97.73888888888888
[ Sat Jul  6 12:17:05 2024 ] 	Batch(4500/7879) done. Loss: 0.1019  lr:0.000010
[ Sat Jul  6 12:17:23 2024 ] 	Batch(4600/7879) done. Loss: 0.1254  lr:0.000010
[ Sat Jul  6 12:17:42 2024 ] 	Batch(4700/7879) done. Loss: 0.0337  lr:0.000010
[ Sat Jul  6 12:18:00 2024 ] 	Batch(4800/7879) done. Loss: 0.0033  lr:0.000010
[ Sat Jul  6 12:18:18 2024 ] 	Batch(4900/7879) done. Loss: 0.0196  lr:0.000010
[ Sat Jul  6 12:18:36 2024 ] 
Training: Epoch [10/120], Step [4999], Loss: 0.10240903496742249, Training Accuracy: 97.69
[ Sat Jul  6 12:18:36 2024 ] 	Batch(5000/7879) done. Loss: 0.0070  lr:0.000010
[ Sat Jul  6 12:18:54 2024 ] 	Batch(5100/7879) done. Loss: 0.0264  lr:0.000010
[ Sat Jul  6 12:19:12 2024 ] 	Batch(5200/7879) done. Loss: 0.1161  lr:0.000010
[ Sat Jul  6 12:19:31 2024 ] 	Batch(5300/7879) done. Loss: 0.2833  lr:0.000010
[ Sat Jul  6 12:19:49 2024 ] 	Batch(5400/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 12:20:07 2024 ] 
Training: Epoch [10/120], Step [5499], Loss: 0.08575990051031113, Training Accuracy: 97.68863636363636
[ Sat Jul  6 12:20:07 2024 ] 	Batch(5500/7879) done. Loss: 0.2335  lr:0.000010
[ Sat Jul  6 12:20:25 2024 ] 	Batch(5600/7879) done. Loss: 0.0242  lr:0.000010
[ Sat Jul  6 12:20:43 2024 ] 	Batch(5700/7879) done. Loss: 0.1381  lr:0.000010
[ Sat Jul  6 12:21:01 2024 ] 	Batch(5800/7879) done. Loss: 0.1218  lr:0.000010
[ Sat Jul  6 12:21:19 2024 ] 	Batch(5900/7879) done. Loss: 0.0020  lr:0.000010
[ Sat Jul  6 12:21:37 2024 ] 
Training: Epoch [10/120], Step [5999], Loss: 0.009839319624006748, Training Accuracy: 97.67291666666667
[ Sat Jul  6 12:21:37 2024 ] 	Batch(6000/7879) done. Loss: 0.1019  lr:0.000010
[ Sat Jul  6 12:21:55 2024 ] 	Batch(6100/7879) done. Loss: 0.3555  lr:0.000010
[ Sat Jul  6 12:22:13 2024 ] 	Batch(6200/7879) done. Loss: 0.0240  lr:0.000010
[ Sat Jul  6 12:22:31 2024 ] 	Batch(6300/7879) done. Loss: 0.1302  lr:0.000010
[ Sat Jul  6 12:22:49 2024 ] 	Batch(6400/7879) done. Loss: 0.0657  lr:0.000010
[ Sat Jul  6 12:23:06 2024 ] 
Training: Epoch [10/120], Step [6499], Loss: 0.0631546601653099, Training Accuracy: 97.67692307692307
[ Sat Jul  6 12:23:07 2024 ] 	Batch(6500/7879) done. Loss: 0.0278  lr:0.000010
[ Sat Jul  6 12:23:25 2024 ] 	Batch(6600/7879) done. Loss: 0.0087  lr:0.000010
[ Sat Jul  6 12:23:43 2024 ] 	Batch(6700/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 12:24:01 2024 ] 	Batch(6800/7879) done. Loss: 0.0169  lr:0.000010
[ Sat Jul  6 12:24:20 2024 ] 	Batch(6900/7879) done. Loss: 0.0078  lr:0.000010
[ Sat Jul  6 12:24:38 2024 ] 
Training: Epoch [10/120], Step [6999], Loss: 0.07174456119537354, Training Accuracy: 97.66964285714286
[ Sat Jul  6 12:24:38 2024 ] 	Batch(7000/7879) done. Loss: 0.0043  lr:0.000010
[ Sat Jul  6 12:24:56 2024 ] 	Batch(7100/7879) done. Loss: 0.0041  lr:0.000010
[ Sat Jul  6 12:25:14 2024 ] 	Batch(7200/7879) done. Loss: 0.0976  lr:0.000010
[ Sat Jul  6 12:25:32 2024 ] 	Batch(7300/7879) done. Loss: 0.0411  lr:0.000010
[ Sat Jul  6 12:25:50 2024 ] 	Batch(7400/7879) done. Loss: 0.3438  lr:0.000010
[ Sat Jul  6 12:26:08 2024 ] 
Training: Epoch [10/120], Step [7499], Loss: 0.07902497798204422, Training Accuracy: 97.66166666666666
[ Sat Jul  6 12:26:08 2024 ] 	Batch(7500/7879) done. Loss: 0.0099  lr:0.000010
[ Sat Jul  6 12:26:26 2024 ] 	Batch(7600/7879) done. Loss: 0.0640  lr:0.000010
[ Sat Jul  6 12:26:45 2024 ] 	Batch(7700/7879) done. Loss: 0.0062  lr:0.000010
[ Sat Jul  6 12:27:03 2024 ] 	Batch(7800/7879) done. Loss: 0.0125  lr:0.000010
[ Sat Jul  6 12:27:18 2024 ] 	Mean training loss: 0.0938.
[ Sat Jul  6 12:27:18 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 12:27:18 2024 ] Training epoch: 12
[ Sat Jul  6 12:27:19 2024 ] 	Batch(0/7879) done. Loss: 0.0932  lr:0.000010
[ Sat Jul  6 12:27:37 2024 ] 	Batch(100/7879) done. Loss: 0.0149  lr:0.000010
[ Sat Jul  6 12:27:55 2024 ] 	Batch(200/7879) done. Loss: 0.0378  lr:0.000010
[ Sat Jul  6 12:28:13 2024 ] 	Batch(300/7879) done. Loss: 0.0138  lr:0.000010
[ Sat Jul  6 12:28:31 2024 ] 	Batch(400/7879) done. Loss: 0.0376  lr:0.000010
[ Sat Jul  6 12:28:48 2024 ] 
Training: Epoch [11/120], Step [499], Loss: 0.2103620171546936, Training Accuracy: 97.7
[ Sat Jul  6 12:28:49 2024 ] 	Batch(500/7879) done. Loss: 0.0259  lr:0.000010
[ Sat Jul  6 12:29:06 2024 ] 	Batch(600/7879) done. Loss: 0.2824  lr:0.000010
[ Sat Jul  6 12:29:24 2024 ] 	Batch(700/7879) done. Loss: 0.0054  lr:0.000010
[ Sat Jul  6 12:29:42 2024 ] 	Batch(800/7879) done. Loss: 0.4186  lr:0.000010
[ Sat Jul  6 12:30:00 2024 ] 	Batch(900/7879) done. Loss: 0.0120  lr:0.000010
[ Sat Jul  6 12:30:18 2024 ] 
Training: Epoch [11/120], Step [999], Loss: 0.41679850220680237, Training Accuracy: 97.675
[ Sat Jul  6 12:30:18 2024 ] 	Batch(1000/7879) done. Loss: 0.0087  lr:0.000010
[ Sat Jul  6 12:30:36 2024 ] 	Batch(1100/7879) done. Loss: 0.1116  lr:0.000010
[ Sat Jul  6 12:30:54 2024 ] 	Batch(1200/7879) done. Loss: 0.0096  lr:0.000010
[ Sat Jul  6 12:31:12 2024 ] 	Batch(1300/7879) done. Loss: 0.0555  lr:0.000010
[ Sat Jul  6 12:31:30 2024 ] 	Batch(1400/7879) done. Loss: 0.0078  lr:0.000010
[ Sat Jul  6 12:31:48 2024 ] 
Training: Epoch [11/120], Step [1499], Loss: 0.09669927507638931, Training Accuracy: 97.76666666666667
[ Sat Jul  6 12:31:48 2024 ] 	Batch(1500/7879) done. Loss: 0.0557  lr:0.000010
[ Sat Jul  6 12:32:06 2024 ] 	Batch(1600/7879) done. Loss: 0.0033  lr:0.000010
[ Sat Jul  6 12:32:25 2024 ] 	Batch(1700/7879) done. Loss: 0.0071  lr:0.000010
[ Sat Jul  6 12:32:43 2024 ] 	Batch(1800/7879) done. Loss: 0.0876  lr:0.000010
[ Sat Jul  6 12:33:02 2024 ] 	Batch(1900/7879) done. Loss: 0.5133  lr:0.000010
[ Sat Jul  6 12:33:20 2024 ] 
Training: Epoch [11/120], Step [1999], Loss: 0.01648028753697872, Training Accuracy: 97.7625
[ Sat Jul  6 12:33:20 2024 ] 	Batch(2000/7879) done. Loss: 0.1169  lr:0.000010
[ Sat Jul  6 12:33:39 2024 ] 	Batch(2100/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 12:33:58 2024 ] 	Batch(2200/7879) done. Loss: 0.0270  lr:0.000010
[ Sat Jul  6 12:34:16 2024 ] 	Batch(2300/7879) done. Loss: 0.0318  lr:0.000010
[ Sat Jul  6 12:34:35 2024 ] 	Batch(2400/7879) done. Loss: 0.0371  lr:0.000010
[ Sat Jul  6 12:34:53 2024 ] 
Training: Epoch [11/120], Step [2499], Loss: 0.04376136139035225, Training Accuracy: 97.705
[ Sat Jul  6 12:34:53 2024 ] 	Batch(2500/7879) done. Loss: 0.2673  lr:0.000010
[ Sat Jul  6 12:35:11 2024 ] 	Batch(2600/7879) done. Loss: 0.0080  lr:0.000010
[ Sat Jul  6 12:35:29 2024 ] 	Batch(2700/7879) done. Loss: 0.0995  lr:0.000010
[ Sat Jul  6 12:35:47 2024 ] 	Batch(2800/7879) done. Loss: 0.0280  lr:0.000010
[ Sat Jul  6 12:36:05 2024 ] 	Batch(2900/7879) done. Loss: 0.0295  lr:0.000010
[ Sat Jul  6 12:36:22 2024 ] 
Training: Epoch [11/120], Step [2999], Loss: 0.2975570261478424, Training Accuracy: 97.64583333333333
[ Sat Jul  6 12:36:23 2024 ] 	Batch(3000/7879) done. Loss: 0.0577  lr:0.000010
[ Sat Jul  6 12:36:41 2024 ] 	Batch(3100/7879) done. Loss: 0.0045  lr:0.000010
[ Sat Jul  6 12:36:59 2024 ] 	Batch(3200/7879) done. Loss: 0.0689  lr:0.000010
[ Sat Jul  6 12:37:17 2024 ] 	Batch(3300/7879) done. Loss: 0.1965  lr:0.000010
[ Sat Jul  6 12:37:35 2024 ] 	Batch(3400/7879) done. Loss: 0.0309  lr:0.000010
[ Sat Jul  6 12:37:52 2024 ] 
Training: Epoch [11/120], Step [3499], Loss: 0.003077549859881401, Training Accuracy: 97.71428571428571
[ Sat Jul  6 12:37:52 2024 ] 	Batch(3500/7879) done. Loss: 0.1009  lr:0.000010
[ Sat Jul  6 12:38:11 2024 ] 	Batch(3600/7879) done. Loss: 0.0243  lr:0.000010
[ Sat Jul  6 12:38:28 2024 ] 	Batch(3700/7879) done. Loss: 0.0433  lr:0.000010
[ Sat Jul  6 12:38:46 2024 ] 	Batch(3800/7879) done. Loss: 0.6239  lr:0.000010
[ Sat Jul  6 12:39:04 2024 ] 	Batch(3900/7879) done. Loss: 0.3354  lr:0.000010
[ Sat Jul  6 12:39:22 2024 ] 
Training: Epoch [11/120], Step [3999], Loss: 0.030275583267211914, Training Accuracy: 97.728125
[ Sat Jul  6 12:39:22 2024 ] 	Batch(4000/7879) done. Loss: 0.2643  lr:0.000010
[ Sat Jul  6 12:39:40 2024 ] 	Batch(4100/7879) done. Loss: 0.0598  lr:0.000010
[ Sat Jul  6 12:39:58 2024 ] 	Batch(4200/7879) done. Loss: 0.1385  lr:0.000010
[ Sat Jul  6 12:40:16 2024 ] 	Batch(4300/7879) done. Loss: 0.7082  lr:0.000010
[ Sat Jul  6 12:40:34 2024 ] 	Batch(4400/7879) done. Loss: 0.0140  lr:0.000010
[ Sat Jul  6 12:40:52 2024 ] 
Training: Epoch [11/120], Step [4499], Loss: 0.20922783017158508, Training Accuracy: 97.74722222222222
[ Sat Jul  6 12:40:52 2024 ] 	Batch(4500/7879) done. Loss: 0.0151  lr:0.000010
[ Sat Jul  6 12:41:10 2024 ] 	Batch(4600/7879) done. Loss: 0.0090  lr:0.000010
[ Sat Jul  6 12:41:28 2024 ] 	Batch(4700/7879) done. Loss: 0.0358  lr:0.000010
[ Sat Jul  6 12:41:46 2024 ] 	Batch(4800/7879) done. Loss: 0.0130  lr:0.000010
[ Sat Jul  6 12:42:04 2024 ] 	Batch(4900/7879) done. Loss: 0.0479  lr:0.000010
[ Sat Jul  6 12:42:22 2024 ] 
Training: Epoch [11/120], Step [4999], Loss: 0.007256105076521635, Training Accuracy: 97.78750000000001
[ Sat Jul  6 12:42:22 2024 ] 	Batch(5000/7879) done. Loss: 0.0981  lr:0.000010
[ Sat Jul  6 12:42:40 2024 ] 	Batch(5100/7879) done. Loss: 0.0100  lr:0.000010
[ Sat Jul  6 12:42:58 2024 ] 	Batch(5200/7879) done. Loss: 0.0119  lr:0.000010
[ Sat Jul  6 12:43:16 2024 ] 	Batch(5300/7879) done. Loss: 0.0734  lr:0.000010
[ Sat Jul  6 12:43:34 2024 ] 	Batch(5400/7879) done. Loss: 0.3859  lr:0.000010
[ Sat Jul  6 12:43:52 2024 ] 
Training: Epoch [11/120], Step [5499], Loss: 0.2112332582473755, Training Accuracy: 97.76818181818182
[ Sat Jul  6 12:43:52 2024 ] 	Batch(5500/7879) done. Loss: 0.3177  lr:0.000010
[ Sat Jul  6 12:44:10 2024 ] 	Batch(5600/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 12:44:28 2024 ] 	Batch(5700/7879) done. Loss: 0.1290  lr:0.000010
[ Sat Jul  6 12:44:46 2024 ] 	Batch(5800/7879) done. Loss: 0.0123  lr:0.000010
[ Sat Jul  6 12:45:04 2024 ] 	Batch(5900/7879) done. Loss: 0.0674  lr:0.000010
[ Sat Jul  6 12:45:22 2024 ] 
Training: Epoch [11/120], Step [5999], Loss: 0.2704942524433136, Training Accuracy: 97.74791666666667
[ Sat Jul  6 12:45:22 2024 ] 	Batch(6000/7879) done. Loss: 0.1918  lr:0.000010
[ Sat Jul  6 12:45:40 2024 ] 	Batch(6100/7879) done. Loss: 0.3249  lr:0.000010
[ Sat Jul  6 12:45:59 2024 ] 	Batch(6200/7879) done. Loss: 0.0050  lr:0.000010
[ Sat Jul  6 12:46:17 2024 ] 	Batch(6300/7879) done. Loss: 0.0232  lr:0.000010
[ Sat Jul  6 12:46:35 2024 ] 	Batch(6400/7879) done. Loss: 0.0050  lr:0.000010
[ Sat Jul  6 12:46:53 2024 ] 
Training: Epoch [11/120], Step [6499], Loss: 0.013794050551950932, Training Accuracy: 97.74038461538461
[ Sat Jul  6 12:46:53 2024 ] 	Batch(6500/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 12:47:11 2024 ] 	Batch(6600/7879) done. Loss: 0.1304  lr:0.000010
[ Sat Jul  6 12:47:29 2024 ] 	Batch(6700/7879) done. Loss: 0.0476  lr:0.000010
[ Sat Jul  6 12:47:47 2024 ] 	Batch(6800/7879) done. Loss: 0.5524  lr:0.000010
[ Sat Jul  6 12:48:05 2024 ] 	Batch(6900/7879) done. Loss: 0.0407  lr:0.000010
[ Sat Jul  6 12:48:23 2024 ] 
Training: Epoch [11/120], Step [6999], Loss: 0.009732346050441265, Training Accuracy: 97.73392857142858
[ Sat Jul  6 12:48:23 2024 ] 	Batch(7000/7879) done. Loss: 0.0026  lr:0.000010
[ Sat Jul  6 12:48:41 2024 ] 	Batch(7100/7879) done. Loss: 0.0479  lr:0.000010
[ Sat Jul  6 12:48:59 2024 ] 	Batch(7200/7879) done. Loss: 0.0198  lr:0.000010
[ Sat Jul  6 12:49:17 2024 ] 	Batch(7300/7879) done. Loss: 0.0571  lr:0.000010
[ Sat Jul  6 12:49:35 2024 ] 	Batch(7400/7879) done. Loss: 0.0777  lr:0.000010
[ Sat Jul  6 12:49:53 2024 ] 
Training: Epoch [11/120], Step [7499], Loss: 0.02798592671751976, Training Accuracy: 97.745
[ Sat Jul  6 12:49:53 2024 ] 	Batch(7500/7879) done. Loss: 0.1959  lr:0.000010
[ Sat Jul  6 12:50:11 2024 ] 	Batch(7600/7879) done. Loss: 0.3174  lr:0.000010
[ Sat Jul  6 12:50:29 2024 ] 	Batch(7700/7879) done. Loss: 0.0338  lr:0.000010
[ Sat Jul  6 12:50:47 2024 ] 	Batch(7800/7879) done. Loss: 0.0378  lr:0.000010
[ Sat Jul  6 12:51:01 2024 ] 	Mean training loss: 0.0895.
[ Sat Jul  6 12:51:01 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 12:51:01 2024 ] Training epoch: 13
[ Sat Jul  6 12:51:02 2024 ] 	Batch(0/7879) done. Loss: 0.0147  lr:0.000010
[ Sat Jul  6 12:51:20 2024 ] 	Batch(100/7879) done. Loss: 0.0613  lr:0.000010
[ Sat Jul  6 12:51:38 2024 ] 	Batch(200/7879) done. Loss: 0.0050  lr:0.000010
[ Sat Jul  6 12:51:56 2024 ] 	Batch(300/7879) done. Loss: 0.0714  lr:0.000010
[ Sat Jul  6 12:52:14 2024 ] 	Batch(400/7879) done. Loss: 0.0550  lr:0.000010
[ Sat Jul  6 12:52:32 2024 ] 
Training: Epoch [12/120], Step [499], Loss: 0.003987081348896027, Training Accuracy: 98.0
[ Sat Jul  6 12:52:32 2024 ] 	Batch(500/7879) done. Loss: 0.0057  lr:0.000010
[ Sat Jul  6 12:52:50 2024 ] 	Batch(600/7879) done. Loss: 0.0664  lr:0.000010
[ Sat Jul  6 12:53:08 2024 ] 	Batch(700/7879) done. Loss: 0.5040  lr:0.000010
[ Sat Jul  6 12:53:26 2024 ] 	Batch(800/7879) done. Loss: 0.0240  lr:0.000010
[ Sat Jul  6 12:53:44 2024 ] 	Batch(900/7879) done. Loss: 0.3248  lr:0.000010
[ Sat Jul  6 12:54:01 2024 ] 
Training: Epoch [12/120], Step [999], Loss: 0.005688495002686977, Training Accuracy: 97.8375
[ Sat Jul  6 12:54:02 2024 ] 	Batch(1000/7879) done. Loss: 0.1882  lr:0.000010
[ Sat Jul  6 12:54:20 2024 ] 	Batch(1100/7879) done. Loss: 0.0085  lr:0.000010
[ Sat Jul  6 12:54:38 2024 ] 	Batch(1200/7879) done. Loss: 0.0260  lr:0.000010
[ Sat Jul  6 12:54:56 2024 ] 	Batch(1300/7879) done. Loss: 0.1598  lr:0.000010
[ Sat Jul  6 12:55:14 2024 ] 	Batch(1400/7879) done. Loss: 0.0866  lr:0.000010
[ Sat Jul  6 12:55:31 2024 ] 
Training: Epoch [12/120], Step [1499], Loss: 0.04932507872581482, Training Accuracy: 97.78333333333333
[ Sat Jul  6 12:55:32 2024 ] 	Batch(1500/7879) done. Loss: 0.0417  lr:0.000010
[ Sat Jul  6 12:55:50 2024 ] 	Batch(1600/7879) done. Loss: 0.0621  lr:0.000010
[ Sat Jul  6 12:56:08 2024 ] 	Batch(1700/7879) done. Loss: 0.0380  lr:0.000010
[ Sat Jul  6 12:56:26 2024 ] 	Batch(1800/7879) done. Loss: 0.0958  lr:0.000010
[ Sat Jul  6 12:56:44 2024 ] 	Batch(1900/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 12:57:01 2024 ] 
Training: Epoch [12/120], Step [1999], Loss: 0.026811566203832626, Training Accuracy: 97.74374999999999
[ Sat Jul  6 12:57:02 2024 ] 	Batch(2000/7879) done. Loss: 0.0179  lr:0.000010
[ Sat Jul  6 12:57:20 2024 ] 	Batch(2100/7879) done. Loss: 0.0217  lr:0.000010
[ Sat Jul  6 12:57:38 2024 ] 	Batch(2200/7879) done. Loss: 0.0053  lr:0.000010
[ Sat Jul  6 12:57:55 2024 ] 	Batch(2300/7879) done. Loss: 0.0112  lr:0.000010
[ Sat Jul  6 12:58:14 2024 ] 	Batch(2400/7879) done. Loss: 0.0217  lr:0.000010
[ Sat Jul  6 12:58:31 2024 ] 
Training: Epoch [12/120], Step [2499], Loss: 0.014676675200462341, Training Accuracy: 97.775
[ Sat Jul  6 12:58:31 2024 ] 	Batch(2500/7879) done. Loss: 0.0622  lr:0.000010
[ Sat Jul  6 12:58:49 2024 ] 	Batch(2600/7879) done. Loss: 0.0984  lr:0.000010
[ Sat Jul  6 12:59:07 2024 ] 	Batch(2700/7879) done. Loss: 0.0273  lr:0.000010
[ Sat Jul  6 12:59:25 2024 ] 	Batch(2800/7879) done. Loss: 0.0355  lr:0.000010
[ Sat Jul  6 12:59:44 2024 ] 	Batch(2900/7879) done. Loss: 0.1868  lr:0.000010
[ Sat Jul  6 13:00:02 2024 ] 
Training: Epoch [12/120], Step [2999], Loss: 0.0029888260178267956, Training Accuracy: 97.84166666666667
[ Sat Jul  6 13:00:03 2024 ] 	Batch(3000/7879) done. Loss: 0.0219  lr:0.000010
[ Sat Jul  6 13:00:21 2024 ] 	Batch(3100/7879) done. Loss: 0.5847  lr:0.000010
[ Sat Jul  6 13:00:40 2024 ] 	Batch(3200/7879) done. Loss: 0.0463  lr:0.000010
[ Sat Jul  6 13:00:58 2024 ] 	Batch(3300/7879) done. Loss: 0.0124  lr:0.000010
[ Sat Jul  6 13:01:16 2024 ] 	Batch(3400/7879) done. Loss: 0.0430  lr:0.000010
[ Sat Jul  6 13:01:34 2024 ] 
Training: Epoch [12/120], Step [3499], Loss: 0.007319694384932518, Training Accuracy: 97.79642857142858
[ Sat Jul  6 13:01:34 2024 ] 	Batch(3500/7879) done. Loss: 0.1492  lr:0.000010
[ Sat Jul  6 13:01:52 2024 ] 	Batch(3600/7879) done. Loss: 0.0941  lr:0.000010
[ Sat Jul  6 13:02:10 2024 ] 	Batch(3700/7879) done. Loss: 0.0250  lr:0.000010
[ Sat Jul  6 13:02:28 2024 ] 	Batch(3800/7879) done. Loss: 0.0277  lr:0.000010
[ Sat Jul  6 13:02:46 2024 ] 	Batch(3900/7879) done. Loss: 0.0101  lr:0.000010
[ Sat Jul  6 13:03:03 2024 ] 
Training: Epoch [12/120], Step [3999], Loss: 0.011984257027506828, Training Accuracy: 97.80312500000001
[ Sat Jul  6 13:03:04 2024 ] 	Batch(4000/7879) done. Loss: 0.0192  lr:0.000010
[ Sat Jul  6 13:03:22 2024 ] 	Batch(4100/7879) done. Loss: 0.1191  lr:0.000010
[ Sat Jul  6 13:03:40 2024 ] 	Batch(4200/7879) done. Loss: 0.0165  lr:0.000010
[ Sat Jul  6 13:03:58 2024 ] 	Batch(4300/7879) done. Loss: 0.0536  lr:0.000010
[ Sat Jul  6 13:04:16 2024 ] 	Batch(4400/7879) done. Loss: 0.0144  lr:0.000010
[ Sat Jul  6 13:04:34 2024 ] 
Training: Epoch [12/120], Step [4499], Loss: 0.007255396340042353, Training Accuracy: 97.82777777777778
[ Sat Jul  6 13:04:34 2024 ] 	Batch(4500/7879) done. Loss: 0.0542  lr:0.000010
[ Sat Jul  6 13:04:53 2024 ] 	Batch(4600/7879) done. Loss: 0.0550  lr:0.000010
[ Sat Jul  6 13:05:11 2024 ] 	Batch(4700/7879) done. Loss: 0.0084  lr:0.000010
[ Sat Jul  6 13:05:29 2024 ] 	Batch(4800/7879) done. Loss: 0.1582  lr:0.000010
[ Sat Jul  6 13:05:47 2024 ] 	Batch(4900/7879) done. Loss: 0.0145  lr:0.000010
[ Sat Jul  6 13:06:05 2024 ] 
Training: Epoch [12/120], Step [4999], Loss: 0.2444286197423935, Training Accuracy: 97.78999999999999
[ Sat Jul  6 13:06:05 2024 ] 	Batch(5000/7879) done. Loss: 0.0031  lr:0.000010
[ Sat Jul  6 13:06:23 2024 ] 	Batch(5100/7879) done. Loss: 0.2567  lr:0.000010
[ Sat Jul  6 13:06:41 2024 ] 	Batch(5200/7879) done. Loss: 0.1114  lr:0.000010
[ Sat Jul  6 13:07:00 2024 ] 	Batch(5300/7879) done. Loss: 0.2189  lr:0.000010
[ Sat Jul  6 13:07:18 2024 ] 	Batch(5400/7879) done. Loss: 0.0694  lr:0.000010
[ Sat Jul  6 13:07:37 2024 ] 
Training: Epoch [12/120], Step [5499], Loss: 0.012862324714660645, Training Accuracy: 97.77045454545454
[ Sat Jul  6 13:07:37 2024 ] 	Batch(5500/7879) done. Loss: 0.0446  lr:0.000010
[ Sat Jul  6 13:07:56 2024 ] 	Batch(5600/7879) done. Loss: 0.0258  lr:0.000010
[ Sat Jul  6 13:08:14 2024 ] 	Batch(5700/7879) done. Loss: 0.0553  lr:0.000010
[ Sat Jul  6 13:08:32 2024 ] 	Batch(5800/7879) done. Loss: 0.0024  lr:0.000010
[ Sat Jul  6 13:08:50 2024 ] 	Batch(5900/7879) done. Loss: 0.0352  lr:0.000010
[ Sat Jul  6 13:09:07 2024 ] 
Training: Epoch [12/120], Step [5999], Loss: 0.1562480479478836, Training Accuracy: 97.76041666666667
[ Sat Jul  6 13:09:08 2024 ] 	Batch(6000/7879) done. Loss: 0.1507  lr:0.000010
[ Sat Jul  6 13:09:26 2024 ] 	Batch(6100/7879) done. Loss: 0.0276  lr:0.000010
[ Sat Jul  6 13:09:44 2024 ] 	Batch(6200/7879) done. Loss: 0.1228  lr:0.000010
[ Sat Jul  6 13:10:01 2024 ] 	Batch(6300/7879) done. Loss: 0.1096  lr:0.000010
[ Sat Jul  6 13:10:20 2024 ] 	Batch(6400/7879) done. Loss: 0.0973  lr:0.000010
[ Sat Jul  6 13:10:37 2024 ] 
Training: Epoch [12/120], Step [6499], Loss: 0.023597348481416702, Training Accuracy: 97.74615384615385
[ Sat Jul  6 13:10:38 2024 ] 	Batch(6500/7879) done. Loss: 0.0456  lr:0.000010
[ Sat Jul  6 13:10:55 2024 ] 	Batch(6600/7879) done. Loss: 0.1742  lr:0.000010
[ Sat Jul  6 13:11:13 2024 ] 	Batch(6700/7879) done. Loss: 0.0403  lr:0.000010
[ Sat Jul  6 13:11:32 2024 ] 	Batch(6800/7879) done. Loss: 0.0793  lr:0.000010
[ Sat Jul  6 13:11:50 2024 ] 	Batch(6900/7879) done. Loss: 0.0298  lr:0.000010
[ Sat Jul  6 13:12:09 2024 ] 
Training: Epoch [12/120], Step [6999], Loss: 0.002342652529478073, Training Accuracy: 97.76428571428572
[ Sat Jul  6 13:12:09 2024 ] 	Batch(7000/7879) done. Loss: 0.0480  lr:0.000010
[ Sat Jul  6 13:12:28 2024 ] 	Batch(7100/7879) done. Loss: 0.1271  lr:0.000010
[ Sat Jul  6 13:12:47 2024 ] 	Batch(7200/7879) done. Loss: 0.0510  lr:0.000010
[ Sat Jul  6 13:13:06 2024 ] 	Batch(7300/7879) done. Loss: 0.0188  lr:0.000010
[ Sat Jul  6 13:13:24 2024 ] 	Batch(7400/7879) done. Loss: 0.0018  lr:0.000010
[ Sat Jul  6 13:13:43 2024 ] 
Training: Epoch [12/120], Step [7499], Loss: 0.11348027735948563, Training Accuracy: 97.77333333333334
[ Sat Jul  6 13:13:43 2024 ] 	Batch(7500/7879) done. Loss: 0.0259  lr:0.000010
[ Sat Jul  6 13:14:02 2024 ] 	Batch(7600/7879) done. Loss: 0.0703  lr:0.000010
[ Sat Jul  6 13:14:20 2024 ] 	Batch(7700/7879) done. Loss: 0.0096  lr:0.000010
[ Sat Jul  6 13:14:39 2024 ] 	Batch(7800/7879) done. Loss: 0.1633  lr:0.000010
[ Sat Jul  6 13:14:53 2024 ] 	Mean training loss: 0.0906.
[ Sat Jul  6 13:14:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 13:14:54 2024 ] Training epoch: 14
[ Sat Jul  6 13:14:54 2024 ] 	Batch(0/7879) done. Loss: 0.1108  lr:0.000010
[ Sat Jul  6 13:15:12 2024 ] 	Batch(100/7879) done. Loss: 0.0031  lr:0.000010
[ Sat Jul  6 13:15:30 2024 ] 	Batch(200/7879) done. Loss: 0.0070  lr:0.000010
[ Sat Jul  6 13:15:48 2024 ] 	Batch(300/7879) done. Loss: 0.0036  lr:0.000010
[ Sat Jul  6 13:16:06 2024 ] 	Batch(400/7879) done. Loss: 0.0572  lr:0.000010
[ Sat Jul  6 13:16:24 2024 ] 
Training: Epoch [13/120], Step [499], Loss: 0.5337742567062378, Training Accuracy: 97.8
[ Sat Jul  6 13:16:24 2024 ] 	Batch(500/7879) done. Loss: 0.2144  lr:0.000010
[ Sat Jul  6 13:16:42 2024 ] 	Batch(600/7879) done. Loss: 0.0509  lr:0.000010
[ Sat Jul  6 13:17:00 2024 ] 	Batch(700/7879) done. Loss: 0.0085  lr:0.000010
[ Sat Jul  6 13:17:18 2024 ] 	Batch(800/7879) done. Loss: 0.1405  lr:0.000010
[ Sat Jul  6 13:17:36 2024 ] 	Batch(900/7879) done. Loss: 0.0288  lr:0.000010
[ Sat Jul  6 13:17:54 2024 ] 
Training: Epoch [13/120], Step [999], Loss: 0.1926194578409195, Training Accuracy: 97.8
[ Sat Jul  6 13:17:54 2024 ] 	Batch(1000/7879) done. Loss: 0.0525  lr:0.000010
[ Sat Jul  6 13:18:13 2024 ] 	Batch(1100/7879) done. Loss: 0.0553  lr:0.000010
[ Sat Jul  6 13:18:31 2024 ] 	Batch(1200/7879) done. Loss: 0.0207  lr:0.000010
[ Sat Jul  6 13:18:49 2024 ] 	Batch(1300/7879) done. Loss: 0.0480  lr:0.000010
[ Sat Jul  6 13:19:07 2024 ] 	Batch(1400/7879) done. Loss: 0.2855  lr:0.000010
[ Sat Jul  6 13:19:25 2024 ] 
Training: Epoch [13/120], Step [1499], Loss: 0.03275096416473389, Training Accuracy: 97.88333333333334
[ Sat Jul  6 13:19:25 2024 ] 	Batch(1500/7879) done. Loss: 0.1334  lr:0.000010
[ Sat Jul  6 13:19:43 2024 ] 	Batch(1600/7879) done. Loss: 0.1268  lr:0.000010
[ Sat Jul  6 13:20:01 2024 ] 	Batch(1700/7879) done. Loss: 0.0376  lr:0.000010
[ Sat Jul  6 13:20:19 2024 ] 	Batch(1800/7879) done. Loss: 0.0878  lr:0.000010
[ Sat Jul  6 13:20:37 2024 ] 	Batch(1900/7879) done. Loss: 0.0011  lr:0.000010
[ Sat Jul  6 13:20:54 2024 ] 
Training: Epoch [13/120], Step [1999], Loss: 0.026465382426977158, Training Accuracy: 97.85000000000001
[ Sat Jul  6 13:20:55 2024 ] 	Batch(2000/7879) done. Loss: 0.0014  lr:0.000010
[ Sat Jul  6 13:21:13 2024 ] 	Batch(2100/7879) done. Loss: 0.0370  lr:0.000010
[ Sat Jul  6 13:21:31 2024 ] 	Batch(2200/7879) done. Loss: 0.0207  lr:0.000010
[ Sat Jul  6 13:21:48 2024 ] 	Batch(2300/7879) done. Loss: 0.0017  lr:0.000010
[ Sat Jul  6 13:22:07 2024 ] 	Batch(2400/7879) done. Loss: 0.0183  lr:0.000010
[ Sat Jul  6 13:22:24 2024 ] 
Training: Epoch [13/120], Step [2499], Loss: 0.1251305639743805, Training Accuracy: 97.81
[ Sat Jul  6 13:22:24 2024 ] 	Batch(2500/7879) done. Loss: 0.0310  lr:0.000010
[ Sat Jul  6 13:22:42 2024 ] 	Batch(2600/7879) done. Loss: 0.0232  lr:0.000010
[ Sat Jul  6 13:23:00 2024 ] 	Batch(2700/7879) done. Loss: 0.0252  lr:0.000010
[ Sat Jul  6 13:23:18 2024 ] 	Batch(2800/7879) done. Loss: 0.2802  lr:0.000010
[ Sat Jul  6 13:23:37 2024 ] 	Batch(2900/7879) done. Loss: 0.2679  lr:0.000010
[ Sat Jul  6 13:23:56 2024 ] 
Training: Epoch [13/120], Step [2999], Loss: 0.015112238004803658, Training Accuracy: 97.7625
[ Sat Jul  6 13:23:56 2024 ] 	Batch(3000/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 13:24:14 2024 ] 	Batch(3100/7879) done. Loss: 0.7103  lr:0.000010
[ Sat Jul  6 13:24:33 2024 ] 	Batch(3200/7879) done. Loss: 0.2117  lr:0.000010
[ Sat Jul  6 13:24:52 2024 ] 	Batch(3300/7879) done. Loss: 0.0222  lr:0.000010
[ Sat Jul  6 13:25:10 2024 ] 	Batch(3400/7879) done. Loss: 0.0514  lr:0.000010
[ Sat Jul  6 13:25:29 2024 ] 
Training: Epoch [13/120], Step [3499], Loss: 0.0367082916200161, Training Accuracy: 97.75
[ Sat Jul  6 13:25:29 2024 ] 	Batch(3500/7879) done. Loss: 0.1098  lr:0.000010
[ Sat Jul  6 13:25:48 2024 ] 	Batch(3600/7879) done. Loss: 0.0476  lr:0.000010
[ Sat Jul  6 13:26:06 2024 ] 	Batch(3700/7879) done. Loss: 0.0113  lr:0.000010
[ Sat Jul  6 13:26:24 2024 ] 	Batch(3800/7879) done. Loss: 0.0270  lr:0.000010
[ Sat Jul  6 13:26:41 2024 ] 	Batch(3900/7879) done. Loss: 0.0361  lr:0.000010
[ Sat Jul  6 13:26:59 2024 ] 
Training: Epoch [13/120], Step [3999], Loss: 0.052393797785043716, Training Accuracy: 97.75
[ Sat Jul  6 13:27:00 2024 ] 	Batch(4000/7879) done. Loss: 0.0093  lr:0.000010
[ Sat Jul  6 13:27:18 2024 ] 	Batch(4100/7879) done. Loss: 0.0032  lr:0.000010
[ Sat Jul  6 13:27:35 2024 ] 	Batch(4200/7879) done. Loss: 0.2746  lr:0.000010
[ Sat Jul  6 13:27:53 2024 ] 	Batch(4300/7879) done. Loss: 0.0066  lr:0.000010
[ Sat Jul  6 13:28:12 2024 ] 	Batch(4400/7879) done. Loss: 0.0298  lr:0.000010
[ Sat Jul  6 13:28:30 2024 ] 
Training: Epoch [13/120], Step [4499], Loss: 0.0013462875504046679, Training Accuracy: 97.80833333333334
[ Sat Jul  6 13:28:30 2024 ] 	Batch(4500/7879) done. Loss: 0.0072  lr:0.000010
[ Sat Jul  6 13:28:49 2024 ] 	Batch(4600/7879) done. Loss: 0.0668  lr:0.000010
[ Sat Jul  6 13:29:07 2024 ] 	Batch(4700/7879) done. Loss: 0.0628  lr:0.000010
[ Sat Jul  6 13:29:26 2024 ] 	Batch(4800/7879) done. Loss: 0.0787  lr:0.000010
[ Sat Jul  6 13:29:45 2024 ] 	Batch(4900/7879) done. Loss: 0.0144  lr:0.000010
[ Sat Jul  6 13:30:03 2024 ] 
Training: Epoch [13/120], Step [4999], Loss: 0.006639450788497925, Training Accuracy: 97.785
[ Sat Jul  6 13:30:03 2024 ] 	Batch(5000/7879) done. Loss: 0.1410  lr:0.000010
[ Sat Jul  6 13:30:22 2024 ] 	Batch(5100/7879) done. Loss: 0.0161  lr:0.000010
[ Sat Jul  6 13:30:41 2024 ] 	Batch(5200/7879) done. Loss: 0.0012  lr:0.000010
[ Sat Jul  6 13:30:59 2024 ] 	Batch(5300/7879) done. Loss: 0.0342  lr:0.000010
[ Sat Jul  6 13:31:18 2024 ] 	Batch(5400/7879) done. Loss: 0.0530  lr:0.000010
[ Sat Jul  6 13:31:36 2024 ] 
Training: Epoch [13/120], Step [5499], Loss: 0.013184863142669201, Training Accuracy: 97.77045454545454
[ Sat Jul  6 13:31:36 2024 ] 	Batch(5500/7879) done. Loss: 0.0370  lr:0.000010
[ Sat Jul  6 13:31:54 2024 ] 	Batch(5600/7879) done. Loss: 0.0045  lr:0.000010
[ Sat Jul  6 13:32:12 2024 ] 	Batch(5700/7879) done. Loss: 0.0083  lr:0.000010
[ Sat Jul  6 13:32:30 2024 ] 	Batch(5800/7879) done. Loss: 0.0195  lr:0.000010
[ Sat Jul  6 13:32:48 2024 ] 	Batch(5900/7879) done. Loss: 0.1183  lr:0.000010
[ Sat Jul  6 13:33:06 2024 ] 
Training: Epoch [13/120], Step [5999], Loss: 0.2857056260108948, Training Accuracy: 97.79166666666667
[ Sat Jul  6 13:33:06 2024 ] 	Batch(6000/7879) done. Loss: 0.0811  lr:0.000010
[ Sat Jul  6 13:33:24 2024 ] 	Batch(6100/7879) done. Loss: 0.3508  lr:0.000010
[ Sat Jul  6 13:33:42 2024 ] 	Batch(6200/7879) done. Loss: 0.0053  lr:0.000010
[ Sat Jul  6 13:34:00 2024 ] 	Batch(6300/7879) done. Loss: 0.0624  lr:0.000010
[ Sat Jul  6 13:34:18 2024 ] 	Batch(6400/7879) done. Loss: 0.0274  lr:0.000010
[ Sat Jul  6 13:34:36 2024 ] 
Training: Epoch [13/120], Step [6499], Loss: 0.16122286021709442, Training Accuracy: 97.79038461538462
[ Sat Jul  6 13:34:36 2024 ] 	Batch(6500/7879) done. Loss: 0.0014  lr:0.000010
[ Sat Jul  6 13:34:55 2024 ] 	Batch(6600/7879) done. Loss: 0.4007  lr:0.000010
[ Sat Jul  6 13:35:14 2024 ] 	Batch(6700/7879) done. Loss: 0.0176  lr:0.000010
[ Sat Jul  6 13:35:32 2024 ] 	Batch(6800/7879) done. Loss: 0.0431  lr:0.000010
[ Sat Jul  6 13:35:50 2024 ] 	Batch(6900/7879) done. Loss: 0.2536  lr:0.000010
[ Sat Jul  6 13:36:08 2024 ] 
Training: Epoch [13/120], Step [6999], Loss: 0.02583790197968483, Training Accuracy: 97.75178571428572
[ Sat Jul  6 13:36:08 2024 ] 	Batch(7000/7879) done. Loss: 0.0412  lr:0.000010
[ Sat Jul  6 13:36:26 2024 ] 	Batch(7100/7879) done. Loss: 0.5510  lr:0.000010
[ Sat Jul  6 13:36:44 2024 ] 	Batch(7200/7879) done. Loss: 0.0114  lr:0.000010
[ Sat Jul  6 13:37:02 2024 ] 	Batch(7300/7879) done. Loss: 0.0647  lr:0.000010
[ Sat Jul  6 13:37:20 2024 ] 	Batch(7400/7879) done. Loss: 0.0257  lr:0.000010
[ Sat Jul  6 13:37:38 2024 ] 
Training: Epoch [13/120], Step [7499], Loss: 0.018438465893268585, Training Accuracy: 97.765
[ Sat Jul  6 13:37:38 2024 ] 	Batch(7500/7879) done. Loss: 0.0116  lr:0.000010
[ Sat Jul  6 13:37:56 2024 ] 	Batch(7600/7879) done. Loss: 0.0132  lr:0.000010
[ Sat Jul  6 13:38:14 2024 ] 	Batch(7700/7879) done. Loss: 0.0221  lr:0.000010
[ Sat Jul  6 13:38:32 2024 ] 	Batch(7800/7879) done. Loss: 0.2271  lr:0.000010
[ Sat Jul  6 13:38:46 2024 ] 	Mean training loss: 0.0916.
[ Sat Jul  6 13:38:46 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 13:38:46 2024 ] Training epoch: 15
[ Sat Jul  6 13:38:47 2024 ] 	Batch(0/7879) done. Loss: 0.0287  lr:0.000010
[ Sat Jul  6 13:39:05 2024 ] 	Batch(100/7879) done. Loss: 0.0067  lr:0.000010
[ Sat Jul  6 13:39:23 2024 ] 	Batch(200/7879) done. Loss: 0.0797  lr:0.000010
[ Sat Jul  6 13:39:41 2024 ] 	Batch(300/7879) done. Loss: 0.0665  lr:0.000010
[ Sat Jul  6 13:39:59 2024 ] 	Batch(400/7879) done. Loss: 0.0264  lr:0.000010
[ Sat Jul  6 13:40:16 2024 ] 
Training: Epoch [14/120], Step [499], Loss: 0.013463061302900314, Training Accuracy: 97.39999999999999
[ Sat Jul  6 13:40:17 2024 ] 	Batch(500/7879) done. Loss: 0.2335  lr:0.000010
[ Sat Jul  6 13:40:34 2024 ] 	Batch(600/7879) done. Loss: 0.0168  lr:0.000010
[ Sat Jul  6 13:40:52 2024 ] 	Batch(700/7879) done. Loss: 0.0299  lr:0.000010
[ Sat Jul  6 13:41:10 2024 ] 	Batch(800/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 13:41:28 2024 ] 	Batch(900/7879) done. Loss: 0.0097  lr:0.000010
[ Sat Jul  6 13:41:46 2024 ] 
Training: Epoch [14/120], Step [999], Loss: 0.04951107129454613, Training Accuracy: 97.625
[ Sat Jul  6 13:41:46 2024 ] 	Batch(1000/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 13:42:04 2024 ] 	Batch(1100/7879) done. Loss: 0.0050  lr:0.000010
[ Sat Jul  6 13:42:22 2024 ] 	Batch(1200/7879) done. Loss: 0.0789  lr:0.000010
[ Sat Jul  6 13:42:40 2024 ] 	Batch(1300/7879) done. Loss: 0.0412  lr:0.000010
[ Sat Jul  6 13:42:58 2024 ] 	Batch(1400/7879) done. Loss: 0.0012  lr:0.000010
[ Sat Jul  6 13:43:16 2024 ] 
Training: Epoch [14/120], Step [1499], Loss: 0.005685318727046251, Training Accuracy: 97.73333333333333
[ Sat Jul  6 13:43:16 2024 ] 	Batch(1500/7879) done. Loss: 0.0199  lr:0.000010
[ Sat Jul  6 13:43:34 2024 ] 	Batch(1600/7879) done. Loss: 0.0435  lr:0.000010
[ Sat Jul  6 13:43:52 2024 ] 	Batch(1700/7879) done. Loss: 0.0674  lr:0.000010
[ Sat Jul  6 13:44:10 2024 ] 	Batch(1800/7879) done. Loss: 0.1392  lr:0.000010
[ Sat Jul  6 13:44:28 2024 ] 	Batch(1900/7879) done. Loss: 0.0406  lr:0.000010
[ Sat Jul  6 13:44:46 2024 ] 
Training: Epoch [14/120], Step [1999], Loss: 0.02499917708337307, Training Accuracy: 97.75
[ Sat Jul  6 13:44:46 2024 ] 	Batch(2000/7879) done. Loss: 0.0100  lr:0.000010
[ Sat Jul  6 13:45:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0248  lr:0.000010
[ Sat Jul  6 13:45:22 2024 ] 	Batch(2200/7879) done. Loss: 0.1502  lr:0.000010
[ Sat Jul  6 13:45:40 2024 ] 	Batch(2300/7879) done. Loss: 0.0366  lr:0.000010
[ Sat Jul  6 13:45:58 2024 ] 	Batch(2400/7879) done. Loss: 0.1454  lr:0.000010
[ Sat Jul  6 13:46:16 2024 ] 
Training: Epoch [14/120], Step [2499], Loss: 0.23415876924991608, Training Accuracy: 97.745
[ Sat Jul  6 13:46:16 2024 ] 	Batch(2500/7879) done. Loss: 0.0044  lr:0.000010
[ Sat Jul  6 13:46:34 2024 ] 	Batch(2600/7879) done. Loss: 0.0066  lr:0.000010
[ Sat Jul  6 13:46:52 2024 ] 	Batch(2700/7879) done. Loss: 0.0827  lr:0.000010
[ Sat Jul  6 13:47:10 2024 ] 	Batch(2800/7879) done. Loss: 0.0300  lr:0.000010
[ Sat Jul  6 13:47:28 2024 ] 	Batch(2900/7879) done. Loss: 0.0874  lr:0.000010
[ Sat Jul  6 13:47:47 2024 ] 
Training: Epoch [14/120], Step [2999], Loss: 0.00158719252794981, Training Accuracy: 97.75833333333334
[ Sat Jul  6 13:47:47 2024 ] 	Batch(3000/7879) done. Loss: 0.0253  lr:0.000010
[ Sat Jul  6 13:48:06 2024 ] 	Batch(3100/7879) done. Loss: 0.0766  lr:0.000010
[ Sat Jul  6 13:48:24 2024 ] 	Batch(3200/7879) done. Loss: 0.2479  lr:0.000010
[ Sat Jul  6 13:48:42 2024 ] 	Batch(3300/7879) done. Loss: 0.1024  lr:0.000010
[ Sat Jul  6 13:49:00 2024 ] 	Batch(3400/7879) done. Loss: 0.0085  lr:0.000010
[ Sat Jul  6 13:49:18 2024 ] 
Training: Epoch [14/120], Step [3499], Loss: 0.16525587439537048, Training Accuracy: 97.74285714285715
[ Sat Jul  6 13:49:18 2024 ] 	Batch(3500/7879) done. Loss: 0.1510  lr:0.000010
[ Sat Jul  6 13:49:36 2024 ] 	Batch(3600/7879) done. Loss: 0.0161  lr:0.000010
[ Sat Jul  6 13:49:54 2024 ] 	Batch(3700/7879) done. Loss: 0.0361  lr:0.000010
[ Sat Jul  6 13:50:12 2024 ] 	Batch(3800/7879) done. Loss: 0.1656  lr:0.000010
[ Sat Jul  6 13:50:30 2024 ] 	Batch(3900/7879) done. Loss: 0.1436  lr:0.000010
[ Sat Jul  6 13:50:48 2024 ] 
Training: Epoch [14/120], Step [3999], Loss: 0.030823253095149994, Training Accuracy: 97.75625
[ Sat Jul  6 13:50:48 2024 ] 	Batch(4000/7879) done. Loss: 0.4390  lr:0.000010
[ Sat Jul  6 13:51:06 2024 ] 	Batch(4100/7879) done. Loss: 0.0015  lr:0.000010
[ Sat Jul  6 13:51:24 2024 ] 	Batch(4200/7879) done. Loss: 0.4529  lr:0.000010
[ Sat Jul  6 13:51:42 2024 ] 	Batch(4300/7879) done. Loss: 0.0095  lr:0.000010
[ Sat Jul  6 13:52:00 2024 ] 	Batch(4400/7879) done. Loss: 0.0107  lr:0.000010
[ Sat Jul  6 13:52:18 2024 ] 
Training: Epoch [14/120], Step [4499], Loss: 0.11317858844995499, Training Accuracy: 97.70555555555556
[ Sat Jul  6 13:52:18 2024 ] 	Batch(4500/7879) done. Loss: 0.0856  lr:0.000010
[ Sat Jul  6 13:52:36 2024 ] 	Batch(4600/7879) done. Loss: 0.0057  lr:0.000010
[ Sat Jul  6 13:52:54 2024 ] 	Batch(4700/7879) done. Loss: 0.0531  lr:0.000010
[ Sat Jul  6 13:53:12 2024 ] 	Batch(4800/7879) done. Loss: 0.0224  lr:0.000010
[ Sat Jul  6 13:53:31 2024 ] 	Batch(4900/7879) done. Loss: 0.0247  lr:0.000010
[ Sat Jul  6 13:53:49 2024 ] 
Training: Epoch [14/120], Step [4999], Loss: 0.025383686646819115, Training Accuracy: 97.7
[ Sat Jul  6 13:53:50 2024 ] 	Batch(5000/7879) done. Loss: 0.0805  lr:0.000010
[ Sat Jul  6 13:54:08 2024 ] 	Batch(5100/7879) done. Loss: 0.0644  lr:0.000010
[ Sat Jul  6 13:54:27 2024 ] 	Batch(5200/7879) done. Loss: 0.1446  lr:0.000010
[ Sat Jul  6 13:54:45 2024 ] 	Batch(5300/7879) done. Loss: 0.1264  lr:0.000010
[ Sat Jul  6 13:55:03 2024 ] 	Batch(5400/7879) done. Loss: 0.0118  lr:0.000010
[ Sat Jul  6 13:55:21 2024 ] 
Training: Epoch [14/120], Step [5499], Loss: 0.1722441464662552, Training Accuracy: 97.69772727272728
[ Sat Jul  6 13:55:21 2024 ] 	Batch(5500/7879) done. Loss: 0.1445  lr:0.000010
[ Sat Jul  6 13:55:39 2024 ] 	Batch(5600/7879) done. Loss: 0.0175  lr:0.000010
[ Sat Jul  6 13:55:57 2024 ] 	Batch(5700/7879) done. Loss: 0.0724  lr:0.000010
[ Sat Jul  6 13:56:15 2024 ] 	Batch(5800/7879) done. Loss: 0.0413  lr:0.000010
[ Sat Jul  6 13:56:33 2024 ] 	Batch(5900/7879) done. Loss: 0.0131  lr:0.000010
[ Sat Jul  6 13:56:51 2024 ] 
Training: Epoch [14/120], Step [5999], Loss: 0.05606700852513313, Training Accuracy: 97.71041666666666
[ Sat Jul  6 13:56:51 2024 ] 	Batch(6000/7879) done. Loss: 0.1032  lr:0.000010
[ Sat Jul  6 13:57:09 2024 ] 	Batch(6100/7879) done. Loss: 0.1366  lr:0.000010
[ Sat Jul  6 13:57:27 2024 ] 	Batch(6200/7879) done. Loss: 0.1672  lr:0.000010
[ Sat Jul  6 13:57:45 2024 ] 	Batch(6300/7879) done. Loss: 0.0500  lr:0.000010
[ Sat Jul  6 13:58:03 2024 ] 	Batch(6400/7879) done. Loss: 0.2619  lr:0.000010
[ Sat Jul  6 13:58:20 2024 ] 
Training: Epoch [14/120], Step [6499], Loss: 0.2019796073436737, Training Accuracy: 97.69423076923077
[ Sat Jul  6 13:58:21 2024 ] 	Batch(6500/7879) done. Loss: 0.0096  lr:0.000010
[ Sat Jul  6 13:58:39 2024 ] 	Batch(6600/7879) done. Loss: 0.0424  lr:0.000010
[ Sat Jul  6 13:58:56 2024 ] 	Batch(6700/7879) done. Loss: 0.0330  lr:0.000010
[ Sat Jul  6 13:59:15 2024 ] 	Batch(6800/7879) done. Loss: 0.0532  lr:0.000010
[ Sat Jul  6 13:59:33 2024 ] 	Batch(6900/7879) done. Loss: 0.0711  lr:0.000010
[ Sat Jul  6 13:59:50 2024 ] 
Training: Epoch [14/120], Step [6999], Loss: 0.005459427367895842, Training Accuracy: 97.69821428571429
[ Sat Jul  6 13:59:50 2024 ] 	Batch(7000/7879) done. Loss: 0.0166  lr:0.000010
[ Sat Jul  6 14:00:08 2024 ] 	Batch(7100/7879) done. Loss: 0.0223  lr:0.000010
[ Sat Jul  6 14:00:26 2024 ] 	Batch(7200/7879) done. Loss: 0.1229  lr:0.000010
[ Sat Jul  6 14:00:45 2024 ] 	Batch(7300/7879) done. Loss: 0.0279  lr:0.000010
[ Sat Jul  6 14:01:04 2024 ] 	Batch(7400/7879) done. Loss: 0.0146  lr:0.000010
[ Sat Jul  6 14:01:22 2024 ] 
Training: Epoch [14/120], Step [7499], Loss: 0.02519228681921959, Training Accuracy: 97.70833333333333
[ Sat Jul  6 14:01:22 2024 ] 	Batch(7500/7879) done. Loss: 0.0821  lr:0.000010
[ Sat Jul  6 14:01:40 2024 ] 	Batch(7600/7879) done. Loss: 0.1414  lr:0.000010
[ Sat Jul  6 14:01:58 2024 ] 	Batch(7700/7879) done. Loss: 0.0907  lr:0.000010
[ Sat Jul  6 14:02:16 2024 ] 	Batch(7800/7879) done. Loss: 0.0638  lr:0.000010
[ Sat Jul  6 14:02:30 2024 ] 	Mean training loss: 0.0933.
[ Sat Jul  6 14:02:30 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 14:02:30 2024 ] Training epoch: 16
[ Sat Jul  6 14:02:31 2024 ] 	Batch(0/7879) done. Loss: 0.0192  lr:0.000010
[ Sat Jul  6 14:02:49 2024 ] 	Batch(100/7879) done. Loss: 0.1837  lr:0.000010
[ Sat Jul  6 14:03:07 2024 ] 	Batch(200/7879) done. Loss: 0.0116  lr:0.000010
[ Sat Jul  6 14:03:25 2024 ] 	Batch(300/7879) done. Loss: 0.0227  lr:0.000010
[ Sat Jul  6 14:03:43 2024 ] 	Batch(400/7879) done. Loss: 0.1488  lr:0.000010
[ Sat Jul  6 14:04:00 2024 ] 
Training: Epoch [15/120], Step [499], Loss: 0.00691289734095335, Training Accuracy: 97.39999999999999
[ Sat Jul  6 14:04:01 2024 ] 	Batch(500/7879) done. Loss: 0.2158  lr:0.000010
[ Sat Jul  6 14:04:19 2024 ] 	Batch(600/7879) done. Loss: 0.0549  lr:0.000010
[ Sat Jul  6 14:04:37 2024 ] 	Batch(700/7879) done. Loss: 0.0059  lr:0.000010
[ Sat Jul  6 14:04:55 2024 ] 	Batch(800/7879) done. Loss: 0.0527  lr:0.000010
[ Sat Jul  6 14:05:13 2024 ] 	Batch(900/7879) done. Loss: 0.5257  lr:0.000010
[ Sat Jul  6 14:05:30 2024 ] 
Training: Epoch [15/120], Step [999], Loss: 0.10771557688713074, Training Accuracy: 97.5375
[ Sat Jul  6 14:05:31 2024 ] 	Batch(1000/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 14:05:49 2024 ] 	Batch(1100/7879) done. Loss: 0.0067  lr:0.000010
[ Sat Jul  6 14:06:07 2024 ] 	Batch(1200/7879) done. Loss: 0.1252  lr:0.000010
[ Sat Jul  6 14:06:25 2024 ] 	Batch(1300/7879) done. Loss: 0.0011  lr:0.000010
[ Sat Jul  6 14:06:43 2024 ] 	Batch(1400/7879) done. Loss: 0.0625  lr:0.000010
[ Sat Jul  6 14:07:00 2024 ] 
Training: Epoch [15/120], Step [1499], Loss: 0.05744394287467003, Training Accuracy: 97.59166666666667
[ Sat Jul  6 14:07:01 2024 ] 	Batch(1500/7879) done. Loss: 0.2071  lr:0.000010
[ Sat Jul  6 14:07:19 2024 ] 	Batch(1600/7879) done. Loss: 0.0177  lr:0.000010
[ Sat Jul  6 14:07:37 2024 ] 	Batch(1700/7879) done. Loss: 0.0181  lr:0.000010
[ Sat Jul  6 14:07:54 2024 ] 	Batch(1800/7879) done. Loss: 0.0298  lr:0.000010
[ Sat Jul  6 14:08:12 2024 ] 	Batch(1900/7879) done. Loss: 0.0528  lr:0.000010
[ Sat Jul  6 14:08:30 2024 ] 
Training: Epoch [15/120], Step [1999], Loss: 0.09909813851118088, Training Accuracy: 97.65625
[ Sat Jul  6 14:08:30 2024 ] 	Batch(2000/7879) done. Loss: 0.0408  lr:0.000010
[ Sat Jul  6 14:08:49 2024 ] 	Batch(2100/7879) done. Loss: 0.0652  lr:0.000010
[ Sat Jul  6 14:09:08 2024 ] 	Batch(2200/7879) done. Loss: 0.1023  lr:0.000010
[ Sat Jul  6 14:09:26 2024 ] 	Batch(2300/7879) done. Loss: 0.0960  lr:0.000010
[ Sat Jul  6 14:09:44 2024 ] 	Batch(2400/7879) done. Loss: 0.3651  lr:0.000010
[ Sat Jul  6 14:10:02 2024 ] 
Training: Epoch [15/120], Step [2499], Loss: 0.014611735008656979, Training Accuracy: 97.585
[ Sat Jul  6 14:10:02 2024 ] 	Batch(2500/7879) done. Loss: 0.0644  lr:0.000010
[ Sat Jul  6 14:10:20 2024 ] 	Batch(2600/7879) done. Loss: 0.0192  lr:0.000010
[ Sat Jul  6 14:10:38 2024 ] 	Batch(2700/7879) done. Loss: 0.0787  lr:0.000010
[ Sat Jul  6 14:10:56 2024 ] 	Batch(2800/7879) done. Loss: 0.0788  lr:0.000010
[ Sat Jul  6 14:11:15 2024 ] 	Batch(2900/7879) done. Loss: 0.1650  lr:0.000010
[ Sat Jul  6 14:11:33 2024 ] 
Training: Epoch [15/120], Step [2999], Loss: 0.06998451054096222, Training Accuracy: 97.62083333333334
[ Sat Jul  6 14:11:33 2024 ] 	Batch(3000/7879) done. Loss: 0.1026  lr:0.000010
[ Sat Jul  6 14:11:52 2024 ] 	Batch(3100/7879) done. Loss: 0.2357  lr:0.000010
[ Sat Jul  6 14:12:10 2024 ] 	Batch(3200/7879) done. Loss: 0.0182  lr:0.000010
[ Sat Jul  6 14:12:28 2024 ] 	Batch(3300/7879) done. Loss: 0.0733  lr:0.000010
[ Sat Jul  6 14:12:46 2024 ] 	Batch(3400/7879) done. Loss: 0.0363  lr:0.000010
[ Sat Jul  6 14:13:03 2024 ] 
Training: Epoch [15/120], Step [3499], Loss: 0.06726007163524628, Training Accuracy: 97.67142857142858
[ Sat Jul  6 14:13:04 2024 ] 	Batch(3500/7879) done. Loss: 0.0763  lr:0.000010
[ Sat Jul  6 14:13:22 2024 ] 	Batch(3600/7879) done. Loss: 0.0116  lr:0.000010
[ Sat Jul  6 14:13:39 2024 ] 	Batch(3700/7879) done. Loss: 0.0506  lr:0.000010
[ Sat Jul  6 14:13:57 2024 ] 	Batch(3800/7879) done. Loss: 0.0369  lr:0.000010
[ Sat Jul  6 14:14:15 2024 ] 	Batch(3900/7879) done. Loss: 0.0362  lr:0.000010
[ Sat Jul  6 14:14:33 2024 ] 
Training: Epoch [15/120], Step [3999], Loss: 0.020906858146190643, Training Accuracy: 97.74062500000001
[ Sat Jul  6 14:14:33 2024 ] 	Batch(4000/7879) done. Loss: 0.0314  lr:0.000010
[ Sat Jul  6 14:14:52 2024 ] 	Batch(4100/7879) done. Loss: 0.2095  lr:0.000010
[ Sat Jul  6 14:15:10 2024 ] 	Batch(4200/7879) done. Loss: 0.2798  lr:0.000010
[ Sat Jul  6 14:15:28 2024 ] 	Batch(4300/7879) done. Loss: 0.0122  lr:0.000010
[ Sat Jul  6 14:15:46 2024 ] 	Batch(4400/7879) done. Loss: 0.0677  lr:0.000010
[ Sat Jul  6 14:16:04 2024 ] 
Training: Epoch [15/120], Step [4499], Loss: 0.5022149682044983, Training Accuracy: 97.73333333333333
[ Sat Jul  6 14:16:04 2024 ] 	Batch(4500/7879) done. Loss: 0.2183  lr:0.000010
[ Sat Jul  6 14:16:22 2024 ] 	Batch(4600/7879) done. Loss: 0.0769  lr:0.000010
[ Sat Jul  6 14:16:40 2024 ] 	Batch(4700/7879) done. Loss: 0.0837  lr:0.000010
[ Sat Jul  6 14:16:58 2024 ] 	Batch(4800/7879) done. Loss: 0.0189  lr:0.000010
[ Sat Jul  6 14:17:16 2024 ] 	Batch(4900/7879) done. Loss: 0.0757  lr:0.000010
[ Sat Jul  6 14:17:33 2024 ] 
Training: Epoch [15/120], Step [4999], Loss: 0.059384461492300034, Training Accuracy: 97.7125
[ Sat Jul  6 14:17:34 2024 ] 	Batch(5000/7879) done. Loss: 0.0303  lr:0.000010
[ Sat Jul  6 14:17:52 2024 ] 	Batch(5100/7879) done. Loss: 0.0148  lr:0.000010
[ Sat Jul  6 14:18:10 2024 ] 	Batch(5200/7879) done. Loss: 0.0728  lr:0.000010
[ Sat Jul  6 14:18:28 2024 ] 	Batch(5300/7879) done. Loss: 0.0152  lr:0.000010
[ Sat Jul  6 14:18:46 2024 ] 	Batch(5400/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 14:19:03 2024 ] 
Training: Epoch [15/120], Step [5499], Loss: 0.04424932971596718, Training Accuracy: 97.74090909090908
[ Sat Jul  6 14:19:04 2024 ] 	Batch(5500/7879) done. Loss: 0.0655  lr:0.000010
[ Sat Jul  6 14:19:22 2024 ] 	Batch(5600/7879) done. Loss: 0.0214  lr:0.000010
[ Sat Jul  6 14:19:40 2024 ] 	Batch(5700/7879) done. Loss: 0.4793  lr:0.000010
[ Sat Jul  6 14:19:59 2024 ] 	Batch(5800/7879) done. Loss: 0.0044  lr:0.000010
[ Sat Jul  6 14:20:17 2024 ] 	Batch(5900/7879) done. Loss: 0.0062  lr:0.000010
[ Sat Jul  6 14:20:36 2024 ] 
Training: Epoch [15/120], Step [5999], Loss: 0.03174910694360733, Training Accuracy: 97.76666666666667
[ Sat Jul  6 14:20:36 2024 ] 	Batch(6000/7879) done. Loss: 0.0685  lr:0.000010
[ Sat Jul  6 14:20:55 2024 ] 	Batch(6100/7879) done. Loss: 0.0946  lr:0.000010
[ Sat Jul  6 14:21:13 2024 ] 	Batch(6200/7879) done. Loss: 0.0815  lr:0.000010
[ Sat Jul  6 14:21:32 2024 ] 	Batch(6300/7879) done. Loss: 0.4071  lr:0.000010
[ Sat Jul  6 14:21:50 2024 ] 	Batch(6400/7879) done. Loss: 0.0323  lr:0.000010
[ Sat Jul  6 14:22:08 2024 ] 
Training: Epoch [15/120], Step [6499], Loss: 0.3400426506996155, Training Accuracy: 97.76923076923076
[ Sat Jul  6 14:22:08 2024 ] 	Batch(6500/7879) done. Loss: 0.0697  lr:0.000010
[ Sat Jul  6 14:22:26 2024 ] 	Batch(6600/7879) done. Loss: 0.0162  lr:0.000010
[ Sat Jul  6 14:22:44 2024 ] 	Batch(6700/7879) done. Loss: 0.0580  lr:0.000010
[ Sat Jul  6 14:23:02 2024 ] 	Batch(6800/7879) done. Loss: 0.1045  lr:0.000010
[ Sat Jul  6 14:23:21 2024 ] 	Batch(6900/7879) done. Loss: 0.0048  lr:0.000010
[ Sat Jul  6 14:23:39 2024 ] 
Training: Epoch [15/120], Step [6999], Loss: 0.08880671858787537, Training Accuracy: 97.74821428571428
[ Sat Jul  6 14:23:39 2024 ] 	Batch(7000/7879) done. Loss: 0.1090  lr:0.000010
[ Sat Jul  6 14:23:58 2024 ] 	Batch(7100/7879) done. Loss: 0.0198  lr:0.000010
[ Sat Jul  6 14:24:17 2024 ] 	Batch(7200/7879) done. Loss: 0.1302  lr:0.000010
[ Sat Jul  6 14:24:35 2024 ] 	Batch(7300/7879) done. Loss: 0.0222  lr:0.000010
[ Sat Jul  6 14:24:54 2024 ] 	Batch(7400/7879) done. Loss: 0.0798  lr:0.000010
[ Sat Jul  6 14:25:12 2024 ] 
Training: Epoch [15/120], Step [7499], Loss: 0.06962884217500687, Training Accuracy: 97.74333333333334
[ Sat Jul  6 14:25:12 2024 ] 	Batch(7500/7879) done. Loss: 0.0452  lr:0.000010
[ Sat Jul  6 14:25:31 2024 ] 	Batch(7600/7879) done. Loss: 0.0804  lr:0.000010
[ Sat Jul  6 14:25:49 2024 ] 	Batch(7700/7879) done. Loss: 0.0197  lr:0.000010
[ Sat Jul  6 14:26:07 2024 ] 	Batch(7800/7879) done. Loss: 0.0024  lr:0.000010
[ Sat Jul  6 14:26:21 2024 ] 	Mean training loss: 0.0912.
[ Sat Jul  6 14:26:21 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 14:26:21 2024 ] Training epoch: 17
[ Sat Jul  6 14:26:22 2024 ] 	Batch(0/7879) done. Loss: 0.0108  lr:0.000010
[ Sat Jul  6 14:26:40 2024 ] 	Batch(100/7879) done. Loss: 0.1418  lr:0.000010
[ Sat Jul  6 14:26:58 2024 ] 	Batch(200/7879) done. Loss: 0.0018  lr:0.000010
[ Sat Jul  6 14:27:16 2024 ] 	Batch(300/7879) done. Loss: 0.1419  lr:0.000010
[ Sat Jul  6 14:27:34 2024 ] 	Batch(400/7879) done. Loss: 0.1988  lr:0.000010
[ Sat Jul  6 14:27:53 2024 ] 
Training: Epoch [16/120], Step [499], Loss: 0.015305762179195881, Training Accuracy: 97.95
[ Sat Jul  6 14:27:53 2024 ] 	Batch(500/7879) done. Loss: 0.0044  lr:0.000010
[ Sat Jul  6 14:28:11 2024 ] 	Batch(600/7879) done. Loss: 0.0653  lr:0.000010
[ Sat Jul  6 14:28:30 2024 ] 	Batch(700/7879) done. Loss: 0.0288  lr:0.000010
[ Sat Jul  6 14:28:48 2024 ] 	Batch(800/7879) done. Loss: 0.1083  lr:0.000010
[ Sat Jul  6 14:29:07 2024 ] 	Batch(900/7879) done. Loss: 0.2352  lr:0.000010
[ Sat Jul  6 14:29:25 2024 ] 
Training: Epoch [16/120], Step [999], Loss: 0.06940703839063644, Training Accuracy: 97.8375
[ Sat Jul  6 14:29:26 2024 ] 	Batch(1000/7879) done. Loss: 0.0472  lr:0.000010
[ Sat Jul  6 14:29:44 2024 ] 	Batch(1100/7879) done. Loss: 0.1008  lr:0.000010
[ Sat Jul  6 14:30:03 2024 ] 	Batch(1200/7879) done. Loss: 0.0105  lr:0.000010
[ Sat Jul  6 14:30:21 2024 ] 	Batch(1300/7879) done. Loss: 0.0133  lr:0.000010
[ Sat Jul  6 14:30:39 2024 ] 	Batch(1400/7879) done. Loss: 0.0230  lr:0.000010
[ Sat Jul  6 14:30:57 2024 ] 
Training: Epoch [16/120], Step [1499], Loss: 0.020939404144883156, Training Accuracy: 97.8
[ Sat Jul  6 14:30:57 2024 ] 	Batch(1500/7879) done. Loss: 0.4479  lr:0.000010
[ Sat Jul  6 14:31:15 2024 ] 	Batch(1600/7879) done. Loss: 0.1859  lr:0.000010
[ Sat Jul  6 14:31:33 2024 ] 	Batch(1700/7879) done. Loss: 0.0184  lr:0.000010
[ Sat Jul  6 14:31:52 2024 ] 	Batch(1800/7879) done. Loss: 0.0259  lr:0.000010
[ Sat Jul  6 14:32:11 2024 ] 	Batch(1900/7879) done. Loss: 0.9906  lr:0.000010
[ Sat Jul  6 14:32:29 2024 ] 
Training: Epoch [16/120], Step [1999], Loss: 0.005885707214474678, Training Accuracy: 97.7625
[ Sat Jul  6 14:32:29 2024 ] 	Batch(2000/7879) done. Loss: 0.0316  lr:0.000010
[ Sat Jul  6 14:32:48 2024 ] 	Batch(2100/7879) done. Loss: 0.0281  lr:0.000010
[ Sat Jul  6 14:33:06 2024 ] 	Batch(2200/7879) done. Loss: 0.0747  lr:0.000010
[ Sat Jul  6 14:33:25 2024 ] 	Batch(2300/7879) done. Loss: 0.0561  lr:0.000010
[ Sat Jul  6 14:33:44 2024 ] 	Batch(2400/7879) done. Loss: 0.0147  lr:0.000010
[ Sat Jul  6 14:34:02 2024 ] 
Training: Epoch [16/120], Step [2499], Loss: 0.10050366073846817, Training Accuracy: 97.785
[ Sat Jul  6 14:34:02 2024 ] 	Batch(2500/7879) done. Loss: 0.0101  lr:0.000010
[ Sat Jul  6 14:34:20 2024 ] 	Batch(2600/7879) done. Loss: 0.1555  lr:0.000010
[ Sat Jul  6 14:34:39 2024 ] 	Batch(2700/7879) done. Loss: 0.0039  lr:0.000010
[ Sat Jul  6 14:34:57 2024 ] 	Batch(2800/7879) done. Loss: 0.0148  lr:0.000010
[ Sat Jul  6 14:35:15 2024 ] 	Batch(2900/7879) done. Loss: 0.0540  lr:0.000010
[ Sat Jul  6 14:35:32 2024 ] 
Training: Epoch [16/120], Step [2999], Loss: 0.015196921303868294, Training Accuracy: 97.79166666666667
[ Sat Jul  6 14:35:33 2024 ] 	Batch(3000/7879) done. Loss: 0.0214  lr:0.000010
[ Sat Jul  6 14:35:51 2024 ] 	Batch(3100/7879) done. Loss: 0.0508  lr:0.000010
[ Sat Jul  6 14:36:09 2024 ] 	Batch(3200/7879) done. Loss: 0.0329  lr:0.000010
[ Sat Jul  6 14:36:27 2024 ] 	Batch(3300/7879) done. Loss: 0.0606  lr:0.000010
[ Sat Jul  6 14:36:44 2024 ] 	Batch(3400/7879) done. Loss: 0.0065  lr:0.000010
[ Sat Jul  6 14:37:02 2024 ] 
Training: Epoch [16/120], Step [3499], Loss: 0.13983751833438873, Training Accuracy: 97.80357142857142
[ Sat Jul  6 14:37:02 2024 ] 	Batch(3500/7879) done. Loss: 0.2362  lr:0.000010
[ Sat Jul  6 14:37:20 2024 ] 	Batch(3600/7879) done. Loss: 0.0645  lr:0.000010
[ Sat Jul  6 14:37:39 2024 ] 	Batch(3700/7879) done. Loss: 0.2965  lr:0.000010
[ Sat Jul  6 14:37:58 2024 ] 	Batch(3800/7879) done. Loss: 0.0062  lr:0.000010
[ Sat Jul  6 14:38:16 2024 ] 	Batch(3900/7879) done. Loss: 0.0162  lr:0.000010
[ Sat Jul  6 14:38:35 2024 ] 
Training: Epoch [16/120], Step [3999], Loss: 0.0010404327185824513, Training Accuracy: 97.828125
[ Sat Jul  6 14:38:35 2024 ] 	Batch(4000/7879) done. Loss: 0.0222  lr:0.000010
[ Sat Jul  6 14:38:53 2024 ] 	Batch(4100/7879) done. Loss: 0.1420  lr:0.000010
[ Sat Jul  6 14:39:12 2024 ] 	Batch(4200/7879) done. Loss: 0.0883  lr:0.000010
[ Sat Jul  6 14:39:30 2024 ] 	Batch(4300/7879) done. Loss: 0.3280  lr:0.000010
[ Sat Jul  6 14:39:49 2024 ] 	Batch(4400/7879) done. Loss: 0.0396  lr:0.000010
[ Sat Jul  6 14:40:07 2024 ] 
Training: Epoch [16/120], Step [4499], Loss: 0.14959612488746643, Training Accuracy: 97.78888888888889
[ Sat Jul  6 14:40:07 2024 ] 	Batch(4500/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 14:40:25 2024 ] 	Batch(4600/7879) done. Loss: 0.2266  lr:0.000010
[ Sat Jul  6 14:40:43 2024 ] 	Batch(4700/7879) done. Loss: 0.2258  lr:0.000010
[ Sat Jul  6 14:41:01 2024 ] 	Batch(4800/7879) done. Loss: 0.0424  lr:0.000010
[ Sat Jul  6 14:41:19 2024 ] 	Batch(4900/7879) done. Loss: 0.0141  lr:0.000010
[ Sat Jul  6 14:41:36 2024 ] 
Training: Epoch [16/120], Step [4999], Loss: 0.02885814756155014, Training Accuracy: 97.7825
[ Sat Jul  6 14:41:37 2024 ] 	Batch(5000/7879) done. Loss: 0.1483  lr:0.000010
[ Sat Jul  6 14:41:55 2024 ] 	Batch(5100/7879) done. Loss: 0.0475  lr:0.000010
[ Sat Jul  6 14:42:13 2024 ] 	Batch(5200/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 14:42:31 2024 ] 	Batch(5300/7879) done. Loss: 0.0298  lr:0.000010
[ Sat Jul  6 14:42:49 2024 ] 	Batch(5400/7879) done. Loss: 0.0248  lr:0.000010
[ Sat Jul  6 14:43:06 2024 ] 
Training: Epoch [16/120], Step [5499], Loss: 0.0360480435192585, Training Accuracy: 97.77045454545454
[ Sat Jul  6 14:43:06 2024 ] 	Batch(5500/7879) done. Loss: 0.0729  lr:0.000010
[ Sat Jul  6 14:43:25 2024 ] 	Batch(5600/7879) done. Loss: 0.0547  lr:0.000010
[ Sat Jul  6 14:43:43 2024 ] 	Batch(5700/7879) done. Loss: 0.0128  lr:0.000010
[ Sat Jul  6 14:44:00 2024 ] 	Batch(5800/7879) done. Loss: 0.0267  lr:0.000010
[ Sat Jul  6 14:44:18 2024 ] 	Batch(5900/7879) done. Loss: 0.0115  lr:0.000010
[ Sat Jul  6 14:44:36 2024 ] 
Training: Epoch [16/120], Step [5999], Loss: 0.009002825245261192, Training Accuracy: 97.76666666666667
[ Sat Jul  6 14:44:37 2024 ] 	Batch(6000/7879) done. Loss: 0.1371  lr:0.000010
[ Sat Jul  6 14:44:55 2024 ] 	Batch(6100/7879) done. Loss: 0.1715  lr:0.000010
[ Sat Jul  6 14:45:13 2024 ] 	Batch(6200/7879) done. Loss: 0.0011  lr:0.000010
[ Sat Jul  6 14:45:31 2024 ] 	Batch(6300/7879) done. Loss: 0.0434  lr:0.000010
[ Sat Jul  6 14:45:50 2024 ] 	Batch(6400/7879) done. Loss: 0.0246  lr:0.000010
[ Sat Jul  6 14:46:08 2024 ] 
Training: Epoch [16/120], Step [6499], Loss: 0.05934320390224457, Training Accuracy: 97.76538461538462
[ Sat Jul  6 14:46:08 2024 ] 	Batch(6500/7879) done. Loss: 0.0282  lr:0.000010
[ Sat Jul  6 14:46:26 2024 ] 	Batch(6600/7879) done. Loss: 0.0884  lr:0.000010
[ Sat Jul  6 14:46:44 2024 ] 	Batch(6700/7879) done. Loss: 0.0071  lr:0.000010
[ Sat Jul  6 14:47:02 2024 ] 	Batch(6800/7879) done. Loss: 0.1152  lr:0.000010
[ Sat Jul  6 14:47:21 2024 ] 	Batch(6900/7879) done. Loss: 0.0187  lr:0.000010
[ Sat Jul  6 14:47:39 2024 ] 
Training: Epoch [16/120], Step [6999], Loss: 0.008290223777294159, Training Accuracy: 97.75892857142857
[ Sat Jul  6 14:47:40 2024 ] 	Batch(7000/7879) done. Loss: 0.1570  lr:0.000010
[ Sat Jul  6 14:47:58 2024 ] 	Batch(7100/7879) done. Loss: 0.0032  lr:0.000010
[ Sat Jul  6 14:48:17 2024 ] 	Batch(7200/7879) done. Loss: 0.0486  lr:0.000010
[ Sat Jul  6 14:48:35 2024 ] 	Batch(7300/7879) done. Loss: 0.0139  lr:0.000010
[ Sat Jul  6 14:48:53 2024 ] 	Batch(7400/7879) done. Loss: 0.0394  lr:0.000010
[ Sat Jul  6 14:49:11 2024 ] 
Training: Epoch [16/120], Step [7499], Loss: 0.024701831862330437, Training Accuracy: 97.73833333333334
[ Sat Jul  6 14:49:11 2024 ] 	Batch(7500/7879) done. Loss: 0.6067  lr:0.000010
[ Sat Jul  6 14:49:29 2024 ] 	Batch(7600/7879) done. Loss: 0.0707  lr:0.000010
[ Sat Jul  6 14:49:47 2024 ] 	Batch(7700/7879) done. Loss: 0.0043  lr:0.000010
[ Sat Jul  6 14:50:05 2024 ] 	Batch(7800/7879) done. Loss: 0.2423  lr:0.000010
[ Sat Jul  6 14:50:19 2024 ] 	Mean training loss: 0.0888.
[ Sat Jul  6 14:50:19 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 14:50:19 2024 ] Training epoch: 18
[ Sat Jul  6 14:50:19 2024 ] 	Batch(0/7879) done. Loss: 0.0281  lr:0.000010
[ Sat Jul  6 14:50:38 2024 ] 	Batch(100/7879) done. Loss: 0.0429  lr:0.000010
[ Sat Jul  6 14:50:56 2024 ] 	Batch(200/7879) done. Loss: 0.1610  lr:0.000010
[ Sat Jul  6 14:51:15 2024 ] 	Batch(300/7879) done. Loss: 0.1215  lr:0.000010
[ Sat Jul  6 14:51:33 2024 ] 	Batch(400/7879) done. Loss: 0.0604  lr:0.000010
[ Sat Jul  6 14:51:51 2024 ] 
Training: Epoch [17/120], Step [499], Loss: 0.03288128226995468, Training Accuracy: 97.725
[ Sat Jul  6 14:51:51 2024 ] 	Batch(500/7879) done. Loss: 0.1928  lr:0.000010
[ Sat Jul  6 14:52:10 2024 ] 	Batch(600/7879) done. Loss: 0.1285  lr:0.000010
[ Sat Jul  6 14:52:28 2024 ] 	Batch(700/7879) done. Loss: 0.0138  lr:0.000010
[ Sat Jul  6 14:52:46 2024 ] 	Batch(800/7879) done. Loss: 0.4888  lr:0.000010
[ Sat Jul  6 14:53:05 2024 ] 	Batch(900/7879) done. Loss: 0.0115  lr:0.000010
[ Sat Jul  6 14:53:23 2024 ] 
Training: Epoch [17/120], Step [999], Loss: 0.07218554615974426, Training Accuracy: 97.6375
[ Sat Jul  6 14:53:23 2024 ] 	Batch(1000/7879) done. Loss: 0.0610  lr:0.000010
[ Sat Jul  6 14:53:42 2024 ] 	Batch(1100/7879) done. Loss: 0.0534  lr:0.000010
[ Sat Jul  6 14:54:00 2024 ] 	Batch(1200/7879) done. Loss: 0.1781  lr:0.000010
[ Sat Jul  6 14:54:18 2024 ] 	Batch(1300/7879) done. Loss: 0.0225  lr:0.000010
[ Sat Jul  6 14:54:37 2024 ] 	Batch(1400/7879) done. Loss: 0.0146  lr:0.000010
[ Sat Jul  6 14:54:55 2024 ] 
Training: Epoch [17/120], Step [1499], Loss: 0.01375446654856205, Training Accuracy: 97.55833333333334
[ Sat Jul  6 14:54:55 2024 ] 	Batch(1500/7879) done. Loss: 0.0097  lr:0.000010
[ Sat Jul  6 14:55:13 2024 ] 	Batch(1600/7879) done. Loss: 0.0210  lr:0.000010
[ Sat Jul  6 14:55:32 2024 ] 	Batch(1700/7879) done. Loss: 0.0065  lr:0.000010
[ Sat Jul  6 14:55:50 2024 ] 	Batch(1800/7879) done. Loss: 0.0122  lr:0.000010
[ Sat Jul  6 14:56:08 2024 ] 	Batch(1900/7879) done. Loss: 0.0163  lr:0.000010
[ Sat Jul  6 14:56:27 2024 ] 
Training: Epoch [17/120], Step [1999], Loss: 0.2036137729883194, Training Accuracy: 97.6125
[ Sat Jul  6 14:56:27 2024 ] 	Batch(2000/7879) done. Loss: 0.0081  lr:0.000010
[ Sat Jul  6 14:56:45 2024 ] 	Batch(2100/7879) done. Loss: 0.1098  lr:0.000010
[ Sat Jul  6 14:57:03 2024 ] 	Batch(2200/7879) done. Loss: 0.0356  lr:0.000010
[ Sat Jul  6 14:57:21 2024 ] 	Batch(2300/7879) done. Loss: 0.0425  lr:0.000010
[ Sat Jul  6 14:57:39 2024 ] 	Batch(2400/7879) done. Loss: 0.0643  lr:0.000010
[ Sat Jul  6 14:57:57 2024 ] 
Training: Epoch [17/120], Step [2499], Loss: 0.0388420931994915, Training Accuracy: 97.61999999999999
[ Sat Jul  6 14:57:57 2024 ] 	Batch(2500/7879) done. Loss: 0.0061  lr:0.000010
[ Sat Jul  6 14:58:15 2024 ] 	Batch(2600/7879) done. Loss: 0.2411  lr:0.000010
[ Sat Jul  6 14:58:33 2024 ] 	Batch(2700/7879) done. Loss: 0.0272  lr:0.000010
[ Sat Jul  6 14:58:51 2024 ] 	Batch(2800/7879) done. Loss: 0.1543  lr:0.000010
[ Sat Jul  6 14:59:09 2024 ] 	Batch(2900/7879) done. Loss: 0.1593  lr:0.000010
[ Sat Jul  6 14:59:27 2024 ] 
Training: Epoch [17/120], Step [2999], Loss: 0.1164073497056961, Training Accuracy: 97.64166666666667
[ Sat Jul  6 14:59:27 2024 ] 	Batch(3000/7879) done. Loss: 0.0089  lr:0.000010
[ Sat Jul  6 14:59:45 2024 ] 	Batch(3100/7879) done. Loss: 0.0021  lr:0.000010
[ Sat Jul  6 15:00:03 2024 ] 	Batch(3200/7879) done. Loss: 0.0442  lr:0.000010
[ Sat Jul  6 15:00:21 2024 ] 	Batch(3300/7879) done. Loss: 0.1365  lr:0.000010
[ Sat Jul  6 15:00:39 2024 ] 	Batch(3400/7879) done. Loss: 0.0365  lr:0.000010
[ Sat Jul  6 15:00:57 2024 ] 
Training: Epoch [17/120], Step [3499], Loss: 0.047457799315452576, Training Accuracy: 97.61428571428571
[ Sat Jul  6 15:00:57 2024 ] 	Batch(3500/7879) done. Loss: 0.0096  lr:0.000010
[ Sat Jul  6 15:01:15 2024 ] 	Batch(3600/7879) done. Loss: 0.0049  lr:0.000010
[ Sat Jul  6 15:01:34 2024 ] 	Batch(3700/7879) done. Loss: 0.0700  lr:0.000010
[ Sat Jul  6 15:01:53 2024 ] 	Batch(3800/7879) done. Loss: 0.1245  lr:0.000010
[ Sat Jul  6 15:02:11 2024 ] 	Batch(3900/7879) done. Loss: 0.0724  lr:0.000010
[ Sat Jul  6 15:02:30 2024 ] 
Training: Epoch [17/120], Step [3999], Loss: 0.007943232543766499, Training Accuracy: 97.625
[ Sat Jul  6 15:02:30 2024 ] 	Batch(4000/7879) done. Loss: 0.0439  lr:0.000010
[ Sat Jul  6 15:02:48 2024 ] 	Batch(4100/7879) done. Loss: 0.1151  lr:0.000010
[ Sat Jul  6 15:03:06 2024 ] 	Batch(4200/7879) done. Loss: 0.1901  lr:0.000010
[ Sat Jul  6 15:03:24 2024 ] 	Batch(4300/7879) done. Loss: 0.0053  lr:0.000010
[ Sat Jul  6 15:03:42 2024 ] 	Batch(4400/7879) done. Loss: 0.1656  lr:0.000010
[ Sat Jul  6 15:04:00 2024 ] 
Training: Epoch [17/120], Step [4499], Loss: 0.057829905301332474, Training Accuracy: 97.67222222222223
[ Sat Jul  6 15:04:00 2024 ] 	Batch(4500/7879) done. Loss: 0.0585  lr:0.000010
[ Sat Jul  6 15:04:19 2024 ] 	Batch(4600/7879) done. Loss: 0.1806  lr:0.000010
[ Sat Jul  6 15:04:38 2024 ] 	Batch(4700/7879) done. Loss: 0.0384  lr:0.000010
[ Sat Jul  6 15:04:56 2024 ] 	Batch(4800/7879) done. Loss: 0.0334  lr:0.000010
[ Sat Jul  6 15:05:14 2024 ] 	Batch(4900/7879) done. Loss: 0.0145  lr:0.000010
[ Sat Jul  6 15:05:32 2024 ] 
Training: Epoch [17/120], Step [4999], Loss: 0.24707232415676117, Training Accuracy: 97.6625
[ Sat Jul  6 15:05:32 2024 ] 	Batch(5000/7879) done. Loss: 0.2845  lr:0.000010
[ Sat Jul  6 15:05:50 2024 ] 	Batch(5100/7879) done. Loss: 0.0062  lr:0.000010
[ Sat Jul  6 15:06:08 2024 ] 	Batch(5200/7879) done. Loss: 0.1293  lr:0.000010
[ Sat Jul  6 15:06:27 2024 ] 	Batch(5300/7879) done. Loss: 0.3322  lr:0.000010
[ Sat Jul  6 15:06:45 2024 ] 	Batch(5400/7879) done. Loss: 0.0887  lr:0.000010
[ Sat Jul  6 15:07:04 2024 ] 
Training: Epoch [17/120], Step [5499], Loss: 0.046452004462480545, Training Accuracy: 97.7
[ Sat Jul  6 15:07:04 2024 ] 	Batch(5500/7879) done. Loss: 0.0102  lr:0.000010
[ Sat Jul  6 15:07:23 2024 ] 	Batch(5600/7879) done. Loss: 0.0096  lr:0.000010
[ Sat Jul  6 15:07:41 2024 ] 	Batch(5700/7879) done. Loss: 0.0187  lr:0.000010
[ Sat Jul  6 15:07:59 2024 ] 	Batch(5800/7879) done. Loss: 0.1326  lr:0.000010
[ Sat Jul  6 15:08:17 2024 ] 	Batch(5900/7879) done. Loss: 0.2248  lr:0.000010
[ Sat Jul  6 15:08:34 2024 ] 
Training: Epoch [17/120], Step [5999], Loss: 0.008546795696020126, Training Accuracy: 97.70833333333333
[ Sat Jul  6 15:08:35 2024 ] 	Batch(6000/7879) done. Loss: 0.0104  lr:0.000010
[ Sat Jul  6 15:08:53 2024 ] 	Batch(6100/7879) done. Loss: 0.0618  lr:0.000010
[ Sat Jul  6 15:09:11 2024 ] 	Batch(6200/7879) done. Loss: 0.0748  lr:0.000010
[ Sat Jul  6 15:09:29 2024 ] 	Batch(6300/7879) done. Loss: 0.0260  lr:0.000010
[ Sat Jul  6 15:09:47 2024 ] 	Batch(6400/7879) done. Loss: 0.0113  lr:0.000010
[ Sat Jul  6 15:10:05 2024 ] 
Training: Epoch [17/120], Step [6499], Loss: 0.05109531059861183, Training Accuracy: 97.7173076923077
[ Sat Jul  6 15:10:05 2024 ] 	Batch(6500/7879) done. Loss: 0.0209  lr:0.000010
[ Sat Jul  6 15:10:24 2024 ] 	Batch(6600/7879) done. Loss: 0.0472  lr:0.000010
[ Sat Jul  6 15:10:42 2024 ] 	Batch(6700/7879) done. Loss: 0.0018  lr:0.000010
[ Sat Jul  6 15:11:01 2024 ] 	Batch(6800/7879) done. Loss: 0.0902  lr:0.000010
[ Sat Jul  6 15:11:19 2024 ] 	Batch(6900/7879) done. Loss: 0.0621  lr:0.000010
[ Sat Jul  6 15:11:37 2024 ] 
Training: Epoch [17/120], Step [6999], Loss: 0.004221652168780565, Training Accuracy: 97.7125
[ Sat Jul  6 15:11:37 2024 ] 	Batch(7000/7879) done. Loss: 0.2899  lr:0.000010
[ Sat Jul  6 15:11:55 2024 ] 	Batch(7100/7879) done. Loss: 0.1074  lr:0.000010
[ Sat Jul  6 15:12:13 2024 ] 	Batch(7200/7879) done. Loss: 0.0291  lr:0.000010
[ Sat Jul  6 15:12:32 2024 ] 	Batch(7300/7879) done. Loss: 0.2421  lr:0.000010
[ Sat Jul  6 15:12:50 2024 ] 	Batch(7400/7879) done. Loss: 0.0391  lr:0.000010
[ Sat Jul  6 15:13:09 2024 ] 
Training: Epoch [17/120], Step [7499], Loss: 0.029146652668714523, Training Accuracy: 97.72
[ Sat Jul  6 15:13:09 2024 ] 	Batch(7500/7879) done. Loss: 0.0434  lr:0.000010
[ Sat Jul  6 15:13:28 2024 ] 	Batch(7600/7879) done. Loss: 0.0279  lr:0.000010
[ Sat Jul  6 15:13:46 2024 ] 	Batch(7700/7879) done. Loss: 0.0651  lr:0.000010
[ Sat Jul  6 15:14:05 2024 ] 	Batch(7800/7879) done. Loss: 0.0402  lr:0.000010
[ Sat Jul  6 15:14:19 2024 ] 	Mean training loss: 0.0903.
[ Sat Jul  6 15:14:19 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 15:14:20 2024 ] Training epoch: 19
[ Sat Jul  6 15:14:20 2024 ] 	Batch(0/7879) done. Loss: 0.1403  lr:0.000010
[ Sat Jul  6 15:14:38 2024 ] 	Batch(100/7879) done. Loss: 0.2146  lr:0.000010
[ Sat Jul  6 15:14:56 2024 ] 	Batch(200/7879) done. Loss: 0.0266  lr:0.000010
[ Sat Jul  6 15:15:14 2024 ] 	Batch(300/7879) done. Loss: 0.1743  lr:0.000010
[ Sat Jul  6 15:15:32 2024 ] 	Batch(400/7879) done. Loss: 0.0104  lr:0.000010
[ Sat Jul  6 15:15:50 2024 ] 
Training: Epoch [18/120], Step [499], Loss: 0.03331257775425911, Training Accuracy: 97.85000000000001
[ Sat Jul  6 15:15:50 2024 ] 	Batch(500/7879) done. Loss: 0.0133  lr:0.000010
[ Sat Jul  6 15:16:08 2024 ] 	Batch(600/7879) done. Loss: 0.1661  lr:0.000010
[ Sat Jul  6 15:16:26 2024 ] 	Batch(700/7879) done. Loss: 0.0061  lr:0.000010
[ Sat Jul  6 15:16:44 2024 ] 	Batch(800/7879) done. Loss: 0.0277  lr:0.000010
[ Sat Jul  6 15:17:02 2024 ] 	Batch(900/7879) done. Loss: 0.0704  lr:0.000010
[ Sat Jul  6 15:17:20 2024 ] 
Training: Epoch [18/120], Step [999], Loss: 0.004530671983957291, Training Accuracy: 97.91250000000001
[ Sat Jul  6 15:17:20 2024 ] 	Batch(1000/7879) done. Loss: 0.0086  lr:0.000010
[ Sat Jul  6 15:17:38 2024 ] 	Batch(1100/7879) done. Loss: 0.0227  lr:0.000010
[ Sat Jul  6 15:17:56 2024 ] 	Batch(1200/7879) done. Loss: 0.1266  lr:0.000010
[ Sat Jul  6 15:18:14 2024 ] 	Batch(1300/7879) done. Loss: 0.0677  lr:0.000010
[ Sat Jul  6 15:18:33 2024 ] 	Batch(1400/7879) done. Loss: 0.0300  lr:0.000010
[ Sat Jul  6 15:18:50 2024 ] 
Training: Epoch [18/120], Step [1499], Loss: 0.02291257120668888, Training Accuracy: 97.86666666666667
[ Sat Jul  6 15:18:50 2024 ] 	Batch(1500/7879) done. Loss: 0.0173  lr:0.000010
[ Sat Jul  6 15:19:09 2024 ] 	Batch(1600/7879) done. Loss: 0.1497  lr:0.000010
[ Sat Jul  6 15:19:26 2024 ] 	Batch(1700/7879) done. Loss: 0.0106  lr:0.000010
[ Sat Jul  6 15:19:44 2024 ] 	Batch(1800/7879) done. Loss: 0.0884  lr:0.000010
[ Sat Jul  6 15:20:02 2024 ] 	Batch(1900/7879) done. Loss: 0.0144  lr:0.000010
[ Sat Jul  6 15:20:20 2024 ] 
Training: Epoch [18/120], Step [1999], Loss: 0.015613122843205929, Training Accuracy: 97.8875
[ Sat Jul  6 15:20:20 2024 ] 	Batch(2000/7879) done. Loss: 0.0466  lr:0.000010
[ Sat Jul  6 15:20:38 2024 ] 	Batch(2100/7879) done. Loss: 0.0150  lr:0.000010
[ Sat Jul  6 15:20:56 2024 ] 	Batch(2200/7879) done. Loss: 0.0274  lr:0.000010
[ Sat Jul  6 15:21:14 2024 ] 	Batch(2300/7879) done. Loss: 0.2889  lr:0.000010
[ Sat Jul  6 15:21:32 2024 ] 	Batch(2400/7879) done. Loss: 0.0685  lr:0.000010
[ Sat Jul  6 15:21:50 2024 ] 
Training: Epoch [18/120], Step [2499], Loss: 0.02999795973300934, Training Accuracy: 97.83500000000001
[ Sat Jul  6 15:21:50 2024 ] 	Batch(2500/7879) done. Loss: 0.0695  lr:0.000010
[ Sat Jul  6 15:22:08 2024 ] 	Batch(2600/7879) done. Loss: 0.0111  lr:0.000010
[ Sat Jul  6 15:22:26 2024 ] 	Batch(2700/7879) done. Loss: 0.5956  lr:0.000010
[ Sat Jul  6 15:22:44 2024 ] 	Batch(2800/7879) done. Loss: 0.0106  lr:0.000010
[ Sat Jul  6 15:23:03 2024 ] 	Batch(2900/7879) done. Loss: 0.0197  lr:0.000010
[ Sat Jul  6 15:23:21 2024 ] 
Training: Epoch [18/120], Step [2999], Loss: 0.04127209633588791, Training Accuracy: 97.8
[ Sat Jul  6 15:23:21 2024 ] 	Batch(3000/7879) done. Loss: 0.1307  lr:0.000010
[ Sat Jul  6 15:23:40 2024 ] 	Batch(3100/7879) done. Loss: 0.0255  lr:0.000010
[ Sat Jul  6 15:23:59 2024 ] 	Batch(3200/7879) done. Loss: 0.0375  lr:0.000010
[ Sat Jul  6 15:24:17 2024 ] 	Batch(3300/7879) done. Loss: 0.1898  lr:0.000010
[ Sat Jul  6 15:24:34 2024 ] 	Batch(3400/7879) done. Loss: 0.0149  lr:0.000010
[ Sat Jul  6 15:24:52 2024 ] 
Training: Epoch [18/120], Step [3499], Loss: 0.3243257999420166, Training Accuracy: 97.76428571428572
[ Sat Jul  6 15:24:52 2024 ] 	Batch(3500/7879) done. Loss: 0.0543  lr:0.000010
[ Sat Jul  6 15:25:11 2024 ] 	Batch(3600/7879) done. Loss: 0.0376  lr:0.000010
[ Sat Jul  6 15:25:28 2024 ] 	Batch(3700/7879) done. Loss: 0.0390  lr:0.000010
[ Sat Jul  6 15:25:46 2024 ] 	Batch(3800/7879) done. Loss: 0.1958  lr:0.000010
[ Sat Jul  6 15:26:04 2024 ] 	Batch(3900/7879) done. Loss: 0.0153  lr:0.000010
[ Sat Jul  6 15:26:22 2024 ] 
Training: Epoch [18/120], Step [3999], Loss: 0.1277168244123459, Training Accuracy: 97.765625
[ Sat Jul  6 15:26:22 2024 ] 	Batch(4000/7879) done. Loss: 0.2404  lr:0.000010
[ Sat Jul  6 15:26:40 2024 ] 	Batch(4100/7879) done. Loss: 0.0140  lr:0.000010
[ Sat Jul  6 15:26:58 2024 ] 	Batch(4200/7879) done. Loss: 0.0235  lr:0.000010
[ Sat Jul  6 15:27:16 2024 ] 	Batch(4300/7879) done. Loss: 0.0030  lr:0.000010
[ Sat Jul  6 15:27:34 2024 ] 	Batch(4400/7879) done. Loss: 0.0883  lr:0.000010
[ Sat Jul  6 15:27:52 2024 ] 
Training: Epoch [18/120], Step [4499], Loss: 0.042754292488098145, Training Accuracy: 97.72777777777777
[ Sat Jul  6 15:27:52 2024 ] 	Batch(4500/7879) done. Loss: 0.3131  lr:0.000010
[ Sat Jul  6 15:28:10 2024 ] 	Batch(4600/7879) done. Loss: 0.0569  lr:0.000010
[ Sat Jul  6 15:28:28 2024 ] 	Batch(4700/7879) done. Loss: 0.3065  lr:0.000010
[ Sat Jul  6 15:28:46 2024 ] 	Batch(4800/7879) done. Loss: 0.0613  lr:0.000010
[ Sat Jul  6 15:29:04 2024 ] 	Batch(4900/7879) done. Loss: 0.0103  lr:0.000010
[ Sat Jul  6 15:29:22 2024 ] 
Training: Epoch [18/120], Step [4999], Loss: 0.007959526963531971, Training Accuracy: 97.7175
[ Sat Jul  6 15:29:22 2024 ] 	Batch(5000/7879) done. Loss: 0.1574  lr:0.000010
[ Sat Jul  6 15:29:40 2024 ] 	Batch(5100/7879) done. Loss: 0.0269  lr:0.000010
[ Sat Jul  6 15:29:58 2024 ] 	Batch(5200/7879) done. Loss: 0.0409  lr:0.000010
[ Sat Jul  6 15:30:16 2024 ] 	Batch(5300/7879) done. Loss: 0.0479  lr:0.000010
[ Sat Jul  6 15:30:34 2024 ] 	Batch(5400/7879) done. Loss: 0.0462  lr:0.000010
[ Sat Jul  6 15:30:52 2024 ] 
Training: Epoch [18/120], Step [5499], Loss: 0.021684831008315086, Training Accuracy: 97.76136363636364
[ Sat Jul  6 15:30:52 2024 ] 	Batch(5500/7879) done. Loss: 0.0229  lr:0.000010
[ Sat Jul  6 15:31:10 2024 ] 	Batch(5600/7879) done. Loss: 0.1381  lr:0.000010
[ Sat Jul  6 15:31:29 2024 ] 	Batch(5700/7879) done. Loss: 0.2731  lr:0.000010
[ Sat Jul  6 15:31:48 2024 ] 	Batch(5800/7879) done. Loss: 0.0470  lr:0.000010
[ Sat Jul  6 15:32:06 2024 ] 	Batch(5900/7879) done. Loss: 0.0581  lr:0.000010
[ Sat Jul  6 15:32:25 2024 ] 
Training: Epoch [18/120], Step [5999], Loss: 0.4728618264198303, Training Accuracy: 97.74583333333334
[ Sat Jul  6 15:32:25 2024 ] 	Batch(6000/7879) done. Loss: 0.0265  lr:0.000010
[ Sat Jul  6 15:32:43 2024 ] 	Batch(6100/7879) done. Loss: 0.0079  lr:0.000010
[ Sat Jul  6 15:33:02 2024 ] 	Batch(6200/7879) done. Loss: 0.0744  lr:0.000010
[ Sat Jul  6 15:33:21 2024 ] 	Batch(6300/7879) done. Loss: 0.0642  lr:0.000010
[ Sat Jul  6 15:33:39 2024 ] 	Batch(6400/7879) done. Loss: 0.0916  lr:0.000010
[ Sat Jul  6 15:33:58 2024 ] 
Training: Epoch [18/120], Step [6499], Loss: 0.38782086968421936, Training Accuracy: 97.73076923076923
[ Sat Jul  6 15:33:58 2024 ] 	Batch(6500/7879) done. Loss: 0.5183  lr:0.000010
[ Sat Jul  6 15:34:16 2024 ] 	Batch(6600/7879) done. Loss: 0.0887  lr:0.000010
[ Sat Jul  6 15:34:34 2024 ] 	Batch(6700/7879) done. Loss: 0.4300  lr:0.000010
[ Sat Jul  6 15:34:52 2024 ] 	Batch(6800/7879) done. Loss: 0.0302  lr:0.000010
[ Sat Jul  6 15:35:11 2024 ] 	Batch(6900/7879) done. Loss: 0.0789  lr:0.000010
[ Sat Jul  6 15:35:29 2024 ] 
Training: Epoch [18/120], Step [6999], Loss: 0.26867368817329407, Training Accuracy: 97.73571428571428
[ Sat Jul  6 15:35:30 2024 ] 	Batch(7000/7879) done. Loss: 0.0211  lr:0.000010
[ Sat Jul  6 15:35:48 2024 ] 	Batch(7100/7879) done. Loss: 0.0105  lr:0.000010
[ Sat Jul  6 15:36:07 2024 ] 	Batch(7200/7879) done. Loss: 0.2670  lr:0.000010
[ Sat Jul  6 15:36:25 2024 ] 	Batch(7300/7879) done. Loss: 0.0534  lr:0.000010
[ Sat Jul  6 15:36:44 2024 ] 	Batch(7400/7879) done. Loss: 0.2154  lr:0.000010
[ Sat Jul  6 15:37:02 2024 ] 
Training: Epoch [18/120], Step [7499], Loss: 0.4178464412689209, Training Accuracy: 97.72833333333332
[ Sat Jul  6 15:37:03 2024 ] 	Batch(7500/7879) done. Loss: 0.0131  lr:0.000010
[ Sat Jul  6 15:37:21 2024 ] 	Batch(7600/7879) done. Loss: 0.0774  lr:0.000010
[ Sat Jul  6 15:37:40 2024 ] 	Batch(7700/7879) done. Loss: 0.1377  lr:0.000010
[ Sat Jul  6 15:37:58 2024 ] 	Batch(7800/7879) done. Loss: 0.4331  lr:0.000010
[ Sat Jul  6 15:38:12 2024 ] 	Mean training loss: 0.0901.
[ Sat Jul  6 15:38:12 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 15:38:12 2024 ] Training epoch: 20
[ Sat Jul  6 15:38:13 2024 ] 	Batch(0/7879) done. Loss: 0.0782  lr:0.000010
[ Sat Jul  6 15:38:31 2024 ] 	Batch(100/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 15:38:50 2024 ] 	Batch(200/7879) done. Loss: 0.0077  lr:0.000010
[ Sat Jul  6 15:39:08 2024 ] 	Batch(300/7879) done. Loss: 0.0669  lr:0.000010
[ Sat Jul  6 15:39:26 2024 ] 	Batch(400/7879) done. Loss: 0.0061  lr:0.000010
[ Sat Jul  6 15:39:44 2024 ] 
Training: Epoch [19/120], Step [499], Loss: 0.08145757764577866, Training Accuracy: 97.775
[ Sat Jul  6 15:39:44 2024 ] 	Batch(500/7879) done. Loss: 0.0082  lr:0.000010
[ Sat Jul  6 15:40:02 2024 ] 	Batch(600/7879) done. Loss: 0.0181  lr:0.000010
[ Sat Jul  6 15:40:20 2024 ] 	Batch(700/7879) done. Loss: 0.0195  lr:0.000010
[ Sat Jul  6 15:40:39 2024 ] 	Batch(800/7879) done. Loss: 0.0902  lr:0.000010
[ Sat Jul  6 15:40:57 2024 ] 	Batch(900/7879) done. Loss: 0.0729  lr:0.000010
[ Sat Jul  6 15:41:15 2024 ] 
Training: Epoch [19/120], Step [999], Loss: 0.009840819984674454, Training Accuracy: 97.8
[ Sat Jul  6 15:41:15 2024 ] 	Batch(1000/7879) done. Loss: 0.0829  lr:0.000010
[ Sat Jul  6 15:41:33 2024 ] 	Batch(1100/7879) done. Loss: 0.0040  lr:0.000010
[ Sat Jul  6 15:41:51 2024 ] 	Batch(1200/7879) done. Loss: 0.0878  lr:0.000010
[ Sat Jul  6 15:42:09 2024 ] 	Batch(1300/7879) done. Loss: 0.1441  lr:0.000010
[ Sat Jul  6 15:42:28 2024 ] 	Batch(1400/7879) done. Loss: 0.0284  lr:0.000010
[ Sat Jul  6 15:42:46 2024 ] 
Training: Epoch [19/120], Step [1499], Loss: 0.0063802930526435375, Training Accuracy: 97.86666666666667
[ Sat Jul  6 15:42:46 2024 ] 	Batch(1500/7879) done. Loss: 0.0240  lr:0.000010
[ Sat Jul  6 15:43:04 2024 ] 	Batch(1600/7879) done. Loss: 0.0385  lr:0.000010
[ Sat Jul  6 15:43:23 2024 ] 	Batch(1700/7879) done. Loss: 0.0110  lr:0.000010
[ Sat Jul  6 15:43:41 2024 ] 	Batch(1800/7879) done. Loss: 0.0249  lr:0.000010
[ Sat Jul  6 15:44:00 2024 ] 	Batch(1900/7879) done. Loss: 0.0849  lr:0.000010
[ Sat Jul  6 15:44:18 2024 ] 
Training: Epoch [19/120], Step [1999], Loss: 0.061308909207582474, Training Accuracy: 97.925
[ Sat Jul  6 15:44:19 2024 ] 	Batch(2000/7879) done. Loss: 0.2424  lr:0.000010
[ Sat Jul  6 15:44:36 2024 ] 	Batch(2100/7879) done. Loss: 0.2275  lr:0.000010
[ Sat Jul  6 15:44:54 2024 ] 	Batch(2200/7879) done. Loss: 0.2205  lr:0.000010
[ Sat Jul  6 15:45:12 2024 ] 	Batch(2300/7879) done. Loss: 0.0027  lr:0.000010
[ Sat Jul  6 15:45:31 2024 ] 	Batch(2400/7879) done. Loss: 0.0429  lr:0.000010
[ Sat Jul  6 15:45:49 2024 ] 
Training: Epoch [19/120], Step [2499], Loss: 0.04092991352081299, Training Accuracy: 97.84
[ Sat Jul  6 15:45:49 2024 ] 	Batch(2500/7879) done. Loss: 0.0644  lr:0.000010
[ Sat Jul  6 15:46:08 2024 ] 	Batch(2600/7879) done. Loss: 0.0296  lr:0.000010
[ Sat Jul  6 15:46:26 2024 ] 	Batch(2700/7879) done. Loss: 0.0965  lr:0.000010
[ Sat Jul  6 15:46:45 2024 ] 	Batch(2800/7879) done. Loss: 0.0445  lr:0.000010
[ Sat Jul  6 15:47:03 2024 ] 	Batch(2900/7879) done. Loss: 0.0071  lr:0.000010
[ Sat Jul  6 15:47:21 2024 ] 
Training: Epoch [19/120], Step [2999], Loss: 0.019316701218485832, Training Accuracy: 97.85833333333333
[ Sat Jul  6 15:47:21 2024 ] 	Batch(3000/7879) done. Loss: 0.4044  lr:0.000010
[ Sat Jul  6 15:47:39 2024 ] 	Batch(3100/7879) done. Loss: 0.0081  lr:0.000010
[ Sat Jul  6 15:47:57 2024 ] 	Batch(3200/7879) done. Loss: 0.0224  lr:0.000010
[ Sat Jul  6 15:48:15 2024 ] 	Batch(3300/7879) done. Loss: 0.0838  lr:0.000010
[ Sat Jul  6 15:48:33 2024 ] 	Batch(3400/7879) done. Loss: 0.0572  lr:0.000010
[ Sat Jul  6 15:48:51 2024 ] 
Training: Epoch [19/120], Step [3499], Loss: 0.014055898413062096, Training Accuracy: 97.84285714285714
[ Sat Jul  6 15:48:51 2024 ] 	Batch(3500/7879) done. Loss: 0.0314  lr:0.000010
[ Sat Jul  6 15:49:09 2024 ] 	Batch(3600/7879) done. Loss: 0.2434  lr:0.000010
[ Sat Jul  6 15:49:27 2024 ] 	Batch(3700/7879) done. Loss: 0.0071  lr:0.000010
[ Sat Jul  6 15:49:45 2024 ] 	Batch(3800/7879) done. Loss: 0.0051  lr:0.000010
[ Sat Jul  6 15:50:03 2024 ] 	Batch(3900/7879) done. Loss: 0.0046  lr:0.000010
[ Sat Jul  6 15:50:21 2024 ] 
Training: Epoch [19/120], Step [3999], Loss: 0.04407111927866936, Training Accuracy: 97.78750000000001
[ Sat Jul  6 15:50:21 2024 ] 	Batch(4000/7879) done. Loss: 0.0139  lr:0.000010
[ Sat Jul  6 15:50:39 2024 ] 	Batch(4100/7879) done. Loss: 0.0205  lr:0.000010
[ Sat Jul  6 15:50:57 2024 ] 	Batch(4200/7879) done. Loss: 0.0111  lr:0.000010
[ Sat Jul  6 15:51:15 2024 ] 	Batch(4300/7879) done. Loss: 0.1245  lr:0.000010
[ Sat Jul  6 15:51:33 2024 ] 	Batch(4400/7879) done. Loss: 0.0181  lr:0.000010
[ Sat Jul  6 15:51:51 2024 ] 
Training: Epoch [19/120], Step [4499], Loss: 0.057581447064876556, Training Accuracy: 97.77222222222223
[ Sat Jul  6 15:51:51 2024 ] 	Batch(4500/7879) done. Loss: 0.0121  lr:0.000010
[ Sat Jul  6 15:52:09 2024 ] 	Batch(4600/7879) done. Loss: 0.0108  lr:0.000010
[ Sat Jul  6 15:52:27 2024 ] 	Batch(4700/7879) done. Loss: 0.0713  lr:0.000010
[ Sat Jul  6 15:52:45 2024 ] 	Batch(4800/7879) done. Loss: 0.0437  lr:0.000010
[ Sat Jul  6 15:53:03 2024 ] 	Batch(4900/7879) done. Loss: 0.1654  lr:0.000010
[ Sat Jul  6 15:53:21 2024 ] 
Training: Epoch [19/120], Step [4999], Loss: 0.08402815461158752, Training Accuracy: 97.75500000000001
[ Sat Jul  6 15:53:21 2024 ] 	Batch(5000/7879) done. Loss: 0.2217  lr:0.000010
[ Sat Jul  6 15:53:39 2024 ] 	Batch(5100/7879) done. Loss: 0.1444  lr:0.000010
[ Sat Jul  6 15:53:57 2024 ] 	Batch(5200/7879) done. Loss: 0.0244  lr:0.000010
[ Sat Jul  6 15:54:15 2024 ] 	Batch(5300/7879) done. Loss: 0.1206  lr:0.000010
[ Sat Jul  6 15:54:33 2024 ] 	Batch(5400/7879) done. Loss: 0.0312  lr:0.000010
[ Sat Jul  6 15:54:51 2024 ] 
Training: Epoch [19/120], Step [5499], Loss: 0.1549663543701172, Training Accuracy: 97.7590909090909
[ Sat Jul  6 15:54:51 2024 ] 	Batch(5500/7879) done. Loss: 0.0024  lr:0.000010
[ Sat Jul  6 15:55:09 2024 ] 	Batch(5600/7879) done. Loss: 0.1452  lr:0.000010
[ Sat Jul  6 15:55:27 2024 ] 	Batch(5700/7879) done. Loss: 0.0126  lr:0.000010
[ Sat Jul  6 15:55:45 2024 ] 	Batch(5800/7879) done. Loss: 0.1963  lr:0.000010
[ Sat Jul  6 15:56:03 2024 ] 	Batch(5900/7879) done. Loss: 0.0206  lr:0.000010
[ Sat Jul  6 15:56:20 2024 ] 
Training: Epoch [19/120], Step [5999], Loss: 0.08490759134292603, Training Accuracy: 97.74166666666667
[ Sat Jul  6 15:56:21 2024 ] 	Batch(6000/7879) done. Loss: 0.4471  lr:0.000010
[ Sat Jul  6 15:56:39 2024 ] 	Batch(6100/7879) done. Loss: 0.1502  lr:0.000010
[ Sat Jul  6 15:56:57 2024 ] 	Batch(6200/7879) done. Loss: 0.0100  lr:0.000010
[ Sat Jul  6 15:57:15 2024 ] 	Batch(6300/7879) done. Loss: 0.0343  lr:0.000010
[ Sat Jul  6 15:57:33 2024 ] 	Batch(6400/7879) done. Loss: 0.0162  lr:0.000010
[ Sat Jul  6 15:57:50 2024 ] 
Training: Epoch [19/120], Step [6499], Loss: 0.10941051691770554, Training Accuracy: 97.72884615384615
[ Sat Jul  6 15:57:51 2024 ] 	Batch(6500/7879) done. Loss: 0.0165  lr:0.000010
[ Sat Jul  6 15:58:09 2024 ] 	Batch(6600/7879) done. Loss: 0.0052  lr:0.000010
[ Sat Jul  6 15:58:26 2024 ] 	Batch(6700/7879) done. Loss: 0.0159  lr:0.000010
[ Sat Jul  6 15:58:45 2024 ] 	Batch(6800/7879) done. Loss: 0.0264  lr:0.000010
[ Sat Jul  6 15:59:03 2024 ] 	Batch(6900/7879) done. Loss: 0.0366  lr:0.000010
[ Sat Jul  6 15:59:20 2024 ] 
Training: Epoch [19/120], Step [6999], Loss: 0.022539369761943817, Training Accuracy: 97.73214285714286
[ Sat Jul  6 15:59:21 2024 ] 	Batch(7000/7879) done. Loss: 0.0988  lr:0.000010
[ Sat Jul  6 15:59:39 2024 ] 	Batch(7100/7879) done. Loss: 0.0289  lr:0.000010
[ Sat Jul  6 15:59:57 2024 ] 	Batch(7200/7879) done. Loss: 0.0681  lr:0.000010
[ Sat Jul  6 16:00:15 2024 ] 	Batch(7300/7879) done. Loss: 0.0716  lr:0.000010
[ Sat Jul  6 16:00:32 2024 ] 	Batch(7400/7879) done. Loss: 0.0086  lr:0.000010
[ Sat Jul  6 16:00:50 2024 ] 
Training: Epoch [19/120], Step [7499], Loss: 0.014029685407876968, Training Accuracy: 97.74000000000001
[ Sat Jul  6 16:00:50 2024 ] 	Batch(7500/7879) done. Loss: 0.0412  lr:0.000010
[ Sat Jul  6 16:01:08 2024 ] 	Batch(7600/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 16:01:26 2024 ] 	Batch(7700/7879) done. Loss: 0.1219  lr:0.000010
[ Sat Jul  6 16:01:44 2024 ] 	Batch(7800/7879) done. Loss: 0.0240  lr:0.000010
[ Sat Jul  6 16:01:58 2024 ] 	Mean training loss: 0.0919.
[ Sat Jul  6 16:01:58 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 16:01:58 2024 ] Eval epoch: 20
[ Sat Jul  6 16:06:45 2024 ] 	Mean val loss of 6365 batches: 1.677497546619391.
[ Sat Jul  6 16:06:45 2024 ] 
Validation: Epoch [19/120], Samples [39619.0/50919], Loss: 0.04461929202079773, Validation Accuracy: 77.8078909640802
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 1 : 200 / 275 = 72 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 2 : 218 / 273 = 79 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 3 : 222 / 273 = 81 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 4 : 232 / 275 = 84 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 5 : 245 / 275 = 89 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 6 : 226 / 275 = 82 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 7 : 253 / 273 = 92 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 8 : 262 / 273 = 95 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 9 : 197 / 273 = 72 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 10 : 127 / 273 = 46 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 11 : 130 / 272 = 47 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 12 : 223 / 271 = 82 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 13 : 268 / 275 = 97 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 14 : 268 / 276 = 97 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 15 : 232 / 273 = 84 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 16 : 222 / 274 = 81 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 17 : 236 / 273 = 86 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 18 : 245 / 274 = 89 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 19 : 254 / 272 = 93 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 20 : 249 / 273 = 91 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 21 : 220 / 274 = 80 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 22 : 237 / 274 = 86 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 23 : 259 / 276 = 93 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 24 : 224 / 274 = 81 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 25 : 261 / 275 = 94 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 26 : 270 / 276 = 97 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 27 : 222 / 275 = 80 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 28 : 167 / 275 = 60 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 29 : 144 / 275 = 52 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 30 : 181 / 276 = 65 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 31 : 244 / 276 = 88 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 32 : 244 / 276 = 88 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 33 : 233 / 276 = 84 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 34 : 242 / 276 = 87 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 35 : 242 / 275 = 88 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 36 : 234 / 276 = 84 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 37 : 246 / 276 = 89 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 38 : 252 / 276 = 91 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 39 : 253 / 276 = 91 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 40 : 187 / 276 = 67 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 41 : 261 / 276 = 94 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 42 : 260 / 275 = 94 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 43 : 185 / 276 = 67 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 44 : 230 / 276 = 83 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 45 : 255 / 276 = 92 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 46 : 220 / 276 = 79 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 47 : 218 / 275 = 79 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 48 : 225 / 275 = 81 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 49 : 231 / 274 = 84 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 50 : 240 / 276 = 86 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 51 : 253 / 276 = 91 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 52 : 237 / 276 = 85 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 53 : 237 / 276 = 85 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 54 : 266 / 274 = 97 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 55 : 242 / 276 = 87 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 56 : 245 / 275 = 89 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 57 : 262 / 276 = 94 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 58 : 268 / 273 = 98 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 59 : 267 / 276 = 96 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 60 : 469 / 561 = 83 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 61 : 455 / 566 = 80 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 62 : 454 / 572 = 79 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 63 : 520 / 570 = 91 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 64 : 435 / 574 = 75 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 65 : 502 / 573 = 87 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 66 : 370 / 573 = 64 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 67 : 419 / 575 = 72 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 68 : 319 / 575 = 55 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 69 : 470 / 575 = 81 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 70 : 237 / 575 = 41 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 71 : 206 / 575 = 35 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 72 : 156 / 571 = 27 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 73 : 213 / 570 = 37 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 74 : 385 / 569 = 67 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 75 : 258 / 573 = 45 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 76 : 360 / 574 = 62 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 77 : 395 / 573 = 68 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 78 : 407 / 575 = 70 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 79 : 543 / 574 = 94 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 80 : 498 / 573 = 86 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 81 : 336 / 575 = 58 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 82 : 387 / 575 = 67 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 83 : 323 / 572 = 56 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 84 : 428 / 574 = 74 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 85 : 397 / 574 = 69 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 86 : 508 / 575 = 88 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 87 : 508 / 576 = 88 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 88 : 434 / 575 = 75 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 89 : 484 / 576 = 84 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 90 : 278 / 574 = 48 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 91 : 461 / 568 = 81 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 92 : 372 / 576 = 64 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 93 : 431 / 573 = 75 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 94 : 506 / 574 = 88 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 95 : 521 / 575 = 90 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 96 : 560 / 575 = 97 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 97 : 554 / 574 = 96 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 98 : 537 / 575 = 93 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 99 : 544 / 574 = 94 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 100 : 490 / 574 = 85 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 101 : 516 / 574 = 89 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 102 : 364 / 575 = 63 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 103 : 496 / 576 = 86 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 104 : 284 / 575 = 49 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 105 : 246 / 575 = 42 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 106 : 327 / 576 = 56 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 107 : 482 / 576 = 83 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 108 : 484 / 575 = 84 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 109 : 372 / 575 = 64 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 110 : 484 / 575 = 84 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 111 : 541 / 576 = 93 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 112 : 545 / 575 = 94 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 113 : 505 / 576 = 87 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 114 : 482 / 576 = 83 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 115 : 503 / 576 = 87 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 116 : 478 / 575 = 83 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 117 : 458 / 575 = 79 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 118 : 490 / 575 = 85 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 119 : 525 / 576 = 91 %
[ Sat Jul  6 16:06:45 2024 ] Accuracy of 120 : 234 / 274 = 85 %
[ Sat Jul  6 16:06:45 2024 ] Training epoch: 21
[ Sat Jul  6 16:06:46 2024 ] 	Batch(0/7879) done. Loss: 0.0468  lr:0.000010
[ Sat Jul  6 16:07:04 2024 ] 	Batch(100/7879) done. Loss: 0.1315  lr:0.000010
[ Sat Jul  6 16:07:23 2024 ] 	Batch(200/7879) done. Loss: 0.0495  lr:0.000010
[ Sat Jul  6 16:07:41 2024 ] 	Batch(300/7879) done. Loss: 0.0109  lr:0.000010
[ Sat Jul  6 16:08:00 2024 ] 	Batch(400/7879) done. Loss: 0.0159  lr:0.000010
[ Sat Jul  6 16:08:18 2024 ] 
Training: Epoch [20/120], Step [499], Loss: 0.003996254410594702, Training Accuracy: 97.675
[ Sat Jul  6 16:08:18 2024 ] 	Batch(500/7879) done. Loss: 0.1162  lr:0.000010
[ Sat Jul  6 16:08:36 2024 ] 	Batch(600/7879) done. Loss: 0.0103  lr:0.000010
[ Sat Jul  6 16:08:54 2024 ] 	Batch(700/7879) done. Loss: 0.0602  lr:0.000010
[ Sat Jul  6 16:09:12 2024 ] 	Batch(800/7879) done. Loss: 0.1320  lr:0.000010
[ Sat Jul  6 16:09:30 2024 ] 	Batch(900/7879) done. Loss: 0.0165  lr:0.000010
[ Sat Jul  6 16:09:48 2024 ] 
Training: Epoch [20/120], Step [999], Loss: 0.15585125982761383, Training Accuracy: 97.82499999999999
[ Sat Jul  6 16:09:48 2024 ] 	Batch(1000/7879) done. Loss: 0.0191  lr:0.000010
[ Sat Jul  6 16:10:06 2024 ] 	Batch(1100/7879) done. Loss: 0.0134  lr:0.000010
[ Sat Jul  6 16:10:24 2024 ] 	Batch(1200/7879) done. Loss: 0.1463  lr:0.000010
[ Sat Jul  6 16:10:42 2024 ] 	Batch(1300/7879) done. Loss: 0.1369  lr:0.000010
[ Sat Jul  6 16:11:00 2024 ] 	Batch(1400/7879) done. Loss: 0.2617  lr:0.000010
[ Sat Jul  6 16:11:17 2024 ] 
Training: Epoch [20/120], Step [1499], Loss: 0.010561911389231682, Training Accuracy: 97.80833333333334
[ Sat Jul  6 16:11:18 2024 ] 	Batch(1500/7879) done. Loss: 0.7533  lr:0.000010
[ Sat Jul  6 16:11:36 2024 ] 	Batch(1600/7879) done. Loss: 0.0221  lr:0.000010
[ Sat Jul  6 16:11:54 2024 ] 	Batch(1700/7879) done. Loss: 0.1054  lr:0.000010
[ Sat Jul  6 16:12:11 2024 ] 	Batch(1800/7879) done. Loss: 0.1226  lr:0.000010
[ Sat Jul  6 16:12:29 2024 ] 	Batch(1900/7879) done. Loss: 0.0211  lr:0.000010
[ Sat Jul  6 16:12:47 2024 ] 
Training: Epoch [20/120], Step [1999], Loss: 0.11363764107227325, Training Accuracy: 97.74374999999999
[ Sat Jul  6 16:12:47 2024 ] 	Batch(2000/7879) done. Loss: 0.0214  lr:0.000010
[ Sat Jul  6 16:13:05 2024 ] 	Batch(2100/7879) done. Loss: 0.1628  lr:0.000010
[ Sat Jul  6 16:13:23 2024 ] 	Batch(2200/7879) done. Loss: 0.1928  lr:0.000010
[ Sat Jul  6 16:13:41 2024 ] 	Batch(2300/7879) done. Loss: 0.0323  lr:0.000010
[ Sat Jul  6 16:13:59 2024 ] 	Batch(2400/7879) done. Loss: 0.1947  lr:0.000010
[ Sat Jul  6 16:14:17 2024 ] 
Training: Epoch [20/120], Step [2499], Loss: 0.011557038873434067, Training Accuracy: 97.725
[ Sat Jul  6 16:14:17 2024 ] 	Batch(2500/7879) done. Loss: 0.0174  lr:0.000010
[ Sat Jul  6 16:14:35 2024 ] 	Batch(2600/7879) done. Loss: 0.0083  lr:0.000010
[ Sat Jul  6 16:14:53 2024 ] 	Batch(2700/7879) done. Loss: 0.0387  lr:0.000010
[ Sat Jul  6 16:15:11 2024 ] 	Batch(2800/7879) done. Loss: 0.0179  lr:0.000010
[ Sat Jul  6 16:15:29 2024 ] 	Batch(2900/7879) done. Loss: 0.2843  lr:0.000010
[ Sat Jul  6 16:15:47 2024 ] 
Training: Epoch [20/120], Step [2999], Loss: 0.15640807151794434, Training Accuracy: 97.775
[ Sat Jul  6 16:15:47 2024 ] 	Batch(3000/7879) done. Loss: 0.4402  lr:0.000010
[ Sat Jul  6 16:16:05 2024 ] 	Batch(3100/7879) done. Loss: 0.0060  lr:0.000010
[ Sat Jul  6 16:16:23 2024 ] 	Batch(3200/7879) done. Loss: 0.0558  lr:0.000010
[ Sat Jul  6 16:16:41 2024 ] 	Batch(3300/7879) done. Loss: 0.0463  lr:0.000010
[ Sat Jul  6 16:16:59 2024 ] 	Batch(3400/7879) done. Loss: 0.1466  lr:0.000010
[ Sat Jul  6 16:17:17 2024 ] 
Training: Epoch [20/120], Step [3499], Loss: 0.5986362099647522, Training Accuracy: 97.73571428571428
[ Sat Jul  6 16:17:17 2024 ] 	Batch(3500/7879) done. Loss: 0.0876  lr:0.000010
[ Sat Jul  6 16:17:35 2024 ] 	Batch(3600/7879) done. Loss: 0.0059  lr:0.000010
[ Sat Jul  6 16:17:53 2024 ] 	Batch(3700/7879) done. Loss: 0.0158  lr:0.000010
[ Sat Jul  6 16:18:11 2024 ] 	Batch(3800/7879) done. Loss: 0.0806  lr:0.000010
[ Sat Jul  6 16:18:29 2024 ] 	Batch(3900/7879) done. Loss: 0.0605  lr:0.000010
[ Sat Jul  6 16:18:47 2024 ] 
Training: Epoch [20/120], Step [3999], Loss: 0.032253000885248184, Training Accuracy: 97.734375
[ Sat Jul  6 16:18:47 2024 ] 	Batch(4000/7879) done. Loss: 0.0215  lr:0.000010
[ Sat Jul  6 16:19:06 2024 ] 	Batch(4100/7879) done. Loss: 0.0062  lr:0.000010
[ Sat Jul  6 16:19:25 2024 ] 	Batch(4200/7879) done. Loss: 1.0993  lr:0.000010
[ Sat Jul  6 16:19:43 2024 ] 	Batch(4300/7879) done. Loss: 0.2868  lr:0.000010
[ Sat Jul  6 16:20:02 2024 ] 	Batch(4400/7879) done. Loss: 0.0350  lr:0.000010
[ Sat Jul  6 16:20:20 2024 ] 
Training: Epoch [20/120], Step [4499], Loss: 0.09494651108980179, Training Accuracy: 97.69166666666666
[ Sat Jul  6 16:20:21 2024 ] 	Batch(4500/7879) done. Loss: 0.2564  lr:0.000010
[ Sat Jul  6 16:20:39 2024 ] 	Batch(4600/7879) done. Loss: 0.0383  lr:0.000010
[ Sat Jul  6 16:20:58 2024 ] 	Batch(4700/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 16:21:17 2024 ] 	Batch(4800/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 16:21:35 2024 ] 	Batch(4900/7879) done. Loss: 0.0864  lr:0.000010
[ Sat Jul  6 16:21:54 2024 ] 
Training: Epoch [20/120], Step [4999], Loss: 0.05565277114510536, Training Accuracy: 97.695
[ Sat Jul  6 16:21:54 2024 ] 	Batch(5000/7879) done. Loss: 0.1020  lr:0.000010
[ Sat Jul  6 16:22:12 2024 ] 	Batch(5100/7879) done. Loss: 0.0320  lr:0.000010
[ Sat Jul  6 16:22:31 2024 ] 	Batch(5200/7879) done. Loss: 0.2269  lr:0.000010
[ Sat Jul  6 16:22:50 2024 ] 	Batch(5300/7879) done. Loss: 0.0472  lr:0.000010
[ Sat Jul  6 16:23:08 2024 ] 	Batch(5400/7879) done. Loss: 0.0996  lr:0.000010
[ Sat Jul  6 16:23:27 2024 ] 
Training: Epoch [20/120], Step [5499], Loss: 0.023290926590561867, Training Accuracy: 97.73863636363636
[ Sat Jul  6 16:23:27 2024 ] 	Batch(5500/7879) done. Loss: 0.0056  lr:0.000010
[ Sat Jul  6 16:23:46 2024 ] 	Batch(5600/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 16:24:04 2024 ] 	Batch(5700/7879) done. Loss: 0.0044  lr:0.000010
[ Sat Jul  6 16:24:23 2024 ] 	Batch(5800/7879) done. Loss: 0.0090  lr:0.000010
[ Sat Jul  6 16:24:41 2024 ] 	Batch(5900/7879) done. Loss: 0.0118  lr:0.000010
[ Sat Jul  6 16:25:00 2024 ] 
Training: Epoch [20/120], Step [5999], Loss: 0.025596274062991142, Training Accuracy: 97.725
[ Sat Jul  6 16:25:00 2024 ] 	Batch(6000/7879) done. Loss: 0.1405  lr:0.000010
[ Sat Jul  6 16:25:18 2024 ] 	Batch(6100/7879) done. Loss: 0.2183  lr:0.000010
[ Sat Jul  6 16:25:36 2024 ] 	Batch(6200/7879) done. Loss: 0.1179  lr:0.000010
[ Sat Jul  6 16:25:54 2024 ] 	Batch(6300/7879) done. Loss: 0.0369  lr:0.000010
[ Sat Jul  6 16:26:12 2024 ] 	Batch(6400/7879) done. Loss: 0.0212  lr:0.000010
[ Sat Jul  6 16:26:30 2024 ] 
Training: Epoch [20/120], Step [6499], Loss: 0.03149346634745598, Training Accuracy: 97.72115384615384
[ Sat Jul  6 16:26:30 2024 ] 	Batch(6500/7879) done. Loss: 0.0317  lr:0.000010
[ Sat Jul  6 16:26:48 2024 ] 	Batch(6600/7879) done. Loss: 0.0162  lr:0.000010
[ Sat Jul  6 16:27:06 2024 ] 	Batch(6700/7879) done. Loss: 0.1233  lr:0.000010
[ Sat Jul  6 16:27:24 2024 ] 	Batch(6800/7879) done. Loss: 0.2944  lr:0.000010
[ Sat Jul  6 16:27:42 2024 ] 	Batch(6900/7879) done. Loss: 0.0450  lr:0.000010
[ Sat Jul  6 16:28:00 2024 ] 
Training: Epoch [20/120], Step [6999], Loss: 0.511898398399353, Training Accuracy: 97.73571428571428
[ Sat Jul  6 16:28:00 2024 ] 	Batch(7000/7879) done. Loss: 0.0784  lr:0.000010
[ Sat Jul  6 16:28:18 2024 ] 	Batch(7100/7879) done. Loss: 0.0396  lr:0.000010
[ Sat Jul  6 16:28:36 2024 ] 	Batch(7200/7879) done. Loss: 0.0146  lr:0.000010
[ Sat Jul  6 16:28:55 2024 ] 	Batch(7300/7879) done. Loss: 0.0215  lr:0.000010
[ Sat Jul  6 16:29:13 2024 ] 	Batch(7400/7879) done. Loss: 0.3135  lr:0.000010
[ Sat Jul  6 16:29:32 2024 ] 
Training: Epoch [20/120], Step [7499], Loss: 0.006649076472967863, Training Accuracy: 97.75166666666667
[ Sat Jul  6 16:29:32 2024 ] 	Batch(7500/7879) done. Loss: 0.0321  lr:0.000010
[ Sat Jul  6 16:29:50 2024 ] 	Batch(7600/7879) done. Loss: 0.0750  lr:0.000010
[ Sat Jul  6 16:30:09 2024 ] 	Batch(7700/7879) done. Loss: 0.2699  lr:0.000010
[ Sat Jul  6 16:30:28 2024 ] 	Batch(7800/7879) done. Loss: 0.0070  lr:0.000010
[ Sat Jul  6 16:30:42 2024 ] 	Mean training loss: 0.0917.
[ Sat Jul  6 16:30:42 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 16:30:42 2024 ] Training epoch: 22
[ Sat Jul  6 16:30:43 2024 ] 	Batch(0/7879) done. Loss: 0.0101  lr:0.000010
[ Sat Jul  6 16:31:01 2024 ] 	Batch(100/7879) done. Loss: 0.0206  lr:0.000010
[ Sat Jul  6 16:31:20 2024 ] 	Batch(200/7879) done. Loss: 0.0066  lr:0.000010
[ Sat Jul  6 16:31:38 2024 ] 	Batch(300/7879) done. Loss: 0.0595  lr:0.000010
[ Sat Jul  6 16:31:56 2024 ] 	Batch(400/7879) done. Loss: 0.0274  lr:0.000010
[ Sat Jul  6 16:32:14 2024 ] 
Training: Epoch [21/120], Step [499], Loss: 0.04113668203353882, Training Accuracy: 98.15
[ Sat Jul  6 16:32:15 2024 ] 	Batch(500/7879) done. Loss: 0.1474  lr:0.000010
[ Sat Jul  6 16:32:33 2024 ] 	Batch(600/7879) done. Loss: 0.0498  lr:0.000010
[ Sat Jul  6 16:32:51 2024 ] 	Batch(700/7879) done. Loss: 0.1466  lr:0.000010
[ Sat Jul  6 16:33:10 2024 ] 	Batch(800/7879) done. Loss: 0.0071  lr:0.000010
[ Sat Jul  6 16:33:28 2024 ] 	Batch(900/7879) done. Loss: 0.0058  lr:0.000010
[ Sat Jul  6 16:33:46 2024 ] 
Training: Epoch [21/120], Step [999], Loss: 0.027629248797893524, Training Accuracy: 98.175
[ Sat Jul  6 16:33:46 2024 ] 	Batch(1000/7879) done. Loss: 0.0180  lr:0.000010
[ Sat Jul  6 16:34:05 2024 ] 	Batch(1100/7879) done. Loss: 0.0044  lr:0.000010
[ Sat Jul  6 16:34:23 2024 ] 	Batch(1200/7879) done. Loss: 0.0146  lr:0.000010
[ Sat Jul  6 16:34:42 2024 ] 	Batch(1300/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 16:35:00 2024 ] 	Batch(1400/7879) done. Loss: 0.0429  lr:0.000010
[ Sat Jul  6 16:35:18 2024 ] 
Training: Epoch [21/120], Step [1499], Loss: 0.00517974654212594, Training Accuracy: 97.99166666666666
[ Sat Jul  6 16:35:18 2024 ] 	Batch(1500/7879) done. Loss: 0.0285  lr:0.000010
[ Sat Jul  6 16:35:37 2024 ] 	Batch(1600/7879) done. Loss: 0.0141  lr:0.000010
[ Sat Jul  6 16:35:55 2024 ] 	Batch(1700/7879) done. Loss: 0.0694  lr:0.000010
[ Sat Jul  6 16:36:14 2024 ] 	Batch(1800/7879) done. Loss: 0.0071  lr:0.000010
[ Sat Jul  6 16:36:32 2024 ] 	Batch(1900/7879) done. Loss: 0.1422  lr:0.000010
[ Sat Jul  6 16:36:51 2024 ] 
Training: Epoch [21/120], Step [1999], Loss: 0.31466495990753174, Training Accuracy: 97.98750000000001
[ Sat Jul  6 16:36:51 2024 ] 	Batch(2000/7879) done. Loss: 0.1081  lr:0.000010
[ Sat Jul  6 16:37:10 2024 ] 	Batch(2100/7879) done. Loss: 0.0680  lr:0.000010
[ Sat Jul  6 16:37:28 2024 ] 	Batch(2200/7879) done. Loss: 0.0368  lr:0.000010
[ Sat Jul  6 16:37:47 2024 ] 	Batch(2300/7879) done. Loss: 0.0721  lr:0.000010
[ Sat Jul  6 16:38:06 2024 ] 	Batch(2400/7879) done. Loss: 0.3511  lr:0.000010
[ Sat Jul  6 16:38:24 2024 ] 
Training: Epoch [21/120], Step [2499], Loss: 0.3249756693840027, Training Accuracy: 97.92
[ Sat Jul  6 16:38:24 2024 ] 	Batch(2500/7879) done. Loss: 0.1124  lr:0.000010
[ Sat Jul  6 16:38:43 2024 ] 	Batch(2600/7879) done. Loss: 0.1736  lr:0.000010
[ Sat Jul  6 16:39:01 2024 ] 	Batch(2700/7879) done. Loss: 0.0749  lr:0.000010
[ Sat Jul  6 16:39:20 2024 ] 	Batch(2800/7879) done. Loss: 0.0431  lr:0.000010
[ Sat Jul  6 16:39:39 2024 ] 	Batch(2900/7879) done. Loss: 0.1016  lr:0.000010
[ Sat Jul  6 16:39:57 2024 ] 
Training: Epoch [21/120], Step [2999], Loss: 0.36720266938209534, Training Accuracy: 97.89166666666667
[ Sat Jul  6 16:39:57 2024 ] 	Batch(3000/7879) done. Loss: 0.0182  lr:0.000010
[ Sat Jul  6 16:40:15 2024 ] 	Batch(3100/7879) done. Loss: 0.2405  lr:0.000010
[ Sat Jul  6 16:40:33 2024 ] 	Batch(3200/7879) done. Loss: 0.0290  lr:0.000010
[ Sat Jul  6 16:40:51 2024 ] 	Batch(3300/7879) done. Loss: 0.0282  lr:0.000010
[ Sat Jul  6 16:41:09 2024 ] 	Batch(3400/7879) done. Loss: 0.0098  lr:0.000010
[ Sat Jul  6 16:41:26 2024 ] 
Training: Epoch [21/120], Step [3499], Loss: 0.42086708545684814, Training Accuracy: 97.91071428571428
[ Sat Jul  6 16:41:27 2024 ] 	Batch(3500/7879) done. Loss: 0.3797  lr:0.000010
[ Sat Jul  6 16:41:45 2024 ] 	Batch(3600/7879) done. Loss: 0.7949  lr:0.000010
[ Sat Jul  6 16:42:03 2024 ] 	Batch(3700/7879) done. Loss: 0.0184  lr:0.000010
[ Sat Jul  6 16:42:20 2024 ] 	Batch(3800/7879) done. Loss: 0.0954  lr:0.000010
[ Sat Jul  6 16:42:38 2024 ] 	Batch(3900/7879) done. Loss: 0.0155  lr:0.000010
[ Sat Jul  6 16:42:56 2024 ] 
Training: Epoch [21/120], Step [3999], Loss: 0.07756450772285461, Training Accuracy: 97.86875
[ Sat Jul  6 16:42:56 2024 ] 	Batch(4000/7879) done. Loss: 0.0976  lr:0.000010
[ Sat Jul  6 16:43:15 2024 ] 	Batch(4100/7879) done. Loss: 0.1408  lr:0.000010
[ Sat Jul  6 16:43:33 2024 ] 	Batch(4200/7879) done. Loss: 0.0120  lr:0.000010
[ Sat Jul  6 16:43:51 2024 ] 	Batch(4300/7879) done. Loss: 0.3556  lr:0.000010
[ Sat Jul  6 16:44:09 2024 ] 	Batch(4400/7879) done. Loss: 0.0161  lr:0.000010
[ Sat Jul  6 16:44:27 2024 ] 
Training: Epoch [21/120], Step [4499], Loss: 0.16550949215888977, Training Accuracy: 97.85000000000001
[ Sat Jul  6 16:44:27 2024 ] 	Batch(4500/7879) done. Loss: 0.0099  lr:0.000010
[ Sat Jul  6 16:44:45 2024 ] 	Batch(4600/7879) done. Loss: 0.2764  lr:0.000010
[ Sat Jul  6 16:45:03 2024 ] 	Batch(4700/7879) done. Loss: 0.2401  lr:0.000010
[ Sat Jul  6 16:45:21 2024 ] 	Batch(4800/7879) done. Loss: 0.1872  lr:0.000010
[ Sat Jul  6 16:45:39 2024 ] 	Batch(4900/7879) done. Loss: 0.1118  lr:0.000010
[ Sat Jul  6 16:45:57 2024 ] 
Training: Epoch [21/120], Step [4999], Loss: 0.03160802647471428, Training Accuracy: 97.83
[ Sat Jul  6 16:45:57 2024 ] 	Batch(5000/7879) done. Loss: 0.1566  lr:0.000010
[ Sat Jul  6 16:46:15 2024 ] 	Batch(5100/7879) done. Loss: 0.1346  lr:0.000010
[ Sat Jul  6 16:46:33 2024 ] 	Batch(5200/7879) done. Loss: 0.0282  lr:0.000010
[ Sat Jul  6 16:46:52 2024 ] 	Batch(5300/7879) done. Loss: 0.0109  lr:0.000010
[ Sat Jul  6 16:47:10 2024 ] 	Batch(5400/7879) done. Loss: 0.0695  lr:0.000010
[ Sat Jul  6 16:47:29 2024 ] 
Training: Epoch [21/120], Step [5499], Loss: 0.024963654577732086, Training Accuracy: 97.84545454545454
[ Sat Jul  6 16:47:29 2024 ] 	Batch(5500/7879) done. Loss: 0.0469  lr:0.000010
[ Sat Jul  6 16:47:48 2024 ] 	Batch(5600/7879) done. Loss: 0.2543  lr:0.000010
[ Sat Jul  6 16:48:06 2024 ] 	Batch(5700/7879) done. Loss: 0.0972  lr:0.000010
[ Sat Jul  6 16:48:25 2024 ] 	Batch(5800/7879) done. Loss: 0.0026  lr:0.000010
[ Sat Jul  6 16:48:43 2024 ] 	Batch(5900/7879) done. Loss: 0.1587  lr:0.000010
[ Sat Jul  6 16:49:02 2024 ] 
Training: Epoch [21/120], Step [5999], Loss: 0.0017231064848601818, Training Accuracy: 97.82291666666667
[ Sat Jul  6 16:49:02 2024 ] 	Batch(6000/7879) done. Loss: 0.2057  lr:0.000010
[ Sat Jul  6 16:49:21 2024 ] 	Batch(6100/7879) done. Loss: 0.0412  lr:0.000010
[ Sat Jul  6 16:49:39 2024 ] 	Batch(6200/7879) done. Loss: 0.0275  lr:0.000010
[ Sat Jul  6 16:49:58 2024 ] 	Batch(6300/7879) done. Loss: 0.0120  lr:0.000010
[ Sat Jul  6 16:50:16 2024 ] 	Batch(6400/7879) done. Loss: 0.0935  lr:0.000010
[ Sat Jul  6 16:50:33 2024 ] 
Training: Epoch [21/120], Step [6499], Loss: 0.008726675063371658, Training Accuracy: 97.82115384615383
[ Sat Jul  6 16:50:34 2024 ] 	Batch(6500/7879) done. Loss: 0.0198  lr:0.000010
[ Sat Jul  6 16:50:52 2024 ] 	Batch(6600/7879) done. Loss: 0.0044  lr:0.000010
[ Sat Jul  6 16:51:09 2024 ] 	Batch(6700/7879) done. Loss: 0.2544  lr:0.000010
[ Sat Jul  6 16:51:28 2024 ] 	Batch(6800/7879) done. Loss: 0.0417  lr:0.000010
[ Sat Jul  6 16:51:46 2024 ] 	Batch(6900/7879) done. Loss: 0.0647  lr:0.000010
[ Sat Jul  6 16:52:04 2024 ] 
Training: Epoch [21/120], Step [6999], Loss: 0.08197171986103058, Training Accuracy: 97.80892857142857
[ Sat Jul  6 16:52:04 2024 ] 	Batch(7000/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 16:52:22 2024 ] 	Batch(7100/7879) done. Loss: 0.0565  lr:0.000010
[ Sat Jul  6 16:52:40 2024 ] 	Batch(7200/7879) done. Loss: 0.0053  lr:0.000010
[ Sat Jul  6 16:52:58 2024 ] 	Batch(7300/7879) done. Loss: 0.1530  lr:0.000010
[ Sat Jul  6 16:53:16 2024 ] 	Batch(7400/7879) done. Loss: 0.1449  lr:0.000010
[ Sat Jul  6 16:53:33 2024 ] 
Training: Epoch [21/120], Step [7499], Loss: 0.04752678796648979, Training Accuracy: 97.785
[ Sat Jul  6 16:53:34 2024 ] 	Batch(7500/7879) done. Loss: 0.0019  lr:0.000010
[ Sat Jul  6 16:53:52 2024 ] 	Batch(7600/7879) done. Loss: 0.0144  lr:0.000010
[ Sat Jul  6 16:54:10 2024 ] 	Batch(7700/7879) done. Loss: 0.0652  lr:0.000010
[ Sat Jul  6 16:54:29 2024 ] 	Batch(7800/7879) done. Loss: 0.0259  lr:0.000010
[ Sat Jul  6 16:54:43 2024 ] 	Mean training loss: 0.0927.
[ Sat Jul  6 16:54:43 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 16:54:43 2024 ] Training epoch: 23
[ Sat Jul  6 16:54:44 2024 ] 	Batch(0/7879) done. Loss: 0.2726  lr:0.000010
[ Sat Jul  6 16:55:02 2024 ] 	Batch(100/7879) done. Loss: 0.0207  lr:0.000010
[ Sat Jul  6 16:55:20 2024 ] 	Batch(200/7879) done. Loss: 0.0377  lr:0.000010
[ Sat Jul  6 16:55:38 2024 ] 	Batch(300/7879) done. Loss: 0.0184  lr:0.000010
[ Sat Jul  6 16:55:56 2024 ] 	Batch(400/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 16:56:14 2024 ] 
Training: Epoch [22/120], Step [499], Loss: 0.0822797417640686, Training Accuracy: 97.95
[ Sat Jul  6 16:56:14 2024 ] 	Batch(500/7879) done. Loss: 0.0067  lr:0.000010
[ Sat Jul  6 16:56:32 2024 ] 	Batch(600/7879) done. Loss: 0.0229  lr:0.000010
[ Sat Jul  6 16:56:50 2024 ] 	Batch(700/7879) done. Loss: 0.1408  lr:0.000010
[ Sat Jul  6 16:57:08 2024 ] 	Batch(800/7879) done. Loss: 0.0390  lr:0.000010
[ Sat Jul  6 16:57:26 2024 ] 	Batch(900/7879) done. Loss: 0.1932  lr:0.000010
[ Sat Jul  6 16:57:45 2024 ] 
Training: Epoch [22/120], Step [999], Loss: 0.04429527372121811, Training Accuracy: 97.8875
[ Sat Jul  6 16:57:45 2024 ] 	Batch(1000/7879) done. Loss: 0.0035  lr:0.000010
[ Sat Jul  6 16:58:04 2024 ] 	Batch(1100/7879) done. Loss: 0.1057  lr:0.000010
[ Sat Jul  6 16:58:22 2024 ] 	Batch(1200/7879) done. Loss: 0.0479  lr:0.000010
[ Sat Jul  6 16:58:40 2024 ] 	Batch(1300/7879) done. Loss: 0.0095  lr:0.000010
[ Sat Jul  6 16:58:58 2024 ] 	Batch(1400/7879) done. Loss: 0.1337  lr:0.000010
[ Sat Jul  6 16:59:16 2024 ] 
Training: Epoch [22/120], Step [1499], Loss: 0.3717672526836395, Training Accuracy: 97.90833333333333
[ Sat Jul  6 16:59:16 2024 ] 	Batch(1500/7879) done. Loss: 0.0410  lr:0.000010
[ Sat Jul  6 16:59:34 2024 ] 	Batch(1600/7879) done. Loss: 0.0662  lr:0.000010
[ Sat Jul  6 16:59:52 2024 ] 	Batch(1700/7879) done. Loss: 0.2413  lr:0.000010
[ Sat Jul  6 17:00:10 2024 ] 	Batch(1800/7879) done. Loss: 0.0014  lr:0.000010
[ Sat Jul  6 17:00:28 2024 ] 	Batch(1900/7879) done. Loss: 0.2556  lr:0.000010
[ Sat Jul  6 17:00:46 2024 ] 
Training: Epoch [22/120], Step [1999], Loss: 0.1332196444272995, Training Accuracy: 97.81875
[ Sat Jul  6 17:00:46 2024 ] 	Batch(2000/7879) done. Loss: 0.1034  lr:0.000010
[ Sat Jul  6 17:01:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0076  lr:0.000010
[ Sat Jul  6 17:01:22 2024 ] 	Batch(2200/7879) done. Loss: 0.1475  lr:0.000010
[ Sat Jul  6 17:01:40 2024 ] 	Batch(2300/7879) done. Loss: 0.0728  lr:0.000010
[ Sat Jul  6 17:01:58 2024 ] 	Batch(2400/7879) done. Loss: 0.1526  lr:0.000010
[ Sat Jul  6 17:02:17 2024 ] 
Training: Epoch [22/120], Step [2499], Loss: 0.011762396432459354, Training Accuracy: 97.775
[ Sat Jul  6 17:02:17 2024 ] 	Batch(2500/7879) done. Loss: 0.0550  lr:0.000010
[ Sat Jul  6 17:02:35 2024 ] 	Batch(2600/7879) done. Loss: 0.2032  lr:0.000010
[ Sat Jul  6 17:02:53 2024 ] 	Batch(2700/7879) done. Loss: 0.0165  lr:0.000010
[ Sat Jul  6 17:03:11 2024 ] 	Batch(2800/7879) done. Loss: 0.0053  lr:0.000010
[ Sat Jul  6 17:03:29 2024 ] 	Batch(2900/7879) done. Loss: 0.1881  lr:0.000010
[ Sat Jul  6 17:03:47 2024 ] 
Training: Epoch [22/120], Step [2999], Loss: 0.11082639545202255, Training Accuracy: 97.78750000000001
[ Sat Jul  6 17:03:47 2024 ] 	Batch(3000/7879) done. Loss: 0.0036  lr:0.000010
[ Sat Jul  6 17:04:05 2024 ] 	Batch(3100/7879) done. Loss: 0.0559  lr:0.000010
[ Sat Jul  6 17:04:23 2024 ] 	Batch(3200/7879) done. Loss: 0.0242  lr:0.000010
[ Sat Jul  6 17:04:41 2024 ] 	Batch(3300/7879) done. Loss: 0.1840  lr:0.000010
[ Sat Jul  6 17:04:59 2024 ] 	Batch(3400/7879) done. Loss: 0.0551  lr:0.000010
[ Sat Jul  6 17:05:17 2024 ] 
Training: Epoch [22/120], Step [3499], Loss: 0.005111253820359707, Training Accuracy: 97.76785714285714
[ Sat Jul  6 17:05:17 2024 ] 	Batch(3500/7879) done. Loss: 0.0188  lr:0.000010
[ Sat Jul  6 17:05:35 2024 ] 	Batch(3600/7879) done. Loss: 0.2460  lr:0.000010
[ Sat Jul  6 17:05:53 2024 ] 	Batch(3700/7879) done. Loss: 0.0108  lr:0.000010
[ Sat Jul  6 17:06:11 2024 ] 	Batch(3800/7879) done. Loss: 0.0540  lr:0.000010
[ Sat Jul  6 17:06:29 2024 ] 	Batch(3900/7879) done. Loss: 0.0719  lr:0.000010
[ Sat Jul  6 17:06:46 2024 ] 
Training: Epoch [22/120], Step [3999], Loss: 0.010447612963616848, Training Accuracy: 97.725
[ Sat Jul  6 17:06:47 2024 ] 	Batch(4000/7879) done. Loss: 0.0403  lr:0.000010
[ Sat Jul  6 17:07:05 2024 ] 	Batch(4100/7879) done. Loss: 0.2314  lr:0.000010
[ Sat Jul  6 17:07:24 2024 ] 	Batch(4200/7879) done. Loss: 0.3837  lr:0.000010
[ Sat Jul  6 17:07:42 2024 ] 	Batch(4300/7879) done. Loss: 0.0073  lr:0.000010
[ Sat Jul  6 17:08:01 2024 ] 	Batch(4400/7879) done. Loss: 0.0400  lr:0.000010
[ Sat Jul  6 17:08:20 2024 ] 
Training: Epoch [22/120], Step [4499], Loss: 0.006142285652458668, Training Accuracy: 97.74444444444444
[ Sat Jul  6 17:08:20 2024 ] 	Batch(4500/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 17:08:38 2024 ] 	Batch(4600/7879) done. Loss: 0.0592  lr:0.000010
[ Sat Jul  6 17:08:57 2024 ] 	Batch(4700/7879) done. Loss: 0.0594  lr:0.000010
[ Sat Jul  6 17:09:16 2024 ] 	Batch(4800/7879) done. Loss: 0.0059  lr:0.000010
[ Sat Jul  6 17:09:34 2024 ] 	Batch(4900/7879) done. Loss: 0.0463  lr:0.000010
[ Sat Jul  6 17:09:53 2024 ] 
Training: Epoch [22/120], Step [4999], Loss: 0.028411410748958588, Training Accuracy: 97.75500000000001
[ Sat Jul  6 17:09:53 2024 ] 	Batch(5000/7879) done. Loss: 0.2071  lr:0.000010
[ Sat Jul  6 17:10:11 2024 ] 	Batch(5100/7879) done. Loss: 0.0242  lr:0.000010
[ Sat Jul  6 17:10:30 2024 ] 	Batch(5200/7879) done. Loss: 0.0018  lr:0.000010
[ Sat Jul  6 17:10:49 2024 ] 	Batch(5300/7879) done. Loss: 0.1775  lr:0.000010
[ Sat Jul  6 17:11:07 2024 ] 	Batch(5400/7879) done. Loss: 0.1680  lr:0.000010
[ Sat Jul  6 17:11:26 2024 ] 
Training: Epoch [22/120], Step [5499], Loss: 0.018195083364844322, Training Accuracy: 97.76363636363637
[ Sat Jul  6 17:11:26 2024 ] 	Batch(5500/7879) done. Loss: 0.0265  lr:0.000010
[ Sat Jul  6 17:11:44 2024 ] 	Batch(5600/7879) done. Loss: 0.2249  lr:0.000010
[ Sat Jul  6 17:12:02 2024 ] 	Batch(5700/7879) done. Loss: 0.0123  lr:0.000010
[ Sat Jul  6 17:12:20 2024 ] 	Batch(5800/7879) done. Loss: 0.0394  lr:0.000010
[ Sat Jul  6 17:12:38 2024 ] 	Batch(5900/7879) done. Loss: 0.0240  lr:0.000010
[ Sat Jul  6 17:12:56 2024 ] 
Training: Epoch [22/120], Step [5999], Loss: 0.24926668405532837, Training Accuracy: 97.78541666666666
[ Sat Jul  6 17:12:56 2024 ] 	Batch(6000/7879) done. Loss: 0.0227  lr:0.000010
[ Sat Jul  6 17:13:14 2024 ] 	Batch(6100/7879) done. Loss: 0.0890  lr:0.000010
[ Sat Jul  6 17:13:32 2024 ] 	Batch(6200/7879) done. Loss: 0.0228  lr:0.000010
[ Sat Jul  6 17:13:50 2024 ] 	Batch(6300/7879) done. Loss: 0.0141  lr:0.000010
[ Sat Jul  6 17:14:08 2024 ] 	Batch(6400/7879) done. Loss: 0.2029  lr:0.000010
[ Sat Jul  6 17:14:26 2024 ] 
Training: Epoch [22/120], Step [6499], Loss: 0.23900943994522095, Training Accuracy: 97.76346153846154
[ Sat Jul  6 17:14:26 2024 ] 	Batch(6500/7879) done. Loss: 0.0716  lr:0.000010
[ Sat Jul  6 17:14:44 2024 ] 	Batch(6600/7879) done. Loss: 0.0039  lr:0.000010
[ Sat Jul  6 17:15:02 2024 ] 	Batch(6700/7879) done. Loss: 0.2867  lr:0.000010
[ Sat Jul  6 17:15:20 2024 ] 	Batch(6800/7879) done. Loss: 0.0173  lr:0.000010
[ Sat Jul  6 17:15:38 2024 ] 	Batch(6900/7879) done. Loss: 0.0129  lr:0.000010
[ Sat Jul  6 17:15:55 2024 ] 
Training: Epoch [22/120], Step [6999], Loss: 0.5517600178718567, Training Accuracy: 97.7625
[ Sat Jul  6 17:15:56 2024 ] 	Batch(7000/7879) done. Loss: 0.1571  lr:0.000010
[ Sat Jul  6 17:16:13 2024 ] 	Batch(7100/7879) done. Loss: 0.0244  lr:0.000010
[ Sat Jul  6 17:16:32 2024 ] 	Batch(7200/7879) done. Loss: 0.0133  lr:0.000010
[ Sat Jul  6 17:16:50 2024 ] 	Batch(7300/7879) done. Loss: 0.1078  lr:0.000010
[ Sat Jul  6 17:17:09 2024 ] 	Batch(7400/7879) done. Loss: 0.0636  lr:0.000010
[ Sat Jul  6 17:17:27 2024 ] 
Training: Epoch [22/120], Step [7499], Loss: 0.07411853224039078, Training Accuracy: 97.76666666666667
[ Sat Jul  6 17:17:27 2024 ] 	Batch(7500/7879) done. Loss: 0.0070  lr:0.000010
[ Sat Jul  6 17:17:46 2024 ] 	Batch(7600/7879) done. Loss: 0.3381  lr:0.000010
[ Sat Jul  6 17:18:04 2024 ] 	Batch(7700/7879) done. Loss: 0.1195  lr:0.000010
[ Sat Jul  6 17:18:22 2024 ] 	Batch(7800/7879) done. Loss: 0.0661  lr:0.000010
[ Sat Jul  6 17:18:36 2024 ] 	Mean training loss: 0.0929.
[ Sat Jul  6 17:18:36 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 17:18:36 2024 ] Training epoch: 24
[ Sat Jul  6 17:18:37 2024 ] 	Batch(0/7879) done. Loss: 0.0741  lr:0.000010
[ Sat Jul  6 17:18:55 2024 ] 	Batch(100/7879) done. Loss: 0.0544  lr:0.000010
[ Sat Jul  6 17:19:13 2024 ] 	Batch(200/7879) done. Loss: 0.0435  lr:0.000010
[ Sat Jul  6 17:19:31 2024 ] 	Batch(300/7879) done. Loss: 0.0966  lr:0.000010
[ Sat Jul  6 17:19:49 2024 ] 	Batch(400/7879) done. Loss: 0.0552  lr:0.000010
[ Sat Jul  6 17:20:07 2024 ] 
Training: Epoch [23/120], Step [499], Loss: 0.0644506886601448, Training Accuracy: 97.8
[ Sat Jul  6 17:20:07 2024 ] 	Batch(500/7879) done. Loss: 0.0537  lr:0.000010
[ Sat Jul  6 17:20:25 2024 ] 	Batch(600/7879) done. Loss: 0.0159  lr:0.000010
[ Sat Jul  6 17:20:42 2024 ] 	Batch(700/7879) done. Loss: 0.0150  lr:0.000010
[ Sat Jul  6 17:21:00 2024 ] 	Batch(800/7879) done. Loss: 0.3873  lr:0.000010
[ Sat Jul  6 17:21:19 2024 ] 	Batch(900/7879) done. Loss: 0.2001  lr:0.000010
[ Sat Jul  6 17:21:37 2024 ] 
Training: Epoch [23/120], Step [999], Loss: 0.1845727115869522, Training Accuracy: 97.6
[ Sat Jul  6 17:21:37 2024 ] 	Batch(1000/7879) done. Loss: 0.1163  lr:0.000010
[ Sat Jul  6 17:21:55 2024 ] 	Batch(1100/7879) done. Loss: 0.1737  lr:0.000010
[ Sat Jul  6 17:22:13 2024 ] 	Batch(1200/7879) done. Loss: 0.0669  lr:0.000010
[ Sat Jul  6 17:22:32 2024 ] 	Batch(1300/7879) done. Loss: 0.1274  lr:0.000010
[ Sat Jul  6 17:22:50 2024 ] 	Batch(1400/7879) done. Loss: 0.4313  lr:0.000010
[ Sat Jul  6 17:23:09 2024 ] 
Training: Epoch [23/120], Step [1499], Loss: 0.00588826322928071, Training Accuracy: 97.6
[ Sat Jul  6 17:23:09 2024 ] 	Batch(1500/7879) done. Loss: 0.0417  lr:0.000010
[ Sat Jul  6 17:23:28 2024 ] 	Batch(1600/7879) done. Loss: 0.0022  lr:0.000010
[ Sat Jul  6 17:23:46 2024 ] 	Batch(1700/7879) done. Loss: 0.0206  lr:0.000010
[ Sat Jul  6 17:24:05 2024 ] 	Batch(1800/7879) done. Loss: 0.0588  lr:0.000010
[ Sat Jul  6 17:24:24 2024 ] 	Batch(1900/7879) done. Loss: 0.0693  lr:0.000010
[ Sat Jul  6 17:24:42 2024 ] 
Training: Epoch [23/120], Step [1999], Loss: 0.005683363880962133, Training Accuracy: 97.69375
[ Sat Jul  6 17:24:42 2024 ] 	Batch(2000/7879) done. Loss: 0.0730  lr:0.000010
[ Sat Jul  6 17:25:00 2024 ] 	Batch(2100/7879) done. Loss: 0.0240  lr:0.000010
[ Sat Jul  6 17:25:18 2024 ] 	Batch(2200/7879) done. Loss: 0.0098  lr:0.000010
[ Sat Jul  6 17:25:36 2024 ] 	Batch(2300/7879) done. Loss: 0.3727  lr:0.000010
[ Sat Jul  6 17:25:54 2024 ] 	Batch(2400/7879) done. Loss: 0.0096  lr:0.000010
[ Sat Jul  6 17:26:12 2024 ] 
Training: Epoch [23/120], Step [2499], Loss: 0.018929868936538696, Training Accuracy: 97.67
[ Sat Jul  6 17:26:12 2024 ] 	Batch(2500/7879) done. Loss: 0.0058  lr:0.000010
[ Sat Jul  6 17:26:30 2024 ] 	Batch(2600/7879) done. Loss: 0.0035  lr:0.000010
[ Sat Jul  6 17:26:48 2024 ] 	Batch(2700/7879) done. Loss: 0.3309  lr:0.000010
[ Sat Jul  6 17:27:06 2024 ] 	Batch(2800/7879) done. Loss: 0.0148  lr:0.000010
[ Sat Jul  6 17:27:24 2024 ] 	Batch(2900/7879) done. Loss: 0.0484  lr:0.000010
[ Sat Jul  6 17:27:42 2024 ] 
Training: Epoch [23/120], Step [2999], Loss: 0.04315750673413277, Training Accuracy: 97.6125
[ Sat Jul  6 17:27:42 2024 ] 	Batch(3000/7879) done. Loss: 0.0485  lr:0.000010
[ Sat Jul  6 17:28:00 2024 ] 	Batch(3100/7879) done. Loss: 0.1097  lr:0.000010
[ Sat Jul  6 17:28:18 2024 ] 	Batch(3200/7879) done. Loss: 0.0165  lr:0.000010
[ Sat Jul  6 17:28:36 2024 ] 	Batch(3300/7879) done. Loss: 0.0450  lr:0.000010
[ Sat Jul  6 17:28:55 2024 ] 	Batch(3400/7879) done. Loss: 0.0566  lr:0.000010
[ Sat Jul  6 17:29:13 2024 ] 
Training: Epoch [23/120], Step [3499], Loss: 0.02191251702606678, Training Accuracy: 97.63214285714285
[ Sat Jul  6 17:29:13 2024 ] 	Batch(3500/7879) done. Loss: 0.0401  lr:0.000010
[ Sat Jul  6 17:29:32 2024 ] 	Batch(3600/7879) done. Loss: 0.0025  lr:0.000010
[ Sat Jul  6 17:29:50 2024 ] 	Batch(3700/7879) done. Loss: 0.0805  lr:0.000010
[ Sat Jul  6 17:30:08 2024 ] 	Batch(3800/7879) done. Loss: 0.0236  lr:0.000010
[ Sat Jul  6 17:30:25 2024 ] 	Batch(3900/7879) done. Loss: 0.0547  lr:0.000010
[ Sat Jul  6 17:30:43 2024 ] 
Training: Epoch [23/120], Step [3999], Loss: 0.056336309760808945, Training Accuracy: 97.675
[ Sat Jul  6 17:30:43 2024 ] 	Batch(4000/7879) done. Loss: 0.0293  lr:0.000010
[ Sat Jul  6 17:31:01 2024 ] 	Batch(4100/7879) done. Loss: 0.0166  lr:0.000010
[ Sat Jul  6 17:31:19 2024 ] 	Batch(4200/7879) done. Loss: 0.0834  lr:0.000010
[ Sat Jul  6 17:31:37 2024 ] 	Batch(4300/7879) done. Loss: 0.0623  lr:0.000010
[ Sat Jul  6 17:31:55 2024 ] 	Batch(4400/7879) done. Loss: 0.0202  lr:0.000010
[ Sat Jul  6 17:32:14 2024 ] 
Training: Epoch [23/120], Step [4499], Loss: 0.024883264675736427, Training Accuracy: 97.69444444444444
[ Sat Jul  6 17:32:14 2024 ] 	Batch(4500/7879) done. Loss: 0.0149  lr:0.000010
[ Sat Jul  6 17:32:32 2024 ] 	Batch(4600/7879) done. Loss: 0.2335  lr:0.000010
[ Sat Jul  6 17:32:50 2024 ] 	Batch(4700/7879) done. Loss: 0.1465  lr:0.000010
[ Sat Jul  6 17:33:08 2024 ] 	Batch(4800/7879) done. Loss: 0.0245  lr:0.000010
[ Sat Jul  6 17:33:26 2024 ] 	Batch(4900/7879) done. Loss: 0.0183  lr:0.000010
[ Sat Jul  6 17:33:44 2024 ] 
Training: Epoch [23/120], Step [4999], Loss: 0.04092255234718323, Training Accuracy: 97.69250000000001
[ Sat Jul  6 17:33:44 2024 ] 	Batch(5000/7879) done. Loss: 0.0027  lr:0.000010
[ Sat Jul  6 17:34:02 2024 ] 	Batch(5100/7879) done. Loss: 0.1683  lr:0.000010
[ Sat Jul  6 17:34:20 2024 ] 	Batch(5200/7879) done. Loss: 0.0622  lr:0.000010
[ Sat Jul  6 17:34:38 2024 ] 	Batch(5300/7879) done. Loss: 0.0200  lr:0.000010
[ Sat Jul  6 17:34:56 2024 ] 	Batch(5400/7879) done. Loss: 0.0243  lr:0.000010
[ Sat Jul  6 17:35:14 2024 ] 
Training: Epoch [23/120], Step [5499], Loss: 0.011103128083050251, Training Accuracy: 97.69545454545454
[ Sat Jul  6 17:35:15 2024 ] 	Batch(5500/7879) done. Loss: 0.0624  lr:0.000010
[ Sat Jul  6 17:35:33 2024 ] 	Batch(5600/7879) done. Loss: 0.0030  lr:0.000010
[ Sat Jul  6 17:35:51 2024 ] 	Batch(5700/7879) done. Loss: 0.0189  lr:0.000010
[ Sat Jul  6 17:36:09 2024 ] 	Batch(5800/7879) done. Loss: 0.1431  lr:0.000010
[ Sat Jul  6 17:36:27 2024 ] 	Batch(5900/7879) done. Loss: 0.2662  lr:0.000010
[ Sat Jul  6 17:36:45 2024 ] 
Training: Epoch [23/120], Step [5999], Loss: 0.10222657769918442, Training Accuracy: 97.72291666666668
[ Sat Jul  6 17:36:45 2024 ] 	Batch(6000/7879) done. Loss: 0.0417  lr:0.000010
[ Sat Jul  6 17:37:04 2024 ] 	Batch(6100/7879) done. Loss: 0.0455  lr:0.000010
[ Sat Jul  6 17:37:23 2024 ] 	Batch(6200/7879) done. Loss: 0.1083  lr:0.000010
[ Sat Jul  6 17:37:41 2024 ] 	Batch(6300/7879) done. Loss: 0.0540  lr:0.000010
[ Sat Jul  6 17:37:59 2024 ] 	Batch(6400/7879) done. Loss: 0.0087  lr:0.000010
[ Sat Jul  6 17:38:18 2024 ] 
Training: Epoch [23/120], Step [6499], Loss: 0.5123968720436096, Training Accuracy: 97.71346153846154
[ Sat Jul  6 17:38:18 2024 ] 	Batch(6500/7879) done. Loss: 0.5468  lr:0.000010
[ Sat Jul  6 17:38:36 2024 ] 	Batch(6600/7879) done. Loss: 0.0241  lr:0.000010
[ Sat Jul  6 17:38:55 2024 ] 	Batch(6700/7879) done. Loss: 0.0672  lr:0.000010
[ Sat Jul  6 17:39:14 2024 ] 	Batch(6800/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 17:39:32 2024 ] 	Batch(6900/7879) done. Loss: 0.0724  lr:0.000010
[ Sat Jul  6 17:39:49 2024 ] 
Training: Epoch [23/120], Step [6999], Loss: 0.02805381268262863, Training Accuracy: 97.72142857142858
[ Sat Jul  6 17:39:50 2024 ] 	Batch(7000/7879) done. Loss: 0.0908  lr:0.000010
[ Sat Jul  6 17:40:07 2024 ] 	Batch(7100/7879) done. Loss: 0.0353  lr:0.000010
[ Sat Jul  6 17:40:26 2024 ] 	Batch(7200/7879) done. Loss: 0.0075  lr:0.000010
[ Sat Jul  6 17:40:43 2024 ] 	Batch(7300/7879) done. Loss: 0.0052  lr:0.000010
[ Sat Jul  6 17:41:01 2024 ] 	Batch(7400/7879) done. Loss: 0.0092  lr:0.000010
[ Sat Jul  6 17:41:19 2024 ] 
Training: Epoch [23/120], Step [7499], Loss: 0.02475919760763645, Training Accuracy: 97.72833333333332
[ Sat Jul  6 17:41:19 2024 ] 	Batch(7500/7879) done. Loss: 0.0804  lr:0.000010
[ Sat Jul  6 17:41:37 2024 ] 	Batch(7600/7879) done. Loss: 0.0056  lr:0.000010
[ Sat Jul  6 17:41:56 2024 ] 	Batch(7700/7879) done. Loss: 0.0451  lr:0.000010
[ Sat Jul  6 17:42:15 2024 ] 	Batch(7800/7879) done. Loss: 0.0114  lr:0.000010
[ Sat Jul  6 17:42:29 2024 ] 	Mean training loss: 0.0915.
[ Sat Jul  6 17:42:29 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 17:42:29 2024 ] Training epoch: 25
[ Sat Jul  6 17:42:30 2024 ] 	Batch(0/7879) done. Loss: 0.0397  lr:0.000010
[ Sat Jul  6 17:42:48 2024 ] 	Batch(100/7879) done. Loss: 0.4591  lr:0.000010
[ Sat Jul  6 17:43:06 2024 ] 	Batch(200/7879) done. Loss: 0.2868  lr:0.000010
[ Sat Jul  6 17:43:24 2024 ] 	Batch(300/7879) done. Loss: 0.0351  lr:0.000010
[ Sat Jul  6 17:43:42 2024 ] 	Batch(400/7879) done. Loss: 0.0308  lr:0.000010
[ Sat Jul  6 17:43:59 2024 ] 
Training: Epoch [24/120], Step [499], Loss: 0.020751401782035828, Training Accuracy: 97.8
[ Sat Jul  6 17:44:00 2024 ] 	Batch(500/7879) done. Loss: 0.0326  lr:0.000010
[ Sat Jul  6 17:44:17 2024 ] 	Batch(600/7879) done. Loss: 0.0045  lr:0.000010
[ Sat Jul  6 17:44:35 2024 ] 	Batch(700/7879) done. Loss: 0.0307  lr:0.000010
[ Sat Jul  6 17:44:54 2024 ] 	Batch(800/7879) done. Loss: 0.0401  lr:0.000010
[ Sat Jul  6 17:45:12 2024 ] 	Batch(900/7879) done. Loss: 0.0814  lr:0.000010
[ Sat Jul  6 17:45:29 2024 ] 
Training: Epoch [24/120], Step [999], Loss: 0.06038151681423187, Training Accuracy: 97.82499999999999
[ Sat Jul  6 17:45:29 2024 ] 	Batch(1000/7879) done. Loss: 0.1079  lr:0.000010
[ Sat Jul  6 17:45:47 2024 ] 	Batch(1100/7879) done. Loss: 0.1452  lr:0.000010
[ Sat Jul  6 17:46:06 2024 ] 	Batch(1200/7879) done. Loss: 0.0070  lr:0.000010
[ Sat Jul  6 17:46:23 2024 ] 	Batch(1300/7879) done. Loss: 0.0210  lr:0.000010
[ Sat Jul  6 17:46:41 2024 ] 	Batch(1400/7879) done. Loss: 0.2919  lr:0.000010
[ Sat Jul  6 17:46:59 2024 ] 
Training: Epoch [24/120], Step [1499], Loss: 0.10100266337394714, Training Accuracy: 97.925
[ Sat Jul  6 17:46:59 2024 ] 	Batch(1500/7879) done. Loss: 0.0061  lr:0.000010
[ Sat Jul  6 17:47:17 2024 ] 	Batch(1600/7879) done. Loss: 0.0152  lr:0.000010
[ Sat Jul  6 17:47:35 2024 ] 	Batch(1700/7879) done. Loss: 0.0276  lr:0.000010
[ Sat Jul  6 17:47:53 2024 ] 	Batch(1800/7879) done. Loss: 0.1302  lr:0.000010
[ Sat Jul  6 17:48:11 2024 ] 	Batch(1900/7879) done. Loss: 0.1294  lr:0.000010
[ Sat Jul  6 17:48:29 2024 ] 
Training: Epoch [24/120], Step [1999], Loss: 0.016171708703041077, Training Accuracy: 97.96875
[ Sat Jul  6 17:48:29 2024 ] 	Batch(2000/7879) done. Loss: 0.0094  lr:0.000010
[ Sat Jul  6 17:48:47 2024 ] 	Batch(2100/7879) done. Loss: 0.0677  lr:0.000010
[ Sat Jul  6 17:49:05 2024 ] 	Batch(2200/7879) done. Loss: 0.1657  lr:0.000010
[ Sat Jul  6 17:49:23 2024 ] 	Batch(2300/7879) done. Loss: 0.0760  lr:0.000010
[ Sat Jul  6 17:49:41 2024 ] 	Batch(2400/7879) done. Loss: 0.2074  lr:0.000010
[ Sat Jul  6 17:49:59 2024 ] 
Training: Epoch [24/120], Step [2499], Loss: 0.09645723551511765, Training Accuracy: 97.815
[ Sat Jul  6 17:49:59 2024 ] 	Batch(2500/7879) done. Loss: 0.0778  lr:0.000010
[ Sat Jul  6 17:50:17 2024 ] 	Batch(2600/7879) done. Loss: 0.2389  lr:0.000010
[ Sat Jul  6 17:50:35 2024 ] 	Batch(2700/7879) done. Loss: 0.0931  lr:0.000010
[ Sat Jul  6 17:50:53 2024 ] 	Batch(2800/7879) done. Loss: 0.0353  lr:0.000010
[ Sat Jul  6 17:51:11 2024 ] 	Batch(2900/7879) done. Loss: 0.0103  lr:0.000010
[ Sat Jul  6 17:51:29 2024 ] 
Training: Epoch [24/120], Step [2999], Loss: 0.48917391896247864, Training Accuracy: 97.775
[ Sat Jul  6 17:51:29 2024 ] 	Batch(3000/7879) done. Loss: 0.0133  lr:0.000010
[ Sat Jul  6 17:51:47 2024 ] 	Batch(3100/7879) done. Loss: 0.2902  lr:0.000010
[ Sat Jul  6 17:52:05 2024 ] 	Batch(3200/7879) done. Loss: 0.0160  lr:0.000010
[ Sat Jul  6 17:52:23 2024 ] 	Batch(3300/7879) done. Loss: 0.1686  lr:0.000010
[ Sat Jul  6 17:52:41 2024 ] 	Batch(3400/7879) done. Loss: 0.0940  lr:0.000010
[ Sat Jul  6 17:52:59 2024 ] 
Training: Epoch [24/120], Step [3499], Loss: 0.5354967713356018, Training Accuracy: 97.76785714285714
[ Sat Jul  6 17:52:59 2024 ] 	Batch(3500/7879) done. Loss: 0.0793  lr:0.000010
[ Sat Jul  6 17:53:17 2024 ] 	Batch(3600/7879) done. Loss: 0.1273  lr:0.000010
[ Sat Jul  6 17:53:35 2024 ] 	Batch(3700/7879) done. Loss: 0.0120  lr:0.000010
[ Sat Jul  6 17:53:53 2024 ] 	Batch(3800/7879) done. Loss: 0.2394  lr:0.000010
[ Sat Jul  6 17:54:11 2024 ] 	Batch(3900/7879) done. Loss: 0.1620  lr:0.000010
[ Sat Jul  6 17:54:29 2024 ] 
Training: Epoch [24/120], Step [3999], Loss: 0.007839501835405827, Training Accuracy: 97.746875
[ Sat Jul  6 17:54:29 2024 ] 	Batch(4000/7879) done. Loss: 0.0169  lr:0.000010
[ Sat Jul  6 17:54:47 2024 ] 	Batch(4100/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 17:55:05 2024 ] 	Batch(4200/7879) done. Loss: 0.0048  lr:0.000010
[ Sat Jul  6 17:55:23 2024 ] 	Batch(4300/7879) done. Loss: 0.0051  lr:0.000010
[ Sat Jul  6 17:55:41 2024 ] 	Batch(4400/7879) done. Loss: 0.0982  lr:0.000010
[ Sat Jul  6 17:55:59 2024 ] 
Training: Epoch [24/120], Step [4499], Loss: 0.18008674681186676, Training Accuracy: 97.74444444444444
[ Sat Jul  6 17:55:59 2024 ] 	Batch(4500/7879) done. Loss: 0.0025  lr:0.000010
[ Sat Jul  6 17:56:17 2024 ] 	Batch(4600/7879) done. Loss: 0.0010  lr:0.000010
[ Sat Jul  6 17:56:35 2024 ] 	Batch(4700/7879) done. Loss: 0.0791  lr:0.000010
[ Sat Jul  6 17:56:53 2024 ] 	Batch(4800/7879) done. Loss: 0.0078  lr:0.000010
[ Sat Jul  6 17:57:11 2024 ] 	Batch(4900/7879) done. Loss: 0.2713  lr:0.000010
[ Sat Jul  6 17:57:28 2024 ] 
Training: Epoch [24/120], Step [4999], Loss: 0.010408968664705753, Training Accuracy: 97.785
[ Sat Jul  6 17:57:29 2024 ] 	Batch(5000/7879) done. Loss: 0.1156  lr:0.000010
[ Sat Jul  6 17:57:46 2024 ] 	Batch(5100/7879) done. Loss: 0.3938  lr:0.000010
[ Sat Jul  6 17:58:05 2024 ] 	Batch(5200/7879) done. Loss: 0.0323  lr:0.000010
[ Sat Jul  6 17:58:22 2024 ] 	Batch(5300/7879) done. Loss: 0.1670  lr:0.000010
[ Sat Jul  6 17:58:40 2024 ] 	Batch(5400/7879) done. Loss: 0.1327  lr:0.000010
[ Sat Jul  6 17:58:58 2024 ] 
Training: Epoch [24/120], Step [5499], Loss: 0.03058123216032982, Training Accuracy: 97.79545454545455
[ Sat Jul  6 17:58:58 2024 ] 	Batch(5500/7879) done. Loss: 0.0168  lr:0.000010
[ Sat Jul  6 17:59:16 2024 ] 	Batch(5600/7879) done. Loss: 0.1925  lr:0.000010
[ Sat Jul  6 17:59:35 2024 ] 	Batch(5700/7879) done. Loss: 0.0114  lr:0.000010
[ Sat Jul  6 17:59:54 2024 ] 	Batch(5800/7879) done. Loss: 0.0156  lr:0.000010
[ Sat Jul  6 18:00:12 2024 ] 	Batch(5900/7879) done. Loss: 0.4591  lr:0.000010
[ Sat Jul  6 18:00:31 2024 ] 
Training: Epoch [24/120], Step [5999], Loss: 0.33397001028060913, Training Accuracy: 97.78333333333333
[ Sat Jul  6 18:00:31 2024 ] 	Batch(6000/7879) done. Loss: 0.0384  lr:0.000010
[ Sat Jul  6 18:00:49 2024 ] 	Batch(6100/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 18:01:07 2024 ] 	Batch(6200/7879) done. Loss: 0.0043  lr:0.000010
[ Sat Jul  6 18:01:25 2024 ] 	Batch(6300/7879) done. Loss: 0.0601  lr:0.000010
[ Sat Jul  6 18:01:43 2024 ] 	Batch(6400/7879) done. Loss: 0.0456  lr:0.000010
[ Sat Jul  6 18:02:01 2024 ] 
Training: Epoch [24/120], Step [6499], Loss: 0.007633207831531763, Training Accuracy: 97.78653846153847
[ Sat Jul  6 18:02:01 2024 ] 	Batch(6500/7879) done. Loss: 0.1063  lr:0.000010
[ Sat Jul  6 18:02:20 2024 ] 	Batch(6600/7879) done. Loss: 0.0330  lr:0.000010
[ Sat Jul  6 18:02:38 2024 ] 	Batch(6700/7879) done. Loss: 0.1940  lr:0.000010
[ Sat Jul  6 18:02:57 2024 ] 	Batch(6800/7879) done. Loss: 0.1192  lr:0.000010
[ Sat Jul  6 18:03:16 2024 ] 	Batch(6900/7879) done. Loss: 0.0572  lr:0.000010
[ Sat Jul  6 18:03:34 2024 ] 
Training: Epoch [24/120], Step [6999], Loss: 0.06888320297002792, Training Accuracy: 97.79107142857143
[ Sat Jul  6 18:03:34 2024 ] 	Batch(7000/7879) done. Loss: 0.0145  lr:0.000010
[ Sat Jul  6 18:03:53 2024 ] 	Batch(7100/7879) done. Loss: 0.1820  lr:0.000010
[ Sat Jul  6 18:04:12 2024 ] 	Batch(7200/7879) done. Loss: 0.0199  lr:0.000010
[ Sat Jul  6 18:04:30 2024 ] 	Batch(7300/7879) done. Loss: 0.3742  lr:0.000010
[ Sat Jul  6 18:04:48 2024 ] 	Batch(7400/7879) done. Loss: 0.2146  lr:0.000010
[ Sat Jul  6 18:05:06 2024 ] 
Training: Epoch [24/120], Step [7499], Loss: 0.02251606062054634, Training Accuracy: 97.77166666666666
[ Sat Jul  6 18:05:06 2024 ] 	Batch(7500/7879) done. Loss: 0.2767  lr:0.000010
[ Sat Jul  6 18:05:24 2024 ] 	Batch(7600/7879) done. Loss: 0.0463  lr:0.000010
[ Sat Jul  6 18:05:42 2024 ] 	Batch(7700/7879) done. Loss: 0.0161  lr:0.000010
[ Sat Jul  6 18:06:00 2024 ] 	Batch(7800/7879) done. Loss: 0.3457  lr:0.000010
[ Sat Jul  6 18:06:14 2024 ] 	Mean training loss: 0.0900.
[ Sat Jul  6 18:06:14 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 18:06:14 2024 ] Training epoch: 26
[ Sat Jul  6 18:06:15 2024 ] 	Batch(0/7879) done. Loss: 0.2270  lr:0.000010
[ Sat Jul  6 18:06:33 2024 ] 	Batch(100/7879) done. Loss: 0.0630  lr:0.000010
[ Sat Jul  6 18:06:52 2024 ] 	Batch(200/7879) done. Loss: 0.0215  lr:0.000010
[ Sat Jul  6 18:07:11 2024 ] 	Batch(300/7879) done. Loss: 0.0365  lr:0.000010
[ Sat Jul  6 18:07:29 2024 ] 	Batch(400/7879) done. Loss: 0.0200  lr:0.000010
[ Sat Jul  6 18:07:48 2024 ] 
Training: Epoch [25/120], Step [499], Loss: 0.19590531289577484, Training Accuracy: 97.775
[ Sat Jul  6 18:07:48 2024 ] 	Batch(500/7879) done. Loss: 0.0235  lr:0.000010
[ Sat Jul  6 18:08:07 2024 ] 	Batch(600/7879) done. Loss: 0.0857  lr:0.000010
[ Sat Jul  6 18:08:25 2024 ] 	Batch(700/7879) done. Loss: 0.0047  lr:0.000010
[ Sat Jul  6 18:08:44 2024 ] 	Batch(800/7879) done. Loss: 0.0851  lr:0.000010
[ Sat Jul  6 18:09:01 2024 ] 	Batch(900/7879) done. Loss: 0.0093  lr:0.000010
[ Sat Jul  6 18:09:19 2024 ] 
Training: Epoch [25/120], Step [999], Loss: 0.22468863427639008, Training Accuracy: 97.625
[ Sat Jul  6 18:09:19 2024 ] 	Batch(1000/7879) done. Loss: 0.0081  lr:0.000010
[ Sat Jul  6 18:09:37 2024 ] 	Batch(1100/7879) done. Loss: 0.0144  lr:0.000010
[ Sat Jul  6 18:09:55 2024 ] 	Batch(1200/7879) done. Loss: 0.0202  lr:0.000010
[ Sat Jul  6 18:10:13 2024 ] 	Batch(1300/7879) done. Loss: 0.1229  lr:0.000010
[ Sat Jul  6 18:10:31 2024 ] 	Batch(1400/7879) done. Loss: 0.5322  lr:0.000010
[ Sat Jul  6 18:10:49 2024 ] 
Training: Epoch [25/120], Step [1499], Loss: 0.20415820181369781, Training Accuracy: 97.68333333333334
[ Sat Jul  6 18:10:49 2024 ] 	Batch(1500/7879) done. Loss: 0.1604  lr:0.000010
[ Sat Jul  6 18:11:07 2024 ] 	Batch(1600/7879) done. Loss: 0.0131  lr:0.000010
[ Sat Jul  6 18:11:25 2024 ] 	Batch(1700/7879) done. Loss: 0.1552  lr:0.000010
[ Sat Jul  6 18:11:43 2024 ] 	Batch(1800/7879) done. Loss: 0.0179  lr:0.000010
[ Sat Jul  6 18:12:01 2024 ] 	Batch(1900/7879) done. Loss: 0.0049  lr:0.000010
[ Sat Jul  6 18:12:19 2024 ] 
Training: Epoch [25/120], Step [1999], Loss: 0.020586946979165077, Training Accuracy: 97.6375
[ Sat Jul  6 18:12:19 2024 ] 	Batch(2000/7879) done. Loss: 0.0465  lr:0.000010
[ Sat Jul  6 18:12:37 2024 ] 	Batch(2100/7879) done. Loss: 0.0259  lr:0.000010
[ Sat Jul  6 18:12:55 2024 ] 	Batch(2200/7879) done. Loss: 0.1007  lr:0.000010
[ Sat Jul  6 18:13:13 2024 ] 	Batch(2300/7879) done. Loss: 0.0711  lr:0.000010
[ Sat Jul  6 18:13:31 2024 ] 	Batch(2400/7879) done. Loss: 0.0023  lr:0.000010
[ Sat Jul  6 18:13:49 2024 ] 
Training: Epoch [25/120], Step [2499], Loss: 0.042478013783693314, Training Accuracy: 97.64500000000001
[ Sat Jul  6 18:13:49 2024 ] 	Batch(2500/7879) done. Loss: 0.0078  lr:0.000010
[ Sat Jul  6 18:14:07 2024 ] 	Batch(2600/7879) done. Loss: 0.2109  lr:0.000010
[ Sat Jul  6 18:14:25 2024 ] 	Batch(2700/7879) done. Loss: 0.0305  lr:0.000010
[ Sat Jul  6 18:14:43 2024 ] 	Batch(2800/7879) done. Loss: 0.0215  lr:0.000010
[ Sat Jul  6 18:15:01 2024 ] 	Batch(2900/7879) done. Loss: 0.1111  lr:0.000010
[ Sat Jul  6 18:15:20 2024 ] 
Training: Epoch [25/120], Step [2999], Loss: 0.13089776039123535, Training Accuracy: 97.69583333333334
[ Sat Jul  6 18:15:20 2024 ] 	Batch(3000/7879) done. Loss: 0.1162  lr:0.000010
[ Sat Jul  6 18:15:38 2024 ] 	Batch(3100/7879) done. Loss: 0.0321  lr:0.000010
[ Sat Jul  6 18:15:57 2024 ] 	Batch(3200/7879) done. Loss: 0.0314  lr:0.000010
[ Sat Jul  6 18:16:15 2024 ] 	Batch(3300/7879) done. Loss: 0.1183  lr:0.000010
[ Sat Jul  6 18:16:33 2024 ] 	Batch(3400/7879) done. Loss: 0.1133  lr:0.000010
[ Sat Jul  6 18:16:51 2024 ] 
Training: Epoch [25/120], Step [3499], Loss: 0.005975569598376751, Training Accuracy: 97.67857142857143
[ Sat Jul  6 18:16:51 2024 ] 	Batch(3500/7879) done. Loss: 0.1929  lr:0.000010
[ Sat Jul  6 18:17:09 2024 ] 	Batch(3600/7879) done. Loss: 0.0020  lr:0.000010
[ Sat Jul  6 18:17:27 2024 ] 	Batch(3700/7879) done. Loss: 0.5379  lr:0.000010
[ Sat Jul  6 18:17:45 2024 ] 	Batch(3800/7879) done. Loss: 0.0291  lr:0.000010
[ Sat Jul  6 18:18:03 2024 ] 	Batch(3900/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 18:18:21 2024 ] 
Training: Epoch [25/120], Step [3999], Loss: 0.10017542541027069, Training Accuracy: 97.73125
[ Sat Jul  6 18:18:21 2024 ] 	Batch(4000/7879) done. Loss: 0.1934  lr:0.000010
[ Sat Jul  6 18:18:39 2024 ] 	Batch(4100/7879) done. Loss: 0.0194  lr:0.000010
[ Sat Jul  6 18:18:57 2024 ] 	Batch(4200/7879) done. Loss: 0.4056  lr:0.000010
[ Sat Jul  6 18:19:15 2024 ] 	Batch(4300/7879) done. Loss: 0.0734  lr:0.000010
[ Sat Jul  6 18:19:33 2024 ] 	Batch(4400/7879) done. Loss: 0.2665  lr:0.000010
[ Sat Jul  6 18:19:50 2024 ] 
Training: Epoch [25/120], Step [4499], Loss: 0.010960984043776989, Training Accuracy: 97.76944444444445
[ Sat Jul  6 18:19:51 2024 ] 	Batch(4500/7879) done. Loss: 0.0222  lr:0.000010
[ Sat Jul  6 18:20:09 2024 ] 	Batch(4600/7879) done. Loss: 0.0367  lr:0.000010
[ Sat Jul  6 18:20:26 2024 ] 	Batch(4700/7879) done. Loss: 0.1046  lr:0.000010
[ Sat Jul  6 18:20:45 2024 ] 	Batch(4800/7879) done. Loss: 0.0972  lr:0.000010
[ Sat Jul  6 18:21:03 2024 ] 	Batch(4900/7879) done. Loss: 0.0327  lr:0.000010
[ Sat Jul  6 18:21:21 2024 ] 
Training: Epoch [25/120], Step [4999], Loss: 0.25758686661720276, Training Accuracy: 97.75500000000001
[ Sat Jul  6 18:21:22 2024 ] 	Batch(5000/7879) done. Loss: 0.0643  lr:0.000010
[ Sat Jul  6 18:21:40 2024 ] 	Batch(5100/7879) done. Loss: 0.0035  lr:0.000010
[ Sat Jul  6 18:21:59 2024 ] 	Batch(5200/7879) done. Loss: 0.0434  lr:0.000010
[ Sat Jul  6 18:22:18 2024 ] 	Batch(5300/7879) done. Loss: 0.0268  lr:0.000010
[ Sat Jul  6 18:22:36 2024 ] 	Batch(5400/7879) done. Loss: 0.0213  lr:0.000010
[ Sat Jul  6 18:22:54 2024 ] 
Training: Epoch [25/120], Step [5499], Loss: 0.04615827649831772, Training Accuracy: 97.78181818181818
[ Sat Jul  6 18:22:55 2024 ] 	Batch(5500/7879) done. Loss: 0.8493  lr:0.000010
[ Sat Jul  6 18:23:13 2024 ] 	Batch(5600/7879) done. Loss: 0.4131  lr:0.000010
[ Sat Jul  6 18:23:31 2024 ] 	Batch(5700/7879) done. Loss: 0.0144  lr:0.000010
[ Sat Jul  6 18:23:49 2024 ] 	Batch(5800/7879) done. Loss: 0.1161  lr:0.000010
[ Sat Jul  6 18:24:07 2024 ] 	Batch(5900/7879) done. Loss: 0.0276  lr:0.000010
[ Sat Jul  6 18:24:24 2024 ] 
Training: Epoch [25/120], Step [5999], Loss: 0.026369186118245125, Training Accuracy: 97.74374999999999
[ Sat Jul  6 18:24:25 2024 ] 	Batch(6000/7879) done. Loss: 0.0127  lr:0.000010
[ Sat Jul  6 18:24:43 2024 ] 	Batch(6100/7879) done. Loss: 0.1823  lr:0.000010
[ Sat Jul  6 18:25:02 2024 ] 	Batch(6200/7879) done. Loss: 0.0530  lr:0.000010
[ Sat Jul  6 18:25:20 2024 ] 	Batch(6300/7879) done. Loss: 0.0046  lr:0.000010
[ Sat Jul  6 18:25:39 2024 ] 	Batch(6400/7879) done. Loss: 0.0500  lr:0.000010
[ Sat Jul  6 18:25:57 2024 ] 
Training: Epoch [25/120], Step [6499], Loss: 0.07259654253721237, Training Accuracy: 97.75
[ Sat Jul  6 18:25:57 2024 ] 	Batch(6500/7879) done. Loss: 0.0651  lr:0.000010
[ Sat Jul  6 18:26:15 2024 ] 	Batch(6600/7879) done. Loss: 0.0147  lr:0.000010
[ Sat Jul  6 18:26:33 2024 ] 	Batch(6700/7879) done. Loss: 0.0585  lr:0.000010
[ Sat Jul  6 18:26:51 2024 ] 	Batch(6800/7879) done. Loss: 0.0822  lr:0.000010
[ Sat Jul  6 18:27:09 2024 ] 	Batch(6900/7879) done. Loss: 0.0234  lr:0.000010
[ Sat Jul  6 18:27:27 2024 ] 
Training: Epoch [25/120], Step [6999], Loss: 0.04733629897236824, Training Accuracy: 97.76785714285714
[ Sat Jul  6 18:27:27 2024 ] 	Batch(7000/7879) done. Loss: 0.0139  lr:0.000010
[ Sat Jul  6 18:27:45 2024 ] 	Batch(7100/7879) done. Loss: 0.1566  lr:0.000010
[ Sat Jul  6 18:28:03 2024 ] 	Batch(7200/7879) done. Loss: 0.0472  lr:0.000010
[ Sat Jul  6 18:28:22 2024 ] 	Batch(7300/7879) done. Loss: 0.0313  lr:0.000010
[ Sat Jul  6 18:28:40 2024 ] 	Batch(7400/7879) done. Loss: 0.0268  lr:0.000010
[ Sat Jul  6 18:28:58 2024 ] 
Training: Epoch [25/120], Step [7499], Loss: 0.17242375016212463, Training Accuracy: 97.785
[ Sat Jul  6 18:28:58 2024 ] 	Batch(7500/7879) done. Loss: 0.0655  lr:0.000010
[ Sat Jul  6 18:29:16 2024 ] 	Batch(7600/7879) done. Loss: 0.0680  lr:0.000010
[ Sat Jul  6 18:29:34 2024 ] 	Batch(7700/7879) done. Loss: 0.0659  lr:0.000010
[ Sat Jul  6 18:29:52 2024 ] 	Batch(7800/7879) done. Loss: 0.0235  lr:0.000010
[ Sat Jul  6 18:30:06 2024 ] 	Mean training loss: 0.0894.
[ Sat Jul  6 18:30:06 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 18:30:06 2024 ] Training epoch: 27
[ Sat Jul  6 18:30:07 2024 ] 	Batch(0/7879) done. Loss: 0.0072  lr:0.000010
[ Sat Jul  6 18:30:25 2024 ] 	Batch(100/7879) done. Loss: 0.2572  lr:0.000010
[ Sat Jul  6 18:30:43 2024 ] 	Batch(200/7879) done. Loss: 0.0255  lr:0.000010
[ Sat Jul  6 18:31:01 2024 ] 	Batch(300/7879) done. Loss: 0.0510  lr:0.000010
[ Sat Jul  6 18:31:19 2024 ] 	Batch(400/7879) done. Loss: 0.0171  lr:0.000010
[ Sat Jul  6 18:31:36 2024 ] 
Training: Epoch [26/120], Step [499], Loss: 0.1668614149093628, Training Accuracy: 97.625
[ Sat Jul  6 18:31:36 2024 ] 	Batch(500/7879) done. Loss: 0.0055  lr:0.000010
[ Sat Jul  6 18:31:54 2024 ] 	Batch(600/7879) done. Loss: 0.0702  lr:0.000010
[ Sat Jul  6 18:32:12 2024 ] 	Batch(700/7879) done. Loss: 0.1070  lr:0.000010
[ Sat Jul  6 18:32:30 2024 ] 	Batch(800/7879) done. Loss: 0.0107  lr:0.000010
[ Sat Jul  6 18:32:48 2024 ] 	Batch(900/7879) done. Loss: 0.1222  lr:0.000010
[ Sat Jul  6 18:33:06 2024 ] 
Training: Epoch [26/120], Step [999], Loss: 0.02165832184255123, Training Accuracy: 97.65
[ Sat Jul  6 18:33:06 2024 ] 	Batch(1000/7879) done. Loss: 0.0503  lr:0.000010
[ Sat Jul  6 18:33:24 2024 ] 	Batch(1100/7879) done. Loss: 0.3218  lr:0.000010
[ Sat Jul  6 18:33:42 2024 ] 	Batch(1200/7879) done. Loss: 0.0187  lr:0.000010
[ Sat Jul  6 18:34:00 2024 ] 	Batch(1300/7879) done. Loss: 0.0100  lr:0.000010
[ Sat Jul  6 18:34:18 2024 ] 	Batch(1400/7879) done. Loss: 0.1016  lr:0.000010
[ Sat Jul  6 18:34:36 2024 ] 
Training: Epoch [26/120], Step [1499], Loss: 0.2365848571062088, Training Accuracy: 97.675
[ Sat Jul  6 18:34:36 2024 ] 	Batch(1500/7879) done. Loss: 0.0121  lr:0.000010
[ Sat Jul  6 18:34:54 2024 ] 	Batch(1600/7879) done. Loss: 0.0229  lr:0.000010
[ Sat Jul  6 18:35:12 2024 ] 	Batch(1700/7879) done. Loss: 0.0174  lr:0.000010
[ Sat Jul  6 18:35:30 2024 ] 	Batch(1800/7879) done. Loss: 0.0941  lr:0.000010
[ Sat Jul  6 18:35:48 2024 ] 	Batch(1900/7879) done. Loss: 0.0018  lr:0.000010
[ Sat Jul  6 18:36:06 2024 ] 
Training: Epoch [26/120], Step [1999], Loss: 0.052008941769599915, Training Accuracy: 97.68124999999999
[ Sat Jul  6 18:36:06 2024 ] 	Batch(2000/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 18:36:24 2024 ] 	Batch(2100/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 18:36:42 2024 ] 	Batch(2200/7879) done. Loss: 0.0024  lr:0.000010
[ Sat Jul  6 18:37:00 2024 ] 	Batch(2300/7879) done. Loss: 0.3472  lr:0.000010
[ Sat Jul  6 18:37:18 2024 ] 	Batch(2400/7879) done. Loss: 0.0435  lr:0.000010
[ Sat Jul  6 18:37:36 2024 ] 
Training: Epoch [26/120], Step [2499], Loss: 0.06530268490314484, Training Accuracy: 97.775
[ Sat Jul  6 18:37:36 2024 ] 	Batch(2500/7879) done. Loss: 0.0715  lr:0.000010
[ Sat Jul  6 18:37:54 2024 ] 	Batch(2600/7879) done. Loss: 0.0405  lr:0.000010
[ Sat Jul  6 18:38:12 2024 ] 	Batch(2700/7879) done. Loss: 0.0218  lr:0.000010
[ Sat Jul  6 18:38:30 2024 ] 	Batch(2800/7879) done. Loss: 0.5209  lr:0.000010
[ Sat Jul  6 18:38:48 2024 ] 	Batch(2900/7879) done. Loss: 0.0063  lr:0.000010
[ Sat Jul  6 18:39:05 2024 ] 
Training: Epoch [26/120], Step [2999], Loss: 0.006472351029515266, Training Accuracy: 97.70833333333333
[ Sat Jul  6 18:39:06 2024 ] 	Batch(3000/7879) done. Loss: 0.1314  lr:0.000010
[ Sat Jul  6 18:39:23 2024 ] 	Batch(3100/7879) done. Loss: 0.1116  lr:0.000010
[ Sat Jul  6 18:39:42 2024 ] 	Batch(3200/7879) done. Loss: 0.1105  lr:0.000010
[ Sat Jul  6 18:39:59 2024 ] 	Batch(3300/7879) done. Loss: 0.8729  lr:0.000010
[ Sat Jul  6 18:40:17 2024 ] 	Batch(3400/7879) done. Loss: 0.0191  lr:0.000010
[ Sat Jul  6 18:40:35 2024 ] 
Training: Epoch [26/120], Step [3499], Loss: 0.04184885323047638, Training Accuracy: 97.63214285714285
[ Sat Jul  6 18:40:35 2024 ] 	Batch(3500/7879) done. Loss: 0.0058  lr:0.000010
[ Sat Jul  6 18:40:53 2024 ] 	Batch(3600/7879) done. Loss: 0.0260  lr:0.000010
[ Sat Jul  6 18:41:11 2024 ] 	Batch(3700/7879) done. Loss: 0.0054  lr:0.000010
[ Sat Jul  6 18:41:29 2024 ] 	Batch(3800/7879) done. Loss: 0.0537  lr:0.000010
[ Sat Jul  6 18:41:47 2024 ] 	Batch(3900/7879) done. Loss: 0.1802  lr:0.000010
[ Sat Jul  6 18:42:05 2024 ] 
Training: Epoch [26/120], Step [3999], Loss: 0.03148134797811508, Training Accuracy: 97.696875
[ Sat Jul  6 18:42:05 2024 ] 	Batch(4000/7879) done. Loss: 0.0668  lr:0.000010
[ Sat Jul  6 18:42:23 2024 ] 	Batch(4100/7879) done. Loss: 0.0048  lr:0.000010
[ Sat Jul  6 18:42:41 2024 ] 	Batch(4200/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 18:42:59 2024 ] 	Batch(4300/7879) done. Loss: 0.0560  lr:0.000010
[ Sat Jul  6 18:43:17 2024 ] 	Batch(4400/7879) done. Loss: 0.0476  lr:0.000010
[ Sat Jul  6 18:43:35 2024 ] 
Training: Epoch [26/120], Step [4499], Loss: 0.004988798871636391, Training Accuracy: 97.75
[ Sat Jul  6 18:43:35 2024 ] 	Batch(4500/7879) done. Loss: 0.0107  lr:0.000010
[ Sat Jul  6 18:43:53 2024 ] 	Batch(4600/7879) done. Loss: 0.0423  lr:0.000010
[ Sat Jul  6 18:44:10 2024 ] 	Batch(4700/7879) done. Loss: 0.0787  lr:0.000010
[ Sat Jul  6 18:44:28 2024 ] 	Batch(4800/7879) done. Loss: 0.0437  lr:0.000010
[ Sat Jul  6 18:44:46 2024 ] 	Batch(4900/7879) done. Loss: 0.3969  lr:0.000010
[ Sat Jul  6 18:45:04 2024 ] 
Training: Epoch [26/120], Step [4999], Loss: 0.10469512641429901, Training Accuracy: 97.745
[ Sat Jul  6 18:45:04 2024 ] 	Batch(5000/7879) done. Loss: 0.0156  lr:0.000010
[ Sat Jul  6 18:45:22 2024 ] 	Batch(5100/7879) done. Loss: 0.0084  lr:0.000010
[ Sat Jul  6 18:45:40 2024 ] 	Batch(5200/7879) done. Loss: 0.2868  lr:0.000010
[ Sat Jul  6 18:45:58 2024 ] 	Batch(5300/7879) done. Loss: 0.1311  lr:0.000010
[ Sat Jul  6 18:46:16 2024 ] 	Batch(5400/7879) done. Loss: 0.0013  lr:0.000010
[ Sat Jul  6 18:46:34 2024 ] 
Training: Epoch [26/120], Step [5499], Loss: 0.013919802382588387, Training Accuracy: 97.76818181818182
[ Sat Jul  6 18:46:34 2024 ] 	Batch(5500/7879) done. Loss: 0.0099  lr:0.000010
[ Sat Jul  6 18:46:52 2024 ] 	Batch(5600/7879) done. Loss: 0.0142  lr:0.000010
[ Sat Jul  6 18:47:10 2024 ] 	Batch(5700/7879) done. Loss: 0.2344  lr:0.000010
[ Sat Jul  6 18:47:28 2024 ] 	Batch(5800/7879) done. Loss: 0.4888  lr:0.000010
[ Sat Jul  6 18:47:46 2024 ] 	Batch(5900/7879) done. Loss: 0.0704  lr:0.000010
[ Sat Jul  6 18:48:04 2024 ] 
Training: Epoch [26/120], Step [5999], Loss: 0.09308042377233505, Training Accuracy: 97.75416666666666
[ Sat Jul  6 18:48:04 2024 ] 	Batch(6000/7879) done. Loss: 0.0474  lr:0.000010
[ Sat Jul  6 18:48:22 2024 ] 	Batch(6100/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 18:48:40 2024 ] 	Batch(6200/7879) done. Loss: 0.3130  lr:0.000010
[ Sat Jul  6 18:48:58 2024 ] 	Batch(6300/7879) done. Loss: 0.1842  lr:0.000010
[ Sat Jul  6 18:49:16 2024 ] 	Batch(6400/7879) done. Loss: 0.0499  lr:0.000010
[ Sat Jul  6 18:49:34 2024 ] 
Training: Epoch [26/120], Step [6499], Loss: 0.003394611645489931, Training Accuracy: 97.7673076923077
[ Sat Jul  6 18:49:34 2024 ] 	Batch(6500/7879) done. Loss: 0.2431  lr:0.000010
[ Sat Jul  6 18:49:53 2024 ] 	Batch(6600/7879) done. Loss: 0.0562  lr:0.000010
[ Sat Jul  6 18:50:11 2024 ] 	Batch(6700/7879) done. Loss: 0.1580  lr:0.000010
[ Sat Jul  6 18:50:29 2024 ] 	Batch(6800/7879) done. Loss: 0.0498  lr:0.000010
[ Sat Jul  6 18:50:47 2024 ] 	Batch(6900/7879) done. Loss: 0.0036  lr:0.000010
[ Sat Jul  6 18:51:05 2024 ] 
Training: Epoch [26/120], Step [6999], Loss: 0.216932475566864, Training Accuracy: 97.78035714285714
[ Sat Jul  6 18:51:05 2024 ] 	Batch(7000/7879) done. Loss: 0.2965  lr:0.000010
[ Sat Jul  6 18:51:23 2024 ] 	Batch(7100/7879) done. Loss: 0.0202  lr:0.000010
[ Sat Jul  6 18:51:42 2024 ] 	Batch(7200/7879) done. Loss: 0.0328  lr:0.000010
[ Sat Jul  6 18:52:00 2024 ] 	Batch(7300/7879) done. Loss: 0.0327  lr:0.000010
[ Sat Jul  6 18:52:18 2024 ] 	Batch(7400/7879) done. Loss: 0.2438  lr:0.000010
[ Sat Jul  6 18:52:36 2024 ] 
Training: Epoch [26/120], Step [7499], Loss: 0.017611771821975708, Training Accuracy: 97.78999999999999
[ Sat Jul  6 18:52:36 2024 ] 	Batch(7500/7879) done. Loss: 0.0493  lr:0.000010
[ Sat Jul  6 18:52:54 2024 ] 	Batch(7600/7879) done. Loss: 0.0027  lr:0.000010
[ Sat Jul  6 18:53:13 2024 ] 	Batch(7700/7879) done. Loss: 0.0106  lr:0.000010
[ Sat Jul  6 18:53:31 2024 ] 	Batch(7800/7879) done. Loss: 0.6531  lr:0.000010
[ Sat Jul  6 18:53:45 2024 ] 	Mean training loss: 0.0884.
[ Sat Jul  6 18:53:45 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 18:53:45 2024 ] Training epoch: 28
[ Sat Jul  6 18:53:46 2024 ] 	Batch(0/7879) done. Loss: 0.0123  lr:0.000010
[ Sat Jul  6 18:54:04 2024 ] 	Batch(100/7879) done. Loss: 0.0180  lr:0.000010
[ Sat Jul  6 18:54:22 2024 ] 	Batch(200/7879) done. Loss: 0.0473  lr:0.000010
[ Sat Jul  6 18:54:40 2024 ] 	Batch(300/7879) done. Loss: 0.0513  lr:0.000010
[ Sat Jul  6 18:54:58 2024 ] 	Batch(400/7879) done. Loss: 0.0576  lr:0.000010
[ Sat Jul  6 18:55:16 2024 ] 
Training: Epoch [27/120], Step [499], Loss: 0.016743239015340805, Training Accuracy: 98.3
[ Sat Jul  6 18:55:16 2024 ] 	Batch(500/7879) done. Loss: 0.0068  lr:0.000010
[ Sat Jul  6 18:55:34 2024 ] 	Batch(600/7879) done. Loss: 0.0543  lr:0.000010
[ Sat Jul  6 18:55:52 2024 ] 	Batch(700/7879) done. Loss: 0.0236  lr:0.000010
[ Sat Jul  6 18:56:10 2024 ] 	Batch(800/7879) done. Loss: 0.0021  lr:0.000010
[ Sat Jul  6 18:56:28 2024 ] 	Batch(900/7879) done. Loss: 0.0464  lr:0.000010
[ Sat Jul  6 18:56:46 2024 ] 
Training: Epoch [27/120], Step [999], Loss: 0.004149493295699358, Training Accuracy: 98.0625
[ Sat Jul  6 18:56:46 2024 ] 	Batch(1000/7879) done. Loss: 0.2584  lr:0.000010
[ Sat Jul  6 18:57:04 2024 ] 	Batch(1100/7879) done. Loss: 0.0896  lr:0.000010
[ Sat Jul  6 18:57:22 2024 ] 	Batch(1200/7879) done. Loss: 0.0176  lr:0.000010
[ Sat Jul  6 18:57:40 2024 ] 	Batch(1300/7879) done. Loss: 0.0050  lr:0.000010
[ Sat Jul  6 18:57:59 2024 ] 	Batch(1400/7879) done. Loss: 0.0015  lr:0.000010
[ Sat Jul  6 18:58:17 2024 ] 
Training: Epoch [27/120], Step [1499], Loss: 0.29780787229537964, Training Accuracy: 97.95
[ Sat Jul  6 18:58:17 2024 ] 	Batch(1500/7879) done. Loss: 0.0202  lr:0.000010
[ Sat Jul  6 18:58:35 2024 ] 	Batch(1600/7879) done. Loss: 0.0462  lr:0.000010
[ Sat Jul  6 18:58:53 2024 ] 	Batch(1700/7879) done. Loss: 0.0895  lr:0.000010
[ Sat Jul  6 18:59:11 2024 ] 	Batch(1800/7879) done. Loss: 0.3541  lr:0.000010
[ Sat Jul  6 18:59:28 2024 ] 	Batch(1900/7879) done. Loss: 0.0087  lr:0.000010
[ Sat Jul  6 18:59:46 2024 ] 
Training: Epoch [27/120], Step [1999], Loss: 0.21550723910331726, Training Accuracy: 97.85625
[ Sat Jul  6 18:59:46 2024 ] 	Batch(2000/7879) done. Loss: 0.0580  lr:0.000010
[ Sat Jul  6 19:00:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0010  lr:0.000010
[ Sat Jul  6 19:00:22 2024 ] 	Batch(2200/7879) done. Loss: 0.2856  lr:0.000010
[ Sat Jul  6 19:00:40 2024 ] 	Batch(2300/7879) done. Loss: 0.0128  lr:0.000010
[ Sat Jul  6 19:00:58 2024 ] 	Batch(2400/7879) done. Loss: 0.1547  lr:0.000010
[ Sat Jul  6 19:01:16 2024 ] 
Training: Epoch [27/120], Step [2499], Loss: 0.01904667541384697, Training Accuracy: 97.905
[ Sat Jul  6 19:01:16 2024 ] 	Batch(2500/7879) done. Loss: 0.0213  lr:0.000010
[ Sat Jul  6 19:01:34 2024 ] 	Batch(2600/7879) done. Loss: 0.0061  lr:0.000010
[ Sat Jul  6 19:01:52 2024 ] 	Batch(2700/7879) done. Loss: 0.0779  lr:0.000010
[ Sat Jul  6 19:02:10 2024 ] 	Batch(2800/7879) done. Loss: 0.1525  lr:0.000010
[ Sat Jul  6 19:02:28 2024 ] 	Batch(2900/7879) done. Loss: 0.0102  lr:0.000010
[ Sat Jul  6 19:02:46 2024 ] 
Training: Epoch [27/120], Step [2999], Loss: 0.08047948032617569, Training Accuracy: 97.87083333333332
[ Sat Jul  6 19:02:46 2024 ] 	Batch(3000/7879) done. Loss: 0.1722  lr:0.000010
[ Sat Jul  6 19:03:04 2024 ] 	Batch(3100/7879) done. Loss: 0.0862  lr:0.000010
[ Sat Jul  6 19:03:22 2024 ] 	Batch(3200/7879) done. Loss: 0.0047  lr:0.000010
[ Sat Jul  6 19:03:40 2024 ] 	Batch(3300/7879) done. Loss: 0.0657  lr:0.000010
[ Sat Jul  6 19:03:58 2024 ] 	Batch(3400/7879) done. Loss: 0.1288  lr:0.000010
[ Sat Jul  6 19:04:16 2024 ] 
Training: Epoch [27/120], Step [3499], Loss: 0.539350152015686, Training Accuracy: 97.875
[ Sat Jul  6 19:04:16 2024 ] 	Batch(3500/7879) done. Loss: 0.0414  lr:0.000010
[ Sat Jul  6 19:04:34 2024 ] 	Batch(3600/7879) done. Loss: 0.0067  lr:0.000010
[ Sat Jul  6 19:04:52 2024 ] 	Batch(3700/7879) done. Loss: 0.0309  lr:0.000010
[ Sat Jul  6 19:05:10 2024 ] 	Batch(3800/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 19:05:28 2024 ] 	Batch(3900/7879) done. Loss: 0.0374  lr:0.000010
[ Sat Jul  6 19:05:45 2024 ] 
Training: Epoch [27/120], Step [3999], Loss: 0.015587901696562767, Training Accuracy: 97.884375
[ Sat Jul  6 19:05:46 2024 ] 	Batch(4000/7879) done. Loss: 0.0099  lr:0.000010
[ Sat Jul  6 19:06:04 2024 ] 	Batch(4100/7879) done. Loss: 0.0218  lr:0.000010
[ Sat Jul  6 19:06:22 2024 ] 	Batch(4200/7879) done. Loss: 0.1238  lr:0.000010
[ Sat Jul  6 19:06:39 2024 ] 	Batch(4300/7879) done. Loss: 0.0037  lr:0.000010
[ Sat Jul  6 19:06:58 2024 ] 	Batch(4400/7879) done. Loss: 0.4090  lr:0.000010
[ Sat Jul  6 19:07:15 2024 ] 
Training: Epoch [27/120], Step [4499], Loss: 0.0030781878158450127, Training Accuracy: 97.90555555555555
[ Sat Jul  6 19:07:15 2024 ] 	Batch(4500/7879) done. Loss: 0.0067  lr:0.000010
[ Sat Jul  6 19:07:33 2024 ] 	Batch(4600/7879) done. Loss: 0.1962  lr:0.000010
[ Sat Jul  6 19:07:51 2024 ] 	Batch(4700/7879) done. Loss: 0.0937  lr:0.000010
[ Sat Jul  6 19:08:09 2024 ] 	Batch(4800/7879) done. Loss: 0.1129  lr:0.000010
[ Sat Jul  6 19:08:28 2024 ] 	Batch(4900/7879) done. Loss: 0.0055  lr:0.000010
[ Sat Jul  6 19:08:46 2024 ] 
Training: Epoch [27/120], Step [4999], Loss: 0.12207745760679245, Training Accuracy: 97.89
[ Sat Jul  6 19:08:46 2024 ] 	Batch(5000/7879) done. Loss: 0.1808  lr:0.000010
[ Sat Jul  6 19:09:04 2024 ] 	Batch(5100/7879) done. Loss: 0.0856  lr:0.000010
[ Sat Jul  6 19:09:22 2024 ] 	Batch(5200/7879) done. Loss: 0.0072  lr:0.000010
[ Sat Jul  6 19:09:40 2024 ] 	Batch(5300/7879) done. Loss: 0.0400  lr:0.000010
[ Sat Jul  6 19:09:58 2024 ] 	Batch(5400/7879) done. Loss: 0.0186  lr:0.000010
[ Sat Jul  6 19:10:15 2024 ] 
Training: Epoch [27/120], Step [5499], Loss: 0.1908230483531952, Training Accuracy: 97.92727272727274
[ Sat Jul  6 19:10:15 2024 ] 	Batch(5500/7879) done. Loss: 0.4434  lr:0.000010
[ Sat Jul  6 19:10:34 2024 ] 	Batch(5600/7879) done. Loss: 0.1316  lr:0.000010
[ Sat Jul  6 19:10:52 2024 ] 	Batch(5700/7879) done. Loss: 0.0565  lr:0.000010
[ Sat Jul  6 19:11:09 2024 ] 	Batch(5800/7879) done. Loss: 0.1040  lr:0.000010
[ Sat Jul  6 19:11:27 2024 ] 	Batch(5900/7879) done. Loss: 0.0239  lr:0.000010
[ Sat Jul  6 19:11:45 2024 ] 
Training: Epoch [27/120], Step [5999], Loss: 0.1537558138370514, Training Accuracy: 97.92916666666667
[ Sat Jul  6 19:11:45 2024 ] 	Batch(6000/7879) done. Loss: 0.1863  lr:0.000010
[ Sat Jul  6 19:12:04 2024 ] 	Batch(6100/7879) done. Loss: 0.1488  lr:0.000010
[ Sat Jul  6 19:12:23 2024 ] 	Batch(6200/7879) done. Loss: 0.0147  lr:0.000010
[ Sat Jul  6 19:12:41 2024 ] 	Batch(6300/7879) done. Loss: 0.3900  lr:0.000010
[ Sat Jul  6 19:13:00 2024 ] 	Batch(6400/7879) done. Loss: 0.2567  lr:0.000010
[ Sat Jul  6 19:13:18 2024 ] 
Training: Epoch [27/120], Step [6499], Loss: 0.003936052788048983, Training Accuracy: 97.91346153846155
[ Sat Jul  6 19:13:19 2024 ] 	Batch(6500/7879) done. Loss: 0.0583  lr:0.000010
[ Sat Jul  6 19:13:37 2024 ] 	Batch(6600/7879) done. Loss: 0.0700  lr:0.000010
[ Sat Jul  6 19:13:55 2024 ] 	Batch(6700/7879) done. Loss: 0.0393  lr:0.000010
[ Sat Jul  6 19:14:13 2024 ] 	Batch(6800/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 19:14:32 2024 ] 	Batch(6900/7879) done. Loss: 0.1441  lr:0.000010
[ Sat Jul  6 19:14:50 2024 ] 
Training: Epoch [27/120], Step [6999], Loss: 0.004456185735762119, Training Accuracy: 97.91785714285714
[ Sat Jul  6 19:14:50 2024 ] 	Batch(7000/7879) done. Loss: 0.0065  lr:0.000010
[ Sat Jul  6 19:15:09 2024 ] 	Batch(7100/7879) done. Loss: 0.0660  lr:0.000010
[ Sat Jul  6 19:15:28 2024 ] 	Batch(7200/7879) done. Loss: 0.0247  lr:0.000010
[ Sat Jul  6 19:15:46 2024 ] 	Batch(7300/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 19:16:04 2024 ] 	Batch(7400/7879) done. Loss: 0.2173  lr:0.000010
[ Sat Jul  6 19:16:22 2024 ] 
Training: Epoch [27/120], Step [7499], Loss: 0.03553804010152817, Training Accuracy: 97.89166666666667
[ Sat Jul  6 19:16:22 2024 ] 	Batch(7500/7879) done. Loss: 0.0293  lr:0.000010
[ Sat Jul  6 19:16:40 2024 ] 	Batch(7600/7879) done. Loss: 0.3146  lr:0.000010
[ Sat Jul  6 19:16:59 2024 ] 	Batch(7700/7879) done. Loss: 0.1974  lr:0.000010
[ Sat Jul  6 19:17:17 2024 ] 	Batch(7800/7879) done. Loss: 0.0078  lr:0.000010
[ Sat Jul  6 19:17:32 2024 ] 	Mean training loss: 0.0879.
[ Sat Jul  6 19:17:32 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 19:17:32 2024 ] Training epoch: 29
[ Sat Jul  6 19:17:33 2024 ] 	Batch(0/7879) done. Loss: 0.0141  lr:0.000010
[ Sat Jul  6 19:17:51 2024 ] 	Batch(100/7879) done. Loss: 0.0234  lr:0.000010
[ Sat Jul  6 19:18:09 2024 ] 	Batch(200/7879) done. Loss: 0.9315  lr:0.000010
[ Sat Jul  6 19:18:27 2024 ] 	Batch(300/7879) done. Loss: 0.0135  lr:0.000010
[ Sat Jul  6 19:18:46 2024 ] 	Batch(400/7879) done. Loss: 0.1009  lr:0.000010
[ Sat Jul  6 19:19:04 2024 ] 
Training: Epoch [28/120], Step [499], Loss: 0.16282522678375244, Training Accuracy: 97.25
[ Sat Jul  6 19:19:04 2024 ] 	Batch(500/7879) done. Loss: 0.0348  lr:0.000010
[ Sat Jul  6 19:19:23 2024 ] 	Batch(600/7879) done. Loss: 0.0667  lr:0.000010
[ Sat Jul  6 19:19:41 2024 ] 	Batch(700/7879) done. Loss: 0.0204  lr:0.000010
[ Sat Jul  6 19:20:00 2024 ] 	Batch(800/7879) done. Loss: 0.0465  lr:0.000010
[ Sat Jul  6 19:20:19 2024 ] 	Batch(900/7879) done. Loss: 0.0497  lr:0.000010
[ Sat Jul  6 19:20:37 2024 ] 
Training: Epoch [28/120], Step [999], Loss: 0.05440282076597214, Training Accuracy: 97.55
[ Sat Jul  6 19:20:37 2024 ] 	Batch(1000/7879) done. Loss: 0.0863  lr:0.000010
[ Sat Jul  6 19:20:55 2024 ] 	Batch(1100/7879) done. Loss: 0.0201  lr:0.000010
[ Sat Jul  6 19:21:13 2024 ] 	Batch(1200/7879) done. Loss: 0.0404  lr:0.000010
[ Sat Jul  6 19:21:31 2024 ] 	Batch(1300/7879) done. Loss: 0.1376  lr:0.000010
[ Sat Jul  6 19:21:49 2024 ] 	Batch(1400/7879) done. Loss: 0.2898  lr:0.000010
[ Sat Jul  6 19:22:07 2024 ] 
Training: Epoch [28/120], Step [1499], Loss: 0.3050101399421692, Training Accuracy: 97.68333333333334
[ Sat Jul  6 19:22:07 2024 ] 	Batch(1500/7879) done. Loss: 0.0293  lr:0.000010
[ Sat Jul  6 19:22:25 2024 ] 	Batch(1600/7879) done. Loss: 0.0925  lr:0.000010
[ Sat Jul  6 19:22:43 2024 ] 	Batch(1700/7879) done. Loss: 0.1624  lr:0.000010
[ Sat Jul  6 19:23:01 2024 ] 	Batch(1800/7879) done. Loss: 0.3318  lr:0.000010
[ Sat Jul  6 19:23:19 2024 ] 	Batch(1900/7879) done. Loss: 0.0973  lr:0.000010
[ Sat Jul  6 19:23:37 2024 ] 
Training: Epoch [28/120], Step [1999], Loss: 0.017184097319841385, Training Accuracy: 97.7125
[ Sat Jul  6 19:23:37 2024 ] 	Batch(2000/7879) done. Loss: 0.0208  lr:0.000010
[ Sat Jul  6 19:23:55 2024 ] 	Batch(2100/7879) done. Loss: 0.0385  lr:0.000010
[ Sat Jul  6 19:24:13 2024 ] 	Batch(2200/7879) done. Loss: 0.2002  lr:0.000010
[ Sat Jul  6 19:24:31 2024 ] 	Batch(2300/7879) done. Loss: 0.4663  lr:0.000010
[ Sat Jul  6 19:24:49 2024 ] 	Batch(2400/7879) done. Loss: 0.1046  lr:0.000010
[ Sat Jul  6 19:25:07 2024 ] 
Training: Epoch [28/120], Step [2499], Loss: 0.011990985833108425, Training Accuracy: 97.685
[ Sat Jul  6 19:25:07 2024 ] 	Batch(2500/7879) done. Loss: 0.0062  lr:0.000010
[ Sat Jul  6 19:25:25 2024 ] 	Batch(2600/7879) done. Loss: 0.0458  lr:0.000010
[ Sat Jul  6 19:25:43 2024 ] 	Batch(2700/7879) done. Loss: 0.1321  lr:0.000010
[ Sat Jul  6 19:26:01 2024 ] 	Batch(2800/7879) done. Loss: 0.0168  lr:0.000010
[ Sat Jul  6 19:26:19 2024 ] 	Batch(2900/7879) done. Loss: 0.0703  lr:0.000010
[ Sat Jul  6 19:26:37 2024 ] 
Training: Epoch [28/120], Step [2999], Loss: 0.09718085825443268, Training Accuracy: 97.70416666666667
[ Sat Jul  6 19:26:37 2024 ] 	Batch(3000/7879) done. Loss: 0.0406  lr:0.000010
[ Sat Jul  6 19:26:55 2024 ] 	Batch(3100/7879) done. Loss: 0.0378  lr:0.000010
[ Sat Jul  6 19:27:13 2024 ] 	Batch(3200/7879) done. Loss: 0.0255  lr:0.000010
[ Sat Jul  6 19:27:31 2024 ] 	Batch(3300/7879) done. Loss: 0.0014  lr:0.000010
[ Sat Jul  6 19:27:49 2024 ] 	Batch(3400/7879) done. Loss: 0.0037  lr:0.000010
[ Sat Jul  6 19:28:06 2024 ] 
Training: Epoch [28/120], Step [3499], Loss: 0.00679510785266757, Training Accuracy: 97.76428571428572
[ Sat Jul  6 19:28:07 2024 ] 	Batch(3500/7879) done. Loss: 0.0948  lr:0.000010
[ Sat Jul  6 19:28:25 2024 ] 	Batch(3600/7879) done. Loss: 0.2654  lr:0.000010
[ Sat Jul  6 19:28:43 2024 ] 	Batch(3700/7879) done. Loss: 0.0020  lr:0.000010
[ Sat Jul  6 19:29:01 2024 ] 	Batch(3800/7879) done. Loss: 0.0015  lr:0.000010
[ Sat Jul  6 19:29:18 2024 ] 	Batch(3900/7879) done. Loss: 0.4331  lr:0.000010
[ Sat Jul  6 19:29:36 2024 ] 
Training: Epoch [28/120], Step [3999], Loss: 0.0859905481338501, Training Accuracy: 97.73125
[ Sat Jul  6 19:29:37 2024 ] 	Batch(4000/7879) done. Loss: 0.4764  lr:0.000010
[ Sat Jul  6 19:29:54 2024 ] 	Batch(4100/7879) done. Loss: 0.0109  lr:0.000010
[ Sat Jul  6 19:30:12 2024 ] 	Batch(4200/7879) done. Loss: 0.2366  lr:0.000010
[ Sat Jul  6 19:30:30 2024 ] 	Batch(4300/7879) done. Loss: 0.0150  lr:0.000010
[ Sat Jul  6 19:30:48 2024 ] 	Batch(4400/7879) done. Loss: 0.0701  lr:0.000010
[ Sat Jul  6 19:31:06 2024 ] 
Training: Epoch [28/120], Step [4499], Loss: 0.030120011419057846, Training Accuracy: 97.70277777777778
[ Sat Jul  6 19:31:06 2024 ] 	Batch(4500/7879) done. Loss: 0.0814  lr:0.000010
[ Sat Jul  6 19:31:24 2024 ] 	Batch(4600/7879) done. Loss: 0.0893  lr:0.000010
[ Sat Jul  6 19:31:42 2024 ] 	Batch(4700/7879) done. Loss: 0.1064  lr:0.000010
[ Sat Jul  6 19:32:00 2024 ] 	Batch(4800/7879) done. Loss: 0.0443  lr:0.000010
[ Sat Jul  6 19:32:19 2024 ] 	Batch(4900/7879) done. Loss: 0.0984  lr:0.000010
[ Sat Jul  6 19:32:37 2024 ] 
Training: Epoch [28/120], Step [4999], Loss: 0.004754732828587294, Training Accuracy: 97.715
[ Sat Jul  6 19:32:37 2024 ] 	Batch(5000/7879) done. Loss: 0.1644  lr:0.000010
[ Sat Jul  6 19:32:56 2024 ] 	Batch(5100/7879) done. Loss: 0.0603  lr:0.000010
[ Sat Jul  6 19:33:14 2024 ] 	Batch(5200/7879) done. Loss: 0.0109  lr:0.000010
[ Sat Jul  6 19:33:32 2024 ] 	Batch(5300/7879) done. Loss: 0.0130  lr:0.000010
[ Sat Jul  6 19:33:50 2024 ] 	Batch(5400/7879) done. Loss: 0.3270  lr:0.000010
[ Sat Jul  6 19:34:07 2024 ] 
Training: Epoch [28/120], Step [5499], Loss: 0.0033880448900163174, Training Accuracy: 97.73636363636363
[ Sat Jul  6 19:34:08 2024 ] 	Batch(5500/7879) done. Loss: 0.0251  lr:0.000010
[ Sat Jul  6 19:34:26 2024 ] 	Batch(5600/7879) done. Loss: 0.0096  lr:0.000010
[ Sat Jul  6 19:34:44 2024 ] 	Batch(5700/7879) done. Loss: 0.0034  lr:0.000010
[ Sat Jul  6 19:35:01 2024 ] 	Batch(5800/7879) done. Loss: 0.0207  lr:0.000010
[ Sat Jul  6 19:35:19 2024 ] 	Batch(5900/7879) done. Loss: 0.0186  lr:0.000010
[ Sat Jul  6 19:35:37 2024 ] 
Training: Epoch [28/120], Step [5999], Loss: 0.011087584309279919, Training Accuracy: 97.76458333333333
[ Sat Jul  6 19:35:37 2024 ] 	Batch(6000/7879) done. Loss: 0.3129  lr:0.000010
[ Sat Jul  6 19:35:55 2024 ] 	Batch(6100/7879) done. Loss: 0.1409  lr:0.000010
[ Sat Jul  6 19:36:13 2024 ] 	Batch(6200/7879) done. Loss: 0.0419  lr:0.000010
[ Sat Jul  6 19:36:31 2024 ] 	Batch(6300/7879) done. Loss: 0.0331  lr:0.000010
[ Sat Jul  6 19:36:49 2024 ] 	Batch(6400/7879) done. Loss: 0.2390  lr:0.000010
[ Sat Jul  6 19:37:08 2024 ] 
Training: Epoch [28/120], Step [6499], Loss: 0.09678440541028976, Training Accuracy: 97.74038461538461
[ Sat Jul  6 19:37:08 2024 ] 	Batch(6500/7879) done. Loss: 0.1182  lr:0.000010
[ Sat Jul  6 19:37:26 2024 ] 	Batch(6600/7879) done. Loss: 0.0095  lr:0.000010
[ Sat Jul  6 19:37:45 2024 ] 	Batch(6700/7879) done. Loss: 0.3273  lr:0.000010
[ Sat Jul  6 19:38:04 2024 ] 	Batch(6800/7879) done. Loss: 0.0333  lr:0.000010
[ Sat Jul  6 19:38:22 2024 ] 	Batch(6900/7879) done. Loss: 0.0262  lr:0.000010
[ Sat Jul  6 19:38:39 2024 ] 
Training: Epoch [28/120], Step [6999], Loss: 0.14041557908058167, Training Accuracy: 97.75178571428572
[ Sat Jul  6 19:38:40 2024 ] 	Batch(7000/7879) done. Loss: 0.0346  lr:0.000010
[ Sat Jul  6 19:38:57 2024 ] 	Batch(7100/7879) done. Loss: 0.0584  lr:0.000010
[ Sat Jul  6 19:39:16 2024 ] 	Batch(7200/7879) done. Loss: 0.0131  lr:0.000010
[ Sat Jul  6 19:39:34 2024 ] 	Batch(7300/7879) done. Loss: 0.0210  lr:0.000010
[ Sat Jul  6 19:39:53 2024 ] 	Batch(7400/7879) done. Loss: 0.0337  lr:0.000010
[ Sat Jul  6 19:40:11 2024 ] 
Training: Epoch [28/120], Step [7499], Loss: 0.00558211375027895, Training Accuracy: 97.77166666666666
[ Sat Jul  6 19:40:11 2024 ] 	Batch(7500/7879) done. Loss: 0.1622  lr:0.000010
[ Sat Jul  6 19:40:30 2024 ] 	Batch(7600/7879) done. Loss: 0.0322  lr:0.000010
[ Sat Jul  6 19:40:49 2024 ] 	Batch(7700/7879) done. Loss: 0.0263  lr:0.000010
[ Sat Jul  6 19:41:07 2024 ] 	Batch(7800/7879) done. Loss: 0.0270  lr:0.000010
[ Sat Jul  6 19:41:22 2024 ] 	Mean training loss: 0.0896.
[ Sat Jul  6 19:41:22 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 19:41:22 2024 ] Training epoch: 30
[ Sat Jul  6 19:41:22 2024 ] 	Batch(0/7879) done. Loss: 0.5240  lr:0.000010
[ Sat Jul  6 19:41:41 2024 ] 	Batch(100/7879) done. Loss: 0.0893  lr:0.000010
[ Sat Jul  6 19:41:59 2024 ] 	Batch(200/7879) done. Loss: 0.0541  lr:0.000010
[ Sat Jul  6 19:42:17 2024 ] 	Batch(300/7879) done. Loss: 0.0032  lr:0.000010
[ Sat Jul  6 19:42:35 2024 ] 	Batch(400/7879) done. Loss: 0.0888  lr:0.000010
[ Sat Jul  6 19:42:54 2024 ] 
Training: Epoch [29/120], Step [499], Loss: 0.20330429077148438, Training Accuracy: 97.675
[ Sat Jul  6 19:42:54 2024 ] 	Batch(500/7879) done. Loss: 0.0066  lr:0.000010
[ Sat Jul  6 19:43:12 2024 ] 	Batch(600/7879) done. Loss: 0.0783  lr:0.000010
[ Sat Jul  6 19:43:30 2024 ] 	Batch(700/7879) done. Loss: 0.0035  lr:0.000010
[ Sat Jul  6 19:43:49 2024 ] 	Batch(800/7879) done. Loss: 0.3255  lr:0.000010
[ Sat Jul  6 19:44:07 2024 ] 	Batch(900/7879) done. Loss: 0.0676  lr:0.000010
[ Sat Jul  6 19:44:25 2024 ] 
Training: Epoch [29/120], Step [999], Loss: 0.038564540445804596, Training Accuracy: 97.8875
[ Sat Jul  6 19:44:25 2024 ] 	Batch(1000/7879) done. Loss: 0.0455  lr:0.000010
[ Sat Jul  6 19:44:44 2024 ] 	Batch(1100/7879) done. Loss: 0.0301  lr:0.000010
[ Sat Jul  6 19:45:02 2024 ] 	Batch(1200/7879) done. Loss: 0.0501  lr:0.000010
[ Sat Jul  6 19:45:20 2024 ] 	Batch(1300/7879) done. Loss: 0.5305  lr:0.000010
[ Sat Jul  6 19:45:38 2024 ] 	Batch(1400/7879) done. Loss: 0.0371  lr:0.000010
[ Sat Jul  6 19:45:57 2024 ] 
Training: Epoch [29/120], Step [1499], Loss: 0.05942525714635849, Training Accuracy: 97.85000000000001
[ Sat Jul  6 19:45:57 2024 ] 	Batch(1500/7879) done. Loss: 0.2955  lr:0.000010
[ Sat Jul  6 19:46:16 2024 ] 	Batch(1600/7879) done. Loss: 0.0252  lr:0.000010
[ Sat Jul  6 19:46:34 2024 ] 	Batch(1700/7879) done. Loss: 0.0077  lr:0.000010
[ Sat Jul  6 19:46:52 2024 ] 	Batch(1800/7879) done. Loss: 0.1227  lr:0.000010
[ Sat Jul  6 19:47:11 2024 ] 	Batch(1900/7879) done. Loss: 0.0094  lr:0.000010
[ Sat Jul  6 19:47:29 2024 ] 
Training: Epoch [29/120], Step [1999], Loss: 0.1379324048757553, Training Accuracy: 97.83125
[ Sat Jul  6 19:47:29 2024 ] 	Batch(2000/7879) done. Loss: 0.0918  lr:0.000010
[ Sat Jul  6 19:47:47 2024 ] 	Batch(2100/7879) done. Loss: 0.0092  lr:0.000010
[ Sat Jul  6 19:48:05 2024 ] 	Batch(2200/7879) done. Loss: 0.2268  lr:0.000010
[ Sat Jul  6 19:48:24 2024 ] 	Batch(2300/7879) done. Loss: 0.0302  lr:0.000010
[ Sat Jul  6 19:48:43 2024 ] 	Batch(2400/7879) done. Loss: 0.0956  lr:0.000010
[ Sat Jul  6 19:49:00 2024 ] 
Training: Epoch [29/120], Step [2499], Loss: 0.010346145369112492, Training Accuracy: 97.86500000000001
[ Sat Jul  6 19:49:00 2024 ] 	Batch(2500/7879) done. Loss: 0.0305  lr:0.000010
[ Sat Jul  6 19:49:18 2024 ] 	Batch(2600/7879) done. Loss: 0.0653  lr:0.000010
[ Sat Jul  6 19:49:36 2024 ] 	Batch(2700/7879) done. Loss: 0.0840  lr:0.000010
[ Sat Jul  6 19:49:54 2024 ] 	Batch(2800/7879) done. Loss: 0.0070  lr:0.000010
[ Sat Jul  6 19:50:12 2024 ] 	Batch(2900/7879) done. Loss: 0.0435  lr:0.000010
[ Sat Jul  6 19:50:30 2024 ] 
Training: Epoch [29/120], Step [2999], Loss: 0.44400399923324585, Training Accuracy: 97.8375
[ Sat Jul  6 19:50:30 2024 ] 	Batch(3000/7879) done. Loss: 0.0802  lr:0.000010
[ Sat Jul  6 19:50:48 2024 ] 	Batch(3100/7879) done. Loss: 0.1167  lr:0.000010
[ Sat Jul  6 19:51:06 2024 ] 	Batch(3200/7879) done. Loss: 0.0597  lr:0.000010
[ Sat Jul  6 19:51:24 2024 ] 	Batch(3300/7879) done. Loss: 0.0702  lr:0.000010
[ Sat Jul  6 19:51:42 2024 ] 	Batch(3400/7879) done. Loss: 0.0494  lr:0.000010
[ Sat Jul  6 19:52:00 2024 ] 
Training: Epoch [29/120], Step [3499], Loss: 0.04126046970486641, Training Accuracy: 97.83214285714286
[ Sat Jul  6 19:52:00 2024 ] 	Batch(3500/7879) done. Loss: 0.0527  lr:0.000010
[ Sat Jul  6 19:52:18 2024 ] 	Batch(3600/7879) done. Loss: 0.0428  lr:0.000010
[ Sat Jul  6 19:52:37 2024 ] 	Batch(3700/7879) done. Loss: 0.0054  lr:0.000010
[ Sat Jul  6 19:52:54 2024 ] 	Batch(3800/7879) done. Loss: 0.0134  lr:0.000010
[ Sat Jul  6 19:53:12 2024 ] 	Batch(3900/7879) done. Loss: 0.0846  lr:0.000010
[ Sat Jul  6 19:53:30 2024 ] 
Training: Epoch [29/120], Step [3999], Loss: 0.03728840500116348, Training Accuracy: 97.840625
[ Sat Jul  6 19:53:31 2024 ] 	Batch(4000/7879) done. Loss: 0.0024  lr:0.000010
[ Sat Jul  6 19:53:48 2024 ] 	Batch(4100/7879) done. Loss: 0.0220  lr:0.000010
[ Sat Jul  6 19:54:06 2024 ] 	Batch(4200/7879) done. Loss: 0.8040  lr:0.000010
[ Sat Jul  6 19:54:24 2024 ] 	Batch(4300/7879) done. Loss: 0.0511  lr:0.000010
[ Sat Jul  6 19:54:43 2024 ] 	Batch(4400/7879) done. Loss: 0.0518  lr:0.000010
[ Sat Jul  6 19:55:00 2024 ] 
Training: Epoch [29/120], Step [4499], Loss: 0.01388383936136961, Training Accuracy: 97.86388888888888
[ Sat Jul  6 19:55:00 2024 ] 	Batch(4500/7879) done. Loss: 0.0169  lr:0.000010
[ Sat Jul  6 19:55:18 2024 ] 	Batch(4600/7879) done. Loss: 0.0114  lr:0.000010
[ Sat Jul  6 19:55:36 2024 ] 	Batch(4700/7879) done. Loss: 0.1096  lr:0.000010
[ Sat Jul  6 19:55:54 2024 ] 	Batch(4800/7879) done. Loss: 0.1068  lr:0.000010
[ Sat Jul  6 19:56:12 2024 ] 	Batch(4900/7879) done. Loss: 0.0102  lr:0.000010
[ Sat Jul  6 19:56:30 2024 ] 
Training: Epoch [29/120], Step [4999], Loss: 0.14884762465953827, Training Accuracy: 97.875
[ Sat Jul  6 19:56:30 2024 ] 	Batch(5000/7879) done. Loss: 0.0272  lr:0.000010
[ Sat Jul  6 19:56:48 2024 ] 	Batch(5100/7879) done. Loss: 0.0814  lr:0.000010
[ Sat Jul  6 19:57:06 2024 ] 	Batch(5200/7879) done. Loss: 0.0437  lr:0.000010
[ Sat Jul  6 19:57:24 2024 ] 	Batch(5300/7879) done. Loss: 0.0600  lr:0.000010
[ Sat Jul  6 19:57:42 2024 ] 	Batch(5400/7879) done. Loss: 0.3127  lr:0.000010
[ Sat Jul  6 19:58:00 2024 ] 
Training: Epoch [29/120], Step [5499], Loss: 0.1485818475484848, Training Accuracy: 97.82954545454545
[ Sat Jul  6 19:58:00 2024 ] 	Batch(5500/7879) done. Loss: 0.2940  lr:0.000010
[ Sat Jul  6 19:58:18 2024 ] 	Batch(5600/7879) done. Loss: 0.0246  lr:0.000010
[ Sat Jul  6 19:58:36 2024 ] 	Batch(5700/7879) done. Loss: 0.0506  lr:0.000010
[ Sat Jul  6 19:58:54 2024 ] 	Batch(5800/7879) done. Loss: 0.0249  lr:0.000010
[ Sat Jul  6 19:59:12 2024 ] 	Batch(5900/7879) done. Loss: 0.0026  lr:0.000010
[ Sat Jul  6 19:59:30 2024 ] 
Training: Epoch [29/120], Step [5999], Loss: 0.1341320127248764, Training Accuracy: 97.79583333333333
[ Sat Jul  6 19:59:30 2024 ] 	Batch(6000/7879) done. Loss: 0.0222  lr:0.000010
[ Sat Jul  6 19:59:48 2024 ] 	Batch(6100/7879) done. Loss: 0.0200  lr:0.000010
[ Sat Jul  6 20:00:06 2024 ] 	Batch(6200/7879) done. Loss: 0.0155  lr:0.000010
[ Sat Jul  6 20:00:24 2024 ] 	Batch(6300/7879) done. Loss: 0.0269  lr:0.000010
[ Sat Jul  6 20:00:42 2024 ] 	Batch(6400/7879) done. Loss: 0.0239  lr:0.000010
[ Sat Jul  6 20:01:00 2024 ] 
Training: Epoch [29/120], Step [6499], Loss: 0.007941095158457756, Training Accuracy: 97.80192307692307
[ Sat Jul  6 20:01:00 2024 ] 	Batch(6500/7879) done. Loss: 0.0744  lr:0.000010
[ Sat Jul  6 20:01:18 2024 ] 	Batch(6600/7879) done. Loss: 0.0158  lr:0.000010
[ Sat Jul  6 20:01:36 2024 ] 	Batch(6700/7879) done. Loss: 0.0338  lr:0.000010
[ Sat Jul  6 20:01:54 2024 ] 	Batch(6800/7879) done. Loss: 0.4139  lr:0.000010
[ Sat Jul  6 20:02:12 2024 ] 	Batch(6900/7879) done. Loss: 0.1495  lr:0.000010
[ Sat Jul  6 20:02:29 2024 ] 
Training: Epoch [29/120], Step [6999], Loss: 0.016629980877041817, Training Accuracy: 97.80714285714286
[ Sat Jul  6 20:02:30 2024 ] 	Batch(7000/7879) done. Loss: 0.4539  lr:0.000010
[ Sat Jul  6 20:02:48 2024 ] 	Batch(7100/7879) done. Loss: 0.1768  lr:0.000010
[ Sat Jul  6 20:03:06 2024 ] 	Batch(7200/7879) done. Loss: 0.1660  lr:0.000010
[ Sat Jul  6 20:03:24 2024 ] 	Batch(7300/7879) done. Loss: 0.0999  lr:0.000010
[ Sat Jul  6 20:03:42 2024 ] 	Batch(7400/7879) done. Loss: 0.2457  lr:0.000010
[ Sat Jul  6 20:03:59 2024 ] 
Training: Epoch [29/120], Step [7499], Loss: 0.04120530188083649, Training Accuracy: 97.815
[ Sat Jul  6 20:04:00 2024 ] 	Batch(7500/7879) done. Loss: 0.0142  lr:0.000010
[ Sat Jul  6 20:04:18 2024 ] 	Batch(7600/7879) done. Loss: 0.1045  lr:0.000010
[ Sat Jul  6 20:04:36 2024 ] 	Batch(7700/7879) done. Loss: 0.0567  lr:0.000010
[ Sat Jul  6 20:04:55 2024 ] 	Batch(7800/7879) done. Loss: 0.2272  lr:0.000010
[ Sat Jul  6 20:05:09 2024 ] 	Mean training loss: 0.0891.
[ Sat Jul  6 20:05:09 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 20:05:09 2024 ] Eval epoch: 30
[ Sat Jul  6 20:09:56 2024 ] 	Mean val loss of 6365 batches: 1.637178204360221.
[ Sat Jul  6 20:09:56 2024 ] 
Validation: Epoch [29/120], Samples [39525.0/50919], Loss: 0.03051057830452919, Validation Accuracy: 77.62328403935662
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 1 : 201 / 275 = 73 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 2 : 224 / 273 = 82 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 3 : 225 / 273 = 82 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 4 : 222 / 275 = 80 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 5 : 238 / 275 = 86 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 6 : 228 / 275 = 82 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 7 : 253 / 273 = 92 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 8 : 263 / 273 = 96 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 9 : 185 / 273 = 67 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 10 : 122 / 273 = 44 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 11 : 122 / 272 = 44 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 12 : 219 / 271 = 80 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 13 : 260 / 275 = 94 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 14 : 268 / 276 = 97 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 15 : 227 / 273 = 83 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 16 : 228 / 274 = 83 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 17 : 233 / 273 = 85 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 18 : 241 / 274 = 87 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 19 : 255 / 272 = 93 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 20 : 252 / 273 = 92 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 21 : 229 / 274 = 83 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 22 : 230 / 274 = 83 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 23 : 258 / 276 = 93 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 24 : 222 / 274 = 81 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 25 : 261 / 275 = 94 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 26 : 271 / 276 = 98 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 27 : 221 / 275 = 80 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 28 : 173 / 275 = 62 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 29 : 139 / 275 = 50 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 30 : 173 / 276 = 62 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 31 : 237 / 276 = 85 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 32 : 243 / 276 = 88 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 33 : 235 / 276 = 85 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 34 : 242 / 276 = 87 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 35 : 240 / 275 = 87 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 36 : 234 / 276 = 84 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 37 : 247 / 276 = 89 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 38 : 251 / 276 = 90 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 39 : 253 / 276 = 91 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 40 : 192 / 276 = 69 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 41 : 260 / 276 = 94 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 42 : 261 / 275 = 94 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 43 : 185 / 276 = 67 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 44 : 240 / 276 = 86 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 45 : 251 / 276 = 90 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 46 : 223 / 276 = 80 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 47 : 219 / 275 = 79 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 48 : 228 / 275 = 82 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 49 : 223 / 274 = 81 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 50 : 239 / 276 = 86 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 51 : 251 / 276 = 90 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 52 : 237 / 276 = 85 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 53 : 226 / 276 = 81 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 54 : 267 / 274 = 97 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 55 : 237 / 276 = 85 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 56 : 238 / 275 = 86 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 57 : 267 / 276 = 96 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 58 : 267 / 273 = 97 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 59 : 265 / 276 = 96 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 60 : 471 / 561 = 83 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 61 : 466 / 566 = 82 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 62 : 458 / 572 = 80 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 63 : 522 / 570 = 91 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 64 : 436 / 574 = 75 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 65 : 500 / 573 = 87 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 66 : 385 / 573 = 67 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 67 : 425 / 575 = 73 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 68 : 330 / 575 = 57 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 69 : 459 / 575 = 79 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 70 : 242 / 575 = 42 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 71 : 213 / 575 = 37 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 72 : 146 / 571 = 25 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 73 : 222 / 570 = 38 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 74 : 377 / 569 = 66 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 75 : 239 / 573 = 41 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 76 : 356 / 574 = 62 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 77 : 406 / 573 = 70 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 78 : 415 / 575 = 72 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 79 : 544 / 574 = 94 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 80 : 499 / 573 = 87 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 81 : 321 / 575 = 55 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 82 : 377 / 575 = 65 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 83 : 309 / 572 = 54 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 84 : 443 / 574 = 77 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 85 : 419 / 574 = 72 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 86 : 513 / 575 = 89 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 87 : 502 / 576 = 87 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 88 : 426 / 575 = 74 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 89 : 477 / 576 = 82 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 90 : 242 / 574 = 42 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 91 : 466 / 568 = 82 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 92 : 382 / 576 = 66 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 93 : 415 / 573 = 72 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 94 : 500 / 574 = 87 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 95 : 519 / 575 = 90 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 96 : 561 / 575 = 97 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 97 : 556 / 574 = 96 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 98 : 541 / 575 = 94 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 99 : 544 / 574 = 94 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 100 : 494 / 574 = 86 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 101 : 522 / 574 = 90 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 102 : 339 / 575 = 58 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 103 : 483 / 576 = 83 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 104 : 292 / 575 = 50 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 105 : 252 / 575 = 43 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 106 : 323 / 576 = 56 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 107 : 479 / 576 = 83 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 108 : 486 / 575 = 84 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 109 : 332 / 575 = 57 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 110 : 487 / 575 = 84 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 111 : 535 / 576 = 92 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 112 : 545 / 575 = 94 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 113 : 513 / 576 = 89 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 114 : 489 / 576 = 84 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 115 : 525 / 576 = 91 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 116 : 475 / 575 = 82 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 117 : 480 / 575 = 83 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 118 : 490 / 575 = 85 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 119 : 527 / 576 = 91 %
[ Sat Jul  6 20:09:56 2024 ] Accuracy of 120 : 232 / 274 = 84 %
[ Sat Jul  6 20:09:56 2024 ] Training epoch: 31
[ Sat Jul  6 20:09:56 2024 ] 	Batch(0/7879) done. Loss: 0.2518  lr:0.000010
[ Sat Jul  6 20:10:14 2024 ] 	Batch(100/7879) done. Loss: 0.0325  lr:0.000010
[ Sat Jul  6 20:10:32 2024 ] 	Batch(200/7879) done. Loss: 0.0264  lr:0.000010
[ Sat Jul  6 20:10:50 2024 ] 	Batch(300/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 20:11:08 2024 ] 	Batch(400/7879) done. Loss: 0.0850  lr:0.000010
[ Sat Jul  6 20:11:26 2024 ] 
Training: Epoch [30/120], Step [499], Loss: 0.11194204539060593, Training Accuracy: 97.8
[ Sat Jul  6 20:11:26 2024 ] 	Batch(500/7879) done. Loss: 0.0291  lr:0.000010
[ Sat Jul  6 20:11:44 2024 ] 	Batch(600/7879) done. Loss: 0.0969  lr:0.000010
[ Sat Jul  6 20:12:02 2024 ] 	Batch(700/7879) done. Loss: 0.0281  lr:0.000010
[ Sat Jul  6 20:12:20 2024 ] 	Batch(800/7879) done. Loss: 0.0169  lr:0.000010
[ Sat Jul  6 20:12:38 2024 ] 	Batch(900/7879) done. Loss: 0.1162  lr:0.000010
[ Sat Jul  6 20:12:57 2024 ] 
Training: Epoch [30/120], Step [999], Loss: 0.009802273474633694, Training Accuracy: 97.85000000000001
[ Sat Jul  6 20:12:57 2024 ] 	Batch(1000/7879) done. Loss: 0.0392  lr:0.000010
[ Sat Jul  6 20:13:16 2024 ] 	Batch(1100/7879) done. Loss: 0.0515  lr:0.000010
[ Sat Jul  6 20:13:34 2024 ] 	Batch(1200/7879) done. Loss: 0.0104  lr:0.000010
[ Sat Jul  6 20:13:52 2024 ] 	Batch(1300/7879) done. Loss: 0.0052  lr:0.000010
[ Sat Jul  6 20:14:10 2024 ] 	Batch(1400/7879) done. Loss: 0.0010  lr:0.000010
[ Sat Jul  6 20:14:28 2024 ] 
Training: Epoch [30/120], Step [1499], Loss: 0.04205114766955376, Training Accuracy: 97.91666666666666
[ Sat Jul  6 20:14:28 2024 ] 	Batch(1500/7879) done. Loss: 0.2264  lr:0.000010
[ Sat Jul  6 20:14:46 2024 ] 	Batch(1600/7879) done. Loss: 0.0058  lr:0.000010
[ Sat Jul  6 20:15:04 2024 ] 	Batch(1700/7879) done. Loss: 0.1322  lr:0.000010
[ Sat Jul  6 20:15:22 2024 ] 	Batch(1800/7879) done. Loss: 0.2665  lr:0.000010
[ Sat Jul  6 20:15:40 2024 ] 	Batch(1900/7879) done. Loss: 0.2703  lr:0.000010
[ Sat Jul  6 20:15:58 2024 ] 
Training: Epoch [30/120], Step [1999], Loss: 0.02585103176534176, Training Accuracy: 97.79375
[ Sat Jul  6 20:15:58 2024 ] 	Batch(2000/7879) done. Loss: 0.1059  lr:0.000010
[ Sat Jul  6 20:16:16 2024 ] 	Batch(2100/7879) done. Loss: 0.0299  lr:0.000010
[ Sat Jul  6 20:16:35 2024 ] 	Batch(2200/7879) done. Loss: 0.0671  lr:0.000010
[ Sat Jul  6 20:16:54 2024 ] 	Batch(2300/7879) done. Loss: 0.0082  lr:0.000010
[ Sat Jul  6 20:17:12 2024 ] 	Batch(2400/7879) done. Loss: 0.0839  lr:0.000010
[ Sat Jul  6 20:17:30 2024 ] 
Training: Epoch [30/120], Step [2499], Loss: 0.2239852249622345, Training Accuracy: 97.8
[ Sat Jul  6 20:17:30 2024 ] 	Batch(2500/7879) done. Loss: 0.0612  lr:0.000010
[ Sat Jul  6 20:17:48 2024 ] 	Batch(2600/7879) done. Loss: 0.0870  lr:0.000010
[ Sat Jul  6 20:18:06 2024 ] 	Batch(2700/7879) done. Loss: 0.0970  lr:0.000010
[ Sat Jul  6 20:18:24 2024 ] 	Batch(2800/7879) done. Loss: 0.0556  lr:0.000010
[ Sat Jul  6 20:18:42 2024 ] 	Batch(2900/7879) done. Loss: 0.0176  lr:0.000010
[ Sat Jul  6 20:19:00 2024 ] 
Training: Epoch [30/120], Step [2999], Loss: 0.07003114372491837, Training Accuracy: 97.78333333333333
[ Sat Jul  6 20:19:00 2024 ] 	Batch(3000/7879) done. Loss: 0.0141  lr:0.000010
[ Sat Jul  6 20:19:18 2024 ] 	Batch(3100/7879) done. Loss: 0.0148  lr:0.000010
[ Sat Jul  6 20:19:36 2024 ] 	Batch(3200/7879) done. Loss: 0.0513  lr:0.000010
[ Sat Jul  6 20:19:54 2024 ] 	Batch(3300/7879) done. Loss: 0.1428  lr:0.000010
[ Sat Jul  6 20:20:12 2024 ] 	Batch(3400/7879) done. Loss: 0.1330  lr:0.000010
[ Sat Jul  6 20:20:29 2024 ] 
Training: Epoch [30/120], Step [3499], Loss: 0.01492516603320837, Training Accuracy: 97.75714285714285
[ Sat Jul  6 20:20:29 2024 ] 	Batch(3500/7879) done. Loss: 0.3410  lr:0.000010
[ Sat Jul  6 20:20:48 2024 ] 	Batch(3600/7879) done. Loss: 0.0083  lr:0.000010
[ Sat Jul  6 20:21:07 2024 ] 	Batch(3700/7879) done. Loss: 0.0063  lr:0.000010
[ Sat Jul  6 20:21:25 2024 ] 	Batch(3800/7879) done. Loss: 0.0019  lr:0.000010
[ Sat Jul  6 20:21:43 2024 ] 	Batch(3900/7879) done. Loss: 0.1070  lr:0.000010
[ Sat Jul  6 20:22:01 2024 ] 
Training: Epoch [30/120], Step [3999], Loss: 0.15088418126106262, Training Accuracy: 97.7625
[ Sat Jul  6 20:22:01 2024 ] 	Batch(4000/7879) done. Loss: 0.2978  lr:0.000010
[ Sat Jul  6 20:22:19 2024 ] 	Batch(4100/7879) done. Loss: 0.1567  lr:0.000010
[ Sat Jul  6 20:22:38 2024 ] 	Batch(4200/7879) done. Loss: 0.0250  lr:0.000010
[ Sat Jul  6 20:22:56 2024 ] 	Batch(4300/7879) done. Loss: 0.0206  lr:0.000010
[ Sat Jul  6 20:23:14 2024 ] 	Batch(4400/7879) done. Loss: 0.0485  lr:0.000010
[ Sat Jul  6 20:23:32 2024 ] 
Training: Epoch [30/120], Step [4499], Loss: 0.022527776658535004, Training Accuracy: 97.73888888888888
[ Sat Jul  6 20:23:33 2024 ] 	Batch(4500/7879) done. Loss: 0.0616  lr:0.000010
[ Sat Jul  6 20:23:51 2024 ] 	Batch(4600/7879) done. Loss: 0.0167  lr:0.000010
[ Sat Jul  6 20:24:10 2024 ] 	Batch(4700/7879) done. Loss: 0.0093  lr:0.000010
[ Sat Jul  6 20:24:28 2024 ] 	Batch(4800/7879) done. Loss: 0.0063  lr:0.000010
[ Sat Jul  6 20:24:46 2024 ] 	Batch(4900/7879) done. Loss: 0.0318  lr:0.000010
[ Sat Jul  6 20:25:04 2024 ] 
Training: Epoch [30/120], Step [4999], Loss: 0.046393368393182755, Training Accuracy: 97.81
[ Sat Jul  6 20:25:04 2024 ] 	Batch(5000/7879) done. Loss: 0.0009  lr:0.000010
[ Sat Jul  6 20:25:22 2024 ] 	Batch(5100/7879) done. Loss: 0.0129  lr:0.000010
[ Sat Jul  6 20:25:40 2024 ] 	Batch(5200/7879) done. Loss: 0.1803  lr:0.000010
[ Sat Jul  6 20:25:58 2024 ] 	Batch(5300/7879) done. Loss: 0.0133  lr:0.000010
[ Sat Jul  6 20:26:16 2024 ] 	Batch(5400/7879) done. Loss: 0.0478  lr:0.000010
[ Sat Jul  6 20:26:34 2024 ] 
Training: Epoch [30/120], Step [5499], Loss: 0.05348571762442589, Training Accuracy: 97.79772727272727
[ Sat Jul  6 20:26:34 2024 ] 	Batch(5500/7879) done. Loss: 0.0048  lr:0.000010
[ Sat Jul  6 20:26:52 2024 ] 	Batch(5600/7879) done. Loss: 0.0821  lr:0.000010
[ Sat Jul  6 20:27:10 2024 ] 	Batch(5700/7879) done. Loss: 0.0304  lr:0.000010
[ Sat Jul  6 20:27:28 2024 ] 	Batch(5800/7879) done. Loss: 0.6262  lr:0.000010
[ Sat Jul  6 20:27:46 2024 ] 	Batch(5900/7879) done. Loss: 0.0541  lr:0.000010
[ Sat Jul  6 20:28:04 2024 ] 
Training: Epoch [30/120], Step [5999], Loss: 0.004387753549963236, Training Accuracy: 97.82499999999999
[ Sat Jul  6 20:28:04 2024 ] 	Batch(6000/7879) done. Loss: 0.0945  lr:0.000010
[ Sat Jul  6 20:28:22 2024 ] 	Batch(6100/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 20:28:40 2024 ] 	Batch(6200/7879) done. Loss: 0.2545  lr:0.000010
[ Sat Jul  6 20:28:58 2024 ] 	Batch(6300/7879) done. Loss: 0.0539  lr:0.000010
[ Sat Jul  6 20:29:16 2024 ] 	Batch(6400/7879) done. Loss: 0.0011  lr:0.000010
[ Sat Jul  6 20:29:34 2024 ] 
Training: Epoch [30/120], Step [6499], Loss: 0.22215890884399414, Training Accuracy: 97.83461538461539
[ Sat Jul  6 20:29:34 2024 ] 	Batch(6500/7879) done. Loss: 0.0626  lr:0.000010
[ Sat Jul  6 20:29:52 2024 ] 	Batch(6600/7879) done. Loss: 0.0272  lr:0.000010
[ Sat Jul  6 20:30:10 2024 ] 	Batch(6700/7879) done. Loss: 0.2885  lr:0.000010
[ Sat Jul  6 20:30:28 2024 ] 	Batch(6800/7879) done. Loss: 0.0684  lr:0.000010
[ Sat Jul  6 20:30:46 2024 ] 	Batch(6900/7879) done. Loss: 0.0297  lr:0.000010
[ Sat Jul  6 20:31:05 2024 ] 
Training: Epoch [30/120], Step [6999], Loss: 0.016024667769670486, Training Accuracy: 97.83214285714286
[ Sat Jul  6 20:31:05 2024 ] 	Batch(7000/7879) done. Loss: 0.0182  lr:0.000010
[ Sat Jul  6 20:31:24 2024 ] 	Batch(7100/7879) done. Loss: 0.0660  lr:0.000010
[ Sat Jul  6 20:31:42 2024 ] 	Batch(7200/7879) done. Loss: 0.0115  lr:0.000010
[ Sat Jul  6 20:32:01 2024 ] 	Batch(7300/7879) done. Loss: 0.0567  lr:0.000010
[ Sat Jul  6 20:32:19 2024 ] 	Batch(7400/7879) done. Loss: 0.0324  lr:0.000010
[ Sat Jul  6 20:32:37 2024 ] 
Training: Epoch [30/120], Step [7499], Loss: 0.042627379298210144, Training Accuracy: 97.83833333333334
[ Sat Jul  6 20:32:37 2024 ] 	Batch(7500/7879) done. Loss: 0.0173  lr:0.000010
[ Sat Jul  6 20:32:55 2024 ] 	Batch(7600/7879) done. Loss: 0.2925  lr:0.000010
[ Sat Jul  6 20:33:13 2024 ] 	Batch(7700/7879) done. Loss: 0.0057  lr:0.000010
[ Sat Jul  6 20:33:31 2024 ] 	Batch(7800/7879) done. Loss: 0.0653  lr:0.000010
[ Sat Jul  6 20:33:45 2024 ] 	Mean training loss: 0.0890.
[ Sat Jul  6 20:33:45 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 20:33:45 2024 ] Training epoch: 32
[ Sat Jul  6 20:33:46 2024 ] 	Batch(0/7879) done. Loss: 0.3322  lr:0.000010
[ Sat Jul  6 20:34:04 2024 ] 	Batch(100/7879) done. Loss: 0.0448  lr:0.000010
[ Sat Jul  6 20:34:22 2024 ] 	Batch(200/7879) done. Loss: 0.0318  lr:0.000010
[ Sat Jul  6 20:34:40 2024 ] 	Batch(300/7879) done. Loss: 0.0326  lr:0.000010
[ Sat Jul  6 20:34:58 2024 ] 	Batch(400/7879) done. Loss: 0.0264  lr:0.000010
[ Sat Jul  6 20:35:16 2024 ] 
Training: Epoch [31/120], Step [499], Loss: 0.0645361840724945, Training Accuracy: 97.975
[ Sat Jul  6 20:35:16 2024 ] 	Batch(500/7879) done. Loss: 0.0134  lr:0.000010
[ Sat Jul  6 20:35:34 2024 ] 	Batch(600/7879) done. Loss: 0.0077  lr:0.000010
[ Sat Jul  6 20:35:52 2024 ] 	Batch(700/7879) done. Loss: 0.1069  lr:0.000010
[ Sat Jul  6 20:36:10 2024 ] 	Batch(800/7879) done. Loss: 0.0089  lr:0.000010
[ Sat Jul  6 20:36:28 2024 ] 	Batch(900/7879) done. Loss: 0.0189  lr:0.000010
[ Sat Jul  6 20:36:46 2024 ] 
Training: Epoch [31/120], Step [999], Loss: 0.09777805954217911, Training Accuracy: 98.0
[ Sat Jul  6 20:36:46 2024 ] 	Batch(1000/7879) done. Loss: 0.1870  lr:0.000010
[ Sat Jul  6 20:37:04 2024 ] 	Batch(1100/7879) done. Loss: 0.0917  lr:0.000010
[ Sat Jul  6 20:37:22 2024 ] 	Batch(1200/7879) done. Loss: 0.0799  lr:0.000010
[ Sat Jul  6 20:37:41 2024 ] 	Batch(1300/7879) done. Loss: 0.1753  lr:0.000010
[ Sat Jul  6 20:37:59 2024 ] 	Batch(1400/7879) done. Loss: 0.0048  lr:0.000010
[ Sat Jul  6 20:38:18 2024 ] 
Training: Epoch [31/120], Step [1499], Loss: 0.05946891009807587, Training Accuracy: 97.90833333333333
[ Sat Jul  6 20:38:18 2024 ] 	Batch(1500/7879) done. Loss: 0.0269  lr:0.000010
[ Sat Jul  6 20:38:36 2024 ] 	Batch(1600/7879) done. Loss: 0.0036  lr:0.000010
[ Sat Jul  6 20:38:55 2024 ] 	Batch(1700/7879) done. Loss: 0.0206  lr:0.000010
[ Sat Jul  6 20:39:13 2024 ] 	Batch(1800/7879) done. Loss: 0.0566  lr:0.000010
[ Sat Jul  6 20:39:31 2024 ] 	Batch(1900/7879) done. Loss: 0.0391  lr:0.000010
[ Sat Jul  6 20:39:49 2024 ] 
Training: Epoch [31/120], Step [1999], Loss: 0.1010379046201706, Training Accuracy: 97.85000000000001
[ Sat Jul  6 20:39:49 2024 ] 	Batch(2000/7879) done. Loss: 0.0495  lr:0.000010
[ Sat Jul  6 20:40:07 2024 ] 	Batch(2100/7879) done. Loss: 0.0785  lr:0.000010
[ Sat Jul  6 20:40:25 2024 ] 	Batch(2200/7879) done. Loss: 0.0691  lr:0.000010
[ Sat Jul  6 20:40:43 2024 ] 	Batch(2300/7879) done. Loss: 0.0172  lr:0.000010
[ Sat Jul  6 20:41:01 2024 ] 	Batch(2400/7879) done. Loss: 0.1395  lr:0.000010
[ Sat Jul  6 20:41:20 2024 ] 
Training: Epoch [31/120], Step [2499], Loss: 0.017037387937307358, Training Accuracy: 97.875
[ Sat Jul  6 20:41:20 2024 ] 	Batch(2500/7879) done. Loss: 0.0484  lr:0.000010
[ Sat Jul  6 20:41:39 2024 ] 	Batch(2600/7879) done. Loss: 0.0052  lr:0.000010
[ Sat Jul  6 20:41:57 2024 ] 	Batch(2700/7879) done. Loss: 0.0199  lr:0.000010
[ Sat Jul  6 20:42:16 2024 ] 	Batch(2800/7879) done. Loss: 0.0109  lr:0.000010
[ Sat Jul  6 20:42:34 2024 ] 	Batch(2900/7879) done. Loss: 0.0189  lr:0.000010
[ Sat Jul  6 20:42:52 2024 ] 
Training: Epoch [31/120], Step [2999], Loss: 0.01509751658886671, Training Accuracy: 97.89166666666667
[ Sat Jul  6 20:42:52 2024 ] 	Batch(3000/7879) done. Loss: 0.0650  lr:0.000010
[ Sat Jul  6 20:43:10 2024 ] 	Batch(3100/7879) done. Loss: 0.0056  lr:0.000010
[ Sat Jul  6 20:43:28 2024 ] 	Batch(3200/7879) done. Loss: 0.1185  lr:0.000010
[ Sat Jul  6 20:43:46 2024 ] 	Batch(3300/7879) done. Loss: 0.0321  lr:0.000010
[ Sat Jul  6 20:44:04 2024 ] 	Batch(3400/7879) done. Loss: 0.0645  lr:0.000010
[ Sat Jul  6 20:44:22 2024 ] 
Training: Epoch [31/120], Step [3499], Loss: 0.012600891292095184, Training Accuracy: 97.84285714285714
[ Sat Jul  6 20:44:22 2024 ] 	Batch(3500/7879) done. Loss: 0.1832  lr:0.000010
[ Sat Jul  6 20:44:40 2024 ] 	Batch(3600/7879) done. Loss: 0.0058  lr:0.000010
[ Sat Jul  6 20:44:58 2024 ] 	Batch(3700/7879) done. Loss: 0.0159  lr:0.000010
[ Sat Jul  6 20:45:17 2024 ] 	Batch(3800/7879) done. Loss: 0.0159  lr:0.000010
[ Sat Jul  6 20:45:35 2024 ] 	Batch(3900/7879) done. Loss: 0.1095  lr:0.000010
[ Sat Jul  6 20:45:53 2024 ] 
Training: Epoch [31/120], Step [3999], Loss: 0.04076692461967468, Training Accuracy: 97.840625
[ Sat Jul  6 20:45:53 2024 ] 	Batch(4000/7879) done. Loss: 0.2492  lr:0.000010
[ Sat Jul  6 20:46:11 2024 ] 	Batch(4100/7879) done. Loss: 0.0201  lr:0.000010
[ Sat Jul  6 20:46:30 2024 ] 	Batch(4200/7879) done. Loss: 0.0142  lr:0.000010
[ Sat Jul  6 20:46:49 2024 ] 	Batch(4300/7879) done. Loss: 0.1452  lr:0.000010
[ Sat Jul  6 20:47:07 2024 ] 	Batch(4400/7879) done. Loss: 0.0376  lr:0.000010
[ Sat Jul  6 20:47:24 2024 ] 
Training: Epoch [31/120], Step [4499], Loss: 0.029136382043361664, Training Accuracy: 97.78055555555557
[ Sat Jul  6 20:47:25 2024 ] 	Batch(4500/7879) done. Loss: 0.0313  lr:0.000010
[ Sat Jul  6 20:47:43 2024 ] 	Batch(4600/7879) done. Loss: 0.0147  lr:0.000010
[ Sat Jul  6 20:48:01 2024 ] 	Batch(4700/7879) done. Loss: 0.0638  lr:0.000010
[ Sat Jul  6 20:48:19 2024 ] 	Batch(4800/7879) done. Loss: 0.0060  lr:0.000010
[ Sat Jul  6 20:48:37 2024 ] 	Batch(4900/7879) done. Loss: 0.2812  lr:0.000010
[ Sat Jul  6 20:48:54 2024 ] 
Training: Epoch [31/120], Step [4999], Loss: 0.14099416136741638, Training Accuracy: 97.7525
[ Sat Jul  6 20:48:54 2024 ] 	Batch(5000/7879) done. Loss: 0.0716  lr:0.000010
[ Sat Jul  6 20:49:12 2024 ] 	Batch(5100/7879) done. Loss: 0.0874  lr:0.000010
[ Sat Jul  6 20:49:30 2024 ] 	Batch(5200/7879) done. Loss: 0.1196  lr:0.000010
[ Sat Jul  6 20:49:48 2024 ] 	Batch(5300/7879) done. Loss: 0.0093  lr:0.000010
[ Sat Jul  6 20:50:06 2024 ] 	Batch(5400/7879) done. Loss: 0.0346  lr:0.000010
[ Sat Jul  6 20:50:24 2024 ] 
Training: Epoch [31/120], Step [5499], Loss: 0.010266095399856567, Training Accuracy: 97.76363636363637
[ Sat Jul  6 20:50:24 2024 ] 	Batch(5500/7879) done. Loss: 0.0770  lr:0.000010
[ Sat Jul  6 20:50:42 2024 ] 	Batch(5600/7879) done. Loss: 0.0008  lr:0.000010
[ Sat Jul  6 20:51:00 2024 ] 	Batch(5700/7879) done. Loss: 0.0230  lr:0.000010
[ Sat Jul  6 20:51:18 2024 ] 	Batch(5800/7879) done. Loss: 0.1742  lr:0.000010
[ Sat Jul  6 20:51:36 2024 ] 	Batch(5900/7879) done. Loss: 0.0702  lr:0.000010
[ Sat Jul  6 20:51:54 2024 ] 
Training: Epoch [31/120], Step [5999], Loss: 0.3281484842300415, Training Accuracy: 97.75416666666666
[ Sat Jul  6 20:51:54 2024 ] 	Batch(6000/7879) done. Loss: 0.1410  lr:0.000010
[ Sat Jul  6 20:52:12 2024 ] 	Batch(6100/7879) done. Loss: 0.0053  lr:0.000010
[ Sat Jul  6 20:52:30 2024 ] 	Batch(6200/7879) done. Loss: 0.0318  lr:0.000010
[ Sat Jul  6 20:52:48 2024 ] 	Batch(6300/7879) done. Loss: 0.0021  lr:0.000010
[ Sat Jul  6 20:53:07 2024 ] 	Batch(6400/7879) done. Loss: 0.0623  lr:0.000010
[ Sat Jul  6 20:53:24 2024 ] 
Training: Epoch [31/120], Step [6499], Loss: 0.013575069606304169, Training Accuracy: 97.7673076923077
[ Sat Jul  6 20:53:25 2024 ] 	Batch(6500/7879) done. Loss: 0.4916  lr:0.000010
[ Sat Jul  6 20:53:43 2024 ] 	Batch(6600/7879) done. Loss: 0.0448  lr:0.000010
[ Sat Jul  6 20:54:00 2024 ] 	Batch(6700/7879) done. Loss: 0.0953  lr:0.000010
[ Sat Jul  6 20:54:19 2024 ] 	Batch(6800/7879) done. Loss: 0.0322  lr:0.000010
[ Sat Jul  6 20:54:37 2024 ] 	Batch(6900/7879) done. Loss: 0.2360  lr:0.000010
[ Sat Jul  6 20:54:56 2024 ] 
Training: Epoch [31/120], Step [6999], Loss: 0.012401753105223179, Training Accuracy: 97.75535714285715
[ Sat Jul  6 20:54:56 2024 ] 	Batch(7000/7879) done. Loss: 0.2839  lr:0.000010
[ Sat Jul  6 20:55:14 2024 ] 	Batch(7100/7879) done. Loss: 0.0302  lr:0.000010
[ Sat Jul  6 20:55:33 2024 ] 	Batch(7200/7879) done. Loss: 0.0434  lr:0.000010
[ Sat Jul  6 20:55:52 2024 ] 	Batch(7300/7879) done. Loss: 0.0057  lr:0.000010
[ Sat Jul  6 20:56:10 2024 ] 	Batch(7400/7879) done. Loss: 0.2721  lr:0.000010
[ Sat Jul  6 20:56:29 2024 ] 
Training: Epoch [31/120], Step [7499], Loss: 0.30905628204345703, Training Accuracy: 97.76833333333333
[ Sat Jul  6 20:56:29 2024 ] 	Batch(7500/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 20:56:48 2024 ] 	Batch(7600/7879) done. Loss: 0.0513  lr:0.000010
[ Sat Jul  6 20:57:06 2024 ] 	Batch(7700/7879) done. Loss: 0.0209  lr:0.000010
[ Sat Jul  6 20:57:25 2024 ] 	Batch(7800/7879) done. Loss: 0.0137  lr:0.000010
[ Sat Jul  6 20:57:39 2024 ] 	Mean training loss: 0.0887.
[ Sat Jul  6 20:57:39 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 20:57:39 2024 ] Training epoch: 33
[ Sat Jul  6 20:57:40 2024 ] 	Batch(0/7879) done. Loss: 0.0153  lr:0.000010
[ Sat Jul  6 20:57:58 2024 ] 	Batch(100/7879) done. Loss: 0.0355  lr:0.000010
[ Sat Jul  6 20:58:16 2024 ] 	Batch(200/7879) done. Loss: 0.0243  lr:0.000010
[ Sat Jul  6 20:58:34 2024 ] 	Batch(300/7879) done. Loss: 0.0402  lr:0.000010
[ Sat Jul  6 20:58:52 2024 ] 	Batch(400/7879) done. Loss: 0.1085  lr:0.000010
[ Sat Jul  6 20:59:10 2024 ] 
Training: Epoch [32/120], Step [499], Loss: 0.2131350040435791, Training Accuracy: 98.175
[ Sat Jul  6 20:59:10 2024 ] 	Batch(500/7879) done. Loss: 0.1049  lr:0.000010
[ Sat Jul  6 20:59:28 2024 ] 	Batch(600/7879) done. Loss: 0.0101  lr:0.000010
[ Sat Jul  6 20:59:46 2024 ] 	Batch(700/7879) done. Loss: 0.0479  lr:0.000010
[ Sat Jul  6 21:00:04 2024 ] 	Batch(800/7879) done. Loss: 0.0194  lr:0.000010
[ Sat Jul  6 21:00:22 2024 ] 	Batch(900/7879) done. Loss: 0.0106  lr:0.000010
[ Sat Jul  6 21:00:40 2024 ] 
Training: Epoch [32/120], Step [999], Loss: 0.01793971098959446, Training Accuracy: 97.89999999999999
[ Sat Jul  6 21:00:40 2024 ] 	Batch(1000/7879) done. Loss: 0.0699  lr:0.000010
[ Sat Jul  6 21:00:58 2024 ] 	Batch(1100/7879) done. Loss: 0.3162  lr:0.000010
[ Sat Jul  6 21:01:16 2024 ] 	Batch(1200/7879) done. Loss: 0.0914  lr:0.000010
[ Sat Jul  6 21:01:34 2024 ] 	Batch(1300/7879) done. Loss: 0.0493  lr:0.000010
[ Sat Jul  6 21:01:52 2024 ] 	Batch(1400/7879) done. Loss: 0.1767  lr:0.000010
[ Sat Jul  6 21:02:10 2024 ] 
Training: Epoch [32/120], Step [1499], Loss: 0.11667438596487045, Training Accuracy: 97.89166666666667
[ Sat Jul  6 21:02:10 2024 ] 	Batch(1500/7879) done. Loss: 0.0700  lr:0.000010
[ Sat Jul  6 21:02:28 2024 ] 	Batch(1600/7879) done. Loss: 0.0468  lr:0.000010
[ Sat Jul  6 21:02:46 2024 ] 	Batch(1700/7879) done. Loss: 0.0680  lr:0.000010
[ Sat Jul  6 21:03:04 2024 ] 	Batch(1800/7879) done. Loss: 0.1007  lr:0.000010
[ Sat Jul  6 21:03:22 2024 ] 	Batch(1900/7879) done. Loss: 0.0258  lr:0.000010
[ Sat Jul  6 21:03:40 2024 ] 
Training: Epoch [32/120], Step [1999], Loss: 0.02165699563920498, Training Accuracy: 97.85000000000001
[ Sat Jul  6 21:03:40 2024 ] 	Batch(2000/7879) done. Loss: 0.0440  lr:0.000010
[ Sat Jul  6 21:03:58 2024 ] 	Batch(2100/7879) done. Loss: 0.0350  lr:0.000010
[ Sat Jul  6 21:04:16 2024 ] 	Batch(2200/7879) done. Loss: 0.0994  lr:0.000010
[ Sat Jul  6 21:04:34 2024 ] 	Batch(2300/7879) done. Loss: 0.0601  lr:0.000010
[ Sat Jul  6 21:04:52 2024 ] 	Batch(2400/7879) done. Loss: 0.0141  lr:0.000010
[ Sat Jul  6 21:05:10 2024 ] 
Training: Epoch [32/120], Step [2499], Loss: 0.24351967871189117, Training Accuracy: 97.875
[ Sat Jul  6 21:05:10 2024 ] 	Batch(2500/7879) done. Loss: 0.0057  lr:0.000010
[ Sat Jul  6 21:05:29 2024 ] 	Batch(2600/7879) done. Loss: 0.2069  lr:0.000010
[ Sat Jul  6 21:05:48 2024 ] 	Batch(2700/7879) done. Loss: 0.0386  lr:0.000010
[ Sat Jul  6 21:06:06 2024 ] 	Batch(2800/7879) done. Loss: 0.0383  lr:0.000010
[ Sat Jul  6 21:06:24 2024 ] 	Batch(2900/7879) done. Loss: 0.0136  lr:0.000010
[ Sat Jul  6 21:06:42 2024 ] 
Training: Epoch [32/120], Step [2999], Loss: 0.0007168503361754119, Training Accuracy: 97.89166666666667
[ Sat Jul  6 21:06:42 2024 ] 	Batch(3000/7879) done. Loss: 0.0262  lr:0.000010
[ Sat Jul  6 21:07:00 2024 ] 	Batch(3100/7879) done. Loss: 0.2834  lr:0.000010
[ Sat Jul  6 21:07:18 2024 ] 	Batch(3200/7879) done. Loss: 0.0192  lr:0.000010
[ Sat Jul  6 21:07:36 2024 ] 	Batch(3300/7879) done. Loss: 0.2217  lr:0.000010
[ Sat Jul  6 21:07:54 2024 ] 	Batch(3400/7879) done. Loss: 0.2892  lr:0.000010
[ Sat Jul  6 21:08:12 2024 ] 
Training: Epoch [32/120], Step [3499], Loss: 0.07540500164031982, Training Accuracy: 97.8892857142857
[ Sat Jul  6 21:08:12 2024 ] 	Batch(3500/7879) done. Loss: 0.0534  lr:0.000010
[ Sat Jul  6 21:08:30 2024 ] 	Batch(3600/7879) done. Loss: 0.0804  lr:0.000010
[ Sat Jul  6 21:08:48 2024 ] 	Batch(3700/7879) done. Loss: 0.0083  lr:0.000010
[ Sat Jul  6 21:09:06 2024 ] 	Batch(3800/7879) done. Loss: 0.0425  lr:0.000010
[ Sat Jul  6 21:09:24 2024 ] 	Batch(3900/7879) done. Loss: 0.0304  lr:0.000010
[ Sat Jul  6 21:09:42 2024 ] 
Training: Epoch [32/120], Step [3999], Loss: 0.25311926007270813, Training Accuracy: 97.875
[ Sat Jul  6 21:09:42 2024 ] 	Batch(4000/7879) done. Loss: 0.0956  lr:0.000010
[ Sat Jul  6 21:10:00 2024 ] 	Batch(4100/7879) done. Loss: 0.0077  lr:0.000010
[ Sat Jul  6 21:10:18 2024 ] 	Batch(4200/7879) done. Loss: 0.0604  lr:0.000010
[ Sat Jul  6 21:10:36 2024 ] 	Batch(4300/7879) done. Loss: 0.0268  lr:0.000010
[ Sat Jul  6 21:10:54 2024 ] 	Batch(4400/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 21:11:12 2024 ] 
Training: Epoch [32/120], Step [4499], Loss: 0.07108750939369202, Training Accuracy: 97.875
[ Sat Jul  6 21:11:12 2024 ] 	Batch(4500/7879) done. Loss: 0.0378  lr:0.000010
[ Sat Jul  6 21:11:30 2024 ] 	Batch(4600/7879) done. Loss: 0.0141  lr:0.000010
[ Sat Jul  6 21:11:49 2024 ] 	Batch(4700/7879) done. Loss: 0.1767  lr:0.000010
[ Sat Jul  6 21:12:07 2024 ] 	Batch(4800/7879) done. Loss: 0.0188  lr:0.000010
[ Sat Jul  6 21:12:25 2024 ] 	Batch(4900/7879) done. Loss: 0.5007  lr:0.000010
[ Sat Jul  6 21:12:43 2024 ] 
Training: Epoch [32/120], Step [4999], Loss: 0.00706580700352788, Training Accuracy: 97.8775
[ Sat Jul  6 21:12:43 2024 ] 	Batch(5000/7879) done. Loss: 0.2474  lr:0.000010
[ Sat Jul  6 21:13:01 2024 ] 	Batch(5100/7879) done. Loss: 0.0258  lr:0.000010
[ Sat Jul  6 21:13:19 2024 ] 	Batch(5200/7879) done. Loss: 0.1340  lr:0.000010
[ Sat Jul  6 21:13:37 2024 ] 	Batch(5300/7879) done. Loss: 0.1587  lr:0.000010
[ Sat Jul  6 21:13:55 2024 ] 	Batch(5400/7879) done. Loss: 0.0229  lr:0.000010
[ Sat Jul  6 21:14:13 2024 ] 
Training: Epoch [32/120], Step [5499], Loss: 0.25944602489471436, Training Accuracy: 97.87954545454546
[ Sat Jul  6 21:14:13 2024 ] 	Batch(5500/7879) done. Loss: 0.0130  lr:0.000010
[ Sat Jul  6 21:14:31 2024 ] 	Batch(5600/7879) done. Loss: 0.2472  lr:0.000010
[ Sat Jul  6 21:14:49 2024 ] 	Batch(5700/7879) done. Loss: 0.1514  lr:0.000010
[ Sat Jul  6 21:15:07 2024 ] 	Batch(5800/7879) done. Loss: 0.1238  lr:0.000010
[ Sat Jul  6 21:15:25 2024 ] 	Batch(5900/7879) done. Loss: 0.1097  lr:0.000010
[ Sat Jul  6 21:15:43 2024 ] 
Training: Epoch [32/120], Step [5999], Loss: 0.1574552059173584, Training Accuracy: 97.88333333333334
[ Sat Jul  6 21:15:43 2024 ] 	Batch(6000/7879) done. Loss: 0.0348  lr:0.000010
[ Sat Jul  6 21:16:01 2024 ] 	Batch(6100/7879) done. Loss: 0.0549  lr:0.000010
[ Sat Jul  6 21:16:19 2024 ] 	Batch(6200/7879) done. Loss: 0.0368  lr:0.000010
[ Sat Jul  6 21:16:37 2024 ] 	Batch(6300/7879) done. Loss: 0.0029  lr:0.000010
[ Sat Jul  6 21:16:55 2024 ] 	Batch(6400/7879) done. Loss: 0.0126  lr:0.000010
[ Sat Jul  6 21:17:13 2024 ] 
Training: Epoch [32/120], Step [6499], Loss: 0.0029281422030180693, Training Accuracy: 97.86730769230769
[ Sat Jul  6 21:17:14 2024 ] 	Batch(6500/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 21:17:32 2024 ] 	Batch(6600/7879) done. Loss: 0.1904  lr:0.000010
[ Sat Jul  6 21:17:50 2024 ] 	Batch(6700/7879) done. Loss: 0.0588  lr:0.000010
[ Sat Jul  6 21:18:09 2024 ] 	Batch(6800/7879) done. Loss: 0.0266  lr:0.000010
[ Sat Jul  6 21:18:27 2024 ] 	Batch(6900/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 21:18:45 2024 ] 
Training: Epoch [32/120], Step [6999], Loss: 0.06289777159690857, Training Accuracy: 97.85714285714285
[ Sat Jul  6 21:18:46 2024 ] 	Batch(7000/7879) done. Loss: 0.4487  lr:0.000010
[ Sat Jul  6 21:19:04 2024 ] 	Batch(7100/7879) done. Loss: 0.0210  lr:0.000010
[ Sat Jul  6 21:19:22 2024 ] 	Batch(7200/7879) done. Loss: 0.0986  lr:0.000010
[ Sat Jul  6 21:19:40 2024 ] 	Batch(7300/7879) done. Loss: 0.0547  lr:0.000010
[ Sat Jul  6 21:19:58 2024 ] 	Batch(7400/7879) done. Loss: 0.0966  lr:0.000010
[ Sat Jul  6 21:20:16 2024 ] 
Training: Epoch [32/120], Step [7499], Loss: 0.08390332758426666, Training Accuracy: 97.87
[ Sat Jul  6 21:20:16 2024 ] 	Batch(7500/7879) done. Loss: 0.1257  lr:0.000010
[ Sat Jul  6 21:20:34 2024 ] 	Batch(7600/7879) done. Loss: 0.1353  lr:0.000010
[ Sat Jul  6 21:20:52 2024 ] 	Batch(7700/7879) done. Loss: 0.2313  lr:0.000010
[ Sat Jul  6 21:21:10 2024 ] 	Batch(7800/7879) done. Loss: 0.0165  lr:0.000010
[ Sat Jul  6 21:21:24 2024 ] 	Mean training loss: 0.0857.
[ Sat Jul  6 21:21:24 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 21:21:24 2024 ] Training epoch: 34
[ Sat Jul  6 21:21:25 2024 ] 	Batch(0/7879) done. Loss: 0.2923  lr:0.000010
[ Sat Jul  6 21:21:43 2024 ] 	Batch(100/7879) done. Loss: 0.0168  lr:0.000010
[ Sat Jul  6 21:22:01 2024 ] 	Batch(200/7879) done. Loss: 0.0046  lr:0.000010
[ Sat Jul  6 21:22:20 2024 ] 	Batch(300/7879) done. Loss: 0.1272  lr:0.000010
[ Sat Jul  6 21:22:38 2024 ] 	Batch(400/7879) done. Loss: 0.2736  lr:0.000010
[ Sat Jul  6 21:22:56 2024 ] 
Training: Epoch [33/120], Step [499], Loss: 0.030893700197339058, Training Accuracy: 98.05
[ Sat Jul  6 21:22:56 2024 ] 	Batch(500/7879) done. Loss: 0.0279  lr:0.000010
[ Sat Jul  6 21:23:14 2024 ] 	Batch(600/7879) done. Loss: 0.0155  lr:0.000010
[ Sat Jul  6 21:23:32 2024 ] 	Batch(700/7879) done. Loss: 0.4053  lr:0.000010
[ Sat Jul  6 21:23:50 2024 ] 	Batch(800/7879) done. Loss: 0.0047  lr:0.000010
[ Sat Jul  6 21:24:08 2024 ] 	Batch(900/7879) done. Loss: 0.2569  lr:0.000010
[ Sat Jul  6 21:24:26 2024 ] 
Training: Epoch [33/120], Step [999], Loss: 0.28972795605659485, Training Accuracy: 97.8125
[ Sat Jul  6 21:24:26 2024 ] 	Batch(1000/7879) done. Loss: 0.0333  lr:0.000010
[ Sat Jul  6 21:24:44 2024 ] 	Batch(1100/7879) done. Loss: 0.0734  lr:0.000010
[ Sat Jul  6 21:25:02 2024 ] 	Batch(1200/7879) done. Loss: 0.0241  lr:0.000010
[ Sat Jul  6 21:25:21 2024 ] 	Batch(1300/7879) done. Loss: 0.0194  lr:0.000010
[ Sat Jul  6 21:25:39 2024 ] 	Batch(1400/7879) done. Loss: 0.0051  lr:0.000010
[ Sat Jul  6 21:25:58 2024 ] 
Training: Epoch [33/120], Step [1499], Loss: 0.021320577710866928, Training Accuracy: 97.74166666666667
[ Sat Jul  6 21:25:58 2024 ] 	Batch(1500/7879) done. Loss: 0.1697  lr:0.000010
[ Sat Jul  6 21:26:17 2024 ] 	Batch(1600/7879) done. Loss: 0.0610  lr:0.000010
[ Sat Jul  6 21:26:34 2024 ] 	Batch(1700/7879) done. Loss: 0.0618  lr:0.000010
[ Sat Jul  6 21:26:53 2024 ] 	Batch(1800/7879) done. Loss: 0.0094  lr:0.000010
[ Sat Jul  6 21:27:11 2024 ] 	Batch(1900/7879) done. Loss: 0.0189  lr:0.000010
[ Sat Jul  6 21:27:28 2024 ] 
Training: Epoch [33/120], Step [1999], Loss: 0.01632688194513321, Training Accuracy: 97.86875
[ Sat Jul  6 21:27:29 2024 ] 	Batch(2000/7879) done. Loss: 0.0866  lr:0.000010
[ Sat Jul  6 21:27:47 2024 ] 	Batch(2100/7879) done. Loss: 0.0813  lr:0.000010
[ Sat Jul  6 21:28:06 2024 ] 	Batch(2200/7879) done. Loss: 0.2379  lr:0.000010
[ Sat Jul  6 21:28:24 2024 ] 	Batch(2300/7879) done. Loss: 0.0811  lr:0.000010
[ Sat Jul  6 21:28:43 2024 ] 	Batch(2400/7879) done. Loss: 0.0124  lr:0.000010
[ Sat Jul  6 21:29:02 2024 ] 
Training: Epoch [33/120], Step [2499], Loss: 0.3455030620098114, Training Accuracy: 97.82
[ Sat Jul  6 21:29:02 2024 ] 	Batch(2500/7879) done. Loss: 0.0486  lr:0.000010
[ Sat Jul  6 21:29:20 2024 ] 	Batch(2600/7879) done. Loss: 0.0331  lr:0.000010
[ Sat Jul  6 21:29:39 2024 ] 	Batch(2700/7879) done. Loss: 0.0255  lr:0.000010
[ Sat Jul  6 21:29:58 2024 ] 	Batch(2800/7879) done. Loss: 0.1457  lr:0.000010
[ Sat Jul  6 21:30:16 2024 ] 	Batch(2900/7879) done. Loss: 0.0411  lr:0.000010
[ Sat Jul  6 21:30:34 2024 ] 
Training: Epoch [33/120], Step [2999], Loss: 0.15307629108428955, Training Accuracy: 97.81666666666666
[ Sat Jul  6 21:30:34 2024 ] 	Batch(3000/7879) done. Loss: 0.2084  lr:0.000010
[ Sat Jul  6 21:30:52 2024 ] 	Batch(3100/7879) done. Loss: 0.2213  lr:0.000010
[ Sat Jul  6 21:31:10 2024 ] 	Batch(3200/7879) done. Loss: 0.0517  lr:0.000010
[ Sat Jul  6 21:31:28 2024 ] 	Batch(3300/7879) done. Loss: 0.0261  lr:0.000010
[ Sat Jul  6 21:31:45 2024 ] 	Batch(3400/7879) done. Loss: 0.1151  lr:0.000010
[ Sat Jul  6 21:32:03 2024 ] 
Training: Epoch [33/120], Step [3499], Loss: 0.0645805299282074, Training Accuracy: 97.84642857142856
[ Sat Jul  6 21:32:03 2024 ] 	Batch(3500/7879) done. Loss: 0.0602  lr:0.000010
[ Sat Jul  6 21:32:22 2024 ] 	Batch(3600/7879) done. Loss: 0.0153  lr:0.000010
[ Sat Jul  6 21:32:39 2024 ] 	Batch(3700/7879) done. Loss: 0.1736  lr:0.000010
[ Sat Jul  6 21:32:57 2024 ] 	Batch(3800/7879) done. Loss: 0.0640  lr:0.000010
[ Sat Jul  6 21:33:15 2024 ] 	Batch(3900/7879) done. Loss: 0.0561  lr:0.000010
[ Sat Jul  6 21:33:33 2024 ] 
Training: Epoch [33/120], Step [3999], Loss: 0.34940916299819946, Training Accuracy: 97.84375
[ Sat Jul  6 21:33:33 2024 ] 	Batch(4000/7879) done. Loss: 0.1174  lr:0.000010
[ Sat Jul  6 21:33:51 2024 ] 	Batch(4100/7879) done. Loss: 0.1771  lr:0.000010
[ Sat Jul  6 21:34:09 2024 ] 	Batch(4200/7879) done. Loss: 0.0672  lr:0.000010
[ Sat Jul  6 21:34:27 2024 ] 	Batch(4300/7879) done. Loss: 0.0030  lr:0.000010
[ Sat Jul  6 21:34:45 2024 ] 	Batch(4400/7879) done. Loss: 0.0019  lr:0.000010
[ Sat Jul  6 21:35:03 2024 ] 
Training: Epoch [33/120], Step [4499], Loss: 0.00021975465642753989, Training Accuracy: 97.84444444444445
[ Sat Jul  6 21:35:03 2024 ] 	Batch(4500/7879) done. Loss: 0.0080  lr:0.000010
[ Sat Jul  6 21:35:21 2024 ] 	Batch(4600/7879) done. Loss: 0.0369  lr:0.000010
[ Sat Jul  6 21:35:39 2024 ] 	Batch(4700/7879) done. Loss: 0.0214  lr:0.000010
[ Sat Jul  6 21:35:57 2024 ] 	Batch(4800/7879) done. Loss: 0.0244  lr:0.000010
[ Sat Jul  6 21:36:15 2024 ] 	Batch(4900/7879) done. Loss: 0.1508  lr:0.000010
[ Sat Jul  6 21:36:33 2024 ] 
Training: Epoch [33/120], Step [4999], Loss: 0.19455629587173462, Training Accuracy: 97.8475
[ Sat Jul  6 21:36:33 2024 ] 	Batch(5000/7879) done. Loss: 0.0757  lr:0.000010
[ Sat Jul  6 21:36:51 2024 ] 	Batch(5100/7879) done. Loss: 0.0046  lr:0.000010
[ Sat Jul  6 21:37:09 2024 ] 	Batch(5200/7879) done. Loss: 0.0791  lr:0.000010
[ Sat Jul  6 21:37:28 2024 ] 	Batch(5300/7879) done. Loss: 0.0651  lr:0.000010
[ Sat Jul  6 21:37:46 2024 ] 	Batch(5400/7879) done. Loss: 0.1020  lr:0.000010
[ Sat Jul  6 21:38:05 2024 ] 
Training: Epoch [33/120], Step [5499], Loss: 0.02471006102859974, Training Accuracy: 97.85454545454544
[ Sat Jul  6 21:38:05 2024 ] 	Batch(5500/7879) done. Loss: 0.0036  lr:0.000010
[ Sat Jul  6 21:38:24 2024 ] 	Batch(5600/7879) done. Loss: 0.0342  lr:0.000010
[ Sat Jul  6 21:38:42 2024 ] 	Batch(5700/7879) done. Loss: 0.0197  lr:0.000010
[ Sat Jul  6 21:39:00 2024 ] 	Batch(5800/7879) done. Loss: 0.0951  lr:0.000010
[ Sat Jul  6 21:39:18 2024 ] 	Batch(5900/7879) done. Loss: 0.0912  lr:0.000010
[ Sat Jul  6 21:39:35 2024 ] 
Training: Epoch [33/120], Step [5999], Loss: 0.021210649982094765, Training Accuracy: 97.87083333333332
[ Sat Jul  6 21:39:36 2024 ] 	Batch(6000/7879) done. Loss: 0.0560  lr:0.000010
[ Sat Jul  6 21:39:54 2024 ] 	Batch(6100/7879) done. Loss: 0.4183  lr:0.000010
[ Sat Jul  6 21:40:12 2024 ] 	Batch(6200/7879) done. Loss: 0.0667  lr:0.000010
[ Sat Jul  6 21:40:30 2024 ] 	Batch(6300/7879) done. Loss: 0.0417  lr:0.000010
[ Sat Jul  6 21:40:48 2024 ] 	Batch(6400/7879) done. Loss: 0.0421  lr:0.000010
[ Sat Jul  6 21:41:06 2024 ] 
Training: Epoch [33/120], Step [6499], Loss: 0.005684434901922941, Training Accuracy: 97.86538461538463
[ Sat Jul  6 21:41:07 2024 ] 	Batch(6500/7879) done. Loss: 0.0050  lr:0.000010
[ Sat Jul  6 21:41:25 2024 ] 	Batch(6600/7879) done. Loss: 0.1803  lr:0.000010
[ Sat Jul  6 21:41:43 2024 ] 	Batch(6700/7879) done. Loss: 0.0095  lr:0.000010
[ Sat Jul  6 21:42:02 2024 ] 	Batch(6800/7879) done. Loss: 0.0445  lr:0.000010
[ Sat Jul  6 21:42:20 2024 ] 	Batch(6900/7879) done. Loss: 0.2083  lr:0.000010
[ Sat Jul  6 21:42:37 2024 ] 
Training: Epoch [33/120], Step [6999], Loss: 0.003990198951214552, Training Accuracy: 97.87142857142858
[ Sat Jul  6 21:42:38 2024 ] 	Batch(7000/7879) done. Loss: 0.0685  lr:0.000010
[ Sat Jul  6 21:42:56 2024 ] 	Batch(7100/7879) done. Loss: 0.0079  lr:0.000010
[ Sat Jul  6 21:43:14 2024 ] 	Batch(7200/7879) done. Loss: 0.0288  lr:0.000010
[ Sat Jul  6 21:43:32 2024 ] 	Batch(7300/7879) done. Loss: 0.2133  lr:0.000010
[ Sat Jul  6 21:43:50 2024 ] 	Batch(7400/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 21:44:07 2024 ] 
Training: Epoch [33/120], Step [7499], Loss: 0.07588811218738556, Training Accuracy: 97.90833333333333
[ Sat Jul  6 21:44:07 2024 ] 	Batch(7500/7879) done. Loss: 0.0139  lr:0.000010
[ Sat Jul  6 21:44:26 2024 ] 	Batch(7600/7879) done. Loss: 0.0435  lr:0.000010
[ Sat Jul  6 21:44:44 2024 ] 	Batch(7700/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 21:45:02 2024 ] 	Batch(7800/7879) done. Loss: 0.1247  lr:0.000010
[ Sat Jul  6 21:45:16 2024 ] 	Mean training loss: 0.0873.
[ Sat Jul  6 21:45:16 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 21:45:16 2024 ] Training epoch: 35
[ Sat Jul  6 21:45:16 2024 ] 	Batch(0/7879) done. Loss: 0.5054  lr:0.000010
[ Sat Jul  6 21:45:35 2024 ] 	Batch(100/7879) done. Loss: 0.0118  lr:0.000010
[ Sat Jul  6 21:45:53 2024 ] 	Batch(200/7879) done. Loss: 0.0464  lr:0.000010
[ Sat Jul  6 21:46:11 2024 ] 	Batch(300/7879) done. Loss: 0.2021  lr:0.000010
[ Sat Jul  6 21:46:29 2024 ] 	Batch(400/7879) done. Loss: 0.0585  lr:0.000010
[ Sat Jul  6 21:46:47 2024 ] 
Training: Epoch [34/120], Step [499], Loss: 0.06251920014619827, Training Accuracy: 97.6
[ Sat Jul  6 21:46:47 2024 ] 	Batch(500/7879) done. Loss: 0.0355  lr:0.000010
[ Sat Jul  6 21:47:05 2024 ] 	Batch(600/7879) done. Loss: 0.0080  lr:0.000010
[ Sat Jul  6 21:47:23 2024 ] 	Batch(700/7879) done. Loss: 0.0367  lr:0.000010
[ Sat Jul  6 21:47:41 2024 ] 	Batch(800/7879) done. Loss: 0.1312  lr:0.000010
[ Sat Jul  6 21:47:59 2024 ] 	Batch(900/7879) done. Loss: 0.0076  lr:0.000010
[ Sat Jul  6 21:48:17 2024 ] 
Training: Epoch [34/120], Step [999], Loss: 0.14757193624973297, Training Accuracy: 97.8
[ Sat Jul  6 21:48:17 2024 ] 	Batch(1000/7879) done. Loss: 0.0783  lr:0.000010
[ Sat Jul  6 21:48:35 2024 ] 	Batch(1100/7879) done. Loss: 0.0058  lr:0.000010
[ Sat Jul  6 21:48:53 2024 ] 	Batch(1200/7879) done. Loss: 0.4935  lr:0.000010
[ Sat Jul  6 21:49:11 2024 ] 	Batch(1300/7879) done. Loss: 0.2367  lr:0.000010
[ Sat Jul  6 21:49:29 2024 ] 	Batch(1400/7879) done. Loss: 0.0659  lr:0.000010
[ Sat Jul  6 21:49:47 2024 ] 
Training: Epoch [34/120], Step [1499], Loss: 0.019695866852998734, Training Accuracy: 97.75833333333334
[ Sat Jul  6 21:49:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0528  lr:0.000010
[ Sat Jul  6 21:50:05 2024 ] 	Batch(1600/7879) done. Loss: 0.0190  lr:0.000010
[ Sat Jul  6 21:50:23 2024 ] 	Batch(1700/7879) done. Loss: 0.0896  lr:0.000010
[ Sat Jul  6 21:50:41 2024 ] 	Batch(1800/7879) done. Loss: 0.0943  lr:0.000010
[ Sat Jul  6 21:50:59 2024 ] 	Batch(1900/7879) done. Loss: 0.0059  lr:0.000010
[ Sat Jul  6 21:51:17 2024 ] 
Training: Epoch [34/120], Step [1999], Loss: 0.022980190813541412, Training Accuracy: 97.71875
[ Sat Jul  6 21:51:17 2024 ] 	Batch(2000/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 21:51:35 2024 ] 	Batch(2100/7879) done. Loss: 0.0042  lr:0.000010
[ Sat Jul  6 21:51:53 2024 ] 	Batch(2200/7879) done. Loss: 0.0028  lr:0.000010
[ Sat Jul  6 21:52:11 2024 ] 	Batch(2300/7879) done. Loss: 0.0516  lr:0.000010
[ Sat Jul  6 21:52:29 2024 ] 	Batch(2400/7879) done. Loss: 0.1015  lr:0.000010
[ Sat Jul  6 21:52:47 2024 ] 
Training: Epoch [34/120], Step [2499], Loss: 0.0010000840993598104, Training Accuracy: 97.685
[ Sat Jul  6 21:52:47 2024 ] 	Batch(2500/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 21:53:05 2024 ] 	Batch(2600/7879) done. Loss: 0.0212  lr:0.000010
[ Sat Jul  6 21:53:23 2024 ] 	Batch(2700/7879) done. Loss: 0.2440  lr:0.000010
[ Sat Jul  6 21:53:41 2024 ] 	Batch(2800/7879) done. Loss: 0.0151  lr:0.000010
[ Sat Jul  6 21:53:59 2024 ] 	Batch(2900/7879) done. Loss: 0.0039  lr:0.000010
[ Sat Jul  6 21:54:17 2024 ] 
Training: Epoch [34/120], Step [2999], Loss: 0.02141425386071205, Training Accuracy: 97.72916666666667
[ Sat Jul  6 21:54:17 2024 ] 	Batch(3000/7879) done. Loss: 0.0188  lr:0.000010
[ Sat Jul  6 21:54:35 2024 ] 	Batch(3100/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 21:54:53 2024 ] 	Batch(3200/7879) done. Loss: 0.0251  lr:0.000010
[ Sat Jul  6 21:55:11 2024 ] 	Batch(3300/7879) done. Loss: 0.0585  lr:0.000010
[ Sat Jul  6 21:55:28 2024 ] 	Batch(3400/7879) done. Loss: 0.1670  lr:0.000010
[ Sat Jul  6 21:55:46 2024 ] 
Training: Epoch [34/120], Step [3499], Loss: 0.029571615159511566, Training Accuracy: 97.75
[ Sat Jul  6 21:55:46 2024 ] 	Batch(3500/7879) done. Loss: 0.0172  lr:0.000010
[ Sat Jul  6 21:56:04 2024 ] 	Batch(3600/7879) done. Loss: 0.0527  lr:0.000010
[ Sat Jul  6 21:56:22 2024 ] 	Batch(3700/7879) done. Loss: 0.0136  lr:0.000010
[ Sat Jul  6 21:56:40 2024 ] 	Batch(3800/7879) done. Loss: 0.0401  lr:0.000010
[ Sat Jul  6 21:56:58 2024 ] 	Batch(3900/7879) done. Loss: 0.1498  lr:0.000010
[ Sat Jul  6 21:57:16 2024 ] 
Training: Epoch [34/120], Step [3999], Loss: 0.026432745158672333, Training Accuracy: 97.76875000000001
[ Sat Jul  6 21:57:16 2024 ] 	Batch(4000/7879) done. Loss: 0.0191  lr:0.000010
[ Sat Jul  6 21:57:35 2024 ] 	Batch(4100/7879) done. Loss: 0.0263  lr:0.000010
[ Sat Jul  6 21:57:53 2024 ] 	Batch(4200/7879) done. Loss: 0.1491  lr:0.000010
[ Sat Jul  6 21:58:11 2024 ] 	Batch(4300/7879) done. Loss: 0.0308  lr:0.000010
[ Sat Jul  6 21:58:30 2024 ] 	Batch(4400/7879) done. Loss: 0.0673  lr:0.000010
[ Sat Jul  6 21:58:48 2024 ] 
Training: Epoch [34/120], Step [4499], Loss: 0.009844498708844185, Training Accuracy: 97.78888888888889
[ Sat Jul  6 21:58:48 2024 ] 	Batch(4500/7879) done. Loss: 0.0619  lr:0.000010
[ Sat Jul  6 21:59:06 2024 ] 	Batch(4600/7879) done. Loss: 0.0287  lr:0.000010
[ Sat Jul  6 21:59:24 2024 ] 	Batch(4700/7879) done. Loss: 0.4902  lr:0.000010
[ Sat Jul  6 21:59:42 2024 ] 	Batch(4800/7879) done. Loss: 0.1943  lr:0.000010
[ Sat Jul  6 22:00:00 2024 ] 	Batch(4900/7879) done. Loss: 0.0214  lr:0.000010
[ Sat Jul  6 22:00:18 2024 ] 
Training: Epoch [34/120], Step [4999], Loss: 0.5179853439331055, Training Accuracy: 97.8075
[ Sat Jul  6 22:00:18 2024 ] 	Batch(5000/7879) done. Loss: 0.0797  lr:0.000010
[ Sat Jul  6 22:00:36 2024 ] 	Batch(5100/7879) done. Loss: 0.4474  lr:0.000010
[ Sat Jul  6 22:00:54 2024 ] 	Batch(5200/7879) done. Loss: 0.2933  lr:0.000010
[ Sat Jul  6 22:01:13 2024 ] 	Batch(5300/7879) done. Loss: 0.1165  lr:0.000010
[ Sat Jul  6 22:01:31 2024 ] 	Batch(5400/7879) done. Loss: 0.1798  lr:0.000010
[ Sat Jul  6 22:01:49 2024 ] 
Training: Epoch [34/120], Step [5499], Loss: 0.05877968296408653, Training Accuracy: 97.82499999999999
[ Sat Jul  6 22:01:49 2024 ] 	Batch(5500/7879) done. Loss: 0.0024  lr:0.000010
[ Sat Jul  6 22:02:07 2024 ] 	Batch(5600/7879) done. Loss: 0.2437  lr:0.000010
[ Sat Jul  6 22:02:25 2024 ] 	Batch(5700/7879) done. Loss: 0.1557  lr:0.000010
[ Sat Jul  6 22:02:43 2024 ] 	Batch(5800/7879) done. Loss: 0.0089  lr:0.000010
[ Sat Jul  6 22:03:01 2024 ] 	Batch(5900/7879) done. Loss: 0.0908  lr:0.000010
[ Sat Jul  6 22:03:19 2024 ] 
Training: Epoch [34/120], Step [5999], Loss: 0.03893303498625755, Training Accuracy: 97.78750000000001
[ Sat Jul  6 22:03:19 2024 ] 	Batch(6000/7879) done. Loss: 0.1176  lr:0.000010
[ Sat Jul  6 22:03:37 2024 ] 	Batch(6100/7879) done. Loss: 0.0389  lr:0.000010
[ Sat Jul  6 22:03:55 2024 ] 	Batch(6200/7879) done. Loss: 0.0236  lr:0.000010
[ Sat Jul  6 22:04:13 2024 ] 	Batch(6300/7879) done. Loss: 0.0564  lr:0.000010
[ Sat Jul  6 22:04:31 2024 ] 	Batch(6400/7879) done. Loss: 0.0363  lr:0.000010
[ Sat Jul  6 22:04:49 2024 ] 
Training: Epoch [34/120], Step [6499], Loss: 0.8047912120819092, Training Accuracy: 97.81346153846154
[ Sat Jul  6 22:04:49 2024 ] 	Batch(6500/7879) done. Loss: 0.0474  lr:0.000010
[ Sat Jul  6 22:05:07 2024 ] 	Batch(6600/7879) done. Loss: 0.2607  lr:0.000010
[ Sat Jul  6 22:05:25 2024 ] 	Batch(6700/7879) done. Loss: 0.1411  lr:0.000010
[ Sat Jul  6 22:05:44 2024 ] 	Batch(6800/7879) done. Loss: 0.0821  lr:0.000010
[ Sat Jul  6 22:06:02 2024 ] 	Batch(6900/7879) done. Loss: 0.0024  lr:0.000010
[ Sat Jul  6 22:06:20 2024 ] 
Training: Epoch [34/120], Step [6999], Loss: 0.016304705291986465, Training Accuracy: 97.82142857142857
[ Sat Jul  6 22:06:20 2024 ] 	Batch(7000/7879) done. Loss: 0.0056  lr:0.000010
[ Sat Jul  6 22:06:38 2024 ] 	Batch(7100/7879) done. Loss: 0.0036  lr:0.000010
[ Sat Jul  6 22:06:57 2024 ] 	Batch(7200/7879) done. Loss: 0.2276  lr:0.000010
[ Sat Jul  6 22:07:15 2024 ] 	Batch(7300/7879) done. Loss: 0.0295  lr:0.000010
[ Sat Jul  6 22:07:34 2024 ] 	Batch(7400/7879) done. Loss: 0.0319  lr:0.000010
[ Sat Jul  6 22:07:53 2024 ] 
Training: Epoch [34/120], Step [7499], Loss: 0.012860335409641266, Training Accuracy: 97.82666666666667
[ Sat Jul  6 22:07:53 2024 ] 	Batch(7500/7879) done. Loss: 0.1362  lr:0.000010
[ Sat Jul  6 22:08:12 2024 ] 	Batch(7600/7879) done. Loss: 0.2483  lr:0.000010
[ Sat Jul  6 22:08:30 2024 ] 	Batch(7700/7879) done. Loss: 0.0039  lr:0.000010
[ Sat Jul  6 22:08:48 2024 ] 	Batch(7800/7879) done. Loss: 0.0428  lr:0.000010
[ Sat Jul  6 22:09:02 2024 ] 	Mean training loss: 0.0856.
[ Sat Jul  6 22:09:02 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 22:09:02 2024 ] Training epoch: 36
[ Sat Jul  6 22:09:03 2024 ] 	Batch(0/7879) done. Loss: 0.0478  lr:0.000010
[ Sat Jul  6 22:09:21 2024 ] 	Batch(100/7879) done. Loss: 0.0346  lr:0.000010
[ Sat Jul  6 22:09:39 2024 ] 	Batch(200/7879) done. Loss: 0.1318  lr:0.000010
[ Sat Jul  6 22:09:57 2024 ] 	Batch(300/7879) done. Loss: 0.5188  lr:0.000010
[ Sat Jul  6 22:10:15 2024 ] 	Batch(400/7879) done. Loss: 0.0127  lr:0.000010
[ Sat Jul  6 22:10:33 2024 ] 
Training: Epoch [35/120], Step [499], Loss: 0.13949762284755707, Training Accuracy: 97.875
[ Sat Jul  6 22:10:33 2024 ] 	Batch(500/7879) done. Loss: 0.0969  lr:0.000010
[ Sat Jul  6 22:10:51 2024 ] 	Batch(600/7879) done. Loss: 0.0323  lr:0.000010
[ Sat Jul  6 22:11:09 2024 ] 	Batch(700/7879) done. Loss: 0.1098  lr:0.000010
[ Sat Jul  6 22:11:27 2024 ] 	Batch(800/7879) done. Loss: 0.0538  lr:0.000010
[ Sat Jul  6 22:11:45 2024 ] 	Batch(900/7879) done. Loss: 0.0188  lr:0.000010
[ Sat Jul  6 22:12:03 2024 ] 
Training: Epoch [35/120], Step [999], Loss: 0.08551047742366791, Training Accuracy: 97.8375
[ Sat Jul  6 22:12:03 2024 ] 	Batch(1000/7879) done. Loss: 0.0376  lr:0.000010
[ Sat Jul  6 22:12:21 2024 ] 	Batch(1100/7879) done. Loss: 0.0067  lr:0.000010
[ Sat Jul  6 22:12:39 2024 ] 	Batch(1200/7879) done. Loss: 0.0121  lr:0.000010
[ Sat Jul  6 22:12:57 2024 ] 	Batch(1300/7879) done. Loss: 0.0264  lr:0.000010
[ Sat Jul  6 22:13:15 2024 ] 	Batch(1400/7879) done. Loss: 0.5771  lr:0.000010
[ Sat Jul  6 22:13:33 2024 ] 
Training: Epoch [35/120], Step [1499], Loss: 0.19158701598644257, Training Accuracy: 97.875
[ Sat Jul  6 22:13:33 2024 ] 	Batch(1500/7879) done. Loss: 0.0121  lr:0.000010
[ Sat Jul  6 22:13:51 2024 ] 	Batch(1600/7879) done. Loss: 0.0266  lr:0.000010
[ Sat Jul  6 22:14:09 2024 ] 	Batch(1700/7879) done. Loss: 0.0870  lr:0.000010
[ Sat Jul  6 22:14:27 2024 ] 	Batch(1800/7879) done. Loss: 0.0805  lr:0.000010
[ Sat Jul  6 22:14:45 2024 ] 	Batch(1900/7879) done. Loss: 0.0062  lr:0.000010
[ Sat Jul  6 22:15:03 2024 ] 
Training: Epoch [35/120], Step [1999], Loss: 0.003217823803424835, Training Accuracy: 97.89999999999999
[ Sat Jul  6 22:15:03 2024 ] 	Batch(2000/7879) done. Loss: 0.1284  lr:0.000010
[ Sat Jul  6 22:15:21 2024 ] 	Batch(2100/7879) done. Loss: 0.0025  lr:0.000010
[ Sat Jul  6 22:15:39 2024 ] 	Batch(2200/7879) done. Loss: 0.0184  lr:0.000010
[ Sat Jul  6 22:15:57 2024 ] 	Batch(2300/7879) done. Loss: 0.0033  lr:0.000010
[ Sat Jul  6 22:16:15 2024 ] 	Batch(2400/7879) done. Loss: 0.0440  lr:0.000010
[ Sat Jul  6 22:16:33 2024 ] 
Training: Epoch [35/120], Step [2499], Loss: 0.043734148144721985, Training Accuracy: 97.91
[ Sat Jul  6 22:16:33 2024 ] 	Batch(2500/7879) done. Loss: 0.0235  lr:0.000010
[ Sat Jul  6 22:16:51 2024 ] 	Batch(2600/7879) done. Loss: 0.0307  lr:0.000010
[ Sat Jul  6 22:17:09 2024 ] 	Batch(2700/7879) done. Loss: 0.4073  lr:0.000010
[ Sat Jul  6 22:17:27 2024 ] 	Batch(2800/7879) done. Loss: 0.0544  lr:0.000010
[ Sat Jul  6 22:17:45 2024 ] 	Batch(2900/7879) done. Loss: 0.0367  lr:0.000010
[ Sat Jul  6 22:18:03 2024 ] 
Training: Epoch [35/120], Step [2999], Loss: 0.015842711552977562, Training Accuracy: 97.88333333333334
[ Sat Jul  6 22:18:03 2024 ] 	Batch(3000/7879) done. Loss: 0.1386  lr:0.000010
[ Sat Jul  6 22:18:21 2024 ] 	Batch(3100/7879) done. Loss: 0.0118  lr:0.000010
[ Sat Jul  6 22:18:39 2024 ] 	Batch(3200/7879) done. Loss: 0.3813  lr:0.000010
[ Sat Jul  6 22:18:57 2024 ] 	Batch(3300/7879) done. Loss: 0.0746  lr:0.000010
[ Sat Jul  6 22:19:15 2024 ] 	Batch(3400/7879) done. Loss: 0.0251  lr:0.000010
[ Sat Jul  6 22:19:32 2024 ] 
Training: Epoch [35/120], Step [3499], Loss: 0.45702946186065674, Training Accuracy: 97.8607142857143
[ Sat Jul  6 22:19:32 2024 ] 	Batch(3500/7879) done. Loss: 0.0429  lr:0.000010
[ Sat Jul  6 22:19:51 2024 ] 	Batch(3600/7879) done. Loss: 0.0138  lr:0.000010
[ Sat Jul  6 22:20:09 2024 ] 	Batch(3700/7879) done. Loss: 0.3656  lr:0.000010
[ Sat Jul  6 22:20:28 2024 ] 	Batch(3800/7879) done. Loss: 0.0520  lr:0.000010
[ Sat Jul  6 22:20:46 2024 ] 	Batch(3900/7879) done. Loss: 0.0215  lr:0.000010
[ Sat Jul  6 22:21:05 2024 ] 
Training: Epoch [35/120], Step [3999], Loss: 0.21951445937156677, Training Accuracy: 97.89999999999999
[ Sat Jul  6 22:21:05 2024 ] 	Batch(4000/7879) done. Loss: 0.0079  lr:0.000010
[ Sat Jul  6 22:21:24 2024 ] 	Batch(4100/7879) done. Loss: 0.0479  lr:0.000010
[ Sat Jul  6 22:21:42 2024 ] 	Batch(4200/7879) done. Loss: 0.1005  lr:0.000010
[ Sat Jul  6 22:22:01 2024 ] 	Batch(4300/7879) done. Loss: 0.0630  lr:0.000010
[ Sat Jul  6 22:22:20 2024 ] 	Batch(4400/7879) done. Loss: 0.0406  lr:0.000010
[ Sat Jul  6 22:22:38 2024 ] 
Training: Epoch [35/120], Step [4499], Loss: 0.013508627191185951, Training Accuracy: 97.90555555555555
[ Sat Jul  6 22:22:38 2024 ] 	Batch(4500/7879) done. Loss: 0.0064  lr:0.000010
[ Sat Jul  6 22:22:57 2024 ] 	Batch(4600/7879) done. Loss: 0.2139  lr:0.000010
[ Sat Jul  6 22:23:15 2024 ] 	Batch(4700/7879) done. Loss: 0.1022  lr:0.000010
[ Sat Jul  6 22:23:34 2024 ] 	Batch(4800/7879) done. Loss: 0.2052  lr:0.000010
[ Sat Jul  6 22:23:52 2024 ] 	Batch(4900/7879) done. Loss: 0.0960  lr:0.000010
[ Sat Jul  6 22:24:10 2024 ] 
Training: Epoch [35/120], Step [4999], Loss: 0.026860112324357033, Training Accuracy: 97.895
[ Sat Jul  6 22:24:10 2024 ] 	Batch(5000/7879) done. Loss: 0.0083  lr:0.000010
[ Sat Jul  6 22:24:28 2024 ] 	Batch(5100/7879) done. Loss: 0.0437  lr:0.000010
[ Sat Jul  6 22:24:46 2024 ] 	Batch(5200/7879) done. Loss: 0.0926  lr:0.000010
[ Sat Jul  6 22:25:04 2024 ] 	Batch(5300/7879) done. Loss: 0.0065  lr:0.000010
[ Sat Jul  6 22:25:22 2024 ] 	Batch(5400/7879) done. Loss: 0.0086  lr:0.000010
[ Sat Jul  6 22:25:40 2024 ] 
Training: Epoch [35/120], Step [5499], Loss: 0.10044387727975845, Training Accuracy: 97.89090909090909
[ Sat Jul  6 22:25:40 2024 ] 	Batch(5500/7879) done. Loss: 0.0031  lr:0.000010
[ Sat Jul  6 22:25:58 2024 ] 	Batch(5600/7879) done. Loss: 0.0133  lr:0.000010
[ Sat Jul  6 22:26:17 2024 ] 	Batch(5700/7879) done. Loss: 0.1188  lr:0.000010
[ Sat Jul  6 22:26:35 2024 ] 	Batch(5800/7879) done. Loss: 0.0627  lr:0.000010
[ Sat Jul  6 22:26:53 2024 ] 	Batch(5900/7879) done. Loss: 0.0632  lr:0.000010
[ Sat Jul  6 22:27:11 2024 ] 
Training: Epoch [35/120], Step [5999], Loss: 0.20934276282787323, Training Accuracy: 97.89583333333334
[ Sat Jul  6 22:27:11 2024 ] 	Batch(6000/7879) done. Loss: 0.0513  lr:0.000010
[ Sat Jul  6 22:27:30 2024 ] 	Batch(6100/7879) done. Loss: 0.0719  lr:0.000010
[ Sat Jul  6 22:27:48 2024 ] 	Batch(6200/7879) done. Loss: 0.0367  lr:0.000010
[ Sat Jul  6 22:28:07 2024 ] 	Batch(6300/7879) done. Loss: 0.0214  lr:0.000010
[ Sat Jul  6 22:28:25 2024 ] 	Batch(6400/7879) done. Loss: 0.0642  lr:0.000010
[ Sat Jul  6 22:28:43 2024 ] 
Training: Epoch [35/120], Step [6499], Loss: 0.2797465920448303, Training Accuracy: 97.86730769230769
[ Sat Jul  6 22:28:43 2024 ] 	Batch(6500/7879) done. Loss: 0.2476  lr:0.000010
[ Sat Jul  6 22:29:01 2024 ] 	Batch(6600/7879) done. Loss: 0.1316  lr:0.000010
[ Sat Jul  6 22:29:19 2024 ] 	Batch(6700/7879) done. Loss: 0.0682  lr:0.000010
[ Sat Jul  6 22:29:37 2024 ] 	Batch(6800/7879) done. Loss: 0.0259  lr:0.000010
[ Sat Jul  6 22:29:55 2024 ] 	Batch(6900/7879) done. Loss: 0.0291  lr:0.000010
[ Sat Jul  6 22:30:12 2024 ] 
Training: Epoch [35/120], Step [6999], Loss: 0.0327964685857296, Training Accuracy: 97.83214285714286
[ Sat Jul  6 22:30:13 2024 ] 	Batch(7000/7879) done. Loss: 0.3480  lr:0.000010
[ Sat Jul  6 22:30:31 2024 ] 	Batch(7100/7879) done. Loss: 0.0035  lr:0.000010
[ Sat Jul  6 22:30:49 2024 ] 	Batch(7200/7879) done. Loss: 0.0204  lr:0.000010
[ Sat Jul  6 22:31:07 2024 ] 	Batch(7300/7879) done. Loss: 0.1185  lr:0.000010
[ Sat Jul  6 22:31:25 2024 ] 	Batch(7400/7879) done. Loss: 0.2257  lr:0.000010
[ Sat Jul  6 22:31:42 2024 ] 
Training: Epoch [35/120], Step [7499], Loss: 0.05670240521430969, Training Accuracy: 97.84333333333333
[ Sat Jul  6 22:31:43 2024 ] 	Batch(7500/7879) done. Loss: 0.0555  lr:0.000010
[ Sat Jul  6 22:32:01 2024 ] 	Batch(7600/7879) done. Loss: 0.1999  lr:0.000010
[ Sat Jul  6 22:32:19 2024 ] 	Batch(7700/7879) done. Loss: 0.0016  lr:0.000010
[ Sat Jul  6 22:32:36 2024 ] 	Batch(7800/7879) done. Loss: 0.0738  lr:0.000010
[ Sat Jul  6 22:32:51 2024 ] 	Mean training loss: 0.0874.
[ Sat Jul  6 22:32:51 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 22:32:51 2024 ] Training epoch: 37
[ Sat Jul  6 22:32:51 2024 ] 	Batch(0/7879) done. Loss: 0.0143  lr:0.000010
[ Sat Jul  6 22:33:10 2024 ] 	Batch(100/7879) done. Loss: 0.0840  lr:0.000010
[ Sat Jul  6 22:33:28 2024 ] 	Batch(200/7879) done. Loss: 0.0086  lr:0.000010
[ Sat Jul  6 22:33:47 2024 ] 	Batch(300/7879) done. Loss: 0.4058  lr:0.000010
[ Sat Jul  6 22:34:06 2024 ] 	Batch(400/7879) done. Loss: 0.4902  lr:0.000010
[ Sat Jul  6 22:34:24 2024 ] 
Training: Epoch [36/120], Step [499], Loss: 0.0020082835108041763, Training Accuracy: 98.1
[ Sat Jul  6 22:34:24 2024 ] 	Batch(500/7879) done. Loss: 0.1523  lr:0.000010
[ Sat Jul  6 22:34:43 2024 ] 	Batch(600/7879) done. Loss: 0.0059  lr:0.000010
[ Sat Jul  6 22:35:01 2024 ] 	Batch(700/7879) done. Loss: 0.0334  lr:0.000010
[ Sat Jul  6 22:35:20 2024 ] 	Batch(800/7879) done. Loss: 0.0038  lr:0.000010
[ Sat Jul  6 22:35:39 2024 ] 	Batch(900/7879) done. Loss: 0.0066  lr:0.000010
[ Sat Jul  6 22:35:57 2024 ] 
Training: Epoch [36/120], Step [999], Loss: 0.08052582293748856, Training Accuracy: 98.02499999999999
[ Sat Jul  6 22:35:57 2024 ] 	Batch(1000/7879) done. Loss: 0.1532  lr:0.000010
[ Sat Jul  6 22:36:16 2024 ] 	Batch(1100/7879) done. Loss: 0.1164  lr:0.000010
[ Sat Jul  6 22:36:35 2024 ] 	Batch(1200/7879) done. Loss: 0.1258  lr:0.000010
[ Sat Jul  6 22:36:53 2024 ] 	Batch(1300/7879) done. Loss: 0.1489  lr:0.000010
[ Sat Jul  6 22:37:11 2024 ] 	Batch(1400/7879) done. Loss: 0.0300  lr:0.000010
[ Sat Jul  6 22:37:28 2024 ] 
Training: Epoch [36/120], Step [1499], Loss: 0.05735155940055847, Training Accuracy: 97.93333333333332
[ Sat Jul  6 22:37:29 2024 ] 	Batch(1500/7879) done. Loss: 0.0504  lr:0.000010
[ Sat Jul  6 22:37:47 2024 ] 	Batch(1600/7879) done. Loss: 0.0120  lr:0.000010
[ Sat Jul  6 22:38:05 2024 ] 	Batch(1700/7879) done. Loss: 0.0025  lr:0.000010
[ Sat Jul  6 22:38:24 2024 ] 	Batch(1800/7879) done. Loss: 0.0770  lr:0.000010
[ Sat Jul  6 22:38:42 2024 ] 	Batch(1900/7879) done. Loss: 0.1960  lr:0.000010
[ Sat Jul  6 22:39:01 2024 ] 
Training: Epoch [36/120], Step [1999], Loss: 0.4613338112831116, Training Accuracy: 98.00625
[ Sat Jul  6 22:39:01 2024 ] 	Batch(2000/7879) done. Loss: 0.0220  lr:0.000010
[ Sat Jul  6 22:39:19 2024 ] 	Batch(2100/7879) done. Loss: 0.0201  lr:0.000010
[ Sat Jul  6 22:39:37 2024 ] 	Batch(2200/7879) done. Loss: 0.0498  lr:0.000010
[ Sat Jul  6 22:39:55 2024 ] 	Batch(2300/7879) done. Loss: 0.0924  lr:0.000010
[ Sat Jul  6 22:40:13 2024 ] 	Batch(2400/7879) done. Loss: 0.0279  lr:0.000010
[ Sat Jul  6 22:40:32 2024 ] 
Training: Epoch [36/120], Step [2499], Loss: 0.014645654708147049, Training Accuracy: 98.03
[ Sat Jul  6 22:40:32 2024 ] 	Batch(2500/7879) done. Loss: 0.0089  lr:0.000010
[ Sat Jul  6 22:40:51 2024 ] 	Batch(2600/7879) done. Loss: 0.2851  lr:0.000010
[ Sat Jul  6 22:41:09 2024 ] 	Batch(2700/7879) done. Loss: 0.0055  lr:0.000010
[ Sat Jul  6 22:41:28 2024 ] 	Batch(2800/7879) done. Loss: 0.0057  lr:0.000010
[ Sat Jul  6 22:41:47 2024 ] 	Batch(2900/7879) done. Loss: 0.1927  lr:0.000010
[ Sat Jul  6 22:42:05 2024 ] 
Training: Epoch [36/120], Step [2999], Loss: 0.04026852548122406, Training Accuracy: 97.975
[ Sat Jul  6 22:42:05 2024 ] 	Batch(3000/7879) done. Loss: 0.1408  lr:0.000010
[ Sat Jul  6 22:42:24 2024 ] 	Batch(3100/7879) done. Loss: 0.0411  lr:0.000010
[ Sat Jul  6 22:42:43 2024 ] 	Batch(3200/7879) done. Loss: 0.0209  lr:0.000010
[ Sat Jul  6 22:43:01 2024 ] 	Batch(3300/7879) done. Loss: 0.0062  lr:0.000010
[ Sat Jul  6 22:43:20 2024 ] 	Batch(3400/7879) done. Loss: 0.3212  lr:0.000010
[ Sat Jul  6 22:43:38 2024 ] 
Training: Epoch [36/120], Step [3499], Loss: 0.06372048705816269, Training Accuracy: 97.95357142857142
[ Sat Jul  6 22:43:38 2024 ] 	Batch(3500/7879) done. Loss: 0.0316  lr:0.000010
[ Sat Jul  6 22:43:57 2024 ] 	Batch(3600/7879) done. Loss: 0.0054  lr:0.000010
[ Sat Jul  6 22:44:15 2024 ] 	Batch(3700/7879) done. Loss: 0.0192  lr:0.000010
[ Sat Jul  6 22:44:33 2024 ] 	Batch(3800/7879) done. Loss: 0.0174  lr:0.000010
[ Sat Jul  6 22:44:51 2024 ] 	Batch(3900/7879) done. Loss: 0.0369  lr:0.000010
[ Sat Jul  6 22:45:08 2024 ] 
Training: Epoch [36/120], Step [3999], Loss: 0.4876943826675415, Training Accuracy: 97.96875
[ Sat Jul  6 22:45:09 2024 ] 	Batch(4000/7879) done. Loss: 0.0117  lr:0.000010
[ Sat Jul  6 22:45:27 2024 ] 	Batch(4100/7879) done. Loss: 0.0183  lr:0.000010
[ Sat Jul  6 22:45:46 2024 ] 	Batch(4200/7879) done. Loss: 0.0931  lr:0.000010
[ Sat Jul  6 22:46:04 2024 ] 	Batch(4300/7879) done. Loss: 0.3173  lr:0.000010
[ Sat Jul  6 22:46:23 2024 ] 	Batch(4400/7879) done. Loss: 0.2676  lr:0.000010
[ Sat Jul  6 22:46:42 2024 ] 
Training: Epoch [36/120], Step [4499], Loss: 0.08458076417446136, Training Accuracy: 97.91944444444445
[ Sat Jul  6 22:46:42 2024 ] 	Batch(4500/7879) done. Loss: 0.0361  lr:0.000010
[ Sat Jul  6 22:47:00 2024 ] 	Batch(4600/7879) done. Loss: 0.0270  lr:0.000010
[ Sat Jul  6 22:47:19 2024 ] 	Batch(4700/7879) done. Loss: 0.0195  lr:0.000010
[ Sat Jul  6 22:47:38 2024 ] 	Batch(4800/7879) done. Loss: 0.2708  lr:0.000010
[ Sat Jul  6 22:47:56 2024 ] 	Batch(4900/7879) done. Loss: 0.3595  lr:0.000010
[ Sat Jul  6 22:48:13 2024 ] 
Training: Epoch [36/120], Step [4999], Loss: 0.010907948948442936, Training Accuracy: 97.89750000000001
[ Sat Jul  6 22:48:14 2024 ] 	Batch(5000/7879) done. Loss: 0.0339  lr:0.000010
[ Sat Jul  6 22:48:32 2024 ] 	Batch(5100/7879) done. Loss: 0.3406  lr:0.000010
[ Sat Jul  6 22:48:50 2024 ] 	Batch(5200/7879) done. Loss: 0.1909  lr:0.000010
[ Sat Jul  6 22:49:08 2024 ] 	Batch(5300/7879) done. Loss: 0.0578  lr:0.000010
[ Sat Jul  6 22:49:25 2024 ] 	Batch(5400/7879) done. Loss: 0.0021  lr:0.000010
[ Sat Jul  6 22:49:43 2024 ] 
Training: Epoch [36/120], Step [5499], Loss: 0.08760978281497955, Training Accuracy: 97.92727272727274
[ Sat Jul  6 22:49:43 2024 ] 	Batch(5500/7879) done. Loss: 0.0070  lr:0.000010
[ Sat Jul  6 22:50:01 2024 ] 	Batch(5600/7879) done. Loss: 0.0291  lr:0.000010
[ Sat Jul  6 22:50:19 2024 ] 	Batch(5700/7879) done. Loss: 0.1086  lr:0.000010
[ Sat Jul  6 22:50:37 2024 ] 	Batch(5800/7879) done. Loss: 0.0394  lr:0.000010
[ Sat Jul  6 22:50:55 2024 ] 	Batch(5900/7879) done. Loss: 0.0361  lr:0.000010
[ Sat Jul  6 22:51:13 2024 ] 
Training: Epoch [36/120], Step [5999], Loss: 0.09248393028974533, Training Accuracy: 97.93333333333332
[ Sat Jul  6 22:51:13 2024 ] 	Batch(6000/7879) done. Loss: 0.1060  lr:0.000010
[ Sat Jul  6 22:51:31 2024 ] 	Batch(6100/7879) done. Loss: 0.1007  lr:0.000010
[ Sat Jul  6 22:51:49 2024 ] 	Batch(6200/7879) done. Loss: 0.2924  lr:0.000010
[ Sat Jul  6 22:52:07 2024 ] 	Batch(6300/7879) done. Loss: 0.0044  lr:0.000010
[ Sat Jul  6 22:52:25 2024 ] 	Batch(6400/7879) done. Loss: 0.0324  lr:0.000010
[ Sat Jul  6 22:52:43 2024 ] 
Training: Epoch [36/120], Step [6499], Loss: 0.0022640726529061794, Training Accuracy: 97.93653846153846
[ Sat Jul  6 22:52:43 2024 ] 	Batch(6500/7879) done. Loss: 0.0104  lr:0.000010
[ Sat Jul  6 22:53:01 2024 ] 	Batch(6600/7879) done. Loss: 0.0118  lr:0.000010
[ Sat Jul  6 22:53:19 2024 ] 	Batch(6700/7879) done. Loss: 0.0426  lr:0.000010
[ Sat Jul  6 22:53:37 2024 ] 	Batch(6800/7879) done. Loss: 0.0091  lr:0.000010
[ Sat Jul  6 22:53:55 2024 ] 	Batch(6900/7879) done. Loss: 0.0377  lr:0.000010
[ Sat Jul  6 22:54:13 2024 ] 
Training: Epoch [36/120], Step [6999], Loss: 0.09013331681489944, Training Accuracy: 97.94107142857142
[ Sat Jul  6 22:54:13 2024 ] 	Batch(7000/7879) done. Loss: 0.0731  lr:0.000010
[ Sat Jul  6 22:54:31 2024 ] 	Batch(7100/7879) done. Loss: 0.2082  lr:0.000010
[ Sat Jul  6 22:54:49 2024 ] 	Batch(7200/7879) done. Loss: 0.0025  lr:0.000010
[ Sat Jul  6 22:55:07 2024 ] 	Batch(7300/7879) done. Loss: 0.0663  lr:0.000010
[ Sat Jul  6 22:55:25 2024 ] 	Batch(7400/7879) done. Loss: 0.1240  lr:0.000010
[ Sat Jul  6 22:55:43 2024 ] 
Training: Epoch [36/120], Step [7499], Loss: 0.14834190905094147, Training Accuracy: 97.93333333333332
[ Sat Jul  6 22:55:43 2024 ] 	Batch(7500/7879) done. Loss: 0.0844  lr:0.000010
[ Sat Jul  6 22:56:01 2024 ] 	Batch(7600/7879) done. Loss: 0.1837  lr:0.000010
[ Sat Jul  6 22:56:19 2024 ] 	Batch(7700/7879) done. Loss: 0.0257  lr:0.000010
[ Sat Jul  6 22:56:37 2024 ] 	Batch(7800/7879) done. Loss: 0.0459  lr:0.000010
[ Sat Jul  6 22:56:51 2024 ] 	Mean training loss: 0.0849.
[ Sat Jul  6 22:56:51 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 22:56:51 2024 ] Training epoch: 38
[ Sat Jul  6 22:56:52 2024 ] 	Batch(0/7879) done. Loss: 0.0141  lr:0.000010
[ Sat Jul  6 22:57:10 2024 ] 	Batch(100/7879) done. Loss: 0.0098  lr:0.000010
[ Sat Jul  6 22:57:28 2024 ] 	Batch(200/7879) done. Loss: 0.0236  lr:0.000010
[ Sat Jul  6 22:57:46 2024 ] 	Batch(300/7879) done. Loss: 0.3529  lr:0.000010
[ Sat Jul  6 22:58:04 2024 ] 	Batch(400/7879) done. Loss: 0.0205  lr:0.000010
[ Sat Jul  6 22:58:22 2024 ] 
Training: Epoch [37/120], Step [499], Loss: 0.01518199872225523, Training Accuracy: 97.95
[ Sat Jul  6 22:58:22 2024 ] 	Batch(500/7879) done. Loss: 0.0233  lr:0.000010
[ Sat Jul  6 22:58:40 2024 ] 	Batch(600/7879) done. Loss: 0.0097  lr:0.000010
[ Sat Jul  6 22:58:58 2024 ] 	Batch(700/7879) done. Loss: 0.0192  lr:0.000010
[ Sat Jul  6 22:59:16 2024 ] 	Batch(800/7879) done. Loss: 0.0125  lr:0.000010
[ Sat Jul  6 22:59:34 2024 ] 	Batch(900/7879) done. Loss: 0.1403  lr:0.000010
[ Sat Jul  6 22:59:52 2024 ] 
Training: Epoch [37/120], Step [999], Loss: 0.012835178524255753, Training Accuracy: 98.075
[ Sat Jul  6 22:59:52 2024 ] 	Batch(1000/7879) done. Loss: 0.0212  lr:0.000010
[ Sat Jul  6 23:00:10 2024 ] 	Batch(1100/7879) done. Loss: 0.0515  lr:0.000010
[ Sat Jul  6 23:00:28 2024 ] 	Batch(1200/7879) done. Loss: 0.4111  lr:0.000010
[ Sat Jul  6 23:00:46 2024 ] 	Batch(1300/7879) done. Loss: 0.2687  lr:0.000010
[ Sat Jul  6 23:01:04 2024 ] 	Batch(1400/7879) done. Loss: 0.0071  lr:0.000010
[ Sat Jul  6 23:01:22 2024 ] 
Training: Epoch [37/120], Step [1499], Loss: 0.042838238179683685, Training Accuracy: 98.02499999999999
[ Sat Jul  6 23:01:22 2024 ] 	Batch(1500/7879) done. Loss: 0.0030  lr:0.000010
[ Sat Jul  6 23:01:41 2024 ] 	Batch(1600/7879) done. Loss: 0.0327  lr:0.000010
[ Sat Jul  6 23:01:59 2024 ] 	Batch(1700/7879) done. Loss: 0.0658  lr:0.000010
[ Sat Jul  6 23:02:17 2024 ] 	Batch(1800/7879) done. Loss: 0.0114  lr:0.000010
[ Sat Jul  6 23:02:35 2024 ] 	Batch(1900/7879) done. Loss: 0.1449  lr:0.000010
[ Sat Jul  6 23:02:53 2024 ] 
Training: Epoch [37/120], Step [1999], Loss: 0.12302424758672714, Training Accuracy: 97.91875
[ Sat Jul  6 23:02:53 2024 ] 	Batch(2000/7879) done. Loss: 0.0228  lr:0.000010
[ Sat Jul  6 23:03:11 2024 ] 	Batch(2100/7879) done. Loss: 0.0558  lr:0.000010
[ Sat Jul  6 23:03:29 2024 ] 	Batch(2200/7879) done. Loss: 0.1856  lr:0.000010
[ Sat Jul  6 23:03:47 2024 ] 	Batch(2300/7879) done. Loss: 0.0676  lr:0.000010
[ Sat Jul  6 23:04:05 2024 ] 	Batch(2400/7879) done. Loss: 0.1134  lr:0.000010
[ Sat Jul  6 23:04:23 2024 ] 
Training: Epoch [37/120], Step [2499], Loss: 0.015055807307362556, Training Accuracy: 97.875
[ Sat Jul  6 23:04:23 2024 ] 	Batch(2500/7879) done. Loss: 0.0906  lr:0.000010
[ Sat Jul  6 23:04:42 2024 ] 	Batch(2600/7879) done. Loss: 0.0288  lr:0.000010
[ Sat Jul  6 23:05:00 2024 ] 	Batch(2700/7879) done. Loss: 0.4489  lr:0.000010
[ Sat Jul  6 23:05:18 2024 ] 	Batch(2800/7879) done. Loss: 0.1180  lr:0.000010
[ Sat Jul  6 23:05:36 2024 ] 	Batch(2900/7879) done. Loss: 0.0124  lr:0.000010
[ Sat Jul  6 23:05:53 2024 ] 
Training: Epoch [37/120], Step [2999], Loss: 0.045500196516513824, Training Accuracy: 97.89583333333334
[ Sat Jul  6 23:05:54 2024 ] 	Batch(3000/7879) done. Loss: 0.0040  lr:0.000010
[ Sat Jul  6 23:06:12 2024 ] 	Batch(3100/7879) done. Loss: 0.1721  lr:0.000010
[ Sat Jul  6 23:06:30 2024 ] 	Batch(3200/7879) done. Loss: 0.0466  lr:0.000010
[ Sat Jul  6 23:06:48 2024 ] 	Batch(3300/7879) done. Loss: 0.0093  lr:0.000010
[ Sat Jul  6 23:07:05 2024 ] 	Batch(3400/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 23:07:23 2024 ] 
Training: Epoch [37/120], Step [3499], Loss: 0.07849518954753876, Training Accuracy: 97.91428571428571
[ Sat Jul  6 23:07:23 2024 ] 	Batch(3500/7879) done. Loss: 0.1680  lr:0.000010
[ Sat Jul  6 23:07:41 2024 ] 	Batch(3600/7879) done. Loss: 0.0221  lr:0.000010
[ Sat Jul  6 23:07:59 2024 ] 	Batch(3700/7879) done. Loss: 0.0081  lr:0.000010
[ Sat Jul  6 23:08:17 2024 ] 	Batch(3800/7879) done. Loss: 0.0850  lr:0.000010
[ Sat Jul  6 23:08:35 2024 ] 	Batch(3900/7879) done. Loss: 0.0603  lr:0.000010
[ Sat Jul  6 23:08:53 2024 ] 
Training: Epoch [37/120], Step [3999], Loss: 0.36621835827827454, Training Accuracy: 97.865625
[ Sat Jul  6 23:08:53 2024 ] 	Batch(4000/7879) done. Loss: 0.0174  lr:0.000010
[ Sat Jul  6 23:09:11 2024 ] 	Batch(4100/7879) done. Loss: 0.2276  lr:0.000010
[ Sat Jul  6 23:09:29 2024 ] 	Batch(4200/7879) done. Loss: 0.0320  lr:0.000010
[ Sat Jul  6 23:09:47 2024 ] 	Batch(4300/7879) done. Loss: 0.0578  lr:0.000010
[ Sat Jul  6 23:10:05 2024 ] 	Batch(4400/7879) done. Loss: 0.0371  lr:0.000010
[ Sat Jul  6 23:10:23 2024 ] 
Training: Epoch [37/120], Step [4499], Loss: 0.003078434383496642, Training Accuracy: 97.86111111111111
[ Sat Jul  6 23:10:23 2024 ] 	Batch(4500/7879) done. Loss: 0.0529  lr:0.000010
[ Sat Jul  6 23:10:41 2024 ] 	Batch(4600/7879) done. Loss: 0.2178  lr:0.000010
[ Sat Jul  6 23:10:59 2024 ] 	Batch(4700/7879) done. Loss: 0.0496  lr:0.000010
[ Sat Jul  6 23:11:17 2024 ] 	Batch(4800/7879) done. Loss: 0.0274  lr:0.000010
[ Sat Jul  6 23:11:35 2024 ] 	Batch(4900/7879) done. Loss: 0.0785  lr:0.000010
[ Sat Jul  6 23:11:53 2024 ] 
Training: Epoch [37/120], Step [4999], Loss: 0.010315445251762867, Training Accuracy: 97.86500000000001
[ Sat Jul  6 23:11:53 2024 ] 	Batch(5000/7879) done. Loss: 0.0071  lr:0.000010
[ Sat Jul  6 23:12:11 2024 ] 	Batch(5100/7879) done. Loss: 0.0744  lr:0.000010
[ Sat Jul  6 23:12:29 2024 ] 	Batch(5200/7879) done. Loss: 0.0083  lr:0.000010
[ Sat Jul  6 23:12:48 2024 ] 	Batch(5300/7879) done. Loss: 0.1025  lr:0.000010
[ Sat Jul  6 23:13:06 2024 ] 	Batch(5400/7879) done. Loss: 0.0109  lr:0.000010
[ Sat Jul  6 23:13:25 2024 ] 
Training: Epoch [37/120], Step [5499], Loss: 0.03684267774224281, Training Accuracy: 97.85454545454544
[ Sat Jul  6 23:13:25 2024 ] 	Batch(5500/7879) done. Loss: 0.0055  lr:0.000010
[ Sat Jul  6 23:13:43 2024 ] 	Batch(5600/7879) done. Loss: 0.0985  lr:0.000010
[ Sat Jul  6 23:14:01 2024 ] 	Batch(5700/7879) done. Loss: 0.6016  lr:0.000010
[ Sat Jul  6 23:14:19 2024 ] 	Batch(5800/7879) done. Loss: 0.0074  lr:0.000010
[ Sat Jul  6 23:14:37 2024 ] 	Batch(5900/7879) done. Loss: 0.0181  lr:0.000010
[ Sat Jul  6 23:14:55 2024 ] 
Training: Epoch [37/120], Step [5999], Loss: 0.06480715423822403, Training Accuracy: 97.875
[ Sat Jul  6 23:14:55 2024 ] 	Batch(6000/7879) done. Loss: 0.0234  lr:0.000010
[ Sat Jul  6 23:15:13 2024 ] 	Batch(6100/7879) done. Loss: 0.0383  lr:0.000010
[ Sat Jul  6 23:15:31 2024 ] 	Batch(6200/7879) done. Loss: 0.0164  lr:0.000010
[ Sat Jul  6 23:15:49 2024 ] 	Batch(6300/7879) done. Loss: 0.2277  lr:0.000010
[ Sat Jul  6 23:16:07 2024 ] 	Batch(6400/7879) done. Loss: 0.1155  lr:0.000010
[ Sat Jul  6 23:16:25 2024 ] 
Training: Epoch [37/120], Step [6499], Loss: 0.03868270292878151, Training Accuracy: 97.87115384615385
[ Sat Jul  6 23:16:25 2024 ] 	Batch(6500/7879) done. Loss: 0.0144  lr:0.000010
[ Sat Jul  6 23:16:43 2024 ] 	Batch(6600/7879) done. Loss: 0.0389  lr:0.000010
[ Sat Jul  6 23:17:01 2024 ] 	Batch(6700/7879) done. Loss: 0.1303  lr:0.000010
[ Sat Jul  6 23:17:19 2024 ] 	Batch(6800/7879) done. Loss: 0.1306  lr:0.000010
[ Sat Jul  6 23:17:38 2024 ] 	Batch(6900/7879) done. Loss: 0.0101  lr:0.000010
[ Sat Jul  6 23:17:56 2024 ] 
Training: Epoch [37/120], Step [6999], Loss: 0.01659126579761505, Training Accuracy: 97.875
[ Sat Jul  6 23:17:56 2024 ] 	Batch(7000/7879) done. Loss: 0.3140  lr:0.000010
[ Sat Jul  6 23:18:15 2024 ] 	Batch(7100/7879) done. Loss: 0.2586  lr:0.000010
[ Sat Jul  6 23:18:34 2024 ] 	Batch(7200/7879) done. Loss: 0.0296  lr:0.000010
[ Sat Jul  6 23:18:52 2024 ] 	Batch(7300/7879) done. Loss: 0.0280  lr:0.000010
[ Sat Jul  6 23:19:11 2024 ] 	Batch(7400/7879) done. Loss: 0.0196  lr:0.000010
[ Sat Jul  6 23:19:29 2024 ] 
Training: Epoch [37/120], Step [7499], Loss: 0.10209449380636215, Training Accuracy: 97.85666666666667
[ Sat Jul  6 23:19:29 2024 ] 	Batch(7500/7879) done. Loss: 0.1384  lr:0.000010
[ Sat Jul  6 23:19:48 2024 ] 	Batch(7600/7879) done. Loss: 0.0058  lr:0.000010
[ Sat Jul  6 23:20:06 2024 ] 	Batch(7700/7879) done. Loss: 0.1489  lr:0.000010
[ Sat Jul  6 23:20:24 2024 ] 	Batch(7800/7879) done. Loss: 0.1014  lr:0.000010
[ Sat Jul  6 23:20:38 2024 ] 	Mean training loss: 0.0886.
[ Sat Jul  6 23:20:38 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul  6 23:20:38 2024 ] Training epoch: 39
[ Sat Jul  6 23:20:38 2024 ] 	Batch(0/7879) done. Loss: 0.2060  lr:0.000010
[ Sat Jul  6 23:20:57 2024 ] 	Batch(100/7879) done. Loss: 0.0965  lr:0.000010
[ Sat Jul  6 23:21:15 2024 ] 	Batch(200/7879) done. Loss: 0.0420  lr:0.000010
[ Sat Jul  6 23:21:33 2024 ] 	Batch(300/7879) done. Loss: 0.0070  lr:0.000010
[ Sat Jul  6 23:21:52 2024 ] 	Batch(400/7879) done. Loss: 0.0094  lr:0.000010
[ Sat Jul  6 23:22:10 2024 ] 
Training: Epoch [38/120], Step [499], Loss: 0.03207352012395859, Training Accuracy: 98.125
[ Sat Jul  6 23:22:10 2024 ] 	Batch(500/7879) done. Loss: 0.0163  lr:0.000010
[ Sat Jul  6 23:22:29 2024 ] 	Batch(600/7879) done. Loss: 0.0240  lr:0.000010
[ Sat Jul  6 23:22:47 2024 ] 	Batch(700/7879) done. Loss: 0.0083  lr:0.000010
[ Sat Jul  6 23:23:06 2024 ] 	Batch(800/7879) done. Loss: 0.0510  lr:0.000010
[ Sat Jul  6 23:23:24 2024 ] 	Batch(900/7879) done. Loss: 0.2251  lr:0.000010
[ Sat Jul  6 23:23:42 2024 ] 
Training: Epoch [38/120], Step [999], Loss: 0.047078561037778854, Training Accuracy: 97.85000000000001
[ Sat Jul  6 23:23:42 2024 ] 	Batch(1000/7879) done. Loss: 0.0174  lr:0.000010
[ Sat Jul  6 23:24:01 2024 ] 	Batch(1100/7879) done. Loss: 0.5550  lr:0.000010
[ Sat Jul  6 23:24:19 2024 ] 	Batch(1200/7879) done. Loss: 0.3418  lr:0.000010
[ Sat Jul  6 23:24:37 2024 ] 	Batch(1300/7879) done. Loss: 0.0069  lr:0.000010
[ Sat Jul  6 23:24:56 2024 ] 	Batch(1400/7879) done. Loss: 0.0068  lr:0.000010
[ Sat Jul  6 23:25:14 2024 ] 
Training: Epoch [38/120], Step [1499], Loss: 0.003077753121033311, Training Accuracy: 97.85000000000001
[ Sat Jul  6 23:25:14 2024 ] 	Batch(1500/7879) done. Loss: 0.0350  lr:0.000010
[ Sat Jul  6 23:25:32 2024 ] 	Batch(1600/7879) done. Loss: 0.0025  lr:0.000010
[ Sat Jul  6 23:25:51 2024 ] 	Batch(1700/7879) done. Loss: 0.0707  lr:0.000010
[ Sat Jul  6 23:26:09 2024 ] 	Batch(1800/7879) done. Loss: 0.0123  lr:0.000010
[ Sat Jul  6 23:26:27 2024 ] 	Batch(1900/7879) done. Loss: 0.4042  lr:0.000010
[ Sat Jul  6 23:26:46 2024 ] 
Training: Epoch [38/120], Step [1999], Loss: 0.010246898047626019, Training Accuracy: 97.93125
[ Sat Jul  6 23:26:46 2024 ] 	Batch(2000/7879) done. Loss: 0.0077  lr:0.000010
[ Sat Jul  6 23:27:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0136  lr:0.000010
[ Sat Jul  6 23:27:23 2024 ] 	Batch(2200/7879) done. Loss: 0.0192  lr:0.000010
[ Sat Jul  6 23:27:41 2024 ] 	Batch(2300/7879) done. Loss: 0.1590  lr:0.000010
[ Sat Jul  6 23:28:00 2024 ] 	Batch(2400/7879) done. Loss: 0.0055  lr:0.000010
[ Sat Jul  6 23:28:18 2024 ] 
Training: Epoch [38/120], Step [2499], Loss: 0.022923992946743965, Training Accuracy: 97.86
[ Sat Jul  6 23:28:18 2024 ] 	Batch(2500/7879) done. Loss: 0.0939  lr:0.000010
[ Sat Jul  6 23:28:36 2024 ] 	Batch(2600/7879) done. Loss: 0.0441  lr:0.000010
[ Sat Jul  6 23:28:54 2024 ] 	Batch(2700/7879) done. Loss: 0.1723  lr:0.000010
[ Sat Jul  6 23:29:12 2024 ] 	Batch(2800/7879) done. Loss: 0.0146  lr:0.000010
[ Sat Jul  6 23:29:31 2024 ] 	Batch(2900/7879) done. Loss: 0.3114  lr:0.000010
[ Sat Jul  6 23:29:49 2024 ] 
Training: Epoch [38/120], Step [2999], Loss: 0.010776524432003498, Training Accuracy: 97.82083333333334
[ Sat Jul  6 23:29:49 2024 ] 	Batch(3000/7879) done. Loss: 0.0245  lr:0.000010
[ Sat Jul  6 23:30:08 2024 ] 	Batch(3100/7879) done. Loss: 0.1895  lr:0.000010
[ Sat Jul  6 23:30:26 2024 ] 	Batch(3200/7879) done. Loss: 0.0578  lr:0.000010
[ Sat Jul  6 23:30:44 2024 ] 	Batch(3300/7879) done. Loss: 0.1201  lr:0.000010
[ Sat Jul  6 23:31:02 2024 ] 	Batch(3400/7879) done. Loss: 0.0677  lr:0.000010
[ Sat Jul  6 23:31:20 2024 ] 
Training: Epoch [38/120], Step [3499], Loss: 0.31964111328125, Training Accuracy: 97.84642857142856
[ Sat Jul  6 23:31:20 2024 ] 	Batch(3500/7879) done. Loss: 0.0110  lr:0.000010
[ Sat Jul  6 23:31:38 2024 ] 	Batch(3600/7879) done. Loss: 0.0027  lr:0.000010
[ Sat Jul  6 23:31:56 2024 ] 	Batch(3700/7879) done. Loss: 0.0288  lr:0.000010
[ Sat Jul  6 23:32:14 2024 ] 	Batch(3800/7879) done. Loss: 0.0647  lr:0.000010
[ Sat Jul  6 23:32:32 2024 ] 	Batch(3900/7879) done. Loss: 0.0187  lr:0.000010
[ Sat Jul  6 23:32:50 2024 ] 
Training: Epoch [38/120], Step [3999], Loss: 0.00047191468183882535, Training Accuracy: 97.85000000000001
[ Sat Jul  6 23:32:50 2024 ] 	Batch(4000/7879) done. Loss: 0.0271  lr:0.000010
[ Sat Jul  6 23:33:08 2024 ] 	Batch(4100/7879) done. Loss: 0.0697  lr:0.000010
[ Sat Jul  6 23:33:26 2024 ] 	Batch(4200/7879) done. Loss: 0.0244  lr:0.000010
[ Sat Jul  6 23:33:44 2024 ] 	Batch(4300/7879) done. Loss: 0.0150  lr:0.000010
[ Sat Jul  6 23:34:02 2024 ] 	Batch(4400/7879) done. Loss: 0.1673  lr:0.000010
[ Sat Jul  6 23:34:20 2024 ] 
Training: Epoch [38/120], Step [4499], Loss: 0.004343011882156134, Training Accuracy: 97.85555555555555
[ Sat Jul  6 23:34:20 2024 ] 	Batch(4500/7879) done. Loss: 0.0292  lr:0.000010
[ Sat Jul  6 23:34:38 2024 ] 	Batch(4600/7879) done. Loss: 0.2523  lr:0.000010
[ Sat Jul  6 23:34:56 2024 ] 	Batch(4700/7879) done. Loss: 0.0433  lr:0.000010
[ Sat Jul  6 23:35:14 2024 ] 	Batch(4800/7879) done. Loss: 0.0021  lr:0.000010
[ Sat Jul  6 23:35:32 2024 ] 	Batch(4900/7879) done. Loss: 0.0776  lr:0.000010
[ Sat Jul  6 23:35:50 2024 ] 
Training: Epoch [38/120], Step [4999], Loss: 0.04849943891167641, Training Accuracy: 97.87
[ Sat Jul  6 23:35:50 2024 ] 	Batch(5000/7879) done. Loss: 0.0269  lr:0.000010
[ Sat Jul  6 23:36:08 2024 ] 	Batch(5100/7879) done. Loss: 0.0215  lr:0.000010
[ Sat Jul  6 23:36:26 2024 ] 	Batch(5200/7879) done. Loss: 0.0485  lr:0.000010
[ Sat Jul  6 23:36:44 2024 ] 	Batch(5300/7879) done. Loss: 0.0658  lr:0.000010
[ Sat Jul  6 23:37:03 2024 ] 	Batch(5400/7879) done. Loss: 0.0779  lr:0.000010
[ Sat Jul  6 23:37:21 2024 ] 
Training: Epoch [38/120], Step [5499], Loss: 0.019309839233756065, Training Accuracy: 97.85227272727272
[ Sat Jul  6 23:37:21 2024 ] 	Batch(5500/7879) done. Loss: 0.0533  lr:0.000010
[ Sat Jul  6 23:37:40 2024 ] 	Batch(5600/7879) done. Loss: 0.0558  lr:0.000010
[ Sat Jul  6 23:37:58 2024 ] 	Batch(5700/7879) done. Loss: 0.3530  lr:0.000010
[ Sat Jul  6 23:38:16 2024 ] 	Batch(5800/7879) done. Loss: 0.0920  lr:0.000010
[ Sat Jul  6 23:38:34 2024 ] 	Batch(5900/7879) done. Loss: 0.0014  lr:0.000010
[ Sat Jul  6 23:38:52 2024 ] 
Training: Epoch [38/120], Step [5999], Loss: 0.05633537471294403, Training Accuracy: 97.82083333333334
[ Sat Jul  6 23:38:52 2024 ] 	Batch(6000/7879) done. Loss: 0.1090  lr:0.000010
[ Sat Jul  6 23:39:10 2024 ] 	Batch(6100/7879) done. Loss: 0.0706  lr:0.000010
[ Sat Jul  6 23:39:28 2024 ] 	Batch(6200/7879) done. Loss: 0.0138  lr:0.000010
[ Sat Jul  6 23:39:46 2024 ] 	Batch(6300/7879) done. Loss: 0.0184  lr:0.000010
[ Sat Jul  6 23:40:04 2024 ] 	Batch(6400/7879) done. Loss: 0.3049  lr:0.000010
[ Sat Jul  6 23:40:22 2024 ] 
Training: Epoch [38/120], Step [6499], Loss: 0.012412386946380138, Training Accuracy: 97.8
[ Sat Jul  6 23:40:22 2024 ] 	Batch(6500/7879) done. Loss: 0.0631  lr:0.000010
[ Sat Jul  6 23:40:40 2024 ] 	Batch(6600/7879) done. Loss: 0.1569  lr:0.000010
[ Sat Jul  6 23:40:58 2024 ] 	Batch(6700/7879) done. Loss: 0.1372  lr:0.000010
[ Sat Jul  6 23:41:16 2024 ] 	Batch(6800/7879) done. Loss: 0.1668  lr:0.000010
[ Sat Jul  6 23:41:34 2024 ] 	Batch(6900/7879) done. Loss: 0.1428  lr:0.000010
[ Sat Jul  6 23:41:52 2024 ] 
Training: Epoch [38/120], Step [6999], Loss: 0.06829573214054108, Training Accuracy: 97.8125
[ Sat Jul  6 23:41:52 2024 ] 	Batch(7000/7879) done. Loss: 0.0081  lr:0.000010
[ Sat Jul  6 23:42:10 2024 ] 	Batch(7100/7879) done. Loss: 0.0770  lr:0.000010
[ Sat Jul  6 23:42:28 2024 ] 	Batch(7200/7879) done. Loss: 0.0084  lr:0.000010
[ Sat Jul  6 23:42:46 2024 ] 	Batch(7300/7879) done. Loss: 0.0231  lr:0.000010
[ Sat Jul  6 23:43:04 2024 ] 	Batch(7400/7879) done. Loss: 0.0990  lr:0.000010
[ Sat Jul  6 23:43:22 2024 ] 
Training: Epoch [38/120], Step [7499], Loss: 0.00752084469422698, Training Accuracy: 97.82666666666667
[ Sat Jul  6 23:43:22 2024 ] 	Batch(7500/7879) done. Loss: 0.0053  lr:0.000010
[ Sat Jul  6 23:43:40 2024 ] 	Batch(7600/7879) done. Loss: 0.0988  lr:0.000010
[ Sat Jul  6 23:43:58 2024 ] 	Batch(7700/7879) done. Loss: 0.0389  lr:0.000010
[ Sat Jul  6 23:44:16 2024 ] 	Batch(7800/7879) done. Loss: 0.0338  lr:0.000010
[ Sat Jul  6 23:44:30 2024 ] 	Mean training loss: 0.0860.
[ Sat Jul  6 23:44:30 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul  6 23:44:30 2024 ] Training epoch: 40
[ Sat Jul  6 23:44:31 2024 ] 	Batch(0/7879) done. Loss: 0.0642  lr:0.000010
[ Sat Jul  6 23:44:49 2024 ] 	Batch(100/7879) done. Loss: 0.0242  lr:0.000010
[ Sat Jul  6 23:45:07 2024 ] 	Batch(200/7879) done. Loss: 0.1892  lr:0.000010
[ Sat Jul  6 23:45:26 2024 ] 	Batch(300/7879) done. Loss: 0.0506  lr:0.000010
[ Sat Jul  6 23:45:44 2024 ] 	Batch(400/7879) done. Loss: 0.0010  lr:0.000010
[ Sat Jul  6 23:46:02 2024 ] 
Training: Epoch [39/120], Step [499], Loss: 0.16648778319358826, Training Accuracy: 97.925
[ Sat Jul  6 23:46:02 2024 ] 	Batch(500/7879) done. Loss: 0.0327  lr:0.000010
[ Sat Jul  6 23:46:20 2024 ] 	Batch(600/7879) done. Loss: 0.0379  lr:0.000010
[ Sat Jul  6 23:46:38 2024 ] 	Batch(700/7879) done. Loss: 0.0091  lr:0.000010
[ Sat Jul  6 23:46:56 2024 ] 	Batch(800/7879) done. Loss: 0.0248  lr:0.000010
[ Sat Jul  6 23:47:14 2024 ] 	Batch(900/7879) done. Loss: 0.2924  lr:0.000010
[ Sat Jul  6 23:47:32 2024 ] 
Training: Epoch [39/120], Step [999], Loss: 0.2903936505317688, Training Accuracy: 97.9375
[ Sat Jul  6 23:47:32 2024 ] 	Batch(1000/7879) done. Loss: 0.0120  lr:0.000010
[ Sat Jul  6 23:47:50 2024 ] 	Batch(1100/7879) done. Loss: 0.1946  lr:0.000010
[ Sat Jul  6 23:48:08 2024 ] 	Batch(1200/7879) done. Loss: 0.0873  lr:0.000010
[ Sat Jul  6 23:48:26 2024 ] 	Batch(1300/7879) done. Loss: 0.0333  lr:0.000010
[ Sat Jul  6 23:48:44 2024 ] 	Batch(1400/7879) done. Loss: 0.0201  lr:0.000010
[ Sat Jul  6 23:49:01 2024 ] 
Training: Epoch [39/120], Step [1499], Loss: 0.20076213777065277, Training Accuracy: 97.93333333333332
[ Sat Jul  6 23:49:02 2024 ] 	Batch(1500/7879) done. Loss: 0.0171  lr:0.000010
[ Sat Jul  6 23:49:20 2024 ] 	Batch(1600/7879) done. Loss: 0.0080  lr:0.000010
[ Sat Jul  6 23:49:38 2024 ] 	Batch(1700/7879) done. Loss: 0.0669  lr:0.000010
[ Sat Jul  6 23:49:56 2024 ] 	Batch(1800/7879) done. Loss: 0.0668  lr:0.000010
[ Sat Jul  6 23:50:14 2024 ] 	Batch(1900/7879) done. Loss: 0.0304  lr:0.000010
[ Sat Jul  6 23:50:31 2024 ] 
Training: Epoch [39/120], Step [1999], Loss: 0.010832455940544605, Training Accuracy: 97.91250000000001
[ Sat Jul  6 23:50:32 2024 ] 	Batch(2000/7879) done. Loss: 0.1597  lr:0.000010
[ Sat Jul  6 23:50:50 2024 ] 	Batch(2100/7879) done. Loss: 0.0997  lr:0.000010
[ Sat Jul  6 23:51:09 2024 ] 	Batch(2200/7879) done. Loss: 0.0134  lr:0.000010
[ Sat Jul  6 23:51:28 2024 ] 	Batch(2300/7879) done. Loss: 0.0359  lr:0.000010
[ Sat Jul  6 23:51:46 2024 ] 	Batch(2400/7879) done. Loss: 0.0340  lr:0.000010
[ Sat Jul  6 23:52:05 2024 ] 
Training: Epoch [39/120], Step [2499], Loss: 0.04672388732433319, Training Accuracy: 97.86
[ Sat Jul  6 23:52:05 2024 ] 	Batch(2500/7879) done. Loss: 0.0357  lr:0.000010
[ Sat Jul  6 23:52:23 2024 ] 	Batch(2600/7879) done. Loss: 0.2592  lr:0.000010
[ Sat Jul  6 23:52:42 2024 ] 	Batch(2700/7879) done. Loss: 0.0484  lr:0.000010
[ Sat Jul  6 23:53:00 2024 ] 	Batch(2800/7879) done. Loss: 0.5622  lr:0.000010
[ Sat Jul  6 23:53:18 2024 ] 	Batch(2900/7879) done. Loss: 0.0764  lr:0.000010
[ Sat Jul  6 23:53:36 2024 ] 
Training: Epoch [39/120], Step [2999], Loss: 0.04275451600551605, Training Accuracy: 97.86666666666667
[ Sat Jul  6 23:53:36 2024 ] 	Batch(3000/7879) done. Loss: 0.1854  lr:0.000010
[ Sat Jul  6 23:53:54 2024 ] 	Batch(3100/7879) done. Loss: 0.0234  lr:0.000010
[ Sat Jul  6 23:54:12 2024 ] 	Batch(3200/7879) done. Loss: 0.1348  lr:0.000010
[ Sat Jul  6 23:54:30 2024 ] 	Batch(3300/7879) done. Loss: 0.0642  lr:0.000010
[ Sat Jul  6 23:54:48 2024 ] 	Batch(3400/7879) done. Loss: 0.0647  lr:0.000010
[ Sat Jul  6 23:55:05 2024 ] 
Training: Epoch [39/120], Step [3499], Loss: 0.074797622859478, Training Accuracy: 97.84285714285714
[ Sat Jul  6 23:55:05 2024 ] 	Batch(3500/7879) done. Loss: 0.1880  lr:0.000010
[ Sat Jul  6 23:55:24 2024 ] 	Batch(3600/7879) done. Loss: 0.0455  lr:0.000010
[ Sat Jul  6 23:55:42 2024 ] 	Batch(3700/7879) done. Loss: 0.1980  lr:0.000010
[ Sat Jul  6 23:56:01 2024 ] 	Batch(3800/7879) done. Loss: 0.1718  lr:0.000010
[ Sat Jul  6 23:56:19 2024 ] 	Batch(3900/7879) done. Loss: 0.0654  lr:0.000010
[ Sat Jul  6 23:56:38 2024 ] 
Training: Epoch [39/120], Step [3999], Loss: 0.010448487475514412, Training Accuracy: 97.8875
[ Sat Jul  6 23:56:38 2024 ] 	Batch(4000/7879) done. Loss: 0.0011  lr:0.000010
[ Sat Jul  6 23:56:57 2024 ] 	Batch(4100/7879) done. Loss: 0.3507  lr:0.000010
[ Sat Jul  6 23:57:15 2024 ] 	Batch(4200/7879) done. Loss: 0.0264  lr:0.000010
[ Sat Jul  6 23:57:34 2024 ] 	Batch(4300/7879) done. Loss: 0.0105  lr:0.000010
[ Sat Jul  6 23:57:53 2024 ] 	Batch(4400/7879) done. Loss: 0.0486  lr:0.000010
[ Sat Jul  6 23:58:11 2024 ] 
Training: Epoch [39/120], Step [4499], Loss: 0.04468013346195221, Training Accuracy: 97.87222222222223
[ Sat Jul  6 23:58:11 2024 ] 	Batch(4500/7879) done. Loss: 0.0216  lr:0.000010
[ Sat Jul  6 23:58:30 2024 ] 	Batch(4600/7879) done. Loss: 0.0341  lr:0.000010
[ Sat Jul  6 23:58:48 2024 ] 	Batch(4700/7879) done. Loss: 0.0232  lr:0.000010
[ Sat Jul  6 23:59:07 2024 ] 	Batch(4800/7879) done. Loss: 0.0638  lr:0.000010
[ Sat Jul  6 23:59:26 2024 ] 	Batch(4900/7879) done. Loss: 0.0362  lr:0.000010
[ Sat Jul  6 23:59:44 2024 ] 
Training: Epoch [39/120], Step [4999], Loss: 0.041146159172058105, Training Accuracy: 97.905
[ Sat Jul  6 23:59:44 2024 ] 	Batch(5000/7879) done. Loss: 0.0144  lr:0.000010
[ Sun Jul  7 00:00:02 2024 ] 	Batch(5100/7879) done. Loss: 0.1462  lr:0.000010
[ Sun Jul  7 00:00:20 2024 ] 	Batch(5200/7879) done. Loss: 0.0408  lr:0.000010
[ Sun Jul  7 00:00:38 2024 ] 	Batch(5300/7879) done. Loss: 0.1223  lr:0.000010
[ Sun Jul  7 00:00:56 2024 ] 	Batch(5400/7879) done. Loss: 0.4648  lr:0.000010
[ Sun Jul  7 00:01:14 2024 ] 
Training: Epoch [39/120], Step [5499], Loss: 0.09302747994661331, Training Accuracy: 97.87727272727273
[ Sun Jul  7 00:01:14 2024 ] 	Batch(5500/7879) done. Loss: 0.3838  lr:0.000010
[ Sun Jul  7 00:01:32 2024 ] 	Batch(5600/7879) done. Loss: 0.0883  lr:0.000010
[ Sun Jul  7 00:01:50 2024 ] 	Batch(5700/7879) done. Loss: 0.0015  lr:0.000010
[ Sun Jul  7 00:02:08 2024 ] 	Batch(5800/7879) done. Loss: 0.0064  lr:0.000010
[ Sun Jul  7 00:02:26 2024 ] 	Batch(5900/7879) done. Loss: 0.0097  lr:0.000010
[ Sun Jul  7 00:02:44 2024 ] 
Training: Epoch [39/120], Step [5999], Loss: 0.01817956194281578, Training Accuracy: 97.87708333333333
[ Sun Jul  7 00:02:44 2024 ] 	Batch(6000/7879) done. Loss: 0.1427  lr:0.000010
[ Sun Jul  7 00:03:03 2024 ] 	Batch(6100/7879) done. Loss: 0.1338  lr:0.000010
[ Sun Jul  7 00:03:21 2024 ] 	Batch(6200/7879) done. Loss: 0.1791  lr:0.000010
[ Sun Jul  7 00:03:40 2024 ] 	Batch(6300/7879) done. Loss: 0.2646  lr:0.000010
[ Sun Jul  7 00:03:59 2024 ] 	Batch(6400/7879) done. Loss: 0.1490  lr:0.000010
[ Sun Jul  7 00:04:17 2024 ] 
Training: Epoch [39/120], Step [6499], Loss: 0.006324250251054764, Training Accuracy: 97.86923076923077
[ Sun Jul  7 00:04:17 2024 ] 	Batch(6500/7879) done. Loss: 0.0011  lr:0.000010
[ Sun Jul  7 00:04:36 2024 ] 	Batch(6600/7879) done. Loss: 0.1724  lr:0.000010
[ Sun Jul  7 00:04:55 2024 ] 	Batch(6700/7879) done. Loss: 0.1135  lr:0.000010
[ Sun Jul  7 00:05:13 2024 ] 	Batch(6800/7879) done. Loss: 0.0568  lr:0.000010
[ Sun Jul  7 00:05:31 2024 ] 	Batch(6900/7879) done. Loss: 0.0087  lr:0.000010
[ Sun Jul  7 00:05:48 2024 ] 
Training: Epoch [39/120], Step [6999], Loss: 0.007660123985260725, Training Accuracy: 97.90535714285714
[ Sun Jul  7 00:05:49 2024 ] 	Batch(7000/7879) done. Loss: 0.0238  lr:0.000010
[ Sun Jul  7 00:06:07 2024 ] 	Batch(7100/7879) done. Loss: 0.0453  lr:0.000010
[ Sun Jul  7 00:06:25 2024 ] 	Batch(7200/7879) done. Loss: 0.0069  lr:0.000010
[ Sun Jul  7 00:06:43 2024 ] 	Batch(7300/7879) done. Loss: 0.0134  lr:0.000010
[ Sun Jul  7 00:07:00 2024 ] 	Batch(7400/7879) done. Loss: 0.0120  lr:0.000010
[ Sun Jul  7 00:07:18 2024 ] 
Training: Epoch [39/120], Step [7499], Loss: 0.07709558308124542, Training Accuracy: 97.92666666666666
[ Sun Jul  7 00:07:18 2024 ] 	Batch(7500/7879) done. Loss: 0.1004  lr:0.000010
[ Sun Jul  7 00:07:36 2024 ] 	Batch(7600/7879) done. Loss: 0.0788  lr:0.000010
[ Sun Jul  7 00:07:55 2024 ] 	Batch(7700/7879) done. Loss: 0.2045  lr:0.000010
[ Sun Jul  7 00:08:13 2024 ] 	Batch(7800/7879) done. Loss: 0.1182  lr:0.000010
[ Sun Jul  7 00:08:27 2024 ] 	Mean training loss: 0.0880.
[ Sun Jul  7 00:08:27 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 00:08:27 2024 ] Eval epoch: 40
[ Sun Jul  7 00:13:13 2024 ] 	Mean val loss of 6365 batches: 1.6347129832459821.
[ Sun Jul  7 00:13:13 2024 ] 
Validation: Epoch [39/120], Samples [39564.0/50919], Loss: 0.03193681314587593, Validation Accuracy: 77.69987627408237
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 1 : 201 / 275 = 73 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 2 : 212 / 273 = 77 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 3 : 225 / 273 = 82 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 4 : 221 / 275 = 80 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 5 : 243 / 275 = 88 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 6 : 229 / 275 = 83 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 7 : 253 / 273 = 92 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 8 : 263 / 273 = 96 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 9 : 188 / 273 = 68 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 10 : 120 / 273 = 43 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 11 : 134 / 272 = 49 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 12 : 222 / 271 = 81 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 13 : 268 / 275 = 97 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 14 : 268 / 276 = 97 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 15 : 235 / 273 = 86 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 16 : 224 / 274 = 81 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 17 : 240 / 273 = 87 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 18 : 242 / 274 = 88 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 19 : 256 / 272 = 94 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 20 : 250 / 273 = 91 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 21 : 219 / 274 = 79 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 22 : 232 / 274 = 84 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 23 : 256 / 276 = 92 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 24 : 223 / 274 = 81 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 25 : 260 / 275 = 94 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 26 : 271 / 276 = 98 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 27 : 215 / 275 = 78 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 28 : 162 / 275 = 58 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 29 : 147 / 275 = 53 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 30 : 180 / 276 = 65 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 31 : 241 / 276 = 87 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 32 : 243 / 276 = 88 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 33 : 236 / 276 = 85 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 34 : 241 / 276 = 87 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 35 : 241 / 275 = 87 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 36 : 232 / 276 = 84 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 37 : 244 / 276 = 88 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 38 : 254 / 276 = 92 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 39 : 249 / 276 = 90 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 40 : 191 / 276 = 69 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 41 : 260 / 276 = 94 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 42 : 259 / 275 = 94 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 43 : 182 / 276 = 65 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 44 : 247 / 276 = 89 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 45 : 248 / 276 = 89 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 46 : 223 / 276 = 80 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 47 : 222 / 275 = 80 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 48 : 228 / 275 = 82 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 49 : 230 / 274 = 83 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 50 : 243 / 276 = 88 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 51 : 256 / 276 = 92 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 52 : 238 / 276 = 86 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 53 : 233 / 276 = 84 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 54 : 270 / 274 = 98 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 55 : 239 / 276 = 86 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 56 : 242 / 275 = 88 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 57 : 266 / 276 = 96 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 58 : 268 / 273 = 98 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 59 : 267 / 276 = 96 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 60 : 474 / 561 = 84 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 61 : 458 / 566 = 80 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 62 : 448 / 572 = 78 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 63 : 525 / 570 = 92 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 64 : 440 / 574 = 76 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 65 : 496 / 573 = 86 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 66 : 388 / 573 = 67 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 67 : 412 / 575 = 71 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 68 : 324 / 575 = 56 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 69 : 453 / 575 = 78 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 70 : 231 / 575 = 40 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 71 : 212 / 575 = 36 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 72 : 149 / 571 = 26 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 73 : 223 / 570 = 39 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 74 : 386 / 569 = 67 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 75 : 241 / 573 = 42 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 76 : 364 / 574 = 63 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 77 : 398 / 573 = 69 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 78 : 410 / 575 = 71 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 79 : 543 / 574 = 94 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 80 : 503 / 573 = 87 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 81 : 327 / 575 = 56 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 82 : 376 / 575 = 65 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 83 : 322 / 572 = 56 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 84 : 442 / 574 = 77 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 85 : 416 / 574 = 72 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 86 : 513 / 575 = 89 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 87 : 504 / 576 = 87 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 88 : 426 / 575 = 74 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 89 : 492 / 576 = 85 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 90 : 249 / 574 = 43 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 91 : 462 / 568 = 81 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 92 : 366 / 576 = 63 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 93 : 423 / 573 = 73 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 94 : 508 / 574 = 88 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 95 : 518 / 575 = 90 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 96 : 562 / 575 = 97 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 97 : 553 / 574 = 96 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 98 : 544 / 575 = 94 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 99 : 546 / 574 = 95 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 100 : 502 / 574 = 87 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 101 : 513 / 574 = 89 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 102 : 341 / 575 = 59 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 103 : 489 / 576 = 84 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 104 : 294 / 575 = 51 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 105 : 239 / 575 = 41 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 106 : 321 / 576 = 55 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 107 : 486 / 576 = 84 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 108 : 496 / 575 = 86 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 109 : 341 / 575 = 59 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 110 : 484 / 575 = 84 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 111 : 535 / 576 = 92 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 112 : 549 / 575 = 95 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 113 : 511 / 576 = 88 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 114 : 472 / 576 = 81 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 115 : 521 / 576 = 90 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 116 : 467 / 575 = 81 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 117 : 468 / 575 = 81 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 118 : 493 / 575 = 85 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 119 : 532 / 576 = 92 %
[ Sun Jul  7 00:13:13 2024 ] Accuracy of 120 : 231 / 274 = 84 %
[ Sun Jul  7 00:13:13 2024 ] Training epoch: 41
[ Sun Jul  7 00:13:14 2024 ] 	Batch(0/7879) done. Loss: 0.0433  lr:0.000010
[ Sun Jul  7 00:13:32 2024 ] 	Batch(100/7879) done. Loss: 0.0162  lr:0.000010
[ Sun Jul  7 00:13:50 2024 ] 	Batch(200/7879) done. Loss: 0.7109  lr:0.000010
[ Sun Jul  7 00:14:08 2024 ] 	Batch(300/7879) done. Loss: 0.0282  lr:0.000010
[ Sun Jul  7 00:14:26 2024 ] 	Batch(400/7879) done. Loss: 0.0339  lr:0.000010
[ Sun Jul  7 00:14:44 2024 ] 
Training: Epoch [40/120], Step [499], Loss: 0.004352633375674486, Training Accuracy: 97.7
[ Sun Jul  7 00:14:44 2024 ] 	Batch(500/7879) done. Loss: 0.0429  lr:0.000010
[ Sun Jul  7 00:15:02 2024 ] 	Batch(600/7879) done. Loss: 0.0174  lr:0.000010
[ Sun Jul  7 00:15:19 2024 ] 	Batch(700/7879) done. Loss: 0.0122  lr:0.000010
[ Sun Jul  7 00:15:38 2024 ] 	Batch(800/7879) done. Loss: 0.0984  lr:0.000010
[ Sun Jul  7 00:15:55 2024 ] 	Batch(900/7879) done. Loss: 0.0238  lr:0.000010
[ Sun Jul  7 00:16:13 2024 ] 
Training: Epoch [40/120], Step [999], Loss: 0.04536091536283493, Training Accuracy: 97.6375
[ Sun Jul  7 00:16:13 2024 ] 	Batch(1000/7879) done. Loss: 0.1480  lr:0.000010
[ Sun Jul  7 00:16:31 2024 ] 	Batch(1100/7879) done. Loss: 0.0541  lr:0.000010
[ Sun Jul  7 00:16:49 2024 ] 	Batch(1200/7879) done. Loss: 0.0315  lr:0.000010
[ Sun Jul  7 00:17:07 2024 ] 	Batch(1300/7879) done. Loss: 0.1341  lr:0.000010
[ Sun Jul  7 00:17:25 2024 ] 	Batch(1400/7879) done. Loss: 0.0837  lr:0.000010
[ Sun Jul  7 00:17:43 2024 ] 
Training: Epoch [40/120], Step [1499], Loss: 0.08533462136983871, Training Accuracy: 97.7
[ Sun Jul  7 00:17:43 2024 ] 	Batch(1500/7879) done. Loss: 0.0057  lr:0.000010
[ Sun Jul  7 00:18:01 2024 ] 	Batch(1600/7879) done. Loss: 0.0291  lr:0.000010
[ Sun Jul  7 00:18:19 2024 ] 	Batch(1700/7879) done. Loss: 0.0115  lr:0.000010
[ Sun Jul  7 00:18:37 2024 ] 	Batch(1800/7879) done. Loss: 0.0290  lr:0.000010
[ Sun Jul  7 00:18:55 2024 ] 	Batch(1900/7879) done. Loss: 0.0231  lr:0.000010
[ Sun Jul  7 00:19:13 2024 ] 
Training: Epoch [40/120], Step [1999], Loss: 0.00980995874851942, Training Accuracy: 97.71875
[ Sun Jul  7 00:19:13 2024 ] 	Batch(2000/7879) done. Loss: 0.0047  lr:0.000010
[ Sun Jul  7 00:19:31 2024 ] 	Batch(2100/7879) done. Loss: 0.2549  lr:0.000010
[ Sun Jul  7 00:19:49 2024 ] 	Batch(2200/7879) done. Loss: 0.0124  lr:0.000010
[ Sun Jul  7 00:20:07 2024 ] 	Batch(2300/7879) done. Loss: 0.1831  lr:0.000010
[ Sun Jul  7 00:20:25 2024 ] 	Batch(2400/7879) done. Loss: 0.0165  lr:0.000010
[ Sun Jul  7 00:20:42 2024 ] 
Training: Epoch [40/120], Step [2499], Loss: 0.08735271543264389, Training Accuracy: 97.76
[ Sun Jul  7 00:20:43 2024 ] 	Batch(2500/7879) done. Loss: 0.1044  lr:0.000010
[ Sun Jul  7 00:21:00 2024 ] 	Batch(2600/7879) done. Loss: 0.1010  lr:0.000010
[ Sun Jul  7 00:21:18 2024 ] 	Batch(2700/7879) done. Loss: 0.0105  lr:0.000010
[ Sun Jul  7 00:21:36 2024 ] 	Batch(2800/7879) done. Loss: 0.0996  lr:0.000010
[ Sun Jul  7 00:21:54 2024 ] 	Batch(2900/7879) done. Loss: 0.0485  lr:0.000010
[ Sun Jul  7 00:22:12 2024 ] 
Training: Epoch [40/120], Step [2999], Loss: 0.3275088965892792, Training Accuracy: 97.77916666666667
[ Sun Jul  7 00:22:12 2024 ] 	Batch(3000/7879) done. Loss: 0.0155  lr:0.000010
[ Sun Jul  7 00:22:30 2024 ] 	Batch(3100/7879) done. Loss: 0.2263  lr:0.000010
[ Sun Jul  7 00:22:48 2024 ] 	Batch(3200/7879) done. Loss: 0.0414  lr:0.000010
[ Sun Jul  7 00:23:06 2024 ] 	Batch(3300/7879) done. Loss: 0.1217  lr:0.000010
[ Sun Jul  7 00:23:24 2024 ] 	Batch(3400/7879) done. Loss: 0.0981  lr:0.000010
[ Sun Jul  7 00:23:42 2024 ] 
Training: Epoch [40/120], Step [3499], Loss: 0.09862945973873138, Training Accuracy: 97.73571428571428
[ Sun Jul  7 00:23:43 2024 ] 	Batch(3500/7879) done. Loss: 0.0029  lr:0.000010
[ Sun Jul  7 00:24:01 2024 ] 	Batch(3600/7879) done. Loss: 0.0176  lr:0.000010
[ Sun Jul  7 00:24:20 2024 ] 	Batch(3700/7879) done. Loss: 0.0204  lr:0.000010
[ Sun Jul  7 00:24:38 2024 ] 	Batch(3800/7879) done. Loss: 0.2619  lr:0.000010
[ Sun Jul  7 00:24:57 2024 ] 	Batch(3900/7879) done. Loss: 0.1015  lr:0.000010
[ Sun Jul  7 00:25:15 2024 ] 
Training: Epoch [40/120], Step [3999], Loss: 0.03946861997246742, Training Accuracy: 97.76875000000001
[ Sun Jul  7 00:25:16 2024 ] 	Batch(4000/7879) done. Loss: 0.0339  lr:0.000010
[ Sun Jul  7 00:25:33 2024 ] 	Batch(4100/7879) done. Loss: 0.0160  lr:0.000010
[ Sun Jul  7 00:25:51 2024 ] 	Batch(4200/7879) done. Loss: 0.0041  lr:0.000010
[ Sun Jul  7 00:26:09 2024 ] 	Batch(4300/7879) done. Loss: 0.0064  lr:0.000010
[ Sun Jul  7 00:26:27 2024 ] 	Batch(4400/7879) done. Loss: 0.0450  lr:0.000010
[ Sun Jul  7 00:26:45 2024 ] 
Training: Epoch [40/120], Step [4499], Loss: 0.16434305906295776, Training Accuracy: 97.75
[ Sun Jul  7 00:26:45 2024 ] 	Batch(4500/7879) done. Loss: 0.0216  lr:0.000010
[ Sun Jul  7 00:27:03 2024 ] 	Batch(4600/7879) done. Loss: 0.0010  lr:0.000010
[ Sun Jul  7 00:27:21 2024 ] 	Batch(4700/7879) done. Loss: 0.0596  lr:0.000010
[ Sun Jul  7 00:27:39 2024 ] 	Batch(4800/7879) done. Loss: 0.0504  lr:0.000010
[ Sun Jul  7 00:27:58 2024 ] 	Batch(4900/7879) done. Loss: 0.1522  lr:0.000010
[ Sun Jul  7 00:28:16 2024 ] 
Training: Epoch [40/120], Step [4999], Loss: 0.0189780592918396, Training Accuracy: 97.75500000000001
[ Sun Jul  7 00:28:16 2024 ] 	Batch(5000/7879) done. Loss: 0.2852  lr:0.000010
[ Sun Jul  7 00:28:35 2024 ] 	Batch(5100/7879) done. Loss: 0.1525  lr:0.000010
[ Sun Jul  7 00:28:54 2024 ] 	Batch(5200/7879) done. Loss: 0.1660  lr:0.000010
[ Sun Jul  7 00:29:12 2024 ] 	Batch(5300/7879) done. Loss: 0.2069  lr:0.000010
[ Sun Jul  7 00:29:31 2024 ] 	Batch(5400/7879) done. Loss: 0.0748  lr:0.000010
[ Sun Jul  7 00:29:49 2024 ] 
Training: Epoch [40/120], Step [5499], Loss: 0.2914431393146515, Training Accuracy: 97.74772727272727
[ Sun Jul  7 00:29:49 2024 ] 	Batch(5500/7879) done. Loss: 0.1758  lr:0.000010
[ Sun Jul  7 00:30:08 2024 ] 	Batch(5600/7879) done. Loss: 0.1596  lr:0.000010
[ Sun Jul  7 00:30:27 2024 ] 	Batch(5700/7879) done. Loss: 0.0390  lr:0.000010
[ Sun Jul  7 00:30:45 2024 ] 	Batch(5800/7879) done. Loss: 0.1094  lr:0.000010
[ Sun Jul  7 00:31:04 2024 ] 	Batch(5900/7879) done. Loss: 0.0011  lr:0.000010
[ Sun Jul  7 00:31:22 2024 ] 
Training: Epoch [40/120], Step [5999], Loss: 0.07015474885702133, Training Accuracy: 97.77291666666666
[ Sun Jul  7 00:31:22 2024 ] 	Batch(6000/7879) done. Loss: 0.0636  lr:0.000010
[ Sun Jul  7 00:31:40 2024 ] 	Batch(6100/7879) done. Loss: 0.0151  lr:0.000010
[ Sun Jul  7 00:31:58 2024 ] 	Batch(6200/7879) done. Loss: 0.0041  lr:0.000010
[ Sun Jul  7 00:32:16 2024 ] 	Batch(6300/7879) done. Loss: 0.1086  lr:0.000010
[ Sun Jul  7 00:32:34 2024 ] 	Batch(6400/7879) done. Loss: 0.0383  lr:0.000010
[ Sun Jul  7 00:32:52 2024 ] 
Training: Epoch [40/120], Step [6499], Loss: 0.05002637207508087, Training Accuracy: 97.8
[ Sun Jul  7 00:32:52 2024 ] 	Batch(6500/7879) done. Loss: 0.0252  lr:0.000010
[ Sun Jul  7 00:33:10 2024 ] 	Batch(6600/7879) done. Loss: 0.0283  lr:0.000010
[ Sun Jul  7 00:33:28 2024 ] 	Batch(6700/7879) done. Loss: 0.0788  lr:0.000010
[ Sun Jul  7 00:33:46 2024 ] 	Batch(6800/7879) done. Loss: 0.0182  lr:0.000010
[ Sun Jul  7 00:34:04 2024 ] 	Batch(6900/7879) done. Loss: 0.0134  lr:0.000010
[ Sun Jul  7 00:34:22 2024 ] 
Training: Epoch [40/120], Step [6999], Loss: 0.19383972883224487, Training Accuracy: 97.80714285714286
[ Sun Jul  7 00:34:22 2024 ] 	Batch(7000/7879) done. Loss: 0.0997  lr:0.000010
[ Sun Jul  7 00:34:40 2024 ] 	Batch(7100/7879) done. Loss: 0.0125  lr:0.000010
[ Sun Jul  7 00:34:58 2024 ] 	Batch(7200/7879) done. Loss: 0.0348  lr:0.000010
[ Sun Jul  7 00:35:16 2024 ] 	Batch(7300/7879) done. Loss: 0.1126  lr:0.000010
[ Sun Jul  7 00:35:34 2024 ] 	Batch(7400/7879) done. Loss: 0.0645  lr:0.000010
[ Sun Jul  7 00:35:52 2024 ] 
Training: Epoch [40/120], Step [7499], Loss: 0.02599131129682064, Training Accuracy: 97.78833333333333
[ Sun Jul  7 00:35:53 2024 ] 	Batch(7500/7879) done. Loss: 0.0240  lr:0.000010
[ Sun Jul  7 00:36:11 2024 ] 	Batch(7600/7879) done. Loss: 0.0432  lr:0.000010
[ Sun Jul  7 00:36:29 2024 ] 	Batch(7700/7879) done. Loss: 0.2339  lr:0.000010
[ Sun Jul  7 00:36:48 2024 ] 	Batch(7800/7879) done. Loss: 0.0170  lr:0.000010
[ Sun Jul  7 00:37:02 2024 ] 	Mean training loss: 0.0919.
[ Sun Jul  7 00:37:02 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 00:37:02 2024 ] Training epoch: 42
[ Sun Jul  7 00:37:03 2024 ] 	Batch(0/7879) done. Loss: 0.0765  lr:0.000010
[ Sun Jul  7 00:37:21 2024 ] 	Batch(100/7879) done. Loss: 0.0512  lr:0.000010
[ Sun Jul  7 00:37:39 2024 ] 	Batch(200/7879) done. Loss: 0.0047  lr:0.000010
[ Sun Jul  7 00:37:57 2024 ] 	Batch(300/7879) done. Loss: 0.0383  lr:0.000010
[ Sun Jul  7 00:38:15 2024 ] 	Batch(400/7879) done. Loss: 0.0260  lr:0.000010
[ Sun Jul  7 00:38:33 2024 ] 
Training: Epoch [41/120], Step [499], Loss: 0.02954782359302044, Training Accuracy: 97.52499999999999
[ Sun Jul  7 00:38:33 2024 ] 	Batch(500/7879) done. Loss: 0.0376  lr:0.000010
[ Sun Jul  7 00:38:51 2024 ] 	Batch(600/7879) done. Loss: 0.0747  lr:0.000010
[ Sun Jul  7 00:39:09 2024 ] 	Batch(700/7879) done. Loss: 0.0130  lr:0.000010
[ Sun Jul  7 00:39:27 2024 ] 	Batch(800/7879) done. Loss: 0.1253  lr:0.000010
[ Sun Jul  7 00:39:45 2024 ] 	Batch(900/7879) done. Loss: 0.0167  lr:0.000010
[ Sun Jul  7 00:40:03 2024 ] 
Training: Epoch [41/120], Step [999], Loss: 0.037317417562007904, Training Accuracy: 97.8125
[ Sun Jul  7 00:40:03 2024 ] 	Batch(1000/7879) done. Loss: 0.1320  lr:0.000010
[ Sun Jul  7 00:40:22 2024 ] 	Batch(1100/7879) done. Loss: 0.0241  lr:0.000010
[ Sun Jul  7 00:40:40 2024 ] 	Batch(1200/7879) done. Loss: 0.0827  lr:0.000010
[ Sun Jul  7 00:40:58 2024 ] 	Batch(1300/7879) done. Loss: 0.1151  lr:0.000010
[ Sun Jul  7 00:41:17 2024 ] 	Batch(1400/7879) done. Loss: 0.1310  lr:0.000010
[ Sun Jul  7 00:41:35 2024 ] 
Training: Epoch [41/120], Step [1499], Loss: 0.0018459067214280367, Training Accuracy: 97.76666666666667
[ Sun Jul  7 00:41:35 2024 ] 	Batch(1500/7879) done. Loss: 0.0566  lr:0.000010
[ Sun Jul  7 00:41:54 2024 ] 	Batch(1600/7879) done. Loss: 0.0110  lr:0.000010
[ Sun Jul  7 00:42:12 2024 ] 	Batch(1700/7879) done. Loss: 0.0122  lr:0.000010
[ Sun Jul  7 00:42:30 2024 ] 	Batch(1800/7879) done. Loss: 0.1676  lr:0.000010
[ Sun Jul  7 00:42:48 2024 ] 	Batch(1900/7879) done. Loss: 0.0367  lr:0.000010
[ Sun Jul  7 00:43:06 2024 ] 
Training: Epoch [41/120], Step [1999], Loss: 0.06939952820539474, Training Accuracy: 97.8875
[ Sun Jul  7 00:43:06 2024 ] 	Batch(2000/7879) done. Loss: 0.1124  lr:0.000010
[ Sun Jul  7 00:43:24 2024 ] 	Batch(2100/7879) done. Loss: 0.1837  lr:0.000010
[ Sun Jul  7 00:43:42 2024 ] 	Batch(2200/7879) done. Loss: 0.0022  lr:0.000010
[ Sun Jul  7 00:44:00 2024 ] 	Batch(2300/7879) done. Loss: 0.0656  lr:0.000010
[ Sun Jul  7 00:44:18 2024 ] 	Batch(2400/7879) done. Loss: 0.0689  lr:0.000010
[ Sun Jul  7 00:44:35 2024 ] 
Training: Epoch [41/120], Step [2499], Loss: 0.14160512387752533, Training Accuracy: 97.89
[ Sun Jul  7 00:44:36 2024 ] 	Batch(2500/7879) done. Loss: 0.2388  lr:0.000010
[ Sun Jul  7 00:44:53 2024 ] 	Batch(2600/7879) done. Loss: 0.0141  lr:0.000010
[ Sun Jul  7 00:45:11 2024 ] 	Batch(2700/7879) done. Loss: 0.3995  lr:0.000010
[ Sun Jul  7 00:45:29 2024 ] 	Batch(2800/7879) done. Loss: 0.0405  lr:0.000010
[ Sun Jul  7 00:45:48 2024 ] 	Batch(2900/7879) done. Loss: 0.0030  lr:0.000010
[ Sun Jul  7 00:46:06 2024 ] 
Training: Epoch [41/120], Step [2999], Loss: 0.030997691676020622, Training Accuracy: 97.80416666666667
[ Sun Jul  7 00:46:07 2024 ] 	Batch(3000/7879) done. Loss: 0.1566  lr:0.000010
[ Sun Jul  7 00:46:25 2024 ] 	Batch(3100/7879) done. Loss: 0.0558  lr:0.000010
[ Sun Jul  7 00:46:44 2024 ] 	Batch(3200/7879) done. Loss: 0.0442  lr:0.000010
[ Sun Jul  7 00:47:02 2024 ] 	Batch(3300/7879) done. Loss: 0.1095  lr:0.000010
[ Sun Jul  7 00:47:21 2024 ] 	Batch(3400/7879) done. Loss: 0.0161  lr:0.000010
[ Sun Jul  7 00:47:39 2024 ] 
Training: Epoch [41/120], Step [3499], Loss: 0.0044960202649235725, Training Accuracy: 97.82142857142857
[ Sun Jul  7 00:47:40 2024 ] 	Batch(3500/7879) done. Loss: 0.1756  lr:0.000010
[ Sun Jul  7 00:47:58 2024 ] 	Batch(3600/7879) done. Loss: 0.0764  lr:0.000010
[ Sun Jul  7 00:48:17 2024 ] 	Batch(3700/7879) done. Loss: 0.1653  lr:0.000010
[ Sun Jul  7 00:48:35 2024 ] 	Batch(3800/7879) done. Loss: 0.0164  lr:0.000010
[ Sun Jul  7 00:48:54 2024 ] 	Batch(3900/7879) done. Loss: 0.0843  lr:0.000010
[ Sun Jul  7 00:49:12 2024 ] 
Training: Epoch [41/120], Step [3999], Loss: 0.01464321743696928, Training Accuracy: 97.79062499999999
[ Sun Jul  7 00:49:13 2024 ] 	Batch(4000/7879) done. Loss: 0.0291  lr:0.000010
[ Sun Jul  7 00:49:31 2024 ] 	Batch(4100/7879) done. Loss: 0.2380  lr:0.000010
[ Sun Jul  7 00:49:50 2024 ] 	Batch(4200/7879) done. Loss: 0.0049  lr:0.000010
[ Sun Jul  7 00:50:08 2024 ] 	Batch(4300/7879) done. Loss: 0.0385  lr:0.000010
[ Sun Jul  7 00:50:27 2024 ] 	Batch(4400/7879) done. Loss: 0.0511  lr:0.000010
[ Sun Jul  7 00:50:45 2024 ] 
Training: Epoch [41/120], Step [4499], Loss: 0.0003400742425583303, Training Accuracy: 97.775
[ Sun Jul  7 00:50:45 2024 ] 	Batch(4500/7879) done. Loss: 0.0280  lr:0.000010
[ Sun Jul  7 00:51:03 2024 ] 	Batch(4600/7879) done. Loss: 0.0031  lr:0.000010
[ Sun Jul  7 00:51:21 2024 ] 	Batch(4700/7879) done. Loss: 0.0969  lr:0.000010
[ Sun Jul  7 00:51:39 2024 ] 	Batch(4800/7879) done. Loss: 0.0073  lr:0.000010
[ Sun Jul  7 00:51:58 2024 ] 	Batch(4900/7879) done. Loss: 0.0859  lr:0.000010
[ Sun Jul  7 00:52:16 2024 ] 
Training: Epoch [41/120], Step [4999], Loss: 0.01849444769322872, Training Accuracy: 97.78999999999999
[ Sun Jul  7 00:52:16 2024 ] 	Batch(5000/7879) done. Loss: 0.0399  lr:0.000010
[ Sun Jul  7 00:52:35 2024 ] 	Batch(5100/7879) done. Loss: 0.0070  lr:0.000010
[ Sun Jul  7 00:52:53 2024 ] 	Batch(5200/7879) done. Loss: 0.1926  lr:0.000010
[ Sun Jul  7 00:53:12 2024 ] 	Batch(5300/7879) done. Loss: 0.0967  lr:0.000010
[ Sun Jul  7 00:53:31 2024 ] 	Batch(5400/7879) done. Loss: 0.0052  lr:0.000010
[ Sun Jul  7 00:53:49 2024 ] 
Training: Epoch [41/120], Step [5499], Loss: 0.041073694825172424, Training Accuracy: 97.80454545454546
[ Sun Jul  7 00:53:49 2024 ] 	Batch(5500/7879) done. Loss: 0.0383  lr:0.000010
[ Sun Jul  7 00:54:08 2024 ] 	Batch(5600/7879) done. Loss: 0.0342  lr:0.000010
[ Sun Jul  7 00:54:26 2024 ] 	Batch(5700/7879) done. Loss: 0.0255  lr:0.000010
[ Sun Jul  7 00:54:44 2024 ] 	Batch(5800/7879) done. Loss: 0.0235  lr:0.000010
[ Sun Jul  7 00:55:02 2024 ] 	Batch(5900/7879) done. Loss: 0.0280  lr:0.000010
[ Sun Jul  7 00:55:19 2024 ] 
Training: Epoch [41/120], Step [5999], Loss: 0.01745494268834591, Training Accuracy: 97.82291666666667
[ Sun Jul  7 00:55:20 2024 ] 	Batch(6000/7879) done. Loss: 0.0064  lr:0.000010
[ Sun Jul  7 00:55:38 2024 ] 	Batch(6100/7879) done. Loss: 0.0012  lr:0.000010
[ Sun Jul  7 00:55:57 2024 ] 	Batch(6200/7879) done. Loss: 0.0113  lr:0.000010
[ Sun Jul  7 00:56:15 2024 ] 	Batch(6300/7879) done. Loss: 0.1154  lr:0.000010
[ Sun Jul  7 00:56:34 2024 ] 	Batch(6400/7879) done. Loss: 0.0320  lr:0.000010
[ Sun Jul  7 00:56:51 2024 ] 
Training: Epoch [41/120], Step [6499], Loss: 0.007504470180720091, Training Accuracy: 97.83461538461539
[ Sun Jul  7 00:56:52 2024 ] 	Batch(6500/7879) done. Loss: 0.3579  lr:0.000010
[ Sun Jul  7 00:57:09 2024 ] 	Batch(6600/7879) done. Loss: 0.3008  lr:0.000010
[ Sun Jul  7 00:57:27 2024 ] 	Batch(6700/7879) done. Loss: 0.2095  lr:0.000010
[ Sun Jul  7 00:57:46 2024 ] 	Batch(6800/7879) done. Loss: 0.2378  lr:0.000010
[ Sun Jul  7 00:58:03 2024 ] 	Batch(6900/7879) done. Loss: 0.0360  lr:0.000010
[ Sun Jul  7 00:58:21 2024 ] 
Training: Epoch [41/120], Step [6999], Loss: 0.12401669472455978, Training Accuracy: 97.83214285714286
[ Sun Jul  7 00:58:21 2024 ] 	Batch(7000/7879) done. Loss: 0.3626  lr:0.000010
[ Sun Jul  7 00:58:39 2024 ] 	Batch(7100/7879) done. Loss: 0.0207  lr:0.000010
[ Sun Jul  7 00:58:57 2024 ] 	Batch(7200/7879) done. Loss: 0.2145  lr:0.000010
[ Sun Jul  7 00:59:15 2024 ] 	Batch(7300/7879) done. Loss: 0.0165  lr:0.000010
[ Sun Jul  7 00:59:33 2024 ] 	Batch(7400/7879) done. Loss: 0.0257  lr:0.000010
[ Sun Jul  7 00:59:51 2024 ] 
Training: Epoch [41/120], Step [7499], Loss: 0.412878155708313, Training Accuracy: 97.815
[ Sun Jul  7 00:59:51 2024 ] 	Batch(7500/7879) done. Loss: 0.0927  lr:0.000010
[ Sun Jul  7 01:00:09 2024 ] 	Batch(7600/7879) done. Loss: 0.1290  lr:0.000010
[ Sun Jul  7 01:00:27 2024 ] 	Batch(7700/7879) done. Loss: 0.0542  lr:0.000010
[ Sun Jul  7 01:00:45 2024 ] 	Batch(7800/7879) done. Loss: 0.0483  lr:0.000010
[ Sun Jul  7 01:00:59 2024 ] 	Mean training loss: 0.0896.
[ Sun Jul  7 01:00:59 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 01:00:59 2024 ] Training epoch: 43
[ Sun Jul  7 01:01:00 2024 ] 	Batch(0/7879) done. Loss: 0.0185  lr:0.000010
[ Sun Jul  7 01:01:18 2024 ] 	Batch(100/7879) done. Loss: 0.0163  lr:0.000010
[ Sun Jul  7 01:01:36 2024 ] 	Batch(200/7879) done. Loss: 0.0167  lr:0.000010
[ Sun Jul  7 01:01:54 2024 ] 	Batch(300/7879) done. Loss: 0.0061  lr:0.000010
[ Sun Jul  7 01:02:12 2024 ] 	Batch(400/7879) done. Loss: 0.0421  lr:0.000010
[ Sun Jul  7 01:02:30 2024 ] 
Training: Epoch [42/120], Step [499], Loss: 0.061940811574459076, Training Accuracy: 98.0
[ Sun Jul  7 01:02:30 2024 ] 	Batch(500/7879) done. Loss: 0.0066  lr:0.000010
[ Sun Jul  7 01:02:48 2024 ] 	Batch(600/7879) done. Loss: 0.1501  lr:0.000010
[ Sun Jul  7 01:03:06 2024 ] 	Batch(700/7879) done. Loss: 0.0326  lr:0.000010
[ Sun Jul  7 01:03:24 2024 ] 	Batch(800/7879) done. Loss: 0.0540  lr:0.000010
[ Sun Jul  7 01:03:42 2024 ] 	Batch(900/7879) done. Loss: 0.0315  lr:0.000010
[ Sun Jul  7 01:03:59 2024 ] 
Training: Epoch [42/120], Step [999], Loss: 0.0037404403556138277, Training Accuracy: 97.925
[ Sun Jul  7 01:04:00 2024 ] 	Batch(1000/7879) done. Loss: 0.0215  lr:0.000010
[ Sun Jul  7 01:04:18 2024 ] 	Batch(1100/7879) done. Loss: 0.0326  lr:0.000010
[ Sun Jul  7 01:04:36 2024 ] 	Batch(1200/7879) done. Loss: 0.0018  lr:0.000010
[ Sun Jul  7 01:04:54 2024 ] 	Batch(1300/7879) done. Loss: 0.4482  lr:0.000010
[ Sun Jul  7 01:05:12 2024 ] 	Batch(1400/7879) done. Loss: 0.0365  lr:0.000010
[ Sun Jul  7 01:05:29 2024 ] 
Training: Epoch [42/120], Step [1499], Loss: 0.03964146226644516, Training Accuracy: 97.89166666666667
[ Sun Jul  7 01:05:29 2024 ] 	Batch(1500/7879) done. Loss: 0.0305  lr:0.000010
[ Sun Jul  7 01:05:48 2024 ] 	Batch(1600/7879) done. Loss: 0.1164  lr:0.000010
[ Sun Jul  7 01:06:06 2024 ] 	Batch(1700/7879) done. Loss: 0.0071  lr:0.000010
[ Sun Jul  7 01:06:24 2024 ] 	Batch(1800/7879) done. Loss: 0.0831  lr:0.000010
[ Sun Jul  7 01:06:41 2024 ] 	Batch(1900/7879) done. Loss: 0.0270  lr:0.000010
[ Sun Jul  7 01:06:59 2024 ] 
Training: Epoch [42/120], Step [1999], Loss: 0.0008770247222855687, Training Accuracy: 97.8875
[ Sun Jul  7 01:07:00 2024 ] 	Batch(2000/7879) done. Loss: 0.0942  lr:0.000010
[ Sun Jul  7 01:07:18 2024 ] 	Batch(2100/7879) done. Loss: 0.1029  lr:0.000010
[ Sun Jul  7 01:07:37 2024 ] 	Batch(2200/7879) done. Loss: 0.1269  lr:0.000010
[ Sun Jul  7 01:07:55 2024 ] 	Batch(2300/7879) done. Loss: 0.2888  lr:0.000010
[ Sun Jul  7 01:08:14 2024 ] 	Batch(2400/7879) done. Loss: 0.2084  lr:0.000010
[ Sun Jul  7 01:08:32 2024 ] 
Training: Epoch [42/120], Step [2499], Loss: 0.022525286301970482, Training Accuracy: 97.955
[ Sun Jul  7 01:08:32 2024 ] 	Batch(2500/7879) done. Loss: 0.0232  lr:0.000010
[ Sun Jul  7 01:08:50 2024 ] 	Batch(2600/7879) done. Loss: 0.0543  lr:0.000010
[ Sun Jul  7 01:09:08 2024 ] 	Batch(2700/7879) done. Loss: 0.0371  lr:0.000010
[ Sun Jul  7 01:09:26 2024 ] 	Batch(2800/7879) done. Loss: 0.0990  lr:0.000010
[ Sun Jul  7 01:09:44 2024 ] 	Batch(2900/7879) done. Loss: 0.0380  lr:0.000010
[ Sun Jul  7 01:10:01 2024 ] 
Training: Epoch [42/120], Step [2999], Loss: 0.0779653787612915, Training Accuracy: 98.00416666666666
[ Sun Jul  7 01:10:02 2024 ] 	Batch(3000/7879) done. Loss: 0.0897  lr:0.000010
[ Sun Jul  7 01:10:20 2024 ] 	Batch(3100/7879) done. Loss: 0.0921  lr:0.000010
[ Sun Jul  7 01:10:38 2024 ] 	Batch(3200/7879) done. Loss: 0.0365  lr:0.000010
[ Sun Jul  7 01:10:56 2024 ] 	Batch(3300/7879) done. Loss: 0.0213  lr:0.000010
[ Sun Jul  7 01:11:13 2024 ] 	Batch(3400/7879) done. Loss: 0.1405  lr:0.000010
[ Sun Jul  7 01:11:31 2024 ] 
Training: Epoch [42/120], Step [3499], Loss: 0.5387431979179382, Training Accuracy: 97.97142857142858
[ Sun Jul  7 01:11:31 2024 ] 	Batch(3500/7879) done. Loss: 0.1905  lr:0.000010
[ Sun Jul  7 01:11:49 2024 ] 	Batch(3600/7879) done. Loss: 0.0331  lr:0.000010
[ Sun Jul  7 01:12:08 2024 ] 	Batch(3700/7879) done. Loss: 0.0493  lr:0.000010
[ Sun Jul  7 01:12:25 2024 ] 	Batch(3800/7879) done. Loss: 0.2891  lr:0.000010
[ Sun Jul  7 01:12:43 2024 ] 	Batch(3900/7879) done. Loss: 0.3475  lr:0.000010
[ Sun Jul  7 01:13:01 2024 ] 
Training: Epoch [42/120], Step [3999], Loss: 0.0873074010014534, Training Accuracy: 97.953125
[ Sun Jul  7 01:13:01 2024 ] 	Batch(4000/7879) done. Loss: 0.0066  lr:0.000010
[ Sun Jul  7 01:13:19 2024 ] 	Batch(4100/7879) done. Loss: 0.1138  lr:0.000010
[ Sun Jul  7 01:13:37 2024 ] 	Batch(4200/7879) done. Loss: 0.0551  lr:0.000010
[ Sun Jul  7 01:13:55 2024 ] 	Batch(4300/7879) done. Loss: 0.0097  lr:0.000010
[ Sun Jul  7 01:14:13 2024 ] 	Batch(4400/7879) done. Loss: 0.1731  lr:0.000010
[ Sun Jul  7 01:14:31 2024 ] 
Training: Epoch [42/120], Step [4499], Loss: 0.0875261127948761, Training Accuracy: 97.95277777777778
[ Sun Jul  7 01:14:31 2024 ] 	Batch(4500/7879) done. Loss: 0.0389  lr:0.000010
[ Sun Jul  7 01:14:49 2024 ] 	Batch(4600/7879) done. Loss: 0.0062  lr:0.000010
[ Sun Jul  7 01:15:07 2024 ] 	Batch(4700/7879) done. Loss: 0.0319  lr:0.000010
[ Sun Jul  7 01:15:25 2024 ] 	Batch(4800/7879) done. Loss: 0.0061  lr:0.000010
[ Sun Jul  7 01:15:43 2024 ] 	Batch(4900/7879) done. Loss: 0.1336  lr:0.000010
[ Sun Jul  7 01:16:01 2024 ] 
Training: Epoch [42/120], Step [4999], Loss: 0.014163792133331299, Training Accuracy: 97.97
[ Sun Jul  7 01:16:01 2024 ] 	Batch(5000/7879) done. Loss: 0.2434  lr:0.000010
[ Sun Jul  7 01:16:19 2024 ] 	Batch(5100/7879) done. Loss: 0.0576  lr:0.000010
[ Sun Jul  7 01:16:37 2024 ] 	Batch(5200/7879) done. Loss: 0.0469  lr:0.000010
[ Sun Jul  7 01:16:55 2024 ] 	Batch(5300/7879) done. Loss: 0.0195  lr:0.000010
[ Sun Jul  7 01:17:14 2024 ] 	Batch(5400/7879) done. Loss: 0.1168  lr:0.000010
[ Sun Jul  7 01:17:31 2024 ] 
Training: Epoch [42/120], Step [5499], Loss: 0.005430246237665415, Training Accuracy: 97.98409090909091
[ Sun Jul  7 01:17:32 2024 ] 	Batch(5500/7879) done. Loss: 0.0150  lr:0.000010
[ Sun Jul  7 01:17:50 2024 ] 	Batch(5600/7879) done. Loss: 0.0025  lr:0.000010
[ Sun Jul  7 01:18:08 2024 ] 	Batch(5700/7879) done. Loss: 0.0084  lr:0.000010
[ Sun Jul  7 01:18:26 2024 ] 	Batch(5800/7879) done. Loss: 0.0037  lr:0.000010
[ Sun Jul  7 01:18:44 2024 ] 	Batch(5900/7879) done. Loss: 0.0131  lr:0.000010
[ Sun Jul  7 01:19:02 2024 ] 
Training: Epoch [42/120], Step [5999], Loss: 0.04037417471408844, Training Accuracy: 97.97916666666666
[ Sun Jul  7 01:19:03 2024 ] 	Batch(6000/7879) done. Loss: 0.0651  lr:0.000010
[ Sun Jul  7 01:19:20 2024 ] 	Batch(6100/7879) done. Loss: 0.2391  lr:0.000010
[ Sun Jul  7 01:19:38 2024 ] 	Batch(6200/7879) done. Loss: 0.0366  lr:0.000010
[ Sun Jul  7 01:19:56 2024 ] 	Batch(6300/7879) done. Loss: 0.1700  lr:0.000010
[ Sun Jul  7 01:20:14 2024 ] 	Batch(6400/7879) done. Loss: 0.0119  lr:0.000010
[ Sun Jul  7 01:20:32 2024 ] 
Training: Epoch [42/120], Step [6499], Loss: 0.0041503203101456165, Training Accuracy: 97.9826923076923
[ Sun Jul  7 01:20:32 2024 ] 	Batch(6500/7879) done. Loss: 0.5387  lr:0.000010
[ Sun Jul  7 01:20:50 2024 ] 	Batch(6600/7879) done. Loss: 0.0035  lr:0.000010
[ Sun Jul  7 01:21:08 2024 ] 	Batch(6700/7879) done. Loss: 0.0037  lr:0.000010
[ Sun Jul  7 01:21:26 2024 ] 	Batch(6800/7879) done. Loss: 0.0477  lr:0.000010
[ Sun Jul  7 01:21:45 2024 ] 	Batch(6900/7879) done. Loss: 0.1384  lr:0.000010
[ Sun Jul  7 01:22:03 2024 ] 
Training: Epoch [42/120], Step [6999], Loss: 0.011103232391178608, Training Accuracy: 97.98392857142856
[ Sun Jul  7 01:22:04 2024 ] 	Batch(7000/7879) done. Loss: 0.1931  lr:0.000010
[ Sun Jul  7 01:22:22 2024 ] 	Batch(7100/7879) done. Loss: 0.0070  lr:0.000010
[ Sun Jul  7 01:22:41 2024 ] 	Batch(7200/7879) done. Loss: 0.1106  lr:0.000010
[ Sun Jul  7 01:22:59 2024 ] 	Batch(7300/7879) done. Loss: 0.0189  lr:0.000010
[ Sun Jul  7 01:23:17 2024 ] 	Batch(7400/7879) done. Loss: 0.0578  lr:0.000010
[ Sun Jul  7 01:23:35 2024 ] 
Training: Epoch [42/120], Step [7499], Loss: 0.01075331587344408, Training Accuracy: 97.98166666666667
[ Sun Jul  7 01:23:35 2024 ] 	Batch(7500/7879) done. Loss: 0.0547  lr:0.000010
[ Sun Jul  7 01:23:53 2024 ] 	Batch(7600/7879) done. Loss: 0.0046  lr:0.000010
[ Sun Jul  7 01:24:12 2024 ] 	Batch(7700/7879) done. Loss: 0.0809  lr:0.000010
[ Sun Jul  7 01:24:31 2024 ] 	Batch(7800/7879) done. Loss: 0.0066  lr:0.000010
[ Sun Jul  7 01:24:45 2024 ] 	Mean training loss: 0.0853.
[ Sun Jul  7 01:24:45 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 01:24:45 2024 ] Training epoch: 44
[ Sun Jul  7 01:24:46 2024 ] 	Batch(0/7879) done. Loss: 0.0141  lr:0.000010
[ Sun Jul  7 01:25:04 2024 ] 	Batch(100/7879) done. Loss: 0.3678  lr:0.000010
[ Sun Jul  7 01:25:22 2024 ] 	Batch(200/7879) done. Loss: 0.7701  lr:0.000010
[ Sun Jul  7 01:25:41 2024 ] 	Batch(300/7879) done. Loss: 0.0032  lr:0.000010
[ Sun Jul  7 01:25:59 2024 ] 	Batch(400/7879) done. Loss: 0.0855  lr:0.000010
[ Sun Jul  7 01:26:17 2024 ] 
Training: Epoch [43/120], Step [499], Loss: 0.04942715913057327, Training Accuracy: 97.7
[ Sun Jul  7 01:26:18 2024 ] 	Batch(500/7879) done. Loss: 0.0411  lr:0.000010
[ Sun Jul  7 01:26:36 2024 ] 	Batch(600/7879) done. Loss: 0.1121  lr:0.000010
[ Sun Jul  7 01:26:54 2024 ] 	Batch(700/7879) done. Loss: 0.0700  lr:0.000010
[ Sun Jul  7 01:27:13 2024 ] 	Batch(800/7879) done. Loss: 0.6655  lr:0.000010
[ Sun Jul  7 01:27:31 2024 ] 	Batch(900/7879) done. Loss: 0.0960  lr:0.000010
[ Sun Jul  7 01:27:49 2024 ] 
Training: Epoch [43/120], Step [999], Loss: 0.03864863142371178, Training Accuracy: 97.78750000000001
[ Sun Jul  7 01:27:49 2024 ] 	Batch(1000/7879) done. Loss: 0.0233  lr:0.000010
[ Sun Jul  7 01:28:07 2024 ] 	Batch(1100/7879) done. Loss: 0.1309  lr:0.000010
[ Sun Jul  7 01:28:25 2024 ] 	Batch(1200/7879) done. Loss: 0.0114  lr:0.000010
[ Sun Jul  7 01:28:43 2024 ] 	Batch(1300/7879) done. Loss: 0.0261  lr:0.000010
[ Sun Jul  7 01:29:01 2024 ] 	Batch(1400/7879) done. Loss: 0.0049  lr:0.000010
[ Sun Jul  7 01:29:19 2024 ] 
Training: Epoch [43/120], Step [1499], Loss: 0.08320464193820953, Training Accuracy: 97.85833333333333
[ Sun Jul  7 01:29:19 2024 ] 	Batch(1500/7879) done. Loss: 0.2184  lr:0.000010
[ Sun Jul  7 01:29:37 2024 ] 	Batch(1600/7879) done. Loss: 0.0773  lr:0.000010
[ Sun Jul  7 01:29:55 2024 ] 	Batch(1700/7879) done. Loss: 0.0639  lr:0.000010
[ Sun Jul  7 01:30:13 2024 ] 	Batch(1800/7879) done. Loss: 0.0881  lr:0.000010
[ Sun Jul  7 01:30:31 2024 ] 	Batch(1900/7879) done. Loss: 0.0531  lr:0.000010
[ Sun Jul  7 01:30:49 2024 ] 
Training: Epoch [43/120], Step [1999], Loss: 0.05396108329296112, Training Accuracy: 97.78125
[ Sun Jul  7 01:30:49 2024 ] 	Batch(2000/7879) done. Loss: 0.1234  lr:0.000010
[ Sun Jul  7 01:31:07 2024 ] 	Batch(2100/7879) done. Loss: 0.0034  lr:0.000010
[ Sun Jul  7 01:31:25 2024 ] 	Batch(2200/7879) done. Loss: 0.0077  lr:0.000010
[ Sun Jul  7 01:31:43 2024 ] 	Batch(2300/7879) done. Loss: 0.0186  lr:0.000010
[ Sun Jul  7 01:32:01 2024 ] 	Batch(2400/7879) done. Loss: 0.0863  lr:0.000010
[ Sun Jul  7 01:32:19 2024 ] 
Training: Epoch [43/120], Step [2499], Loss: 0.18074840307235718, Training Accuracy: 97.82499999999999
[ Sun Jul  7 01:32:19 2024 ] 	Batch(2500/7879) done. Loss: 0.0895  lr:0.000010
[ Sun Jul  7 01:32:37 2024 ] 	Batch(2600/7879) done. Loss: 0.0263  lr:0.000010
[ Sun Jul  7 01:32:55 2024 ] 	Batch(2700/7879) done. Loss: 0.0058  lr:0.000010
[ Sun Jul  7 01:33:13 2024 ] 	Batch(2800/7879) done. Loss: 0.0310  lr:0.000010
[ Sun Jul  7 01:33:31 2024 ] 	Batch(2900/7879) done. Loss: 0.0089  lr:0.000010
[ Sun Jul  7 01:33:50 2024 ] 
Training: Epoch [43/120], Step [2999], Loss: 0.07912278175354004, Training Accuracy: 97.8625
[ Sun Jul  7 01:33:50 2024 ] 	Batch(3000/7879) done. Loss: 0.1065  lr:0.000010
[ Sun Jul  7 01:34:09 2024 ] 	Batch(3100/7879) done. Loss: 0.0141  lr:0.000010
[ Sun Jul  7 01:34:27 2024 ] 	Batch(3200/7879) done. Loss: 0.0070  lr:0.000010
[ Sun Jul  7 01:34:45 2024 ] 	Batch(3300/7879) done. Loss: 0.0304  lr:0.000010
[ Sun Jul  7 01:35:03 2024 ] 	Batch(3400/7879) done. Loss: 0.0679  lr:0.000010
[ Sun Jul  7 01:35:21 2024 ] 
Training: Epoch [43/120], Step [3499], Loss: 0.09068310260772705, Training Accuracy: 97.88571428571429
[ Sun Jul  7 01:35:21 2024 ] 	Batch(3500/7879) done. Loss: 0.2328  lr:0.000010
[ Sun Jul  7 01:35:39 2024 ] 	Batch(3600/7879) done. Loss: 0.0127  lr:0.000010
[ Sun Jul  7 01:35:57 2024 ] 	Batch(3700/7879) done. Loss: 0.0258  lr:0.000010
[ Sun Jul  7 01:36:15 2024 ] 	Batch(3800/7879) done. Loss: 0.0659  lr:0.000010
[ Sun Jul  7 01:36:33 2024 ] 	Batch(3900/7879) done. Loss: 0.0447  lr:0.000010
[ Sun Jul  7 01:36:51 2024 ] 
Training: Epoch [43/120], Step [3999], Loss: 0.050652213394641876, Training Accuracy: 97.85000000000001
[ Sun Jul  7 01:36:51 2024 ] 	Batch(4000/7879) done. Loss: 0.2708  lr:0.000010
[ Sun Jul  7 01:37:09 2024 ] 	Batch(4100/7879) done. Loss: 0.0237  lr:0.000010
[ Sun Jul  7 01:37:27 2024 ] 	Batch(4200/7879) done. Loss: 0.0177  lr:0.000010
[ Sun Jul  7 01:37:45 2024 ] 	Batch(4300/7879) done. Loss: 0.0364  lr:0.000010
[ Sun Jul  7 01:38:03 2024 ] 	Batch(4400/7879) done. Loss: 0.0722  lr:0.000010
[ Sun Jul  7 01:38:21 2024 ] 
Training: Epoch [43/120], Step [4499], Loss: 0.1435142606496811, Training Accuracy: 97.86111111111111
[ Sun Jul  7 01:38:21 2024 ] 	Batch(4500/7879) done. Loss: 0.2251  lr:0.000010
[ Sun Jul  7 01:38:39 2024 ] 	Batch(4600/7879) done. Loss: 0.0192  lr:0.000010
[ Sun Jul  7 01:38:57 2024 ] 	Batch(4700/7879) done. Loss: 0.2138  lr:0.000010
[ Sun Jul  7 01:39:15 2024 ] 	Batch(4800/7879) done. Loss: 0.0890  lr:0.000010
[ Sun Jul  7 01:39:33 2024 ] 	Batch(4900/7879) done. Loss: 0.2349  lr:0.000010
[ Sun Jul  7 01:39:50 2024 ] 
Training: Epoch [43/120], Step [4999], Loss: 0.46435868740081787, Training Accuracy: 97.83
[ Sun Jul  7 01:39:51 2024 ] 	Batch(5000/7879) done. Loss: 0.0049  lr:0.000010
[ Sun Jul  7 01:40:09 2024 ] 	Batch(5100/7879) done. Loss: 0.0474  lr:0.000010
[ Sun Jul  7 01:40:27 2024 ] 	Batch(5200/7879) done. Loss: 0.0850  lr:0.000010
[ Sun Jul  7 01:40:44 2024 ] 	Batch(5300/7879) done. Loss: 0.0797  lr:0.000010
[ Sun Jul  7 01:41:02 2024 ] 	Batch(5400/7879) done. Loss: 0.0214  lr:0.000010
[ Sun Jul  7 01:41:20 2024 ] 
Training: Epoch [43/120], Step [5499], Loss: 0.5153917074203491, Training Accuracy: 97.81590909090909
[ Sun Jul  7 01:41:20 2024 ] 	Batch(5500/7879) done. Loss: 0.0287  lr:0.000010
[ Sun Jul  7 01:41:38 2024 ] 	Batch(5600/7879) done. Loss: 0.0426  lr:0.000010
[ Sun Jul  7 01:41:56 2024 ] 	Batch(5700/7879) done. Loss: 0.0027  lr:0.000010
[ Sun Jul  7 01:42:14 2024 ] 	Batch(5800/7879) done. Loss: 0.0032  lr:0.000010
[ Sun Jul  7 01:42:32 2024 ] 	Batch(5900/7879) done. Loss: 0.0141  lr:0.000010
[ Sun Jul  7 01:42:50 2024 ] 
Training: Epoch [43/120], Step [5999], Loss: 0.015313046053051949, Training Accuracy: 97.8
[ Sun Jul  7 01:42:50 2024 ] 	Batch(6000/7879) done. Loss: 0.0457  lr:0.000010
[ Sun Jul  7 01:43:08 2024 ] 	Batch(6100/7879) done. Loss: 0.0035  lr:0.000010
[ Sun Jul  7 01:43:26 2024 ] 	Batch(6200/7879) done. Loss: 0.0191  lr:0.000010
[ Sun Jul  7 01:43:44 2024 ] 	Batch(6300/7879) done. Loss: 0.0248  lr:0.000010
[ Sun Jul  7 01:44:02 2024 ] 	Batch(6400/7879) done. Loss: 0.2527  lr:0.000010
[ Sun Jul  7 01:44:20 2024 ] 
Training: Epoch [43/120], Step [6499], Loss: 0.5223857760429382, Training Accuracy: 97.80576923076923
[ Sun Jul  7 01:44:20 2024 ] 	Batch(6500/7879) done. Loss: 0.0419  lr:0.000010
[ Sun Jul  7 01:44:38 2024 ] 	Batch(6600/7879) done. Loss: 0.0537  lr:0.000010
[ Sun Jul  7 01:44:56 2024 ] 	Batch(6700/7879) done. Loss: 0.1434  lr:0.000010
[ Sun Jul  7 01:45:14 2024 ] 	Batch(6800/7879) done. Loss: 0.1185  lr:0.000010
[ Sun Jul  7 01:45:32 2024 ] 	Batch(6900/7879) done. Loss: 0.0267  lr:0.000010
[ Sun Jul  7 01:45:49 2024 ] 
Training: Epoch [43/120], Step [6999], Loss: 0.014500666409730911, Training Accuracy: 97.83392857142857
[ Sun Jul  7 01:45:49 2024 ] 	Batch(7000/7879) done. Loss: 0.0197  lr:0.000010
[ Sun Jul  7 01:46:07 2024 ] 	Batch(7100/7879) done. Loss: 0.2842  lr:0.000010
[ Sun Jul  7 01:46:25 2024 ] 	Batch(7200/7879) done. Loss: 0.0588  lr:0.000010
[ Sun Jul  7 01:46:43 2024 ] 	Batch(7300/7879) done. Loss: 0.0277  lr:0.000010
[ Sun Jul  7 01:47:01 2024 ] 	Batch(7400/7879) done. Loss: 0.0579  lr:0.000010
[ Sun Jul  7 01:47:19 2024 ] 
Training: Epoch [43/120], Step [7499], Loss: 0.0059974039904773235, Training Accuracy: 97.87
[ Sun Jul  7 01:47:19 2024 ] 	Batch(7500/7879) done. Loss: 0.1222  lr:0.000010
[ Sun Jul  7 01:47:37 2024 ] 	Batch(7600/7879) done. Loss: 0.2327  lr:0.000010
[ Sun Jul  7 01:47:55 2024 ] 	Batch(7700/7879) done. Loss: 0.0322  lr:0.000010
[ Sun Jul  7 01:48:13 2024 ] 	Batch(7800/7879) done. Loss: 0.0541  lr:0.000010
[ Sun Jul  7 01:48:27 2024 ] 	Mean training loss: 0.0874.
[ Sun Jul  7 01:48:27 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 01:48:27 2024 ] Training epoch: 45
[ Sun Jul  7 01:48:28 2024 ] 	Batch(0/7879) done. Loss: 0.1553  lr:0.000010
[ Sun Jul  7 01:48:46 2024 ] 	Batch(100/7879) done. Loss: 0.0484  lr:0.000010
[ Sun Jul  7 01:49:04 2024 ] 	Batch(200/7879) done. Loss: 0.1940  lr:0.000010
[ Sun Jul  7 01:49:22 2024 ] 	Batch(300/7879) done. Loss: 0.0421  lr:0.000010
[ Sun Jul  7 01:49:40 2024 ] 	Batch(400/7879) done. Loss: 0.0020  lr:0.000010
[ Sun Jul  7 01:49:57 2024 ] 
Training: Epoch [44/120], Step [499], Loss: 0.25052282214164734, Training Accuracy: 97.625
[ Sun Jul  7 01:49:58 2024 ] 	Batch(500/7879) done. Loss: 0.0160  lr:0.000010
[ Sun Jul  7 01:50:15 2024 ] 	Batch(600/7879) done. Loss: 0.0609  lr:0.000010
[ Sun Jul  7 01:50:33 2024 ] 	Batch(700/7879) done. Loss: 0.0331  lr:0.000010
[ Sun Jul  7 01:50:51 2024 ] 	Batch(800/7879) done. Loss: 0.0251  lr:0.000010
[ Sun Jul  7 01:51:10 2024 ] 	Batch(900/7879) done. Loss: 0.2860  lr:0.000010
[ Sun Jul  7 01:51:28 2024 ] 
Training: Epoch [44/120], Step [999], Loss: 0.01968417316675186, Training Accuracy: 97.82499999999999
[ Sun Jul  7 01:51:28 2024 ] 	Batch(1000/7879) done. Loss: 0.1029  lr:0.000010
[ Sun Jul  7 01:51:46 2024 ] 	Batch(1100/7879) done. Loss: 0.2165  lr:0.000010
[ Sun Jul  7 01:52:04 2024 ] 	Batch(1200/7879) done. Loss: 0.0820  lr:0.000010
[ Sun Jul  7 01:52:22 2024 ] 	Batch(1300/7879) done. Loss: 0.0034  lr:0.000010
[ Sun Jul  7 01:52:40 2024 ] 	Batch(1400/7879) done. Loss: 0.0319  lr:0.000010
[ Sun Jul  7 01:52:58 2024 ] 
Training: Epoch [44/120], Step [1499], Loss: 0.014387190341949463, Training Accuracy: 97.81666666666666
[ Sun Jul  7 01:52:58 2024 ] 	Batch(1500/7879) done. Loss: 0.0072  lr:0.000010
[ Sun Jul  7 01:53:16 2024 ] 	Batch(1600/7879) done. Loss: 0.0874  lr:0.000010
[ Sun Jul  7 01:53:34 2024 ] 	Batch(1700/7879) done. Loss: 0.0478  lr:0.000010
[ Sun Jul  7 01:53:52 2024 ] 	Batch(1800/7879) done. Loss: 0.0178  lr:0.000010
[ Sun Jul  7 01:54:10 2024 ] 	Batch(1900/7879) done. Loss: 0.0053  lr:0.000010
[ Sun Jul  7 01:54:28 2024 ] 
Training: Epoch [44/120], Step [1999], Loss: 0.05333693325519562, Training Accuracy: 97.96249999999999
[ Sun Jul  7 01:54:28 2024 ] 	Batch(2000/7879) done. Loss: 0.0204  lr:0.000010
[ Sun Jul  7 01:54:46 2024 ] 	Batch(2100/7879) done. Loss: 0.0515  lr:0.000010
[ Sun Jul  7 01:55:04 2024 ] 	Batch(2200/7879) done. Loss: 0.0301  lr:0.000010
[ Sun Jul  7 01:55:22 2024 ] 	Batch(2300/7879) done. Loss: 0.0189  lr:0.000010
[ Sun Jul  7 01:55:40 2024 ] 	Batch(2400/7879) done. Loss: 0.0079  lr:0.000010
[ Sun Jul  7 01:55:58 2024 ] 
Training: Epoch [44/120], Step [2499], Loss: 0.015386980958282948, Training Accuracy: 97.985
[ Sun Jul  7 01:55:58 2024 ] 	Batch(2500/7879) done. Loss: 0.0405  lr:0.000010
[ Sun Jul  7 01:56:16 2024 ] 	Batch(2600/7879) done. Loss: 0.0779  lr:0.000010
[ Sun Jul  7 01:56:34 2024 ] 	Batch(2700/7879) done. Loss: 0.0230  lr:0.000010
[ Sun Jul  7 01:56:52 2024 ] 	Batch(2800/7879) done. Loss: 0.0314  lr:0.000010
[ Sun Jul  7 01:57:10 2024 ] 	Batch(2900/7879) done. Loss: 0.0139  lr:0.000010
[ Sun Jul  7 01:57:27 2024 ] 
Training: Epoch [44/120], Step [2999], Loss: 0.10982725024223328, Training Accuracy: 98.0
[ Sun Jul  7 01:57:27 2024 ] 	Batch(3000/7879) done. Loss: 0.0063  lr:0.000010
[ Sun Jul  7 01:57:45 2024 ] 	Batch(3100/7879) done. Loss: 0.1060  lr:0.000010
[ Sun Jul  7 01:58:03 2024 ] 	Batch(3200/7879) done. Loss: 0.0100  lr:0.000010
[ Sun Jul  7 01:58:21 2024 ] 	Batch(3300/7879) done. Loss: 0.0219  lr:0.000010
[ Sun Jul  7 01:58:39 2024 ] 	Batch(3400/7879) done. Loss: 0.0946  lr:0.000010
[ Sun Jul  7 01:58:57 2024 ] 
Training: Epoch [44/120], Step [3499], Loss: 0.2302599996328354, Training Accuracy: 97.96785714285714
[ Sun Jul  7 01:58:57 2024 ] 	Batch(3500/7879) done. Loss: 0.2245  lr:0.000010
[ Sun Jul  7 01:59:15 2024 ] 	Batch(3600/7879) done. Loss: 0.0683  lr:0.000010
[ Sun Jul  7 01:59:33 2024 ] 	Batch(3700/7879) done. Loss: 0.0378  lr:0.000010
[ Sun Jul  7 01:59:51 2024 ] 	Batch(3800/7879) done. Loss: 0.1396  lr:0.000010
[ Sun Jul  7 02:00:09 2024 ] 	Batch(3900/7879) done. Loss: 0.2603  lr:0.000010
[ Sun Jul  7 02:00:27 2024 ] 
Training: Epoch [44/120], Step [3999], Loss: 0.031274355947971344, Training Accuracy: 97.95625
[ Sun Jul  7 02:00:27 2024 ] 	Batch(4000/7879) done. Loss: 0.0643  lr:0.000010
[ Sun Jul  7 02:00:45 2024 ] 	Batch(4100/7879) done. Loss: 0.0396  lr:0.000010
[ Sun Jul  7 02:01:03 2024 ] 	Batch(4200/7879) done. Loss: 0.1214  lr:0.000010
[ Sun Jul  7 02:01:21 2024 ] 	Batch(4300/7879) done. Loss: 0.0115  lr:0.000010
[ Sun Jul  7 02:01:39 2024 ] 	Batch(4400/7879) done. Loss: 0.0415  lr:0.000010
[ Sun Jul  7 02:01:57 2024 ] 
Training: Epoch [44/120], Step [4499], Loss: 0.031021500006318092, Training Accuracy: 97.96944444444445
[ Sun Jul  7 02:01:57 2024 ] 	Batch(4500/7879) done. Loss: 0.0545  lr:0.000010
[ Sun Jul  7 02:02:15 2024 ] 	Batch(4600/7879) done. Loss: 0.0365  lr:0.000010
[ Sun Jul  7 02:02:33 2024 ] 	Batch(4700/7879) done. Loss: 0.0045  lr:0.000010
[ Sun Jul  7 02:02:51 2024 ] 	Batch(4800/7879) done. Loss: 0.3651  lr:0.000010
[ Sun Jul  7 02:03:09 2024 ] 	Batch(4900/7879) done. Loss: 0.1090  lr:0.000010
[ Sun Jul  7 02:03:27 2024 ] 
Training: Epoch [44/120], Step [4999], Loss: 0.10896749794483185, Training Accuracy: 97.925
[ Sun Jul  7 02:03:27 2024 ] 	Batch(5000/7879) done. Loss: 0.0346  lr:0.000010
[ Sun Jul  7 02:03:45 2024 ] 	Batch(5100/7879) done. Loss: 0.0170  lr:0.000010
[ Sun Jul  7 02:04:03 2024 ] 	Batch(5200/7879) done. Loss: 0.0295  lr:0.000010
[ Sun Jul  7 02:04:21 2024 ] 	Batch(5300/7879) done. Loss: 0.0023  lr:0.000010
[ Sun Jul  7 02:04:39 2024 ] 	Batch(5400/7879) done. Loss: 0.0272  lr:0.000010
[ Sun Jul  7 02:04:56 2024 ] 
Training: Epoch [44/120], Step [5499], Loss: 0.37424609065055847, Training Accuracy: 97.88636363636364
[ Sun Jul  7 02:04:57 2024 ] 	Batch(5500/7879) done. Loss: 0.1368  lr:0.000010
[ Sun Jul  7 02:05:15 2024 ] 	Batch(5600/7879) done. Loss: 0.0592  lr:0.000010
[ Sun Jul  7 02:05:33 2024 ] 	Batch(5700/7879) done. Loss: 0.2989  lr:0.000010
[ Sun Jul  7 02:05:50 2024 ] 	Batch(5800/7879) done. Loss: 0.0657  lr:0.000010
[ Sun Jul  7 02:06:09 2024 ] 	Batch(5900/7879) done. Loss: 0.0426  lr:0.000010
[ Sun Jul  7 02:06:27 2024 ] 
Training: Epoch [44/120], Step [5999], Loss: 0.006704204250127077, Training Accuracy: 97.90625
[ Sun Jul  7 02:06:27 2024 ] 	Batch(6000/7879) done. Loss: 0.0448  lr:0.000010
[ Sun Jul  7 02:06:45 2024 ] 	Batch(6100/7879) done. Loss: 0.0712  lr:0.000010
[ Sun Jul  7 02:07:04 2024 ] 	Batch(6200/7879) done. Loss: 0.0660  lr:0.000010
[ Sun Jul  7 02:07:23 2024 ] 	Batch(6300/7879) done. Loss: 0.0074  lr:0.000010
[ Sun Jul  7 02:07:41 2024 ] 	Batch(6400/7879) done. Loss: 0.0308  lr:0.000010
[ Sun Jul  7 02:08:00 2024 ] 
Training: Epoch [44/120], Step [6499], Loss: 0.008962534368038177, Training Accuracy: 97.88269230769231
[ Sun Jul  7 02:08:00 2024 ] 	Batch(6500/7879) done. Loss: 0.0269  lr:0.000010
[ Sun Jul  7 02:08:18 2024 ] 	Batch(6600/7879) done. Loss: 0.1326  lr:0.000010
[ Sun Jul  7 02:08:37 2024 ] 	Batch(6700/7879) done. Loss: 0.0094  lr:0.000010
[ Sun Jul  7 02:08:56 2024 ] 	Batch(6800/7879) done. Loss: 0.0348  lr:0.000010
[ Sun Jul  7 02:09:14 2024 ] 	Batch(6900/7879) done. Loss: 0.0273  lr:0.000010
[ Sun Jul  7 02:09:33 2024 ] 
Training: Epoch [44/120], Step [6999], Loss: 0.00691232131794095, Training Accuracy: 97.90892857142856
[ Sun Jul  7 02:09:33 2024 ] 	Batch(7000/7879) done. Loss: 0.0293  lr:0.000010
[ Sun Jul  7 02:09:51 2024 ] 	Batch(7100/7879) done. Loss: 0.1274  lr:0.000010
[ Sun Jul  7 02:10:09 2024 ] 	Batch(7200/7879) done. Loss: 0.0309  lr:0.000010
[ Sun Jul  7 02:10:27 2024 ] 	Batch(7300/7879) done. Loss: 0.0210  lr:0.000010
[ Sun Jul  7 02:10:45 2024 ] 	Batch(7400/7879) done. Loss: 0.1394  lr:0.000010
[ Sun Jul  7 02:11:03 2024 ] 
Training: Epoch [44/120], Step [7499], Loss: 0.02414817363023758, Training Accuracy: 97.91166666666666
[ Sun Jul  7 02:11:03 2024 ] 	Batch(7500/7879) done. Loss: 0.0369  lr:0.000010
[ Sun Jul  7 02:11:21 2024 ] 	Batch(7600/7879) done. Loss: 0.0173  lr:0.000010
[ Sun Jul  7 02:11:39 2024 ] 	Batch(7700/7879) done. Loss: 0.0086  lr:0.000010
[ Sun Jul  7 02:11:57 2024 ] 	Batch(7800/7879) done. Loss: 0.0124  lr:0.000010
[ Sun Jul  7 02:12:11 2024 ] 	Mean training loss: 0.0865.
[ Sun Jul  7 02:12:11 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 02:12:11 2024 ] Training epoch: 46
[ Sun Jul  7 02:12:12 2024 ] 	Batch(0/7879) done. Loss: 0.0200  lr:0.000010
[ Sun Jul  7 02:12:30 2024 ] 	Batch(100/7879) done. Loss: 0.2075  lr:0.000010
[ Sun Jul  7 02:12:48 2024 ] 	Batch(200/7879) done. Loss: 0.0407  lr:0.000010
[ Sun Jul  7 02:13:07 2024 ] 	Batch(300/7879) done. Loss: 0.0046  lr:0.000010
[ Sun Jul  7 02:13:25 2024 ] 	Batch(400/7879) done. Loss: 0.0725  lr:0.000010
[ Sun Jul  7 02:13:43 2024 ] 
Training: Epoch [45/120], Step [499], Loss: 0.017225466668605804, Training Accuracy: 97.925
[ Sun Jul  7 02:13:44 2024 ] 	Batch(500/7879) done. Loss: 0.0437  lr:0.000010
[ Sun Jul  7 02:14:02 2024 ] 	Batch(600/7879) done. Loss: 0.0304  lr:0.000010
[ Sun Jul  7 02:14:20 2024 ] 	Batch(700/7879) done. Loss: 0.9015  lr:0.000010
[ Sun Jul  7 02:14:39 2024 ] 	Batch(800/7879) done. Loss: 0.0020  lr:0.000010
[ Sun Jul  7 02:14:57 2024 ] 	Batch(900/7879) done. Loss: 0.0609  lr:0.000010
[ Sun Jul  7 02:15:15 2024 ] 
Training: Epoch [45/120], Step [999], Loss: 0.2259027510881424, Training Accuracy: 97.8625
[ Sun Jul  7 02:15:15 2024 ] 	Batch(1000/7879) done. Loss: 0.0278  lr:0.000010
[ Sun Jul  7 02:15:34 2024 ] 	Batch(1100/7879) done. Loss: 0.0143  lr:0.000010
[ Sun Jul  7 02:15:52 2024 ] 	Batch(1200/7879) done. Loss: 0.1635  lr:0.000010
[ Sun Jul  7 02:16:10 2024 ] 	Batch(1300/7879) done. Loss: 0.1019  lr:0.000010
[ Sun Jul  7 02:16:28 2024 ] 	Batch(1400/7879) done. Loss: 0.0924  lr:0.000010
[ Sun Jul  7 02:16:46 2024 ] 
Training: Epoch [45/120], Step [1499], Loss: 0.005932099651545286, Training Accuracy: 97.91666666666666
[ Sun Jul  7 02:16:46 2024 ] 	Batch(1500/7879) done. Loss: 0.0342  lr:0.000010
[ Sun Jul  7 02:17:04 2024 ] 	Batch(1600/7879) done. Loss: 0.0185  lr:0.000010
[ Sun Jul  7 02:17:22 2024 ] 	Batch(1700/7879) done. Loss: 0.1192  lr:0.000010
[ Sun Jul  7 02:17:40 2024 ] 	Batch(1800/7879) done. Loss: 0.0440  lr:0.000010
[ Sun Jul  7 02:17:58 2024 ] 	Batch(1900/7879) done. Loss: 0.0265  lr:0.000010
[ Sun Jul  7 02:18:16 2024 ] 
Training: Epoch [45/120], Step [1999], Loss: 0.05967332422733307, Training Accuracy: 97.8625
[ Sun Jul  7 02:18:16 2024 ] 	Batch(2000/7879) done. Loss: 0.0189  lr:0.000010
[ Sun Jul  7 02:18:34 2024 ] 	Batch(2100/7879) done. Loss: 0.0260  lr:0.000010
[ Sun Jul  7 02:18:52 2024 ] 	Batch(2200/7879) done. Loss: 0.0246  lr:0.000010
[ Sun Jul  7 02:19:10 2024 ] 	Batch(2300/7879) done. Loss: 0.0277  lr:0.000010
[ Sun Jul  7 02:19:29 2024 ] 	Batch(2400/7879) done. Loss: 0.0887  lr:0.000010
[ Sun Jul  7 02:19:47 2024 ] 
Training: Epoch [45/120], Step [2499], Loss: 0.02948141284286976, Training Accuracy: 97.91
[ Sun Jul  7 02:19:47 2024 ] 	Batch(2500/7879) done. Loss: 0.0072  lr:0.000010
[ Sun Jul  7 02:20:05 2024 ] 	Batch(2600/7879) done. Loss: 0.3361  lr:0.000010
[ Sun Jul  7 02:20:23 2024 ] 	Batch(2700/7879) done. Loss: 0.0656  lr:0.000010
[ Sun Jul  7 02:20:41 2024 ] 	Batch(2800/7879) done. Loss: 0.2658  lr:0.000010
[ Sun Jul  7 02:20:58 2024 ] 	Batch(2900/7879) done. Loss: 0.0400  lr:0.000010
[ Sun Jul  7 02:21:16 2024 ] 
Training: Epoch [45/120], Step [2999], Loss: 0.10160348564386368, Training Accuracy: 97.87083333333332
[ Sun Jul  7 02:21:16 2024 ] 	Batch(3000/7879) done. Loss: 0.0442  lr:0.000010
[ Sun Jul  7 02:21:34 2024 ] 	Batch(3100/7879) done. Loss: 0.0529  lr:0.000010
[ Sun Jul  7 02:21:52 2024 ] 	Batch(3200/7879) done. Loss: 0.0227  lr:0.000010
[ Sun Jul  7 02:22:11 2024 ] 	Batch(3300/7879) done. Loss: 0.0290  lr:0.000010
[ Sun Jul  7 02:22:29 2024 ] 	Batch(3400/7879) done. Loss: 0.2301  lr:0.000010
[ Sun Jul  7 02:22:47 2024 ] 
Training: Epoch [45/120], Step [3499], Loss: 0.002330977935343981, Training Accuracy: 97.90357142857142
[ Sun Jul  7 02:22:47 2024 ] 	Batch(3500/7879) done. Loss: 0.0619  lr:0.000010
[ Sun Jul  7 02:23:05 2024 ] 	Batch(3600/7879) done. Loss: 0.2789  lr:0.000010
[ Sun Jul  7 02:23:23 2024 ] 	Batch(3700/7879) done. Loss: 0.2564  lr:0.000010
[ Sun Jul  7 02:23:41 2024 ] 	Batch(3800/7879) done. Loss: 0.0064  lr:0.000010
[ Sun Jul  7 02:23:59 2024 ] 	Batch(3900/7879) done. Loss: 0.0066  lr:0.000010
[ Sun Jul  7 02:24:16 2024 ] 
Training: Epoch [45/120], Step [3999], Loss: 0.10590718686580658, Training Accuracy: 97.925
[ Sun Jul  7 02:24:17 2024 ] 	Batch(4000/7879) done. Loss: 0.1708  lr:0.000010
[ Sun Jul  7 02:24:35 2024 ] 	Batch(4100/7879) done. Loss: 0.0230  lr:0.000010
[ Sun Jul  7 02:24:52 2024 ] 	Batch(4200/7879) done. Loss: 0.1524  lr:0.000010
[ Sun Jul  7 02:25:10 2024 ] 	Batch(4300/7879) done. Loss: 0.0336  lr:0.000010
[ Sun Jul  7 02:25:28 2024 ] 	Batch(4400/7879) done. Loss: 0.0551  lr:0.000010
[ Sun Jul  7 02:25:46 2024 ] 
Training: Epoch [45/120], Step [4499], Loss: 0.16444449126720428, Training Accuracy: 97.93333333333332
[ Sun Jul  7 02:25:46 2024 ] 	Batch(4500/7879) done. Loss: 0.0206  lr:0.000010
[ Sun Jul  7 02:26:04 2024 ] 	Batch(4600/7879) done. Loss: 0.0603  lr:0.000010
[ Sun Jul  7 02:26:22 2024 ] 	Batch(4700/7879) done. Loss: 0.0248  lr:0.000010
[ Sun Jul  7 02:26:40 2024 ] 	Batch(4800/7879) done. Loss: 0.0221  lr:0.000010
[ Sun Jul  7 02:26:58 2024 ] 	Batch(4900/7879) done. Loss: 0.2453  lr:0.000010
[ Sun Jul  7 02:27:16 2024 ] 
Training: Epoch [45/120], Step [4999], Loss: 0.004125684499740601, Training Accuracy: 97.955
[ Sun Jul  7 02:27:16 2024 ] 	Batch(5000/7879) done. Loss: 0.0171  lr:0.000010
[ Sun Jul  7 02:27:34 2024 ] 	Batch(5100/7879) done. Loss: 0.0743  lr:0.000010
[ Sun Jul  7 02:27:52 2024 ] 	Batch(5200/7879) done. Loss: 0.1927  lr:0.000010
[ Sun Jul  7 02:28:10 2024 ] 	Batch(5300/7879) done. Loss: 0.0017  lr:0.000010
[ Sun Jul  7 02:28:28 2024 ] 	Batch(5400/7879) done. Loss: 0.0247  lr:0.000010
[ Sun Jul  7 02:28:46 2024 ] 
Training: Epoch [45/120], Step [5499], Loss: 0.11376890540122986, Training Accuracy: 97.96363636363637
[ Sun Jul  7 02:28:46 2024 ] 	Batch(5500/7879) done. Loss: 0.1675  lr:0.000010
[ Sun Jul  7 02:29:04 2024 ] 	Batch(5600/7879) done. Loss: 0.0401  lr:0.000010
[ Sun Jul  7 02:29:22 2024 ] 	Batch(5700/7879) done. Loss: 0.0224  lr:0.000010
[ Sun Jul  7 02:29:40 2024 ] 	Batch(5800/7879) done. Loss: 0.1400  lr:0.000010
[ Sun Jul  7 02:29:58 2024 ] 	Batch(5900/7879) done. Loss: 0.0089  lr:0.000010
[ Sun Jul  7 02:30:16 2024 ] 
Training: Epoch [45/120], Step [5999], Loss: 0.1980593055486679, Training Accuracy: 97.95416666666667
[ Sun Jul  7 02:30:16 2024 ] 	Batch(6000/7879) done. Loss: 0.0329  lr:0.000010
[ Sun Jul  7 02:30:34 2024 ] 	Batch(6100/7879) done. Loss: 0.0057  lr:0.000010
[ Sun Jul  7 02:30:52 2024 ] 	Batch(6200/7879) done. Loss: 0.1020  lr:0.000010
[ Sun Jul  7 02:31:10 2024 ] 	Batch(6300/7879) done. Loss: 0.0438  lr:0.000010
[ Sun Jul  7 02:31:28 2024 ] 	Batch(6400/7879) done. Loss: 0.0247  lr:0.000010
[ Sun Jul  7 02:31:46 2024 ] 
Training: Epoch [45/120], Step [6499], Loss: 0.16615766286849976, Training Accuracy: 97.96923076923076
[ Sun Jul  7 02:31:46 2024 ] 	Batch(6500/7879) done. Loss: 0.0391  lr:0.000010
[ Sun Jul  7 02:32:04 2024 ] 	Batch(6600/7879) done. Loss: 0.0382  lr:0.000010
[ Sun Jul  7 02:32:22 2024 ] 	Batch(6700/7879) done. Loss: 0.0357  lr:0.000010
[ Sun Jul  7 02:32:40 2024 ] 	Batch(6800/7879) done. Loss: 0.1207  lr:0.000010
[ Sun Jul  7 02:32:58 2024 ] 	Batch(6900/7879) done. Loss: 0.0484  lr:0.000010
[ Sun Jul  7 02:33:16 2024 ] 
Training: Epoch [45/120], Step [6999], Loss: 0.005026687402278185, Training Accuracy: 97.99642857142857
[ Sun Jul  7 02:33:16 2024 ] 	Batch(7000/7879) done. Loss: 0.0136  lr:0.000010
[ Sun Jul  7 02:33:34 2024 ] 	Batch(7100/7879) done. Loss: 0.0048  lr:0.000010
[ Sun Jul  7 02:33:52 2024 ] 	Batch(7200/7879) done. Loss: 0.0072  lr:0.000010
[ Sun Jul  7 02:34:10 2024 ] 	Batch(7300/7879) done. Loss: 0.0052  lr:0.000010
[ Sun Jul  7 02:34:28 2024 ] 	Batch(7400/7879) done. Loss: 0.0459  lr:0.000010
[ Sun Jul  7 02:34:46 2024 ] 
Training: Epoch [45/120], Step [7499], Loss: 0.40504541993141174, Training Accuracy: 97.97333333333333
[ Sun Jul  7 02:34:46 2024 ] 	Batch(7500/7879) done. Loss: 0.0305  lr:0.000010
[ Sun Jul  7 02:35:04 2024 ] 	Batch(7600/7879) done. Loss: 0.0039  lr:0.000010
[ Sun Jul  7 02:35:22 2024 ] 	Batch(7700/7879) done. Loss: 0.1199  lr:0.000010
[ Sun Jul  7 02:35:40 2024 ] 	Batch(7800/7879) done. Loss: 0.0128  lr:0.000010
[ Sun Jul  7 02:35:54 2024 ] 	Mean training loss: 0.0860.
[ Sun Jul  7 02:35:54 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 02:35:54 2024 ] Training epoch: 47
[ Sun Jul  7 02:35:54 2024 ] 	Batch(0/7879) done. Loss: 0.0397  lr:0.000010
[ Sun Jul  7 02:36:12 2024 ] 	Batch(100/7879) done. Loss: 0.0047  lr:0.000010
[ Sun Jul  7 02:36:30 2024 ] 	Batch(200/7879) done. Loss: 0.0402  lr:0.000010
[ Sun Jul  7 02:36:48 2024 ] 	Batch(300/7879) done. Loss: 0.0009  lr:0.000010
[ Sun Jul  7 02:37:06 2024 ] 	Batch(400/7879) done. Loss: 0.0119  lr:0.000010
[ Sun Jul  7 02:37:24 2024 ] 
Training: Epoch [46/120], Step [499], Loss: 0.8461670875549316, Training Accuracy: 98.02499999999999
[ Sun Jul  7 02:37:24 2024 ] 	Batch(500/7879) done. Loss: 0.0132  lr:0.000010
[ Sun Jul  7 02:37:42 2024 ] 	Batch(600/7879) done. Loss: 0.0576  lr:0.000010
[ Sun Jul  7 02:38:00 2024 ] 	Batch(700/7879) done. Loss: 0.0054  lr:0.000010
[ Sun Jul  7 02:38:18 2024 ] 	Batch(800/7879) done. Loss: 0.1028  lr:0.000010
[ Sun Jul  7 02:38:36 2024 ] 	Batch(900/7879) done. Loss: 0.0720  lr:0.000010
[ Sun Jul  7 02:38:54 2024 ] 
Training: Epoch [46/120], Step [999], Loss: 0.1871013343334198, Training Accuracy: 98.1625
[ Sun Jul  7 02:38:54 2024 ] 	Batch(1000/7879) done. Loss: 0.1797  lr:0.000010
[ Sun Jul  7 02:39:12 2024 ] 	Batch(1100/7879) done. Loss: 0.1385  lr:0.000010
[ Sun Jul  7 02:39:30 2024 ] 	Batch(1200/7879) done. Loss: 0.0273  lr:0.000010
[ Sun Jul  7 02:39:48 2024 ] 	Batch(1300/7879) done. Loss: 0.1303  lr:0.000010
[ Sun Jul  7 02:40:06 2024 ] 	Batch(1400/7879) done. Loss: 0.0584  lr:0.000010
[ Sun Jul  7 02:40:24 2024 ] 
Training: Epoch [46/120], Step [1499], Loss: 0.03249989077448845, Training Accuracy: 98.04166666666667
[ Sun Jul  7 02:40:24 2024 ] 	Batch(1500/7879) done. Loss: 0.0667  lr:0.000010
[ Sun Jul  7 02:40:42 2024 ] 	Batch(1600/7879) done. Loss: 0.0039  lr:0.000010
[ Sun Jul  7 02:41:00 2024 ] 	Batch(1700/7879) done. Loss: 0.0155  lr:0.000010
[ Sun Jul  7 02:41:18 2024 ] 	Batch(1800/7879) done. Loss: 0.0226  lr:0.000010
[ Sun Jul  7 02:41:36 2024 ] 	Batch(1900/7879) done. Loss: 0.0821  lr:0.000010
[ Sun Jul  7 02:41:54 2024 ] 
Training: Epoch [46/120], Step [1999], Loss: 0.0040724435821175575, Training Accuracy: 97.98750000000001
[ Sun Jul  7 02:41:54 2024 ] 	Batch(2000/7879) done. Loss: 0.0275  lr:0.000010
[ Sun Jul  7 02:42:12 2024 ] 	Batch(2100/7879) done. Loss: 0.0092  lr:0.000010
[ Sun Jul  7 02:42:30 2024 ] 	Batch(2200/7879) done. Loss: 0.0080  lr:0.000010
[ Sun Jul  7 02:42:48 2024 ] 	Batch(2300/7879) done. Loss: 0.1742  lr:0.000010
[ Sun Jul  7 02:43:06 2024 ] 	Batch(2400/7879) done. Loss: 0.1533  lr:0.000010
[ Sun Jul  7 02:43:24 2024 ] 
Training: Epoch [46/120], Step [2499], Loss: 0.052597593516111374, Training Accuracy: 98.0
[ Sun Jul  7 02:43:24 2024 ] 	Batch(2500/7879) done. Loss: 0.0036  lr:0.000010
[ Sun Jul  7 02:43:43 2024 ] 	Batch(2600/7879) done. Loss: 0.0360  lr:0.000010
[ Sun Jul  7 02:44:01 2024 ] 	Batch(2700/7879) done. Loss: 0.0561  lr:0.000010
[ Sun Jul  7 02:44:19 2024 ] 	Batch(2800/7879) done. Loss: 0.0654  lr:0.000010
[ Sun Jul  7 02:44:37 2024 ] 	Batch(2900/7879) done. Loss: 0.1555  lr:0.000010
[ Sun Jul  7 02:44:55 2024 ] 
Training: Epoch [46/120], Step [2999], Loss: 0.008555477485060692, Training Accuracy: 97.95833333333334
[ Sun Jul  7 02:44:55 2024 ] 	Batch(3000/7879) done. Loss: 0.0565  lr:0.000010
[ Sun Jul  7 02:45:13 2024 ] 	Batch(3100/7879) done. Loss: 0.0068  lr:0.000010
[ Sun Jul  7 02:45:31 2024 ] 	Batch(3200/7879) done. Loss: 0.0671  lr:0.000010
[ Sun Jul  7 02:45:49 2024 ] 	Batch(3300/7879) done. Loss: 0.0032  lr:0.000010
[ Sun Jul  7 02:46:07 2024 ] 	Batch(3400/7879) done. Loss: 0.0253  lr:0.000010
[ Sun Jul  7 02:46:25 2024 ] 
Training: Epoch [46/120], Step [3499], Loss: 0.4015006124973297, Training Accuracy: 97.98928571428571
[ Sun Jul  7 02:46:25 2024 ] 	Batch(3500/7879) done. Loss: 0.0459  lr:0.000010
[ Sun Jul  7 02:46:43 2024 ] 	Batch(3600/7879) done. Loss: 0.0062  lr:0.000010
[ Sun Jul  7 02:47:02 2024 ] 	Batch(3700/7879) done. Loss: 0.0146  lr:0.000010
[ Sun Jul  7 02:47:20 2024 ] 	Batch(3800/7879) done. Loss: 0.0198  lr:0.000010
[ Sun Jul  7 02:47:38 2024 ] 	Batch(3900/7879) done. Loss: 0.0364  lr:0.000010
[ Sun Jul  7 02:47:55 2024 ] 
Training: Epoch [46/120], Step [3999], Loss: 0.015318880788981915, Training Accuracy: 97.953125
[ Sun Jul  7 02:47:56 2024 ] 	Batch(4000/7879) done. Loss: 0.0326  lr:0.000010
[ Sun Jul  7 02:48:14 2024 ] 	Batch(4100/7879) done. Loss: 0.0411  lr:0.000010
[ Sun Jul  7 02:48:31 2024 ] 	Batch(4200/7879) done. Loss: 0.0641  lr:0.000010
[ Sun Jul  7 02:48:49 2024 ] 	Batch(4300/7879) done. Loss: 0.0375  lr:0.000010
[ Sun Jul  7 02:49:08 2024 ] 	Batch(4400/7879) done. Loss: 0.2666  lr:0.000010
[ Sun Jul  7 02:49:25 2024 ] 
Training: Epoch [46/120], Step [4499], Loss: 0.06385468691587448, Training Accuracy: 97.91944444444445
[ Sun Jul  7 02:49:25 2024 ] 	Batch(4500/7879) done. Loss: 0.2541  lr:0.000010
[ Sun Jul  7 02:49:43 2024 ] 	Batch(4600/7879) done. Loss: 0.0250  lr:0.000010
[ Sun Jul  7 02:50:01 2024 ] 	Batch(4700/7879) done. Loss: 0.0070  lr:0.000010
[ Sun Jul  7 02:50:19 2024 ] 	Batch(4800/7879) done. Loss: 0.0074  lr:0.000010
[ Sun Jul  7 02:50:37 2024 ] 	Batch(4900/7879) done. Loss: 0.0127  lr:0.000010
[ Sun Jul  7 02:50:55 2024 ] 
Training: Epoch [46/120], Step [4999], Loss: 0.05822516232728958, Training Accuracy: 97.9075
[ Sun Jul  7 02:50:55 2024 ] 	Batch(5000/7879) done. Loss: 0.0802  lr:0.000010
[ Sun Jul  7 02:51:13 2024 ] 	Batch(5100/7879) done. Loss: 0.0084  lr:0.000010
[ Sun Jul  7 02:51:31 2024 ] 	Batch(5200/7879) done. Loss: 0.0345  lr:0.000010
[ Sun Jul  7 02:51:49 2024 ] 	Batch(5300/7879) done. Loss: 0.0445  lr:0.000010
[ Sun Jul  7 02:52:07 2024 ] 	Batch(5400/7879) done. Loss: 0.1488  lr:0.000010
[ Sun Jul  7 02:52:25 2024 ] 
Training: Epoch [46/120], Step [5499], Loss: 0.18525578081607819, Training Accuracy: 97.95454545454545
[ Sun Jul  7 02:52:25 2024 ] 	Batch(5500/7879) done. Loss: 0.3090  lr:0.000010
[ Sun Jul  7 02:52:43 2024 ] 	Batch(5600/7879) done. Loss: 0.0148  lr:0.000010
[ Sun Jul  7 02:53:01 2024 ] 	Batch(5700/7879) done. Loss: 0.1140  lr:0.000010
[ Sun Jul  7 02:53:19 2024 ] 	Batch(5800/7879) done. Loss: 0.0510  lr:0.000010
[ Sun Jul  7 02:53:37 2024 ] 	Batch(5900/7879) done. Loss: 0.0040  lr:0.000010
[ Sun Jul  7 02:53:54 2024 ] 
Training: Epoch [46/120], Step [5999], Loss: 0.30514657497406006, Training Accuracy: 97.94583333333333
[ Sun Jul  7 02:53:55 2024 ] 	Batch(6000/7879) done. Loss: 0.0403  lr:0.000010
[ Sun Jul  7 02:54:13 2024 ] 	Batch(6100/7879) done. Loss: 0.2674  lr:0.000010
[ Sun Jul  7 02:54:31 2024 ] 	Batch(6200/7879) done. Loss: 0.1667  lr:0.000010
[ Sun Jul  7 02:54:49 2024 ] 	Batch(6300/7879) done. Loss: 0.1388  lr:0.000010
[ Sun Jul  7 02:55:07 2024 ] 	Batch(6400/7879) done. Loss: 0.0531  lr:0.000010
[ Sun Jul  7 02:55:24 2024 ] 
Training: Epoch [46/120], Step [6499], Loss: 0.018301408737897873, Training Accuracy: 97.94423076923077
[ Sun Jul  7 02:55:25 2024 ] 	Batch(6500/7879) done. Loss: 0.0170  lr:0.000010
[ Sun Jul  7 02:55:43 2024 ] 	Batch(6600/7879) done. Loss: 0.2506  lr:0.000010
[ Sun Jul  7 02:56:02 2024 ] 	Batch(6700/7879) done. Loss: 0.2354  lr:0.000010
[ Sun Jul  7 02:56:21 2024 ] 	Batch(6800/7879) done. Loss: 0.0811  lr:0.000010
[ Sun Jul  7 02:56:39 2024 ] 	Batch(6900/7879) done. Loss: 0.1623  lr:0.000010
[ Sun Jul  7 02:56:56 2024 ] 
Training: Epoch [46/120], Step [6999], Loss: 0.24763786792755127, Training Accuracy: 97.95714285714286
[ Sun Jul  7 02:56:56 2024 ] 	Batch(7000/7879) done. Loss: 0.0340  lr:0.000010
[ Sun Jul  7 02:57:14 2024 ] 	Batch(7100/7879) done. Loss: 0.0239  lr:0.000010
[ Sun Jul  7 02:57:33 2024 ] 	Batch(7200/7879) done. Loss: 0.0265  lr:0.000010
[ Sun Jul  7 02:57:51 2024 ] 	Batch(7300/7879) done. Loss: 0.0859  lr:0.000010
[ Sun Jul  7 02:58:10 2024 ] 	Batch(7400/7879) done. Loss: 0.0535  lr:0.000010
[ Sun Jul  7 02:58:29 2024 ] 
Training: Epoch [46/120], Step [7499], Loss: 0.2105206996202469, Training Accuracy: 97.94500000000001
[ Sun Jul  7 02:58:29 2024 ] 	Batch(7500/7879) done. Loss: 0.0224  lr:0.000010
[ Sun Jul  7 02:58:48 2024 ] 	Batch(7600/7879) done. Loss: 0.0180  lr:0.000010
[ Sun Jul  7 02:59:06 2024 ] 	Batch(7700/7879) done. Loss: 0.5390  lr:0.000010
[ Sun Jul  7 02:59:24 2024 ] 	Batch(7800/7879) done. Loss: 0.0375  lr:0.000010
[ Sun Jul  7 02:59:38 2024 ] 	Mean training loss: 0.0832.
[ Sun Jul  7 02:59:38 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 02:59:38 2024 ] Training epoch: 48
[ Sun Jul  7 02:59:39 2024 ] 	Batch(0/7879) done. Loss: 0.0040  lr:0.000010
[ Sun Jul  7 02:59:57 2024 ] 	Batch(100/7879) done. Loss: 0.0024  lr:0.000010
[ Sun Jul  7 03:00:15 2024 ] 	Batch(200/7879) done. Loss: 0.0410  lr:0.000010
[ Sun Jul  7 03:00:34 2024 ] 	Batch(300/7879) done. Loss: 0.1475  lr:0.000010
[ Sun Jul  7 03:00:52 2024 ] 	Batch(400/7879) done. Loss: 0.0140  lr:0.000010
[ Sun Jul  7 03:01:10 2024 ] 
Training: Epoch [47/120], Step [499], Loss: 0.051702797412872314, Training Accuracy: 98.35000000000001
[ Sun Jul  7 03:01:10 2024 ] 	Batch(500/7879) done. Loss: 0.1054  lr:0.000010
[ Sun Jul  7 03:01:28 2024 ] 	Batch(600/7879) done. Loss: 0.0100  lr:0.000010
[ Sun Jul  7 03:01:46 2024 ] 	Batch(700/7879) done. Loss: 0.0347  lr:0.000010
[ Sun Jul  7 03:02:05 2024 ] 	Batch(800/7879) done. Loss: 0.0829  lr:0.000010
[ Sun Jul  7 03:02:23 2024 ] 	Batch(900/7879) done. Loss: 0.0089  lr:0.000010
[ Sun Jul  7 03:02:42 2024 ] 
Training: Epoch [47/120], Step [999], Loss: 0.09145642817020416, Training Accuracy: 98.1125
[ Sun Jul  7 03:02:42 2024 ] 	Batch(1000/7879) done. Loss: 0.0684  lr:0.000010
[ Sun Jul  7 03:03:00 2024 ] 	Batch(1100/7879) done. Loss: 0.0504  lr:0.000010
[ Sun Jul  7 03:03:18 2024 ] 	Batch(1200/7879) done. Loss: 0.1086  lr:0.000010
[ Sun Jul  7 03:03:37 2024 ] 	Batch(1300/7879) done. Loss: 0.0562  lr:0.000010
[ Sun Jul  7 03:03:56 2024 ] 	Batch(1400/7879) done. Loss: 0.0982  lr:0.000010
[ Sun Jul  7 03:04:14 2024 ] 
Training: Epoch [47/120], Step [1499], Loss: 0.05366908013820648, Training Accuracy: 98.10833333333333
[ Sun Jul  7 03:04:14 2024 ] 	Batch(1500/7879) done. Loss: 0.0010  lr:0.000010
[ Sun Jul  7 03:04:33 2024 ] 	Batch(1600/7879) done. Loss: 0.0343  lr:0.000010
[ Sun Jul  7 03:04:51 2024 ] 	Batch(1700/7879) done. Loss: 0.0100  lr:0.000010
[ Sun Jul  7 03:05:10 2024 ] 	Batch(1800/7879) done. Loss: 0.0696  lr:0.000010
[ Sun Jul  7 03:05:28 2024 ] 	Batch(1900/7879) done. Loss: 0.0667  lr:0.000010
[ Sun Jul  7 03:05:46 2024 ] 
Training: Epoch [47/120], Step [1999], Loss: 0.0008250768878497183, Training Accuracy: 98.01875
[ Sun Jul  7 03:05:46 2024 ] 	Batch(2000/7879) done. Loss: 0.0758  lr:0.000010
[ Sun Jul  7 03:06:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0087  lr:0.000010
[ Sun Jul  7 03:06:22 2024 ] 	Batch(2200/7879) done. Loss: 0.0210  lr:0.000010
[ Sun Jul  7 03:06:40 2024 ] 	Batch(2300/7879) done. Loss: 0.0056  lr:0.000010
[ Sun Jul  7 03:06:58 2024 ] 	Batch(2400/7879) done. Loss: 0.0072  lr:0.000010
[ Sun Jul  7 03:07:16 2024 ] 
Training: Epoch [47/120], Step [2499], Loss: 0.00540197966620326, Training Accuracy: 98.05
[ Sun Jul  7 03:07:16 2024 ] 	Batch(2500/7879) done. Loss: 0.3095  lr:0.000010
[ Sun Jul  7 03:07:34 2024 ] 	Batch(2600/7879) done. Loss: 0.0237  lr:0.000010
[ Sun Jul  7 03:07:52 2024 ] 	Batch(2700/7879) done. Loss: 0.0039  lr:0.000010
[ Sun Jul  7 03:08:10 2024 ] 	Batch(2800/7879) done. Loss: 0.2811  lr:0.000010
[ Sun Jul  7 03:08:28 2024 ] 	Batch(2900/7879) done. Loss: 0.0158  lr:0.000010
[ Sun Jul  7 03:08:46 2024 ] 
Training: Epoch [47/120], Step [2999], Loss: 0.04115498065948486, Training Accuracy: 98.0125
[ Sun Jul  7 03:08:46 2024 ] 	Batch(3000/7879) done. Loss: 0.0808  lr:0.000010
[ Sun Jul  7 03:09:04 2024 ] 	Batch(3100/7879) done. Loss: 0.0113  lr:0.000010
[ Sun Jul  7 03:09:22 2024 ] 	Batch(3200/7879) done. Loss: 0.4188  lr:0.000010
[ Sun Jul  7 03:09:40 2024 ] 	Batch(3300/7879) done. Loss: 0.0445  lr:0.000010
[ Sun Jul  7 03:09:58 2024 ] 	Batch(3400/7879) done. Loss: 0.1522  lr:0.000010
[ Sun Jul  7 03:10:16 2024 ] 
Training: Epoch [47/120], Step [3499], Loss: 0.040541768074035645, Training Accuracy: 97.975
[ Sun Jul  7 03:10:16 2024 ] 	Batch(3500/7879) done. Loss: 0.0326  lr:0.000010
[ Sun Jul  7 03:10:34 2024 ] 	Batch(3600/7879) done. Loss: 0.0729  lr:0.000010
[ Sun Jul  7 03:10:52 2024 ] 	Batch(3700/7879) done. Loss: 0.0477  lr:0.000010
[ Sun Jul  7 03:11:10 2024 ] 	Batch(3800/7879) done. Loss: 0.0161  lr:0.000010
[ Sun Jul  7 03:11:28 2024 ] 	Batch(3900/7879) done. Loss: 0.0701  lr:0.000010
[ Sun Jul  7 03:11:45 2024 ] 
Training: Epoch [47/120], Step [3999], Loss: 0.23901142179965973, Training Accuracy: 98.00625
[ Sun Jul  7 03:11:46 2024 ] 	Batch(4000/7879) done. Loss: 0.0066  lr:0.000010
[ Sun Jul  7 03:12:04 2024 ] 	Batch(4100/7879) done. Loss: 0.0053  lr:0.000010
[ Sun Jul  7 03:12:22 2024 ] 	Batch(4200/7879) done. Loss: 0.0225  lr:0.000010
[ Sun Jul  7 03:12:40 2024 ] 	Batch(4300/7879) done. Loss: 0.0069  lr:0.000010
[ Sun Jul  7 03:12:58 2024 ] 	Batch(4400/7879) done. Loss: 0.6509  lr:0.000010
[ Sun Jul  7 03:13:15 2024 ] 
Training: Epoch [47/120], Step [4499], Loss: 0.016741590574383736, Training Accuracy: 98.01666666666667
[ Sun Jul  7 03:13:16 2024 ] 	Batch(4500/7879) done. Loss: 0.0853  lr:0.000010
[ Sun Jul  7 03:13:34 2024 ] 	Batch(4600/7879) done. Loss: 0.0386  lr:0.000010
[ Sun Jul  7 03:13:51 2024 ] 	Batch(4700/7879) done. Loss: 0.1010  lr:0.000010
[ Sun Jul  7 03:14:10 2024 ] 	Batch(4800/7879) done. Loss: 0.0127  lr:0.000010
[ Sun Jul  7 03:14:27 2024 ] 	Batch(4900/7879) done. Loss: 0.0182  lr:0.000010
[ Sun Jul  7 03:14:45 2024 ] 
Training: Epoch [47/120], Step [4999], Loss: 0.007912755943834782, Training Accuracy: 98.0275
[ Sun Jul  7 03:14:45 2024 ] 	Batch(5000/7879) done. Loss: 0.0143  lr:0.000010
[ Sun Jul  7 03:15:03 2024 ] 	Batch(5100/7879) done. Loss: 0.0571  lr:0.000010
[ Sun Jul  7 03:15:21 2024 ] 	Batch(5200/7879) done. Loss: 0.1358  lr:0.000010
[ Sun Jul  7 03:15:40 2024 ] 	Batch(5300/7879) done. Loss: 0.2746  lr:0.000010
[ Sun Jul  7 03:15:59 2024 ] 	Batch(5400/7879) done. Loss: 0.0418  lr:0.000010
[ Sun Jul  7 03:16:17 2024 ] 
Training: Epoch [47/120], Step [5499], Loss: 0.09263762086629868, Training Accuracy: 98.03636363636363
[ Sun Jul  7 03:16:17 2024 ] 	Batch(5500/7879) done. Loss: 0.1006  lr:0.000010
[ Sun Jul  7 03:16:36 2024 ] 	Batch(5600/7879) done. Loss: 0.0064  lr:0.000010
[ Sun Jul  7 03:16:54 2024 ] 	Batch(5700/7879) done. Loss: 0.0466  lr:0.000010
[ Sun Jul  7 03:17:12 2024 ] 	Batch(5800/7879) done. Loss: 0.0385  lr:0.000010
[ Sun Jul  7 03:17:30 2024 ] 	Batch(5900/7879) done. Loss: 0.0907  lr:0.000010
[ Sun Jul  7 03:17:48 2024 ] 
Training: Epoch [47/120], Step [5999], Loss: 0.03029409982264042, Training Accuracy: 98.0375
[ Sun Jul  7 03:17:48 2024 ] 	Batch(6000/7879) done. Loss: 0.0478  lr:0.000010
[ Sun Jul  7 03:18:06 2024 ] 	Batch(6100/7879) done. Loss: 0.0070  lr:0.000010
[ Sun Jul  7 03:18:24 2024 ] 	Batch(6200/7879) done. Loss: 0.0061  lr:0.000010
[ Sun Jul  7 03:18:42 2024 ] 	Batch(6300/7879) done. Loss: 0.4810  lr:0.000010
[ Sun Jul  7 03:19:01 2024 ] 	Batch(6400/7879) done. Loss: 0.1696  lr:0.000010
[ Sun Jul  7 03:19:19 2024 ] 
Training: Epoch [47/120], Step [6499], Loss: 0.04193858057260513, Training Accuracy: 98.0423076923077
[ Sun Jul  7 03:19:19 2024 ] 	Batch(6500/7879) done. Loss: 0.0279  lr:0.000010
[ Sun Jul  7 03:19:37 2024 ] 	Batch(6600/7879) done. Loss: 0.1344  lr:0.000010
[ Sun Jul  7 03:19:55 2024 ] 	Batch(6700/7879) done. Loss: 0.0429  lr:0.000010
[ Sun Jul  7 03:20:13 2024 ] 	Batch(6800/7879) done. Loss: 0.0310  lr:0.000010
[ Sun Jul  7 03:20:31 2024 ] 	Batch(6900/7879) done. Loss: 0.0035  lr:0.000010
[ Sun Jul  7 03:20:49 2024 ] 
Training: Epoch [47/120], Step [6999], Loss: 0.010807913728058338, Training Accuracy: 98.05178571428571
[ Sun Jul  7 03:20:49 2024 ] 	Batch(7000/7879) done. Loss: 0.0015  lr:0.000010
[ Sun Jul  7 03:21:08 2024 ] 	Batch(7100/7879) done. Loss: 0.1665  lr:0.000010
[ Sun Jul  7 03:21:26 2024 ] 	Batch(7200/7879) done. Loss: 0.1302  lr:0.000010
[ Sun Jul  7 03:21:44 2024 ] 	Batch(7300/7879) done. Loss: 0.1028  lr:0.000010
[ Sun Jul  7 03:22:02 2024 ] 	Batch(7400/7879) done. Loss: 0.1070  lr:0.000010
[ Sun Jul  7 03:22:20 2024 ] 
Training: Epoch [47/120], Step [7499], Loss: 0.03411533683538437, Training Accuracy: 98.02499999999999
[ Sun Jul  7 03:22:20 2024 ] 	Batch(7500/7879) done. Loss: 0.1147  lr:0.000010
[ Sun Jul  7 03:22:38 2024 ] 	Batch(7600/7879) done. Loss: 0.0133  lr:0.000010
[ Sun Jul  7 03:22:56 2024 ] 	Batch(7700/7879) done. Loss: 0.0262  lr:0.000010
[ Sun Jul  7 03:23:15 2024 ] 	Batch(7800/7879) done. Loss: 0.0366  lr:0.000010
[ Sun Jul  7 03:23:29 2024 ] 	Mean training loss: 0.0815.
[ Sun Jul  7 03:23:29 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 03:23:29 2024 ] Training epoch: 49
[ Sun Jul  7 03:23:29 2024 ] 	Batch(0/7879) done. Loss: 0.2417  lr:0.000010
[ Sun Jul  7 03:23:48 2024 ] 	Batch(100/7879) done. Loss: 0.0254  lr:0.000010
[ Sun Jul  7 03:24:06 2024 ] 	Batch(200/7879) done. Loss: 0.0175  lr:0.000010
[ Sun Jul  7 03:24:25 2024 ] 	Batch(300/7879) done. Loss: 0.0045  lr:0.000010
[ Sun Jul  7 03:24:43 2024 ] 	Batch(400/7879) done. Loss: 0.0170  lr:0.000010
[ Sun Jul  7 03:25:02 2024 ] 
Training: Epoch [48/120], Step [499], Loss: 0.0344456322491169, Training Accuracy: 97.85000000000001
[ Sun Jul  7 03:25:02 2024 ] 	Batch(500/7879) done. Loss: 0.0916  lr:0.000010
[ Sun Jul  7 03:25:20 2024 ] 	Batch(600/7879) done. Loss: 0.0313  lr:0.000010
[ Sun Jul  7 03:25:39 2024 ] 	Batch(700/7879) done. Loss: 0.0470  lr:0.000010
[ Sun Jul  7 03:25:57 2024 ] 	Batch(800/7879) done. Loss: 0.1405  lr:0.000010
[ Sun Jul  7 03:26:15 2024 ] 	Batch(900/7879) done. Loss: 0.0643  lr:0.000010
[ Sun Jul  7 03:26:33 2024 ] 
Training: Epoch [48/120], Step [999], Loss: 0.026173070073127747, Training Accuracy: 98.1375
[ Sun Jul  7 03:26:33 2024 ] 	Batch(1000/7879) done. Loss: 0.1493  lr:0.000010
[ Sun Jul  7 03:26:51 2024 ] 	Batch(1100/7879) done. Loss: 0.1807  lr:0.000010
[ Sun Jul  7 03:27:09 2024 ] 	Batch(1200/7879) done. Loss: 0.2963  lr:0.000010
[ Sun Jul  7 03:27:27 2024 ] 	Batch(1300/7879) done. Loss: 0.0248  lr:0.000010
[ Sun Jul  7 03:27:45 2024 ] 	Batch(1400/7879) done. Loss: 0.0104  lr:0.000010
[ Sun Jul  7 03:28:02 2024 ] 
Training: Epoch [48/120], Step [1499], Loss: 0.0077167171984910965, Training Accuracy: 97.98333333333333
[ Sun Jul  7 03:28:03 2024 ] 	Batch(1500/7879) done. Loss: 0.0223  lr:0.000010
[ Sun Jul  7 03:28:21 2024 ] 	Batch(1600/7879) done. Loss: 0.0042  lr:0.000010
[ Sun Jul  7 03:28:39 2024 ] 	Batch(1700/7879) done. Loss: 0.1448  lr:0.000010
[ Sun Jul  7 03:28:56 2024 ] 	Batch(1800/7879) done. Loss: 0.0037  lr:0.000010
[ Sun Jul  7 03:29:14 2024 ] 	Batch(1900/7879) done. Loss: 0.0133  lr:0.000010
[ Sun Jul  7 03:29:32 2024 ] 
Training: Epoch [48/120], Step [1999], Loss: 0.029615364968776703, Training Accuracy: 98.0375
[ Sun Jul  7 03:29:32 2024 ] 	Batch(2000/7879) done. Loss: 0.0293  lr:0.000010
[ Sun Jul  7 03:29:51 2024 ] 	Batch(2100/7879) done. Loss: 0.0287  lr:0.000010
[ Sun Jul  7 03:30:09 2024 ] 	Batch(2200/7879) done. Loss: 0.2040  lr:0.000010
[ Sun Jul  7 03:30:27 2024 ] 	Batch(2300/7879) done. Loss: 0.1879  lr:0.000010
[ Sun Jul  7 03:30:45 2024 ] 	Batch(2400/7879) done. Loss: 0.0192  lr:0.000010
[ Sun Jul  7 03:31:03 2024 ] 
Training: Epoch [48/120], Step [2499], Loss: 0.1074463427066803, Training Accuracy: 98.02
[ Sun Jul  7 03:31:03 2024 ] 	Batch(2500/7879) done. Loss: 0.0579  lr:0.000010
[ Sun Jul  7 03:31:21 2024 ] 	Batch(2600/7879) done. Loss: 0.0485  lr:0.000010
[ Sun Jul  7 03:31:39 2024 ] 	Batch(2700/7879) done. Loss: 0.1048  lr:0.000010
[ Sun Jul  7 03:31:57 2024 ] 	Batch(2800/7879) done. Loss: 0.0762  lr:0.000010
[ Sun Jul  7 03:32:15 2024 ] 	Batch(2900/7879) done. Loss: 0.0488  lr:0.000010
[ Sun Jul  7 03:32:32 2024 ] 
Training: Epoch [48/120], Step [2999], Loss: 0.10580603778362274, Training Accuracy: 98.03333333333333
[ Sun Jul  7 03:32:33 2024 ] 	Batch(3000/7879) done. Loss: 0.0434  lr:0.000010
[ Sun Jul  7 03:32:51 2024 ] 	Batch(3100/7879) done. Loss: 0.0020  lr:0.000010
[ Sun Jul  7 03:33:09 2024 ] 	Batch(3200/7879) done. Loss: 0.0166  lr:0.000010
[ Sun Jul  7 03:33:27 2024 ] 	Batch(3300/7879) done. Loss: 0.1197  lr:0.000010
[ Sun Jul  7 03:33:44 2024 ] 	Batch(3400/7879) done. Loss: 0.3445  lr:0.000010
[ Sun Jul  7 03:34:02 2024 ] 
Training: Epoch [48/120], Step [3499], Loss: 0.02248528227210045, Training Accuracy: 98.06071428571428
[ Sun Jul  7 03:34:02 2024 ] 	Batch(3500/7879) done. Loss: 0.1419  lr:0.000010
[ Sun Jul  7 03:34:20 2024 ] 	Batch(3600/7879) done. Loss: 0.0168  lr:0.000010
[ Sun Jul  7 03:34:38 2024 ] 	Batch(3700/7879) done. Loss: 0.0192  lr:0.000010
[ Sun Jul  7 03:34:56 2024 ] 	Batch(3800/7879) done. Loss: 0.0096  lr:0.000010
[ Sun Jul  7 03:35:14 2024 ] 	Batch(3900/7879) done. Loss: 0.1837  lr:0.000010
[ Sun Jul  7 03:35:32 2024 ] 
Training: Epoch [48/120], Step [3999], Loss: 0.16950471699237823, Training Accuracy: 98.040625
[ Sun Jul  7 03:35:32 2024 ] 	Batch(4000/7879) done. Loss: 0.0814  lr:0.000010
[ Sun Jul  7 03:35:50 2024 ] 	Batch(4100/7879) done. Loss: 0.0060  lr:0.000010
[ Sun Jul  7 03:36:08 2024 ] 	Batch(4200/7879) done. Loss: 0.0434  lr:0.000010
[ Sun Jul  7 03:36:26 2024 ] 	Batch(4300/7879) done. Loss: 0.0043  lr:0.000010
[ Sun Jul  7 03:36:44 2024 ] 	Batch(4400/7879) done. Loss: 0.0357  lr:0.000010
[ Sun Jul  7 03:37:02 2024 ] 
Training: Epoch [48/120], Step [4499], Loss: 0.02499261312186718, Training Accuracy: 98.03611111111111
[ Sun Jul  7 03:37:02 2024 ] 	Batch(4500/7879) done. Loss: 0.0499  lr:0.000010
[ Sun Jul  7 03:37:20 2024 ] 	Batch(4600/7879) done. Loss: 0.0037  lr:0.000010
[ Sun Jul  7 03:37:38 2024 ] 	Batch(4700/7879) done. Loss: 0.0192  lr:0.000010
[ Sun Jul  7 03:37:56 2024 ] 	Batch(4800/7879) done. Loss: 0.0270  lr:0.000010
[ Sun Jul  7 03:38:15 2024 ] 	Batch(4900/7879) done. Loss: 0.2711  lr:0.000010
[ Sun Jul  7 03:38:33 2024 ] 
Training: Epoch [48/120], Step [4999], Loss: 0.1864316612482071, Training Accuracy: 98.0425
[ Sun Jul  7 03:38:33 2024 ] 	Batch(5000/7879) done. Loss: 0.0251  lr:0.000010
[ Sun Jul  7 03:38:52 2024 ] 	Batch(5100/7879) done. Loss: 0.0688  lr:0.000010
[ Sun Jul  7 03:39:11 2024 ] 	Batch(5200/7879) done. Loss: 0.0934  lr:0.000010
[ Sun Jul  7 03:39:28 2024 ] 	Batch(5300/7879) done. Loss: 0.1558  lr:0.000010
[ Sun Jul  7 03:39:46 2024 ] 	Batch(5400/7879) done. Loss: 0.0532  lr:0.000010
[ Sun Jul  7 03:40:04 2024 ] 
Training: Epoch [48/120], Step [5499], Loss: 0.0437895730137825, Training Accuracy: 98.0409090909091
[ Sun Jul  7 03:40:04 2024 ] 	Batch(5500/7879) done. Loss: 0.0469  lr:0.000010
[ Sun Jul  7 03:40:22 2024 ] 	Batch(5600/7879) done. Loss: 0.0171  lr:0.000010
[ Sun Jul  7 03:40:40 2024 ] 	Batch(5700/7879) done. Loss: 0.0536  lr:0.000010
[ Sun Jul  7 03:40:58 2024 ] 	Batch(5800/7879) done. Loss: 0.0055  lr:0.000010
[ Sun Jul  7 03:41:16 2024 ] 	Batch(5900/7879) done. Loss: 0.0190  lr:0.000010
[ Sun Jul  7 03:41:34 2024 ] 
Training: Epoch [48/120], Step [5999], Loss: 0.22105419635772705, Training Accuracy: 98.03125
[ Sun Jul  7 03:41:34 2024 ] 	Batch(6000/7879) done. Loss: 0.0527  lr:0.000010
[ Sun Jul  7 03:41:53 2024 ] 	Batch(6100/7879) done. Loss: 0.0920  lr:0.000010
[ Sun Jul  7 03:42:11 2024 ] 	Batch(6200/7879) done. Loss: 0.0621  lr:0.000010
[ Sun Jul  7 03:42:29 2024 ] 	Batch(6300/7879) done. Loss: 0.1581  lr:0.000010
[ Sun Jul  7 03:42:47 2024 ] 	Batch(6400/7879) done. Loss: 0.0061  lr:0.000010
[ Sun Jul  7 03:43:05 2024 ] 
Training: Epoch [48/120], Step [6499], Loss: 0.004647891037166119, Training Accuracy: 98.00769230769231
[ Sun Jul  7 03:43:06 2024 ] 	Batch(6500/7879) done. Loss: 0.0400  lr:0.000010
[ Sun Jul  7 03:43:24 2024 ] 	Batch(6600/7879) done. Loss: 0.0599  lr:0.000010
[ Sun Jul  7 03:43:43 2024 ] 	Batch(6700/7879) done. Loss: 0.0647  lr:0.000010
[ Sun Jul  7 03:44:01 2024 ] 	Batch(6800/7879) done. Loss: 0.1477  lr:0.000010
[ Sun Jul  7 03:44:20 2024 ] 	Batch(6900/7879) done. Loss: 0.0246  lr:0.000010
[ Sun Jul  7 03:44:38 2024 ] 
Training: Epoch [48/120], Step [6999], Loss: 0.05123904347419739, Training Accuracy: 97.99642857142857
[ Sun Jul  7 03:44:39 2024 ] 	Batch(7000/7879) done. Loss: 0.0480  lr:0.000010
[ Sun Jul  7 03:44:57 2024 ] 	Batch(7100/7879) done. Loss: 0.1931  lr:0.000010
[ Sun Jul  7 03:45:16 2024 ] 	Batch(7200/7879) done. Loss: 0.0263  lr:0.000010
[ Sun Jul  7 03:45:34 2024 ] 	Batch(7300/7879) done. Loss: 0.0457  lr:0.000010
[ Sun Jul  7 03:45:52 2024 ] 	Batch(7400/7879) done. Loss: 0.0315  lr:0.000010
[ Sun Jul  7 03:46:10 2024 ] 
Training: Epoch [48/120], Step [7499], Loss: 0.5077324509620667, Training Accuracy: 98.015
[ Sun Jul  7 03:46:10 2024 ] 	Batch(7500/7879) done. Loss: 0.0173  lr:0.000010
[ Sun Jul  7 03:46:28 2024 ] 	Batch(7600/7879) done. Loss: 0.0589  lr:0.000010
[ Sun Jul  7 03:46:46 2024 ] 	Batch(7700/7879) done. Loss: 0.0193  lr:0.000010
[ Sun Jul  7 03:47:04 2024 ] 	Batch(7800/7879) done. Loss: 0.0646  lr:0.000010
[ Sun Jul  7 03:47:18 2024 ] 	Mean training loss: 0.0823.
[ Sun Jul  7 03:47:18 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 03:47:18 2024 ] Training epoch: 50
[ Sun Jul  7 03:47:19 2024 ] 	Batch(0/7879) done. Loss: 0.0169  lr:0.000010
[ Sun Jul  7 03:47:37 2024 ] 	Batch(100/7879) done. Loss: 0.1107  lr:0.000010
[ Sun Jul  7 03:47:55 2024 ] 	Batch(200/7879) done. Loss: 0.4921  lr:0.000010
[ Sun Jul  7 03:48:13 2024 ] 	Batch(300/7879) done. Loss: 0.0260  lr:0.000010
[ Sun Jul  7 03:48:31 2024 ] 	Batch(400/7879) done. Loss: 0.0703  lr:0.000010
[ Sun Jul  7 03:48:49 2024 ] 
Training: Epoch [49/120], Step [499], Loss: 0.15956151485443115, Training Accuracy: 97.7
[ Sun Jul  7 03:48:49 2024 ] 	Batch(500/7879) done. Loss: 0.0192  lr:0.000010
[ Sun Jul  7 03:49:07 2024 ] 	Batch(600/7879) done. Loss: 0.0298  lr:0.000010
[ Sun Jul  7 03:49:25 2024 ] 	Batch(700/7879) done. Loss: 0.0220  lr:0.000010
[ Sun Jul  7 03:49:43 2024 ] 	Batch(800/7879) done. Loss: 0.1049  lr:0.000010
[ Sun Jul  7 03:50:02 2024 ] 	Batch(900/7879) done. Loss: 0.0074  lr:0.000010
[ Sun Jul  7 03:50:20 2024 ] 
Training: Epoch [49/120], Step [999], Loss: 0.01902572624385357, Training Accuracy: 97.7125
[ Sun Jul  7 03:50:20 2024 ] 	Batch(1000/7879) done. Loss: 0.0177  lr:0.000010
[ Sun Jul  7 03:50:38 2024 ] 	Batch(1100/7879) done. Loss: 0.5186  lr:0.000010
[ Sun Jul  7 03:50:56 2024 ] 	Batch(1200/7879) done. Loss: 0.0338  lr:0.000010
[ Sun Jul  7 03:51:14 2024 ] 	Batch(1300/7879) done. Loss: 0.1461  lr:0.000010
[ Sun Jul  7 03:51:32 2024 ] 	Batch(1400/7879) done. Loss: 0.2531  lr:0.000010
[ Sun Jul  7 03:51:50 2024 ] 
Training: Epoch [49/120], Step [1499], Loss: 0.014688083902001381, Training Accuracy: 97.96666666666667
[ Sun Jul  7 03:51:50 2024 ] 	Batch(1500/7879) done. Loss: 0.0103  lr:0.000010
[ Sun Jul  7 03:52:08 2024 ] 	Batch(1600/7879) done. Loss: 0.3539  lr:0.000010
[ Sun Jul  7 03:52:27 2024 ] 	Batch(1700/7879) done. Loss: 0.1799  lr:0.000010
[ Sun Jul  7 03:52:45 2024 ] 	Batch(1800/7879) done. Loss: 0.0161  lr:0.000010
[ Sun Jul  7 03:53:03 2024 ] 	Batch(1900/7879) done. Loss: 0.0840  lr:0.000010
[ Sun Jul  7 03:53:21 2024 ] 
Training: Epoch [49/120], Step [1999], Loss: 0.07906001806259155, Training Accuracy: 97.95625
[ Sun Jul  7 03:53:21 2024 ] 	Batch(2000/7879) done. Loss: 0.4489  lr:0.000010
[ Sun Jul  7 03:53:39 2024 ] 	Batch(2100/7879) done. Loss: 0.0072  lr:0.000010
[ Sun Jul  7 03:53:57 2024 ] 	Batch(2200/7879) done. Loss: 0.0149  lr:0.000010
[ Sun Jul  7 03:54:15 2024 ] 	Batch(2300/7879) done. Loss: 0.0363  lr:0.000010
[ Sun Jul  7 03:54:33 2024 ] 	Batch(2400/7879) done. Loss: 0.1903  lr:0.000010
[ Sun Jul  7 03:54:50 2024 ] 
Training: Epoch [49/120], Step [2499], Loss: 0.007103797979652882, Training Accuracy: 98.015
[ Sun Jul  7 03:54:51 2024 ] 	Batch(2500/7879) done. Loss: 0.0071  lr:0.000010
[ Sun Jul  7 03:55:08 2024 ] 	Batch(2600/7879) done. Loss: 0.0681  lr:0.000010
[ Sun Jul  7 03:55:26 2024 ] 	Batch(2700/7879) done. Loss: 0.0196  lr:0.000010
[ Sun Jul  7 03:55:44 2024 ] 	Batch(2800/7879) done. Loss: 0.0015  lr:0.000010
[ Sun Jul  7 03:56:03 2024 ] 	Batch(2900/7879) done. Loss: 0.2259  lr:0.000010
[ Sun Jul  7 03:56:21 2024 ] 
Training: Epoch [49/120], Step [2999], Loss: 0.004411296918988228, Training Accuracy: 98.0375
[ Sun Jul  7 03:56:21 2024 ] 	Batch(3000/7879) done. Loss: 0.0518  lr:0.000010
[ Sun Jul  7 03:56:39 2024 ] 	Batch(3100/7879) done. Loss: 0.0273  lr:0.000010
[ Sun Jul  7 03:56:58 2024 ] 	Batch(3200/7879) done. Loss: 0.1227  lr:0.000010
[ Sun Jul  7 03:57:16 2024 ] 	Batch(3300/7879) done. Loss: 0.1740  lr:0.000010
[ Sun Jul  7 03:57:34 2024 ] 	Batch(3400/7879) done. Loss: 0.0101  lr:0.000010
[ Sun Jul  7 03:57:51 2024 ] 
Training: Epoch [49/120], Step [3499], Loss: 0.14946910738945007, Training Accuracy: 97.96428571428571
[ Sun Jul  7 03:57:52 2024 ] 	Batch(3500/7879) done. Loss: 0.3892  lr:0.000010
[ Sun Jul  7 03:58:10 2024 ] 	Batch(3600/7879) done. Loss: 0.0308  lr:0.000010
[ Sun Jul  7 03:58:28 2024 ] 	Batch(3700/7879) done. Loss: 0.3236  lr:0.000010
[ Sun Jul  7 03:58:45 2024 ] 	Batch(3800/7879) done. Loss: 0.0167  lr:0.000010
[ Sun Jul  7 03:59:03 2024 ] 	Batch(3900/7879) done. Loss: 0.0038  lr:0.000010
[ Sun Jul  7 03:59:21 2024 ] 
Training: Epoch [49/120], Step [3999], Loss: 0.20451529324054718, Training Accuracy: 97.9375
[ Sun Jul  7 03:59:21 2024 ] 	Batch(4000/7879) done. Loss: 0.0173  lr:0.000010
[ Sun Jul  7 03:59:39 2024 ] 	Batch(4100/7879) done. Loss: 0.0351  lr:0.000010
[ Sun Jul  7 03:59:57 2024 ] 	Batch(4200/7879) done. Loss: 0.0031  lr:0.000010
[ Sun Jul  7 04:00:15 2024 ] 	Batch(4300/7879) done. Loss: 0.0425  lr:0.000010
[ Sun Jul  7 04:00:33 2024 ] 	Batch(4400/7879) done. Loss: 0.0211  lr:0.000010
[ Sun Jul  7 04:00:51 2024 ] 
Training: Epoch [49/120], Step [4499], Loss: 0.013697407208383083, Training Accuracy: 97.96111111111111
[ Sun Jul  7 04:00:51 2024 ] 	Batch(4500/7879) done. Loss: 0.0342  lr:0.000010
[ Sun Jul  7 04:01:09 2024 ] 	Batch(4600/7879) done. Loss: 0.3986  lr:0.000010
[ Sun Jul  7 04:01:27 2024 ] 	Batch(4700/7879) done. Loss: 0.2007  lr:0.000010
[ Sun Jul  7 04:01:45 2024 ] 	Batch(4800/7879) done. Loss: 0.0242  lr:0.000010
[ Sun Jul  7 04:02:03 2024 ] 	Batch(4900/7879) done. Loss: 0.0150  lr:0.000010
[ Sun Jul  7 04:02:21 2024 ] 
Training: Epoch [49/120], Step [4999], Loss: 0.14829415082931519, Training Accuracy: 97.98
[ Sun Jul  7 04:02:21 2024 ] 	Batch(5000/7879) done. Loss: 0.0307  lr:0.000010
[ Sun Jul  7 04:02:39 2024 ] 	Batch(5100/7879) done. Loss: 0.0372  lr:0.000010
[ Sun Jul  7 04:02:57 2024 ] 	Batch(5200/7879) done. Loss: 0.2067  lr:0.000010
[ Sun Jul  7 04:03:16 2024 ] 	Batch(5300/7879) done. Loss: 0.1615  lr:0.000010
[ Sun Jul  7 04:03:34 2024 ] 	Batch(5400/7879) done. Loss: 0.0209  lr:0.000010
[ Sun Jul  7 04:03:53 2024 ] 
Training: Epoch [49/120], Step [5499], Loss: 0.07664436101913452, Training Accuracy: 98.02045454545456
[ Sun Jul  7 04:03:53 2024 ] 	Batch(5500/7879) done. Loss: 0.0462  lr:0.000010
[ Sun Jul  7 04:04:12 2024 ] 	Batch(5600/7879) done. Loss: 0.0149  lr:0.000010
[ Sun Jul  7 04:04:30 2024 ] 	Batch(5700/7879) done. Loss: 0.0337  lr:0.000010
[ Sun Jul  7 04:04:49 2024 ] 	Batch(5800/7879) done. Loss: 0.0058  lr:0.000010
[ Sun Jul  7 04:05:07 2024 ] 	Batch(5900/7879) done. Loss: 0.1977  lr:0.000010
[ Sun Jul  7 04:05:26 2024 ] 
Training: Epoch [49/120], Step [5999], Loss: 0.1308574229478836, Training Accuracy: 98.03958333333334
[ Sun Jul  7 04:05:26 2024 ] 	Batch(6000/7879) done. Loss: 0.0219  lr:0.000010
[ Sun Jul  7 04:05:44 2024 ] 	Batch(6100/7879) done. Loss: 0.2492  lr:0.000010
[ Sun Jul  7 04:06:03 2024 ] 	Batch(6200/7879) done. Loss: 0.0324  lr:0.000010
[ Sun Jul  7 04:06:22 2024 ] 	Batch(6300/7879) done. Loss: 0.1489  lr:0.000010
[ Sun Jul  7 04:06:40 2024 ] 	Batch(6400/7879) done. Loss: 0.1158  lr:0.000010
[ Sun Jul  7 04:06:58 2024 ] 
Training: Epoch [49/120], Step [6499], Loss: 0.005166316404938698, Training Accuracy: 98.06346153846154
[ Sun Jul  7 04:06:58 2024 ] 	Batch(6500/7879) done. Loss: 0.0198  lr:0.000010
[ Sun Jul  7 04:07:16 2024 ] 	Batch(6600/7879) done. Loss: 0.0256  lr:0.000010
[ Sun Jul  7 04:07:34 2024 ] 	Batch(6700/7879) done. Loss: 0.1953  lr:0.000010
[ Sun Jul  7 04:07:52 2024 ] 	Batch(6800/7879) done. Loss: 0.0616  lr:0.000010
[ Sun Jul  7 04:08:11 2024 ] 	Batch(6900/7879) done. Loss: 0.0175  lr:0.000010
[ Sun Jul  7 04:08:29 2024 ] 
Training: Epoch [49/120], Step [6999], Loss: 0.020944545045495033, Training Accuracy: 98.04821428571428
[ Sun Jul  7 04:08:29 2024 ] 	Batch(7000/7879) done. Loss: 0.0411  lr:0.000010
[ Sun Jul  7 04:08:48 2024 ] 	Batch(7100/7879) done. Loss: 0.0049  lr:0.000010
[ Sun Jul  7 04:09:06 2024 ] 	Batch(7200/7879) done. Loss: 0.0347  lr:0.000010
[ Sun Jul  7 04:09:24 2024 ] 	Batch(7300/7879) done. Loss: 0.0109  lr:0.000010
[ Sun Jul  7 04:09:42 2024 ] 	Batch(7400/7879) done. Loss: 0.1463  lr:0.000010
[ Sun Jul  7 04:10:00 2024 ] 
Training: Epoch [49/120], Step [7499], Loss: 0.06339775025844574, Training Accuracy: 98.03
[ Sun Jul  7 04:10:00 2024 ] 	Batch(7500/7879) done. Loss: 0.0444  lr:0.000010
[ Sun Jul  7 04:10:18 2024 ] 	Batch(7600/7879) done. Loss: 0.3178  lr:0.000010
[ Sun Jul  7 04:10:37 2024 ] 	Batch(7700/7879) done. Loss: 0.0086  lr:0.000010
[ Sun Jul  7 04:10:55 2024 ] 	Batch(7800/7879) done. Loss: 0.0719  lr:0.000010
[ Sun Jul  7 04:11:10 2024 ] 	Mean training loss: 0.0810.
[ Sun Jul  7 04:11:10 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 04:11:10 2024 ] Eval epoch: 50
[ Sun Jul  7 04:15:56 2024 ] 	Mean val loss of 6365 batches: 1.6722868342805044.
[ Sun Jul  7 04:15:56 2024 ] 
Validation: Epoch [49/120], Samples [39436.0/50919], Loss: 0.030435457825660706, Validation Accuracy: 77.44849663190557
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 1 : 200 / 275 = 72 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 2 : 216 / 273 = 79 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 3 : 220 / 273 = 80 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 4 : 222 / 275 = 80 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 5 : 242 / 275 = 88 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 6 : 225 / 275 = 81 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 7 : 248 / 273 = 90 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 8 : 262 / 273 = 95 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 9 : 173 / 273 = 63 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 10 : 131 / 273 = 47 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 11 : 136 / 272 = 50 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 12 : 229 / 271 = 84 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 13 : 269 / 275 = 97 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 14 : 266 / 276 = 96 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 15 : 232 / 273 = 84 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 16 : 226 / 274 = 82 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 17 : 236 / 273 = 86 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 18 : 240 / 274 = 87 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 19 : 255 / 272 = 93 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 20 : 250 / 273 = 91 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 21 : 228 / 274 = 83 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 22 : 232 / 274 = 84 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 23 : 257 / 276 = 93 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 24 : 221 / 274 = 80 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 25 : 259 / 275 = 94 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 26 : 271 / 276 = 98 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 27 : 211 / 275 = 76 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 28 : 169 / 275 = 61 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 29 : 134 / 275 = 48 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 30 : 179 / 276 = 64 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 31 : 246 / 276 = 89 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 32 : 243 / 276 = 88 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 33 : 237 / 276 = 85 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 34 : 244 / 276 = 88 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 35 : 242 / 275 = 88 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 36 : 238 / 276 = 86 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 37 : 245 / 276 = 88 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 38 : 252 / 276 = 91 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 39 : 255 / 276 = 92 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 40 : 193 / 276 = 69 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 41 : 260 / 276 = 94 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 42 : 261 / 275 = 94 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 43 : 186 / 276 = 67 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 44 : 246 / 276 = 89 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 45 : 246 / 276 = 89 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 46 : 216 / 276 = 78 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 47 : 222 / 275 = 80 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 48 : 225 / 275 = 81 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 49 : 228 / 274 = 83 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 50 : 239 / 276 = 86 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 51 : 246 / 276 = 89 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 52 : 229 / 276 = 82 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 53 : 233 / 276 = 84 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 54 : 268 / 274 = 97 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 55 : 241 / 276 = 87 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 56 : 243 / 275 = 88 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 57 : 267 / 276 = 96 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 58 : 268 / 273 = 98 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 59 : 266 / 276 = 96 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 60 : 470 / 561 = 83 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 61 : 459 / 566 = 81 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 62 : 463 / 572 = 80 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 63 : 520 / 570 = 91 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 64 : 424 / 574 = 73 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 65 : 502 / 573 = 87 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 66 : 371 / 573 = 64 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 67 : 427 / 575 = 74 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 68 : 328 / 575 = 57 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 69 : 461 / 575 = 80 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 70 : 241 / 575 = 41 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 71 : 192 / 575 = 33 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 72 : 150 / 571 = 26 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 73 : 222 / 570 = 38 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 74 : 382 / 569 = 67 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 75 : 253 / 573 = 44 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 76 : 342 / 574 = 59 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 77 : 396 / 573 = 69 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 78 : 421 / 575 = 73 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 79 : 544 / 574 = 94 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 80 : 501 / 573 = 87 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 81 : 330 / 575 = 57 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 82 : 391 / 575 = 68 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 83 : 297 / 572 = 51 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 84 : 437 / 574 = 76 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 85 : 404 / 574 = 70 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 86 : 510 / 575 = 88 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 87 : 493 / 576 = 85 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 88 : 413 / 575 = 71 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 89 : 491 / 576 = 85 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 90 : 255 / 574 = 44 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 91 : 458 / 568 = 80 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 92 : 369 / 576 = 64 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 93 : 418 / 573 = 72 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 94 : 511 / 574 = 89 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 95 : 517 / 575 = 89 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 96 : 562 / 575 = 97 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 97 : 552 / 574 = 96 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 98 : 539 / 575 = 93 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 99 : 548 / 574 = 95 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 100 : 484 / 574 = 84 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 101 : 517 / 574 = 90 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 102 : 353 / 575 = 61 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 103 : 491 / 576 = 85 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 104 : 288 / 575 = 50 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 105 : 243 / 575 = 42 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 106 : 327 / 576 = 56 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 107 : 493 / 576 = 85 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 108 : 484 / 575 = 84 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 109 : 352 / 575 = 61 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 110 : 477 / 575 = 82 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 111 : 529 / 576 = 91 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 112 : 547 / 575 = 95 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 113 : 509 / 576 = 88 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 114 : 471 / 576 = 81 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 115 : 510 / 576 = 88 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 116 : 465 / 575 = 80 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 117 : 461 / 575 = 80 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 118 : 482 / 575 = 83 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 119 : 525 / 576 = 91 %
[ Sun Jul  7 04:15:56 2024 ] Accuracy of 120 : 240 / 274 = 87 %
[ Sun Jul  7 04:15:56 2024 ] Training epoch: 51
[ Sun Jul  7 04:15:57 2024 ] 	Batch(0/7879) done. Loss: 0.0526  lr:0.000010
[ Sun Jul  7 04:16:15 2024 ] 	Batch(100/7879) done. Loss: 0.1055  lr:0.000010
[ Sun Jul  7 04:16:32 2024 ] 	Batch(200/7879) done. Loss: 0.0067  lr:0.000010
[ Sun Jul  7 04:16:50 2024 ] 	Batch(300/7879) done. Loss: 0.0246  lr:0.000010
[ Sun Jul  7 04:17:08 2024 ] 	Batch(400/7879) done. Loss: 0.0271  lr:0.000010
[ Sun Jul  7 04:17:26 2024 ] 
Training: Epoch [50/120], Step [499], Loss: 0.004528667777776718, Training Accuracy: 98.075
[ Sun Jul  7 04:17:26 2024 ] 	Batch(500/7879) done. Loss: 0.0910  lr:0.000010
[ Sun Jul  7 04:17:45 2024 ] 	Batch(600/7879) done. Loss: 0.0136  lr:0.000010
[ Sun Jul  7 04:18:03 2024 ] 	Batch(700/7879) done. Loss: 0.0413  lr:0.000010
[ Sun Jul  7 04:18:21 2024 ] 	Batch(800/7879) done. Loss: 0.1285  lr:0.000010
[ Sun Jul  7 04:18:39 2024 ] 	Batch(900/7879) done. Loss: 0.0081  lr:0.000010
[ Sun Jul  7 04:18:57 2024 ] 
Training: Epoch [50/120], Step [999], Loss: 0.01341179572045803, Training Accuracy: 98.225
[ Sun Jul  7 04:18:57 2024 ] 	Batch(1000/7879) done. Loss: 0.0538  lr:0.000010
[ Sun Jul  7 04:19:15 2024 ] 	Batch(1100/7879) done. Loss: 0.0076  lr:0.000010
[ Sun Jul  7 04:19:33 2024 ] 	Batch(1200/7879) done. Loss: 0.1197  lr:0.000010
[ Sun Jul  7 04:19:51 2024 ] 	Batch(1300/7879) done. Loss: 0.2373  lr:0.000010
[ Sun Jul  7 04:20:09 2024 ] 	Batch(1400/7879) done. Loss: 0.1353  lr:0.000010
[ Sun Jul  7 04:20:26 2024 ] 
Training: Epoch [50/120], Step [1499], Loss: 0.024033285677433014, Training Accuracy: 98.05
[ Sun Jul  7 04:20:27 2024 ] 	Batch(1500/7879) done. Loss: 0.0071  lr:0.000010
[ Sun Jul  7 04:20:45 2024 ] 	Batch(1600/7879) done. Loss: 0.0051  lr:0.000010
[ Sun Jul  7 04:21:03 2024 ] 	Batch(1700/7879) done. Loss: 0.0314  lr:0.000010
[ Sun Jul  7 04:21:20 2024 ] 	Batch(1800/7879) done. Loss: 0.0024  lr:0.000010
[ Sun Jul  7 04:21:38 2024 ] 	Batch(1900/7879) done. Loss: 0.5163  lr:0.000010
[ Sun Jul  7 04:21:56 2024 ] 
Training: Epoch [50/120], Step [1999], Loss: 0.22445818781852722, Training Accuracy: 98.175
[ Sun Jul  7 04:21:56 2024 ] 	Batch(2000/7879) done. Loss: 0.0241  lr:0.000010
[ Sun Jul  7 04:22:14 2024 ] 	Batch(2100/7879) done. Loss: 0.0329  lr:0.000010
[ Sun Jul  7 04:22:32 2024 ] 	Batch(2200/7879) done. Loss: 0.1174  lr:0.000010
[ Sun Jul  7 04:22:50 2024 ] 	Batch(2300/7879) done. Loss: 0.0125  lr:0.000010
[ Sun Jul  7 04:23:08 2024 ] 	Batch(2400/7879) done. Loss: 0.3825  lr:0.000010
[ Sun Jul  7 04:23:26 2024 ] 
Training: Epoch [50/120], Step [2499], Loss: 0.21575912833213806, Training Accuracy: 98.1
[ Sun Jul  7 04:23:26 2024 ] 	Batch(2500/7879) done. Loss: 0.1537  lr:0.000010
[ Sun Jul  7 04:23:44 2024 ] 	Batch(2600/7879) done. Loss: 0.2435  lr:0.000010
[ Sun Jul  7 04:24:02 2024 ] 	Batch(2700/7879) done. Loss: 0.0184  lr:0.000010
[ Sun Jul  7 04:24:20 2024 ] 	Batch(2800/7879) done. Loss: 0.0943  lr:0.000010
[ Sun Jul  7 04:24:38 2024 ] 	Batch(2900/7879) done. Loss: 0.0423  lr:0.000010
[ Sun Jul  7 04:24:57 2024 ] 
Training: Epoch [50/120], Step [2999], Loss: 0.006308101583272219, Training Accuracy: 98.11666666666666
[ Sun Jul  7 04:24:57 2024 ] 	Batch(3000/7879) done. Loss: 0.0486  lr:0.000010
[ Sun Jul  7 04:25:15 2024 ] 	Batch(3100/7879) done. Loss: 0.0252  lr:0.000010
[ Sun Jul  7 04:25:34 2024 ] 	Batch(3200/7879) done. Loss: 0.0617  lr:0.000010
[ Sun Jul  7 04:25:52 2024 ] 	Batch(3300/7879) done. Loss: 0.1564  lr:0.000010
[ Sun Jul  7 04:26:10 2024 ] 	Batch(3400/7879) done. Loss: 0.1497  lr:0.000010
[ Sun Jul  7 04:26:28 2024 ] 
Training: Epoch [50/120], Step [3499], Loss: 0.014838777482509613, Training Accuracy: 98.10357142857143
[ Sun Jul  7 04:26:28 2024 ] 	Batch(3500/7879) done. Loss: 0.1205  lr:0.000010
[ Sun Jul  7 04:26:46 2024 ] 	Batch(3600/7879) done. Loss: 0.0069  lr:0.000010
[ Sun Jul  7 04:27:04 2024 ] 	Batch(3700/7879) done. Loss: 0.0479  lr:0.000010
[ Sun Jul  7 04:27:22 2024 ] 	Batch(3800/7879) done. Loss: 0.1511  lr:0.000010
[ Sun Jul  7 04:27:39 2024 ] 	Batch(3900/7879) done. Loss: 0.0543  lr:0.000010
[ Sun Jul  7 04:27:57 2024 ] 
Training: Epoch [50/120], Step [3999], Loss: 0.004850667901337147, Training Accuracy: 98.14375
[ Sun Jul  7 04:27:58 2024 ] 	Batch(4000/7879) done. Loss: 0.0565  lr:0.000010
[ Sun Jul  7 04:28:15 2024 ] 	Batch(4100/7879) done. Loss: 0.0132  lr:0.000010
[ Sun Jul  7 04:28:33 2024 ] 	Batch(4200/7879) done. Loss: 0.0454  lr:0.000010
[ Sun Jul  7 04:28:51 2024 ] 	Batch(4300/7879) done. Loss: 0.0066  lr:0.000010
[ Sun Jul  7 04:29:09 2024 ] 	Batch(4400/7879) done. Loss: 0.0143  lr:0.000010
[ Sun Jul  7 04:29:27 2024 ] 
Training: Epoch [50/120], Step [4499], Loss: 0.034607987850904465, Training Accuracy: 98.09722222222223
[ Sun Jul  7 04:29:27 2024 ] 	Batch(4500/7879) done. Loss: 0.0191  lr:0.000010
[ Sun Jul  7 04:29:45 2024 ] 	Batch(4600/7879) done. Loss: 0.1740  lr:0.000010
[ Sun Jul  7 04:30:03 2024 ] 	Batch(4700/7879) done. Loss: 0.1062  lr:0.000010
[ Sun Jul  7 04:30:21 2024 ] 	Batch(4800/7879) done. Loss: 0.0070  lr:0.000010
[ Sun Jul  7 04:30:40 2024 ] 	Batch(4900/7879) done. Loss: 0.1867  lr:0.000010
[ Sun Jul  7 04:30:58 2024 ] 
Training: Epoch [50/120], Step [4999], Loss: 0.013534420169889927, Training Accuracy: 98.125
[ Sun Jul  7 04:30:58 2024 ] 	Batch(5000/7879) done. Loss: 0.0020  lr:0.000010
[ Sun Jul  7 04:31:17 2024 ] 	Batch(5100/7879) done. Loss: 0.0090  lr:0.000010
[ Sun Jul  7 04:31:36 2024 ] 	Batch(5200/7879) done. Loss: 0.0919  lr:0.000010
[ Sun Jul  7 04:31:54 2024 ] 	Batch(5300/7879) done. Loss: 0.0065  lr:0.000010
[ Sun Jul  7 04:32:13 2024 ] 	Batch(5400/7879) done. Loss: 0.2719  lr:0.000010
[ Sun Jul  7 04:32:31 2024 ] 
Training: Epoch [50/120], Step [5499], Loss: 0.03355522081255913, Training Accuracy: 98.13863636363637
[ Sun Jul  7 04:32:31 2024 ] 	Batch(5500/7879) done. Loss: 0.0705  lr:0.000010
[ Sun Jul  7 04:32:50 2024 ] 	Batch(5600/7879) done. Loss: 0.0280  lr:0.000010
[ Sun Jul  7 04:33:08 2024 ] 	Batch(5700/7879) done. Loss: 0.2019  lr:0.000010
[ Sun Jul  7 04:33:26 2024 ] 	Batch(5800/7879) done. Loss: 0.0297  lr:0.000010
[ Sun Jul  7 04:33:44 2024 ] 	Batch(5900/7879) done. Loss: 0.0100  lr:0.000010
[ Sun Jul  7 04:34:02 2024 ] 
Training: Epoch [50/120], Step [5999], Loss: 0.038318660110235214, Training Accuracy: 98.10624999999999
[ Sun Jul  7 04:34:02 2024 ] 	Batch(6000/7879) done. Loss: 0.0009  lr:0.000010
[ Sun Jul  7 04:34:20 2024 ] 	Batch(6100/7879) done. Loss: 0.0268  lr:0.000010
[ Sun Jul  7 04:34:38 2024 ] 	Batch(6200/7879) done. Loss: 0.0667  lr:0.000010
[ Sun Jul  7 04:34:56 2024 ] 	Batch(6300/7879) done. Loss: 0.0055  lr:0.000010
[ Sun Jul  7 04:35:14 2024 ] 	Batch(6400/7879) done. Loss: 0.0838  lr:0.000010
[ Sun Jul  7 04:35:33 2024 ] 
Training: Epoch [50/120], Step [6499], Loss: 0.0165729857981205, Training Accuracy: 98.09615384615384
[ Sun Jul  7 04:35:33 2024 ] 	Batch(6500/7879) done. Loss: 0.1870  lr:0.000010
[ Sun Jul  7 04:35:51 2024 ] 	Batch(6600/7879) done. Loss: 0.0071  lr:0.000010
[ Sun Jul  7 04:36:09 2024 ] 	Batch(6700/7879) done. Loss: 0.4227  lr:0.000010
[ Sun Jul  7 04:36:27 2024 ] 	Batch(6800/7879) done. Loss: 0.2679  lr:0.000010
[ Sun Jul  7 04:36:45 2024 ] 	Batch(6900/7879) done. Loss: 0.0078  lr:0.000010
[ Sun Jul  7 04:37:03 2024 ] 
Training: Epoch [50/120], Step [6999], Loss: 0.08751969784498215, Training Accuracy: 98.10535714285714
[ Sun Jul  7 04:37:03 2024 ] 	Batch(7000/7879) done. Loss: 0.0063  lr:0.000010
[ Sun Jul  7 04:37:21 2024 ] 	Batch(7100/7879) done. Loss: 0.0175  lr:0.000010
[ Sun Jul  7 04:37:39 2024 ] 	Batch(7200/7879) done. Loss: 0.0857  lr:0.000010
[ Sun Jul  7 04:37:57 2024 ] 	Batch(7300/7879) done. Loss: 0.1933  lr:0.000010
[ Sun Jul  7 04:38:15 2024 ] 	Batch(7400/7879) done. Loss: 0.0115  lr:0.000010
[ Sun Jul  7 04:38:32 2024 ] 
Training: Epoch [50/120], Step [7499], Loss: 0.03192046657204628, Training Accuracy: 98.11999999999999
[ Sun Jul  7 04:38:32 2024 ] 	Batch(7500/7879) done. Loss: 0.5422  lr:0.000010
[ Sun Jul  7 04:38:51 2024 ] 	Batch(7600/7879) done. Loss: 0.0067  lr:0.000010
[ Sun Jul  7 04:39:09 2024 ] 	Batch(7700/7879) done. Loss: 0.0319  lr:0.000010
[ Sun Jul  7 04:39:26 2024 ] 	Batch(7800/7879) done. Loss: 0.0454  lr:0.000010
[ Sun Jul  7 04:39:40 2024 ] 	Mean training loss: 0.0808.
[ Sun Jul  7 04:39:40 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 04:39:41 2024 ] Training epoch: 52
[ Sun Jul  7 04:39:41 2024 ] 	Batch(0/7879) done. Loss: 0.0112  lr:0.000010
[ Sun Jul  7 04:39:59 2024 ] 	Batch(100/7879) done. Loss: 0.1604  lr:0.000010
[ Sun Jul  7 04:40:17 2024 ] 	Batch(200/7879) done. Loss: 0.0589  lr:0.000010
[ Sun Jul  7 04:40:35 2024 ] 	Batch(300/7879) done. Loss: 0.1120  lr:0.000010
[ Sun Jul  7 04:40:53 2024 ] 	Batch(400/7879) done. Loss: 0.3036  lr:0.000010
[ Sun Jul  7 04:41:11 2024 ] 
Training: Epoch [51/120], Step [499], Loss: 0.0018052638042718172, Training Accuracy: 98.125
[ Sun Jul  7 04:41:11 2024 ] 	Batch(500/7879) done. Loss: 0.5850  lr:0.000010
[ Sun Jul  7 04:41:29 2024 ] 	Batch(600/7879) done. Loss: 0.1862  lr:0.000010
[ Sun Jul  7 04:41:47 2024 ] 	Batch(700/7879) done. Loss: 0.0202  lr:0.000010
[ Sun Jul  7 04:42:05 2024 ] 	Batch(800/7879) done. Loss: 0.1023  lr:0.000010
[ Sun Jul  7 04:42:23 2024 ] 	Batch(900/7879) done. Loss: 0.0261  lr:0.000010
[ Sun Jul  7 04:42:42 2024 ] 
Training: Epoch [51/120], Step [999], Loss: 0.023655438795685768, Training Accuracy: 98.0375
[ Sun Jul  7 04:42:42 2024 ] 	Batch(1000/7879) done. Loss: 0.0168  lr:0.000010
[ Sun Jul  7 04:43:00 2024 ] 	Batch(1100/7879) done. Loss: 0.0309  lr:0.000010
[ Sun Jul  7 04:43:19 2024 ] 	Batch(1200/7879) done. Loss: 0.0644  lr:0.000010
[ Sun Jul  7 04:43:37 2024 ] 	Batch(1300/7879) done. Loss: 0.0125  lr:0.000010
[ Sun Jul  7 04:43:55 2024 ] 	Batch(1400/7879) done. Loss: 0.1526  lr:0.000010
[ Sun Jul  7 04:44:13 2024 ] 
Training: Epoch [51/120], Step [1499], Loss: 0.26481595635414124, Training Accuracy: 98.1
[ Sun Jul  7 04:44:13 2024 ] 	Batch(1500/7879) done. Loss: 0.0124  lr:0.000010
[ Sun Jul  7 04:44:31 2024 ] 	Batch(1600/7879) done. Loss: 0.0276  lr:0.000010
[ Sun Jul  7 04:44:49 2024 ] 	Batch(1700/7879) done. Loss: 0.0097  lr:0.000010
[ Sun Jul  7 04:45:07 2024 ] 	Batch(1800/7879) done. Loss: 0.0155  lr:0.000010
[ Sun Jul  7 04:45:25 2024 ] 	Batch(1900/7879) done. Loss: 0.0016  lr:0.000010
[ Sun Jul  7 04:45:43 2024 ] 
Training: Epoch [51/120], Step [1999], Loss: 0.13165506720542908, Training Accuracy: 98.10624999999999
[ Sun Jul  7 04:45:43 2024 ] 	Batch(2000/7879) done. Loss: 0.0335  lr:0.000010
[ Sun Jul  7 04:46:01 2024 ] 	Batch(2100/7879) done. Loss: 0.0173  lr:0.000010
[ Sun Jul  7 04:46:19 2024 ] 	Batch(2200/7879) done. Loss: 0.0187  lr:0.000010
[ Sun Jul  7 04:46:37 2024 ] 	Batch(2300/7879) done. Loss: 0.0129  lr:0.000010
[ Sun Jul  7 04:46:55 2024 ] 	Batch(2400/7879) done. Loss: 0.6124  lr:0.000010
[ Sun Jul  7 04:47:13 2024 ] 
Training: Epoch [51/120], Step [2499], Loss: 0.006322273053228855, Training Accuracy: 98.045
[ Sun Jul  7 04:47:13 2024 ] 	Batch(2500/7879) done. Loss: 0.0357  lr:0.000010
[ Sun Jul  7 04:47:31 2024 ] 	Batch(2600/7879) done. Loss: 0.1140  lr:0.000010
[ Sun Jul  7 04:47:49 2024 ] 	Batch(2700/7879) done. Loss: 0.5356  lr:0.000010
[ Sun Jul  7 04:48:07 2024 ] 	Batch(2800/7879) done. Loss: 0.1627  lr:0.000010
[ Sun Jul  7 04:48:25 2024 ] 	Batch(2900/7879) done. Loss: 0.0405  lr:0.000010
[ Sun Jul  7 04:48:44 2024 ] 
Training: Epoch [51/120], Step [2999], Loss: 0.34155353903770447, Training Accuracy: 97.99583333333334
[ Sun Jul  7 04:48:44 2024 ] 	Batch(3000/7879) done. Loss: 0.0111  lr:0.000010
[ Sun Jul  7 04:49:02 2024 ] 	Batch(3100/7879) done. Loss: 0.0590  lr:0.000010
[ Sun Jul  7 04:49:21 2024 ] 	Batch(3200/7879) done. Loss: 0.0862  lr:0.000010
[ Sun Jul  7 04:49:39 2024 ] 	Batch(3300/7879) done. Loss: 0.2151  lr:0.000010
[ Sun Jul  7 04:49:57 2024 ] 	Batch(3400/7879) done. Loss: 0.0439  lr:0.000010
[ Sun Jul  7 04:50:15 2024 ] 
Training: Epoch [51/120], Step [3499], Loss: 0.03947099298238754, Training Accuracy: 97.97142857142858
[ Sun Jul  7 04:50:15 2024 ] 	Batch(3500/7879) done. Loss: 0.0892  lr:0.000010
[ Sun Jul  7 04:50:33 2024 ] 	Batch(3600/7879) done. Loss: 0.0099  lr:0.000010
[ Sun Jul  7 04:50:51 2024 ] 	Batch(3700/7879) done. Loss: 0.0155  lr:0.000010
[ Sun Jul  7 04:51:10 2024 ] 	Batch(3800/7879) done. Loss: 0.0048  lr:0.000010
[ Sun Jul  7 04:51:28 2024 ] 	Batch(3900/7879) done. Loss: 0.0458  lr:0.000010
[ Sun Jul  7 04:51:45 2024 ] 
Training: Epoch [51/120], Step [3999], Loss: 0.14420217275619507, Training Accuracy: 98.015625
[ Sun Jul  7 04:51:46 2024 ] 	Batch(4000/7879) done. Loss: 0.0986  lr:0.000010
[ Sun Jul  7 04:52:04 2024 ] 	Batch(4100/7879) done. Loss: 0.0102  lr:0.000010
[ Sun Jul  7 04:52:23 2024 ] 	Batch(4200/7879) done. Loss: 0.1554  lr:0.000010
[ Sun Jul  7 04:52:41 2024 ] 	Batch(4300/7879) done. Loss: 0.0844  lr:0.000010
[ Sun Jul  7 04:53:00 2024 ] 	Batch(4400/7879) done. Loss: 0.0242  lr:0.000010
[ Sun Jul  7 04:53:18 2024 ] 
Training: Epoch [51/120], Step [4499], Loss: 0.38933414220809937, Training Accuracy: 98.05
[ Sun Jul  7 04:53:18 2024 ] 	Batch(4500/7879) done. Loss: 0.1641  lr:0.000010
[ Sun Jul  7 04:53:36 2024 ] 	Batch(4600/7879) done. Loss: 0.0586  lr:0.000010
[ Sun Jul  7 04:53:54 2024 ] 	Batch(4700/7879) done. Loss: 0.0247  lr:0.000010
[ Sun Jul  7 04:54:12 2024 ] 	Batch(4800/7879) done. Loss: 0.0448  lr:0.000010
[ Sun Jul  7 04:54:30 2024 ] 	Batch(4900/7879) done. Loss: 0.0201  lr:0.000010
[ Sun Jul  7 04:54:48 2024 ] 
Training: Epoch [51/120], Step [4999], Loss: 0.010491374880075455, Training Accuracy: 98.0825
[ Sun Jul  7 04:54:48 2024 ] 	Batch(5000/7879) done. Loss: 0.0025  lr:0.000010
[ Sun Jul  7 04:55:06 2024 ] 	Batch(5100/7879) done. Loss: 0.0017  lr:0.000010
[ Sun Jul  7 04:55:24 2024 ] 	Batch(5200/7879) done. Loss: 0.0331  lr:0.000010
[ Sun Jul  7 04:55:42 2024 ] 	Batch(5300/7879) done. Loss: 0.1224  lr:0.000010
[ Sun Jul  7 04:56:00 2024 ] 	Batch(5400/7879) done. Loss: 0.0774  lr:0.000010
[ Sun Jul  7 04:56:18 2024 ] 
Training: Epoch [51/120], Step [5499], Loss: 0.18442730605602264, Training Accuracy: 98.10454545454546
[ Sun Jul  7 04:56:18 2024 ] 	Batch(5500/7879) done. Loss: 0.0435  lr:0.000010
[ Sun Jul  7 04:56:36 2024 ] 	Batch(5600/7879) done. Loss: 0.0248  lr:0.000010
[ Sun Jul  7 04:56:54 2024 ] 	Batch(5700/7879) done. Loss: 0.0067  lr:0.000010
[ Sun Jul  7 04:57:12 2024 ] 	Batch(5800/7879) done. Loss: 0.0030  lr:0.000010
[ Sun Jul  7 04:57:30 2024 ] 	Batch(5900/7879) done. Loss: 0.0578  lr:0.000010
[ Sun Jul  7 04:57:48 2024 ] 
Training: Epoch [51/120], Step [5999], Loss: 0.004426966421306133, Training Accuracy: 98.08333333333333
[ Sun Jul  7 04:57:48 2024 ] 	Batch(6000/7879) done. Loss: 0.0967  lr:0.000010
[ Sun Jul  7 04:58:06 2024 ] 	Batch(6100/7879) done. Loss: 0.0334  lr:0.000010
[ Sun Jul  7 04:58:24 2024 ] 	Batch(6200/7879) done. Loss: 0.0185  lr:0.000010
[ Sun Jul  7 04:58:42 2024 ] 	Batch(6300/7879) done. Loss: 0.4455  lr:0.000010
[ Sun Jul  7 04:59:00 2024 ] 	Batch(6400/7879) done. Loss: 0.0363  lr:0.000010
[ Sun Jul  7 04:59:17 2024 ] 
Training: Epoch [51/120], Step [6499], Loss: 0.005397375673055649, Training Accuracy: 98.10576923076924
[ Sun Jul  7 04:59:18 2024 ] 	Batch(6500/7879) done. Loss: 0.0136  lr:0.000010
[ Sun Jul  7 04:59:35 2024 ] 	Batch(6600/7879) done. Loss: 0.0199  lr:0.000010
[ Sun Jul  7 04:59:53 2024 ] 	Batch(6700/7879) done. Loss: 0.0234  lr:0.000010
[ Sun Jul  7 05:00:11 2024 ] 	Batch(6800/7879) done. Loss: 0.2061  lr:0.000010
[ Sun Jul  7 05:00:29 2024 ] 	Batch(6900/7879) done. Loss: 0.0081  lr:0.000010
[ Sun Jul  7 05:00:47 2024 ] 
Training: Epoch [51/120], Step [6999], Loss: 0.0007484174566343427, Training Accuracy: 98.1125
[ Sun Jul  7 05:00:47 2024 ] 	Batch(7000/7879) done. Loss: 0.0111  lr:0.000010
[ Sun Jul  7 05:01:05 2024 ] 	Batch(7100/7879) done. Loss: 0.1087  lr:0.000010
[ Sun Jul  7 05:01:23 2024 ] 	Batch(7200/7879) done. Loss: 0.0152  lr:0.000010
[ Sun Jul  7 05:01:41 2024 ] 	Batch(7300/7879) done. Loss: 0.0318  lr:0.000010
[ Sun Jul  7 05:01:59 2024 ] 	Batch(7400/7879) done. Loss: 0.0120  lr:0.000010
[ Sun Jul  7 05:02:17 2024 ] 
Training: Epoch [51/120], Step [7499], Loss: 0.014637600630521774, Training Accuracy: 98.105
[ Sun Jul  7 05:02:17 2024 ] 	Batch(7500/7879) done. Loss: 0.1398  lr:0.000010
[ Sun Jul  7 05:02:35 2024 ] 	Batch(7600/7879) done. Loss: 0.0467  lr:0.000010
[ Sun Jul  7 05:02:53 2024 ] 	Batch(7700/7879) done. Loss: 0.0307  lr:0.000010
[ Sun Jul  7 05:03:11 2024 ] 	Batch(7800/7879) done. Loss: 0.0033  lr:0.000010
[ Sun Jul  7 05:03:25 2024 ] 	Mean training loss: 0.0799.
[ Sun Jul  7 05:03:25 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 05:03:25 2024 ] Training epoch: 53
[ Sun Jul  7 05:03:26 2024 ] 	Batch(0/7879) done. Loss: 0.2763  lr:0.000010
[ Sun Jul  7 05:03:44 2024 ] 	Batch(100/7879) done. Loss: 0.0241  lr:0.000010
[ Sun Jul  7 05:04:02 2024 ] 	Batch(200/7879) done. Loss: 0.0057  lr:0.000010
[ Sun Jul  7 05:04:20 2024 ] 	Batch(300/7879) done. Loss: 0.0360  lr:0.000010
[ Sun Jul  7 05:04:39 2024 ] 	Batch(400/7879) done. Loss: 0.0606  lr:0.000010
[ Sun Jul  7 05:04:57 2024 ] 
Training: Epoch [52/120], Step [499], Loss: 0.012428308837115765, Training Accuracy: 98.475
[ Sun Jul  7 05:04:58 2024 ] 	Batch(500/7879) done. Loss: 0.0301  lr:0.000010
[ Sun Jul  7 05:05:16 2024 ] 	Batch(600/7879) done. Loss: 0.3245  lr:0.000010
[ Sun Jul  7 05:05:35 2024 ] 	Batch(700/7879) done. Loss: 0.3289  lr:0.000010
[ Sun Jul  7 05:05:53 2024 ] 	Batch(800/7879) done. Loss: 0.1781  lr:0.000010
[ Sun Jul  7 05:06:12 2024 ] 	Batch(900/7879) done. Loss: 0.1356  lr:0.000010
[ Sun Jul  7 05:06:30 2024 ] 
Training: Epoch [52/120], Step [999], Loss: 0.14632238447666168, Training Accuracy: 98.175
[ Sun Jul  7 05:06:30 2024 ] 	Batch(1000/7879) done. Loss: 0.0709  lr:0.000010
[ Sun Jul  7 05:06:48 2024 ] 	Batch(1100/7879) done. Loss: 0.0438  lr:0.000010
[ Sun Jul  7 05:07:06 2024 ] 	Batch(1200/7879) done. Loss: 0.0684  lr:0.000010
[ Sun Jul  7 05:07:24 2024 ] 	Batch(1300/7879) done. Loss: 0.0434  lr:0.000010
[ Sun Jul  7 05:07:42 2024 ] 	Batch(1400/7879) done. Loss: 0.1308  lr:0.000010
[ Sun Jul  7 05:08:00 2024 ] 
Training: Epoch [52/120], Step [1499], Loss: 0.024376407265663147, Training Accuracy: 98.16666666666667
[ Sun Jul  7 05:08:00 2024 ] 	Batch(1500/7879) done. Loss: 0.0209  lr:0.000010
[ Sun Jul  7 05:08:18 2024 ] 	Batch(1600/7879) done. Loss: 0.0092  lr:0.000010
[ Sun Jul  7 05:08:36 2024 ] 	Batch(1700/7879) done. Loss: 0.0148  lr:0.000010
[ Sun Jul  7 05:08:54 2024 ] 	Batch(1800/7879) done. Loss: 0.1287  lr:0.000010
[ Sun Jul  7 05:09:12 2024 ] 	Batch(1900/7879) done. Loss: 0.0477  lr:0.000010
[ Sun Jul  7 05:09:30 2024 ] 
Training: Epoch [52/120], Step [1999], Loss: 0.015142896212637424, Training Accuracy: 98.2
[ Sun Jul  7 05:09:30 2024 ] 	Batch(2000/7879) done. Loss: 0.0138  lr:0.000010
[ Sun Jul  7 05:09:48 2024 ] 	Batch(2100/7879) done. Loss: 0.0402  lr:0.000010
[ Sun Jul  7 05:10:06 2024 ] 	Batch(2200/7879) done. Loss: 0.0991  lr:0.000010
[ Sun Jul  7 05:10:24 2024 ] 	Batch(2300/7879) done. Loss: 0.0784  lr:0.000010
[ Sun Jul  7 05:10:42 2024 ] 	Batch(2400/7879) done. Loss: 0.0148  lr:0.000010
[ Sun Jul  7 05:11:00 2024 ] 
Training: Epoch [52/120], Step [2499], Loss: 0.30916911363601685, Training Accuracy: 98.175
[ Sun Jul  7 05:11:00 2024 ] 	Batch(2500/7879) done. Loss: 0.2156  lr:0.000010
[ Sun Jul  7 05:11:18 2024 ] 	Batch(2600/7879) done. Loss: 0.0020  lr:0.000010
[ Sun Jul  7 05:11:36 2024 ] 	Batch(2700/7879) done. Loss: 0.0087  lr:0.000010
[ Sun Jul  7 05:11:54 2024 ] 	Batch(2800/7879) done. Loss: 0.0629  lr:0.000010
[ Sun Jul  7 05:12:12 2024 ] 	Batch(2900/7879) done. Loss: 0.0964  lr:0.000010
[ Sun Jul  7 05:12:29 2024 ] 
Training: Epoch [52/120], Step [2999], Loss: 0.0594915896654129, Training Accuracy: 98.20416666666667
[ Sun Jul  7 05:12:29 2024 ] 	Batch(3000/7879) done. Loss: 0.0523  lr:0.000010
[ Sun Jul  7 05:12:47 2024 ] 	Batch(3100/7879) done. Loss: 0.0437  lr:0.000010
[ Sun Jul  7 05:13:05 2024 ] 	Batch(3200/7879) done. Loss: 0.0412  lr:0.000010
[ Sun Jul  7 05:13:24 2024 ] 	Batch(3300/7879) done. Loss: 0.0128  lr:0.000010
[ Sun Jul  7 05:13:43 2024 ] 	Batch(3400/7879) done. Loss: 0.1051  lr:0.000010
[ Sun Jul  7 05:14:01 2024 ] 
Training: Epoch [52/120], Step [3499], Loss: 0.012174827046692371, Training Accuracy: 98.22857142857143
[ Sun Jul  7 05:14:01 2024 ] 	Batch(3500/7879) done. Loss: 0.0506  lr:0.000010
[ Sun Jul  7 05:14:20 2024 ] 	Batch(3600/7879) done. Loss: 0.0196  lr:0.000010
[ Sun Jul  7 05:14:38 2024 ] 	Batch(3700/7879) done. Loss: 0.0234  lr:0.000010
[ Sun Jul  7 05:14:57 2024 ] 	Batch(3800/7879) done. Loss: 0.3541  lr:0.000010
[ Sun Jul  7 05:15:15 2024 ] 	Batch(3900/7879) done. Loss: 0.0607  lr:0.000010
[ Sun Jul  7 05:15:34 2024 ] 
Training: Epoch [52/120], Step [3999], Loss: 0.07544131577014923, Training Accuracy: 98.221875
[ Sun Jul  7 05:15:34 2024 ] 	Batch(4000/7879) done. Loss: 0.2711  lr:0.000010
[ Sun Jul  7 05:15:53 2024 ] 	Batch(4100/7879) done. Loss: 0.0491  lr:0.000010
[ Sun Jul  7 05:16:11 2024 ] 	Batch(4200/7879) done. Loss: 0.0208  lr:0.000010
[ Sun Jul  7 05:16:30 2024 ] 	Batch(4300/7879) done. Loss: 0.0284  lr:0.000010
[ Sun Jul  7 05:16:48 2024 ] 	Batch(4400/7879) done. Loss: 0.0457  lr:0.000010
[ Sun Jul  7 05:17:06 2024 ] 
Training: Epoch [52/120], Step [4499], Loss: 0.024026915431022644, Training Accuracy: 98.175
[ Sun Jul  7 05:17:06 2024 ] 	Batch(4500/7879) done. Loss: 0.1778  lr:0.000010
[ Sun Jul  7 05:17:24 2024 ] 	Batch(4600/7879) done. Loss: 0.1926  lr:0.000010
[ Sun Jul  7 05:17:42 2024 ] 	Batch(4700/7879) done. Loss: 0.0195  lr:0.000010
[ Sun Jul  7 05:18:00 2024 ] 	Batch(4800/7879) done. Loss: 0.0013  lr:0.000010
[ Sun Jul  7 05:18:18 2024 ] 	Batch(4900/7879) done. Loss: 0.0091  lr:0.000010
[ Sun Jul  7 05:18:36 2024 ] 
Training: Epoch [52/120], Step [4999], Loss: 0.2958720922470093, Training Accuracy: 98.17
[ Sun Jul  7 05:18:36 2024 ] 	Batch(5000/7879) done. Loss: 0.0360  lr:0.000010
[ Sun Jul  7 05:18:54 2024 ] 	Batch(5100/7879) done. Loss: 0.0267  lr:0.000010
[ Sun Jul  7 05:19:12 2024 ] 	Batch(5200/7879) done. Loss: 0.0051  lr:0.000010
[ Sun Jul  7 05:19:30 2024 ] 	Batch(5300/7879) done. Loss: 0.0234  lr:0.000010
[ Sun Jul  7 05:19:48 2024 ] 	Batch(5400/7879) done. Loss: 0.0990  lr:0.000010
[ Sun Jul  7 05:20:06 2024 ] 
Training: Epoch [52/120], Step [5499], Loss: 0.030187178403139114, Training Accuracy: 98.175
[ Sun Jul  7 05:20:06 2024 ] 	Batch(5500/7879) done. Loss: 0.0300  lr:0.000010
[ Sun Jul  7 05:20:24 2024 ] 	Batch(5600/7879) done. Loss: 0.1692  lr:0.000010
[ Sun Jul  7 05:20:42 2024 ] 	Batch(5700/7879) done. Loss: 0.0067  lr:0.000010
[ Sun Jul  7 05:21:00 2024 ] 	Batch(5800/7879) done. Loss: 0.0217  lr:0.000010
[ Sun Jul  7 05:21:18 2024 ] 	Batch(5900/7879) done. Loss: 0.0974  lr:0.000010
[ Sun Jul  7 05:21:35 2024 ] 
Training: Epoch [52/120], Step [5999], Loss: 0.02567269466817379, Training Accuracy: 98.19166666666666
[ Sun Jul  7 05:21:36 2024 ] 	Batch(6000/7879) done. Loss: 0.0033  lr:0.000010
[ Sun Jul  7 05:21:54 2024 ] 	Batch(6100/7879) done. Loss: 0.0182  lr:0.000010
[ Sun Jul  7 05:22:12 2024 ] 	Batch(6200/7879) done. Loss: 0.3855  lr:0.000010
[ Sun Jul  7 05:22:30 2024 ] 	Batch(6300/7879) done. Loss: 0.0609  lr:0.000010
[ Sun Jul  7 05:22:48 2024 ] 	Batch(6400/7879) done. Loss: 0.0318  lr:0.000010
[ Sun Jul  7 05:23:06 2024 ] 
Training: Epoch [52/120], Step [6499], Loss: 0.008086106739938259, Training Accuracy: 98.20961538461539
[ Sun Jul  7 05:23:06 2024 ] 	Batch(6500/7879) done. Loss: 0.2039  lr:0.000010
[ Sun Jul  7 05:23:24 2024 ] 	Batch(6600/7879) done. Loss: 0.2943  lr:0.000010
[ Sun Jul  7 05:23:42 2024 ] 	Batch(6700/7879) done. Loss: 0.0862  lr:0.000010
[ Sun Jul  7 05:24:00 2024 ] 	Batch(6800/7879) done. Loss: 0.0273  lr:0.000010
[ Sun Jul  7 05:24:17 2024 ] 	Batch(6900/7879) done. Loss: 0.1266  lr:0.000010
[ Sun Jul  7 05:24:35 2024 ] 
Training: Epoch [52/120], Step [6999], Loss: 0.02509436011314392, Training Accuracy: 98.21428571428571
[ Sun Jul  7 05:24:35 2024 ] 	Batch(7000/7879) done. Loss: 0.0016  lr:0.000010
[ Sun Jul  7 05:24:53 2024 ] 	Batch(7100/7879) done. Loss: 0.0863  lr:0.000010
[ Sun Jul  7 05:25:11 2024 ] 	Batch(7200/7879) done. Loss: 0.1487  lr:0.000010
[ Sun Jul  7 05:25:29 2024 ] 	Batch(7300/7879) done. Loss: 0.0424  lr:0.000010
[ Sun Jul  7 05:25:47 2024 ] 	Batch(7400/7879) done. Loss: 0.1075  lr:0.000010
[ Sun Jul  7 05:26:05 2024 ] 
Training: Epoch [52/120], Step [7499], Loss: 0.007283675484359264, Training Accuracy: 98.20333333333333
[ Sun Jul  7 05:26:05 2024 ] 	Batch(7500/7879) done. Loss: 0.0249  lr:0.000010
[ Sun Jul  7 05:26:23 2024 ] 	Batch(7600/7879) done. Loss: 0.0050  lr:0.000010
[ Sun Jul  7 05:26:41 2024 ] 	Batch(7700/7879) done. Loss: 0.2646  lr:0.000010
[ Sun Jul  7 05:26:59 2024 ] 	Batch(7800/7879) done. Loss: 0.0554  lr:0.000010
[ Sun Jul  7 05:27:13 2024 ] 	Mean training loss: 0.0784.
[ Sun Jul  7 05:27:13 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 05:27:13 2024 ] Training epoch: 54
[ Sun Jul  7 05:27:14 2024 ] 	Batch(0/7879) done. Loss: 0.0297  lr:0.000010
[ Sun Jul  7 05:27:32 2024 ] 	Batch(100/7879) done. Loss: 0.0512  lr:0.000010
[ Sun Jul  7 05:27:49 2024 ] 	Batch(200/7879) done. Loss: 0.1456  lr:0.000010
[ Sun Jul  7 05:28:07 2024 ] 	Batch(300/7879) done. Loss: 0.0308  lr:0.000010
[ Sun Jul  7 05:28:25 2024 ] 	Batch(400/7879) done. Loss: 0.0237  lr:0.000010
[ Sun Jul  7 05:28:43 2024 ] 
Training: Epoch [53/120], Step [499], Loss: 0.004438374191522598, Training Accuracy: 98.125
[ Sun Jul  7 05:28:43 2024 ] 	Batch(500/7879) done. Loss: 0.0333  lr:0.000010
[ Sun Jul  7 05:29:01 2024 ] 	Batch(600/7879) done. Loss: 0.0088  lr:0.000010
[ Sun Jul  7 05:29:19 2024 ] 	Batch(700/7879) done. Loss: 0.0110  lr:0.000010
[ Sun Jul  7 05:29:37 2024 ] 	Batch(800/7879) done. Loss: 0.0526  lr:0.000010
[ Sun Jul  7 05:29:55 2024 ] 	Batch(900/7879) done. Loss: 0.0109  lr:0.000010
[ Sun Jul  7 05:30:13 2024 ] 
Training: Epoch [53/120], Step [999], Loss: 0.0017355462769046426, Training Accuracy: 98.02499999999999
[ Sun Jul  7 05:30:13 2024 ] 	Batch(1000/7879) done. Loss: 0.2879  lr:0.000010
[ Sun Jul  7 05:30:31 2024 ] 	Batch(1100/7879) done. Loss: 0.0368  lr:0.000010
[ Sun Jul  7 05:30:49 2024 ] 	Batch(1200/7879) done. Loss: 0.0228  lr:0.000010
[ Sun Jul  7 05:31:07 2024 ] 	Batch(1300/7879) done. Loss: 0.0040  lr:0.000010
[ Sun Jul  7 05:31:25 2024 ] 	Batch(1400/7879) done. Loss: 0.0821  lr:0.000010
[ Sun Jul  7 05:31:42 2024 ] 
Training: Epoch [53/120], Step [1499], Loss: 0.035134244710206985, Training Accuracy: 98.075
[ Sun Jul  7 05:31:43 2024 ] 	Batch(1500/7879) done. Loss: 0.0800  lr:0.000010
[ Sun Jul  7 05:32:01 2024 ] 	Batch(1600/7879) done. Loss: 0.0141  lr:0.000010
[ Sun Jul  7 05:32:19 2024 ] 	Batch(1700/7879) done. Loss: 0.0098  lr:0.000010
[ Sun Jul  7 05:32:38 2024 ] 	Batch(1800/7879) done. Loss: 0.0766  lr:0.000010
[ Sun Jul  7 05:32:56 2024 ] 	Batch(1900/7879) done. Loss: 0.1115  lr:0.000010
[ Sun Jul  7 05:33:15 2024 ] 
Training: Epoch [53/120], Step [1999], Loss: 0.07609745860099792, Training Accuracy: 98.18124999999999
[ Sun Jul  7 05:33:15 2024 ] 	Batch(2000/7879) done. Loss: 0.0254  lr:0.000010
[ Sun Jul  7 05:33:34 2024 ] 	Batch(2100/7879) done. Loss: 0.5401  lr:0.000010
[ Sun Jul  7 05:33:52 2024 ] 	Batch(2200/7879) done. Loss: 0.0138  lr:0.000010
[ Sun Jul  7 05:34:11 2024 ] 	Batch(2300/7879) done. Loss: 0.0182  lr:0.000010
[ Sun Jul  7 05:34:30 2024 ] 	Batch(2400/7879) done. Loss: 0.0159  lr:0.000010
[ Sun Jul  7 05:34:47 2024 ] 
Training: Epoch [53/120], Step [2499], Loss: 0.18695345520973206, Training Accuracy: 98.11
[ Sun Jul  7 05:34:48 2024 ] 	Batch(2500/7879) done. Loss: 0.0158  lr:0.000010
[ Sun Jul  7 05:35:06 2024 ] 	Batch(2600/7879) done. Loss: 0.1100  lr:0.000010
[ Sun Jul  7 05:35:23 2024 ] 	Batch(2700/7879) done. Loss: 0.0550  lr:0.000010
[ Sun Jul  7 05:35:42 2024 ] 	Batch(2800/7879) done. Loss: 0.0253  lr:0.000010
[ Sun Jul  7 05:36:00 2024 ] 	Batch(2900/7879) done. Loss: 0.0065  lr:0.000010
[ Sun Jul  7 05:36:19 2024 ] 
Training: Epoch [53/120], Step [2999], Loss: 0.005246483720839024, Training Accuracy: 98.11666666666666
[ Sun Jul  7 05:36:19 2024 ] 	Batch(3000/7879) done. Loss: 0.0041  lr:0.000010
[ Sun Jul  7 05:36:37 2024 ] 	Batch(3100/7879) done. Loss: 0.3210  lr:0.000010
[ Sun Jul  7 05:36:56 2024 ] 	Batch(3200/7879) done. Loss: 0.1671  lr:0.000010
[ Sun Jul  7 05:37:15 2024 ] 	Batch(3300/7879) done. Loss: 0.1416  lr:0.000010
[ Sun Jul  7 05:37:33 2024 ] 	Batch(3400/7879) done. Loss: 0.0135  lr:0.000010
[ Sun Jul  7 05:37:51 2024 ] 
Training: Epoch [53/120], Step [3499], Loss: 0.023957915604114532, Training Accuracy: 98.13214285714285
[ Sun Jul  7 05:37:51 2024 ] 	Batch(3500/7879) done. Loss: 0.4571  lr:0.000010
[ Sun Jul  7 05:38:09 2024 ] 	Batch(3600/7879) done. Loss: 0.1368  lr:0.000010
[ Sun Jul  7 05:38:27 2024 ] 	Batch(3700/7879) done. Loss: 0.0152  lr:0.000010
[ Sun Jul  7 05:38:45 2024 ] 	Batch(3800/7879) done. Loss: 0.0034  lr:0.000010
[ Sun Jul  7 05:39:03 2024 ] 	Batch(3900/7879) done. Loss: 0.0037  lr:0.000010
[ Sun Jul  7 05:39:20 2024 ] 
Training: Epoch [53/120], Step [3999], Loss: 0.029250625520944595, Training Accuracy: 98.171875
[ Sun Jul  7 05:39:21 2024 ] 	Batch(4000/7879) done. Loss: 0.1407  lr:0.000010
[ Sun Jul  7 05:39:39 2024 ] 	Batch(4100/7879) done. Loss: 0.3454  lr:0.000010
[ Sun Jul  7 05:39:58 2024 ] 	Batch(4200/7879) done. Loss: 0.0221  lr:0.000010
[ Sun Jul  7 05:40:16 2024 ] 	Batch(4300/7879) done. Loss: 0.5844  lr:0.000010
[ Sun Jul  7 05:40:35 2024 ] 	Batch(4400/7879) done. Loss: 0.0590  lr:0.000010
[ Sun Jul  7 05:40:53 2024 ] 
Training: Epoch [53/120], Step [4499], Loss: 0.07920445501804352, Training Accuracy: 98.175
[ Sun Jul  7 05:40:54 2024 ] 	Batch(4500/7879) done. Loss: 0.0043  lr:0.000010
[ Sun Jul  7 05:41:12 2024 ] 	Batch(4600/7879) done. Loss: 0.0740  lr:0.000010
[ Sun Jul  7 05:41:31 2024 ] 	Batch(4700/7879) done. Loss: 0.0449  lr:0.000010
[ Sun Jul  7 05:41:49 2024 ] 	Batch(4800/7879) done. Loss: 0.0283  lr:0.000010
[ Sun Jul  7 05:42:08 2024 ] 	Batch(4900/7879) done. Loss: 0.0850  lr:0.000010
[ Sun Jul  7 05:42:26 2024 ] 
Training: Epoch [53/120], Step [4999], Loss: 0.023722685873508453, Training Accuracy: 98.185
[ Sun Jul  7 05:42:27 2024 ] 	Batch(5000/7879) done. Loss: 0.0571  lr:0.000010
[ Sun Jul  7 05:42:45 2024 ] 	Batch(5100/7879) done. Loss: 0.4057  lr:0.000010
[ Sun Jul  7 05:43:04 2024 ] 	Batch(5200/7879) done. Loss: 0.0250  lr:0.000010
[ Sun Jul  7 05:43:22 2024 ] 	Batch(5300/7879) done. Loss: 0.0194  lr:0.000010
[ Sun Jul  7 05:43:40 2024 ] 	Batch(5400/7879) done. Loss: 0.0175  lr:0.000010
[ Sun Jul  7 05:43:57 2024 ] 
Training: Epoch [53/120], Step [5499], Loss: 0.08701224625110626, Training Accuracy: 98.19318181818181
[ Sun Jul  7 05:43:58 2024 ] 	Batch(5500/7879) done. Loss: 0.0208  lr:0.000010
[ Sun Jul  7 05:44:16 2024 ] 	Batch(5600/7879) done. Loss: 0.0861  lr:0.000010
[ Sun Jul  7 05:44:34 2024 ] 	Batch(5700/7879) done. Loss: 0.0637  lr:0.000010
[ Sun Jul  7 05:44:53 2024 ] 	Batch(5800/7879) done. Loss: 0.0189  lr:0.000010
[ Sun Jul  7 05:45:11 2024 ] 	Batch(5900/7879) done. Loss: 0.0253  lr:0.000010
[ Sun Jul  7 05:45:30 2024 ] 
Training: Epoch [53/120], Step [5999], Loss: 0.03499815613031387, Training Accuracy: 98.20625
[ Sun Jul  7 05:45:30 2024 ] 	Batch(6000/7879) done. Loss: 0.0274  lr:0.000010
[ Sun Jul  7 05:45:48 2024 ] 	Batch(6100/7879) done. Loss: 0.0246  lr:0.000010
[ Sun Jul  7 05:46:06 2024 ] 	Batch(6200/7879) done. Loss: 0.0091  lr:0.000010
[ Sun Jul  7 05:46:24 2024 ] 	Batch(6300/7879) done. Loss: 0.0103  lr:0.000010
[ Sun Jul  7 05:46:42 2024 ] 	Batch(6400/7879) done. Loss: 0.0153  lr:0.000010
[ Sun Jul  7 05:47:00 2024 ] 
Training: Epoch [53/120], Step [6499], Loss: 0.006242321338504553, Training Accuracy: 98.21730769230768
[ Sun Jul  7 05:47:00 2024 ] 	Batch(6500/7879) done. Loss: 0.0286  lr:0.000010
[ Sun Jul  7 05:47:18 2024 ] 	Batch(6600/7879) done. Loss: 0.1223  lr:0.000010
[ Sun Jul  7 05:47:36 2024 ] 	Batch(6700/7879) done. Loss: 0.0183  lr:0.000010
[ Sun Jul  7 05:47:54 2024 ] 	Batch(6800/7879) done. Loss: 0.2982  lr:0.000010
[ Sun Jul  7 05:48:13 2024 ] 	Batch(6900/7879) done. Loss: 0.0414  lr:0.000010
[ Sun Jul  7 05:48:31 2024 ] 
Training: Epoch [53/120], Step [6999], Loss: 0.06549515575170517, Training Accuracy: 98.24107142857143
[ Sun Jul  7 05:48:31 2024 ] 	Batch(7000/7879) done. Loss: 0.0127  lr:0.000010
[ Sun Jul  7 05:48:49 2024 ] 	Batch(7100/7879) done. Loss: 0.2084  lr:0.000010
[ Sun Jul  7 05:49:07 2024 ] 	Batch(7200/7879) done. Loss: 0.0776  lr:0.000010
[ Sun Jul  7 05:49:25 2024 ] 	Batch(7300/7879) done. Loss: 0.1252  lr:0.000010
[ Sun Jul  7 05:49:43 2024 ] 	Batch(7400/7879) done. Loss: 0.0420  lr:0.000010
[ Sun Jul  7 05:50:01 2024 ] 
Training: Epoch [53/120], Step [7499], Loss: 0.050343360751867294, Training Accuracy: 98.23166666666665
[ Sun Jul  7 05:50:01 2024 ] 	Batch(7500/7879) done. Loss: 0.0258  lr:0.000010
[ Sun Jul  7 05:50:19 2024 ] 	Batch(7600/7879) done. Loss: 0.0061  lr:0.000010
[ Sun Jul  7 05:50:37 2024 ] 	Batch(7700/7879) done. Loss: 0.0081  lr:0.000010
[ Sun Jul  7 05:50:55 2024 ] 	Batch(7800/7879) done. Loss: 0.0087  lr:0.000010
[ Sun Jul  7 05:51:09 2024 ] 	Mean training loss: 0.0777.
[ Sun Jul  7 05:51:09 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 05:51:09 2024 ] Training epoch: 55
[ Sun Jul  7 05:51:10 2024 ] 	Batch(0/7879) done. Loss: 0.0498  lr:0.000010
[ Sun Jul  7 05:51:28 2024 ] 	Batch(100/7879) done. Loss: 0.1312  lr:0.000010
[ Sun Jul  7 05:51:46 2024 ] 	Batch(200/7879) done. Loss: 0.0461  lr:0.000010
[ Sun Jul  7 05:52:05 2024 ] 	Batch(300/7879) done. Loss: 0.1025  lr:0.000010
[ Sun Jul  7 05:52:23 2024 ] 	Batch(400/7879) done. Loss: 0.0363  lr:0.000010
[ Sun Jul  7 05:52:41 2024 ] 
Training: Epoch [54/120], Step [499], Loss: 0.016924912109971046, Training Accuracy: 98.02499999999999
[ Sun Jul  7 05:52:42 2024 ] 	Batch(500/7879) done. Loss: 0.0321  lr:0.000010
[ Sun Jul  7 05:53:00 2024 ] 	Batch(600/7879) done. Loss: 0.0120  lr:0.000010
[ Sun Jul  7 05:53:18 2024 ] 	Batch(700/7879) done. Loss: 0.0866  lr:0.000010
[ Sun Jul  7 05:53:37 2024 ] 	Batch(800/7879) done. Loss: 0.1753  lr:0.000010
[ Sun Jul  7 05:53:55 2024 ] 	Batch(900/7879) done. Loss: 0.0907  lr:0.000010
[ Sun Jul  7 05:54:13 2024 ] 
Training: Epoch [54/120], Step [999], Loss: 0.3322509825229645, Training Accuracy: 98.1875
[ Sun Jul  7 05:54:13 2024 ] 	Batch(1000/7879) done. Loss: 0.0225  lr:0.000010
[ Sun Jul  7 05:54:32 2024 ] 	Batch(1100/7879) done. Loss: 0.1177  lr:0.000010
[ Sun Jul  7 05:54:50 2024 ] 	Batch(1200/7879) done. Loss: 0.0323  lr:0.000010
[ Sun Jul  7 05:55:08 2024 ] 	Batch(1300/7879) done. Loss: 0.0605  lr:0.000010
[ Sun Jul  7 05:55:27 2024 ] 	Batch(1400/7879) done. Loss: 0.0332  lr:0.000010
[ Sun Jul  7 05:55:45 2024 ] 
Training: Epoch [54/120], Step [1499], Loss: 0.0042649139650166035, Training Accuracy: 98.33333333333333
[ Sun Jul  7 05:55:45 2024 ] 	Batch(1500/7879) done. Loss: 0.0777  lr:0.000010
[ Sun Jul  7 05:56:04 2024 ] 	Batch(1600/7879) done. Loss: 0.0385  lr:0.000010
[ Sun Jul  7 05:56:22 2024 ] 	Batch(1700/7879) done. Loss: 0.0352  lr:0.000010
[ Sun Jul  7 05:56:40 2024 ] 	Batch(1800/7879) done. Loss: 0.0337  lr:0.000010
[ Sun Jul  7 05:56:58 2024 ] 	Batch(1900/7879) done. Loss: 0.0099  lr:0.000010
[ Sun Jul  7 05:57:16 2024 ] 
Training: Epoch [54/120], Step [1999], Loss: 0.9906919002532959, Training Accuracy: 98.24374999999999
[ Sun Jul  7 05:57:16 2024 ] 	Batch(2000/7879) done. Loss: 0.1463  lr:0.000010
[ Sun Jul  7 05:57:34 2024 ] 	Batch(2100/7879) done. Loss: 0.0036  lr:0.000010
[ Sun Jul  7 05:57:52 2024 ] 	Batch(2200/7879) done. Loss: 0.1268  lr:0.000010
[ Sun Jul  7 05:58:10 2024 ] 	Batch(2300/7879) done. Loss: 0.0113  lr:0.000010
[ Sun Jul  7 05:58:28 2024 ] 	Batch(2400/7879) done. Loss: 0.4348  lr:0.000010
[ Sun Jul  7 05:58:45 2024 ] 
Training: Epoch [54/120], Step [2499], Loss: 0.008637476712465286, Training Accuracy: 98.25
[ Sun Jul  7 05:58:46 2024 ] 	Batch(2500/7879) done. Loss: 0.0425  lr:0.000010
[ Sun Jul  7 05:59:04 2024 ] 	Batch(2600/7879) done. Loss: 0.0644  lr:0.000010
[ Sun Jul  7 05:59:22 2024 ] 	Batch(2700/7879) done. Loss: 0.1371  lr:0.000010
[ Sun Jul  7 05:59:41 2024 ] 	Batch(2800/7879) done. Loss: 0.0096  lr:0.000010
[ Sun Jul  7 05:59:59 2024 ] 	Batch(2900/7879) done. Loss: 0.0856  lr:0.000010
[ Sun Jul  7 06:00:17 2024 ] 
Training: Epoch [54/120], Step [2999], Loss: 0.03227036073803902, Training Accuracy: 98.225
[ Sun Jul  7 06:00:17 2024 ] 	Batch(3000/7879) done. Loss: 0.0044  lr:0.000010
[ Sun Jul  7 06:00:35 2024 ] 	Batch(3100/7879) done. Loss: 0.1244  lr:0.000010
[ Sun Jul  7 06:00:53 2024 ] 	Batch(3200/7879) done. Loss: 0.0202  lr:0.000010
[ Sun Jul  7 06:01:11 2024 ] 	Batch(3300/7879) done. Loss: 0.0982  lr:0.000010
[ Sun Jul  7 06:01:29 2024 ] 	Batch(3400/7879) done. Loss: 0.0143  lr:0.000010
[ Sun Jul  7 06:01:47 2024 ] 
Training: Epoch [54/120], Step [3499], Loss: 0.18240824341773987, Training Accuracy: 98.25714285714285
[ Sun Jul  7 06:01:47 2024 ] 	Batch(3500/7879) done. Loss: 0.0541  lr:0.000010
[ Sun Jul  7 06:02:05 2024 ] 	Batch(3600/7879) done. Loss: 0.0616  lr:0.000010
[ Sun Jul  7 06:02:23 2024 ] 	Batch(3700/7879) done. Loss: 0.2441  lr:0.000010
[ Sun Jul  7 06:02:41 2024 ] 	Batch(3800/7879) done. Loss: 0.0996  lr:0.000010
[ Sun Jul  7 06:02:59 2024 ] 	Batch(3900/7879) done. Loss: 0.0037  lr:0.000010
[ Sun Jul  7 06:03:17 2024 ] 
Training: Epoch [54/120], Step [3999], Loss: 0.07348191738128662, Training Accuracy: 98.24374999999999
[ Sun Jul  7 06:03:17 2024 ] 	Batch(4000/7879) done. Loss: 0.0058  lr:0.000010
[ Sun Jul  7 06:03:35 2024 ] 	Batch(4100/7879) done. Loss: 0.1536  lr:0.000010
[ Sun Jul  7 06:03:53 2024 ] 	Batch(4200/7879) done. Loss: 0.0551  lr:0.000010
[ Sun Jul  7 06:04:11 2024 ] 	Batch(4300/7879) done. Loss: 0.2199  lr:0.000010
[ Sun Jul  7 06:04:29 2024 ] 	Batch(4400/7879) done. Loss: 0.0354  lr:0.000010
[ Sun Jul  7 06:04:47 2024 ] 
Training: Epoch [54/120], Step [4499], Loss: 0.007542201317846775, Training Accuracy: 98.275
[ Sun Jul  7 06:04:47 2024 ] 	Batch(4500/7879) done. Loss: 0.1203  lr:0.000010
[ Sun Jul  7 06:05:05 2024 ] 	Batch(4600/7879) done. Loss: 0.0357  lr:0.000010
[ Sun Jul  7 06:05:23 2024 ] 	Batch(4700/7879) done. Loss: 0.0071  lr:0.000010
[ Sun Jul  7 06:05:41 2024 ] 	Batch(4800/7879) done. Loss: 0.1013  lr:0.000010
[ Sun Jul  7 06:05:59 2024 ] 	Batch(4900/7879) done. Loss: 0.1442  lr:0.000010
[ Sun Jul  7 06:06:18 2024 ] 
Training: Epoch [54/120], Step [4999], Loss: 0.08978976309299469, Training Accuracy: 98.28
[ Sun Jul  7 06:06:18 2024 ] 	Batch(5000/7879) done. Loss: 0.0194  lr:0.000010
[ Sun Jul  7 06:06:36 2024 ] 	Batch(5100/7879) done. Loss: 0.0026  lr:0.000010
[ Sun Jul  7 06:06:55 2024 ] 	Batch(5200/7879) done. Loss: 0.1175  lr:0.000010
[ Sun Jul  7 06:07:13 2024 ] 	Batch(5300/7879) done. Loss: 0.2496  lr:0.000010
[ Sun Jul  7 06:07:31 2024 ] 	Batch(5400/7879) done. Loss: 0.1061  lr:0.000010
[ Sun Jul  7 06:07:49 2024 ] 
Training: Epoch [54/120], Step [5499], Loss: 0.03245805948972702, Training Accuracy: 98.26590909090909
[ Sun Jul  7 06:07:49 2024 ] 	Batch(5500/7879) done. Loss: 0.0063  lr:0.000010
[ Sun Jul  7 06:08:07 2024 ] 	Batch(5600/7879) done. Loss: 0.0235  lr:0.000010
[ Sun Jul  7 06:08:25 2024 ] 	Batch(5700/7879) done. Loss: 0.0884  lr:0.000010
[ Sun Jul  7 06:08:44 2024 ] 	Batch(5800/7879) done. Loss: 0.0179  lr:0.000010
[ Sun Jul  7 06:09:03 2024 ] 	Batch(5900/7879) done. Loss: 0.0050  lr:0.000010
[ Sun Jul  7 06:09:21 2024 ] 
Training: Epoch [54/120], Step [5999], Loss: 0.049531638622283936, Training Accuracy: 98.27916666666667
[ Sun Jul  7 06:09:21 2024 ] 	Batch(6000/7879) done. Loss: 0.0235  lr:0.000010
[ Sun Jul  7 06:09:40 2024 ] 	Batch(6100/7879) done. Loss: 0.0104  lr:0.000010
[ Sun Jul  7 06:09:58 2024 ] 	Batch(6200/7879) done. Loss: 0.0059  lr:0.000010
[ Sun Jul  7 06:10:17 2024 ] 	Batch(6300/7879) done. Loss: 0.0096  lr:0.000010
[ Sun Jul  7 06:10:35 2024 ] 	Batch(6400/7879) done. Loss: 0.1381  lr:0.000010
[ Sun Jul  7 06:10:53 2024 ] 
Training: Epoch [54/120], Step [6499], Loss: 0.0696890652179718, Training Accuracy: 98.29230769230769
[ Sun Jul  7 06:10:53 2024 ] 	Batch(6500/7879) done. Loss: 0.0023  lr:0.000010
[ Sun Jul  7 06:11:11 2024 ] 	Batch(6600/7879) done. Loss: 0.0117  lr:0.000010
[ Sun Jul  7 06:11:29 2024 ] 	Batch(6700/7879) done. Loss: 0.0382  lr:0.000010
[ Sun Jul  7 06:11:47 2024 ] 	Batch(6800/7879) done. Loss: 0.0486  lr:0.000010
[ Sun Jul  7 06:12:05 2024 ] 	Batch(6900/7879) done. Loss: 0.0212  lr:0.000010
[ Sun Jul  7 06:12:23 2024 ] 
Training: Epoch [54/120], Step [6999], Loss: 0.02981925755739212, Training Accuracy: 98.30357142857142
[ Sun Jul  7 06:12:23 2024 ] 	Batch(7000/7879) done. Loss: 0.0459  lr:0.000010
[ Sun Jul  7 06:12:41 2024 ] 	Batch(7100/7879) done. Loss: 0.0174  lr:0.000010
[ Sun Jul  7 06:12:59 2024 ] 	Batch(7200/7879) done. Loss: 0.0116  lr:0.000010
[ Sun Jul  7 06:13:17 2024 ] 	Batch(7300/7879) done. Loss: 0.0093  lr:0.000010
[ Sun Jul  7 06:13:35 2024 ] 	Batch(7400/7879) done. Loss: 0.0247  lr:0.000010
[ Sun Jul  7 06:13:53 2024 ] 
Training: Epoch [54/120], Step [7499], Loss: 0.01206792239099741, Training Accuracy: 98.29666666666667
[ Sun Jul  7 06:13:53 2024 ] 	Batch(7500/7879) done. Loss: 0.0072  lr:0.000010
[ Sun Jul  7 06:14:11 2024 ] 	Batch(7600/7879) done. Loss: 0.0501  lr:0.000010
[ Sun Jul  7 06:14:29 2024 ] 	Batch(7700/7879) done. Loss: 0.0973  lr:0.000010
[ Sun Jul  7 06:14:47 2024 ] 	Batch(7800/7879) done. Loss: 0.0132  lr:0.000010
[ Sun Jul  7 06:15:01 2024 ] 	Mean training loss: 0.0741.
[ Sun Jul  7 06:15:01 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 06:15:01 2024 ] Training epoch: 56
[ Sun Jul  7 06:15:02 2024 ] 	Batch(0/7879) done. Loss: 0.0389  lr:0.000010
[ Sun Jul  7 06:15:20 2024 ] 	Batch(100/7879) done. Loss: 0.0143  lr:0.000010
[ Sun Jul  7 06:15:38 2024 ] 	Batch(200/7879) done. Loss: 0.0113  lr:0.000010
[ Sun Jul  7 06:15:57 2024 ] 	Batch(300/7879) done. Loss: 0.0111  lr:0.000010
[ Sun Jul  7 06:16:15 2024 ] 	Batch(400/7879) done. Loss: 0.0039  lr:0.000010
[ Sun Jul  7 06:16:33 2024 ] 
Training: Epoch [55/120], Step [499], Loss: 0.6367106437683105, Training Accuracy: 97.975
[ Sun Jul  7 06:16:33 2024 ] 	Batch(500/7879) done. Loss: 0.0247  lr:0.000010
[ Sun Jul  7 06:16:52 2024 ] 	Batch(600/7879) done. Loss: 0.0052  lr:0.000010
[ Sun Jul  7 06:17:10 2024 ] 	Batch(700/7879) done. Loss: 0.0321  lr:0.000010
[ Sun Jul  7 06:17:28 2024 ] 	Batch(800/7879) done. Loss: 0.3427  lr:0.000010
[ Sun Jul  7 06:17:46 2024 ] 	Batch(900/7879) done. Loss: 0.0103  lr:0.000010
[ Sun Jul  7 06:18:04 2024 ] 
Training: Epoch [55/120], Step [999], Loss: 0.08627196401357651, Training Accuracy: 98.1375
[ Sun Jul  7 06:18:04 2024 ] 	Batch(1000/7879) done. Loss: 0.0110  lr:0.000010
[ Sun Jul  7 06:18:22 2024 ] 	Batch(1100/7879) done. Loss: 0.0127  lr:0.000010
[ Sun Jul  7 06:18:40 2024 ] 	Batch(1200/7879) done. Loss: 0.0315  lr:0.000010
[ Sun Jul  7 06:18:58 2024 ] 	Batch(1300/7879) done. Loss: 0.0413  lr:0.000010
[ Sun Jul  7 06:19:16 2024 ] 	Batch(1400/7879) done. Loss: 0.0141  lr:0.000010
[ Sun Jul  7 06:19:34 2024 ] 
Training: Epoch [55/120], Step [1499], Loss: 0.006068429443985224, Training Accuracy: 98.30833333333334
[ Sun Jul  7 06:19:34 2024 ] 	Batch(1500/7879) done. Loss: 0.0616  lr:0.000010
[ Sun Jul  7 06:19:52 2024 ] 	Batch(1600/7879) done. Loss: 0.0829  lr:0.000010
[ Sun Jul  7 06:20:10 2024 ] 	Batch(1700/7879) done. Loss: 0.0139  lr:0.000010
[ Sun Jul  7 06:20:28 2024 ] 	Batch(1800/7879) done. Loss: 0.1546  lr:0.000010
[ Sun Jul  7 06:20:46 2024 ] 	Batch(1900/7879) done. Loss: 0.0121  lr:0.000010
[ Sun Jul  7 06:21:03 2024 ] 
Training: Epoch [55/120], Step [1999], Loss: 0.1045631393790245, Training Accuracy: 98.29375
[ Sun Jul  7 06:21:04 2024 ] 	Batch(2000/7879) done. Loss: 0.1862  lr:0.000010
[ Sun Jul  7 06:21:22 2024 ] 	Batch(2100/7879) done. Loss: 0.0689  lr:0.000010
[ Sun Jul  7 06:21:40 2024 ] 	Batch(2200/7879) done. Loss: 0.0800  lr:0.000010
[ Sun Jul  7 06:21:58 2024 ] 	Batch(2300/7879) done. Loss: 0.0984  lr:0.000010
[ Sun Jul  7 06:22:16 2024 ] 	Batch(2400/7879) done. Loss: 0.0138  lr:0.000010
[ Sun Jul  7 06:22:34 2024 ] 
Training: Epoch [55/120], Step [2499], Loss: 0.08528658002614975, Training Accuracy: 98.185
[ Sun Jul  7 06:22:34 2024 ] 	Batch(2500/7879) done. Loss: 0.1759  lr:0.000010
[ Sun Jul  7 06:22:52 2024 ] 	Batch(2600/7879) done. Loss: 0.0187  lr:0.000010
[ Sun Jul  7 06:23:10 2024 ] 	Batch(2700/7879) done. Loss: 0.0732  lr:0.000010
[ Sun Jul  7 06:23:28 2024 ] 	Batch(2800/7879) done. Loss: 0.0367  lr:0.000010
[ Sun Jul  7 06:23:46 2024 ] 	Batch(2900/7879) done. Loss: 0.0042  lr:0.000010
[ Sun Jul  7 06:24:04 2024 ] 
Training: Epoch [55/120], Step [2999], Loss: 0.06237784028053284, Training Accuracy: 98.24583333333334
[ Sun Jul  7 06:24:05 2024 ] 	Batch(3000/7879) done. Loss: 0.0182  lr:0.000010
[ Sun Jul  7 06:24:23 2024 ] 	Batch(3100/7879) done. Loss: 0.0048  lr:0.000010
[ Sun Jul  7 06:24:41 2024 ] 	Batch(3200/7879) done. Loss: 0.1036  lr:0.000010
[ Sun Jul  7 06:24:59 2024 ] 	Batch(3300/7879) done. Loss: 0.1450  lr:0.000010
[ Sun Jul  7 06:25:18 2024 ] 	Batch(3400/7879) done. Loss: 0.1361  lr:0.000010
[ Sun Jul  7 06:25:36 2024 ] 
Training: Epoch [55/120], Step [3499], Loss: 0.17603659629821777, Training Accuracy: 98.24642857142857
[ Sun Jul  7 06:25:36 2024 ] 	Batch(3500/7879) done. Loss: 0.0056  lr:0.000010
[ Sun Jul  7 06:25:55 2024 ] 	Batch(3600/7879) done. Loss: 0.0072  lr:0.000010
[ Sun Jul  7 06:26:12 2024 ] 	Batch(3700/7879) done. Loss: 0.0759  lr:0.000010
[ Sun Jul  7 06:26:30 2024 ] 	Batch(3800/7879) done. Loss: 0.0239  lr:0.000010
[ Sun Jul  7 06:26:48 2024 ] 	Batch(3900/7879) done. Loss: 0.0159  lr:0.000010
[ Sun Jul  7 06:27:06 2024 ] 
Training: Epoch [55/120], Step [3999], Loss: 0.048434264957904816, Training Accuracy: 98.278125
[ Sun Jul  7 06:27:06 2024 ] 	Batch(4000/7879) done. Loss: 0.0876  lr:0.000010
[ Sun Jul  7 06:27:25 2024 ] 	Batch(4100/7879) done. Loss: 0.0820  lr:0.000010
[ Sun Jul  7 06:27:43 2024 ] 	Batch(4200/7879) done. Loss: 0.0194  lr:0.000010
[ Sun Jul  7 06:28:02 2024 ] 	Batch(4300/7879) done. Loss: 0.0132  lr:0.000010
[ Sun Jul  7 06:28:21 2024 ] 	Batch(4400/7879) done. Loss: 0.1057  lr:0.000010
[ Sun Jul  7 06:28:39 2024 ] 
Training: Epoch [55/120], Step [4499], Loss: 0.2842145562171936, Training Accuracy: 98.28611111111111
[ Sun Jul  7 06:28:39 2024 ] 	Batch(4500/7879) done. Loss: 0.0394  lr:0.000010
[ Sun Jul  7 06:28:58 2024 ] 	Batch(4600/7879) done. Loss: 0.0092  lr:0.000010
[ Sun Jul  7 06:29:16 2024 ] 	Batch(4700/7879) done. Loss: 0.0326  lr:0.000010
[ Sun Jul  7 06:29:35 2024 ] 	Batch(4800/7879) done. Loss: 0.2910  lr:0.000010
[ Sun Jul  7 06:29:54 2024 ] 	Batch(4900/7879) done. Loss: 0.0196  lr:0.000010
[ Sun Jul  7 06:30:12 2024 ] 
Training: Epoch [55/120], Step [4999], Loss: 0.06738287210464478, Training Accuracy: 98.3125
[ Sun Jul  7 06:30:12 2024 ] 	Batch(5000/7879) done. Loss: 0.0790  lr:0.000010
[ Sun Jul  7 06:30:31 2024 ] 	Batch(5100/7879) done. Loss: 0.2803  lr:0.000010
[ Sun Jul  7 06:30:49 2024 ] 	Batch(5200/7879) done. Loss: 0.0084  lr:0.000010
[ Sun Jul  7 06:31:07 2024 ] 	Batch(5300/7879) done. Loss: 0.1565  lr:0.000010
[ Sun Jul  7 06:31:25 2024 ] 	Batch(5400/7879) done. Loss: 0.0102  lr:0.000010
[ Sun Jul  7 06:31:43 2024 ] 
Training: Epoch [55/120], Step [5499], Loss: 0.028551805764436722, Training Accuracy: 98.30681818181817
[ Sun Jul  7 06:31:43 2024 ] 	Batch(5500/7879) done. Loss: 0.0035  lr:0.000010
[ Sun Jul  7 06:32:01 2024 ] 	Batch(5600/7879) done. Loss: 0.0334  lr:0.000010
[ Sun Jul  7 06:32:19 2024 ] 	Batch(5700/7879) done. Loss: 0.0184  lr:0.000010
[ Sun Jul  7 06:32:37 2024 ] 	Batch(5800/7879) done. Loss: 0.0192  lr:0.000010
[ Sun Jul  7 06:32:55 2024 ] 	Batch(5900/7879) done. Loss: 0.0037  lr:0.000010
[ Sun Jul  7 06:33:13 2024 ] 
Training: Epoch [55/120], Step [5999], Loss: 0.1851148158311844, Training Accuracy: 98.28333333333333
[ Sun Jul  7 06:33:13 2024 ] 	Batch(6000/7879) done. Loss: 0.0026  lr:0.000010
[ Sun Jul  7 06:33:31 2024 ] 	Batch(6100/7879) done. Loss: 0.0053  lr:0.000010
[ Sun Jul  7 06:33:49 2024 ] 	Batch(6200/7879) done. Loss: 0.0134  lr:0.000010
[ Sun Jul  7 06:34:07 2024 ] 	Batch(6300/7879) done. Loss: 0.0115  lr:0.000010
[ Sun Jul  7 06:34:25 2024 ] 	Batch(6400/7879) done. Loss: 0.0037  lr:0.000010
[ Sun Jul  7 06:34:43 2024 ] 
Training: Epoch [55/120], Step [6499], Loss: 0.35915088653564453, Training Accuracy: 98.29423076923078
[ Sun Jul  7 06:34:43 2024 ] 	Batch(6500/7879) done. Loss: 0.0066  lr:0.000010
[ Sun Jul  7 06:35:01 2024 ] 	Batch(6600/7879) done. Loss: 0.0974  lr:0.000010
[ Sun Jul  7 06:35:19 2024 ] 	Batch(6700/7879) done. Loss: 0.0678  lr:0.000010
[ Sun Jul  7 06:35:37 2024 ] 	Batch(6800/7879) done. Loss: 0.0347  lr:0.000010
[ Sun Jul  7 06:35:55 2024 ] 	Batch(6900/7879) done. Loss: 0.2350  lr:0.000010
[ Sun Jul  7 06:36:13 2024 ] 
Training: Epoch [55/120], Step [6999], Loss: 0.06036468967795372, Training Accuracy: 98.29285714285714
[ Sun Jul  7 06:36:13 2024 ] 	Batch(7000/7879) done. Loss: 0.0146  lr:0.000010
[ Sun Jul  7 06:36:31 2024 ] 	Batch(7100/7879) done. Loss: 0.0648  lr:0.000010
[ Sun Jul  7 06:36:49 2024 ] 	Batch(7200/7879) done. Loss: 0.0163  lr:0.000010
[ Sun Jul  7 06:37:07 2024 ] 	Batch(7300/7879) done. Loss: 0.0455  lr:0.000010
[ Sun Jul  7 06:37:25 2024 ] 	Batch(7400/7879) done. Loss: 0.0183  lr:0.000010
[ Sun Jul  7 06:37:43 2024 ] 
Training: Epoch [55/120], Step [7499], Loss: 0.016610872000455856, Training Accuracy: 98.28833333333333
[ Sun Jul  7 06:37:43 2024 ] 	Batch(7500/7879) done. Loss: 0.4345  lr:0.000010
[ Sun Jul  7 06:38:01 2024 ] 	Batch(7600/7879) done. Loss: 0.0259  lr:0.000010
[ Sun Jul  7 06:38:20 2024 ] 	Batch(7700/7879) done. Loss: 0.0837  lr:0.000010
[ Sun Jul  7 06:38:38 2024 ] 	Batch(7800/7879) done. Loss: 0.0032  lr:0.000010
[ Sun Jul  7 06:38:53 2024 ] 	Mean training loss: 0.0760.
[ Sun Jul  7 06:38:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 06:38:53 2024 ] Training epoch: 57
[ Sun Jul  7 06:38:54 2024 ] 	Batch(0/7879) done. Loss: 0.0474  lr:0.000010
[ Sun Jul  7 06:39:11 2024 ] 	Batch(100/7879) done. Loss: 0.0447  lr:0.000010
[ Sun Jul  7 06:39:29 2024 ] 	Batch(200/7879) done. Loss: 0.0158  lr:0.000010
[ Sun Jul  7 06:39:47 2024 ] 	Batch(300/7879) done. Loss: 0.0491  lr:0.000010
[ Sun Jul  7 06:40:05 2024 ] 	Batch(400/7879) done. Loss: 0.0484  lr:0.000010
[ Sun Jul  7 06:40:23 2024 ] 
Training: Epoch [56/120], Step [499], Loss: 0.09083006531000137, Training Accuracy: 98.225
[ Sun Jul  7 06:40:23 2024 ] 	Batch(500/7879) done. Loss: 0.1579  lr:0.000010
[ Sun Jul  7 06:40:41 2024 ] 	Batch(600/7879) done. Loss: 0.0765  lr:0.000010
[ Sun Jul  7 06:40:59 2024 ] 	Batch(700/7879) done. Loss: 0.0448  lr:0.000010
[ Sun Jul  7 06:41:17 2024 ] 	Batch(800/7879) done. Loss: 0.0140  lr:0.000010
[ Sun Jul  7 06:41:36 2024 ] 	Batch(900/7879) done. Loss: 0.0172  lr:0.000010
[ Sun Jul  7 06:41:53 2024 ] 
Training: Epoch [56/120], Step [999], Loss: 0.007879633456468582, Training Accuracy: 98.28750000000001
[ Sun Jul  7 06:41:54 2024 ] 	Batch(1000/7879) done. Loss: 0.4485  lr:0.000010
[ Sun Jul  7 06:42:12 2024 ] 	Batch(1100/7879) done. Loss: 0.5215  lr:0.000010
[ Sun Jul  7 06:42:30 2024 ] 	Batch(1200/7879) done. Loss: 0.1234  lr:0.000010
[ Sun Jul  7 06:42:48 2024 ] 	Batch(1300/7879) done. Loss: 0.0767  lr:0.000010
[ Sun Jul  7 06:43:07 2024 ] 	Batch(1400/7879) done. Loss: 0.0923  lr:0.000010
[ Sun Jul  7 06:43:25 2024 ] 
Training: Epoch [56/120], Step [1499], Loss: 0.07269121706485748, Training Accuracy: 98.34166666666667
[ Sun Jul  7 06:43:25 2024 ] 	Batch(1500/7879) done. Loss: 0.0215  lr:0.000010
[ Sun Jul  7 06:43:44 2024 ] 	Batch(1600/7879) done. Loss: 0.0196  lr:0.000010
[ Sun Jul  7 06:44:03 2024 ] 	Batch(1700/7879) done. Loss: 0.1602  lr:0.000010
[ Sun Jul  7 06:44:21 2024 ] 	Batch(1800/7879) done. Loss: 0.0298  lr:0.000010
[ Sun Jul  7 06:44:40 2024 ] 	Batch(1900/7879) done. Loss: 0.0661  lr:0.000010
[ Sun Jul  7 06:44:57 2024 ] 
Training: Epoch [56/120], Step [1999], Loss: 0.09123367816209793, Training Accuracy: 98.31875
[ Sun Jul  7 06:44:58 2024 ] 	Batch(2000/7879) done. Loss: 0.0274  lr:0.000010
[ Sun Jul  7 06:45:16 2024 ] 	Batch(2100/7879) done. Loss: 0.0337  lr:0.000010
[ Sun Jul  7 06:45:34 2024 ] 	Batch(2200/7879) done. Loss: 0.0419  lr:0.000010
[ Sun Jul  7 06:45:51 2024 ] 	Batch(2300/7879) done. Loss: 0.0503  lr:0.000010
[ Sun Jul  7 06:46:09 2024 ] 	Batch(2400/7879) done. Loss: 0.0498  lr:0.000010
[ Sun Jul  7 06:46:27 2024 ] 
Training: Epoch [56/120], Step [2499], Loss: 0.03730098158121109, Training Accuracy: 98.335
[ Sun Jul  7 06:46:27 2024 ] 	Batch(2500/7879) done. Loss: 0.0254  lr:0.000010
[ Sun Jul  7 06:46:45 2024 ] 	Batch(2600/7879) done. Loss: 0.0264  lr:0.000010
[ Sun Jul  7 06:47:03 2024 ] 	Batch(2700/7879) done. Loss: 0.1202  lr:0.000010
[ Sun Jul  7 06:47:22 2024 ] 	Batch(2800/7879) done. Loss: 0.0180  lr:0.000010
[ Sun Jul  7 06:47:39 2024 ] 	Batch(2900/7879) done. Loss: 0.0015  lr:0.000010
[ Sun Jul  7 06:47:57 2024 ] 
Training: Epoch [56/120], Step [2999], Loss: 0.01330227218568325, Training Accuracy: 98.30833333333334
[ Sun Jul  7 06:47:57 2024 ] 	Batch(3000/7879) done. Loss: 0.1460  lr:0.000010
[ Sun Jul  7 06:48:15 2024 ] 	Batch(3100/7879) done. Loss: 0.0475  lr:0.000010
[ Sun Jul  7 06:48:33 2024 ] 	Batch(3200/7879) done. Loss: 0.2529  lr:0.000010
[ Sun Jul  7 06:48:51 2024 ] 	Batch(3300/7879) done. Loss: 0.0213  lr:0.000010
[ Sun Jul  7 06:49:09 2024 ] 	Batch(3400/7879) done. Loss: 0.0090  lr:0.000010
[ Sun Jul  7 06:49:27 2024 ] 
Training: Epoch [56/120], Step [3499], Loss: 0.0028249924071133137, Training Accuracy: 98.29285714285714
[ Sun Jul  7 06:49:27 2024 ] 	Batch(3500/7879) done. Loss: 0.0809  lr:0.000010
[ Sun Jul  7 06:49:45 2024 ] 	Batch(3600/7879) done. Loss: 0.2259  lr:0.000010
[ Sun Jul  7 06:50:03 2024 ] 	Batch(3700/7879) done. Loss: 0.0341  lr:0.000010
[ Sun Jul  7 06:50:21 2024 ] 	Batch(3800/7879) done. Loss: 0.2816  lr:0.000010
[ Sun Jul  7 06:50:39 2024 ] 	Batch(3900/7879) done. Loss: 0.0150  lr:0.000010
[ Sun Jul  7 06:50:57 2024 ] 
Training: Epoch [56/120], Step [3999], Loss: 0.009121636860072613, Training Accuracy: 98.31875
[ Sun Jul  7 06:50:57 2024 ] 	Batch(4000/7879) done. Loss: 0.0169  lr:0.000010
[ Sun Jul  7 06:51:15 2024 ] 	Batch(4100/7879) done. Loss: 0.0138  lr:0.000010
[ Sun Jul  7 06:51:33 2024 ] 	Batch(4200/7879) done. Loss: 0.0344  lr:0.000010
[ Sun Jul  7 06:51:51 2024 ] 	Batch(4300/7879) done. Loss: 0.0619  lr:0.000010
[ Sun Jul  7 06:52:09 2024 ] 	Batch(4400/7879) done. Loss: 0.2832  lr:0.000010
[ Sun Jul  7 06:52:26 2024 ] 
Training: Epoch [56/120], Step [4499], Loss: 0.007803753018379211, Training Accuracy: 98.35277777777777
[ Sun Jul  7 06:52:27 2024 ] 	Batch(4500/7879) done. Loss: 0.6042  lr:0.000010
[ Sun Jul  7 06:52:45 2024 ] 	Batch(4600/7879) done. Loss: 0.1411  lr:0.000010
[ Sun Jul  7 06:53:02 2024 ] 	Batch(4700/7879) done. Loss: 0.1037  lr:0.000010
[ Sun Jul  7 06:53:21 2024 ] 	Batch(4800/7879) done. Loss: 0.0303  lr:0.000010
[ Sun Jul  7 06:53:39 2024 ] 	Batch(4900/7879) done. Loss: 0.0984  lr:0.000010
[ Sun Jul  7 06:53:58 2024 ] 
Training: Epoch [56/120], Step [4999], Loss: 0.06724944710731506, Training Accuracy: 98.3625
[ Sun Jul  7 06:53:58 2024 ] 	Batch(5000/7879) done. Loss: 0.1148  lr:0.000010
[ Sun Jul  7 06:54:16 2024 ] 	Batch(5100/7879) done. Loss: 0.0111  lr:0.000010
[ Sun Jul  7 06:54:35 2024 ] 	Batch(5200/7879) done. Loss: 0.0149  lr:0.000010
[ Sun Jul  7 06:54:53 2024 ] 	Batch(5300/7879) done. Loss: 0.0466  lr:0.000010
[ Sun Jul  7 06:55:11 2024 ] 	Batch(5400/7879) done. Loss: 0.0083  lr:0.000010
[ Sun Jul  7 06:55:29 2024 ] 
Training: Epoch [56/120], Step [5499], Loss: 0.1877766251564026, Training Accuracy: 98.375
[ Sun Jul  7 06:55:29 2024 ] 	Batch(5500/7879) done. Loss: 0.0602  lr:0.000010
[ Sun Jul  7 06:55:47 2024 ] 	Batch(5600/7879) done. Loss: 0.0534  lr:0.000010
[ Sun Jul  7 06:56:05 2024 ] 	Batch(5700/7879) done. Loss: 0.0937  lr:0.000010
[ Sun Jul  7 06:56:24 2024 ] 	Batch(5800/7879) done. Loss: 0.0447  lr:0.000010
[ Sun Jul  7 06:56:42 2024 ] 	Batch(5900/7879) done. Loss: 0.0435  lr:0.000010
[ Sun Jul  7 06:57:00 2024 ] 
Training: Epoch [56/120], Step [5999], Loss: 0.08039861172437668, Training Accuracy: 98.38541666666667
[ Sun Jul  7 06:57:00 2024 ] 	Batch(6000/7879) done. Loss: 0.0102  lr:0.000010
[ Sun Jul  7 06:57:19 2024 ] 	Batch(6100/7879) done. Loss: 0.0632  lr:0.000010
[ Sun Jul  7 06:57:37 2024 ] 	Batch(6200/7879) done. Loss: 0.0604  lr:0.000010
[ Sun Jul  7 06:57:56 2024 ] 	Batch(6300/7879) done. Loss: 0.0122  lr:0.000010
[ Sun Jul  7 06:58:15 2024 ] 	Batch(6400/7879) done. Loss: 0.0074  lr:0.000010
[ Sun Jul  7 06:58:33 2024 ] 
Training: Epoch [56/120], Step [6499], Loss: 0.040064699947834015, Training Accuracy: 98.39423076923077
[ Sun Jul  7 06:58:33 2024 ] 	Batch(6500/7879) done. Loss: 0.0030  lr:0.000010
[ Sun Jul  7 06:58:51 2024 ] 	Batch(6600/7879) done. Loss: 0.0328  lr:0.000010
[ Sun Jul  7 06:59:09 2024 ] 	Batch(6700/7879) done. Loss: 0.0058  lr:0.000010
[ Sun Jul  7 06:59:27 2024 ] 	Batch(6800/7879) done. Loss: 0.0999  lr:0.000010
[ Sun Jul  7 06:59:45 2024 ] 	Batch(6900/7879) done. Loss: 0.0320  lr:0.000010
[ Sun Jul  7 07:00:03 2024 ] 
Training: Epoch [56/120], Step [6999], Loss: 0.017484527081251144, Training Accuracy: 98.3892857142857
[ Sun Jul  7 07:00:03 2024 ] 	Batch(7000/7879) done. Loss: 0.0101  lr:0.000010
[ Sun Jul  7 07:00:21 2024 ] 	Batch(7100/7879) done. Loss: 0.0108  lr:0.000010
[ Sun Jul  7 07:00:39 2024 ] 	Batch(7200/7879) done. Loss: 0.0731  lr:0.000010
[ Sun Jul  7 07:00:57 2024 ] 	Batch(7300/7879) done. Loss: 0.0482  lr:0.000010
[ Sun Jul  7 07:01:15 2024 ] 	Batch(7400/7879) done. Loss: 0.0455  lr:0.000010
[ Sun Jul  7 07:01:33 2024 ] 
Training: Epoch [56/120], Step [7499], Loss: 0.021036367863416672, Training Accuracy: 98.37333333333333
[ Sun Jul  7 07:01:33 2024 ] 	Batch(7500/7879) done. Loss: 0.0413  lr:0.000010
[ Sun Jul  7 07:01:51 2024 ] 	Batch(7600/7879) done. Loss: 0.0190  lr:0.000010
[ Sun Jul  7 07:02:09 2024 ] 	Batch(7700/7879) done. Loss: 0.0321  lr:0.000010
[ Sun Jul  7 07:02:27 2024 ] 	Batch(7800/7879) done. Loss: 0.1225  lr:0.000010
[ Sun Jul  7 07:02:41 2024 ] 	Mean training loss: 0.0727.
[ Sun Jul  7 07:02:41 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 07:02:41 2024 ] Training epoch: 58
[ Sun Jul  7 07:02:42 2024 ] 	Batch(0/7879) done. Loss: 0.1822  lr:0.000010
[ Sun Jul  7 07:03:00 2024 ] 	Batch(100/7879) done. Loss: 0.0564  lr:0.000010
[ Sun Jul  7 07:03:18 2024 ] 	Batch(200/7879) done. Loss: 0.1196  lr:0.000010
[ Sun Jul  7 07:03:37 2024 ] 	Batch(300/7879) done. Loss: 0.0037  lr:0.000010
[ Sun Jul  7 07:03:55 2024 ] 	Batch(400/7879) done. Loss: 0.0035  lr:0.000010
[ Sun Jul  7 07:04:13 2024 ] 
Training: Epoch [57/120], Step [499], Loss: 0.1455674171447754, Training Accuracy: 98.55000000000001
[ Sun Jul  7 07:04:14 2024 ] 	Batch(500/7879) done. Loss: 0.0226  lr:0.000010
[ Sun Jul  7 07:04:32 2024 ] 	Batch(600/7879) done. Loss: 0.0104  lr:0.000010
[ Sun Jul  7 07:04:50 2024 ] 	Batch(700/7879) done. Loss: 0.2293  lr:0.000010
[ Sun Jul  7 07:05:09 2024 ] 	Batch(800/7879) done. Loss: 0.0021  lr:0.000010
[ Sun Jul  7 07:05:26 2024 ] 	Batch(900/7879) done. Loss: 0.1106  lr:0.000010
[ Sun Jul  7 07:05:44 2024 ] 
Training: Epoch [57/120], Step [999], Loss: 0.014070290140807629, Training Accuracy: 98.58749999999999
[ Sun Jul  7 07:05:44 2024 ] 	Batch(1000/7879) done. Loss: 0.0074  lr:0.000010
[ Sun Jul  7 07:06:02 2024 ] 	Batch(1100/7879) done. Loss: 0.0133  lr:0.000010
[ Sun Jul  7 07:06:20 2024 ] 	Batch(1200/7879) done. Loss: 0.0051  lr:0.000010
[ Sun Jul  7 07:06:38 2024 ] 	Batch(1300/7879) done. Loss: 0.0080  lr:0.000010
[ Sun Jul  7 07:06:56 2024 ] 	Batch(1400/7879) done. Loss: 0.0417  lr:0.000010
[ Sun Jul  7 07:07:14 2024 ] 
Training: Epoch [57/120], Step [1499], Loss: 0.31867989897727966, Training Accuracy: 98.59166666666667
[ Sun Jul  7 07:07:14 2024 ] 	Batch(1500/7879) done. Loss: 0.0556  lr:0.000010
[ Sun Jul  7 07:07:32 2024 ] 	Batch(1600/7879) done. Loss: 0.0934  lr:0.000010
[ Sun Jul  7 07:07:50 2024 ] 	Batch(1700/7879) done. Loss: 0.0163  lr:0.000010
[ Sun Jul  7 07:08:08 2024 ] 	Batch(1800/7879) done. Loss: 0.1600  lr:0.000010
[ Sun Jul  7 07:08:26 2024 ] 	Batch(1900/7879) done. Loss: 0.0593  lr:0.000010
[ Sun Jul  7 07:08:44 2024 ] 
Training: Epoch [57/120], Step [1999], Loss: 0.01755552552640438, Training Accuracy: 98.49375
[ Sun Jul  7 07:08:44 2024 ] 	Batch(2000/7879) done. Loss: 0.0193  lr:0.000010
[ Sun Jul  7 07:09:02 2024 ] 	Batch(2100/7879) done. Loss: 0.0687  lr:0.000010
[ Sun Jul  7 07:09:20 2024 ] 	Batch(2200/7879) done. Loss: 0.3517  lr:0.000010
[ Sun Jul  7 07:09:38 2024 ] 	Batch(2300/7879) done. Loss: 0.0565  lr:0.000010
[ Sun Jul  7 07:09:56 2024 ] 	Batch(2400/7879) done. Loss: 0.0465  lr:0.000010
[ Sun Jul  7 07:10:13 2024 ] 
Training: Epoch [57/120], Step [2499], Loss: 0.0032722605392336845, Training Accuracy: 98.52
[ Sun Jul  7 07:10:14 2024 ] 	Batch(2500/7879) done. Loss: 0.0389  lr:0.000010
[ Sun Jul  7 07:10:32 2024 ] 	Batch(2600/7879) done. Loss: 0.1760  lr:0.000010
[ Sun Jul  7 07:10:49 2024 ] 	Batch(2700/7879) done. Loss: 0.0461  lr:0.000010
[ Sun Jul  7 07:11:08 2024 ] 	Batch(2800/7879) done. Loss: 0.0756  lr:0.000010
[ Sun Jul  7 07:11:25 2024 ] 	Batch(2900/7879) done. Loss: 0.1414  lr:0.000010
[ Sun Jul  7 07:11:43 2024 ] 
Training: Epoch [57/120], Step [2999], Loss: 0.03064270317554474, Training Accuracy: 98.52499999999999
[ Sun Jul  7 07:11:43 2024 ] 	Batch(3000/7879) done. Loss: 0.1411  lr:0.000010
[ Sun Jul  7 07:12:01 2024 ] 	Batch(3100/7879) done. Loss: 0.0084  lr:0.000010
[ Sun Jul  7 07:12:19 2024 ] 	Batch(3200/7879) done. Loss: 0.0372  lr:0.000010
[ Sun Jul  7 07:12:37 2024 ] 	Batch(3300/7879) done. Loss: 0.0660  lr:0.000010
[ Sun Jul  7 07:12:55 2024 ] 	Batch(3400/7879) done. Loss: 0.1839  lr:0.000010
[ Sun Jul  7 07:13:14 2024 ] 
Training: Epoch [57/120], Step [3499], Loss: 0.02014073356986046, Training Accuracy: 98.45357142857142
[ Sun Jul  7 07:13:14 2024 ] 	Batch(3500/7879) done. Loss: 0.0073  lr:0.000010
[ Sun Jul  7 07:13:33 2024 ] 	Batch(3600/7879) done. Loss: 0.0088  lr:0.000010
[ Sun Jul  7 07:13:51 2024 ] 	Batch(3700/7879) done. Loss: 0.0403  lr:0.000010
[ Sun Jul  7 07:14:08 2024 ] 	Batch(3800/7879) done. Loss: 0.4352  lr:0.000010
[ Sun Jul  7 07:14:26 2024 ] 	Batch(3900/7879) done. Loss: 0.0128  lr:0.000010
[ Sun Jul  7 07:14:44 2024 ] 
Training: Epoch [57/120], Step [3999], Loss: 0.042348287999629974, Training Accuracy: 98.421875
[ Sun Jul  7 07:14:45 2024 ] 	Batch(4000/7879) done. Loss: 0.0604  lr:0.000010
[ Sun Jul  7 07:15:03 2024 ] 	Batch(4100/7879) done. Loss: 0.0232  lr:0.000010
[ Sun Jul  7 07:15:21 2024 ] 	Batch(4200/7879) done. Loss: 0.2015  lr:0.000010
[ Sun Jul  7 07:15:39 2024 ] 	Batch(4300/7879) done. Loss: 0.0784  lr:0.000010
[ Sun Jul  7 07:15:57 2024 ] 	Batch(4400/7879) done. Loss: 0.0149  lr:0.000010
[ Sun Jul  7 07:16:15 2024 ] 
Training: Epoch [57/120], Step [4499], Loss: 0.03697444126009941, Training Accuracy: 98.40833333333333
[ Sun Jul  7 07:16:15 2024 ] 	Batch(4500/7879) done. Loss: 0.0122  lr:0.000010
[ Sun Jul  7 07:16:33 2024 ] 	Batch(4600/7879) done. Loss: 0.0019  lr:0.000010
[ Sun Jul  7 07:16:51 2024 ] 	Batch(4700/7879) done. Loss: 0.0098  lr:0.000010
[ Sun Jul  7 07:17:09 2024 ] 	Batch(4800/7879) done. Loss: 0.0729  lr:0.000010
[ Sun Jul  7 07:17:27 2024 ] 	Batch(4900/7879) done. Loss: 0.0024  lr:0.000010
[ Sun Jul  7 07:17:46 2024 ] 
Training: Epoch [57/120], Step [4999], Loss: 0.020131755620241165, Training Accuracy: 98.46249999999999
[ Sun Jul  7 07:17:46 2024 ] 	Batch(5000/7879) done. Loss: 0.0365  lr:0.000010
[ Sun Jul  7 07:18:05 2024 ] 	Batch(5100/7879) done. Loss: 0.0155  lr:0.000010
[ Sun Jul  7 07:18:23 2024 ] 	Batch(5200/7879) done. Loss: 0.2870  lr:0.000010
[ Sun Jul  7 07:18:41 2024 ] 	Batch(5300/7879) done. Loss: 0.0400  lr:0.000010
[ Sun Jul  7 07:18:59 2024 ] 	Batch(5400/7879) done. Loss: 0.0062  lr:0.000010
[ Sun Jul  7 07:19:17 2024 ] 
Training: Epoch [57/120], Step [5499], Loss: 0.03982526808977127, Training Accuracy: 98.43409090909091
[ Sun Jul  7 07:19:17 2024 ] 	Batch(5500/7879) done. Loss: 0.0600  lr:0.000010
[ Sun Jul  7 07:19:35 2024 ] 	Batch(5600/7879) done. Loss: 0.0172  lr:0.000010
[ Sun Jul  7 07:19:53 2024 ] 	Batch(5700/7879) done. Loss: 0.0722  lr:0.000010
[ Sun Jul  7 07:20:11 2024 ] 	Batch(5800/7879) done. Loss: 0.0021  lr:0.000010
[ Sun Jul  7 07:20:29 2024 ] 	Batch(5900/7879) done. Loss: 0.0436  lr:0.000010
[ Sun Jul  7 07:20:47 2024 ] 
Training: Epoch [57/120], Step [5999], Loss: 0.11576050519943237, Training Accuracy: 98.4375
[ Sun Jul  7 07:20:47 2024 ] 	Batch(6000/7879) done. Loss: 0.1348  lr:0.000010
[ Sun Jul  7 07:21:05 2024 ] 	Batch(6100/7879) done. Loss: 0.0406  lr:0.000010
[ Sun Jul  7 07:21:23 2024 ] 	Batch(6200/7879) done. Loss: 0.0673  lr:0.000010
[ Sun Jul  7 07:21:41 2024 ] 	Batch(6300/7879) done. Loss: 0.0079  lr:0.000010
[ Sun Jul  7 07:21:59 2024 ] 	Batch(6400/7879) done. Loss: 0.0173  lr:0.000010
[ Sun Jul  7 07:22:17 2024 ] 
Training: Epoch [57/120], Step [6499], Loss: 0.07333026081323624, Training Accuracy: 98.44807692307693
[ Sun Jul  7 07:22:17 2024 ] 	Batch(6500/7879) done. Loss: 0.0175  lr:0.000010
[ Sun Jul  7 07:22:35 2024 ] 	Batch(6600/7879) done. Loss: 0.0154  lr:0.000010
[ Sun Jul  7 07:22:53 2024 ] 	Batch(6700/7879) done. Loss: 0.1172  lr:0.000010
[ Sun Jul  7 07:23:11 2024 ] 	Batch(6800/7879) done. Loss: 0.1579  lr:0.000010
[ Sun Jul  7 07:23:29 2024 ] 	Batch(6900/7879) done. Loss: 0.2018  lr:0.000010
[ Sun Jul  7 07:23:47 2024 ] 
Training: Epoch [57/120], Step [6999], Loss: 0.11043088138103485, Training Accuracy: 98.45892857142857
[ Sun Jul  7 07:23:47 2024 ] 	Batch(7000/7879) done. Loss: 0.0071  lr:0.000010
[ Sun Jul  7 07:24:05 2024 ] 	Batch(7100/7879) done. Loss: 0.0394  lr:0.000010
[ Sun Jul  7 07:24:23 2024 ] 	Batch(7200/7879) done. Loss: 0.0060  lr:0.000010
[ Sun Jul  7 07:24:41 2024 ] 	Batch(7300/7879) done. Loss: 0.0137  lr:0.000010
[ Sun Jul  7 07:24:59 2024 ] 	Batch(7400/7879) done. Loss: 0.1037  lr:0.000010
[ Sun Jul  7 07:25:16 2024 ] 
Training: Epoch [57/120], Step [7499], Loss: 0.05103937163949013, Training Accuracy: 98.48333333333333
[ Sun Jul  7 07:25:17 2024 ] 	Batch(7500/7879) done. Loss: 0.0332  lr:0.000010
[ Sun Jul  7 07:25:35 2024 ] 	Batch(7600/7879) done. Loss: 0.0969  lr:0.000010
[ Sun Jul  7 07:25:53 2024 ] 	Batch(7700/7879) done. Loss: 0.1929  lr:0.000010
[ Sun Jul  7 07:26:10 2024 ] 	Batch(7800/7879) done. Loss: 0.0053  lr:0.000010
[ Sun Jul  7 07:26:25 2024 ] 	Mean training loss: 0.0697.
[ Sun Jul  7 07:26:25 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 07:26:25 2024 ] Training epoch: 59
[ Sun Jul  7 07:26:25 2024 ] 	Batch(0/7879) done. Loss: 0.0467  lr:0.000010
[ Sun Jul  7 07:26:43 2024 ] 	Batch(100/7879) done. Loss: 0.0662  lr:0.000010
[ Sun Jul  7 07:27:01 2024 ] 	Batch(200/7879) done. Loss: 0.0141  lr:0.000010
[ Sun Jul  7 07:27:19 2024 ] 	Batch(300/7879) done. Loss: 0.0152  lr:0.000010
[ Sun Jul  7 07:27:37 2024 ] 	Batch(400/7879) done. Loss: 0.0952  lr:0.000010
[ Sun Jul  7 07:27:55 2024 ] 
Training: Epoch [58/120], Step [499], Loss: 0.013712044805288315, Training Accuracy: 98.625
[ Sun Jul  7 07:27:56 2024 ] 	Batch(500/7879) done. Loss: 0.0396  lr:0.000010
[ Sun Jul  7 07:28:14 2024 ] 	Batch(600/7879) done. Loss: 0.1115  lr:0.000010
[ Sun Jul  7 07:28:32 2024 ] 	Batch(700/7879) done. Loss: 0.0440  lr:0.000010
[ Sun Jul  7 07:28:50 2024 ] 	Batch(800/7879) done. Loss: 0.1274  lr:0.000010
[ Sun Jul  7 07:29:08 2024 ] 	Batch(900/7879) done. Loss: 0.0047  lr:0.000010
[ Sun Jul  7 07:29:26 2024 ] 
Training: Epoch [58/120], Step [999], Loss: 0.037201933562755585, Training Accuracy: 98.3625
[ Sun Jul  7 07:29:26 2024 ] 	Batch(1000/7879) done. Loss: 0.0192  lr:0.000010
[ Sun Jul  7 07:29:44 2024 ] 	Batch(1100/7879) done. Loss: 0.1006  lr:0.000010
[ Sun Jul  7 07:30:02 2024 ] 	Batch(1200/7879) done. Loss: 0.0254  lr:0.000010
[ Sun Jul  7 07:30:20 2024 ] 	Batch(1300/7879) done. Loss: 0.0379  lr:0.000010
[ Sun Jul  7 07:30:38 2024 ] 	Batch(1400/7879) done. Loss: 0.0332  lr:0.000010
[ Sun Jul  7 07:30:56 2024 ] 
Training: Epoch [58/120], Step [1499], Loss: 0.07071582973003387, Training Accuracy: 98.52499999999999
[ Sun Jul  7 07:30:56 2024 ] 	Batch(1500/7879) done. Loss: 0.0162  lr:0.000010
[ Sun Jul  7 07:31:14 2024 ] 	Batch(1600/7879) done. Loss: 0.0995  lr:0.000010
[ Sun Jul  7 07:31:32 2024 ] 	Batch(1700/7879) done. Loss: 0.3253  lr:0.000010
[ Sun Jul  7 07:31:51 2024 ] 	Batch(1800/7879) done. Loss: 0.0088  lr:0.000010
[ Sun Jul  7 07:32:09 2024 ] 	Batch(1900/7879) done. Loss: 0.0793  lr:0.000010
[ Sun Jul  7 07:32:26 2024 ] 
Training: Epoch [58/120], Step [1999], Loss: 0.10746707767248154, Training Accuracy: 98.6125
[ Sun Jul  7 07:32:27 2024 ] 	Batch(2000/7879) done. Loss: 0.0162  lr:0.000010
[ Sun Jul  7 07:32:45 2024 ] 	Batch(2100/7879) done. Loss: 0.0089  lr:0.000010
[ Sun Jul  7 07:33:03 2024 ] 	Batch(2200/7879) done. Loss: 0.0009  lr:0.000010
[ Sun Jul  7 07:33:20 2024 ] 	Batch(2300/7879) done. Loss: 0.1111  lr:0.000010
[ Sun Jul  7 07:33:39 2024 ] 	Batch(2400/7879) done. Loss: 0.0402  lr:0.000010
[ Sun Jul  7 07:33:56 2024 ] 
Training: Epoch [58/120], Step [2499], Loss: 0.0890776515007019, Training Accuracy: 98.61999999999999
[ Sun Jul  7 07:33:56 2024 ] 	Batch(2500/7879) done. Loss: 0.0381  lr:0.000010
[ Sun Jul  7 07:34:14 2024 ] 	Batch(2600/7879) done. Loss: 0.0524  lr:0.000010
[ Sun Jul  7 07:34:32 2024 ] 	Batch(2700/7879) done. Loss: 0.2803  lr:0.000010
[ Sun Jul  7 07:34:50 2024 ] 	Batch(2800/7879) done. Loss: 0.0396  lr:0.000010
[ Sun Jul  7 07:35:08 2024 ] 	Batch(2900/7879) done. Loss: 0.0930  lr:0.000010
[ Sun Jul  7 07:35:26 2024 ] 
Training: Epoch [58/120], Step [2999], Loss: 0.016245009377598763, Training Accuracy: 98.66666666666667
[ Sun Jul  7 07:35:26 2024 ] 	Batch(3000/7879) done. Loss: 0.0366  lr:0.000010
[ Sun Jul  7 07:35:44 2024 ] 	Batch(3100/7879) done. Loss: 0.6211  lr:0.000010
[ Sun Jul  7 07:36:02 2024 ] 	Batch(3200/7879) done. Loss: 0.0159  lr:0.000010
[ Sun Jul  7 07:36:21 2024 ] 	Batch(3300/7879) done. Loss: 0.0483  lr:0.000010
[ Sun Jul  7 07:36:39 2024 ] 	Batch(3400/7879) done. Loss: 0.0082  lr:0.000010
[ Sun Jul  7 07:36:58 2024 ] 
Training: Epoch [58/120], Step [3499], Loss: 0.12664197385311127, Training Accuracy: 98.69642857142857
[ Sun Jul  7 07:36:58 2024 ] 	Batch(3500/7879) done. Loss: 0.1703  lr:0.000010
[ Sun Jul  7 07:37:17 2024 ] 	Batch(3600/7879) done. Loss: 0.1162  lr:0.000010
[ Sun Jul  7 07:37:35 2024 ] 	Batch(3700/7879) done. Loss: 0.0305  lr:0.000010
[ Sun Jul  7 07:37:54 2024 ] 	Batch(3800/7879) done. Loss: 0.0140  lr:0.000010
[ Sun Jul  7 07:38:12 2024 ] 	Batch(3900/7879) done. Loss: 0.0238  lr:0.000010
[ Sun Jul  7 07:38:30 2024 ] 
Training: Epoch [58/120], Step [3999], Loss: 0.07440035045146942, Training Accuracy: 98.6875
[ Sun Jul  7 07:38:30 2024 ] 	Batch(4000/7879) done. Loss: 0.3173  lr:0.000010
[ Sun Jul  7 07:38:48 2024 ] 	Batch(4100/7879) done. Loss: 0.0518  lr:0.000010
[ Sun Jul  7 07:39:06 2024 ] 	Batch(4200/7879) done. Loss: 0.0764  lr:0.000010
[ Sun Jul  7 07:39:24 2024 ] 	Batch(4300/7879) done. Loss: 0.0315  lr:0.000010
[ Sun Jul  7 07:39:42 2024 ] 	Batch(4400/7879) done. Loss: 0.0152  lr:0.000010
[ Sun Jul  7 07:40:00 2024 ] 
Training: Epoch [58/120], Step [4499], Loss: 0.054803185164928436, Training Accuracy: 98.675
[ Sun Jul  7 07:40:00 2024 ] 	Batch(4500/7879) done. Loss: 0.0242  lr:0.000010
[ Sun Jul  7 07:40:18 2024 ] 	Batch(4600/7879) done. Loss: 0.0483  lr:0.000010
[ Sun Jul  7 07:40:36 2024 ] 	Batch(4700/7879) done. Loss: 0.4593  lr:0.000010
[ Sun Jul  7 07:40:54 2024 ] 	Batch(4800/7879) done. Loss: 0.0425  lr:0.000010
[ Sun Jul  7 07:41:12 2024 ] 	Batch(4900/7879) done. Loss: 0.0104  lr:0.000010
[ Sun Jul  7 07:41:29 2024 ] 
Training: Epoch [58/120], Step [4999], Loss: 0.00564747117459774, Training Accuracy: 98.685
[ Sun Jul  7 07:41:30 2024 ] 	Batch(5000/7879) done. Loss: 0.0086  lr:0.000010
[ Sun Jul  7 07:41:47 2024 ] 	Batch(5100/7879) done. Loss: 0.0124  lr:0.000010
[ Sun Jul  7 07:42:06 2024 ] 	Batch(5200/7879) done. Loss: 0.0021  lr:0.000010
[ Sun Jul  7 07:42:23 2024 ] 	Batch(5300/7879) done. Loss: 0.0073  lr:0.000010
[ Sun Jul  7 07:42:41 2024 ] 	Batch(5400/7879) done. Loss: 0.0326  lr:0.000010
[ Sun Jul  7 07:42:59 2024 ] 
Training: Epoch [58/120], Step [5499], Loss: 0.002557446015998721, Training Accuracy: 98.67954545454546
[ Sun Jul  7 07:42:59 2024 ] 	Batch(5500/7879) done. Loss: 0.0032  lr:0.000010
[ Sun Jul  7 07:43:18 2024 ] 	Batch(5600/7879) done. Loss: 0.1575  lr:0.000010
[ Sun Jul  7 07:43:35 2024 ] 	Batch(5700/7879) done. Loss: 0.3695  lr:0.000010
[ Sun Jul  7 07:43:53 2024 ] 	Batch(5800/7879) done. Loss: 0.0989  lr:0.000010
[ Sun Jul  7 07:44:11 2024 ] 	Batch(5900/7879) done. Loss: 0.0058  lr:0.000010
[ Sun Jul  7 07:44:29 2024 ] 
Training: Epoch [58/120], Step [5999], Loss: 0.022173011675477028, Training Accuracy: 98.71458333333332
[ Sun Jul  7 07:44:29 2024 ] 	Batch(6000/7879) done. Loss: 0.0052  lr:0.000010
[ Sun Jul  7 07:44:47 2024 ] 	Batch(6100/7879) done. Loss: 0.0210  lr:0.000010
[ Sun Jul  7 07:45:05 2024 ] 	Batch(6200/7879) done. Loss: 0.0127  lr:0.000010
[ Sun Jul  7 07:45:23 2024 ] 	Batch(6300/7879) done. Loss: 0.0470  lr:0.000010
[ Sun Jul  7 07:45:41 2024 ] 	Batch(6400/7879) done. Loss: 0.5320  lr:0.000010
[ Sun Jul  7 07:46:00 2024 ] 
Training: Epoch [58/120], Step [6499], Loss: 0.021404428407549858, Training Accuracy: 98.73461538461538
[ Sun Jul  7 07:46:00 2024 ] 	Batch(6500/7879) done. Loss: 0.0117  lr:0.000010
[ Sun Jul  7 07:46:18 2024 ] 	Batch(6600/7879) done. Loss: 0.1713  lr:0.000010
[ Sun Jul  7 07:46:37 2024 ] 	Batch(6700/7879) done. Loss: 0.0406  lr:0.000010
[ Sun Jul  7 07:46:56 2024 ] 	Batch(6800/7879) done. Loss: 0.0346  lr:0.000010
[ Sun Jul  7 07:47:14 2024 ] 	Batch(6900/7879) done. Loss: 0.0556  lr:0.000010
[ Sun Jul  7 07:47:31 2024 ] 
Training: Epoch [58/120], Step [6999], Loss: 0.09856440871953964, Training Accuracy: 98.72142857142858
[ Sun Jul  7 07:47:32 2024 ] 	Batch(7000/7879) done. Loss: 0.0204  lr:0.000010
[ Sun Jul  7 07:47:50 2024 ] 	Batch(7100/7879) done. Loss: 0.0453  lr:0.000010
[ Sun Jul  7 07:48:08 2024 ] 	Batch(7200/7879) done. Loss: 0.0169  lr:0.000010
[ Sun Jul  7 07:48:26 2024 ] 	Batch(7300/7879) done. Loss: 0.0054  lr:0.000010
[ Sun Jul  7 07:48:43 2024 ] 	Batch(7400/7879) done. Loss: 0.0218  lr:0.000010
[ Sun Jul  7 07:49:01 2024 ] 
Training: Epoch [58/120], Step [7499], Loss: 0.02440772019326687, Training Accuracy: 98.72999999999999
[ Sun Jul  7 07:49:01 2024 ] 	Batch(7500/7879) done. Loss: 0.0298  lr:0.000010
[ Sun Jul  7 07:49:19 2024 ] 	Batch(7600/7879) done. Loss: 0.0999  lr:0.000010
[ Sun Jul  7 07:49:37 2024 ] 	Batch(7700/7879) done. Loss: 0.0761  lr:0.000010
[ Sun Jul  7 07:49:55 2024 ] 	Batch(7800/7879) done. Loss: 0.0130  lr:0.000010
[ Sun Jul  7 07:50:09 2024 ] 	Mean training loss: 0.0624.
[ Sun Jul  7 07:50:09 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 07:50:10 2024 ] Training epoch: 60
[ Sun Jul  7 07:50:10 2024 ] 	Batch(0/7879) done. Loss: 0.1362  lr:0.000010
[ Sun Jul  7 07:50:28 2024 ] 	Batch(100/7879) done. Loss: 0.0248  lr:0.000010
[ Sun Jul  7 07:50:47 2024 ] 	Batch(200/7879) done. Loss: 0.0359  lr:0.000010
[ Sun Jul  7 07:51:05 2024 ] 	Batch(300/7879) done. Loss: 0.0282  lr:0.000010
[ Sun Jul  7 07:51:24 2024 ] 	Batch(400/7879) done. Loss: 0.2822  lr:0.000010
[ Sun Jul  7 07:51:42 2024 ] 
Training: Epoch [59/120], Step [499], Loss: 0.046104565262794495, Training Accuracy: 98.3
[ Sun Jul  7 07:51:42 2024 ] 	Batch(500/7879) done. Loss: 0.0067  lr:0.000010
[ Sun Jul  7 07:52:01 2024 ] 	Batch(600/7879) done. Loss: 0.0162  lr:0.000010
[ Sun Jul  7 07:52:19 2024 ] 	Batch(700/7879) done. Loss: 0.0029  lr:0.000010
[ Sun Jul  7 07:52:38 2024 ] 	Batch(800/7879) done. Loss: 0.0040  lr:0.000010
[ Sun Jul  7 07:52:56 2024 ] 	Batch(900/7879) done. Loss: 0.0154  lr:0.000010
[ Sun Jul  7 07:53:14 2024 ] 
Training: Epoch [59/120], Step [999], Loss: 0.07669910788536072, Training Accuracy: 98.55000000000001
[ Sun Jul  7 07:53:14 2024 ] 	Batch(1000/7879) done. Loss: 0.0253  lr:0.000010
[ Sun Jul  7 07:53:32 2024 ] 	Batch(1100/7879) done. Loss: 0.0023  lr:0.000010
[ Sun Jul  7 07:53:50 2024 ] 	Batch(1200/7879) done. Loss: 0.0084  lr:0.000010
[ Sun Jul  7 07:54:08 2024 ] 	Batch(1300/7879) done. Loss: 0.1261  lr:0.000010
[ Sun Jul  7 07:54:26 2024 ] 	Batch(1400/7879) done. Loss: 0.2156  lr:0.000010
[ Sun Jul  7 07:54:44 2024 ] 
Training: Epoch [59/120], Step [1499], Loss: 0.24707303941249847, Training Accuracy: 98.575
[ Sun Jul  7 07:54:44 2024 ] 	Batch(1500/7879) done. Loss: 0.0185  lr:0.000010
[ Sun Jul  7 07:55:02 2024 ] 	Batch(1600/7879) done. Loss: 0.0344  lr:0.000010
[ Sun Jul  7 07:55:20 2024 ] 	Batch(1700/7879) done. Loss: 0.0115  lr:0.000010
[ Sun Jul  7 07:55:38 2024 ] 	Batch(1800/7879) done. Loss: 0.0229  lr:0.000010
[ Sun Jul  7 07:55:56 2024 ] 	Batch(1900/7879) done. Loss: 0.5651  lr:0.000010
[ Sun Jul  7 07:56:14 2024 ] 
Training: Epoch [59/120], Step [1999], Loss: 0.002052868949249387, Training Accuracy: 98.64375
[ Sun Jul  7 07:56:14 2024 ] 	Batch(2000/7879) done. Loss: 0.0607  lr:0.000010
[ Sun Jul  7 07:56:32 2024 ] 	Batch(2100/7879) done. Loss: 0.0222  lr:0.000010
[ Sun Jul  7 07:56:50 2024 ] 	Batch(2200/7879) done. Loss: 0.0545  lr:0.000010
[ Sun Jul  7 07:57:08 2024 ] 	Batch(2300/7879) done. Loss: 0.0115  lr:0.000010
[ Sun Jul  7 07:57:26 2024 ] 	Batch(2400/7879) done. Loss: 0.0231  lr:0.000010
[ Sun Jul  7 07:57:44 2024 ] 
Training: Epoch [59/120], Step [2499], Loss: 0.002909828210249543, Training Accuracy: 98.715
[ Sun Jul  7 07:57:44 2024 ] 	Batch(2500/7879) done. Loss: 0.0025  lr:0.000010
[ Sun Jul  7 07:58:02 2024 ] 	Batch(2600/7879) done. Loss: 0.0025  lr:0.000010
[ Sun Jul  7 07:58:20 2024 ] 	Batch(2700/7879) done. Loss: 0.0049  lr:0.000010
[ Sun Jul  7 07:58:38 2024 ] 	Batch(2800/7879) done. Loss: 0.1717  lr:0.000010
[ Sun Jul  7 07:58:56 2024 ] 	Batch(2900/7879) done. Loss: 0.0097  lr:0.000010
[ Sun Jul  7 07:59:13 2024 ] 
Training: Epoch [59/120], Step [2999], Loss: 0.03492249548435211, Training Accuracy: 98.72083333333333
[ Sun Jul  7 07:59:14 2024 ] 	Batch(3000/7879) done. Loss: 0.0599  lr:0.000010
[ Sun Jul  7 07:59:31 2024 ] 	Batch(3100/7879) done. Loss: 0.0109  lr:0.000010
[ Sun Jul  7 07:59:50 2024 ] 	Batch(3200/7879) done. Loss: 0.0129  lr:0.000010
[ Sun Jul  7 08:00:07 2024 ] 	Batch(3300/7879) done. Loss: 0.0189  lr:0.000010
[ Sun Jul  7 08:00:25 2024 ] 	Batch(3400/7879) done. Loss: 0.0494  lr:0.000010
[ Sun Jul  7 08:00:43 2024 ] 
Training: Epoch [59/120], Step [3499], Loss: 0.008612440899014473, Training Accuracy: 98.78571428571429
[ Sun Jul  7 08:00:43 2024 ] 	Batch(3500/7879) done. Loss: 0.0274  lr:0.000010
[ Sun Jul  7 08:01:01 2024 ] 	Batch(3600/7879) done. Loss: 0.0627  lr:0.000010
[ Sun Jul  7 08:01:19 2024 ] 	Batch(3700/7879) done. Loss: 0.0236  lr:0.000010
[ Sun Jul  7 08:01:37 2024 ] 	Batch(3800/7879) done. Loss: 0.0475  lr:0.000010
[ Sun Jul  7 08:01:55 2024 ] 	Batch(3900/7879) done. Loss: 0.0090  lr:0.000010
[ Sun Jul  7 08:02:13 2024 ] 
Training: Epoch [59/120], Step [3999], Loss: 0.10779494047164917, Training Accuracy: 98.83125
[ Sun Jul  7 08:02:13 2024 ] 	Batch(4000/7879) done. Loss: 0.2536  lr:0.000010
[ Sun Jul  7 08:02:31 2024 ] 	Batch(4100/7879) done. Loss: 0.0441  lr:0.000010
[ Sun Jul  7 08:02:49 2024 ] 	Batch(4200/7879) done. Loss: 0.0472  lr:0.000010
[ Sun Jul  7 08:03:07 2024 ] 	Batch(4300/7879) done. Loss: 0.0164  lr:0.000010
[ Sun Jul  7 08:03:25 2024 ] 	Batch(4400/7879) done. Loss: 0.0258  lr:0.000010
[ Sun Jul  7 08:03:43 2024 ] 
Training: Epoch [59/120], Step [4499], Loss: 0.008750751614570618, Training Accuracy: 98.85000000000001
[ Sun Jul  7 08:03:43 2024 ] 	Batch(4500/7879) done. Loss: 0.0081  lr:0.000010
[ Sun Jul  7 08:04:01 2024 ] 	Batch(4600/7879) done. Loss: 0.0063  lr:0.000010
[ Sun Jul  7 08:04:19 2024 ] 	Batch(4700/7879) done. Loss: 0.0301  lr:0.000010
[ Sun Jul  7 08:04:37 2024 ] 	Batch(4800/7879) done. Loss: 0.0036  lr:0.000010
[ Sun Jul  7 08:04:56 2024 ] 	Batch(4900/7879) done. Loss: 0.0243  lr:0.000010
[ Sun Jul  7 08:05:14 2024 ] 
Training: Epoch [59/120], Step [4999], Loss: 0.01871410384774208, Training Accuracy: 98.88749999999999
[ Sun Jul  7 08:05:15 2024 ] 	Batch(5000/7879) done. Loss: 0.0212  lr:0.000010
[ Sun Jul  7 08:05:33 2024 ] 	Batch(5100/7879) done. Loss: 0.0146  lr:0.000010
[ Sun Jul  7 08:05:52 2024 ] 	Batch(5200/7879) done. Loss: 0.0017  lr:0.000010
[ Sun Jul  7 08:06:10 2024 ] 	Batch(5300/7879) done. Loss: 0.0068  lr:0.000010
[ Sun Jul  7 08:06:28 2024 ] 	Batch(5400/7879) done. Loss: 0.0232  lr:0.000010
[ Sun Jul  7 08:06:46 2024 ] 
Training: Epoch [59/120], Step [5499], Loss: 0.3270079791545868, Training Accuracy: 98.9
[ Sun Jul  7 08:06:46 2024 ] 	Batch(5500/7879) done. Loss: 0.0722  lr:0.000010
[ Sun Jul  7 08:07:04 2024 ] 	Batch(5600/7879) done. Loss: 0.0039  lr:0.000010
[ Sun Jul  7 08:07:22 2024 ] 	Batch(5700/7879) done. Loss: 0.0569  lr:0.000010
[ Sun Jul  7 08:07:40 2024 ] 	Batch(5800/7879) done. Loss: 0.0118  lr:0.000010
[ Sun Jul  7 08:07:58 2024 ] 	Batch(5900/7879) done. Loss: 0.3227  lr:0.000010
[ Sun Jul  7 08:08:15 2024 ] 
Training: Epoch [59/120], Step [5999], Loss: 0.003936558961868286, Training Accuracy: 98.92291666666667
[ Sun Jul  7 08:08:16 2024 ] 	Batch(6000/7879) done. Loss: 0.0160  lr:0.000010
[ Sun Jul  7 08:08:34 2024 ] 	Batch(6100/7879) done. Loss: 0.0081  lr:0.000010
[ Sun Jul  7 08:08:52 2024 ] 	Batch(6200/7879) done. Loss: 0.0225  lr:0.000010
[ Sun Jul  7 08:09:10 2024 ] 	Batch(6300/7879) done. Loss: 0.0922  lr:0.000010
[ Sun Jul  7 08:09:28 2024 ] 	Batch(6400/7879) done. Loss: 0.0415  lr:0.000010
[ Sun Jul  7 08:09:45 2024 ] 
Training: Epoch [59/120], Step [6499], Loss: 0.008792080916464329, Training Accuracy: 98.94615384615385
[ Sun Jul  7 08:09:45 2024 ] 	Batch(6500/7879) done. Loss: 0.0458  lr:0.000010
[ Sun Jul  7 08:10:03 2024 ] 	Batch(6600/7879) done. Loss: 0.1475  lr:0.000010
[ Sun Jul  7 08:10:21 2024 ] 	Batch(6700/7879) done. Loss: 0.0475  lr:0.000010
[ Sun Jul  7 08:10:39 2024 ] 	Batch(6800/7879) done. Loss: 0.0179  lr:0.000010
[ Sun Jul  7 08:10:57 2024 ] 	Batch(6900/7879) done. Loss: 0.0273  lr:0.000010
[ Sun Jul  7 08:11:15 2024 ] 
Training: Epoch [59/120], Step [6999], Loss: 0.0169232040643692, Training Accuracy: 98.98035714285714
[ Sun Jul  7 08:11:15 2024 ] 	Batch(7000/7879) done. Loss: 0.0429  lr:0.000010
[ Sun Jul  7 08:11:33 2024 ] 	Batch(7100/7879) done. Loss: 0.0258  lr:0.000010
[ Sun Jul  7 08:11:51 2024 ] 	Batch(7200/7879) done. Loss: 0.0472  lr:0.000010
[ Sun Jul  7 08:12:10 2024 ] 	Batch(7300/7879) done. Loss: 0.0096  lr:0.000010
[ Sun Jul  7 08:12:28 2024 ] 	Batch(7400/7879) done. Loss: 0.0047  lr:0.000010
[ Sun Jul  7 08:12:47 2024 ] 
Training: Epoch [59/120], Step [7499], Loss: 0.11203707754611969, Training Accuracy: 99.00999999999999
[ Sun Jul  7 08:12:47 2024 ] 	Batch(7500/7879) done. Loss: 0.0512  lr:0.000010
[ Sun Jul  7 08:13:06 2024 ] 	Batch(7600/7879) done. Loss: 0.0019  lr:0.000010
[ Sun Jul  7 08:13:24 2024 ] 	Batch(7700/7879) done. Loss: 0.0880  lr:0.000010
[ Sun Jul  7 08:13:43 2024 ] 	Batch(7800/7879) done. Loss: 0.0242  lr:0.000010
[ Sun Jul  7 08:13:57 2024 ] 	Mean training loss: 0.0549.
[ Sun Jul  7 08:13:57 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 08:13:58 2024 ] Eval epoch: 60
[ Sun Jul  7 08:18:44 2024 ] 	Mean val loss of 6365 batches: 1.7244940045823158.
[ Sun Jul  7 08:18:44 2024 ] 
Validation: Epoch [59/120], Samples [39365.0/50919], Loss: 0.05260947346687317, Validation Accuracy: 77.30905948663563
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 1 : 204 / 275 = 74 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 2 : 217 / 273 = 79 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 3 : 222 / 273 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 4 : 218 / 275 = 79 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 5 : 244 / 275 = 88 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 6 : 228 / 275 = 82 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 7 : 251 / 273 = 91 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 8 : 262 / 273 = 95 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 9 : 174 / 273 = 63 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 10 : 119 / 273 = 43 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 11 : 127 / 272 = 46 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 12 : 222 / 271 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 13 : 267 / 275 = 97 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 14 : 267 / 276 = 96 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 15 : 233 / 273 = 85 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 16 : 221 / 274 = 80 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 17 : 236 / 273 = 86 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 18 : 237 / 274 = 86 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 19 : 252 / 272 = 92 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 20 : 252 / 273 = 92 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 21 : 222 / 274 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 22 : 228 / 274 = 83 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 23 : 258 / 276 = 93 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 24 : 222 / 274 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 25 : 260 / 275 = 94 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 26 : 271 / 276 = 98 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 27 : 224 / 275 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 28 : 173 / 275 = 62 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 29 : 147 / 275 = 53 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 30 : 172 / 276 = 62 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 31 : 236 / 276 = 85 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 32 : 246 / 276 = 89 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 33 : 237 / 276 = 85 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 34 : 241 / 276 = 87 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 35 : 236 / 275 = 85 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 36 : 232 / 276 = 84 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 37 : 246 / 276 = 89 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 38 : 257 / 276 = 93 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 39 : 254 / 276 = 92 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 40 : 187 / 276 = 67 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 41 : 260 / 276 = 94 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 42 : 262 / 275 = 95 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 43 : 173 / 276 = 62 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 44 : 242 / 276 = 87 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 45 : 250 / 276 = 90 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 46 : 216 / 276 = 78 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 47 : 221 / 275 = 80 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 48 : 223 / 275 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 49 : 228 / 274 = 83 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 50 : 238 / 276 = 86 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 51 : 250 / 276 = 90 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 52 : 232 / 276 = 84 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 53 : 229 / 276 = 82 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 54 : 269 / 274 = 98 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 55 : 243 / 276 = 88 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 56 : 247 / 275 = 89 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 57 : 266 / 276 = 96 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 58 : 267 / 273 = 97 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 59 : 266 / 276 = 96 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 60 : 474 / 561 = 84 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 61 : 452 / 566 = 79 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 62 : 455 / 572 = 79 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 63 : 516 / 570 = 90 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 64 : 432 / 574 = 75 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 65 : 498 / 573 = 86 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 66 : 415 / 573 = 72 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 67 : 418 / 575 = 72 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 68 : 335 / 575 = 58 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 69 : 467 / 575 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 70 : 228 / 575 = 39 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 71 : 194 / 575 = 33 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 72 : 150 / 571 = 26 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 73 : 193 / 570 = 33 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 74 : 379 / 569 = 66 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 75 : 258 / 573 = 45 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 76 : 343 / 574 = 59 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 77 : 411 / 573 = 71 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 78 : 403 / 575 = 70 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 79 : 547 / 574 = 95 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 80 : 498 / 573 = 86 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 81 : 312 / 575 = 54 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 82 : 380 / 575 = 66 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 83 : 327 / 572 = 57 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 84 : 425 / 574 = 74 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 85 : 403 / 574 = 70 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 86 : 512 / 575 = 89 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 87 : 503 / 576 = 87 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 88 : 437 / 575 = 76 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 89 : 472 / 576 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 90 : 272 / 574 = 47 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 91 : 463 / 568 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 92 : 365 / 576 = 63 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 93 : 418 / 573 = 72 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 94 : 512 / 574 = 89 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 95 : 521 / 575 = 90 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 96 : 560 / 575 = 97 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 97 : 551 / 574 = 95 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 98 : 543 / 575 = 94 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 99 : 543 / 574 = 94 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 100 : 484 / 574 = 84 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 101 : 517 / 574 = 90 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 102 : 339 / 575 = 58 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 103 : 500 / 576 = 86 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 104 : 271 / 575 = 47 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 105 : 245 / 575 = 42 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 106 : 321 / 576 = 55 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 107 : 484 / 576 = 84 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 108 : 474 / 575 = 82 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 109 : 365 / 575 = 63 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 110 : 479 / 575 = 83 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 111 : 535 / 576 = 92 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 112 : 543 / 575 = 94 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 113 : 497 / 576 = 86 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 114 : 470 / 576 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 115 : 503 / 576 = 87 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 116 : 468 / 575 = 81 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 117 : 460 / 575 = 80 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 118 : 491 / 575 = 85 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 119 : 515 / 576 = 89 %
[ Sun Jul  7 08:18:44 2024 ] Accuracy of 120 : 235 / 274 = 85 %
[ Sun Jul  7 08:18:44 2024 ] Training epoch: 61
[ Sun Jul  7 08:18:45 2024 ] 	Batch(0/7879) done. Loss: 0.1981  lr:0.000000
[ Sun Jul  7 08:19:03 2024 ] 	Batch(100/7879) done. Loss: 0.3375  lr:0.000000
[ Sun Jul  7 08:19:21 2024 ] 	Batch(200/7879) done. Loss: 0.0255  lr:0.000000
[ Sun Jul  7 08:19:39 2024 ] 	Batch(300/7879) done. Loss: 0.0192  lr:0.000000
[ Sun Jul  7 08:19:57 2024 ] 	Batch(400/7879) done. Loss: 0.3931  lr:0.000000
[ Sun Jul  7 08:20:15 2024 ] 
Training: Epoch [60/120], Step [499], Loss: 0.20806089043617249, Training Accuracy: 97.3
[ Sun Jul  7 08:20:15 2024 ] 	Batch(500/7879) done. Loss: 0.0225  lr:0.000000
[ Sun Jul  7 08:20:33 2024 ] 	Batch(600/7879) done. Loss: 0.0358  lr:0.000000
[ Sun Jul  7 08:20:51 2024 ] 	Batch(700/7879) done. Loss: 0.2514  lr:0.000000
[ Sun Jul  7 08:21:09 2024 ] 	Batch(800/7879) done. Loss: 0.0885  lr:0.000000
[ Sun Jul  7 08:21:27 2024 ] 	Batch(900/7879) done. Loss: 0.0012  lr:0.000000
[ Sun Jul  7 08:21:44 2024 ] 
Training: Epoch [60/120], Step [999], Loss: 0.1371167004108429, Training Accuracy: 97.6125
[ Sun Jul  7 08:21:44 2024 ] 	Batch(1000/7879) done. Loss: 0.0268  lr:0.000000
[ Sun Jul  7 08:22:02 2024 ] 	Batch(1100/7879) done. Loss: 0.0211  lr:0.000000
[ Sun Jul  7 08:22:20 2024 ] 	Batch(1200/7879) done. Loss: 0.1440  lr:0.000000
[ Sun Jul  7 08:22:38 2024 ] 	Batch(1300/7879) done. Loss: 0.0840  lr:0.000000
[ Sun Jul  7 08:22:56 2024 ] 	Batch(1400/7879) done. Loss: 0.1098  lr:0.000000
[ Sun Jul  7 08:23:14 2024 ] 
Training: Epoch [60/120], Step [1499], Loss: 0.15103992819786072, Training Accuracy: 97.75833333333334
[ Sun Jul  7 08:23:14 2024 ] 	Batch(1500/7879) done. Loss: 0.0516  lr:0.000000
[ Sun Jul  7 08:23:32 2024 ] 	Batch(1600/7879) done. Loss: 0.1517  lr:0.000000
[ Sun Jul  7 08:23:50 2024 ] 	Batch(1700/7879) done. Loss: 0.0040  lr:0.000000
[ Sun Jul  7 08:24:08 2024 ] 	Batch(1800/7879) done. Loss: 0.0134  lr:0.000000
[ Sun Jul  7 08:24:26 2024 ] 	Batch(1900/7879) done. Loss: 0.0923  lr:0.000000
[ Sun Jul  7 08:24:44 2024 ] 
Training: Epoch [60/120], Step [1999], Loss: 0.1525644063949585, Training Accuracy: 97.81875
[ Sun Jul  7 08:24:44 2024 ] 	Batch(2000/7879) done. Loss: 0.0996  lr:0.000000
[ Sun Jul  7 08:25:02 2024 ] 	Batch(2100/7879) done. Loss: 0.0837  lr:0.000000
[ Sun Jul  7 08:25:20 2024 ] 	Batch(2200/7879) done. Loss: 0.0043  lr:0.000000
[ Sun Jul  7 08:25:38 2024 ] 	Batch(2300/7879) done. Loss: 0.1620  lr:0.000000
[ Sun Jul  7 08:25:56 2024 ] 	Batch(2400/7879) done. Loss: 0.0466  lr:0.000000
[ Sun Jul  7 08:26:14 2024 ] 
Training: Epoch [60/120], Step [2499], Loss: 0.1580471396446228, Training Accuracy: 97.785
[ Sun Jul  7 08:26:14 2024 ] 	Batch(2500/7879) done. Loss: 0.0384  lr:0.000000
[ Sun Jul  7 08:26:32 2024 ] 	Batch(2600/7879) done. Loss: 0.0642  lr:0.000000
[ Sun Jul  7 08:26:50 2024 ] 	Batch(2700/7879) done. Loss: 0.1531  lr:0.000000
[ Sun Jul  7 08:27:08 2024 ] 	Batch(2800/7879) done. Loss: 0.0282  lr:0.000000
[ Sun Jul  7 08:27:26 2024 ] 	Batch(2900/7879) done. Loss: 0.0576  lr:0.000000
[ Sun Jul  7 08:27:44 2024 ] 
Training: Epoch [60/120], Step [2999], Loss: 0.06070387363433838, Training Accuracy: 97.76666666666667
[ Sun Jul  7 08:27:44 2024 ] 	Batch(3000/7879) done. Loss: 0.0418  lr:0.000000
[ Sun Jul  7 08:28:02 2024 ] 	Batch(3100/7879) done. Loss: 0.0077  lr:0.000000
[ Sun Jul  7 08:28:20 2024 ] 	Batch(3200/7879) done. Loss: 0.0276  lr:0.000000
[ Sun Jul  7 08:28:39 2024 ] 	Batch(3300/7879) done. Loss: 0.2551  lr:0.000000
[ Sun Jul  7 08:28:58 2024 ] 	Batch(3400/7879) done. Loss: 0.0193  lr:0.000000
[ Sun Jul  7 08:29:16 2024 ] 
Training: Epoch [60/120], Step [3499], Loss: 0.024137523025274277, Training Accuracy: 97.75
[ Sun Jul  7 08:29:16 2024 ] 	Batch(3500/7879) done. Loss: 0.0194  lr:0.000000
[ Sun Jul  7 08:29:35 2024 ] 	Batch(3600/7879) done. Loss: 0.0089  lr:0.000000
[ Sun Jul  7 08:29:53 2024 ] 	Batch(3700/7879) done. Loss: 0.0186  lr:0.000000
[ Sun Jul  7 08:30:12 2024 ] 	Batch(3800/7879) done. Loss: 0.0032  lr:0.000000
[ Sun Jul  7 08:30:31 2024 ] 	Batch(3900/7879) done. Loss: 0.1721  lr:0.000000
[ Sun Jul  7 08:30:49 2024 ] 
Training: Epoch [60/120], Step [3999], Loss: 0.01675867661833763, Training Accuracy: 97.79375
[ Sun Jul  7 08:30:49 2024 ] 	Batch(4000/7879) done. Loss: 0.0100  lr:0.000000
[ Sun Jul  7 08:31:08 2024 ] 	Batch(4100/7879) done. Loss: 0.0423  lr:0.000000
[ Sun Jul  7 08:31:26 2024 ] 	Batch(4200/7879) done. Loss: 0.0030  lr:0.000000
[ Sun Jul  7 08:31:45 2024 ] 	Batch(4300/7879) done. Loss: 0.1443  lr:0.000000
[ Sun Jul  7 08:32:04 2024 ] 	Batch(4400/7879) done. Loss: 0.1877  lr:0.000000
[ Sun Jul  7 08:32:22 2024 ] 
Training: Epoch [60/120], Step [4499], Loss: 0.027369247749447823, Training Accuracy: 97.80277777777778
[ Sun Jul  7 08:32:22 2024 ] 	Batch(4500/7879) done. Loss: 0.0861  lr:0.000000
[ Sun Jul  7 08:32:41 2024 ] 	Batch(4600/7879) done. Loss: 0.0110  lr:0.000000
[ Sun Jul  7 08:32:59 2024 ] 	Batch(4700/7879) done. Loss: 0.0405  lr:0.000000
[ Sun Jul  7 08:33:18 2024 ] 	Batch(4800/7879) done. Loss: 0.2402  lr:0.000000
[ Sun Jul  7 08:33:37 2024 ] 	Batch(4900/7879) done. Loss: 0.0403  lr:0.000000
[ Sun Jul  7 08:33:55 2024 ] 
Training: Epoch [60/120], Step [4999], Loss: 0.02774207666516304, Training Accuracy: 97.8175
[ Sun Jul  7 08:33:55 2024 ] 	Batch(5000/7879) done. Loss: 0.0175  lr:0.000000
[ Sun Jul  7 08:34:14 2024 ] 	Batch(5100/7879) done. Loss: 0.0113  lr:0.000000
[ Sun Jul  7 08:34:32 2024 ] 	Batch(5200/7879) done. Loss: 0.0323  lr:0.000000
[ Sun Jul  7 08:34:50 2024 ] 	Batch(5300/7879) done. Loss: 0.0808  lr:0.000000
[ Sun Jul  7 08:35:08 2024 ] 	Batch(5400/7879) done. Loss: 0.1365  lr:0.000000
[ Sun Jul  7 08:35:26 2024 ] 
Training: Epoch [60/120], Step [5499], Loss: 0.07839196175336838, Training Accuracy: 97.83181818181819
[ Sun Jul  7 08:35:26 2024 ] 	Batch(5500/7879) done. Loss: 0.0703  lr:0.000000
[ Sun Jul  7 08:35:44 2024 ] 	Batch(5600/7879) done. Loss: 0.0375  lr:0.000000
[ Sun Jul  7 08:36:03 2024 ] 	Batch(5700/7879) done. Loss: 0.1229  lr:0.000000
[ Sun Jul  7 08:36:21 2024 ] 	Batch(5800/7879) done. Loss: 0.4806  lr:0.000000
[ Sun Jul  7 08:36:40 2024 ] 	Batch(5900/7879) done. Loss: 0.0772  lr:0.000000
[ Sun Jul  7 08:36:58 2024 ] 
Training: Epoch [60/120], Step [5999], Loss: 0.05745416134595871, Training Accuracy: 97.82083333333334
[ Sun Jul  7 08:36:58 2024 ] 	Batch(6000/7879) done. Loss: 0.0061  lr:0.000000
[ Sun Jul  7 08:37:17 2024 ] 	Batch(6100/7879) done. Loss: 0.1680  lr:0.000000
[ Sun Jul  7 08:37:36 2024 ] 	Batch(6200/7879) done. Loss: 0.0046  lr:0.000000
[ Sun Jul  7 08:37:54 2024 ] 	Batch(6300/7879) done. Loss: 0.0334  lr:0.000000
[ Sun Jul  7 08:38:12 2024 ] 	Batch(6400/7879) done. Loss: 0.0048  lr:0.000000
[ Sun Jul  7 08:38:31 2024 ] 
Training: Epoch [60/120], Step [6499], Loss: 0.03314433619379997, Training Accuracy: 97.82499999999999
[ Sun Jul  7 08:38:31 2024 ] 	Batch(6500/7879) done. Loss: 0.2245  lr:0.000000
[ Sun Jul  7 08:38:49 2024 ] 	Batch(6600/7879) done. Loss: 0.0525  lr:0.000000
[ Sun Jul  7 08:39:07 2024 ] 	Batch(6700/7879) done. Loss: 0.0375  lr:0.000000
[ Sun Jul  7 08:39:25 2024 ] 	Batch(6800/7879) done. Loss: 0.0083  lr:0.000000
[ Sun Jul  7 08:39:44 2024 ] 	Batch(6900/7879) done. Loss: 0.0016  lr:0.000000
[ Sun Jul  7 08:40:02 2024 ] 
Training: Epoch [60/120], Step [6999], Loss: 0.2080308198928833, Training Accuracy: 97.82142857142857
[ Sun Jul  7 08:40:02 2024 ] 	Batch(7000/7879) done. Loss: 0.1039  lr:0.000000
[ Sun Jul  7 08:40:20 2024 ] 	Batch(7100/7879) done. Loss: 0.0454  lr:0.000000
[ Sun Jul  7 08:40:38 2024 ] 	Batch(7200/7879) done. Loss: 0.0083  lr:0.000000
[ Sun Jul  7 08:40:57 2024 ] 	Batch(7300/7879) done. Loss: 0.1170  lr:0.000000
[ Sun Jul  7 08:41:15 2024 ] 	Batch(7400/7879) done. Loss: 0.0993  lr:0.000000
[ Sun Jul  7 08:41:34 2024 ] 
Training: Epoch [60/120], Step [7499], Loss: 0.5772925019264221, Training Accuracy: 97.85666666666667
[ Sun Jul  7 08:41:34 2024 ] 	Batch(7500/7879) done. Loss: 0.2248  lr:0.000000
[ Sun Jul  7 08:41:53 2024 ] 	Batch(7600/7879) done. Loss: 0.1203  lr:0.000000
[ Sun Jul  7 08:42:11 2024 ] 	Batch(7700/7879) done. Loss: 0.1367  lr:0.000000
[ Sun Jul  7 08:42:29 2024 ] 	Batch(7800/7879) done. Loss: 0.0181  lr:0.000000
[ Sun Jul  7 08:42:43 2024 ] 	Mean training loss: 0.0875.
[ Sun Jul  7 08:42:43 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 08:42:43 2024 ] Training epoch: 62
[ Sun Jul  7 08:42:44 2024 ] 	Batch(0/7879) done. Loss: 0.0490  lr:0.000000
[ Sun Jul  7 08:43:02 2024 ] 	Batch(100/7879) done. Loss: 0.1833  lr:0.000000
[ Sun Jul  7 08:43:20 2024 ] 	Batch(200/7879) done. Loss: 0.0480  lr:0.000000
[ Sun Jul  7 08:43:38 2024 ] 	Batch(300/7879) done. Loss: 0.1345  lr:0.000000
[ Sun Jul  7 08:43:56 2024 ] 	Batch(400/7879) done. Loss: 0.1132  lr:0.000000
[ Sun Jul  7 08:44:13 2024 ] 
Training: Epoch [61/120], Step [499], Loss: 0.006193687207996845, Training Accuracy: 97.75
[ Sun Jul  7 08:44:14 2024 ] 	Batch(500/7879) done. Loss: 0.0733  lr:0.000000
[ Sun Jul  7 08:44:31 2024 ] 	Batch(600/7879) done. Loss: 0.3019  lr:0.000000
[ Sun Jul  7 08:44:49 2024 ] 	Batch(700/7879) done. Loss: 0.2336  lr:0.000000
[ Sun Jul  7 08:45:07 2024 ] 	Batch(800/7879) done. Loss: 0.0114  lr:0.000000
[ Sun Jul  7 08:45:25 2024 ] 	Batch(900/7879) done. Loss: 0.0298  lr:0.000000
[ Sun Jul  7 08:45:43 2024 ] 
Training: Epoch [61/120], Step [999], Loss: 0.33192065358161926, Training Accuracy: 97.89999999999999
[ Sun Jul  7 08:45:43 2024 ] 	Batch(1000/7879) done. Loss: 0.0089  lr:0.000000
[ Sun Jul  7 08:46:01 2024 ] 	Batch(1100/7879) done. Loss: 0.0138  lr:0.000000
[ Sun Jul  7 08:46:19 2024 ] 	Batch(1200/7879) done. Loss: 0.3735  lr:0.000000
[ Sun Jul  7 08:46:38 2024 ] 	Batch(1300/7879) done. Loss: 0.2409  lr:0.000000
[ Sun Jul  7 08:46:56 2024 ] 	Batch(1400/7879) done. Loss: 0.0197  lr:0.000000
[ Sun Jul  7 08:47:14 2024 ] 
Training: Epoch [61/120], Step [1499], Loss: 0.2176990658044815, Training Accuracy: 97.925
[ Sun Jul  7 08:47:15 2024 ] 	Batch(1500/7879) done. Loss: 0.0267  lr:0.000000
[ Sun Jul  7 08:47:33 2024 ] 	Batch(1600/7879) done. Loss: 0.0012  lr:0.000000
[ Sun Jul  7 08:47:51 2024 ] 	Batch(1700/7879) done. Loss: 0.0163  lr:0.000000
[ Sun Jul  7 08:48:09 2024 ] 	Batch(1800/7879) done. Loss: 0.1041  lr:0.000000
[ Sun Jul  7 08:48:26 2024 ] 	Batch(1900/7879) done. Loss: 0.1279  lr:0.000000
[ Sun Jul  7 08:48:44 2024 ] 
Training: Epoch [61/120], Step [1999], Loss: 0.05786706507205963, Training Accuracy: 97.82499999999999
[ Sun Jul  7 08:48:44 2024 ] 	Batch(2000/7879) done. Loss: 0.1501  lr:0.000000
[ Sun Jul  7 08:49:02 2024 ] 	Batch(2100/7879) done. Loss: 0.0438  lr:0.000000
[ Sun Jul  7 08:49:20 2024 ] 	Batch(2200/7879) done. Loss: 0.0873  lr:0.000000
[ Sun Jul  7 08:49:38 2024 ] 	Batch(2300/7879) done. Loss: 0.0591  lr:0.000000
[ Sun Jul  7 08:49:57 2024 ] 	Batch(2400/7879) done. Loss: 0.1172  lr:0.000000
[ Sun Jul  7 08:50:14 2024 ] 
Training: Epoch [61/120], Step [2499], Loss: 0.027588054537773132, Training Accuracy: 97.94
[ Sun Jul  7 08:50:14 2024 ] 	Batch(2500/7879) done. Loss: 0.0131  lr:0.000000
[ Sun Jul  7 08:50:32 2024 ] 	Batch(2600/7879) done. Loss: 0.0565  lr:0.000000
[ Sun Jul  7 08:50:50 2024 ] 	Batch(2700/7879) done. Loss: 0.0159  lr:0.000000
[ Sun Jul  7 08:51:08 2024 ] 	Batch(2800/7879) done. Loss: 0.0044  lr:0.000000
[ Sun Jul  7 08:51:26 2024 ] 	Batch(2900/7879) done. Loss: 0.0301  lr:0.000000
[ Sun Jul  7 08:51:44 2024 ] 
Training: Epoch [61/120], Step [2999], Loss: 0.007562697399407625, Training Accuracy: 97.85833333333333
[ Sun Jul  7 08:51:44 2024 ] 	Batch(3000/7879) done. Loss: 0.0077  lr:0.000000
[ Sun Jul  7 08:52:02 2024 ] 	Batch(3100/7879) done. Loss: 0.0264  lr:0.000000
[ Sun Jul  7 08:52:20 2024 ] 	Batch(3200/7879) done. Loss: 0.0973  lr:0.000000
[ Sun Jul  7 08:52:38 2024 ] 	Batch(3300/7879) done. Loss: 0.0264  lr:0.000000
[ Sun Jul  7 08:52:56 2024 ] 	Batch(3400/7879) done. Loss: 0.0482  lr:0.000000
[ Sun Jul  7 08:53:14 2024 ] 
Training: Epoch [61/120], Step [3499], Loss: 0.005680161528289318, Training Accuracy: 97.88571428571429
[ Sun Jul  7 08:53:14 2024 ] 	Batch(3500/7879) done. Loss: 0.0711  lr:0.000000
[ Sun Jul  7 08:53:32 2024 ] 	Batch(3600/7879) done. Loss: 0.1204  lr:0.000000
[ Sun Jul  7 08:53:50 2024 ] 	Batch(3700/7879) done. Loss: 0.0267  lr:0.000000
[ Sun Jul  7 08:54:08 2024 ] 	Batch(3800/7879) done. Loss: 0.0768  lr:0.000000
[ Sun Jul  7 08:54:26 2024 ] 	Batch(3900/7879) done. Loss: 0.0069  lr:0.000000
[ Sun Jul  7 08:54:43 2024 ] 
Training: Epoch [61/120], Step [3999], Loss: 0.006781122647225857, Training Accuracy: 97.871875
[ Sun Jul  7 08:54:44 2024 ] 	Batch(4000/7879) done. Loss: 0.2228  lr:0.000000
[ Sun Jul  7 08:55:02 2024 ] 	Batch(4100/7879) done. Loss: 0.0468  lr:0.000000
[ Sun Jul  7 08:55:20 2024 ] 	Batch(4200/7879) done. Loss: 0.0478  lr:0.000000
[ Sun Jul  7 08:55:38 2024 ] 	Batch(4300/7879) done. Loss: 0.0641  lr:0.000000
[ Sun Jul  7 08:55:56 2024 ] 	Batch(4400/7879) done. Loss: 0.0093  lr:0.000000
[ Sun Jul  7 08:56:15 2024 ] 
Training: Epoch [61/120], Step [4499], Loss: 0.030124438926577568, Training Accuracy: 97.87222222222223
[ Sun Jul  7 08:56:15 2024 ] 	Batch(4500/7879) done. Loss: 0.0101  lr:0.000000
[ Sun Jul  7 08:56:34 2024 ] 	Batch(4600/7879) done. Loss: 0.0327  lr:0.000000
[ Sun Jul  7 08:56:52 2024 ] 	Batch(4700/7879) done. Loss: 0.0169  lr:0.000000
[ Sun Jul  7 08:57:11 2024 ] 	Batch(4800/7879) done. Loss: 0.0232  lr:0.000000
[ Sun Jul  7 08:57:30 2024 ] 	Batch(4900/7879) done. Loss: 0.0737  lr:0.000000
[ Sun Jul  7 08:57:48 2024 ] 
Training: Epoch [61/120], Step [4999], Loss: 0.048227954655885696, Training Accuracy: 97.8725
[ Sun Jul  7 08:57:49 2024 ] 	Batch(5000/7879) done. Loss: 0.0695  lr:0.000000
[ Sun Jul  7 08:58:07 2024 ] 	Batch(5100/7879) done. Loss: 0.0182  lr:0.000000
[ Sun Jul  7 08:58:26 2024 ] 	Batch(5200/7879) done. Loss: 0.1110  lr:0.000000
[ Sun Jul  7 08:58:45 2024 ] 	Batch(5300/7879) done. Loss: 0.0120  lr:0.000000
[ Sun Jul  7 08:59:03 2024 ] 	Batch(5400/7879) done. Loss: 0.0159  lr:0.000000
[ Sun Jul  7 08:59:22 2024 ] 
Training: Epoch [61/120], Step [5499], Loss: 0.002295392332598567, Training Accuracy: 97.875
[ Sun Jul  7 08:59:22 2024 ] 	Batch(5500/7879) done. Loss: 0.0212  lr:0.000000
[ Sun Jul  7 08:59:41 2024 ] 	Batch(5600/7879) done. Loss: 0.0164  lr:0.000000
[ Sun Jul  7 08:59:59 2024 ] 	Batch(5700/7879) done. Loss: 0.0247  lr:0.000000
[ Sun Jul  7 09:00:17 2024 ] 	Batch(5800/7879) done. Loss: 0.0607  lr:0.000000
[ Sun Jul  7 09:00:34 2024 ] 	Batch(5900/7879) done. Loss: 0.2217  lr:0.000000
[ Sun Jul  7 09:00:52 2024 ] 
Training: Epoch [61/120], Step [5999], Loss: 0.03576308861374855, Training Accuracy: 97.86041666666667
[ Sun Jul  7 09:00:53 2024 ] 	Batch(6000/7879) done. Loss: 0.2870  lr:0.000000
[ Sun Jul  7 09:01:11 2024 ] 	Batch(6100/7879) done. Loss: 0.0430  lr:0.000000
[ Sun Jul  7 09:01:30 2024 ] 	Batch(6200/7879) done. Loss: 0.2247  lr:0.000000
[ Sun Jul  7 09:01:48 2024 ] 	Batch(6300/7879) done. Loss: 0.5300  lr:0.000000
[ Sun Jul  7 09:02:07 2024 ] 	Batch(6400/7879) done. Loss: 0.0122  lr:0.000000
[ Sun Jul  7 09:02:25 2024 ] 
Training: Epoch [61/120], Step [6499], Loss: 0.49717235565185547, Training Accuracy: 97.84807692307692
[ Sun Jul  7 09:02:26 2024 ] 	Batch(6500/7879) done. Loss: 0.0811  lr:0.000000
[ Sun Jul  7 09:02:44 2024 ] 	Batch(6600/7879) done. Loss: 0.0077  lr:0.000000
[ Sun Jul  7 09:03:03 2024 ] 	Batch(6700/7879) done. Loss: 0.0057  lr:0.000000
[ Sun Jul  7 09:03:22 2024 ] 	Batch(6800/7879) done. Loss: 0.0612  lr:0.000000
[ Sun Jul  7 09:03:40 2024 ] 	Batch(6900/7879) done. Loss: 0.1176  lr:0.000000
[ Sun Jul  7 09:03:59 2024 ] 
Training: Epoch [61/120], Step [6999], Loss: 0.003960947506129742, Training Accuracy: 97.8625
[ Sun Jul  7 09:03:59 2024 ] 	Batch(7000/7879) done. Loss: 0.0451  lr:0.000000
[ Sun Jul  7 09:04:17 2024 ] 	Batch(7100/7879) done. Loss: 0.0835  lr:0.000000
[ Sun Jul  7 09:04:36 2024 ] 	Batch(7200/7879) done. Loss: 0.0693  lr:0.000000
[ Sun Jul  7 09:04:54 2024 ] 	Batch(7300/7879) done. Loss: 0.0599  lr:0.000000
[ Sun Jul  7 09:05:11 2024 ] 	Batch(7400/7879) done. Loss: 0.0597  lr:0.000000
[ Sun Jul  7 09:05:29 2024 ] 
Training: Epoch [61/120], Step [7499], Loss: 0.009887762367725372, Training Accuracy: 97.88166666666666
[ Sun Jul  7 09:05:29 2024 ] 	Batch(7500/7879) done. Loss: 0.0047  lr:0.000000
[ Sun Jul  7 09:05:47 2024 ] 	Batch(7600/7879) done. Loss: 0.0149  lr:0.000000
[ Sun Jul  7 09:06:05 2024 ] 	Batch(7700/7879) done. Loss: 0.0205  lr:0.000000
[ Sun Jul  7 09:06:23 2024 ] 	Batch(7800/7879) done. Loss: 0.0933  lr:0.000000
[ Sun Jul  7 09:06:37 2024 ] 	Mean training loss: 0.0873.
[ Sun Jul  7 09:06:37 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 09:06:38 2024 ] Training epoch: 63
[ Sun Jul  7 09:06:38 2024 ] 	Batch(0/7879) done. Loss: 0.0110  lr:0.000000
[ Sun Jul  7 09:06:56 2024 ] 	Batch(100/7879) done. Loss: 0.0125  lr:0.000000
[ Sun Jul  7 09:07:14 2024 ] 	Batch(200/7879) done. Loss: 0.0474  lr:0.000000
[ Sun Jul  7 09:07:32 2024 ] 	Batch(300/7879) done. Loss: 0.0450  lr:0.000000
[ Sun Jul  7 09:07:51 2024 ] 	Batch(400/7879) done. Loss: 0.0522  lr:0.000000
[ Sun Jul  7 09:08:09 2024 ] 
Training: Epoch [62/120], Step [499], Loss: 0.035368312150239944, Training Accuracy: 97.775
[ Sun Jul  7 09:08:10 2024 ] 	Batch(500/7879) done. Loss: 0.0132  lr:0.000000
[ Sun Jul  7 09:08:28 2024 ] 	Batch(600/7879) done. Loss: 0.0394  lr:0.000000
[ Sun Jul  7 09:08:47 2024 ] 	Batch(700/7879) done. Loss: 0.1446  lr:0.000000
[ Sun Jul  7 09:09:05 2024 ] 	Batch(800/7879) done. Loss: 0.0174  lr:0.000000
[ Sun Jul  7 09:09:23 2024 ] 	Batch(900/7879) done. Loss: 0.0373  lr:0.000000
[ Sun Jul  7 09:09:41 2024 ] 
Training: Epoch [62/120], Step [999], Loss: 0.034466322511434555, Training Accuracy: 97.8375
[ Sun Jul  7 09:09:41 2024 ] 	Batch(1000/7879) done. Loss: 0.0578  lr:0.000000
[ Sun Jul  7 09:09:59 2024 ] 	Batch(1100/7879) done. Loss: 0.0303  lr:0.000000
[ Sun Jul  7 09:10:17 2024 ] 	Batch(1200/7879) done. Loss: 0.0029  lr:0.000000
[ Sun Jul  7 09:10:36 2024 ] 	Batch(1300/7879) done. Loss: 0.0155  lr:0.000000
[ Sun Jul  7 09:10:54 2024 ] 	Batch(1400/7879) done. Loss: 0.0210  lr:0.000000
[ Sun Jul  7 09:11:13 2024 ] 
Training: Epoch [62/120], Step [1499], Loss: 0.011728925630450249, Training Accuracy: 97.95
[ Sun Jul  7 09:11:13 2024 ] 	Batch(1500/7879) done. Loss: 0.1480  lr:0.000000
[ Sun Jul  7 09:11:32 2024 ] 	Batch(1600/7879) done. Loss: 0.0179  lr:0.000000
[ Sun Jul  7 09:11:50 2024 ] 	Batch(1700/7879) done. Loss: 0.0057  lr:0.000000
[ Sun Jul  7 09:12:09 2024 ] 	Batch(1800/7879) done. Loss: 0.0225  lr:0.000000
[ Sun Jul  7 09:12:27 2024 ] 	Batch(1900/7879) done. Loss: 0.0289  lr:0.000000
[ Sun Jul  7 09:12:45 2024 ] 
Training: Epoch [62/120], Step [1999], Loss: 0.007117970380932093, Training Accuracy: 97.98125
[ Sun Jul  7 09:12:46 2024 ] 	Batch(2000/7879) done. Loss: 0.0105  lr:0.000000
[ Sun Jul  7 09:13:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0641  lr:0.000000
[ Sun Jul  7 09:13:22 2024 ] 	Batch(2200/7879) done. Loss: 0.0151  lr:0.000000
[ Sun Jul  7 09:13:39 2024 ] 	Batch(2300/7879) done. Loss: 0.0182  lr:0.000000
[ Sun Jul  7 09:13:58 2024 ] 	Batch(2400/7879) done. Loss: 0.0565  lr:0.000000
[ Sun Jul  7 09:14:16 2024 ] 
Training: Epoch [62/120], Step [2499], Loss: 0.0586758553981781, Training Accuracy: 97.92999999999999
[ Sun Jul  7 09:14:16 2024 ] 	Batch(2500/7879) done. Loss: 0.0226  lr:0.000000
[ Sun Jul  7 09:14:35 2024 ] 	Batch(2600/7879) done. Loss: 0.4433  lr:0.000000
[ Sun Jul  7 09:14:53 2024 ] 	Batch(2700/7879) done. Loss: 0.0202  lr:0.000000
[ Sun Jul  7 09:15:11 2024 ] 	Batch(2800/7879) done. Loss: 0.0065  lr:0.000000
[ Sun Jul  7 09:15:29 2024 ] 	Batch(2900/7879) done. Loss: 0.1056  lr:0.000000
[ Sun Jul  7 09:15:47 2024 ] 
Training: Epoch [62/120], Step [2999], Loss: 0.39788898825645447, Training Accuracy: 97.87916666666666
[ Sun Jul  7 09:15:47 2024 ] 	Batch(3000/7879) done. Loss: 0.0877  lr:0.000000
[ Sun Jul  7 09:16:05 2024 ] 	Batch(3100/7879) done. Loss: 0.0084  lr:0.000000
[ Sun Jul  7 09:16:23 2024 ] 	Batch(3200/7879) done. Loss: 0.0030  lr:0.000000
[ Sun Jul  7 09:16:41 2024 ] 	Batch(3300/7879) done. Loss: 0.1044  lr:0.000000
[ Sun Jul  7 09:16:59 2024 ] 	Batch(3400/7879) done. Loss: 0.0019  lr:0.000000
[ Sun Jul  7 09:17:16 2024 ] 
Training: Epoch [62/120], Step [3499], Loss: 0.019783219322562218, Training Accuracy: 97.89999999999999
[ Sun Jul  7 09:17:17 2024 ] 	Batch(3500/7879) done. Loss: 0.0119  lr:0.000000
[ Sun Jul  7 09:17:35 2024 ] 	Batch(3600/7879) done. Loss: 0.0837  lr:0.000000
[ Sun Jul  7 09:17:53 2024 ] 	Batch(3700/7879) done. Loss: 0.1217  lr:0.000000
[ Sun Jul  7 09:18:10 2024 ] 	Batch(3800/7879) done. Loss: 0.0217  lr:0.000000
[ Sun Jul  7 09:18:28 2024 ] 	Batch(3900/7879) done. Loss: 0.1419  lr:0.000000
[ Sun Jul  7 09:18:46 2024 ] 
Training: Epoch [62/120], Step [3999], Loss: 0.02080305479466915, Training Accuracy: 97.89999999999999
[ Sun Jul  7 09:18:47 2024 ] 	Batch(4000/7879) done. Loss: 0.0649  lr:0.000000
[ Sun Jul  7 09:19:04 2024 ] 	Batch(4100/7879) done. Loss: 0.0126  lr:0.000000
[ Sun Jul  7 09:19:22 2024 ] 	Batch(4200/7879) done. Loss: 0.0480  lr:0.000000
[ Sun Jul  7 09:19:40 2024 ] 	Batch(4300/7879) done. Loss: 0.0127  lr:0.000000
[ Sun Jul  7 09:19:58 2024 ] 	Batch(4400/7879) done. Loss: 0.2606  lr:0.000000
[ Sun Jul  7 09:20:16 2024 ] 
Training: Epoch [62/120], Step [4499], Loss: 0.06710360944271088, Training Accuracy: 97.89999999999999
[ Sun Jul  7 09:20:16 2024 ] 	Batch(4500/7879) done. Loss: 0.2416  lr:0.000000
[ Sun Jul  7 09:20:34 2024 ] 	Batch(4600/7879) done. Loss: 0.0276  lr:0.000000
[ Sun Jul  7 09:20:52 2024 ] 	Batch(4700/7879) done. Loss: 0.0272  lr:0.000000
[ Sun Jul  7 09:21:10 2024 ] 	Batch(4800/7879) done. Loss: 0.0171  lr:0.000000
[ Sun Jul  7 09:21:28 2024 ] 	Batch(4900/7879) done. Loss: 0.0158  lr:0.000000
[ Sun Jul  7 09:21:46 2024 ] 
Training: Epoch [62/120], Step [4999], Loss: 0.025115013122558594, Training Accuracy: 97.88
[ Sun Jul  7 09:21:46 2024 ] 	Batch(5000/7879) done. Loss: 0.0245  lr:0.000000
[ Sun Jul  7 09:22:04 2024 ] 	Batch(5100/7879) done. Loss: 0.0275  lr:0.000000
[ Sun Jul  7 09:22:22 2024 ] 	Batch(5200/7879) done. Loss: 0.1232  lr:0.000000
[ Sun Jul  7 09:22:41 2024 ] 	Batch(5300/7879) done. Loss: 0.0589  lr:0.000000
[ Sun Jul  7 09:22:59 2024 ] 	Batch(5400/7879) done. Loss: 0.0795  lr:0.000000
[ Sun Jul  7 09:23:18 2024 ] 
Training: Epoch [62/120], Step [5499], Loss: 0.03244420513510704, Training Accuracy: 97.875
[ Sun Jul  7 09:23:18 2024 ] 	Batch(5500/7879) done. Loss: 0.3049  lr:0.000000
[ Sun Jul  7 09:23:37 2024 ] 	Batch(5600/7879) done. Loss: 0.0427  lr:0.000000
[ Sun Jul  7 09:23:55 2024 ] 	Batch(5700/7879) done. Loss: 0.3214  lr:0.000000
[ Sun Jul  7 09:24:12 2024 ] 	Batch(5800/7879) done. Loss: 0.0091  lr:0.000000
[ Sun Jul  7 09:24:30 2024 ] 	Batch(5900/7879) done. Loss: 0.1679  lr:0.000000
[ Sun Jul  7 09:24:48 2024 ] 
Training: Epoch [62/120], Step [5999], Loss: 0.06666520237922668, Training Accuracy: 97.83958333333334
[ Sun Jul  7 09:24:48 2024 ] 	Batch(6000/7879) done. Loss: 0.0042  lr:0.000000
[ Sun Jul  7 09:25:06 2024 ] 	Batch(6100/7879) done. Loss: 0.0449  lr:0.000000
[ Sun Jul  7 09:25:24 2024 ] 	Batch(6200/7879) done. Loss: 0.0283  lr:0.000000
[ Sun Jul  7 09:25:42 2024 ] 	Batch(6300/7879) done. Loss: 0.0967  lr:0.000000
[ Sun Jul  7 09:26:00 2024 ] 	Batch(6400/7879) done. Loss: 0.0939  lr:0.000000
[ Sun Jul  7 09:26:18 2024 ] 
Training: Epoch [62/120], Step [6499], Loss: 0.20905077457427979, Training Accuracy: 97.8326923076923
[ Sun Jul  7 09:26:18 2024 ] 	Batch(6500/7879) done. Loss: 0.1055  lr:0.000000
[ Sun Jul  7 09:26:36 2024 ] 	Batch(6600/7879) done. Loss: 0.0194  lr:0.000000
[ Sun Jul  7 09:26:54 2024 ] 	Batch(6700/7879) done. Loss: 0.0036  lr:0.000000
[ Sun Jul  7 09:27:12 2024 ] 	Batch(6800/7879) done. Loss: 0.0050  lr:0.000000
[ Sun Jul  7 09:27:31 2024 ] 	Batch(6900/7879) done. Loss: 0.1517  lr:0.000000
[ Sun Jul  7 09:27:49 2024 ] 
Training: Epoch [62/120], Step [6999], Loss: 0.06434764713048935, Training Accuracy: 97.86428571428571
[ Sun Jul  7 09:27:49 2024 ] 	Batch(7000/7879) done. Loss: 0.0129  lr:0.000000
[ Sun Jul  7 09:28:08 2024 ] 	Batch(7100/7879) done. Loss: 0.0114  lr:0.000000
[ Sun Jul  7 09:28:27 2024 ] 	Batch(7200/7879) done. Loss: 0.3410  lr:0.000000
[ Sun Jul  7 09:28:44 2024 ] 	Batch(7300/7879) done. Loss: 0.1086  lr:0.000000
[ Sun Jul  7 09:29:02 2024 ] 	Batch(7400/7879) done. Loss: 0.0122  lr:0.000000
[ Sun Jul  7 09:29:20 2024 ] 
Training: Epoch [62/120], Step [7499], Loss: 0.014509618282318115, Training Accuracy: 97.85833333333333
[ Sun Jul  7 09:29:20 2024 ] 	Batch(7500/7879) done. Loss: 0.0025  lr:0.000000
[ Sun Jul  7 09:29:38 2024 ] 	Batch(7600/7879) done. Loss: 0.0834  lr:0.000000
[ Sun Jul  7 09:29:56 2024 ] 	Batch(7700/7879) done. Loss: 0.1885  lr:0.000000
[ Sun Jul  7 09:30:15 2024 ] 	Batch(7800/7879) done. Loss: 0.0033  lr:0.000000
[ Sun Jul  7 09:30:29 2024 ] 	Mean training loss: 0.0873.
[ Sun Jul  7 09:30:29 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 09:30:29 2024 ] Training epoch: 64
[ Sun Jul  7 09:30:30 2024 ] 	Batch(0/7879) done. Loss: 0.0237  lr:0.000000
[ Sun Jul  7 09:30:49 2024 ] 	Batch(100/7879) done. Loss: 0.0183  lr:0.000000
[ Sun Jul  7 09:31:07 2024 ] 	Batch(200/7879) done. Loss: 0.1906  lr:0.000000
[ Sun Jul  7 09:31:26 2024 ] 	Batch(300/7879) done. Loss: 0.0138  lr:0.000000
[ Sun Jul  7 09:31:44 2024 ] 	Batch(400/7879) done. Loss: 0.1473  lr:0.000000
[ Sun Jul  7 09:32:02 2024 ] 
Training: Epoch [63/120], Step [499], Loss: 0.1344815492630005, Training Accuracy: 97.8
[ Sun Jul  7 09:32:02 2024 ] 	Batch(500/7879) done. Loss: 0.1239  lr:0.000000
[ Sun Jul  7 09:32:20 2024 ] 	Batch(600/7879) done. Loss: 0.2340  lr:0.000000
[ Sun Jul  7 09:32:38 2024 ] 	Batch(700/7879) done. Loss: 0.0313  lr:0.000000
[ Sun Jul  7 09:32:56 2024 ] 	Batch(800/7879) done. Loss: 0.0520  lr:0.000000
[ Sun Jul  7 09:33:14 2024 ] 	Batch(900/7879) done. Loss: 0.3344  lr:0.000000
[ Sun Jul  7 09:33:32 2024 ] 
Training: Epoch [63/120], Step [999], Loss: 0.07320079207420349, Training Accuracy: 97.91250000000001
[ Sun Jul  7 09:33:32 2024 ] 	Batch(1000/7879) done. Loss: 0.0219  lr:0.000000
[ Sun Jul  7 09:33:50 2024 ] 	Batch(1100/7879) done. Loss: 0.0043  lr:0.000000
[ Sun Jul  7 09:34:08 2024 ] 	Batch(1200/7879) done. Loss: 0.1035  lr:0.000000
[ Sun Jul  7 09:34:26 2024 ] 	Batch(1300/7879) done. Loss: 0.0055  lr:0.000000
[ Sun Jul  7 09:34:44 2024 ] 	Batch(1400/7879) done. Loss: 0.0660  lr:0.000000
[ Sun Jul  7 09:35:02 2024 ] 
Training: Epoch [63/120], Step [1499], Loss: 0.08712939918041229, Training Accuracy: 97.82499999999999
[ Sun Jul  7 09:35:02 2024 ] 	Batch(1500/7879) done. Loss: 0.2661  lr:0.000000
[ Sun Jul  7 09:35:20 2024 ] 	Batch(1600/7879) done. Loss: 0.1019  lr:0.000000
[ Sun Jul  7 09:35:38 2024 ] 	Batch(1700/7879) done. Loss: 0.0259  lr:0.000000
[ Sun Jul  7 09:35:56 2024 ] 	Batch(1800/7879) done. Loss: 0.0377  lr:0.000000
[ Sun Jul  7 09:36:14 2024 ] 	Batch(1900/7879) done. Loss: 0.0473  lr:0.000000
[ Sun Jul  7 09:36:32 2024 ] 
Training: Epoch [63/120], Step [1999], Loss: 0.04801313206553459, Training Accuracy: 97.725
[ Sun Jul  7 09:36:32 2024 ] 	Batch(2000/7879) done. Loss: 0.0491  lr:0.000000
[ Sun Jul  7 09:36:50 2024 ] 	Batch(2100/7879) done. Loss: 0.0170  lr:0.000000
[ Sun Jul  7 09:37:08 2024 ] 	Batch(2200/7879) done. Loss: 0.0332  lr:0.000000
[ Sun Jul  7 09:37:26 2024 ] 	Batch(2300/7879) done. Loss: 0.0513  lr:0.000000
[ Sun Jul  7 09:37:44 2024 ] 	Batch(2400/7879) done. Loss: 0.1271  lr:0.000000
[ Sun Jul  7 09:38:02 2024 ] 
Training: Epoch [63/120], Step [2499], Loss: 0.07931128889322281, Training Accuracy: 97.705
[ Sun Jul  7 09:38:02 2024 ] 	Batch(2500/7879) done. Loss: 0.2267  lr:0.000000
[ Sun Jul  7 09:38:20 2024 ] 	Batch(2600/7879) done. Loss: 0.0849  lr:0.000000
[ Sun Jul  7 09:38:38 2024 ] 	Batch(2700/7879) done. Loss: 0.1725  lr:0.000000
[ Sun Jul  7 09:38:56 2024 ] 	Batch(2800/7879) done. Loss: 0.0199  lr:0.000000
[ Sun Jul  7 09:39:14 2024 ] 	Batch(2900/7879) done. Loss: 0.0026  lr:0.000000
[ Sun Jul  7 09:39:32 2024 ] 
Training: Epoch [63/120], Step [2999], Loss: 0.08220712095499039, Training Accuracy: 97.71666666666667
[ Sun Jul  7 09:39:32 2024 ] 	Batch(3000/7879) done. Loss: 0.1320  lr:0.000000
[ Sun Jul  7 09:39:50 2024 ] 	Batch(3100/7879) done. Loss: 0.1365  lr:0.000000
[ Sun Jul  7 09:40:08 2024 ] 	Batch(3200/7879) done. Loss: 0.0248  lr:0.000000
[ Sun Jul  7 09:40:26 2024 ] 	Batch(3300/7879) done. Loss: 0.0554  lr:0.000000
[ Sun Jul  7 09:40:44 2024 ] 	Batch(3400/7879) done. Loss: 0.0136  lr:0.000000
[ Sun Jul  7 09:41:01 2024 ] 
Training: Epoch [63/120], Step [3499], Loss: 0.027385970577597618, Training Accuracy: 97.74642857142857
[ Sun Jul  7 09:41:01 2024 ] 	Batch(3500/7879) done. Loss: 0.0964  lr:0.000000
[ Sun Jul  7 09:41:20 2024 ] 	Batch(3600/7879) done. Loss: 0.0304  lr:0.000000
[ Sun Jul  7 09:41:38 2024 ] 	Batch(3700/7879) done. Loss: 0.0413  lr:0.000000
[ Sun Jul  7 09:41:57 2024 ] 	Batch(3800/7879) done. Loss: 0.1156  lr:0.000000
[ Sun Jul  7 09:42:15 2024 ] 	Batch(3900/7879) done. Loss: 0.0248  lr:0.000000
[ Sun Jul  7 09:42:34 2024 ] 
Training: Epoch [63/120], Step [3999], Loss: 0.011405247263610363, Training Accuracy: 97.753125
[ Sun Jul  7 09:42:34 2024 ] 	Batch(4000/7879) done. Loss: 0.1032  lr:0.000000
[ Sun Jul  7 09:42:52 2024 ] 	Batch(4100/7879) done. Loss: 0.0036  lr:0.000000
[ Sun Jul  7 09:43:10 2024 ] 	Batch(4200/7879) done. Loss: 0.0461  lr:0.000000
[ Sun Jul  7 09:43:28 2024 ] 	Batch(4300/7879) done. Loss: 0.0083  lr:0.000000
[ Sun Jul  7 09:43:46 2024 ] 	Batch(4400/7879) done. Loss: 0.0880  lr:0.000000
[ Sun Jul  7 09:44:04 2024 ] 
Training: Epoch [63/120], Step [4499], Loss: 0.6220483183860779, Training Accuracy: 97.78888888888889
[ Sun Jul  7 09:44:04 2024 ] 	Batch(4500/7879) done. Loss: 0.0620  lr:0.000000
[ Sun Jul  7 09:44:22 2024 ] 	Batch(4600/7879) done. Loss: 0.0296  lr:0.000000
[ Sun Jul  7 09:44:40 2024 ] 	Batch(4700/7879) done. Loss: 0.0592  lr:0.000000
[ Sun Jul  7 09:44:58 2024 ] 	Batch(4800/7879) done. Loss: 0.2756  lr:0.000000
[ Sun Jul  7 09:45:16 2024 ] 	Batch(4900/7879) done. Loss: 0.2410  lr:0.000000
[ Sun Jul  7 09:45:34 2024 ] 
Training: Epoch [63/120], Step [4999], Loss: 0.005694238469004631, Training Accuracy: 97.7975
[ Sun Jul  7 09:45:34 2024 ] 	Batch(5000/7879) done. Loss: 0.0060  lr:0.000000
[ Sun Jul  7 09:45:52 2024 ] 	Batch(5100/7879) done. Loss: 0.1409  lr:0.000000
[ Sun Jul  7 09:46:10 2024 ] 	Batch(5200/7879) done. Loss: 0.4098  lr:0.000000
[ Sun Jul  7 09:46:28 2024 ] 	Batch(5300/7879) done. Loss: 0.1086  lr:0.000000
[ Sun Jul  7 09:46:46 2024 ] 	Batch(5400/7879) done. Loss: 0.0550  lr:0.000000
[ Sun Jul  7 09:47:04 2024 ] 
Training: Epoch [63/120], Step [5499], Loss: 0.1744534969329834, Training Accuracy: 97.79318181818182
[ Sun Jul  7 09:47:04 2024 ] 	Batch(5500/7879) done. Loss: 0.0304  lr:0.000000
[ Sun Jul  7 09:47:22 2024 ] 	Batch(5600/7879) done. Loss: 0.0616  lr:0.000000
[ Sun Jul  7 09:47:40 2024 ] 	Batch(5700/7879) done. Loss: 0.0433  lr:0.000000
[ Sun Jul  7 09:47:59 2024 ] 	Batch(5800/7879) done. Loss: 0.0238  lr:0.000000
[ Sun Jul  7 09:48:18 2024 ] 	Batch(5900/7879) done. Loss: 0.2417  lr:0.000000
[ Sun Jul  7 09:48:36 2024 ] 
Training: Epoch [63/120], Step [5999], Loss: 0.0986805334687233, Training Accuracy: 97.80624999999999
[ Sun Jul  7 09:48:36 2024 ] 	Batch(6000/7879) done. Loss: 0.0679  lr:0.000000
[ Sun Jul  7 09:48:55 2024 ] 	Batch(6100/7879) done. Loss: 0.0148  lr:0.000000
[ Sun Jul  7 09:49:13 2024 ] 	Batch(6200/7879) done. Loss: 0.0291  lr:0.000000
[ Sun Jul  7 09:49:32 2024 ] 	Batch(6300/7879) done. Loss: 0.0190  lr:0.000000
[ Sun Jul  7 09:49:51 2024 ] 	Batch(6400/7879) done. Loss: 0.0410  lr:0.000000
[ Sun Jul  7 09:50:09 2024 ] 
Training: Epoch [63/120], Step [6499], Loss: 0.05448509752750397, Training Accuracy: 97.8173076923077
[ Sun Jul  7 09:50:09 2024 ] 	Batch(6500/7879) done. Loss: 0.0492  lr:0.000000
[ Sun Jul  7 09:50:28 2024 ] 	Batch(6600/7879) done. Loss: 0.0099  lr:0.000000
[ Sun Jul  7 09:50:46 2024 ] 	Batch(6700/7879) done. Loss: 0.0056  lr:0.000000
[ Sun Jul  7 09:51:05 2024 ] 	Batch(6800/7879) done. Loss: 0.1966  lr:0.000000
[ Sun Jul  7 09:51:24 2024 ] 	Batch(6900/7879) done. Loss: 0.0652  lr:0.000000
[ Sun Jul  7 09:51:42 2024 ] 
Training: Epoch [63/120], Step [6999], Loss: 0.019549328833818436, Training Accuracy: 97.82678571428572
[ Sun Jul  7 09:51:42 2024 ] 	Batch(7000/7879) done. Loss: 0.0503  lr:0.000000
[ Sun Jul  7 09:52:01 2024 ] 	Batch(7100/7879) done. Loss: 0.1219  lr:0.000000
[ Sun Jul  7 09:52:20 2024 ] 	Batch(7200/7879) done. Loss: 0.1417  lr:0.000000
[ Sun Jul  7 09:52:37 2024 ] 	Batch(7300/7879) done. Loss: 0.0317  lr:0.000000
[ Sun Jul  7 09:52:55 2024 ] 	Batch(7400/7879) done. Loss: 0.0066  lr:0.000000
[ Sun Jul  7 09:53:13 2024 ] 
Training: Epoch [63/120], Step [7499], Loss: 0.046226777136325836, Training Accuracy: 97.81166666666667
[ Sun Jul  7 09:53:13 2024 ] 	Batch(7500/7879) done. Loss: 0.0484  lr:0.000000
[ Sun Jul  7 09:53:31 2024 ] 	Batch(7600/7879) done. Loss: 0.0160  lr:0.000000
[ Sun Jul  7 09:53:49 2024 ] 	Batch(7700/7879) done. Loss: 0.0080  lr:0.000000
[ Sun Jul  7 09:54:07 2024 ] 	Batch(7800/7879) done. Loss: 0.0068  lr:0.000000
[ Sun Jul  7 09:54:22 2024 ] 	Mean training loss: 0.0882.
[ Sun Jul  7 09:54:22 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul  7 09:54:22 2024 ] Training epoch: 65
[ Sun Jul  7 09:54:22 2024 ] 	Batch(0/7879) done. Loss: 0.0067  lr:0.000000
[ Sun Jul  7 09:54:40 2024 ] 	Batch(100/7879) done. Loss: 0.0637  lr:0.000000
[ Sun Jul  7 09:54:58 2024 ] 	Batch(200/7879) done. Loss: 0.1395  lr:0.000000
[ Sun Jul  7 09:55:16 2024 ] 	Batch(300/7879) done. Loss: 0.0867  lr:0.000000
[ Sun Jul  7 09:55:34 2024 ] 	Batch(400/7879) done. Loss: 0.0286  lr:0.000000
[ Sun Jul  7 09:55:52 2024 ] 
Training: Epoch [64/120], Step [499], Loss: 0.0063032167963683605, Training Accuracy: 97.95
[ Sun Jul  7 09:55:52 2024 ] 	Batch(500/7879) done. Loss: 0.1931  lr:0.000000
[ Sun Jul  7 09:56:10 2024 ] 	Batch(600/7879) done. Loss: 0.3336  lr:0.000000
[ Sun Jul  7 09:56:28 2024 ] 	Batch(700/7879) done. Loss: 0.0144  lr:0.000000
[ Sun Jul  7 09:56:46 2024 ] 	Batch(800/7879) done. Loss: 0.0122  lr:0.000000
[ Sun Jul  7 09:57:04 2024 ] 	Batch(900/7879) done. Loss: 0.0413  lr:0.000000
[ Sun Jul  7 09:57:21 2024 ] 
Training: Epoch [64/120], Step [999], Loss: 0.10053548961877823, Training Accuracy: 97.8875
[ Sun Jul  7 09:57:22 2024 ] 	Batch(1000/7879) done. Loss: 0.0026  lr:0.000000
[ Sun Jul  7 09:57:40 2024 ] 	Batch(1100/7879) done. Loss: 0.1895  lr:0.000000
[ Sun Jul  7 09:57:58 2024 ] 	Batch(1200/7879) done. Loss: 0.0495  lr:0.000000
[ Sun Jul  7 09:58:17 2024 ] 	Batch(1300/7879) done. Loss: 0.6147  lr:0.000000
[ Sun Jul  7 09:58:35 2024 ] 	Batch(1400/7879) done. Loss: 0.0384  lr:0.000000
[ Sun Jul  7 09:58:53 2024 ] 
Training: Epoch [64/120], Step [1499], Loss: 0.13210776448249817, Training Accuracy: 97.85833333333333
[ Sun Jul  7 09:58:53 2024 ] 	Batch(1500/7879) done. Loss: 0.0066  lr:0.000000
[ Sun Jul  7 09:59:12 2024 ] 	Batch(1600/7879) done. Loss: 0.0194  lr:0.000000
[ Sun Jul  7 09:59:30 2024 ] 	Batch(1700/7879) done. Loss: 0.0496  lr:0.000000
[ Sun Jul  7 09:59:49 2024 ] 	Batch(1800/7879) done. Loss: 0.0664  lr:0.000000
[ Sun Jul  7 10:00:07 2024 ] 	Batch(1900/7879) done. Loss: 0.0951  lr:0.000000
[ Sun Jul  7 10:00:26 2024 ] 
Training: Epoch [64/120], Step [1999], Loss: 0.017337223514914513, Training Accuracy: 97.8375
[ Sun Jul  7 10:00:26 2024 ] 	Batch(2000/7879) done. Loss: 0.0059  lr:0.000000
[ Sun Jul  7 10:00:44 2024 ] 	Batch(2100/7879) done. Loss: 0.0209  lr:0.000000
[ Sun Jul  7 10:01:02 2024 ] 	Batch(2200/7879) done. Loss: 0.1266  lr:0.000000
[ Sun Jul  7 10:06:47 2024 ] Load weights from prova20/epoch10_model_old.pt.
[ Sun Jul  7 10:06:47 2024 ] Eval epoch: 1
[ Sun Jul  7 10:11:43 2024 ] 	Mean test loss of 6365 batches: 1.5151873108523335.
[ Sun Jul  7 10:11:43 2024 ] 	Class1 Precision: 61.68%, Recall: 85.77%
[ Sun Jul  7 10:11:43 2024 ] 	Class2 Precision: 64.22%, Recall: 73.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class3 Precision: 72.37%, Recall: 80.59%
[ Sun Jul  7 10:11:43 2024 ] 	Class4 Precision: 88.28%, Recall: 82.78%
[ Sun Jul  7 10:11:43 2024 ] 	Class5 Precision: 79.85%, Recall: 79.27%
[ Sun Jul  7 10:11:43 2024 ] 	Class6 Precision: 90.07%, Recall: 89.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class7 Precision: 83.76%, Recall: 82.55%
[ Sun Jul  7 10:11:43 2024 ] 	Class8 Precision: 95.40%, Recall: 91.21%
[ Sun Jul  7 10:11:43 2024 ] 	Class9 Precision: 97.42%, Recall: 96.70%
[ Sun Jul  7 10:11:43 2024 ] 	Class10 Precision: 79.84%, Recall: 71.06%
[ Sun Jul  7 10:11:43 2024 ] 	Class11 Precision: 43.73%, Recall: 42.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class12 Precision: 31.59%, Recall: 46.69%
[ Sun Jul  7 10:11:43 2024 ] 	Class13 Precision: 74.75%, Recall: 84.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class14 Precision: 89.86%, Recall: 96.73%
[ Sun Jul  7 10:11:43 2024 ] 	Class15 Precision: 88.70%, Recall: 96.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class16 Precision: 42.23%, Recall: 81.68%
[ Sun Jul  7 10:11:43 2024 ] 	Class17 Precision: 66.47%, Recall: 81.02%
[ Sun Jul  7 10:11:43 2024 ] 	Class18 Precision: 76.97%, Recall: 85.71%
[ Sun Jul  7 10:11:43 2024 ] 	Class19 Precision: 73.16%, Recall: 90.51%
[ Sun Jul  7 10:11:43 2024 ] 	Class20 Precision: 95.88%, Recall: 94.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class21 Precision: 94.62%, Recall: 90.11%
[ Sun Jul  7 10:11:43 2024 ] 	Class22 Precision: 90.73%, Recall: 82.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class23 Precision: 73.27%, Recall: 85.04%
[ Sun Jul  7 10:11:43 2024 ] 	Class24 Precision: 91.52%, Recall: 93.84%
[ Sun Jul  7 10:11:43 2024 ] 	Class25 Precision: 70.06%, Recall: 82.85%
[ Sun Jul  7 10:11:43 2024 ] 	Class26 Precision: 96.63%, Recall: 93.82%
[ Sun Jul  7 10:11:43 2024 ] 	Class27 Precision: 98.18%, Recall: 97.83%
[ Sun Jul  7 10:11:43 2024 ] 	Class28 Precision: 70.51%, Recall: 80.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class29 Precision: 26.25%, Recall: 62.91%
[ Sun Jul  7 10:11:43 2024 ] 	Class30 Precision: 50.00%, Recall: 52.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class31 Precision: 63.77%, Recall: 61.23%
[ Sun Jul  7 10:11:43 2024 ] 	Class32 Precision: 71.43%, Recall: 86.96%
[ Sun Jul  7 10:11:43 2024 ] 	Class33 Precision: 70.40%, Recall: 88.77%
[ Sun Jul  7 10:11:43 2024 ] 	Class34 Precision: 55.48%, Recall: 86.23%
[ Sun Jul  7 10:11:43 2024 ] 	Class35 Precision: 86.96%, Recall: 86.96%
[ Sun Jul  7 10:11:43 2024 ] 	Class36 Precision: 80.20%, Recall: 86.91%
[ Sun Jul  7 10:11:43 2024 ] 	Class37 Precision: 70.61%, Recall: 84.42%
[ Sun Jul  7 10:11:43 2024 ] 	Class38 Precision: 64.75%, Recall: 89.86%
[ Sun Jul  7 10:11:43 2024 ] 	Class39 Precision: 73.18%, Recall: 90.94%
[ Sun Jul  7 10:11:43 2024 ] 	Class40 Precision: 84.28%, Recall: 91.30%
[ Sun Jul  7 10:11:43 2024 ] 	Class41 Precision: 63.09%, Recall: 68.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class42 Precision: 92.45%, Recall: 93.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class43 Precision: 92.58%, Recall: 95.27%
[ Sun Jul  7 10:11:43 2024 ] 	Class44 Precision: 70.47%, Recall: 64.86%
[ Sun Jul  7 10:11:43 2024 ] 	Class45 Precision: 72.35%, Recall: 89.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class46 Precision: 80.39%, Recall: 89.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class47 Precision: 76.03%, Recall: 80.43%
[ Sun Jul  7 10:11:43 2024 ] 	Class48 Precision: 80.88%, Recall: 80.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class49 Precision: 83.33%, Recall: 80.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class50 Precision: 71.97%, Recall: 82.48%
[ Sun Jul  7 10:11:43 2024 ] 	Class51 Precision: 88.60%, Recall: 87.32%
[ Sun Jul  7 10:11:43 2024 ] 	Class52 Precision: 94.72%, Recall: 90.94%
[ Sun Jul  7 10:11:43 2024 ] 	Class53 Precision: 79.33%, Recall: 86.23%
[ Sun Jul  7 10:11:43 2024 ] 	Class54 Precision: 54.76%, Recall: 85.51%
[ Sun Jul  7 10:11:43 2024 ] 	Class55 Precision: 85.13%, Recall: 98.18%
[ Sun Jul  7 10:11:43 2024 ] 	Class56 Precision: 67.70%, Recall: 87.32%
[ Sun Jul  7 10:11:43 2024 ] 	Class57 Precision: 84.32%, Recall: 88.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class58 Precision: 86.69%, Recall: 96.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class59 Precision: 82.77%, Recall: 98.53%
[ Sun Jul  7 10:11:43 2024 ] 	Class60 Precision: 88.70%, Recall: 96.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class61 Precision: 89.25%, Recall: 82.89%
[ Sun Jul  7 10:11:43 2024 ] 	Class62 Precision: 94.65%, Recall: 78.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class63 Precision: 83.36%, Recall: 80.59%
[ Sun Jul  7 10:11:43 2024 ] 	Class64 Precision: 94.55%, Recall: 91.23%
[ Sun Jul  7 10:11:43 2024 ] 	Class65 Precision: 91.51%, Recall: 75.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class66 Precision: 93.15%, Recall: 87.78%
[ Sun Jul  7 10:11:43 2024 ] 	Class67 Precision: 57.70%, Recall: 71.90%
[ Sun Jul  7 10:11:43 2024 ] 	Class68 Precision: 87.74%, Recall: 72.17%
[ Sun Jul  7 10:11:43 2024 ] 	Class69 Precision: 46.70%, Recall: 56.70%
[ Sun Jul  7 10:11:43 2024 ] 	Class70 Precision: 76.51%, Recall: 81.57%
[ Sun Jul  7 10:11:43 2024 ] 	Class71 Precision: 46.54%, Recall: 39.83%
[ Sun Jul  7 10:11:43 2024 ] 	Class72 Precision: 34.78%, Recall: 37.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class73 Precision: 52.40%, Recall: 24.87%
[ Sun Jul  7 10:11:43 2024 ] 	Class74 Precision: 54.36%, Recall: 37.19%
[ Sun Jul  7 10:11:43 2024 ] 	Class75 Precision: 48.66%, Recall: 67.14%
[ Sun Jul  7 10:11:43 2024 ] 	Class76 Precision: 61.29%, Recall: 43.11%
[ Sun Jul  7 10:11:43 2024 ] 	Class77 Precision: 73.64%, Recall: 61.32%
[ Sun Jul  7 10:11:43 2024 ] 	Class78 Precision: 62.17%, Recall: 69.11%
[ Sun Jul  7 10:11:43 2024 ] 	Class79 Precision: 66.50%, Recall: 71.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class80 Precision: 96.10%, Recall: 94.43%
[ Sun Jul  7 10:11:43 2024 ] 	Class81 Precision: 84.29%, Recall: 87.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class82 Precision: 69.23%, Recall: 56.35%
[ Sun Jul  7 10:11:43 2024 ] 	Class83 Precision: 69.45%, Recall: 68.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class84 Precision: 49.10%, Recall: 57.17%
[ Sun Jul  7 10:11:43 2024 ] 	Class85 Precision: 92.27%, Recall: 74.91%
[ Sun Jul  7 10:11:43 2024 ] 	Class86 Precision: 73.04%, Recall: 69.86%
[ Sun Jul  7 10:11:43 2024 ] 	Class87 Precision: 94.84%, Recall: 89.57%
[ Sun Jul  7 10:11:43 2024 ] 	Class88 Precision: 84.92%, Recall: 85.07%
[ Sun Jul  7 10:11:43 2024 ] 	Class89 Precision: 86.75%, Recall: 70.61%
[ Sun Jul  7 10:11:43 2024 ] 	Class90 Precision: 80.88%, Recall: 82.99%
[ Sun Jul  7 10:11:43 2024 ] 	Class91 Precision: 74.02%, Recall: 42.68%
[ Sun Jul  7 10:11:43 2024 ] 	Class92 Precision: 88.37%, Recall: 82.92%
[ Sun Jul  7 10:11:43 2024 ] 	Class93 Precision: 81.86%, Recall: 61.11%
[ Sun Jul  7 10:11:43 2024 ] 	Class94 Precision: 85.25%, Recall: 72.60%
[ Sun Jul  7 10:11:43 2024 ] 	Class95 Precision: 90.04%, Recall: 88.15%
[ Sun Jul  7 10:11:43 2024 ] 	Class96 Precision: 87.67%, Recall: 90.26%
[ Sun Jul  7 10:11:43 2024 ] 	Class97 Precision: 98.25%, Recall: 97.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class98 Precision: 98.59%, Recall: 97.21%
[ Sun Jul  7 10:11:43 2024 ] 	Class99 Precision: 94.42%, Recall: 94.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class100 Precision: 92.52%, Recall: 94.77%
[ Sun Jul  7 10:11:43 2024 ] 	Class101 Precision: 98.45%, Recall: 88.33%
[ Sun Jul  7 10:11:43 2024 ] 	Class102 Precision: 92.88%, Recall: 88.68%
[ Sun Jul  7 10:11:43 2024 ] 	Class103 Precision: 58.89%, Recall: 58.78%
[ Sun Jul  7 10:11:43 2024 ] 	Class104 Precision: 89.96%, Recall: 85.59%
[ Sun Jul  7 10:11:43 2024 ] 	Class105 Precision: 62.96%, Recall: 47.30%
[ Sun Jul  7 10:11:43 2024 ] 	Class106 Precision: 75.38%, Recall: 43.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class107 Precision: 63.60%, Recall: 57.64%
[ Sun Jul  7 10:11:43 2024 ] 	Class108 Precision: 78.12%, Recall: 83.68%
[ Sun Jul  7 10:11:43 2024 ] 	Class109 Precision: 87.23%, Recall: 84.35%
[ Sun Jul  7 10:11:43 2024 ] 	Class110 Precision: 79.57%, Recall: 58.26%
[ Sun Jul  7 10:11:43 2024 ] 	Class111 Precision: 90.44%, Recall: 82.26%
[ Sun Jul  7 10:11:43 2024 ] 	Class112 Precision: 92.55%, Recall: 92.71%
[ Sun Jul  7 10:11:43 2024 ] 	Class113 Precision: 95.67%, Recall: 96.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class114 Precision: 96.79%, Recall: 88.89%
[ Sun Jul  7 10:11:43 2024 ] 	Class115 Precision: 82.57%, Recall: 84.72%
[ Sun Jul  7 10:11:43 2024 ] 	Class116 Precision: 90.21%, Recall: 89.58%
[ Sun Jul  7 10:11:43 2024 ] 	Class117 Precision: 84.42%, Recall: 82.96%
[ Sun Jul  7 10:11:43 2024 ] 	Class118 Precision: 93.79%, Recall: 78.78%
[ Sun Jul  7 10:11:43 2024 ] 	Class119 Precision: 93.52%, Recall: 85.39%
[ Sun Jul  7 10:11:43 2024 ] 	Class120 Precision: 96.02%, Recall: 92.19%
[ Sun Jul  7 10:11:43 2024 ] 	Class1 Top1: 85.77%
[ Sun Jul  7 10:11:43 2024 ] 	Class2 Top1: 73.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class3 Top1: 80.59%
[ Sun Jul  7 10:11:43 2024 ] 	Class4 Top1: 82.78%
[ Sun Jul  7 10:11:43 2024 ] 	Class5 Top1: 79.27%
[ Sun Jul  7 10:11:43 2024 ] 	Class6 Top1: 89.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class7 Top1: 82.55%
[ Sun Jul  7 10:11:43 2024 ] 	Class8 Top1: 91.21%
[ Sun Jul  7 10:11:43 2024 ] 	Class9 Top1: 96.70%
[ Sun Jul  7 10:11:43 2024 ] 	Class10 Top1: 71.06%
[ Sun Jul  7 10:11:43 2024 ] 	Class11 Top1: 42.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class12 Top1: 46.69%
[ Sun Jul  7 10:11:43 2024 ] 	Class13 Top1: 84.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class14 Top1: 96.73%
[ Sun Jul  7 10:11:43 2024 ] 	Class15 Top1: 96.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class16 Top1: 81.68%
[ Sun Jul  7 10:11:43 2024 ] 	Class17 Top1: 81.02%
[ Sun Jul  7 10:11:43 2024 ] 	Class18 Top1: 85.71%
[ Sun Jul  7 10:11:43 2024 ] 	Class19 Top1: 90.51%
[ Sun Jul  7 10:11:43 2024 ] 	Class20 Top1: 94.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class21 Top1: 90.11%
[ Sun Jul  7 10:11:43 2024 ] 	Class22 Top1: 82.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class23 Top1: 85.04%
[ Sun Jul  7 10:11:43 2024 ] 	Class24 Top1: 93.84%
[ Sun Jul  7 10:11:43 2024 ] 	Class25 Top1: 82.85%
[ Sun Jul  7 10:11:43 2024 ] 	Class26 Top1: 93.82%
[ Sun Jul  7 10:11:43 2024 ] 	Class27 Top1: 97.83%
[ Sun Jul  7 10:11:43 2024 ] 	Class28 Top1: 80.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class29 Top1: 62.91%
[ Sun Jul  7 10:11:43 2024 ] 	Class30 Top1: 52.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class31 Top1: 61.23%
[ Sun Jul  7 10:11:43 2024 ] 	Class32 Top1: 86.96%
[ Sun Jul  7 10:11:43 2024 ] 	Class33 Top1: 88.77%
[ Sun Jul  7 10:11:43 2024 ] 	Class34 Top1: 86.23%
[ Sun Jul  7 10:11:43 2024 ] 	Class35 Top1: 86.96%
[ Sun Jul  7 10:11:43 2024 ] 	Class36 Top1: 86.91%
[ Sun Jul  7 10:11:43 2024 ] 	Class37 Top1: 84.42%
[ Sun Jul  7 10:11:43 2024 ] 	Class38 Top1: 89.86%
[ Sun Jul  7 10:11:43 2024 ] 	Class39 Top1: 90.94%
[ Sun Jul  7 10:11:43 2024 ] 	Class40 Top1: 91.30%
[ Sun Jul  7 10:11:43 2024 ] 	Class41 Top1: 68.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class42 Top1: 93.12%
[ Sun Jul  7 10:11:43 2024 ] 	Class43 Top1: 95.27%
[ Sun Jul  7 10:11:43 2024 ] 	Class44 Top1: 64.86%
[ Sun Jul  7 10:11:43 2024 ] 	Class45 Top1: 89.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class46 Top1: 89.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class47 Top1: 80.43%
[ Sun Jul  7 10:11:43 2024 ] 	Class48 Top1: 80.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class49 Top1: 80.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class50 Top1: 82.48%
[ Sun Jul  7 10:11:43 2024 ] 	Class51 Top1: 87.32%
[ Sun Jul  7 10:11:43 2024 ] 	Class52 Top1: 90.94%
[ Sun Jul  7 10:11:43 2024 ] 	Class53 Top1: 86.23%
[ Sun Jul  7 10:11:43 2024 ] 	Class54 Top1: 85.51%
[ Sun Jul  7 10:11:43 2024 ] 	Class55 Top1: 98.18%
[ Sun Jul  7 10:11:43 2024 ] 	Class56 Top1: 87.32%
[ Sun Jul  7 10:11:43 2024 ] 	Class57 Top1: 88.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class58 Top1: 96.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class59 Top1: 98.53%
[ Sun Jul  7 10:11:43 2024 ] 	Class60 Top1: 96.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class61 Top1: 82.89%
[ Sun Jul  7 10:11:43 2024 ] 	Class62 Top1: 78.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class63 Top1: 80.59%
[ Sun Jul  7 10:11:43 2024 ] 	Class64 Top1: 91.23%
[ Sun Jul  7 10:11:43 2024 ] 	Class65 Top1: 75.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class66 Top1: 87.78%
[ Sun Jul  7 10:11:43 2024 ] 	Class67 Top1: 71.90%
[ Sun Jul  7 10:11:43 2024 ] 	Class68 Top1: 72.17%
[ Sun Jul  7 10:11:43 2024 ] 	Class69 Top1: 56.70%
[ Sun Jul  7 10:11:43 2024 ] 	Class70 Top1: 81.57%
[ Sun Jul  7 10:11:43 2024 ] 	Class71 Top1: 39.83%
[ Sun Jul  7 10:11:43 2024 ] 	Class72 Top1: 37.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class73 Top1: 24.87%
[ Sun Jul  7 10:11:43 2024 ] 	Class74 Top1: 37.19%
[ Sun Jul  7 10:11:43 2024 ] 	Class75 Top1: 67.14%
[ Sun Jul  7 10:11:43 2024 ] 	Class76 Top1: 43.11%
[ Sun Jul  7 10:11:43 2024 ] 	Class77 Top1: 61.32%
[ Sun Jul  7 10:11:43 2024 ] 	Class78 Top1: 69.11%
[ Sun Jul  7 10:11:43 2024 ] 	Class79 Top1: 71.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class80 Top1: 94.43%
[ Sun Jul  7 10:11:43 2024 ] 	Class81 Top1: 87.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class82 Top1: 56.35%
[ Sun Jul  7 10:11:43 2024 ] 	Class83 Top1: 68.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class84 Top1: 57.17%
[ Sun Jul  7 10:11:43 2024 ] 	Class85 Top1: 74.91%
[ Sun Jul  7 10:11:43 2024 ] 	Class86 Top1: 69.86%
[ Sun Jul  7 10:11:43 2024 ] 	Class87 Top1: 89.57%
[ Sun Jul  7 10:11:43 2024 ] 	Class88 Top1: 85.07%
[ Sun Jul  7 10:11:43 2024 ] 	Class89 Top1: 70.61%
[ Sun Jul  7 10:11:43 2024 ] 	Class90 Top1: 82.99%
[ Sun Jul  7 10:11:43 2024 ] 	Class91 Top1: 42.68%
[ Sun Jul  7 10:11:43 2024 ] 	Class92 Top1: 82.92%
[ Sun Jul  7 10:11:43 2024 ] 	Class93 Top1: 61.11%
[ Sun Jul  7 10:11:43 2024 ] 	Class94 Top1: 72.60%
[ Sun Jul  7 10:11:43 2024 ] 	Class95 Top1: 88.15%
[ Sun Jul  7 10:11:43 2024 ] 	Class96 Top1: 90.26%
[ Sun Jul  7 10:11:43 2024 ] 	Class97 Top1: 97.74%
[ Sun Jul  7 10:11:43 2024 ] 	Class98 Top1: 97.21%
[ Sun Jul  7 10:11:43 2024 ] 	Class99 Top1: 94.09%
[ Sun Jul  7 10:11:43 2024 ] 	Class100 Top1: 94.77%
[ Sun Jul  7 10:11:43 2024 ] 	Class101 Top1: 88.33%
[ Sun Jul  7 10:11:43 2024 ] 	Class102 Top1: 88.68%
[ Sun Jul  7 10:11:43 2024 ] 	Class103 Top1: 58.78%
[ Sun Jul  7 10:11:43 2024 ] 	Class104 Top1: 85.59%
[ Sun Jul  7 10:11:43 2024 ] 	Class105 Top1: 47.30%
[ Sun Jul  7 10:11:43 2024 ] 	Class106 Top1: 43.13%
[ Sun Jul  7 10:11:43 2024 ] 	Class107 Top1: 57.64%
[ Sun Jul  7 10:11:43 2024 ] 	Class108 Top1: 83.68%
[ Sun Jul  7 10:11:43 2024 ] 	Class109 Top1: 84.35%
[ Sun Jul  7 10:11:43 2024 ] 	Class110 Top1: 58.26%
[ Sun Jul  7 10:11:43 2024 ] 	Class111 Top1: 82.26%
[ Sun Jul  7 10:11:43 2024 ] 	Class112 Top1: 92.71%
[ Sun Jul  7 10:11:43 2024 ] 	Class113 Top1: 96.00%
[ Sun Jul  7 10:11:43 2024 ] 	Class114 Top1: 88.89%
[ Sun Jul  7 10:11:43 2024 ] 	Class115 Top1: 84.72%
[ Sun Jul  7 10:11:43 2024 ] 	Class116 Top1: 89.58%
[ Sun Jul  7 10:11:43 2024 ] 	Class117 Top1: 82.96%
[ Sun Jul  7 10:11:43 2024 ] 	Class118 Top1: 78.78%
[ Sun Jul  7 10:11:43 2024 ] 	Class119 Top1: 85.39%
[ Sun Jul  7 10:11:43 2024 ] 	Class120 Top1: 92.19%
[ Sun Jul  7 10:11:43 2024 ] 	Top1: 79.17%
[ Sun Jul  7 10:11:44 2024 ] 	Class1 Top5: 95.62%
[ Sun Jul  7 10:11:44 2024 ] 	Class2 Top5: 85.09%
[ Sun Jul  7 10:11:44 2024 ] 	Class3 Top5: 92.67%
[ Sun Jul  7 10:11:44 2024 ] 	Class4 Top5: 94.14%
[ Sun Jul  7 10:11:44 2024 ] 	Class5 Top5: 88.36%
[ Sun Jul  7 10:11:44 2024 ] 	Class6 Top5: 97.45%
[ Sun Jul  7 10:11:44 2024 ] 	Class7 Top5: 93.82%
[ Sun Jul  7 10:11:44 2024 ] 	Class8 Top5: 96.70%
[ Sun Jul  7 10:11:44 2024 ] 	Class9 Top5: 99.27%
[ Sun Jul  7 10:11:44 2024 ] 	Class10 Top5: 92.31%
[ Sun Jul  7 10:11:44 2024 ] 	Class11 Top5: 87.18%
[ Sun Jul  7 10:11:44 2024 ] 	Class12 Top5: 91.18%
[ Sun Jul  7 10:11:44 2024 ] 	Class13 Top5: 95.94%
[ Sun Jul  7 10:11:44 2024 ] 	Class14 Top5: 99.64%
[ Sun Jul  7 10:11:44 2024 ] 	Class15 Top5: 99.64%
[ Sun Jul  7 10:11:44 2024 ] 	Class16 Top5: 98.53%
[ Sun Jul  7 10:11:44 2024 ] 	Class17 Top5: 95.99%
[ Sun Jul  7 10:11:44 2024 ] 	Class18 Top5: 97.44%
[ Sun Jul  7 10:11:44 2024 ] 	Class19 Top5: 98.54%
[ Sun Jul  7 10:11:44 2024 ] 	Class20 Top5: 98.90%
[ Sun Jul  7 10:11:44 2024 ] 	Class21 Top5: 97.80%
[ Sun Jul  7 10:11:44 2024 ] 	Class22 Top5: 95.26%
[ Sun Jul  7 10:11:44 2024 ] 	Class23 Top5: 96.72%
[ Sun Jul  7 10:11:44 2024 ] 	Class24 Top5: 97.83%
[ Sun Jul  7 10:11:44 2024 ] 	Class25 Top5: 93.80%
[ Sun Jul  7 10:11:44 2024 ] 	Class26 Top5: 97.82%
[ Sun Jul  7 10:11:44 2024 ] 	Class27 Top5: 99.28%
[ Sun Jul  7 10:11:44 2024 ] 	Class28 Top5: 95.27%
[ Sun Jul  7 10:11:44 2024 ] 	Class29 Top5: 89.09%
[ Sun Jul  7 10:11:44 2024 ] 	Class30 Top5: 86.91%
[ Sun Jul  7 10:11:44 2024 ] 	Class31 Top5: 91.67%
[ Sun Jul  7 10:11:44 2024 ] 	Class32 Top5: 95.29%
[ Sun Jul  7 10:11:44 2024 ] 	Class33 Top5: 94.93%
[ Sun Jul  7 10:11:44 2024 ] 	Class34 Top5: 97.83%
[ Sun Jul  7 10:11:44 2024 ] 	Class35 Top5: 96.01%
[ Sun Jul  7 10:11:44 2024 ] 	Class36 Top5: 93.09%
[ Sun Jul  7 10:11:44 2024 ] 	Class37 Top5: 97.10%
[ Sun Jul  7 10:11:44 2024 ] 	Class38 Top5: 97.10%
[ Sun Jul  7 10:11:44 2024 ] 	Class39 Top5: 98.19%
[ Sun Jul  7 10:11:44 2024 ] 	Class40 Top5: 98.91%
[ Sun Jul  7 10:11:44 2024 ] 	Class41 Top5: 88.77%
[ Sun Jul  7 10:11:44 2024 ] 	Class42 Top5: 99.28%
[ Sun Jul  7 10:11:44 2024 ] 	Class43 Top5: 99.27%
[ Sun Jul  7 10:11:44 2024 ] 	Class44 Top5: 92.75%
[ Sun Jul  7 10:11:44 2024 ] 	Class45 Top5: 98.55%
[ Sun Jul  7 10:11:44 2024 ] 	Class46 Top5: 97.10%
[ Sun Jul  7 10:11:44 2024 ] 	Class47 Top5: 95.29%
[ Sun Jul  7 10:11:44 2024 ] 	Class48 Top5: 98.91%
[ Sun Jul  7 10:11:44 2024 ] 	Class49 Top5: 94.91%
[ Sun Jul  7 10:11:44 2024 ] 	Class50 Top5: 95.62%
[ Sun Jul  7 10:11:44 2024 ] 	Class51 Top5: 96.01%
[ Sun Jul  7 10:11:44 2024 ] 	Class52 Top5: 98.19%
[ Sun Jul  7 10:11:44 2024 ] 	Class53 Top5: 96.38%
[ Sun Jul  7 10:11:44 2024 ] 	Class54 Top5: 97.46%
[ Sun Jul  7 10:11:44 2024 ] 	Class55 Top5: 99.27%
[ Sun Jul  7 10:11:44 2024 ] 	Class56 Top5: 97.46%
[ Sun Jul  7 10:11:44 2024 ] 	Class57 Top5: 97.82%
[ Sun Jul  7 10:11:44 2024 ] 	Class58 Top5: 97.83%
[ Sun Jul  7 10:11:44 2024 ] 	Class59 Top5: 99.63%
[ Sun Jul  7 10:11:44 2024 ] 	Class60 Top5: 99.64%
[ Sun Jul  7 10:11:44 2024 ] 	Class61 Top5: 93.23%
[ Sun Jul  7 10:11:44 2024 ] 	Class62 Top5: 93.11%
[ Sun Jul  7 10:11:44 2024 ] 	Class63 Top5: 90.91%
[ Sun Jul  7 10:11:44 2024 ] 	Class64 Top5: 94.74%
[ Sun Jul  7 10:11:44 2024 ] 	Class65 Top5: 88.33%
[ Sun Jul  7 10:11:44 2024 ] 	Class66 Top5: 93.37%
[ Sun Jul  7 10:11:44 2024 ] 	Class67 Top5: 92.32%
[ Sun Jul  7 10:11:44 2024 ] 	Class68 Top5: 89.74%
[ Sun Jul  7 10:11:44 2024 ] 	Class69 Top5: 92.00%
[ Sun Jul  7 10:11:44 2024 ] 	Class70 Top5: 93.57%
[ Sun Jul  7 10:11:44 2024 ] 	Class71 Top5: 90.43%
[ Sun Jul  7 10:11:44 2024 ] 	Class72 Top5: 92.00%
[ Sun Jul  7 10:11:44 2024 ] 	Class73 Top5: 80.04%
[ Sun Jul  7 10:11:44 2024 ] 	Class74 Top5: 80.00%
[ Sun Jul  7 10:11:44 2024 ] 	Class75 Top5: 91.56%
[ Sun Jul  7 10:11:44 2024 ] 	Class76 Top5: 79.93%
[ Sun Jul  7 10:11:44 2024 ] 	Class77 Top5: 81.53%
[ Sun Jul  7 10:11:44 2024 ] 	Class78 Top5: 90.58%
[ Sun Jul  7 10:11:44 2024 ] 	Class79 Top5: 92.52%
[ Sun Jul  7 10:11:44 2024 ] 	Class80 Top5: 97.74%
[ Sun Jul  7 10:11:44 2024 ] 	Class81 Top5: 94.42%
[ Sun Jul  7 10:11:44 2024 ] 	Class82 Top5: 84.52%
[ Sun Jul  7 10:11:44 2024 ] 	Class83 Top5: 88.35%
[ Sun Jul  7 10:11:44 2024 ] 	Class84 Top5: 92.13%
[ Sun Jul  7 10:11:44 2024 ] 	Class85 Top5: 90.94%
[ Sun Jul  7 10:11:44 2024 ] 	Class86 Top5: 90.59%
[ Sun Jul  7 10:11:44 2024 ] 	Class87 Top5: 96.00%
[ Sun Jul  7 10:11:44 2024 ] 	Class88 Top5: 96.70%
[ Sun Jul  7 10:11:44 2024 ] 	Class89 Top5: 91.13%
[ Sun Jul  7 10:11:44 2024 ] 	Class90 Top5: 94.79%
[ Sun Jul  7 10:11:44 2024 ] 	Class91 Top5: 82.58%
[ Sun Jul  7 10:11:44 2024 ] 	Class92 Top5: 95.60%
[ Sun Jul  7 10:11:44 2024 ] 	Class93 Top5: 82.47%
[ Sun Jul  7 10:11:44 2024 ] 	Class94 Top5: 91.45%
[ Sun Jul  7 10:11:44 2024 ] 	Class95 Top5: 98.26%
[ Sun Jul  7 10:11:44 2024 ] 	Class96 Top5: 96.70%
[ Sun Jul  7 10:11:44 2024 ] 	Class97 Top5: 98.61%
[ Sun Jul  7 10:11:44 2024 ] 	Class98 Top5: 99.13%
[ Sun Jul  7 10:11:44 2024 ] 	Class99 Top5: 97.04%
[ Sun Jul  7 10:11:44 2024 ] 	Class100 Top5: 97.91%
[ Sun Jul  7 10:11:44 2024 ] 	Class101 Top5: 96.52%
[ Sun Jul  7 10:11:44 2024 ] 	Class102 Top5: 95.12%
[ Sun Jul  7 10:11:44 2024 ] 	Class103 Top5: 92.70%
[ Sun Jul  7 10:11:44 2024 ] 	Class104 Top5: 97.57%
[ Sun Jul  7 10:11:44 2024 ] 	Class105 Top5: 85.91%
[ Sun Jul  7 10:11:44 2024 ] 	Class106 Top5: 81.39%
[ Sun Jul  7 10:11:44 2024 ] 	Class107 Top5: 86.81%
[ Sun Jul  7 10:11:44 2024 ] 	Class108 Top5: 94.62%
[ Sun Jul  7 10:11:44 2024 ] 	Class109 Top5: 95.65%
[ Sun Jul  7 10:11:44 2024 ] 	Class110 Top5: 90.26%
[ Sun Jul  7 10:11:44 2024 ] 	Class111 Top5: 94.26%
[ Sun Jul  7 10:11:44 2024 ] 	Class112 Top5: 98.26%
[ Sun Jul  7 10:11:44 2024 ] 	Class113 Top5: 98.26%
[ Sun Jul  7 10:11:44 2024 ] 	Class114 Top5: 97.57%
[ Sun Jul  7 10:11:44 2024 ] 	Class115 Top5: 98.61%
[ Sun Jul  7 10:11:44 2024 ] 	Class116 Top5: 99.13%
[ Sun Jul  7 10:11:44 2024 ] 	Class117 Top5: 94.96%
[ Sun Jul  7 10:11:44 2024 ] 	Class118 Top5: 97.04%
[ Sun Jul  7 10:11:44 2024 ] 	Class119 Top5: 97.04%
[ Sun Jul  7 10:11:44 2024 ] 	Class120 Top5: 97.40%
[ Sun Jul  7 10:11:44 2024 ] 	Top5: 94.12%
[ Sun Jul  7 10:11:44 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': False, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xsub/train_joint_120.npy', 'label_path': 'new_data_processed/xsub/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xsub/val_joint_120.npy', 'label_path': 'new_data_processed/xsub/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': True, 'only_attention': True, 'tcn_attention': False, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': 'prova20/epoch10_model_old.pt', 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': True, 'scheduler': 1, 'base_lr': 1e-05, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Sun Jul  7 10:11:44 2024 ] Training epoch: 1
[ Sun Jul  7 10:11:45 2024 ] 	Batch(0/7879) done. Loss: 0.1485  lr:0.000010
[ Sun Jul  7 10:12:02 2024 ] 	Batch(100/7879) done. Loss: 0.0222  lr:0.000010
[ Sun Jul  7 10:12:19 2024 ] 	Batch(200/7879) done. Loss: 0.0365  lr:0.000010
[ Sun Jul  7 10:12:37 2024 ] 	Batch(300/7879) done. Loss: 0.0866  lr:0.000010
[ Sun Jul  7 10:12:55 2024 ] 	Batch(400/7879) done. Loss: 0.1131  lr:0.000010
[ Sun Jul  7 10:13:12 2024 ] 
Training: Epoch [0/120], Step [499], Loss: 0.018018852919340134, Training Accuracy: 97.725
[ Sun Jul  7 10:13:12 2024 ] 	Batch(500/7879) done. Loss: 0.0143  lr:0.000010
[ Sun Jul  7 10:13:29 2024 ] 	Batch(600/7879) done. Loss: 0.1848  lr:0.000010
[ Sun Jul  7 10:13:47 2024 ] 	Batch(700/7879) done. Loss: 0.0067  lr:0.000010
