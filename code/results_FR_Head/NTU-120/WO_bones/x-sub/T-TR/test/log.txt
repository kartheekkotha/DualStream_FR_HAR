[ Sun Jul  7 10:39:34 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xsub/train_joint_120.npy', 'label_path': 'new_data_processed/xsub/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xsub/val_joint_120.npy', 'label_path': 'new_data_processed/xsub/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': False, 'only_attention': True, 'tcn_attention': True, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': False, 'scheduler': 1, 'base_lr': 0.01, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Sun Jul  7 10:39:34 2024 ] Training epoch: 1
[ Sun Jul  7 10:39:36 2024 ] 	Batch(0/7879) done. Loss: 9.3882  lr:0.010000
[ Sun Jul  7 10:39:58 2024 ] 	Batch(100/7879) done. Loss: 4.6860  lr:0.010000
[ Sun Jul  7 10:40:21 2024 ] 	Batch(200/7879) done. Loss: 5.2039  lr:0.010000
[ Sun Jul  7 10:40:43 2024 ] 	Batch(300/7879) done. Loss: 5.1087  lr:0.010000
[ Sun Jul  7 10:41:06 2024 ] 	Batch(400/7879) done. Loss: 4.0468  lr:0.010000
[ Sun Jul  7 10:41:28 2024 ] 
Training: Epoch [0/120], Step [499], Loss: 4.210439682006836, Training Accuracy: 3.05
[ Sun Jul  7 10:41:28 2024 ] 	Batch(500/7879) done. Loss: 4.8171  lr:0.010000
[ Sun Jul  7 10:41:51 2024 ] 	Batch(600/7879) done. Loss: 4.3878  lr:0.010000
[ Sun Jul  7 10:42:13 2024 ] 	Batch(700/7879) done. Loss: 3.9619  lr:0.010000
[ Sun Jul  7 10:42:36 2024 ] 	Batch(800/7879) done. Loss: 3.8847  lr:0.010000
[ Sun Jul  7 10:42:59 2024 ] 	Batch(900/7879) done. Loss: 4.4889  lr:0.010000
[ Sun Jul  7 10:43:22 2024 ] 
Training: Epoch [0/120], Step [999], Loss: 3.311349630355835, Training Accuracy: 4.2625
[ Sun Jul  7 10:43:22 2024 ] 	Batch(1000/7879) done. Loss: 4.1079  lr:0.010000
[ Sun Jul  7 10:43:45 2024 ] 	Batch(1100/7879) done. Loss: 4.4352  lr:0.010000
[ Sun Jul  7 10:44:07 2024 ] 	Batch(1200/7879) done. Loss: 3.3178  lr:0.010000
[ Sun Jul  7 10:44:30 2024 ] 	Batch(1300/7879) done. Loss: 3.4179  lr:0.010000
[ Sun Jul  7 10:44:52 2024 ] 	Batch(1400/7879) done. Loss: 4.0822  lr:0.010000
[ Sun Jul  7 10:45:15 2024 ] 
Training: Epoch [0/120], Step [1499], Loss: 3.8909523487091064, Training Accuracy: 5.375
[ Sun Jul  7 10:45:15 2024 ] 	Batch(1500/7879) done. Loss: 3.8325  lr:0.010000
[ Sun Jul  7 10:45:38 2024 ] 	Batch(1600/7879) done. Loss: 3.5226  lr:0.010000
[ Sun Jul  7 10:46:01 2024 ] 	Batch(1700/7879) done. Loss: 3.4852  lr:0.010000
[ Sun Jul  7 10:46:24 2024 ] 	Batch(1800/7879) done. Loss: 3.9351  lr:0.010000
[ Sun Jul  7 10:46:46 2024 ] 	Batch(1900/7879) done. Loss: 4.1342  lr:0.010000
[ Sun Jul  7 10:47:09 2024 ] 
Training: Epoch [0/120], Step [1999], Loss: 4.148006439208984, Training Accuracy: 6.5625
[ Sun Jul  7 10:47:09 2024 ] 	Batch(2000/7879) done. Loss: 3.1837  lr:0.010000
[ Sun Jul  7 10:47:31 2024 ] 	Batch(2100/7879) done. Loss: 4.7763  lr:0.010000
[ Sun Jul  7 10:47:54 2024 ] 	Batch(2200/7879) done. Loss: 3.2407  lr:0.010000
[ Sun Jul  7 10:48:16 2024 ] 	Batch(2300/7879) done. Loss: 4.2107  lr:0.010000
[ Sun Jul  7 10:48:39 2024 ] 	Batch(2400/7879) done. Loss: 3.9711  lr:0.010000
[ Sun Jul  7 10:49:01 2024 ] 
Training: Epoch [0/120], Step [2499], Loss: 3.6401681900024414, Training Accuracy: 7.779999999999999
[ Sun Jul  7 10:49:02 2024 ] 	Batch(2500/7879) done. Loss: 2.9732  lr:0.010000
[ Sun Jul  7 10:49:25 2024 ] 	Batch(2600/7879) done. Loss: 2.5821  lr:0.010000
[ Sun Jul  7 10:49:48 2024 ] 	Batch(2700/7879) done. Loss: 3.4763  lr:0.010000
[ Sun Jul  7 10:50:11 2024 ] 	Batch(2800/7879) done. Loss: 2.9382  lr:0.010000
[ Sun Jul  7 10:50:33 2024 ] 	Batch(2900/7879) done. Loss: 2.6834  lr:0.010000
[ Sun Jul  7 10:50:56 2024 ] 
Training: Epoch [0/120], Step [2999], Loss: 3.4411239624023438, Training Accuracy: 8.983333333333334
[ Sun Jul  7 10:50:56 2024 ] 	Batch(3000/7879) done. Loss: 3.3842  lr:0.010000
[ Sun Jul  7 10:51:18 2024 ] 	Batch(3100/7879) done. Loss: 3.3582  lr:0.010000
[ Sun Jul  7 10:51:41 2024 ] 	Batch(3200/7879) done. Loss: 4.4247  lr:0.010000
[ Sun Jul  7 10:52:03 2024 ] 	Batch(3300/7879) done. Loss: 3.7203  lr:0.010000
[ Sun Jul  7 10:52:25 2024 ] 	Batch(3400/7879) done. Loss: 4.1307  lr:0.010000
[ Sun Jul  7 10:52:48 2024 ] 
Training: Epoch [0/120], Step [3499], Loss: 4.382144451141357, Training Accuracy: 10.060714285714285
[ Sun Jul  7 10:52:48 2024 ] 	Batch(3500/7879) done. Loss: 3.1058  lr:0.010000
[ Sun Jul  7 10:53:10 2024 ] 	Batch(3600/7879) done. Loss: 3.4040  lr:0.010000
[ Sun Jul  7 10:53:33 2024 ] 	Batch(3700/7879) done. Loss: 3.6961  lr:0.010000
[ Sun Jul  7 10:53:57 2024 ] 	Batch(3800/7879) done. Loss: 3.7973  lr:0.010000
[ Sun Jul  7 10:54:20 2024 ] 	Batch(3900/7879) done. Loss: 2.2038  lr:0.010000
[ Sun Jul  7 10:54:42 2024 ] 
Training: Epoch [0/120], Step [3999], Loss: 3.5179038047790527, Training Accuracy: 11.109375
[ Sun Jul  7 10:54:43 2024 ] 	Batch(4000/7879) done. Loss: 3.1636  lr:0.010000
[ Sun Jul  7 10:55:05 2024 ] 	Batch(4100/7879) done. Loss: 2.8503  lr:0.010000
[ Sun Jul  7 10:55:28 2024 ] 	Batch(4200/7879) done. Loss: 2.7539  lr:0.010000
[ Sun Jul  7 10:55:50 2024 ] 	Batch(4300/7879) done. Loss: 3.3190  lr:0.010000
[ Sun Jul  7 10:56:12 2024 ] 	Batch(4400/7879) done. Loss: 4.0211  lr:0.010000
[ Sun Jul  7 10:56:35 2024 ] 
Training: Epoch [0/120], Step [4499], Loss: 4.527373790740967, Training Accuracy: 12.136111111111111
[ Sun Jul  7 10:56:36 2024 ] 	Batch(4500/7879) done. Loss: 2.4568  lr:0.010000
[ Sun Jul  7 10:56:59 2024 ] 	Batch(4600/7879) done. Loss: 2.9022  lr:0.010000
[ Sun Jul  7 10:57:22 2024 ] 	Batch(4700/7879) done. Loss: 2.6096  lr:0.010000
[ Sun Jul  7 10:57:45 2024 ] 	Batch(4800/7879) done. Loss: 3.0190  lr:0.010000
[ Sun Jul  7 10:58:08 2024 ] 	Batch(4900/7879) done. Loss: 3.1656  lr:0.010000
[ Sun Jul  7 10:58:31 2024 ] 
Training: Epoch [0/120], Step [4999], Loss: 3.2144956588745117, Training Accuracy: 13.18
[ Sun Jul  7 10:58:31 2024 ] 	Batch(5000/7879) done. Loss: 3.9048  lr:0.010000
[ Sun Jul  7 10:58:54 2024 ] 	Batch(5100/7879) done. Loss: 2.8785  lr:0.010000
[ Sun Jul  7 10:59:17 2024 ] 	Batch(5200/7879) done. Loss: 3.3425  lr:0.010000
[ Sun Jul  7 10:59:40 2024 ] 	Batch(5300/7879) done. Loss: 3.3472  lr:0.010000
[ Sun Jul  7 11:00:03 2024 ] 	Batch(5400/7879) done. Loss: 2.1678  lr:0.010000
[ Sun Jul  7 11:00:26 2024 ] 
Training: Epoch [0/120], Step [5499], Loss: 3.3834712505340576, Training Accuracy: 14.286363636363635
[ Sun Jul  7 11:00:26 2024 ] 	Batch(5500/7879) done. Loss: 2.4831  lr:0.010000
[ Sun Jul  7 11:00:49 2024 ] 	Batch(5600/7879) done. Loss: 3.1127  lr:0.010000
[ Sun Jul  7 11:01:11 2024 ] 	Batch(5700/7879) done. Loss: 3.3127  lr:0.010000
[ Sun Jul  7 11:01:33 2024 ] 	Batch(5800/7879) done. Loss: 3.0908  lr:0.010000
[ Sun Jul  7 11:01:56 2024 ] 	Batch(5900/7879) done. Loss: 2.3957  lr:0.010000
[ Sun Jul  7 11:02:18 2024 ] 
Training: Epoch [0/120], Step [5999], Loss: 3.0783329010009766, Training Accuracy: 15.214583333333334
[ Sun Jul  7 11:02:18 2024 ] 	Batch(6000/7879) done. Loss: 3.2248  lr:0.010000
[ Sun Jul  7 11:02:41 2024 ] 	Batch(6100/7879) done. Loss: 2.4226  lr:0.010000
[ Sun Jul  7 11:03:03 2024 ] 	Batch(6200/7879) done. Loss: 3.1262  lr:0.010000
[ Sun Jul  7 11:03:25 2024 ] 	Batch(6300/7879) done. Loss: 1.8661  lr:0.010000
[ Sun Jul  7 11:03:48 2024 ] 	Batch(6400/7879) done. Loss: 2.9648  lr:0.010000
[ Sun Jul  7 11:04:10 2024 ] 
Training: Epoch [0/120], Step [6499], Loss: 2.4830188751220703, Training Accuracy: 16.26153846153846
[ Sun Jul  7 11:04:10 2024 ] 	Batch(6500/7879) done. Loss: 2.4451  lr:0.010000
[ Sun Jul  7 11:04:32 2024 ] 	Batch(6600/7879) done. Loss: 2.4115  lr:0.010000
[ Sun Jul  7 11:04:55 2024 ] 	Batch(6700/7879) done. Loss: 2.6637  lr:0.010000
[ Sun Jul  7 11:05:17 2024 ] 	Batch(6800/7879) done. Loss: 2.6567  lr:0.010000
[ Sun Jul  7 11:05:39 2024 ] 	Batch(6900/7879) done. Loss: 3.0241  lr:0.010000
[ Sun Jul  7 11:06:01 2024 ] 
Training: Epoch [0/120], Step [6999], Loss: 1.8797086477279663, Training Accuracy: 17.110714285714284
[ Sun Jul  7 11:06:02 2024 ] 	Batch(7000/7879) done. Loss: 1.8835  lr:0.010000
[ Sun Jul  7 11:06:24 2024 ] 	Batch(7100/7879) done. Loss: 2.7813  lr:0.010000
[ Sun Jul  7 11:06:47 2024 ] 	Batch(7200/7879) done. Loss: 2.0311  lr:0.010000
[ Sun Jul  7 11:07:10 2024 ] 	Batch(7300/7879) done. Loss: 2.4727  lr:0.010000
[ Sun Jul  7 11:07:32 2024 ] 	Batch(7400/7879) done. Loss: 3.6574  lr:0.010000
[ Sun Jul  7 11:07:55 2024 ] 
Training: Epoch [0/120], Step [7499], Loss: 1.3962920904159546, Training Accuracy: 18.071666666666665
[ Sun Jul  7 11:07:55 2024 ] 	Batch(7500/7879) done. Loss: 3.6419  lr:0.010000
[ Sun Jul  7 11:08:17 2024 ] 	Batch(7600/7879) done. Loss: 2.5108  lr:0.010000
[ Sun Jul  7 11:08:39 2024 ] 	Batch(7700/7879) done. Loss: 2.2098  lr:0.010000
[ Sun Jul  7 11:09:01 2024 ] 	Batch(7800/7879) done. Loss: 2.8342  lr:0.010000
[ Sun Jul  7 11:09:18 2024 ] 	Mean training loss: 3.3012.
[ Sun Jul  7 11:09:18 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul  7 11:09:18 2024 ] Training epoch: 2
[ Sun Jul  7 11:09:19 2024 ] 	Batch(0/7879) done. Loss: 3.3578  lr:0.010000
[ Sun Jul  7 11:09:42 2024 ] 	Batch(100/7879) done. Loss: 3.5675  lr:0.010000
[ Sun Jul  7 11:10:05 2024 ] 	Batch(200/7879) done. Loss: 2.4377  lr:0.010000
[ Sun Jul  7 11:10:28 2024 ] 	Batch(300/7879) done. Loss: 3.5030  lr:0.010000
[ Sun Jul  7 11:10:52 2024 ] 	Batch(400/7879) done. Loss: 3.3185  lr:0.010000
[ Sun Jul  7 11:11:15 2024 ] 
Training: Epoch [1/120], Step [499], Loss: 3.0203678607940674, Training Accuracy: 32.6
[ Sun Jul  7 11:11:15 2024 ] 	Batch(500/7879) done. Loss: 2.1126  lr:0.010000
[ Sun Jul  7 11:11:39 2024 ] 	Batch(600/7879) done. Loss: 2.1578  lr:0.010000
[ Sun Jul  7 11:12:02 2024 ] 	Batch(700/7879) done. Loss: 3.6020  lr:0.010000
[ Sun Jul  7 11:12:25 2024 ] 	Batch(800/7879) done. Loss: 2.9997  lr:0.010000
[ Sun Jul  7 11:12:48 2024 ] 	Batch(900/7879) done. Loss: 2.3207  lr:0.010000
[ Sun Jul  7 11:13:11 2024 ] 
Training: Epoch [1/120], Step [999], Loss: 2.6915526390075684, Training Accuracy: 33.8125
[ Sun Jul  7 11:13:12 2024 ] 	Batch(1000/7879) done. Loss: 1.6468  lr:0.010000
[ Sun Jul  7 11:13:35 2024 ] 	Batch(1100/7879) done. Loss: 2.0025  lr:0.010000
[ Sun Jul  7 11:13:58 2024 ] 	Batch(1200/7879) done. Loss: 3.3792  lr:0.010000
[ Sun Jul  7 11:14:21 2024 ] 	Batch(1300/7879) done. Loss: 1.5043  lr:0.010000
[ Sun Jul  7 11:14:45 2024 ] 	Batch(1400/7879) done. Loss: 4.4100  lr:0.010000
[ Sun Jul  7 11:15:08 2024 ] 
Training: Epoch [1/120], Step [1499], Loss: 2.167865037918091, Training Accuracy: 33.78333333333333
[ Sun Jul  7 11:15:08 2024 ] 	Batch(1500/7879) done. Loss: 2.3552  lr:0.010000
[ Sun Jul  7 11:15:32 2024 ] 	Batch(1600/7879) done. Loss: 1.9366  lr:0.010000
[ Sun Jul  7 11:15:55 2024 ] 	Batch(1700/7879) done. Loss: 3.6988  lr:0.010000
[ Sun Jul  7 11:16:19 2024 ] 	Batch(1800/7879) done. Loss: 3.3674  lr:0.010000
[ Sun Jul  7 11:16:42 2024 ] 	Batch(1900/7879) done. Loss: 2.8257  lr:0.010000
[ Sun Jul  7 11:17:05 2024 ] 
Training: Epoch [1/120], Step [1999], Loss: 2.662693977355957, Training Accuracy: 34.30625
[ Sun Jul  7 11:17:06 2024 ] 	Batch(2000/7879) done. Loss: 2.7672  lr:0.010000
[ Sun Jul  7 11:17:29 2024 ] 	Batch(2100/7879) done. Loss: 2.4172  lr:0.010000
[ Sun Jul  7 11:17:53 2024 ] 	Batch(2200/7879) done. Loss: 2.2077  lr:0.010000
[ Sun Jul  7 11:18:16 2024 ] 	Batch(2300/7879) done. Loss: 3.2392  lr:0.010000
[ Sun Jul  7 11:18:40 2024 ] 	Batch(2400/7879) done. Loss: 3.1854  lr:0.010000
[ Sun Jul  7 11:19:03 2024 ] 
Training: Epoch [1/120], Step [2499], Loss: 2.86753249168396, Training Accuracy: 35.045
[ Sun Jul  7 11:19:03 2024 ] 	Batch(2500/7879) done. Loss: 2.8523  lr:0.010000
[ Sun Jul  7 11:19:27 2024 ] 	Batch(2600/7879) done. Loss: 2.3007  lr:0.010000
[ Sun Jul  7 11:19:50 2024 ] 	Batch(2700/7879) done. Loss: 1.6747  lr:0.010000
[ Sun Jul  7 11:20:14 2024 ] 	Batch(2800/7879) done. Loss: 3.2073  lr:0.010000
[ Sun Jul  7 11:20:37 2024 ] 	Batch(2900/7879) done. Loss: 2.8982  lr:0.010000
[ Sun Jul  7 11:21:00 2024 ] 
Training: Epoch [1/120], Step [2999], Loss: 2.6647348403930664, Training Accuracy: 35.875
[ Sun Jul  7 11:21:01 2024 ] 	Batch(3000/7879) done. Loss: 2.6375  lr:0.010000
[ Sun Jul  7 11:21:24 2024 ] 	Batch(3100/7879) done. Loss: 2.0934  lr:0.010000
[ Sun Jul  7 11:21:48 2024 ] 	Batch(3200/7879) done. Loss: 2.5400  lr:0.010000
[ Sun Jul  7 11:22:11 2024 ] 	Batch(3300/7879) done. Loss: 3.1117  lr:0.010000
[ Sun Jul  7 11:22:35 2024 ] 	Batch(3400/7879) done. Loss: 2.1425  lr:0.010000
[ Sun Jul  7 11:22:58 2024 ] 
Training: Epoch [1/120], Step [3499], Loss: 1.176852822303772, Training Accuracy: 36.19285714285714
[ Sun Jul  7 11:22:58 2024 ] 	Batch(3500/7879) done. Loss: 1.4829  lr:0.010000
[ Sun Jul  7 11:23:21 2024 ] 	Batch(3600/7879) done. Loss: 3.6468  lr:0.010000
[ Sun Jul  7 11:23:44 2024 ] 	Batch(3700/7879) done. Loss: 2.6394  lr:0.010000
[ Sun Jul  7 11:24:07 2024 ] 	Batch(3800/7879) done. Loss: 2.9860  lr:0.010000
[ Sun Jul  7 11:24:30 2024 ] 	Batch(3900/7879) done. Loss: 2.4390  lr:0.010000
[ Sun Jul  7 11:24:52 2024 ] 
Training: Epoch [1/120], Step [3999], Loss: 2.262240171432495, Training Accuracy: 36.61875
[ Sun Jul  7 11:24:53 2024 ] 	Batch(4000/7879) done. Loss: 2.2265  lr:0.010000
[ Sun Jul  7 11:25:16 2024 ] 	Batch(4100/7879) done. Loss: 2.1534  lr:0.010000
[ Sun Jul  7 11:25:38 2024 ] 	Batch(4200/7879) done. Loss: 2.7034  lr:0.010000
[ Sun Jul  7 11:26:01 2024 ] 	Batch(4300/7879) done. Loss: 2.2460  lr:0.010000
[ Sun Jul  7 11:26:24 2024 ] 	Batch(4400/7879) done. Loss: 2.1028  lr:0.010000
[ Sun Jul  7 11:26:47 2024 ] 
Training: Epoch [1/120], Step [4499], Loss: 2.0156898498535156, Training Accuracy: 36.93055555555556
[ Sun Jul  7 11:26:47 2024 ] 	Batch(4500/7879) done. Loss: 2.5415  lr:0.010000
[ Sun Jul  7 11:27:11 2024 ] 	Batch(4600/7879) done. Loss: 2.5313  lr:0.010000
[ Sun Jul  7 11:27:34 2024 ] 	Batch(4700/7879) done. Loss: 2.3266  lr:0.010000
[ Sun Jul  7 11:27:58 2024 ] 	Batch(4800/7879) done. Loss: 2.2875  lr:0.010000
[ Sun Jul  7 11:28:21 2024 ] 	Batch(4900/7879) done. Loss: 1.1075  lr:0.010000
[ Sun Jul  7 11:28:45 2024 ] 
Training: Epoch [1/120], Step [4999], Loss: 2.7781033515930176, Training Accuracy: 37.5325
[ Sun Jul  7 11:28:45 2024 ] 	Batch(5000/7879) done. Loss: 1.8229  lr:0.010000
[ Sun Jul  7 11:29:08 2024 ] 	Batch(5100/7879) done. Loss: 2.9624  lr:0.010000
[ Sun Jul  7 11:29:31 2024 ] 	Batch(5200/7879) done. Loss: 2.5290  lr:0.010000
[ Sun Jul  7 11:29:54 2024 ] 	Batch(5300/7879) done. Loss: 1.3683  lr:0.010000
[ Sun Jul  7 11:30:17 2024 ] 	Batch(5400/7879) done. Loss: 2.1355  lr:0.010000
[ Sun Jul  7 11:30:39 2024 ] 
Training: Epoch [1/120], Step [5499], Loss: 2.445974349975586, Training Accuracy: 38.02045454545455
[ Sun Jul  7 11:30:39 2024 ] 	Batch(5500/7879) done. Loss: 2.2200  lr:0.010000
[ Sun Jul  7 11:31:02 2024 ] 	Batch(5600/7879) done. Loss: 2.1477  lr:0.010000
[ Sun Jul  7 11:31:25 2024 ] 	Batch(5700/7879) done. Loss: 2.5474  lr:0.010000
[ Sun Jul  7 11:31:47 2024 ] 	Batch(5800/7879) done. Loss: 1.0652  lr:0.010000
[ Sun Jul  7 11:32:10 2024 ] 	Batch(5900/7879) done. Loss: 1.7291  lr:0.010000
[ Sun Jul  7 11:32:33 2024 ] 
Training: Epoch [1/120], Step [5999], Loss: 1.8720828294754028, Training Accuracy: 38.4
[ Sun Jul  7 11:32:33 2024 ] 	Batch(6000/7879) done. Loss: 1.5588  lr:0.010000
[ Sun Jul  7 11:32:56 2024 ] 	Batch(6100/7879) done. Loss: 2.0326  lr:0.010000
[ Sun Jul  7 11:33:18 2024 ] 	Batch(6200/7879) done. Loss: 2.5413  lr:0.010000
[ Sun Jul  7 11:33:41 2024 ] 	Batch(6300/7879) done. Loss: 2.3686  lr:0.010000
[ Sun Jul  7 11:34:04 2024 ] 	Batch(6400/7879) done. Loss: 1.8375  lr:0.010000
[ Sun Jul  7 11:34:26 2024 ] 
Training: Epoch [1/120], Step [6499], Loss: 1.5666940212249756, Training Accuracy: 38.75
[ Sun Jul  7 11:34:27 2024 ] 	Batch(6500/7879) done. Loss: 2.3141  lr:0.010000
[ Sun Jul  7 11:34:49 2024 ] 	Batch(6600/7879) done. Loss: 1.4035  lr:0.010000
[ Sun Jul  7 11:35:12 2024 ] 	Batch(6700/7879) done. Loss: 2.4963  lr:0.010000
[ Sun Jul  7 11:35:35 2024 ] 	Batch(6800/7879) done. Loss: 1.4688  lr:0.010000
[ Sun Jul  7 11:35:58 2024 ] 	Batch(6900/7879) done. Loss: 2.7113  lr:0.010000
[ Sun Jul  7 11:36:22 2024 ] 
Training: Epoch [1/120], Step [6999], Loss: 2.6164166927337646, Training Accuracy: 39.15892857142857
[ Sun Jul  7 11:36:22 2024 ] 	Batch(7000/7879) done. Loss: 1.2398  lr:0.010000
[ Sun Jul  7 11:36:45 2024 ] 	Batch(7100/7879) done. Loss: 1.6641  lr:0.010000
[ Sun Jul  7 11:37:08 2024 ] 	Batch(7200/7879) done. Loss: 1.7701  lr:0.010000
[ Sun Jul  7 11:37:30 2024 ] 	Batch(7300/7879) done. Loss: 1.9869  lr:0.010000
[ Sun Jul  7 11:37:53 2024 ] 	Batch(7400/7879) done. Loss: 2.0481  lr:0.010000
[ Sun Jul  7 11:38:15 2024 ] 
Training: Epoch [1/120], Step [7499], Loss: 1.4906786680221558, Training Accuracy: 39.50666666666667
[ Sun Jul  7 11:38:16 2024 ] 	Batch(7500/7879) done. Loss: 1.3264  lr:0.010000
[ Sun Jul  7 11:38:38 2024 ] 	Batch(7600/7879) done. Loss: 1.6686  lr:0.010000
[ Sun Jul  7 11:39:02 2024 ] 	Batch(7700/7879) done. Loss: 3.6816  lr:0.010000
[ Sun Jul  7 11:39:25 2024 ] 	Batch(7800/7879) done. Loss: 2.9236  lr:0.010000
[ Sun Jul  7 11:39:44 2024 ] 	Mean training loss: 2.2135.
[ Sun Jul  7 11:39:44 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul  7 11:39:44 2024 ] Training epoch: 3
[ Sun Jul  7 11:39:44 2024 ] 	Batch(0/7879) done. Loss: 1.4061  lr:0.010000
[ Sun Jul  7 11:40:07 2024 ] 	Batch(100/7879) done. Loss: 2.8562  lr:0.010000
[ Sun Jul  7 11:40:30 2024 ] 	Batch(200/7879) done. Loss: 2.1722  lr:0.010000
[ Sun Jul  7 11:40:52 2024 ] 	Batch(300/7879) done. Loss: 2.0846  lr:0.010000
[ Sun Jul  7 11:41:15 2024 ] 	Batch(400/7879) done. Loss: 3.0060  lr:0.010000
[ Sun Jul  7 11:41:38 2024 ] 
Training: Epoch [2/120], Step [499], Loss: 1.543609857559204, Training Accuracy: 43.7
[ Sun Jul  7 11:41:38 2024 ] 	Batch(500/7879) done. Loss: 1.7719  lr:0.010000
[ Sun Jul  7 11:42:01 2024 ] 	Batch(600/7879) done. Loss: 1.8693  lr:0.010000
[ Sun Jul  7 11:42:23 2024 ] 	Batch(700/7879) done. Loss: 1.3787  lr:0.010000
[ Sun Jul  7 11:42:46 2024 ] 	Batch(800/7879) done. Loss: 1.0668  lr:0.010000
[ Sun Jul  7 11:43:09 2024 ] 	Batch(900/7879) done. Loss: 2.6630  lr:0.010000
[ Sun Jul  7 11:43:32 2024 ] 
Training: Epoch [2/120], Step [999], Loss: 1.6206789016723633, Training Accuracy: 45.375
[ Sun Jul  7 11:43:32 2024 ] 	Batch(1000/7879) done. Loss: 1.1034  lr:0.010000
[ Sun Jul  7 11:43:54 2024 ] 	Batch(1100/7879) done. Loss: 3.2879  lr:0.010000
[ Sun Jul  7 11:44:17 2024 ] 	Batch(1200/7879) done. Loss: 2.3482  lr:0.010000
[ Sun Jul  7 11:44:40 2024 ] 	Batch(1300/7879) done. Loss: 1.7788  lr:0.010000
[ Sun Jul  7 11:45:03 2024 ] 	Batch(1400/7879) done. Loss: 1.4635  lr:0.010000
[ Sun Jul  7 11:45:25 2024 ] 
Training: Epoch [2/120], Step [1499], Loss: 2.6088173389434814, Training Accuracy: 45.483333333333334
[ Sun Jul  7 11:45:26 2024 ] 	Batch(1500/7879) done. Loss: 2.1176  lr:0.010000
[ Sun Jul  7 11:45:49 2024 ] 	Batch(1600/7879) done. Loss: 2.5473  lr:0.010000
[ Sun Jul  7 11:46:11 2024 ] 	Batch(1700/7879) done. Loss: 1.2574  lr:0.010000
[ Sun Jul  7 11:46:34 2024 ] 	Batch(1800/7879) done. Loss: 2.1257  lr:0.010000
[ Sun Jul  7 11:46:57 2024 ] 	Batch(1900/7879) done. Loss: 1.3444  lr:0.010000
[ Sun Jul  7 11:47:19 2024 ] 
Training: Epoch [2/120], Step [1999], Loss: 1.1914784908294678, Training Accuracy: 45.9875
[ Sun Jul  7 11:47:20 2024 ] 	Batch(2000/7879) done. Loss: 1.5066  lr:0.010000
[ Sun Jul  7 11:47:42 2024 ] 	Batch(2100/7879) done. Loss: 1.8338  lr:0.010000
[ Sun Jul  7 11:48:05 2024 ] 	Batch(2200/7879) done. Loss: 1.8174  lr:0.010000
[ Sun Jul  7 11:48:28 2024 ] 	Batch(2300/7879) done. Loss: 1.6882  lr:0.010000
[ Sun Jul  7 11:48:51 2024 ] 	Batch(2400/7879) done. Loss: 2.6300  lr:0.010000
[ Sun Jul  7 11:49:14 2024 ] 
Training: Epoch [2/120], Step [2499], Loss: 1.5729122161865234, Training Accuracy: 46.265
[ Sun Jul  7 11:49:14 2024 ] 	Batch(2500/7879) done. Loss: 2.0310  lr:0.010000
[ Sun Jul  7 11:49:37 2024 ] 	Batch(2600/7879) done. Loss: 1.4679  lr:0.010000
[ Sun Jul  7 11:49:59 2024 ] 	Batch(2700/7879) done. Loss: 1.9414  lr:0.010000
[ Sun Jul  7 11:50:22 2024 ] 	Batch(2800/7879) done. Loss: 1.7595  lr:0.010000
[ Sun Jul  7 11:50:46 2024 ] 	Batch(2900/7879) done. Loss: 3.6893  lr:0.010000
[ Sun Jul  7 11:51:09 2024 ] 
Training: Epoch [2/120], Step [2999], Loss: 0.861681342124939, Training Accuracy: 46.71666666666667
[ Sun Jul  7 11:51:09 2024 ] 	Batch(3000/7879) done. Loss: 2.8485  lr:0.010000
[ Sun Jul  7 11:51:33 2024 ] 	Batch(3100/7879) done. Loss: 1.9381  lr:0.010000
[ Sun Jul  7 11:51:56 2024 ] 	Batch(3200/7879) done. Loss: 2.2978  lr:0.010000
[ Sun Jul  7 11:52:19 2024 ] 	Batch(3300/7879) done. Loss: 2.4782  lr:0.010000
[ Sun Jul  7 11:52:42 2024 ] 	Batch(3400/7879) done. Loss: 1.6563  lr:0.010000
[ Sun Jul  7 11:53:04 2024 ] 
Training: Epoch [2/120], Step [3499], Loss: 2.028892755508423, Training Accuracy: 46.82857142857143
[ Sun Jul  7 11:53:05 2024 ] 	Batch(3500/7879) done. Loss: 1.9985  lr:0.010000
[ Sun Jul  7 11:53:27 2024 ] 	Batch(3600/7879) done. Loss: 2.2009  lr:0.010000
[ Sun Jul  7 11:53:51 2024 ] 	Batch(3700/7879) done. Loss: 1.0578  lr:0.010000
[ Sun Jul  7 11:54:14 2024 ] 	Batch(3800/7879) done. Loss: 1.3599  lr:0.010000
[ Sun Jul  7 11:54:38 2024 ] 	Batch(3900/7879) done. Loss: 1.2224  lr:0.010000
[ Sun Jul  7 11:55:01 2024 ] 
Training: Epoch [2/120], Step [3999], Loss: 2.097555637359619, Training Accuracy: 47.003125000000004
[ Sun Jul  7 11:55:02 2024 ] 	Batch(4000/7879) done. Loss: 1.1445  lr:0.010000
[ Sun Jul  7 11:55:24 2024 ] 	Batch(4100/7879) done. Loss: 1.9452  lr:0.010000
[ Sun Jul  7 11:55:47 2024 ] 	Batch(4200/7879) done. Loss: 2.3670  lr:0.010000
[ Sun Jul  7 11:56:10 2024 ] 	Batch(4300/7879) done. Loss: 2.3606  lr:0.010000
[ Sun Jul  7 11:56:33 2024 ] 	Batch(4400/7879) done. Loss: 0.6801  lr:0.010000
[ Sun Jul  7 11:56:55 2024 ] 
Training: Epoch [2/120], Step [4499], Loss: 1.3886690139770508, Training Accuracy: 47.41388888888889
[ Sun Jul  7 11:56:56 2024 ] 	Batch(4500/7879) done. Loss: 2.1335  lr:0.010000
[ Sun Jul  7 11:57:18 2024 ] 	Batch(4600/7879) done. Loss: 1.6547  lr:0.010000
[ Sun Jul  7 11:57:41 2024 ] 	Batch(4700/7879) done. Loss: 1.2873  lr:0.010000
[ Sun Jul  7 11:58:04 2024 ] 	Batch(4800/7879) done. Loss: 2.1753  lr:0.010000
[ Sun Jul  7 11:58:27 2024 ] 	Batch(4900/7879) done. Loss: 2.3104  lr:0.010000
[ Sun Jul  7 11:58:49 2024 ] 
Training: Epoch [2/120], Step [4999], Loss: 1.9219553470611572, Training Accuracy: 47.65
[ Sun Jul  7 11:58:49 2024 ] 	Batch(5000/7879) done. Loss: 1.6426  lr:0.010000
[ Sun Jul  7 11:59:12 2024 ] 	Batch(5100/7879) done. Loss: 1.4740  lr:0.010000
[ Sun Jul  7 11:59:35 2024 ] 	Batch(5200/7879) done. Loss: 1.6534  lr:0.010000
[ Sun Jul  7 11:59:58 2024 ] 	Batch(5300/7879) done. Loss: 0.9685  lr:0.010000
[ Sun Jul  7 12:00:21 2024 ] 	Batch(5400/7879) done. Loss: 2.6517  lr:0.010000
[ Sun Jul  7 12:00:43 2024 ] 
Training: Epoch [2/120], Step [5499], Loss: 1.5958274602890015, Training Accuracy: 47.68181818181818
[ Sun Jul  7 12:00:43 2024 ] 	Batch(5500/7879) done. Loss: 1.3353  lr:0.010000
[ Sun Jul  7 12:01:06 2024 ] 	Batch(5600/7879) done. Loss: 2.1161  lr:0.010000
[ Sun Jul  7 12:01:29 2024 ] 	Batch(5700/7879) done. Loss: 0.7735  lr:0.010000
[ Sun Jul  7 12:01:52 2024 ] 	Batch(5800/7879) done. Loss: 2.3947  lr:0.010000
[ Sun Jul  7 12:02:15 2024 ] 	Batch(5900/7879) done. Loss: 1.6098  lr:0.010000
[ Sun Jul  7 12:02:37 2024 ] 
Training: Epoch [2/120], Step [5999], Loss: 0.9104048013687134, Training Accuracy: 48.06666666666667
[ Sun Jul  7 12:02:38 2024 ] 	Batch(6000/7879) done. Loss: 1.4539  lr:0.010000
[ Sun Jul  7 12:03:00 2024 ] 	Batch(6100/7879) done. Loss: 2.1608  lr:0.010000
[ Sun Jul  7 12:03:23 2024 ] 	Batch(6200/7879) done. Loss: 3.5458  lr:0.010000
[ Sun Jul  7 12:03:46 2024 ] 	Batch(6300/7879) done. Loss: 3.1941  lr:0.010000
[ Sun Jul  7 12:04:08 2024 ] 	Batch(6400/7879) done. Loss: 1.4010  lr:0.010000
[ Sun Jul  7 12:04:31 2024 ] 
Training: Epoch [2/120], Step [6499], Loss: 1.8847789764404297, Training Accuracy: 48.25
[ Sun Jul  7 12:04:31 2024 ] 	Batch(6500/7879) done. Loss: 1.0458  lr:0.010000
[ Sun Jul  7 12:04:54 2024 ] 	Batch(6600/7879) done. Loss: 1.5119  lr:0.010000
[ Sun Jul  7 12:05:16 2024 ] 	Batch(6700/7879) done. Loss: 1.5457  lr:0.010000
[ Sun Jul  7 12:05:39 2024 ] 	Batch(6800/7879) done. Loss: 2.6436  lr:0.010000
[ Sun Jul  7 12:06:02 2024 ] 	Batch(6900/7879) done. Loss: 1.9907  lr:0.010000
[ Sun Jul  7 12:06:25 2024 ] 
Training: Epoch [2/120], Step [6999], Loss: 1.5459413528442383, Training Accuracy: 48.40714285714286
[ Sun Jul  7 12:06:25 2024 ] 	Batch(7000/7879) done. Loss: 1.3318  lr:0.010000
[ Sun Jul  7 12:06:48 2024 ] 	Batch(7100/7879) done. Loss: 1.1062  lr:0.010000
[ Sun Jul  7 12:07:10 2024 ] 	Batch(7200/7879) done. Loss: 3.1653  lr:0.010000
[ Sun Jul  7 12:07:33 2024 ] 	Batch(7300/7879) done. Loss: 1.4341  lr:0.010000
[ Sun Jul  7 12:07:56 2024 ] 	Batch(7400/7879) done. Loss: 1.7890  lr:0.010000
[ Sun Jul  7 12:08:18 2024 ] 
Training: Epoch [2/120], Step [7499], Loss: 2.1595566272735596, Training Accuracy: 48.626666666666665
[ Sun Jul  7 12:08:19 2024 ] 	Batch(7500/7879) done. Loss: 1.0392  lr:0.010000
[ Sun Jul  7 12:08:41 2024 ] 	Batch(7600/7879) done. Loss: 1.3172  lr:0.010000
[ Sun Jul  7 12:09:05 2024 ] 	Batch(7700/7879) done. Loss: 2.2006  lr:0.010000
[ Sun Jul  7 12:09:28 2024 ] 	Batch(7800/7879) done. Loss: 1.3460  lr:0.010000
[ Sun Jul  7 12:09:47 2024 ] 	Mean training loss: 1.8578.
[ Sun Jul  7 12:09:47 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 12:09:47 2024 ] Training epoch: 4
[ Sun Jul  7 12:09:47 2024 ] 	Batch(0/7879) done. Loss: 1.3197  lr:0.010000
[ Sun Jul  7 12:10:10 2024 ] 	Batch(100/7879) done. Loss: 1.3420  lr:0.010000
[ Sun Jul  7 12:10:33 2024 ] 	Batch(200/7879) done. Loss: 1.4485  lr:0.010000
[ Sun Jul  7 12:10:55 2024 ] 	Batch(300/7879) done. Loss: 0.7209  lr:0.010000
[ Sun Jul  7 12:11:18 2024 ] 	Batch(400/7879) done. Loss: 2.2723  lr:0.010000
[ Sun Jul  7 12:11:41 2024 ] 
Training: Epoch [3/120], Step [499], Loss: 1.4512715339660645, Training Accuracy: 52.675000000000004
[ Sun Jul  7 12:11:41 2024 ] 	Batch(500/7879) done. Loss: 1.7727  lr:0.010000
[ Sun Jul  7 12:12:04 2024 ] 	Batch(600/7879) done. Loss: 1.4017  lr:0.010000
[ Sun Jul  7 12:12:26 2024 ] 	Batch(700/7879) done. Loss: 2.3057  lr:0.010000
[ Sun Jul  7 12:12:49 2024 ] 	Batch(800/7879) done. Loss: 1.7185  lr:0.010000
[ Sun Jul  7 12:13:12 2024 ] 	Batch(900/7879) done. Loss: 2.2748  lr:0.010000
[ Sun Jul  7 12:13:35 2024 ] 
Training: Epoch [3/120], Step [999], Loss: 1.787721872329712, Training Accuracy: 52.9875
[ Sun Jul  7 12:13:35 2024 ] 	Batch(1000/7879) done. Loss: 1.6254  lr:0.010000
[ Sun Jul  7 12:13:57 2024 ] 	Batch(1100/7879) done. Loss: 1.5912  lr:0.010000
[ Sun Jul  7 12:14:20 2024 ] 	Batch(1200/7879) done. Loss: 1.7443  lr:0.010000
[ Sun Jul  7 12:14:43 2024 ] 	Batch(1300/7879) done. Loss: 0.6176  lr:0.010000
[ Sun Jul  7 12:15:06 2024 ] 	Batch(1400/7879) done. Loss: 1.0533  lr:0.010000
[ Sun Jul  7 12:15:28 2024 ] 
Training: Epoch [3/120], Step [1499], Loss: 1.51626455783844, Training Accuracy: 52.84166666666666
[ Sun Jul  7 12:15:28 2024 ] 	Batch(1500/7879) done. Loss: 1.0437  lr:0.010000
[ Sun Jul  7 12:15:51 2024 ] 	Batch(1600/7879) done. Loss: 1.6083  lr:0.010000
[ Sun Jul  7 12:16:14 2024 ] 	Batch(1700/7879) done. Loss: 1.5566  lr:0.010000
[ Sun Jul  7 12:16:37 2024 ] 	Batch(1800/7879) done. Loss: 1.3801  lr:0.010000
[ Sun Jul  7 12:16:59 2024 ] 	Batch(1900/7879) done. Loss: 2.0760  lr:0.010000
[ Sun Jul  7 12:17:22 2024 ] 
Training: Epoch [3/120], Step [1999], Loss: 1.843409538269043, Training Accuracy: 52.81875
[ Sun Jul  7 12:17:22 2024 ] 	Batch(2000/7879) done. Loss: 1.2681  lr:0.010000
[ Sun Jul  7 12:17:45 2024 ] 	Batch(2100/7879) done. Loss: 1.5195  lr:0.010000
[ Sun Jul  7 12:18:08 2024 ] 	Batch(2200/7879) done. Loss: 0.3396  lr:0.010000
[ Sun Jul  7 12:18:30 2024 ] 	Batch(2300/7879) done. Loss: 0.5640  lr:0.010000
[ Sun Jul  7 12:18:53 2024 ] 	Batch(2400/7879) done. Loss: 2.1569  lr:0.010000
[ Sun Jul  7 12:19:16 2024 ] 
Training: Epoch [3/120], Step [2499], Loss: 1.5222837924957275, Training Accuracy: 52.754999999999995
[ Sun Jul  7 12:19:16 2024 ] 	Batch(2500/7879) done. Loss: 1.4416  lr:0.010000
[ Sun Jul  7 12:19:39 2024 ] 	Batch(2600/7879) done. Loss: 2.0150  lr:0.010000
[ Sun Jul  7 12:20:01 2024 ] 	Batch(2700/7879) done. Loss: 1.6471  lr:0.010000
[ Sun Jul  7 12:20:24 2024 ] 	Batch(2800/7879) done. Loss: 2.4517  lr:0.010000
[ Sun Jul  7 12:20:47 2024 ] 	Batch(2900/7879) done. Loss: 2.0004  lr:0.010000
[ Sun Jul  7 12:21:10 2024 ] 
Training: Epoch [3/120], Step [2999], Loss: 2.163606882095337, Training Accuracy: 52.758333333333326
[ Sun Jul  7 12:21:10 2024 ] 	Batch(3000/7879) done. Loss: 1.9982  lr:0.010000
[ Sun Jul  7 12:21:32 2024 ] 	Batch(3100/7879) done. Loss: 0.6252  lr:0.010000
[ Sun Jul  7 12:21:56 2024 ] 	Batch(3200/7879) done. Loss: 0.9161  lr:0.010000
[ Sun Jul  7 12:22:18 2024 ] 	Batch(3300/7879) done. Loss: 2.0673  lr:0.010000
[ Sun Jul  7 12:22:41 2024 ] 	Batch(3400/7879) done. Loss: 1.3757  lr:0.010000
[ Sun Jul  7 12:23:04 2024 ] 
Training: Epoch [3/120], Step [3499], Loss: 0.9770744442939758, Training Accuracy: 53.01428571428571
[ Sun Jul  7 12:23:04 2024 ] 	Batch(3500/7879) done. Loss: 2.1166  lr:0.010000
[ Sun Jul  7 12:23:27 2024 ] 	Batch(3600/7879) done. Loss: 1.0867  lr:0.010000
[ Sun Jul  7 12:23:49 2024 ] 	Batch(3700/7879) done. Loss: 2.4453  lr:0.010000
[ Sun Jul  7 12:24:12 2024 ] 	Batch(3800/7879) done. Loss: 0.7168  lr:0.010000
[ Sun Jul  7 12:24:35 2024 ] 	Batch(3900/7879) done. Loss: 0.7476  lr:0.010000
[ Sun Jul  7 12:24:57 2024 ] 
Training: Epoch [3/120], Step [3999], Loss: 0.9792964458465576, Training Accuracy: 53.28125
[ Sun Jul  7 12:24:58 2024 ] 	Batch(4000/7879) done. Loss: 2.3886  lr:0.010000
[ Sun Jul  7 12:25:20 2024 ] 	Batch(4100/7879) done. Loss: 1.7723  lr:0.010000
[ Sun Jul  7 12:25:43 2024 ] 	Batch(4200/7879) done. Loss: 2.1227  lr:0.010000
[ Sun Jul  7 12:26:06 2024 ] 	Batch(4300/7879) done. Loss: 2.0411  lr:0.010000
[ Sun Jul  7 12:26:29 2024 ] 	Batch(4400/7879) done. Loss: 1.9275  lr:0.010000
[ Sun Jul  7 12:26:51 2024 ] 
Training: Epoch [3/120], Step [4499], Loss: 1.7977001667022705, Training Accuracy: 53.61111111111111
[ Sun Jul  7 12:26:51 2024 ] 	Batch(4500/7879) done. Loss: 1.4395  lr:0.010000
[ Sun Jul  7 12:27:14 2024 ] 	Batch(4600/7879) done. Loss: 1.6937  lr:0.010000
[ Sun Jul  7 12:27:37 2024 ] 	Batch(4700/7879) done. Loss: 1.8344  lr:0.010000
[ Sun Jul  7 12:28:00 2024 ] 	Batch(4800/7879) done. Loss: 2.0371  lr:0.010000
[ Sun Jul  7 12:28:22 2024 ] 	Batch(4900/7879) done. Loss: 1.1953  lr:0.010000
[ Sun Jul  7 12:28:45 2024 ] 
Training: Epoch [3/120], Step [4999], Loss: 0.5915130376815796, Training Accuracy: 53.4775
[ Sun Jul  7 12:28:45 2024 ] 	Batch(5000/7879) done. Loss: 1.4693  lr:0.010000
[ Sun Jul  7 12:29:08 2024 ] 	Batch(5100/7879) done. Loss: 1.9448  lr:0.010000
[ Sun Jul  7 12:29:31 2024 ] 	Batch(5200/7879) done. Loss: 0.6290  lr:0.010000
[ Sun Jul  7 12:29:53 2024 ] 	Batch(5300/7879) done. Loss: 1.3615  lr:0.010000
[ Sun Jul  7 12:30:16 2024 ] 	Batch(5400/7879) done. Loss: 0.7640  lr:0.010000
[ Sun Jul  7 12:30:39 2024 ] 
Training: Epoch [3/120], Step [5499], Loss: 1.7735086679458618, Training Accuracy: 53.50681818181818
[ Sun Jul  7 12:30:39 2024 ] 	Batch(5500/7879) done. Loss: 1.3677  lr:0.010000
[ Sun Jul  7 12:31:02 2024 ] 	Batch(5600/7879) done. Loss: 0.8391  lr:0.010000
[ Sun Jul  7 12:31:25 2024 ] 	Batch(5700/7879) done. Loss: 2.2160  lr:0.010000
[ Sun Jul  7 12:31:48 2024 ] 	Batch(5800/7879) done. Loss: 1.5994  lr:0.010000
[ Sun Jul  7 12:32:11 2024 ] 	Batch(5900/7879) done. Loss: 1.8517  lr:0.010000
[ Sun Jul  7 12:32:33 2024 ] 
Training: Epoch [3/120], Step [5999], Loss: 1.591996669769287, Training Accuracy: 53.6625
[ Sun Jul  7 12:32:33 2024 ] 	Batch(6000/7879) done. Loss: 1.3607  lr:0.010000
[ Sun Jul  7 12:32:56 2024 ] 	Batch(6100/7879) done. Loss: 0.9342  lr:0.010000
[ Sun Jul  7 12:33:19 2024 ] 	Batch(6200/7879) done. Loss: 1.9194  lr:0.010000
[ Sun Jul  7 12:33:41 2024 ] 	Batch(6300/7879) done. Loss: 1.2830  lr:0.010000
[ Sun Jul  7 12:34:04 2024 ] 	Batch(6400/7879) done. Loss: 1.7265  lr:0.010000
[ Sun Jul  7 12:34:27 2024 ] 
Training: Epoch [3/120], Step [6499], Loss: 2.581409215927124, Training Accuracy: 53.75576923076923
[ Sun Jul  7 12:34:27 2024 ] 	Batch(6500/7879) done. Loss: 2.5444  lr:0.010000
[ Sun Jul  7 12:34:50 2024 ] 	Batch(6600/7879) done. Loss: 1.3903  lr:0.010000
[ Sun Jul  7 12:35:12 2024 ] 	Batch(6700/7879) done. Loss: 1.9408  lr:0.010000
[ Sun Jul  7 12:35:35 2024 ] 	Batch(6800/7879) done. Loss: 0.8291  lr:0.010000
[ Sun Jul  7 12:35:58 2024 ] 	Batch(6900/7879) done. Loss: 0.8915  lr:0.010000
[ Sun Jul  7 12:36:21 2024 ] 
Training: Epoch [3/120], Step [6999], Loss: 1.6100062131881714, Training Accuracy: 53.880357142857136
[ Sun Jul  7 12:36:21 2024 ] 	Batch(7000/7879) done. Loss: 1.3846  lr:0.010000
[ Sun Jul  7 12:36:43 2024 ] 	Batch(7100/7879) done. Loss: 2.5283  lr:0.010000
[ Sun Jul  7 12:37:06 2024 ] 	Batch(7200/7879) done. Loss: 1.3496  lr:0.010000
[ Sun Jul  7 12:37:29 2024 ] 	Batch(7300/7879) done. Loss: 3.2133  lr:0.010000
[ Sun Jul  7 12:37:52 2024 ] 	Batch(7400/7879) done. Loss: 1.1693  lr:0.010000
[ Sun Jul  7 12:38:14 2024 ] 
Training: Epoch [3/120], Step [7499], Loss: 1.7835420370101929, Training Accuracy: 53.93333333333333
[ Sun Jul  7 12:38:14 2024 ] 	Batch(7500/7879) done. Loss: 2.1637  lr:0.010000
[ Sun Jul  7 12:38:37 2024 ] 	Batch(7600/7879) done. Loss: 0.9579  lr:0.010000
[ Sun Jul  7 12:39:00 2024 ] 	Batch(7700/7879) done. Loss: 2.7780  lr:0.010000
[ Sun Jul  7 12:39:23 2024 ] 	Batch(7800/7879) done. Loss: 2.2886  lr:0.010000
[ Sun Jul  7 12:39:40 2024 ] 	Mean training loss: 1.6335.
[ Sun Jul  7 12:39:40 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 12:39:41 2024 ] Training epoch: 5
[ Sun Jul  7 12:39:41 2024 ] 	Batch(0/7879) done. Loss: 1.6233  lr:0.010000
[ Sun Jul  7 12:40:04 2024 ] 	Batch(100/7879) done. Loss: 1.5034  lr:0.010000
[ Sun Jul  7 12:40:26 2024 ] 	Batch(200/7879) done. Loss: 1.3612  lr:0.010000
[ Sun Jul  7 12:40:49 2024 ] 	Batch(300/7879) done. Loss: 0.9729  lr:0.010000
[ Sun Jul  7 12:41:12 2024 ] 	Batch(400/7879) done. Loss: 2.1972  lr:0.010000
[ Sun Jul  7 12:41:34 2024 ] 
Training: Epoch [4/120], Step [499], Loss: 2.158073902130127, Training Accuracy: 57.49999999999999
[ Sun Jul  7 12:41:34 2024 ] 	Batch(500/7879) done. Loss: 2.3153  lr:0.010000
[ Sun Jul  7 12:41:57 2024 ] 	Batch(600/7879) done. Loss: 1.5185  lr:0.010000
[ Sun Jul  7 12:42:20 2024 ] 	Batch(700/7879) done. Loss: 1.7347  lr:0.010000
[ Sun Jul  7 12:42:43 2024 ] 	Batch(800/7879) done. Loss: 1.2087  lr:0.010000
[ Sun Jul  7 12:43:06 2024 ] 	Batch(900/7879) done. Loss: 1.0764  lr:0.010000
[ Sun Jul  7 12:43:29 2024 ] 
Training: Epoch [4/120], Step [999], Loss: 1.1087872982025146, Training Accuracy: 56.7875
[ Sun Jul  7 12:43:30 2024 ] 	Batch(1000/7879) done. Loss: 3.2574  lr:0.010000
[ Sun Jul  7 12:43:53 2024 ] 	Batch(1100/7879) done. Loss: 1.9303  lr:0.010000
[ Sun Jul  7 12:44:17 2024 ] 	Batch(1200/7879) done. Loss: 2.0664  lr:0.010000
[ Sun Jul  7 12:44:39 2024 ] 	Batch(1300/7879) done. Loss: 2.2660  lr:0.010000
[ Sun Jul  7 12:45:02 2024 ] 	Batch(1400/7879) done. Loss: 1.3393  lr:0.010000
[ Sun Jul  7 12:45:25 2024 ] 
Training: Epoch [4/120], Step [1499], Loss: 2.397876262664795, Training Accuracy: 56.81666666666667
[ Sun Jul  7 12:45:25 2024 ] 	Batch(1500/7879) done. Loss: 0.9783  lr:0.010000
[ Sun Jul  7 12:45:48 2024 ] 	Batch(1600/7879) done. Loss: 1.0845  lr:0.010000
[ Sun Jul  7 12:46:10 2024 ] 	Batch(1700/7879) done. Loss: 1.3821  lr:0.010000
[ Sun Jul  7 12:46:33 2024 ] 	Batch(1800/7879) done. Loss: 2.1138  lr:0.010000
[ Sun Jul  7 12:46:56 2024 ] 	Batch(1900/7879) done. Loss: 1.6617  lr:0.010000
[ Sun Jul  7 12:47:18 2024 ] 
Training: Epoch [4/120], Step [1999], Loss: 1.1592779159545898, Training Accuracy: 56.78125000000001
[ Sun Jul  7 12:47:19 2024 ] 	Batch(2000/7879) done. Loss: 1.6601  lr:0.010000
[ Sun Jul  7 12:47:42 2024 ] 	Batch(2100/7879) done. Loss: 1.4224  lr:0.010000
[ Sun Jul  7 12:48:05 2024 ] 	Batch(2200/7879) done. Loss: 2.1736  lr:0.010000
[ Sun Jul  7 12:48:29 2024 ] 	Batch(2300/7879) done. Loss: 1.8875  lr:0.010000
[ Sun Jul  7 12:48:53 2024 ] 	Batch(2400/7879) done. Loss: 1.4659  lr:0.010000
[ Sun Jul  7 12:49:15 2024 ] 
Training: Epoch [4/120], Step [2499], Loss: 1.7633600234985352, Training Accuracy: 56.915000000000006
[ Sun Jul  7 12:49:15 2024 ] 	Batch(2500/7879) done. Loss: 2.9902  lr:0.010000
[ Sun Jul  7 12:49:38 2024 ] 	Batch(2600/7879) done. Loss: 1.9203  lr:0.010000
[ Sun Jul  7 12:50:01 2024 ] 	Batch(2700/7879) done. Loss: 0.6867  lr:0.010000
[ Sun Jul  7 12:50:24 2024 ] 	Batch(2800/7879) done. Loss: 0.3743  lr:0.010000
[ Sun Jul  7 12:50:46 2024 ] 	Batch(2900/7879) done. Loss: 3.3568  lr:0.010000
[ Sun Jul  7 12:51:09 2024 ] 
Training: Epoch [4/120], Step [2999], Loss: 1.7164667844772339, Training Accuracy: 57.14583333333333
[ Sun Jul  7 12:51:09 2024 ] 	Batch(3000/7879) done. Loss: 1.9708  lr:0.010000
[ Sun Jul  7 12:51:32 2024 ] 	Batch(3100/7879) done. Loss: 1.9336  lr:0.010000
[ Sun Jul  7 12:51:55 2024 ] 	Batch(3200/7879) done. Loss: 0.7364  lr:0.010000
[ Sun Jul  7 12:52:18 2024 ] 	Batch(3300/7879) done. Loss: 1.5668  lr:0.010000
[ Sun Jul  7 12:52:41 2024 ] 	Batch(3400/7879) done. Loss: 2.2133  lr:0.010000
[ Sun Jul  7 12:53:04 2024 ] 
Training: Epoch [4/120], Step [3499], Loss: 2.2196714878082275, Training Accuracy: 57.33571428571429
[ Sun Jul  7 12:53:05 2024 ] 	Batch(3500/7879) done. Loss: 2.7221  lr:0.010000
[ Sun Jul  7 12:53:28 2024 ] 	Batch(3600/7879) done. Loss: 0.7412  lr:0.010000
[ Sun Jul  7 12:53:50 2024 ] 	Batch(3700/7879) done. Loss: 1.6514  lr:0.010000
[ Sun Jul  7 12:54:13 2024 ] 	Batch(3800/7879) done. Loss: 0.5184  lr:0.010000
[ Sun Jul  7 12:54:36 2024 ] 	Batch(3900/7879) done. Loss: 1.6458  lr:0.010000
[ Sun Jul  7 12:54:58 2024 ] 
Training: Epoch [4/120], Step [3999], Loss: 2.1641883850097656, Training Accuracy: 57.362500000000004
[ Sun Jul  7 12:54:59 2024 ] 	Batch(4000/7879) done. Loss: 2.0249  lr:0.010000
[ Sun Jul  7 12:55:21 2024 ] 	Batch(4100/7879) done. Loss: 2.2760  lr:0.010000
[ Sun Jul  7 12:55:44 2024 ] 	Batch(4200/7879) done. Loss: 1.5341  lr:0.010000
[ Sun Jul  7 12:56:07 2024 ] 	Batch(4300/7879) done. Loss: 1.1463  lr:0.010000
[ Sun Jul  7 12:56:30 2024 ] 	Batch(4400/7879) done. Loss: 0.4980  lr:0.010000
[ Sun Jul  7 12:56:53 2024 ] 
Training: Epoch [4/120], Step [4499], Loss: 1.0861536264419556, Training Accuracy: 57.31388888888888
[ Sun Jul  7 12:56:53 2024 ] 	Batch(4500/7879) done. Loss: 1.5256  lr:0.010000
[ Sun Jul  7 12:57:16 2024 ] 	Batch(4600/7879) done. Loss: 1.6032  lr:0.010000
[ Sun Jul  7 12:57:39 2024 ] 	Batch(4700/7879) done. Loss: 1.0655  lr:0.010000
[ Sun Jul  7 12:58:02 2024 ] 	Batch(4800/7879) done. Loss: 0.9075  lr:0.010000
[ Sun Jul  7 12:58:25 2024 ] 	Batch(4900/7879) done. Loss: 1.7551  lr:0.010000
[ Sun Jul  7 12:58:47 2024 ] 
Training: Epoch [4/120], Step [4999], Loss: 2.7611896991729736, Training Accuracy: 57.457499999999996
[ Sun Jul  7 12:58:48 2024 ] 	Batch(5000/7879) done. Loss: 1.9928  lr:0.010000
[ Sun Jul  7 12:59:10 2024 ] 	Batch(5100/7879) done. Loss: 1.2539  lr:0.010000
[ Sun Jul  7 12:59:33 2024 ] 	Batch(5200/7879) done. Loss: 1.3624  lr:0.010000
[ Sun Jul  7 12:59:56 2024 ] 	Batch(5300/7879) done. Loss: 1.6560  lr:0.010000
[ Sun Jul  7 13:00:19 2024 ] 	Batch(5400/7879) done. Loss: 0.9716  lr:0.010000
[ Sun Jul  7 13:00:41 2024 ] 
Training: Epoch [4/120], Step [5499], Loss: 1.1554436683654785, Training Accuracy: 57.522727272727266
[ Sun Jul  7 13:00:41 2024 ] 	Batch(5500/7879) done. Loss: 1.2795  lr:0.010000
[ Sun Jul  7 13:01:04 2024 ] 	Batch(5600/7879) done. Loss: 2.4469  lr:0.010000
[ Sun Jul  7 13:01:27 2024 ] 	Batch(5700/7879) done. Loss: 1.1817  lr:0.010000
[ Sun Jul  7 13:01:50 2024 ] 	Batch(5800/7879) done. Loss: 0.5344  lr:0.010000
[ Sun Jul  7 13:02:12 2024 ] 	Batch(5900/7879) done. Loss: 1.3272  lr:0.010000
[ Sun Jul  7 13:02:35 2024 ] 
Training: Epoch [4/120], Step [5999], Loss: 1.673422932624817, Training Accuracy: 57.766666666666666
[ Sun Jul  7 13:02:35 2024 ] 	Batch(6000/7879) done. Loss: 1.7620  lr:0.010000
[ Sun Jul  7 13:02:58 2024 ] 	Batch(6100/7879) done. Loss: 1.2771  lr:0.010000
[ Sun Jul  7 13:03:21 2024 ] 	Batch(6200/7879) done. Loss: 2.0461  lr:0.010000
[ Sun Jul  7 13:03:43 2024 ] 	Batch(6300/7879) done. Loss: 2.3526  lr:0.010000
[ Sun Jul  7 13:04:06 2024 ] 	Batch(6400/7879) done. Loss: 1.3071  lr:0.010000
[ Sun Jul  7 13:04:29 2024 ] 
Training: Epoch [4/120], Step [6499], Loss: 2.9638893604278564, Training Accuracy: 57.761538461538464
[ Sun Jul  7 13:04:29 2024 ] 	Batch(6500/7879) done. Loss: 1.6648  lr:0.010000
[ Sun Jul  7 13:04:52 2024 ] 	Batch(6600/7879) done. Loss: 1.7804  lr:0.010000
[ Sun Jul  7 13:05:14 2024 ] 	Batch(6700/7879) done. Loss: 1.1119  lr:0.010000
[ Sun Jul  7 13:05:37 2024 ] 	Batch(6800/7879) done. Loss: 0.8649  lr:0.010000
[ Sun Jul  7 13:06:00 2024 ] 	Batch(6900/7879) done. Loss: 1.2095  lr:0.010000
[ Sun Jul  7 13:06:23 2024 ] 
Training: Epoch [4/120], Step [6999], Loss: 1.133980393409729, Training Accuracy: 57.84107142857143
[ Sun Jul  7 13:06:23 2024 ] 	Batch(7000/7879) done. Loss: 1.4579  lr:0.010000
[ Sun Jul  7 13:06:46 2024 ] 	Batch(7100/7879) done. Loss: 1.0611  lr:0.010000
[ Sun Jul  7 13:07:09 2024 ] 	Batch(7200/7879) done. Loss: 0.9797  lr:0.010000
[ Sun Jul  7 13:07:32 2024 ] 	Batch(7300/7879) done. Loss: 1.0086  lr:0.010000
[ Sun Jul  7 13:07:55 2024 ] 	Batch(7400/7879) done. Loss: 1.2028  lr:0.010000
[ Sun Jul  7 13:08:17 2024 ] 
Training: Epoch [4/120], Step [7499], Loss: 1.1283035278320312, Training Accuracy: 58.004999999999995
[ Sun Jul  7 13:08:17 2024 ] 	Batch(7500/7879) done. Loss: 1.2236  lr:0.010000
[ Sun Jul  7 13:08:40 2024 ] 	Batch(7600/7879) done. Loss: 1.1946  lr:0.010000
[ Sun Jul  7 13:09:03 2024 ] 	Batch(7700/7879) done. Loss: 1.7330  lr:0.010000
[ Sun Jul  7 13:09:26 2024 ] 	Batch(7800/7879) done. Loss: 1.3071  lr:0.010000
[ Sun Jul  7 13:09:43 2024 ] 	Mean training loss: 1.4924.
[ Sun Jul  7 13:09:43 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 13:09:43 2024 ] Training epoch: 6
[ Sun Jul  7 13:09:44 2024 ] 	Batch(0/7879) done. Loss: 1.6500  lr:0.010000
[ Sun Jul  7 13:10:07 2024 ] 	Batch(100/7879) done. Loss: 0.9383  lr:0.010000
[ Sun Jul  7 13:10:29 2024 ] 	Batch(200/7879) done. Loss: 0.5314  lr:0.010000
[ Sun Jul  7 13:10:52 2024 ] 	Batch(300/7879) done. Loss: 1.4859  lr:0.010000
[ Sun Jul  7 13:11:15 2024 ] 	Batch(400/7879) done. Loss: 0.8764  lr:0.010000
[ Sun Jul  7 13:11:37 2024 ] 
Training: Epoch [5/120], Step [499], Loss: 1.0955504179000854, Training Accuracy: 58.5
[ Sun Jul  7 13:11:38 2024 ] 	Batch(500/7879) done. Loss: 1.2489  lr:0.010000
[ Sun Jul  7 13:12:00 2024 ] 	Batch(600/7879) done. Loss: 2.7971  lr:0.010000
[ Sun Jul  7 13:12:23 2024 ] 	Batch(700/7879) done. Loss: 1.1608  lr:0.010000
[ Sun Jul  7 13:12:46 2024 ] 	Batch(800/7879) done. Loss: 0.9795  lr:0.010000
[ Sun Jul  7 13:13:09 2024 ] 	Batch(900/7879) done. Loss: 0.6288  lr:0.010000
[ Sun Jul  7 13:13:32 2024 ] 
Training: Epoch [5/120], Step [999], Loss: 1.4235973358154297, Training Accuracy: 59.112500000000004
[ Sun Jul  7 13:13:33 2024 ] 	Batch(1000/7879) done. Loss: 1.8579  lr:0.010000
[ Sun Jul  7 13:13:56 2024 ] 	Batch(1100/7879) done. Loss: 1.7803  lr:0.010000
[ Sun Jul  7 13:14:20 2024 ] 	Batch(1200/7879) done. Loss: 1.5855  lr:0.010000
[ Sun Jul  7 13:14:43 2024 ] 	Batch(1300/7879) done. Loss: 1.8615  lr:0.010000
[ Sun Jul  7 13:15:07 2024 ] 	Batch(1400/7879) done. Loss: 2.0835  lr:0.010000
[ Sun Jul  7 13:15:30 2024 ] 
Training: Epoch [5/120], Step [1499], Loss: 1.222656011581421, Training Accuracy: 59.741666666666674
[ Sun Jul  7 13:15:30 2024 ] 	Batch(1500/7879) done. Loss: 1.0842  lr:0.010000
[ Sun Jul  7 13:15:54 2024 ] 	Batch(1600/7879) done. Loss: 0.4804  lr:0.010000
[ Sun Jul  7 13:16:16 2024 ] 	Batch(1700/7879) done. Loss: 0.6119  lr:0.010000
[ Sun Jul  7 13:16:39 2024 ] 	Batch(1800/7879) done. Loss: 1.0979  lr:0.010000
[ Sun Jul  7 13:17:02 2024 ] 	Batch(1900/7879) done. Loss: 0.6972  lr:0.010000
[ Sun Jul  7 13:17:24 2024 ] 
Training: Epoch [5/120], Step [1999], Loss: 1.4422630071640015, Training Accuracy: 60.2375
[ Sun Jul  7 13:17:25 2024 ] 	Batch(2000/7879) done. Loss: 2.4968  lr:0.010000
[ Sun Jul  7 13:17:48 2024 ] 	Batch(2100/7879) done. Loss: 0.7593  lr:0.010000
[ Sun Jul  7 13:18:11 2024 ] 	Batch(2200/7879) done. Loss: 0.4481  lr:0.010000
[ Sun Jul  7 13:18:34 2024 ] 	Batch(2300/7879) done. Loss: 1.3214  lr:0.010000
[ Sun Jul  7 13:18:57 2024 ] 	Batch(2400/7879) done. Loss: 1.9415  lr:0.010000
[ Sun Jul  7 13:19:19 2024 ] 
Training: Epoch [5/120], Step [2499], Loss: 0.7324562668800354, Training Accuracy: 60.545
[ Sun Jul  7 13:19:20 2024 ] 	Batch(2500/7879) done. Loss: 0.7915  lr:0.010000
[ Sun Jul  7 13:19:42 2024 ] 	Batch(2600/7879) done. Loss: 0.8717  lr:0.010000
[ Sun Jul  7 13:20:05 2024 ] 	Batch(2700/7879) done. Loss: 1.8207  lr:0.010000
[ Sun Jul  7 13:20:28 2024 ] 	Batch(2800/7879) done. Loss: 0.3772  lr:0.010000
[ Sun Jul  7 13:20:51 2024 ] 	Batch(2900/7879) done. Loss: 0.6187  lr:0.010000
[ Sun Jul  7 13:21:13 2024 ] 
Training: Epoch [5/120], Step [2999], Loss: 2.037916421890259, Training Accuracy: 60.45
[ Sun Jul  7 13:21:13 2024 ] 	Batch(3000/7879) done. Loss: 1.4731  lr:0.010000
[ Sun Jul  7 13:21:36 2024 ] 	Batch(3100/7879) done. Loss: 1.4523  lr:0.010000
[ Sun Jul  7 13:21:59 2024 ] 	Batch(3200/7879) done. Loss: 0.8424  lr:0.010000
[ Sun Jul  7 13:22:22 2024 ] 	Batch(3300/7879) done. Loss: 1.1837  lr:0.010000
[ Sun Jul  7 13:22:44 2024 ] 	Batch(3400/7879) done. Loss: 1.4726  lr:0.010000
[ Sun Jul  7 13:23:07 2024 ] 
Training: Epoch [5/120], Step [3499], Loss: 1.7233779430389404, Training Accuracy: 60.33571428571428
[ Sun Jul  7 13:23:07 2024 ] 	Batch(3500/7879) done. Loss: 0.8873  lr:0.010000
[ Sun Jul  7 13:23:30 2024 ] 	Batch(3600/7879) done. Loss: 1.5226  lr:0.010000
[ Sun Jul  7 13:23:53 2024 ] 	Batch(3700/7879) done. Loss: 1.6763  lr:0.010000
[ Sun Jul  7 13:24:15 2024 ] 	Batch(3800/7879) done. Loss: 1.1713  lr:0.010000
[ Sun Jul  7 13:24:38 2024 ] 	Batch(3900/7879) done. Loss: 1.3863  lr:0.010000
[ Sun Jul  7 13:25:00 2024 ] 
Training: Epoch [5/120], Step [3999], Loss: 0.5320630073547363, Training Accuracy: 60.365625
[ Sun Jul  7 13:25:01 2024 ] 	Batch(4000/7879) done. Loss: 2.3271  lr:0.010000
[ Sun Jul  7 13:25:24 2024 ] 	Batch(4100/7879) done. Loss: 2.3835  lr:0.010000
[ Sun Jul  7 13:25:46 2024 ] 	Batch(4200/7879) done. Loss: 2.0436  lr:0.010000
[ Sun Jul  7 13:26:09 2024 ] 	Batch(4300/7879) done. Loss: 1.4885  lr:0.010000
[ Sun Jul  7 13:26:32 2024 ] 	Batch(4400/7879) done. Loss: 0.6267  lr:0.010000
[ Sun Jul  7 13:26:55 2024 ] 
Training: Epoch [5/120], Step [4499], Loss: 1.3987619876861572, Training Accuracy: 60.50277777777778
[ Sun Jul  7 13:26:55 2024 ] 	Batch(4500/7879) done. Loss: 1.1126  lr:0.010000
[ Sun Jul  7 13:27:17 2024 ] 	Batch(4600/7879) done. Loss: 1.0157  lr:0.010000
[ Sun Jul  7 13:27:40 2024 ] 	Batch(4700/7879) done. Loss: 2.5064  lr:0.010000
[ Sun Jul  7 13:28:03 2024 ] 	Batch(4800/7879) done. Loss: 1.2164  lr:0.010000
[ Sun Jul  7 13:28:26 2024 ] 	Batch(4900/7879) done. Loss: 0.8685  lr:0.010000
[ Sun Jul  7 13:28:49 2024 ] 
Training: Epoch [5/120], Step [4999], Loss: 0.9441699981689453, Training Accuracy: 60.51499999999999
[ Sun Jul  7 13:28:49 2024 ] 	Batch(5000/7879) done. Loss: 0.5050  lr:0.010000
[ Sun Jul  7 13:29:12 2024 ] 	Batch(5100/7879) done. Loss: 1.0563  lr:0.010000
[ Sun Jul  7 13:29:35 2024 ] 	Batch(5200/7879) done. Loss: 0.7940  lr:0.010000
[ Sun Jul  7 13:29:58 2024 ] 	Batch(5300/7879) done. Loss: 1.1805  lr:0.010000
[ Sun Jul  7 13:30:21 2024 ] 	Batch(5400/7879) done. Loss: 1.5851  lr:0.010000
[ Sun Jul  7 13:30:43 2024 ] 
Training: Epoch [5/120], Step [5499], Loss: 1.0659716129302979, Training Accuracy: 60.65454545454545
[ Sun Jul  7 13:30:43 2024 ] 	Batch(5500/7879) done. Loss: 1.5917  lr:0.010000
[ Sun Jul  7 13:31:06 2024 ] 	Batch(5600/7879) done. Loss: 1.4413  lr:0.010000
[ Sun Jul  7 13:31:29 2024 ] 	Batch(5700/7879) done. Loss: 2.2706  lr:0.010000
[ Sun Jul  7 13:31:52 2024 ] 	Batch(5800/7879) done. Loss: 1.0044  lr:0.010000
[ Sun Jul  7 13:32:15 2024 ] 	Batch(5900/7879) done. Loss: 0.8482  lr:0.010000
[ Sun Jul  7 13:32:37 2024 ] 
Training: Epoch [5/120], Step [5999], Loss: 1.6378183364868164, Training Accuracy: 60.741666666666674
[ Sun Jul  7 13:32:37 2024 ] 	Batch(6000/7879) done. Loss: 1.6822  lr:0.010000
[ Sun Jul  7 13:33:01 2024 ] 	Batch(6100/7879) done. Loss: 1.1981  lr:0.010000
[ Sun Jul  7 13:33:24 2024 ] 	Batch(6200/7879) done. Loss: 1.2753  lr:0.010000
[ Sun Jul  7 13:33:47 2024 ] 	Batch(6300/7879) done. Loss: 1.5412  lr:0.010000
[ Sun Jul  7 13:34:10 2024 ] 	Batch(6400/7879) done. Loss: 0.9998  lr:0.010000
[ Sun Jul  7 13:34:34 2024 ] 
Training: Epoch [5/120], Step [6499], Loss: 1.937088131904602, Training Accuracy: 60.80384615384615
[ Sun Jul  7 13:34:34 2024 ] 	Batch(6500/7879) done. Loss: 1.8431  lr:0.010000
[ Sun Jul  7 13:34:56 2024 ] 	Batch(6600/7879) done. Loss: 1.1668  lr:0.010000
[ Sun Jul  7 13:35:19 2024 ] 	Batch(6700/7879) done. Loss: 2.3357  lr:0.010000
[ Sun Jul  7 13:35:42 2024 ] 	Batch(6800/7879) done. Loss: 0.6161  lr:0.010000
[ Sun Jul  7 13:36:05 2024 ] 	Batch(6900/7879) done. Loss: 2.3603  lr:0.010000
[ Sun Jul  7 13:36:27 2024 ] 
Training: Epoch [5/120], Step [6999], Loss: 2.3807010650634766, Training Accuracy: 60.91607142857143
[ Sun Jul  7 13:36:27 2024 ] 	Batch(7000/7879) done. Loss: 2.0824  lr:0.010000
[ Sun Jul  7 13:36:50 2024 ] 	Batch(7100/7879) done. Loss: 1.3568  lr:0.010000
[ Sun Jul  7 13:37:13 2024 ] 	Batch(7200/7879) done. Loss: 2.9256  lr:0.010000
[ Sun Jul  7 13:37:36 2024 ] 	Batch(7300/7879) done. Loss: 0.9901  lr:0.010000
[ Sun Jul  7 13:37:59 2024 ] 	Batch(7400/7879) done. Loss: 2.7050  lr:0.010000
[ Sun Jul  7 13:38:22 2024 ] 
Training: Epoch [5/120], Step [7499], Loss: 1.550624966621399, Training Accuracy: 60.92666666666666
[ Sun Jul  7 13:38:23 2024 ] 	Batch(7500/7879) done. Loss: 0.3889  lr:0.010000
[ Sun Jul  7 13:38:46 2024 ] 	Batch(7600/7879) done. Loss: 1.7633  lr:0.010000
[ Sun Jul  7 13:39:08 2024 ] 	Batch(7700/7879) done. Loss: 0.8843  lr:0.010000
[ Sun Jul  7 13:39:31 2024 ] 	Batch(7800/7879) done. Loss: 0.4908  lr:0.010000
[ Sun Jul  7 13:39:49 2024 ] 	Mean training loss: 1.3823.
[ Sun Jul  7 13:39:49 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 13:39:49 2024 ] Training epoch: 7
[ Sun Jul  7 13:39:49 2024 ] 	Batch(0/7879) done. Loss: 1.8997  lr:0.010000
[ Sun Jul  7 13:40:12 2024 ] 	Batch(100/7879) done. Loss: 2.1786  lr:0.010000
[ Sun Jul  7 13:40:35 2024 ] 	Batch(200/7879) done. Loss: 1.0323  lr:0.010000
[ Sun Jul  7 13:40:57 2024 ] 	Batch(300/7879) done. Loss: 1.2964  lr:0.010000
[ Sun Jul  7 13:41:20 2024 ] 	Batch(400/7879) done. Loss: 1.8350  lr:0.010000
[ Sun Jul  7 13:41:43 2024 ] 
Training: Epoch [6/120], Step [499], Loss: 0.9135019183158875, Training Accuracy: 61.6
[ Sun Jul  7 13:41:43 2024 ] 	Batch(500/7879) done. Loss: 0.9864  lr:0.010000
[ Sun Jul  7 13:42:06 2024 ] 	Batch(600/7879) done. Loss: 1.3713  lr:0.010000
[ Sun Jul  7 13:42:28 2024 ] 	Batch(700/7879) done. Loss: 1.0558  lr:0.010000
[ Sun Jul  7 13:42:51 2024 ] 	Batch(800/7879) done. Loss: 1.0838  lr:0.010000
[ Sun Jul  7 13:43:14 2024 ] 	Batch(900/7879) done. Loss: 1.6181  lr:0.010000
[ Sun Jul  7 13:43:37 2024 ] 
Training: Epoch [6/120], Step [999], Loss: 1.61630117893219, Training Accuracy: 62.4375
[ Sun Jul  7 13:43:37 2024 ] 	Batch(1000/7879) done. Loss: 0.8632  lr:0.010000
[ Sun Jul  7 13:43:59 2024 ] 	Batch(1100/7879) done. Loss: 0.7628  lr:0.010000
[ Sun Jul  7 13:44:22 2024 ] 	Batch(1200/7879) done. Loss: 1.1202  lr:0.010000
[ Sun Jul  7 13:44:45 2024 ] 	Batch(1300/7879) done. Loss: 1.4192  lr:0.010000
[ Sun Jul  7 13:45:08 2024 ] 	Batch(1400/7879) done. Loss: 0.8295  lr:0.010000
[ Sun Jul  7 13:45:30 2024 ] 
Training: Epoch [6/120], Step [1499], Loss: 0.9397813081741333, Training Accuracy: 62.958333333333336
[ Sun Jul  7 13:45:31 2024 ] 	Batch(1500/7879) done. Loss: 1.1540  lr:0.010000
[ Sun Jul  7 13:45:53 2024 ] 	Batch(1600/7879) done. Loss: 1.3045  lr:0.010000
[ Sun Jul  7 13:46:16 2024 ] 	Batch(1700/7879) done. Loss: 0.9586  lr:0.010000
[ Sun Jul  7 13:46:39 2024 ] 	Batch(1800/7879) done. Loss: 1.3561  lr:0.010000
[ Sun Jul  7 13:47:02 2024 ] 	Batch(1900/7879) done. Loss: 0.8107  lr:0.010000
[ Sun Jul  7 13:47:24 2024 ] 
Training: Epoch [6/120], Step [1999], Loss: 1.2873544692993164, Training Accuracy: 62.925
[ Sun Jul  7 13:47:25 2024 ] 	Batch(2000/7879) done. Loss: 0.9933  lr:0.010000
[ Sun Jul  7 13:47:47 2024 ] 	Batch(2100/7879) done. Loss: 0.8278  lr:0.010000
[ Sun Jul  7 13:48:10 2024 ] 	Batch(2200/7879) done. Loss: 1.2225  lr:0.010000
[ Sun Jul  7 13:48:33 2024 ] 	Batch(2300/7879) done. Loss: 1.3788  lr:0.010000
[ Sun Jul  7 13:48:56 2024 ] 	Batch(2400/7879) done. Loss: 1.2218  lr:0.010000
[ Sun Jul  7 13:49:18 2024 ] 
Training: Epoch [6/120], Step [2499], Loss: 1.0205340385437012, Training Accuracy: 63.044999999999995
[ Sun Jul  7 13:49:19 2024 ] 	Batch(2500/7879) done. Loss: 1.4162  lr:0.010000
[ Sun Jul  7 13:49:41 2024 ] 	Batch(2600/7879) done. Loss: 1.7410  lr:0.010000
[ Sun Jul  7 13:50:04 2024 ] 	Batch(2700/7879) done. Loss: 1.0900  lr:0.010000
[ Sun Jul  7 13:50:27 2024 ] 	Batch(2800/7879) done. Loss: 1.8769  lr:0.010000
[ Sun Jul  7 13:50:50 2024 ] 	Batch(2900/7879) done. Loss: 1.7511  lr:0.010000
[ Sun Jul  7 13:51:12 2024 ] 
Training: Epoch [6/120], Step [2999], Loss: 1.618255376815796, Training Accuracy: 62.97083333333333
[ Sun Jul  7 13:51:12 2024 ] 	Batch(3000/7879) done. Loss: 1.4130  lr:0.010000
[ Sun Jul  7 13:51:35 2024 ] 	Batch(3100/7879) done. Loss: 0.9171  lr:0.010000
[ Sun Jul  7 13:51:58 2024 ] 	Batch(3200/7879) done. Loss: 0.9841  lr:0.010000
[ Sun Jul  7 13:52:21 2024 ] 	Batch(3300/7879) done. Loss: 1.3826  lr:0.010000
[ Sun Jul  7 13:52:44 2024 ] 	Batch(3400/7879) done. Loss: 0.5939  lr:0.010000
[ Sun Jul  7 13:53:06 2024 ] 
Training: Epoch [6/120], Step [3499], Loss: 2.4607555866241455, Training Accuracy: 63.114285714285714
[ Sun Jul  7 13:53:07 2024 ] 	Batch(3500/7879) done. Loss: 2.5482  lr:0.010000
[ Sun Jul  7 13:53:30 2024 ] 	Batch(3600/7879) done. Loss: 0.7059  lr:0.010000
[ Sun Jul  7 13:53:53 2024 ] 	Batch(3700/7879) done. Loss: 0.7755  lr:0.010000
[ Sun Jul  7 13:54:16 2024 ] 	Batch(3800/7879) done. Loss: 1.4356  lr:0.010000
[ Sun Jul  7 13:54:39 2024 ] 	Batch(3900/7879) done. Loss: 1.8464  lr:0.010000
[ Sun Jul  7 13:55:01 2024 ] 
Training: Epoch [6/120], Step [3999], Loss: 1.3889825344085693, Training Accuracy: 63.212500000000006
[ Sun Jul  7 13:55:02 2024 ] 	Batch(4000/7879) done. Loss: 1.1890  lr:0.010000
[ Sun Jul  7 13:55:24 2024 ] 	Batch(4100/7879) done. Loss: 1.9110  lr:0.010000
[ Sun Jul  7 13:55:47 2024 ] 	Batch(4200/7879) done. Loss: 1.2665  lr:0.010000
[ Sun Jul  7 13:56:10 2024 ] 	Batch(4300/7879) done. Loss: 1.2921  lr:0.010000
[ Sun Jul  7 13:56:33 2024 ] 	Batch(4400/7879) done. Loss: 2.6499  lr:0.010000
[ Sun Jul  7 13:56:55 2024 ] 
Training: Epoch [6/120], Step [4499], Loss: 1.8448867797851562, Training Accuracy: 63.336111111111116
[ Sun Jul  7 13:56:55 2024 ] 	Batch(4500/7879) done. Loss: 0.4454  lr:0.010000
[ Sun Jul  7 13:57:18 2024 ] 	Batch(4600/7879) done. Loss: 2.0604  lr:0.010000
[ Sun Jul  7 13:57:41 2024 ] 	Batch(4700/7879) done. Loss: 0.5881  lr:0.010000
[ Sun Jul  7 13:58:04 2024 ] 	Batch(4800/7879) done. Loss: 0.5762  lr:0.010000
[ Sun Jul  7 13:58:27 2024 ] 	Batch(4900/7879) done. Loss: 0.9518  lr:0.010000
[ Sun Jul  7 13:58:49 2024 ] 
Training: Epoch [6/120], Step [4999], Loss: 1.4557747840881348, Training Accuracy: 63.395
[ Sun Jul  7 13:58:49 2024 ] 	Batch(5000/7879) done. Loss: 1.6188  lr:0.010000
[ Sun Jul  7 13:59:12 2024 ] 	Batch(5100/7879) done. Loss: 1.2425  lr:0.010000
[ Sun Jul  7 13:59:34 2024 ] 	Batch(5200/7879) done. Loss: 1.4624  lr:0.010000
[ Sun Jul  7 13:59:57 2024 ] 	Batch(5300/7879) done. Loss: 1.7941  lr:0.010000
[ Sun Jul  7 14:00:19 2024 ] 	Batch(5400/7879) done. Loss: 0.3853  lr:0.010000
[ Sun Jul  7 14:00:42 2024 ] 
Training: Epoch [6/120], Step [5499], Loss: 1.8390154838562012, Training Accuracy: 63.44545454545455
[ Sun Jul  7 14:00:42 2024 ] 	Batch(5500/7879) done. Loss: 0.9213  lr:0.010000
[ Sun Jul  7 14:01:05 2024 ] 	Batch(5600/7879) done. Loss: 0.0564  lr:0.010000
[ Sun Jul  7 14:01:27 2024 ] 	Batch(5700/7879) done. Loss: 1.9619  lr:0.010000
[ Sun Jul  7 14:01:50 2024 ] 	Batch(5800/7879) done. Loss: 1.4367  lr:0.010000
[ Sun Jul  7 14:02:12 2024 ] 	Batch(5900/7879) done. Loss: 1.3445  lr:0.010000
[ Sun Jul  7 14:02:35 2024 ] 
Training: Epoch [6/120], Step [5999], Loss: 0.8182600736618042, Training Accuracy: 63.56666666666667
[ Sun Jul  7 14:02:35 2024 ] 	Batch(6000/7879) done. Loss: 1.2765  lr:0.010000
[ Sun Jul  7 14:02:58 2024 ] 	Batch(6100/7879) done. Loss: 0.5061  lr:0.010000
[ Sun Jul  7 14:03:22 2024 ] 	Batch(6200/7879) done. Loss: 1.7290  lr:0.010000
[ Sun Jul  7 14:03:45 2024 ] 	Batch(6300/7879) done. Loss: 0.8581  lr:0.010000
[ Sun Jul  7 14:04:09 2024 ] 	Batch(6400/7879) done. Loss: 2.1144  lr:0.010000
[ Sun Jul  7 14:04:32 2024 ] 
Training: Epoch [6/120], Step [6499], Loss: 2.507298469543457, Training Accuracy: 63.54615384615384
[ Sun Jul  7 14:04:32 2024 ] 	Batch(6500/7879) done. Loss: 1.0431  lr:0.010000
[ Sun Jul  7 14:04:55 2024 ] 	Batch(6600/7879) done. Loss: 1.0879  lr:0.010000
[ Sun Jul  7 14:05:19 2024 ] 	Batch(6700/7879) done. Loss: 1.9569  lr:0.010000
[ Sun Jul  7 14:05:42 2024 ] 	Batch(6800/7879) done. Loss: 1.5426  lr:0.010000
[ Sun Jul  7 14:06:05 2024 ] 	Batch(6900/7879) done. Loss: 1.7362  lr:0.010000
[ Sun Jul  7 14:06:29 2024 ] 
Training: Epoch [6/120], Step [6999], Loss: 2.026845932006836, Training Accuracy: 63.56964285714286
[ Sun Jul  7 14:06:29 2024 ] 	Batch(7000/7879) done. Loss: 1.4637  lr:0.010000
[ Sun Jul  7 14:06:52 2024 ] 	Batch(7100/7879) done. Loss: 0.7607  lr:0.010000
[ Sun Jul  7 14:07:16 2024 ] 	Batch(7200/7879) done. Loss: 1.5711  lr:0.010000
[ Sun Jul  7 14:07:39 2024 ] 	Batch(7300/7879) done. Loss: 1.1456  lr:0.010000
[ Sun Jul  7 14:08:02 2024 ] 	Batch(7400/7879) done. Loss: 1.3002  lr:0.010000
[ Sun Jul  7 14:08:25 2024 ] 
Training: Epoch [6/120], Step [7499], Loss: 1.5694392919540405, Training Accuracy: 63.531666666666666
[ Sun Jul  7 14:08:25 2024 ] 	Batch(7500/7879) done. Loss: 1.2842  lr:0.010000
[ Sun Jul  7 14:08:48 2024 ] 	Batch(7600/7879) done. Loss: 0.8048  lr:0.010000
[ Sun Jul  7 14:09:11 2024 ] 	Batch(7700/7879) done. Loss: 0.7722  lr:0.010000
[ Sun Jul  7 14:09:33 2024 ] 	Batch(7800/7879) done. Loss: 1.0573  lr:0.010000
[ Sun Jul  7 14:09:51 2024 ] 	Mean training loss: 1.2862.
[ Sun Jul  7 14:09:51 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 14:09:51 2024 ] Training epoch: 8
[ Sun Jul  7 14:09:52 2024 ] 	Batch(0/7879) done. Loss: 0.5926  lr:0.010000
[ Sun Jul  7 14:10:15 2024 ] 	Batch(100/7879) done. Loss: 0.3969  lr:0.010000
[ Sun Jul  7 14:10:38 2024 ] 	Batch(200/7879) done. Loss: 0.5493  lr:0.010000
[ Sun Jul  7 14:11:01 2024 ] 	Batch(300/7879) done. Loss: 1.7115  lr:0.010000
[ Sun Jul  7 14:11:24 2024 ] 	Batch(400/7879) done. Loss: 1.6048  lr:0.010000
[ Sun Jul  7 14:11:47 2024 ] 
Training: Epoch [7/120], Step [499], Loss: 0.9880425333976746, Training Accuracy: 65.375
[ Sun Jul  7 14:11:47 2024 ] 	Batch(500/7879) done. Loss: 1.9624  lr:0.010000
[ Sun Jul  7 14:12:10 2024 ] 	Batch(600/7879) done. Loss: 2.0423  lr:0.010000
[ Sun Jul  7 14:12:32 2024 ] 	Batch(700/7879) done. Loss: 0.9915  lr:0.010000
[ Sun Jul  7 14:12:55 2024 ] 	Batch(800/7879) done. Loss: 1.9087  lr:0.010000
[ Sun Jul  7 14:13:17 2024 ] 	Batch(900/7879) done. Loss: 0.8183  lr:0.010000
[ Sun Jul  7 14:13:40 2024 ] 
Training: Epoch [7/120], Step [999], Loss: 1.3076328039169312, Training Accuracy: 65.14999999999999
[ Sun Jul  7 14:13:40 2024 ] 	Batch(1000/7879) done. Loss: 1.9590  lr:0.010000
[ Sun Jul  7 14:14:03 2024 ] 	Batch(1100/7879) done. Loss: 1.2727  lr:0.010000
[ Sun Jul  7 14:14:25 2024 ] 	Batch(1200/7879) done. Loss: 0.6304  lr:0.010000
[ Sun Jul  7 14:14:48 2024 ] 	Batch(1300/7879) done. Loss: 1.7653  lr:0.010000
[ Sun Jul  7 14:15:11 2024 ] 	Batch(1400/7879) done. Loss: 1.0743  lr:0.010000
[ Sun Jul  7 14:15:33 2024 ] 
Training: Epoch [7/120], Step [1499], Loss: 0.3156626224517822, Training Accuracy: 64.96666666666667
[ Sun Jul  7 14:15:33 2024 ] 	Batch(1500/7879) done. Loss: 1.2354  lr:0.010000
[ Sun Jul  7 14:15:56 2024 ] 	Batch(1600/7879) done. Loss: 1.5742  lr:0.010000
[ Sun Jul  7 14:16:19 2024 ] 	Batch(1700/7879) done. Loss: 1.2664  lr:0.010000
[ Sun Jul  7 14:16:42 2024 ] 	Batch(1800/7879) done. Loss: 1.1440  lr:0.010000
[ Sun Jul  7 14:17:04 2024 ] 	Batch(1900/7879) done. Loss: 1.7857  lr:0.010000
[ Sun Jul  7 14:17:27 2024 ] 
Training: Epoch [7/120], Step [1999], Loss: 0.3931465744972229, Training Accuracy: 65.25625000000001
[ Sun Jul  7 14:17:27 2024 ] 	Batch(2000/7879) done. Loss: 0.9388  lr:0.010000
[ Sun Jul  7 14:17:50 2024 ] 	Batch(2100/7879) done. Loss: 1.3489  lr:0.010000
[ Sun Jul  7 14:18:13 2024 ] 	Batch(2200/7879) done. Loss: 0.6454  lr:0.010000
[ Sun Jul  7 14:18:36 2024 ] 	Batch(2300/7879) done. Loss: 1.2432  lr:0.010000
[ Sun Jul  7 14:18:59 2024 ] 	Batch(2400/7879) done. Loss: 0.7733  lr:0.010000
[ Sun Jul  7 14:19:21 2024 ] 
Training: Epoch [7/120], Step [2499], Loss: 0.9830626249313354, Training Accuracy: 65.27
[ Sun Jul  7 14:19:22 2024 ] 	Batch(2500/7879) done. Loss: 0.7680  lr:0.010000
[ Sun Jul  7 14:19:44 2024 ] 	Batch(2600/7879) done. Loss: 1.6007  lr:0.010000
[ Sun Jul  7 14:20:07 2024 ] 	Batch(2700/7879) done. Loss: 0.9728  lr:0.010000
[ Sun Jul  7 14:20:30 2024 ] 	Batch(2800/7879) done. Loss: 0.2572  lr:0.010000
[ Sun Jul  7 14:20:53 2024 ] 	Batch(2900/7879) done. Loss: 1.3638  lr:0.010000
[ Sun Jul  7 14:21:15 2024 ] 
Training: Epoch [7/120], Step [2999], Loss: 0.9632035493850708, Training Accuracy: 65.36666666666666
[ Sun Jul  7 14:21:15 2024 ] 	Batch(3000/7879) done. Loss: 1.6845  lr:0.010000
[ Sun Jul  7 14:21:38 2024 ] 	Batch(3100/7879) done. Loss: 0.4005  lr:0.010000
[ Sun Jul  7 14:22:01 2024 ] 	Batch(3200/7879) done. Loss: 1.1278  lr:0.010000
[ Sun Jul  7 14:22:24 2024 ] 	Batch(3300/7879) done. Loss: 0.9554  lr:0.010000
[ Sun Jul  7 14:22:46 2024 ] 	Batch(3400/7879) done. Loss: 1.3159  lr:0.010000
[ Sun Jul  7 14:23:09 2024 ] 
Training: Epoch [7/120], Step [3499], Loss: 2.256422996520996, Training Accuracy: 65.2
[ Sun Jul  7 14:23:09 2024 ] 	Batch(3500/7879) done. Loss: 0.9111  lr:0.010000
[ Sun Jul  7 14:23:32 2024 ] 	Batch(3600/7879) done. Loss: 0.5878  lr:0.010000
[ Sun Jul  7 14:23:55 2024 ] 	Batch(3700/7879) done. Loss: 0.5800  lr:0.010000
[ Sun Jul  7 14:24:18 2024 ] 	Batch(3800/7879) done. Loss: 0.1466  lr:0.010000
[ Sun Jul  7 14:24:40 2024 ] 	Batch(3900/7879) done. Loss: 1.1763  lr:0.010000
[ Sun Jul  7 14:25:03 2024 ] 
Training: Epoch [7/120], Step [3999], Loss: 1.6925700902938843, Training Accuracy: 65.296875
[ Sun Jul  7 14:25:03 2024 ] 	Batch(4000/7879) done. Loss: 0.9440  lr:0.010000
[ Sun Jul  7 14:25:27 2024 ] 	Batch(4100/7879) done. Loss: 1.2556  lr:0.010000
[ Sun Jul  7 14:25:50 2024 ] 	Batch(4200/7879) done. Loss: 0.5298  lr:0.010000
[ Sun Jul  7 14:26:14 2024 ] 	Batch(4300/7879) done. Loss: 2.4750  lr:0.010000
[ Sun Jul  7 14:26:36 2024 ] 	Batch(4400/7879) done. Loss: 1.0917  lr:0.010000
[ Sun Jul  7 14:26:59 2024 ] 
Training: Epoch [7/120], Step [4499], Loss: 2.08912992477417, Training Accuracy: 65.28055555555555
[ Sun Jul  7 14:26:59 2024 ] 	Batch(4500/7879) done. Loss: 1.5363  lr:0.010000
[ Sun Jul  7 14:27:22 2024 ] 	Batch(4600/7879) done. Loss: 2.2900  lr:0.010000
[ Sun Jul  7 14:27:45 2024 ] 	Batch(4700/7879) done. Loss: 0.6522  lr:0.010000
[ Sun Jul  7 14:28:08 2024 ] 	Batch(4800/7879) done. Loss: 1.4270  lr:0.010000
[ Sun Jul  7 14:28:30 2024 ] 	Batch(4900/7879) done. Loss: 1.3471  lr:0.010000
[ Sun Jul  7 14:28:53 2024 ] 
Training: Epoch [7/120], Step [4999], Loss: 2.0220398902893066, Training Accuracy: 65.18
[ Sun Jul  7 14:28:53 2024 ] 	Batch(5000/7879) done. Loss: 0.7337  lr:0.010000
[ Sun Jul  7 14:29:16 2024 ] 	Batch(5100/7879) done. Loss: 0.5574  lr:0.010000
[ Sun Jul  7 14:29:38 2024 ] 	Batch(5200/7879) done. Loss: 0.5485  lr:0.010000
[ Sun Jul  7 14:30:01 2024 ] 	Batch(5300/7879) done. Loss: 0.4380  lr:0.010000
[ Sun Jul  7 14:30:24 2024 ] 	Batch(5400/7879) done. Loss: 1.1843  lr:0.010000
[ Sun Jul  7 14:30:46 2024 ] 
Training: Epoch [7/120], Step [5499], Loss: 2.5995123386383057, Training Accuracy: 65.2409090909091
[ Sun Jul  7 14:30:47 2024 ] 	Batch(5500/7879) done. Loss: 0.3810  lr:0.010000
[ Sun Jul  7 14:31:09 2024 ] 	Batch(5600/7879) done. Loss: 2.3998  lr:0.010000
[ Sun Jul  7 14:31:32 2024 ] 	Batch(5700/7879) done. Loss: 0.5241  lr:0.010000
[ Sun Jul  7 14:31:55 2024 ] 	Batch(5800/7879) done. Loss: 0.7481  lr:0.010000
[ Sun Jul  7 14:32:18 2024 ] 	Batch(5900/7879) done. Loss: 1.3643  lr:0.010000
[ Sun Jul  7 14:32:40 2024 ] 
Training: Epoch [7/120], Step [5999], Loss: 2.0706632137298584, Training Accuracy: 65.25833333333333
[ Sun Jul  7 14:32:40 2024 ] 	Batch(6000/7879) done. Loss: 0.6950  lr:0.010000
[ Sun Jul  7 14:33:03 2024 ] 	Batch(6100/7879) done. Loss: 0.8376  lr:0.010000
[ Sun Jul  7 14:33:26 2024 ] 	Batch(6200/7879) done. Loss: 0.4485  lr:0.010000
[ Sun Jul  7 14:33:49 2024 ] 	Batch(6300/7879) done. Loss: 0.2923  lr:0.010000
[ Sun Jul  7 14:34:12 2024 ] 	Batch(6400/7879) done. Loss: 1.1124  lr:0.010000
[ Sun Jul  7 14:34:34 2024 ] 
Training: Epoch [7/120], Step [6499], Loss: 0.5675094127655029, Training Accuracy: 65.325
[ Sun Jul  7 14:34:34 2024 ] 	Batch(6500/7879) done. Loss: 0.5048  lr:0.010000
[ Sun Jul  7 14:34:57 2024 ] 	Batch(6600/7879) done. Loss: 0.4244  lr:0.010000
[ Sun Jul  7 14:35:20 2024 ] 	Batch(6700/7879) done. Loss: 0.2676  lr:0.010000
[ Sun Jul  7 14:35:43 2024 ] 	Batch(6800/7879) done. Loss: 1.4218  lr:0.010000
[ Sun Jul  7 14:36:05 2024 ] 	Batch(6900/7879) done. Loss: 1.5608  lr:0.010000
[ Sun Jul  7 14:36:28 2024 ] 
Training: Epoch [7/120], Step [6999], Loss: 0.6874246597290039, Training Accuracy: 65.38035714285715
[ Sun Jul  7 14:36:28 2024 ] 	Batch(7000/7879) done. Loss: 1.4734  lr:0.010000
[ Sun Jul  7 14:36:51 2024 ] 	Batch(7100/7879) done. Loss: 1.5947  lr:0.010000
[ Sun Jul  7 14:37:14 2024 ] 	Batch(7200/7879) done. Loss: 1.0943  lr:0.010000
[ Sun Jul  7 14:37:36 2024 ] 	Batch(7300/7879) done. Loss: 0.9022  lr:0.010000
[ Sun Jul  7 14:37:59 2024 ] 	Batch(7400/7879) done. Loss: 1.5297  lr:0.010000
[ Sun Jul  7 14:38:22 2024 ] 
Training: Epoch [7/120], Step [7499], Loss: 0.7708815932273865, Training Accuracy: 65.515
[ Sun Jul  7 14:38:22 2024 ] 	Batch(7500/7879) done. Loss: 0.9409  lr:0.010000
[ Sun Jul  7 14:38:45 2024 ] 	Batch(7600/7879) done. Loss: 0.7985  lr:0.010000
[ Sun Jul  7 14:39:07 2024 ] 	Batch(7700/7879) done. Loss: 1.4895  lr:0.010000
[ Sun Jul  7 14:39:30 2024 ] 	Batch(7800/7879) done. Loss: 1.0323  lr:0.010000
[ Sun Jul  7 14:39:48 2024 ] 	Mean training loss: 1.2039.
[ Sun Jul  7 14:39:48 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 14:39:49 2024 ] Training epoch: 9
[ Sun Jul  7 14:39:49 2024 ] 	Batch(0/7879) done. Loss: 1.8601  lr:0.010000
[ Sun Jul  7 14:40:12 2024 ] 	Batch(100/7879) done. Loss: 1.1918  lr:0.010000
[ Sun Jul  7 14:40:35 2024 ] 	Batch(200/7879) done. Loss: 1.0555  lr:0.010000
[ Sun Jul  7 14:40:57 2024 ] 	Batch(300/7879) done. Loss: 1.5310  lr:0.010000
[ Sun Jul  7 14:41:20 2024 ] 	Batch(400/7879) done. Loss: 1.2121  lr:0.010000
[ Sun Jul  7 14:41:43 2024 ] 
Training: Epoch [8/120], Step [499], Loss: 0.4773638844490051, Training Accuracy: 67.0
[ Sun Jul  7 14:41:43 2024 ] 	Batch(500/7879) done. Loss: 0.5695  lr:0.010000
[ Sun Jul  7 14:42:06 2024 ] 	Batch(600/7879) done. Loss: 1.4268  lr:0.010000
[ Sun Jul  7 14:42:28 2024 ] 	Batch(700/7879) done. Loss: 1.1495  lr:0.010000
[ Sun Jul  7 14:42:51 2024 ] 	Batch(800/7879) done. Loss: 0.6048  lr:0.010000
[ Sun Jul  7 14:43:14 2024 ] 	Batch(900/7879) done. Loss: 0.7966  lr:0.010000
[ Sun Jul  7 14:43:36 2024 ] 
Training: Epoch [8/120], Step [999], Loss: 0.5191268920898438, Training Accuracy: 67.0
[ Sun Jul  7 14:43:37 2024 ] 	Batch(1000/7879) done. Loss: 1.6292  lr:0.010000
[ Sun Jul  7 14:43:59 2024 ] 	Batch(1100/7879) done. Loss: 1.5075  lr:0.010000
[ Sun Jul  7 14:44:22 2024 ] 	Batch(1200/7879) done. Loss: 0.8753  lr:0.010000
[ Sun Jul  7 14:44:45 2024 ] 	Batch(1300/7879) done. Loss: 1.9989  lr:0.010000
[ Sun Jul  7 14:45:08 2024 ] 	Batch(1400/7879) done. Loss: 1.9403  lr:0.010000
[ Sun Jul  7 14:45:31 2024 ] 
Training: Epoch [8/120], Step [1499], Loss: 1.5197900533676147, Training Accuracy: 67.15833333333333
[ Sun Jul  7 14:45:32 2024 ] 	Batch(1500/7879) done. Loss: 1.0118  lr:0.010000
[ Sun Jul  7 14:45:55 2024 ] 	Batch(1600/7879) done. Loss: 1.6227  lr:0.010000
[ Sun Jul  7 14:46:17 2024 ] 	Batch(1700/7879) done. Loss: 0.5453  lr:0.010000
[ Sun Jul  7 14:46:40 2024 ] 	Batch(1800/7879) done. Loss: 0.7362  lr:0.010000
[ Sun Jul  7 14:47:03 2024 ] 	Batch(1900/7879) done. Loss: 0.8566  lr:0.010000
[ Sun Jul  7 14:47:25 2024 ] 
Training: Epoch [8/120], Step [1999], Loss: 1.4412071704864502, Training Accuracy: 67.33749999999999
[ Sun Jul  7 14:47:26 2024 ] 	Batch(2000/7879) done. Loss: 0.7417  lr:0.010000
[ Sun Jul  7 14:47:49 2024 ] 	Batch(2100/7879) done. Loss: 0.7943  lr:0.010000
[ Sun Jul  7 14:48:11 2024 ] 	Batch(2200/7879) done. Loss: 0.2733  lr:0.010000
[ Sun Jul  7 14:48:34 2024 ] 	Batch(2300/7879) done. Loss: 0.7442  lr:0.010000
[ Sun Jul  7 14:48:57 2024 ] 	Batch(2400/7879) done. Loss: 0.6561  lr:0.010000
[ Sun Jul  7 14:49:19 2024 ] 
Training: Epoch [8/120], Step [2499], Loss: 0.6266400814056396, Training Accuracy: 67.10000000000001
[ Sun Jul  7 14:49:20 2024 ] 	Batch(2500/7879) done. Loss: 1.4650  lr:0.010000
[ Sun Jul  7 14:49:42 2024 ] 	Batch(2600/7879) done. Loss: 1.3456  lr:0.010000
[ Sun Jul  7 14:50:05 2024 ] 	Batch(2700/7879) done. Loss: 0.4740  lr:0.010000
[ Sun Jul  7 14:50:28 2024 ] 	Batch(2800/7879) done. Loss: 1.0547  lr:0.010000
[ Sun Jul  7 14:50:51 2024 ] 	Batch(2900/7879) done. Loss: 0.6828  lr:0.010000
[ Sun Jul  7 14:51:13 2024 ] 
Training: Epoch [8/120], Step [2999], Loss: 0.932958722114563, Training Accuracy: 67.13333333333334
[ Sun Jul  7 14:51:13 2024 ] 	Batch(3000/7879) done. Loss: 2.0411  lr:0.010000
[ Sun Jul  7 14:51:36 2024 ] 	Batch(3100/7879) done. Loss: 0.5082  lr:0.010000
[ Sun Jul  7 14:51:59 2024 ] 	Batch(3200/7879) done. Loss: 1.3732  lr:0.010000
[ Sun Jul  7 14:52:22 2024 ] 	Batch(3300/7879) done. Loss: 1.2323  lr:0.010000
[ Sun Jul  7 14:52:44 2024 ] 	Batch(3400/7879) done. Loss: 1.6659  lr:0.010000
[ Sun Jul  7 14:53:07 2024 ] 
Training: Epoch [8/120], Step [3499], Loss: 1.0567567348480225, Training Accuracy: 67.08214285714286
[ Sun Jul  7 14:53:07 2024 ] 	Batch(3500/7879) done. Loss: 1.2544  lr:0.010000
[ Sun Jul  7 14:53:30 2024 ] 	Batch(3600/7879) done. Loss: 0.6985  lr:0.010000
[ Sun Jul  7 14:53:53 2024 ] 	Batch(3700/7879) done. Loss: 0.7495  lr:0.010000
[ Sun Jul  7 14:54:15 2024 ] 	Batch(3800/7879) done. Loss: 1.3298  lr:0.010000
[ Sun Jul  7 14:54:39 2024 ] 	Batch(3900/7879) done. Loss: 0.5037  lr:0.010000
[ Sun Jul  7 14:55:01 2024 ] 
Training: Epoch [8/120], Step [3999], Loss: 1.0929687023162842, Training Accuracy: 67.190625
[ Sun Jul  7 14:55:02 2024 ] 	Batch(4000/7879) done. Loss: 0.6289  lr:0.010000
[ Sun Jul  7 14:55:25 2024 ] 	Batch(4100/7879) done. Loss: 1.3144  lr:0.010000
[ Sun Jul  7 14:55:47 2024 ] 	Batch(4200/7879) done. Loss: 0.9511  lr:0.010000
[ Sun Jul  7 14:56:10 2024 ] 	Batch(4300/7879) done. Loss: 1.3500  lr:0.010000
[ Sun Jul  7 14:56:33 2024 ] 	Batch(4400/7879) done. Loss: 0.6059  lr:0.010000
[ Sun Jul  7 14:56:55 2024 ] 
Training: Epoch [8/120], Step [4499], Loss: 1.6568013429641724, Training Accuracy: 67.26111111111112
[ Sun Jul  7 14:56:56 2024 ] 	Batch(4500/7879) done. Loss: 0.7082  lr:0.010000
[ Sun Jul  7 14:57:18 2024 ] 	Batch(4600/7879) done. Loss: 2.6679  lr:0.010000
[ Sun Jul  7 14:57:41 2024 ] 	Batch(4700/7879) done. Loss: 0.7513  lr:0.010000
[ Sun Jul  7 14:58:04 2024 ] 	Batch(4800/7879) done. Loss: 0.9966  lr:0.010000
[ Sun Jul  7 14:58:27 2024 ] 	Batch(4900/7879) done. Loss: 1.1197  lr:0.010000
[ Sun Jul  7 14:58:49 2024 ] 
Training: Epoch [8/120], Step [4999], Loss: 0.9031863808631897, Training Accuracy: 67.345
[ Sun Jul  7 14:58:50 2024 ] 	Batch(5000/7879) done. Loss: 0.7811  lr:0.010000
[ Sun Jul  7 14:59:12 2024 ] 	Batch(5100/7879) done. Loss: 0.4165  lr:0.010000
[ Sun Jul  7 14:59:35 2024 ] 	Batch(5200/7879) done. Loss: 0.6100  lr:0.010000
[ Sun Jul  7 14:59:59 2024 ] 	Batch(5300/7879) done. Loss: 1.8787  lr:0.010000
[ Sun Jul  7 15:00:22 2024 ] 	Batch(5400/7879) done. Loss: 0.7393  lr:0.010000
[ Sun Jul  7 15:00:44 2024 ] 
Training: Epoch [8/120], Step [5499], Loss: 1.085874319076538, Training Accuracy: 67.30454545454545
[ Sun Jul  7 15:00:45 2024 ] 	Batch(5500/7879) done. Loss: 1.9705  lr:0.010000
[ Sun Jul  7 15:01:07 2024 ] 	Batch(5600/7879) done. Loss: 0.6515  lr:0.010000
[ Sun Jul  7 15:01:30 2024 ] 	Batch(5700/7879) done. Loss: 1.3759  lr:0.010000
[ Sun Jul  7 15:01:53 2024 ] 	Batch(5800/7879) done. Loss: 0.5968  lr:0.010000
[ Sun Jul  7 15:02:16 2024 ] 	Batch(5900/7879) done. Loss: 0.7993  lr:0.010000
[ Sun Jul  7 15:02:38 2024 ] 
Training: Epoch [8/120], Step [5999], Loss: 1.0828150510787964, Training Accuracy: 67.31875000000001
[ Sun Jul  7 15:02:39 2024 ] 	Batch(6000/7879) done. Loss: 1.2267  lr:0.010000
[ Sun Jul  7 15:03:01 2024 ] 	Batch(6100/7879) done. Loss: 0.9163  lr:0.010000
[ Sun Jul  7 15:03:24 2024 ] 	Batch(6200/7879) done. Loss: 0.6743  lr:0.010000
[ Sun Jul  7 15:03:47 2024 ] 	Batch(6300/7879) done. Loss: 0.5665  lr:0.010000
[ Sun Jul  7 15:04:10 2024 ] 	Batch(6400/7879) done. Loss: 1.0248  lr:0.010000
[ Sun Jul  7 15:04:32 2024 ] 
Training: Epoch [8/120], Step [6499], Loss: 1.593222737312317, Training Accuracy: 67.33846153846153
[ Sun Jul  7 15:04:32 2024 ] 	Batch(6500/7879) done. Loss: 0.4409  lr:0.010000
[ Sun Jul  7 15:04:55 2024 ] 	Batch(6600/7879) done. Loss: 1.5435  lr:0.010000
[ Sun Jul  7 15:05:18 2024 ] 	Batch(6700/7879) done. Loss: 2.0041  lr:0.010000
[ Sun Jul  7 15:05:41 2024 ] 	Batch(6800/7879) done. Loss: 0.4509  lr:0.010000
[ Sun Jul  7 15:06:03 2024 ] 	Batch(6900/7879) done. Loss: 2.2985  lr:0.010000
[ Sun Jul  7 15:06:26 2024 ] 
Training: Epoch [8/120], Step [6999], Loss: 1.2571361064910889, Training Accuracy: 67.36785714285715
[ Sun Jul  7 15:06:26 2024 ] 	Batch(7000/7879) done. Loss: 0.4236  lr:0.010000
[ Sun Jul  7 15:06:49 2024 ] 	Batch(7100/7879) done. Loss: 0.6279  lr:0.010000
[ Sun Jul  7 15:07:12 2024 ] 	Batch(7200/7879) done. Loss: 1.0545  lr:0.010000
[ Sun Jul  7 15:07:35 2024 ] 	Batch(7300/7879) done. Loss: 0.3947  lr:0.010000
[ Sun Jul  7 15:07:58 2024 ] 	Batch(7400/7879) done. Loss: 0.6068  lr:0.010000
[ Sun Jul  7 15:08:20 2024 ] 
Training: Epoch [8/120], Step [7499], Loss: 0.685825765132904, Training Accuracy: 67.47833333333332
[ Sun Jul  7 15:08:21 2024 ] 	Batch(7500/7879) done. Loss: 0.5004  lr:0.010000
[ Sun Jul  7 15:08:43 2024 ] 	Batch(7600/7879) done. Loss: 0.3390  lr:0.010000
[ Sun Jul  7 15:09:06 2024 ] 	Batch(7700/7879) done. Loss: 1.6707  lr:0.010000
[ Sun Jul  7 15:09:29 2024 ] 	Batch(7800/7879) done. Loss: 0.5390  lr:0.010000
[ Sun Jul  7 15:09:47 2024 ] 	Mean training loss: 1.1282.
[ Sun Jul  7 15:09:47 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 15:09:47 2024 ] Training epoch: 10
[ Sun Jul  7 15:09:47 2024 ] 	Batch(0/7879) done. Loss: 1.4518  lr:0.010000
[ Sun Jul  7 15:10:10 2024 ] 	Batch(100/7879) done. Loss: 1.3141  lr:0.010000
[ Sun Jul  7 15:10:33 2024 ] 	Batch(200/7879) done. Loss: 1.7775  lr:0.010000
[ Sun Jul  7 15:10:56 2024 ] 	Batch(300/7879) done. Loss: 1.0055  lr:0.010000
[ Sun Jul  7 15:11:18 2024 ] 	Batch(400/7879) done. Loss: 2.0050  lr:0.010000
[ Sun Jul  7 15:11:41 2024 ] 
Training: Epoch [9/120], Step [499], Loss: 0.9039604663848877, Training Accuracy: 68.625
[ Sun Jul  7 15:11:41 2024 ] 	Batch(500/7879) done. Loss: 0.8334  lr:0.010000
[ Sun Jul  7 15:12:04 2024 ] 	Batch(600/7879) done. Loss: 1.2758  lr:0.010000
[ Sun Jul  7 15:12:27 2024 ] 	Batch(700/7879) done. Loss: 1.9364  lr:0.010000
[ Sun Jul  7 15:12:50 2024 ] 	Batch(800/7879) done. Loss: 1.6800  lr:0.010000
[ Sun Jul  7 15:13:13 2024 ] 	Batch(900/7879) done. Loss: 1.4952  lr:0.010000
[ Sun Jul  7 15:13:35 2024 ] 
Training: Epoch [9/120], Step [999], Loss: 0.9081006050109863, Training Accuracy: 68.125
[ Sun Jul  7 15:13:36 2024 ] 	Batch(1000/7879) done. Loss: 0.9597  lr:0.010000
[ Sun Jul  7 15:13:58 2024 ] 	Batch(1100/7879) done. Loss: 0.4890  lr:0.010000
[ Sun Jul  7 15:14:21 2024 ] 	Batch(1200/7879) done. Loss: 0.8048  lr:0.010000
[ Sun Jul  7 15:14:44 2024 ] 	Batch(1300/7879) done. Loss: 1.1613  lr:0.010000
[ Sun Jul  7 15:15:07 2024 ] 	Batch(1400/7879) done. Loss: 1.7262  lr:0.010000
[ Sun Jul  7 15:15:29 2024 ] 
Training: Epoch [9/120], Step [1499], Loss: 0.4368252456188202, Training Accuracy: 68.28333333333333
[ Sun Jul  7 15:15:29 2024 ] 	Batch(1500/7879) done. Loss: 1.1446  lr:0.010000
[ Sun Jul  7 15:15:52 2024 ] 	Batch(1600/7879) done. Loss: 1.3942  lr:0.010000
[ Sun Jul  7 15:16:15 2024 ] 	Batch(1700/7879) done. Loss: 0.6699  lr:0.010000
[ Sun Jul  7 15:16:38 2024 ] 	Batch(1800/7879) done. Loss: 1.1625  lr:0.010000
[ Sun Jul  7 15:17:00 2024 ] 	Batch(1900/7879) done. Loss: 0.4917  lr:0.010000
[ Sun Jul  7 15:17:23 2024 ] 
Training: Epoch [9/120], Step [1999], Loss: 1.8887959718704224, Training Accuracy: 68.60625
[ Sun Jul  7 15:17:23 2024 ] 	Batch(2000/7879) done. Loss: 0.5677  lr:0.010000
[ Sun Jul  7 15:17:47 2024 ] 	Batch(2100/7879) done. Loss: 0.8640  lr:0.010000
[ Sun Jul  7 15:18:10 2024 ] 	Batch(2200/7879) done. Loss: 0.5023  lr:0.010000
[ Sun Jul  7 15:18:33 2024 ] 	Batch(2300/7879) done. Loss: 2.2050  lr:0.010000
[ Sun Jul  7 15:18:56 2024 ] 	Batch(2400/7879) done. Loss: 0.6315  lr:0.010000
[ Sun Jul  7 15:19:19 2024 ] 
Training: Epoch [9/120], Step [2499], Loss: 1.3825316429138184, Training Accuracy: 68.685
[ Sun Jul  7 15:19:19 2024 ] 	Batch(2500/7879) done. Loss: 0.4766  lr:0.010000
[ Sun Jul  7 15:19:42 2024 ] 	Batch(2600/7879) done. Loss: 0.8045  lr:0.010000
[ Sun Jul  7 15:20:06 2024 ] 	Batch(2700/7879) done. Loss: 0.2623  lr:0.010000
[ Sun Jul  7 15:20:29 2024 ] 	Batch(2800/7879) done. Loss: 1.3681  lr:0.010000
[ Sun Jul  7 15:20:51 2024 ] 	Batch(2900/7879) done. Loss: 0.8427  lr:0.010000
[ Sun Jul  7 15:21:14 2024 ] 
Training: Epoch [9/120], Step [2999], Loss: 0.7671324610710144, Training Accuracy: 68.96666666666667
[ Sun Jul  7 15:21:14 2024 ] 	Batch(3000/7879) done. Loss: 0.2400  lr:0.010000
[ Sun Jul  7 15:21:37 2024 ] 	Batch(3100/7879) done. Loss: 0.5396  lr:0.010000
[ Sun Jul  7 15:22:00 2024 ] 	Batch(3200/7879) done. Loss: 1.1582  lr:0.010000
[ Sun Jul  7 15:22:22 2024 ] 	Batch(3300/7879) done. Loss: 0.9410  lr:0.010000
[ Sun Jul  7 15:22:45 2024 ] 	Batch(3400/7879) done. Loss: 1.0336  lr:0.010000
[ Sun Jul  7 15:23:08 2024 ] 
Training: Epoch [9/120], Step [3499], Loss: 1.6307315826416016, Training Accuracy: 68.90357142857142
[ Sun Jul  7 15:23:08 2024 ] 	Batch(3500/7879) done. Loss: 1.1489  lr:0.010000
[ Sun Jul  7 15:23:31 2024 ] 	Batch(3600/7879) done. Loss: 0.4439  lr:0.010000
[ Sun Jul  7 15:23:54 2024 ] 	Batch(3700/7879) done. Loss: 1.3660  lr:0.010000
[ Sun Jul  7 15:24:16 2024 ] 	Batch(3800/7879) done. Loss: 1.6259  lr:0.010000
[ Sun Jul  7 15:24:39 2024 ] 	Batch(3900/7879) done. Loss: 0.4958  lr:0.010000
[ Sun Jul  7 15:25:02 2024 ] 
Training: Epoch [9/120], Step [3999], Loss: 1.899057149887085, Training Accuracy: 68.78125
[ Sun Jul  7 15:25:02 2024 ] 	Batch(4000/7879) done. Loss: 1.1189  lr:0.010000
[ Sun Jul  7 15:25:25 2024 ] 	Batch(4100/7879) done. Loss: 1.4029  lr:0.010000
[ Sun Jul  7 15:25:47 2024 ] 	Batch(4200/7879) done. Loss: 0.8443  lr:0.010000
[ Sun Jul  7 15:26:10 2024 ] 	Batch(4300/7879) done. Loss: 0.8106  lr:0.010000
[ Sun Jul  7 15:26:33 2024 ] 	Batch(4400/7879) done. Loss: 1.3591  lr:0.010000
[ Sun Jul  7 15:26:56 2024 ] 
Training: Epoch [9/120], Step [4499], Loss: 0.9032581448554993, Training Accuracy: 68.75
[ Sun Jul  7 15:26:56 2024 ] 	Batch(4500/7879) done. Loss: 1.2873  lr:0.010000
[ Sun Jul  7 15:27:18 2024 ] 	Batch(4600/7879) done. Loss: 0.7943  lr:0.010000
[ Sun Jul  7 15:27:41 2024 ] 	Batch(4700/7879) done. Loss: 1.3058  lr:0.010000
[ Sun Jul  7 15:28:04 2024 ] 	Batch(4800/7879) done. Loss: 0.9753  lr:0.010000
[ Sun Jul  7 15:28:27 2024 ] 	Batch(4900/7879) done. Loss: 0.4521  lr:0.010000
[ Sun Jul  7 15:28:49 2024 ] 
Training: Epoch [9/120], Step [4999], Loss: 1.000797152519226, Training Accuracy: 68.6525
[ Sun Jul  7 15:28:49 2024 ] 	Batch(5000/7879) done. Loss: 1.5855  lr:0.010000
[ Sun Jul  7 15:29:12 2024 ] 	Batch(5100/7879) done. Loss: 0.8650  lr:0.010000
[ Sun Jul  7 15:29:35 2024 ] 	Batch(5200/7879) done. Loss: 0.7319  lr:0.010000
[ Sun Jul  7 15:29:57 2024 ] 	Batch(5300/7879) done. Loss: 1.5955  lr:0.010000
[ Sun Jul  7 15:30:20 2024 ] 	Batch(5400/7879) done. Loss: 1.1882  lr:0.010000
[ Sun Jul  7 15:30:43 2024 ] 
Training: Epoch [9/120], Step [5499], Loss: 1.0488033294677734, Training Accuracy: 68.75454545454545
[ Sun Jul  7 15:30:43 2024 ] 	Batch(5500/7879) done. Loss: 1.1002  lr:0.010000
[ Sun Jul  7 15:31:06 2024 ] 	Batch(5600/7879) done. Loss: 0.6914  lr:0.010000
[ Sun Jul  7 15:31:28 2024 ] 	Batch(5700/7879) done. Loss: 2.0051  lr:0.010000
[ Sun Jul  7 15:31:51 2024 ] 	Batch(5800/7879) done. Loss: 1.5156  lr:0.010000
[ Sun Jul  7 15:32:14 2024 ] 	Batch(5900/7879) done. Loss: 0.9997  lr:0.010000
[ Sun Jul  7 15:32:37 2024 ] 
Training: Epoch [9/120], Step [5999], Loss: 0.9359016418457031, Training Accuracy: 68.77291666666666
[ Sun Jul  7 15:32:37 2024 ] 	Batch(6000/7879) done. Loss: 0.7224  lr:0.010000
[ Sun Jul  7 15:33:00 2024 ] 	Batch(6100/7879) done. Loss: 2.0535  lr:0.010000
[ Sun Jul  7 15:33:22 2024 ] 	Batch(6200/7879) done. Loss: 0.4387  lr:0.010000
[ Sun Jul  7 15:33:45 2024 ] 	Batch(6300/7879) done. Loss: 1.0766  lr:0.010000
[ Sun Jul  7 15:34:08 2024 ] 	Batch(6400/7879) done. Loss: 0.3830  lr:0.010000
[ Sun Jul  7 15:34:30 2024 ] 
Training: Epoch [9/120], Step [6499], Loss: 1.3747508525848389, Training Accuracy: 68.8
[ Sun Jul  7 15:34:31 2024 ] 	Batch(6500/7879) done. Loss: 0.5038  lr:0.010000
[ Sun Jul  7 15:34:53 2024 ] 	Batch(6600/7879) done. Loss: 0.7988  lr:0.010000
[ Sun Jul  7 15:35:16 2024 ] 	Batch(6700/7879) done. Loss: 0.8538  lr:0.010000
[ Sun Jul  7 15:35:39 2024 ] 	Batch(6800/7879) done. Loss: 1.1469  lr:0.010000
[ Sun Jul  7 15:36:01 2024 ] 	Batch(6900/7879) done. Loss: 1.1017  lr:0.010000
[ Sun Jul  7 15:36:24 2024 ] 
Training: Epoch [9/120], Step [6999], Loss: 0.8973047733306885, Training Accuracy: 68.86785714285715
[ Sun Jul  7 15:36:24 2024 ] 	Batch(7000/7879) done. Loss: 0.8222  lr:0.010000
[ Sun Jul  7 15:36:47 2024 ] 	Batch(7100/7879) done. Loss: 0.3307  lr:0.010000
[ Sun Jul  7 15:37:10 2024 ] 	Batch(7200/7879) done. Loss: 0.3023  lr:0.010000
[ Sun Jul  7 15:37:32 2024 ] 	Batch(7300/7879) done. Loss: 0.5525  lr:0.010000
[ Sun Jul  7 15:37:55 2024 ] 	Batch(7400/7879) done. Loss: 0.3827  lr:0.010000
[ Sun Jul  7 15:38:18 2024 ] 
Training: Epoch [9/120], Step [7499], Loss: 1.1059963703155518, Training Accuracy: 68.92166666666667
[ Sun Jul  7 15:38:18 2024 ] 	Batch(7500/7879) done. Loss: 0.7245  lr:0.010000
[ Sun Jul  7 15:38:41 2024 ] 	Batch(7600/7879) done. Loss: 0.7570  lr:0.010000
[ Sun Jul  7 15:39:04 2024 ] 	Batch(7700/7879) done. Loss: 1.2048  lr:0.010000
[ Sun Jul  7 15:39:27 2024 ] 	Batch(7800/7879) done. Loss: 0.8798  lr:0.010000
[ Sun Jul  7 15:39:44 2024 ] 	Mean training loss: 1.0656.
[ Sun Jul  7 15:39:44 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 15:39:44 2024 ] Eval epoch: 10
[ Sun Jul  7 15:45:42 2024 ] 	Mean val loss of 6365 batches: 1.3126740847378808.
[ Sun Jul  7 15:45:42 2024 ] 
Validation: Epoch [9/120], Samples [32860.0/50919], Loss: 0.28565090894699097, Validation Accuracy: 64.53386751507296
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 1 : 148 / 275 = 53 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 2 : 214 / 273 = 78 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 3 : 190 / 273 = 69 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 4 : 196 / 275 = 71 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 5 : 173 / 275 = 62 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 6 : 190 / 275 = 69 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 7 : 249 / 273 = 91 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 8 : 246 / 273 = 90 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 9 : 82 / 273 = 30 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 10 : 84 / 273 = 30 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 11 : 61 / 272 = 22 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 12 : 167 / 271 = 61 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 13 : 223 / 275 = 81 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 14 : 249 / 276 = 90 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 15 : 206 / 273 = 75 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 16 : 74 / 274 = 27 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 17 : 184 / 273 = 67 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 18 : 194 / 274 = 70 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 19 : 213 / 272 = 78 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 20 : 223 / 273 = 81 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 21 : 208 / 274 = 75 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 22 : 192 / 274 = 70 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 23 : 231 / 276 = 83 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 24 : 211 / 274 = 77 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 25 : 262 / 275 = 95 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 26 : 236 / 276 = 85 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 27 : 233 / 275 = 84 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 28 : 142 / 275 = 51 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 29 : 169 / 275 = 61 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 30 : 201 / 276 = 72 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 31 : 200 / 276 = 72 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 32 : 219 / 276 = 79 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 33 : 218 / 276 = 78 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 34 : 240 / 276 = 86 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 35 : 244 / 275 = 88 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 36 : 128 / 276 = 46 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 37 : 243 / 276 = 88 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 38 : 246 / 276 = 89 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 39 : 210 / 276 = 76 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 40 : 104 / 276 = 37 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 41 : 269 / 276 = 97 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 42 : 248 / 275 = 90 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 43 : 137 / 276 = 49 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 44 : 216 / 276 = 78 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 45 : 246 / 276 = 89 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 46 : 138 / 276 = 50 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 47 : 207 / 275 = 75 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 48 : 224 / 275 = 81 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 49 : 184 / 274 = 67 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 50 : 226 / 276 = 81 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 51 : 227 / 276 = 82 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 52 : 211 / 276 = 76 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 53 : 246 / 276 = 89 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 54 : 245 / 274 = 89 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 55 : 217 / 276 = 78 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 56 : 216 / 275 = 78 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 57 : 255 / 276 = 92 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 58 : 241 / 273 = 88 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 59 : 258 / 276 = 93 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 60 : 410 / 561 = 73 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 61 : 431 / 566 = 76 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 62 : 296 / 572 = 51 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 63 : 460 / 570 = 80 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 64 : 303 / 574 = 52 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 65 : 452 / 573 = 78 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 66 : 436 / 573 = 76 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 67 : 318 / 575 = 55 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 68 : 213 / 575 = 37 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 69 : 489 / 575 = 85 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 70 : 234 / 575 = 40 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 71 : 196 / 575 = 34 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 72 : 22 / 571 = 3 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 73 : 72 / 570 = 12 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 74 : 284 / 569 = 49 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 75 : 218 / 573 = 38 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 76 : 252 / 574 = 43 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 77 : 283 / 573 = 49 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 78 : 326 / 575 = 56 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 79 : 495 / 574 = 86 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 80 : 425 / 573 = 74 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 81 : 256 / 575 = 44 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 82 : 287 / 575 = 49 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 83 : 33 / 572 = 5 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 84 : 303 / 574 = 52 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 85 : 301 / 574 = 52 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 86 : 367 / 575 = 63 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 87 : 412 / 576 = 71 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 88 : 360 / 575 = 62 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 89 : 215 / 576 = 37 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 90 : 132 / 574 = 22 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 91 : 379 / 568 = 66 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 92 : 379 / 576 = 65 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 93 : 114 / 573 = 19 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 94 : 491 / 574 = 85 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 95 : 473 / 575 = 82 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 96 : 550 / 575 = 95 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 97 : 552 / 574 = 96 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 98 : 517 / 575 = 89 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 99 : 487 / 574 = 84 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 100 : 426 / 574 = 74 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 101 : 494 / 574 = 86 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 102 : 164 / 575 = 28 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 103 : 383 / 576 = 66 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 104 : 185 / 575 = 32 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 105 : 67 / 575 = 11 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 106 : 221 / 576 = 38 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 107 : 370 / 576 = 64 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 108 : 362 / 575 = 62 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 109 : 326 / 575 = 56 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 110 : 394 / 575 = 68 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 111 : 489 / 576 = 84 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 112 : 535 / 575 = 93 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 113 : 495 / 576 = 85 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 114 : 448 / 576 = 77 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 115 : 446 / 576 = 77 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 116 : 464 / 575 = 80 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 117 : 423 / 575 = 73 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 118 : 398 / 575 = 69 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 119 : 460 / 576 = 79 %
[ Sun Jul  7 15:45:42 2024 ] Accuracy of 120 : 173 / 274 = 63 %
[ Sun Jul  7 15:45:42 2024 ] Training epoch: 11
[ Sun Jul  7 15:45:42 2024 ] 	Batch(0/7879) done. Loss: 1.3570  lr:0.010000
[ Sun Jul  7 15:46:05 2024 ] 	Batch(100/7879) done. Loss: 1.3336  lr:0.010000
[ Sun Jul  7 15:46:28 2024 ] 	Batch(200/7879) done. Loss: 1.2617  lr:0.010000
[ Sun Jul  7 15:46:50 2024 ] 	Batch(300/7879) done. Loss: 1.4291  lr:0.010000
[ Sun Jul  7 15:47:13 2024 ] 	Batch(400/7879) done. Loss: 1.0992  lr:0.010000
[ Sun Jul  7 15:47:36 2024 ] 
Training: Epoch [10/120], Step [499], Loss: 0.6081651449203491, Training Accuracy: 70.1
[ Sun Jul  7 15:47:36 2024 ] 	Batch(500/7879) done. Loss: 0.9838  lr:0.010000
[ Sun Jul  7 15:47:59 2024 ] 	Batch(600/7879) done. Loss: 0.5635  lr:0.010000
[ Sun Jul  7 15:48:21 2024 ] 	Batch(700/7879) done. Loss: 0.4811  lr:0.010000
[ Sun Jul  7 15:48:44 2024 ] 	Batch(800/7879) done. Loss: 1.1025  lr:0.010000
[ Sun Jul  7 15:49:07 2024 ] 	Batch(900/7879) done. Loss: 1.2911  lr:0.010000
[ Sun Jul  7 15:49:30 2024 ] 
Training: Epoch [10/120], Step [999], Loss: 1.084998369216919, Training Accuracy: 70.5125
[ Sun Jul  7 15:49:30 2024 ] 	Batch(1000/7879) done. Loss: 0.5557  lr:0.010000
[ Sun Jul  7 15:49:53 2024 ] 	Batch(1100/7879) done. Loss: 0.5890  lr:0.010000
[ Sun Jul  7 15:50:16 2024 ] 	Batch(1200/7879) done. Loss: 2.2026  lr:0.010000
[ Sun Jul  7 15:50:39 2024 ] 	Batch(1300/7879) done. Loss: 1.5569  lr:0.010000
[ Sun Jul  7 15:51:03 2024 ] 	Batch(1400/7879) done. Loss: 0.2697  lr:0.010000
[ Sun Jul  7 15:51:26 2024 ] 
Training: Epoch [10/120], Step [1499], Loss: 0.80349200963974, Training Accuracy: 70.54166666666667
[ Sun Jul  7 15:51:26 2024 ] 	Batch(1500/7879) done. Loss: 0.5885  lr:0.010000
[ Sun Jul  7 15:51:50 2024 ] 	Batch(1600/7879) done. Loss: 1.0773  lr:0.010000
[ Sun Jul  7 15:52:12 2024 ] 	Batch(1700/7879) done. Loss: 0.8140  lr:0.010000
[ Sun Jul  7 15:52:35 2024 ] 	Batch(1800/7879) done. Loss: 1.7274  lr:0.010000
[ Sun Jul  7 15:52:58 2024 ] 	Batch(1900/7879) done. Loss: 0.5205  lr:0.010000
[ Sun Jul  7 15:53:20 2024 ] 
Training: Epoch [10/120], Step [1999], Loss: 0.7045041918754578, Training Accuracy: 70.64375000000001
[ Sun Jul  7 15:53:20 2024 ] 	Batch(2000/7879) done. Loss: 1.1804  lr:0.010000
[ Sun Jul  7 15:53:43 2024 ] 	Batch(2100/7879) done. Loss: 1.4652  lr:0.010000
[ Sun Jul  7 15:54:06 2024 ] 	Batch(2200/7879) done. Loss: 0.4068  lr:0.010000
[ Sun Jul  7 15:54:28 2024 ] 	Batch(2300/7879) done. Loss: 2.5364  lr:0.010000
[ Sun Jul  7 15:54:51 2024 ] 	Batch(2400/7879) done. Loss: 1.1960  lr:0.010000
[ Sun Jul  7 15:55:14 2024 ] 
Training: Epoch [10/120], Step [2499], Loss: 0.7667763829231262, Training Accuracy: 70.65
[ Sun Jul  7 15:55:14 2024 ] 	Batch(2500/7879) done. Loss: 1.9998  lr:0.010000
[ Sun Jul  7 15:55:37 2024 ] 	Batch(2600/7879) done. Loss: 1.1142  lr:0.010000
[ Sun Jul  7 15:55:59 2024 ] 	Batch(2700/7879) done. Loss: 2.0674  lr:0.010000
[ Sun Jul  7 15:56:22 2024 ] 	Batch(2800/7879) done. Loss: 0.7609  lr:0.010000
[ Sun Jul  7 15:56:45 2024 ] 	Batch(2900/7879) done. Loss: 0.5575  lr:0.010000
[ Sun Jul  7 15:57:07 2024 ] 
Training: Epoch [10/120], Step [2999], Loss: 0.4615393877029419, Training Accuracy: 70.42916666666666
[ Sun Jul  7 15:57:07 2024 ] 	Batch(3000/7879) done. Loss: 0.4227  lr:0.010000
[ Sun Jul  7 15:57:30 2024 ] 	Batch(3100/7879) done. Loss: 1.9617  lr:0.010000
[ Sun Jul  7 15:57:53 2024 ] 	Batch(3200/7879) done. Loss: 0.8470  lr:0.010000
[ Sun Jul  7 15:58:16 2024 ] 	Batch(3300/7879) done. Loss: 0.6429  lr:0.010000
[ Sun Jul  7 15:58:38 2024 ] 	Batch(3400/7879) done. Loss: 0.8613  lr:0.010000
[ Sun Jul  7 15:59:01 2024 ] 
Training: Epoch [10/120], Step [3499], Loss: 1.5326650142669678, Training Accuracy: 70.43571428571428
[ Sun Jul  7 15:59:01 2024 ] 	Batch(3500/7879) done. Loss: 0.8006  lr:0.010000
[ Sun Jul  7 15:59:24 2024 ] 	Batch(3600/7879) done. Loss: 0.3423  lr:0.010000
[ Sun Jul  7 15:59:47 2024 ] 	Batch(3700/7879) done. Loss: 0.6561  lr:0.010000
[ Sun Jul  7 16:00:09 2024 ] 	Batch(3800/7879) done. Loss: 1.5984  lr:0.010000
[ Sun Jul  7 16:00:32 2024 ] 	Batch(3900/7879) done. Loss: 0.6363  lr:0.010000
[ Sun Jul  7 16:00:55 2024 ] 
Training: Epoch [10/120], Step [3999], Loss: 0.5623195767402649, Training Accuracy: 70.440625
[ Sun Jul  7 16:00:55 2024 ] 	Batch(4000/7879) done. Loss: 1.3778  lr:0.010000
[ Sun Jul  7 16:01:18 2024 ] 	Batch(4100/7879) done. Loss: 0.8145  lr:0.010000
[ Sun Jul  7 16:01:40 2024 ] 	Batch(4200/7879) done. Loss: 1.0717  lr:0.010000
[ Sun Jul  7 16:02:03 2024 ] 	Batch(4300/7879) done. Loss: 1.8589  lr:0.010000
[ Sun Jul  7 16:02:26 2024 ] 	Batch(4400/7879) done. Loss: 0.6872  lr:0.010000
[ Sun Jul  7 16:02:48 2024 ] 
Training: Epoch [10/120], Step [4499], Loss: 1.0110621452331543, Training Accuracy: 70.24444444444444
[ Sun Jul  7 16:02:49 2024 ] 	Batch(4500/7879) done. Loss: 2.1368  lr:0.010000
[ Sun Jul  7 16:03:11 2024 ] 	Batch(4600/7879) done. Loss: 1.1983  lr:0.010000
[ Sun Jul  7 16:03:34 2024 ] 	Batch(4700/7879) done. Loss: 0.2841  lr:0.010000
[ Sun Jul  7 16:03:57 2024 ] 	Batch(4800/7879) done. Loss: 0.4693  lr:0.010000
[ Sun Jul  7 16:04:20 2024 ] 	Batch(4900/7879) done. Loss: 0.6241  lr:0.010000
[ Sun Jul  7 16:04:44 2024 ] 
Training: Epoch [10/120], Step [4999], Loss: 1.2184560298919678, Training Accuracy: 70.13000000000001
[ Sun Jul  7 16:04:44 2024 ] 	Batch(5000/7879) done. Loss: 0.9332  lr:0.010000
[ Sun Jul  7 16:05:07 2024 ] 	Batch(5100/7879) done. Loss: 0.8917  lr:0.010000
[ Sun Jul  7 16:05:30 2024 ] 	Batch(5200/7879) done. Loss: 1.3335  lr:0.010000
[ Sun Jul  7 16:05:52 2024 ] 	Batch(5300/7879) done. Loss: 1.2965  lr:0.010000
[ Sun Jul  7 16:06:15 2024 ] 	Batch(5400/7879) done. Loss: 0.9224  lr:0.010000
[ Sun Jul  7 16:06:38 2024 ] 
Training: Epoch [10/120], Step [5499], Loss: 1.4082165956497192, Training Accuracy: 70.22500000000001
[ Sun Jul  7 16:06:38 2024 ] 	Batch(5500/7879) done. Loss: 2.0745  lr:0.010000
[ Sun Jul  7 16:07:01 2024 ] 	Batch(5600/7879) done. Loss: 0.4665  lr:0.010000
[ Sun Jul  7 16:07:23 2024 ] 	Batch(5700/7879) done. Loss: 0.4397  lr:0.010000
[ Sun Jul  7 16:07:46 2024 ] 	Batch(5800/7879) done. Loss: 0.7575  lr:0.010000
[ Sun Jul  7 16:08:09 2024 ] 	Batch(5900/7879) done. Loss: 1.0314  lr:0.010000
[ Sun Jul  7 16:08:31 2024 ] 
Training: Epoch [10/120], Step [5999], Loss: 1.509687066078186, Training Accuracy: 70.21041666666666
[ Sun Jul  7 16:08:32 2024 ] 	Batch(6000/7879) done. Loss: 0.5009  lr:0.010000
[ Sun Jul  7 16:08:54 2024 ] 	Batch(6100/7879) done. Loss: 0.9039  lr:0.010000
[ Sun Jul  7 16:09:17 2024 ] 	Batch(6200/7879) done. Loss: 1.4666  lr:0.010000
[ Sun Jul  7 16:09:40 2024 ] 	Batch(6300/7879) done. Loss: 0.7301  lr:0.010000
[ Sun Jul  7 16:10:02 2024 ] 	Batch(6400/7879) done. Loss: 0.3302  lr:0.010000
[ Sun Jul  7 16:10:25 2024 ] 
Training: Epoch [10/120], Step [6499], Loss: 1.5745491981506348, Training Accuracy: 70.35961538461538
[ Sun Jul  7 16:10:25 2024 ] 	Batch(6500/7879) done. Loss: 0.3912  lr:0.010000
[ Sun Jul  7 16:10:48 2024 ] 	Batch(6600/7879) done. Loss: 0.9539  lr:0.010000
[ Sun Jul  7 16:11:10 2024 ] 	Batch(6700/7879) done. Loss: 0.8645  lr:0.010000
[ Sun Jul  7 16:11:33 2024 ] 	Batch(6800/7879) done. Loss: 0.8292  lr:0.010000
[ Sun Jul  7 16:11:56 2024 ] 	Batch(6900/7879) done. Loss: 0.5748  lr:0.010000
[ Sun Jul  7 16:12:18 2024 ] 
Training: Epoch [10/120], Step [6999], Loss: 0.7942473888397217, Training Accuracy: 70.34285714285714
[ Sun Jul  7 16:12:19 2024 ] 	Batch(7000/7879) done. Loss: 1.0908  lr:0.010000
[ Sun Jul  7 16:12:41 2024 ] 	Batch(7100/7879) done. Loss: 0.5607  lr:0.010000
[ Sun Jul  7 16:13:04 2024 ] 	Batch(7200/7879) done. Loss: 0.8584  lr:0.010000
[ Sun Jul  7 16:13:27 2024 ] 	Batch(7300/7879) done. Loss: 0.9497  lr:0.010000
[ Sun Jul  7 16:13:50 2024 ] 	Batch(7400/7879) done. Loss: 0.8888  lr:0.010000
[ Sun Jul  7 16:14:12 2024 ] 
Training: Epoch [10/120], Step [7499], Loss: 0.6664248108863831, Training Accuracy: 70.405
[ Sun Jul  7 16:14:12 2024 ] 	Batch(7500/7879) done. Loss: 1.5957  lr:0.010000
[ Sun Jul  7 16:14:35 2024 ] 	Batch(7600/7879) done. Loss: 1.7646  lr:0.010000
[ Sun Jul  7 16:14:58 2024 ] 	Batch(7700/7879) done. Loss: 2.2212  lr:0.010000
[ Sun Jul  7 16:15:20 2024 ] 	Batch(7800/7879) done. Loss: 0.4272  lr:0.010000
[ Sun Jul  7 16:15:38 2024 ] 	Mean training loss: 1.0252.
[ Sun Jul  7 16:15:38 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 16:15:38 2024 ] Training epoch: 12
[ Sun Jul  7 16:15:39 2024 ] 	Batch(0/7879) done. Loss: 1.5773  lr:0.010000
[ Sun Jul  7 16:16:02 2024 ] 	Batch(100/7879) done. Loss: 0.9016  lr:0.010000
[ Sun Jul  7 16:16:24 2024 ] 	Batch(200/7879) done. Loss: 0.3188  lr:0.010000
[ Sun Jul  7 16:16:47 2024 ] 	Batch(300/7879) done. Loss: 1.4875  lr:0.010000
[ Sun Jul  7 16:17:10 2024 ] 	Batch(400/7879) done. Loss: 0.5573  lr:0.010000
[ Sun Jul  7 16:17:32 2024 ] 
Training: Epoch [11/120], Step [499], Loss: 0.9759522080421448, Training Accuracy: 70.85000000000001
[ Sun Jul  7 16:17:33 2024 ] 	Batch(500/7879) done. Loss: 1.2600  lr:0.010000
[ Sun Jul  7 16:17:55 2024 ] 	Batch(600/7879) done. Loss: 1.4550  lr:0.010000
[ Sun Jul  7 16:18:18 2024 ] 	Batch(700/7879) done. Loss: 1.2641  lr:0.010000
[ Sun Jul  7 16:18:41 2024 ] 	Batch(800/7879) done. Loss: 0.6755  lr:0.010000
[ Sun Jul  7 16:19:04 2024 ] 	Batch(900/7879) done. Loss: 0.6208  lr:0.010000
[ Sun Jul  7 16:19:28 2024 ] 
Training: Epoch [11/120], Step [999], Loss: 1.003386378288269, Training Accuracy: 71.675
[ Sun Jul  7 16:19:28 2024 ] 	Batch(1000/7879) done. Loss: 0.4841  lr:0.010000
[ Sun Jul  7 16:19:51 2024 ] 	Batch(1100/7879) done. Loss: 1.1202  lr:0.010000
[ Sun Jul  7 16:20:15 2024 ] 	Batch(1200/7879) done. Loss: 1.2867  lr:0.010000
[ Sun Jul  7 16:20:38 2024 ] 	Batch(1300/7879) done. Loss: 1.5695  lr:0.010000
[ Sun Jul  7 16:21:01 2024 ] 	Batch(1400/7879) done. Loss: 1.6730  lr:0.010000
[ Sun Jul  7 16:21:23 2024 ] 
Training: Epoch [11/120], Step [1499], Loss: 0.24512767791748047, Training Accuracy: 71.75
[ Sun Jul  7 16:21:23 2024 ] 	Batch(1500/7879) done. Loss: 0.6250  lr:0.010000
[ Sun Jul  7 16:21:46 2024 ] 	Batch(1600/7879) done. Loss: 0.9567  lr:0.010000
[ Sun Jul  7 16:22:09 2024 ] 	Batch(1700/7879) done. Loss: 0.2411  lr:0.010000
[ Sun Jul  7 16:22:32 2024 ] 	Batch(1800/7879) done. Loss: 0.3265  lr:0.010000
[ Sun Jul  7 16:22:55 2024 ] 	Batch(1900/7879) done. Loss: 1.0066  lr:0.010000
[ Sun Jul  7 16:23:17 2024 ] 
Training: Epoch [11/120], Step [1999], Loss: 0.651100218296051, Training Accuracy: 71.3875
[ Sun Jul  7 16:23:17 2024 ] 	Batch(2000/7879) done. Loss: 0.3048  lr:0.010000
[ Sun Jul  7 16:23:40 2024 ] 	Batch(2100/7879) done. Loss: 1.2020  lr:0.010000
[ Sun Jul  7 16:24:03 2024 ] 	Batch(2200/7879) done. Loss: 0.3865  lr:0.010000
[ Sun Jul  7 16:24:25 2024 ] 	Batch(2300/7879) done. Loss: 1.3251  lr:0.010000
[ Sun Jul  7 16:24:49 2024 ] 	Batch(2400/7879) done. Loss: 1.1120  lr:0.010000
[ Sun Jul  7 16:25:12 2024 ] 
Training: Epoch [11/120], Step [2499], Loss: 0.9011106491088867, Training Accuracy: 71.17999999999999
[ Sun Jul  7 16:25:12 2024 ] 	Batch(2500/7879) done. Loss: 0.5473  lr:0.010000
[ Sun Jul  7 16:25:36 2024 ] 	Batch(2600/7879) done. Loss: 1.1473  lr:0.010000
[ Sun Jul  7 16:25:59 2024 ] 	Batch(2700/7879) done. Loss: 0.8203  lr:0.010000
[ Sun Jul  7 16:26:23 2024 ] 	Batch(2800/7879) done. Loss: 1.4230  lr:0.010000
[ Sun Jul  7 16:26:45 2024 ] 	Batch(2900/7879) done. Loss: 1.2806  lr:0.010000
[ Sun Jul  7 16:27:08 2024 ] 
Training: Epoch [11/120], Step [2999], Loss: 1.0813279151916504, Training Accuracy: 71.14166666666667
[ Sun Jul  7 16:27:08 2024 ] 	Batch(3000/7879) done. Loss: 1.1063  lr:0.010000
[ Sun Jul  7 16:27:31 2024 ] 	Batch(3100/7879) done. Loss: 0.3199  lr:0.010000
[ Sun Jul  7 16:27:54 2024 ] 	Batch(3200/7879) done. Loss: 1.3457  lr:0.010000
[ Sun Jul  7 16:28:17 2024 ] 	Batch(3300/7879) done. Loss: 1.4853  lr:0.010000
[ Sun Jul  7 16:28:41 2024 ] 	Batch(3400/7879) done. Loss: 0.4395  lr:0.010000
[ Sun Jul  7 16:29:04 2024 ] 
Training: Epoch [11/120], Step [3499], Loss: 1.4686963558197021, Training Accuracy: 71.15714285714286
[ Sun Jul  7 16:29:04 2024 ] 	Batch(3500/7879) done. Loss: 1.1904  lr:0.010000
[ Sun Jul  7 16:29:27 2024 ] 	Batch(3600/7879) done. Loss: 0.4422  lr:0.010000
[ Sun Jul  7 16:29:50 2024 ] 	Batch(3700/7879) done. Loss: 0.4254  lr:0.010000
[ Sun Jul  7 16:30:12 2024 ] 	Batch(3800/7879) done. Loss: 0.7446  lr:0.010000
[ Sun Jul  7 16:30:35 2024 ] 	Batch(3900/7879) done. Loss: 0.8415  lr:0.010000
[ Sun Jul  7 16:30:57 2024 ] 
Training: Epoch [11/120], Step [3999], Loss: 1.0516555309295654, Training Accuracy: 71.275
[ Sun Jul  7 16:30:58 2024 ] 	Batch(4000/7879) done. Loss: 0.6705  lr:0.010000
[ Sun Jul  7 16:31:21 2024 ] 	Batch(4100/7879) done. Loss: 0.7322  lr:0.010000
[ Sun Jul  7 16:31:43 2024 ] 	Batch(4200/7879) done. Loss: 1.7150  lr:0.010000
[ Sun Jul  7 16:32:06 2024 ] 	Batch(4300/7879) done. Loss: 0.6731  lr:0.010000
[ Sun Jul  7 16:32:29 2024 ] 	Batch(4400/7879) done. Loss: 1.4643  lr:0.010000
[ Sun Jul  7 16:32:51 2024 ] 
Training: Epoch [11/120], Step [4499], Loss: 1.4024393558502197, Training Accuracy: 71.41944444444445
[ Sun Jul  7 16:32:52 2024 ] 	Batch(4500/7879) done. Loss: 0.8378  lr:0.010000
[ Sun Jul  7 16:33:14 2024 ] 	Batch(4600/7879) done. Loss: 0.8851  lr:0.010000
[ Sun Jul  7 16:33:37 2024 ] 	Batch(4700/7879) done. Loss: 0.7905  lr:0.010000
[ Sun Jul  7 16:34:00 2024 ] 	Batch(4800/7879) done. Loss: 1.4300  lr:0.010000
[ Sun Jul  7 16:34:23 2024 ] 	Batch(4900/7879) done. Loss: 0.5113  lr:0.010000
[ Sun Jul  7 16:34:47 2024 ] 
Training: Epoch [11/120], Step [4999], Loss: 1.4836050271987915, Training Accuracy: 71.4275
[ Sun Jul  7 16:34:47 2024 ] 	Batch(5000/7879) done. Loss: 0.5600  lr:0.010000
[ Sun Jul  7 16:35:10 2024 ] 	Batch(5100/7879) done. Loss: 0.5366  lr:0.010000
[ Sun Jul  7 16:35:34 2024 ] 	Batch(5200/7879) done. Loss: 0.7742  lr:0.010000
[ Sun Jul  7 16:35:57 2024 ] 	Batch(5300/7879) done. Loss: 1.0372  lr:0.010000
[ Sun Jul  7 16:36:21 2024 ] 	Batch(5400/7879) done. Loss: 1.9133  lr:0.010000
[ Sun Jul  7 16:36:44 2024 ] 
Training: Epoch [11/120], Step [5499], Loss: 0.6575838327407837, Training Accuracy: 71.38863636363637
[ Sun Jul  7 16:36:44 2024 ] 	Batch(5500/7879) done. Loss: 1.5363  lr:0.010000
[ Sun Jul  7 16:37:07 2024 ] 	Batch(5600/7879) done. Loss: 0.6126  lr:0.010000
[ Sun Jul  7 16:37:30 2024 ] 	Batch(5700/7879) done. Loss: 0.7315  lr:0.010000
[ Sun Jul  7 16:37:52 2024 ] 	Batch(5800/7879) done. Loss: 1.1252  lr:0.010000
[ Sun Jul  7 16:38:15 2024 ] 	Batch(5900/7879) done. Loss: 0.4877  lr:0.010000
[ Sun Jul  7 16:38:38 2024 ] 
Training: Epoch [11/120], Step [5999], Loss: 1.1988019943237305, Training Accuracy: 71.41666666666666
[ Sun Jul  7 16:38:38 2024 ] 	Batch(6000/7879) done. Loss: 1.1622  lr:0.010000
[ Sun Jul  7 16:39:01 2024 ] 	Batch(6100/7879) done. Loss: 0.3771  lr:0.010000
[ Sun Jul  7 16:39:24 2024 ] 	Batch(6200/7879) done. Loss: 1.1282  lr:0.010000
[ Sun Jul  7 16:39:46 2024 ] 	Batch(6300/7879) done. Loss: 0.2240  lr:0.010000
[ Sun Jul  7 16:40:09 2024 ] 	Batch(6400/7879) done. Loss: 1.2229  lr:0.010000
[ Sun Jul  7 16:40:32 2024 ] 
Training: Epoch [11/120], Step [6499], Loss: 1.0374667644500732, Training Accuracy: 71.44615384615385
[ Sun Jul  7 16:40:32 2024 ] 	Batch(6500/7879) done. Loss: 0.3271  lr:0.010000
[ Sun Jul  7 16:40:55 2024 ] 	Batch(6600/7879) done. Loss: 0.6375  lr:0.010000
[ Sun Jul  7 16:41:17 2024 ] 	Batch(6700/7879) done. Loss: 0.3945  lr:0.010000
[ Sun Jul  7 16:41:40 2024 ] 	Batch(6800/7879) done. Loss: 0.4172  lr:0.010000
[ Sun Jul  7 16:42:03 2024 ] 	Batch(6900/7879) done. Loss: 1.2635  lr:0.010000
[ Sun Jul  7 16:42:25 2024 ] 
Training: Epoch [11/120], Step [6999], Loss: 0.4010393023490906, Training Accuracy: 71.4125
[ Sun Jul  7 16:42:26 2024 ] 	Batch(7000/7879) done. Loss: 0.5139  lr:0.010000
[ Sun Jul  7 16:42:48 2024 ] 	Batch(7100/7879) done. Loss: 1.1529  lr:0.010000
[ Sun Jul  7 16:43:11 2024 ] 	Batch(7200/7879) done. Loss: 1.2775  lr:0.010000
[ Sun Jul  7 16:43:34 2024 ] 	Batch(7300/7879) done. Loss: 0.6683  lr:0.010000
[ Sun Jul  7 16:43:57 2024 ] 	Batch(7400/7879) done. Loss: 1.3004  lr:0.010000
[ Sun Jul  7 16:44:19 2024 ] 
Training: Epoch [11/120], Step [7499], Loss: 1.3080518245697021, Training Accuracy: 71.44166666666666
[ Sun Jul  7 16:44:19 2024 ] 	Batch(7500/7879) done. Loss: 1.5455  lr:0.010000
[ Sun Jul  7 16:44:42 2024 ] 	Batch(7600/7879) done. Loss: 1.1183  lr:0.010000
[ Sun Jul  7 16:45:05 2024 ] 	Batch(7700/7879) done. Loss: 0.4445  lr:0.010000
[ Sun Jul  7 16:45:28 2024 ] 	Batch(7800/7879) done. Loss: 0.9594  lr:0.010000
[ Sun Jul  7 16:45:45 2024 ] 	Mean training loss: 0.9748.
[ Sun Jul  7 16:45:45 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 16:45:45 2024 ] Training epoch: 13
[ Sun Jul  7 16:45:46 2024 ] 	Batch(0/7879) done. Loss: 1.0346  lr:0.010000
[ Sun Jul  7 16:46:09 2024 ] 	Batch(100/7879) done. Loss: 1.1501  lr:0.010000
[ Sun Jul  7 16:46:33 2024 ] 	Batch(200/7879) done. Loss: 1.9136  lr:0.010000
[ Sun Jul  7 16:46:56 2024 ] 	Batch(300/7879) done. Loss: 1.6502  lr:0.010000
[ Sun Jul  7 16:47:19 2024 ] 	Batch(400/7879) done. Loss: 1.0837  lr:0.010000
[ Sun Jul  7 16:47:41 2024 ] 
Training: Epoch [12/120], Step [499], Loss: 1.8950650691986084, Training Accuracy: 73.35000000000001
[ Sun Jul  7 16:47:41 2024 ] 	Batch(500/7879) done. Loss: 1.3660  lr:0.010000
[ Sun Jul  7 16:48:04 2024 ] 	Batch(600/7879) done. Loss: 1.0115  lr:0.010000
[ Sun Jul  7 16:48:27 2024 ] 	Batch(700/7879) done. Loss: 1.5164  lr:0.010000
[ Sun Jul  7 16:48:50 2024 ] 	Batch(800/7879) done. Loss: 2.1929  lr:0.010000
[ Sun Jul  7 16:49:12 2024 ] 	Batch(900/7879) done. Loss: 0.1719  lr:0.010000
[ Sun Jul  7 16:49:35 2024 ] 
Training: Epoch [12/120], Step [999], Loss: 0.7810794711112976, Training Accuracy: 72.6125
[ Sun Jul  7 16:49:35 2024 ] 	Batch(1000/7879) done. Loss: 0.9171  lr:0.010000
[ Sun Jul  7 16:49:58 2024 ] 	Batch(1100/7879) done. Loss: 0.3701  lr:0.010000
[ Sun Jul  7 16:50:21 2024 ] 	Batch(1200/7879) done. Loss: 0.7554  lr:0.010000
[ Sun Jul  7 16:50:44 2024 ] 	Batch(1300/7879) done. Loss: 0.6849  lr:0.010000
[ Sun Jul  7 16:51:08 2024 ] 	Batch(1400/7879) done. Loss: 1.5091  lr:0.010000
[ Sun Jul  7 16:51:31 2024 ] 
Training: Epoch [12/120], Step [1499], Loss: 1.3466426134109497, Training Accuracy: 72.68333333333334
[ Sun Jul  7 16:51:31 2024 ] 	Batch(1500/7879) done. Loss: 0.8981  lr:0.010000
[ Sun Jul  7 16:51:55 2024 ] 	Batch(1600/7879) done. Loss: 0.7483  lr:0.010000
[ Sun Jul  7 16:52:18 2024 ] 	Batch(1700/7879) done. Loss: 0.8889  lr:0.010000
[ Sun Jul  7 16:52:41 2024 ] 	Batch(1800/7879) done. Loss: 0.5694  lr:0.010000
[ Sun Jul  7 16:53:05 2024 ] 	Batch(1900/7879) done. Loss: 1.1798  lr:0.010000
[ Sun Jul  7 16:53:28 2024 ] 
Training: Epoch [12/120], Step [1999], Loss: 0.8374578356742859, Training Accuracy: 72.60625
[ Sun Jul  7 16:53:28 2024 ] 	Batch(2000/7879) done. Loss: 0.7604  lr:0.010000
[ Sun Jul  7 16:53:51 2024 ] 	Batch(2100/7879) done. Loss: 1.3248  lr:0.010000
[ Sun Jul  7 16:54:13 2024 ] 	Batch(2200/7879) done. Loss: 0.2833  lr:0.010000
[ Sun Jul  7 16:54:36 2024 ] 	Batch(2300/7879) done. Loss: 1.0361  lr:0.010000
[ Sun Jul  7 16:54:59 2024 ] 	Batch(2400/7879) done. Loss: 0.7260  lr:0.010000
[ Sun Jul  7 16:55:21 2024 ] 
Training: Epoch [12/120], Step [2499], Loss: 1.1284852027893066, Training Accuracy: 72.53
[ Sun Jul  7 16:55:22 2024 ] 	Batch(2500/7879) done. Loss: 1.4795  lr:0.010000
[ Sun Jul  7 16:55:44 2024 ] 	Batch(2600/7879) done. Loss: 0.9245  lr:0.010000
[ Sun Jul  7 16:56:07 2024 ] 	Batch(2700/7879) done. Loss: 0.8390  lr:0.010000
[ Sun Jul  7 16:56:30 2024 ] 	Batch(2800/7879) done. Loss: 0.4309  lr:0.010000
[ Sun Jul  7 16:56:53 2024 ] 	Batch(2900/7879) done. Loss: 0.6148  lr:0.010000
[ Sun Jul  7 16:57:17 2024 ] 
Training: Epoch [12/120], Step [2999], Loss: 1.0811638832092285, Training Accuracy: 72.78333333333333
[ Sun Jul  7 16:57:17 2024 ] 	Batch(3000/7879) done. Loss: 0.7610  lr:0.010000
[ Sun Jul  7 16:57:39 2024 ] 	Batch(3100/7879) done. Loss: 0.3514  lr:0.010000
[ Sun Jul  7 16:58:02 2024 ] 	Batch(3200/7879) done. Loss: 0.6635  lr:0.010000
[ Sun Jul  7 16:58:25 2024 ] 	Batch(3300/7879) done. Loss: 2.1719  lr:0.010000
[ Sun Jul  7 16:58:48 2024 ] 	Batch(3400/7879) done. Loss: 1.7967  lr:0.010000
[ Sun Jul  7 16:59:10 2024 ] 
Training: Epoch [12/120], Step [3499], Loss: 2.037973642349243, Training Accuracy: 72.99285714285715
[ Sun Jul  7 16:59:10 2024 ] 	Batch(3500/7879) done. Loss: 0.9133  lr:0.010000
[ Sun Jul  7 16:59:33 2024 ] 	Batch(3600/7879) done. Loss: 0.5759  lr:0.010000
[ Sun Jul  7 16:59:56 2024 ] 	Batch(3700/7879) done. Loss: 0.0496  lr:0.010000
[ Sun Jul  7 17:00:19 2024 ] 	Batch(3800/7879) done. Loss: 0.8151  lr:0.010000
[ Sun Jul  7 17:00:42 2024 ] 	Batch(3900/7879) done. Loss: 2.0010  lr:0.010000
[ Sun Jul  7 17:01:05 2024 ] 
Training: Epoch [12/120], Step [3999], Loss: 0.6603580713272095, Training Accuracy: 72.93437499999999
[ Sun Jul  7 17:01:05 2024 ] 	Batch(4000/7879) done. Loss: 1.1434  lr:0.010000
[ Sun Jul  7 17:01:28 2024 ] 	Batch(4100/7879) done. Loss: 0.3809  lr:0.010000
[ Sun Jul  7 17:01:50 2024 ] 	Batch(4200/7879) done. Loss: 2.0232  lr:0.010000
[ Sun Jul  7 17:02:13 2024 ] 	Batch(4300/7879) done. Loss: 0.1562  lr:0.010000
[ Sun Jul  7 17:02:36 2024 ] 	Batch(4400/7879) done. Loss: 0.4520  lr:0.010000
[ Sun Jul  7 17:02:58 2024 ] 
Training: Epoch [12/120], Step [4499], Loss: 0.559089183807373, Training Accuracy: 72.88055555555556
[ Sun Jul  7 17:02:58 2024 ] 	Batch(4500/7879) done. Loss: 0.7783  lr:0.010000
[ Sun Jul  7 17:03:21 2024 ] 	Batch(4600/7879) done. Loss: 0.8000  lr:0.010000
[ Sun Jul  7 17:03:44 2024 ] 	Batch(4700/7879) done. Loss: 0.2898  lr:0.010000
[ Sun Jul  7 17:04:07 2024 ] 	Batch(4800/7879) done. Loss: 0.9003  lr:0.010000
[ Sun Jul  7 17:04:30 2024 ] 	Batch(4900/7879) done. Loss: 0.7603  lr:0.010000
[ Sun Jul  7 17:04:53 2024 ] 
Training: Epoch [12/120], Step [4999], Loss: 1.0807384252548218, Training Accuracy: 72.9025
[ Sun Jul  7 17:04:54 2024 ] 	Batch(5000/7879) done. Loss: 0.6688  lr:0.010000
[ Sun Jul  7 17:05:17 2024 ] 	Batch(5100/7879) done. Loss: 1.8012  lr:0.010000
[ Sun Jul  7 17:05:40 2024 ] 	Batch(5200/7879) done. Loss: 0.9630  lr:0.010000
[ Sun Jul  7 17:06:03 2024 ] 	Batch(5300/7879) done. Loss: 0.8950  lr:0.010000
[ Sun Jul  7 17:06:26 2024 ] 	Batch(5400/7879) done. Loss: 1.3737  lr:0.010000
[ Sun Jul  7 17:06:48 2024 ] 
Training: Epoch [12/120], Step [5499], Loss: 0.7330365777015686, Training Accuracy: 72.81590909090909
[ Sun Jul  7 17:06:49 2024 ] 	Batch(5500/7879) done. Loss: 0.7947  lr:0.010000
[ Sun Jul  7 17:07:11 2024 ] 	Batch(5600/7879) done. Loss: 1.1425  lr:0.010000
[ Sun Jul  7 17:07:35 2024 ] 	Batch(5700/7879) done. Loss: 1.7409  lr:0.010000
[ Sun Jul  7 17:07:58 2024 ] 	Batch(5800/7879) done. Loss: 1.1341  lr:0.010000
[ Sun Jul  7 17:08:22 2024 ] 	Batch(5900/7879) done. Loss: 0.7547  lr:0.010000
[ Sun Jul  7 17:08:44 2024 ] 
Training: Epoch [12/120], Step [5999], Loss: 1.6098122596740723, Training Accuracy: 72.86041666666667
[ Sun Jul  7 17:08:44 2024 ] 	Batch(6000/7879) done. Loss: 0.8903  lr:0.010000
[ Sun Jul  7 17:09:07 2024 ] 	Batch(6100/7879) done. Loss: 1.2610  lr:0.010000
[ Sun Jul  7 17:09:30 2024 ] 	Batch(6200/7879) done. Loss: 2.9107  lr:0.010000
[ Sun Jul  7 17:09:52 2024 ] 	Batch(6300/7879) done. Loss: 1.6184  lr:0.010000
[ Sun Jul  7 17:10:15 2024 ] 	Batch(6400/7879) done. Loss: 0.9798  lr:0.010000
[ Sun Jul  7 17:10:38 2024 ] 
Training: Epoch [12/120], Step [6499], Loss: 0.20392479002475739, Training Accuracy: 72.79615384615384
[ Sun Jul  7 17:10:38 2024 ] 	Batch(6500/7879) done. Loss: 1.6402  lr:0.010000
[ Sun Jul  7 17:11:01 2024 ] 	Batch(6600/7879) done. Loss: 0.6899  lr:0.010000
[ Sun Jul  7 17:11:24 2024 ] 	Batch(6700/7879) done. Loss: 1.1921  lr:0.010000
[ Sun Jul  7 17:11:47 2024 ] 	Batch(6800/7879) done. Loss: 1.3999  lr:0.010000
[ Sun Jul  7 17:12:09 2024 ] 	Batch(6900/7879) done. Loss: 0.9561  lr:0.010000
[ Sun Jul  7 17:12:32 2024 ] 
Training: Epoch [12/120], Step [6999], Loss: 1.152533769607544, Training Accuracy: 72.83571428571427
[ Sun Jul  7 17:12:32 2024 ] 	Batch(7000/7879) done. Loss: 0.6823  lr:0.010000
[ Sun Jul  7 17:12:55 2024 ] 	Batch(7100/7879) done. Loss: 1.1373  lr:0.010000
[ Sun Jul  7 17:13:17 2024 ] 	Batch(7200/7879) done. Loss: 1.1943  lr:0.010000
[ Sun Jul  7 17:13:40 2024 ] 	Batch(7300/7879) done. Loss: 0.9428  lr:0.010000
[ Sun Jul  7 17:14:03 2024 ] 	Batch(7400/7879) done. Loss: 2.0613  lr:0.010000
[ Sun Jul  7 17:14:25 2024 ] 
Training: Epoch [12/120], Step [7499], Loss: 0.7924215197563171, Training Accuracy: 72.77
[ Sun Jul  7 17:14:26 2024 ] 	Batch(7500/7879) done. Loss: 1.0785  lr:0.010000
[ Sun Jul  7 17:14:48 2024 ] 	Batch(7600/7879) done. Loss: 0.6543  lr:0.010000
[ Sun Jul  7 17:15:11 2024 ] 	Batch(7700/7879) done. Loss: 0.5223  lr:0.010000
[ Sun Jul  7 17:15:34 2024 ] 	Batch(7800/7879) done. Loss: 1.0354  lr:0.010000
[ Sun Jul  7 17:15:51 2024 ] 	Mean training loss: 0.9297.
[ Sun Jul  7 17:15:51 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 17:15:52 2024 ] Training epoch: 14
[ Sun Jul  7 17:15:52 2024 ] 	Batch(0/7879) done. Loss: 1.0824  lr:0.010000
[ Sun Jul  7 17:16:15 2024 ] 	Batch(100/7879) done. Loss: 0.8004  lr:0.010000
[ Sun Jul  7 17:16:37 2024 ] 	Batch(200/7879) done. Loss: 0.8477  lr:0.010000
[ Sun Jul  7 17:17:00 2024 ] 	Batch(300/7879) done. Loss: 0.9548  lr:0.010000
[ Sun Jul  7 17:17:23 2024 ] 	Batch(400/7879) done. Loss: 0.8273  lr:0.010000
[ Sun Jul  7 17:17:46 2024 ] 
Training: Epoch [13/120], Step [499], Loss: 0.7544413805007935, Training Accuracy: 73.32499999999999
[ Sun Jul  7 17:17:46 2024 ] 	Batch(500/7879) done. Loss: 1.0050  lr:0.010000
[ Sun Jul  7 17:18:09 2024 ] 	Batch(600/7879) done. Loss: 1.1013  lr:0.010000
[ Sun Jul  7 17:18:31 2024 ] 	Batch(700/7879) done. Loss: 0.3376  lr:0.010000
[ Sun Jul  7 17:18:54 2024 ] 	Batch(800/7879) done. Loss: 0.3431  lr:0.010000
[ Sun Jul  7 17:19:17 2024 ] 	Batch(900/7879) done. Loss: 1.0879  lr:0.010000
[ Sun Jul  7 17:19:39 2024 ] 
Training: Epoch [13/120], Step [999], Loss: 0.7291868329048157, Training Accuracy: 74.075
[ Sun Jul  7 17:19:40 2024 ] 	Batch(1000/7879) done. Loss: 0.9457  lr:0.010000
[ Sun Jul  7 17:20:02 2024 ] 	Batch(1100/7879) done. Loss: 0.2659  lr:0.010000
[ Sun Jul  7 17:20:25 2024 ] 	Batch(1200/7879) done. Loss: 0.8806  lr:0.010000
[ Sun Jul  7 17:20:48 2024 ] 	Batch(1300/7879) done. Loss: 0.5714  lr:0.010000
[ Sun Jul  7 17:21:11 2024 ] 	Batch(1400/7879) done. Loss: 1.8098  lr:0.010000
[ Sun Jul  7 17:21:33 2024 ] 
Training: Epoch [13/120], Step [1499], Loss: 0.2668446898460388, Training Accuracy: 74.43333333333332
[ Sun Jul  7 17:21:33 2024 ] 	Batch(1500/7879) done. Loss: 0.8807  lr:0.010000
[ Sun Jul  7 17:21:56 2024 ] 	Batch(1600/7879) done. Loss: 1.1561  lr:0.010000
[ Sun Jul  7 17:22:19 2024 ] 	Batch(1700/7879) done. Loss: 1.1645  lr:0.010000
[ Sun Jul  7 17:22:41 2024 ] 	Batch(1800/7879) done. Loss: 1.3249  lr:0.010000
[ Sun Jul  7 17:23:04 2024 ] 	Batch(1900/7879) done. Loss: 0.1440  lr:0.010000
[ Sun Jul  7 17:23:27 2024 ] 
Training: Epoch [13/120], Step [1999], Loss: 1.8522180318832397, Training Accuracy: 74.36874999999999
[ Sun Jul  7 17:23:27 2024 ] 	Batch(2000/7879) done. Loss: 1.3984  lr:0.010000
[ Sun Jul  7 17:23:50 2024 ] 	Batch(2100/7879) done. Loss: 0.7501  lr:0.010000
[ Sun Jul  7 17:24:12 2024 ] 	Batch(2200/7879) done. Loss: 1.0388  lr:0.010000
[ Sun Jul  7 17:24:35 2024 ] 	Batch(2300/7879) done. Loss: 0.8382  lr:0.010000
[ Sun Jul  7 17:24:58 2024 ] 	Batch(2400/7879) done. Loss: 1.4141  lr:0.010000
[ Sun Jul  7 17:25:20 2024 ] 
Training: Epoch [13/120], Step [2499], Loss: 1.542487382888794, Training Accuracy: 74.295
[ Sun Jul  7 17:25:21 2024 ] 	Batch(2500/7879) done. Loss: 0.1118  lr:0.010000
[ Sun Jul  7 17:25:43 2024 ] 	Batch(2600/7879) done. Loss: 1.0936  lr:0.010000
[ Sun Jul  7 17:26:06 2024 ] 	Batch(2700/7879) done. Loss: 1.0390  lr:0.010000
[ Sun Jul  7 17:26:29 2024 ] 	Batch(2800/7879) done. Loss: 0.7310  lr:0.010000
[ Sun Jul  7 17:26:52 2024 ] 	Batch(2900/7879) done. Loss: 0.5519  lr:0.010000
[ Sun Jul  7 17:27:14 2024 ] 
Training: Epoch [13/120], Step [2999], Loss: 1.210377812385559, Training Accuracy: 74.04166666666666
[ Sun Jul  7 17:27:14 2024 ] 	Batch(3000/7879) done. Loss: 1.0583  lr:0.010000
[ Sun Jul  7 17:27:37 2024 ] 	Batch(3100/7879) done. Loss: 1.1565  lr:0.010000
[ Sun Jul  7 17:28:00 2024 ] 	Batch(3200/7879) done. Loss: 0.6847  lr:0.010000
[ Sun Jul  7 17:28:23 2024 ] 	Batch(3300/7879) done. Loss: 0.4530  lr:0.010000
[ Sun Jul  7 17:28:47 2024 ] 	Batch(3400/7879) done. Loss: 2.1534  lr:0.010000
[ Sun Jul  7 17:29:10 2024 ] 
Training: Epoch [13/120], Step [3499], Loss: 1.4764600992202759, Training Accuracy: 73.83214285714286
[ Sun Jul  7 17:29:10 2024 ] 	Batch(3500/7879) done. Loss: 0.5859  lr:0.010000
[ Sun Jul  7 17:29:34 2024 ] 	Batch(3600/7879) done. Loss: 0.0636  lr:0.010000
[ Sun Jul  7 17:29:56 2024 ] 	Batch(3700/7879) done. Loss: 0.3781  lr:0.010000
[ Sun Jul  7 17:30:19 2024 ] 	Batch(3800/7879) done. Loss: 0.4584  lr:0.010000
[ Sun Jul  7 17:30:42 2024 ] 	Batch(3900/7879) done. Loss: 0.1155  lr:0.010000
[ Sun Jul  7 17:31:04 2024 ] 
Training: Epoch [13/120], Step [3999], Loss: 1.2133032083511353, Training Accuracy: 73.796875
[ Sun Jul  7 17:31:05 2024 ] 	Batch(4000/7879) done. Loss: 0.8329  lr:0.010000
[ Sun Jul  7 17:31:28 2024 ] 	Batch(4100/7879) done. Loss: 0.9638  lr:0.010000
[ Sun Jul  7 17:31:50 2024 ] 	Batch(4200/7879) done. Loss: 0.5244  lr:0.010000
[ Sun Jul  7 17:32:13 2024 ] 	Batch(4300/7879) done. Loss: 0.5842  lr:0.010000
[ Sun Jul  7 17:32:36 2024 ] 	Batch(4400/7879) done. Loss: 1.2243  lr:0.010000
[ Sun Jul  7 17:32:58 2024 ] 
Training: Epoch [13/120], Step [4499], Loss: 0.8803023099899292, Training Accuracy: 73.70555555555556
[ Sun Jul  7 17:32:59 2024 ] 	Batch(4500/7879) done. Loss: 0.9484  lr:0.010000
[ Sun Jul  7 17:33:21 2024 ] 	Batch(4600/7879) done. Loss: 1.8680  lr:0.010000
[ Sun Jul  7 17:33:44 2024 ] 	Batch(4700/7879) done. Loss: 0.9606  lr:0.010000
[ Sun Jul  7 17:34:07 2024 ] 	Batch(4800/7879) done. Loss: 1.0764  lr:0.010000
[ Sun Jul  7 17:34:30 2024 ] 	Batch(4900/7879) done. Loss: 0.3748  lr:0.010000
[ Sun Jul  7 17:34:52 2024 ] 
Training: Epoch [13/120], Step [4999], Loss: 0.8735243678092957, Training Accuracy: 73.6175
[ Sun Jul  7 17:34:52 2024 ] 	Batch(5000/7879) done. Loss: 1.6469  lr:0.010000
[ Sun Jul  7 17:35:15 2024 ] 	Batch(5100/7879) done. Loss: 1.2464  lr:0.010000
[ Sun Jul  7 17:35:38 2024 ] 	Batch(5200/7879) done. Loss: 1.7221  lr:0.010000
[ Sun Jul  7 17:36:01 2024 ] 	Batch(5300/7879) done. Loss: 0.8832  lr:0.010000
[ Sun Jul  7 17:36:25 2024 ] 	Batch(5400/7879) done. Loss: 0.8189  lr:0.010000
[ Sun Jul  7 17:36:48 2024 ] 
Training: Epoch [13/120], Step [5499], Loss: 0.6065905690193176, Training Accuracy: 73.51136363636364
[ Sun Jul  7 17:36:48 2024 ] 	Batch(5500/7879) done. Loss: 1.2290  lr:0.010000
[ Sun Jul  7 17:37:12 2024 ] 	Batch(5600/7879) done. Loss: 0.8747  lr:0.010000
[ Sun Jul  7 17:37:35 2024 ] 	Batch(5700/7879) done. Loss: 1.1030  lr:0.010000
[ Sun Jul  7 17:37:59 2024 ] 	Batch(5800/7879) done. Loss: 1.4433  lr:0.010000
[ Sun Jul  7 17:38:22 2024 ] 	Batch(5900/7879) done. Loss: 0.8152  lr:0.010000
[ Sun Jul  7 17:38:45 2024 ] 
Training: Epoch [13/120], Step [5999], Loss: 0.8291712999343872, Training Accuracy: 73.5375
[ Sun Jul  7 17:38:46 2024 ] 	Batch(6000/7879) done. Loss: 1.3842  lr:0.010000
[ Sun Jul  7 17:39:09 2024 ] 	Batch(6100/7879) done. Loss: 1.9752  lr:0.010000
[ Sun Jul  7 17:39:31 2024 ] 	Batch(6200/7879) done. Loss: 0.6128  lr:0.010000
[ Sun Jul  7 17:39:54 2024 ] 	Batch(6300/7879) done. Loss: 1.6583  lr:0.010000
[ Sun Jul  7 17:40:17 2024 ] 	Batch(6400/7879) done. Loss: 0.8114  lr:0.010000
[ Sun Jul  7 17:40:39 2024 ] 
Training: Epoch [13/120], Step [6499], Loss: 0.9586845636367798, Training Accuracy: 73.51730769230768
[ Sun Jul  7 17:40:39 2024 ] 	Batch(6500/7879) done. Loss: 0.3184  lr:0.010000
[ Sun Jul  7 17:41:02 2024 ] 	Batch(6600/7879) done. Loss: 0.4912  lr:0.010000
[ Sun Jul  7 17:41:25 2024 ] 	Batch(6700/7879) done. Loss: 0.4334  lr:0.010000
[ Sun Jul  7 17:41:48 2024 ] 	Batch(6800/7879) done. Loss: 0.6547  lr:0.010000
[ Sun Jul  7 17:42:10 2024 ] 	Batch(6900/7879) done. Loss: 1.6804  lr:0.010000
[ Sun Jul  7 17:42:33 2024 ] 
Training: Epoch [13/120], Step [6999], Loss: 1.4017467498779297, Training Accuracy: 73.52499999999999
[ Sun Jul  7 17:42:33 2024 ] 	Batch(7000/7879) done. Loss: 1.4253  lr:0.010000
[ Sun Jul  7 17:42:56 2024 ] 	Batch(7100/7879) done. Loss: 0.4617  lr:0.010000
[ Sun Jul  7 17:43:19 2024 ] 	Batch(7200/7879) done. Loss: 0.2368  lr:0.010000
[ Sun Jul  7 17:43:41 2024 ] 	Batch(7300/7879) done. Loss: 0.2751  lr:0.010000
[ Sun Jul  7 17:44:04 2024 ] 	Batch(7400/7879) done. Loss: 1.2217  lr:0.010000
[ Sun Jul  7 17:44:27 2024 ] 
Training: Epoch [13/120], Step [7499], Loss: 0.5189207792282104, Training Accuracy: 73.575
[ Sun Jul  7 17:44:27 2024 ] 	Batch(7500/7879) done. Loss: 0.3248  lr:0.010000
[ Sun Jul  7 17:44:50 2024 ] 	Batch(7600/7879) done. Loss: 1.0239  lr:0.010000
[ Sun Jul  7 17:45:13 2024 ] 	Batch(7700/7879) done. Loss: 0.9466  lr:0.010000
[ Sun Jul  7 17:45:36 2024 ] 	Batch(7800/7879) done. Loss: 1.0521  lr:0.010000
[ Sun Jul  7 17:45:54 2024 ] 	Mean training loss: 0.8874.
[ Sun Jul  7 17:45:54 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 17:45:54 2024 ] Training epoch: 15
[ Sun Jul  7 17:45:55 2024 ] 	Batch(0/7879) done. Loss: 0.2675  lr:0.010000
[ Sun Jul  7 17:46:18 2024 ] 	Batch(100/7879) done. Loss: 0.6168  lr:0.010000
[ Sun Jul  7 17:46:41 2024 ] 	Batch(200/7879) done. Loss: 0.6433  lr:0.010000
[ Sun Jul  7 17:47:04 2024 ] 	Batch(300/7879) done. Loss: 0.7808  lr:0.010000
[ Sun Jul  7 17:47:27 2024 ] 	Batch(400/7879) done. Loss: 1.2891  lr:0.010000
[ Sun Jul  7 17:47:50 2024 ] 
Training: Epoch [14/120], Step [499], Loss: 0.15796130895614624, Training Accuracy: 74.4
[ Sun Jul  7 17:47:50 2024 ] 	Batch(500/7879) done. Loss: 1.8766  lr:0.010000
[ Sun Jul  7 17:48:13 2024 ] 	Batch(600/7879) done. Loss: 0.8351  lr:0.010000
[ Sun Jul  7 17:48:36 2024 ] 	Batch(700/7879) done. Loss: 1.1507  lr:0.010000
[ Sun Jul  7 17:49:00 2024 ] 	Batch(800/7879) done. Loss: 0.7956  lr:0.010000
[ Sun Jul  7 17:49:23 2024 ] 	Batch(900/7879) done. Loss: 0.8129  lr:0.010000
[ Sun Jul  7 17:49:46 2024 ] 
Training: Epoch [14/120], Step [999], Loss: 1.046032190322876, Training Accuracy: 74.05000000000001
[ Sun Jul  7 17:49:46 2024 ] 	Batch(1000/7879) done. Loss: 1.1143  lr:0.010000
[ Sun Jul  7 17:50:09 2024 ] 	Batch(1100/7879) done. Loss: 0.4901  lr:0.010000
[ Sun Jul  7 17:50:32 2024 ] 	Batch(1200/7879) done. Loss: 1.4856  lr:0.010000
[ Sun Jul  7 17:50:55 2024 ] 	Batch(1300/7879) done. Loss: 0.8638  lr:0.010000
[ Sun Jul  7 17:51:19 2024 ] 	Batch(1400/7879) done. Loss: 1.5248  lr:0.010000
[ Sun Jul  7 17:51:42 2024 ] 
Training: Epoch [14/120], Step [1499], Loss: 0.2533774971961975, Training Accuracy: 74.34166666666667
[ Sun Jul  7 17:51:42 2024 ] 	Batch(1500/7879) done. Loss: 1.0661  lr:0.010000
[ Sun Jul  7 17:52:05 2024 ] 	Batch(1600/7879) done. Loss: 1.1390  lr:0.010000
[ Sun Jul  7 17:52:29 2024 ] 	Batch(1700/7879) done. Loss: 1.2994  lr:0.010000
[ Sun Jul  7 17:52:52 2024 ] 	Batch(1800/7879) done. Loss: 0.8064  lr:0.010000
[ Sun Jul  7 17:53:15 2024 ] 	Batch(1900/7879) done. Loss: 1.0286  lr:0.010000
[ Sun Jul  7 17:53:39 2024 ] 
Training: Epoch [14/120], Step [1999], Loss: 0.42336946725845337, Training Accuracy: 74.38749999999999
[ Sun Jul  7 17:53:39 2024 ] 	Batch(2000/7879) done. Loss: 0.7349  lr:0.010000
[ Sun Jul  7 17:54:02 2024 ] 	Batch(2100/7879) done. Loss: 0.7655  lr:0.010000
[ Sun Jul  7 17:54:26 2024 ] 	Batch(2200/7879) done. Loss: 0.8860  lr:0.010000
[ Sun Jul  7 17:54:49 2024 ] 	Batch(2300/7879) done. Loss: 0.7334  lr:0.010000
[ Sun Jul  7 17:55:12 2024 ] 	Batch(2400/7879) done. Loss: 0.8482  lr:0.010000
[ Sun Jul  7 17:55:36 2024 ] 
Training: Epoch [14/120], Step [2499], Loss: 1.2200149297714233, Training Accuracy: 74.39
[ Sun Jul  7 17:55:36 2024 ] 	Batch(2500/7879) done. Loss: 0.4932  lr:0.010000
[ Sun Jul  7 17:55:59 2024 ] 	Batch(2600/7879) done. Loss: 0.3696  lr:0.010000
[ Sun Jul  7 17:56:22 2024 ] 	Batch(2700/7879) done. Loss: 0.8882  lr:0.010000
[ Sun Jul  7 17:56:46 2024 ] 	Batch(2800/7879) done. Loss: 0.7499  lr:0.010000
[ Sun Jul  7 17:57:09 2024 ] 	Batch(2900/7879) done. Loss: 1.4435  lr:0.010000
[ Sun Jul  7 17:57:32 2024 ] 
Training: Epoch [14/120], Step [2999], Loss: 0.27091091871261597, Training Accuracy: 74.30833333333334
[ Sun Jul  7 17:57:33 2024 ] 	Batch(3000/7879) done. Loss: 0.6015  lr:0.010000
[ Sun Jul  7 17:57:56 2024 ] 	Batch(3100/7879) done. Loss: 0.3773  lr:0.010000
[ Sun Jul  7 17:58:19 2024 ] 	Batch(3200/7879) done. Loss: 1.4807  lr:0.010000
[ Sun Jul  7 17:58:43 2024 ] 	Batch(3300/7879) done. Loss: 0.8011  lr:0.010000
[ Sun Jul  7 17:59:06 2024 ] 	Batch(3400/7879) done. Loss: 0.4851  lr:0.010000
[ Sun Jul  7 17:59:28 2024 ] 
Training: Epoch [14/120], Step [3499], Loss: 1.2645132541656494, Training Accuracy: 74.13571428571429
[ Sun Jul  7 17:59:28 2024 ] 	Batch(3500/7879) done. Loss: 0.7408  lr:0.010000
[ Sun Jul  7 17:59:51 2024 ] 	Batch(3600/7879) done. Loss: 0.2758  lr:0.010000
[ Sun Jul  7 18:00:14 2024 ] 	Batch(3700/7879) done. Loss: 0.1694  lr:0.010000
[ Sun Jul  7 18:00:36 2024 ] 	Batch(3800/7879) done. Loss: 0.9757  lr:0.010000
[ Sun Jul  7 18:00:59 2024 ] 	Batch(3900/7879) done. Loss: 0.6705  lr:0.010000
[ Sun Jul  7 18:01:21 2024 ] 
Training: Epoch [14/120], Step [3999], Loss: 1.5271728038787842, Training Accuracy: 74.30312500000001
[ Sun Jul  7 18:01:22 2024 ] 	Batch(4000/7879) done. Loss: 0.4785  lr:0.010000
[ Sun Jul  7 18:01:44 2024 ] 	Batch(4100/7879) done. Loss: 0.8184  lr:0.010000
[ Sun Jul  7 18:02:07 2024 ] 	Batch(4200/7879) done. Loss: 0.5040  lr:0.010000
[ Sun Jul  7 18:02:29 2024 ] 	Batch(4300/7879) done. Loss: 0.6288  lr:0.010000
[ Sun Jul  7 18:02:52 2024 ] 	Batch(4400/7879) done. Loss: 1.1038  lr:0.010000
[ Sun Jul  7 18:03:14 2024 ] 
Training: Epoch [14/120], Step [4499], Loss: 0.7093914151191711, Training Accuracy: 74.34166666666667
[ Sun Jul  7 18:03:15 2024 ] 	Batch(4500/7879) done. Loss: 1.3274  lr:0.010000
[ Sun Jul  7 18:03:37 2024 ] 	Batch(4600/7879) done. Loss: 1.1074  lr:0.010000
[ Sun Jul  7 18:04:00 2024 ] 	Batch(4700/7879) done. Loss: 0.7569  lr:0.010000
[ Sun Jul  7 18:04:23 2024 ] 	Batch(4800/7879) done. Loss: 0.8239  lr:0.010000
[ Sun Jul  7 18:04:45 2024 ] 	Batch(4900/7879) done. Loss: 0.6569  lr:0.010000
[ Sun Jul  7 18:05:08 2024 ] 
Training: Epoch [14/120], Step [4999], Loss: 1.3762120008468628, Training Accuracy: 74.35000000000001
[ Sun Jul  7 18:05:08 2024 ] 	Batch(5000/7879) done. Loss: 0.9358  lr:0.010000
[ Sun Jul  7 18:05:31 2024 ] 	Batch(5100/7879) done. Loss: 0.6863  lr:0.010000
[ Sun Jul  7 18:05:54 2024 ] 	Batch(5200/7879) done. Loss: 1.7206  lr:0.010000
[ Sun Jul  7 18:06:16 2024 ] 	Batch(5300/7879) done. Loss: 0.8605  lr:0.010000
[ Sun Jul  7 18:06:39 2024 ] 	Batch(5400/7879) done. Loss: 1.3770  lr:0.010000
[ Sun Jul  7 18:07:02 2024 ] 
Training: Epoch [14/120], Step [5499], Loss: 1.0637471675872803, Training Accuracy: 74.375
[ Sun Jul  7 18:07:02 2024 ] 	Batch(5500/7879) done. Loss: 0.4604  lr:0.010000
[ Sun Jul  7 18:07:25 2024 ] 	Batch(5600/7879) done. Loss: 0.5291  lr:0.010000
[ Sun Jul  7 18:07:47 2024 ] 	Batch(5700/7879) done. Loss: 0.8476  lr:0.010000
[ Sun Jul  7 18:08:10 2024 ] 	Batch(5800/7879) done. Loss: 0.4977  lr:0.010000
[ Sun Jul  7 18:08:33 2024 ] 	Batch(5900/7879) done. Loss: 0.3693  lr:0.010000
[ Sun Jul  7 18:08:55 2024 ] 
Training: Epoch [14/120], Step [5999], Loss: 0.5314984917640686, Training Accuracy: 74.27291666666666
[ Sun Jul  7 18:08:56 2024 ] 	Batch(6000/7879) done. Loss: 0.5840  lr:0.010000
[ Sun Jul  7 18:09:18 2024 ] 	Batch(6100/7879) done. Loss: 0.8267  lr:0.010000
[ Sun Jul  7 18:09:41 2024 ] 	Batch(6200/7879) done. Loss: 0.3347  lr:0.010000
[ Sun Jul  7 18:10:04 2024 ] 	Batch(6300/7879) done. Loss: 0.3315  lr:0.010000
[ Sun Jul  7 18:10:27 2024 ] 	Batch(6400/7879) done. Loss: 0.9518  lr:0.010000
[ Sun Jul  7 18:10:49 2024 ] 
Training: Epoch [14/120], Step [6499], Loss: 0.6192321181297302, Training Accuracy: 74.275
[ Sun Jul  7 18:10:49 2024 ] 	Batch(6500/7879) done. Loss: 0.8627  lr:0.010000
[ Sun Jul  7 18:11:12 2024 ] 	Batch(6600/7879) done. Loss: 1.1451  lr:0.010000
[ Sun Jul  7 18:11:35 2024 ] 	Batch(6700/7879) done. Loss: 1.4977  lr:0.010000
[ Sun Jul  7 18:11:58 2024 ] 	Batch(6800/7879) done. Loss: 1.1061  lr:0.010000
[ Sun Jul  7 18:12:21 2024 ] 	Batch(6900/7879) done. Loss: 0.4519  lr:0.010000
[ Sun Jul  7 18:12:44 2024 ] 
Training: Epoch [14/120], Step [6999], Loss: 0.6292745471000671, Training Accuracy: 74.27678571428571
[ Sun Jul  7 18:12:45 2024 ] 	Batch(7000/7879) done. Loss: 0.3366  lr:0.010000
[ Sun Jul  7 18:13:08 2024 ] 	Batch(7100/7879) done. Loss: 0.7668  lr:0.010000
[ Sun Jul  7 18:13:32 2024 ] 	Batch(7200/7879) done. Loss: 2.6729  lr:0.010000
[ Sun Jul  7 18:13:55 2024 ] 	Batch(7300/7879) done. Loss: 0.6967  lr:0.010000
[ Sun Jul  7 18:14:19 2024 ] 	Batch(7400/7879) done. Loss: 0.2694  lr:0.010000
[ Sun Jul  7 18:14:42 2024 ] 
Training: Epoch [14/120], Step [7499], Loss: 0.8797670602798462, Training Accuracy: 74.22333333333333
[ Sun Jul  7 18:14:42 2024 ] 	Batch(7500/7879) done. Loss: 0.9649  lr:0.010000
[ Sun Jul  7 18:15:06 2024 ] 	Batch(7600/7879) done. Loss: 0.6192  lr:0.010000
[ Sun Jul  7 18:15:28 2024 ] 	Batch(7700/7879) done. Loss: 1.1794  lr:0.010000
[ Sun Jul  7 18:15:51 2024 ] 	Batch(7800/7879) done. Loss: 0.2598  lr:0.010000
[ Sun Jul  7 18:16:09 2024 ] 	Mean training loss: 0.8653.
[ Sun Jul  7 18:16:09 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 18:16:09 2024 ] Training epoch: 16
[ Sun Jul  7 18:16:10 2024 ] 	Batch(0/7879) done. Loss: 2.0922  lr:0.010000
[ Sun Jul  7 18:16:33 2024 ] 	Batch(100/7879) done. Loss: 0.4745  lr:0.010000
[ Sun Jul  7 18:16:56 2024 ] 	Batch(200/7879) done. Loss: 0.7523  lr:0.010000
[ Sun Jul  7 18:17:20 2024 ] 	Batch(300/7879) done. Loss: 0.2536  lr:0.010000
[ Sun Jul  7 18:17:43 2024 ] 	Batch(400/7879) done. Loss: 0.2984  lr:0.010000
[ Sun Jul  7 18:18:05 2024 ] 
Training: Epoch [15/120], Step [499], Loss: 0.48062822222709656, Training Accuracy: 75.375
[ Sun Jul  7 18:18:05 2024 ] 	Batch(500/7879) done. Loss: 1.2160  lr:0.010000
[ Sun Jul  7 18:18:28 2024 ] 	Batch(600/7879) done. Loss: 0.7779  lr:0.010000
[ Sun Jul  7 18:18:51 2024 ] 	Batch(700/7879) done. Loss: 0.2748  lr:0.010000
[ Sun Jul  7 18:19:14 2024 ] 	Batch(800/7879) done. Loss: 1.6397  lr:0.010000
[ Sun Jul  7 18:19:37 2024 ] 	Batch(900/7879) done. Loss: 0.4744  lr:0.010000
[ Sun Jul  7 18:20:00 2024 ] 
Training: Epoch [15/120], Step [999], Loss: 0.7980353236198425, Training Accuracy: 75.7
[ Sun Jul  7 18:20:01 2024 ] 	Batch(1000/7879) done. Loss: 1.2780  lr:0.010000
[ Sun Jul  7 18:20:24 2024 ] 	Batch(1100/7879) done. Loss: 0.4186  lr:0.010000
[ Sun Jul  7 18:20:48 2024 ] 	Batch(1200/7879) done. Loss: 1.5056  lr:0.010000
[ Sun Jul  7 18:21:11 2024 ] 	Batch(1300/7879) done. Loss: 0.9183  lr:0.010000
[ Sun Jul  7 18:21:35 2024 ] 	Batch(1400/7879) done. Loss: 1.0390  lr:0.010000
[ Sun Jul  7 18:21:58 2024 ] 
Training: Epoch [15/120], Step [1499], Loss: 1.5824129581451416, Training Accuracy: 75.275
[ Sun Jul  7 18:21:58 2024 ] 	Batch(1500/7879) done. Loss: 0.2635  lr:0.010000
[ Sun Jul  7 18:22:22 2024 ] 	Batch(1600/7879) done. Loss: 0.6697  lr:0.010000
[ Sun Jul  7 18:22:44 2024 ] 	Batch(1700/7879) done. Loss: 0.2629  lr:0.010000
[ Sun Jul  7 18:23:07 2024 ] 	Batch(1800/7879) done. Loss: 0.2112  lr:0.010000
[ Sun Jul  7 18:23:30 2024 ] 	Batch(1900/7879) done. Loss: 1.0168  lr:0.010000
[ Sun Jul  7 18:23:52 2024 ] 
Training: Epoch [15/120], Step [1999], Loss: 0.49719667434692383, Training Accuracy: 75.44375
[ Sun Jul  7 18:23:53 2024 ] 	Batch(2000/7879) done. Loss: 0.8885  lr:0.010000
[ Sun Jul  7 18:24:15 2024 ] 	Batch(2100/7879) done. Loss: 0.7034  lr:0.010000
[ Sun Jul  7 18:24:38 2024 ] 	Batch(2200/7879) done. Loss: 1.1092  lr:0.010000
[ Sun Jul  7 18:25:01 2024 ] 	Batch(2300/7879) done. Loss: 0.5619  lr:0.010000
[ Sun Jul  7 18:25:24 2024 ] 	Batch(2400/7879) done. Loss: 0.9623  lr:0.010000
[ Sun Jul  7 18:25:46 2024 ] 
Training: Epoch [15/120], Step [2499], Loss: 0.9305874705314636, Training Accuracy: 75.22
[ Sun Jul  7 18:25:47 2024 ] 	Batch(2500/7879) done. Loss: 0.8616  lr:0.010000
[ Sun Jul  7 18:26:09 2024 ] 	Batch(2600/7879) done. Loss: 0.3230  lr:0.010000
[ Sun Jul  7 18:26:32 2024 ] 	Batch(2700/7879) done. Loss: 0.5637  lr:0.010000
[ Sun Jul  7 18:26:55 2024 ] 	Batch(2800/7879) done. Loss: 0.4565  lr:0.010000
[ Sun Jul  7 18:27:18 2024 ] 	Batch(2900/7879) done. Loss: 0.3042  lr:0.010000
[ Sun Jul  7 18:27:40 2024 ] 
Training: Epoch [15/120], Step [2999], Loss: 0.839475154876709, Training Accuracy: 75.3375
[ Sun Jul  7 18:27:40 2024 ] 	Batch(3000/7879) done. Loss: 0.9308  lr:0.010000
[ Sun Jul  7 18:28:03 2024 ] 	Batch(3100/7879) done. Loss: 0.9392  lr:0.010000
[ Sun Jul  7 18:28:26 2024 ] 	Batch(3200/7879) done. Loss: 1.5321  lr:0.010000
[ Sun Jul  7 18:28:49 2024 ] 	Batch(3300/7879) done. Loss: 1.7578  lr:0.010000
[ Sun Jul  7 18:29:11 2024 ] 	Batch(3400/7879) done. Loss: 0.5960  lr:0.010000
[ Sun Jul  7 18:29:34 2024 ] 
Training: Epoch [15/120], Step [3499], Loss: 0.9432446360588074, Training Accuracy: 75.32857142857144
[ Sun Jul  7 18:29:34 2024 ] 	Batch(3500/7879) done. Loss: 0.6291  lr:0.010000
[ Sun Jul  7 18:29:57 2024 ] 	Batch(3600/7879) done. Loss: 0.5408  lr:0.010000
[ Sun Jul  7 18:30:19 2024 ] 	Batch(3700/7879) done. Loss: 0.8252  lr:0.010000
[ Sun Jul  7 18:30:42 2024 ] 	Batch(3800/7879) done. Loss: 0.5131  lr:0.010000
[ Sun Jul  7 18:31:05 2024 ] 	Batch(3900/7879) done. Loss: 0.5792  lr:0.010000
[ Sun Jul  7 18:31:27 2024 ] 
Training: Epoch [15/120], Step [3999], Loss: 0.6486908793449402, Training Accuracy: 75.36562500000001
[ Sun Jul  7 18:31:28 2024 ] 	Batch(4000/7879) done. Loss: 0.9438  lr:0.010000
[ Sun Jul  7 18:31:51 2024 ] 	Batch(4100/7879) done. Loss: 0.5810  lr:0.010000
[ Sun Jul  7 18:32:13 2024 ] 	Batch(4200/7879) done. Loss: 0.6208  lr:0.010000
[ Sun Jul  7 18:32:36 2024 ] 	Batch(4300/7879) done. Loss: 0.3853  lr:0.010000
[ Sun Jul  7 18:32:59 2024 ] 	Batch(4400/7879) done. Loss: 0.9938  lr:0.010000
[ Sun Jul  7 18:33:21 2024 ] 
Training: Epoch [15/120], Step [4499], Loss: 1.7610433101654053, Training Accuracy: 75.41388888888889
[ Sun Jul  7 18:33:21 2024 ] 	Batch(4500/7879) done. Loss: 0.9972  lr:0.010000
[ Sun Jul  7 18:33:44 2024 ] 	Batch(4600/7879) done. Loss: 1.3243  lr:0.010000
[ Sun Jul  7 18:34:07 2024 ] 	Batch(4700/7879) done. Loss: 0.5395  lr:0.010000
[ Sun Jul  7 18:34:30 2024 ] 	Batch(4800/7879) done. Loss: 0.3842  lr:0.010000
[ Sun Jul  7 18:34:53 2024 ] 	Batch(4900/7879) done. Loss: 0.8140  lr:0.010000
[ Sun Jul  7 18:35:16 2024 ] 
Training: Epoch [15/120], Step [4999], Loss: 1.1811376810073853, Training Accuracy: 75.38
[ Sun Jul  7 18:35:17 2024 ] 	Batch(5000/7879) done. Loss: 1.2459  lr:0.010000
[ Sun Jul  7 18:35:40 2024 ] 	Batch(5100/7879) done. Loss: 1.0169  lr:0.010000
[ Sun Jul  7 18:36:03 2024 ] 	Batch(5200/7879) done. Loss: 0.8672  lr:0.010000
[ Sun Jul  7 18:36:26 2024 ] 	Batch(5300/7879) done. Loss: 1.6274  lr:0.010000
[ Sun Jul  7 18:36:50 2024 ] 	Batch(5400/7879) done. Loss: 0.6288  lr:0.010000
[ Sun Jul  7 18:37:12 2024 ] 
Training: Epoch [15/120], Step [5499], Loss: 0.1797517091035843, Training Accuracy: 75.35909090909091
[ Sun Jul  7 18:37:12 2024 ] 	Batch(5500/7879) done. Loss: 0.3679  lr:0.010000
[ Sun Jul  7 18:37:35 2024 ] 	Batch(5600/7879) done. Loss: 0.3836  lr:0.010000
[ Sun Jul  7 18:37:58 2024 ] 	Batch(5700/7879) done. Loss: 0.4815  lr:0.010000
[ Sun Jul  7 18:38:21 2024 ] 	Batch(5800/7879) done. Loss: 0.7906  lr:0.010000
[ Sun Jul  7 18:38:43 2024 ] 	Batch(5900/7879) done. Loss: 0.4827  lr:0.010000
[ Sun Jul  7 18:39:06 2024 ] 
Training: Epoch [15/120], Step [5999], Loss: 0.44762301445007324, Training Accuracy: 75.27916666666667
[ Sun Jul  7 18:39:06 2024 ] 	Batch(6000/7879) done. Loss: 0.2315  lr:0.010000
[ Sun Jul  7 18:39:29 2024 ] 	Batch(6100/7879) done. Loss: 1.8704  lr:0.010000
[ Sun Jul  7 18:39:52 2024 ] 	Batch(6200/7879) done. Loss: 0.4215  lr:0.010000
[ Sun Jul  7 18:40:14 2024 ] 	Batch(6300/7879) done. Loss: 0.3710  lr:0.010000
[ Sun Jul  7 18:40:37 2024 ] 	Batch(6400/7879) done. Loss: 0.4167  lr:0.010000
[ Sun Jul  7 18:41:00 2024 ] 
Training: Epoch [15/120], Step [6499], Loss: 0.1171877533197403, Training Accuracy: 75.37307692307692
[ Sun Jul  7 18:41:00 2024 ] 	Batch(6500/7879) done. Loss: 0.2285  lr:0.010000
[ Sun Jul  7 18:41:23 2024 ] 	Batch(6600/7879) done. Loss: 0.2026  lr:0.010000
[ Sun Jul  7 18:41:45 2024 ] 	Batch(6700/7879) done. Loss: 0.2544  lr:0.010000
[ Sun Jul  7 18:42:08 2024 ] 	Batch(6800/7879) done. Loss: 0.3064  lr:0.010000
[ Sun Jul  7 18:42:31 2024 ] 	Batch(6900/7879) done. Loss: 0.7876  lr:0.010000
[ Sun Jul  7 18:42:53 2024 ] 
Training: Epoch [15/120], Step [6999], Loss: 0.8347054123878479, Training Accuracy: 75.37321428571428
[ Sun Jul  7 18:42:53 2024 ] 	Batch(7000/7879) done. Loss: 1.1552  lr:0.010000
[ Sun Jul  7 18:43:16 2024 ] 	Batch(7100/7879) done. Loss: 0.7722  lr:0.010000
[ Sun Jul  7 18:43:39 2024 ] 	Batch(7200/7879) done. Loss: 0.7604  lr:0.010000
[ Sun Jul  7 18:44:02 2024 ] 	Batch(7300/7879) done. Loss: 0.4999  lr:0.010000
[ Sun Jul  7 18:44:26 2024 ] 	Batch(7400/7879) done. Loss: 0.2764  lr:0.010000
[ Sun Jul  7 18:44:48 2024 ] 
Training: Epoch [15/120], Step [7499], Loss: 0.8562497496604919, Training Accuracy: 75.27833333333334
[ Sun Jul  7 18:44:49 2024 ] 	Batch(7500/7879) done. Loss: 0.8471  lr:0.010000
[ Sun Jul  7 18:45:11 2024 ] 	Batch(7600/7879) done. Loss: 0.7111  lr:0.010000
[ Sun Jul  7 18:45:34 2024 ] 	Batch(7700/7879) done. Loss: 0.5653  lr:0.010000
[ Sun Jul  7 18:45:57 2024 ] 	Batch(7800/7879) done. Loss: 0.6208  lr:0.010000
[ Sun Jul  7 18:46:15 2024 ] 	Mean training loss: 0.8314.
[ Sun Jul  7 18:46:15 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 18:46:15 2024 ] Training epoch: 17
[ Sun Jul  7 18:46:15 2024 ] 	Batch(0/7879) done. Loss: 0.5673  lr:0.010000
[ Sun Jul  7 18:46:38 2024 ] 	Batch(100/7879) done. Loss: 0.3422  lr:0.010000
[ Sun Jul  7 18:47:01 2024 ] 	Batch(200/7879) done. Loss: 0.3038  lr:0.010000
[ Sun Jul  7 18:47:23 2024 ] 	Batch(300/7879) done. Loss: 0.2298  lr:0.010000
[ Sun Jul  7 18:47:46 2024 ] 	Batch(400/7879) done. Loss: 0.5287  lr:0.010000
[ Sun Jul  7 18:48:09 2024 ] 
Training: Epoch [16/120], Step [499], Loss: 1.000801682472229, Training Accuracy: 76.175
[ Sun Jul  7 18:48:09 2024 ] 	Batch(500/7879) done. Loss: 1.3302  lr:0.010000
[ Sun Jul  7 18:48:31 2024 ] 	Batch(600/7879) done. Loss: 0.9893  lr:0.010000
[ Sun Jul  7 18:48:54 2024 ] 	Batch(700/7879) done. Loss: 0.3779  lr:0.010000
[ Sun Jul  7 18:49:17 2024 ] 	Batch(800/7879) done. Loss: 1.2797  lr:0.010000
[ Sun Jul  7 18:49:39 2024 ] 	Batch(900/7879) done. Loss: 1.3936  lr:0.010000
[ Sun Jul  7 18:50:02 2024 ] 
Training: Epoch [16/120], Step [999], Loss: 1.1161885261535645, Training Accuracy: 76.325
[ Sun Jul  7 18:50:02 2024 ] 	Batch(1000/7879) done. Loss: 1.1201  lr:0.010000
[ Sun Jul  7 18:50:24 2024 ] 	Batch(1100/7879) done. Loss: 0.6813  lr:0.010000
[ Sun Jul  7 18:50:47 2024 ] 	Batch(1200/7879) done. Loss: 0.5353  lr:0.010000
[ Sun Jul  7 18:51:10 2024 ] 	Batch(1300/7879) done. Loss: 1.1419  lr:0.010000
[ Sun Jul  7 18:51:33 2024 ] 	Batch(1400/7879) done. Loss: 0.4860  lr:0.010000
[ Sun Jul  7 18:51:55 2024 ] 
Training: Epoch [16/120], Step [1499], Loss: 0.43478623032569885, Training Accuracy: 76.11666666666666
[ Sun Jul  7 18:51:55 2024 ] 	Batch(1500/7879) done. Loss: 0.0715  lr:0.010000
[ Sun Jul  7 18:52:18 2024 ] 	Batch(1600/7879) done. Loss: 0.9904  lr:0.010000
[ Sun Jul  7 18:52:41 2024 ] 	Batch(1700/7879) done. Loss: 0.3732  lr:0.010000
[ Sun Jul  7 18:53:05 2024 ] 	Batch(1800/7879) done. Loss: 0.2798  lr:0.010000
[ Sun Jul  7 18:53:28 2024 ] 	Batch(1900/7879) done. Loss: 1.0534  lr:0.010000
[ Sun Jul  7 18:53:50 2024 ] 
Training: Epoch [16/120], Step [1999], Loss: 0.47737568616867065, Training Accuracy: 76.23124999999999
[ Sun Jul  7 18:53:50 2024 ] 	Batch(2000/7879) done. Loss: 0.4849  lr:0.010000
[ Sun Jul  7 18:54:14 2024 ] 	Batch(2100/7879) done. Loss: 1.0080  lr:0.010000
[ Sun Jul  7 18:54:36 2024 ] 	Batch(2200/7879) done. Loss: 1.7817  lr:0.010000
[ Sun Jul  7 18:54:59 2024 ] 	Batch(2300/7879) done. Loss: 0.5564  lr:0.010000
[ Sun Jul  7 18:55:22 2024 ] 	Batch(2400/7879) done. Loss: 0.4568  lr:0.010000
[ Sun Jul  7 18:55:44 2024 ] 
Training: Epoch [16/120], Step [2499], Loss: 0.8233346343040466, Training Accuracy: 76.165
[ Sun Jul  7 18:55:44 2024 ] 	Batch(2500/7879) done. Loss: 1.3059  lr:0.010000
[ Sun Jul  7 18:56:07 2024 ] 	Batch(2600/7879) done. Loss: 1.5072  lr:0.010000
[ Sun Jul  7 18:56:29 2024 ] 	Batch(2700/7879) done. Loss: 1.7454  lr:0.010000
[ Sun Jul  7 18:56:52 2024 ] 	Batch(2800/7879) done. Loss: 1.2636  lr:0.010000
[ Sun Jul  7 18:57:15 2024 ] 	Batch(2900/7879) done. Loss: 1.6106  lr:0.010000
[ Sun Jul  7 18:57:37 2024 ] 
Training: Epoch [16/120], Step [2999], Loss: 2.4001784324645996, Training Accuracy: 75.97916666666666
[ Sun Jul  7 18:57:37 2024 ] 	Batch(3000/7879) done. Loss: 0.5710  lr:0.010000
[ Sun Jul  7 18:58:00 2024 ] 	Batch(3100/7879) done. Loss: 0.6466  lr:0.010000
[ Sun Jul  7 18:58:23 2024 ] 	Batch(3200/7879) done. Loss: 0.7061  lr:0.010000
[ Sun Jul  7 18:58:45 2024 ] 	Batch(3300/7879) done. Loss: 1.3977  lr:0.010000
[ Sun Jul  7 18:59:08 2024 ] 	Batch(3400/7879) done. Loss: 0.6598  lr:0.010000
[ Sun Jul  7 18:59:30 2024 ] 
Training: Epoch [16/120], Step [3499], Loss: 2.1116421222686768, Training Accuracy: 76.075
[ Sun Jul  7 18:59:30 2024 ] 	Batch(3500/7879) done. Loss: 0.7223  lr:0.010000
[ Sun Jul  7 18:59:53 2024 ] 	Batch(3600/7879) done. Loss: 0.5411  lr:0.010000
[ Sun Jul  7 19:00:16 2024 ] 	Batch(3700/7879) done. Loss: 1.0611  lr:0.010000
[ Sun Jul  7 19:00:38 2024 ] 	Batch(3800/7879) done. Loss: 1.0605  lr:0.010000
[ Sun Jul  7 19:01:01 2024 ] 	Batch(3900/7879) done. Loss: 0.8685  lr:0.010000
[ Sun Jul  7 19:01:23 2024 ] 
Training: Epoch [16/120], Step [3999], Loss: 0.42086878418922424, Training Accuracy: 76.14999999999999
[ Sun Jul  7 19:01:23 2024 ] 	Batch(4000/7879) done. Loss: 1.4750  lr:0.010000
[ Sun Jul  7 19:01:47 2024 ] 	Batch(4100/7879) done. Loss: 0.5188  lr:0.010000
[ Sun Jul  7 19:02:09 2024 ] 	Batch(4200/7879) done. Loss: 0.9357  lr:0.010000
[ Sun Jul  7 19:02:32 2024 ] 	Batch(4300/7879) done. Loss: 0.8212  lr:0.010000
[ Sun Jul  7 19:02:55 2024 ] 	Batch(4400/7879) done. Loss: 1.5097  lr:0.010000
[ Sun Jul  7 19:03:18 2024 ] 
Training: Epoch [16/120], Step [4499], Loss: 0.46608051657676697, Training Accuracy: 76.13888888888889
[ Sun Jul  7 19:03:18 2024 ] 	Batch(4500/7879) done. Loss: 0.4996  lr:0.010000
[ Sun Jul  7 19:03:41 2024 ] 	Batch(4600/7879) done. Loss: 1.1078  lr:0.010000
[ Sun Jul  7 19:04:05 2024 ] 	Batch(4700/7879) done. Loss: 0.2206  lr:0.010000
[ Sun Jul  7 19:04:28 2024 ] 	Batch(4800/7879) done. Loss: 0.6242  lr:0.010000
[ Sun Jul  7 19:04:51 2024 ] 	Batch(4900/7879) done. Loss: 0.9343  lr:0.010000
[ Sun Jul  7 19:05:13 2024 ] 
Training: Epoch [16/120], Step [4999], Loss: 0.9169701337814331, Training Accuracy: 76.2
[ Sun Jul  7 19:05:14 2024 ] 	Batch(5000/7879) done. Loss: 0.5239  lr:0.010000
[ Sun Jul  7 19:05:36 2024 ] 	Batch(5100/7879) done. Loss: 0.3919  lr:0.010000
[ Sun Jul  7 19:05:59 2024 ] 	Batch(5200/7879) done. Loss: 0.5573  lr:0.010000
[ Sun Jul  7 19:06:22 2024 ] 	Batch(5300/7879) done. Loss: 0.6035  lr:0.010000
[ Sun Jul  7 19:06:46 2024 ] 	Batch(5400/7879) done. Loss: 0.2044  lr:0.010000
[ Sun Jul  7 19:07:09 2024 ] 
Training: Epoch [16/120], Step [5499], Loss: 1.155798077583313, Training Accuracy: 76.3090909090909
[ Sun Jul  7 19:07:09 2024 ] 	Batch(5500/7879) done. Loss: 1.2082  lr:0.010000
[ Sun Jul  7 19:07:32 2024 ] 	Batch(5600/7879) done. Loss: 1.6075  lr:0.010000
[ Sun Jul  7 19:07:54 2024 ] 	Batch(5700/7879) done. Loss: 0.8889  lr:0.010000
[ Sun Jul  7 19:08:17 2024 ] 	Batch(5800/7879) done. Loss: 1.0083  lr:0.010000
[ Sun Jul  7 19:08:39 2024 ] 	Batch(5900/7879) done. Loss: 0.2575  lr:0.010000
[ Sun Jul  7 19:09:02 2024 ] 
Training: Epoch [16/120], Step [5999], Loss: 1.03744637966156, Training Accuracy: 76.39375000000001
[ Sun Jul  7 19:09:02 2024 ] 	Batch(6000/7879) done. Loss: 1.2290  lr:0.010000
[ Sun Jul  7 19:09:24 2024 ] 	Batch(6100/7879) done. Loss: 0.5821  lr:0.010000
[ Sun Jul  7 19:09:47 2024 ] 	Batch(6200/7879) done. Loss: 1.2606  lr:0.010000
[ Sun Jul  7 19:10:10 2024 ] 	Batch(6300/7879) done. Loss: 0.6282  lr:0.010000
[ Sun Jul  7 19:10:32 2024 ] 	Batch(6400/7879) done. Loss: 0.5872  lr:0.010000
[ Sun Jul  7 19:10:55 2024 ] 
Training: Epoch [16/120], Step [6499], Loss: 0.7955574989318848, Training Accuracy: 76.42115384615384
[ Sun Jul  7 19:10:55 2024 ] 	Batch(6500/7879) done. Loss: 1.1664  lr:0.010000
[ Sun Jul  7 19:11:18 2024 ] 	Batch(6600/7879) done. Loss: 0.3233  lr:0.010000
[ Sun Jul  7 19:11:40 2024 ] 	Batch(6700/7879) done. Loss: 0.6625  lr:0.010000
[ Sun Jul  7 19:12:03 2024 ] 	Batch(6800/7879) done. Loss: 0.5689  lr:0.010000
[ Sun Jul  7 19:12:26 2024 ] 	Batch(6900/7879) done. Loss: 0.4517  lr:0.010000
[ Sun Jul  7 19:12:48 2024 ] 
Training: Epoch [16/120], Step [6999], Loss: 1.2344722747802734, Training Accuracy: 76.41964285714286
[ Sun Jul  7 19:12:49 2024 ] 	Batch(7000/7879) done. Loss: 0.5640  lr:0.010000
[ Sun Jul  7 19:13:11 2024 ] 	Batch(7100/7879) done. Loss: 1.3786  lr:0.010000
[ Sun Jul  7 19:13:34 2024 ] 	Batch(7200/7879) done. Loss: 0.6881  lr:0.010000
[ Sun Jul  7 19:13:56 2024 ] 	Batch(7300/7879) done. Loss: 1.5435  lr:0.010000
[ Sun Jul  7 19:14:19 2024 ] 	Batch(7400/7879) done. Loss: 0.9293  lr:0.010000
[ Sun Jul  7 19:14:41 2024 ] 
Training: Epoch [16/120], Step [7499], Loss: 0.7574719786643982, Training Accuracy: 76.36166666666668
[ Sun Jul  7 19:14:41 2024 ] 	Batch(7500/7879) done. Loss: 1.0103  lr:0.010000
[ Sun Jul  7 19:15:04 2024 ] 	Batch(7600/7879) done. Loss: 0.4722  lr:0.010000
[ Sun Jul  7 19:15:27 2024 ] 	Batch(7700/7879) done. Loss: 0.3397  lr:0.010000
[ Sun Jul  7 19:15:49 2024 ] 	Batch(7800/7879) done. Loss: 0.2563  lr:0.010000
[ Sun Jul  7 19:16:07 2024 ] 	Mean training loss: 0.7934.
[ Sun Jul  7 19:16:07 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 19:16:07 2024 ] Training epoch: 18
[ Sun Jul  7 19:16:08 2024 ] 	Batch(0/7879) done. Loss: 0.2215  lr:0.010000
[ Sun Jul  7 19:16:30 2024 ] 	Batch(100/7879) done. Loss: 0.8825  lr:0.010000
[ Sun Jul  7 19:16:53 2024 ] 	Batch(200/7879) done. Loss: 0.5530  lr:0.010000
[ Sun Jul  7 19:17:15 2024 ] 	Batch(300/7879) done. Loss: 0.5116  lr:0.010000
[ Sun Jul  7 19:17:38 2024 ] 	Batch(400/7879) done. Loss: 1.5163  lr:0.010000
[ Sun Jul  7 19:18:00 2024 ] 
Training: Epoch [17/120], Step [499], Loss: 0.7945318222045898, Training Accuracy: 77.47500000000001
[ Sun Jul  7 19:18:01 2024 ] 	Batch(500/7879) done. Loss: 0.4518  lr:0.010000
[ Sun Jul  7 19:18:23 2024 ] 	Batch(600/7879) done. Loss: 1.1979  lr:0.010000
[ Sun Jul  7 19:18:46 2024 ] 	Batch(700/7879) done. Loss: 0.8111  lr:0.010000
[ Sun Jul  7 19:19:08 2024 ] 	Batch(800/7879) done. Loss: 0.5511  lr:0.010000
[ Sun Jul  7 19:19:32 2024 ] 	Batch(900/7879) done. Loss: 1.0794  lr:0.010000
[ Sun Jul  7 19:19:55 2024 ] 
Training: Epoch [17/120], Step [999], Loss: 0.6236153841018677, Training Accuracy: 77.2375
[ Sun Jul  7 19:19:55 2024 ] 	Batch(1000/7879) done. Loss: 0.4756  lr:0.010000
[ Sun Jul  7 19:20:18 2024 ] 	Batch(1100/7879) done. Loss: 1.1347  lr:0.010000
[ Sun Jul  7 19:20:42 2024 ] 	Batch(1200/7879) done. Loss: 0.8631  lr:0.010000
[ Sun Jul  7 19:21:04 2024 ] 	Batch(1300/7879) done. Loss: 1.4143  lr:0.010000
[ Sun Jul  7 19:21:27 2024 ] 	Batch(1400/7879) done. Loss: 0.9248  lr:0.010000
[ Sun Jul  7 19:21:49 2024 ] 
Training: Epoch [17/120], Step [1499], Loss: 0.7676646709442139, Training Accuracy: 77.17500000000001
[ Sun Jul  7 19:21:49 2024 ] 	Batch(1500/7879) done. Loss: 0.8563  lr:0.010000
[ Sun Jul  7 19:22:12 2024 ] 	Batch(1600/7879) done. Loss: 0.7732  lr:0.010000
[ Sun Jul  7 19:22:36 2024 ] 	Batch(1700/7879) done. Loss: 0.4421  lr:0.010000
[ Sun Jul  7 19:22:58 2024 ] 	Batch(1800/7879) done. Loss: 0.8998  lr:0.010000
[ Sun Jul  7 19:23:21 2024 ] 	Batch(1900/7879) done. Loss: 0.8560  lr:0.010000
[ Sun Jul  7 19:23:43 2024 ] 
Training: Epoch [17/120], Step [1999], Loss: 0.5092986822128296, Training Accuracy: 77.10000000000001
[ Sun Jul  7 19:23:43 2024 ] 	Batch(2000/7879) done. Loss: 0.5012  lr:0.010000
[ Sun Jul  7 19:24:07 2024 ] 	Batch(2100/7879) done. Loss: 1.2545  lr:0.010000
[ Sun Jul  7 19:24:30 2024 ] 	Batch(2200/7879) done. Loss: 0.4899  lr:0.010000
[ Sun Jul  7 19:24:54 2024 ] 	Batch(2300/7879) done. Loss: 0.4910  lr:0.010000
[ Sun Jul  7 19:25:17 2024 ] 	Batch(2400/7879) done. Loss: 0.3593  lr:0.010000
[ Sun Jul  7 19:25:39 2024 ] 
Training: Epoch [17/120], Step [2499], Loss: 0.9360167384147644, Training Accuracy: 77.075
[ Sun Jul  7 19:25:40 2024 ] 	Batch(2500/7879) done. Loss: 0.8102  lr:0.010000
[ Sun Jul  7 19:26:02 2024 ] 	Batch(2600/7879) done. Loss: 0.4614  lr:0.010000
[ Sun Jul  7 19:26:25 2024 ] 	Batch(2700/7879) done. Loss: 1.8893  lr:0.010000
[ Sun Jul  7 19:26:48 2024 ] 	Batch(2800/7879) done. Loss: 1.3932  lr:0.010000
[ Sun Jul  7 19:27:10 2024 ] 	Batch(2900/7879) done. Loss: 1.8238  lr:0.010000
[ Sun Jul  7 19:27:33 2024 ] 
Training: Epoch [17/120], Step [2999], Loss: 1.5338069200515747, Training Accuracy: 77.15833333333333
[ Sun Jul  7 19:27:33 2024 ] 	Batch(3000/7879) done. Loss: 0.7479  lr:0.010000
[ Sun Jul  7 19:27:55 2024 ] 	Batch(3100/7879) done. Loss: 0.2818  lr:0.010000
[ Sun Jul  7 19:28:18 2024 ] 	Batch(3200/7879) done. Loss: 1.0609  lr:0.010000
[ Sun Jul  7 19:28:41 2024 ] 	Batch(3300/7879) done. Loss: 1.6044  lr:0.010000
[ Sun Jul  7 19:29:03 2024 ] 	Batch(3400/7879) done. Loss: 0.2412  lr:0.010000
[ Sun Jul  7 19:29:26 2024 ] 
Training: Epoch [17/120], Step [3499], Loss: 0.7193410992622375, Training Accuracy: 77.01785714285714
[ Sun Jul  7 19:29:26 2024 ] 	Batch(3500/7879) done. Loss: 0.3926  lr:0.010000
[ Sun Jul  7 19:29:49 2024 ] 	Batch(3600/7879) done. Loss: 1.0574  lr:0.010000
[ Sun Jul  7 19:30:11 2024 ] 	Batch(3700/7879) done. Loss: 0.8097  lr:0.010000
[ Sun Jul  7 19:30:34 2024 ] 	Batch(3800/7879) done. Loss: 0.6067  lr:0.010000
[ Sun Jul  7 19:30:56 2024 ] 	Batch(3900/7879) done. Loss: 0.6581  lr:0.010000
[ Sun Jul  7 19:31:19 2024 ] 
Training: Epoch [17/120], Step [3999], Loss: 0.14416836202144623, Training Accuracy: 77.021875
[ Sun Jul  7 19:31:19 2024 ] 	Batch(4000/7879) done. Loss: 0.8464  lr:0.010000
[ Sun Jul  7 19:31:42 2024 ] 	Batch(4100/7879) done. Loss: 1.0210  lr:0.010000
[ Sun Jul  7 19:32:04 2024 ] 	Batch(4200/7879) done. Loss: 0.2231  lr:0.010000
[ Sun Jul  7 19:32:27 2024 ] 	Batch(4300/7879) done. Loss: 0.5275  lr:0.010000
[ Sun Jul  7 19:32:50 2024 ] 	Batch(4400/7879) done. Loss: 1.6572  lr:0.010000
[ Sun Jul  7 19:33:13 2024 ] 
Training: Epoch [17/120], Step [4499], Loss: 1.5037918090820312, Training Accuracy: 77.025
[ Sun Jul  7 19:33:13 2024 ] 	Batch(4500/7879) done. Loss: 0.4841  lr:0.010000
[ Sun Jul  7 19:33:36 2024 ] 	Batch(4600/7879) done. Loss: 0.3644  lr:0.010000
[ Sun Jul  7 19:34:00 2024 ] 	Batch(4700/7879) done. Loss: 0.1487  lr:0.010000
[ Sun Jul  7 19:34:23 2024 ] 	Batch(4800/7879) done. Loss: 0.7119  lr:0.010000
[ Sun Jul  7 19:34:46 2024 ] 	Batch(4900/7879) done. Loss: 1.3332  lr:0.010000
[ Sun Jul  7 19:35:09 2024 ] 
Training: Epoch [17/120], Step [4999], Loss: 0.2716647982597351, Training Accuracy: 76.96
[ Sun Jul  7 19:35:09 2024 ] 	Batch(5000/7879) done. Loss: 0.1733  lr:0.010000
[ Sun Jul  7 19:35:33 2024 ] 	Batch(5100/7879) done. Loss: 0.5150  lr:0.010000
[ Sun Jul  7 19:35:55 2024 ] 	Batch(5200/7879) done. Loss: 0.6246  lr:0.010000
[ Sun Jul  7 19:36:19 2024 ] 	Batch(5300/7879) done. Loss: 0.6338  lr:0.010000
[ Sun Jul  7 19:36:42 2024 ] 	Batch(5400/7879) done. Loss: 0.6817  lr:0.010000
[ Sun Jul  7 19:37:05 2024 ] 
Training: Epoch [17/120], Step [5499], Loss: 0.4956532418727875, Training Accuracy: 76.92727272727272
[ Sun Jul  7 19:37:05 2024 ] 	Batch(5500/7879) done. Loss: 0.0719  lr:0.010000
[ Sun Jul  7 19:37:28 2024 ] 	Batch(5600/7879) done. Loss: 0.4813  lr:0.010000
[ Sun Jul  7 19:37:50 2024 ] 	Batch(5700/7879) done. Loss: 0.2457  lr:0.010000
[ Sun Jul  7 19:38:13 2024 ] 	Batch(5800/7879) done. Loss: 0.3509  lr:0.010000
[ Sun Jul  7 19:38:35 2024 ] 	Batch(5900/7879) done. Loss: 0.7676  lr:0.010000
[ Sun Jul  7 19:38:58 2024 ] 
Training: Epoch [17/120], Step [5999], Loss: 0.48480117321014404, Training Accuracy: 76.88541666666666
[ Sun Jul  7 19:38:58 2024 ] 	Batch(6000/7879) done. Loss: 0.3661  lr:0.010000
[ Sun Jul  7 19:39:21 2024 ] 	Batch(6100/7879) done. Loss: 0.5436  lr:0.010000
[ Sun Jul  7 19:39:43 2024 ] 	Batch(6200/7879) done. Loss: 0.4950  lr:0.010000
[ Sun Jul  7 19:40:06 2024 ] 	Batch(6300/7879) done. Loss: 0.4430  lr:0.010000
[ Sun Jul  7 19:40:28 2024 ] 	Batch(6400/7879) done. Loss: 0.3555  lr:0.010000
[ Sun Jul  7 19:40:51 2024 ] 
Training: Epoch [17/120], Step [6499], Loss: 0.8612293601036072, Training Accuracy: 76.92307692307693
[ Sun Jul  7 19:40:51 2024 ] 	Batch(6500/7879) done. Loss: 0.9663  lr:0.010000
[ Sun Jul  7 19:41:14 2024 ] 	Batch(6600/7879) done. Loss: 0.6348  lr:0.010000
[ Sun Jul  7 19:41:36 2024 ] 	Batch(6700/7879) done. Loss: 0.4994  lr:0.010000
[ Sun Jul  7 19:41:59 2024 ] 	Batch(6800/7879) done. Loss: 0.5971  lr:0.010000
[ Sun Jul  7 19:42:22 2024 ] 	Batch(6900/7879) done. Loss: 0.6347  lr:0.010000
[ Sun Jul  7 19:42:44 2024 ] 
Training: Epoch [17/120], Step [6999], Loss: 1.3793853521347046, Training Accuracy: 76.95357142857144
[ Sun Jul  7 19:42:44 2024 ] 	Batch(7000/7879) done. Loss: 1.3236  lr:0.010000
[ Sun Jul  7 19:43:07 2024 ] 	Batch(7100/7879) done. Loss: 2.0064  lr:0.010000
[ Sun Jul  7 19:43:29 2024 ] 	Batch(7200/7879) done. Loss: 0.4259  lr:0.010000
[ Sun Jul  7 19:43:52 2024 ] 	Batch(7300/7879) done. Loss: 1.1888  lr:0.010000
[ Sun Jul  7 19:44:15 2024 ] 	Batch(7400/7879) done. Loss: 0.6128  lr:0.010000
[ Sun Jul  7 19:44:37 2024 ] 
Training: Epoch [17/120], Step [7499], Loss: 1.6947739124298096, Training Accuracy: 76.995
[ Sun Jul  7 19:44:37 2024 ] 	Batch(7500/7879) done. Loss: 0.9644  lr:0.010000
[ Sun Jul  7 19:45:00 2024 ] 	Batch(7600/7879) done. Loss: 1.0172  lr:0.010000
[ Sun Jul  7 19:45:22 2024 ] 	Batch(7700/7879) done. Loss: 1.3759  lr:0.010000
[ Sun Jul  7 19:45:45 2024 ] 	Batch(7800/7879) done. Loss: 0.2263  lr:0.010000
[ Sun Jul  7 19:46:03 2024 ] 	Mean training loss: 0.7834.
[ Sun Jul  7 19:46:03 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 19:46:03 2024 ] Training epoch: 19
[ Sun Jul  7 19:46:03 2024 ] 	Batch(0/7879) done. Loss: 1.4756  lr:0.010000
[ Sun Jul  7 19:46:27 2024 ] 	Batch(100/7879) done. Loss: 0.5938  lr:0.010000
[ Sun Jul  7 19:46:50 2024 ] 	Batch(200/7879) done. Loss: 0.2189  lr:0.010000
[ Sun Jul  7 19:47:13 2024 ] 	Batch(300/7879) done. Loss: 1.0333  lr:0.010000
[ Sun Jul  7 19:47:37 2024 ] 	Batch(400/7879) done. Loss: 0.5035  lr:0.010000
[ Sun Jul  7 19:48:00 2024 ] 
Training: Epoch [18/120], Step [499], Loss: 0.5632744431495667, Training Accuracy: 78.325
[ Sun Jul  7 19:48:00 2024 ] 	Batch(500/7879) done. Loss: 0.4086  lr:0.010000
[ Sun Jul  7 19:48:23 2024 ] 	Batch(600/7879) done. Loss: 0.7717  lr:0.010000
[ Sun Jul  7 19:48:46 2024 ] 	Batch(700/7879) done. Loss: 0.4524  lr:0.010000
[ Sun Jul  7 19:49:10 2024 ] 	Batch(800/7879) done. Loss: 1.3596  lr:0.010000
[ Sun Jul  7 19:49:33 2024 ] 	Batch(900/7879) done. Loss: 0.2595  lr:0.010000
[ Sun Jul  7 19:49:56 2024 ] 
Training: Epoch [18/120], Step [999], Loss: 1.0170015096664429, Training Accuracy: 78.4375
[ Sun Jul  7 19:49:57 2024 ] 	Batch(1000/7879) done. Loss: 0.3283  lr:0.010000
[ Sun Jul  7 19:50:20 2024 ] 	Batch(1100/7879) done. Loss: 0.1087  lr:0.010000
[ Sun Jul  7 19:50:43 2024 ] 	Batch(1200/7879) done. Loss: 0.2821  lr:0.010000
[ Sun Jul  7 19:51:07 2024 ] 	Batch(1300/7879) done. Loss: 0.1824  lr:0.010000
[ Sun Jul  7 19:51:30 2024 ] 	Batch(1400/7879) done. Loss: 1.1635  lr:0.010000
[ Sun Jul  7 19:51:53 2024 ] 
Training: Epoch [18/120], Step [1499], Loss: 0.7540422677993774, Training Accuracy: 77.98333333333333
[ Sun Jul  7 19:51:53 2024 ] 	Batch(1500/7879) done. Loss: 0.6124  lr:0.010000
[ Sun Jul  7 19:52:17 2024 ] 	Batch(1600/7879) done. Loss: 0.7233  lr:0.010000
[ Sun Jul  7 19:52:40 2024 ] 	Batch(1700/7879) done. Loss: 0.5438  lr:0.010000
[ Sun Jul  7 19:53:03 2024 ] 	Batch(1800/7879) done. Loss: 0.7087  lr:0.010000
[ Sun Jul  7 19:53:27 2024 ] 	Batch(1900/7879) done. Loss: 1.3673  lr:0.010000
[ Sun Jul  7 19:53:49 2024 ] 
Training: Epoch [18/120], Step [1999], Loss: 0.1396969109773636, Training Accuracy: 78.06875
[ Sun Jul  7 19:53:50 2024 ] 	Batch(2000/7879) done. Loss: 0.3744  lr:0.010000
[ Sun Jul  7 19:54:13 2024 ] 	Batch(2100/7879) done. Loss: 0.5450  lr:0.010000
[ Sun Jul  7 19:54:36 2024 ] 	Batch(2200/7879) done. Loss: 0.6183  lr:0.010000
[ Sun Jul  7 19:54:59 2024 ] 	Batch(2300/7879) done. Loss: 0.3362  lr:0.010000
[ Sun Jul  7 19:55:23 2024 ] 	Batch(2400/7879) done. Loss: 0.2994  lr:0.010000
[ Sun Jul  7 19:55:46 2024 ] 
Training: Epoch [18/120], Step [2499], Loss: 0.8576936721801758, Training Accuracy: 78.27
[ Sun Jul  7 19:55:46 2024 ] 	Batch(2500/7879) done. Loss: 0.2809  lr:0.010000
[ Sun Jul  7 19:56:10 2024 ] 	Batch(2600/7879) done. Loss: 0.4336  lr:0.010000
[ Sun Jul  7 19:56:33 2024 ] 	Batch(2700/7879) done. Loss: 0.3044  lr:0.010000
[ Sun Jul  7 19:56:56 2024 ] 	Batch(2800/7879) done. Loss: 0.8859  lr:0.010000
[ Sun Jul  7 19:57:19 2024 ] 	Batch(2900/7879) done. Loss: 0.8193  lr:0.010000
[ Sun Jul  7 19:57:42 2024 ] 
Training: Epoch [18/120], Step [2999], Loss: 0.18011382222175598, Training Accuracy: 78.19166666666668
[ Sun Jul  7 19:57:42 2024 ] 	Batch(3000/7879) done. Loss: 1.0621  lr:0.010000
[ Sun Jul  7 19:58:04 2024 ] 	Batch(3100/7879) done. Loss: 1.1419  lr:0.010000
[ Sun Jul  7 19:58:27 2024 ] 	Batch(3200/7879) done. Loss: 0.7551  lr:0.010000
[ Sun Jul  7 19:58:50 2024 ] 	Batch(3300/7879) done. Loss: 0.2997  lr:0.010000
[ Sun Jul  7 19:59:12 2024 ] 	Batch(3400/7879) done. Loss: 0.3821  lr:0.010000
[ Sun Jul  7 19:59:35 2024 ] 
Training: Epoch [18/120], Step [3499], Loss: 0.4173010289669037, Training Accuracy: 78.125
[ Sun Jul  7 19:59:35 2024 ] 	Batch(3500/7879) done. Loss: 0.8482  lr:0.010000
[ Sun Jul  7 19:59:58 2024 ] 	Batch(3600/7879) done. Loss: 0.5145  lr:0.010000
[ Sun Jul  7 20:00:20 2024 ] 	Batch(3700/7879) done. Loss: 0.9940  lr:0.010000
[ Sun Jul  7 20:00:43 2024 ] 	Batch(3800/7879) done. Loss: 1.8701  lr:0.010000
[ Sun Jul  7 20:01:05 2024 ] 	Batch(3900/7879) done. Loss: 0.7041  lr:0.010000
[ Sun Jul  7 20:01:28 2024 ] 
Training: Epoch [18/120], Step [3999], Loss: 0.7045709490776062, Training Accuracy: 77.99374999999999
[ Sun Jul  7 20:01:28 2024 ] 	Batch(4000/7879) done. Loss: 0.8241  lr:0.010000
[ Sun Jul  7 20:01:51 2024 ] 	Batch(4100/7879) done. Loss: 1.0534  lr:0.010000
[ Sun Jul  7 20:02:13 2024 ] 	Batch(4200/7879) done. Loss: 0.4224  lr:0.010000
[ Sun Jul  7 20:02:36 2024 ] 	Batch(4300/7879) done. Loss: 0.6662  lr:0.010000
[ Sun Jul  7 20:02:59 2024 ] 	Batch(4400/7879) done. Loss: 1.0108  lr:0.010000
[ Sun Jul  7 20:03:21 2024 ] 
Training: Epoch [18/120], Step [4499], Loss: 1.0103055238723755, Training Accuracy: 77.95555555555555
[ Sun Jul  7 20:03:21 2024 ] 	Batch(4500/7879) done. Loss: 0.8134  lr:0.010000
[ Sun Jul  7 20:03:44 2024 ] 	Batch(4600/7879) done. Loss: 1.1768  lr:0.010000
[ Sun Jul  7 20:04:06 2024 ] 	Batch(4700/7879) done. Loss: 0.9219  lr:0.010000
[ Sun Jul  7 20:04:29 2024 ] 	Batch(4800/7879) done. Loss: 0.3427  lr:0.010000
[ Sun Jul  7 20:04:52 2024 ] 	Batch(4900/7879) done. Loss: 0.9490  lr:0.010000
[ Sun Jul  7 20:05:15 2024 ] 
Training: Epoch [18/120], Step [4999], Loss: 0.7211556434631348, Training Accuracy: 77.985
[ Sun Jul  7 20:05:15 2024 ] 	Batch(5000/7879) done. Loss: 0.8826  lr:0.010000
[ Sun Jul  7 20:05:38 2024 ] 	Batch(5100/7879) done. Loss: 0.4662  lr:0.010000
[ Sun Jul  7 20:06:01 2024 ] 	Batch(5200/7879) done. Loss: 1.3504  lr:0.010000
[ Sun Jul  7 20:06:23 2024 ] 	Batch(5300/7879) done. Loss: 0.3418  lr:0.010000
[ Sun Jul  7 20:06:46 2024 ] 	Batch(5400/7879) done. Loss: 0.0765  lr:0.010000
[ Sun Jul  7 20:07:08 2024 ] 
Training: Epoch [18/120], Step [5499], Loss: 0.4732383191585541, Training Accuracy: 77.93636363636364
[ Sun Jul  7 20:07:08 2024 ] 	Batch(5500/7879) done. Loss: 1.9206  lr:0.010000
[ Sun Jul  7 20:07:31 2024 ] 	Batch(5600/7879) done. Loss: 0.4083  lr:0.010000
[ Sun Jul  7 20:07:54 2024 ] 	Batch(5700/7879) done. Loss: 1.0822  lr:0.010000
[ Sun Jul  7 20:08:16 2024 ] 	Batch(5800/7879) done. Loss: 0.0607  lr:0.010000
[ Sun Jul  7 20:08:39 2024 ] 	Batch(5900/7879) done. Loss: 0.1345  lr:0.010000
[ Sun Jul  7 20:09:01 2024 ] 
Training: Epoch [18/120], Step [5999], Loss: 0.4741959273815155, Training Accuracy: 77.8125
[ Sun Jul  7 20:09:01 2024 ] 	Batch(6000/7879) done. Loss: 2.3494  lr:0.010000
[ Sun Jul  7 20:09:24 2024 ] 	Batch(6100/7879) done. Loss: 0.2869  lr:0.010000
[ Sun Jul  7 20:09:47 2024 ] 	Batch(6200/7879) done. Loss: 1.6223  lr:0.010000
[ Sun Jul  7 20:10:09 2024 ] 	Batch(6300/7879) done. Loss: 0.4996  lr:0.010000
[ Sun Jul  7 20:10:32 2024 ] 	Batch(6400/7879) done. Loss: 0.0803  lr:0.010000
[ Sun Jul  7 20:10:54 2024 ] 
Training: Epoch [18/120], Step [6499], Loss: 0.2945534884929657, Training Accuracy: 77.79038461538461
[ Sun Jul  7 20:10:55 2024 ] 	Batch(6500/7879) done. Loss: 0.9480  lr:0.010000
[ Sun Jul  7 20:11:17 2024 ] 	Batch(6600/7879) done. Loss: 0.5702  lr:0.010000
[ Sun Jul  7 20:11:40 2024 ] 	Batch(6700/7879) done. Loss: 1.2933  lr:0.010000
[ Sun Jul  7 20:12:02 2024 ] 	Batch(6800/7879) done. Loss: 0.3324  lr:0.010000
[ Sun Jul  7 20:12:26 2024 ] 	Batch(6900/7879) done. Loss: 1.1703  lr:0.010000
[ Sun Jul  7 20:12:49 2024 ] 
Training: Epoch [18/120], Step [6999], Loss: 1.1600488424301147, Training Accuracy: 77.78214285714286
[ Sun Jul  7 20:12:49 2024 ] 	Batch(7000/7879) done. Loss: 0.5041  lr:0.010000
[ Sun Jul  7 20:13:12 2024 ] 	Batch(7100/7879) done. Loss: 0.6861  lr:0.010000
[ Sun Jul  7 20:13:36 2024 ] 	Batch(7200/7879) done. Loss: 0.8239  lr:0.010000
[ Sun Jul  7 20:13:59 2024 ] 	Batch(7300/7879) done. Loss: 0.4422  lr:0.010000
[ Sun Jul  7 20:14:21 2024 ] 	Batch(7400/7879) done. Loss: 2.2750  lr:0.010000
[ Sun Jul  7 20:14:44 2024 ] 
Training: Epoch [18/120], Step [7499], Loss: 0.7167872190475464, Training Accuracy: 77.73333333333333
[ Sun Jul  7 20:14:44 2024 ] 	Batch(7500/7879) done. Loss: 0.2193  lr:0.010000
[ Sun Jul  7 20:15:07 2024 ] 	Batch(7600/7879) done. Loss: 1.0000  lr:0.010000
[ Sun Jul  7 20:15:29 2024 ] 	Batch(7700/7879) done. Loss: 0.5532  lr:0.010000
[ Sun Jul  7 20:15:52 2024 ] 	Batch(7800/7879) done. Loss: 0.8248  lr:0.010000
[ Sun Jul  7 20:16:09 2024 ] 	Mean training loss: 0.7586.
[ Sun Jul  7 20:16:09 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 20:16:09 2024 ] Training epoch: 20
[ Sun Jul  7 20:16:10 2024 ] 	Batch(0/7879) done. Loss: 1.8041  lr:0.010000
[ Sun Jul  7 20:16:33 2024 ] 	Batch(100/7879) done. Loss: 0.2746  lr:0.010000
[ Sun Jul  7 20:16:55 2024 ] 	Batch(200/7879) done. Loss: 0.1176  lr:0.010000
[ Sun Jul  7 20:17:18 2024 ] 	Batch(300/7879) done. Loss: 0.7310  lr:0.010000
[ Sun Jul  7 20:17:40 2024 ] 	Batch(400/7879) done. Loss: 0.6272  lr:0.010000
[ Sun Jul  7 20:18:03 2024 ] 
Training: Epoch [19/120], Step [499], Loss: 0.9953112602233887, Training Accuracy: 78.10000000000001
[ Sun Jul  7 20:18:03 2024 ] 	Batch(500/7879) done. Loss: 0.4921  lr:0.010000
[ Sun Jul  7 20:18:25 2024 ] 	Batch(600/7879) done. Loss: 0.6973  lr:0.010000
[ Sun Jul  7 20:18:48 2024 ] 	Batch(700/7879) done. Loss: 0.4755  lr:0.010000
[ Sun Jul  7 20:19:11 2024 ] 	Batch(800/7879) done. Loss: 0.3636  lr:0.010000
[ Sun Jul  7 20:19:33 2024 ] 	Batch(900/7879) done. Loss: 0.5895  lr:0.010000
[ Sun Jul  7 20:19:56 2024 ] 
Training: Epoch [19/120], Step [999], Loss: 0.9554363489151001, Training Accuracy: 78.175
[ Sun Jul  7 20:19:56 2024 ] 	Batch(1000/7879) done. Loss: 0.3024  lr:0.010000
[ Sun Jul  7 20:20:18 2024 ] 	Batch(1100/7879) done. Loss: 0.5927  lr:0.010000
[ Sun Jul  7 20:20:41 2024 ] 	Batch(1200/7879) done. Loss: 0.2350  lr:0.010000
[ Sun Jul  7 20:21:04 2024 ] 	Batch(1300/7879) done. Loss: 0.4649  lr:0.010000
[ Sun Jul  7 20:21:26 2024 ] 	Batch(1400/7879) done. Loss: 0.9801  lr:0.010000
[ Sun Jul  7 20:21:49 2024 ] 
Training: Epoch [19/120], Step [1499], Loss: 1.4373118877410889, Training Accuracy: 78.375
[ Sun Jul  7 20:21:49 2024 ] 	Batch(1500/7879) done. Loss: 1.1746  lr:0.010000
[ Sun Jul  7 20:22:12 2024 ] 	Batch(1600/7879) done. Loss: 0.5587  lr:0.010000
[ Sun Jul  7 20:22:34 2024 ] 	Batch(1700/7879) done. Loss: 0.6192  lr:0.010000
[ Sun Jul  7 20:22:57 2024 ] 	Batch(1800/7879) done. Loss: 0.8090  lr:0.010000
[ Sun Jul  7 20:23:19 2024 ] 	Batch(1900/7879) done. Loss: 0.9962  lr:0.010000
[ Sun Jul  7 20:23:42 2024 ] 
Training: Epoch [19/120], Step [1999], Loss: 1.0883572101593018, Training Accuracy: 78.40625
[ Sun Jul  7 20:23:42 2024 ] 	Batch(2000/7879) done. Loss: 0.7327  lr:0.010000
[ Sun Jul  7 20:24:05 2024 ] 	Batch(2100/7879) done. Loss: 0.6461  lr:0.010000
[ Sun Jul  7 20:24:27 2024 ] 	Batch(2200/7879) done. Loss: 0.9943  lr:0.010000
[ Sun Jul  7 20:24:50 2024 ] 	Batch(2300/7879) done. Loss: 0.8047  lr:0.010000
[ Sun Jul  7 20:25:13 2024 ] 	Batch(2400/7879) done. Loss: 0.5398  lr:0.010000
[ Sun Jul  7 20:25:35 2024 ] 
Training: Epoch [19/120], Step [2499], Loss: 0.9952706694602966, Training Accuracy: 78.245
[ Sun Jul  7 20:25:35 2024 ] 	Batch(2500/7879) done. Loss: 1.3732  lr:0.010000
[ Sun Jul  7 20:25:58 2024 ] 	Batch(2600/7879) done. Loss: 1.1859  lr:0.010000
[ Sun Jul  7 20:26:21 2024 ] 	Batch(2700/7879) done. Loss: 0.5682  lr:0.010000
[ Sun Jul  7 20:26:44 2024 ] 	Batch(2800/7879) done. Loss: 0.4374  lr:0.010000
[ Sun Jul  7 20:27:08 2024 ] 	Batch(2900/7879) done. Loss: 1.0303  lr:0.010000
[ Sun Jul  7 20:27:30 2024 ] 
Training: Epoch [19/120], Step [2999], Loss: 0.4308822751045227, Training Accuracy: 78.17083333333333
[ Sun Jul  7 20:27:31 2024 ] 	Batch(3000/7879) done. Loss: 1.2410  lr:0.010000
[ Sun Jul  7 20:27:54 2024 ] 	Batch(3100/7879) done. Loss: 0.9817  lr:0.010000
[ Sun Jul  7 20:28:17 2024 ] 	Batch(3200/7879) done. Loss: 0.4461  lr:0.010000
[ Sun Jul  7 20:28:40 2024 ] 	Batch(3300/7879) done. Loss: 0.4907  lr:0.010000
[ Sun Jul  7 20:29:03 2024 ] 	Batch(3400/7879) done. Loss: 1.1180  lr:0.010000
[ Sun Jul  7 20:29:26 2024 ] 
Training: Epoch [19/120], Step [3499], Loss: 1.0405055284500122, Training Accuracy: 78.31428571428572
[ Sun Jul  7 20:29:26 2024 ] 	Batch(3500/7879) done. Loss: 0.5874  lr:0.010000
[ Sun Jul  7 20:29:50 2024 ] 	Batch(3600/7879) done. Loss: 0.1110  lr:0.010000
[ Sun Jul  7 20:30:13 2024 ] 	Batch(3700/7879) done. Loss: 0.4699  lr:0.010000
[ Sun Jul  7 20:30:36 2024 ] 	Batch(3800/7879) done. Loss: 0.9910  lr:0.010000
[ Sun Jul  7 20:30:58 2024 ] 	Batch(3900/7879) done. Loss: 0.2223  lr:0.010000
[ Sun Jul  7 20:31:21 2024 ] 
Training: Epoch [19/120], Step [3999], Loss: 0.542127788066864, Training Accuracy: 78.2875
[ Sun Jul  7 20:31:21 2024 ] 	Batch(4000/7879) done. Loss: 0.5241  lr:0.010000
[ Sun Jul  7 20:31:44 2024 ] 	Batch(4100/7879) done. Loss: 0.7264  lr:0.010000
[ Sun Jul  7 20:32:07 2024 ] 	Batch(4200/7879) done. Loss: 0.1533  lr:0.010000
[ Sun Jul  7 20:32:30 2024 ] 	Batch(4300/7879) done. Loss: 0.3308  lr:0.010000
[ Sun Jul  7 20:32:53 2024 ] 	Batch(4400/7879) done. Loss: 0.4631  lr:0.010000
[ Sun Jul  7 20:33:16 2024 ] 
Training: Epoch [19/120], Step [4499], Loss: 0.8914504051208496, Training Accuracy: 78.38055555555556
[ Sun Jul  7 20:33:16 2024 ] 	Batch(4500/7879) done. Loss: 0.2235  lr:0.010000
[ Sun Jul  7 20:33:40 2024 ] 	Batch(4600/7879) done. Loss: 1.1416  lr:0.010000
[ Sun Jul  7 20:34:03 2024 ] 	Batch(4700/7879) done. Loss: 0.3046  lr:0.010000
[ Sun Jul  7 20:34:27 2024 ] 	Batch(4800/7879) done. Loss: 2.2952  lr:0.010000
[ Sun Jul  7 20:34:50 2024 ] 	Batch(4900/7879) done. Loss: 0.4338  lr:0.010000
[ Sun Jul  7 20:35:13 2024 ] 
Training: Epoch [19/120], Step [4999], Loss: 0.10099511593580246, Training Accuracy: 78.3725
[ Sun Jul  7 20:35:14 2024 ] 	Batch(5000/7879) done. Loss: 0.6899  lr:0.010000
[ Sun Jul  7 20:35:37 2024 ] 	Batch(5100/7879) done. Loss: 0.6220  lr:0.010000
[ Sun Jul  7 20:36:01 2024 ] 	Batch(5200/7879) done. Loss: 0.5836  lr:0.010000
[ Sun Jul  7 20:36:24 2024 ] 	Batch(5300/7879) done. Loss: 0.7334  lr:0.010000
[ Sun Jul  7 20:36:47 2024 ] 	Batch(5400/7879) done. Loss: 0.2314  lr:0.010000
[ Sun Jul  7 20:37:11 2024 ] 
Training: Epoch [19/120], Step [5499], Loss: 0.7698886394500732, Training Accuracy: 78.39772727272727
[ Sun Jul  7 20:37:11 2024 ] 	Batch(5500/7879) done. Loss: 0.7688  lr:0.010000
[ Sun Jul  7 20:37:35 2024 ] 	Batch(5600/7879) done. Loss: 1.4036  lr:0.010000
[ Sun Jul  7 20:37:58 2024 ] 	Batch(5700/7879) done. Loss: 0.2699  lr:0.010000
[ Sun Jul  7 20:38:21 2024 ] 	Batch(5800/7879) done. Loss: 0.0345  lr:0.010000
[ Sun Jul  7 20:38:45 2024 ] 	Batch(5900/7879) done. Loss: 0.7545  lr:0.010000
[ Sun Jul  7 20:39:08 2024 ] 
Training: Epoch [19/120], Step [5999], Loss: 0.9976634383201599, Training Accuracy: 78.36874999999999
[ Sun Jul  7 20:39:08 2024 ] 	Batch(6000/7879) done. Loss: 0.9220  lr:0.010000
[ Sun Jul  7 20:39:31 2024 ] 	Batch(6100/7879) done. Loss: 1.2054  lr:0.010000
[ Sun Jul  7 20:39:54 2024 ] 	Batch(6200/7879) done. Loss: 0.6622  lr:0.010000
[ Sun Jul  7 20:40:17 2024 ] 	Batch(6300/7879) done. Loss: 0.6228  lr:0.010000
[ Sun Jul  7 20:40:39 2024 ] 	Batch(6400/7879) done. Loss: 0.5274  lr:0.010000
[ Sun Jul  7 20:41:02 2024 ] 
Training: Epoch [19/120], Step [6499], Loss: 0.8206897974014282, Training Accuracy: 78.34423076923078
[ Sun Jul  7 20:41:02 2024 ] 	Batch(6500/7879) done. Loss: 0.4716  lr:0.010000
[ Sun Jul  7 20:41:25 2024 ] 	Batch(6600/7879) done. Loss: 0.7042  lr:0.010000
[ Sun Jul  7 20:41:47 2024 ] 	Batch(6700/7879) done. Loss: 0.0700  lr:0.010000
[ Sun Jul  7 20:42:10 2024 ] 	Batch(6800/7879) done. Loss: 1.0260  lr:0.010000
[ Sun Jul  7 20:42:33 2024 ] 	Batch(6900/7879) done. Loss: 0.8427  lr:0.010000
[ Sun Jul  7 20:42:55 2024 ] 
Training: Epoch [19/120], Step [6999], Loss: 0.5396725535392761, Training Accuracy: 78.28214285714286
[ Sun Jul  7 20:42:56 2024 ] 	Batch(7000/7879) done. Loss: 0.7467  lr:0.010000
[ Sun Jul  7 20:43:18 2024 ] 	Batch(7100/7879) done. Loss: 0.8829  lr:0.010000
[ Sun Jul  7 20:43:41 2024 ] 	Batch(7200/7879) done. Loss: 0.4865  lr:0.010000
[ Sun Jul  7 20:44:04 2024 ] 	Batch(7300/7879) done. Loss: 0.2867  lr:0.010000
[ Sun Jul  7 20:44:28 2024 ] 	Batch(7400/7879) done. Loss: 0.6013  lr:0.010000
[ Sun Jul  7 20:44:50 2024 ] 
Training: Epoch [19/120], Step [7499], Loss: 0.8245666027069092, Training Accuracy: 78.22166666666666
[ Sun Jul  7 20:44:50 2024 ] 	Batch(7500/7879) done. Loss: 0.7477  lr:0.010000
[ Sun Jul  7 20:45:14 2024 ] 	Batch(7600/7879) done. Loss: 0.8224  lr:0.010000
[ Sun Jul  7 20:45:37 2024 ] 	Batch(7700/7879) done. Loss: 0.5914  lr:0.010000
[ Sun Jul  7 20:46:01 2024 ] 	Batch(7800/7879) done. Loss: 0.4737  lr:0.010000
[ Sun Jul  7 20:46:19 2024 ] 	Mean training loss: 0.7269.
[ Sun Jul  7 20:46:19 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 20:46:19 2024 ] Eval epoch: 20
[ Sun Jul  7 20:52:16 2024 ] 	Mean val loss of 6365 batches: 1.1795289476301731.
[ Sun Jul  7 20:52:16 2024 ] 
Validation: Epoch [19/120], Samples [35402.0/50919], Loss: 1.4400081634521484, Validation Accuracy: 69.52611009642766
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 1 : 189 / 275 = 68 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 2 : 219 / 273 = 80 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 3 : 205 / 273 = 75 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 4 : 201 / 275 = 73 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 5 : 214 / 275 = 77 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 6 : 196 / 275 = 71 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 7 : 254 / 273 = 93 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 8 : 258 / 273 = 94 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 9 : 164 / 273 = 60 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 10 : 86 / 273 = 31 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 11 : 118 / 272 = 43 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 12 : 172 / 271 = 63 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 13 : 262 / 275 = 95 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 14 : 247 / 276 = 89 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 15 : 225 / 273 = 82 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 16 : 146 / 274 = 53 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 17 : 215 / 273 = 78 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 18 : 196 / 274 = 71 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 19 : 239 / 272 = 87 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 20 : 248 / 273 = 90 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 21 : 212 / 274 = 77 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 22 : 219 / 274 = 79 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 23 : 245 / 276 = 88 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 24 : 211 / 274 = 77 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 25 : 249 / 275 = 90 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 26 : 264 / 276 = 95 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 27 : 233 / 275 = 84 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 28 : 135 / 275 = 49 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 29 : 154 / 275 = 56 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 30 : 162 / 276 = 58 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 31 : 242 / 276 = 87 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 32 : 227 / 276 = 82 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 33 : 240 / 276 = 86 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 34 : 241 / 276 = 87 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 35 : 223 / 275 = 81 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 36 : 182 / 276 = 65 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 37 : 236 / 276 = 85 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 38 : 246 / 276 = 89 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 39 : 228 / 276 = 82 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 40 : 124 / 276 = 44 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 41 : 243 / 276 = 88 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 42 : 250 / 275 = 90 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 43 : 122 / 276 = 44 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 44 : 213 / 276 = 77 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 45 : 266 / 276 = 96 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 46 : 234 / 276 = 84 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 47 : 214 / 275 = 77 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 48 : 225 / 275 = 81 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 49 : 195 / 274 = 71 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 50 : 237 / 276 = 85 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 51 : 245 / 276 = 88 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 52 : 207 / 276 = 75 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 53 : 237 / 276 = 85 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 54 : 241 / 274 = 87 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 55 : 226 / 276 = 81 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 56 : 231 / 275 = 84 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 57 : 257 / 276 = 93 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 58 : 251 / 273 = 91 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 59 : 239 / 276 = 86 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 60 : 431 / 561 = 76 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 61 : 456 / 566 = 80 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 62 : 368 / 572 = 64 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 63 : 482 / 570 = 84 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 64 : 373 / 574 = 64 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 65 : 466 / 573 = 81 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 66 : 264 / 573 = 46 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 67 : 398 / 575 = 69 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 68 : 195 / 575 = 33 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 69 : 466 / 575 = 81 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 70 : 380 / 575 = 66 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 71 : 101 / 575 = 17 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 72 : 54 / 571 = 9 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 73 : 291 / 570 = 51 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 74 : 251 / 569 = 44 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 75 : 301 / 573 = 52 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 76 : 164 / 574 = 28 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 77 : 316 / 573 = 55 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 78 : 378 / 575 = 65 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 79 : 539 / 574 = 93 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 80 : 385 / 573 = 67 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 81 : 238 / 575 = 41 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 82 : 351 / 575 = 61 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 83 : 79 / 572 = 13 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 84 : 325 / 574 = 56 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 85 : 239 / 574 = 41 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 86 : 495 / 575 = 86 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 87 : 387 / 576 = 67 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 88 : 258 / 575 = 44 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 89 : 442 / 576 = 76 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 90 : 282 / 574 = 49 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 91 : 358 / 568 = 63 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 92 : 333 / 576 = 57 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 93 : 326 / 573 = 56 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 94 : 503 / 574 = 87 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 95 : 534 / 575 = 92 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 96 : 540 / 575 = 93 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 97 : 546 / 574 = 95 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 98 : 511 / 575 = 88 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 99 : 513 / 574 = 89 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 100 : 447 / 574 = 77 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 101 : 516 / 574 = 89 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 102 : 401 / 575 = 69 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 103 : 454 / 576 = 78 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 104 : 256 / 575 = 44 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 105 : 228 / 575 = 39 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 106 : 99 / 576 = 17 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 107 : 507 / 576 = 88 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 108 : 402 / 575 = 69 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 109 : 359 / 575 = 62 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 110 : 479 / 575 = 83 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 111 : 489 / 576 = 84 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 112 : 531 / 575 = 92 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 113 : 459 / 576 = 79 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 114 : 384 / 576 = 66 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 115 : 403 / 576 = 69 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 116 : 441 / 575 = 76 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 117 : 442 / 575 = 76 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 118 : 392 / 575 = 68 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 119 : 517 / 576 = 89 %
[ Sun Jul  7 20:52:16 2024 ] Accuracy of 120 : 217 / 274 = 79 %
[ Sun Jul  7 20:52:16 2024 ] Training epoch: 21
[ Sun Jul  7 20:52:17 2024 ] 	Batch(0/7879) done. Loss: 1.4092  lr:0.010000
[ Sun Jul  7 20:52:40 2024 ] 	Batch(100/7879) done. Loss: 2.0630  lr:0.010000
[ Sun Jul  7 20:53:03 2024 ] 	Batch(200/7879) done. Loss: 0.1183  lr:0.010000
[ Sun Jul  7 20:53:26 2024 ] 	Batch(300/7879) done. Loss: 0.4759  lr:0.010000
[ Sun Jul  7 20:53:49 2024 ] 	Batch(400/7879) done. Loss: 0.2187  lr:0.010000
[ Sun Jul  7 20:54:12 2024 ] 
Training: Epoch [20/120], Step [499], Loss: 0.23075278103351593, Training Accuracy: 79.07499999999999
[ Sun Jul  7 20:54:12 2024 ] 	Batch(500/7879) done. Loss: 0.0471  lr:0.010000
[ Sun Jul  7 20:54:35 2024 ] 	Batch(600/7879) done. Loss: 0.6726  lr:0.010000
[ Sun Jul  7 20:54:58 2024 ] 	Batch(700/7879) done. Loss: 0.3707  lr:0.010000
[ Sun Jul  7 20:55:21 2024 ] 	Batch(800/7879) done. Loss: 1.5952  lr:0.010000
[ Sun Jul  7 20:55:44 2024 ] 	Batch(900/7879) done. Loss: 0.9249  lr:0.010000
[ Sun Jul  7 20:56:07 2024 ] 
Training: Epoch [20/120], Step [999], Loss: 0.2001110017299652, Training Accuracy: 78.5875
[ Sun Jul  7 20:56:07 2024 ] 	Batch(1000/7879) done. Loss: 0.2765  lr:0.010000
[ Sun Jul  7 20:56:30 2024 ] 	Batch(1100/7879) done. Loss: 0.5463  lr:0.010000
[ Sun Jul  7 20:56:53 2024 ] 	Batch(1200/7879) done. Loss: 0.6887  lr:0.010000
[ Sun Jul  7 20:57:16 2024 ] 	Batch(1300/7879) done. Loss: 0.7454  lr:0.010000
[ Sun Jul  7 20:57:39 2024 ] 	Batch(1400/7879) done. Loss: 0.8917  lr:0.010000
[ Sun Jul  7 20:58:02 2024 ] 
Training: Epoch [20/120], Step [1499], Loss: 0.6256392598152161, Training Accuracy: 79.08333333333334
[ Sun Jul  7 20:58:02 2024 ] 	Batch(1500/7879) done. Loss: 0.1619  lr:0.010000
[ Sun Jul  7 20:58:25 2024 ] 	Batch(1600/7879) done. Loss: 1.3724  lr:0.010000
[ Sun Jul  7 20:58:48 2024 ] 	Batch(1700/7879) done. Loss: 0.3012  lr:0.010000
[ Sun Jul  7 20:59:11 2024 ] 	Batch(1800/7879) done. Loss: 0.4965  lr:0.010000
[ Sun Jul  7 20:59:34 2024 ] 	Batch(1900/7879) done. Loss: 1.3135  lr:0.010000
[ Sun Jul  7 20:59:58 2024 ] 
Training: Epoch [20/120], Step [1999], Loss: 0.13757450878620148, Training Accuracy: 78.93124999999999
[ Sun Jul  7 20:59:58 2024 ] 	Batch(2000/7879) done. Loss: 0.3228  lr:0.010000
[ Sun Jul  7 21:00:22 2024 ] 	Batch(2100/7879) done. Loss: 0.4838  lr:0.010000
[ Sun Jul  7 21:00:45 2024 ] 	Batch(2200/7879) done. Loss: 0.8442  lr:0.010000
[ Sun Jul  7 21:01:09 2024 ] 	Batch(2300/7879) done. Loss: 0.6804  lr:0.010000
[ Sun Jul  7 21:01:33 2024 ] 	Batch(2400/7879) done. Loss: 0.6452  lr:0.010000
[ Sun Jul  7 21:01:56 2024 ] 
Training: Epoch [20/120], Step [2499], Loss: 0.422728568315506, Training Accuracy: 78.92
[ Sun Jul  7 21:01:56 2024 ] 	Batch(2500/7879) done. Loss: 0.3097  lr:0.010000
[ Sun Jul  7 21:02:19 2024 ] 	Batch(2600/7879) done. Loss: 0.5023  lr:0.010000
[ Sun Jul  7 21:02:43 2024 ] 	Batch(2700/7879) done. Loss: 0.3329  lr:0.010000
[ Sun Jul  7 21:03:06 2024 ] 	Batch(2800/7879) done. Loss: 0.5187  lr:0.010000
[ Sun Jul  7 21:03:28 2024 ] 	Batch(2900/7879) done. Loss: 1.2285  lr:0.010000
[ Sun Jul  7 21:03:51 2024 ] 
Training: Epoch [20/120], Step [2999], Loss: 0.8584225177764893, Training Accuracy: 79.00416666666666
[ Sun Jul  7 21:03:51 2024 ] 	Batch(3000/7879) done. Loss: 0.1426  lr:0.010000
[ Sun Jul  7 21:04:14 2024 ] 	Batch(3100/7879) done. Loss: 1.3159  lr:0.010000
[ Sun Jul  7 21:04:37 2024 ] 	Batch(3200/7879) done. Loss: 1.6816  lr:0.010000
[ Sun Jul  7 21:04:59 2024 ] 	Batch(3300/7879) done. Loss: 0.1907  lr:0.010000
[ Sun Jul  7 21:05:22 2024 ] 	Batch(3400/7879) done. Loss: 0.1476  lr:0.010000
[ Sun Jul  7 21:05:44 2024 ] 
Training: Epoch [20/120], Step [3499], Loss: 0.8086274266242981, Training Accuracy: 78.78571428571428
[ Sun Jul  7 21:05:45 2024 ] 	Batch(3500/7879) done. Loss: 0.0530  lr:0.010000
[ Sun Jul  7 21:06:08 2024 ] 	Batch(3600/7879) done. Loss: 0.9008  lr:0.010000
[ Sun Jul  7 21:06:31 2024 ] 	Batch(3700/7879) done. Loss: 0.3595  lr:0.010000
[ Sun Jul  7 21:06:55 2024 ] 	Batch(3800/7879) done. Loss: 0.8766  lr:0.010000
[ Sun Jul  7 21:07:18 2024 ] 	Batch(3900/7879) done. Loss: 0.6039  lr:0.010000
[ Sun Jul  7 21:07:42 2024 ] 
Training: Epoch [20/120], Step [3999], Loss: 0.42003902792930603, Training Accuracy: 78.790625
[ Sun Jul  7 21:07:42 2024 ] 	Batch(4000/7879) done. Loss: 0.2514  lr:0.010000
[ Sun Jul  7 21:08:05 2024 ] 	Batch(4100/7879) done. Loss: 0.8597  lr:0.010000
[ Sun Jul  7 21:08:29 2024 ] 	Batch(4200/7879) done. Loss: 1.2408  lr:0.010000
[ Sun Jul  7 21:08:52 2024 ] 	Batch(4300/7879) done. Loss: 1.2827  lr:0.010000
[ Sun Jul  7 21:09:14 2024 ] 	Batch(4400/7879) done. Loss: 0.2517  lr:0.010000
[ Sun Jul  7 21:09:38 2024 ] 
Training: Epoch [20/120], Step [4499], Loss: 0.5188493728637695, Training Accuracy: 78.78055555555555
[ Sun Jul  7 21:09:38 2024 ] 	Batch(4500/7879) done. Loss: 1.1386  lr:0.010000
[ Sun Jul  7 21:10:01 2024 ] 	Batch(4600/7879) done. Loss: 0.8313  lr:0.010000
[ Sun Jul  7 21:10:24 2024 ] 	Batch(4700/7879) done. Loss: 0.9189  lr:0.010000
[ Sun Jul  7 21:10:46 2024 ] 	Batch(4800/7879) done. Loss: 1.5036  lr:0.010000
[ Sun Jul  7 21:11:09 2024 ] 	Batch(4900/7879) done. Loss: 1.5391  lr:0.010000
[ Sun Jul  7 21:11:32 2024 ] 
Training: Epoch [20/120], Step [4999], Loss: 1.6526010036468506, Training Accuracy: 78.75750000000001
[ Sun Jul  7 21:11:32 2024 ] 	Batch(5000/7879) done. Loss: 0.0875  lr:0.010000
[ Sun Jul  7 21:11:55 2024 ] 	Batch(5100/7879) done. Loss: 0.1668  lr:0.010000
[ Sun Jul  7 21:12:17 2024 ] 	Batch(5200/7879) done. Loss: 1.1307  lr:0.010000
[ Sun Jul  7 21:12:41 2024 ] 	Batch(5300/7879) done. Loss: 0.4408  lr:0.010000
[ Sun Jul  7 21:13:04 2024 ] 	Batch(5400/7879) done. Loss: 1.1725  lr:0.010000
[ Sun Jul  7 21:13:27 2024 ] 
Training: Epoch [20/120], Step [5499], Loss: 1.5143026113510132, Training Accuracy: 78.81590909090909
[ Sun Jul  7 21:13:28 2024 ] 	Batch(5500/7879) done. Loss: 0.6546  lr:0.010000
[ Sun Jul  7 21:13:50 2024 ] 	Batch(5600/7879) done. Loss: 1.7269  lr:0.010000
[ Sun Jul  7 21:14:13 2024 ] 	Batch(5700/7879) done. Loss: 0.7228  lr:0.010000
[ Sun Jul  7 21:14:36 2024 ] 	Batch(5800/7879) done. Loss: 0.5454  lr:0.010000
[ Sun Jul  7 21:14:59 2024 ] 	Batch(5900/7879) done. Loss: 0.4725  lr:0.010000
[ Sun Jul  7 21:15:21 2024 ] 
Training: Epoch [20/120], Step [5999], Loss: 1.2526447772979736, Training Accuracy: 78.77916666666667
[ Sun Jul  7 21:15:21 2024 ] 	Batch(6000/7879) done. Loss: 1.1286  lr:0.010000
[ Sun Jul  7 21:15:44 2024 ] 	Batch(6100/7879) done. Loss: 0.9179  lr:0.010000
[ Sun Jul  7 21:16:07 2024 ] 	Batch(6200/7879) done. Loss: 0.9533  lr:0.010000
[ Sun Jul  7 21:16:29 2024 ] 	Batch(6300/7879) done. Loss: 1.0732  lr:0.010000
[ Sun Jul  7 21:16:52 2024 ] 	Batch(6400/7879) done. Loss: 0.3744  lr:0.010000
[ Sun Jul  7 21:17:16 2024 ] 
Training: Epoch [20/120], Step [6499], Loss: 0.769233226776123, Training Accuracy: 78.77115384615384
[ Sun Jul  7 21:17:16 2024 ] 	Batch(6500/7879) done. Loss: 1.5100  lr:0.010000
[ Sun Jul  7 21:17:39 2024 ] 	Batch(6600/7879) done. Loss: 0.7622  lr:0.010000
[ Sun Jul  7 21:18:02 2024 ] 	Batch(6700/7879) done. Loss: 0.7378  lr:0.010000
[ Sun Jul  7 21:18:25 2024 ] 	Batch(6800/7879) done. Loss: 0.9145  lr:0.010000
[ Sun Jul  7 21:18:48 2024 ] 	Batch(6900/7879) done. Loss: 1.1226  lr:0.010000
[ Sun Jul  7 21:19:10 2024 ] 
Training: Epoch [20/120], Step [6999], Loss: 0.7178969383239746, Training Accuracy: 78.71607142857144
[ Sun Jul  7 21:19:11 2024 ] 	Batch(7000/7879) done. Loss: 1.5129  lr:0.010000
[ Sun Jul  7 21:19:33 2024 ] 	Batch(7100/7879) done. Loss: 0.7532  lr:0.010000
[ Sun Jul  7 21:19:56 2024 ] 	Batch(7200/7879) done. Loss: 0.5186  lr:0.010000
[ Sun Jul  7 21:20:19 2024 ] 	Batch(7300/7879) done. Loss: 1.8471  lr:0.010000
[ Sun Jul  7 21:20:41 2024 ] 	Batch(7400/7879) done. Loss: 0.5988  lr:0.010000
[ Sun Jul  7 21:21:04 2024 ] 
Training: Epoch [20/120], Step [7499], Loss: 0.7682567238807678, Training Accuracy: 78.68
[ Sun Jul  7 21:21:04 2024 ] 	Batch(7500/7879) done. Loss: 0.6334  lr:0.010000
[ Sun Jul  7 21:21:27 2024 ] 	Batch(7600/7879) done. Loss: 0.0223  lr:0.010000
[ Sun Jul  7 21:21:50 2024 ] 	Batch(7700/7879) done. Loss: 0.8596  lr:0.010000
[ Sun Jul  7 21:22:12 2024 ] 	Batch(7800/7879) done. Loss: 0.8875  lr:0.010000
[ Sun Jul  7 21:22:30 2024 ] 	Mean training loss: 0.7043.
[ Sun Jul  7 21:22:30 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul  7 21:22:30 2024 ] Training epoch: 22
[ Sun Jul  7 21:22:31 2024 ] 	Batch(0/7879) done. Loss: 0.5376  lr:0.010000
[ Sun Jul  7 21:22:53 2024 ] 	Batch(100/7879) done. Loss: 0.1046  lr:0.010000
[ Sun Jul  7 21:23:16 2024 ] 	Batch(200/7879) done. Loss: 1.5418  lr:0.010000
[ Sun Jul  7 21:23:39 2024 ] 	Batch(300/7879) done. Loss: 0.9166  lr:0.010000
[ Sun Jul  7 21:24:02 2024 ] 	Batch(400/7879) done. Loss: 0.6798  lr:0.010000
[ Sun Jul  7 21:24:24 2024 ] 
Training: Epoch [21/120], Step [499], Loss: 0.6890011429786682, Training Accuracy: 78.9
[ Sun Jul  7 21:24:25 2024 ] 	Batch(500/7879) done. Loss: 0.4744  lr:0.010000
[ Sun Jul  7 21:24:47 2024 ] 	Batch(600/7879) done. Loss: 0.7555  lr:0.010000
[ Sun Jul  7 21:25:10 2024 ] 	Batch(700/7879) done. Loss: 0.5332  lr:0.010000
[ Sun Jul  7 21:25:33 2024 ] 	Batch(800/7879) done. Loss: 1.2357  lr:0.010000
[ Sun Jul  7 21:25:55 2024 ] 	Batch(900/7879) done. Loss: 1.2580  lr:0.010000
[ Sun Jul  7 21:26:18 2024 ] 
Training: Epoch [21/120], Step [999], Loss: 0.530146598815918, Training Accuracy: 78.6875
[ Sun Jul  7 21:26:18 2024 ] 	Batch(1000/7879) done. Loss: 0.6688  lr:0.010000
[ Sun Jul  7 21:26:41 2024 ] 	Batch(1100/7879) done. Loss: 0.5582  lr:0.010000
[ Sun Jul  7 21:27:04 2024 ] 	Batch(1200/7879) done. Loss: 0.8180  lr:0.010000
[ Sun Jul  7 21:27:26 2024 ] 	Batch(1300/7879) done. Loss: 1.2082  lr:0.010000
[ Sun Jul  7 21:27:50 2024 ] 	Batch(1400/7879) done. Loss: 0.6640  lr:0.010000
[ Sun Jul  7 21:28:13 2024 ] 
Training: Epoch [21/120], Step [1499], Loss: 0.4814431071281433, Training Accuracy: 78.975
[ Sun Jul  7 21:28:13 2024 ] 	Batch(1500/7879) done. Loss: 0.5765  lr:0.010000
[ Sun Jul  7 21:28:37 2024 ] 	Batch(1600/7879) done. Loss: 0.5581  lr:0.010000
[ Sun Jul  7 21:29:00 2024 ] 	Batch(1700/7879) done. Loss: 0.5107  lr:0.010000
[ Sun Jul  7 21:29:24 2024 ] 	Batch(1800/7879) done. Loss: 1.0156  lr:0.010000
[ Sun Jul  7 21:29:47 2024 ] 	Batch(1900/7879) done. Loss: 0.3393  lr:0.010000
[ Sun Jul  7 21:30:09 2024 ] 
Training: Epoch [21/120], Step [1999], Loss: 0.37021684646606445, Training Accuracy: 78.99374999999999
[ Sun Jul  7 21:30:09 2024 ] 	Batch(2000/7879) done. Loss: 0.1634  lr:0.010000
[ Sun Jul  7 21:30:32 2024 ] 	Batch(2100/7879) done. Loss: 0.4495  lr:0.010000
[ Sun Jul  7 21:30:55 2024 ] 	Batch(2200/7879) done. Loss: 0.7268  lr:0.010000
[ Sun Jul  7 21:31:18 2024 ] 	Batch(2300/7879) done. Loss: 0.8120  lr:0.010000
[ Sun Jul  7 21:31:40 2024 ] 	Batch(2400/7879) done. Loss: 0.1810  lr:0.010000
[ Sun Jul  7 21:32:03 2024 ] 
Training: Epoch [21/120], Step [2499], Loss: 2.1240792274475098, Training Accuracy: 78.94
[ Sun Jul  7 21:32:03 2024 ] 	Batch(2500/7879) done. Loss: 0.6568  lr:0.010000
[ Sun Jul  7 21:32:26 2024 ] 	Batch(2600/7879) done. Loss: 0.7298  lr:0.010000
[ Sun Jul  7 21:32:48 2024 ] 	Batch(2700/7879) done. Loss: 0.6344  lr:0.010000
[ Sun Jul  7 21:33:11 2024 ] 	Batch(2800/7879) done. Loss: 1.0540  lr:0.010000
[ Sun Jul  7 21:33:34 2024 ] 	Batch(2900/7879) done. Loss: 0.6719  lr:0.010000
[ Sun Jul  7 21:33:56 2024 ] 
Training: Epoch [21/120], Step [2999], Loss: 0.1780296415090561, Training Accuracy: 79.09583333333333
[ Sun Jul  7 21:33:57 2024 ] 	Batch(3000/7879) done. Loss: 0.7782  lr:0.010000
[ Sun Jul  7 21:34:19 2024 ] 	Batch(3100/7879) done. Loss: 0.4839  lr:0.010000
[ Sun Jul  7 21:34:42 2024 ] 	Batch(3200/7879) done. Loss: 0.6464  lr:0.010000
[ Sun Jul  7 21:35:06 2024 ] 	Batch(3300/7879) done. Loss: 0.2516  lr:0.010000
[ Sun Jul  7 21:35:29 2024 ] 	Batch(3400/7879) done. Loss: 0.6166  lr:0.010000
[ Sun Jul  7 21:35:52 2024 ] 
Training: Epoch [21/120], Step [3499], Loss: 0.8478568196296692, Training Accuracy: 79.07499999999999
[ Sun Jul  7 21:35:52 2024 ] 	Batch(3500/7879) done. Loss: 0.3716  lr:0.010000
[ Sun Jul  7 21:36:15 2024 ] 	Batch(3600/7879) done. Loss: 0.8752  lr:0.010000
[ Sun Jul  7 21:36:38 2024 ] 	Batch(3700/7879) done. Loss: 0.4563  lr:0.010000
[ Sun Jul  7 21:37:00 2024 ] 	Batch(3800/7879) done. Loss: 0.5497  lr:0.010000
[ Sun Jul  7 21:37:23 2024 ] 	Batch(3900/7879) done. Loss: 0.7632  lr:0.010000
[ Sun Jul  7 21:37:46 2024 ] 
Training: Epoch [21/120], Step [3999], Loss: 1.1876990795135498, Training Accuracy: 78.95
[ Sun Jul  7 21:37:46 2024 ] 	Batch(4000/7879) done. Loss: 0.8795  lr:0.010000
[ Sun Jul  7 21:38:09 2024 ] 	Batch(4100/7879) done. Loss: 0.5104  lr:0.010000
[ Sun Jul  7 21:38:32 2024 ] 	Batch(4200/7879) done. Loss: 0.7883  lr:0.010000
[ Sun Jul  7 21:38:54 2024 ] 	Batch(4300/7879) done. Loss: 0.6054  lr:0.010000
[ Sun Jul  7 21:39:17 2024 ] 	Batch(4400/7879) done. Loss: 0.0735  lr:0.010000
[ Sun Jul  7 21:39:40 2024 ] 
Training: Epoch [21/120], Step [4499], Loss: 0.5792367458343506, Training Accuracy: 78.91666666666667
[ Sun Jul  7 21:39:40 2024 ] 	Batch(4500/7879) done. Loss: 1.1165  lr:0.010000
[ Sun Jul  7 21:40:03 2024 ] 	Batch(4600/7879) done. Loss: 0.1036  lr:0.010000
[ Sun Jul  7 21:40:25 2024 ] 	Batch(4700/7879) done. Loss: 0.3090  lr:0.010000
[ Sun Jul  7 21:40:48 2024 ] 	Batch(4800/7879) done. Loss: 0.5642  lr:0.010000
[ Sun Jul  7 21:41:12 2024 ] 	Batch(4900/7879) done. Loss: 0.2901  lr:0.010000
[ Sun Jul  7 21:41:34 2024 ] 
Training: Epoch [21/120], Step [4999], Loss: 0.9060689210891724, Training Accuracy: 79.045
[ Sun Jul  7 21:41:34 2024 ] 	Batch(5000/7879) done. Loss: 1.5177  lr:0.010000
[ Sun Jul  7 21:41:57 2024 ] 	Batch(5100/7879) done. Loss: 0.3076  lr:0.010000
[ Sun Jul  7 21:42:20 2024 ] 	Batch(5200/7879) done. Loss: 0.3876  lr:0.010000
[ Sun Jul  7 21:42:43 2024 ] 	Batch(5300/7879) done. Loss: 0.4539  lr:0.010000
[ Sun Jul  7 21:43:06 2024 ] 	Batch(5400/7879) done. Loss: 1.1748  lr:0.010000
[ Sun Jul  7 21:43:28 2024 ] 
Training: Epoch [21/120], Step [5499], Loss: 1.2996946573257446, Training Accuracy: 79.07045454545455
[ Sun Jul  7 21:43:28 2024 ] 	Batch(5500/7879) done. Loss: 1.5226  lr:0.010000
[ Sun Jul  7 21:43:51 2024 ] 	Batch(5600/7879) done. Loss: 0.9085  lr:0.010000
[ Sun Jul  7 21:44:14 2024 ] 	Batch(5700/7879) done. Loss: 0.3899  lr:0.010000
[ Sun Jul  7 21:44:37 2024 ] 	Batch(5800/7879) done. Loss: 0.5384  lr:0.010000
[ Sun Jul  7 21:44:59 2024 ] 	Batch(5900/7879) done. Loss: 0.4084  lr:0.010000
[ Sun Jul  7 21:45:22 2024 ] 
Training: Epoch [21/120], Step [5999], Loss: 1.2197544574737549, Training Accuracy: 79.10625
[ Sun Jul  7 21:45:22 2024 ] 	Batch(6000/7879) done. Loss: 0.2515  lr:0.010000
[ Sun Jul  7 21:45:45 2024 ] 	Batch(6100/7879) done. Loss: 0.4512  lr:0.010000
[ Sun Jul  7 21:46:08 2024 ] 	Batch(6200/7879) done. Loss: 0.0385  lr:0.010000
[ Sun Jul  7 21:46:30 2024 ] 	Batch(6300/7879) done. Loss: 0.8022  lr:0.010000
[ Sun Jul  7 21:46:53 2024 ] 	Batch(6400/7879) done. Loss: 0.1510  lr:0.010000
[ Sun Jul  7 21:47:16 2024 ] 
Training: Epoch [21/120], Step [6499], Loss: 0.37121960520744324, Training Accuracy: 79.06346153846154
[ Sun Jul  7 21:47:16 2024 ] 	Batch(6500/7879) done. Loss: 1.5023  lr:0.010000
[ Sun Jul  7 21:47:39 2024 ] 	Batch(6600/7879) done. Loss: 1.0299  lr:0.010000
[ Sun Jul  7 21:48:01 2024 ] 	Batch(6700/7879) done. Loss: 1.6316  lr:0.010000
[ Sun Jul  7 21:48:24 2024 ] 	Batch(6800/7879) done. Loss: 0.8523  lr:0.010000
[ Sun Jul  7 21:48:47 2024 ] 	Batch(6900/7879) done. Loss: 1.1205  lr:0.010000
[ Sun Jul  7 21:49:09 2024 ] 
Training: Epoch [21/120], Step [6999], Loss: 0.6200461983680725, Training Accuracy: 78.97321428571429
[ Sun Jul  7 21:49:10 2024 ] 	Batch(7000/7879) done. Loss: 0.0802  lr:0.010000
[ Sun Jul  7 21:49:32 2024 ] 	Batch(7100/7879) done. Loss: 1.5752  lr:0.010000
[ Sun Jul  7 21:49:55 2024 ] 	Batch(7200/7879) done. Loss: 1.0884  lr:0.010000
[ Sun Jul  7 21:50:18 2024 ] 	Batch(7300/7879) done. Loss: 0.7899  lr:0.010000
[ Sun Jul  7 21:50:41 2024 ] 	Batch(7400/7879) done. Loss: 0.9052  lr:0.010000
[ Sun Jul  7 21:51:03 2024 ] 
Training: Epoch [21/120], Step [7499], Loss: 0.6259879469871521, Training Accuracy: 78.89
[ Sun Jul  7 21:51:03 2024 ] 	Batch(7500/7879) done. Loss: 0.9189  lr:0.010000
[ Sun Jul  7 21:51:26 2024 ] 	Batch(7600/7879) done. Loss: 0.4288  lr:0.010000
[ Sun Jul  7 21:51:49 2024 ] 	Batch(7700/7879) done. Loss: 0.6951  lr:0.010000
[ Sun Jul  7 21:52:12 2024 ] 	Batch(7800/7879) done. Loss: 0.0248  lr:0.010000
[ Sun Jul  7 21:52:29 2024 ] 	Mean training loss: 0.7013.
[ Sun Jul  7 21:52:29 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 21:52:29 2024 ] Training epoch: 23
[ Sun Jul  7 21:52:30 2024 ] 	Batch(0/7879) done. Loss: 0.4244  lr:0.010000
[ Sun Jul  7 21:52:53 2024 ] 	Batch(100/7879) done. Loss: 0.4128  lr:0.010000
[ Sun Jul  7 21:53:15 2024 ] 	Batch(200/7879) done. Loss: 0.6002  lr:0.010000
[ Sun Jul  7 21:53:38 2024 ] 	Batch(300/7879) done. Loss: 0.8971  lr:0.010000
[ Sun Jul  7 21:54:01 2024 ] 	Batch(400/7879) done. Loss: 0.6025  lr:0.010000
[ Sun Jul  7 21:54:23 2024 ] 
Training: Epoch [22/120], Step [499], Loss: 0.8372436761856079, Training Accuracy: 81.125
[ Sun Jul  7 21:54:23 2024 ] 	Batch(500/7879) done. Loss: 0.4146  lr:0.010000
[ Sun Jul  7 21:54:46 2024 ] 	Batch(600/7879) done. Loss: 0.4982  lr:0.010000
[ Sun Jul  7 21:55:09 2024 ] 	Batch(700/7879) done. Loss: 0.0897  lr:0.010000
[ Sun Jul  7 21:55:32 2024 ] 	Batch(800/7879) done. Loss: 1.0300  lr:0.010000
[ Sun Jul  7 21:55:55 2024 ] 	Batch(900/7879) done. Loss: 0.8823  lr:0.010000
[ Sun Jul  7 21:56:17 2024 ] 
Training: Epoch [22/120], Step [999], Loss: 0.4555579125881195, Training Accuracy: 80.6875
[ Sun Jul  7 21:56:17 2024 ] 	Batch(1000/7879) done. Loss: 0.4180  lr:0.010000
[ Sun Jul  7 21:56:40 2024 ] 	Batch(1100/7879) done. Loss: 0.2076  lr:0.010000
[ Sun Jul  7 21:57:03 2024 ] 	Batch(1200/7879) done. Loss: 0.4114  lr:0.010000
[ Sun Jul  7 21:57:26 2024 ] 	Batch(1300/7879) done. Loss: 0.4943  lr:0.010000
[ Sun Jul  7 21:57:48 2024 ] 	Batch(1400/7879) done. Loss: 1.2881  lr:0.010000
[ Sun Jul  7 21:58:11 2024 ] 
Training: Epoch [22/120], Step [1499], Loss: 1.0515769720077515, Training Accuracy: 80.05833333333334
[ Sun Jul  7 21:58:11 2024 ] 	Batch(1500/7879) done. Loss: 0.3104  lr:0.010000
[ Sun Jul  7 21:58:34 2024 ] 	Batch(1600/7879) done. Loss: 1.7588  lr:0.010000
[ Sun Jul  7 21:58:57 2024 ] 	Batch(1700/7879) done. Loss: 0.2168  lr:0.010000
[ Sun Jul  7 21:59:21 2024 ] 	Batch(1800/7879) done. Loss: 0.8292  lr:0.010000
[ Sun Jul  7 21:59:44 2024 ] 	Batch(1900/7879) done. Loss: 0.5295  lr:0.010000
[ Sun Jul  7 22:00:07 2024 ] 
Training: Epoch [22/120], Step [1999], Loss: 0.8915913701057434, Training Accuracy: 79.93124999999999
[ Sun Jul  7 22:00:07 2024 ] 	Batch(2000/7879) done. Loss: 0.0387  lr:0.010000
[ Sun Jul  7 22:00:30 2024 ] 	Batch(2100/7879) done. Loss: 0.7187  lr:0.010000
[ Sun Jul  7 22:00:52 2024 ] 	Batch(2200/7879) done. Loss: 0.2491  lr:0.010000
[ Sun Jul  7 22:01:15 2024 ] 	Batch(2300/7879) done. Loss: 1.8786  lr:0.010000
[ Sun Jul  7 22:01:38 2024 ] 	Batch(2400/7879) done. Loss: 0.2670  lr:0.010000
[ Sun Jul  7 22:02:00 2024 ] 
Training: Epoch [22/120], Step [2499], Loss: 0.3247152268886566, Training Accuracy: 79.975
[ Sun Jul  7 22:02:01 2024 ] 	Batch(2500/7879) done. Loss: 0.5562  lr:0.010000
[ Sun Jul  7 22:02:23 2024 ] 	Batch(2600/7879) done. Loss: 0.7122  lr:0.010000
[ Sun Jul  7 22:02:46 2024 ] 	Batch(2700/7879) done. Loss: 0.7782  lr:0.010000
[ Sun Jul  7 22:03:09 2024 ] 	Batch(2800/7879) done. Loss: 0.2479  lr:0.010000
[ Sun Jul  7 22:03:32 2024 ] 	Batch(2900/7879) done. Loss: 0.3378  lr:0.010000
[ Sun Jul  7 22:03:54 2024 ] 
Training: Epoch [22/120], Step [2999], Loss: 0.8111999034881592, Training Accuracy: 80.0125
[ Sun Jul  7 22:03:54 2024 ] 	Batch(3000/7879) done. Loss: 0.8399  lr:0.010000
[ Sun Jul  7 22:04:17 2024 ] 	Batch(3100/7879) done. Loss: 0.4351  lr:0.010000
[ Sun Jul  7 22:04:40 2024 ] 	Batch(3200/7879) done. Loss: 0.7495  lr:0.010000
[ Sun Jul  7 22:05:02 2024 ] 	Batch(3300/7879) done. Loss: 0.1543  lr:0.010000
[ Sun Jul  7 22:05:25 2024 ] 	Batch(3400/7879) done. Loss: 1.0715  lr:0.010000
[ Sun Jul  7 22:05:48 2024 ] 
Training: Epoch [22/120], Step [3499], Loss: 1.00862717628479, Training Accuracy: 79.875
[ Sun Jul  7 22:05:48 2024 ] 	Batch(3500/7879) done. Loss: 0.0735  lr:0.010000
[ Sun Jul  7 22:06:11 2024 ] 	Batch(3600/7879) done. Loss: 0.1476  lr:0.010000
[ Sun Jul  7 22:06:33 2024 ] 	Batch(3700/7879) done. Loss: 0.5758  lr:0.010000
[ Sun Jul  7 22:06:56 2024 ] 	Batch(3800/7879) done. Loss: 0.3822  lr:0.010000
[ Sun Jul  7 22:07:19 2024 ] 	Batch(3900/7879) done. Loss: 0.2937  lr:0.010000
[ Sun Jul  7 22:07:41 2024 ] 
Training: Epoch [22/120], Step [3999], Loss: 0.2612614929676056, Training Accuracy: 79.934375
[ Sun Jul  7 22:07:42 2024 ] 	Batch(4000/7879) done. Loss: 1.4887  lr:0.010000
[ Sun Jul  7 22:08:04 2024 ] 	Batch(4100/7879) done. Loss: 0.8420  lr:0.010000
[ Sun Jul  7 22:08:27 2024 ] 	Batch(4200/7879) done. Loss: 0.2925  lr:0.010000
[ Sun Jul  7 22:08:50 2024 ] 	Batch(4300/7879) done. Loss: 0.2849  lr:0.010000
[ Sun Jul  7 22:09:13 2024 ] 	Batch(4400/7879) done. Loss: 0.7929  lr:0.010000
[ Sun Jul  7 22:09:35 2024 ] 
Training: Epoch [22/120], Step [4499], Loss: 0.822492778301239, Training Accuracy: 79.93055555555556
[ Sun Jul  7 22:09:35 2024 ] 	Batch(4500/7879) done. Loss: 0.9228  lr:0.010000
[ Sun Jul  7 22:09:58 2024 ] 	Batch(4600/7879) done. Loss: 0.5983  lr:0.010000
[ Sun Jul  7 22:10:21 2024 ] 	Batch(4700/7879) done. Loss: 0.7273  lr:0.010000
[ Sun Jul  7 22:10:43 2024 ] 	Batch(4800/7879) done. Loss: 0.3485  lr:0.010000
[ Sun Jul  7 22:11:07 2024 ] 	Batch(4900/7879) done. Loss: 0.3035  lr:0.010000
[ Sun Jul  7 22:11:30 2024 ] 
Training: Epoch [22/120], Step [4999], Loss: 0.9525632262229919, Training Accuracy: 79.9225
[ Sun Jul  7 22:11:30 2024 ] 	Batch(5000/7879) done. Loss: 0.1775  lr:0.010000
[ Sun Jul  7 22:11:54 2024 ] 	Batch(5100/7879) done. Loss: 0.2420  lr:0.010000
[ Sun Jul  7 22:12:18 2024 ] 	Batch(5200/7879) done. Loss: 0.0440  lr:0.010000
[ Sun Jul  7 22:12:41 2024 ] 	Batch(5300/7879) done. Loss: 0.0866  lr:0.010000
[ Sun Jul  7 22:13:05 2024 ] 	Batch(5400/7879) done. Loss: 2.0316  lr:0.010000
[ Sun Jul  7 22:13:28 2024 ] 
Training: Epoch [22/120], Step [5499], Loss: 0.11695552617311478, Training Accuracy: 79.81818181818183
[ Sun Jul  7 22:13:28 2024 ] 	Batch(5500/7879) done. Loss: 0.9124  lr:0.010000
[ Sun Jul  7 22:13:52 2024 ] 	Batch(5600/7879) done. Loss: 0.8376  lr:0.010000
[ Sun Jul  7 22:14:14 2024 ] 	Batch(5700/7879) done. Loss: 0.4274  lr:0.010000
[ Sun Jul  7 22:14:37 2024 ] 	Batch(5800/7879) done. Loss: 0.1803  lr:0.010000
[ Sun Jul  7 22:15:00 2024 ] 	Batch(5900/7879) done. Loss: 1.3729  lr:0.010000
[ Sun Jul  7 22:15:22 2024 ] 
Training: Epoch [22/120], Step [5999], Loss: 1.1369997262954712, Training Accuracy: 79.83541666666667
[ Sun Jul  7 22:15:23 2024 ] 	Batch(6000/7879) done. Loss: 0.8147  lr:0.010000
[ Sun Jul  7 22:15:45 2024 ] 	Batch(6100/7879) done. Loss: 0.9709  lr:0.010000
[ Sun Jul  7 22:16:08 2024 ] 	Batch(6200/7879) done. Loss: 0.2695  lr:0.010000
[ Sun Jul  7 22:16:31 2024 ] 	Batch(6300/7879) done. Loss: 0.6719  lr:0.010000
[ Sun Jul  7 22:16:54 2024 ] 	Batch(6400/7879) done. Loss: 0.7504  lr:0.010000
[ Sun Jul  7 22:17:16 2024 ] 
Training: Epoch [22/120], Step [6499], Loss: 0.9538807272911072, Training Accuracy: 79.81923076923077
[ Sun Jul  7 22:17:16 2024 ] 	Batch(6500/7879) done. Loss: 0.2001  lr:0.010000
[ Sun Jul  7 22:17:39 2024 ] 	Batch(6600/7879) done. Loss: 0.4545  lr:0.010000
[ Sun Jul  7 22:18:02 2024 ] 	Batch(6700/7879) done. Loss: 0.4061  lr:0.010000
[ Sun Jul  7 22:18:25 2024 ] 	Batch(6800/7879) done. Loss: 0.0461  lr:0.010000
[ Sun Jul  7 22:18:47 2024 ] 	Batch(6900/7879) done. Loss: 0.2056  lr:0.010000
[ Sun Jul  7 22:19:10 2024 ] 
Training: Epoch [22/120], Step [6999], Loss: 0.13336831331253052, Training Accuracy: 79.90892857142858
[ Sun Jul  7 22:19:10 2024 ] 	Batch(7000/7879) done. Loss: 0.7152  lr:0.010000
[ Sun Jul  7 22:19:33 2024 ] 	Batch(7100/7879) done. Loss: 0.1729  lr:0.010000
[ Sun Jul  7 22:19:56 2024 ] 	Batch(7200/7879) done. Loss: 0.5001  lr:0.010000
[ Sun Jul  7 22:20:19 2024 ] 	Batch(7300/7879) done. Loss: 0.9713  lr:0.010000
[ Sun Jul  7 22:20:43 2024 ] 	Batch(7400/7879) done. Loss: 0.9404  lr:0.010000
[ Sun Jul  7 22:21:05 2024 ] 
Training: Epoch [22/120], Step [7499], Loss: 0.28725361824035645, Training Accuracy: 79.81833333333334
[ Sun Jul  7 22:21:05 2024 ] 	Batch(7500/7879) done. Loss: 1.1682  lr:0.010000
[ Sun Jul  7 22:21:28 2024 ] 	Batch(7600/7879) done. Loss: 0.8417  lr:0.010000
[ Sun Jul  7 22:21:51 2024 ] 	Batch(7700/7879) done. Loss: 0.0334  lr:0.010000
[ Sun Jul  7 22:22:14 2024 ] 	Batch(7800/7879) done. Loss: 0.3188  lr:0.010000
[ Sun Jul  7 22:22:31 2024 ] 	Mean training loss: 0.6801.
[ Sun Jul  7 22:22:31 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 22:22:32 2024 ] Training epoch: 24
[ Sun Jul  7 22:22:32 2024 ] 	Batch(0/7879) done. Loss: 0.6880  lr:0.010000
[ Sun Jul  7 22:22:55 2024 ] 	Batch(100/7879) done. Loss: 0.5731  lr:0.010000
[ Sun Jul  7 22:23:18 2024 ] 	Batch(200/7879) done. Loss: 1.4614  lr:0.010000
[ Sun Jul  7 22:23:40 2024 ] 	Batch(300/7879) done. Loss: 1.2871  lr:0.010000
[ Sun Jul  7 22:24:03 2024 ] 	Batch(400/7879) done. Loss: 0.1770  lr:0.010000
[ Sun Jul  7 22:24:26 2024 ] 
Training: Epoch [23/120], Step [499], Loss: 0.7449366450309753, Training Accuracy: 81.075
[ Sun Jul  7 22:24:26 2024 ] 	Batch(500/7879) done. Loss: 0.7421  lr:0.010000
[ Sun Jul  7 22:24:48 2024 ] 	Batch(600/7879) done. Loss: 0.6464  lr:0.010000
[ Sun Jul  7 22:25:11 2024 ] 	Batch(700/7879) done. Loss: 0.1139  lr:0.010000
[ Sun Jul  7 22:25:34 2024 ] 	Batch(800/7879) done. Loss: 0.0959  lr:0.010000
[ Sun Jul  7 22:25:57 2024 ] 	Batch(900/7879) done. Loss: 0.9307  lr:0.010000
[ Sun Jul  7 22:26:19 2024 ] 
Training: Epoch [23/120], Step [999], Loss: 0.6550126075744629, Training Accuracy: 81.5
[ Sun Jul  7 22:26:19 2024 ] 	Batch(1000/7879) done. Loss: 0.2138  lr:0.010000
[ Sun Jul  7 22:26:42 2024 ] 	Batch(1100/7879) done. Loss: 0.2625  lr:0.010000
[ Sun Jul  7 22:27:05 2024 ] 	Batch(1200/7879) done. Loss: 0.4328  lr:0.010000
[ Sun Jul  7 22:27:28 2024 ] 	Batch(1300/7879) done. Loss: 1.0351  lr:0.010000
[ Sun Jul  7 22:27:51 2024 ] 	Batch(1400/7879) done. Loss: 0.4616  lr:0.010000
[ Sun Jul  7 22:28:14 2024 ] 
Training: Epoch [23/120], Step [1499], Loss: 0.09782601147890091, Training Accuracy: 81.58333333333333
[ Sun Jul  7 22:28:14 2024 ] 	Batch(1500/7879) done. Loss: 0.8853  lr:0.010000
[ Sun Jul  7 22:28:37 2024 ] 	Batch(1600/7879) done. Loss: 0.1145  lr:0.010000
[ Sun Jul  7 22:29:00 2024 ] 	Batch(1700/7879) done. Loss: 0.7913  lr:0.010000
[ Sun Jul  7 22:29:22 2024 ] 	Batch(1800/7879) done. Loss: 0.5515  lr:0.010000
[ Sun Jul  7 22:29:45 2024 ] 	Batch(1900/7879) done. Loss: 0.0261  lr:0.010000
[ Sun Jul  7 22:30:08 2024 ] 
Training: Epoch [23/120], Step [1999], Loss: 0.45938223600387573, Training Accuracy: 81.19375
[ Sun Jul  7 22:30:08 2024 ] 	Batch(2000/7879) done. Loss: 0.7441  lr:0.010000
[ Sun Jul  7 22:30:31 2024 ] 	Batch(2100/7879) done. Loss: 0.1355  lr:0.010000
[ Sun Jul  7 22:30:54 2024 ] 	Batch(2200/7879) done. Loss: 1.3572  lr:0.010000
[ Sun Jul  7 22:31:16 2024 ] 	Batch(2300/7879) done. Loss: 0.2832  lr:0.010000
[ Sun Jul  7 22:31:39 2024 ] 	Batch(2400/7879) done. Loss: 0.2900  lr:0.010000
[ Sun Jul  7 22:32:02 2024 ] 
Training: Epoch [23/120], Step [2499], Loss: 0.8857352137565613, Training Accuracy: 80.72500000000001
[ Sun Jul  7 22:32:02 2024 ] 	Batch(2500/7879) done. Loss: 0.2368  lr:0.010000
[ Sun Jul  7 22:32:24 2024 ] 	Batch(2600/7879) done. Loss: 0.3654  lr:0.010000
[ Sun Jul  7 22:32:47 2024 ] 	Batch(2700/7879) done. Loss: 1.8548  lr:0.010000
[ Sun Jul  7 22:33:10 2024 ] 	Batch(2800/7879) done. Loss: 0.7804  lr:0.010000
[ Sun Jul  7 22:33:33 2024 ] 	Batch(2900/7879) done. Loss: 0.9398  lr:0.010000
[ Sun Jul  7 22:33:55 2024 ] 
Training: Epoch [23/120], Step [2999], Loss: 1.3056021928787231, Training Accuracy: 80.6125
[ Sun Jul  7 22:33:55 2024 ] 	Batch(3000/7879) done. Loss: 0.4483  lr:0.010000
[ Sun Jul  7 22:34:18 2024 ] 	Batch(3100/7879) done. Loss: 1.3768  lr:0.010000
[ Sun Jul  7 22:34:41 2024 ] 	Batch(3200/7879) done. Loss: 0.9386  lr:0.010000
[ Sun Jul  7 22:35:04 2024 ] 	Batch(3300/7879) done. Loss: 0.8253  lr:0.010000
[ Sun Jul  7 22:35:26 2024 ] 	Batch(3400/7879) done. Loss: 0.1119  lr:0.010000
[ Sun Jul  7 22:35:49 2024 ] 
Training: Epoch [23/120], Step [3499], Loss: 0.7943159937858582, Training Accuracy: 80.51785714285714
[ Sun Jul  7 22:35:49 2024 ] 	Batch(3500/7879) done. Loss: 0.3498  lr:0.010000
[ Sun Jul  7 22:36:12 2024 ] 	Batch(3600/7879) done. Loss: 1.4575  lr:0.010000
[ Sun Jul  7 22:36:35 2024 ] 	Batch(3700/7879) done. Loss: 0.5472  lr:0.010000
[ Sun Jul  7 22:36:58 2024 ] 	Batch(3800/7879) done. Loss: 0.3221  lr:0.010000
[ Sun Jul  7 22:37:20 2024 ] 	Batch(3900/7879) done. Loss: 0.1312  lr:0.010000
[ Sun Jul  7 22:37:43 2024 ] 
Training: Epoch [23/120], Step [3999], Loss: 0.9251002073287964, Training Accuracy: 80.496875
[ Sun Jul  7 22:37:43 2024 ] 	Batch(4000/7879) done. Loss: 0.9643  lr:0.010000
[ Sun Jul  7 22:38:06 2024 ] 	Batch(4100/7879) done. Loss: 0.3463  lr:0.010000
[ Sun Jul  7 22:38:28 2024 ] 	Batch(4200/7879) done. Loss: 0.5930  lr:0.010000
[ Sun Jul  7 22:38:51 2024 ] 	Batch(4300/7879) done. Loss: 1.3688  lr:0.010000
[ Sun Jul  7 22:39:14 2024 ] 	Batch(4400/7879) done. Loss: 0.7675  lr:0.010000
[ Sun Jul  7 22:39:36 2024 ] 
Training: Epoch [23/120], Step [4499], Loss: 0.660424530506134, Training Accuracy: 80.28333333333333
[ Sun Jul  7 22:39:37 2024 ] 	Batch(4500/7879) done. Loss: 0.1634  lr:0.010000
[ Sun Jul  7 22:39:59 2024 ] 	Batch(4600/7879) done. Loss: 0.2064  lr:0.010000
[ Sun Jul  7 22:40:22 2024 ] 	Batch(4700/7879) done. Loss: 0.2512  lr:0.010000
[ Sun Jul  7 22:40:45 2024 ] 	Batch(4800/7879) done. Loss: 0.4557  lr:0.010000
[ Sun Jul  7 22:41:08 2024 ] 	Batch(4900/7879) done. Loss: 0.3575  lr:0.010000
[ Sun Jul  7 22:41:31 2024 ] 
Training: Epoch [23/120], Step [4999], Loss: 1.1507216691970825, Training Accuracy: 80.16
[ Sun Jul  7 22:41:31 2024 ] 	Batch(5000/7879) done. Loss: 0.6190  lr:0.010000
[ Sun Jul  7 22:41:55 2024 ] 	Batch(5100/7879) done. Loss: 0.8542  lr:0.010000
[ Sun Jul  7 22:42:18 2024 ] 	Batch(5200/7879) done. Loss: 0.5850  lr:0.010000
[ Sun Jul  7 22:42:42 2024 ] 	Batch(5300/7879) done. Loss: 1.0641  lr:0.010000
[ Sun Jul  7 22:43:05 2024 ] 	Batch(5400/7879) done. Loss: 1.2138  lr:0.010000
[ Sun Jul  7 22:43:29 2024 ] 
Training: Epoch [23/120], Step [5499], Loss: 1.0982451438903809, Training Accuracy: 80.10000000000001
[ Sun Jul  7 22:43:29 2024 ] 	Batch(5500/7879) done. Loss: 0.8216  lr:0.010000
[ Sun Jul  7 22:43:52 2024 ] 	Batch(5600/7879) done. Loss: 0.6969  lr:0.010000
[ Sun Jul  7 22:44:16 2024 ] 	Batch(5700/7879) done. Loss: 0.1819  lr:0.010000
[ Sun Jul  7 22:44:39 2024 ] 	Batch(5800/7879) done. Loss: 0.2999  lr:0.010000
[ Sun Jul  7 22:45:03 2024 ] 	Batch(5900/7879) done. Loss: 0.5646  lr:0.010000
[ Sun Jul  7 22:45:26 2024 ] 
Training: Epoch [23/120], Step [5999], Loss: 0.21170124411582947, Training Accuracy: 80.12708333333333
[ Sun Jul  7 22:45:26 2024 ] 	Batch(6000/7879) done. Loss: 0.0592  lr:0.010000
[ Sun Jul  7 22:45:50 2024 ] 	Batch(6100/7879) done. Loss: 1.4041  lr:0.010000
[ Sun Jul  7 22:46:13 2024 ] 	Batch(6200/7879) done. Loss: 0.0818  lr:0.010000
[ Sun Jul  7 22:46:37 2024 ] 	Batch(6300/7879) done. Loss: 0.4936  lr:0.010000
[ Sun Jul  7 22:47:00 2024 ] 	Batch(6400/7879) done. Loss: 0.5866  lr:0.010000
[ Sun Jul  7 22:47:24 2024 ] 
Training: Epoch [23/120], Step [6499], Loss: 0.5271317958831787, Training Accuracy: 80.13846153846154
[ Sun Jul  7 22:47:24 2024 ] 	Batch(6500/7879) done. Loss: 0.3553  lr:0.010000
[ Sun Jul  7 22:47:47 2024 ] 	Batch(6600/7879) done. Loss: 1.0216  lr:0.010000
[ Sun Jul  7 22:48:11 2024 ] 	Batch(6700/7879) done. Loss: 0.8500  lr:0.010000
[ Sun Jul  7 22:48:34 2024 ] 	Batch(6800/7879) done. Loss: 0.4762  lr:0.010000
[ Sun Jul  7 22:48:57 2024 ] 	Batch(6900/7879) done. Loss: 1.1149  lr:0.010000
[ Sun Jul  7 22:49:19 2024 ] 
Training: Epoch [23/120], Step [6999], Loss: 0.9854398369789124, Training Accuracy: 80.13928571428572
[ Sun Jul  7 22:49:20 2024 ] 	Batch(7000/7879) done. Loss: 1.5023  lr:0.010000
[ Sun Jul  7 22:49:42 2024 ] 	Batch(7100/7879) done. Loss: 0.4235  lr:0.010000
[ Sun Jul  7 22:50:05 2024 ] 	Batch(7200/7879) done. Loss: 1.1374  lr:0.010000
[ Sun Jul  7 22:50:29 2024 ] 	Batch(7300/7879) done. Loss: 1.0012  lr:0.010000
[ Sun Jul  7 22:50:52 2024 ] 	Batch(7400/7879) done. Loss: 0.4626  lr:0.010000
[ Sun Jul  7 22:51:15 2024 ] 
Training: Epoch [23/120], Step [7499], Loss: 0.5104730725288391, Training Accuracy: 80.15333333333334
[ Sun Jul  7 22:51:15 2024 ] 	Batch(7500/7879) done. Loss: 0.6078  lr:0.010000
[ Sun Jul  7 22:51:38 2024 ] 	Batch(7600/7879) done. Loss: 0.4036  lr:0.010000
[ Sun Jul  7 22:52:01 2024 ] 	Batch(7700/7879) done. Loss: 0.1383  lr:0.010000
[ Sun Jul  7 22:52:24 2024 ] 	Batch(7800/7879) done. Loss: 0.4123  lr:0.010000
[ Sun Jul  7 22:52:41 2024 ] 	Mean training loss: 0.6638.
[ Sun Jul  7 22:52:41 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 22:52:41 2024 ] Training epoch: 25
[ Sun Jul  7 22:52:42 2024 ] 	Batch(0/7879) done. Loss: 1.2338  lr:0.010000
[ Sun Jul  7 22:53:05 2024 ] 	Batch(100/7879) done. Loss: 1.2627  lr:0.010000
[ Sun Jul  7 22:53:28 2024 ] 	Batch(200/7879) done. Loss: 1.6034  lr:0.010000
[ Sun Jul  7 22:53:52 2024 ] 	Batch(300/7879) done. Loss: 1.0035  lr:0.010000
[ Sun Jul  7 22:54:15 2024 ] 	Batch(400/7879) done. Loss: 0.5252  lr:0.010000
[ Sun Jul  7 22:54:39 2024 ] 
Training: Epoch [24/120], Step [499], Loss: 1.343630075454712, Training Accuracy: 79.875
[ Sun Jul  7 22:54:39 2024 ] 	Batch(500/7879) done. Loss: 0.3096  lr:0.010000
[ Sun Jul  7 22:55:02 2024 ] 	Batch(600/7879) done. Loss: 0.8001  lr:0.010000
[ Sun Jul  7 22:55:26 2024 ] 	Batch(700/7879) done. Loss: 0.2160  lr:0.010000
[ Sun Jul  7 22:55:49 2024 ] 	Batch(800/7879) done. Loss: 1.5668  lr:0.010000
[ Sun Jul  7 22:56:13 2024 ] 	Batch(900/7879) done. Loss: 0.5113  lr:0.010000
[ Sun Jul  7 22:56:36 2024 ] 
Training: Epoch [24/120], Step [999], Loss: 0.45885372161865234, Training Accuracy: 80.4
[ Sun Jul  7 22:56:36 2024 ] 	Batch(1000/7879) done. Loss: 0.4893  lr:0.010000
[ Sun Jul  7 22:57:00 2024 ] 	Batch(1100/7879) done. Loss: 0.3208  lr:0.010000
[ Sun Jul  7 22:57:24 2024 ] 	Batch(1200/7879) done. Loss: 0.0416  lr:0.010000
[ Sun Jul  7 22:57:47 2024 ] 	Batch(1300/7879) done. Loss: 0.5756  lr:0.010000
[ Sun Jul  7 22:58:10 2024 ] 	Batch(1400/7879) done. Loss: 0.1961  lr:0.010000
[ Sun Jul  7 22:58:33 2024 ] 
Training: Epoch [24/120], Step [1499], Loss: 0.8273851871490479, Training Accuracy: 80.35833333333333
[ Sun Jul  7 22:58:33 2024 ] 	Batch(1500/7879) done. Loss: 1.2793  lr:0.010000
[ Sun Jul  7 22:58:56 2024 ] 	Batch(1600/7879) done. Loss: 0.6412  lr:0.010000
[ Sun Jul  7 22:59:19 2024 ] 	Batch(1700/7879) done. Loss: 0.7888  lr:0.010000
[ Sun Jul  7 22:59:42 2024 ] 	Batch(1800/7879) done. Loss: 0.6557  lr:0.010000
[ Sun Jul  7 23:00:05 2024 ] 	Batch(1900/7879) done. Loss: 0.4001  lr:0.010000
[ Sun Jul  7 23:00:28 2024 ] 
Training: Epoch [24/120], Step [1999], Loss: 0.2389916032552719, Training Accuracy: 80.28125
[ Sun Jul  7 23:00:29 2024 ] 	Batch(2000/7879) done. Loss: 0.3656  lr:0.010000
[ Sun Jul  7 23:00:52 2024 ] 	Batch(2100/7879) done. Loss: 0.7143  lr:0.010000
[ Sun Jul  7 23:01:15 2024 ] 	Batch(2200/7879) done. Loss: 1.0324  lr:0.010000
[ Sun Jul  7 23:01:39 2024 ] 	Batch(2300/7879) done. Loss: 0.3931  lr:0.010000
[ Sun Jul  7 23:02:02 2024 ] 	Batch(2400/7879) done. Loss: 0.3529  lr:0.010000
[ Sun Jul  7 23:02:25 2024 ] 
Training: Epoch [24/120], Step [2499], Loss: 0.4134212136268616, Training Accuracy: 80.39500000000001
[ Sun Jul  7 23:02:25 2024 ] 	Batch(2500/7879) done. Loss: 1.4099  lr:0.010000
[ Sun Jul  7 23:02:49 2024 ] 	Batch(2600/7879) done. Loss: 0.1839  lr:0.010000
[ Sun Jul  7 23:03:12 2024 ] 	Batch(2700/7879) done. Loss: 0.1422  lr:0.010000
[ Sun Jul  7 23:03:34 2024 ] 	Batch(2800/7879) done. Loss: 0.8967  lr:0.010000
[ Sun Jul  7 23:03:57 2024 ] 	Batch(2900/7879) done. Loss: 0.3637  lr:0.010000
[ Sun Jul  7 23:04:20 2024 ] 
Training: Epoch [24/120], Step [2999], Loss: 0.17010852694511414, Training Accuracy: 80.34166666666667
[ Sun Jul  7 23:04:20 2024 ] 	Batch(3000/7879) done. Loss: 1.1337  lr:0.010000
[ Sun Jul  7 23:04:43 2024 ] 	Batch(3100/7879) done. Loss: 0.2177  lr:0.010000
[ Sun Jul  7 23:05:05 2024 ] 	Batch(3200/7879) done. Loss: 0.2350  lr:0.010000
[ Sun Jul  7 23:05:28 2024 ] 	Batch(3300/7879) done. Loss: 0.4790  lr:0.010000
[ Sun Jul  7 23:05:51 2024 ] 	Batch(3400/7879) done. Loss: 1.2738  lr:0.010000
[ Sun Jul  7 23:06:13 2024 ] 
Training: Epoch [24/120], Step [3499], Loss: 0.034910526126623154, Training Accuracy: 80.36785714285715
[ Sun Jul  7 23:06:14 2024 ] 	Batch(3500/7879) done. Loss: 0.2850  lr:0.010000
[ Sun Jul  7 23:06:36 2024 ] 	Batch(3600/7879) done. Loss: 0.8615  lr:0.010000
[ Sun Jul  7 23:06:59 2024 ] 	Batch(3700/7879) done. Loss: 0.4552  lr:0.010000
[ Sun Jul  7 23:07:22 2024 ] 	Batch(3800/7879) done. Loss: 0.1260  lr:0.010000
[ Sun Jul  7 23:07:44 2024 ] 	Batch(3900/7879) done. Loss: 0.7343  lr:0.010000
[ Sun Jul  7 23:08:07 2024 ] 
Training: Epoch [24/120], Step [3999], Loss: 0.9369617104530334, Training Accuracy: 80.38125000000001
[ Sun Jul  7 23:08:07 2024 ] 	Batch(4000/7879) done. Loss: 0.1872  lr:0.010000
[ Sun Jul  7 23:08:30 2024 ] 	Batch(4100/7879) done. Loss: 0.4071  lr:0.010000
[ Sun Jul  7 23:08:53 2024 ] 	Batch(4200/7879) done. Loss: 0.5214  lr:0.010000
[ Sun Jul  7 23:09:15 2024 ] 	Batch(4300/7879) done. Loss: 0.5100  lr:0.010000
[ Sun Jul  7 23:09:38 2024 ] 	Batch(4400/7879) done. Loss: 0.9994  lr:0.010000
[ Sun Jul  7 23:10:01 2024 ] 
Training: Epoch [24/120], Step [4499], Loss: 0.14371110498905182, Training Accuracy: 80.31944444444444
[ Sun Jul  7 23:10:01 2024 ] 	Batch(4500/7879) done. Loss: 0.6196  lr:0.010000
[ Sun Jul  7 23:10:24 2024 ] 	Batch(4600/7879) done. Loss: 0.3348  lr:0.010000
[ Sun Jul  7 23:10:46 2024 ] 	Batch(4700/7879) done. Loss: 1.2111  lr:0.010000
[ Sun Jul  7 23:11:09 2024 ] 	Batch(4800/7879) done. Loss: 0.6132  lr:0.010000
[ Sun Jul  7 23:11:32 2024 ] 	Batch(4900/7879) done. Loss: 0.5882  lr:0.010000
[ Sun Jul  7 23:11:55 2024 ] 
Training: Epoch [24/120], Step [4999], Loss: 0.5656805634498596, Training Accuracy: 80.35
[ Sun Jul  7 23:11:55 2024 ] 	Batch(5000/7879) done. Loss: 0.9647  lr:0.010000
[ Sun Jul  7 23:12:18 2024 ] 	Batch(5100/7879) done. Loss: 0.8363  lr:0.010000
[ Sun Jul  7 23:12:41 2024 ] 	Batch(5200/7879) done. Loss: 0.0292  lr:0.010000
[ Sun Jul  7 23:13:03 2024 ] 	Batch(5300/7879) done. Loss: 0.2444  lr:0.010000
[ Sun Jul  7 23:13:26 2024 ] 	Batch(5400/7879) done. Loss: 0.5725  lr:0.010000
[ Sun Jul  7 23:13:49 2024 ] 
Training: Epoch [24/120], Step [5499], Loss: 1.2917330265045166, Training Accuracy: 80.39090909090909
[ Sun Jul  7 23:13:49 2024 ] 	Batch(5500/7879) done. Loss: 0.4569  lr:0.010000
[ Sun Jul  7 23:14:12 2024 ] 	Batch(5600/7879) done. Loss: 0.5536  lr:0.010000
[ Sun Jul  7 23:14:34 2024 ] 	Batch(5700/7879) done. Loss: 0.2698  lr:0.010000
[ Sun Jul  7 23:14:57 2024 ] 	Batch(5800/7879) done. Loss: 0.4309  lr:0.010000
[ Sun Jul  7 23:15:20 2024 ] 	Batch(5900/7879) done. Loss: 0.2132  lr:0.010000
[ Sun Jul  7 23:15:42 2024 ] 
Training: Epoch [24/120], Step [5999], Loss: 0.32395508885383606, Training Accuracy: 80.48333333333333
[ Sun Jul  7 23:15:43 2024 ] 	Batch(6000/7879) done. Loss: 0.5773  lr:0.010000
[ Sun Jul  7 23:16:06 2024 ] 	Batch(6100/7879) done. Loss: 0.3892  lr:0.010000
[ Sun Jul  7 23:16:28 2024 ] 	Batch(6200/7879) done. Loss: 0.8731  lr:0.010000
[ Sun Jul  7 23:16:51 2024 ] 	Batch(6300/7879) done. Loss: 0.5993  lr:0.010000
[ Sun Jul  7 23:17:14 2024 ] 	Batch(6400/7879) done. Loss: 0.7033  lr:0.010000
[ Sun Jul  7 23:17:36 2024 ] 
Training: Epoch [24/120], Step [6499], Loss: 0.2954443693161011, Training Accuracy: 80.4
[ Sun Jul  7 23:17:37 2024 ] 	Batch(6500/7879) done. Loss: 0.5894  lr:0.010000
[ Sun Jul  7 23:17:59 2024 ] 	Batch(6600/7879) done. Loss: 0.9958  lr:0.010000
[ Sun Jul  7 23:18:22 2024 ] 	Batch(6700/7879) done. Loss: 0.1606  lr:0.010000
[ Sun Jul  7 23:18:45 2024 ] 	Batch(6800/7879) done. Loss: 1.1623  lr:0.010000
[ Sun Jul  7 23:19:08 2024 ] 	Batch(6900/7879) done. Loss: 0.4322  lr:0.010000
[ Sun Jul  7 23:19:32 2024 ] 
Training: Epoch [24/120], Step [6999], Loss: 1.2455170154571533, Training Accuracy: 80.46428571428571
[ Sun Jul  7 23:19:32 2024 ] 	Batch(7000/7879) done. Loss: 0.5975  lr:0.010000
[ Sun Jul  7 23:19:55 2024 ] 	Batch(7100/7879) done. Loss: 1.0645  lr:0.010000
[ Sun Jul  7 23:20:19 2024 ] 	Batch(7200/7879) done. Loss: 0.8958  lr:0.010000
[ Sun Jul  7 23:20:42 2024 ] 	Batch(7300/7879) done. Loss: 0.2783  lr:0.010000
[ Sun Jul  7 23:21:06 2024 ] 	Batch(7400/7879) done. Loss: 1.9627  lr:0.010000
[ Sun Jul  7 23:21:29 2024 ] 
Training: Epoch [24/120], Step [7499], Loss: 0.8234500885009766, Training Accuracy: 80.45166666666667
[ Sun Jul  7 23:21:29 2024 ] 	Batch(7500/7879) done. Loss: 0.3021  lr:0.010000
[ Sun Jul  7 23:21:52 2024 ] 	Batch(7600/7879) done. Loss: 0.5569  lr:0.010000
[ Sun Jul  7 23:22:15 2024 ] 	Batch(7700/7879) done. Loss: 1.4552  lr:0.010000
[ Sun Jul  7 23:22:38 2024 ] 	Batch(7800/7879) done. Loss: 0.5556  lr:0.010000
[ Sun Jul  7 23:22:55 2024 ] 	Mean training loss: 0.6431.
[ Sun Jul  7 23:22:55 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 23:22:55 2024 ] Training epoch: 26
[ Sun Jul  7 23:22:56 2024 ] 	Batch(0/7879) done. Loss: 0.3219  lr:0.010000
[ Sun Jul  7 23:23:19 2024 ] 	Batch(100/7879) done. Loss: 1.1359  lr:0.010000
[ Sun Jul  7 23:23:41 2024 ] 	Batch(200/7879) done. Loss: 0.3499  lr:0.010000
[ Sun Jul  7 23:24:04 2024 ] 	Batch(300/7879) done. Loss: 0.6153  lr:0.010000
[ Sun Jul  7 23:24:27 2024 ] 	Batch(400/7879) done. Loss: 0.2473  lr:0.010000
[ Sun Jul  7 23:24:49 2024 ] 
Training: Epoch [25/120], Step [499], Loss: 0.9951716065406799, Training Accuracy: 82.05
[ Sun Jul  7 23:24:50 2024 ] 	Batch(500/7879) done. Loss: 0.4180  lr:0.010000
[ Sun Jul  7 23:25:12 2024 ] 	Batch(600/7879) done. Loss: 0.6802  lr:0.010000
[ Sun Jul  7 23:25:36 2024 ] 	Batch(700/7879) done. Loss: 0.2771  lr:0.010000
[ Sun Jul  7 23:25:59 2024 ] 	Batch(800/7879) done. Loss: 1.0280  lr:0.010000
[ Sun Jul  7 23:26:23 2024 ] 	Batch(900/7879) done. Loss: 0.4654  lr:0.010000
[ Sun Jul  7 23:26:45 2024 ] 
Training: Epoch [25/120], Step [999], Loss: 1.8946270942687988, Training Accuracy: 81.525
[ Sun Jul  7 23:26:45 2024 ] 	Batch(1000/7879) done. Loss: 1.4834  lr:0.010000
[ Sun Jul  7 23:27:08 2024 ] 	Batch(1100/7879) done. Loss: 0.4978  lr:0.010000
[ Sun Jul  7 23:27:31 2024 ] 	Batch(1200/7879) done. Loss: 1.9532  lr:0.010000
[ Sun Jul  7 23:27:54 2024 ] 	Batch(1300/7879) done. Loss: 0.3881  lr:0.010000
[ Sun Jul  7 23:28:18 2024 ] 	Batch(1400/7879) done. Loss: 0.7451  lr:0.010000
[ Sun Jul  7 23:28:41 2024 ] 
Training: Epoch [25/120], Step [1499], Loss: 1.1242107152938843, Training Accuracy: 80.89166666666667
[ Sun Jul  7 23:28:41 2024 ] 	Batch(1500/7879) done. Loss: 1.0827  lr:0.010000
[ Sun Jul  7 23:29:04 2024 ] 	Batch(1600/7879) done. Loss: 1.4773  lr:0.010000
[ Sun Jul  7 23:29:27 2024 ] 	Batch(1700/7879) done. Loss: 0.1068  lr:0.010000
[ Sun Jul  7 23:29:50 2024 ] 	Batch(1800/7879) done. Loss: 0.7758  lr:0.010000
[ Sun Jul  7 23:30:12 2024 ] 	Batch(1900/7879) done. Loss: 0.1385  lr:0.010000
[ Sun Jul  7 23:30:35 2024 ] 
Training: Epoch [25/120], Step [1999], Loss: 0.5921751856803894, Training Accuracy: 81.2125
[ Sun Jul  7 23:30:35 2024 ] 	Batch(2000/7879) done. Loss: 0.7222  lr:0.010000
[ Sun Jul  7 23:30:58 2024 ] 	Batch(2100/7879) done. Loss: 0.1966  lr:0.010000
[ Sun Jul  7 23:31:21 2024 ] 	Batch(2200/7879) done. Loss: 0.5189  lr:0.010000
[ Sun Jul  7 23:31:43 2024 ] 	Batch(2300/7879) done. Loss: 0.5691  lr:0.010000
[ Sun Jul  7 23:32:06 2024 ] 	Batch(2400/7879) done. Loss: 0.3716  lr:0.010000
[ Sun Jul  7 23:32:29 2024 ] 
Training: Epoch [25/120], Step [2499], Loss: 1.2633410692214966, Training Accuracy: 81.04
[ Sun Jul  7 23:32:29 2024 ] 	Batch(2500/7879) done. Loss: 0.1953  lr:0.010000
[ Sun Jul  7 23:32:52 2024 ] 	Batch(2600/7879) done. Loss: 0.6652  lr:0.010000
[ Sun Jul  7 23:33:15 2024 ] 	Batch(2700/7879) done. Loss: 0.5853  lr:0.010000
[ Sun Jul  7 23:33:38 2024 ] 	Batch(2800/7879) done. Loss: 0.3266  lr:0.010000
[ Sun Jul  7 23:34:00 2024 ] 	Batch(2900/7879) done. Loss: 0.7332  lr:0.010000
[ Sun Jul  7 23:34:23 2024 ] 
Training: Epoch [25/120], Step [2999], Loss: 0.48869213461875916, Training Accuracy: 80.9875
[ Sun Jul  7 23:34:23 2024 ] 	Batch(3000/7879) done. Loss: 0.3312  lr:0.010000
[ Sun Jul  7 23:34:46 2024 ] 	Batch(3100/7879) done. Loss: 0.2095  lr:0.010000
[ Sun Jul  7 23:35:09 2024 ] 	Batch(3200/7879) done. Loss: 0.4377  lr:0.010000
[ Sun Jul  7 23:35:32 2024 ] 	Batch(3300/7879) done. Loss: 0.8175  lr:0.010000
[ Sun Jul  7 23:35:54 2024 ] 	Batch(3400/7879) done. Loss: 0.3155  lr:0.010000
[ Sun Jul  7 23:36:17 2024 ] 
Training: Epoch [25/120], Step [3499], Loss: 0.3094666600227356, Training Accuracy: 80.88214285714285
[ Sun Jul  7 23:36:17 2024 ] 	Batch(3500/7879) done. Loss: 0.3193  lr:0.010000
[ Sun Jul  7 23:36:40 2024 ] 	Batch(3600/7879) done. Loss: 1.5183  lr:0.010000
[ Sun Jul  7 23:37:03 2024 ] 	Batch(3700/7879) done. Loss: 0.5464  lr:0.010000
[ Sun Jul  7 23:37:27 2024 ] 	Batch(3800/7879) done. Loss: 0.1338  lr:0.010000
[ Sun Jul  7 23:37:50 2024 ] 	Batch(3900/7879) done. Loss: 0.9154  lr:0.010000
[ Sun Jul  7 23:38:13 2024 ] 
Training: Epoch [25/120], Step [3999], Loss: 0.7691830396652222, Training Accuracy: 80.8625
[ Sun Jul  7 23:38:14 2024 ] 	Batch(4000/7879) done. Loss: 0.8300  lr:0.010000
[ Sun Jul  7 23:38:37 2024 ] 	Batch(4100/7879) done. Loss: 0.7907  lr:0.010000
[ Sun Jul  7 23:39:01 2024 ] 	Batch(4200/7879) done. Loss: 0.7437  lr:0.010000
[ Sun Jul  7 23:39:24 2024 ] 	Batch(4300/7879) done. Loss: 0.4822  lr:0.010000
[ Sun Jul  7 23:39:48 2024 ] 	Batch(4400/7879) done. Loss: 0.1535  lr:0.010000
[ Sun Jul  7 23:40:11 2024 ] 
Training: Epoch [25/120], Step [4499], Loss: 0.4279071092605591, Training Accuracy: 80.91666666666667
[ Sun Jul  7 23:40:11 2024 ] 	Batch(4500/7879) done. Loss: 0.1241  lr:0.010000
[ Sun Jul  7 23:40:35 2024 ] 	Batch(4600/7879) done. Loss: 0.0360  lr:0.010000
[ Sun Jul  7 23:40:58 2024 ] 	Batch(4700/7879) done. Loss: 0.2190  lr:0.010000
[ Sun Jul  7 23:41:22 2024 ] 	Batch(4800/7879) done. Loss: 0.7692  lr:0.010000
[ Sun Jul  7 23:41:45 2024 ] 	Batch(4900/7879) done. Loss: 0.1363  lr:0.010000
[ Sun Jul  7 23:42:08 2024 ] 
Training: Epoch [25/120], Step [4999], Loss: 0.6097103357315063, Training Accuracy: 80.985
[ Sun Jul  7 23:42:08 2024 ] 	Batch(5000/7879) done. Loss: 0.2675  lr:0.010000
[ Sun Jul  7 23:42:30 2024 ] 	Batch(5100/7879) done. Loss: 0.3664  lr:0.010000
[ Sun Jul  7 23:42:53 2024 ] 	Batch(5200/7879) done. Loss: 0.6036  lr:0.010000
[ Sun Jul  7 23:43:16 2024 ] 	Batch(5300/7879) done. Loss: 0.2841  lr:0.010000
[ Sun Jul  7 23:43:39 2024 ] 	Batch(5400/7879) done. Loss: 0.8993  lr:0.010000
[ Sun Jul  7 23:44:01 2024 ] 
Training: Epoch [25/120], Step [5499], Loss: 0.5874061584472656, Training Accuracy: 80.89545454545456
[ Sun Jul  7 23:44:01 2024 ] 	Batch(5500/7879) done. Loss: 0.7860  lr:0.010000
[ Sun Jul  7 23:44:24 2024 ] 	Batch(5600/7879) done. Loss: 0.2928  lr:0.010000
[ Sun Jul  7 23:44:48 2024 ] 	Batch(5700/7879) done. Loss: 0.9818  lr:0.010000
[ Sun Jul  7 23:45:11 2024 ] 	Batch(5800/7879) done. Loss: 0.5947  lr:0.010000
[ Sun Jul  7 23:45:34 2024 ] 	Batch(5900/7879) done. Loss: 0.6105  lr:0.010000
[ Sun Jul  7 23:45:57 2024 ] 
Training: Epoch [25/120], Step [5999], Loss: 0.31557220220565796, Training Accuracy: 80.85
[ Sun Jul  7 23:45:57 2024 ] 	Batch(6000/7879) done. Loss: 0.2839  lr:0.010000
[ Sun Jul  7 23:46:20 2024 ] 	Batch(6100/7879) done. Loss: 0.4963  lr:0.010000
[ Sun Jul  7 23:46:42 2024 ] 	Batch(6200/7879) done. Loss: 0.5926  lr:0.010000
[ Sun Jul  7 23:47:05 2024 ] 	Batch(6300/7879) done. Loss: 1.7858  lr:0.010000
[ Sun Jul  7 23:47:28 2024 ] 	Batch(6400/7879) done. Loss: 1.3016  lr:0.010000
[ Sun Jul  7 23:47:51 2024 ] 
Training: Epoch [25/120], Step [6499], Loss: 0.6109718084335327, Training Accuracy: 80.82692307692308
[ Sun Jul  7 23:47:51 2024 ] 	Batch(6500/7879) done. Loss: 0.0748  lr:0.010000
[ Sun Jul  7 23:48:13 2024 ] 	Batch(6600/7879) done. Loss: 0.9923  lr:0.010000
[ Sun Jul  7 23:48:36 2024 ] 	Batch(6700/7879) done. Loss: 0.4586  lr:0.010000
[ Sun Jul  7 23:48:59 2024 ] 	Batch(6800/7879) done. Loss: 0.6129  lr:0.010000
[ Sun Jul  7 23:49:22 2024 ] 	Batch(6900/7879) done. Loss: 0.2183  lr:0.010000
[ Sun Jul  7 23:49:46 2024 ] 
Training: Epoch [25/120], Step [6999], Loss: 0.6001380681991577, Training Accuracy: 80.86607142857143
[ Sun Jul  7 23:49:46 2024 ] 	Batch(7000/7879) done. Loss: 0.5409  lr:0.010000
[ Sun Jul  7 23:50:09 2024 ] 	Batch(7100/7879) done. Loss: 0.6008  lr:0.010000
[ Sun Jul  7 23:50:32 2024 ] 	Batch(7200/7879) done. Loss: 0.4092  lr:0.010000
[ Sun Jul  7 23:50:55 2024 ] 	Batch(7300/7879) done. Loss: 0.6421  lr:0.010000
[ Sun Jul  7 23:51:18 2024 ] 	Batch(7400/7879) done. Loss: 0.0983  lr:0.010000
[ Sun Jul  7 23:51:40 2024 ] 
Training: Epoch [25/120], Step [7499], Loss: 0.1505451202392578, Training Accuracy: 80.85666666666667
[ Sun Jul  7 23:51:40 2024 ] 	Batch(7500/7879) done. Loss: 0.6230  lr:0.010000
[ Sun Jul  7 23:52:03 2024 ] 	Batch(7600/7879) done. Loss: 0.1351  lr:0.010000
[ Sun Jul  7 23:52:26 2024 ] 	Batch(7700/7879) done. Loss: 0.5777  lr:0.010000
[ Sun Jul  7 23:52:48 2024 ] 	Batch(7800/7879) done. Loss: 0.4012  lr:0.010000
[ Sun Jul  7 23:53:06 2024 ] 	Mean training loss: 0.6361.
[ Sun Jul  7 23:53:06 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul  7 23:53:06 2024 ] Training epoch: 27
[ Sun Jul  7 23:53:07 2024 ] 	Batch(0/7879) done. Loss: 0.4808  lr:0.010000
[ Sun Jul  7 23:53:30 2024 ] 	Batch(100/7879) done. Loss: 0.8425  lr:0.010000
[ Sun Jul  7 23:53:52 2024 ] 	Batch(200/7879) done. Loss: 0.1658  lr:0.010000
[ Sun Jul  7 23:54:15 2024 ] 	Batch(300/7879) done. Loss: 0.4900  lr:0.010000
[ Sun Jul  7 23:54:38 2024 ] 	Batch(400/7879) done. Loss: 0.2457  lr:0.010000
[ Sun Jul  7 23:55:01 2024 ] 
Training: Epoch [26/120], Step [499], Loss: 0.07811156660318375, Training Accuracy: 82.375
[ Sun Jul  7 23:55:01 2024 ] 	Batch(500/7879) done. Loss: 0.1235  lr:0.010000
[ Sun Jul  7 23:55:24 2024 ] 	Batch(600/7879) done. Loss: 0.2020  lr:0.010000
[ Sun Jul  7 23:55:46 2024 ] 	Batch(700/7879) done. Loss: 0.2955  lr:0.010000
[ Sun Jul  7 23:56:09 2024 ] 	Batch(800/7879) done. Loss: 0.4553  lr:0.010000
[ Sun Jul  7 23:56:33 2024 ] 	Batch(900/7879) done. Loss: 0.1239  lr:0.010000
[ Sun Jul  7 23:56:56 2024 ] 
Training: Epoch [26/120], Step [999], Loss: 0.8080785870552063, Training Accuracy: 82.7125
[ Sun Jul  7 23:56:56 2024 ] 	Batch(1000/7879) done. Loss: 0.2246  lr:0.010000
[ Sun Jul  7 23:57:18 2024 ] 	Batch(1100/7879) done. Loss: 0.1472  lr:0.010000
[ Sun Jul  7 23:57:41 2024 ] 	Batch(1200/7879) done. Loss: 0.5685  lr:0.010000
[ Sun Jul  7 23:58:04 2024 ] 	Batch(1300/7879) done. Loss: 0.9036  lr:0.010000
[ Sun Jul  7 23:58:27 2024 ] 	Batch(1400/7879) done. Loss: 0.3055  lr:0.010000
[ Sun Jul  7 23:58:49 2024 ] 
Training: Epoch [26/120], Step [1499], Loss: 0.013722475618124008, Training Accuracy: 82.24166666666667
[ Sun Jul  7 23:58:49 2024 ] 	Batch(1500/7879) done. Loss: 0.7286  lr:0.010000
[ Sun Jul  7 23:59:12 2024 ] 	Batch(1600/7879) done. Loss: 0.1499  lr:0.010000
[ Sun Jul  7 23:59:35 2024 ] 	Batch(1700/7879) done. Loss: 0.2791  lr:0.010000
[ Sun Jul  7 23:59:58 2024 ] 	Batch(1800/7879) done. Loss: 1.0719  lr:0.010000
[ Mon Jul  8 00:00:20 2024 ] 	Batch(1900/7879) done. Loss: 0.6153  lr:0.010000
[ Mon Jul  8 00:00:43 2024 ] 
Training: Epoch [26/120], Step [1999], Loss: 0.4798504710197449, Training Accuracy: 82.1875
[ Mon Jul  8 00:00:43 2024 ] 	Batch(2000/7879) done. Loss: 1.0317  lr:0.010000
[ Mon Jul  8 00:01:06 2024 ] 	Batch(2100/7879) done. Loss: 0.3054  lr:0.010000
[ Mon Jul  8 00:01:28 2024 ] 	Batch(2200/7879) done. Loss: 0.8187  lr:0.010000
[ Mon Jul  8 00:01:51 2024 ] 	Batch(2300/7879) done. Loss: 0.2009  lr:0.010000
[ Mon Jul  8 00:02:14 2024 ] 	Batch(2400/7879) done. Loss: 1.2859  lr:0.010000
[ Mon Jul  8 00:02:37 2024 ] 
Training: Epoch [26/120], Step [2499], Loss: 0.7479004859924316, Training Accuracy: 82.045
[ Mon Jul  8 00:02:37 2024 ] 	Batch(2500/7879) done. Loss: 1.4558  lr:0.010000
[ Mon Jul  8 00:03:01 2024 ] 	Batch(2600/7879) done. Loss: 0.0787  lr:0.010000
[ Mon Jul  8 00:03:24 2024 ] 	Batch(2700/7879) done. Loss: 1.3667  lr:0.010000
[ Mon Jul  8 00:03:48 2024 ] 	Batch(2800/7879) done. Loss: 0.9746  lr:0.010000
[ Mon Jul  8 00:04:11 2024 ] 	Batch(2900/7879) done. Loss: 1.0787  lr:0.010000
[ Mon Jul  8 00:04:33 2024 ] 
Training: Epoch [26/120], Step [2999], Loss: 0.12010841071605682, Training Accuracy: 82.04583333333333
[ Mon Jul  8 00:04:34 2024 ] 	Batch(3000/7879) done. Loss: 0.9653  lr:0.010000
[ Mon Jul  8 00:04:57 2024 ] 	Batch(3100/7879) done. Loss: 0.3866  lr:0.010000
[ Mon Jul  8 00:05:20 2024 ] 	Batch(3200/7879) done. Loss: 0.1641  lr:0.010000
[ Mon Jul  8 00:05:43 2024 ] 	Batch(3300/7879) done. Loss: 1.2426  lr:0.010000
[ Mon Jul  8 00:06:06 2024 ] 	Batch(3400/7879) done. Loss: 1.1232  lr:0.010000
[ Mon Jul  8 00:06:29 2024 ] 
Training: Epoch [26/120], Step [3499], Loss: 0.2295144647359848, Training Accuracy: 82.0
[ Mon Jul  8 00:06:29 2024 ] 	Batch(3500/7879) done. Loss: 0.1042  lr:0.010000
[ Mon Jul  8 00:06:52 2024 ] 	Batch(3600/7879) done. Loss: 1.0544  lr:0.010000
[ Mon Jul  8 00:07:15 2024 ] 	Batch(3700/7879) done. Loss: 0.4231  lr:0.010000
[ Mon Jul  8 00:07:38 2024 ] 	Batch(3800/7879) done. Loss: 0.6163  lr:0.010000
[ Mon Jul  8 00:08:02 2024 ] 	Batch(3900/7879) done. Loss: 0.2458  lr:0.010000
[ Mon Jul  8 00:08:25 2024 ] 
Training: Epoch [26/120], Step [3999], Loss: 0.26594120264053345, Training Accuracy: 82.04375
[ Mon Jul  8 00:08:26 2024 ] 	Batch(4000/7879) done. Loss: 1.0241  lr:0.010000
[ Mon Jul  8 00:08:49 2024 ] 	Batch(4100/7879) done. Loss: 0.7324  lr:0.010000
[ Mon Jul  8 00:09:12 2024 ] 	Batch(4200/7879) done. Loss: 0.7766  lr:0.010000
[ Mon Jul  8 00:09:36 2024 ] 	Batch(4300/7879) done. Loss: 0.5672  lr:0.010000
[ Mon Jul  8 00:09:59 2024 ] 	Batch(4400/7879) done. Loss: 0.4316  lr:0.010000
[ Mon Jul  8 00:10:23 2024 ] 
Training: Epoch [26/120], Step [4499], Loss: 0.6783432960510254, Training Accuracy: 81.85277777777777
[ Mon Jul  8 00:10:23 2024 ] 	Batch(4500/7879) done. Loss: 0.1991  lr:0.010000
[ Mon Jul  8 00:10:46 2024 ] 	Batch(4600/7879) done. Loss: 0.3295  lr:0.010000
[ Mon Jul  8 00:11:09 2024 ] 	Batch(4700/7879) done. Loss: 1.0679  lr:0.010000
[ Mon Jul  8 00:11:32 2024 ] 	Batch(4800/7879) done. Loss: 0.1270  lr:0.010000
[ Mon Jul  8 00:11:55 2024 ] 	Batch(4900/7879) done. Loss: 0.3979  lr:0.010000
[ Mon Jul  8 00:12:17 2024 ] 
Training: Epoch [26/120], Step [4999], Loss: 0.6662799119949341, Training Accuracy: 81.7125
[ Mon Jul  8 00:12:17 2024 ] 	Batch(5000/7879) done. Loss: 1.6113  lr:0.010000
[ Mon Jul  8 00:12:40 2024 ] 	Batch(5100/7879) done. Loss: 0.3916  lr:0.010000
[ Mon Jul  8 00:13:03 2024 ] 	Batch(5200/7879) done. Loss: 1.2110  lr:0.010000
[ Mon Jul  8 00:13:26 2024 ] 	Batch(5300/7879) done. Loss: 0.4188  lr:0.010000
[ Mon Jul  8 00:13:50 2024 ] 	Batch(5400/7879) done. Loss: 0.6094  lr:0.010000
[ Mon Jul  8 00:14:13 2024 ] 
Training: Epoch [26/120], Step [5499], Loss: 0.38214486837387085, Training Accuracy: 81.61818181818182
[ Mon Jul  8 00:14:13 2024 ] 	Batch(5500/7879) done. Loss: 0.1817  lr:0.010000
[ Mon Jul  8 00:14:37 2024 ] 	Batch(5600/7879) done. Loss: 0.4398  lr:0.010000
[ Mon Jul  8 00:15:00 2024 ] 	Batch(5700/7879) done. Loss: 0.3904  lr:0.010000
[ Mon Jul  8 00:15:24 2024 ] 	Batch(5800/7879) done. Loss: 0.3596  lr:0.010000
[ Mon Jul  8 00:15:47 2024 ] 	Batch(5900/7879) done. Loss: 0.5670  lr:0.010000
[ Mon Jul  8 00:16:09 2024 ] 
Training: Epoch [26/120], Step [5999], Loss: 0.1772954761981964, Training Accuracy: 81.57916666666667
[ Mon Jul  8 00:16:10 2024 ] 	Batch(6000/7879) done. Loss: 0.1317  lr:0.010000
[ Mon Jul  8 00:16:32 2024 ] 	Batch(6100/7879) done. Loss: 1.1200  lr:0.010000
[ Mon Jul  8 00:16:55 2024 ] 	Batch(6200/7879) done. Loss: 0.5470  lr:0.010000
[ Mon Jul  8 00:17:18 2024 ] 	Batch(6300/7879) done. Loss: 0.1822  lr:0.010000
[ Mon Jul  8 00:17:41 2024 ] 	Batch(6400/7879) done. Loss: 0.3366  lr:0.010000
[ Mon Jul  8 00:18:03 2024 ] 
Training: Epoch [26/120], Step [6499], Loss: 0.986051619052887, Training Accuracy: 81.61346153846154
[ Mon Jul  8 00:18:03 2024 ] 	Batch(6500/7879) done. Loss: 0.7721  lr:0.010000
[ Mon Jul  8 00:18:26 2024 ] 	Batch(6600/7879) done. Loss: 0.0365  lr:0.010000
[ Mon Jul  8 00:18:49 2024 ] 	Batch(6700/7879) done. Loss: 1.3753  lr:0.010000
[ Mon Jul  8 00:19:12 2024 ] 	Batch(6800/7879) done. Loss: 0.3730  lr:0.010000
[ Mon Jul  8 00:19:34 2024 ] 	Batch(6900/7879) done. Loss: 0.2908  lr:0.010000
[ Mon Jul  8 00:19:57 2024 ] 
Training: Epoch [26/120], Step [6999], Loss: 1.0567021369934082, Training Accuracy: 81.525
[ Mon Jul  8 00:19:57 2024 ] 	Batch(7000/7879) done. Loss: 0.4319  lr:0.010000
[ Mon Jul  8 00:20:20 2024 ] 	Batch(7100/7879) done. Loss: 1.3117  lr:0.010000
[ Mon Jul  8 00:20:43 2024 ] 	Batch(7200/7879) done. Loss: 0.2329  lr:0.010000
[ Mon Jul  8 00:21:05 2024 ] 	Batch(7300/7879) done. Loss: 1.0292  lr:0.010000
[ Mon Jul  8 00:21:28 2024 ] 	Batch(7400/7879) done. Loss: 0.1784  lr:0.010000
[ Mon Jul  8 00:21:51 2024 ] 
Training: Epoch [26/120], Step [7499], Loss: 0.6403559446334839, Training Accuracy: 81.44500000000001
[ Mon Jul  8 00:21:51 2024 ] 	Batch(7500/7879) done. Loss: 0.3065  lr:0.010000
[ Mon Jul  8 00:22:14 2024 ] 	Batch(7600/7879) done. Loss: 0.5041  lr:0.010000
[ Mon Jul  8 00:22:36 2024 ] 	Batch(7700/7879) done. Loss: 0.3021  lr:0.010000
[ Mon Jul  8 00:22:59 2024 ] 	Batch(7800/7879) done. Loss: 0.4118  lr:0.010000
[ Mon Jul  8 00:23:17 2024 ] 	Mean training loss: 0.6181.
[ Mon Jul  8 00:23:17 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 00:23:17 2024 ] Training epoch: 28
[ Mon Jul  8 00:23:17 2024 ] 	Batch(0/7879) done. Loss: 1.6683  lr:0.010000
[ Mon Jul  8 00:23:40 2024 ] 	Batch(100/7879) done. Loss: 0.1991  lr:0.010000
[ Mon Jul  8 00:24:03 2024 ] 	Batch(200/7879) done. Loss: 0.5784  lr:0.010000
[ Mon Jul  8 00:24:25 2024 ] 	Batch(300/7879) done. Loss: 0.6365  lr:0.010000
[ Mon Jul  8 00:24:48 2024 ] 	Batch(400/7879) done. Loss: 0.4957  lr:0.010000
[ Mon Jul  8 00:25:11 2024 ] 
Training: Epoch [27/120], Step [499], Loss: 1.5569936037063599, Training Accuracy: 82.325
[ Mon Jul  8 00:25:11 2024 ] 	Batch(500/7879) done. Loss: 0.6050  lr:0.010000
[ Mon Jul  8 00:25:34 2024 ] 	Batch(600/7879) done. Loss: 0.1433  lr:0.010000
[ Mon Jul  8 00:25:57 2024 ] 	Batch(700/7879) done. Loss: 0.4930  lr:0.010000
[ Mon Jul  8 00:26:21 2024 ] 	Batch(800/7879) done. Loss: 0.2512  lr:0.010000
[ Mon Jul  8 00:26:44 2024 ] 	Batch(900/7879) done. Loss: 1.9875  lr:0.010000
[ Mon Jul  8 00:27:07 2024 ] 
Training: Epoch [27/120], Step [999], Loss: 0.08852051198482513, Training Accuracy: 82.8625
[ Mon Jul  8 00:27:08 2024 ] 	Batch(1000/7879) done. Loss: 0.5332  lr:0.010000
[ Mon Jul  8 00:27:31 2024 ] 	Batch(1100/7879) done. Loss: 1.0980  lr:0.010000
[ Mon Jul  8 00:27:55 2024 ] 	Batch(1200/7879) done. Loss: 0.5558  lr:0.010000
[ Mon Jul  8 00:28:17 2024 ] 	Batch(1300/7879) done. Loss: 0.3474  lr:0.010000
[ Mon Jul  8 00:28:40 2024 ] 	Batch(1400/7879) done. Loss: 0.9185  lr:0.010000
[ Mon Jul  8 00:29:03 2024 ] 
Training: Epoch [27/120], Step [1499], Loss: 0.6020461916923523, Training Accuracy: 82.55833333333334
[ Mon Jul  8 00:29:03 2024 ] 	Batch(1500/7879) done. Loss: 0.6504  lr:0.010000
[ Mon Jul  8 00:29:26 2024 ] 	Batch(1600/7879) done. Loss: 1.3131  lr:0.010000
[ Mon Jul  8 00:29:48 2024 ] 	Batch(1700/7879) done. Loss: 0.9018  lr:0.010000
[ Mon Jul  8 00:30:11 2024 ] 	Batch(1800/7879) done. Loss: 0.3440  lr:0.010000
[ Mon Jul  8 00:30:34 2024 ] 	Batch(1900/7879) done. Loss: 0.3746  lr:0.010000
[ Mon Jul  8 00:30:57 2024 ] 
Training: Epoch [27/120], Step [1999], Loss: 0.3498222529888153, Training Accuracy: 82.69375
[ Mon Jul  8 00:30:57 2024 ] 	Batch(2000/7879) done. Loss: 0.5974  lr:0.010000
[ Mon Jul  8 00:31:20 2024 ] 	Batch(2100/7879) done. Loss: 0.4739  lr:0.010000
[ Mon Jul  8 00:31:43 2024 ] 	Batch(2200/7879) done. Loss: 0.4286  lr:0.010000
[ Mon Jul  8 00:32:05 2024 ] 	Batch(2300/7879) done. Loss: 0.6899  lr:0.010000
[ Mon Jul  8 00:32:28 2024 ] 	Batch(2400/7879) done. Loss: 0.0889  lr:0.010000
[ Mon Jul  8 00:32:51 2024 ] 
Training: Epoch [27/120], Step [2499], Loss: 1.0285483598709106, Training Accuracy: 82.54
[ Mon Jul  8 00:32:51 2024 ] 	Batch(2500/7879) done. Loss: 0.1982  lr:0.010000
[ Mon Jul  8 00:33:14 2024 ] 	Batch(2600/7879) done. Loss: 0.6653  lr:0.010000
[ Mon Jul  8 00:33:36 2024 ] 	Batch(2700/7879) done. Loss: 0.3175  lr:0.010000
[ Mon Jul  8 00:33:59 2024 ] 	Batch(2800/7879) done. Loss: 0.9658  lr:0.010000
[ Mon Jul  8 00:34:22 2024 ] 	Batch(2900/7879) done. Loss: 0.6290  lr:0.010000
[ Mon Jul  8 00:34:45 2024 ] 
Training: Epoch [27/120], Step [2999], Loss: 0.8262580633163452, Training Accuracy: 82.65
[ Mon Jul  8 00:34:45 2024 ] 	Batch(3000/7879) done. Loss: 1.1548  lr:0.010000
[ Mon Jul  8 00:35:08 2024 ] 	Batch(3100/7879) done. Loss: 0.4893  lr:0.010000
[ Mon Jul  8 00:35:30 2024 ] 	Batch(3200/7879) done. Loss: 0.8785  lr:0.010000
[ Mon Jul  8 00:35:54 2024 ] 	Batch(3300/7879) done. Loss: 1.1246  lr:0.010000
[ Mon Jul  8 00:36:17 2024 ] 	Batch(3400/7879) done. Loss: 0.8243  lr:0.010000
[ Mon Jul  8 00:36:41 2024 ] 
Training: Epoch [27/120], Step [3499], Loss: 1.198143720626831, Training Accuracy: 82.39285714285714
[ Mon Jul  8 00:36:41 2024 ] 	Batch(3500/7879) done. Loss: 0.9899  lr:0.010000
[ Mon Jul  8 00:37:04 2024 ] 	Batch(3600/7879) done. Loss: 0.6160  lr:0.010000
[ Mon Jul  8 00:37:27 2024 ] 	Batch(3700/7879) done. Loss: 0.5074  lr:0.010000
[ Mon Jul  8 00:37:50 2024 ] 	Batch(3800/7879) done. Loss: 0.7547  lr:0.010000
[ Mon Jul  8 00:38:13 2024 ] 	Batch(3900/7879) done. Loss: 0.3694  lr:0.010000
[ Mon Jul  8 00:38:35 2024 ] 
Training: Epoch [27/120], Step [3999], Loss: 1.0608415603637695, Training Accuracy: 82.23125
[ Mon Jul  8 00:38:35 2024 ] 	Batch(4000/7879) done. Loss: 0.5569  lr:0.010000
[ Mon Jul  8 00:38:58 2024 ] 	Batch(4100/7879) done. Loss: 1.3684  lr:0.010000
[ Mon Jul  8 00:39:21 2024 ] 	Batch(4200/7879) done. Loss: 0.1794  lr:0.010000
[ Mon Jul  8 00:39:44 2024 ] 	Batch(4300/7879) done. Loss: 0.3104  lr:0.010000
[ Mon Jul  8 00:40:06 2024 ] 	Batch(4400/7879) done. Loss: 0.8624  lr:0.010000
[ Mon Jul  8 00:40:29 2024 ] 
Training: Epoch [27/120], Step [4499], Loss: 0.09306134283542633, Training Accuracy: 82.19722222222222
[ Mon Jul  8 00:40:29 2024 ] 	Batch(4500/7879) done. Loss: 0.8307  lr:0.010000
[ Mon Jul  8 00:40:52 2024 ] 	Batch(4600/7879) done. Loss: 0.7420  lr:0.010000
[ Mon Jul  8 00:41:14 2024 ] 	Batch(4700/7879) done. Loss: 0.3719  lr:0.010000
[ Mon Jul  8 00:41:37 2024 ] 	Batch(4800/7879) done. Loss: 2.4197  lr:0.010000
[ Mon Jul  8 00:42:00 2024 ] 	Batch(4900/7879) done. Loss: 0.4721  lr:0.010000
[ Mon Jul  8 00:42:23 2024 ] 
Training: Epoch [27/120], Step [4999], Loss: 0.708487868309021, Training Accuracy: 82.07
[ Mon Jul  8 00:42:23 2024 ] 	Batch(5000/7879) done. Loss: 0.8988  lr:0.010000
[ Mon Jul  8 00:42:45 2024 ] 	Batch(5100/7879) done. Loss: 0.6459  lr:0.010000
[ Mon Jul  8 00:43:08 2024 ] 	Batch(5200/7879) done. Loss: 0.1887  lr:0.010000
[ Mon Jul  8 00:43:31 2024 ] 	Batch(5300/7879) done. Loss: 0.4702  lr:0.010000
[ Mon Jul  8 00:43:54 2024 ] 	Batch(5400/7879) done. Loss: 0.2605  lr:0.010000
[ Mon Jul  8 00:44:16 2024 ] 
Training: Epoch [27/120], Step [5499], Loss: 1.456860899925232, Training Accuracy: 81.94545454545454
[ Mon Jul  8 00:44:16 2024 ] 	Batch(5500/7879) done. Loss: 0.7877  lr:0.010000
[ Mon Jul  8 00:44:39 2024 ] 	Batch(5600/7879) done. Loss: 0.6736  lr:0.010000
[ Mon Jul  8 00:45:02 2024 ] 	Batch(5700/7879) done. Loss: 0.4522  lr:0.010000
[ Mon Jul  8 00:45:25 2024 ] 	Batch(5800/7879) done. Loss: 1.0522  lr:0.010000
[ Mon Jul  8 00:45:47 2024 ] 	Batch(5900/7879) done. Loss: 0.4062  lr:0.010000
[ Mon Jul  8 00:46:10 2024 ] 
Training: Epoch [27/120], Step [5999], Loss: 0.3606635332107544, Training Accuracy: 81.90208333333334
[ Mon Jul  8 00:46:10 2024 ] 	Batch(6000/7879) done. Loss: 0.7329  lr:0.010000
[ Mon Jul  8 00:46:34 2024 ] 	Batch(6100/7879) done. Loss: 1.0002  lr:0.010000
[ Mon Jul  8 00:46:57 2024 ] 	Batch(6200/7879) done. Loss: 0.1899  lr:0.010000
[ Mon Jul  8 00:47:21 2024 ] 	Batch(6300/7879) done. Loss: 0.1460  lr:0.010000
[ Mon Jul  8 00:47:44 2024 ] 	Batch(6400/7879) done. Loss: 0.3173  lr:0.010000
[ Mon Jul  8 00:48:06 2024 ] 
Training: Epoch [27/120], Step [6499], Loss: 0.3832886219024658, Training Accuracy: 81.80576923076923
[ Mon Jul  8 00:48:07 2024 ] 	Batch(6500/7879) done. Loss: 0.1457  lr:0.010000
[ Mon Jul  8 00:48:29 2024 ] 	Batch(6600/7879) done. Loss: 1.8223  lr:0.010000
[ Mon Jul  8 00:48:52 2024 ] 	Batch(6700/7879) done. Loss: 0.4219  lr:0.010000
[ Mon Jul  8 00:49:15 2024 ] 	Batch(6800/7879) done. Loss: 0.5267  lr:0.010000
[ Mon Jul  8 00:49:38 2024 ] 	Batch(6900/7879) done. Loss: 1.1578  lr:0.010000
[ Mon Jul  8 00:50:00 2024 ] 
Training: Epoch [27/120], Step [6999], Loss: 0.4037695825099945, Training Accuracy: 81.69642857142857
[ Mon Jul  8 00:50:00 2024 ] 	Batch(7000/7879) done. Loss: 0.4228  lr:0.010000
[ Mon Jul  8 00:50:23 2024 ] 	Batch(7100/7879) done. Loss: 2.2202  lr:0.010000
[ Mon Jul  8 00:50:46 2024 ] 	Batch(7200/7879) done. Loss: 0.8445  lr:0.010000
[ Mon Jul  8 00:51:09 2024 ] 	Batch(7300/7879) done. Loss: 0.2381  lr:0.010000
[ Mon Jul  8 00:51:31 2024 ] 	Batch(7400/7879) done. Loss: 0.5751  lr:0.010000
[ Mon Jul  8 00:51:54 2024 ] 
Training: Epoch [27/120], Step [7499], Loss: 1.186211347579956, Training Accuracy: 81.64833333333334
[ Mon Jul  8 00:51:54 2024 ] 	Batch(7500/7879) done. Loss: 0.8448  lr:0.010000
[ Mon Jul  8 00:52:17 2024 ] 	Batch(7600/7879) done. Loss: 0.2748  lr:0.010000
[ Mon Jul  8 00:52:40 2024 ] 	Batch(7700/7879) done. Loss: 1.0595  lr:0.010000
[ Mon Jul  8 00:53:02 2024 ] 	Batch(7800/7879) done. Loss: 0.2953  lr:0.010000
[ Mon Jul  8 00:53:20 2024 ] 	Mean training loss: 0.6119.
[ Mon Jul  8 00:53:20 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 00:53:20 2024 ] Training epoch: 29
[ Mon Jul  8 00:53:21 2024 ] 	Batch(0/7879) done. Loss: 0.2451  lr:0.010000
[ Mon Jul  8 00:53:44 2024 ] 	Batch(100/7879) done. Loss: 0.6879  lr:0.010000
[ Mon Jul  8 00:54:08 2024 ] 	Batch(200/7879) done. Loss: 0.2917  lr:0.010000
[ Mon Jul  8 00:54:31 2024 ] 	Batch(300/7879) done. Loss: 1.2234  lr:0.010000
[ Mon Jul  8 00:54:55 2024 ] 	Batch(400/7879) done. Loss: 1.3477  lr:0.010000
[ Mon Jul  8 00:55:18 2024 ] 
Training: Epoch [28/120], Step [499], Loss: 0.3156282603740692, Training Accuracy: 83.3
[ Mon Jul  8 00:55:18 2024 ] 	Batch(500/7879) done. Loss: 0.7484  lr:0.010000
[ Mon Jul  8 00:55:42 2024 ] 	Batch(600/7879) done. Loss: 0.3923  lr:0.010000
[ Mon Jul  8 00:56:05 2024 ] 	Batch(700/7879) done. Loss: 0.2546  lr:0.010000
[ Mon Jul  8 00:56:28 2024 ] 	Batch(800/7879) done. Loss: 0.3557  lr:0.010000
[ Mon Jul  8 00:56:51 2024 ] 	Batch(900/7879) done. Loss: 0.3815  lr:0.010000
[ Mon Jul  8 00:57:13 2024 ] 
Training: Epoch [28/120], Step [999], Loss: 1.1140096187591553, Training Accuracy: 83.125
[ Mon Jul  8 00:57:13 2024 ] 	Batch(1000/7879) done. Loss: 0.4046  lr:0.010000
[ Mon Jul  8 00:57:36 2024 ] 	Batch(1100/7879) done. Loss: 1.2322  lr:0.010000
[ Mon Jul  8 00:57:59 2024 ] 	Batch(1200/7879) done. Loss: 0.4308  lr:0.010000
[ Mon Jul  8 00:58:22 2024 ] 	Batch(1300/7879) done. Loss: 0.4282  lr:0.010000
[ Mon Jul  8 00:58:44 2024 ] 	Batch(1400/7879) done. Loss: 0.8719  lr:0.010000
[ Mon Jul  8 00:59:07 2024 ] 
Training: Epoch [28/120], Step [1499], Loss: 0.13336335122585297, Training Accuracy: 83.09166666666667
[ Mon Jul  8 00:59:07 2024 ] 	Batch(1500/7879) done. Loss: 0.3318  lr:0.010000
[ Mon Jul  8 00:59:30 2024 ] 	Batch(1600/7879) done. Loss: 0.8580  lr:0.010000
[ Mon Jul  8 00:59:52 2024 ] 	Batch(1700/7879) done. Loss: 0.5854  lr:0.010000
[ Mon Jul  8 01:00:15 2024 ] 	Batch(1800/7879) done. Loss: 0.0305  lr:0.010000
[ Mon Jul  8 01:00:38 2024 ] 	Batch(1900/7879) done. Loss: 0.8816  lr:0.010000
[ Mon Jul  8 01:01:00 2024 ] 
Training: Epoch [28/120], Step [1999], Loss: 0.7551469206809998, Training Accuracy: 82.83749999999999
[ Mon Jul  8 01:01:01 2024 ] 	Batch(2000/7879) done. Loss: 0.9945  lr:0.010000
[ Mon Jul  8 01:01:24 2024 ] 	Batch(2100/7879) done. Loss: 1.0948  lr:0.010000
[ Mon Jul  8 01:01:46 2024 ] 	Batch(2200/7879) done. Loss: 0.4825  lr:0.010000
[ Mon Jul  8 01:02:09 2024 ] 	Batch(2300/7879) done. Loss: 0.4328  lr:0.010000
[ Mon Jul  8 01:02:32 2024 ] 	Batch(2400/7879) done. Loss: 0.4978  lr:0.010000
[ Mon Jul  8 01:02:54 2024 ] 
Training: Epoch [28/120], Step [2499], Loss: 0.8115164637565613, Training Accuracy: 82.705
[ Mon Jul  8 01:02:54 2024 ] 	Batch(2500/7879) done. Loss: 0.6393  lr:0.010000
[ Mon Jul  8 01:03:17 2024 ] 	Batch(2600/7879) done. Loss: 0.1941  lr:0.010000
[ Mon Jul  8 01:03:40 2024 ] 	Batch(2700/7879) done. Loss: 0.7202  lr:0.010000
[ Mon Jul  8 01:04:03 2024 ] 	Batch(2800/7879) done. Loss: 0.8177  lr:0.010000
[ Mon Jul  8 01:04:26 2024 ] 	Batch(2900/7879) done. Loss: 0.5140  lr:0.010000
[ Mon Jul  8 01:04:49 2024 ] 
Training: Epoch [28/120], Step [2999], Loss: 0.7877053022384644, Training Accuracy: 82.62916666666666
[ Mon Jul  8 01:04:50 2024 ] 	Batch(3000/7879) done. Loss: 0.7543  lr:0.010000
[ Mon Jul  8 01:05:13 2024 ] 	Batch(3100/7879) done. Loss: 0.3085  lr:0.010000
[ Mon Jul  8 01:05:36 2024 ] 	Batch(3200/7879) done. Loss: 0.3130  lr:0.010000
[ Mon Jul  8 01:06:00 2024 ] 	Batch(3300/7879) done. Loss: 0.4459  lr:0.010000
[ Mon Jul  8 01:06:23 2024 ] 	Batch(3400/7879) done. Loss: 0.4106  lr:0.010000
[ Mon Jul  8 01:06:46 2024 ] 
Training: Epoch [28/120], Step [3499], Loss: 0.24537812173366547, Training Accuracy: 82.44642857142857
[ Mon Jul  8 01:06:46 2024 ] 	Batch(3500/7879) done. Loss: 0.3930  lr:0.010000
[ Mon Jul  8 01:07:09 2024 ] 	Batch(3600/7879) done. Loss: 0.2498  lr:0.010000
[ Mon Jul  8 01:07:32 2024 ] 	Batch(3700/7879) done. Loss: 0.7936  lr:0.010000
[ Mon Jul  8 01:07:54 2024 ] 	Batch(3800/7879) done. Loss: 0.2601  lr:0.010000
[ Mon Jul  8 01:08:17 2024 ] 	Batch(3900/7879) done. Loss: 0.7573  lr:0.010000
[ Mon Jul  8 01:08:39 2024 ] 
Training: Epoch [28/120], Step [3999], Loss: 0.116603322327137, Training Accuracy: 82.403125
[ Mon Jul  8 01:08:40 2024 ] 	Batch(4000/7879) done. Loss: 0.7391  lr:0.010000
[ Mon Jul  8 01:09:02 2024 ] 	Batch(4100/7879) done. Loss: 0.3646  lr:0.010000
[ Mon Jul  8 01:09:25 2024 ] 	Batch(4200/7879) done. Loss: 0.6747  lr:0.010000
[ Mon Jul  8 01:09:48 2024 ] 	Batch(4300/7879) done. Loss: 0.7570  lr:0.010000
[ Mon Jul  8 01:10:11 2024 ] 	Batch(4400/7879) done. Loss: 0.0578  lr:0.010000
[ Mon Jul  8 01:10:33 2024 ] 
Training: Epoch [28/120], Step [4499], Loss: 0.28748127818107605, Training Accuracy: 82.44166666666666
[ Mon Jul  8 01:10:34 2024 ] 	Batch(4500/7879) done. Loss: 0.2570  lr:0.010000
[ Mon Jul  8 01:10:56 2024 ] 	Batch(4600/7879) done. Loss: 0.5042  lr:0.010000
[ Mon Jul  8 01:11:19 2024 ] 	Batch(4700/7879) done. Loss: 0.6740  lr:0.010000
[ Mon Jul  8 01:11:42 2024 ] 	Batch(4800/7879) done. Loss: 1.1277  lr:0.010000
[ Mon Jul  8 01:12:05 2024 ] 	Batch(4900/7879) done. Loss: 0.2470  lr:0.010000
[ Mon Jul  8 01:12:27 2024 ] 
Training: Epoch [28/120], Step [4999], Loss: 0.4420430064201355, Training Accuracy: 82.465
[ Mon Jul  8 01:12:27 2024 ] 	Batch(5000/7879) done. Loss: 0.9452  lr:0.010000
[ Mon Jul  8 01:12:50 2024 ] 	Batch(5100/7879) done. Loss: 0.1786  lr:0.010000
[ Mon Jul  8 01:13:13 2024 ] 	Batch(5200/7879) done. Loss: 0.3560  lr:0.010000
[ Mon Jul  8 01:13:36 2024 ] 	Batch(5300/7879) done. Loss: 2.5684  lr:0.010000
[ Mon Jul  8 01:13:58 2024 ] 	Batch(5400/7879) done. Loss: 0.4985  lr:0.010000
[ Mon Jul  8 01:14:21 2024 ] 
Training: Epoch [28/120], Step [5499], Loss: 0.7582426071166992, Training Accuracy: 82.44318181818183
[ Mon Jul  8 01:14:21 2024 ] 	Batch(5500/7879) done. Loss: 0.2752  lr:0.010000
[ Mon Jul  8 01:14:44 2024 ] 	Batch(5600/7879) done. Loss: 0.4571  lr:0.010000
[ Mon Jul  8 01:15:07 2024 ] 	Batch(5700/7879) done. Loss: 0.9019  lr:0.010000
[ Mon Jul  8 01:15:30 2024 ] 	Batch(5800/7879) done. Loss: 1.3603  lr:0.010000
[ Mon Jul  8 01:15:53 2024 ] 	Batch(5900/7879) done. Loss: 0.0267  lr:0.010000
[ Mon Jul  8 01:16:16 2024 ] 
Training: Epoch [28/120], Step [5999], Loss: 0.7204893827438354, Training Accuracy: 82.41458333333334
[ Mon Jul  8 01:16:16 2024 ] 	Batch(6000/7879) done. Loss: 0.1434  lr:0.010000
[ Mon Jul  8 01:16:39 2024 ] 	Batch(6100/7879) done. Loss: 0.8271  lr:0.010000
[ Mon Jul  8 01:17:01 2024 ] 	Batch(6200/7879) done. Loss: 0.2178  lr:0.010000
[ Mon Jul  8 01:17:24 2024 ] 	Batch(6300/7879) done. Loss: 0.7596  lr:0.010000
[ Mon Jul  8 01:17:47 2024 ] 	Batch(6400/7879) done. Loss: 0.5305  lr:0.010000
[ Mon Jul  8 01:18:09 2024 ] 
Training: Epoch [28/120], Step [6499], Loss: 0.598842978477478, Training Accuracy: 82.32884615384616
[ Mon Jul  8 01:18:10 2024 ] 	Batch(6500/7879) done. Loss: 0.4180  lr:0.010000
[ Mon Jul  8 01:18:32 2024 ] 	Batch(6600/7879) done. Loss: 0.9763  lr:0.010000
[ Mon Jul  8 01:18:55 2024 ] 	Batch(6700/7879) done. Loss: 0.9423  lr:0.010000
[ Mon Jul  8 01:19:18 2024 ] 	Batch(6800/7879) done. Loss: 1.5836  lr:0.010000
[ Mon Jul  8 01:19:41 2024 ] 	Batch(6900/7879) done. Loss: 1.0571  lr:0.010000
[ Mon Jul  8 01:20:03 2024 ] 
Training: Epoch [28/120], Step [6999], Loss: 0.2661036252975464, Training Accuracy: 82.21964285714286
[ Mon Jul  8 01:20:03 2024 ] 	Batch(7000/7879) done. Loss: 0.1766  lr:0.010000
[ Mon Jul  8 01:20:26 2024 ] 	Batch(7100/7879) done. Loss: 1.2457  lr:0.010000
[ Mon Jul  8 01:20:49 2024 ] 	Batch(7200/7879) done. Loss: 0.6560  lr:0.010000
[ Mon Jul  8 01:21:12 2024 ] 	Batch(7300/7879) done. Loss: 0.0679  lr:0.010000
[ Mon Jul  8 01:21:34 2024 ] 	Batch(7400/7879) done. Loss: 1.0550  lr:0.010000
[ Mon Jul  8 01:21:57 2024 ] 
Training: Epoch [28/120], Step [7499], Loss: 0.35860180854797363, Training Accuracy: 82.14333333333333
[ Mon Jul  8 01:21:57 2024 ] 	Batch(7500/7879) done. Loss: 0.7523  lr:0.010000
[ Mon Jul  8 01:22:20 2024 ] 	Batch(7600/7879) done. Loss: 0.2893  lr:0.010000
[ Mon Jul  8 01:22:43 2024 ] 	Batch(7700/7879) done. Loss: 0.7252  lr:0.010000
[ Mon Jul  8 01:23:06 2024 ] 	Batch(7800/7879) done. Loss: 0.2828  lr:0.010000
[ Mon Jul  8 01:23:23 2024 ] 	Mean training loss: 0.5942.
[ Mon Jul  8 01:23:23 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 01:23:24 2024 ] Training epoch: 30
[ Mon Jul  8 01:23:24 2024 ] 	Batch(0/7879) done. Loss: 0.7054  lr:0.010000
[ Mon Jul  8 01:23:47 2024 ] 	Batch(100/7879) done. Loss: 0.2130  lr:0.010000
[ Mon Jul  8 01:24:10 2024 ] 	Batch(200/7879) done. Loss: 0.5724  lr:0.010000
[ Mon Jul  8 01:24:32 2024 ] 	Batch(300/7879) done. Loss: 0.6867  lr:0.010000
[ Mon Jul  8 01:24:55 2024 ] 	Batch(400/7879) done. Loss: 0.5912  lr:0.010000
[ Mon Jul  8 01:25:18 2024 ] 
Training: Epoch [29/120], Step [499], Loss: 0.40321603417396545, Training Accuracy: 83.95
[ Mon Jul  8 01:25:18 2024 ] 	Batch(500/7879) done. Loss: 0.8093  lr:0.010000
[ Mon Jul  8 01:25:40 2024 ] 	Batch(600/7879) done. Loss: 0.5534  lr:0.010000
[ Mon Jul  8 01:26:03 2024 ] 	Batch(700/7879) done. Loss: 0.4528  lr:0.010000
[ Mon Jul  8 01:26:26 2024 ] 	Batch(800/7879) done. Loss: 0.0693  lr:0.010000
[ Mon Jul  8 01:26:49 2024 ] 	Batch(900/7879) done. Loss: 0.3700  lr:0.010000
[ Mon Jul  8 01:27:11 2024 ] 
Training: Epoch [29/120], Step [999], Loss: 1.626617431640625, Training Accuracy: 83.55
[ Mon Jul  8 01:27:11 2024 ] 	Batch(1000/7879) done. Loss: 0.4619  lr:0.010000
[ Mon Jul  8 01:27:34 2024 ] 	Batch(1100/7879) done. Loss: 0.4860  lr:0.010000
[ Mon Jul  8 01:27:57 2024 ] 	Batch(1200/7879) done. Loss: 0.9597  lr:0.010000
[ Mon Jul  8 01:28:20 2024 ] 	Batch(1300/7879) done. Loss: 0.9637  lr:0.010000
[ Mon Jul  8 01:28:43 2024 ] 	Batch(1400/7879) done. Loss: 0.7480  lr:0.010000
[ Mon Jul  8 01:29:05 2024 ] 
Training: Epoch [29/120], Step [1499], Loss: 0.5982493758201599, Training Accuracy: 83.025
[ Mon Jul  8 01:29:06 2024 ] 	Batch(1500/7879) done. Loss: 0.5649  lr:0.010000
[ Mon Jul  8 01:29:28 2024 ] 	Batch(1600/7879) done. Loss: 0.2814  lr:0.010000
[ Mon Jul  8 01:29:51 2024 ] 	Batch(1700/7879) done. Loss: 0.2618  lr:0.010000
[ Mon Jul  8 01:30:14 2024 ] 	Batch(1800/7879) done. Loss: 0.0863  lr:0.010000
[ Mon Jul  8 01:30:37 2024 ] 	Batch(1900/7879) done. Loss: 1.1501  lr:0.010000
[ Mon Jul  8 01:30:59 2024 ] 
Training: Epoch [29/120], Step [1999], Loss: 0.16245847940444946, Training Accuracy: 82.83749999999999
[ Mon Jul  8 01:30:59 2024 ] 	Batch(2000/7879) done. Loss: 0.4946  lr:0.010000
[ Mon Jul  8 01:31:22 2024 ] 	Batch(2100/7879) done. Loss: 1.4664  lr:0.010000
[ Mon Jul  8 01:31:45 2024 ] 	Batch(2200/7879) done. Loss: 0.4851  lr:0.010000
[ Mon Jul  8 01:32:07 2024 ] 	Batch(2300/7879) done. Loss: 0.7895  lr:0.010000
[ Mon Jul  8 01:32:30 2024 ] 	Batch(2400/7879) done. Loss: 0.7870  lr:0.010000
[ Mon Jul  8 01:32:53 2024 ] 
Training: Epoch [29/120], Step [2499], Loss: 0.42545372247695923, Training Accuracy: 82.705
[ Mon Jul  8 01:32:53 2024 ] 	Batch(2500/7879) done. Loss: 0.1784  lr:0.010000
[ Mon Jul  8 01:33:16 2024 ] 	Batch(2600/7879) done. Loss: 0.1229  lr:0.010000
[ Mon Jul  8 01:33:38 2024 ] 	Batch(2700/7879) done. Loss: 0.6389  lr:0.010000
[ Mon Jul  8 01:34:01 2024 ] 	Batch(2800/7879) done. Loss: 0.7331  lr:0.010000
[ Mon Jul  8 01:34:24 2024 ] 	Batch(2900/7879) done. Loss: 1.2391  lr:0.010000
[ Mon Jul  8 01:34:46 2024 ] 
Training: Epoch [29/120], Step [2999], Loss: 0.33011358976364136, Training Accuracy: 82.78333333333333
[ Mon Jul  8 01:34:47 2024 ] 	Batch(3000/7879) done. Loss: 0.5480  lr:0.010000
[ Mon Jul  8 01:35:09 2024 ] 	Batch(3100/7879) done. Loss: 0.3044  lr:0.010000
[ Mon Jul  8 01:35:32 2024 ] 	Batch(3200/7879) done. Loss: 0.3111  lr:0.010000
[ Mon Jul  8 01:35:55 2024 ] 	Batch(3300/7879) done. Loss: 0.3898  lr:0.010000
[ Mon Jul  8 01:36:17 2024 ] 	Batch(3400/7879) done. Loss: 0.3764  lr:0.010000
[ Mon Jul  8 01:36:40 2024 ] 
Training: Epoch [29/120], Step [3499], Loss: 0.2847990393638611, Training Accuracy: 82.71071428571429
[ Mon Jul  8 01:36:40 2024 ] 	Batch(3500/7879) done. Loss: 0.6854  lr:0.010000
[ Mon Jul  8 01:37:03 2024 ] 	Batch(3600/7879) done. Loss: 0.2250  lr:0.010000
[ Mon Jul  8 01:37:26 2024 ] 	Batch(3700/7879) done. Loss: 0.0897  lr:0.010000
[ Mon Jul  8 01:37:48 2024 ] 	Batch(3800/7879) done. Loss: 0.7235  lr:0.010000
[ Mon Jul  8 01:38:11 2024 ] 	Batch(3900/7879) done. Loss: 0.0884  lr:0.010000
[ Mon Jul  8 01:38:34 2024 ] 
Training: Epoch [29/120], Step [3999], Loss: 0.47485920786857605, Training Accuracy: 82.67812500000001
[ Mon Jul  8 01:38:34 2024 ] 	Batch(4000/7879) done. Loss: 0.7965  lr:0.010000
[ Mon Jul  8 01:38:57 2024 ] 	Batch(4100/7879) done. Loss: 0.8256  lr:0.010000
[ Mon Jul  8 01:39:19 2024 ] 	Batch(4200/7879) done. Loss: 0.3712  lr:0.010000
[ Mon Jul  8 01:39:42 2024 ] 	Batch(4300/7879) done. Loss: 0.9637  lr:0.010000
[ Mon Jul  8 01:40:05 2024 ] 	Batch(4400/7879) done. Loss: 0.0968  lr:0.010000
[ Mon Jul  8 01:40:27 2024 ] 
Training: Epoch [29/120], Step [4499], Loss: 0.9804950952529907, Training Accuracy: 82.61388888888888
[ Mon Jul  8 01:40:28 2024 ] 	Batch(4500/7879) done. Loss: 0.1716  lr:0.010000
[ Mon Jul  8 01:40:50 2024 ] 	Batch(4600/7879) done. Loss: 0.8145  lr:0.010000
[ Mon Jul  8 01:41:13 2024 ] 	Batch(4700/7879) done. Loss: 2.2351  lr:0.010000
[ Mon Jul  8 01:41:36 2024 ] 	Batch(4800/7879) done. Loss: 0.9624  lr:0.010000
[ Mon Jul  8 01:41:59 2024 ] 	Batch(4900/7879) done. Loss: 0.2119  lr:0.010000
[ Mon Jul  8 01:42:21 2024 ] 
Training: Epoch [29/120], Step [4999], Loss: 0.21442531049251556, Training Accuracy: 82.56750000000001
[ Mon Jul  8 01:42:21 2024 ] 	Batch(5000/7879) done. Loss: 0.4726  lr:0.010000
[ Mon Jul  8 01:42:44 2024 ] 	Batch(5100/7879) done. Loss: 0.1573  lr:0.010000
[ Mon Jul  8 01:43:07 2024 ] 	Batch(5200/7879) done. Loss: 1.5619  lr:0.010000
[ Mon Jul  8 01:43:30 2024 ] 	Batch(5300/7879) done. Loss: 0.6325  lr:0.010000
[ Mon Jul  8 01:43:54 2024 ] 	Batch(5400/7879) done. Loss: 0.5428  lr:0.010000
[ Mon Jul  8 01:44:17 2024 ] 
Training: Epoch [29/120], Step [5499], Loss: 0.2252296656370163, Training Accuracy: 82.53636363636365
[ Mon Jul  8 01:44:17 2024 ] 	Batch(5500/7879) done. Loss: 0.5132  lr:0.010000
[ Mon Jul  8 01:44:41 2024 ] 	Batch(5600/7879) done. Loss: 0.1166  lr:0.010000
[ Mon Jul  8 01:45:04 2024 ] 	Batch(5700/7879) done. Loss: 0.1571  lr:0.010000
[ Mon Jul  8 01:45:27 2024 ] 	Batch(5800/7879) done. Loss: 0.4342  lr:0.010000
[ Mon Jul  8 01:45:49 2024 ] 	Batch(5900/7879) done. Loss: 0.2045  lr:0.010000
[ Mon Jul  8 01:46:12 2024 ] 
Training: Epoch [29/120], Step [5999], Loss: 0.6791059970855713, Training Accuracy: 82.48333333333333
[ Mon Jul  8 01:46:12 2024 ] 	Batch(6000/7879) done. Loss: 0.9423  lr:0.010000
[ Mon Jul  8 01:46:35 2024 ] 	Batch(6100/7879) done. Loss: 0.5111  lr:0.010000
[ Mon Jul  8 01:46:58 2024 ] 	Batch(6200/7879) done. Loss: 0.6120  lr:0.010000
[ Mon Jul  8 01:47:21 2024 ] 	Batch(6300/7879) done. Loss: 0.6787  lr:0.010000
[ Mon Jul  8 01:47:43 2024 ] 	Batch(6400/7879) done. Loss: 0.0947  lr:0.010000
[ Mon Jul  8 01:48:06 2024 ] 
Training: Epoch [29/120], Step [6499], Loss: 0.3571337163448334, Training Accuracy: 82.45961538461538
[ Mon Jul  8 01:48:06 2024 ] 	Batch(6500/7879) done. Loss: 0.2378  lr:0.010000
[ Mon Jul  8 01:48:29 2024 ] 	Batch(6600/7879) done. Loss: 0.5779  lr:0.010000
[ Mon Jul  8 01:48:52 2024 ] 	Batch(6700/7879) done. Loss: 0.0709  lr:0.010000
[ Mon Jul  8 01:49:14 2024 ] 	Batch(6800/7879) done. Loss: 0.5916  lr:0.010000
[ Mon Jul  8 01:49:37 2024 ] 	Batch(6900/7879) done. Loss: 1.0745  lr:0.010000
[ Mon Jul  8 01:50:00 2024 ] 
Training: Epoch [29/120], Step [6999], Loss: 0.2733665108680725, Training Accuracy: 82.41785714285714
[ Mon Jul  8 01:50:00 2024 ] 	Batch(7000/7879) done. Loss: 0.3498  lr:0.010000
[ Mon Jul  8 01:50:23 2024 ] 	Batch(7100/7879) done. Loss: 0.5326  lr:0.010000
[ Mon Jul  8 01:50:45 2024 ] 	Batch(7200/7879) done. Loss: 0.7854  lr:0.010000
[ Mon Jul  8 01:51:08 2024 ] 	Batch(7300/7879) done. Loss: 0.7796  lr:0.010000
[ Mon Jul  8 01:51:31 2024 ] 	Batch(7400/7879) done. Loss: 0.1473  lr:0.010000
[ Mon Jul  8 01:51:53 2024 ] 
Training: Epoch [29/120], Step [7499], Loss: 1.079508662223816, Training Accuracy: 82.35166666666667
[ Mon Jul  8 01:51:53 2024 ] 	Batch(7500/7879) done. Loss: 0.7355  lr:0.010000
[ Mon Jul  8 01:52:16 2024 ] 	Batch(7600/7879) done. Loss: 0.1123  lr:0.010000
[ Mon Jul  8 01:52:39 2024 ] 	Batch(7700/7879) done. Loss: 0.3194  lr:0.010000
[ Mon Jul  8 01:53:02 2024 ] 	Batch(7800/7879) done. Loss: 1.4022  lr:0.010000
[ Mon Jul  8 01:53:19 2024 ] 	Mean training loss: 0.5842.
[ Mon Jul  8 01:53:19 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 01:53:20 2024 ] Eval epoch: 30
[ Mon Jul  8 01:59:17 2024 ] 	Mean val loss of 6365 batches: 1.192618687565088.
[ Mon Jul  8 01:59:17 2024 ] 
Validation: Epoch [29/120], Samples [35714.0/50919], Loss: 0.5356419086456299, Validation Accuracy: 70.13884797423358
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 1 : 189 / 275 = 68 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 2 : 216 / 273 = 79 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 3 : 173 / 273 = 63 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 4 : 213 / 275 = 77 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 5 : 229 / 275 = 83 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 6 : 198 / 275 = 72 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 7 : 254 / 273 = 93 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 8 : 265 / 273 = 97 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 9 : 202 / 273 = 73 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 10 : 67 / 273 = 24 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 11 : 74 / 272 = 27 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 12 : 222 / 271 = 81 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 13 : 257 / 275 = 93 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 14 : 260 / 276 = 94 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 15 : 222 / 273 = 81 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 16 : 101 / 274 = 36 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 17 : 157 / 273 = 57 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 18 : 214 / 274 = 78 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 19 : 215 / 272 = 79 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 20 : 237 / 273 = 86 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 21 : 203 / 274 = 74 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 22 : 235 / 274 = 85 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 23 : 231 / 276 = 83 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 24 : 230 / 274 = 83 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 25 : 259 / 275 = 94 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 26 : 269 / 276 = 97 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 27 : 233 / 275 = 84 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 28 : 185 / 275 = 67 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 29 : 156 / 275 = 56 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 30 : 166 / 276 = 60 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 31 : 233 / 276 = 84 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 32 : 230 / 276 = 83 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 33 : 230 / 276 = 83 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 34 : 232 / 276 = 84 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 35 : 238 / 275 = 86 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 36 : 197 / 276 = 71 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 37 : 241 / 276 = 87 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 38 : 201 / 276 = 72 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 39 : 186 / 276 = 67 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 40 : 162 / 276 = 58 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 41 : 247 / 276 = 89 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 42 : 248 / 275 = 90 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 43 : 143 / 276 = 51 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 44 : 222 / 276 = 80 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 45 : 252 / 276 = 91 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 46 : 227 / 276 = 82 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 47 : 212 / 275 = 77 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 48 : 206 / 275 = 74 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 49 : 216 / 274 = 78 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 50 : 209 / 276 = 75 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 51 : 225 / 276 = 81 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 52 : 181 / 276 = 65 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 53 : 233 / 276 = 84 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 54 : 260 / 274 = 94 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 55 : 231 / 276 = 83 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 56 : 249 / 275 = 90 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 57 : 258 / 276 = 93 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 58 : 262 / 273 = 95 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 59 : 256 / 276 = 92 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 60 : 461 / 561 = 82 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 61 : 465 / 566 = 82 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 62 : 374 / 572 = 65 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 63 : 470 / 570 = 82 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 64 : 400 / 574 = 69 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 65 : 513 / 573 = 89 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 66 : 389 / 573 = 67 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 67 : 334 / 575 = 58 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 68 : 297 / 575 = 51 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 69 : 493 / 575 = 85 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 70 : 228 / 575 = 39 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 71 : 193 / 575 = 33 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 72 : 87 / 571 = 15 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 73 : 147 / 570 = 25 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 74 : 312 / 569 = 54 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 75 : 400 / 573 = 69 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 76 : 309 / 574 = 53 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 77 : 366 / 573 = 63 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 78 : 395 / 575 = 68 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 79 : 516 / 574 = 89 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 80 : 490 / 573 = 85 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 81 : 238 / 575 = 41 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 82 : 333 / 575 = 57 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 83 : 141 / 572 = 24 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 84 : 353 / 574 = 61 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 85 : 228 / 574 = 39 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 86 : 462 / 575 = 80 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 87 : 393 / 576 = 68 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 88 : 381 / 575 = 66 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 89 : 335 / 576 = 58 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 90 : 178 / 574 = 31 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 91 : 398 / 568 = 70 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 92 : 379 / 576 = 65 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 93 : 193 / 573 = 33 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 94 : 520 / 574 = 90 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 95 : 482 / 575 = 83 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 96 : 557 / 575 = 96 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 97 : 536 / 574 = 93 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 98 : 505 / 575 = 87 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 99 : 515 / 574 = 89 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 100 : 415 / 574 = 72 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 101 : 523 / 574 = 91 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 102 : 361 / 575 = 62 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 103 : 436 / 576 = 75 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 104 : 239 / 575 = 41 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 105 : 127 / 575 = 22 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 106 : 194 / 576 = 33 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 107 : 357 / 576 = 61 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 108 : 378 / 575 = 65 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 109 : 339 / 575 = 58 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 110 : 553 / 575 = 96 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 111 : 487 / 576 = 84 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 112 : 516 / 575 = 89 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 113 : 460 / 576 = 79 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 114 : 500 / 576 = 86 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 115 : 439 / 576 = 76 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 116 : 499 / 575 = 86 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 117 : 389 / 575 = 67 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 118 : 380 / 575 = 66 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 119 : 508 / 576 = 88 %
[ Mon Jul  8 01:59:17 2024 ] Accuracy of 120 : 229 / 274 = 83 %
[ Mon Jul  8 01:59:18 2024 ] Training epoch: 31
[ Mon Jul  8 01:59:18 2024 ] 	Batch(0/7879) done. Loss: 0.4283  lr:0.010000
[ Mon Jul  8 01:59:41 2024 ] 	Batch(100/7879) done. Loss: 0.1833  lr:0.010000
[ Mon Jul  8 02:00:03 2024 ] 	Batch(200/7879) done. Loss: 0.4795  lr:0.010000
[ Mon Jul  8 02:00:26 2024 ] 	Batch(300/7879) done. Loss: 0.6396  lr:0.010000
[ Mon Jul  8 02:00:49 2024 ] 	Batch(400/7879) done. Loss: 0.3863  lr:0.010000
[ Mon Jul  8 02:01:11 2024 ] 
Training: Epoch [30/120], Step [499], Loss: 0.4697025418281555, Training Accuracy: 83.525
[ Mon Jul  8 02:01:12 2024 ] 	Batch(500/7879) done. Loss: 0.7514  lr:0.010000
[ Mon Jul  8 02:01:34 2024 ] 	Batch(600/7879) done. Loss: 0.0249  lr:0.010000
[ Mon Jul  8 02:01:57 2024 ] 	Batch(700/7879) done. Loss: 0.3871  lr:0.010000
[ Mon Jul  8 02:02:20 2024 ] 	Batch(800/7879) done. Loss: 0.5154  lr:0.010000
[ Mon Jul  8 02:02:43 2024 ] 	Batch(900/7879) done. Loss: 1.6926  lr:0.010000
[ Mon Jul  8 02:03:06 2024 ] 
Training: Epoch [30/120], Step [999], Loss: 0.45461735129356384, Training Accuracy: 83.175
[ Mon Jul  8 02:03:06 2024 ] 	Batch(1000/7879) done. Loss: 0.0666  lr:0.010000
[ Mon Jul  8 02:03:28 2024 ] 	Batch(1100/7879) done. Loss: 0.6663  lr:0.010000
[ Mon Jul  8 02:03:51 2024 ] 	Batch(1200/7879) done. Loss: 0.3012  lr:0.010000
[ Mon Jul  8 02:04:15 2024 ] 	Batch(1300/7879) done. Loss: 0.8114  lr:0.010000
[ Mon Jul  8 02:04:38 2024 ] 	Batch(1400/7879) done. Loss: 0.6100  lr:0.010000
[ Mon Jul  8 02:05:01 2024 ] 
Training: Epoch [30/120], Step [1499], Loss: 0.5986465215682983, Training Accuracy: 83.175
[ Mon Jul  8 02:05:02 2024 ] 	Batch(1500/7879) done. Loss: 0.4923  lr:0.010000
[ Mon Jul  8 02:05:25 2024 ] 	Batch(1600/7879) done. Loss: 0.3278  lr:0.010000
[ Mon Jul  8 02:05:49 2024 ] 	Batch(1700/7879) done. Loss: 0.5219  lr:0.010000
[ Mon Jul  8 02:06:12 2024 ] 	Batch(1800/7879) done. Loss: 0.9744  lr:0.010000
[ Mon Jul  8 02:06:35 2024 ] 	Batch(1900/7879) done. Loss: 0.6309  lr:0.010000
[ Mon Jul  8 02:06:59 2024 ] 
Training: Epoch [30/120], Step [1999], Loss: 0.5745294690132141, Training Accuracy: 83.25
[ Mon Jul  8 02:06:59 2024 ] 	Batch(2000/7879) done. Loss: 1.1600  lr:0.010000
[ Mon Jul  8 02:07:22 2024 ] 	Batch(2100/7879) done. Loss: 0.3311  lr:0.010000
[ Mon Jul  8 02:07:46 2024 ] 	Batch(2200/7879) done. Loss: 0.5041  lr:0.010000
[ Mon Jul  8 02:08:09 2024 ] 	Batch(2300/7879) done. Loss: 0.3500  lr:0.010000
[ Mon Jul  8 02:08:33 2024 ] 	Batch(2400/7879) done. Loss: 0.7757  lr:0.010000
[ Mon Jul  8 02:08:55 2024 ] 
Training: Epoch [30/120], Step [2499], Loss: 0.1332612782716751, Training Accuracy: 83.12
[ Mon Jul  8 02:08:56 2024 ] 	Batch(2500/7879) done. Loss: 0.3549  lr:0.010000
[ Mon Jul  8 02:09:18 2024 ] 	Batch(2600/7879) done. Loss: 0.2025  lr:0.010000
[ Mon Jul  8 02:09:41 2024 ] 	Batch(2700/7879) done. Loss: 0.1387  lr:0.010000
[ Mon Jul  8 02:10:04 2024 ] 	Batch(2800/7879) done. Loss: 0.8226  lr:0.010000
[ Mon Jul  8 02:10:26 2024 ] 	Batch(2900/7879) done. Loss: 1.0077  lr:0.010000
[ Mon Jul  8 02:10:49 2024 ] 
Training: Epoch [30/120], Step [2999], Loss: 1.1533832550048828, Training Accuracy: 83.10833333333333
[ Mon Jul  8 02:10:49 2024 ] 	Batch(3000/7879) done. Loss: 0.1669  lr:0.010000
[ Mon Jul  8 02:11:12 2024 ] 	Batch(3100/7879) done. Loss: 1.1210  lr:0.010000
[ Mon Jul  8 02:11:35 2024 ] 	Batch(3200/7879) done. Loss: 0.5996  lr:0.010000
[ Mon Jul  8 02:11:57 2024 ] 	Batch(3300/7879) done. Loss: 0.5614  lr:0.010000
[ Mon Jul  8 02:12:20 2024 ] 	Batch(3400/7879) done. Loss: 0.8689  lr:0.010000
[ Mon Jul  8 02:12:43 2024 ] 
Training: Epoch [30/120], Step [3499], Loss: 0.3575705289840698, Training Accuracy: 82.97857142857143
[ Mon Jul  8 02:12:43 2024 ] 	Batch(3500/7879) done. Loss: 0.2712  lr:0.010000
[ Mon Jul  8 02:13:06 2024 ] 	Batch(3600/7879) done. Loss: 0.5898  lr:0.010000
[ Mon Jul  8 02:13:28 2024 ] 	Batch(3700/7879) done. Loss: 0.3965  lr:0.010000
[ Mon Jul  8 02:13:51 2024 ] 	Batch(3800/7879) done. Loss: 0.7043  lr:0.010000
[ Mon Jul  8 02:14:14 2024 ] 	Batch(3900/7879) done. Loss: 0.2231  lr:0.010000
[ Mon Jul  8 02:14:36 2024 ] 
Training: Epoch [30/120], Step [3999], Loss: 0.11394216120243073, Training Accuracy: 82.95625
[ Mon Jul  8 02:14:36 2024 ] 	Batch(4000/7879) done. Loss: 0.3600  lr:0.010000
[ Mon Jul  8 02:15:00 2024 ] 	Batch(4100/7879) done. Loss: 1.3786  lr:0.010000
[ Mon Jul  8 02:15:23 2024 ] 	Batch(4200/7879) done. Loss: 0.1740  lr:0.010000
[ Mon Jul  8 02:15:47 2024 ] 	Batch(4300/7879) done. Loss: 0.6231  lr:0.010000
[ Mon Jul  8 02:16:10 2024 ] 	Batch(4400/7879) done. Loss: 1.1161  lr:0.010000
[ Mon Jul  8 02:16:33 2024 ] 
Training: Epoch [30/120], Step [4499], Loss: 0.3453378975391388, Training Accuracy: 82.88333333333333
[ Mon Jul  8 02:16:33 2024 ] 	Batch(4500/7879) done. Loss: 1.4773  lr:0.010000
[ Mon Jul  8 02:16:56 2024 ] 	Batch(4600/7879) done. Loss: 0.1694  lr:0.010000
[ Mon Jul  8 02:17:19 2024 ] 	Batch(4700/7879) done. Loss: 0.4889  lr:0.010000
[ Mon Jul  8 02:17:42 2024 ] 	Batch(4800/7879) done. Loss: 0.4553  lr:0.010000
[ Mon Jul  8 02:18:04 2024 ] 	Batch(4900/7879) done. Loss: 1.6236  lr:0.010000
[ Mon Jul  8 02:18:27 2024 ] 
Training: Epoch [30/120], Step [4999], Loss: 0.49187198281288147, Training Accuracy: 82.7775
[ Mon Jul  8 02:18:27 2024 ] 	Batch(5000/7879) done. Loss: 0.8124  lr:0.010000
[ Mon Jul  8 02:18:50 2024 ] 	Batch(5100/7879) done. Loss: 0.7482  lr:0.010000
[ Mon Jul  8 02:19:13 2024 ] 	Batch(5200/7879) done. Loss: 0.8179  lr:0.010000
[ Mon Jul  8 02:19:35 2024 ] 	Batch(5300/7879) done. Loss: 0.3089  lr:0.010000
[ Mon Jul  8 02:19:58 2024 ] 	Batch(5400/7879) done. Loss: 0.4896  lr:0.010000
[ Mon Jul  8 02:20:20 2024 ] 
Training: Epoch [30/120], Step [5499], Loss: 1.0252166986465454, Training Accuracy: 82.72954545454544
[ Mon Jul  8 02:20:21 2024 ] 	Batch(5500/7879) done. Loss: 1.1330  lr:0.010000
[ Mon Jul  8 02:20:43 2024 ] 	Batch(5600/7879) done. Loss: 0.4239  lr:0.010000
[ Mon Jul  8 02:21:07 2024 ] 	Batch(5700/7879) done. Loss: 1.0177  lr:0.010000
[ Mon Jul  8 02:21:30 2024 ] 	Batch(5800/7879) done. Loss: 0.0670  lr:0.010000
[ Mon Jul  8 02:21:53 2024 ] 	Batch(5900/7879) done. Loss: 0.5607  lr:0.010000
[ Mon Jul  8 02:22:17 2024 ] 
Training: Epoch [30/120], Step [5999], Loss: 0.5348422527313232, Training Accuracy: 82.6375
[ Mon Jul  8 02:22:17 2024 ] 	Batch(6000/7879) done. Loss: 0.0905  lr:0.010000
[ Mon Jul  8 02:22:40 2024 ] 	Batch(6100/7879) done. Loss: 0.0407  lr:0.010000
[ Mon Jul  8 02:23:03 2024 ] 	Batch(6200/7879) done. Loss: 0.1365  lr:0.010000
[ Mon Jul  8 02:23:25 2024 ] 	Batch(6300/7879) done. Loss: 0.4084  lr:0.010000
[ Mon Jul  8 02:23:48 2024 ] 	Batch(6400/7879) done. Loss: 0.3177  lr:0.010000
[ Mon Jul  8 02:24:11 2024 ] 
Training: Epoch [30/120], Step [6499], Loss: 0.5898184776306152, Training Accuracy: 82.54423076923077
[ Mon Jul  8 02:24:11 2024 ] 	Batch(6500/7879) done. Loss: 0.2904  lr:0.010000
[ Mon Jul  8 02:24:34 2024 ] 	Batch(6600/7879) done. Loss: 0.4570  lr:0.010000
[ Mon Jul  8 02:24:56 2024 ] 	Batch(6700/7879) done. Loss: 0.3562  lr:0.010000
[ Mon Jul  8 02:25:19 2024 ] 	Batch(6800/7879) done. Loss: 0.5376  lr:0.010000
[ Mon Jul  8 02:25:42 2024 ] 	Batch(6900/7879) done. Loss: 1.5947  lr:0.010000
[ Mon Jul  8 02:26:04 2024 ] 
Training: Epoch [30/120], Step [6999], Loss: 1.388289213180542, Training Accuracy: 82.36964285714285
[ Mon Jul  8 02:26:05 2024 ] 	Batch(7000/7879) done. Loss: 0.1658  lr:0.010000
[ Mon Jul  8 02:26:27 2024 ] 	Batch(7100/7879) done. Loss: 0.5867  lr:0.010000
[ Mon Jul  8 02:26:50 2024 ] 	Batch(7200/7879) done. Loss: 0.3296  lr:0.010000
[ Mon Jul  8 02:27:14 2024 ] 	Batch(7300/7879) done. Loss: 0.4379  lr:0.010000
[ Mon Jul  8 02:27:37 2024 ] 	Batch(7400/7879) done. Loss: 0.5892  lr:0.010000
[ Mon Jul  8 02:28:00 2024 ] 
Training: Epoch [30/120], Step [7499], Loss: 1.0020742416381836, Training Accuracy: 82.37
[ Mon Jul  8 02:28:00 2024 ] 	Batch(7500/7879) done. Loss: 0.8750  lr:0.010000
[ Mon Jul  8 02:28:24 2024 ] 	Batch(7600/7879) done. Loss: 0.9840  lr:0.010000
[ Mon Jul  8 02:28:47 2024 ] 	Batch(7700/7879) done. Loss: 0.5724  lr:0.010000
[ Mon Jul  8 02:29:09 2024 ] 	Batch(7800/7879) done. Loss: 0.4478  lr:0.010000
[ Mon Jul  8 02:29:27 2024 ] 	Mean training loss: 0.5701.
[ Mon Jul  8 02:29:27 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 02:29:27 2024 ] Training epoch: 32
[ Mon Jul  8 02:29:28 2024 ] 	Batch(0/7879) done. Loss: 0.7392  lr:0.010000
[ Mon Jul  8 02:29:50 2024 ] 	Batch(100/7879) done. Loss: 0.3424  lr:0.010000
[ Mon Jul  8 02:30:13 2024 ] 	Batch(200/7879) done. Loss: 0.5358  lr:0.010000
[ Mon Jul  8 02:30:36 2024 ] 	Batch(300/7879) done. Loss: 0.3042  lr:0.010000
[ Mon Jul  8 02:30:59 2024 ] 	Batch(400/7879) done. Loss: 1.1148  lr:0.010000
[ Mon Jul  8 02:31:21 2024 ] 
Training: Epoch [31/120], Step [499], Loss: 0.391999751329422, Training Accuracy: 83.95
[ Mon Jul  8 02:31:21 2024 ] 	Batch(500/7879) done. Loss: 1.5803  lr:0.010000
[ Mon Jul  8 02:31:44 2024 ] 	Batch(600/7879) done. Loss: 0.8942  lr:0.010000
[ Mon Jul  8 02:32:07 2024 ] 	Batch(700/7879) done. Loss: 0.4804  lr:0.010000
[ Mon Jul  8 02:32:29 2024 ] 	Batch(800/7879) done. Loss: 0.6144  lr:0.010000
[ Mon Jul  8 02:32:53 2024 ] 	Batch(900/7879) done. Loss: 1.2248  lr:0.010000
[ Mon Jul  8 02:33:16 2024 ] 
Training: Epoch [31/120], Step [999], Loss: 0.47415193915367126, Training Accuracy: 83.475
[ Mon Jul  8 02:33:16 2024 ] 	Batch(1000/7879) done. Loss: 0.4499  lr:0.010000
[ Mon Jul  8 02:33:39 2024 ] 	Batch(1100/7879) done. Loss: 1.0460  lr:0.010000
[ Mon Jul  8 02:34:02 2024 ] 	Batch(1200/7879) done. Loss: 0.1291  lr:0.010000
[ Mon Jul  8 02:34:24 2024 ] 	Batch(1300/7879) done. Loss: 0.0442  lr:0.010000
[ Mon Jul  8 02:34:47 2024 ] 	Batch(1400/7879) done. Loss: 0.1306  lr:0.010000
[ Mon Jul  8 02:35:09 2024 ] 
Training: Epoch [31/120], Step [1499], Loss: 0.8413047194480896, Training Accuracy: 83.06666666666666
[ Mon Jul  8 02:35:10 2024 ] 	Batch(1500/7879) done. Loss: 0.5356  lr:0.010000
[ Mon Jul  8 02:35:32 2024 ] 	Batch(1600/7879) done. Loss: 0.5228  lr:0.010000
[ Mon Jul  8 02:35:55 2024 ] 	Batch(1700/7879) done. Loss: 0.7658  lr:0.010000
[ Mon Jul  8 02:36:18 2024 ] 	Batch(1800/7879) done. Loss: 0.7867  lr:0.010000
[ Mon Jul  8 02:36:41 2024 ] 	Batch(1900/7879) done. Loss: 0.3011  lr:0.010000
[ Mon Jul  8 02:37:03 2024 ] 
Training: Epoch [31/120], Step [1999], Loss: 0.28026267886161804, Training Accuracy: 83.09375
[ Mon Jul  8 02:37:04 2024 ] 	Batch(2000/7879) done. Loss: 0.4020  lr:0.010000
[ Mon Jul  8 02:37:26 2024 ] 	Batch(2100/7879) done. Loss: 1.2414  lr:0.010000
[ Mon Jul  8 02:37:49 2024 ] 	Batch(2200/7879) done. Loss: 1.0961  lr:0.010000
[ Mon Jul  8 02:38:12 2024 ] 	Batch(2300/7879) done. Loss: 0.3395  lr:0.010000
[ Mon Jul  8 02:38:35 2024 ] 	Batch(2400/7879) done. Loss: 0.2606  lr:0.010000
[ Mon Jul  8 02:38:58 2024 ] 
Training: Epoch [31/120], Step [2499], Loss: 0.0831737071275711, Training Accuracy: 83.095
[ Mon Jul  8 02:38:58 2024 ] 	Batch(2500/7879) done. Loss: 0.4465  lr:0.010000
[ Mon Jul  8 02:39:22 2024 ] 	Batch(2600/7879) done. Loss: 0.2996  lr:0.010000
[ Mon Jul  8 02:39:45 2024 ] 	Batch(2700/7879) done. Loss: 1.1138  lr:0.010000
[ Mon Jul  8 02:40:09 2024 ] 	Batch(2800/7879) done. Loss: 0.6532  lr:0.010000
[ Mon Jul  8 02:40:32 2024 ] 	Batch(2900/7879) done. Loss: 0.8374  lr:0.010000
[ Mon Jul  8 02:40:55 2024 ] 
Training: Epoch [31/120], Step [2999], Loss: 0.4286608397960663, Training Accuracy: 83.00833333333333
[ Mon Jul  8 02:40:56 2024 ] 	Batch(3000/7879) done. Loss: 0.7849  lr:0.010000
[ Mon Jul  8 02:41:19 2024 ] 	Batch(3100/7879) done. Loss: 0.6602  lr:0.010000
[ Mon Jul  8 02:41:43 2024 ] 	Batch(3200/7879) done. Loss: 0.2491  lr:0.010000
[ Mon Jul  8 02:42:06 2024 ] 	Batch(3300/7879) done. Loss: 0.3402  lr:0.010000
[ Mon Jul  8 02:42:30 2024 ] 	Batch(3400/7879) done. Loss: 0.1427  lr:0.010000
[ Mon Jul  8 02:42:53 2024 ] 
Training: Epoch [31/120], Step [3499], Loss: 0.7420775890350342, Training Accuracy: 83.14642857142857
[ Mon Jul  8 02:42:53 2024 ] 	Batch(3500/7879) done. Loss: 0.1709  lr:0.010000
[ Mon Jul  8 02:43:16 2024 ] 	Batch(3600/7879) done. Loss: 0.1940  lr:0.010000
[ Mon Jul  8 02:43:40 2024 ] 	Batch(3700/7879) done. Loss: 0.0497  lr:0.010000
[ Mon Jul  8 02:44:02 2024 ] 	Batch(3800/7879) done. Loss: 0.5443  lr:0.010000
[ Mon Jul  8 02:44:25 2024 ] 	Batch(3900/7879) done. Loss: 0.2643  lr:0.010000
[ Mon Jul  8 02:44:48 2024 ] 
Training: Epoch [31/120], Step [3999], Loss: 0.8548712134361267, Training Accuracy: 83.2
[ Mon Jul  8 02:44:48 2024 ] 	Batch(4000/7879) done. Loss: 0.9495  lr:0.010000
[ Mon Jul  8 02:45:12 2024 ] 	Batch(4100/7879) done. Loss: 0.2546  lr:0.010000
[ Mon Jul  8 02:45:36 2024 ] 	Batch(4200/7879) done. Loss: 0.2362  lr:0.010000
[ Mon Jul  8 02:46:00 2024 ] 	Batch(4300/7879) done. Loss: 0.3154  lr:0.010000
[ Mon Jul  8 02:46:24 2024 ] 	Batch(4400/7879) done. Loss: 0.2669  lr:0.010000
[ Mon Jul  8 02:46:47 2024 ] 
Training: Epoch [31/120], Step [4499], Loss: 0.5438300371170044, Training Accuracy: 83.07222222222222
[ Mon Jul  8 02:46:48 2024 ] 	Batch(4500/7879) done. Loss: 1.0773  lr:0.010000
[ Mon Jul  8 02:47:11 2024 ] 	Batch(4600/7879) done. Loss: 1.0858  lr:0.010000
[ Mon Jul  8 02:47:34 2024 ] 	Batch(4700/7879) done. Loss: 0.7203  lr:0.010000
[ Mon Jul  8 02:47:58 2024 ] 	Batch(4800/7879) done. Loss: 0.1969  lr:0.010000
[ Mon Jul  8 02:48:21 2024 ] 	Batch(4900/7879) done. Loss: 0.4314  lr:0.010000
[ Mon Jul  8 02:48:43 2024 ] 
Training: Epoch [31/120], Step [4999], Loss: 0.720792293548584, Training Accuracy: 82.94
[ Mon Jul  8 02:48:43 2024 ] 	Batch(5000/7879) done. Loss: 0.4633  lr:0.010000
[ Mon Jul  8 02:49:06 2024 ] 	Batch(5100/7879) done. Loss: 0.5872  lr:0.010000
[ Mon Jul  8 02:49:29 2024 ] 	Batch(5200/7879) done. Loss: 0.1912  lr:0.010000
[ Mon Jul  8 02:49:52 2024 ] 	Batch(5300/7879) done. Loss: 0.3014  lr:0.010000
[ Mon Jul  8 02:50:14 2024 ] 	Batch(5400/7879) done. Loss: 0.6209  lr:0.010000
[ Mon Jul  8 02:50:37 2024 ] 
Training: Epoch [31/120], Step [5499], Loss: 0.5072669982910156, Training Accuracy: 82.91363636363637
[ Mon Jul  8 02:50:37 2024 ] 	Batch(5500/7879) done. Loss: 0.5326  lr:0.010000
[ Mon Jul  8 02:51:00 2024 ] 	Batch(5600/7879) done. Loss: 0.7439  lr:0.010000
[ Mon Jul  8 02:51:23 2024 ] 	Batch(5700/7879) done. Loss: 0.9333  lr:0.010000
[ Mon Jul  8 02:51:47 2024 ] 	Batch(5800/7879) done. Loss: 0.4934  lr:0.010000
[ Mon Jul  8 02:52:10 2024 ] 	Batch(5900/7879) done. Loss: 0.0727  lr:0.010000
[ Mon Jul  8 02:52:33 2024 ] 
Training: Epoch [31/120], Step [5999], Loss: 0.7275533080101013, Training Accuracy: 82.87916666666668
[ Mon Jul  8 02:52:34 2024 ] 	Batch(6000/7879) done. Loss: 0.5130  lr:0.010000
[ Mon Jul  8 02:52:57 2024 ] 	Batch(6100/7879) done. Loss: 0.4569  lr:0.010000
[ Mon Jul  8 02:53:20 2024 ] 	Batch(6200/7879) done. Loss: 0.2930  lr:0.010000
[ Mon Jul  8 02:53:43 2024 ] 	Batch(6300/7879) done. Loss: 0.1022  lr:0.010000
[ Mon Jul  8 02:54:06 2024 ] 	Batch(6400/7879) done. Loss: 0.4834  lr:0.010000
[ Mon Jul  8 02:54:28 2024 ] 
Training: Epoch [31/120], Step [6499], Loss: 0.14760752022266388, Training Accuracy: 82.86346153846154
[ Mon Jul  8 02:54:29 2024 ] 	Batch(6500/7879) done. Loss: 0.3739  lr:0.010000
[ Mon Jul  8 02:54:51 2024 ] 	Batch(6600/7879) done. Loss: 1.4584  lr:0.010000
[ Mon Jul  8 02:55:14 2024 ] 	Batch(6700/7879) done. Loss: 0.6996  lr:0.010000
[ Mon Jul  8 02:55:37 2024 ] 	Batch(6800/7879) done. Loss: 0.4466  lr:0.010000
[ Mon Jul  8 02:56:00 2024 ] 	Batch(6900/7879) done. Loss: 0.3509  lr:0.010000
[ Mon Jul  8 02:56:22 2024 ] 
Training: Epoch [31/120], Step [6999], Loss: 0.2072242945432663, Training Accuracy: 82.81071428571428
[ Mon Jul  8 02:56:22 2024 ] 	Batch(7000/7879) done. Loss: 0.7063  lr:0.010000
[ Mon Jul  8 02:56:45 2024 ] 	Batch(7100/7879) done. Loss: 0.1586  lr:0.010000
[ Mon Jul  8 02:57:08 2024 ] 	Batch(7200/7879) done. Loss: 0.4137  lr:0.010000
[ Mon Jul  8 02:57:31 2024 ] 	Batch(7300/7879) done. Loss: 0.2253  lr:0.010000
[ Mon Jul  8 02:57:53 2024 ] 	Batch(7400/7879) done. Loss: 0.4691  lr:0.010000
[ Mon Jul  8 02:58:16 2024 ] 
Training: Epoch [31/120], Step [7499], Loss: 0.8538213968276978, Training Accuracy: 82.77666666666667
[ Mon Jul  8 02:58:16 2024 ] 	Batch(7500/7879) done. Loss: 0.2603  lr:0.010000
[ Mon Jul  8 02:58:39 2024 ] 	Batch(7600/7879) done. Loss: 0.6520  lr:0.010000
[ Mon Jul  8 02:59:03 2024 ] 	Batch(7700/7879) done. Loss: 0.6403  lr:0.010000
[ Mon Jul  8 02:59:26 2024 ] 	Batch(7800/7879) done. Loss: 0.9513  lr:0.010000
[ Mon Jul  8 02:59:44 2024 ] 	Mean training loss: 0.5695.
[ Mon Jul  8 02:59:44 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 02:59:45 2024 ] Training epoch: 33
[ Mon Jul  8 02:59:45 2024 ] 	Batch(0/7879) done. Loss: 0.6135  lr:0.010000
[ Mon Jul  8 03:00:08 2024 ] 	Batch(100/7879) done. Loss: 0.2325  lr:0.010000
[ Mon Jul  8 03:00:30 2024 ] 	Batch(200/7879) done. Loss: 0.4998  lr:0.010000
[ Mon Jul  8 03:00:53 2024 ] 	Batch(300/7879) done. Loss: 0.5845  lr:0.010000
[ Mon Jul  8 03:01:16 2024 ] 	Batch(400/7879) done. Loss: 0.0355  lr:0.010000
[ Mon Jul  8 03:01:38 2024 ] 
Training: Epoch [32/120], Step [499], Loss: 0.0676472932100296, Training Accuracy: 84.82499999999999
[ Mon Jul  8 03:01:39 2024 ] 	Batch(500/7879) done. Loss: 1.7367  lr:0.010000
[ Mon Jul  8 03:02:01 2024 ] 	Batch(600/7879) done. Loss: 0.7476  lr:0.010000
[ Mon Jul  8 03:02:24 2024 ] 	Batch(700/7879) done. Loss: 0.7293  lr:0.010000
[ Mon Jul  8 03:02:47 2024 ] 	Batch(800/7879) done. Loss: 1.4392  lr:0.010000
[ Mon Jul  8 03:03:10 2024 ] 	Batch(900/7879) done. Loss: 1.0954  lr:0.010000
[ Mon Jul  8 03:03:32 2024 ] 
Training: Epoch [32/120], Step [999], Loss: 0.3362829089164734, Training Accuracy: 84.48750000000001
[ Mon Jul  8 03:03:32 2024 ] 	Batch(1000/7879) done. Loss: 0.6203  lr:0.010000
[ Mon Jul  8 03:03:55 2024 ] 	Batch(1100/7879) done. Loss: 0.3212  lr:0.010000
[ Mon Jul  8 03:04:18 2024 ] 	Batch(1200/7879) done. Loss: 0.0861  lr:0.010000
[ Mon Jul  8 03:04:40 2024 ] 	Batch(1300/7879) done. Loss: 0.3566  lr:0.010000
[ Mon Jul  8 03:05:03 2024 ] 	Batch(1400/7879) done. Loss: 0.2736  lr:0.010000
[ Mon Jul  8 03:05:26 2024 ] 
Training: Epoch [32/120], Step [1499], Loss: 0.29719874262809753, Training Accuracy: 84.46666666666667
[ Mon Jul  8 03:05:26 2024 ] 	Batch(1500/7879) done. Loss: 0.6667  lr:0.010000
[ Mon Jul  8 03:05:49 2024 ] 	Batch(1600/7879) done. Loss: 1.8305  lr:0.010000
[ Mon Jul  8 03:06:11 2024 ] 	Batch(1700/7879) done. Loss: 0.2639  lr:0.010000
[ Mon Jul  8 03:06:34 2024 ] 	Batch(1800/7879) done. Loss: 0.6305  lr:0.010000
[ Mon Jul  8 03:06:57 2024 ] 	Batch(1900/7879) done. Loss: 0.4621  lr:0.010000
[ Mon Jul  8 03:07:19 2024 ] 
Training: Epoch [32/120], Step [1999], Loss: 0.33846789598464966, Training Accuracy: 84.3125
[ Mon Jul  8 03:07:20 2024 ] 	Batch(2000/7879) done. Loss: 0.3140  lr:0.010000
[ Mon Jul  8 03:07:42 2024 ] 	Batch(2100/7879) done. Loss: 0.6459  lr:0.010000
[ Mon Jul  8 03:08:05 2024 ] 	Batch(2200/7879) done. Loss: 0.0678  lr:0.010000
[ Mon Jul  8 03:08:28 2024 ] 	Batch(2300/7879) done. Loss: 0.5045  lr:0.010000
[ Mon Jul  8 03:08:51 2024 ] 	Batch(2400/7879) done. Loss: 0.1442  lr:0.010000
[ Mon Jul  8 03:09:14 2024 ] 
Training: Epoch [32/120], Step [2499], Loss: 0.3976026177406311, Training Accuracy: 84.25
[ Mon Jul  8 03:09:14 2024 ] 	Batch(2500/7879) done. Loss: 0.0110  lr:0.010000
[ Mon Jul  8 03:09:38 2024 ] 	Batch(2600/7879) done. Loss: 1.0151  lr:0.010000
[ Mon Jul  8 03:10:01 2024 ] 	Batch(2700/7879) done. Loss: 0.8009  lr:0.010000
[ Mon Jul  8 03:10:25 2024 ] 	Batch(2800/7879) done. Loss: 1.0469  lr:0.010000
[ Mon Jul  8 03:10:48 2024 ] 	Batch(2900/7879) done. Loss: 0.7212  lr:0.010000
[ Mon Jul  8 03:11:12 2024 ] 
Training: Epoch [32/120], Step [2999], Loss: 0.6102137565612793, Training Accuracy: 83.92916666666666
[ Mon Jul  8 03:11:12 2024 ] 	Batch(3000/7879) done. Loss: 1.3286  lr:0.010000
[ Mon Jul  8 03:11:35 2024 ] 	Batch(3100/7879) done. Loss: 0.1739  lr:0.010000
[ Mon Jul  8 03:11:59 2024 ] 	Batch(3200/7879) done. Loss: 0.4938  lr:0.010000
[ Mon Jul  8 03:12:22 2024 ] 	Batch(3300/7879) done. Loss: 0.4130  lr:0.010000
[ Mon Jul  8 03:12:46 2024 ] 	Batch(3400/7879) done. Loss: 0.2572  lr:0.010000
[ Mon Jul  8 03:13:09 2024 ] 
Training: Epoch [32/120], Step [3499], Loss: 0.6976315379142761, Training Accuracy: 83.78214285714286
[ Mon Jul  8 03:13:09 2024 ] 	Batch(3500/7879) done. Loss: 0.3834  lr:0.010000
[ Mon Jul  8 03:13:33 2024 ] 	Batch(3600/7879) done. Loss: 0.7766  lr:0.010000
[ Mon Jul  8 03:13:56 2024 ] 	Batch(3700/7879) done. Loss: 0.7268  lr:0.010000
[ Mon Jul  8 03:14:20 2024 ] 	Batch(3800/7879) done. Loss: 0.9288  lr:0.010000
[ Mon Jul  8 03:14:43 2024 ] 	Batch(3900/7879) done. Loss: 1.3830  lr:0.010000
[ Mon Jul  8 03:15:06 2024 ] 
Training: Epoch [32/120], Step [3999], Loss: 0.5876739025115967, Training Accuracy: 83.6125
[ Mon Jul  8 03:15:07 2024 ] 	Batch(4000/7879) done. Loss: 0.3764  lr:0.010000
[ Mon Jul  8 03:15:29 2024 ] 	Batch(4100/7879) done. Loss: 0.2232  lr:0.010000
[ Mon Jul  8 03:15:52 2024 ] 	Batch(4200/7879) done. Loss: 1.3946  lr:0.010000
[ Mon Jul  8 03:16:15 2024 ] 	Batch(4300/7879) done. Loss: 1.0873  lr:0.010000
[ Mon Jul  8 03:16:38 2024 ] 	Batch(4400/7879) done. Loss: 0.1303  lr:0.010000
[ Mon Jul  8 03:17:00 2024 ] 
Training: Epoch [32/120], Step [4499], Loss: 0.7180294990539551, Training Accuracy: 83.62222222222222
[ Mon Jul  8 03:17:00 2024 ] 	Batch(4500/7879) done. Loss: 0.3688  lr:0.010000
[ Mon Jul  8 03:17:23 2024 ] 	Batch(4600/7879) done. Loss: 0.6941  lr:0.010000
[ Mon Jul  8 03:17:46 2024 ] 	Batch(4700/7879) done. Loss: 0.0538  lr:0.010000
[ Mon Jul  8 03:18:09 2024 ] 	Batch(4800/7879) done. Loss: 0.2471  lr:0.010000
[ Mon Jul  8 03:18:31 2024 ] 	Batch(4900/7879) done. Loss: 0.3904  lr:0.010000
[ Mon Jul  8 03:18:54 2024 ] 
Training: Epoch [32/120], Step [4999], Loss: 1.0996967554092407, Training Accuracy: 83.5325
[ Mon Jul  8 03:18:54 2024 ] 	Batch(5000/7879) done. Loss: 0.4871  lr:0.010000
[ Mon Jul  8 03:19:17 2024 ] 	Batch(5100/7879) done. Loss: 0.3062  lr:0.010000
[ Mon Jul  8 03:19:40 2024 ] 	Batch(5200/7879) done. Loss: 0.6680  lr:0.010000
[ Mon Jul  8 03:20:02 2024 ] 	Batch(5300/7879) done. Loss: 0.6214  lr:0.010000
[ Mon Jul  8 03:20:25 2024 ] 	Batch(5400/7879) done. Loss: 0.4176  lr:0.010000
[ Mon Jul  8 03:20:48 2024 ] 
Training: Epoch [32/120], Step [5499], Loss: 0.39618566632270813, Training Accuracy: 83.60227272727273
[ Mon Jul  8 03:20:48 2024 ] 	Batch(5500/7879) done. Loss: 0.3868  lr:0.010000
[ Mon Jul  8 03:21:11 2024 ] 	Batch(5600/7879) done. Loss: 0.1893  lr:0.010000
[ Mon Jul  8 03:21:33 2024 ] 	Batch(5700/7879) done. Loss: 0.5803  lr:0.010000
[ Mon Jul  8 03:21:56 2024 ] 	Batch(5800/7879) done. Loss: 0.4010  lr:0.010000
[ Mon Jul  8 03:22:19 2024 ] 	Batch(5900/7879) done. Loss: 1.1368  lr:0.010000
[ Mon Jul  8 03:22:41 2024 ] 
Training: Epoch [32/120], Step [5999], Loss: 0.7876769304275513, Training Accuracy: 83.66458333333333
[ Mon Jul  8 03:22:42 2024 ] 	Batch(6000/7879) done. Loss: 0.6067  lr:0.010000
[ Mon Jul  8 03:23:05 2024 ] 	Batch(6100/7879) done. Loss: 0.8476  lr:0.010000
[ Mon Jul  8 03:23:29 2024 ] 	Batch(6200/7879) done. Loss: 0.1247  lr:0.010000
[ Mon Jul  8 03:23:52 2024 ] 	Batch(6300/7879) done. Loss: 0.0169  lr:0.010000
[ Mon Jul  8 03:24:16 2024 ] 	Batch(6400/7879) done. Loss: 1.8309  lr:0.010000
[ Mon Jul  8 03:24:39 2024 ] 
Training: Epoch [32/120], Step [6499], Loss: 0.1574678122997284, Training Accuracy: 83.61538461538461
[ Mon Jul  8 03:24:39 2024 ] 	Batch(6500/7879) done. Loss: 1.6455  lr:0.010000
[ Mon Jul  8 03:25:02 2024 ] 	Batch(6600/7879) done. Loss: 0.3523  lr:0.010000
[ Mon Jul  8 03:25:25 2024 ] 	Batch(6700/7879) done. Loss: 0.2285  lr:0.010000
[ Mon Jul  8 03:25:48 2024 ] 	Batch(6800/7879) done. Loss: 0.3853  lr:0.010000
[ Mon Jul  8 03:26:11 2024 ] 	Batch(6900/7879) done. Loss: 0.3503  lr:0.010000
[ Mon Jul  8 03:26:33 2024 ] 
Training: Epoch [32/120], Step [6999], Loss: 0.3229280114173889, Training Accuracy: 83.58035714285714
[ Mon Jul  8 03:26:33 2024 ] 	Batch(7000/7879) done. Loss: 0.1197  lr:0.010000
[ Mon Jul  8 03:26:56 2024 ] 	Batch(7100/7879) done. Loss: 0.2765  lr:0.010000
[ Mon Jul  8 03:27:19 2024 ] 	Batch(7200/7879) done. Loss: 0.6434  lr:0.010000
[ Mon Jul  8 03:27:42 2024 ] 	Batch(7300/7879) done. Loss: 0.6835  lr:0.010000
[ Mon Jul  8 03:28:06 2024 ] 	Batch(7400/7879) done. Loss: 0.2519  lr:0.010000
[ Mon Jul  8 03:28:29 2024 ] 
Training: Epoch [32/120], Step [7499], Loss: 0.4386956989765167, Training Accuracy: 83.555
[ Mon Jul  8 03:28:29 2024 ] 	Batch(7500/7879) done. Loss: 0.3980  lr:0.010000
[ Mon Jul  8 03:28:53 2024 ] 	Batch(7600/7879) done. Loss: 1.5569  lr:0.010000
[ Mon Jul  8 03:29:16 2024 ] 	Batch(7700/7879) done. Loss: 0.2387  lr:0.010000
[ Mon Jul  8 03:29:40 2024 ] 	Batch(7800/7879) done. Loss: 0.8867  lr:0.010000
[ Mon Jul  8 03:29:58 2024 ] 	Mean training loss: 0.5475.
[ Mon Jul  8 03:29:58 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 03:29:58 2024 ] Training epoch: 34
[ Mon Jul  8 03:29:59 2024 ] 	Batch(0/7879) done. Loss: 0.9841  lr:0.010000
[ Mon Jul  8 03:30:22 2024 ] 	Batch(100/7879) done. Loss: 0.7477  lr:0.010000
[ Mon Jul  8 03:30:44 2024 ] 	Batch(200/7879) done. Loss: 0.3077  lr:0.010000
[ Mon Jul  8 03:31:07 2024 ] 	Batch(300/7879) done. Loss: 0.0677  lr:0.010000
[ Mon Jul  8 03:31:30 2024 ] 	Batch(400/7879) done. Loss: 0.3429  lr:0.010000
[ Mon Jul  8 03:31:52 2024 ] 
Training: Epoch [33/120], Step [499], Loss: 1.3656065464019775, Training Accuracy: 84.25
[ Mon Jul  8 03:31:53 2024 ] 	Batch(500/7879) done. Loss: 0.5341  lr:0.010000
[ Mon Jul  8 03:32:15 2024 ] 	Batch(600/7879) done. Loss: 0.5702  lr:0.010000
[ Mon Jul  8 03:32:38 2024 ] 	Batch(700/7879) done. Loss: 0.8578  lr:0.010000
[ Mon Jul  8 03:33:01 2024 ] 	Batch(800/7879) done. Loss: 0.0916  lr:0.010000
[ Mon Jul  8 03:33:23 2024 ] 	Batch(900/7879) done. Loss: 0.6204  lr:0.010000
[ Mon Jul  8 03:33:46 2024 ] 
Training: Epoch [33/120], Step [999], Loss: 0.29086095094680786, Training Accuracy: 84.0375
[ Mon Jul  8 03:33:46 2024 ] 	Batch(1000/7879) done. Loss: 0.1720  lr:0.010000
[ Mon Jul  8 03:34:09 2024 ] 	Batch(1100/7879) done. Loss: 0.2681  lr:0.010000
[ Mon Jul  8 03:34:32 2024 ] 	Batch(1200/7879) done. Loss: 0.7658  lr:0.010000
[ Mon Jul  8 03:34:54 2024 ] 	Batch(1300/7879) done. Loss: 0.2299  lr:0.010000
[ Mon Jul  8 03:35:17 2024 ] 	Batch(1400/7879) done. Loss: 0.9684  lr:0.010000
[ Mon Jul  8 03:35:40 2024 ] 
Training: Epoch [33/120], Step [1499], Loss: 0.09226565808057785, Training Accuracy: 84.025
[ Mon Jul  8 03:35:40 2024 ] 	Batch(1500/7879) done. Loss: 0.8572  lr:0.010000
[ Mon Jul  8 03:36:03 2024 ] 	Batch(1600/7879) done. Loss: 0.3216  lr:0.010000
[ Mon Jul  8 03:36:26 2024 ] 	Batch(1700/7879) done. Loss: 0.3210  lr:0.010000
[ Mon Jul  8 03:36:50 2024 ] 	Batch(1800/7879) done. Loss: 0.7521  lr:0.010000
[ Mon Jul  8 03:37:13 2024 ] 	Batch(1900/7879) done. Loss: 0.6771  lr:0.010000
[ Mon Jul  8 03:37:36 2024 ] 
Training: Epoch [33/120], Step [1999], Loss: 1.3605784177780151, Training Accuracy: 84.0125
[ Mon Jul  8 03:37:36 2024 ] 	Batch(2000/7879) done. Loss: 1.1238  lr:0.010000
[ Mon Jul  8 03:37:59 2024 ] 	Batch(2100/7879) done. Loss: 1.0259  lr:0.010000
[ Mon Jul  8 03:38:21 2024 ] 	Batch(2200/7879) done. Loss: 0.1437  lr:0.010000
[ Mon Jul  8 03:38:44 2024 ] 	Batch(2300/7879) done. Loss: 0.5022  lr:0.010000
[ Mon Jul  8 03:39:07 2024 ] 	Batch(2400/7879) done. Loss: 0.1170  lr:0.010000
[ Mon Jul  8 03:39:30 2024 ] 
Training: Epoch [33/120], Step [2499], Loss: 0.40852826833724976, Training Accuracy: 84.065
[ Mon Jul  8 03:39:30 2024 ] 	Batch(2500/7879) done. Loss: 0.5492  lr:0.010000
[ Mon Jul  8 03:39:53 2024 ] 	Batch(2600/7879) done. Loss: 0.1233  lr:0.010000
[ Mon Jul  8 03:40:15 2024 ] 	Batch(2700/7879) done. Loss: 1.0539  lr:0.010000
[ Mon Jul  8 03:40:38 2024 ] 	Batch(2800/7879) done. Loss: 0.4366  lr:0.010000
[ Mon Jul  8 03:41:01 2024 ] 	Batch(2900/7879) done. Loss: 0.2761  lr:0.010000
[ Mon Jul  8 03:41:23 2024 ] 
Training: Epoch [33/120], Step [2999], Loss: 0.17870362102985382, Training Accuracy: 84.0
[ Mon Jul  8 03:41:23 2024 ] 	Batch(3000/7879) done. Loss: 1.0092  lr:0.010000
[ Mon Jul  8 03:41:46 2024 ] 	Batch(3100/7879) done. Loss: 0.3928  lr:0.010000
[ Mon Jul  8 03:42:09 2024 ] 	Batch(3200/7879) done. Loss: 0.0484  lr:0.010000
[ Mon Jul  8 03:42:33 2024 ] 	Batch(3300/7879) done. Loss: 0.6159  lr:0.010000
[ Mon Jul  8 03:42:56 2024 ] 	Batch(3400/7879) done. Loss: 0.4080  lr:0.010000
[ Mon Jul  8 03:43:19 2024 ] 
Training: Epoch [33/120], Step [3499], Loss: 0.320667564868927, Training Accuracy: 84.05
[ Mon Jul  8 03:43:20 2024 ] 	Batch(3500/7879) done. Loss: 2.4404  lr:0.010000
[ Mon Jul  8 03:43:43 2024 ] 	Batch(3600/7879) done. Loss: 0.8900  lr:0.010000
[ Mon Jul  8 03:44:07 2024 ] 	Batch(3700/7879) done. Loss: 0.3171  lr:0.010000
[ Mon Jul  8 03:44:29 2024 ] 	Batch(3800/7879) done. Loss: 0.8482  lr:0.010000
[ Mon Jul  8 03:44:52 2024 ] 	Batch(3900/7879) done. Loss: 0.3010  lr:0.010000
[ Mon Jul  8 03:45:14 2024 ] 
Training: Epoch [33/120], Step [3999], Loss: 0.35773009061813354, Training Accuracy: 83.85625
[ Mon Jul  8 03:45:15 2024 ] 	Batch(4000/7879) done. Loss: 0.6066  lr:0.010000
[ Mon Jul  8 03:45:38 2024 ] 	Batch(4100/7879) done. Loss: 0.2423  lr:0.010000
[ Mon Jul  8 03:46:00 2024 ] 	Batch(4200/7879) done. Loss: 1.1188  lr:0.010000
[ Mon Jul  8 03:46:23 2024 ] 	Batch(4300/7879) done. Loss: 0.8050  lr:0.010000
[ Mon Jul  8 03:46:46 2024 ] 	Batch(4400/7879) done. Loss: 0.6504  lr:0.010000
[ Mon Jul  8 03:47:08 2024 ] 
Training: Epoch [33/120], Step [4499], Loss: 0.14043760299682617, Training Accuracy: 83.775
[ Mon Jul  8 03:47:09 2024 ] 	Batch(4500/7879) done. Loss: 0.2280  lr:0.010000
[ Mon Jul  8 03:47:31 2024 ] 	Batch(4600/7879) done. Loss: 0.1584  lr:0.010000
[ Mon Jul  8 03:47:54 2024 ] 	Batch(4700/7879) done. Loss: 0.1924  lr:0.010000
[ Mon Jul  8 03:48:17 2024 ] 	Batch(4800/7879) done. Loss: 0.0784  lr:0.010000
[ Mon Jul  8 03:48:40 2024 ] 	Batch(4900/7879) done. Loss: 0.3138  lr:0.010000
[ Mon Jul  8 03:49:02 2024 ] 
Training: Epoch [33/120], Step [4999], Loss: 0.2968021631240845, Training Accuracy: 83.695
[ Mon Jul  8 03:49:02 2024 ] 	Batch(5000/7879) done. Loss: 0.4558  lr:0.010000
[ Mon Jul  8 03:49:25 2024 ] 	Batch(5100/7879) done. Loss: 0.5873  lr:0.010000
[ Mon Jul  8 03:49:48 2024 ] 	Batch(5200/7879) done. Loss: 0.5293  lr:0.010000
[ Mon Jul  8 03:50:11 2024 ] 	Batch(5300/7879) done. Loss: 0.9091  lr:0.010000
[ Mon Jul  8 03:50:33 2024 ] 	Batch(5400/7879) done. Loss: 0.1242  lr:0.010000
[ Mon Jul  8 03:50:56 2024 ] 
Training: Epoch [33/120], Step [5499], Loss: 0.942872941493988, Training Accuracy: 83.61590909090908
[ Mon Jul  8 03:50:56 2024 ] 	Batch(5500/7879) done. Loss: 1.0986  lr:0.010000
[ Mon Jul  8 03:51:19 2024 ] 	Batch(5600/7879) done. Loss: 0.3298  lr:0.010000
[ Mon Jul  8 03:51:41 2024 ] 	Batch(5700/7879) done. Loss: 0.7002  lr:0.010000
[ Mon Jul  8 03:52:04 2024 ] 	Batch(5800/7879) done. Loss: 0.0756  lr:0.010000
[ Mon Jul  8 03:52:27 2024 ] 	Batch(5900/7879) done. Loss: 0.8250  lr:0.010000
[ Mon Jul  8 03:52:49 2024 ] 
Training: Epoch [33/120], Step [5999], Loss: 0.11762158572673798, Training Accuracy: 83.64791666666666
[ Mon Jul  8 03:52:50 2024 ] 	Batch(6000/7879) done. Loss: 0.4267  lr:0.010000
[ Mon Jul  8 03:53:12 2024 ] 	Batch(6100/7879) done. Loss: 0.4743  lr:0.010000
[ Mon Jul  8 03:53:35 2024 ] 	Batch(6200/7879) done. Loss: 0.1973  lr:0.010000
[ Mon Jul  8 03:53:58 2024 ] 	Batch(6300/7879) done. Loss: 0.1600  lr:0.010000
[ Mon Jul  8 03:54:21 2024 ] 	Batch(6400/7879) done. Loss: 0.3304  lr:0.010000
[ Mon Jul  8 03:54:43 2024 ] 
Training: Epoch [33/120], Step [6499], Loss: 0.4890402853488922, Training Accuracy: 83.56153846153846
[ Mon Jul  8 03:54:43 2024 ] 	Batch(6500/7879) done. Loss: 0.3567  lr:0.010000
[ Mon Jul  8 03:55:06 2024 ] 	Batch(6600/7879) done. Loss: 0.3673  lr:0.010000
[ Mon Jul  8 03:55:29 2024 ] 	Batch(6700/7879) done. Loss: 0.5932  lr:0.010000
[ Mon Jul  8 03:55:52 2024 ] 	Batch(6800/7879) done. Loss: 0.4032  lr:0.010000
[ Mon Jul  8 03:56:14 2024 ] 	Batch(6900/7879) done. Loss: 0.1033  lr:0.010000
[ Mon Jul  8 03:56:37 2024 ] 
Training: Epoch [33/120], Step [6999], Loss: 0.3176005482673645, Training Accuracy: 83.49642857142857
[ Mon Jul  8 03:56:37 2024 ] 	Batch(7000/7879) done. Loss: 0.1937  lr:0.010000
[ Mon Jul  8 03:57:00 2024 ] 	Batch(7100/7879) done. Loss: 0.3262  lr:0.010000
[ Mon Jul  8 03:57:23 2024 ] 	Batch(7200/7879) done. Loss: 0.3274  lr:0.010000
[ Mon Jul  8 03:57:45 2024 ] 	Batch(7300/7879) done. Loss: 0.1834  lr:0.010000
[ Mon Jul  8 03:58:08 2024 ] 	Batch(7400/7879) done. Loss: 0.4285  lr:0.010000
[ Mon Jul  8 03:58:30 2024 ] 
Training: Epoch [33/120], Step [7499], Loss: 0.38528522849082947, Training Accuracy: 83.5
[ Mon Jul  8 03:58:31 2024 ] 	Batch(7500/7879) done. Loss: 0.1541  lr:0.010000
[ Mon Jul  8 03:58:53 2024 ] 	Batch(7600/7879) done. Loss: 0.2089  lr:0.010000
[ Mon Jul  8 03:59:16 2024 ] 	Batch(7700/7879) done. Loss: 0.6114  lr:0.010000
[ Mon Jul  8 03:59:39 2024 ] 	Batch(7800/7879) done. Loss: 0.5089  lr:0.010000
[ Mon Jul  8 03:59:57 2024 ] 	Mean training loss: 0.5437.
[ Mon Jul  8 03:59:57 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 03:59:57 2024 ] Training epoch: 35
[ Mon Jul  8 03:59:57 2024 ] 	Batch(0/7879) done. Loss: 0.1321  lr:0.010000
[ Mon Jul  8 04:00:20 2024 ] 	Batch(100/7879) done. Loss: 1.3853  lr:0.010000
[ Mon Jul  8 04:00:43 2024 ] 	Batch(200/7879) done. Loss: 0.2112  lr:0.010000
[ Mon Jul  8 04:01:05 2024 ] 	Batch(300/7879) done. Loss: 0.4328  lr:0.010000
[ Mon Jul  8 04:01:28 2024 ] 	Batch(400/7879) done. Loss: 1.0518  lr:0.010000
[ Mon Jul  8 04:01:51 2024 ] 
Training: Epoch [34/120], Step [499], Loss: 1.136655330657959, Training Accuracy: 83.6
[ Mon Jul  8 04:01:51 2024 ] 	Batch(500/7879) done. Loss: 0.2287  lr:0.010000
[ Mon Jul  8 04:02:14 2024 ] 	Batch(600/7879) done. Loss: 1.6373  lr:0.010000
[ Mon Jul  8 04:02:36 2024 ] 	Batch(700/7879) done. Loss: 0.3887  lr:0.010000
[ Mon Jul  8 04:02:59 2024 ] 	Batch(800/7879) done. Loss: 0.2118  lr:0.010000
[ Mon Jul  8 04:03:22 2024 ] 	Batch(900/7879) done. Loss: 0.2769  lr:0.010000
[ Mon Jul  8 04:03:45 2024 ] 
Training: Epoch [34/120], Step [999], Loss: 0.051788732409477234, Training Accuracy: 83.96249999999999
[ Mon Jul  8 04:03:45 2024 ] 	Batch(1000/7879) done. Loss: 0.3092  lr:0.010000
[ Mon Jul  8 04:04:07 2024 ] 	Batch(1100/7879) done. Loss: 0.0203  lr:0.010000
[ Mon Jul  8 04:04:30 2024 ] 	Batch(1200/7879) done. Loss: 0.3501  lr:0.010000
[ Mon Jul  8 04:04:53 2024 ] 	Batch(1300/7879) done. Loss: 0.4191  lr:0.010000
[ Mon Jul  8 04:05:16 2024 ] 	Batch(1400/7879) done. Loss: 0.2236  lr:0.010000
[ Mon Jul  8 04:05:38 2024 ] 
Training: Epoch [34/120], Step [1499], Loss: 0.26728805899620056, Training Accuracy: 84.20833333333333
[ Mon Jul  8 04:05:39 2024 ] 	Batch(1500/7879) done. Loss: 1.0146  lr:0.010000
[ Mon Jul  8 04:06:01 2024 ] 	Batch(1600/7879) done. Loss: 0.7838  lr:0.010000
[ Mon Jul  8 04:06:24 2024 ] 	Batch(1700/7879) done. Loss: 0.1150  lr:0.010000
[ Mon Jul  8 04:06:47 2024 ] 	Batch(1800/7879) done. Loss: 0.2441  lr:0.010000
[ Mon Jul  8 04:07:10 2024 ] 	Batch(1900/7879) done. Loss: 0.4970  lr:0.010000
[ Mon Jul  8 04:07:32 2024 ] 
Training: Epoch [34/120], Step [1999], Loss: 0.4951086640357971, Training Accuracy: 84.125
[ Mon Jul  8 04:07:33 2024 ] 	Batch(2000/7879) done. Loss: 1.3912  lr:0.010000
[ Mon Jul  8 04:07:55 2024 ] 	Batch(2100/7879) done. Loss: 0.5827  lr:0.010000
[ Mon Jul  8 04:08:18 2024 ] 	Batch(2200/7879) done. Loss: 1.0925  lr:0.010000
[ Mon Jul  8 04:08:41 2024 ] 	Batch(2300/7879) done. Loss: 0.6566  lr:0.010000
[ Mon Jul  8 04:09:04 2024 ] 	Batch(2400/7879) done. Loss: 0.4798  lr:0.010000
[ Mon Jul  8 04:09:26 2024 ] 
Training: Epoch [34/120], Step [2499], Loss: 0.5028092861175537, Training Accuracy: 84.21499999999999
[ Mon Jul  8 04:09:26 2024 ] 	Batch(2500/7879) done. Loss: 0.3926  lr:0.010000
[ Mon Jul  8 04:09:49 2024 ] 	Batch(2600/7879) done. Loss: 0.6510  lr:0.010000
[ Mon Jul  8 04:10:12 2024 ] 	Batch(2700/7879) done. Loss: 0.2721  lr:0.010000
[ Mon Jul  8 04:10:34 2024 ] 	Batch(2800/7879) done. Loss: 0.4676  lr:0.010000
[ Mon Jul  8 04:10:57 2024 ] 	Batch(2900/7879) done. Loss: 0.3280  lr:0.010000
[ Mon Jul  8 04:11:20 2024 ] 
Training: Epoch [34/120], Step [2999], Loss: 0.1682356745004654, Training Accuracy: 84.15
[ Mon Jul  8 04:11:20 2024 ] 	Batch(3000/7879) done. Loss: 0.1962  lr:0.010000
[ Mon Jul  8 04:11:42 2024 ] 	Batch(3100/7879) done. Loss: 0.0612  lr:0.010000
[ Mon Jul  8 04:12:05 2024 ] 	Batch(3200/7879) done. Loss: 0.3274  lr:0.010000
[ Mon Jul  8 04:12:28 2024 ] 	Batch(3300/7879) done. Loss: 0.6091  lr:0.010000
[ Mon Jul  8 04:12:51 2024 ] 	Batch(3400/7879) done. Loss: 1.7030  lr:0.010000
[ Mon Jul  8 04:13:13 2024 ] 
Training: Epoch [34/120], Step [3499], Loss: 0.6746785044670105, Training Accuracy: 84.17142857142858
[ Mon Jul  8 04:13:13 2024 ] 	Batch(3500/7879) done. Loss: 0.3722  lr:0.010000
[ Mon Jul  8 04:13:36 2024 ] 	Batch(3600/7879) done. Loss: 1.3095  lr:0.010000
[ Mon Jul  8 04:13:59 2024 ] 	Batch(3700/7879) done. Loss: 1.1796  lr:0.010000
[ Mon Jul  8 04:14:22 2024 ] 	Batch(3800/7879) done. Loss: 0.4301  lr:0.010000
[ Mon Jul  8 04:14:44 2024 ] 	Batch(3900/7879) done. Loss: 0.3703  lr:0.010000
[ Mon Jul  8 04:15:07 2024 ] 
Training: Epoch [34/120], Step [3999], Loss: 0.07733241468667984, Training Accuracy: 84.196875
[ Mon Jul  8 04:15:07 2024 ] 	Batch(4000/7879) done. Loss: 0.3918  lr:0.010000
[ Mon Jul  8 04:15:30 2024 ] 	Batch(4100/7879) done. Loss: 0.7808  lr:0.010000
[ Mon Jul  8 04:15:53 2024 ] 	Batch(4200/7879) done. Loss: 0.7685  lr:0.010000
[ Mon Jul  8 04:16:15 2024 ] 	Batch(4300/7879) done. Loss: 0.5327  lr:0.010000
[ Mon Jul  8 04:16:38 2024 ] 	Batch(4400/7879) done. Loss: 0.1445  lr:0.010000
[ Mon Jul  8 04:17:01 2024 ] 
Training: Epoch [34/120], Step [4499], Loss: 1.438498854637146, Training Accuracy: 84.14722222222221
[ Mon Jul  8 04:17:01 2024 ] 	Batch(4500/7879) done. Loss: 0.4807  lr:0.010000
[ Mon Jul  8 04:17:24 2024 ] 	Batch(4600/7879) done. Loss: 0.9154  lr:0.010000
[ Mon Jul  8 04:17:46 2024 ] 	Batch(4700/7879) done. Loss: 0.3045  lr:0.010000
[ Mon Jul  8 04:18:09 2024 ] 	Batch(4800/7879) done. Loss: 0.4262  lr:0.010000
[ Mon Jul  8 04:18:32 2024 ] 	Batch(4900/7879) done. Loss: 0.5259  lr:0.010000
[ Mon Jul  8 04:18:55 2024 ] 
Training: Epoch [34/120], Step [4999], Loss: 0.35095468163490295, Training Accuracy: 84.07249999999999
[ Mon Jul  8 04:18:55 2024 ] 	Batch(5000/7879) done. Loss: 0.3922  lr:0.010000
[ Mon Jul  8 04:19:18 2024 ] 	Batch(5100/7879) done. Loss: 0.3230  lr:0.010000
[ Mon Jul  8 04:19:40 2024 ] 	Batch(5200/7879) done. Loss: 0.6615  lr:0.010000
[ Mon Jul  8 04:20:03 2024 ] 	Batch(5300/7879) done. Loss: 0.3046  lr:0.010000
[ Mon Jul  8 04:20:26 2024 ] 	Batch(5400/7879) done. Loss: 1.0949  lr:0.010000
[ Mon Jul  8 04:20:48 2024 ] 
Training: Epoch [34/120], Step [5499], Loss: 0.5628060698509216, Training Accuracy: 83.9909090909091
[ Mon Jul  8 04:20:49 2024 ] 	Batch(5500/7879) done. Loss: 0.2002  lr:0.010000
[ Mon Jul  8 04:21:11 2024 ] 	Batch(5600/7879) done. Loss: 0.4312  lr:0.010000
[ Mon Jul  8 04:21:34 2024 ] 	Batch(5700/7879) done. Loss: 0.2203  lr:0.010000
[ Mon Jul  8 04:21:57 2024 ] 	Batch(5800/7879) done. Loss: 0.2112  lr:0.010000
[ Mon Jul  8 04:22:20 2024 ] 	Batch(5900/7879) done. Loss: 0.8261  lr:0.010000
[ Mon Jul  8 04:22:42 2024 ] 
Training: Epoch [34/120], Step [5999], Loss: 0.6487643718719482, Training Accuracy: 83.89791666666667
[ Mon Jul  8 04:22:43 2024 ] 	Batch(6000/7879) done. Loss: 0.4906  lr:0.010000
[ Mon Jul  8 04:23:05 2024 ] 	Batch(6100/7879) done. Loss: 1.3287  lr:0.010000
[ Mon Jul  8 04:23:28 2024 ] 	Batch(6200/7879) done. Loss: 0.2712  lr:0.010000
[ Mon Jul  8 04:23:51 2024 ] 	Batch(6300/7879) done. Loss: 0.5691  lr:0.010000
[ Mon Jul  8 04:24:14 2024 ] 	Batch(6400/7879) done. Loss: 0.7879  lr:0.010000
[ Mon Jul  8 04:24:37 2024 ] 
Training: Epoch [34/120], Step [6499], Loss: 0.2780623733997345, Training Accuracy: 83.79423076923077
[ Mon Jul  8 04:24:37 2024 ] 	Batch(6500/7879) done. Loss: 0.6091  lr:0.010000
[ Mon Jul  8 04:25:01 2024 ] 	Batch(6600/7879) done. Loss: 0.0813  lr:0.010000
[ Mon Jul  8 04:25:24 2024 ] 	Batch(6700/7879) done. Loss: 1.8416  lr:0.010000
[ Mon Jul  8 04:25:48 2024 ] 	Batch(6800/7879) done. Loss: 0.1856  lr:0.010000
[ Mon Jul  8 04:26:11 2024 ] 	Batch(6900/7879) done. Loss: 0.2587  lr:0.010000
[ Mon Jul  8 04:26:35 2024 ] 
Training: Epoch [34/120], Step [6999], Loss: 0.5365373492240906, Training Accuracy: 83.76964285714286
[ Mon Jul  8 04:26:35 2024 ] 	Batch(7000/7879) done. Loss: 0.6062  lr:0.010000
[ Mon Jul  8 04:26:58 2024 ] 	Batch(7100/7879) done. Loss: 0.3821  lr:0.010000
[ Mon Jul  8 04:27:22 2024 ] 	Batch(7200/7879) done. Loss: 0.0530  lr:0.010000
[ Mon Jul  8 04:27:45 2024 ] 	Batch(7300/7879) done. Loss: 1.8478  lr:0.010000
[ Mon Jul  8 04:28:08 2024 ] 	Batch(7400/7879) done. Loss: 0.1631  lr:0.010000
[ Mon Jul  8 04:28:31 2024 ] 
Training: Epoch [34/120], Step [7499], Loss: 0.7265084981918335, Training Accuracy: 83.81
[ Mon Jul  8 04:28:31 2024 ] 	Batch(7500/7879) done. Loss: 0.4777  lr:0.010000
[ Mon Jul  8 04:28:54 2024 ] 	Batch(7600/7879) done. Loss: 0.2332  lr:0.010000
[ Mon Jul  8 04:29:17 2024 ] 	Batch(7700/7879) done. Loss: 0.3274  lr:0.010000
[ Mon Jul  8 04:29:39 2024 ] 	Batch(7800/7879) done. Loss: 0.6476  lr:0.010000
[ Mon Jul  8 04:29:57 2024 ] 	Mean training loss: 0.5338.
[ Mon Jul  8 04:29:57 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 04:29:57 2024 ] Training epoch: 36
[ Mon Jul  8 04:29:58 2024 ] 	Batch(0/7879) done. Loss: 0.3762  lr:0.010000
[ Mon Jul  8 04:30:21 2024 ] 	Batch(100/7879) done. Loss: 0.5401  lr:0.010000
[ Mon Jul  8 04:30:43 2024 ] 	Batch(200/7879) done. Loss: 0.4703  lr:0.010000
[ Mon Jul  8 04:31:06 2024 ] 	Batch(300/7879) done. Loss: 0.1856  lr:0.010000
[ Mon Jul  8 04:31:29 2024 ] 	Batch(400/7879) done. Loss: 0.7401  lr:0.010000
[ Mon Jul  8 04:31:52 2024 ] 
Training: Epoch [35/120], Step [499], Loss: 0.25653737783432007, Training Accuracy: 84.8
[ Mon Jul  8 04:31:52 2024 ] 	Batch(500/7879) done. Loss: 0.6810  lr:0.010000
[ Mon Jul  8 04:32:14 2024 ] 	Batch(600/7879) done. Loss: 0.6015  lr:0.010000
[ Mon Jul  8 04:32:37 2024 ] 	Batch(700/7879) done. Loss: 0.2649  lr:0.010000
[ Mon Jul  8 04:33:00 2024 ] 	Batch(800/7879) done. Loss: 0.6672  lr:0.010000
[ Mon Jul  8 04:33:23 2024 ] 	Batch(900/7879) done. Loss: 0.2031  lr:0.010000
[ Mon Jul  8 04:33:45 2024 ] 
Training: Epoch [35/120], Step [999], Loss: 0.48544836044311523, Training Accuracy: 84.85000000000001
[ Mon Jul  8 04:33:45 2024 ] 	Batch(1000/7879) done. Loss: 0.2731  lr:0.010000
[ Mon Jul  8 04:34:08 2024 ] 	Batch(1100/7879) done. Loss: 0.4874  lr:0.010000
[ Mon Jul  8 04:34:31 2024 ] 	Batch(1200/7879) done. Loss: 0.1900  lr:0.010000
[ Mon Jul  8 04:34:54 2024 ] 	Batch(1300/7879) done. Loss: 0.5536  lr:0.010000
[ Mon Jul  8 04:35:16 2024 ] 	Batch(1400/7879) done. Loss: 0.3567  lr:0.010000
[ Mon Jul  8 04:35:39 2024 ] 
Training: Epoch [35/120], Step [1499], Loss: 0.11603251099586487, Training Accuracy: 84.48333333333333
[ Mon Jul  8 04:35:39 2024 ] 	Batch(1500/7879) done. Loss: 1.3410  lr:0.010000
[ Mon Jul  8 04:36:02 2024 ] 	Batch(1600/7879) done. Loss: 0.8128  lr:0.010000
[ Mon Jul  8 04:36:25 2024 ] 	Batch(1700/7879) done. Loss: 0.0569  lr:0.010000
[ Mon Jul  8 04:36:47 2024 ] 	Batch(1800/7879) done. Loss: 0.8856  lr:0.010000
[ Mon Jul  8 04:37:10 2024 ] 	Batch(1900/7879) done. Loss: 0.2535  lr:0.010000
[ Mon Jul  8 04:37:32 2024 ] 
Training: Epoch [35/120], Step [1999], Loss: 0.6893668174743652, Training Accuracy: 84.36875
[ Mon Jul  8 04:37:33 2024 ] 	Batch(2000/7879) done. Loss: 0.0284  lr:0.010000
[ Mon Jul  8 04:37:55 2024 ] 	Batch(2100/7879) done. Loss: 0.3977  lr:0.010000
[ Mon Jul  8 04:38:18 2024 ] 	Batch(2200/7879) done. Loss: 0.4220  lr:0.010000
[ Mon Jul  8 04:38:41 2024 ] 	Batch(2300/7879) done. Loss: 0.2300  lr:0.010000
[ Mon Jul  8 04:39:04 2024 ] 	Batch(2400/7879) done. Loss: 0.8272  lr:0.010000
[ Mon Jul  8 04:39:26 2024 ] 
Training: Epoch [35/120], Step [2499], Loss: 1.0453238487243652, Training Accuracy: 84.365
[ Mon Jul  8 04:39:26 2024 ] 	Batch(2500/7879) done. Loss: 0.2198  lr:0.010000
[ Mon Jul  8 04:39:49 2024 ] 	Batch(2600/7879) done. Loss: 0.9497  lr:0.010000
[ Mon Jul  8 04:40:12 2024 ] 	Batch(2700/7879) done. Loss: 0.8091  lr:0.010000
[ Mon Jul  8 04:40:35 2024 ] 	Batch(2800/7879) done. Loss: 0.2201  lr:0.010000
[ Mon Jul  8 04:40:58 2024 ] 	Batch(2900/7879) done. Loss: 0.4663  lr:0.010000
[ Mon Jul  8 04:41:20 2024 ] 
Training: Epoch [35/120], Step [2999], Loss: 1.3309054374694824, Training Accuracy: 84.4125
[ Mon Jul  8 04:41:20 2024 ] 	Batch(3000/7879) done. Loss: 0.3732  lr:0.010000
[ Mon Jul  8 04:41:43 2024 ] 	Batch(3100/7879) done. Loss: 1.4491  lr:0.010000
[ Mon Jul  8 04:42:06 2024 ] 	Batch(3200/7879) done. Loss: 1.1566  lr:0.010000
[ Mon Jul  8 04:42:29 2024 ] 	Batch(3300/7879) done. Loss: 0.1707  lr:0.010000
[ Mon Jul  8 04:42:51 2024 ] 	Batch(3400/7879) done. Loss: 0.2495  lr:0.010000
[ Mon Jul  8 04:43:14 2024 ] 
Training: Epoch [35/120], Step [3499], Loss: 0.4202486276626587, Training Accuracy: 84.325
[ Mon Jul  8 04:43:14 2024 ] 	Batch(3500/7879) done. Loss: 0.7237  lr:0.010000
[ Mon Jul  8 04:43:37 2024 ] 	Batch(3600/7879) done. Loss: 0.5693  lr:0.010000
[ Mon Jul  8 04:44:00 2024 ] 	Batch(3700/7879) done. Loss: 0.2201  lr:0.010000
[ Mon Jul  8 04:44:22 2024 ] 	Batch(3800/7879) done. Loss: 0.6871  lr:0.010000
[ Mon Jul  8 04:44:45 2024 ] 	Batch(3900/7879) done. Loss: 0.6103  lr:0.010000
[ Mon Jul  8 04:45:08 2024 ] 
Training: Epoch [35/120], Step [3999], Loss: 0.45268481969833374, Training Accuracy: 84.278125
[ Mon Jul  8 04:45:08 2024 ] 	Batch(4000/7879) done. Loss: 0.6022  lr:0.010000
[ Mon Jul  8 04:45:31 2024 ] 	Batch(4100/7879) done. Loss: 1.2158  lr:0.010000
[ Mon Jul  8 04:45:53 2024 ] 	Batch(4200/7879) done. Loss: 0.8956  lr:0.010000
[ Mon Jul  8 04:46:16 2024 ] 	Batch(4300/7879) done. Loss: 0.7881  lr:0.010000
[ Mon Jul  8 04:46:39 2024 ] 	Batch(4400/7879) done. Loss: 0.3398  lr:0.010000
[ Mon Jul  8 04:47:03 2024 ] 
Training: Epoch [35/120], Step [4499], Loss: 0.6301946043968201, Training Accuracy: 84.21111111111111
[ Mon Jul  8 04:47:03 2024 ] 	Batch(4500/7879) done. Loss: 0.3246  lr:0.010000
[ Mon Jul  8 04:47:26 2024 ] 	Batch(4600/7879) done. Loss: 0.8171  lr:0.010000
[ Mon Jul  8 04:47:50 2024 ] 	Batch(4700/7879) done. Loss: 0.6215  lr:0.010000
[ Mon Jul  8 04:48:13 2024 ] 	Batch(4800/7879) done. Loss: 0.1997  lr:0.010000
[ Mon Jul  8 04:48:36 2024 ] 	Batch(4900/7879) done. Loss: 1.3770  lr:0.010000
[ Mon Jul  8 04:48:59 2024 ] 
Training: Epoch [35/120], Step [4999], Loss: 0.02101353369653225, Training Accuracy: 84.1875
[ Mon Jul  8 04:48:59 2024 ] 	Batch(5000/7879) done. Loss: 0.2175  lr:0.010000
[ Mon Jul  8 04:49:22 2024 ] 	Batch(5100/7879) done. Loss: 0.5049  lr:0.010000
[ Mon Jul  8 04:49:45 2024 ] 	Batch(5200/7879) done. Loss: 0.0737  lr:0.010000
[ Mon Jul  8 04:50:07 2024 ] 	Batch(5300/7879) done. Loss: 1.0264  lr:0.010000
[ Mon Jul  8 04:50:30 2024 ] 	Batch(5400/7879) done. Loss: 0.4072  lr:0.010000
[ Mon Jul  8 04:50:53 2024 ] 
Training: Epoch [35/120], Step [5499], Loss: 0.7848595976829529, Training Accuracy: 84.24318181818182
[ Mon Jul  8 04:50:53 2024 ] 	Batch(5500/7879) done. Loss: 0.3068  lr:0.010000
[ Mon Jul  8 04:51:16 2024 ] 	Batch(5600/7879) done. Loss: 0.6915  lr:0.010000
[ Mon Jul  8 04:51:39 2024 ] 	Batch(5700/7879) done. Loss: 0.3488  lr:0.010000
[ Mon Jul  8 04:52:02 2024 ] 	Batch(5800/7879) done. Loss: 0.4335  lr:0.010000
[ Mon Jul  8 04:52:25 2024 ] 	Batch(5900/7879) done. Loss: 0.0652  lr:0.010000
[ Mon Jul  8 04:52:47 2024 ] 
Training: Epoch [35/120], Step [5999], Loss: 0.09745753556489944, Training Accuracy: 84.17708333333334
[ Mon Jul  8 04:52:48 2024 ] 	Batch(6000/7879) done. Loss: 0.2749  lr:0.010000
[ Mon Jul  8 04:53:11 2024 ] 	Batch(6100/7879) done. Loss: 0.8630  lr:0.010000
[ Mon Jul  8 04:53:33 2024 ] 	Batch(6200/7879) done. Loss: 0.2057  lr:0.010000
[ Mon Jul  8 04:53:56 2024 ] 	Batch(6300/7879) done. Loss: 0.6108  lr:0.010000
[ Mon Jul  8 04:54:19 2024 ] 	Batch(6400/7879) done. Loss: 0.4856  lr:0.010000
[ Mon Jul  8 04:54:41 2024 ] 
Training: Epoch [35/120], Step [6499], Loss: 0.3013826608657837, Training Accuracy: 84.09615384615384
[ Mon Jul  8 04:54:41 2024 ] 	Batch(6500/7879) done. Loss: 0.7715  lr:0.010000
[ Mon Jul  8 04:55:04 2024 ] 	Batch(6600/7879) done. Loss: 0.9023  lr:0.010000
[ Mon Jul  8 04:55:27 2024 ] 	Batch(6700/7879) done. Loss: 0.2462  lr:0.010000
[ Mon Jul  8 04:55:50 2024 ] 	Batch(6800/7879) done. Loss: 0.6316  lr:0.010000
[ Mon Jul  8 04:56:12 2024 ] 	Batch(6900/7879) done. Loss: 1.5387  lr:0.010000
[ Mon Jul  8 04:56:35 2024 ] 
Training: Epoch [35/120], Step [6999], Loss: 1.0789583921432495, Training Accuracy: 84.01964285714286
[ Mon Jul  8 04:56:35 2024 ] 	Batch(7000/7879) done. Loss: 0.7856  lr:0.010000
[ Mon Jul  8 04:56:58 2024 ] 	Batch(7100/7879) done. Loss: 0.0628  lr:0.010000
[ Mon Jul  8 04:57:21 2024 ] 	Batch(7200/7879) done. Loss: 0.3693  lr:0.010000
[ Mon Jul  8 04:57:43 2024 ] 	Batch(7300/7879) done. Loss: 0.5652  lr:0.010000
[ Mon Jul  8 04:58:06 2024 ] 	Batch(7400/7879) done. Loss: 0.2270  lr:0.010000
[ Mon Jul  8 04:58:28 2024 ] 
Training: Epoch [35/120], Step [7499], Loss: 0.30023613572120667, Training Accuracy: 83.955
[ Mon Jul  8 04:58:29 2024 ] 	Batch(7500/7879) done. Loss: 0.6565  lr:0.010000
[ Mon Jul  8 04:58:52 2024 ] 	Batch(7600/7879) done. Loss: 0.9035  lr:0.010000
[ Mon Jul  8 04:59:14 2024 ] 	Batch(7700/7879) done. Loss: 0.4997  lr:0.010000
[ Mon Jul  8 04:59:38 2024 ] 	Batch(7800/7879) done. Loss: 0.9087  lr:0.010000
[ Mon Jul  8 04:59:56 2024 ] 	Mean training loss: 0.5261.
[ Mon Jul  8 04:59:56 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 04:59:56 2024 ] Training epoch: 37
[ Mon Jul  8 04:59:57 2024 ] 	Batch(0/7879) done. Loss: 0.7090  lr:0.010000
[ Mon Jul  8 05:00:19 2024 ] 	Batch(100/7879) done. Loss: 0.3595  lr:0.010000
[ Mon Jul  8 05:00:42 2024 ] 	Batch(200/7879) done. Loss: 0.2977  lr:0.010000
[ Mon Jul  8 05:01:05 2024 ] 	Batch(300/7879) done. Loss: 0.7848  lr:0.010000
[ Mon Jul  8 05:01:28 2024 ] 	Batch(400/7879) done. Loss: 0.2713  lr:0.010000
[ Mon Jul  8 05:01:50 2024 ] 
Training: Epoch [36/120], Step [499], Loss: 0.16716605424880981, Training Accuracy: 85.9
[ Mon Jul  8 05:01:50 2024 ] 	Batch(500/7879) done. Loss: 0.1073  lr:0.010000
[ Mon Jul  8 05:02:13 2024 ] 	Batch(600/7879) done. Loss: 0.1250  lr:0.010000
[ Mon Jul  8 05:02:36 2024 ] 	Batch(700/7879) done. Loss: 0.3088  lr:0.010000
[ Mon Jul  8 05:02:58 2024 ] 	Batch(800/7879) done. Loss: 0.5606  lr:0.010000
[ Mon Jul  8 05:03:22 2024 ] 	Batch(900/7879) done. Loss: 0.5118  lr:0.010000
[ Mon Jul  8 05:03:45 2024 ] 
Training: Epoch [36/120], Step [999], Loss: 0.3806552290916443, Training Accuracy: 84.89999999999999
[ Mon Jul  8 05:03:45 2024 ] 	Batch(1000/7879) done. Loss: 0.8900  lr:0.010000
[ Mon Jul  8 05:04:09 2024 ] 	Batch(1100/7879) done. Loss: 0.4817  lr:0.010000
[ Mon Jul  8 05:04:32 2024 ] 	Batch(1200/7879) done. Loss: 0.4541  lr:0.010000
[ Mon Jul  8 05:04:55 2024 ] 	Batch(1300/7879) done. Loss: 0.7154  lr:0.010000
[ Mon Jul  8 05:05:18 2024 ] 	Batch(1400/7879) done. Loss: 0.5154  lr:0.010000
[ Mon Jul  8 05:05:40 2024 ] 
Training: Epoch [36/120], Step [1499], Loss: 0.42091888189315796, Training Accuracy: 84.71666666666667
[ Mon Jul  8 05:05:40 2024 ] 	Batch(1500/7879) done. Loss: 0.4687  lr:0.010000
[ Mon Jul  8 05:06:03 2024 ] 	Batch(1600/7879) done. Loss: 0.3486  lr:0.010000
[ Mon Jul  8 05:06:26 2024 ] 	Batch(1700/7879) done. Loss: 0.6376  lr:0.010000
[ Mon Jul  8 05:06:49 2024 ] 	Batch(1800/7879) done. Loss: 0.0479  lr:0.010000
[ Mon Jul  8 05:07:11 2024 ] 	Batch(1900/7879) done. Loss: 0.5680  lr:0.010000
[ Mon Jul  8 05:07:34 2024 ] 
Training: Epoch [36/120], Step [1999], Loss: 0.5876526832580566, Training Accuracy: 84.625
[ Mon Jul  8 05:07:34 2024 ] 	Batch(2000/7879) done. Loss: 0.2084  lr:0.010000
[ Mon Jul  8 05:07:57 2024 ] 	Batch(2100/7879) done. Loss: 0.1312  lr:0.010000
[ Mon Jul  8 05:08:19 2024 ] 	Batch(2200/7879) done. Loss: 0.0336  lr:0.010000
[ Mon Jul  8 05:08:42 2024 ] 	Batch(2300/7879) done. Loss: 1.6608  lr:0.010000
[ Mon Jul  8 05:09:05 2024 ] 	Batch(2400/7879) done. Loss: 0.1522  lr:0.010000
[ Mon Jul  8 05:09:28 2024 ] 
Training: Epoch [36/120], Step [2499], Loss: 0.8930845260620117, Training Accuracy: 84.455
[ Mon Jul  8 05:09:28 2024 ] 	Batch(2500/7879) done. Loss: 0.2362  lr:0.010000
[ Mon Jul  8 05:09:52 2024 ] 	Batch(2600/7879) done. Loss: 0.4267  lr:0.010000
[ Mon Jul  8 05:10:15 2024 ] 	Batch(2700/7879) done. Loss: 0.1077  lr:0.010000
[ Mon Jul  8 05:10:39 2024 ] 	Batch(2800/7879) done. Loss: 0.0551  lr:0.010000
[ Mon Jul  8 05:11:02 2024 ] 	Batch(2900/7879) done. Loss: 0.4875  lr:0.010000
[ Mon Jul  8 05:11:24 2024 ] 
Training: Epoch [36/120], Step [2999], Loss: 0.5134109258651733, Training Accuracy: 84.29583333333333
[ Mon Jul  8 05:11:24 2024 ] 	Batch(3000/7879) done. Loss: 0.4615  lr:0.010000
[ Mon Jul  8 05:11:47 2024 ] 	Batch(3100/7879) done. Loss: 0.4459  lr:0.010000
[ Mon Jul  8 05:12:11 2024 ] 	Batch(3200/7879) done. Loss: 0.7699  lr:0.010000
[ Mon Jul  8 05:12:34 2024 ] 	Batch(3300/7879) done. Loss: 0.6917  lr:0.010000
[ Mon Jul  8 05:12:58 2024 ] 	Batch(3400/7879) done. Loss: 0.4325  lr:0.010000
[ Mon Jul  8 05:13:21 2024 ] 
Training: Epoch [36/120], Step [3499], Loss: 1.516618251800537, Training Accuracy: 84.26785714285714
[ Mon Jul  8 05:13:21 2024 ] 	Batch(3500/7879) done. Loss: 0.9465  lr:0.010000
[ Mon Jul  8 05:13:45 2024 ] 	Batch(3600/7879) done. Loss: 1.2956  lr:0.010000
[ Mon Jul  8 05:14:08 2024 ] 	Batch(3700/7879) done. Loss: 0.7633  lr:0.010000
[ Mon Jul  8 05:14:30 2024 ] 	Batch(3800/7879) done. Loss: 0.6858  lr:0.010000
[ Mon Jul  8 05:14:53 2024 ] 	Batch(3900/7879) done. Loss: 0.5766  lr:0.010000
[ Mon Jul  8 05:15:16 2024 ] 
Training: Epoch [36/120], Step [3999], Loss: 0.27732691168785095, Training Accuracy: 84.36875
[ Mon Jul  8 05:15:16 2024 ] 	Batch(4000/7879) done. Loss: 0.0905  lr:0.010000
[ Mon Jul  8 05:15:40 2024 ] 	Batch(4100/7879) done. Loss: 0.7415  lr:0.010000
[ Mon Jul  8 05:16:02 2024 ] 	Batch(4200/7879) done. Loss: 0.8016  lr:0.010000
[ Mon Jul  8 05:16:25 2024 ] 	Batch(4300/7879) done. Loss: 1.0258  lr:0.010000
[ Mon Jul  8 05:16:48 2024 ] 	Batch(4400/7879) done. Loss: 0.8272  lr:0.010000
[ Mon Jul  8 05:17:10 2024 ] 
Training: Epoch [36/120], Step [4499], Loss: 0.05817880481481552, Training Accuracy: 84.38888888888889
[ Mon Jul  8 05:17:11 2024 ] 	Batch(4500/7879) done. Loss: 0.2968  lr:0.010000
[ Mon Jul  8 05:17:33 2024 ] 	Batch(4600/7879) done. Loss: 0.4909  lr:0.010000
[ Mon Jul  8 05:17:56 2024 ] 	Batch(4700/7879) done. Loss: 0.4995  lr:0.010000
[ Mon Jul  8 05:18:19 2024 ] 	Batch(4800/7879) done. Loss: 0.5988  lr:0.010000
[ Mon Jul  8 05:18:41 2024 ] 	Batch(4900/7879) done. Loss: 0.6144  lr:0.010000
[ Mon Jul  8 05:19:04 2024 ] 
Training: Epoch [36/120], Step [4999], Loss: 0.2805928587913513, Training Accuracy: 84.35000000000001
[ Mon Jul  8 05:19:04 2024 ] 	Batch(5000/7879) done. Loss: 0.2087  lr:0.010000
[ Mon Jul  8 05:19:27 2024 ] 	Batch(5100/7879) done. Loss: 0.2957  lr:0.010000
[ Mon Jul  8 05:19:50 2024 ] 	Batch(5200/7879) done. Loss: 0.1324  lr:0.010000
[ Mon Jul  8 05:20:13 2024 ] 	Batch(5300/7879) done. Loss: 0.1392  lr:0.010000
[ Mon Jul  8 05:20:36 2024 ] 	Batch(5400/7879) done. Loss: 0.2665  lr:0.010000
[ Mon Jul  8 05:20:58 2024 ] 
Training: Epoch [36/120], Step [5499], Loss: 0.7551864981651306, Training Accuracy: 84.4090909090909
[ Mon Jul  8 05:20:59 2024 ] 	Batch(5500/7879) done. Loss: 0.1037  lr:0.010000
[ Mon Jul  8 05:21:21 2024 ] 	Batch(5600/7879) done. Loss: 0.1861  lr:0.010000
[ Mon Jul  8 05:21:44 2024 ] 	Batch(5700/7879) done. Loss: 0.2090  lr:0.010000
[ Mon Jul  8 05:22:07 2024 ] 	Batch(5800/7879) done. Loss: 0.6464  lr:0.010000
[ Mon Jul  8 05:22:30 2024 ] 	Batch(5900/7879) done. Loss: 0.4636  lr:0.010000
[ Mon Jul  8 05:22:52 2024 ] 
Training: Epoch [36/120], Step [5999], Loss: 1.0755078792572021, Training Accuracy: 84.30416666666667
[ Mon Jul  8 05:22:53 2024 ] 	Batch(6000/7879) done. Loss: 0.3216  lr:0.010000
[ Mon Jul  8 05:23:15 2024 ] 	Batch(6100/7879) done. Loss: 0.9122  lr:0.010000
[ Mon Jul  8 05:23:38 2024 ] 	Batch(6200/7879) done. Loss: 0.0951  lr:0.010000
[ Mon Jul  8 05:24:01 2024 ] 	Batch(6300/7879) done. Loss: 0.5474  lr:0.010000
[ Mon Jul  8 05:24:24 2024 ] 	Batch(6400/7879) done. Loss: 0.1554  lr:0.010000
[ Mon Jul  8 05:24:46 2024 ] 
Training: Epoch [36/120], Step [6499], Loss: 0.9868071675300598, Training Accuracy: 84.2673076923077
[ Mon Jul  8 05:24:46 2024 ] 	Batch(6500/7879) done. Loss: 0.5758  lr:0.010000
[ Mon Jul  8 05:25:09 2024 ] 	Batch(6600/7879) done. Loss: 0.3291  lr:0.010000
[ Mon Jul  8 05:25:32 2024 ] 	Batch(6700/7879) done. Loss: 0.7488  lr:0.010000
[ Mon Jul  8 05:25:54 2024 ] 	Batch(6800/7879) done. Loss: 0.7392  lr:0.010000
[ Mon Jul  8 05:26:17 2024 ] 	Batch(6900/7879) done. Loss: 0.5024  lr:0.010000
[ Mon Jul  8 05:26:40 2024 ] 
Training: Epoch [36/120], Step [6999], Loss: 0.5172523856163025, Training Accuracy: 84.25178571428572
[ Mon Jul  8 05:26:40 2024 ] 	Batch(7000/7879) done. Loss: 0.3627  lr:0.010000
[ Mon Jul  8 05:27:02 2024 ] 	Batch(7100/7879) done. Loss: 0.6447  lr:0.010000
[ Mon Jul  8 05:27:25 2024 ] 	Batch(7200/7879) done. Loss: 1.1581  lr:0.010000
[ Mon Jul  8 05:27:48 2024 ] 	Batch(7300/7879) done. Loss: 0.4964  lr:0.010000
[ Mon Jul  8 05:28:11 2024 ] 	Batch(7400/7879) done. Loss: 0.4210  lr:0.010000
[ Mon Jul  8 05:28:33 2024 ] 
Training: Epoch [36/120], Step [7499], Loss: 1.3410645723342896, Training Accuracy: 84.15333333333334
[ Mon Jul  8 05:28:33 2024 ] 	Batch(7500/7879) done. Loss: 0.1466  lr:0.010000
[ Mon Jul  8 05:28:56 2024 ] 	Batch(7600/7879) done. Loss: 0.6430  lr:0.010000
[ Mon Jul  8 05:29:19 2024 ] 	Batch(7700/7879) done. Loss: 0.5302  lr:0.010000
[ Mon Jul  8 05:29:42 2024 ] 	Batch(7800/7879) done. Loss: 0.9974  lr:0.010000
[ Mon Jul  8 05:29:59 2024 ] 	Mean training loss: 0.5159.
[ Mon Jul  8 05:29:59 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 05:29:59 2024 ] Training epoch: 38
[ Mon Jul  8 05:30:00 2024 ] 	Batch(0/7879) done. Loss: 0.4805  lr:0.010000
[ Mon Jul  8 05:30:23 2024 ] 	Batch(100/7879) done. Loss: 0.1529  lr:0.010000
[ Mon Jul  8 05:30:47 2024 ] 	Batch(200/7879) done. Loss: 0.0812  lr:0.010000
[ Mon Jul  8 05:31:10 2024 ] 	Batch(300/7879) done. Loss: 1.1503  lr:0.010000
[ Mon Jul  8 05:31:34 2024 ] 	Batch(400/7879) done. Loss: 1.3376  lr:0.010000
[ Mon Jul  8 05:31:57 2024 ] 
Training: Epoch [37/120], Step [499], Loss: 0.3243725895881653, Training Accuracy: 85.075
[ Mon Jul  8 05:31:57 2024 ] 	Batch(500/7879) done. Loss: 0.0787  lr:0.010000
[ Mon Jul  8 05:32:21 2024 ] 	Batch(600/7879) done. Loss: 0.6852  lr:0.010000
[ Mon Jul  8 05:32:44 2024 ] 	Batch(700/7879) done. Loss: 0.0241  lr:0.010000
[ Mon Jul  8 05:33:08 2024 ] 	Batch(800/7879) done. Loss: 0.1730  lr:0.010000
[ Mon Jul  8 05:33:31 2024 ] 	Batch(900/7879) done. Loss: 0.0663  lr:0.010000
[ Mon Jul  8 05:33:53 2024 ] 
Training: Epoch [37/120], Step [999], Loss: 0.48425355553627014, Training Accuracy: 85.3875
[ Mon Jul  8 05:33:54 2024 ] 	Batch(1000/7879) done. Loss: 0.7273  lr:0.010000
[ Mon Jul  8 05:34:17 2024 ] 	Batch(1100/7879) done. Loss: 0.2987  lr:0.010000
[ Mon Jul  8 05:34:40 2024 ] 	Batch(1200/7879) done. Loss: 0.9014  lr:0.010000
[ Mon Jul  8 05:35:03 2024 ] 	Batch(1300/7879) done. Loss: 0.7136  lr:0.010000
[ Mon Jul  8 05:35:26 2024 ] 	Batch(1400/7879) done. Loss: 0.4495  lr:0.010000
[ Mon Jul  8 05:35:50 2024 ] 
Training: Epoch [37/120], Step [1499], Loss: 0.29770058393478394, Training Accuracy: 85.28333333333333
[ Mon Jul  8 05:35:50 2024 ] 	Batch(1500/7879) done. Loss: 0.9271  lr:0.010000
[ Mon Jul  8 05:36:13 2024 ] 	Batch(1600/7879) done. Loss: 0.2465  lr:0.010000
[ Mon Jul  8 05:36:36 2024 ] 	Batch(1700/7879) done. Loss: 0.3144  lr:0.010000
[ Mon Jul  8 05:36:59 2024 ] 	Batch(1800/7879) done. Loss: 0.2201  lr:0.010000
[ Mon Jul  8 05:37:21 2024 ] 	Batch(1900/7879) done. Loss: 0.2013  lr:0.010000
[ Mon Jul  8 05:37:44 2024 ] 
Training: Epoch [37/120], Step [1999], Loss: 0.5677410364151001, Training Accuracy: 85.32499999999999
[ Mon Jul  8 05:37:44 2024 ] 	Batch(2000/7879) done. Loss: 0.6163  lr:0.010000
[ Mon Jul  8 05:38:08 2024 ] 	Batch(2100/7879) done. Loss: 0.2104  lr:0.010000
[ Mon Jul  8 05:38:31 2024 ] 	Batch(2200/7879) done. Loss: 0.3717  lr:0.010000
[ Mon Jul  8 05:38:55 2024 ] 	Batch(2300/7879) done. Loss: 0.4948  lr:0.010000
[ Mon Jul  8 05:39:18 2024 ] 	Batch(2400/7879) done. Loss: 0.3374  lr:0.010000
[ Mon Jul  8 05:39:41 2024 ] 
Training: Epoch [37/120], Step [2499], Loss: 0.3911527097225189, Training Accuracy: 85.095
[ Mon Jul  8 05:39:41 2024 ] 	Batch(2500/7879) done. Loss: 0.1694  lr:0.010000
[ Mon Jul  8 05:40:04 2024 ] 	Batch(2600/7879) done. Loss: 1.0807  lr:0.010000
[ Mon Jul  8 05:40:26 2024 ] 	Batch(2700/7879) done. Loss: 0.6481  lr:0.010000
[ Mon Jul  8 05:40:49 2024 ] 	Batch(2800/7879) done. Loss: 0.7805  lr:0.010000
[ Mon Jul  8 05:41:12 2024 ] 	Batch(2900/7879) done. Loss: 0.2864  lr:0.010000
[ Mon Jul  8 05:41:34 2024 ] 
Training: Epoch [37/120], Step [2999], Loss: 0.46473708748817444, Training Accuracy: 84.99166666666666
[ Mon Jul  8 05:41:35 2024 ] 	Batch(3000/7879) done. Loss: 0.3810  lr:0.010000
[ Mon Jul  8 05:41:57 2024 ] 	Batch(3100/7879) done. Loss: 1.1305  lr:0.010000
[ Mon Jul  8 05:42:20 2024 ] 	Batch(3200/7879) done. Loss: 0.2707  lr:0.010000
[ Mon Jul  8 05:42:43 2024 ] 	Batch(3300/7879) done. Loss: 0.5150  lr:0.010000
[ Mon Jul  8 05:43:06 2024 ] 	Batch(3400/7879) done. Loss: 0.6277  lr:0.010000
[ Mon Jul  8 05:43:28 2024 ] 
Training: Epoch [37/120], Step [3499], Loss: 0.28395769000053406, Training Accuracy: 84.95357142857142
[ Mon Jul  8 05:43:28 2024 ] 	Batch(3500/7879) done. Loss: 0.1799  lr:0.010000
[ Mon Jul  8 05:43:51 2024 ] 	Batch(3600/7879) done. Loss: 0.3089  lr:0.010000
[ Mon Jul  8 05:44:14 2024 ] 	Batch(3700/7879) done. Loss: 0.0800  lr:0.010000
[ Mon Jul  8 05:44:37 2024 ] 	Batch(3800/7879) done. Loss: 1.6598  lr:0.010000
[ Mon Jul  8 05:44:59 2024 ] 	Batch(3900/7879) done. Loss: 0.0595  lr:0.010000
[ Mon Jul  8 05:45:22 2024 ] 
Training: Epoch [37/120], Step [3999], Loss: 0.2322913557291031, Training Accuracy: 84.93437499999999
[ Mon Jul  8 05:45:22 2024 ] 	Batch(4000/7879) done. Loss: 0.0782  lr:0.010000
[ Mon Jul  8 05:45:46 2024 ] 	Batch(4100/7879) done. Loss: 0.0806  lr:0.010000
[ Mon Jul  8 05:46:09 2024 ] 	Batch(4200/7879) done. Loss: 1.2027  lr:0.010000
[ Mon Jul  8 05:46:32 2024 ] 	Batch(4300/7879) done. Loss: 0.2115  lr:0.010000
[ Mon Jul  8 05:46:56 2024 ] 	Batch(4400/7879) done. Loss: 0.4128  lr:0.010000
[ Mon Jul  8 05:47:19 2024 ] 
Training: Epoch [37/120], Step [4499], Loss: 0.3653543293476105, Training Accuracy: 84.95555555555555
[ Mon Jul  8 05:47:19 2024 ] 	Batch(4500/7879) done. Loss: 0.7715  lr:0.010000
[ Mon Jul  8 05:47:42 2024 ] 	Batch(4600/7879) done. Loss: 0.1221  lr:0.010000
[ Mon Jul  8 05:48:04 2024 ] 	Batch(4700/7879) done. Loss: 0.9786  lr:0.010000
[ Mon Jul  8 05:48:27 2024 ] 	Batch(4800/7879) done. Loss: 0.3028  lr:0.010000
[ Mon Jul  8 05:48:50 2024 ] 	Batch(4900/7879) done. Loss: 0.4642  lr:0.010000
[ Mon Jul  8 05:49:12 2024 ] 
Training: Epoch [37/120], Step [4999], Loss: 0.23470264673233032, Training Accuracy: 84.935
[ Mon Jul  8 05:49:12 2024 ] 	Batch(5000/7879) done. Loss: 0.2877  lr:0.010000
[ Mon Jul  8 05:49:35 2024 ] 	Batch(5100/7879) done. Loss: 0.3781  lr:0.010000
[ Mon Jul  8 05:49:58 2024 ] 	Batch(5200/7879) done. Loss: 0.6693  lr:0.010000
[ Mon Jul  8 05:50:21 2024 ] 	Batch(5300/7879) done. Loss: 0.7697  lr:0.010000
[ Mon Jul  8 05:50:43 2024 ] 	Batch(5400/7879) done. Loss: 0.5210  lr:0.010000
[ Mon Jul  8 05:51:06 2024 ] 
Training: Epoch [37/120], Step [5499], Loss: 0.3160901665687561, Training Accuracy: 84.95227272727273
[ Mon Jul  8 05:51:06 2024 ] 	Batch(5500/7879) done. Loss: 0.2329  lr:0.010000
[ Mon Jul  8 05:51:29 2024 ] 	Batch(5600/7879) done. Loss: 0.9518  lr:0.010000
[ Mon Jul  8 05:51:52 2024 ] 	Batch(5700/7879) done. Loss: 0.1935  lr:0.010000
[ Mon Jul  8 05:52:14 2024 ] 	Batch(5800/7879) done. Loss: 0.3924  lr:0.010000
[ Mon Jul  8 05:52:37 2024 ] 	Batch(5900/7879) done. Loss: 0.1866  lr:0.010000
[ Mon Jul  8 05:52:59 2024 ] 
Training: Epoch [37/120], Step [5999], Loss: 0.12522023916244507, Training Accuracy: 84.82916666666667
[ Mon Jul  8 05:53:00 2024 ] 	Batch(6000/7879) done. Loss: 0.5697  lr:0.010000
[ Mon Jul  8 05:53:23 2024 ] 	Batch(6100/7879) done. Loss: 0.5812  lr:0.010000
[ Mon Jul  8 05:53:45 2024 ] 	Batch(6200/7879) done. Loss: 0.8278  lr:0.010000
[ Mon Jul  8 05:54:08 2024 ] 	Batch(6300/7879) done. Loss: 1.2299  lr:0.010000
[ Mon Jul  8 05:54:31 2024 ] 	Batch(6400/7879) done. Loss: 1.0396  lr:0.010000
[ Mon Jul  8 05:54:53 2024 ] 
Training: Epoch [37/120], Step [6499], Loss: 0.2494148313999176, Training Accuracy: 84.73846153846154
[ Mon Jul  8 05:54:53 2024 ] 	Batch(6500/7879) done. Loss: 0.7902  lr:0.010000
[ Mon Jul  8 05:55:16 2024 ] 	Batch(6600/7879) done. Loss: 0.0241  lr:0.010000
[ Mon Jul  8 05:55:39 2024 ] 	Batch(6700/7879) done. Loss: 0.0856  lr:0.010000
[ Mon Jul  8 05:56:02 2024 ] 	Batch(6800/7879) done. Loss: 0.7785  lr:0.010000
[ Mon Jul  8 05:56:24 2024 ] 	Batch(6900/7879) done. Loss: 0.1841  lr:0.010000
[ Mon Jul  8 05:56:47 2024 ] 
Training: Epoch [37/120], Step [6999], Loss: 1.033663272857666, Training Accuracy: 84.64821428571429
[ Mon Jul  8 05:56:47 2024 ] 	Batch(7000/7879) done. Loss: 0.8094  lr:0.010000
[ Mon Jul  8 05:57:10 2024 ] 	Batch(7100/7879) done. Loss: 0.1560  lr:0.010000
[ Mon Jul  8 05:57:32 2024 ] 	Batch(7200/7879) done. Loss: 0.2344  lr:0.010000
[ Mon Jul  8 05:57:55 2024 ] 	Batch(7300/7879) done. Loss: 0.5171  lr:0.010000
[ Mon Jul  8 05:58:18 2024 ] 	Batch(7400/7879) done. Loss: 0.6932  lr:0.010000
[ Mon Jul  8 05:58:40 2024 ] 
Training: Epoch [37/120], Step [7499], Loss: 0.18914683163166046, Training Accuracy: 84.625
[ Mon Jul  8 05:58:40 2024 ] 	Batch(7500/7879) done. Loss: 0.4888  lr:0.010000
[ Mon Jul  8 05:59:03 2024 ] 	Batch(7600/7879) done. Loss: 0.8205  lr:0.010000
[ Mon Jul  8 05:59:27 2024 ] 	Batch(7700/7879) done. Loss: 0.4639  lr:0.010000
[ Mon Jul  8 05:59:50 2024 ] 	Batch(7800/7879) done. Loss: 0.3635  lr:0.010000
[ Mon Jul  8 06:00:09 2024 ] 	Mean training loss: 0.5050.
[ Mon Jul  8 06:00:09 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 06:00:09 2024 ] Training epoch: 39
[ Mon Jul  8 06:00:09 2024 ] 	Batch(0/7879) done. Loss: 0.5014  lr:0.010000
[ Mon Jul  8 06:00:32 2024 ] 	Batch(100/7879) done. Loss: 0.0219  lr:0.010000
[ Mon Jul  8 06:00:55 2024 ] 	Batch(200/7879) done. Loss: 1.2662  lr:0.010000
[ Mon Jul  8 06:01:17 2024 ] 	Batch(300/7879) done. Loss: 0.4637  lr:0.010000
[ Mon Jul  8 06:01:40 2024 ] 	Batch(400/7879) done. Loss: 0.9462  lr:0.010000
[ Mon Jul  8 06:02:03 2024 ] 
Training: Epoch [38/120], Step [499], Loss: 0.11042673140764236, Training Accuracy: 85.55
[ Mon Jul  8 06:02:03 2024 ] 	Batch(500/7879) done. Loss: 0.1273  lr:0.010000
[ Mon Jul  8 06:02:26 2024 ] 	Batch(600/7879) done. Loss: 0.4195  lr:0.010000
[ Mon Jul  8 06:02:48 2024 ] 	Batch(700/7879) done. Loss: 0.1400  lr:0.010000
[ Mon Jul  8 06:03:11 2024 ] 	Batch(800/7879) done. Loss: 0.6143  lr:0.010000
[ Mon Jul  8 06:03:35 2024 ] 	Batch(900/7879) done. Loss: 0.2668  lr:0.010000
[ Mon Jul  8 06:03:58 2024 ] 
Training: Epoch [38/120], Step [999], Loss: 1.2424466609954834, Training Accuracy: 85.78750000000001
[ Mon Jul  8 06:03:58 2024 ] 	Batch(1000/7879) done. Loss: 0.5093  lr:0.010000
[ Mon Jul  8 06:04:21 2024 ] 	Batch(1100/7879) done. Loss: 1.2151  lr:0.010000
[ Mon Jul  8 06:04:45 2024 ] 	Batch(1200/7879) done. Loss: 0.3004  lr:0.010000
[ Mon Jul  8 06:05:08 2024 ] 	Batch(1300/7879) done. Loss: 0.0161  lr:0.010000
[ Mon Jul  8 06:05:30 2024 ] 	Batch(1400/7879) done. Loss: 0.0160  lr:0.010000
[ Mon Jul  8 06:05:53 2024 ] 
Training: Epoch [38/120], Step [1499], Loss: 0.1826775074005127, Training Accuracy: 85.575
[ Mon Jul  8 06:05:53 2024 ] 	Batch(1500/7879) done. Loss: 0.2740  lr:0.010000
[ Mon Jul  8 06:06:16 2024 ] 	Batch(1600/7879) done. Loss: 2.0160  lr:0.010000
[ Mon Jul  8 06:06:39 2024 ] 	Batch(1700/7879) done. Loss: 0.2317  lr:0.010000
[ Mon Jul  8 06:07:02 2024 ] 	Batch(1800/7879) done. Loss: 0.1910  lr:0.010000
[ Mon Jul  8 06:07:25 2024 ] 	Batch(1900/7879) done. Loss: 0.8516  lr:0.010000
[ Mon Jul  8 06:07:47 2024 ] 
Training: Epoch [38/120], Step [1999], Loss: 0.46753647923469543, Training Accuracy: 85.3
[ Mon Jul  8 06:07:48 2024 ] 	Batch(2000/7879) done. Loss: 0.5447  lr:0.010000
[ Mon Jul  8 06:08:10 2024 ] 	Batch(2100/7879) done. Loss: 0.2340  lr:0.010000
[ Mon Jul  8 06:08:33 2024 ] 	Batch(2200/7879) done. Loss: 0.7722  lr:0.010000
[ Mon Jul  8 06:08:56 2024 ] 	Batch(2300/7879) done. Loss: 0.6950  lr:0.010000
[ Mon Jul  8 06:09:19 2024 ] 	Batch(2400/7879) done. Loss: 0.1183  lr:0.010000
[ Mon Jul  8 06:09:41 2024 ] 
Training: Epoch [38/120], Step [2499], Loss: 0.3455270528793335, Training Accuracy: 85.21
[ Mon Jul  8 06:09:41 2024 ] 	Batch(2500/7879) done. Loss: 0.4115  lr:0.010000
[ Mon Jul  8 06:10:04 2024 ] 	Batch(2600/7879) done. Loss: 0.4671  lr:0.010000
[ Mon Jul  8 06:10:27 2024 ] 	Batch(2700/7879) done. Loss: 0.1323  lr:0.010000
[ Mon Jul  8 06:10:50 2024 ] 	Batch(2800/7879) done. Loss: 0.7535  lr:0.010000
[ Mon Jul  8 06:11:13 2024 ] 	Batch(2900/7879) done. Loss: 0.3500  lr:0.010000
[ Mon Jul  8 06:11:36 2024 ] 
Training: Epoch [38/120], Step [2999], Loss: 1.3671962022781372, Training Accuracy: 85.0875
[ Mon Jul  8 06:11:36 2024 ] 	Batch(3000/7879) done. Loss: 0.1263  lr:0.010000
[ Mon Jul  8 06:12:00 2024 ] 	Batch(3100/7879) done. Loss: 0.2896  lr:0.010000
[ Mon Jul  8 06:12:24 2024 ] 	Batch(3200/7879) done. Loss: 0.5927  lr:0.010000
[ Mon Jul  8 06:12:46 2024 ] 	Batch(3300/7879) done. Loss: 0.5689  lr:0.010000
[ Mon Jul  8 06:13:09 2024 ] 	Batch(3400/7879) done. Loss: 0.2563  lr:0.010000
[ Mon Jul  8 06:13:31 2024 ] 
Training: Epoch [38/120], Step [3499], Loss: 0.16887034475803375, Training Accuracy: 85.08571428571429
[ Mon Jul  8 06:13:32 2024 ] 	Batch(3500/7879) done. Loss: 0.9299  lr:0.010000
[ Mon Jul  8 06:13:54 2024 ] 	Batch(3600/7879) done. Loss: 1.6892  lr:0.010000
[ Mon Jul  8 06:14:17 2024 ] 	Batch(3700/7879) done. Loss: 0.7992  lr:0.010000
[ Mon Jul  8 06:14:40 2024 ] 	Batch(3800/7879) done. Loss: 0.0832  lr:0.010000
[ Mon Jul  8 06:15:02 2024 ] 	Batch(3900/7879) done. Loss: 1.2938  lr:0.010000
[ Mon Jul  8 06:15:25 2024 ] 
Training: Epoch [38/120], Step [3999], Loss: 0.8511597514152527, Training Accuracy: 85.1125
[ Mon Jul  8 06:15:25 2024 ] 	Batch(4000/7879) done. Loss: 0.5128  lr:0.010000
[ Mon Jul  8 06:15:48 2024 ] 	Batch(4100/7879) done. Loss: 0.2530  lr:0.010000
[ Mon Jul  8 06:16:11 2024 ] 	Batch(4200/7879) done. Loss: 0.2220  lr:0.010000
[ Mon Jul  8 06:16:33 2024 ] 	Batch(4300/7879) done. Loss: 0.3441  lr:0.010000
[ Mon Jul  8 06:16:56 2024 ] 	Batch(4400/7879) done. Loss: 0.3979  lr:0.010000
[ Mon Jul  8 06:17:19 2024 ] 
Training: Epoch [38/120], Step [4499], Loss: 0.482115238904953, Training Accuracy: 85.04722222222222
[ Mon Jul  8 06:17:19 2024 ] 	Batch(4500/7879) done. Loss: 0.1622  lr:0.010000
[ Mon Jul  8 06:17:42 2024 ] 	Batch(4600/7879) done. Loss: 0.2845  lr:0.010000
[ Mon Jul  8 06:18:04 2024 ] 	Batch(4700/7879) done. Loss: 0.1011  lr:0.010000
[ Mon Jul  8 06:18:27 2024 ] 	Batch(4800/7879) done. Loss: 0.2790  lr:0.010000
[ Mon Jul  8 06:18:50 2024 ] 	Batch(4900/7879) done. Loss: 0.3324  lr:0.010000
[ Mon Jul  8 06:19:12 2024 ] 
Training: Epoch [38/120], Step [4999], Loss: 0.6134774088859558, Training Accuracy: 85.075
[ Mon Jul  8 06:19:12 2024 ] 	Batch(5000/7879) done. Loss: 0.4340  lr:0.010000
[ Mon Jul  8 06:19:35 2024 ] 	Batch(5100/7879) done. Loss: 0.7369  lr:0.010000
[ Mon Jul  8 06:19:58 2024 ] 	Batch(5200/7879) done. Loss: 0.7343  lr:0.010000
[ Mon Jul  8 06:20:21 2024 ] 	Batch(5300/7879) done. Loss: 0.8383  lr:0.010000
[ Mon Jul  8 06:20:43 2024 ] 	Batch(5400/7879) done. Loss: 0.9789  lr:0.010000
[ Mon Jul  8 06:21:06 2024 ] 
Training: Epoch [38/120], Step [5499], Loss: 0.25058871507644653, Training Accuracy: 84.94772727272726
[ Mon Jul  8 06:21:06 2024 ] 	Batch(5500/7879) done. Loss: 0.2213  lr:0.010000
[ Mon Jul  8 06:21:29 2024 ] 	Batch(5600/7879) done. Loss: 1.2438  lr:0.010000
[ Mon Jul  8 06:21:51 2024 ] 	Batch(5700/7879) done. Loss: 0.4015  lr:0.010000
[ Mon Jul  8 06:22:14 2024 ] 	Batch(5800/7879) done. Loss: 0.3421  lr:0.010000
[ Mon Jul  8 06:22:37 2024 ] 	Batch(5900/7879) done. Loss: 0.3362  lr:0.010000
[ Mon Jul  8 06:22:59 2024 ] 
Training: Epoch [38/120], Step [5999], Loss: 0.513020932674408, Training Accuracy: 84.90208333333334
[ Mon Jul  8 06:23:00 2024 ] 	Batch(6000/7879) done. Loss: 0.2500  lr:0.010000
[ Mon Jul  8 06:23:23 2024 ] 	Batch(6100/7879) done. Loss: 0.4215  lr:0.010000
[ Mon Jul  8 06:23:46 2024 ] 	Batch(6200/7879) done. Loss: 0.2450  lr:0.010000
[ Mon Jul  8 06:24:10 2024 ] 	Batch(6300/7879) done. Loss: 0.1663  lr:0.010000
[ Mon Jul  8 06:24:34 2024 ] 	Batch(6400/7879) done. Loss: 0.5777  lr:0.010000
[ Mon Jul  8 06:24:57 2024 ] 
Training: Epoch [38/120], Step [6499], Loss: 0.21195761859416962, Training Accuracy: 84.79615384615384
[ Mon Jul  8 06:24:57 2024 ] 	Batch(6500/7879) done. Loss: 0.2515  lr:0.010000
[ Mon Jul  8 06:25:20 2024 ] 	Batch(6600/7879) done. Loss: 0.4841  lr:0.010000
[ Mon Jul  8 06:25:44 2024 ] 	Batch(6700/7879) done. Loss: 0.1448  lr:0.010000
[ Mon Jul  8 06:26:08 2024 ] 	Batch(6800/7879) done. Loss: 0.0665  lr:0.010000
[ Mon Jul  8 06:26:31 2024 ] 	Batch(6900/7879) done. Loss: 0.3381  lr:0.010000
[ Mon Jul  8 06:26:55 2024 ] 
Training: Epoch [38/120], Step [6999], Loss: 0.34375840425491333, Training Accuracy: 84.80357142857143
[ Mon Jul  8 06:26:55 2024 ] 	Batch(7000/7879) done. Loss: 1.6438  lr:0.010000
[ Mon Jul  8 06:27:18 2024 ] 	Batch(7100/7879) done. Loss: 0.3010  lr:0.010000
[ Mon Jul  8 06:27:40 2024 ] 	Batch(7200/7879) done. Loss: 0.3218  lr:0.010000
[ Mon Jul  8 06:28:03 2024 ] 	Batch(7300/7879) done. Loss: 0.1196  lr:0.010000
[ Mon Jul  8 06:28:26 2024 ] 	Batch(7400/7879) done. Loss: 1.6823  lr:0.010000
[ Mon Jul  8 06:28:48 2024 ] 
Training: Epoch [38/120], Step [7499], Loss: 0.9572664499282837, Training Accuracy: 84.79166666666667
[ Mon Jul  8 06:28:49 2024 ] 	Batch(7500/7879) done. Loss: 0.0857  lr:0.010000
[ Mon Jul  8 06:29:11 2024 ] 	Batch(7600/7879) done. Loss: 0.1585  lr:0.010000
[ Mon Jul  8 06:29:34 2024 ] 	Batch(7700/7879) done. Loss: 0.4027  lr:0.010000
[ Mon Jul  8 06:29:57 2024 ] 	Batch(7800/7879) done. Loss: 0.7602  lr:0.010000
[ Mon Jul  8 06:30:15 2024 ] 	Mean training loss: 0.4920.
[ Mon Jul  8 06:30:15 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 06:30:15 2024 ] Training epoch: 40
[ Mon Jul  8 06:30:15 2024 ] 	Batch(0/7879) done. Loss: 0.0256  lr:0.010000
[ Mon Jul  8 06:30:39 2024 ] 	Batch(100/7879) done. Loss: 0.5866  lr:0.010000
[ Mon Jul  8 06:31:02 2024 ] 	Batch(200/7879) done. Loss: 0.8719  lr:0.010000
[ Mon Jul  8 06:31:26 2024 ] 	Batch(300/7879) done. Loss: 0.7205  lr:0.010000
[ Mon Jul  8 06:31:49 2024 ] 	Batch(400/7879) done. Loss: 0.0367  lr:0.010000
[ Mon Jul  8 06:32:11 2024 ] 
Training: Epoch [39/120], Step [499], Loss: 0.13611774146556854, Training Accuracy: 85.15
[ Mon Jul  8 06:32:12 2024 ] 	Batch(500/7879) done. Loss: 0.9268  lr:0.010000
[ Mon Jul  8 06:32:34 2024 ] 	Batch(600/7879) done. Loss: 0.1808  lr:0.010000
[ Mon Jul  8 06:32:57 2024 ] 	Batch(700/7879) done. Loss: 0.4251  lr:0.010000
[ Mon Jul  8 06:33:20 2024 ] 	Batch(800/7879) done. Loss: 0.4534  lr:0.010000
[ Mon Jul  8 06:33:43 2024 ] 	Batch(900/7879) done. Loss: 0.5430  lr:0.010000
[ Mon Jul  8 06:34:05 2024 ] 
Training: Epoch [39/120], Step [999], Loss: 0.3709479570388794, Training Accuracy: 86.05000000000001
[ Mon Jul  8 06:34:06 2024 ] 	Batch(1000/7879) done. Loss: 0.4424  lr:0.010000
[ Mon Jul  8 06:34:28 2024 ] 	Batch(1100/7879) done. Loss: 0.4485  lr:0.010000
[ Mon Jul  8 06:34:51 2024 ] 	Batch(1200/7879) done. Loss: 0.0817  lr:0.010000
[ Mon Jul  8 06:35:15 2024 ] 	Batch(1300/7879) done. Loss: 0.0953  lr:0.010000
[ Mon Jul  8 06:35:38 2024 ] 	Batch(1400/7879) done. Loss: 0.4208  lr:0.010000
[ Mon Jul  8 06:36:01 2024 ] 
Training: Epoch [39/120], Step [1499], Loss: 0.4249516725540161, Training Accuracy: 86.0
[ Mon Jul  8 06:36:01 2024 ] 	Batch(1500/7879) done. Loss: 0.5343  lr:0.010000
[ Mon Jul  8 06:36:24 2024 ] 	Batch(1600/7879) done. Loss: 1.1785  lr:0.010000
[ Mon Jul  8 06:36:47 2024 ] 	Batch(1700/7879) done. Loss: 1.6106  lr:0.010000
[ Mon Jul  8 06:37:10 2024 ] 	Batch(1800/7879) done. Loss: 0.3290  lr:0.010000
[ Mon Jul  8 06:37:32 2024 ] 	Batch(1900/7879) done. Loss: 0.6629  lr:0.010000
[ Mon Jul  8 06:37:55 2024 ] 
Training: Epoch [39/120], Step [1999], Loss: 0.4610762298107147, Training Accuracy: 86.05625
[ Mon Jul  8 06:37:55 2024 ] 	Batch(2000/7879) done. Loss: 0.0265  lr:0.010000
[ Mon Jul  8 06:38:18 2024 ] 	Batch(2100/7879) done. Loss: 0.6024  lr:0.010000
[ Mon Jul  8 06:38:41 2024 ] 	Batch(2200/7879) done. Loss: 2.0668  lr:0.010000
[ Mon Jul  8 06:39:04 2024 ] 	Batch(2300/7879) done. Loss: 0.9132  lr:0.010000
[ Mon Jul  8 06:39:27 2024 ] 	Batch(2400/7879) done. Loss: 0.3711  lr:0.010000
[ Mon Jul  8 06:39:49 2024 ] 
Training: Epoch [39/120], Step [2499], Loss: 0.09158844500780106, Training Accuracy: 85.82
[ Mon Jul  8 06:39:49 2024 ] 	Batch(2500/7879) done. Loss: 0.6515  lr:0.010000
[ Mon Jul  8 06:40:12 2024 ] 	Batch(2600/7879) done. Loss: 0.4234  lr:0.010000
[ Mon Jul  8 06:40:35 2024 ] 	Batch(2700/7879) done. Loss: 0.3166  lr:0.010000
[ Mon Jul  8 06:40:58 2024 ] 	Batch(2800/7879) done. Loss: 0.3803  lr:0.010000
[ Mon Jul  8 06:41:20 2024 ] 	Batch(2900/7879) done. Loss: 0.1289  lr:0.010000
[ Mon Jul  8 06:41:43 2024 ] 
Training: Epoch [39/120], Step [2999], Loss: 0.49885621666908264, Training Accuracy: 85.62083333333334
[ Mon Jul  8 06:41:43 2024 ] 	Batch(3000/7879) done. Loss: 0.0585  lr:0.010000
[ Mon Jul  8 06:42:06 2024 ] 	Batch(3100/7879) done. Loss: 0.2058  lr:0.010000
[ Mon Jul  8 06:42:28 2024 ] 	Batch(3200/7879) done. Loss: 0.6267  lr:0.010000
[ Mon Jul  8 06:42:51 2024 ] 	Batch(3300/7879) done. Loss: 1.5342  lr:0.010000
[ Mon Jul  8 06:43:14 2024 ] 	Batch(3400/7879) done. Loss: 0.0508  lr:0.010000
[ Mon Jul  8 06:43:36 2024 ] 
Training: Epoch [39/120], Step [3499], Loss: 0.3790185749530792, Training Accuracy: 85.52142857142857
[ Mon Jul  8 06:43:36 2024 ] 	Batch(3500/7879) done. Loss: 0.1233  lr:0.010000
[ Mon Jul  8 06:43:59 2024 ] 	Batch(3600/7879) done. Loss: 0.0943  lr:0.010000
[ Mon Jul  8 06:44:22 2024 ] 	Batch(3700/7879) done. Loss: 0.3533  lr:0.010000
[ Mon Jul  8 06:44:45 2024 ] 	Batch(3800/7879) done. Loss: 0.5770  lr:0.010000
[ Mon Jul  8 06:45:07 2024 ] 	Batch(3900/7879) done. Loss: 0.5909  lr:0.010000
[ Mon Jul  8 06:45:30 2024 ] 
Training: Epoch [39/120], Step [3999], Loss: 0.31295737624168396, Training Accuracy: 85.24374999999999
[ Mon Jul  8 06:45:30 2024 ] 	Batch(4000/7879) done. Loss: 0.6564  lr:0.010000
[ Mon Jul  8 06:45:53 2024 ] 	Batch(4100/7879) done. Loss: 1.2846  lr:0.010000
[ Mon Jul  8 06:46:16 2024 ] 	Batch(4200/7879) done. Loss: 0.8788  lr:0.010000
[ Mon Jul  8 06:46:38 2024 ] 	Batch(4300/7879) done. Loss: 0.0713  lr:0.010000
[ Mon Jul  8 06:47:01 2024 ] 	Batch(4400/7879) done. Loss: 1.1288  lr:0.010000
[ Mon Jul  8 06:47:24 2024 ] 
Training: Epoch [39/120], Step [4499], Loss: 1.155439019203186, Training Accuracy: 85.15833333333333
[ Mon Jul  8 06:47:24 2024 ] 	Batch(4500/7879) done. Loss: 0.1142  lr:0.010000
[ Mon Jul  8 06:47:47 2024 ] 	Batch(4600/7879) done. Loss: 1.0798  lr:0.010000
[ Mon Jul  8 06:48:09 2024 ] 	Batch(4700/7879) done. Loss: 0.4002  lr:0.010000
[ Mon Jul  8 06:48:32 2024 ] 	Batch(4800/7879) done. Loss: 0.0157  lr:0.010000
[ Mon Jul  8 06:48:55 2024 ] 	Batch(4900/7879) done. Loss: 0.6240  lr:0.010000
[ Mon Jul  8 06:49:17 2024 ] 
Training: Epoch [39/120], Step [4999], Loss: 0.43159550428390503, Training Accuracy: 85.1625
[ Mon Jul  8 06:49:17 2024 ] 	Batch(5000/7879) done. Loss: 1.0051  lr:0.010000
[ Mon Jul  8 06:49:40 2024 ] 	Batch(5100/7879) done. Loss: 0.3753  lr:0.010000
[ Mon Jul  8 06:50:03 2024 ] 	Batch(5200/7879) done. Loss: 0.2198  lr:0.010000
[ Mon Jul  8 06:50:26 2024 ] 	Batch(5300/7879) done. Loss: 0.2792  lr:0.010000
[ Mon Jul  8 06:50:48 2024 ] 	Batch(5400/7879) done. Loss: 0.8244  lr:0.010000
[ Mon Jul  8 06:51:11 2024 ] 
Training: Epoch [39/120], Step [5499], Loss: 0.56393963098526, Training Accuracy: 85.11136363636363
[ Mon Jul  8 06:51:11 2024 ] 	Batch(5500/7879) done. Loss: 0.5594  lr:0.010000
[ Mon Jul  8 06:51:34 2024 ] 	Batch(5600/7879) done. Loss: 0.7863  lr:0.010000
[ Mon Jul  8 06:51:56 2024 ] 	Batch(5700/7879) done. Loss: 1.2048  lr:0.010000
[ Mon Jul  8 06:52:19 2024 ] 	Batch(5800/7879) done. Loss: 1.3474  lr:0.010000
[ Mon Jul  8 06:52:42 2024 ] 	Batch(5900/7879) done. Loss: 0.4679  lr:0.010000
[ Mon Jul  8 06:53:04 2024 ] 
Training: Epoch [39/120], Step [5999], Loss: 0.6117174625396729, Training Accuracy: 85.05625
[ Mon Jul  8 06:53:05 2024 ] 	Batch(6000/7879) done. Loss: 0.8534  lr:0.010000
[ Mon Jul  8 06:53:28 2024 ] 	Batch(6100/7879) done. Loss: 0.6982  lr:0.010000
[ Mon Jul  8 06:53:50 2024 ] 	Batch(6200/7879) done. Loss: 0.1292  lr:0.010000
[ Mon Jul  8 06:54:13 2024 ] 	Batch(6300/7879) done. Loss: 0.3447  lr:0.010000
[ Mon Jul  8 06:54:36 2024 ] 	Batch(6400/7879) done. Loss: 0.0329  lr:0.010000
[ Mon Jul  8 06:54:58 2024 ] 
Training: Epoch [39/120], Step [6499], Loss: 0.6100402474403381, Training Accuracy: 85.04038461538461
[ Mon Jul  8 06:54:58 2024 ] 	Batch(6500/7879) done. Loss: 0.9379  lr:0.010000
[ Mon Jul  8 06:55:21 2024 ] 	Batch(6600/7879) done. Loss: 0.3991  lr:0.010000
[ Mon Jul  8 06:55:44 2024 ] 	Batch(6700/7879) done. Loss: 0.6365  lr:0.010000
[ Mon Jul  8 06:56:07 2024 ] 	Batch(6800/7879) done. Loss: 0.7824  lr:0.010000
[ Mon Jul  8 06:56:30 2024 ] 	Batch(6900/7879) done. Loss: 0.4765  lr:0.010000
[ Mon Jul  8 06:56:53 2024 ] 
Training: Epoch [39/120], Step [6999], Loss: 0.5917485952377319, Training Accuracy: 84.98214285714286
[ Mon Jul  8 06:56:54 2024 ] 	Batch(7000/7879) done. Loss: 0.5263  lr:0.010000
[ Mon Jul  8 06:57:17 2024 ] 	Batch(7100/7879) done. Loss: 0.6920  lr:0.010000
[ Mon Jul  8 06:57:40 2024 ] 	Batch(7200/7879) done. Loss: 0.1789  lr:0.010000
[ Mon Jul  8 06:58:02 2024 ] 	Batch(7300/7879) done. Loss: 0.5019  lr:0.010000
[ Mon Jul  8 06:58:25 2024 ] 	Batch(7400/7879) done. Loss: 0.2358  lr:0.010000
[ Mon Jul  8 06:58:47 2024 ] 
Training: Epoch [39/120], Step [7499], Loss: 1.2008254528045654, Training Accuracy: 84.92666666666666
[ Mon Jul  8 06:58:48 2024 ] 	Batch(7500/7879) done. Loss: 0.3388  lr:0.010000
[ Mon Jul  8 06:59:10 2024 ] 	Batch(7600/7879) done. Loss: 0.6289  lr:0.010000
[ Mon Jul  8 06:59:33 2024 ] 	Batch(7700/7879) done. Loss: 1.0039  lr:0.010000
[ Mon Jul  8 06:59:56 2024 ] 	Batch(7800/7879) done. Loss: 0.3778  lr:0.010000
[ Mon Jul  8 07:00:14 2024 ] 	Mean training loss: 0.4909.
[ Mon Jul  8 07:00:14 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 07:00:14 2024 ] Eval epoch: 40
[ Mon Jul  8 07:06:10 2024 ] 	Mean val loss of 6365 batches: 1.1700205925153557.
[ Mon Jul  8 07:06:10 2024 ] 
Validation: Epoch [39/120], Samples [36655.0/50919], Loss: 0.18284329771995544, Validation Accuracy: 71.9868811249239
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 1 : 179 / 275 = 65 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 2 : 203 / 273 = 74 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 3 : 238 / 273 = 87 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 4 : 230 / 275 = 83 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 5 : 209 / 275 = 76 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 6 : 182 / 275 = 66 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 7 : 246 / 273 = 90 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 8 : 261 / 273 = 95 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 9 : 180 / 273 = 65 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 10 : 95 / 273 = 34 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 11 : 151 / 272 = 55 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 12 : 199 / 271 = 73 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 13 : 257 / 275 = 93 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 14 : 240 / 276 = 86 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 15 : 199 / 273 = 72 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 16 : 106 / 274 = 38 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 17 : 255 / 273 = 93 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 18 : 126 / 274 = 45 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 19 : 245 / 272 = 90 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 20 : 241 / 273 = 88 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 21 : 203 / 274 = 74 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 22 : 231 / 274 = 84 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 23 : 248 / 276 = 89 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 24 : 202 / 274 = 73 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 25 : 264 / 275 = 96 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 26 : 270 / 276 = 97 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 27 : 245 / 275 = 89 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 28 : 138 / 275 = 50 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 29 : 174 / 275 = 63 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 30 : 107 / 276 = 38 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 31 : 199 / 276 = 72 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 32 : 232 / 276 = 84 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 33 : 228 / 276 = 82 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 34 : 212 / 276 = 76 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 35 : 253 / 275 = 92 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 36 : 182 / 276 = 65 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 37 : 238 / 276 = 86 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 38 : 246 / 276 = 89 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 39 : 233 / 276 = 84 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 40 : 145 / 276 = 52 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 41 : 264 / 276 = 95 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 42 : 237 / 275 = 86 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 43 : 107 / 276 = 38 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 44 : 265 / 276 = 96 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 45 : 231 / 276 = 83 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 46 : 236 / 276 = 85 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 47 : 160 / 275 = 58 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 48 : 234 / 275 = 85 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 49 : 203 / 274 = 74 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 50 : 225 / 276 = 81 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 51 : 258 / 276 = 93 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 52 : 218 / 276 = 78 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 53 : 239 / 276 = 86 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 54 : 246 / 274 = 89 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 55 : 214 / 276 = 77 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 56 : 227 / 275 = 82 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 57 : 254 / 276 = 92 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 58 : 259 / 273 = 94 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 59 : 242 / 276 = 87 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 60 : 441 / 561 = 78 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 61 : 480 / 566 = 84 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 62 : 327 / 572 = 57 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 63 : 452 / 570 = 79 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 64 : 389 / 574 = 67 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 65 : 466 / 573 = 81 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 66 : 450 / 573 = 78 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 67 : 339 / 575 = 58 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 68 : 381 / 575 = 66 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 69 : 436 / 575 = 75 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 70 : 180 / 575 = 31 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 71 : 206 / 575 = 35 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 72 : 52 / 571 = 9 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 73 : 227 / 570 = 39 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 74 : 396 / 569 = 69 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 75 : 287 / 573 = 50 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 76 : 298 / 574 = 51 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 77 : 343 / 573 = 59 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 78 : 365 / 575 = 63 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 79 : 543 / 574 = 94 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 80 : 418 / 573 = 72 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 81 : 315 / 575 = 54 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 82 : 359 / 575 = 62 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 83 : 116 / 572 = 20 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 84 : 394 / 574 = 68 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 85 : 313 / 574 = 54 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 86 : 492 / 575 = 85 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 87 : 426 / 576 = 73 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 88 : 457 / 575 = 79 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 89 : 339 / 576 = 58 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 90 : 315 / 574 = 54 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 91 : 384 / 568 = 67 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 92 : 376 / 576 = 65 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 93 : 365 / 573 = 63 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 94 : 517 / 574 = 90 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 95 : 504 / 575 = 87 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 96 : 546 / 575 = 94 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 97 : 547 / 574 = 95 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 98 : 527 / 575 = 91 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 99 : 524 / 574 = 91 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 100 : 427 / 574 = 74 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 101 : 509 / 574 = 88 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 102 : 308 / 575 = 53 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 103 : 458 / 576 = 79 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 104 : 221 / 575 = 38 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 105 : 211 / 575 = 36 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 106 : 246 / 576 = 42 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 107 : 419 / 576 = 72 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 108 : 270 / 575 = 46 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 109 : 450 / 575 = 78 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 110 : 538 / 575 = 93 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 111 : 518 / 576 = 89 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 112 : 542 / 575 = 94 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 113 : 511 / 576 = 88 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 114 : 469 / 576 = 81 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 115 : 475 / 576 = 82 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 116 : 505 / 575 = 87 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 117 : 493 / 575 = 85 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 118 : 449 / 575 = 78 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 119 : 514 / 576 = 89 %
[ Mon Jul  8 07:06:10 2024 ] Accuracy of 120 : 219 / 274 = 79 %
[ Mon Jul  8 07:06:10 2024 ] Training epoch: 41
[ Mon Jul  8 07:06:11 2024 ] 	Batch(0/7879) done. Loss: 0.2511  lr:0.010000
[ Mon Jul  8 07:06:34 2024 ] 	Batch(100/7879) done. Loss: 0.3790  lr:0.010000
[ Mon Jul  8 07:06:56 2024 ] 	Batch(200/7879) done. Loss: 0.4729  lr:0.010000
[ Mon Jul  8 07:07:19 2024 ] 	Batch(300/7879) done. Loss: 1.1204  lr:0.010000
[ Mon Jul  8 07:07:42 2024 ] 	Batch(400/7879) done. Loss: 0.1413  lr:0.010000
[ Mon Jul  8 07:08:04 2024 ] 
Training: Epoch [40/120], Step [499], Loss: 0.01664046384394169, Training Accuracy: 85.3
[ Mon Jul  8 07:08:05 2024 ] 	Batch(500/7879) done. Loss: 0.1721  lr:0.010000
[ Mon Jul  8 07:08:27 2024 ] 	Batch(600/7879) done. Loss: 0.1980  lr:0.010000
[ Mon Jul  8 07:08:50 2024 ] 	Batch(700/7879) done. Loss: 0.8041  lr:0.010000
[ Mon Jul  8 07:09:13 2024 ] 	Batch(800/7879) done. Loss: 0.8628  lr:0.010000
[ Mon Jul  8 07:09:35 2024 ] 	Batch(900/7879) done. Loss: 0.1832  lr:0.010000
[ Mon Jul  8 07:09:58 2024 ] 
Training: Epoch [40/120], Step [999], Loss: 0.8999472856521606, Training Accuracy: 85.725
[ Mon Jul  8 07:09:58 2024 ] 	Batch(1000/7879) done. Loss: 0.6365  lr:0.010000
[ Mon Jul  8 07:10:21 2024 ] 	Batch(1100/7879) done. Loss: 0.2088  lr:0.010000
[ Mon Jul  8 07:10:44 2024 ] 	Batch(1200/7879) done. Loss: 0.5509  lr:0.010000
[ Mon Jul  8 07:11:07 2024 ] 	Batch(1300/7879) done. Loss: 0.8396  lr:0.010000
[ Mon Jul  8 07:11:31 2024 ] 	Batch(1400/7879) done. Loss: 0.3272  lr:0.010000
[ Mon Jul  8 07:11:54 2024 ] 
Training: Epoch [40/120], Step [1499], Loss: 0.8889347910881042, Training Accuracy: 85.975
[ Mon Jul  8 07:11:54 2024 ] 	Batch(1500/7879) done. Loss: 0.1360  lr:0.010000
[ Mon Jul  8 07:12:18 2024 ] 	Batch(1600/7879) done. Loss: 0.8637  lr:0.010000
[ Mon Jul  8 07:12:41 2024 ] 	Batch(1700/7879) done. Loss: 0.4440  lr:0.010000
[ Mon Jul  8 07:13:05 2024 ] 	Batch(1800/7879) done. Loss: 1.2904  lr:0.010000
[ Mon Jul  8 07:13:28 2024 ] 	Batch(1900/7879) done. Loss: 0.1272  lr:0.010000
[ Mon Jul  8 07:13:51 2024 ] 
Training: Epoch [40/120], Step [1999], Loss: 0.5785074234008789, Training Accuracy: 86.03125
[ Mon Jul  8 07:13:52 2024 ] 	Batch(2000/7879) done. Loss: 0.5372  lr:0.010000
[ Mon Jul  8 07:14:14 2024 ] 	Batch(2100/7879) done. Loss: 0.3896  lr:0.010000
[ Mon Jul  8 07:14:37 2024 ] 	Batch(2200/7879) done. Loss: 0.4114  lr:0.010000
[ Mon Jul  8 07:15:00 2024 ] 	Batch(2300/7879) done. Loss: 0.3595  lr:0.010000
[ Mon Jul  8 07:15:22 2024 ] 	Batch(2400/7879) done. Loss: 0.6110  lr:0.010000
[ Mon Jul  8 07:15:46 2024 ] 
Training: Epoch [40/120], Step [2499], Loss: 1.0643508434295654, Training Accuracy: 85.96000000000001
[ Mon Jul  8 07:15:46 2024 ] 	Batch(2500/7879) done. Loss: 0.0977  lr:0.010000
[ Mon Jul  8 07:16:09 2024 ] 	Batch(2600/7879) done. Loss: 1.0439  lr:0.010000
[ Mon Jul  8 07:16:31 2024 ] 	Batch(2700/7879) done. Loss: 0.4080  lr:0.010000
[ Mon Jul  8 07:16:54 2024 ] 	Batch(2800/7879) done. Loss: 0.0511  lr:0.010000
[ Mon Jul  8 07:17:17 2024 ] 	Batch(2900/7879) done. Loss: 0.1290  lr:0.010000
[ Mon Jul  8 07:17:39 2024 ] 
Training: Epoch [40/120], Step [2999], Loss: 0.8378969430923462, Training Accuracy: 85.75
[ Mon Jul  8 07:17:40 2024 ] 	Batch(3000/7879) done. Loss: 0.5483  lr:0.010000
[ Mon Jul  8 07:18:02 2024 ] 	Batch(3100/7879) done. Loss: 0.4567  lr:0.010000
[ Mon Jul  8 07:18:25 2024 ] 	Batch(3200/7879) done. Loss: 0.4723  lr:0.010000
[ Mon Jul  8 07:18:48 2024 ] 	Batch(3300/7879) done. Loss: 1.0547  lr:0.010000
[ Mon Jul  8 07:19:11 2024 ] 	Batch(3400/7879) done. Loss: 0.3788  lr:0.010000
[ Mon Jul  8 07:19:33 2024 ] 
Training: Epoch [40/120], Step [3499], Loss: 0.30848145484924316, Training Accuracy: 85.83928571428572
[ Mon Jul  8 07:19:33 2024 ] 	Batch(3500/7879) done. Loss: 0.5875  lr:0.010000
[ Mon Jul  8 07:19:56 2024 ] 	Batch(3600/7879) done. Loss: 0.3803  lr:0.010000
[ Mon Jul  8 07:20:20 2024 ] 	Batch(3700/7879) done. Loss: 0.6063  lr:0.010000
[ Mon Jul  8 07:20:43 2024 ] 	Batch(3800/7879) done. Loss: 0.3935  lr:0.010000
[ Mon Jul  8 07:21:05 2024 ] 	Batch(3900/7879) done. Loss: 0.6392  lr:0.010000
[ Mon Jul  8 07:21:28 2024 ] 
Training: Epoch [40/120], Step [3999], Loss: 0.4007633924484253, Training Accuracy: 85.8125
[ Mon Jul  8 07:21:28 2024 ] 	Batch(4000/7879) done. Loss: 0.0586  lr:0.010000
[ Mon Jul  8 07:21:51 2024 ] 	Batch(4100/7879) done. Loss: 0.4445  lr:0.010000
[ Mon Jul  8 07:22:14 2024 ] 	Batch(4200/7879) done. Loss: 1.2353  lr:0.010000
[ Mon Jul  8 07:22:36 2024 ] 	Batch(4300/7879) done. Loss: 0.3372  lr:0.010000
[ Mon Jul  8 07:22:59 2024 ] 	Batch(4400/7879) done. Loss: 0.5216  lr:0.010000
[ Mon Jul  8 07:23:22 2024 ] 
Training: Epoch [40/120], Step [4499], Loss: 0.8267021179199219, Training Accuracy: 85.63333333333333
[ Mon Jul  8 07:23:22 2024 ] 	Batch(4500/7879) done. Loss: 0.0736  lr:0.010000
[ Mon Jul  8 07:23:45 2024 ] 	Batch(4600/7879) done. Loss: 0.0806  lr:0.010000
[ Mon Jul  8 07:24:07 2024 ] 	Batch(4700/7879) done. Loss: 0.4477  lr:0.010000
[ Mon Jul  8 07:24:30 2024 ] 	Batch(4800/7879) done. Loss: 0.5236  lr:0.010000
[ Mon Jul  8 07:24:53 2024 ] 	Batch(4900/7879) done. Loss: 0.5766  lr:0.010000
[ Mon Jul  8 07:25:15 2024 ] 
Training: Epoch [40/120], Step [4999], Loss: 0.6745409369468689, Training Accuracy: 85.44
[ Mon Jul  8 07:25:15 2024 ] 	Batch(5000/7879) done. Loss: 0.2942  lr:0.010000
[ Mon Jul  8 07:25:38 2024 ] 	Batch(5100/7879) done. Loss: 0.4034  lr:0.010000
[ Mon Jul  8 07:26:01 2024 ] 	Batch(5200/7879) done. Loss: 0.1739  lr:0.010000
[ Mon Jul  8 07:26:24 2024 ] 	Batch(5300/7879) done. Loss: 0.0782  lr:0.010000
[ Mon Jul  8 07:26:46 2024 ] 	Batch(5400/7879) done. Loss: 1.4904  lr:0.010000
[ Mon Jul  8 07:27:09 2024 ] 
Training: Epoch [40/120], Step [5499], Loss: 0.20461079478263855, Training Accuracy: 85.32499999999999
[ Mon Jul  8 07:27:09 2024 ] 	Batch(5500/7879) done. Loss: 0.1946  lr:0.010000
[ Mon Jul  8 07:27:32 2024 ] 	Batch(5600/7879) done. Loss: 0.2866  lr:0.010000
[ Mon Jul  8 07:27:55 2024 ] 	Batch(5700/7879) done. Loss: 0.1238  lr:0.010000
[ Mon Jul  8 07:28:19 2024 ] 	Batch(5800/7879) done. Loss: 0.4140  lr:0.010000
[ Mon Jul  8 07:28:42 2024 ] 	Batch(5900/7879) done. Loss: 0.7220  lr:0.010000
[ Mon Jul  8 07:29:05 2024 ] 
Training: Epoch [40/120], Step [5999], Loss: 1.1058220863342285, Training Accuracy: 85.35416666666666
[ Mon Jul  8 07:29:06 2024 ] 	Batch(6000/7879) done. Loss: 0.5145  lr:0.010000
[ Mon Jul  8 07:29:28 2024 ] 	Batch(6100/7879) done. Loss: 0.0610  lr:0.010000
[ Mon Jul  8 07:29:51 2024 ] 	Batch(6200/7879) done. Loss: 0.2168  lr:0.010000
[ Mon Jul  8 07:30:14 2024 ] 	Batch(6300/7879) done. Loss: 0.8751  lr:0.010000
[ Mon Jul  8 07:30:36 2024 ] 	Batch(6400/7879) done. Loss: 0.5333  lr:0.010000
[ Mon Jul  8 07:30:59 2024 ] 
Training: Epoch [40/120], Step [6499], Loss: 1.3589212894439697, Training Accuracy: 85.29807692307692
[ Mon Jul  8 07:31:00 2024 ] 	Batch(6500/7879) done. Loss: 0.4708  lr:0.010000
[ Mon Jul  8 07:31:22 2024 ] 	Batch(6600/7879) done. Loss: 0.5778  lr:0.010000
[ Mon Jul  8 07:31:45 2024 ] 	Batch(6700/7879) done. Loss: 0.4015  lr:0.010000
[ Mon Jul  8 07:32:08 2024 ] 	Batch(6800/7879) done. Loss: 0.5920  lr:0.010000
[ Mon Jul  8 07:32:30 2024 ] 	Batch(6900/7879) done. Loss: 0.5426  lr:0.010000
[ Mon Jul  8 07:32:53 2024 ] 
Training: Epoch [40/120], Step [6999], Loss: 0.09441379457712173, Training Accuracy: 85.225
[ Mon Jul  8 07:32:53 2024 ] 	Batch(7000/7879) done. Loss: 0.8860  lr:0.010000
[ Mon Jul  8 07:33:16 2024 ] 	Batch(7100/7879) done. Loss: 0.6205  lr:0.010000
[ Mon Jul  8 07:33:39 2024 ] 	Batch(7200/7879) done. Loss: 0.2816  lr:0.010000
[ Mon Jul  8 07:34:01 2024 ] 	Batch(7300/7879) done. Loss: 0.5977  lr:0.010000
[ Mon Jul  8 07:34:24 2024 ] 	Batch(7400/7879) done. Loss: 0.1388  lr:0.010000
[ Mon Jul  8 07:34:46 2024 ] 
Training: Epoch [40/120], Step [7499], Loss: 0.7415922284126282, Training Accuracy: 85.18
[ Mon Jul  8 07:34:47 2024 ] 	Batch(7500/7879) done. Loss: 1.2438  lr:0.010000
[ Mon Jul  8 07:35:09 2024 ] 	Batch(7600/7879) done. Loss: 0.6408  lr:0.010000
[ Mon Jul  8 07:35:32 2024 ] 	Batch(7700/7879) done. Loss: 0.5124  lr:0.010000
[ Mon Jul  8 07:35:55 2024 ] 	Batch(7800/7879) done. Loss: 1.0760  lr:0.010000
[ Mon Jul  8 07:36:13 2024 ] 	Mean training loss: 0.4931.
[ Mon Jul  8 07:36:13 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 07:36:13 2024 ] Training epoch: 42
[ Mon Jul  8 07:36:13 2024 ] 	Batch(0/7879) done. Loss: 0.1031  lr:0.010000
[ Mon Jul  8 07:36:36 2024 ] 	Batch(100/7879) done. Loss: 0.2853  lr:0.010000
[ Mon Jul  8 07:36:59 2024 ] 	Batch(200/7879) done. Loss: 0.5494  lr:0.010000
[ Mon Jul  8 07:37:21 2024 ] 	Batch(300/7879) done. Loss: 0.8902  lr:0.010000
[ Mon Jul  8 07:37:44 2024 ] 	Batch(400/7879) done. Loss: 0.5628  lr:0.010000
[ Mon Jul  8 07:38:07 2024 ] 
Training: Epoch [41/120], Step [499], Loss: 0.61891108751297, Training Accuracy: 86.725
[ Mon Jul  8 07:38:07 2024 ] 	Batch(500/7879) done. Loss: 0.4702  lr:0.010000
[ Mon Jul  8 07:38:29 2024 ] 	Batch(600/7879) done. Loss: 0.0526  lr:0.010000
[ Mon Jul  8 07:38:52 2024 ] 	Batch(700/7879) done. Loss: 0.3355  lr:0.010000
[ Mon Jul  8 07:39:15 2024 ] 	Batch(800/7879) done. Loss: 1.0433  lr:0.010000
[ Mon Jul  8 07:39:38 2024 ] 	Batch(900/7879) done. Loss: 1.1668  lr:0.010000
[ Mon Jul  8 07:40:01 2024 ] 
Training: Epoch [41/120], Step [999], Loss: 0.9935584664344788, Training Accuracy: 86.425
[ Mon Jul  8 07:40:02 2024 ] 	Batch(1000/7879) done. Loss: 1.0012  lr:0.010000
[ Mon Jul  8 07:40:24 2024 ] 	Batch(1100/7879) done. Loss: 0.4015  lr:0.010000
[ Mon Jul  8 07:40:47 2024 ] 	Batch(1200/7879) done. Loss: 0.0714  lr:0.010000
[ Mon Jul  8 07:41:10 2024 ] 	Batch(1300/7879) done. Loss: 0.9807  lr:0.010000
[ Mon Jul  8 07:41:33 2024 ] 	Batch(1400/7879) done. Loss: 1.7077  lr:0.010000
[ Mon Jul  8 07:41:55 2024 ] 
Training: Epoch [41/120], Step [1499], Loss: 0.7135799527168274, Training Accuracy: 85.94166666666668
[ Mon Jul  8 07:41:55 2024 ] 	Batch(1500/7879) done. Loss: 0.2047  lr:0.010000
[ Mon Jul  8 07:42:18 2024 ] 	Batch(1600/7879) done. Loss: 0.0242  lr:0.010000
[ Mon Jul  8 07:42:41 2024 ] 	Batch(1700/7879) done. Loss: 0.6448  lr:0.010000
[ Mon Jul  8 07:43:04 2024 ] 	Batch(1800/7879) done. Loss: 0.1123  lr:0.010000
[ Mon Jul  8 07:43:26 2024 ] 	Batch(1900/7879) done. Loss: 0.1273  lr:0.010000
[ Mon Jul  8 07:43:49 2024 ] 
Training: Epoch [41/120], Step [1999], Loss: 0.3319716155529022, Training Accuracy: 85.73125
[ Mon Jul  8 07:43:49 2024 ] 	Batch(2000/7879) done. Loss: 1.0720  lr:0.010000
[ Mon Jul  8 07:44:12 2024 ] 	Batch(2100/7879) done. Loss: 0.2851  lr:0.010000
[ Mon Jul  8 07:44:35 2024 ] 	Batch(2200/7879) done. Loss: 0.0592  lr:0.010000
[ Mon Jul  8 07:44:57 2024 ] 	Batch(2300/7879) done. Loss: 0.2667  lr:0.010000
[ Mon Jul  8 07:45:20 2024 ] 	Batch(2400/7879) done. Loss: 0.5272  lr:0.010000
[ Mon Jul  8 07:45:43 2024 ] 
Training: Epoch [41/120], Step [2499], Loss: 0.012036009691655636, Training Accuracy: 85.71
[ Mon Jul  8 07:45:43 2024 ] 	Batch(2500/7879) done. Loss: 0.7473  lr:0.010000
[ Mon Jul  8 07:46:06 2024 ] 	Batch(2600/7879) done. Loss: 0.0619  lr:0.010000
[ Mon Jul  8 07:46:28 2024 ] 	Batch(2700/7879) done. Loss: 0.9986  lr:0.010000
[ Mon Jul  8 07:46:51 2024 ] 	Batch(2800/7879) done. Loss: 0.2104  lr:0.010000
[ Mon Jul  8 07:47:15 2024 ] 	Batch(2900/7879) done. Loss: 0.4048  lr:0.010000
[ Mon Jul  8 07:47:38 2024 ] 
Training: Epoch [41/120], Step [2999], Loss: 0.638556182384491, Training Accuracy: 85.5125
[ Mon Jul  8 07:47:38 2024 ] 	Batch(3000/7879) done. Loss: 0.6313  lr:0.010000
[ Mon Jul  8 07:48:01 2024 ] 	Batch(3100/7879) done. Loss: 0.5558  lr:0.010000
[ Mon Jul  8 07:48:25 2024 ] 	Batch(3200/7879) done. Loss: 0.3724  lr:0.010000
[ Mon Jul  8 07:48:47 2024 ] 	Batch(3300/7879) done. Loss: 0.6231  lr:0.010000
[ Mon Jul  8 07:49:10 2024 ] 	Batch(3400/7879) done. Loss: 0.5893  lr:0.010000
[ Mon Jul  8 07:49:33 2024 ] 
Training: Epoch [41/120], Step [3499], Loss: 0.8292079567909241, Training Accuracy: 85.26428571428572
[ Mon Jul  8 07:49:33 2024 ] 	Batch(3500/7879) done. Loss: 0.6877  lr:0.010000
[ Mon Jul  8 07:49:56 2024 ] 	Batch(3600/7879) done. Loss: 0.0411  lr:0.010000
[ Mon Jul  8 07:50:18 2024 ] 	Batch(3700/7879) done. Loss: 0.8364  lr:0.010000
[ Mon Jul  8 07:50:41 2024 ] 	Batch(3800/7879) done. Loss: 1.0535  lr:0.010000
[ Mon Jul  8 07:51:04 2024 ] 	Batch(3900/7879) done. Loss: 0.3963  lr:0.010000
[ Mon Jul  8 07:51:26 2024 ] 
Training: Epoch [41/120], Step [3999], Loss: 0.3632347583770752, Training Accuracy: 85.11875
[ Mon Jul  8 07:51:27 2024 ] 	Batch(4000/7879) done. Loss: 0.8032  lr:0.010000
[ Mon Jul  8 07:51:49 2024 ] 	Batch(4100/7879) done. Loss: 1.5057  lr:0.010000
[ Mon Jul  8 07:52:12 2024 ] 	Batch(4200/7879) done. Loss: 0.1951  lr:0.010000
[ Mon Jul  8 07:52:35 2024 ] 	Batch(4300/7879) done. Loss: 0.2053  lr:0.010000
[ Mon Jul  8 07:52:58 2024 ] 	Batch(4400/7879) done. Loss: 0.0907  lr:0.010000
[ Mon Jul  8 07:53:21 2024 ] 
Training: Epoch [41/120], Step [4499], Loss: 0.16419927775859833, Training Accuracy: 85.15833333333333
[ Mon Jul  8 07:53:21 2024 ] 	Batch(4500/7879) done. Loss: 0.3048  lr:0.010000
[ Mon Jul  8 07:53:44 2024 ] 	Batch(4600/7879) done. Loss: 0.2926  lr:0.010000
[ Mon Jul  8 07:54:08 2024 ] 	Batch(4700/7879) done. Loss: 1.0247  lr:0.010000
[ Mon Jul  8 07:54:31 2024 ] 	Batch(4800/7879) done. Loss: 0.1677  lr:0.010000
[ Mon Jul  8 07:54:53 2024 ] 	Batch(4900/7879) done. Loss: 0.5668  lr:0.010000
[ Mon Jul  8 07:55:16 2024 ] 
Training: Epoch [41/120], Step [4999], Loss: 0.5336701273918152, Training Accuracy: 85.06
[ Mon Jul  8 07:55:16 2024 ] 	Batch(5000/7879) done. Loss: 0.4940  lr:0.010000
[ Mon Jul  8 07:55:40 2024 ] 	Batch(5100/7879) done. Loss: 0.7864  lr:0.010000
[ Mon Jul  8 07:56:03 2024 ] 	Batch(5200/7879) done. Loss: 0.5515  lr:0.010000
[ Mon Jul  8 07:56:27 2024 ] 	Batch(5300/7879) done. Loss: 0.9322  lr:0.010000
[ Mon Jul  8 07:56:50 2024 ] 	Batch(5400/7879) done. Loss: 0.9785  lr:0.010000
[ Mon Jul  8 07:57:13 2024 ] 
Training: Epoch [41/120], Step [5499], Loss: 0.9596235156059265, Training Accuracy: 84.98863636363636
[ Mon Jul  8 07:57:14 2024 ] 	Batch(5500/7879) done. Loss: 0.4193  lr:0.010000
[ Mon Jul  8 07:57:37 2024 ] 	Batch(5600/7879) done. Loss: 1.0071  lr:0.010000
[ Mon Jul  8 07:58:01 2024 ] 	Batch(5700/7879) done. Loss: 0.3596  lr:0.010000
[ Mon Jul  8 07:58:24 2024 ] 	Batch(5800/7879) done. Loss: 0.8624  lr:0.010000
[ Mon Jul  8 07:58:47 2024 ] 	Batch(5900/7879) done. Loss: 0.2666  lr:0.010000
[ Mon Jul  8 07:59:11 2024 ] 
Training: Epoch [41/120], Step [5999], Loss: 0.44432753324508667, Training Accuracy: 84.88541666666667
[ Mon Jul  8 07:59:11 2024 ] 	Batch(6000/7879) done. Loss: 0.5310  lr:0.010000
[ Mon Jul  8 07:59:34 2024 ] 	Batch(6100/7879) done. Loss: 0.3056  lr:0.010000
[ Mon Jul  8 07:59:56 2024 ] 	Batch(6200/7879) done. Loss: 0.3915  lr:0.010000
[ Mon Jul  8 08:00:19 2024 ] 	Batch(6300/7879) done. Loss: 0.7443  lr:0.010000
[ Mon Jul  8 08:00:42 2024 ] 	Batch(6400/7879) done. Loss: 0.4051  lr:0.010000
[ Mon Jul  8 08:01:04 2024 ] 
Training: Epoch [41/120], Step [6499], Loss: 0.4970596730709076, Training Accuracy: 84.96923076923078
[ Mon Jul  8 08:01:05 2024 ] 	Batch(6500/7879) done. Loss: 0.2172  lr:0.010000
[ Mon Jul  8 08:01:27 2024 ] 	Batch(6600/7879) done. Loss: 0.3736  lr:0.010000
[ Mon Jul  8 08:01:50 2024 ] 	Batch(6700/7879) done. Loss: 0.5870  lr:0.010000
[ Mon Jul  8 08:02:13 2024 ] 	Batch(6800/7879) done. Loss: 0.0729  lr:0.010000
[ Mon Jul  8 08:02:36 2024 ] 	Batch(6900/7879) done. Loss: 0.4259  lr:0.010000
[ Mon Jul  8 08:02:59 2024 ] 
Training: Epoch [41/120], Step [6999], Loss: 0.3266763687133789, Training Accuracy: 85.00357142857143
[ Mon Jul  8 08:03:00 2024 ] 	Batch(7000/7879) done. Loss: 1.7087  lr:0.010000
[ Mon Jul  8 08:03:23 2024 ] 	Batch(7100/7879) done. Loss: 0.0816  lr:0.010000
[ Mon Jul  8 08:03:47 2024 ] 	Batch(7200/7879) done. Loss: 0.1190  lr:0.010000
[ Mon Jul  8 08:04:10 2024 ] 	Batch(7300/7879) done. Loss: 0.8015  lr:0.010000
[ Mon Jul  8 08:04:34 2024 ] 	Batch(7400/7879) done. Loss: 0.2494  lr:0.010000
[ Mon Jul  8 08:04:57 2024 ] 
Training: Epoch [41/120], Step [7499], Loss: 0.6716284155845642, Training Accuracy: 84.98
[ Mon Jul  8 08:04:57 2024 ] 	Batch(7500/7879) done. Loss: 0.5367  lr:0.010000
[ Mon Jul  8 08:05:21 2024 ] 	Batch(7600/7879) done. Loss: 0.0117  lr:0.010000
[ Mon Jul  8 08:05:43 2024 ] 	Batch(7700/7879) done. Loss: 1.2168  lr:0.010000
[ Mon Jul  8 08:06:06 2024 ] 	Batch(7800/7879) done. Loss: 0.3968  lr:0.010000
[ Mon Jul  8 08:06:24 2024 ] 	Mean training loss: 0.4833.
[ Mon Jul  8 08:06:24 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 08:06:24 2024 ] Training epoch: 43
[ Mon Jul  8 08:06:24 2024 ] 	Batch(0/7879) done. Loss: 0.4108  lr:0.010000
[ Mon Jul  8 08:06:47 2024 ] 	Batch(100/7879) done. Loss: 0.0666  lr:0.010000
[ Mon Jul  8 08:07:10 2024 ] 	Batch(200/7879) done. Loss: 0.0569  lr:0.010000
[ Mon Jul  8 08:07:33 2024 ] 	Batch(300/7879) done. Loss: 0.5924  lr:0.010000
[ Mon Jul  8 08:07:56 2024 ] 	Batch(400/7879) done. Loss: 0.1751  lr:0.010000
[ Mon Jul  8 08:08:18 2024 ] 
Training: Epoch [42/120], Step [499], Loss: 0.42656049132347107, Training Accuracy: 86.425
[ Mon Jul  8 08:08:18 2024 ] 	Batch(500/7879) done. Loss: 0.4524  lr:0.010000
[ Mon Jul  8 08:08:41 2024 ] 	Batch(600/7879) done. Loss: 0.0487  lr:0.010000
[ Mon Jul  8 08:09:04 2024 ] 	Batch(700/7879) done. Loss: 0.9984  lr:0.010000
[ Mon Jul  8 08:09:26 2024 ] 	Batch(800/7879) done. Loss: 0.1455  lr:0.010000
[ Mon Jul  8 08:09:49 2024 ] 	Batch(900/7879) done. Loss: 0.9340  lr:0.010000
[ Mon Jul  8 08:10:12 2024 ] 
Training: Epoch [42/120], Step [999], Loss: 0.42702528834342957, Training Accuracy: 86.2875
[ Mon Jul  8 08:10:12 2024 ] 	Batch(1000/7879) done. Loss: 0.1388  lr:0.010000
[ Mon Jul  8 08:10:35 2024 ] 	Batch(1100/7879) done. Loss: 0.5617  lr:0.010000
[ Mon Jul  8 08:10:57 2024 ] 	Batch(1200/7879) done. Loss: 0.3302  lr:0.010000
[ Mon Jul  8 08:11:20 2024 ] 	Batch(1300/7879) done. Loss: 0.2178  lr:0.010000
[ Mon Jul  8 08:11:43 2024 ] 	Batch(1400/7879) done. Loss: 0.3251  lr:0.010000
[ Mon Jul  8 08:12:05 2024 ] 
Training: Epoch [42/120], Step [1499], Loss: 1.0960330963134766, Training Accuracy: 86.075
[ Mon Jul  8 08:12:06 2024 ] 	Batch(1500/7879) done. Loss: 0.8399  lr:0.010000
[ Mon Jul  8 08:12:28 2024 ] 	Batch(1600/7879) done. Loss: 0.1269  lr:0.010000
[ Mon Jul  8 08:12:51 2024 ] 	Batch(1700/7879) done. Loss: 0.3068  lr:0.010000
[ Mon Jul  8 08:13:14 2024 ] 	Batch(1800/7879) done. Loss: 0.4516  lr:0.010000
[ Mon Jul  8 08:13:37 2024 ] 	Batch(1900/7879) done. Loss: 0.7266  lr:0.010000
[ Mon Jul  8 08:13:59 2024 ] 
Training: Epoch [42/120], Step [1999], Loss: 0.1604703813791275, Training Accuracy: 86.08749999999999
[ Mon Jul  8 08:13:59 2024 ] 	Batch(2000/7879) done. Loss: 0.1913  lr:0.010000
[ Mon Jul  8 08:14:22 2024 ] 	Batch(2100/7879) done. Loss: 0.7577  lr:0.010000
[ Mon Jul  8 08:14:45 2024 ] 	Batch(2200/7879) done. Loss: 0.7451  lr:0.010000
[ Mon Jul  8 08:15:08 2024 ] 	Batch(2300/7879) done. Loss: 0.1376  lr:0.010000
[ Mon Jul  8 08:15:31 2024 ] 	Batch(2400/7879) done. Loss: 0.4896  lr:0.010000
[ Mon Jul  8 08:15:53 2024 ] 
Training: Epoch [42/120], Step [2499], Loss: 0.10951419174671173, Training Accuracy: 86.02
[ Mon Jul  8 08:15:53 2024 ] 	Batch(2500/7879) done. Loss: 0.3044  lr:0.010000
[ Mon Jul  8 08:16:16 2024 ] 	Batch(2600/7879) done. Loss: 0.2350  lr:0.010000
[ Mon Jul  8 08:16:39 2024 ] 	Batch(2700/7879) done. Loss: 0.1683  lr:0.010000
[ Mon Jul  8 08:17:02 2024 ] 	Batch(2800/7879) done. Loss: 0.5556  lr:0.010000
[ Mon Jul  8 08:17:25 2024 ] 	Batch(2900/7879) done. Loss: 0.0939  lr:0.010000
[ Mon Jul  8 08:17:48 2024 ] 
Training: Epoch [42/120], Step [2999], Loss: 0.5558953285217285, Training Accuracy: 86.0125
[ Mon Jul  8 08:17:49 2024 ] 	Batch(3000/7879) done. Loss: 0.3665  lr:0.010000
[ Mon Jul  8 08:18:12 2024 ] 	Batch(3100/7879) done. Loss: 0.7806  lr:0.010000
[ Mon Jul  8 08:18:35 2024 ] 	Batch(3200/7879) done. Loss: 0.5284  lr:0.010000
[ Mon Jul  8 08:18:58 2024 ] 	Batch(3300/7879) done. Loss: 0.4921  lr:0.010000
[ Mon Jul  8 08:19:21 2024 ] 	Batch(3400/7879) done. Loss: 0.4245  lr:0.010000
[ Mon Jul  8 08:19:43 2024 ] 
Training: Epoch [42/120], Step [3499], Loss: 0.19076918065547943, Training Accuracy: 86.075
[ Mon Jul  8 08:19:43 2024 ] 	Batch(3500/7879) done. Loss: 0.0160  lr:0.010000
[ Mon Jul  8 08:20:06 2024 ] 	Batch(3600/7879) done. Loss: 0.6941  lr:0.010000
[ Mon Jul  8 08:20:30 2024 ] 	Batch(3700/7879) done. Loss: 0.8570  lr:0.010000
[ Mon Jul  8 08:20:53 2024 ] 	Batch(3800/7879) done. Loss: 0.0957  lr:0.010000
[ Mon Jul  8 08:21:17 2024 ] 	Batch(3900/7879) done. Loss: 0.3000  lr:0.010000
[ Mon Jul  8 08:21:40 2024 ] 
Training: Epoch [42/120], Step [3999], Loss: 0.10831715911626816, Training Accuracy: 86.07187499999999
[ Mon Jul  8 08:21:40 2024 ] 	Batch(4000/7879) done. Loss: 1.0071  lr:0.010000
[ Mon Jul  8 08:22:03 2024 ] 	Batch(4100/7879) done. Loss: 0.3729  lr:0.010000
[ Mon Jul  8 08:22:27 2024 ] 	Batch(4200/7879) done. Loss: 0.8284  lr:0.010000
[ Mon Jul  8 08:22:49 2024 ] 	Batch(4300/7879) done. Loss: 0.7543  lr:0.010000
[ Mon Jul  8 08:23:12 2024 ] 	Batch(4400/7879) done. Loss: 0.1111  lr:0.010000
[ Mon Jul  8 08:23:35 2024 ] 
Training: Epoch [42/120], Step [4499], Loss: 0.4613248109817505, Training Accuracy: 85.9
[ Mon Jul  8 08:23:35 2024 ] 	Batch(4500/7879) done. Loss: 0.9104  lr:0.010000
[ Mon Jul  8 08:23:57 2024 ] 	Batch(4600/7879) done. Loss: 0.0815  lr:0.010000
[ Mon Jul  8 08:24:20 2024 ] 	Batch(4700/7879) done. Loss: 0.6955  lr:0.010000
[ Mon Jul  8 08:24:43 2024 ] 	Batch(4800/7879) done. Loss: 0.1413  lr:0.010000
[ Mon Jul  8 08:25:06 2024 ] 	Batch(4900/7879) done. Loss: 0.1117  lr:0.010000
[ Mon Jul  8 08:25:30 2024 ] 
Training: Epoch [42/120], Step [4999], Loss: 0.5572110414505005, Training Accuracy: 85.87
[ Mon Jul  8 08:25:30 2024 ] 	Batch(5000/7879) done. Loss: 1.4597  lr:0.010000
[ Mon Jul  8 08:25:53 2024 ] 	Batch(5100/7879) done. Loss: 0.2850  lr:0.010000
[ Mon Jul  8 08:26:17 2024 ] 	Batch(5200/7879) done. Loss: 0.9898  lr:0.010000
[ Mon Jul  8 08:26:40 2024 ] 	Batch(5300/7879) done. Loss: 0.9468  lr:0.010000
[ Mon Jul  8 08:27:04 2024 ] 	Batch(5400/7879) done. Loss: 0.3265  lr:0.010000
[ Mon Jul  8 08:27:27 2024 ] 
Training: Epoch [42/120], Step [5499], Loss: 0.7875955104827881, Training Accuracy: 85.79545454545455
[ Mon Jul  8 08:27:27 2024 ] 	Batch(5500/7879) done. Loss: 0.6899  lr:0.010000
[ Mon Jul  8 08:27:51 2024 ] 	Batch(5600/7879) done. Loss: 1.2735  lr:0.010000
[ Mon Jul  8 08:28:14 2024 ] 	Batch(5700/7879) done. Loss: 0.2310  lr:0.010000
[ Mon Jul  8 08:28:38 2024 ] 	Batch(5800/7879) done. Loss: 0.4188  lr:0.010000
[ Mon Jul  8 08:29:01 2024 ] 	Batch(5900/7879) done. Loss: 0.2176  lr:0.010000
[ Mon Jul  8 08:29:23 2024 ] 
Training: Epoch [42/120], Step [5999], Loss: 1.3685195446014404, Training Accuracy: 85.72916666666667
[ Mon Jul  8 08:29:24 2024 ] 	Batch(6000/7879) done. Loss: 0.1016  lr:0.010000
[ Mon Jul  8 08:29:46 2024 ] 	Batch(6100/7879) done. Loss: 0.4610  lr:0.010000
[ Mon Jul  8 08:30:09 2024 ] 	Batch(6200/7879) done. Loss: 0.0097  lr:0.010000
[ Mon Jul  8 08:30:32 2024 ] 	Batch(6300/7879) done. Loss: 0.2764  lr:0.010000
[ Mon Jul  8 08:30:55 2024 ] 	Batch(6400/7879) done. Loss: 0.3174  lr:0.010000
[ Mon Jul  8 08:31:18 2024 ] 
Training: Epoch [42/120], Step [6499], Loss: 0.06324712187051773, Training Accuracy: 85.65961538461538
[ Mon Jul  8 08:31:18 2024 ] 	Batch(6500/7879) done. Loss: 0.1155  lr:0.010000
[ Mon Jul  8 08:31:42 2024 ] 	Batch(6600/7879) done. Loss: 0.0179  lr:0.010000
[ Mon Jul  8 08:32:05 2024 ] 	Batch(6700/7879) done. Loss: 0.7239  lr:0.010000
[ Mon Jul  8 08:32:29 2024 ] 	Batch(6800/7879) done. Loss: 0.3538  lr:0.010000
[ Mon Jul  8 08:32:51 2024 ] 	Batch(6900/7879) done. Loss: 0.4008  lr:0.010000
[ Mon Jul  8 08:33:14 2024 ] 
Training: Epoch [42/120], Step [6999], Loss: 0.6209214329719543, Training Accuracy: 85.62678571428572
[ Mon Jul  8 08:33:14 2024 ] 	Batch(7000/7879) done. Loss: 0.5546  lr:0.010000
[ Mon Jul  8 08:33:37 2024 ] 	Batch(7100/7879) done. Loss: 0.0833  lr:0.010000
[ Mon Jul  8 08:34:00 2024 ] 	Batch(7200/7879) done. Loss: 0.3069  lr:0.010000
[ Mon Jul  8 08:34:23 2024 ] 	Batch(7300/7879) done. Loss: 0.6422  lr:0.010000
[ Mon Jul  8 08:34:45 2024 ] 	Batch(7400/7879) done. Loss: 1.1488  lr:0.010000
[ Mon Jul  8 08:35:08 2024 ] 
Training: Epoch [42/120], Step [7499], Loss: 0.47379642724990845, Training Accuracy: 85.65166666666667
[ Mon Jul  8 08:35:08 2024 ] 	Batch(7500/7879) done. Loss: 0.3169  lr:0.010000
[ Mon Jul  8 08:35:31 2024 ] 	Batch(7600/7879) done. Loss: 0.2886  lr:0.010000
[ Mon Jul  8 08:35:54 2024 ] 	Batch(7700/7879) done. Loss: 0.4618  lr:0.010000
[ Mon Jul  8 08:36:16 2024 ] 	Batch(7800/7879) done. Loss: 0.3540  lr:0.010000
[ Mon Jul  8 08:36:34 2024 ] 	Mean training loss: 0.4724.
[ Mon Jul  8 08:36:34 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 08:36:34 2024 ] Training epoch: 44
[ Mon Jul  8 08:36:35 2024 ] 	Batch(0/7879) done. Loss: 0.3752  lr:0.010000
[ Mon Jul  8 08:36:58 2024 ] 	Batch(100/7879) done. Loss: 0.3994  lr:0.010000
[ Mon Jul  8 08:37:21 2024 ] 	Batch(200/7879) done. Loss: 0.6773  lr:0.010000
[ Mon Jul  8 08:37:44 2024 ] 	Batch(300/7879) done. Loss: 0.7819  lr:0.010000
[ Mon Jul  8 08:38:07 2024 ] 	Batch(400/7879) done. Loss: 0.2251  lr:0.010000
[ Mon Jul  8 08:38:29 2024 ] 
Training: Epoch [43/120], Step [499], Loss: 0.19295547902584076, Training Accuracy: 86.3
[ Mon Jul  8 08:38:29 2024 ] 	Batch(500/7879) done. Loss: 1.1004  lr:0.010000
[ Mon Jul  8 08:38:52 2024 ] 	Batch(600/7879) done. Loss: 0.0912  lr:0.010000
[ Mon Jul  8 08:39:15 2024 ] 	Batch(700/7879) done. Loss: 0.6753  lr:0.010000
[ Mon Jul  8 08:39:38 2024 ] 	Batch(800/7879) done. Loss: 0.1407  lr:0.010000
[ Mon Jul  8 08:40:00 2024 ] 	Batch(900/7879) done. Loss: 0.2172  lr:0.010000
[ Mon Jul  8 08:40:23 2024 ] 
Training: Epoch [43/120], Step [999], Loss: 0.22971072793006897, Training Accuracy: 85.95
[ Mon Jul  8 08:40:23 2024 ] 	Batch(1000/7879) done. Loss: 1.0900  lr:0.010000
[ Mon Jul  8 08:40:46 2024 ] 	Batch(1100/7879) done. Loss: 0.3921  lr:0.010000
[ Mon Jul  8 08:41:09 2024 ] 	Batch(1200/7879) done. Loss: 0.0699  lr:0.010000
[ Mon Jul  8 08:41:31 2024 ] 	Batch(1300/7879) done. Loss: 0.0797  lr:0.010000
[ Mon Jul  8 08:41:54 2024 ] 	Batch(1400/7879) done. Loss: 0.1080  lr:0.010000
[ Mon Jul  8 08:42:17 2024 ] 
Training: Epoch [43/120], Step [1499], Loss: 0.04163447022438049, Training Accuracy: 86.0
[ Mon Jul  8 08:42:17 2024 ] 	Batch(1500/7879) done. Loss: 0.3891  lr:0.010000
[ Mon Jul  8 08:42:40 2024 ] 	Batch(1600/7879) done. Loss: 0.1506  lr:0.010000
[ Mon Jul  8 08:43:02 2024 ] 	Batch(1700/7879) done. Loss: 0.5500  lr:0.010000
[ Mon Jul  8 08:43:25 2024 ] 	Batch(1800/7879) done. Loss: 0.2561  lr:0.010000
[ Mon Jul  8 08:43:48 2024 ] 	Batch(1900/7879) done. Loss: 0.3080  lr:0.010000
[ Mon Jul  8 08:44:10 2024 ] 
Training: Epoch [43/120], Step [1999], Loss: 0.1556701362133026, Training Accuracy: 86.39375
[ Mon Jul  8 08:44:10 2024 ] 	Batch(2000/7879) done. Loss: 0.6093  lr:0.010000
[ Mon Jul  8 08:44:33 2024 ] 	Batch(2100/7879) done. Loss: 0.5161  lr:0.010000
[ Mon Jul  8 08:44:56 2024 ] 	Batch(2200/7879) done. Loss: 0.0545  lr:0.010000
[ Mon Jul  8 08:45:19 2024 ] 	Batch(2300/7879) done. Loss: 0.2574  lr:0.010000
[ Mon Jul  8 08:45:43 2024 ] 	Batch(2400/7879) done. Loss: 0.3891  lr:0.010000
[ Mon Jul  8 08:46:06 2024 ] 
Training: Epoch [43/120], Step [2499], Loss: 0.15537506341934204, Training Accuracy: 86.39
[ Mon Jul  8 08:46:06 2024 ] 	Batch(2500/7879) done. Loss: 0.1613  lr:0.010000
[ Mon Jul  8 08:46:29 2024 ] 	Batch(2600/7879) done. Loss: 0.1608  lr:0.010000
[ Mon Jul  8 08:46:53 2024 ] 	Batch(2700/7879) done. Loss: 0.5187  lr:0.010000
[ Mon Jul  8 08:47:16 2024 ] 	Batch(2800/7879) done. Loss: 0.1852  lr:0.010000
[ Mon Jul  8 08:47:39 2024 ] 	Batch(2900/7879) done. Loss: 0.4813  lr:0.010000
[ Mon Jul  8 08:48:02 2024 ] 
Training: Epoch [43/120], Step [2999], Loss: 0.08088167011737823, Training Accuracy: 86.44166666666668
[ Mon Jul  8 08:48:03 2024 ] 	Batch(3000/7879) done. Loss: 0.3172  lr:0.010000
[ Mon Jul  8 08:48:25 2024 ] 	Batch(3100/7879) done. Loss: 0.3868  lr:0.010000
[ Mon Jul  8 08:48:48 2024 ] 	Batch(3200/7879) done. Loss: 0.5422  lr:0.010000
[ Mon Jul  8 08:49:11 2024 ] 	Batch(3300/7879) done. Loss: 0.4901  lr:0.010000
[ Mon Jul  8 08:49:33 2024 ] 	Batch(3400/7879) done. Loss: 0.1132  lr:0.010000
[ Mon Jul  8 08:49:56 2024 ] 
Training: Epoch [43/120], Step [3499], Loss: 0.1276426911354065, Training Accuracy: 86.47500000000001
[ Mon Jul  8 08:49:56 2024 ] 	Batch(3500/7879) done. Loss: 0.9812  lr:0.010000
[ Mon Jul  8 08:50:18 2024 ] 	Batch(3600/7879) done. Loss: 0.0516  lr:0.010000
[ Mon Jul  8 08:50:41 2024 ] 	Batch(3700/7879) done. Loss: 0.1660  lr:0.010000
[ Mon Jul  8 08:51:04 2024 ] 	Batch(3800/7879) done. Loss: 1.3977  lr:0.010000
[ Mon Jul  8 08:51:26 2024 ] 	Batch(3900/7879) done. Loss: 0.5584  lr:0.010000
[ Mon Jul  8 08:51:49 2024 ] 
Training: Epoch [43/120], Step [3999], Loss: 0.11938781291246414, Training Accuracy: 86.3375
[ Mon Jul  8 08:51:49 2024 ] 	Batch(4000/7879) done. Loss: 0.5991  lr:0.010000
[ Mon Jul  8 08:52:12 2024 ] 	Batch(4100/7879) done. Loss: 0.1997  lr:0.010000
[ Mon Jul  8 08:52:34 2024 ] 	Batch(4200/7879) done. Loss: 0.2526  lr:0.010000
[ Mon Jul  8 08:52:57 2024 ] 	Batch(4300/7879) done. Loss: 1.0247  lr:0.010000
[ Mon Jul  8 08:53:20 2024 ] 	Batch(4400/7879) done. Loss: 0.3869  lr:0.010000
[ Mon Jul  8 08:53:42 2024 ] 
Training: Epoch [43/120], Step [4499], Loss: 1.0958259105682373, Training Accuracy: 86.31666666666666
[ Mon Jul  8 08:53:42 2024 ] 	Batch(4500/7879) done. Loss: 0.2403  lr:0.010000
[ Mon Jul  8 08:54:05 2024 ] 	Batch(4600/7879) done. Loss: 0.2059  lr:0.010000
[ Mon Jul  8 08:54:28 2024 ] 	Batch(4700/7879) done. Loss: 0.4828  lr:0.010000
[ Mon Jul  8 08:54:51 2024 ] 	Batch(4800/7879) done. Loss: 0.2123  lr:0.010000
[ Mon Jul  8 08:55:15 2024 ] 	Batch(4900/7879) done. Loss: 0.1939  lr:0.010000
[ Mon Jul  8 08:55:37 2024 ] 
Training: Epoch [43/120], Step [4999], Loss: 0.47267526388168335, Training Accuracy: 86.275
[ Mon Jul  8 08:55:38 2024 ] 	Batch(5000/7879) done. Loss: 0.1182  lr:0.010000
[ Mon Jul  8 08:56:01 2024 ] 	Batch(5100/7879) done. Loss: 0.5185  lr:0.010000
[ Mon Jul  8 08:56:24 2024 ] 	Batch(5200/7879) done. Loss: 0.3291  lr:0.010000
[ Mon Jul  8 08:56:48 2024 ] 	Batch(5300/7879) done. Loss: 0.4574  lr:0.010000
[ Mon Jul  8 08:57:11 2024 ] 	Batch(5400/7879) done. Loss: 1.1536  lr:0.010000
[ Mon Jul  8 08:57:34 2024 ] 
Training: Epoch [43/120], Step [5499], Loss: 0.497042715549469, Training Accuracy: 86.16363636363637
[ Mon Jul  8 08:57:35 2024 ] 	Batch(5500/7879) done. Loss: 0.1543  lr:0.010000
[ Mon Jul  8 08:57:58 2024 ] 	Batch(5600/7879) done. Loss: 0.9991  lr:0.010000
[ Mon Jul  8 08:58:22 2024 ] 	Batch(5700/7879) done. Loss: 0.8661  lr:0.010000
[ Mon Jul  8 08:58:45 2024 ] 	Batch(5800/7879) done. Loss: 0.4271  lr:0.010000
[ Mon Jul  8 08:59:08 2024 ] 	Batch(5900/7879) done. Loss: 0.9615  lr:0.010000
[ Mon Jul  8 08:59:31 2024 ] 
Training: Epoch [43/120], Step [5999], Loss: 0.2985960841178894, Training Accuracy: 86.12083333333334
[ Mon Jul  8 08:59:31 2024 ] 	Batch(6000/7879) done. Loss: 0.9626  lr:0.010000
[ Mon Jul  8 08:59:54 2024 ] 	Batch(6100/7879) done. Loss: 0.0651  lr:0.010000
[ Mon Jul  8 09:00:16 2024 ] 	Batch(6200/7879) done. Loss: 0.3083  lr:0.010000
[ Mon Jul  8 09:00:39 2024 ] 	Batch(6300/7879) done. Loss: 0.2564  lr:0.010000
[ Mon Jul  8 09:01:02 2024 ] 	Batch(6400/7879) done. Loss: 0.1173  lr:0.010000
[ Mon Jul  8 09:01:25 2024 ] 
Training: Epoch [43/120], Step [6499], Loss: 0.17356276512145996, Training Accuracy: 86.12115384615385
[ Mon Jul  8 09:01:25 2024 ] 	Batch(6500/7879) done. Loss: 0.5205  lr:0.010000
[ Mon Jul  8 09:01:48 2024 ] 	Batch(6600/7879) done. Loss: 0.0448  lr:0.010000
[ Mon Jul  8 09:02:10 2024 ] 	Batch(6700/7879) done. Loss: 0.2352  lr:0.010000
[ Mon Jul  8 09:02:33 2024 ] 	Batch(6800/7879) done. Loss: 0.1390  lr:0.010000
[ Mon Jul  8 09:02:56 2024 ] 	Batch(6900/7879) done. Loss: 0.5001  lr:0.010000
[ Mon Jul  8 09:03:18 2024 ] 
Training: Epoch [43/120], Step [6999], Loss: 0.344413697719574, Training Accuracy: 86.01785714285714
[ Mon Jul  8 09:03:19 2024 ] 	Batch(7000/7879) done. Loss: 0.1125  lr:0.010000
[ Mon Jul  8 09:03:41 2024 ] 	Batch(7100/7879) done. Loss: 0.0914  lr:0.010000
[ Mon Jul  8 09:04:04 2024 ] 	Batch(7200/7879) done. Loss: 0.2268  lr:0.010000
[ Mon Jul  8 09:04:27 2024 ] 	Batch(7300/7879) done. Loss: 0.5629  lr:0.010000
[ Mon Jul  8 09:04:50 2024 ] 	Batch(7400/7879) done. Loss: 0.3829  lr:0.010000
[ Mon Jul  8 09:05:12 2024 ] 
Training: Epoch [43/120], Step [7499], Loss: 0.8256348967552185, Training Accuracy: 85.995
[ Mon Jul  8 09:05:12 2024 ] 	Batch(7500/7879) done. Loss: 0.0621  lr:0.010000
[ Mon Jul  8 09:05:35 2024 ] 	Batch(7600/7879) done. Loss: 0.3503  lr:0.010000
[ Mon Jul  8 09:05:58 2024 ] 	Batch(7700/7879) done. Loss: 0.1764  lr:0.010000
[ Mon Jul  8 09:06:21 2024 ] 	Batch(7800/7879) done. Loss: 0.3824  lr:0.010000
[ Mon Jul  8 09:06:38 2024 ] 	Mean training loss: 0.4637.
[ Mon Jul  8 09:06:38 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 09:06:38 2024 ] Training epoch: 45
[ Mon Jul  8 09:06:39 2024 ] 	Batch(0/7879) done. Loss: 0.1419  lr:0.010000
[ Mon Jul  8 09:07:02 2024 ] 	Batch(100/7879) done. Loss: 0.8009  lr:0.010000
[ Mon Jul  8 09:07:24 2024 ] 	Batch(200/7879) done. Loss: 0.1592  lr:0.010000
[ Mon Jul  8 09:07:47 2024 ] 	Batch(300/7879) done. Loss: 1.4903  lr:0.010000
[ Mon Jul  8 09:08:10 2024 ] 	Batch(400/7879) done. Loss: 0.4577  lr:0.010000
[ Mon Jul  8 09:08:32 2024 ] 
Training: Epoch [44/120], Step [499], Loss: 0.5363048315048218, Training Accuracy: 88.35
[ Mon Jul  8 09:08:33 2024 ] 	Batch(500/7879) done. Loss: 0.3655  lr:0.010000
[ Mon Jul  8 09:08:55 2024 ] 	Batch(600/7879) done. Loss: 0.2159  lr:0.010000
[ Mon Jul  8 09:09:18 2024 ] 	Batch(700/7879) done. Loss: 0.1262  lr:0.010000
[ Mon Jul  8 09:09:41 2024 ] 	Batch(800/7879) done. Loss: 1.0774  lr:0.010000
[ Mon Jul  8 09:10:04 2024 ] 	Batch(900/7879) done. Loss: 0.0365  lr:0.010000
[ Mon Jul  8 09:10:26 2024 ] 
Training: Epoch [44/120], Step [999], Loss: 0.23247987031936646, Training Accuracy: 87.575
[ Mon Jul  8 09:10:26 2024 ] 	Batch(1000/7879) done. Loss: 0.6860  lr:0.010000
[ Mon Jul  8 09:10:49 2024 ] 	Batch(1100/7879) done. Loss: 1.4262  lr:0.010000
[ Mon Jul  8 09:11:12 2024 ] 	Batch(1200/7879) done. Loss: 0.2083  lr:0.010000
[ Mon Jul  8 09:11:35 2024 ] 	Batch(1300/7879) done. Loss: 0.9503  lr:0.010000
[ Mon Jul  8 09:11:57 2024 ] 	Batch(1400/7879) done. Loss: 0.0404  lr:0.010000
[ Mon Jul  8 09:12:20 2024 ] 
Training: Epoch [44/120], Step [1499], Loss: 0.3740537166595459, Training Accuracy: 87.15833333333333
[ Mon Jul  8 09:12:20 2024 ] 	Batch(1500/7879) done. Loss: 0.4541  lr:0.010000
[ Mon Jul  8 09:12:43 2024 ] 	Batch(1600/7879) done. Loss: 1.1497  lr:0.010000
[ Mon Jul  8 09:13:05 2024 ] 	Batch(1700/7879) done. Loss: 1.2411  lr:0.010000
[ Mon Jul  8 09:13:28 2024 ] 	Batch(1800/7879) done. Loss: 0.3382  lr:0.010000
[ Mon Jul  8 09:13:51 2024 ] 	Batch(1900/7879) done. Loss: 0.7361  lr:0.010000
[ Mon Jul  8 09:14:13 2024 ] 
Training: Epoch [44/120], Step [1999], Loss: 0.19312337040901184, Training Accuracy: 86.96249999999999
[ Mon Jul  8 09:14:14 2024 ] 	Batch(2000/7879) done. Loss: 0.4637  lr:0.010000
[ Mon Jul  8 09:14:37 2024 ] 	Batch(2100/7879) done. Loss: 0.4731  lr:0.010000
[ Mon Jul  8 09:14:59 2024 ] 	Batch(2200/7879) done. Loss: 0.4611  lr:0.010000
[ Mon Jul  8 09:15:22 2024 ] 	Batch(2300/7879) done. Loss: 0.6187  lr:0.010000
[ Mon Jul  8 09:15:45 2024 ] 	Batch(2400/7879) done. Loss: 0.3769  lr:0.010000
[ Mon Jul  8 09:16:07 2024 ] 
Training: Epoch [44/120], Step [2499], Loss: 0.23581735789775848, Training Accuracy: 86.745
[ Mon Jul  8 09:16:08 2024 ] 	Batch(2500/7879) done. Loss: 0.6755  lr:0.010000
[ Mon Jul  8 09:16:30 2024 ] 	Batch(2600/7879) done. Loss: 0.0240  lr:0.010000
[ Mon Jul  8 09:16:53 2024 ] 	Batch(2700/7879) done. Loss: 1.3102  lr:0.010000
[ Mon Jul  8 09:17:16 2024 ] 	Batch(2800/7879) done. Loss: 0.7460  lr:0.010000
[ Mon Jul  8 09:17:39 2024 ] 	Batch(2900/7879) done. Loss: 0.1418  lr:0.010000
[ Mon Jul  8 09:18:01 2024 ] 
Training: Epoch [44/120], Step [2999], Loss: 0.3147125840187073, Training Accuracy: 86.65833333333333
[ Mon Jul  8 09:18:01 2024 ] 	Batch(3000/7879) done. Loss: 0.8422  lr:0.010000
[ Mon Jul  8 09:18:24 2024 ] 	Batch(3100/7879) done. Loss: 0.6999  lr:0.010000
[ Mon Jul  8 09:18:47 2024 ] 	Batch(3200/7879) done. Loss: 0.7315  lr:0.010000
[ Mon Jul  8 09:19:09 2024 ] 	Batch(3300/7879) done. Loss: 0.1244  lr:0.010000
[ Mon Jul  8 09:19:32 2024 ] 	Batch(3400/7879) done. Loss: 0.4557  lr:0.010000
[ Mon Jul  8 09:19:55 2024 ] 
Training: Epoch [44/120], Step [3499], Loss: 0.5989996790885925, Training Accuracy: 86.275
[ Mon Jul  8 09:19:55 2024 ] 	Batch(3500/7879) done. Loss: 0.1881  lr:0.010000
[ Mon Jul  8 09:20:18 2024 ] 	Batch(3600/7879) done. Loss: 0.5527  lr:0.010000
[ Mon Jul  8 09:20:40 2024 ] 	Batch(3700/7879) done. Loss: 0.4823  lr:0.010000
[ Mon Jul  8 09:21:03 2024 ] 	Batch(3800/7879) done. Loss: 0.1847  lr:0.010000
[ Mon Jul  8 09:21:26 2024 ] 	Batch(3900/7879) done. Loss: 0.4421  lr:0.010000
[ Mon Jul  8 09:21:48 2024 ] 
Training: Epoch [44/120], Step [3999], Loss: 0.19670985639095306, Training Accuracy: 86.19375000000001
[ Mon Jul  8 09:21:49 2024 ] 	Batch(4000/7879) done. Loss: 0.1131  lr:0.010000
[ Mon Jul  8 09:22:11 2024 ] 	Batch(4100/7879) done. Loss: 0.4359  lr:0.010000
[ Mon Jul  8 09:22:34 2024 ] 	Batch(4200/7879) done. Loss: 0.5711  lr:0.010000
[ Mon Jul  8 09:22:57 2024 ] 	Batch(4300/7879) done. Loss: 0.9732  lr:0.010000
[ Mon Jul  8 09:23:20 2024 ] 	Batch(4400/7879) done. Loss: 0.3519  lr:0.010000
[ Mon Jul  8 09:23:42 2024 ] 
Training: Epoch [44/120], Step [4499], Loss: 0.9737016558647156, Training Accuracy: 86.22222222222223
[ Mon Jul  8 09:23:42 2024 ] 	Batch(4500/7879) done. Loss: 0.2642  lr:0.010000
[ Mon Jul  8 09:24:05 2024 ] 	Batch(4600/7879) done. Loss: 0.4342  lr:0.010000
[ Mon Jul  8 09:24:28 2024 ] 	Batch(4700/7879) done. Loss: 1.4822  lr:0.010000
[ Mon Jul  8 09:24:50 2024 ] 	Batch(4800/7879) done. Loss: 0.5197  lr:0.010000
[ Mon Jul  8 09:25:13 2024 ] 	Batch(4900/7879) done. Loss: 0.5944  lr:0.010000
[ Mon Jul  8 09:25:36 2024 ] 
Training: Epoch [44/120], Step [4999], Loss: 0.5509920120239258, Training Accuracy: 86.14500000000001
[ Mon Jul  8 09:25:36 2024 ] 	Batch(5000/7879) done. Loss: 0.9862  lr:0.010000
[ Mon Jul  8 09:25:58 2024 ] 	Batch(5100/7879) done. Loss: 0.5634  lr:0.010000
[ Mon Jul  8 09:26:21 2024 ] 	Batch(5200/7879) done. Loss: 0.7265  lr:0.010000
[ Mon Jul  8 09:26:44 2024 ] 	Batch(5300/7879) done. Loss: 0.2889  lr:0.010000
[ Mon Jul  8 09:27:07 2024 ] 	Batch(5400/7879) done. Loss: 0.4591  lr:0.010000
[ Mon Jul  8 09:27:29 2024 ] 
Training: Epoch [44/120], Step [5499], Loss: 0.08355464041233063, Training Accuracy: 85.94772727272726
[ Mon Jul  8 09:27:29 2024 ] 	Batch(5500/7879) done. Loss: 0.3479  lr:0.010000
[ Mon Jul  8 09:27:52 2024 ] 	Batch(5600/7879) done. Loss: 1.0881  lr:0.010000
[ Mon Jul  8 09:28:15 2024 ] 	Batch(5700/7879) done. Loss: 0.9871  lr:0.010000
[ Mon Jul  8 09:28:38 2024 ] 	Batch(5800/7879) done. Loss: 0.3104  lr:0.010000
[ Mon Jul  8 09:29:00 2024 ] 	Batch(5900/7879) done. Loss: 0.8606  lr:0.010000
[ Mon Jul  8 09:29:23 2024 ] 
Training: Epoch [44/120], Step [5999], Loss: 0.11287923902273178, Training Accuracy: 85.83958333333334
[ Mon Jul  8 09:29:23 2024 ] 	Batch(6000/7879) done. Loss: 1.9208  lr:0.010000
[ Mon Jul  8 09:29:46 2024 ] 	Batch(6100/7879) done. Loss: 1.4120  lr:0.010000
[ Mon Jul  8 09:30:09 2024 ] 	Batch(6200/7879) done. Loss: 0.0748  lr:0.010000
[ Mon Jul  8 09:30:31 2024 ] 	Batch(6300/7879) done. Loss: 0.6647  lr:0.010000
[ Mon Jul  8 09:30:54 2024 ] 	Batch(6400/7879) done. Loss: 0.2377  lr:0.010000
[ Mon Jul  8 09:31:17 2024 ] 
Training: Epoch [44/120], Step [6499], Loss: 0.36008498072624207, Training Accuracy: 85.76346153846154
[ Mon Jul  8 09:31:17 2024 ] 	Batch(6500/7879) done. Loss: 0.2599  lr:0.010000
[ Mon Jul  8 09:31:40 2024 ] 	Batch(6600/7879) done. Loss: 0.8349  lr:0.010000
[ Mon Jul  8 09:32:02 2024 ] 	Batch(6700/7879) done. Loss: 0.0568  lr:0.010000
[ Mon Jul  8 09:32:25 2024 ] 	Batch(6800/7879) done. Loss: 0.2157  lr:0.010000
[ Mon Jul  8 09:32:48 2024 ] 	Batch(6900/7879) done. Loss: 0.2450  lr:0.010000
[ Mon Jul  8 09:33:10 2024 ] 
Training: Epoch [44/120], Step [6999], Loss: 0.8326770663261414, Training Accuracy: 85.74285714285715
[ Mon Jul  8 09:33:11 2024 ] 	Batch(7000/7879) done. Loss: 0.9200  lr:0.010000
[ Mon Jul  8 09:33:33 2024 ] 	Batch(7100/7879) done. Loss: 0.2752  lr:0.010000
[ Mon Jul  8 09:33:56 2024 ] 	Batch(7200/7879) done. Loss: 0.5204  lr:0.010000
[ Mon Jul  8 09:34:19 2024 ] 	Batch(7300/7879) done. Loss: 0.7121  lr:0.010000
[ Mon Jul  8 09:34:42 2024 ] 	Batch(7400/7879) done. Loss: 0.6131  lr:0.010000
[ Mon Jul  8 09:35:04 2024 ] 
Training: Epoch [44/120], Step [7499], Loss: 0.014091451652348042, Training Accuracy: 85.76333333333334
[ Mon Jul  8 09:35:05 2024 ] 	Batch(7500/7879) done. Loss: 0.9823  lr:0.010000
[ Mon Jul  8 09:35:28 2024 ] 	Batch(7600/7879) done. Loss: 0.0078  lr:0.010000
[ Mon Jul  8 09:35:50 2024 ] 	Batch(7700/7879) done. Loss: 0.4940  lr:0.010000
[ Mon Jul  8 09:36:13 2024 ] 	Batch(7800/7879) done. Loss: 0.2057  lr:0.010000
[ Mon Jul  8 09:36:31 2024 ] 	Mean training loss: 0.4636.
[ Mon Jul  8 09:36:31 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 09:36:31 2024 ] Training epoch: 46
[ Mon Jul  8 09:36:31 2024 ] 	Batch(0/7879) done. Loss: 0.6436  lr:0.010000
[ Mon Jul  8 09:36:54 2024 ] 	Batch(100/7879) done. Loss: 0.2465  lr:0.010000
[ Mon Jul  8 09:37:17 2024 ] 	Batch(200/7879) done. Loss: 0.5992  lr:0.010000
[ Mon Jul  8 09:37:40 2024 ] 	Batch(300/7879) done. Loss: 0.5290  lr:0.010000
[ Mon Jul  8 09:38:03 2024 ] 	Batch(400/7879) done. Loss: 0.6272  lr:0.010000
[ Mon Jul  8 09:38:25 2024 ] 
Training: Epoch [45/120], Step [499], Loss: 0.20336516201496124, Training Accuracy: 87.575
[ Mon Jul  8 09:38:25 2024 ] 	Batch(500/7879) done. Loss: 0.5605  lr:0.010000
[ Mon Jul  8 09:38:48 2024 ] 	Batch(600/7879) done. Loss: 1.3763  lr:0.010000
[ Mon Jul  8 09:39:11 2024 ] 	Batch(700/7879) done. Loss: 0.0309  lr:0.010000
[ Mon Jul  8 09:39:33 2024 ] 	Batch(800/7879) done. Loss: 0.0179  lr:0.010000
[ Mon Jul  8 09:39:56 2024 ] 	Batch(900/7879) done. Loss: 0.8459  lr:0.010000
[ Mon Jul  8 09:40:19 2024 ] 
Training: Epoch [45/120], Step [999], Loss: 0.13593065738677979, Training Accuracy: 86.5625
[ Mon Jul  8 09:40:19 2024 ] 	Batch(1000/7879) done. Loss: 0.3399  lr:0.010000
[ Mon Jul  8 09:40:42 2024 ] 	Batch(1100/7879) done. Loss: 0.5409  lr:0.010000
[ Mon Jul  8 09:41:04 2024 ] 	Batch(1200/7879) done. Loss: 0.1709  lr:0.010000
[ Mon Jul  8 09:41:27 2024 ] 	Batch(1300/7879) done. Loss: 0.5468  lr:0.010000
[ Mon Jul  8 09:41:50 2024 ] 	Batch(1400/7879) done. Loss: 0.4949  lr:0.010000
[ Mon Jul  8 09:42:12 2024 ] 
Training: Epoch [45/120], Step [1499], Loss: 0.19972701370716095, Training Accuracy: 86.55833333333334
[ Mon Jul  8 09:42:13 2024 ] 	Batch(1500/7879) done. Loss: 0.9142  lr:0.010000
[ Mon Jul  8 09:42:35 2024 ] 	Batch(1600/7879) done. Loss: 0.6258  lr:0.010000
[ Mon Jul  8 09:42:58 2024 ] 	Batch(1700/7879) done. Loss: 0.0612  lr:0.010000
[ Mon Jul  8 09:43:21 2024 ] 	Batch(1800/7879) done. Loss: 0.5659  lr:0.010000
[ Mon Jul  8 09:43:43 2024 ] 	Batch(1900/7879) done. Loss: 0.5072  lr:0.010000
[ Mon Jul  8 09:44:06 2024 ] 
Training: Epoch [45/120], Step [1999], Loss: 0.12515220046043396, Training Accuracy: 86.59375
[ Mon Jul  8 09:44:06 2024 ] 	Batch(2000/7879) done. Loss: 0.0915  lr:0.010000
[ Mon Jul  8 09:44:29 2024 ] 	Batch(2100/7879) done. Loss: 1.0671  lr:0.010000
[ Mon Jul  8 09:44:52 2024 ] 	Batch(2200/7879) done. Loss: 0.6685  lr:0.010000
[ Mon Jul  8 09:45:14 2024 ] 	Batch(2300/7879) done. Loss: 0.4320  lr:0.010000
[ Mon Jul  8 09:45:37 2024 ] 	Batch(2400/7879) done. Loss: 0.5036  lr:0.010000
[ Mon Jul  8 09:45:59 2024 ] 
Training: Epoch [45/120], Step [2499], Loss: 0.1278918981552124, Training Accuracy: 86.3
[ Mon Jul  8 09:46:00 2024 ] 	Batch(2500/7879) done. Loss: 0.7090  lr:0.010000
[ Mon Jul  8 09:46:22 2024 ] 	Batch(2600/7879) done. Loss: 0.3079  lr:0.010000
[ Mon Jul  8 09:46:45 2024 ] 	Batch(2700/7879) done. Loss: 0.2076  lr:0.010000
[ Mon Jul  8 09:47:08 2024 ] 	Batch(2800/7879) done. Loss: 0.0900  lr:0.010000
[ Mon Jul  8 09:47:31 2024 ] 	Batch(2900/7879) done. Loss: 0.7347  lr:0.010000
[ Mon Jul  8 09:47:53 2024 ] 
Training: Epoch [45/120], Step [2999], Loss: 0.22241638600826263, Training Accuracy: 86.29583333333333
[ Mon Jul  8 09:47:53 2024 ] 	Batch(3000/7879) done. Loss: 0.3262  lr:0.010000
[ Mon Jul  8 09:48:16 2024 ] 	Batch(3100/7879) done. Loss: 0.4463  lr:0.010000
[ Mon Jul  8 09:48:39 2024 ] 	Batch(3200/7879) done. Loss: 0.2558  lr:0.010000
[ Mon Jul  8 09:49:01 2024 ] 	Batch(3300/7879) done. Loss: 0.3811  lr:0.010000
[ Mon Jul  8 09:49:24 2024 ] 	Batch(3400/7879) done. Loss: 0.1202  lr:0.010000
[ Mon Jul  8 09:49:47 2024 ] 
Training: Epoch [45/120], Step [3499], Loss: 0.5280253291130066, Training Accuracy: 86.35000000000001
[ Mon Jul  8 09:49:47 2024 ] 	Batch(3500/7879) done. Loss: 0.3300  lr:0.010000
[ Mon Jul  8 09:50:10 2024 ] 	Batch(3600/7879) done. Loss: 0.7386  lr:0.010000
[ Mon Jul  8 09:50:34 2024 ] 	Batch(3700/7879) done. Loss: 0.2819  lr:0.010000
[ Mon Jul  8 09:50:57 2024 ] 	Batch(3800/7879) done. Loss: 0.0939  lr:0.010000
[ Mon Jul  8 09:51:20 2024 ] 	Batch(3900/7879) done. Loss: 0.1558  lr:0.010000
[ Mon Jul  8 09:51:43 2024 ] 
Training: Epoch [45/120], Step [3999], Loss: 0.23238815367221832, Training Accuracy: 86.26875000000001
[ Mon Jul  8 09:51:43 2024 ] 	Batch(4000/7879) done. Loss: 0.4948  lr:0.010000
[ Mon Jul  8 09:52:06 2024 ] 	Batch(4100/7879) done. Loss: 0.0765  lr:0.010000
[ Mon Jul  8 09:52:29 2024 ] 	Batch(4200/7879) done. Loss: 0.5162  lr:0.010000
[ Mon Jul  8 09:52:51 2024 ] 	Batch(4300/7879) done. Loss: 0.2129  lr:0.010000
[ Mon Jul  8 09:53:14 2024 ] 	Batch(4400/7879) done. Loss: 0.0763  lr:0.010000
[ Mon Jul  8 09:53:37 2024 ] 
Training: Epoch [45/120], Step [4499], Loss: 0.4146837294101715, Training Accuracy: 86.26944444444445
[ Mon Jul  8 09:53:37 2024 ] 	Batch(4500/7879) done. Loss: 0.6195  lr:0.010000
[ Mon Jul  8 09:53:59 2024 ] 	Batch(4600/7879) done. Loss: 0.2625  lr:0.010000
[ Mon Jul  8 09:54:22 2024 ] 	Batch(4700/7879) done. Loss: 2.1063  lr:0.010000
[ Mon Jul  8 09:54:45 2024 ] 	Batch(4800/7879) done. Loss: 0.3962  lr:0.010000
[ Mon Jul  8 09:55:08 2024 ] 	Batch(4900/7879) done. Loss: 0.4957  lr:0.010000
[ Mon Jul  8 09:55:30 2024 ] 
Training: Epoch [45/120], Step [4999], Loss: 0.49022549390792847, Training Accuracy: 86.2425
[ Mon Jul  8 09:55:30 2024 ] 	Batch(5000/7879) done. Loss: 0.5709  lr:0.010000
[ Mon Jul  8 09:55:53 2024 ] 	Batch(5100/7879) done. Loss: 0.2401  lr:0.010000
[ Mon Jul  8 09:56:16 2024 ] 	Batch(5200/7879) done. Loss: 0.9561  lr:0.010000
[ Mon Jul  8 09:56:39 2024 ] 	Batch(5300/7879) done. Loss: 0.2025  lr:0.010000
[ Mon Jul  8 09:57:02 2024 ] 	Batch(5400/7879) done. Loss: 0.9961  lr:0.010000
[ Mon Jul  8 09:57:25 2024 ] 
Training: Epoch [45/120], Step [5499], Loss: 0.42851144075393677, Training Accuracy: 86.0840909090909
[ Mon Jul  8 09:57:25 2024 ] 	Batch(5500/7879) done. Loss: 0.5549  lr:0.010000
[ Mon Jul  8 09:57:48 2024 ] 	Batch(5600/7879) done. Loss: 0.2604  lr:0.010000
[ Mon Jul  8 09:58:10 2024 ] 	Batch(5700/7879) done. Loss: 1.0422  lr:0.010000
[ Mon Jul  8 09:58:33 2024 ] 	Batch(5800/7879) done. Loss: 0.0515  lr:0.010000
[ Mon Jul  8 09:58:56 2024 ] 	Batch(5900/7879) done. Loss: 0.9756  lr:0.010000
[ Mon Jul  8 09:59:18 2024 ] 
Training: Epoch [45/120], Step [5999], Loss: 1.435970425605774, Training Accuracy: 86.07083333333333
[ Mon Jul  8 09:59:19 2024 ] 	Batch(6000/7879) done. Loss: 0.0798  lr:0.010000
[ Mon Jul  8 09:59:41 2024 ] 	Batch(6100/7879) done. Loss: 0.0322  lr:0.010000
[ Mon Jul  8 10:00:04 2024 ] 	Batch(6200/7879) done. Loss: 0.2309  lr:0.010000
[ Mon Jul  8 10:00:27 2024 ] 	Batch(6300/7879) done. Loss: 0.1706  lr:0.010000
[ Mon Jul  8 10:00:50 2024 ] 	Batch(6400/7879) done. Loss: 0.4009  lr:0.010000
[ Mon Jul  8 10:01:12 2024 ] 
Training: Epoch [45/120], Step [6499], Loss: 0.8546288013458252, Training Accuracy: 85.97307692307692
[ Mon Jul  8 10:01:12 2024 ] 	Batch(6500/7879) done. Loss: 0.6082  lr:0.010000
[ Mon Jul  8 10:01:35 2024 ] 	Batch(6600/7879) done. Loss: 0.3814  lr:0.010000
[ Mon Jul  8 10:01:58 2024 ] 	Batch(6700/7879) done. Loss: 0.2742  lr:0.010000
[ Mon Jul  8 10:02:20 2024 ] 	Batch(6800/7879) done. Loss: 0.3937  lr:0.010000
[ Mon Jul  8 10:02:43 2024 ] 	Batch(6900/7879) done. Loss: 0.1980  lr:0.010000
[ Mon Jul  8 10:03:06 2024 ] 
Training: Epoch [45/120], Step [6999], Loss: 0.11701793968677521, Training Accuracy: 85.93035714285713
[ Mon Jul  8 10:03:06 2024 ] 	Batch(7000/7879) done. Loss: 0.0841  lr:0.010000
[ Mon Jul  8 10:03:28 2024 ] 	Batch(7100/7879) done. Loss: 0.1144  lr:0.010000
[ Mon Jul  8 10:03:51 2024 ] 	Batch(7200/7879) done. Loss: 0.0321  lr:0.010000
[ Mon Jul  8 10:04:14 2024 ] 	Batch(7300/7879) done. Loss: 0.6565  lr:0.010000
[ Mon Jul  8 10:04:37 2024 ] 	Batch(7400/7879) done. Loss: 0.1237  lr:0.010000
[ Mon Jul  8 10:04:59 2024 ] 
Training: Epoch [45/120], Step [7499], Loss: 0.24602025747299194, Training Accuracy: 85.88166666666666
[ Mon Jul  8 10:04:59 2024 ] 	Batch(7500/7879) done. Loss: 0.1895  lr:0.010000
[ Mon Jul  8 10:05:22 2024 ] 	Batch(7600/7879) done. Loss: 0.8068  lr:0.010000
[ Mon Jul  8 10:05:45 2024 ] 	Batch(7700/7879) done. Loss: 0.3961  lr:0.010000
[ Mon Jul  8 10:06:08 2024 ] 	Batch(7800/7879) done. Loss: 0.4542  lr:0.010000
[ Mon Jul  8 10:06:25 2024 ] 	Mean training loss: 0.4577.
[ Mon Jul  8 10:06:25 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 10:06:25 2024 ] Training epoch: 47
[ Mon Jul  8 10:06:26 2024 ] 	Batch(0/7879) done. Loss: 0.4825  lr:0.010000
[ Mon Jul  8 10:06:49 2024 ] 	Batch(100/7879) done. Loss: 0.4354  lr:0.010000
[ Mon Jul  8 10:07:12 2024 ] 	Batch(200/7879) done. Loss: 0.0785  lr:0.010000
[ Mon Jul  8 10:07:36 2024 ] 	Batch(300/7879) done. Loss: 0.4837  lr:0.010000
[ Mon Jul  8 10:07:59 2024 ] 	Batch(400/7879) done. Loss: 0.0429  lr:0.010000
[ Mon Jul  8 10:08:23 2024 ] 
Training: Epoch [46/120], Step [499], Loss: 0.1938629448413849, Training Accuracy: 87.175
[ Mon Jul  8 10:08:23 2024 ] 	Batch(500/7879) done. Loss: 0.1591  lr:0.010000
[ Mon Jul  8 10:08:46 2024 ] 	Batch(600/7879) done. Loss: 1.1595  lr:0.010000
[ Mon Jul  8 10:09:10 2024 ] 	Batch(700/7879) done. Loss: 0.7553  lr:0.010000
[ Mon Jul  8 10:09:33 2024 ] 	Batch(800/7879) done. Loss: 0.1155  lr:0.010000
[ Mon Jul  8 10:09:57 2024 ] 	Batch(900/7879) done. Loss: 1.0540  lr:0.010000
[ Mon Jul  8 10:10:20 2024 ] 
Training: Epoch [46/120], Step [999], Loss: 0.16522078216075897, Training Accuracy: 86.8
[ Mon Jul  8 10:10:20 2024 ] 	Batch(1000/7879) done. Loss: 0.1858  lr:0.010000
[ Mon Jul  8 10:10:43 2024 ] 	Batch(1100/7879) done. Loss: 1.6041  lr:0.010000
[ Mon Jul  8 10:11:07 2024 ] 	Batch(1200/7879) done. Loss: 0.2764  lr:0.010000
[ Mon Jul  8 10:11:31 2024 ] 	Batch(1300/7879) done. Loss: 0.9130  lr:0.010000
[ Mon Jul  8 10:11:54 2024 ] 	Batch(1400/7879) done. Loss: 0.2431  lr:0.010000
[ Mon Jul  8 10:12:17 2024 ] 
Training: Epoch [46/120], Step [1499], Loss: 0.19575068354606628, Training Accuracy: 86.74166666666666
[ Mon Jul  8 10:12:17 2024 ] 	Batch(1500/7879) done. Loss: 0.6987  lr:0.010000
[ Mon Jul  8 10:12:40 2024 ] 	Batch(1600/7879) done. Loss: 0.6007  lr:0.010000
[ Mon Jul  8 10:13:03 2024 ] 	Batch(1700/7879) done. Loss: 0.2479  lr:0.010000
[ Mon Jul  8 10:13:25 2024 ] 	Batch(1800/7879) done. Loss: 0.8435  lr:0.010000
[ Mon Jul  8 10:13:48 2024 ] 	Batch(1900/7879) done. Loss: 0.1930  lr:0.010000
[ Mon Jul  8 10:14:10 2024 ] 
Training: Epoch [46/120], Step [1999], Loss: 0.9119911789894104, Training Accuracy: 86.6625
[ Mon Jul  8 10:14:11 2024 ] 	Batch(2000/7879) done. Loss: 0.1433  lr:0.010000
[ Mon Jul  8 10:14:33 2024 ] 	Batch(2100/7879) done. Loss: 0.2321  lr:0.010000
[ Mon Jul  8 10:14:56 2024 ] 	Batch(2200/7879) done. Loss: 0.2292  lr:0.010000
[ Mon Jul  8 10:15:19 2024 ] 	Batch(2300/7879) done. Loss: 0.3815  lr:0.010000
[ Mon Jul  8 10:15:42 2024 ] 	Batch(2400/7879) done. Loss: 0.0587  lr:0.010000
[ Mon Jul  8 10:16:04 2024 ] 
Training: Epoch [46/120], Step [2499], Loss: 0.5938571691513062, Training Accuracy: 86.665
[ Mon Jul  8 10:16:04 2024 ] 	Batch(2500/7879) done. Loss: 0.2932  lr:0.010000
[ Mon Jul  8 10:16:27 2024 ] 	Batch(2600/7879) done. Loss: 0.3842  lr:0.010000
[ Mon Jul  8 10:16:50 2024 ] 	Batch(2700/7879) done. Loss: 0.1479  lr:0.010000
[ Mon Jul  8 10:17:14 2024 ] 	Batch(2800/7879) done. Loss: 0.0645  lr:0.010000
[ Mon Jul  8 10:17:37 2024 ] 	Batch(2900/7879) done. Loss: 0.7687  lr:0.010000
[ Mon Jul  8 10:18:00 2024 ] 
Training: Epoch [46/120], Step [2999], Loss: 0.8794914484024048, Training Accuracy: 86.72083333333333
[ Mon Jul  8 10:18:00 2024 ] 	Batch(3000/7879) done. Loss: 0.2417  lr:0.010000
[ Mon Jul  8 10:18:24 2024 ] 	Batch(3100/7879) done. Loss: 0.4820  lr:0.010000
[ Mon Jul  8 10:18:48 2024 ] 	Batch(3200/7879) done. Loss: 0.4048  lr:0.010000
[ Mon Jul  8 10:19:11 2024 ] 	Batch(3300/7879) done. Loss: 0.6093  lr:0.010000
[ Mon Jul  8 10:19:34 2024 ] 	Batch(3400/7879) done. Loss: 0.1254  lr:0.010000
[ Mon Jul  8 10:19:58 2024 ] 
Training: Epoch [46/120], Step [3499], Loss: 0.5471534132957458, Training Accuracy: 86.50357142857142
[ Mon Jul  8 10:19:58 2024 ] 	Batch(3500/7879) done. Loss: 0.2127  lr:0.010000
[ Mon Jul  8 10:20:21 2024 ] 	Batch(3600/7879) done. Loss: 1.4882  lr:0.010000
[ Mon Jul  8 10:20:44 2024 ] 	Batch(3700/7879) done. Loss: 0.0603  lr:0.010000
[ Mon Jul  8 10:21:06 2024 ] 	Batch(3800/7879) done. Loss: 0.9449  lr:0.010000
[ Mon Jul  8 10:21:29 2024 ] 	Batch(3900/7879) done. Loss: 0.2851  lr:0.010000
[ Mon Jul  8 10:21:51 2024 ] 
Training: Epoch [46/120], Step [3999], Loss: 0.6402658820152283, Training Accuracy: 86.3
[ Mon Jul  8 10:21:52 2024 ] 	Batch(4000/7879) done. Loss: 0.7931  lr:0.010000
[ Mon Jul  8 10:22:15 2024 ] 	Batch(4100/7879) done. Loss: 0.7401  lr:0.010000
[ Mon Jul  8 10:22:37 2024 ] 	Batch(4200/7879) done. Loss: 0.7663  lr:0.010000
[ Mon Jul  8 10:23:00 2024 ] 	Batch(4300/7879) done. Loss: 0.3698  lr:0.010000
[ Mon Jul  8 10:23:23 2024 ] 	Batch(4400/7879) done. Loss: 0.6543  lr:0.010000
[ Mon Jul  8 10:23:45 2024 ] 
Training: Epoch [46/120], Step [4499], Loss: 0.2776614725589752, Training Accuracy: 86.40833333333333
[ Mon Jul  8 10:23:46 2024 ] 	Batch(4500/7879) done. Loss: 0.8021  lr:0.010000
[ Mon Jul  8 10:24:08 2024 ] 	Batch(4600/7879) done. Loss: 0.6875  lr:0.010000
[ Mon Jul  8 10:24:31 2024 ] 	Batch(4700/7879) done. Loss: 0.1044  lr:0.010000
[ Mon Jul  8 10:24:54 2024 ] 	Batch(4800/7879) done. Loss: 0.4152  lr:0.010000
[ Mon Jul  8 10:25:17 2024 ] 	Batch(4900/7879) done. Loss: 0.6209  lr:0.010000
[ Mon Jul  8 10:25:39 2024 ] 
Training: Epoch [46/120], Step [4999], Loss: 0.3098956048488617, Training Accuracy: 86.30250000000001
[ Mon Jul  8 10:25:39 2024 ] 	Batch(5000/7879) done. Loss: 0.2585  lr:0.010000
[ Mon Jul  8 10:26:02 2024 ] 	Batch(5100/7879) done. Loss: 0.5488  lr:0.010000
[ Mon Jul  8 10:26:25 2024 ] 	Batch(5200/7879) done. Loss: 0.3159  lr:0.010000
[ Mon Jul  8 10:26:47 2024 ] 	Batch(5300/7879) done. Loss: 0.1530  lr:0.010000
[ Mon Jul  8 10:27:10 2024 ] 	Batch(5400/7879) done. Loss: 0.3344  lr:0.010000
[ Mon Jul  8 10:27:33 2024 ] 
Training: Epoch [46/120], Step [5499], Loss: 0.7896344065666199, Training Accuracy: 86.39090909090909
[ Mon Jul  8 10:27:33 2024 ] 	Batch(5500/7879) done. Loss: 0.4798  lr:0.010000
[ Mon Jul  8 10:27:56 2024 ] 	Batch(5600/7879) done. Loss: 1.0831  lr:0.010000
[ Mon Jul  8 10:28:18 2024 ] 	Batch(5700/7879) done. Loss: 0.6916  lr:0.010000
[ Mon Jul  8 10:28:41 2024 ] 	Batch(5800/7879) done. Loss: 0.2652  lr:0.010000
[ Mon Jul  8 10:29:04 2024 ] 	Batch(5900/7879) done. Loss: 0.7431  lr:0.010000
[ Mon Jul  8 10:29:26 2024 ] 
Training: Epoch [46/120], Step [5999], Loss: 0.8686111569404602, Training Accuracy: 86.375
[ Mon Jul  8 10:29:27 2024 ] 	Batch(6000/7879) done. Loss: 0.0359  lr:0.010000
[ Mon Jul  8 10:29:50 2024 ] 	Batch(6100/7879) done. Loss: 0.6716  lr:0.010000
[ Mon Jul  8 10:30:12 2024 ] 	Batch(6200/7879) done. Loss: 0.6575  lr:0.010000
[ Mon Jul  8 10:30:35 2024 ] 	Batch(6300/7879) done. Loss: 1.2695  lr:0.010000
[ Mon Jul  8 10:30:58 2024 ] 	Batch(6400/7879) done. Loss: 0.0487  lr:0.010000
[ Mon Jul  8 10:31:20 2024 ] 
Training: Epoch [46/120], Step [6499], Loss: 0.98740553855896, Training Accuracy: 86.28461538461538
[ Mon Jul  8 10:31:21 2024 ] 	Batch(6500/7879) done. Loss: 0.1247  lr:0.010000
[ Mon Jul  8 10:31:43 2024 ] 	Batch(6600/7879) done. Loss: 0.3193  lr:0.010000
[ Mon Jul  8 10:32:06 2024 ] 	Batch(6700/7879) done. Loss: 0.4254  lr:0.010000
[ Mon Jul  8 10:32:29 2024 ] 	Batch(6800/7879) done. Loss: 0.5836  lr:0.010000
[ Mon Jul  8 10:32:52 2024 ] 	Batch(6900/7879) done. Loss: 0.4682  lr:0.010000
[ Mon Jul  8 10:33:14 2024 ] 
Training: Epoch [46/120], Step [6999], Loss: 0.22642077505588531, Training Accuracy: 86.22321428571429
[ Mon Jul  8 10:33:14 2024 ] 	Batch(7000/7879) done. Loss: 0.4808  lr:0.010000
[ Mon Jul  8 10:33:37 2024 ] 	Batch(7100/7879) done. Loss: 0.2222  lr:0.010000
[ Mon Jul  8 10:34:00 2024 ] 	Batch(7200/7879) done. Loss: 1.3600  lr:0.010000
[ Mon Jul  8 10:34:23 2024 ] 	Batch(7300/7879) done. Loss: 0.2053  lr:0.010000
[ Mon Jul  8 10:34:45 2024 ] 	Batch(7400/7879) done. Loss: 0.1814  lr:0.010000
[ Mon Jul  8 10:35:08 2024 ] 
Training: Epoch [46/120], Step [7499], Loss: 0.1336549073457718, Training Accuracy: 86.16333333333334
[ Mon Jul  8 10:35:08 2024 ] 	Batch(7500/7879) done. Loss: 0.3099  lr:0.010000
[ Mon Jul  8 10:35:31 2024 ] 	Batch(7600/7879) done. Loss: 0.2028  lr:0.010000
[ Mon Jul  8 10:35:54 2024 ] 	Batch(7700/7879) done. Loss: 0.0412  lr:0.010000
[ Mon Jul  8 10:36:16 2024 ] 	Batch(7800/7879) done. Loss: 0.5353  lr:0.010000
[ Mon Jul  8 10:36:34 2024 ] 	Mean training loss: 0.4460.
[ Mon Jul  8 10:36:34 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 10:36:34 2024 ] Training epoch: 48
[ Mon Jul  8 10:36:35 2024 ] 	Batch(0/7879) done. Loss: 0.1758  lr:0.010000
[ Mon Jul  8 10:36:57 2024 ] 	Batch(100/7879) done. Loss: 0.3478  lr:0.010000
[ Mon Jul  8 10:37:20 2024 ] 	Batch(200/7879) done. Loss: 0.1121  lr:0.010000
[ Mon Jul  8 10:37:43 2024 ] 	Batch(300/7879) done. Loss: 0.4006  lr:0.010000
[ Mon Jul  8 10:38:07 2024 ] 	Batch(400/7879) done. Loss: 0.2967  lr:0.010000
[ Mon Jul  8 10:38:30 2024 ] 
Training: Epoch [47/120], Step [499], Loss: 0.32411226630210876, Training Accuracy: 87.3
[ Mon Jul  8 10:38:30 2024 ] 	Batch(500/7879) done. Loss: 0.3979  lr:0.010000
[ Mon Jul  8 10:38:53 2024 ] 	Batch(600/7879) done. Loss: 0.3174  lr:0.010000
[ Mon Jul  8 10:39:16 2024 ] 	Batch(700/7879) done. Loss: 0.1453  lr:0.010000
[ Mon Jul  8 10:39:39 2024 ] 	Batch(800/7879) done. Loss: 0.4920  lr:0.010000
[ Mon Jul  8 10:40:01 2024 ] 	Batch(900/7879) done. Loss: 0.5401  lr:0.010000
[ Mon Jul  8 10:40:24 2024 ] 
Training: Epoch [47/120], Step [999], Loss: 0.5970636606216431, Training Accuracy: 87.75
[ Mon Jul  8 10:40:24 2024 ] 	Batch(1000/7879) done. Loss: 0.1159  lr:0.010000
[ Mon Jul  8 10:40:47 2024 ] 	Batch(1100/7879) done. Loss: 0.5638  lr:0.010000
[ Mon Jul  8 10:41:10 2024 ] 	Batch(1200/7879) done. Loss: 0.8125  lr:0.010000
[ Mon Jul  8 10:41:33 2024 ] 	Batch(1300/7879) done. Loss: 0.0655  lr:0.010000
[ Mon Jul  8 10:41:56 2024 ] 	Batch(1400/7879) done. Loss: 0.3001  lr:0.010000
[ Mon Jul  8 10:42:18 2024 ] 
Training: Epoch [47/120], Step [1499], Loss: 0.16352203488349915, Training Accuracy: 87.18333333333334
[ Mon Jul  8 10:42:18 2024 ] 	Batch(1500/7879) done. Loss: 0.2421  lr:0.010000
[ Mon Jul  8 10:42:41 2024 ] 	Batch(1600/7879) done. Loss: 0.0858  lr:0.010000
[ Mon Jul  8 10:43:05 2024 ] 	Batch(1700/7879) done. Loss: 0.7949  lr:0.010000
[ Mon Jul  8 10:43:28 2024 ] 	Batch(1800/7879) done. Loss: 0.5248  lr:0.010000
[ Mon Jul  8 10:43:52 2024 ] 	Batch(1900/7879) done. Loss: 0.6716  lr:0.010000
[ Mon Jul  8 10:44:15 2024 ] 
Training: Epoch [47/120], Step [1999], Loss: 0.2670600414276123, Training Accuracy: 87.2
[ Mon Jul  8 10:44:15 2024 ] 	Batch(2000/7879) done. Loss: 0.1322  lr:0.010000
[ Mon Jul  8 10:44:39 2024 ] 	Batch(2100/7879) done. Loss: 0.6424  lr:0.010000
[ Mon Jul  8 10:45:02 2024 ] 	Batch(2200/7879) done. Loss: 1.1784  lr:0.010000
[ Mon Jul  8 10:45:25 2024 ] 	Batch(2300/7879) done. Loss: 1.4089  lr:0.010000
[ Mon Jul  8 10:45:48 2024 ] 	Batch(2400/7879) done. Loss: 0.7884  lr:0.010000
[ Mon Jul  8 10:46:11 2024 ] 
Training: Epoch [47/120], Step [2499], Loss: 0.5133628845214844, Training Accuracy: 87.175
[ Mon Jul  8 10:46:11 2024 ] 	Batch(2500/7879) done. Loss: 0.1503  lr:0.010000
[ Mon Jul  8 10:46:33 2024 ] 	Batch(2600/7879) done. Loss: 0.0390  lr:0.010000
[ Mon Jul  8 10:46:56 2024 ] 	Batch(2700/7879) done. Loss: 0.2841  lr:0.010000
[ Mon Jul  8 10:47:19 2024 ] 	Batch(2800/7879) done. Loss: 0.0730  lr:0.010000
[ Mon Jul  8 10:47:42 2024 ] 	Batch(2900/7879) done. Loss: 0.4121  lr:0.010000
[ Mon Jul  8 10:48:04 2024 ] 
Training: Epoch [47/120], Step [2999], Loss: 0.5307379961013794, Training Accuracy: 86.925
[ Mon Jul  8 10:48:04 2024 ] 	Batch(3000/7879) done. Loss: 0.4450  lr:0.010000
[ Mon Jul  8 10:48:27 2024 ] 	Batch(3100/7879) done. Loss: 0.6542  lr:0.010000
[ Mon Jul  8 10:48:50 2024 ] 	Batch(3200/7879) done. Loss: 0.6355  lr:0.010000
[ Mon Jul  8 10:49:13 2024 ] 	Batch(3300/7879) done. Loss: 0.3224  lr:0.010000
[ Mon Jul  8 10:49:35 2024 ] 	Batch(3400/7879) done. Loss: 0.2629  lr:0.010000
[ Mon Jul  8 10:49:58 2024 ] 
Training: Epoch [47/120], Step [3499], Loss: 0.21422293782234192, Training Accuracy: 86.84642857142858
[ Mon Jul  8 10:49:58 2024 ] 	Batch(3500/7879) done. Loss: 0.3383  lr:0.010000
[ Mon Jul  8 10:50:21 2024 ] 	Batch(3600/7879) done. Loss: 0.8773  lr:0.010000
[ Mon Jul  8 10:50:44 2024 ] 	Batch(3700/7879) done. Loss: 0.1648  lr:0.010000
[ Mon Jul  8 10:51:06 2024 ] 	Batch(3800/7879) done. Loss: 1.0544  lr:0.010000
[ Mon Jul  8 10:51:29 2024 ] 	Batch(3900/7879) done. Loss: 1.1066  lr:0.010000
[ Mon Jul  8 10:51:51 2024 ] 
Training: Epoch [47/120], Step [3999], Loss: 0.7208965420722961, Training Accuracy: 86.81875
[ Mon Jul  8 10:51:52 2024 ] 	Batch(4000/7879) done. Loss: 0.8921  lr:0.010000
[ Mon Jul  8 10:52:15 2024 ] 	Batch(4100/7879) done. Loss: 1.8136  lr:0.010000
[ Mon Jul  8 10:52:37 2024 ] 	Batch(4200/7879) done. Loss: 0.6042  lr:0.010000
[ Mon Jul  8 10:53:00 2024 ] 	Batch(4300/7879) done. Loss: 0.4657  lr:0.010000
[ Mon Jul  8 10:53:23 2024 ] 	Batch(4400/7879) done. Loss: 0.3822  lr:0.010000
[ Mon Jul  8 10:53:45 2024 ] 
Training: Epoch [47/120], Step [4499], Loss: 0.1361524611711502, Training Accuracy: 86.72222222222223
[ Mon Jul  8 10:53:46 2024 ] 	Batch(4500/7879) done. Loss: 0.2208  lr:0.010000
[ Mon Jul  8 10:54:08 2024 ] 	Batch(4600/7879) done. Loss: 0.1170  lr:0.010000
[ Mon Jul  8 10:54:31 2024 ] 	Batch(4700/7879) done. Loss: 0.1579  lr:0.010000
[ Mon Jul  8 10:54:54 2024 ] 	Batch(4800/7879) done. Loss: 1.0255  lr:0.010000
[ Mon Jul  8 10:55:16 2024 ] 	Batch(4900/7879) done. Loss: 0.1646  lr:0.010000
[ Mon Jul  8 10:55:39 2024 ] 
Training: Epoch [47/120], Step [4999], Loss: 0.2945355772972107, Training Accuracy: 86.665
[ Mon Jul  8 10:55:39 2024 ] 	Batch(5000/7879) done. Loss: 1.4305  lr:0.010000
[ Mon Jul  8 10:56:02 2024 ] 	Batch(5100/7879) done. Loss: 0.7329  lr:0.010000
[ Mon Jul  8 10:56:25 2024 ] 	Batch(5200/7879) done. Loss: 0.7294  lr:0.010000
[ Mon Jul  8 10:56:48 2024 ] 	Batch(5300/7879) done. Loss: 0.5459  lr:0.010000
[ Mon Jul  8 10:57:11 2024 ] 	Batch(5400/7879) done. Loss: 0.1169  lr:0.010000
[ Mon Jul  8 10:57:34 2024 ] 
Training: Epoch [47/120], Step [5499], Loss: 0.14185312390327454, Training Accuracy: 86.68636363636364
[ Mon Jul  8 10:57:35 2024 ] 	Batch(5500/7879) done. Loss: 0.1973  lr:0.010000
[ Mon Jul  8 10:57:58 2024 ] 	Batch(5600/7879) done. Loss: 0.7065  lr:0.010000
[ Mon Jul  8 10:58:21 2024 ] 	Batch(5700/7879) done. Loss: 0.5269  lr:0.010000
[ Mon Jul  8 10:58:44 2024 ] 	Batch(5800/7879) done. Loss: 0.0664  lr:0.010000
[ Mon Jul  8 10:59:06 2024 ] 	Batch(5900/7879) done. Loss: 0.5940  lr:0.010000
[ Mon Jul  8 10:59:29 2024 ] 
Training: Epoch [47/120], Step [5999], Loss: 0.10708776116371155, Training Accuracy: 86.65208333333332
[ Mon Jul  8 10:59:29 2024 ] 	Batch(6000/7879) done. Loss: 0.4042  lr:0.010000
[ Mon Jul  8 10:59:52 2024 ] 	Batch(6100/7879) done. Loss: 0.4824  lr:0.010000
[ Mon Jul  8 11:00:15 2024 ] 	Batch(6200/7879) done. Loss: 0.8909  lr:0.010000
[ Mon Jul  8 11:00:37 2024 ] 	Batch(6300/7879) done. Loss: 0.3827  lr:0.010000
[ Mon Jul  8 11:01:00 2024 ] 	Batch(6400/7879) done. Loss: 0.6566  lr:0.010000
[ Mon Jul  8 11:01:23 2024 ] 
Training: Epoch [47/120], Step [6499], Loss: 0.5217375159263611, Training Accuracy: 86.62692307692308
[ Mon Jul  8 11:01:23 2024 ] 	Batch(6500/7879) done. Loss: 0.0479  lr:0.010000
[ Mon Jul  8 11:01:46 2024 ] 	Batch(6600/7879) done. Loss: 0.1255  lr:0.010000
[ Mon Jul  8 11:02:08 2024 ] 	Batch(6700/7879) done. Loss: 0.1255  lr:0.010000
[ Mon Jul  8 11:02:31 2024 ] 	Batch(6800/7879) done. Loss: 0.0890  lr:0.010000
[ Mon Jul  8 11:02:54 2024 ] 	Batch(6900/7879) done. Loss: 0.9132  lr:0.010000
[ Mon Jul  8 11:03:16 2024 ] 
Training: Epoch [47/120], Step [6999], Loss: 0.19484886527061462, Training Accuracy: 86.6
[ Mon Jul  8 11:03:16 2024 ] 	Batch(7000/7879) done. Loss: 0.4148  lr:0.010000
[ Mon Jul  8 11:03:39 2024 ] 	Batch(7100/7879) done. Loss: 0.5284  lr:0.010000
[ Mon Jul  8 11:04:02 2024 ] 	Batch(7200/7879) done. Loss: 0.2866  lr:0.010000
[ Mon Jul  8 11:04:25 2024 ] 	Batch(7300/7879) done. Loss: 0.7609  lr:0.010000
[ Mon Jul  8 11:04:48 2024 ] 	Batch(7400/7879) done. Loss: 0.4688  lr:0.010000
[ Mon Jul  8 11:05:10 2024 ] 
Training: Epoch [47/120], Step [7499], Loss: 0.13145431876182556, Training Accuracy: 86.53333333333333
[ Mon Jul  8 11:05:10 2024 ] 	Batch(7500/7879) done. Loss: 0.6091  lr:0.010000
[ Mon Jul  8 11:05:33 2024 ] 	Batch(7600/7879) done. Loss: 0.2932  lr:0.010000
[ Mon Jul  8 11:05:57 2024 ] 	Batch(7700/7879) done. Loss: 0.2388  lr:0.010000
[ Mon Jul  8 11:06:20 2024 ] 	Batch(7800/7879) done. Loss: 0.9019  lr:0.010000
[ Mon Jul  8 11:06:39 2024 ] 	Mean training loss: 0.4365.
[ Mon Jul  8 11:06:39 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 11:06:39 2024 ] Training epoch: 49
[ Mon Jul  8 11:06:39 2024 ] 	Batch(0/7879) done. Loss: 0.7139  lr:0.010000
[ Mon Jul  8 11:07:02 2024 ] 	Batch(100/7879) done. Loss: 0.2515  lr:0.010000
[ Mon Jul  8 11:07:25 2024 ] 	Batch(200/7879) done. Loss: 0.4702  lr:0.010000
[ Mon Jul  8 11:07:48 2024 ] 	Batch(300/7879) done. Loss: 0.6036  lr:0.010000
[ Mon Jul  8 11:08:11 2024 ] 	Batch(400/7879) done. Loss: 0.2593  lr:0.010000
[ Mon Jul  8 11:08:33 2024 ] 
Training: Epoch [48/120], Step [499], Loss: 0.29102107882499695, Training Accuracy: 87.1
[ Mon Jul  8 11:08:33 2024 ] 	Batch(500/7879) done. Loss: 0.4013  lr:0.010000
[ Mon Jul  8 11:08:56 2024 ] 	Batch(600/7879) done. Loss: 0.7337  lr:0.010000
[ Mon Jul  8 11:09:19 2024 ] 	Batch(700/7879) done. Loss: 0.0294  lr:0.010000
[ Mon Jul  8 11:09:42 2024 ] 	Batch(800/7879) done. Loss: 0.0085  lr:0.010000
[ Mon Jul  8 11:10:05 2024 ] 	Batch(900/7879) done. Loss: 0.5781  lr:0.010000
[ Mon Jul  8 11:10:29 2024 ] 
Training: Epoch [48/120], Step [999], Loss: 0.143030047416687, Training Accuracy: 87.4625
[ Mon Jul  8 11:10:29 2024 ] 	Batch(1000/7879) done. Loss: 0.0659  lr:0.010000
[ Mon Jul  8 11:10:52 2024 ] 	Batch(1100/7879) done. Loss: 0.7190  lr:0.010000
[ Mon Jul  8 11:11:16 2024 ] 	Batch(1200/7879) done. Loss: 0.9322  lr:0.010000
[ Mon Jul  8 11:11:39 2024 ] 	Batch(1300/7879) done. Loss: 0.0778  lr:0.010000
[ Mon Jul  8 11:12:01 2024 ] 	Batch(1400/7879) done. Loss: 0.1077  lr:0.010000
[ Mon Jul  8 11:12:24 2024 ] 
Training: Epoch [48/120], Step [1499], Loss: 0.08681751042604446, Training Accuracy: 87.375
[ Mon Jul  8 11:12:24 2024 ] 	Batch(1500/7879) done. Loss: 0.5719  lr:0.010000
[ Mon Jul  8 11:12:47 2024 ] 	Batch(1600/7879) done. Loss: 0.3770  lr:0.010000
[ Mon Jul  8 11:13:09 2024 ] 	Batch(1700/7879) done. Loss: 0.3485  lr:0.010000
[ Mon Jul  8 11:13:32 2024 ] 	Batch(1800/7879) done. Loss: 1.0205  lr:0.010000
[ Mon Jul  8 11:13:55 2024 ] 	Batch(1900/7879) done. Loss: 0.6894  lr:0.010000
[ Mon Jul  8 11:14:17 2024 ] 
Training: Epoch [48/120], Step [1999], Loss: 0.16493654251098633, Training Accuracy: 87.30624999999999
[ Mon Jul  8 11:14:18 2024 ] 	Batch(2000/7879) done. Loss: 0.0331  lr:0.010000
[ Mon Jul  8 11:14:41 2024 ] 	Batch(2100/7879) done. Loss: 1.7055  lr:0.010000
[ Mon Jul  8 11:15:04 2024 ] 	Batch(2200/7879) done. Loss: 0.2477  lr:0.010000
[ Mon Jul  8 11:15:27 2024 ] 	Batch(2300/7879) done. Loss: 0.9174  lr:0.010000
[ Mon Jul  8 11:15:50 2024 ] 	Batch(2400/7879) done. Loss: 0.1372  lr:0.010000
[ Mon Jul  8 11:16:13 2024 ] 
Training: Epoch [48/120], Step [2499], Loss: 0.5000343322753906, Training Accuracy: 87.225
[ Mon Jul  8 11:16:13 2024 ] 	Batch(2500/7879) done. Loss: 1.2202  lr:0.010000
[ Mon Jul  8 11:16:36 2024 ] 	Batch(2600/7879) done. Loss: 0.5075  lr:0.010000
[ Mon Jul  8 11:16:59 2024 ] 	Batch(2700/7879) done. Loss: 0.2552  lr:0.010000
[ Mon Jul  8 11:17:22 2024 ] 	Batch(2800/7879) done. Loss: 0.5826  lr:0.010000
[ Mon Jul  8 11:17:44 2024 ] 	Batch(2900/7879) done. Loss: 0.4836  lr:0.010000
[ Mon Jul  8 11:18:07 2024 ] 
Training: Epoch [48/120], Step [2999], Loss: 0.4847344160079956, Training Accuracy: 87.25416666666666
[ Mon Jul  8 11:18:07 2024 ] 	Batch(3000/7879) done. Loss: 0.5362  lr:0.010000
[ Mon Jul  8 11:18:30 2024 ] 	Batch(3100/7879) done. Loss: 0.2411  lr:0.010000
[ Mon Jul  8 11:18:53 2024 ] 	Batch(3200/7879) done. Loss: 0.7343  lr:0.010000
[ Mon Jul  8 11:19:15 2024 ] 	Batch(3300/7879) done. Loss: 0.4999  lr:0.010000
[ Mon Jul  8 11:19:38 2024 ] 	Batch(3400/7879) done. Loss: 0.4068  lr:0.010000
[ Mon Jul  8 11:20:01 2024 ] 
Training: Epoch [48/120], Step [3499], Loss: 0.31036242842674255, Training Accuracy: 87.10357142857143
[ Mon Jul  8 11:20:01 2024 ] 	Batch(3500/7879) done. Loss: 0.4612  lr:0.010000
[ Mon Jul  8 11:20:24 2024 ] 	Batch(3600/7879) done. Loss: 0.5071  lr:0.010000
[ Mon Jul  8 11:20:47 2024 ] 	Batch(3700/7879) done. Loss: 0.9575  lr:0.010000
[ Mon Jul  8 11:21:10 2024 ] 	Batch(3800/7879) done. Loss: 0.1880  lr:0.010000
[ Mon Jul  8 11:21:34 2024 ] 	Batch(3900/7879) done. Loss: 0.5285  lr:0.010000
[ Mon Jul  8 11:21:57 2024 ] 
Training: Epoch [48/120], Step [3999], Loss: 0.3751335144042969, Training Accuracy: 87.028125
[ Mon Jul  8 11:21:58 2024 ] 	Batch(4000/7879) done. Loss: 0.2157  lr:0.010000
[ Mon Jul  8 11:22:21 2024 ] 	Batch(4100/7879) done. Loss: 0.2733  lr:0.010000
[ Mon Jul  8 11:22:44 2024 ] 	Batch(4200/7879) done. Loss: 0.2298  lr:0.010000
[ Mon Jul  8 11:23:08 2024 ] 	Batch(4300/7879) done. Loss: 0.2420  lr:0.010000
[ Mon Jul  8 11:23:31 2024 ] 	Batch(4400/7879) done. Loss: 0.4868  lr:0.010000
[ Mon Jul  8 11:23:55 2024 ] 
Training: Epoch [48/120], Step [4499], Loss: 0.08573506027460098, Training Accuracy: 86.95
[ Mon Jul  8 11:23:55 2024 ] 	Batch(4500/7879) done. Loss: 0.0506  lr:0.010000
[ Mon Jul  8 11:24:18 2024 ] 	Batch(4600/7879) done. Loss: 0.1341  lr:0.010000
[ Mon Jul  8 11:24:42 2024 ] 	Batch(4700/7879) done. Loss: 0.4817  lr:0.010000
[ Mon Jul  8 11:25:05 2024 ] 	Batch(4800/7879) done. Loss: 0.4210  lr:0.010000
[ Mon Jul  8 11:25:29 2024 ] 	Batch(4900/7879) done. Loss: 0.4723  lr:0.010000
[ Mon Jul  8 11:25:52 2024 ] 
Training: Epoch [48/120], Step [4999], Loss: 0.5007307529449463, Training Accuracy: 86.92999999999999
[ Mon Jul  8 11:25:52 2024 ] 	Batch(5000/7879) done. Loss: 0.2061  lr:0.010000
[ Mon Jul  8 11:26:16 2024 ] 	Batch(5100/7879) done. Loss: 0.4386  lr:0.010000
[ Mon Jul  8 11:26:39 2024 ] 	Batch(5200/7879) done. Loss: 0.5307  lr:0.010000
[ Mon Jul  8 11:27:03 2024 ] 	Batch(5300/7879) done. Loss: 0.6464  lr:0.010000
[ Mon Jul  8 11:27:26 2024 ] 	Batch(5400/7879) done. Loss: 0.4886  lr:0.010000
[ Mon Jul  8 11:27:49 2024 ] 
Training: Epoch [48/120], Step [5499], Loss: 0.3701532185077667, Training Accuracy: 86.83181818181818
[ Mon Jul  8 11:27:50 2024 ] 	Batch(5500/7879) done. Loss: 0.6109  lr:0.010000
[ Mon Jul  8 11:28:13 2024 ] 	Batch(5600/7879) done. Loss: 0.6114  lr:0.010000
[ Mon Jul  8 11:28:37 2024 ] 	Batch(5700/7879) done. Loss: 0.4108  lr:0.010000
[ Mon Jul  8 11:29:00 2024 ] 	Batch(5800/7879) done. Loss: 0.1051  lr:0.010000
[ Mon Jul  8 11:29:23 2024 ] 	Batch(5900/7879) done. Loss: 0.4804  lr:0.010000
[ Mon Jul  8 11:29:47 2024 ] 
Training: Epoch [48/120], Step [5999], Loss: 0.3155781924724579, Training Accuracy: 86.70625
[ Mon Jul  8 11:29:47 2024 ] 	Batch(6000/7879) done. Loss: 0.2851  lr:0.010000
[ Mon Jul  8 11:30:10 2024 ] 	Batch(6100/7879) done. Loss: 0.5577  lr:0.010000
[ Mon Jul  8 11:30:33 2024 ] 	Batch(6200/7879) done. Loss: 1.3870  lr:0.010000
[ Mon Jul  8 11:30:55 2024 ] 	Batch(6300/7879) done. Loss: 0.0674  lr:0.010000
[ Mon Jul  8 11:31:18 2024 ] 	Batch(6400/7879) done. Loss: 0.2215  lr:0.010000
[ Mon Jul  8 11:31:41 2024 ] 
Training: Epoch [48/120], Step [6499], Loss: 0.6549493074417114, Training Accuracy: 86.61730769230769
[ Mon Jul  8 11:31:41 2024 ] 	Batch(6500/7879) done. Loss: 0.0303  lr:0.010000
[ Mon Jul  8 11:32:04 2024 ] 	Batch(6600/7879) done. Loss: 0.4719  lr:0.010000
[ Mon Jul  8 11:32:26 2024 ] 	Batch(6700/7879) done. Loss: 0.8873  lr:0.010000
[ Mon Jul  8 11:32:50 2024 ] 	Batch(6800/7879) done. Loss: 0.7515  lr:0.010000
[ Mon Jul  8 11:33:12 2024 ] 	Batch(6900/7879) done. Loss: 1.0363  lr:0.010000
[ Mon Jul  8 11:33:35 2024 ] 
Training: Epoch [48/120], Step [6999], Loss: 0.17177198827266693, Training Accuracy: 86.5767857142857
[ Mon Jul  8 11:33:35 2024 ] 	Batch(7000/7879) done. Loss: 0.2262  lr:0.010000
[ Mon Jul  8 11:33:58 2024 ] 	Batch(7100/7879) done. Loss: 0.1810  lr:0.010000
[ Mon Jul  8 11:34:20 2024 ] 	Batch(7200/7879) done. Loss: 0.5522  lr:0.010000
[ Mon Jul  8 11:34:43 2024 ] 	Batch(7300/7879) done. Loss: 0.7688  lr:0.010000
[ Mon Jul  8 11:35:06 2024 ] 	Batch(7400/7879) done. Loss: 0.0463  lr:0.010000
[ Mon Jul  8 11:35:28 2024 ] 
Training: Epoch [48/120], Step [7499], Loss: 0.0439518466591835, Training Accuracy: 86.565
[ Mon Jul  8 11:35:29 2024 ] 	Batch(7500/7879) done. Loss: 0.2437  lr:0.010000
[ Mon Jul  8 11:35:51 2024 ] 	Batch(7600/7879) done. Loss: 0.0749  lr:0.010000
[ Mon Jul  8 11:36:14 2024 ] 	Batch(7700/7879) done. Loss: 0.1316  lr:0.010000
[ Mon Jul  8 11:36:37 2024 ] 	Batch(7800/7879) done. Loss: 0.0846  lr:0.010000
[ Mon Jul  8 11:36:55 2024 ] 	Mean training loss: 0.4424.
[ Mon Jul  8 11:36:55 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 11:36:55 2024 ] Training epoch: 50
[ Mon Jul  8 11:36:55 2024 ] 	Batch(0/7879) done. Loss: 0.6014  lr:0.010000
[ Mon Jul  8 11:37:18 2024 ] 	Batch(100/7879) done. Loss: 0.2491  lr:0.010000
[ Mon Jul  8 11:37:41 2024 ] 	Batch(200/7879) done. Loss: 0.2764  lr:0.010000
[ Mon Jul  8 11:38:03 2024 ] 	Batch(300/7879) done. Loss: 0.1921  lr:0.010000
[ Mon Jul  8 11:38:26 2024 ] 	Batch(400/7879) done. Loss: 0.3286  lr:0.010000
[ Mon Jul  8 11:38:49 2024 ] 
Training: Epoch [49/120], Step [499], Loss: 0.914821982383728, Training Accuracy: 87.64999999999999
[ Mon Jul  8 11:38:49 2024 ] 	Batch(500/7879) done. Loss: 0.1527  lr:0.010000
[ Mon Jul  8 11:39:11 2024 ] 	Batch(600/7879) done. Loss: 0.0796  lr:0.010000
[ Mon Jul  8 11:39:34 2024 ] 	Batch(700/7879) done. Loss: 0.2479  lr:0.010000
[ Mon Jul  8 11:39:57 2024 ] 	Batch(800/7879) done. Loss: 0.6147  lr:0.010000
[ Mon Jul  8 11:40:20 2024 ] 	Batch(900/7879) done. Loss: 0.1316  lr:0.010000
[ Mon Jul  8 11:40:42 2024 ] 
Training: Epoch [49/120], Step [999], Loss: 0.2611125111579895, Training Accuracy: 87.7125
[ Mon Jul  8 11:40:42 2024 ] 	Batch(1000/7879) done. Loss: 0.5761  lr:0.010000
[ Mon Jul  8 11:41:05 2024 ] 	Batch(1100/7879) done. Loss: 0.6341  lr:0.010000
[ Mon Jul  8 11:41:28 2024 ] 	Batch(1200/7879) done. Loss: 0.2507  lr:0.010000
[ Mon Jul  8 11:41:51 2024 ] 	Batch(1300/7879) done. Loss: 0.4291  lr:0.010000
[ Mon Jul  8 11:42:15 2024 ] 	Batch(1400/7879) done. Loss: 0.1497  lr:0.010000
[ Mon Jul  8 11:42:37 2024 ] 
Training: Epoch [49/120], Step [1499], Loss: 1.0963518619537354, Training Accuracy: 87.65833333333333
[ Mon Jul  8 11:42:37 2024 ] 	Batch(1500/7879) done. Loss: 0.1323  lr:0.010000
[ Mon Jul  8 11:43:00 2024 ] 	Batch(1600/7879) done. Loss: 0.7172  lr:0.010000
[ Mon Jul  8 11:43:23 2024 ] 	Batch(1700/7879) done. Loss: 0.0952  lr:0.010000
[ Mon Jul  8 11:43:46 2024 ] 	Batch(1800/7879) done. Loss: 0.1501  lr:0.010000
[ Mon Jul  8 11:44:08 2024 ] 	Batch(1900/7879) done. Loss: 0.7492  lr:0.010000
[ Mon Jul  8 11:44:31 2024 ] 
Training: Epoch [49/120], Step [1999], Loss: 0.8616584539413452, Training Accuracy: 87.86874999999999
[ Mon Jul  8 11:44:31 2024 ] 	Batch(2000/7879) done. Loss: 0.4433  lr:0.010000
[ Mon Jul  8 11:44:54 2024 ] 	Batch(2100/7879) done. Loss: 0.7571  lr:0.010000
[ Mon Jul  8 11:45:17 2024 ] 	Batch(2200/7879) done. Loss: 0.1445  lr:0.010000
[ Mon Jul  8 11:45:39 2024 ] 	Batch(2300/7879) done. Loss: 0.0329  lr:0.010000
[ Mon Jul  8 11:46:02 2024 ] 	Batch(2400/7879) done. Loss: 0.0221  lr:0.010000
[ Mon Jul  8 11:46:26 2024 ] 
Training: Epoch [49/120], Step [2499], Loss: 0.3313266932964325, Training Accuracy: 88.02499999999999
[ Mon Jul  8 11:46:26 2024 ] 	Batch(2500/7879) done. Loss: 0.6430  lr:0.010000
[ Mon Jul  8 11:46:49 2024 ] 	Batch(2600/7879) done. Loss: 0.4220  lr:0.010000
[ Mon Jul  8 11:47:13 2024 ] 	Batch(2700/7879) done. Loss: 0.1270  lr:0.010000
[ Mon Jul  8 11:47:36 2024 ] 	Batch(2800/7879) done. Loss: 0.4169  lr:0.010000
[ Mon Jul  8 11:48:00 2024 ] 	Batch(2900/7879) done. Loss: 0.6416  lr:0.010000
[ Mon Jul  8 11:48:22 2024 ] 
Training: Epoch [49/120], Step [2999], Loss: 0.04345894604921341, Training Accuracy: 87.79583333333333
[ Mon Jul  8 11:48:23 2024 ] 	Batch(3000/7879) done. Loss: 0.1155  lr:0.010000
[ Mon Jul  8 11:48:45 2024 ] 	Batch(3100/7879) done. Loss: 0.2638  lr:0.010000
[ Mon Jul  8 11:49:08 2024 ] 	Batch(3200/7879) done. Loss: 0.6012  lr:0.010000
[ Mon Jul  8 11:49:31 2024 ] 	Batch(3300/7879) done. Loss: 0.5176  lr:0.010000
[ Mon Jul  8 11:49:53 2024 ] 	Batch(3400/7879) done. Loss: 0.3441  lr:0.010000
[ Mon Jul  8 11:50:16 2024 ] 
Training: Epoch [49/120], Step [3499], Loss: 0.2926918566226959, Training Accuracy: 87.575
[ Mon Jul  8 11:50:16 2024 ] 	Batch(3500/7879) done. Loss: 0.0521  lr:0.010000
[ Mon Jul  8 11:50:39 2024 ] 	Batch(3600/7879) done. Loss: 0.9148  lr:0.010000
[ Mon Jul  8 11:51:02 2024 ] 	Batch(3700/7879) done. Loss: 0.3139  lr:0.010000
[ Mon Jul  8 11:51:24 2024 ] 	Batch(3800/7879) done. Loss: 0.1483  lr:0.010000
[ Mon Jul  8 11:51:47 2024 ] 	Batch(3900/7879) done. Loss: 0.2397  lr:0.010000
[ Mon Jul  8 11:52:10 2024 ] 
Training: Epoch [49/120], Step [3999], Loss: 0.8683239817619324, Training Accuracy: 87.465625
[ Mon Jul  8 11:52:10 2024 ] 	Batch(4000/7879) done. Loss: 0.0902  lr:0.010000
[ Mon Jul  8 11:52:33 2024 ] 	Batch(4100/7879) done. Loss: 1.0534  lr:0.010000
[ Mon Jul  8 11:52:56 2024 ] 	Batch(4200/7879) done. Loss: 0.4401  lr:0.010000
[ Mon Jul  8 11:53:18 2024 ] 	Batch(4300/7879) done. Loss: 0.7878  lr:0.010000
[ Mon Jul  8 11:53:41 2024 ] 	Batch(4400/7879) done. Loss: 1.5113  lr:0.010000
[ Mon Jul  8 11:54:04 2024 ] 
Training: Epoch [49/120], Step [4499], Loss: 0.1016036793589592, Training Accuracy: 87.35555555555555
[ Mon Jul  8 11:54:04 2024 ] 	Batch(4500/7879) done. Loss: 1.1954  lr:0.010000
[ Mon Jul  8 11:54:27 2024 ] 	Batch(4600/7879) done. Loss: 0.0564  lr:0.010000
[ Mon Jul  8 11:54:49 2024 ] 	Batch(4700/7879) done. Loss: 0.7092  lr:0.010000
[ Mon Jul  8 11:55:12 2024 ] 	Batch(4800/7879) done. Loss: 0.6092  lr:0.010000
[ Mon Jul  8 11:55:34 2024 ] 	Batch(4900/7879) done. Loss: 0.6634  lr:0.010000
[ Mon Jul  8 11:55:57 2024 ] 
Training: Epoch [49/120], Step [4999], Loss: 0.6295900940895081, Training Accuracy: 87.31750000000001
[ Mon Jul  8 11:55:57 2024 ] 	Batch(5000/7879) done. Loss: 0.1044  lr:0.010000
[ Mon Jul  8 11:56:20 2024 ] 	Batch(5100/7879) done. Loss: 0.9868  lr:0.010000
[ Mon Jul  8 11:56:42 2024 ] 	Batch(5200/7879) done. Loss: 1.1156  lr:0.010000
[ Mon Jul  8 11:57:05 2024 ] 	Batch(5300/7879) done. Loss: 0.1171  lr:0.010000
[ Mon Jul  8 11:57:28 2024 ] 	Batch(5400/7879) done. Loss: 0.6010  lr:0.010000
[ Mon Jul  8 11:57:50 2024 ] 
Training: Epoch [49/120], Step [5499], Loss: 0.13338159024715424, Training Accuracy: 87.15681818181818
[ Mon Jul  8 11:57:50 2024 ] 	Batch(5500/7879) done. Loss: 0.3780  lr:0.010000
[ Mon Jul  8 11:58:13 2024 ] 	Batch(5600/7879) done. Loss: 0.0074  lr:0.010000
[ Mon Jul  8 11:58:36 2024 ] 	Batch(5700/7879) done. Loss: 0.1791  lr:0.010000
[ Mon Jul  8 11:58:59 2024 ] 	Batch(5800/7879) done. Loss: 0.5016  lr:0.010000
[ Mon Jul  8 11:59:22 2024 ] 	Batch(5900/7879) done. Loss: 0.3492  lr:0.010000
[ Mon Jul  8 11:59:44 2024 ] 
Training: Epoch [49/120], Step [5999], Loss: 0.45511889457702637, Training Accuracy: 87.08749999999999
[ Mon Jul  8 11:59:45 2024 ] 	Batch(6000/7879) done. Loss: 0.4495  lr:0.010000
[ Mon Jul  8 12:00:07 2024 ] 	Batch(6100/7879) done. Loss: 0.5529  lr:0.010000
[ Mon Jul  8 12:00:30 2024 ] 	Batch(6200/7879) done. Loss: 0.4389  lr:0.010000
[ Mon Jul  8 12:00:53 2024 ] 	Batch(6300/7879) done. Loss: 0.9328  lr:0.010000
[ Mon Jul  8 12:01:16 2024 ] 	Batch(6400/7879) done. Loss: 0.8197  lr:0.010000
[ Mon Jul  8 12:01:38 2024 ] 
Training: Epoch [49/120], Step [6499], Loss: 0.718379020690918, Training Accuracy: 87.03653846153846
[ Mon Jul  8 12:01:38 2024 ] 	Batch(6500/7879) done. Loss: 0.4271  lr:0.010000
[ Mon Jul  8 12:02:01 2024 ] 	Batch(6600/7879) done. Loss: 0.9827  lr:0.010000
[ Mon Jul  8 12:02:24 2024 ] 	Batch(6700/7879) done. Loss: 0.1517  lr:0.010000
[ Mon Jul  8 12:02:47 2024 ] 	Batch(6800/7879) done. Loss: 0.0239  lr:0.010000
[ Mon Jul  8 12:03:10 2024 ] 	Batch(6900/7879) done. Loss: 0.1968  lr:0.010000
[ Mon Jul  8 12:03:32 2024 ] 
Training: Epoch [49/120], Step [6999], Loss: 0.8616395592689514, Training Accuracy: 87.03214285714286
[ Mon Jul  8 12:03:32 2024 ] 	Batch(7000/7879) done. Loss: 0.1680  lr:0.010000
[ Mon Jul  8 12:03:55 2024 ] 	Batch(7100/7879) done. Loss: 0.1964  lr:0.010000
[ Mon Jul  8 12:04:18 2024 ] 	Batch(7200/7879) done. Loss: 0.0995  lr:0.010000
[ Mon Jul  8 12:04:41 2024 ] 	Batch(7300/7879) done. Loss: 0.0758  lr:0.010000
[ Mon Jul  8 12:05:04 2024 ] 	Batch(7400/7879) done. Loss: 0.3752  lr:0.010000
[ Mon Jul  8 12:05:26 2024 ] 
Training: Epoch [49/120], Step [7499], Loss: 1.8069205284118652, Training Accuracy: 86.89166666666667
[ Mon Jul  8 12:05:27 2024 ] 	Batch(7500/7879) done. Loss: 0.1842  lr:0.010000
[ Mon Jul  8 12:05:50 2024 ] 	Batch(7600/7879) done. Loss: 0.1037  lr:0.010000
[ Mon Jul  8 12:06:12 2024 ] 	Batch(7700/7879) done. Loss: 0.3251  lr:0.010000
[ Mon Jul  8 12:06:35 2024 ] 	Batch(7800/7879) done. Loss: 0.3974  lr:0.010000
[ Mon Jul  8 12:06:53 2024 ] 	Mean training loss: 0.4252.
[ Mon Jul  8 12:06:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 12:06:53 2024 ] Eval epoch: 50
[ Mon Jul  8 12:12:50 2024 ] 	Mean val loss of 6365 batches: 1.2336338588010123.
[ Mon Jul  8 12:12:50 2024 ] 
Validation: Epoch [49/120], Samples [36599.0/50919], Loss: 0.5506160259246826, Validation Accuracy: 71.87690253147156
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 1 : 192 / 275 = 69 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 2 : 204 / 273 = 74 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 3 : 203 / 273 = 74 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 4 : 219 / 275 = 79 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 5 : 223 / 275 = 81 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 6 : 208 / 275 = 75 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 7 : 254 / 273 = 93 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 8 : 260 / 273 = 95 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 9 : 166 / 273 = 60 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 10 : 144 / 273 = 52 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 11 : 120 / 272 = 44 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 12 : 196 / 271 = 72 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 13 : 269 / 275 = 97 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 14 : 247 / 276 = 89 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 15 : 181 / 273 = 66 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 16 : 118 / 274 = 43 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 17 : 234 / 273 = 85 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 18 : 236 / 274 = 86 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 19 : 239 / 272 = 87 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 20 : 255 / 273 = 93 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 21 : 242 / 274 = 88 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 22 : 247 / 274 = 90 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 23 : 242 / 276 = 87 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 24 : 220 / 274 = 80 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 25 : 271 / 275 = 98 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 26 : 254 / 276 = 92 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 27 : 218 / 275 = 79 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 28 : 114 / 275 = 41 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 29 : 215 / 275 = 78 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 30 : 222 / 276 = 80 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 31 : 238 / 276 = 86 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 32 : 236 / 276 = 85 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 33 : 195 / 276 = 70 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 34 : 248 / 276 = 89 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 35 : 230 / 275 = 83 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 36 : 211 / 276 = 76 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 37 : 255 / 276 = 92 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 38 : 239 / 276 = 86 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 39 : 251 / 276 = 90 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 40 : 149 / 276 = 53 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 41 : 252 / 276 = 91 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 42 : 259 / 275 = 94 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 43 : 178 / 276 = 64 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 44 : 234 / 276 = 84 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 45 : 256 / 276 = 92 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 46 : 205 / 276 = 74 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 47 : 200 / 275 = 72 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 48 : 192 / 275 = 69 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 49 : 210 / 274 = 76 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 50 : 243 / 276 = 88 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 51 : 251 / 276 = 90 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 52 : 223 / 276 = 80 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 53 : 246 / 276 = 89 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 54 : 256 / 274 = 93 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 55 : 234 / 276 = 84 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 56 : 220 / 275 = 80 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 57 : 263 / 276 = 95 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 58 : 265 / 273 = 97 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 59 : 256 / 276 = 92 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 60 : 458 / 561 = 81 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 61 : 428 / 566 = 75 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 62 : 382 / 572 = 66 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 63 : 455 / 570 = 79 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 64 : 409 / 574 = 71 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 65 : 454 / 573 = 79 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 66 : 421 / 573 = 73 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 67 : 384 / 575 = 66 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 68 : 262 / 575 = 45 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 69 : 440 / 575 = 76 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 70 : 385 / 575 = 66 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 71 : 28 / 575 = 4 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 72 : 140 / 571 = 24 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 73 : 203 / 570 = 35 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 74 : 310 / 569 = 54 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 75 : 246 / 573 = 42 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 76 : 294 / 574 = 51 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 77 : 317 / 573 = 55 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 78 : 335 / 575 = 58 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 79 : 545 / 574 = 94 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 80 : 375 / 573 = 65 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 81 : 260 / 575 = 45 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 82 : 369 / 575 = 64 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 83 : 237 / 572 = 41 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 84 : 321 / 574 = 55 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 85 : 286 / 574 = 49 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 86 : 439 / 575 = 76 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 87 : 446 / 576 = 77 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 88 : 365 / 575 = 63 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 89 : 389 / 576 = 67 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 90 : 305 / 574 = 53 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 91 : 443 / 568 = 77 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 92 : 366 / 576 = 63 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 93 : 409 / 573 = 71 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 94 : 537 / 574 = 93 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 95 : 500 / 575 = 86 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 96 : 555 / 575 = 96 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 97 : 551 / 574 = 95 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 98 : 526 / 575 = 91 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 99 : 531 / 574 = 92 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 100 : 390 / 574 = 67 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 101 : 539 / 574 = 93 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 102 : 284 / 575 = 49 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 103 : 465 / 576 = 80 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 104 : 258 / 575 = 44 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 105 : 261 / 575 = 45 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 106 : 227 / 576 = 39 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 107 : 475 / 576 = 82 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 108 : 369 / 575 = 64 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 109 : 280 / 575 = 48 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 110 : 473 / 575 = 82 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 111 : 502 / 576 = 87 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 112 : 505 / 575 = 87 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 113 : 468 / 576 = 81 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 114 : 447 / 576 = 77 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 115 : 470 / 576 = 81 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 116 : 471 / 575 = 81 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 117 : 415 / 575 = 72 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 118 : 391 / 575 = 68 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 119 : 456 / 576 = 79 %
[ Mon Jul  8 12:12:50 2024 ] Accuracy of 120 : 239 / 274 = 87 %
[ Mon Jul  8 12:12:50 2024 ] Training epoch: 51
[ Mon Jul  8 12:12:50 2024 ] 	Batch(0/7879) done. Loss: 0.4944  lr:0.010000
[ Mon Jul  8 12:13:13 2024 ] 	Batch(100/7879) done. Loss: 1.6807  lr:0.010000
[ Mon Jul  8 12:13:36 2024 ] 	Batch(200/7879) done. Loss: 0.1807  lr:0.010000
[ Mon Jul  8 12:13:58 2024 ] 	Batch(300/7879) done. Loss: 0.4986  lr:0.010000
[ Mon Jul  8 12:14:21 2024 ] 	Batch(400/7879) done. Loss: 0.2032  lr:0.010000
[ Mon Jul  8 12:14:44 2024 ] 
Training: Epoch [50/120], Step [499], Loss: 0.6029008626937866, Training Accuracy: 87.4
[ Mon Jul  8 12:14:44 2024 ] 	Batch(500/7879) done. Loss: 0.0138  lr:0.010000
[ Mon Jul  8 12:15:07 2024 ] 	Batch(600/7879) done. Loss: 0.8369  lr:0.010000
[ Mon Jul  8 12:15:29 2024 ] 	Batch(700/7879) done. Loss: 0.4409  lr:0.010000
[ Mon Jul  8 12:15:52 2024 ] 	Batch(800/7879) done. Loss: 0.5730  lr:0.010000
[ Mon Jul  8 12:16:15 2024 ] 	Batch(900/7879) done. Loss: 0.0419  lr:0.010000
[ Mon Jul  8 12:16:37 2024 ] 
Training: Epoch [50/120], Step [999], Loss: 0.34174609184265137, Training Accuracy: 87.1
[ Mon Jul  8 12:16:38 2024 ] 	Batch(1000/7879) done. Loss: 0.7382  lr:0.010000
[ Mon Jul  8 12:17:00 2024 ] 	Batch(1100/7879) done. Loss: 0.0647  lr:0.010000
[ Mon Jul  8 12:17:23 2024 ] 	Batch(1200/7879) done. Loss: 0.4861  lr:0.010000
[ Mon Jul  8 12:17:46 2024 ] 	Batch(1300/7879) done. Loss: 0.5325  lr:0.010000
[ Mon Jul  8 12:18:09 2024 ] 	Batch(1400/7879) done. Loss: 0.0119  lr:0.010000
[ Mon Jul  8 12:18:31 2024 ] 
Training: Epoch [50/120], Step [1499], Loss: 0.21119792759418488, Training Accuracy: 87.28333333333333
[ Mon Jul  8 12:18:32 2024 ] 	Batch(1500/7879) done. Loss: 1.5909  lr:0.010000
[ Mon Jul  8 12:18:54 2024 ] 	Batch(1600/7879) done. Loss: 0.6170  lr:0.010000
[ Mon Jul  8 12:19:17 2024 ] 	Batch(1700/7879) done. Loss: 0.9260  lr:0.010000
[ Mon Jul  8 12:19:40 2024 ] 	Batch(1800/7879) done. Loss: 0.3006  lr:0.010000
[ Mon Jul  8 12:20:03 2024 ] 	Batch(1900/7879) done. Loss: 0.1258  lr:0.010000
[ Mon Jul  8 12:20:25 2024 ] 
Training: Epoch [50/120], Step [1999], Loss: 0.3481978476047516, Training Accuracy: 87.33125000000001
[ Mon Jul  8 12:20:26 2024 ] 	Batch(2000/7879) done. Loss: 0.0284  lr:0.010000
[ Mon Jul  8 12:20:49 2024 ] 	Batch(2100/7879) done. Loss: 0.4660  lr:0.010000
[ Mon Jul  8 12:21:12 2024 ] 	Batch(2200/7879) done. Loss: 0.4156  lr:0.010000
[ Mon Jul  8 12:21:35 2024 ] 	Batch(2300/7879) done. Loss: 0.3109  lr:0.010000
[ Mon Jul  8 12:21:58 2024 ] 	Batch(2400/7879) done. Loss: 0.4481  lr:0.010000
[ Mon Jul  8 12:22:21 2024 ] 
Training: Epoch [50/120], Step [2499], Loss: 0.3499270975589752, Training Accuracy: 87.435
[ Mon Jul  8 12:22:21 2024 ] 	Batch(2500/7879) done. Loss: 0.1050  lr:0.010000
[ Mon Jul  8 12:22:43 2024 ] 	Batch(2600/7879) done. Loss: 0.0251  lr:0.010000
[ Mon Jul  8 12:23:06 2024 ] 	Batch(2700/7879) done. Loss: 0.0779  lr:0.010000
[ Mon Jul  8 12:23:29 2024 ] 	Batch(2800/7879) done. Loss: 0.5148  lr:0.010000
[ Mon Jul  8 12:23:52 2024 ] 	Batch(2900/7879) done. Loss: 0.6584  lr:0.010000
[ Mon Jul  8 12:24:14 2024 ] 
Training: Epoch [50/120], Step [2999], Loss: 0.08845648169517517, Training Accuracy: 87.35833333333333
[ Mon Jul  8 12:24:14 2024 ] 	Batch(3000/7879) done. Loss: 1.0021  lr:0.010000
[ Mon Jul  8 12:24:37 2024 ] 	Batch(3100/7879) done. Loss: 0.7402  lr:0.010000
[ Mon Jul  8 12:25:00 2024 ] 	Batch(3200/7879) done. Loss: 0.3820  lr:0.010000
[ Mon Jul  8 12:25:23 2024 ] 	Batch(3300/7879) done. Loss: 0.4059  lr:0.010000
[ Mon Jul  8 12:25:47 2024 ] 	Batch(3400/7879) done. Loss: 0.1281  lr:0.010000
[ Mon Jul  8 12:26:10 2024 ] 
Training: Epoch [50/120], Step [3499], Loss: 0.8699880838394165, Training Accuracy: 87.26785714285714
[ Mon Jul  8 12:26:10 2024 ] 	Batch(3500/7879) done. Loss: 0.4664  lr:0.010000
[ Mon Jul  8 12:26:34 2024 ] 	Batch(3600/7879) done. Loss: 1.0779  lr:0.010000
[ Mon Jul  8 12:26:56 2024 ] 	Batch(3700/7879) done. Loss: 2.7837  lr:0.010000
[ Mon Jul  8 12:27:19 2024 ] 	Batch(3800/7879) done. Loss: 0.3632  lr:0.010000
[ Mon Jul  8 12:27:42 2024 ] 	Batch(3900/7879) done. Loss: 0.9050  lr:0.010000
[ Mon Jul  8 12:28:04 2024 ] 
Training: Epoch [50/120], Step [3999], Loss: 0.5896499156951904, Training Accuracy: 87.215625
[ Mon Jul  8 12:28:05 2024 ] 	Batch(4000/7879) done. Loss: 0.2029  lr:0.010000
[ Mon Jul  8 12:28:27 2024 ] 	Batch(4100/7879) done. Loss: 0.1296  lr:0.010000
[ Mon Jul  8 12:28:50 2024 ] 	Batch(4200/7879) done. Loss: 0.5841  lr:0.010000
[ Mon Jul  8 12:29:13 2024 ] 	Batch(4300/7879) done. Loss: 0.9158  lr:0.010000
[ Mon Jul  8 12:29:36 2024 ] 	Batch(4400/7879) done. Loss: 0.2693  lr:0.010000
[ Mon Jul  8 12:29:58 2024 ] 
Training: Epoch [50/120], Step [4499], Loss: 0.7033079862594604, Training Accuracy: 87.15277777777779
[ Mon Jul  8 12:29:58 2024 ] 	Batch(4500/7879) done. Loss: 0.4803  lr:0.010000
[ Mon Jul  8 12:30:22 2024 ] 	Batch(4600/7879) done. Loss: 0.3996  lr:0.010000
[ Mon Jul  8 12:30:44 2024 ] 	Batch(4700/7879) done. Loss: 0.0345  lr:0.010000
[ Mon Jul  8 12:31:07 2024 ] 	Batch(4800/7879) done. Loss: 0.6286  lr:0.010000
[ Mon Jul  8 12:31:30 2024 ] 	Batch(4900/7879) done. Loss: 0.2812  lr:0.010000
[ Mon Jul  8 12:31:53 2024 ] 
Training: Epoch [50/120], Step [4999], Loss: 0.5946657657623291, Training Accuracy: 87.1225
[ Mon Jul  8 12:31:53 2024 ] 	Batch(5000/7879) done. Loss: 0.1293  lr:0.010000
[ Mon Jul  8 12:32:16 2024 ] 	Batch(5100/7879) done. Loss: 0.3158  lr:0.010000
[ Mon Jul  8 12:32:38 2024 ] 	Batch(5200/7879) done. Loss: 0.1090  lr:0.010000
[ Mon Jul  8 12:33:01 2024 ] 	Batch(5300/7879) done. Loss: 0.7853  lr:0.010000
[ Mon Jul  8 12:33:24 2024 ] 	Batch(5400/7879) done. Loss: 0.2287  lr:0.010000
[ Mon Jul  8 12:33:46 2024 ] 
Training: Epoch [50/120], Step [5499], Loss: 0.32963067293167114, Training Accuracy: 87.06136363636364
[ Mon Jul  8 12:33:47 2024 ] 	Batch(5500/7879) done. Loss: 0.0452  lr:0.010000
[ Mon Jul  8 12:34:09 2024 ] 	Batch(5600/7879) done. Loss: 0.0722  lr:0.010000
[ Mon Jul  8 12:34:32 2024 ] 	Batch(5700/7879) done. Loss: 0.0594  lr:0.010000
[ Mon Jul  8 12:34:55 2024 ] 	Batch(5800/7879) done. Loss: 0.5047  lr:0.010000
[ Mon Jul  8 12:35:18 2024 ] 	Batch(5900/7879) done. Loss: 0.1649  lr:0.010000
[ Mon Jul  8 12:35:40 2024 ] 
Training: Epoch [50/120], Step [5999], Loss: 0.6171360611915588, Training Accuracy: 86.95833333333334
[ Mon Jul  8 12:35:40 2024 ] 	Batch(6000/7879) done. Loss: 0.7405  lr:0.010000
[ Mon Jul  8 12:36:04 2024 ] 	Batch(6100/7879) done. Loss: 0.1374  lr:0.010000
[ Mon Jul  8 12:36:27 2024 ] 	Batch(6200/7879) done. Loss: 0.0756  lr:0.010000
[ Mon Jul  8 12:36:50 2024 ] 	Batch(6300/7879) done. Loss: 0.2164  lr:0.010000
[ Mon Jul  8 12:37:13 2024 ] 	Batch(6400/7879) done. Loss: 0.5092  lr:0.010000
[ Mon Jul  8 12:37:35 2024 ] 
Training: Epoch [50/120], Step [6499], Loss: 0.31007102131843567, Training Accuracy: 86.91153846153846
[ Mon Jul  8 12:37:35 2024 ] 	Batch(6500/7879) done. Loss: 0.5414  lr:0.010000
[ Mon Jul  8 12:37:58 2024 ] 	Batch(6600/7879) done. Loss: 0.4969  lr:0.010000
[ Mon Jul  8 12:38:21 2024 ] 	Batch(6700/7879) done. Loss: 0.7146  lr:0.010000
[ Mon Jul  8 12:38:44 2024 ] 	Batch(6800/7879) done. Loss: 0.3177  lr:0.010000
[ Mon Jul  8 12:39:07 2024 ] 	Batch(6900/7879) done. Loss: 0.3415  lr:0.010000
[ Mon Jul  8 12:39:30 2024 ] 
Training: Epoch [50/120], Step [6999], Loss: 0.21332062780857086, Training Accuracy: 86.88392857142857
[ Mon Jul  8 12:39:31 2024 ] 	Batch(7000/7879) done. Loss: 0.1881  lr:0.010000
[ Mon Jul  8 12:39:54 2024 ] 	Batch(7100/7879) done. Loss: 0.0812  lr:0.010000
[ Mon Jul  8 12:40:18 2024 ] 	Batch(7200/7879) done. Loss: 0.2003  lr:0.010000
[ Mon Jul  8 12:40:41 2024 ] 	Batch(7300/7879) done. Loss: 0.7063  lr:0.010000
[ Mon Jul  8 12:41:05 2024 ] 	Batch(7400/7879) done. Loss: 0.6445  lr:0.010000
[ Mon Jul  8 12:41:28 2024 ] 
Training: Epoch [50/120], Step [7499], Loss: 0.3744526505470276, Training Accuracy: 86.83166666666666
[ Mon Jul  8 12:41:28 2024 ] 	Batch(7500/7879) done. Loss: 0.2160  lr:0.010000
[ Mon Jul  8 12:41:51 2024 ] 	Batch(7600/7879) done. Loss: 0.0920  lr:0.010000
[ Mon Jul  8 12:42:14 2024 ] 	Batch(7700/7879) done. Loss: 0.4679  lr:0.010000
[ Mon Jul  8 12:42:37 2024 ] 	Batch(7800/7879) done. Loss: 1.2558  lr:0.010000
[ Mon Jul  8 12:42:55 2024 ] 	Mean training loss: 0.4224.
[ Mon Jul  8 12:42:55 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 12:42:55 2024 ] Training epoch: 52
[ Mon Jul  8 12:42:55 2024 ] 	Batch(0/7879) done. Loss: 0.1112  lr:0.010000
[ Mon Jul  8 12:43:18 2024 ] 	Batch(100/7879) done. Loss: 0.7142  lr:0.010000
[ Mon Jul  8 12:43:41 2024 ] 	Batch(200/7879) done. Loss: 0.4589  lr:0.010000
[ Mon Jul  8 12:44:03 2024 ] 	Batch(300/7879) done. Loss: 0.0862  lr:0.010000
[ Mon Jul  8 12:44:26 2024 ] 	Batch(400/7879) done. Loss: 0.3735  lr:0.010000
[ Mon Jul  8 12:44:49 2024 ] 
Training: Epoch [51/120], Step [499], Loss: 0.3522436320781708, Training Accuracy: 89.1
[ Mon Jul  8 12:44:49 2024 ] 	Batch(500/7879) done. Loss: 0.2343  lr:0.010000
[ Mon Jul  8 12:45:12 2024 ] 	Batch(600/7879) done. Loss: 0.4268  lr:0.010000
[ Mon Jul  8 12:45:34 2024 ] 	Batch(700/7879) done. Loss: 0.2176  lr:0.010000
[ Mon Jul  8 12:45:57 2024 ] 	Batch(800/7879) done. Loss: 0.0855  lr:0.010000
[ Mon Jul  8 12:46:20 2024 ] 	Batch(900/7879) done. Loss: 0.1494  lr:0.010000
[ Mon Jul  8 12:46:43 2024 ] 
Training: Epoch [51/120], Step [999], Loss: 0.6127190589904785, Training Accuracy: 87.925
[ Mon Jul  8 12:46:43 2024 ] 	Batch(1000/7879) done. Loss: 0.1914  lr:0.010000
[ Mon Jul  8 12:47:05 2024 ] 	Batch(1100/7879) done. Loss: 1.0266  lr:0.010000
[ Mon Jul  8 12:47:28 2024 ] 	Batch(1200/7879) done. Loss: 1.1619  lr:0.010000
[ Mon Jul  8 12:47:51 2024 ] 	Batch(1300/7879) done. Loss: 0.1798  lr:0.010000
[ Mon Jul  8 12:48:14 2024 ] 	Batch(1400/7879) done. Loss: 0.5274  lr:0.010000
[ Mon Jul  8 12:48:36 2024 ] 
Training: Epoch [51/120], Step [1499], Loss: 0.35976266860961914, Training Accuracy: 87.76666666666667
[ Mon Jul  8 12:48:36 2024 ] 	Batch(1500/7879) done. Loss: 0.3260  lr:0.010000
[ Mon Jul  8 12:48:59 2024 ] 	Batch(1600/7879) done. Loss: 0.4014  lr:0.010000
[ Mon Jul  8 12:49:23 2024 ] 	Batch(1700/7879) done. Loss: 0.0262  lr:0.010000
[ Mon Jul  8 12:49:46 2024 ] 	Batch(1800/7879) done. Loss: 0.6750  lr:0.010000
[ Mon Jul  8 12:50:09 2024 ] 	Batch(1900/7879) done. Loss: 0.3526  lr:0.010000
[ Mon Jul  8 12:50:33 2024 ] 
Training: Epoch [51/120], Step [1999], Loss: 0.0852959081530571, Training Accuracy: 87.7
[ Mon Jul  8 12:50:33 2024 ] 	Batch(2000/7879) done. Loss: 0.1557  lr:0.010000
[ Mon Jul  8 12:50:56 2024 ] 	Batch(2100/7879) done. Loss: 0.0396  lr:0.010000
[ Mon Jul  8 12:51:20 2024 ] 	Batch(2200/7879) done. Loss: 0.2647  lr:0.010000
[ Mon Jul  8 12:51:43 2024 ] 	Batch(2300/7879) done. Loss: 0.0167  lr:0.010000
[ Mon Jul  8 12:52:06 2024 ] 	Batch(2400/7879) done. Loss: 0.6665  lr:0.010000
[ Mon Jul  8 12:52:28 2024 ] 
Training: Epoch [51/120], Step [2499], Loss: 0.8288205862045288, Training Accuracy: 87.685
[ Mon Jul  8 12:52:29 2024 ] 	Batch(2500/7879) done. Loss: 0.0652  lr:0.010000
[ Mon Jul  8 12:52:51 2024 ] 	Batch(2600/7879) done. Loss: 0.2523  lr:0.010000
[ Mon Jul  8 12:53:14 2024 ] 	Batch(2700/7879) done. Loss: 0.6671  lr:0.010000
[ Mon Jul  8 12:53:37 2024 ] 	Batch(2800/7879) done. Loss: 0.6242  lr:0.010000
[ Mon Jul  8 12:54:00 2024 ] 	Batch(2900/7879) done. Loss: 0.5751  lr:0.010000
[ Mon Jul  8 12:54:22 2024 ] 
Training: Epoch [51/120], Step [2999], Loss: 0.7747938632965088, Training Accuracy: 87.39583333333333
[ Mon Jul  8 12:54:22 2024 ] 	Batch(3000/7879) done. Loss: 0.2939  lr:0.010000
[ Mon Jul  8 12:54:45 2024 ] 	Batch(3100/7879) done. Loss: 0.2823  lr:0.010000
[ Mon Jul  8 12:55:08 2024 ] 	Batch(3200/7879) done. Loss: 0.6100  lr:0.010000
[ Mon Jul  8 12:55:30 2024 ] 	Batch(3300/7879) done. Loss: 0.2122  lr:0.010000
[ Mon Jul  8 12:55:53 2024 ] 	Batch(3400/7879) done. Loss: 0.2493  lr:0.010000
[ Mon Jul  8 12:56:16 2024 ] 
Training: Epoch [51/120], Step [3499], Loss: 0.5913063287734985, Training Accuracy: 87.23928571428571
[ Mon Jul  8 12:56:16 2024 ] 	Batch(3500/7879) done. Loss: 0.6672  lr:0.010000
[ Mon Jul  8 12:56:39 2024 ] 	Batch(3600/7879) done. Loss: 0.0614  lr:0.010000
[ Mon Jul  8 12:57:01 2024 ] 	Batch(3700/7879) done. Loss: 0.1456  lr:0.010000
[ Mon Jul  8 12:57:25 2024 ] 	Batch(3800/7879) done. Loss: 0.3393  lr:0.010000
[ Mon Jul  8 12:57:48 2024 ] 	Batch(3900/7879) done. Loss: 0.0279  lr:0.010000
[ Mon Jul  8 12:58:12 2024 ] 
Training: Epoch [51/120], Step [3999], Loss: 0.717444121837616, Training Accuracy: 87.15625
[ Mon Jul  8 12:58:12 2024 ] 	Batch(4000/7879) done. Loss: 0.3067  lr:0.010000
[ Mon Jul  8 12:58:35 2024 ] 	Batch(4100/7879) done. Loss: 0.1135  lr:0.010000
[ Mon Jul  8 12:58:57 2024 ] 	Batch(4200/7879) done. Loss: 0.0645  lr:0.010000
[ Mon Jul  8 12:59:20 2024 ] 	Batch(4300/7879) done. Loss: 0.2221  lr:0.010000
[ Mon Jul  8 12:59:43 2024 ] 	Batch(4400/7879) done. Loss: 0.0875  lr:0.010000
[ Mon Jul  8 13:00:05 2024 ] 
Training: Epoch [51/120], Step [4499], Loss: 0.5511803030967712, Training Accuracy: 87.19166666666666
[ Mon Jul  8 13:00:05 2024 ] 	Batch(4500/7879) done. Loss: 0.7718  lr:0.010000
[ Mon Jul  8 13:00:28 2024 ] 	Batch(4600/7879) done. Loss: 0.7070  lr:0.010000
[ Mon Jul  8 13:00:51 2024 ] 	Batch(4700/7879) done. Loss: 0.5863  lr:0.010000
[ Mon Jul  8 13:01:14 2024 ] 	Batch(4800/7879) done. Loss: 0.2445  lr:0.010000
[ Mon Jul  8 13:01:36 2024 ] 	Batch(4900/7879) done. Loss: 0.0688  lr:0.010000
[ Mon Jul  8 13:01:59 2024 ] 
Training: Epoch [51/120], Step [4999], Loss: 0.23143789172172546, Training Accuracy: 87.11
[ Mon Jul  8 13:01:59 2024 ] 	Batch(5000/7879) done. Loss: 0.1721  lr:0.010000
[ Mon Jul  8 13:02:22 2024 ] 	Batch(5100/7879) done. Loss: 0.1139  lr:0.010000
[ Mon Jul  8 13:02:46 2024 ] 	Batch(5200/7879) done. Loss: 0.5275  lr:0.010000
[ Mon Jul  8 13:03:08 2024 ] 	Batch(5300/7879) done. Loss: 0.7103  lr:0.010000
[ Mon Jul  8 13:03:31 2024 ] 	Batch(5400/7879) done. Loss: 1.3621  lr:0.010000
[ Mon Jul  8 13:03:54 2024 ] 
Training: Epoch [51/120], Step [5499], Loss: 0.2726818025112152, Training Accuracy: 87.12727272727273
[ Mon Jul  8 13:03:54 2024 ] 	Batch(5500/7879) done. Loss: 0.2610  lr:0.010000
[ Mon Jul  8 13:04:17 2024 ] 	Batch(5600/7879) done. Loss: 0.3227  lr:0.010000
[ Mon Jul  8 13:04:39 2024 ] 	Batch(5700/7879) done. Loss: 0.2858  lr:0.010000
[ Mon Jul  8 13:05:02 2024 ] 	Batch(5800/7879) done. Loss: 0.8251  lr:0.010000
[ Mon Jul  8 13:05:25 2024 ] 	Batch(5900/7879) done. Loss: 0.8199  lr:0.010000
[ Mon Jul  8 13:05:47 2024 ] 
Training: Epoch [51/120], Step [5999], Loss: 0.7010084986686707, Training Accuracy: 87.06041666666667
[ Mon Jul  8 13:05:47 2024 ] 	Batch(6000/7879) done. Loss: 0.4705  lr:0.010000
[ Mon Jul  8 13:06:11 2024 ] 	Batch(6100/7879) done. Loss: 0.1616  lr:0.010000
[ Mon Jul  8 13:06:34 2024 ] 	Batch(6200/7879) done. Loss: 0.0286  lr:0.010000
[ Mon Jul  8 13:06:58 2024 ] 	Batch(6300/7879) done. Loss: 0.9051  lr:0.010000
[ Mon Jul  8 13:07:21 2024 ] 	Batch(6400/7879) done. Loss: 0.0164  lr:0.010000
[ Mon Jul  8 13:07:44 2024 ] 
Training: Epoch [51/120], Step [6499], Loss: 0.4512764513492584, Training Accuracy: 87.11538461538461
[ Mon Jul  8 13:07:44 2024 ] 	Batch(6500/7879) done. Loss: 0.0514  lr:0.010000
[ Mon Jul  8 13:08:07 2024 ] 	Batch(6600/7879) done. Loss: 0.0442  lr:0.010000
[ Mon Jul  8 13:08:29 2024 ] 	Batch(6700/7879) done. Loss: 0.3984  lr:0.010000
[ Mon Jul  8 13:08:52 2024 ] 	Batch(6800/7879) done. Loss: 0.4321  lr:0.010000
[ Mon Jul  8 13:09:15 2024 ] 	Batch(6900/7879) done. Loss: 0.1590  lr:0.010000
[ Mon Jul  8 13:09:37 2024 ] 
Training: Epoch [51/120], Step [6999], Loss: 0.3678639829158783, Training Accuracy: 87.0767857142857
[ Mon Jul  8 13:09:38 2024 ] 	Batch(7000/7879) done. Loss: 0.4447  lr:0.010000
[ Mon Jul  8 13:10:00 2024 ] 	Batch(7100/7879) done. Loss: 0.1191  lr:0.010000
[ Mon Jul  8 13:10:23 2024 ] 	Batch(7200/7879) done. Loss: 0.0882  lr:0.010000
[ Mon Jul  8 13:10:46 2024 ] 	Batch(7300/7879) done. Loss: 0.3990  lr:0.010000
[ Mon Jul  8 13:11:08 2024 ] 	Batch(7400/7879) done. Loss: 0.8306  lr:0.010000
[ Mon Jul  8 13:11:31 2024 ] 
Training: Epoch [51/120], Step [7499], Loss: 0.10682651400566101, Training Accuracy: 86.99833333333333
[ Mon Jul  8 13:11:31 2024 ] 	Batch(7500/7879) done. Loss: 0.5215  lr:0.010000
[ Mon Jul  8 13:11:54 2024 ] 	Batch(7600/7879) done. Loss: 0.1083  lr:0.010000
[ Mon Jul  8 13:12:17 2024 ] 	Batch(7700/7879) done. Loss: 0.3471  lr:0.010000
[ Mon Jul  8 13:12:39 2024 ] 	Batch(7800/7879) done. Loss: 0.2036  lr:0.010000
[ Mon Jul  8 13:12:57 2024 ] 	Mean training loss: 0.4183.
[ Mon Jul  8 13:12:57 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 13:12:57 2024 ] Training epoch: 53
[ Mon Jul  8 13:12:58 2024 ] 	Batch(0/7879) done. Loss: 0.3935  lr:0.010000
[ Mon Jul  8 13:13:21 2024 ] 	Batch(100/7879) done. Loss: 0.3462  lr:0.010000
[ Mon Jul  8 13:13:45 2024 ] 	Batch(200/7879) done. Loss: 1.3383  lr:0.010000
[ Mon Jul  8 13:14:08 2024 ] 	Batch(300/7879) done. Loss: 0.1151  lr:0.010000
[ Mon Jul  8 13:14:32 2024 ] 	Batch(400/7879) done. Loss: 0.1083  lr:0.010000
[ Mon Jul  8 13:14:55 2024 ] 
Training: Epoch [52/120], Step [499], Loss: 0.14820638298988342, Training Accuracy: 88.02499999999999
[ Mon Jul  8 13:14:55 2024 ] 	Batch(500/7879) done. Loss: 0.5022  lr:0.010000
[ Mon Jul  8 13:15:19 2024 ] 	Batch(600/7879) done. Loss: 0.0594  lr:0.010000
[ Mon Jul  8 13:15:42 2024 ] 	Batch(700/7879) done. Loss: 0.4244  lr:0.010000
[ Mon Jul  8 13:16:06 2024 ] 	Batch(800/7879) done. Loss: 0.2201  lr:0.010000
[ Mon Jul  8 13:16:28 2024 ] 	Batch(900/7879) done. Loss: 0.3224  lr:0.010000
[ Mon Jul  8 13:16:51 2024 ] 
Training: Epoch [52/120], Step [999], Loss: 0.3073772192001343, Training Accuracy: 87.9625
[ Mon Jul  8 13:16:51 2024 ] 	Batch(1000/7879) done. Loss: 0.4311  lr:0.010000
[ Mon Jul  8 13:17:14 2024 ] 	Batch(1100/7879) done. Loss: 0.1241  lr:0.010000
[ Mon Jul  8 13:17:37 2024 ] 	Batch(1200/7879) done. Loss: 0.0821  lr:0.010000
[ Mon Jul  8 13:17:59 2024 ] 	Batch(1300/7879) done. Loss: 0.1503  lr:0.010000
[ Mon Jul  8 13:18:22 2024 ] 	Batch(1400/7879) done. Loss: 0.3925  lr:0.010000
[ Mon Jul  8 13:18:45 2024 ] 
Training: Epoch [52/120], Step [1499], Loss: 0.8806384801864624, Training Accuracy: 87.91666666666667
[ Mon Jul  8 13:18:45 2024 ] 	Batch(1500/7879) done. Loss: 1.1905  lr:0.010000
[ Mon Jul  8 13:19:08 2024 ] 	Batch(1600/7879) done. Loss: 0.1448  lr:0.010000
[ Mon Jul  8 13:19:30 2024 ] 	Batch(1700/7879) done. Loss: 0.5597  lr:0.010000
[ Mon Jul  8 13:19:53 2024 ] 	Batch(1800/7879) done. Loss: 0.4512  lr:0.010000
[ Mon Jul  8 13:20:16 2024 ] 	Batch(1900/7879) done. Loss: 0.2578  lr:0.010000
[ Mon Jul  8 13:20:38 2024 ] 
Training: Epoch [52/120], Step [1999], Loss: 0.29407164454460144, Training Accuracy: 87.64999999999999
[ Mon Jul  8 13:20:39 2024 ] 	Batch(2000/7879) done. Loss: 0.5620  lr:0.010000
[ Mon Jul  8 13:21:01 2024 ] 	Batch(2100/7879) done. Loss: 0.0141  lr:0.010000
[ Mon Jul  8 13:21:24 2024 ] 	Batch(2200/7879) done. Loss: 0.6168  lr:0.010000
[ Mon Jul  8 13:21:47 2024 ] 	Batch(2300/7879) done. Loss: 1.2056  lr:0.010000
[ Mon Jul  8 13:22:09 2024 ] 	Batch(2400/7879) done. Loss: 0.3590  lr:0.010000
[ Mon Jul  8 13:22:32 2024 ] 
Training: Epoch [52/120], Step [2499], Loss: 0.22613577544689178, Training Accuracy: 87.66000000000001
[ Mon Jul  8 13:22:32 2024 ] 	Batch(2500/7879) done. Loss: 0.1637  lr:0.010000
[ Mon Jul  8 13:22:55 2024 ] 	Batch(2600/7879) done. Loss: 0.1284  lr:0.010000
[ Mon Jul  8 13:23:17 2024 ] 	Batch(2700/7879) done. Loss: 0.2679  lr:0.010000
[ Mon Jul  8 13:23:40 2024 ] 	Batch(2800/7879) done. Loss: 0.2582  lr:0.010000
[ Mon Jul  8 13:24:03 2024 ] 	Batch(2900/7879) done. Loss: 0.1368  lr:0.010000
[ Mon Jul  8 13:24:25 2024 ] 
Training: Epoch [52/120], Step [2999], Loss: 0.18208594620227814, Training Accuracy: 87.55416666666666
[ Mon Jul  8 13:24:26 2024 ] 	Batch(3000/7879) done. Loss: 0.3555  lr:0.010000
[ Mon Jul  8 13:24:48 2024 ] 	Batch(3100/7879) done. Loss: 0.4113  lr:0.010000
[ Mon Jul  8 13:25:11 2024 ] 	Batch(3200/7879) done. Loss: 0.3801  lr:0.010000
[ Mon Jul  8 13:25:34 2024 ] 	Batch(3300/7879) done. Loss: 0.2726  lr:0.010000
[ Mon Jul  8 13:25:56 2024 ] 	Batch(3400/7879) done. Loss: 0.0983  lr:0.010000
[ Mon Jul  8 13:26:19 2024 ] 
Training: Epoch [52/120], Step [3499], Loss: 0.07054043561220169, Training Accuracy: 87.54642857142856
[ Mon Jul  8 13:26:19 2024 ] 	Batch(3500/7879) done. Loss: 0.5331  lr:0.010000
[ Mon Jul  8 13:26:42 2024 ] 	Batch(3600/7879) done. Loss: 0.0219  lr:0.010000
[ Mon Jul  8 13:27:05 2024 ] 	Batch(3700/7879) done. Loss: 1.6083  lr:0.010000
[ Mon Jul  8 13:27:27 2024 ] 	Batch(3800/7879) done. Loss: 0.4054  lr:0.010000
[ Mon Jul  8 13:27:50 2024 ] 	Batch(3900/7879) done. Loss: 0.8401  lr:0.010000
[ Mon Jul  8 13:28:13 2024 ] 
Training: Epoch [52/120], Step [3999], Loss: 0.1652427464723587, Training Accuracy: 87.528125
[ Mon Jul  8 13:28:13 2024 ] 	Batch(4000/7879) done. Loss: 0.3462  lr:0.010000
[ Mon Jul  8 13:28:36 2024 ] 	Batch(4100/7879) done. Loss: 0.8057  lr:0.010000
[ Mon Jul  8 13:28:58 2024 ] 	Batch(4200/7879) done. Loss: 0.7510  lr:0.010000
[ Mon Jul  8 13:29:21 2024 ] 	Batch(4300/7879) done. Loss: 0.0309  lr:0.010000
[ Mon Jul  8 13:29:44 2024 ] 	Batch(4400/7879) done. Loss: 0.6327  lr:0.010000
[ Mon Jul  8 13:30:07 2024 ] 
Training: Epoch [52/120], Step [4499], Loss: 0.1481044888496399, Training Accuracy: 87.46111111111111
[ Mon Jul  8 13:30:07 2024 ] 	Batch(4500/7879) done. Loss: 0.0321  lr:0.010000
[ Mon Jul  8 13:30:29 2024 ] 	Batch(4600/7879) done. Loss: 1.1858  lr:0.010000
[ Mon Jul  8 13:30:52 2024 ] 	Batch(4700/7879) done. Loss: 0.3472  lr:0.010000
[ Mon Jul  8 13:31:15 2024 ] 	Batch(4800/7879) done. Loss: 0.1381  lr:0.010000
[ Mon Jul  8 13:31:38 2024 ] 	Batch(4900/7879) done. Loss: 0.7294  lr:0.010000
[ Mon Jul  8 13:32:01 2024 ] 
Training: Epoch [52/120], Step [4999], Loss: 0.762926459312439, Training Accuracy: 87.3475
[ Mon Jul  8 13:32:01 2024 ] 	Batch(5000/7879) done. Loss: 0.1387  lr:0.010000
[ Mon Jul  8 13:32:24 2024 ] 	Batch(5100/7879) done. Loss: 0.2024  lr:0.010000
[ Mon Jul  8 13:32:47 2024 ] 	Batch(5200/7879) done. Loss: 0.1255  lr:0.010000
[ Mon Jul  8 13:33:10 2024 ] 	Batch(5300/7879) done. Loss: 0.3515  lr:0.010000
[ Mon Jul  8 13:33:32 2024 ] 	Batch(5400/7879) done. Loss: 0.4911  lr:0.010000
[ Mon Jul  8 13:33:55 2024 ] 
Training: Epoch [52/120], Step [5499], Loss: 0.8141366243362427, Training Accuracy: 87.325
[ Mon Jul  8 13:33:55 2024 ] 	Batch(5500/7879) done. Loss: 0.1838  lr:0.010000
[ Mon Jul  8 13:34:18 2024 ] 	Batch(5600/7879) done. Loss: 0.1999  lr:0.010000
[ Mon Jul  8 13:34:41 2024 ] 	Batch(5700/7879) done. Loss: 0.9111  lr:0.010000
[ Mon Jul  8 13:35:03 2024 ] 	Batch(5800/7879) done. Loss: 0.2016  lr:0.010000
[ Mon Jul  8 13:35:26 2024 ] 	Batch(5900/7879) done. Loss: 0.1229  lr:0.010000
[ Mon Jul  8 13:35:49 2024 ] 
Training: Epoch [52/120], Step [5999], Loss: 1.2334129810333252, Training Accuracy: 87.2125
[ Mon Jul  8 13:35:49 2024 ] 	Batch(6000/7879) done. Loss: 0.2602  lr:0.010000
[ Mon Jul  8 13:36:12 2024 ] 	Batch(6100/7879) done. Loss: 0.3686  lr:0.010000
[ Mon Jul  8 13:36:34 2024 ] 	Batch(6200/7879) done. Loss: 0.1242  lr:0.010000
[ Mon Jul  8 13:36:57 2024 ] 	Batch(6300/7879) done. Loss: 0.4194  lr:0.010000
[ Mon Jul  8 13:37:20 2024 ] 	Batch(6400/7879) done. Loss: 0.0139  lr:0.010000
[ Mon Jul  8 13:37:43 2024 ] 
Training: Epoch [52/120], Step [6499], Loss: 0.12827882170677185, Training Accuracy: 87.09615384615384
[ Mon Jul  8 13:37:43 2024 ] 	Batch(6500/7879) done. Loss: 0.3514  lr:0.010000
[ Mon Jul  8 13:38:05 2024 ] 	Batch(6600/7879) done. Loss: 0.5541  lr:0.010000
[ Mon Jul  8 13:38:28 2024 ] 	Batch(6700/7879) done. Loss: 0.3330  lr:0.010000
[ Mon Jul  8 13:38:51 2024 ] 	Batch(6800/7879) done. Loss: 0.2909  lr:0.010000
[ Mon Jul  8 13:39:14 2024 ] 	Batch(6900/7879) done. Loss: 0.5945  lr:0.010000
[ Mon Jul  8 13:39:36 2024 ] 
Training: Epoch [52/120], Step [6999], Loss: 0.7628225088119507, Training Accuracy: 87.02499999999999
[ Mon Jul  8 13:39:36 2024 ] 	Batch(7000/7879) done. Loss: 0.8748  lr:0.010000
[ Mon Jul  8 13:39:59 2024 ] 	Batch(7100/7879) done. Loss: 0.0527  lr:0.010000
[ Mon Jul  8 13:40:22 2024 ] 	Batch(7200/7879) done. Loss: 1.0027  lr:0.010000
[ Mon Jul  8 13:40:45 2024 ] 	Batch(7300/7879) done. Loss: 0.0922  lr:0.010000
[ Mon Jul  8 13:41:09 2024 ] 	Batch(7400/7879) done. Loss: 0.0249  lr:0.010000
[ Mon Jul  8 13:41:32 2024 ] 
Training: Epoch [52/120], Step [7499], Loss: 0.43142154812812805, Training Accuracy: 86.955
[ Mon Jul  8 13:41:32 2024 ] 	Batch(7500/7879) done. Loss: 0.2572  lr:0.010000
[ Mon Jul  8 13:41:56 2024 ] 	Batch(7600/7879) done. Loss: 0.7265  lr:0.010000
[ Mon Jul  8 13:42:19 2024 ] 	Batch(7700/7879) done. Loss: 0.1837  lr:0.010000
[ Mon Jul  8 13:42:43 2024 ] 	Batch(7800/7879) done. Loss: 0.5955  lr:0.010000
[ Mon Jul  8 13:43:01 2024 ] 	Mean training loss: 0.4201.
[ Mon Jul  8 13:43:01 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 13:43:01 2024 ] Training epoch: 54
[ Mon Jul  8 13:43:02 2024 ] 	Batch(0/7879) done. Loss: 0.0800  lr:0.010000
[ Mon Jul  8 13:43:25 2024 ] 	Batch(100/7879) done. Loss: 0.3931  lr:0.010000
[ Mon Jul  8 13:43:48 2024 ] 	Batch(200/7879) done. Loss: 0.0062  lr:0.010000
[ Mon Jul  8 13:44:12 2024 ] 	Batch(300/7879) done. Loss: 0.0144  lr:0.010000
[ Mon Jul  8 13:44:35 2024 ] 	Batch(400/7879) done. Loss: 0.1790  lr:0.010000
[ Mon Jul  8 13:44:59 2024 ] 
Training: Epoch [53/120], Step [499], Loss: 0.4109863042831421, Training Accuracy: 89.25
[ Mon Jul  8 13:44:59 2024 ] 	Batch(500/7879) done. Loss: 0.4366  lr:0.010000
[ Mon Jul  8 13:45:22 2024 ] 	Batch(600/7879) done. Loss: 1.0492  lr:0.010000
[ Mon Jul  8 13:45:46 2024 ] 	Batch(700/7879) done. Loss: 0.3425  lr:0.010000
[ Mon Jul  8 13:46:09 2024 ] 	Batch(800/7879) done. Loss: 0.3738  lr:0.010000
[ Mon Jul  8 13:46:32 2024 ] 	Batch(900/7879) done. Loss: 0.1990  lr:0.010000
[ Mon Jul  8 13:46:55 2024 ] 
Training: Epoch [53/120], Step [999], Loss: 0.7046226859092712, Training Accuracy: 89.125
[ Mon Jul  8 13:46:55 2024 ] 	Batch(1000/7879) done. Loss: 0.0748  lr:0.010000
[ Mon Jul  8 13:47:17 2024 ] 	Batch(1100/7879) done. Loss: 0.0384  lr:0.010000
[ Mon Jul  8 13:47:40 2024 ] 	Batch(1200/7879) done. Loss: 0.1256  lr:0.010000
[ Mon Jul  8 13:48:03 2024 ] 	Batch(1300/7879) done. Loss: 0.0350  lr:0.010000
[ Mon Jul  8 13:48:26 2024 ] 	Batch(1400/7879) done. Loss: 0.1115  lr:0.010000
[ Mon Jul  8 13:48:48 2024 ] 
Training: Epoch [53/120], Step [1499], Loss: 0.5165189504623413, Training Accuracy: 88.45833333333334
[ Mon Jul  8 13:48:48 2024 ] 	Batch(1500/7879) done. Loss: 0.2287  lr:0.010000
[ Mon Jul  8 13:49:11 2024 ] 	Batch(1600/7879) done. Loss: 0.2094  lr:0.010000
[ Mon Jul  8 13:49:34 2024 ] 	Batch(1700/7879) done. Loss: 0.6400  lr:0.010000
[ Mon Jul  8 13:49:57 2024 ] 	Batch(1800/7879) done. Loss: 0.2253  lr:0.010000
[ Mon Jul  8 13:50:21 2024 ] 	Batch(1900/7879) done. Loss: 1.0349  lr:0.010000
[ Mon Jul  8 13:50:44 2024 ] 
Training: Epoch [53/120], Step [1999], Loss: 0.5985351204872131, Training Accuracy: 88.39375000000001
[ Mon Jul  8 13:50:44 2024 ] 	Batch(2000/7879) done. Loss: 0.0400  lr:0.010000
[ Mon Jul  8 13:51:08 2024 ] 	Batch(2100/7879) done. Loss: 0.0202  lr:0.010000
[ Mon Jul  8 13:51:31 2024 ] 	Batch(2200/7879) done. Loss: 0.1233  lr:0.010000
[ Mon Jul  8 13:51:54 2024 ] 	Batch(2300/7879) done. Loss: 0.2069  lr:0.010000
[ Mon Jul  8 13:52:17 2024 ] 	Batch(2400/7879) done. Loss: 0.5105  lr:0.010000
[ Mon Jul  8 13:52:40 2024 ] 
Training: Epoch [53/120], Step [2499], Loss: 0.19128790497779846, Training Accuracy: 88.36500000000001
[ Mon Jul  8 13:52:40 2024 ] 	Batch(2500/7879) done. Loss: 0.6636  lr:0.010000
[ Mon Jul  8 13:53:03 2024 ] 	Batch(2600/7879) done. Loss: 0.1081  lr:0.010000
[ Mon Jul  8 13:53:26 2024 ] 	Batch(2700/7879) done. Loss: 0.2565  lr:0.010000
[ Mon Jul  8 13:53:48 2024 ] 	Batch(2800/7879) done. Loss: 0.8785  lr:0.010000
[ Mon Jul  8 13:54:12 2024 ] 	Batch(2900/7879) done. Loss: 0.0573  lr:0.010000
[ Mon Jul  8 13:54:35 2024 ] 
Training: Epoch [53/120], Step [2999], Loss: 0.642990231513977, Training Accuracy: 88.17916666666666
[ Mon Jul  8 13:54:35 2024 ] 	Batch(3000/7879) done. Loss: 0.4622  lr:0.010000
[ Mon Jul  8 13:54:58 2024 ] 	Batch(3100/7879) done. Loss: 0.4299  lr:0.010000
[ Mon Jul  8 13:55:21 2024 ] 	Batch(3200/7879) done. Loss: 0.1783  lr:0.010000
[ Mon Jul  8 13:55:43 2024 ] 	Batch(3300/7879) done. Loss: 0.2178  lr:0.010000
[ Mon Jul  8 13:56:06 2024 ] 	Batch(3400/7879) done. Loss: 0.3718  lr:0.010000
[ Mon Jul  8 13:56:29 2024 ] 
Training: Epoch [53/120], Step [3499], Loss: 0.3290611803531647, Training Accuracy: 88.13214285714285
[ Mon Jul  8 13:56:29 2024 ] 	Batch(3500/7879) done. Loss: 0.1875  lr:0.010000
[ Mon Jul  8 13:56:52 2024 ] 	Batch(3600/7879) done. Loss: 0.1131  lr:0.010000
[ Mon Jul  8 13:57:14 2024 ] 	Batch(3700/7879) done. Loss: 0.3445  lr:0.010000
[ Mon Jul  8 13:57:37 2024 ] 	Batch(3800/7879) done. Loss: 0.2970  lr:0.010000
[ Mon Jul  8 13:58:00 2024 ] 	Batch(3900/7879) done. Loss: 0.3923  lr:0.010000
[ Mon Jul  8 13:58:22 2024 ] 
Training: Epoch [53/120], Step [3999], Loss: 0.3395336866378784, Training Accuracy: 87.953125
[ Mon Jul  8 13:58:23 2024 ] 	Batch(4000/7879) done. Loss: 0.1400  lr:0.010000
[ Mon Jul  8 13:58:45 2024 ] 	Batch(4100/7879) done. Loss: 0.5142  lr:0.010000
[ Mon Jul  8 13:59:08 2024 ] 	Batch(4200/7879) done. Loss: 0.6029  lr:0.010000
[ Mon Jul  8 13:59:31 2024 ] 	Batch(4300/7879) done. Loss: 0.3255  lr:0.010000
[ Mon Jul  8 13:59:53 2024 ] 	Batch(4400/7879) done. Loss: 0.3726  lr:0.010000
[ Mon Jul  8 14:00:17 2024 ] 
Training: Epoch [53/120], Step [4499], Loss: 0.06947672367095947, Training Accuracy: 87.85555555555555
[ Mon Jul  8 14:00:17 2024 ] 	Batch(4500/7879) done. Loss: 0.4606  lr:0.010000
[ Mon Jul  8 14:00:40 2024 ] 	Batch(4600/7879) done. Loss: 0.1774  lr:0.010000
[ Mon Jul  8 14:01:03 2024 ] 	Batch(4700/7879) done. Loss: 0.3517  lr:0.010000
[ Mon Jul  8 14:01:26 2024 ] 	Batch(4800/7879) done. Loss: 0.2112  lr:0.010000
[ Mon Jul  8 14:01:49 2024 ] 	Batch(4900/7879) done. Loss: 0.0380  lr:0.010000
[ Mon Jul  8 14:02:11 2024 ] 
Training: Epoch [53/120], Step [4999], Loss: 0.2152971476316452, Training Accuracy: 87.7125
[ Mon Jul  8 14:02:11 2024 ] 	Batch(5000/7879) done. Loss: 0.2812  lr:0.010000
[ Mon Jul  8 14:02:34 2024 ] 	Batch(5100/7879) done. Loss: 0.6571  lr:0.010000
[ Mon Jul  8 14:02:57 2024 ] 	Batch(5200/7879) done. Loss: 1.0898  lr:0.010000
[ Mon Jul  8 14:03:20 2024 ] 	Batch(5300/7879) done. Loss: 0.4891  lr:0.010000
[ Mon Jul  8 14:03:42 2024 ] 	Batch(5400/7879) done. Loss: 0.1667  lr:0.010000
[ Mon Jul  8 14:04:05 2024 ] 
Training: Epoch [53/120], Step [5499], Loss: 0.1237049326300621, Training Accuracy: 87.67045454545455
[ Mon Jul  8 14:04:05 2024 ] 	Batch(5500/7879) done. Loss: 0.1146  lr:0.010000
[ Mon Jul  8 14:04:28 2024 ] 	Batch(5600/7879) done. Loss: 0.5550  lr:0.010000
[ Mon Jul  8 14:04:50 2024 ] 	Batch(5700/7879) done. Loss: 0.3679  lr:0.010000
[ Mon Jul  8 14:05:13 2024 ] 	Batch(5800/7879) done. Loss: 0.1164  lr:0.010000
[ Mon Jul  8 14:05:36 2024 ] 	Batch(5900/7879) done. Loss: 0.7524  lr:0.010000
[ Mon Jul  8 14:05:58 2024 ] 
Training: Epoch [53/120], Step [5999], Loss: 0.13372428715229034, Training Accuracy: 87.56458333333333
[ Mon Jul  8 14:05:59 2024 ] 	Batch(6000/7879) done. Loss: 0.3101  lr:0.010000
[ Mon Jul  8 14:06:21 2024 ] 	Batch(6100/7879) done. Loss: 0.3712  lr:0.010000
[ Mon Jul  8 14:06:44 2024 ] 	Batch(6200/7879) done. Loss: 0.3511  lr:0.010000
[ Mon Jul  8 14:07:07 2024 ] 	Batch(6300/7879) done. Loss: 0.1275  lr:0.010000
[ Mon Jul  8 14:07:30 2024 ] 	Batch(6400/7879) done. Loss: 0.0115  lr:0.010000
[ Mon Jul  8 14:07:52 2024 ] 
Training: Epoch [53/120], Step [6499], Loss: 0.3630981743335724, Training Accuracy: 87.42115384615384
[ Mon Jul  8 14:07:52 2024 ] 	Batch(6500/7879) done. Loss: 0.7872  lr:0.010000
[ Mon Jul  8 14:08:15 2024 ] 	Batch(6600/7879) done. Loss: 0.7411  lr:0.010000
[ Mon Jul  8 14:08:38 2024 ] 	Batch(6700/7879) done. Loss: 0.1981  lr:0.010000
[ Mon Jul  8 14:09:00 2024 ] 	Batch(6800/7879) done. Loss: 0.1952  lr:0.010000
[ Mon Jul  8 14:09:23 2024 ] 	Batch(6900/7879) done. Loss: 0.7695  lr:0.010000
[ Mon Jul  8 14:09:46 2024 ] 
Training: Epoch [53/120], Step [6999], Loss: 0.5767471194267273, Training Accuracy: 87.4375
[ Mon Jul  8 14:09:46 2024 ] 	Batch(7000/7879) done. Loss: 0.6841  lr:0.010000
[ Mon Jul  8 14:10:09 2024 ] 	Batch(7100/7879) done. Loss: 0.0834  lr:0.010000
[ Mon Jul  8 14:10:32 2024 ] 	Batch(7200/7879) done. Loss: 0.9372  lr:0.010000
[ Mon Jul  8 14:10:54 2024 ] 	Batch(7300/7879) done. Loss: 0.1222  lr:0.010000
[ Mon Jul  8 14:11:17 2024 ] 	Batch(7400/7879) done. Loss: 0.0470  lr:0.010000
[ Mon Jul  8 14:11:39 2024 ] 
Training: Epoch [53/120], Step [7499], Loss: 0.16005849838256836, Training Accuracy: 87.395
[ Mon Jul  8 14:11:40 2024 ] 	Batch(7500/7879) done. Loss: 0.7041  lr:0.010000
[ Mon Jul  8 14:12:02 2024 ] 	Batch(7600/7879) done. Loss: 0.1063  lr:0.010000
[ Mon Jul  8 14:12:25 2024 ] 	Batch(7700/7879) done. Loss: 0.2075  lr:0.010000
[ Mon Jul  8 14:12:48 2024 ] 	Batch(7800/7879) done. Loss: 0.8131  lr:0.010000
[ Mon Jul  8 14:13:05 2024 ] 	Mean training loss: 0.4103.
[ Mon Jul  8 14:13:05 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 14:13:05 2024 ] Training epoch: 55
[ Mon Jul  8 14:13:06 2024 ] 	Batch(0/7879) done. Loss: 0.3655  lr:0.010000
[ Mon Jul  8 14:13:29 2024 ] 	Batch(100/7879) done. Loss: 0.7569  lr:0.010000
[ Mon Jul  8 14:13:51 2024 ] 	Batch(200/7879) done. Loss: 0.1865  lr:0.010000
[ Mon Jul  8 14:14:14 2024 ] 	Batch(300/7879) done. Loss: 0.0166  lr:0.010000
[ Mon Jul  8 14:14:37 2024 ] 	Batch(400/7879) done. Loss: 0.4285  lr:0.010000
[ Mon Jul  8 14:14:59 2024 ] 
Training: Epoch [54/120], Step [499], Loss: 0.5887224078178406, Training Accuracy: 88.75
[ Mon Jul  8 14:14:59 2024 ] 	Batch(500/7879) done. Loss: 0.2195  lr:0.010000
[ Mon Jul  8 14:15:22 2024 ] 	Batch(600/7879) done. Loss: 0.6312  lr:0.010000
[ Mon Jul  8 14:15:45 2024 ] 	Batch(700/7879) done. Loss: 1.0392  lr:0.010000
[ Mon Jul  8 14:16:08 2024 ] 	Batch(800/7879) done. Loss: 0.5303  lr:0.010000
[ Mon Jul  8 14:16:30 2024 ] 	Batch(900/7879) done. Loss: 0.1380  lr:0.010000
[ Mon Jul  8 14:16:53 2024 ] 
Training: Epoch [54/120], Step [999], Loss: 0.33309683203697205, Training Accuracy: 88.225
[ Mon Jul  8 14:16:53 2024 ] 	Batch(1000/7879) done. Loss: 0.5203  lr:0.010000
[ Mon Jul  8 14:17:16 2024 ] 	Batch(1100/7879) done. Loss: 0.3411  lr:0.010000
[ Mon Jul  8 14:17:39 2024 ] 	Batch(1200/7879) done. Loss: 0.4682  lr:0.010000
[ Mon Jul  8 14:18:01 2024 ] 	Batch(1300/7879) done. Loss: 0.1947  lr:0.010000
[ Mon Jul  8 14:18:24 2024 ] 	Batch(1400/7879) done. Loss: 0.0904  lr:0.010000
[ Mon Jul  8 14:18:46 2024 ] 
Training: Epoch [54/120], Step [1499], Loss: 0.045867763459682465, Training Accuracy: 88.04166666666666
[ Mon Jul  8 14:18:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0851  lr:0.010000
[ Mon Jul  8 14:19:10 2024 ] 	Batch(1600/7879) done. Loss: 0.0670  lr:0.010000
[ Mon Jul  8 14:19:33 2024 ] 	Batch(1700/7879) done. Loss: 0.3101  lr:0.010000
[ Mon Jul  8 14:19:56 2024 ] 	Batch(1800/7879) done. Loss: 0.5468  lr:0.010000
[ Mon Jul  8 14:20:19 2024 ] 	Batch(1900/7879) done. Loss: 0.3649  lr:0.010000
[ Mon Jul  8 14:20:42 2024 ] 
Training: Epoch [54/120], Step [1999], Loss: 0.39480337500572205, Training Accuracy: 88.06875
[ Mon Jul  8 14:20:42 2024 ] 	Batch(2000/7879) done. Loss: 0.6762  lr:0.010000
[ Mon Jul  8 14:21:05 2024 ] 	Batch(2100/7879) done. Loss: 0.3741  lr:0.010000
[ Mon Jul  8 14:21:28 2024 ] 	Batch(2200/7879) done. Loss: 0.2529  lr:0.010000
[ Mon Jul  8 14:21:51 2024 ] 	Batch(2300/7879) done. Loss: 0.6524  lr:0.010000
[ Mon Jul  8 14:22:14 2024 ] 	Batch(2400/7879) done. Loss: 0.3287  lr:0.010000
[ Mon Jul  8 14:22:36 2024 ] 
Training: Epoch [54/120], Step [2499], Loss: 0.3474811911582947, Training Accuracy: 88.03
[ Mon Jul  8 14:22:36 2024 ] 	Batch(2500/7879) done. Loss: 0.5361  lr:0.010000
[ Mon Jul  8 14:22:59 2024 ] 	Batch(2600/7879) done. Loss: 0.3318  lr:0.010000
[ Mon Jul  8 14:23:22 2024 ] 	Batch(2700/7879) done. Loss: 0.1716  lr:0.010000
[ Mon Jul  8 14:23:45 2024 ] 	Batch(2800/7879) done. Loss: 0.4191  lr:0.010000
[ Mon Jul  8 14:24:07 2024 ] 	Batch(2900/7879) done. Loss: 0.1414  lr:0.010000
[ Mon Jul  8 14:24:30 2024 ] 
Training: Epoch [54/120], Step [2999], Loss: 0.6302827596664429, Training Accuracy: 87.92083333333333
[ Mon Jul  8 14:24:30 2024 ] 	Batch(3000/7879) done. Loss: 0.6748  lr:0.010000
[ Mon Jul  8 14:24:53 2024 ] 	Batch(3100/7879) done. Loss: 0.0582  lr:0.010000
[ Mon Jul  8 14:25:15 2024 ] 	Batch(3200/7879) done. Loss: 0.2175  lr:0.010000
[ Mon Jul  8 14:25:38 2024 ] 	Batch(3300/7879) done. Loss: 0.5587  lr:0.010000
[ Mon Jul  8 14:26:01 2024 ] 	Batch(3400/7879) done. Loss: 0.2072  lr:0.010000
[ Mon Jul  8 14:26:23 2024 ] 
Training: Epoch [54/120], Step [3499], Loss: 0.1744949072599411, Training Accuracy: 87.875
[ Mon Jul  8 14:26:23 2024 ] 	Batch(3500/7879) done. Loss: 0.1978  lr:0.010000
[ Mon Jul  8 14:26:46 2024 ] 	Batch(3600/7879) done. Loss: 0.3755  lr:0.010000
[ Mon Jul  8 14:27:10 2024 ] 	Batch(3700/7879) done. Loss: 0.4608  lr:0.010000
[ Mon Jul  8 14:27:33 2024 ] 	Batch(3800/7879) done. Loss: 0.2834  lr:0.010000
[ Mon Jul  8 14:27:57 2024 ] 	Batch(3900/7879) done. Loss: 0.2469  lr:0.010000
[ Mon Jul  8 14:28:20 2024 ] 
Training: Epoch [54/120], Step [3999], Loss: 0.7505861520767212, Training Accuracy: 87.86562500000001
[ Mon Jul  8 14:28:20 2024 ] 	Batch(4000/7879) done. Loss: 0.2166  lr:0.010000
[ Mon Jul  8 14:28:44 2024 ] 	Batch(4100/7879) done. Loss: 1.1270  lr:0.010000
[ Mon Jul  8 14:29:07 2024 ] 	Batch(4200/7879) done. Loss: 0.0048  lr:0.010000
[ Mon Jul  8 14:29:31 2024 ] 	Batch(4300/7879) done. Loss: 0.1610  lr:0.010000
[ Mon Jul  8 14:29:54 2024 ] 	Batch(4400/7879) done. Loss: 0.4422  lr:0.010000
[ Mon Jul  8 14:30:18 2024 ] 
Training: Epoch [54/120], Step [4499], Loss: 0.12327101081609726, Training Accuracy: 87.76388888888889
[ Mon Jul  8 14:30:18 2024 ] 	Batch(4500/7879) done. Loss: 0.1438  lr:0.010000
[ Mon Jul  8 14:30:41 2024 ] 	Batch(4600/7879) done. Loss: 0.0985  lr:0.010000
[ Mon Jul  8 14:31:05 2024 ] 	Batch(4700/7879) done. Loss: 0.5113  lr:0.010000
[ Mon Jul  8 14:31:28 2024 ] 	Batch(4800/7879) done. Loss: 0.1113  lr:0.010000
[ Mon Jul  8 14:31:51 2024 ] 	Batch(4900/7879) done. Loss: 0.2617  lr:0.010000
[ Mon Jul  8 14:32:13 2024 ] 
Training: Epoch [54/120], Step [4999], Loss: 0.2585355043411255, Training Accuracy: 87.55
[ Mon Jul  8 14:32:14 2024 ] 	Batch(5000/7879) done. Loss: 0.1132  lr:0.010000
[ Mon Jul  8 14:32:36 2024 ] 	Batch(5100/7879) done. Loss: 0.7861  lr:0.010000
[ Mon Jul  8 14:32:59 2024 ] 	Batch(5200/7879) done. Loss: 0.0241  lr:0.010000
[ Mon Jul  8 14:33:23 2024 ] 	Batch(5300/7879) done. Loss: 0.8064  lr:0.010000
[ Mon Jul  8 14:33:46 2024 ] 	Batch(5400/7879) done. Loss: 0.5092  lr:0.010000
[ Mon Jul  8 14:34:09 2024 ] 
Training: Epoch [54/120], Step [5499], Loss: 0.44535350799560547, Training Accuracy: 87.44545454545455
[ Mon Jul  8 14:34:10 2024 ] 	Batch(5500/7879) done. Loss: 0.3649  lr:0.010000
[ Mon Jul  8 14:34:33 2024 ] 	Batch(5600/7879) done. Loss: 0.2729  lr:0.010000
[ Mon Jul  8 14:34:56 2024 ] 	Batch(5700/7879) done. Loss: 0.2828  lr:0.010000
[ Mon Jul  8 14:35:19 2024 ] 	Batch(5800/7879) done. Loss: 1.4829  lr:0.010000
[ Mon Jul  8 14:35:41 2024 ] 	Batch(5900/7879) done. Loss: 0.2315  lr:0.010000
[ Mon Jul  8 14:36:04 2024 ] 
Training: Epoch [54/120], Step [5999], Loss: 0.1812894195318222, Training Accuracy: 87.4
[ Mon Jul  8 14:36:04 2024 ] 	Batch(6000/7879) done. Loss: 0.3945  lr:0.010000
[ Mon Jul  8 14:36:28 2024 ] 	Batch(6100/7879) done. Loss: 0.8261  lr:0.010000
[ Mon Jul  8 14:36:51 2024 ] 	Batch(6200/7879) done. Loss: 0.6915  lr:0.010000
[ Mon Jul  8 14:37:14 2024 ] 	Batch(6300/7879) done. Loss: 0.0514  lr:0.010000
[ Mon Jul  8 14:37:38 2024 ] 	Batch(6400/7879) done. Loss: 0.6060  lr:0.010000
[ Mon Jul  8 14:38:01 2024 ] 
Training: Epoch [54/120], Step [6499], Loss: 0.038779787719249725, Training Accuracy: 87.4326923076923
[ Mon Jul  8 14:38:01 2024 ] 	Batch(6500/7879) done. Loss: 0.1453  lr:0.010000
[ Mon Jul  8 14:38:24 2024 ] 	Batch(6600/7879) done. Loss: 1.0838  lr:0.010000
[ Mon Jul  8 14:38:46 2024 ] 	Batch(6700/7879) done. Loss: 0.2439  lr:0.010000
[ Mon Jul  8 14:39:09 2024 ] 	Batch(6800/7879) done. Loss: 1.6037  lr:0.010000
[ Mon Jul  8 14:39:32 2024 ] 	Batch(6900/7879) done. Loss: 0.2700  lr:0.010000
[ Mon Jul  8 14:39:54 2024 ] 
Training: Epoch [54/120], Step [6999], Loss: 0.15621037781238556, Training Accuracy: 87.43571428571428
[ Mon Jul  8 14:39:55 2024 ] 	Batch(7000/7879) done. Loss: 0.2215  lr:0.010000
[ Mon Jul  8 14:40:17 2024 ] 	Batch(7100/7879) done. Loss: 0.7148  lr:0.010000
[ Mon Jul  8 14:40:40 2024 ] 	Batch(7200/7879) done. Loss: 0.1884  lr:0.010000
[ Mon Jul  8 14:41:03 2024 ] 	Batch(7300/7879) done. Loss: 0.8027  lr:0.010000
[ Mon Jul  8 14:41:26 2024 ] 	Batch(7400/7879) done. Loss: 0.1924  lr:0.010000
[ Mon Jul  8 14:41:48 2024 ] 
Training: Epoch [54/120], Step [7499], Loss: 0.07482105493545532, Training Accuracy: 87.42166666666667
[ Mon Jul  8 14:41:48 2024 ] 	Batch(7500/7879) done. Loss: 0.3685  lr:0.010000
[ Mon Jul  8 14:42:11 2024 ] 	Batch(7600/7879) done. Loss: 0.3777  lr:0.010000
[ Mon Jul  8 14:42:34 2024 ] 	Batch(7700/7879) done. Loss: 0.3090  lr:0.010000
[ Mon Jul  8 14:42:56 2024 ] 	Batch(7800/7879) done. Loss: 0.7328  lr:0.010000
[ Mon Jul  8 14:43:14 2024 ] 	Mean training loss: 0.4079.
[ Mon Jul  8 14:43:14 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 14:43:14 2024 ] Training epoch: 56
[ Mon Jul  8 14:43:15 2024 ] 	Batch(0/7879) done. Loss: 0.4723  lr:0.010000
[ Mon Jul  8 14:43:38 2024 ] 	Batch(100/7879) done. Loss: 0.2043  lr:0.010000
[ Mon Jul  8 14:44:01 2024 ] 	Batch(200/7879) done. Loss: 0.6326  lr:0.010000
[ Mon Jul  8 14:44:25 2024 ] 	Batch(300/7879) done. Loss: 0.2385  lr:0.010000
[ Mon Jul  8 14:44:48 2024 ] 	Batch(400/7879) done. Loss: 0.5175  lr:0.010000
[ Mon Jul  8 14:45:12 2024 ] 
Training: Epoch [55/120], Step [499], Loss: 0.14523307979106903, Training Accuracy: 88.25
[ Mon Jul  8 14:45:12 2024 ] 	Batch(500/7879) done. Loss: 0.3177  lr:0.010000
[ Mon Jul  8 14:45:35 2024 ] 	Batch(600/7879) done. Loss: 0.0641  lr:0.010000
[ Mon Jul  8 14:45:59 2024 ] 	Batch(700/7879) done. Loss: 0.1897  lr:0.010000
[ Mon Jul  8 14:46:22 2024 ] 	Batch(800/7879) done. Loss: 0.1404  lr:0.010000
[ Mon Jul  8 14:46:45 2024 ] 	Batch(900/7879) done. Loss: 0.5003  lr:0.010000
[ Mon Jul  8 14:47:07 2024 ] 
Training: Epoch [55/120], Step [999], Loss: 0.22861595451831818, Training Accuracy: 88.925
[ Mon Jul  8 14:47:08 2024 ] 	Batch(1000/7879) done. Loss: 0.1633  lr:0.010000
[ Mon Jul  8 14:47:30 2024 ] 	Batch(1100/7879) done. Loss: 0.1571  lr:0.010000
[ Mon Jul  8 14:47:53 2024 ] 	Batch(1200/7879) done. Loss: 0.6551  lr:0.010000
[ Mon Jul  8 14:48:16 2024 ] 	Batch(1300/7879) done. Loss: 0.5737  lr:0.010000
[ Mon Jul  8 14:48:39 2024 ] 	Batch(1400/7879) done. Loss: 0.4218  lr:0.010000
[ Mon Jul  8 14:49:02 2024 ] 
Training: Epoch [55/120], Step [1499], Loss: 0.5614327788352966, Training Accuracy: 88.94166666666666
[ Mon Jul  8 14:49:02 2024 ] 	Batch(1500/7879) done. Loss: 0.0188  lr:0.010000
[ Mon Jul  8 14:49:25 2024 ] 	Batch(1600/7879) done. Loss: 0.0108  lr:0.010000
[ Mon Jul  8 14:49:47 2024 ] 	Batch(1700/7879) done. Loss: 0.4953  lr:0.010000
[ Mon Jul  8 14:50:10 2024 ] 	Batch(1800/7879) done. Loss: 0.4092  lr:0.010000
[ Mon Jul  8 14:50:33 2024 ] 	Batch(1900/7879) done. Loss: 0.1887  lr:0.010000
[ Mon Jul  8 14:50:55 2024 ] 
Training: Epoch [55/120], Step [1999], Loss: 0.22365990281105042, Training Accuracy: 88.89375000000001
[ Mon Jul  8 14:50:56 2024 ] 	Batch(2000/7879) done. Loss: 0.1279  lr:0.010000
[ Mon Jul  8 14:51:19 2024 ] 	Batch(2100/7879) done. Loss: 0.0739  lr:0.010000
[ Mon Jul  8 14:51:42 2024 ] 	Batch(2200/7879) done. Loss: 0.5487  lr:0.010000
[ Mon Jul  8 14:52:06 2024 ] 	Batch(2300/7879) done. Loss: 0.4378  lr:0.010000
[ Mon Jul  8 14:52:29 2024 ] 	Batch(2400/7879) done. Loss: 0.2110  lr:0.010000
[ Mon Jul  8 14:52:52 2024 ] 
Training: Epoch [55/120], Step [2499], Loss: 0.20235899090766907, Training Accuracy: 88.81
[ Mon Jul  8 14:52:53 2024 ] 	Batch(2500/7879) done. Loss: 0.7581  lr:0.010000
[ Mon Jul  8 14:53:16 2024 ] 	Batch(2600/7879) done. Loss: 0.4193  lr:0.010000
[ Mon Jul  8 14:53:39 2024 ] 	Batch(2700/7879) done. Loss: 0.3661  lr:0.010000
[ Mon Jul  8 14:54:02 2024 ] 	Batch(2800/7879) done. Loss: 0.4343  lr:0.010000
[ Mon Jul  8 14:54:24 2024 ] 	Batch(2900/7879) done. Loss: 0.0407  lr:0.010000
[ Mon Jul  8 14:54:47 2024 ] 
Training: Epoch [55/120], Step [2999], Loss: 0.10901860892772675, Training Accuracy: 88.73333333333333
[ Mon Jul  8 14:54:47 2024 ] 	Batch(3000/7879) done. Loss: 0.6240  lr:0.010000
[ Mon Jul  8 14:55:10 2024 ] 	Batch(3100/7879) done. Loss: 0.2269  lr:0.010000
[ Mon Jul  8 14:55:33 2024 ] 	Batch(3200/7879) done. Loss: 0.7773  lr:0.010000
[ Mon Jul  8 14:55:55 2024 ] 	Batch(3300/7879) done. Loss: 0.2295  lr:0.010000
[ Mon Jul  8 14:56:18 2024 ] 	Batch(3400/7879) done. Loss: 0.4872  lr:0.010000
[ Mon Jul  8 14:56:40 2024 ] 
Training: Epoch [55/120], Step [3499], Loss: 0.6491975784301758, Training Accuracy: 88.57142857142857
[ Mon Jul  8 14:56:41 2024 ] 	Batch(3500/7879) done. Loss: 0.0049  lr:0.010000
[ Mon Jul  8 14:57:04 2024 ] 	Batch(3600/7879) done. Loss: 0.6814  lr:0.010000
[ Mon Jul  8 14:57:27 2024 ] 	Batch(3700/7879) done. Loss: 0.4745  lr:0.010000
[ Mon Jul  8 14:57:49 2024 ] 	Batch(3800/7879) done. Loss: 0.1944  lr:0.010000
[ Mon Jul  8 14:58:12 2024 ] 	Batch(3900/7879) done. Loss: 0.4186  lr:0.010000
[ Mon Jul  8 14:58:34 2024 ] 
Training: Epoch [55/120], Step [3999], Loss: 0.18757477402687073, Training Accuracy: 88.41875
[ Mon Jul  8 14:58:35 2024 ] 	Batch(4000/7879) done. Loss: 0.4591  lr:0.010000
[ Mon Jul  8 14:58:57 2024 ] 	Batch(4100/7879) done. Loss: 0.1043  lr:0.010000
[ Mon Jul  8 14:59:20 2024 ] 	Batch(4200/7879) done. Loss: 0.6565  lr:0.010000
[ Mon Jul  8 14:59:43 2024 ] 	Batch(4300/7879) done. Loss: 0.1750  lr:0.010000
[ Mon Jul  8 15:00:06 2024 ] 	Batch(4400/7879) done. Loss: 0.4196  lr:0.010000
[ Mon Jul  8 15:00:28 2024 ] 
Training: Epoch [55/120], Step [4499], Loss: 0.010534530505537987, Training Accuracy: 88.275
[ Mon Jul  8 15:00:28 2024 ] 	Batch(4500/7879) done. Loss: 0.4051  lr:0.010000
[ Mon Jul  8 15:00:51 2024 ] 	Batch(4600/7879) done. Loss: 0.6822  lr:0.010000
[ Mon Jul  8 15:01:14 2024 ] 	Batch(4700/7879) done. Loss: 0.4439  lr:0.010000
[ Mon Jul  8 15:01:36 2024 ] 	Batch(4800/7879) done. Loss: 0.0982  lr:0.010000
[ Mon Jul  8 15:02:00 2024 ] 	Batch(4900/7879) done. Loss: 0.3409  lr:0.010000
[ Mon Jul  8 15:02:23 2024 ] 
Training: Epoch [55/120], Step [4999], Loss: 0.1674250215291977, Training Accuracy: 88.16000000000001
[ Mon Jul  8 15:02:23 2024 ] 	Batch(5000/7879) done. Loss: 0.2598  lr:0.010000
[ Mon Jul  8 15:02:47 2024 ] 	Batch(5100/7879) done. Loss: 0.1258  lr:0.010000
[ Mon Jul  8 15:03:10 2024 ] 	Batch(5200/7879) done. Loss: 0.3088  lr:0.010000
[ Mon Jul  8 15:03:33 2024 ] 	Batch(5300/7879) done. Loss: 1.0196  lr:0.010000
[ Mon Jul  8 15:03:56 2024 ] 	Batch(5400/7879) done. Loss: 0.0630  lr:0.010000
[ Mon Jul  8 15:04:18 2024 ] 
Training: Epoch [55/120], Step [5499], Loss: 0.17502646148204803, Training Accuracy: 87.92272727272727
[ Mon Jul  8 15:04:18 2024 ] 	Batch(5500/7879) done. Loss: 0.5749  lr:0.010000
[ Mon Jul  8 15:04:41 2024 ] 	Batch(5600/7879) done. Loss: 0.0932  lr:0.010000
[ Mon Jul  8 15:05:04 2024 ] 	Batch(5700/7879) done. Loss: 0.7923  lr:0.010000
[ Mon Jul  8 15:05:27 2024 ] 	Batch(5800/7879) done. Loss: 0.4632  lr:0.010000
[ Mon Jul  8 15:05:49 2024 ] 	Batch(5900/7879) done. Loss: 0.1797  lr:0.010000
[ Mon Jul  8 15:06:12 2024 ] 
Training: Epoch [55/120], Step [5999], Loss: 0.08076745271682739, Training Accuracy: 87.82708333333333
[ Mon Jul  8 15:06:12 2024 ] 	Batch(6000/7879) done. Loss: 0.1692  lr:0.010000
[ Mon Jul  8 15:06:35 2024 ] 	Batch(6100/7879) done. Loss: 0.2133  lr:0.010000
[ Mon Jul  8 15:06:58 2024 ] 	Batch(6200/7879) done. Loss: 0.0847  lr:0.010000
[ Mon Jul  8 15:07:20 2024 ] 	Batch(6300/7879) done. Loss: 1.1944  lr:0.010000
[ Mon Jul  8 15:07:43 2024 ] 	Batch(6400/7879) done. Loss: 0.1711  lr:0.010000
[ Mon Jul  8 15:08:06 2024 ] 
Training: Epoch [55/120], Step [6499], Loss: 1.1083900928497314, Training Accuracy: 87.77307692307691
[ Mon Jul  8 15:08:06 2024 ] 	Batch(6500/7879) done. Loss: 0.5979  lr:0.010000
[ Mon Jul  8 15:08:29 2024 ] 	Batch(6600/7879) done. Loss: 0.1463  lr:0.010000
[ Mon Jul  8 15:08:51 2024 ] 	Batch(6700/7879) done. Loss: 0.4638  lr:0.010000
[ Mon Jul  8 15:09:14 2024 ] 	Batch(6800/7879) done. Loss: 0.2063  lr:0.010000
[ Mon Jul  8 15:09:37 2024 ] 	Batch(6900/7879) done. Loss: 0.3254  lr:0.010000
[ Mon Jul  8 15:09:59 2024 ] 
Training: Epoch [55/120], Step [6999], Loss: 0.3159200847148895, Training Accuracy: 87.73928571428571
[ Mon Jul  8 15:10:00 2024 ] 	Batch(7000/7879) done. Loss: 0.3351  lr:0.010000
[ Mon Jul  8 15:10:22 2024 ] 	Batch(7100/7879) done. Loss: 1.0791  lr:0.010000
[ Mon Jul  8 15:10:45 2024 ] 	Batch(7200/7879) done. Loss: 0.1740  lr:0.010000
[ Mon Jul  8 15:11:08 2024 ] 	Batch(7300/7879) done. Loss: 0.3896  lr:0.010000
[ Mon Jul  8 15:11:31 2024 ] 	Batch(7400/7879) done. Loss: 0.2176  lr:0.010000
[ Mon Jul  8 15:11:53 2024 ] 
Training: Epoch [55/120], Step [7499], Loss: 0.6215435862541199, Training Accuracy: 87.62333333333333
[ Mon Jul  8 15:11:53 2024 ] 	Batch(7500/7879) done. Loss: 0.0764  lr:0.010000
[ Mon Jul  8 15:12:16 2024 ] 	Batch(7600/7879) done. Loss: 0.0655  lr:0.010000
[ Mon Jul  8 15:12:39 2024 ] 	Batch(7700/7879) done. Loss: 0.4103  lr:0.010000
[ Mon Jul  8 15:13:02 2024 ] 	Batch(7800/7879) done. Loss: 0.3059  lr:0.010000
[ Mon Jul  8 15:13:20 2024 ] 	Mean training loss: 0.4013.
[ Mon Jul  8 15:13:20 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 15:13:20 2024 ] Training epoch: 57
[ Mon Jul  8 15:13:21 2024 ] 	Batch(0/7879) done. Loss: 0.6650  lr:0.010000
[ Mon Jul  8 15:13:44 2024 ] 	Batch(100/7879) done. Loss: 0.0258  lr:0.010000
[ Mon Jul  8 15:14:06 2024 ] 	Batch(200/7879) done. Loss: 0.6435  lr:0.010000
[ Mon Jul  8 15:14:29 2024 ] 	Batch(300/7879) done. Loss: 0.4098  lr:0.010000
[ Mon Jul  8 15:14:52 2024 ] 	Batch(400/7879) done. Loss: 0.1513  lr:0.010000
[ Mon Jul  8 15:15:14 2024 ] 
Training: Epoch [56/120], Step [499], Loss: 0.013644051738083363, Training Accuracy: 89.4
[ Mon Jul  8 15:15:15 2024 ] 	Batch(500/7879) done. Loss: 0.0241  lr:0.010000
[ Mon Jul  8 15:15:37 2024 ] 	Batch(600/7879) done. Loss: 0.4463  lr:0.010000
[ Mon Jul  8 15:16:00 2024 ] 	Batch(700/7879) done. Loss: 0.2446  lr:0.010000
[ Mon Jul  8 15:16:23 2024 ] 	Batch(800/7879) done. Loss: 0.4136  lr:0.010000
[ Mon Jul  8 15:16:46 2024 ] 	Batch(900/7879) done. Loss: 0.0899  lr:0.010000
[ Mon Jul  8 15:17:09 2024 ] 
Training: Epoch [56/120], Step [999], Loss: 0.4147375524044037, Training Accuracy: 89.51249999999999
[ Mon Jul  8 15:17:10 2024 ] 	Batch(1000/7879) done. Loss: 0.1107  lr:0.010000
[ Mon Jul  8 15:17:33 2024 ] 	Batch(1100/7879) done. Loss: 0.2664  lr:0.010000
[ Mon Jul  8 15:17:57 2024 ] 	Batch(1200/7879) done. Loss: 0.1414  lr:0.010000
[ Mon Jul  8 15:18:19 2024 ] 	Batch(1300/7879) done. Loss: 0.2378  lr:0.010000
[ Mon Jul  8 15:18:42 2024 ] 	Batch(1400/7879) done. Loss: 0.4860  lr:0.010000
[ Mon Jul  8 15:19:04 2024 ] 
Training: Epoch [56/120], Step [1499], Loss: 0.2095552384853363, Training Accuracy: 89.275
[ Mon Jul  8 15:19:05 2024 ] 	Batch(1500/7879) done. Loss: 0.6656  lr:0.010000
[ Mon Jul  8 15:19:28 2024 ] 	Batch(1600/7879) done. Loss: 0.6868  lr:0.010000
[ Mon Jul  8 15:19:50 2024 ] 	Batch(1700/7879) done. Loss: 0.5658  lr:0.010000
[ Mon Jul  8 15:20:13 2024 ] 	Batch(1800/7879) done. Loss: 0.0573  lr:0.010000
[ Mon Jul  8 15:20:36 2024 ] 	Batch(1900/7879) done. Loss: 0.7377  lr:0.010000
[ Mon Jul  8 15:20:58 2024 ] 
Training: Epoch [56/120], Step [1999], Loss: 1.6636099815368652, Training Accuracy: 88.9625
[ Mon Jul  8 15:20:59 2024 ] 	Batch(2000/7879) done. Loss: 0.2680  lr:0.010000
[ Mon Jul  8 15:21:22 2024 ] 	Batch(2100/7879) done. Loss: 0.3511  lr:0.010000
[ Mon Jul  8 15:21:44 2024 ] 	Batch(2200/7879) done. Loss: 0.0121  lr:0.010000
[ Mon Jul  8 15:22:07 2024 ] 	Batch(2300/7879) done. Loss: 0.4225  lr:0.010000
[ Mon Jul  8 15:22:31 2024 ] 	Batch(2400/7879) done. Loss: 0.2360  lr:0.010000
[ Mon Jul  8 15:22:54 2024 ] 
Training: Epoch [56/120], Step [2499], Loss: 0.13886147737503052, Training Accuracy: 88.55499999999999
[ Mon Jul  8 15:22:54 2024 ] 	Batch(2500/7879) done. Loss: 1.4013  lr:0.010000
[ Mon Jul  8 15:23:17 2024 ] 	Batch(2600/7879) done. Loss: 0.2941  lr:0.010000
[ Mon Jul  8 15:23:40 2024 ] 	Batch(2700/7879) done. Loss: 0.5337  lr:0.010000
[ Mon Jul  8 15:24:03 2024 ] 	Batch(2800/7879) done. Loss: 0.4641  lr:0.010000
[ Mon Jul  8 15:24:25 2024 ] 	Batch(2900/7879) done. Loss: 1.2419  lr:0.010000
[ Mon Jul  8 15:24:48 2024 ] 
Training: Epoch [56/120], Step [2999], Loss: 0.485299289226532, Training Accuracy: 88.49583333333332
[ Mon Jul  8 15:24:48 2024 ] 	Batch(3000/7879) done. Loss: 0.1114  lr:0.010000
[ Mon Jul  8 15:25:11 2024 ] 	Batch(3100/7879) done. Loss: 0.5429  lr:0.010000
[ Mon Jul  8 15:25:34 2024 ] 	Batch(3200/7879) done. Loss: 0.4445  lr:0.010000
[ Mon Jul  8 15:25:57 2024 ] 	Batch(3300/7879) done. Loss: 0.2937  lr:0.010000
[ Mon Jul  8 15:26:21 2024 ] 	Batch(3400/7879) done. Loss: 0.0987  lr:0.010000
[ Mon Jul  8 15:26:44 2024 ] 
Training: Epoch [56/120], Step [3499], Loss: 0.5649845004081726, Training Accuracy: 88.53928571428571
[ Mon Jul  8 15:26:44 2024 ] 	Batch(3500/7879) done. Loss: 0.2143  lr:0.010000
[ Mon Jul  8 15:27:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0715  lr:0.010000
[ Mon Jul  8 15:27:31 2024 ] 	Batch(3700/7879) done. Loss: 0.1951  lr:0.010000
[ Mon Jul  8 15:27:55 2024 ] 	Batch(3800/7879) done. Loss: 1.4281  lr:0.010000
[ Mon Jul  8 15:28:18 2024 ] 	Batch(3900/7879) done. Loss: 0.0582  lr:0.010000
[ Mon Jul  8 15:28:41 2024 ] 
Training: Epoch [56/120], Step [3999], Loss: 0.06298156082630157, Training Accuracy: 88.375
[ Mon Jul  8 15:28:42 2024 ] 	Batch(4000/7879) done. Loss: 0.5333  lr:0.010000
[ Mon Jul  8 15:29:04 2024 ] 	Batch(4100/7879) done. Loss: 0.4117  lr:0.010000
[ Mon Jul  8 15:29:27 2024 ] 	Batch(4200/7879) done. Loss: 0.6625  lr:0.010000
[ Mon Jul  8 15:29:50 2024 ] 	Batch(4300/7879) done. Loss: 0.7919  lr:0.010000
[ Mon Jul  8 15:30:13 2024 ] 	Batch(4400/7879) done. Loss: 0.5735  lr:0.010000
[ Mon Jul  8 15:30:36 2024 ] 
Training: Epoch [56/120], Step [4499], Loss: 0.538680374622345, Training Accuracy: 88.23333333333333
[ Mon Jul  8 15:30:36 2024 ] 	Batch(4500/7879) done. Loss: 0.4923  lr:0.010000
[ Mon Jul  8 15:30:59 2024 ] 	Batch(4600/7879) done. Loss: 0.6922  lr:0.010000
[ Mon Jul  8 15:31:23 2024 ] 	Batch(4700/7879) done. Loss: 0.1988  lr:0.010000
[ Mon Jul  8 15:31:46 2024 ] 	Batch(4800/7879) done. Loss: 0.6026  lr:0.010000
[ Mon Jul  8 15:32:10 2024 ] 	Batch(4900/7879) done. Loss: 0.1091  lr:0.010000
[ Mon Jul  8 15:32:33 2024 ] 
Training: Epoch [56/120], Step [4999], Loss: 0.9282851219177246, Training Accuracy: 88.1
[ Mon Jul  8 15:32:33 2024 ] 	Batch(5000/7879) done. Loss: 0.6188  lr:0.010000
[ Mon Jul  8 15:32:56 2024 ] 	Batch(5100/7879) done. Loss: 0.1464  lr:0.010000
[ Mon Jul  8 15:33:19 2024 ] 	Batch(5200/7879) done. Loss: 0.5364  lr:0.010000
[ Mon Jul  8 15:33:42 2024 ] 	Batch(5300/7879) done. Loss: 0.9209  lr:0.010000
[ Mon Jul  8 15:34:05 2024 ] 	Batch(5400/7879) done. Loss: 0.0227  lr:0.010000
[ Mon Jul  8 15:34:27 2024 ] 
Training: Epoch [56/120], Step [5499], Loss: 0.038748398423194885, Training Accuracy: 88.01136363636364
[ Mon Jul  8 15:34:27 2024 ] 	Batch(5500/7879) done. Loss: 0.8259  lr:0.010000
[ Mon Jul  8 15:34:50 2024 ] 	Batch(5600/7879) done. Loss: 0.7078  lr:0.010000
[ Mon Jul  8 15:35:13 2024 ] 	Batch(5700/7879) done. Loss: 0.1329  lr:0.010000
[ Mon Jul  8 15:35:36 2024 ] 	Batch(5800/7879) done. Loss: 0.5407  lr:0.010000
[ Mon Jul  8 15:35:59 2024 ] 	Batch(5900/7879) done. Loss: 0.5168  lr:0.010000
[ Mon Jul  8 15:36:21 2024 ] 
Training: Epoch [56/120], Step [5999], Loss: 0.02008604258298874, Training Accuracy: 87.88333333333334
[ Mon Jul  8 15:36:22 2024 ] 	Batch(6000/7879) done. Loss: 0.0922  lr:0.010000
[ Mon Jul  8 15:36:44 2024 ] 	Batch(6100/7879) done. Loss: 0.4910  lr:0.010000
[ Mon Jul  8 15:37:07 2024 ] 	Batch(6200/7879) done. Loss: 0.2178  lr:0.010000
[ Mon Jul  8 15:37:30 2024 ] 	Batch(6300/7879) done. Loss: 0.8952  lr:0.010000
[ Mon Jul  8 15:37:52 2024 ] 	Batch(6400/7879) done. Loss: 1.0803  lr:0.010000
[ Mon Jul  8 15:38:15 2024 ] 
Training: Epoch [56/120], Step [6499], Loss: 0.2791281044483185, Training Accuracy: 87.7673076923077
[ Mon Jul  8 15:38:15 2024 ] 	Batch(6500/7879) done. Loss: 0.2103  lr:0.010000
[ Mon Jul  8 15:38:38 2024 ] 	Batch(6600/7879) done. Loss: 0.0900  lr:0.010000
[ Mon Jul  8 15:39:00 2024 ] 	Batch(6700/7879) done. Loss: 0.3623  lr:0.010000
[ Mon Jul  8 15:39:23 2024 ] 	Batch(6800/7879) done. Loss: 0.2867  lr:0.010000
[ Mon Jul  8 15:39:46 2024 ] 	Batch(6900/7879) done. Loss: 0.4917  lr:0.010000
[ Mon Jul  8 15:40:08 2024 ] 
Training: Epoch [56/120], Step [6999], Loss: 0.27051788568496704, Training Accuracy: 87.69285714285714
[ Mon Jul  8 15:40:09 2024 ] 	Batch(7000/7879) done. Loss: 0.7973  lr:0.010000
[ Mon Jul  8 15:40:31 2024 ] 	Batch(7100/7879) done. Loss: 2.3233  lr:0.010000
[ Mon Jul  8 15:40:54 2024 ] 	Batch(7200/7879) done. Loss: 0.0686  lr:0.010000
[ Mon Jul  8 15:41:17 2024 ] 	Batch(7300/7879) done. Loss: 0.1477  lr:0.010000
[ Mon Jul  8 15:41:40 2024 ] 	Batch(7400/7879) done. Loss: 0.2394  lr:0.010000
[ Mon Jul  8 15:42:02 2024 ] 
Training: Epoch [56/120], Step [7499], Loss: 0.4273779094219208, Training Accuracy: 87.70333333333333
[ Mon Jul  8 15:42:02 2024 ] 	Batch(7500/7879) done. Loss: 0.5689  lr:0.010000
[ Mon Jul  8 15:42:25 2024 ] 	Batch(7600/7879) done. Loss: 0.6732  lr:0.010000
[ Mon Jul  8 15:42:48 2024 ] 	Batch(7700/7879) done. Loss: 0.6420  lr:0.010000
[ Mon Jul  8 15:43:10 2024 ] 	Batch(7800/7879) done. Loss: 0.8906  lr:0.010000
[ Mon Jul  8 15:43:28 2024 ] 	Mean training loss: 0.3931.
[ Mon Jul  8 15:43:28 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 15:43:28 2024 ] Training epoch: 58
[ Mon Jul  8 15:43:29 2024 ] 	Batch(0/7879) done. Loss: 0.1931  lr:0.010000
[ Mon Jul  8 15:43:52 2024 ] 	Batch(100/7879) done. Loss: 0.5367  lr:0.010000
[ Mon Jul  8 15:44:14 2024 ] 	Batch(200/7879) done. Loss: 0.0463  lr:0.010000
[ Mon Jul  8 15:44:38 2024 ] 	Batch(300/7879) done. Loss: 0.5930  lr:0.010000
[ Mon Jul  8 15:45:01 2024 ] 	Batch(400/7879) done. Loss: 0.4219  lr:0.010000
[ Mon Jul  8 15:45:25 2024 ] 
Training: Epoch [57/120], Step [499], Loss: 0.07079629600048065, Training Accuracy: 89.2
[ Mon Jul  8 15:45:25 2024 ] 	Batch(500/7879) done. Loss: 0.0510  lr:0.010000
[ Mon Jul  8 15:45:47 2024 ] 	Batch(600/7879) done. Loss: 0.4753  lr:0.010000
[ Mon Jul  8 15:46:10 2024 ] 	Batch(700/7879) done. Loss: 0.2646  lr:0.010000
[ Mon Jul  8 15:46:33 2024 ] 	Batch(800/7879) done. Loss: 0.4545  lr:0.010000
[ Mon Jul  8 15:46:56 2024 ] 	Batch(900/7879) done. Loss: 0.6614  lr:0.010000
[ Mon Jul  8 15:47:18 2024 ] 
Training: Epoch [57/120], Step [999], Loss: 0.4819762706756592, Training Accuracy: 89.05
[ Mon Jul  8 15:47:18 2024 ] 	Batch(1000/7879) done. Loss: 0.5616  lr:0.010000
[ Mon Jul  8 15:47:41 2024 ] 	Batch(1100/7879) done. Loss: 0.1238  lr:0.010000
[ Mon Jul  8 15:48:04 2024 ] 	Batch(1200/7879) done. Loss: 0.1071  lr:0.010000
[ Mon Jul  8 15:48:27 2024 ] 	Batch(1300/7879) done. Loss: 0.4016  lr:0.010000
[ Mon Jul  8 15:48:49 2024 ] 	Batch(1400/7879) done. Loss: 0.2374  lr:0.010000
[ Mon Jul  8 15:49:12 2024 ] 
Training: Epoch [57/120], Step [1499], Loss: 0.051359016448259354, Training Accuracy: 89.05
[ Mon Jul  8 15:49:12 2024 ] 	Batch(1500/7879) done. Loss: 0.1115  lr:0.010000
[ Mon Jul  8 15:49:35 2024 ] 	Batch(1600/7879) done. Loss: 0.4465  lr:0.010000
[ Mon Jul  8 15:49:57 2024 ] 	Batch(1700/7879) done. Loss: 0.1040  lr:0.010000
[ Mon Jul  8 15:50:20 2024 ] 	Batch(1800/7879) done. Loss: 0.2744  lr:0.010000
[ Mon Jul  8 15:50:43 2024 ] 	Batch(1900/7879) done. Loss: 0.0781  lr:0.010000
[ Mon Jul  8 15:51:05 2024 ] 
Training: Epoch [57/120], Step [1999], Loss: 0.8305706977844238, Training Accuracy: 88.98125
[ Mon Jul  8 15:51:06 2024 ] 	Batch(2000/7879) done. Loss: 0.1570  lr:0.010000
[ Mon Jul  8 15:51:28 2024 ] 	Batch(2100/7879) done. Loss: 0.0571  lr:0.010000
[ Mon Jul  8 15:51:51 2024 ] 	Batch(2200/7879) done. Loss: 0.2087  lr:0.010000
[ Mon Jul  8 15:52:14 2024 ] 	Batch(2300/7879) done. Loss: 0.4300  lr:0.010000
[ Mon Jul  8 15:52:36 2024 ] 	Batch(2400/7879) done. Loss: 0.0683  lr:0.010000
[ Mon Jul  8 15:52:59 2024 ] 
Training: Epoch [57/120], Step [2499], Loss: 0.35188111662864685, Training Accuracy: 88.68
[ Mon Jul  8 15:52:59 2024 ] 	Batch(2500/7879) done. Loss: 0.6192  lr:0.010000
[ Mon Jul  8 15:53:22 2024 ] 	Batch(2600/7879) done. Loss: 0.2417  lr:0.010000
[ Mon Jul  8 15:53:44 2024 ] 	Batch(2700/7879) done. Loss: 0.3854  lr:0.010000
[ Mon Jul  8 15:54:07 2024 ] 	Batch(2800/7879) done. Loss: 0.1231  lr:0.010000
[ Mon Jul  8 15:54:30 2024 ] 	Batch(2900/7879) done. Loss: 0.1364  lr:0.010000
[ Mon Jul  8 15:54:52 2024 ] 
Training: Epoch [57/120], Step [2999], Loss: 0.831854522228241, Training Accuracy: 88.5
[ Mon Jul  8 15:54:53 2024 ] 	Batch(3000/7879) done. Loss: 0.3400  lr:0.010000
[ Mon Jul  8 15:55:15 2024 ] 	Batch(3100/7879) done. Loss: 0.1486  lr:0.010000
[ Mon Jul  8 15:55:38 2024 ] 	Batch(3200/7879) done. Loss: 0.7581  lr:0.010000
[ Mon Jul  8 15:56:01 2024 ] 	Batch(3300/7879) done. Loss: 0.1110  lr:0.010000
[ Mon Jul  8 15:56:23 2024 ] 	Batch(3400/7879) done. Loss: 0.2004  lr:0.010000
[ Mon Jul  8 15:56:46 2024 ] 
Training: Epoch [57/120], Step [3499], Loss: 0.12157408148050308, Training Accuracy: 88.67142857142856
[ Mon Jul  8 15:56:46 2024 ] 	Batch(3500/7879) done. Loss: 0.4264  lr:0.010000
[ Mon Jul  8 15:57:09 2024 ] 	Batch(3600/7879) done. Loss: 0.4789  lr:0.010000
[ Mon Jul  8 15:57:32 2024 ] 	Batch(3700/7879) done. Loss: 0.4476  lr:0.010000
[ Mon Jul  8 15:57:54 2024 ] 	Batch(3800/7879) done. Loss: 0.4952  lr:0.010000
[ Mon Jul  8 15:58:17 2024 ] 	Batch(3900/7879) done. Loss: 0.0329  lr:0.010000
[ Mon Jul  8 15:58:39 2024 ] 
Training: Epoch [57/120], Step [3999], Loss: 0.3671741783618927, Training Accuracy: 88.634375
[ Mon Jul  8 15:58:40 2024 ] 	Batch(4000/7879) done. Loss: 0.1226  lr:0.010000
[ Mon Jul  8 15:59:02 2024 ] 	Batch(4100/7879) done. Loss: 1.1588  lr:0.010000
[ Mon Jul  8 15:59:25 2024 ] 	Batch(4200/7879) done. Loss: 0.0660  lr:0.010000
[ Mon Jul  8 15:59:48 2024 ] 	Batch(4300/7879) done. Loss: 0.1335  lr:0.010000
[ Mon Jul  8 16:00:11 2024 ] 	Batch(4400/7879) done. Loss: 1.3595  lr:0.010000
[ Mon Jul  8 16:00:33 2024 ] 
Training: Epoch [57/120], Step [4499], Loss: 0.4205107092857361, Training Accuracy: 88.48333333333333
[ Mon Jul  8 16:00:33 2024 ] 	Batch(4500/7879) done. Loss: 0.2496  lr:0.010000
[ Mon Jul  8 16:00:56 2024 ] 	Batch(4600/7879) done. Loss: 0.0473  lr:0.010000
[ Mon Jul  8 16:01:19 2024 ] 	Batch(4700/7879) done. Loss: 0.1738  lr:0.010000
[ Mon Jul  8 16:01:42 2024 ] 	Batch(4800/7879) done. Loss: 0.1233  lr:0.010000
[ Mon Jul  8 16:02:05 2024 ] 	Batch(4900/7879) done. Loss: 0.6277  lr:0.010000
[ Mon Jul  8 16:02:28 2024 ] 
Training: Epoch [57/120], Step [4999], Loss: 0.5681076049804688, Training Accuracy: 88.36500000000001
[ Mon Jul  8 16:02:28 2024 ] 	Batch(5000/7879) done. Loss: 0.1668  lr:0.010000
[ Mon Jul  8 16:02:50 2024 ] 	Batch(5100/7879) done. Loss: 0.5527  lr:0.010000
[ Mon Jul  8 16:03:13 2024 ] 	Batch(5200/7879) done. Loss: 0.1253  lr:0.010000
[ Mon Jul  8 16:03:36 2024 ] 	Batch(5300/7879) done. Loss: 0.1876  lr:0.010000
[ Mon Jul  8 16:03:59 2024 ] 	Batch(5400/7879) done. Loss: 0.0702  lr:0.010000
[ Mon Jul  8 16:04:21 2024 ] 
Training: Epoch [57/120], Step [5499], Loss: 0.5675609707832336, Training Accuracy: 88.29772727272727
[ Mon Jul  8 16:04:21 2024 ] 	Batch(5500/7879) done. Loss: 0.3273  lr:0.010000
[ Mon Jul  8 16:04:44 2024 ] 	Batch(5600/7879) done. Loss: 0.1140  lr:0.010000
[ Mon Jul  8 16:05:07 2024 ] 	Batch(5700/7879) done. Loss: 0.1212  lr:0.010000
[ Mon Jul  8 16:05:29 2024 ] 	Batch(5800/7879) done. Loss: 0.2323  lr:0.010000
[ Mon Jul  8 16:05:52 2024 ] 	Batch(5900/7879) done. Loss: 0.0812  lr:0.010000
[ Mon Jul  8 16:06:14 2024 ] 
Training: Epoch [57/120], Step [5999], Loss: 0.8309329152107239, Training Accuracy: 88.27083333333333
[ Mon Jul  8 16:06:15 2024 ] 	Batch(6000/7879) done. Loss: 1.0863  lr:0.010000
[ Mon Jul  8 16:06:38 2024 ] 	Batch(6100/7879) done. Loss: 0.1748  lr:0.010000
[ Mon Jul  8 16:07:00 2024 ] 	Batch(6200/7879) done. Loss: 0.2149  lr:0.010000
[ Mon Jul  8 16:07:23 2024 ] 	Batch(6300/7879) done. Loss: 0.6025  lr:0.010000
[ Mon Jul  8 16:07:46 2024 ] 	Batch(6400/7879) done. Loss: 0.6387  lr:0.010000
[ Mon Jul  8 16:08:08 2024 ] 
Training: Epoch [57/120], Step [6499], Loss: 0.17300967872142792, Training Accuracy: 88.17884615384615
[ Mon Jul  8 16:08:08 2024 ] 	Batch(6500/7879) done. Loss: 0.1296  lr:0.010000
[ Mon Jul  8 16:08:31 2024 ] 	Batch(6600/7879) done. Loss: 0.7766  lr:0.010000
[ Mon Jul  8 16:08:54 2024 ] 	Batch(6700/7879) done. Loss: 0.2848  lr:0.010000
[ Mon Jul  8 16:09:16 2024 ] 	Batch(6800/7879) done. Loss: 0.1042  lr:0.010000
[ Mon Jul  8 16:09:40 2024 ] 	Batch(6900/7879) done. Loss: 0.6127  lr:0.010000
[ Mon Jul  8 16:10:03 2024 ] 
Training: Epoch [57/120], Step [6999], Loss: 1.0973261594772339, Training Accuracy: 88.10178571428573
[ Mon Jul  8 16:10:03 2024 ] 	Batch(7000/7879) done. Loss: 0.2171  lr:0.010000
[ Mon Jul  8 16:10:27 2024 ] 	Batch(7100/7879) done. Loss: 0.0698  lr:0.010000
[ Mon Jul  8 16:10:50 2024 ] 	Batch(7200/7879) done. Loss: 0.5295  lr:0.010000
[ Mon Jul  8 16:11:13 2024 ] 	Batch(7300/7879) done. Loss: 0.2194  lr:0.010000
[ Mon Jul  8 16:11:36 2024 ] 	Batch(7400/7879) done. Loss: 0.4617  lr:0.010000
[ Mon Jul  8 16:11:58 2024 ] 
Training: Epoch [57/120], Step [7499], Loss: 0.33572566509246826, Training Accuracy: 87.99333333333334
[ Mon Jul  8 16:11:58 2024 ] 	Batch(7500/7879) done. Loss: 1.2847  lr:0.010000
[ Mon Jul  8 16:12:21 2024 ] 	Batch(7600/7879) done. Loss: 0.0851  lr:0.010000
[ Mon Jul  8 16:12:45 2024 ] 	Batch(7700/7879) done. Loss: 0.2403  lr:0.010000
[ Mon Jul  8 16:13:08 2024 ] 	Batch(7800/7879) done. Loss: 0.0746  lr:0.010000
[ Mon Jul  8 16:13:26 2024 ] 	Mean training loss: 0.3957.
[ Mon Jul  8 16:13:26 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 16:13:27 2024 ] Training epoch: 59
[ Mon Jul  8 16:13:27 2024 ] 	Batch(0/7879) done. Loss: 0.4071  lr:0.010000
[ Mon Jul  8 16:13:50 2024 ] 	Batch(100/7879) done. Loss: 0.9224  lr:0.010000
[ Mon Jul  8 16:14:12 2024 ] 	Batch(200/7879) done. Loss: 0.1441  lr:0.010000
[ Mon Jul  8 16:14:35 2024 ] 	Batch(300/7879) done. Loss: 0.0238  lr:0.010000
[ Mon Jul  8 16:14:58 2024 ] 	Batch(400/7879) done. Loss: 0.1047  lr:0.010000
[ Mon Jul  8 16:15:20 2024 ] 
Training: Epoch [58/120], Step [499], Loss: 0.5195306539535522, Training Accuracy: 88.6
[ Mon Jul  8 16:15:21 2024 ] 	Batch(500/7879) done. Loss: 0.1339  lr:0.010000
[ Mon Jul  8 16:15:43 2024 ] 	Batch(600/7879) done. Loss: 0.2312  lr:0.010000
[ Mon Jul  8 16:16:06 2024 ] 	Batch(700/7879) done. Loss: 0.3607  lr:0.010000
[ Mon Jul  8 16:16:29 2024 ] 	Batch(800/7879) done. Loss: 0.9812  lr:0.010000
[ Mon Jul  8 16:16:52 2024 ] 	Batch(900/7879) done. Loss: 0.3769  lr:0.010000
[ Mon Jul  8 16:17:14 2024 ] 
Training: Epoch [58/120], Step [999], Loss: 0.03650383651256561, Training Accuracy: 89.1625
[ Mon Jul  8 16:17:14 2024 ] 	Batch(1000/7879) done. Loss: 0.1061  lr:0.010000
[ Mon Jul  8 16:17:37 2024 ] 	Batch(1100/7879) done. Loss: 0.4049  lr:0.010000
[ Mon Jul  8 16:18:00 2024 ] 	Batch(1200/7879) done. Loss: 0.0382  lr:0.010000
[ Mon Jul  8 16:18:23 2024 ] 	Batch(1300/7879) done. Loss: 0.2750  lr:0.010000
[ Mon Jul  8 16:18:47 2024 ] 	Batch(1400/7879) done. Loss: 0.5880  lr:0.010000
[ Mon Jul  8 16:19:10 2024 ] 
Training: Epoch [58/120], Step [1499], Loss: 0.3417831063270569, Training Accuracy: 89.19166666666666
[ Mon Jul  8 16:19:10 2024 ] 	Batch(1500/7879) done. Loss: 0.1104  lr:0.010000
[ Mon Jul  8 16:19:34 2024 ] 	Batch(1600/7879) done. Loss: 0.1002  lr:0.010000
[ Mon Jul  8 16:19:57 2024 ] 	Batch(1700/7879) done. Loss: 0.0124  lr:0.010000
[ Mon Jul  8 16:20:19 2024 ] 	Batch(1800/7879) done. Loss: 0.4722  lr:0.010000
[ Mon Jul  8 16:20:42 2024 ] 	Batch(1900/7879) done. Loss: 0.9312  lr:0.010000
[ Mon Jul  8 16:21:05 2024 ] 
Training: Epoch [58/120], Step [1999], Loss: 0.7770686149597168, Training Accuracy: 88.825
[ Mon Jul  8 16:21:05 2024 ] 	Batch(2000/7879) done. Loss: 0.1126  lr:0.010000
[ Mon Jul  8 16:21:28 2024 ] 	Batch(2100/7879) done. Loss: 0.5236  lr:0.010000
[ Mon Jul  8 16:21:50 2024 ] 	Batch(2200/7879) done. Loss: 0.2321  lr:0.010000
[ Mon Jul  8 16:22:13 2024 ] 	Batch(2300/7879) done. Loss: 0.0672  lr:0.010000
[ Mon Jul  8 16:22:36 2024 ] 	Batch(2400/7879) done. Loss: 0.5021  lr:0.010000
[ Mon Jul  8 16:22:59 2024 ] 
Training: Epoch [58/120], Step [2499], Loss: 0.2810339331626892, Training Accuracy: 88.735
[ Mon Jul  8 16:22:59 2024 ] 	Batch(2500/7879) done. Loss: 1.5172  lr:0.010000
[ Mon Jul  8 16:23:23 2024 ] 	Batch(2600/7879) done. Loss: 0.1419  lr:0.010000
[ Mon Jul  8 16:23:46 2024 ] 	Batch(2700/7879) done. Loss: 0.4028  lr:0.010000
[ Mon Jul  8 16:24:09 2024 ] 	Batch(2800/7879) done. Loss: 0.0164  lr:0.010000
[ Mon Jul  8 16:24:31 2024 ] 	Batch(2900/7879) done. Loss: 1.2169  lr:0.010000
[ Mon Jul  8 16:24:54 2024 ] 
Training: Epoch [58/120], Step [2999], Loss: 0.8763649463653564, Training Accuracy: 88.69583333333333
[ Mon Jul  8 16:24:54 2024 ] 	Batch(3000/7879) done. Loss: 0.1457  lr:0.010000
[ Mon Jul  8 16:25:17 2024 ] 	Batch(3100/7879) done. Loss: 0.2416  lr:0.010000
[ Mon Jul  8 16:25:40 2024 ] 	Batch(3200/7879) done. Loss: 0.0320  lr:0.010000
[ Mon Jul  8 16:26:02 2024 ] 	Batch(3300/7879) done. Loss: 0.1445  lr:0.010000
[ Mon Jul  8 16:26:25 2024 ] 	Batch(3400/7879) done. Loss: 0.1165  lr:0.010000
[ Mon Jul  8 16:26:48 2024 ] 
Training: Epoch [58/120], Step [3499], Loss: 0.1395551562309265, Training Accuracy: 88.55
[ Mon Jul  8 16:26:48 2024 ] 	Batch(3500/7879) done. Loss: 0.6404  lr:0.010000
[ Mon Jul  8 16:27:11 2024 ] 	Batch(3600/7879) done. Loss: 0.1181  lr:0.010000
[ Mon Jul  8 16:27:33 2024 ] 	Batch(3700/7879) done. Loss: 0.3393  lr:0.010000
[ Mon Jul  8 16:27:56 2024 ] 	Batch(3800/7879) done. Loss: 0.7577  lr:0.010000
[ Mon Jul  8 16:28:19 2024 ] 	Batch(3900/7879) done. Loss: 0.6920  lr:0.010000
[ Mon Jul  8 16:28:41 2024 ] 
Training: Epoch [58/120], Step [3999], Loss: 0.6431558132171631, Training Accuracy: 88.38125
[ Mon Jul  8 16:28:42 2024 ] 	Batch(4000/7879) done. Loss: 0.9519  lr:0.010000
[ Mon Jul  8 16:29:05 2024 ] 	Batch(4100/7879) done. Loss: 0.4054  lr:0.010000
[ Mon Jul  8 16:29:29 2024 ] 	Batch(4200/7879) done. Loss: 0.3665  lr:0.010000
[ Mon Jul  8 16:29:52 2024 ] 	Batch(4300/7879) done. Loss: 0.2294  lr:0.010000
[ Mon Jul  8 16:30:15 2024 ] 	Batch(4400/7879) done. Loss: 0.1049  lr:0.010000
[ Mon Jul  8 16:30:38 2024 ] 
Training: Epoch [58/120], Step [4499], Loss: 0.5370951294898987, Training Accuracy: 88.26944444444445
[ Mon Jul  8 16:30:38 2024 ] 	Batch(4500/7879) done. Loss: 0.5233  lr:0.010000
[ Mon Jul  8 16:31:01 2024 ] 	Batch(4600/7879) done. Loss: 0.2757  lr:0.010000
[ Mon Jul  8 16:31:24 2024 ] 	Batch(4700/7879) done. Loss: 0.5319  lr:0.010000
[ Mon Jul  8 16:31:47 2024 ] 	Batch(4800/7879) done. Loss: 0.7494  lr:0.010000
[ Mon Jul  8 16:32:10 2024 ] 	Batch(4900/7879) done. Loss: 0.1399  lr:0.010000
[ Mon Jul  8 16:32:32 2024 ] 
Training: Epoch [58/120], Step [4999], Loss: 0.6078876256942749, Training Accuracy: 88.14999999999999
[ Mon Jul  8 16:32:32 2024 ] 	Batch(5000/7879) done. Loss: 0.5312  lr:0.010000
[ Mon Jul  8 16:32:55 2024 ] 	Batch(5100/7879) done. Loss: 0.0857  lr:0.010000
[ Mon Jul  8 16:33:18 2024 ] 	Batch(5200/7879) done. Loss: 0.8035  lr:0.010000
[ Mon Jul  8 16:33:41 2024 ] 	Batch(5300/7879) done. Loss: 0.6191  lr:0.010000
[ Mon Jul  8 16:34:03 2024 ] 	Batch(5400/7879) done. Loss: 0.1441  lr:0.010000
[ Mon Jul  8 16:34:26 2024 ] 
Training: Epoch [58/120], Step [5499], Loss: 0.051561389118433, Training Accuracy: 88.16590909090908
[ Mon Jul  8 16:34:26 2024 ] 	Batch(5500/7879) done. Loss: 0.7325  lr:0.010000
[ Mon Jul  8 16:34:49 2024 ] 	Batch(5600/7879) done. Loss: 1.2399  lr:0.010000
[ Mon Jul  8 16:35:12 2024 ] 	Batch(5700/7879) done. Loss: 0.4565  lr:0.010000
[ Mon Jul  8 16:35:34 2024 ] 	Batch(5800/7879) done. Loss: 0.0726  lr:0.010000
[ Mon Jul  8 16:35:57 2024 ] 	Batch(5900/7879) done. Loss: 0.2270  lr:0.010000
[ Mon Jul  8 16:36:20 2024 ] 
Training: Epoch [58/120], Step [5999], Loss: 0.042121246457099915, Training Accuracy: 88.08125
[ Mon Jul  8 16:36:20 2024 ] 	Batch(6000/7879) done. Loss: 0.5769  lr:0.010000
[ Mon Jul  8 16:36:43 2024 ] 	Batch(6100/7879) done. Loss: 0.3423  lr:0.010000
[ Mon Jul  8 16:37:06 2024 ] 	Batch(6200/7879) done. Loss: 0.6217  lr:0.010000
[ Mon Jul  8 16:37:28 2024 ] 	Batch(6300/7879) done. Loss: 0.0064  lr:0.010000
[ Mon Jul  8 16:37:51 2024 ] 	Batch(6400/7879) done. Loss: 1.1001  lr:0.010000
[ Mon Jul  8 16:38:14 2024 ] 
Training: Epoch [58/120], Step [6499], Loss: 0.3364519774913788, Training Accuracy: 87.99807692307692
[ Mon Jul  8 16:38:14 2024 ] 	Batch(6500/7879) done. Loss: 0.6040  lr:0.010000
[ Mon Jul  8 16:38:37 2024 ] 	Batch(6600/7879) done. Loss: 0.2843  lr:0.010000
[ Mon Jul  8 16:38:59 2024 ] 	Batch(6700/7879) done. Loss: 0.4025  lr:0.010000
[ Mon Jul  8 16:39:22 2024 ] 	Batch(6800/7879) done. Loss: 0.1820  lr:0.010000
[ Mon Jul  8 16:39:45 2024 ] 	Batch(6900/7879) done. Loss: 0.1473  lr:0.010000
[ Mon Jul  8 16:40:07 2024 ] 
Training: Epoch [58/120], Step [6999], Loss: 0.085943304002285, Training Accuracy: 87.8875
[ Mon Jul  8 16:40:07 2024 ] 	Batch(7000/7879) done. Loss: 1.3559  lr:0.010000
[ Mon Jul  8 16:40:30 2024 ] 	Batch(7100/7879) done. Loss: 0.7289  lr:0.010000
[ Mon Jul  8 16:40:53 2024 ] 	Batch(7200/7879) done. Loss: 0.4109  lr:0.010000
[ Mon Jul  8 16:41:16 2024 ] 	Batch(7300/7879) done. Loss: 0.2111  lr:0.010000
[ Mon Jul  8 16:41:38 2024 ] 	Batch(7400/7879) done. Loss: 0.3279  lr:0.010000
[ Mon Jul  8 16:42:01 2024 ] 
Training: Epoch [58/120], Step [7499], Loss: 0.9298044443130493, Training Accuracy: 87.775
[ Mon Jul  8 16:42:01 2024 ] 	Batch(7500/7879) done. Loss: 0.1093  lr:0.010000
[ Mon Jul  8 16:42:24 2024 ] 	Batch(7600/7879) done. Loss: 0.1905  lr:0.010000
[ Mon Jul  8 16:42:46 2024 ] 	Batch(7700/7879) done. Loss: 0.5417  lr:0.010000
[ Mon Jul  8 16:43:09 2024 ] 	Batch(7800/7879) done. Loss: 0.2917  lr:0.010000
[ Mon Jul  8 16:43:27 2024 ] 	Mean training loss: 0.3979.
[ Mon Jul  8 16:43:27 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 16:43:27 2024 ] Training epoch: 60
[ Mon Jul  8 16:43:28 2024 ] 	Batch(0/7879) done. Loss: 0.0640  lr:0.010000
[ Mon Jul  8 16:43:50 2024 ] 	Batch(100/7879) done. Loss: 0.1603  lr:0.010000
[ Mon Jul  8 16:44:13 2024 ] 	Batch(200/7879) done. Loss: 0.3435  lr:0.010000
[ Mon Jul  8 16:44:36 2024 ] 	Batch(300/7879) done. Loss: 0.1290  lr:0.010000
[ Mon Jul  8 16:44:59 2024 ] 	Batch(400/7879) done. Loss: 0.0176  lr:0.010000
[ Mon Jul  8 16:45:21 2024 ] 
Training: Epoch [59/120], Step [499], Loss: 0.030133042484521866, Training Accuracy: 88.825
[ Mon Jul  8 16:45:21 2024 ] 	Batch(500/7879) done. Loss: 0.0106  lr:0.010000
[ Mon Jul  8 16:45:44 2024 ] 	Batch(600/7879) done. Loss: 0.1313  lr:0.010000
[ Mon Jul  8 16:46:07 2024 ] 	Batch(700/7879) done. Loss: 0.1409  lr:0.010000
[ Mon Jul  8 16:46:30 2024 ] 	Batch(800/7879) done. Loss: 1.2461  lr:0.010000
[ Mon Jul  8 16:46:52 2024 ] 	Batch(900/7879) done. Loss: 0.8159  lr:0.010000
[ Mon Jul  8 16:47:15 2024 ] 
Training: Epoch [59/120], Step [999], Loss: 0.3318221867084503, Training Accuracy: 88.64999999999999
[ Mon Jul  8 16:47:15 2024 ] 	Batch(1000/7879) done. Loss: 0.0588  lr:0.010000
[ Mon Jul  8 16:47:38 2024 ] 	Batch(1100/7879) done. Loss: 0.0759  lr:0.010000
[ Mon Jul  8 16:48:00 2024 ] 	Batch(1200/7879) done. Loss: 0.1118  lr:0.010000
[ Mon Jul  8 16:48:24 2024 ] 	Batch(1300/7879) done. Loss: 0.4320  lr:0.010000
[ Mon Jul  8 16:48:47 2024 ] 	Batch(1400/7879) done. Loss: 0.8733  lr:0.010000
[ Mon Jul  8 16:49:10 2024 ] 
Training: Epoch [59/120], Step [1499], Loss: 0.18388037383556366, Training Accuracy: 89.05833333333332
[ Mon Jul  8 16:49:11 2024 ] 	Batch(1500/7879) done. Loss: 0.1075  lr:0.010000
[ Mon Jul  8 16:49:34 2024 ] 	Batch(1600/7879) done. Loss: 0.4497  lr:0.010000
[ Mon Jul  8 16:49:58 2024 ] 	Batch(1700/7879) done. Loss: 0.2301  lr:0.010000
[ Mon Jul  8 16:50:21 2024 ] 	Batch(1800/7879) done. Loss: 0.0134  lr:0.010000
[ Mon Jul  8 16:50:43 2024 ] 	Batch(1900/7879) done. Loss: 0.4152  lr:0.010000
[ Mon Jul  8 16:51:06 2024 ] 
Training: Epoch [59/120], Step [1999], Loss: 1.130814552307129, Training Accuracy: 88.725
[ Mon Jul  8 16:51:06 2024 ] 	Batch(2000/7879) done. Loss: 0.6708  lr:0.010000
[ Mon Jul  8 16:51:29 2024 ] 	Batch(2100/7879) done. Loss: 0.0384  lr:0.010000
[ Mon Jul  8 16:51:52 2024 ] 	Batch(2200/7879) done. Loss: 0.1470  lr:0.010000
[ Mon Jul  8 16:52:14 2024 ] 	Batch(2300/7879) done. Loss: 0.0254  lr:0.010000
[ Mon Jul  8 16:52:37 2024 ] 	Batch(2400/7879) done. Loss: 0.4239  lr:0.010000
[ Mon Jul  8 16:53:01 2024 ] 
Training: Epoch [59/120], Step [2499], Loss: 0.7467044591903687, Training Accuracy: 88.73
[ Mon Jul  8 16:53:01 2024 ] 	Batch(2500/7879) done. Loss: 0.4297  lr:0.010000
[ Mon Jul  8 16:53:24 2024 ] 	Batch(2600/7879) done. Loss: 0.1490  lr:0.010000
[ Mon Jul  8 16:53:48 2024 ] 	Batch(2700/7879) done. Loss: 0.4074  lr:0.010000
[ Mon Jul  8 16:54:11 2024 ] 	Batch(2800/7879) done. Loss: 0.2703  lr:0.010000
[ Mon Jul  8 16:54:34 2024 ] 	Batch(2900/7879) done. Loss: 0.7230  lr:0.010000
[ Mon Jul  8 16:54:57 2024 ] 
Training: Epoch [59/120], Step [2999], Loss: 0.14209425449371338, Training Accuracy: 88.6375
[ Mon Jul  8 16:54:57 2024 ] 	Batch(3000/7879) done. Loss: 0.2067  lr:0.010000
[ Mon Jul  8 16:55:20 2024 ] 	Batch(3100/7879) done. Loss: 0.4613  lr:0.010000
[ Mon Jul  8 16:55:42 2024 ] 	Batch(3200/7879) done. Loss: 0.1316  lr:0.010000
[ Mon Jul  8 16:56:05 2024 ] 	Batch(3300/7879) done. Loss: 0.2762  lr:0.010000
[ Mon Jul  8 16:56:28 2024 ] 	Batch(3400/7879) done. Loss: 0.2528  lr:0.010000
[ Mon Jul  8 16:56:50 2024 ] 
Training: Epoch [59/120], Step [3499], Loss: 0.6544065475463867, Training Accuracy: 88.56785714285714
[ Mon Jul  8 16:56:51 2024 ] 	Batch(3500/7879) done. Loss: 0.1840  lr:0.010000
[ Mon Jul  8 16:57:13 2024 ] 	Batch(3600/7879) done. Loss: 0.7897  lr:0.010000
[ Mon Jul  8 16:57:37 2024 ] 	Batch(3700/7879) done. Loss: 0.2167  lr:0.010000
[ Mon Jul  8 16:57:59 2024 ] 	Batch(3800/7879) done. Loss: 0.3759  lr:0.010000
[ Mon Jul  8 16:58:22 2024 ] 	Batch(3900/7879) done. Loss: 0.4090  lr:0.010000
[ Mon Jul  8 16:58:45 2024 ] 
Training: Epoch [59/120], Step [3999], Loss: 0.15606750547885895, Training Accuracy: 88.5125
[ Mon Jul  8 16:58:45 2024 ] 	Batch(4000/7879) done. Loss: 0.0391  lr:0.010000
[ Mon Jul  8 16:59:08 2024 ] 	Batch(4100/7879) done. Loss: 0.2678  lr:0.010000
[ Mon Jul  8 16:59:31 2024 ] 	Batch(4200/7879) done. Loss: 0.1595  lr:0.010000
[ Mon Jul  8 16:59:55 2024 ] 	Batch(4300/7879) done. Loss: 0.4843  lr:0.010000
[ Mon Jul  8 17:00:17 2024 ] 	Batch(4400/7879) done. Loss: 0.2492  lr:0.010000
[ Mon Jul  8 17:00:40 2024 ] 
Training: Epoch [59/120], Step [4499], Loss: 0.5652705430984497, Training Accuracy: 88.50277777777778
[ Mon Jul  8 17:00:40 2024 ] 	Batch(4500/7879) done. Loss: 0.1295  lr:0.010000
[ Mon Jul  8 17:01:03 2024 ] 	Batch(4600/7879) done. Loss: 0.2514  lr:0.010000
[ Mon Jul  8 17:01:26 2024 ] 	Batch(4700/7879) done. Loss: 0.3763  lr:0.010000
[ Mon Jul  8 17:01:48 2024 ] 	Batch(4800/7879) done. Loss: 0.3155  lr:0.010000
[ Mon Jul  8 17:02:11 2024 ] 	Batch(4900/7879) done. Loss: 0.3697  lr:0.010000
[ Mon Jul  8 17:02:34 2024 ] 
Training: Epoch [59/120], Step [4999], Loss: 0.3960656523704529, Training Accuracy: 88.455
[ Mon Jul  8 17:02:34 2024 ] 	Batch(5000/7879) done. Loss: 1.0966  lr:0.010000
[ Mon Jul  8 17:02:56 2024 ] 	Batch(5100/7879) done. Loss: 0.4134  lr:0.010000
[ Mon Jul  8 17:03:19 2024 ] 	Batch(5200/7879) done. Loss: 0.5308  lr:0.010000
[ Mon Jul  8 17:03:43 2024 ] 	Batch(5300/7879) done. Loss: 0.3562  lr:0.010000
[ Mon Jul  8 17:04:06 2024 ] 	Batch(5400/7879) done. Loss: 0.3214  lr:0.010000
[ Mon Jul  8 17:04:29 2024 ] 
Training: Epoch [59/120], Step [5499], Loss: 0.5132815837860107, Training Accuracy: 88.33636363636363
[ Mon Jul  8 17:04:29 2024 ] 	Batch(5500/7879) done. Loss: 0.2203  lr:0.010000
[ Mon Jul  8 17:04:53 2024 ] 	Batch(5600/7879) done. Loss: 1.6632  lr:0.010000
[ Mon Jul  8 17:05:15 2024 ] 	Batch(5700/7879) done. Loss: 0.0424  lr:0.010000
[ Mon Jul  8 17:05:38 2024 ] 	Batch(5800/7879) done. Loss: 1.6227  lr:0.010000
[ Mon Jul  8 17:06:01 2024 ] 	Batch(5900/7879) done. Loss: 0.7696  lr:0.010000
[ Mon Jul  8 17:06:23 2024 ] 
Training: Epoch [59/120], Step [5999], Loss: 0.4288731515407562, Training Accuracy: 88.25833333333334
[ Mon Jul  8 17:06:24 2024 ] 	Batch(6000/7879) done. Loss: 0.2570  lr:0.010000
[ Mon Jul  8 17:06:46 2024 ] 	Batch(6100/7879) done. Loss: 0.5390  lr:0.010000
[ Mon Jul  8 17:07:09 2024 ] 	Batch(6200/7879) done. Loss: 0.0970  lr:0.010000
[ Mon Jul  8 17:07:32 2024 ] 	Batch(6300/7879) done. Loss: 0.2286  lr:0.010000
[ Mon Jul  8 17:07:54 2024 ] 	Batch(6400/7879) done. Loss: 0.5633  lr:0.010000
[ Mon Jul  8 17:08:17 2024 ] 
Training: Epoch [59/120], Step [6499], Loss: 0.8008214235305786, Training Accuracy: 88.18461538461538
[ Mon Jul  8 17:08:17 2024 ] 	Batch(6500/7879) done. Loss: 0.6748  lr:0.010000
[ Mon Jul  8 17:08:40 2024 ] 	Batch(6600/7879) done. Loss: 0.2698  lr:0.010000
[ Mon Jul  8 17:09:02 2024 ] 	Batch(6700/7879) done. Loss: 0.7917  lr:0.010000
[ Mon Jul  8 17:09:25 2024 ] 	Batch(6800/7879) done. Loss: 0.3696  lr:0.010000
[ Mon Jul  8 17:09:48 2024 ] 	Batch(6900/7879) done. Loss: 0.1057  lr:0.010000
[ Mon Jul  8 17:10:10 2024 ] 
Training: Epoch [59/120], Step [6999], Loss: 0.05693816393613815, Training Accuracy: 88.04464285714285
[ Mon Jul  8 17:10:11 2024 ] 	Batch(7000/7879) done. Loss: 0.1722  lr:0.010000
[ Mon Jul  8 17:10:33 2024 ] 	Batch(7100/7879) done. Loss: 0.5038  lr:0.010000
[ Mon Jul  8 17:10:56 2024 ] 	Batch(7200/7879) done. Loss: 0.7649  lr:0.010000
[ Mon Jul  8 17:11:19 2024 ] 	Batch(7300/7879) done. Loss: 0.4838  lr:0.010000
[ Mon Jul  8 17:11:42 2024 ] 	Batch(7400/7879) done. Loss: 0.1732  lr:0.010000
[ Mon Jul  8 17:12:04 2024 ] 
Training: Epoch [59/120], Step [7499], Loss: 0.02716522291302681, Training Accuracy: 87.98333333333333
[ Mon Jul  8 17:12:04 2024 ] 	Batch(7500/7879) done. Loss: 0.3617  lr:0.010000
[ Mon Jul  8 17:12:27 2024 ] 	Batch(7600/7879) done. Loss: 0.1291  lr:0.010000
[ Mon Jul  8 17:12:50 2024 ] 	Batch(7700/7879) done. Loss: 0.4068  lr:0.010000
[ Mon Jul  8 17:13:13 2024 ] 	Batch(7800/7879) done. Loss: 0.0677  lr:0.010000
[ Mon Jul  8 17:13:30 2024 ] 	Mean training loss: 0.3891.
[ Mon Jul  8 17:13:30 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 17:13:31 2024 ] Eval epoch: 60
[ Mon Jul  8 17:19:27 2024 ] 	Mean val loss of 6365 batches: 1.154256988379287.
[ Mon Jul  8 17:19:27 2024 ] 
Validation: Epoch [59/120], Samples [37087.0/50919], Loss: 0.04550861194729805, Validation Accuracy: 72.83528741727056
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 1 : 189 / 275 = 68 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 2 : 196 / 273 = 71 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 3 : 199 / 273 = 72 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 4 : 193 / 275 = 70 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 5 : 199 / 275 = 72 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 6 : 205 / 275 = 74 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 7 : 240 / 273 = 87 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 8 : 263 / 273 = 96 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 9 : 176 / 273 = 64 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 10 : 75 / 273 = 27 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 11 : 130 / 272 = 47 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 12 : 183 / 271 = 67 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 13 : 257 / 275 = 93 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 14 : 236 / 276 = 85 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 15 : 199 / 273 = 72 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 16 : 153 / 274 = 55 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 17 : 220 / 273 = 80 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 18 : 201 / 274 = 73 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 19 : 236 / 272 = 86 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 20 : 241 / 273 = 88 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 21 : 229 / 274 = 83 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 22 : 258 / 274 = 94 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 23 : 229 / 276 = 82 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 24 : 221 / 274 = 80 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 25 : 267 / 275 = 97 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 26 : 256 / 276 = 92 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 27 : 181 / 275 = 65 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 28 : 148 / 275 = 53 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 29 : 166 / 275 = 60 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 30 : 151 / 276 = 54 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 31 : 214 / 276 = 77 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 32 : 223 / 276 = 80 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 33 : 223 / 276 = 80 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 34 : 226 / 276 = 81 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 35 : 224 / 275 = 81 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 36 : 232 / 276 = 84 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 37 : 240 / 276 = 86 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 38 : 225 / 276 = 81 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 39 : 229 / 276 = 82 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 40 : 196 / 276 = 71 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 41 : 237 / 276 = 85 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 42 : 230 / 275 = 83 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 43 : 144 / 276 = 52 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 44 : 241 / 276 = 87 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 45 : 258 / 276 = 93 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 46 : 217 / 276 = 78 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 47 : 222 / 275 = 80 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 48 : 190 / 275 = 69 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 49 : 188 / 274 = 68 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 50 : 208 / 276 = 75 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 51 : 251 / 276 = 90 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 52 : 226 / 276 = 81 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 53 : 236 / 276 = 85 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 54 : 244 / 274 = 89 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 55 : 220 / 276 = 79 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 56 : 224 / 275 = 81 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 57 : 259 / 276 = 93 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 58 : 259 / 273 = 94 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 59 : 239 / 276 = 86 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 60 : 484 / 561 = 86 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 61 : 493 / 566 = 87 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 62 : 437 / 572 = 76 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 63 : 483 / 570 = 84 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 64 : 418 / 574 = 72 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 65 : 473 / 573 = 82 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 66 : 365 / 573 = 63 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 67 : 443 / 575 = 77 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 68 : 341 / 575 = 59 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 69 : 490 / 575 = 85 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 70 : 169 / 575 = 29 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 71 : 159 / 575 = 27 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 72 : 66 / 571 = 11 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 73 : 284 / 570 = 49 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 74 : 376 / 569 = 66 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 75 : 352 / 573 = 61 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 76 : 308 / 574 = 53 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 77 : 391 / 573 = 68 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 78 : 411 / 575 = 71 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 79 : 535 / 574 = 93 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 80 : 487 / 573 = 84 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 81 : 144 / 575 = 25 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 82 : 309 / 575 = 53 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 83 : 104 / 572 = 18 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 84 : 412 / 574 = 71 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 85 : 322 / 574 = 56 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 86 : 470 / 575 = 81 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 87 : 454 / 576 = 78 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 88 : 394 / 575 = 68 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 89 : 420 / 576 = 72 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 90 : 174 / 574 = 30 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 91 : 415 / 568 = 73 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 92 : 354 / 576 = 61 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 93 : 369 / 573 = 64 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 94 : 522 / 574 = 90 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 95 : 537 / 575 = 93 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 96 : 557 / 575 = 96 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 97 : 544 / 574 = 94 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 98 : 527 / 575 = 91 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 99 : 520 / 574 = 90 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 100 : 451 / 574 = 78 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 101 : 530 / 574 = 92 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 102 : 347 / 575 = 60 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 103 : 442 / 576 = 76 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 104 : 252 / 575 = 43 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 105 : 225 / 575 = 39 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 106 : 261 / 576 = 45 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 107 : 536 / 576 = 93 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 108 : 487 / 575 = 84 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 109 : 386 / 575 = 67 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 110 : 505 / 575 = 87 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 111 : 510 / 576 = 88 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 112 : 542 / 575 = 94 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 113 : 494 / 576 = 85 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 114 : 453 / 576 = 78 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 115 : 470 / 576 = 81 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 116 : 375 / 575 = 65 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 117 : 507 / 575 = 88 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 118 : 438 / 575 = 76 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 119 : 493 / 576 = 85 %
[ Mon Jul  8 17:19:27 2024 ] Accuracy of 120 : 248 / 274 = 90 %
[ Mon Jul  8 17:19:27 2024 ] Training epoch: 61
[ Mon Jul  8 17:19:27 2024 ] 	Batch(0/7879) done. Loss: 0.2130  lr:0.000100
[ Mon Jul  8 17:19:50 2024 ] 	Batch(100/7879) done. Loss: 0.0698  lr:0.000100
[ Mon Jul  8 17:20:13 2024 ] 	Batch(200/7879) done. Loss: 0.2778  lr:0.000100
[ Mon Jul  8 17:20:35 2024 ] 	Batch(300/7879) done. Loss: 0.3805  lr:0.000100
[ Mon Jul  8 17:20:58 2024 ] 	Batch(400/7879) done. Loss: 0.2389  lr:0.000100
[ Mon Jul  8 17:21:21 2024 ] 
Training: Epoch [60/120], Step [499], Loss: 0.4132564067840576, Training Accuracy: 89.325
[ Mon Jul  8 17:21:21 2024 ] 	Batch(500/7879) done. Loss: 0.3983  lr:0.000100
[ Mon Jul  8 17:21:44 2024 ] 	Batch(600/7879) done. Loss: 0.2002  lr:0.000100
[ Mon Jul  8 17:22:06 2024 ] 	Batch(700/7879) done. Loss: 1.3125  lr:0.000100
[ Mon Jul  8 17:22:29 2024 ] 	Batch(800/7879) done. Loss: 0.0865  lr:0.000100
[ Mon Jul  8 17:22:52 2024 ] 	Batch(900/7879) done. Loss: 0.0996  lr:0.000100
[ Mon Jul  8 17:23:14 2024 ] 
Training: Epoch [60/120], Step [999], Loss: 0.42628827691078186, Training Accuracy: 89.525
[ Mon Jul  8 17:23:14 2024 ] 	Batch(1000/7879) done. Loss: 0.5703  lr:0.000100
[ Mon Jul  8 17:23:37 2024 ] 	Batch(1100/7879) done. Loss: 0.2077  lr:0.000100
[ Mon Jul  8 17:24:00 2024 ] 	Batch(1200/7879) done. Loss: 0.0907  lr:0.000100
[ Mon Jul  8 17:24:23 2024 ] 	Batch(1300/7879) done. Loss: 0.0864  lr:0.000100
[ Mon Jul  8 17:24:45 2024 ] 	Batch(1400/7879) done. Loss: 1.3624  lr:0.000100
[ Mon Jul  8 17:25:08 2024 ] 
Training: Epoch [60/120], Step [1499], Loss: 0.0803738534450531, Training Accuracy: 89.95833333333333
[ Mon Jul  8 17:25:08 2024 ] 	Batch(1500/7879) done. Loss: 0.2151  lr:0.000100
[ Mon Jul  8 17:25:31 2024 ] 	Batch(1600/7879) done. Loss: 0.3124  lr:0.000100
[ Mon Jul  8 17:25:53 2024 ] 	Batch(1700/7879) done. Loss: 0.4937  lr:0.000100
[ Mon Jul  8 17:26:16 2024 ] 	Batch(1800/7879) done. Loss: 0.3584  lr:0.000100
[ Mon Jul  8 17:26:39 2024 ] 	Batch(1900/7879) done. Loss: 0.8428  lr:0.000100
[ Mon Jul  8 17:27:01 2024 ] 
Training: Epoch [60/120], Step [1999], Loss: 0.8578534722328186, Training Accuracy: 90.25625000000001
[ Mon Jul  8 17:27:01 2024 ] 	Batch(2000/7879) done. Loss: 0.1571  lr:0.000100
[ Mon Jul  8 17:27:24 2024 ] 	Batch(2100/7879) done. Loss: 0.4248  lr:0.000100
[ Mon Jul  8 17:27:47 2024 ] 	Batch(2200/7879) done. Loss: 0.5570  lr:0.000100
[ Mon Jul  8 17:28:10 2024 ] 	Batch(2300/7879) done. Loss: 0.6396  lr:0.000100
[ Mon Jul  8 17:28:33 2024 ] 	Batch(2400/7879) done. Loss: 0.1124  lr:0.000100
[ Mon Jul  8 17:28:56 2024 ] 
Training: Epoch [60/120], Step [2499], Loss: 0.040792278945446014, Training Accuracy: 90.42999999999999
[ Mon Jul  8 17:28:56 2024 ] 	Batch(2500/7879) done. Loss: 0.4757  lr:0.000100
[ Mon Jul  8 17:29:19 2024 ] 	Batch(2600/7879) done. Loss: 0.6153  lr:0.000100
[ Mon Jul  8 17:29:41 2024 ] 	Batch(2700/7879) done. Loss: 0.1598  lr:0.000100
[ Mon Jul  8 17:30:04 2024 ] 	Batch(2800/7879) done. Loss: 0.0406  lr:0.000100
[ Mon Jul  8 17:30:28 2024 ] 	Batch(2900/7879) done. Loss: 0.0669  lr:0.000100
[ Mon Jul  8 17:30:51 2024 ] 
Training: Epoch [60/120], Step [2999], Loss: 0.04507432132959366, Training Accuracy: 90.49166666666667
[ Mon Jul  8 17:30:51 2024 ] 	Batch(3000/7879) done. Loss: 0.1606  lr:0.000100
[ Mon Jul  8 17:31:14 2024 ] 	Batch(3100/7879) done. Loss: 0.0530  lr:0.000100
[ Mon Jul  8 17:31:38 2024 ] 	Batch(3200/7879) done. Loss: 0.3135  lr:0.000100
[ Mon Jul  8 17:32:01 2024 ] 	Batch(3300/7879) done. Loss: 0.6646  lr:0.000100
[ Mon Jul  8 17:32:25 2024 ] 	Batch(3400/7879) done. Loss: 0.1788  lr:0.000100
[ Mon Jul  8 17:32:48 2024 ] 
Training: Epoch [60/120], Step [3499], Loss: 0.1156277284026146, Training Accuracy: 90.60000000000001
[ Mon Jul  8 17:32:48 2024 ] 	Batch(3500/7879) done. Loss: 0.2173  lr:0.000100
[ Mon Jul  8 17:33:11 2024 ] 	Batch(3600/7879) done. Loss: 0.1184  lr:0.000100
[ Mon Jul  8 17:33:33 2024 ] 	Batch(3700/7879) done. Loss: 0.5786  lr:0.000100
[ Mon Jul  8 17:33:56 2024 ] 	Batch(3800/7879) done. Loss: 0.7810  lr:0.000100
[ Mon Jul  8 17:34:19 2024 ] 	Batch(3900/7879) done. Loss: 0.3669  lr:0.000100
[ Mon Jul  8 17:34:41 2024 ] 
Training: Epoch [60/120], Step [3999], Loss: 0.06789293140172958, Training Accuracy: 90.75
[ Mon Jul  8 17:34:42 2024 ] 	Batch(4000/7879) done. Loss: 0.1669  lr:0.000100
[ Mon Jul  8 17:35:05 2024 ] 	Batch(4100/7879) done. Loss: 0.3262  lr:0.000100
[ Mon Jul  8 17:35:27 2024 ] 	Batch(4200/7879) done. Loss: 0.3035  lr:0.000100
[ Mon Jul  8 17:35:50 2024 ] 	Batch(4300/7879) done. Loss: 0.0364  lr:0.000100
[ Mon Jul  8 17:36:13 2024 ] 	Batch(4400/7879) done. Loss: 0.5550  lr:0.000100
[ Mon Jul  8 17:36:35 2024 ] 
Training: Epoch [60/120], Step [4499], Loss: 0.3146943151950836, Training Accuracy: 90.91944444444444
[ Mon Jul  8 17:36:35 2024 ] 	Batch(4500/7879) done. Loss: 0.3886  lr:0.000100
[ Mon Jul  8 17:36:58 2024 ] 	Batch(4600/7879) done. Loss: 0.0948  lr:0.000100
[ Mon Jul  8 17:37:21 2024 ] 	Batch(4700/7879) done. Loss: 0.0103  lr:0.000100
[ Mon Jul  8 17:37:43 2024 ] 	Batch(4800/7879) done. Loss: 0.3913  lr:0.000100
[ Mon Jul  8 17:38:06 2024 ] 	Batch(4900/7879) done. Loss: 0.5455  lr:0.000100
[ Mon Jul  8 17:38:29 2024 ] 
Training: Epoch [60/120], Step [4999], Loss: 0.05712777376174927, Training Accuracy: 91.09
[ Mon Jul  8 17:38:29 2024 ] 	Batch(5000/7879) done. Loss: 0.0372  lr:0.000100
[ Mon Jul  8 17:38:52 2024 ] 	Batch(5100/7879) done. Loss: 0.6658  lr:0.000100
[ Mon Jul  8 17:39:15 2024 ] 	Batch(5200/7879) done. Loss: 0.0319  lr:0.000100
[ Mon Jul  8 17:39:37 2024 ] 	Batch(5300/7879) done. Loss: 0.1097  lr:0.000100
[ Mon Jul  8 17:40:00 2024 ] 	Batch(5400/7879) done. Loss: 0.0411  lr:0.000100
[ Mon Jul  8 17:40:22 2024 ] 
Training: Epoch [60/120], Step [5499], Loss: 0.0357302650809288, Training Accuracy: 91.22954545454546
[ Mon Jul  8 17:40:23 2024 ] 	Batch(5500/7879) done. Loss: 0.0860  lr:0.000100
[ Mon Jul  8 17:40:45 2024 ] 	Batch(5600/7879) done. Loss: 0.4770  lr:0.000100
[ Mon Jul  8 17:41:09 2024 ] 	Batch(5700/7879) done. Loss: 0.1695  lr:0.000100
[ Mon Jul  8 17:41:32 2024 ] 	Batch(5800/7879) done. Loss: 0.2760  lr:0.000100
[ Mon Jul  8 17:41:56 2024 ] 	Batch(5900/7879) done. Loss: 0.0292  lr:0.000100
[ Mon Jul  8 17:42:19 2024 ] 
Training: Epoch [60/120], Step [5999], Loss: 0.011662237346172333, Training Accuracy: 91.28125
[ Mon Jul  8 17:42:19 2024 ] 	Batch(6000/7879) done. Loss: 0.2748  lr:0.000100
[ Mon Jul  8 17:42:42 2024 ] 	Batch(6100/7879) done. Loss: 0.0736  lr:0.000100
[ Mon Jul  8 17:43:05 2024 ] 	Batch(6200/7879) done. Loss: 0.3073  lr:0.000100
[ Mon Jul  8 17:43:27 2024 ] 	Batch(6300/7879) done. Loss: 0.1428  lr:0.000100
[ Mon Jul  8 17:43:50 2024 ] 	Batch(6400/7879) done. Loss: 0.5941  lr:0.000100
[ Mon Jul  8 17:44:13 2024 ] 
Training: Epoch [60/120], Step [6499], Loss: 0.0056353467516601086, Training Accuracy: 91.42884615384615
[ Mon Jul  8 17:44:13 2024 ] 	Batch(6500/7879) done. Loss: 0.0460  lr:0.000100
[ Mon Jul  8 17:44:36 2024 ] 	Batch(6600/7879) done. Loss: 0.5323  lr:0.000100
[ Mon Jul  8 17:44:58 2024 ] 	Batch(6700/7879) done. Loss: 0.4775  lr:0.000100
[ Mon Jul  8 17:45:21 2024 ] 	Batch(6800/7879) done. Loss: 0.2061  lr:0.000100
[ Mon Jul  8 17:45:44 2024 ] 	Batch(6900/7879) done. Loss: 0.2944  lr:0.000100
[ Mon Jul  8 17:46:06 2024 ] 
Training: Epoch [60/120], Step [6999], Loss: 0.07958187162876129, Training Accuracy: 91.4875
[ Mon Jul  8 17:46:07 2024 ] 	Batch(7000/7879) done. Loss: 0.1293  lr:0.000100
[ Mon Jul  8 17:46:29 2024 ] 	Batch(7100/7879) done. Loss: 0.2892  lr:0.000100
[ Mon Jul  8 17:46:52 2024 ] 	Batch(7200/7879) done. Loss: 0.7671  lr:0.000100
[ Mon Jul  8 17:47:16 2024 ] 	Batch(7300/7879) done. Loss: 0.2027  lr:0.000100
[ Mon Jul  8 17:47:39 2024 ] 	Batch(7400/7879) done. Loss: 0.0906  lr:0.000100
[ Mon Jul  8 17:48:02 2024 ] 
Training: Epoch [60/120], Step [7499], Loss: 0.06815887242555618, Training Accuracy: 91.55833333333334
[ Mon Jul  8 17:48:03 2024 ] 	Batch(7500/7879) done. Loss: 0.0224  lr:0.000100
[ Mon Jul  8 17:48:26 2024 ] 	Batch(7600/7879) done. Loss: 0.9818  lr:0.000100
[ Mon Jul  8 17:48:49 2024 ] 	Batch(7700/7879) done. Loss: 0.1194  lr:0.000100
[ Mon Jul  8 17:49:12 2024 ] 	Batch(7800/7879) done. Loss: 0.2669  lr:0.000100
[ Mon Jul  8 17:49:30 2024 ] 	Mean training loss: 0.2812.
[ Mon Jul  8 17:49:30 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 17:49:30 2024 ] Training epoch: 62
[ Mon Jul  8 17:49:30 2024 ] 	Batch(0/7879) done. Loss: 0.1535  lr:0.000100
[ Mon Jul  8 17:49:53 2024 ] 	Batch(100/7879) done. Loss: 0.7669  lr:0.000100
[ Mon Jul  8 17:50:16 2024 ] 	Batch(200/7879) done. Loss: 0.1960  lr:0.000100
[ Mon Jul  8 17:50:38 2024 ] 	Batch(300/7879) done. Loss: 0.0950  lr:0.000100
[ Mon Jul  8 17:51:01 2024 ] 	Batch(400/7879) done. Loss: 0.0759  lr:0.000100
[ Mon Jul  8 17:51:24 2024 ] 
Training: Epoch [61/120], Step [499], Loss: 0.39658620953559875, Training Accuracy: 92.325
[ Mon Jul  8 17:51:24 2024 ] 	Batch(500/7879) done. Loss: 0.0388  lr:0.000100
[ Mon Jul  8 17:51:46 2024 ] 	Batch(600/7879) done. Loss: 0.1855  lr:0.000100
[ Mon Jul  8 17:52:09 2024 ] 	Batch(700/7879) done. Loss: 0.5268  lr:0.000100
[ Mon Jul  8 17:52:32 2024 ] 	Batch(800/7879) done. Loss: 0.1187  lr:0.000100
[ Mon Jul  8 17:52:55 2024 ] 	Batch(900/7879) done. Loss: 0.0104  lr:0.000100
[ Mon Jul  8 17:53:17 2024 ] 
Training: Epoch [61/120], Step [999], Loss: 0.1083088219165802, Training Accuracy: 93.1125
[ Mon Jul  8 17:53:17 2024 ] 	Batch(1000/7879) done. Loss: 0.2730  lr:0.000100
[ Mon Jul  8 17:53:40 2024 ] 	Batch(1100/7879) done. Loss: 0.1656  lr:0.000100
[ Mon Jul  8 17:54:03 2024 ] 	Batch(1200/7879) done. Loss: 0.2731  lr:0.000100
[ Mon Jul  8 17:54:25 2024 ] 	Batch(1300/7879) done. Loss: 0.6748  lr:0.000100
[ Mon Jul  8 17:54:48 2024 ] 	Batch(1400/7879) done. Loss: 0.8217  lr:0.000100
[ Mon Jul  8 17:55:10 2024 ] 
Training: Epoch [61/120], Step [1499], Loss: 0.055899862200021744, Training Accuracy: 92.90833333333333
[ Mon Jul  8 17:55:11 2024 ] 	Batch(1500/7879) done. Loss: 0.0413  lr:0.000100
[ Mon Jul  8 17:55:33 2024 ] 	Batch(1600/7879) done. Loss: 0.4841  lr:0.000100
[ Mon Jul  8 17:55:56 2024 ] 	Batch(1700/7879) done. Loss: 0.2611  lr:0.000100
[ Mon Jul  8 17:56:19 2024 ] 	Batch(1800/7879) done. Loss: 0.1356  lr:0.000100
[ Mon Jul  8 17:56:42 2024 ] 	Batch(1900/7879) done. Loss: 0.5983  lr:0.000100
[ Mon Jul  8 17:57:04 2024 ] 
Training: Epoch [61/120], Step [1999], Loss: 0.5663434267044067, Training Accuracy: 93.025
[ Mon Jul  8 17:57:04 2024 ] 	Batch(2000/7879) done. Loss: 0.5428  lr:0.000100
[ Mon Jul  8 17:57:27 2024 ] 	Batch(2100/7879) done. Loss: 0.3865  lr:0.000100
[ Mon Jul  8 17:57:50 2024 ] 	Batch(2200/7879) done. Loss: 0.2688  lr:0.000100
[ Mon Jul  8 17:58:12 2024 ] 	Batch(2300/7879) done. Loss: 0.2025  lr:0.000100
[ Mon Jul  8 17:58:35 2024 ] 	Batch(2400/7879) done. Loss: 0.1074  lr:0.000100
[ Mon Jul  8 17:58:58 2024 ] 
Training: Epoch [61/120], Step [2499], Loss: 0.43378767371177673, Training Accuracy: 93.17
[ Mon Jul  8 17:58:58 2024 ] 	Batch(2500/7879) done. Loss: 0.3076  lr:0.000100
[ Mon Jul  8 17:59:21 2024 ] 	Batch(2600/7879) done. Loss: 0.4793  lr:0.000100
[ Mon Jul  8 17:59:43 2024 ] 	Batch(2700/7879) done. Loss: 0.0240  lr:0.000100
[ Mon Jul  8 18:00:06 2024 ] 	Batch(2800/7879) done. Loss: 0.0256  lr:0.000100
[ Mon Jul  8 18:00:29 2024 ] 	Batch(2900/7879) done. Loss: 0.3818  lr:0.000100
[ Mon Jul  8 18:00:51 2024 ] 
Training: Epoch [61/120], Step [2999], Loss: 0.07592768967151642, Training Accuracy: 93.08333333333333
[ Mon Jul  8 18:00:51 2024 ] 	Batch(3000/7879) done. Loss: 0.7264  lr:0.000100
[ Mon Jul  8 18:01:14 2024 ] 	Batch(3100/7879) done. Loss: 0.2599  lr:0.000100
[ Mon Jul  8 18:01:37 2024 ] 	Batch(3200/7879) done. Loss: 0.2049  lr:0.000100
[ Mon Jul  8 18:02:00 2024 ] 	Batch(3300/7879) done. Loss: 0.0746  lr:0.000100
[ Mon Jul  8 18:02:22 2024 ] 	Batch(3400/7879) done. Loss: 0.1711  lr:0.000100
[ Mon Jul  8 18:02:45 2024 ] 
Training: Epoch [61/120], Step [3499], Loss: 0.2096383422613144, Training Accuracy: 93.03214285714286
[ Mon Jul  8 18:02:45 2024 ] 	Batch(3500/7879) done. Loss: 0.3067  lr:0.000100
[ Mon Jul  8 18:03:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0031  lr:0.000100
[ Mon Jul  8 18:03:31 2024 ] 	Batch(3700/7879) done. Loss: 0.0501  lr:0.000100
[ Mon Jul  8 18:03:53 2024 ] 	Batch(3800/7879) done. Loss: 0.1619  lr:0.000100
[ Mon Jul  8 18:04:17 2024 ] 	Batch(3900/7879) done. Loss: 0.0714  lr:0.000100
[ Mon Jul  8 18:04:40 2024 ] 
Training: Epoch [61/120], Step [3999], Loss: 0.5409917235374451, Training Accuracy: 93.0625
[ Mon Jul  8 18:04:40 2024 ] 	Batch(4000/7879) done. Loss: 0.7514  lr:0.000100
[ Mon Jul  8 18:05:04 2024 ] 	Batch(4100/7879) done. Loss: 0.1276  lr:0.000100
[ Mon Jul  8 18:05:27 2024 ] 	Batch(4200/7879) done. Loss: 0.1156  lr:0.000100
[ Mon Jul  8 18:05:51 2024 ] 	Batch(4300/7879) done. Loss: 0.0715  lr:0.000100
[ Mon Jul  8 18:06:14 2024 ] 	Batch(4400/7879) done. Loss: 0.1507  lr:0.000100
[ Mon Jul  8 18:06:37 2024 ] 
Training: Epoch [61/120], Step [4499], Loss: 0.10246056318283081, Training Accuracy: 93.06944444444444
[ Mon Jul  8 18:06:37 2024 ] 	Batch(4500/7879) done. Loss: 0.0493  lr:0.000100
[ Mon Jul  8 18:07:00 2024 ] 	Batch(4600/7879) done. Loss: 0.0676  lr:0.000100
[ Mon Jul  8 18:07:22 2024 ] 	Batch(4700/7879) done. Loss: 0.4079  lr:0.000100
[ Mon Jul  8 18:07:45 2024 ] 	Batch(4800/7879) done. Loss: 0.0308  lr:0.000100
[ Mon Jul  8 18:08:08 2024 ] 	Batch(4900/7879) done. Loss: 0.0689  lr:0.000100
[ Mon Jul  8 18:08:30 2024 ] 
Training: Epoch [61/120], Step [4999], Loss: 0.22321084141731262, Training Accuracy: 93.08999999999999
[ Mon Jul  8 18:08:30 2024 ] 	Batch(5000/7879) done. Loss: 0.2844  lr:0.000100
[ Mon Jul  8 18:08:53 2024 ] 	Batch(5100/7879) done. Loss: 0.5165  lr:0.000100
[ Mon Jul  8 18:09:16 2024 ] 	Batch(5200/7879) done. Loss: 0.1852  lr:0.000100
[ Mon Jul  8 18:09:39 2024 ] 	Batch(5300/7879) done. Loss: 0.0197  lr:0.000100
[ Mon Jul  8 18:10:01 2024 ] 	Batch(5400/7879) done. Loss: 0.0327  lr:0.000100
[ Mon Jul  8 18:10:24 2024 ] 
Training: Epoch [61/120], Step [5499], Loss: 0.4903298616409302, Training Accuracy: 93.10681818181818
[ Mon Jul  8 18:10:24 2024 ] 	Batch(5500/7879) done. Loss: 0.2172  lr:0.000100
[ Mon Jul  8 18:10:47 2024 ] 	Batch(5600/7879) done. Loss: 0.1104  lr:0.000100
[ Mon Jul  8 18:11:10 2024 ] 	Batch(5700/7879) done. Loss: 0.4831  lr:0.000100
[ Mon Jul  8 18:11:32 2024 ] 	Batch(5800/7879) done. Loss: 0.3024  lr:0.000100
[ Mon Jul  8 18:11:55 2024 ] 	Batch(5900/7879) done. Loss: 0.0907  lr:0.000100
[ Mon Jul  8 18:12:18 2024 ] 
Training: Epoch [61/120], Step [5999], Loss: 0.18487459421157837, Training Accuracy: 93.06041666666667
[ Mon Jul  8 18:12:18 2024 ] 	Batch(6000/7879) done. Loss: 0.2805  lr:0.000100
[ Mon Jul  8 18:12:41 2024 ] 	Batch(6100/7879) done. Loss: 0.2271  lr:0.000100
[ Mon Jul  8 18:13:04 2024 ] 	Batch(6200/7879) done. Loss: 1.1609  lr:0.000100
[ Mon Jul  8 18:13:26 2024 ] 	Batch(6300/7879) done. Loss: 0.1941  lr:0.000100
[ Mon Jul  8 18:13:50 2024 ] 	Batch(6400/7879) done. Loss: 0.1175  lr:0.000100
[ Mon Jul  8 18:14:13 2024 ] 
Training: Epoch [61/120], Step [6499], Loss: 0.48305651545524597, Training Accuracy: 93.09807692307692
[ Mon Jul  8 18:14:13 2024 ] 	Batch(6500/7879) done. Loss: 0.0419  lr:0.000100
[ Mon Jul  8 18:14:37 2024 ] 	Batch(6600/7879) done. Loss: 0.0515  lr:0.000100
[ Mon Jul  8 18:15:00 2024 ] 	Batch(6700/7879) done. Loss: 0.0316  lr:0.000100
[ Mon Jul  8 18:15:24 2024 ] 	Batch(6800/7879) done. Loss: 0.1189  lr:0.000100
[ Mon Jul  8 18:15:46 2024 ] 	Batch(6900/7879) done. Loss: 0.3098  lr:0.000100
[ Mon Jul  8 18:16:09 2024 ] 
Training: Epoch [61/120], Step [6999], Loss: 0.3846301734447479, Training Accuracy: 93.11964285714286
[ Mon Jul  8 18:16:09 2024 ] 	Batch(7000/7879) done. Loss: 0.0678  lr:0.000100
[ Mon Jul  8 18:16:32 2024 ] 	Batch(7100/7879) done. Loss: 0.2710  lr:0.000100
[ Mon Jul  8 18:16:55 2024 ] 	Batch(7200/7879) done. Loss: 0.1321  lr:0.000100
[ Mon Jul  8 18:17:18 2024 ] 	Batch(7300/7879) done. Loss: 0.0356  lr:0.000100
[ Mon Jul  8 18:17:42 2024 ] 	Batch(7400/7879) done. Loss: 0.2960  lr:0.000100
[ Mon Jul  8 18:18:05 2024 ] 
Training: Epoch [61/120], Step [7499], Loss: 0.1076662465929985, Training Accuracy: 93.16833333333334
[ Mon Jul  8 18:18:05 2024 ] 	Batch(7500/7879) done. Loss: 0.0729  lr:0.000100
[ Mon Jul  8 18:18:29 2024 ] 	Batch(7600/7879) done. Loss: 0.1361  lr:0.000100
[ Mon Jul  8 18:18:52 2024 ] 	Batch(7700/7879) done. Loss: 0.5463  lr:0.000100
[ Mon Jul  8 18:19:15 2024 ] 	Batch(7800/7879) done. Loss: 0.4315  lr:0.000100
[ Mon Jul  8 18:19:34 2024 ] 	Mean training loss: 0.2341.
[ Mon Jul  8 18:19:34 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 18:19:34 2024 ] Training epoch: 63
[ Mon Jul  8 18:19:35 2024 ] 	Batch(0/7879) done. Loss: 0.0187  lr:0.000100
[ Mon Jul  8 18:19:57 2024 ] 	Batch(100/7879) done. Loss: 0.2796  lr:0.000100
[ Mon Jul  8 18:20:20 2024 ] 	Batch(200/7879) done. Loss: 0.2296  lr:0.000100
[ Mon Jul  8 18:20:42 2024 ] 	Batch(300/7879) done. Loss: 0.0476  lr:0.000100
[ Mon Jul  8 18:21:05 2024 ] 	Batch(400/7879) done. Loss: 1.5449  lr:0.000100
[ Mon Jul  8 18:21:28 2024 ] 
Training: Epoch [62/120], Step [499], Loss: 0.012919332832098007, Training Accuracy: 94.25
[ Mon Jul  8 18:21:28 2024 ] 	Batch(500/7879) done. Loss: 0.1901  lr:0.000100
[ Mon Jul  8 18:21:51 2024 ] 	Batch(600/7879) done. Loss: 0.5091  lr:0.000100
[ Mon Jul  8 18:22:13 2024 ] 	Batch(700/7879) done. Loss: 0.3009  lr:0.000100
[ Mon Jul  8 18:22:36 2024 ] 	Batch(800/7879) done. Loss: 0.0232  lr:0.000100
[ Mon Jul  8 18:22:59 2024 ] 	Batch(900/7879) done. Loss: 0.0875  lr:0.000100
[ Mon Jul  8 18:23:21 2024 ] 
Training: Epoch [62/120], Step [999], Loss: 0.10972903668880463, Training Accuracy: 94.0125
[ Mon Jul  8 18:23:22 2024 ] 	Batch(1000/7879) done. Loss: 0.1047  lr:0.000100
[ Mon Jul  8 18:23:44 2024 ] 	Batch(1100/7879) done. Loss: 0.0581  lr:0.000100
[ Mon Jul  8 18:24:07 2024 ] 	Batch(1200/7879) done. Loss: 0.0220  lr:0.000100
[ Mon Jul  8 18:24:30 2024 ] 	Batch(1300/7879) done. Loss: 0.1500  lr:0.000100
[ Mon Jul  8 18:24:52 2024 ] 	Batch(1400/7879) done. Loss: 0.1747  lr:0.000100
[ Mon Jul  8 18:25:15 2024 ] 
Training: Epoch [62/120], Step [1499], Loss: 0.031956981867551804, Training Accuracy: 94.03333333333333
[ Mon Jul  8 18:25:15 2024 ] 	Batch(1500/7879) done. Loss: 0.1096  lr:0.000100
[ Mon Jul  8 18:25:38 2024 ] 	Batch(1600/7879) done. Loss: 0.6571  lr:0.000100
[ Mon Jul  8 18:26:01 2024 ] 	Batch(1700/7879) done. Loss: 0.0186  lr:0.000100
[ Mon Jul  8 18:26:23 2024 ] 	Batch(1800/7879) done. Loss: 0.2311  lr:0.000100
[ Mon Jul  8 18:26:46 2024 ] 	Batch(1900/7879) done. Loss: 0.0127  lr:0.000100
[ Mon Jul  8 18:27:08 2024 ] 
Training: Epoch [62/120], Step [1999], Loss: 0.34201306104660034, Training Accuracy: 93.94375000000001
[ Mon Jul  8 18:27:09 2024 ] 	Batch(2000/7879) done. Loss: 0.1206  lr:0.000100
[ Mon Jul  8 18:27:32 2024 ] 	Batch(2100/7879) done. Loss: 0.0042  lr:0.000100
[ Mon Jul  8 18:27:56 2024 ] 	Batch(2200/7879) done. Loss: 0.0978  lr:0.000100
[ Mon Jul  8 18:28:19 2024 ] 	Batch(2300/7879) done. Loss: 0.4326  lr:0.000100
[ Mon Jul  8 18:28:42 2024 ] 	Batch(2400/7879) done. Loss: 0.5663  lr:0.000100
[ Mon Jul  8 18:29:04 2024 ] 
Training: Epoch [62/120], Step [2499], Loss: 0.13360004127025604, Training Accuracy: 93.83500000000001
[ Mon Jul  8 18:29:04 2024 ] 	Batch(2500/7879) done. Loss: 0.1447  lr:0.000100
[ Mon Jul  8 18:29:27 2024 ] 	Batch(2600/7879) done. Loss: 0.0511  lr:0.000100
[ Mon Jul  8 18:29:50 2024 ] 	Batch(2700/7879) done. Loss: 0.1266  lr:0.000100
[ Mon Jul  8 18:30:13 2024 ] 	Batch(2800/7879) done. Loss: 0.9114  lr:0.000100
[ Mon Jul  8 18:30:35 2024 ] 	Batch(2900/7879) done. Loss: 0.1130  lr:0.000100
[ Mon Jul  8 18:30:58 2024 ] 
Training: Epoch [62/120], Step [2999], Loss: 0.5907873511314392, Training Accuracy: 93.85416666666667
[ Mon Jul  8 18:30:58 2024 ] 	Batch(3000/7879) done. Loss: 0.2271  lr:0.000100
[ Mon Jul  8 18:31:21 2024 ] 	Batch(3100/7879) done. Loss: 0.3450  lr:0.000100
[ Mon Jul  8 18:31:44 2024 ] 	Batch(3200/7879) done. Loss: 0.0061  lr:0.000100
[ Mon Jul  8 18:32:06 2024 ] 	Batch(3300/7879) done. Loss: 0.0311  lr:0.000100
[ Mon Jul  8 18:32:29 2024 ] 	Batch(3400/7879) done. Loss: 0.3456  lr:0.000100
[ Mon Jul  8 18:32:51 2024 ] 
Training: Epoch [62/120], Step [3499], Loss: 0.012241208925843239, Training Accuracy: 93.82857142857142
[ Mon Jul  8 18:32:52 2024 ] 	Batch(3500/7879) done. Loss: 0.7370  lr:0.000100
[ Mon Jul  8 18:33:14 2024 ] 	Batch(3600/7879) done. Loss: 0.3285  lr:0.000100
[ Mon Jul  8 18:33:37 2024 ] 	Batch(3700/7879) done. Loss: 0.7608  lr:0.000100
[ Mon Jul  8 18:34:00 2024 ] 	Batch(3800/7879) done. Loss: 0.1089  lr:0.000100
[ Mon Jul  8 18:34:22 2024 ] 	Batch(3900/7879) done. Loss: 0.3518  lr:0.000100
[ Mon Jul  8 18:34:45 2024 ] 
Training: Epoch [62/120], Step [3999], Loss: 0.051180385053157806, Training Accuracy: 93.69375
[ Mon Jul  8 18:34:45 2024 ] 	Batch(4000/7879) done. Loss: 0.1411  lr:0.000100
[ Mon Jul  8 18:35:08 2024 ] 	Batch(4100/7879) done. Loss: 0.1304  lr:0.000100
[ Mon Jul  8 18:35:31 2024 ] 	Batch(4200/7879) done. Loss: 0.1183  lr:0.000100
[ Mon Jul  8 18:35:53 2024 ] 	Batch(4300/7879) done. Loss: 0.1199  lr:0.000100
[ Mon Jul  8 18:36:16 2024 ] 	Batch(4400/7879) done. Loss: 0.2054  lr:0.000100
[ Mon Jul  8 18:36:39 2024 ] 
Training: Epoch [62/120], Step [4499], Loss: 0.48287421464920044, Training Accuracy: 93.71666666666667
[ Mon Jul  8 18:36:39 2024 ] 	Batch(4500/7879) done. Loss: 0.0365  lr:0.000100
[ Mon Jul  8 18:37:02 2024 ] 	Batch(4600/7879) done. Loss: 0.1299  lr:0.000100
[ Mon Jul  8 18:37:24 2024 ] 	Batch(4700/7879) done. Loss: 0.0879  lr:0.000100
[ Mon Jul  8 18:37:47 2024 ] 	Batch(4800/7879) done. Loss: 0.5712  lr:0.000100
[ Mon Jul  8 18:38:10 2024 ] 	Batch(4900/7879) done. Loss: 0.0765  lr:0.000100
[ Mon Jul  8 18:38:32 2024 ] 
Training: Epoch [62/120], Step [4999], Loss: 0.47934895753860474, Training Accuracy: 93.7225
[ Mon Jul  8 18:38:32 2024 ] 	Batch(5000/7879) done. Loss: 0.2418  lr:0.000100
[ Mon Jul  8 18:38:55 2024 ] 	Batch(5100/7879) done. Loss: 0.3402  lr:0.000100
[ Mon Jul  8 18:39:18 2024 ] 	Batch(5200/7879) done. Loss: 0.3263  lr:0.000100
[ Mon Jul  8 18:39:41 2024 ] 	Batch(5300/7879) done. Loss: 0.3354  lr:0.000100
[ Mon Jul  8 18:40:03 2024 ] 	Batch(5400/7879) done. Loss: 0.0374  lr:0.000100
[ Mon Jul  8 18:40:26 2024 ] 
Training: Epoch [62/120], Step [5499], Loss: 0.3230646252632141, Training Accuracy: 93.75909090909092
[ Mon Jul  8 18:40:26 2024 ] 	Batch(5500/7879) done. Loss: 0.0870  lr:0.000100
[ Mon Jul  8 18:40:49 2024 ] 	Batch(5600/7879) done. Loss: 0.2069  lr:0.000100
[ Mon Jul  8 18:41:12 2024 ] 	Batch(5700/7879) done. Loss: 0.1033  lr:0.000100
[ Mon Jul  8 18:41:34 2024 ] 	Batch(5800/7879) done. Loss: 0.2842  lr:0.000100
[ Mon Jul  8 18:41:57 2024 ] 	Batch(5900/7879) done. Loss: 0.2226  lr:0.000100
[ Mon Jul  8 18:42:20 2024 ] 
Training: Epoch [62/120], Step [5999], Loss: 0.028162727132439613, Training Accuracy: 93.80833333333334
[ Mon Jul  8 18:42:20 2024 ] 	Batch(6000/7879) done. Loss: 0.4723  lr:0.000100
[ Mon Jul  8 18:42:43 2024 ] 	Batch(6100/7879) done. Loss: 0.4504  lr:0.000100
[ Mon Jul  8 18:43:05 2024 ] 	Batch(6200/7879) done. Loss: 0.7028  lr:0.000100
[ Mon Jul  8 18:43:28 2024 ] 	Batch(6300/7879) done. Loss: 0.3144  lr:0.000100
[ Mon Jul  8 18:43:51 2024 ] 	Batch(6400/7879) done. Loss: 0.6047  lr:0.000100
[ Mon Jul  8 18:44:14 2024 ] 
Training: Epoch [62/120], Step [6499], Loss: 0.4566556215286255, Training Accuracy: 93.8
[ Mon Jul  8 18:44:14 2024 ] 	Batch(6500/7879) done. Loss: 0.0657  lr:0.000100
[ Mon Jul  8 18:44:38 2024 ] 	Batch(6600/7879) done. Loss: 0.4931  lr:0.000100
[ Mon Jul  8 18:45:01 2024 ] 	Batch(6700/7879) done. Loss: 0.0417  lr:0.000100
[ Mon Jul  8 18:45:25 2024 ] 	Batch(6800/7879) done. Loss: 0.0884  lr:0.000100
[ Mon Jul  8 18:45:48 2024 ] 	Batch(6900/7879) done. Loss: 0.2406  lr:0.000100
[ Mon Jul  8 18:46:12 2024 ] 
Training: Epoch [62/120], Step [6999], Loss: 0.02128206379711628, Training Accuracy: 93.80178571428571
[ Mon Jul  8 18:46:12 2024 ] 	Batch(7000/7879) done. Loss: 0.2036  lr:0.000100
[ Mon Jul  8 18:46:35 2024 ] 	Batch(7100/7879) done. Loss: 0.1628  lr:0.000100
[ Mon Jul  8 18:46:59 2024 ] 	Batch(7200/7879) done. Loss: 0.1537  lr:0.000100
[ Mon Jul  8 18:47:22 2024 ] 	Batch(7300/7879) done. Loss: 0.4039  lr:0.000100
[ Mon Jul  8 18:47:44 2024 ] 	Batch(7400/7879) done. Loss: 0.1222  lr:0.000100
[ Mon Jul  8 18:48:07 2024 ] 
Training: Epoch [62/120], Step [7499], Loss: 0.34800487756729126, Training Accuracy: 93.79333333333332
[ Mon Jul  8 18:48:07 2024 ] 	Batch(7500/7879) done. Loss: 0.0997  lr:0.000100
[ Mon Jul  8 18:48:30 2024 ] 	Batch(7600/7879) done. Loss: 0.0088  lr:0.000100
[ Mon Jul  8 18:48:52 2024 ] 	Batch(7700/7879) done. Loss: 0.0970  lr:0.000100
[ Mon Jul  8 18:49:15 2024 ] 	Batch(7800/7879) done. Loss: 0.0924  lr:0.000100
[ Mon Jul  8 18:49:33 2024 ] 	Mean training loss: 0.2174.
[ Mon Jul  8 18:49:33 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 18:49:33 2024 ] Training epoch: 64
[ Mon Jul  8 18:49:34 2024 ] 	Batch(0/7879) done. Loss: 0.1700  lr:0.000100
[ Mon Jul  8 18:49:56 2024 ] 	Batch(100/7879) done. Loss: 0.0661  lr:0.000100
[ Mon Jul  8 18:50:19 2024 ] 	Batch(200/7879) done. Loss: 0.1903  lr:0.000100
[ Mon Jul  8 18:50:42 2024 ] 	Batch(300/7879) done. Loss: 1.2596  lr:0.000100
[ Mon Jul  8 18:51:04 2024 ] 	Batch(400/7879) done. Loss: 0.1832  lr:0.000100
[ Mon Jul  8 18:51:27 2024 ] 
Training: Epoch [63/120], Step [499], Loss: 0.01489774975925684, Training Accuracy: 94.3
[ Mon Jul  8 18:51:27 2024 ] 	Batch(500/7879) done. Loss: 0.0925  lr:0.000100
[ Mon Jul  8 18:51:50 2024 ] 	Batch(600/7879) done. Loss: 0.6356  lr:0.000100
[ Mon Jul  8 18:52:12 2024 ] 	Batch(700/7879) done. Loss: 0.3164  lr:0.000100
[ Mon Jul  8 18:52:35 2024 ] 	Batch(800/7879) done. Loss: 0.4198  lr:0.000100
[ Mon Jul  8 18:52:58 2024 ] 	Batch(900/7879) done. Loss: 0.0721  lr:0.000100
[ Mon Jul  8 18:53:20 2024 ] 
Training: Epoch [63/120], Step [999], Loss: 0.0022970065474510193, Training Accuracy: 94.0
[ Mon Jul  8 18:53:20 2024 ] 	Batch(1000/7879) done. Loss: 0.0074  lr:0.000100
[ Mon Jul  8 18:53:43 2024 ] 	Batch(1100/7879) done. Loss: 0.4062  lr:0.000100
[ Mon Jul  8 18:54:06 2024 ] 	Batch(1200/7879) done. Loss: 0.0454  lr:0.000100
[ Mon Jul  8 18:54:28 2024 ] 	Batch(1300/7879) done. Loss: 0.0989  lr:0.000100
[ Mon Jul  8 18:54:51 2024 ] 	Batch(1400/7879) done. Loss: 0.0168  lr:0.000100
[ Mon Jul  8 18:55:14 2024 ] 
Training: Epoch [63/120], Step [1499], Loss: 0.03900481015443802, Training Accuracy: 94.025
[ Mon Jul  8 18:55:14 2024 ] 	Batch(1500/7879) done. Loss: 0.0862  lr:0.000100
[ Mon Jul  8 18:55:37 2024 ] 	Batch(1600/7879) done. Loss: 0.1992  lr:0.000100
[ Mon Jul  8 18:56:00 2024 ] 	Batch(1700/7879) done. Loss: 1.5478  lr:0.000100
[ Mon Jul  8 18:56:23 2024 ] 	Batch(1800/7879) done. Loss: 0.0406  lr:0.000100
[ Mon Jul  8 18:56:46 2024 ] 	Batch(1900/7879) done. Loss: 0.0126  lr:0.000100
[ Mon Jul  8 18:57:09 2024 ] 
Training: Epoch [63/120], Step [1999], Loss: 0.4192204177379608, Training Accuracy: 93.90625
[ Mon Jul  8 18:57:10 2024 ] 	Batch(2000/7879) done. Loss: 0.0779  lr:0.000100
[ Mon Jul  8 18:57:32 2024 ] 	Batch(2100/7879) done. Loss: 0.1486  lr:0.000100
[ Mon Jul  8 18:57:55 2024 ] 	Batch(2200/7879) done. Loss: 0.0135  lr:0.000100
[ Mon Jul  8 18:58:18 2024 ] 	Batch(2300/7879) done. Loss: 0.1090  lr:0.000100
[ Mon Jul  8 18:58:40 2024 ] 	Batch(2400/7879) done. Loss: 0.2115  lr:0.000100
[ Mon Jul  8 18:59:03 2024 ] 
Training: Epoch [63/120], Step [2499], Loss: 0.06879503279924393, Training Accuracy: 93.87
[ Mon Jul  8 18:59:03 2024 ] 	Batch(2500/7879) done. Loss: 0.0688  lr:0.000100
[ Mon Jul  8 18:59:25 2024 ] 	Batch(2600/7879) done. Loss: 0.3581  lr:0.000100
[ Mon Jul  8 18:59:48 2024 ] 	Batch(2700/7879) done. Loss: 0.3777  lr:0.000100
[ Mon Jul  8 19:00:11 2024 ] 	Batch(2800/7879) done. Loss: 0.2041  lr:0.000100
[ Mon Jul  8 19:00:33 2024 ] 	Batch(2900/7879) done. Loss: 0.1951  lr:0.000100
[ Mon Jul  8 19:00:56 2024 ] 
Training: Epoch [63/120], Step [2999], Loss: 0.469074010848999, Training Accuracy: 93.99166666666666
[ Mon Jul  8 19:00:56 2024 ] 	Batch(3000/7879) done. Loss: 0.1219  lr:0.000100
[ Mon Jul  8 19:01:18 2024 ] 	Batch(3100/7879) done. Loss: 0.0175  lr:0.000100
[ Mon Jul  8 19:01:41 2024 ] 	Batch(3200/7879) done. Loss: 0.1954  lr:0.000100
[ Mon Jul  8 19:02:04 2024 ] 	Batch(3300/7879) done. Loss: 0.1337  lr:0.000100
[ Mon Jul  8 19:02:26 2024 ] 	Batch(3400/7879) done. Loss: 0.1961  lr:0.000100
[ Mon Jul  8 19:02:48 2024 ] 
Training: Epoch [63/120], Step [3499], Loss: 0.09584859013557434, Training Accuracy: 94.01071428571429
[ Mon Jul  8 19:02:49 2024 ] 	Batch(3500/7879) done. Loss: 0.6612  lr:0.000100
[ Mon Jul  8 19:03:11 2024 ] 	Batch(3600/7879) done. Loss: 0.3952  lr:0.000100
[ Mon Jul  8 19:03:35 2024 ] 	Batch(3700/7879) done. Loss: 0.1737  lr:0.000100
[ Mon Jul  8 19:03:58 2024 ] 	Batch(3800/7879) done. Loss: 0.3660  lr:0.000100
[ Mon Jul  8 19:04:21 2024 ] 	Batch(3900/7879) done. Loss: 0.0954  lr:0.000100
[ Mon Jul  8 19:04:44 2024 ] 
Training: Epoch [63/120], Step [3999], Loss: 0.13187816739082336, Training Accuracy: 94.05624999999999
[ Mon Jul  8 19:04:45 2024 ] 	Batch(4000/7879) done. Loss: 0.7397  lr:0.000100
[ Mon Jul  8 19:05:08 2024 ] 	Batch(4100/7879) done. Loss: 0.2353  lr:0.000100
[ Mon Jul  8 19:05:31 2024 ] 	Batch(4200/7879) done. Loss: 0.1121  lr:0.000100
[ Mon Jul  8 19:05:55 2024 ] 	Batch(4300/7879) done. Loss: 0.2137  lr:0.000100
[ Mon Jul  8 19:06:18 2024 ] 	Batch(4400/7879) done. Loss: 0.0363  lr:0.000100
[ Mon Jul  8 19:06:41 2024 ] 
Training: Epoch [63/120], Step [4499], Loss: 0.004689735360443592, Training Accuracy: 94.0611111111111
[ Mon Jul  8 19:06:41 2024 ] 	Batch(4500/7879) done. Loss: 0.0905  lr:0.000100
[ Mon Jul  8 19:07:05 2024 ] 	Batch(4600/7879) done. Loss: 0.0338  lr:0.000100
[ Mon Jul  8 19:07:28 2024 ] 	Batch(4700/7879) done. Loss: 0.0445  lr:0.000100
[ Mon Jul  8 19:07:51 2024 ] 	Batch(4800/7879) done. Loss: 0.0612  lr:0.000100
[ Mon Jul  8 19:08:15 2024 ] 	Batch(4900/7879) done. Loss: 0.0348  lr:0.000100
[ Mon Jul  8 19:08:38 2024 ] 
Training: Epoch [63/120], Step [4999], Loss: 0.004302481655031443, Training Accuracy: 94.015
[ Mon Jul  8 19:08:38 2024 ] 	Batch(5000/7879) done. Loss: 0.2038  lr:0.000100
[ Mon Jul  8 19:09:01 2024 ] 	Batch(5100/7879) done. Loss: 0.0871  lr:0.000100
[ Mon Jul  8 19:09:25 2024 ] 	Batch(5200/7879) done. Loss: 0.3400  lr:0.000100
[ Mon Jul  8 19:09:47 2024 ] 	Batch(5300/7879) done. Loss: 0.0093  lr:0.000100
[ Mon Jul  8 19:10:10 2024 ] 	Batch(5400/7879) done. Loss: 0.4585  lr:0.000100
[ Mon Jul  8 19:10:32 2024 ] 
Training: Epoch [63/120], Step [5499], Loss: 0.03048374317586422, Training Accuracy: 93.99772727272727
[ Mon Jul  8 19:10:32 2024 ] 	Batch(5500/7879) done. Loss: 0.3386  lr:0.000100
[ Mon Jul  8 19:10:55 2024 ] 	Batch(5600/7879) done. Loss: 0.1123  lr:0.000100
[ Mon Jul  8 19:11:18 2024 ] 	Batch(5700/7879) done. Loss: 0.4647  lr:0.000100
[ Mon Jul  8 19:11:41 2024 ] 	Batch(5800/7879) done. Loss: 0.5938  lr:0.000100
[ Mon Jul  8 19:12:03 2024 ] 	Batch(5900/7879) done. Loss: 0.2867  lr:0.000100
[ Mon Jul  8 19:12:26 2024 ] 
Training: Epoch [63/120], Step [5999], Loss: 0.44110190868377686, Training Accuracy: 94.03333333333333
[ Mon Jul  8 19:12:26 2024 ] 	Batch(6000/7879) done. Loss: 0.0884  lr:0.000100
[ Mon Jul  8 19:12:49 2024 ] 	Batch(6100/7879) done. Loss: 0.0596  lr:0.000100
[ Mon Jul  8 19:13:11 2024 ] 	Batch(6200/7879) done. Loss: 0.8467  lr:0.000100
[ Mon Jul  8 19:13:34 2024 ] 	Batch(6300/7879) done. Loss: 0.2093  lr:0.000100
[ Mon Jul  8 19:13:57 2024 ] 	Batch(6400/7879) done. Loss: 0.0771  lr:0.000100
[ Mon Jul  8 19:14:19 2024 ] 
Training: Epoch [63/120], Step [6499], Loss: 0.11649999022483826, Training Accuracy: 94.04423076923078
[ Mon Jul  8 19:14:19 2024 ] 	Batch(6500/7879) done. Loss: 0.6604  lr:0.000100
[ Mon Jul  8 19:14:42 2024 ] 	Batch(6600/7879) done. Loss: 0.2599  lr:0.000100
[ Mon Jul  8 19:15:04 2024 ] 	Batch(6700/7879) done. Loss: 0.0015  lr:0.000100
[ Mon Jul  8 19:15:27 2024 ] 	Batch(6800/7879) done. Loss: 0.0849  lr:0.000100
[ Mon Jul  8 19:15:50 2024 ] 	Batch(6900/7879) done. Loss: 0.6551  lr:0.000100
[ Mon Jul  8 19:16:13 2024 ] 
Training: Epoch [63/120], Step [6999], Loss: 0.1318673938512802, Training Accuracy: 94.08214285714286
[ Mon Jul  8 19:16:13 2024 ] 	Batch(7000/7879) done. Loss: 0.0899  lr:0.000100
[ Mon Jul  8 19:16:36 2024 ] 	Batch(7100/7879) done. Loss: 0.6275  lr:0.000100
[ Mon Jul  8 19:16:59 2024 ] 	Batch(7200/7879) done. Loss: 0.8775  lr:0.000100
[ Mon Jul  8 19:17:22 2024 ] 	Batch(7300/7879) done. Loss: 0.8685  lr:0.000100
[ Mon Jul  8 19:17:44 2024 ] 	Batch(7400/7879) done. Loss: 0.1638  lr:0.000100
[ Mon Jul  8 19:18:07 2024 ] 
Training: Epoch [63/120], Step [7499], Loss: 0.023925429210066795, Training Accuracy: 94.08333333333333
[ Mon Jul  8 19:18:07 2024 ] 	Batch(7500/7879) done. Loss: 0.4562  lr:0.000100
[ Mon Jul  8 19:18:29 2024 ] 	Batch(7600/7879) done. Loss: 0.1141  lr:0.000100
[ Mon Jul  8 19:18:52 2024 ] 	Batch(7700/7879) done. Loss: 0.1845  lr:0.000100
[ Mon Jul  8 19:19:15 2024 ] 	Batch(7800/7879) done. Loss: 0.0778  lr:0.000100
[ Mon Jul  8 19:19:32 2024 ] 	Mean training loss: 0.2030.
[ Mon Jul  8 19:19:32 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 19:19:33 2024 ] Training epoch: 65
[ Mon Jul  8 19:19:33 2024 ] 	Batch(0/7879) done. Loss: 0.0891  lr:0.000100
[ Mon Jul  8 19:19:56 2024 ] 	Batch(100/7879) done. Loss: 0.0088  lr:0.000100
[ Mon Jul  8 19:20:18 2024 ] 	Batch(200/7879) done. Loss: 0.2766  lr:0.000100
[ Mon Jul  8 19:20:41 2024 ] 	Batch(300/7879) done. Loss: 0.0971  lr:0.000100
[ Mon Jul  8 19:21:04 2024 ] 	Batch(400/7879) done. Loss: 0.0448  lr:0.000100
[ Mon Jul  8 19:21:26 2024 ] 
Training: Epoch [64/120], Step [499], Loss: 0.38354891538619995, Training Accuracy: 94.89999999999999
[ Mon Jul  8 19:21:26 2024 ] 	Batch(500/7879) done. Loss: 0.0407  lr:0.000100
[ Mon Jul  8 19:21:49 2024 ] 	Batch(600/7879) done. Loss: 0.0151  lr:0.000100
[ Mon Jul  8 19:22:11 2024 ] 	Batch(700/7879) done. Loss: 0.3299  lr:0.000100
[ Mon Jul  8 19:22:34 2024 ] 	Batch(800/7879) done. Loss: 0.6164  lr:0.000100
[ Mon Jul  8 19:22:56 2024 ] 	Batch(900/7879) done. Loss: 0.0594  lr:0.000100
[ Mon Jul  8 19:23:19 2024 ] 
Training: Epoch [64/120], Step [999], Loss: 0.11667193472385406, Training Accuracy: 94.75
[ Mon Jul  8 19:23:19 2024 ] 	Batch(1000/7879) done. Loss: 0.4025  lr:0.000100
[ Mon Jul  8 19:23:41 2024 ] 	Batch(1100/7879) done. Loss: 0.2115  lr:0.000100
[ Mon Jul  8 19:24:04 2024 ] 	Batch(1200/7879) done. Loss: 0.0676  lr:0.000100
[ Mon Jul  8 19:24:27 2024 ] 	Batch(1300/7879) done. Loss: 0.1073  lr:0.000100
[ Mon Jul  8 19:24:51 2024 ] 	Batch(1400/7879) done. Loss: 0.2280  lr:0.000100
[ Mon Jul  8 19:25:14 2024 ] 
Training: Epoch [64/120], Step [1499], Loss: 0.42297178506851196, Training Accuracy: 94.64166666666667
[ Mon Jul  8 19:25:14 2024 ] 	Batch(1500/7879) done. Loss: 0.0590  lr:0.000100
[ Mon Jul  8 19:25:38 2024 ] 	Batch(1600/7879) done. Loss: 0.2013  lr:0.000100
[ Mon Jul  8 19:26:00 2024 ] 	Batch(1700/7879) done. Loss: 0.0213  lr:0.000100
[ Mon Jul  8 19:26:23 2024 ] 	Batch(1800/7879) done. Loss: 0.0585  lr:0.000100
[ Mon Jul  8 19:26:45 2024 ] 	Batch(1900/7879) done. Loss: 0.0827  lr:0.000100
[ Mon Jul  8 19:27:08 2024 ] 
Training: Epoch [64/120], Step [1999], Loss: 0.22120200097560883, Training Accuracy: 94.6375
[ Mon Jul  8 19:27:08 2024 ] 	Batch(2000/7879) done. Loss: 0.1323  lr:0.000100
[ Mon Jul  8 19:27:30 2024 ] 	Batch(2100/7879) done. Loss: 0.0327  lr:0.000100
[ Mon Jul  8 19:27:53 2024 ] 	Batch(2200/7879) done. Loss: 0.7562  lr:0.000100
[ Mon Jul  8 19:28:16 2024 ] 	Batch(2300/7879) done. Loss: 0.0270  lr:0.000100
[ Mon Jul  8 19:28:38 2024 ] 	Batch(2400/7879) done. Loss: 0.3363  lr:0.000100
[ Mon Jul  8 19:29:01 2024 ] 
Training: Epoch [64/120], Step [2499], Loss: 0.026850197464227676, Training Accuracy: 94.49
[ Mon Jul  8 19:29:01 2024 ] 	Batch(2500/7879) done. Loss: 0.0338  lr:0.000100
[ Mon Jul  8 19:29:23 2024 ] 	Batch(2600/7879) done. Loss: 0.3766  lr:0.000100
[ Mon Jul  8 19:29:46 2024 ] 	Batch(2700/7879) done. Loss: 0.5176  lr:0.000100
[ Mon Jul  8 19:30:09 2024 ] 	Batch(2800/7879) done. Loss: 0.2061  lr:0.000100
[ Mon Jul  8 19:30:31 2024 ] 	Batch(2900/7879) done. Loss: 0.0176  lr:0.000100
[ Mon Jul  8 19:30:54 2024 ] 
Training: Epoch [64/120], Step [2999], Loss: 0.14157485961914062, Training Accuracy: 94.49583333333334
[ Mon Jul  8 19:30:54 2024 ] 	Batch(3000/7879) done. Loss: 0.0555  lr:0.000100
[ Mon Jul  8 19:31:16 2024 ] 	Batch(3100/7879) done. Loss: 0.2614  lr:0.000100
[ Mon Jul  8 19:31:39 2024 ] 	Batch(3200/7879) done. Loss: 0.1449  lr:0.000100
[ Mon Jul  8 19:32:02 2024 ] 	Batch(3300/7879) done. Loss: 0.0484  lr:0.000100
[ Mon Jul  8 19:32:24 2024 ] 	Batch(3400/7879) done. Loss: 0.1944  lr:0.000100
[ Mon Jul  8 19:32:46 2024 ] 
Training: Epoch [64/120], Step [3499], Loss: 0.03476425260305405, Training Accuracy: 94.48571428571428
[ Mon Jul  8 19:32:47 2024 ] 	Batch(3500/7879) done. Loss: 0.1665  lr:0.000100
[ Mon Jul  8 19:33:09 2024 ] 	Batch(3600/7879) done. Loss: 0.4027  lr:0.000100
[ Mon Jul  8 19:33:32 2024 ] 	Batch(3700/7879) done. Loss: 0.0279  lr:0.000100
[ Mon Jul  8 19:33:54 2024 ] 	Batch(3800/7879) done. Loss: 0.0308  lr:0.000100
[ Mon Jul  8 19:34:17 2024 ] 	Batch(3900/7879) done. Loss: 0.1018  lr:0.000100
[ Mon Jul  8 19:34:39 2024 ] 
Training: Epoch [64/120], Step [3999], Loss: 0.18524309992790222, Training Accuracy: 94.484375
[ Mon Jul  8 19:34:40 2024 ] 	Batch(4000/7879) done. Loss: 0.0484  lr:0.000100
[ Mon Jul  8 19:35:02 2024 ] 	Batch(4100/7879) done. Loss: 0.1455  lr:0.000100
[ Mon Jul  8 19:35:25 2024 ] 	Batch(4200/7879) done. Loss: 0.1605  lr:0.000100
[ Mon Jul  8 19:35:47 2024 ] 	Batch(4300/7879) done. Loss: 0.2980  lr:0.000100
[ Mon Jul  8 19:36:10 2024 ] 	Batch(4400/7879) done. Loss: 0.1368  lr:0.000100
[ Mon Jul  8 19:36:32 2024 ] 
Training: Epoch [64/120], Step [4499], Loss: 0.10281239449977875, Training Accuracy: 94.475
[ Mon Jul  8 19:36:32 2024 ] 	Batch(4500/7879) done. Loss: 0.0446  lr:0.000100
[ Mon Jul  8 19:36:55 2024 ] 	Batch(4600/7879) done. Loss: 0.1959  lr:0.000100
[ Mon Jul  8 19:37:17 2024 ] 	Batch(4700/7879) done. Loss: 0.3057  lr:0.000100
[ Mon Jul  8 19:37:40 2024 ] 	Batch(4800/7879) done. Loss: 0.3847  lr:0.000100
[ Mon Jul  8 19:38:03 2024 ] 	Batch(4900/7879) done. Loss: 0.1148  lr:0.000100
[ Mon Jul  8 19:38:25 2024 ] 
Training: Epoch [64/120], Step [4999], Loss: 0.6240959763526917, Training Accuracy: 94.41000000000001
[ Mon Jul  8 19:38:25 2024 ] 	Batch(5000/7879) done. Loss: 0.1084  lr:0.000100
[ Mon Jul  8 19:38:48 2024 ] 	Batch(5100/7879) done. Loss: 0.0097  lr:0.000100
[ Mon Jul  8 19:39:10 2024 ] 	Batch(5200/7879) done. Loss: 0.1448  lr:0.000100
[ Mon Jul  8 19:39:33 2024 ] 	Batch(5300/7879) done. Loss: 0.2485  lr:0.000100
[ Mon Jul  8 19:39:55 2024 ] 	Batch(5400/7879) done. Loss: 0.3425  lr:0.000100
[ Mon Jul  8 19:40:18 2024 ] 
Training: Epoch [64/120], Step [5499], Loss: 0.1830340027809143, Training Accuracy: 94.36363636363636
[ Mon Jul  8 19:40:18 2024 ] 	Batch(5500/7879) done. Loss: 0.1973  lr:0.000100
[ Mon Jul  8 19:40:41 2024 ] 	Batch(5600/7879) done. Loss: 0.0214  lr:0.000100
[ Mon Jul  8 19:41:04 2024 ] 	Batch(5700/7879) done. Loss: 0.0311  lr:0.000100
[ Mon Jul  8 19:41:26 2024 ] 	Batch(5800/7879) done. Loss: 0.0463  lr:0.000100
[ Mon Jul  8 19:41:49 2024 ] 	Batch(5900/7879) done. Loss: 0.0108  lr:0.000100
[ Mon Jul  8 19:42:11 2024 ] 
Training: Epoch [64/120], Step [5999], Loss: 0.049984101206064224, Training Accuracy: 94.32916666666667
[ Mon Jul  8 19:42:12 2024 ] 	Batch(6000/7879) done. Loss: 0.0911  lr:0.000100
[ Mon Jul  8 19:42:34 2024 ] 	Batch(6100/7879) done. Loss: 0.0361  lr:0.000100
[ Mon Jul  8 19:42:57 2024 ] 	Batch(6200/7879) done. Loss: 0.0539  lr:0.000100
[ Mon Jul  8 19:43:19 2024 ] 	Batch(6300/7879) done. Loss: 0.2129  lr:0.000100
[ Mon Jul  8 19:43:42 2024 ] 	Batch(6400/7879) done. Loss: 0.0851  lr:0.000100
[ Mon Jul  8 19:44:04 2024 ] 
Training: Epoch [64/120], Step [6499], Loss: 0.2816343903541565, Training Accuracy: 94.29807692307692
[ Mon Jul  8 19:44:05 2024 ] 	Batch(6500/7879) done. Loss: 0.1847  lr:0.000100
[ Mon Jul  8 19:44:27 2024 ] 	Batch(6600/7879) done. Loss: 0.0354  lr:0.000100
[ Mon Jul  8 19:44:50 2024 ] 	Batch(6700/7879) done. Loss: 0.0050  lr:0.000100
[ Mon Jul  8 19:45:12 2024 ] 	Batch(6800/7879) done. Loss: 0.0797  lr:0.000100
[ Mon Jul  8 19:45:35 2024 ] 	Batch(6900/7879) done. Loss: 0.0116  lr:0.000100
[ Mon Jul  8 19:45:57 2024 ] 
Training: Epoch [64/120], Step [6999], Loss: 0.15148532390594482, Training Accuracy: 94.28571428571428
[ Mon Jul  8 19:45:58 2024 ] 	Batch(7000/7879) done. Loss: 0.4178  lr:0.000100
[ Mon Jul  8 19:46:20 2024 ] 	Batch(7100/7879) done. Loss: 0.0656  lr:0.000100
[ Mon Jul  8 19:46:43 2024 ] 	Batch(7200/7879) done. Loss: 0.2869  lr:0.000100
[ Mon Jul  8 19:47:05 2024 ] 	Batch(7300/7879) done. Loss: 0.0156  lr:0.000100
[ Mon Jul  8 19:47:28 2024 ] 	Batch(7400/7879) done. Loss: 0.1488  lr:0.000100
[ Mon Jul  8 19:47:50 2024 ] 
Training: Epoch [64/120], Step [7499], Loss: 0.3238341808319092, Training Accuracy: 94.255
[ Mon Jul  8 19:47:50 2024 ] 	Batch(7500/7879) done. Loss: 0.1546  lr:0.000100
[ Mon Jul  8 19:48:13 2024 ] 	Batch(7600/7879) done. Loss: 0.2285  lr:0.000100
[ Mon Jul  8 19:48:37 2024 ] 	Batch(7700/7879) done. Loss: 0.0372  lr:0.000100
[ Mon Jul  8 19:49:00 2024 ] 	Batch(7800/7879) done. Loss: 0.0973  lr:0.000100
[ Mon Jul  8 19:49:17 2024 ] 	Mean training loss: 0.2044.
[ Mon Jul  8 19:49:17 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 19:49:17 2024 ] Training epoch: 66
[ Mon Jul  8 19:49:18 2024 ] 	Batch(0/7879) done. Loss: 0.0909  lr:0.000100
[ Mon Jul  8 19:49:41 2024 ] 	Batch(100/7879) done. Loss: 0.1839  lr:0.000100
[ Mon Jul  8 19:50:03 2024 ] 	Batch(200/7879) done. Loss: 0.0491  lr:0.000100
[ Mon Jul  8 19:50:26 2024 ] 	Batch(300/7879) done. Loss: 0.8579  lr:0.000100
[ Mon Jul  8 19:50:49 2024 ] 	Batch(400/7879) done. Loss: 0.3164  lr:0.000100
[ Mon Jul  8 19:51:11 2024 ] 
Training: Epoch [65/120], Step [499], Loss: 0.3027278780937195, Training Accuracy: 94.025
[ Mon Jul  8 19:51:11 2024 ] 	Batch(500/7879) done. Loss: 0.0503  lr:0.000100
[ Mon Jul  8 19:51:34 2024 ] 	Batch(600/7879) done. Loss: 0.2839  lr:0.000100
[ Mon Jul  8 19:51:56 2024 ] 	Batch(700/7879) done. Loss: 0.2082  lr:0.000100
[ Mon Jul  8 19:52:19 2024 ] 	Batch(800/7879) done. Loss: 0.0901  lr:0.000100
[ Mon Jul  8 19:52:42 2024 ] 	Batch(900/7879) done. Loss: 0.1399  lr:0.000100
[ Mon Jul  8 19:53:04 2024 ] 
Training: Epoch [65/120], Step [999], Loss: 0.264653742313385, Training Accuracy: 94.36250000000001
[ Mon Jul  8 19:53:05 2024 ] 	Batch(1000/7879) done. Loss: 0.0718  lr:0.000100
[ Mon Jul  8 19:53:27 2024 ] 	Batch(1100/7879) done. Loss: 0.1881  lr:0.000100
[ Mon Jul  8 19:53:50 2024 ] 	Batch(1200/7879) done. Loss: 0.5743  lr:0.000100
[ Mon Jul  8 19:54:13 2024 ] 	Batch(1300/7879) done. Loss: 0.0066  lr:0.000100
[ Mon Jul  8 19:54:36 2024 ] 	Batch(1400/7879) done. Loss: 0.4468  lr:0.000100
[ Mon Jul  8 19:54:58 2024 ] 
Training: Epoch [65/120], Step [1499], Loss: 0.05127464234828949, Training Accuracy: 94.35833333333333
[ Mon Jul  8 19:54:58 2024 ] 	Batch(1500/7879) done. Loss: 0.0184  lr:0.000100
[ Mon Jul  8 19:55:21 2024 ] 	Batch(1600/7879) done. Loss: 0.0427  lr:0.000100
[ Mon Jul  8 19:55:44 2024 ] 	Batch(1700/7879) done. Loss: 0.0100  lr:0.000100
[ Mon Jul  8 19:56:07 2024 ] 	Batch(1800/7879) done. Loss: 0.2159  lr:0.000100
[ Mon Jul  8 19:56:29 2024 ] 	Batch(1900/7879) done. Loss: 0.2188  lr:0.000100
[ Mon Jul  8 19:56:52 2024 ] 
Training: Epoch [65/120], Step [1999], Loss: 0.0729159489274025, Training Accuracy: 94.5
[ Mon Jul  8 19:56:52 2024 ] 	Batch(2000/7879) done. Loss: 0.1222  lr:0.000100
[ Mon Jul  8 19:57:15 2024 ] 	Batch(2100/7879) done. Loss: 0.1316  lr:0.000100
[ Mon Jul  8 19:57:37 2024 ] 	Batch(2200/7879) done. Loss: 0.1113  lr:0.000100
[ Mon Jul  8 19:58:00 2024 ] 	Batch(2300/7879) done. Loss: 0.3703  lr:0.000100
[ Mon Jul  8 19:58:23 2024 ] 	Batch(2400/7879) done. Loss: 0.0342  lr:0.000100
[ Mon Jul  8 19:58:45 2024 ] 
Training: Epoch [65/120], Step [2499], Loss: 0.02826462872326374, Training Accuracy: 94.475
[ Mon Jul  8 19:58:46 2024 ] 	Batch(2500/7879) done. Loss: 0.0271  lr:0.000100
[ Mon Jul  8 19:59:08 2024 ] 	Batch(2600/7879) done. Loss: 0.1281  lr:0.000100
[ Mon Jul  8 19:59:31 2024 ] 	Batch(2700/7879) done. Loss: 0.1439  lr:0.000100
[ Mon Jul  8 19:59:53 2024 ] 	Batch(2800/7879) done. Loss: 0.2631  lr:0.000100
[ Mon Jul  8 20:00:16 2024 ] 	Batch(2900/7879) done. Loss: 0.0632  lr:0.000100
[ Mon Jul  8 20:00:38 2024 ] 
Training: Epoch [65/120], Step [2999], Loss: 0.04428568109869957, Training Accuracy: 94.52916666666667
[ Mon Jul  8 20:00:38 2024 ] 	Batch(3000/7879) done. Loss: 0.0125  lr:0.000100
[ Mon Jul  8 20:01:01 2024 ] 	Batch(3100/7879) done. Loss: 1.0187  lr:0.000100
[ Mon Jul  8 20:01:24 2024 ] 	Batch(3200/7879) done. Loss: 0.0091  lr:0.000100
[ Mon Jul  8 20:01:46 2024 ] 	Batch(3300/7879) done. Loss: 0.0517  lr:0.000100
[ Mon Jul  8 20:02:09 2024 ] 	Batch(3400/7879) done. Loss: 0.1689  lr:0.000100
[ Mon Jul  8 20:02:31 2024 ] 
Training: Epoch [65/120], Step [3499], Loss: 0.08506027609109879, Training Accuracy: 94.53214285714286
[ Mon Jul  8 20:02:31 2024 ] 	Batch(3500/7879) done. Loss: 0.5292  lr:0.000100
[ Mon Jul  8 20:02:54 2024 ] 	Batch(3600/7879) done. Loss: 0.0644  lr:0.000100
[ Mon Jul  8 20:03:17 2024 ] 	Batch(3700/7879) done. Loss: 0.5932  lr:0.000100
[ Mon Jul  8 20:03:39 2024 ] 	Batch(3800/7879) done. Loss: 0.0218  lr:0.000100
[ Mon Jul  8 20:04:02 2024 ] 	Batch(3900/7879) done. Loss: 0.7226  lr:0.000100
[ Mon Jul  8 20:04:24 2024 ] 
Training: Epoch [65/120], Step [3999], Loss: 0.16743329167366028, Training Accuracy: 94.528125
[ Mon Jul  8 20:04:24 2024 ] 	Batch(4000/7879) done. Loss: 0.0865  lr:0.000100
[ Mon Jul  8 20:04:47 2024 ] 	Batch(4100/7879) done. Loss: 0.0385  lr:0.000100
[ Mon Jul  8 20:05:09 2024 ] 	Batch(4200/7879) done. Loss: 0.2197  lr:0.000100
[ Mon Jul  8 20:05:32 2024 ] 	Batch(4300/7879) done. Loss: 0.2453  lr:0.000100
[ Mon Jul  8 20:05:55 2024 ] 	Batch(4400/7879) done. Loss: 0.2837  lr:0.000100
[ Mon Jul  8 20:06:17 2024 ] 
Training: Epoch [65/120], Step [4499], Loss: 0.17802900075912476, Training Accuracy: 94.58888888888889
[ Mon Jul  8 20:06:17 2024 ] 	Batch(4500/7879) done. Loss: 0.4607  lr:0.000100
[ Mon Jul  8 20:06:40 2024 ] 	Batch(4600/7879) done. Loss: 0.0140  lr:0.000100
[ Mon Jul  8 20:07:02 2024 ] 	Batch(4700/7879) done. Loss: 0.0552  lr:0.000100
[ Mon Jul  8 20:07:25 2024 ] 	Batch(4800/7879) done. Loss: 0.5394  lr:0.000100
[ Mon Jul  8 20:07:48 2024 ] 	Batch(4900/7879) done. Loss: 0.0279  lr:0.000100
[ Mon Jul  8 20:08:11 2024 ] 
Training: Epoch [65/120], Step [4999], Loss: 0.47377076745033264, Training Accuracy: 94.6025
[ Mon Jul  8 20:08:12 2024 ] 	Batch(5000/7879) done. Loss: 0.3855  lr:0.000100
[ Mon Jul  8 20:08:35 2024 ] 	Batch(5100/7879) done. Loss: 0.0045  lr:0.000100
[ Mon Jul  8 20:08:58 2024 ] 	Batch(5200/7879) done. Loss: 0.0700  lr:0.000100
[ Mon Jul  8 20:09:22 2024 ] 	Batch(5300/7879) done. Loss: 0.6788  lr:0.000100
[ Mon Jul  8 20:09:45 2024 ] 	Batch(5400/7879) done. Loss: 0.1894  lr:0.000100
[ Mon Jul  8 20:10:08 2024 ] 
Training: Epoch [65/120], Step [5499], Loss: 0.5219017267227173, Training Accuracy: 94.62045454545455
[ Mon Jul  8 20:10:08 2024 ] 	Batch(5500/7879) done. Loss: 0.1242  lr:0.000100
[ Mon Jul  8 20:10:32 2024 ] 	Batch(5600/7879) done. Loss: 0.3761  lr:0.000100
[ Mon Jul  8 20:10:55 2024 ] 	Batch(5700/7879) done. Loss: 0.2436  lr:0.000100
[ Mon Jul  8 20:11:18 2024 ] 	Batch(5800/7879) done. Loss: 0.7073  lr:0.000100
[ Mon Jul  8 20:11:41 2024 ] 	Batch(5900/7879) done. Loss: 0.0240  lr:0.000100
[ Mon Jul  8 20:12:05 2024 ] 
Training: Epoch [65/120], Step [5999], Loss: 0.7226589918136597, Training Accuracy: 94.61874999999999
[ Mon Jul  8 20:12:05 2024 ] 	Batch(6000/7879) done. Loss: 0.0558  lr:0.000100
[ Mon Jul  8 20:12:27 2024 ] 	Batch(6100/7879) done. Loss: 0.0435  lr:0.000100
[ Mon Jul  8 20:12:50 2024 ] 	Batch(6200/7879) done. Loss: 0.0761  lr:0.000100
[ Mon Jul  8 20:13:13 2024 ] 	Batch(6300/7879) done. Loss: 0.5444  lr:0.000100
[ Mon Jul  8 20:13:35 2024 ] 	Batch(6400/7879) done. Loss: 0.5151  lr:0.000100
[ Mon Jul  8 20:13:58 2024 ] 
Training: Epoch [65/120], Step [6499], Loss: 0.004346031229943037, Training Accuracy: 94.59038461538462
[ Mon Jul  8 20:13:58 2024 ] 	Batch(6500/7879) done. Loss: 0.1874  lr:0.000100
[ Mon Jul  8 20:14:20 2024 ] 	Batch(6600/7879) done. Loss: 0.1461  lr:0.000100
[ Mon Jul  8 20:14:43 2024 ] 	Batch(6700/7879) done. Loss: 0.5439  lr:0.000100
[ Mon Jul  8 20:15:06 2024 ] 	Batch(6800/7879) done. Loss: 0.0668  lr:0.000100
[ Mon Jul  8 20:15:28 2024 ] 	Batch(6900/7879) done. Loss: 0.0212  lr:0.000100
[ Mon Jul  8 20:15:51 2024 ] 
Training: Epoch [65/120], Step [6999], Loss: 0.01329847238957882, Training Accuracy: 94.57142857142857
[ Mon Jul  8 20:15:51 2024 ] 	Batch(7000/7879) done. Loss: 0.3197  lr:0.000100
[ Mon Jul  8 20:16:13 2024 ] 	Batch(7100/7879) done. Loss: 0.2266  lr:0.000100
[ Mon Jul  8 20:16:36 2024 ] 	Batch(7200/7879) done. Loss: 0.2438  lr:0.000100
[ Mon Jul  8 20:16:58 2024 ] 	Batch(7300/7879) done. Loss: 0.1718  lr:0.000100
[ Mon Jul  8 20:17:21 2024 ] 	Batch(7400/7879) done. Loss: 0.6768  lr:0.000100
[ Mon Jul  8 20:17:43 2024 ] 
Training: Epoch [65/120], Step [7499], Loss: 0.1006414145231247, Training Accuracy: 94.53
[ Mon Jul  8 20:17:44 2024 ] 	Batch(7500/7879) done. Loss: 0.0380  lr:0.000100
[ Mon Jul  8 20:18:06 2024 ] 	Batch(7600/7879) done. Loss: 0.1073  lr:0.000100
[ Mon Jul  8 20:18:29 2024 ] 	Batch(7700/7879) done. Loss: 0.1440  lr:0.000100
[ Mon Jul  8 20:18:51 2024 ] 	Batch(7800/7879) done. Loss: 0.7048  lr:0.000100
[ Mon Jul  8 20:19:09 2024 ] 	Mean training loss: 0.1949.
[ Mon Jul  8 20:19:09 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 20:19:09 2024 ] Training epoch: 67
[ Mon Jul  8 20:19:10 2024 ] 	Batch(0/7879) done. Loss: 0.5472  lr:0.000100
[ Mon Jul  8 20:19:32 2024 ] 	Batch(100/7879) done. Loss: 0.0890  lr:0.000100
[ Mon Jul  8 20:19:55 2024 ] 	Batch(200/7879) done. Loss: 0.1231  lr:0.000100
[ Mon Jul  8 20:20:17 2024 ] 	Batch(300/7879) done. Loss: 0.1298  lr:0.000100
[ Mon Jul  8 20:20:40 2024 ] 	Batch(400/7879) done. Loss: 0.0985  lr:0.000100
[ Mon Jul  8 20:21:02 2024 ] 
Training: Epoch [66/120], Step [499], Loss: 0.5531938076019287, Training Accuracy: 95.075
[ Mon Jul  8 20:21:03 2024 ] 	Batch(500/7879) done. Loss: 0.0615  lr:0.000100
[ Mon Jul  8 20:21:25 2024 ] 	Batch(600/7879) done. Loss: 0.4951  lr:0.000100
[ Mon Jul  8 20:21:48 2024 ] 	Batch(700/7879) done. Loss: 0.0113  lr:0.000100
[ Mon Jul  8 20:22:10 2024 ] 	Batch(800/7879) done. Loss: 0.2327  lr:0.000100
[ Mon Jul  8 20:22:33 2024 ] 	Batch(900/7879) done. Loss: 0.0775  lr:0.000100
[ Mon Jul  8 20:22:55 2024 ] 
Training: Epoch [66/120], Step [999], Loss: 0.5614716410636902, Training Accuracy: 94.9125
[ Mon Jul  8 20:22:56 2024 ] 	Batch(1000/7879) done. Loss: 0.3286  lr:0.000100
[ Mon Jul  8 20:23:18 2024 ] 	Batch(1100/7879) done. Loss: 0.2733  lr:0.000100
[ Mon Jul  8 20:23:41 2024 ] 	Batch(1200/7879) done. Loss: 0.2678  lr:0.000100
[ Mon Jul  8 20:24:04 2024 ] 	Batch(1300/7879) done. Loss: 0.3594  lr:0.000100
[ Mon Jul  8 20:24:26 2024 ] 	Batch(1400/7879) done. Loss: 0.4074  lr:0.000100
[ Mon Jul  8 20:24:49 2024 ] 
Training: Epoch [66/120], Step [1499], Loss: 0.7595934867858887, Training Accuracy: 94.45833333333333
[ Mon Jul  8 20:24:49 2024 ] 	Batch(1500/7879) done. Loss: 0.1655  lr:0.000100
[ Mon Jul  8 20:25:12 2024 ] 	Batch(1600/7879) done. Loss: 0.2086  lr:0.000100
[ Mon Jul  8 20:25:35 2024 ] 	Batch(1700/7879) done. Loss: 0.3715  lr:0.000100
[ Mon Jul  8 20:25:58 2024 ] 	Batch(1800/7879) done. Loss: 0.0428  lr:0.000100
[ Mon Jul  8 20:26:22 2024 ] 	Batch(1900/7879) done. Loss: 0.1817  lr:0.000100
[ Mon Jul  8 20:26:45 2024 ] 
Training: Epoch [66/120], Step [1999], Loss: 0.03596579656004906, Training Accuracy: 94.46875
[ Mon Jul  8 20:26:45 2024 ] 	Batch(2000/7879) done. Loss: 0.0933  lr:0.000100
[ Mon Jul  8 20:27:08 2024 ] 	Batch(2100/7879) done. Loss: 0.0739  lr:0.000100
[ Mon Jul  8 20:27:30 2024 ] 	Batch(2200/7879) done. Loss: 0.0157  lr:0.000100
[ Mon Jul  8 20:27:53 2024 ] 	Batch(2300/7879) done. Loss: 0.5298  lr:0.000100
[ Mon Jul  8 20:28:16 2024 ] 	Batch(2400/7879) done. Loss: 0.0597  lr:0.000100
[ Mon Jul  8 20:28:38 2024 ] 
Training: Epoch [66/120], Step [2499], Loss: 0.10090973973274231, Training Accuracy: 94.545
[ Mon Jul  8 20:28:38 2024 ] 	Batch(2500/7879) done. Loss: 0.0895  lr:0.000100
[ Mon Jul  8 20:29:01 2024 ] 	Batch(2600/7879) done. Loss: 0.4911  lr:0.000100
[ Mon Jul  8 20:29:23 2024 ] 	Batch(2700/7879) done. Loss: 0.4545  lr:0.000100
[ Mon Jul  8 20:29:46 2024 ] 	Batch(2800/7879) done. Loss: 0.2865  lr:0.000100
[ Mon Jul  8 20:30:09 2024 ] 	Batch(2900/7879) done. Loss: 0.0744  lr:0.000100
[ Mon Jul  8 20:30:31 2024 ] 
Training: Epoch [66/120], Step [2999], Loss: 0.3100089728832245, Training Accuracy: 94.55416666666666
[ Mon Jul  8 20:30:31 2024 ] 	Batch(3000/7879) done. Loss: 0.2523  lr:0.000100
[ Mon Jul  8 20:30:54 2024 ] 	Batch(3100/7879) done. Loss: 0.1053  lr:0.000100
[ Mon Jul  8 20:31:16 2024 ] 	Batch(3200/7879) done. Loss: 0.1877  lr:0.000100
[ Mon Jul  8 20:31:39 2024 ] 	Batch(3300/7879) done. Loss: 0.0311  lr:0.000100
[ Mon Jul  8 20:32:02 2024 ] 	Batch(3400/7879) done. Loss: 0.0538  lr:0.000100
[ Mon Jul  8 20:32:24 2024 ] 
Training: Epoch [66/120], Step [3499], Loss: 0.6639710664749146, Training Accuracy: 94.63571428571429
[ Mon Jul  8 20:32:24 2024 ] 	Batch(3500/7879) done. Loss: 0.0172  lr:0.000100
[ Mon Jul  8 20:32:47 2024 ] 	Batch(3600/7879) done. Loss: 0.0452  lr:0.000100
[ Mon Jul  8 20:33:10 2024 ] 	Batch(3700/7879) done. Loss: 0.1531  lr:0.000100
[ Mon Jul  8 20:33:34 2024 ] 	Batch(3800/7879) done. Loss: 0.0242  lr:0.000100
[ Mon Jul  8 20:33:57 2024 ] 	Batch(3900/7879) done. Loss: 0.2406  lr:0.000100
[ Mon Jul  8 20:34:20 2024 ] 
Training: Epoch [66/120], Step [3999], Loss: 0.06455796957015991, Training Accuracy: 94.671875
[ Mon Jul  8 20:34:20 2024 ] 	Batch(4000/7879) done. Loss: 0.0484  lr:0.000100
[ Mon Jul  8 20:34:44 2024 ] 	Batch(4100/7879) done. Loss: 0.0057  lr:0.000100
[ Mon Jul  8 20:35:07 2024 ] 	Batch(4200/7879) done. Loss: 0.0889  lr:0.000100
[ Mon Jul  8 20:35:30 2024 ] 	Batch(4300/7879) done. Loss: 0.0623  lr:0.000100
[ Mon Jul  8 20:35:54 2024 ] 	Batch(4400/7879) done. Loss: 0.0306  lr:0.000100
[ Mon Jul  8 20:36:16 2024 ] 
Training: Epoch [66/120], Step [4499], Loss: 0.5363134145736694, Training Accuracy: 94.66944444444444
[ Mon Jul  8 20:36:16 2024 ] 	Batch(4500/7879) done. Loss: 0.1388  lr:0.000100
[ Mon Jul  8 20:36:39 2024 ] 	Batch(4600/7879) done. Loss: 0.0357  lr:0.000100
[ Mon Jul  8 20:37:01 2024 ] 	Batch(4700/7879) done. Loss: 0.1853  lr:0.000100
[ Mon Jul  8 20:37:24 2024 ] 	Batch(4800/7879) done. Loss: 0.0040  lr:0.000100
[ Mon Jul  8 20:37:48 2024 ] 	Batch(4900/7879) done. Loss: 0.1015  lr:0.000100
[ Mon Jul  8 20:38:11 2024 ] 
Training: Epoch [66/120], Step [4999], Loss: 0.09769472479820251, Training Accuracy: 94.5925
[ Mon Jul  8 20:38:11 2024 ] 	Batch(5000/7879) done. Loss: 0.0066  lr:0.000100
[ Mon Jul  8 20:38:34 2024 ] 	Batch(5100/7879) done. Loss: 0.5081  lr:0.000100
[ Mon Jul  8 20:38:58 2024 ] 	Batch(5200/7879) done. Loss: 0.0232  lr:0.000100
[ Mon Jul  8 20:39:20 2024 ] 	Batch(5300/7879) done. Loss: 0.3087  lr:0.000100
[ Mon Jul  8 20:39:43 2024 ] 	Batch(5400/7879) done. Loss: 0.0261  lr:0.000100
[ Mon Jul  8 20:40:05 2024 ] 
Training: Epoch [66/120], Step [5499], Loss: 0.07915729284286499, Training Accuracy: 94.6590909090909
[ Mon Jul  8 20:40:05 2024 ] 	Batch(5500/7879) done. Loss: 0.0881  lr:0.000100
[ Mon Jul  8 20:40:28 2024 ] 	Batch(5600/7879) done. Loss: 0.0957  lr:0.000100
[ Mon Jul  8 20:40:51 2024 ] 	Batch(5700/7879) done. Loss: 0.1346  lr:0.000100
[ Mon Jul  8 20:41:13 2024 ] 	Batch(5800/7879) done. Loss: 0.0097  lr:0.000100
[ Mon Jul  8 20:41:36 2024 ] 	Batch(5900/7879) done. Loss: 0.1151  lr:0.000100
[ Mon Jul  8 20:41:58 2024 ] 
Training: Epoch [66/120], Step [5999], Loss: 0.0929223820567131, Training Accuracy: 94.64166666666667
[ Mon Jul  8 20:41:59 2024 ] 	Batch(6000/7879) done. Loss: 0.6911  lr:0.000100
[ Mon Jul  8 20:42:21 2024 ] 	Batch(6100/7879) done. Loss: 0.2639  lr:0.000100
[ Mon Jul  8 20:42:44 2024 ] 	Batch(6200/7879) done. Loss: 0.0127  lr:0.000100
[ Mon Jul  8 20:43:06 2024 ] 	Batch(6300/7879) done. Loss: 0.4013  lr:0.000100
[ Mon Jul  8 20:43:29 2024 ] 	Batch(6400/7879) done. Loss: 0.0228  lr:0.000100
[ Mon Jul  8 20:43:52 2024 ] 
Training: Epoch [66/120], Step [6499], Loss: 0.24927011132240295, Training Accuracy: 94.61153846153846
[ Mon Jul  8 20:43:52 2024 ] 	Batch(6500/7879) done. Loss: 0.0566  lr:0.000100
[ Mon Jul  8 20:44:16 2024 ] 	Batch(6600/7879) done. Loss: 0.1509  lr:0.000100
[ Mon Jul  8 20:44:39 2024 ] 	Batch(6700/7879) done. Loss: 0.2808  lr:0.000100
[ Mon Jul  8 20:45:02 2024 ] 	Batch(6800/7879) done. Loss: 0.5638  lr:0.000100
[ Mon Jul  8 20:45:26 2024 ] 	Batch(6900/7879) done. Loss: 0.0646  lr:0.000100
[ Mon Jul  8 20:45:49 2024 ] 
Training: Epoch [66/120], Step [6999], Loss: 0.03394730016589165, Training Accuracy: 94.60357142857143
[ Mon Jul  8 20:45:49 2024 ] 	Batch(7000/7879) done. Loss: 0.0548  lr:0.000100
[ Mon Jul  8 20:46:12 2024 ] 	Batch(7100/7879) done. Loss: 0.0256  lr:0.000100
[ Mon Jul  8 20:46:36 2024 ] 	Batch(7200/7879) done. Loss: 0.1047  lr:0.000100
[ Mon Jul  8 20:46:59 2024 ] 	Batch(7300/7879) done. Loss: 0.0311  lr:0.000100
[ Mon Jul  8 20:47:22 2024 ] 	Batch(7400/7879) done. Loss: 0.0185  lr:0.000100
[ Mon Jul  8 20:47:46 2024 ] 
Training: Epoch [66/120], Step [7499], Loss: 0.04351574555039406, Training Accuracy: 94.61
[ Mon Jul  8 20:47:46 2024 ] 	Batch(7500/7879) done. Loss: 0.1877  lr:0.000100
[ Mon Jul  8 20:48:09 2024 ] 	Batch(7600/7879) done. Loss: 0.1188  lr:0.000100
[ Mon Jul  8 20:48:33 2024 ] 	Batch(7700/7879) done. Loss: 0.0283  lr:0.000100
[ Mon Jul  8 20:48:56 2024 ] 	Batch(7800/7879) done. Loss: 0.0969  lr:0.000100
[ Mon Jul  8 20:49:14 2024 ] 	Mean training loss: 0.1933.
[ Mon Jul  8 20:49:14 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 20:49:14 2024 ] Training epoch: 68
[ Mon Jul  8 20:49:15 2024 ] 	Batch(0/7879) done. Loss: 0.4904  lr:0.000100
[ Mon Jul  8 20:49:38 2024 ] 	Batch(100/7879) done. Loss: 0.3000  lr:0.000100
[ Mon Jul  8 20:50:00 2024 ] 	Batch(200/7879) done. Loss: 0.7584  lr:0.000100
[ Mon Jul  8 20:50:23 2024 ] 	Batch(300/7879) done. Loss: 0.0797  lr:0.000100
[ Mon Jul  8 20:50:46 2024 ] 	Batch(400/7879) done. Loss: 0.8937  lr:0.000100
[ Mon Jul  8 20:51:08 2024 ] 
Training: Epoch [67/120], Step [499], Loss: 0.5026432871818542, Training Accuracy: 95.1
[ Mon Jul  8 20:51:08 2024 ] 	Batch(500/7879) done. Loss: 0.4427  lr:0.000100
[ Mon Jul  8 20:51:31 2024 ] 	Batch(600/7879) done. Loss: 0.0898  lr:0.000100
[ Mon Jul  8 20:51:53 2024 ] 	Batch(700/7879) done. Loss: 0.2039  lr:0.000100
[ Mon Jul  8 20:52:16 2024 ] 	Batch(800/7879) done. Loss: 0.3223  lr:0.000100
[ Mon Jul  8 20:52:39 2024 ] 	Batch(900/7879) done. Loss: 0.3217  lr:0.000100
[ Mon Jul  8 20:53:01 2024 ] 
Training: Epoch [67/120], Step [999], Loss: 0.06785639375448227, Training Accuracy: 94.925
[ Mon Jul  8 20:53:01 2024 ] 	Batch(1000/7879) done. Loss: 0.6069  lr:0.000100
[ Mon Jul  8 20:53:24 2024 ] 	Batch(1100/7879) done. Loss: 0.0678  lr:0.000100
[ Mon Jul  8 20:53:46 2024 ] 	Batch(1200/7879) done. Loss: 0.1425  lr:0.000100
[ Mon Jul  8 20:54:09 2024 ] 	Batch(1300/7879) done. Loss: 0.1010  lr:0.000100
[ Mon Jul  8 20:54:32 2024 ] 	Batch(1400/7879) done. Loss: 0.0083  lr:0.000100
[ Mon Jul  8 20:54:54 2024 ] 
Training: Epoch [67/120], Step [1499], Loss: 0.3981395661830902, Training Accuracy: 95.075
[ Mon Jul  8 20:54:54 2024 ] 	Batch(1500/7879) done. Loss: 0.1234  lr:0.000100
[ Mon Jul  8 20:55:17 2024 ] 	Batch(1600/7879) done. Loss: 0.3168  lr:0.000100
[ Mon Jul  8 20:55:39 2024 ] 	Batch(1700/7879) done. Loss: 0.0895  lr:0.000100
[ Mon Jul  8 20:56:02 2024 ] 	Batch(1800/7879) done. Loss: 0.3833  lr:0.000100
[ Mon Jul  8 20:56:25 2024 ] 	Batch(1900/7879) done. Loss: 0.0111  lr:0.000100
[ Mon Jul  8 20:56:47 2024 ] 
Training: Epoch [67/120], Step [1999], Loss: 0.014674566686153412, Training Accuracy: 94.99375
[ Mon Jul  8 20:56:48 2024 ] 	Batch(2000/7879) done. Loss: 0.0118  lr:0.000100
[ Mon Jul  8 20:57:10 2024 ] 	Batch(2100/7879) done. Loss: 0.0275  lr:0.000100
[ Mon Jul  8 20:57:33 2024 ] 	Batch(2200/7879) done. Loss: 0.0318  lr:0.000100
[ Mon Jul  8 20:57:56 2024 ] 	Batch(2300/7879) done. Loss: 0.1355  lr:0.000100
[ Mon Jul  8 20:58:18 2024 ] 	Batch(2400/7879) done. Loss: 0.2047  lr:0.000100
[ Mon Jul  8 20:58:41 2024 ] 
Training: Epoch [67/120], Step [2499], Loss: 0.38057252764701843, Training Accuracy: 94.92
[ Mon Jul  8 20:58:41 2024 ] 	Batch(2500/7879) done. Loss: 0.0162  lr:0.000100
[ Mon Jul  8 20:59:04 2024 ] 	Batch(2600/7879) done. Loss: 0.3838  lr:0.000100
[ Mon Jul  8 20:59:26 2024 ] 	Batch(2700/7879) done. Loss: 0.4590  lr:0.000100
[ Mon Jul  8 20:59:49 2024 ] 	Batch(2800/7879) done. Loss: 0.5552  lr:0.000100
[ Mon Jul  8 21:00:11 2024 ] 	Batch(2900/7879) done. Loss: 0.6975  lr:0.000100
[ Mon Jul  8 21:00:34 2024 ] 
Training: Epoch [67/120], Step [2999], Loss: 0.3886032998561859, Training Accuracy: 94.83749999999999
[ Mon Jul  8 21:00:34 2024 ] 	Batch(3000/7879) done. Loss: 0.1661  lr:0.000100
[ Mon Jul  8 21:00:56 2024 ] 	Batch(3100/7879) done. Loss: 0.2723  lr:0.000100
[ Mon Jul  8 21:01:19 2024 ] 	Batch(3200/7879) done. Loss: 0.0164  lr:0.000100
[ Mon Jul  8 21:01:43 2024 ] 	Batch(3300/7879) done. Loss: 0.2196  lr:0.000100
[ Mon Jul  8 21:02:06 2024 ] 	Batch(3400/7879) done. Loss: 0.0446  lr:0.000100
[ Mon Jul  8 21:02:29 2024 ] 
Training: Epoch [67/120], Step [3499], Loss: 0.21379147469997406, Training Accuracy: 94.81428571428572
[ Mon Jul  8 21:02:29 2024 ] 	Batch(3500/7879) done. Loss: 0.0136  lr:0.000100
[ Mon Jul  8 21:02:53 2024 ] 	Batch(3600/7879) done. Loss: 0.3652  lr:0.000100
[ Mon Jul  8 21:03:16 2024 ] 	Batch(3700/7879) done. Loss: 0.0569  lr:0.000100
[ Mon Jul  8 21:03:39 2024 ] 	Batch(3800/7879) done. Loss: 0.0215  lr:0.000100
[ Mon Jul  8 21:04:02 2024 ] 	Batch(3900/7879) done. Loss: 0.2170  lr:0.000100
[ Mon Jul  8 21:04:26 2024 ] 
Training: Epoch [67/120], Step [3999], Loss: 0.02352774888277054, Training Accuracy: 94.746875
[ Mon Jul  8 21:04:26 2024 ] 	Batch(4000/7879) done. Loss: 0.3339  lr:0.000100
[ Mon Jul  8 21:04:49 2024 ] 	Batch(4100/7879) done. Loss: 0.1399  lr:0.000100
[ Mon Jul  8 21:05:13 2024 ] 	Batch(4200/7879) done. Loss: 0.7089  lr:0.000100
[ Mon Jul  8 21:05:36 2024 ] 	Batch(4300/7879) done. Loss: 0.0532  lr:0.000100
[ Mon Jul  8 21:05:59 2024 ] 	Batch(4400/7879) done. Loss: 0.3328  lr:0.000100
[ Mon Jul  8 21:06:21 2024 ] 
Training: Epoch [67/120], Step [4499], Loss: 0.17287413775920868, Training Accuracy: 94.73055555555555
[ Mon Jul  8 21:06:22 2024 ] 	Batch(4500/7879) done. Loss: 0.2319  lr:0.000100
[ Mon Jul  8 21:06:44 2024 ] 	Batch(4600/7879) done. Loss: 0.1932  lr:0.000100
[ Mon Jul  8 21:07:07 2024 ] 	Batch(4700/7879) done. Loss: 0.0105  lr:0.000100
[ Mon Jul  8 21:07:29 2024 ] 	Batch(4800/7879) done. Loss: 0.4662  lr:0.000100
[ Mon Jul  8 21:07:52 2024 ] 	Batch(4900/7879) done. Loss: 0.3338  lr:0.000100
[ Mon Jul  8 21:08:14 2024 ] 
Training: Epoch [67/120], Step [4999], Loss: 0.17719003558158875, Training Accuracy: 94.785
[ Mon Jul  8 21:08:15 2024 ] 	Batch(5000/7879) done. Loss: 0.0601  lr:0.000100
[ Mon Jul  8 21:08:37 2024 ] 	Batch(5100/7879) done. Loss: 0.1355  lr:0.000100
[ Mon Jul  8 21:09:00 2024 ] 	Batch(5200/7879) done. Loss: 0.0675  lr:0.000100
[ Mon Jul  8 21:09:22 2024 ] 	Batch(5300/7879) done. Loss: 0.0311  lr:0.000100
[ Mon Jul  8 21:09:45 2024 ] 	Batch(5400/7879) done. Loss: 0.0259  lr:0.000100
[ Mon Jul  8 21:10:07 2024 ] 
Training: Epoch [67/120], Step [5499], Loss: 0.0021588641684502363, Training Accuracy: 94.81363636363636
[ Mon Jul  8 21:10:08 2024 ] 	Batch(5500/7879) done. Loss: 0.0060  lr:0.000100
[ Mon Jul  8 21:10:30 2024 ] 	Batch(5600/7879) done. Loss: 0.2279  lr:0.000100
[ Mon Jul  8 21:10:53 2024 ] 	Batch(5700/7879) done. Loss: 0.0634  lr:0.000100
[ Mon Jul  8 21:11:15 2024 ] 	Batch(5800/7879) done. Loss: 0.2221  lr:0.000100
[ Mon Jul  8 21:11:38 2024 ] 	Batch(5900/7879) done. Loss: 0.6112  lr:0.000100
[ Mon Jul  8 21:12:00 2024 ] 
Training: Epoch [67/120], Step [5999], Loss: 0.019157277420163155, Training Accuracy: 94.78958333333334
[ Mon Jul  8 21:12:01 2024 ] 	Batch(6000/7879) done. Loss: 0.1734  lr:0.000100
[ Mon Jul  8 21:12:23 2024 ] 	Batch(6100/7879) done. Loss: 0.4507  lr:0.000100
[ Mon Jul  8 21:12:46 2024 ] 	Batch(6200/7879) done. Loss: 0.0951  lr:0.000100
[ Mon Jul  8 21:13:08 2024 ] 	Batch(6300/7879) done. Loss: 0.1056  lr:0.000100
[ Mon Jul  8 21:13:31 2024 ] 	Batch(6400/7879) done. Loss: 0.1084  lr:0.000100
[ Mon Jul  8 21:13:53 2024 ] 
Training: Epoch [67/120], Step [6499], Loss: 0.06934256851673126, Training Accuracy: 94.78653846153846
[ Mon Jul  8 21:13:54 2024 ] 	Batch(6500/7879) done. Loss: 0.0602  lr:0.000100
[ Mon Jul  8 21:14:16 2024 ] 	Batch(6600/7879) done. Loss: 0.0249  lr:0.000100
[ Mon Jul  8 21:14:39 2024 ] 	Batch(6700/7879) done. Loss: 0.2184  lr:0.000100
[ Mon Jul  8 21:15:02 2024 ] 	Batch(6800/7879) done. Loss: 0.0962  lr:0.000100
[ Mon Jul  8 21:15:24 2024 ] 	Batch(6900/7879) done. Loss: 0.1478  lr:0.000100
[ Mon Jul  8 21:15:46 2024 ] 
Training: Epoch [67/120], Step [6999], Loss: 0.07650205492973328, Training Accuracy: 94.82142857142857
[ Mon Jul  8 21:15:47 2024 ] 	Batch(7000/7879) done. Loss: 0.0594  lr:0.000100
[ Mon Jul  8 21:16:09 2024 ] 	Batch(7100/7879) done. Loss: 0.0737  lr:0.000100
[ Mon Jul  8 21:16:32 2024 ] 	Batch(7200/7879) done. Loss: 0.4920  lr:0.000100
[ Mon Jul  8 21:16:55 2024 ] 	Batch(7300/7879) done. Loss: 0.0950  lr:0.000100
[ Mon Jul  8 21:17:17 2024 ] 	Batch(7400/7879) done. Loss: 0.2906  lr:0.000100
[ Mon Jul  8 21:17:40 2024 ] 
Training: Epoch [67/120], Step [7499], Loss: 0.04834097996354103, Training Accuracy: 94.86
[ Mon Jul  8 21:17:40 2024 ] 	Batch(7500/7879) done. Loss: 0.0317  lr:0.000100
[ Mon Jul  8 21:18:03 2024 ] 	Batch(7600/7879) done. Loss: 0.1533  lr:0.000100
[ Mon Jul  8 21:18:25 2024 ] 	Batch(7700/7879) done. Loss: 0.6346  lr:0.000100
[ Mon Jul  8 21:18:48 2024 ] 	Batch(7800/7879) done. Loss: 0.6245  lr:0.000100
[ Mon Jul  8 21:19:05 2024 ] 	Mean training loss: 0.1839.
[ Mon Jul  8 21:19:05 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 21:19:05 2024 ] Training epoch: 69
[ Mon Jul  8 21:19:06 2024 ] 	Batch(0/7879) done. Loss: 0.0438  lr:0.000100
[ Mon Jul  8 21:19:29 2024 ] 	Batch(100/7879) done. Loss: 0.1566  lr:0.000100
[ Mon Jul  8 21:19:51 2024 ] 	Batch(200/7879) done. Loss: 0.0453  lr:0.000100
[ Mon Jul  8 21:20:14 2024 ] 	Batch(300/7879) done. Loss: 0.2367  lr:0.000100
[ Mon Jul  8 21:20:37 2024 ] 	Batch(400/7879) done. Loss: 0.3711  lr:0.000100
[ Mon Jul  8 21:21:00 2024 ] 
Training: Epoch [68/120], Step [499], Loss: 0.21086397767066956, Training Accuracy: 95.72500000000001
[ Mon Jul  8 21:21:00 2024 ] 	Batch(500/7879) done. Loss: 0.0691  lr:0.000100
[ Mon Jul  8 21:21:23 2024 ] 	Batch(600/7879) done. Loss: 0.3817  lr:0.000100
[ Mon Jul  8 21:21:46 2024 ] 	Batch(700/7879) done. Loss: 0.4680  lr:0.000100
[ Mon Jul  8 21:22:09 2024 ] 	Batch(800/7879) done. Loss: 0.1568  lr:0.000100
[ Mon Jul  8 21:22:32 2024 ] 	Batch(900/7879) done. Loss: 0.3556  lr:0.000100
[ Mon Jul  8 21:22:55 2024 ] 
Training: Epoch [68/120], Step [999], Loss: 0.2446340024471283, Training Accuracy: 95.0625
[ Mon Jul  8 21:22:55 2024 ] 	Batch(1000/7879) done. Loss: 0.2999  lr:0.000100
[ Mon Jul  8 21:23:18 2024 ] 	Batch(1100/7879) done. Loss: 0.0469  lr:0.000100
[ Mon Jul  8 21:23:41 2024 ] 	Batch(1200/7879) done. Loss: 0.9174  lr:0.000100
[ Mon Jul  8 21:24:03 2024 ] 	Batch(1300/7879) done. Loss: 0.4334  lr:0.000100
[ Mon Jul  8 21:24:26 2024 ] 	Batch(1400/7879) done. Loss: 0.0312  lr:0.000100
[ Mon Jul  8 21:24:48 2024 ] 
Training: Epoch [68/120], Step [1499], Loss: 0.03489797189831734, Training Accuracy: 95.19166666666666
[ Mon Jul  8 21:24:48 2024 ] 	Batch(1500/7879) done. Loss: 0.0242  lr:0.000100
[ Mon Jul  8 21:25:11 2024 ] 	Batch(1600/7879) done. Loss: 0.4360  lr:0.000100
[ Mon Jul  8 21:25:34 2024 ] 	Batch(1700/7879) done. Loss: 0.1359  lr:0.000100
[ Mon Jul  8 21:25:56 2024 ] 	Batch(1800/7879) done. Loss: 0.1617  lr:0.000100
[ Mon Jul  8 21:26:19 2024 ] 	Batch(1900/7879) done. Loss: 0.2798  lr:0.000100
[ Mon Jul  8 21:26:41 2024 ] 
Training: Epoch [68/120], Step [1999], Loss: 0.23774534463882446, Training Accuracy: 95.1375
[ Mon Jul  8 21:26:42 2024 ] 	Batch(2000/7879) done. Loss: 0.0959  lr:0.000100
[ Mon Jul  8 21:27:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0946  lr:0.000100
[ Mon Jul  8 21:27:27 2024 ] 	Batch(2200/7879) done. Loss: 0.1587  lr:0.000100
[ Mon Jul  8 21:27:49 2024 ] 	Batch(2300/7879) done. Loss: 0.0913  lr:0.000100
[ Mon Jul  8 21:28:12 2024 ] 	Batch(2400/7879) done. Loss: 0.0418  lr:0.000100
[ Mon Jul  8 21:28:34 2024 ] 
Training: Epoch [68/120], Step [2499], Loss: 0.1333901733160019, Training Accuracy: 95.205
[ Mon Jul  8 21:28:35 2024 ] 	Batch(2500/7879) done. Loss: 0.0155  lr:0.000100
[ Mon Jul  8 21:28:57 2024 ] 	Batch(2600/7879) done. Loss: 0.0958  lr:0.000100
[ Mon Jul  8 21:29:20 2024 ] 	Batch(2700/7879) done. Loss: 0.1848  lr:0.000100
[ Mon Jul  8 21:29:42 2024 ] 	Batch(2800/7879) done. Loss: 0.0436  lr:0.000100
[ Mon Jul  8 21:30:05 2024 ] 	Batch(2900/7879) done. Loss: 0.0625  lr:0.000100
[ Mon Jul  8 21:30:27 2024 ] 
Training: Epoch [68/120], Step [2999], Loss: 0.043435003608465195, Training Accuracy: 95.09583333333333
[ Mon Jul  8 21:30:28 2024 ] 	Batch(3000/7879) done. Loss: 0.1221  lr:0.000100
[ Mon Jul  8 21:30:50 2024 ] 	Batch(3100/7879) done. Loss: 0.3143  lr:0.000100
[ Mon Jul  8 21:31:13 2024 ] 	Batch(3200/7879) done. Loss: 0.1758  lr:0.000100
[ Mon Jul  8 21:31:36 2024 ] 	Batch(3300/7879) done. Loss: 0.1323  lr:0.000100
[ Mon Jul  8 21:32:00 2024 ] 	Batch(3400/7879) done. Loss: 0.0212  lr:0.000100
[ Mon Jul  8 21:32:23 2024 ] 
Training: Epoch [68/120], Step [3499], Loss: 0.020161671563982964, Training Accuracy: 95.03928571428571
[ Mon Jul  8 21:32:23 2024 ] 	Batch(3500/7879) done. Loss: 0.0195  lr:0.000100
[ Mon Jul  8 21:32:47 2024 ] 	Batch(3600/7879) done. Loss: 0.1677  lr:0.000100
[ Mon Jul  8 21:33:09 2024 ] 	Batch(3700/7879) done. Loss: 0.0357  lr:0.000100
[ Mon Jul  8 21:33:32 2024 ] 	Batch(3800/7879) done. Loss: 0.2718  lr:0.000100
[ Mon Jul  8 21:33:55 2024 ] 	Batch(3900/7879) done. Loss: 0.0596  lr:0.000100
[ Mon Jul  8 21:34:17 2024 ] 
Training: Epoch [68/120], Step [3999], Loss: 0.2408648580312729, Training Accuracy: 95.03125
[ Mon Jul  8 21:34:18 2024 ] 	Batch(4000/7879) done. Loss: 0.0618  lr:0.000100
[ Mon Jul  8 21:34:40 2024 ] 	Batch(4100/7879) done. Loss: 0.4370  lr:0.000100
[ Mon Jul  8 21:35:03 2024 ] 	Batch(4200/7879) done. Loss: 0.1418  lr:0.000100
[ Mon Jul  8 21:35:26 2024 ] 	Batch(4300/7879) done. Loss: 0.0139  lr:0.000100
[ Mon Jul  8 21:35:49 2024 ] 	Batch(4400/7879) done. Loss: 0.0335  lr:0.000100
[ Mon Jul  8 21:36:11 2024 ] 
Training: Epoch [68/120], Step [4499], Loss: 0.013869675807654858, Training Accuracy: 95.01111111111112
[ Mon Jul  8 21:36:11 2024 ] 	Batch(4500/7879) done. Loss: 0.0069  lr:0.000100
[ Mon Jul  8 21:36:34 2024 ] 	Batch(4600/7879) done. Loss: 0.0765  lr:0.000100
[ Mon Jul  8 21:36:57 2024 ] 	Batch(4700/7879) done. Loss: 0.0432  lr:0.000100
[ Mon Jul  8 21:37:19 2024 ] 	Batch(4800/7879) done. Loss: 0.1660  lr:0.000100
[ Mon Jul  8 21:37:42 2024 ] 	Batch(4900/7879) done. Loss: 0.0161  lr:0.000100
[ Mon Jul  8 21:38:05 2024 ] 
Training: Epoch [68/120], Step [4999], Loss: 0.018624573945999146, Training Accuracy: 95.0
[ Mon Jul  8 21:38:05 2024 ] 	Batch(5000/7879) done. Loss: 0.1452  lr:0.000100
[ Mon Jul  8 21:38:28 2024 ] 	Batch(5100/7879) done. Loss: 0.0408  lr:0.000100
[ Mon Jul  8 21:38:50 2024 ] 	Batch(5200/7879) done. Loss: 0.0016  lr:0.000100
[ Mon Jul  8 21:39:13 2024 ] 	Batch(5300/7879) done. Loss: 0.0661  lr:0.000100
[ Mon Jul  8 21:39:36 2024 ] 	Batch(5400/7879) done. Loss: 0.0341  lr:0.000100
[ Mon Jul  8 21:39:58 2024 ] 
Training: Epoch [68/120], Step [5499], Loss: 0.07616863399744034, Training Accuracy: 94.975
[ Mon Jul  8 21:39:58 2024 ] 	Batch(5500/7879) done. Loss: 0.1167  lr:0.000100
[ Mon Jul  8 21:40:21 2024 ] 	Batch(5600/7879) done. Loss: 0.0466  lr:0.000100
[ Mon Jul  8 21:40:44 2024 ] 	Batch(5700/7879) done. Loss: 0.0212  lr:0.000100
[ Mon Jul  8 21:41:07 2024 ] 	Batch(5800/7879) done. Loss: 0.8169  lr:0.000100
[ Mon Jul  8 21:41:29 2024 ] 	Batch(5900/7879) done. Loss: 0.3135  lr:0.000100
[ Mon Jul  8 21:41:52 2024 ] 
Training: Epoch [68/120], Step [5999], Loss: 0.0370662547647953, Training Accuracy: 94.94791666666667
[ Mon Jul  8 21:41:52 2024 ] 	Batch(6000/7879) done. Loss: 0.0102  lr:0.000100
[ Mon Jul  8 21:42:15 2024 ] 	Batch(6100/7879) done. Loss: 0.2336  lr:0.000100
[ Mon Jul  8 21:42:37 2024 ] 	Batch(6200/7879) done. Loss: 0.0050  lr:0.000100
[ Mon Jul  8 21:43:00 2024 ] 	Batch(6300/7879) done. Loss: 0.1016  lr:0.000100
[ Mon Jul  8 21:43:23 2024 ] 	Batch(6400/7879) done. Loss: 0.0509  lr:0.000100
[ Mon Jul  8 21:43:46 2024 ] 
Training: Epoch [68/120], Step [6499], Loss: 0.03656383603811264, Training Accuracy: 94.9423076923077
[ Mon Jul  8 21:43:46 2024 ] 	Batch(6500/7879) done. Loss: 0.0981  lr:0.000100
[ Mon Jul  8 21:44:08 2024 ] 	Batch(6600/7879) done. Loss: 0.1889  lr:0.000100
[ Mon Jul  8 21:44:31 2024 ] 	Batch(6700/7879) done. Loss: 0.4241  lr:0.000100
[ Mon Jul  8 21:44:54 2024 ] 	Batch(6800/7879) done. Loss: 0.0999  lr:0.000100
[ Mon Jul  8 21:45:17 2024 ] 	Batch(6900/7879) done. Loss: 0.2598  lr:0.000100
[ Mon Jul  8 21:45:41 2024 ] 
Training: Epoch [68/120], Step [6999], Loss: 0.1000719740986824, Training Accuracy: 94.94821428571429
[ Mon Jul  8 21:45:41 2024 ] 	Batch(7000/7879) done. Loss: 0.1935  lr:0.000100
[ Mon Jul  8 21:46:04 2024 ] 	Batch(7100/7879) done. Loss: 0.0520  lr:0.000100
[ Mon Jul  8 21:46:28 2024 ] 	Batch(7200/7879) done. Loss: 0.0877  lr:0.000100
[ Mon Jul  8 21:46:50 2024 ] 	Batch(7300/7879) done. Loss: 0.4190  lr:0.000100
[ Mon Jul  8 21:47:13 2024 ] 	Batch(7400/7879) done. Loss: 0.1827  lr:0.000100
[ Mon Jul  8 21:47:36 2024 ] 
Training: Epoch [68/120], Step [7499], Loss: 0.12908898293972015, Training Accuracy: 94.93
[ Mon Jul  8 21:47:36 2024 ] 	Batch(7500/7879) done. Loss: 0.2628  lr:0.000100
[ Mon Jul  8 21:47:59 2024 ] 	Batch(7600/7879) done. Loss: 0.1442  lr:0.000100
[ Mon Jul  8 21:48:21 2024 ] 	Batch(7700/7879) done. Loss: 0.0028  lr:0.000100
[ Mon Jul  8 21:48:44 2024 ] 	Batch(7800/7879) done. Loss: 0.1658  lr:0.000100
[ Mon Jul  8 21:49:02 2024 ] 	Mean training loss: 0.1779.
[ Mon Jul  8 21:49:02 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 21:49:02 2024 ] Training epoch: 70
[ Mon Jul  8 21:49:02 2024 ] 	Batch(0/7879) done. Loss: 0.0316  lr:0.000100
[ Mon Jul  8 21:49:25 2024 ] 	Batch(100/7879) done. Loss: 1.2978  lr:0.000100
[ Mon Jul  8 21:49:48 2024 ] 	Batch(200/7879) done. Loss: 0.6950  lr:0.000100
[ Mon Jul  8 21:50:11 2024 ] 	Batch(300/7879) done. Loss: 0.0275  lr:0.000100
[ Mon Jul  8 21:50:33 2024 ] 	Batch(400/7879) done. Loss: 0.0978  lr:0.000100
[ Mon Jul  8 21:50:56 2024 ] 
Training: Epoch [69/120], Step [499], Loss: 0.03291990980505943, Training Accuracy: 95.42500000000001
[ Mon Jul  8 21:50:56 2024 ] 	Batch(500/7879) done. Loss: 0.0439  lr:0.000100
[ Mon Jul  8 21:51:19 2024 ] 	Batch(600/7879) done. Loss: 1.1086  lr:0.000100
[ Mon Jul  8 21:51:41 2024 ] 	Batch(700/7879) done. Loss: 0.1049  lr:0.000100
[ Mon Jul  8 21:52:04 2024 ] 	Batch(800/7879) done. Loss: 0.1396  lr:0.000100
[ Mon Jul  8 21:52:27 2024 ] 	Batch(900/7879) done. Loss: 0.0051  lr:0.000100
[ Mon Jul  8 21:52:49 2024 ] 
Training: Epoch [69/120], Step [999], Loss: 0.38328632712364197, Training Accuracy: 95.3
[ Mon Jul  8 21:52:50 2024 ] 	Batch(1000/7879) done. Loss: 0.0913  lr:0.000100
[ Mon Jul  8 21:53:12 2024 ] 	Batch(1100/7879) done. Loss: 0.3126  lr:0.000100
[ Mon Jul  8 21:53:35 2024 ] 	Batch(1200/7879) done. Loss: 0.0278  lr:0.000100
[ Mon Jul  8 21:53:58 2024 ] 	Batch(1300/7879) done. Loss: 0.5319  lr:0.000100
[ Mon Jul  8 21:54:21 2024 ] 	Batch(1400/7879) done. Loss: 0.1486  lr:0.000100
[ Mon Jul  8 21:54:43 2024 ] 
Training: Epoch [69/120], Step [1499], Loss: 0.020130127668380737, Training Accuracy: 95.31666666666668
[ Mon Jul  8 21:54:43 2024 ] 	Batch(1500/7879) done. Loss: 0.0750  lr:0.000100
[ Mon Jul  8 21:55:06 2024 ] 	Batch(1600/7879) done. Loss: 0.0143  lr:0.000100
[ Mon Jul  8 21:55:29 2024 ] 	Batch(1700/7879) done. Loss: 0.1468  lr:0.000100
[ Mon Jul  8 21:55:51 2024 ] 	Batch(1800/7879) done. Loss: 0.0945  lr:0.000100
[ Mon Jul  8 21:56:14 2024 ] 	Batch(1900/7879) done. Loss: 0.1821  lr:0.000100
[ Mon Jul  8 21:56:37 2024 ] 
Training: Epoch [69/120], Step [1999], Loss: 0.0930376946926117, Training Accuracy: 95.28125
[ Mon Jul  8 21:56:37 2024 ] 	Batch(2000/7879) done. Loss: 0.1881  lr:0.000100
[ Mon Jul  8 21:57:00 2024 ] 	Batch(2100/7879) done. Loss: 0.0057  lr:0.000100
[ Mon Jul  8 21:57:23 2024 ] 	Batch(2200/7879) done. Loss: 0.3418  lr:0.000100
[ Mon Jul  8 21:57:45 2024 ] 	Batch(2300/7879) done. Loss: 0.0143  lr:0.000100
[ Mon Jul  8 21:58:08 2024 ] 	Batch(2400/7879) done. Loss: 0.0050  lr:0.000100
[ Mon Jul  8 21:58:31 2024 ] 
Training: Epoch [69/120], Step [2499], Loss: 0.09292963147163391, Training Accuracy: 95.24000000000001
[ Mon Jul  8 21:58:31 2024 ] 	Batch(2500/7879) done. Loss: 0.1143  lr:0.000100
[ Mon Jul  8 21:58:53 2024 ] 	Batch(2600/7879) done. Loss: 0.0671  lr:0.000100
[ Mon Jul  8 21:59:16 2024 ] 	Batch(2700/7879) done. Loss: 0.0269  lr:0.000100
[ Mon Jul  8 21:59:39 2024 ] 	Batch(2800/7879) done. Loss: 0.1750  lr:0.000100
[ Mon Jul  8 22:00:02 2024 ] 	Batch(2900/7879) done. Loss: 0.4977  lr:0.000100
[ Mon Jul  8 22:00:25 2024 ] 
Training: Epoch [69/120], Step [2999], Loss: 0.029619906097650528, Training Accuracy: 95.2625
[ Mon Jul  8 22:00:25 2024 ] 	Batch(3000/7879) done. Loss: 0.0393  lr:0.000100
[ Mon Jul  8 22:00:48 2024 ] 	Batch(3100/7879) done. Loss: 0.0708  lr:0.000100
[ Mon Jul  8 22:01:12 2024 ] 	Batch(3200/7879) done. Loss: 0.1998  lr:0.000100
[ Mon Jul  8 22:01:35 2024 ] 	Batch(3300/7879) done. Loss: 0.2862  lr:0.000100
[ Mon Jul  8 22:01:59 2024 ] 	Batch(3400/7879) done. Loss: 0.0704  lr:0.000100
[ Mon Jul  8 22:02:22 2024 ] 
Training: Epoch [69/120], Step [3499], Loss: 0.26012712717056274, Training Accuracy: 95.10714285714286
[ Mon Jul  8 22:02:22 2024 ] 	Batch(3500/7879) done. Loss: 0.0310  lr:0.000100
[ Mon Jul  8 22:02:46 2024 ] 	Batch(3600/7879) done. Loss: 0.4698  lr:0.000100
[ Mon Jul  8 22:03:09 2024 ] 	Batch(3700/7879) done. Loss: 0.0256  lr:0.000100
[ Mon Jul  8 22:03:31 2024 ] 	Batch(3800/7879) done. Loss: 0.1988  lr:0.000100
[ Mon Jul  8 22:03:54 2024 ] 	Batch(3900/7879) done. Loss: 0.0133  lr:0.000100
[ Mon Jul  8 22:04:16 2024 ] 
Training: Epoch [69/120], Step [3999], Loss: 0.0754258781671524, Training Accuracy: 95.03437500000001
[ Mon Jul  8 22:04:17 2024 ] 	Batch(4000/7879) done. Loss: 0.2829  lr:0.000100
[ Mon Jul  8 22:04:39 2024 ] 	Batch(4100/7879) done. Loss: 0.2370  lr:0.000100
[ Mon Jul  8 22:05:02 2024 ] 	Batch(4200/7879) done. Loss: 0.0274  lr:0.000100
[ Mon Jul  8 22:05:25 2024 ] 	Batch(4300/7879) done. Loss: 0.0841  lr:0.000100
[ Mon Jul  8 22:05:48 2024 ] 	Batch(4400/7879) done. Loss: 0.1631  lr:0.000100
[ Mon Jul  8 22:06:10 2024 ] 
Training: Epoch [69/120], Step [4499], Loss: 0.028525039553642273, Training Accuracy: 95.02777777777777
[ Mon Jul  8 22:06:10 2024 ] 	Batch(4500/7879) done. Loss: 0.0233  lr:0.000100
[ Mon Jul  8 22:06:33 2024 ] 	Batch(4600/7879) done. Loss: 0.4880  lr:0.000100
[ Mon Jul  8 22:06:56 2024 ] 	Batch(4700/7879) done. Loss: 0.0508  lr:0.000100
[ Mon Jul  8 22:07:19 2024 ] 	Batch(4800/7879) done. Loss: 0.0977  lr:0.000100
[ Mon Jul  8 22:07:41 2024 ] 	Batch(4900/7879) done. Loss: 0.0465  lr:0.000100
[ Mon Jul  8 22:08:04 2024 ] 
Training: Epoch [69/120], Step [4999], Loss: 0.061681635677814484, Training Accuracy: 95.015
[ Mon Jul  8 22:08:04 2024 ] 	Batch(5000/7879) done. Loss: 0.0720  lr:0.000100
[ Mon Jul  8 22:08:27 2024 ] 	Batch(5100/7879) done. Loss: 0.1550  lr:0.000100
[ Mon Jul  8 22:08:50 2024 ] 	Batch(5200/7879) done. Loss: 0.7243  lr:0.000100
[ Mon Jul  8 22:09:12 2024 ] 	Batch(5300/7879) done. Loss: 0.2887  lr:0.000100
[ Mon Jul  8 22:09:35 2024 ] 	Batch(5400/7879) done. Loss: 0.0820  lr:0.000100
[ Mon Jul  8 22:09:58 2024 ] 
Training: Epoch [69/120], Step [5499], Loss: 0.13276204466819763, Training Accuracy: 95.0340909090909
[ Mon Jul  8 22:09:58 2024 ] 	Batch(5500/7879) done. Loss: 0.3146  lr:0.000100
[ Mon Jul  8 22:10:21 2024 ] 	Batch(5600/7879) done. Loss: 0.0385  lr:0.000100
[ Mon Jul  8 22:10:43 2024 ] 	Batch(5700/7879) done. Loss: 0.4854  lr:0.000100
[ Mon Jul  8 22:11:06 2024 ] 	Batch(5800/7879) done. Loss: 0.2571  lr:0.000100
[ Mon Jul  8 22:11:29 2024 ] 	Batch(5900/7879) done. Loss: 0.0939  lr:0.000100
[ Mon Jul  8 22:11:51 2024 ] 
Training: Epoch [69/120], Step [5999], Loss: 0.5824803113937378, Training Accuracy: 94.99583333333334
[ Mon Jul  8 22:11:52 2024 ] 	Batch(6000/7879) done. Loss: 0.0646  lr:0.000100
[ Mon Jul  8 22:12:15 2024 ] 	Batch(6100/7879) done. Loss: 0.4925  lr:0.000100
[ Mon Jul  8 22:12:37 2024 ] 	Batch(6200/7879) done. Loss: 0.5737  lr:0.000100
[ Mon Jul  8 22:13:00 2024 ] 	Batch(6300/7879) done. Loss: 0.2107  lr:0.000100
[ Mon Jul  8 22:13:23 2024 ] 	Batch(6400/7879) done. Loss: 0.2274  lr:0.000100
[ Mon Jul  8 22:13:46 2024 ] 
Training: Epoch [69/120], Step [6499], Loss: 0.29864466190338135, Training Accuracy: 94.99038461538461
[ Mon Jul  8 22:13:46 2024 ] 	Batch(6500/7879) done. Loss: 0.0771  lr:0.000100
[ Mon Jul  8 22:14:10 2024 ] 	Batch(6600/7879) done. Loss: 0.0192  lr:0.000100
[ Mon Jul  8 22:14:33 2024 ] 	Batch(6700/7879) done. Loss: 0.1098  lr:0.000100
[ Mon Jul  8 22:14:57 2024 ] 	Batch(6800/7879) done. Loss: 0.1691  lr:0.000100
[ Mon Jul  8 22:15:20 2024 ] 	Batch(6900/7879) done. Loss: 0.1192  lr:0.000100
[ Mon Jul  8 22:15:43 2024 ] 
Training: Epoch [69/120], Step [6999], Loss: 0.4297797381877899, Training Accuracy: 94.97678571428571
[ Mon Jul  8 22:15:44 2024 ] 	Batch(7000/7879) done. Loss: 0.1321  lr:0.000100
[ Mon Jul  8 22:16:07 2024 ] 	Batch(7100/7879) done. Loss: 0.1199  lr:0.000100
[ Mon Jul  8 22:16:31 2024 ] 	Batch(7200/7879) done. Loss: 0.2575  lr:0.000100
[ Mon Jul  8 22:16:54 2024 ] 	Batch(7300/7879) done. Loss: 0.1635  lr:0.000100
[ Mon Jul  8 22:17:18 2024 ] 	Batch(7400/7879) done. Loss: 0.1756  lr:0.000100
[ Mon Jul  8 22:17:41 2024 ] 
Training: Epoch [69/120], Step [7499], Loss: 0.09110908210277557, Training Accuracy: 94.94166666666666
[ Mon Jul  8 22:17:41 2024 ] 	Batch(7500/7879) done. Loss: 0.3225  lr:0.000100
[ Mon Jul  8 22:18:05 2024 ] 	Batch(7600/7879) done. Loss: 0.1769  lr:0.000100
[ Mon Jul  8 22:18:28 2024 ] 	Batch(7700/7879) done. Loss: 0.4890  lr:0.000100
[ Mon Jul  8 22:18:51 2024 ] 	Batch(7800/7879) done. Loss: 0.1459  lr:0.000100
[ Mon Jul  8 22:19:10 2024 ] 	Mean training loss: 0.1793.
[ Mon Jul  8 22:19:10 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 22:19:10 2024 ] Eval epoch: 70
[ Mon Jul  8 22:25:06 2024 ] 	Mean val loss of 6365 batches: 0.9983724623723332.
[ Mon Jul  8 22:25:06 2024 ] 
Validation: Epoch [69/120], Samples [39140.0/50919], Loss: 0.2676827311515808, Validation Accuracy: 76.86718120937175
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 1 : 201 / 275 = 73 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 2 : 231 / 273 = 84 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 3 : 225 / 273 = 82 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 4 : 222 / 275 = 80 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 5 : 231 / 275 = 84 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 6 : 208 / 275 = 75 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 7 : 253 / 273 = 92 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 8 : 263 / 273 = 96 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 9 : 196 / 273 = 71 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 10 : 116 / 273 = 42 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 11 : 139 / 272 = 51 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 12 : 220 / 271 = 81 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 13 : 266 / 275 = 96 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 14 : 264 / 276 = 95 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 15 : 212 / 273 = 77 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 16 : 165 / 274 = 60 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 17 : 241 / 273 = 88 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 18 : 236 / 274 = 86 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 19 : 238 / 272 = 87 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 20 : 251 / 273 = 91 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 21 : 228 / 274 = 83 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 22 : 249 / 274 = 90 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 23 : 248 / 276 = 89 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 24 : 235 / 274 = 85 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 25 : 259 / 275 = 94 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 26 : 262 / 276 = 94 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 27 : 228 / 275 = 82 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 28 : 170 / 275 = 61 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 29 : 150 / 275 = 54 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 30 : 175 / 276 = 63 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 31 : 230 / 276 = 83 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 32 : 242 / 276 = 87 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 33 : 229 / 276 = 82 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 34 : 242 / 276 = 87 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 35 : 236 / 275 = 85 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 36 : 225 / 276 = 81 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 37 : 252 / 276 = 91 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 38 : 241 / 276 = 87 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 39 : 237 / 276 = 85 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 40 : 189 / 276 = 68 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 41 : 260 / 276 = 94 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 42 : 249 / 275 = 90 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 43 : 173 / 276 = 62 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 44 : 255 / 276 = 92 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 45 : 260 / 276 = 94 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 46 : 222 / 276 = 80 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 47 : 204 / 275 = 74 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 48 : 219 / 275 = 79 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 49 : 221 / 274 = 80 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 50 : 240 / 276 = 86 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 51 : 255 / 276 = 92 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 52 : 238 / 276 = 86 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 53 : 244 / 276 = 88 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 54 : 257 / 274 = 93 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 55 : 237 / 276 = 85 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 56 : 246 / 275 = 89 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 57 : 269 / 276 = 97 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 58 : 267 / 273 = 97 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 59 : 251 / 276 = 90 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 60 : 465 / 561 = 82 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 61 : 470 / 566 = 83 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 62 : 380 / 572 = 66 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 63 : 485 / 570 = 85 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 64 : 426 / 574 = 74 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 65 : 495 / 573 = 86 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 66 : 400 / 573 = 69 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 67 : 390 / 575 = 67 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 68 : 362 / 575 = 62 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 69 : 471 / 575 = 81 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 70 : 211 / 575 = 36 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 71 : 246 / 575 = 42 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 72 : 95 / 571 = 16 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 73 : 257 / 570 = 45 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 74 : 351 / 569 = 61 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 75 : 375 / 573 = 65 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 76 : 378 / 574 = 65 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 77 : 368 / 573 = 64 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 78 : 449 / 575 = 78 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 79 : 545 / 574 = 94 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 80 : 477 / 573 = 83 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 81 : 313 / 575 = 54 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 82 : 352 / 575 = 61 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 83 : 243 / 572 = 42 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 84 : 421 / 574 = 73 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 85 : 380 / 574 = 66 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 86 : 483 / 575 = 84 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 87 : 485 / 576 = 84 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 88 : 418 / 575 = 72 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 89 : 445 / 576 = 77 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 90 : 234 / 574 = 40 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 91 : 454 / 568 = 79 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 92 : 412 / 576 = 71 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 93 : 339 / 573 = 59 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 94 : 521 / 574 = 90 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 95 : 542 / 575 = 94 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 96 : 557 / 575 = 96 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 97 : 551 / 574 = 95 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 98 : 538 / 575 = 93 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 99 : 531 / 574 = 92 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 100 : 459 / 574 = 79 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 101 : 515 / 574 = 89 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 102 : 332 / 575 = 57 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 103 : 493 / 576 = 85 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 104 : 264 / 575 = 45 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 105 : 250 / 575 = 43 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 106 : 320 / 576 = 55 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 107 : 499 / 576 = 86 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 108 : 476 / 575 = 82 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 109 : 403 / 575 = 70 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 110 : 513 / 575 = 89 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 111 : 542 / 576 = 94 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 112 : 538 / 575 = 93 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 113 : 522 / 576 = 90 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 114 : 506 / 576 = 87 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 115 : 525 / 576 = 91 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 116 : 484 / 575 = 84 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 117 : 495 / 575 = 86 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 118 : 462 / 575 = 80 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 119 : 513 / 576 = 89 %
[ Mon Jul  8 22:25:06 2024 ] Accuracy of 120 : 242 / 274 = 88 %
[ Mon Jul  8 22:25:06 2024 ] Training epoch: 71
[ Mon Jul  8 22:25:07 2024 ] 	Batch(0/7879) done. Loss: 0.1974  lr:0.000100
[ Mon Jul  8 22:25:30 2024 ] 	Batch(100/7879) done. Loss: 0.0470  lr:0.000100
[ Mon Jul  8 22:25:53 2024 ] 	Batch(200/7879) done. Loss: 0.1486  lr:0.000100
[ Mon Jul  8 22:26:16 2024 ] 	Batch(300/7879) done. Loss: 0.3770  lr:0.000100
[ Mon Jul  8 22:26:40 2024 ] 	Batch(400/7879) done. Loss: 0.2912  lr:0.000100
[ Mon Jul  8 22:27:02 2024 ] 
Training: Epoch [70/120], Step [499], Loss: 0.014116082340478897, Training Accuracy: 95.375
[ Mon Jul  8 22:27:03 2024 ] 	Batch(500/7879) done. Loss: 0.1823  lr:0.000100
[ Mon Jul  8 22:27:26 2024 ] 	Batch(600/7879) done. Loss: 0.0706  lr:0.000100
[ Mon Jul  8 22:27:49 2024 ] 	Batch(700/7879) done. Loss: 0.0141  lr:0.000100
[ Mon Jul  8 22:28:12 2024 ] 	Batch(800/7879) done. Loss: 0.1058  lr:0.000100
[ Mon Jul  8 22:28:35 2024 ] 	Batch(900/7879) done. Loss: 0.2230  lr:0.000100
[ Mon Jul  8 22:28:58 2024 ] 
Training: Epoch [70/120], Step [999], Loss: 0.6274159550666809, Training Accuracy: 95.2125
[ Mon Jul  8 22:28:58 2024 ] 	Batch(1000/7879) done. Loss: 0.2951  lr:0.000100
[ Mon Jul  8 22:29:21 2024 ] 	Batch(1100/7879) done. Loss: 0.0044  lr:0.000100
[ Mon Jul  8 22:29:45 2024 ] 	Batch(1200/7879) done. Loss: 0.5073  lr:0.000100
[ Mon Jul  8 22:30:08 2024 ] 	Batch(1300/7879) done. Loss: 0.0101  lr:0.000100
[ Mon Jul  8 22:30:31 2024 ] 	Batch(1400/7879) done. Loss: 0.0573  lr:0.000100
[ Mon Jul  8 22:30:54 2024 ] 
Training: Epoch [70/120], Step [1499], Loss: 0.06613663583993912, Training Accuracy: 95.075
[ Mon Jul  8 22:30:54 2024 ] 	Batch(1500/7879) done. Loss: 0.0511  lr:0.000100
[ Mon Jul  8 22:31:17 2024 ] 	Batch(1600/7879) done. Loss: 0.1912  lr:0.000100
[ Mon Jul  8 22:31:41 2024 ] 	Batch(1700/7879) done. Loss: 0.4454  lr:0.000100
[ Mon Jul  8 22:32:04 2024 ] 	Batch(1800/7879) done. Loss: 0.0743  lr:0.000100
[ Mon Jul  8 22:32:27 2024 ] 	Batch(1900/7879) done. Loss: 0.0149  lr:0.000100
[ Mon Jul  8 22:32:50 2024 ] 
Training: Epoch [70/120], Step [1999], Loss: 0.05266813188791275, Training Accuracy: 95.14375000000001
[ Mon Jul  8 22:32:50 2024 ] 	Batch(2000/7879) done. Loss: 0.0135  lr:0.000100
[ Mon Jul  8 22:33:14 2024 ] 	Batch(2100/7879) done. Loss: 0.0211  lr:0.000100
[ Mon Jul  8 22:33:38 2024 ] 	Batch(2200/7879) done. Loss: 0.1197  lr:0.000100
[ Mon Jul  8 22:34:01 2024 ] 	Batch(2300/7879) done. Loss: 0.0495  lr:0.000100
[ Mon Jul  8 22:34:23 2024 ] 	Batch(2400/7879) done. Loss: 0.0816  lr:0.000100
[ Mon Jul  8 22:34:46 2024 ] 
Training: Epoch [70/120], Step [2499], Loss: 0.011096158996224403, Training Accuracy: 95.145
[ Mon Jul  8 22:34:46 2024 ] 	Batch(2500/7879) done. Loss: 0.0596  lr:0.000100
[ Mon Jul  8 22:35:09 2024 ] 	Batch(2600/7879) done. Loss: 0.1657  lr:0.000100
[ Mon Jul  8 22:35:31 2024 ] 	Batch(2700/7879) done. Loss: 0.0622  lr:0.000100
[ Mon Jul  8 22:35:54 2024 ] 	Batch(2800/7879) done. Loss: 0.0779  lr:0.000100
[ Mon Jul  8 22:36:17 2024 ] 	Batch(2900/7879) done. Loss: 0.0737  lr:0.000100
[ Mon Jul  8 22:36:39 2024 ] 
Training: Epoch [70/120], Step [2999], Loss: 0.08875686675310135, Training Accuracy: 95.16250000000001
[ Mon Jul  8 22:36:40 2024 ] 	Batch(3000/7879) done. Loss: 0.0311  lr:0.000100
[ Mon Jul  8 22:37:02 2024 ] 	Batch(3100/7879) done. Loss: 0.1043  lr:0.000100
[ Mon Jul  8 22:37:25 2024 ] 	Batch(3200/7879) done. Loss: 0.3629  lr:0.000100
[ Mon Jul  8 22:37:48 2024 ] 	Batch(3300/7879) done. Loss: 0.1619  lr:0.000100
[ Mon Jul  8 22:38:11 2024 ] 	Batch(3400/7879) done. Loss: 0.3679  lr:0.000100
[ Mon Jul  8 22:38:33 2024 ] 
Training: Epoch [70/120], Step [3499], Loss: 0.5178529024124146, Training Accuracy: 95.12142857142857
[ Mon Jul  8 22:38:33 2024 ] 	Batch(3500/7879) done. Loss: 0.1454  lr:0.000100
[ Mon Jul  8 22:38:56 2024 ] 	Batch(3600/7879) done. Loss: 0.0018  lr:0.000100
[ Mon Jul  8 22:39:19 2024 ] 	Batch(3700/7879) done. Loss: 0.1864  lr:0.000100
[ Mon Jul  8 22:39:41 2024 ] 	Batch(3800/7879) done. Loss: 0.5676  lr:0.000100
[ Mon Jul  8 22:40:04 2024 ] 	Batch(3900/7879) done. Loss: 0.2341  lr:0.000100
[ Mon Jul  8 22:40:27 2024 ] 
Training: Epoch [70/120], Step [3999], Loss: 0.06292825192213058, Training Accuracy: 95.096875
[ Mon Jul  8 22:40:27 2024 ] 	Batch(4000/7879) done. Loss: 0.0874  lr:0.000100
[ Mon Jul  8 22:40:50 2024 ] 	Batch(4100/7879) done. Loss: 0.5915  lr:0.000100
[ Mon Jul  8 22:41:12 2024 ] 	Batch(4200/7879) done. Loss: 0.0530  lr:0.000100
[ Mon Jul  8 22:41:35 2024 ] 	Batch(4300/7879) done. Loss: 0.2140  lr:0.000100
[ Mon Jul  8 22:41:58 2024 ] 	Batch(4400/7879) done. Loss: 0.1598  lr:0.000100
[ Mon Jul  8 22:42:20 2024 ] 
Training: Epoch [70/120], Step [4499], Loss: 0.03442368283867836, Training Accuracy: 95.11944444444444
[ Mon Jul  8 22:42:20 2024 ] 	Batch(4500/7879) done. Loss: 0.0075  lr:0.000100
[ Mon Jul  8 22:42:43 2024 ] 	Batch(4600/7879) done. Loss: 0.1201  lr:0.000100
[ Mon Jul  8 22:43:06 2024 ] 	Batch(4700/7879) done. Loss: 0.0491  lr:0.000100
[ Mon Jul  8 22:43:29 2024 ] 	Batch(4800/7879) done. Loss: 0.2601  lr:0.000100
[ Mon Jul  8 22:43:51 2024 ] 	Batch(4900/7879) done. Loss: 0.7859  lr:0.000100
[ Mon Jul  8 22:44:14 2024 ] 
Training: Epoch [70/120], Step [4999], Loss: 0.09020569920539856, Training Accuracy: 95.08
[ Mon Jul  8 22:44:14 2024 ] 	Batch(5000/7879) done. Loss: 0.0145  lr:0.000100
[ Mon Jul  8 22:44:37 2024 ] 	Batch(5100/7879) done. Loss: 0.0208  lr:0.000100
[ Mon Jul  8 22:44:59 2024 ] 	Batch(5200/7879) done. Loss: 0.0785  lr:0.000100
[ Mon Jul  8 22:45:22 2024 ] 	Batch(5300/7879) done. Loss: 0.1367  lr:0.000100
[ Mon Jul  8 22:45:45 2024 ] 	Batch(5400/7879) done. Loss: 0.2691  lr:0.000100
[ Mon Jul  8 22:46:07 2024 ] 
Training: Epoch [70/120], Step [5499], Loss: 0.14693424105644226, Training Accuracy: 95.10227272727273
[ Mon Jul  8 22:46:08 2024 ] 	Batch(5500/7879) done. Loss: 0.0479  lr:0.000100
[ Mon Jul  8 22:46:31 2024 ] 	Batch(5600/7879) done. Loss: 0.0542  lr:0.000100
[ Mon Jul  8 22:46:53 2024 ] 	Batch(5700/7879) done. Loss: 0.7159  lr:0.000100
[ Mon Jul  8 22:47:16 2024 ] 	Batch(5800/7879) done. Loss: 0.0405  lr:0.000100
[ Mon Jul  8 22:47:39 2024 ] 	Batch(5900/7879) done. Loss: 0.3897  lr:0.000100
[ Mon Jul  8 22:48:01 2024 ] 
Training: Epoch [70/120], Step [5999], Loss: 0.06281702220439911, Training Accuracy: 95.125
[ Mon Jul  8 22:48:02 2024 ] 	Batch(6000/7879) done. Loss: 0.1089  lr:0.000100
[ Mon Jul  8 22:48:24 2024 ] 	Batch(6100/7879) done. Loss: 0.1311  lr:0.000100
[ Mon Jul  8 22:48:47 2024 ] 	Batch(6200/7879) done. Loss: 0.1378  lr:0.000100
[ Mon Jul  8 22:49:11 2024 ] 	Batch(6300/7879) done. Loss: 0.0092  lr:0.000100
[ Mon Jul  8 22:49:34 2024 ] 	Batch(6400/7879) done. Loss: 0.0768  lr:0.000100
[ Mon Jul  8 22:49:58 2024 ] 
Training: Epoch [70/120], Step [6499], Loss: 0.04767771065235138, Training Accuracy: 95.15384615384616
[ Mon Jul  8 22:49:58 2024 ] 	Batch(6500/7879) done. Loss: 0.0143  lr:0.000100
[ Mon Jul  8 22:50:21 2024 ] 	Batch(6600/7879) done. Loss: 0.0152  lr:0.000100
[ Mon Jul  8 22:50:43 2024 ] 	Batch(6700/7879) done. Loss: 0.0376  lr:0.000100
[ Mon Jul  8 22:51:06 2024 ] 	Batch(6800/7879) done. Loss: 0.0941  lr:0.000100
[ Mon Jul  8 22:51:29 2024 ] 	Batch(6900/7879) done. Loss: 0.0477  lr:0.000100
[ Mon Jul  8 22:51:51 2024 ] 
Training: Epoch [70/120], Step [6999], Loss: 0.12514176964759827, Training Accuracy: 95.18214285714286
[ Mon Jul  8 22:51:52 2024 ] 	Batch(7000/7879) done. Loss: 0.0146  lr:0.000100
[ Mon Jul  8 22:52:14 2024 ] 	Batch(7100/7879) done. Loss: 0.2982  lr:0.000100
[ Mon Jul  8 22:52:37 2024 ] 	Batch(7200/7879) done. Loss: 0.1733  lr:0.000100
[ Mon Jul  8 22:53:00 2024 ] 	Batch(7300/7879) done. Loss: 0.0197  lr:0.000100
[ Mon Jul  8 22:53:23 2024 ] 	Batch(7400/7879) done. Loss: 0.5004  lr:0.000100
[ Mon Jul  8 22:53:45 2024 ] 
Training: Epoch [70/120], Step [7499], Loss: 0.368757963180542, Training Accuracy: 95.145
[ Mon Jul  8 22:53:45 2024 ] 	Batch(7500/7879) done. Loss: 0.2850  lr:0.000100
[ Mon Jul  8 22:54:08 2024 ] 	Batch(7600/7879) done. Loss: 0.2721  lr:0.000100
[ Mon Jul  8 22:54:31 2024 ] 	Batch(7700/7879) done. Loss: 0.3075  lr:0.000100
[ Mon Jul  8 22:54:54 2024 ] 	Batch(7800/7879) done. Loss: 0.0269  lr:0.000100
[ Mon Jul  8 22:55:13 2024 ] 	Mean training loss: 0.1778.
[ Mon Jul  8 22:55:13 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 22:55:13 2024 ] Training epoch: 72
[ Mon Jul  8 22:55:13 2024 ] 	Batch(0/7879) done. Loss: 0.0575  lr:0.000100
[ Mon Jul  8 22:55:36 2024 ] 	Batch(100/7879) done. Loss: 0.8278  lr:0.000100
[ Mon Jul  8 22:55:59 2024 ] 	Batch(200/7879) done. Loss: 0.4818  lr:0.000100
[ Mon Jul  8 22:56:21 2024 ] 	Batch(300/7879) done. Loss: 0.2734  lr:0.000100
[ Mon Jul  8 22:56:44 2024 ] 	Batch(400/7879) done. Loss: 0.3300  lr:0.000100
[ Mon Jul  8 22:57:07 2024 ] 
Training: Epoch [71/120], Step [499], Loss: 0.2081618309020996, Training Accuracy: 95.1
[ Mon Jul  8 22:57:07 2024 ] 	Batch(500/7879) done. Loss: 0.1837  lr:0.000100
[ Mon Jul  8 22:57:29 2024 ] 	Batch(600/7879) done. Loss: 0.0394  lr:0.000100
[ Mon Jul  8 22:57:52 2024 ] 	Batch(700/7879) done. Loss: 0.2325  lr:0.000100
[ Mon Jul  8 22:58:15 2024 ] 	Batch(800/7879) done. Loss: 0.1271  lr:0.000100
[ Mon Jul  8 22:58:38 2024 ] 	Batch(900/7879) done. Loss: 0.2509  lr:0.000100
[ Mon Jul  8 22:59:00 2024 ] 
Training: Epoch [71/120], Step [999], Loss: 0.12166769802570343, Training Accuracy: 94.975
[ Mon Jul  8 22:59:00 2024 ] 	Batch(1000/7879) done. Loss: 0.3881  lr:0.000100
[ Mon Jul  8 22:59:23 2024 ] 	Batch(1100/7879) done. Loss: 0.2257  lr:0.000100
[ Mon Jul  8 22:59:46 2024 ] 	Batch(1200/7879) done. Loss: 0.0669  lr:0.000100
[ Mon Jul  8 23:00:09 2024 ] 	Batch(1300/7879) done. Loss: 0.1092  lr:0.000100
[ Mon Jul  8 23:00:32 2024 ] 	Batch(1400/7879) done. Loss: 0.0882  lr:0.000100
[ Mon Jul  8 23:00:55 2024 ] 
Training: Epoch [71/120], Step [1499], Loss: 0.054076481610536575, Training Accuracy: 94.91666666666667
[ Mon Jul  8 23:00:55 2024 ] 	Batch(1500/7879) done. Loss: 0.1266  lr:0.000100
[ Mon Jul  8 23:01:18 2024 ] 	Batch(1600/7879) done. Loss: 0.0851  lr:0.000100
[ Mon Jul  8 23:01:40 2024 ] 	Batch(1700/7879) done. Loss: 0.1015  lr:0.000100
[ Mon Jul  8 23:02:03 2024 ] 	Batch(1800/7879) done. Loss: 0.2384  lr:0.000100
[ Mon Jul  8 23:02:26 2024 ] 	Batch(1900/7879) done. Loss: 0.0328  lr:0.000100
[ Mon Jul  8 23:02:48 2024 ] 
Training: Epoch [71/120], Step [1999], Loss: 0.010955533012747765, Training Accuracy: 94.90625
[ Mon Jul  8 23:02:49 2024 ] 	Batch(2000/7879) done. Loss: 0.0515  lr:0.000100
[ Mon Jul  8 23:03:11 2024 ] 	Batch(2100/7879) done. Loss: 0.0374  lr:0.000100
[ Mon Jul  8 23:03:34 2024 ] 	Batch(2200/7879) done. Loss: 0.0662  lr:0.000100
[ Mon Jul  8 23:03:57 2024 ] 	Batch(2300/7879) done. Loss: 0.2823  lr:0.000100
[ Mon Jul  8 23:04:20 2024 ] 	Batch(2400/7879) done. Loss: 0.3157  lr:0.000100
[ Mon Jul  8 23:04:42 2024 ] 
Training: Epoch [71/120], Step [2499], Loss: 0.04454278200864792, Training Accuracy: 94.94500000000001
[ Mon Jul  8 23:04:42 2024 ] 	Batch(2500/7879) done. Loss: 0.2156  lr:0.000100
[ Mon Jul  8 23:05:05 2024 ] 	Batch(2600/7879) done. Loss: 0.2297  lr:0.000100
[ Mon Jul  8 23:05:28 2024 ] 	Batch(2700/7879) done. Loss: 0.0303  lr:0.000100
[ Mon Jul  8 23:05:50 2024 ] 	Batch(2800/7879) done. Loss: 0.0158  lr:0.000100
[ Mon Jul  8 23:06:13 2024 ] 	Batch(2900/7879) done. Loss: 0.3626  lr:0.000100
[ Mon Jul  8 23:06:36 2024 ] 
Training: Epoch [71/120], Step [2999], Loss: 0.0988038182258606, Training Accuracy: 95.0875
[ Mon Jul  8 23:06:36 2024 ] 	Batch(3000/7879) done. Loss: 0.1347  lr:0.000100
[ Mon Jul  8 23:06:58 2024 ] 	Batch(3100/7879) done. Loss: 0.3048  lr:0.000100
[ Mon Jul  8 23:07:21 2024 ] 	Batch(3200/7879) done. Loss: 0.1922  lr:0.000100
[ Mon Jul  8 23:07:44 2024 ] 	Batch(3300/7879) done. Loss: 0.0114  lr:0.000100
[ Mon Jul  8 23:08:07 2024 ] 	Batch(3400/7879) done. Loss: 0.1119  lr:0.000100
[ Mon Jul  8 23:08:29 2024 ] 
Training: Epoch [71/120], Step [3499], Loss: 0.013299142941832542, Training Accuracy: 95.12857142857143
[ Mon Jul  8 23:08:29 2024 ] 	Batch(3500/7879) done. Loss: 0.1284  lr:0.000100
[ Mon Jul  8 23:08:52 2024 ] 	Batch(3600/7879) done. Loss: 0.2956  lr:0.000100
[ Mon Jul  8 23:09:15 2024 ] 	Batch(3700/7879) done. Loss: 0.0959  lr:0.000100
[ Mon Jul  8 23:09:38 2024 ] 	Batch(3800/7879) done. Loss: 0.2727  lr:0.000100
[ Mon Jul  8 23:10:00 2024 ] 	Batch(3900/7879) done. Loss: 0.4804  lr:0.000100
[ Mon Jul  8 23:10:23 2024 ] 
Training: Epoch [71/120], Step [3999], Loss: 0.3351175785064697, Training Accuracy: 95.159375
[ Mon Jul  8 23:10:23 2024 ] 	Batch(4000/7879) done. Loss: 0.0708  lr:0.000100
[ Mon Jul  8 23:10:46 2024 ] 	Batch(4100/7879) done. Loss: 0.2702  lr:0.000100
[ Mon Jul  8 23:11:09 2024 ] 	Batch(4200/7879) done. Loss: 0.4119  lr:0.000100
[ Mon Jul  8 23:11:31 2024 ] 	Batch(4300/7879) done. Loss: 0.1997  lr:0.000100
[ Mon Jul  8 23:11:54 2024 ] 	Batch(4400/7879) done. Loss: 0.0343  lr:0.000100
[ Mon Jul  8 23:12:17 2024 ] 
Training: Epoch [71/120], Step [4499], Loss: 0.06557008624076843, Training Accuracy: 95.16666666666667
[ Mon Jul  8 23:12:17 2024 ] 	Batch(4500/7879) done. Loss: 0.0478  lr:0.000100
[ Mon Jul  8 23:12:39 2024 ] 	Batch(4600/7879) done. Loss: 0.0439  lr:0.000100
[ Mon Jul  8 23:13:02 2024 ] 	Batch(4700/7879) done. Loss: 0.1673  lr:0.000100
[ Mon Jul  8 23:13:25 2024 ] 	Batch(4800/7879) done. Loss: 0.0209  lr:0.000100
[ Mon Jul  8 23:13:48 2024 ] 	Batch(4900/7879) done. Loss: 0.3465  lr:0.000100
[ Mon Jul  8 23:14:10 2024 ] 
Training: Epoch [71/120], Step [4999], Loss: 0.2147693634033203, Training Accuracy: 95.16250000000001
[ Mon Jul  8 23:14:10 2024 ] 	Batch(5000/7879) done. Loss: 0.5459  lr:0.000100
[ Mon Jul  8 23:14:33 2024 ] 	Batch(5100/7879) done. Loss: 0.0541  lr:0.000100
[ Mon Jul  8 23:14:56 2024 ] 	Batch(5200/7879) done. Loss: 0.4498  lr:0.000100
[ Mon Jul  8 23:15:18 2024 ] 	Batch(5300/7879) done. Loss: 0.3364  lr:0.000100
[ Mon Jul  8 23:15:41 2024 ] 	Batch(5400/7879) done. Loss: 0.1165  lr:0.000100
[ Mon Jul  8 23:16:04 2024 ] 
Training: Epoch [71/120], Step [5499], Loss: 0.03721613809466362, Training Accuracy: 95.20454545454545
[ Mon Jul  8 23:16:04 2024 ] 	Batch(5500/7879) done. Loss: 0.2351  lr:0.000100
[ Mon Jul  8 23:16:27 2024 ] 	Batch(5600/7879) done. Loss: 0.3337  lr:0.000100
[ Mon Jul  8 23:16:50 2024 ] 	Batch(5700/7879) done. Loss: 0.5851  lr:0.000100
[ Mon Jul  8 23:17:13 2024 ] 	Batch(5800/7879) done. Loss: 0.1270  lr:0.000100
[ Mon Jul  8 23:17:37 2024 ] 	Batch(5900/7879) done. Loss: 0.0886  lr:0.000100
[ Mon Jul  8 23:18:00 2024 ] 
Training: Epoch [71/120], Step [5999], Loss: 0.08569314330816269, Training Accuracy: 95.2125
[ Mon Jul  8 23:18:00 2024 ] 	Batch(6000/7879) done. Loss: 0.1775  lr:0.000100
[ Mon Jul  8 23:18:23 2024 ] 	Batch(6100/7879) done. Loss: 0.0514  lr:0.000100
[ Mon Jul  8 23:18:46 2024 ] 	Batch(6200/7879) done. Loss: 0.2075  lr:0.000100
[ Mon Jul  8 23:19:09 2024 ] 	Batch(6300/7879) done. Loss: 0.0666  lr:0.000100
[ Mon Jul  8 23:19:31 2024 ] 	Batch(6400/7879) done. Loss: 0.0711  lr:0.000100
[ Mon Jul  8 23:19:55 2024 ] 
Training: Epoch [71/120], Step [6499], Loss: 0.12934155762195587, Training Accuracy: 95.1923076923077
[ Mon Jul  8 23:19:55 2024 ] 	Batch(6500/7879) done. Loss: 0.2548  lr:0.000100
[ Mon Jul  8 23:20:18 2024 ] 	Batch(6600/7879) done. Loss: 0.0141  lr:0.000100
[ Mon Jul  8 23:20:42 2024 ] 	Batch(6700/7879) done. Loss: 0.2484  lr:0.000100
[ Mon Jul  8 23:21:05 2024 ] 	Batch(6800/7879) done. Loss: 0.1995  lr:0.000100
[ Mon Jul  8 23:21:28 2024 ] 	Batch(6900/7879) done. Loss: 0.0367  lr:0.000100
[ Mon Jul  8 23:21:51 2024 ] 
Training: Epoch [71/120], Step [6999], Loss: 0.4306630492210388, Training Accuracy: 95.16607142857143
[ Mon Jul  8 23:21:51 2024 ] 	Batch(7000/7879) done. Loss: 0.2337  lr:0.000100
[ Mon Jul  8 23:22:14 2024 ] 	Batch(7100/7879) done. Loss: 0.0573  lr:0.000100
[ Mon Jul  8 23:22:37 2024 ] 	Batch(7200/7879) done. Loss: 0.4091  lr:0.000100
[ Mon Jul  8 23:23:00 2024 ] 	Batch(7300/7879) done. Loss: 0.1225  lr:0.000100
[ Mon Jul  8 23:23:22 2024 ] 	Batch(7400/7879) done. Loss: 0.9010  lr:0.000100
[ Mon Jul  8 23:23:45 2024 ] 
Training: Epoch [71/120], Step [7499], Loss: 0.02385827526450157, Training Accuracy: 95.14
[ Mon Jul  8 23:23:45 2024 ] 	Batch(7500/7879) done. Loss: 0.0329  lr:0.000100
[ Mon Jul  8 23:24:08 2024 ] 	Batch(7600/7879) done. Loss: 0.3766  lr:0.000100
[ Mon Jul  8 23:24:31 2024 ] 	Batch(7700/7879) done. Loss: 0.0985  lr:0.000100
[ Mon Jul  8 23:24:53 2024 ] 	Batch(7800/7879) done. Loss: 0.0608  lr:0.000100
[ Mon Jul  8 23:25:11 2024 ] 	Mean training loss: 0.1770.
[ Mon Jul  8 23:25:11 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 23:25:11 2024 ] Training epoch: 73
[ Mon Jul  8 23:25:12 2024 ] 	Batch(0/7879) done. Loss: 0.2759  lr:0.000100
[ Mon Jul  8 23:25:35 2024 ] 	Batch(100/7879) done. Loss: 0.0175  lr:0.000100
[ Mon Jul  8 23:25:58 2024 ] 	Batch(200/7879) done. Loss: 0.1616  lr:0.000100
[ Mon Jul  8 23:26:22 2024 ] 	Batch(300/7879) done. Loss: 0.0089  lr:0.000100
[ Mon Jul  8 23:26:45 2024 ] 	Batch(400/7879) done. Loss: 0.0103  lr:0.000100
[ Mon Jul  8 23:27:09 2024 ] 
Training: Epoch [72/120], Step [499], Loss: 0.1592826247215271, Training Accuracy: 94.575
[ Mon Jul  8 23:27:09 2024 ] 	Batch(500/7879) done. Loss: 0.0095  lr:0.000100
[ Mon Jul  8 23:27:32 2024 ] 	Batch(600/7879) done. Loss: 0.2069  lr:0.000100
[ Mon Jul  8 23:27:56 2024 ] 	Batch(700/7879) done. Loss: 0.2078  lr:0.000100
[ Mon Jul  8 23:28:19 2024 ] 	Batch(800/7879) done. Loss: 0.2602  lr:0.000100
[ Mon Jul  8 23:28:43 2024 ] 	Batch(900/7879) done. Loss: 0.1062  lr:0.000100
[ Mon Jul  8 23:29:06 2024 ] 
Training: Epoch [72/120], Step [999], Loss: 0.21382282674312592, Training Accuracy: 94.925
[ Mon Jul  8 23:29:06 2024 ] 	Batch(1000/7879) done. Loss: 0.1950  lr:0.000100
[ Mon Jul  8 23:29:30 2024 ] 	Batch(1100/7879) done. Loss: 0.1122  lr:0.000100
[ Mon Jul  8 23:29:53 2024 ] 	Batch(1200/7879) done. Loss: 0.0446  lr:0.000100
[ Mon Jul  8 23:30:16 2024 ] 	Batch(1300/7879) done. Loss: 0.0246  lr:0.000100
[ Mon Jul  8 23:30:38 2024 ] 	Batch(1400/7879) done. Loss: 0.7280  lr:0.000100
[ Mon Jul  8 23:31:01 2024 ] 
Training: Epoch [72/120], Step [1499], Loss: 0.2112278938293457, Training Accuracy: 95.24166666666667
[ Mon Jul  8 23:31:01 2024 ] 	Batch(1500/7879) done. Loss: 0.1939  lr:0.000100
[ Mon Jul  8 23:31:24 2024 ] 	Batch(1600/7879) done. Loss: 0.0578  lr:0.000100
[ Mon Jul  8 23:31:47 2024 ] 	Batch(1700/7879) done. Loss: 0.0846  lr:0.000100
[ Mon Jul  8 23:32:11 2024 ] 	Batch(1800/7879) done. Loss: 0.1214  lr:0.000100
[ Mon Jul  8 23:32:34 2024 ] 	Batch(1900/7879) done. Loss: 0.0534  lr:0.000100
[ Mon Jul  8 23:32:57 2024 ] 
Training: Epoch [72/120], Step [1999], Loss: 0.25978773832321167, Training Accuracy: 95.3125
[ Mon Jul  8 23:32:58 2024 ] 	Batch(2000/7879) done. Loss: 0.0082  lr:0.000100
[ Mon Jul  8 23:33:21 2024 ] 	Batch(2100/7879) done. Loss: 0.1127  lr:0.000100
[ Mon Jul  8 23:33:43 2024 ] 	Batch(2200/7879) done. Loss: 0.0116  lr:0.000100
[ Mon Jul  8 23:34:06 2024 ] 	Batch(2300/7879) done. Loss: 0.4574  lr:0.000100
[ Mon Jul  8 23:34:29 2024 ] 	Batch(2400/7879) done. Loss: 0.0122  lr:0.000100
[ Mon Jul  8 23:34:51 2024 ] 
Training: Epoch [72/120], Step [2499], Loss: 0.1285640448331833, Training Accuracy: 95.34
[ Mon Jul  8 23:34:51 2024 ] 	Batch(2500/7879) done. Loss: 0.5543  lr:0.000100
[ Mon Jul  8 23:35:14 2024 ] 	Batch(2600/7879) done. Loss: 0.0337  lr:0.000100
[ Mon Jul  8 23:35:37 2024 ] 	Batch(2700/7879) done. Loss: 0.0222  lr:0.000100
[ Mon Jul  8 23:36:00 2024 ] 	Batch(2800/7879) done. Loss: 0.4017  lr:0.000100
[ Mon Jul  8 23:36:22 2024 ] 	Batch(2900/7879) done. Loss: 0.5142  lr:0.000100
[ Mon Jul  8 23:36:45 2024 ] 
Training: Epoch [72/120], Step [2999], Loss: 0.02750326506793499, Training Accuracy: 95.33333333333334
[ Mon Jul  8 23:36:45 2024 ] 	Batch(3000/7879) done. Loss: 0.1552  lr:0.000100
[ Mon Jul  8 23:37:08 2024 ] 	Batch(3100/7879) done. Loss: 0.0292  lr:0.000100
[ Mon Jul  8 23:37:31 2024 ] 	Batch(3200/7879) done. Loss: 0.1946  lr:0.000100
[ Mon Jul  8 23:37:54 2024 ] 	Batch(3300/7879) done. Loss: 0.1816  lr:0.000100
[ Mon Jul  8 23:38:16 2024 ] 	Batch(3400/7879) done. Loss: 0.1130  lr:0.000100
[ Mon Jul  8 23:38:39 2024 ] 
Training: Epoch [72/120], Step [3499], Loss: 0.0859137549996376, Training Accuracy: 95.39642857142857
[ Mon Jul  8 23:38:39 2024 ] 	Batch(3500/7879) done. Loss: 0.0687  lr:0.000100
[ Mon Jul  8 23:39:02 2024 ] 	Batch(3600/7879) done. Loss: 0.1431  lr:0.000100
[ Mon Jul  8 23:39:25 2024 ] 	Batch(3700/7879) done. Loss: 0.2001  lr:0.000100
[ Mon Jul  8 23:39:49 2024 ] 	Batch(3800/7879) done. Loss: 0.1690  lr:0.000100
[ Mon Jul  8 23:40:12 2024 ] 	Batch(3900/7879) done. Loss: 0.0209  lr:0.000100
[ Mon Jul  8 23:40:35 2024 ] 
Training: Epoch [72/120], Step [3999], Loss: 0.1569252908229828, Training Accuracy: 95.34375
[ Mon Jul  8 23:40:36 2024 ] 	Batch(4000/7879) done. Loss: 0.0865  lr:0.000100
[ Mon Jul  8 23:40:59 2024 ] 	Batch(4100/7879) done. Loss: 0.2337  lr:0.000100
[ Mon Jul  8 23:41:22 2024 ] 	Batch(4200/7879) done. Loss: 0.0030  lr:0.000100
[ Mon Jul  8 23:41:46 2024 ] 	Batch(4300/7879) done. Loss: 0.0904  lr:0.000100
[ Mon Jul  8 23:42:10 2024 ] 	Batch(4400/7879) done. Loss: 0.0194  lr:0.000100
[ Mon Jul  8 23:42:32 2024 ] 
Training: Epoch [72/120], Step [4499], Loss: 0.18955391645431519, Training Accuracy: 95.40833333333333
[ Mon Jul  8 23:42:33 2024 ] 	Batch(4500/7879) done. Loss: 0.4784  lr:0.000100
[ Mon Jul  8 23:42:55 2024 ] 	Batch(4600/7879) done. Loss: 0.0878  lr:0.000100
[ Mon Jul  8 23:43:18 2024 ] 	Batch(4700/7879) done. Loss: 0.0618  lr:0.000100
[ Mon Jul  8 23:43:41 2024 ] 	Batch(4800/7879) done. Loss: 0.0671  lr:0.000100
[ Mon Jul  8 23:44:04 2024 ] 	Batch(4900/7879) done. Loss: 0.4049  lr:0.000100
[ Mon Jul  8 23:44:27 2024 ] 
Training: Epoch [72/120], Step [4999], Loss: 0.07229186594486237, Training Accuracy: 95.42
[ Mon Jul  8 23:44:27 2024 ] 	Batch(5000/7879) done. Loss: 0.0650  lr:0.000100
[ Mon Jul  8 23:44:50 2024 ] 	Batch(5100/7879) done. Loss: 0.0156  lr:0.000100
[ Mon Jul  8 23:45:13 2024 ] 	Batch(5200/7879) done. Loss: 0.0343  lr:0.000100
[ Mon Jul  8 23:45:36 2024 ] 	Batch(5300/7879) done. Loss: 0.0017  lr:0.000100
[ Mon Jul  8 23:46:00 2024 ] 	Batch(5400/7879) done. Loss: 0.0769  lr:0.000100
[ Mon Jul  8 23:46:23 2024 ] 
Training: Epoch [72/120], Step [5499], Loss: 0.10540767014026642, Training Accuracy: 95.4659090909091
[ Mon Jul  8 23:46:23 2024 ] 	Batch(5500/7879) done. Loss: 0.2025  lr:0.000100
[ Mon Jul  8 23:46:47 2024 ] 	Batch(5600/7879) done. Loss: 0.0592  lr:0.000100
[ Mon Jul  8 23:47:10 2024 ] 	Batch(5700/7879) done. Loss: 0.1453  lr:0.000100
[ Mon Jul  8 23:47:32 2024 ] 	Batch(5800/7879) done. Loss: 0.2981  lr:0.000100
[ Mon Jul  8 23:47:55 2024 ] 	Batch(5900/7879) done. Loss: 0.0073  lr:0.000100
[ Mon Jul  8 23:48:18 2024 ] 
Training: Epoch [72/120], Step [5999], Loss: 0.07746827602386475, Training Accuracy: 95.45208333333333
[ Mon Jul  8 23:48:18 2024 ] 	Batch(6000/7879) done. Loss: 0.3147  lr:0.000100
[ Mon Jul  8 23:48:41 2024 ] 	Batch(6100/7879) done. Loss: 0.0670  lr:0.000100
[ Mon Jul  8 23:49:03 2024 ] 	Batch(6200/7879) done. Loss: 0.0327  lr:0.000100
[ Mon Jul  8 23:49:26 2024 ] 	Batch(6300/7879) done. Loss: 0.1688  lr:0.000100
[ Mon Jul  8 23:49:49 2024 ] 	Batch(6400/7879) done. Loss: 0.1212  lr:0.000100
[ Mon Jul  8 23:50:11 2024 ] 
Training: Epoch [72/120], Step [6499], Loss: 0.2970156669616699, Training Accuracy: 95.39230769230768
[ Mon Jul  8 23:50:12 2024 ] 	Batch(6500/7879) done. Loss: 0.0117  lr:0.000100
[ Mon Jul  8 23:50:34 2024 ] 	Batch(6600/7879) done. Loss: 0.0617  lr:0.000100
[ Mon Jul  8 23:50:57 2024 ] 	Batch(6700/7879) done. Loss: 0.3990  lr:0.000100
[ Mon Jul  8 23:51:20 2024 ] 	Batch(6800/7879) done. Loss: 0.0188  lr:0.000100
[ Mon Jul  8 23:51:43 2024 ] 	Batch(6900/7879) done. Loss: 0.0414  lr:0.000100
[ Mon Jul  8 23:52:06 2024 ] 
Training: Epoch [72/120], Step [6999], Loss: 0.4965362250804901, Training Accuracy: 95.36607142857143
[ Mon Jul  8 23:52:06 2024 ] 	Batch(7000/7879) done. Loss: 0.0109  lr:0.000100
[ Mon Jul  8 23:52:29 2024 ] 	Batch(7100/7879) done. Loss: 0.0180  lr:0.000100
[ Mon Jul  8 23:52:51 2024 ] 	Batch(7200/7879) done. Loss: 0.2155  lr:0.000100
[ Mon Jul  8 23:53:14 2024 ] 	Batch(7300/7879) done. Loss: 0.0752  lr:0.000100
[ Mon Jul  8 23:53:37 2024 ] 	Batch(7400/7879) done. Loss: 0.1649  lr:0.000100
[ Mon Jul  8 23:53:59 2024 ] 
Training: Epoch [72/120], Step [7499], Loss: 0.01593390665948391, Training Accuracy: 95.38
[ Mon Jul  8 23:54:00 2024 ] 	Batch(7500/7879) done. Loss: 0.2325  lr:0.000100
[ Mon Jul  8 23:54:22 2024 ] 	Batch(7600/7879) done. Loss: 0.0010  lr:0.000100
[ Mon Jul  8 23:54:45 2024 ] 	Batch(7700/7879) done. Loss: 0.1265  lr:0.000100
[ Mon Jul  8 23:55:08 2024 ] 	Batch(7800/7879) done. Loss: 0.1379  lr:0.000100
[ Mon Jul  8 23:55:25 2024 ] 	Mean training loss: 0.1700.
[ Mon Jul  8 23:55:25 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul  8 23:55:26 2024 ] Training epoch: 74
[ Mon Jul  8 23:55:26 2024 ] 	Batch(0/7879) done. Loss: 0.0569  lr:0.000100
[ Mon Jul  8 23:55:49 2024 ] 	Batch(100/7879) done. Loss: 0.9723  lr:0.000100
[ Mon Jul  8 23:56:12 2024 ] 	Batch(200/7879) done. Loss: 0.0150  lr:0.000100
[ Mon Jul  8 23:56:34 2024 ] 	Batch(300/7879) done. Loss: 0.1033  lr:0.000100
[ Mon Jul  8 23:56:57 2024 ] 	Batch(400/7879) done. Loss: 0.0553  lr:0.000100
[ Mon Jul  8 23:57:19 2024 ] 
Training: Epoch [73/120], Step [499], Loss: 0.20505179464817047, Training Accuracy: 95.375
[ Mon Jul  8 23:57:20 2024 ] 	Batch(500/7879) done. Loss: 0.1427  lr:0.000100
[ Mon Jul  8 23:57:42 2024 ] 	Batch(600/7879) done. Loss: 0.4425  lr:0.000100
[ Mon Jul  8 23:58:05 2024 ] 	Batch(700/7879) done. Loss: 0.2231  lr:0.000100
[ Mon Jul  8 23:58:28 2024 ] 	Batch(800/7879) done. Loss: 0.0443  lr:0.000100
[ Mon Jul  8 23:58:52 2024 ] 	Batch(900/7879) done. Loss: 0.0163  lr:0.000100
[ Mon Jul  8 23:59:15 2024 ] 
Training: Epoch [73/120], Step [999], Loss: 0.10806015133857727, Training Accuracy: 95.1875
[ Mon Jul  8 23:59:15 2024 ] 	Batch(1000/7879) done. Loss: 0.0730  lr:0.000100
[ Mon Jul  8 23:59:39 2024 ] 	Batch(1100/7879) done. Loss: 0.3902  lr:0.000100
[ Tue Jul  9 00:00:02 2024 ] 	Batch(1200/7879) done. Loss: 0.2683  lr:0.000100
[ Tue Jul  9 00:00:25 2024 ] 	Batch(1300/7879) done. Loss: 0.0218  lr:0.000100
[ Tue Jul  9 00:00:48 2024 ] 	Batch(1400/7879) done. Loss: 0.0444  lr:0.000100
[ Tue Jul  9 00:01:10 2024 ] 
Training: Epoch [73/120], Step [1499], Loss: 0.021868709474802017, Training Accuracy: 95.275
[ Tue Jul  9 00:01:10 2024 ] 	Batch(1500/7879) done. Loss: 0.1779  lr:0.000100
[ Tue Jul  9 00:01:33 2024 ] 	Batch(1600/7879) done. Loss: 0.0194  lr:0.000100
[ Tue Jul  9 00:01:56 2024 ] 	Batch(1700/7879) done. Loss: 0.1637  lr:0.000100
[ Tue Jul  9 00:02:18 2024 ] 	Batch(1800/7879) done. Loss: 0.5114  lr:0.000100
[ Tue Jul  9 00:02:41 2024 ] 	Batch(1900/7879) done. Loss: 0.1657  lr:0.000100
[ Tue Jul  9 00:03:04 2024 ] 
Training: Epoch [73/120], Step [1999], Loss: 0.014127422124147415, Training Accuracy: 95.08125
[ Tue Jul  9 00:03:04 2024 ] 	Batch(2000/7879) done. Loss: 0.1645  lr:0.000100
[ Tue Jul  9 00:03:27 2024 ] 	Batch(2100/7879) done. Loss: 0.5271  lr:0.000100
[ Tue Jul  9 00:03:51 2024 ] 	Batch(2200/7879) done. Loss: 0.2340  lr:0.000100
[ Tue Jul  9 00:04:14 2024 ] 	Batch(2300/7879) done. Loss: 0.0915  lr:0.000100
[ Tue Jul  9 00:04:38 2024 ] 	Batch(2400/7879) done. Loss: 0.4922  lr:0.000100
[ Tue Jul  9 00:05:01 2024 ] 
Training: Epoch [73/120], Step [2499], Loss: 0.06795620918273926, Training Accuracy: 95.14
[ Tue Jul  9 00:05:01 2024 ] 	Batch(2500/7879) done. Loss: 0.2851  lr:0.000100
[ Tue Jul  9 00:05:25 2024 ] 	Batch(2600/7879) done. Loss: 0.0238  lr:0.000100
[ Tue Jul  9 00:05:48 2024 ] 	Batch(2700/7879) done. Loss: 0.0680  lr:0.000100
[ Tue Jul  9 00:06:11 2024 ] 	Batch(2800/7879) done. Loss: 0.0241  lr:0.000100
[ Tue Jul  9 00:06:34 2024 ] 	Batch(2900/7879) done. Loss: 0.3909  lr:0.000100
[ Tue Jul  9 00:06:56 2024 ] 
Training: Epoch [73/120], Step [2999], Loss: 0.04324089735746384, Training Accuracy: 95.18333333333334
[ Tue Jul  9 00:06:57 2024 ] 	Batch(3000/7879) done. Loss: 0.0141  lr:0.000100
[ Tue Jul  9 00:07:19 2024 ] 	Batch(3100/7879) done. Loss: 0.1503  lr:0.000100
[ Tue Jul  9 00:07:42 2024 ] 	Batch(3200/7879) done. Loss: 0.3448  lr:0.000100
[ Tue Jul  9 00:08:06 2024 ] 	Batch(3300/7879) done. Loss: 0.1021  lr:0.000100
[ Tue Jul  9 00:08:29 2024 ] 	Batch(3400/7879) done. Loss: 0.2296  lr:0.000100
[ Tue Jul  9 00:08:52 2024 ] 
Training: Epoch [73/120], Step [3499], Loss: 0.37982988357543945, Training Accuracy: 95.16428571428571
[ Tue Jul  9 00:08:52 2024 ] 	Batch(3500/7879) done. Loss: 0.5510  lr:0.000100
[ Tue Jul  9 00:09:16 2024 ] 	Batch(3600/7879) done. Loss: 0.2749  lr:0.000100
[ Tue Jul  9 00:09:39 2024 ] 	Batch(3700/7879) done. Loss: 0.0950  lr:0.000100
[ Tue Jul  9 00:10:03 2024 ] 	Batch(3800/7879) done. Loss: 0.1047  lr:0.000100
[ Tue Jul  9 00:10:26 2024 ] 	Batch(3900/7879) done. Loss: 0.0975  lr:0.000100
[ Tue Jul  9 00:10:48 2024 ] 
Training: Epoch [73/120], Step [3999], Loss: 0.09749548137187958, Training Accuracy: 95.19375
[ Tue Jul  9 00:10:49 2024 ] 	Batch(4000/7879) done. Loss: 0.0774  lr:0.000100
[ Tue Jul  9 00:11:11 2024 ] 	Batch(4100/7879) done. Loss: 0.1586  lr:0.000100
[ Tue Jul  9 00:11:34 2024 ] 	Batch(4200/7879) done. Loss: 0.1308  lr:0.000100
[ Tue Jul  9 00:11:57 2024 ] 	Batch(4300/7879) done. Loss: 0.0529  lr:0.000100
[ Tue Jul  9 00:12:20 2024 ] 	Batch(4400/7879) done. Loss: 0.3861  lr:0.000100
[ Tue Jul  9 00:12:42 2024 ] 
Training: Epoch [73/120], Step [4499], Loss: 0.19022315740585327, Training Accuracy: 95.20555555555555
[ Tue Jul  9 00:12:42 2024 ] 	Batch(4500/7879) done. Loss: 0.1255  lr:0.000100
[ Tue Jul  9 00:13:05 2024 ] 	Batch(4600/7879) done. Loss: 0.0424  lr:0.000100
[ Tue Jul  9 00:13:28 2024 ] 	Batch(4700/7879) done. Loss: 0.0201  lr:0.000100
[ Tue Jul  9 00:13:50 2024 ] 	Batch(4800/7879) done. Loss: 0.1595  lr:0.000100
[ Tue Jul  9 00:14:13 2024 ] 	Batch(4900/7879) done. Loss: 0.1191  lr:0.000100
[ Tue Jul  9 00:14:36 2024 ] 
Training: Epoch [73/120], Step [4999], Loss: 0.018111353740096092, Training Accuracy: 95.2475
[ Tue Jul  9 00:14:36 2024 ] 	Batch(5000/7879) done. Loss: 0.1597  lr:0.000100
[ Tue Jul  9 00:14:59 2024 ] 	Batch(5100/7879) done. Loss: 0.2007  lr:0.000100
[ Tue Jul  9 00:15:22 2024 ] 	Batch(5200/7879) done. Loss: 0.0425  lr:0.000100
[ Tue Jul  9 00:15:45 2024 ] 	Batch(5300/7879) done. Loss: 0.8462  lr:0.000100
[ Tue Jul  9 00:16:07 2024 ] 	Batch(5400/7879) done. Loss: 0.0205  lr:0.000100
[ Tue Jul  9 00:16:30 2024 ] 
Training: Epoch [73/120], Step [5499], Loss: 0.03512249141931534, Training Accuracy: 95.24772727272726
[ Tue Jul  9 00:16:30 2024 ] 	Batch(5500/7879) done. Loss: 0.2186  lr:0.000100
[ Tue Jul  9 00:16:53 2024 ] 	Batch(5600/7879) done. Loss: 0.0819  lr:0.000100
[ Tue Jul  9 00:17:16 2024 ] 	Batch(5700/7879) done. Loss: 0.0376  lr:0.000100
[ Tue Jul  9 00:17:39 2024 ] 	Batch(5800/7879) done. Loss: 0.0692  lr:0.000100
[ Tue Jul  9 00:18:01 2024 ] 	Batch(5900/7879) done. Loss: 0.0201  lr:0.000100
[ Tue Jul  9 00:18:24 2024 ] 
Training: Epoch [73/120], Step [5999], Loss: 0.06444617360830307, Training Accuracy: 95.30208333333333
[ Tue Jul  9 00:18:24 2024 ] 	Batch(6000/7879) done. Loss: 0.0396  lr:0.000100
[ Tue Jul  9 00:18:47 2024 ] 	Batch(6100/7879) done. Loss: 0.0500  lr:0.000100
[ Tue Jul  9 00:19:10 2024 ] 	Batch(6200/7879) done. Loss: 0.0122  lr:0.000100
[ Tue Jul  9 00:19:32 2024 ] 	Batch(6300/7879) done. Loss: 0.4809  lr:0.000100
[ Tue Jul  9 00:19:56 2024 ] 	Batch(6400/7879) done. Loss: 0.2579  lr:0.000100
[ Tue Jul  9 00:20:18 2024 ] 
Training: Epoch [73/120], Step [6499], Loss: 0.11851301789283752, Training Accuracy: 95.32692307692308
[ Tue Jul  9 00:20:18 2024 ] 	Batch(6500/7879) done. Loss: 0.0175  lr:0.000100
[ Tue Jul  9 00:20:41 2024 ] 	Batch(6600/7879) done. Loss: 0.2205  lr:0.000100
[ Tue Jul  9 00:21:04 2024 ] 	Batch(6700/7879) done. Loss: 0.1416  lr:0.000100
[ Tue Jul  9 00:21:26 2024 ] 	Batch(6800/7879) done. Loss: 0.0851  lr:0.000100
[ Tue Jul  9 00:21:49 2024 ] 	Batch(6900/7879) done. Loss: 0.1184  lr:0.000100
[ Tue Jul  9 00:22:12 2024 ] 
Training: Epoch [73/120], Step [6999], Loss: 0.3086801767349243, Training Accuracy: 95.3267857142857
[ Tue Jul  9 00:22:12 2024 ] 	Batch(7000/7879) done. Loss: 0.2563  lr:0.000100
[ Tue Jul  9 00:22:35 2024 ] 	Batch(7100/7879) done. Loss: 0.0493  lr:0.000100
[ Tue Jul  9 00:22:57 2024 ] 	Batch(7200/7879) done. Loss: 0.4993  lr:0.000100
[ Tue Jul  9 00:23:20 2024 ] 	Batch(7300/7879) done. Loss: 0.2865  lr:0.000100
[ Tue Jul  9 00:23:43 2024 ] 	Batch(7400/7879) done. Loss: 0.0557  lr:0.000100
[ Tue Jul  9 00:24:05 2024 ] 
Training: Epoch [73/120], Step [7499], Loss: 0.14412039518356323, Training Accuracy: 95.31500000000001
[ Tue Jul  9 00:24:05 2024 ] 	Batch(7500/7879) done. Loss: 0.3578  lr:0.000100
[ Tue Jul  9 00:24:28 2024 ] 	Batch(7600/7879) done. Loss: 0.2354  lr:0.000100
[ Tue Jul  9 00:24:51 2024 ] 	Batch(7700/7879) done. Loss: 0.0932  lr:0.000100
[ Tue Jul  9 00:25:14 2024 ] 	Batch(7800/7879) done. Loss: 0.0865  lr:0.000100
[ Tue Jul  9 00:25:31 2024 ] 	Mean training loss: 0.1730.
[ Tue Jul  9 00:25:31 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 00:25:31 2024 ] Training epoch: 75
[ Tue Jul  9 00:25:32 2024 ] 	Batch(0/7879) done. Loss: 0.0446  lr:0.000100
[ Tue Jul  9 00:25:55 2024 ] 	Batch(100/7879) done. Loss: 0.0133  lr:0.000100
[ Tue Jul  9 00:26:17 2024 ] 	Batch(200/7879) done. Loss: 0.0117  lr:0.000100
[ Tue Jul  9 00:26:40 2024 ] 	Batch(300/7879) done. Loss: 0.0727  lr:0.000100
[ Tue Jul  9 00:27:03 2024 ] 	Batch(400/7879) done. Loss: 0.3543  lr:0.000100
[ Tue Jul  9 00:27:25 2024 ] 
Training: Epoch [74/120], Step [499], Loss: 0.33749091625213623, Training Accuracy: 95.72500000000001
[ Tue Jul  9 00:27:26 2024 ] 	Batch(500/7879) done. Loss: 0.1329  lr:0.000100
[ Tue Jul  9 00:27:48 2024 ] 	Batch(600/7879) done. Loss: 0.2104  lr:0.000100
[ Tue Jul  9 00:28:11 2024 ] 	Batch(700/7879) done. Loss: 0.0202  lr:0.000100
[ Tue Jul  9 00:28:34 2024 ] 	Batch(800/7879) done. Loss: 0.2434  lr:0.000100
[ Tue Jul  9 00:28:57 2024 ] 	Batch(900/7879) done. Loss: 0.0709  lr:0.000100
[ Tue Jul  9 00:29:19 2024 ] 
Training: Epoch [74/120], Step [999], Loss: 0.45433783531188965, Training Accuracy: 95.6125
[ Tue Jul  9 00:29:19 2024 ] 	Batch(1000/7879) done. Loss: 0.1068  lr:0.000100
[ Tue Jul  9 00:29:42 2024 ] 	Batch(1100/7879) done. Loss: 0.2804  lr:0.000100
[ Tue Jul  9 00:30:05 2024 ] 	Batch(1200/7879) done. Loss: 0.3387  lr:0.000100
[ Tue Jul  9 00:30:28 2024 ] 	Batch(1300/7879) done. Loss: 0.5203  lr:0.000100
[ Tue Jul  9 00:30:50 2024 ] 	Batch(1400/7879) done. Loss: 0.0506  lr:0.000100
[ Tue Jul  9 00:31:13 2024 ] 
Training: Epoch [74/120], Step [1499], Loss: 0.8130292892456055, Training Accuracy: 95.61666666666667
[ Tue Jul  9 00:31:13 2024 ] 	Batch(1500/7879) done. Loss: 0.0657  lr:0.000100
[ Tue Jul  9 00:31:36 2024 ] 	Batch(1600/7879) done. Loss: 0.1682  lr:0.000100
[ Tue Jul  9 00:31:58 2024 ] 	Batch(1700/7879) done. Loss: 0.4684  lr:0.000100
[ Tue Jul  9 00:32:21 2024 ] 	Batch(1800/7879) done. Loss: 0.0266  lr:0.000100
[ Tue Jul  9 00:32:44 2024 ] 	Batch(1900/7879) done. Loss: 0.0512  lr:0.000100
[ Tue Jul  9 00:33:06 2024 ] 
Training: Epoch [74/120], Step [1999], Loss: 0.03473050147294998, Training Accuracy: 95.575
[ Tue Jul  9 00:33:07 2024 ] 	Batch(2000/7879) done. Loss: 0.0750  lr:0.000100
[ Tue Jul  9 00:33:29 2024 ] 	Batch(2100/7879) done. Loss: 0.0532  lr:0.000100
[ Tue Jul  9 00:33:52 2024 ] 	Batch(2200/7879) done. Loss: 0.0220  lr:0.000100
[ Tue Jul  9 00:34:15 2024 ] 	Batch(2300/7879) done. Loss: 0.0108  lr:0.000100
[ Tue Jul  9 00:34:38 2024 ] 	Batch(2400/7879) done. Loss: 0.0586  lr:0.000100
[ Tue Jul  9 00:35:00 2024 ] 
Training: Epoch [74/120], Step [2499], Loss: 0.019654124975204468, Training Accuracy: 95.53
[ Tue Jul  9 00:35:01 2024 ] 	Batch(2500/7879) done. Loss: 0.0023  lr:0.000100
[ Tue Jul  9 00:35:23 2024 ] 	Batch(2600/7879) done. Loss: 0.0439  lr:0.000100
[ Tue Jul  9 00:35:46 2024 ] 	Batch(2700/7879) done. Loss: 0.3776  lr:0.000100
[ Tue Jul  9 00:36:09 2024 ] 	Batch(2800/7879) done. Loss: 0.0209  lr:0.000100
[ Tue Jul  9 00:36:31 2024 ] 	Batch(2900/7879) done. Loss: 0.0463  lr:0.000100
[ Tue Jul  9 00:36:54 2024 ] 
Training: Epoch [74/120], Step [2999], Loss: 0.1264295130968094, Training Accuracy: 95.5625
[ Tue Jul  9 00:36:54 2024 ] 	Batch(3000/7879) done. Loss: 0.0190  lr:0.000100
[ Tue Jul  9 00:37:17 2024 ] 	Batch(3100/7879) done. Loss: 0.1279  lr:0.000100
[ Tue Jul  9 00:37:40 2024 ] 	Batch(3200/7879) done. Loss: 0.0711  lr:0.000100
[ Tue Jul  9 00:38:02 2024 ] 	Batch(3300/7879) done. Loss: 0.1806  lr:0.000100
[ Tue Jul  9 00:38:25 2024 ] 	Batch(3400/7879) done. Loss: 0.0537  lr:0.000100
[ Tue Jul  9 00:38:47 2024 ] 
Training: Epoch [74/120], Step [3499], Loss: 0.24739238619804382, Training Accuracy: 95.55357142857143
[ Tue Jul  9 00:38:48 2024 ] 	Batch(3500/7879) done. Loss: 0.8986  lr:0.000100
[ Tue Jul  9 00:39:10 2024 ] 	Batch(3600/7879) done. Loss: 0.2672  lr:0.000100
[ Tue Jul  9 00:39:33 2024 ] 	Batch(3700/7879) done. Loss: 0.0284  lr:0.000100
[ Tue Jul  9 00:39:56 2024 ] 	Batch(3800/7879) done. Loss: 0.0084  lr:0.000100
[ Tue Jul  9 00:40:18 2024 ] 	Batch(3900/7879) done. Loss: 0.4325  lr:0.000100
[ Tue Jul  9 00:40:41 2024 ] 
Training: Epoch [74/120], Step [3999], Loss: 0.16807147860527039, Training Accuracy: 95.521875
[ Tue Jul  9 00:40:41 2024 ] 	Batch(4000/7879) done. Loss: 0.0758  lr:0.000100
[ Tue Jul  9 00:41:05 2024 ] 	Batch(4100/7879) done. Loss: 0.1494  lr:0.000100
[ Tue Jul  9 00:41:28 2024 ] 	Batch(4200/7879) done. Loss: 0.0793  lr:0.000100
[ Tue Jul  9 00:41:52 2024 ] 	Batch(4300/7879) done. Loss: 0.1374  lr:0.000100
[ Tue Jul  9 00:42:15 2024 ] 	Batch(4400/7879) done. Loss: 0.6321  lr:0.000100
[ Tue Jul  9 00:42:37 2024 ] 
Training: Epoch [74/120], Step [4499], Loss: 0.2154645174741745, Training Accuracy: 95.475
[ Tue Jul  9 00:42:37 2024 ] 	Batch(4500/7879) done. Loss: 0.0079  lr:0.000100
[ Tue Jul  9 00:43:00 2024 ] 	Batch(4600/7879) done. Loss: 0.0168  lr:0.000100
[ Tue Jul  9 00:43:23 2024 ] 	Batch(4700/7879) done. Loss: 0.0360  lr:0.000100
[ Tue Jul  9 00:43:46 2024 ] 	Batch(4800/7879) done. Loss: 0.1196  lr:0.000100
[ Tue Jul  9 00:44:09 2024 ] 	Batch(4900/7879) done. Loss: 0.1012  lr:0.000100
[ Tue Jul  9 00:44:31 2024 ] 
Training: Epoch [74/120], Step [4999], Loss: 0.1891232430934906, Training Accuracy: 95.4775
[ Tue Jul  9 00:44:31 2024 ] 	Batch(5000/7879) done. Loss: 0.4084  lr:0.000100
[ Tue Jul  9 00:44:54 2024 ] 	Batch(5100/7879) done. Loss: 0.5153  lr:0.000100
[ Tue Jul  9 00:45:17 2024 ] 	Batch(5200/7879) done. Loss: 0.1480  lr:0.000100
[ Tue Jul  9 00:45:39 2024 ] 	Batch(5300/7879) done. Loss: 0.0814  lr:0.000100
[ Tue Jul  9 00:46:02 2024 ] 	Batch(5400/7879) done. Loss: 0.2201  lr:0.000100
[ Tue Jul  9 00:46:25 2024 ] 
Training: Epoch [74/120], Step [5499], Loss: 0.7042067050933838, Training Accuracy: 95.50681818181819
[ Tue Jul  9 00:46:25 2024 ] 	Batch(5500/7879) done. Loss: 0.0278  lr:0.000100
[ Tue Jul  9 00:46:48 2024 ] 	Batch(5600/7879) done. Loss: 0.0164  lr:0.000100
[ Tue Jul  9 00:47:10 2024 ] 	Batch(5700/7879) done. Loss: 0.2249  lr:0.000100
[ Tue Jul  9 00:47:33 2024 ] 	Batch(5800/7879) done. Loss: 0.0282  lr:0.000100
[ Tue Jul  9 00:47:56 2024 ] 	Batch(5900/7879) done. Loss: 0.0382  lr:0.000100
[ Tue Jul  9 00:48:18 2024 ] 
Training: Epoch [74/120], Step [5999], Loss: 0.030066486448049545, Training Accuracy: 95.56041666666667
[ Tue Jul  9 00:48:18 2024 ] 	Batch(6000/7879) done. Loss: 0.2494  lr:0.000100
[ Tue Jul  9 00:48:41 2024 ] 	Batch(6100/7879) done. Loss: 0.2339  lr:0.000100
[ Tue Jul  9 00:49:04 2024 ] 	Batch(6200/7879) done. Loss: 0.6094  lr:0.000100
[ Tue Jul  9 00:49:27 2024 ] 	Batch(6300/7879) done. Loss: 0.3103  lr:0.000100
[ Tue Jul  9 00:49:49 2024 ] 	Batch(6400/7879) done. Loss: 0.0096  lr:0.000100
[ Tue Jul  9 00:50:12 2024 ] 
Training: Epoch [74/120], Step [6499], Loss: 0.018637029454112053, Training Accuracy: 95.5673076923077
[ Tue Jul  9 00:50:12 2024 ] 	Batch(6500/7879) done. Loss: 0.0895  lr:0.000100
[ Tue Jul  9 00:50:35 2024 ] 	Batch(6600/7879) done. Loss: 0.0072  lr:0.000100
[ Tue Jul  9 00:50:58 2024 ] 	Batch(6700/7879) done. Loss: 0.0511  lr:0.000100
[ Tue Jul  9 00:51:20 2024 ] 	Batch(6800/7879) done. Loss: 0.0371  lr:0.000100
[ Tue Jul  9 00:51:43 2024 ] 	Batch(6900/7879) done. Loss: 0.2125  lr:0.000100
[ Tue Jul  9 00:52:06 2024 ] 
Training: Epoch [74/120], Step [6999], Loss: 0.29909029603004456, Training Accuracy: 95.525
[ Tue Jul  9 00:52:06 2024 ] 	Batch(7000/7879) done. Loss: 0.1630  lr:0.000100
[ Tue Jul  9 00:52:29 2024 ] 	Batch(7100/7879) done. Loss: 0.4387  lr:0.000100
[ Tue Jul  9 00:52:51 2024 ] 	Batch(7200/7879) done. Loss: 0.5900  lr:0.000100
[ Tue Jul  9 00:53:15 2024 ] 	Batch(7300/7879) done. Loss: 0.3463  lr:0.000100
[ Tue Jul  9 00:53:38 2024 ] 	Batch(7400/7879) done. Loss: 0.1509  lr:0.000100
[ Tue Jul  9 00:54:02 2024 ] 
Training: Epoch [74/120], Step [7499], Loss: 0.42101994156837463, Training Accuracy: 95.55166666666666
[ Tue Jul  9 00:54:02 2024 ] 	Batch(7500/7879) done. Loss: 0.1242  lr:0.000100
[ Tue Jul  9 00:54:25 2024 ] 	Batch(7600/7879) done. Loss: 0.1627  lr:0.000100
[ Tue Jul  9 00:54:48 2024 ] 	Batch(7700/7879) done. Loss: 0.3556  lr:0.000100
[ Tue Jul  9 00:55:11 2024 ] 	Batch(7800/7879) done. Loss: 0.0077  lr:0.000100
[ Tue Jul  9 00:55:29 2024 ] 	Mean training loss: 0.1625.
[ Tue Jul  9 00:55:29 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 00:55:29 2024 ] Training epoch: 76
[ Tue Jul  9 00:55:29 2024 ] 	Batch(0/7879) done. Loss: 0.0903  lr:0.000100
[ Tue Jul  9 00:55:52 2024 ] 	Batch(100/7879) done. Loss: 0.0112  lr:0.000100
[ Tue Jul  9 00:56:15 2024 ] 	Batch(200/7879) done. Loss: 0.0049  lr:0.000100
[ Tue Jul  9 00:56:39 2024 ] 	Batch(300/7879) done. Loss: 0.0545  lr:0.000100
[ Tue Jul  9 00:57:02 2024 ] 	Batch(400/7879) done. Loss: 0.0146  lr:0.000100
[ Tue Jul  9 00:57:25 2024 ] 
Training: Epoch [75/120], Step [499], Loss: 0.16693417727947235, Training Accuracy: 95.35
[ Tue Jul  9 00:57:25 2024 ] 	Batch(500/7879) done. Loss: 0.0764  lr:0.000100
[ Tue Jul  9 00:57:48 2024 ] 	Batch(600/7879) done. Loss: 0.2628  lr:0.000100
[ Tue Jul  9 00:58:11 2024 ] 	Batch(700/7879) done. Loss: 0.0119  lr:0.000100
[ Tue Jul  9 00:58:34 2024 ] 	Batch(800/7879) done. Loss: 0.0697  lr:0.000100
[ Tue Jul  9 00:58:57 2024 ] 	Batch(900/7879) done. Loss: 0.1557  lr:0.000100
[ Tue Jul  9 00:59:20 2024 ] 
Training: Epoch [75/120], Step [999], Loss: 0.19597361981868744, Training Accuracy: 95.4375
[ Tue Jul  9 00:59:21 2024 ] 	Batch(1000/7879) done. Loss: 0.1245  lr:0.000100
[ Tue Jul  9 00:59:43 2024 ] 	Batch(1100/7879) done. Loss: 0.0553  lr:0.000100
[ Tue Jul  9 01:00:06 2024 ] 	Batch(1200/7879) done. Loss: 0.0505  lr:0.000100
[ Tue Jul  9 01:00:30 2024 ] 	Batch(1300/7879) done. Loss: 0.1774  lr:0.000100
[ Tue Jul  9 01:00:53 2024 ] 	Batch(1400/7879) done. Loss: 0.0861  lr:0.000100
[ Tue Jul  9 01:01:16 2024 ] 
Training: Epoch [75/120], Step [1499], Loss: 0.1497121900320053, Training Accuracy: 95.44166666666666
[ Tue Jul  9 01:01:17 2024 ] 	Batch(1500/7879) done. Loss: 0.0227  lr:0.000100
[ Tue Jul  9 01:01:40 2024 ] 	Batch(1600/7879) done. Loss: 0.7102  lr:0.000100
[ Tue Jul  9 01:02:03 2024 ] 	Batch(1700/7879) done. Loss: 0.0361  lr:0.000100
[ Tue Jul  9 01:02:25 2024 ] 	Batch(1800/7879) done. Loss: 0.0235  lr:0.000100
[ Tue Jul  9 01:02:48 2024 ] 	Batch(1900/7879) done. Loss: 0.7960  lr:0.000100
[ Tue Jul  9 01:03:11 2024 ] 
Training: Epoch [75/120], Step [1999], Loss: 0.059434451162815094, Training Accuracy: 95.5
[ Tue Jul  9 01:03:11 2024 ] 	Batch(2000/7879) done. Loss: 0.3199  lr:0.000100
[ Tue Jul  9 01:03:34 2024 ] 	Batch(2100/7879) done. Loss: 0.0146  lr:0.000100
[ Tue Jul  9 01:03:56 2024 ] 	Batch(2200/7879) done. Loss: 0.0850  lr:0.000100
[ Tue Jul  9 01:04:19 2024 ] 	Batch(2300/7879) done. Loss: 0.1980  lr:0.000100
[ Tue Jul  9 01:04:42 2024 ] 	Batch(2400/7879) done. Loss: 0.0363  lr:0.000100
[ Tue Jul  9 01:05:04 2024 ] 
Training: Epoch [75/120], Step [2499], Loss: 0.316715270280838, Training Accuracy: 95.375
[ Tue Jul  9 01:05:04 2024 ] 	Batch(2500/7879) done. Loss: 0.0524  lr:0.000100
[ Tue Jul  9 01:05:27 2024 ] 	Batch(2600/7879) done. Loss: 0.1371  lr:0.000100
[ Tue Jul  9 01:05:50 2024 ] 	Batch(2700/7879) done. Loss: 0.0181  lr:0.000100
[ Tue Jul  9 01:06:13 2024 ] 	Batch(2800/7879) done. Loss: 0.6837  lr:0.000100
[ Tue Jul  9 01:06:35 2024 ] 	Batch(2900/7879) done. Loss: 0.0428  lr:0.000100
[ Tue Jul  9 01:06:58 2024 ] 
Training: Epoch [75/120], Step [2999], Loss: 0.024625524878501892, Training Accuracy: 95.42500000000001
[ Tue Jul  9 01:06:58 2024 ] 	Batch(3000/7879) done. Loss: 0.3832  lr:0.000100
[ Tue Jul  9 01:07:21 2024 ] 	Batch(3100/7879) done. Loss: 0.0331  lr:0.000100
[ Tue Jul  9 01:07:44 2024 ] 	Batch(3200/7879) done. Loss: 0.2067  lr:0.000100
[ Tue Jul  9 01:08:07 2024 ] 	Batch(3300/7879) done. Loss: 0.2258  lr:0.000100
[ Tue Jul  9 01:08:30 2024 ] 	Batch(3400/7879) done. Loss: 0.2052  lr:0.000100
[ Tue Jul  9 01:08:52 2024 ] 
Training: Epoch [75/120], Step [3499], Loss: 0.1508418768644333, Training Accuracy: 95.38214285714287
[ Tue Jul  9 01:08:53 2024 ] 	Batch(3500/7879) done. Loss: 0.0061  lr:0.000100
[ Tue Jul  9 01:09:15 2024 ] 	Batch(3600/7879) done. Loss: 0.0355  lr:0.000100
[ Tue Jul  9 01:09:38 2024 ] 	Batch(3700/7879) done. Loss: 0.2050  lr:0.000100
[ Tue Jul  9 01:10:01 2024 ] 	Batch(3800/7879) done. Loss: 0.4563  lr:0.000100
[ Tue Jul  9 01:10:24 2024 ] 	Batch(3900/7879) done. Loss: 0.1838  lr:0.000100
[ Tue Jul  9 01:10:46 2024 ] 
Training: Epoch [75/120], Step [3999], Loss: 0.061231229454278946, Training Accuracy: 95.42500000000001
[ Tue Jul  9 01:10:46 2024 ] 	Batch(4000/7879) done. Loss: 0.2761  lr:0.000100
[ Tue Jul  9 01:11:09 2024 ] 	Batch(4100/7879) done. Loss: 0.0621  lr:0.000100
[ Tue Jul  9 01:11:32 2024 ] 	Batch(4200/7879) done. Loss: 0.0579  lr:0.000100
[ Tue Jul  9 01:11:55 2024 ] 	Batch(4300/7879) done. Loss: 0.0679  lr:0.000100
[ Tue Jul  9 01:12:18 2024 ] 	Batch(4400/7879) done. Loss: 0.1509  lr:0.000100
[ Tue Jul  9 01:12:40 2024 ] 
Training: Epoch [75/120], Step [4499], Loss: 0.07718434929847717, Training Accuracy: 95.41666666666667
[ Tue Jul  9 01:12:40 2024 ] 	Batch(4500/7879) done. Loss: 0.0769  lr:0.000100
[ Tue Jul  9 01:13:03 2024 ] 	Batch(4600/7879) done. Loss: 0.2006  lr:0.000100
[ Tue Jul  9 01:13:26 2024 ] 	Batch(4700/7879) done. Loss: 0.0060  lr:0.000100
[ Tue Jul  9 01:13:48 2024 ] 	Batch(4800/7879) done. Loss: 0.0694  lr:0.000100
[ Tue Jul  9 01:14:11 2024 ] 	Batch(4900/7879) done. Loss: 0.1108  lr:0.000100
[ Tue Jul  9 01:14:34 2024 ] 
Training: Epoch [75/120], Step [4999], Loss: 0.07492762804031372, Training Accuracy: 95.38499999999999
[ Tue Jul  9 01:14:34 2024 ] 	Batch(5000/7879) done. Loss: 0.2211  lr:0.000100
[ Tue Jul  9 01:14:57 2024 ] 	Batch(5100/7879) done. Loss: 0.1843  lr:0.000100
[ Tue Jul  9 01:15:19 2024 ] 	Batch(5200/7879) done. Loss: 0.3235  lr:0.000100
[ Tue Jul  9 01:15:42 2024 ] 	Batch(5300/7879) done. Loss: 0.0236  lr:0.000100
[ Tue Jul  9 01:16:05 2024 ] 	Batch(5400/7879) done. Loss: 0.0213  lr:0.000100
[ Tue Jul  9 01:16:27 2024 ] 
Training: Epoch [75/120], Step [5499], Loss: 0.05678752809762955, Training Accuracy: 95.41363636363637
[ Tue Jul  9 01:16:27 2024 ] 	Batch(5500/7879) done. Loss: 0.1146  lr:0.000100
[ Tue Jul  9 01:16:50 2024 ] 	Batch(5600/7879) done. Loss: 0.1231  lr:0.000100
[ Tue Jul  9 01:17:13 2024 ] 	Batch(5700/7879) done. Loss: 0.1244  lr:0.000100
[ Tue Jul  9 01:17:36 2024 ] 	Batch(5800/7879) done. Loss: 0.0642  lr:0.000100
[ Tue Jul  9 01:17:58 2024 ] 	Batch(5900/7879) done. Loss: 0.0629  lr:0.000100
[ Tue Jul  9 01:18:21 2024 ] 
Training: Epoch [75/120], Step [5999], Loss: 0.10728257894515991, Training Accuracy: 95.44166666666666
[ Tue Jul  9 01:18:21 2024 ] 	Batch(6000/7879) done. Loss: 0.1062  lr:0.000100
[ Tue Jul  9 01:18:44 2024 ] 	Batch(6100/7879) done. Loss: 0.2449  lr:0.000100
[ Tue Jul  9 01:19:07 2024 ] 	Batch(6200/7879) done. Loss: 0.0030  lr:0.000100
[ Tue Jul  9 01:19:29 2024 ] 	Batch(6300/7879) done. Loss: 0.2756  lr:0.000100
[ Tue Jul  9 01:19:52 2024 ] 	Batch(6400/7879) done. Loss: 0.1052  lr:0.000100
[ Tue Jul  9 01:20:16 2024 ] 
Training: Epoch [75/120], Step [6499], Loss: 0.44695159792900085, Training Accuracy: 95.46923076923078
[ Tue Jul  9 01:20:16 2024 ] 	Batch(6500/7879) done. Loss: 0.1272  lr:0.000100
[ Tue Jul  9 01:20:39 2024 ] 	Batch(6600/7879) done. Loss: 0.1852  lr:0.000100
[ Tue Jul  9 01:21:03 2024 ] 	Batch(6700/7879) done. Loss: 0.0175  lr:0.000100
[ Tue Jul  9 01:21:26 2024 ] 	Batch(6800/7879) done. Loss: 0.2051  lr:0.000100
[ Tue Jul  9 01:21:50 2024 ] 	Batch(6900/7879) done. Loss: 0.0054  lr:0.000100
[ Tue Jul  9 01:22:13 2024 ] 
Training: Epoch [75/120], Step [6999], Loss: 0.1163693368434906, Training Accuracy: 95.47857142857143
[ Tue Jul  9 01:22:13 2024 ] 	Batch(7000/7879) done. Loss: 0.1597  lr:0.000100
[ Tue Jul  9 01:22:36 2024 ] 	Batch(7100/7879) done. Loss: 0.1341  lr:0.000100
[ Tue Jul  9 01:22:59 2024 ] 	Batch(7200/7879) done. Loss: 0.1595  lr:0.000100
[ Tue Jul  9 01:23:22 2024 ] 	Batch(7300/7879) done. Loss: 0.7283  lr:0.000100
[ Tue Jul  9 01:23:46 2024 ] 	Batch(7400/7879) done. Loss: 0.1490  lr:0.000100
[ Tue Jul  9 01:24:09 2024 ] 
Training: Epoch [75/120], Step [7499], Loss: 0.01232373807579279, Training Accuracy: 95.47333333333333
[ Tue Jul  9 01:24:09 2024 ] 	Batch(7500/7879) done. Loss: 0.0174  lr:0.000100
[ Tue Jul  9 01:24:33 2024 ] 	Batch(7600/7879) done. Loss: 0.5279  lr:0.000100
[ Tue Jul  9 01:24:56 2024 ] 	Batch(7700/7879) done. Loss: 0.0706  lr:0.000100
[ Tue Jul  9 01:25:18 2024 ] 	Batch(7800/7879) done. Loss: 0.0640  lr:0.000100
[ Tue Jul  9 01:25:36 2024 ] 	Mean training loss: 0.1619.
[ Tue Jul  9 01:25:36 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 01:25:36 2024 ] Training epoch: 77
[ Tue Jul  9 01:25:37 2024 ] 	Batch(0/7879) done. Loss: 0.3506  lr:0.000100
[ Tue Jul  9 01:26:00 2024 ] 	Batch(100/7879) done. Loss: 0.0716  lr:0.000100
[ Tue Jul  9 01:26:23 2024 ] 	Batch(200/7879) done. Loss: 0.1713  lr:0.000100
[ Tue Jul  9 01:26:45 2024 ] 	Batch(300/7879) done. Loss: 0.4553  lr:0.000100
[ Tue Jul  9 01:27:08 2024 ] 	Batch(400/7879) done. Loss: 0.1290  lr:0.000100
[ Tue Jul  9 01:27:31 2024 ] 
Training: Epoch [76/120], Step [499], Loss: 0.17627599835395813, Training Accuracy: 95.19999999999999
[ Tue Jul  9 01:27:31 2024 ] 	Batch(500/7879) done. Loss: 0.0172  lr:0.000100
[ Tue Jul  9 01:27:54 2024 ] 	Batch(600/7879) done. Loss: 0.0023  lr:0.000100
[ Tue Jul  9 01:28:16 2024 ] 	Batch(700/7879) done. Loss: 0.0201  lr:0.000100
[ Tue Jul  9 01:28:39 2024 ] 	Batch(800/7879) done. Loss: 0.4230  lr:0.000100
[ Tue Jul  9 01:29:02 2024 ] 	Batch(900/7879) done. Loss: 0.2508  lr:0.000100
[ Tue Jul  9 01:29:24 2024 ] 
Training: Epoch [76/120], Step [999], Loss: 0.005165887530893087, Training Accuracy: 95.4375
[ Tue Jul  9 01:29:25 2024 ] 	Batch(1000/7879) done. Loss: 0.0534  lr:0.000100
[ Tue Jul  9 01:29:47 2024 ] 	Batch(1100/7879) done. Loss: 0.1029  lr:0.000100
[ Tue Jul  9 01:30:10 2024 ] 	Batch(1200/7879) done. Loss: 0.0906  lr:0.000100
[ Tue Jul  9 01:30:33 2024 ] 	Batch(1300/7879) done. Loss: 0.2067  lr:0.000100
[ Tue Jul  9 01:30:55 2024 ] 	Batch(1400/7879) done. Loss: 0.0015  lr:0.000100
[ Tue Jul  9 01:31:18 2024 ] 
Training: Epoch [76/120], Step [1499], Loss: 0.44826188683509827, Training Accuracy: 95.48333333333333
[ Tue Jul  9 01:31:18 2024 ] 	Batch(1500/7879) done. Loss: 0.2048  lr:0.000100
[ Tue Jul  9 01:31:41 2024 ] 	Batch(1600/7879) done. Loss: 0.7390  lr:0.000100
[ Tue Jul  9 01:32:04 2024 ] 	Batch(1700/7879) done. Loss: 0.0382  lr:0.000100
[ Tue Jul  9 01:32:26 2024 ] 	Batch(1800/7879) done. Loss: 0.5307  lr:0.000100
[ Tue Jul  9 01:32:49 2024 ] 	Batch(1900/7879) done. Loss: 0.1261  lr:0.000100
[ Tue Jul  9 01:33:11 2024 ] 
Training: Epoch [76/120], Step [1999], Loss: 0.27790623903274536, Training Accuracy: 95.42500000000001
[ Tue Jul  9 01:33:12 2024 ] 	Batch(2000/7879) done. Loss: 0.7386  lr:0.000100
[ Tue Jul  9 01:33:35 2024 ] 	Batch(2100/7879) done. Loss: 0.0066  lr:0.000100
[ Tue Jul  9 01:33:57 2024 ] 	Batch(2200/7879) done. Loss: 0.6934  lr:0.000100
[ Tue Jul  9 01:34:20 2024 ] 	Batch(2300/7879) done. Loss: 0.0487  lr:0.000100
[ Tue Jul  9 01:34:43 2024 ] 	Batch(2400/7879) done. Loss: 0.0626  lr:0.000100
[ Tue Jul  9 01:35:05 2024 ] 
Training: Epoch [76/120], Step [2499], Loss: 0.22273747622966766, Training Accuracy: 95.44500000000001
[ Tue Jul  9 01:35:06 2024 ] 	Batch(2500/7879) done. Loss: 0.4730  lr:0.000100
[ Tue Jul  9 01:35:28 2024 ] 	Batch(2600/7879) done. Loss: 0.2962  lr:0.000100
[ Tue Jul  9 01:35:51 2024 ] 	Batch(2700/7879) done. Loss: 0.0978  lr:0.000100
[ Tue Jul  9 01:36:14 2024 ] 	Batch(2800/7879) done. Loss: 0.0103  lr:0.000100
[ Tue Jul  9 01:36:37 2024 ] 	Batch(2900/7879) done. Loss: 0.2634  lr:0.000100
[ Tue Jul  9 01:37:01 2024 ] 
Training: Epoch [76/120], Step [2999], Loss: 0.065297931432724, Training Accuracy: 95.44583333333333
[ Tue Jul  9 01:37:01 2024 ] 	Batch(3000/7879) done. Loss: 0.0845  lr:0.000100
[ Tue Jul  9 01:37:24 2024 ] 	Batch(3100/7879) done. Loss: 0.0301  lr:0.000100
[ Tue Jul  9 01:37:48 2024 ] 	Batch(3200/7879) done. Loss: 0.0223  lr:0.000100
[ Tue Jul  9 01:38:10 2024 ] 	Batch(3300/7879) done. Loss: 0.0093  lr:0.000100
[ Tue Jul  9 01:38:33 2024 ] 	Batch(3400/7879) done. Loss: 0.2544  lr:0.000100
[ Tue Jul  9 01:38:56 2024 ] 
Training: Epoch [76/120], Step [3499], Loss: 0.05845990777015686, Training Accuracy: 95.48571428571428
[ Tue Jul  9 01:38:56 2024 ] 	Batch(3500/7879) done. Loss: 0.3483  lr:0.000100
[ Tue Jul  9 01:39:19 2024 ] 	Batch(3600/7879) done. Loss: 0.6736  lr:0.000100
[ Tue Jul  9 01:39:41 2024 ] 	Batch(3700/7879) done. Loss: 0.2519  lr:0.000100
[ Tue Jul  9 01:40:04 2024 ] 	Batch(3800/7879) done. Loss: 0.0293  lr:0.000100
[ Tue Jul  9 01:40:27 2024 ] 	Batch(3900/7879) done. Loss: 0.0971  lr:0.000100
[ Tue Jul  9 01:40:49 2024 ] 
Training: Epoch [76/120], Step [3999], Loss: 0.4773494601249695, Training Accuracy: 95.503125
[ Tue Jul  9 01:40:50 2024 ] 	Batch(4000/7879) done. Loss: 0.1166  lr:0.000100
[ Tue Jul  9 01:41:13 2024 ] 	Batch(4100/7879) done. Loss: 0.2996  lr:0.000100
[ Tue Jul  9 01:41:37 2024 ] 	Batch(4200/7879) done. Loss: 0.1742  lr:0.000100
[ Tue Jul  9 01:42:00 2024 ] 	Batch(4300/7879) done. Loss: 0.0080  lr:0.000100
[ Tue Jul  9 01:42:24 2024 ] 	Batch(4400/7879) done. Loss: 0.2536  lr:0.000100
[ Tue Jul  9 01:42:47 2024 ] 
Training: Epoch [76/120], Step [4499], Loss: 0.07795219868421555, Training Accuracy: 95.54166666666667
[ Tue Jul  9 01:42:47 2024 ] 	Batch(4500/7879) done. Loss: 0.0360  lr:0.000100
[ Tue Jul  9 01:43:10 2024 ] 	Batch(4600/7879) done. Loss: 0.0647  lr:0.000100
[ Tue Jul  9 01:43:33 2024 ] 	Batch(4700/7879) done. Loss: 0.0058  lr:0.000100
[ Tue Jul  9 01:43:56 2024 ] 	Batch(4800/7879) done. Loss: 0.0548  lr:0.000100
[ Tue Jul  9 01:44:19 2024 ] 	Batch(4900/7879) done. Loss: 0.7078  lr:0.000100
[ Tue Jul  9 01:44:42 2024 ] 
Training: Epoch [76/120], Step [4999], Loss: 0.0327720008790493, Training Accuracy: 95.5625
[ Tue Jul  9 01:44:42 2024 ] 	Batch(5000/7879) done. Loss: 0.2884  lr:0.000100
[ Tue Jul  9 01:45:05 2024 ] 	Batch(5100/7879) done. Loss: 0.0331  lr:0.000100
[ Tue Jul  9 01:45:28 2024 ] 	Batch(5200/7879) done. Loss: 0.1982  lr:0.000100
[ Tue Jul  9 01:45:51 2024 ] 	Batch(5300/7879) done. Loss: 0.0230  lr:0.000100
[ Tue Jul  9 01:46:13 2024 ] 	Batch(5400/7879) done. Loss: 0.0769  lr:0.000100
[ Tue Jul  9 01:46:36 2024 ] 
Training: Epoch [76/120], Step [5499], Loss: 0.08865337073802948, Training Accuracy: 95.56136363636364
[ Tue Jul  9 01:46:36 2024 ] 	Batch(5500/7879) done. Loss: 0.0148  lr:0.000100
[ Tue Jul  9 01:46:59 2024 ] 	Batch(5600/7879) done. Loss: 0.0728  lr:0.000100
[ Tue Jul  9 01:47:22 2024 ] 	Batch(5700/7879) done. Loss: 0.0601  lr:0.000100
[ Tue Jul  9 01:47:44 2024 ] 	Batch(5800/7879) done. Loss: 0.4274  lr:0.000100
[ Tue Jul  9 01:48:07 2024 ] 	Batch(5900/7879) done. Loss: 0.0666  lr:0.000100
[ Tue Jul  9 01:48:30 2024 ] 
Training: Epoch [76/120], Step [5999], Loss: 0.036717381328344345, Training Accuracy: 95.54375
[ Tue Jul  9 01:48:30 2024 ] 	Batch(6000/7879) done. Loss: 0.2396  lr:0.000100
[ Tue Jul  9 01:48:53 2024 ] 	Batch(6100/7879) done. Loss: 0.4846  lr:0.000100
[ Tue Jul  9 01:49:15 2024 ] 	Batch(6200/7879) done. Loss: 0.0945  lr:0.000100
[ Tue Jul  9 01:49:38 2024 ] 	Batch(6300/7879) done. Loss: 0.0531  lr:0.000100
[ Tue Jul  9 01:50:01 2024 ] 	Batch(6400/7879) done. Loss: 0.1307  lr:0.000100
[ Tue Jul  9 01:50:23 2024 ] 
Training: Epoch [76/120], Step [6499], Loss: 0.11221969127655029, Training Accuracy: 95.54807692307692
[ Tue Jul  9 01:50:24 2024 ] 	Batch(6500/7879) done. Loss: 0.0462  lr:0.000100
[ Tue Jul  9 01:50:46 2024 ] 	Batch(6600/7879) done. Loss: 0.0190  lr:0.000100
[ Tue Jul  9 01:51:09 2024 ] 	Batch(6700/7879) done. Loss: 0.2319  lr:0.000100
[ Tue Jul  9 01:51:32 2024 ] 	Batch(6800/7879) done. Loss: 0.3512  lr:0.000100
[ Tue Jul  9 01:51:55 2024 ] 	Batch(6900/7879) done. Loss: 0.0741  lr:0.000100
[ Tue Jul  9 01:52:17 2024 ] 
Training: Epoch [76/120], Step [6999], Loss: 0.23351378738880157, Training Accuracy: 95.54285714285714
[ Tue Jul  9 01:52:17 2024 ] 	Batch(7000/7879) done. Loss: 0.3313  lr:0.000100
[ Tue Jul  9 01:52:40 2024 ] 	Batch(7100/7879) done. Loss: 0.2048  lr:0.000100
[ Tue Jul  9 01:53:04 2024 ] 	Batch(7200/7879) done. Loss: 0.1454  lr:0.000100
[ Tue Jul  9 01:53:27 2024 ] 	Batch(7300/7879) done. Loss: 0.0254  lr:0.000100
[ Tue Jul  9 01:53:49 2024 ] 	Batch(7400/7879) done. Loss: 0.0124  lr:0.000100
[ Tue Jul  9 01:54:12 2024 ] 
Training: Epoch [76/120], Step [7499], Loss: 0.04286685958504677, Training Accuracy: 95.545
[ Tue Jul  9 01:54:12 2024 ] 	Batch(7500/7879) done. Loss: 0.1693  lr:0.000100
[ Tue Jul  9 01:54:35 2024 ] 	Batch(7600/7879) done. Loss: 0.1191  lr:0.000100
[ Tue Jul  9 01:54:58 2024 ] 	Batch(7700/7879) done. Loss: 0.0779  lr:0.000100
[ Tue Jul  9 01:55:20 2024 ] 	Batch(7800/7879) done. Loss: 0.1064  lr:0.000100
[ Tue Jul  9 01:55:38 2024 ] 	Mean training loss: 0.1607.
[ Tue Jul  9 01:55:38 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 01:55:38 2024 ] Training epoch: 78
[ Tue Jul  9 01:55:39 2024 ] 	Batch(0/7879) done. Loss: 0.0170  lr:0.000100
[ Tue Jul  9 01:56:02 2024 ] 	Batch(100/7879) done. Loss: 0.0270  lr:0.000100
[ Tue Jul  9 01:56:25 2024 ] 	Batch(200/7879) done. Loss: 0.0507  lr:0.000100
[ Tue Jul  9 01:56:48 2024 ] 	Batch(300/7879) done. Loss: 0.2899  lr:0.000100
[ Tue Jul  9 01:57:11 2024 ] 	Batch(400/7879) done. Loss: 0.1944  lr:0.000100
[ Tue Jul  9 01:57:34 2024 ] 
Training: Epoch [77/120], Step [499], Loss: 0.056952498853206635, Training Accuracy: 95.775
[ Tue Jul  9 01:57:34 2024 ] 	Batch(500/7879) done. Loss: 0.6350  lr:0.000100
[ Tue Jul  9 01:57:57 2024 ] 	Batch(600/7879) done. Loss: 0.0573  lr:0.000100
[ Tue Jul  9 01:58:20 2024 ] 	Batch(700/7879) done. Loss: 1.0441  lr:0.000100
[ Tue Jul  9 01:58:44 2024 ] 	Batch(800/7879) done. Loss: 0.2709  lr:0.000100
[ Tue Jul  9 01:59:07 2024 ] 	Batch(900/7879) done. Loss: 0.1949  lr:0.000100
[ Tue Jul  9 01:59:29 2024 ] 
Training: Epoch [77/120], Step [999], Loss: 0.012597872875630856, Training Accuracy: 95.4375
[ Tue Jul  9 01:59:30 2024 ] 	Batch(1000/7879) done. Loss: 0.1657  lr:0.000100
[ Tue Jul  9 01:59:53 2024 ] 	Batch(1100/7879) done. Loss: 0.3457  lr:0.000100
[ Tue Jul  9 02:00:16 2024 ] 	Batch(1200/7879) done. Loss: 0.2383  lr:0.000100
[ Tue Jul  9 02:00:40 2024 ] 	Batch(1300/7879) done. Loss: 0.0278  lr:0.000100
[ Tue Jul  9 02:01:03 2024 ] 	Batch(1400/7879) done. Loss: 0.0909  lr:0.000100
[ Tue Jul  9 02:01:26 2024 ] 
Training: Epoch [77/120], Step [1499], Loss: 0.22184816002845764, Training Accuracy: 95.54166666666667
[ Tue Jul  9 02:01:27 2024 ] 	Batch(1500/7879) done. Loss: 0.0523  lr:0.000100
[ Tue Jul  9 02:01:50 2024 ] 	Batch(1600/7879) done. Loss: 0.0330  lr:0.000100
[ Tue Jul  9 02:02:14 2024 ] 	Batch(1700/7879) done. Loss: 0.0544  lr:0.000100
[ Tue Jul  9 02:02:37 2024 ] 	Batch(1800/7879) done. Loss: 0.0410  lr:0.000100
[ Tue Jul  9 02:03:01 2024 ] 	Batch(1900/7879) done. Loss: 0.1119  lr:0.000100
[ Tue Jul  9 02:03:24 2024 ] 
Training: Epoch [77/120], Step [1999], Loss: 0.219244122505188, Training Accuracy: 95.46249999999999
[ Tue Jul  9 02:03:24 2024 ] 	Batch(2000/7879) done. Loss: 0.1537  lr:0.000100
[ Tue Jul  9 02:03:47 2024 ] 	Batch(2100/7879) done. Loss: 0.1355  lr:0.000100
[ Tue Jul  9 02:04:10 2024 ] 	Batch(2200/7879) done. Loss: 0.0194  lr:0.000100
[ Tue Jul  9 02:04:32 2024 ] 	Batch(2300/7879) done. Loss: 0.1194  lr:0.000100
[ Tue Jul  9 02:04:55 2024 ] 	Batch(2400/7879) done. Loss: 0.0956  lr:0.000100
[ Tue Jul  9 02:05:18 2024 ] 
Training: Epoch [77/120], Step [2499], Loss: 0.017136795446276665, Training Accuracy: 95.45
[ Tue Jul  9 02:05:18 2024 ] 	Batch(2500/7879) done. Loss: 0.1845  lr:0.000100
[ Tue Jul  9 02:05:41 2024 ] 	Batch(2600/7879) done. Loss: 0.5653  lr:0.000100
[ Tue Jul  9 02:06:03 2024 ] 	Batch(2700/7879) done. Loss: 0.1910  lr:0.000100
[ Tue Jul  9 02:06:26 2024 ] 	Batch(2800/7879) done. Loss: 0.2031  lr:0.000100
[ Tue Jul  9 02:06:50 2024 ] 	Batch(2900/7879) done. Loss: 0.0059  lr:0.000100
[ Tue Jul  9 02:07:13 2024 ] 
Training: Epoch [77/120], Step [2999], Loss: 0.01977827027440071, Training Accuracy: 95.44583333333333
[ Tue Jul  9 02:07:13 2024 ] 	Batch(3000/7879) done. Loss: 0.4518  lr:0.000100
[ Tue Jul  9 02:07:35 2024 ] 	Batch(3100/7879) done. Loss: 0.0964  lr:0.000100
[ Tue Jul  9 02:07:58 2024 ] 	Batch(3200/7879) done. Loss: 0.0069  lr:0.000100
[ Tue Jul  9 02:08:21 2024 ] 	Batch(3300/7879) done. Loss: 0.4210  lr:0.000100
[ Tue Jul  9 02:08:44 2024 ] 	Batch(3400/7879) done. Loss: 0.0344  lr:0.000100
[ Tue Jul  9 02:09:06 2024 ] 
Training: Epoch [77/120], Step [3499], Loss: 0.36300480365753174, Training Accuracy: 95.575
[ Tue Jul  9 02:09:06 2024 ] 	Batch(3500/7879) done. Loss: 0.0116  lr:0.000100
[ Tue Jul  9 02:09:29 2024 ] 	Batch(3600/7879) done. Loss: 0.0574  lr:0.000100
[ Tue Jul  9 02:09:52 2024 ] 	Batch(3700/7879) done. Loss: 0.1772  lr:0.000100
[ Tue Jul  9 02:10:15 2024 ] 	Batch(3800/7879) done. Loss: 0.1169  lr:0.000100
[ Tue Jul  9 02:10:37 2024 ] 	Batch(3900/7879) done. Loss: 0.0590  lr:0.000100
[ Tue Jul  9 02:11:00 2024 ] 
Training: Epoch [77/120], Step [3999], Loss: 0.4516090750694275, Training Accuracy: 95.56875000000001
[ Tue Jul  9 02:11:00 2024 ] 	Batch(4000/7879) done. Loss: 0.4428  lr:0.000100
[ Tue Jul  9 02:11:23 2024 ] 	Batch(4100/7879) done. Loss: 0.0071  lr:0.000100
[ Tue Jul  9 02:11:45 2024 ] 	Batch(4200/7879) done. Loss: 0.0028  lr:0.000100
[ Tue Jul  9 02:12:08 2024 ] 	Batch(4300/7879) done. Loss: 0.1769  lr:0.000100
[ Tue Jul  9 02:12:31 2024 ] 	Batch(4400/7879) done. Loss: 0.0556  lr:0.000100
[ Tue Jul  9 02:12:54 2024 ] 
Training: Epoch [77/120], Step [4499], Loss: 0.13887791335582733, Training Accuracy: 95.5611111111111
[ Tue Jul  9 02:12:54 2024 ] 	Batch(4500/7879) done. Loss: 0.0277  lr:0.000100
[ Tue Jul  9 02:13:18 2024 ] 	Batch(4600/7879) done. Loss: 0.0506  lr:0.000100
[ Tue Jul  9 02:13:41 2024 ] 	Batch(4700/7879) done. Loss: 0.1008  lr:0.000100
[ Tue Jul  9 02:14:05 2024 ] 	Batch(4800/7879) done. Loss: 0.2340  lr:0.000100
[ Tue Jul  9 02:14:28 2024 ] 	Batch(4900/7879) done. Loss: 0.0281  lr:0.000100
[ Tue Jul  9 02:14:50 2024 ] 
Training: Epoch [77/120], Step [4999], Loss: 0.07021299004554749, Training Accuracy: 95.5175
[ Tue Jul  9 02:14:50 2024 ] 	Batch(5000/7879) done. Loss: 0.1593  lr:0.000100
[ Tue Jul  9 02:15:13 2024 ] 	Batch(5100/7879) done. Loss: 0.0734  lr:0.000100
[ Tue Jul  9 02:15:36 2024 ] 	Batch(5200/7879) done. Loss: 0.1906  lr:0.000100
[ Tue Jul  9 02:15:58 2024 ] 	Batch(5300/7879) done. Loss: 0.0757  lr:0.000100
[ Tue Jul  9 02:16:21 2024 ] 	Batch(5400/7879) done. Loss: 0.2691  lr:0.000100
[ Tue Jul  9 02:16:44 2024 ] 
Training: Epoch [77/120], Step [5499], Loss: 0.024474838748574257, Training Accuracy: 95.51136363636363
[ Tue Jul  9 02:16:44 2024 ] 	Batch(5500/7879) done. Loss: 0.2356  lr:0.000100
[ Tue Jul  9 02:17:07 2024 ] 	Batch(5600/7879) done. Loss: 0.0513  lr:0.000100
[ Tue Jul  9 02:17:29 2024 ] 	Batch(5700/7879) done. Loss: 0.0206  lr:0.000100
[ Tue Jul  9 02:17:52 2024 ] 	Batch(5800/7879) done. Loss: 0.0292  lr:0.000100
[ Tue Jul  9 02:18:15 2024 ] 	Batch(5900/7879) done. Loss: 0.2038  lr:0.000100
[ Tue Jul  9 02:18:37 2024 ] 
Training: Epoch [77/120], Step [5999], Loss: 0.10203468054533005, Training Accuracy: 95.525
[ Tue Jul  9 02:18:38 2024 ] 	Batch(6000/7879) done. Loss: 0.0051  lr:0.000100
[ Tue Jul  9 02:19:00 2024 ] 	Batch(6100/7879) done. Loss: 0.4950  lr:0.000100
[ Tue Jul  9 02:19:23 2024 ] 	Batch(6200/7879) done. Loss: 0.0822  lr:0.000100
[ Tue Jul  9 02:19:46 2024 ] 	Batch(6300/7879) done. Loss: 0.0814  lr:0.000100
[ Tue Jul  9 02:20:09 2024 ] 	Batch(6400/7879) done. Loss: 0.0045  lr:0.000100
[ Tue Jul  9 02:20:31 2024 ] 
Training: Epoch [77/120], Step [6499], Loss: 0.03535836935043335, Training Accuracy: 95.51538461538462
[ Tue Jul  9 02:20:31 2024 ] 	Batch(6500/7879) done. Loss: 0.1941  lr:0.000100
[ Tue Jul  9 02:20:54 2024 ] 	Batch(6600/7879) done. Loss: 0.0145  lr:0.000100
[ Tue Jul  9 02:21:17 2024 ] 	Batch(6700/7879) done. Loss: 0.3467  lr:0.000100
[ Tue Jul  9 02:21:40 2024 ] 	Batch(6800/7879) done. Loss: 0.2157  lr:0.000100
[ Tue Jul  9 02:22:03 2024 ] 	Batch(6900/7879) done. Loss: 0.1340  lr:0.000100
[ Tue Jul  9 02:22:26 2024 ] 
Training: Epoch [77/120], Step [6999], Loss: 0.13408330082893372, Training Accuracy: 95.5125
[ Tue Jul  9 02:22:26 2024 ] 	Batch(7000/7879) done. Loss: 0.3637  lr:0.000100
[ Tue Jul  9 02:22:49 2024 ] 	Batch(7100/7879) done. Loss: 0.0127  lr:0.000100
[ Tue Jul  9 02:23:11 2024 ] 	Batch(7200/7879) done. Loss: 0.1161  lr:0.000100
[ Tue Jul  9 02:23:34 2024 ] 	Batch(7300/7879) done. Loss: 0.3088  lr:0.000100
[ Tue Jul  9 02:23:57 2024 ] 	Batch(7400/7879) done. Loss: 0.1389  lr:0.000100
[ Tue Jul  9 02:24:19 2024 ] 
Training: Epoch [77/120], Step [7499], Loss: 0.15406763553619385, Training Accuracy: 95.48833333333333
[ Tue Jul  9 02:24:19 2024 ] 	Batch(7500/7879) done. Loss: 0.1501  lr:0.000100
[ Tue Jul  9 02:24:42 2024 ] 	Batch(7600/7879) done. Loss: 0.1503  lr:0.000100
[ Tue Jul  9 02:25:05 2024 ] 	Batch(7700/7879) done. Loss: 0.0279  lr:0.000100
[ Tue Jul  9 02:25:28 2024 ] 	Batch(7800/7879) done. Loss: 0.1212  lr:0.000100
[ Tue Jul  9 02:25:46 2024 ] 	Mean training loss: 0.1624.
[ Tue Jul  9 02:25:46 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 02:25:46 2024 ] Training epoch: 79
[ Tue Jul  9 02:25:46 2024 ] 	Batch(0/7879) done. Loss: 0.0083  lr:0.000100
[ Tue Jul  9 02:26:09 2024 ] 	Batch(100/7879) done. Loss: 0.0338  lr:0.000100
[ Tue Jul  9 02:26:32 2024 ] 	Batch(200/7879) done. Loss: 0.1351  lr:0.000100
[ Tue Jul  9 02:26:54 2024 ] 	Batch(300/7879) done. Loss: 0.0502  lr:0.000100
[ Tue Jul  9 02:27:17 2024 ] 	Batch(400/7879) done. Loss: 0.2899  lr:0.000100
[ Tue Jul  9 02:27:40 2024 ] 
Training: Epoch [78/120], Step [499], Loss: 0.3071412146091461, Training Accuracy: 95.45
[ Tue Jul  9 02:27:40 2024 ] 	Batch(500/7879) done. Loss: 0.5023  lr:0.000100
[ Tue Jul  9 02:28:03 2024 ] 	Batch(600/7879) done. Loss: 0.0345  lr:0.000100
[ Tue Jul  9 02:28:26 2024 ] 	Batch(700/7879) done. Loss: 0.0365  lr:0.000100
[ Tue Jul  9 02:28:48 2024 ] 	Batch(800/7879) done. Loss: 0.0107  lr:0.000100
[ Tue Jul  9 02:29:11 2024 ] 	Batch(900/7879) done. Loss: 0.1267  lr:0.000100
[ Tue Jul  9 02:29:34 2024 ] 
Training: Epoch [78/120], Step [999], Loss: 0.10412254929542542, Training Accuracy: 95.6125
[ Tue Jul  9 02:29:34 2024 ] 	Batch(1000/7879) done. Loss: 0.7463  lr:0.000100
[ Tue Jul  9 02:29:56 2024 ] 	Batch(1100/7879) done. Loss: 0.1285  lr:0.000100
[ Tue Jul  9 02:30:19 2024 ] 	Batch(1200/7879) done. Loss: 0.0710  lr:0.000100
[ Tue Jul  9 02:30:42 2024 ] 	Batch(1300/7879) done. Loss: 0.0296  lr:0.000100
[ Tue Jul  9 02:31:05 2024 ] 	Batch(1400/7879) done. Loss: 0.0222  lr:0.000100
[ Tue Jul  9 02:31:27 2024 ] 
Training: Epoch [78/120], Step [1499], Loss: 0.5558316111564636, Training Accuracy: 95.72500000000001
[ Tue Jul  9 02:31:27 2024 ] 	Batch(1500/7879) done. Loss: 0.0880  lr:0.000100
[ Tue Jul  9 02:31:50 2024 ] 	Batch(1600/7879) done. Loss: 0.1759  lr:0.000100
[ Tue Jul  9 02:32:13 2024 ] 	Batch(1700/7879) done. Loss: 0.1188  lr:0.000100
[ Tue Jul  9 02:32:36 2024 ] 	Batch(1800/7879) done. Loss: 0.2842  lr:0.000100
[ Tue Jul  9 02:32:58 2024 ] 	Batch(1900/7879) done. Loss: 0.3804  lr:0.000100
[ Tue Jul  9 02:33:21 2024 ] 
Training: Epoch [78/120], Step [1999], Loss: 0.026607241481542587, Training Accuracy: 95.73125
[ Tue Jul  9 02:33:21 2024 ] 	Batch(2000/7879) done. Loss: 0.2369  lr:0.000100
[ Tue Jul  9 02:33:44 2024 ] 	Batch(2100/7879) done. Loss: 0.0988  lr:0.000100
[ Tue Jul  9 02:34:07 2024 ] 	Batch(2200/7879) done. Loss: 0.0618  lr:0.000100
[ Tue Jul  9 02:34:29 2024 ] 	Batch(2300/7879) done. Loss: 0.1267  lr:0.000100
[ Tue Jul  9 02:34:52 2024 ] 	Batch(2400/7879) done. Loss: 0.3485  lr:0.000100
[ Tue Jul  9 02:35:15 2024 ] 
Training: Epoch [78/120], Step [2499], Loss: 0.3695037364959717, Training Accuracy: 95.67
[ Tue Jul  9 02:35:15 2024 ] 	Batch(2500/7879) done. Loss: 0.0495  lr:0.000100
[ Tue Jul  9 02:35:38 2024 ] 	Batch(2600/7879) done. Loss: 0.1615  lr:0.000100
[ Tue Jul  9 02:36:00 2024 ] 	Batch(2700/7879) done. Loss: 0.1490  lr:0.000100
[ Tue Jul  9 02:36:23 2024 ] 	Batch(2800/7879) done. Loss: 0.1907  lr:0.000100
[ Tue Jul  9 02:36:46 2024 ] 	Batch(2900/7879) done. Loss: 0.5064  lr:0.000100
[ Tue Jul  9 02:37:08 2024 ] 
Training: Epoch [78/120], Step [2999], Loss: 0.030957352370023727, Training Accuracy: 95.64166666666667
[ Tue Jul  9 02:37:09 2024 ] 	Batch(3000/7879) done. Loss: 0.0681  lr:0.000100
[ Tue Jul  9 02:37:31 2024 ] 	Batch(3100/7879) done. Loss: 0.1659  lr:0.000100
[ Tue Jul  9 02:37:54 2024 ] 	Batch(3200/7879) done. Loss: 0.0926  lr:0.000100
[ Tue Jul  9 02:38:17 2024 ] 	Batch(3300/7879) done. Loss: 0.0863  lr:0.000100
[ Tue Jul  9 02:38:40 2024 ] 	Batch(3400/7879) done. Loss: 0.1591  lr:0.000100
[ Tue Jul  9 02:39:02 2024 ] 
Training: Epoch [78/120], Step [3499], Loss: 0.09352126717567444, Training Accuracy: 95.53214285714286
[ Tue Jul  9 02:39:02 2024 ] 	Batch(3500/7879) done. Loss: 0.0645  lr:0.000100
[ Tue Jul  9 02:39:25 2024 ] 	Batch(3600/7879) done. Loss: 0.0501  lr:0.000100
[ Tue Jul  9 02:39:48 2024 ] 	Batch(3700/7879) done. Loss: 0.2549  lr:0.000100
[ Tue Jul  9 02:40:11 2024 ] 	Batch(3800/7879) done. Loss: 0.3915  lr:0.000100
[ Tue Jul  9 02:40:33 2024 ] 	Batch(3900/7879) done. Loss: 0.2158  lr:0.000100
[ Tue Jul  9 02:40:56 2024 ] 
Training: Epoch [78/120], Step [3999], Loss: 0.019180389121174812, Training Accuracy: 95.54375
[ Tue Jul  9 02:40:56 2024 ] 	Batch(4000/7879) done. Loss: 0.0892  lr:0.000100
[ Tue Jul  9 02:41:19 2024 ] 	Batch(4100/7879) done. Loss: 0.4045  lr:0.000100
[ Tue Jul  9 02:41:42 2024 ] 	Batch(4200/7879) done. Loss: 0.0196  lr:0.000100
[ Tue Jul  9 02:42:05 2024 ] 	Batch(4300/7879) done. Loss: 0.0644  lr:0.000100
[ Tue Jul  9 02:42:28 2024 ] 	Batch(4400/7879) done. Loss: 0.3451  lr:0.000100
[ Tue Jul  9 02:42:50 2024 ] 
Training: Epoch [78/120], Step [4499], Loss: 0.0053514097817242146, Training Accuracy: 95.575
[ Tue Jul  9 02:42:51 2024 ] 	Batch(4500/7879) done. Loss: 0.1012  lr:0.000100
[ Tue Jul  9 02:43:13 2024 ] 	Batch(4600/7879) done. Loss: 0.1200  lr:0.000100
[ Tue Jul  9 02:43:36 2024 ] 	Batch(4700/7879) done. Loss: 0.0147  lr:0.000100
[ Tue Jul  9 02:43:59 2024 ] 	Batch(4800/7879) done. Loss: 0.0185  lr:0.000100
[ Tue Jul  9 02:44:22 2024 ] 	Batch(4900/7879) done. Loss: 0.0049  lr:0.000100
[ Tue Jul  9 02:44:44 2024 ] 
Training: Epoch [78/120], Step [4999], Loss: 0.6443164944648743, Training Accuracy: 95.635
[ Tue Jul  9 02:44:45 2024 ] 	Batch(5000/7879) done. Loss: 0.0065  lr:0.000100
[ Tue Jul  9 02:45:07 2024 ] 	Batch(5100/7879) done. Loss: 0.1369  lr:0.000100
[ Tue Jul  9 02:45:30 2024 ] 	Batch(5200/7879) done. Loss: 0.1067  lr:0.000100
[ Tue Jul  9 02:45:53 2024 ] 	Batch(5300/7879) done. Loss: 0.2439  lr:0.000100
[ Tue Jul  9 02:46:15 2024 ] 	Batch(5400/7879) done. Loss: 0.1512  lr:0.000100
[ Tue Jul  9 02:46:39 2024 ] 
Training: Epoch [78/120], Step [5499], Loss: 0.0028923274949193, Training Accuracy: 95.625
[ Tue Jul  9 02:46:39 2024 ] 	Batch(5500/7879) done. Loss: 0.0817  lr:0.000100
[ Tue Jul  9 02:47:03 2024 ] 	Batch(5600/7879) done. Loss: 0.3785  lr:0.000100
[ Tue Jul  9 02:47:26 2024 ] 	Batch(5700/7879) done. Loss: 0.1460  lr:0.000100
[ Tue Jul  9 02:47:49 2024 ] 	Batch(5800/7879) done. Loss: 0.0983  lr:0.000100
[ Tue Jul  9 02:48:13 2024 ] 	Batch(5900/7879) done. Loss: 0.6862  lr:0.000100
[ Tue Jul  9 02:48:36 2024 ] 
Training: Epoch [78/120], Step [5999], Loss: 0.3582133650779724, Training Accuracy: 95.6875
[ Tue Jul  9 02:48:36 2024 ] 	Batch(6000/7879) done. Loss: 0.0232  lr:0.000100
[ Tue Jul  9 02:48:59 2024 ] 	Batch(6100/7879) done. Loss: 0.7284  lr:0.000100
[ Tue Jul  9 02:49:22 2024 ] 	Batch(6200/7879) done. Loss: 0.0236  lr:0.000100
[ Tue Jul  9 02:49:45 2024 ] 	Batch(6300/7879) done. Loss: 0.1981  lr:0.000100
[ Tue Jul  9 02:50:08 2024 ] 	Batch(6400/7879) done. Loss: 0.6549  lr:0.000100
[ Tue Jul  9 02:50:31 2024 ] 
Training: Epoch [78/120], Step [6499], Loss: 0.0635383129119873, Training Accuracy: 95.68076923076923
[ Tue Jul  9 02:50:32 2024 ] 	Batch(6500/7879) done. Loss: 0.1356  lr:0.000100
[ Tue Jul  9 02:50:55 2024 ] 	Batch(6600/7879) done. Loss: 0.0797  lr:0.000100
[ Tue Jul  9 02:51:19 2024 ] 	Batch(6700/7879) done. Loss: 0.0252  lr:0.000100
[ Tue Jul  9 02:51:42 2024 ] 	Batch(6800/7879) done. Loss: 0.0388  lr:0.000100
[ Tue Jul  9 02:52:06 2024 ] 	Batch(6900/7879) done. Loss: 0.0500  lr:0.000100
[ Tue Jul  9 02:52:28 2024 ] 
Training: Epoch [78/120], Step [6999], Loss: 0.1978684812784195, Training Accuracy: 95.70535714285714
[ Tue Jul  9 02:52:28 2024 ] 	Batch(7000/7879) done. Loss: 0.0933  lr:0.000100
[ Tue Jul  9 02:52:51 2024 ] 	Batch(7100/7879) done. Loss: 0.0490  lr:0.000100
[ Tue Jul  9 02:53:14 2024 ] 	Batch(7200/7879) done. Loss: 0.0036  lr:0.000100
[ Tue Jul  9 02:53:37 2024 ] 	Batch(7300/7879) done. Loss: 0.2384  lr:0.000100
[ Tue Jul  9 02:54:01 2024 ] 	Batch(7400/7879) done. Loss: 0.6430  lr:0.000100
[ Tue Jul  9 02:54:23 2024 ] 
Training: Epoch [78/120], Step [7499], Loss: 0.0067179156467318535, Training Accuracy: 95.69833333333332
[ Tue Jul  9 02:54:24 2024 ] 	Batch(7500/7879) done. Loss: 0.4648  lr:0.000100
[ Tue Jul  9 02:54:46 2024 ] 	Batch(7600/7879) done. Loss: 0.1007  lr:0.000100
[ Tue Jul  9 02:55:09 2024 ] 	Batch(7700/7879) done. Loss: 0.4217  lr:0.000100
[ Tue Jul  9 02:55:32 2024 ] 	Batch(7800/7879) done. Loss: 0.0883  lr:0.000100
[ Tue Jul  9 02:55:50 2024 ] 	Mean training loss: 0.1564.
[ Tue Jul  9 02:55:50 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 02:55:50 2024 ] Training epoch: 80
[ Tue Jul  9 02:55:50 2024 ] 	Batch(0/7879) done. Loss: 0.1645  lr:0.000100
[ Tue Jul  9 02:56:13 2024 ] 	Batch(100/7879) done. Loss: 0.0089  lr:0.000100
[ Tue Jul  9 02:56:36 2024 ] 	Batch(200/7879) done. Loss: 0.0044  lr:0.000100
[ Tue Jul  9 02:56:59 2024 ] 	Batch(300/7879) done. Loss: 0.0474  lr:0.000100
[ Tue Jul  9 02:57:23 2024 ] 	Batch(400/7879) done. Loss: 0.0414  lr:0.000100
[ Tue Jul  9 02:57:45 2024 ] 
Training: Epoch [79/120], Step [499], Loss: 0.10266215354204178, Training Accuracy: 95.775
[ Tue Jul  9 02:57:46 2024 ] 	Batch(500/7879) done. Loss: 0.0829  lr:0.000100
[ Tue Jul  9 02:58:09 2024 ] 	Batch(600/7879) done. Loss: 0.1861  lr:0.000100
[ Tue Jul  9 02:58:32 2024 ] 	Batch(700/7879) done. Loss: 0.0434  lr:0.000100
[ Tue Jul  9 02:58:55 2024 ] 	Batch(800/7879) done. Loss: 0.1352  lr:0.000100
[ Tue Jul  9 02:59:18 2024 ] 	Batch(900/7879) done. Loss: 0.0166  lr:0.000100
[ Tue Jul  9 02:59:40 2024 ] 
Training: Epoch [79/120], Step [999], Loss: 0.06438782811164856, Training Accuracy: 95.6125
[ Tue Jul  9 02:59:41 2024 ] 	Batch(1000/7879) done. Loss: 0.6039  lr:0.000100
[ Tue Jul  9 03:00:03 2024 ] 	Batch(1100/7879) done. Loss: 0.1302  lr:0.000100
[ Tue Jul  9 03:00:26 2024 ] 	Batch(1200/7879) done. Loss: 0.0471  lr:0.000100
[ Tue Jul  9 03:00:49 2024 ] 	Batch(1300/7879) done. Loss: 0.3611  lr:0.000100
[ Tue Jul  9 03:01:11 2024 ] 	Batch(1400/7879) done. Loss: 0.2281  lr:0.000100
[ Tue Jul  9 03:01:34 2024 ] 
Training: Epoch [79/120], Step [1499], Loss: 0.13664905726909637, Training Accuracy: 95.74166666666667
[ Tue Jul  9 03:01:34 2024 ] 	Batch(1500/7879) done. Loss: 0.1951  lr:0.000100
[ Tue Jul  9 03:01:57 2024 ] 	Batch(1600/7879) done. Loss: 0.1871  lr:0.000100
[ Tue Jul  9 03:02:20 2024 ] 	Batch(1700/7879) done. Loss: 0.4120  lr:0.000100
[ Tue Jul  9 03:02:42 2024 ] 	Batch(1800/7879) done. Loss: 0.0245  lr:0.000100
[ Tue Jul  9 03:03:05 2024 ] 	Batch(1900/7879) done. Loss: 0.1379  lr:0.000100
[ Tue Jul  9 03:03:27 2024 ] 
Training: Epoch [79/120], Step [1999], Loss: 0.07897122204303741, Training Accuracy: 95.69375
[ Tue Jul  9 03:03:28 2024 ] 	Batch(2000/7879) done. Loss: 0.1958  lr:0.000100
[ Tue Jul  9 03:03:50 2024 ] 	Batch(2100/7879) done. Loss: 0.2849  lr:0.000100
[ Tue Jul  9 03:04:13 2024 ] 	Batch(2200/7879) done. Loss: 0.0463  lr:0.000100
[ Tue Jul  9 03:04:36 2024 ] 	Batch(2300/7879) done. Loss: 0.0344  lr:0.000100
[ Tue Jul  9 03:04:59 2024 ] 	Batch(2400/7879) done. Loss: 0.0895  lr:0.000100
[ Tue Jul  9 03:05:21 2024 ] 
Training: Epoch [79/120], Step [2499], Loss: 0.18209093809127808, Training Accuracy: 95.735
[ Tue Jul  9 03:05:21 2024 ] 	Batch(2500/7879) done. Loss: 0.0873  lr:0.000100
[ Tue Jul  9 03:05:44 2024 ] 	Batch(2600/7879) done. Loss: 0.2538  lr:0.000100
[ Tue Jul  9 03:06:07 2024 ] 	Batch(2700/7879) done. Loss: 0.0443  lr:0.000100
[ Tue Jul  9 03:06:29 2024 ] 	Batch(2800/7879) done. Loss: 0.1639  lr:0.000100
[ Tue Jul  9 03:06:53 2024 ] 	Batch(2900/7879) done. Loss: 0.1868  lr:0.000100
[ Tue Jul  9 03:07:16 2024 ] 
Training: Epoch [79/120], Step [2999], Loss: 0.22516658902168274, Training Accuracy: 95.675
[ Tue Jul  9 03:07:16 2024 ] 	Batch(3000/7879) done. Loss: 0.0566  lr:0.000100
[ Tue Jul  9 03:07:40 2024 ] 	Batch(3100/7879) done. Loss: 0.0063  lr:0.000100
[ Tue Jul  9 03:08:03 2024 ] 	Batch(3200/7879) done. Loss: 0.2241  lr:0.000100
[ Tue Jul  9 03:08:26 2024 ] 	Batch(3300/7879) done. Loss: 0.1216  lr:0.000100
[ Tue Jul  9 03:08:49 2024 ] 	Batch(3400/7879) done. Loss: 0.2622  lr:0.000100
[ Tue Jul  9 03:09:11 2024 ] 
Training: Epoch [79/120], Step [3499], Loss: 0.23468472063541412, Training Accuracy: 95.64642857142857
[ Tue Jul  9 03:09:11 2024 ] 	Batch(3500/7879) done. Loss: 0.1993  lr:0.000100
[ Tue Jul  9 03:09:34 2024 ] 	Batch(3600/7879) done. Loss: 0.0192  lr:0.000100
[ Tue Jul  9 03:09:57 2024 ] 	Batch(3700/7879) done. Loss: 0.5021  lr:0.000100
[ Tue Jul  9 03:10:20 2024 ] 	Batch(3800/7879) done. Loss: 0.7068  lr:0.000100
[ Tue Jul  9 03:10:42 2024 ] 	Batch(3900/7879) done. Loss: 0.1557  lr:0.000100
[ Tue Jul  9 03:11:05 2024 ] 
Training: Epoch [79/120], Step [3999], Loss: 0.015382318757474422, Training Accuracy: 95.703125
[ Tue Jul  9 03:11:05 2024 ] 	Batch(4000/7879) done. Loss: 0.3992  lr:0.000100
[ Tue Jul  9 03:11:28 2024 ] 	Batch(4100/7879) done. Loss: 0.0111  lr:0.000100
[ Tue Jul  9 03:11:51 2024 ] 	Batch(4200/7879) done. Loss: 0.2288  lr:0.000100
[ Tue Jul  9 03:12:13 2024 ] 	Batch(4300/7879) done. Loss: 0.0839  lr:0.000100
[ Tue Jul  9 03:12:36 2024 ] 	Batch(4400/7879) done. Loss: 0.3385  lr:0.000100
[ Tue Jul  9 03:12:59 2024 ] 
Training: Epoch [79/120], Step [4499], Loss: 0.21003232896327972, Training Accuracy: 95.70277777777778
[ Tue Jul  9 03:12:59 2024 ] 	Batch(4500/7879) done. Loss: 0.0295  lr:0.000100
[ Tue Jul  9 03:13:22 2024 ] 	Batch(4600/7879) done. Loss: 0.0568  lr:0.000100
[ Tue Jul  9 03:13:44 2024 ] 	Batch(4700/7879) done. Loss: 0.2739  lr:0.000100
[ Tue Jul  9 03:14:07 2024 ] 	Batch(4800/7879) done. Loss: 0.0189  lr:0.000100
[ Tue Jul  9 03:14:30 2024 ] 	Batch(4900/7879) done. Loss: 0.4232  lr:0.000100
[ Tue Jul  9 03:14:53 2024 ] 
Training: Epoch [79/120], Step [4999], Loss: 0.055127210915088654, Training Accuracy: 95.755
[ Tue Jul  9 03:14:53 2024 ] 	Batch(5000/7879) done. Loss: 0.0351  lr:0.000100
[ Tue Jul  9 03:15:15 2024 ] 	Batch(5100/7879) done. Loss: 0.0511  lr:0.000100
[ Tue Jul  9 03:15:38 2024 ] 	Batch(5200/7879) done. Loss: 0.1590  lr:0.000100
[ Tue Jul  9 03:16:01 2024 ] 	Batch(5300/7879) done. Loss: 0.0491  lr:0.000100
[ Tue Jul  9 03:16:24 2024 ] 	Batch(5400/7879) done. Loss: 0.0282  lr:0.000100
[ Tue Jul  9 03:16:46 2024 ] 
Training: Epoch [79/120], Step [5499], Loss: 0.022843146696686745, Training Accuracy: 95.75454545454546
[ Tue Jul  9 03:16:46 2024 ] 	Batch(5500/7879) done. Loss: 0.3955  lr:0.000100
[ Tue Jul  9 03:17:09 2024 ] 	Batch(5600/7879) done. Loss: 0.1413  lr:0.000100
[ Tue Jul  9 03:17:32 2024 ] 	Batch(5700/7879) done. Loss: 0.0230  lr:0.000100
[ Tue Jul  9 03:17:55 2024 ] 	Batch(5800/7879) done. Loss: 0.2165  lr:0.000100
[ Tue Jul  9 03:18:17 2024 ] 	Batch(5900/7879) done. Loss: 0.0831  lr:0.000100
[ Tue Jul  9 03:18:40 2024 ] 
Training: Epoch [79/120], Step [5999], Loss: 0.008729931898415089, Training Accuracy: 95.72500000000001
[ Tue Jul  9 03:18:40 2024 ] 	Batch(6000/7879) done. Loss: 0.0696  lr:0.000100
[ Tue Jul  9 03:19:03 2024 ] 	Batch(6100/7879) done. Loss: 0.0368  lr:0.000100
[ Tue Jul  9 03:19:26 2024 ] 	Batch(6200/7879) done. Loss: 0.0914  lr:0.000100
[ Tue Jul  9 03:19:48 2024 ] 	Batch(6300/7879) done. Loss: 0.4981  lr:0.000100
[ Tue Jul  9 03:20:11 2024 ] 	Batch(6400/7879) done. Loss: 0.6347  lr:0.000100
[ Tue Jul  9 03:20:34 2024 ] 
Training: Epoch [79/120], Step [6499], Loss: 0.028137098997831345, Training Accuracy: 95.72692307692307
[ Tue Jul  9 03:20:34 2024 ] 	Batch(6500/7879) done. Loss: 0.0501  lr:0.000100
[ Tue Jul  9 03:20:57 2024 ] 	Batch(6600/7879) done. Loss: 0.0069  lr:0.000100
[ Tue Jul  9 03:21:19 2024 ] 	Batch(6700/7879) done. Loss: 0.1410  lr:0.000100
[ Tue Jul  9 03:21:42 2024 ] 	Batch(6800/7879) done. Loss: 0.6903  lr:0.000100
[ Tue Jul  9 03:22:05 2024 ] 	Batch(6900/7879) done. Loss: 0.0502  lr:0.000100
[ Tue Jul  9 03:22:27 2024 ] 
Training: Epoch [79/120], Step [6999], Loss: 0.2412666231393814, Training Accuracy: 95.70357142857144
[ Tue Jul  9 03:22:28 2024 ] 	Batch(7000/7879) done. Loss: 0.0656  lr:0.000100
[ Tue Jul  9 03:22:50 2024 ] 	Batch(7100/7879) done. Loss: 0.1211  lr:0.000100
[ Tue Jul  9 03:23:13 2024 ] 	Batch(7200/7879) done. Loss: 0.1668  lr:0.000100
[ Tue Jul  9 03:23:37 2024 ] 	Batch(7300/7879) done. Loss: 0.0188  lr:0.000100
[ Tue Jul  9 03:24:00 2024 ] 	Batch(7400/7879) done. Loss: 0.1121  lr:0.000100
[ Tue Jul  9 03:24:22 2024 ] 
Training: Epoch [79/120], Step [7499], Loss: 0.07771460711956024, Training Accuracy: 95.74833333333333
[ Tue Jul  9 03:24:22 2024 ] 	Batch(7500/7879) done. Loss: 0.0821  lr:0.000100
[ Tue Jul  9 03:24:45 2024 ] 	Batch(7600/7879) done. Loss: 0.0184  lr:0.000100
[ Tue Jul  9 03:25:08 2024 ] 	Batch(7700/7879) done. Loss: 0.2024  lr:0.000100
[ Tue Jul  9 03:25:31 2024 ] 	Batch(7800/7879) done. Loss: 0.0754  lr:0.000100
[ Tue Jul  9 03:25:48 2024 ] 	Mean training loss: 0.1569.
[ Tue Jul  9 03:25:48 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 03:25:48 2024 ] Eval epoch: 80
[ Tue Jul  9 03:31:45 2024 ] 	Mean val loss of 6365 batches: 0.9930016669869705.
[ Tue Jul  9 03:31:45 2024 ] 
Validation: Epoch [79/120], Samples [39490.0/50919], Loss: 0.315153568983078, Validation Accuracy: 77.5545474184489
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 1 : 191 / 275 = 69 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 2 : 235 / 273 = 86 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 3 : 232 / 273 = 84 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 4 : 223 / 275 = 81 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 5 : 232 / 275 = 84 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 6 : 224 / 275 = 81 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 7 : 257 / 273 = 94 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 8 : 265 / 273 = 97 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 9 : 201 / 273 = 73 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 10 : 120 / 273 = 43 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 11 : 141 / 272 = 51 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 12 : 231 / 271 = 85 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 13 : 266 / 275 = 96 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 14 : 263 / 276 = 95 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 15 : 222 / 273 = 81 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 16 : 154 / 274 = 56 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 17 : 239 / 273 = 87 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 18 : 238 / 274 = 86 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 19 : 245 / 272 = 90 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 20 : 256 / 273 = 93 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 21 : 233 / 274 = 85 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 22 : 249 / 274 = 90 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 23 : 248 / 276 = 89 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 24 : 245 / 274 = 89 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 25 : 268 / 275 = 97 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 26 : 270 / 276 = 97 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 27 : 223 / 275 = 81 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 28 : 164 / 275 = 59 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 29 : 169 / 275 = 61 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 30 : 169 / 276 = 61 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 31 : 231 / 276 = 83 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 32 : 244 / 276 = 88 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 33 : 231 / 276 = 83 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 34 : 241 / 276 = 87 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 35 : 242 / 275 = 88 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 36 : 215 / 276 = 77 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 37 : 253 / 276 = 91 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 38 : 243 / 276 = 88 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 39 : 239 / 276 = 86 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 40 : 197 / 276 = 71 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 41 : 262 / 276 = 94 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 42 : 253 / 275 = 92 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 43 : 194 / 276 = 70 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 44 : 260 / 276 = 94 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 45 : 258 / 276 = 93 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 46 : 227 / 276 = 82 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 47 : 207 / 275 = 75 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 48 : 224 / 275 = 81 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 49 : 220 / 274 = 80 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 50 : 238 / 276 = 86 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 51 : 257 / 276 = 93 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 52 : 246 / 276 = 89 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 53 : 245 / 276 = 88 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 54 : 261 / 274 = 95 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 55 : 239 / 276 = 86 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 56 : 251 / 275 = 91 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 57 : 270 / 276 = 97 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 58 : 263 / 273 = 96 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 59 : 250 / 276 = 90 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 60 : 468 / 561 = 83 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 61 : 473 / 566 = 83 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 62 : 404 / 572 = 70 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 63 : 480 / 570 = 84 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 64 : 402 / 574 = 70 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 65 : 499 / 573 = 87 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 66 : 396 / 573 = 69 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 67 : 394 / 575 = 68 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 68 : 371 / 575 = 64 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 69 : 475 / 575 = 82 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 70 : 250 / 575 = 43 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 71 : 209 / 575 = 36 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 72 : 87 / 571 = 15 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 73 : 288 / 570 = 50 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 74 : 346 / 569 = 60 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 75 : 372 / 573 = 64 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 76 : 382 / 574 = 66 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 77 : 399 / 573 = 69 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 78 : 440 / 575 = 76 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 79 : 538 / 574 = 93 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 80 : 458 / 573 = 79 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 81 : 340 / 575 = 59 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 82 : 325 / 575 = 56 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 83 : 270 / 572 = 47 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 84 : 417 / 574 = 72 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 85 : 384 / 574 = 66 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 86 : 496 / 575 = 86 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 87 : 479 / 576 = 83 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 88 : 422 / 575 = 73 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 89 : 442 / 576 = 76 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 90 : 264 / 574 = 45 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 91 : 438 / 568 = 77 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 92 : 426 / 576 = 73 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 93 : 386 / 573 = 67 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 94 : 515 / 574 = 89 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 95 : 536 / 575 = 93 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 96 : 559 / 575 = 97 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 97 : 551 / 574 = 95 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 98 : 540 / 575 = 93 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 99 : 535 / 574 = 93 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 100 : 457 / 574 = 79 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 101 : 527 / 574 = 91 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 102 : 358 / 575 = 62 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 103 : 499 / 576 = 86 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 104 : 281 / 575 = 48 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 105 : 263 / 575 = 45 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 106 : 333 / 576 = 57 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 107 : 505 / 576 = 87 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 108 : 472 / 575 = 82 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 109 : 391 / 575 = 68 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 110 : 502 / 575 = 87 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 111 : 539 / 576 = 93 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 112 : 536 / 575 = 93 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 113 : 511 / 576 = 88 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 114 : 517 / 576 = 89 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 115 : 527 / 576 = 91 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 116 : 479 / 575 = 83 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 117 : 481 / 575 = 83 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 118 : 476 / 575 = 82 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 119 : 506 / 576 = 87 %
[ Tue Jul  9 03:31:45 2024 ] Accuracy of 120 : 240 / 274 = 87 %
[ Tue Jul  9 03:31:45 2024 ] Training epoch: 81
[ Tue Jul  9 03:31:45 2024 ] 	Batch(0/7879) done. Loss: 0.0234  lr:0.000100
[ Tue Jul  9 03:32:08 2024 ] 	Batch(100/7879) done. Loss: 0.0043  lr:0.000100
[ Tue Jul  9 03:32:31 2024 ] 	Batch(200/7879) done. Loss: 0.0336  lr:0.000100
[ Tue Jul  9 03:32:53 2024 ] 	Batch(300/7879) done. Loss: 0.1052  lr:0.000100
[ Tue Jul  9 03:33:16 2024 ] 	Batch(400/7879) done. Loss: 0.3526  lr:0.000100
[ Tue Jul  9 03:33:39 2024 ] 
Training: Epoch [80/120], Step [499], Loss: 0.03849974274635315, Training Accuracy: 95.5
[ Tue Jul  9 03:33:39 2024 ] 	Batch(500/7879) done. Loss: 0.0516  lr:0.000100
[ Tue Jul  9 03:34:02 2024 ] 	Batch(600/7879) done. Loss: 0.1798  lr:0.000100
[ Tue Jul  9 03:34:24 2024 ] 	Batch(700/7879) done. Loss: 0.5967  lr:0.000100
[ Tue Jul  9 03:34:47 2024 ] 	Batch(800/7879) done. Loss: 0.5159  lr:0.000100
[ Tue Jul  9 03:35:10 2024 ] 	Batch(900/7879) done. Loss: 0.0954  lr:0.000100
[ Tue Jul  9 03:35:32 2024 ] 
Training: Epoch [80/120], Step [999], Loss: 0.023940715938806534, Training Accuracy: 95.5125
[ Tue Jul  9 03:35:33 2024 ] 	Batch(1000/7879) done. Loss: 0.3996  lr:0.000100
[ Tue Jul  9 03:35:55 2024 ] 	Batch(1100/7879) done. Loss: 0.5259  lr:0.000100
[ Tue Jul  9 03:36:18 2024 ] 	Batch(1200/7879) done. Loss: 0.0055  lr:0.000100
[ Tue Jul  9 03:36:41 2024 ] 	Batch(1300/7879) done. Loss: 0.0938  lr:0.000100
[ Tue Jul  9 03:37:03 2024 ] 	Batch(1400/7879) done. Loss: 0.0806  lr:0.000100
[ Tue Jul  9 03:37:26 2024 ] 
Training: Epoch [80/120], Step [1499], Loss: 0.1098327487707138, Training Accuracy: 95.72500000000001
[ Tue Jul  9 03:37:26 2024 ] 	Batch(1500/7879) done. Loss: 0.2675  lr:0.000100
[ Tue Jul  9 03:37:49 2024 ] 	Batch(1600/7879) done. Loss: 0.0638  lr:0.000100
[ Tue Jul  9 03:38:12 2024 ] 	Batch(1700/7879) done. Loss: 0.0553  lr:0.000100
[ Tue Jul  9 03:38:34 2024 ] 	Batch(1800/7879) done. Loss: 0.0425  lr:0.000100
[ Tue Jul  9 03:38:57 2024 ] 	Batch(1900/7879) done. Loss: 0.1323  lr:0.000100
[ Tue Jul  9 03:39:19 2024 ] 
Training: Epoch [80/120], Step [1999], Loss: 0.1732996255159378, Training Accuracy: 95.7375
[ Tue Jul  9 03:39:20 2024 ] 	Batch(2000/7879) done. Loss: 0.1107  lr:0.000100
[ Tue Jul  9 03:39:42 2024 ] 	Batch(2100/7879) done. Loss: 0.0314  lr:0.000100
[ Tue Jul  9 03:40:05 2024 ] 	Batch(2200/7879) done. Loss: 0.2271  lr:0.000100
[ Tue Jul  9 03:40:28 2024 ] 	Batch(2300/7879) done. Loss: 0.3272  lr:0.000100
[ Tue Jul  9 03:40:51 2024 ] 	Batch(2400/7879) done. Loss: 0.0966  lr:0.000100
[ Tue Jul  9 03:41:14 2024 ] 
Training: Epoch [80/120], Step [2499], Loss: 0.06592658907175064, Training Accuracy: 95.83
[ Tue Jul  9 03:41:14 2024 ] 	Batch(2500/7879) done. Loss: 0.0090  lr:0.000100
[ Tue Jul  9 03:41:37 2024 ] 	Batch(2600/7879) done. Loss: 0.4960  lr:0.000100
[ Tue Jul  9 03:42:00 2024 ] 	Batch(2700/7879) done. Loss: 0.0942  lr:0.000100
[ Tue Jul  9 03:42:23 2024 ] 	Batch(2800/7879) done. Loss: 0.0519  lr:0.000100
[ Tue Jul  9 03:42:46 2024 ] 	Batch(2900/7879) done. Loss: 0.3236  lr:0.000100
[ Tue Jul  9 03:43:08 2024 ] 
Training: Epoch [80/120], Step [2999], Loss: 0.0022225119173526764, Training Accuracy: 95.825
[ Tue Jul  9 03:43:08 2024 ] 	Batch(3000/7879) done. Loss: 0.0260  lr:0.000100
[ Tue Jul  9 03:43:31 2024 ] 	Batch(3100/7879) done. Loss: 0.1711  lr:0.000100
[ Tue Jul  9 03:43:54 2024 ] 	Batch(3200/7879) done. Loss: 0.3087  lr:0.000100
[ Tue Jul  9 03:44:17 2024 ] 	Batch(3300/7879) done. Loss: 0.0101  lr:0.000100
[ Tue Jul  9 03:44:40 2024 ] 	Batch(3400/7879) done. Loss: 0.0814  lr:0.000100
[ Tue Jul  9 03:45:03 2024 ] 
Training: Epoch [80/120], Step [3499], Loss: 0.10593702644109726, Training Accuracy: 95.83214285714286
[ Tue Jul  9 03:45:04 2024 ] 	Batch(3500/7879) done. Loss: 0.2893  lr:0.000100
[ Tue Jul  9 03:45:27 2024 ] 	Batch(3600/7879) done. Loss: 0.2165  lr:0.000100
[ Tue Jul  9 03:45:49 2024 ] 	Batch(3700/7879) done. Loss: 0.8646  lr:0.000100
[ Tue Jul  9 03:46:12 2024 ] 	Batch(3800/7879) done. Loss: 0.0870  lr:0.000100
[ Tue Jul  9 03:46:35 2024 ] 	Batch(3900/7879) done. Loss: 0.0860  lr:0.000100
[ Tue Jul  9 03:46:57 2024 ] 
Training: Epoch [80/120], Step [3999], Loss: 0.09568280726671219, Training Accuracy: 95.803125
[ Tue Jul  9 03:46:57 2024 ] 	Batch(4000/7879) done. Loss: 0.1666  lr:0.000100
[ Tue Jul  9 03:47:21 2024 ] 	Batch(4100/7879) done. Loss: 0.0016  lr:0.000100
[ Tue Jul  9 03:47:44 2024 ] 	Batch(4200/7879) done. Loss: 0.0640  lr:0.000100
[ Tue Jul  9 03:48:08 2024 ] 	Batch(4300/7879) done. Loss: 0.2794  lr:0.000100
[ Tue Jul  9 03:48:31 2024 ] 	Batch(4400/7879) done. Loss: 0.0501  lr:0.000100
[ Tue Jul  9 03:48:54 2024 ] 
Training: Epoch [80/120], Step [4499], Loss: 0.17149627208709717, Training Accuracy: 95.80833333333332
[ Tue Jul  9 03:48:54 2024 ] 	Batch(4500/7879) done. Loss: 0.1133  lr:0.000100
[ Tue Jul  9 03:49:17 2024 ] 	Batch(4600/7879) done. Loss: 0.0132  lr:0.000100
[ Tue Jul  9 03:49:39 2024 ] 	Batch(4700/7879) done. Loss: 0.0499  lr:0.000100
[ Tue Jul  9 03:50:02 2024 ] 	Batch(4800/7879) done. Loss: 0.1021  lr:0.000100
[ Tue Jul  9 03:50:26 2024 ] 	Batch(4900/7879) done. Loss: 0.0051  lr:0.000100
[ Tue Jul  9 03:50:49 2024 ] 
Training: Epoch [80/120], Step [4999], Loss: 0.15277527272701263, Training Accuracy: 95.7825
[ Tue Jul  9 03:50:49 2024 ] 	Batch(5000/7879) done. Loss: 0.0072  lr:0.000100
[ Tue Jul  9 03:51:13 2024 ] 	Batch(5100/7879) done. Loss: 0.1073  lr:0.000100
[ Tue Jul  9 03:51:36 2024 ] 	Batch(5200/7879) done. Loss: 0.0040  lr:0.000100
[ Tue Jul  9 03:51:59 2024 ] 	Batch(5300/7879) done. Loss: 0.1459  lr:0.000100
[ Tue Jul  9 03:52:22 2024 ] 	Batch(5400/7879) done. Loss: 0.1849  lr:0.000100
[ Tue Jul  9 03:52:44 2024 ] 
Training: Epoch [80/120], Step [5499], Loss: 0.003640176262706518, Training Accuracy: 95.79318181818182
[ Tue Jul  9 03:52:44 2024 ] 	Batch(5500/7879) done. Loss: 0.2118  lr:0.000100
[ Tue Jul  9 03:53:07 2024 ] 	Batch(5600/7879) done. Loss: 0.0262  lr:0.000100
[ Tue Jul  9 03:53:30 2024 ] 	Batch(5700/7879) done. Loss: 0.3753  lr:0.000100
[ Tue Jul  9 03:53:53 2024 ] 	Batch(5800/7879) done. Loss: 0.0655  lr:0.000100
[ Tue Jul  9 03:54:15 2024 ] 	Batch(5900/7879) done. Loss: 0.0364  lr:0.000100
[ Tue Jul  9 03:54:38 2024 ] 
Training: Epoch [80/120], Step [5999], Loss: 0.2588871717453003, Training Accuracy: 95.78750000000001
[ Tue Jul  9 03:54:38 2024 ] 	Batch(6000/7879) done. Loss: 0.0027  lr:0.000100
[ Tue Jul  9 03:55:01 2024 ] 	Batch(6100/7879) done. Loss: 0.0063  lr:0.000100
[ Tue Jul  9 03:55:24 2024 ] 	Batch(6200/7879) done. Loss: 0.0332  lr:0.000100
[ Tue Jul  9 03:55:46 2024 ] 	Batch(6300/7879) done. Loss: 0.0068  lr:0.000100
[ Tue Jul  9 03:56:09 2024 ] 	Batch(6400/7879) done. Loss: 0.2467  lr:0.000100
[ Tue Jul  9 03:56:32 2024 ] 
Training: Epoch [80/120], Step [6499], Loss: 0.004611183889210224, Training Accuracy: 95.79807692307692
[ Tue Jul  9 03:56:32 2024 ] 	Batch(6500/7879) done. Loss: 0.2343  lr:0.000100
[ Tue Jul  9 03:56:54 2024 ] 	Batch(6600/7879) done. Loss: 0.2383  lr:0.000100
[ Tue Jul  9 03:57:17 2024 ] 	Batch(6700/7879) done. Loss: 0.2759  lr:0.000100
[ Tue Jul  9 03:57:40 2024 ] 	Batch(6800/7879) done. Loss: 0.0206  lr:0.000100
[ Tue Jul  9 03:58:03 2024 ] 	Batch(6900/7879) done. Loss: 0.0323  lr:0.000100
[ Tue Jul  9 03:58:27 2024 ] 
Training: Epoch [80/120], Step [6999], Loss: 0.02309795655310154, Training Accuracy: 95.78571428571429
[ Tue Jul  9 03:58:27 2024 ] 	Batch(7000/7879) done. Loss: 0.1024  lr:0.000100
[ Tue Jul  9 03:58:50 2024 ] 	Batch(7100/7879) done. Loss: 0.4210  lr:0.000100
[ Tue Jul  9 03:59:13 2024 ] 	Batch(7200/7879) done. Loss: 0.0357  lr:0.000100
[ Tue Jul  9 03:59:36 2024 ] 	Batch(7300/7879) done. Loss: 0.2130  lr:0.000100
[ Tue Jul  9 03:59:58 2024 ] 	Batch(7400/7879) done. Loss: 0.4687  lr:0.000100
[ Tue Jul  9 04:00:21 2024 ] 
Training: Epoch [80/120], Step [7499], Loss: 0.07140252739191055, Training Accuracy: 95.78666666666666
[ Tue Jul  9 04:00:21 2024 ] 	Batch(7500/7879) done. Loss: 0.0385  lr:0.000100
[ Tue Jul  9 04:00:44 2024 ] 	Batch(7600/7879) done. Loss: 0.1734  lr:0.000100
[ Tue Jul  9 04:01:07 2024 ] 	Batch(7700/7879) done. Loss: 0.0418  lr:0.000100
[ Tue Jul  9 04:01:30 2024 ] 	Batch(7800/7879) done. Loss: 0.1105  lr:0.000100
[ Tue Jul  9 04:01:47 2024 ] 	Mean training loss: 0.1554.
[ Tue Jul  9 04:01:47 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 04:01:47 2024 ] Training epoch: 82
[ Tue Jul  9 04:01:48 2024 ] 	Batch(0/7879) done. Loss: 0.4397  lr:0.000100
[ Tue Jul  9 04:02:11 2024 ] 	Batch(100/7879) done. Loss: 0.0036  lr:0.000100
[ Tue Jul  9 04:02:33 2024 ] 	Batch(200/7879) done. Loss: 0.0176  lr:0.000100
[ Tue Jul  9 04:02:56 2024 ] 	Batch(300/7879) done. Loss: 0.0297  lr:0.000100
[ Tue Jul  9 04:03:19 2024 ] 	Batch(400/7879) done. Loss: 0.0135  lr:0.000100
[ Tue Jul  9 04:03:41 2024 ] 
Training: Epoch [81/120], Step [499], Loss: 0.3104400038719177, Training Accuracy: 95.775
[ Tue Jul  9 04:03:41 2024 ] 	Batch(500/7879) done. Loss: 0.0679  lr:0.000100
[ Tue Jul  9 04:04:04 2024 ] 	Batch(600/7879) done. Loss: 0.0085  lr:0.000100
[ Tue Jul  9 04:04:27 2024 ] 	Batch(700/7879) done. Loss: 0.2337  lr:0.000100
[ Tue Jul  9 04:04:50 2024 ] 	Batch(800/7879) done. Loss: 0.8489  lr:0.000100
[ Tue Jul  9 04:05:12 2024 ] 	Batch(900/7879) done. Loss: 0.4308  lr:0.000100
[ Tue Jul  9 04:05:35 2024 ] 
Training: Epoch [81/120], Step [999], Loss: 0.0045472015626728535, Training Accuracy: 95.85000000000001
[ Tue Jul  9 04:05:35 2024 ] 	Batch(1000/7879) done. Loss: 0.0156  lr:0.000100
[ Tue Jul  9 04:05:58 2024 ] 	Batch(1100/7879) done. Loss: 0.0350  lr:0.000100
[ Tue Jul  9 04:06:20 2024 ] 	Batch(1200/7879) done. Loss: 0.1370  lr:0.000100
[ Tue Jul  9 04:06:43 2024 ] 	Batch(1300/7879) done. Loss: 0.0147  lr:0.000100
[ Tue Jul  9 04:07:06 2024 ] 	Batch(1400/7879) done. Loss: 0.3002  lr:0.000100
[ Tue Jul  9 04:07:28 2024 ] 
Training: Epoch [81/120], Step [1499], Loss: 0.7253103256225586, Training Accuracy: 95.86666666666666
[ Tue Jul  9 04:07:28 2024 ] 	Batch(1500/7879) done. Loss: 0.1672  lr:0.000100
[ Tue Jul  9 04:07:51 2024 ] 	Batch(1600/7879) done. Loss: 0.0254  lr:0.000100
[ Tue Jul  9 04:08:14 2024 ] 	Batch(1700/7879) done. Loss: 0.1105  lr:0.000100
[ Tue Jul  9 04:08:37 2024 ] 	Batch(1800/7879) done. Loss: 0.0130  lr:0.000100
[ Tue Jul  9 04:08:59 2024 ] 	Batch(1900/7879) done. Loss: 0.0242  lr:0.000100
[ Tue Jul  9 04:09:22 2024 ] 
Training: Epoch [81/120], Step [1999], Loss: 0.029959850013256073, Training Accuracy: 95.71875
[ Tue Jul  9 04:09:22 2024 ] 	Batch(2000/7879) done. Loss: 0.0052  lr:0.000100
[ Tue Jul  9 04:09:45 2024 ] 	Batch(2100/7879) done. Loss: 0.0835  lr:0.000100
[ Tue Jul  9 04:10:08 2024 ] 	Batch(2200/7879) done. Loss: 0.0174  lr:0.000100
[ Tue Jul  9 04:10:31 2024 ] 	Batch(2300/7879) done. Loss: 0.3949  lr:0.000100
[ Tue Jul  9 04:10:54 2024 ] 	Batch(2400/7879) done. Loss: 0.1330  lr:0.000100
[ Tue Jul  9 04:11:17 2024 ] 
Training: Epoch [81/120], Step [2499], Loss: 0.5342721343040466, Training Accuracy: 95.61500000000001
[ Tue Jul  9 04:11:17 2024 ] 	Batch(2500/7879) done. Loss: 0.0360  lr:0.000100
[ Tue Jul  9 04:11:40 2024 ] 	Batch(2600/7879) done. Loss: 0.3660  lr:0.000100
[ Tue Jul  9 04:12:04 2024 ] 	Batch(2700/7879) done. Loss: 0.0314  lr:0.000100
[ Tue Jul  9 04:12:27 2024 ] 	Batch(2800/7879) done. Loss: 0.2467  lr:0.000100
[ Tue Jul  9 04:12:51 2024 ] 	Batch(2900/7879) done. Loss: 0.0430  lr:0.000100
[ Tue Jul  9 04:13:14 2024 ] 
Training: Epoch [81/120], Step [2999], Loss: 0.0047855800949037075, Training Accuracy: 95.7
[ Tue Jul  9 04:13:14 2024 ] 	Batch(3000/7879) done. Loss: 0.0902  lr:0.000100
[ Tue Jul  9 04:13:38 2024 ] 	Batch(3100/7879) done. Loss: 0.1113  lr:0.000100
[ Tue Jul  9 04:14:01 2024 ] 	Batch(3200/7879) done. Loss: 0.1308  lr:0.000100
[ Tue Jul  9 04:14:25 2024 ] 	Batch(3300/7879) done. Loss: 0.4093  lr:0.000100
[ Tue Jul  9 04:14:48 2024 ] 	Batch(3400/7879) done. Loss: 0.0202  lr:0.000100
[ Tue Jul  9 04:15:12 2024 ] 
Training: Epoch [81/120], Step [3499], Loss: 0.044848598539829254, Training Accuracy: 95.63928571428572
[ Tue Jul  9 04:15:12 2024 ] 	Batch(3500/7879) done. Loss: 0.0062  lr:0.000100
[ Tue Jul  9 04:15:35 2024 ] 	Batch(3600/7879) done. Loss: 0.4856  lr:0.000100
[ Tue Jul  9 04:15:59 2024 ] 	Batch(3700/7879) done. Loss: 0.1129  lr:0.000100
[ Tue Jul  9 04:16:21 2024 ] 	Batch(3800/7879) done. Loss: 0.1814  lr:0.000100
[ Tue Jul  9 04:16:44 2024 ] 	Batch(3900/7879) done. Loss: 0.0456  lr:0.000100
[ Tue Jul  9 04:17:07 2024 ] 
Training: Epoch [81/120], Step [3999], Loss: 0.1092795580625534, Training Accuracy: 95.66875
[ Tue Jul  9 04:17:07 2024 ] 	Batch(4000/7879) done. Loss: 0.1670  lr:0.000100
[ Tue Jul  9 04:17:30 2024 ] 	Batch(4100/7879) done. Loss: 0.1271  lr:0.000100
[ Tue Jul  9 04:17:52 2024 ] 	Batch(4200/7879) done. Loss: 0.0932  lr:0.000100
[ Tue Jul  9 04:18:15 2024 ] 	Batch(4300/7879) done. Loss: 0.1150  lr:0.000100
[ Tue Jul  9 04:18:38 2024 ] 	Batch(4400/7879) done. Loss: 0.5451  lr:0.000100
[ Tue Jul  9 04:19:00 2024 ] 
Training: Epoch [81/120], Step [4499], Loss: 0.029171129688620567, Training Accuracy: 95.7
[ Tue Jul  9 04:19:01 2024 ] 	Batch(4500/7879) done. Loss: 0.0765  lr:0.000100
[ Tue Jul  9 04:19:24 2024 ] 	Batch(4600/7879) done. Loss: 0.0440  lr:0.000100
[ Tue Jul  9 04:19:46 2024 ] 	Batch(4700/7879) done. Loss: 0.4496  lr:0.000100
[ Tue Jul  9 04:20:10 2024 ] 	Batch(4800/7879) done. Loss: 0.6159  lr:0.000100
[ Tue Jul  9 04:20:32 2024 ] 	Batch(4900/7879) done. Loss: 0.1035  lr:0.000100
[ Tue Jul  9 04:20:55 2024 ] 
Training: Epoch [81/120], Step [4999], Loss: 0.20332756638526917, Training Accuracy: 95.6975
[ Tue Jul  9 04:20:55 2024 ] 	Batch(5000/7879) done. Loss: 0.4476  lr:0.000100
[ Tue Jul  9 04:21:18 2024 ] 	Batch(5100/7879) done. Loss: 0.2022  lr:0.000100
[ Tue Jul  9 04:21:40 2024 ] 	Batch(5200/7879) done. Loss: 0.4376  lr:0.000100
[ Tue Jul  9 04:22:03 2024 ] 	Batch(5300/7879) done. Loss: 0.0435  lr:0.000100
[ Tue Jul  9 04:22:26 2024 ] 	Batch(5400/7879) done. Loss: 0.0518  lr:0.000100
[ Tue Jul  9 04:22:48 2024 ] 
Training: Epoch [81/120], Step [5499], Loss: 0.4636480510234833, Training Accuracy: 95.73409090909091
[ Tue Jul  9 04:22:48 2024 ] 	Batch(5500/7879) done. Loss: 0.0280  lr:0.000100
[ Tue Jul  9 04:23:11 2024 ] 	Batch(5600/7879) done. Loss: 0.0205  lr:0.000100
[ Tue Jul  9 04:23:35 2024 ] 	Batch(5700/7879) done. Loss: 0.3104  lr:0.000100
[ Tue Jul  9 04:23:57 2024 ] 	Batch(5800/7879) done. Loss: 0.2486  lr:0.000100
[ Tue Jul  9 04:24:20 2024 ] 	Batch(5900/7879) done. Loss: 0.0312  lr:0.000100
[ Tue Jul  9 04:24:42 2024 ] 
Training: Epoch [81/120], Step [5999], Loss: 0.29916083812713623, Training Accuracy: 95.73333333333333
[ Tue Jul  9 04:24:43 2024 ] 	Batch(6000/7879) done. Loss: 0.4503  lr:0.000100
[ Tue Jul  9 04:25:06 2024 ] 	Batch(6100/7879) done. Loss: 0.2896  lr:0.000100
[ Tue Jul  9 04:25:29 2024 ] 	Batch(6200/7879) done. Loss: 0.0776  lr:0.000100
[ Tue Jul  9 04:25:52 2024 ] 	Batch(6300/7879) done. Loss: 0.1742  lr:0.000100
[ Tue Jul  9 04:26:15 2024 ] 	Batch(6400/7879) done. Loss: 0.0714  lr:0.000100
[ Tue Jul  9 04:26:38 2024 ] 
Training: Epoch [81/120], Step [6499], Loss: 0.5370870232582092, Training Accuracy: 95.75576923076923
[ Tue Jul  9 04:26:38 2024 ] 	Batch(6500/7879) done. Loss: 0.1171  lr:0.000100
[ Tue Jul  9 04:27:01 2024 ] 	Batch(6600/7879) done. Loss: 0.1814  lr:0.000100
[ Tue Jul  9 04:27:24 2024 ] 	Batch(6700/7879) done. Loss: 0.0426  lr:0.000100
[ Tue Jul  9 04:27:47 2024 ] 	Batch(6800/7879) done. Loss: 0.0968  lr:0.000100
[ Tue Jul  9 04:28:09 2024 ] 	Batch(6900/7879) done. Loss: 0.0945  lr:0.000100
[ Tue Jul  9 04:28:32 2024 ] 
Training: Epoch [81/120], Step [6999], Loss: 0.12642717361450195, Training Accuracy: 95.76428571428572
[ Tue Jul  9 04:28:32 2024 ] 	Batch(7000/7879) done. Loss: 0.0078  lr:0.000100
[ Tue Jul  9 04:28:55 2024 ] 	Batch(7100/7879) done. Loss: 0.0780  lr:0.000100
[ Tue Jul  9 04:29:18 2024 ] 	Batch(7200/7879) done. Loss: 0.8029  lr:0.000100
[ Tue Jul  9 04:29:41 2024 ] 	Batch(7300/7879) done. Loss: 0.0261  lr:0.000100
[ Tue Jul  9 04:30:05 2024 ] 	Batch(7400/7879) done. Loss: 0.0031  lr:0.000100
[ Tue Jul  9 04:30:27 2024 ] 
Training: Epoch [81/120], Step [7499], Loss: 0.07222844660282135, Training Accuracy: 95.76166666666667
[ Tue Jul  9 04:30:27 2024 ] 	Batch(7500/7879) done. Loss: 0.0751  lr:0.000100
[ Tue Jul  9 04:30:50 2024 ] 	Batch(7600/7879) done. Loss: 0.4273  lr:0.000100
[ Tue Jul  9 04:31:13 2024 ] 	Batch(7700/7879) done. Loss: 0.1255  lr:0.000100
[ Tue Jul  9 04:31:36 2024 ] 	Batch(7800/7879) done. Loss: 0.2459  lr:0.000100
[ Tue Jul  9 04:31:53 2024 ] 	Mean training loss: 0.1541.
[ Tue Jul  9 04:31:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 04:31:53 2024 ] Training epoch: 83
[ Tue Jul  9 04:31:54 2024 ] 	Batch(0/7879) done. Loss: 0.0207  lr:0.000100
[ Tue Jul  9 04:32:17 2024 ] 	Batch(100/7879) done. Loss: 0.0303  lr:0.000100
[ Tue Jul  9 04:32:40 2024 ] 	Batch(200/7879) done. Loss: 0.1016  lr:0.000100
[ Tue Jul  9 04:33:02 2024 ] 	Batch(300/7879) done. Loss: 0.1146  lr:0.000100
[ Tue Jul  9 04:33:25 2024 ] 	Batch(400/7879) done. Loss: 0.3375  lr:0.000100
[ Tue Jul  9 04:33:48 2024 ] 
Training: Epoch [82/120], Step [499], Loss: 0.022453680634498596, Training Accuracy: 95.89999999999999
[ Tue Jul  9 04:33:48 2024 ] 	Batch(500/7879) done. Loss: 0.1121  lr:0.000100
[ Tue Jul  9 04:34:11 2024 ] 	Batch(600/7879) done. Loss: 0.0898  lr:0.000100
[ Tue Jul  9 04:34:33 2024 ] 	Batch(700/7879) done. Loss: 0.0339  lr:0.000100
[ Tue Jul  9 04:34:56 2024 ] 	Batch(800/7879) done. Loss: 0.3677  lr:0.000100
[ Tue Jul  9 04:35:20 2024 ] 	Batch(900/7879) done. Loss: 0.1275  lr:0.000100
[ Tue Jul  9 04:35:43 2024 ] 
Training: Epoch [82/120], Step [999], Loss: 0.24670742452144623, Training Accuracy: 96.025
[ Tue Jul  9 04:35:43 2024 ] 	Batch(1000/7879) done. Loss: 0.0505  lr:0.000100
[ Tue Jul  9 04:36:06 2024 ] 	Batch(1100/7879) done. Loss: 0.2009  lr:0.000100
[ Tue Jul  9 04:36:30 2024 ] 	Batch(1200/7879) done. Loss: 0.0536  lr:0.000100
[ Tue Jul  9 04:36:54 2024 ] 	Batch(1300/7879) done. Loss: 0.0667  lr:0.000100
[ Tue Jul  9 04:37:17 2024 ] 	Batch(1400/7879) done. Loss: 0.1234  lr:0.000100
[ Tue Jul  9 04:37:40 2024 ] 
Training: Epoch [82/120], Step [1499], Loss: 0.7408350110054016, Training Accuracy: 96.01666666666667
[ Tue Jul  9 04:37:40 2024 ] 	Batch(1500/7879) done. Loss: 0.2093  lr:0.000100
[ Tue Jul  9 04:38:04 2024 ] 	Batch(1600/7879) done. Loss: 0.2496  lr:0.000100
[ Tue Jul  9 04:38:27 2024 ] 	Batch(1700/7879) done. Loss: 0.3727  lr:0.000100
[ Tue Jul  9 04:38:49 2024 ] 	Batch(1800/7879) done. Loss: 0.1420  lr:0.000100
[ Tue Jul  9 04:39:12 2024 ] 	Batch(1900/7879) done. Loss: 0.0245  lr:0.000100
[ Tue Jul  9 04:39:35 2024 ] 
Training: Epoch [82/120], Step [1999], Loss: 0.1769430935382843, Training Accuracy: 96.0
[ Tue Jul  9 04:39:35 2024 ] 	Batch(2000/7879) done. Loss: 0.0181  lr:0.000100
[ Tue Jul  9 04:39:58 2024 ] 	Batch(2100/7879) done. Loss: 0.2912  lr:0.000100
[ Tue Jul  9 04:40:20 2024 ] 	Batch(2200/7879) done. Loss: 0.0590  lr:0.000100
[ Tue Jul  9 04:40:43 2024 ] 	Batch(2300/7879) done. Loss: 0.4334  lr:0.000100
[ Tue Jul  9 04:41:06 2024 ] 	Batch(2400/7879) done. Loss: 0.0605  lr:0.000100
[ Tue Jul  9 04:41:28 2024 ] 
Training: Epoch [82/120], Step [2499], Loss: 0.7012026309967041, Training Accuracy: 95.94500000000001
[ Tue Jul  9 04:41:29 2024 ] 	Batch(2500/7879) done. Loss: 0.1644  lr:0.000100
[ Tue Jul  9 04:41:51 2024 ] 	Batch(2600/7879) done. Loss: 0.0424  lr:0.000100
[ Tue Jul  9 04:42:14 2024 ] 	Batch(2700/7879) done. Loss: 0.0949  lr:0.000100
[ Tue Jul  9 04:42:37 2024 ] 	Batch(2800/7879) done. Loss: 0.3192  lr:0.000100
[ Tue Jul  9 04:43:00 2024 ] 	Batch(2900/7879) done. Loss: 0.6492  lr:0.000100
[ Tue Jul  9 04:43:22 2024 ] 
Training: Epoch [82/120], Step [2999], Loss: 0.10995616763830185, Training Accuracy: 96.00416666666666
[ Tue Jul  9 04:43:22 2024 ] 	Batch(3000/7879) done. Loss: 0.1083  lr:0.000100
[ Tue Jul  9 04:43:45 2024 ] 	Batch(3100/7879) done. Loss: 0.0835  lr:0.000100
[ Tue Jul  9 04:44:08 2024 ] 	Batch(3200/7879) done. Loss: 0.0361  lr:0.000100
[ Tue Jul  9 04:44:31 2024 ] 	Batch(3300/7879) done. Loss: 0.3451  lr:0.000100
[ Tue Jul  9 04:44:53 2024 ] 	Batch(3400/7879) done. Loss: 0.0224  lr:0.000100
[ Tue Jul  9 04:45:16 2024 ] 
Training: Epoch [82/120], Step [3499], Loss: 0.29295507073402405, Training Accuracy: 96.02142857142857
[ Tue Jul  9 04:45:16 2024 ] 	Batch(3500/7879) done. Loss: 0.2612  lr:0.000100
[ Tue Jul  9 04:45:39 2024 ] 	Batch(3600/7879) done. Loss: 0.0384  lr:0.000100
[ Tue Jul  9 04:46:02 2024 ] 	Batch(3700/7879) done. Loss: 0.1427  lr:0.000100
[ Tue Jul  9 04:46:24 2024 ] 	Batch(3800/7879) done. Loss: 0.1296  lr:0.000100
[ Tue Jul  9 04:46:47 2024 ] 	Batch(3900/7879) done. Loss: 0.2616  lr:0.000100
[ Tue Jul  9 04:47:09 2024 ] 
Training: Epoch [82/120], Step [3999], Loss: 0.06238622963428497, Training Accuracy: 96.08125
[ Tue Jul  9 04:47:10 2024 ] 	Batch(4000/7879) done. Loss: 0.1901  lr:0.000100
[ Tue Jul  9 04:47:32 2024 ] 	Batch(4100/7879) done. Loss: 0.1696  lr:0.000100
[ Tue Jul  9 04:47:55 2024 ] 	Batch(4200/7879) done. Loss: 0.1925  lr:0.000100
[ Tue Jul  9 04:48:18 2024 ] 	Batch(4300/7879) done. Loss: 0.0058  lr:0.000100
[ Tue Jul  9 04:48:41 2024 ] 	Batch(4400/7879) done. Loss: 0.0606  lr:0.000100
[ Tue Jul  9 04:49:04 2024 ] 
Training: Epoch [82/120], Step [4499], Loss: 0.2912144958972931, Training Accuracy: 95.96111111111111
[ Tue Jul  9 04:49:04 2024 ] 	Batch(4500/7879) done. Loss: 0.5425  lr:0.000100
[ Tue Jul  9 04:49:28 2024 ] 	Batch(4600/7879) done. Loss: 0.0772  lr:0.000100
[ Tue Jul  9 04:49:51 2024 ] 	Batch(4700/7879) done. Loss: 0.0679  lr:0.000100
[ Tue Jul  9 04:50:15 2024 ] 	Batch(4800/7879) done. Loss: 0.0318  lr:0.000100
[ Tue Jul  9 04:50:39 2024 ] 	Batch(4900/7879) done. Loss: 0.0469  lr:0.000100
[ Tue Jul  9 04:51:02 2024 ] 
Training: Epoch [82/120], Step [4999], Loss: 0.06867796182632446, Training Accuracy: 95.92500000000001
[ Tue Jul  9 04:51:02 2024 ] 	Batch(5000/7879) done. Loss: 0.0365  lr:0.000100
[ Tue Jul  9 04:51:25 2024 ] 	Batch(5100/7879) done. Loss: 0.1151  lr:0.000100
[ Tue Jul  9 04:51:49 2024 ] 	Batch(5200/7879) done. Loss: 0.0018  lr:0.000100
[ Tue Jul  9 04:52:13 2024 ] 	Batch(5300/7879) done. Loss: 0.4027  lr:0.000100
[ Tue Jul  9 04:52:36 2024 ] 	Batch(5400/7879) done. Loss: 0.6338  lr:0.000100
[ Tue Jul  9 04:52:59 2024 ] 
Training: Epoch [82/120], Step [5499], Loss: 0.06158348172903061, Training Accuracy: 95.88409090909092
[ Tue Jul  9 04:52:59 2024 ] 	Batch(5500/7879) done. Loss: 0.1256  lr:0.000100
[ Tue Jul  9 04:53:23 2024 ] 	Batch(5600/7879) done. Loss: 0.0631  lr:0.000100
[ Tue Jul  9 04:53:46 2024 ] 	Batch(5700/7879) done. Loss: 0.0099  lr:0.000100
[ Tue Jul  9 04:54:10 2024 ] 	Batch(5800/7879) done. Loss: 0.0606  lr:0.000100
[ Tue Jul  9 04:54:33 2024 ] 	Batch(5900/7879) done. Loss: 0.0364  lr:0.000100
[ Tue Jul  9 04:54:55 2024 ] 
Training: Epoch [82/120], Step [5999], Loss: 0.0059364428743720055, Training Accuracy: 95.89999999999999
[ Tue Jul  9 04:54:56 2024 ] 	Batch(6000/7879) done. Loss: 0.1294  lr:0.000100
[ Tue Jul  9 04:55:19 2024 ] 	Batch(6100/7879) done. Loss: 0.0257  lr:0.000100
[ Tue Jul  9 04:55:43 2024 ] 	Batch(6200/7879) done. Loss: 0.1786  lr:0.000100
[ Tue Jul  9 04:56:05 2024 ] 	Batch(6300/7879) done. Loss: 0.0679  lr:0.000100
[ Tue Jul  9 04:56:28 2024 ] 	Batch(6400/7879) done. Loss: 0.1769  lr:0.000100
[ Tue Jul  9 04:56:51 2024 ] 
Training: Epoch [82/120], Step [6499], Loss: 0.5616573095321655, Training Accuracy: 95.85192307692307
[ Tue Jul  9 04:56:51 2024 ] 	Batch(6500/7879) done. Loss: 0.0456  lr:0.000100
[ Tue Jul  9 04:57:14 2024 ] 	Batch(6600/7879) done. Loss: 0.0103  lr:0.000100
[ Tue Jul  9 04:57:37 2024 ] 	Batch(6700/7879) done. Loss: 0.4943  lr:0.000100
[ Tue Jul  9 04:57:59 2024 ] 	Batch(6800/7879) done. Loss: 0.6329  lr:0.000100
[ Tue Jul  9 04:58:22 2024 ] 	Batch(6900/7879) done. Loss: 0.3994  lr:0.000100
[ Tue Jul  9 04:58:45 2024 ] 
Training: Epoch [82/120], Step [6999], Loss: 0.172071173787117, Training Accuracy: 95.80714285714286
[ Tue Jul  9 04:58:45 2024 ] 	Batch(7000/7879) done. Loss: 0.3433  lr:0.000100
[ Tue Jul  9 04:59:08 2024 ] 	Batch(7100/7879) done. Loss: 0.0876  lr:0.000100
[ Tue Jul  9 04:59:31 2024 ] 	Batch(7200/7879) done. Loss: 0.0179  lr:0.000100
[ Tue Jul  9 04:59:54 2024 ] 	Batch(7300/7879) done. Loss: 0.1590  lr:0.000100
[ Tue Jul  9 05:00:17 2024 ] 	Batch(7400/7879) done. Loss: 0.0432  lr:0.000100
[ Tue Jul  9 05:00:39 2024 ] 
Training: Epoch [82/120], Step [7499], Loss: 0.059718236327171326, Training Accuracy: 95.78333333333333
[ Tue Jul  9 05:00:40 2024 ] 	Batch(7500/7879) done. Loss: 0.0040  lr:0.000100
[ Tue Jul  9 05:01:02 2024 ] 	Batch(7600/7879) done. Loss: 0.0741  lr:0.000100
[ Tue Jul  9 05:01:25 2024 ] 	Batch(7700/7879) done. Loss: 0.0476  lr:0.000100
[ Tue Jul  9 05:01:48 2024 ] 	Batch(7800/7879) done. Loss: 0.0085  lr:0.000100
[ Tue Jul  9 05:02:05 2024 ] 	Mean training loss: 0.1511.
[ Tue Jul  9 05:02:05 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 05:02:06 2024 ] Training epoch: 84
[ Tue Jul  9 05:02:06 2024 ] 	Batch(0/7879) done. Loss: 0.0295  lr:0.000100
[ Tue Jul  9 05:02:29 2024 ] 	Batch(100/7879) done. Loss: 0.3351  lr:0.000100
[ Tue Jul  9 05:02:52 2024 ] 	Batch(200/7879) done. Loss: 0.0899  lr:0.000100
[ Tue Jul  9 05:03:14 2024 ] 	Batch(300/7879) done. Loss: 0.0017  lr:0.000100
[ Tue Jul  9 05:03:37 2024 ] 	Batch(400/7879) done. Loss: 0.3303  lr:0.000100
[ Tue Jul  9 05:04:00 2024 ] 
Training: Epoch [83/120], Step [499], Loss: 0.4007909595966339, Training Accuracy: 95.325
[ Tue Jul  9 05:04:00 2024 ] 	Batch(500/7879) done. Loss: 0.0335  lr:0.000100
[ Tue Jul  9 05:04:22 2024 ] 	Batch(600/7879) done. Loss: 0.2376  lr:0.000100
[ Tue Jul  9 05:04:45 2024 ] 	Batch(700/7879) done. Loss: 0.0908  lr:0.000100
[ Tue Jul  9 05:05:08 2024 ] 	Batch(800/7879) done. Loss: 0.0065  lr:0.000100
[ Tue Jul  9 05:05:31 2024 ] 	Batch(900/7879) done. Loss: 0.5907  lr:0.000100
[ Tue Jul  9 05:05:53 2024 ] 
Training: Epoch [83/120], Step [999], Loss: 0.045797113329172134, Training Accuracy: 95.76249999999999
[ Tue Jul  9 05:05:53 2024 ] 	Batch(1000/7879) done. Loss: 0.1038  lr:0.000100
[ Tue Jul  9 05:06:16 2024 ] 	Batch(1100/7879) done. Loss: 0.3663  lr:0.000100
[ Tue Jul  9 05:06:39 2024 ] 	Batch(1200/7879) done. Loss: 0.3219  lr:0.000100
[ Tue Jul  9 05:07:02 2024 ] 	Batch(1300/7879) done. Loss: 0.3062  lr:0.000100
[ Tue Jul  9 05:07:24 2024 ] 	Batch(1400/7879) done. Loss: 0.1610  lr:0.000100
[ Tue Jul  9 05:07:47 2024 ] 
Training: Epoch [83/120], Step [1499], Loss: 0.41348931193351746, Training Accuracy: 95.73333333333333
[ Tue Jul  9 05:07:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0967  lr:0.000100
[ Tue Jul  9 05:08:10 2024 ] 	Batch(1600/7879) done. Loss: 0.0663  lr:0.000100
[ Tue Jul  9 05:08:33 2024 ] 	Batch(1700/7879) done. Loss: 0.0066  lr:0.000100
[ Tue Jul  9 05:08:57 2024 ] 	Batch(1800/7879) done. Loss: 0.0971  lr:0.000100
[ Tue Jul  9 05:09:20 2024 ] 	Batch(1900/7879) done. Loss: 0.2385  lr:0.000100
[ Tue Jul  9 05:09:43 2024 ] 
Training: Epoch [83/120], Step [1999], Loss: 0.3107072114944458, Training Accuracy: 95.7375
[ Tue Jul  9 05:09:44 2024 ] 	Batch(2000/7879) done. Loss: 0.2491  lr:0.000100
[ Tue Jul  9 05:10:07 2024 ] 	Batch(2100/7879) done. Loss: 0.2628  lr:0.000100
[ Tue Jul  9 05:10:31 2024 ] 	Batch(2200/7879) done. Loss: 0.0968  lr:0.000100
[ Tue Jul  9 05:10:54 2024 ] 	Batch(2300/7879) done. Loss: 0.0367  lr:0.000100
[ Tue Jul  9 05:11:18 2024 ] 	Batch(2400/7879) done. Loss: 0.1465  lr:0.000100
[ Tue Jul  9 05:11:41 2024 ] 
Training: Epoch [83/120], Step [2499], Loss: 0.03204376995563507, Training Accuracy: 95.67999999999999
[ Tue Jul  9 05:11:41 2024 ] 	Batch(2500/7879) done. Loss: 0.0288  lr:0.000100
[ Tue Jul  9 05:12:04 2024 ] 	Batch(2600/7879) done. Loss: 0.4373  lr:0.000100
[ Tue Jul  9 05:12:28 2024 ] 	Batch(2700/7879) done. Loss: 0.3957  lr:0.000100
[ Tue Jul  9 05:12:51 2024 ] 	Batch(2800/7879) done. Loss: 0.0188  lr:0.000100
[ Tue Jul  9 05:13:14 2024 ] 	Batch(2900/7879) done. Loss: 0.0188  lr:0.000100
[ Tue Jul  9 05:13:37 2024 ] 
Training: Epoch [83/120], Step [2999], Loss: 0.011371243745088577, Training Accuracy: 95.66666666666667
[ Tue Jul  9 05:13:37 2024 ] 	Batch(3000/7879) done. Loss: 0.1276  lr:0.000100
[ Tue Jul  9 05:14:00 2024 ] 	Batch(3100/7879) done. Loss: 0.0668  lr:0.000100
[ Tue Jul  9 05:14:23 2024 ] 	Batch(3200/7879) done. Loss: 0.0891  lr:0.000100
[ Tue Jul  9 05:14:47 2024 ] 	Batch(3300/7879) done. Loss: 0.0074  lr:0.000100
[ Tue Jul  9 05:15:10 2024 ] 	Batch(3400/7879) done. Loss: 0.0692  lr:0.000100
[ Tue Jul  9 05:15:34 2024 ] 
Training: Epoch [83/120], Step [3499], Loss: 0.2625146210193634, Training Accuracy: 95.71785714285714
[ Tue Jul  9 05:15:34 2024 ] 	Batch(3500/7879) done. Loss: 0.0228  lr:0.000100
[ Tue Jul  9 05:15:57 2024 ] 	Batch(3600/7879) done. Loss: 0.0048  lr:0.000100
[ Tue Jul  9 05:16:21 2024 ] 	Batch(3700/7879) done. Loss: 0.1058  lr:0.000100
[ Tue Jul  9 05:16:44 2024 ] 	Batch(3800/7879) done. Loss: 0.0363  lr:0.000100
[ Tue Jul  9 05:17:07 2024 ] 	Batch(3900/7879) done. Loss: 0.4484  lr:0.000100
[ Tue Jul  9 05:17:29 2024 ] 
Training: Epoch [83/120], Step [3999], Loss: 0.05710892379283905, Training Accuracy: 95.715625
[ Tue Jul  9 05:17:30 2024 ] 	Batch(4000/7879) done. Loss: 0.0447  lr:0.000100
[ Tue Jul  9 05:17:52 2024 ] 	Batch(4100/7879) done. Loss: 0.1002  lr:0.000100
[ Tue Jul  9 05:18:15 2024 ] 	Batch(4200/7879) done. Loss: 0.0083  lr:0.000100
[ Tue Jul  9 05:18:38 2024 ] 	Batch(4300/7879) done. Loss: 0.1022  lr:0.000100
[ Tue Jul  9 05:19:01 2024 ] 	Batch(4400/7879) done. Loss: 0.0518  lr:0.000100
[ Tue Jul  9 05:19:23 2024 ] 
Training: Epoch [83/120], Step [4499], Loss: 0.35271915793418884, Training Accuracy: 95.70555555555555
[ Tue Jul  9 05:19:23 2024 ] 	Batch(4500/7879) done. Loss: 0.4617  lr:0.000100
[ Tue Jul  9 05:19:46 2024 ] 	Batch(4600/7879) done. Loss: 0.0403  lr:0.000100
[ Tue Jul  9 05:20:09 2024 ] 	Batch(4700/7879) done. Loss: 0.0487  lr:0.000100
[ Tue Jul  9 05:20:32 2024 ] 	Batch(4800/7879) done. Loss: 0.2171  lr:0.000100
[ Tue Jul  9 05:20:55 2024 ] 	Batch(4900/7879) done. Loss: 0.3290  lr:0.000100
[ Tue Jul  9 05:21:17 2024 ] 
Training: Epoch [83/120], Step [4999], Loss: 0.14077132940292358, Training Accuracy: 95.7775
[ Tue Jul  9 05:21:17 2024 ] 	Batch(5000/7879) done. Loss: 0.0219  lr:0.000100
[ Tue Jul  9 05:21:40 2024 ] 	Batch(5100/7879) done. Loss: 0.6638  lr:0.000100
[ Tue Jul  9 05:22:04 2024 ] 	Batch(5200/7879) done. Loss: 0.1148  lr:0.000100
[ Tue Jul  9 05:22:27 2024 ] 	Batch(5300/7879) done. Loss: 0.5439  lr:0.000100
[ Tue Jul  9 05:22:50 2024 ] 	Batch(5400/7879) done. Loss: 0.1658  lr:0.000100
[ Tue Jul  9 05:23:12 2024 ] 
Training: Epoch [83/120], Step [5499], Loss: 0.026461195200681686, Training Accuracy: 95.75227272727273
[ Tue Jul  9 05:23:13 2024 ] 	Batch(5500/7879) done. Loss: 0.1293  lr:0.000100
[ Tue Jul  9 05:23:36 2024 ] 	Batch(5600/7879) done. Loss: 0.0608  lr:0.000100
[ Tue Jul  9 05:23:58 2024 ] 	Batch(5700/7879) done. Loss: 0.0332  lr:0.000100
[ Tue Jul  9 05:24:21 2024 ] 	Batch(5800/7879) done. Loss: 0.1556  lr:0.000100
[ Tue Jul  9 05:24:44 2024 ] 	Batch(5900/7879) done. Loss: 0.6515  lr:0.000100
[ Tue Jul  9 05:25:06 2024 ] 
Training: Epoch [83/120], Step [5999], Loss: 0.3183845579624176, Training Accuracy: 95.77291666666666
[ Tue Jul  9 05:25:06 2024 ] 	Batch(6000/7879) done. Loss: 0.0372  lr:0.000100
[ Tue Jul  9 05:25:29 2024 ] 	Batch(6100/7879) done. Loss: 0.3751  lr:0.000100
[ Tue Jul  9 05:25:52 2024 ] 	Batch(6200/7879) done. Loss: 0.1254  lr:0.000100
[ Tue Jul  9 05:26:14 2024 ] 	Batch(6300/7879) done. Loss: 0.1631  lr:0.000100
[ Tue Jul  9 05:26:37 2024 ] 	Batch(6400/7879) done. Loss: 0.1170  lr:0.000100
[ Tue Jul  9 05:27:00 2024 ] 
Training: Epoch [83/120], Step [6499], Loss: 0.11455222964286804, Training Accuracy: 95.79038461538461
[ Tue Jul  9 05:27:00 2024 ] 	Batch(6500/7879) done. Loss: 0.1309  lr:0.000100
[ Tue Jul  9 05:27:23 2024 ] 	Batch(6600/7879) done. Loss: 0.0633  lr:0.000100
[ Tue Jul  9 05:27:45 2024 ] 	Batch(6700/7879) done. Loss: 0.0010  lr:0.000100
[ Tue Jul  9 05:28:08 2024 ] 	Batch(6800/7879) done. Loss: 0.0337  lr:0.000100
[ Tue Jul  9 05:28:31 2024 ] 	Batch(6900/7879) done. Loss: 0.0217  lr:0.000100
[ Tue Jul  9 05:28:53 2024 ] 
Training: Epoch [83/120], Step [6999], Loss: 0.38477420806884766, Training Accuracy: 95.825
[ Tue Jul  9 05:28:54 2024 ] 	Batch(7000/7879) done. Loss: 0.0616  lr:0.000100
[ Tue Jul  9 05:29:16 2024 ] 	Batch(7100/7879) done. Loss: 0.2481  lr:0.000100
[ Tue Jul  9 05:29:39 2024 ] 	Batch(7200/7879) done. Loss: 0.1032  lr:0.000100
[ Tue Jul  9 05:30:02 2024 ] 	Batch(7300/7879) done. Loss: 0.2916  lr:0.000100
[ Tue Jul  9 05:30:25 2024 ] 	Batch(7400/7879) done. Loss: 0.0247  lr:0.000100
[ Tue Jul  9 05:30:47 2024 ] 
Training: Epoch [83/120], Step [7499], Loss: 0.10409431159496307, Training Accuracy: 95.815
[ Tue Jul  9 05:30:47 2024 ] 	Batch(7500/7879) done. Loss: 0.5833  lr:0.000100
[ Tue Jul  9 05:31:10 2024 ] 	Batch(7600/7879) done. Loss: 0.0184  lr:0.000100
[ Tue Jul  9 05:31:34 2024 ] 	Batch(7700/7879) done. Loss: 0.1437  lr:0.000100
[ Tue Jul  9 05:31:56 2024 ] 	Batch(7800/7879) done. Loss: 0.0231  lr:0.000100
[ Tue Jul  9 05:32:14 2024 ] 	Mean training loss: 0.1569.
[ Tue Jul  9 05:32:14 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 05:32:14 2024 ] Training epoch: 85
[ Tue Jul  9 05:32:15 2024 ] 	Batch(0/7879) done. Loss: 0.0092  lr:0.000100
[ Tue Jul  9 05:32:38 2024 ] 	Batch(100/7879) done. Loss: 0.1793  lr:0.000100
[ Tue Jul  9 05:33:00 2024 ] 	Batch(200/7879) done. Loss: 0.1418  lr:0.000100
[ Tue Jul  9 05:33:23 2024 ] 	Batch(300/7879) done. Loss: 0.0195  lr:0.000100
[ Tue Jul  9 05:33:46 2024 ] 	Batch(400/7879) done. Loss: 0.0667  lr:0.000100
[ Tue Jul  9 05:34:09 2024 ] 
Training: Epoch [84/120], Step [499], Loss: 0.09713682532310486, Training Accuracy: 95.95
[ Tue Jul  9 05:34:09 2024 ] 	Batch(500/7879) done. Loss: 0.0164  lr:0.000100
[ Tue Jul  9 05:34:32 2024 ] 	Batch(600/7879) done. Loss: 0.3409  lr:0.000100
[ Tue Jul  9 05:34:54 2024 ] 	Batch(700/7879) done. Loss: 0.0208  lr:0.000100
[ Tue Jul  9 05:35:17 2024 ] 	Batch(800/7879) done. Loss: 0.0263  lr:0.000100
[ Tue Jul  9 05:35:40 2024 ] 	Batch(900/7879) done. Loss: 0.0950  lr:0.000100
[ Tue Jul  9 05:36:03 2024 ] 
Training: Epoch [84/120], Step [999], Loss: 0.3250652253627777, Training Accuracy: 96.26249999999999
[ Tue Jul  9 05:36:03 2024 ] 	Batch(1000/7879) done. Loss: 0.0780  lr:0.000100
[ Tue Jul  9 05:36:25 2024 ] 	Batch(1100/7879) done. Loss: 0.1558  lr:0.000100
[ Tue Jul  9 05:36:48 2024 ] 	Batch(1200/7879) done. Loss: 0.0238  lr:0.000100
[ Tue Jul  9 05:37:11 2024 ] 	Batch(1300/7879) done. Loss: 0.0058  lr:0.000100
[ Tue Jul  9 05:37:34 2024 ] 	Batch(1400/7879) done. Loss: 0.0650  lr:0.000100
[ Tue Jul  9 05:37:56 2024 ] 
Training: Epoch [84/120], Step [1499], Loss: 0.09860744327306747, Training Accuracy: 96.08333333333333
[ Tue Jul  9 05:37:57 2024 ] 	Batch(1500/7879) done. Loss: 0.2003  lr:0.000100
[ Tue Jul  9 05:38:20 2024 ] 	Batch(1600/7879) done. Loss: 0.6382  lr:0.000100
[ Tue Jul  9 05:38:43 2024 ] 	Batch(1700/7879) done. Loss: 0.1666  lr:0.000100
[ Tue Jul  9 05:39:05 2024 ] 	Batch(1800/7879) done. Loss: 0.2002  lr:0.000100
[ Tue Jul  9 05:39:28 2024 ] 	Batch(1900/7879) done. Loss: 0.2001  lr:0.000100
[ Tue Jul  9 05:39:51 2024 ] 
Training: Epoch [84/120], Step [1999], Loss: 0.034728217869997025, Training Accuracy: 96.0375
[ Tue Jul  9 05:39:51 2024 ] 	Batch(2000/7879) done. Loss: 0.0123  lr:0.000100
[ Tue Jul  9 05:40:14 2024 ] 	Batch(2100/7879) done. Loss: 0.0287  lr:0.000100
[ Tue Jul  9 05:40:36 2024 ] 	Batch(2200/7879) done. Loss: 0.5271  lr:0.000100
[ Tue Jul  9 05:40:59 2024 ] 	Batch(2300/7879) done. Loss: 0.0777  lr:0.000100
[ Tue Jul  9 05:41:22 2024 ] 	Batch(2400/7879) done. Loss: 0.2508  lr:0.000100
[ Tue Jul  9 05:41:45 2024 ] 
Training: Epoch [84/120], Step [2499], Loss: 0.20319870114326477, Training Accuracy: 95.99499999999999
[ Tue Jul  9 05:41:46 2024 ] 	Batch(2500/7879) done. Loss: 0.0088  lr:0.000100
[ Tue Jul  9 05:42:08 2024 ] 	Batch(2600/7879) done. Loss: 0.0680  lr:0.000100
[ Tue Jul  9 05:42:31 2024 ] 	Batch(2700/7879) done. Loss: 0.1916  lr:0.000100
[ Tue Jul  9 05:42:54 2024 ] 	Batch(2800/7879) done. Loss: 0.0153  lr:0.000100
[ Tue Jul  9 05:43:17 2024 ] 	Batch(2900/7879) done. Loss: 0.0458  lr:0.000100
[ Tue Jul  9 05:43:39 2024 ] 
Training: Epoch [84/120], Step [2999], Loss: 0.08054257184267044, Training Accuracy: 95.97916666666667
[ Tue Jul  9 05:43:40 2024 ] 	Batch(3000/7879) done. Loss: 0.1499  lr:0.000100
[ Tue Jul  9 05:44:03 2024 ] 	Batch(3100/7879) done. Loss: 0.2976  lr:0.000100
[ Tue Jul  9 05:44:26 2024 ] 	Batch(3200/7879) done. Loss: 0.0958  lr:0.000100
[ Tue Jul  9 05:44:48 2024 ] 	Batch(3300/7879) done. Loss: 0.0356  lr:0.000100
[ Tue Jul  9 05:45:11 2024 ] 	Batch(3400/7879) done. Loss: 0.3411  lr:0.000100
[ Tue Jul  9 05:45:33 2024 ] 
Training: Epoch [84/120], Step [3499], Loss: 0.10726916044950485, Training Accuracy: 95.98928571428571
[ Tue Jul  9 05:45:34 2024 ] 	Batch(3500/7879) done. Loss: 0.0321  lr:0.000100
[ Tue Jul  9 05:45:56 2024 ] 	Batch(3600/7879) done. Loss: 0.1007  lr:0.000100
[ Tue Jul  9 05:46:19 2024 ] 	Batch(3700/7879) done. Loss: 0.0901  lr:0.000100
[ Tue Jul  9 05:46:42 2024 ] 	Batch(3800/7879) done. Loss: 0.0184  lr:0.000100
[ Tue Jul  9 05:47:05 2024 ] 	Batch(3900/7879) done. Loss: 0.3115  lr:0.000100
[ Tue Jul  9 05:47:27 2024 ] 
Training: Epoch [84/120], Step [3999], Loss: 0.019145002588629723, Training Accuracy: 95.921875
[ Tue Jul  9 05:47:27 2024 ] 	Batch(4000/7879) done. Loss: 0.0611  lr:0.000100
[ Tue Jul  9 05:47:50 2024 ] 	Batch(4100/7879) done. Loss: 0.0740  lr:0.000100
[ Tue Jul  9 05:48:13 2024 ] 	Batch(4200/7879) done. Loss: 0.1286  lr:0.000100
[ Tue Jul  9 05:48:36 2024 ] 	Batch(4300/7879) done. Loss: 0.0539  lr:0.000100
[ Tue Jul  9 05:48:59 2024 ] 	Batch(4400/7879) done. Loss: 0.0149  lr:0.000100
[ Tue Jul  9 05:49:21 2024 ] 
Training: Epoch [84/120], Step [4499], Loss: 0.16179007291793823, Training Accuracy: 95.92777777777778
[ Tue Jul  9 05:49:21 2024 ] 	Batch(4500/7879) done. Loss: 0.1883  lr:0.000100
[ Tue Jul  9 05:49:44 2024 ] 	Batch(4600/7879) done. Loss: 0.2742  lr:0.000100
[ Tue Jul  9 05:50:07 2024 ] 	Batch(4700/7879) done. Loss: 0.0158  lr:0.000100
[ Tue Jul  9 05:50:30 2024 ] 	Batch(4800/7879) done. Loss: 0.1022  lr:0.000100
[ Tue Jul  9 05:50:53 2024 ] 	Batch(4900/7879) done. Loss: 0.1801  lr:0.000100
[ Tue Jul  9 05:51:15 2024 ] 
Training: Epoch [84/120], Step [4999], Loss: 0.07235939800739288, Training Accuracy: 95.91
[ Tue Jul  9 05:51:16 2024 ] 	Batch(5000/7879) done. Loss: 0.1778  lr:0.000100
[ Tue Jul  9 05:51:38 2024 ] 	Batch(5100/7879) done. Loss: 0.0679  lr:0.000100
[ Tue Jul  9 05:52:01 2024 ] 	Batch(5200/7879) done. Loss: 0.0550  lr:0.000100
[ Tue Jul  9 05:52:24 2024 ] 	Batch(5300/7879) done. Loss: 0.1078  lr:0.000100
[ Tue Jul  9 05:52:47 2024 ] 	Batch(5400/7879) done. Loss: 0.0604  lr:0.000100
[ Tue Jul  9 05:53:09 2024 ] 
Training: Epoch [84/120], Step [5499], Loss: 0.04250851273536682, Training Accuracy: 95.93636363636364
[ Tue Jul  9 05:53:09 2024 ] 	Batch(5500/7879) done. Loss: 0.0243  lr:0.000100
[ Tue Jul  9 05:53:32 2024 ] 	Batch(5600/7879) done. Loss: 0.0830  lr:0.000100
[ Tue Jul  9 05:53:55 2024 ] 	Batch(5700/7879) done. Loss: 0.0563  lr:0.000100
[ Tue Jul  9 05:54:17 2024 ] 	Batch(5800/7879) done. Loss: 0.0253  lr:0.000100
[ Tue Jul  9 05:54:40 2024 ] 	Batch(5900/7879) done. Loss: 0.3135  lr:0.000100
[ Tue Jul  9 05:55:02 2024 ] 
Training: Epoch [84/120], Step [5999], Loss: 0.2401331216096878, Training Accuracy: 95.95833333333333
[ Tue Jul  9 05:55:02 2024 ] 	Batch(6000/7879) done. Loss: 0.0481  lr:0.000100
[ Tue Jul  9 05:55:25 2024 ] 	Batch(6100/7879) done. Loss: 0.2529  lr:0.000100
[ Tue Jul  9 05:55:47 2024 ] 	Batch(6200/7879) done. Loss: 0.3276  lr:0.000100
[ Tue Jul  9 05:56:10 2024 ] 	Batch(6300/7879) done. Loss: 0.0180  lr:0.000100
[ Tue Jul  9 05:56:33 2024 ] 	Batch(6400/7879) done. Loss: 0.0252  lr:0.000100
[ Tue Jul  9 05:56:55 2024 ] 
Training: Epoch [84/120], Step [6499], Loss: 0.12716594338417053, Training Accuracy: 95.92692307692307
[ Tue Jul  9 05:56:55 2024 ] 	Batch(6500/7879) done. Loss: 0.6095  lr:0.000100
[ Tue Jul  9 05:57:18 2024 ] 	Batch(6600/7879) done. Loss: 0.2238  lr:0.000100
[ Tue Jul  9 05:57:41 2024 ] 	Batch(6700/7879) done. Loss: 0.4638  lr:0.000100
[ Tue Jul  9 05:58:03 2024 ] 	Batch(6800/7879) done. Loss: 0.1015  lr:0.000100
[ Tue Jul  9 05:58:26 2024 ] 	Batch(6900/7879) done. Loss: 0.2054  lr:0.000100
[ Tue Jul  9 05:58:48 2024 ] 
Training: Epoch [84/120], Step [6999], Loss: 0.07567449659109116, Training Accuracy: 95.90892857142858
[ Tue Jul  9 05:58:49 2024 ] 	Batch(7000/7879) done. Loss: 0.0753  lr:0.000100
[ Tue Jul  9 05:59:11 2024 ] 	Batch(7100/7879) done. Loss: 0.0278  lr:0.000100
[ Tue Jul  9 05:59:34 2024 ] 	Batch(7200/7879) done. Loss: 0.1231  lr:0.000100
[ Tue Jul  9 05:59:56 2024 ] 	Batch(7300/7879) done. Loss: 0.0572  lr:0.000100
[ Tue Jul  9 06:00:19 2024 ] 	Batch(7400/7879) done. Loss: 0.0743  lr:0.000100
[ Tue Jul  9 06:00:41 2024 ] 
Training: Epoch [84/120], Step [7499], Loss: 0.03413081541657448, Training Accuracy: 95.90666666666667
[ Tue Jul  9 06:00:42 2024 ] 	Batch(7500/7879) done. Loss: 0.0247  lr:0.000100
[ Tue Jul  9 06:01:04 2024 ] 	Batch(7600/7879) done. Loss: 0.0426  lr:0.000100
[ Tue Jul  9 06:01:27 2024 ] 	Batch(7700/7879) done. Loss: 0.2735  lr:0.000100
[ Tue Jul  9 06:01:49 2024 ] 	Batch(7800/7879) done. Loss: 0.3401  lr:0.000100
[ Tue Jul  9 06:02:07 2024 ] 	Mean training loss: 0.1513.
[ Tue Jul  9 06:02:07 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 06:02:07 2024 ] Training epoch: 86
[ Tue Jul  9 06:02:08 2024 ] 	Batch(0/7879) done. Loss: 0.0054  lr:0.000100
[ Tue Jul  9 06:02:31 2024 ] 	Batch(100/7879) done. Loss: 0.0115  lr:0.000100
[ Tue Jul  9 06:02:53 2024 ] 	Batch(200/7879) done. Loss: 0.0322  lr:0.000100
[ Tue Jul  9 06:03:16 2024 ] 	Batch(300/7879) done. Loss: 0.6569  lr:0.000100
[ Tue Jul  9 06:03:39 2024 ] 	Batch(400/7879) done. Loss: 0.3317  lr:0.000100
[ Tue Jul  9 06:04:01 2024 ] 
Training: Epoch [85/120], Step [499], Loss: 0.011699555441737175, Training Accuracy: 95.625
[ Tue Jul  9 06:04:01 2024 ] 	Batch(500/7879) done. Loss: 0.0481  lr:0.000100
[ Tue Jul  9 06:04:24 2024 ] 	Batch(600/7879) done. Loss: 0.0347  lr:0.000100
[ Tue Jul  9 06:04:46 2024 ] 	Batch(700/7879) done. Loss: 0.0240  lr:0.000100
[ Tue Jul  9 06:05:09 2024 ] 	Batch(800/7879) done. Loss: 0.7095  lr:0.000100
[ Tue Jul  9 06:05:31 2024 ] 	Batch(900/7879) done. Loss: 0.2653  lr:0.000100
[ Tue Jul  9 06:05:54 2024 ] 
Training: Epoch [85/120], Step [999], Loss: 0.009975005872547626, Training Accuracy: 95.875
[ Tue Jul  9 06:05:54 2024 ] 	Batch(1000/7879) done. Loss: 0.2337  lr:0.000100
[ Tue Jul  9 06:06:16 2024 ] 	Batch(1100/7879) done. Loss: 0.0176  lr:0.000100
[ Tue Jul  9 06:06:39 2024 ] 	Batch(1200/7879) done. Loss: 0.0175  lr:0.000100
[ Tue Jul  9 06:07:02 2024 ] 	Batch(1300/7879) done. Loss: 0.0986  lr:0.000100
[ Tue Jul  9 06:07:24 2024 ] 	Batch(1400/7879) done. Loss: 0.1120  lr:0.000100
[ Tue Jul  9 06:07:47 2024 ] 
Training: Epoch [85/120], Step [1499], Loss: 0.2971971929073334, Training Accuracy: 95.8
[ Tue Jul  9 06:07:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0937  lr:0.000100
[ Tue Jul  9 06:08:10 2024 ] 	Batch(1600/7879) done. Loss: 0.1932  lr:0.000100
[ Tue Jul  9 06:08:32 2024 ] 	Batch(1700/7879) done. Loss: 0.1044  lr:0.000100
[ Tue Jul  9 06:08:55 2024 ] 	Batch(1800/7879) done. Loss: 0.0556  lr:0.000100
[ Tue Jul  9 06:09:17 2024 ] 	Batch(1900/7879) done. Loss: 0.0077  lr:0.000100
[ Tue Jul  9 06:09:40 2024 ] 
Training: Epoch [85/120], Step [1999], Loss: 0.1530616134405136, Training Accuracy: 95.9125
[ Tue Jul  9 06:09:40 2024 ] 	Batch(2000/7879) done. Loss: 0.0671  lr:0.000100
[ Tue Jul  9 06:10:03 2024 ] 	Batch(2100/7879) done. Loss: 0.0472  lr:0.000100
[ Tue Jul  9 06:10:27 2024 ] 	Batch(2200/7879) done. Loss: 0.0510  lr:0.000100
[ Tue Jul  9 06:10:50 2024 ] 	Batch(2300/7879) done. Loss: 0.0577  lr:0.000100
[ Tue Jul  9 06:11:12 2024 ] 	Batch(2400/7879) done. Loss: 0.2374  lr:0.000100
[ Tue Jul  9 06:11:35 2024 ] 
Training: Epoch [85/120], Step [2499], Loss: 0.061115432530641556, Training Accuracy: 95.99499999999999
[ Tue Jul  9 06:11:35 2024 ] 	Batch(2500/7879) done. Loss: 0.1087  lr:0.000100
[ Tue Jul  9 06:11:58 2024 ] 	Batch(2600/7879) done. Loss: 0.9603  lr:0.000100
[ Tue Jul  9 06:12:20 2024 ] 	Batch(2700/7879) done. Loss: 0.1598  lr:0.000100
[ Tue Jul  9 06:12:43 2024 ] 	Batch(2800/7879) done. Loss: 0.3253  lr:0.000100
[ Tue Jul  9 06:13:05 2024 ] 	Batch(2900/7879) done. Loss: 0.2519  lr:0.000100
[ Tue Jul  9 06:13:28 2024 ] 
Training: Epoch [85/120], Step [2999], Loss: 0.14437875151634216, Training Accuracy: 95.975
[ Tue Jul  9 06:13:28 2024 ] 	Batch(3000/7879) done. Loss: 0.1133  lr:0.000100
[ Tue Jul  9 06:13:51 2024 ] 	Batch(3100/7879) done. Loss: 0.0104  lr:0.000100
[ Tue Jul  9 06:14:13 2024 ] 	Batch(3200/7879) done. Loss: 0.0792  lr:0.000100
[ Tue Jul  9 06:14:36 2024 ] 	Batch(3300/7879) done. Loss: 0.1579  lr:0.000100
[ Tue Jul  9 06:14:59 2024 ] 	Batch(3400/7879) done. Loss: 0.0245  lr:0.000100
[ Tue Jul  9 06:15:21 2024 ] 
Training: Epoch [85/120], Step [3499], Loss: 0.04385268688201904, Training Accuracy: 95.87142857142858
[ Tue Jul  9 06:15:21 2024 ] 	Batch(3500/7879) done. Loss: 0.8526  lr:0.000100
[ Tue Jul  9 06:15:44 2024 ] 	Batch(3600/7879) done. Loss: 0.1191  lr:0.000100
[ Tue Jul  9 06:16:07 2024 ] 	Batch(3700/7879) done. Loss: 0.1648  lr:0.000100
[ Tue Jul  9 06:16:29 2024 ] 	Batch(3800/7879) done. Loss: 0.1210  lr:0.000100
[ Tue Jul  9 06:16:52 2024 ] 	Batch(3900/7879) done. Loss: 0.8594  lr:0.000100
[ Tue Jul  9 06:17:14 2024 ] 
Training: Epoch [85/120], Step [3999], Loss: 0.0964774414896965, Training Accuracy: 95.875
[ Tue Jul  9 06:17:15 2024 ] 	Batch(4000/7879) done. Loss: 0.1905  lr:0.000100
[ Tue Jul  9 06:17:37 2024 ] 	Batch(4100/7879) done. Loss: 0.0094  lr:0.000100
[ Tue Jul  9 06:18:00 2024 ] 	Batch(4200/7879) done. Loss: 0.2300  lr:0.000100
[ Tue Jul  9 06:18:23 2024 ] 	Batch(4300/7879) done. Loss: 0.1039  lr:0.000100
[ Tue Jul  9 06:18:45 2024 ] 	Batch(4400/7879) done. Loss: 0.1347  lr:0.000100
[ Tue Jul  9 06:19:08 2024 ] 
Training: Epoch [85/120], Step [4499], Loss: 0.31583312153816223, Training Accuracy: 95.95277777777778
[ Tue Jul  9 06:19:08 2024 ] 	Batch(4500/7879) done. Loss: 0.0280  lr:0.000100
[ Tue Jul  9 06:19:31 2024 ] 	Batch(4600/7879) done. Loss: 0.2014  lr:0.000100
[ Tue Jul  9 06:19:53 2024 ] 	Batch(4700/7879) done. Loss: 0.1113  lr:0.000100
[ Tue Jul  9 06:20:16 2024 ] 	Batch(4800/7879) done. Loss: 0.2508  lr:0.000100
[ Tue Jul  9 06:20:38 2024 ] 	Batch(4900/7879) done. Loss: 0.0418  lr:0.000100
[ Tue Jul  9 06:21:01 2024 ] 
Training: Epoch [85/120], Step [4999], Loss: 0.11534282565116882, Training Accuracy: 95.96249999999999
[ Tue Jul  9 06:21:01 2024 ] 	Batch(5000/7879) done. Loss: 0.0312  lr:0.000100
[ Tue Jul  9 06:21:24 2024 ] 	Batch(5100/7879) done. Loss: 0.0126  lr:0.000100
[ Tue Jul  9 06:21:46 2024 ] 	Batch(5200/7879) done. Loss: 0.2516  lr:0.000100
[ Tue Jul  9 06:22:09 2024 ] 	Batch(5300/7879) done. Loss: 0.1329  lr:0.000100
[ Tue Jul  9 06:22:31 2024 ] 	Batch(5400/7879) done. Loss: 0.2515  lr:0.000100
[ Tue Jul  9 06:22:54 2024 ] 
Training: Epoch [85/120], Step [5499], Loss: 0.034874144941568375, Training Accuracy: 95.93409090909091
[ Tue Jul  9 06:22:54 2024 ] 	Batch(5500/7879) done. Loss: 0.5552  lr:0.000100
[ Tue Jul  9 06:23:17 2024 ] 	Batch(5600/7879) done. Loss: 0.3279  lr:0.000100
[ Tue Jul  9 06:23:39 2024 ] 	Batch(5700/7879) done. Loss: 0.0020  lr:0.000100
[ Tue Jul  9 06:24:02 2024 ] 	Batch(5800/7879) done. Loss: 0.3632  lr:0.000100
[ Tue Jul  9 06:24:24 2024 ] 	Batch(5900/7879) done. Loss: 0.0245  lr:0.000100
[ Tue Jul  9 06:24:47 2024 ] 
Training: Epoch [85/120], Step [5999], Loss: 0.5982377529144287, Training Accuracy: 95.95833333333333
[ Tue Jul  9 06:24:47 2024 ] 	Batch(6000/7879) done. Loss: 0.0650  lr:0.000100
[ Tue Jul  9 06:25:10 2024 ] 	Batch(6100/7879) done. Loss: 0.0562  lr:0.000100
[ Tue Jul  9 06:25:32 2024 ] 	Batch(6200/7879) done. Loss: 0.0388  lr:0.000100
[ Tue Jul  9 06:25:55 2024 ] 	Batch(6300/7879) done. Loss: 0.0032  lr:0.000100
[ Tue Jul  9 06:26:18 2024 ] 	Batch(6400/7879) done. Loss: 0.1991  lr:0.000100
[ Tue Jul  9 06:26:40 2024 ] 
Training: Epoch [85/120], Step [6499], Loss: 0.053122080862522125, Training Accuracy: 95.95192307692308
[ Tue Jul  9 06:26:40 2024 ] 	Batch(6500/7879) done. Loss: 0.1510  lr:0.000100
[ Tue Jul  9 06:27:03 2024 ] 	Batch(6600/7879) done. Loss: 0.2810  lr:0.000100
[ Tue Jul  9 06:27:25 2024 ] 	Batch(6700/7879) done. Loss: 0.0334  lr:0.000100
[ Tue Jul  9 06:27:48 2024 ] 	Batch(6800/7879) done. Loss: 0.0271  lr:0.000100
[ Tue Jul  9 06:28:11 2024 ] 	Batch(6900/7879) done. Loss: 0.0400  lr:0.000100
[ Tue Jul  9 06:28:33 2024 ] 
Training: Epoch [85/120], Step [6999], Loss: 0.05051577091217041, Training Accuracy: 95.90357142857144
[ Tue Jul  9 06:28:33 2024 ] 	Batch(7000/7879) done. Loss: 0.3155  lr:0.000100
[ Tue Jul  9 06:28:56 2024 ] 	Batch(7100/7879) done. Loss: 0.0179  lr:0.000100
[ Tue Jul  9 06:29:18 2024 ] 	Batch(7200/7879) done. Loss: 0.0764  lr:0.000100
[ Tue Jul  9 06:29:41 2024 ] 	Batch(7300/7879) done. Loss: 0.3235  lr:0.000100
[ Tue Jul  9 06:30:04 2024 ] 	Batch(7400/7879) done. Loss: 0.0046  lr:0.000100
[ Tue Jul  9 06:30:26 2024 ] 
Training: Epoch [85/120], Step [7499], Loss: 0.03358342498540878, Training Accuracy: 95.87
[ Tue Jul  9 06:30:26 2024 ] 	Batch(7500/7879) done. Loss: 0.0460  lr:0.000100
[ Tue Jul  9 06:30:49 2024 ] 	Batch(7600/7879) done. Loss: 0.0673  lr:0.000100
[ Tue Jul  9 06:31:12 2024 ] 	Batch(7700/7879) done. Loss: 0.0660  lr:0.000100
[ Tue Jul  9 06:31:34 2024 ] 	Batch(7800/7879) done. Loss: 0.0468  lr:0.000100
[ Tue Jul  9 06:31:52 2024 ] 	Mean training loss: 0.1494.
[ Tue Jul  9 06:31:52 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 06:31:52 2024 ] Training epoch: 87
[ Tue Jul  9 06:31:52 2024 ] 	Batch(0/7879) done. Loss: 0.2602  lr:0.000100
[ Tue Jul  9 06:32:15 2024 ] 	Batch(100/7879) done. Loss: 0.1010  lr:0.000100
[ Tue Jul  9 06:32:38 2024 ] 	Batch(200/7879) done. Loss: 0.0129  lr:0.000100
[ Tue Jul  9 06:33:00 2024 ] 	Batch(300/7879) done. Loss: 0.0608  lr:0.000100
[ Tue Jul  9 06:33:23 2024 ] 	Batch(400/7879) done. Loss: 0.2330  lr:0.000100
[ Tue Jul  9 06:33:45 2024 ] 
Training: Epoch [86/120], Step [499], Loss: 0.0700778216123581, Training Accuracy: 96.075
[ Tue Jul  9 06:33:46 2024 ] 	Batch(500/7879) done. Loss: 0.3893  lr:0.000100
[ Tue Jul  9 06:34:08 2024 ] 	Batch(600/7879) done. Loss: 0.3098  lr:0.000100
[ Tue Jul  9 06:34:31 2024 ] 	Batch(700/7879) done. Loss: 0.0296  lr:0.000100
[ Tue Jul  9 06:34:53 2024 ] 	Batch(800/7879) done. Loss: 0.1384  lr:0.000100
[ Tue Jul  9 06:35:16 2024 ] 	Batch(900/7879) done. Loss: 0.0173  lr:0.000100
[ Tue Jul  9 06:35:38 2024 ] 
Training: Epoch [86/120], Step [999], Loss: 0.05006349831819534, Training Accuracy: 95.9125
[ Tue Jul  9 06:35:39 2024 ] 	Batch(1000/7879) done. Loss: 0.0488  lr:0.000100
[ Tue Jul  9 06:36:01 2024 ] 	Batch(1100/7879) done. Loss: 0.1429  lr:0.000100
[ Tue Jul  9 06:36:24 2024 ] 	Batch(1200/7879) done. Loss: 0.3531  lr:0.000100
[ Tue Jul  9 06:36:46 2024 ] 	Batch(1300/7879) done. Loss: 0.0488  lr:0.000100
[ Tue Jul  9 06:37:09 2024 ] 	Batch(1400/7879) done. Loss: 0.0484  lr:0.000100
[ Tue Jul  9 06:37:31 2024 ] 
Training: Epoch [86/120], Step [1499], Loss: 0.11257550120353699, Training Accuracy: 96.16666666666667
[ Tue Jul  9 06:37:32 2024 ] 	Batch(1500/7879) done. Loss: 0.3972  lr:0.000100
[ Tue Jul  9 06:37:54 2024 ] 	Batch(1600/7879) done. Loss: 0.0163  lr:0.000100
[ Tue Jul  9 06:38:17 2024 ] 	Batch(1700/7879) done. Loss: 0.1439  lr:0.000100
[ Tue Jul  9 06:38:39 2024 ] 	Batch(1800/7879) done. Loss: 0.0238  lr:0.000100
[ Tue Jul  9 06:39:02 2024 ] 	Batch(1900/7879) done. Loss: 0.2679  lr:0.000100
[ Tue Jul  9 06:39:24 2024 ] 
Training: Epoch [86/120], Step [1999], Loss: 0.37418612837791443, Training Accuracy: 95.9875
[ Tue Jul  9 06:39:25 2024 ] 	Batch(2000/7879) done. Loss: 0.1634  lr:0.000100
[ Tue Jul  9 06:39:47 2024 ] 	Batch(2100/7879) done. Loss: 0.0397  lr:0.000100
[ Tue Jul  9 06:40:10 2024 ] 	Batch(2200/7879) done. Loss: 0.2529  lr:0.000100
[ Tue Jul  9 06:40:32 2024 ] 	Batch(2300/7879) done. Loss: 0.1717  lr:0.000100
[ Tue Jul  9 06:40:55 2024 ] 	Batch(2400/7879) done. Loss: 0.0279  lr:0.000100
[ Tue Jul  9 06:41:18 2024 ] 
Training: Epoch [86/120], Step [2499], Loss: 0.09313870221376419, Training Accuracy: 95.96000000000001
[ Tue Jul  9 06:41:18 2024 ] 	Batch(2500/7879) done. Loss: 0.0825  lr:0.000100
[ Tue Jul  9 06:41:42 2024 ] 	Batch(2600/7879) done. Loss: 0.4316  lr:0.000100
[ Tue Jul  9 06:42:05 2024 ] 	Batch(2700/7879) done. Loss: 0.0308  lr:0.000100
[ Tue Jul  9 06:42:28 2024 ] 	Batch(2800/7879) done. Loss: 0.0730  lr:0.000100
[ Tue Jul  9 06:42:51 2024 ] 	Batch(2900/7879) done. Loss: 0.0153  lr:0.000100
[ Tue Jul  9 06:43:13 2024 ] 
Training: Epoch [86/120], Step [2999], Loss: 0.3247545063495636, Training Accuracy: 95.96666666666667
[ Tue Jul  9 06:43:13 2024 ] 	Batch(3000/7879) done. Loss: 0.0334  lr:0.000100
[ Tue Jul  9 06:43:36 2024 ] 	Batch(3100/7879) done. Loss: 0.4468  lr:0.000100
[ Tue Jul  9 06:43:59 2024 ] 	Batch(3200/7879) done. Loss: 0.6165  lr:0.000100
[ Tue Jul  9 06:44:21 2024 ] 	Batch(3300/7879) done. Loss: 0.0410  lr:0.000100
[ Tue Jul  9 06:44:44 2024 ] 	Batch(3400/7879) done. Loss: 0.0131  lr:0.000100
[ Tue Jul  9 06:45:06 2024 ] 
Training: Epoch [86/120], Step [3499], Loss: 0.25833040475845337, Training Accuracy: 95.99642857142857
[ Tue Jul  9 06:45:06 2024 ] 	Batch(3500/7879) done. Loss: 0.0378  lr:0.000100
[ Tue Jul  9 06:45:29 2024 ] 	Batch(3600/7879) done. Loss: 0.2441  lr:0.000100
[ Tue Jul  9 06:45:52 2024 ] 	Batch(3700/7879) done. Loss: 0.0942  lr:0.000100
[ Tue Jul  9 06:46:14 2024 ] 	Batch(3800/7879) done. Loss: 0.0945  lr:0.000100
[ Tue Jul  9 06:46:37 2024 ] 	Batch(3900/7879) done. Loss: 0.6443  lr:0.000100
[ Tue Jul  9 06:46:59 2024 ] 
Training: Epoch [86/120], Step [3999], Loss: 0.07338470220565796, Training Accuracy: 96.0125
[ Tue Jul  9 06:46:59 2024 ] 	Batch(4000/7879) done. Loss: 0.0206  lr:0.000100
[ Tue Jul  9 06:47:22 2024 ] 	Batch(4100/7879) done. Loss: 0.0148  lr:0.000100
[ Tue Jul  9 06:47:45 2024 ] 	Batch(4200/7879) done. Loss: 0.1450  lr:0.000100
[ Tue Jul  9 06:48:07 2024 ] 	Batch(4300/7879) done. Loss: 0.0274  lr:0.000100
[ Tue Jul  9 06:48:30 2024 ] 	Batch(4400/7879) done. Loss: 0.1579  lr:0.000100
[ Tue Jul  9 06:48:52 2024 ] 
Training: Epoch [86/120], Step [4499], Loss: 0.07180369645357132, Training Accuracy: 96.09166666666667
[ Tue Jul  9 06:48:52 2024 ] 	Batch(4500/7879) done. Loss: 0.1938  lr:0.000100
[ Tue Jul  9 06:49:15 2024 ] 	Batch(4600/7879) done. Loss: 0.1194  lr:0.000100
[ Tue Jul  9 06:49:38 2024 ] 	Batch(4700/7879) done. Loss: 0.0564  lr:0.000100
[ Tue Jul  9 06:50:00 2024 ] 	Batch(4800/7879) done. Loss: 0.1243  lr:0.000100
[ Tue Jul  9 06:50:23 2024 ] 	Batch(4900/7879) done. Loss: 0.4207  lr:0.000100
[ Tue Jul  9 06:50:45 2024 ] 
Training: Epoch [86/120], Step [4999], Loss: 0.036468274891376495, Training Accuracy: 96.0275
[ Tue Jul  9 06:50:45 2024 ] 	Batch(5000/7879) done. Loss: 0.0118  lr:0.000100
[ Tue Jul  9 06:51:08 2024 ] 	Batch(5100/7879) done. Loss: 0.0876  lr:0.000100
[ Tue Jul  9 06:51:31 2024 ] 	Batch(5200/7879) done. Loss: 0.0724  lr:0.000100
[ Tue Jul  9 06:51:53 2024 ] 	Batch(5300/7879) done. Loss: 0.0299  lr:0.000100
[ Tue Jul  9 06:52:16 2024 ] 	Batch(5400/7879) done. Loss: 0.5001  lr:0.000100
[ Tue Jul  9 06:52:38 2024 ] 
Training: Epoch [86/120], Step [5499], Loss: 0.04665716737508774, Training Accuracy: 96.02045454545454
[ Tue Jul  9 06:52:38 2024 ] 	Batch(5500/7879) done. Loss: 0.3943  lr:0.000100
[ Tue Jul  9 06:53:01 2024 ] 	Batch(5600/7879) done. Loss: 0.0053  lr:0.000100
[ Tue Jul  9 06:53:24 2024 ] 	Batch(5700/7879) done. Loss: 0.0463  lr:0.000100
[ Tue Jul  9 06:53:46 2024 ] 	Batch(5800/7879) done. Loss: 0.2586  lr:0.000100
[ Tue Jul  9 06:54:09 2024 ] 	Batch(5900/7879) done. Loss: 0.0116  lr:0.000100
[ Tue Jul  9 06:54:31 2024 ] 
Training: Epoch [86/120], Step [5999], Loss: 0.2091197818517685, Training Accuracy: 96.02708333333332
[ Tue Jul  9 06:54:32 2024 ] 	Batch(6000/7879) done. Loss: 0.1056  lr:0.000100
[ Tue Jul  9 06:54:54 2024 ] 	Batch(6100/7879) done. Loss: 0.0425  lr:0.000100
[ Tue Jul  9 06:55:17 2024 ] 	Batch(6200/7879) done. Loss: 0.0142  lr:0.000100
[ Tue Jul  9 06:55:39 2024 ] 	Batch(6300/7879) done. Loss: 0.0222  lr:0.000100
[ Tue Jul  9 06:56:02 2024 ] 	Batch(6400/7879) done. Loss: 0.0431  lr:0.000100
[ Tue Jul  9 06:56:24 2024 ] 
Training: Epoch [86/120], Step [6499], Loss: 0.011805007234215736, Training Accuracy: 96.04038461538461
[ Tue Jul  9 06:56:25 2024 ] 	Batch(6500/7879) done. Loss: 0.0222  lr:0.000100
[ Tue Jul  9 06:56:47 2024 ] 	Batch(6600/7879) done. Loss: 0.0136  lr:0.000100
[ Tue Jul  9 06:57:10 2024 ] 	Batch(6700/7879) done. Loss: 0.0926  lr:0.000100
[ Tue Jul  9 06:57:33 2024 ] 	Batch(6800/7879) done. Loss: 0.0228  lr:0.000100
[ Tue Jul  9 06:57:55 2024 ] 	Batch(6900/7879) done. Loss: 0.0653  lr:0.000100
[ Tue Jul  9 06:58:17 2024 ] 
Training: Epoch [86/120], Step [6999], Loss: 0.03410416841506958, Training Accuracy: 96.025
[ Tue Jul  9 06:58:18 2024 ] 	Batch(7000/7879) done. Loss: 0.0053  lr:0.000100
[ Tue Jul  9 06:58:40 2024 ] 	Batch(7100/7879) done. Loss: 0.1805  lr:0.000100
[ Tue Jul  9 06:59:03 2024 ] 	Batch(7200/7879) done. Loss: 0.0037  lr:0.000100
[ Tue Jul  9 06:59:25 2024 ] 	Batch(7300/7879) done. Loss: 0.1500  lr:0.000100
[ Tue Jul  9 06:59:48 2024 ] 	Batch(7400/7879) done. Loss: 0.1827  lr:0.000100
[ Tue Jul  9 07:00:10 2024 ] 
Training: Epoch [86/120], Step [7499], Loss: 0.2628137171268463, Training Accuracy: 95.99833333333333
[ Tue Jul  9 07:00:11 2024 ] 	Batch(7500/7879) done. Loss: 0.0175  lr:0.000100
[ Tue Jul  9 07:00:33 2024 ] 	Batch(7600/7879) done. Loss: 0.0055  lr:0.000100
[ Tue Jul  9 07:00:56 2024 ] 	Batch(7700/7879) done. Loss: 0.3331  lr:0.000100
[ Tue Jul  9 07:01:18 2024 ] 	Batch(7800/7879) done. Loss: 0.1341  lr:0.000100
[ Tue Jul  9 07:01:36 2024 ] 	Mean training loss: 0.1450.
[ Tue Jul  9 07:01:36 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 07:01:36 2024 ] Training epoch: 88
[ Tue Jul  9 07:01:37 2024 ] 	Batch(0/7879) done. Loss: 0.1499  lr:0.000100
[ Tue Jul  9 07:01:59 2024 ] 	Batch(100/7879) done. Loss: 0.0647  lr:0.000100
[ Tue Jul  9 07:02:22 2024 ] 	Batch(200/7879) done. Loss: 0.1635  lr:0.000100
[ Tue Jul  9 07:02:44 2024 ] 	Batch(300/7879) done. Loss: 0.0723  lr:0.000100
[ Tue Jul  9 07:03:07 2024 ] 	Batch(400/7879) done. Loss: 0.4780  lr:0.000100
[ Tue Jul  9 07:03:29 2024 ] 
Training: Epoch [87/120], Step [499], Loss: 0.207666277885437, Training Accuracy: 95.875
[ Tue Jul  9 07:03:30 2024 ] 	Batch(500/7879) done. Loss: 0.0398  lr:0.000100
[ Tue Jul  9 07:03:52 2024 ] 	Batch(600/7879) done. Loss: 0.0257  lr:0.000100
[ Tue Jul  9 07:04:15 2024 ] 	Batch(700/7879) done. Loss: 0.0198  lr:0.000100
[ Tue Jul  9 07:04:37 2024 ] 	Batch(800/7879) done. Loss: 0.2077  lr:0.000100
[ Tue Jul  9 07:05:00 2024 ] 	Batch(900/7879) done. Loss: 0.0470  lr:0.000100
[ Tue Jul  9 07:05:22 2024 ] 
Training: Epoch [87/120], Step [999], Loss: 0.013229037635028362, Training Accuracy: 95.8875
[ Tue Jul  9 07:05:22 2024 ] 	Batch(1000/7879) done. Loss: 0.0664  lr:0.000100
[ Tue Jul  9 07:05:45 2024 ] 	Batch(1100/7879) done. Loss: 0.1069  lr:0.000100
[ Tue Jul  9 07:06:08 2024 ] 	Batch(1200/7879) done. Loss: 0.1119  lr:0.000100
[ Tue Jul  9 07:06:31 2024 ] 	Batch(1300/7879) done. Loss: 0.4246  lr:0.000100
[ Tue Jul  9 07:06:53 2024 ] 	Batch(1400/7879) done. Loss: 0.0102  lr:0.000100
[ Tue Jul  9 07:07:15 2024 ] 
Training: Epoch [87/120], Step [1499], Loss: 0.016365736722946167, Training Accuracy: 95.94166666666666
[ Tue Jul  9 07:07:16 2024 ] 	Batch(1500/7879) done. Loss: 0.0351  lr:0.000100
[ Tue Jul  9 07:07:38 2024 ] 	Batch(1600/7879) done. Loss: 0.2309  lr:0.000100
[ Tue Jul  9 07:08:01 2024 ] 	Batch(1700/7879) done. Loss: 0.0915  lr:0.000100
[ Tue Jul  9 07:08:23 2024 ] 	Batch(1800/7879) done. Loss: 0.1273  lr:0.000100
[ Tue Jul  9 07:08:46 2024 ] 	Batch(1900/7879) done. Loss: 0.0148  lr:0.000100
[ Tue Jul  9 07:09:08 2024 ] 
Training: Epoch [87/120], Step [1999], Loss: 0.14364607632160187, Training Accuracy: 95.95
[ Tue Jul  9 07:09:09 2024 ] 	Batch(2000/7879) done. Loss: 0.0671  lr:0.000100
[ Tue Jul  9 07:09:31 2024 ] 	Batch(2100/7879) done. Loss: 0.0348  lr:0.000100
[ Tue Jul  9 07:09:54 2024 ] 	Batch(2200/7879) done. Loss: 0.2080  lr:0.000100
[ Tue Jul  9 07:10:16 2024 ] 	Batch(2300/7879) done. Loss: 0.2101  lr:0.000100
[ Tue Jul  9 07:10:39 2024 ] 	Batch(2400/7879) done. Loss: 0.4168  lr:0.000100
[ Tue Jul  9 07:11:01 2024 ] 
Training: Epoch [87/120], Step [2499], Loss: 0.15732011198997498, Training Accuracy: 95.955
[ Tue Jul  9 07:11:02 2024 ] 	Batch(2500/7879) done. Loss: 0.0456  lr:0.000100
[ Tue Jul  9 07:11:24 2024 ] 	Batch(2600/7879) done. Loss: 0.2531  lr:0.000100
[ Tue Jul  9 07:11:47 2024 ] 	Batch(2700/7879) done. Loss: 0.1406  lr:0.000100
[ Tue Jul  9 07:12:09 2024 ] 	Batch(2800/7879) done. Loss: 0.0107  lr:0.000100
[ Tue Jul  9 07:12:32 2024 ] 	Batch(2900/7879) done. Loss: 0.0333  lr:0.000100
[ Tue Jul  9 07:12:54 2024 ] 
Training: Epoch [87/120], Step [2999], Loss: 0.0771755501627922, Training Accuracy: 95.95416666666667
[ Tue Jul  9 07:12:55 2024 ] 	Batch(3000/7879) done. Loss: 0.1264  lr:0.000100
[ Tue Jul  9 07:13:17 2024 ] 	Batch(3100/7879) done. Loss: 0.7716  lr:0.000100
[ Tue Jul  9 07:13:40 2024 ] 	Batch(3200/7879) done. Loss: 0.0047  lr:0.000100
[ Tue Jul  9 07:14:02 2024 ] 	Batch(3300/7879) done. Loss: 0.1192  lr:0.000100
[ Tue Jul  9 07:14:25 2024 ] 	Batch(3400/7879) done. Loss: 0.1204  lr:0.000100
[ Tue Jul  9 07:14:47 2024 ] 
Training: Epoch [87/120], Step [3499], Loss: 0.06815163791179657, Training Accuracy: 95.96071428571429
[ Tue Jul  9 07:14:48 2024 ] 	Batch(3500/7879) done. Loss: 0.1557  lr:0.000100
[ Tue Jul  9 07:15:10 2024 ] 	Batch(3600/7879) done. Loss: 0.6419  lr:0.000100
[ Tue Jul  9 07:15:33 2024 ] 	Batch(3700/7879) done. Loss: 0.3446  lr:0.000100
[ Tue Jul  9 07:15:55 2024 ] 	Batch(3800/7879) done. Loss: 0.0777  lr:0.000100
[ Tue Jul  9 07:16:18 2024 ] 	Batch(3900/7879) done. Loss: 0.2185  lr:0.000100
[ Tue Jul  9 07:16:40 2024 ] 
Training: Epoch [87/120], Step [3999], Loss: 0.15020039677619934, Training Accuracy: 95.975
[ Tue Jul  9 07:16:41 2024 ] 	Batch(4000/7879) done. Loss: 0.0216  lr:0.000100
[ Tue Jul  9 07:17:03 2024 ] 	Batch(4100/7879) done. Loss: 0.5842  lr:0.000100
[ Tue Jul  9 07:17:26 2024 ] 	Batch(4200/7879) done. Loss: 0.0295  lr:0.000100
[ Tue Jul  9 07:17:49 2024 ] 	Batch(4300/7879) done. Loss: 0.6849  lr:0.000100
[ Tue Jul  9 07:18:12 2024 ] 	Batch(4400/7879) done. Loss: 0.0911  lr:0.000100
[ Tue Jul  9 07:18:34 2024 ] 
Training: Epoch [87/120], Step [4499], Loss: 0.32505571842193604, Training Accuracy: 95.94166666666666
[ Tue Jul  9 07:18:35 2024 ] 	Batch(4500/7879) done. Loss: 0.1193  lr:0.000100
[ Tue Jul  9 07:18:57 2024 ] 	Batch(4600/7879) done. Loss: 0.3628  lr:0.000100
[ Tue Jul  9 07:19:20 2024 ] 	Batch(4700/7879) done. Loss: 0.4020  lr:0.000100
[ Tue Jul  9 07:19:43 2024 ] 	Batch(4800/7879) done. Loss: 0.0206  lr:0.000100
[ Tue Jul  9 07:20:06 2024 ] 	Batch(4900/7879) done. Loss: 0.0971  lr:0.000100
[ Tue Jul  9 07:20:28 2024 ] 
Training: Epoch [87/120], Step [4999], Loss: 0.03298363834619522, Training Accuracy: 95.93
[ Tue Jul  9 07:20:28 2024 ] 	Batch(5000/7879) done. Loss: 0.0262  lr:0.000100
[ Tue Jul  9 07:20:51 2024 ] 	Batch(5100/7879) done. Loss: 0.0138  lr:0.000100
[ Tue Jul  9 07:21:14 2024 ] 	Batch(5200/7879) done. Loss: 0.1169  lr:0.000100
[ Tue Jul  9 07:21:37 2024 ] 	Batch(5300/7879) done. Loss: 0.0052  lr:0.000100
[ Tue Jul  9 07:21:59 2024 ] 	Batch(5400/7879) done. Loss: 0.3623  lr:0.000100
[ Tue Jul  9 07:22:22 2024 ] 
Training: Epoch [87/120], Step [5499], Loss: 0.14482097327709198, Training Accuracy: 95.93409090909091
[ Tue Jul  9 07:22:22 2024 ] 	Batch(5500/7879) done. Loss: 0.0180  lr:0.000100
[ Tue Jul  9 07:22:45 2024 ] 	Batch(5600/7879) done. Loss: 0.0794  lr:0.000100
[ Tue Jul  9 07:23:08 2024 ] 	Batch(5700/7879) done. Loss: 0.0746  lr:0.000100
[ Tue Jul  9 07:23:30 2024 ] 	Batch(5800/7879) done. Loss: 0.2088  lr:0.000100
[ Tue Jul  9 07:23:53 2024 ] 	Batch(5900/7879) done. Loss: 0.0385  lr:0.000100
[ Tue Jul  9 07:24:16 2024 ] 
Training: Epoch [87/120], Step [5999], Loss: 0.004798318259418011, Training Accuracy: 95.92500000000001
[ Tue Jul  9 07:24:16 2024 ] 	Batch(6000/7879) done. Loss: 0.2915  lr:0.000100
[ Tue Jul  9 07:24:39 2024 ] 	Batch(6100/7879) done. Loss: 0.7148  lr:0.000100
[ Tue Jul  9 07:25:01 2024 ] 	Batch(6200/7879) done. Loss: 0.1033  lr:0.000100
[ Tue Jul  9 07:25:24 2024 ] 	Batch(6300/7879) done. Loss: 0.4199  lr:0.000100
[ Tue Jul  9 07:25:47 2024 ] 	Batch(6400/7879) done. Loss: 0.2806  lr:0.000100
[ Tue Jul  9 07:26:10 2024 ] 
Training: Epoch [87/120], Step [6499], Loss: 0.14690236747264862, Training Accuracy: 95.88653846153846
[ Tue Jul  9 07:26:10 2024 ] 	Batch(6500/7879) done. Loss: 0.3847  lr:0.000100
[ Tue Jul  9 07:26:34 2024 ] 	Batch(6600/7879) done. Loss: 0.5852  lr:0.000100
[ Tue Jul  9 07:26:57 2024 ] 	Batch(6700/7879) done. Loss: 0.0208  lr:0.000100
[ Tue Jul  9 07:27:21 2024 ] 	Batch(6800/7879) done. Loss: 0.3102  lr:0.000100
[ Tue Jul  9 07:27:43 2024 ] 	Batch(6900/7879) done. Loss: 0.2817  lr:0.000100
[ Tue Jul  9 07:28:06 2024 ] 
Training: Epoch [87/120], Step [6999], Loss: 0.10428036004304886, Training Accuracy: 95.91964285714286
[ Tue Jul  9 07:28:06 2024 ] 	Batch(7000/7879) done. Loss: 0.2301  lr:0.000100
[ Tue Jul  9 07:28:29 2024 ] 	Batch(7100/7879) done. Loss: 0.0144  lr:0.000100
[ Tue Jul  9 07:28:52 2024 ] 	Batch(7200/7879) done. Loss: 0.1793  lr:0.000100
[ Tue Jul  9 07:29:15 2024 ] 	Batch(7300/7879) done. Loss: 0.3067  lr:0.000100
[ Tue Jul  9 07:29:39 2024 ] 	Batch(7400/7879) done. Loss: 0.0037  lr:0.000100
[ Tue Jul  9 07:30:02 2024 ] 
Training: Epoch [87/120], Step [7499], Loss: 0.09570659697055817, Training Accuracy: 95.91666666666666
[ Tue Jul  9 07:30:02 2024 ] 	Batch(7500/7879) done. Loss: 0.0098  lr:0.000100
[ Tue Jul  9 07:30:26 2024 ] 	Batch(7600/7879) done. Loss: 0.0260  lr:0.000100
[ Tue Jul  9 07:30:49 2024 ] 	Batch(7700/7879) done. Loss: 0.1697  lr:0.000100
[ Tue Jul  9 07:31:13 2024 ] 	Batch(7800/7879) done. Loss: 0.7903  lr:0.000100
[ Tue Jul  9 07:31:31 2024 ] 	Mean training loss: 0.1485.
[ Tue Jul  9 07:31:31 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 07:31:31 2024 ] Training epoch: 89
[ Tue Jul  9 07:31:32 2024 ] 	Batch(0/7879) done. Loss: 0.0141  lr:0.000100
[ Tue Jul  9 07:31:55 2024 ] 	Batch(100/7879) done. Loss: 0.0386  lr:0.000100
[ Tue Jul  9 07:32:18 2024 ] 	Batch(200/7879) done. Loss: 0.1574  lr:0.000100
[ Tue Jul  9 07:32:41 2024 ] 	Batch(300/7879) done. Loss: 0.1564  lr:0.000100
[ Tue Jul  9 07:33:04 2024 ] 	Batch(400/7879) done. Loss: 0.0623  lr:0.000100
[ Tue Jul  9 07:33:26 2024 ] 
Training: Epoch [88/120], Step [499], Loss: 0.01588658057153225, Training Accuracy: 96.15
[ Tue Jul  9 07:33:26 2024 ] 	Batch(500/7879) done. Loss: 0.0088  lr:0.000100
[ Tue Jul  9 07:33:49 2024 ] 	Batch(600/7879) done. Loss: 0.3422  lr:0.000100
[ Tue Jul  9 07:34:12 2024 ] 	Batch(700/7879) done. Loss: 0.0853  lr:0.000100
[ Tue Jul  9 07:34:34 2024 ] 	Batch(800/7879) done. Loss: 0.0831  lr:0.000100
[ Tue Jul  9 07:34:58 2024 ] 	Batch(900/7879) done. Loss: 0.1925  lr:0.000100
[ Tue Jul  9 07:35:21 2024 ] 
Training: Epoch [88/120], Step [999], Loss: 0.03881713002920151, Training Accuracy: 96.1375
[ Tue Jul  9 07:35:21 2024 ] 	Batch(1000/7879) done. Loss: 0.4194  lr:0.000100
[ Tue Jul  9 07:35:44 2024 ] 	Batch(1100/7879) done. Loss: 0.0363  lr:0.000100
[ Tue Jul  9 07:36:08 2024 ] 	Batch(1200/7879) done. Loss: 0.0541  lr:0.000100
[ Tue Jul  9 07:36:31 2024 ] 	Batch(1300/7879) done. Loss: 0.0251  lr:0.000100
[ Tue Jul  9 07:36:54 2024 ] 	Batch(1400/7879) done. Loss: 0.0160  lr:0.000100
[ Tue Jul  9 07:37:17 2024 ] 
Training: Epoch [88/120], Step [1499], Loss: 0.17448276281356812, Training Accuracy: 95.99166666666666
[ Tue Jul  9 07:37:18 2024 ] 	Batch(1500/7879) done. Loss: 0.0790  lr:0.000100
[ Tue Jul  9 07:37:41 2024 ] 	Batch(1600/7879) done. Loss: 0.0814  lr:0.000100
[ Tue Jul  9 07:38:04 2024 ] 	Batch(1700/7879) done. Loss: 0.7824  lr:0.000100
[ Tue Jul  9 07:38:26 2024 ] 	Batch(1800/7879) done. Loss: 0.0760  lr:0.000100
[ Tue Jul  9 07:38:49 2024 ] 	Batch(1900/7879) done. Loss: 0.1998  lr:0.000100
[ Tue Jul  9 07:39:11 2024 ] 
Training: Epoch [88/120], Step [1999], Loss: 0.07990092784166336, Training Accuracy: 95.96875
[ Tue Jul  9 07:39:12 2024 ] 	Batch(2000/7879) done. Loss: 0.3735  lr:0.000100
[ Tue Jul  9 07:39:35 2024 ] 	Batch(2100/7879) done. Loss: 0.0818  lr:0.000100
[ Tue Jul  9 07:39:58 2024 ] 	Batch(2200/7879) done. Loss: 0.0101  lr:0.000100
[ Tue Jul  9 07:40:21 2024 ] 	Batch(2300/7879) done. Loss: 0.2019  lr:0.000100
[ Tue Jul  9 07:40:45 2024 ] 	Batch(2400/7879) done. Loss: 0.0120  lr:0.000100
[ Tue Jul  9 07:41:08 2024 ] 
Training: Epoch [88/120], Step [2499], Loss: 0.49816885590553284, Training Accuracy: 96.08
[ Tue Jul  9 07:41:08 2024 ] 	Batch(2500/7879) done. Loss: 0.0159  lr:0.000100
[ Tue Jul  9 07:41:31 2024 ] 	Batch(2600/7879) done. Loss: 0.0741  lr:0.000100
[ Tue Jul  9 07:41:55 2024 ] 	Batch(2700/7879) done. Loss: 0.0954  lr:0.000100
[ Tue Jul  9 07:42:18 2024 ] 	Batch(2800/7879) done. Loss: 0.0564  lr:0.000100
[ Tue Jul  9 07:42:42 2024 ] 	Batch(2900/7879) done. Loss: 0.2048  lr:0.000100
[ Tue Jul  9 07:43:05 2024 ] 
Training: Epoch [88/120], Step [2999], Loss: 0.028395315632224083, Training Accuracy: 96.13333333333334
[ Tue Jul  9 07:43:05 2024 ] 	Batch(3000/7879) done. Loss: 0.3306  lr:0.000100
[ Tue Jul  9 07:43:28 2024 ] 	Batch(3100/7879) done. Loss: 0.0434  lr:0.000100
[ Tue Jul  9 07:43:52 2024 ] 	Batch(3200/7879) done. Loss: 0.1425  lr:0.000100
[ Tue Jul  9 07:44:15 2024 ] 	Batch(3300/7879) done. Loss: 0.0230  lr:0.000100
[ Tue Jul  9 07:44:38 2024 ] 	Batch(3400/7879) done. Loss: 0.1713  lr:0.000100
[ Tue Jul  9 07:45:01 2024 ] 
Training: Epoch [88/120], Step [3499], Loss: 0.0677449181675911, Training Accuracy: 96.14285714285714
[ Tue Jul  9 07:45:01 2024 ] 	Batch(3500/7879) done. Loss: 0.1306  lr:0.000100
[ Tue Jul  9 07:45:24 2024 ] 	Batch(3600/7879) done. Loss: 0.0843  lr:0.000100
[ Tue Jul  9 07:45:47 2024 ] 	Batch(3700/7879) done. Loss: 0.1733  lr:0.000100
[ Tue Jul  9 07:46:09 2024 ] 	Batch(3800/7879) done. Loss: 0.1369  lr:0.000100
[ Tue Jul  9 07:46:32 2024 ] 	Batch(3900/7879) done. Loss: 0.1129  lr:0.000100
[ Tue Jul  9 07:46:54 2024 ] 
Training: Epoch [88/120], Step [3999], Loss: 0.031018422916531563, Training Accuracy: 96.17812500000001
[ Tue Jul  9 07:46:55 2024 ] 	Batch(4000/7879) done. Loss: 0.0259  lr:0.000100
[ Tue Jul  9 07:47:17 2024 ] 	Batch(4100/7879) done. Loss: 0.0247  lr:0.000100
[ Tue Jul  9 07:47:40 2024 ] 	Batch(4200/7879) done. Loss: 0.0266  lr:0.000100
[ Tue Jul  9 07:48:02 2024 ] 	Batch(4300/7879) done. Loss: 0.2153  lr:0.000100
[ Tue Jul  9 07:48:25 2024 ] 	Batch(4400/7879) done. Loss: 0.1173  lr:0.000100
[ Tue Jul  9 07:48:47 2024 ] 
Training: Epoch [88/120], Step [4499], Loss: 0.08964996784925461, Training Accuracy: 96.16666666666667
[ Tue Jul  9 07:48:48 2024 ] 	Batch(4500/7879) done. Loss: 0.1346  lr:0.000100
[ Tue Jul  9 07:49:10 2024 ] 	Batch(4600/7879) done. Loss: 0.1789  lr:0.000100
[ Tue Jul  9 07:49:33 2024 ] 	Batch(4700/7879) done. Loss: 0.2850  lr:0.000100
[ Tue Jul  9 07:49:55 2024 ] 	Batch(4800/7879) done. Loss: 0.0240  lr:0.000100
[ Tue Jul  9 07:50:19 2024 ] 	Batch(4900/7879) done. Loss: 0.4035  lr:0.000100
[ Tue Jul  9 07:50:42 2024 ] 
Training: Epoch [88/120], Step [4999], Loss: 0.039072126150131226, Training Accuracy: 96.145
[ Tue Jul  9 07:50:42 2024 ] 	Batch(5000/7879) done. Loss: 0.3422  lr:0.000100
[ Tue Jul  9 07:51:05 2024 ] 	Batch(5100/7879) done. Loss: 0.0368  lr:0.000100
[ Tue Jul  9 07:51:28 2024 ] 	Batch(5200/7879) done. Loss: 0.0990  lr:0.000100
[ Tue Jul  9 07:51:51 2024 ] 	Batch(5300/7879) done. Loss: 0.0197  lr:0.000100
[ Tue Jul  9 07:52:13 2024 ] 	Batch(5400/7879) done. Loss: 0.0421  lr:0.000100
[ Tue Jul  9 07:52:36 2024 ] 
Training: Epoch [88/120], Step [5499], Loss: 0.012816422618925571, Training Accuracy: 96.12045454545455
[ Tue Jul  9 07:52:36 2024 ] 	Batch(5500/7879) done. Loss: 0.1633  lr:0.000100
[ Tue Jul  9 07:52:59 2024 ] 	Batch(5600/7879) done. Loss: 0.0825  lr:0.000100
[ Tue Jul  9 07:53:21 2024 ] 	Batch(5700/7879) done. Loss: 0.0980  lr:0.000100
[ Tue Jul  9 07:53:44 2024 ] 	Batch(5800/7879) done. Loss: 0.2916  lr:0.000100
[ Tue Jul  9 07:54:06 2024 ] 	Batch(5900/7879) done. Loss: 0.0673  lr:0.000100
[ Tue Jul  9 07:54:29 2024 ] 
Training: Epoch [88/120], Step [5999], Loss: 0.12030673772096634, Training Accuracy: 96.02708333333332
[ Tue Jul  9 07:54:29 2024 ] 	Batch(6000/7879) done. Loss: 0.0359  lr:0.000100
[ Tue Jul  9 07:54:52 2024 ] 	Batch(6100/7879) done. Loss: 0.0287  lr:0.000100
[ Tue Jul  9 07:55:14 2024 ] 	Batch(6200/7879) done. Loss: 0.3429  lr:0.000100
[ Tue Jul  9 07:55:37 2024 ] 	Batch(6300/7879) done. Loss: 0.1066  lr:0.000100
[ Tue Jul  9 07:56:00 2024 ] 	Batch(6400/7879) done. Loss: 0.8727  lr:0.000100
[ Tue Jul  9 07:56:22 2024 ] 
Training: Epoch [88/120], Step [6499], Loss: 0.12575048208236694, Training Accuracy: 96.00192307692308
[ Tue Jul  9 07:56:22 2024 ] 	Batch(6500/7879) done. Loss: 0.0222  lr:0.000100
[ Tue Jul  9 07:56:45 2024 ] 	Batch(6600/7879) done. Loss: 0.4246  lr:0.000100
[ Tue Jul  9 07:57:08 2024 ] 	Batch(6700/7879) done. Loss: 0.0410  lr:0.000100
[ Tue Jul  9 07:57:30 2024 ] 	Batch(6800/7879) done. Loss: 0.1576  lr:0.000100
[ Tue Jul  9 07:57:53 2024 ] 	Batch(6900/7879) done. Loss: 0.0298  lr:0.000100
[ Tue Jul  9 07:58:15 2024 ] 
Training: Epoch [88/120], Step [6999], Loss: 0.5097368359565735, Training Accuracy: 96.01071428571429
[ Tue Jul  9 07:58:16 2024 ] 	Batch(7000/7879) done. Loss: 0.0960  lr:0.000100
[ Tue Jul  9 07:58:38 2024 ] 	Batch(7100/7879) done. Loss: 0.0619  lr:0.000100
[ Tue Jul  9 07:59:01 2024 ] 	Batch(7200/7879) done. Loss: 0.0710  lr:0.000100
[ Tue Jul  9 07:59:24 2024 ] 	Batch(7300/7879) done. Loss: 0.1881  lr:0.000100
[ Tue Jul  9 07:59:46 2024 ] 	Batch(7400/7879) done. Loss: 0.0290  lr:0.000100
[ Tue Jul  9 08:00:09 2024 ] 
Training: Epoch [88/120], Step [7499], Loss: 0.1312810480594635, Training Accuracy: 96.00666666666666
[ Tue Jul  9 08:00:09 2024 ] 	Batch(7500/7879) done. Loss: 0.0407  lr:0.000100
[ Tue Jul  9 08:00:31 2024 ] 	Batch(7600/7879) done. Loss: 0.1744  lr:0.000100
[ Tue Jul  9 08:00:54 2024 ] 	Batch(7700/7879) done. Loss: 0.1639  lr:0.000100
[ Tue Jul  9 08:01:17 2024 ] 	Batch(7800/7879) done. Loss: 0.0373  lr:0.000100
[ Tue Jul  9 08:01:34 2024 ] 	Mean training loss: 0.1458.
[ Tue Jul  9 08:01:34 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 08:01:35 2024 ] Training epoch: 90
[ Tue Jul  9 08:01:35 2024 ] 	Batch(0/7879) done. Loss: 0.1016  lr:0.000100
[ Tue Jul  9 08:01:58 2024 ] 	Batch(100/7879) done. Loss: 0.3392  lr:0.000100
[ Tue Jul  9 08:02:21 2024 ] 	Batch(200/7879) done. Loss: 0.1831  lr:0.000100
[ Tue Jul  9 08:02:44 2024 ] 	Batch(300/7879) done. Loss: 0.0361  lr:0.000100
[ Tue Jul  9 08:03:06 2024 ] 	Batch(400/7879) done. Loss: 0.4219  lr:0.000100
[ Tue Jul  9 08:03:29 2024 ] 
Training: Epoch [89/120], Step [499], Loss: 0.1360628753900528, Training Accuracy: 96.22500000000001
[ Tue Jul  9 08:03:29 2024 ] 	Batch(500/7879) done. Loss: 0.0764  lr:0.000100
[ Tue Jul  9 08:03:52 2024 ] 	Batch(600/7879) done. Loss: 0.0188  lr:0.000100
[ Tue Jul  9 08:04:14 2024 ] 	Batch(700/7879) done. Loss: 0.2966  lr:0.000100
[ Tue Jul  9 08:04:37 2024 ] 	Batch(800/7879) done. Loss: 0.0169  lr:0.000100
[ Tue Jul  9 08:05:00 2024 ] 	Batch(900/7879) done. Loss: 0.1116  lr:0.000100
[ Tue Jul  9 08:05:22 2024 ] 
Training: Epoch [89/120], Step [999], Loss: 0.00860226433724165, Training Accuracy: 96.3125
[ Tue Jul  9 08:05:23 2024 ] 	Batch(1000/7879) done. Loss: 0.1266  lr:0.000100
[ Tue Jul  9 08:05:45 2024 ] 	Batch(1100/7879) done. Loss: 0.0667  lr:0.000100
[ Tue Jul  9 08:06:08 2024 ] 	Batch(1200/7879) done. Loss: 0.0332  lr:0.000100
[ Tue Jul  9 08:06:31 2024 ] 	Batch(1300/7879) done. Loss: 0.0649  lr:0.000100
[ Tue Jul  9 08:06:53 2024 ] 	Batch(1400/7879) done. Loss: 0.1340  lr:0.000100
[ Tue Jul  9 08:07:16 2024 ] 
Training: Epoch [89/120], Step [1499], Loss: 0.01409645564854145, Training Accuracy: 96.25
[ Tue Jul  9 08:07:16 2024 ] 	Batch(1500/7879) done. Loss: 0.0475  lr:0.000100
[ Tue Jul  9 08:07:39 2024 ] 	Batch(1600/7879) done. Loss: 0.1020  lr:0.000100
[ Tue Jul  9 08:08:02 2024 ] 	Batch(1700/7879) done. Loss: 0.1262  lr:0.000100
[ Tue Jul  9 08:08:24 2024 ] 	Batch(1800/7879) done. Loss: 0.0356  lr:0.000100
[ Tue Jul  9 08:08:47 2024 ] 	Batch(1900/7879) done. Loss: 0.1423  lr:0.000100
[ Tue Jul  9 08:09:10 2024 ] 
Training: Epoch [89/120], Step [1999], Loss: 0.03157993033528328, Training Accuracy: 96.04375
[ Tue Jul  9 08:09:10 2024 ] 	Batch(2000/7879) done. Loss: 0.2218  lr:0.000100
[ Tue Jul  9 08:09:33 2024 ] 	Batch(2100/7879) done. Loss: 0.2654  lr:0.000100
[ Tue Jul  9 08:09:55 2024 ] 	Batch(2200/7879) done. Loss: 0.0616  lr:0.000100
[ Tue Jul  9 08:10:18 2024 ] 	Batch(2300/7879) done. Loss: 0.1366  lr:0.000100
[ Tue Jul  9 08:10:41 2024 ] 	Batch(2400/7879) done. Loss: 0.0456  lr:0.000100
[ Tue Jul  9 08:11:03 2024 ] 
Training: Epoch [89/120], Step [2499], Loss: 0.7222117185592651, Training Accuracy: 96.095
[ Tue Jul  9 08:11:04 2024 ] 	Batch(2500/7879) done. Loss: 0.2886  lr:0.000100
[ Tue Jul  9 08:11:26 2024 ] 	Batch(2600/7879) done. Loss: 0.6134  lr:0.000100
[ Tue Jul  9 08:11:49 2024 ] 	Batch(2700/7879) done. Loss: 0.0334  lr:0.000100
[ Tue Jul  9 08:12:12 2024 ] 	Batch(2800/7879) done. Loss: 0.3011  lr:0.000100
[ Tue Jul  9 08:12:35 2024 ] 	Batch(2900/7879) done. Loss: 0.0228  lr:0.000100
[ Tue Jul  9 08:12:58 2024 ] 
Training: Epoch [89/120], Step [2999], Loss: 0.14560064673423767, Training Accuracy: 96.14583333333333
[ Tue Jul  9 08:12:58 2024 ] 	Batch(3000/7879) done. Loss: 0.0416  lr:0.000100
[ Tue Jul  9 08:13:20 2024 ] 	Batch(3100/7879) done. Loss: 0.0582  lr:0.000100
[ Tue Jul  9 08:13:43 2024 ] 	Batch(3200/7879) done. Loss: 0.0568  lr:0.000100
[ Tue Jul  9 08:14:06 2024 ] 	Batch(3300/7879) done. Loss: 0.0194  lr:0.000100
[ Tue Jul  9 08:14:29 2024 ] 	Batch(3400/7879) done. Loss: 0.0756  lr:0.000100
[ Tue Jul  9 08:14:51 2024 ] 
Training: Epoch [89/120], Step [3499], Loss: 0.004791741259396076, Training Accuracy: 96.15714285714286
[ Tue Jul  9 08:14:51 2024 ] 	Batch(3500/7879) done. Loss: 0.0564  lr:0.000100
[ Tue Jul  9 08:15:14 2024 ] 	Batch(3600/7879) done. Loss: 0.1478  lr:0.000100
[ Tue Jul  9 08:15:38 2024 ] 	Batch(3700/7879) done. Loss: 0.0233  lr:0.000100
[ Tue Jul  9 08:16:01 2024 ] 	Batch(3800/7879) done. Loss: 0.0095  lr:0.000100
[ Tue Jul  9 08:16:25 2024 ] 	Batch(3900/7879) done. Loss: 0.0504  lr:0.000100
[ Tue Jul  9 08:16:48 2024 ] 
Training: Epoch [89/120], Step [3999], Loss: 0.006667460780590773, Training Accuracy: 96.1125
[ Tue Jul  9 08:16:48 2024 ] 	Batch(4000/7879) done. Loss: 0.0229  lr:0.000100
[ Tue Jul  9 08:17:12 2024 ] 	Batch(4100/7879) done. Loss: 0.4131  lr:0.000100
[ Tue Jul  9 08:17:35 2024 ] 	Batch(4200/7879) done. Loss: 0.0450  lr:0.000100
[ Tue Jul  9 08:17:57 2024 ] 	Batch(4300/7879) done. Loss: 0.0895  lr:0.000100
[ Tue Jul  9 08:18:20 2024 ] 	Batch(4400/7879) done. Loss: 0.1215  lr:0.000100
[ Tue Jul  9 08:18:43 2024 ] 
Training: Epoch [89/120], Step [4499], Loss: 0.013356835581362247, Training Accuracy: 96.125
[ Tue Jul  9 08:18:43 2024 ] 	Batch(4500/7879) done. Loss: 0.0134  lr:0.000100
[ Tue Jul  9 08:19:06 2024 ] 	Batch(4600/7879) done. Loss: 0.0468  lr:0.000100
[ Tue Jul  9 08:19:28 2024 ] 	Batch(4700/7879) done. Loss: 0.0675  lr:0.000100
[ Tue Jul  9 08:19:51 2024 ] 	Batch(4800/7879) done. Loss: 0.1100  lr:0.000100
[ Tue Jul  9 08:20:14 2024 ] 	Batch(4900/7879) done. Loss: 0.1896  lr:0.000100
[ Tue Jul  9 08:20:36 2024 ] 
Training: Epoch [89/120], Step [4999], Loss: 0.005709806922823191, Training Accuracy: 96.15
[ Tue Jul  9 08:20:36 2024 ] 	Batch(5000/7879) done. Loss: 0.2366  lr:0.000100
[ Tue Jul  9 08:20:59 2024 ] 	Batch(5100/7879) done. Loss: 0.2006  lr:0.000100
[ Tue Jul  9 08:21:22 2024 ] 	Batch(5200/7879) done. Loss: 0.2706  lr:0.000100
[ Tue Jul  9 08:21:45 2024 ] 	Batch(5300/7879) done. Loss: 0.1782  lr:0.000100
[ Tue Jul  9 08:22:07 2024 ] 	Batch(5400/7879) done. Loss: 0.1450  lr:0.000100
[ Tue Jul  9 08:22:30 2024 ] 
Training: Epoch [89/120], Step [5499], Loss: 0.14260736107826233, Training Accuracy: 96.11818181818181
[ Tue Jul  9 08:22:30 2024 ] 	Batch(5500/7879) done. Loss: 0.0327  lr:0.000100
[ Tue Jul  9 08:22:53 2024 ] 	Batch(5600/7879) done. Loss: 0.1559  lr:0.000100
[ Tue Jul  9 08:23:16 2024 ] 	Batch(5700/7879) done. Loss: 0.7507  lr:0.000100
[ Tue Jul  9 08:23:40 2024 ] 	Batch(5800/7879) done. Loss: 0.2646  lr:0.000100
[ Tue Jul  9 08:24:03 2024 ] 	Batch(5900/7879) done. Loss: 0.0315  lr:0.000100
[ Tue Jul  9 08:24:27 2024 ] 
Training: Epoch [89/120], Step [5999], Loss: 0.2867010533809662, Training Accuracy: 96.10416666666667
[ Tue Jul  9 08:24:27 2024 ] 	Batch(6000/7879) done. Loss: 0.0607  lr:0.000100
[ Tue Jul  9 08:24:50 2024 ] 	Batch(6100/7879) done. Loss: 0.3057  lr:0.000100
[ Tue Jul  9 08:25:12 2024 ] 	Batch(6200/7879) done. Loss: 0.1427  lr:0.000100
[ Tue Jul  9 08:25:35 2024 ] 	Batch(6300/7879) done. Loss: 0.4557  lr:0.000100
[ Tue Jul  9 08:25:58 2024 ] 	Batch(6400/7879) done. Loss: 0.0686  lr:0.000100
[ Tue Jul  9 08:26:21 2024 ] 
Training: Epoch [89/120], Step [6499], Loss: 0.04870517924427986, Training Accuracy: 96.1
[ Tue Jul  9 08:26:21 2024 ] 	Batch(6500/7879) done. Loss: 0.0047  lr:0.000100
[ Tue Jul  9 08:26:44 2024 ] 	Batch(6600/7879) done. Loss: 0.0388  lr:0.000100
[ Tue Jul  9 08:27:06 2024 ] 	Batch(6700/7879) done. Loss: 0.1608  lr:0.000100
[ Tue Jul  9 08:27:29 2024 ] 	Batch(6800/7879) done. Loss: 0.0302  lr:0.000100
[ Tue Jul  9 08:27:52 2024 ] 	Batch(6900/7879) done. Loss: 0.0055  lr:0.000100
[ Tue Jul  9 08:28:15 2024 ] 
Training: Epoch [89/120], Step [6999], Loss: 0.01789053902029991, Training Accuracy: 96.1125
[ Tue Jul  9 08:28:15 2024 ] 	Batch(7000/7879) done. Loss: 0.0295  lr:0.000100
[ Tue Jul  9 08:28:37 2024 ] 	Batch(7100/7879) done. Loss: 0.0249  lr:0.000100
[ Tue Jul  9 08:29:00 2024 ] 	Batch(7200/7879) done. Loss: 0.0237  lr:0.000100
[ Tue Jul  9 08:29:24 2024 ] 	Batch(7300/7879) done. Loss: 0.0161  lr:0.000100
[ Tue Jul  9 08:29:47 2024 ] 	Batch(7400/7879) done. Loss: 0.0046  lr:0.000100
[ Tue Jul  9 08:30:10 2024 ] 
Training: Epoch [89/120], Step [7499], Loss: 0.008369015529751778, Training Accuracy: 96.10666666666667
[ Tue Jul  9 08:30:11 2024 ] 	Batch(7500/7879) done. Loss: 0.0977  lr:0.000100
[ Tue Jul  9 08:30:34 2024 ] 	Batch(7600/7879) done. Loss: 0.1621  lr:0.000100
[ Tue Jul  9 08:30:58 2024 ] 	Batch(7700/7879) done. Loss: 0.0910  lr:0.000100
[ Tue Jul  9 08:31:21 2024 ] 	Batch(7800/7879) done. Loss: 0.1615  lr:0.000100
[ Tue Jul  9 08:31:39 2024 ] 	Mean training loss: 0.1435.
[ Tue Jul  9 08:31:39 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 08:31:40 2024 ] Eval epoch: 90
[ Tue Jul  9 08:37:36 2024 ] 	Mean val loss of 6365 batches: 1.0187462910700338.
[ Tue Jul  9 08:37:36 2024 ] 
Validation: Epoch [89/120], Samples [39378.0/50919], Loss: 0.3786768317222595, Validation Accuracy: 77.33459023154423
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 1 : 200 / 275 = 72 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 2 : 230 / 273 = 84 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 3 : 237 / 273 = 86 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 4 : 223 / 275 = 81 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 5 : 221 / 275 = 80 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 6 : 207 / 275 = 75 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 7 : 244 / 273 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 8 : 265 / 273 = 97 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 9 : 195 / 273 = 71 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 10 : 125 / 273 = 45 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 11 : 157 / 272 = 57 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 12 : 213 / 271 = 78 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 13 : 263 / 275 = 95 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 14 : 262 / 276 = 94 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 15 : 223 / 273 = 81 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 16 : 154 / 274 = 56 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 17 : 238 / 273 = 87 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 18 : 231 / 274 = 84 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 19 : 243 / 272 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 20 : 254 / 273 = 93 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 21 : 222 / 274 = 81 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 22 : 249 / 274 = 90 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 23 : 243 / 276 = 88 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 24 : 244 / 274 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 25 : 264 / 275 = 96 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 26 : 269 / 276 = 97 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 27 : 229 / 275 = 83 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 28 : 180 / 275 = 65 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 29 : 147 / 275 = 53 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 30 : 175 / 276 = 63 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 31 : 231 / 276 = 83 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 32 : 239 / 276 = 86 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 33 : 233 / 276 = 84 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 34 : 239 / 276 = 86 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 35 : 238 / 275 = 86 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 36 : 221 / 276 = 80 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 37 : 254 / 276 = 92 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 38 : 239 / 276 = 86 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 39 : 240 / 276 = 86 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 40 : 203 / 276 = 73 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 41 : 263 / 276 = 95 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 42 : 251 / 275 = 91 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 43 : 187 / 276 = 67 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 44 : 259 / 276 = 93 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 45 : 259 / 276 = 93 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 46 : 230 / 276 = 83 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 47 : 204 / 275 = 74 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 48 : 223 / 275 = 81 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 49 : 225 / 274 = 82 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 50 : 237 / 276 = 85 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 51 : 255 / 276 = 92 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 52 : 250 / 276 = 90 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 53 : 242 / 276 = 87 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 54 : 255 / 274 = 93 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 55 : 237 / 276 = 85 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 56 : 246 / 275 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 57 : 269 / 276 = 97 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 58 : 267 / 273 = 97 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 59 : 248 / 276 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 60 : 473 / 561 = 84 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 61 : 479 / 566 = 84 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 62 : 409 / 572 = 71 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 63 : 484 / 570 = 84 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 64 : 416 / 574 = 72 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 65 : 501 / 573 = 87 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 66 : 409 / 573 = 71 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 67 : 391 / 575 = 68 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 68 : 362 / 575 = 62 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 69 : 474 / 575 = 82 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 70 : 226 / 575 = 39 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 71 : 252 / 575 = 43 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 72 : 103 / 571 = 18 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 73 : 254 / 570 = 44 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 74 : 353 / 569 = 62 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 75 : 344 / 573 = 60 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 76 : 369 / 574 = 64 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 77 : 371 / 573 = 64 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 78 : 445 / 575 = 77 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 79 : 550 / 574 = 95 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 80 : 462 / 573 = 80 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 81 : 343 / 575 = 59 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 82 : 341 / 575 = 59 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 83 : 250 / 572 = 43 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 84 : 435 / 574 = 75 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 85 : 380 / 574 = 66 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 86 : 492 / 575 = 85 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 87 : 493 / 576 = 85 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 88 : 390 / 575 = 67 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 89 : 452 / 576 = 78 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 90 : 260 / 574 = 45 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 91 : 447 / 568 = 78 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 92 : 405 / 576 = 70 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 93 : 324 / 573 = 56 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 94 : 515 / 574 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 95 : 537 / 575 = 93 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 96 : 556 / 575 = 96 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 97 : 553 / 574 = 96 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 98 : 539 / 575 = 93 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 99 : 540 / 574 = 94 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 100 : 458 / 574 = 79 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 101 : 519 / 574 = 90 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 102 : 352 / 575 = 61 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 103 : 486 / 576 = 84 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 104 : 292 / 575 = 50 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 105 : 269 / 575 = 46 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 106 : 319 / 576 = 55 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 107 : 513 / 576 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 108 : 482 / 575 = 83 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 109 : 400 / 575 = 69 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 110 : 515 / 575 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 111 : 544 / 576 = 94 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 112 : 544 / 575 = 94 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 113 : 525 / 576 = 91 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 114 : 512 / 576 = 88 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 115 : 523 / 576 = 90 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 116 : 483 / 575 = 84 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 117 : 481 / 575 = 83 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 118 : 473 / 575 = 82 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 119 : 514 / 576 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Accuracy of 120 : 244 / 274 = 89 %
[ Tue Jul  9 08:37:36 2024 ] Training epoch: 91
[ Tue Jul  9 08:37:36 2024 ] 	Batch(0/7879) done. Loss: 0.0258  lr:0.000001
[ Tue Jul  9 08:37:59 2024 ] 	Batch(100/7879) done. Loss: 0.0414  lr:0.000001
[ Tue Jul  9 08:38:22 2024 ] 	Batch(200/7879) done. Loss: 0.6090  lr:0.000001
[ Tue Jul  9 08:38:44 2024 ] 	Batch(300/7879) done. Loss: 0.0665  lr:0.000001
[ Tue Jul  9 08:39:07 2024 ] 	Batch(400/7879) done. Loss: 0.0054  lr:0.000001
[ Tue Jul  9 08:39:30 2024 ] 
Training: Epoch [90/120], Step [499], Loss: 0.13446538150310516, Training Accuracy: 96.42500000000001
[ Tue Jul  9 08:39:30 2024 ] 	Batch(500/7879) done. Loss: 0.0103  lr:0.000001
[ Tue Jul  9 08:39:52 2024 ] 	Batch(600/7879) done. Loss: 0.0010  lr:0.000001
[ Tue Jul  9 08:40:15 2024 ] 	Batch(700/7879) done. Loss: 0.1611  lr:0.000001
[ Tue Jul  9 08:40:38 2024 ] 	Batch(800/7879) done. Loss: 0.0706  lr:0.000001
[ Tue Jul  9 08:41:01 2024 ] 	Batch(900/7879) done. Loss: 0.0786  lr:0.000001
[ Tue Jul  9 08:41:24 2024 ] 
Training: Epoch [90/120], Step [999], Loss: 0.007988900877535343, Training Accuracy: 96.1375
[ Tue Jul  9 08:41:24 2024 ] 	Batch(1000/7879) done. Loss: 0.0093  lr:0.000001
[ Tue Jul  9 08:41:47 2024 ] 	Batch(1100/7879) done. Loss: 0.0267  lr:0.000001
[ Tue Jul  9 08:42:10 2024 ] 	Batch(1200/7879) done. Loss: 0.5311  lr:0.000001
[ Tue Jul  9 08:42:33 2024 ] 	Batch(1300/7879) done. Loss: 0.2104  lr:0.000001
[ Tue Jul  9 08:42:57 2024 ] 	Batch(1400/7879) done. Loss: 0.0880  lr:0.000001
[ Tue Jul  9 08:43:20 2024 ] 
Training: Epoch [90/120], Step [1499], Loss: 0.26678466796875, Training Accuracy: 96.21666666666667
[ Tue Jul  9 08:43:20 2024 ] 	Batch(1500/7879) done. Loss: 0.0155  lr:0.000001
[ Tue Jul  9 08:43:44 2024 ] 	Batch(1600/7879) done. Loss: 0.2771  lr:0.000001
[ Tue Jul  9 08:44:07 2024 ] 	Batch(1700/7879) done. Loss: 0.3058  lr:0.000001
[ Tue Jul  9 08:44:31 2024 ] 	Batch(1800/7879) done. Loss: 0.3628  lr:0.000001
[ Tue Jul  9 08:44:54 2024 ] 	Batch(1900/7879) done. Loss: 0.0732  lr:0.000001
[ Tue Jul  9 08:45:17 2024 ] 
Training: Epoch [90/120], Step [1999], Loss: 0.25724321603775024, Training Accuracy: 96.11874999999999
[ Tue Jul  9 08:45:18 2024 ] 	Batch(2000/7879) done. Loss: 0.0306  lr:0.000001
[ Tue Jul  9 08:45:41 2024 ] 	Batch(2100/7879) done. Loss: 0.0526  lr:0.000001
[ Tue Jul  9 08:46:03 2024 ] 	Batch(2200/7879) done. Loss: 0.1507  lr:0.000001
[ Tue Jul  9 08:46:26 2024 ] 	Batch(2300/7879) done. Loss: 0.0714  lr:0.000001
[ Tue Jul  9 08:46:49 2024 ] 	Batch(2400/7879) done. Loss: 0.0328  lr:0.000001
[ Tue Jul  9 08:47:12 2024 ] 
Training: Epoch [90/120], Step [2499], Loss: 0.007958941161632538, Training Accuracy: 96.08
[ Tue Jul  9 08:47:12 2024 ] 	Batch(2500/7879) done. Loss: 0.0503  lr:0.000001
[ Tue Jul  9 08:47:35 2024 ] 	Batch(2600/7879) done. Loss: 0.3482  lr:0.000001
[ Tue Jul  9 08:47:57 2024 ] 	Batch(2700/7879) done. Loss: 0.0030  lr:0.000001
[ Tue Jul  9 08:48:20 2024 ] 	Batch(2800/7879) done. Loss: 0.0502  lr:0.000001
[ Tue Jul  9 08:48:43 2024 ] 	Batch(2900/7879) done. Loss: 0.0812  lr:0.000001
[ Tue Jul  9 08:49:06 2024 ] 
Training: Epoch [90/120], Step [2999], Loss: 0.24105024337768555, Training Accuracy: 96.05416666666666
[ Tue Jul  9 08:49:06 2024 ] 	Batch(3000/7879) done. Loss: 0.2396  lr:0.000001
[ Tue Jul  9 08:49:28 2024 ] 	Batch(3100/7879) done. Loss: 0.0713  lr:0.000001
[ Tue Jul  9 08:49:51 2024 ] 	Batch(3200/7879) done. Loss: 0.1069  lr:0.000001
[ Tue Jul  9 08:50:14 2024 ] 	Batch(3300/7879) done. Loss: 0.1112  lr:0.000001
[ Tue Jul  9 08:50:37 2024 ] 	Batch(3400/7879) done. Loss: 0.4515  lr:0.000001
[ Tue Jul  9 08:50:59 2024 ] 
Training: Epoch [90/120], Step [3499], Loss: 0.034632738679647446, Training Accuracy: 96.05714285714285
[ Tue Jul  9 08:50:59 2024 ] 	Batch(3500/7879) done. Loss: 0.3670  lr:0.000001
[ Tue Jul  9 08:51:22 2024 ] 	Batch(3600/7879) done. Loss: 0.0239  lr:0.000001
[ Tue Jul  9 08:51:45 2024 ] 	Batch(3700/7879) done. Loss: 0.2838  lr:0.000001
[ Tue Jul  9 08:52:08 2024 ] 	Batch(3800/7879) done. Loss: 0.2873  lr:0.000001
[ Tue Jul  9 08:52:30 2024 ] 	Batch(3900/7879) done. Loss: 0.0678  lr:0.000001
[ Tue Jul  9 08:52:53 2024 ] 
Training: Epoch [90/120], Step [3999], Loss: 0.1723545789718628, Training Accuracy: 96.065625
[ Tue Jul  9 08:52:53 2024 ] 	Batch(4000/7879) done. Loss: 0.4346  lr:0.000001
[ Tue Jul  9 08:53:16 2024 ] 	Batch(4100/7879) done. Loss: 0.0324  lr:0.000001
[ Tue Jul  9 08:53:39 2024 ] 	Batch(4200/7879) done. Loss: 0.1049  lr:0.000001
[ Tue Jul  9 08:54:01 2024 ] 	Batch(4300/7879) done. Loss: 0.0248  lr:0.000001
[ Tue Jul  9 08:54:24 2024 ] 	Batch(4400/7879) done. Loss: 0.1953  lr:0.000001
[ Tue Jul  9 08:54:47 2024 ] 
Training: Epoch [90/120], Step [4499], Loss: 0.4027806222438812, Training Accuracy: 96.08888888888889
[ Tue Jul  9 08:54:47 2024 ] 	Batch(4500/7879) done. Loss: 0.4938  lr:0.000001
[ Tue Jul  9 08:55:10 2024 ] 	Batch(4600/7879) done. Loss: 0.0151  lr:0.000001
[ Tue Jul  9 08:55:32 2024 ] 	Batch(4700/7879) done. Loss: 0.0677  lr:0.000001
[ Tue Jul  9 08:55:55 2024 ] 	Batch(4800/7879) done. Loss: 0.0225  lr:0.000001
[ Tue Jul  9 08:56:18 2024 ] 	Batch(4900/7879) done. Loss: 0.1265  lr:0.000001
[ Tue Jul  9 08:56:40 2024 ] 
Training: Epoch [90/120], Step [4999], Loss: 0.013208478689193726, Training Accuracy: 96.1275
[ Tue Jul  9 08:56:41 2024 ] 	Batch(5000/7879) done. Loss: 0.0863  lr:0.000001
[ Tue Jul  9 08:57:03 2024 ] 	Batch(5100/7879) done. Loss: 0.2196  lr:0.000001
[ Tue Jul  9 08:57:26 2024 ] 	Batch(5200/7879) done. Loss: 0.0642  lr:0.000001
[ Tue Jul  9 08:57:49 2024 ] 	Batch(5300/7879) done. Loss: 0.0311  lr:0.000001
[ Tue Jul  9 08:58:12 2024 ] 	Batch(5400/7879) done. Loss: 0.1129  lr:0.000001
[ Tue Jul  9 08:58:34 2024 ] 
Training: Epoch [90/120], Step [5499], Loss: 0.06056215614080429, Training Accuracy: 96.125
[ Tue Jul  9 08:58:34 2024 ] 	Batch(5500/7879) done. Loss: 0.2636  lr:0.000001
[ Tue Jul  9 08:58:57 2024 ] 	Batch(5600/7879) done. Loss: 0.0839  lr:0.000001
[ Tue Jul  9 08:59:20 2024 ] 	Batch(5700/7879) done. Loss: 0.3771  lr:0.000001
[ Tue Jul  9 08:59:42 2024 ] 	Batch(5800/7879) done. Loss: 0.0156  lr:0.000001
[ Tue Jul  9 09:00:05 2024 ] 	Batch(5900/7879) done. Loss: 0.0447  lr:0.000001
[ Tue Jul  9 09:00:28 2024 ] 
Training: Epoch [90/120], Step [5999], Loss: 0.008600611239671707, Training Accuracy: 96.1
[ Tue Jul  9 09:00:28 2024 ] 	Batch(6000/7879) done. Loss: 0.0507  lr:0.000001
[ Tue Jul  9 09:00:51 2024 ] 	Batch(6100/7879) done. Loss: 0.0932  lr:0.000001
[ Tue Jul  9 09:01:13 2024 ] 	Batch(6200/7879) done. Loss: 0.1753  lr:0.000001
[ Tue Jul  9 09:01:36 2024 ] 	Batch(6300/7879) done. Loss: 0.0141  lr:0.000001
[ Tue Jul  9 09:01:59 2024 ] 	Batch(6400/7879) done. Loss: 0.0180  lr:0.000001
[ Tue Jul  9 09:02:22 2024 ] 
Training: Epoch [90/120], Step [6499], Loss: 0.24138173460960388, Training Accuracy: 96.11730769230769
[ Tue Jul  9 09:02:22 2024 ] 	Batch(6500/7879) done. Loss: 0.1061  lr:0.000001
[ Tue Jul  9 09:02:46 2024 ] 	Batch(6600/7879) done. Loss: 0.2026  lr:0.000001
[ Tue Jul  9 09:03:09 2024 ] 	Batch(6700/7879) done. Loss: 0.3951  lr:0.000001
[ Tue Jul  9 09:03:33 2024 ] 	Batch(6800/7879) done. Loss: 0.0821  lr:0.000001
[ Tue Jul  9 09:03:56 2024 ] 	Batch(6900/7879) done. Loss: 0.0220  lr:0.000001
[ Tue Jul  9 09:04:19 2024 ] 
Training: Epoch [90/120], Step [6999], Loss: 0.2451256960630417, Training Accuracy: 96.09821428571429
[ Tue Jul  9 09:04:19 2024 ] 	Batch(7000/7879) done. Loss: 0.3109  lr:0.000001
[ Tue Jul  9 09:04:43 2024 ] 	Batch(7100/7879) done. Loss: 0.0929  lr:0.000001
[ Tue Jul  9 09:05:07 2024 ] 	Batch(7200/7879) done. Loss: 0.2402  lr:0.000001
[ Tue Jul  9 09:05:30 2024 ] 	Batch(7300/7879) done. Loss: 0.2239  lr:0.000001
[ Tue Jul  9 09:05:52 2024 ] 	Batch(7400/7879) done. Loss: 0.0249  lr:0.000001
[ Tue Jul  9 09:06:15 2024 ] 
Training: Epoch [90/120], Step [7499], Loss: 0.01400915626436472, Training Accuracy: 96.09833333333333
[ Tue Jul  9 09:06:15 2024 ] 	Batch(7500/7879) done. Loss: 0.0257  lr:0.000001
[ Tue Jul  9 09:06:38 2024 ] 	Batch(7600/7879) done. Loss: 0.0050  lr:0.000001
[ Tue Jul  9 09:07:01 2024 ] 	Batch(7700/7879) done. Loss: 0.0154  lr:0.000001
[ Tue Jul  9 09:07:23 2024 ] 	Batch(7800/7879) done. Loss: 0.2215  lr:0.000001
[ Tue Jul  9 09:07:41 2024 ] 	Mean training loss: 0.1444.
[ Tue Jul  9 09:07:41 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 09:07:41 2024 ] Training epoch: 92
[ Tue Jul  9 09:07:42 2024 ] 	Batch(0/7879) done. Loss: 0.5310  lr:0.000001
[ Tue Jul  9 09:08:04 2024 ] 	Batch(100/7879) done. Loss: 0.0070  lr:0.000001
[ Tue Jul  9 09:08:27 2024 ] 	Batch(200/7879) done. Loss: 0.3869  lr:0.000001
[ Tue Jul  9 09:08:50 2024 ] 	Batch(300/7879) done. Loss: 0.0647  lr:0.000001
[ Tue Jul  9 09:09:12 2024 ] 	Batch(400/7879) done. Loss: 0.1645  lr:0.000001
[ Tue Jul  9 09:09:35 2024 ] 
Training: Epoch [91/120], Step [499], Loss: 0.2130613476037979, Training Accuracy: 96.075
[ Tue Jul  9 09:09:35 2024 ] 	Batch(500/7879) done. Loss: 0.0628  lr:0.000001
[ Tue Jul  9 09:09:58 2024 ] 	Batch(600/7879) done. Loss: 0.2076  lr:0.000001
[ Tue Jul  9 09:10:21 2024 ] 	Batch(700/7879) done. Loss: 0.2486  lr:0.000001
[ Tue Jul  9 09:10:43 2024 ] 	Batch(800/7879) done. Loss: 0.2819  lr:0.000001
[ Tue Jul  9 09:11:06 2024 ] 	Batch(900/7879) done. Loss: 0.0099  lr:0.000001
[ Tue Jul  9 09:11:29 2024 ] 
Training: Epoch [91/120], Step [999], Loss: 0.4521830677986145, Training Accuracy: 96.2375
[ Tue Jul  9 09:11:29 2024 ] 	Batch(1000/7879) done. Loss: 0.0441  lr:0.000001
[ Tue Jul  9 09:11:52 2024 ] 	Batch(1100/7879) done. Loss: 0.2385  lr:0.000001
[ Tue Jul  9 09:12:14 2024 ] 	Batch(1200/7879) done. Loss: 0.2119  lr:0.000001
[ Tue Jul  9 09:12:37 2024 ] 	Batch(1300/7879) done. Loss: 0.1454  lr:0.000001
[ Tue Jul  9 09:13:00 2024 ] 	Batch(1400/7879) done. Loss: 0.0652  lr:0.000001
[ Tue Jul  9 09:13:22 2024 ] 
Training: Epoch [91/120], Step [1499], Loss: 0.07163655012845993, Training Accuracy: 96.34166666666667
[ Tue Jul  9 09:13:23 2024 ] 	Batch(1500/7879) done. Loss: 0.0796  lr:0.000001
[ Tue Jul  9 09:13:45 2024 ] 	Batch(1600/7879) done. Loss: 0.0250  lr:0.000001
[ Tue Jul  9 09:14:08 2024 ] 	Batch(1700/7879) done. Loss: 0.1987  lr:0.000001
[ Tue Jul  9 09:14:31 2024 ] 	Batch(1800/7879) done. Loss: 0.0190  lr:0.000001
[ Tue Jul  9 09:14:53 2024 ] 	Batch(1900/7879) done. Loss: 0.4069  lr:0.000001
[ Tue Jul  9 09:15:16 2024 ] 
Training: Epoch [91/120], Step [1999], Loss: 0.03670010343194008, Training Accuracy: 96.3125
[ Tue Jul  9 09:15:16 2024 ] 	Batch(2000/7879) done. Loss: 0.0705  lr:0.000001
[ Tue Jul  9 09:15:40 2024 ] 	Batch(2100/7879) done. Loss: 0.0070  lr:0.000001
[ Tue Jul  9 09:16:03 2024 ] 	Batch(2200/7879) done. Loss: 0.0236  lr:0.000001
[ Tue Jul  9 09:16:27 2024 ] 	Batch(2300/7879) done. Loss: 0.0381  lr:0.000001
[ Tue Jul  9 09:16:50 2024 ] 	Batch(2400/7879) done. Loss: 0.0097  lr:0.000001
[ Tue Jul  9 09:17:13 2024 ] 
Training: Epoch [91/120], Step [2499], Loss: 0.11536827683448792, Training Accuracy: 96.235
[ Tue Jul  9 09:17:13 2024 ] 	Batch(2500/7879) done. Loss: 0.1460  lr:0.000001
[ Tue Jul  9 09:17:36 2024 ] 	Batch(2600/7879) done. Loss: 0.0621  lr:0.000001
[ Tue Jul  9 09:17:58 2024 ] 	Batch(2700/7879) done. Loss: 0.2614  lr:0.000001
[ Tue Jul  9 09:18:21 2024 ] 	Batch(2800/7879) done. Loss: 0.1272  lr:0.000001
[ Tue Jul  9 09:18:45 2024 ] 	Batch(2900/7879) done. Loss: 0.0253  lr:0.000001
[ Tue Jul  9 09:19:08 2024 ] 
Training: Epoch [91/120], Step [2999], Loss: 0.6060529947280884, Training Accuracy: 96.22083333333333
[ Tue Jul  9 09:19:08 2024 ] 	Batch(3000/7879) done. Loss: 0.2002  lr:0.000001
[ Tue Jul  9 09:19:32 2024 ] 	Batch(3100/7879) done. Loss: 0.1190  lr:0.000001
[ Tue Jul  9 09:19:55 2024 ] 	Batch(3200/7879) done. Loss: 0.2338  lr:0.000001
[ Tue Jul  9 09:20:18 2024 ] 	Batch(3300/7879) done. Loss: 0.0532  lr:0.000001
[ Tue Jul  9 09:20:41 2024 ] 	Batch(3400/7879) done. Loss: 0.3360  lr:0.000001
[ Tue Jul  9 09:21:03 2024 ] 
Training: Epoch [91/120], Step [3499], Loss: 0.2962246239185333, Training Accuracy: 96.125
[ Tue Jul  9 09:21:03 2024 ] 	Batch(3500/7879) done. Loss: 0.2326  lr:0.000001
[ Tue Jul  9 09:21:26 2024 ] 	Batch(3600/7879) done. Loss: 0.2295  lr:0.000001
[ Tue Jul  9 09:21:49 2024 ] 	Batch(3700/7879) done. Loss: 0.1391  lr:0.000001
[ Tue Jul  9 09:22:13 2024 ] 	Batch(3800/7879) done. Loss: 0.0221  lr:0.000001
[ Tue Jul  9 09:22:36 2024 ] 	Batch(3900/7879) done. Loss: 0.0621  lr:0.000001
[ Tue Jul  9 09:23:00 2024 ] 
Training: Epoch [91/120], Step [3999], Loss: 0.4556170105934143, Training Accuracy: 96.14375
[ Tue Jul  9 09:23:00 2024 ] 	Batch(4000/7879) done. Loss: 0.0137  lr:0.000001
[ Tue Jul  9 09:23:23 2024 ] 	Batch(4100/7879) done. Loss: 0.0140  lr:0.000001
[ Tue Jul  9 09:23:47 2024 ] 	Batch(4200/7879) done. Loss: 0.0346  lr:0.000001
[ Tue Jul  9 09:24:10 2024 ] 	Batch(4300/7879) done. Loss: 0.0272  lr:0.000001
[ Tue Jul  9 09:24:34 2024 ] 	Batch(4400/7879) done. Loss: 0.0172  lr:0.000001
[ Tue Jul  9 09:24:57 2024 ] 
Training: Epoch [91/120], Step [4499], Loss: 0.018657488748431206, Training Accuracy: 96.16388888888889
[ Tue Jul  9 09:24:57 2024 ] 	Batch(4500/7879) done. Loss: 0.4212  lr:0.000001
[ Tue Jul  9 09:25:20 2024 ] 	Batch(4600/7879) done. Loss: 0.0598  lr:0.000001
[ Tue Jul  9 09:25:43 2024 ] 	Batch(4700/7879) done. Loss: 0.0293  lr:0.000001
[ Tue Jul  9 09:26:06 2024 ] 	Batch(4800/7879) done. Loss: 0.0354  lr:0.000001
[ Tue Jul  9 09:26:29 2024 ] 	Batch(4900/7879) done. Loss: 0.0074  lr:0.000001
[ Tue Jul  9 09:26:53 2024 ] 
Training: Epoch [91/120], Step [4999], Loss: 0.1737247109413147, Training Accuracy: 96.1575
[ Tue Jul  9 09:26:53 2024 ] 	Batch(5000/7879) done. Loss: 0.1122  lr:0.000001
[ Tue Jul  9 09:27:16 2024 ] 	Batch(5100/7879) done. Loss: 0.0211  lr:0.000001
[ Tue Jul  9 09:27:40 2024 ] 	Batch(5200/7879) done. Loss: 0.0235  lr:0.000001
[ Tue Jul  9 09:28:03 2024 ] 	Batch(5300/7879) done. Loss: 0.2341  lr:0.000001
[ Tue Jul  9 09:28:26 2024 ] 	Batch(5400/7879) done. Loss: 0.0466  lr:0.000001
[ Tue Jul  9 09:28:49 2024 ] 
Training: Epoch [91/120], Step [5499], Loss: 0.04246305301785469, Training Accuracy: 96.14090909090909
[ Tue Jul  9 09:28:49 2024 ] 	Batch(5500/7879) done. Loss: 0.1284  lr:0.000001
[ Tue Jul  9 09:29:12 2024 ] 	Batch(5600/7879) done. Loss: 0.0494  lr:0.000001
[ Tue Jul  9 09:29:34 2024 ] 	Batch(5700/7879) done. Loss: 0.0855  lr:0.000001
[ Tue Jul  9 09:29:57 2024 ] 	Batch(5800/7879) done. Loss: 0.0727  lr:0.000001
[ Tue Jul  9 09:30:19 2024 ] 	Batch(5900/7879) done. Loss: 0.1312  lr:0.000001
[ Tue Jul  9 09:30:42 2024 ] 
Training: Epoch [91/120], Step [5999], Loss: 0.1345817595720291, Training Accuracy: 96.15625
[ Tue Jul  9 09:30:42 2024 ] 	Batch(6000/7879) done. Loss: 0.0789  lr:0.000001
[ Tue Jul  9 09:31:05 2024 ] 	Batch(6100/7879) done. Loss: 0.1772  lr:0.000001
[ Tue Jul  9 09:31:27 2024 ] 	Batch(6200/7879) done. Loss: 0.0099  lr:0.000001
[ Tue Jul  9 09:31:50 2024 ] 	Batch(6300/7879) done. Loss: 0.5128  lr:0.000001
[ Tue Jul  9 09:32:13 2024 ] 	Batch(6400/7879) done. Loss: 0.0752  lr:0.000001
[ Tue Jul  9 09:32:35 2024 ] 
Training: Epoch [91/120], Step [6499], Loss: 0.06766867637634277, Training Accuracy: 96.15769230769232
[ Tue Jul  9 09:32:35 2024 ] 	Batch(6500/7879) done. Loss: 0.5291  lr:0.000001
[ Tue Jul  9 09:32:58 2024 ] 	Batch(6600/7879) done. Loss: 0.3168  lr:0.000001
[ Tue Jul  9 09:33:20 2024 ] 	Batch(6700/7879) done. Loss: 0.2690  lr:0.000001
[ Tue Jul  9 09:33:43 2024 ] 	Batch(6800/7879) done. Loss: 0.1141  lr:0.000001
[ Tue Jul  9 09:34:06 2024 ] 	Batch(6900/7879) done. Loss: 0.0492  lr:0.000001
[ Tue Jul  9 09:34:28 2024 ] 
Training: Epoch [91/120], Step [6999], Loss: 0.06784756481647491, Training Accuracy: 96.15357142857142
[ Tue Jul  9 09:34:28 2024 ] 	Batch(7000/7879) done. Loss: 0.1009  lr:0.000001
[ Tue Jul  9 09:34:51 2024 ] 	Batch(7100/7879) done. Loss: 0.0492  lr:0.000001
[ Tue Jul  9 09:35:14 2024 ] 	Batch(7200/7879) done. Loss: 0.0346  lr:0.000001
[ Tue Jul  9 09:35:36 2024 ] 	Batch(7300/7879) done. Loss: 0.2051  lr:0.000001
[ Tue Jul  9 09:35:59 2024 ] 	Batch(7400/7879) done. Loss: 0.7217  lr:0.000001
[ Tue Jul  9 09:36:21 2024 ] 
Training: Epoch [91/120], Step [7499], Loss: 0.009769302792847157, Training Accuracy: 96.155
[ Tue Jul  9 09:36:21 2024 ] 	Batch(7500/7879) done. Loss: 0.1490  lr:0.000001
[ Tue Jul  9 09:36:44 2024 ] 	Batch(7600/7879) done. Loss: 0.2826  lr:0.000001
[ Tue Jul  9 09:37:07 2024 ] 	Batch(7700/7879) done. Loss: 0.2823  lr:0.000001
[ Tue Jul  9 09:37:29 2024 ] 	Batch(7800/7879) done. Loss: 0.0560  lr:0.000001
[ Tue Jul  9 09:37:47 2024 ] 	Mean training loss: 0.1479.
[ Tue Jul  9 09:37:47 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 09:37:47 2024 ] Training epoch: 93
[ Tue Jul  9 09:37:48 2024 ] 	Batch(0/7879) done. Loss: 0.1244  lr:0.000001
[ Tue Jul  9 09:38:11 2024 ] 	Batch(100/7879) done. Loss: 0.0500  lr:0.000001
[ Tue Jul  9 09:38:34 2024 ] 	Batch(200/7879) done. Loss: 0.7520  lr:0.000001
[ Tue Jul  9 09:38:58 2024 ] 	Batch(300/7879) done. Loss: 0.3547  lr:0.000001
[ Tue Jul  9 09:39:21 2024 ] 	Batch(400/7879) done. Loss: 0.1028  lr:0.000001
[ Tue Jul  9 09:39:45 2024 ] 
Training: Epoch [92/120], Step [499], Loss: 0.25493815541267395, Training Accuracy: 96.275
[ Tue Jul  9 09:39:45 2024 ] 	Batch(500/7879) done. Loss: 0.0174  lr:0.000001
[ Tue Jul  9 09:40:08 2024 ] 	Batch(600/7879) done. Loss: 0.1836  lr:0.000001
[ Tue Jul  9 09:40:31 2024 ] 	Batch(700/7879) done. Loss: 0.0291  lr:0.000001
[ Tue Jul  9 09:40:54 2024 ] 	Batch(800/7879) done. Loss: 0.4648  lr:0.000001
[ Tue Jul  9 09:41:16 2024 ] 	Batch(900/7879) done. Loss: 0.1161  lr:0.000001
[ Tue Jul  9 09:41:39 2024 ] 
Training: Epoch [92/120], Step [999], Loss: 0.1295357197523117, Training Accuracy: 96.1125
[ Tue Jul  9 09:41:39 2024 ] 	Batch(1000/7879) done. Loss: 0.0016  lr:0.000001
[ Tue Jul  9 09:42:02 2024 ] 	Batch(1100/7879) done. Loss: 0.2369  lr:0.000001
[ Tue Jul  9 09:42:25 2024 ] 	Batch(1200/7879) done. Loss: 0.0089  lr:0.000001
[ Tue Jul  9 09:42:47 2024 ] 	Batch(1300/7879) done. Loss: 0.1766  lr:0.000001
[ Tue Jul  9 09:43:10 2024 ] 	Batch(1400/7879) done. Loss: 0.0516  lr:0.000001
[ Tue Jul  9 09:43:32 2024 ] 
Training: Epoch [92/120], Step [1499], Loss: 0.05181266739964485, Training Accuracy: 95.875
[ Tue Jul  9 09:43:33 2024 ] 	Batch(1500/7879) done. Loss: 0.8877  lr:0.000001
[ Tue Jul  9 09:43:56 2024 ] 	Batch(1600/7879) done. Loss: 0.1659  lr:0.000001
[ Tue Jul  9 09:44:18 2024 ] 	Batch(1700/7879) done. Loss: 0.0528  lr:0.000001
[ Tue Jul  9 09:44:41 2024 ] 	Batch(1800/7879) done. Loss: 0.0094  lr:0.000001
[ Tue Jul  9 09:45:04 2024 ] 	Batch(1900/7879) done. Loss: 0.1997  lr:0.000001
[ Tue Jul  9 09:45:26 2024 ] 
Training: Epoch [92/120], Step [1999], Loss: 0.3382895290851593, Training Accuracy: 96.025
[ Tue Jul  9 09:45:26 2024 ] 	Batch(2000/7879) done. Loss: 0.0629  lr:0.000001
[ Tue Jul  9 09:45:49 2024 ] 	Batch(2100/7879) done. Loss: 0.3637  lr:0.000001
[ Tue Jul  9 09:46:12 2024 ] 	Batch(2200/7879) done. Loss: 0.0764  lr:0.000001
[ Tue Jul  9 09:46:35 2024 ] 	Batch(2300/7879) done. Loss: 0.0286  lr:0.000001
[ Tue Jul  9 09:46:57 2024 ] 	Batch(2400/7879) done. Loss: 0.5679  lr:0.000001
[ Tue Jul  9 09:47:20 2024 ] 
Training: Epoch [92/120], Step [2499], Loss: 0.028029555454850197, Training Accuracy: 96.035
[ Tue Jul  9 09:47:20 2024 ] 	Batch(2500/7879) done. Loss: 0.1855  lr:0.000001
[ Tue Jul  9 09:47:42 2024 ] 	Batch(2600/7879) done. Loss: 0.3268  lr:0.000001
[ Tue Jul  9 09:48:05 2024 ] 	Batch(2700/7879) done. Loss: 0.1428  lr:0.000001
[ Tue Jul  9 09:48:28 2024 ] 	Batch(2800/7879) done. Loss: 0.0358  lr:0.000001
[ Tue Jul  9 09:48:51 2024 ] 	Batch(2900/7879) done. Loss: 0.4330  lr:0.000001
[ Tue Jul  9 09:49:13 2024 ] 
Training: Epoch [92/120], Step [2999], Loss: 0.002100932179018855, Training Accuracy: 96.00416666666666
[ Tue Jul  9 09:49:13 2024 ] 	Batch(3000/7879) done. Loss: 0.0197  lr:0.000001
[ Tue Jul  9 09:49:36 2024 ] 	Batch(3100/7879) done. Loss: 0.2311  lr:0.000001
[ Tue Jul  9 09:49:58 2024 ] 	Batch(3200/7879) done. Loss: 0.2127  lr:0.000001
[ Tue Jul  9 09:50:21 2024 ] 	Batch(3300/7879) done. Loss: 0.0378  lr:0.000001
[ Tue Jul  9 09:50:44 2024 ] 	Batch(3400/7879) done. Loss: 0.0301  lr:0.000001
[ Tue Jul  9 09:51:06 2024 ] 
Training: Epoch [92/120], Step [3499], Loss: 0.12114641815423965, Training Accuracy: 96.03571428571429
[ Tue Jul  9 09:51:06 2024 ] 	Batch(3500/7879) done. Loss: 0.0167  lr:0.000001
[ Tue Jul  9 09:51:29 2024 ] 	Batch(3600/7879) done. Loss: 0.5080  lr:0.000001
[ Tue Jul  9 09:51:51 2024 ] 	Batch(3700/7879) done. Loss: 0.6760  lr:0.000001
[ Tue Jul  9 09:52:14 2024 ] 	Batch(3800/7879) done. Loss: 0.0185  lr:0.000001
[ Tue Jul  9 09:52:36 2024 ] 	Batch(3900/7879) done. Loss: 0.1981  lr:0.000001
[ Tue Jul  9 09:52:59 2024 ] 
Training: Epoch [92/120], Step [3999], Loss: 0.007038883399218321, Training Accuracy: 96.075
[ Tue Jul  9 09:52:59 2024 ] 	Batch(4000/7879) done. Loss: 0.1437  lr:0.000001
[ Tue Jul  9 09:53:22 2024 ] 	Batch(4100/7879) done. Loss: 0.0560  lr:0.000001
[ Tue Jul  9 09:53:44 2024 ] 	Batch(4200/7879) done. Loss: 0.0468  lr:0.000001
[ Tue Jul  9 09:54:07 2024 ] 	Batch(4300/7879) done. Loss: 0.1136  lr:0.000001
[ Tue Jul  9 09:54:29 2024 ] 	Batch(4400/7879) done. Loss: 0.2125  lr:0.000001
[ Tue Jul  9 09:54:52 2024 ] 
Training: Epoch [92/120], Step [4499], Loss: 0.04985653609037399, Training Accuracy: 96.03055555555555
[ Tue Jul  9 09:54:52 2024 ] 	Batch(4500/7879) done. Loss: 0.0216  lr:0.000001
[ Tue Jul  9 09:55:15 2024 ] 	Batch(4600/7879) done. Loss: 0.1576  lr:0.000001
[ Tue Jul  9 09:55:37 2024 ] 	Batch(4700/7879) done. Loss: 0.0265  lr:0.000001
[ Tue Jul  9 09:56:00 2024 ] 	Batch(4800/7879) done. Loss: 0.0143  lr:0.000001
[ Tue Jul  9 09:56:23 2024 ] 	Batch(4900/7879) done. Loss: 0.1980  lr:0.000001
[ Tue Jul  9 09:56:45 2024 ] 
Training: Epoch [92/120], Step [4999], Loss: 0.5397245287895203, Training Accuracy: 96.1325
[ Tue Jul  9 09:56:45 2024 ] 	Batch(5000/7879) done. Loss: 0.5214  lr:0.000001
[ Tue Jul  9 09:57:08 2024 ] 	Batch(5100/7879) done. Loss: 0.0886  lr:0.000001
[ Tue Jul  9 09:57:31 2024 ] 	Batch(5200/7879) done. Loss: 0.0484  lr:0.000001
[ Tue Jul  9 09:57:53 2024 ] 	Batch(5300/7879) done. Loss: 0.0764  lr:0.000001
[ Tue Jul  9 09:58:17 2024 ] 	Batch(5400/7879) done. Loss: 0.2153  lr:0.000001
[ Tue Jul  9 09:58:39 2024 ] 
Training: Epoch [92/120], Step [5499], Loss: 0.02316148765385151, Training Accuracy: 96.13181818181819
[ Tue Jul  9 09:58:39 2024 ] 	Batch(5500/7879) done. Loss: 0.0469  lr:0.000001
[ Tue Jul  9 09:59:02 2024 ] 	Batch(5600/7879) done. Loss: 0.0920  lr:0.000001
[ Tue Jul  9 09:59:25 2024 ] 	Batch(5700/7879) done. Loss: 0.2152  lr:0.000001
[ Tue Jul  9 09:59:47 2024 ] 	Batch(5800/7879) done. Loss: 0.1975  lr:0.000001
[ Tue Jul  9 10:00:10 2024 ] 	Batch(5900/7879) done. Loss: 0.6401  lr:0.000001
[ Tue Jul  9 10:00:32 2024 ] 
Training: Epoch [92/120], Step [5999], Loss: 0.018086228519678116, Training Accuracy: 96.08541666666667
[ Tue Jul  9 10:00:32 2024 ] 	Batch(6000/7879) done. Loss: 0.0156  lr:0.000001
[ Tue Jul  9 10:00:55 2024 ] 	Batch(6100/7879) done. Loss: 0.0299  lr:0.000001
[ Tue Jul  9 10:01:17 2024 ] 	Batch(6200/7879) done. Loss: 0.2398  lr:0.000001
[ Tue Jul  9 10:01:40 2024 ] 	Batch(6300/7879) done. Loss: 0.4744  lr:0.000001
[ Tue Jul  9 10:02:03 2024 ] 	Batch(6400/7879) done. Loss: 0.0166  lr:0.000001
[ Tue Jul  9 10:02:26 2024 ] 
Training: Epoch [92/120], Step [6499], Loss: 0.15498432517051697, Training Accuracy: 96.09423076923078
[ Tue Jul  9 10:02:26 2024 ] 	Batch(6500/7879) done. Loss: 0.0909  lr:0.000001
[ Tue Jul  9 10:02:49 2024 ] 	Batch(6600/7879) done. Loss: 0.7295  lr:0.000001
[ Tue Jul  9 10:03:13 2024 ] 	Batch(6700/7879) done. Loss: 0.1160  lr:0.000001
[ Tue Jul  9 10:03:36 2024 ] 	Batch(6800/7879) done. Loss: 0.0200  lr:0.000001
[ Tue Jul  9 10:03:59 2024 ] 	Batch(6900/7879) done. Loss: 0.2033  lr:0.000001
[ Tue Jul  9 10:04:21 2024 ] 
Training: Epoch [92/120], Step [6999], Loss: 0.10862483084201813, Training Accuracy: 96.11428571428571
[ Tue Jul  9 10:04:21 2024 ] 	Batch(7000/7879) done. Loss: 0.5541  lr:0.000001
[ Tue Jul  9 10:04:44 2024 ] 	Batch(7100/7879) done. Loss: 0.0556  lr:0.000001
[ Tue Jul  9 10:05:07 2024 ] 	Batch(7200/7879) done. Loss: 0.0740  lr:0.000001
[ Tue Jul  9 10:05:29 2024 ] 	Batch(7300/7879) done. Loss: 0.0660  lr:0.000001
[ Tue Jul  9 10:05:52 2024 ] 	Batch(7400/7879) done. Loss: 0.3667  lr:0.000001
[ Tue Jul  9 10:06:14 2024 ] 
Training: Epoch [92/120], Step [7499], Loss: 0.166127547621727, Training Accuracy: 96.09666666666666
[ Tue Jul  9 10:06:14 2024 ] 	Batch(7500/7879) done. Loss: 0.2896  lr:0.000001
[ Tue Jul  9 10:06:37 2024 ] 	Batch(7600/7879) done. Loss: 0.0516  lr:0.000001
[ Tue Jul  9 10:07:00 2024 ] 	Batch(7700/7879) done. Loss: 0.0383  lr:0.000001
[ Tue Jul  9 10:07:22 2024 ] 	Batch(7800/7879) done. Loss: 0.0478  lr:0.000001
[ Tue Jul  9 10:07:40 2024 ] 	Mean training loss: 0.1458.
[ Tue Jul  9 10:07:40 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 10:07:40 2024 ] Training epoch: 94
[ Tue Jul  9 10:07:40 2024 ] 	Batch(0/7879) done. Loss: 0.0077  lr:0.000001
[ Tue Jul  9 10:08:03 2024 ] 	Batch(100/7879) done. Loss: 0.0062  lr:0.000001
[ Tue Jul  9 10:08:26 2024 ] 	Batch(200/7879) done. Loss: 0.0082  lr:0.000001
[ Tue Jul  9 10:08:48 2024 ] 	Batch(300/7879) done. Loss: 0.2216  lr:0.000001
[ Tue Jul  9 10:09:11 2024 ] 	Batch(400/7879) done. Loss: 0.1204  lr:0.000001
[ Tue Jul  9 10:09:33 2024 ] 
Training: Epoch [93/120], Step [499], Loss: 0.13847194612026215, Training Accuracy: 96.22500000000001
[ Tue Jul  9 10:09:33 2024 ] 	Batch(500/7879) done. Loss: 0.0048  lr:0.000001
[ Tue Jul  9 10:09:56 2024 ] 	Batch(600/7879) done. Loss: 0.0274  lr:0.000001
[ Tue Jul  9 10:10:18 2024 ] 	Batch(700/7879) done. Loss: 0.0255  lr:0.000001
[ Tue Jul  9 10:10:41 2024 ] 	Batch(800/7879) done. Loss: 0.1783  lr:0.000001
[ Tue Jul  9 10:11:04 2024 ] 	Batch(900/7879) done. Loss: 0.1433  lr:0.000001
[ Tue Jul  9 10:11:26 2024 ] 
Training: Epoch [93/120], Step [999], Loss: 0.1193380057811737, Training Accuracy: 96.26249999999999
[ Tue Jul  9 10:11:26 2024 ] 	Batch(1000/7879) done. Loss: 0.0503  lr:0.000001
[ Tue Jul  9 10:11:49 2024 ] 	Batch(1100/7879) done. Loss: 0.2982  lr:0.000001
[ Tue Jul  9 10:12:12 2024 ] 	Batch(1200/7879) done. Loss: 0.4928  lr:0.000001
[ Tue Jul  9 10:12:34 2024 ] 	Batch(1300/7879) done. Loss: 0.3194  lr:0.000001
[ Tue Jul  9 10:12:57 2024 ] 	Batch(1400/7879) done. Loss: 0.1492  lr:0.000001
[ Tue Jul  9 10:13:19 2024 ] 
Training: Epoch [93/120], Step [1499], Loss: 0.44763973355293274, Training Accuracy: 96.05833333333334
[ Tue Jul  9 10:13:19 2024 ] 	Batch(1500/7879) done. Loss: 0.3366  lr:0.000001
[ Tue Jul  9 10:13:42 2024 ] 	Batch(1600/7879) done. Loss: 0.2490  lr:0.000001
[ Tue Jul  9 10:14:04 2024 ] 	Batch(1700/7879) done. Loss: 0.2274  lr:0.000001
[ Tue Jul  9 10:14:27 2024 ] 	Batch(1800/7879) done. Loss: 0.1972  lr:0.000001
[ Tue Jul  9 10:14:49 2024 ] 	Batch(1900/7879) done. Loss: 0.0605  lr:0.000001
[ Tue Jul  9 10:15:12 2024 ] 
Training: Epoch [93/120], Step [1999], Loss: 0.13177889585494995, Training Accuracy: 96.04375
[ Tue Jul  9 10:15:12 2024 ] 	Batch(2000/7879) done. Loss: 0.0233  lr:0.000001
[ Tue Jul  9 10:15:35 2024 ] 	Batch(2100/7879) done. Loss: 0.0632  lr:0.000001
[ Tue Jul  9 10:15:57 2024 ] 	Batch(2200/7879) done. Loss: 0.0671  lr:0.000001
[ Tue Jul  9 10:16:20 2024 ] 	Batch(2300/7879) done. Loss: 0.0602  lr:0.000001
[ Tue Jul  9 10:16:43 2024 ] 	Batch(2400/7879) done. Loss: 0.0179  lr:0.000001
[ Tue Jul  9 10:17:05 2024 ] 
Training: Epoch [93/120], Step [2499], Loss: 0.7924588918685913, Training Accuracy: 96.03
[ Tue Jul  9 10:17:05 2024 ] 	Batch(2500/7879) done. Loss: 0.3155  lr:0.000001
[ Tue Jul  9 10:17:28 2024 ] 	Batch(2600/7879) done. Loss: 0.4089  lr:0.000001
[ Tue Jul  9 10:17:50 2024 ] 	Batch(2700/7879) done. Loss: 0.2006  lr:0.000001
[ Tue Jul  9 10:18:13 2024 ] 	Batch(2800/7879) done. Loss: 0.0126  lr:0.000001
[ Tue Jul  9 10:18:35 2024 ] 	Batch(2900/7879) done. Loss: 0.1769  lr:0.000001
[ Tue Jul  9 10:18:58 2024 ] 
Training: Epoch [93/120], Step [2999], Loss: 0.1351771205663681, Training Accuracy: 96.04583333333333
[ Tue Jul  9 10:18:58 2024 ] 	Batch(3000/7879) done. Loss: 0.0147  lr:0.000001
[ Tue Jul  9 10:19:20 2024 ] 	Batch(3100/7879) done. Loss: 0.0342  lr:0.000001
[ Tue Jul  9 10:19:43 2024 ] 	Batch(3200/7879) done. Loss: 0.0525  lr:0.000001
[ Tue Jul  9 10:20:06 2024 ] 	Batch(3300/7879) done. Loss: 0.0124  lr:0.000001
[ Tue Jul  9 10:20:28 2024 ] 	Batch(3400/7879) done. Loss: 0.3329  lr:0.000001
[ Tue Jul  9 10:20:50 2024 ] 
Training: Epoch [93/120], Step [3499], Loss: 0.010333985090255737, Training Accuracy: 96.01428571428572
[ Tue Jul  9 10:20:51 2024 ] 	Batch(3500/7879) done. Loss: 0.4831  lr:0.000001
[ Tue Jul  9 10:21:13 2024 ] 	Batch(3600/7879) done. Loss: 0.0398  lr:0.000001
[ Tue Jul  9 10:21:37 2024 ] 	Batch(3700/7879) done. Loss: 0.0513  lr:0.000001
[ Tue Jul  9 10:22:00 2024 ] 	Batch(3800/7879) done. Loss: 0.0401  lr:0.000001
[ Tue Jul  9 10:22:22 2024 ] 	Batch(3900/7879) done. Loss: 0.1535  lr:0.000001
[ Tue Jul  9 10:22:45 2024 ] 
Training: Epoch [93/120], Step [3999], Loss: 0.04689694941043854, Training Accuracy: 95.975
[ Tue Jul  9 10:22:45 2024 ] 	Batch(4000/7879) done. Loss: 0.0178  lr:0.000001
[ Tue Jul  9 10:23:08 2024 ] 	Batch(4100/7879) done. Loss: 0.0748  lr:0.000001
[ Tue Jul  9 10:23:30 2024 ] 	Batch(4200/7879) done. Loss: 0.0670  lr:0.000001
[ Tue Jul  9 10:23:53 2024 ] 	Batch(4300/7879) done. Loss: 0.1063  lr:0.000001
[ Tue Jul  9 10:24:15 2024 ] 	Batch(4400/7879) done. Loss: 0.5625  lr:0.000001
[ Tue Jul  9 10:24:38 2024 ] 
Training: Epoch [93/120], Step [4499], Loss: 0.00388599606230855, Training Accuracy: 95.99722222222222
[ Tue Jul  9 10:24:39 2024 ] 	Batch(4500/7879) done. Loss: 0.2266  lr:0.000001
[ Tue Jul  9 10:25:02 2024 ] 	Batch(4600/7879) done. Loss: 0.0538  lr:0.000001
[ Tue Jul  9 10:25:25 2024 ] 	Batch(4700/7879) done. Loss: 0.0893  lr:0.000001
[ Tue Jul  9 10:25:49 2024 ] 	Batch(4800/7879) done. Loss: 0.0956  lr:0.000001
[ Tue Jul  9 10:26:11 2024 ] 	Batch(4900/7879) done. Loss: 0.4415  lr:0.000001
[ Tue Jul  9 10:26:33 2024 ] 
Training: Epoch [93/120], Step [4999], Loss: 0.0031484528444707394, Training Accuracy: 96.0025
[ Tue Jul  9 10:26:34 2024 ] 	Batch(5000/7879) done. Loss: 0.0038  lr:0.000001
[ Tue Jul  9 10:26:56 2024 ] 	Batch(5100/7879) done. Loss: 0.3195  lr:0.000001
[ Tue Jul  9 10:27:19 2024 ] 	Batch(5200/7879) done. Loss: 0.0817  lr:0.000001
[ Tue Jul  9 10:27:42 2024 ] 	Batch(5300/7879) done. Loss: 0.1211  lr:0.000001
[ Tue Jul  9 10:28:04 2024 ] 	Batch(5400/7879) done. Loss: 0.1463  lr:0.000001
[ Tue Jul  9 10:28:27 2024 ] 
Training: Epoch [93/120], Step [5499], Loss: 0.023148663341999054, Training Accuracy: 96.025
[ Tue Jul  9 10:28:27 2024 ] 	Batch(5500/7879) done. Loss: 0.4969  lr:0.000001
[ Tue Jul  9 10:28:50 2024 ] 	Batch(5600/7879) done. Loss: 0.3535  lr:0.000001
[ Tue Jul  9 10:29:12 2024 ] 	Batch(5700/7879) done. Loss: 0.0934  lr:0.000001
[ Tue Jul  9 10:29:35 2024 ] 	Batch(5800/7879) done. Loss: 0.3767  lr:0.000001
[ Tue Jul  9 10:29:57 2024 ] 	Batch(5900/7879) done. Loss: 0.0044  lr:0.000001
[ Tue Jul  9 10:30:20 2024 ] 
Training: Epoch [93/120], Step [5999], Loss: 0.017897894605994225, Training Accuracy: 96.05208333333334
[ Tue Jul  9 10:30:20 2024 ] 	Batch(6000/7879) done. Loss: 0.0083  lr:0.000001
[ Tue Jul  9 10:30:43 2024 ] 	Batch(6100/7879) done. Loss: 0.0085  lr:0.000001
[ Tue Jul  9 10:31:05 2024 ] 	Batch(6200/7879) done. Loss: 0.0096  lr:0.000001
[ Tue Jul  9 10:31:28 2024 ] 	Batch(6300/7879) done. Loss: 0.1539  lr:0.000001
[ Tue Jul  9 10:31:51 2024 ] 	Batch(6400/7879) done. Loss: 0.0752  lr:0.000001
[ Tue Jul  9 10:32:13 2024 ] 
Training: Epoch [93/120], Step [6499], Loss: 0.2534341514110565, Training Accuracy: 95.99038461538461
[ Tue Jul  9 10:32:13 2024 ] 	Batch(6500/7879) done. Loss: 0.3003  lr:0.000001
[ Tue Jul  9 10:32:36 2024 ] 	Batch(6600/7879) done. Loss: 0.1590  lr:0.000001
[ Tue Jul  9 10:32:58 2024 ] 	Batch(6700/7879) done. Loss: 0.1643  lr:0.000001
[ Tue Jul  9 10:33:21 2024 ] 	Batch(6800/7879) done. Loss: 0.1149  lr:0.000001
[ Tue Jul  9 10:33:44 2024 ] 	Batch(6900/7879) done. Loss: 0.1431  lr:0.000001
[ Tue Jul  9 10:34:06 2024 ] 
Training: Epoch [93/120], Step [6999], Loss: 0.0410391204059124, Training Accuracy: 96.00714285714285
[ Tue Jul  9 10:34:06 2024 ] 	Batch(7000/7879) done. Loss: 0.1896  lr:0.000001
[ Tue Jul  9 10:34:29 2024 ] 	Batch(7100/7879) done. Loss: 0.2357  lr:0.000001
[ Tue Jul  9 10:34:51 2024 ] 	Batch(7200/7879) done. Loss: 0.0922  lr:0.000001
[ Tue Jul  9 10:35:14 2024 ] 	Batch(7300/7879) done. Loss: 0.2387  lr:0.000001
[ Tue Jul  9 10:35:37 2024 ] 	Batch(7400/7879) done. Loss: 0.0594  lr:0.000001
[ Tue Jul  9 10:35:59 2024 ] 
Training: Epoch [93/120], Step [7499], Loss: 0.3204619884490967, Training Accuracy: 96.01166666666666
[ Tue Jul  9 10:35:59 2024 ] 	Batch(7500/7879) done. Loss: 0.0464  lr:0.000001
[ Tue Jul  9 10:36:22 2024 ] 	Batch(7600/7879) done. Loss: 0.1432  lr:0.000001
[ Tue Jul  9 10:36:44 2024 ] 	Batch(7700/7879) done. Loss: 0.2410  lr:0.000001
[ Tue Jul  9 10:37:07 2024 ] 	Batch(7800/7879) done. Loss: 0.3216  lr:0.000001
[ Tue Jul  9 10:37:24 2024 ] 	Mean training loss: 0.1496.
[ Tue Jul  9 10:37:24 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 10:37:25 2024 ] Training epoch: 95
[ Tue Jul  9 10:37:25 2024 ] 	Batch(0/7879) done. Loss: 0.0066  lr:0.000001
[ Tue Jul  9 10:37:48 2024 ] 	Batch(100/7879) done. Loss: 0.2962  lr:0.000001
[ Tue Jul  9 10:38:11 2024 ] 	Batch(200/7879) done. Loss: 0.5396  lr:0.000001
[ Tue Jul  9 10:38:34 2024 ] 	Batch(300/7879) done. Loss: 0.0344  lr:0.000001
[ Tue Jul  9 10:38:58 2024 ] 	Batch(400/7879) done. Loss: 0.5778  lr:0.000001
[ Tue Jul  9 10:39:20 2024 ] 
Training: Epoch [94/120], Step [499], Loss: 0.04366682469844818, Training Accuracy: 95.675
[ Tue Jul  9 10:39:21 2024 ] 	Batch(500/7879) done. Loss: 0.0287  lr:0.000001
[ Tue Jul  9 10:39:43 2024 ] 	Batch(600/7879) done. Loss: 0.0076  lr:0.000001
[ Tue Jul  9 10:40:06 2024 ] 	Batch(700/7879) done. Loss: 0.3945  lr:0.000001
[ Tue Jul  9 10:40:29 2024 ] 	Batch(800/7879) done. Loss: 0.3409  lr:0.000001
[ Tue Jul  9 10:40:51 2024 ] 	Batch(900/7879) done. Loss: 0.0364  lr:0.000001
[ Tue Jul  9 10:41:13 2024 ] 
Training: Epoch [94/120], Step [999], Loss: 0.0397791713476181, Training Accuracy: 96.075
[ Tue Jul  9 10:41:14 2024 ] 	Batch(1000/7879) done. Loss: 0.0075  lr:0.000001
[ Tue Jul  9 10:41:36 2024 ] 	Batch(1100/7879) done. Loss: 0.0266  lr:0.000001
[ Tue Jul  9 10:41:59 2024 ] 	Batch(1200/7879) done. Loss: 0.1033  lr:0.000001
[ Tue Jul  9 10:42:21 2024 ] 	Batch(1300/7879) done. Loss: 0.0166  lr:0.000001
[ Tue Jul  9 10:42:44 2024 ] 	Batch(1400/7879) done. Loss: 0.0114  lr:0.000001
[ Tue Jul  9 10:43:06 2024 ] 
Training: Epoch [94/120], Step [1499], Loss: 0.30641189217567444, Training Accuracy: 96.04166666666667
[ Tue Jul  9 10:43:06 2024 ] 	Batch(1500/7879) done. Loss: 0.0086  lr:0.000001
[ Tue Jul  9 10:43:29 2024 ] 	Batch(1600/7879) done. Loss: 0.2311  lr:0.000001
[ Tue Jul  9 10:43:52 2024 ] 	Batch(1700/7879) done. Loss: 0.0682  lr:0.000001
[ Tue Jul  9 10:44:14 2024 ] 	Batch(1800/7879) done. Loss: 0.0253  lr:0.000001
[ Tue Jul  9 10:44:37 2024 ] 	Batch(1900/7879) done. Loss: 0.1624  lr:0.000001
[ Tue Jul  9 10:44:59 2024 ] 
Training: Epoch [94/120], Step [1999], Loss: 0.08682036399841309, Training Accuracy: 96.0625
[ Tue Jul  9 10:45:00 2024 ] 	Batch(2000/7879) done. Loss: 0.1181  lr:0.000001
[ Tue Jul  9 10:45:22 2024 ] 	Batch(2100/7879) done. Loss: 0.2218  lr:0.000001
[ Tue Jul  9 10:45:45 2024 ] 	Batch(2200/7879) done. Loss: 0.0610  lr:0.000001
[ Tue Jul  9 10:46:07 2024 ] 	Batch(2300/7879) done. Loss: 0.0379  lr:0.000001
[ Tue Jul  9 10:46:30 2024 ] 	Batch(2400/7879) done. Loss: 0.2679  lr:0.000001
[ Tue Jul  9 10:46:52 2024 ] 
Training: Epoch [94/120], Step [2499], Loss: 0.6008403301239014, Training Accuracy: 96.095
[ Tue Jul  9 10:46:53 2024 ] 	Batch(2500/7879) done. Loss: 0.0750  lr:0.000001
[ Tue Jul  9 10:47:15 2024 ] 	Batch(2600/7879) done. Loss: 0.0125  lr:0.000001
[ Tue Jul  9 10:47:38 2024 ] 	Batch(2700/7879) done. Loss: 0.0902  lr:0.000001
[ Tue Jul  9 10:48:01 2024 ] 	Batch(2800/7879) done. Loss: 0.1237  lr:0.000001
[ Tue Jul  9 10:48:23 2024 ] 	Batch(2900/7879) done. Loss: 0.0207  lr:0.000001
[ Tue Jul  9 10:48:45 2024 ] 
Training: Epoch [94/120], Step [2999], Loss: 0.021194621920585632, Training Accuracy: 96.04166666666667
[ Tue Jul  9 10:48:46 2024 ] 	Batch(3000/7879) done. Loss: 0.0560  lr:0.000001
[ Tue Jul  9 10:49:08 2024 ] 	Batch(3100/7879) done. Loss: 0.0704  lr:0.000001
[ Tue Jul  9 10:49:31 2024 ] 	Batch(3200/7879) done. Loss: 0.3001  lr:0.000001
[ Tue Jul  9 10:49:54 2024 ] 	Batch(3300/7879) done. Loss: 0.0445  lr:0.000001
[ Tue Jul  9 10:50:17 2024 ] 	Batch(3400/7879) done. Loss: 0.3387  lr:0.000001
[ Tue Jul  9 10:50:41 2024 ] 
Training: Epoch [94/120], Step [3499], Loss: 0.07703933119773865, Training Accuracy: 96.05
[ Tue Jul  9 10:50:41 2024 ] 	Batch(3500/7879) done. Loss: 0.0765  lr:0.000001
[ Tue Jul  9 10:51:04 2024 ] 	Batch(3600/7879) done. Loss: 0.0435  lr:0.000001
[ Tue Jul  9 10:51:27 2024 ] 	Batch(3700/7879) done. Loss: 0.0223  lr:0.000001
[ Tue Jul  9 10:51:50 2024 ] 	Batch(3800/7879) done. Loss: 0.0256  lr:0.000001
[ Tue Jul  9 10:52:13 2024 ] 	Batch(3900/7879) done. Loss: 0.0206  lr:0.000001
[ Tue Jul  9 10:52:36 2024 ] 
Training: Epoch [94/120], Step [3999], Loss: 0.013290277682244778, Training Accuracy: 96.05625
[ Tue Jul  9 10:52:36 2024 ] 	Batch(4000/7879) done. Loss: 0.0997  lr:0.000001
[ Tue Jul  9 10:53:00 2024 ] 	Batch(4100/7879) done. Loss: 0.1996  lr:0.000001
[ Tue Jul  9 10:53:23 2024 ] 	Batch(4200/7879) done. Loss: 0.0966  lr:0.000001
[ Tue Jul  9 10:53:46 2024 ] 	Batch(4300/7879) done. Loss: 0.2751  lr:0.000001
[ Tue Jul  9 10:54:10 2024 ] 	Batch(4400/7879) done. Loss: 0.1376  lr:0.000001
[ Tue Jul  9 10:54:33 2024 ] 
Training: Epoch [94/120], Step [4499], Loss: 0.20296069979667664, Training Accuracy: 96.10277777777779
[ Tue Jul  9 10:54:33 2024 ] 	Batch(4500/7879) done. Loss: 0.3795  lr:0.000001
[ Tue Jul  9 10:54:56 2024 ] 	Batch(4600/7879) done. Loss: 0.0307  lr:0.000001
[ Tue Jul  9 10:55:19 2024 ] 	Batch(4700/7879) done. Loss: 0.0065  lr:0.000001
[ Tue Jul  9 10:55:41 2024 ] 	Batch(4800/7879) done. Loss: 0.0759  lr:0.000001
[ Tue Jul  9 10:56:04 2024 ] 	Batch(4900/7879) done. Loss: 0.0555  lr:0.000001
[ Tue Jul  9 10:56:26 2024 ] 
Training: Epoch [94/120], Step [4999], Loss: 0.06837215274572372, Training Accuracy: 96.1025
[ Tue Jul  9 10:56:27 2024 ] 	Batch(5000/7879) done. Loss: 0.0085  lr:0.000001
[ Tue Jul  9 10:56:49 2024 ] 	Batch(5100/7879) done. Loss: 0.1078  lr:0.000001
[ Tue Jul  9 10:57:12 2024 ] 	Batch(5200/7879) done. Loss: 0.0033  lr:0.000001
[ Tue Jul  9 10:57:35 2024 ] 	Batch(5300/7879) done. Loss: 0.0097  lr:0.000001
[ Tue Jul  9 10:57:57 2024 ] 	Batch(5400/7879) done. Loss: 0.4900  lr:0.000001
[ Tue Jul  9 10:58:20 2024 ] 
Training: Epoch [94/120], Step [5499], Loss: 0.020505594089627266, Training Accuracy: 96.12045454545455
[ Tue Jul  9 10:58:20 2024 ] 	Batch(5500/7879) done. Loss: 0.1344  lr:0.000001
[ Tue Jul  9 10:58:42 2024 ] 	Batch(5600/7879) done. Loss: 0.0237  lr:0.000001
[ Tue Jul  9 10:59:05 2024 ] 	Batch(5700/7879) done. Loss: 0.0593  lr:0.000001
[ Tue Jul  9 10:59:28 2024 ] 	Batch(5800/7879) done. Loss: 0.4506  lr:0.000001
[ Tue Jul  9 10:59:50 2024 ] 	Batch(5900/7879) done. Loss: 0.0201  lr:0.000001
[ Tue Jul  9 11:00:12 2024 ] 
Training: Epoch [94/120], Step [5999], Loss: 0.1426251083612442, Training Accuracy: 96.11874999999999
[ Tue Jul  9 11:00:13 2024 ] 	Batch(6000/7879) done. Loss: 0.1235  lr:0.000001
[ Tue Jul  9 11:00:35 2024 ] 	Batch(6100/7879) done. Loss: 0.3695  lr:0.000001
[ Tue Jul  9 11:00:58 2024 ] 	Batch(6200/7879) done. Loss: 0.0378  lr:0.000001
[ Tue Jul  9 11:01:21 2024 ] 	Batch(6300/7879) done. Loss: 0.0089  lr:0.000001
[ Tue Jul  9 11:01:43 2024 ] 	Batch(6400/7879) done. Loss: 0.0218  lr:0.000001
[ Tue Jul  9 11:02:06 2024 ] 
Training: Epoch [94/120], Step [6499], Loss: 0.17722287774085999, Training Accuracy: 96.10961538461538
[ Tue Jul  9 11:02:06 2024 ] 	Batch(6500/7879) done. Loss: 0.0035  lr:0.000001
[ Tue Jul  9 11:02:28 2024 ] 	Batch(6600/7879) done. Loss: 0.0211  lr:0.000001
[ Tue Jul  9 11:02:51 2024 ] 	Batch(6700/7879) done. Loss: 0.0892  lr:0.000001
[ Tue Jul  9 11:03:14 2024 ] 	Batch(6800/7879) done. Loss: 0.1777  lr:0.000001
[ Tue Jul  9 11:03:36 2024 ] 	Batch(6900/7879) done. Loss: 0.0797  lr:0.000001
[ Tue Jul  9 11:03:59 2024 ] 
Training: Epoch [94/120], Step [6999], Loss: 0.39716362953186035, Training Accuracy: 96.11428571428571
[ Tue Jul  9 11:03:59 2024 ] 	Batch(7000/7879) done. Loss: 0.0158  lr:0.000001
[ Tue Jul  9 11:04:21 2024 ] 	Batch(7100/7879) done. Loss: 0.0191  lr:0.000001
[ Tue Jul  9 11:04:44 2024 ] 	Batch(7200/7879) done. Loss: 0.0591  lr:0.000001
[ Tue Jul  9 11:05:07 2024 ] 	Batch(7300/7879) done. Loss: 0.1822  lr:0.000001
[ Tue Jul  9 11:05:29 2024 ] 	Batch(7400/7879) done. Loss: 0.4167  lr:0.000001
[ Tue Jul  9 11:05:51 2024 ] 
Training: Epoch [94/120], Step [7499], Loss: 0.029886681586503983, Training Accuracy: 96.115
[ Tue Jul  9 11:05:52 2024 ] 	Batch(7500/7879) done. Loss: 0.1361  lr:0.000001
[ Tue Jul  9 11:06:14 2024 ] 	Batch(7600/7879) done. Loss: 0.0630  lr:0.000001
[ Tue Jul  9 11:06:37 2024 ] 	Batch(7700/7879) done. Loss: 0.1640  lr:0.000001
[ Tue Jul  9 11:06:59 2024 ] 	Batch(7800/7879) done. Loss: 0.1198  lr:0.000001
[ Tue Jul  9 11:07:17 2024 ] 	Mean training loss: 0.1435.
[ Tue Jul  9 11:07:17 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 11:07:17 2024 ] Training epoch: 96
[ Tue Jul  9 11:07:18 2024 ] 	Batch(0/7879) done. Loss: 0.0032  lr:0.000001
[ Tue Jul  9 11:07:40 2024 ] 	Batch(100/7879) done. Loss: 0.0681  lr:0.000001
[ Tue Jul  9 11:08:03 2024 ] 	Batch(200/7879) done. Loss: 0.2475  lr:0.000001
[ Tue Jul  9 11:08:25 2024 ] 	Batch(300/7879) done. Loss: 0.4529  lr:0.000001
[ Tue Jul  9 11:08:48 2024 ] 	Batch(400/7879) done. Loss: 0.0049  lr:0.000001
[ Tue Jul  9 11:09:10 2024 ] 
Training: Epoch [95/120], Step [499], Loss: 0.024275977164506912, Training Accuracy: 96.475
[ Tue Jul  9 11:09:11 2024 ] 	Batch(500/7879) done. Loss: 0.0506  lr:0.000001
[ Tue Jul  9 11:09:33 2024 ] 	Batch(600/7879) done. Loss: 0.2060  lr:0.000001
[ Tue Jul  9 11:09:56 2024 ] 	Batch(700/7879) done. Loss: 0.0236  lr:0.000001
[ Tue Jul  9 11:10:19 2024 ] 	Batch(800/7879) done. Loss: 0.1714  lr:0.000001
[ Tue Jul  9 11:10:41 2024 ] 	Batch(900/7879) done. Loss: 0.0039  lr:0.000001
[ Tue Jul  9 11:11:03 2024 ] 
Training: Epoch [95/120], Step [999], Loss: 0.014006394892930984, Training Accuracy: 96.575
[ Tue Jul  9 11:11:04 2024 ] 	Batch(1000/7879) done. Loss: 0.0775  lr:0.000001
[ Tue Jul  9 11:11:26 2024 ] 	Batch(1100/7879) done. Loss: 0.7683  lr:0.000001
[ Tue Jul  9 11:11:49 2024 ] 	Batch(1200/7879) done. Loss: 0.0443  lr:0.000001
[ Tue Jul  9 11:12:12 2024 ] 	Batch(1300/7879) done. Loss: 0.0457  lr:0.000001
[ Tue Jul  9 11:12:34 2024 ] 	Batch(1400/7879) done. Loss: 0.0837  lr:0.000001
[ Tue Jul  9 11:12:56 2024 ] 
Training: Epoch [95/120], Step [1499], Loss: 0.006940571125596762, Training Accuracy: 96.38333333333333
[ Tue Jul  9 11:12:57 2024 ] 	Batch(1500/7879) done. Loss: 1.1395  lr:0.000001
[ Tue Jul  9 11:13:19 2024 ] 	Batch(1600/7879) done. Loss: 0.1141  lr:0.000001
[ Tue Jul  9 11:13:42 2024 ] 	Batch(1700/7879) done. Loss: 0.0862  lr:0.000001
[ Tue Jul  9 11:14:04 2024 ] 	Batch(1800/7879) done. Loss: 0.1102  lr:0.000001
[ Tue Jul  9 11:14:27 2024 ] 	Batch(1900/7879) done. Loss: 0.1115  lr:0.000001
[ Tue Jul  9 11:14:49 2024 ] 
Training: Epoch [95/120], Step [1999], Loss: 0.2711607813835144, Training Accuracy: 96.36875
[ Tue Jul  9 11:14:50 2024 ] 	Batch(2000/7879) done. Loss: 0.1781  lr:0.000001
[ Tue Jul  9 11:15:12 2024 ] 	Batch(2100/7879) done. Loss: 0.1880  lr:0.000001
[ Tue Jul  9 11:15:35 2024 ] 	Batch(2200/7879) done. Loss: 0.0314  lr:0.000001
[ Tue Jul  9 11:15:57 2024 ] 	Batch(2300/7879) done. Loss: 0.0490  lr:0.000001
[ Tue Jul  9 11:16:20 2024 ] 	Batch(2400/7879) done. Loss: 0.0278  lr:0.000001
[ Tue Jul  9 11:16:42 2024 ] 
Training: Epoch [95/120], Step [2499], Loss: 0.049912262707948685, Training Accuracy: 96.31
[ Tue Jul  9 11:16:43 2024 ] 	Batch(2500/7879) done. Loss: 0.0358  lr:0.000001
[ Tue Jul  9 11:17:05 2024 ] 	Batch(2600/7879) done. Loss: 0.0793  lr:0.000001
[ Tue Jul  9 11:17:28 2024 ] 	Batch(2700/7879) done. Loss: 0.2779  lr:0.000001
[ Tue Jul  9 11:17:50 2024 ] 	Batch(2800/7879) done. Loss: 0.2349  lr:0.000001
[ Tue Jul  9 11:18:14 2024 ] 	Batch(2900/7879) done. Loss: 0.1770  lr:0.000001
[ Tue Jul  9 11:18:37 2024 ] 
Training: Epoch [95/120], Step [2999], Loss: 0.37450578808784485, Training Accuracy: 96.26249999999999
[ Tue Jul  9 11:18:37 2024 ] 	Batch(3000/7879) done. Loss: 0.0221  lr:0.000001
[ Tue Jul  9 11:19:00 2024 ] 	Batch(3100/7879) done. Loss: 0.1354  lr:0.000001
[ Tue Jul  9 11:19:23 2024 ] 	Batch(3200/7879) done. Loss: 0.2366  lr:0.000001
[ Tue Jul  9 11:19:46 2024 ] 	Batch(3300/7879) done. Loss: 0.0547  lr:0.000001
[ Tue Jul  9 11:20:09 2024 ] 	Batch(3400/7879) done. Loss: 0.3789  lr:0.000001
[ Tue Jul  9 11:20:31 2024 ] 
Training: Epoch [95/120], Step [3499], Loss: 0.011691189371049404, Training Accuracy: 96.30714285714286
[ Tue Jul  9 11:20:31 2024 ] 	Batch(3500/7879) done. Loss: 0.0696  lr:0.000001
[ Tue Jul  9 11:20:54 2024 ] 	Batch(3600/7879) done. Loss: 0.1271  lr:0.000001
[ Tue Jul  9 11:21:17 2024 ] 	Batch(3700/7879) done. Loss: 0.4290  lr:0.000001
[ Tue Jul  9 11:21:40 2024 ] 	Batch(3800/7879) done. Loss: 0.0246  lr:0.000001
[ Tue Jul  9 11:22:04 2024 ] 	Batch(3900/7879) done. Loss: 0.0838  lr:0.000001
[ Tue Jul  9 11:22:27 2024 ] 
Training: Epoch [95/120], Step [3999], Loss: 0.15059438347816467, Training Accuracy: 96.35312499999999
[ Tue Jul  9 11:22:27 2024 ] 	Batch(4000/7879) done. Loss: 0.3573  lr:0.000001
[ Tue Jul  9 11:22:50 2024 ] 	Batch(4100/7879) done. Loss: 0.0772  lr:0.000001
[ Tue Jul  9 11:23:14 2024 ] 	Batch(4200/7879) done. Loss: 0.0935  lr:0.000001
[ Tue Jul  9 11:23:37 2024 ] 	Batch(4300/7879) done. Loss: 0.1545  lr:0.000001
[ Tue Jul  9 11:24:00 2024 ] 	Batch(4400/7879) done. Loss: 0.3542  lr:0.000001
[ Tue Jul  9 11:24:23 2024 ] 
Training: Epoch [95/120], Step [4499], Loss: 0.0039497376419603825, Training Accuracy: 96.3
[ Tue Jul  9 11:24:23 2024 ] 	Batch(4500/7879) done. Loss: 0.0112  lr:0.000001
[ Tue Jul  9 11:24:46 2024 ] 	Batch(4600/7879) done. Loss: 0.0156  lr:0.000001
[ Tue Jul  9 11:25:08 2024 ] 	Batch(4700/7879) done. Loss: 0.1949  lr:0.000001
[ Tue Jul  9 11:25:31 2024 ] 	Batch(4800/7879) done. Loss: 0.1781  lr:0.000001
[ Tue Jul  9 11:25:54 2024 ] 	Batch(4900/7879) done. Loss: 0.3057  lr:0.000001
[ Tue Jul  9 11:26:17 2024 ] 
Training: Epoch [95/120], Step [4999], Loss: 0.060299765318632126, Training Accuracy: 96.32
[ Tue Jul  9 11:26:17 2024 ] 	Batch(5000/7879) done. Loss: 0.1877  lr:0.000001
[ Tue Jul  9 11:26:41 2024 ] 	Batch(5100/7879) done. Loss: 0.0086  lr:0.000001
[ Tue Jul  9 11:27:04 2024 ] 	Batch(5200/7879) done. Loss: 0.3117  lr:0.000001
[ Tue Jul  9 11:27:27 2024 ] 	Batch(5300/7879) done. Loss: 0.5784  lr:0.000001
[ Tue Jul  9 11:27:51 2024 ] 	Batch(5400/7879) done. Loss: 0.4318  lr:0.000001
[ Tue Jul  9 11:28:14 2024 ] 
Training: Epoch [95/120], Step [5499], Loss: 0.09311877936124802, Training Accuracy: 96.2590909090909
[ Tue Jul  9 11:28:14 2024 ] 	Batch(5500/7879) done. Loss: 0.0796  lr:0.000001
[ Tue Jul  9 11:28:38 2024 ] 	Batch(5600/7879) done. Loss: 0.0197  lr:0.000001
[ Tue Jul  9 11:29:01 2024 ] 	Batch(5700/7879) done. Loss: 0.3729  lr:0.000001
[ Tue Jul  9 11:29:24 2024 ] 	Batch(5800/7879) done. Loss: 0.2964  lr:0.000001
[ Tue Jul  9 11:29:47 2024 ] 	Batch(5900/7879) done. Loss: 0.1028  lr:0.000001
[ Tue Jul  9 11:30:10 2024 ] 
Training: Epoch [95/120], Step [5999], Loss: 0.08854766935110092, Training Accuracy: 96.27708333333334
[ Tue Jul  9 11:30:11 2024 ] 	Batch(6000/7879) done. Loss: 0.0956  lr:0.000001
[ Tue Jul  9 11:30:34 2024 ] 	Batch(6100/7879) done. Loss: 0.0266  lr:0.000001
[ Tue Jul  9 11:30:56 2024 ] 	Batch(6200/7879) done. Loss: 0.0678  lr:0.000001
[ Tue Jul  9 11:31:19 2024 ] 	Batch(6300/7879) done. Loss: 0.0330  lr:0.000001
[ Tue Jul  9 11:31:42 2024 ] 	Batch(6400/7879) done. Loss: 0.1316  lr:0.000001
[ Tue Jul  9 11:32:05 2024 ] 
Training: Epoch [95/120], Step [6499], Loss: 0.09518349915742874, Training Accuracy: 96.26538461538462
[ Tue Jul  9 11:32:05 2024 ] 	Batch(6500/7879) done. Loss: 0.0168  lr:0.000001
[ Tue Jul  9 11:32:28 2024 ] 	Batch(6600/7879) done. Loss: 0.0263  lr:0.000001
[ Tue Jul  9 11:32:51 2024 ] 	Batch(6700/7879) done. Loss: 0.0008  lr:0.000001
[ Tue Jul  9 11:33:13 2024 ] 	Batch(6800/7879) done. Loss: 0.0766  lr:0.000001
[ Tue Jul  9 11:33:36 2024 ] 	Batch(6900/7879) done. Loss: 0.0087  lr:0.000001
[ Tue Jul  9 11:33:58 2024 ] 
Training: Epoch [95/120], Step [6999], Loss: 0.08321048319339752, Training Accuracy: 96.23928571428571
[ Tue Jul  9 11:33:58 2024 ] 	Batch(7000/7879) done. Loss: 0.0083  lr:0.000001
[ Tue Jul  9 11:34:21 2024 ] 	Batch(7100/7879) done. Loss: 0.2267  lr:0.000001
[ Tue Jul  9 11:34:44 2024 ] 	Batch(7200/7879) done. Loss: 0.1118  lr:0.000001
[ Tue Jul  9 11:35:06 2024 ] 	Batch(7300/7879) done. Loss: 0.0139  lr:0.000001
[ Tue Jul  9 11:35:29 2024 ] 	Batch(7400/7879) done. Loss: 0.1396  lr:0.000001
[ Tue Jul  9 11:35:51 2024 ] 
Training: Epoch [95/120], Step [7499], Loss: 0.2658765912055969, Training Accuracy: 96.21499999999999
[ Tue Jul  9 11:35:51 2024 ] 	Batch(7500/7879) done. Loss: 0.5280  lr:0.000001
[ Tue Jul  9 11:36:14 2024 ] 	Batch(7600/7879) done. Loss: 0.1015  lr:0.000001
[ Tue Jul  9 11:36:37 2024 ] 	Batch(7700/7879) done. Loss: 0.0416  lr:0.000001
[ Tue Jul  9 11:36:59 2024 ] 	Batch(7800/7879) done. Loss: 0.0214  lr:0.000001
[ Tue Jul  9 11:37:17 2024 ] 	Mean training loss: 0.1448.
[ Tue Jul  9 11:37:17 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 11:37:17 2024 ] Training epoch: 97
[ Tue Jul  9 11:37:17 2024 ] 	Batch(0/7879) done. Loss: 0.5660  lr:0.000001
[ Tue Jul  9 11:37:40 2024 ] 	Batch(100/7879) done. Loss: 0.0125  lr:0.000001
[ Tue Jul  9 11:38:03 2024 ] 	Batch(200/7879) done. Loss: 0.1856  lr:0.000001
[ Tue Jul  9 11:38:25 2024 ] 	Batch(300/7879) done. Loss: 0.1197  lr:0.000001
[ Tue Jul  9 11:38:48 2024 ] 	Batch(400/7879) done. Loss: 0.0729  lr:0.000001
[ Tue Jul  9 11:39:10 2024 ] 
Training: Epoch [96/120], Step [499], Loss: 0.04329816251993179, Training Accuracy: 96.275
[ Tue Jul  9 11:39:10 2024 ] 	Batch(500/7879) done. Loss: 0.0484  lr:0.000001
[ Tue Jul  9 11:39:33 2024 ] 	Batch(600/7879) done. Loss: 0.0570  lr:0.000001
[ Tue Jul  9 11:39:55 2024 ] 	Batch(700/7879) done. Loss: 0.0447  lr:0.000001
[ Tue Jul  9 11:40:18 2024 ] 	Batch(800/7879) done. Loss: 0.4441  lr:0.000001
[ Tue Jul  9 11:40:41 2024 ] 	Batch(900/7879) done. Loss: 0.0299  lr:0.000001
[ Tue Jul  9 11:41:04 2024 ] 
Training: Epoch [96/120], Step [999], Loss: 0.29146242141723633, Training Accuracy: 96.3125
[ Tue Jul  9 11:41:05 2024 ] 	Batch(1000/7879) done. Loss: 0.2307  lr:0.000001
[ Tue Jul  9 11:41:28 2024 ] 	Batch(1100/7879) done. Loss: 0.0304  lr:0.000001
[ Tue Jul  9 11:41:51 2024 ] 	Batch(1200/7879) done. Loss: 0.5422  lr:0.000001
[ Tue Jul  9 11:42:14 2024 ] 	Batch(1300/7879) done. Loss: 0.0564  lr:0.000001
[ Tue Jul  9 11:42:36 2024 ] 	Batch(1400/7879) done. Loss: 0.0190  lr:0.000001
[ Tue Jul  9 11:42:59 2024 ] 
Training: Epoch [96/120], Step [1499], Loss: 0.009254032745957375, Training Accuracy: 96.325
[ Tue Jul  9 11:42:59 2024 ] 	Batch(1500/7879) done. Loss: 0.1195  lr:0.000001
[ Tue Jul  9 11:43:22 2024 ] 	Batch(1600/7879) done. Loss: 0.0972  lr:0.000001
[ Tue Jul  9 11:43:44 2024 ] 	Batch(1700/7879) done. Loss: 0.8812  lr:0.000001
[ Tue Jul  9 11:44:07 2024 ] 	Batch(1800/7879) done. Loss: 0.0416  lr:0.000001
[ Tue Jul  9 11:44:29 2024 ] 	Batch(1900/7879) done. Loss: 0.0174  lr:0.000001
[ Tue Jul  9 11:44:52 2024 ] 
Training: Epoch [96/120], Step [1999], Loss: 0.1296927034854889, Training Accuracy: 96.16875
[ Tue Jul  9 11:44:52 2024 ] 	Batch(2000/7879) done. Loss: 0.0389  lr:0.000001
[ Tue Jul  9 11:45:14 2024 ] 	Batch(2100/7879) done. Loss: 0.0384  lr:0.000001
[ Tue Jul  9 11:45:37 2024 ] 	Batch(2200/7879) done. Loss: 0.1874  lr:0.000001
[ Tue Jul  9 11:46:00 2024 ] 	Batch(2300/7879) done. Loss: 0.0138  lr:0.000001
[ Tue Jul  9 11:46:22 2024 ] 	Batch(2400/7879) done. Loss: 0.0146  lr:0.000001
[ Tue Jul  9 11:46:46 2024 ] 
Training: Epoch [96/120], Step [2499], Loss: 0.06141422688961029, Training Accuracy: 96.235
[ Tue Jul  9 11:46:46 2024 ] 	Batch(2500/7879) done. Loss: 0.0763  lr:0.000001
[ Tue Jul  9 11:47:10 2024 ] 	Batch(2600/7879) done. Loss: 0.2291  lr:0.000001
[ Tue Jul  9 11:47:33 2024 ] 	Batch(2700/7879) done. Loss: 0.3818  lr:0.000001
[ Tue Jul  9 11:47:57 2024 ] 	Batch(2800/7879) done. Loss: 0.2704  lr:0.000001
[ Tue Jul  9 11:48:20 2024 ] 	Batch(2900/7879) done. Loss: 0.1461  lr:0.000001
[ Tue Jul  9 11:48:42 2024 ] 
Training: Epoch [96/120], Step [2999], Loss: 0.001986774383112788, Training Accuracy: 96.22916666666667
[ Tue Jul  9 11:48:42 2024 ] 	Batch(3000/7879) done. Loss: 0.1342  lr:0.000001
[ Tue Jul  9 11:49:05 2024 ] 	Batch(3100/7879) done. Loss: 0.0235  lr:0.000001
[ Tue Jul  9 11:49:28 2024 ] 	Batch(3200/7879) done. Loss: 0.0246  lr:0.000001
[ Tue Jul  9 11:49:50 2024 ] 	Batch(3300/7879) done. Loss: 0.3536  lr:0.000001
[ Tue Jul  9 11:50:13 2024 ] 	Batch(3400/7879) done. Loss: 0.0387  lr:0.000001
[ Tue Jul  9 11:50:35 2024 ] 
Training: Epoch [96/120], Step [3499], Loss: 0.01833185739815235, Training Accuracy: 96.16428571428571
[ Tue Jul  9 11:50:35 2024 ] 	Batch(3500/7879) done. Loss: 0.1360  lr:0.000001
[ Tue Jul  9 11:50:58 2024 ] 	Batch(3600/7879) done. Loss: 0.0274  lr:0.000001
[ Tue Jul  9 11:51:21 2024 ] 	Batch(3700/7879) done. Loss: 0.1131  lr:0.000001
[ Tue Jul  9 11:51:43 2024 ] 	Batch(3800/7879) done. Loss: 0.3857  lr:0.000001
[ Tue Jul  9 11:52:06 2024 ] 	Batch(3900/7879) done. Loss: 0.0475  lr:0.000001
[ Tue Jul  9 11:52:28 2024 ] 
Training: Epoch [96/120], Step [3999], Loss: 0.4366770088672638, Training Accuracy: 96.16875
[ Tue Jul  9 11:52:28 2024 ] 	Batch(4000/7879) done. Loss: 0.1673  lr:0.000001
[ Tue Jul  9 11:52:51 2024 ] 	Batch(4100/7879) done. Loss: 0.0105  lr:0.000001
[ Tue Jul  9 11:53:14 2024 ] 	Batch(4200/7879) done. Loss: 0.1075  lr:0.000001
[ Tue Jul  9 11:53:36 2024 ] 	Batch(4300/7879) done. Loss: 0.4226  lr:0.000001
[ Tue Jul  9 11:53:59 2024 ] 	Batch(4400/7879) done. Loss: 0.0232  lr:0.000001
[ Tue Jul  9 11:54:22 2024 ] 
Training: Epoch [96/120], Step [4499], Loss: 0.03914860263466835, Training Accuracy: 96.17777777777778
[ Tue Jul  9 11:54:22 2024 ] 	Batch(4500/7879) done. Loss: 0.0663  lr:0.000001
[ Tue Jul  9 11:54:45 2024 ] 	Batch(4600/7879) done. Loss: 0.0296  lr:0.000001
[ Tue Jul  9 11:55:07 2024 ] 	Batch(4700/7879) done. Loss: 0.1265  lr:0.000001
[ Tue Jul  9 11:55:30 2024 ] 	Batch(4800/7879) done. Loss: 0.3536  lr:0.000001
[ Tue Jul  9 11:55:53 2024 ] 	Batch(4900/7879) done. Loss: 0.0911  lr:0.000001
[ Tue Jul  9 11:56:15 2024 ] 
Training: Epoch [96/120], Step [4999], Loss: 0.10772542655467987, Training Accuracy: 96.1825
[ Tue Jul  9 11:56:15 2024 ] 	Batch(5000/7879) done. Loss: 0.0292  lr:0.000001
[ Tue Jul  9 11:56:38 2024 ] 	Batch(5100/7879) done. Loss: 0.2222  lr:0.000001
[ Tue Jul  9 11:57:01 2024 ] 	Batch(5200/7879) done. Loss: 0.0628  lr:0.000001
[ Tue Jul  9 11:57:23 2024 ] 	Batch(5300/7879) done. Loss: 0.3514  lr:0.000001
[ Tue Jul  9 11:57:46 2024 ] 	Batch(5400/7879) done. Loss: 0.4077  lr:0.000001
[ Tue Jul  9 11:58:08 2024 ] 
Training: Epoch [96/120], Step [5499], Loss: 0.05464000254869461, Training Accuracy: 96.14318181818182
[ Tue Jul  9 11:58:08 2024 ] 	Batch(5500/7879) done. Loss: 0.2104  lr:0.000001
[ Tue Jul  9 11:58:31 2024 ] 	Batch(5600/7879) done. Loss: 0.0273  lr:0.000001
[ Tue Jul  9 11:58:54 2024 ] 	Batch(5700/7879) done. Loss: 0.1762  lr:0.000001
[ Tue Jul  9 11:59:16 2024 ] 	Batch(5800/7879) done. Loss: 0.2426  lr:0.000001
[ Tue Jul  9 11:59:39 2024 ] 	Batch(5900/7879) done. Loss: 0.0619  lr:0.000001
[ Tue Jul  9 12:00:01 2024 ] 
Training: Epoch [96/120], Step [5999], Loss: 0.03865870088338852, Training Accuracy: 96.1375
[ Tue Jul  9 12:00:02 2024 ] 	Batch(6000/7879) done. Loss: 0.1180  lr:0.000001
[ Tue Jul  9 12:00:24 2024 ] 	Batch(6100/7879) done. Loss: 0.0933  lr:0.000001
[ Tue Jul  9 12:00:47 2024 ] 	Batch(6200/7879) done. Loss: 0.2617  lr:0.000001
[ Tue Jul  9 12:01:09 2024 ] 	Batch(6300/7879) done. Loss: 0.2810  lr:0.000001
[ Tue Jul  9 12:01:32 2024 ] 	Batch(6400/7879) done. Loss: 0.2320  lr:0.000001
[ Tue Jul  9 12:01:54 2024 ] 
Training: Epoch [96/120], Step [6499], Loss: 0.09771765023469925, Training Accuracy: 96.1403846153846
[ Tue Jul  9 12:01:55 2024 ] 	Batch(6500/7879) done. Loss: 0.1850  lr:0.000001
[ Tue Jul  9 12:02:17 2024 ] 	Batch(6600/7879) done. Loss: 0.0192  lr:0.000001
[ Tue Jul  9 12:02:40 2024 ] 	Batch(6700/7879) done. Loss: 0.0085  lr:0.000001
[ Tue Jul  9 12:03:02 2024 ] 	Batch(6800/7879) done. Loss: 0.1083  lr:0.000001
[ Tue Jul  9 12:03:26 2024 ] 	Batch(6900/7879) done. Loss: 0.5667  lr:0.000001
[ Tue Jul  9 12:03:48 2024 ] 
Training: Epoch [96/120], Step [6999], Loss: 0.03719894215464592, Training Accuracy: 96.14821428571429
[ Tue Jul  9 12:03:48 2024 ] 	Batch(7000/7879) done. Loss: 0.1124  lr:0.000001
[ Tue Jul  9 12:04:11 2024 ] 	Batch(7100/7879) done. Loss: 0.0768  lr:0.000001
[ Tue Jul  9 12:04:34 2024 ] 	Batch(7200/7879) done. Loss: 0.1007  lr:0.000001
[ Tue Jul  9 12:04:56 2024 ] 	Batch(7300/7879) done. Loss: 0.1359  lr:0.000001
[ Tue Jul  9 12:05:19 2024 ] 	Batch(7400/7879) done. Loss: 0.0531  lr:0.000001
[ Tue Jul  9 12:05:41 2024 ] 
Training: Epoch [96/120], Step [7499], Loss: 0.12729385495185852, Training Accuracy: 96.11833333333333
[ Tue Jul  9 12:05:41 2024 ] 	Batch(7500/7879) done. Loss: 0.1061  lr:0.000001
[ Tue Jul  9 12:06:04 2024 ] 	Batch(7600/7879) done. Loss: 0.3484  lr:0.000001
[ Tue Jul  9 12:06:27 2024 ] 	Batch(7700/7879) done. Loss: 0.0528  lr:0.000001
[ Tue Jul  9 12:06:49 2024 ] 	Batch(7800/7879) done. Loss: 0.1207  lr:0.000001
[ Tue Jul  9 12:07:07 2024 ] 	Mean training loss: 0.1433.
[ Tue Jul  9 12:07:07 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 12:07:07 2024 ] Training epoch: 98
[ Tue Jul  9 12:07:07 2024 ] 	Batch(0/7879) done. Loss: 0.0205  lr:0.000001
[ Tue Jul  9 12:07:30 2024 ] 	Batch(100/7879) done. Loss: 0.0968  lr:0.000001
[ Tue Jul  9 12:07:52 2024 ] 	Batch(200/7879) done. Loss: 0.0137  lr:0.000001
[ Tue Jul  9 12:08:15 2024 ] 	Batch(300/7879) done. Loss: 0.4199  lr:0.000001
[ Tue Jul  9 12:08:38 2024 ] 	Batch(400/7879) done. Loss: 0.0613  lr:0.000001
[ Tue Jul  9 12:09:00 2024 ] 
Training: Epoch [97/120], Step [499], Loss: 0.15247756242752075, Training Accuracy: 95.825
[ Tue Jul  9 12:09:00 2024 ] 	Batch(500/7879) done. Loss: 0.0407  lr:0.000001
[ Tue Jul  9 12:09:23 2024 ] 	Batch(600/7879) done. Loss: 0.1748  lr:0.000001
[ Tue Jul  9 12:09:45 2024 ] 	Batch(700/7879) done. Loss: 0.1144  lr:0.000001
[ Tue Jul  9 12:10:08 2024 ] 	Batch(800/7879) done. Loss: 0.0368  lr:0.000001
[ Tue Jul  9 12:10:32 2024 ] 	Batch(900/7879) done. Loss: 0.0251  lr:0.000001
[ Tue Jul  9 12:10:55 2024 ] 
Training: Epoch [97/120], Step [999], Loss: 0.022277947515249252, Training Accuracy: 95.96249999999999
[ Tue Jul  9 12:10:55 2024 ] 	Batch(1000/7879) done. Loss: 0.0058  lr:0.000001
[ Tue Jul  9 12:11:18 2024 ] 	Batch(1100/7879) done. Loss: 0.1426  lr:0.000001
[ Tue Jul  9 12:11:42 2024 ] 	Batch(1200/7879) done. Loss: 0.0388  lr:0.000001
[ Tue Jul  9 12:12:04 2024 ] 	Batch(1300/7879) done. Loss: 0.1635  lr:0.000001
[ Tue Jul  9 12:12:27 2024 ] 	Batch(1400/7879) done. Loss: 0.3246  lr:0.000001
[ Tue Jul  9 12:12:49 2024 ] 
Training: Epoch [97/120], Step [1499], Loss: 0.04610668495297432, Training Accuracy: 96.04166666666667
[ Tue Jul  9 12:12:49 2024 ] 	Batch(1500/7879) done. Loss: 0.0981  lr:0.000001
[ Tue Jul  9 12:13:12 2024 ] 	Batch(1600/7879) done. Loss: 0.0848  lr:0.000001
[ Tue Jul  9 12:13:35 2024 ] 	Batch(1700/7879) done. Loss: 0.0139  lr:0.000001
[ Tue Jul  9 12:13:57 2024 ] 	Batch(1800/7879) done. Loss: 0.1507  lr:0.000001
[ Tue Jul  9 12:14:20 2024 ] 	Batch(1900/7879) done. Loss: 0.0294  lr:0.000001
[ Tue Jul  9 12:14:42 2024 ] 
Training: Epoch [97/120], Step [1999], Loss: 0.4021121859550476, Training Accuracy: 96.09375
[ Tue Jul  9 12:14:42 2024 ] 	Batch(2000/7879) done. Loss: 0.3895  lr:0.000001
[ Tue Jul  9 12:15:05 2024 ] 	Batch(2100/7879) done. Loss: 0.0606  lr:0.000001
[ Tue Jul  9 12:15:28 2024 ] 	Batch(2200/7879) done. Loss: 0.0166  lr:0.000001
[ Tue Jul  9 12:15:50 2024 ] 	Batch(2300/7879) done. Loss: 0.0208  lr:0.000001
[ Tue Jul  9 12:16:13 2024 ] 	Batch(2400/7879) done. Loss: 0.4098  lr:0.000001
[ Tue Jul  9 12:16:36 2024 ] 
Training: Epoch [97/120], Step [2499], Loss: 0.2988126873970032, Training Accuracy: 96.1
[ Tue Jul  9 12:16:36 2024 ] 	Batch(2500/7879) done. Loss: 0.3557  lr:0.000001
[ Tue Jul  9 12:16:59 2024 ] 	Batch(2600/7879) done. Loss: 0.1290  lr:0.000001
[ Tue Jul  9 12:17:21 2024 ] 	Batch(2700/7879) done. Loss: 0.0105  lr:0.000001
[ Tue Jul  9 12:17:44 2024 ] 	Batch(2800/7879) done. Loss: 0.1935  lr:0.000001
[ Tue Jul  9 12:18:07 2024 ] 	Batch(2900/7879) done. Loss: 0.2943  lr:0.000001
[ Tue Jul  9 12:18:29 2024 ] 
Training: Epoch [97/120], Step [2999], Loss: 0.08673039078712463, Training Accuracy: 96.12916666666666
[ Tue Jul  9 12:18:30 2024 ] 	Batch(3000/7879) done. Loss: 0.0177  lr:0.000001
[ Tue Jul  9 12:18:52 2024 ] 	Batch(3100/7879) done. Loss: 0.3380  lr:0.000001
[ Tue Jul  9 12:19:15 2024 ] 	Batch(3200/7879) done. Loss: 0.0719  lr:0.000001
[ Tue Jul  9 12:19:38 2024 ] 	Batch(3300/7879) done. Loss: 0.2937  lr:0.000001
[ Tue Jul  9 12:20:01 2024 ] 	Batch(3400/7879) done. Loss: 0.0477  lr:0.000001
[ Tue Jul  9 12:20:23 2024 ] 
Training: Epoch [97/120], Step [3499], Loss: 0.08958189189434052, Training Accuracy: 96.15357142857142
[ Tue Jul  9 12:20:23 2024 ] 	Batch(3500/7879) done. Loss: 0.2145  lr:0.000001
[ Tue Jul  9 12:20:46 2024 ] 	Batch(3600/7879) done. Loss: 0.0499  lr:0.000001
[ Tue Jul  9 12:21:09 2024 ] 	Batch(3700/7879) done. Loss: 0.0110  lr:0.000001
[ Tue Jul  9 12:21:32 2024 ] 	Batch(3800/7879) done. Loss: 0.1674  lr:0.000001
[ Tue Jul  9 12:21:54 2024 ] 	Batch(3900/7879) done. Loss: 0.0963  lr:0.000001
[ Tue Jul  9 12:22:17 2024 ] 
Training: Epoch [97/120], Step [3999], Loss: 0.07371421903371811, Training Accuracy: 96.1875
[ Tue Jul  9 12:22:17 2024 ] 	Batch(4000/7879) done. Loss: 0.0127  lr:0.000001
[ Tue Jul  9 12:22:40 2024 ] 	Batch(4100/7879) done. Loss: 0.0740  lr:0.000001
[ Tue Jul  9 12:23:02 2024 ] 	Batch(4200/7879) done. Loss: 0.4943  lr:0.000001
[ Tue Jul  9 12:23:25 2024 ] 	Batch(4300/7879) done. Loss: 0.2292  lr:0.000001
[ Tue Jul  9 12:23:48 2024 ] 	Batch(4400/7879) done. Loss: 0.0325  lr:0.000001
[ Tue Jul  9 12:24:10 2024 ] 
Training: Epoch [97/120], Step [4499], Loss: 0.25361111760139465, Training Accuracy: 96.19722222222222
[ Tue Jul  9 12:24:11 2024 ] 	Batch(4500/7879) done. Loss: 0.5253  lr:0.000001
[ Tue Jul  9 12:24:33 2024 ] 	Batch(4600/7879) done. Loss: 0.0449  lr:0.000001
[ Tue Jul  9 12:24:56 2024 ] 	Batch(4700/7879) done. Loss: 0.1531  lr:0.000001
[ Tue Jul  9 12:25:19 2024 ] 	Batch(4800/7879) done. Loss: 0.1714  lr:0.000001
[ Tue Jul  9 12:25:42 2024 ] 	Batch(4900/7879) done. Loss: 0.0822  lr:0.000001
[ Tue Jul  9 12:26:04 2024 ] 
Training: Epoch [97/120], Step [4999], Loss: 0.028019625693559647, Training Accuracy: 96.22
[ Tue Jul  9 12:26:04 2024 ] 	Batch(5000/7879) done. Loss: 0.1632  lr:0.000001
[ Tue Jul  9 12:26:27 2024 ] 	Batch(5100/7879) done. Loss: 0.0482  lr:0.000001
[ Tue Jul  9 12:26:50 2024 ] 	Batch(5200/7879) done. Loss: 0.0316  lr:0.000001
[ Tue Jul  9 12:27:13 2024 ] 	Batch(5300/7879) done. Loss: 0.0423  lr:0.000001
[ Tue Jul  9 12:27:35 2024 ] 	Batch(5400/7879) done. Loss: 0.2717  lr:0.000001
[ Tue Jul  9 12:27:58 2024 ] 
Training: Epoch [97/120], Step [5499], Loss: 0.40988439321517944, Training Accuracy: 96.26136363636364
[ Tue Jul  9 12:27:58 2024 ] 	Batch(5500/7879) done. Loss: 0.1763  lr:0.000001
[ Tue Jul  9 12:28:21 2024 ] 	Batch(5600/7879) done. Loss: 0.0926  lr:0.000001
[ Tue Jul  9 12:28:43 2024 ] 	Batch(5700/7879) done. Loss: 0.0130  lr:0.000001
[ Tue Jul  9 12:29:06 2024 ] 	Batch(5800/7879) done. Loss: 0.0473  lr:0.000001
[ Tue Jul  9 12:29:29 2024 ] 	Batch(5900/7879) done. Loss: 0.0253  lr:0.000001
[ Tue Jul  9 12:29:51 2024 ] 
Training: Epoch [97/120], Step [5999], Loss: 0.16444070637226105, Training Accuracy: 96.29791666666667
[ Tue Jul  9 12:29:52 2024 ] 	Batch(6000/7879) done. Loss: 0.0456  lr:0.000001
[ Tue Jul  9 12:30:14 2024 ] 	Batch(6100/7879) done. Loss: 0.1052  lr:0.000001
[ Tue Jul  9 12:30:37 2024 ] 	Batch(6200/7879) done. Loss: 0.2735  lr:0.000001
[ Tue Jul  9 12:31:00 2024 ] 	Batch(6300/7879) done. Loss: 0.2080  lr:0.000001
[ Tue Jul  9 12:31:23 2024 ] 	Batch(6400/7879) done. Loss: 0.2239  lr:0.000001
[ Tue Jul  9 12:31:46 2024 ] 
Training: Epoch [97/120], Step [6499], Loss: 0.004016905091702938, Training Accuracy: 96.31923076923077
[ Tue Jul  9 12:31:46 2024 ] 	Batch(6500/7879) done. Loss: 0.1838  lr:0.000001
[ Tue Jul  9 12:32:09 2024 ] 	Batch(6600/7879) done. Loss: 0.0994  lr:0.000001
[ Tue Jul  9 12:32:33 2024 ] 	Batch(6700/7879) done. Loss: 0.0479  lr:0.000001
[ Tue Jul  9 12:32:56 2024 ] 	Batch(6800/7879) done. Loss: 1.1128  lr:0.000001
[ Tue Jul  9 12:33:18 2024 ] 	Batch(6900/7879) done. Loss: 0.1008  lr:0.000001
[ Tue Jul  9 12:33:41 2024 ] 
Training: Epoch [97/120], Step [6999], Loss: 0.21082539856433868, Training Accuracy: 96.29464285714285
[ Tue Jul  9 12:33:41 2024 ] 	Batch(7000/7879) done. Loss: 0.1068  lr:0.000001
[ Tue Jul  9 12:34:04 2024 ] 	Batch(7100/7879) done. Loss: 0.4323  lr:0.000001
[ Tue Jul  9 12:34:26 2024 ] 	Batch(7200/7879) done. Loss: 0.2420  lr:0.000001
[ Tue Jul  9 12:34:49 2024 ] 	Batch(7300/7879) done. Loss: 0.0514  lr:0.000001
[ Tue Jul  9 12:35:12 2024 ] 	Batch(7400/7879) done. Loss: 0.0530  lr:0.000001
[ Tue Jul  9 12:35:34 2024 ] 
Training: Epoch [97/120], Step [7499], Loss: 0.18126200139522552, Training Accuracy: 96.30333333333333
[ Tue Jul  9 12:35:35 2024 ] 	Batch(7500/7879) done. Loss: 0.2319  lr:0.000001
[ Tue Jul  9 12:35:58 2024 ] 	Batch(7600/7879) done. Loss: 0.0752  lr:0.000001
[ Tue Jul  9 12:36:20 2024 ] 	Batch(7700/7879) done. Loss: 0.0353  lr:0.000001
[ Tue Jul  9 12:36:43 2024 ] 	Batch(7800/7879) done. Loss: 0.2284  lr:0.000001
[ Tue Jul  9 12:37:01 2024 ] 	Mean training loss: 0.1412.
[ Tue Jul  9 12:37:01 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 12:37:01 2024 ] Training epoch: 99
[ Tue Jul  9 12:37:01 2024 ] 	Batch(0/7879) done. Loss: 0.0298  lr:0.000001
[ Tue Jul  9 12:37:24 2024 ] 	Batch(100/7879) done. Loss: 0.0713  lr:0.000001
[ Tue Jul  9 12:37:47 2024 ] 	Batch(200/7879) done. Loss: 0.0789  lr:0.000001
[ Tue Jul  9 12:38:09 2024 ] 	Batch(300/7879) done. Loss: 0.2226  lr:0.000001
[ Tue Jul  9 12:38:32 2024 ] 	Batch(400/7879) done. Loss: 0.2447  lr:0.000001
[ Tue Jul  9 12:38:55 2024 ] 
Training: Epoch [98/120], Step [499], Loss: 0.030652623623609543, Training Accuracy: 96.125
[ Tue Jul  9 12:38:55 2024 ] 	Batch(500/7879) done. Loss: 0.1108  lr:0.000001
[ Tue Jul  9 12:39:18 2024 ] 	Batch(600/7879) done. Loss: 0.3349  lr:0.000001
[ Tue Jul  9 12:39:40 2024 ] 	Batch(700/7879) done. Loss: 0.0782  lr:0.000001
[ Tue Jul  9 12:40:03 2024 ] 	Batch(800/7879) done. Loss: 0.4696  lr:0.000001
[ Tue Jul  9 12:40:26 2024 ] 	Batch(900/7879) done. Loss: 0.1395  lr:0.000001
[ Tue Jul  9 12:40:48 2024 ] 
Training: Epoch [98/120], Step [999], Loss: 0.3519865870475769, Training Accuracy: 96.2
[ Tue Jul  9 12:40:48 2024 ] 	Batch(1000/7879) done. Loss: 0.2904  lr:0.000001
[ Tue Jul  9 12:41:11 2024 ] 	Batch(1100/7879) done. Loss: 0.2919  lr:0.000001
[ Tue Jul  9 12:41:34 2024 ] 	Batch(1200/7879) done. Loss: 0.0092  lr:0.000001
[ Tue Jul  9 12:41:57 2024 ] 	Batch(1300/7879) done. Loss: 0.3610  lr:0.000001
[ Tue Jul  9 12:42:19 2024 ] 	Batch(1400/7879) done. Loss: 0.1164  lr:0.000001
[ Tue Jul  9 12:42:42 2024 ] 
Training: Epoch [98/120], Step [1499], Loss: 0.2773626446723938, Training Accuracy: 96.10833333333333
[ Tue Jul  9 12:42:42 2024 ] 	Batch(1500/7879) done. Loss: 0.0044  lr:0.000001
[ Tue Jul  9 12:43:05 2024 ] 	Batch(1600/7879) done. Loss: 0.1503  lr:0.000001
[ Tue Jul  9 12:43:27 2024 ] 	Batch(1700/7879) done. Loss: 0.0444  lr:0.000001
[ Tue Jul  9 12:43:50 2024 ] 	Batch(1800/7879) done. Loss: 0.4723  lr:0.000001
[ Tue Jul  9 12:44:13 2024 ] 	Batch(1900/7879) done. Loss: 0.0725  lr:0.000001
[ Tue Jul  9 12:44:35 2024 ] 
Training: Epoch [98/120], Step [1999], Loss: 0.2727833390235901, Training Accuracy: 96.075
[ Tue Jul  9 12:44:36 2024 ] 	Batch(2000/7879) done. Loss: 0.0993  lr:0.000001
[ Tue Jul  9 12:44:58 2024 ] 	Batch(2100/7879) done. Loss: 0.0262  lr:0.000001
[ Tue Jul  9 12:45:21 2024 ] 	Batch(2200/7879) done. Loss: 0.1908  lr:0.000001
[ Tue Jul  9 12:45:44 2024 ] 	Batch(2300/7879) done. Loss: 0.1228  lr:0.000001
[ Tue Jul  9 12:46:07 2024 ] 	Batch(2400/7879) done. Loss: 0.1883  lr:0.000001
[ Tue Jul  9 12:46:30 2024 ] 
Training: Epoch [98/120], Step [2499], Loss: 0.03592418506741524, Training Accuracy: 96.045
[ Tue Jul  9 12:46:30 2024 ] 	Batch(2500/7879) done. Loss: 0.2739  lr:0.000001
[ Tue Jul  9 12:46:53 2024 ] 	Batch(2600/7879) done. Loss: 0.0273  lr:0.000001
[ Tue Jul  9 12:47:17 2024 ] 	Batch(2700/7879) done. Loss: 0.2104  lr:0.000001
[ Tue Jul  9 12:47:40 2024 ] 	Batch(2800/7879) done. Loss: 0.0742  lr:0.000001
[ Tue Jul  9 12:48:02 2024 ] 	Batch(2900/7879) done. Loss: 0.0418  lr:0.000001
[ Tue Jul  9 12:48:25 2024 ] 
Training: Epoch [98/120], Step [2999], Loss: 0.057001303881406784, Training Accuracy: 96.0
[ Tue Jul  9 12:48:25 2024 ] 	Batch(3000/7879) done. Loss: 0.0503  lr:0.000001
[ Tue Jul  9 12:48:48 2024 ] 	Batch(3100/7879) done. Loss: 0.6362  lr:0.000001
[ Tue Jul  9 12:49:10 2024 ] 	Batch(3200/7879) done. Loss: 0.0913  lr:0.000001
[ Tue Jul  9 12:49:33 2024 ] 	Batch(3300/7879) done. Loss: 0.2109  lr:0.000001
[ Tue Jul  9 12:49:56 2024 ] 	Batch(3400/7879) done. Loss: 0.0093  lr:0.000001
[ Tue Jul  9 12:50:18 2024 ] 
Training: Epoch [98/120], Step [3499], Loss: 0.10960526764392853, Training Accuracy: 95.99285714285715
[ Tue Jul  9 12:50:18 2024 ] 	Batch(3500/7879) done. Loss: 0.0822  lr:0.000001
[ Tue Jul  9 12:50:41 2024 ] 	Batch(3600/7879) done. Loss: 0.0217  lr:0.000001
[ Tue Jul  9 12:51:04 2024 ] 	Batch(3700/7879) done. Loss: 0.1744  lr:0.000001
[ Tue Jul  9 12:51:27 2024 ] 	Batch(3800/7879) done. Loss: 0.0703  lr:0.000001
[ Tue Jul  9 12:51:50 2024 ] 	Batch(3900/7879) done. Loss: 0.0123  lr:0.000001
[ Tue Jul  9 12:52:13 2024 ] 
Training: Epoch [98/120], Step [3999], Loss: 0.010590936057269573, Training Accuracy: 96.03125
[ Tue Jul  9 12:52:13 2024 ] 	Batch(4000/7879) done. Loss: 0.0494  lr:0.000001
[ Tue Jul  9 12:52:36 2024 ] 	Batch(4100/7879) done. Loss: 0.0102  lr:0.000001
[ Tue Jul  9 12:52:58 2024 ] 	Batch(4200/7879) done. Loss: 0.0692  lr:0.000001
[ Tue Jul  9 12:53:21 2024 ] 	Batch(4300/7879) done. Loss: 0.3517  lr:0.000001
[ Tue Jul  9 12:53:44 2024 ] 	Batch(4400/7879) done. Loss: 0.1450  lr:0.000001
[ Tue Jul  9 12:54:06 2024 ] 
Training: Epoch [98/120], Step [4499], Loss: 0.08664125204086304, Training Accuracy: 96.05
[ Tue Jul  9 12:54:07 2024 ] 	Batch(4500/7879) done. Loss: 0.1046  lr:0.000001
[ Tue Jul  9 12:54:29 2024 ] 	Batch(4600/7879) done. Loss: 0.0995  lr:0.000001
[ Tue Jul  9 12:54:52 2024 ] 	Batch(4700/7879) done. Loss: 0.3303  lr:0.000001
[ Tue Jul  9 12:55:15 2024 ] 	Batch(4800/7879) done. Loss: 0.1084  lr:0.000001
[ Tue Jul  9 12:55:38 2024 ] 	Batch(4900/7879) done. Loss: 0.0620  lr:0.000001
[ Tue Jul  9 12:56:00 2024 ] 
Training: Epoch [98/120], Step [4999], Loss: 0.01629304699599743, Training Accuracy: 96.09
[ Tue Jul  9 12:56:00 2024 ] 	Batch(5000/7879) done. Loss: 0.1680  lr:0.000001
[ Tue Jul  9 12:56:23 2024 ] 	Batch(5100/7879) done. Loss: 0.0754  lr:0.000001
[ Tue Jul  9 12:56:46 2024 ] 	Batch(5200/7879) done. Loss: 0.0065  lr:0.000001
[ Tue Jul  9 12:57:09 2024 ] 	Batch(5300/7879) done. Loss: 0.0719  lr:0.000001
[ Tue Jul  9 12:57:33 2024 ] 	Batch(5400/7879) done. Loss: 0.1034  lr:0.000001
[ Tue Jul  9 12:57:56 2024 ] 
Training: Epoch [98/120], Step [5499], Loss: 0.5939198732376099, Training Accuracy: 96.03863636363637
[ Tue Jul  9 12:57:56 2024 ] 	Batch(5500/7879) done. Loss: 0.2946  lr:0.000001
[ Tue Jul  9 12:58:20 2024 ] 	Batch(5600/7879) done. Loss: 0.0412  lr:0.000001
[ Tue Jul  9 12:58:43 2024 ] 	Batch(5700/7879) done. Loss: 0.0150  lr:0.000001
[ Tue Jul  9 12:59:07 2024 ] 	Batch(5800/7879) done. Loss: 0.2314  lr:0.000001
[ Tue Jul  9 12:59:30 2024 ] 	Batch(5900/7879) done. Loss: 0.0170  lr:0.000001
[ Tue Jul  9 12:59:54 2024 ] 
Training: Epoch [98/120], Step [5999], Loss: 0.030799295753240585, Training Accuracy: 96.07291666666666
[ Tue Jul  9 12:59:54 2024 ] 	Batch(6000/7879) done. Loss: 0.0090  lr:0.000001
[ Tue Jul  9 13:00:18 2024 ] 	Batch(6100/7879) done. Loss: 0.0844  lr:0.000001
[ Tue Jul  9 13:00:42 2024 ] 	Batch(6200/7879) done. Loss: 0.0311  lr:0.000001
[ Tue Jul  9 13:01:05 2024 ] 	Batch(6300/7879) done. Loss: 0.4952  lr:0.000001
[ Tue Jul  9 13:01:29 2024 ] 	Batch(6400/7879) done. Loss: 0.3996  lr:0.000001
[ Tue Jul  9 13:01:52 2024 ] 
Training: Epoch [98/120], Step [6499], Loss: 0.24523380398750305, Training Accuracy: 96.10192307692309
[ Tue Jul  9 13:01:52 2024 ] 	Batch(6500/7879) done. Loss: 0.1725  lr:0.000001
[ Tue Jul  9 13:02:15 2024 ] 	Batch(6600/7879) done. Loss: 0.2627  lr:0.000001
[ Tue Jul  9 13:02:38 2024 ] 	Batch(6700/7879) done. Loss: 0.1493  lr:0.000001
[ Tue Jul  9 13:03:01 2024 ] 	Batch(6800/7879) done. Loss: 0.0497  lr:0.000001
[ Tue Jul  9 13:03:25 2024 ] 	Batch(6900/7879) done. Loss: 0.0261  lr:0.000001
[ Tue Jul  9 13:03:48 2024 ] 
Training: Epoch [98/120], Step [6999], Loss: 0.07693274319171906, Training Accuracy: 96.12857142857143
[ Tue Jul  9 13:03:48 2024 ] 	Batch(7000/7879) done. Loss: 0.3159  lr:0.000001
[ Tue Jul  9 13:04:11 2024 ] 	Batch(7100/7879) done. Loss: 0.0735  lr:0.000001
[ Tue Jul  9 13:04:34 2024 ] 	Batch(7200/7879) done. Loss: 0.0176  lr:0.000001
[ Tue Jul  9 13:04:58 2024 ] 	Batch(7300/7879) done. Loss: 0.2215  lr:0.000001
[ Tue Jul  9 13:05:21 2024 ] 	Batch(7400/7879) done. Loss: 0.6104  lr:0.000001
[ Tue Jul  9 13:05:44 2024 ] 
Training: Epoch [98/120], Step [7499], Loss: 0.12877227365970612, Training Accuracy: 96.125
[ Tue Jul  9 13:05:44 2024 ] 	Batch(7500/7879) done. Loss: 0.1369  lr:0.000001
[ Tue Jul  9 13:06:07 2024 ] 	Batch(7600/7879) done. Loss: 0.1748  lr:0.000001
[ Tue Jul  9 13:06:30 2024 ] 	Batch(7700/7879) done. Loss: 0.1689  lr:0.000001
[ Tue Jul  9 13:06:53 2024 ] 	Batch(7800/7879) done. Loss: 0.6327  lr:0.000001
[ Tue Jul  9 13:07:12 2024 ] 	Mean training loss: 0.1419.
[ Tue Jul  9 13:07:12 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul  9 13:07:12 2024 ] Training epoch: 100
[ Tue Jul  9 13:07:13 2024 ] 	Batch(0/7879) done. Loss: 0.0702  lr:0.000001
[ Tue Jul  9 13:07:35 2024 ] 	Batch(100/7879) done. Loss: 0.3517  lr:0.000001
[ Tue Jul  9 13:07:59 2024 ] 	Batch(200/7879) done. Loss: 0.0960  lr:0.000001
[ Tue Jul  9 13:08:22 2024 ] 	Batch(300/7879) done. Loss: 0.1761  lr:0.000001
[ Tue Jul  9 13:08:46 2024 ] 	Batch(400/7879) done. Loss: 0.0426  lr:0.000001
[ Tue Jul  9 13:09:08 2024 ] 
Training: Epoch [99/120], Step [499], Loss: 0.13355089724063873, Training Accuracy: 96.275
[ Tue Jul  9 13:09:09 2024 ] 	Batch(500/7879) done. Loss: 0.0699  lr:0.000001
[ Tue Jul  9 13:09:31 2024 ] 	Batch(600/7879) done. Loss: 0.1451  lr:0.000001
[ Tue Jul  9 13:09:54 2024 ] 	Batch(700/7879) done. Loss: 0.0639  lr:0.000001
[ Tue Jul  9 13:10:17 2024 ] 	Batch(800/7879) done. Loss: 0.2018  lr:0.000001
[ Tue Jul  9 13:10:39 2024 ] 	Batch(900/7879) done. Loss: 0.1247  lr:0.000001
[ Tue Jul  9 13:11:02 2024 ] 
Training: Epoch [99/120], Step [999], Loss: 0.013097361661493778, Training Accuracy: 96.1125
[ Tue Jul  9 13:11:02 2024 ] 	Batch(1000/7879) done. Loss: 0.3440  lr:0.000001
[ Tue Jul  9 13:11:25 2024 ] 	Batch(1100/7879) done. Loss: 0.3993  lr:0.000001
[ Tue Jul  9 13:11:48 2024 ] 	Batch(1200/7879) done. Loss: 0.3291  lr:0.000001
[ Tue Jul  9 13:12:10 2024 ] 	Batch(1300/7879) done. Loss: 0.0229  lr:0.000001
[ Tue Jul  9 13:12:33 2024 ] 	Batch(1400/7879) done. Loss: 0.0580  lr:0.000001
[ Tue Jul  9 13:12:56 2024 ] 
Training: Epoch [99/120], Step [1499], Loss: 0.20388132333755493, Training Accuracy: 96.05
[ Tue Jul  9 13:12:56 2024 ] 	Batch(1500/7879) done. Loss: 0.2332  lr:0.000001
[ Tue Jul  9 13:13:19 2024 ] 	Batch(1600/7879) done. Loss: 0.0238  lr:0.000001
[ Tue Jul  9 13:13:41 2024 ] 	Batch(1700/7879) done. Loss: 0.1331  lr:0.000001
[ Tue Jul  9 13:14:04 2024 ] 	Batch(1800/7879) done. Loss: 0.0406  lr:0.000001
[ Tue Jul  9 13:14:27 2024 ] 	Batch(1900/7879) done. Loss: 0.1812  lr:0.000001
[ Tue Jul  9 13:14:49 2024 ] 
Training: Epoch [99/120], Step [1999], Loss: 0.1364908218383789, Training Accuracy: 96.13125
[ Tue Jul  9 13:14:50 2024 ] 	Batch(2000/7879) done. Loss: 0.1974  lr:0.000001
[ Tue Jul  9 13:15:12 2024 ] 	Batch(2100/7879) done. Loss: 0.1702  lr:0.000001
[ Tue Jul  9 13:15:35 2024 ] 	Batch(2200/7879) done. Loss: 0.0504  lr:0.000001
[ Tue Jul  9 13:15:58 2024 ] 	Batch(2300/7879) done. Loss: 0.2198  lr:0.000001
[ Tue Jul  9 13:16:21 2024 ] 	Batch(2400/7879) done. Loss: 0.0597  lr:0.000001
[ Tue Jul  9 13:16:44 2024 ] 
Training: Epoch [99/120], Step [2499], Loss: 0.06797099113464355, Training Accuracy: 96.065
[ Tue Jul  9 13:16:44 2024 ] 	Batch(2500/7879) done. Loss: 0.3582  lr:0.000001
[ Tue Jul  9 13:17:07 2024 ] 	Batch(2600/7879) done. Loss: 0.0256  lr:0.000001
[ Tue Jul  9 13:17:29 2024 ] 	Batch(2700/7879) done. Loss: 0.0282  lr:0.000001
[ Tue Jul  9 13:17:52 2024 ] 	Batch(2800/7879) done. Loss: 0.1734  lr:0.000001
[ Tue Jul  9 13:18:15 2024 ] 	Batch(2900/7879) done. Loss: 0.0692  lr:0.000001
[ Tue Jul  9 13:18:37 2024 ] 
Training: Epoch [99/120], Step [2999], Loss: 0.018653875216841698, Training Accuracy: 96.11666666666666
[ Tue Jul  9 13:18:37 2024 ] 	Batch(3000/7879) done. Loss: 0.0849  lr:0.000001
[ Tue Jul  9 13:19:00 2024 ] 	Batch(3100/7879) done. Loss: 0.0133  lr:0.000001
[ Tue Jul  9 13:19:23 2024 ] 	Batch(3200/7879) done. Loss: 0.1802  lr:0.000001
[ Tue Jul  9 13:19:46 2024 ] 	Batch(3300/7879) done. Loss: 0.7009  lr:0.000001
[ Tue Jul  9 13:20:09 2024 ] 	Batch(3400/7879) done. Loss: 0.0313  lr:0.000001
[ Tue Jul  9 13:20:32 2024 ] 
Training: Epoch [99/120], Step [3499], Loss: 0.005823708139359951, Training Accuracy: 96.1
[ Tue Jul  9 13:20:32 2024 ] 	Batch(3500/7879) done. Loss: 0.1312  lr:0.000001
[ Tue Jul  9 13:20:56 2024 ] 	Batch(3600/7879) done. Loss: 0.0344  lr:0.000001
[ Tue Jul  9 13:21:19 2024 ] 	Batch(3700/7879) done. Loss: 0.1778  lr:0.000001
[ Tue Jul  9 13:21:42 2024 ] 	Batch(3800/7879) done. Loss: 0.3162  lr:0.000001
[ Tue Jul  9 13:22:05 2024 ] 	Batch(3900/7879) done. Loss: 0.1220  lr:0.000001
[ Tue Jul  9 13:22:27 2024 ] 
Training: Epoch [99/120], Step [3999], Loss: 0.11590790003538132, Training Accuracy: 96.046875
[ Tue Jul  9 13:22:28 2024 ] 	Batch(4000/7879) done. Loss: 0.5676  lr:0.000001
[ Tue Jul  9 13:22:50 2024 ] 	Batch(4100/7879) done. Loss: 0.2422  lr:0.000001
[ Tue Jul  9 13:23:13 2024 ] 	Batch(4200/7879) done. Loss: 0.0171  lr:0.000001
[ Tue Jul  9 13:23:36 2024 ] 	Batch(4300/7879) done. Loss: 0.0290  lr:0.000001
[ Tue Jul  9 13:23:58 2024 ] 	Batch(4400/7879) done. Loss: 0.0787  lr:0.000001
[ Tue Jul  9 13:24:21 2024 ] 
Training: Epoch [99/120], Step [4499], Loss: 0.1856354922056198, Training Accuracy: 96.01111111111112
[ Tue Jul  9 13:24:21 2024 ] 	Batch(4500/7879) done. Loss: 0.0114  lr:0.000001
[ Tue Jul  9 13:24:44 2024 ] 	Batch(4600/7879) done. Loss: 0.1503  lr:0.000001
[ Tue Jul  9 13:25:07 2024 ] 	Batch(4700/7879) done. Loss: 0.0438  lr:0.000001
[ Tue Jul  9 13:25:29 2024 ] 	Batch(4800/7879) done. Loss: 0.6029  lr:0.000001
[ Tue Jul  9 13:25:53 2024 ] 	Batch(4900/7879) done. Loss: 0.0150  lr:0.000001
[ Tue Jul  9 13:26:16 2024 ] 
Training: Epoch [99/120], Step [4999], Loss: 0.002608611946925521, Training Accuracy: 96.045
[ Tue Jul  9 13:26:16 2024 ] 	Batch(5000/7879) done. Loss: 0.0108  lr:0.000001
[ Tue Jul  9 13:26:40 2024 ] 	Batch(5100/7879) done. Loss: 0.0121  lr:0.000001
[ Tue Jul  9 13:27:03 2024 ] 	Batch(5200/7879) done. Loss: 0.0382  lr:0.000001
[ Tue Jul  9 13:27:26 2024 ] 	Batch(5300/7879) done. Loss: 0.0282  lr:0.000001
[ Tue Jul  9 13:27:49 2024 ] 	Batch(5400/7879) done. Loss: 0.0342  lr:0.000001
[ Tue Jul  9 13:28:12 2024 ] 
Training: Epoch [99/120], Step [5499], Loss: 0.018422331660985947, Training Accuracy: 96.08409090909092
[ Tue Jul  9 13:28:12 2024 ] 	Batch(5500/7879) done. Loss: 0.0348  lr:0.000001
[ Tue Jul  9 13:28:35 2024 ] 	Batch(5600/7879) done. Loss: 0.3286  lr:0.000001
[ Tue Jul  9 13:28:58 2024 ] 	Batch(5700/7879) done. Loss: 0.0180  lr:0.000001
[ Tue Jul  9 13:29:20 2024 ] 	Batch(5800/7879) done. Loss: 0.0011  lr:0.000001
[ Tue Jul  9 13:29:43 2024 ] 	Batch(5900/7879) done. Loss: 0.5253  lr:0.000001
[ Tue Jul  9 13:30:05 2024 ] 
Training: Epoch [99/120], Step [5999], Loss: 0.1861489862203598, Training Accuracy: 96.025
[ Tue Jul  9 13:30:06 2024 ] 	Batch(6000/7879) done. Loss: 0.0207  lr:0.000001
[ Tue Jul  9 13:30:28 2024 ] 	Batch(6100/7879) done. Loss: 0.1891  lr:0.000001
[ Tue Jul  9 13:30:51 2024 ] 	Batch(6200/7879) done. Loss: 0.0741  lr:0.000001
[ Tue Jul  9 13:31:14 2024 ] 	Batch(6300/7879) done. Loss: 0.2784  lr:0.000001
[ Tue Jul  9 13:31:37 2024 ] 	Batch(6400/7879) done. Loss: 0.0048  lr:0.000001
[ Tue Jul  9 13:31:59 2024 ] 
Training: Epoch [99/120], Step [6499], Loss: 0.14604364335536957, Training Accuracy: 96.04230769230769
[ Tue Jul  9 13:31:59 2024 ] 	Batch(6500/7879) done. Loss: 0.1204  lr:0.000001
[ Tue Jul  9 13:32:22 2024 ] 	Batch(6600/7879) done. Loss: 0.1309  lr:0.000001
[ Tue Jul  9 13:32:45 2024 ] 	Batch(6700/7879) done. Loss: 0.0286  lr:0.000001
[ Tue Jul  9 13:33:08 2024 ] 	Batch(6800/7879) done. Loss: 0.1009  lr:0.000001
[ Tue Jul  9 13:33:31 2024 ] 	Batch(6900/7879) done. Loss: 0.4705  lr:0.000001
[ Tue Jul  9 13:33:54 2024 ] 
Training: Epoch [99/120], Step [6999], Loss: 0.1998930722475052, Training Accuracy: 96.09285714285714
[ Tue Jul  9 13:33:55 2024 ] 	Batch(7000/7879) done. Loss: 0.4686  lr:0.000001
[ Tue Jul  9 13:34:18 2024 ] 	Batch(7100/7879) done. Loss: 0.2660  lr:0.000001
[ Tue Jul  9 13:34:42 2024 ] 	Batch(7200/7879) done. Loss: 0.0736  lr:0.000001
[ Tue Jul  9 13:35:04 2024 ] 	Batch(7300/7879) done. Loss: 0.2488  lr:0.000001
[ Tue Jul  9 13:35:27 2024 ] 	Batch(7400/7879) done. Loss: 0.0873  lr:0.000001
[ Tue Jul  9 13:35:50 2024 ] 
Training: Epoch [99/120], Step [7499], Loss: 0.05587495490908623, Training Accuracy: 96.09833333333333
[ Tue Jul  9 13:35:50 2024 ] 	Batch(7500/7879) done. Loss: 0.0171  lr:0.000001
[ Tue Jul  9 13:36:13 2024 ] 	Batch(7600/7879) done. Loss: 0.0042  lr:0.000001
[ Tue Jul  9 13:36:35 2024 ] 	Batch(7700/7879) done. Loss: 0.2333  lr:0.000001
[ Tue Jul  9 13:36:58 2024 ] 	Batch(7800/7879) done. Loss: 0.0613  lr:0.000001
[ Tue Jul  9 13:37:16 2024 ] 	Mean training loss: 0.1420.
[ Tue Jul  9 13:37:16 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 13:37:16 2024 ] Eval epoch: 100
[ Tue Jul  9 13:43:12 2024 ] 	Mean val loss of 6365 batches: 1.0230762327869523.
[ Tue Jul  9 13:43:12 2024 ] 
Validation: Epoch [99/120], Samples [39448.0/50919], Loss: 0.44206181168556213, Validation Accuracy: 77.47206347335965
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 1 : 200 / 275 = 72 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 2 : 228 / 273 = 83 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 3 : 230 / 273 = 84 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 4 : 228 / 275 = 82 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 5 : 231 / 275 = 84 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 6 : 217 / 275 = 78 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 7 : 257 / 273 = 94 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 8 : 265 / 273 = 97 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 9 : 191 / 273 = 69 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 10 : 117 / 273 = 42 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 11 : 150 / 272 = 55 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 12 : 221 / 271 = 81 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 13 : 265 / 275 = 96 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 14 : 263 / 276 = 95 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 15 : 211 / 273 = 77 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 16 : 165 / 274 = 60 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 17 : 237 / 273 = 86 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 18 : 235 / 274 = 85 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 19 : 242 / 272 = 88 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 20 : 253 / 273 = 92 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 21 : 233 / 274 = 85 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 22 : 251 / 274 = 91 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 23 : 250 / 276 = 90 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 24 : 246 / 274 = 89 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 25 : 262 / 275 = 95 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 26 : 268 / 276 = 97 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 27 : 229 / 275 = 83 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 28 : 185 / 275 = 67 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 29 : 148 / 275 = 53 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 30 : 173 / 276 = 62 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 31 : 236 / 276 = 85 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 32 : 241 / 276 = 87 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 33 : 232 / 276 = 84 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 34 : 240 / 276 = 86 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 35 : 241 / 275 = 87 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 36 : 224 / 276 = 81 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 37 : 253 / 276 = 91 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 38 : 243 / 276 = 88 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 39 : 235 / 276 = 85 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 40 : 208 / 276 = 75 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 41 : 263 / 276 = 95 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 42 : 253 / 275 = 92 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 43 : 176 / 276 = 63 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 44 : 253 / 276 = 91 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 45 : 257 / 276 = 93 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 46 : 225 / 276 = 81 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 47 : 199 / 275 = 72 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 48 : 230 / 275 = 83 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 49 : 224 / 274 = 81 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 50 : 238 / 276 = 86 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 51 : 254 / 276 = 92 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 52 : 249 / 276 = 90 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 53 : 241 / 276 = 87 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 54 : 260 / 274 = 94 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 55 : 238 / 276 = 86 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 56 : 251 / 275 = 91 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 57 : 269 / 276 = 97 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 58 : 268 / 273 = 98 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 59 : 254 / 276 = 92 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 60 : 473 / 561 = 84 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 61 : 477 / 566 = 84 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 62 : 404 / 572 = 70 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 63 : 483 / 570 = 84 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 64 : 429 / 574 = 74 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 65 : 495 / 573 = 86 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 66 : 400 / 573 = 69 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 67 : 397 / 575 = 69 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 68 : 364 / 575 = 63 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 69 : 480 / 575 = 83 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 70 : 222 / 575 = 38 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 71 : 250 / 575 = 43 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 72 : 97 / 571 = 16 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 73 : 252 / 570 = 44 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 74 : 340 / 569 = 59 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 75 : 358 / 573 = 62 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 76 : 374 / 574 = 65 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 77 : 374 / 573 = 65 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 78 : 447 / 575 = 77 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 79 : 543 / 574 = 94 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 80 : 460 / 573 = 80 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 81 : 319 / 575 = 55 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 82 : 347 / 575 = 60 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 83 : 264 / 572 = 46 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 84 : 430 / 574 = 74 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 85 : 359 / 574 = 62 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 86 : 509 / 575 = 88 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 87 : 495 / 576 = 85 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 88 : 397 / 575 = 69 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 89 : 453 / 576 = 78 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 90 : 261 / 574 = 45 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 91 : 455 / 568 = 80 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 92 : 410 / 576 = 71 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 93 : 378 / 573 = 65 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 94 : 519 / 574 = 90 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 95 : 535 / 575 = 93 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 96 : 558 / 575 = 97 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 97 : 552 / 574 = 96 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 98 : 536 / 575 = 93 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 99 : 531 / 574 = 92 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 100 : 467 / 574 = 81 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 101 : 531 / 574 = 92 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 102 : 326 / 575 = 56 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 103 : 494 / 576 = 85 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 104 : 277 / 575 = 48 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 105 : 292 / 575 = 50 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 106 : 305 / 576 = 52 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 107 : 509 / 576 = 88 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 108 : 485 / 575 = 84 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 109 : 409 / 575 = 71 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 110 : 523 / 575 = 90 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 111 : 541 / 576 = 93 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 112 : 541 / 575 = 94 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 113 : 520 / 576 = 90 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 114 : 492 / 576 = 85 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 115 : 524 / 576 = 90 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 116 : 484 / 575 = 84 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 117 : 471 / 575 = 81 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 118 : 474 / 575 = 82 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 119 : 506 / 576 = 87 %
[ Tue Jul  9 13:43:12 2024 ] Accuracy of 120 : 244 / 274 = 89 %
[ Tue Jul  9 13:43:12 2024 ] Training epoch: 101
[ Tue Jul  9 13:43:13 2024 ] 	Batch(0/7879) done. Loss: 0.3562  lr:0.000001
[ Tue Jul  9 13:43:36 2024 ] 	Batch(100/7879) done. Loss: 0.0041  lr:0.000001
[ Tue Jul  9 13:43:59 2024 ] 	Batch(200/7879) done. Loss: 0.0837  lr:0.000001
[ Tue Jul  9 13:44:21 2024 ] 	Batch(300/7879) done. Loss: 0.3726  lr:0.000001
[ Tue Jul  9 13:44:44 2024 ] 	Batch(400/7879) done. Loss: 0.0557  lr:0.000001
[ Tue Jul  9 13:45:06 2024 ] 
Training: Epoch [100/120], Step [499], Loss: 0.07783011347055435, Training Accuracy: 95.95
[ Tue Jul  9 13:45:07 2024 ] 	Batch(500/7879) done. Loss: 0.0297  lr:0.000001
[ Tue Jul  9 13:45:29 2024 ] 	Batch(600/7879) done. Loss: 0.3737  lr:0.000001
[ Tue Jul  9 13:45:52 2024 ] 	Batch(700/7879) done. Loss: 0.1810  lr:0.000001
[ Tue Jul  9 13:46:15 2024 ] 	Batch(800/7879) done. Loss: 0.0035  lr:0.000001
[ Tue Jul  9 13:46:37 2024 ] 	Batch(900/7879) done. Loss: 0.0044  lr:0.000001
[ Tue Jul  9 13:47:00 2024 ] 
Training: Epoch [100/120], Step [999], Loss: 0.012318714521825314, Training Accuracy: 96.2
[ Tue Jul  9 13:47:00 2024 ] 	Batch(1000/7879) done. Loss: 0.2632  lr:0.000001
[ Tue Jul  9 13:47:24 2024 ] 	Batch(1100/7879) done. Loss: 0.1067  lr:0.000001
[ Tue Jul  9 13:47:47 2024 ] 	Batch(1200/7879) done. Loss: 0.0129  lr:0.000001
[ Tue Jul  9 13:48:11 2024 ] 	Batch(1300/7879) done. Loss: 0.2307  lr:0.000001
[ Tue Jul  9 13:48:34 2024 ] 	Batch(1400/7879) done. Loss: 0.5179  lr:0.000001
[ Tue Jul  9 13:48:57 2024 ] 
Training: Epoch [100/120], Step [1499], Loss: 0.012377187609672546, Training Accuracy: 96.30833333333332
[ Tue Jul  9 13:48:58 2024 ] 	Batch(1500/7879) done. Loss: 0.0334  lr:0.000001
[ Tue Jul  9 13:49:21 2024 ] 	Batch(1600/7879) done. Loss: 0.3677  lr:0.000001
[ Tue Jul  9 13:49:44 2024 ] 	Batch(1700/7879) done. Loss: 0.3732  lr:0.000001
[ Tue Jul  9 13:50:07 2024 ] 	Batch(1800/7879) done. Loss: 0.0812  lr:0.000001
[ Tue Jul  9 13:50:29 2024 ] 	Batch(1900/7879) done. Loss: 0.1968  lr:0.000001
[ Tue Jul  9 13:50:52 2024 ] 
Training: Epoch [100/120], Step [1999], Loss: 0.025472290813922882, Training Accuracy: 96.1875
[ Tue Jul  9 13:50:52 2024 ] 	Batch(2000/7879) done. Loss: 0.1143  lr:0.000001
[ Tue Jul  9 13:51:15 2024 ] 	Batch(2100/7879) done. Loss: 0.2322  lr:0.000001
[ Tue Jul  9 13:51:38 2024 ] 	Batch(2200/7879) done. Loss: 0.0081  lr:0.000001
[ Tue Jul  9 13:52:00 2024 ] 	Batch(2300/7879) done. Loss: 0.0822  lr:0.000001
[ Tue Jul  9 13:52:23 2024 ] 	Batch(2400/7879) done. Loss: 0.2318  lr:0.000001
[ Tue Jul  9 13:52:46 2024 ] 
Training: Epoch [100/120], Step [2499], Loss: 0.0711979940533638, Training Accuracy: 96.205
[ Tue Jul  9 13:52:46 2024 ] 	Batch(2500/7879) done. Loss: 0.2745  lr:0.000001
[ Tue Jul  9 13:53:09 2024 ] 	Batch(2600/7879) done. Loss: 0.2927  lr:0.000001
[ Tue Jul  9 13:53:31 2024 ] 	Batch(2700/7879) done. Loss: 0.4818  lr:0.000001
[ Tue Jul  9 13:53:54 2024 ] 	Batch(2800/7879) done. Loss: 0.0827  lr:0.000001
[ Tue Jul  9 13:54:17 2024 ] 	Batch(2900/7879) done. Loss: 0.2754  lr:0.000001
[ Tue Jul  9 13:54:39 2024 ] 
Training: Epoch [100/120], Step [2999], Loss: 0.011893384158611298, Training Accuracy: 96.1375
[ Tue Jul  9 13:54:39 2024 ] 	Batch(3000/7879) done. Loss: 0.0303  lr:0.000001
[ Tue Jul  9 13:55:02 2024 ] 	Batch(3100/7879) done. Loss: 0.6048  lr:0.000001
[ Tue Jul  9 13:55:25 2024 ] 	Batch(3200/7879) done. Loss: 0.0269  lr:0.000001
[ Tue Jul  9 13:55:49 2024 ] 	Batch(3300/7879) done. Loss: 0.0242  lr:0.000001
[ Tue Jul  9 13:56:12 2024 ] 	Batch(3400/7879) done. Loss: 0.0736  lr:0.000001
[ Tue Jul  9 13:56:35 2024 ] 
Training: Epoch [100/120], Step [3499], Loss: 0.15183955430984497, Training Accuracy: 96.2
[ Tue Jul  9 13:56:35 2024 ] 	Batch(3500/7879) done. Loss: 0.4975  lr:0.000001
[ Tue Jul  9 13:56:59 2024 ] 	Batch(3600/7879) done. Loss: 0.2526  lr:0.000001
[ Tue Jul  9 13:57:22 2024 ] 	Batch(3700/7879) done. Loss: 0.0979  lr:0.000001
[ Tue Jul  9 13:57:45 2024 ] 	Batch(3800/7879) done. Loss: 0.1617  lr:0.000001
[ Tue Jul  9 13:58:08 2024 ] 	Batch(3900/7879) done. Loss: 0.2244  lr:0.000001
[ Tue Jul  9 13:58:30 2024 ] 
Training: Epoch [100/120], Step [3999], Loss: 0.19407866895198822, Training Accuracy: 96.24374999999999
[ Tue Jul  9 13:58:31 2024 ] 	Batch(4000/7879) done. Loss: 0.0349  lr:0.000001
[ Tue Jul  9 13:58:54 2024 ] 	Batch(4100/7879) done. Loss: 0.0486  lr:0.000001
[ Tue Jul  9 13:59:16 2024 ] 	Batch(4200/7879) done. Loss: 0.0278  lr:0.000001
[ Tue Jul  9 13:59:39 2024 ] 	Batch(4300/7879) done. Loss: 0.0756  lr:0.000001
[ Tue Jul  9 14:00:02 2024 ] 	Batch(4400/7879) done. Loss: 0.0070  lr:0.000001
[ Tue Jul  9 14:00:24 2024 ] 
Training: Epoch [100/120], Step [4499], Loss: 0.01215333491563797, Training Accuracy: 96.29722222222222
[ Tue Jul  9 14:00:25 2024 ] 	Batch(4500/7879) done. Loss: 0.0383  lr:0.000001
[ Tue Jul  9 14:00:47 2024 ] 	Batch(4600/7879) done. Loss: 0.0050  lr:0.000001
[ Tue Jul  9 14:01:10 2024 ] 	Batch(4700/7879) done. Loss: 0.1181  lr:0.000001
[ Tue Jul  9 14:01:33 2024 ] 	Batch(4800/7879) done. Loss: 0.0814  lr:0.000001
[ Tue Jul  9 14:01:55 2024 ] 	Batch(4900/7879) done. Loss: 0.0245  lr:0.000001
[ Tue Jul  9 14:02:18 2024 ] 
Training: Epoch [100/120], Step [4999], Loss: 0.2313004732131958, Training Accuracy: 96.265
[ Tue Jul  9 14:02:18 2024 ] 	Batch(5000/7879) done. Loss: 0.0429  lr:0.000001
[ Tue Jul  9 14:02:41 2024 ] 	Batch(5100/7879) done. Loss: 0.0404  lr:0.000001
[ Tue Jul  9 14:03:04 2024 ] 	Batch(5200/7879) done. Loss: 0.0589  lr:0.000001
[ Tue Jul  9 14:03:26 2024 ] 	Batch(5300/7879) done. Loss: 0.3858  lr:0.000001
[ Tue Jul  9 14:03:49 2024 ] 	Batch(5400/7879) done. Loss: 0.2477  lr:0.000001
[ Tue Jul  9 14:04:12 2024 ] 
Training: Epoch [100/120], Step [5499], Loss: 0.7016274333000183, Training Accuracy: 96.20681818181818
[ Tue Jul  9 14:04:12 2024 ] 	Batch(5500/7879) done. Loss: 0.2442  lr:0.000001
[ Tue Jul  9 14:04:35 2024 ] 	Batch(5600/7879) done. Loss: 0.0115  lr:0.000001
[ Tue Jul  9 14:04:57 2024 ] 	Batch(5700/7879) done. Loss: 0.2746  lr:0.000001
[ Tue Jul  9 14:05:20 2024 ] 	Batch(5800/7879) done. Loss: 0.0157  lr:0.000001
[ Tue Jul  9 14:05:43 2024 ] 	Batch(5900/7879) done. Loss: 0.0126  lr:0.000001
[ Tue Jul  9 14:06:05 2024 ] 
Training: Epoch [100/120], Step [5999], Loss: 0.03404766693711281, Training Accuracy: 96.20416666666667
[ Tue Jul  9 14:06:06 2024 ] 	Batch(6000/7879) done. Loss: 0.0280  lr:0.000001
[ Tue Jul  9 14:06:28 2024 ] 	Batch(6100/7879) done. Loss: 0.0916  lr:0.000001
[ Tue Jul  9 14:06:51 2024 ] 	Batch(6200/7879) done. Loss: 0.0391  lr:0.000001
[ Tue Jul  9 14:07:14 2024 ] 	Batch(6300/7879) done. Loss: 0.3484  lr:0.000001
[ Tue Jul  9 14:07:37 2024 ] 	Batch(6400/7879) done. Loss: 0.6582  lr:0.000001
[ Tue Jul  9 14:07:59 2024 ] 
Training: Epoch [100/120], Step [6499], Loss: 0.020300935953855515, Training Accuracy: 96.15769230769232
[ Tue Jul  9 14:07:59 2024 ] 	Batch(6500/7879) done. Loss: 0.0055  lr:0.000001
[ Tue Jul  9 14:08:22 2024 ] 	Batch(6600/7879) done. Loss: 0.4380  lr:0.000001
[ Tue Jul  9 14:08:45 2024 ] 	Batch(6700/7879) done. Loss: 0.1027  lr:0.000001
[ Tue Jul  9 14:09:08 2024 ] 	Batch(6800/7879) done. Loss: 0.0546  lr:0.000001
[ Tue Jul  9 14:09:31 2024 ] 	Batch(6900/7879) done. Loss: 0.2621  lr:0.000001
[ Tue Jul  9 14:09:53 2024 ] 
Training: Epoch [100/120], Step [6999], Loss: 0.5990427732467651, Training Accuracy: 96.12678571428572
[ Tue Jul  9 14:09:54 2024 ] 	Batch(7000/7879) done. Loss: 0.0058  lr:0.000001
[ Tue Jul  9 14:10:16 2024 ] 	Batch(7100/7879) done. Loss: 0.0555  lr:0.000001
[ Tue Jul  9 14:10:39 2024 ] 	Batch(7200/7879) done. Loss: 0.0917  lr:0.000001
[ Tue Jul  9 14:11:02 2024 ] 	Batch(7300/7879) done. Loss: 0.6871  lr:0.000001
[ Tue Jul  9 14:11:25 2024 ] 	Batch(7400/7879) done. Loss: 0.2480  lr:0.000001
[ Tue Jul  9 14:11:47 2024 ] 
Training: Epoch [100/120], Step [7499], Loss: 0.20360523462295532, Training Accuracy: 96.11333333333333
[ Tue Jul  9 14:11:47 2024 ] 	Batch(7500/7879) done. Loss: 0.5031  lr:0.000001
[ Tue Jul  9 14:12:10 2024 ] 	Batch(7600/7879) done. Loss: 0.0043  lr:0.000001
[ Tue Jul  9 14:12:33 2024 ] 	Batch(7700/7879) done. Loss: 0.1491  lr:0.000001
[ Tue Jul  9 14:12:56 2024 ] 	Batch(7800/7879) done. Loss: 0.0767  lr:0.000001
[ Tue Jul  9 14:13:13 2024 ] 	Mean training loss: 0.1458.
[ Tue Jul  9 14:13:13 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 14:13:13 2024 ] Training epoch: 102
[ Tue Jul  9 14:13:14 2024 ] 	Batch(0/7879) done. Loss: 0.0377  lr:0.000001
[ Tue Jul  9 14:13:37 2024 ] 	Batch(100/7879) done. Loss: 0.0279  lr:0.000001
[ Tue Jul  9 14:14:00 2024 ] 	Batch(200/7879) done. Loss: 0.0080  lr:0.000001
[ Tue Jul  9 14:14:23 2024 ] 	Batch(300/7879) done. Loss: 0.0247  lr:0.000001
[ Tue Jul  9 14:14:47 2024 ] 	Batch(400/7879) done. Loss: 0.3794  lr:0.000001
[ Tue Jul  9 14:15:09 2024 ] 
Training: Epoch [101/120], Step [499], Loss: 0.09105226397514343, Training Accuracy: 95.95
[ Tue Jul  9 14:15:10 2024 ] 	Batch(500/7879) done. Loss: 0.0182  lr:0.000001
[ Tue Jul  9 14:15:33 2024 ] 	Batch(600/7879) done. Loss: 0.1381  lr:0.000001
[ Tue Jul  9 14:15:56 2024 ] 	Batch(700/7879) done. Loss: 0.2475  lr:0.000001
[ Tue Jul  9 14:16:19 2024 ] 	Batch(800/7879) done. Loss: 0.2183  lr:0.000001
[ Tue Jul  9 14:16:42 2024 ] 	Batch(900/7879) done. Loss: 0.0139  lr:0.000001
[ Tue Jul  9 14:17:05 2024 ] 
Training: Epoch [101/120], Step [999], Loss: 0.04310229420661926, Training Accuracy: 96.125
[ Tue Jul  9 14:17:05 2024 ] 	Batch(1000/7879) done. Loss: 0.1970  lr:0.000001
[ Tue Jul  9 14:17:28 2024 ] 	Batch(1100/7879) done. Loss: 0.4706  lr:0.000001
[ Tue Jul  9 14:17:52 2024 ] 	Batch(1200/7879) done. Loss: 0.0145  lr:0.000001
[ Tue Jul  9 14:18:15 2024 ] 	Batch(1300/7879) done. Loss: 0.0047  lr:0.000001
[ Tue Jul  9 14:18:38 2024 ] 	Batch(1400/7879) done. Loss: 0.0592  lr:0.000001
[ Tue Jul  9 14:19:01 2024 ] 
Training: Epoch [101/120], Step [1499], Loss: 0.28259724378585815, Training Accuracy: 96.05
[ Tue Jul  9 14:19:01 2024 ] 	Batch(1500/7879) done. Loss: 0.0304  lr:0.000001
[ Tue Jul  9 14:19:24 2024 ] 	Batch(1600/7879) done. Loss: 0.0127  lr:0.000001
[ Tue Jul  9 14:19:47 2024 ] 	Batch(1700/7879) done. Loss: 0.0100  lr:0.000001
[ Tue Jul  9 14:20:10 2024 ] 	Batch(1800/7879) done. Loss: 0.1791  lr:0.000001
[ Tue Jul  9 14:20:33 2024 ] 	Batch(1900/7879) done. Loss: 0.0022  lr:0.000001
[ Tue Jul  9 14:20:56 2024 ] 
Training: Epoch [101/120], Step [1999], Loss: 0.036785755306482315, Training Accuracy: 96.1
[ Tue Jul  9 14:20:56 2024 ] 	Batch(2000/7879) done. Loss: 0.2305  lr:0.000001
[ Tue Jul  9 14:21:20 2024 ] 	Batch(2100/7879) done. Loss: 0.2313  lr:0.000001
[ Tue Jul  9 14:21:43 2024 ] 	Batch(2200/7879) done. Loss: 0.1466  lr:0.000001
[ Tue Jul  9 14:22:06 2024 ] 	Batch(2300/7879) done. Loss: 0.1816  lr:0.000001
[ Tue Jul  9 14:22:29 2024 ] 	Batch(2400/7879) done. Loss: 0.0024  lr:0.000001
[ Tue Jul  9 14:22:52 2024 ] 
Training: Epoch [101/120], Step [2499], Loss: 0.24073362350463867, Training Accuracy: 96.025
[ Tue Jul  9 14:22:52 2024 ] 	Batch(2500/7879) done. Loss: 0.0004  lr:0.000001
[ Tue Jul  9 14:23:15 2024 ] 	Batch(2600/7879) done. Loss: 0.0726  lr:0.000001
[ Tue Jul  9 14:23:39 2024 ] 	Batch(2700/7879) done. Loss: 0.1554  lr:0.000001
[ Tue Jul  9 14:24:02 2024 ] 	Batch(2800/7879) done. Loss: 0.0643  lr:0.000001
[ Tue Jul  9 14:24:25 2024 ] 	Batch(2900/7879) done. Loss: 0.4175  lr:0.000001
[ Tue Jul  9 14:24:47 2024 ] 
Training: Epoch [101/120], Step [2999], Loss: 0.46296486258506775, Training Accuracy: 96.08333333333333
[ Tue Jul  9 14:24:47 2024 ] 	Batch(3000/7879) done. Loss: 0.0249  lr:0.000001
[ Tue Jul  9 14:25:10 2024 ] 	Batch(3100/7879) done. Loss: 0.0143  lr:0.000001
[ Tue Jul  9 14:25:33 2024 ] 	Batch(3200/7879) done. Loss: 0.1225  lr:0.000001
[ Tue Jul  9 14:25:55 2024 ] 	Batch(3300/7879) done. Loss: 0.0605  lr:0.000001
[ Tue Jul  9 14:26:18 2024 ] 	Batch(3400/7879) done. Loss: 0.0021  lr:0.000001
[ Tue Jul  9 14:26:41 2024 ] 
Training: Epoch [101/120], Step [3499], Loss: 0.3056795299053192, Training Accuracy: 96.16428571428571
[ Tue Jul  9 14:26:41 2024 ] 	Batch(3500/7879) done. Loss: 0.1348  lr:0.000001
[ Tue Jul  9 14:27:04 2024 ] 	Batch(3600/7879) done. Loss: 0.2788  lr:0.000001
[ Tue Jul  9 14:27:26 2024 ] 	Batch(3700/7879) done. Loss: 0.0955  lr:0.000001
[ Tue Jul  9 14:27:49 2024 ] 	Batch(3800/7879) done. Loss: 0.0367  lr:0.000001
[ Tue Jul  9 14:28:12 2024 ] 	Batch(3900/7879) done. Loss: 0.0260  lr:0.000001
[ Tue Jul  9 14:28:34 2024 ] 
Training: Epoch [101/120], Step [3999], Loss: 0.025775456801056862, Training Accuracy: 96.203125
[ Tue Jul  9 14:28:34 2024 ] 	Batch(4000/7879) done. Loss: 0.2837  lr:0.000001
[ Tue Jul  9 14:28:58 2024 ] 	Batch(4100/7879) done. Loss: 0.2117  lr:0.000001
[ Tue Jul  9 14:29:21 2024 ] 	Batch(4200/7879) done. Loss: 0.0545  lr:0.000001
[ Tue Jul  9 14:29:45 2024 ] 	Batch(4300/7879) done. Loss: 0.0217  lr:0.000001
[ Tue Jul  9 14:30:08 2024 ] 	Batch(4400/7879) done. Loss: 0.5203  lr:0.000001
[ Tue Jul  9 14:30:32 2024 ] 
Training: Epoch [101/120], Step [4499], Loss: 0.3961425721645355, Training Accuracy: 96.18333333333334
[ Tue Jul  9 14:30:32 2024 ] 	Batch(4500/7879) done. Loss: 0.0134  lr:0.000001
[ Tue Jul  9 14:30:55 2024 ] 	Batch(4600/7879) done. Loss: 0.0927  lr:0.000001
[ Tue Jul  9 14:31:19 2024 ] 	Batch(4700/7879) done. Loss: 0.0806  lr:0.000001
[ Tue Jul  9 14:31:42 2024 ] 	Batch(4800/7879) done. Loss: 0.0049  lr:0.000001
[ Tue Jul  9 14:32:05 2024 ] 	Batch(4900/7879) done. Loss: 0.1952  lr:0.000001
[ Tue Jul  9 14:32:28 2024 ] 
Training: Epoch [101/120], Step [4999], Loss: 0.1296442747116089, Training Accuracy: 96.1475
[ Tue Jul  9 14:32:28 2024 ] 	Batch(5000/7879) done. Loss: 0.0423  lr:0.000001
[ Tue Jul  9 14:32:51 2024 ] 	Batch(5100/7879) done. Loss: 0.2768  lr:0.000001
[ Tue Jul  9 14:33:13 2024 ] 	Batch(5200/7879) done. Loss: 0.0415  lr:0.000001
[ Tue Jul  9 14:33:37 2024 ] 	Batch(5300/7879) done. Loss: 0.2610  lr:0.000001
[ Tue Jul  9 14:34:00 2024 ] 	Batch(5400/7879) done. Loss: 0.1640  lr:0.000001
[ Tue Jul  9 14:34:23 2024 ] 
Training: Epoch [101/120], Step [5499], Loss: 0.09170777350664139, Training Accuracy: 96.17727272727272
[ Tue Jul  9 14:34:23 2024 ] 	Batch(5500/7879) done. Loss: 0.2248  lr:0.000001
[ Tue Jul  9 14:34:46 2024 ] 	Batch(5600/7879) done. Loss: 0.4505  lr:0.000001
[ Tue Jul  9 14:35:08 2024 ] 	Batch(5700/7879) done. Loss: 0.0174  lr:0.000001
[ Tue Jul  9 14:35:31 2024 ] 	Batch(5800/7879) done. Loss: 0.3689  lr:0.000001
[ Tue Jul  9 14:35:54 2024 ] 	Batch(5900/7879) done. Loss: 0.0474  lr:0.000001
[ Tue Jul  9 14:36:16 2024 ] 
Training: Epoch [101/120], Step [5999], Loss: 0.15692396461963654, Training Accuracy: 96.15625
[ Tue Jul  9 14:36:17 2024 ] 	Batch(6000/7879) done. Loss: 0.1446  lr:0.000001
[ Tue Jul  9 14:36:39 2024 ] 	Batch(6100/7879) done. Loss: 0.1970  lr:0.000001
[ Tue Jul  9 14:37:02 2024 ] 	Batch(6200/7879) done. Loss: 0.2150  lr:0.000001
[ Tue Jul  9 14:37:25 2024 ] 	Batch(6300/7879) done. Loss: 0.0172  lr:0.000001
[ Tue Jul  9 14:37:48 2024 ] 	Batch(6400/7879) done. Loss: 0.2147  lr:0.000001
[ Tue Jul  9 14:38:10 2024 ] 
Training: Epoch [101/120], Step [6499], Loss: 0.1622306853532791, Training Accuracy: 96.1826923076923
[ Tue Jul  9 14:38:10 2024 ] 	Batch(6500/7879) done. Loss: 0.0015  lr:0.000001
[ Tue Jul  9 14:38:33 2024 ] 	Batch(6600/7879) done. Loss: 0.3500  lr:0.000001
[ Tue Jul  9 14:38:56 2024 ] 	Batch(6700/7879) done. Loss: 0.0404  lr:0.000001
[ Tue Jul  9 14:39:18 2024 ] 	Batch(6800/7879) done. Loss: 0.0741  lr:0.000001
[ Tue Jul  9 14:39:41 2024 ] 	Batch(6900/7879) done. Loss: 0.0123  lr:0.000001
[ Tue Jul  9 14:40:04 2024 ] 
Training: Epoch [101/120], Step [6999], Loss: 0.19426698982715607, Training Accuracy: 96.16964285714286
[ Tue Jul  9 14:40:04 2024 ] 	Batch(7000/7879) done. Loss: 0.0509  lr:0.000001
[ Tue Jul  9 14:40:27 2024 ] 	Batch(7100/7879) done. Loss: 0.2759  lr:0.000001
[ Tue Jul  9 14:40:50 2024 ] 	Batch(7200/7879) done. Loss: 0.0279  lr:0.000001
[ Tue Jul  9 14:41:12 2024 ] 	Batch(7300/7879) done. Loss: 0.0079  lr:0.000001
[ Tue Jul  9 14:41:35 2024 ] 	Batch(7400/7879) done. Loss: 0.1132  lr:0.000001
[ Tue Jul  9 14:41:58 2024 ] 
Training: Epoch [101/120], Step [7499], Loss: 0.05349814519286156, Training Accuracy: 96.18333333333334
[ Tue Jul  9 14:41:58 2024 ] 	Batch(7500/7879) done. Loss: 0.5331  lr:0.000001
[ Tue Jul  9 14:42:21 2024 ] 	Batch(7600/7879) done. Loss: 0.2661  lr:0.000001
[ Tue Jul  9 14:42:43 2024 ] 	Batch(7700/7879) done. Loss: 0.0451  lr:0.000001
[ Tue Jul  9 14:43:06 2024 ] 	Batch(7800/7879) done. Loss: 0.0666  lr:0.000001
[ Tue Jul  9 14:43:24 2024 ] 	Mean training loss: 0.1438.
[ Tue Jul  9 14:43:24 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul  9 14:43:24 2024 ] Training epoch: 103
[ Tue Jul  9 14:43:25 2024 ] 	Batch(0/7879) done. Loss: 0.0645  lr:0.000001
[ Tue Jul  9 14:43:47 2024 ] 	Batch(100/7879) done. Loss: 0.0497  lr:0.000001
[ Tue Jul  9 14:44:10 2024 ] 	Batch(200/7879) done. Loss: 0.1415  lr:0.000001
[ Tue Jul  9 14:44:33 2024 ] 	Batch(300/7879) done. Loss: 0.1432  lr:0.000001
[ Tue Jul  9 14:44:56 2024 ] 	Batch(400/7879) done. Loss: 0.0566  lr:0.000001
[ Tue Jul  9 14:45:18 2024 ] 
Training: Epoch [102/120], Step [499], Loss: 0.06648502498865128, Training Accuracy: 95.92500000000001
[ Tue Jul  9 14:45:18 2024 ] 	Batch(500/7879) done. Loss: 0.1117  lr:0.000001
[ Tue Jul  9 14:45:41 2024 ] 	Batch(600/7879) done. Loss: 0.0286  lr:0.000001
[ Tue Jul  9 14:46:04 2024 ] 	Batch(700/7879) done. Loss: 0.0776  lr:0.000001
[ Tue Jul  9 14:46:26 2024 ] 	Batch(800/7879) done. Loss: 0.0035  lr:0.000001
[ Tue Jul  9 14:46:50 2024 ] 	Batch(900/7879) done. Loss: 0.0660  lr:0.000001
[ Tue Jul  9 14:47:12 2024 ] 
Training: Epoch [102/120], Step [999], Loss: 0.005355463828891516, Training Accuracy: 95.8875
[ Tue Jul  9 14:47:12 2024 ] 	Batch(1000/7879) done. Loss: 0.1112  lr:0.000001
[ Tue Jul  9 14:47:35 2024 ] 	Batch(1100/7879) done. Loss: 0.0535  lr:0.000001
[ Tue Jul  9 14:47:58 2024 ] 	Batch(1200/7879) done. Loss: 0.0179  lr:0.000001
[ Tue Jul  9 14:48:21 2024 ] 	Batch(1300/7879) done. Loss: 0.3155  lr:0.000001
[ Tue Jul  9 14:48:43 2024 ] 	Batch(1400/7879) done. Loss: 0.0214  lr:0.000001
[ Tue Jul  9 14:49:06 2024 ] 
Training: Epoch [102/120], Step [1499], Loss: 0.27157992124557495, Training Accuracy: 96.04166666666667
[ Tue Jul  9 14:49:06 2024 ] 	Batch(1500/7879) done. Loss: 1.2821  lr:0.000001
[ Tue Jul  9 14:49:29 2024 ] 	Batch(1600/7879) done. Loss: 0.0084  lr:0.000001
[ Tue Jul  9 14:49:52 2024 ] 	Batch(1700/7879) done. Loss: 0.0260  lr:0.000001
[ Tue Jul  9 14:50:15 2024 ] 	Batch(1800/7879) done. Loss: 0.0589  lr:0.000001
[ Tue Jul  9 14:50:38 2024 ] 	Batch(1900/7879) done. Loss: 0.1410  lr:0.000001
[ Tue Jul  9 14:51:00 2024 ] 
Training: Epoch [102/120], Step [1999], Loss: 0.06952832639217377, Training Accuracy: 96.03125
[ Tue Jul  9 14:51:00 2024 ] 	Batch(2000/7879) done. Loss: 0.1441  lr:0.000001
[ Tue Jul  9 14:51:24 2024 ] 	Batch(2100/7879) done. Loss: 0.3193  lr:0.000001
[ Tue Jul  9 14:51:46 2024 ] 	Batch(2200/7879) done. Loss: 0.1352  lr:0.000001
[ Tue Jul  9 14:52:09 2024 ] 	Batch(2300/7879) done. Loss: 0.1706  lr:0.000001
[ Tue Jul  9 14:52:32 2024 ] 	Batch(2400/7879) done. Loss: 0.0373  lr:0.000001
[ Tue Jul  9 14:52:55 2024 ] 
Training: Epoch [102/120], Step [2499], Loss: 0.3009468615055084, Training Accuracy: 96.0
[ Tue Jul  9 14:52:55 2024 ] 	Batch(2500/7879) done. Loss: 0.0247  lr:0.000001
[ Tue Jul  9 14:53:19 2024 ] 	Batch(2600/7879) done. Loss: 0.0839  lr:0.000001
[ Tue Jul  9 14:53:42 2024 ] 	Batch(2700/7879) done. Loss: 0.0048  lr:0.000001
[ Tue Jul  9 14:54:06 2024 ] 	Batch(2800/7879) done. Loss: 0.3851  lr:0.000001
[ Tue Jul  9 14:54:28 2024 ] 	Batch(2900/7879) done. Loss: 0.1601  lr:0.000001
[ Tue Jul  9 14:54:51 2024 ] 
Training: Epoch [102/120], Step [2999], Loss: 0.03385003283619881, Training Accuracy: 95.97083333333333
[ Tue Jul  9 14:54:51 2024 ] 	Batch(3000/7879) done. Loss: 0.1348  lr:0.000001
[ Tue Jul  9 14:55:14 2024 ] 	Batch(3100/7879) done. Loss: 0.0166  lr:0.000001
[ Tue Jul  9 14:55:37 2024 ] 	Batch(3200/7879) done. Loss: 0.0298  lr:0.000001
[ Tue Jul  9 14:55:59 2024 ] 	Batch(3300/7879) done. Loss: 0.2141  lr:0.000001
[ Tue Jul  9 14:56:22 2024 ] 	Batch(3400/7879) done. Loss: 0.1799  lr:0.000001
[ Tue Jul  9 14:56:44 2024 ] 
Training: Epoch [102/120], Step [3499], Loss: 0.0481453537940979, Training Accuracy: 96.00714285714285
[ Tue Jul  9 14:56:45 2024 ] 	Batch(3500/7879) done. Loss: 0.2298  lr:0.000001
[ Tue Jul  9 14:57:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0147  lr:0.000001
[ Tue Jul  9 14:57:30 2024 ] 	Batch(3700/7879) done. Loss: 0.1003  lr:0.000001
[ Tue Jul  9 14:57:53 2024 ] 	Batch(3800/7879) done. Loss: 0.3259  lr:0.000001
[ Tue Jul  9 14:58:16 2024 ] 	Batch(3900/7879) done. Loss: 0.0316  lr:0.000001
[ Tue Jul  9 14:58:38 2024 ] 
Training: Epoch [102/120], Step [3999], Loss: 0.1877840906381607, Training Accuracy: 95.99375
[ Tue Jul  9 14:58:39 2024 ] 	Batch(4000/7879) done. Loss: 0.0562  lr:0.000001
[ Tue Jul  9 14:59:02 2024 ] 	Batch(4100/7879) done. Loss: 0.1066  lr:0.000001
[ Tue Jul  9 14:59:25 2024 ] 	Batch(4200/7879) done. Loss: 0.0283  lr:0.000001
[ Tue Jul  9 14:59:49 2024 ] 	Batch(4300/7879) done. Loss: 0.2979  lr:0.000001
[ Tue Jul  9 15:00:12 2024 ] 	Batch(4400/7879) done. Loss: 0.0094  lr:0.000001
[ Tue Jul  9 15:00:35 2024 ] 
Training: Epoch [102/120], Step [4499], Loss: 0.16846616566181183, Training Accuracy: 96.05833333333334
[ Tue Jul  9 15:00:35 2024 ] 	Batch(4500/7879) done. Loss: 0.1597  lr:0.000001
[ Tue Jul  9 15:00:58 2024 ] 	Batch(4600/7879) done. Loss: 0.1442  lr:0.000001
[ Tue Jul  9 15:01:21 2024 ] 	Batch(4700/7879) done. Loss: 0.1296  lr:0.000001
[ Tue Jul  9 15:01:44 2024 ] 	Batch(4800/7879) done. Loss: 0.1099  lr:0.000001
[ Tue Jul  9 15:02:06 2024 ] 	Batch(4900/7879) done. Loss: 0.5079  lr:0.000001
[ Tue Jul  9 15:02:29 2024 ] 
Training: Epoch [102/120], Step [4999], Loss: 0.4009259343147278, Training Accuracy: 96.0625
[ Tue Jul  9 15:02:29 2024 ] 	Batch(5000/7879) done. Loss: 0.1366  lr:0.000001
[ Tue Jul  9 15:02:52 2024 ] 	Batch(5100/7879) done. Loss: 0.0143  lr:0.000001
[ Tue Jul  9 15:03:14 2024 ] 	Batch(5200/7879) done. Loss: 0.1364  lr:0.000001
[ Tue Jul  9 15:03:37 2024 ] 	Batch(5300/7879) done. Loss: 0.0545  lr:0.000001
[ Tue Jul  9 15:04:00 2024 ] 	Batch(5400/7879) done. Loss: 0.0045  lr:0.000001
[ Tue Jul  9 15:04:22 2024 ] 
Training: Epoch [102/120], Step [5499], Loss: 0.06822289526462555, Training Accuracy: 96.08181818181818
[ Tue Jul  9 15:04:22 2024 ] 	Batch(5500/7879) done. Loss: 0.0863  lr:0.000001
[ Tue Jul  9 15:04:45 2024 ] 	Batch(5600/7879) done. Loss: 0.0084  lr:0.000001
[ Tue Jul  9 15:05:08 2024 ] 	Batch(5700/7879) done. Loss: 0.0085  lr:0.000001
[ Tue Jul  9 15:05:31 2024 ] 	Batch(5800/7879) done. Loss: 0.0166  lr:0.000001
[ Tue Jul  9 15:05:53 2024 ] 	Batch(5900/7879) done. Loss: 0.2468  lr:0.000001
[ Tue Jul  9 15:06:16 2024 ] 
Training: Epoch [102/120], Step [5999], Loss: 0.03836805745959282, Training Accuracy: 96.1125
[ Tue Jul  9 15:06:16 2024 ] 	Batch(6000/7879) done. Loss: 0.0435  lr:0.000001
[ Tue Jul  9 15:06:39 2024 ] 	Batch(6100/7879) done. Loss: 0.8673  lr:0.000001
[ Tue Jul  9 15:07:01 2024 ] 	Batch(6200/7879) done. Loss: 0.3717  lr:0.000001
[ Tue Jul  9 15:07:24 2024 ] 	Batch(6300/7879) done. Loss: 0.0266  lr:0.000001
[ Tue Jul  9 15:07:47 2024 ] 	Batch(6400/7879) done. Loss: 0.2062  lr:0.000001
[ Tue Jul  9 15:08:10 2024 ] 
Training: Epoch [102/120], Step [6499], Loss: 0.022502833977341652, Training Accuracy: 96.10576923076923
[ Tue Jul  9 15:08:10 2024 ] 	Batch(6500/7879) done. Loss: 0.0151  lr:0.000001
[ Tue Jul  9 15:08:34 2024 ] 	Batch(6600/7879) done. Loss: 0.1586  lr:0.000001
[ Tue Jul  9 15:08:57 2024 ] 	Batch(6700/7879) done. Loss: 0.1549  lr:0.000001
[ Tue Jul  9 15:09:21 2024 ] 	Batch(6800/7879) done. Loss: 0.0548  lr:0.000001
[ Tue Jul  9 15:09:43 2024 ] 	Batch(6900/7879) done. Loss: 0.3323  lr:0.000001
[ Tue Jul  9 15:10:06 2024 ] 
Training: Epoch [102/120], Step [6999], Loss: 0.005406069103628397, Training Accuracy: 96.10357142857143
[ Tue Jul  9 15:10:06 2024 ] 	Batch(7000/7879) done. Loss: 0.0417  lr:0.000001
[ Tue Jul  9 15:10:29 2024 ] 	Batch(7100/7879) done. Loss: 0.0210  lr:0.000001
[ Tue Jul  9 15:10:52 2024 ] 	Batch(7200/7879) done. Loss: 0.4807  lr:0.000001
[ Tue Jul  9 15:11:14 2024 ] 	Batch(7300/7879) done. Loss: 0.0119  lr:0.000001
[ Tue Jul  9 15:11:37 2024 ] 	Batch(7400/7879) done. Loss: 0.0291  lr:0.000001
[ Tue Jul  9 15:11:59 2024 ] 
Training: Epoch [102/120], Step [7499], Loss: 0.35304099321365356, Training Accuracy: 96.07666666666667
[ Tue Jul  9 15:12:00 2024 ] 	Batch(7500/7879) done. Loss: 0.1371  lr:0.000001
[ Tue Jul  9 15:12:22 2024 ] 	Batch(7600/7879) done. Loss: 0.0893  lr:0.000001
[ Tue Jul  9 15:12:46 2024 ] 	Batch(7700/7879) done. Loss: 0.2411  lr:0.000001
[ Tue Jul  9 15:13:09 2024 ] 	Batch(7800/7879) done. Loss: 0.3263  lr:0.000001
[ Tue Jul  9 15:13:28 2024 ] 	Mean training loss: 0.1428.
[ Tue Jul  9 15:13:28 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 15:13:28 2024 ] Training epoch: 104
[ Tue Jul  9 15:13:28 2024 ] 	Batch(0/7879) done. Loss: 0.0312  lr:0.000001
[ Tue Jul  9 15:13:51 2024 ] 	Batch(100/7879) done. Loss: 0.0592  lr:0.000001
[ Tue Jul  9 15:14:14 2024 ] 	Batch(200/7879) done. Loss: 0.0079  lr:0.000001
[ Tue Jul  9 15:14:36 2024 ] 	Batch(300/7879) done. Loss: 0.0260  lr:0.000001
[ Tue Jul  9 15:14:59 2024 ] 	Batch(400/7879) done. Loss: 0.0050  lr:0.000001
[ Tue Jul  9 15:15:22 2024 ] 
Training: Epoch [103/120], Step [499], Loss: 0.2736111581325531, Training Accuracy: 95.72500000000001
[ Tue Jul  9 15:15:22 2024 ] 	Batch(500/7879) done. Loss: 0.1201  lr:0.000001
[ Tue Jul  9 15:15:45 2024 ] 	Batch(600/7879) done. Loss: 0.0166  lr:0.000001
[ Tue Jul  9 15:16:07 2024 ] 	Batch(700/7879) done. Loss: 0.2442  lr:0.000001
[ Tue Jul  9 15:16:30 2024 ] 	Batch(800/7879) done. Loss: 0.0149  lr:0.000001
[ Tue Jul  9 15:16:53 2024 ] 	Batch(900/7879) done. Loss: 0.0225  lr:0.000001
[ Tue Jul  9 15:17:16 2024 ] 
Training: Epoch [103/120], Step [999], Loss: 0.004398907069116831, Training Accuracy: 95.8125
[ Tue Jul  9 15:17:16 2024 ] 	Batch(1000/7879) done. Loss: 0.1494  lr:0.000001
[ Tue Jul  9 15:17:38 2024 ] 	Batch(1100/7879) done. Loss: 0.0119  lr:0.000001
[ Tue Jul  9 15:18:01 2024 ] 	Batch(1200/7879) done. Loss: 0.2199  lr:0.000001
[ Tue Jul  9 15:18:25 2024 ] 	Batch(1300/7879) done. Loss: 0.1050  lr:0.000001
[ Tue Jul  9 15:18:48 2024 ] 	Batch(1400/7879) done. Loss: 0.6993  lr:0.000001
[ Tue Jul  9 15:19:11 2024 ] 
Training: Epoch [103/120], Step [1499], Loss: 0.11315397918224335, Training Accuracy: 95.86666666666666
[ Tue Jul  9 15:19:12 2024 ] 	Batch(1500/7879) done. Loss: 0.0031  lr:0.000001
[ Tue Jul  9 15:19:35 2024 ] 	Batch(1600/7879) done. Loss: 0.1738  lr:0.000001
[ Tue Jul  9 15:19:59 2024 ] 	Batch(1700/7879) done. Loss: 0.4449  lr:0.000001
[ Tue Jul  9 15:20:21 2024 ] 	Batch(1800/7879) done. Loss: 0.3311  lr:0.000001
[ Tue Jul  9 15:20:44 2024 ] 	Batch(1900/7879) done. Loss: 0.2021  lr:0.000001
[ Tue Jul  9 15:21:07 2024 ] 
Training: Epoch [103/120], Step [1999], Loss: 0.07569773495197296, Training Accuracy: 96.0375
[ Tue Jul  9 15:21:07 2024 ] 	Batch(2000/7879) done. Loss: 0.0813  lr:0.000001
[ Tue Jul  9 15:21:30 2024 ] 	Batch(2100/7879) done. Loss: 0.1145  lr:0.000001
[ Tue Jul  9 15:21:54 2024 ] 	Batch(2200/7879) done. Loss: 0.2478  lr:0.000001
[ Tue Jul  9 15:22:17 2024 ] 	Batch(2300/7879) done. Loss: 0.0177  lr:0.000001
[ Tue Jul  9 15:22:41 2024 ] 	Batch(2400/7879) done. Loss: 0.1130  lr:0.000001
[ Tue Jul  9 15:23:04 2024 ] 
Training: Epoch [103/120], Step [2499], Loss: 0.128103107213974, Training Accuracy: 96.165
[ Tue Jul  9 15:23:04 2024 ] 	Batch(2500/7879) done. Loss: 0.1031  lr:0.000001
[ Tue Jul  9 15:23:27 2024 ] 	Batch(2600/7879) done. Loss: 0.4485  lr:0.000001
[ Tue Jul  9 15:23:50 2024 ] 	Batch(2700/7879) done. Loss: 0.1230  lr:0.000001
[ Tue Jul  9 15:24:13 2024 ] 	Batch(2800/7879) done. Loss: 0.0799  lr:0.000001
[ Tue Jul  9 15:24:36 2024 ] 	Batch(2900/7879) done. Loss: 0.0855  lr:0.000001
[ Tue Jul  9 15:24:59 2024 ] 
Training: Epoch [103/120], Step [2999], Loss: 0.2392035275697708, Training Accuracy: 96.1625
[ Tue Jul  9 15:24:59 2024 ] 	Batch(3000/7879) done. Loss: 0.3506  lr:0.000001
[ Tue Jul  9 15:25:21 2024 ] 	Batch(3100/7879) done. Loss: 0.0840  lr:0.000001
[ Tue Jul  9 15:25:44 2024 ] 	Batch(3200/7879) done. Loss: 0.2227  lr:0.000001
[ Tue Jul  9 15:26:07 2024 ] 	Batch(3300/7879) done. Loss: 0.0206  lr:0.000001
[ Tue Jul  9 15:26:30 2024 ] 	Batch(3400/7879) done. Loss: 0.0735  lr:0.000001
[ Tue Jul  9 15:26:52 2024 ] 
Training: Epoch [103/120], Step [3499], Loss: 0.040479883551597595, Training Accuracy: 96.11428571428571
[ Tue Jul  9 15:26:52 2024 ] 	Batch(3500/7879) done. Loss: 0.0906  lr:0.000001
[ Tue Jul  9 15:27:15 2024 ] 	Batch(3600/7879) done. Loss: 0.1015  lr:0.000001
[ Tue Jul  9 15:27:38 2024 ] 	Batch(3700/7879) done. Loss: 0.3035  lr:0.000001
[ Tue Jul  9 15:28:01 2024 ] 	Batch(3800/7879) done. Loss: 0.3222  lr:0.000001
[ Tue Jul  9 15:28:23 2024 ] 	Batch(3900/7879) done. Loss: 0.0248  lr:0.000001
[ Tue Jul  9 15:28:46 2024 ] 
Training: Epoch [103/120], Step [3999], Loss: 0.08313427865505219, Training Accuracy: 96.096875
[ Tue Jul  9 15:28:46 2024 ] 	Batch(4000/7879) done. Loss: 0.0630  lr:0.000001
[ Tue Jul  9 15:29:09 2024 ] 	Batch(4100/7879) done. Loss: 0.3092  lr:0.000001
[ Tue Jul  9 15:29:32 2024 ] 	Batch(4200/7879) done. Loss: 0.0890  lr:0.000001
[ Tue Jul  9 15:29:55 2024 ] 	Batch(4300/7879) done. Loss: 0.4302  lr:0.000001
[ Tue Jul  9 15:30:18 2024 ] 	Batch(4400/7879) done. Loss: 0.2159  lr:0.000001
[ Tue Jul  9 15:30:41 2024 ] 
Training: Epoch [103/120], Step [4499], Loss: 0.018269039690494537, Training Accuracy: 96.16111111111111
[ Tue Jul  9 15:30:42 2024 ] 	Batch(4500/7879) done. Loss: 0.0282  lr:0.000001
[ Tue Jul  9 15:31:05 2024 ] 	Batch(4600/7879) done. Loss: 0.1383  lr:0.000001
[ Tue Jul  9 15:31:29 2024 ] 	Batch(4700/7879) done. Loss: 0.0238  lr:0.000001
[ Tue Jul  9 15:31:52 2024 ] 	Batch(4800/7879) done. Loss: 0.0088  lr:0.000001
[ Tue Jul  9 15:32:16 2024 ] 	Batch(4900/7879) done. Loss: 0.2468  lr:0.000001
[ Tue Jul  9 15:32:39 2024 ] 
Training: Epoch [103/120], Step [4999], Loss: 0.03156374767422676, Training Accuracy: 96.16
[ Tue Jul  9 15:32:39 2024 ] 	Batch(5000/7879) done. Loss: 0.0395  lr:0.000001
[ Tue Jul  9 15:33:02 2024 ] 	Batch(5100/7879) done. Loss: 0.0418  lr:0.000001
[ Tue Jul  9 15:33:26 2024 ] 	Batch(5200/7879) done. Loss: 0.0980  lr:0.000001
[ Tue Jul  9 15:33:49 2024 ] 	Batch(5300/7879) done. Loss: 0.0667  lr:0.000001
[ Tue Jul  9 15:34:11 2024 ] 	Batch(5400/7879) done. Loss: 0.4303  lr:0.000001
[ Tue Jul  9 15:34:34 2024 ] 
Training: Epoch [103/120], Step [5499], Loss: 0.07314097881317139, Training Accuracy: 96.15227272727273
[ Tue Jul  9 15:34:34 2024 ] 	Batch(5500/7879) done. Loss: 0.0152  lr:0.000001
[ Tue Jul  9 15:34:57 2024 ] 	Batch(5600/7879) done. Loss: 0.2571  lr:0.000001
[ Tue Jul  9 15:35:20 2024 ] 	Batch(5700/7879) done. Loss: 0.0548  lr:0.000001
[ Tue Jul  9 15:35:42 2024 ] 	Batch(5800/7879) done. Loss: 0.0071  lr:0.000001
[ Tue Jul  9 15:36:05 2024 ] 	Batch(5900/7879) done. Loss: 0.1324  lr:0.000001
[ Tue Jul  9 15:36:28 2024 ] 
Training: Epoch [103/120], Step [5999], Loss: 0.020971251651644707, Training Accuracy: 96.15208333333334
[ Tue Jul  9 15:36:28 2024 ] 	Batch(6000/7879) done. Loss: 0.1185  lr:0.000001
[ Tue Jul  9 15:36:51 2024 ] 	Batch(6100/7879) done. Loss: 0.0825  lr:0.000001
[ Tue Jul  9 15:37:13 2024 ] 	Batch(6200/7879) done. Loss: 0.0548  lr:0.000001
[ Tue Jul  9 15:37:36 2024 ] 	Batch(6300/7879) done. Loss: 0.1294  lr:0.000001
[ Tue Jul  9 15:37:59 2024 ] 	Batch(6400/7879) done. Loss: 0.0799  lr:0.000001
[ Tue Jul  9 15:38:21 2024 ] 
Training: Epoch [103/120], Step [6499], Loss: 0.1711764931678772, Training Accuracy: 96.14807692307691
[ Tue Jul  9 15:38:22 2024 ] 	Batch(6500/7879) done. Loss: 0.0190  lr:0.000001
[ Tue Jul  9 15:38:44 2024 ] 	Batch(6600/7879) done. Loss: 0.3970  lr:0.000001
[ Tue Jul  9 15:39:07 2024 ] 	Batch(6700/7879) done. Loss: 0.1259  lr:0.000001
[ Tue Jul  9 15:39:30 2024 ] 	Batch(6800/7879) done. Loss: 0.0494  lr:0.000001
[ Tue Jul  9 15:39:53 2024 ] 	Batch(6900/7879) done. Loss: 0.0204  lr:0.000001
[ Tue Jul  9 15:40:17 2024 ] 
Training: Epoch [103/120], Step [6999], Loss: 0.1911088526248932, Training Accuracy: 96.14821428571429
[ Tue Jul  9 15:40:17 2024 ] 	Batch(7000/7879) done. Loss: 0.0210  lr:0.000001
[ Tue Jul  9 15:40:40 2024 ] 	Batch(7100/7879) done. Loss: 0.2143  lr:0.000001
[ Tue Jul  9 15:41:04 2024 ] 	Batch(7200/7879) done. Loss: 0.2346  lr:0.000001
[ Tue Jul  9 15:41:27 2024 ] 	Batch(7300/7879) done. Loss: 0.2039  lr:0.000001
[ Tue Jul  9 15:41:51 2024 ] 	Batch(7400/7879) done. Loss: 0.0996  lr:0.000001
[ Tue Jul  9 15:42:14 2024 ] 
Training: Epoch [103/120], Step [7499], Loss: 0.12430861592292786, Training Accuracy: 96.17
[ Tue Jul  9 15:42:14 2024 ] 	Batch(7500/7879) done. Loss: 0.4063  lr:0.000001
[ Tue Jul  9 15:42:38 2024 ] 	Batch(7600/7879) done. Loss: 0.2285  lr:0.000001
[ Tue Jul  9 15:43:00 2024 ] 	Batch(7700/7879) done. Loss: 0.2566  lr:0.000001
[ Tue Jul  9 15:43:23 2024 ] 	Batch(7800/7879) done. Loss: 0.1779  lr:0.000001
[ Tue Jul  9 15:43:41 2024 ] 	Mean training loss: 0.1424.
[ Tue Jul  9 15:43:41 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 15:43:41 2024 ] Training epoch: 105
[ Tue Jul  9 15:43:42 2024 ] 	Batch(0/7879) done. Loss: 0.1350  lr:0.000001
[ Tue Jul  9 15:44:05 2024 ] 	Batch(100/7879) done. Loss: 0.0384  lr:0.000001
[ Tue Jul  9 15:44:28 2024 ] 	Batch(200/7879) done. Loss: 0.4680  lr:0.000001
[ Tue Jul  9 15:44:51 2024 ] 	Batch(300/7879) done. Loss: 0.0313  lr:0.000001
[ Tue Jul  9 15:45:14 2024 ] 	Batch(400/7879) done. Loss: 0.1286  lr:0.000001
[ Tue Jul  9 15:45:37 2024 ] 
Training: Epoch [104/120], Step [499], Loss: 0.003611149499192834, Training Accuracy: 95.92500000000001
[ Tue Jul  9 15:45:37 2024 ] 	Batch(500/7879) done. Loss: 0.2740  lr:0.000001
[ Tue Jul  9 15:46:00 2024 ] 	Batch(600/7879) done. Loss: 0.2155  lr:0.000001
[ Tue Jul  9 15:46:22 2024 ] 	Batch(700/7879) done. Loss: 0.1247  lr:0.000001
[ Tue Jul  9 15:46:45 2024 ] 	Batch(800/7879) done. Loss: 0.0621  lr:0.000001
[ Tue Jul  9 15:47:09 2024 ] 	Batch(900/7879) done. Loss: 0.1950  lr:0.000001
[ Tue Jul  9 15:47:32 2024 ] 
Training: Epoch [104/120], Step [999], Loss: 0.05863424763083458, Training Accuracy: 96.1
[ Tue Jul  9 15:47:32 2024 ] 	Batch(1000/7879) done. Loss: 0.1350  lr:0.000001
[ Tue Jul  9 15:47:55 2024 ] 	Batch(1100/7879) done. Loss: 0.0531  lr:0.000001
[ Tue Jul  9 15:48:19 2024 ] 	Batch(1200/7879) done. Loss: 0.0429  lr:0.000001
[ Tue Jul  9 15:48:42 2024 ] 	Batch(1300/7879) done. Loss: 0.2195  lr:0.000001
[ Tue Jul  9 15:49:05 2024 ] 	Batch(1400/7879) done. Loss: 0.1527  lr:0.000001
[ Tue Jul  9 15:49:27 2024 ] 
Training: Epoch [104/120], Step [1499], Loss: 0.08194472640752792, Training Accuracy: 96.01666666666667
[ Tue Jul  9 15:49:27 2024 ] 	Batch(1500/7879) done. Loss: 0.4208  lr:0.000001
[ Tue Jul  9 15:49:50 2024 ] 	Batch(1600/7879) done. Loss: 0.1556  lr:0.000001
[ Tue Jul  9 15:50:13 2024 ] 	Batch(1700/7879) done. Loss: 0.0037  lr:0.000001
[ Tue Jul  9 15:50:36 2024 ] 	Batch(1800/7879) done. Loss: 0.1211  lr:0.000001
[ Tue Jul  9 15:50:58 2024 ] 	Batch(1900/7879) done. Loss: 0.1499  lr:0.000001
[ Tue Jul  9 15:51:21 2024 ] 
Training: Epoch [104/120], Step [1999], Loss: 0.03887517750263214, Training Accuracy: 96.00625000000001
[ Tue Jul  9 15:51:21 2024 ] 	Batch(2000/7879) done. Loss: 0.7166  lr:0.000001
[ Tue Jul  9 15:51:44 2024 ] 	Batch(2100/7879) done. Loss: 0.1389  lr:0.000001
[ Tue Jul  9 15:52:06 2024 ] 	Batch(2200/7879) done. Loss: 0.0429  lr:0.000001
[ Tue Jul  9 15:52:29 2024 ] 	Batch(2300/7879) done. Loss: 0.0363  lr:0.000001
[ Tue Jul  9 15:52:52 2024 ] 	Batch(2400/7879) done. Loss: 0.0248  lr:0.000001
[ Tue Jul  9 15:53:14 2024 ] 
Training: Epoch [104/120], Step [2499], Loss: 0.21825873851776123, Training Accuracy: 96.05
[ Tue Jul  9 15:53:15 2024 ] 	Batch(2500/7879) done. Loss: 0.1377  lr:0.000001
[ Tue Jul  9 15:53:37 2024 ] 	Batch(2600/7879) done. Loss: 0.0955  lr:0.000001
[ Tue Jul  9 15:54:00 2024 ] 	Batch(2700/7879) done. Loss: 0.3385  lr:0.000001
[ Tue Jul  9 15:54:23 2024 ] 	Batch(2800/7879) done. Loss: 0.0269  lr:0.000001
[ Tue Jul  9 15:54:46 2024 ] 	Batch(2900/7879) done. Loss: 0.1607  lr:0.000001
[ Tue Jul  9 15:55:08 2024 ] 
Training: Epoch [104/120], Step [2999], Loss: 0.013207145035266876, Training Accuracy: 96.13333333333334
[ Tue Jul  9 15:55:08 2024 ] 	Batch(3000/7879) done. Loss: 0.0619  lr:0.000001
[ Tue Jul  9 15:55:31 2024 ] 	Batch(3100/7879) done. Loss: 0.1728  lr:0.000001
[ Tue Jul  9 15:55:55 2024 ] 	Batch(3200/7879) done. Loss: 0.0636  lr:0.000001
[ Tue Jul  9 15:56:17 2024 ] 	Batch(3300/7879) done. Loss: 0.1788  lr:0.000001
[ Tue Jul  9 15:56:40 2024 ] 	Batch(3400/7879) done. Loss: 0.0338  lr:0.000001
[ Tue Jul  9 15:57:03 2024 ] 
Training: Epoch [104/120], Step [3499], Loss: 0.24057495594024658, Training Accuracy: 96.14642857142857
[ Tue Jul  9 15:57:03 2024 ] 	Batch(3500/7879) done. Loss: 0.0339  lr:0.000001
[ Tue Jul  9 15:57:26 2024 ] 	Batch(3600/7879) done. Loss: 0.1100  lr:0.000001
[ Tue Jul  9 15:57:48 2024 ] 	Batch(3700/7879) done. Loss: 0.0673  lr:0.000001
[ Tue Jul  9 15:58:11 2024 ] 	Batch(3800/7879) done. Loss: 0.0573  lr:0.000001
[ Tue Jul  9 15:58:34 2024 ] 	Batch(3900/7879) done. Loss: 0.0047  lr:0.000001
[ Tue Jul  9 15:58:56 2024 ] 
Training: Epoch [104/120], Step [3999], Loss: 0.05567505210638046, Training Accuracy: 96.14375
[ Tue Jul  9 15:58:57 2024 ] 	Batch(4000/7879) done. Loss: 0.0114  lr:0.000001
[ Tue Jul  9 15:59:19 2024 ] 	Batch(4100/7879) done. Loss: 0.2519  lr:0.000001
[ Tue Jul  9 15:59:42 2024 ] 	Batch(4200/7879) done. Loss: 0.3631  lr:0.000001
[ Tue Jul  9 16:00:05 2024 ] 	Batch(4300/7879) done. Loss: 0.0301  lr:0.000001
[ Tue Jul  9 16:00:28 2024 ] 	Batch(4400/7879) done. Loss: 0.0589  lr:0.000001
[ Tue Jul  9 16:00:50 2024 ] 
Training: Epoch [104/120], Step [4499], Loss: 0.006459962576627731, Training Accuracy: 96.10833333333333
[ Tue Jul  9 16:00:50 2024 ] 	Batch(4500/7879) done. Loss: 0.3742  lr:0.000001
[ Tue Jul  9 16:01:13 2024 ] 	Batch(4600/7879) done. Loss: 0.0144  lr:0.000001
[ Tue Jul  9 16:01:36 2024 ] 	Batch(4700/7879) done. Loss: 0.0097  lr:0.000001
[ Tue Jul  9 16:01:59 2024 ] 	Batch(4800/7879) done. Loss: 0.1591  lr:0.000001
[ Tue Jul  9 16:02:21 2024 ] 	Batch(4900/7879) done. Loss: 0.2667  lr:0.000001
[ Tue Jul  9 16:02:44 2024 ] 
Training: Epoch [104/120], Step [4999], Loss: 0.08861309289932251, Training Accuracy: 96.13000000000001
[ Tue Jul  9 16:02:44 2024 ] 	Batch(5000/7879) done. Loss: 0.0196  lr:0.000001
[ Tue Jul  9 16:03:07 2024 ] 	Batch(5100/7879) done. Loss: 0.1670  lr:0.000001
[ Tue Jul  9 16:03:30 2024 ] 	Batch(5200/7879) done. Loss: 0.0997  lr:0.000001
[ Tue Jul  9 16:03:52 2024 ] 	Batch(5300/7879) done. Loss: 0.0668  lr:0.000001
[ Tue Jul  9 16:04:15 2024 ] 	Batch(5400/7879) done. Loss: 0.0498  lr:0.000001
[ Tue Jul  9 16:04:38 2024 ] 
Training: Epoch [104/120], Step [5499], Loss: 0.23357506096363068, Training Accuracy: 96.125
[ Tue Jul  9 16:04:38 2024 ] 	Batch(5500/7879) done. Loss: 0.2188  lr:0.000001
[ Tue Jul  9 16:05:01 2024 ] 	Batch(5600/7879) done. Loss: 0.0692  lr:0.000001
[ Tue Jul  9 16:05:23 2024 ] 	Batch(5700/7879) done. Loss: 0.6002  lr:0.000001
[ Tue Jul  9 16:05:46 2024 ] 	Batch(5800/7879) done. Loss: 0.0998  lr:0.000001
[ Tue Jul  9 16:06:09 2024 ] 	Batch(5900/7879) done. Loss: 0.1604  lr:0.000001
[ Tue Jul  9 16:06:31 2024 ] 
Training: Epoch [104/120], Step [5999], Loss: 0.11913970112800598, Training Accuracy: 96.13333333333334
[ Tue Jul  9 16:06:32 2024 ] 	Batch(6000/7879) done. Loss: 0.2206  lr:0.000001
[ Tue Jul  9 16:06:54 2024 ] 	Batch(6100/7879) done. Loss: 0.1789  lr:0.000001
[ Tue Jul  9 16:07:17 2024 ] 	Batch(6200/7879) done. Loss: 0.2980  lr:0.000001
[ Tue Jul  9 16:07:40 2024 ] 	Batch(6300/7879) done. Loss: 0.0497  lr:0.000001
[ Tue Jul  9 16:08:03 2024 ] 	Batch(6400/7879) done. Loss: 0.0608  lr:0.000001
[ Tue Jul  9 16:08:25 2024 ] 
Training: Epoch [104/120], Step [6499], Loss: 0.019002005457878113, Training Accuracy: 96.14615384615385
[ Tue Jul  9 16:08:25 2024 ] 	Batch(6500/7879) done. Loss: 0.1203  lr:0.000001
[ Tue Jul  9 16:08:48 2024 ] 	Batch(6600/7879) done. Loss: 0.1755  lr:0.000001
[ Tue Jul  9 16:09:11 2024 ] 	Batch(6700/7879) done. Loss: 0.0301  lr:0.000001
[ Tue Jul  9 16:09:34 2024 ] 	Batch(6800/7879) done. Loss: 0.0181  lr:0.000001
[ Tue Jul  9 16:09:56 2024 ] 	Batch(6900/7879) done. Loss: 0.0085  lr:0.000001
[ Tue Jul  9 16:10:19 2024 ] 
Training: Epoch [104/120], Step [6999], Loss: 0.3321475684642792, Training Accuracy: 96.17678571428571
[ Tue Jul  9 16:10:19 2024 ] 	Batch(7000/7879) done. Loss: 0.0528  lr:0.000001
[ Tue Jul  9 16:10:42 2024 ] 	Batch(7100/7879) done. Loss: 0.1166  lr:0.000001
[ Tue Jul  9 16:11:05 2024 ] 	Batch(7200/7879) done. Loss: 0.0546  lr:0.000001
[ Tue Jul  9 16:11:28 2024 ] 	Batch(7300/7879) done. Loss: 0.5292  lr:0.000001
[ Tue Jul  9 16:11:50 2024 ] 	Batch(7400/7879) done. Loss: 0.0476  lr:0.000001
[ Tue Jul  9 16:12:13 2024 ] 
Training: Epoch [104/120], Step [7499], Loss: 0.06112891435623169, Training Accuracy: 96.20833333333333
[ Tue Jul  9 16:12:13 2024 ] 	Batch(7500/7879) done. Loss: 0.1795  lr:0.000001
[ Tue Jul  9 16:12:36 2024 ] 	Batch(7600/7879) done. Loss: 0.2192  lr:0.000001
[ Tue Jul  9 16:12:59 2024 ] 	Batch(7700/7879) done. Loss: 0.0624  lr:0.000001
[ Tue Jul  9 16:13:21 2024 ] 	Batch(7800/7879) done. Loss: 0.0069  lr:0.000001
[ Tue Jul  9 16:13:39 2024 ] 	Mean training loss: 0.1401.
[ Tue Jul  9 16:13:39 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 16:13:39 2024 ] Training epoch: 106
[ Tue Jul  9 16:13:40 2024 ] 	Batch(0/7879) done. Loss: 0.0731  lr:0.000001
[ Tue Jul  9 16:14:03 2024 ] 	Batch(100/7879) done. Loss: 0.1991  lr:0.000001
[ Tue Jul  9 16:14:25 2024 ] 	Batch(200/7879) done. Loss: 0.0126  lr:0.000001
[ Tue Jul  9 16:14:48 2024 ] 	Batch(300/7879) done. Loss: 0.0055  lr:0.000001
[ Tue Jul  9 16:15:11 2024 ] 	Batch(400/7879) done. Loss: 0.0100  lr:0.000001
[ Tue Jul  9 16:15:33 2024 ] 
Training: Epoch [105/120], Step [499], Loss: 0.08185914903879166, Training Accuracy: 95.89999999999999
[ Tue Jul  9 16:15:33 2024 ] 	Batch(500/7879) done. Loss: 0.0622  lr:0.000001
[ Tue Jul  9 16:15:56 2024 ] 	Batch(600/7879) done. Loss: 0.0109  lr:0.000001
[ Tue Jul  9 16:16:19 2024 ] 	Batch(700/7879) done. Loss: 0.1993  lr:0.000001
[ Tue Jul  9 16:16:42 2024 ] 	Batch(800/7879) done. Loss: 0.0107  lr:0.000001
[ Tue Jul  9 16:17:04 2024 ] 	Batch(900/7879) done. Loss: 0.1957  lr:0.000001
[ Tue Jul  9 16:17:27 2024 ] 
Training: Epoch [105/120], Step [999], Loss: 0.12454387545585632, Training Accuracy: 96.1125
[ Tue Jul  9 16:17:27 2024 ] 	Batch(1000/7879) done. Loss: 0.1205  lr:0.000001
[ Tue Jul  9 16:17:50 2024 ] 	Batch(1100/7879) done. Loss: 0.2130  lr:0.000001
[ Tue Jul  9 16:18:13 2024 ] 	Batch(1200/7879) done. Loss: 0.0112  lr:0.000001
[ Tue Jul  9 16:18:35 2024 ] 	Batch(1300/7879) done. Loss: 0.2589  lr:0.000001
[ Tue Jul  9 16:18:58 2024 ] 	Batch(1400/7879) done. Loss: 0.0326  lr:0.000001
[ Tue Jul  9 16:19:21 2024 ] 
Training: Epoch [105/120], Step [1499], Loss: 0.010274581611156464, Training Accuracy: 96.26666666666667
[ Tue Jul  9 16:19:21 2024 ] 	Batch(1500/7879) done. Loss: 0.2413  lr:0.000001
[ Tue Jul  9 16:19:44 2024 ] 	Batch(1600/7879) done. Loss: 0.0232  lr:0.000001
[ Tue Jul  9 16:20:06 2024 ] 	Batch(1700/7879) done. Loss: 0.0757  lr:0.000001
[ Tue Jul  9 16:20:29 2024 ] 	Batch(1800/7879) done. Loss: 0.0768  lr:0.000001
[ Tue Jul  9 16:20:52 2024 ] 	Batch(1900/7879) done. Loss: 0.0562  lr:0.000001
[ Tue Jul  9 16:21:16 2024 ] 
Training: Epoch [105/120], Step [1999], Loss: 0.05780545994639397, Training Accuracy: 96.1625
[ Tue Jul  9 16:21:16 2024 ] 	Batch(2000/7879) done. Loss: 0.0224  lr:0.000001
[ Tue Jul  9 16:21:39 2024 ] 	Batch(2100/7879) done. Loss: 0.0122  lr:0.000001
[ Tue Jul  9 16:22:01 2024 ] 	Batch(2200/7879) done. Loss: 0.0855  lr:0.000001
[ Tue Jul  9 16:22:24 2024 ] 	Batch(2300/7879) done. Loss: 0.0447  lr:0.000001
[ Tue Jul  9 16:22:47 2024 ] 	Batch(2400/7879) done. Loss: 0.0754  lr:0.000001
[ Tue Jul  9 16:23:10 2024 ] 
Training: Epoch [105/120], Step [2499], Loss: 0.0366634763777256, Training Accuracy: 96.09
[ Tue Jul  9 16:23:11 2024 ] 	Batch(2500/7879) done. Loss: 0.0694  lr:0.000001
[ Tue Jul  9 16:23:34 2024 ] 	Batch(2600/7879) done. Loss: 0.0430  lr:0.000001
[ Tue Jul  9 16:23:58 2024 ] 	Batch(2700/7879) done. Loss: 0.2279  lr:0.000001
[ Tue Jul  9 16:24:21 2024 ] 	Batch(2800/7879) done. Loss: 0.0357  lr:0.000001
[ Tue Jul  9 16:24:44 2024 ] 	Batch(2900/7879) done. Loss: 0.0496  lr:0.000001
[ Tue Jul  9 16:25:06 2024 ] 
Training: Epoch [105/120], Step [2999], Loss: 0.07368266582489014, Training Accuracy: 96.05
[ Tue Jul  9 16:25:07 2024 ] 	Batch(3000/7879) done. Loss: 0.1427  lr:0.000001
[ Tue Jul  9 16:25:29 2024 ] 	Batch(3100/7879) done. Loss: 0.0837  lr:0.000001
[ Tue Jul  9 16:25:52 2024 ] 	Batch(3200/7879) done. Loss: 0.0890  lr:0.000001
[ Tue Jul  9 16:26:15 2024 ] 	Batch(3300/7879) done. Loss: 0.0810  lr:0.000001
[ Tue Jul  9 16:26:38 2024 ] 	Batch(3400/7879) done. Loss: 0.1565  lr:0.000001
[ Tue Jul  9 16:27:00 2024 ] 
Training: Epoch [105/120], Step [3499], Loss: 0.04574454575777054, Training Accuracy: 96.09285714285714
[ Tue Jul  9 16:27:00 2024 ] 	Batch(3500/7879) done. Loss: 0.0213  lr:0.000001
[ Tue Jul  9 16:27:23 2024 ] 	Batch(3600/7879) done. Loss: 0.0056  lr:0.000001
[ Tue Jul  9 16:27:47 2024 ] 	Batch(3700/7879) done. Loss: 0.1880  lr:0.000001
[ Tue Jul  9 16:28:10 2024 ] 	Batch(3800/7879) done. Loss: 0.0818  lr:0.000001
[ Tue Jul  9 16:28:32 2024 ] 	Batch(3900/7879) done. Loss: 0.0090  lr:0.000001
[ Tue Jul  9 16:28:55 2024 ] 
Training: Epoch [105/120], Step [3999], Loss: 0.23029503226280212, Training Accuracy: 96.1875
[ Tue Jul  9 16:28:55 2024 ] 	Batch(4000/7879) done. Loss: 0.0037  lr:0.000001
[ Tue Jul  9 16:29:18 2024 ] 	Batch(4100/7879) done. Loss: 0.0141  lr:0.000001
[ Tue Jul  9 16:29:41 2024 ] 	Batch(4200/7879) done. Loss: 0.0433  lr:0.000001
[ Tue Jul  9 16:30:03 2024 ] 	Batch(4300/7879) done. Loss: 0.0736  lr:0.000001
[ Tue Jul  9 16:30:26 2024 ] 	Batch(4400/7879) done. Loss: 0.0083  lr:0.000001
[ Tue Jul  9 16:30:49 2024 ] 
Training: Epoch [105/120], Step [4499], Loss: 0.03137512505054474, Training Accuracy: 96.23333333333333
[ Tue Jul  9 16:30:49 2024 ] 	Batch(4500/7879) done. Loss: 0.1858  lr:0.000001
[ Tue Jul  9 16:31:12 2024 ] 	Batch(4600/7879) done. Loss: 0.0354  lr:0.000001
[ Tue Jul  9 16:31:34 2024 ] 	Batch(4700/7879) done. Loss: 0.6318  lr:0.000001
[ Tue Jul  9 16:31:57 2024 ] 	Batch(4800/7879) done. Loss: 0.1564  lr:0.000001
[ Tue Jul  9 16:32:20 2024 ] 	Batch(4900/7879) done. Loss: 0.1136  lr:0.000001
[ Tue Jul  9 16:32:42 2024 ] 
Training: Epoch [105/120], Step [4999], Loss: 0.17820848524570465, Training Accuracy: 96.17
[ Tue Jul  9 16:32:43 2024 ] 	Batch(5000/7879) done. Loss: 0.0093  lr:0.000001
[ Tue Jul  9 16:33:05 2024 ] 	Batch(5100/7879) done. Loss: 0.2217  lr:0.000001
[ Tue Jul  9 16:33:28 2024 ] 	Batch(5200/7879) done. Loss: 0.2056  lr:0.000001
[ Tue Jul  9 16:33:51 2024 ] 	Batch(5300/7879) done. Loss: 0.0121  lr:0.000001
[ Tue Jul  9 16:34:14 2024 ] 	Batch(5400/7879) done. Loss: 0.2617  lr:0.000001
[ Tue Jul  9 16:34:36 2024 ] 
Training: Epoch [105/120], Step [5499], Loss: 0.04347791150212288, Training Accuracy: 96.17954545454546
[ Tue Jul  9 16:34:36 2024 ] 	Batch(5500/7879) done. Loss: 0.1037  lr:0.000001
[ Tue Jul  9 16:34:59 2024 ] 	Batch(5600/7879) done. Loss: 0.3339  lr:0.000001
[ Tue Jul  9 16:35:22 2024 ] 	Batch(5700/7879) done. Loss: 0.1084  lr:0.000001
[ Tue Jul  9 16:35:45 2024 ] 	Batch(5800/7879) done. Loss: 0.0060  lr:0.000001
[ Tue Jul  9 16:36:07 2024 ] 	Batch(5900/7879) done. Loss: 0.0366  lr:0.000001
[ Tue Jul  9 16:36:30 2024 ] 
Training: Epoch [105/120], Step [5999], Loss: 0.03473401814699173, Training Accuracy: 96.16458333333333
[ Tue Jul  9 16:36:30 2024 ] 	Batch(6000/7879) done. Loss: 0.0114  lr:0.000001
[ Tue Jul  9 16:36:54 2024 ] 	Batch(6100/7879) done. Loss: 0.3659  lr:0.000001
[ Tue Jul  9 16:37:17 2024 ] 	Batch(6200/7879) done. Loss: 0.0670  lr:0.000001
[ Tue Jul  9 16:37:39 2024 ] 	Batch(6300/7879) done. Loss: 0.0418  lr:0.000001
[ Tue Jul  9 16:38:02 2024 ] 	Batch(6400/7879) done. Loss: 0.1350  lr:0.000001
[ Tue Jul  9 16:38:25 2024 ] 
Training: Epoch [105/120], Step [6499], Loss: 0.037033811211586, Training Accuracy: 96.175
[ Tue Jul  9 16:38:25 2024 ] 	Batch(6500/7879) done. Loss: 0.0238  lr:0.000001
[ Tue Jul  9 16:38:48 2024 ] 	Batch(6600/7879) done. Loss: 0.2329  lr:0.000001
[ Tue Jul  9 16:39:10 2024 ] 	Batch(6700/7879) done. Loss: 0.0788  lr:0.000001
[ Tue Jul  9 16:39:33 2024 ] 	Batch(6800/7879) done. Loss: 0.0346  lr:0.000001
[ Tue Jul  9 16:39:56 2024 ] 	Batch(6900/7879) done. Loss: 0.0228  lr:0.000001
[ Tue Jul  9 16:40:19 2024 ] 
Training: Epoch [105/120], Step [6999], Loss: 0.13566891849040985, Training Accuracy: 96.1875
[ Tue Jul  9 16:40:19 2024 ] 	Batch(7000/7879) done. Loss: 0.2116  lr:0.000001
[ Tue Jul  9 16:40:42 2024 ] 	Batch(7100/7879) done. Loss: 0.1346  lr:0.000001
[ Tue Jul  9 16:41:04 2024 ] 	Batch(7200/7879) done. Loss: 0.3410  lr:0.000001
[ Tue Jul  9 16:41:27 2024 ] 	Batch(7300/7879) done. Loss: 0.1863  lr:0.000001
[ Tue Jul  9 16:41:50 2024 ] 	Batch(7400/7879) done. Loss: 0.1342  lr:0.000001
[ Tue Jul  9 16:42:12 2024 ] 
Training: Epoch [105/120], Step [7499], Loss: 0.1417234092950821, Training Accuracy: 96.19333333333333
[ Tue Jul  9 16:42:12 2024 ] 	Batch(7500/7879) done. Loss: 0.1266  lr:0.000001
[ Tue Jul  9 16:42:35 2024 ] 	Batch(7600/7879) done. Loss: 0.1141  lr:0.000001
[ Tue Jul  9 16:42:58 2024 ] 	Batch(7700/7879) done. Loss: 0.0719  lr:0.000001
[ Tue Jul  9 16:43:21 2024 ] 	Batch(7800/7879) done. Loss: 0.0874  lr:0.000001
[ Tue Jul  9 16:43:39 2024 ] 	Mean training loss: 0.1406.
[ Tue Jul  9 16:43:39 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 16:43:39 2024 ] Training epoch: 107
[ Tue Jul  9 16:43:39 2024 ] 	Batch(0/7879) done. Loss: 0.1123  lr:0.000001
[ Tue Jul  9 16:44:02 2024 ] 	Batch(100/7879) done. Loss: 0.0374  lr:0.000001
[ Tue Jul  9 16:44:25 2024 ] 	Batch(200/7879) done. Loss: 0.1874  lr:0.000001
[ Tue Jul  9 16:44:47 2024 ] 	Batch(300/7879) done. Loss: 0.5725  lr:0.000001
[ Tue Jul  9 16:45:10 2024 ] 	Batch(400/7879) done. Loss: 0.1699  lr:0.000001
[ Tue Jul  9 16:45:33 2024 ] 
Training: Epoch [106/120], Step [499], Loss: 0.0648350641131401, Training Accuracy: 96.325
[ Tue Jul  9 16:45:33 2024 ] 	Batch(500/7879) done. Loss: 0.2278  lr:0.000001
[ Tue Jul  9 16:45:56 2024 ] 	Batch(600/7879) done. Loss: 0.0234  lr:0.000001
[ Tue Jul  9 16:46:18 2024 ] 	Batch(700/7879) done. Loss: 0.2538  lr:0.000001
[ Tue Jul  9 16:46:41 2024 ] 	Batch(800/7879) done. Loss: 0.2344  lr:0.000001
[ Tue Jul  9 16:47:05 2024 ] 	Batch(900/7879) done. Loss: 0.0430  lr:0.000001
[ Tue Jul  9 16:47:27 2024 ] 
Training: Epoch [106/120], Step [999], Loss: 0.06563306599855423, Training Accuracy: 96.2125
[ Tue Jul  9 16:47:27 2024 ] 	Batch(1000/7879) done. Loss: 0.3079  lr:0.000001
[ Tue Jul  9 16:47:50 2024 ] 	Batch(1100/7879) done. Loss: 0.1758  lr:0.000001
[ Tue Jul  9 16:48:13 2024 ] 	Batch(1200/7879) done. Loss: 0.0089  lr:0.000001
[ Tue Jul  9 16:48:36 2024 ] 	Batch(1300/7879) done. Loss: 0.7621  lr:0.000001
[ Tue Jul  9 16:48:58 2024 ] 	Batch(1400/7879) done. Loss: 0.0949  lr:0.000001
[ Tue Jul  9 16:49:21 2024 ] 
Training: Epoch [106/120], Step [1499], Loss: 0.2071525752544403, Training Accuracy: 96.14166666666667
[ Tue Jul  9 16:49:21 2024 ] 	Batch(1500/7879) done. Loss: 0.2344  lr:0.000001
[ Tue Jul  9 16:49:44 2024 ] 	Batch(1600/7879) done. Loss: 0.0275  lr:0.000001
[ Tue Jul  9 16:50:06 2024 ] 	Batch(1700/7879) done. Loss: 0.1887  lr:0.000001
[ Tue Jul  9 16:50:29 2024 ] 	Batch(1800/7879) done. Loss: 0.0109  lr:0.000001
[ Tue Jul  9 16:50:52 2024 ] 	Batch(1900/7879) done. Loss: 0.1563  lr:0.000001
[ Tue Jul  9 16:51:14 2024 ] 
Training: Epoch [106/120], Step [1999], Loss: 0.035913947969675064, Training Accuracy: 96.06875000000001
[ Tue Jul  9 16:51:15 2024 ] 	Batch(2000/7879) done. Loss: 0.0748  lr:0.000001
[ Tue Jul  9 16:51:37 2024 ] 	Batch(2100/7879) done. Loss: 0.2540  lr:0.000001
[ Tue Jul  9 16:52:00 2024 ] 	Batch(2200/7879) done. Loss: 0.0182  lr:0.000001
[ Tue Jul  9 16:52:23 2024 ] 	Batch(2300/7879) done. Loss: 0.0515  lr:0.000001
[ Tue Jul  9 16:52:46 2024 ] 	Batch(2400/7879) done. Loss: 0.3172  lr:0.000001
[ Tue Jul  9 16:53:09 2024 ] 
Training: Epoch [106/120], Step [2499], Loss: 0.2767535150051117, Training Accuracy: 96.125
[ Tue Jul  9 16:53:09 2024 ] 	Batch(2500/7879) done. Loss: 0.1867  lr:0.000001
[ Tue Jul  9 16:53:32 2024 ] 	Batch(2600/7879) done. Loss: 0.1118  lr:0.000001
[ Tue Jul  9 16:53:55 2024 ] 	Batch(2700/7879) done. Loss: 0.0919  lr:0.000001
[ Tue Jul  9 16:54:18 2024 ] 	Batch(2800/7879) done. Loss: 0.0222  lr:0.000001
[ Tue Jul  9 16:54:40 2024 ] 	Batch(2900/7879) done. Loss: 0.2785  lr:0.000001
[ Tue Jul  9 16:55:03 2024 ] 
Training: Epoch [106/120], Step [2999], Loss: 0.17814241349697113, Training Accuracy: 96.18333333333334
[ Tue Jul  9 16:55:03 2024 ] 	Batch(3000/7879) done. Loss: 0.0877  lr:0.000001
[ Tue Jul  9 16:55:26 2024 ] 	Batch(3100/7879) done. Loss: 0.4341  lr:0.000001
[ Tue Jul  9 16:55:49 2024 ] 	Batch(3200/7879) done. Loss: 0.0410  lr:0.000001
[ Tue Jul  9 16:56:11 2024 ] 	Batch(3300/7879) done. Loss: 0.1432  lr:0.000001
[ Tue Jul  9 16:56:34 2024 ] 	Batch(3400/7879) done. Loss: 0.1458  lr:0.000001
[ Tue Jul  9 16:56:56 2024 ] 
Training: Epoch [106/120], Step [3499], Loss: 0.012282897718250751, Training Accuracy: 96.18928571428572
[ Tue Jul  9 16:56:57 2024 ] 	Batch(3500/7879) done. Loss: 0.2560  lr:0.000001
[ Tue Jul  9 16:57:20 2024 ] 	Batch(3600/7879) done. Loss: 0.1188  lr:0.000001
[ Tue Jul  9 16:57:42 2024 ] 	Batch(3700/7879) done. Loss: 0.0083  lr:0.000001
[ Tue Jul  9 16:58:05 2024 ] 	Batch(3800/7879) done. Loss: 0.1514  lr:0.000001
[ Tue Jul  9 16:58:28 2024 ] 	Batch(3900/7879) done. Loss: 0.0498  lr:0.000001
[ Tue Jul  9 16:58:50 2024 ] 
Training: Epoch [106/120], Step [3999], Loss: 0.21477468311786652, Training Accuracy: 96.1625
[ Tue Jul  9 16:58:50 2024 ] 	Batch(4000/7879) done. Loss: 0.0203  lr:0.000001
[ Tue Jul  9 16:59:13 2024 ] 	Batch(4100/7879) done. Loss: 0.4092  lr:0.000001
[ Tue Jul  9 16:59:36 2024 ] 	Batch(4200/7879) done. Loss: 0.1161  lr:0.000001
[ Tue Jul  9 16:59:59 2024 ] 	Batch(4300/7879) done. Loss: 0.0852  lr:0.000001
[ Tue Jul  9 17:00:21 2024 ] 	Batch(4400/7879) done. Loss: 0.0624  lr:0.000001
[ Tue Jul  9 17:00:44 2024 ] 
Training: Epoch [106/120], Step [4499], Loss: 0.06765946745872498, Training Accuracy: 96.15
[ Tue Jul  9 17:00:44 2024 ] 	Batch(4500/7879) done. Loss: 0.0101  lr:0.000001
[ Tue Jul  9 17:01:07 2024 ] 	Batch(4600/7879) done. Loss: 0.0355  lr:0.000001
[ Tue Jul  9 17:01:30 2024 ] 	Batch(4700/7879) done. Loss: 0.2638  lr:0.000001
[ Tue Jul  9 17:01:52 2024 ] 	Batch(4800/7879) done. Loss: 0.0121  lr:0.000001
[ Tue Jul  9 17:02:15 2024 ] 	Batch(4900/7879) done. Loss: 0.0218  lr:0.000001
[ Tue Jul  9 17:02:38 2024 ] 
Training: Epoch [106/120], Step [4999], Loss: 0.015596115961670876, Training Accuracy: 96.155
[ Tue Jul  9 17:02:38 2024 ] 	Batch(5000/7879) done. Loss: 0.0102  lr:0.000001
[ Tue Jul  9 17:03:01 2024 ] 	Batch(5100/7879) done. Loss: 0.0245  lr:0.000001
[ Tue Jul  9 17:03:23 2024 ] 	Batch(5200/7879) done. Loss: 0.0170  lr:0.000001
[ Tue Jul  9 17:03:46 2024 ] 	Batch(5300/7879) done. Loss: 0.0152  lr:0.000001
[ Tue Jul  9 17:04:09 2024 ] 	Batch(5400/7879) done. Loss: 0.0421  lr:0.000001
[ Tue Jul  9 17:04:31 2024 ] 
Training: Epoch [106/120], Step [5499], Loss: 0.01694094017148018, Training Accuracy: 96.1340909090909
[ Tue Jul  9 17:04:31 2024 ] 	Batch(5500/7879) done. Loss: 0.0379  lr:0.000001
[ Tue Jul  9 17:04:54 2024 ] 	Batch(5600/7879) done. Loss: 0.1744  lr:0.000001
[ Tue Jul  9 17:05:17 2024 ] 	Batch(5700/7879) done. Loss: 0.0200  lr:0.000001
[ Tue Jul  9 17:05:40 2024 ] 	Batch(5800/7879) done. Loss: 0.0522  lr:0.000001
[ Tue Jul  9 17:06:04 2024 ] 	Batch(5900/7879) done. Loss: 0.0279  lr:0.000001
[ Tue Jul  9 17:06:27 2024 ] 
Training: Epoch [106/120], Step [5999], Loss: 0.06772306561470032, Training Accuracy: 96.17291666666667
[ Tue Jul  9 17:06:27 2024 ] 	Batch(6000/7879) done. Loss: 0.4158  lr:0.000001
[ Tue Jul  9 17:06:50 2024 ] 	Batch(6100/7879) done. Loss: 0.1692  lr:0.000001
[ Tue Jul  9 17:07:13 2024 ] 	Batch(6200/7879) done. Loss: 0.0432  lr:0.000001
[ Tue Jul  9 17:07:35 2024 ] 	Batch(6300/7879) done. Loss: 0.1024  lr:0.000001
[ Tue Jul  9 17:07:58 2024 ] 	Batch(6400/7879) done. Loss: 0.1196  lr:0.000001
[ Tue Jul  9 17:08:21 2024 ] 
Training: Epoch [106/120], Step [6499], Loss: 0.013016670942306519, Training Accuracy: 96.16923076923078
[ Tue Jul  9 17:08:21 2024 ] 	Batch(6500/7879) done. Loss: 0.1362  lr:0.000001
[ Tue Jul  9 17:08:45 2024 ] 	Batch(6600/7879) done. Loss: 0.0128  lr:0.000001
[ Tue Jul  9 17:09:08 2024 ] 	Batch(6700/7879) done. Loss: 0.1069  lr:0.000001
[ Tue Jul  9 17:09:32 2024 ] 	Batch(6800/7879) done. Loss: 0.1148  lr:0.000001
[ Tue Jul  9 17:09:54 2024 ] 	Batch(6900/7879) done. Loss: 0.2265  lr:0.000001
[ Tue Jul  9 17:10:17 2024 ] 
Training: Epoch [106/120], Step [6999], Loss: 0.1274941861629486, Training Accuracy: 96.16607142857143
[ Tue Jul  9 17:10:17 2024 ] 	Batch(7000/7879) done. Loss: 0.0772  lr:0.000001
[ Tue Jul  9 17:10:39 2024 ] 	Batch(7100/7879) done. Loss: 0.3910  lr:0.000001
[ Tue Jul  9 17:11:02 2024 ] 	Batch(7200/7879) done. Loss: 0.0300  lr:0.000001
[ Tue Jul  9 17:11:25 2024 ] 	Batch(7300/7879) done. Loss: 0.0278  lr:0.000001
[ Tue Jul  9 17:11:47 2024 ] 	Batch(7400/7879) done. Loss: 0.1812  lr:0.000001
[ Tue Jul  9 17:12:10 2024 ] 
Training: Epoch [106/120], Step [7499], Loss: 0.09966709464788437, Training Accuracy: 96.15833333333333
[ Tue Jul  9 17:12:10 2024 ] 	Batch(7500/7879) done. Loss: 0.0345  lr:0.000001
[ Tue Jul  9 17:12:33 2024 ] 	Batch(7600/7879) done. Loss: 0.1730  lr:0.000001
[ Tue Jul  9 17:12:55 2024 ] 	Batch(7700/7879) done. Loss: 0.0685  lr:0.000001
[ Tue Jul  9 17:13:18 2024 ] 	Batch(7800/7879) done. Loss: 0.0368  lr:0.000001
[ Tue Jul  9 17:13:36 2024 ] 	Mean training loss: 0.1407.
[ Tue Jul  9 17:13:36 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 17:13:36 2024 ] Training epoch: 108
[ Tue Jul  9 17:13:36 2024 ] 	Batch(0/7879) done. Loss: 0.0143  lr:0.000001
[ Tue Jul  9 17:13:59 2024 ] 	Batch(100/7879) done. Loss: 0.0022  lr:0.000001
[ Tue Jul  9 17:14:22 2024 ] 	Batch(200/7879) done. Loss: 0.0269  lr:0.000001
[ Tue Jul  9 17:14:44 2024 ] 	Batch(300/7879) done. Loss: 0.0272  lr:0.000001
[ Tue Jul  9 17:15:07 2024 ] 	Batch(400/7879) done. Loss: 0.0922  lr:0.000001
[ Tue Jul  9 17:15:29 2024 ] 
Training: Epoch [107/120], Step [499], Loss: 0.21984948217868805, Training Accuracy: 96.39999999999999
[ Tue Jul  9 17:15:29 2024 ] 	Batch(500/7879) done. Loss: 0.4804  lr:0.000001
[ Tue Jul  9 17:15:52 2024 ] 	Batch(600/7879) done. Loss: 0.0267  lr:0.000001
[ Tue Jul  9 17:16:15 2024 ] 	Batch(700/7879) done. Loss: 0.0135  lr:0.000001
[ Tue Jul  9 17:16:37 2024 ] 	Batch(800/7879) done. Loss: 0.1003  lr:0.000001
[ Tue Jul  9 17:17:00 2024 ] 	Batch(900/7879) done. Loss: 0.0882  lr:0.000001
[ Tue Jul  9 17:17:22 2024 ] 
Training: Epoch [107/120], Step [999], Loss: 0.03594789281487465, Training Accuracy: 96.33749999999999
[ Tue Jul  9 17:17:22 2024 ] 	Batch(1000/7879) done. Loss: 0.0131  lr:0.000001
[ Tue Jul  9 17:17:45 2024 ] 	Batch(1100/7879) done. Loss: 0.1159  lr:0.000001
[ Tue Jul  9 17:18:08 2024 ] 	Batch(1200/7879) done. Loss: 0.0224  lr:0.000001
[ Tue Jul  9 17:18:31 2024 ] 	Batch(1300/7879) done. Loss: 0.0190  lr:0.000001
[ Tue Jul  9 17:18:54 2024 ] 	Batch(1400/7879) done. Loss: 0.2377  lr:0.000001
[ Tue Jul  9 17:19:16 2024 ] 
Training: Epoch [107/120], Step [1499], Loss: 0.563549816608429, Training Accuracy: 96.56666666666666
[ Tue Jul  9 17:19:16 2024 ] 	Batch(1500/7879) done. Loss: 0.1025  lr:0.000001
[ Tue Jul  9 17:19:39 2024 ] 	Batch(1600/7879) done. Loss: 0.0368  lr:0.000001
[ Tue Jul  9 17:20:02 2024 ] 	Batch(1700/7879) done. Loss: 0.0520  lr:0.000001
[ Tue Jul  9 17:20:24 2024 ] 	Batch(1800/7879) done. Loss: 0.0046  lr:0.000001
[ Tue Jul  9 17:20:47 2024 ] 	Batch(1900/7879) done. Loss: 0.0786  lr:0.000001
[ Tue Jul  9 17:21:09 2024 ] 
Training: Epoch [107/120], Step [1999], Loss: 0.28418126702308655, Training Accuracy: 96.53125
[ Tue Jul  9 17:21:10 2024 ] 	Batch(2000/7879) done. Loss: 0.0555  lr:0.000001
[ Tue Jul  9 17:21:32 2024 ] 	Batch(2100/7879) done. Loss: 0.0870  lr:0.000001
[ Tue Jul  9 17:21:55 2024 ] 	Batch(2200/7879) done. Loss: 0.0427  lr:0.000001
[ Tue Jul  9 17:22:17 2024 ] 	Batch(2300/7879) done. Loss: 0.0160  lr:0.000001
[ Tue Jul  9 17:22:40 2024 ] 	Batch(2400/7879) done. Loss: 0.0263  lr:0.000001
[ Tue Jul  9 17:23:03 2024 ] 
Training: Epoch [107/120], Step [2499], Loss: 0.08510351926088333, Training Accuracy: 96.5
[ Tue Jul  9 17:23:03 2024 ] 	Batch(2500/7879) done. Loss: 0.1499  lr:0.000001
[ Tue Jul  9 17:23:25 2024 ] 	Batch(2600/7879) done. Loss: 0.2524  lr:0.000001
[ Tue Jul  9 17:23:48 2024 ] 	Batch(2700/7879) done. Loss: 0.2536  lr:0.000001
[ Tue Jul  9 17:24:11 2024 ] 	Batch(2800/7879) done. Loss: 0.0303  lr:0.000001
[ Tue Jul  9 17:24:34 2024 ] 	Batch(2900/7879) done. Loss: 0.0102  lr:0.000001
[ Tue Jul  9 17:24:57 2024 ] 
Training: Epoch [107/120], Step [2999], Loss: 0.08746987581253052, Training Accuracy: 96.42500000000001
[ Tue Jul  9 17:24:57 2024 ] 	Batch(3000/7879) done. Loss: 0.1282  lr:0.000001
[ Tue Jul  9 17:25:20 2024 ] 	Batch(3100/7879) done. Loss: 0.0651  lr:0.000001
[ Tue Jul  9 17:25:43 2024 ] 	Batch(3200/7879) done. Loss: 0.1415  lr:0.000001
[ Tue Jul  9 17:26:06 2024 ] 	Batch(3300/7879) done. Loss: 0.0700  lr:0.000001
[ Tue Jul  9 17:26:28 2024 ] 	Batch(3400/7879) done. Loss: 0.0045  lr:0.000001
[ Tue Jul  9 17:26:51 2024 ] 
Training: Epoch [107/120], Step [3499], Loss: 0.07173381000757217, Training Accuracy: 96.38571428571429
[ Tue Jul  9 17:26:51 2024 ] 	Batch(3500/7879) done. Loss: 0.0709  lr:0.000001
[ Tue Jul  9 17:27:14 2024 ] 	Batch(3600/7879) done. Loss: 0.0700  lr:0.000001
[ Tue Jul  9 17:27:37 2024 ] 	Batch(3700/7879) done. Loss: 0.0511  lr:0.000001
[ Tue Jul  9 17:27:59 2024 ] 	Batch(3800/7879) done. Loss: 0.0008  lr:0.000001
[ Tue Jul  9 17:28:22 2024 ] 	Batch(3900/7879) done. Loss: 0.6136  lr:0.000001
[ Tue Jul  9 17:28:45 2024 ] 
Training: Epoch [107/120], Step [3999], Loss: 0.07235798239707947, Training Accuracy: 96.38125
[ Tue Jul  9 17:28:45 2024 ] 	Batch(4000/7879) done. Loss: 0.1201  lr:0.000001
[ Tue Jul  9 17:29:09 2024 ] 	Batch(4100/7879) done. Loss: 0.0223  lr:0.000001
[ Tue Jul  9 17:29:32 2024 ] 	Batch(4200/7879) done. Loss: 0.3114  lr:0.000001
[ Tue Jul  9 17:29:56 2024 ] 	Batch(4300/7879) done. Loss: 0.3454  lr:0.000001
[ Tue Jul  9 17:30:19 2024 ] 	Batch(4400/7879) done. Loss: 0.2511  lr:0.000001
[ Tue Jul  9 17:30:42 2024 ] 
Training: Epoch [107/120], Step [4499], Loss: 0.00814739242196083, Training Accuracy: 96.31388888888888
[ Tue Jul  9 17:30:42 2024 ] 	Batch(4500/7879) done. Loss: 0.0339  lr:0.000001
[ Tue Jul  9 17:31:05 2024 ] 	Batch(4600/7879) done. Loss: 0.1041  lr:0.000001
[ Tue Jul  9 17:31:27 2024 ] 	Batch(4700/7879) done. Loss: 0.3714  lr:0.000001
[ Tue Jul  9 17:31:50 2024 ] 	Batch(4800/7879) done. Loss: 0.0620  lr:0.000001
[ Tue Jul  9 17:32:13 2024 ] 	Batch(4900/7879) done. Loss: 0.2315  lr:0.000001
[ Tue Jul  9 17:32:35 2024 ] 
Training: Epoch [107/120], Step [4999], Loss: 0.07162488996982574, Training Accuracy: 96.28999999999999
[ Tue Jul  9 17:32:36 2024 ] 	Batch(5000/7879) done. Loss: 0.0054  lr:0.000001
[ Tue Jul  9 17:32:58 2024 ] 	Batch(5100/7879) done. Loss: 0.0145  lr:0.000001
[ Tue Jul  9 17:33:21 2024 ] 	Batch(5200/7879) done. Loss: 0.2775  lr:0.000001
[ Tue Jul  9 17:33:44 2024 ] 	Batch(5300/7879) done. Loss: 0.0166  lr:0.000001
[ Tue Jul  9 17:34:07 2024 ] 	Batch(5400/7879) done. Loss: 0.7105  lr:0.000001
[ Tue Jul  9 17:34:29 2024 ] 
Training: Epoch [107/120], Step [5499], Loss: 0.2917283773422241, Training Accuracy: 96.29545454545455
[ Tue Jul  9 17:34:29 2024 ] 	Batch(5500/7879) done. Loss: 0.0361  lr:0.000001
[ Tue Jul  9 17:34:52 2024 ] 	Batch(5600/7879) done. Loss: 0.1022  lr:0.000001
[ Tue Jul  9 17:35:15 2024 ] 	Batch(5700/7879) done. Loss: 0.2181  lr:0.000001
[ Tue Jul  9 17:35:38 2024 ] 	Batch(5800/7879) done. Loss: 0.0350  lr:0.000001
[ Tue Jul  9 17:36:00 2024 ] 	Batch(5900/7879) done. Loss: 0.0920  lr:0.000001
[ Tue Jul  9 17:36:23 2024 ] 
Training: Epoch [107/120], Step [5999], Loss: 0.14194795489311218, Training Accuracy: 96.32708333333333
[ Tue Jul  9 17:36:23 2024 ] 	Batch(6000/7879) done. Loss: 0.0080  lr:0.000001
[ Tue Jul  9 17:36:46 2024 ] 	Batch(6100/7879) done. Loss: 0.0110  lr:0.000001
[ Tue Jul  9 17:37:09 2024 ] 	Batch(6200/7879) done. Loss: 0.2189  lr:0.000001
[ Tue Jul  9 17:37:31 2024 ] 	Batch(6300/7879) done. Loss: 0.5819  lr:0.000001
[ Tue Jul  9 17:37:54 2024 ] 	Batch(6400/7879) done. Loss: 0.2186  lr:0.000001
[ Tue Jul  9 17:38:18 2024 ] 
Training: Epoch [107/120], Step [6499], Loss: 0.09210243821144104, Training Accuracy: 96.33461538461539
[ Tue Jul  9 17:38:18 2024 ] 	Batch(6500/7879) done. Loss: 0.0936  lr:0.000001
[ Tue Jul  9 17:38:41 2024 ] 	Batch(6600/7879) done. Loss: 0.1704  lr:0.000001
[ Tue Jul  9 17:39:05 2024 ] 	Batch(6700/7879) done. Loss: 0.0431  lr:0.000001
[ Tue Jul  9 17:39:28 2024 ] 	Batch(6800/7879) done. Loss: 0.0351  lr:0.000001
[ Tue Jul  9 17:39:52 2024 ] 	Batch(6900/7879) done. Loss: 0.0162  lr:0.000001
[ Tue Jul  9 17:40:15 2024 ] 
Training: Epoch [107/120], Step [6999], Loss: 0.04992945119738579, Training Accuracy: 96.33214285714286
[ Tue Jul  9 17:40:15 2024 ] 	Batch(7000/7879) done. Loss: 0.0439  lr:0.000001
[ Tue Jul  9 17:40:39 2024 ] 	Batch(7100/7879) done. Loss: 0.1388  lr:0.000001
[ Tue Jul  9 17:41:02 2024 ] 	Batch(7200/7879) done. Loss: 0.0809  lr:0.000001
[ Tue Jul  9 17:41:26 2024 ] 	Batch(7300/7879) done. Loss: 0.0858  lr:0.000001
[ Tue Jul  9 17:41:49 2024 ] 	Batch(7400/7879) done. Loss: 0.0392  lr:0.000001
[ Tue Jul  9 17:42:12 2024 ] 
Training: Epoch [107/120], Step [7499], Loss: 0.12494228035211563, Training Accuracy: 96.32666666666667
[ Tue Jul  9 17:42:13 2024 ] 	Batch(7500/7879) done. Loss: 0.0262  lr:0.000001
[ Tue Jul  9 17:42:36 2024 ] 	Batch(7600/7879) done. Loss: 0.2988  lr:0.000001
[ Tue Jul  9 17:43:00 2024 ] 	Batch(7700/7879) done. Loss: 0.0807  lr:0.000001
[ Tue Jul  9 17:43:23 2024 ] 	Batch(7800/7879) done. Loss: 0.0278  lr:0.000001
[ Tue Jul  9 17:43:42 2024 ] 	Mean training loss: 0.1390.
[ Tue Jul  9 17:43:42 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 17:43:42 2024 ] Training epoch: 109
[ Tue Jul  9 17:43:42 2024 ] 	Batch(0/7879) done. Loss: 0.5839  lr:0.000001
[ Tue Jul  9 17:44:05 2024 ] 	Batch(100/7879) done. Loss: 0.0115  lr:0.000001
[ Tue Jul  9 17:44:28 2024 ] 	Batch(200/7879) done. Loss: 0.5396  lr:0.000001
[ Tue Jul  9 17:44:50 2024 ] 	Batch(300/7879) done. Loss: 0.2631  lr:0.000001
[ Tue Jul  9 17:45:13 2024 ] 	Batch(400/7879) done. Loss: 0.0382  lr:0.000001
[ Tue Jul  9 17:45:36 2024 ] 
Training: Epoch [108/120], Step [499], Loss: 0.12282675504684448, Training Accuracy: 95.825
[ Tue Jul  9 17:45:36 2024 ] 	Batch(500/7879) done. Loss: 0.0117  lr:0.000001
[ Tue Jul  9 17:45:58 2024 ] 	Batch(600/7879) done. Loss: 0.0384  lr:0.000001
[ Tue Jul  9 17:46:21 2024 ] 	Batch(700/7879) done. Loss: 0.1722  lr:0.000001
[ Tue Jul  9 17:46:44 2024 ] 	Batch(800/7879) done. Loss: 0.0741  lr:0.000001
[ Tue Jul  9 17:47:07 2024 ] 	Batch(900/7879) done. Loss: 0.1759  lr:0.000001
[ Tue Jul  9 17:47:29 2024 ] 
Training: Epoch [108/120], Step [999], Loss: 0.1735021471977234, Training Accuracy: 96.46249999999999
[ Tue Jul  9 17:47:29 2024 ] 	Batch(1000/7879) done. Loss: 0.3575  lr:0.000001
[ Tue Jul  9 17:47:52 2024 ] 	Batch(1100/7879) done. Loss: 0.4287  lr:0.000001
[ Tue Jul  9 17:48:15 2024 ] 	Batch(1200/7879) done. Loss: 0.0324  lr:0.000001
[ Tue Jul  9 17:48:38 2024 ] 	Batch(1300/7879) done. Loss: 0.0490  lr:0.000001
[ Tue Jul  9 17:49:00 2024 ] 	Batch(1400/7879) done. Loss: 0.1715  lr:0.000001
[ Tue Jul  9 17:49:23 2024 ] 
Training: Epoch [108/120], Step [1499], Loss: 0.018203727900981903, Training Accuracy: 96.31666666666666
[ Tue Jul  9 17:49:23 2024 ] 	Batch(1500/7879) done. Loss: 0.1927  lr:0.000001
[ Tue Jul  9 17:49:46 2024 ] 	Batch(1600/7879) done. Loss: 0.4196  lr:0.000001
[ Tue Jul  9 17:50:08 2024 ] 	Batch(1700/7879) done. Loss: 0.0499  lr:0.000001
[ Tue Jul  9 17:50:31 2024 ] 	Batch(1800/7879) done. Loss: 0.0636  lr:0.000001
[ Tue Jul  9 17:50:54 2024 ] 	Batch(1900/7879) done. Loss: 0.4281  lr:0.000001
[ Tue Jul  9 17:51:16 2024 ] 
Training: Epoch [108/120], Step [1999], Loss: 0.06834570318460464, Training Accuracy: 96.25625
[ Tue Jul  9 17:51:17 2024 ] 	Batch(2000/7879) done. Loss: 0.0120  lr:0.000001
[ Tue Jul  9 17:51:39 2024 ] 	Batch(2100/7879) done. Loss: 0.0216  lr:0.000001
[ Tue Jul  9 17:52:02 2024 ] 	Batch(2200/7879) done. Loss: 0.0310  lr:0.000001
[ Tue Jul  9 17:52:24 2024 ] 	Batch(2300/7879) done. Loss: 0.0290  lr:0.000001
[ Tue Jul  9 17:52:47 2024 ] 	Batch(2400/7879) done. Loss: 0.0383  lr:0.000001
[ Tue Jul  9 17:53:10 2024 ] 
Training: Epoch [108/120], Step [2499], Loss: 0.1681211292743683, Training Accuracy: 96.28
[ Tue Jul  9 17:53:10 2024 ] 	Batch(2500/7879) done. Loss: 0.5576  lr:0.000001
[ Tue Jul  9 17:53:32 2024 ] 	Batch(2600/7879) done. Loss: 0.0575  lr:0.000001
[ Tue Jul  9 17:53:55 2024 ] 	Batch(2700/7879) done. Loss: 0.0969  lr:0.000001
[ Tue Jul  9 17:54:18 2024 ] 	Batch(2800/7879) done. Loss: 0.2853  lr:0.000001
[ Tue Jul  9 17:54:41 2024 ] 	Batch(2900/7879) done. Loss: 0.0046  lr:0.000001
[ Tue Jul  9 17:55:03 2024 ] 
Training: Epoch [108/120], Step [2999], Loss: 0.03820023685693741, Training Accuracy: 96.275
[ Tue Jul  9 17:55:03 2024 ] 	Batch(3000/7879) done. Loss: 0.0465  lr:0.000001
[ Tue Jul  9 17:55:26 2024 ] 	Batch(3100/7879) done. Loss: 0.0390  lr:0.000001
[ Tue Jul  9 17:55:49 2024 ] 	Batch(3200/7879) done. Loss: 0.0952  lr:0.000001
[ Tue Jul  9 17:56:12 2024 ] 	Batch(3300/7879) done. Loss: 0.0152  lr:0.000001
[ Tue Jul  9 17:56:36 2024 ] 	Batch(3400/7879) done. Loss: 0.0282  lr:0.000001
[ Tue Jul  9 17:56:59 2024 ] 
Training: Epoch [108/120], Step [3499], Loss: 0.012583121657371521, Training Accuracy: 96.28571428571429
[ Tue Jul  9 17:56:59 2024 ] 	Batch(3500/7879) done. Loss: 0.1430  lr:0.000001
[ Tue Jul  9 17:57:22 2024 ] 	Batch(3600/7879) done. Loss: 0.3419  lr:0.000001
[ Tue Jul  9 17:57:44 2024 ] 	Batch(3700/7879) done. Loss: 0.0665  lr:0.000001
[ Tue Jul  9 17:58:07 2024 ] 	Batch(3800/7879) done. Loss: 0.2567  lr:0.000001
[ Tue Jul  9 17:58:29 2024 ] 	Batch(3900/7879) done. Loss: 0.0637  lr:0.000001
[ Tue Jul  9 17:58:52 2024 ] 
Training: Epoch [108/120], Step [3999], Loss: 0.332868367433548, Training Accuracy: 96.215625
[ Tue Jul  9 17:58:52 2024 ] 	Batch(4000/7879) done. Loss: 0.0158  lr:0.000001
[ Tue Jul  9 17:59:15 2024 ] 	Batch(4100/7879) done. Loss: 0.0902  lr:0.000001
[ Tue Jul  9 17:59:37 2024 ] 	Batch(4200/7879) done. Loss: 0.1525  lr:0.000001
[ Tue Jul  9 18:00:00 2024 ] 	Batch(4300/7879) done. Loss: 0.1426  lr:0.000001
[ Tue Jul  9 18:00:22 2024 ] 	Batch(4400/7879) done. Loss: 0.2853  lr:0.000001
[ Tue Jul  9 18:00:45 2024 ] 
Training: Epoch [108/120], Step [4499], Loss: 0.43769994378089905, Training Accuracy: 96.21388888888889
[ Tue Jul  9 18:00:45 2024 ] 	Batch(4500/7879) done. Loss: 0.1065  lr:0.000001
[ Tue Jul  9 18:01:08 2024 ] 	Batch(4600/7879) done. Loss: 0.0635  lr:0.000001
[ Tue Jul  9 18:01:30 2024 ] 	Batch(4700/7879) done. Loss: 0.0357  lr:0.000001
[ Tue Jul  9 18:01:53 2024 ] 	Batch(4800/7879) done. Loss: 0.0368  lr:0.000001
[ Tue Jul  9 18:02:16 2024 ] 	Batch(4900/7879) done. Loss: 0.2293  lr:0.000001
[ Tue Jul  9 18:02:38 2024 ] 
Training: Epoch [108/120], Step [4999], Loss: 0.02163330279290676, Training Accuracy: 96.2025
[ Tue Jul  9 18:02:38 2024 ] 	Batch(5000/7879) done. Loss: 0.0776  lr:0.000001
[ Tue Jul  9 18:03:01 2024 ] 	Batch(5100/7879) done. Loss: 0.3340  lr:0.000001
[ Tue Jul  9 18:03:23 2024 ] 	Batch(5200/7879) done. Loss: 0.0174  lr:0.000001
[ Tue Jul  9 18:03:47 2024 ] 	Batch(5300/7879) done. Loss: 0.1285  lr:0.000001
[ Tue Jul  9 18:04:10 2024 ] 	Batch(5400/7879) done. Loss: 0.1297  lr:0.000001
[ Tue Jul  9 18:04:32 2024 ] 
Training: Epoch [108/120], Step [5499], Loss: 0.18572521209716797, Training Accuracy: 96.22500000000001
[ Tue Jul  9 18:04:33 2024 ] 	Batch(5500/7879) done. Loss: 0.4930  lr:0.000001
[ Tue Jul  9 18:04:56 2024 ] 	Batch(5600/7879) done. Loss: 0.0569  lr:0.000001
[ Tue Jul  9 18:05:18 2024 ] 	Batch(5700/7879) done. Loss: 0.0539  lr:0.000001
[ Tue Jul  9 18:05:41 2024 ] 	Batch(5800/7879) done. Loss: 0.0352  lr:0.000001
[ Tue Jul  9 18:06:03 2024 ] 	Batch(5900/7879) done. Loss: 0.0182  lr:0.000001
[ Tue Jul  9 18:06:26 2024 ] 
Training: Epoch [108/120], Step [5999], Loss: 0.05539896711707115, Training Accuracy: 96.23125
[ Tue Jul  9 18:06:26 2024 ] 	Batch(6000/7879) done. Loss: 0.2824  lr:0.000001
[ Tue Jul  9 18:06:49 2024 ] 	Batch(6100/7879) done. Loss: 0.0892  lr:0.000001
[ Tue Jul  9 18:07:11 2024 ] 	Batch(6200/7879) done. Loss: 0.3844  lr:0.000001
[ Tue Jul  9 18:07:34 2024 ] 	Batch(6300/7879) done. Loss: 0.6393  lr:0.000001
[ Tue Jul  9 18:07:57 2024 ] 	Batch(6400/7879) done. Loss: 0.0429  lr:0.000001
[ Tue Jul  9 18:08:19 2024 ] 
Training: Epoch [108/120], Step [6499], Loss: 0.09907728433609009, Training Accuracy: 96.20384615384616
[ Tue Jul  9 18:08:20 2024 ] 	Batch(6500/7879) done. Loss: 0.2240  lr:0.000001
[ Tue Jul  9 18:08:43 2024 ] 	Batch(6600/7879) done. Loss: 0.0989  lr:0.000001
[ Tue Jul  9 18:09:05 2024 ] 	Batch(6700/7879) done. Loss: 0.0762  lr:0.000001
[ Tue Jul  9 18:09:28 2024 ] 	Batch(6800/7879) done. Loss: 0.0404  lr:0.000001
[ Tue Jul  9 18:09:51 2024 ] 	Batch(6900/7879) done. Loss: 0.0758  lr:0.000001
[ Tue Jul  9 18:10:13 2024 ] 
Training: Epoch [108/120], Step [6999], Loss: 0.11400157958269119, Training Accuracy: 96.21964285714286
[ Tue Jul  9 18:10:13 2024 ] 	Batch(7000/7879) done. Loss: 0.3216  lr:0.000001
[ Tue Jul  9 18:10:36 2024 ] 	Batch(7100/7879) done. Loss: 0.1506  lr:0.000001
[ Tue Jul  9 18:10:59 2024 ] 	Batch(7200/7879) done. Loss: 0.0412  lr:0.000001
[ Tue Jul  9 18:11:21 2024 ] 	Batch(7300/7879) done. Loss: 0.1663  lr:0.000001
[ Tue Jul  9 18:11:44 2024 ] 	Batch(7400/7879) done. Loss: 0.3531  lr:0.000001
[ Tue Jul  9 18:12:06 2024 ] 
Training: Epoch [108/120], Step [7499], Loss: 0.005828628782182932, Training Accuracy: 96.20166666666667
[ Tue Jul  9 18:12:06 2024 ] 	Batch(7500/7879) done. Loss: 0.2106  lr:0.000001
[ Tue Jul  9 18:12:29 2024 ] 	Batch(7600/7879) done. Loss: 0.0081  lr:0.000001
[ Tue Jul  9 18:12:52 2024 ] 	Batch(7700/7879) done. Loss: 0.2441  lr:0.000001
[ Tue Jul  9 18:13:16 2024 ] 	Batch(7800/7879) done. Loss: 0.0091  lr:0.000001
[ Tue Jul  9 18:13:34 2024 ] 	Mean training loss: 0.1404.
[ Tue Jul  9 18:13:34 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 18:13:34 2024 ] Training epoch: 110
[ Tue Jul  9 18:13:35 2024 ] 	Batch(0/7879) done. Loss: 0.3210  lr:0.000001
[ Tue Jul  9 18:13:57 2024 ] 	Batch(100/7879) done. Loss: 0.0163  lr:0.000001
[ Tue Jul  9 18:14:20 2024 ] 	Batch(200/7879) done. Loss: 0.1725  lr:0.000001
[ Tue Jul  9 18:14:42 2024 ] 	Batch(300/7879) done. Loss: 0.0276  lr:0.000001
[ Tue Jul  9 18:15:05 2024 ] 	Batch(400/7879) done. Loss: 0.0907  lr:0.000001
[ Tue Jul  9 18:15:27 2024 ] 
Training: Epoch [109/120], Step [499], Loss: 0.1916884481906891, Training Accuracy: 96.475
[ Tue Jul  9 18:15:28 2024 ] 	Batch(500/7879) done. Loss: 0.0263  lr:0.000001
[ Tue Jul  9 18:15:50 2024 ] 	Batch(600/7879) done. Loss: 0.2321  lr:0.000001
[ Tue Jul  9 18:16:13 2024 ] 	Batch(700/7879) done. Loss: 0.2147  lr:0.000001
[ Tue Jul  9 18:16:36 2024 ] 	Batch(800/7879) done. Loss: 0.0524  lr:0.000001
[ Tue Jul  9 18:16:58 2024 ] 	Batch(900/7879) done. Loss: 0.1258  lr:0.000001
[ Tue Jul  9 18:17:20 2024 ] 
Training: Epoch [109/120], Step [999], Loss: 0.05107240751385689, Training Accuracy: 96.4125
[ Tue Jul  9 18:17:21 2024 ] 	Batch(1000/7879) done. Loss: 0.1607  lr:0.000001
[ Tue Jul  9 18:17:43 2024 ] 	Batch(1100/7879) done. Loss: 0.2311  lr:0.000001
[ Tue Jul  9 18:18:06 2024 ] 	Batch(1200/7879) done. Loss: 0.1737  lr:0.000001
[ Tue Jul  9 18:18:29 2024 ] 	Batch(1300/7879) done. Loss: 0.6924  lr:0.000001
[ Tue Jul  9 18:18:52 2024 ] 	Batch(1400/7879) done. Loss: 0.0373  lr:0.000001
[ Tue Jul  9 18:19:15 2024 ] 
Training: Epoch [109/120], Step [1499], Loss: 0.11440574377775192, Training Accuracy: 96.31666666666666
[ Tue Jul  9 18:19:15 2024 ] 	Batch(1500/7879) done. Loss: 0.0509  lr:0.000001
[ Tue Jul  9 18:19:38 2024 ] 	Batch(1600/7879) done. Loss: 0.4804  lr:0.000001
[ Tue Jul  9 18:20:00 2024 ] 	Batch(1700/7879) done. Loss: 0.4895  lr:0.000001
[ Tue Jul  9 18:20:23 2024 ] 	Batch(1800/7879) done. Loss: 0.0364  lr:0.000001
[ Tue Jul  9 18:20:46 2024 ] 	Batch(1900/7879) done. Loss: 0.3036  lr:0.000001
[ Tue Jul  9 18:21:08 2024 ] 
Training: Epoch [109/120], Step [1999], Loss: 0.1307123303413391, Training Accuracy: 96.275
[ Tue Jul  9 18:21:08 2024 ] 	Batch(2000/7879) done. Loss: 0.0223  lr:0.000001
[ Tue Jul  9 18:21:32 2024 ] 	Batch(2100/7879) done. Loss: 0.0875  lr:0.000001
[ Tue Jul  9 18:21:55 2024 ] 	Batch(2200/7879) done. Loss: 0.0249  lr:0.000001
[ Tue Jul  9 18:22:18 2024 ] 	Batch(2300/7879) done. Loss: 0.1805  lr:0.000001
[ Tue Jul  9 18:22:40 2024 ] 	Batch(2400/7879) done. Loss: 0.0405  lr:0.000001
[ Tue Jul  9 18:23:03 2024 ] 
Training: Epoch [109/120], Step [2499], Loss: 0.018635526299476624, Training Accuracy: 96.265
[ Tue Jul  9 18:23:03 2024 ] 	Batch(2500/7879) done. Loss: 0.0506  lr:0.000001
[ Tue Jul  9 18:23:26 2024 ] 	Batch(2600/7879) done. Loss: 0.0503  lr:0.000001
[ Tue Jul  9 18:23:48 2024 ] 	Batch(2700/7879) done. Loss: 0.1717  lr:0.000001
[ Tue Jul  9 18:24:11 2024 ] 	Batch(2800/7879) done. Loss: 0.0937  lr:0.000001
[ Tue Jul  9 18:24:33 2024 ] 	Batch(2900/7879) done. Loss: 0.1228  lr:0.000001
[ Tue Jul  9 18:24:56 2024 ] 
Training: Epoch [109/120], Step [2999], Loss: 0.05277620255947113, Training Accuracy: 96.12916666666666
[ Tue Jul  9 18:24:56 2024 ] 	Batch(3000/7879) done. Loss: 0.0432  lr:0.000001
[ Tue Jul  9 18:25:19 2024 ] 	Batch(3100/7879) done. Loss: 0.0201  lr:0.000001
[ Tue Jul  9 18:25:41 2024 ] 	Batch(3200/7879) done. Loss: 0.2353  lr:0.000001
[ Tue Jul  9 18:26:04 2024 ] 	Batch(3300/7879) done. Loss: 0.0016  lr:0.000001
[ Tue Jul  9 18:26:26 2024 ] 	Batch(3400/7879) done. Loss: 0.0100  lr:0.000001
[ Tue Jul  9 18:26:49 2024 ] 
Training: Epoch [109/120], Step [3499], Loss: 0.03276004269719124, Training Accuracy: 96.22500000000001
[ Tue Jul  9 18:26:49 2024 ] 	Batch(3500/7879) done. Loss: 0.0369  lr:0.000001
[ Tue Jul  9 18:27:12 2024 ] 	Batch(3600/7879) done. Loss: 0.0167  lr:0.000001
[ Tue Jul  9 18:27:35 2024 ] 	Batch(3700/7879) done. Loss: 0.1728  lr:0.000001
[ Tue Jul  9 18:27:57 2024 ] 	Batch(3800/7879) done. Loss: 0.0990  lr:0.000001
[ Tue Jul  9 18:28:20 2024 ] 	Batch(3900/7879) done. Loss: 0.6393  lr:0.000001
[ Tue Jul  9 18:28:42 2024 ] 
Training: Epoch [109/120], Step [3999], Loss: 0.2037549763917923, Training Accuracy: 96.16875
[ Tue Jul  9 18:28:43 2024 ] 	Batch(4000/7879) done. Loss: 0.3373  lr:0.000001
[ Tue Jul  9 18:29:06 2024 ] 	Batch(4100/7879) done. Loss: 0.4694  lr:0.000001
[ Tue Jul  9 18:29:29 2024 ] 	Batch(4200/7879) done. Loss: 0.3085  lr:0.000001
[ Tue Jul  9 18:29:53 2024 ] 	Batch(4300/7879) done. Loss: 0.2607  lr:0.000001
[ Tue Jul  9 18:30:16 2024 ] 	Batch(4400/7879) done. Loss: 0.0737  lr:0.000001
[ Tue Jul  9 18:30:38 2024 ] 
Training: Epoch [109/120], Step [4499], Loss: 0.03547344729304314, Training Accuracy: 96.09722222222223
[ Tue Jul  9 18:30:39 2024 ] 	Batch(4500/7879) done. Loss: 0.1544  lr:0.000001
[ Tue Jul  9 18:31:01 2024 ] 	Batch(4600/7879) done. Loss: 0.0069  lr:0.000001
[ Tue Jul  9 18:31:24 2024 ] 	Batch(4700/7879) done. Loss: 0.0269  lr:0.000001
[ Tue Jul  9 18:31:46 2024 ] 	Batch(4800/7879) done. Loss: 0.1494  lr:0.000001
[ Tue Jul  9 18:32:09 2024 ] 	Batch(4900/7879) done. Loss: 0.0769  lr:0.000001
[ Tue Jul  9 18:32:31 2024 ] 
Training: Epoch [109/120], Step [4999], Loss: 0.8291861414909363, Training Accuracy: 96.085
[ Tue Jul  9 18:32:31 2024 ] 	Batch(5000/7879) done. Loss: 0.1856  lr:0.000001
[ Tue Jul  9 18:32:54 2024 ] 	Batch(5100/7879) done. Loss: 0.0764  lr:0.000001
[ Tue Jul  9 18:33:17 2024 ] 	Batch(5200/7879) done. Loss: 0.1838  lr:0.000001
[ Tue Jul  9 18:33:39 2024 ] 	Batch(5300/7879) done. Loss: 0.5241  lr:0.000001
[ Tue Jul  9 18:34:02 2024 ] 	Batch(5400/7879) done. Loss: 0.1766  lr:0.000001
[ Tue Jul  9 18:34:24 2024 ] 
Training: Epoch [109/120], Step [5499], Loss: 0.32631516456604004, Training Accuracy: 96.08181818181818
[ Tue Jul  9 18:34:24 2024 ] 	Batch(5500/7879) done. Loss: 0.1977  lr:0.000001
[ Tue Jul  9 18:34:47 2024 ] 	Batch(5600/7879) done. Loss: 0.0614  lr:0.000001
[ Tue Jul  9 18:35:10 2024 ] 	Batch(5700/7879) done. Loss: 0.0350  lr:0.000001
[ Tue Jul  9 18:35:32 2024 ] 	Batch(5800/7879) done. Loss: 0.0735  lr:0.000001
[ Tue Jul  9 18:35:55 2024 ] 	Batch(5900/7879) done. Loss: 0.1159  lr:0.000001
[ Tue Jul  9 18:36:17 2024 ] 
Training: Epoch [109/120], Step [5999], Loss: 0.1892825961112976, Training Accuracy: 96.12291666666667
[ Tue Jul  9 18:36:18 2024 ] 	Batch(6000/7879) done. Loss: 0.0311  lr:0.000001
[ Tue Jul  9 18:36:40 2024 ] 	Batch(6100/7879) done. Loss: 0.0466  lr:0.000001
[ Tue Jul  9 18:37:03 2024 ] 	Batch(6200/7879) done. Loss: 0.0484  lr:0.000001
[ Tue Jul  9 18:37:25 2024 ] 	Batch(6300/7879) done. Loss: 0.0079  lr:0.000001
[ Tue Jul  9 18:37:48 2024 ] 	Batch(6400/7879) done. Loss: 0.1715  lr:0.000001
[ Tue Jul  9 18:38:10 2024 ] 
Training: Epoch [109/120], Step [6499], Loss: 0.05970657244324684, Training Accuracy: 96.1
[ Tue Jul  9 18:38:11 2024 ] 	Batch(6500/7879) done. Loss: 0.3142  lr:0.000001
[ Tue Jul  9 18:38:33 2024 ] 	Batch(6600/7879) done. Loss: 0.0218  lr:0.000001
[ Tue Jul  9 18:38:56 2024 ] 	Batch(6700/7879) done. Loss: 0.1752  lr:0.000001
[ Tue Jul  9 18:39:18 2024 ] 	Batch(6800/7879) done. Loss: 0.0764  lr:0.000001
[ Tue Jul  9 18:39:41 2024 ] 	Batch(6900/7879) done. Loss: 0.3237  lr:0.000001
[ Tue Jul  9 18:40:03 2024 ] 
Training: Epoch [109/120], Step [6999], Loss: 0.09143353998661041, Training Accuracy: 96.08571428571429
[ Tue Jul  9 18:40:04 2024 ] 	Batch(7000/7879) done. Loss: 0.0237  lr:0.000001
[ Tue Jul  9 18:40:26 2024 ] 	Batch(7100/7879) done. Loss: 0.0280  lr:0.000001
[ Tue Jul  9 18:40:49 2024 ] 	Batch(7200/7879) done. Loss: 0.0366  lr:0.000001
[ Tue Jul  9 18:41:12 2024 ] 	Batch(7300/7879) done. Loss: 0.0214  lr:0.000001
[ Tue Jul  9 18:41:34 2024 ] 	Batch(7400/7879) done. Loss: 0.2378  lr:0.000001
[ Tue Jul  9 18:41:57 2024 ] 
Training: Epoch [109/120], Step [7499], Loss: 0.0469391904771328, Training Accuracy: 96.03333333333333
[ Tue Jul  9 18:41:57 2024 ] 	Batch(7500/7879) done. Loss: 0.0210  lr:0.000001
[ Tue Jul  9 18:42:20 2024 ] 	Batch(7600/7879) done. Loss: 0.0272  lr:0.000001
[ Tue Jul  9 18:42:43 2024 ] 	Batch(7700/7879) done. Loss: 0.2037  lr:0.000001
[ Tue Jul  9 18:43:05 2024 ] 	Batch(7800/7879) done. Loss: 0.1383  lr:0.000001
[ Tue Jul  9 18:43:23 2024 ] 	Mean training loss: 0.1435.
[ Tue Jul  9 18:43:23 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 18:43:23 2024 ] Eval epoch: 110
[ Tue Jul  9 18:49:20 2024 ] 	Mean val loss of 6365 batches: 0.998231725372532.
[ Tue Jul  9 18:49:20 2024 ] 
Validation: Epoch [109/120], Samples [39413.0/50919], Loss: 0.3947819769382477, Validation Accuracy: 77.40332685245194
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 1 : 193 / 275 = 70 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 2 : 228 / 273 = 83 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 3 : 229 / 273 = 83 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 4 : 230 / 275 = 83 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 5 : 224 / 275 = 81 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 6 : 222 / 275 = 80 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 7 : 253 / 273 = 92 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 8 : 266 / 273 = 97 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 9 : 199 / 273 = 72 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 10 : 117 / 273 = 42 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 11 : 151 / 272 = 55 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 12 : 219 / 271 = 80 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 13 : 266 / 275 = 96 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 14 : 255 / 276 = 92 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 15 : 209 / 273 = 76 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 16 : 174 / 274 = 63 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 17 : 240 / 273 = 87 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 18 : 235 / 274 = 85 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 19 : 245 / 272 = 90 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 20 : 253 / 273 = 92 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 21 : 234 / 274 = 85 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 22 : 253 / 274 = 92 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 23 : 252 / 276 = 91 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 24 : 235 / 274 = 85 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 25 : 265 / 275 = 96 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 26 : 270 / 276 = 97 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 27 : 227 / 275 = 82 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 28 : 172 / 275 = 62 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 29 : 152 / 275 = 55 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 30 : 175 / 276 = 63 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 31 : 232 / 276 = 84 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 32 : 246 / 276 = 89 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 33 : 234 / 276 = 84 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 34 : 241 / 276 = 87 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 35 : 240 / 275 = 87 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 36 : 232 / 276 = 84 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 37 : 250 / 276 = 90 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 38 : 238 / 276 = 86 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 39 : 238 / 276 = 86 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 40 : 197 / 276 = 71 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 41 : 262 / 276 = 94 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 42 : 251 / 275 = 91 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 43 : 200 / 276 = 72 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 44 : 255 / 276 = 92 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 45 : 261 / 276 = 94 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 46 : 233 / 276 = 84 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 47 : 201 / 275 = 73 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 48 : 220 / 275 = 80 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 49 : 219 / 274 = 79 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 50 : 233 / 276 = 84 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 51 : 258 / 276 = 93 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 52 : 244 / 276 = 88 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 53 : 236 / 276 = 85 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 54 : 253 / 274 = 92 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 55 : 241 / 276 = 87 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 56 : 244 / 275 = 88 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 57 : 267 / 276 = 96 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 58 : 265 / 273 = 97 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 59 : 244 / 276 = 88 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 60 : 472 / 561 = 84 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 61 : 468 / 566 = 82 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 62 : 416 / 572 = 72 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 63 : 472 / 570 = 82 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 64 : 419 / 574 = 72 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 65 : 503 / 573 = 87 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 66 : 402 / 573 = 70 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 67 : 397 / 575 = 69 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 68 : 386 / 575 = 67 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 69 : 464 / 575 = 80 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 70 : 220 / 575 = 38 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 71 : 207 / 575 = 36 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 72 : 85 / 571 = 14 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 73 : 250 / 570 = 43 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 74 : 364 / 569 = 63 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 75 : 353 / 573 = 61 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 76 : 370 / 574 = 64 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 77 : 387 / 573 = 67 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 78 : 450 / 575 = 78 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 79 : 540 / 574 = 94 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 80 : 468 / 573 = 81 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 81 : 303 / 575 = 52 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 82 : 327 / 575 = 56 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 83 : 286 / 572 = 50 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 84 : 443 / 574 = 77 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 85 : 360 / 574 = 62 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 86 : 506 / 575 = 88 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 87 : 492 / 576 = 85 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 88 : 412 / 575 = 71 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 89 : 439 / 576 = 76 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 90 : 254 / 574 = 44 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 91 : 445 / 568 = 78 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 92 : 412 / 576 = 71 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 93 : 397 / 573 = 69 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 94 : 519 / 574 = 90 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 95 : 534 / 575 = 92 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 96 : 555 / 575 = 96 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 97 : 551 / 574 = 95 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 98 : 532 / 575 = 92 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 99 : 512 / 574 = 89 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 100 : 452 / 574 = 78 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 101 : 536 / 574 = 93 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 102 : 357 / 575 = 62 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 103 : 493 / 576 = 85 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 104 : 296 / 575 = 51 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 105 : 278 / 575 = 48 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 106 : 334 / 576 = 57 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 107 : 505 / 576 = 87 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 108 : 471 / 575 = 81 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 109 : 414 / 575 = 72 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 110 : 513 / 575 = 89 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 111 : 543 / 576 = 94 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 112 : 542 / 575 = 94 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 113 : 519 / 576 = 90 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 114 : 510 / 576 = 88 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 115 : 519 / 576 = 90 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 116 : 482 / 575 = 83 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 117 : 486 / 575 = 84 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 118 : 469 / 575 = 81 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 119 : 499 / 576 = 86 %
[ Tue Jul  9 18:49:20 2024 ] Accuracy of 120 : 245 / 274 = 89 %
[ Tue Jul  9 18:49:20 2024 ] Training epoch: 111
[ Tue Jul  9 18:49:20 2024 ] 	Batch(0/7879) done. Loss: 0.1680  lr:0.000001
[ Tue Jul  9 18:49:43 2024 ] 	Batch(100/7879) done. Loss: 0.0039  lr:0.000001
[ Tue Jul  9 18:50:06 2024 ] 	Batch(200/7879) done. Loss: 0.4534  lr:0.000001
[ Tue Jul  9 18:50:28 2024 ] 	Batch(300/7879) done. Loss: 0.0329  lr:0.000001
[ Tue Jul  9 18:50:51 2024 ] 	Batch(400/7879) done. Loss: 0.0191  lr:0.000001
[ Tue Jul  9 18:51:14 2024 ] 
Training: Epoch [110/120], Step [499], Loss: 0.2372329980134964, Training Accuracy: 96.3
[ Tue Jul  9 18:51:14 2024 ] 	Batch(500/7879) done. Loss: 0.0085  lr:0.000001
[ Tue Jul  9 18:51:37 2024 ] 	Batch(600/7879) done. Loss: 0.0991  lr:0.000001
[ Tue Jul  9 18:51:59 2024 ] 	Batch(700/7879) done. Loss: 0.3360  lr:0.000001
[ Tue Jul  9 18:52:22 2024 ] 	Batch(800/7879) done. Loss: 0.0764  lr:0.000001
[ Tue Jul  9 18:52:45 2024 ] 	Batch(900/7879) done. Loss: 0.0859  lr:0.000001
[ Tue Jul  9 18:53:08 2024 ] 
Training: Epoch [110/120], Step [999], Loss: 0.1785970777273178, Training Accuracy: 96.28750000000001
[ Tue Jul  9 18:53:08 2024 ] 	Batch(1000/7879) done. Loss: 0.1812  lr:0.000001
[ Tue Jul  9 18:53:31 2024 ] 	Batch(1100/7879) done. Loss: 0.3668  lr:0.000001
[ Tue Jul  9 18:53:54 2024 ] 	Batch(1200/7879) done. Loss: 0.0290  lr:0.000001
[ Tue Jul  9 18:54:17 2024 ] 	Batch(1300/7879) done. Loss: 0.1255  lr:0.000001
[ Tue Jul  9 18:54:40 2024 ] 	Batch(1400/7879) done. Loss: 0.1484  lr:0.000001
[ Tue Jul  9 18:55:02 2024 ] 
Training: Epoch [110/120], Step [1499], Loss: 0.16045011579990387, Training Accuracy: 96.5
[ Tue Jul  9 18:55:03 2024 ] 	Batch(1500/7879) done. Loss: 0.0106  lr:0.000001
[ Tue Jul  9 18:55:26 2024 ] 	Batch(1600/7879) done. Loss: 0.1043  lr:0.000001
[ Tue Jul  9 18:55:48 2024 ] 	Batch(1700/7879) done. Loss: 0.4439  lr:0.000001
[ Tue Jul  9 18:56:11 2024 ] 	Batch(1800/7879) done. Loss: 0.0683  lr:0.000001
[ Tue Jul  9 18:56:34 2024 ] 	Batch(1900/7879) done. Loss: 0.1582  lr:0.000001
[ Tue Jul  9 18:56:56 2024 ] 
Training: Epoch [110/120], Step [1999], Loss: 0.22228263318538666, Training Accuracy: 96.475
[ Tue Jul  9 18:56:57 2024 ] 	Batch(2000/7879) done. Loss: 0.0671  lr:0.000001
[ Tue Jul  9 18:57:20 2024 ] 	Batch(2100/7879) done. Loss: 0.1631  lr:0.000001
[ Tue Jul  9 18:57:42 2024 ] 	Batch(2200/7879) done. Loss: 0.1273  lr:0.000001
[ Tue Jul  9 18:58:05 2024 ] 	Batch(2300/7879) done. Loss: 0.1216  lr:0.000001
[ Tue Jul  9 18:58:28 2024 ] 	Batch(2400/7879) done. Loss: 0.0085  lr:0.000001
[ Tue Jul  9 18:58:50 2024 ] 
Training: Epoch [110/120], Step [2499], Loss: 0.054708730429410934, Training Accuracy: 96.41499999999999
[ Tue Jul  9 18:58:51 2024 ] 	Batch(2500/7879) done. Loss: 0.1171  lr:0.000001
[ Tue Jul  9 18:59:13 2024 ] 	Batch(2600/7879) done. Loss: 0.0737  lr:0.000001
[ Tue Jul  9 18:59:36 2024 ] 	Batch(2700/7879) done. Loss: 0.1143  lr:0.000001
[ Tue Jul  9 18:59:59 2024 ] 	Batch(2800/7879) done. Loss: 0.3300  lr:0.000001
[ Tue Jul  9 19:00:22 2024 ] 	Batch(2900/7879) done. Loss: 0.0030  lr:0.000001
[ Tue Jul  9 19:00:44 2024 ] 
Training: Epoch [110/120], Step [2999], Loss: 0.05599435791373253, Training Accuracy: 96.37916666666668
[ Tue Jul  9 19:00:45 2024 ] 	Batch(3000/7879) done. Loss: 0.1047  lr:0.000001
[ Tue Jul  9 19:01:07 2024 ] 	Batch(3100/7879) done. Loss: 0.0710  lr:0.000001
[ Tue Jul  9 19:01:30 2024 ] 	Batch(3200/7879) done. Loss: 0.0863  lr:0.000001
[ Tue Jul  9 19:01:54 2024 ] 	Batch(3300/7879) done. Loss: 0.0673  lr:0.000001
[ Tue Jul  9 19:02:17 2024 ] 	Batch(3400/7879) done. Loss: 0.1552  lr:0.000001
[ Tue Jul  9 19:02:39 2024 ] 
Training: Epoch [110/120], Step [3499], Loss: 0.25022339820861816, Training Accuracy: 96.33928571428572
[ Tue Jul  9 19:02:39 2024 ] 	Batch(3500/7879) done. Loss: 0.2126  lr:0.000001
[ Tue Jul  9 19:03:02 2024 ] 	Batch(3600/7879) done. Loss: 0.5022  lr:0.000001
[ Tue Jul  9 19:03:24 2024 ] 	Batch(3700/7879) done. Loss: 0.0020  lr:0.000001
[ Tue Jul  9 19:03:47 2024 ] 	Batch(3800/7879) done. Loss: 0.0478  lr:0.000001
[ Tue Jul  9 19:04:10 2024 ] 	Batch(3900/7879) done. Loss: 0.2021  lr:0.000001
[ Tue Jul  9 19:04:32 2024 ] 
Training: Epoch [110/120], Step [3999], Loss: 0.4101302921772003, Training Accuracy: 96.365625
[ Tue Jul  9 19:04:32 2024 ] 	Batch(4000/7879) done. Loss: 0.0461  lr:0.000001
[ Tue Jul  9 19:04:55 2024 ] 	Batch(4100/7879) done. Loss: 0.3389  lr:0.000001
[ Tue Jul  9 19:05:18 2024 ] 	Batch(4200/7879) done. Loss: 0.0834  lr:0.000001
[ Tue Jul  9 19:05:41 2024 ] 	Batch(4300/7879) done. Loss: 0.0051  lr:0.000001
[ Tue Jul  9 19:06:04 2024 ] 	Batch(4400/7879) done. Loss: 0.1156  lr:0.000001
[ Tue Jul  9 19:06:27 2024 ] 
Training: Epoch [110/120], Step [4499], Loss: 0.11551730334758759, Training Accuracy: 96.34444444444445
[ Tue Jul  9 19:06:27 2024 ] 	Batch(4500/7879) done. Loss: 0.3690  lr:0.000001
[ Tue Jul  9 19:06:51 2024 ] 	Batch(4600/7879) done. Loss: 0.1298  lr:0.000001
[ Tue Jul  9 19:07:14 2024 ] 	Batch(4700/7879) done. Loss: 0.0213  lr:0.000001
[ Tue Jul  9 19:07:37 2024 ] 	Batch(4800/7879) done. Loss: 0.0404  lr:0.000001
[ Tue Jul  9 19:08:01 2024 ] 	Batch(4900/7879) done. Loss: 0.0210  lr:0.000001
[ Tue Jul  9 19:08:24 2024 ] 
Training: Epoch [110/120], Step [4999], Loss: 0.03384065628051758, Training Accuracy: 96.36
[ Tue Jul  9 19:08:24 2024 ] 	Batch(5000/7879) done. Loss: 0.1154  lr:0.000001
[ Tue Jul  9 19:08:48 2024 ] 	Batch(5100/7879) done. Loss: 0.1264  lr:0.000001
[ Tue Jul  9 19:09:11 2024 ] 	Batch(5200/7879) done. Loss: 0.1189  lr:0.000001
[ Tue Jul  9 19:09:34 2024 ] 	Batch(5300/7879) done. Loss: 0.1855  lr:0.000001
[ Tue Jul  9 19:09:58 2024 ] 	Batch(5400/7879) done. Loss: 0.5838  lr:0.000001
[ Tue Jul  9 19:10:21 2024 ] 
Training: Epoch [110/120], Step [5499], Loss: 0.18093854188919067, Training Accuracy: 96.31590909090909
[ Tue Jul  9 19:10:21 2024 ] 	Batch(5500/7879) done. Loss: 0.3491  lr:0.000001
[ Tue Jul  9 19:10:44 2024 ] 	Batch(5600/7879) done. Loss: 0.0194  lr:0.000001
[ Tue Jul  9 19:11:08 2024 ] 	Batch(5700/7879) done. Loss: 0.0586  lr:0.000001
[ Tue Jul  9 19:11:31 2024 ] 	Batch(5800/7879) done. Loss: 0.3504  lr:0.000001
[ Tue Jul  9 19:11:54 2024 ] 	Batch(5900/7879) done. Loss: 0.0425  lr:0.000001
[ Tue Jul  9 19:12:17 2024 ] 
Training: Epoch [110/120], Step [5999], Loss: 0.35880517959594727, Training Accuracy: 96.30833333333332
[ Tue Jul  9 19:12:18 2024 ] 	Batch(6000/7879) done. Loss: 0.0536  lr:0.000001
[ Tue Jul  9 19:12:41 2024 ] 	Batch(6100/7879) done. Loss: 0.0268  lr:0.000001
[ Tue Jul  9 19:13:04 2024 ] 	Batch(6200/7879) done. Loss: 0.0655  lr:0.000001
[ Tue Jul  9 19:13:28 2024 ] 	Batch(6300/7879) done. Loss: 0.3529  lr:0.000001
[ Tue Jul  9 19:13:51 2024 ] 	Batch(6400/7879) done. Loss: 0.2101  lr:0.000001
[ Tue Jul  9 19:14:13 2024 ] 
Training: Epoch [110/120], Step [6499], Loss: 0.05192122980952263, Training Accuracy: 96.33846153846154
[ Tue Jul  9 19:14:13 2024 ] 	Batch(6500/7879) done. Loss: 0.0377  lr:0.000001
[ Tue Jul  9 19:14:36 2024 ] 	Batch(6600/7879) done. Loss: 0.0427  lr:0.000001
[ Tue Jul  9 19:14:59 2024 ] 	Batch(6700/7879) done. Loss: 0.0103  lr:0.000001
[ Tue Jul  9 19:15:21 2024 ] 	Batch(6800/7879) done. Loss: 0.0218  lr:0.000001
[ Tue Jul  9 19:15:45 2024 ] 	Batch(6900/7879) done. Loss: 0.0738  lr:0.000001
[ Tue Jul  9 19:16:08 2024 ] 
Training: Epoch [110/120], Step [6999], Loss: 0.025224871933460236, Training Accuracy: 96.32321428571429
[ Tue Jul  9 19:16:08 2024 ] 	Batch(7000/7879) done. Loss: 0.0096  lr:0.000001
[ Tue Jul  9 19:16:31 2024 ] 	Batch(7100/7879) done. Loss: 0.0961  lr:0.000001
[ Tue Jul  9 19:16:54 2024 ] 	Batch(7200/7879) done. Loss: 0.3698  lr:0.000001
[ Tue Jul  9 19:17:16 2024 ] 	Batch(7300/7879) done. Loss: 0.0588  lr:0.000001
[ Tue Jul  9 19:17:39 2024 ] 	Batch(7400/7879) done. Loss: 0.2208  lr:0.000001
[ Tue Jul  9 19:18:01 2024 ] 
Training: Epoch [110/120], Step [7499], Loss: 0.025707844644784927, Training Accuracy: 96.29666666666667
[ Tue Jul  9 19:18:01 2024 ] 	Batch(7500/7879) done. Loss: 0.0403  lr:0.000001
[ Tue Jul  9 19:18:24 2024 ] 	Batch(7600/7879) done. Loss: 0.3563  lr:0.000001
[ Tue Jul  9 19:18:47 2024 ] 	Batch(7700/7879) done. Loss: 0.1394  lr:0.000001
[ Tue Jul  9 19:19:09 2024 ] 	Batch(7800/7879) done. Loss: 0.5337  lr:0.000001
[ Tue Jul  9 19:19:27 2024 ] 	Mean training loss: 0.1437.
[ Tue Jul  9 19:19:27 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 19:19:27 2024 ] Training epoch: 112
[ Tue Jul  9 19:19:27 2024 ] 	Batch(0/7879) done. Loss: 0.6017  lr:0.000001
[ Tue Jul  9 19:19:50 2024 ] 	Batch(100/7879) done. Loss: 0.4138  lr:0.000001
[ Tue Jul  9 19:20:13 2024 ] 	Batch(200/7879) done. Loss: 0.2383  lr:0.000001
[ Tue Jul  9 19:20:35 2024 ] 	Batch(300/7879) done. Loss: 0.1681  lr:0.000001
[ Tue Jul  9 19:20:58 2024 ] 	Batch(400/7879) done. Loss: 0.0040  lr:0.000001
[ Tue Jul  9 19:21:20 2024 ] 
Training: Epoch [111/120], Step [499], Loss: 0.12100005149841309, Training Accuracy: 96.39999999999999
[ Tue Jul  9 19:21:21 2024 ] 	Batch(500/7879) done. Loss: 0.2970  lr:0.000001
[ Tue Jul  9 19:21:43 2024 ] 	Batch(600/7879) done. Loss: 0.1300  lr:0.000001
[ Tue Jul  9 19:22:06 2024 ] 	Batch(700/7879) done. Loss: 0.2496  lr:0.000001
[ Tue Jul  9 19:22:28 2024 ] 	Batch(800/7879) done. Loss: 0.0305  lr:0.000001
[ Tue Jul  9 19:22:51 2024 ] 	Batch(900/7879) done. Loss: 0.0625  lr:0.000001
[ Tue Jul  9 19:23:13 2024 ] 
Training: Epoch [111/120], Step [999], Loss: 0.28201237320899963, Training Accuracy: 96.39999999999999
[ Tue Jul  9 19:23:14 2024 ] 	Batch(1000/7879) done. Loss: 0.1668  lr:0.000001
[ Tue Jul  9 19:23:36 2024 ] 	Batch(1100/7879) done. Loss: 0.0079  lr:0.000001
[ Tue Jul  9 19:23:59 2024 ] 	Batch(1200/7879) done. Loss: 0.1337  lr:0.000001
[ Tue Jul  9 19:24:22 2024 ] 	Batch(1300/7879) done. Loss: 0.0814  lr:0.000001
[ Tue Jul  9 19:24:45 2024 ] 	Batch(1400/7879) done. Loss: 0.0681  lr:0.000001
[ Tue Jul  9 19:25:08 2024 ] 
Training: Epoch [111/120], Step [1499], Loss: 0.13130220770835876, Training Accuracy: 96.40833333333333
[ Tue Jul  9 19:25:09 2024 ] 	Batch(1500/7879) done. Loss: 0.1373  lr:0.000001
[ Tue Jul  9 19:25:32 2024 ] 	Batch(1600/7879) done. Loss: 0.1654  lr:0.000001
[ Tue Jul  9 19:25:55 2024 ] 	Batch(1700/7879) done. Loss: 0.4141  lr:0.000001
[ Tue Jul  9 19:26:19 2024 ] 	Batch(1800/7879) done. Loss: 0.0925  lr:0.000001
[ Tue Jul  9 19:26:42 2024 ] 	Batch(1900/7879) done. Loss: 0.1605  lr:0.000001
[ Tue Jul  9 19:27:05 2024 ] 
Training: Epoch [111/120], Step [1999], Loss: 0.07751113921403885, Training Accuracy: 96.28750000000001
[ Tue Jul  9 19:27:06 2024 ] 	Batch(2000/7879) done. Loss: 0.1539  lr:0.000001
[ Tue Jul  9 19:27:29 2024 ] 	Batch(2100/7879) done. Loss: 0.0167  lr:0.000001
[ Tue Jul  9 19:27:52 2024 ] 	Batch(2200/7879) done. Loss: 0.1957  lr:0.000001
[ Tue Jul  9 19:28:16 2024 ] 	Batch(2300/7879) done. Loss: 0.0635  lr:0.000001
[ Tue Jul  9 19:28:39 2024 ] 	Batch(2400/7879) done. Loss: 0.0759  lr:0.000001
[ Tue Jul  9 19:29:02 2024 ] 
Training: Epoch [111/120], Step [2499], Loss: 0.1645224392414093, Training Accuracy: 96.195
[ Tue Jul  9 19:29:02 2024 ] 	Batch(2500/7879) done. Loss: 0.1238  lr:0.000001
[ Tue Jul  9 19:29:26 2024 ] 	Batch(2600/7879) done. Loss: 0.4023  lr:0.000001
[ Tue Jul  9 19:29:49 2024 ] 	Batch(2700/7879) done. Loss: 0.1701  lr:0.000001
[ Tue Jul  9 19:30:11 2024 ] 	Batch(2800/7879) done. Loss: 0.2976  lr:0.000001
[ Tue Jul  9 19:30:34 2024 ] 	Batch(2900/7879) done. Loss: 0.1202  lr:0.000001
[ Tue Jul  9 19:30:56 2024 ] 
Training: Epoch [111/120], Step [2999], Loss: 0.32132387161254883, Training Accuracy: 96.20833333333333
[ Tue Jul  9 19:30:57 2024 ] 	Batch(3000/7879) done. Loss: 0.0382  lr:0.000001
[ Tue Jul  9 19:31:19 2024 ] 	Batch(3100/7879) done. Loss: 0.0116  lr:0.000001
[ Tue Jul  9 19:31:42 2024 ] 	Batch(3200/7879) done. Loss: 0.0043  lr:0.000001
[ Tue Jul  9 19:32:04 2024 ] 	Batch(3300/7879) done. Loss: 0.2612  lr:0.000001
[ Tue Jul  9 19:32:27 2024 ] 	Batch(3400/7879) done. Loss: 0.0473  lr:0.000001
[ Tue Jul  9 19:32:49 2024 ] 
Training: Epoch [111/120], Step [3499], Loss: 0.1196717917919159, Training Accuracy: 96.06428571428572
[ Tue Jul  9 19:32:50 2024 ] 	Batch(3500/7879) done. Loss: 0.4277  lr:0.000001
[ Tue Jul  9 19:33:12 2024 ] 	Batch(3600/7879) done. Loss: 0.0455  lr:0.000001
[ Tue Jul  9 19:33:35 2024 ] 	Batch(3700/7879) done. Loss: 0.0047  lr:0.000001
[ Tue Jul  9 19:33:57 2024 ] 	Batch(3800/7879) done. Loss: 0.0061  lr:0.000001
[ Tue Jul  9 19:34:20 2024 ] 	Batch(3900/7879) done. Loss: 0.2011  lr:0.000001
[ Tue Jul  9 19:34:42 2024 ] 
Training: Epoch [111/120], Step [3999], Loss: 0.02671630308032036, Training Accuracy: 96.040625
[ Tue Jul  9 19:34:43 2024 ] 	Batch(4000/7879) done. Loss: 0.0256  lr:0.000001
[ Tue Jul  9 19:35:05 2024 ] 	Batch(4100/7879) done. Loss: 0.0837  lr:0.000001
[ Tue Jul  9 19:35:28 2024 ] 	Batch(4200/7879) done. Loss: 0.0056  lr:0.000001
[ Tue Jul  9 19:35:51 2024 ] 	Batch(4300/7879) done. Loss: 0.0300  lr:0.000001
[ Tue Jul  9 19:36:13 2024 ] 	Batch(4400/7879) done. Loss: 0.1355  lr:0.000001
[ Tue Jul  9 19:36:36 2024 ] 
Training: Epoch [111/120], Step [4499], Loss: 0.02473197504878044, Training Accuracy: 96.08888888888889
[ Tue Jul  9 19:36:37 2024 ] 	Batch(4500/7879) done. Loss: 0.6626  lr:0.000001
[ Tue Jul  9 19:37:00 2024 ] 	Batch(4600/7879) done. Loss: 0.1448  lr:0.000001
[ Tue Jul  9 19:37:23 2024 ] 	Batch(4700/7879) done. Loss: 0.0023  lr:0.000001
[ Tue Jul  9 19:37:46 2024 ] 	Batch(4800/7879) done. Loss: 0.0255  lr:0.000001
[ Tue Jul  9 19:38:09 2024 ] 	Batch(4900/7879) done. Loss: 0.0061  lr:0.000001
[ Tue Jul  9 19:38:31 2024 ] 
Training: Epoch [111/120], Step [4999], Loss: 0.06663352996110916, Training Accuracy: 96.09
[ Tue Jul  9 19:38:31 2024 ] 	Batch(5000/7879) done. Loss: 0.1402  lr:0.000001
[ Tue Jul  9 19:38:54 2024 ] 	Batch(5100/7879) done. Loss: 0.0834  lr:0.000001
[ Tue Jul  9 19:39:17 2024 ] 	Batch(5200/7879) done. Loss: 0.1688  lr:0.000001
[ Tue Jul  9 19:39:39 2024 ] 	Batch(5300/7879) done. Loss: 0.3404  lr:0.000001
[ Tue Jul  9 19:40:02 2024 ] 	Batch(5400/7879) done. Loss: 0.0881  lr:0.000001
[ Tue Jul  9 19:40:24 2024 ] 
Training: Epoch [111/120], Step [5499], Loss: 0.13190259039402008, Training Accuracy: 96.05909090909091
[ Tue Jul  9 19:40:24 2024 ] 	Batch(5500/7879) done. Loss: 0.3707  lr:0.000001
[ Tue Jul  9 19:40:47 2024 ] 	Batch(5600/7879) done. Loss: 0.2634  lr:0.000001
[ Tue Jul  9 19:41:10 2024 ] 	Batch(5700/7879) done. Loss: 0.0363  lr:0.000001
[ Tue Jul  9 19:41:32 2024 ] 	Batch(5800/7879) done. Loss: 0.1168  lr:0.000001
[ Tue Jul  9 19:41:55 2024 ] 	Batch(5900/7879) done. Loss: 0.1097  lr:0.000001
[ Tue Jul  9 19:42:17 2024 ] 
Training: Epoch [111/120], Step [5999], Loss: 0.05610133334994316, Training Accuracy: 96.05625
[ Tue Jul  9 19:42:18 2024 ] 	Batch(6000/7879) done. Loss: 0.4058  lr:0.000001
[ Tue Jul  9 19:42:40 2024 ] 	Batch(6100/7879) done. Loss: 0.0232  lr:0.000001
[ Tue Jul  9 19:43:03 2024 ] 	Batch(6200/7879) done. Loss: 0.0089  lr:0.000001
[ Tue Jul  9 19:43:25 2024 ] 	Batch(6300/7879) done. Loss: 0.0696  lr:0.000001
[ Tue Jul  9 19:43:48 2024 ] 	Batch(6400/7879) done. Loss: 0.0039  lr:0.000001
[ Tue Jul  9 19:44:10 2024 ] 
Training: Epoch [111/120], Step [6499], Loss: 0.22017356753349304, Training Accuracy: 96.08269230769231
[ Tue Jul  9 19:44:11 2024 ] 	Batch(6500/7879) done. Loss: 0.1137  lr:0.000001
[ Tue Jul  9 19:44:34 2024 ] 	Batch(6600/7879) done. Loss: 0.0207  lr:0.000001
[ Tue Jul  9 19:44:56 2024 ] 	Batch(6700/7879) done. Loss: 0.0350  lr:0.000001
[ Tue Jul  9 19:45:19 2024 ] 	Batch(6800/7879) done. Loss: 0.0531  lr:0.000001
[ Tue Jul  9 19:45:41 2024 ] 	Batch(6900/7879) done. Loss: 0.0760  lr:0.000001
[ Tue Jul  9 19:46:04 2024 ] 
Training: Epoch [111/120], Step [6999], Loss: 0.008365887217223644, Training Accuracy: 96.06071428571428
[ Tue Jul  9 19:46:04 2024 ] 	Batch(7000/7879) done. Loss: 0.0587  lr:0.000001
[ Tue Jul  9 19:46:27 2024 ] 	Batch(7100/7879) done. Loss: 0.2082  lr:0.000001
[ Tue Jul  9 19:46:49 2024 ] 	Batch(7200/7879) done. Loss: 0.1150  lr:0.000001
[ Tue Jul  9 19:47:12 2024 ] 	Batch(7300/7879) done. Loss: 0.0078  lr:0.000001
[ Tue Jul  9 19:47:35 2024 ] 	Batch(7400/7879) done. Loss: 0.1057  lr:0.000001
[ Tue Jul  9 19:47:57 2024 ] 
Training: Epoch [111/120], Step [7499], Loss: 0.16782239079475403, Training Accuracy: 96.08333333333333
[ Tue Jul  9 19:47:57 2024 ] 	Batch(7500/7879) done. Loss: 0.0121  lr:0.000001
[ Tue Jul  9 19:48:20 2024 ] 	Batch(7600/7879) done. Loss: 0.0733  lr:0.000001
[ Tue Jul  9 19:48:42 2024 ] 	Batch(7700/7879) done. Loss: 0.1925  lr:0.000001
[ Tue Jul  9 19:49:05 2024 ] 	Batch(7800/7879) done. Loss: 0.0121  lr:0.000001
[ Tue Jul  9 19:49:23 2024 ] 	Mean training loss: 0.1442.
[ Tue Jul  9 19:49:23 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 19:49:23 2024 ] Training epoch: 113
[ Tue Jul  9 19:49:23 2024 ] 	Batch(0/7879) done. Loss: 0.0240  lr:0.000001
[ Tue Jul  9 19:49:46 2024 ] 	Batch(100/7879) done. Loss: 0.0114  lr:0.000001
[ Tue Jul  9 19:50:08 2024 ] 	Batch(200/7879) done. Loss: 0.4428  lr:0.000001
[ Tue Jul  9 19:50:31 2024 ] 	Batch(300/7879) done. Loss: 0.5291  lr:0.000001
[ Tue Jul  9 19:50:54 2024 ] 	Batch(400/7879) done. Loss: 0.0940  lr:0.000001
[ Tue Jul  9 19:51:16 2024 ] 
Training: Epoch [112/120], Step [499], Loss: 0.10387199372053146, Training Accuracy: 96.275
[ Tue Jul  9 19:51:16 2024 ] 	Batch(500/7879) done. Loss: 0.1013  lr:0.000001
[ Tue Jul  9 19:51:39 2024 ] 	Batch(600/7879) done. Loss: 0.0775  lr:0.000001
[ Tue Jul  9 19:52:01 2024 ] 	Batch(700/7879) done. Loss: 0.0584  lr:0.000001
[ Tue Jul  9 19:52:24 2024 ] 	Batch(800/7879) done. Loss: 0.0289  lr:0.000001
[ Tue Jul  9 19:52:47 2024 ] 	Batch(900/7879) done. Loss: 0.0584  lr:0.000001
[ Tue Jul  9 19:53:09 2024 ] 
Training: Epoch [112/120], Step [999], Loss: 0.002124589867889881, Training Accuracy: 96.325
[ Tue Jul  9 19:53:09 2024 ] 	Batch(1000/7879) done. Loss: 0.0087  lr:0.000001
[ Tue Jul  9 19:53:32 2024 ] 	Batch(1100/7879) done. Loss: 0.0531  lr:0.000001
[ Tue Jul  9 19:53:54 2024 ] 	Batch(1200/7879) done. Loss: 0.2030  lr:0.000001
[ Tue Jul  9 19:54:17 2024 ] 	Batch(1300/7879) done. Loss: 0.0711  lr:0.000001
[ Tue Jul  9 19:54:40 2024 ] 	Batch(1400/7879) done. Loss: 0.0158  lr:0.000001
[ Tue Jul  9 19:55:02 2024 ] 
Training: Epoch [112/120], Step [1499], Loss: 0.06363827735185623, Training Accuracy: 96.31666666666666
[ Tue Jul  9 19:55:02 2024 ] 	Batch(1500/7879) done. Loss: 0.0397  lr:0.000001
[ Tue Jul  9 19:55:25 2024 ] 	Batch(1600/7879) done. Loss: 0.2753  lr:0.000001
[ Tue Jul  9 19:55:47 2024 ] 	Batch(1700/7879) done. Loss: 0.0304  lr:0.000001
[ Tue Jul  9 19:56:10 2024 ] 	Batch(1800/7879) done. Loss: 0.2937  lr:0.000001
[ Tue Jul  9 19:56:33 2024 ] 	Batch(1900/7879) done. Loss: 0.1360  lr:0.000001
[ Tue Jul  9 19:56:55 2024 ] 
Training: Epoch [112/120], Step [1999], Loss: 0.026868054643273354, Training Accuracy: 96.275
[ Tue Jul  9 19:56:55 2024 ] 	Batch(2000/7879) done. Loss: 0.2363  lr:0.000001
[ Tue Jul  9 19:57:18 2024 ] 	Batch(2100/7879) done. Loss: 0.2747  lr:0.000001
[ Tue Jul  9 19:57:40 2024 ] 	Batch(2200/7879) done. Loss: 0.0310  lr:0.000001
[ Tue Jul  9 19:58:03 2024 ] 	Batch(2300/7879) done. Loss: 0.0577  lr:0.000001
[ Tue Jul  9 19:58:26 2024 ] 	Batch(2400/7879) done. Loss: 0.0993  lr:0.000001
[ Tue Jul  9 19:58:48 2024 ] 
Training: Epoch [112/120], Step [2499], Loss: 0.007654705084860325, Training Accuracy: 96.265
[ Tue Jul  9 19:58:48 2024 ] 	Batch(2500/7879) done. Loss: 0.0114  lr:0.000001
[ Tue Jul  9 19:59:11 2024 ] 	Batch(2600/7879) done. Loss: 0.1089  lr:0.000001
[ Tue Jul  9 19:59:34 2024 ] 	Batch(2700/7879) done. Loss: 0.0119  lr:0.000001
[ Tue Jul  9 19:59:56 2024 ] 	Batch(2800/7879) done. Loss: 0.0411  lr:0.000001
[ Tue Jul  9 20:00:19 2024 ] 	Batch(2900/7879) done. Loss: 0.0440  lr:0.000001
[ Tue Jul  9 20:00:41 2024 ] 
Training: Epoch [112/120], Step [2999], Loss: 0.0022217477671802044, Training Accuracy: 96.22083333333333
[ Tue Jul  9 20:00:41 2024 ] 	Batch(3000/7879) done. Loss: 0.2449  lr:0.000001
[ Tue Jul  9 20:01:04 2024 ] 	Batch(3100/7879) done. Loss: 0.1946  lr:0.000001
[ Tue Jul  9 20:01:27 2024 ] 	Batch(3200/7879) done. Loss: 0.1882  lr:0.000001
[ Tue Jul  9 20:01:50 2024 ] 	Batch(3300/7879) done. Loss: 0.3193  lr:0.000001
[ Tue Jul  9 20:02:13 2024 ] 	Batch(3400/7879) done. Loss: 0.1300  lr:0.000001
[ Tue Jul  9 20:02:36 2024 ] 
Training: Epoch [112/120], Step [3499], Loss: 0.02182392030954361, Training Accuracy: 96.26428571428572
[ Tue Jul  9 20:02:37 2024 ] 	Batch(3500/7879) done. Loss: 0.5031  lr:0.000001
[ Tue Jul  9 20:03:00 2024 ] 	Batch(3600/7879) done. Loss: 0.1053  lr:0.000001
[ Tue Jul  9 20:03:23 2024 ] 	Batch(3700/7879) done. Loss: 0.0648  lr:0.000001
[ Tue Jul  9 20:03:47 2024 ] 	Batch(3800/7879) done. Loss: 0.0917  lr:0.000001
[ Tue Jul  9 20:04:10 2024 ] 	Batch(3900/7879) done. Loss: 0.1516  lr:0.000001
[ Tue Jul  9 20:04:33 2024 ] 
Training: Epoch [112/120], Step [3999], Loss: 0.12802954018115997, Training Accuracy: 96.28125
[ Tue Jul  9 20:04:33 2024 ] 	Batch(4000/7879) done. Loss: 0.0210  lr:0.000001
[ Tue Jul  9 20:04:56 2024 ] 	Batch(4100/7879) done. Loss: 0.1228  lr:0.000001
[ Tue Jul  9 20:05:19 2024 ] 	Batch(4200/7879) done. Loss: 0.0678  lr:0.000001
[ Tue Jul  9 20:05:41 2024 ] 	Batch(4300/7879) done. Loss: 0.0202  lr:0.000001
[ Tue Jul  9 20:06:04 2024 ] 	Batch(4400/7879) done. Loss: 0.2090  lr:0.000001
[ Tue Jul  9 20:06:27 2024 ] 
Training: Epoch [112/120], Step [4499], Loss: 0.08737123012542725, Training Accuracy: 96.23055555555555
[ Tue Jul  9 20:06:27 2024 ] 	Batch(4500/7879) done. Loss: 0.0886  lr:0.000001
[ Tue Jul  9 20:06:49 2024 ] 	Batch(4600/7879) done. Loss: 0.3710  lr:0.000001
[ Tue Jul  9 20:07:12 2024 ] 	Batch(4700/7879) done. Loss: 0.1807  lr:0.000001
[ Tue Jul  9 20:07:35 2024 ] 	Batch(4800/7879) done. Loss: 0.2981  lr:0.000001
[ Tue Jul  9 20:07:58 2024 ] 	Batch(4900/7879) done. Loss: 0.1306  lr:0.000001
[ Tue Jul  9 20:08:21 2024 ] 
Training: Epoch [112/120], Step [4999], Loss: 0.1256995052099228, Training Accuracy: 96.2375
[ Tue Jul  9 20:08:21 2024 ] 	Batch(5000/7879) done. Loss: 0.0591  lr:0.000001
[ Tue Jul  9 20:08:43 2024 ] 	Batch(5100/7879) done. Loss: 0.0648  lr:0.000001
[ Tue Jul  9 20:09:06 2024 ] 	Batch(5200/7879) done. Loss: 0.0261  lr:0.000001
[ Tue Jul  9 20:09:29 2024 ] 	Batch(5300/7879) done. Loss: 0.5503  lr:0.000001
[ Tue Jul  9 20:09:52 2024 ] 	Batch(5400/7879) done. Loss: 0.6560  lr:0.000001
[ Tue Jul  9 20:10:14 2024 ] 
Training: Epoch [112/120], Step [5499], Loss: 0.508545994758606, Training Accuracy: 96.21363636363637
[ Tue Jul  9 20:10:14 2024 ] 	Batch(5500/7879) done. Loss: 0.0716  lr:0.000001
[ Tue Jul  9 20:10:37 2024 ] 	Batch(5600/7879) done. Loss: 0.0528  lr:0.000001
[ Tue Jul  9 20:11:00 2024 ] 	Batch(5700/7879) done. Loss: 0.0087  lr:0.000001
[ Tue Jul  9 20:11:23 2024 ] 	Batch(5800/7879) done. Loss: 0.2022  lr:0.000001
[ Tue Jul  9 20:11:45 2024 ] 	Batch(5900/7879) done. Loss: 0.1237  lr:0.000001
[ Tue Jul  9 20:12:08 2024 ] 
Training: Epoch [112/120], Step [5999], Loss: 0.27227094769477844, Training Accuracy: 96.19791666666667
[ Tue Jul  9 20:12:08 2024 ] 	Batch(6000/7879) done. Loss: 0.0377  lr:0.000001
[ Tue Jul  9 20:12:31 2024 ] 	Batch(6100/7879) done. Loss: 0.1525  lr:0.000001
[ Tue Jul  9 20:12:53 2024 ] 	Batch(6200/7879) done. Loss: 0.1801  lr:0.000001
[ Tue Jul  9 20:13:16 2024 ] 	Batch(6300/7879) done. Loss: 0.0383  lr:0.000001
[ Tue Jul  9 20:13:39 2024 ] 	Batch(6400/7879) done. Loss: 0.4084  lr:0.000001
[ Tue Jul  9 20:14:01 2024 ] 
Training: Epoch [112/120], Step [6499], Loss: 0.028495093807578087, Training Accuracy: 96.2076923076923
[ Tue Jul  9 20:14:02 2024 ] 	Batch(6500/7879) done. Loss: 0.2852  lr:0.000001
[ Tue Jul  9 20:14:24 2024 ] 	Batch(6600/7879) done. Loss: 0.0759  lr:0.000001
[ Tue Jul  9 20:14:47 2024 ] 	Batch(6700/7879) done. Loss: 0.0031  lr:0.000001
[ Tue Jul  9 20:15:10 2024 ] 	Batch(6800/7879) done. Loss: 0.1065  lr:0.000001
[ Tue Jul  9 20:15:32 2024 ] 	Batch(6900/7879) done. Loss: 0.4772  lr:0.000001
[ Tue Jul  9 20:15:55 2024 ] 
Training: Epoch [112/120], Step [6999], Loss: 0.4127422869205475, Training Accuracy: 96.20535714285714
[ Tue Jul  9 20:15:55 2024 ] 	Batch(7000/7879) done. Loss: 0.0685  lr:0.000001
[ Tue Jul  9 20:16:18 2024 ] 	Batch(7100/7879) done. Loss: 0.1259  lr:0.000001
[ Tue Jul  9 20:16:41 2024 ] 	Batch(7200/7879) done. Loss: 0.6488  lr:0.000001
[ Tue Jul  9 20:17:03 2024 ] 	Batch(7300/7879) done. Loss: 0.0732  lr:0.000001
[ Tue Jul  9 20:17:26 2024 ] 	Batch(7400/7879) done. Loss: 0.1206  lr:0.000001
[ Tue Jul  9 20:17:49 2024 ] 
Training: Epoch [112/120], Step [7499], Loss: 0.06344376504421234, Training Accuracy: 96.21
[ Tue Jul  9 20:17:49 2024 ] 	Batch(7500/7879) done. Loss: 0.0900  lr:0.000001
[ Tue Jul  9 20:18:12 2024 ] 	Batch(7600/7879) done. Loss: 0.0872  lr:0.000001
[ Tue Jul  9 20:18:34 2024 ] 	Batch(7700/7879) done. Loss: 0.1407  lr:0.000001
[ Tue Jul  9 20:18:57 2024 ] 	Batch(7800/7879) done. Loss: 0.0197  lr:0.000001
[ Tue Jul  9 20:19:15 2024 ] 	Mean training loss: 0.1416.
[ Tue Jul  9 20:19:15 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 20:19:15 2024 ] Training epoch: 114
[ Tue Jul  9 20:19:16 2024 ] 	Batch(0/7879) done. Loss: 0.0214  lr:0.000001
[ Tue Jul  9 20:19:39 2024 ] 	Batch(100/7879) done. Loss: 0.3521  lr:0.000001
[ Tue Jul  9 20:20:03 2024 ] 	Batch(200/7879) done. Loss: 0.1040  lr:0.000001
[ Tue Jul  9 20:20:25 2024 ] 	Batch(300/7879) done. Loss: 0.2224  lr:0.000001
[ Tue Jul  9 20:20:48 2024 ] 	Batch(400/7879) done. Loss: 0.2115  lr:0.000001
[ Tue Jul  9 20:21:11 2024 ] 
Training: Epoch [113/120], Step [499], Loss: 0.3593815565109253, Training Accuracy: 96.2
[ Tue Jul  9 20:21:11 2024 ] 	Batch(500/7879) done. Loss: 0.0836  lr:0.000001
[ Tue Jul  9 20:21:34 2024 ] 	Batch(600/7879) done. Loss: 0.0920  lr:0.000001
[ Tue Jul  9 20:21:56 2024 ] 	Batch(700/7879) done. Loss: 0.3078  lr:0.000001
[ Tue Jul  9 20:22:19 2024 ] 	Batch(800/7879) done. Loss: 0.0040  lr:0.000001
[ Tue Jul  9 20:22:42 2024 ] 	Batch(900/7879) done. Loss: 0.0416  lr:0.000001
[ Tue Jul  9 20:23:04 2024 ] 
Training: Epoch [113/120], Step [999], Loss: 0.024251500144600868, Training Accuracy: 96.025
[ Tue Jul  9 20:23:04 2024 ] 	Batch(1000/7879) done. Loss: 0.1201  lr:0.000001
[ Tue Jul  9 20:23:27 2024 ] 	Batch(1100/7879) done. Loss: 0.5509  lr:0.000001
[ Tue Jul  9 20:23:50 2024 ] 	Batch(1200/7879) done. Loss: 0.0903  lr:0.000001
[ Tue Jul  9 20:24:13 2024 ] 	Batch(1300/7879) done. Loss: 0.0208  lr:0.000001
[ Tue Jul  9 20:24:36 2024 ] 	Batch(1400/7879) done. Loss: 0.0380  lr:0.000001
[ Tue Jul  9 20:24:59 2024 ] 
Training: Epoch [113/120], Step [1499], Loss: 0.01769898273050785, Training Accuracy: 96.00833333333333
[ Tue Jul  9 20:25:00 2024 ] 	Batch(1500/7879) done. Loss: 0.0960  lr:0.000001
[ Tue Jul  9 20:25:23 2024 ] 	Batch(1600/7879) done. Loss: 0.0295  lr:0.000001
[ Tue Jul  9 20:25:46 2024 ] 	Batch(1700/7879) done. Loss: 0.0063  lr:0.000001
[ Tue Jul  9 20:26:08 2024 ] 	Batch(1800/7879) done. Loss: 0.0753  lr:0.000001
[ Tue Jul  9 20:26:31 2024 ] 	Batch(1900/7879) done. Loss: 0.0419  lr:0.000001
[ Tue Jul  9 20:26:53 2024 ] 
Training: Epoch [113/120], Step [1999], Loss: 0.06996536254882812, Training Accuracy: 96.025
[ Tue Jul  9 20:26:53 2024 ] 	Batch(2000/7879) done. Loss: 0.0125  lr:0.000001
[ Tue Jul  9 20:27:16 2024 ] 	Batch(2100/7879) done. Loss: 0.4236  lr:0.000001
[ Tue Jul  9 20:27:39 2024 ] 	Batch(2200/7879) done. Loss: 0.0051  lr:0.000001
[ Tue Jul  9 20:28:01 2024 ] 	Batch(2300/7879) done. Loss: 0.0322  lr:0.000001
[ Tue Jul  9 20:28:24 2024 ] 	Batch(2400/7879) done. Loss: 0.0121  lr:0.000001
[ Tue Jul  9 20:28:46 2024 ] 
Training: Epoch [113/120], Step [2499], Loss: 0.10587096214294434, Training Accuracy: 95.99499999999999
[ Tue Jul  9 20:28:46 2024 ] 	Batch(2500/7879) done. Loss: 0.1255  lr:0.000001
[ Tue Jul  9 20:29:09 2024 ] 	Batch(2600/7879) done. Loss: 0.0629  lr:0.000001
[ Tue Jul  9 20:29:32 2024 ] 	Batch(2700/7879) done. Loss: 0.1863  lr:0.000001
[ Tue Jul  9 20:29:54 2024 ] 	Batch(2800/7879) done. Loss: 0.0479  lr:0.000001
[ Tue Jul  9 20:30:18 2024 ] 	Batch(2900/7879) done. Loss: 0.1964  lr:0.000001
[ Tue Jul  9 20:30:41 2024 ] 
Training: Epoch [113/120], Step [2999], Loss: 0.23076710104942322, Training Accuracy: 96.0375
[ Tue Jul  9 20:30:41 2024 ] 	Batch(3000/7879) done. Loss: 0.1047  lr:0.000001
[ Tue Jul  9 20:31:04 2024 ] 	Batch(3100/7879) done. Loss: 0.0267  lr:0.000001
[ Tue Jul  9 20:31:28 2024 ] 	Batch(3200/7879) done. Loss: 0.2941  lr:0.000001
[ Tue Jul  9 20:31:50 2024 ] 	Batch(3300/7879) done. Loss: 0.5453  lr:0.000001
[ Tue Jul  9 20:32:13 2024 ] 	Batch(3400/7879) done. Loss: 0.0967  lr:0.000001
[ Tue Jul  9 20:32:35 2024 ] 
Training: Epoch [113/120], Step [3499], Loss: 0.03413582220673561, Training Accuracy: 96.14285714285714
[ Tue Jul  9 20:32:35 2024 ] 	Batch(3500/7879) done. Loss: 0.2401  lr:0.000001
[ Tue Jul  9 20:32:58 2024 ] 	Batch(3600/7879) done. Loss: 0.0547  lr:0.000001
[ Tue Jul  9 20:33:21 2024 ] 	Batch(3700/7879) done. Loss: 0.0673  lr:0.000001
[ Tue Jul  9 20:33:45 2024 ] 	Batch(3800/7879) done. Loss: 0.1107  lr:0.000001
[ Tue Jul  9 20:34:08 2024 ] 	Batch(3900/7879) done. Loss: 0.0052  lr:0.000001
[ Tue Jul  9 20:34:31 2024 ] 
Training: Epoch [113/120], Step [3999], Loss: 0.216811865568161, Training Accuracy: 96.165625
[ Tue Jul  9 20:34:31 2024 ] 	Batch(4000/7879) done. Loss: 0.1938  lr:0.000001
[ Tue Jul  9 20:34:55 2024 ] 	Batch(4100/7879) done. Loss: 0.0023  lr:0.000001
[ Tue Jul  9 20:35:18 2024 ] 	Batch(4200/7879) done. Loss: 0.1308  lr:0.000001
[ Tue Jul  9 20:35:41 2024 ] 	Batch(4300/7879) done. Loss: 0.1001  lr:0.000001
[ Tue Jul  9 20:36:05 2024 ] 	Batch(4400/7879) done. Loss: 0.0724  lr:0.000001
[ Tue Jul  9 20:36:28 2024 ] 
Training: Epoch [113/120], Step [4499], Loss: 0.17148524522781372, Training Accuracy: 96.16388888888889
[ Tue Jul  9 20:36:28 2024 ] 	Batch(4500/7879) done. Loss: 0.0113  lr:0.000001
[ Tue Jul  9 20:36:50 2024 ] 	Batch(4600/7879) done. Loss: 0.1501  lr:0.000001
[ Tue Jul  9 20:37:13 2024 ] 	Batch(4700/7879) done. Loss: 0.0652  lr:0.000001
[ Tue Jul  9 20:37:36 2024 ] 	Batch(4800/7879) done. Loss: 0.0247  lr:0.000001
[ Tue Jul  9 20:37:58 2024 ] 	Batch(4900/7879) done. Loss: 0.0681  lr:0.000001
[ Tue Jul  9 20:38:21 2024 ] 
Training: Epoch [113/120], Step [4999], Loss: 0.16616861522197723, Training Accuracy: 96.13000000000001
[ Tue Jul  9 20:38:21 2024 ] 	Batch(5000/7879) done. Loss: 0.1767  lr:0.000001
[ Tue Jul  9 20:38:43 2024 ] 	Batch(5100/7879) done. Loss: 0.0391  lr:0.000001
[ Tue Jul  9 20:39:06 2024 ] 	Batch(5200/7879) done. Loss: 0.1446  lr:0.000001
[ Tue Jul  9 20:39:29 2024 ] 	Batch(5300/7879) done. Loss: 0.1831  lr:0.000001
[ Tue Jul  9 20:39:52 2024 ] 	Batch(5400/7879) done. Loss: 0.3516  lr:0.000001
[ Tue Jul  9 20:40:14 2024 ] 
Training: Epoch [113/120], Step [5499], Loss: 0.29571104049682617, Training Accuracy: 96.10681818181817
[ Tue Jul  9 20:40:14 2024 ] 	Batch(5500/7879) done. Loss: 0.0203  lr:0.000001
[ Tue Jul  9 20:40:37 2024 ] 	Batch(5600/7879) done. Loss: 0.0290  lr:0.000001
[ Tue Jul  9 20:41:00 2024 ] 	Batch(5700/7879) done. Loss: 0.1386  lr:0.000001
[ Tue Jul  9 20:41:22 2024 ] 	Batch(5800/7879) done. Loss: 0.0289  lr:0.000001
[ Tue Jul  9 20:41:45 2024 ] 	Batch(5900/7879) done. Loss: 0.0418  lr:0.000001
[ Tue Jul  9 20:42:08 2024 ] 
Training: Epoch [113/120], Step [5999], Loss: 0.026073001325130463, Training Accuracy: 96.11666666666666
[ Tue Jul  9 20:42:08 2024 ] 	Batch(6000/7879) done. Loss: 0.4769  lr:0.000001
[ Tue Jul  9 20:42:31 2024 ] 	Batch(6100/7879) done. Loss: 0.0030  lr:0.000001
[ Tue Jul  9 20:42:53 2024 ] 	Batch(6200/7879) done. Loss: 0.0508  lr:0.000001
[ Tue Jul  9 20:43:16 2024 ] 	Batch(6300/7879) done. Loss: 0.1428  lr:0.000001
[ Tue Jul  9 20:43:39 2024 ] 	Batch(6400/7879) done. Loss: 0.0472  lr:0.000001
[ Tue Jul  9 20:44:01 2024 ] 
Training: Epoch [113/120], Step [6499], Loss: 0.004726075567305088, Training Accuracy: 96.1403846153846
[ Tue Jul  9 20:44:01 2024 ] 	Batch(6500/7879) done. Loss: 0.1073  lr:0.000001
[ Tue Jul  9 20:44:24 2024 ] 	Batch(6600/7879) done. Loss: 0.1615  lr:0.000001
[ Tue Jul  9 20:44:47 2024 ] 	Batch(6700/7879) done. Loss: 0.5064  lr:0.000001
[ Tue Jul  9 20:45:10 2024 ] 	Batch(6800/7879) done. Loss: 0.0617  lr:0.000001
[ Tue Jul  9 20:45:32 2024 ] 	Batch(6900/7879) done. Loss: 0.0177  lr:0.000001
[ Tue Jul  9 20:45:55 2024 ] 
Training: Epoch [113/120], Step [6999], Loss: 0.008357763290405273, Training Accuracy: 96.16428571428571
[ Tue Jul  9 20:45:55 2024 ] 	Batch(7000/7879) done. Loss: 0.0272  lr:0.000001
[ Tue Jul  9 20:46:18 2024 ] 	Batch(7100/7879) done. Loss: 0.1572  lr:0.000001
[ Tue Jul  9 20:46:41 2024 ] 	Batch(7200/7879) done. Loss: 0.6973  lr:0.000001
[ Tue Jul  9 20:47:03 2024 ] 	Batch(7300/7879) done. Loss: 0.0866  lr:0.000001
[ Tue Jul  9 20:47:26 2024 ] 	Batch(7400/7879) done. Loss: 0.0550  lr:0.000001
[ Tue Jul  9 20:47:49 2024 ] 
Training: Epoch [113/120], Step [7499], Loss: 0.024771036580204964, Training Accuracy: 96.145
[ Tue Jul  9 20:47:49 2024 ] 	Batch(7500/7879) done. Loss: 0.4483  lr:0.000001
[ Tue Jul  9 20:48:12 2024 ] 	Batch(7600/7879) done. Loss: 0.0425  lr:0.000001
[ Tue Jul  9 20:48:34 2024 ] 	Batch(7700/7879) done. Loss: 0.0730  lr:0.000001
[ Tue Jul  9 20:48:57 2024 ] 	Batch(7800/7879) done. Loss: 0.0496  lr:0.000001
[ Tue Jul  9 20:49:15 2024 ] 	Mean training loss: 0.1440.
[ Tue Jul  9 20:49:15 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 20:49:15 2024 ] Training epoch: 115
[ Tue Jul  9 20:49:16 2024 ] 	Batch(0/7879) done. Loss: 0.0382  lr:0.000001
[ Tue Jul  9 20:49:38 2024 ] 	Batch(100/7879) done. Loss: 0.1708  lr:0.000001
[ Tue Jul  9 20:50:01 2024 ] 	Batch(200/7879) done. Loss: 0.0114  lr:0.000001
[ Tue Jul  9 20:50:23 2024 ] 	Batch(300/7879) done. Loss: 0.0269  lr:0.000001
[ Tue Jul  9 20:50:46 2024 ] 	Batch(400/7879) done. Loss: 0.3049  lr:0.000001
[ Tue Jul  9 20:51:09 2024 ] 
Training: Epoch [114/120], Step [499], Loss: 0.0531514473259449, Training Accuracy: 96.35000000000001
[ Tue Jul  9 20:51:09 2024 ] 	Batch(500/7879) done. Loss: 0.0545  lr:0.000001
[ Tue Jul  9 20:51:32 2024 ] 	Batch(600/7879) done. Loss: 0.0921  lr:0.000001
[ Tue Jul  9 20:51:54 2024 ] 	Batch(700/7879) done. Loss: 0.6474  lr:0.000001
[ Tue Jul  9 20:52:17 2024 ] 	Batch(800/7879) done. Loss: 0.2037  lr:0.000001
[ Tue Jul  9 20:52:40 2024 ] 	Batch(900/7879) done. Loss: 0.0164  lr:0.000001
[ Tue Jul  9 20:53:02 2024 ] 
Training: Epoch [114/120], Step [999], Loss: 0.003666047705337405, Training Accuracy: 96.025
[ Tue Jul  9 20:53:02 2024 ] 	Batch(1000/7879) done. Loss: 0.0038  lr:0.000001
[ Tue Jul  9 20:53:25 2024 ] 	Batch(1100/7879) done. Loss: 0.0240  lr:0.000001
[ Tue Jul  9 20:53:48 2024 ] 	Batch(1200/7879) done. Loss: 0.1426  lr:0.000001
[ Tue Jul  9 20:54:11 2024 ] 	Batch(1300/7879) done. Loss: 0.1376  lr:0.000001
[ Tue Jul  9 20:54:33 2024 ] 	Batch(1400/7879) done. Loss: 0.0278  lr:0.000001
[ Tue Jul  9 20:54:56 2024 ] 
Training: Epoch [114/120], Step [1499], Loss: 0.13921929895877838, Training Accuracy: 96.1
[ Tue Jul  9 20:54:56 2024 ] 	Batch(1500/7879) done. Loss: 0.0308  lr:0.000001
[ Tue Jul  9 20:55:19 2024 ] 	Batch(1600/7879) done. Loss: 0.0407  lr:0.000001
[ Tue Jul  9 20:55:41 2024 ] 	Batch(1700/7879) done. Loss: 0.8969  lr:0.000001
[ Tue Jul  9 20:56:04 2024 ] 	Batch(1800/7879) done. Loss: 0.1446  lr:0.000001
[ Tue Jul  9 20:56:27 2024 ] 	Batch(1900/7879) done. Loss: 0.0595  lr:0.000001
[ Tue Jul  9 20:56:49 2024 ] 
Training: Epoch [114/120], Step [1999], Loss: 0.11329641938209534, Training Accuracy: 96.05
[ Tue Jul  9 20:56:50 2024 ] 	Batch(2000/7879) done. Loss: 0.3367  lr:0.000001
[ Tue Jul  9 20:57:13 2024 ] 	Batch(2100/7879) done. Loss: 0.0062  lr:0.000001
[ Tue Jul  9 20:57:36 2024 ] 	Batch(2200/7879) done. Loss: 0.0169  lr:0.000001
[ Tue Jul  9 20:58:00 2024 ] 	Batch(2300/7879) done. Loss: 0.0059  lr:0.000001
[ Tue Jul  9 20:58:23 2024 ] 	Batch(2400/7879) done. Loss: 0.0109  lr:0.000001
[ Tue Jul  9 20:58:46 2024 ] 
Training: Epoch [114/120], Step [2499], Loss: 0.2699112296104431, Training Accuracy: 96.005
[ Tue Jul  9 20:58:46 2024 ] 	Batch(2500/7879) done. Loss: 0.0093  lr:0.000001
[ Tue Jul  9 20:59:09 2024 ] 	Batch(2600/7879) done. Loss: 0.1913  lr:0.000001
[ Tue Jul  9 20:59:31 2024 ] 	Batch(2700/7879) done. Loss: 0.2665  lr:0.000001
[ Tue Jul  9 20:59:54 2024 ] 	Batch(2800/7879) done. Loss: 0.3912  lr:0.000001
[ Tue Jul  9 21:00:17 2024 ] 	Batch(2900/7879) done. Loss: 0.0725  lr:0.000001
[ Tue Jul  9 21:00:39 2024 ] 
Training: Epoch [114/120], Step [2999], Loss: 0.030227426439523697, Training Accuracy: 95.96666666666667
[ Tue Jul  9 21:00:39 2024 ] 	Batch(3000/7879) done. Loss: 0.0729  lr:0.000001
[ Tue Jul  9 21:01:02 2024 ] 	Batch(3100/7879) done. Loss: 0.0858  lr:0.000001
[ Tue Jul  9 21:01:25 2024 ] 	Batch(3200/7879) done. Loss: 0.0033  lr:0.000001
[ Tue Jul  9 21:01:48 2024 ] 	Batch(3300/7879) done. Loss: 0.1095  lr:0.000001
[ Tue Jul  9 21:02:10 2024 ] 	Batch(3400/7879) done. Loss: 0.0303  lr:0.000001
[ Tue Jul  9 21:02:33 2024 ] 
Training: Epoch [114/120], Step [3499], Loss: 0.19980676472187042, Training Accuracy: 95.92857142857143
[ Tue Jul  9 21:02:33 2024 ] 	Batch(3500/7879) done. Loss: 0.0942  lr:0.000001
[ Tue Jul  9 21:02:56 2024 ] 	Batch(3600/7879) done. Loss: 0.0107  lr:0.000001
[ Tue Jul  9 21:03:18 2024 ] 	Batch(3700/7879) done. Loss: 0.1594  lr:0.000001
[ Tue Jul  9 21:03:41 2024 ] 	Batch(3800/7879) done. Loss: 0.0092  lr:0.000001
[ Tue Jul  9 21:04:04 2024 ] 	Batch(3900/7879) done. Loss: 0.1614  lr:0.000001
[ Tue Jul  9 21:04:26 2024 ] 
Training: Epoch [114/120], Step [3999], Loss: 0.08550022542476654, Training Accuracy: 95.94375000000001
[ Tue Jul  9 21:04:26 2024 ] 	Batch(4000/7879) done. Loss: 0.2697  lr:0.000001
[ Tue Jul  9 21:04:49 2024 ] 	Batch(4100/7879) done. Loss: 0.0123  lr:0.000001
[ Tue Jul  9 21:05:12 2024 ] 	Batch(4200/7879) done. Loss: 0.0655  lr:0.000001
[ Tue Jul  9 21:05:35 2024 ] 	Batch(4300/7879) done. Loss: 0.0083  lr:0.000001
[ Tue Jul  9 21:05:58 2024 ] 	Batch(4400/7879) done. Loss: 0.1811  lr:0.000001
[ Tue Jul  9 21:06:20 2024 ] 
Training: Epoch [114/120], Step [4499], Loss: 0.07740917056798935, Training Accuracy: 96.01666666666667
[ Tue Jul  9 21:06:20 2024 ] 	Batch(4500/7879) done. Loss: 0.0183  lr:0.000001
[ Tue Jul  9 21:06:43 2024 ] 	Batch(4600/7879) done. Loss: 0.0704  lr:0.000001
[ Tue Jul  9 21:07:05 2024 ] 	Batch(4700/7879) done. Loss: 0.0126  lr:0.000001
[ Tue Jul  9 21:07:28 2024 ] 	Batch(4800/7879) done. Loss: 0.2880  lr:0.000001
[ Tue Jul  9 21:07:51 2024 ] 	Batch(4900/7879) done. Loss: 0.2415  lr:0.000001
[ Tue Jul  9 21:08:13 2024 ] 
Training: Epoch [114/120], Step [4999], Loss: 0.7103848457336426, Training Accuracy: 96.0525
[ Tue Jul  9 21:08:14 2024 ] 	Batch(5000/7879) done. Loss: 0.0260  lr:0.000001
[ Tue Jul  9 21:08:36 2024 ] 	Batch(5100/7879) done. Loss: 0.0262  lr:0.000001
[ Tue Jul  9 21:08:59 2024 ] 	Batch(5200/7879) done. Loss: 0.0297  lr:0.000001
[ Tue Jul  9 21:09:23 2024 ] 	Batch(5300/7879) done. Loss: 0.0826  lr:0.000001
[ Tue Jul  9 21:09:46 2024 ] 	Batch(5400/7879) done. Loss: 0.0580  lr:0.000001
[ Tue Jul  9 21:10:09 2024 ] 
Training: Epoch [114/120], Step [5499], Loss: 0.121468186378479, Training Accuracy: 96.06818181818181
[ Tue Jul  9 21:10:09 2024 ] 	Batch(5500/7879) done. Loss: 0.1378  lr:0.000001
[ Tue Jul  9 21:10:33 2024 ] 	Batch(5600/7879) done. Loss: 0.0166  lr:0.000001
[ Tue Jul  9 21:10:55 2024 ] 	Batch(5700/7879) done. Loss: 0.1633  lr:0.000001
[ Tue Jul  9 21:11:18 2024 ] 	Batch(5800/7879) done. Loss: 0.1980  lr:0.000001
[ Tue Jul  9 21:11:41 2024 ] 	Batch(5900/7879) done. Loss: 0.3451  lr:0.000001
[ Tue Jul  9 21:12:03 2024 ] 
Training: Epoch [114/120], Step [5999], Loss: 0.26059597730636597, Training Accuracy: 96.06458333333333
[ Tue Jul  9 21:12:04 2024 ] 	Batch(6000/7879) done. Loss: 0.1469  lr:0.000001
[ Tue Jul  9 21:12:26 2024 ] 	Batch(6100/7879) done. Loss: 0.1521  lr:0.000001
[ Tue Jul  9 21:12:49 2024 ] 	Batch(6200/7879) done. Loss: 0.0473  lr:0.000001
[ Tue Jul  9 21:13:12 2024 ] 	Batch(6300/7879) done. Loss: 0.2931  lr:0.000001
[ Tue Jul  9 21:13:34 2024 ] 	Batch(6400/7879) done. Loss: 0.0659  lr:0.000001
[ Tue Jul  9 21:13:57 2024 ] 
Training: Epoch [114/120], Step [6499], Loss: 0.005204316694289446, Training Accuracy: 96.11730769230769
[ Tue Jul  9 21:13:57 2024 ] 	Batch(6500/7879) done. Loss: 0.0387  lr:0.000001
[ Tue Jul  9 21:14:20 2024 ] 	Batch(6600/7879) done. Loss: 0.0535  lr:0.000001
[ Tue Jul  9 21:14:42 2024 ] 	Batch(6700/7879) done. Loss: 0.0562  lr:0.000001
[ Tue Jul  9 21:15:06 2024 ] 	Batch(6800/7879) done. Loss: 0.0561  lr:0.000001
[ Tue Jul  9 21:15:29 2024 ] 	Batch(6900/7879) done. Loss: 0.4394  lr:0.000001
[ Tue Jul  9 21:15:53 2024 ] 
Training: Epoch [114/120], Step [6999], Loss: 0.26778388023376465, Training Accuracy: 96.09821428571429
[ Tue Jul  9 21:15:53 2024 ] 	Batch(7000/7879) done. Loss: 0.0339  lr:0.000001
[ Tue Jul  9 21:16:16 2024 ] 	Batch(7100/7879) done. Loss: 0.0477  lr:0.000001
[ Tue Jul  9 21:16:40 2024 ] 	Batch(7200/7879) done. Loss: 0.2897  lr:0.000001
[ Tue Jul  9 21:17:03 2024 ] 	Batch(7300/7879) done. Loss: 0.0510  lr:0.000001
[ Tue Jul  9 21:17:26 2024 ] 	Batch(7400/7879) done. Loss: 0.0216  lr:0.000001
[ Tue Jul  9 21:17:50 2024 ] 
Training: Epoch [114/120], Step [7499], Loss: 0.1765311062335968, Training Accuracy: 96.10666666666667
[ Tue Jul  9 21:17:50 2024 ] 	Batch(7500/7879) done. Loss: 0.0075  lr:0.000001
[ Tue Jul  9 21:18:13 2024 ] 	Batch(7600/7879) done. Loss: 0.2817  lr:0.000001
[ Tue Jul  9 21:18:36 2024 ] 	Batch(7700/7879) done. Loss: 0.0383  lr:0.000001
[ Tue Jul  9 21:18:59 2024 ] 	Batch(7800/7879) done. Loss: 0.5110  lr:0.000001
[ Tue Jul  9 21:19:16 2024 ] 	Mean training loss: 0.1432.
[ Tue Jul  9 21:19:16 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 21:19:16 2024 ] Training epoch: 116
[ Tue Jul  9 21:19:17 2024 ] 	Batch(0/7879) done. Loss: 0.0489  lr:0.000001
[ Tue Jul  9 21:19:40 2024 ] 	Batch(100/7879) done. Loss: 0.0755  lr:0.000001
[ Tue Jul  9 21:20:03 2024 ] 	Batch(200/7879) done. Loss: 0.0398  lr:0.000001
[ Tue Jul  9 21:20:26 2024 ] 	Batch(300/7879) done. Loss: 0.1226  lr:0.000001
[ Tue Jul  9 21:20:49 2024 ] 	Batch(400/7879) done. Loss: 0.1162  lr:0.000001
[ Tue Jul  9 21:21:12 2024 ] 
Training: Epoch [115/120], Step [499], Loss: 0.07105696201324463, Training Accuracy: 96.275
[ Tue Jul  9 21:21:12 2024 ] 	Batch(500/7879) done. Loss: 0.0327  lr:0.000001
[ Tue Jul  9 21:21:35 2024 ] 	Batch(600/7879) done. Loss: 0.0185  lr:0.000001
[ Tue Jul  9 21:21:58 2024 ] 	Batch(700/7879) done. Loss: 0.3962  lr:0.000001
[ Tue Jul  9 21:22:22 2024 ] 	Batch(800/7879) done. Loss: 0.0636  lr:0.000001
[ Tue Jul  9 21:22:45 2024 ] 	Batch(900/7879) done. Loss: 0.1863  lr:0.000001
[ Tue Jul  9 21:23:08 2024 ] 
Training: Epoch [115/120], Step [999], Loss: 0.12297987192869186, Training Accuracy: 96.3625
[ Tue Jul  9 21:23:08 2024 ] 	Batch(1000/7879) done. Loss: 0.2216  lr:0.000001
[ Tue Jul  9 21:23:31 2024 ] 	Batch(1100/7879) done. Loss: 0.0236  lr:0.000001
[ Tue Jul  9 21:23:54 2024 ] 	Batch(1200/7879) done. Loss: 0.2548  lr:0.000001
[ Tue Jul  9 21:24:17 2024 ] 	Batch(1300/7879) done. Loss: 0.0361  lr:0.000001
[ Tue Jul  9 21:24:41 2024 ] 	Batch(1400/7879) done. Loss: 0.0086  lr:0.000001
[ Tue Jul  9 21:25:03 2024 ] 
Training: Epoch [115/120], Step [1499], Loss: 0.1860182136297226, Training Accuracy: 96.175
[ Tue Jul  9 21:25:04 2024 ] 	Batch(1500/7879) done. Loss: 0.0169  lr:0.000001
[ Tue Jul  9 21:25:27 2024 ] 	Batch(1600/7879) done. Loss: 0.0462  lr:0.000001
[ Tue Jul  9 21:25:50 2024 ] 	Batch(1700/7879) done. Loss: 0.9415  lr:0.000001
[ Tue Jul  9 21:26:13 2024 ] 	Batch(1800/7879) done. Loss: 0.0293  lr:0.000001
[ Tue Jul  9 21:26:36 2024 ] 	Batch(1900/7879) done. Loss: 0.1107  lr:0.000001
[ Tue Jul  9 21:26:59 2024 ] 
Training: Epoch [115/120], Step [1999], Loss: 0.023049671202898026, Training Accuracy: 96.1625
[ Tue Jul  9 21:26:59 2024 ] 	Batch(2000/7879) done. Loss: 0.2663  lr:0.000001
[ Tue Jul  9 21:27:23 2024 ] 	Batch(2100/7879) done. Loss: 0.5163  lr:0.000001
[ Tue Jul  9 21:27:46 2024 ] 	Batch(2200/7879) done. Loss: 0.0560  lr:0.000001
[ Tue Jul  9 21:28:09 2024 ] 	Batch(2300/7879) done. Loss: 0.2157  lr:0.000001
[ Tue Jul  9 21:28:32 2024 ] 	Batch(2400/7879) done. Loss: 0.1558  lr:0.000001
[ Tue Jul  9 21:28:55 2024 ] 
Training: Epoch [115/120], Step [2499], Loss: 0.04561449587345123, Training Accuracy: 96.255
[ Tue Jul  9 21:28:55 2024 ] 	Batch(2500/7879) done. Loss: 0.0316  lr:0.000001
[ Tue Jul  9 21:29:18 2024 ] 	Batch(2600/7879) done. Loss: 0.0016  lr:0.000001
[ Tue Jul  9 21:29:42 2024 ] 	Batch(2700/7879) done. Loss: 0.0783  lr:0.000001
[ Tue Jul  9 21:30:05 2024 ] 	Batch(2800/7879) done. Loss: 0.1651  lr:0.000001
[ Tue Jul  9 21:30:28 2024 ] 	Batch(2900/7879) done. Loss: 0.1186  lr:0.000001
[ Tue Jul  9 21:30:51 2024 ] 
Training: Epoch [115/120], Step [2999], Loss: 0.07857956737279892, Training Accuracy: 96.18333333333334
[ Tue Jul  9 21:30:51 2024 ] 	Batch(3000/7879) done. Loss: 0.3292  lr:0.000001
[ Tue Jul  9 21:31:14 2024 ] 	Batch(3100/7879) done. Loss: 0.0249  lr:0.000001
[ Tue Jul  9 21:31:38 2024 ] 	Batch(3200/7879) done. Loss: 0.0607  lr:0.000001
[ Tue Jul  9 21:32:01 2024 ] 	Batch(3300/7879) done. Loss: 0.0439  lr:0.000001
[ Tue Jul  9 21:32:24 2024 ] 	Batch(3400/7879) done. Loss: 0.0503  lr:0.000001
[ Tue Jul  9 21:32:47 2024 ] 
Training: Epoch [115/120], Step [3499], Loss: 0.24186185002326965, Training Accuracy: 96.18928571428572
[ Tue Jul  9 21:32:47 2024 ] 	Batch(3500/7879) done. Loss: 0.0372  lr:0.000001
[ Tue Jul  9 21:33:10 2024 ] 	Batch(3600/7879) done. Loss: 0.0240  lr:0.000001
[ Tue Jul  9 21:33:33 2024 ] 	Batch(3700/7879) done. Loss: 0.1578  lr:0.000001
[ Tue Jul  9 21:33:55 2024 ] 	Batch(3800/7879) done. Loss: 0.1596  lr:0.000001
[ Tue Jul  9 21:34:18 2024 ] 	Batch(3900/7879) done. Loss: 0.6258  lr:0.000001
[ Tue Jul  9 21:34:41 2024 ] 
Training: Epoch [115/120], Step [3999], Loss: 0.05424151197075844, Training Accuracy: 96.2
[ Tue Jul  9 21:34:41 2024 ] 	Batch(4000/7879) done. Loss: 0.1094  lr:0.000001
[ Tue Jul  9 21:35:04 2024 ] 	Batch(4100/7879) done. Loss: 0.0057  lr:0.000001
[ Tue Jul  9 21:35:27 2024 ] 	Batch(4200/7879) done. Loss: 0.3919  lr:0.000001
[ Tue Jul  9 21:35:50 2024 ] 	Batch(4300/7879) done. Loss: 0.1068  lr:0.000001
[ Tue Jul  9 21:36:13 2024 ] 	Batch(4400/7879) done. Loss: 0.0587  lr:0.000001
[ Tue Jul  9 21:36:36 2024 ] 
Training: Epoch [115/120], Step [4499], Loss: 0.009497336111962795, Training Accuracy: 96.175
[ Tue Jul  9 21:36:36 2024 ] 	Batch(4500/7879) done. Loss: 0.0220  lr:0.000001
[ Tue Jul  9 21:36:59 2024 ] 	Batch(4600/7879) done. Loss: 0.3193  lr:0.000001
[ Tue Jul  9 21:37:21 2024 ] 	Batch(4700/7879) done. Loss: 0.1422  lr:0.000001
[ Tue Jul  9 21:37:44 2024 ] 	Batch(4800/7879) done. Loss: 0.0218  lr:0.000001
[ Tue Jul  9 21:38:07 2024 ] 	Batch(4900/7879) done. Loss: 0.4808  lr:0.000001
[ Tue Jul  9 21:38:29 2024 ] 
Training: Epoch [115/120], Step [4999], Loss: 0.033686403185129166, Training Accuracy: 96.17999999999999
[ Tue Jul  9 21:38:30 2024 ] 	Batch(5000/7879) done. Loss: 0.0622  lr:0.000001
[ Tue Jul  9 21:38:52 2024 ] 	Batch(5100/7879) done. Loss: 0.0592  lr:0.000001
[ Tue Jul  9 21:39:15 2024 ] 	Batch(5200/7879) done. Loss: 0.0199  lr:0.000001
[ Tue Jul  9 21:39:39 2024 ] 	Batch(5300/7879) done. Loss: 0.2321  lr:0.000001
[ Tue Jul  9 21:40:01 2024 ] 	Batch(5400/7879) done. Loss: 0.0401  lr:0.000001
[ Tue Jul  9 21:40:24 2024 ] 
Training: Epoch [115/120], Step [5499], Loss: 0.02128220535814762, Training Accuracy: 96.17272727272727
[ Tue Jul  9 21:40:24 2024 ] 	Batch(5500/7879) done. Loss: 0.0096  lr:0.000001
[ Tue Jul  9 21:40:47 2024 ] 	Batch(5600/7879) done. Loss: 0.0774  lr:0.000001
[ Tue Jul  9 21:41:10 2024 ] 	Batch(5700/7879) done. Loss: 0.0376  lr:0.000001
[ Tue Jul  9 21:41:33 2024 ] 	Batch(5800/7879) done. Loss: 0.0619  lr:0.000001
[ Tue Jul  9 21:41:56 2024 ] 	Batch(5900/7879) done. Loss: 0.2263  lr:0.000001
[ Tue Jul  9 21:42:19 2024 ] 
Training: Epoch [115/120], Step [5999], Loss: 0.06824838370084763, Training Accuracy: 96.20625000000001
[ Tue Jul  9 21:42:19 2024 ] 	Batch(6000/7879) done. Loss: 0.0097  lr:0.000001
[ Tue Jul  9 21:42:42 2024 ] 	Batch(6100/7879) done. Loss: 0.0175  lr:0.000001
[ Tue Jul  9 21:43:05 2024 ] 	Batch(6200/7879) done. Loss: 0.0067  lr:0.000001
[ Tue Jul  9 21:43:28 2024 ] 	Batch(6300/7879) done. Loss: 0.0489  lr:0.000001
[ Tue Jul  9 21:43:51 2024 ] 	Batch(6400/7879) done. Loss: 0.7383  lr:0.000001
[ Tue Jul  9 21:44:13 2024 ] 
Training: Epoch [115/120], Step [6499], Loss: 0.016464803367853165, Training Accuracy: 96.21538461538461
[ Tue Jul  9 21:44:13 2024 ] 	Batch(6500/7879) done. Loss: 0.0094  lr:0.000001
[ Tue Jul  9 21:44:36 2024 ] 	Batch(6600/7879) done. Loss: 0.0211  lr:0.000001
[ Tue Jul  9 21:44:59 2024 ] 	Batch(6700/7879) done. Loss: 0.1931  lr:0.000001
[ Tue Jul  9 21:45:22 2024 ] 	Batch(6800/7879) done. Loss: 1.3441  lr:0.000001
[ Tue Jul  9 21:45:45 2024 ] 	Batch(6900/7879) done. Loss: 0.0119  lr:0.000001
[ Tue Jul  9 21:46:07 2024 ] 
Training: Epoch [115/120], Step [6999], Loss: 0.477932333946228, Training Accuracy: 96.19107142857143
[ Tue Jul  9 21:46:07 2024 ] 	Batch(7000/7879) done. Loss: 0.0645  lr:0.000001
[ Tue Jul  9 21:46:30 2024 ] 	Batch(7100/7879) done. Loss: 0.5712  lr:0.000001
[ Tue Jul  9 21:46:53 2024 ] 	Batch(7200/7879) done. Loss: 0.0404  lr:0.000001
[ Tue Jul  9 21:47:16 2024 ] 	Batch(7300/7879) done. Loss: 0.0377  lr:0.000001
[ Tue Jul  9 21:47:39 2024 ] 	Batch(7400/7879) done. Loss: 0.0368  lr:0.000001
[ Tue Jul  9 21:48:02 2024 ] 
Training: Epoch [115/120], Step [7499], Loss: 0.007754088379442692, Training Accuracy: 96.16166666666666
[ Tue Jul  9 21:48:02 2024 ] 	Batch(7500/7879) done. Loss: 0.2346  lr:0.000001
[ Tue Jul  9 21:48:25 2024 ] 	Batch(7600/7879) done. Loss: 0.0726  lr:0.000001
[ Tue Jul  9 21:48:48 2024 ] 	Batch(7700/7879) done. Loss: 0.0025  lr:0.000001
[ Tue Jul  9 21:49:11 2024 ] 	Batch(7800/7879) done. Loss: 0.0189  lr:0.000001
[ Tue Jul  9 21:49:30 2024 ] 	Mean training loss: 0.1420.
[ Tue Jul  9 21:49:30 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul  9 21:49:30 2024 ] Training epoch: 117
[ Tue Jul  9 21:49:31 2024 ] 	Batch(0/7879) done. Loss: 0.0276  lr:0.000001
[ Tue Jul  9 21:49:54 2024 ] 	Batch(100/7879) done. Loss: 0.0418  lr:0.000001
[ Tue Jul  9 21:50:18 2024 ] 	Batch(200/7879) done. Loss: 0.3798  lr:0.000001
[ Tue Jul  9 21:50:41 2024 ] 	Batch(300/7879) done. Loss: 0.0057  lr:0.000001
[ Tue Jul  9 21:51:04 2024 ] 	Batch(400/7879) done. Loss: 0.0633  lr:0.000001
[ Tue Jul  9 21:51:27 2024 ] 
Training: Epoch [116/120], Step [499], Loss: 0.05040653422474861, Training Accuracy: 95.775
[ Tue Jul  9 21:51:27 2024 ] 	Batch(500/7879) done. Loss: 0.5812  lr:0.000001
[ Tue Jul  9 21:51:50 2024 ] 	Batch(600/7879) done. Loss: 0.1442  lr:0.000001
[ Tue Jul  9 21:52:13 2024 ] 	Batch(700/7879) done. Loss: 0.0813  lr:0.000001
[ Tue Jul  9 21:52:37 2024 ] 	Batch(800/7879) done. Loss: 0.0795  lr:0.000001
[ Tue Jul  9 21:53:00 2024 ] 	Batch(900/7879) done. Loss: 0.0656  lr:0.000001
[ Tue Jul  9 21:53:23 2024 ] 
Training: Epoch [116/120], Step [999], Loss: 0.30199137330055237, Training Accuracy: 95.825
[ Tue Jul  9 21:53:23 2024 ] 	Batch(1000/7879) done. Loss: 0.0080  lr:0.000001
[ Tue Jul  9 21:53:46 2024 ] 	Batch(1100/7879) done. Loss: 0.3353  lr:0.000001
[ Tue Jul  9 21:54:09 2024 ] 	Batch(1200/7879) done. Loss: 0.1676  lr:0.000001
[ Tue Jul  9 21:54:33 2024 ] 	Batch(1300/7879) done. Loss: 0.0404  lr:0.000001
[ Tue Jul  9 21:54:57 2024 ] 	Batch(1400/7879) done. Loss: 0.4513  lr:0.000001
[ Tue Jul  9 21:55:19 2024 ] 
Training: Epoch [116/120], Step [1499], Loss: 0.23500441014766693, Training Accuracy: 95.89999999999999
[ Tue Jul  9 21:55:19 2024 ] 	Batch(1500/7879) done. Loss: 0.1784  lr:0.000001
[ Tue Jul  9 21:55:43 2024 ] 	Batch(1600/7879) done. Loss: 0.3100  lr:0.000001
[ Tue Jul  9 21:56:06 2024 ] 	Batch(1700/7879) done. Loss: 0.2342  lr:0.000001
[ Tue Jul  9 21:56:29 2024 ] 	Batch(1800/7879) done. Loss: 0.1317  lr:0.000001
[ Tue Jul  9 21:56:52 2024 ] 	Batch(1900/7879) done. Loss: 0.0316  lr:0.000001
[ Tue Jul  9 21:57:14 2024 ] 
Training: Epoch [116/120], Step [1999], Loss: 0.18921521306037903, Training Accuracy: 95.89999999999999
[ Tue Jul  9 21:57:15 2024 ] 	Batch(2000/7879) done. Loss: 0.0103  lr:0.000001
[ Tue Jul  9 21:57:38 2024 ] 	Batch(2100/7879) done. Loss: 0.3249  lr:0.000001
[ Tue Jul  9 21:58:01 2024 ] 	Batch(2200/7879) done. Loss: 0.3992  lr:0.000001
[ Tue Jul  9 21:58:23 2024 ] 	Batch(2300/7879) done. Loss: 0.0130  lr:0.000001
[ Tue Jul  9 21:58:46 2024 ] 	Batch(2400/7879) done. Loss: 0.5597  lr:0.000001
[ Tue Jul  9 21:59:08 2024 ] 
Training: Epoch [116/120], Step [2499], Loss: 0.14860035479068756, Training Accuracy: 95.94500000000001
[ Tue Jul  9 21:59:08 2024 ] 	Batch(2500/7879) done. Loss: 0.1277  lr:0.000001
[ Tue Jul  9 21:59:31 2024 ] 	Batch(2600/7879) done. Loss: 0.0266  lr:0.000001
[ Tue Jul  9 21:59:53 2024 ] 	Batch(2700/7879) done. Loss: 0.0773  lr:0.000001
[ Tue Jul  9 22:00:16 2024 ] 	Batch(2800/7879) done. Loss: 0.0449  lr:0.000001
[ Tue Jul  9 22:00:39 2024 ] 	Batch(2900/7879) done. Loss: 0.1753  lr:0.000001
[ Tue Jul  9 22:01:02 2024 ] 
Training: Epoch [116/120], Step [2999], Loss: 0.14568345248699188, Training Accuracy: 96.02916666666667
[ Tue Jul  9 22:01:03 2024 ] 	Batch(3000/7879) done. Loss: 0.1048  lr:0.000001
[ Tue Jul  9 22:01:25 2024 ] 	Batch(3100/7879) done. Loss: 0.0011  lr:0.000001
[ Tue Jul  9 22:01:48 2024 ] 	Batch(3200/7879) done. Loss: 0.0011  lr:0.000001
[ Tue Jul  9 22:02:11 2024 ] 	Batch(3300/7879) done. Loss: 0.1336  lr:0.000001
[ Tue Jul  9 22:02:34 2024 ] 	Batch(3400/7879) done. Loss: 0.4007  lr:0.000001
[ Tue Jul  9 22:02:56 2024 ] 
Training: Epoch [116/120], Step [3499], Loss: 0.2971177101135254, Training Accuracy: 96.04642857142856
[ Tue Jul  9 22:02:57 2024 ] 	Batch(3500/7879) done. Loss: 0.0406  lr:0.000001
[ Tue Jul  9 22:03:19 2024 ] 	Batch(3600/7879) done. Loss: 0.5532  lr:0.000001
[ Tue Jul  9 22:03:42 2024 ] 	Batch(3700/7879) done. Loss: 0.0604  lr:0.000001
[ Tue Jul  9 22:04:05 2024 ] 	Batch(3800/7879) done. Loss: 0.1431  lr:0.000001
[ Tue Jul  9 22:04:28 2024 ] 	Batch(3900/7879) done. Loss: 0.1053  lr:0.000001
[ Tue Jul  9 22:04:50 2024 ] 
Training: Epoch [116/120], Step [3999], Loss: 0.21530942618846893, Training Accuracy: 96.125
[ Tue Jul  9 22:04:50 2024 ] 	Batch(4000/7879) done. Loss: 0.0573  lr:0.000001
[ Tue Jul  9 22:05:13 2024 ] 	Batch(4100/7879) done. Loss: 0.0709  lr:0.000001
[ Tue Jul  9 22:05:35 2024 ] 	Batch(4200/7879) done. Loss: 0.0458  lr:0.000001
[ Tue Jul  9 22:05:58 2024 ] 	Batch(4300/7879) done. Loss: 0.1072  lr:0.000001
[ Tue Jul  9 22:06:21 2024 ] 	Batch(4400/7879) done. Loss: 0.1794  lr:0.000001
[ Tue Jul  9 22:06:43 2024 ] 
Training: Epoch [116/120], Step [4499], Loss: 0.0575997456908226, Training Accuracy: 96.1361111111111
[ Tue Jul  9 22:06:43 2024 ] 	Batch(4500/7879) done. Loss: 0.1082  lr:0.000001
[ Tue Jul  9 22:07:06 2024 ] 	Batch(4600/7879) done. Loss: 0.2179  lr:0.000001
[ Tue Jul  9 22:07:28 2024 ] 	Batch(4700/7879) done. Loss: 0.0942  lr:0.000001
[ Tue Jul  9 22:07:51 2024 ] 	Batch(4800/7879) done. Loss: 0.0463  lr:0.000001
[ Tue Jul  9 22:08:14 2024 ] 	Batch(4900/7879) done. Loss: 0.0659  lr:0.000001
[ Tue Jul  9 22:08:36 2024 ] 
Training: Epoch [116/120], Step [4999], Loss: 0.03280038759112358, Training Accuracy: 96.135
[ Tue Jul  9 22:08:36 2024 ] 	Batch(5000/7879) done. Loss: 0.3214  lr:0.000001
[ Tue Jul  9 22:08:59 2024 ] 	Batch(5100/7879) done. Loss: 0.0201  lr:0.000001
[ Tue Jul  9 22:09:22 2024 ] 	Batch(5200/7879) done. Loss: 0.3277  lr:0.000001
[ Tue Jul  9 22:09:44 2024 ] 	Batch(5300/7879) done. Loss: 0.0122  lr:0.000001
[ Tue Jul  9 22:10:07 2024 ] 	Batch(5400/7879) done. Loss: 0.1292  lr:0.000001
[ Tue Jul  9 22:10:29 2024 ] 
Training: Epoch [116/120], Step [5499], Loss: 0.06115734949707985, Training Accuracy: 96.175
[ Tue Jul  9 22:10:29 2024 ] 	Batch(5500/7879) done. Loss: 0.2282  lr:0.000001
[ Tue Jul  9 22:10:52 2024 ] 	Batch(5600/7879) done. Loss: 0.0766  lr:0.000001
[ Tue Jul  9 22:11:14 2024 ] 	Batch(5700/7879) done. Loss: 0.0893  lr:0.000001
[ Tue Jul  9 22:11:37 2024 ] 	Batch(5800/7879) done. Loss: 0.1271  lr:0.000001
[ Tue Jul  9 22:12:00 2024 ] 	Batch(5900/7879) done. Loss: 0.1187  lr:0.000001
[ Tue Jul  9 22:12:23 2024 ] 
Training: Epoch [116/120], Step [5999], Loss: 0.03923039138317108, Training Accuracy: 96.1875
[ Tue Jul  9 22:12:23 2024 ] 	Batch(6000/7879) done. Loss: 0.5339  lr:0.000001
[ Tue Jul  9 22:12:45 2024 ] 	Batch(6100/7879) done. Loss: 0.1149  lr:0.000001
[ Tue Jul  9 22:13:08 2024 ] 	Batch(6200/7879) done. Loss: 0.0454  lr:0.000001
[ Tue Jul  9 22:13:31 2024 ] 	Batch(6300/7879) done. Loss: 0.0068  lr:0.000001
[ Tue Jul  9 22:13:53 2024 ] 	Batch(6400/7879) done. Loss: 0.0981  lr:0.000001
[ Tue Jul  9 22:14:16 2024 ] 
Training: Epoch [116/120], Step [6499], Loss: 0.3244872987270355, Training Accuracy: 96.20192307692308
[ Tue Jul  9 22:14:16 2024 ] 	Batch(6500/7879) done. Loss: 0.3716  lr:0.000001
[ Tue Jul  9 22:14:38 2024 ] 	Batch(6600/7879) done. Loss: 0.2073  lr:0.000001
[ Tue Jul  9 22:15:01 2024 ] 	Batch(6700/7879) done. Loss: 0.0079  lr:0.000001
[ Tue Jul  9 22:15:24 2024 ] 	Batch(6800/7879) done. Loss: 0.0682  lr:0.000001
[ Tue Jul  9 22:15:46 2024 ] 	Batch(6900/7879) done. Loss: 0.2665  lr:0.000001
[ Tue Jul  9 22:16:09 2024 ] 
Training: Epoch [116/120], Step [6999], Loss: 0.05531751736998558, Training Accuracy: 96.1625
[ Tue Jul  9 22:16:09 2024 ] 	Batch(7000/7879) done. Loss: 0.1729  lr:0.000001
[ Tue Jul  9 22:16:31 2024 ] 	Batch(7100/7879) done. Loss: 0.0787  lr:0.000001
[ Tue Jul  9 22:16:54 2024 ] 	Batch(7200/7879) done. Loss: 0.1676  lr:0.000001
[ Tue Jul  9 22:17:17 2024 ] 	Batch(7300/7879) done. Loss: 0.0098  lr:0.000001
[ Tue Jul  9 22:17:41 2024 ] 	Batch(7400/7879) done. Loss: 0.1771  lr:0.000001
[ Tue Jul  9 22:18:04 2024 ] 
Training: Epoch [116/120], Step [7499], Loss: 0.00517056230455637, Training Accuracy: 96.155
[ Tue Jul  9 22:18:04 2024 ] 	Batch(7500/7879) done. Loss: 0.0845  lr:0.000001
[ Tue Jul  9 22:18:27 2024 ] 	Batch(7600/7879) done. Loss: 0.0341  lr:0.000001
[ Tue Jul  9 22:18:49 2024 ] 	Batch(7700/7879) done. Loss: 0.1297  lr:0.000001
[ Tue Jul  9 22:19:12 2024 ] 	Batch(7800/7879) done. Loss: 0.0117  lr:0.000001
[ Tue Jul  9 22:19:29 2024 ] 	Mean training loss: 0.1423.
[ Tue Jul  9 22:19:29 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 22:19:29 2024 ] Training epoch: 118
[ Tue Jul  9 22:19:30 2024 ] 	Batch(0/7879) done. Loss: 0.0558  lr:0.000001
[ Tue Jul  9 22:19:53 2024 ] 	Batch(100/7879) done. Loss: 0.0081  lr:0.000001
[ Tue Jul  9 22:20:15 2024 ] 	Batch(200/7879) done. Loss: 0.0173  lr:0.000001
[ Tue Jul  9 22:20:38 2024 ] 	Batch(300/7879) done. Loss: 0.2084  lr:0.000001
[ Tue Jul  9 22:21:00 2024 ] 	Batch(400/7879) done. Loss: 0.1624  lr:0.000001
[ Tue Jul  9 22:21:23 2024 ] 
Training: Epoch [117/120], Step [499], Loss: 0.006579108536243439, Training Accuracy: 96.3
[ Tue Jul  9 22:21:23 2024 ] 	Batch(500/7879) done. Loss: 0.0998  lr:0.000001
[ Tue Jul  9 22:21:46 2024 ] 	Batch(600/7879) done. Loss: 0.0274  lr:0.000001
[ Tue Jul  9 22:22:08 2024 ] 	Batch(700/7879) done. Loss: 0.0086  lr:0.000001
[ Tue Jul  9 22:22:31 2024 ] 	Batch(800/7879) done. Loss: 0.1083  lr:0.000001
[ Tue Jul  9 22:22:53 2024 ] 	Batch(900/7879) done. Loss: 0.1334  lr:0.000001
[ Tue Jul  9 22:23:16 2024 ] 
Training: Epoch [117/120], Step [999], Loss: 0.10887709259986877, Training Accuracy: 96.28750000000001
[ Tue Jul  9 22:23:16 2024 ] 	Batch(1000/7879) done. Loss: 0.3830  lr:0.000001
[ Tue Jul  9 22:23:39 2024 ] 	Batch(1100/7879) done. Loss: 0.0780  lr:0.000001
[ Tue Jul  9 22:24:01 2024 ] 	Batch(1200/7879) done. Loss: 0.0869  lr:0.000001
[ Tue Jul  9 22:24:25 2024 ] 	Batch(1300/7879) done. Loss: 0.0019  lr:0.000001
[ Tue Jul  9 22:24:48 2024 ] 	Batch(1400/7879) done. Loss: 0.0451  lr:0.000001
[ Tue Jul  9 22:25:10 2024 ] 
Training: Epoch [117/120], Step [1499], Loss: 0.005296770483255386, Training Accuracy: 96.28333333333333
[ Tue Jul  9 22:25:10 2024 ] 	Batch(1500/7879) done. Loss: 0.0225  lr:0.000001
[ Tue Jul  9 22:25:33 2024 ] 	Batch(1600/7879) done. Loss: 0.0305  lr:0.000001
[ Tue Jul  9 22:25:56 2024 ] 	Batch(1700/7879) done. Loss: 0.0699  lr:0.000001
[ Tue Jul  9 22:26:18 2024 ] 	Batch(1800/7879) done. Loss: 0.0038  lr:0.000001
[ Tue Jul  9 22:26:41 2024 ] 	Batch(1900/7879) done. Loss: 0.0509  lr:0.000001
[ Tue Jul  9 22:27:03 2024 ] 
Training: Epoch [117/120], Step [1999], Loss: 0.19784203171730042, Training Accuracy: 96.25
[ Tue Jul  9 22:27:04 2024 ] 	Batch(2000/7879) done. Loss: 0.1600  lr:0.000001
[ Tue Jul  9 22:27:26 2024 ] 	Batch(2100/7879) done. Loss: 0.0239  lr:0.000001
[ Tue Jul  9 22:27:49 2024 ] 	Batch(2200/7879) done. Loss: 0.1106  lr:0.000001
[ Tue Jul  9 22:28:12 2024 ] 	Batch(2300/7879) done. Loss: 0.0870  lr:0.000001
[ Tue Jul  9 22:28:35 2024 ] 	Batch(2400/7879) done. Loss: 0.0189  lr:0.000001
[ Tue Jul  9 22:28:57 2024 ] 
Training: Epoch [117/120], Step [2499], Loss: 0.02229672484099865, Training Accuracy: 96.275
[ Tue Jul  9 22:28:57 2024 ] 	Batch(2500/7879) done. Loss: 0.1529  lr:0.000001
[ Tue Jul  9 22:29:20 2024 ] 	Batch(2600/7879) done. Loss: 0.1186  lr:0.000001
[ Tue Jul  9 22:29:43 2024 ] 	Batch(2700/7879) done. Loss: 0.0284  lr:0.000001
[ Tue Jul  9 22:30:06 2024 ] 	Batch(2800/7879) done. Loss: 0.1082  lr:0.000001
[ Tue Jul  9 22:30:29 2024 ] 	Batch(2900/7879) done. Loss: 0.0298  lr:0.000001
[ Tue Jul  9 22:30:51 2024 ] 
Training: Epoch [117/120], Step [2999], Loss: 0.0036623459309339523, Training Accuracy: 96.22083333333333
[ Tue Jul  9 22:30:51 2024 ] 	Batch(3000/7879) done. Loss: 0.0066  lr:0.000001
[ Tue Jul  9 22:31:14 2024 ] 	Batch(3100/7879) done. Loss: 0.2116  lr:0.000001
[ Tue Jul  9 22:31:37 2024 ] 	Batch(3200/7879) done. Loss: 0.2830  lr:0.000001
[ Tue Jul  9 22:32:00 2024 ] 	Batch(3300/7879) done. Loss: 0.0281  lr:0.000001
[ Tue Jul  9 22:32:23 2024 ] 	Batch(3400/7879) done. Loss: 0.2651  lr:0.000001
[ Tue Jul  9 22:32:45 2024 ] 
Training: Epoch [117/120], Step [3499], Loss: 0.021089928224682808, Training Accuracy: 96.18571428571428
[ Tue Jul  9 22:32:45 2024 ] 	Batch(3500/7879) done. Loss: 0.3864  lr:0.000001
[ Tue Jul  9 22:33:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0890  lr:0.000001
[ Tue Jul  9 22:33:31 2024 ] 	Batch(3700/7879) done. Loss: 0.0096  lr:0.000001
[ Tue Jul  9 22:33:54 2024 ] 	Batch(3800/7879) done. Loss: 0.1260  lr:0.000001
[ Tue Jul  9 22:34:16 2024 ] 	Batch(3900/7879) done. Loss: 0.0481  lr:0.000001
[ Tue Jul  9 22:34:39 2024 ] 
Training: Epoch [117/120], Step [3999], Loss: 0.033926572650671005, Training Accuracy: 96.1625
[ Tue Jul  9 22:34:39 2024 ] 	Batch(4000/7879) done. Loss: 0.0466  lr:0.000001
[ Tue Jul  9 22:35:02 2024 ] 	Batch(4100/7879) done. Loss: 0.0489  lr:0.000001
[ Tue Jul  9 22:35:25 2024 ] 	Batch(4200/7879) done. Loss: 0.0530  lr:0.000001
[ Tue Jul  9 22:35:47 2024 ] 	Batch(4300/7879) done. Loss: 0.0751  lr:0.000001
[ Tue Jul  9 22:36:10 2024 ] 	Batch(4400/7879) done. Loss: 0.1670  lr:0.000001
[ Tue Jul  9 22:36:34 2024 ] 
Training: Epoch [117/120], Step [4499], Loss: 0.010761287063360214, Training Accuracy: 96.22777777777777
[ Tue Jul  9 22:36:34 2024 ] 	Batch(4500/7879) done. Loss: 0.0708  lr:0.000001
[ Tue Jul  9 22:36:57 2024 ] 	Batch(4600/7879) done. Loss: 0.0979  lr:0.000001
[ Tue Jul  9 22:37:21 2024 ] 	Batch(4700/7879) done. Loss: 0.0321  lr:0.000001
[ Tue Jul  9 22:37:44 2024 ] 	Batch(4800/7879) done. Loss: 0.0077  lr:0.000001
[ Tue Jul  9 22:38:07 2024 ] 	Batch(4900/7879) done. Loss: 0.0201  lr:0.000001
[ Tue Jul  9 22:38:30 2024 ] 
Training: Epoch [117/120], Step [4999], Loss: 0.49991416931152344, Training Accuracy: 96.2125
[ Tue Jul  9 22:38:30 2024 ] 	Batch(5000/7879) done. Loss: 0.1743  lr:0.000001
[ Tue Jul  9 22:38:53 2024 ] 	Batch(5100/7879) done. Loss: 0.1057  lr:0.000001
[ Tue Jul  9 22:39:16 2024 ] 	Batch(5200/7879) done. Loss: 0.0607  lr:0.000001
[ Tue Jul  9 22:39:38 2024 ] 	Batch(5300/7879) done. Loss: 0.0702  lr:0.000001
[ Tue Jul  9 22:40:01 2024 ] 	Batch(5400/7879) done. Loss: 0.0501  lr:0.000001
[ Tue Jul  9 22:40:24 2024 ] 
Training: Epoch [117/120], Step [5499], Loss: 0.03914250433444977, Training Accuracy: 96.20681818181818
[ Tue Jul  9 22:40:24 2024 ] 	Batch(5500/7879) done. Loss: 0.4707  lr:0.000001
[ Tue Jul  9 22:40:47 2024 ] 	Batch(5600/7879) done. Loss: 0.1321  lr:0.000001
[ Tue Jul  9 22:41:10 2024 ] 	Batch(5700/7879) done. Loss: 0.0396  lr:0.000001
[ Tue Jul  9 22:41:32 2024 ] 	Batch(5800/7879) done. Loss: 0.0313  lr:0.000001
[ Tue Jul  9 22:41:55 2024 ] 	Batch(5900/7879) done. Loss: 0.0825  lr:0.000001
[ Tue Jul  9 22:42:17 2024 ] 
Training: Epoch [117/120], Step [5999], Loss: 0.060038842260837555, Training Accuracy: 96.17083333333333
[ Tue Jul  9 22:42:18 2024 ] 	Batch(6000/7879) done. Loss: 0.0408  lr:0.000001
[ Tue Jul  9 22:42:41 2024 ] 	Batch(6100/7879) done. Loss: 0.2890  lr:0.000001
[ Tue Jul  9 22:43:03 2024 ] 	Batch(6200/7879) done. Loss: 0.2442  lr:0.000001
[ Tue Jul  9 22:43:26 2024 ] 	Batch(6300/7879) done. Loss: 0.3472  lr:0.000001
[ Tue Jul  9 22:43:49 2024 ] 	Batch(6400/7879) done. Loss: 0.0134  lr:0.000001
[ Tue Jul  9 22:44:12 2024 ] 
Training: Epoch [117/120], Step [6499], Loss: 0.1930035501718521, Training Accuracy: 96.14615384615385
[ Tue Jul  9 22:44:12 2024 ] 	Batch(6500/7879) done. Loss: 0.2758  lr:0.000001
[ Tue Jul  9 22:44:34 2024 ] 	Batch(6600/7879) done. Loss: 0.1816  lr:0.000001
[ Tue Jul  9 22:44:57 2024 ] 	Batch(6700/7879) done. Loss: 0.1589  lr:0.000001
[ Tue Jul  9 22:45:20 2024 ] 	Batch(6800/7879) done. Loss: 0.4555  lr:0.000001
[ Tue Jul  9 22:45:43 2024 ] 	Batch(6900/7879) done. Loss: 0.0360  lr:0.000001
[ Tue Jul  9 22:46:05 2024 ] 
Training: Epoch [117/120], Step [6999], Loss: 0.09399276226758957, Training Accuracy: 96.10357142857143
[ Tue Jul  9 22:46:06 2024 ] 	Batch(7000/7879) done. Loss: 0.0835  lr:0.000001
[ Tue Jul  9 22:46:28 2024 ] 	Batch(7100/7879) done. Loss: 0.6527  lr:0.000001
[ Tue Jul  9 22:46:51 2024 ] 	Batch(7200/7879) done. Loss: 0.1203  lr:0.000001
[ Tue Jul  9 22:47:14 2024 ] 	Batch(7300/7879) done. Loss: 0.0037  lr:0.000001
[ Tue Jul  9 22:47:37 2024 ] 	Batch(7400/7879) done. Loss: 0.0646  lr:0.000001
[ Tue Jul  9 22:47:59 2024 ] 
Training: Epoch [117/120], Step [7499], Loss: 0.14309240877628326, Training Accuracy: 96.08333333333333
[ Tue Jul  9 22:48:00 2024 ] 	Batch(7500/7879) done. Loss: 0.0276  lr:0.000001
[ Tue Jul  9 22:48:22 2024 ] 	Batch(7600/7879) done. Loss: 0.1787  lr:0.000001
[ Tue Jul  9 22:48:45 2024 ] 	Batch(7700/7879) done. Loss: 0.1083  lr:0.000001
[ Tue Jul  9 22:49:08 2024 ] 	Batch(7800/7879) done. Loss: 0.0144  lr:0.000001
[ Tue Jul  9 22:49:26 2024 ] 	Mean training loss: 0.1406.
[ Tue Jul  9 22:49:26 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 22:49:26 2024 ] Training epoch: 119
[ Tue Jul  9 22:49:26 2024 ] 	Batch(0/7879) done. Loss: 0.0060  lr:0.000001
[ Tue Jul  9 22:49:49 2024 ] 	Batch(100/7879) done. Loss: 0.2142  lr:0.000001
[ Tue Jul  9 22:50:12 2024 ] 	Batch(200/7879) done. Loss: 0.0237  lr:0.000001
[ Tue Jul  9 22:50:35 2024 ] 	Batch(300/7879) done. Loss: 0.0692  lr:0.000001
[ Tue Jul  9 22:50:57 2024 ] 	Batch(400/7879) done. Loss: 0.3593  lr:0.000001
[ Tue Jul  9 22:51:20 2024 ] 
Training: Epoch [118/120], Step [499], Loss: 0.024837829172611237, Training Accuracy: 96.8
[ Tue Jul  9 22:51:20 2024 ] 	Batch(500/7879) done. Loss: 0.5587  lr:0.000001
[ Tue Jul  9 22:51:43 2024 ] 	Batch(600/7879) done. Loss: 0.1497  lr:0.000001
[ Tue Jul  9 22:52:06 2024 ] 	Batch(700/7879) done. Loss: 0.3022  lr:0.000001
[ Tue Jul  9 22:52:29 2024 ] 	Batch(800/7879) done. Loss: 0.2932  lr:0.000001
[ Tue Jul  9 22:52:52 2024 ] 	Batch(900/7879) done. Loss: 0.0081  lr:0.000001
[ Tue Jul  9 22:53:14 2024 ] 
Training: Epoch [118/120], Step [999], Loss: 0.045650895684957504, Training Accuracy: 96.5125
[ Tue Jul  9 22:53:14 2024 ] 	Batch(1000/7879) done. Loss: 0.1427  lr:0.000001
[ Tue Jul  9 22:53:37 2024 ] 	Batch(1100/7879) done. Loss: 0.0416  lr:0.000001
[ Tue Jul  9 22:54:00 2024 ] 	Batch(1200/7879) done. Loss: 0.4670  lr:0.000001
[ Tue Jul  9 22:54:23 2024 ] 	Batch(1300/7879) done. Loss: 0.0897  lr:0.000001
[ Tue Jul  9 22:54:45 2024 ] 	Batch(1400/7879) done. Loss: 0.1395  lr:0.000001
[ Tue Jul  9 22:55:08 2024 ] 
Training: Epoch [118/120], Step [1499], Loss: 0.03209231048822403, Training Accuracy: 96.46666666666667
[ Tue Jul  9 22:55:08 2024 ] 	Batch(1500/7879) done. Loss: 0.1883  lr:0.000001
[ Tue Jul  9 22:55:31 2024 ] 	Batch(1600/7879) done. Loss: 0.1720  lr:0.000001
[ Tue Jul  9 22:55:54 2024 ] 	Batch(1700/7879) done. Loss: 0.0312  lr:0.000001
[ Tue Jul  9 22:56:16 2024 ] 	Batch(1800/7879) done. Loss: 0.0767  lr:0.000001
[ Tue Jul  9 22:56:39 2024 ] 	Batch(1900/7879) done. Loss: 0.1742  lr:0.000001
[ Tue Jul  9 22:57:02 2024 ] 
Training: Epoch [118/120], Step [1999], Loss: 0.011760873720049858, Training Accuracy: 96.23125
[ Tue Jul  9 22:57:03 2024 ] 	Batch(2000/7879) done. Loss: 0.0090  lr:0.000001
[ Tue Jul  9 22:57:25 2024 ] 	Batch(2100/7879) done. Loss: 0.1058  lr:0.000001
[ Tue Jul  9 22:57:48 2024 ] 	Batch(2200/7879) done. Loss: 0.1395  lr:0.000001
[ Tue Jul  9 22:58:11 2024 ] 	Batch(2300/7879) done. Loss: 0.5411  lr:0.000001
[ Tue Jul  9 22:58:34 2024 ] 	Batch(2400/7879) done. Loss: 0.0129  lr:0.000001
[ Tue Jul  9 22:58:57 2024 ] 
Training: Epoch [118/120], Step [2499], Loss: 0.05740341171622276, Training Accuracy: 96.28
[ Tue Jul  9 22:58:58 2024 ] 	Batch(2500/7879) done. Loss: 0.1478  lr:0.000001
[ Tue Jul  9 22:59:21 2024 ] 	Batch(2600/7879) done. Loss: 0.0356  lr:0.000001
[ Tue Jul  9 22:59:45 2024 ] 	Batch(2700/7879) done. Loss: 0.0127  lr:0.000001
[ Tue Jul  9 23:00:09 2024 ] 	Batch(2800/7879) done. Loss: 0.1972  lr:0.000001
[ Tue Jul  9 23:00:32 2024 ] 	Batch(2900/7879) done. Loss: 0.1336  lr:0.000001
[ Tue Jul  9 23:00:55 2024 ] 
Training: Epoch [118/120], Step [2999], Loss: 0.04161424934864044, Training Accuracy: 96.2375
[ Tue Jul  9 23:00:55 2024 ] 	Batch(3000/7879) done. Loss: 0.1467  lr:0.000001
[ Tue Jul  9 23:01:18 2024 ] 	Batch(3100/7879) done. Loss: 0.2542  lr:0.000001
[ Tue Jul  9 23:01:41 2024 ] 	Batch(3200/7879) done. Loss: 0.0969  lr:0.000001
[ Tue Jul  9 23:02:03 2024 ] 	Batch(3300/7879) done. Loss: 0.0231  lr:0.000001
[ Tue Jul  9 23:02:26 2024 ] 	Batch(3400/7879) done. Loss: 0.0320  lr:0.000001
[ Tue Jul  9 23:02:48 2024 ] 
Training: Epoch [118/120], Step [3499], Loss: 0.006261046975851059, Training Accuracy: 96.22142857142858
[ Tue Jul  9 23:02:49 2024 ] 	Batch(3500/7879) done. Loss: 0.1095  lr:0.000001
[ Tue Jul  9 23:03:11 2024 ] 	Batch(3600/7879) done. Loss: 0.1961  lr:0.000001
[ Tue Jul  9 23:03:34 2024 ] 	Batch(3700/7879) done. Loss: 0.2423  lr:0.000001
[ Tue Jul  9 23:03:57 2024 ] 	Batch(3800/7879) done. Loss: 0.0511  lr:0.000001
[ Tue Jul  9 23:04:19 2024 ] 	Batch(3900/7879) done. Loss: 0.0215  lr:0.000001
[ Tue Jul  9 23:04:42 2024 ] 
Training: Epoch [118/120], Step [3999], Loss: 0.05070377141237259, Training Accuracy: 96.215625
[ Tue Jul  9 23:04:42 2024 ] 	Batch(4000/7879) done. Loss: 0.0439  lr:0.000001
[ Tue Jul  9 23:05:05 2024 ] 	Batch(4100/7879) done. Loss: 0.0536  lr:0.000001
[ Tue Jul  9 23:05:27 2024 ] 	Batch(4200/7879) done. Loss: 0.2947  lr:0.000001
[ Tue Jul  9 23:05:50 2024 ] 	Batch(4300/7879) done. Loss: 0.3647  lr:0.000001
[ Tue Jul  9 23:06:12 2024 ] 	Batch(4400/7879) done. Loss: 0.2359  lr:0.000001
[ Tue Jul  9 23:06:35 2024 ] 
Training: Epoch [118/120], Step [4499], Loss: 0.010106594301760197, Training Accuracy: 96.21111111111111
[ Tue Jul  9 23:06:36 2024 ] 	Batch(4500/7879) done. Loss: 0.3379  lr:0.000001
[ Tue Jul  9 23:06:58 2024 ] 	Batch(4600/7879) done. Loss: 0.0241  lr:0.000001
[ Tue Jul  9 23:07:21 2024 ] 	Batch(4700/7879) done. Loss: 0.2825  lr:0.000001
[ Tue Jul  9 23:07:44 2024 ] 	Batch(4800/7879) done. Loss: 0.0067  lr:0.000001
[ Tue Jul  9 23:08:06 2024 ] 	Batch(4900/7879) done. Loss: 0.1543  lr:0.000001
[ Tue Jul  9 23:08:29 2024 ] 
Training: Epoch [118/120], Step [4999], Loss: 0.4482000172138214, Training Accuracy: 96.185
[ Tue Jul  9 23:08:29 2024 ] 	Batch(5000/7879) done. Loss: 0.1084  lr:0.000001
[ Tue Jul  9 23:08:51 2024 ] 	Batch(5100/7879) done. Loss: 0.0646  lr:0.000001
[ Tue Jul  9 23:09:14 2024 ] 	Batch(5200/7879) done. Loss: 0.0629  lr:0.000001
[ Tue Jul  9 23:09:37 2024 ] 	Batch(5300/7879) done. Loss: 0.0470  lr:0.000001
[ Tue Jul  9 23:09:59 2024 ] 	Batch(5400/7879) done. Loss: 0.1614  lr:0.000001
[ Tue Jul  9 23:10:22 2024 ] 
Training: Epoch [118/120], Step [5499], Loss: 0.11697587370872498, Training Accuracy: 96.20681818181818
[ Tue Jul  9 23:10:22 2024 ] 	Batch(5500/7879) done. Loss: 0.3811  lr:0.000001
[ Tue Jul  9 23:10:45 2024 ] 	Batch(5600/7879) done. Loss: 0.0604  lr:0.000001
[ Tue Jul  9 23:11:08 2024 ] 	Batch(5700/7879) done. Loss: 0.0165  lr:0.000001
[ Tue Jul  9 23:11:31 2024 ] 	Batch(5800/7879) done. Loss: 0.1606  lr:0.000001
[ Tue Jul  9 23:11:54 2024 ] 	Batch(5900/7879) done. Loss: 0.1409  lr:0.000001
[ Tue Jul  9 23:12:17 2024 ] 
Training: Epoch [118/120], Step [5999], Loss: 0.01528959907591343, Training Accuracy: 96.19375
[ Tue Jul  9 23:12:18 2024 ] 	Batch(6000/7879) done. Loss: 0.4354  lr:0.000001
[ Tue Jul  9 23:12:41 2024 ] 	Batch(6100/7879) done. Loss: 0.0244  lr:0.000001
[ Tue Jul  9 23:13:05 2024 ] 	Batch(6200/7879) done. Loss: 0.0085  lr:0.000001
[ Tue Jul  9 23:13:28 2024 ] 	Batch(6300/7879) done. Loss: 0.5199  lr:0.000001
[ Tue Jul  9 23:13:51 2024 ] 	Batch(6400/7879) done. Loss: 0.0041  lr:0.000001
[ Tue Jul  9 23:14:15 2024 ] 
Training: Epoch [118/120], Step [6499], Loss: 0.383468359708786, Training Accuracy: 96.17115384615384
[ Tue Jul  9 23:14:15 2024 ] 	Batch(6500/7879) done. Loss: 0.3754  lr:0.000001
[ Tue Jul  9 23:14:38 2024 ] 	Batch(6600/7879) done. Loss: 0.0236  lr:0.000001
[ Tue Jul  9 23:15:01 2024 ] 	Batch(6700/7879) done. Loss: 0.0546  lr:0.000001
[ Tue Jul  9 23:15:25 2024 ] 	Batch(6800/7879) done. Loss: 0.0604  lr:0.000001
[ Tue Jul  9 23:15:48 2024 ] 	Batch(6900/7879) done. Loss: 0.4277  lr:0.000001
[ Tue Jul  9 23:16:11 2024 ] 
Training: Epoch [118/120], Step [6999], Loss: 0.2876323461532593, Training Accuracy: 96.12857142857143
[ Tue Jul  9 23:16:11 2024 ] 	Batch(7000/7879) done. Loss: 0.1907  lr:0.000001
[ Tue Jul  9 23:16:35 2024 ] 	Batch(7100/7879) done. Loss: 0.0293  lr:0.000001
[ Tue Jul  9 23:16:58 2024 ] 	Batch(7200/7879) done. Loss: 0.0095  lr:0.000001
[ Tue Jul  9 23:17:22 2024 ] 	Batch(7300/7879) done. Loss: 0.0352  lr:0.000001
[ Tue Jul  9 23:17:45 2024 ] 	Batch(7400/7879) done. Loss: 0.4178  lr:0.000001
[ Tue Jul  9 23:18:08 2024 ] 
Training: Epoch [118/120], Step [7499], Loss: 0.0021338884253054857, Training Accuracy: 96.16166666666666
[ Tue Jul  9 23:18:08 2024 ] 	Batch(7500/7879) done. Loss: 0.0142  lr:0.000001
[ Tue Jul  9 23:18:31 2024 ] 	Batch(7600/7879) done. Loss: 0.0128  lr:0.000001
[ Tue Jul  9 23:18:54 2024 ] 	Batch(7700/7879) done. Loss: 0.0424  lr:0.000001
[ Tue Jul  9 23:19:17 2024 ] 	Batch(7800/7879) done. Loss: 0.0250  lr:0.000001
[ Tue Jul  9 23:19:35 2024 ] 	Mean training loss: 0.1417.
[ Tue Jul  9 23:19:35 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 23:19:36 2024 ] Training epoch: 120
[ Tue Jul  9 23:19:36 2024 ] 	Batch(0/7879) done. Loss: 0.3352  lr:0.000001
[ Tue Jul  9 23:19:59 2024 ] 	Batch(100/7879) done. Loss: 0.0686  lr:0.000001
[ Tue Jul  9 23:20:22 2024 ] 	Batch(200/7879) done. Loss: 0.9084  lr:0.000001
[ Tue Jul  9 23:20:46 2024 ] 	Batch(300/7879) done. Loss: 0.0167  lr:0.000001
[ Tue Jul  9 23:21:09 2024 ] 	Batch(400/7879) done. Loss: 0.1061  lr:0.000001
[ Tue Jul  9 23:21:32 2024 ] 
Training: Epoch [119/120], Step [499], Loss: 0.025461381301283836, Training Accuracy: 96.1
[ Tue Jul  9 23:21:32 2024 ] 	Batch(500/7879) done. Loss: 0.1536  lr:0.000001
[ Tue Jul  9 23:21:54 2024 ] 	Batch(600/7879) done. Loss: 0.0137  lr:0.000001
[ Tue Jul  9 23:22:17 2024 ] 	Batch(700/7879) done. Loss: 0.1897  lr:0.000001
[ Tue Jul  9 23:22:40 2024 ] 	Batch(800/7879) done. Loss: 0.0301  lr:0.000001
[ Tue Jul  9 23:23:02 2024 ] 	Batch(900/7879) done. Loss: 0.0120  lr:0.000001
[ Tue Jul  9 23:23:25 2024 ] 
Training: Epoch [119/120], Step [999], Loss: 0.4421793818473816, Training Accuracy: 96.22500000000001
[ Tue Jul  9 23:23:25 2024 ] 	Batch(1000/7879) done. Loss: 0.0167  lr:0.000001
[ Tue Jul  9 23:23:47 2024 ] 	Batch(1100/7879) done. Loss: 0.1689  lr:0.000001
[ Tue Jul  9 23:24:10 2024 ] 	Batch(1200/7879) done. Loss: 0.1701  lr:0.000001
[ Tue Jul  9 23:24:33 2024 ] 	Batch(1300/7879) done. Loss: 0.0473  lr:0.000001
[ Tue Jul  9 23:24:55 2024 ] 	Batch(1400/7879) done. Loss: 0.6273  lr:0.000001
[ Tue Jul  9 23:25:18 2024 ] 
Training: Epoch [119/120], Step [1499], Loss: 0.032002825289964676, Training Accuracy: 96.39999999999999
[ Tue Jul  9 23:25:18 2024 ] 	Batch(1500/7879) done. Loss: 0.0105  lr:0.000001
[ Tue Jul  9 23:25:40 2024 ] 	Batch(1600/7879) done. Loss: 0.2912  lr:0.000001
[ Tue Jul  9 23:26:04 2024 ] 	Batch(1700/7879) done. Loss: 0.5343  lr:0.000001
[ Tue Jul  9 23:26:27 2024 ] 	Batch(1800/7879) done. Loss: 0.0429  lr:0.000001
[ Tue Jul  9 23:26:50 2024 ] 	Batch(1900/7879) done. Loss: 0.2327  lr:0.000001
[ Tue Jul  9 23:27:13 2024 ] 
Training: Epoch [119/120], Step [1999], Loss: 0.021731166169047356, Training Accuracy: 96.43125
[ Tue Jul  9 23:27:14 2024 ] 	Batch(2000/7879) done. Loss: 0.0228  lr:0.000001
[ Tue Jul  9 23:27:36 2024 ] 	Batch(2100/7879) done. Loss: 0.0282  lr:0.000001
[ Tue Jul  9 23:27:59 2024 ] 	Batch(2200/7879) done. Loss: 0.3457  lr:0.000001
[ Tue Jul  9 23:28:21 2024 ] 	Batch(2300/7879) done. Loss: 0.2229  lr:0.000001
[ Tue Jul  9 23:28:44 2024 ] 	Batch(2400/7879) done. Loss: 0.2156  lr:0.000001
[ Tue Jul  9 23:29:06 2024 ] 
Training: Epoch [119/120], Step [2499], Loss: 0.3611356019973755, Training Accuracy: 96.47
[ Tue Jul  9 23:29:07 2024 ] 	Batch(2500/7879) done. Loss: 0.0046  lr:0.000001
[ Tue Jul  9 23:29:29 2024 ] 	Batch(2600/7879) done. Loss: 0.0096  lr:0.000001
[ Tue Jul  9 23:29:52 2024 ] 	Batch(2700/7879) done. Loss: 0.1281  lr:0.000001
[ Tue Jul  9 23:30:14 2024 ] 	Batch(2800/7879) done. Loss: 0.3389  lr:0.000001
[ Tue Jul  9 23:30:37 2024 ] 	Batch(2900/7879) done. Loss: 0.0697  lr:0.000001
[ Tue Jul  9 23:30:59 2024 ] 
Training: Epoch [119/120], Step [2999], Loss: 0.08007586002349854, Training Accuracy: 96.4375
[ Tue Jul  9 23:31:00 2024 ] 	Batch(3000/7879) done. Loss: 0.1622  lr:0.000001
[ Tue Jul  9 23:31:22 2024 ] 	Batch(3100/7879) done. Loss: 0.0388  lr:0.000001
[ Tue Jul  9 23:31:45 2024 ] 	Batch(3200/7879) done. Loss: 0.0099  lr:0.000001
[ Tue Jul  9 23:32:09 2024 ] 	Batch(3300/7879) done. Loss: 0.6001  lr:0.000001
[ Tue Jul  9 23:32:32 2024 ] 	Batch(3400/7879) done. Loss: 0.0054  lr:0.000001
[ Tue Jul  9 23:32:55 2024 ] 
Training: Epoch [119/120], Step [3499], Loss: 0.04192308709025383, Training Accuracy: 96.41071428571428
[ Tue Jul  9 23:32:55 2024 ] 	Batch(3500/7879) done. Loss: 0.0595  lr:0.000001
[ Tue Jul  9 23:33:18 2024 ] 	Batch(3600/7879) done. Loss: 0.3301  lr:0.000001
[ Tue Jul  9 23:33:42 2024 ] 	Batch(3700/7879) done. Loss: 0.0831  lr:0.000001
[ Tue Jul  9 23:34:05 2024 ] 	Batch(3800/7879) done. Loss: 0.2535  lr:0.000001
[ Tue Jul  9 23:34:28 2024 ] 	Batch(3900/7879) done. Loss: 0.2605  lr:0.000001
[ Tue Jul  9 23:34:51 2024 ] 
Training: Epoch [119/120], Step [3999], Loss: 0.02321418933570385, Training Accuracy: 96.421875
[ Tue Jul  9 23:34:52 2024 ] 	Batch(4000/7879) done. Loss: 0.0467  lr:0.000001
[ Tue Jul  9 23:35:15 2024 ] 	Batch(4100/7879) done. Loss: 0.1373  lr:0.000001
[ Tue Jul  9 23:35:38 2024 ] 	Batch(4200/7879) done. Loss: 0.1981  lr:0.000001
[ Tue Jul  9 23:36:01 2024 ] 	Batch(4300/7879) done. Loss: 0.0346  lr:0.000001
[ Tue Jul  9 23:36:24 2024 ] 	Batch(4400/7879) done. Loss: 0.1986  lr:0.000001
[ Tue Jul  9 23:36:47 2024 ] 
Training: Epoch [119/120], Step [4499], Loss: 0.10653466731309891, Training Accuracy: 96.36944444444444
[ Tue Jul  9 23:36:48 2024 ] 	Batch(4500/7879) done. Loss: 0.0205  lr:0.000001
[ Tue Jul  9 23:37:11 2024 ] 	Batch(4600/7879) done. Loss: 0.4499  lr:0.000001
[ Tue Jul  9 23:37:33 2024 ] 	Batch(4700/7879) done. Loss: 0.0095  lr:0.000001
[ Tue Jul  9 23:37:56 2024 ] 	Batch(4800/7879) done. Loss: 0.3475  lr:0.000001
[ Tue Jul  9 23:38:19 2024 ] 	Batch(4900/7879) done. Loss: 0.2532  lr:0.000001
[ Tue Jul  9 23:38:41 2024 ] 
Training: Epoch [119/120], Step [4999], Loss: 0.033951468765735626, Training Accuracy: 96.3275
[ Tue Jul  9 23:38:41 2024 ] 	Batch(5000/7879) done. Loss: 0.2488  lr:0.000001
[ Tue Jul  9 23:39:04 2024 ] 	Batch(5100/7879) done. Loss: 0.0455  lr:0.000001
[ Tue Jul  9 23:39:27 2024 ] 	Batch(5200/7879) done. Loss: 0.0825  lr:0.000001
[ Tue Jul  9 23:39:49 2024 ] 	Batch(5300/7879) done. Loss: 0.1214  lr:0.000001
[ Tue Jul  9 23:40:12 2024 ] 	Batch(5400/7879) done. Loss: 0.0512  lr:0.000001
[ Tue Jul  9 23:40:34 2024 ] 
Training: Epoch [119/120], Step [5499], Loss: 0.028406303375959396, Training Accuracy: 96.26136363636364
[ Tue Jul  9 23:40:34 2024 ] 	Batch(5500/7879) done. Loss: 0.1498  lr:0.000001
[ Tue Jul  9 23:40:57 2024 ] 	Batch(5600/7879) done. Loss: 0.1071  lr:0.000001
[ Tue Jul  9 23:41:20 2024 ] 	Batch(5700/7879) done. Loss: 0.1891  lr:0.000001
[ Tue Jul  9 23:41:42 2024 ] 	Batch(5800/7879) done. Loss: 0.5593  lr:0.000001
[ Tue Jul  9 23:42:05 2024 ] 	Batch(5900/7879) done. Loss: 0.0206  lr:0.000001
[ Tue Jul  9 23:42:27 2024 ] 
Training: Epoch [119/120], Step [5999], Loss: 0.06390488892793655, Training Accuracy: 96.26041666666667
[ Tue Jul  9 23:42:28 2024 ] 	Batch(6000/7879) done. Loss: 0.1525  lr:0.000001
[ Tue Jul  9 23:42:50 2024 ] 	Batch(6100/7879) done. Loss: 0.0227  lr:0.000001
[ Tue Jul  9 23:43:13 2024 ] 	Batch(6200/7879) done. Loss: 0.4815  lr:0.000001
[ Tue Jul  9 23:43:35 2024 ] 	Batch(6300/7879) done. Loss: 0.2567  lr:0.000001
[ Tue Jul  9 23:43:58 2024 ] 	Batch(6400/7879) done. Loss: 0.1555  lr:0.000001
[ Tue Jul  9 23:44:20 2024 ] 
Training: Epoch [119/120], Step [6499], Loss: 0.2513778507709503, Training Accuracy: 96.25961538461539
[ Tue Jul  9 23:44:21 2024 ] 	Batch(6500/7879) done. Loss: 0.1653  lr:0.000001
[ Tue Jul  9 23:44:43 2024 ] 	Batch(6600/7879) done. Loss: 0.0779  lr:0.000001
[ Tue Jul  9 23:45:06 2024 ] 	Batch(6700/7879) done. Loss: 0.0231  lr:0.000001
[ Tue Jul  9 23:45:28 2024 ] 	Batch(6800/7879) done. Loss: 0.0613  lr:0.000001
[ Tue Jul  9 23:45:52 2024 ] 	Batch(6900/7879) done. Loss: 0.0646  lr:0.000001
[ Tue Jul  9 23:46:15 2024 ] 
Training: Epoch [119/120], Step [6999], Loss: 0.04075547307729721, Training Accuracy: 96.25714285714285
[ Tue Jul  9 23:46:15 2024 ] 	Batch(7000/7879) done. Loss: 0.0731  lr:0.000001
[ Tue Jul  9 23:46:38 2024 ] 	Batch(7100/7879) done. Loss: 0.0620  lr:0.000001
[ Tue Jul  9 23:47:01 2024 ] 	Batch(7200/7879) done. Loss: 0.6177  lr:0.000001
[ Tue Jul  9 23:47:24 2024 ] 	Batch(7300/7879) done. Loss: 0.1451  lr:0.000001
[ Tue Jul  9 23:47:46 2024 ] 	Batch(7400/7879) done. Loss: 0.0148  lr:0.000001
[ Tue Jul  9 23:48:09 2024 ] 
Training: Epoch [119/120], Step [7499], Loss: 0.002019717125222087, Training Accuracy: 96.25166666666667
[ Tue Jul  9 23:48:09 2024 ] 	Batch(7500/7879) done. Loss: 0.2421  lr:0.000001
[ Tue Jul  9 23:48:32 2024 ] 	Batch(7600/7879) done. Loss: 0.2877  lr:0.000001
[ Tue Jul  9 23:48:54 2024 ] 	Batch(7700/7879) done. Loss: 0.2969  lr:0.000001
[ Tue Jul  9 23:49:17 2024 ] 	Batch(7800/7879) done. Loss: 0.0127  lr:0.000001
[ Tue Jul  9 23:49:35 2024 ] 	Mean training loss: 0.1390.
[ Tue Jul  9 23:49:35 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul  9 23:49:35 2024 ] Eval epoch: 120
[ Tue Jul  9 23:55:31 2024 ] 	Mean val loss of 6365 batches: 1.017563039812066.
[ Tue Jul  9 23:55:31 2024 ] 
Validation: Epoch [119/120], Samples [39451.0/50919], Loss: 0.4690995216369629, Validation Accuracy: 77.47795518372317
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 1 : 194 / 275 = 70 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 2 : 237 / 273 = 86 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 3 : 232 / 273 = 84 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 4 : 227 / 275 = 82 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 5 : 232 / 275 = 84 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 6 : 214 / 275 = 77 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 7 : 254 / 273 = 93 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 8 : 266 / 273 = 97 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 9 : 205 / 273 = 75 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 10 : 112 / 273 = 41 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 11 : 141 / 272 = 51 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 12 : 219 / 271 = 80 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 13 : 264 / 275 = 96 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 14 : 261 / 276 = 94 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 15 : 211 / 273 = 77 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 16 : 165 / 274 = 60 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 17 : 241 / 273 = 88 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 18 : 235 / 274 = 85 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 19 : 245 / 272 = 90 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 20 : 253 / 273 = 92 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 21 : 227 / 274 = 82 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 22 : 248 / 274 = 90 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 23 : 248 / 276 = 89 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 24 : 242 / 274 = 88 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 25 : 265 / 275 = 96 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 26 : 268 / 276 = 97 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 27 : 236 / 275 = 85 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 28 : 179 / 275 = 65 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 29 : 160 / 275 = 58 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 30 : 168 / 276 = 60 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 31 : 237 / 276 = 85 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 32 : 236 / 276 = 85 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 33 : 235 / 276 = 85 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 34 : 243 / 276 = 88 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 35 : 232 / 275 = 84 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 36 : 219 / 276 = 79 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 37 : 252 / 276 = 91 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 38 : 245 / 276 = 88 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 39 : 236 / 276 = 85 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 40 : 193 / 276 = 69 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 41 : 264 / 276 = 95 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 42 : 251 / 275 = 91 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 43 : 184 / 276 = 66 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 44 : 256 / 276 = 92 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 45 : 260 / 276 = 94 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 46 : 223 / 276 = 80 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 47 : 202 / 275 = 73 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 48 : 226 / 275 = 82 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 49 : 211 / 274 = 77 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 50 : 240 / 276 = 86 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 51 : 258 / 276 = 93 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 52 : 250 / 276 = 90 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 53 : 238 / 276 = 86 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 54 : 258 / 274 = 94 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 55 : 241 / 276 = 87 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 56 : 246 / 275 = 89 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 57 : 269 / 276 = 97 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 58 : 267 / 273 = 97 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 59 : 252 / 276 = 91 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 60 : 475 / 561 = 84 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 61 : 464 / 566 = 81 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 62 : 411 / 572 = 71 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 63 : 491 / 570 = 86 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 64 : 410 / 574 = 71 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 65 : 505 / 573 = 88 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 66 : 415 / 573 = 72 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 67 : 388 / 575 = 67 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 68 : 367 / 575 = 63 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 69 : 469 / 575 = 81 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 70 : 252 / 575 = 43 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 71 : 248 / 575 = 43 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 72 : 95 / 571 = 16 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 73 : 265 / 570 = 46 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 74 : 325 / 569 = 57 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 75 : 386 / 573 = 67 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 76 : 370 / 574 = 64 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 77 : 389 / 573 = 67 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 78 : 427 / 575 = 74 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 79 : 549 / 574 = 95 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 80 : 457 / 573 = 79 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 81 : 377 / 575 = 65 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 82 : 355 / 575 = 61 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 83 : 274 / 572 = 47 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 84 : 427 / 574 = 74 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 85 : 370 / 574 = 64 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 86 : 501 / 575 = 87 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 87 : 480 / 576 = 83 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 88 : 394 / 575 = 68 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 89 : 442 / 576 = 76 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 90 : 250 / 574 = 43 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 91 : 451 / 568 = 79 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 92 : 427 / 576 = 74 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 93 : 380 / 573 = 66 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 94 : 526 / 574 = 91 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 95 : 539 / 575 = 93 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 96 : 557 / 575 = 96 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 97 : 549 / 574 = 95 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 98 : 539 / 575 = 93 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 99 : 523 / 574 = 91 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 100 : 461 / 574 = 80 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 101 : 530 / 574 = 92 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 102 : 335 / 575 = 58 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 103 : 488 / 576 = 84 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 104 : 239 / 575 = 41 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 105 : 272 / 575 = 47 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 106 : 330 / 576 = 57 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 107 : 508 / 576 = 88 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 108 : 470 / 575 = 81 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 109 : 406 / 575 = 70 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 110 : 508 / 575 = 88 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 111 : 539 / 576 = 93 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 112 : 539 / 575 = 93 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 113 : 519 / 576 = 90 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 114 : 504 / 576 = 87 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 115 : 529 / 576 = 91 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 116 : 484 / 575 = 84 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 117 : 474 / 575 = 82 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 118 : 474 / 575 = 82 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 119 : 510 / 576 = 88 %
[ Tue Jul  9 23:55:31 2024 ] Accuracy of 120 : 240 / 274 = 87 %
[ Tue Jul  9 23:55:31 2024 ] Load weights from ./prova20/epoch119_model.pt.
[ Wed Jul 10 20:04:56 2024 ] Load weights from prova20/epoch119_model.pt.
[ Wed Jul 10 20:04:56 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xsub/train_joint_120.npy', 'label_path': 'new_data_processed/xsub/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xsub/val_joint_120.npy', 'label_path': 'new_data_processed/xsub/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': False, 'only_attention': True, 'tcn_attention': True, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': 'prova20/epoch119_model.pt', 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': False, 'scheduler': 1, 'base_lr': 1e-06, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Wed Jul 10 20:04:56 2024 ] Training epoch: 1
[ Wed Jul 10 20:04:58 2024 ] 	Batch(0/7879) done. Loss: 0.1404  lr:0.000001
[ Wed Jul 10 20:05:20 2024 ] 	Batch(100/7879) done. Loss: 0.0451  lr:0.000001
[ Wed Jul 10 20:05:42 2024 ] 	Batch(200/7879) done. Loss: 0.0303  lr:0.000001
[ Wed Jul 10 20:06:04 2024 ] 	Batch(300/7879) done. Loss: 0.3479  lr:0.000001
[ Wed Jul 10 20:06:26 2024 ] 	Batch(400/7879) done. Loss: 0.1351  lr:0.000001
[ Wed Jul 10 20:06:48 2024 ] 
Training: Epoch [0/120], Step [499], Loss: 0.34024596214294434, Training Accuracy: 96.22500000000001
[ Wed Jul 10 20:06:48 2024 ] 	Batch(500/7879) done. Loss: 0.0202  lr:0.000001
[ Wed Jul 10 20:07:10 2024 ] 	Batch(600/7879) done. Loss: 0.0377  lr:0.000001
[ Wed Jul 10 20:07:32 2024 ] 	Batch(700/7879) done. Loss: 0.0361  lr:0.000001
[ Wed Jul 10 20:07:55 2024 ] 	Batch(800/7879) done. Loss: 0.0047  lr:0.000001
[ Wed Jul 10 20:08:17 2024 ] 	Batch(900/7879) done. Loss: 0.0881  lr:0.000001
[ Wed Jul 10 20:08:40 2024 ] 
Training: Epoch [0/120], Step [999], Loss: 0.01443811971694231, Training Accuracy: 96.375
[ Wed Jul 10 20:08:40 2024 ] 	Batch(1000/7879) done. Loss: 0.0078  lr:0.000001
[ Wed Jul 10 20:09:03 2024 ] 	Batch(1100/7879) done. Loss: 0.2876  lr:0.000001
[ Wed Jul 10 20:09:25 2024 ] 	Batch(1200/7879) done. Loss: 0.0210  lr:0.000001
[ Wed Jul 10 20:09:47 2024 ] 	Batch(1300/7879) done. Loss: 0.1451  lr:0.000001
[ Wed Jul 10 20:10:09 2024 ] 	Batch(1400/7879) done. Loss: 0.2460  lr:0.000001
[ Wed Jul 10 20:10:31 2024 ] 
Training: Epoch [0/120], Step [1499], Loss: 0.42984405159950256, Training Accuracy: 96.5
[ Wed Jul 10 20:10:31 2024 ] 	Batch(1500/7879) done. Loss: 0.1270  lr:0.000001
[ Wed Jul 10 20:10:54 2024 ] 	Batch(1600/7879) done. Loss: 0.1268  lr:0.000001
[ Wed Jul 10 20:11:16 2024 ] 	Batch(1700/7879) done. Loss: 0.1829  lr:0.000001
[ Wed Jul 10 20:11:38 2024 ] 	Batch(1800/7879) done. Loss: 0.3317  lr:0.000001
[ Wed Jul 10 20:12:00 2024 ] 	Batch(1900/7879) done. Loss: 0.1595  lr:0.000001
[ Wed Jul 10 20:12:22 2024 ] 
Training: Epoch [0/120], Step [1999], Loss: 0.21578890085220337, Training Accuracy: 96.33749999999999
[ Wed Jul 10 20:12:22 2024 ] 	Batch(2000/7879) done. Loss: 0.0174  lr:0.000001
[ Wed Jul 10 20:12:44 2024 ] 	Batch(2100/7879) done. Loss: 0.8580  lr:0.000001
[ Wed Jul 10 20:13:06 2024 ] 	Batch(2200/7879) done. Loss: 0.0171  lr:0.000001
[ Wed Jul 10 20:13:28 2024 ] 	Batch(2300/7879) done. Loss: 0.2090  lr:0.000001
[ Wed Jul 10 20:13:50 2024 ] 	Batch(2400/7879) done. Loss: 0.0147  lr:0.000001
[ Wed Jul 10 20:14:12 2024 ] 
Training: Epoch [0/120], Step [2499], Loss: 0.36073365807533264, Training Accuracy: 96.35000000000001
[ Wed Jul 10 20:14:12 2024 ] 	Batch(2500/7879) done. Loss: 0.0458  lr:0.000001
[ Wed Jul 10 20:14:34 2024 ] 	Batch(2600/7879) done. Loss: 0.0589  lr:0.000001
[ Wed Jul 10 20:14:56 2024 ] 	Batch(2700/7879) done. Loss: 0.0281  lr:0.000001
[ Wed Jul 10 20:15:18 2024 ] 	Batch(2800/7879) done. Loss: 0.0448  lr:0.000001
[ Wed Jul 10 20:15:40 2024 ] 	Batch(2900/7879) done. Loss: 0.1010  lr:0.000001
[ Wed Jul 10 20:16:02 2024 ] 
Training: Epoch [0/120], Step [2999], Loss: 0.04062632471323013, Training Accuracy: 96.26249999999999
[ Wed Jul 10 20:16:02 2024 ] 	Batch(3000/7879) done. Loss: 0.0242  lr:0.000001
[ Wed Jul 10 20:16:24 2024 ] 	Batch(3100/7879) done. Loss: 0.2387  lr:0.000001
[ Wed Jul 10 20:16:47 2024 ] 	Batch(3200/7879) done. Loss: 0.0424  lr:0.000001
[ Wed Jul 10 20:17:09 2024 ] 	Batch(3300/7879) done. Loss: 0.1200  lr:0.000001
[ Wed Jul 10 20:17:32 2024 ] 	Batch(3400/7879) done. Loss: 0.2358  lr:0.000001
[ Wed Jul 10 20:17:54 2024 ] 
Training: Epoch [0/120], Step [3499], Loss: 0.02209179848432541, Training Accuracy: 96.30357142857143
[ Wed Jul 10 20:17:55 2024 ] 	Batch(3500/7879) done. Loss: 0.0325  lr:0.000001
[ Wed Jul 10 20:18:18 2024 ] 	Batch(3600/7879) done. Loss: 0.0260  lr:0.000001
[ Wed Jul 10 20:18:40 2024 ] 	Batch(3700/7879) done. Loss: 0.0109  lr:0.000001
[ Wed Jul 10 20:19:02 2024 ] 	Batch(3800/7879) done. Loss: 0.1112  lr:0.000001
[ Wed Jul 10 20:19:23 2024 ] 	Batch(3900/7879) done. Loss: 0.3450  lr:0.000001
[ Wed Jul 10 20:19:45 2024 ] 
Training: Epoch [0/120], Step [3999], Loss: 0.020431391894817352, Training Accuracy: 96.346875
[ Wed Jul 10 20:19:46 2024 ] 	Batch(4000/7879) done. Loss: 0.5496  lr:0.000001
[ Wed Jul 10 20:20:08 2024 ] 	Batch(4100/7879) done. Loss: 0.0555  lr:0.000001
[ Wed Jul 10 20:20:30 2024 ] 	Batch(4200/7879) done. Loss: 0.4188  lr:0.000001
[ Wed Jul 10 20:20:51 2024 ] 	Batch(4300/7879) done. Loss: 0.1145  lr:0.000001
[ Wed Jul 10 20:21:14 2024 ] 	Batch(4400/7879) done. Loss: 0.1367  lr:0.000001
[ Wed Jul 10 20:21:35 2024 ] 
Training: Epoch [0/120], Step [4499], Loss: 0.3619225323200226, Training Accuracy: 96.35833333333333
[ Wed Jul 10 20:21:36 2024 ] 	Batch(4500/7879) done. Loss: 0.0281  lr:0.000001
[ Wed Jul 10 20:21:58 2024 ] 	Batch(4600/7879) done. Loss: 0.1578  lr:0.000001
[ Wed Jul 10 20:22:20 2024 ] 	Batch(4700/7879) done. Loss: 0.0687  lr:0.000001
[ Wed Jul 10 20:22:42 2024 ] 	Batch(4800/7879) done. Loss: 0.0544  lr:0.000001
[ Wed Jul 10 20:23:04 2024 ] 	Batch(4900/7879) done. Loss: 0.3459  lr:0.000001
[ Wed Jul 10 20:23:25 2024 ] 
Training: Epoch [0/120], Step [4999], Loss: 0.06887549161911011, Training Accuracy: 96.3325
[ Wed Jul 10 20:23:26 2024 ] 	Batch(5000/7879) done. Loss: 0.2964  lr:0.000001
[ Wed Jul 10 20:23:48 2024 ] 	Batch(5100/7879) done. Loss: 0.1514  lr:0.000001
[ Wed Jul 10 20:24:10 2024 ] 	Batch(5200/7879) done. Loss: 0.0684  lr:0.000001
[ Wed Jul 10 20:24:32 2024 ] 	Batch(5300/7879) done. Loss: 0.3094  lr:0.000001
[ Wed Jul 10 20:24:55 2024 ] 	Batch(5400/7879) done. Loss: 0.0250  lr:0.000001
[ Wed Jul 10 20:25:17 2024 ] 
Training: Epoch [0/120], Step [5499], Loss: 0.24146221578121185, Training Accuracy: 96.3
[ Wed Jul 10 20:25:17 2024 ] 	Batch(5500/7879) done. Loss: 0.0062  lr:0.000001
[ Wed Jul 10 20:25:39 2024 ] 	Batch(5600/7879) done. Loss: 0.1109  lr:0.000001
[ Wed Jul 10 20:26:02 2024 ] 	Batch(5700/7879) done. Loss: 0.3599  lr:0.000001
[ Wed Jul 10 20:26:25 2024 ] 	Batch(5800/7879) done. Loss: 0.1129  lr:0.000001
[ Wed Jul 10 20:26:47 2024 ] 	Batch(5900/7879) done. Loss: 0.0349  lr:0.000001
[ Wed Jul 10 20:27:10 2024 ] 
Training: Epoch [0/120], Step [5999], Loss: 0.239262193441391, Training Accuracy: 96.28958333333333
[ Wed Jul 10 20:27:10 2024 ] 	Batch(6000/7879) done. Loss: 0.1244  lr:0.000001
[ Wed Jul 10 20:27:33 2024 ] 	Batch(6100/7879) done. Loss: 0.2107  lr:0.000001
[ Wed Jul 10 20:27:55 2024 ] 	Batch(6200/7879) done. Loss: 0.4302  lr:0.000001
[ Wed Jul 10 20:28:18 2024 ] 	Batch(6300/7879) done. Loss: 0.1016  lr:0.000001
[ Wed Jul 10 20:28:41 2024 ] 	Batch(6400/7879) done. Loss: 0.4737  lr:0.000001
[ Wed Jul 10 20:29:03 2024 ] 
Training: Epoch [0/120], Step [6499], Loss: 0.1249280795454979, Training Accuracy: 96.3
[ Wed Jul 10 20:29:03 2024 ] 	Batch(6500/7879) done. Loss: 0.0140  lr:0.000001
[ Wed Jul 10 20:29:25 2024 ] 	Batch(6600/7879) done. Loss: 0.1014  lr:0.000001
[ Wed Jul 10 20:29:47 2024 ] 	Batch(6700/7879) done. Loss: 0.0829  lr:0.000001
[ Wed Jul 10 20:30:10 2024 ] 	Batch(6800/7879) done. Loss: 0.1347  lr:0.000001
[ Wed Jul 10 20:30:32 2024 ] 	Batch(6900/7879) done. Loss: 0.0239  lr:0.000001
[ Wed Jul 10 20:30:54 2024 ] 
Training: Epoch [0/120], Step [6999], Loss: 0.0038082441315054893, Training Accuracy: 96.26785714285714
[ Wed Jul 10 20:30:54 2024 ] 	Batch(7000/7879) done. Loss: 0.3830  lr:0.000001
[ Wed Jul 10 20:31:16 2024 ] 	Batch(7100/7879) done. Loss: 0.1230  lr:0.000001
[ Wed Jul 10 20:31:38 2024 ] 	Batch(7200/7879) done. Loss: 0.0069  lr:0.000001
[ Wed Jul 10 20:32:00 2024 ] 	Batch(7300/7879) done. Loss: 0.2615  lr:0.000001
[ Wed Jul 10 20:32:23 2024 ] 	Batch(7400/7879) done. Loss: 0.0910  lr:0.000001
[ Wed Jul 10 20:32:45 2024 ] 
Training: Epoch [0/120], Step [7499], Loss: 0.29030436277389526, Training Accuracy: 96.26
[ Wed Jul 10 20:32:45 2024 ] 	Batch(7500/7879) done. Loss: 0.2821  lr:0.000001
[ Wed Jul 10 20:33:08 2024 ] 	Batch(7600/7879) done. Loss: 0.2409  lr:0.000001
[ Wed Jul 10 20:33:30 2024 ] 	Batch(7700/7879) done. Loss: 0.0748  lr:0.000001
[ Wed Jul 10 20:33:52 2024 ] 	Batch(7800/7879) done. Loss: 0.0617  lr:0.000001
[ Wed Jul 10 20:34:10 2024 ] 	Mean training loss: 0.1405.
[ Wed Jul 10 20:34:10 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 10 20:34:10 2024 ] Training epoch: 2
[ Wed Jul 10 20:34:10 2024 ] 	Batch(0/7879) done. Loss: 0.2814  lr:0.000001
[ Wed Jul 10 20:34:33 2024 ] 	Batch(100/7879) done. Loss: 0.0356  lr:0.000001
[ Wed Jul 10 20:34:56 2024 ] 	Batch(200/7879) done. Loss: 0.0744  lr:0.000001
[ Wed Jul 10 20:35:19 2024 ] 	Batch(300/7879) done. Loss: 0.0938  lr:0.000001
[ Wed Jul 10 20:35:42 2024 ] 	Batch(400/7879) done. Loss: 0.3224  lr:0.000001
[ Wed Jul 10 20:36:05 2024 ] 
Training: Epoch [1/120], Step [499], Loss: 0.1502377688884735, Training Accuracy: 96.22500000000001
[ Wed Jul 10 20:36:05 2024 ] 	Batch(500/7879) done. Loss: 0.1707  lr:0.000001
[ Wed Jul 10 20:36:28 2024 ] 	Batch(600/7879) done. Loss: 0.1257  lr:0.000001
[ Wed Jul 10 20:36:51 2024 ] 	Batch(700/7879) done. Loss: 0.2711  lr:0.000001
[ Wed Jul 10 20:37:15 2024 ] 	Batch(800/7879) done. Loss: 0.5556  lr:0.000001
[ Wed Jul 10 20:37:38 2024 ] 	Batch(900/7879) done. Loss: 0.3276  lr:0.000001
[ Wed Jul 10 20:38:01 2024 ] 
Training: Epoch [1/120], Step [999], Loss: 0.22916877269744873, Training Accuracy: 96.125
[ Wed Jul 10 20:38:01 2024 ] 	Batch(1000/7879) done. Loss: 0.0361  lr:0.000001
[ Wed Jul 10 20:38:24 2024 ] 	Batch(1100/7879) done. Loss: 0.0607  lr:0.000001
[ Wed Jul 10 20:38:47 2024 ] 	Batch(1200/7879) done. Loss: 0.3260  lr:0.000001
[ Wed Jul 10 20:39:11 2024 ] 	Batch(1300/7879) done. Loss: 0.0032  lr:0.000001
[ Wed Jul 10 20:39:35 2024 ] 	Batch(1400/7879) done. Loss: 0.6607  lr:0.000001
[ Wed Jul 10 20:39:58 2024 ] 
Training: Epoch [1/120], Step [1499], Loss: 0.2418196201324463, Training Accuracy: 96.175
[ Wed Jul 10 20:39:59 2024 ] 	Batch(1500/7879) done. Loss: 0.3284  lr:0.000001
[ Wed Jul 10 20:40:23 2024 ] 	Batch(1600/7879) done. Loss: 0.0683  lr:0.000001
[ Wed Jul 10 20:40:46 2024 ] 	Batch(1700/7879) done. Loss: 0.0425  lr:0.000001
[ Wed Jul 10 20:41:10 2024 ] 	Batch(1800/7879) done. Loss: 0.0588  lr:0.000001
[ Wed Jul 10 20:41:34 2024 ] 	Batch(1900/7879) done. Loss: 0.0253  lr:0.000001
[ Wed Jul 10 20:41:57 2024 ] 
Training: Epoch [1/120], Step [1999], Loss: 0.05445971339941025, Training Accuracy: 96.15625
[ Wed Jul 10 20:41:57 2024 ] 	Batch(2000/7879) done. Loss: 0.6142  lr:0.000001
[ Wed Jul 10 20:42:20 2024 ] 	Batch(2100/7879) done. Loss: 0.0642  lr:0.000001
[ Wed Jul 10 20:42:43 2024 ] 	Batch(2200/7879) done. Loss: 0.2330  lr:0.000001
[ Wed Jul 10 20:43:06 2024 ] 	Batch(2300/7879) done. Loss: 0.1875  lr:0.000001
[ Wed Jul 10 20:43:29 2024 ] 	Batch(2400/7879) done. Loss: 0.1658  lr:0.000001
[ Wed Jul 10 20:43:52 2024 ] 
Training: Epoch [1/120], Step [2499], Loss: 0.0632265955209732, Training Accuracy: 96.135
[ Wed Jul 10 20:43:52 2024 ] 	Batch(2500/7879) done. Loss: 0.4430  lr:0.000001
[ Wed Jul 10 20:44:16 2024 ] 	Batch(2600/7879) done. Loss: 0.6074  lr:0.000001
[ Wed Jul 10 20:44:39 2024 ] 	Batch(2700/7879) done. Loss: 0.0213  lr:0.000001
[ Wed Jul 10 20:45:02 2024 ] 	Batch(2800/7879) done. Loss: 0.0878  lr:0.000001
[ Wed Jul 10 20:45:25 2024 ] 	Batch(2900/7879) done. Loss: 0.0346  lr:0.000001
[ Wed Jul 10 20:45:48 2024 ] 
Training: Epoch [1/120], Step [2999], Loss: 0.11509852111339569, Training Accuracy: 96.08333333333333
[ Wed Jul 10 20:45:48 2024 ] 	Batch(3000/7879) done. Loss: 0.3915  lr:0.000001
[ Wed Jul 10 20:46:11 2024 ] 	Batch(3100/7879) done. Loss: 0.0558  lr:0.000001
[ Wed Jul 10 20:46:34 2024 ] 	Batch(3200/7879) done. Loss: 0.0380  lr:0.000001
[ Wed Jul 10 20:46:58 2024 ] 	Batch(3300/7879) done. Loss: 0.1322  lr:0.000001
[ Wed Jul 10 20:47:22 2024 ] 	Batch(3400/7879) done. Loss: 0.0809  lr:0.000001
[ Wed Jul 10 20:47:45 2024 ] 
Training: Epoch [1/120], Step [3499], Loss: 0.043438348919153214, Training Accuracy: 96.09285714285714
[ Wed Jul 10 20:47:45 2024 ] 	Batch(3500/7879) done. Loss: 0.0140  lr:0.000001
[ Wed Jul 10 20:48:09 2024 ] 	Batch(3600/7879) done. Loss: 0.3493  lr:0.000001
[ Wed Jul 10 20:48:32 2024 ] 	Batch(3700/7879) done. Loss: 0.1137  lr:0.000001
[ Wed Jul 10 20:48:55 2024 ] 	Batch(3800/7879) done. Loss: 0.2893  lr:0.000001
[ Wed Jul 10 20:49:18 2024 ] 	Batch(3900/7879) done. Loss: 0.0277  lr:0.000001
[ Wed Jul 10 20:49:41 2024 ] 
Training: Epoch [1/120], Step [3999], Loss: 0.15103162825107574, Training Accuracy: 96.221875
[ Wed Jul 10 20:49:41 2024 ] 	Batch(4000/7879) done. Loss: 0.1096  lr:0.000001
[ Wed Jul 10 20:50:04 2024 ] 	Batch(4100/7879) done. Loss: 0.1365  lr:0.000001
[ Wed Jul 10 20:50:27 2024 ] 	Batch(4200/7879) done. Loss: 0.0790  lr:0.000001
[ Wed Jul 10 20:50:49 2024 ] 	Batch(4300/7879) done. Loss: 0.2597  lr:0.000001
[ Wed Jul 10 20:51:12 2024 ] 	Batch(4400/7879) done. Loss: 0.6335  lr:0.000001
[ Wed Jul 10 20:51:35 2024 ] 
Training: Epoch [1/120], Step [4499], Loss: 0.03684302791953087, Training Accuracy: 96.19722222222222
[ Wed Jul 10 20:51:36 2024 ] 	Batch(4500/7879) done. Loss: 0.1152  lr:0.000001
[ Wed Jul 10 20:51:59 2024 ] 	Batch(4600/7879) done. Loss: 0.1528  lr:0.000001
[ Wed Jul 10 20:52:21 2024 ] 	Batch(4700/7879) done. Loss: 0.1398  lr:0.000001
[ Wed Jul 10 20:52:44 2024 ] 	Batch(4800/7879) done. Loss: 0.0447  lr:0.000001
[ Wed Jul 10 20:53:07 2024 ] 	Batch(4900/7879) done. Loss: 0.0976  lr:0.000001
[ Wed Jul 10 20:53:29 2024 ] 
Training: Epoch [1/120], Step [4999], Loss: 0.1886776089668274, Training Accuracy: 96.2
[ Wed Jul 10 20:53:29 2024 ] 	Batch(5000/7879) done. Loss: 0.4023  lr:0.000001
[ Wed Jul 10 20:53:52 2024 ] 	Batch(5100/7879) done. Loss: 0.1814  lr:0.000001
[ Wed Jul 10 20:54:14 2024 ] 	Batch(5200/7879) done. Loss: 0.0401  lr:0.000001
[ Wed Jul 10 20:54:37 2024 ] 	Batch(5300/7879) done. Loss: 0.0068  lr:0.000001
[ Wed Jul 10 20:55:00 2024 ] 	Batch(5400/7879) done. Loss: 0.0251  lr:0.000001
[ Wed Jul 10 20:55:22 2024 ] 
Training: Epoch [1/120], Step [5499], Loss: 0.07338376343250275, Training Accuracy: 96.20227272727273
[ Wed Jul 10 20:55:22 2024 ] 	Batch(5500/7879) done. Loss: 0.0290  lr:0.000001
[ Wed Jul 10 20:55:45 2024 ] 	Batch(5600/7879) done. Loss: 0.0174  lr:0.000001
[ Wed Jul 10 20:56:08 2024 ] 	Batch(5700/7879) done. Loss: 0.1294  lr:0.000001
[ Wed Jul 10 20:56:30 2024 ] 	Batch(5800/7879) done. Loss: 0.0276  lr:0.000001
[ Wed Jul 10 20:56:53 2024 ] 	Batch(5900/7879) done. Loss: 0.0455  lr:0.000001
[ Wed Jul 10 20:57:16 2024 ] 
Training: Epoch [1/120], Step [5999], Loss: 0.05737984552979469, Training Accuracy: 96.18541666666667
[ Wed Jul 10 20:57:16 2024 ] 	Batch(6000/7879) done. Loss: 0.0196  lr:0.000001
[ Wed Jul 10 20:57:39 2024 ] 	Batch(6100/7879) done. Loss: 0.0266  lr:0.000001
[ Wed Jul 10 20:58:02 2024 ] 	Batch(6200/7879) done. Loss: 0.3688  lr:0.000001
[ Wed Jul 10 20:58:26 2024 ] 	Batch(6300/7879) done. Loss: 0.0309  lr:0.000001
[ Wed Jul 10 20:58:49 2024 ] 	Batch(6400/7879) done. Loss: 0.3252  lr:0.000001
[ Wed Jul 10 20:59:12 2024 ] 
Training: Epoch [1/120], Step [6499], Loss: 0.24401672184467316, Training Accuracy: 96.1826923076923
[ Wed Jul 10 20:59:12 2024 ] 	Batch(6500/7879) done. Loss: 0.8281  lr:0.000001
[ Wed Jul 10 20:59:35 2024 ] 	Batch(6600/7879) done. Loss: 0.1346  lr:0.000001
[ Wed Jul 10 20:59:58 2024 ] 	Batch(6700/7879) done. Loss: 0.3468  lr:0.000001
[ Wed Jul 10 21:00:21 2024 ] 	Batch(6800/7879) done. Loss: 0.0195  lr:0.000001
[ Wed Jul 10 21:00:44 2024 ] 	Batch(6900/7879) done. Loss: 0.1633  lr:0.000001
[ Wed Jul 10 21:01:07 2024 ] 
Training: Epoch [1/120], Step [6999], Loss: 0.1351122260093689, Training Accuracy: 96.20357142857144
[ Wed Jul 10 21:01:07 2024 ] 	Batch(7000/7879) done. Loss: 0.0678  lr:0.000001
[ Wed Jul 10 21:01:30 2024 ] 	Batch(7100/7879) done. Loss: 0.1028  lr:0.000001
[ Wed Jul 10 21:01:53 2024 ] 	Batch(7200/7879) done. Loss: 0.1101  lr:0.000001
[ Wed Jul 10 21:02:16 2024 ] 	Batch(7300/7879) done. Loss: 0.2574  lr:0.000001
[ Wed Jul 10 21:02:39 2024 ] 	Batch(7400/7879) done. Loss: 0.0691  lr:0.000001
[ Wed Jul 10 21:03:02 2024 ] 
Training: Epoch [1/120], Step [7499], Loss: 0.024661514908075333, Training Accuracy: 96.19166666666666
[ Wed Jul 10 21:03:02 2024 ] 	Batch(7500/7879) done. Loss: 0.0294  lr:0.000001
[ Wed Jul 10 21:03:25 2024 ] 	Batch(7600/7879) done. Loss: 0.0097  lr:0.000001
[ Wed Jul 10 21:03:47 2024 ] 	Batch(7700/7879) done. Loss: 0.3913  lr:0.000001
[ Wed Jul 10 21:04:10 2024 ] 	Batch(7800/7879) done. Loss: 0.1051  lr:0.000001
[ Wed Jul 10 21:04:28 2024 ] 	Mean training loss: 0.1440.
[ Wed Jul 10 21:04:28 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Wed Jul 10 21:04:28 2024 ] Training epoch: 3
[ Wed Jul 10 21:04:29 2024 ] 	Batch(0/7879) done. Loss: 0.0216  lr:0.000001
[ Wed Jul 10 21:04:51 2024 ] 	Batch(100/7879) done. Loss: 0.0862  lr:0.000001
[ Wed Jul 10 21:05:14 2024 ] 	Batch(200/7879) done. Loss: 0.1716  lr:0.000001
[ Wed Jul 10 21:05:37 2024 ] 	Batch(300/7879) done. Loss: 0.2523  lr:0.000001
[ Wed Jul 10 21:06:01 2024 ] 	Batch(400/7879) done. Loss: 0.2875  lr:0.000001
[ Wed Jul 10 21:06:24 2024 ] 
Training: Epoch [2/120], Step [499], Loss: 0.0023781796917319298, Training Accuracy: 96.175
[ Wed Jul 10 21:06:24 2024 ] 	Batch(500/7879) done. Loss: 0.0686  lr:0.000001
[ Wed Jul 10 21:06:47 2024 ] 	Batch(600/7879) done. Loss: 0.0182  lr:0.000001
[ Wed Jul 10 21:07:10 2024 ] 	Batch(700/7879) done. Loss: 0.0158  lr:0.000001
[ Wed Jul 10 21:07:33 2024 ] 	Batch(800/7879) done. Loss: 0.1135  lr:0.000001
[ Wed Jul 10 21:07:56 2024 ] 	Batch(900/7879) done. Loss: 0.1260  lr:0.000001
[ Wed Jul 10 21:08:19 2024 ] 
Training: Epoch [2/120], Step [999], Loss: 0.06487181037664413, Training Accuracy: 96.2125
[ Wed Jul 10 21:08:19 2024 ] 	Batch(1000/7879) done. Loss: 0.1894  lr:0.000001
[ Wed Jul 10 21:08:42 2024 ] 	Batch(1100/7879) done. Loss: 0.1282  lr:0.000001
[ Wed Jul 10 21:09:05 2024 ] 	Batch(1200/7879) done. Loss: 0.0286  lr:0.000001
[ Wed Jul 10 21:09:27 2024 ] 	Batch(1300/7879) done. Loss: 0.0846  lr:0.000001
[ Wed Jul 10 21:09:50 2024 ] 	Batch(1400/7879) done. Loss: 0.0344  lr:0.000001
[ Wed Jul 10 21:10:12 2024 ] 
Training: Epoch [2/120], Step [1499], Loss: 0.48128852248191833, Training Accuracy: 96.14166666666667
[ Wed Jul 10 21:10:13 2024 ] 	Batch(1500/7879) done. Loss: 0.1443  lr:0.000001
[ Wed Jul 10 21:10:35 2024 ] 	Batch(1600/7879) done. Loss: 0.0263  lr:0.000001
[ Wed Jul 10 21:10:59 2024 ] 	Batch(1700/7879) done. Loss: 0.0185  lr:0.000001
[ Wed Jul 10 21:11:22 2024 ] 	Batch(1800/7879) done. Loss: 0.2551  lr:0.000001
[ Wed Jul 10 21:11:45 2024 ] 	Batch(1900/7879) done. Loss: 0.0380  lr:0.000001
[ Wed Jul 10 21:12:07 2024 ] 
Training: Epoch [2/120], Step [1999], Loss: 0.047492556273937225, Training Accuracy: 96.15625
[ Wed Jul 10 21:12:08 2024 ] 	Batch(2000/7879) done. Loss: 0.3185  lr:0.000001
[ Wed Jul 10 21:12:30 2024 ] 	Batch(2100/7879) done. Loss: 0.0183  lr:0.000001
[ Wed Jul 10 21:12:53 2024 ] 	Batch(2200/7879) done. Loss: 0.1500  lr:0.000001
[ Wed Jul 10 21:13:15 2024 ] 	Batch(2300/7879) done. Loss: 0.1202  lr:0.000001
[ Wed Jul 10 21:13:38 2024 ] 	Batch(2400/7879) done. Loss: 0.5876  lr:0.000001
[ Wed Jul 10 21:14:00 2024 ] 
Training: Epoch [2/120], Step [2499], Loss: 0.007628115825355053, Training Accuracy: 96.125
[ Wed Jul 10 21:14:01 2024 ] 	Batch(2500/7879) done. Loss: 0.0137  lr:0.000001
[ Wed Jul 10 21:14:23 2024 ] 	Batch(2600/7879) done. Loss: 0.0782  lr:0.000001
[ Wed Jul 10 21:14:46 2024 ] 	Batch(2700/7879) done. Loss: 0.0775  lr:0.000001
[ Wed Jul 10 21:15:08 2024 ] 	Batch(2800/7879) done. Loss: 0.1149  lr:0.000001
[ Wed Jul 10 21:15:31 2024 ] 	Batch(2900/7879) done. Loss: 0.2838  lr:0.000001
[ Wed Jul 10 21:15:53 2024 ] 
Training: Epoch [2/120], Step [2999], Loss: 0.04342273622751236, Training Accuracy: 96.1125
[ Wed Jul 10 21:15:54 2024 ] 	Batch(3000/7879) done. Loss: 0.1112  lr:0.000001
[ Wed Jul 10 21:16:16 2024 ] 	Batch(3100/7879) done. Loss: 0.0142  lr:0.000001
[ Wed Jul 10 21:16:39 2024 ] 	Batch(3200/7879) done. Loss: 0.4696  lr:0.000001
[ Wed Jul 10 21:17:01 2024 ] 	Batch(3300/7879) done. Loss: 0.1102  lr:0.000001
[ Wed Jul 10 21:17:24 2024 ] 	Batch(3400/7879) done. Loss: 0.1221  lr:0.000001
[ Wed Jul 10 21:17:46 2024 ] 
Training: Epoch [2/120], Step [3499], Loss: 0.12095083296298981, Training Accuracy: 96.09285714285714
[ Wed Jul 10 21:17:47 2024 ] 	Batch(3500/7879) done. Loss: 0.0786  lr:0.000001
[ Wed Jul 10 21:18:09 2024 ] 	Batch(3600/7879) done. Loss: 0.0440  lr:0.000001
[ Wed Jul 10 21:18:32 2024 ] 	Batch(3700/7879) done. Loss: 0.0253  lr:0.000001
[ Wed Jul 10 21:18:54 2024 ] 	Batch(3800/7879) done. Loss: 0.0522  lr:0.000001
[ Wed Jul 10 21:19:17 2024 ] 	Batch(3900/7879) done. Loss: 0.0664  lr:0.000001
[ Wed Jul 10 21:19:39 2024 ] 
Training: Epoch [2/120], Step [3999], Loss: 0.01834353432059288, Training Accuracy: 96.059375
[ Wed Jul 10 21:19:40 2024 ] 	Batch(4000/7879) done. Loss: 0.1288  lr:0.000001
[ Wed Jul 10 21:20:02 2024 ] 	Batch(4100/7879) done. Loss: 0.0575  lr:0.000001
[ Wed Jul 10 21:20:25 2024 ] 	Batch(4200/7879) done. Loss: 0.0168  lr:0.000001
[ Wed Jul 10 21:20:48 2024 ] 	Batch(4300/7879) done. Loss: 0.0704  lr:0.000001
[ Wed Jul 10 21:21:11 2024 ] 	Batch(4400/7879) done. Loss: 0.0060  lr:0.000001
[ Wed Jul 10 21:21:35 2024 ] 
Training: Epoch [2/120], Step [4499], Loss: 0.014217214658856392, Training Accuracy: 96.08611111111112
[ Wed Jul 10 21:21:35 2024 ] 	Batch(4500/7879) done. Loss: 0.0997  lr:0.000001
[ Wed Jul 10 21:21:58 2024 ] 	Batch(4600/7879) done. Loss: 0.1115  lr:0.000001
[ Wed Jul 10 21:22:21 2024 ] 	Batch(4700/7879) done. Loss: 0.0344  lr:0.000001
[ Wed Jul 10 21:22:44 2024 ] 	Batch(4800/7879) done. Loss: 0.1266  lr:0.000001
[ Wed Jul 10 21:23:07 2024 ] 	Batch(4900/7879) done. Loss: 0.3594  lr:0.000001
[ Wed Jul 10 21:23:30 2024 ] 
Training: Epoch [2/120], Step [4999], Loss: 0.5255924463272095, Training Accuracy: 96.0875
[ Wed Jul 10 21:23:30 2024 ] 	Batch(5000/7879) done. Loss: 0.1159  lr:0.000001
[ Wed Jul 10 21:23:53 2024 ] 	Batch(5100/7879) done. Loss: 0.0116  lr:0.000001
[ Wed Jul 10 21:24:16 2024 ] 	Batch(5200/7879) done. Loss: 0.0568  lr:0.000001
[ Wed Jul 10 21:24:39 2024 ] 	Batch(5300/7879) done. Loss: 0.0295  lr:0.000001
[ Wed Jul 10 21:25:02 2024 ] 	Batch(5400/7879) done. Loss: 0.1806  lr:0.000001
[ Wed Jul 10 21:25:26 2024 ] 
Training: Epoch [2/120], Step [5499], Loss: 0.00565315643325448, Training Accuracy: 96.11136363636363
[ Wed Jul 10 21:25:26 2024 ] 	Batch(5500/7879) done. Loss: 0.0125  lr:0.000001
[ Wed Jul 10 21:25:50 2024 ] 	Batch(5600/7879) done. Loss: 0.0383  lr:0.000001
[ Wed Jul 10 21:26:14 2024 ] 	Batch(5700/7879) done. Loss: 0.0138  lr:0.000001
[ Wed Jul 10 21:26:37 2024 ] 	Batch(5800/7879) done. Loss: 0.7437  lr:0.000001
[ Wed Jul 10 21:27:01 2024 ] 	Batch(5900/7879) done. Loss: 0.1602  lr:0.000001
[ Wed Jul 10 21:27:24 2024 ] 
Training: Epoch [2/120], Step [5999], Loss: 0.012364987283945084, Training Accuracy: 96.13333333333334
[ Wed Jul 10 21:27:24 2024 ] 	Batch(6000/7879) done. Loss: 0.0213  lr:0.000001
[ Wed Jul 10 21:27:47 2024 ] 	Batch(6100/7879) done. Loss: 0.0765  lr:0.000001
[ Wed Jul 10 21:28:10 2024 ] 	Batch(6200/7879) done. Loss: 0.0329  lr:0.000001
[ Wed Jul 10 21:28:33 2024 ] 	Batch(6300/7879) done. Loss: 0.0888  lr:0.000001
[ Wed Jul 10 21:28:57 2024 ] 	Batch(6400/7879) done. Loss: 0.2281  lr:0.000001
[ Wed Jul 10 21:29:20 2024 ] 
Training: Epoch [2/120], Step [6499], Loss: 0.27423524856567383, Training Accuracy: 96.175
[ Wed Jul 10 21:29:21 2024 ] 	Batch(6500/7879) done. Loss: 0.1951  lr:0.000001
[ Wed Jul 10 21:29:44 2024 ] 	Batch(6600/7879) done. Loss: 0.1455  lr:0.000001
[ Wed Jul 10 21:30:07 2024 ] 	Batch(6700/7879) done. Loss: 0.0398  lr:0.000001
[ Wed Jul 10 21:30:30 2024 ] 	Batch(6800/7879) done. Loss: 0.4343  lr:0.000001
[ Wed Jul 10 21:30:54 2024 ] 	Batch(6900/7879) done. Loss: 0.0829  lr:0.000001
[ Wed Jul 10 21:31:18 2024 ] 
Training: Epoch [2/120], Step [6999], Loss: 0.28981858491897583, Training Accuracy: 96.16607142857143
[ Wed Jul 10 21:31:18 2024 ] 	Batch(7000/7879) done. Loss: 0.0390  lr:0.000001
[ Wed Jul 10 21:31:42 2024 ] 	Batch(7100/7879) done. Loss: 0.0075  lr:0.000001
[ Wed Jul 10 21:32:05 2024 ] 	Batch(7200/7879) done. Loss: 0.0170  lr:0.000001
[ Wed Jul 10 21:32:29 2024 ] 	Batch(7300/7879) done. Loss: 0.0327  lr:0.000001
[ Wed Jul 10 21:32:52 2024 ] 	Batch(7400/7879) done. Loss: 0.0706  lr:0.000001
[ Wed Jul 10 21:33:15 2024 ] 
Training: Epoch [2/120], Step [7499], Loss: 0.11010248214006424, Training Accuracy: 96.20666666666666
[ Wed Jul 10 21:33:16 2024 ] 	Batch(7500/7879) done. Loss: 0.1361  lr:0.000001
[ Wed Jul 10 21:33:39 2024 ] 	Batch(7600/7879) done. Loss: 0.0632  lr:0.000001
[ Wed Jul 10 21:34:02 2024 ] 	Batch(7700/7879) done. Loss: 0.0612  lr:0.000001
[ Wed Jul 10 21:34:25 2024 ] 	Batch(7800/7879) done. Loss: 0.0565  lr:0.000001
[ Wed Jul 10 21:34:43 2024 ] 	Mean training loss: 0.1421.
[ Wed Jul 10 21:34:43 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Wed Jul 10 21:34:44 2024 ] Training epoch: 4
[ Wed Jul 10 21:34:44 2024 ] 	Batch(0/7879) done. Loss: 0.0069  lr:0.000001
[ Wed Jul 10 21:35:08 2024 ] 	Batch(100/7879) done. Loss: 0.0359  lr:0.000001
[ Wed Jul 10 21:35:31 2024 ] 	Batch(200/7879) done. Loss: 0.2859  lr:0.000001
[ Wed Jul 10 21:35:55 2024 ] 	Batch(300/7879) done. Loss: 0.0176  lr:0.000001
[ Wed Jul 10 21:36:19 2024 ] 	Batch(400/7879) done. Loss: 0.1836  lr:0.000001
[ Wed Jul 10 21:36:42 2024 ] 
Training: Epoch [3/120], Step [499], Loss: 0.06924702972173691, Training Accuracy: 95.975
[ Wed Jul 10 21:36:42 2024 ] 	Batch(500/7879) done. Loss: 0.0789  lr:0.000001
[ Wed Jul 10 21:37:05 2024 ] 	Batch(600/7879) done. Loss: 0.0459  lr:0.000001
[ Wed Jul 10 21:37:28 2024 ] 	Batch(700/7879) done. Loss: 0.0726  lr:0.000001
[ Wed Jul 10 21:37:52 2024 ] 	Batch(800/7879) done. Loss: 0.0274  lr:0.000001
[ Wed Jul 10 21:38:16 2024 ] 	Batch(900/7879) done. Loss: 0.0770  lr:0.000001
[ Wed Jul 10 21:38:39 2024 ] 
Training: Epoch [3/120], Step [999], Loss: 0.01639077439904213, Training Accuracy: 95.9125
[ Wed Jul 10 21:38:39 2024 ] 	Batch(1000/7879) done. Loss: 0.0210  lr:0.000001
[ Wed Jul 10 21:39:02 2024 ] 	Batch(1100/7879) done. Loss: 0.0145  lr:0.000001
[ Wed Jul 10 21:39:25 2024 ] 	Batch(1200/7879) done. Loss: 0.0129  lr:0.000001
[ Wed Jul 10 21:39:49 2024 ] 	Batch(1300/7879) done. Loss: 0.0322  lr:0.000001
[ Wed Jul 10 21:40:12 2024 ] 	Batch(1400/7879) done. Loss: 0.0548  lr:0.000001
[ Wed Jul 10 21:40:34 2024 ] 
Training: Epoch [3/120], Step [1499], Loss: 0.08357212692499161, Training Accuracy: 96.18333333333334
[ Wed Jul 10 21:40:35 2024 ] 	Batch(1500/7879) done. Loss: 0.0999  lr:0.000001
[ Wed Jul 10 21:40:58 2024 ] 	Batch(1600/7879) done. Loss: 0.0332  lr:0.000001
[ Wed Jul 10 21:41:21 2024 ] 	Batch(1700/7879) done. Loss: 0.2326  lr:0.000001
[ Wed Jul 10 21:41:44 2024 ] 	Batch(1800/7879) done. Loss: 0.0103  lr:0.000001
[ Wed Jul 10 21:42:07 2024 ] 	Batch(1900/7879) done. Loss: 0.0966  lr:0.000001
[ Wed Jul 10 21:42:29 2024 ] 
Training: Epoch [3/120], Step [1999], Loss: 0.08193347603082657, Training Accuracy: 96.15
[ Wed Jul 10 21:42:29 2024 ] 	Batch(2000/7879) done. Loss: 0.0084  lr:0.000001
[ Wed Jul 10 21:42:52 2024 ] 	Batch(2100/7879) done. Loss: 0.0669  lr:0.000001
[ Wed Jul 10 21:43:15 2024 ] 	Batch(2200/7879) done. Loss: 0.0133  lr:0.000001
[ Wed Jul 10 21:43:38 2024 ] 	Batch(2300/7879) done. Loss: 0.0054  lr:0.000001
[ Wed Jul 10 21:44:00 2024 ] 	Batch(2400/7879) done. Loss: 0.0744  lr:0.000001
[ Wed Jul 10 21:44:23 2024 ] 
Training: Epoch [3/120], Step [2499], Loss: 0.045791324228048325, Training Accuracy: 96.205
[ Wed Jul 10 21:44:23 2024 ] 	Batch(2500/7879) done. Loss: 0.1473  lr:0.000001
[ Wed Jul 10 21:44:46 2024 ] 	Batch(2600/7879) done. Loss: 0.1313  lr:0.000001
[ Wed Jul 10 21:45:09 2024 ] 	Batch(2700/7879) done. Loss: 0.0103  lr:0.000001
[ Wed Jul 10 21:45:32 2024 ] 	Batch(2800/7879) done. Loss: 0.1655  lr:0.000001
[ Wed Jul 10 21:45:55 2024 ] 	Batch(2900/7879) done. Loss: 0.2420  lr:0.000001
[ Wed Jul 10 21:46:17 2024 ] 
Training: Epoch [3/120], Step [2999], Loss: 0.20504210889339447, Training Accuracy: 96.25833333333334
[ Wed Jul 10 21:46:18 2024 ] 	Batch(3000/7879) done. Loss: 0.4800  lr:0.000001
[ Wed Jul 10 21:46:40 2024 ] 	Batch(3100/7879) done. Loss: 0.0242  lr:0.000001
[ Wed Jul 10 21:47:03 2024 ] 	Batch(3200/7879) done. Loss: 0.0849  lr:0.000001
[ Wed Jul 10 21:47:26 2024 ] 	Batch(3300/7879) done. Loss: 0.3305  lr:0.000001
[ Wed Jul 10 21:47:49 2024 ] 	Batch(3400/7879) done. Loss: 0.0136  lr:0.000001
[ Wed Jul 10 21:48:11 2024 ] 
Training: Epoch [3/120], Step [3499], Loss: 0.09642782807350159, Training Accuracy: 96.31071428571428
[ Wed Jul 10 21:48:11 2024 ] 	Batch(3500/7879) done. Loss: 0.1257  lr:0.000001
[ Wed Jul 10 21:48:34 2024 ] 	Batch(3600/7879) done. Loss: 0.0164  lr:0.000001
[ Wed Jul 10 21:48:57 2024 ] 	Batch(3700/7879) done. Loss: 0.1725  lr:0.000001
[ Wed Jul 10 21:49:20 2024 ] 	Batch(3800/7879) done. Loss: 0.0173  lr:0.000001
[ Wed Jul 10 21:49:42 2024 ] 	Batch(3900/7879) done. Loss: 0.0991  lr:0.000001
[ Wed Jul 10 21:50:05 2024 ] 
Training: Epoch [3/120], Step [3999], Loss: 0.07307441532611847, Training Accuracy: 96.3125
[ Wed Jul 10 21:50:05 2024 ] 	Batch(4000/7879) done. Loss: 0.0607  lr:0.000001
[ Wed Jul 10 21:50:28 2024 ] 	Batch(4100/7879) done. Loss: 0.0615  lr:0.000001
[ Wed Jul 10 21:50:51 2024 ] 	Batch(4200/7879) done. Loss: 0.1044  lr:0.000001
[ Wed Jul 10 21:51:14 2024 ] 	Batch(4300/7879) done. Loss: 0.0140  lr:0.000001
[ Wed Jul 10 21:51:38 2024 ] 	Batch(4400/7879) done. Loss: 0.2611  lr:0.000001
[ Wed Jul 10 21:52:01 2024 ] 
Training: Epoch [3/120], Step [4499], Loss: 0.10823061317205429, Training Accuracy: 96.3638888888889
[ Wed Jul 10 21:52:01 2024 ] 	Batch(4500/7879) done. Loss: 0.3077  lr:0.000001
[ Wed Jul 10 21:52:25 2024 ] 	Batch(4600/7879) done. Loss: 0.0150  lr:0.000001
[ Wed Jul 10 21:52:48 2024 ] 	Batch(4700/7879) done. Loss: 0.1904  lr:0.000001
[ Wed Jul 10 21:53:11 2024 ] 	Batch(4800/7879) done. Loss: 0.0324  lr:0.000001
[ Wed Jul 10 21:53:34 2024 ] 	Batch(4900/7879) done. Loss: 0.1499  lr:0.000001
[ Wed Jul 10 21:53:57 2024 ] 
Training: Epoch [3/120], Step [4999], Loss: 0.007419951260089874, Training Accuracy: 96.34
[ Wed Jul 10 21:53:57 2024 ] 	Batch(5000/7879) done. Loss: 0.0302  lr:0.000001
[ Wed Jul 10 21:54:20 2024 ] 	Batch(5100/7879) done. Loss: 0.4748  lr:0.000001
[ Wed Jul 10 21:54:43 2024 ] 	Batch(5200/7879) done. Loss: 0.0036  lr:0.000001
[ Wed Jul 10 21:55:06 2024 ] 	Batch(5300/7879) done. Loss: 0.0274  lr:0.000001
[ Wed Jul 10 21:55:29 2024 ] 	Batch(5400/7879) done. Loss: 0.0128  lr:0.000001
[ Wed Jul 10 21:55:52 2024 ] 
Training: Epoch [3/120], Step [5499], Loss: 0.20792503654956818, Training Accuracy: 96.27272727272728
[ Wed Jul 10 21:55:52 2024 ] 	Batch(5500/7879) done. Loss: 0.0189  lr:0.000001
[ Wed Jul 10 21:56:15 2024 ] 	Batch(5600/7879) done. Loss: 0.1454  lr:0.000001
[ Wed Jul 10 21:56:38 2024 ] 	Batch(5700/7879) done. Loss: 0.1140  lr:0.000001
[ Wed Jul 10 21:57:01 2024 ] 	Batch(5800/7879) done. Loss: 0.0801  lr:0.000001
[ Wed Jul 10 21:57:24 2024 ] 	Batch(5900/7879) done. Loss: 0.2811  lr:0.000001
[ Wed Jul 10 21:57:47 2024 ] 
Training: Epoch [3/120], Step [5999], Loss: 0.03363987058401108, Training Accuracy: 96.23333333333333
[ Wed Jul 10 21:57:48 2024 ] 	Batch(6000/7879) done. Loss: 0.0639  lr:0.000001
[ Wed Jul 10 21:58:11 2024 ] 	Batch(6100/7879) done. Loss: 0.0198  lr:0.000001
[ Wed Jul 10 21:58:34 2024 ] 	Batch(6200/7879) done. Loss: 0.0311  lr:0.000001
[ Wed Jul 10 21:58:57 2024 ] 	Batch(6300/7879) done. Loss: 0.0374  lr:0.000001
[ Wed Jul 10 21:59:20 2024 ] 	Batch(6400/7879) done. Loss: 0.0844  lr:0.000001
[ Wed Jul 10 21:59:43 2024 ] 
Training: Epoch [3/120], Step [6499], Loss: 0.36294907331466675, Training Accuracy: 96.2326923076923
[ Wed Jul 10 21:59:43 2024 ] 	Batch(6500/7879) done. Loss: 0.3538  lr:0.000001
[ Wed Jul 10 22:00:06 2024 ] 	Batch(6600/7879) done. Loss: 0.0580  lr:0.000001
[ Wed Jul 10 22:00:29 2024 ] 	Batch(6700/7879) done. Loss: 0.0658  lr:0.000001
[ Wed Jul 10 22:00:52 2024 ] 	Batch(6800/7879) done. Loss: 0.1250  lr:0.000001
[ Wed Jul 10 22:01:15 2024 ] 	Batch(6900/7879) done. Loss: 0.1450  lr:0.000001
[ Wed Jul 10 22:01:38 2024 ] 
Training: Epoch [3/120], Step [6999], Loss: 0.13737118244171143, Training Accuracy: 96.2357142857143
[ Wed Jul 10 22:01:38 2024 ] 	Batch(7000/7879) done. Loss: 0.1166  lr:0.000001
[ Wed Jul 10 22:02:01 2024 ] 	Batch(7100/7879) done. Loss: 0.2252  lr:0.000001
[ Wed Jul 10 22:02:24 2024 ] 	Batch(7200/7879) done. Loss: 0.0257  lr:0.000001
[ Wed Jul 10 22:02:47 2024 ] 	Batch(7300/7879) done. Loss: 0.3659  lr:0.000001
[ Wed Jul 10 22:03:10 2024 ] 	Batch(7400/7879) done. Loss: 0.4635  lr:0.000001
[ Wed Jul 10 22:03:33 2024 ] 
Training: Epoch [3/120], Step [7499], Loss: 0.012751178815960884, Training Accuracy: 96.22166666666668
[ Wed Jul 10 22:03:33 2024 ] 	Batch(7500/7879) done. Loss: 0.2786  lr:0.000001
[ Wed Jul 10 22:03:56 2024 ] 	Batch(7600/7879) done. Loss: 0.0047  lr:0.000001
[ Wed Jul 10 22:04:20 2024 ] 	Batch(7700/7879) done. Loss: 0.1428  lr:0.000001
[ Wed Jul 10 22:04:43 2024 ] 	Batch(7800/7879) done. Loss: 0.1674  lr:0.000001
[ Wed Jul 10 22:05:01 2024 ] 	Mean training loss: 0.1423.
[ Wed Jul 10 22:05:01 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Wed Jul 10 22:05:01 2024 ] Training epoch: 5
[ Wed Jul 10 22:05:02 2024 ] 	Batch(0/7879) done. Loss: 0.0046  lr:0.000001
[ Wed Jul 10 22:05:25 2024 ] 	Batch(100/7879) done. Loss: 0.0694  lr:0.000001
[ Wed Jul 10 22:05:48 2024 ] 	Batch(200/7879) done. Loss: 0.0031  lr:0.000001
[ Wed Jul 10 22:06:11 2024 ] 	Batch(300/7879) done. Loss: 0.0560  lr:0.000001
[ Wed Jul 10 22:06:34 2024 ] 	Batch(400/7879) done. Loss: 0.1693  lr:0.000001
[ Wed Jul 10 22:06:56 2024 ] 
Training: Epoch [4/120], Step [499], Loss: 0.24986866116523743, Training Accuracy: 96.625
[ Wed Jul 10 22:06:56 2024 ] 	Batch(500/7879) done. Loss: 0.2350  lr:0.000001
[ Wed Jul 10 22:07:19 2024 ] 	Batch(600/7879) done. Loss: 0.1103  lr:0.000001
[ Wed Jul 10 22:07:42 2024 ] 	Batch(700/7879) done. Loss: 0.0496  lr:0.000001
[ Wed Jul 10 22:08:05 2024 ] 	Batch(800/7879) done. Loss: 0.0556  lr:0.000001
[ Wed Jul 10 22:08:27 2024 ] 	Batch(900/7879) done. Loss: 0.0268  lr:0.000001
[ Wed Jul 10 22:08:50 2024 ] 
Training: Epoch [4/120], Step [999], Loss: 0.20032420754432678, Training Accuracy: 96.6125
[ Wed Jul 10 22:08:50 2024 ] 	Batch(1000/7879) done. Loss: 0.1988  lr:0.000001
[ Wed Jul 10 22:09:13 2024 ] 	Batch(1100/7879) done. Loss: 0.1779  lr:0.000001
[ Wed Jul 10 22:09:36 2024 ] 	Batch(1200/7879) done. Loss: 0.2622  lr:0.000001
[ Wed Jul 10 22:09:59 2024 ] 	Batch(1300/7879) done. Loss: 0.0894  lr:0.000001
[ Wed Jul 10 22:10:21 2024 ] 	Batch(1400/7879) done. Loss: 0.2297  lr:0.000001
[ Wed Jul 10 22:10:44 2024 ] 
Training: Epoch [4/120], Step [1499], Loss: 0.45092636346817017, Training Accuracy: 96.48333333333333
[ Wed Jul 10 22:10:44 2024 ] 	Batch(1500/7879) done. Loss: 0.0104  lr:0.000001
[ Wed Jul 10 22:11:07 2024 ] 	Batch(1600/7879) done. Loss: 0.0043  lr:0.000001
[ Wed Jul 10 22:11:30 2024 ] 	Batch(1700/7879) done. Loss: 0.0382  lr:0.000001
[ Wed Jul 10 22:11:52 2024 ] 	Batch(1800/7879) done. Loss: 0.1011  lr:0.000001
[ Wed Jul 10 22:12:15 2024 ] 	Batch(1900/7879) done. Loss: 0.2655  lr:0.000001
[ Wed Jul 10 22:12:38 2024 ] 
Training: Epoch [4/120], Step [1999], Loss: 0.0025863812770694494, Training Accuracy: 96.50625000000001
[ Wed Jul 10 22:12:38 2024 ] 	Batch(2000/7879) done. Loss: 0.2205  lr:0.000001
[ Wed Jul 10 22:13:01 2024 ] 	Batch(2100/7879) done. Loss: 0.2357  lr:0.000001
[ Wed Jul 10 22:13:23 2024 ] 	Batch(2200/7879) done. Loss: 0.0872  lr:0.000001
[ Wed Jul 10 22:13:46 2024 ] 	Batch(2300/7879) done. Loss: 0.2182  lr:0.000001
[ Wed Jul 10 22:14:09 2024 ] 	Batch(2400/7879) done. Loss: 0.1499  lr:0.000001
[ Wed Jul 10 22:14:32 2024 ] 
Training: Epoch [4/120], Step [2499], Loss: 0.054749879986047745, Training Accuracy: 96.405
[ Wed Jul 10 22:14:32 2024 ] 	Batch(2500/7879) done. Loss: 0.0750  lr:0.000001
[ Wed Jul 10 22:14:54 2024 ] 	Batch(2600/7879) done. Loss: 0.0560  lr:0.000001
[ Wed Jul 10 22:15:17 2024 ] 	Batch(2700/7879) done. Loss: 0.3242  lr:0.000001
[ Wed Jul 10 22:15:40 2024 ] 	Batch(2800/7879) done. Loss: 0.0101  lr:0.000001
[ Wed Jul 10 22:16:03 2024 ] 	Batch(2900/7879) done. Loss: 0.4163  lr:0.000001
[ Wed Jul 10 22:16:25 2024 ] 
Training: Epoch [4/120], Step [2999], Loss: 0.4124119281768799, Training Accuracy: 96.39583333333334
[ Wed Jul 10 22:16:25 2024 ] 	Batch(3000/7879) done. Loss: 0.0151  lr:0.000001
[ Wed Jul 10 22:16:48 2024 ] 	Batch(3100/7879) done. Loss: 0.0196  lr:0.000001
[ Wed Jul 10 22:17:11 2024 ] 	Batch(3200/7879) done. Loss: 0.0573  lr:0.000001
[ Wed Jul 10 22:17:34 2024 ] 	Batch(3300/7879) done. Loss: 0.1014  lr:0.000001
[ Wed Jul 10 22:17:56 2024 ] 	Batch(3400/7879) done. Loss: 0.3614  lr:0.000001
[ Wed Jul 10 22:18:19 2024 ] 
Training: Epoch [4/120], Step [3499], Loss: 0.16134262084960938, Training Accuracy: 96.41071428571428
[ Wed Jul 10 22:18:19 2024 ] 	Batch(3500/7879) done. Loss: 0.1607  lr:0.000001
[ Wed Jul 10 22:18:42 2024 ] 	Batch(3600/7879) done. Loss: 0.0108  lr:0.000001
[ Wed Jul 10 22:19:05 2024 ] 	Batch(3700/7879) done. Loss: 0.0766  lr:0.000001
[ Wed Jul 10 22:19:27 2024 ] 	Batch(3800/7879) done. Loss: 0.0035  lr:0.000001
[ Wed Jul 10 22:19:50 2024 ] 	Batch(3900/7879) done. Loss: 0.2650  lr:0.000001
[ Wed Jul 10 22:20:13 2024 ] 
Training: Epoch [4/120], Step [3999], Loss: 0.17032146453857422, Training Accuracy: 96.34375
[ Wed Jul 10 22:20:14 2024 ] 	Batch(4000/7879) done. Loss: 0.3593  lr:0.000001
[ Wed Jul 10 22:20:37 2024 ] 	Batch(4100/7879) done. Loss: 0.0778  lr:0.000001
[ Wed Jul 10 22:21:00 2024 ] 	Batch(4200/7879) done. Loss: 0.0384  lr:0.000001
[ Wed Jul 10 22:21:23 2024 ] 	Batch(4300/7879) done. Loss: 0.0378  lr:0.000001
[ Wed Jul 10 22:21:46 2024 ] 	Batch(4400/7879) done. Loss: 0.1568  lr:0.000001
[ Wed Jul 10 22:22:09 2024 ] 
Training: Epoch [4/120], Step [4499], Loss: 0.1833907663822174, Training Accuracy: 96.29166666666666
[ Wed Jul 10 22:22:10 2024 ] 	Batch(4500/7879) done. Loss: 0.1959  lr:0.000001
[ Wed Jul 10 22:22:33 2024 ] 	Batch(4600/7879) done. Loss: 0.3102  lr:0.000001
[ Wed Jul 10 22:22:56 2024 ] 	Batch(4700/7879) done. Loss: 0.0926  lr:0.000001
[ Wed Jul 10 22:23:19 2024 ] 	Batch(4800/7879) done. Loss: 0.0133  lr:0.000001
[ Wed Jul 10 22:23:42 2024 ] 	Batch(4900/7879) done. Loss: 0.0378  lr:0.000001
[ Wed Jul 10 22:24:05 2024 ] 
Training: Epoch [4/120], Step [4999], Loss: 0.0742303878068924, Training Accuracy: 96.3125
[ Wed Jul 10 22:24:05 2024 ] 	Batch(5000/7879) done. Loss: 0.1380  lr:0.000001
[ Wed Jul 10 22:24:28 2024 ] 	Batch(5100/7879) done. Loss: 0.0816  lr:0.000001
[ Wed Jul 10 22:24:51 2024 ] 	Batch(5200/7879) done. Loss: 0.0535  lr:0.000001
[ Wed Jul 10 22:25:14 2024 ] 	Batch(5300/7879) done. Loss: 0.0655  lr:0.000001
[ Wed Jul 10 22:25:37 2024 ] 	Batch(5400/7879) done. Loss: 0.0092  lr:0.000001
[ Wed Jul 10 22:26:00 2024 ] 
Training: Epoch [4/120], Step [5499], Loss: 0.19272109866142273, Training Accuracy: 96.29318181818182
[ Wed Jul 10 22:26:00 2024 ] 	Batch(5500/7879) done. Loss: 0.0897  lr:0.000001
[ Wed Jul 10 22:26:24 2024 ] 	Batch(5600/7879) done. Loss: 0.3603  lr:0.000001
[ Wed Jul 10 22:26:47 2024 ] 	Batch(5700/7879) done. Loss: 0.0185  lr:0.000001
[ Wed Jul 10 22:27:10 2024 ] 	Batch(5800/7879) done. Loss: 0.2355  lr:0.000001
[ Wed Jul 10 22:27:33 2024 ] 	Batch(5900/7879) done. Loss: 0.2725  lr:0.000001
[ Wed Jul 10 22:27:55 2024 ] 
Training: Epoch [4/120], Step [5999], Loss: 0.059649404138326645, Training Accuracy: 96.28125
[ Wed Jul 10 22:27:56 2024 ] 	Batch(6000/7879) done. Loss: 0.3159  lr:0.000001
[ Wed Jul 10 22:28:19 2024 ] 	Batch(6100/7879) done. Loss: 0.0376  lr:0.000001
[ Wed Jul 10 22:28:42 2024 ] 	Batch(6200/7879) done. Loss: 0.2229  lr:0.000001
[ Wed Jul 10 22:29:05 2024 ] 	Batch(6300/7879) done. Loss: 0.2858  lr:0.000001
[ Wed Jul 10 22:29:28 2024 ] 	Batch(6400/7879) done. Loss: 0.4092  lr:0.000001
[ Wed Jul 10 22:29:51 2024 ] 
Training: Epoch [4/120], Step [6499], Loss: 0.23582997918128967, Training Accuracy: 96.22884615384616
[ Wed Jul 10 22:29:52 2024 ] 	Batch(6500/7879) done. Loss: 0.1062  lr:0.000001
[ Wed Jul 10 22:30:15 2024 ] 	Batch(6600/7879) done. Loss: 0.1632  lr:0.000001
[ Wed Jul 10 22:30:38 2024 ] 	Batch(6700/7879) done. Loss: 0.0318  lr:0.000001
[ Wed Jul 10 22:31:01 2024 ] 	Batch(6800/7879) done. Loss: 0.0948  lr:0.000001
[ Wed Jul 10 22:31:24 2024 ] 	Batch(6900/7879) done. Loss: 0.5016  lr:0.000001
[ Wed Jul 10 22:31:47 2024 ] 
Training: Epoch [4/120], Step [6999], Loss: 0.22322525084018707, Training Accuracy: 96.20714285714286
[ Wed Jul 10 22:31:47 2024 ] 	Batch(7000/7879) done. Loss: 0.0113  lr:0.000001
[ Wed Jul 10 22:32:10 2024 ] 	Batch(7100/7879) done. Loss: 0.0514  lr:0.000001
[ Wed Jul 10 22:32:33 2024 ] 	Batch(7200/7879) done. Loss: 0.0417  lr:0.000001
[ Wed Jul 10 22:32:56 2024 ] 	Batch(7300/7879) done. Loss: 0.0359  lr:0.000001
[ Wed Jul 10 22:33:19 2024 ] 	Batch(7400/7879) done. Loss: 0.1427  lr:0.000001
[ Wed Jul 10 22:33:42 2024 ] 
Training: Epoch [4/120], Step [7499], Loss: 0.028466012328863144, Training Accuracy: 96.22833333333334
[ Wed Jul 10 22:33:42 2024 ] 	Batch(7500/7879) done. Loss: 0.0757  lr:0.000001
[ Wed Jul 10 22:34:05 2024 ] 	Batch(7600/7879) done. Loss: 0.0937  lr:0.000001
[ Wed Jul 10 22:34:29 2024 ] 	Batch(7700/7879) done. Loss: 0.0477  lr:0.000001
[ Wed Jul 10 22:34:52 2024 ] 	Batch(7800/7879) done. Loss: 0.1525  lr:0.000001
[ Wed Jul 10 22:35:10 2024 ] 	Mean training loss: 0.1427.
[ Wed Jul 10 22:35:10 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Wed Jul 10 22:35:10 2024 ] Training epoch: 6
[ Wed Jul 10 22:35:11 2024 ] 	Batch(0/7879) done. Loss: 0.0185  lr:0.000001
[ Wed Jul 10 22:35:34 2024 ] 	Batch(100/7879) done. Loss: 0.1100  lr:0.000001
[ Wed Jul 10 22:35:57 2024 ] 	Batch(200/7879) done. Loss: 0.0087  lr:0.000001
[ Wed Jul 10 22:36:20 2024 ] 	Batch(300/7879) done. Loss: 0.2595  lr:0.000001
[ Wed Jul 10 22:36:44 2024 ] 	Batch(400/7879) done. Loss: 0.0216  lr:0.000001
[ Wed Jul 10 22:37:07 2024 ] 
Training: Epoch [5/120], Step [499], Loss: 0.028973765671253204, Training Accuracy: 96.075
[ Wed Jul 10 22:37:07 2024 ] 	Batch(500/7879) done. Loss: 0.2176  lr:0.000001
[ Wed Jul 10 22:37:30 2024 ] 	Batch(600/7879) done. Loss: 0.0469  lr:0.000001
[ Wed Jul 10 22:37:53 2024 ] 	Batch(700/7879) done. Loss: 0.1313  lr:0.000001
[ Wed Jul 10 22:38:16 2024 ] 	Batch(800/7879) done. Loss: 0.0411  lr:0.000001
[ Wed Jul 10 22:38:40 2024 ] 	Batch(900/7879) done. Loss: 0.1799  lr:0.000001
[ Wed Jul 10 22:39:03 2024 ] 
Training: Epoch [5/120], Step [999], Loss: 0.21106883883476257, Training Accuracy: 95.775
[ Wed Jul 10 22:39:03 2024 ] 	Batch(1000/7879) done. Loss: 0.0399  lr:0.000001
[ Wed Jul 10 22:39:26 2024 ] 	Batch(1100/7879) done. Loss: 0.1351  lr:0.000001
[ Wed Jul 10 22:39:49 2024 ] 	Batch(1200/7879) done. Loss: 0.0850  lr:0.000001
[ Wed Jul 10 22:40:12 2024 ] 	Batch(1300/7879) done. Loss: 0.0367  lr:0.000001
[ Wed Jul 10 22:40:35 2024 ] 	Batch(1400/7879) done. Loss: 0.1003  lr:0.000001
[ Wed Jul 10 22:40:58 2024 ] 
Training: Epoch [5/120], Step [1499], Loss: 0.04651351645588875, Training Accuracy: 95.98333333333333
[ Wed Jul 10 22:40:58 2024 ] 	Batch(1500/7879) done. Loss: 0.0437  lr:0.000001
[ Wed Jul 10 22:41:22 2024 ] 	Batch(1600/7879) done. Loss: 0.1704  lr:0.000001
[ Wed Jul 10 22:41:46 2024 ] 	Batch(1700/7879) done. Loss: 0.0573  lr:0.000001
[ Wed Jul 10 22:42:10 2024 ] 	Batch(1800/7879) done. Loss: 0.1599  lr:0.000001
[ Wed Jul 10 22:42:34 2024 ] 	Batch(1900/7879) done. Loss: 0.0368  lr:0.000001
[ Wed Jul 10 22:42:57 2024 ] 
Training: Epoch [5/120], Step [1999], Loss: 0.20653922855854034, Training Accuracy: 96.0625
[ Wed Jul 10 22:42:58 2024 ] 	Batch(2000/7879) done. Loss: 0.1347  lr:0.000001
[ Wed Jul 10 22:43:21 2024 ] 	Batch(2100/7879) done. Loss: 0.0152  lr:0.000001
[ Wed Jul 10 22:43:44 2024 ] 	Batch(2200/7879) done. Loss: 0.0143  lr:0.000001
[ Wed Jul 10 22:44:07 2024 ] 	Batch(2300/7879) done. Loss: 0.0178  lr:0.000001
[ Wed Jul 10 22:44:30 2024 ] 	Batch(2400/7879) done. Loss: 0.5584  lr:0.000001
[ Wed Jul 10 22:44:53 2024 ] 
Training: Epoch [5/120], Step [2499], Loss: 0.08581017702817917, Training Accuracy: 96.185
[ Wed Jul 10 22:44:53 2024 ] 	Batch(2500/7879) done. Loss: 0.1536  lr:0.000001
[ Wed Jul 10 22:45:16 2024 ] 	Batch(2600/7879) done. Loss: 0.0400  lr:0.000001
[ Wed Jul 10 22:45:39 2024 ] 	Batch(2700/7879) done. Loss: 0.1845  lr:0.000001
[ Wed Jul 10 22:46:02 2024 ] 	Batch(2800/7879) done. Loss: 0.0598  lr:0.000001
[ Wed Jul 10 22:46:25 2024 ] 	Batch(2900/7879) done. Loss: 0.0120  lr:0.000001
[ Wed Jul 10 22:46:48 2024 ] 
Training: Epoch [5/120], Step [2999], Loss: 0.4501485526561737, Training Accuracy: 96.04166666666667
[ Wed Jul 10 22:46:48 2024 ] 	Batch(3000/7879) done. Loss: 0.1967  lr:0.000001
[ Wed Jul 10 22:47:11 2024 ] 	Batch(3100/7879) done. Loss: 0.2950  lr:0.000001
[ Wed Jul 10 22:47:34 2024 ] 	Batch(3200/7879) done. Loss: 0.0124  lr:0.000001
[ Wed Jul 10 22:47:57 2024 ] 	Batch(3300/7879) done. Loss: 0.0210  lr:0.000001
[ Wed Jul 10 22:48:20 2024 ] 	Batch(3400/7879) done. Loss: 0.1331  lr:0.000001
[ Wed Jul 10 22:48:43 2024 ] 
Training: Epoch [5/120], Step [3499], Loss: 0.1559755653142929, Training Accuracy: 96.06428571428572
[ Wed Jul 10 22:48:43 2024 ] 	Batch(3500/7879) done. Loss: 0.0461  lr:0.000001
[ Wed Jul 10 22:49:06 2024 ] 	Batch(3600/7879) done. Loss: 0.0224  lr:0.000001
[ Wed Jul 10 22:49:30 2024 ] 	Batch(3700/7879) done. Loss: 0.0839  lr:0.000001
[ Wed Jul 10 22:49:53 2024 ] 	Batch(3800/7879) done. Loss: 0.0786  lr:0.000001
[ Wed Jul 10 22:50:16 2024 ] 	Batch(3900/7879) done. Loss: 0.2324  lr:0.000001
[ Wed Jul 10 22:50:38 2024 ] 
Training: Epoch [5/120], Step [3999], Loss: 0.011132783256471157, Training Accuracy: 96.1
[ Wed Jul 10 22:50:39 2024 ] 	Batch(4000/7879) done. Loss: 0.5186  lr:0.000001
[ Wed Jul 10 22:51:02 2024 ] 	Batch(4100/7879) done. Loss: 0.0238  lr:0.000001
[ Wed Jul 10 22:51:25 2024 ] 	Batch(4200/7879) done. Loss: 0.5950  lr:0.000001
[ Wed Jul 10 22:51:48 2024 ] 	Batch(4300/7879) done. Loss: 0.3030  lr:0.000001
[ Wed Jul 10 22:52:10 2024 ] 	Batch(4400/7879) done. Loss: 0.0117  lr:0.000001
[ Wed Jul 10 22:52:33 2024 ] 
Training: Epoch [5/120], Step [4499], Loss: 0.009052700363099575, Training Accuracy: 96.125
[ Wed Jul 10 22:52:33 2024 ] 	Batch(4500/7879) done. Loss: 0.0156  lr:0.000001
[ Wed Jul 10 22:52:56 2024 ] 	Batch(4600/7879) done. Loss: 0.0323  lr:0.000001
[ Wed Jul 10 22:53:18 2024 ] 	Batch(4700/7879) done. Loss: 0.5910  lr:0.000001
[ Wed Jul 10 22:53:41 2024 ] 	Batch(4800/7879) done. Loss: 0.1345  lr:0.000001
[ Wed Jul 10 22:54:04 2024 ] 	Batch(4900/7879) done. Loss: 0.1639  lr:0.000001
[ Wed Jul 10 22:54:26 2024 ] 
Training: Epoch [5/120], Step [4999], Loss: 0.02306097187101841, Training Accuracy: 96.10499999999999
[ Wed Jul 10 22:54:26 2024 ] 	Batch(5000/7879) done. Loss: 0.0146  lr:0.000001
[ Wed Jul 10 22:54:50 2024 ] 	Batch(5100/7879) done. Loss: 0.0228  lr:0.000001
[ Wed Jul 10 22:55:14 2024 ] 	Batch(5200/7879) done. Loss: 0.0780  lr:0.000001
[ Wed Jul 10 22:55:36 2024 ] 	Batch(5300/7879) done. Loss: 0.0068  lr:0.000001
[ Wed Jul 10 22:55:59 2024 ] 	Batch(5400/7879) done. Loss: 0.0969  lr:0.000001
[ Wed Jul 10 22:56:22 2024 ] 
Training: Epoch [5/120], Step [5499], Loss: 0.018381275236606598, Training Accuracy: 96.13181818181819
[ Wed Jul 10 22:56:22 2024 ] 	Batch(5500/7879) done. Loss: 0.0318  lr:0.000001
[ Wed Jul 10 22:56:45 2024 ] 	Batch(5600/7879) done. Loss: 0.0625  lr:0.000001
[ Wed Jul 10 22:57:08 2024 ] 	Batch(5700/7879) done. Loss: 0.3280  lr:0.000001
[ Wed Jul 10 22:57:31 2024 ] 	Batch(5800/7879) done. Loss: 0.0793  lr:0.000001
[ Wed Jul 10 22:57:55 2024 ] 	Batch(5900/7879) done. Loss: 0.0621  lr:0.000001
[ Wed Jul 10 22:58:18 2024 ] 
Training: Epoch [5/120], Step [5999], Loss: 0.13229097425937653, Training Accuracy: 96.12291666666667
[ Wed Jul 10 22:58:18 2024 ] 	Batch(6000/7879) done. Loss: 0.0208  lr:0.000001
[ Wed Jul 10 22:58:41 2024 ] 	Batch(6100/7879) done. Loss: 0.1605  lr:0.000001
[ Wed Jul 10 22:59:04 2024 ] 	Batch(6200/7879) done. Loss: 0.1532  lr:0.000001
[ Wed Jul 10 22:59:27 2024 ] 	Batch(6300/7879) done. Loss: 0.3222  lr:0.000001
[ Wed Jul 10 22:59:50 2024 ] 	Batch(6400/7879) done. Loss: 0.1216  lr:0.000001
[ Wed Jul 10 23:00:12 2024 ] 
Training: Epoch [5/120], Step [6499], Loss: 0.1678737848997116, Training Accuracy: 96.07884615384616
[ Wed Jul 10 23:00:12 2024 ] 	Batch(6500/7879) done. Loss: 0.1083  lr:0.000001
[ Wed Jul 10 23:00:35 2024 ] 	Batch(6600/7879) done. Loss: 0.0515  lr:0.000001
[ Wed Jul 10 23:00:58 2024 ] 	Batch(6700/7879) done. Loss: 0.2444  lr:0.000001
[ Wed Jul 10 23:01:20 2024 ] 	Batch(6800/7879) done. Loss: 0.0662  lr:0.000001
[ Wed Jul 10 23:01:43 2024 ] 	Batch(6900/7879) done. Loss: 0.8157  lr:0.000001
[ Wed Jul 10 23:02:05 2024 ] 
Training: Epoch [5/120], Step [6999], Loss: 0.31390348076820374, Training Accuracy: 96.09821428571429
[ Wed Jul 10 23:02:06 2024 ] 	Batch(7000/7879) done. Loss: 0.1271  lr:0.000001
[ Wed Jul 10 23:02:28 2024 ] 	Batch(7100/7879) done. Loss: 0.3786  lr:0.000001
[ Wed Jul 10 23:02:51 2024 ] 	Batch(7200/7879) done. Loss: 0.0812  lr:0.000001
[ Wed Jul 10 23:03:13 2024 ] 	Batch(7300/7879) done. Loss: 0.1394  lr:0.000001
[ Wed Jul 10 23:03:36 2024 ] 	Batch(7400/7879) done. Loss: 0.4656  lr:0.000001
[ Wed Jul 10 23:03:58 2024 ] 
Training: Epoch [5/120], Step [7499], Loss: 0.21824970841407776, Training Accuracy: 96.10833333333333
[ Wed Jul 10 23:03:58 2024 ] 	Batch(7500/7879) done. Loss: 0.0081  lr:0.000001
[ Wed Jul 10 23:04:21 2024 ] 	Batch(7600/7879) done. Loss: 0.2013  lr:0.000001
[ Wed Jul 10 23:04:44 2024 ] 	Batch(7700/7879) done. Loss: 0.0968  lr:0.000001
[ Wed Jul 10 23:05:07 2024 ] 	Batch(7800/7879) done. Loss: 0.0614  lr:0.000001
[ Wed Jul 10 23:05:25 2024 ] 	Mean training loss: 0.1434.
[ Wed Jul 10 23:05:25 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Wed Jul 10 23:05:25 2024 ] Training epoch: 7
[ Wed Jul 10 23:05:25 2024 ] 	Batch(0/7879) done. Loss: 0.4529  lr:0.000001
[ Wed Jul 10 23:05:48 2024 ] 	Batch(100/7879) done. Loss: 0.0464  lr:0.000001
[ Wed Jul 10 23:06:11 2024 ] 	Batch(200/7879) done. Loss: 0.0971  lr:0.000001
[ Wed Jul 10 23:06:33 2024 ] 	Batch(300/7879) done. Loss: 0.1612  lr:0.000001
[ Wed Jul 10 23:06:56 2024 ] 	Batch(400/7879) done. Loss: 0.1652  lr:0.000001
[ Wed Jul 10 23:07:18 2024 ] 
Training: Epoch [6/120], Step [499], Loss: 0.08972060680389404, Training Accuracy: 96.025
[ Wed Jul 10 23:07:18 2024 ] 	Batch(500/7879) done. Loss: 0.0865  lr:0.000001
[ Wed Jul 10 23:07:41 2024 ] 	Batch(600/7879) done. Loss: 0.3494  lr:0.000001
[ Wed Jul 10 23:08:04 2024 ] 	Batch(700/7879) done. Loss: 0.0033  lr:0.000001
[ Wed Jul 10 23:08:26 2024 ] 	Batch(800/7879) done. Loss: 0.0050  lr:0.000001
[ Wed Jul 10 23:08:49 2024 ] 	Batch(900/7879) done. Loss: 0.1738  lr:0.000001
[ Wed Jul 10 23:09:11 2024 ] 
Training: Epoch [6/120], Step [999], Loss: 0.24929577112197876, Training Accuracy: 95.75
[ Wed Jul 10 23:09:11 2024 ] 	Batch(1000/7879) done. Loss: 0.1413  lr:0.000001
[ Wed Jul 10 23:09:34 2024 ] 	Batch(1100/7879) done. Loss: 0.0016  lr:0.000001
[ Wed Jul 10 23:09:57 2024 ] 	Batch(1200/7879) done. Loss: 0.0210  lr:0.000001
[ Wed Jul 10 23:10:19 2024 ] 	Batch(1300/7879) done. Loss: 0.0529  lr:0.000001
[ Wed Jul 10 23:10:42 2024 ] 	Batch(1400/7879) done. Loss: 0.0441  lr:0.000001
[ Wed Jul 10 23:11:04 2024 ] 
Training: Epoch [6/120], Step [1499], Loss: 0.15940740704536438, Training Accuracy: 96.075
[ Wed Jul 10 23:11:04 2024 ] 	Batch(1500/7879) done. Loss: 0.3995  lr:0.000001
[ Wed Jul 10 23:11:27 2024 ] 	Batch(1600/7879) done. Loss: 0.3023  lr:0.000001
[ Wed Jul 10 23:11:50 2024 ] 	Batch(1700/7879) done. Loss: 0.0870  lr:0.000001
[ Wed Jul 10 23:12:14 2024 ] 	Batch(1800/7879) done. Loss: 0.1873  lr:0.000001
[ Wed Jul 10 23:12:37 2024 ] 	Batch(1900/7879) done. Loss: 0.1013  lr:0.000001
[ Wed Jul 10 23:13:00 2024 ] 
Training: Epoch [6/120], Step [1999], Loss: 0.23055602610111237, Training Accuracy: 96.08125
[ Wed Jul 10 23:13:00 2024 ] 	Batch(2000/7879) done. Loss: 0.2776  lr:0.000001
[ Wed Jul 10 23:13:23 2024 ] 	Batch(2100/7879) done. Loss: 0.2460  lr:0.000001
[ Wed Jul 10 23:13:45 2024 ] 	Batch(2200/7879) done. Loss: 0.0137  lr:0.000001
[ Wed Jul 10 23:14:08 2024 ] 	Batch(2300/7879) done. Loss: 0.3658  lr:0.000001
[ Wed Jul 10 23:14:31 2024 ] 	Batch(2400/7879) done. Loss: 0.1420  lr:0.000001
[ Wed Jul 10 23:14:53 2024 ] 
Training: Epoch [6/120], Step [2499], Loss: 0.03890369087457657, Training Accuracy: 96.045
[ Wed Jul 10 23:14:53 2024 ] 	Batch(2500/7879) done. Loss: 0.0106  lr:0.000001
[ Wed Jul 10 23:15:16 2024 ] 	Batch(2600/7879) done. Loss: 0.0925  lr:0.000001
[ Wed Jul 10 23:15:38 2024 ] 	Batch(2700/7879) done. Loss: 0.1227  lr:0.000001
[ Wed Jul 10 23:16:01 2024 ] 	Batch(2800/7879) done. Loss: 0.5005  lr:0.000001
[ Wed Jul 10 23:16:25 2024 ] 	Batch(2900/7879) done. Loss: 0.2897  lr:0.000001
[ Wed Jul 10 23:16:48 2024 ] 
Training: Epoch [6/120], Step [2999], Loss: 0.06352916359901428, Training Accuracy: 96.1625
[ Wed Jul 10 23:16:48 2024 ] 	Batch(3000/7879) done. Loss: 0.2960  lr:0.000001
[ Wed Jul 10 23:17:11 2024 ] 	Batch(3100/7879) done. Loss: 0.1153  lr:0.000001
[ Wed Jul 10 23:17:35 2024 ] 	Batch(3200/7879) done. Loss: 0.2783  lr:0.000001
[ Wed Jul 10 23:17:57 2024 ] 	Batch(3300/7879) done. Loss: 0.0663  lr:0.000001
[ Wed Jul 10 23:18:20 2024 ] 	Batch(3400/7879) done. Loss: 0.0228  lr:0.000001
[ Wed Jul 10 23:18:42 2024 ] 
Training: Epoch [6/120], Step [3499], Loss: 0.41535234451293945, Training Accuracy: 96.10357142857143
[ Wed Jul 10 23:18:42 2024 ] 	Batch(3500/7879) done. Loss: 0.1571  lr:0.000001
[ Wed Jul 10 23:19:05 2024 ] 	Batch(3600/7879) done. Loss: 0.0162  lr:0.000001
[ Wed Jul 10 23:19:28 2024 ] 	Batch(3700/7879) done. Loss: 0.0177  lr:0.000001
[ Wed Jul 10 23:19:50 2024 ] 	Batch(3800/7879) done. Loss: 0.1954  lr:0.000001
[ Wed Jul 10 23:20:13 2024 ] 	Batch(3900/7879) done. Loss: 0.3127  lr:0.000001
[ Wed Jul 10 23:20:35 2024 ] 
Training: Epoch [6/120], Step [3999], Loss: 0.16322675347328186, Training Accuracy: 96.075
[ Wed Jul 10 23:20:36 2024 ] 	Batch(4000/7879) done. Loss: 0.0365  lr:0.000001
[ Wed Jul 10 23:20:59 2024 ] 	Batch(4100/7879) done. Loss: 0.1673  lr:0.000001
[ Wed Jul 10 23:21:22 2024 ] 	Batch(4200/7879) done. Loss: 0.0428  lr:0.000001
[ Wed Jul 10 23:21:46 2024 ] 	Batch(4300/7879) done. Loss: 0.2221  lr:0.000001
[ Wed Jul 10 23:22:10 2024 ] 	Batch(4400/7879) done. Loss: 0.1549  lr:0.000001
[ Wed Jul 10 23:22:33 2024 ] 
Training: Epoch [6/120], Step [4499], Loss: 0.5406438708305359, Training Accuracy: 96.1138888888889
[ Wed Jul 10 23:22:33 2024 ] 	Batch(4500/7879) done. Loss: 0.0104  lr:0.000001
[ Wed Jul 10 23:22:56 2024 ] 	Batch(4600/7879) done. Loss: 0.1011  lr:0.000001
[ Wed Jul 10 23:23:18 2024 ] 	Batch(4700/7879) done. Loss: 0.0148  lr:0.000001
[ Wed Jul 10 23:23:41 2024 ] 	Batch(4800/7879) done. Loss: 0.0220  lr:0.000001
[ Wed Jul 10 23:24:04 2024 ] 	Batch(4900/7879) done. Loss: 0.0030  lr:0.000001
[ Wed Jul 10 23:24:27 2024 ] 
Training: Epoch [6/120], Step [4999], Loss: 0.02704329416155815, Training Accuracy: 96.135
[ Wed Jul 10 23:24:28 2024 ] 	Batch(5000/7879) done. Loss: 0.1473  lr:0.000001
[ Wed Jul 10 23:24:51 2024 ] 	Batch(5100/7879) done. Loss: 0.0694  lr:0.000001
[ Wed Jul 10 23:25:14 2024 ] 	Batch(5200/7879) done. Loss: 0.0325  lr:0.000001
[ Wed Jul 10 23:25:36 2024 ] 	Batch(5300/7879) done. Loss: 0.0924  lr:0.000001
[ Wed Jul 10 23:25:59 2024 ] 	Batch(5400/7879) done. Loss: 0.1153  lr:0.000001
[ Wed Jul 10 23:26:21 2024 ] 
Training: Epoch [6/120], Step [5499], Loss: 0.11702980846166611, Training Accuracy: 96.13863636363637
[ Wed Jul 10 23:26:21 2024 ] 	Batch(5500/7879) done. Loss: 0.0699  lr:0.000001
[ Wed Jul 10 23:26:44 2024 ] 	Batch(5600/7879) done. Loss: 0.0012  lr:0.000001
[ Wed Jul 10 23:27:07 2024 ] 	Batch(5700/7879) done. Loss: 0.3745  lr:0.000001
[ Wed Jul 10 23:27:29 2024 ] 	Batch(5800/7879) done. Loss: 0.0597  lr:0.000001
[ Wed Jul 10 23:27:52 2024 ] 	Batch(5900/7879) done. Loss: 0.3450  lr:0.000001
[ Wed Jul 10 23:28:14 2024 ] 
Training: Epoch [6/120], Step [5999], Loss: 0.029748747125267982, Training Accuracy: 96.15416666666667
[ Wed Jul 10 23:28:14 2024 ] 	Batch(6000/7879) done. Loss: 0.0396  lr:0.000001
[ Wed Jul 10 23:28:37 2024 ] 	Batch(6100/7879) done. Loss: 0.0224  lr:0.000001
[ Wed Jul 10 23:28:59 2024 ] 	Batch(6200/7879) done. Loss: 0.0994  lr:0.000001
[ Wed Jul 10 23:29:22 2024 ] 	Batch(6300/7879) done. Loss: 0.1148  lr:0.000001
[ Wed Jul 10 23:29:45 2024 ] 	Batch(6400/7879) done. Loss: 0.2338  lr:0.000001
[ Wed Jul 10 23:30:08 2024 ] 
Training: Epoch [6/120], Step [6499], Loss: 0.5205985307693481, Training Accuracy: 96.1423076923077
[ Wed Jul 10 23:30:09 2024 ] 	Batch(6500/7879) done. Loss: 0.1552  lr:0.000001
[ Wed Jul 10 23:30:32 2024 ] 	Batch(6600/7879) done. Loss: 0.1126  lr:0.000001
[ Wed Jul 10 23:30:55 2024 ] 	Batch(6700/7879) done. Loss: 0.3605  lr:0.000001
[ Wed Jul 10 23:31:19 2024 ] 	Batch(6800/7879) done. Loss: 0.2228  lr:0.000001
[ Wed Jul 10 23:31:42 2024 ] 	Batch(6900/7879) done. Loss: 0.0665  lr:0.000001
[ Wed Jul 10 23:32:05 2024 ] 
Training: Epoch [6/120], Step [6999], Loss: 0.125448077917099, Training Accuracy: 96.1625
[ Wed Jul 10 23:32:05 2024 ] 	Batch(7000/7879) done. Loss: 0.0581  lr:0.000001
[ Wed Jul 10 23:32:28 2024 ] 	Batch(7100/7879) done. Loss: 0.0411  lr:0.000001
[ Wed Jul 10 23:32:52 2024 ] 	Batch(7200/7879) done. Loss: 0.0271  lr:0.000001
[ Wed Jul 10 23:33:15 2024 ] 	Batch(7300/7879) done. Loss: 0.1740  lr:0.000001
[ Wed Jul 10 23:33:38 2024 ] 	Batch(7400/7879) done. Loss: 0.5291  lr:0.000001
[ Wed Jul 10 23:34:01 2024 ] 
Training: Epoch [6/120], Step [7499], Loss: 0.014894910156726837, Training Accuracy: 96.14666666666668
[ Wed Jul 10 23:34:01 2024 ] 	Batch(7500/7879) done. Loss: 0.2057  lr:0.000001
[ Wed Jul 10 23:34:24 2024 ] 	Batch(7600/7879) done. Loss: 0.1208  lr:0.000001
[ Wed Jul 10 23:34:48 2024 ] 	Batch(7700/7879) done. Loss: 0.0560  lr:0.000001
[ Wed Jul 10 23:35:11 2024 ] 	Batch(7800/7879) done. Loss: 0.3189  lr:0.000001
[ Wed Jul 10 23:35:29 2024 ] 	Mean training loss: 0.1422.
[ Wed Jul 10 23:35:29 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 10 23:35:29 2024 ] Training epoch: 8
[ Wed Jul 10 23:35:30 2024 ] 	Batch(0/7879) done. Loss: 0.0186  lr:0.000001
[ Wed Jul 10 23:35:53 2024 ] 	Batch(100/7879) done. Loss: 0.0243  lr:0.000001
[ Wed Jul 10 23:36:17 2024 ] 	Batch(200/7879) done. Loss: 0.1446  lr:0.000001
[ Wed Jul 10 23:36:40 2024 ] 	Batch(300/7879) done. Loss: 0.2097  lr:0.000001
[ Wed Jul 10 23:37:03 2024 ] 	Batch(400/7879) done. Loss: 0.3779  lr:0.000001
[ Wed Jul 10 23:37:26 2024 ] 
Training: Epoch [7/120], Step [499], Loss: 0.3250700533390045, Training Accuracy: 96.2
[ Wed Jul 10 23:37:27 2024 ] 	Batch(500/7879) done. Loss: 0.3225  lr:0.000001
[ Wed Jul 10 23:37:50 2024 ] 	Batch(600/7879) done. Loss: 0.8392  lr:0.000001
[ Wed Jul 10 23:38:13 2024 ] 	Batch(700/7879) done. Loss: 0.1733  lr:0.000001
[ Wed Jul 10 23:38:36 2024 ] 	Batch(800/7879) done. Loss: 0.6359  lr:0.000001
[ Wed Jul 10 23:38:59 2024 ] 	Batch(900/7879) done. Loss: 0.0062  lr:0.000001
[ Wed Jul 10 23:39:22 2024 ] 
Training: Epoch [7/120], Step [999], Loss: 0.30268344283103943, Training Accuracy: 96.33749999999999
[ Wed Jul 10 23:39:22 2024 ] 	Batch(1000/7879) done. Loss: 0.0822  lr:0.000001
[ Wed Jul 10 23:39:45 2024 ] 	Batch(1100/7879) done. Loss: 0.1507  lr:0.000001
[ Wed Jul 10 23:40:08 2024 ] 	Batch(1200/7879) done. Loss: 0.0200  lr:0.000001
[ Wed Jul 10 23:40:31 2024 ] 	Batch(1300/7879) done. Loss: 0.0484  lr:0.000001
[ Wed Jul 10 23:40:54 2024 ] 	Batch(1400/7879) done. Loss: 0.3501  lr:0.000001
[ Wed Jul 10 23:41:17 2024 ] 
Training: Epoch [7/120], Step [1499], Loss: 0.1419348120689392, Training Accuracy: 96.275
[ Wed Jul 10 23:41:17 2024 ] 	Batch(1500/7879) done. Loss: 0.1810  lr:0.000001
[ Wed Jul 10 23:41:41 2024 ] 	Batch(1600/7879) done. Loss: 0.2051  lr:0.000001
[ Wed Jul 10 23:42:04 2024 ] 	Batch(1700/7879) done. Loss: 0.2258  lr:0.000001
[ Wed Jul 10 23:42:27 2024 ] 	Batch(1800/7879) done. Loss: 0.0420  lr:0.000001
[ Wed Jul 10 23:42:50 2024 ] 	Batch(1900/7879) done. Loss: 0.3107  lr:0.000001
[ Wed Jul 10 23:43:12 2024 ] 
Training: Epoch [7/120], Step [1999], Loss: 0.0029780231416225433, Training Accuracy: 96.26249999999999
[ Wed Jul 10 23:43:13 2024 ] 	Batch(2000/7879) done. Loss: 0.0794  lr:0.000001
[ Wed Jul 10 23:43:36 2024 ] 	Batch(2100/7879) done. Loss: 0.1706  lr:0.000001
[ Wed Jul 10 23:43:59 2024 ] 	Batch(2200/7879) done. Loss: 0.0121  lr:0.000001
[ Wed Jul 10 23:44:22 2024 ] 	Batch(2300/7879) done. Loss: 0.0766  lr:0.000001
[ Wed Jul 10 23:44:46 2024 ] 	Batch(2400/7879) done. Loss: 0.0085  lr:0.000001
[ Wed Jul 10 23:45:09 2024 ] 
Training: Epoch [7/120], Step [2499], Loss: 0.06647191941738129, Training Accuracy: 96.255
[ Wed Jul 10 23:45:09 2024 ] 	Batch(2500/7879) done. Loss: 0.2834  lr:0.000001
[ Wed Jul 10 23:45:32 2024 ] 	Batch(2600/7879) done. Loss: 0.1231  lr:0.000001
[ Wed Jul 10 23:45:55 2024 ] 	Batch(2700/7879) done. Loss: 0.1472  lr:0.000001
[ Wed Jul 10 23:46:19 2024 ] 	Batch(2800/7879) done. Loss: 0.0020  lr:0.000001
[ Wed Jul 10 23:46:42 2024 ] 	Batch(2900/7879) done. Loss: 0.0275  lr:0.000001
[ Wed Jul 10 23:47:05 2024 ] 
Training: Epoch [7/120], Step [2999], Loss: 0.014944065362215042, Training Accuracy: 96.23333333333333
[ Wed Jul 10 23:47:05 2024 ] 	Batch(3000/7879) done. Loss: 0.1837  lr:0.000001
[ Wed Jul 10 23:47:28 2024 ] 	Batch(3100/7879) done. Loss: 0.0060  lr:0.000001
[ Wed Jul 10 23:47:51 2024 ] 	Batch(3200/7879) done. Loss: 0.1134  lr:0.000001
[ Wed Jul 10 23:48:15 2024 ] 	Batch(3300/7879) done. Loss: 0.0161  lr:0.000001
[ Wed Jul 10 23:48:38 2024 ] 	Batch(3400/7879) done. Loss: 0.2042  lr:0.000001
[ Wed Jul 10 23:49:01 2024 ] 
Training: Epoch [7/120], Step [3499], Loss: 0.11472772061824799, Training Accuracy: 96.2
[ Wed Jul 10 23:49:01 2024 ] 	Batch(3500/7879) done. Loss: 0.0656  lr:0.000001
[ Wed Jul 10 23:49:24 2024 ] 	Batch(3600/7879) done. Loss: 0.0295  lr:0.000001
[ Wed Jul 10 23:49:47 2024 ] 	Batch(3700/7879) done. Loss: 0.0247  lr:0.000001
[ Wed Jul 10 23:50:10 2024 ] 	Batch(3800/7879) done. Loss: 0.0051  lr:0.000001
[ Wed Jul 10 23:50:33 2024 ] 	Batch(3900/7879) done. Loss: 0.4361  lr:0.000001
[ Wed Jul 10 23:50:56 2024 ] 
Training: Epoch [7/120], Step [3999], Loss: 0.04821310192346573, Training Accuracy: 96.2
[ Wed Jul 10 23:50:56 2024 ] 	Batch(4000/7879) done. Loss: 0.1477  lr:0.000001
[ Wed Jul 10 23:51:20 2024 ] 	Batch(4100/7879) done. Loss: 0.0456  lr:0.000001
[ Wed Jul 10 23:51:44 2024 ] 	Batch(4200/7879) done. Loss: 0.0979  lr:0.000001
[ Wed Jul 10 23:52:07 2024 ] 	Batch(4300/7879) done. Loss: 0.1567  lr:0.000001
[ Wed Jul 10 23:52:30 2024 ] 	Batch(4400/7879) done. Loss: 0.1372  lr:0.000001
[ Wed Jul 10 23:52:53 2024 ] 
Training: Epoch [7/120], Step [4499], Loss: 0.19789092242717743, Training Accuracy: 96.21388888888889
[ Wed Jul 10 23:52:54 2024 ] 	Batch(4500/7879) done. Loss: 0.0694  lr:0.000001
[ Wed Jul 10 23:53:17 2024 ] 	Batch(4600/7879) done. Loss: 0.3918  lr:0.000001
[ Wed Jul 10 23:53:41 2024 ] 	Batch(4700/7879) done. Loss: 0.1452  lr:0.000001
[ Wed Jul 10 23:54:04 2024 ] 	Batch(4800/7879) done. Loss: 0.0821  lr:0.000001
[ Wed Jul 10 23:54:27 2024 ] 	Batch(4900/7879) done. Loss: 0.1623  lr:0.000001
[ Wed Jul 10 23:54:50 2024 ] 
Training: Epoch [7/120], Step [4999], Loss: 0.16499662399291992, Training Accuracy: 96.165
[ Wed Jul 10 23:54:50 2024 ] 	Batch(5000/7879) done. Loss: 0.0881  lr:0.000001
[ Wed Jul 10 23:55:13 2024 ] 	Batch(5100/7879) done. Loss: 0.1247  lr:0.000001
[ Wed Jul 10 23:55:37 2024 ] 	Batch(5200/7879) done. Loss: 0.0093  lr:0.000001
[ Wed Jul 10 23:56:00 2024 ] 	Batch(5300/7879) done. Loss: 0.0496  lr:0.000001
[ Wed Jul 10 23:56:22 2024 ] 	Batch(5400/7879) done. Loss: 0.3871  lr:0.000001
[ Wed Jul 10 23:56:45 2024 ] 
Training: Epoch [7/120], Step [5499], Loss: 0.10386931896209717, Training Accuracy: 96.175
[ Wed Jul 10 23:56:45 2024 ] 	Batch(5500/7879) done. Loss: 0.0158  lr:0.000001
[ Wed Jul 10 23:57:08 2024 ] 	Batch(5600/7879) done. Loss: 0.0943  lr:0.000001
[ Wed Jul 10 23:57:31 2024 ] 	Batch(5700/7879) done. Loss: 0.0688  lr:0.000001
[ Wed Jul 10 23:57:53 2024 ] 	Batch(5800/7879) done. Loss: 0.0302  lr:0.000001
[ Wed Jul 10 23:58:16 2024 ] 	Batch(5900/7879) done. Loss: 0.0402  lr:0.000001
[ Wed Jul 10 23:58:38 2024 ] 
Training: Epoch [7/120], Step [5999], Loss: 0.6933566331863403, Training Accuracy: 96.16875
[ Wed Jul 10 23:58:38 2024 ] 	Batch(6000/7879) done. Loss: 0.2933  lr:0.000001
[ Wed Jul 10 23:59:01 2024 ] 	Batch(6100/7879) done. Loss: 0.0246  lr:0.000001
[ Wed Jul 10 23:59:24 2024 ] 	Batch(6200/7879) done. Loss: 0.0383  lr:0.000001
[ Wed Jul 10 23:59:46 2024 ] 	Batch(6300/7879) done. Loss: 0.0214  lr:0.000001
[ Thu Jul 11 00:00:09 2024 ] 	Batch(6400/7879) done. Loss: 0.0357  lr:0.000001
[ Thu Jul 11 00:00:32 2024 ] 
Training: Epoch [7/120], Step [6499], Loss: 0.011480473913252354, Training Accuracy: 96.17307692307693
[ Thu Jul 11 00:00:32 2024 ] 	Batch(6500/7879) done. Loss: 0.0077  lr:0.000001
[ Thu Jul 11 00:00:56 2024 ] 	Batch(6600/7879) done. Loss: 0.0134  lr:0.000001
[ Thu Jul 11 00:01:19 2024 ] 	Batch(6700/7879) done. Loss: 0.0016  lr:0.000001
[ Thu Jul 11 00:01:42 2024 ] 	Batch(6800/7879) done. Loss: 0.1721  lr:0.000001
[ Thu Jul 11 00:02:06 2024 ] 	Batch(6900/7879) done. Loss: 0.0658  lr:0.000001
[ Thu Jul 11 00:02:29 2024 ] 
Training: Epoch [7/120], Step [6999], Loss: 0.004531088285148144, Training Accuracy: 96.18035714285715
[ Thu Jul 11 00:02:29 2024 ] 	Batch(7000/7879) done. Loss: 0.1023  lr:0.000001
[ Thu Jul 11 00:02:53 2024 ] 	Batch(7100/7879) done. Loss: 0.0513  lr:0.000001
[ Thu Jul 11 00:03:16 2024 ] 	Batch(7200/7879) done. Loss: 0.1032  lr:0.000001
[ Thu Jul 11 00:03:40 2024 ] 	Batch(7300/7879) done. Loss: 0.0581  lr:0.000001
[ Thu Jul 11 00:04:03 2024 ] 	Batch(7400/7879) done. Loss: 0.0982  lr:0.000001
[ Thu Jul 11 00:04:26 2024 ] 
Training: Epoch [7/120], Step [7499], Loss: 0.057486701756715775, Training Accuracy: 96.2
[ Thu Jul 11 00:04:26 2024 ] 	Batch(7500/7879) done. Loss: 0.1295  lr:0.000001
[ Thu Jul 11 00:04:49 2024 ] 	Batch(7600/7879) done. Loss: 0.0401  lr:0.000001
[ Thu Jul 11 00:05:11 2024 ] 	Batch(7700/7879) done. Loss: 0.1694  lr:0.000001
[ Thu Jul 11 00:05:34 2024 ] 	Batch(7800/7879) done. Loss: 0.1461  lr:0.000001
[ Thu Jul 11 00:05:52 2024 ] 	Mean training loss: 0.1424.
[ Thu Jul 11 00:05:52 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Thu Jul 11 00:05:52 2024 ] Training epoch: 9
[ Thu Jul 11 00:05:53 2024 ] 	Batch(0/7879) done. Loss: 0.7704  lr:0.000001
[ Thu Jul 11 00:06:15 2024 ] 	Batch(100/7879) done. Loss: 0.1664  lr:0.000001
[ Thu Jul 11 00:06:38 2024 ] 	Batch(200/7879) done. Loss: 0.0357  lr:0.000001
[ Thu Jul 11 00:07:01 2024 ] 	Batch(300/7879) done. Loss: 0.0730  lr:0.000001
[ Thu Jul 11 00:07:24 2024 ] 	Batch(400/7879) done. Loss: 0.0164  lr:0.000001
[ Thu Jul 11 00:07:46 2024 ] 
Training: Epoch [8/120], Step [499], Loss: 0.023748409003019333, Training Accuracy: 96.35000000000001
[ Thu Jul 11 00:07:46 2024 ] 	Batch(500/7879) done. Loss: 0.0178  lr:0.000001
[ Thu Jul 11 00:08:09 2024 ] 	Batch(600/7879) done. Loss: 0.1515  lr:0.000001
[ Thu Jul 11 00:08:32 2024 ] 	Batch(700/7879) done. Loss: 0.1678  lr:0.000001
[ Thu Jul 11 00:08:55 2024 ] 	Batch(800/7879) done. Loss: 0.1307  lr:0.000001
[ Thu Jul 11 00:09:18 2024 ] 	Batch(900/7879) done. Loss: 0.0133  lr:0.000001
[ Thu Jul 11 00:09:40 2024 ] 
Training: Epoch [8/120], Step [999], Loss: 0.02659977786242962, Training Accuracy: 96.35000000000001
[ Thu Jul 11 00:09:41 2024 ] 	Batch(1000/7879) done. Loss: 0.0578  lr:0.000001
[ Thu Jul 11 00:10:03 2024 ] 	Batch(1100/7879) done. Loss: 0.0788  lr:0.000001
[ Thu Jul 11 00:10:26 2024 ] 	Batch(1200/7879) done. Loss: 0.1226  lr:0.000001
[ Thu Jul 11 00:10:49 2024 ] 	Batch(1300/7879) done. Loss: 0.3419  lr:0.000001
[ Thu Jul 11 00:11:11 2024 ] 	Batch(1400/7879) done. Loss: 0.0444  lr:0.000001
[ Thu Jul 11 00:11:34 2024 ] 
Training: Epoch [8/120], Step [1499], Loss: 0.08810161054134369, Training Accuracy: 96.41666666666666
[ Thu Jul 11 00:11:35 2024 ] 	Batch(1500/7879) done. Loss: 0.0389  lr:0.000001
[ Thu Jul 11 00:11:58 2024 ] 	Batch(1600/7879) done. Loss: 0.3909  lr:0.000001
[ Thu Jul 11 00:12:20 2024 ] 	Batch(1700/7879) done. Loss: 0.0536  lr:0.000001
[ Thu Jul 11 00:12:43 2024 ] 	Batch(1800/7879) done. Loss: 0.0461  lr:0.000001
[ Thu Jul 11 00:13:06 2024 ] 	Batch(1900/7879) done. Loss: 0.1005  lr:0.000001
[ Thu Jul 11 00:13:29 2024 ] 
Training: Epoch [8/120], Step [1999], Loss: 0.021090323105454445, Training Accuracy: 96.4125
[ Thu Jul 11 00:13:29 2024 ] 	Batch(2000/7879) done. Loss: 0.2697  lr:0.000001
[ Thu Jul 11 00:13:52 2024 ] 	Batch(2100/7879) done. Loss: 0.0057  lr:0.000001
[ Thu Jul 11 00:14:14 2024 ] 	Batch(2200/7879) done. Loss: 0.0402  lr:0.000001
[ Thu Jul 11 00:14:37 2024 ] 	Batch(2300/7879) done. Loss: 0.1237  lr:0.000001
[ Thu Jul 11 00:15:00 2024 ] 	Batch(2400/7879) done. Loss: 0.0088  lr:0.000001
[ Thu Jul 11 00:15:23 2024 ] 
Training: Epoch [8/120], Step [2499], Loss: 0.009447852149605751, Training Accuracy: 96.31
[ Thu Jul 11 00:15:23 2024 ] 	Batch(2500/7879) done. Loss: 0.1533  lr:0.000001
[ Thu Jul 11 00:15:45 2024 ] 	Batch(2600/7879) done. Loss: 0.0551  lr:0.000001
[ Thu Jul 11 00:16:08 2024 ] 	Batch(2700/7879) done. Loss: 0.0175  lr:0.000001
[ Thu Jul 11 00:16:31 2024 ] 	Batch(2800/7879) done. Loss: 0.1899  lr:0.000001
[ Thu Jul 11 00:16:54 2024 ] 	Batch(2900/7879) done. Loss: 0.0173  lr:0.000001
[ Thu Jul 11 00:17:16 2024 ] 
Training: Epoch [8/120], Step [2999], Loss: 0.11886292695999146, Training Accuracy: 96.34583333333333
[ Thu Jul 11 00:17:17 2024 ] 	Batch(3000/7879) done. Loss: 0.6938  lr:0.000001
[ Thu Jul 11 00:17:39 2024 ] 	Batch(3100/7879) done. Loss: 0.1332  lr:0.000001
[ Thu Jul 11 00:18:02 2024 ] 	Batch(3200/7879) done. Loss: 0.1329  lr:0.000001
[ Thu Jul 11 00:18:25 2024 ] 	Batch(3300/7879) done. Loss: 0.2058  lr:0.000001
[ Thu Jul 11 00:18:48 2024 ] 	Batch(3400/7879) done. Loss: 0.2128  lr:0.000001
[ Thu Jul 11 00:19:10 2024 ] 
Training: Epoch [8/120], Step [3499], Loss: 0.12174521386623383, Training Accuracy: 96.28928571428571
[ Thu Jul 11 00:19:10 2024 ] 	Batch(3500/7879) done. Loss: 0.0621  lr:0.000001
[ Thu Jul 11 00:19:33 2024 ] 	Batch(3600/7879) done. Loss: 0.2042  lr:0.000001
[ Thu Jul 11 00:19:56 2024 ] 	Batch(3700/7879) done. Loss: 0.1375  lr:0.000001
[ Thu Jul 11 00:20:19 2024 ] 	Batch(3800/7879) done. Loss: 0.5548  lr:0.000001
[ Thu Jul 11 00:20:42 2024 ] 	Batch(3900/7879) done. Loss: 0.0662  lr:0.000001
[ Thu Jul 11 00:21:04 2024 ] 
Training: Epoch [8/120], Step [3999], Loss: 0.10112035274505615, Training Accuracy: 96.28750000000001
[ Thu Jul 11 00:21:04 2024 ] 	Batch(4000/7879) done. Loss: 0.0923  lr:0.000001
[ Thu Jul 11 00:21:28 2024 ] 	Batch(4100/7879) done. Loss: 0.0451  lr:0.000001
[ Thu Jul 11 00:21:50 2024 ] 	Batch(4200/7879) done. Loss: 0.0246  lr:0.000001
[ Thu Jul 11 00:22:13 2024 ] 	Batch(4300/7879) done. Loss: 0.0116  lr:0.000001
[ Thu Jul 11 00:22:36 2024 ] 	Batch(4400/7879) done. Loss: 0.0153  lr:0.000001
[ Thu Jul 11 00:22:59 2024 ] 
Training: Epoch [8/120], Step [4499], Loss: 0.2774679362773895, Training Accuracy: 96.25277777777778
[ Thu Jul 11 00:22:59 2024 ] 	Batch(4500/7879) done. Loss: 0.1273  lr:0.000001
[ Thu Jul 11 00:23:22 2024 ] 	Batch(4600/7879) done. Loss: 0.0243  lr:0.000001
[ Thu Jul 11 00:23:44 2024 ] 	Batch(4700/7879) done. Loss: 0.0879  lr:0.000001
[ Thu Jul 11 00:24:07 2024 ] 	Batch(4800/7879) done. Loss: 0.1189  lr:0.000001
[ Thu Jul 11 00:24:30 2024 ] 	Batch(4900/7879) done. Loss: 0.0701  lr:0.000001
[ Thu Jul 11 00:24:53 2024 ] 
Training: Epoch [8/120], Step [4999], Loss: 0.060071419924497604, Training Accuracy: 96.2325
[ Thu Jul 11 00:24:53 2024 ] 	Batch(5000/7879) done. Loss: 0.3906  lr:0.000001
[ Thu Jul 11 00:25:16 2024 ] 	Batch(5100/7879) done. Loss: 0.0302  lr:0.000001
[ Thu Jul 11 00:25:39 2024 ] 	Batch(5200/7879) done. Loss: 0.1362  lr:0.000001
[ Thu Jul 11 00:26:02 2024 ] 	Batch(5300/7879) done. Loss: 0.0894  lr:0.000001
[ Thu Jul 11 00:26:25 2024 ] 	Batch(5400/7879) done. Loss: 0.0272  lr:0.000001
[ Thu Jul 11 00:26:48 2024 ] 
Training: Epoch [8/120], Step [5499], Loss: 0.25860893726348877, Training Accuracy: 96.23409090909091
[ Thu Jul 11 00:26:48 2024 ] 	Batch(5500/7879) done. Loss: 0.1223  lr:0.000001
[ Thu Jul 11 00:27:12 2024 ] 	Batch(5600/7879) done. Loss: 0.0057  lr:0.000001
[ Thu Jul 11 00:27:35 2024 ] 	Batch(5700/7879) done. Loss: 0.0529  lr:0.000001
[ Thu Jul 11 00:27:58 2024 ] 	Batch(5800/7879) done. Loss: 0.0620  lr:0.000001
[ Thu Jul 11 00:28:22 2024 ] 	Batch(5900/7879) done. Loss: 0.0047  lr:0.000001
[ Thu Jul 11 00:28:45 2024 ] 
Training: Epoch [8/120], Step [5999], Loss: 0.018194597214460373, Training Accuracy: 96.21875
[ Thu Jul 11 00:28:45 2024 ] 	Batch(6000/7879) done. Loss: 0.5810  lr:0.000001
[ Thu Jul 11 00:29:09 2024 ] 	Batch(6100/7879) done. Loss: 0.0430  lr:0.000001
[ Thu Jul 11 00:29:32 2024 ] 	Batch(6200/7879) done. Loss: 0.0202  lr:0.000001
[ Thu Jul 11 00:29:55 2024 ] 	Batch(6300/7879) done. Loss: 0.1575  lr:0.000001
[ Thu Jul 11 00:30:19 2024 ] 	Batch(6400/7879) done. Loss: 0.0612  lr:0.000001
[ Thu Jul 11 00:30:41 2024 ] 
Training: Epoch [8/120], Step [6499], Loss: 0.3885836601257324, Training Accuracy: 96.19423076923077
[ Thu Jul 11 00:30:41 2024 ] 	Batch(6500/7879) done. Loss: 0.0878  lr:0.000001
[ Thu Jul 11 00:31:04 2024 ] 	Batch(6600/7879) done. Loss: 0.0691  lr:0.000001
[ Thu Jul 11 00:31:27 2024 ] 	Batch(6700/7879) done. Loss: 0.0508  lr:0.000001
[ Thu Jul 11 00:31:49 2024 ] 	Batch(6800/7879) done. Loss: 0.0197  lr:0.000001
[ Thu Jul 11 00:32:12 2024 ] 	Batch(6900/7879) done. Loss: 0.3583  lr:0.000001
[ Thu Jul 11 00:32:34 2024 ] 
Training: Epoch [8/120], Step [6999], Loss: 0.044751688838005066, Training Accuracy: 96.19107142857143
[ Thu Jul 11 00:32:35 2024 ] 	Batch(7000/7879) done. Loss: 0.0577  lr:0.000001
[ Thu Jul 11 00:32:57 2024 ] 	Batch(7100/7879) done. Loss: 0.0911  lr:0.000001
[ Thu Jul 11 00:33:20 2024 ] 	Batch(7200/7879) done. Loss: 0.3981  lr:0.000001
[ Thu Jul 11 00:33:42 2024 ] 	Batch(7300/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul 11 00:34:05 2024 ] 	Batch(7400/7879) done. Loss: 0.0068  lr:0.000001
[ Thu Jul 11 00:34:27 2024 ] 
Training: Epoch [8/120], Step [7499], Loss: 0.029130132868885994, Training Accuracy: 96.22
[ Thu Jul 11 00:34:28 2024 ] 	Batch(7500/7879) done. Loss: 0.2328  lr:0.000001
[ Thu Jul 11 00:34:50 2024 ] 	Batch(7600/7879) done. Loss: 0.0121  lr:0.000001
[ Thu Jul 11 00:35:13 2024 ] 	Batch(7700/7879) done. Loss: 0.1206  lr:0.000001
[ Thu Jul 11 00:35:35 2024 ] 	Batch(7800/7879) done. Loss: 0.0314  lr:0.000001
[ Thu Jul 11 00:35:53 2024 ] 	Mean training loss: 0.1425.
[ Thu Jul 11 00:35:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 00:35:53 2024 ] Training epoch: 10
[ Thu Jul 11 00:35:54 2024 ] 	Batch(0/7879) done. Loss: 0.3734  lr:0.000001
[ Thu Jul 11 00:36:16 2024 ] 	Batch(100/7879) done. Loss: 0.1440  lr:0.000001
[ Thu Jul 11 00:36:39 2024 ] 	Batch(200/7879) done. Loss: 0.1149  lr:0.000001
[ Thu Jul 11 00:37:02 2024 ] 	Batch(300/7879) done. Loss: 0.0520  lr:0.000001
[ Thu Jul 11 00:37:24 2024 ] 	Batch(400/7879) done. Loss: 0.7575  lr:0.000001
[ Thu Jul 11 00:37:47 2024 ] 
Training: Epoch [9/120], Step [499], Loss: 0.05274619534611702, Training Accuracy: 96.275
[ Thu Jul 11 00:37:47 2024 ] 	Batch(500/7879) done. Loss: 0.0464  lr:0.000001
[ Thu Jul 11 00:38:10 2024 ] 	Batch(600/7879) done. Loss: 0.2628  lr:0.000001
[ Thu Jul 11 00:38:32 2024 ] 	Batch(700/7879) done. Loss: 0.2484  lr:0.000001
[ Thu Jul 11 00:38:55 2024 ] 	Batch(800/7879) done. Loss: 0.0942  lr:0.000001
[ Thu Jul 11 00:39:18 2024 ] 	Batch(900/7879) done. Loss: 0.0956  lr:0.000001
[ Thu Jul 11 00:39:40 2024 ] 
Training: Epoch [9/120], Step [999], Loss: 0.1061549186706543, Training Accuracy: 96.0625
[ Thu Jul 11 00:39:40 2024 ] 	Batch(1000/7879) done. Loss: 0.0470  lr:0.000001
[ Thu Jul 11 00:40:03 2024 ] 	Batch(1100/7879) done. Loss: 0.0255  lr:0.000001
[ Thu Jul 11 00:40:26 2024 ] 	Batch(1200/7879) done. Loss: 0.0412  lr:0.000001
[ Thu Jul 11 00:40:48 2024 ] 	Batch(1300/7879) done. Loss: 0.0873  lr:0.000001
[ Thu Jul 11 00:41:11 2024 ] 	Batch(1400/7879) done. Loss: 0.3082  lr:0.000001
[ Thu Jul 11 00:41:33 2024 ] 
Training: Epoch [9/120], Step [1499], Loss: 0.028487438336014748, Training Accuracy: 96.2
[ Thu Jul 11 00:41:33 2024 ] 	Batch(1500/7879) done. Loss: 0.0877  lr:0.000001
[ Thu Jul 11 00:41:56 2024 ] 	Batch(1600/7879) done. Loss: 0.1628  lr:0.000001
[ Thu Jul 11 00:42:19 2024 ] 	Batch(1700/7879) done. Loss: 0.2070  lr:0.000001
[ Thu Jul 11 00:42:41 2024 ] 	Batch(1800/7879) done. Loss: 0.0099  lr:0.000001
[ Thu Jul 11 00:43:04 2024 ] 	Batch(1900/7879) done. Loss: 0.0464  lr:0.000001
[ Thu Jul 11 00:43:26 2024 ] 
Training: Epoch [9/120], Step [1999], Loss: 0.3743661344051361, Training Accuracy: 96.11874999999999
[ Thu Jul 11 00:43:26 2024 ] 	Batch(2000/7879) done. Loss: 0.0521  lr:0.000001
[ Thu Jul 11 00:43:49 2024 ] 	Batch(2100/7879) done. Loss: 0.1558  lr:0.000001
[ Thu Jul 11 00:44:12 2024 ] 	Batch(2200/7879) done. Loss: 0.0069  lr:0.000001
[ Thu Jul 11 00:44:34 2024 ] 	Batch(2300/7879) done. Loss: 0.0893  lr:0.000001
[ Thu Jul 11 00:44:57 2024 ] 	Batch(2400/7879) done. Loss: 0.1490  lr:0.000001
[ Thu Jul 11 00:45:19 2024 ] 
Training: Epoch [9/120], Step [2499], Loss: 0.016085399314761162, Training Accuracy: 96.175
[ Thu Jul 11 00:45:19 2024 ] 	Batch(2500/7879) done. Loss: 0.2943  lr:0.000001
[ Thu Jul 11 00:45:42 2024 ] 	Batch(2600/7879) done. Loss: 0.0486  lr:0.000001
[ Thu Jul 11 00:46:05 2024 ] 	Batch(2700/7879) done. Loss: 0.0065  lr:0.000001
[ Thu Jul 11 00:46:27 2024 ] 	Batch(2800/7879) done. Loss: 0.0628  lr:0.000001
[ Thu Jul 11 00:46:50 2024 ] 	Batch(2900/7879) done. Loss: 0.0985  lr:0.000001
[ Thu Jul 11 00:47:12 2024 ] 
Training: Epoch [9/120], Step [2999], Loss: 0.058183491230010986, Training Accuracy: 96.1125
[ Thu Jul 11 00:47:12 2024 ] 	Batch(3000/7879) done. Loss: 0.0056  lr:0.000001
[ Thu Jul 11 00:47:35 2024 ] 	Batch(3100/7879) done. Loss: 0.0169  lr:0.000001
[ Thu Jul 11 00:47:58 2024 ] 	Batch(3200/7879) done. Loss: 0.1322  lr:0.000001
[ Thu Jul 11 00:48:20 2024 ] 	Batch(3300/7879) done. Loss: 0.1818  lr:0.000001
[ Thu Jul 11 00:48:43 2024 ] 	Batch(3400/7879) done. Loss: 0.0792  lr:0.000001
[ Thu Jul 11 00:49:05 2024 ] 
Training: Epoch [9/120], Step [3499], Loss: 0.12011142075061798, Training Accuracy: 96.05
[ Thu Jul 11 00:49:05 2024 ] 	Batch(3500/7879) done. Loss: 0.3970  lr:0.000001
[ Thu Jul 11 00:49:28 2024 ] 	Batch(3600/7879) done. Loss: 0.0333  lr:0.000001
[ Thu Jul 11 00:49:51 2024 ] 	Batch(3700/7879) done. Loss: 0.0188  lr:0.000001
[ Thu Jul 11 00:50:14 2024 ] 	Batch(3800/7879) done. Loss: 0.0921  lr:0.000001
[ Thu Jul 11 00:50:36 2024 ] 	Batch(3900/7879) done. Loss: 0.0778  lr:0.000001
[ Thu Jul 11 00:50:59 2024 ] 
Training: Epoch [9/120], Step [3999], Loss: 0.07215546071529388, Training Accuracy: 96.06875000000001
[ Thu Jul 11 00:50:59 2024 ] 	Batch(4000/7879) done. Loss: 0.0357  lr:0.000001
[ Thu Jul 11 00:51:22 2024 ] 	Batch(4100/7879) done. Loss: 0.3017  lr:0.000001
[ Thu Jul 11 00:51:44 2024 ] 	Batch(4200/7879) done. Loss: 0.1260  lr:0.000001
[ Thu Jul 11 00:52:07 2024 ] 	Batch(4300/7879) done. Loss: 0.0318  lr:0.000001
[ Thu Jul 11 00:52:30 2024 ] 	Batch(4400/7879) done. Loss: 0.1075  lr:0.000001
[ Thu Jul 11 00:52:52 2024 ] 
Training: Epoch [9/120], Step [4499], Loss: 0.24749912321567535, Training Accuracy: 96.09722222222223
[ Thu Jul 11 00:52:52 2024 ] 	Batch(4500/7879) done. Loss: 0.0336  lr:0.000001
[ Thu Jul 11 00:53:15 2024 ] 	Batch(4600/7879) done. Loss: 0.3414  lr:0.000001
[ Thu Jul 11 00:53:38 2024 ] 	Batch(4700/7879) done. Loss: 0.0190  lr:0.000001
[ Thu Jul 11 00:54:00 2024 ] 	Batch(4800/7879) done. Loss: 0.1092  lr:0.000001
[ Thu Jul 11 00:54:23 2024 ] 	Batch(4900/7879) done. Loss: 0.0064  lr:0.000001
[ Thu Jul 11 00:54:45 2024 ] 
Training: Epoch [9/120], Step [4999], Loss: 0.07400378584861755, Training Accuracy: 96.0775
[ Thu Jul 11 00:54:45 2024 ] 	Batch(5000/7879) done. Loss: 0.2224  lr:0.000001
[ Thu Jul 11 00:55:08 2024 ] 	Batch(5100/7879) done. Loss: 0.0609  lr:0.000001
[ Thu Jul 11 00:55:31 2024 ] 	Batch(5200/7879) done. Loss: 0.0391  lr:0.000001
[ Thu Jul 11 00:55:53 2024 ] 	Batch(5300/7879) done. Loss: 0.1835  lr:0.000001
[ Thu Jul 11 00:56:16 2024 ] 	Batch(5400/7879) done. Loss: 0.2023  lr:0.000001
[ Thu Jul 11 00:56:38 2024 ] 
Training: Epoch [9/120], Step [5499], Loss: 0.2506754696369171, Training Accuracy: 96.075
[ Thu Jul 11 00:56:39 2024 ] 	Batch(5500/7879) done. Loss: 0.1451  lr:0.000001
[ Thu Jul 11 00:57:02 2024 ] 	Batch(5600/7879) done. Loss: 0.0674  lr:0.000001
[ Thu Jul 11 00:57:24 2024 ] 	Batch(5700/7879) done. Loss: 0.1293  lr:0.000001
[ Thu Jul 11 00:57:47 2024 ] 	Batch(5800/7879) done. Loss: 0.4333  lr:0.000001
[ Thu Jul 11 00:58:09 2024 ] 	Batch(5900/7879) done. Loss: 0.0587  lr:0.000001
[ Thu Jul 11 00:58:32 2024 ] 
Training: Epoch [9/120], Step [5999], Loss: 0.10717710107564926, Training Accuracy: 96.05833333333334
[ Thu Jul 11 00:58:32 2024 ] 	Batch(6000/7879) done. Loss: 0.0797  lr:0.000001
[ Thu Jul 11 00:58:55 2024 ] 	Batch(6100/7879) done. Loss: 0.3541  lr:0.000001
[ Thu Jul 11 00:59:18 2024 ] 	Batch(6200/7879) done. Loss: 0.0092  lr:0.000001
[ Thu Jul 11 00:59:40 2024 ] 	Batch(6300/7879) done. Loss: 0.1710  lr:0.000001
[ Thu Jul 11 01:00:04 2024 ] 	Batch(6400/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul 11 01:00:27 2024 ] 
Training: Epoch [9/120], Step [6499], Loss: 0.17132000625133514, Training Accuracy: 96.1
[ Thu Jul 11 01:00:27 2024 ] 	Batch(6500/7879) done. Loss: 0.0192  lr:0.000001
[ Thu Jul 11 01:00:50 2024 ] 	Batch(6600/7879) done. Loss: 0.0332  lr:0.000001
[ Thu Jul 11 01:01:13 2024 ] 	Batch(6700/7879) done. Loss: 0.0309  lr:0.000001
[ Thu Jul 11 01:01:37 2024 ] 	Batch(6800/7879) done. Loss: 0.1203  lr:0.000001
[ Thu Jul 11 01:02:00 2024 ] 	Batch(6900/7879) done. Loss: 0.0574  lr:0.000001
[ Thu Jul 11 01:02:23 2024 ] 
Training: Epoch [9/120], Step [6999], Loss: 0.23271974921226501, Training Accuracy: 96.11071428571428
[ Thu Jul 11 01:02:23 2024 ] 	Batch(7000/7879) done. Loss: 0.1099  lr:0.000001
[ Thu Jul 11 01:02:46 2024 ] 	Batch(7100/7879) done. Loss: 0.0015  lr:0.000001
[ Thu Jul 11 01:03:09 2024 ] 	Batch(7200/7879) done. Loss: 0.0171  lr:0.000001
[ Thu Jul 11 01:03:31 2024 ] 	Batch(7300/7879) done. Loss: 0.0257  lr:0.000001
[ Thu Jul 11 01:03:54 2024 ] 	Batch(7400/7879) done. Loss: 0.1069  lr:0.000001
[ Thu Jul 11 01:04:17 2024 ] 
Training: Epoch [9/120], Step [7499], Loss: 0.3670986294746399, Training Accuracy: 96.135
[ Thu Jul 11 01:04:17 2024 ] 	Batch(7500/7879) done. Loss: 0.0064  lr:0.000001
[ Thu Jul 11 01:04:41 2024 ] 	Batch(7600/7879) done. Loss: 0.0395  lr:0.000001
[ Thu Jul 11 01:05:04 2024 ] 	Batch(7700/7879) done. Loss: 0.2608  lr:0.000001
[ Thu Jul 11 01:05:27 2024 ] 	Batch(7800/7879) done. Loss: 0.0227  lr:0.000001
[ Thu Jul 11 01:05:45 2024 ] 	Mean training loss: 0.1443.
[ Thu Jul 11 01:05:45 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 01:05:45 2024 ] Eval epoch: 10
[ Thu Jul 11 01:11:42 2024 ] 	Mean val loss of 6365 batches: 1.0055174639569102.
[ Thu Jul 11 01:11:42 2024 ] 
Validation: Epoch [9/120], Samples [39674.0/50919], Loss: 0.4194847643375397, Validation Accuracy: 77.91590565407805
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 1 : 197 / 275 = 71 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 2 : 232 / 273 = 84 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 3 : 228 / 273 = 83 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 4 : 222 / 275 = 80 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 5 : 231 / 275 = 84 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 6 : 214 / 275 = 77 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 7 : 253 / 273 = 92 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 8 : 265 / 273 = 97 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 9 : 197 / 273 = 72 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 10 : 121 / 273 = 44 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 11 : 153 / 272 = 56 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 12 : 219 / 271 = 80 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 13 : 265 / 275 = 96 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 14 : 262 / 276 = 94 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 15 : 217 / 273 = 79 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 16 : 164 / 274 = 59 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 17 : 237 / 273 = 86 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 18 : 232 / 274 = 84 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 19 : 245 / 272 = 90 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 20 : 253 / 273 = 92 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 21 : 225 / 274 = 82 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 22 : 247 / 274 = 90 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 23 : 249 / 276 = 90 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 24 : 242 / 274 = 88 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 25 : 261 / 275 = 94 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 26 : 269 / 276 = 97 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 27 : 230 / 275 = 83 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 28 : 168 / 275 = 61 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 29 : 147 / 275 = 53 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 30 : 168 / 276 = 60 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 31 : 223 / 276 = 80 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 32 : 235 / 276 = 85 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 33 : 230 / 276 = 83 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 34 : 243 / 276 = 88 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 35 : 239 / 275 = 86 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 36 : 217 / 276 = 78 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 37 : 250 / 276 = 90 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 38 : 243 / 276 = 88 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 39 : 235 / 276 = 85 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 40 : 203 / 276 = 73 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 41 : 262 / 276 = 94 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 42 : 250 / 275 = 90 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 43 : 189 / 276 = 68 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 44 : 253 / 276 = 91 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 45 : 260 / 276 = 94 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 46 : 233 / 276 = 84 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 47 : 207 / 275 = 75 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 48 : 222 / 275 = 80 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 49 : 204 / 274 = 74 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 50 : 240 / 276 = 86 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 51 : 255 / 276 = 92 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 52 : 238 / 276 = 86 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 53 : 234 / 276 = 84 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 54 : 257 / 274 = 93 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 55 : 237 / 276 = 85 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 56 : 245 / 275 = 89 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 57 : 269 / 276 = 97 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 58 : 263 / 273 = 96 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 59 : 251 / 276 = 90 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 60 : 470 / 561 = 83 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 61 : 491 / 566 = 86 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 62 : 418 / 572 = 73 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 63 : 481 / 570 = 84 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 64 : 419 / 574 = 72 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 65 : 504 / 573 = 87 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 66 : 409 / 573 = 71 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 67 : 399 / 575 = 69 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 68 : 374 / 575 = 65 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 69 : 474 / 575 = 82 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 70 : 231 / 575 = 40 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 71 : 238 / 575 = 41 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 72 : 111 / 571 = 19 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 73 : 259 / 570 = 45 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 74 : 368 / 569 = 64 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 75 : 357 / 573 = 62 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 76 : 366 / 574 = 63 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 77 : 386 / 573 = 67 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 78 : 452 / 575 = 78 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 79 : 546 / 574 = 95 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 80 : 462 / 573 = 80 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 81 : 360 / 575 = 62 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 82 : 355 / 575 = 61 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 83 : 282 / 572 = 49 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 84 : 438 / 574 = 76 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 85 : 386 / 574 = 67 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 86 : 508 / 575 = 88 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 87 : 500 / 576 = 86 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 88 : 404 / 575 = 70 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 89 : 441 / 576 = 76 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 90 : 263 / 574 = 45 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 91 : 456 / 568 = 80 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 92 : 427 / 576 = 74 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 93 : 377 / 573 = 65 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 94 : 513 / 574 = 89 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 95 : 538 / 575 = 93 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 96 : 556 / 575 = 96 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 97 : 552 / 574 = 96 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 98 : 534 / 575 = 92 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 99 : 533 / 574 = 92 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 100 : 451 / 574 = 78 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 101 : 532 / 574 = 92 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 102 : 357 / 575 = 62 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 103 : 507 / 576 = 88 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 104 : 306 / 575 = 53 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 105 : 285 / 575 = 49 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 106 : 326 / 576 = 56 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 107 : 508 / 576 = 88 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 108 : 481 / 575 = 83 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 109 : 417 / 575 = 72 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 110 : 511 / 575 = 88 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 111 : 545 / 576 = 94 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 112 : 546 / 575 = 94 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 113 : 524 / 576 = 90 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 114 : 515 / 576 = 89 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 115 : 536 / 576 = 93 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 116 : 471 / 575 = 81 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 117 : 494 / 575 = 85 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 118 : 473 / 575 = 82 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 119 : 507 / 576 = 88 %
[ Thu Jul 11 01:11:42 2024 ] Accuracy of 120 : 244 / 274 = 89 %
[ Thu Jul 11 01:11:42 2024 ] Training epoch: 11
[ Thu Jul 11 01:11:42 2024 ] 	Batch(0/7879) done. Loss: 0.3174  lr:0.000001
[ Thu Jul 11 01:12:05 2024 ] 	Batch(100/7879) done. Loss: 0.4703  lr:0.000001
[ Thu Jul 11 01:12:29 2024 ] 	Batch(200/7879) done. Loss: 0.0660  lr:0.000001
[ Thu Jul 11 01:12:52 2024 ] 	Batch(300/7879) done. Loss: 0.0955  lr:0.000001
[ Thu Jul 11 01:13:15 2024 ] 	Batch(400/7879) done. Loss: 0.0371  lr:0.000001
[ Thu Jul 11 01:13:38 2024 ] 
Training: Epoch [10/120], Step [499], Loss: 0.09517474472522736, Training Accuracy: 96.375
[ Thu Jul 11 01:13:38 2024 ] 	Batch(500/7879) done. Loss: 0.0079  lr:0.000001
[ Thu Jul 11 01:14:01 2024 ] 	Batch(600/7879) done. Loss: 0.0142  lr:0.000001
[ Thu Jul 11 01:14:24 2024 ] 	Batch(700/7879) done. Loss: 0.0351  lr:0.000001
[ Thu Jul 11 01:14:48 2024 ] 	Batch(800/7879) done. Loss: 0.0600  lr:0.000001
[ Thu Jul 11 01:15:11 2024 ] 	Batch(900/7879) done. Loss: 0.2785  lr:0.000001
[ Thu Jul 11 01:15:34 2024 ] 
Training: Epoch [10/120], Step [999], Loss: 0.18073834478855133, Training Accuracy: 96.1
[ Thu Jul 11 01:15:34 2024 ] 	Batch(1000/7879) done. Loss: 0.0030  lr:0.000001
[ Thu Jul 11 01:15:57 2024 ] 	Batch(1100/7879) done. Loss: 0.2573  lr:0.000001
[ Thu Jul 11 01:16:20 2024 ] 	Batch(1200/7879) done. Loss: 0.2464  lr:0.000001
[ Thu Jul 11 01:16:44 2024 ] 	Batch(1300/7879) done. Loss: 0.5066  lr:0.000001
[ Thu Jul 11 01:17:07 2024 ] 	Batch(1400/7879) done. Loss: 0.0126  lr:0.000001
[ Thu Jul 11 01:17:30 2024 ] 
Training: Epoch [10/120], Step [1499], Loss: 0.04194934666156769, Training Accuracy: 96.05833333333334
[ Thu Jul 11 01:17:30 2024 ] 	Batch(1500/7879) done. Loss: 0.0142  lr:0.000001
[ Thu Jul 11 01:17:53 2024 ] 	Batch(1600/7879) done. Loss: 0.1985  lr:0.000001
[ Thu Jul 11 01:18:16 2024 ] 	Batch(1700/7879) done. Loss: 0.0175  lr:0.000001
[ Thu Jul 11 01:18:40 2024 ] 	Batch(1800/7879) done. Loss: 0.4922  lr:0.000001
[ Thu Jul 11 01:19:03 2024 ] 	Batch(1900/7879) done. Loss: 0.0085  lr:0.000001
[ Thu Jul 11 01:19:26 2024 ] 
Training: Epoch [10/120], Step [1999], Loss: 0.016830040141940117, Training Accuracy: 96.15
[ Thu Jul 11 01:19:26 2024 ] 	Batch(2000/7879) done. Loss: 0.0165  lr:0.000001
[ Thu Jul 11 01:19:49 2024 ] 	Batch(2100/7879) done. Loss: 0.2240  lr:0.000001
[ Thu Jul 11 01:20:12 2024 ] 	Batch(2200/7879) done. Loss: 0.0111  lr:0.000001
[ Thu Jul 11 01:20:36 2024 ] 	Batch(2300/7879) done. Loss: 0.0686  lr:0.000001
[ Thu Jul 11 01:20:59 2024 ] 	Batch(2400/7879) done. Loss: 0.2072  lr:0.000001
[ Thu Jul 11 01:21:22 2024 ] 
Training: Epoch [10/120], Step [2499], Loss: 0.05602782592177391, Training Accuracy: 96.22500000000001
[ Thu Jul 11 01:21:22 2024 ] 	Batch(2500/7879) done. Loss: 0.0277  lr:0.000001
[ Thu Jul 11 01:21:45 2024 ] 	Batch(2600/7879) done. Loss: 0.0491  lr:0.000001
[ Thu Jul 11 01:22:08 2024 ] 	Batch(2700/7879) done. Loss: 0.1418  lr:0.000001
[ Thu Jul 11 01:22:31 2024 ] 	Batch(2800/7879) done. Loss: 0.0254  lr:0.000001
[ Thu Jul 11 01:22:54 2024 ] 	Batch(2900/7879) done. Loss: 0.0277  lr:0.000001
[ Thu Jul 11 01:23:16 2024 ] 
Training: Epoch [10/120], Step [2999], Loss: 0.09640289098024368, Training Accuracy: 96.26249999999999
[ Thu Jul 11 01:23:17 2024 ] 	Batch(3000/7879) done. Loss: 0.1890  lr:0.000001
[ Thu Jul 11 01:23:39 2024 ] 	Batch(3100/7879) done. Loss: 0.2700  lr:0.000001
[ Thu Jul 11 01:24:02 2024 ] 	Batch(3200/7879) done. Loss: 0.1190  lr:0.000001
[ Thu Jul 11 01:24:25 2024 ] 	Batch(3300/7879) done. Loss: 0.0359  lr:0.000001
[ Thu Jul 11 01:24:48 2024 ] 	Batch(3400/7879) done. Loss: 0.1762  lr:0.000001
[ Thu Jul 11 01:25:10 2024 ] 
Training: Epoch [10/120], Step [3499], Loss: 0.01503167487680912, Training Accuracy: 96.24285714285715
[ Thu Jul 11 01:25:10 2024 ] 	Batch(3500/7879) done. Loss: 0.0151  lr:0.000001
[ Thu Jul 11 01:25:33 2024 ] 	Batch(3600/7879) done. Loss: 0.0238  lr:0.000001
[ Thu Jul 11 01:25:56 2024 ] 	Batch(3700/7879) done. Loss: 0.0193  lr:0.000001
[ Thu Jul 11 01:26:18 2024 ] 	Batch(3800/7879) done. Loss: 0.0501  lr:0.000001
[ Thu Jul 11 01:26:41 2024 ] 	Batch(3900/7879) done. Loss: 0.0327  lr:0.000001
[ Thu Jul 11 01:27:04 2024 ] 
Training: Epoch [10/120], Step [3999], Loss: 0.06491420418024063, Training Accuracy: 96.2125
[ Thu Jul 11 01:27:04 2024 ] 	Batch(4000/7879) done. Loss: 0.2662  lr:0.000001
[ Thu Jul 11 01:27:27 2024 ] 	Batch(4100/7879) done. Loss: 0.1104  lr:0.000001
[ Thu Jul 11 01:27:50 2024 ] 	Batch(4200/7879) done. Loss: 0.2839  lr:0.000001
[ Thu Jul 11 01:28:12 2024 ] 	Batch(4300/7879) done. Loss: 0.3348  lr:0.000001
[ Thu Jul 11 01:28:36 2024 ] 	Batch(4400/7879) done. Loss: 0.0437  lr:0.000001
[ Thu Jul 11 01:28:58 2024 ] 
Training: Epoch [10/120], Step [4499], Loss: 0.10656269639730453, Training Accuracy: 96.13333333333334
[ Thu Jul 11 01:28:58 2024 ] 	Batch(4500/7879) done. Loss: 0.1372  lr:0.000001
[ Thu Jul 11 01:29:21 2024 ] 	Batch(4600/7879) done. Loss: 0.0568  lr:0.000001
[ Thu Jul 11 01:29:44 2024 ] 	Batch(4700/7879) done. Loss: 0.0026  lr:0.000001
[ Thu Jul 11 01:30:07 2024 ] 	Batch(4800/7879) done. Loss: 0.0103  lr:0.000001
[ Thu Jul 11 01:30:29 2024 ] 	Batch(4900/7879) done. Loss: 0.0415  lr:0.000001
[ Thu Jul 11 01:30:52 2024 ] 
Training: Epoch [10/120], Step [4999], Loss: 0.060435909777879715, Training Accuracy: 96.08250000000001
[ Thu Jul 11 01:30:52 2024 ] 	Batch(5000/7879) done. Loss: 0.0387  lr:0.000001
[ Thu Jul 11 01:31:15 2024 ] 	Batch(5100/7879) done. Loss: 0.2696  lr:0.000001
[ Thu Jul 11 01:31:38 2024 ] 	Batch(5200/7879) done. Loss: 0.0710  lr:0.000001
[ Thu Jul 11 01:32:00 2024 ] 	Batch(5300/7879) done. Loss: 0.1635  lr:0.000001
[ Thu Jul 11 01:32:23 2024 ] 	Batch(5400/7879) done. Loss: 0.0904  lr:0.000001
[ Thu Jul 11 01:32:46 2024 ] 
Training: Epoch [10/120], Step [5499], Loss: 0.18369382619857788, Training Accuracy: 96.04772727272727
[ Thu Jul 11 01:32:46 2024 ] 	Batch(5500/7879) done. Loss: 0.4743  lr:0.000001
[ Thu Jul 11 01:33:09 2024 ] 	Batch(5600/7879) done. Loss: 0.0247  lr:0.000001
[ Thu Jul 11 01:33:32 2024 ] 	Batch(5700/7879) done. Loss: 0.0472  lr:0.000001
[ Thu Jul 11 01:33:56 2024 ] 	Batch(5800/7879) done. Loss: 0.0151  lr:0.000001
[ Thu Jul 11 01:34:19 2024 ] 	Batch(5900/7879) done. Loss: 0.0148  lr:0.000001
[ Thu Jul 11 01:34:42 2024 ] 
Training: Epoch [10/120], Step [5999], Loss: 0.25484809279441833, Training Accuracy: 96.05833333333334
[ Thu Jul 11 01:34:42 2024 ] 	Batch(6000/7879) done. Loss: 0.0115  lr:0.000001
[ Thu Jul 11 01:35:05 2024 ] 	Batch(6100/7879) done. Loss: 0.4048  lr:0.000001
[ Thu Jul 11 01:35:28 2024 ] 	Batch(6200/7879) done. Loss: 0.1295  lr:0.000001
[ Thu Jul 11 01:35:51 2024 ] 	Batch(6300/7879) done. Loss: 0.0403  lr:0.000001
[ Thu Jul 11 01:36:14 2024 ] 	Batch(6400/7879) done. Loss: 0.2581  lr:0.000001
[ Thu Jul 11 01:36:36 2024 ] 
Training: Epoch [10/120], Step [6499], Loss: 0.06642971187829971, Training Accuracy: 96.07884615384616
[ Thu Jul 11 01:36:36 2024 ] 	Batch(6500/7879) done. Loss: 0.0077  lr:0.000001
[ Thu Jul 11 01:36:59 2024 ] 	Batch(6600/7879) done. Loss: 0.2346  lr:0.000001
[ Thu Jul 11 01:37:22 2024 ] 	Batch(6700/7879) done. Loss: 0.0058  lr:0.000001
[ Thu Jul 11 01:37:45 2024 ] 	Batch(6800/7879) done. Loss: 0.1615  lr:0.000001
[ Thu Jul 11 01:38:08 2024 ] 	Batch(6900/7879) done. Loss: 0.0447  lr:0.000001
[ Thu Jul 11 01:38:31 2024 ] 
Training: Epoch [10/120], Step [6999], Loss: 0.1743442416191101, Training Accuracy: 96.0875
[ Thu Jul 11 01:38:31 2024 ] 	Batch(7000/7879) done. Loss: 0.1985  lr:0.000001
[ Thu Jul 11 01:38:54 2024 ] 	Batch(7100/7879) done. Loss: 0.0700  lr:0.000001
[ Thu Jul 11 01:39:17 2024 ] 	Batch(7200/7879) done. Loss: 0.1453  lr:0.000001
[ Thu Jul 11 01:39:40 2024 ] 	Batch(7300/7879) done. Loss: 0.0341  lr:0.000001
[ Thu Jul 11 01:40:02 2024 ] 	Batch(7400/7879) done. Loss: 0.0436  lr:0.000001
[ Thu Jul 11 01:40:25 2024 ] 
Training: Epoch [10/120], Step [7499], Loss: 0.03344734013080597, Training Accuracy: 96.12666666666667
[ Thu Jul 11 01:40:25 2024 ] 	Batch(7500/7879) done. Loss: 0.2784  lr:0.000001
[ Thu Jul 11 01:40:48 2024 ] 	Batch(7600/7879) done. Loss: 0.0624  lr:0.000001
[ Thu Jul 11 01:41:11 2024 ] 	Batch(7700/7879) done. Loss: 0.4894  lr:0.000001
[ Thu Jul 11 01:41:34 2024 ] 	Batch(7800/7879) done. Loss: 0.0192  lr:0.000001
[ Thu Jul 11 01:41:51 2024 ] 	Mean training loss: 0.1445.
[ Thu Jul 11 01:41:51 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Thu Jul 11 01:41:51 2024 ] Training epoch: 12
[ Thu Jul 11 01:41:52 2024 ] 	Batch(0/7879) done. Loss: 0.1364  lr:0.000001
[ Thu Jul 11 01:42:15 2024 ] 	Batch(100/7879) done. Loss: 0.0537  lr:0.000001
[ Thu Jul 11 01:42:37 2024 ] 	Batch(200/7879) done. Loss: 0.0033  lr:0.000001
[ Thu Jul 11 01:43:00 2024 ] 	Batch(300/7879) done. Loss: 0.1493  lr:0.000001
[ Thu Jul 11 01:43:23 2024 ] 	Batch(400/7879) done. Loss: 0.0113  lr:0.000001
[ Thu Jul 11 01:43:46 2024 ] 
Training: Epoch [11/120], Step [499], Loss: 0.0231337808072567, Training Accuracy: 96.65
[ Thu Jul 11 01:43:46 2024 ] 	Batch(500/7879) done. Loss: 0.1156  lr:0.000001
[ Thu Jul 11 01:44:09 2024 ] 	Batch(600/7879) done. Loss: 0.0315  lr:0.000001
[ Thu Jul 11 01:44:31 2024 ] 	Batch(700/7879) done. Loss: 0.2392  lr:0.000001
[ Thu Jul 11 01:44:54 2024 ] 	Batch(800/7879) done. Loss: 0.1065  lr:0.000001
[ Thu Jul 11 01:45:17 2024 ] 	Batch(900/7879) done. Loss: 0.0251  lr:0.000001
[ Thu Jul 11 01:45:39 2024 ] 
Training: Epoch [11/120], Step [999], Loss: 0.050563812255859375, Training Accuracy: 96.4375
[ Thu Jul 11 01:45:40 2024 ] 	Batch(1000/7879) done. Loss: 0.0409  lr:0.000001
[ Thu Jul 11 01:46:02 2024 ] 	Batch(1100/7879) done. Loss: 0.2208  lr:0.000001
[ Thu Jul 11 01:46:25 2024 ] 	Batch(1200/7879) done. Loss: 0.3160  lr:0.000001
[ Thu Jul 11 01:46:48 2024 ] 	Batch(1300/7879) done. Loss: 0.4018  lr:0.000001
[ Thu Jul 11 01:47:11 2024 ] 	Batch(1400/7879) done. Loss: 0.2872  lr:0.000001
[ Thu Jul 11 01:47:34 2024 ] 
Training: Epoch [11/120], Step [1499], Loss: 0.0023914100602269173, Training Accuracy: 96.20833333333333
[ Thu Jul 11 01:47:34 2024 ] 	Batch(1500/7879) done. Loss: 0.0999  lr:0.000001
[ Thu Jul 11 01:47:58 2024 ] 	Batch(1600/7879) done. Loss: 0.0289  lr:0.000001
[ Thu Jul 11 01:48:21 2024 ] 	Batch(1700/7879) done. Loss: 0.0037  lr:0.000001
[ Thu Jul 11 01:48:44 2024 ] 	Batch(1800/7879) done. Loss: 0.0178  lr:0.000001
[ Thu Jul 11 01:49:07 2024 ] 	Batch(1900/7879) done. Loss: 0.1424  lr:0.000001
[ Thu Jul 11 01:49:30 2024 ] 
Training: Epoch [11/120], Step [1999], Loss: 0.032295629382133484, Training Accuracy: 96.04375
[ Thu Jul 11 01:49:30 2024 ] 	Batch(2000/7879) done. Loss: 0.0070  lr:0.000001
[ Thu Jul 11 01:49:53 2024 ] 	Batch(2100/7879) done. Loss: 0.2506  lr:0.000001
[ Thu Jul 11 01:50:16 2024 ] 	Batch(2200/7879) done. Loss: 0.0220  lr:0.000001
[ Thu Jul 11 01:50:39 2024 ] 	Batch(2300/7879) done. Loss: 0.0722  lr:0.000001
[ Thu Jul 11 01:51:02 2024 ] 	Batch(2400/7879) done. Loss: 0.0479  lr:0.000001
[ Thu Jul 11 01:51:25 2024 ] 
Training: Epoch [11/120], Step [2499], Loss: 0.09753711521625519, Training Accuracy: 96.045
[ Thu Jul 11 01:51:25 2024 ] 	Batch(2500/7879) done. Loss: 0.0049  lr:0.000001
[ Thu Jul 11 01:51:49 2024 ] 	Batch(2600/7879) done. Loss: 0.2091  lr:0.000001
[ Thu Jul 11 01:52:12 2024 ] 	Batch(2700/7879) done. Loss: 0.0556  lr:0.000001
[ Thu Jul 11 01:52:35 2024 ] 	Batch(2800/7879) done. Loss: 0.0240  lr:0.000001
[ Thu Jul 11 01:52:58 2024 ] 	Batch(2900/7879) done. Loss: 0.0837  lr:0.000001
[ Thu Jul 11 01:53:21 2024 ] 
Training: Epoch [11/120], Step [2999], Loss: 0.06054357811808586, Training Accuracy: 96.075
[ Thu Jul 11 01:53:21 2024 ] 	Batch(3000/7879) done. Loss: 0.0675  lr:0.000001
[ Thu Jul 11 01:53:44 2024 ] 	Batch(3100/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul 11 01:54:07 2024 ] 	Batch(3200/7879) done. Loss: 0.0288  lr:0.000001
[ Thu Jul 11 01:54:31 2024 ] 	Batch(3300/7879) done. Loss: 0.3994  lr:0.000001
[ Thu Jul 11 01:54:54 2024 ] 	Batch(3400/7879) done. Loss: 0.0061  lr:0.000001
[ Thu Jul 11 01:55:18 2024 ] 
Training: Epoch [11/120], Step [3499], Loss: 0.1437787264585495, Training Accuracy: 96.07142857142857
[ Thu Jul 11 01:55:18 2024 ] 	Batch(3500/7879) done. Loss: 0.2691  lr:0.000001
[ Thu Jul 11 01:55:42 2024 ] 	Batch(3600/7879) done. Loss: 0.0088  lr:0.000001
[ Thu Jul 11 01:56:05 2024 ] 	Batch(3700/7879) done. Loss: 0.0322  lr:0.000001
[ Thu Jul 11 01:56:28 2024 ] 	Batch(3800/7879) done. Loss: 0.0198  lr:0.000001
[ Thu Jul 11 01:56:51 2024 ] 	Batch(3900/7879) done. Loss: 0.0502  lr:0.000001
[ Thu Jul 11 01:57:14 2024 ] 
Training: Epoch [11/120], Step [3999], Loss: 0.1300845891237259, Training Accuracy: 96.0875
[ Thu Jul 11 01:57:14 2024 ] 	Batch(4000/7879) done. Loss: 0.1611  lr:0.000001
[ Thu Jul 11 01:57:37 2024 ] 	Batch(4100/7879) done. Loss: 0.6552  lr:0.000001
[ Thu Jul 11 01:58:00 2024 ] 	Batch(4200/7879) done. Loss: 0.0499  lr:0.000001
[ Thu Jul 11 01:58:23 2024 ] 	Batch(4300/7879) done. Loss: 0.0322  lr:0.000001
[ Thu Jul 11 01:58:46 2024 ] 	Batch(4400/7879) done. Loss: 0.0724  lr:0.000001
[ Thu Jul 11 01:59:09 2024 ] 
Training: Epoch [11/120], Step [4499], Loss: 0.4724368155002594, Training Accuracy: 96.09722222222223
[ Thu Jul 11 01:59:09 2024 ] 	Batch(4500/7879) done. Loss: 0.0034  lr:0.000001
[ Thu Jul 11 01:59:32 2024 ] 	Batch(4600/7879) done. Loss: 0.1272  lr:0.000001
[ Thu Jul 11 01:59:55 2024 ] 	Batch(4700/7879) done. Loss: 0.2052  lr:0.000001
[ Thu Jul 11 02:00:19 2024 ] 	Batch(4800/7879) done. Loss: 0.1110  lr:0.000001
[ Thu Jul 11 02:00:42 2024 ] 	Batch(4900/7879) done. Loss: 0.0138  lr:0.000001
[ Thu Jul 11 02:01:04 2024 ] 
Training: Epoch [11/120], Step [4999], Loss: 0.12726153433322906, Training Accuracy: 96.045
[ Thu Jul 11 02:01:05 2024 ] 	Batch(5000/7879) done. Loss: 0.0193  lr:0.000001
[ Thu Jul 11 02:01:28 2024 ] 	Batch(5100/7879) done. Loss: 0.0398  lr:0.000001
[ Thu Jul 11 02:01:51 2024 ] 	Batch(5200/7879) done. Loss: 0.0757  lr:0.000001
[ Thu Jul 11 02:02:15 2024 ] 	Batch(5300/7879) done. Loss: 0.1375  lr:0.000001
[ Thu Jul 11 02:02:39 2024 ] 	Batch(5400/7879) done. Loss: 0.3071  lr:0.000001
[ Thu Jul 11 02:03:02 2024 ] 
Training: Epoch [11/120], Step [5499], Loss: 0.04157286882400513, Training Accuracy: 96.03863636363637
[ Thu Jul 11 02:03:02 2024 ] 	Batch(5500/7879) done. Loss: 0.3610  lr:0.000001
[ Thu Jul 11 02:03:26 2024 ] 	Batch(5600/7879) done. Loss: 0.1552  lr:0.000001
[ Thu Jul 11 02:03:50 2024 ] 	Batch(5700/7879) done. Loss: 0.2465  lr:0.000001
[ Thu Jul 11 02:04:14 2024 ] 	Batch(5800/7879) done. Loss: 0.0528  lr:0.000001
[ Thu Jul 11 02:04:38 2024 ] 	Batch(5900/7879) done. Loss: 0.0210  lr:0.000001
[ Thu Jul 11 02:05:01 2024 ] 
Training: Epoch [11/120], Step [5999], Loss: 0.14614027738571167, Training Accuracy: 96.05
[ Thu Jul 11 02:05:01 2024 ] 	Batch(6000/7879) done. Loss: 0.1821  lr:0.000001
[ Thu Jul 11 02:05:24 2024 ] 	Batch(6100/7879) done. Loss: 0.1077  lr:0.000001
[ Thu Jul 11 02:05:47 2024 ] 	Batch(6200/7879) done. Loss: 0.1523  lr:0.000001
[ Thu Jul 11 02:06:10 2024 ] 	Batch(6300/7879) done. Loss: 0.0225  lr:0.000001
[ Thu Jul 11 02:06:33 2024 ] 	Batch(6400/7879) done. Loss: 0.0257  lr:0.000001
[ Thu Jul 11 02:06:55 2024 ] 
Training: Epoch [11/120], Step [6499], Loss: 0.30716684460639954, Training Accuracy: 96.05384615384615
[ Thu Jul 11 02:06:55 2024 ] 	Batch(6500/7879) done. Loss: 0.0352  lr:0.000001
[ Thu Jul 11 02:07:18 2024 ] 	Batch(6600/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul 11 02:07:40 2024 ] 	Batch(6700/7879) done. Loss: 0.0294  lr:0.000001
[ Thu Jul 11 02:08:03 2024 ] 	Batch(6800/7879) done. Loss: 0.0387  lr:0.000001
[ Thu Jul 11 02:08:26 2024 ] 	Batch(6900/7879) done. Loss: 0.3410  lr:0.000001
[ Thu Jul 11 02:08:48 2024 ] 
Training: Epoch [11/120], Step [6999], Loss: 0.046077437698841095, Training Accuracy: 96.08392857142857
[ Thu Jul 11 02:08:48 2024 ] 	Batch(7000/7879) done. Loss: 0.0761  lr:0.000001
[ Thu Jul 11 02:09:11 2024 ] 	Batch(7100/7879) done. Loss: 0.1623  lr:0.000001
[ Thu Jul 11 02:09:34 2024 ] 	Batch(7200/7879) done. Loss: 0.0775  lr:0.000001
[ Thu Jul 11 02:09:56 2024 ] 	Batch(7300/7879) done. Loss: 0.0520  lr:0.000001
[ Thu Jul 11 02:10:19 2024 ] 	Batch(7400/7879) done. Loss: 0.6169  lr:0.000001
[ Thu Jul 11 02:10:41 2024 ] 
Training: Epoch [11/120], Step [7499], Loss: 0.5109143853187561, Training Accuracy: 96.10499999999999
[ Thu Jul 11 02:10:41 2024 ] 	Batch(7500/7879) done. Loss: 0.5622  lr:0.000001
[ Thu Jul 11 02:11:04 2024 ] 	Batch(7600/7879) done. Loss: 0.1192  lr:0.000001
[ Thu Jul 11 02:11:27 2024 ] 	Batch(7700/7879) done. Loss: 0.0070  lr:0.000001
[ Thu Jul 11 02:11:49 2024 ] 	Batch(7800/7879) done. Loss: 0.0589  lr:0.000001
[ Thu Jul 11 02:12:07 2024 ] 	Mean training loss: 0.1393.
[ Thu Jul 11 02:12:07 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Thu Jul 11 02:12:07 2024 ] Training epoch: 13
[ Thu Jul 11 02:12:08 2024 ] 	Batch(0/7879) done. Loss: 0.0293  lr:0.000001
[ Thu Jul 11 02:12:30 2024 ] 	Batch(100/7879) done. Loss: 0.1945  lr:0.000001
[ Thu Jul 11 02:12:53 2024 ] 	Batch(200/7879) done. Loss: 0.4517  lr:0.000001
[ Thu Jul 11 02:13:15 2024 ] 	Batch(300/7879) done. Loss: 0.1754  lr:0.000001
[ Thu Jul 11 02:13:39 2024 ] 	Batch(400/7879) done. Loss: 0.2390  lr:0.000001
[ Thu Jul 11 02:14:02 2024 ] 
Training: Epoch [12/120], Step [499], Loss: 0.5373702049255371, Training Accuracy: 95.92500000000001
[ Thu Jul 11 02:14:02 2024 ] 	Batch(500/7879) done. Loss: 0.3322  lr:0.000001
[ Thu Jul 11 02:14:25 2024 ] 	Batch(600/7879) done. Loss: 0.1597  lr:0.000001
[ Thu Jul 11 02:14:49 2024 ] 	Batch(700/7879) done. Loss: 0.2009  lr:0.000001
[ Thu Jul 11 02:15:12 2024 ] 	Batch(800/7879) done. Loss: 0.8615  lr:0.000001
[ Thu Jul 11 02:15:36 2024 ] 	Batch(900/7879) done. Loss: 0.0292  lr:0.000001
[ Thu Jul 11 02:15:59 2024 ] 
Training: Epoch [12/120], Step [999], Loss: 0.26518499851226807, Training Accuracy: 95.78750000000001
[ Thu Jul 11 02:15:59 2024 ] 	Batch(1000/7879) done. Loss: 0.2006  lr:0.000001
[ Thu Jul 11 02:16:22 2024 ] 	Batch(1100/7879) done. Loss: 0.0169  lr:0.000001
[ Thu Jul 11 02:16:45 2024 ] 	Batch(1200/7879) done. Loss: 0.1811  lr:0.000001
[ Thu Jul 11 02:17:09 2024 ] 	Batch(1300/7879) done. Loss: 0.0842  lr:0.000001
[ Thu Jul 11 02:17:31 2024 ] 	Batch(1400/7879) done. Loss: 0.4078  lr:0.000001
[ Thu Jul 11 02:17:54 2024 ] 
Training: Epoch [12/120], Step [1499], Loss: 0.060888227075338364, Training Accuracy: 95.95
[ Thu Jul 11 02:17:54 2024 ] 	Batch(1500/7879) done. Loss: 0.2796  lr:0.000001
[ Thu Jul 11 02:18:17 2024 ] 	Batch(1600/7879) done. Loss: 0.0438  lr:0.000001
[ Thu Jul 11 02:18:39 2024 ] 	Batch(1700/7879) done. Loss: 0.1963  lr:0.000001
[ Thu Jul 11 02:19:02 2024 ] 	Batch(1800/7879) done. Loss: 0.0130  lr:0.000001
[ Thu Jul 11 02:19:24 2024 ] 	Batch(1900/7879) done. Loss: 0.0137  lr:0.000001
[ Thu Jul 11 02:19:47 2024 ] 
Training: Epoch [12/120], Step [1999], Loss: 0.24181132018566132, Training Accuracy: 95.95
[ Thu Jul 11 02:19:47 2024 ] 	Batch(2000/7879) done. Loss: 0.3073  lr:0.000001
[ Thu Jul 11 02:20:09 2024 ] 	Batch(2100/7879) done. Loss: 0.2421  lr:0.000001
[ Thu Jul 11 02:20:32 2024 ] 	Batch(2200/7879) done. Loss: 0.0084  lr:0.000001
[ Thu Jul 11 02:20:55 2024 ] 	Batch(2300/7879) done. Loss: 0.4299  lr:0.000001
[ Thu Jul 11 02:21:17 2024 ] 	Batch(2400/7879) done. Loss: 0.1152  lr:0.000001
[ Thu Jul 11 02:21:40 2024 ] 
Training: Epoch [12/120], Step [2499], Loss: 0.12336383014917374, Training Accuracy: 95.98
[ Thu Jul 11 02:21:40 2024 ] 	Batch(2500/7879) done. Loss: 0.5094  lr:0.000001
[ Thu Jul 11 02:22:02 2024 ] 	Batch(2600/7879) done. Loss: 0.3473  lr:0.000001
[ Thu Jul 11 02:22:25 2024 ] 	Batch(2700/7879) done. Loss: 0.3104  lr:0.000001
[ Thu Jul 11 02:22:48 2024 ] 	Batch(2800/7879) done. Loss: 0.0307  lr:0.000001
[ Thu Jul 11 02:23:10 2024 ] 	Batch(2900/7879) done. Loss: 0.0070  lr:0.000001
[ Thu Jul 11 02:23:32 2024 ] 
Training: Epoch [12/120], Step [2999], Loss: 0.2357441782951355, Training Accuracy: 95.95416666666667
[ Thu Jul 11 02:23:33 2024 ] 	Batch(3000/7879) done. Loss: 0.1570  lr:0.000001
[ Thu Jul 11 02:23:55 2024 ] 	Batch(3100/7879) done. Loss: 0.0412  lr:0.000001
[ Thu Jul 11 02:24:18 2024 ] 	Batch(3200/7879) done. Loss: 0.0579  lr:0.000001
[ Thu Jul 11 02:24:41 2024 ] 	Batch(3300/7879) done. Loss: 0.1788  lr:0.000001
[ Thu Jul 11 02:25:03 2024 ] 	Batch(3400/7879) done. Loss: 0.1541  lr:0.000001
[ Thu Jul 11 02:25:25 2024 ] 
Training: Epoch [12/120], Step [3499], Loss: 0.24061083793640137, Training Accuracy: 96.01428571428572
[ Thu Jul 11 02:25:26 2024 ] 	Batch(3500/7879) done. Loss: 0.0913  lr:0.000001
[ Thu Jul 11 02:25:48 2024 ] 	Batch(3600/7879) done. Loss: 0.1247  lr:0.000001
[ Thu Jul 11 02:26:12 2024 ] 	Batch(3700/7879) done. Loss: 0.0030  lr:0.000001
[ Thu Jul 11 02:26:35 2024 ] 	Batch(3800/7879) done. Loss: 0.0344  lr:0.000001
[ Thu Jul 11 02:26:58 2024 ] 	Batch(3900/7879) done. Loss: 0.2079  lr:0.000001
[ Thu Jul 11 02:27:22 2024 ] 
Training: Epoch [12/120], Step [3999], Loss: 0.04848567768931389, Training Accuracy: 96.06875000000001
[ Thu Jul 11 02:27:22 2024 ] 	Batch(4000/7879) done. Loss: 0.1416  lr:0.000001
[ Thu Jul 11 02:27:44 2024 ] 	Batch(4100/7879) done. Loss: 0.0045  lr:0.000001
[ Thu Jul 11 02:28:07 2024 ] 	Batch(4200/7879) done. Loss: 0.3672  lr:0.000001
[ Thu Jul 11 02:28:30 2024 ] 	Batch(4300/7879) done. Loss: 0.0036  lr:0.000001
[ Thu Jul 11 02:28:53 2024 ] 	Batch(4400/7879) done. Loss: 0.0242  lr:0.000001
[ Thu Jul 11 02:29:16 2024 ] 
Training: Epoch [12/120], Step [4499], Loss: 0.015032333321869373, Training Accuracy: 96.0611111111111
[ Thu Jul 11 02:29:16 2024 ] 	Batch(4500/7879) done. Loss: 0.0175  lr:0.000001
[ Thu Jul 11 02:29:40 2024 ] 	Batch(4600/7879) done. Loss: 0.0532  lr:0.000001
[ Thu Jul 11 02:30:03 2024 ] 	Batch(4700/7879) done. Loss: 0.0054  lr:0.000001
[ Thu Jul 11 02:30:26 2024 ] 	Batch(4800/7879) done. Loss: 0.1012  lr:0.000001
[ Thu Jul 11 02:30:50 2024 ] 	Batch(4900/7879) done. Loss: 0.0125  lr:0.000001
[ Thu Jul 11 02:31:12 2024 ] 
Training: Epoch [12/120], Step [4999], Loss: 0.041750963777303696, Training Accuracy: 96.045
[ Thu Jul 11 02:31:13 2024 ] 	Batch(5000/7879) done. Loss: 0.1354  lr:0.000001
[ Thu Jul 11 02:31:35 2024 ] 	Batch(5100/7879) done. Loss: 0.2133  lr:0.000001
[ Thu Jul 11 02:31:59 2024 ] 	Batch(5200/7879) done. Loss: 0.4477  lr:0.000001
[ Thu Jul 11 02:32:22 2024 ] 	Batch(5300/7879) done. Loss: 0.0340  lr:0.000001
[ Thu Jul 11 02:32:44 2024 ] 	Batch(5400/7879) done. Loss: 0.4188  lr:0.000001
[ Thu Jul 11 02:33:07 2024 ] 
Training: Epoch [12/120], Step [5499], Loss: 0.023573409765958786, Training Accuracy: 96.04545454545455
[ Thu Jul 11 02:33:07 2024 ] 	Batch(5500/7879) done. Loss: 0.0581  lr:0.000001
[ Thu Jul 11 02:33:30 2024 ] 	Batch(5600/7879) done. Loss: 0.4021  lr:0.000001
[ Thu Jul 11 02:33:52 2024 ] 	Batch(5700/7879) done. Loss: 0.1731  lr:0.000001
[ Thu Jul 11 02:34:15 2024 ] 	Batch(5800/7879) done. Loss: 0.0133  lr:0.000001
[ Thu Jul 11 02:34:37 2024 ] 	Batch(5900/7879) done. Loss: 0.0090  lr:0.000001
[ Thu Jul 11 02:35:00 2024 ] 
Training: Epoch [12/120], Step [5999], Loss: 0.3743095397949219, Training Accuracy: 96.07083333333334
[ Thu Jul 11 02:35:00 2024 ] 	Batch(6000/7879) done. Loss: 0.0215  lr:0.000001
[ Thu Jul 11 02:35:23 2024 ] 	Batch(6100/7879) done. Loss: 0.1800  lr:0.000001
[ Thu Jul 11 02:35:45 2024 ] 	Batch(6200/7879) done. Loss: 0.3337  lr:0.000001
[ Thu Jul 11 02:36:08 2024 ] 	Batch(6300/7879) done. Loss: 0.5031  lr:0.000001
[ Thu Jul 11 02:36:31 2024 ] 	Batch(6400/7879) done. Loss: 0.0787  lr:0.000001
[ Thu Jul 11 02:36:53 2024 ] 
Training: Epoch [12/120], Step [6499], Loss: 0.02084909752011299, Training Accuracy: 96.0673076923077
[ Thu Jul 11 02:36:53 2024 ] 	Batch(6500/7879) done. Loss: 0.1050  lr:0.000001
[ Thu Jul 11 02:37:16 2024 ] 	Batch(6600/7879) done. Loss: 0.0273  lr:0.000001
[ Thu Jul 11 02:37:39 2024 ] 	Batch(6700/7879) done. Loss: 0.0759  lr:0.000001
[ Thu Jul 11 02:38:01 2024 ] 	Batch(6800/7879) done. Loss: 0.2207  lr:0.000001
[ Thu Jul 11 02:38:24 2024 ] 	Batch(6900/7879) done. Loss: 0.1730  lr:0.000001
[ Thu Jul 11 02:38:46 2024 ] 
Training: Epoch [12/120], Step [6999], Loss: 0.17467164993286133, Training Accuracy: 96.08214285714286
[ Thu Jul 11 02:38:47 2024 ] 	Batch(7000/7879) done. Loss: 0.0233  lr:0.000001
[ Thu Jul 11 02:39:09 2024 ] 	Batch(7100/7879) done. Loss: 0.4777  lr:0.000001
[ Thu Jul 11 02:39:32 2024 ] 	Batch(7200/7879) done. Loss: 0.0223  lr:0.000001
[ Thu Jul 11 02:39:55 2024 ] 	Batch(7300/7879) done. Loss: 0.2336  lr:0.000001
[ Thu Jul 11 02:40:17 2024 ] 	Batch(7400/7879) done. Loss: 0.2422  lr:0.000001
[ Thu Jul 11 02:40:40 2024 ] 
Training: Epoch [12/120], Step [7499], Loss: 0.40857696533203125, Training Accuracy: 96.08166666666666
[ Thu Jul 11 02:40:40 2024 ] 	Batch(7500/7879) done. Loss: 0.0064  lr:0.000001
[ Thu Jul 11 02:41:03 2024 ] 	Batch(7600/7879) done. Loss: 0.0873  lr:0.000001
[ Thu Jul 11 02:41:25 2024 ] 	Batch(7700/7879) done. Loss: 0.0075  lr:0.000001
[ Thu Jul 11 02:41:48 2024 ] 	Batch(7800/7879) done. Loss: 0.4215  lr:0.000001
[ Thu Jul 11 02:42:06 2024 ] 	Mean training loss: 0.1431.
[ Thu Jul 11 02:42:06 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 02:42:06 2024 ] Training epoch: 14
[ Thu Jul 11 02:42:06 2024 ] 	Batch(0/7879) done. Loss: 0.0612  lr:0.000001
[ Thu Jul 11 02:42:30 2024 ] 	Batch(100/7879) done. Loss: 0.2424  lr:0.000001
[ Thu Jul 11 02:42:53 2024 ] 	Batch(200/7879) done. Loss: 0.0574  lr:0.000001
[ Thu Jul 11 02:43:16 2024 ] 	Batch(300/7879) done. Loss: 0.0289  lr:0.000001
[ Thu Jul 11 02:43:39 2024 ] 	Batch(400/7879) done. Loss: 0.3267  lr:0.000001
[ Thu Jul 11 02:44:02 2024 ] 
Training: Epoch [13/120], Step [499], Loss: 0.17783774435520172, Training Accuracy: 95.95
[ Thu Jul 11 02:44:02 2024 ] 	Batch(500/7879) done. Loss: 0.1966  lr:0.000001
[ Thu Jul 11 02:44:25 2024 ] 	Batch(600/7879) done. Loss: 0.0488  lr:0.000001
[ Thu Jul 11 02:44:47 2024 ] 	Batch(700/7879) done. Loss: 0.0442  lr:0.000001
[ Thu Jul 11 02:45:10 2024 ] 	Batch(800/7879) done. Loss: 0.0120  lr:0.000001
[ Thu Jul 11 02:45:33 2024 ] 	Batch(900/7879) done. Loss: 0.0223  lr:0.000001
[ Thu Jul 11 02:45:55 2024 ] 
Training: Epoch [13/120], Step [999], Loss: 0.03458818420767784, Training Accuracy: 96.2
[ Thu Jul 11 02:45:56 2024 ] 	Batch(1000/7879) done. Loss: 0.1095  lr:0.000001
[ Thu Jul 11 02:46:18 2024 ] 	Batch(1100/7879) done. Loss: 0.0186  lr:0.000001
[ Thu Jul 11 02:46:41 2024 ] 	Batch(1200/7879) done. Loss: 0.0995  lr:0.000001
[ Thu Jul 11 02:47:04 2024 ] 	Batch(1300/7879) done. Loss: 0.0171  lr:0.000001
[ Thu Jul 11 02:47:27 2024 ] 	Batch(1400/7879) done. Loss: 0.0743  lr:0.000001
[ Thu Jul 11 02:47:50 2024 ] 
Training: Epoch [13/120], Step [1499], Loss: 0.0048556262627244, Training Accuracy: 96.36666666666667
[ Thu Jul 11 02:47:50 2024 ] 	Batch(1500/7879) done. Loss: 0.0238  lr:0.000001
[ Thu Jul 11 02:48:13 2024 ] 	Batch(1600/7879) done. Loss: 0.0764  lr:0.000001
[ Thu Jul 11 02:48:35 2024 ] 	Batch(1700/7879) done. Loss: 0.4168  lr:0.000001
[ Thu Jul 11 02:48:58 2024 ] 	Batch(1800/7879) done. Loss: 0.0490  lr:0.000001
[ Thu Jul 11 02:49:21 2024 ] 	Batch(1900/7879) done. Loss: 0.0014  lr:0.000001
[ Thu Jul 11 02:49:43 2024 ] 
Training: Epoch [13/120], Step [1999], Loss: 0.31211772561073303, Training Accuracy: 96.3875
[ Thu Jul 11 02:49:44 2024 ] 	Batch(2000/7879) done. Loss: 0.2363  lr:0.000001
[ Thu Jul 11 02:50:06 2024 ] 	Batch(2100/7879) done. Loss: 0.0167  lr:0.000001
[ Thu Jul 11 02:50:29 2024 ] 	Batch(2200/7879) done. Loss: 0.2359  lr:0.000001
[ Thu Jul 11 02:50:52 2024 ] 	Batch(2300/7879) done. Loss: 0.1183  lr:0.000001
[ Thu Jul 11 02:51:15 2024 ] 	Batch(2400/7879) done. Loss: 0.2027  lr:0.000001
[ Thu Jul 11 02:51:37 2024 ] 
Training: Epoch [13/120], Step [2499], Loss: 0.16541063785552979, Training Accuracy: 96.28999999999999
[ Thu Jul 11 02:51:37 2024 ] 	Batch(2500/7879) done. Loss: 0.0072  lr:0.000001
[ Thu Jul 11 02:52:00 2024 ] 	Batch(2600/7879) done. Loss: 0.0881  lr:0.000001
[ Thu Jul 11 02:52:23 2024 ] 	Batch(2700/7879) done. Loss: 0.1876  lr:0.000001
[ Thu Jul 11 02:52:46 2024 ] 	Batch(2800/7879) done. Loss: 0.0293  lr:0.000001
[ Thu Jul 11 02:53:09 2024 ] 	Batch(2900/7879) done. Loss: 0.1726  lr:0.000001
[ Thu Jul 11 02:53:32 2024 ] 
Training: Epoch [13/120], Step [2999], Loss: 0.5439416766166687, Training Accuracy: 96.26666666666667
[ Thu Jul 11 02:53:32 2024 ] 	Batch(3000/7879) done. Loss: 0.2746  lr:0.000001
[ Thu Jul 11 02:53:55 2024 ] 	Batch(3100/7879) done. Loss: 0.6145  lr:0.000001
[ Thu Jul 11 02:54:18 2024 ] 	Batch(3200/7879) done. Loss: 0.0644  lr:0.000001
[ Thu Jul 11 02:54:40 2024 ] 	Batch(3300/7879) done. Loss: 0.0229  lr:0.000001
[ Thu Jul 11 02:55:03 2024 ] 	Batch(3400/7879) done. Loss: 0.4987  lr:0.000001
[ Thu Jul 11 02:55:26 2024 ] 
Training: Epoch [13/120], Step [3499], Loss: 0.01730802096426487, Training Accuracy: 96.16071428571429
[ Thu Jul 11 02:55:26 2024 ] 	Batch(3500/7879) done. Loss: 0.0131  lr:0.000001
[ Thu Jul 11 02:55:49 2024 ] 	Batch(3600/7879) done. Loss: 0.0049  lr:0.000001
[ Thu Jul 11 02:56:12 2024 ] 	Batch(3700/7879) done. Loss: 0.2709  lr:0.000001
[ Thu Jul 11 02:56:35 2024 ] 	Batch(3800/7879) done. Loss: 0.0101  lr:0.000001
[ Thu Jul 11 02:56:57 2024 ] 	Batch(3900/7879) done. Loss: 0.0647  lr:0.000001
[ Thu Jul 11 02:57:20 2024 ] 
Training: Epoch [13/120], Step [3999], Loss: 0.13381712138652802, Training Accuracy: 96.18124999999999
[ Thu Jul 11 02:57:20 2024 ] 	Batch(4000/7879) done. Loss: 0.0646  lr:0.000001
[ Thu Jul 11 02:57:43 2024 ] 	Batch(4100/7879) done. Loss: 0.2125  lr:0.000001
[ Thu Jul 11 02:58:06 2024 ] 	Batch(4200/7879) done. Loss: 0.0050  lr:0.000001
[ Thu Jul 11 02:58:29 2024 ] 	Batch(4300/7879) done. Loss: 0.0070  lr:0.000001
[ Thu Jul 11 02:58:52 2024 ] 	Batch(4400/7879) done. Loss: 0.2712  lr:0.000001
[ Thu Jul 11 02:59:14 2024 ] 
Training: Epoch [13/120], Step [4499], Loss: 0.08896057307720184, Training Accuracy: 96.19722222222222
[ Thu Jul 11 02:59:14 2024 ] 	Batch(4500/7879) done. Loss: 0.6770  lr:0.000001
[ Thu Jul 11 02:59:37 2024 ] 	Batch(4600/7879) done. Loss: 0.0570  lr:0.000001
[ Thu Jul 11 03:00:00 2024 ] 	Batch(4700/7879) done. Loss: 0.0771  lr:0.000001
[ Thu Jul 11 03:00:23 2024 ] 	Batch(4800/7879) done. Loss: 0.2055  lr:0.000001
[ Thu Jul 11 03:00:46 2024 ] 	Batch(4900/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul 11 03:01:08 2024 ] 
Training: Epoch [13/120], Step [4999], Loss: 0.44426798820495605, Training Accuracy: 96.16
[ Thu Jul 11 03:01:08 2024 ] 	Batch(5000/7879) done. Loss: 0.0456  lr:0.000001
[ Thu Jul 11 03:01:31 2024 ] 	Batch(5100/7879) done. Loss: 0.0765  lr:0.000001
[ Thu Jul 11 03:01:54 2024 ] 	Batch(5200/7879) done. Loss: 0.2899  lr:0.000001
[ Thu Jul 11 03:02:17 2024 ] 	Batch(5300/7879) done. Loss: 0.0768  lr:0.000001
[ Thu Jul 11 03:02:40 2024 ] 	Batch(5400/7879) done. Loss: 0.0133  lr:0.000001
[ Thu Jul 11 03:03:02 2024 ] 
Training: Epoch [13/120], Step [5499], Loss: 0.004721679259091616, Training Accuracy: 96.18863636363636
[ Thu Jul 11 03:03:02 2024 ] 	Batch(5500/7879) done. Loss: 0.0419  lr:0.000001
[ Thu Jul 11 03:03:25 2024 ] 	Batch(5600/7879) done. Loss: 0.0504  lr:0.000001
[ Thu Jul 11 03:03:48 2024 ] 	Batch(5700/7879) done. Loss: 0.1503  lr:0.000001
[ Thu Jul 11 03:04:11 2024 ] 	Batch(5800/7879) done. Loss: 0.1853  lr:0.000001
[ Thu Jul 11 03:04:33 2024 ] 	Batch(5900/7879) done. Loss: 0.1386  lr:0.000001
[ Thu Jul 11 03:04:56 2024 ] 
Training: Epoch [13/120], Step [5999], Loss: 0.08914978802204132, Training Accuracy: 96.175
[ Thu Jul 11 03:04:56 2024 ] 	Batch(6000/7879) done. Loss: 0.1457  lr:0.000001
[ Thu Jul 11 03:05:20 2024 ] 	Batch(6100/7879) done. Loss: 0.4604  lr:0.000001
[ Thu Jul 11 03:05:42 2024 ] 	Batch(6200/7879) done. Loss: 0.0441  lr:0.000001
[ Thu Jul 11 03:06:05 2024 ] 	Batch(6300/7879) done. Loss: 0.3250  lr:0.000001
[ Thu Jul 11 03:06:28 2024 ] 	Batch(6400/7879) done. Loss: 0.2626  lr:0.000001
[ Thu Jul 11 03:06:51 2024 ] 
Training: Epoch [13/120], Step [6499], Loss: 0.034959081560373306, Training Accuracy: 96.16923076923078
[ Thu Jul 11 03:06:51 2024 ] 	Batch(6500/7879) done. Loss: 0.0130  lr:0.000001
[ Thu Jul 11 03:07:13 2024 ] 	Batch(6600/7879) done. Loss: 0.0083  lr:0.000001
[ Thu Jul 11 03:07:36 2024 ] 	Batch(6700/7879) done. Loss: 0.0587  lr:0.000001
[ Thu Jul 11 03:07:59 2024 ] 	Batch(6800/7879) done. Loss: 0.0592  lr:0.000001
[ Thu Jul 11 03:08:22 2024 ] 	Batch(6900/7879) done. Loss: 0.0705  lr:0.000001
[ Thu Jul 11 03:08:44 2024 ] 
Training: Epoch [13/120], Step [6999], Loss: 0.49101296067237854, Training Accuracy: 96.14821428571429
[ Thu Jul 11 03:08:44 2024 ] 	Batch(7000/7879) done. Loss: 0.0637  lr:0.000001
[ Thu Jul 11 03:09:07 2024 ] 	Batch(7100/7879) done. Loss: 0.0183  lr:0.000001
[ Thu Jul 11 03:09:30 2024 ] 	Batch(7200/7879) done. Loss: 0.0056  lr:0.000001
[ Thu Jul 11 03:09:53 2024 ] 	Batch(7300/7879) done. Loss: 0.0371  lr:0.000001
[ Thu Jul 11 03:10:16 2024 ] 	Batch(7400/7879) done. Loss: 0.2629  lr:0.000001
[ Thu Jul 11 03:10:38 2024 ] 
Training: Epoch [13/120], Step [7499], Loss: 0.020055904984474182, Training Accuracy: 96.11833333333333
[ Thu Jul 11 03:10:38 2024 ] 	Batch(7500/7879) done. Loss: 0.0125  lr:0.000001
[ Thu Jul 11 03:11:01 2024 ] 	Batch(7600/7879) done. Loss: 0.0583  lr:0.000001
[ Thu Jul 11 03:11:24 2024 ] 	Batch(7700/7879) done. Loss: 0.1127  lr:0.000001
[ Thu Jul 11 03:11:47 2024 ] 	Batch(7800/7879) done. Loss: 0.0568  lr:0.000001
[ Thu Jul 11 03:12:04 2024 ] 	Mean training loss: 0.1436.
[ Thu Jul 11 03:12:04 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 03:12:05 2024 ] Training epoch: 15
[ Thu Jul 11 03:12:05 2024 ] 	Batch(0/7879) done. Loss: 0.0120  lr:0.000001
[ Thu Jul 11 03:12:28 2024 ] 	Batch(100/7879) done. Loss: 0.0248  lr:0.000001
[ Thu Jul 11 03:12:51 2024 ] 	Batch(200/7879) done. Loss: 0.0330  lr:0.000001
[ Thu Jul 11 03:13:14 2024 ] 	Batch(300/7879) done. Loss: 0.0066  lr:0.000001
[ Thu Jul 11 03:13:37 2024 ] 	Batch(400/7879) done. Loss: 0.3012  lr:0.000001
[ Thu Jul 11 03:14:00 2024 ] 
Training: Epoch [14/120], Step [499], Loss: 0.02129911072552204, Training Accuracy: 95.92500000000001
[ Thu Jul 11 03:14:00 2024 ] 	Batch(500/7879) done. Loss: 0.2542  lr:0.000001
[ Thu Jul 11 03:14:23 2024 ] 	Batch(600/7879) done. Loss: 0.2758  lr:0.000001
[ Thu Jul 11 03:14:46 2024 ] 	Batch(700/7879) done. Loss: 0.1889  lr:0.000001
[ Thu Jul 11 03:15:09 2024 ] 	Batch(800/7879) done. Loss: 0.0851  lr:0.000001
[ Thu Jul 11 03:15:33 2024 ] 	Batch(900/7879) done. Loss: 0.0219  lr:0.000001
[ Thu Jul 11 03:15:56 2024 ] 
Training: Epoch [14/120], Step [999], Loss: 0.07571940124034882, Training Accuracy: 96.0125
[ Thu Jul 11 03:15:56 2024 ] 	Batch(1000/7879) done. Loss: 0.0313  lr:0.000001
[ Thu Jul 11 03:16:19 2024 ] 	Batch(1100/7879) done. Loss: 0.0463  lr:0.000001
[ Thu Jul 11 03:16:43 2024 ] 	Batch(1200/7879) done. Loss: 0.0982  lr:0.000001
[ Thu Jul 11 03:17:06 2024 ] 	Batch(1300/7879) done. Loss: 0.0665  lr:0.000001
[ Thu Jul 11 03:17:29 2024 ] 	Batch(1400/7879) done. Loss: 0.4293  lr:0.000001
[ Thu Jul 11 03:17:52 2024 ] 
Training: Epoch [14/120], Step [1499], Loss: 0.01147184893488884, Training Accuracy: 96.16666666666667
[ Thu Jul 11 03:17:53 2024 ] 	Batch(1500/7879) done. Loss: 0.0778  lr:0.000001
[ Thu Jul 11 03:18:15 2024 ] 	Batch(1600/7879) done. Loss: 0.3033  lr:0.000001
[ Thu Jul 11 03:18:38 2024 ] 	Batch(1700/7879) done. Loss: 0.0070  lr:0.000001
[ Thu Jul 11 03:19:01 2024 ] 	Batch(1800/7879) done. Loss: 0.0847  lr:0.000001
[ Thu Jul 11 03:19:23 2024 ] 	Batch(1900/7879) done. Loss: 0.1610  lr:0.000001
[ Thu Jul 11 03:19:46 2024 ] 
Training: Epoch [14/120], Step [1999], Loss: 0.02597827836871147, Training Accuracy: 96.0375
[ Thu Jul 11 03:19:46 2024 ] 	Batch(2000/7879) done. Loss: 0.0842  lr:0.000001
[ Thu Jul 11 03:20:09 2024 ] 	Batch(2100/7879) done. Loss: 0.2678  lr:0.000001
[ Thu Jul 11 03:20:31 2024 ] 	Batch(2200/7879) done. Loss: 0.2086  lr:0.000001
[ Thu Jul 11 03:20:54 2024 ] 	Batch(2300/7879) done. Loss: 0.0815  lr:0.000001
[ Thu Jul 11 03:21:17 2024 ] 	Batch(2400/7879) done. Loss: 0.2057  lr:0.000001
[ Thu Jul 11 03:21:39 2024 ] 
Training: Epoch [14/120], Step [2499], Loss: 0.05359992757439613, Training Accuracy: 95.985
[ Thu Jul 11 03:21:39 2024 ] 	Batch(2500/7879) done. Loss: 0.0110  lr:0.000001
[ Thu Jul 11 03:22:02 2024 ] 	Batch(2600/7879) done. Loss: 0.0293  lr:0.000001
[ Thu Jul 11 03:22:24 2024 ] 	Batch(2700/7879) done. Loss: 0.2473  lr:0.000001
[ Thu Jul 11 03:22:47 2024 ] 	Batch(2800/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul 11 03:23:10 2024 ] 	Batch(2900/7879) done. Loss: 0.2163  lr:0.000001
[ Thu Jul 11 03:23:33 2024 ] 
Training: Epoch [14/120], Step [2999], Loss: 0.00823301263153553, Training Accuracy: 96.025
[ Thu Jul 11 03:23:33 2024 ] 	Batch(3000/7879) done. Loss: 0.1380  lr:0.000001
[ Thu Jul 11 03:23:56 2024 ] 	Batch(3100/7879) done. Loss: 0.0984  lr:0.000001
[ Thu Jul 11 03:24:19 2024 ] 	Batch(3200/7879) done. Loss: 0.3226  lr:0.000001
[ Thu Jul 11 03:24:41 2024 ] 	Batch(3300/7879) done. Loss: 0.2798  lr:0.000001
[ Thu Jul 11 03:25:04 2024 ] 	Batch(3400/7879) done. Loss: 0.0253  lr:0.000001
[ Thu Jul 11 03:25:26 2024 ] 
Training: Epoch [14/120], Step [3499], Loss: 0.2567461133003235, Training Accuracy: 96.06428571428572
[ Thu Jul 11 03:25:26 2024 ] 	Batch(3500/7879) done. Loss: 0.0079  lr:0.000001
[ Thu Jul 11 03:25:49 2024 ] 	Batch(3600/7879) done. Loss: 0.0077  lr:0.000001
[ Thu Jul 11 03:26:12 2024 ] 	Batch(3700/7879) done. Loss: 0.0386  lr:0.000001
[ Thu Jul 11 03:26:35 2024 ] 	Batch(3800/7879) done. Loss: 0.0830  lr:0.000001
[ Thu Jul 11 03:26:57 2024 ] 	Batch(3900/7879) done. Loss: 0.0869  lr:0.000001
[ Thu Jul 11 03:27:20 2024 ] 
Training: Epoch [14/120], Step [3999], Loss: 0.12517157196998596, Training Accuracy: 96.1
[ Thu Jul 11 03:27:20 2024 ] 	Batch(4000/7879) done. Loss: 0.0973  lr:0.000001
[ Thu Jul 11 03:27:43 2024 ] 	Batch(4100/7879) done. Loss: 0.1197  lr:0.000001
[ Thu Jul 11 03:28:05 2024 ] 	Batch(4200/7879) done. Loss: 0.0992  lr:0.000001
[ Thu Jul 11 03:28:28 2024 ] 	Batch(4300/7879) done. Loss: 0.1571  lr:0.000001
[ Thu Jul 11 03:28:50 2024 ] 	Batch(4400/7879) done. Loss: 0.7104  lr:0.000001
[ Thu Jul 11 03:29:13 2024 ] 
Training: Epoch [14/120], Step [4499], Loss: 0.0844169333577156, Training Accuracy: 96.13888888888889
[ Thu Jul 11 03:29:13 2024 ] 	Batch(4500/7879) done. Loss: 0.4892  lr:0.000001
[ Thu Jul 11 03:29:36 2024 ] 	Batch(4600/7879) done. Loss: 0.6258  lr:0.000001
[ Thu Jul 11 03:29:58 2024 ] 	Batch(4700/7879) done. Loss: 0.0829  lr:0.000001
[ Thu Jul 11 03:30:21 2024 ] 	Batch(4800/7879) done. Loss: 0.0930  lr:0.000001
[ Thu Jul 11 03:30:43 2024 ] 	Batch(4900/7879) done. Loss: 0.0020  lr:0.000001
[ Thu Jul 11 03:31:06 2024 ] 
Training: Epoch [14/120], Step [4999], Loss: 0.054975349456071854, Training Accuracy: 96.19
[ Thu Jul 11 03:31:06 2024 ] 	Batch(5000/7879) done. Loss: 0.1433  lr:0.000001
[ Thu Jul 11 03:31:29 2024 ] 	Batch(5100/7879) done. Loss: 0.2367  lr:0.000001
[ Thu Jul 11 03:31:51 2024 ] 	Batch(5200/7879) done. Loss: 0.3813  lr:0.000001
[ Thu Jul 11 03:32:14 2024 ] 	Batch(5300/7879) done. Loss: 0.1996  lr:0.000001
[ Thu Jul 11 03:32:36 2024 ] 	Batch(5400/7879) done. Loss: 0.0469  lr:0.000001
[ Thu Jul 11 03:32:59 2024 ] 
Training: Epoch [14/120], Step [5499], Loss: 0.15259359776973724, Training Accuracy: 96.19090909090909
[ Thu Jul 11 03:32:59 2024 ] 	Batch(5500/7879) done. Loss: 0.0050  lr:0.000001
[ Thu Jul 11 03:33:22 2024 ] 	Batch(5600/7879) done. Loss: 0.2050  lr:0.000001
[ Thu Jul 11 03:33:45 2024 ] 	Batch(5700/7879) done. Loss: 0.2011  lr:0.000001
[ Thu Jul 11 03:34:08 2024 ] 	Batch(5800/7879) done. Loss: 0.0208  lr:0.000001
[ Thu Jul 11 03:34:32 2024 ] 	Batch(5900/7879) done. Loss: 0.0544  lr:0.000001
[ Thu Jul 11 03:34:54 2024 ] 
Training: Epoch [14/120], Step [5999], Loss: 0.026212021708488464, Training Accuracy: 96.15625
[ Thu Jul 11 03:34:55 2024 ] 	Batch(6000/7879) done. Loss: 0.3187  lr:0.000001
[ Thu Jul 11 03:35:17 2024 ] 	Batch(6100/7879) done. Loss: 0.0303  lr:0.000001
[ Thu Jul 11 03:35:40 2024 ] 	Batch(6200/7879) done. Loss: 0.0091  lr:0.000001
[ Thu Jul 11 03:36:02 2024 ] 	Batch(6300/7879) done. Loss: 0.0313  lr:0.000001
[ Thu Jul 11 03:36:25 2024 ] 	Batch(6400/7879) done. Loss: 0.1274  lr:0.000001
[ Thu Jul 11 03:36:48 2024 ] 
Training: Epoch [14/120], Step [6499], Loss: 0.010397585108876228, Training Accuracy: 96.13076923076923
[ Thu Jul 11 03:36:48 2024 ] 	Batch(6500/7879) done. Loss: 0.1100  lr:0.000001
[ Thu Jul 11 03:37:12 2024 ] 	Batch(6600/7879) done. Loss: 0.4723  lr:0.000001
[ Thu Jul 11 03:37:35 2024 ] 	Batch(6700/7879) done. Loss: 0.1224  lr:0.000001
[ Thu Jul 11 03:37:59 2024 ] 	Batch(6800/7879) done. Loss: 0.3471  lr:0.000001
[ Thu Jul 11 03:38:22 2024 ] 	Batch(6900/7879) done. Loss: 0.0230  lr:0.000001
[ Thu Jul 11 03:38:45 2024 ] 
Training: Epoch [14/120], Step [6999], Loss: 0.11863711476325989, Training Accuracy: 96.09285714285714
[ Thu Jul 11 03:38:45 2024 ] 	Batch(7000/7879) done. Loss: 0.0492  lr:0.000001
[ Thu Jul 11 03:39:08 2024 ] 	Batch(7100/7879) done. Loss: 0.1162  lr:0.000001
[ Thu Jul 11 03:39:32 2024 ] 	Batch(7200/7879) done. Loss: 0.0839  lr:0.000001
[ Thu Jul 11 03:39:55 2024 ] 	Batch(7300/7879) done. Loss: 0.0533  lr:0.000001
[ Thu Jul 11 03:40:19 2024 ] 	Batch(7400/7879) done. Loss: 0.0610  lr:0.000001
[ Thu Jul 11 03:40:42 2024 ] 
Training: Epoch [14/120], Step [7499], Loss: 0.07800731062889099, Training Accuracy: 96.115
[ Thu Jul 11 03:40:42 2024 ] 	Batch(7500/7879) done. Loss: 0.0882  lr:0.000001
[ Thu Jul 11 03:41:05 2024 ] 	Batch(7600/7879) done. Loss: 0.0546  lr:0.000001
[ Thu Jul 11 03:41:28 2024 ] 	Batch(7700/7879) done. Loss: 0.2986  lr:0.000001
[ Thu Jul 11 03:41:51 2024 ] 	Batch(7800/7879) done. Loss: 0.0518  lr:0.000001
[ Thu Jul 11 03:42:08 2024 ] 	Mean training loss: 0.1454.
[ Thu Jul 11 03:42:08 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 03:42:08 2024 ] Training epoch: 16
[ Thu Jul 11 03:42:09 2024 ] 	Batch(0/7879) done. Loss: 0.1408  lr:0.000001
[ Thu Jul 11 03:42:32 2024 ] 	Batch(100/7879) done. Loss: 0.0539  lr:0.000001
[ Thu Jul 11 03:42:54 2024 ] 	Batch(200/7879) done. Loss: 0.2753  lr:0.000001
[ Thu Jul 11 03:43:17 2024 ] 	Batch(300/7879) done. Loss: 0.0158  lr:0.000001
[ Thu Jul 11 03:43:40 2024 ] 	Batch(400/7879) done. Loss: 0.0647  lr:0.000001
[ Thu Jul 11 03:44:02 2024 ] 
Training: Epoch [15/120], Step [499], Loss: 0.025364499539136887, Training Accuracy: 96.75
[ Thu Jul 11 03:44:02 2024 ] 	Batch(500/7879) done. Loss: 0.6144  lr:0.000001
[ Thu Jul 11 03:44:25 2024 ] 	Batch(600/7879) done. Loss: 0.2381  lr:0.000001
[ Thu Jul 11 03:44:47 2024 ] 	Batch(700/7879) done. Loss: 0.0407  lr:0.000001
[ Thu Jul 11 03:45:10 2024 ] 	Batch(800/7879) done. Loss: 0.0635  lr:0.000001
[ Thu Jul 11 03:45:33 2024 ] 	Batch(900/7879) done. Loss: 0.0960  lr:0.000001
[ Thu Jul 11 03:45:55 2024 ] 
Training: Epoch [15/120], Step [999], Loss: 0.06912896782159805, Training Accuracy: 96.3625
[ Thu Jul 11 03:45:55 2024 ] 	Batch(1000/7879) done. Loss: 0.4736  lr:0.000001
[ Thu Jul 11 03:46:18 2024 ] 	Batch(1100/7879) done. Loss: 0.0763  lr:0.000001
[ Thu Jul 11 03:46:41 2024 ] 	Batch(1200/7879) done. Loss: 0.4962  lr:0.000001
[ Thu Jul 11 03:47:03 2024 ] 	Batch(1300/7879) done. Loss: 0.1445  lr:0.000001
[ Thu Jul 11 03:47:26 2024 ] 	Batch(1400/7879) done. Loss: 0.0241  lr:0.000001
[ Thu Jul 11 03:47:48 2024 ] 
Training: Epoch [15/120], Step [1499], Loss: 0.4850674569606781, Training Accuracy: 96.3
[ Thu Jul 11 03:47:48 2024 ] 	Batch(1500/7879) done. Loss: 0.0261  lr:0.000001
[ Thu Jul 11 03:48:11 2024 ] 	Batch(1600/7879) done. Loss: 0.2407  lr:0.000001
[ Thu Jul 11 03:48:33 2024 ] 	Batch(1700/7879) done. Loss: 0.1370  lr:0.000001
[ Thu Jul 11 03:48:56 2024 ] 	Batch(1800/7879) done. Loss: 0.0074  lr:0.000001
[ Thu Jul 11 03:49:19 2024 ] 	Batch(1900/7879) done. Loss: 0.0110  lr:0.000001
[ Thu Jul 11 03:49:42 2024 ] 
Training: Epoch [15/120], Step [1999], Loss: 0.09546561539173126, Training Accuracy: 96.19375
[ Thu Jul 11 03:49:42 2024 ] 	Batch(2000/7879) done. Loss: 0.2486  lr:0.000001
[ Thu Jul 11 03:50:06 2024 ] 	Batch(2100/7879) done. Loss: 0.0664  lr:0.000001
[ Thu Jul 11 03:50:28 2024 ] 	Batch(2200/7879) done. Loss: 0.6163  lr:0.000001
[ Thu Jul 11 03:50:51 2024 ] 	Batch(2300/7879) done. Loss: 0.0043  lr:0.000001
[ Thu Jul 11 03:51:14 2024 ] 	Batch(2400/7879) done. Loss: 0.1236  lr:0.000001
[ Thu Jul 11 03:51:36 2024 ] 
Training: Epoch [15/120], Step [2499], Loss: 0.10200533270835876, Training Accuracy: 96.10499999999999
[ Thu Jul 11 03:51:36 2024 ] 	Batch(2500/7879) done. Loss: 0.2945  lr:0.000001
[ Thu Jul 11 03:51:59 2024 ] 	Batch(2600/7879) done. Loss: 0.0814  lr:0.000001
[ Thu Jul 11 03:52:21 2024 ] 	Batch(2700/7879) done. Loss: 0.0517  lr:0.000001
[ Thu Jul 11 03:52:44 2024 ] 	Batch(2800/7879) done. Loss: 0.0259  lr:0.000001
[ Thu Jul 11 03:53:07 2024 ] 	Batch(2900/7879) done. Loss: 0.0901  lr:0.000001
[ Thu Jul 11 03:53:29 2024 ] 
Training: Epoch [15/120], Step [2999], Loss: 0.09315314888954163, Training Accuracy: 96.05833333333334
[ Thu Jul 11 03:53:29 2024 ] 	Batch(3000/7879) done. Loss: 0.2477  lr:0.000001
[ Thu Jul 11 03:53:52 2024 ] 	Batch(3100/7879) done. Loss: 0.4178  lr:0.000001
[ Thu Jul 11 03:54:15 2024 ] 	Batch(3200/7879) done. Loss: 0.3312  lr:0.000001
[ Thu Jul 11 03:54:37 2024 ] 	Batch(3300/7879) done. Loss: 0.3051  lr:0.000001
[ Thu Jul 11 03:55:00 2024 ] 	Batch(3400/7879) done. Loss: 0.2926  lr:0.000001
[ Thu Jul 11 03:55:22 2024 ] 
Training: Epoch [15/120], Step [3499], Loss: 0.07168953865766525, Training Accuracy: 96.13571428571429
[ Thu Jul 11 03:55:22 2024 ] 	Batch(3500/7879) done. Loss: 0.1131  lr:0.000001
[ Thu Jul 11 03:55:45 2024 ] 	Batch(3600/7879) done. Loss: 0.0375  lr:0.000001
[ Thu Jul 11 03:56:09 2024 ] 	Batch(3700/7879) done. Loss: 0.2395  lr:0.000001
[ Thu Jul 11 03:56:32 2024 ] 	Batch(3800/7879) done. Loss: 0.0598  lr:0.000001
[ Thu Jul 11 03:56:55 2024 ] 	Batch(3900/7879) done. Loss: 0.0413  lr:0.000001
[ Thu Jul 11 03:57:17 2024 ] 
Training: Epoch [15/120], Step [3999], Loss: 0.04871682822704315, Training Accuracy: 96.19375
[ Thu Jul 11 03:57:17 2024 ] 	Batch(4000/7879) done. Loss: 0.4285  lr:0.000001
[ Thu Jul 11 03:57:40 2024 ] 	Batch(4100/7879) done. Loss: 0.1040  lr:0.000001
[ Thu Jul 11 03:58:03 2024 ] 	Batch(4200/7879) done. Loss: 0.0612  lr:0.000001
[ Thu Jul 11 03:58:26 2024 ] 	Batch(4300/7879) done. Loss: 0.0477  lr:0.000001
[ Thu Jul 11 03:58:48 2024 ] 	Batch(4400/7879) done. Loss: 0.0802  lr:0.000001
[ Thu Jul 11 03:59:11 2024 ] 
Training: Epoch [15/120], Step [4499], Loss: 0.935296356678009, Training Accuracy: 96.19444444444444
[ Thu Jul 11 03:59:11 2024 ] 	Batch(4500/7879) done. Loss: 0.0288  lr:0.000001
[ Thu Jul 11 03:59:34 2024 ] 	Batch(4600/7879) done. Loss: 0.0141  lr:0.000001
[ Thu Jul 11 03:59:57 2024 ] 	Batch(4700/7879) done. Loss: 0.0223  lr:0.000001
[ Thu Jul 11 04:00:20 2024 ] 	Batch(4800/7879) done. Loss: 0.0141  lr:0.000001
[ Thu Jul 11 04:00:42 2024 ] 	Batch(4900/7879) done. Loss: 0.0311  lr:0.000001
[ Thu Jul 11 04:01:05 2024 ] 
Training: Epoch [15/120], Step [4999], Loss: 0.45131728053092957, Training Accuracy: 96.195
[ Thu Jul 11 04:01:05 2024 ] 	Batch(5000/7879) done. Loss: 0.0884  lr:0.000001
[ Thu Jul 11 04:01:28 2024 ] 	Batch(5100/7879) done. Loss: 0.1747  lr:0.000001
[ Thu Jul 11 04:01:51 2024 ] 	Batch(5200/7879) done. Loss: 0.0197  lr:0.000001
[ Thu Jul 11 04:02:14 2024 ] 	Batch(5300/7879) done. Loss: 0.2973  lr:0.000001
[ Thu Jul 11 04:02:37 2024 ] 	Batch(5400/7879) done. Loss: 0.0094  lr:0.000001
[ Thu Jul 11 04:03:00 2024 ] 
Training: Epoch [15/120], Step [5499], Loss: 0.002905846806243062, Training Accuracy: 96.18636363636364
[ Thu Jul 11 04:03:00 2024 ] 	Batch(5500/7879) done. Loss: 0.0016  lr:0.000001
[ Thu Jul 11 04:03:23 2024 ] 	Batch(5600/7879) done. Loss: 0.0186  lr:0.000001
[ Thu Jul 11 04:03:47 2024 ] 	Batch(5700/7879) done. Loss: 0.0030  lr:0.000001
[ Thu Jul 11 04:04:10 2024 ] 	Batch(5800/7879) done. Loss: 0.3389  lr:0.000001
[ Thu Jul 11 04:04:33 2024 ] 	Batch(5900/7879) done. Loss: 0.0900  lr:0.000001
[ Thu Jul 11 04:04:56 2024 ] 
Training: Epoch [15/120], Step [5999], Loss: 0.07903464138507843, Training Accuracy: 96.17291666666667
[ Thu Jul 11 04:04:56 2024 ] 	Batch(6000/7879) done. Loss: 0.0433  lr:0.000001
[ Thu Jul 11 04:05:19 2024 ] 	Batch(6100/7879) done. Loss: 0.1627  lr:0.000001
[ Thu Jul 11 04:05:41 2024 ] 	Batch(6200/7879) done. Loss: 0.1506  lr:0.000001
[ Thu Jul 11 04:06:04 2024 ] 	Batch(6300/7879) done. Loss: 0.1154  lr:0.000001
[ Thu Jul 11 04:06:27 2024 ] 	Batch(6400/7879) done. Loss: 0.0255  lr:0.000001
[ Thu Jul 11 04:06:49 2024 ] 
Training: Epoch [15/120], Step [6499], Loss: 0.007860491052269936, Training Accuracy: 96.18653846153846
[ Thu Jul 11 04:06:49 2024 ] 	Batch(6500/7879) done. Loss: 0.0176  lr:0.000001
[ Thu Jul 11 04:07:12 2024 ] 	Batch(6600/7879) done. Loss: 0.0063  lr:0.000001
[ Thu Jul 11 04:07:35 2024 ] 	Batch(6700/7879) done. Loss: 0.0082  lr:0.000001
[ Thu Jul 11 04:07:59 2024 ] 	Batch(6800/7879) done. Loss: 0.0065  lr:0.000001
[ Thu Jul 11 04:08:22 2024 ] 	Batch(6900/7879) done. Loss: 0.5532  lr:0.000001
[ Thu Jul 11 04:08:45 2024 ] 
Training: Epoch [15/120], Step [6999], Loss: 0.016955668106675148, Training Accuracy: 96.21607142857142
[ Thu Jul 11 04:08:45 2024 ] 	Batch(7000/7879) done. Loss: 0.2402  lr:0.000001
[ Thu Jul 11 04:09:09 2024 ] 	Batch(7100/7879) done. Loss: 0.0049  lr:0.000001
[ Thu Jul 11 04:09:32 2024 ] 	Batch(7200/7879) done. Loss: 0.0314  lr:0.000001
[ Thu Jul 11 04:09:55 2024 ] 	Batch(7300/7879) done. Loss: 0.0213  lr:0.000001
[ Thu Jul 11 04:10:19 2024 ] 	Batch(7400/7879) done. Loss: 0.0941  lr:0.000001
[ Thu Jul 11 04:10:42 2024 ] 
Training: Epoch [15/120], Step [7499], Loss: 0.05694644898176193, Training Accuracy: 96.21833333333333
[ Thu Jul 11 04:10:42 2024 ] 	Batch(7500/7879) done. Loss: 0.1232  lr:0.000001
[ Thu Jul 11 04:11:06 2024 ] 	Batch(7600/7879) done. Loss: 0.0276  lr:0.000001
[ Thu Jul 11 04:11:28 2024 ] 	Batch(7700/7879) done. Loss: 0.0132  lr:0.000001
[ Thu Jul 11 04:11:51 2024 ] 	Batch(7800/7879) done. Loss: 0.0355  lr:0.000001
[ Thu Jul 11 04:12:08 2024 ] 	Mean training loss: 0.1416.
[ Thu Jul 11 04:12:08 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 04:12:08 2024 ] Training epoch: 17
[ Thu Jul 11 04:12:09 2024 ] 	Batch(0/7879) done. Loss: 0.1412  lr:0.000001
[ Thu Jul 11 04:12:32 2024 ] 	Batch(100/7879) done. Loss: 0.0183  lr:0.000001
[ Thu Jul 11 04:12:54 2024 ] 	Batch(200/7879) done. Loss: 0.1174  lr:0.000001
[ Thu Jul 11 04:13:17 2024 ] 	Batch(300/7879) done. Loss: 0.0269  lr:0.000001
[ Thu Jul 11 04:13:40 2024 ] 	Batch(400/7879) done. Loss: 0.0848  lr:0.000001
[ Thu Jul 11 04:14:02 2024 ] 
Training: Epoch [16/120], Step [499], Loss: 0.6715155839920044, Training Accuracy: 95.875
[ Thu Jul 11 04:14:02 2024 ] 	Batch(500/7879) done. Loss: 0.2643  lr:0.000001
[ Thu Jul 11 04:14:25 2024 ] 	Batch(600/7879) done. Loss: 0.1750  lr:0.000001
[ Thu Jul 11 04:14:47 2024 ] 	Batch(700/7879) done. Loss: 0.0065  lr:0.000001
[ Thu Jul 11 04:15:10 2024 ] 	Batch(800/7879) done. Loss: 0.0919  lr:0.000001
[ Thu Jul 11 04:15:33 2024 ] 	Batch(900/7879) done. Loss: 0.4852  lr:0.000001
[ Thu Jul 11 04:15:55 2024 ] 
Training: Epoch [16/120], Step [999], Loss: 0.16964659094810486, Training Accuracy: 96.075
[ Thu Jul 11 04:15:55 2024 ] 	Batch(1000/7879) done. Loss: 0.1019  lr:0.000001
[ Thu Jul 11 04:16:18 2024 ] 	Batch(1100/7879) done. Loss: 0.0274  lr:0.000001
[ Thu Jul 11 04:16:41 2024 ] 	Batch(1200/7879) done. Loss: 0.0053  lr:0.000001
[ Thu Jul 11 04:17:03 2024 ] 	Batch(1300/7879) done. Loss: 0.2610  lr:0.000001
[ Thu Jul 11 04:17:26 2024 ] 	Batch(1400/7879) done. Loss: 0.0188  lr:0.000001
[ Thu Jul 11 04:17:48 2024 ] 
Training: Epoch [16/120], Step [1499], Loss: 0.20903007686138153, Training Accuracy: 95.93333333333334
[ Thu Jul 11 04:17:48 2024 ] 	Batch(1500/7879) done. Loss: 0.0091  lr:0.000001
[ Thu Jul 11 04:18:11 2024 ] 	Batch(1600/7879) done. Loss: 0.0864  lr:0.000001
[ Thu Jul 11 04:18:34 2024 ] 	Batch(1700/7879) done. Loss: 0.0080  lr:0.000001
[ Thu Jul 11 04:18:57 2024 ] 	Batch(1800/7879) done. Loss: 0.0271  lr:0.000001
[ Thu Jul 11 04:19:20 2024 ] 	Batch(1900/7879) done. Loss: 0.0981  lr:0.000001
[ Thu Jul 11 04:19:43 2024 ] 
Training: Epoch [16/120], Step [1999], Loss: 0.11801978945732117, Training Accuracy: 96.03125
[ Thu Jul 11 04:19:44 2024 ] 	Batch(2000/7879) done. Loss: 0.1181  lr:0.000001
[ Thu Jul 11 04:20:07 2024 ] 	Batch(2100/7879) done. Loss: 0.0811  lr:0.000001
[ Thu Jul 11 04:20:29 2024 ] 	Batch(2200/7879) done. Loss: 0.2406  lr:0.000001
[ Thu Jul 11 04:20:52 2024 ] 	Batch(2300/7879) done. Loss: 0.2099  lr:0.000001
[ Thu Jul 11 04:21:15 2024 ] 	Batch(2400/7879) done. Loss: 0.0762  lr:0.000001
[ Thu Jul 11 04:21:38 2024 ] 
Training: Epoch [16/120], Step [2499], Loss: 0.05291442945599556, Training Accuracy: 96.025
[ Thu Jul 11 04:21:38 2024 ] 	Batch(2500/7879) done. Loss: 0.0425  lr:0.000001
[ Thu Jul 11 04:22:01 2024 ] 	Batch(2600/7879) done. Loss: 0.2307  lr:0.000001
[ Thu Jul 11 04:22:24 2024 ] 	Batch(2700/7879) done. Loss: 0.3957  lr:0.000001
[ Thu Jul 11 04:22:48 2024 ] 	Batch(2800/7879) done. Loss: 0.4906  lr:0.000001
[ Thu Jul 11 04:23:11 2024 ] 	Batch(2900/7879) done. Loss: 0.1643  lr:0.000001
[ Thu Jul 11 04:23:34 2024 ] 
Training: Epoch [16/120], Step [2999], Loss: 0.7096283435821533, Training Accuracy: 95.92916666666666
[ Thu Jul 11 04:23:35 2024 ] 	Batch(3000/7879) done. Loss: 0.2823  lr:0.000001
[ Thu Jul 11 04:23:57 2024 ] 	Batch(3100/7879) done. Loss: 0.0405  lr:0.000001
[ Thu Jul 11 04:24:20 2024 ] 	Batch(3200/7879) done. Loss: 0.3272  lr:0.000001
[ Thu Jul 11 04:24:44 2024 ] 	Batch(3300/7879) done. Loss: 0.6466  lr:0.000001
[ Thu Jul 11 04:25:07 2024 ] 	Batch(3400/7879) done. Loss: 0.3089  lr:0.000001
[ Thu Jul 11 04:25:31 2024 ] 
Training: Epoch [16/120], Step [3499], Loss: 0.24693769216537476, Training Accuracy: 95.89999999999999
[ Thu Jul 11 04:25:31 2024 ] 	Batch(3500/7879) done. Loss: 0.0343  lr:0.000001
[ Thu Jul 11 04:25:54 2024 ] 	Batch(3600/7879) done. Loss: 0.0086  lr:0.000001
[ Thu Jul 11 04:26:18 2024 ] 	Batch(3700/7879) done. Loss: 0.1220  lr:0.000001
[ Thu Jul 11 04:26:41 2024 ] 	Batch(3800/7879) done. Loss: 0.0879  lr:0.000001
[ Thu Jul 11 04:27:04 2024 ] 	Batch(3900/7879) done. Loss: 0.0393  lr:0.000001
[ Thu Jul 11 04:27:28 2024 ] 
Training: Epoch [16/120], Step [3999], Loss: 0.24506960809230804, Training Accuracy: 95.953125
[ Thu Jul 11 04:27:28 2024 ] 	Batch(4000/7879) done. Loss: 0.8916  lr:0.000001
[ Thu Jul 11 04:27:51 2024 ] 	Batch(4100/7879) done. Loss: 0.0936  lr:0.000001
[ Thu Jul 11 04:28:15 2024 ] 	Batch(4200/7879) done. Loss: 0.0912  lr:0.000001
[ Thu Jul 11 04:28:38 2024 ] 	Batch(4300/7879) done. Loss: 0.2184  lr:0.000001
[ Thu Jul 11 04:29:01 2024 ] 	Batch(4400/7879) done. Loss: 0.5269  lr:0.000001
[ Thu Jul 11 04:29:24 2024 ] 
Training: Epoch [16/120], Step [4499], Loss: 0.1273920089006424, Training Accuracy: 96.02222222222223
[ Thu Jul 11 04:29:24 2024 ] 	Batch(4500/7879) done. Loss: 0.0979  lr:0.000001
[ Thu Jul 11 04:29:47 2024 ] 	Batch(4600/7879) done. Loss: 0.4008  lr:0.000001
[ Thu Jul 11 04:30:09 2024 ] 	Batch(4700/7879) done. Loss: 0.0290  lr:0.000001
[ Thu Jul 11 04:30:32 2024 ] 	Batch(4800/7879) done. Loss: 0.1095  lr:0.000001
[ Thu Jul 11 04:30:55 2024 ] 	Batch(4900/7879) done. Loss: 0.1128  lr:0.000001
[ Thu Jul 11 04:31:17 2024 ] 
Training: Epoch [16/120], Step [4999], Loss: 0.025461098179221153, Training Accuracy: 96.015
[ Thu Jul 11 04:31:17 2024 ] 	Batch(5000/7879) done. Loss: 0.0947  lr:0.000001
[ Thu Jul 11 04:31:40 2024 ] 	Batch(5100/7879) done. Loss: 0.1083  lr:0.000001
[ Thu Jul 11 04:32:03 2024 ] 	Batch(5200/7879) done. Loss: 0.0132  lr:0.000001
[ Thu Jul 11 04:32:25 2024 ] 	Batch(5300/7879) done. Loss: 0.0615  lr:0.000001
[ Thu Jul 11 04:32:48 2024 ] 	Batch(5400/7879) done. Loss: 0.0475  lr:0.000001
[ Thu Jul 11 04:33:10 2024 ] 
Training: Epoch [16/120], Step [5499], Loss: 0.09693072736263275, Training Accuracy: 96.07045454545454
[ Thu Jul 11 04:33:10 2024 ] 	Batch(5500/7879) done. Loss: 0.0963  lr:0.000001
[ Thu Jul 11 04:33:33 2024 ] 	Batch(5600/7879) done. Loss: 0.2323  lr:0.000001
[ Thu Jul 11 04:33:56 2024 ] 	Batch(5700/7879) done. Loss: 0.0786  lr:0.000001
[ Thu Jul 11 04:34:20 2024 ] 	Batch(5800/7879) done. Loss: 0.0919  lr:0.000001
[ Thu Jul 11 04:34:43 2024 ] 	Batch(5900/7879) done. Loss: 0.0531  lr:0.000001
[ Thu Jul 11 04:35:06 2024 ] 
Training: Epoch [16/120], Step [5999], Loss: 0.3280383050441742, Training Accuracy: 96.10208333333333
[ Thu Jul 11 04:35:06 2024 ] 	Batch(6000/7879) done. Loss: 0.0569  lr:0.000001
[ Thu Jul 11 04:35:29 2024 ] 	Batch(6100/7879) done. Loss: 0.1190  lr:0.000001
[ Thu Jul 11 04:35:52 2024 ] 	Batch(6200/7879) done. Loss: 0.3565  lr:0.000001
[ Thu Jul 11 04:36:14 2024 ] 	Batch(6300/7879) done. Loss: 0.1032  lr:0.000001
[ Thu Jul 11 04:36:37 2024 ] 	Batch(6400/7879) done. Loss: 0.1267  lr:0.000001
[ Thu Jul 11 04:37:00 2024 ] 
Training: Epoch [16/120], Step [6499], Loss: 0.04325629770755768, Training Accuracy: 96.08076923076923
[ Thu Jul 11 04:37:00 2024 ] 	Batch(6500/7879) done. Loss: 0.4152  lr:0.000001
[ Thu Jul 11 04:37:24 2024 ] 	Batch(6600/7879) done. Loss: 0.0190  lr:0.000001
[ Thu Jul 11 04:37:47 2024 ] 	Batch(6700/7879) done. Loss: 0.3571  lr:0.000001
[ Thu Jul 11 04:38:10 2024 ] 	Batch(6800/7879) done. Loss: 0.0237  lr:0.000001
[ Thu Jul 11 04:38:34 2024 ] 	Batch(6900/7879) done. Loss: 0.0238  lr:0.000001
[ Thu Jul 11 04:38:57 2024 ] 
Training: Epoch [16/120], Step [6999], Loss: 0.23336827754974365, Training Accuracy: 96.09821428571429
[ Thu Jul 11 04:38:57 2024 ] 	Batch(7000/7879) done. Loss: 0.1466  lr:0.000001
[ Thu Jul 11 04:39:20 2024 ] 	Batch(7100/7879) done. Loss: 0.0120  lr:0.000001
[ Thu Jul 11 04:39:44 2024 ] 	Batch(7200/7879) done. Loss: 0.0653  lr:0.000001
[ Thu Jul 11 04:40:07 2024 ] 	Batch(7300/7879) done. Loss: 0.2570  lr:0.000001
[ Thu Jul 11 04:40:30 2024 ] 	Batch(7400/7879) done. Loss: 0.0226  lr:0.000001
[ Thu Jul 11 04:40:54 2024 ] 
Training: Epoch [16/120], Step [7499], Loss: 0.074822336435318, Training Accuracy: 96.09666666666666
[ Thu Jul 11 04:40:54 2024 ] 	Batch(7500/7879) done. Loss: 0.1430  lr:0.000001
[ Thu Jul 11 04:41:17 2024 ] 	Batch(7600/7879) done. Loss: 0.1081  lr:0.000001
[ Thu Jul 11 04:41:40 2024 ] 	Batch(7700/7879) done. Loss: 0.0617  lr:0.000001
[ Thu Jul 11 04:42:03 2024 ] 	Batch(7800/7879) done. Loss: 0.0034  lr:0.000001
[ Thu Jul 11 04:42:21 2024 ] 	Mean training loss: 0.1389.
[ Thu Jul 11 04:42:21 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 04:42:21 2024 ] Training epoch: 18
[ Thu Jul 11 04:42:21 2024 ] 	Batch(0/7879) done. Loss: 0.0117  lr:0.000001
[ Thu Jul 11 04:42:45 2024 ] 	Batch(100/7879) done. Loss: 0.2822  lr:0.000001
[ Thu Jul 11 04:43:08 2024 ] 	Batch(200/7879) done. Loss: 0.0291  lr:0.000001
[ Thu Jul 11 04:43:30 2024 ] 	Batch(300/7879) done. Loss: 0.0235  lr:0.000001
[ Thu Jul 11 04:43:53 2024 ] 	Batch(400/7879) done. Loss: 0.1021  lr:0.000001
[ Thu Jul 11 04:44:16 2024 ] 
Training: Epoch [17/120], Step [499], Loss: 0.0819726437330246, Training Accuracy: 96.325
[ Thu Jul 11 04:44:16 2024 ] 	Batch(500/7879) done. Loss: 0.0785  lr:0.000001
[ Thu Jul 11 04:44:39 2024 ] 	Batch(600/7879) done. Loss: 0.1782  lr:0.000001
[ Thu Jul 11 04:45:01 2024 ] 	Batch(700/7879) done. Loss: 0.0233  lr:0.000001
[ Thu Jul 11 04:45:24 2024 ] 	Batch(800/7879) done. Loss: 0.0587  lr:0.000001
[ Thu Jul 11 04:45:47 2024 ] 	Batch(900/7879) done. Loss: 0.0328  lr:0.000001
[ Thu Jul 11 04:46:09 2024 ] 
Training: Epoch [17/120], Step [999], Loss: 0.18499335646629333, Training Accuracy: 96.2125
[ Thu Jul 11 04:46:10 2024 ] 	Batch(1000/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul 11 04:46:32 2024 ] 	Batch(1100/7879) done. Loss: 0.1494  lr:0.000001
[ Thu Jul 11 04:46:55 2024 ] 	Batch(1200/7879) done. Loss: 0.1681  lr:0.000001
[ Thu Jul 11 04:47:18 2024 ] 	Batch(1300/7879) done. Loss: 0.2417  lr:0.000001
[ Thu Jul 11 04:47:41 2024 ] 	Batch(1400/7879) done. Loss: 0.1488  lr:0.000001
[ Thu Jul 11 04:48:03 2024 ] 
Training: Epoch [17/120], Step [1499], Loss: 0.06655949354171753, Training Accuracy: 96.14166666666667
[ Thu Jul 11 04:48:03 2024 ] 	Batch(1500/7879) done. Loss: 0.0347  lr:0.000001
[ Thu Jul 11 04:48:26 2024 ] 	Batch(1600/7879) done. Loss: 0.1975  lr:0.000001
[ Thu Jul 11 04:48:49 2024 ] 	Batch(1700/7879) done. Loss: 0.0434  lr:0.000001
[ Thu Jul 11 04:49:12 2024 ] 	Batch(1800/7879) done. Loss: 0.1995  lr:0.000001
[ Thu Jul 11 04:49:35 2024 ] 	Batch(1900/7879) done. Loss: 0.2579  lr:0.000001
[ Thu Jul 11 04:49:58 2024 ] 
Training: Epoch [17/120], Step [1999], Loss: 0.12272358685731888, Training Accuracy: 96.2125
[ Thu Jul 11 04:49:58 2024 ] 	Batch(2000/7879) done. Loss: 0.0113  lr:0.000001
[ Thu Jul 11 04:50:21 2024 ] 	Batch(2100/7879) done. Loss: 0.3172  lr:0.000001
[ Thu Jul 11 04:50:44 2024 ] 	Batch(2200/7879) done. Loss: 0.0714  lr:0.000001
[ Thu Jul 11 04:51:07 2024 ] 	Batch(2300/7879) done. Loss: 0.0379  lr:0.000001
[ Thu Jul 11 04:51:29 2024 ] 	Batch(2400/7879) done. Loss: 0.0966  lr:0.000001
[ Thu Jul 11 04:51:52 2024 ] 
Training: Epoch [17/120], Step [2499], Loss: 0.21837785840034485, Training Accuracy: 96.195
[ Thu Jul 11 04:51:52 2024 ] 	Batch(2500/7879) done. Loss: 0.0868  lr:0.000001
[ Thu Jul 11 04:52:15 2024 ] 	Batch(2600/7879) done. Loss: 0.0317  lr:0.000001
[ Thu Jul 11 04:52:38 2024 ] 	Batch(2700/7879) done. Loss: 0.4787  lr:0.000001
[ Thu Jul 11 04:53:00 2024 ] 	Batch(2800/7879) done. Loss: 0.5566  lr:0.000001
[ Thu Jul 11 04:53:23 2024 ] 	Batch(2900/7879) done. Loss: 0.2086  lr:0.000001
[ Thu Jul 11 04:53:46 2024 ] 
Training: Epoch [17/120], Step [2999], Loss: 0.10812872648239136, Training Accuracy: 96.26666666666667
[ Thu Jul 11 04:53:46 2024 ] 	Batch(3000/7879) done. Loss: 0.1251  lr:0.000001
[ Thu Jul 11 04:54:09 2024 ] 	Batch(3100/7879) done. Loss: 0.1119  lr:0.000001
[ Thu Jul 11 04:54:32 2024 ] 	Batch(3200/7879) done. Loss: 0.3260  lr:0.000001
[ Thu Jul 11 04:54:55 2024 ] 	Batch(3300/7879) done. Loss: 0.6893  lr:0.000001
[ Thu Jul 11 04:55:17 2024 ] 	Batch(3400/7879) done. Loss: 0.0163  lr:0.000001
[ Thu Jul 11 04:55:40 2024 ] 
Training: Epoch [17/120], Step [3499], Loss: 0.06001405045390129, Training Accuracy: 96.21428571428572
[ Thu Jul 11 04:55:40 2024 ] 	Batch(3500/7879) done. Loss: 0.0405  lr:0.000001
[ Thu Jul 11 04:56:03 2024 ] 	Batch(3600/7879) done. Loss: 0.1214  lr:0.000001
[ Thu Jul 11 04:56:26 2024 ] 	Batch(3700/7879) done. Loss: 0.0490  lr:0.000001
[ Thu Jul 11 04:56:48 2024 ] 	Batch(3800/7879) done. Loss: 0.0433  lr:0.000001
[ Thu Jul 11 04:57:11 2024 ] 	Batch(3900/7879) done. Loss: 0.0367  lr:0.000001
[ Thu Jul 11 04:57:34 2024 ] 
Training: Epoch [17/120], Step [3999], Loss: 0.012341389432549477, Training Accuracy: 96.221875
[ Thu Jul 11 04:57:34 2024 ] 	Batch(4000/7879) done. Loss: 0.3028  lr:0.000001
[ Thu Jul 11 04:57:57 2024 ] 	Batch(4100/7879) done. Loss: 0.2248  lr:0.000001
[ Thu Jul 11 04:58:20 2024 ] 	Batch(4200/7879) done. Loss: 0.0168  lr:0.000001
[ Thu Jul 11 04:58:42 2024 ] 	Batch(4300/7879) done. Loss: 0.0419  lr:0.000001
[ Thu Jul 11 04:59:05 2024 ] 	Batch(4400/7879) done. Loss: 0.2254  lr:0.000001
[ Thu Jul 11 04:59:28 2024 ] 
Training: Epoch [17/120], Step [4499], Loss: 0.5404685139656067, Training Accuracy: 96.19166666666666
[ Thu Jul 11 04:59:28 2024 ] 	Batch(4500/7879) done. Loss: 0.0060  lr:0.000001
[ Thu Jul 11 04:59:51 2024 ] 	Batch(4600/7879) done. Loss: 0.0454  lr:0.000001
[ Thu Jul 11 05:00:15 2024 ] 	Batch(4700/7879) done. Loss: 0.0010  lr:0.000001
[ Thu Jul 11 05:00:38 2024 ] 	Batch(4800/7879) done. Loss: 0.3153  lr:0.000001
[ Thu Jul 11 05:01:01 2024 ] 	Batch(4900/7879) done. Loss: 0.1128  lr:0.000001
[ Thu Jul 11 05:01:24 2024 ] 
Training: Epoch [17/120], Step [4999], Loss: 0.03768203780055046, Training Accuracy: 96.2125
[ Thu Jul 11 05:01:24 2024 ] 	Batch(5000/7879) done. Loss: 0.0069  lr:0.000001
[ Thu Jul 11 05:01:47 2024 ] 	Batch(5100/7879) done. Loss: 0.1042  lr:0.000001
[ Thu Jul 11 05:02:10 2024 ] 	Batch(5200/7879) done. Loss: 0.1064  lr:0.000001
[ Thu Jul 11 05:02:34 2024 ] 	Batch(5300/7879) done. Loss: 0.2836  lr:0.000001
[ Thu Jul 11 05:02:58 2024 ] 	Batch(5400/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul 11 05:03:21 2024 ] 
Training: Epoch [17/120], Step [5499], Loss: 0.015389292500913143, Training Accuracy: 96.20454545454545
[ Thu Jul 11 05:03:21 2024 ] 	Batch(5500/7879) done. Loss: 0.0044  lr:0.000001
[ Thu Jul 11 05:03:45 2024 ] 	Batch(5600/7879) done. Loss: 0.0933  lr:0.000001
[ Thu Jul 11 05:04:08 2024 ] 	Batch(5700/7879) done. Loss: 0.0251  lr:0.000001
[ Thu Jul 11 05:04:31 2024 ] 	Batch(5800/7879) done. Loss: 0.0078  lr:0.000001
[ Thu Jul 11 05:04:53 2024 ] 	Batch(5900/7879) done. Loss: 0.0700  lr:0.000001
[ Thu Jul 11 05:05:16 2024 ] 
Training: Epoch [17/120], Step [5999], Loss: 0.06354570388793945, Training Accuracy: 96.25625
[ Thu Jul 11 05:05:16 2024 ] 	Batch(6000/7879) done. Loss: 0.0355  lr:0.000001
[ Thu Jul 11 05:05:40 2024 ] 	Batch(6100/7879) done. Loss: 0.0269  lr:0.000001
[ Thu Jul 11 05:06:03 2024 ] 	Batch(6200/7879) done. Loss: 0.0620  lr:0.000001
[ Thu Jul 11 05:06:26 2024 ] 	Batch(6300/7879) done. Loss: 0.0219  lr:0.000001
[ Thu Jul 11 05:06:50 2024 ] 	Batch(6400/7879) done. Loss: 0.0091  lr:0.000001
[ Thu Jul 11 05:07:13 2024 ] 
Training: Epoch [17/120], Step [6499], Loss: 0.064104825258255, Training Accuracy: 96.27115384615385
[ Thu Jul 11 05:07:13 2024 ] 	Batch(6500/7879) done. Loss: 0.0274  lr:0.000001
[ Thu Jul 11 05:07:37 2024 ] 	Batch(6600/7879) done. Loss: 0.1137  lr:0.000001
[ Thu Jul 11 05:08:00 2024 ] 	Batch(6700/7879) done. Loss: 0.0673  lr:0.000001
[ Thu Jul 11 05:08:23 2024 ] 	Batch(6800/7879) done. Loss: 0.1519  lr:0.000001
[ Thu Jul 11 05:08:46 2024 ] 	Batch(6900/7879) done. Loss: 0.1388  lr:0.000001
[ Thu Jul 11 05:09:08 2024 ] 
Training: Epoch [17/120], Step [6999], Loss: 0.23911383748054504, Training Accuracy: 96.23214285714286
[ Thu Jul 11 05:09:09 2024 ] 	Batch(7000/7879) done. Loss: 0.1162  lr:0.000001
[ Thu Jul 11 05:09:31 2024 ] 	Batch(7100/7879) done. Loss: 0.2060  lr:0.000001
[ Thu Jul 11 05:09:54 2024 ] 	Batch(7200/7879) done. Loss: 0.0424  lr:0.000001
[ Thu Jul 11 05:10:16 2024 ] 	Batch(7300/7879) done. Loss: 0.3135  lr:0.000001
[ Thu Jul 11 05:10:39 2024 ] 	Batch(7400/7879) done. Loss: 0.0507  lr:0.000001
[ Thu Jul 11 05:11:01 2024 ] 
Training: Epoch [17/120], Step [7499], Loss: 0.19667531549930573, Training Accuracy: 96.23333333333333
[ Thu Jul 11 05:11:02 2024 ] 	Batch(7500/7879) done. Loss: 0.1376  lr:0.000001
[ Thu Jul 11 05:11:24 2024 ] 	Batch(7600/7879) done. Loss: 0.1482  lr:0.000001
[ Thu Jul 11 05:11:47 2024 ] 	Batch(7700/7879) done. Loss: 0.2831  lr:0.000001
[ Thu Jul 11 05:12:09 2024 ] 	Batch(7800/7879) done. Loss: 0.0665  lr:0.000001
[ Thu Jul 11 05:12:27 2024 ] 	Mean training loss: 0.1445.
[ Thu Jul 11 05:12:27 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 05:12:27 2024 ] Training epoch: 19
[ Thu Jul 11 05:12:28 2024 ] 	Batch(0/7879) done. Loss: 0.1734  lr:0.000001
[ Thu Jul 11 05:12:50 2024 ] 	Batch(100/7879) done. Loss: 0.2367  lr:0.000001
[ Thu Jul 11 05:13:13 2024 ] 	Batch(200/7879) done. Loss: 0.0428  lr:0.000001
[ Thu Jul 11 05:13:35 2024 ] 	Batch(300/7879) done. Loss: 0.7055  lr:0.000001
[ Thu Jul 11 05:13:58 2024 ] 	Batch(400/7879) done. Loss: 0.0633  lr:0.000001
[ Thu Jul 11 05:14:20 2024 ] 
Training: Epoch [18/120], Step [499], Loss: 0.038659464567899704, Training Accuracy: 96.2
[ Thu Jul 11 05:14:21 2024 ] 	Batch(500/7879) done. Loss: 0.0499  lr:0.000001
[ Thu Jul 11 05:14:43 2024 ] 	Batch(600/7879) done. Loss: 0.1714  lr:0.000001
[ Thu Jul 11 05:15:06 2024 ] 	Batch(700/7879) done. Loss: 0.0468  lr:0.000001
[ Thu Jul 11 05:15:29 2024 ] 	Batch(800/7879) done. Loss: 0.2261  lr:0.000001
[ Thu Jul 11 05:15:51 2024 ] 	Batch(900/7879) done. Loss: 0.0330  lr:0.000001
[ Thu Jul 11 05:16:14 2024 ] 
Training: Epoch [18/120], Step [999], Loss: 0.12772278487682343, Training Accuracy: 96.375
[ Thu Jul 11 05:16:14 2024 ] 	Batch(1000/7879) done. Loss: 0.0271  lr:0.000001
[ Thu Jul 11 05:16:36 2024 ] 	Batch(1100/7879) done. Loss: 0.0422  lr:0.000001
[ Thu Jul 11 05:16:59 2024 ] 	Batch(1200/7879) done. Loss: 0.0529  lr:0.000001
[ Thu Jul 11 05:17:22 2024 ] 	Batch(1300/7879) done. Loss: 0.0162  lr:0.000001
[ Thu Jul 11 05:17:44 2024 ] 	Batch(1400/7879) done. Loss: 0.6595  lr:0.000001
[ Thu Jul 11 05:18:07 2024 ] 
Training: Epoch [18/120], Step [1499], Loss: 0.23140199482440948, Training Accuracy: 96.23333333333333
[ Thu Jul 11 05:18:07 2024 ] 	Batch(1500/7879) done. Loss: 0.0616  lr:0.000001
[ Thu Jul 11 05:18:30 2024 ] 	Batch(1600/7879) done. Loss: 0.0217  lr:0.000001
[ Thu Jul 11 05:18:53 2024 ] 	Batch(1700/7879) done. Loss: 0.0393  lr:0.000001
[ Thu Jul 11 05:19:16 2024 ] 	Batch(1800/7879) done. Loss: 0.0856  lr:0.000001
[ Thu Jul 11 05:19:40 2024 ] 	Batch(1900/7879) done. Loss: 0.5652  lr:0.000001
[ Thu Jul 11 05:20:03 2024 ] 
Training: Epoch [18/120], Step [1999], Loss: 0.005177016369998455, Training Accuracy: 96.3
[ Thu Jul 11 05:20:03 2024 ] 	Batch(2000/7879) done. Loss: 0.0710  lr:0.000001
[ Thu Jul 11 05:20:26 2024 ] 	Batch(2100/7879) done. Loss: 0.2122  lr:0.000001
[ Thu Jul 11 05:20:48 2024 ] 	Batch(2200/7879) done. Loss: 0.0507  lr:0.000001
[ Thu Jul 11 05:21:11 2024 ] 	Batch(2300/7879) done. Loss: 0.0632  lr:0.000001
[ Thu Jul 11 05:21:34 2024 ] 	Batch(2400/7879) done. Loss: 0.3549  lr:0.000001
[ Thu Jul 11 05:21:56 2024 ] 
Training: Epoch [18/120], Step [2499], Loss: 0.21763528883457184, Training Accuracy: 96.39
[ Thu Jul 11 05:21:57 2024 ] 	Batch(2500/7879) done. Loss: 0.0064  lr:0.000001
[ Thu Jul 11 05:22:19 2024 ] 	Batch(2600/7879) done. Loss: 0.0452  lr:0.000001
[ Thu Jul 11 05:22:42 2024 ] 	Batch(2700/7879) done. Loss: 0.0157  lr:0.000001
[ Thu Jul 11 05:23:04 2024 ] 	Batch(2800/7879) done. Loss: 0.0796  lr:0.000001
[ Thu Jul 11 05:23:27 2024 ] 	Batch(2900/7879) done. Loss: 0.0646  lr:0.000001
[ Thu Jul 11 05:23:50 2024 ] 
Training: Epoch [18/120], Step [2999], Loss: 0.0026831943541765213, Training Accuracy: 96.35000000000001
[ Thu Jul 11 05:23:50 2024 ] 	Batch(3000/7879) done. Loss: 0.0602  lr:0.000001
[ Thu Jul 11 05:24:13 2024 ] 	Batch(3100/7879) done. Loss: 0.4490  lr:0.000001
[ Thu Jul 11 05:24:36 2024 ] 	Batch(3200/7879) done. Loss: 0.0737  lr:0.000001
[ Thu Jul 11 05:24:59 2024 ] 	Batch(3300/7879) done. Loss: 0.0645  lr:0.000001
[ Thu Jul 11 05:25:22 2024 ] 	Batch(3400/7879) done. Loss: 0.0331  lr:0.000001
[ Thu Jul 11 05:25:44 2024 ] 
Training: Epoch [18/120], Step [3499], Loss: 0.09378648549318314, Training Accuracy: 96.31785714285715
[ Thu Jul 11 05:25:45 2024 ] 	Batch(3500/7879) done. Loss: 0.0641  lr:0.000001
[ Thu Jul 11 05:26:08 2024 ] 	Batch(3600/7879) done. Loss: 0.0146  lr:0.000001
[ Thu Jul 11 05:26:31 2024 ] 	Batch(3700/7879) done. Loss: 0.4170  lr:0.000001
[ Thu Jul 11 05:26:54 2024 ] 	Batch(3800/7879) done. Loss: 0.2664  lr:0.000001
[ Thu Jul 11 05:27:17 2024 ] 	Batch(3900/7879) done. Loss: 0.0535  lr:0.000001
[ Thu Jul 11 05:27:40 2024 ] 
Training: Epoch [18/120], Step [3999], Loss: 0.14960111677646637, Training Accuracy: 96.34375
[ Thu Jul 11 05:27:41 2024 ] 	Batch(4000/7879) done. Loss: 0.1439  lr:0.000001
[ Thu Jul 11 05:28:04 2024 ] 	Batch(4100/7879) done. Loss: 0.2375  lr:0.000001
[ Thu Jul 11 05:28:27 2024 ] 	Batch(4200/7879) done. Loss: 0.0577  lr:0.000001
[ Thu Jul 11 05:28:50 2024 ] 	Batch(4300/7879) done. Loss: 0.1367  lr:0.000001
[ Thu Jul 11 05:29:13 2024 ] 	Batch(4400/7879) done. Loss: 0.4112  lr:0.000001
[ Thu Jul 11 05:29:36 2024 ] 
Training: Epoch [18/120], Step [4499], Loss: 0.46334078907966614, Training Accuracy: 96.31944444444444
[ Thu Jul 11 05:29:36 2024 ] 	Batch(4500/7879) done. Loss: 0.0021  lr:0.000001
[ Thu Jul 11 05:29:59 2024 ] 	Batch(4600/7879) done. Loss: 0.5068  lr:0.000001
[ Thu Jul 11 05:30:21 2024 ] 	Batch(4700/7879) done. Loss: 0.2614  lr:0.000001
[ Thu Jul 11 05:30:44 2024 ] 	Batch(4800/7879) done. Loss: 0.3397  lr:0.000001
[ Thu Jul 11 05:31:07 2024 ] 	Batch(4900/7879) done. Loss: 0.0630  lr:0.000001
[ Thu Jul 11 05:31:29 2024 ] 
Training: Epoch [18/120], Step [4999], Loss: 0.12473883479833603, Training Accuracy: 96.37
[ Thu Jul 11 05:31:30 2024 ] 	Batch(5000/7879) done. Loss: 0.3773  lr:0.000001
[ Thu Jul 11 05:31:52 2024 ] 	Batch(5100/7879) done. Loss: 0.0353  lr:0.000001
[ Thu Jul 11 05:32:15 2024 ] 	Batch(5200/7879) done. Loss: 0.1098  lr:0.000001
[ Thu Jul 11 05:32:38 2024 ] 	Batch(5300/7879) done. Loss: 0.3750  lr:0.000001
[ Thu Jul 11 05:33:01 2024 ] 	Batch(5400/7879) done. Loss: 0.0116  lr:0.000001
[ Thu Jul 11 05:33:23 2024 ] 
Training: Epoch [18/120], Step [5499], Loss: 0.0900685042142868, Training Accuracy: 96.38181818181818
[ Thu Jul 11 05:33:23 2024 ] 	Batch(5500/7879) done. Loss: 0.2892  lr:0.000001
[ Thu Jul 11 05:33:46 2024 ] 	Batch(5600/7879) done. Loss: 0.0215  lr:0.000001
[ Thu Jul 11 05:34:09 2024 ] 	Batch(5700/7879) done. Loss: 0.2558  lr:0.000001
[ Thu Jul 11 05:34:32 2024 ] 	Batch(5800/7879) done. Loss: 0.0028  lr:0.000001
[ Thu Jul 11 05:34:55 2024 ] 	Batch(5900/7879) done. Loss: 0.0311  lr:0.000001
[ Thu Jul 11 05:35:17 2024 ] 
Training: Epoch [18/120], Step [5999], Loss: 0.3528304100036621, Training Accuracy: 96.35000000000001
[ Thu Jul 11 05:35:18 2024 ] 	Batch(6000/7879) done. Loss: 0.4192  lr:0.000001
[ Thu Jul 11 05:35:41 2024 ] 	Batch(6100/7879) done. Loss: 0.0138  lr:0.000001
[ Thu Jul 11 05:36:04 2024 ] 	Batch(6200/7879) done. Loss: 0.4322  lr:0.000001
[ Thu Jul 11 05:36:27 2024 ] 	Batch(6300/7879) done. Loss: 0.0222  lr:0.000001
[ Thu Jul 11 05:36:50 2024 ] 	Batch(6400/7879) done. Loss: 0.0044  lr:0.000001
[ Thu Jul 11 05:37:13 2024 ] 
Training: Epoch [18/120], Step [6499], Loss: 0.007577751763164997, Training Accuracy: 96.3576923076923
[ Thu Jul 11 05:37:13 2024 ] 	Batch(6500/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul 11 05:37:36 2024 ] 	Batch(6600/7879) done. Loss: 0.1288  lr:0.000001
[ Thu Jul 11 05:37:59 2024 ] 	Batch(6700/7879) done. Loss: 0.0571  lr:0.000001
[ Thu Jul 11 05:38:22 2024 ] 	Batch(6800/7879) done. Loss: 0.0384  lr:0.000001
[ Thu Jul 11 05:38:45 2024 ] 	Batch(6900/7879) done. Loss: 0.1236  lr:0.000001
[ Thu Jul 11 05:39:08 2024 ] 
Training: Epoch [18/120], Step [6999], Loss: 0.21190710365772247, Training Accuracy: 96.35535714285714
[ Thu Jul 11 05:39:08 2024 ] 	Batch(7000/7879) done. Loss: 0.0382  lr:0.000001
[ Thu Jul 11 05:39:31 2024 ] 	Batch(7100/7879) done. Loss: 0.2297  lr:0.000001
[ Thu Jul 11 05:39:55 2024 ] 	Batch(7200/7879) done. Loss: 0.2738  lr:0.000001
[ Thu Jul 11 05:40:18 2024 ] 	Batch(7300/7879) done. Loss: 0.0628  lr:0.000001
[ Thu Jul 11 05:40:40 2024 ] 	Batch(7400/7879) done. Loss: 0.3147  lr:0.000001
[ Thu Jul 11 05:41:03 2024 ] 
Training: Epoch [18/120], Step [7499], Loss: 0.34972578287124634, Training Accuracy: 96.34166666666667
[ Thu Jul 11 05:41:03 2024 ] 	Batch(7500/7879) done. Loss: 0.0040  lr:0.000001
[ Thu Jul 11 05:41:25 2024 ] 	Batch(7600/7879) done. Loss: 0.0607  lr:0.000001
[ Thu Jul 11 05:41:49 2024 ] 	Batch(7700/7879) done. Loss: 0.1923  lr:0.000001
[ Thu Jul 11 05:42:11 2024 ] 	Batch(7800/7879) done. Loss: 0.0272  lr:0.000001
[ Thu Jul 11 05:42:29 2024 ] 	Mean training loss: 0.1397.
[ Thu Jul 11 05:42:29 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 05:42:29 2024 ] Training epoch: 20
[ Thu Jul 11 05:42:30 2024 ] 	Batch(0/7879) done. Loss: 0.7302  lr:0.000001
[ Thu Jul 11 05:42:53 2024 ] 	Batch(100/7879) done. Loss: 0.1309  lr:0.000001
[ Thu Jul 11 05:43:16 2024 ] 	Batch(200/7879) done. Loss: 0.0088  lr:0.000001
[ Thu Jul 11 05:43:39 2024 ] 	Batch(300/7879) done. Loss: 0.1998  lr:0.000001
[ Thu Jul 11 05:44:03 2024 ] 	Batch(400/7879) done. Loss: 0.6485  lr:0.000001
[ Thu Jul 11 05:44:26 2024 ] 
Training: Epoch [19/120], Step [499], Loss: 0.15272006392478943, Training Accuracy: 96.22500000000001
[ Thu Jul 11 05:44:26 2024 ] 	Batch(500/7879) done. Loss: 0.0721  lr:0.000001
[ Thu Jul 11 05:44:49 2024 ] 	Batch(600/7879) done. Loss: 0.1520  lr:0.000001
[ Thu Jul 11 05:45:12 2024 ] 	Batch(700/7879) done. Loss: 0.0339  lr:0.000001
[ Thu Jul 11 05:45:35 2024 ] 	Batch(800/7879) done. Loss: 0.1718  lr:0.000001
[ Thu Jul 11 05:45:57 2024 ] 	Batch(900/7879) done. Loss: 0.3272  lr:0.000001
[ Thu Jul 11 05:46:20 2024 ] 
Training: Epoch [19/120], Step [999], Loss: 0.14007890224456787, Training Accuracy: 96.05
[ Thu Jul 11 05:46:20 2024 ] 	Batch(1000/7879) done. Loss: 0.0132  lr:0.000001
[ Thu Jul 11 05:46:43 2024 ] 	Batch(1100/7879) done. Loss: 0.0798  lr:0.000001
[ Thu Jul 11 05:47:05 2024 ] 	Batch(1200/7879) done. Loss: 0.1387  lr:0.000001
[ Thu Jul 11 05:47:28 2024 ] 	Batch(1300/7879) done. Loss: 0.0161  lr:0.000001
[ Thu Jul 11 05:47:50 2024 ] 	Batch(1400/7879) done. Loss: 0.4165  lr:0.000001
[ Thu Jul 11 05:48:13 2024 ] 
Training: Epoch [19/120], Step [1499], Loss: 0.08403417468070984, Training Accuracy: 96.06666666666666
[ Thu Jul 11 05:48:13 2024 ] 	Batch(1500/7879) done. Loss: 0.1015  lr:0.000001
[ Thu Jul 11 05:48:36 2024 ] 	Batch(1600/7879) done. Loss: 0.1409  lr:0.000001
[ Thu Jul 11 05:48:58 2024 ] 	Batch(1700/7879) done. Loss: 0.2021  lr:0.000001
[ Thu Jul 11 05:49:21 2024 ] 	Batch(1800/7879) done. Loss: 0.1560  lr:0.000001
[ Thu Jul 11 05:49:44 2024 ] 	Batch(1900/7879) done. Loss: 0.2601  lr:0.000001
[ Thu Jul 11 05:50:06 2024 ] 
Training: Epoch [19/120], Step [1999], Loss: 0.11837578564882278, Training Accuracy: 96.08125
[ Thu Jul 11 05:50:06 2024 ] 	Batch(2000/7879) done. Loss: 0.1981  lr:0.000001
[ Thu Jul 11 05:50:30 2024 ] 	Batch(2100/7879) done. Loss: 0.1048  lr:0.000001
[ Thu Jul 11 05:50:53 2024 ] 	Batch(2200/7879) done. Loss: 0.3517  lr:0.000001
[ Thu Jul 11 05:51:16 2024 ] 	Batch(2300/7879) done. Loss: 0.2787  lr:0.000001
[ Thu Jul 11 05:51:40 2024 ] 	Batch(2400/7879) done. Loss: 0.0983  lr:0.000001
[ Thu Jul 11 05:52:03 2024 ] 
Training: Epoch [19/120], Step [2499], Loss: 0.03414585813879967, Training Accuracy: 95.99
[ Thu Jul 11 05:52:03 2024 ] 	Batch(2500/7879) done. Loss: 0.8451  lr:0.000001
[ Thu Jul 11 05:52:26 2024 ] 	Batch(2600/7879) done. Loss: 0.1172  lr:0.000001
[ Thu Jul 11 05:52:50 2024 ] 	Batch(2700/7879) done. Loss: 0.0445  lr:0.000001
[ Thu Jul 11 05:53:13 2024 ] 	Batch(2800/7879) done. Loss: 0.0512  lr:0.000001
[ Thu Jul 11 05:53:36 2024 ] 	Batch(2900/7879) done. Loss: 0.1217  lr:0.000001
[ Thu Jul 11 05:54:00 2024 ] 
Training: Epoch [19/120], Step [2999], Loss: 0.07180188596248627, Training Accuracy: 95.96666666666667
[ Thu Jul 11 05:54:00 2024 ] 	Batch(3000/7879) done. Loss: 0.3472  lr:0.000001
[ Thu Jul 11 05:54:23 2024 ] 	Batch(3100/7879) done. Loss: 0.3111  lr:0.000001
[ Thu Jul 11 05:54:47 2024 ] 	Batch(3200/7879) done. Loss: 0.0992  lr:0.000001
[ Thu Jul 11 05:55:10 2024 ] 	Batch(3300/7879) done. Loss: 0.0693  lr:0.000001
[ Thu Jul 11 05:55:33 2024 ] 	Batch(3400/7879) done. Loss: 0.3981  lr:0.000001
[ Thu Jul 11 05:55:56 2024 ] 
Training: Epoch [19/120], Step [3499], Loss: 0.08749723434448242, Training Accuracy: 95.99642857142857
[ Thu Jul 11 05:55:56 2024 ] 	Batch(3500/7879) done. Loss: 0.0688  lr:0.000001
[ Thu Jul 11 05:56:19 2024 ] 	Batch(3600/7879) done. Loss: 0.0203  lr:0.000001
[ Thu Jul 11 05:56:41 2024 ] 	Batch(3700/7879) done. Loss: 0.1579  lr:0.000001
[ Thu Jul 11 05:57:04 2024 ] 	Batch(3800/7879) done. Loss: 0.1719  lr:0.000001
[ Thu Jul 11 05:57:26 2024 ] 	Batch(3900/7879) done. Loss: 0.0624  lr:0.000001
[ Thu Jul 11 05:57:49 2024 ] 
Training: Epoch [19/120], Step [3999], Loss: 0.0136962179094553, Training Accuracy: 95.990625
[ Thu Jul 11 05:57:49 2024 ] 	Batch(4000/7879) done. Loss: 0.0162  lr:0.000001
[ Thu Jul 11 05:58:12 2024 ] 	Batch(4100/7879) done. Loss: 0.0839  lr:0.000001
[ Thu Jul 11 05:58:34 2024 ] 	Batch(4200/7879) done. Loss: 0.0035  lr:0.000001
[ Thu Jul 11 05:58:57 2024 ] 	Batch(4300/7879) done. Loss: 0.1034  lr:0.000001
[ Thu Jul 11 05:59:20 2024 ] 	Batch(4400/7879) done. Loss: 0.0437  lr:0.000001
[ Thu Jul 11 05:59:42 2024 ] 
Training: Epoch [19/120], Step [4499], Loss: 0.18742306530475616, Training Accuracy: 96.00277777777778
[ Thu Jul 11 05:59:43 2024 ] 	Batch(4500/7879) done. Loss: 0.0334  lr:0.000001
[ Thu Jul 11 06:00:05 2024 ] 	Batch(4600/7879) done. Loss: 0.5861  lr:0.000001
[ Thu Jul 11 06:00:28 2024 ] 	Batch(4700/7879) done. Loss: 0.0542  lr:0.000001
[ Thu Jul 11 06:00:51 2024 ] 	Batch(4800/7879) done. Loss: 0.6197  lr:0.000001
[ Thu Jul 11 06:01:13 2024 ] 	Batch(4900/7879) done. Loss: 0.0682  lr:0.000001
[ Thu Jul 11 06:01:36 2024 ] 
Training: Epoch [19/120], Step [4999], Loss: 0.011702722869813442, Training Accuracy: 96.0
[ Thu Jul 11 06:01:36 2024 ] 	Batch(5000/7879) done. Loss: 0.3503  lr:0.000001
[ Thu Jul 11 06:01:59 2024 ] 	Batch(5100/7879) done. Loss: 0.0536  lr:0.000001
[ Thu Jul 11 06:02:22 2024 ] 	Batch(5200/7879) done. Loss: 0.0679  lr:0.000001
[ Thu Jul 11 06:02:45 2024 ] 	Batch(5300/7879) done. Loss: 0.0969  lr:0.000001
[ Thu Jul 11 06:03:08 2024 ] 	Batch(5400/7879) done. Loss: 0.0434  lr:0.000001
[ Thu Jul 11 06:03:31 2024 ] 
Training: Epoch [19/120], Step [5499], Loss: 0.3376423120498657, Training Accuracy: 96.025
[ Thu Jul 11 06:03:31 2024 ] 	Batch(5500/7879) done. Loss: 0.1619  lr:0.000001
[ Thu Jul 11 06:03:54 2024 ] 	Batch(5600/7879) done. Loss: 0.0834  lr:0.000001
[ Thu Jul 11 06:04:17 2024 ] 	Batch(5700/7879) done. Loss: 0.0211  lr:0.000001
[ Thu Jul 11 06:04:41 2024 ] 	Batch(5800/7879) done. Loss: 0.0007  lr:0.000001
[ Thu Jul 11 06:05:04 2024 ] 	Batch(5900/7879) done. Loss: 0.2176  lr:0.000001
[ Thu Jul 11 06:05:27 2024 ] 
Training: Epoch [19/120], Step [5999], Loss: 0.18442216515541077, Training Accuracy: 96.0375
[ Thu Jul 11 06:05:27 2024 ] 	Batch(6000/7879) done. Loss: 0.3892  lr:0.000001
[ Thu Jul 11 06:05:50 2024 ] 	Batch(6100/7879) done. Loss: 0.7720  lr:0.000001
[ Thu Jul 11 06:06:13 2024 ] 	Batch(6200/7879) done. Loss: 0.2767  lr:0.000001
[ Thu Jul 11 06:06:36 2024 ] 	Batch(6300/7879) done. Loss: 0.0331  lr:0.000001
[ Thu Jul 11 06:06:59 2024 ] 	Batch(6400/7879) done. Loss: 0.2035  lr:0.000001
[ Thu Jul 11 06:07:21 2024 ] 
Training: Epoch [19/120], Step [6499], Loss: 0.07765346765518188, Training Accuracy: 96.01538461538462
[ Thu Jul 11 06:07:21 2024 ] 	Batch(6500/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul 11 06:07:44 2024 ] 	Batch(6600/7879) done. Loss: 0.0162  lr:0.000001
[ Thu Jul 11 06:08:07 2024 ] 	Batch(6700/7879) done. Loss: 0.0052  lr:0.000001
[ Thu Jul 11 06:08:31 2024 ] 	Batch(6800/7879) done. Loss: 0.0491  lr:0.000001
[ Thu Jul 11 06:08:54 2024 ] 	Batch(6900/7879) done. Loss: 0.3120  lr:0.000001
[ Thu Jul 11 06:09:17 2024 ] 
Training: Epoch [19/120], Step [6999], Loss: 0.16389992833137512, Training Accuracy: 96.00892857142857
[ Thu Jul 11 06:09:17 2024 ] 	Batch(7000/7879) done. Loss: 0.3061  lr:0.000001
[ Thu Jul 11 06:09:40 2024 ] 	Batch(7100/7879) done. Loss: 0.0483  lr:0.000001
[ Thu Jul 11 06:10:03 2024 ] 	Batch(7200/7879) done. Loss: 0.0283  lr:0.000001
[ Thu Jul 11 06:10:27 2024 ] 	Batch(7300/7879) done. Loss: 0.0302  lr:0.000001
[ Thu Jul 11 06:10:50 2024 ] 	Batch(7400/7879) done. Loss: 0.0496  lr:0.000001
[ Thu Jul 11 06:11:13 2024 ] 
Training: Epoch [19/120], Step [7499], Loss: 0.07232246547937393, Training Accuracy: 96.03166666666667
[ Thu Jul 11 06:11:13 2024 ] 	Batch(7500/7879) done. Loss: 0.3635  lr:0.000001
[ Thu Jul 11 06:11:36 2024 ] 	Batch(7600/7879) done. Loss: 0.1708  lr:0.000001
[ Thu Jul 11 06:11:59 2024 ] 	Batch(7700/7879) done. Loss: 0.0314  lr:0.000001
[ Thu Jul 11 06:12:22 2024 ] 	Batch(7800/7879) done. Loss: 0.2236  lr:0.000001
[ Thu Jul 11 06:12:40 2024 ] 	Mean training loss: 0.1444.
[ Thu Jul 11 06:12:40 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Thu Jul 11 06:12:40 2024 ] Eval epoch: 20
[ Thu Jul 11 06:18:37 2024 ] 	Mean val loss of 6365 batches: 1.0322996446997283.
[ Thu Jul 11 06:18:37 2024 ] 
Validation: Epoch [19/120], Samples [39211.0/50919], Loss: 0.4435960650444031, Validation Accuracy: 77.00661835464169
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 1 : 192 / 275 = 69 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 2 : 230 / 273 = 84 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 3 : 224 / 273 = 82 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 4 : 230 / 275 = 83 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 5 : 225 / 275 = 81 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 6 : 213 / 275 = 77 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 7 : 252 / 273 = 92 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 8 : 264 / 273 = 96 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 9 : 185 / 273 = 67 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 10 : 116 / 273 = 42 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 11 : 155 / 272 = 56 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 12 : 214 / 271 = 78 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 13 : 265 / 275 = 96 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 14 : 258 / 276 = 93 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 15 : 206 / 273 = 75 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 16 : 173 / 274 = 63 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 17 : 241 / 273 = 88 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 18 : 232 / 274 = 84 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 19 : 238 / 272 = 87 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 20 : 252 / 273 = 92 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 21 : 230 / 274 = 83 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 22 : 249 / 274 = 90 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 23 : 258 / 276 = 93 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 24 : 236 / 274 = 86 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 25 : 263 / 275 = 95 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 26 : 269 / 276 = 97 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 27 : 232 / 275 = 84 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 28 : 176 / 275 = 64 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 29 : 139 / 275 = 50 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 30 : 170 / 276 = 61 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 31 : 231 / 276 = 83 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 32 : 244 / 276 = 88 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 33 : 237 / 276 = 85 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 34 : 240 / 276 = 86 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 35 : 242 / 275 = 88 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 36 : 230 / 276 = 83 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 37 : 251 / 276 = 90 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 38 : 242 / 276 = 87 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 39 : 242 / 276 = 87 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 40 : 191 / 276 = 69 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 41 : 263 / 276 = 95 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 42 : 247 / 275 = 89 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 43 : 173 / 276 = 62 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 44 : 248 / 276 = 89 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 45 : 259 / 276 = 93 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 46 : 210 / 276 = 76 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 47 : 201 / 275 = 73 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 48 : 216 / 275 = 78 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 49 : 209 / 274 = 76 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 50 : 239 / 276 = 86 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 51 : 249 / 276 = 90 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 52 : 246 / 276 = 89 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 53 : 239 / 276 = 86 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 54 : 258 / 274 = 94 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 55 : 236 / 276 = 85 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 56 : 249 / 275 = 90 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 57 : 269 / 276 = 97 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 58 : 262 / 273 = 95 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 59 : 252 / 276 = 91 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 60 : 478 / 561 = 85 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 61 : 464 / 566 = 81 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 62 : 418 / 572 = 73 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 63 : 474 / 570 = 83 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 64 : 428 / 574 = 74 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 65 : 498 / 573 = 86 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 66 : 418 / 573 = 72 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 67 : 402 / 575 = 69 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 68 : 394 / 575 = 68 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 69 : 484 / 575 = 84 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 70 : 220 / 575 = 38 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 71 : 242 / 575 = 42 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 72 : 97 / 571 = 16 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 73 : 239 / 570 = 41 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 74 : 367 / 569 = 64 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 75 : 354 / 573 = 61 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 76 : 353 / 574 = 61 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 77 : 356 / 573 = 62 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 78 : 455 / 575 = 79 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 79 : 549 / 574 = 95 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 80 : 457 / 573 = 79 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 81 : 330 / 575 = 57 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 82 : 365 / 575 = 63 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 83 : 254 / 572 = 44 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 84 : 435 / 574 = 75 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 85 : 375 / 574 = 65 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 86 : 497 / 575 = 86 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 87 : 489 / 576 = 84 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 88 : 399 / 575 = 69 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 89 : 428 / 576 = 74 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 90 : 228 / 574 = 39 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 91 : 448 / 568 = 78 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 92 : 408 / 576 = 70 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 93 : 353 / 573 = 61 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 94 : 526 / 574 = 91 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 95 : 530 / 575 = 92 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 96 : 557 / 575 = 96 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 97 : 551 / 574 = 95 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 98 : 540 / 575 = 93 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 99 : 532 / 574 = 92 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 100 : 459 / 574 = 79 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 101 : 527 / 574 = 91 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 102 : 323 / 575 = 56 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 103 : 477 / 576 = 82 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 104 : 291 / 575 = 50 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 105 : 267 / 575 = 46 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 106 : 317 / 576 = 55 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 107 : 507 / 576 = 88 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 108 : 462 / 575 = 80 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 109 : 411 / 575 = 71 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 110 : 519 / 575 = 90 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 111 : 538 / 576 = 93 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 112 : 542 / 575 = 94 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 113 : 520 / 576 = 90 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 114 : 512 / 576 = 88 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 115 : 523 / 576 = 90 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 116 : 472 / 575 = 82 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 117 : 484 / 575 = 84 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 118 : 465 / 575 = 80 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 119 : 500 / 576 = 86 %
[ Thu Jul 11 06:18:37 2024 ] Accuracy of 120 : 241 / 274 = 87 %
[ Thu Jul 11 06:18:37 2024 ] Training epoch: 21
[ Thu Jul 11 06:18:37 2024 ] 	Batch(0/7879) done. Loss: 0.3108  lr:0.000001
[ Thu Jul 11 06:19:00 2024 ] 	Batch(100/7879) done. Loss: 0.2952  lr:0.000001
[ Thu Jul 11 06:19:23 2024 ] 	Batch(200/7879) done. Loss: 0.0442  lr:0.000001
[ Thu Jul 11 06:19:46 2024 ] 	Batch(300/7879) done. Loss: 0.0141  lr:0.000001
[ Thu Jul 11 06:20:09 2024 ] 	Batch(400/7879) done. Loss: 0.0103  lr:0.000001
[ Thu Jul 11 06:20:32 2024 ] 
Training: Epoch [20/120], Step [499], Loss: 0.09040423482656479, Training Accuracy: 96.35000000000001
[ Thu Jul 11 06:20:33 2024 ] 	Batch(500/7879) done. Loss: 0.0093  lr:0.000001
[ Thu Jul 11 06:20:55 2024 ] 	Batch(600/7879) done. Loss: 0.1147  lr:0.000001
[ Thu Jul 11 06:21:18 2024 ] 	Batch(700/7879) done. Loss: 0.0166  lr:0.000001
[ Thu Jul 11 06:21:42 2024 ] 	Batch(800/7879) done. Loss: 0.7289  lr:0.000001
[ Thu Jul 11 06:22:05 2024 ] 	Batch(900/7879) done. Loss: 0.1336  lr:0.000001
[ Thu Jul 11 06:22:27 2024 ] 
Training: Epoch [20/120], Step [999], Loss: 0.011663069948554039, Training Accuracy: 96.15
[ Thu Jul 11 06:22:28 2024 ] 	Batch(1000/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul 11 06:22:51 2024 ] 	Batch(1100/7879) done. Loss: 0.0144  lr:0.000001
[ Thu Jul 11 06:23:14 2024 ] 	Batch(1200/7879) done. Loss: 0.0122  lr:0.000001
[ Thu Jul 11 06:23:37 2024 ] 	Batch(1300/7879) done. Loss: 0.2076  lr:0.000001
[ Thu Jul 11 06:24:00 2024 ] 	Batch(1400/7879) done. Loss: 0.2344  lr:0.000001
[ Thu Jul 11 06:24:23 2024 ] 
Training: Epoch [20/120], Step [1499], Loss: 0.015649830922484398, Training Accuracy: 96.24166666666667
[ Thu Jul 11 06:24:23 2024 ] 	Batch(1500/7879) done. Loss: 0.0466  lr:0.000001
[ Thu Jul 11 06:24:46 2024 ] 	Batch(1600/7879) done. Loss: 0.7414  lr:0.000001
[ Thu Jul 11 06:25:10 2024 ] 	Batch(1700/7879) done. Loss: 0.0317  lr:0.000001
[ Thu Jul 11 06:25:33 2024 ] 	Batch(1800/7879) done. Loss: 0.2990  lr:0.000001
[ Thu Jul 11 06:25:57 2024 ] 	Batch(1900/7879) done. Loss: 0.4989  lr:0.000001
[ Thu Jul 11 06:26:20 2024 ] 
Training: Epoch [20/120], Step [1999], Loss: 0.00836899969726801, Training Accuracy: 96.2125
[ Thu Jul 11 06:26:20 2024 ] 	Batch(2000/7879) done. Loss: 0.0405  lr:0.000001
[ Thu Jul 11 06:26:43 2024 ] 	Batch(2100/7879) done. Loss: 0.0176  lr:0.000001
[ Thu Jul 11 06:27:05 2024 ] 	Batch(2200/7879) done. Loss: 0.2911  lr:0.000001
[ Thu Jul 11 06:27:28 2024 ] 	Batch(2300/7879) done. Loss: 0.0134  lr:0.000001
[ Thu Jul 11 06:27:50 2024 ] 	Batch(2400/7879) done. Loss: 0.4257  lr:0.000001
[ Thu Jul 11 06:28:13 2024 ] 
Training: Epoch [20/120], Step [2499], Loss: 0.033824093639850616, Training Accuracy: 96.30499999999999
[ Thu Jul 11 06:28:13 2024 ] 	Batch(2500/7879) done. Loss: 0.1345  lr:0.000001
[ Thu Jul 11 06:28:36 2024 ] 	Batch(2600/7879) done. Loss: 0.0131  lr:0.000001
[ Thu Jul 11 06:28:58 2024 ] 	Batch(2700/7879) done. Loss: 0.0159  lr:0.000001
[ Thu Jul 11 06:29:21 2024 ] 	Batch(2800/7879) done. Loss: 0.1242  lr:0.000001
[ Thu Jul 11 06:29:43 2024 ] 	Batch(2900/7879) done. Loss: 0.2625  lr:0.000001
[ Thu Jul 11 06:30:06 2024 ] 
Training: Epoch [20/120], Step [2999], Loss: 0.08517886698246002, Training Accuracy: 96.26249999999999
[ Thu Jul 11 06:30:06 2024 ] 	Batch(3000/7879) done. Loss: 0.0202  lr:0.000001
[ Thu Jul 11 06:30:29 2024 ] 	Batch(3100/7879) done. Loss: 0.0777  lr:0.000001
[ Thu Jul 11 06:30:51 2024 ] 	Batch(3200/7879) done. Loss: 0.2584  lr:0.000001
[ Thu Jul 11 06:31:14 2024 ] 	Batch(3300/7879) done. Loss: 0.0164  lr:0.000001
[ Thu Jul 11 06:31:37 2024 ] 	Batch(3400/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul 11 06:32:00 2024 ] 
Training: Epoch [20/120], Step [3499], Loss: 0.04950859397649765, Training Accuracy: 96.25714285714285
[ Thu Jul 11 06:32:00 2024 ] 	Batch(3500/7879) done. Loss: 0.0167  lr:0.000001
[ Thu Jul 11 06:32:23 2024 ] 	Batch(3600/7879) done. Loss: 0.0799  lr:0.000001
[ Thu Jul 11 06:32:47 2024 ] 	Batch(3700/7879) done. Loss: 0.0086  lr:0.000001
[ Thu Jul 11 06:33:10 2024 ] 	Batch(3800/7879) done. Loss: 0.0766  lr:0.000001
[ Thu Jul 11 06:33:33 2024 ] 	Batch(3900/7879) done. Loss: 0.2107  lr:0.000001
[ Thu Jul 11 06:33:56 2024 ] 
Training: Epoch [20/120], Step [3999], Loss: 0.0362844355404377, Training Accuracy: 96.246875
[ Thu Jul 11 06:33:57 2024 ] 	Batch(4000/7879) done. Loss: 0.0124  lr:0.000001
[ Thu Jul 11 06:34:20 2024 ] 	Batch(4100/7879) done. Loss: 0.0549  lr:0.000001
[ Thu Jul 11 06:34:43 2024 ] 	Batch(4200/7879) done. Loss: 0.2702  lr:0.000001
[ Thu Jul 11 06:35:07 2024 ] 	Batch(4300/7879) done. Loss: 0.5521  lr:0.000001
[ Thu Jul 11 06:35:30 2024 ] 	Batch(4400/7879) done. Loss: 0.0268  lr:0.000001
[ Thu Jul 11 06:35:53 2024 ] 
Training: Epoch [20/120], Step [4499], Loss: 0.01113865152001381, Training Accuracy: 96.2638888888889
[ Thu Jul 11 06:35:53 2024 ] 	Batch(4500/7879) done. Loss: 0.4751  lr:0.000001
[ Thu Jul 11 06:36:15 2024 ] 	Batch(4600/7879) done. Loss: 0.2609  lr:0.000001
[ Thu Jul 11 06:36:38 2024 ] 	Batch(4700/7879) done. Loss: 0.0616  lr:0.000001
[ Thu Jul 11 06:37:01 2024 ] 	Batch(4800/7879) done. Loss: 0.1569  lr:0.000001
[ Thu Jul 11 06:37:23 2024 ] 	Batch(4900/7879) done. Loss: 0.0318  lr:0.000001
[ Thu Jul 11 06:37:46 2024 ] 
Training: Epoch [20/120], Step [4999], Loss: 0.1190020963549614, Training Accuracy: 96.28999999999999
[ Thu Jul 11 06:37:46 2024 ] 	Batch(5000/7879) done. Loss: 0.0032  lr:0.000001
[ Thu Jul 11 06:38:08 2024 ] 	Batch(5100/7879) done. Loss: 0.1047  lr:0.000001
[ Thu Jul 11 06:38:32 2024 ] 	Batch(5200/7879) done. Loss: 0.1580  lr:0.000001
[ Thu Jul 11 06:38:55 2024 ] 	Batch(5300/7879) done. Loss: 0.0305  lr:0.000001
[ Thu Jul 11 06:39:18 2024 ] 	Batch(5400/7879) done. Loss: 0.2597  lr:0.000001
[ Thu Jul 11 06:39:41 2024 ] 
Training: Epoch [20/120], Step [5499], Loss: 0.12302563339471817, Training Accuracy: 96.31136363636364
[ Thu Jul 11 06:39:41 2024 ] 	Batch(5500/7879) done. Loss: 0.0762  lr:0.000001
[ Thu Jul 11 06:40:05 2024 ] 	Batch(5600/7879) done. Loss: 0.6334  lr:0.000001
[ Thu Jul 11 06:40:28 2024 ] 	Batch(5700/7879) done. Loss: 0.0394  lr:0.000001
[ Thu Jul 11 06:40:51 2024 ] 	Batch(5800/7879) done. Loss: 0.0411  lr:0.000001
[ Thu Jul 11 06:41:14 2024 ] 	Batch(5900/7879) done. Loss: 0.1611  lr:0.000001
[ Thu Jul 11 06:41:37 2024 ] 
Training: Epoch [20/120], Step [5999], Loss: 0.3000459671020508, Training Accuracy: 96.28125
[ Thu Jul 11 06:41:38 2024 ] 	Batch(6000/7879) done. Loss: 0.1358  lr:0.000001
[ Thu Jul 11 06:42:00 2024 ] 	Batch(6100/7879) done. Loss: 0.0917  lr:0.000001
[ Thu Jul 11 06:42:23 2024 ] 	Batch(6200/7879) done. Loss: 0.1083  lr:0.000001
[ Thu Jul 11 06:42:46 2024 ] 	Batch(6300/7879) done. Loss: 0.3484  lr:0.000001
[ Thu Jul 11 06:43:09 2024 ] 	Batch(6400/7879) done. Loss: 0.0112  lr:0.000001
[ Thu Jul 11 06:43:31 2024 ] 
Training: Epoch [20/120], Step [6499], Loss: 0.07401834428310394, Training Accuracy: 96.24423076923077
[ Thu Jul 11 06:43:31 2024 ] 	Batch(6500/7879) done. Loss: 0.1740  lr:0.000001
[ Thu Jul 11 06:43:54 2024 ] 	Batch(6600/7879) done. Loss: 0.1148  lr:0.000001
[ Thu Jul 11 06:44:17 2024 ] 	Batch(6700/7879) done. Loss: 0.0817  lr:0.000001
[ Thu Jul 11 06:44:40 2024 ] 	Batch(6800/7879) done. Loss: 0.2367  lr:0.000001
[ Thu Jul 11 06:45:02 2024 ] 	Batch(6900/7879) done. Loss: 0.1815  lr:0.000001
[ Thu Jul 11 06:45:25 2024 ] 
Training: Epoch [20/120], Step [6999], Loss: 0.3607766330242157, Training Accuracy: 96.25714285714285
[ Thu Jul 11 06:45:25 2024 ] 	Batch(7000/7879) done. Loss: 0.1089  lr:0.000001
[ Thu Jul 11 06:45:48 2024 ] 	Batch(7100/7879) done. Loss: 0.0712  lr:0.000001
[ Thu Jul 11 06:46:11 2024 ] 	Batch(7200/7879) done. Loss: 0.0070  lr:0.000001
[ Thu Jul 11 06:46:34 2024 ] 	Batch(7300/7879) done. Loss: 0.9133  lr:0.000001
[ Thu Jul 11 06:46:56 2024 ] 	Batch(7400/7879) done. Loss: 0.0566  lr:0.000001
[ Thu Jul 11 06:47:19 2024 ] 
Training: Epoch [20/120], Step [7499], Loss: 0.1907302737236023, Training Accuracy: 96.22833333333334
[ Thu Jul 11 06:47:19 2024 ] 	Batch(7500/7879) done. Loss: 0.0506  lr:0.000001
[ Thu Jul 11 06:47:42 2024 ] 	Batch(7600/7879) done. Loss: 0.0039  lr:0.000001
[ Thu Jul 11 06:48:05 2024 ] 	Batch(7700/7879) done. Loss: 0.0602  lr:0.000001
[ Thu Jul 11 06:48:28 2024 ] 	Batch(7800/7879) done. Loss: 0.5473  lr:0.000001
[ Thu Jul 11 06:48:45 2024 ] 	Mean training loss: 0.1383.
[ Thu Jul 11 06:48:45 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Thu Jul 11 06:48:46 2024 ] Training epoch: 22
[ Thu Jul 11 06:48:46 2024 ] 	Batch(0/7879) done. Loss: 0.0792  lr:0.000001
[ Thu Jul 11 06:49:09 2024 ] 	Batch(100/7879) done. Loss: 0.0174  lr:0.000001
[ Thu Jul 11 06:49:32 2024 ] 	Batch(200/7879) done. Loss: 0.2676  lr:0.000001
[ Thu Jul 11 06:49:55 2024 ] 	Batch(300/7879) done. Loss: 0.2415  lr:0.000001
[ Thu Jul 11 06:50:18 2024 ] 	Batch(400/7879) done. Loss: 0.2743  lr:0.000001
[ Thu Jul 11 06:50:41 2024 ] 
Training: Epoch [21/120], Step [499], Loss: 0.03546908497810364, Training Accuracy: 96.475
[ Thu Jul 11 06:50:41 2024 ] 	Batch(500/7879) done. Loss: 0.0326  lr:0.000001
[ Thu Jul 11 06:51:04 2024 ] 	Batch(600/7879) done. Loss: 0.0979  lr:0.000001
[ Thu Jul 11 06:51:27 2024 ] 	Batch(700/7879) done. Loss: 0.0825  lr:0.000001
[ Thu Jul 11 06:51:50 2024 ] 	Batch(800/7879) done. Loss: 0.3007  lr:0.000001
[ Thu Jul 11 06:52:13 2024 ] 	Batch(900/7879) done. Loss: 0.0971  lr:0.000001
[ Thu Jul 11 06:52:36 2024 ] 
Training: Epoch [21/120], Step [999], Loss: 0.03195200487971306, Training Accuracy: 96.4125
[ Thu Jul 11 06:52:36 2024 ] 	Batch(1000/7879) done. Loss: 0.1961  lr:0.000001
[ Thu Jul 11 06:52:59 2024 ] 	Batch(1100/7879) done. Loss: 0.0390  lr:0.000001
[ Thu Jul 11 06:53:22 2024 ] 	Batch(1200/7879) done. Loss: 0.3774  lr:0.000001
[ Thu Jul 11 06:53:45 2024 ] 	Batch(1300/7879) done. Loss: 0.3626  lr:0.000001
[ Thu Jul 11 06:54:08 2024 ] 	Batch(1400/7879) done. Loss: 0.2764  lr:0.000001
[ Thu Jul 11 06:54:31 2024 ] 
Training: Epoch [21/120], Step [1499], Loss: 0.03243574872612953, Training Accuracy: 96.2
[ Thu Jul 11 06:54:31 2024 ] 	Batch(1500/7879) done. Loss: 0.1017  lr:0.000001
[ Thu Jul 11 06:54:55 2024 ] 	Batch(1600/7879) done. Loss: 0.0232  lr:0.000001
[ Thu Jul 11 06:55:18 2024 ] 	Batch(1700/7879) done. Loss: 0.0579  lr:0.000001
[ Thu Jul 11 06:55:42 2024 ] 	Batch(1800/7879) done. Loss: 0.0709  lr:0.000001
[ Thu Jul 11 06:56:06 2024 ] 	Batch(1900/7879) done. Loss: 0.0528  lr:0.000001
[ Thu Jul 11 06:56:30 2024 ] 
Training: Epoch [21/120], Step [1999], Loss: 0.015992548316717148, Training Accuracy: 96.35000000000001
[ Thu Jul 11 06:56:30 2024 ] 	Batch(2000/7879) done. Loss: 0.0420  lr:0.000001
[ Thu Jul 11 06:56:53 2024 ] 	Batch(2100/7879) done. Loss: 0.0180  lr:0.000001
[ Thu Jul 11 06:57:16 2024 ] 	Batch(2200/7879) done. Loss: 0.2615  lr:0.000001
[ Thu Jul 11 06:57:39 2024 ] 	Batch(2300/7879) done. Loss: 0.2747  lr:0.000001
[ Thu Jul 11 06:58:02 2024 ] 	Batch(2400/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul 11 06:58:25 2024 ] 
Training: Epoch [21/120], Step [2499], Loss: 0.33841797709465027, Training Accuracy: 96.25
[ Thu Jul 11 06:58:25 2024 ] 	Batch(2500/7879) done. Loss: 0.0084  lr:0.000001
[ Thu Jul 11 06:58:48 2024 ] 	Batch(2600/7879) done. Loss: 0.0113  lr:0.000001
[ Thu Jul 11 06:59:11 2024 ] 	Batch(2700/7879) done. Loss: 0.0459  lr:0.000001
[ Thu Jul 11 06:59:34 2024 ] 	Batch(2800/7879) done. Loss: 0.1085  lr:0.000001
[ Thu Jul 11 06:59:58 2024 ] 	Batch(2900/7879) done. Loss: 0.3025  lr:0.000001
[ Thu Jul 11 07:00:22 2024 ] 
Training: Epoch [21/120], Step [2999], Loss: 0.030693968757987022, Training Accuracy: 96.25833333333334
[ Thu Jul 11 07:00:22 2024 ] 	Batch(3000/7879) done. Loss: 0.3028  lr:0.000001
[ Thu Jul 11 07:00:46 2024 ] 	Batch(3100/7879) done. Loss: 0.1016  lr:0.000001
[ Thu Jul 11 07:01:09 2024 ] 	Batch(3200/7879) done. Loss: 0.1035  lr:0.000001
[ Thu Jul 11 07:01:32 2024 ] 	Batch(3300/7879) done. Loss: 0.0403  lr:0.000001
[ Thu Jul 11 07:01:55 2024 ] 	Batch(3400/7879) done. Loss: 0.1690  lr:0.000001
[ Thu Jul 11 07:02:18 2024 ] 
Training: Epoch [21/120], Step [3499], Loss: 0.05199158191680908, Training Accuracy: 96.28928571428571
[ Thu Jul 11 07:02:18 2024 ] 	Batch(3500/7879) done. Loss: 0.1310  lr:0.000001
[ Thu Jul 11 07:02:42 2024 ] 	Batch(3600/7879) done. Loss: 0.2052  lr:0.000001
[ Thu Jul 11 07:03:05 2024 ] 	Batch(3700/7879) done. Loss: 0.0178  lr:0.000001
[ Thu Jul 11 07:03:29 2024 ] 	Batch(3800/7879) done. Loss: 0.0813  lr:0.000001
[ Thu Jul 11 07:03:53 2024 ] 	Batch(3900/7879) done. Loss: 0.0610  lr:0.000001
[ Thu Jul 11 07:04:16 2024 ] 
Training: Epoch [21/120], Step [3999], Loss: 0.2284906804561615, Training Accuracy: 96.23125
[ Thu Jul 11 07:04:17 2024 ] 	Batch(4000/7879) done. Loss: 0.0659  lr:0.000001
[ Thu Jul 11 07:04:41 2024 ] 	Batch(4100/7879) done. Loss: 0.0535  lr:0.000001
[ Thu Jul 11 07:05:04 2024 ] 	Batch(4200/7879) done. Loss: 0.0471  lr:0.000001
[ Thu Jul 11 07:05:28 2024 ] 	Batch(4300/7879) done. Loss: 0.0464  lr:0.000001
[ Thu Jul 11 07:05:52 2024 ] 	Batch(4400/7879) done. Loss: 0.0020  lr:0.000001
[ Thu Jul 11 07:06:15 2024 ] 
Training: Epoch [21/120], Step [4499], Loss: 0.04892919212579727, Training Accuracy: 96.18333333333334
[ Thu Jul 11 07:06:15 2024 ] 	Batch(4500/7879) done. Loss: 0.5503  lr:0.000001
[ Thu Jul 11 07:06:38 2024 ] 	Batch(4600/7879) done. Loss: 0.0018  lr:0.000001
[ Thu Jul 11 07:07:01 2024 ] 	Batch(4700/7879) done. Loss: 0.3152  lr:0.000001
[ Thu Jul 11 07:07:24 2024 ] 	Batch(4800/7879) done. Loss: 0.0932  lr:0.000001
[ Thu Jul 11 07:07:47 2024 ] 	Batch(4900/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul 11 07:08:10 2024 ] 
Training: Epoch [21/120], Step [4999], Loss: 0.3335493803024292, Training Accuracy: 96.19
[ Thu Jul 11 07:08:10 2024 ] 	Batch(5000/7879) done. Loss: 0.1880  lr:0.000001
[ Thu Jul 11 07:08:33 2024 ] 	Batch(5100/7879) done. Loss: 0.0437  lr:0.000001
[ Thu Jul 11 07:08:56 2024 ] 	Batch(5200/7879) done. Loss: 0.2889  lr:0.000001
[ Thu Jul 11 07:09:19 2024 ] 	Batch(5300/7879) done. Loss: 0.0827  lr:0.000001
[ Thu Jul 11 07:09:42 2024 ] 	Batch(5400/7879) done. Loss: 0.2301  lr:0.000001
[ Thu Jul 11 07:10:05 2024 ] 
Training: Epoch [21/120], Step [5499], Loss: 0.7680099010467529, Training Accuracy: 96.18863636363636
[ Thu Jul 11 07:10:05 2024 ] 	Batch(5500/7879) done. Loss: 0.1104  lr:0.000001
[ Thu Jul 11 07:10:28 2024 ] 	Batch(5600/7879) done. Loss: 0.1823  lr:0.000001
[ Thu Jul 11 07:10:50 2024 ] 	Batch(5700/7879) done. Loss: 0.0375  lr:0.000001
[ Thu Jul 11 07:11:13 2024 ] 	Batch(5800/7879) done. Loss: 0.1417  lr:0.000001
[ Thu Jul 11 07:11:35 2024 ] 	Batch(5900/7879) done. Loss: 0.0541  lr:0.000001
[ Thu Jul 11 07:11:57 2024 ] 
Training: Epoch [21/120], Step [5999], Loss: 0.33715784549713135, Training Accuracy: 96.20208333333333
[ Thu Jul 11 07:11:58 2024 ] 	Batch(6000/7879) done. Loss: 0.0326  lr:0.000001
[ Thu Jul 11 07:12:20 2024 ] 	Batch(6100/7879) done. Loss: 0.0515  lr:0.000001
[ Thu Jul 11 07:12:43 2024 ] 	Batch(6200/7879) done. Loss: 0.0045  lr:0.000001
[ Thu Jul 11 07:13:05 2024 ] 	Batch(6300/7879) done. Loss: 0.1999  lr:0.000001
[ Thu Jul 11 07:13:28 2024 ] 	Batch(6400/7879) done. Loss: 0.0039  lr:0.000001
[ Thu Jul 11 07:13:50 2024 ] 
Training: Epoch [21/120], Step [6499], Loss: 0.05021842569112778, Training Accuracy: 96.22884615384616
[ Thu Jul 11 07:13:51 2024 ] 	Batch(6500/7879) done. Loss: 0.0680  lr:0.000001
[ Thu Jul 11 07:14:13 2024 ] 	Batch(6600/7879) done. Loss: 0.2140  lr:0.000001
[ Thu Jul 11 07:14:36 2024 ] 	Batch(6700/7879) done. Loss: 0.2335  lr:0.000001
[ Thu Jul 11 07:14:58 2024 ] 	Batch(6800/7879) done. Loss: 0.5239  lr:0.000001
[ Thu Jul 11 07:15:21 2024 ] 	Batch(6900/7879) done. Loss: 0.1375  lr:0.000001
[ Thu Jul 11 07:15:43 2024 ] 
Training: Epoch [21/120], Step [6999], Loss: 0.1362977772951126, Training Accuracy: 96.21071428571429
[ Thu Jul 11 07:15:44 2024 ] 	Batch(7000/7879) done. Loss: 0.0248  lr:0.000001
[ Thu Jul 11 07:16:06 2024 ] 	Batch(7100/7879) done. Loss: 0.0497  lr:0.000001
[ Thu Jul 11 07:16:29 2024 ] 	Batch(7200/7879) done. Loss: 0.0791  lr:0.000001
[ Thu Jul 11 07:16:51 2024 ] 	Batch(7300/7879) done. Loss: 0.1020  lr:0.000001
[ Thu Jul 11 07:17:14 2024 ] 	Batch(7400/7879) done. Loss: 0.2937  lr:0.000001
[ Thu Jul 11 07:17:36 2024 ] 
Training: Epoch [21/120], Step [7499], Loss: 0.239909827709198, Training Accuracy: 96.20333333333333
[ Thu Jul 11 07:17:36 2024 ] 	Batch(7500/7879) done. Loss: 0.1527  lr:0.000001
[ Thu Jul 11 07:17:59 2024 ] 	Batch(7600/7879) done. Loss: 0.0151  lr:0.000001
[ Thu Jul 11 07:18:22 2024 ] 	Batch(7700/7879) done. Loss: 0.5335  lr:0.000001
[ Thu Jul 11 07:18:44 2024 ] 	Batch(7800/7879) done. Loss: 0.0029  lr:0.000001
[ Thu Jul 11 07:19:02 2024 ] 	Mean training loss: 0.1420.
[ Thu Jul 11 07:19:02 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Thu Jul 11 07:19:02 2024 ] Training epoch: 23
[ Thu Jul 11 07:19:02 2024 ] 	Batch(0/7879) done. Loss: 0.0981  lr:0.000001
[ Thu Jul 11 07:19:25 2024 ] 	Batch(100/7879) done. Loss: 0.1241  lr:0.000001
[ Thu Jul 11 07:19:48 2024 ] 	Batch(200/7879) done. Loss: 0.0353  lr:0.000001
[ Thu Jul 11 07:20:10 2024 ] 	Batch(300/7879) done. Loss: 0.2051  lr:0.000001
[ Thu Jul 11 07:20:33 2024 ] 	Batch(400/7879) done. Loss: 0.2118  lr:0.000001
[ Thu Jul 11 07:20:55 2024 ] 
Training: Epoch [22/120], Step [499], Loss: 0.05519863963127136, Training Accuracy: 96.125
[ Thu Jul 11 07:20:55 2024 ] 	Batch(500/7879) done. Loss: 0.3214  lr:0.000001
[ Thu Jul 11 07:21:18 2024 ] 	Batch(600/7879) done. Loss: 0.0167  lr:0.000001
[ Thu Jul 11 07:21:40 2024 ] 	Batch(700/7879) done. Loss: 0.0039  lr:0.000001
[ Thu Jul 11 07:22:03 2024 ] 	Batch(800/7879) done. Loss: 0.0658  lr:0.000001
[ Thu Jul 11 07:22:26 2024 ] 	Batch(900/7879) done. Loss: 0.0761  lr:0.000001
[ Thu Jul 11 07:22:48 2024 ] 
Training: Epoch [22/120], Step [999], Loss: 0.01733127050101757, Training Accuracy: 96.0875
[ Thu Jul 11 07:22:48 2024 ] 	Batch(1000/7879) done. Loss: 0.1715  lr:0.000001
[ Thu Jul 11 07:23:11 2024 ] 	Batch(1100/7879) done. Loss: 0.0130  lr:0.000001
[ Thu Jul 11 07:23:34 2024 ] 	Batch(1200/7879) done. Loss: 0.2927  lr:0.000001
[ Thu Jul 11 07:23:56 2024 ] 	Batch(1300/7879) done. Loss: 0.0651  lr:0.000001
[ Thu Jul 11 07:24:19 2024 ] 	Batch(1400/7879) done. Loss: 0.9396  lr:0.000001
[ Thu Jul 11 07:24:41 2024 ] 
Training: Epoch [22/120], Step [1499], Loss: 0.24871651828289032, Training Accuracy: 96.13333333333334
[ Thu Jul 11 07:24:41 2024 ] 	Batch(1500/7879) done. Loss: 0.0178  lr:0.000001
[ Thu Jul 11 07:25:04 2024 ] 	Batch(1600/7879) done. Loss: 0.4401  lr:0.000001
[ Thu Jul 11 07:25:27 2024 ] 	Batch(1700/7879) done. Loss: 0.0259  lr:0.000001
[ Thu Jul 11 07:25:49 2024 ] 	Batch(1800/7879) done. Loss: 0.4636  lr:0.000001
[ Thu Jul 11 07:26:12 2024 ] 	Batch(1900/7879) done. Loss: 0.0930  lr:0.000001
[ Thu Jul 11 07:26:34 2024 ] 
Training: Epoch [22/120], Step [1999], Loss: 0.05708018317818642, Training Accuracy: 96.15625
[ Thu Jul 11 07:26:35 2024 ] 	Batch(2000/7879) done. Loss: 0.0456  lr:0.000001
[ Thu Jul 11 07:26:58 2024 ] 	Batch(2100/7879) done. Loss: 0.1668  lr:0.000001
[ Thu Jul 11 07:27:20 2024 ] 	Batch(2200/7879) done. Loss: 0.0169  lr:0.000001
[ Thu Jul 11 07:27:43 2024 ] 	Batch(2300/7879) done. Loss: 0.5668  lr:0.000001
[ Thu Jul 11 07:28:06 2024 ] 	Batch(2400/7879) done. Loss: 0.0378  lr:0.000001
[ Thu Jul 11 07:28:28 2024 ] 
Training: Epoch [22/120], Step [2499], Loss: 0.06305867433547974, Training Accuracy: 96.19
[ Thu Jul 11 07:28:28 2024 ] 	Batch(2500/7879) done. Loss: 0.2451  lr:0.000001
[ Thu Jul 11 07:28:51 2024 ] 	Batch(2600/7879) done. Loss: 0.2541  lr:0.000001
[ Thu Jul 11 07:29:13 2024 ] 	Batch(2700/7879) done. Loss: 0.0697  lr:0.000001
[ Thu Jul 11 07:29:36 2024 ] 	Batch(2800/7879) done. Loss: 0.0346  lr:0.000001
[ Thu Jul 11 07:29:59 2024 ] 	Batch(2900/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul 11 07:30:21 2024 ] 
Training: Epoch [22/120], Step [2999], Loss: 0.012883839197456837, Training Accuracy: 96.27916666666667
[ Thu Jul 11 07:30:21 2024 ] 	Batch(3000/7879) done. Loss: 0.1762  lr:0.000001
[ Thu Jul 11 07:30:44 2024 ] 	Batch(3100/7879) done. Loss: 0.4004  lr:0.000001
[ Thu Jul 11 07:31:06 2024 ] 	Batch(3200/7879) done. Loss: 0.0193  lr:0.000001
[ Thu Jul 11 07:31:29 2024 ] 	Batch(3300/7879) done. Loss: 0.0547  lr:0.000001
[ Thu Jul 11 07:31:51 2024 ] 	Batch(3400/7879) done. Loss: 0.2793  lr:0.000001
[ Thu Jul 11 07:32:14 2024 ] 
Training: Epoch [22/120], Step [3499], Loss: 0.32949694991111755, Training Accuracy: 96.26785714285714
[ Thu Jul 11 07:32:14 2024 ] 	Batch(3500/7879) done. Loss: 0.0085  lr:0.000001
[ Thu Jul 11 07:32:37 2024 ] 	Batch(3600/7879) done. Loss: 0.0090  lr:0.000001
[ Thu Jul 11 07:32:59 2024 ] 	Batch(3700/7879) done. Loss: 0.3139  lr:0.000001
[ Thu Jul 11 07:33:22 2024 ] 	Batch(3800/7879) done. Loss: 0.0907  lr:0.000001
[ Thu Jul 11 07:33:44 2024 ] 	Batch(3900/7879) done. Loss: 0.0600  lr:0.000001
[ Thu Jul 11 07:34:07 2024 ] 
Training: Epoch [22/120], Step [3999], Loss: 0.04225396737456322, Training Accuracy: 96.29375
[ Thu Jul 11 07:34:07 2024 ] 	Batch(4000/7879) done. Loss: 0.3607  lr:0.000001
[ Thu Jul 11 07:34:30 2024 ] 	Batch(4100/7879) done. Loss: 0.3536  lr:0.000001
[ Thu Jul 11 07:34:52 2024 ] 	Batch(4200/7879) done. Loss: 0.0570  lr:0.000001
[ Thu Jul 11 07:35:15 2024 ] 	Batch(4300/7879) done. Loss: 0.0541  lr:0.000001
[ Thu Jul 11 07:35:38 2024 ] 	Batch(4400/7879) done. Loss: 0.1460  lr:0.000001
[ Thu Jul 11 07:36:00 2024 ] 
Training: Epoch [22/120], Step [4499], Loss: 0.0828024223446846, Training Accuracy: 96.30555555555556
[ Thu Jul 11 07:36:01 2024 ] 	Batch(4500/7879) done. Loss: 0.2788  lr:0.000001
[ Thu Jul 11 07:36:23 2024 ] 	Batch(4600/7879) done. Loss: 0.3089  lr:0.000001
[ Thu Jul 11 07:36:46 2024 ] 	Batch(4700/7879) done. Loss: 0.0342  lr:0.000001
[ Thu Jul 11 07:37:09 2024 ] 	Batch(4800/7879) done. Loss: 0.1663  lr:0.000001
[ Thu Jul 11 07:37:33 2024 ] 	Batch(4900/7879) done. Loss: 0.0173  lr:0.000001
[ Thu Jul 11 07:37:56 2024 ] 
Training: Epoch [22/120], Step [4999], Loss: 0.33227840065956116, Training Accuracy: 96.28750000000001
[ Thu Jul 11 07:37:56 2024 ] 	Batch(5000/7879) done. Loss: 0.0074  lr:0.000001
[ Thu Jul 11 07:38:19 2024 ] 	Batch(5100/7879) done. Loss: 0.0379  lr:0.000001
[ Thu Jul 11 07:38:43 2024 ] 	Batch(5200/7879) done. Loss: 0.0027  lr:0.000001
[ Thu Jul 11 07:39:06 2024 ] 	Batch(5300/7879) done. Loss: 0.0283  lr:0.000001
[ Thu Jul 11 07:39:28 2024 ] 	Batch(5400/7879) done. Loss: 0.3382  lr:0.000001
[ Thu Jul 11 07:39:51 2024 ] 
Training: Epoch [22/120], Step [5499], Loss: 0.020403848960995674, Training Accuracy: 96.22727272727273
[ Thu Jul 11 07:39:51 2024 ] 	Batch(5500/7879) done. Loss: 0.1106  lr:0.000001
[ Thu Jul 11 07:40:14 2024 ] 	Batch(5600/7879) done. Loss: 0.1111  lr:0.000001
[ Thu Jul 11 07:40:37 2024 ] 	Batch(5700/7879) done. Loss: 0.0487  lr:0.000001
[ Thu Jul 11 07:41:00 2024 ] 	Batch(5800/7879) done. Loss: 0.0447  lr:0.000001
[ Thu Jul 11 07:41:22 2024 ] 	Batch(5900/7879) done. Loss: 0.0913  lr:0.000001
[ Thu Jul 11 07:41:45 2024 ] 
Training: Epoch [22/120], Step [5999], Loss: 0.37850573658943176, Training Accuracy: 96.24791666666667
[ Thu Jul 11 07:41:45 2024 ] 	Batch(6000/7879) done. Loss: 0.0691  lr:0.000001
[ Thu Jul 11 07:42:08 2024 ] 	Batch(6100/7879) done. Loss: 0.0702  lr:0.000001
[ Thu Jul 11 07:42:31 2024 ] 	Batch(6200/7879) done. Loss: 0.0187  lr:0.000001
[ Thu Jul 11 07:42:54 2024 ] 	Batch(6300/7879) done. Loss: 0.1537  lr:0.000001
[ Thu Jul 11 07:43:17 2024 ] 	Batch(6400/7879) done. Loss: 0.0061  lr:0.000001
[ Thu Jul 11 07:43:39 2024 ] 
Training: Epoch [22/120], Step [6499], Loss: 0.3743857741355896, Training Accuracy: 96.24615384615385
[ Thu Jul 11 07:43:40 2024 ] 	Batch(6500/7879) done. Loss: 0.0423  lr:0.000001
[ Thu Jul 11 07:44:02 2024 ] 	Batch(6600/7879) done. Loss: 0.2307  lr:0.000001
[ Thu Jul 11 07:44:25 2024 ] 	Batch(6700/7879) done. Loss: 0.1123  lr:0.000001
[ Thu Jul 11 07:44:48 2024 ] 	Batch(6800/7879) done. Loss: 0.0049  lr:0.000001
[ Thu Jul 11 07:45:11 2024 ] 	Batch(6900/7879) done. Loss: 0.1365  lr:0.000001
[ Thu Jul 11 07:45:34 2024 ] 
Training: Epoch [22/120], Step [6999], Loss: 0.013812102377414703, Training Accuracy: 96.25
[ Thu Jul 11 07:45:34 2024 ] 	Batch(7000/7879) done. Loss: 0.0293  lr:0.000001
[ Thu Jul 11 07:45:56 2024 ] 	Batch(7100/7879) done. Loss: 0.0044  lr:0.000001
[ Thu Jul 11 07:46:19 2024 ] 	Batch(7200/7879) done. Loss: 0.0841  lr:0.000001
[ Thu Jul 11 07:46:43 2024 ] 	Batch(7300/7879) done. Loss: 0.3662  lr:0.000001
[ Thu Jul 11 07:47:06 2024 ] 	Batch(7400/7879) done. Loss: 0.1831  lr:0.000001
[ Thu Jul 11 07:47:29 2024 ] 
Training: Epoch [22/120], Step [7499], Loss: 0.05569181963801384, Training Accuracy: 96.21833333333333
[ Thu Jul 11 07:47:29 2024 ] 	Batch(7500/7879) done. Loss: 0.1998  lr:0.000001
[ Thu Jul 11 07:47:53 2024 ] 	Batch(7600/7879) done. Loss: 0.0123  lr:0.000001
[ Thu Jul 11 07:48:15 2024 ] 	Batch(7700/7879) done. Loss: 0.0065  lr:0.000001
[ Thu Jul 11 07:48:38 2024 ] 	Batch(7800/7879) done. Loss: 0.0080  lr:0.000001
[ Thu Jul 11 07:48:56 2024 ] 	Mean training loss: 0.1396.
[ Thu Jul 11 07:48:56 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 07:48:56 2024 ] Training epoch: 24
[ Thu Jul 11 07:48:56 2024 ] 	Batch(0/7879) done. Loss: 0.0327  lr:0.000001
[ Thu Jul 11 07:49:20 2024 ] 	Batch(100/7879) done. Loss: 0.1505  lr:0.000001
[ Thu Jul 11 07:49:43 2024 ] 	Batch(200/7879) done. Loss: 0.3033  lr:0.000001
[ Thu Jul 11 07:50:06 2024 ] 	Batch(300/7879) done. Loss: 0.1860  lr:0.000001
[ Thu Jul 11 07:50:29 2024 ] 	Batch(400/7879) done. Loss: 0.0071  lr:0.000001
[ Thu Jul 11 07:50:52 2024 ] 
Training: Epoch [23/120], Step [499], Loss: 0.23329398036003113, Training Accuracy: 95.95
[ Thu Jul 11 07:50:52 2024 ] 	Batch(500/7879) done. Loss: 0.0216  lr:0.000001
[ Thu Jul 11 07:51:15 2024 ] 	Batch(600/7879) done. Loss: 0.0487  lr:0.000001
[ Thu Jul 11 07:51:38 2024 ] 	Batch(700/7879) done. Loss: 0.0185  lr:0.000001
[ Thu Jul 11 07:52:01 2024 ] 	Batch(800/7879) done. Loss: 0.0364  lr:0.000001
[ Thu Jul 11 07:52:24 2024 ] 	Batch(900/7879) done. Loss: 0.4642  lr:0.000001
[ Thu Jul 11 07:52:46 2024 ] 
Training: Epoch [23/120], Step [999], Loss: 0.034046467393636703, Training Accuracy: 96.33749999999999
[ Thu Jul 11 07:52:46 2024 ] 	Batch(1000/7879) done. Loss: 0.0228  lr:0.000001
[ Thu Jul 11 07:53:09 2024 ] 	Batch(1100/7879) done. Loss: 0.0457  lr:0.000001
[ Thu Jul 11 07:53:31 2024 ] 	Batch(1200/7879) done. Loss: 0.4446  lr:0.000001
[ Thu Jul 11 07:53:54 2024 ] 	Batch(1300/7879) done. Loss: 0.3361  lr:0.000001
[ Thu Jul 11 07:54:18 2024 ] 	Batch(1400/7879) done. Loss: 0.2888  lr:0.000001
[ Thu Jul 11 07:54:41 2024 ] 
Training: Epoch [23/120], Step [1499], Loss: 0.017797274515032768, Training Accuracy: 96.22500000000001
[ Thu Jul 11 07:54:41 2024 ] 	Batch(1500/7879) done. Loss: 0.1929  lr:0.000001
[ Thu Jul 11 07:55:04 2024 ] 	Batch(1600/7879) done. Loss: 0.0125  lr:0.000001
[ Thu Jul 11 07:55:27 2024 ] 	Batch(1700/7879) done. Loss: 0.0557  lr:0.000001
[ Thu Jul 11 07:55:50 2024 ] 	Batch(1800/7879) done. Loss: 0.2962  lr:0.000001
[ Thu Jul 11 07:56:12 2024 ] 	Batch(1900/7879) done. Loss: 0.0019  lr:0.000001
[ Thu Jul 11 07:56:35 2024 ] 
Training: Epoch [23/120], Step [1999], Loss: 0.02852046675980091, Training Accuracy: 96.22500000000001
[ Thu Jul 11 07:56:35 2024 ] 	Batch(2000/7879) done. Loss: 0.1176  lr:0.000001
[ Thu Jul 11 07:56:57 2024 ] 	Batch(2100/7879) done. Loss: 0.0470  lr:0.000001
[ Thu Jul 11 07:57:20 2024 ] 	Batch(2200/7879) done. Loss: 0.2751  lr:0.000001
[ Thu Jul 11 07:57:43 2024 ] 	Batch(2300/7879) done. Loss: 0.0075  lr:0.000001
[ Thu Jul 11 07:58:05 2024 ] 	Batch(2400/7879) done. Loss: 0.1358  lr:0.000001
[ Thu Jul 11 07:58:28 2024 ] 
Training: Epoch [23/120], Step [2499], Loss: 0.22945354878902435, Training Accuracy: 96.135
[ Thu Jul 11 07:58:28 2024 ] 	Batch(2500/7879) done. Loss: 0.0134  lr:0.000001
[ Thu Jul 11 07:58:51 2024 ] 	Batch(2600/7879) done. Loss: 0.0452  lr:0.000001
[ Thu Jul 11 07:59:13 2024 ] 	Batch(2700/7879) done. Loss: 0.2680  lr:0.000001
[ Thu Jul 11 07:59:36 2024 ] 	Batch(2800/7879) done. Loss: 0.3182  lr:0.000001
[ Thu Jul 11 07:59:58 2024 ] 	Batch(2900/7879) done. Loss: 0.0481  lr:0.000001
[ Thu Jul 11 08:00:21 2024 ] 
Training: Epoch [23/120], Step [2999], Loss: 0.0932283103466034, Training Accuracy: 96.10833333333333
[ Thu Jul 11 08:00:21 2024 ] 	Batch(3000/7879) done. Loss: 0.0166  lr:0.000001
[ Thu Jul 11 08:00:44 2024 ] 	Batch(3100/7879) done. Loss: 0.1444  lr:0.000001
[ Thu Jul 11 08:01:06 2024 ] 	Batch(3200/7879) done. Loss: 0.2381  lr:0.000001
[ Thu Jul 11 08:01:29 2024 ] 	Batch(3300/7879) done. Loss: 0.0760  lr:0.000001
[ Thu Jul 11 08:01:51 2024 ] 	Batch(3400/7879) done. Loss: 0.0085  lr:0.000001
[ Thu Jul 11 08:02:14 2024 ] 
Training: Epoch [23/120], Step [3499], Loss: 0.3043564558029175, Training Accuracy: 96.09285714285714
[ Thu Jul 11 08:02:14 2024 ] 	Batch(3500/7879) done. Loss: 0.0189  lr:0.000001
[ Thu Jul 11 08:02:37 2024 ] 	Batch(3600/7879) done. Loss: 0.0210  lr:0.000001
[ Thu Jul 11 08:03:00 2024 ] 	Batch(3700/7879) done. Loss: 0.0461  lr:0.000001
[ Thu Jul 11 08:03:22 2024 ] 	Batch(3800/7879) done. Loss: 0.0560  lr:0.000001
[ Thu Jul 11 08:03:45 2024 ] 	Batch(3900/7879) done. Loss: 0.0158  lr:0.000001
[ Thu Jul 11 08:04:07 2024 ] 
Training: Epoch [23/120], Step [3999], Loss: 0.3824171721935272, Training Accuracy: 96.046875
[ Thu Jul 11 08:04:08 2024 ] 	Batch(4000/7879) done. Loss: 0.0680  lr:0.000001
[ Thu Jul 11 08:04:30 2024 ] 	Batch(4100/7879) done. Loss: 0.0090  lr:0.000001
[ Thu Jul 11 08:04:53 2024 ] 	Batch(4200/7879) done. Loss: 0.1401  lr:0.000001
[ Thu Jul 11 08:05:15 2024 ] 	Batch(4300/7879) done. Loss: 0.4662  lr:0.000001
[ Thu Jul 11 08:05:38 2024 ] 	Batch(4400/7879) done. Loss: 0.1849  lr:0.000001
[ Thu Jul 11 08:06:00 2024 ] 
Training: Epoch [23/120], Step [4499], Loss: 0.10496076941490173, Training Accuracy: 96.04722222222222
[ Thu Jul 11 08:06:01 2024 ] 	Batch(4500/7879) done. Loss: 0.0264  lr:0.000001
[ Thu Jul 11 08:06:23 2024 ] 	Batch(4600/7879) done. Loss: 0.0082  lr:0.000001
[ Thu Jul 11 08:06:46 2024 ] 	Batch(4700/7879) done. Loss: 0.1096  lr:0.000001
[ Thu Jul 11 08:07:08 2024 ] 	Batch(4800/7879) done. Loss: 0.0831  lr:0.000001
[ Thu Jul 11 08:07:31 2024 ] 	Batch(4900/7879) done. Loss: 0.0406  lr:0.000001
[ Thu Jul 11 08:07:54 2024 ] 
Training: Epoch [23/120], Step [4999], Loss: 0.15036329627037048, Training Accuracy: 96.06
[ Thu Jul 11 08:07:54 2024 ] 	Batch(5000/7879) done. Loss: 0.1058  lr:0.000001
[ Thu Jul 11 08:08:16 2024 ] 	Batch(5100/7879) done. Loss: 0.1976  lr:0.000001
[ Thu Jul 11 08:08:39 2024 ] 	Batch(5200/7879) done. Loss: 0.0939  lr:0.000001
[ Thu Jul 11 08:09:02 2024 ] 	Batch(5300/7879) done. Loss: 0.1051  lr:0.000001
[ Thu Jul 11 08:09:24 2024 ] 	Batch(5400/7879) done. Loss: 0.1751  lr:0.000001
[ Thu Jul 11 08:09:47 2024 ] 
Training: Epoch [23/120], Step [5499], Loss: 0.35852205753326416, Training Accuracy: 96.00454545454545
[ Thu Jul 11 08:09:47 2024 ] 	Batch(5500/7879) done. Loss: 0.6368  lr:0.000001
[ Thu Jul 11 08:10:10 2024 ] 	Batch(5600/7879) done. Loss: 0.1224  lr:0.000001
[ Thu Jul 11 08:10:32 2024 ] 	Batch(5700/7879) done. Loss: 0.0085  lr:0.000001
[ Thu Jul 11 08:10:55 2024 ] 	Batch(5800/7879) done. Loss: 0.0376  lr:0.000001
[ Thu Jul 11 08:11:17 2024 ] 	Batch(5900/7879) done. Loss: 0.0863  lr:0.000001
[ Thu Jul 11 08:11:40 2024 ] 
Training: Epoch [23/120], Step [5999], Loss: 0.03516046702861786, Training Accuracy: 96.0625
[ Thu Jul 11 08:11:40 2024 ] 	Batch(6000/7879) done. Loss: 0.0219  lr:0.000001
[ Thu Jul 11 08:12:03 2024 ] 	Batch(6100/7879) done. Loss: 0.2775  lr:0.000001
[ Thu Jul 11 08:12:25 2024 ] 	Batch(6200/7879) done. Loss: 0.0095  lr:0.000001
[ Thu Jul 11 08:12:48 2024 ] 	Batch(6300/7879) done. Loss: 0.0589  lr:0.000001
[ Thu Jul 11 08:13:11 2024 ] 	Batch(6400/7879) done. Loss: 0.1096  lr:0.000001
[ Thu Jul 11 08:13:33 2024 ] 
Training: Epoch [23/120], Step [6499], Loss: 0.12130739539861679, Training Accuracy: 96.1
[ Thu Jul 11 08:13:33 2024 ] 	Batch(6500/7879) done. Loss: 0.0054  lr:0.000001
[ Thu Jul 11 08:13:56 2024 ] 	Batch(6600/7879) done. Loss: 0.3776  lr:0.000001
[ Thu Jul 11 08:14:18 2024 ] 	Batch(6700/7879) done. Loss: 0.5257  lr:0.000001
[ Thu Jul 11 08:14:41 2024 ] 	Batch(6800/7879) done. Loss: 0.1555  lr:0.000001
[ Thu Jul 11 08:15:04 2024 ] 	Batch(6900/7879) done. Loss: 0.4498  lr:0.000001
[ Thu Jul 11 08:15:27 2024 ] 
Training: Epoch [23/120], Step [6999], Loss: 0.12989409267902374, Training Accuracy: 96.10178571428571
[ Thu Jul 11 08:15:27 2024 ] 	Batch(7000/7879) done. Loss: 0.5569  lr:0.000001
[ Thu Jul 11 08:15:49 2024 ] 	Batch(7100/7879) done. Loss: 0.0306  lr:0.000001
[ Thu Jul 11 08:16:12 2024 ] 	Batch(7200/7879) done. Loss: 0.3675  lr:0.000001
[ Thu Jul 11 08:16:34 2024 ] 	Batch(7300/7879) done. Loss: 0.1702  lr:0.000001
[ Thu Jul 11 08:16:57 2024 ] 	Batch(7400/7879) done. Loss: 0.1422  lr:0.000001
[ Thu Jul 11 08:17:19 2024 ] 
Training: Epoch [23/120], Step [7499], Loss: 0.17374876141548157, Training Accuracy: 96.12166666666667
[ Thu Jul 11 08:17:20 2024 ] 	Batch(7500/7879) done. Loss: 0.2196  lr:0.000001
[ Thu Jul 11 08:17:42 2024 ] 	Batch(7600/7879) done. Loss: 0.0750  lr:0.000001
[ Thu Jul 11 08:18:06 2024 ] 	Batch(7700/7879) done. Loss: 0.0365  lr:0.000001
[ Thu Jul 11 08:18:29 2024 ] 	Batch(7800/7879) done. Loss: 0.0837  lr:0.000001
[ Thu Jul 11 08:18:47 2024 ] 	Mean training loss: 0.1461.
[ Thu Jul 11 08:18:47 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 08:18:48 2024 ] Training epoch: 25
[ Thu Jul 11 08:18:48 2024 ] 	Batch(0/7879) done. Loss: 0.3431  lr:0.000001
[ Thu Jul 11 08:19:11 2024 ] 	Batch(100/7879) done. Loss: 0.4112  lr:0.000001
[ Thu Jul 11 08:19:34 2024 ] 	Batch(200/7879) done. Loss: 0.6223  lr:0.000001
[ Thu Jul 11 08:19:57 2024 ] 	Batch(300/7879) done. Loss: 0.4183  lr:0.000001
[ Thu Jul 11 08:20:20 2024 ] 	Batch(400/7879) done. Loss: 0.0701  lr:0.000001
[ Thu Jul 11 08:20:43 2024 ] 
Training: Epoch [24/120], Step [499], Loss: 0.35002434253692627, Training Accuracy: 96.2
[ Thu Jul 11 08:20:43 2024 ] 	Batch(500/7879) done. Loss: 0.0172  lr:0.000001
[ Thu Jul 11 08:21:06 2024 ] 	Batch(600/7879) done. Loss: 0.0557  lr:0.000001
[ Thu Jul 11 08:21:29 2024 ] 	Batch(700/7879) done. Loss: 0.0033  lr:0.000001
[ Thu Jul 11 08:21:53 2024 ] 	Batch(800/7879) done. Loss: 0.7492  lr:0.000001
[ Thu Jul 11 08:22:16 2024 ] 	Batch(900/7879) done. Loss: 0.0148  lr:0.000001
[ Thu Jul 11 08:22:38 2024 ] 
Training: Epoch [24/120], Step [999], Loss: 0.010136181488633156, Training Accuracy: 96.1375
[ Thu Jul 11 08:22:39 2024 ] 	Batch(1000/7879) done. Loss: 0.1858  lr:0.000001
[ Thu Jul 11 08:23:02 2024 ] 	Batch(1100/7879) done. Loss: 0.0493  lr:0.000001
[ Thu Jul 11 08:23:25 2024 ] 	Batch(1200/7879) done. Loss: 0.0053  lr:0.000001
[ Thu Jul 11 08:23:48 2024 ] 	Batch(1300/7879) done. Loss: 0.0764  lr:0.000001
[ Thu Jul 11 08:24:11 2024 ] 	Batch(1400/7879) done. Loss: 0.0908  lr:0.000001
[ Thu Jul 11 08:24:34 2024 ] 
Training: Epoch [24/120], Step [1499], Loss: 0.10088201612234116, Training Accuracy: 96.175
[ Thu Jul 11 08:24:34 2024 ] 	Batch(1500/7879) done. Loss: 0.1582  lr:0.000001
[ Thu Jul 11 08:24:57 2024 ] 	Batch(1600/7879) done. Loss: 0.1347  lr:0.000001
[ Thu Jul 11 08:25:20 2024 ] 	Batch(1700/7879) done. Loss: 0.0706  lr:0.000001
[ Thu Jul 11 08:25:43 2024 ] 	Batch(1800/7879) done. Loss: 0.0169  lr:0.000001
[ Thu Jul 11 08:26:06 2024 ] 	Batch(1900/7879) done. Loss: 0.1219  lr:0.000001
[ Thu Jul 11 08:26:29 2024 ] 
Training: Epoch [24/120], Step [1999], Loss: 0.2122676968574524, Training Accuracy: 96.11874999999999
[ Thu Jul 11 08:26:29 2024 ] 	Batch(2000/7879) done. Loss: 0.2512  lr:0.000001
[ Thu Jul 11 08:26:52 2024 ] 	Batch(2100/7879) done. Loss: 0.0445  lr:0.000001
[ Thu Jul 11 08:27:15 2024 ] 	Batch(2200/7879) done. Loss: 0.1218  lr:0.000001
[ Thu Jul 11 08:27:38 2024 ] 	Batch(2300/7879) done. Loss: 0.0314  lr:0.000001
[ Thu Jul 11 08:28:02 2024 ] 	Batch(2400/7879) done. Loss: 0.1015  lr:0.000001
[ Thu Jul 11 08:28:24 2024 ] 
Training: Epoch [24/120], Step [2499], Loss: 0.017616011202335358, Training Accuracy: 96.17
[ Thu Jul 11 08:28:25 2024 ] 	Batch(2500/7879) done. Loss: 0.4725  lr:0.000001
[ Thu Jul 11 08:28:48 2024 ] 	Batch(2600/7879) done. Loss: 0.0379  lr:0.000001
[ Thu Jul 11 08:29:10 2024 ] 	Batch(2700/7879) done. Loss: 0.0394  lr:0.000001
[ Thu Jul 11 08:29:33 2024 ] 	Batch(2800/7879) done. Loss: 0.1334  lr:0.000001
[ Thu Jul 11 08:29:56 2024 ] 	Batch(2900/7879) done. Loss: 0.0600  lr:0.000001
[ Thu Jul 11 08:30:18 2024 ] 
Training: Epoch [24/120], Step [2999], Loss: 0.09800063818693161, Training Accuracy: 96.13333333333334
[ Thu Jul 11 08:30:18 2024 ] 	Batch(3000/7879) done. Loss: 0.2495  lr:0.000001
[ Thu Jul 11 08:30:41 2024 ] 	Batch(3100/7879) done. Loss: 0.1287  lr:0.000001
[ Thu Jul 11 08:31:03 2024 ] 	Batch(3200/7879) done. Loss: 0.0199  lr:0.000001
[ Thu Jul 11 08:31:26 2024 ] 	Batch(3300/7879) done. Loss: 0.4116  lr:0.000001
[ Thu Jul 11 08:31:49 2024 ] 	Batch(3400/7879) done. Loss: 0.1562  lr:0.000001
[ Thu Jul 11 08:32:11 2024 ] 
Training: Epoch [24/120], Step [3499], Loss: 0.0026995320804417133, Training Accuracy: 96.05
[ Thu Jul 11 08:32:11 2024 ] 	Batch(3500/7879) done. Loss: 0.0140  lr:0.000001
[ Thu Jul 11 08:32:34 2024 ] 	Batch(3600/7879) done. Loss: 0.2175  lr:0.000001
[ Thu Jul 11 08:32:57 2024 ] 	Batch(3700/7879) done. Loss: 0.0172  lr:0.000001
[ Thu Jul 11 08:33:19 2024 ] 	Batch(3800/7879) done. Loss: 0.0240  lr:0.000001
[ Thu Jul 11 08:33:42 2024 ] 	Batch(3900/7879) done. Loss: 0.1616  lr:0.000001
[ Thu Jul 11 08:34:04 2024 ] 
Training: Epoch [24/120], Step [3999], Loss: 0.1067928820848465, Training Accuracy: 96.0625
[ Thu Jul 11 08:34:05 2024 ] 	Batch(4000/7879) done. Loss: 0.0216  lr:0.000001
[ Thu Jul 11 08:34:28 2024 ] 	Batch(4100/7879) done. Loss: 0.0760  lr:0.000001
[ Thu Jul 11 08:34:51 2024 ] 	Batch(4200/7879) done. Loss: 0.0467  lr:0.000001
[ Thu Jul 11 08:35:14 2024 ] 	Batch(4300/7879) done. Loss: 0.1575  lr:0.000001
[ Thu Jul 11 08:35:36 2024 ] 	Batch(4400/7879) done. Loss: 0.1641  lr:0.000001
[ Thu Jul 11 08:35:59 2024 ] 
Training: Epoch [24/120], Step [4499], Loss: 0.020157501101493835, Training Accuracy: 96.08055555555556
[ Thu Jul 11 08:35:59 2024 ] 	Batch(4500/7879) done. Loss: 0.0362  lr:0.000001
[ Thu Jul 11 08:36:21 2024 ] 	Batch(4600/7879) done. Loss: 0.1479  lr:0.000001
[ Thu Jul 11 08:36:44 2024 ] 	Batch(4700/7879) done. Loss: 0.1989  lr:0.000001
[ Thu Jul 11 08:37:07 2024 ] 	Batch(4800/7879) done. Loss: 0.0229  lr:0.000001
[ Thu Jul 11 08:37:29 2024 ] 	Batch(4900/7879) done. Loss: 0.0128  lr:0.000001
[ Thu Jul 11 08:37:52 2024 ] 
Training: Epoch [24/120], Step [4999], Loss: 0.15745511651039124, Training Accuracy: 96.1025
[ Thu Jul 11 08:37:52 2024 ] 	Batch(5000/7879) done. Loss: 0.0486  lr:0.000001
[ Thu Jul 11 08:38:14 2024 ] 	Batch(5100/7879) done. Loss: 0.2107  lr:0.000001
[ Thu Jul 11 08:38:37 2024 ] 	Batch(5200/7879) done. Loss: 0.0030  lr:0.000001
[ Thu Jul 11 08:39:00 2024 ] 	Batch(5300/7879) done. Loss: 0.0347  lr:0.000001
[ Thu Jul 11 08:39:22 2024 ] 	Batch(5400/7879) done. Loss: 0.1354  lr:0.000001
[ Thu Jul 11 08:39:45 2024 ] 
Training: Epoch [24/120], Step [5499], Loss: 0.12707224488258362, Training Accuracy: 96.125
[ Thu Jul 11 08:39:45 2024 ] 	Batch(5500/7879) done. Loss: 0.0949  lr:0.000001
[ Thu Jul 11 08:40:08 2024 ] 	Batch(5600/7879) done. Loss: 0.1237  lr:0.000001
[ Thu Jul 11 08:40:31 2024 ] 	Batch(5700/7879) done. Loss: 0.0215  lr:0.000001
[ Thu Jul 11 08:40:54 2024 ] 	Batch(5800/7879) done. Loss: 0.0180  lr:0.000001
[ Thu Jul 11 08:41:18 2024 ] 	Batch(5900/7879) done. Loss: 0.0200  lr:0.000001
[ Thu Jul 11 08:41:41 2024 ] 
Training: Epoch [24/120], Step [5999], Loss: 0.0970168262720108, Training Accuracy: 96.16041666666668
[ Thu Jul 11 08:41:41 2024 ] 	Batch(6000/7879) done. Loss: 0.1571  lr:0.000001
[ Thu Jul 11 08:42:04 2024 ] 	Batch(6100/7879) done. Loss: 0.0710  lr:0.000001
[ Thu Jul 11 08:42:26 2024 ] 	Batch(6200/7879) done. Loss: 0.2896  lr:0.000001
[ Thu Jul 11 08:42:49 2024 ] 	Batch(6300/7879) done. Loss: 0.2499  lr:0.000001
[ Thu Jul 11 08:43:12 2024 ] 	Batch(6400/7879) done. Loss: 0.1392  lr:0.000001
[ Thu Jul 11 08:43:34 2024 ] 
Training: Epoch [24/120], Step [6499], Loss: 0.18856212496757507, Training Accuracy: 96.16153846153847
[ Thu Jul 11 08:43:34 2024 ] 	Batch(6500/7879) done. Loss: 0.0942  lr:0.000001
[ Thu Jul 11 08:43:57 2024 ] 	Batch(6600/7879) done. Loss: 0.4924  lr:0.000001
[ Thu Jul 11 08:44:20 2024 ] 	Batch(6700/7879) done. Loss: 0.0053  lr:0.000001
[ Thu Jul 11 08:44:42 2024 ] 	Batch(6800/7879) done. Loss: 0.3284  lr:0.000001
[ Thu Jul 11 08:45:05 2024 ] 	Batch(6900/7879) done. Loss: 0.0196  lr:0.000001
[ Thu Jul 11 08:45:27 2024 ] 
Training: Epoch [24/120], Step [6999], Loss: 0.3949822187423706, Training Accuracy: 96.18214285714286
[ Thu Jul 11 08:45:28 2024 ] 	Batch(7000/7879) done. Loss: 0.1168  lr:0.000001
[ Thu Jul 11 08:45:50 2024 ] 	Batch(7100/7879) done. Loss: 0.1505  lr:0.000001
[ Thu Jul 11 08:46:13 2024 ] 	Batch(7200/7879) done. Loss: 0.0562  lr:0.000001
[ Thu Jul 11 08:46:36 2024 ] 	Batch(7300/7879) done. Loss: 0.0487  lr:0.000001
[ Thu Jul 11 08:47:00 2024 ] 	Batch(7400/7879) done. Loss: 0.6005  lr:0.000001
[ Thu Jul 11 08:47:22 2024 ] 
Training: Epoch [24/120], Step [7499], Loss: 0.27014002203941345, Training Accuracy: 96.19166666666666
[ Thu Jul 11 08:47:23 2024 ] 	Batch(7500/7879) done. Loss: 0.1085  lr:0.000001
[ Thu Jul 11 08:47:45 2024 ] 	Batch(7600/7879) done. Loss: 0.2916  lr:0.000001
[ Thu Jul 11 08:48:08 2024 ] 	Batch(7700/7879) done. Loss: 0.4649  lr:0.000001
[ Thu Jul 11 08:48:31 2024 ] 	Batch(7800/7879) done. Loss: 0.0172  lr:0.000001
[ Thu Jul 11 08:48:48 2024 ] 	Mean training loss: 0.1418.
[ Thu Jul 11 08:48:48 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 08:48:48 2024 ] Training epoch: 26
[ Thu Jul 11 08:48:49 2024 ] 	Batch(0/7879) done. Loss: 0.0251  lr:0.000001
[ Thu Jul 11 08:49:12 2024 ] 	Batch(100/7879) done. Loss: 0.2167  lr:0.000001
[ Thu Jul 11 08:49:35 2024 ] 	Batch(200/7879) done. Loss: 0.0333  lr:0.000001
[ Thu Jul 11 08:49:58 2024 ] 	Batch(300/7879) done. Loss: 0.0277  lr:0.000001
[ Thu Jul 11 08:50:21 2024 ] 	Batch(400/7879) done. Loss: 0.0630  lr:0.000001
[ Thu Jul 11 08:50:44 2024 ] 
Training: Epoch [25/120], Step [499], Loss: 0.25287654995918274, Training Accuracy: 96.025
[ Thu Jul 11 08:50:44 2024 ] 	Batch(500/7879) done. Loss: 0.0274  lr:0.000001
[ Thu Jul 11 08:51:07 2024 ] 	Batch(600/7879) done. Loss: 0.1481  lr:0.000001
[ Thu Jul 11 08:51:30 2024 ] 	Batch(700/7879) done. Loss: 0.0625  lr:0.000001
[ Thu Jul 11 08:51:53 2024 ] 	Batch(800/7879) done. Loss: 0.6731  lr:0.000001
[ Thu Jul 11 08:52:15 2024 ] 	Batch(900/7879) done. Loss: 0.0494  lr:0.000001
[ Thu Jul 11 08:52:38 2024 ] 
Training: Epoch [25/120], Step [999], Loss: 1.0147671699523926, Training Accuracy: 96.2
[ Thu Jul 11 08:52:38 2024 ] 	Batch(1000/7879) done. Loss: 0.1453  lr:0.000001
[ Thu Jul 11 08:53:01 2024 ] 	Batch(1100/7879) done. Loss: 0.0981  lr:0.000001
[ Thu Jul 11 08:53:24 2024 ] 	Batch(1200/7879) done. Loss: 0.6139  lr:0.000001
[ Thu Jul 11 08:53:47 2024 ] 	Batch(1300/7879) done. Loss: 0.0814  lr:0.000001
[ Thu Jul 11 08:54:09 2024 ] 	Batch(1400/7879) done. Loss: 0.1076  lr:0.000001
[ Thu Jul 11 08:54:32 2024 ] 
Training: Epoch [25/120], Step [1499], Loss: 0.4023098647594452, Training Accuracy: 96.22500000000001
[ Thu Jul 11 08:54:32 2024 ] 	Batch(1500/7879) done. Loss: 0.3975  lr:0.000001
[ Thu Jul 11 08:54:55 2024 ] 	Batch(1600/7879) done. Loss: 0.2584  lr:0.000001
[ Thu Jul 11 08:55:18 2024 ] 	Batch(1700/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul 11 08:55:41 2024 ] 	Batch(1800/7879) done. Loss: 0.1125  lr:0.000001
[ Thu Jul 11 08:56:04 2024 ] 	Batch(1900/7879) done. Loss: 0.0199  lr:0.000001
[ Thu Jul 11 08:56:27 2024 ] 
Training: Epoch [25/120], Step [1999], Loss: 0.11364877223968506, Training Accuracy: 96.2125
[ Thu Jul 11 08:56:28 2024 ] 	Batch(2000/7879) done. Loss: 0.1817  lr:0.000001
[ Thu Jul 11 08:56:51 2024 ] 	Batch(2100/7879) done. Loss: 0.0301  lr:0.000001
[ Thu Jul 11 08:57:14 2024 ] 	Batch(2200/7879) done. Loss: 0.1236  lr:0.000001
[ Thu Jul 11 08:57:37 2024 ] 	Batch(2300/7879) done. Loss: 0.1383  lr:0.000001
[ Thu Jul 11 08:58:00 2024 ] 	Batch(2400/7879) done. Loss: 0.0237  lr:0.000001
[ Thu Jul 11 08:58:22 2024 ] 
Training: Epoch [25/120], Step [2499], Loss: 0.1642254739999771, Training Accuracy: 96.125
[ Thu Jul 11 08:58:22 2024 ] 	Batch(2500/7879) done. Loss: 0.0173  lr:0.000001
[ Thu Jul 11 08:58:45 2024 ] 	Batch(2600/7879) done. Loss: 0.1019  lr:0.000001
[ Thu Jul 11 08:59:08 2024 ] 	Batch(2700/7879) done. Loss: 0.1810  lr:0.000001
[ Thu Jul 11 08:59:30 2024 ] 	Batch(2800/7879) done. Loss: 0.0314  lr:0.000001
[ Thu Jul 11 08:59:53 2024 ] 	Batch(2900/7879) done. Loss: 0.0893  lr:0.000001
[ Thu Jul 11 09:00:16 2024 ] 
Training: Epoch [25/120], Step [2999], Loss: 0.013079727068543434, Training Accuracy: 96.15833333333333
[ Thu Jul 11 09:00:16 2024 ] 	Batch(3000/7879) done. Loss: 0.0096  lr:0.000001
[ Thu Jul 11 09:00:39 2024 ] 	Batch(3100/7879) done. Loss: 0.0041  lr:0.000001
[ Thu Jul 11 09:01:01 2024 ] 	Batch(3200/7879) done. Loss: 0.1026  lr:0.000001
[ Thu Jul 11 09:01:24 2024 ] 	Batch(3300/7879) done. Loss: 0.2684  lr:0.000001
[ Thu Jul 11 09:01:47 2024 ] 	Batch(3400/7879) done. Loss: 0.0085  lr:0.000001
[ Thu Jul 11 09:02:09 2024 ] 
Training: Epoch [25/120], Step [3499], Loss: 0.1029883474111557, Training Accuracy: 96.14642857142857
[ Thu Jul 11 09:02:10 2024 ] 	Batch(3500/7879) done. Loss: 0.0323  lr:0.000001
[ Thu Jul 11 09:02:33 2024 ] 	Batch(3600/7879) done. Loss: 0.0547  lr:0.000001
[ Thu Jul 11 09:02:56 2024 ] 	Batch(3700/7879) done. Loss: 0.1382  lr:0.000001
[ Thu Jul 11 09:03:19 2024 ] 	Batch(3800/7879) done. Loss: 0.0124  lr:0.000001
[ Thu Jul 11 09:03:42 2024 ] 	Batch(3900/7879) done. Loss: 0.0673  lr:0.000001
[ Thu Jul 11 09:04:05 2024 ] 
Training: Epoch [25/120], Step [3999], Loss: 0.1211995780467987, Training Accuracy: 96.2
[ Thu Jul 11 09:04:05 2024 ] 	Batch(4000/7879) done. Loss: 0.0535  lr:0.000001
[ Thu Jul 11 09:04:28 2024 ] 	Batch(4100/7879) done. Loss: 0.1912  lr:0.000001
[ Thu Jul 11 09:04:51 2024 ] 	Batch(4200/7879) done. Loss: 0.2873  lr:0.000001
[ Thu Jul 11 09:05:13 2024 ] 	Batch(4300/7879) done. Loss: 0.0557  lr:0.000001
[ Thu Jul 11 09:05:36 2024 ] 	Batch(4400/7879) done. Loss: 0.0113  lr:0.000001
[ Thu Jul 11 09:05:59 2024 ] 
Training: Epoch [25/120], Step [4499], Loss: 0.352120578289032, Training Accuracy: 96.16666666666667
[ Thu Jul 11 09:05:59 2024 ] 	Batch(4500/7879) done. Loss: 0.0230  lr:0.000001
[ Thu Jul 11 09:06:22 2024 ] 	Batch(4600/7879) done. Loss: 0.0016  lr:0.000001
[ Thu Jul 11 09:06:44 2024 ] 	Batch(4700/7879) done. Loss: 0.0441  lr:0.000001
[ Thu Jul 11 09:07:07 2024 ] 	Batch(4800/7879) done. Loss: 0.2389  lr:0.000001
[ Thu Jul 11 09:07:30 2024 ] 	Batch(4900/7879) done. Loss: 0.0041  lr:0.000001
[ Thu Jul 11 09:07:53 2024 ] 
Training: Epoch [25/120], Step [4999], Loss: 0.10402900725603104, Training Accuracy: 96.195
[ Thu Jul 11 09:07:53 2024 ] 	Batch(5000/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul 11 09:08:15 2024 ] 	Batch(5100/7879) done. Loss: 0.0074  lr:0.000001
[ Thu Jul 11 09:08:38 2024 ] 	Batch(5200/7879) done. Loss: 0.0835  lr:0.000001
[ Thu Jul 11 09:09:02 2024 ] 	Batch(5300/7879) done. Loss: 0.0034  lr:0.000001
[ Thu Jul 11 09:09:24 2024 ] 	Batch(5400/7879) done. Loss: 0.1615  lr:0.000001
[ Thu Jul 11 09:09:47 2024 ] 
Training: Epoch [25/120], Step [5499], Loss: 0.14285725355148315, Training Accuracy: 96.19318181818181
[ Thu Jul 11 09:09:47 2024 ] 	Batch(5500/7879) done. Loss: 0.0972  lr:0.000001
[ Thu Jul 11 09:10:10 2024 ] 	Batch(5600/7879) done. Loss: 0.1233  lr:0.000001
[ Thu Jul 11 09:10:33 2024 ] 	Batch(5700/7879) done. Loss: 0.1674  lr:0.000001
[ Thu Jul 11 09:10:55 2024 ] 	Batch(5800/7879) done. Loss: 0.0870  lr:0.000001
[ Thu Jul 11 09:11:18 2024 ] 	Batch(5900/7879) done. Loss: 0.0956  lr:0.000001
[ Thu Jul 11 09:11:40 2024 ] 
Training: Epoch [25/120], Step [5999], Loss: 0.015269780531525612, Training Accuracy: 96.17708333333333
[ Thu Jul 11 09:11:41 2024 ] 	Batch(6000/7879) done. Loss: 0.0700  lr:0.000001
[ Thu Jul 11 09:12:04 2024 ] 	Batch(6100/7879) done. Loss: 0.1685  lr:0.000001
[ Thu Jul 11 09:12:26 2024 ] 	Batch(6200/7879) done. Loss: 0.1755  lr:0.000001
[ Thu Jul 11 09:12:49 2024 ] 	Batch(6300/7879) done. Loss: 0.2191  lr:0.000001
[ Thu Jul 11 09:13:12 2024 ] 	Batch(6400/7879) done. Loss: 0.4880  lr:0.000001
[ Thu Jul 11 09:13:34 2024 ] 
Training: Epoch [25/120], Step [6499], Loss: 0.05129120126366615, Training Accuracy: 96.15384615384616
[ Thu Jul 11 09:13:35 2024 ] 	Batch(6500/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul 11 09:13:57 2024 ] 	Batch(6600/7879) done. Loss: 0.3755  lr:0.000001
[ Thu Jul 11 09:14:20 2024 ] 	Batch(6700/7879) done. Loss: 0.2298  lr:0.000001
[ Thu Jul 11 09:14:43 2024 ] 	Batch(6800/7879) done. Loss: 0.2442  lr:0.000001
[ Thu Jul 11 09:15:06 2024 ] 	Batch(6900/7879) done. Loss: 0.0129  lr:0.000001
[ Thu Jul 11 09:15:28 2024 ] 
Training: Epoch [25/120], Step [6999], Loss: 0.04186443239450455, Training Accuracy: 96.18214285714286
[ Thu Jul 11 09:15:28 2024 ] 	Batch(7000/7879) done. Loss: 0.0403  lr:0.000001
[ Thu Jul 11 09:15:51 2024 ] 	Batch(7100/7879) done. Loss: 0.1401  lr:0.000001
[ Thu Jul 11 09:16:14 2024 ] 	Batch(7200/7879) done. Loss: 0.0128  lr:0.000001
[ Thu Jul 11 09:16:37 2024 ] 	Batch(7300/7879) done. Loss: 0.0906  lr:0.000001
[ Thu Jul 11 09:16:59 2024 ] 	Batch(7400/7879) done. Loss: 0.0302  lr:0.000001
[ Thu Jul 11 09:17:22 2024 ] 
Training: Epoch [25/120], Step [7499], Loss: 0.009969300590455532, Training Accuracy: 96.17833333333333
[ Thu Jul 11 09:17:22 2024 ] 	Batch(7500/7879) done. Loss: 0.0544  lr:0.000001
[ Thu Jul 11 09:17:46 2024 ] 	Batch(7600/7879) done. Loss: 0.0057  lr:0.000001
[ Thu Jul 11 09:18:09 2024 ] 	Batch(7700/7879) done. Loss: 0.0211  lr:0.000001
[ Thu Jul 11 09:18:32 2024 ] 	Batch(7800/7879) done. Loss: 0.0975  lr:0.000001
[ Thu Jul 11 09:18:49 2024 ] 	Mean training loss: 0.1412.
[ Thu Jul 11 09:18:49 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 09:18:49 2024 ] Training epoch: 27
[ Thu Jul 11 09:18:50 2024 ] 	Batch(0/7879) done. Loss: 0.0786  lr:0.000001
[ Thu Jul 11 09:19:13 2024 ] 	Batch(100/7879) done. Loss: 0.0772  lr:0.000001
[ Thu Jul 11 09:19:36 2024 ] 	Batch(200/7879) done. Loss: 0.0941  lr:0.000001
[ Thu Jul 11 09:19:59 2024 ] 	Batch(300/7879) done. Loss: 0.0499  lr:0.000001
[ Thu Jul 11 09:20:22 2024 ] 	Batch(400/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul 11 09:20:45 2024 ] 
Training: Epoch [26/120], Step [499], Loss: 0.004215016029775143, Training Accuracy: 96.375
[ Thu Jul 11 09:20:45 2024 ] 	Batch(500/7879) done. Loss: 0.0777  lr:0.000001
[ Thu Jul 11 09:21:07 2024 ] 	Batch(600/7879) done. Loss: 0.1811  lr:0.000001
[ Thu Jul 11 09:21:30 2024 ] 	Batch(700/7879) done. Loss: 0.0930  lr:0.000001
[ Thu Jul 11 09:21:53 2024 ] 	Batch(800/7879) done. Loss: 0.0951  lr:0.000001
[ Thu Jul 11 09:22:16 2024 ] 	Batch(900/7879) done. Loss: 0.0531  lr:0.000001
[ Thu Jul 11 09:22:38 2024 ] 
Training: Epoch [26/120], Step [999], Loss: 0.43186914920806885, Training Accuracy: 96.675
[ Thu Jul 11 09:22:38 2024 ] 	Batch(1000/7879) done. Loss: 0.0557  lr:0.000001
[ Thu Jul 11 09:23:01 2024 ] 	Batch(1100/7879) done. Loss: 0.0405  lr:0.000001
[ Thu Jul 11 09:23:24 2024 ] 	Batch(1200/7879) done. Loss: 0.1254  lr:0.000001
[ Thu Jul 11 09:23:47 2024 ] 	Batch(1300/7879) done. Loss: 0.2130  lr:0.000001
[ Thu Jul 11 09:24:10 2024 ] 	Batch(1400/7879) done. Loss: 0.0958  lr:0.000001
[ Thu Jul 11 09:24:33 2024 ] 
Training: Epoch [26/120], Step [1499], Loss: 0.0006993871065787971, Training Accuracy: 96.475
[ Thu Jul 11 09:24:33 2024 ] 	Batch(1500/7879) done. Loss: 0.2650  lr:0.000001
[ Thu Jul 11 09:24:57 2024 ] 	Batch(1600/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul 11 09:25:19 2024 ] 	Batch(1700/7879) done. Loss: 0.0399  lr:0.000001
[ Thu Jul 11 09:25:42 2024 ] 	Batch(1800/7879) done. Loss: 0.0221  lr:0.000001
[ Thu Jul 11 09:26:05 2024 ] 	Batch(1900/7879) done. Loss: 0.0594  lr:0.000001
[ Thu Jul 11 09:26:27 2024 ] 
Training: Epoch [26/120], Step [1999], Loss: 0.04469253495335579, Training Accuracy: 96.36875
[ Thu Jul 11 09:26:27 2024 ] 	Batch(2000/7879) done. Loss: 0.0235  lr:0.000001
[ Thu Jul 11 09:26:50 2024 ] 	Batch(2100/7879) done. Loss: 0.0519  lr:0.000001
[ Thu Jul 11 09:27:13 2024 ] 	Batch(2200/7879) done. Loss: 0.2324  lr:0.000001
[ Thu Jul 11 09:27:36 2024 ] 	Batch(2300/7879) done. Loss: 0.0101  lr:0.000001
[ Thu Jul 11 09:27:59 2024 ] 	Batch(2400/7879) done. Loss: 0.2740  lr:0.000001
[ Thu Jul 11 09:28:22 2024 ] 
Training: Epoch [26/120], Step [2499], Loss: 0.13185936212539673, Training Accuracy: 96.315
[ Thu Jul 11 09:28:22 2024 ] 	Batch(2500/7879) done. Loss: 0.1089  lr:0.000001
[ Thu Jul 11 09:28:45 2024 ] 	Batch(2600/7879) done. Loss: 0.0056  lr:0.000001
[ Thu Jul 11 09:29:08 2024 ] 	Batch(2700/7879) done. Loss: 0.1229  lr:0.000001
[ Thu Jul 11 09:29:32 2024 ] 	Batch(2800/7879) done. Loss: 0.1478  lr:0.000001
[ Thu Jul 11 09:29:54 2024 ] 	Batch(2900/7879) done. Loss: 0.2552  lr:0.000001
[ Thu Jul 11 09:30:17 2024 ] 
Training: Epoch [26/120], Step [2999], Loss: 0.06058327481150627, Training Accuracy: 96.32083333333334
[ Thu Jul 11 09:30:17 2024 ] 	Batch(3000/7879) done. Loss: 0.0422  lr:0.000001
[ Thu Jul 11 09:30:40 2024 ] 	Batch(3100/7879) done. Loss: 0.3089  lr:0.000001
[ Thu Jul 11 09:31:03 2024 ] 	Batch(3200/7879) done. Loss: 0.0300  lr:0.000001
[ Thu Jul 11 09:31:25 2024 ] 	Batch(3300/7879) done. Loss: 0.0566  lr:0.000001
[ Thu Jul 11 09:31:48 2024 ] 	Batch(3400/7879) done. Loss: 0.1293  lr:0.000001
[ Thu Jul 11 09:32:10 2024 ] 
Training: Epoch [26/120], Step [3499], Loss: 0.006421989295631647, Training Accuracy: 96.31785714285715
[ Thu Jul 11 09:32:11 2024 ] 	Batch(3500/7879) done. Loss: 0.0211  lr:0.000001
[ Thu Jul 11 09:32:33 2024 ] 	Batch(3600/7879) done. Loss: 0.4871  lr:0.000001
[ Thu Jul 11 09:32:56 2024 ] 	Batch(3700/7879) done. Loss: 0.0490  lr:0.000001
[ Thu Jul 11 09:33:18 2024 ] 	Batch(3800/7879) done. Loss: 0.1325  lr:0.000001
[ Thu Jul 11 09:33:41 2024 ] 	Batch(3900/7879) done. Loss: 0.0067  lr:0.000001
[ Thu Jul 11 09:34:03 2024 ] 
Training: Epoch [26/120], Step [3999], Loss: 0.015525305643677711, Training Accuracy: 96.265625
[ Thu Jul 11 09:34:04 2024 ] 	Batch(4000/7879) done. Loss: 0.3723  lr:0.000001
[ Thu Jul 11 09:34:26 2024 ] 	Batch(4100/7879) done. Loss: 0.0962  lr:0.000001
[ Thu Jul 11 09:34:49 2024 ] 	Batch(4200/7879) done. Loss: 0.0694  lr:0.000001
[ Thu Jul 11 09:35:12 2024 ] 	Batch(4300/7879) done. Loss: 0.0640  lr:0.000001
[ Thu Jul 11 09:35:34 2024 ] 	Batch(4400/7879) done. Loss: 0.0568  lr:0.000001
[ Thu Jul 11 09:35:57 2024 ] 
Training: Epoch [26/120], Step [4499], Loss: 0.24782472848892212, Training Accuracy: 96.2638888888889
[ Thu Jul 11 09:35:57 2024 ] 	Batch(4500/7879) done. Loss: 0.1726  lr:0.000001
[ Thu Jul 11 09:36:20 2024 ] 	Batch(4600/7879) done. Loss: 0.2275  lr:0.000001
[ Thu Jul 11 09:36:42 2024 ] 	Batch(4700/7879) done. Loss: 0.2094  lr:0.000001
[ Thu Jul 11 09:37:05 2024 ] 	Batch(4800/7879) done. Loss: 0.0476  lr:0.000001
[ Thu Jul 11 09:37:28 2024 ] 	Batch(4900/7879) done. Loss: 0.1665  lr:0.000001
[ Thu Jul 11 09:37:50 2024 ] 
Training: Epoch [26/120], Step [4999], Loss: 0.0918063223361969, Training Accuracy: 96.26249999999999
[ Thu Jul 11 09:37:50 2024 ] 	Batch(5000/7879) done. Loss: 0.2477  lr:0.000001
[ Thu Jul 11 09:38:13 2024 ] 	Batch(5100/7879) done. Loss: 0.0275  lr:0.000001
[ Thu Jul 11 09:38:37 2024 ] 	Batch(5200/7879) done. Loss: 0.2565  lr:0.000001
[ Thu Jul 11 09:38:59 2024 ] 	Batch(5300/7879) done. Loss: 0.0197  lr:0.000001
[ Thu Jul 11 09:39:22 2024 ] 	Batch(5400/7879) done. Loss: 0.1003  lr:0.000001
[ Thu Jul 11 09:39:44 2024 ] 
Training: Epoch [26/120], Step [5499], Loss: 0.1001991331577301, Training Accuracy: 96.24772727272727
[ Thu Jul 11 09:39:44 2024 ] 	Batch(5500/7879) done. Loss: 0.1436  lr:0.000001
[ Thu Jul 11 09:40:07 2024 ] 	Batch(5600/7879) done. Loss: 0.2448  lr:0.000001
[ Thu Jul 11 09:40:30 2024 ] 	Batch(5700/7879) done. Loss: 0.0285  lr:0.000001
[ Thu Jul 11 09:40:52 2024 ] 	Batch(5800/7879) done. Loss: 0.0919  lr:0.000001
[ Thu Jul 11 09:41:15 2024 ] 	Batch(5900/7879) done. Loss: 0.1252  lr:0.000001
[ Thu Jul 11 09:41:37 2024 ] 
Training: Epoch [26/120], Step [5999], Loss: 0.2111045867204666, Training Accuracy: 96.22291666666666
[ Thu Jul 11 09:41:38 2024 ] 	Batch(6000/7879) done. Loss: 0.0142  lr:0.000001
[ Thu Jul 11 09:42:00 2024 ] 	Batch(6100/7879) done. Loss: 0.5747  lr:0.000001
[ Thu Jul 11 09:42:23 2024 ] 	Batch(6200/7879) done. Loss: 0.0949  lr:0.000001
[ Thu Jul 11 09:42:45 2024 ] 	Batch(6300/7879) done. Loss: 0.0097  lr:0.000001
[ Thu Jul 11 09:43:08 2024 ] 	Batch(6400/7879) done. Loss: 0.0389  lr:0.000001
[ Thu Jul 11 09:43:30 2024 ] 
Training: Epoch [26/120], Step [6499], Loss: 0.17159852385520935, Training Accuracy: 96.25384615384615
[ Thu Jul 11 09:43:31 2024 ] 	Batch(6500/7879) done. Loss: 0.0539  lr:0.000001
[ Thu Jul 11 09:43:53 2024 ] 	Batch(6600/7879) done. Loss: 0.0069  lr:0.000001
[ Thu Jul 11 09:44:16 2024 ] 	Batch(6700/7879) done. Loss: 0.5365  lr:0.000001
[ Thu Jul 11 09:44:39 2024 ] 	Batch(6800/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul 11 09:45:01 2024 ] 	Batch(6900/7879) done. Loss: 0.0562  lr:0.000001
[ Thu Jul 11 09:45:24 2024 ] 
Training: Epoch [26/120], Step [6999], Loss: 0.20004773139953613, Training Accuracy: 96.2125
[ Thu Jul 11 09:45:24 2024 ] 	Batch(7000/7879) done. Loss: 0.0349  lr:0.000001
[ Thu Jul 11 09:45:46 2024 ] 	Batch(7100/7879) done. Loss: 0.3693  lr:0.000001
[ Thu Jul 11 09:46:09 2024 ] 	Batch(7200/7879) done. Loss: 0.0936  lr:0.000001
[ Thu Jul 11 09:46:32 2024 ] 	Batch(7300/7879) done. Loss: 0.4493  lr:0.000001
[ Thu Jul 11 09:46:54 2024 ] 	Batch(7400/7879) done. Loss: 0.0152  lr:0.000001
[ Thu Jul 11 09:47:17 2024 ] 
Training: Epoch [26/120], Step [7499], Loss: 0.23388123512268066, Training Accuracy: 96.18333333333334
[ Thu Jul 11 09:47:17 2024 ] 	Batch(7500/7879) done. Loss: 0.0248  lr:0.000001
[ Thu Jul 11 09:47:40 2024 ] 	Batch(7600/7879) done. Loss: 0.1263  lr:0.000001
[ Thu Jul 11 09:48:02 2024 ] 	Batch(7700/7879) done. Loss: 0.2046  lr:0.000001
[ Thu Jul 11 09:48:25 2024 ] 	Batch(7800/7879) done. Loss: 0.0257  lr:0.000001
[ Thu Jul 11 09:48:42 2024 ] 	Mean training loss: 0.1404.
[ Thu Jul 11 09:48:42 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 09:48:42 2024 ] Training epoch: 28
[ Thu Jul 11 09:48:43 2024 ] 	Batch(0/7879) done. Loss: 0.6047  lr:0.000001
[ Thu Jul 11 09:49:06 2024 ] 	Batch(100/7879) done. Loss: 0.0615  lr:0.000001
[ Thu Jul 11 09:49:28 2024 ] 	Batch(200/7879) done. Loss: 0.0648  lr:0.000001
[ Thu Jul 11 09:49:51 2024 ] 	Batch(300/7879) done. Loss: 0.2115  lr:0.000001
[ Thu Jul 11 09:50:14 2024 ] 	Batch(400/7879) done. Loss: 0.2708  lr:0.000001
[ Thu Jul 11 09:50:36 2024 ] 
Training: Epoch [27/120], Step [499], Loss: 0.08694183826446533, Training Accuracy: 96.125
[ Thu Jul 11 09:50:36 2024 ] 	Batch(500/7879) done. Loss: 0.1030  lr:0.000001
[ Thu Jul 11 09:50:59 2024 ] 	Batch(600/7879) done. Loss: 0.0932  lr:0.000001
[ Thu Jul 11 09:51:21 2024 ] 	Batch(700/7879) done. Loss: 0.0285  lr:0.000001
[ Thu Jul 11 09:51:44 2024 ] 	Batch(800/7879) done. Loss: 0.0852  lr:0.000001
[ Thu Jul 11 09:52:07 2024 ] 	Batch(900/7879) done. Loss: 0.9682  lr:0.000001
[ Thu Jul 11 09:52:29 2024 ] 
Training: Epoch [27/120], Step [999], Loss: 0.06905418634414673, Training Accuracy: 96.525
[ Thu Jul 11 09:52:29 2024 ] 	Batch(1000/7879) done. Loss: 0.0432  lr:0.000001
[ Thu Jul 11 09:52:52 2024 ] 	Batch(1100/7879) done. Loss: 0.5099  lr:0.000001
[ Thu Jul 11 09:53:15 2024 ] 	Batch(1200/7879) done. Loss: 0.1200  lr:0.000001
[ Thu Jul 11 09:53:38 2024 ] 	Batch(1300/7879) done. Loss: 0.0151  lr:0.000001
[ Thu Jul 11 09:54:00 2024 ] 	Batch(1400/7879) done. Loss: 0.0243  lr:0.000001
[ Thu Jul 11 09:54:23 2024 ] 
Training: Epoch [27/120], Step [1499], Loss: 0.11455797404050827, Training Accuracy: 96.39999999999999
[ Thu Jul 11 09:54:23 2024 ] 	Batch(1500/7879) done. Loss: 0.0288  lr:0.000001
[ Thu Jul 11 09:54:46 2024 ] 	Batch(1600/7879) done. Loss: 0.1341  lr:0.000001
[ Thu Jul 11 09:55:09 2024 ] 	Batch(1700/7879) done. Loss: 0.1510  lr:0.000001
[ Thu Jul 11 09:55:32 2024 ] 	Batch(1800/7879) done. Loss: 0.0677  lr:0.000001
[ Thu Jul 11 09:55:56 2024 ] 	Batch(1900/7879) done. Loss: 0.0231  lr:0.000001
[ Thu Jul 11 09:56:19 2024 ] 
Training: Epoch [27/120], Step [1999], Loss: 0.04672809690237045, Training Accuracy: 96.39999999999999
[ Thu Jul 11 09:56:19 2024 ] 	Batch(2000/7879) done. Loss: 0.0188  lr:0.000001
[ Thu Jul 11 09:56:43 2024 ] 	Batch(2100/7879) done. Loss: 0.0697  lr:0.000001
[ Thu Jul 11 09:57:05 2024 ] 	Batch(2200/7879) done. Loss: 0.0318  lr:0.000001
[ Thu Jul 11 09:57:28 2024 ] 	Batch(2300/7879) done. Loss: 0.2394  lr:0.000001
[ Thu Jul 11 09:57:51 2024 ] 	Batch(2400/7879) done. Loss: 0.0188  lr:0.000001
[ Thu Jul 11 09:58:14 2024 ] 
Training: Epoch [27/120], Step [2499], Loss: 0.13451196253299713, Training Accuracy: 96.3
[ Thu Jul 11 09:58:14 2024 ] 	Batch(2500/7879) done. Loss: 0.5264  lr:0.000001
[ Thu Jul 11 09:58:37 2024 ] 	Batch(2600/7879) done. Loss: 0.3835  lr:0.000001
[ Thu Jul 11 09:59:01 2024 ] 	Batch(2700/7879) done. Loss: 0.0271  lr:0.000001
[ Thu Jul 11 09:59:24 2024 ] 	Batch(2800/7879) done. Loss: 0.3634  lr:0.000001
[ Thu Jul 11 09:59:47 2024 ] 	Batch(2900/7879) done. Loss: 0.0420  lr:0.000001
[ Thu Jul 11 10:00:10 2024 ] 
Training: Epoch [27/120], Step [2999], Loss: 0.24875220656394958, Training Accuracy: 96.37916666666668
[ Thu Jul 11 10:00:10 2024 ] 	Batch(3000/7879) done. Loss: 0.2361  lr:0.000001
[ Thu Jul 11 10:00:33 2024 ] 	Batch(3100/7879) done. Loss: 0.0894  lr:0.000001
[ Thu Jul 11 10:00:56 2024 ] 	Batch(3200/7879) done. Loss: 0.1805  lr:0.000001
[ Thu Jul 11 10:01:18 2024 ] 	Batch(3300/7879) done. Loss: 0.3898  lr:0.000001
[ Thu Jul 11 10:01:41 2024 ] 	Batch(3400/7879) done. Loss: 0.1653  lr:0.000001
[ Thu Jul 11 10:02:03 2024 ] 
Training: Epoch [27/120], Step [3499], Loss: 0.1796131730079651, Training Accuracy: 96.30714285714286
[ Thu Jul 11 10:02:03 2024 ] 	Batch(3500/7879) done. Loss: 0.3790  lr:0.000001
[ Thu Jul 11 10:02:26 2024 ] 	Batch(3600/7879) done. Loss: 0.0831  lr:0.000001
[ Thu Jul 11 10:02:49 2024 ] 	Batch(3700/7879) done. Loss: 0.0497  lr:0.000001
[ Thu Jul 11 10:03:11 2024 ] 	Batch(3800/7879) done. Loss: 0.2496  lr:0.000001
[ Thu Jul 11 10:03:34 2024 ] 	Batch(3900/7879) done. Loss: 0.1727  lr:0.000001
[ Thu Jul 11 10:03:56 2024 ] 
Training: Epoch [27/120], Step [3999], Loss: 0.22054314613342285, Training Accuracy: 96.26249999999999
[ Thu Jul 11 10:03:56 2024 ] 	Batch(4000/7879) done. Loss: 0.3272  lr:0.000001
[ Thu Jul 11 10:04:19 2024 ] 	Batch(4100/7879) done. Loss: 0.2764  lr:0.000001
[ Thu Jul 11 10:04:42 2024 ] 	Batch(4200/7879) done. Loss: 0.0892  lr:0.000001
[ Thu Jul 11 10:05:05 2024 ] 	Batch(4300/7879) done. Loss: 0.0303  lr:0.000001
[ Thu Jul 11 10:05:28 2024 ] 	Batch(4400/7879) done. Loss: 0.0970  lr:0.000001
[ Thu Jul 11 10:05:51 2024 ] 
Training: Epoch [27/120], Step [4499], Loss: 0.009946820326149464, Training Accuracy: 96.3
[ Thu Jul 11 10:05:51 2024 ] 	Batch(4500/7879) done. Loss: 0.2866  lr:0.000001
[ Thu Jul 11 10:06:14 2024 ] 	Batch(4600/7879) done. Loss: 0.0151  lr:0.000001
[ Thu Jul 11 10:06:37 2024 ] 	Batch(4700/7879) done. Loss: 0.0745  lr:0.000001
[ Thu Jul 11 10:07:01 2024 ] 	Batch(4800/7879) done. Loss: 0.8916  lr:0.000001
[ Thu Jul 11 10:07:24 2024 ] 	Batch(4900/7879) done. Loss: 0.0380  lr:0.000001
[ Thu Jul 11 10:07:47 2024 ] 
Training: Epoch [27/120], Step [4999], Loss: 0.08711329847574234, Training Accuracy: 96.235
[ Thu Jul 11 10:07:47 2024 ] 	Batch(5000/7879) done. Loss: 0.1369  lr:0.000001
[ Thu Jul 11 10:08:10 2024 ] 	Batch(5100/7879) done. Loss: 0.1077  lr:0.000001
[ Thu Jul 11 10:08:33 2024 ] 	Batch(5200/7879) done. Loss: 0.0337  lr:0.000001
[ Thu Jul 11 10:08:56 2024 ] 	Batch(5300/7879) done. Loss: 0.0456  lr:0.000001
[ Thu Jul 11 10:09:19 2024 ] 	Batch(5400/7879) done. Loss: 0.1535  lr:0.000001
[ Thu Jul 11 10:09:42 2024 ] 
Training: Epoch [27/120], Step [5499], Loss: 0.4146878719329834, Training Accuracy: 96.20227272727273
[ Thu Jul 11 10:09:42 2024 ] 	Batch(5500/7879) done. Loss: 0.2785  lr:0.000001
[ Thu Jul 11 10:10:06 2024 ] 	Batch(5600/7879) done. Loss: 0.4072  lr:0.000001
[ Thu Jul 11 10:10:29 2024 ] 	Batch(5700/7879) done. Loss: 0.1976  lr:0.000001
[ Thu Jul 11 10:10:52 2024 ] 	Batch(5800/7879) done. Loss: 0.5035  lr:0.000001
[ Thu Jul 11 10:11:15 2024 ] 	Batch(5900/7879) done. Loss: 0.2286  lr:0.000001
[ Thu Jul 11 10:11:38 2024 ] 
Training: Epoch [27/120], Step [5999], Loss: 0.0142497094348073, Training Accuracy: 96.18333333333334
[ Thu Jul 11 10:11:38 2024 ] 	Batch(6000/7879) done. Loss: 0.1591  lr:0.000001
[ Thu Jul 11 10:12:01 2024 ] 	Batch(6100/7879) done. Loss: 0.5087  lr:0.000001
[ Thu Jul 11 10:12:24 2024 ] 	Batch(6200/7879) done. Loss: 0.0202  lr:0.000001
[ Thu Jul 11 10:12:47 2024 ] 	Batch(6300/7879) done. Loss: 0.0182  lr:0.000001
[ Thu Jul 11 10:13:10 2024 ] 	Batch(6400/7879) done. Loss: 0.0927  lr:0.000001
[ Thu Jul 11 10:13:33 2024 ] 
Training: Epoch [27/120], Step [6499], Loss: 0.01129133440554142, Training Accuracy: 96.175
[ Thu Jul 11 10:13:33 2024 ] 	Batch(6500/7879) done. Loss: 0.0464  lr:0.000001
[ Thu Jul 11 10:13:56 2024 ] 	Batch(6600/7879) done. Loss: 0.1923  lr:0.000001
[ Thu Jul 11 10:14:19 2024 ] 	Batch(6700/7879) done. Loss: 0.0048  lr:0.000001
[ Thu Jul 11 10:14:43 2024 ] 	Batch(6800/7879) done. Loss: 0.2120  lr:0.000001
[ Thu Jul 11 10:15:06 2024 ] 	Batch(6900/7879) done. Loss: 0.1895  lr:0.000001
[ Thu Jul 11 10:15:28 2024 ] 
Training: Epoch [27/120], Step [6999], Loss: 0.1591973453760147, Training Accuracy: 96.16607142857143
[ Thu Jul 11 10:15:29 2024 ] 	Batch(7000/7879) done. Loss: 0.1230  lr:0.000001
[ Thu Jul 11 10:15:52 2024 ] 	Batch(7100/7879) done. Loss: 0.5775  lr:0.000001
[ Thu Jul 11 10:16:15 2024 ] 	Batch(7200/7879) done. Loss: 0.1105  lr:0.000001
[ Thu Jul 11 10:16:38 2024 ] 	Batch(7300/7879) done. Loss: 0.0484  lr:0.000001
[ Thu Jul 11 10:17:01 2024 ] 	Batch(7400/7879) done. Loss: 0.0089  lr:0.000001
[ Thu Jul 11 10:17:24 2024 ] 
Training: Epoch [27/120], Step [7499], Loss: 0.35656625032424927, Training Accuracy: 96.16666666666667
[ Thu Jul 11 10:17:24 2024 ] 	Batch(7500/7879) done. Loss: 0.4006  lr:0.000001
[ Thu Jul 11 10:17:47 2024 ] 	Batch(7600/7879) done. Loss: 0.0263  lr:0.000001
[ Thu Jul 11 10:18:10 2024 ] 	Batch(7700/7879) done. Loss: 0.2966  lr:0.000001
[ Thu Jul 11 10:18:33 2024 ] 	Batch(7800/7879) done. Loss: 0.0100  lr:0.000001
[ Thu Jul 11 10:18:51 2024 ] 	Mean training loss: 0.1412.
[ Thu Jul 11 10:18:51 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Thu Jul 11 10:18:52 2024 ] Training epoch: 29
[ Thu Jul 11 10:18:52 2024 ] 	Batch(0/7879) done. Loss: 0.0036  lr:0.000001
[ Thu Jul 11 10:19:15 2024 ] 	Batch(100/7879) done. Loss: 0.0263  lr:0.000001
[ Thu Jul 11 10:19:38 2024 ] 	Batch(200/7879) done. Loss: 0.0102  lr:0.000001
[ Thu Jul 11 10:20:01 2024 ] 	Batch(300/7879) done. Loss: 0.7003  lr:0.000001
[ Thu Jul 11 10:20:24 2024 ] 	Batch(400/7879) done. Loss: 0.3744  lr:0.000001
[ Thu Jul 11 10:20:47 2024 ] 
Training: Epoch [28/120], Step [499], Loss: 0.022640986368060112, Training Accuracy: 96.55
[ Thu Jul 11 10:20:47 2024 ] 	Batch(500/7879) done. Loss: 0.1716  lr:0.000001
[ Thu Jul 11 10:21:10 2024 ] 	Batch(600/7879) done. Loss: 0.1833  lr:0.000001
[ Thu Jul 11 10:21:32 2024 ] 	Batch(700/7879) done. Loss: 0.1281  lr:0.000001
[ Thu Jul 11 10:21:55 2024 ] 	Batch(800/7879) done. Loss: 0.0843  lr:0.000001
[ Thu Jul 11 10:22:18 2024 ] 	Batch(900/7879) done. Loss: 0.2460  lr:0.000001
[ Thu Jul 11 10:22:40 2024 ] 
Training: Epoch [28/120], Step [999], Loss: 0.11184123158454895, Training Accuracy: 96.6
[ Thu Jul 11 10:22:40 2024 ] 	Batch(1000/7879) done. Loss: 0.0496  lr:0.000001
[ Thu Jul 11 10:23:03 2024 ] 	Batch(1100/7879) done. Loss: 0.6076  lr:0.000001
[ Thu Jul 11 10:23:26 2024 ] 	Batch(1200/7879) done. Loss: 0.0935  lr:0.000001
[ Thu Jul 11 10:23:48 2024 ] 	Batch(1300/7879) done. Loss: 0.0133  lr:0.000001
[ Thu Jul 11 10:24:11 2024 ] 	Batch(1400/7879) done. Loss: 0.5744  lr:0.000001
[ Thu Jul 11 10:24:33 2024 ] 
Training: Epoch [28/120], Step [1499], Loss: 0.037769392132759094, Training Accuracy: 96.42500000000001
[ Thu Jul 11 10:24:34 2024 ] 	Batch(1500/7879) done. Loss: 0.1056  lr:0.000001
[ Thu Jul 11 10:24:56 2024 ] 	Batch(1600/7879) done. Loss: 0.2783  lr:0.000001
[ Thu Jul 11 10:25:19 2024 ] 	Batch(1700/7879) done. Loss: 0.0491  lr:0.000001
[ Thu Jul 11 10:25:42 2024 ] 	Batch(1800/7879) done. Loss: 0.0333  lr:0.000001
[ Thu Jul 11 10:26:05 2024 ] 	Batch(1900/7879) done. Loss: 0.3568  lr:0.000001
[ Thu Jul 11 10:26:28 2024 ] 
Training: Epoch [28/120], Step [1999], Loss: 0.0560142956674099, Training Accuracy: 96.33749999999999
[ Thu Jul 11 10:26:29 2024 ] 	Batch(2000/7879) done. Loss: 0.0517  lr:0.000001
[ Thu Jul 11 10:26:51 2024 ] 	Batch(2100/7879) done. Loss: 0.4416  lr:0.000001
[ Thu Jul 11 10:27:14 2024 ] 	Batch(2200/7879) done. Loss: 0.0237  lr:0.000001
[ Thu Jul 11 10:27:37 2024 ] 	Batch(2300/7879) done. Loss: 0.1386  lr:0.000001
[ Thu Jul 11 10:27:59 2024 ] 	Batch(2400/7879) done. Loss: 0.0326  lr:0.000001
[ Thu Jul 11 10:28:22 2024 ] 
Training: Epoch [28/120], Step [2499], Loss: 0.20708130300045013, Training Accuracy: 96.30499999999999
[ Thu Jul 11 10:28:22 2024 ] 	Batch(2500/7879) done. Loss: 0.0216  lr:0.000001
[ Thu Jul 11 10:28:45 2024 ] 	Batch(2600/7879) done. Loss: 0.0233  lr:0.000001
[ Thu Jul 11 10:29:07 2024 ] 	Batch(2700/7879) done. Loss: 0.2657  lr:0.000001
[ Thu Jul 11 10:29:30 2024 ] 	Batch(2800/7879) done. Loss: 0.2046  lr:0.000001
[ Thu Jul 11 10:29:52 2024 ] 	Batch(2900/7879) done. Loss: 0.0709  lr:0.000001
[ Thu Jul 11 10:30:15 2024 ] 
Training: Epoch [28/120], Step [2999], Loss: 0.25216683745384216, Training Accuracy: 96.19583333333334
[ Thu Jul 11 10:30:15 2024 ] 	Batch(3000/7879) done. Loss: 0.1540  lr:0.000001
[ Thu Jul 11 10:30:38 2024 ] 	Batch(3100/7879) done. Loss: 0.0273  lr:0.000001
[ Thu Jul 11 10:31:00 2024 ] 	Batch(3200/7879) done. Loss: 0.0831  lr:0.000001
[ Thu Jul 11 10:31:23 2024 ] 	Batch(3300/7879) done. Loss: 0.0121  lr:0.000001
[ Thu Jul 11 10:31:45 2024 ] 	Batch(3400/7879) done. Loss: 0.0927  lr:0.000001
[ Thu Jul 11 10:32:08 2024 ] 
Training: Epoch [28/120], Step [3499], Loss: 0.020476000383496284, Training Accuracy: 96.24642857142857
[ Thu Jul 11 10:32:08 2024 ] 	Batch(3500/7879) done. Loss: 0.0348  lr:0.000001
[ Thu Jul 11 10:32:31 2024 ] 	Batch(3600/7879) done. Loss: 0.0952  lr:0.000001
[ Thu Jul 11 10:32:53 2024 ] 	Batch(3700/7879) done. Loss: 0.1877  lr:0.000001
[ Thu Jul 11 10:33:16 2024 ] 	Batch(3800/7879) done. Loss: 0.0529  lr:0.000001
[ Thu Jul 11 10:33:38 2024 ] 	Batch(3900/7879) done. Loss: 0.1990  lr:0.000001
[ Thu Jul 11 10:34:01 2024 ] 
Training: Epoch [28/120], Step [3999], Loss: 0.0405522957444191, Training Accuracy: 96.209375
[ Thu Jul 11 10:34:01 2024 ] 	Batch(4000/7879) done. Loss: 0.1048  lr:0.000001
[ Thu Jul 11 10:34:24 2024 ] 	Batch(4100/7879) done. Loss: 0.0265  lr:0.000001
[ Thu Jul 11 10:34:47 2024 ] 	Batch(4200/7879) done. Loss: 0.1954  lr:0.000001
[ Thu Jul 11 10:35:10 2024 ] 	Batch(4300/7879) done. Loss: 0.1518  lr:0.000001
[ Thu Jul 11 10:35:33 2024 ] 	Batch(4400/7879) done. Loss: 0.0038  lr:0.000001
[ Thu Jul 11 10:35:56 2024 ] 
Training: Epoch [28/120], Step [4499], Loss: 0.011596745811402798, Training Accuracy: 96.1888888888889
[ Thu Jul 11 10:35:56 2024 ] 	Batch(4500/7879) done. Loss: 0.0860  lr:0.000001
[ Thu Jul 11 10:36:18 2024 ] 	Batch(4600/7879) done. Loss: 0.2262  lr:0.000001
[ Thu Jul 11 10:36:41 2024 ] 	Batch(4700/7879) done. Loss: 0.3024  lr:0.000001
[ Thu Jul 11 10:37:04 2024 ] 	Batch(4800/7879) done. Loss: 0.2834  lr:0.000001
[ Thu Jul 11 10:37:27 2024 ] 	Batch(4900/7879) done. Loss: 0.0203  lr:0.000001
[ Thu Jul 11 10:37:49 2024 ] 
Training: Epoch [28/120], Step [4999], Loss: 0.13679911196231842, Training Accuracy: 96.21
[ Thu Jul 11 10:37:49 2024 ] 	Batch(5000/7879) done. Loss: 0.2822  lr:0.000001
[ Thu Jul 11 10:38:12 2024 ] 	Batch(5100/7879) done. Loss: 0.0326  lr:0.000001
[ Thu Jul 11 10:38:34 2024 ] 	Batch(5200/7879) done. Loss: 0.1243  lr:0.000001
[ Thu Jul 11 10:38:58 2024 ] 	Batch(5300/7879) done. Loss: 0.7424  lr:0.000001
[ Thu Jul 11 10:39:21 2024 ] 	Batch(5400/7879) done. Loss: 0.0915  lr:0.000001
[ Thu Jul 11 10:39:44 2024 ] 
Training: Epoch [28/120], Step [5499], Loss: 0.18167126178741455, Training Accuracy: 96.22045454545454
[ Thu Jul 11 10:39:44 2024 ] 	Batch(5500/7879) done. Loss: 0.0352  lr:0.000001
[ Thu Jul 11 10:40:07 2024 ] 	Batch(5600/7879) done. Loss: 0.1022  lr:0.000001
[ Thu Jul 11 10:40:30 2024 ] 	Batch(5700/7879) done. Loss: 0.6859  lr:0.000001
[ Thu Jul 11 10:40:53 2024 ] 	Batch(5800/7879) done. Loss: 0.1574  lr:0.000001
[ Thu Jul 11 10:41:15 2024 ] 	Batch(5900/7879) done. Loss: 0.0038  lr:0.000001
[ Thu Jul 11 10:41:38 2024 ] 
Training: Epoch [28/120], Step [5999], Loss: 0.31683915853500366, Training Accuracy: 96.25
[ Thu Jul 11 10:41:38 2024 ] 	Batch(6000/7879) done. Loss: 0.0301  lr:0.000001
[ Thu Jul 11 10:42:00 2024 ] 	Batch(6100/7879) done. Loss: 0.1673  lr:0.000001
[ Thu Jul 11 10:42:23 2024 ] 	Batch(6200/7879) done. Loss: 0.0747  lr:0.000001
[ Thu Jul 11 10:42:46 2024 ] 	Batch(6300/7879) done. Loss: 0.3737  lr:0.000001
[ Thu Jul 11 10:43:08 2024 ] 	Batch(6400/7879) done. Loss: 0.0601  lr:0.000001
[ Thu Jul 11 10:43:31 2024 ] 
Training: Epoch [28/120], Step [6499], Loss: 0.18486100435256958, Training Accuracy: 96.2326923076923
[ Thu Jul 11 10:43:31 2024 ] 	Batch(6500/7879) done. Loss: 0.0838  lr:0.000001
[ Thu Jul 11 10:43:53 2024 ] 	Batch(6600/7879) done. Loss: 0.2755  lr:0.000001
[ Thu Jul 11 10:44:16 2024 ] 	Batch(6700/7879) done. Loss: 0.0847  lr:0.000001
[ Thu Jul 11 10:44:39 2024 ] 	Batch(6800/7879) done. Loss: 0.3239  lr:0.000001
[ Thu Jul 11 10:45:02 2024 ] 	Batch(6900/7879) done. Loss: 0.4918  lr:0.000001
[ Thu Jul 11 10:45:24 2024 ] 
Training: Epoch [28/120], Step [6999], Loss: 0.057713016867637634, Training Accuracy: 96.20714285714286
[ Thu Jul 11 10:45:25 2024 ] 	Batch(7000/7879) done. Loss: 0.0220  lr:0.000001
[ Thu Jul 11 10:45:47 2024 ] 	Batch(7100/7879) done. Loss: 0.6231  lr:0.000001
[ Thu Jul 11 10:46:10 2024 ] 	Batch(7200/7879) done. Loss: 0.0559  lr:0.000001
[ Thu Jul 11 10:46:33 2024 ] 	Batch(7300/7879) done. Loss: 0.0284  lr:0.000001
[ Thu Jul 11 10:46:55 2024 ] 	Batch(7400/7879) done. Loss: 0.2105  lr:0.000001
[ Thu Jul 11 10:47:18 2024 ] 
Training: Epoch [28/120], Step [7499], Loss: 0.0737563967704773, Training Accuracy: 96.19
[ Thu Jul 11 10:47:18 2024 ] 	Batch(7500/7879) done. Loss: 0.0731  lr:0.000001
[ Thu Jul 11 10:47:41 2024 ] 	Batch(7600/7879) done. Loss: 0.1715  lr:0.000001
[ Thu Jul 11 10:48:04 2024 ] 	Batch(7700/7879) done. Loss: 0.0881  lr:0.000001
[ Thu Jul 11 10:48:26 2024 ] 	Batch(7800/7879) done. Loss: 0.0317  lr:0.000001
[ Thu Jul 11 10:48:44 2024 ] 	Mean training loss: 0.1427.
[ Thu Jul 11 10:48:44 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 10:48:44 2024 ] Training epoch: 30
[ Thu Jul 11 10:48:45 2024 ] 	Batch(0/7879) done. Loss: 0.0496  lr:0.000001
[ Thu Jul 11 10:49:07 2024 ] 	Batch(100/7879) done. Loss: 0.0273  lr:0.000001
[ Thu Jul 11 10:49:30 2024 ] 	Batch(200/7879) done. Loss: 0.0871  lr:0.000001
[ Thu Jul 11 10:49:53 2024 ] 	Batch(300/7879) done. Loss: 0.0854  lr:0.000001
[ Thu Jul 11 10:50:15 2024 ] 	Batch(400/7879) done. Loss: 0.3016  lr:0.000001
[ Thu Jul 11 10:50:38 2024 ] 
Training: Epoch [29/120], Step [499], Loss: 0.37770089507102966, Training Accuracy: 96.55
[ Thu Jul 11 10:50:38 2024 ] 	Batch(500/7879) done. Loss: 0.0757  lr:0.000001
[ Thu Jul 11 10:51:00 2024 ] 	Batch(600/7879) done. Loss: 0.1841  lr:0.000001
[ Thu Jul 11 10:51:23 2024 ] 	Batch(700/7879) done. Loss: 0.0345  lr:0.000001
[ Thu Jul 11 10:51:46 2024 ] 	Batch(800/7879) done. Loss: 0.0039  lr:0.000001
[ Thu Jul 11 10:52:09 2024 ] 	Batch(900/7879) done. Loss: 0.0703  lr:0.000001
[ Thu Jul 11 10:52:31 2024 ] 
Training: Epoch [29/120], Step [999], Loss: 0.1347995549440384, Training Accuracy: 96.475
[ Thu Jul 11 10:52:32 2024 ] 	Batch(1000/7879) done. Loss: 0.0950  lr:0.000001
[ Thu Jul 11 10:52:54 2024 ] 	Batch(1100/7879) done. Loss: 0.1344  lr:0.000001
[ Thu Jul 11 10:53:17 2024 ] 	Batch(1200/7879) done. Loss: 0.0859  lr:0.000001
[ Thu Jul 11 10:53:39 2024 ] 	Batch(1300/7879) done. Loss: 0.3120  lr:0.000001
[ Thu Jul 11 10:54:02 2024 ] 	Batch(1400/7879) done. Loss: 0.0889  lr:0.000001
[ Thu Jul 11 10:54:24 2024 ] 
Training: Epoch [29/120], Step [1499], Loss: 0.02983521670103073, Training Accuracy: 96.525
[ Thu Jul 11 10:54:25 2024 ] 	Batch(1500/7879) done. Loss: 0.1284  lr:0.000001
[ Thu Jul 11 10:54:47 2024 ] 	Batch(1600/7879) done. Loss: 0.1323  lr:0.000001
[ Thu Jul 11 10:55:10 2024 ] 	Batch(1700/7879) done. Loss: 0.0133  lr:0.000001
[ Thu Jul 11 10:55:32 2024 ] 	Batch(1800/7879) done. Loss: 0.0063  lr:0.000001
[ Thu Jul 11 10:55:55 2024 ] 	Batch(1900/7879) done. Loss: 0.1080  lr:0.000001
[ Thu Jul 11 10:56:17 2024 ] 
Training: Epoch [29/120], Step [1999], Loss: 0.034978076815605164, Training Accuracy: 96.5
[ Thu Jul 11 10:56:18 2024 ] 	Batch(2000/7879) done. Loss: 0.0561  lr:0.000001
[ Thu Jul 11 10:56:40 2024 ] 	Batch(2100/7879) done. Loss: 0.5680  lr:0.000001
[ Thu Jul 11 10:57:03 2024 ] 	Batch(2200/7879) done. Loss: 0.0108  lr:0.000001
[ Thu Jul 11 10:57:25 2024 ] 	Batch(2300/7879) done. Loss: 0.1687  lr:0.000001
[ Thu Jul 11 10:57:48 2024 ] 	Batch(2400/7879) done. Loss: 0.0892  lr:0.000001
[ Thu Jul 11 10:58:11 2024 ] 
Training: Epoch [29/120], Step [2499], Loss: 0.13021813333034515, Training Accuracy: 96.405
[ Thu Jul 11 10:58:11 2024 ] 	Batch(2500/7879) done. Loss: 0.0163  lr:0.000001
[ Thu Jul 11 10:58:33 2024 ] 	Batch(2600/7879) done. Loss: 0.0218  lr:0.000001
[ Thu Jul 11 10:58:56 2024 ] 	Batch(2700/7879) done. Loss: 0.0900  lr:0.000001
[ Thu Jul 11 10:59:19 2024 ] 	Batch(2800/7879) done. Loss: 0.1166  lr:0.000001
[ Thu Jul 11 10:59:42 2024 ] 	Batch(2900/7879) done. Loss: 0.3652  lr:0.000001
[ Thu Jul 11 11:00:05 2024 ] 
Training: Epoch [29/120], Step [2999], Loss: 0.1275058388710022, Training Accuracy: 96.43333333333334
[ Thu Jul 11 11:00:05 2024 ] 	Batch(3000/7879) done. Loss: 0.2138  lr:0.000001
[ Thu Jul 11 11:00:28 2024 ] 	Batch(3100/7879) done. Loss: 0.1601  lr:0.000001
[ Thu Jul 11 11:00:52 2024 ] 	Batch(3200/7879) done. Loss: 0.0747  lr:0.000001
[ Thu Jul 11 11:01:15 2024 ] 	Batch(3300/7879) done. Loss: 0.0922  lr:0.000001
[ Thu Jul 11 11:01:39 2024 ] 	Batch(3400/7879) done. Loss: 0.1726  lr:0.000001
[ Thu Jul 11 11:02:02 2024 ] 
Training: Epoch [29/120], Step [3499], Loss: 0.09221803396940231, Training Accuracy: 96.41428571428573
[ Thu Jul 11 11:02:02 2024 ] 	Batch(3500/7879) done. Loss: 0.0775  lr:0.000001
[ Thu Jul 11 11:02:25 2024 ] 	Batch(3600/7879) done. Loss: 0.0347  lr:0.000001
[ Thu Jul 11 11:02:48 2024 ] 	Batch(3700/7879) done. Loss: 0.0164  lr:0.000001
[ Thu Jul 11 11:03:11 2024 ] 	Batch(3800/7879) done. Loss: 0.1962  lr:0.000001
[ Thu Jul 11 11:03:34 2024 ] 	Batch(3900/7879) done. Loss: 0.0038  lr:0.000001
[ Thu Jul 11 11:03:57 2024 ] 
Training: Epoch [29/120], Step [3999], Loss: 0.1591407060623169, Training Accuracy: 96.409375
[ Thu Jul 11 11:03:58 2024 ] 	Batch(4000/7879) done. Loss: 0.0180  lr:0.000001
[ Thu Jul 11 11:04:21 2024 ] 	Batch(4100/7879) done. Loss: 0.1922  lr:0.000001
[ Thu Jul 11 11:04:44 2024 ] 	Batch(4200/7879) done. Loss: 0.1046  lr:0.000001
[ Thu Jul 11 11:05:08 2024 ] 	Batch(4300/7879) done. Loss: 0.5397  lr:0.000001
[ Thu Jul 11 11:05:30 2024 ] 	Batch(4400/7879) done. Loss: 0.0096  lr:0.000001
[ Thu Jul 11 11:05:53 2024 ] 
Training: Epoch [29/120], Step [4499], Loss: 0.4611686170101166, Training Accuracy: 96.39444444444445
[ Thu Jul 11 11:05:53 2024 ] 	Batch(4500/7879) done. Loss: 0.0254  lr:0.000001
[ Thu Jul 11 11:06:15 2024 ] 	Batch(4600/7879) done. Loss: 0.0901  lr:0.000001
[ Thu Jul 11 11:06:39 2024 ] 	Batch(4700/7879) done. Loss: 1.2450  lr:0.000001
[ Thu Jul 11 11:07:02 2024 ] 	Batch(4800/7879) done. Loss: 0.2444  lr:0.000001
[ Thu Jul 11 11:07:25 2024 ] 	Batch(4900/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul 11 11:07:48 2024 ] 
Training: Epoch [29/120], Step [4999], Loss: 0.014352161437273026, Training Accuracy: 96.38250000000001
[ Thu Jul 11 11:07:49 2024 ] 	Batch(5000/7879) done. Loss: 0.1995  lr:0.000001
[ Thu Jul 11 11:08:12 2024 ] 	Batch(5100/7879) done. Loss: 0.0124  lr:0.000001
[ Thu Jul 11 11:08:35 2024 ] 	Batch(5200/7879) done. Loss: 0.3339  lr:0.000001
[ Thu Jul 11 11:08:58 2024 ] 	Batch(5300/7879) done. Loss: 0.2867  lr:0.000001
[ Thu Jul 11 11:09:21 2024 ] 	Batch(5400/7879) done. Loss: 0.0640  lr:0.000001
[ Thu Jul 11 11:09:43 2024 ] 
Training: Epoch [29/120], Step [5499], Loss: 0.035637833178043365, Training Accuracy: 96.4090909090909
[ Thu Jul 11 11:09:43 2024 ] 	Batch(5500/7879) done. Loss: 0.5221  lr:0.000001
[ Thu Jul 11 11:10:06 2024 ] 	Batch(5600/7879) done. Loss: 0.0161  lr:0.000001
[ Thu Jul 11 11:10:29 2024 ] 	Batch(5700/7879) done. Loss: 0.0151  lr:0.000001
[ Thu Jul 11 11:10:51 2024 ] 	Batch(5800/7879) done. Loss: 0.0291  lr:0.000001
[ Thu Jul 11 11:11:14 2024 ] 	Batch(5900/7879) done. Loss: 0.0391  lr:0.000001
[ Thu Jul 11 11:11:36 2024 ] 
Training: Epoch [29/120], Step [5999], Loss: 0.08440212905406952, Training Accuracy: 96.39583333333334
[ Thu Jul 11 11:11:37 2024 ] 	Batch(6000/7879) done. Loss: 0.5006  lr:0.000001
[ Thu Jul 11 11:11:59 2024 ] 	Batch(6100/7879) done. Loss: 0.1689  lr:0.000001
[ Thu Jul 11 11:12:22 2024 ] 	Batch(6200/7879) done. Loss: 0.4512  lr:0.000001
[ Thu Jul 11 11:12:44 2024 ] 	Batch(6300/7879) done. Loss: 0.0924  lr:0.000001
[ Thu Jul 11 11:13:07 2024 ] 	Batch(6400/7879) done. Loss: 0.0131  lr:0.000001
[ Thu Jul 11 11:13:30 2024 ] 
Training: Epoch [29/120], Step [6499], Loss: 0.2296917587518692, Training Accuracy: 96.37692307692308
[ Thu Jul 11 11:13:30 2024 ] 	Batch(6500/7879) done. Loss: 0.0340  lr:0.000001
[ Thu Jul 11 11:13:52 2024 ] 	Batch(6600/7879) done. Loss: 0.0505  lr:0.000001
[ Thu Jul 11 11:14:15 2024 ] 	Batch(6700/7879) done. Loss: 0.0115  lr:0.000001
[ Thu Jul 11 11:14:38 2024 ] 	Batch(6800/7879) done. Loss: 0.0805  lr:0.000001
[ Thu Jul 11 11:15:00 2024 ] 	Batch(6900/7879) done. Loss: 0.1649  lr:0.000001
[ Thu Jul 11 11:15:22 2024 ] 
Training: Epoch [29/120], Step [6999], Loss: 0.12014173716306686, Training Accuracy: 96.37142857142858
[ Thu Jul 11 11:15:23 2024 ] 	Batch(7000/7879) done. Loss: 0.1282  lr:0.000001
[ Thu Jul 11 11:15:45 2024 ] 	Batch(7100/7879) done. Loss: 0.1044  lr:0.000001
[ Thu Jul 11 11:16:08 2024 ] 	Batch(7200/7879) done. Loss: 0.1545  lr:0.000001
[ Thu Jul 11 11:16:31 2024 ] 	Batch(7300/7879) done. Loss: 0.1191  lr:0.000001
[ Thu Jul 11 11:16:53 2024 ] 	Batch(7400/7879) done. Loss: 0.0241  lr:0.000001
[ Thu Jul 11 11:17:16 2024 ] 
Training: Epoch [29/120], Step [7499], Loss: 0.42657729983329773, Training Accuracy: 96.355
[ Thu Jul 11 11:17:16 2024 ] 	Batch(7500/7879) done. Loss: 0.1654  lr:0.000001
[ Thu Jul 11 11:17:39 2024 ] 	Batch(7600/7879) done. Loss: 0.0072  lr:0.000001
[ Thu Jul 11 11:18:03 2024 ] 	Batch(7700/7879) done. Loss: 0.0540  lr:0.000001
[ Thu Jul 11 11:18:25 2024 ] 	Batch(7800/7879) done. Loss: 0.1894  lr:0.000001
[ Thu Jul 11 11:18:43 2024 ] 	Mean training loss: 0.1396.
[ Thu Jul 11 11:18:43 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 11:18:43 2024 ] Eval epoch: 30
[ Thu Jul 11 11:24:39 2024 ] 	Mean val loss of 6365 batches: 1.015964882622304.
[ Thu Jul 11 11:24:39 2024 ] 
Validation: Epoch [29/120], Samples [39513.0/50919], Loss: 0.5351349711418152, Validation Accuracy: 77.59971719790255
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 1 : 188 / 275 = 68 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 2 : 237 / 273 = 86 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 3 : 227 / 273 = 83 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 4 : 225 / 275 = 81 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 5 : 239 / 275 = 86 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 6 : 214 / 275 = 77 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 7 : 254 / 273 = 93 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 8 : 265 / 273 = 97 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 9 : 201 / 273 = 73 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 10 : 120 / 273 = 43 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 11 : 158 / 272 = 58 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 12 : 228 / 271 = 84 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 13 : 265 / 275 = 96 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 14 : 263 / 276 = 95 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 15 : 218 / 273 = 79 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 16 : 158 / 274 = 57 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 17 : 240 / 273 = 87 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 18 : 234 / 274 = 85 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 19 : 239 / 272 = 87 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 20 : 253 / 273 = 92 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 21 : 232 / 274 = 84 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 22 : 253 / 274 = 92 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 23 : 250 / 276 = 90 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 24 : 240 / 274 = 87 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 25 : 265 / 275 = 96 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 26 : 270 / 276 = 97 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 27 : 235 / 275 = 85 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 28 : 167 / 275 = 60 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 29 : 151 / 275 = 54 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 30 : 175 / 276 = 63 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 31 : 235 / 276 = 85 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 32 : 244 / 276 = 88 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 33 : 231 / 276 = 83 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 34 : 243 / 276 = 88 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 35 : 239 / 275 = 86 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 36 : 228 / 276 = 82 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 37 : 250 / 276 = 90 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 38 : 236 / 276 = 85 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 39 : 247 / 276 = 89 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 40 : 191 / 276 = 69 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 41 : 269 / 276 = 97 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 42 : 254 / 275 = 92 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 43 : 185 / 276 = 67 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 44 : 245 / 276 = 88 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 45 : 257 / 276 = 93 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 46 : 214 / 276 = 77 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 47 : 209 / 275 = 76 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 48 : 219 / 275 = 79 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 49 : 209 / 274 = 76 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 50 : 235 / 276 = 85 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 51 : 252 / 276 = 91 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 52 : 234 / 276 = 84 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 53 : 229 / 276 = 82 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 54 : 257 / 274 = 93 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 55 : 240 / 276 = 86 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 56 : 244 / 275 = 88 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 57 : 269 / 276 = 97 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 58 : 266 / 273 = 97 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 59 : 253 / 276 = 91 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 60 : 472 / 561 = 84 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 61 : 475 / 566 = 83 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 62 : 412 / 572 = 72 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 63 : 485 / 570 = 85 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 64 : 427 / 574 = 74 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 65 : 509 / 573 = 88 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 66 : 430 / 573 = 75 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 67 : 392 / 575 = 68 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 68 : 387 / 575 = 67 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 69 : 475 / 575 = 82 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 70 : 250 / 575 = 43 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 71 : 225 / 575 = 39 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 72 : 120 / 571 = 21 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 73 : 265 / 570 = 46 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 74 : 366 / 569 = 64 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 75 : 367 / 573 = 64 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 76 : 360 / 574 = 62 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 77 : 376 / 573 = 65 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 78 : 440 / 575 = 76 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 79 : 546 / 574 = 95 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 80 : 464 / 573 = 80 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 81 : 374 / 575 = 65 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 82 : 351 / 575 = 61 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 83 : 259 / 572 = 45 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 84 : 446 / 574 = 77 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 85 : 387 / 574 = 67 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 86 : 486 / 575 = 84 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 87 : 487 / 576 = 84 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 88 : 399 / 575 = 69 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 89 : 417 / 576 = 72 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 90 : 223 / 574 = 38 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 91 : 463 / 568 = 81 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 92 : 420 / 576 = 72 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 93 : 379 / 573 = 66 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 94 : 526 / 574 = 91 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 95 : 540 / 575 = 93 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 96 : 559 / 575 = 97 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 97 : 555 / 574 = 96 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 98 : 538 / 575 = 93 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 99 : 530 / 574 = 92 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 100 : 471 / 574 = 82 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 101 : 532 / 574 = 92 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 102 : 328 / 575 = 57 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 103 : 487 / 576 = 84 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 104 : 269 / 575 = 46 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 105 : 272 / 575 = 47 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 106 : 348 / 576 = 60 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 107 : 491 / 576 = 85 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 108 : 466 / 575 = 81 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 109 : 417 / 575 = 72 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 110 : 496 / 575 = 86 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 111 : 540 / 576 = 93 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 112 : 539 / 575 = 93 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 113 : 521 / 576 = 90 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 114 : 507 / 576 = 88 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 115 : 526 / 576 = 91 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 116 : 488 / 575 = 84 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 117 : 475 / 575 = 82 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 118 : 461 / 575 = 80 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 119 : 510 / 576 = 88 %
[ Thu Jul 11 11:24:39 2024 ] Accuracy of 120 : 239 / 274 = 87 %
[ Thu Jul 11 11:24:39 2024 ] Training epoch: 31
[ Thu Jul 11 11:24:40 2024 ] 	Batch(0/7879) done. Loss: 0.0215  lr:0.000001
[ Thu Jul 11 11:25:03 2024 ] 	Batch(100/7879) done. Loss: 0.0390  lr:0.000001
[ Thu Jul 11 11:25:26 2024 ] 	Batch(200/7879) done. Loss: 0.2508  lr:0.000001
[ Thu Jul 11 11:25:50 2024 ] 	Batch(300/7879) done. Loss: 0.2024  lr:0.000001
[ Thu Jul 11 11:26:13 2024 ] 	Batch(400/7879) done. Loss: 0.1500  lr:0.000001
[ Thu Jul 11 11:26:36 2024 ] 
Training: Epoch [30/120], Step [499], Loss: 0.08583275228738785, Training Accuracy: 96.65
[ Thu Jul 11 11:26:37 2024 ] 	Batch(500/7879) done. Loss: 0.1351  lr:0.000001
[ Thu Jul 11 11:27:00 2024 ] 	Batch(600/7879) done. Loss: 0.0009  lr:0.000001
[ Thu Jul 11 11:27:23 2024 ] 	Batch(700/7879) done. Loss: 0.0291  lr:0.000001
[ Thu Jul 11 11:27:47 2024 ] 	Batch(800/7879) done. Loss: 0.1231  lr:0.000001
[ Thu Jul 11 11:28:10 2024 ] 	Batch(900/7879) done. Loss: 0.6498  lr:0.000001
[ Thu Jul 11 11:28:33 2024 ] 
Training: Epoch [30/120], Step [999], Loss: 0.05570502579212189, Training Accuracy: 96.39999999999999
[ Thu Jul 11 11:28:33 2024 ] 	Batch(1000/7879) done. Loss: 0.0057  lr:0.000001
[ Thu Jul 11 11:28:57 2024 ] 	Batch(1100/7879) done. Loss: 0.1460  lr:0.000001
[ Thu Jul 11 11:29:20 2024 ] 	Batch(1200/7879) done. Loss: 0.1605  lr:0.000001
[ Thu Jul 11 11:29:44 2024 ] 	Batch(1300/7879) done. Loss: 0.5647  lr:0.000001
[ Thu Jul 11 11:30:07 2024 ] 	Batch(1400/7879) done. Loss: 0.0557  lr:0.000001
[ Thu Jul 11 11:30:30 2024 ] 
Training: Epoch [30/120], Step [1499], Loss: 0.2888527810573578, Training Accuracy: 96.29166666666666
[ Thu Jul 11 11:30:30 2024 ] 	Batch(1500/7879) done. Loss: 0.3122  lr:0.000001
[ Thu Jul 11 11:30:54 2024 ] 	Batch(1600/7879) done. Loss: 0.0308  lr:0.000001
[ Thu Jul 11 11:31:17 2024 ] 	Batch(1700/7879) done. Loss: 0.2676  lr:0.000001
[ Thu Jul 11 11:31:41 2024 ] 	Batch(1800/7879) done. Loss: 0.0848  lr:0.000001
[ Thu Jul 11 11:32:04 2024 ] 	Batch(1900/7879) done. Loss: 0.3169  lr:0.000001
[ Thu Jul 11 11:32:27 2024 ] 
Training: Epoch [30/120], Step [1999], Loss: 0.03156815096735954, Training Accuracy: 96.33125
[ Thu Jul 11 11:32:27 2024 ] 	Batch(2000/7879) done. Loss: 0.3289  lr:0.000001
[ Thu Jul 11 11:32:51 2024 ] 	Batch(2100/7879) done. Loss: 0.0673  lr:0.000001
[ Thu Jul 11 11:33:14 2024 ] 	Batch(2200/7879) done. Loss: 0.1023  lr:0.000001
[ Thu Jul 11 11:33:37 2024 ] 	Batch(2300/7879) done. Loss: 0.0082  lr:0.000001
[ Thu Jul 11 11:34:01 2024 ] 	Batch(2400/7879) done. Loss: 0.0723  lr:0.000001
[ Thu Jul 11 11:34:24 2024 ] 
Training: Epoch [30/120], Step [2499], Loss: 0.015474853105843067, Training Accuracy: 96.42500000000001
[ Thu Jul 11 11:34:24 2024 ] 	Batch(2500/7879) done. Loss: 0.0485  lr:0.000001
[ Thu Jul 11 11:34:48 2024 ] 	Batch(2600/7879) done. Loss: 0.0282  lr:0.000001
[ Thu Jul 11 11:35:11 2024 ] 	Batch(2700/7879) done. Loss: 0.0325  lr:0.000001
[ Thu Jul 11 11:35:34 2024 ] 	Batch(2800/7879) done. Loss: 0.0567  lr:0.000001
[ Thu Jul 11 11:35:57 2024 ] 	Batch(2900/7879) done. Loss: 0.5429  lr:0.000001
[ Thu Jul 11 11:36:20 2024 ] 
Training: Epoch [30/120], Step [2999], Loss: 0.13761933147907257, Training Accuracy: 96.47916666666667
[ Thu Jul 11 11:36:20 2024 ] 	Batch(3000/7879) done. Loss: 0.0185  lr:0.000001
[ Thu Jul 11 11:36:42 2024 ] 	Batch(3100/7879) done. Loss: 0.2274  lr:0.000001
[ Thu Jul 11 11:37:05 2024 ] 	Batch(3200/7879) done. Loss: 0.4743  lr:0.000001
[ Thu Jul 11 11:37:28 2024 ] 	Batch(3300/7879) done. Loss: 0.1128  lr:0.000001
[ Thu Jul 11 11:37:51 2024 ] 	Batch(3400/7879) done. Loss: 0.2616  lr:0.000001
[ Thu Jul 11 11:38:13 2024 ] 
Training: Epoch [30/120], Step [3499], Loss: 0.11120130121707916, Training Accuracy: 96.33928571428572
[ Thu Jul 11 11:38:13 2024 ] 	Batch(3500/7879) done. Loss: 0.0699  lr:0.000001
[ Thu Jul 11 11:38:36 2024 ] 	Batch(3600/7879) done. Loss: 0.0124  lr:0.000001
[ Thu Jul 11 11:38:59 2024 ] 	Batch(3700/7879) done. Loss: 0.1031  lr:0.000001
[ Thu Jul 11 11:39:22 2024 ] 	Batch(3800/7879) done. Loss: 0.1119  lr:0.000001
[ Thu Jul 11 11:39:44 2024 ] 	Batch(3900/7879) done. Loss: 0.1511  lr:0.000001
[ Thu Jul 11 11:40:07 2024 ] 
Training: Epoch [30/120], Step [3999], Loss: 0.007360853720456362, Training Accuracy: 96.29375
[ Thu Jul 11 11:40:07 2024 ] 	Batch(4000/7879) done. Loss: 0.0727  lr:0.000001
[ Thu Jul 11 11:40:30 2024 ] 	Batch(4100/7879) done. Loss: 0.6656  lr:0.000001
[ Thu Jul 11 11:40:53 2024 ] 	Batch(4200/7879) done. Loss: 0.0135  lr:0.000001
[ Thu Jul 11 11:41:16 2024 ] 	Batch(4300/7879) done. Loss: 0.1203  lr:0.000001
[ Thu Jul 11 11:41:39 2024 ] 	Batch(4400/7879) done. Loss: 0.0840  lr:0.000001
[ Thu Jul 11 11:42:02 2024 ] 
Training: Epoch [30/120], Step [4499], Loss: 0.042227718979120255, Training Accuracy: 96.33055555555555
[ Thu Jul 11 11:42:02 2024 ] 	Batch(4500/7879) done. Loss: 0.4863  lr:0.000001
[ Thu Jul 11 11:42:25 2024 ] 	Batch(4600/7879) done. Loss: 0.0465  lr:0.000001
[ Thu Jul 11 11:42:49 2024 ] 	Batch(4700/7879) done. Loss: 0.0237  lr:0.000001
[ Thu Jul 11 11:43:12 2024 ] 	Batch(4800/7879) done. Loss: 0.0147  lr:0.000001
[ Thu Jul 11 11:43:35 2024 ] 	Batch(4900/7879) done. Loss: 0.4914  lr:0.000001
[ Thu Jul 11 11:43:58 2024 ] 
Training: Epoch [30/120], Step [4999], Loss: 0.025724560022354126, Training Accuracy: 96.2675
[ Thu Jul 11 11:43:58 2024 ] 	Batch(5000/7879) done. Loss: 0.2710  lr:0.000001
[ Thu Jul 11 11:44:22 2024 ] 	Batch(5100/7879) done. Loss: 0.0463  lr:0.000001
[ Thu Jul 11 11:44:45 2024 ] 	Batch(5200/7879) done. Loss: 0.7244  lr:0.000001
[ Thu Jul 11 11:45:08 2024 ] 	Batch(5300/7879) done. Loss: 0.1518  lr:0.000001
[ Thu Jul 11 11:45:31 2024 ] 	Batch(5400/7879) done. Loss: 0.0514  lr:0.000001
[ Thu Jul 11 11:45:54 2024 ] 
Training: Epoch [30/120], Step [5499], Loss: 0.8392859697341919, Training Accuracy: 96.22727272727273
[ Thu Jul 11 11:45:54 2024 ] 	Batch(5500/7879) done. Loss: 0.2756  lr:0.000001
[ Thu Jul 11 11:46:18 2024 ] 	Batch(5600/7879) done. Loss: 0.0025  lr:0.000001
[ Thu Jul 11 11:46:41 2024 ] 	Batch(5700/7879) done. Loss: 0.2554  lr:0.000001
[ Thu Jul 11 11:47:04 2024 ] 	Batch(5800/7879) done. Loss: 0.0286  lr:0.000001
[ Thu Jul 11 11:47:27 2024 ] 	Batch(5900/7879) done. Loss: 0.0517  lr:0.000001
[ Thu Jul 11 11:47:50 2024 ] 
Training: Epoch [30/120], Step [5999], Loss: 0.040925391018390656, Training Accuracy: 96.22291666666666
[ Thu Jul 11 11:47:51 2024 ] 	Batch(6000/7879) done. Loss: 0.0149  lr:0.000001
[ Thu Jul 11 11:48:14 2024 ] 	Batch(6100/7879) done. Loss: 0.0205  lr:0.000001
[ Thu Jul 11 11:48:37 2024 ] 	Batch(6200/7879) done. Loss: 0.0376  lr:0.000001
[ Thu Jul 11 11:49:00 2024 ] 	Batch(6300/7879) done. Loss: 0.3506  lr:0.000001
[ Thu Jul 11 11:49:24 2024 ] 	Batch(6400/7879) done. Loss: 0.0312  lr:0.000001
[ Thu Jul 11 11:49:47 2024 ] 
Training: Epoch [30/120], Step [6499], Loss: 0.01691124029457569, Training Accuracy: 96.22115384615385
[ Thu Jul 11 11:49:47 2024 ] 	Batch(6500/7879) done. Loss: 0.1026  lr:0.000001
[ Thu Jul 11 11:50:10 2024 ] 	Batch(6600/7879) done. Loss: 0.0741  lr:0.000001
[ Thu Jul 11 11:50:33 2024 ] 	Batch(6700/7879) done. Loss: 0.0429  lr:0.000001
[ Thu Jul 11 11:50:56 2024 ] 	Batch(6800/7879) done. Loss: 0.0741  lr:0.000001
[ Thu Jul 11 11:51:19 2024 ] 	Batch(6900/7879) done. Loss: 0.1703  lr:0.000001
[ Thu Jul 11 11:51:42 2024 ] 
Training: Epoch [30/120], Step [6999], Loss: 0.1888042837381363, Training Accuracy: 96.19642857142857
[ Thu Jul 11 11:51:42 2024 ] 	Batch(7000/7879) done. Loss: 0.0941  lr:0.000001
[ Thu Jul 11 11:52:05 2024 ] 	Batch(7100/7879) done. Loss: 0.0470  lr:0.000001
[ Thu Jul 11 11:52:28 2024 ] 	Batch(7200/7879) done. Loss: 0.2140  lr:0.000001
[ Thu Jul 11 11:52:51 2024 ] 	Batch(7300/7879) done. Loss: 0.2437  lr:0.000001
[ Thu Jul 11 11:53:14 2024 ] 	Batch(7400/7879) done. Loss: 0.1129  lr:0.000001
[ Thu Jul 11 11:53:37 2024 ] 
Training: Epoch [30/120], Step [7499], Loss: 0.1823694258928299, Training Accuracy: 96.195
[ Thu Jul 11 11:53:37 2024 ] 	Batch(7500/7879) done. Loss: 0.0745  lr:0.000001
[ Thu Jul 11 11:54:00 2024 ] 	Batch(7600/7879) done. Loss: 0.2145  lr:0.000001
[ Thu Jul 11 11:54:23 2024 ] 	Batch(7700/7879) done. Loss: 0.2305  lr:0.000001
[ Thu Jul 11 11:54:46 2024 ] 	Batch(7800/7879) done. Loss: 0.0338  lr:0.000001
[ Thu Jul 11 11:55:04 2024 ] 	Mean training loss: 0.1384.
[ Thu Jul 11 11:55:04 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Thu Jul 11 11:55:04 2024 ] Training epoch: 32
[ Thu Jul 11 11:55:05 2024 ] 	Batch(0/7879) done. Loss: 0.2059  lr:0.000001
[ Thu Jul 11 11:55:28 2024 ] 	Batch(100/7879) done. Loss: 0.0170  lr:0.000001
[ Thu Jul 11 11:55:51 2024 ] 	Batch(200/7879) done. Loss: 0.0118  lr:0.000001
[ Thu Jul 11 11:56:14 2024 ] 	Batch(300/7879) done. Loss: 0.0242  lr:0.000001
[ Thu Jul 11 11:56:37 2024 ] 	Batch(400/7879) done. Loss: 0.0448  lr:0.000001
[ Thu Jul 11 11:56:59 2024 ] 
Training: Epoch [31/120], Step [499], Loss: 0.032569803297519684, Training Accuracy: 96.65
[ Thu Jul 11 11:56:59 2024 ] 	Batch(500/7879) done. Loss: 0.1640  lr:0.000001
[ Thu Jul 11 11:57:22 2024 ] 	Batch(600/7879) done. Loss: 0.2266  lr:0.000001
[ Thu Jul 11 11:57:45 2024 ] 	Batch(700/7879) done. Loss: 0.0202  lr:0.000001
[ Thu Jul 11 11:58:08 2024 ] 	Batch(800/7879) done. Loss: 0.0359  lr:0.000001
[ Thu Jul 11 11:58:30 2024 ] 	Batch(900/7879) done. Loss: 0.2612  lr:0.000001
[ Thu Jul 11 11:58:53 2024 ] 
Training: Epoch [31/120], Step [999], Loss: 0.022697631269693375, Training Accuracy: 96.45
[ Thu Jul 11 11:58:53 2024 ] 	Batch(1000/7879) done. Loss: 0.0646  lr:0.000001
[ Thu Jul 11 11:59:16 2024 ] 	Batch(1100/7879) done. Loss: 0.1728  lr:0.000001
[ Thu Jul 11 11:59:38 2024 ] 	Batch(1200/7879) done. Loss: 0.0164  lr:0.000001
[ Thu Jul 11 12:00:01 2024 ] 	Batch(1300/7879) done. Loss: 0.0123  lr:0.000001
[ Thu Jul 11 12:00:24 2024 ] 	Batch(1400/7879) done. Loss: 0.0221  lr:0.000001
[ Thu Jul 11 12:00:46 2024 ] 
Training: Epoch [31/120], Step [1499], Loss: 0.39884623885154724, Training Accuracy: 96.30833333333332
[ Thu Jul 11 12:00:46 2024 ] 	Batch(1500/7879) done. Loss: 0.1204  lr:0.000001
[ Thu Jul 11 12:01:09 2024 ] 	Batch(1600/7879) done. Loss: 0.1433  lr:0.000001
[ Thu Jul 11 12:01:32 2024 ] 	Batch(1700/7879) done. Loss: 0.0416  lr:0.000001
[ Thu Jul 11 12:01:55 2024 ] 	Batch(1800/7879) done. Loss: 0.2202  lr:0.000001
[ Thu Jul 11 12:02:18 2024 ] 	Batch(1900/7879) done. Loss: 0.1193  lr:0.000001
[ Thu Jul 11 12:02:40 2024 ] 
Training: Epoch [31/120], Step [1999], Loss: 0.011659301817417145, Training Accuracy: 96.33749999999999
[ Thu Jul 11 12:02:41 2024 ] 	Batch(2000/7879) done. Loss: 0.1617  lr:0.000001
[ Thu Jul 11 12:03:03 2024 ] 	Batch(2100/7879) done. Loss: 0.5132  lr:0.000001
[ Thu Jul 11 12:03:26 2024 ] 	Batch(2200/7879) done. Loss: 0.2130  lr:0.000001
[ Thu Jul 11 12:03:49 2024 ] 	Batch(2300/7879) done. Loss: 0.0303  lr:0.000001
[ Thu Jul 11 12:04:12 2024 ] 	Batch(2400/7879) done. Loss: 0.1111  lr:0.000001
[ Thu Jul 11 12:04:35 2024 ] 
Training: Epoch [31/120], Step [2499], Loss: 0.03999024257063866, Training Accuracy: 96.32
[ Thu Jul 11 12:04:35 2024 ] 	Batch(2500/7879) done. Loss: 0.0985  lr:0.000001
[ Thu Jul 11 12:04:58 2024 ] 	Batch(2600/7879) done. Loss: 0.0156  lr:0.000001
[ Thu Jul 11 12:05:21 2024 ] 	Batch(2700/7879) done. Loss: 0.0178  lr:0.000001
[ Thu Jul 11 12:05:44 2024 ] 	Batch(2800/7879) done. Loss: 0.3469  lr:0.000001
[ Thu Jul 11 12:06:08 2024 ] 	Batch(2900/7879) done. Loss: 0.2226  lr:0.000001
[ Thu Jul 11 12:06:31 2024 ] 
Training: Epoch [31/120], Step [2999], Loss: 0.1707371324300766, Training Accuracy: 96.32916666666667
[ Thu Jul 11 12:06:31 2024 ] 	Batch(3000/7879) done. Loss: 0.1204  lr:0.000001
[ Thu Jul 11 12:06:54 2024 ] 	Batch(3100/7879) done. Loss: 0.2399  lr:0.000001
[ Thu Jul 11 12:07:17 2024 ] 	Batch(3200/7879) done. Loss: 0.0457  lr:0.000001
[ Thu Jul 11 12:07:40 2024 ] 	Batch(3300/7879) done. Loss: 0.0382  lr:0.000001
[ Thu Jul 11 12:08:03 2024 ] 	Batch(3400/7879) done. Loss: 0.0128  lr:0.000001
[ Thu Jul 11 12:08:26 2024 ] 
Training: Epoch [31/120], Step [3499], Loss: 0.013608616776764393, Training Accuracy: 96.31071428571428
[ Thu Jul 11 12:08:26 2024 ] 	Batch(3500/7879) done. Loss: 0.0879  lr:0.000001
[ Thu Jul 11 12:08:49 2024 ] 	Batch(3600/7879) done. Loss: 0.0115  lr:0.000001
[ Thu Jul 11 12:09:12 2024 ] 	Batch(3700/7879) done. Loss: 0.0074  lr:0.000001
[ Thu Jul 11 12:09:35 2024 ] 	Batch(3800/7879) done. Loss: 0.1093  lr:0.000001
[ Thu Jul 11 12:09:58 2024 ] 	Batch(3900/7879) done. Loss: 0.0107  lr:0.000001
[ Thu Jul 11 12:10:21 2024 ] 
Training: Epoch [31/120], Step [3999], Loss: 0.3175598680973053, Training Accuracy: 96.415625
[ Thu Jul 11 12:10:21 2024 ] 	Batch(4000/7879) done. Loss: 0.6688  lr:0.000001
[ Thu Jul 11 12:10:44 2024 ] 	Batch(4100/7879) done. Loss: 0.0680  lr:0.000001
[ Thu Jul 11 12:11:07 2024 ] 	Batch(4200/7879) done. Loss: 0.0107  lr:0.000001
[ Thu Jul 11 12:11:30 2024 ] 	Batch(4300/7879) done. Loss: 0.0824  lr:0.000001
[ Thu Jul 11 12:11:53 2024 ] 	Batch(4400/7879) done. Loss: 0.0331  lr:0.000001
[ Thu Jul 11 12:12:16 2024 ] 
Training: Epoch [31/120], Step [4499], Loss: 0.18720948696136475, Training Accuracy: 96.36111111111111
[ Thu Jul 11 12:12:16 2024 ] 	Batch(4500/7879) done. Loss: 0.2658  lr:0.000001
[ Thu Jul 11 12:12:40 2024 ] 	Batch(4600/7879) done. Loss: 0.0695  lr:0.000001
[ Thu Jul 11 12:13:03 2024 ] 	Batch(4700/7879) done. Loss: 0.0345  lr:0.000001
[ Thu Jul 11 12:13:26 2024 ] 	Batch(4800/7879) done. Loss: 0.0376  lr:0.000001
[ Thu Jul 11 12:13:49 2024 ] 	Batch(4900/7879) done. Loss: 0.0325  lr:0.000001
[ Thu Jul 11 12:14:12 2024 ] 
Training: Epoch [31/120], Step [4999], Loss: 0.13972918689250946, Training Accuracy: 96.3425
[ Thu Jul 11 12:14:12 2024 ] 	Batch(5000/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul 11 12:14:35 2024 ] 	Batch(5100/7879) done. Loss: 0.3200  lr:0.000001
[ Thu Jul 11 12:14:58 2024 ] 	Batch(5200/7879) done. Loss: 0.0320  lr:0.000001
[ Thu Jul 11 12:15:21 2024 ] 	Batch(5300/7879) done. Loss: 0.0704  lr:0.000001
[ Thu Jul 11 12:15:44 2024 ] 	Batch(5400/7879) done. Loss: 0.0925  lr:0.000001
[ Thu Jul 11 12:16:07 2024 ] 
Training: Epoch [31/120], Step [5499], Loss: 0.02226496860384941, Training Accuracy: 96.33863636363637
[ Thu Jul 11 12:16:07 2024 ] 	Batch(5500/7879) done. Loss: 0.1345  lr:0.000001
[ Thu Jul 11 12:16:30 2024 ] 	Batch(5600/7879) done. Loss: 0.1218  lr:0.000001
[ Thu Jul 11 12:16:54 2024 ] 	Batch(5700/7879) done. Loss: 0.0720  lr:0.000001
[ Thu Jul 11 12:17:17 2024 ] 	Batch(5800/7879) done. Loss: 0.0114  lr:0.000001
[ Thu Jul 11 12:17:40 2024 ] 	Batch(5900/7879) done. Loss: 0.0037  lr:0.000001
[ Thu Jul 11 12:18:03 2024 ] 
Training: Epoch [31/120], Step [5999], Loss: 0.3971635103225708, Training Accuracy: 96.28750000000001
[ Thu Jul 11 12:18:04 2024 ] 	Batch(6000/7879) done. Loss: 0.1030  lr:0.000001
[ Thu Jul 11 12:18:27 2024 ] 	Batch(6100/7879) done. Loss: 0.0404  lr:0.000001
[ Thu Jul 11 12:18:49 2024 ] 	Batch(6200/7879) done. Loss: 0.0105  lr:0.000001
[ Thu Jul 11 12:19:12 2024 ] 	Batch(6300/7879) done. Loss: 0.0431  lr:0.000001
[ Thu Jul 11 12:19:35 2024 ] 	Batch(6400/7879) done. Loss: 0.0109  lr:0.000001
[ Thu Jul 11 12:19:57 2024 ] 
Training: Epoch [31/120], Step [6499], Loss: 0.009913904592394829, Training Accuracy: 96.275
[ Thu Jul 11 12:19:58 2024 ] 	Batch(6500/7879) done. Loss: 0.1318  lr:0.000001
[ Thu Jul 11 12:20:20 2024 ] 	Batch(6600/7879) done. Loss: 0.1272  lr:0.000001
[ Thu Jul 11 12:20:43 2024 ] 	Batch(6700/7879) done. Loss: 0.0504  lr:0.000001
[ Thu Jul 11 12:21:07 2024 ] 	Batch(6800/7879) done. Loss: 0.2507  lr:0.000001
[ Thu Jul 11 12:21:30 2024 ] 	Batch(6900/7879) done. Loss: 0.1005  lr:0.000001
[ Thu Jul 11 12:21:53 2024 ] 
Training: Epoch [31/120], Step [6999], Loss: 0.051192350685596466, Training Accuracy: 96.22142857142858
[ Thu Jul 11 12:21:53 2024 ] 	Batch(7000/7879) done. Loss: 0.0299  lr:0.000001
[ Thu Jul 11 12:22:17 2024 ] 	Batch(7100/7879) done. Loss: 0.0085  lr:0.000001
[ Thu Jul 11 12:22:40 2024 ] 	Batch(7200/7879) done. Loss: 0.0697  lr:0.000001
[ Thu Jul 11 12:23:03 2024 ] 	Batch(7300/7879) done. Loss: 0.0086  lr:0.000001
[ Thu Jul 11 12:23:25 2024 ] 	Batch(7400/7879) done. Loss: 0.0671  lr:0.000001
[ Thu Jul 11 12:23:48 2024 ] 
Training: Epoch [31/120], Step [7499], Loss: 0.10269701480865479, Training Accuracy: 96.21499999999999
[ Thu Jul 11 12:23:48 2024 ] 	Batch(7500/7879) done. Loss: 0.0366  lr:0.000001
[ Thu Jul 11 12:24:11 2024 ] 	Batch(7600/7879) done. Loss: 0.1834  lr:0.000001
[ Thu Jul 11 12:24:33 2024 ] 	Batch(7700/7879) done. Loss: 0.1071  lr:0.000001
[ Thu Jul 11 12:24:56 2024 ] 	Batch(7800/7879) done. Loss: 0.1151  lr:0.000001
[ Thu Jul 11 12:25:13 2024 ] 	Mean training loss: 0.1422.
[ Thu Jul 11 12:25:13 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Thu Jul 11 12:25:13 2024 ] Training epoch: 33
[ Thu Jul 11 12:25:14 2024 ] 	Batch(0/7879) done. Loss: 0.1093  lr:0.000001
[ Thu Jul 11 12:25:36 2024 ] 	Batch(100/7879) done. Loss: 0.0285  lr:0.000001
[ Thu Jul 11 12:25:59 2024 ] 	Batch(200/7879) done. Loss: 0.0662  lr:0.000001
[ Thu Jul 11 12:26:22 2024 ] 	Batch(300/7879) done. Loss: 0.3135  lr:0.000001
[ Thu Jul 11 12:26:46 2024 ] 	Batch(400/7879) done. Loss: 0.0526  lr:0.000001
[ Thu Jul 11 12:27:09 2024 ] 
Training: Epoch [32/120], Step [499], Loss: 0.005492192693054676, Training Accuracy: 96.25
[ Thu Jul 11 12:27:09 2024 ] 	Batch(500/7879) done. Loss: 0.4164  lr:0.000001
[ Thu Jul 11 12:27:32 2024 ] 	Batch(600/7879) done. Loss: 0.0433  lr:0.000001
[ Thu Jul 11 12:27:55 2024 ] 	Batch(700/7879) done. Loss: 0.1803  lr:0.000001
[ Thu Jul 11 12:28:19 2024 ] 	Batch(800/7879) done. Loss: 0.7475  lr:0.000001
[ Thu Jul 11 12:28:42 2024 ] 	Batch(900/7879) done. Loss: 0.0912  lr:0.000001
[ Thu Jul 11 12:29:05 2024 ] 
Training: Epoch [32/120], Step [999], Loss: 0.15457171201705933, Training Accuracy: 96.1125
[ Thu Jul 11 12:29:05 2024 ] 	Batch(1000/7879) done. Loss: 0.3116  lr:0.000001
[ Thu Jul 11 12:29:29 2024 ] 	Batch(1100/7879) done. Loss: 0.0711  lr:0.000001
[ Thu Jul 11 12:29:52 2024 ] 	Batch(1200/7879) done. Loss: 0.0633  lr:0.000001
[ Thu Jul 11 12:30:16 2024 ] 	Batch(1300/7879) done. Loss: 0.0242  lr:0.000001
[ Thu Jul 11 12:30:39 2024 ] 	Batch(1400/7879) done. Loss: 0.0385  lr:0.000001
[ Thu Jul 11 12:31:01 2024 ] 
Training: Epoch [32/120], Step [1499], Loss: 0.35907089710235596, Training Accuracy: 96.3
[ Thu Jul 11 12:31:01 2024 ] 	Batch(1500/7879) done. Loss: 0.0798  lr:0.000001
[ Thu Jul 11 12:31:24 2024 ] 	Batch(1600/7879) done. Loss: 0.3937  lr:0.000001
[ Thu Jul 11 12:31:47 2024 ] 	Batch(1700/7879) done. Loss: 0.0247  lr:0.000001
[ Thu Jul 11 12:32:09 2024 ] 	Batch(1800/7879) done. Loss: 0.0596  lr:0.000001
[ Thu Jul 11 12:32:32 2024 ] 	Batch(1900/7879) done. Loss: 0.0559  lr:0.000001
[ Thu Jul 11 12:32:54 2024 ] 
Training: Epoch [32/120], Step [1999], Loss: 0.07399194687604904, Training Accuracy: 96.26875
[ Thu Jul 11 12:32:55 2024 ] 	Batch(2000/7879) done. Loss: 0.1041  lr:0.000001
[ Thu Jul 11 12:33:17 2024 ] 	Batch(2100/7879) done. Loss: 0.1997  lr:0.000001
[ Thu Jul 11 12:33:40 2024 ] 	Batch(2200/7879) done. Loss: 0.0029  lr:0.000001
[ Thu Jul 11 12:34:02 2024 ] 	Batch(2300/7879) done. Loss: 0.0220  lr:0.000001
[ Thu Jul 11 12:34:25 2024 ] 	Batch(2400/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul 11 12:34:48 2024 ] 
Training: Epoch [32/120], Step [2499], Loss: 0.10252591967582703, Training Accuracy: 96.21
[ Thu Jul 11 12:34:48 2024 ] 	Batch(2500/7879) done. Loss: 0.0015  lr:0.000001
[ Thu Jul 11 12:35:11 2024 ] 	Batch(2600/7879) done. Loss: 0.5344  lr:0.000001
[ Thu Jul 11 12:35:33 2024 ] 	Batch(2700/7879) done. Loss: 0.2140  lr:0.000001
[ Thu Jul 11 12:35:56 2024 ] 	Batch(2800/7879) done. Loss: 0.3980  lr:0.000001
[ Thu Jul 11 12:36:19 2024 ] 	Batch(2900/7879) done. Loss: 0.2640  lr:0.000001
[ Thu Jul 11 12:36:42 2024 ] 
Training: Epoch [32/120], Step [2999], Loss: 0.12564438581466675, Training Accuracy: 96.175
[ Thu Jul 11 12:36:42 2024 ] 	Batch(3000/7879) done. Loss: 0.3746  lr:0.000001
[ Thu Jul 11 12:37:06 2024 ] 	Batch(3100/7879) done. Loss: 0.0191  lr:0.000001
[ Thu Jul 11 12:37:29 2024 ] 	Batch(3200/7879) done. Loss: 0.0298  lr:0.000001
[ Thu Jul 11 12:37:53 2024 ] 	Batch(3300/7879) done. Loss: 0.0817  lr:0.000001
[ Thu Jul 11 12:38:16 2024 ] 	Batch(3400/7879) done. Loss: 0.0555  lr:0.000001
[ Thu Jul 11 12:38:39 2024 ] 
Training: Epoch [32/120], Step [3499], Loss: 0.15287503600120544, Training Accuracy: 96.14285714285714
[ Thu Jul 11 12:38:39 2024 ] 	Batch(3500/7879) done. Loss: 0.0429  lr:0.000001
[ Thu Jul 11 12:39:03 2024 ] 	Batch(3600/7879) done. Loss: 0.0493  lr:0.000001
[ Thu Jul 11 12:39:26 2024 ] 	Batch(3700/7879) done. Loss: 0.3488  lr:0.000001
[ Thu Jul 11 12:39:49 2024 ] 	Batch(3800/7879) done. Loss: 0.2940  lr:0.000001
[ Thu Jul 11 12:40:13 2024 ] 	Batch(3900/7879) done. Loss: 0.4041  lr:0.000001
[ Thu Jul 11 12:40:36 2024 ] 
Training: Epoch [32/120], Step [3999], Loss: 0.29821285605430603, Training Accuracy: 96.121875
[ Thu Jul 11 12:40:36 2024 ] 	Batch(4000/7879) done. Loss: 0.0934  lr:0.000001
[ Thu Jul 11 12:41:00 2024 ] 	Batch(4100/7879) done. Loss: 0.0343  lr:0.000001
[ Thu Jul 11 12:41:23 2024 ] 	Batch(4200/7879) done. Loss: 0.3500  lr:0.000001
[ Thu Jul 11 12:41:46 2024 ] 	Batch(4300/7879) done. Loss: 0.7624  lr:0.000001
[ Thu Jul 11 12:42:10 2024 ] 	Batch(4400/7879) done. Loss: 0.0288  lr:0.000001
[ Thu Jul 11 12:42:32 2024 ] 
Training: Epoch [32/120], Step [4499], Loss: 0.35332539677619934, Training Accuracy: 96.12777777777778
[ Thu Jul 11 12:42:32 2024 ] 	Batch(4500/7879) done. Loss: 0.2414  lr:0.000001
[ Thu Jul 11 12:42:55 2024 ] 	Batch(4600/7879) done. Loss: 0.0245  lr:0.000001
[ Thu Jul 11 12:43:17 2024 ] 	Batch(4700/7879) done. Loss: 0.0104  lr:0.000001
[ Thu Jul 11 12:43:40 2024 ] 	Batch(4800/7879) done. Loss: 0.0286  lr:0.000001
[ Thu Jul 11 12:44:03 2024 ] 	Batch(4900/7879) done. Loss: 0.0652  lr:0.000001
[ Thu Jul 11 12:44:25 2024 ] 
Training: Epoch [32/120], Step [4999], Loss: 0.8473618626594543, Training Accuracy: 96.125
[ Thu Jul 11 12:44:25 2024 ] 	Batch(5000/7879) done. Loss: 0.1525  lr:0.000001
[ Thu Jul 11 12:44:48 2024 ] 	Batch(5100/7879) done. Loss: 0.0096  lr:0.000001
[ Thu Jul 11 12:45:11 2024 ] 	Batch(5200/7879) done. Loss: 0.0528  lr:0.000001
[ Thu Jul 11 12:45:33 2024 ] 	Batch(5300/7879) done. Loss: 0.0865  lr:0.000001
[ Thu Jul 11 12:45:56 2024 ] 	Batch(5400/7879) done. Loss: 0.0398  lr:0.000001
[ Thu Jul 11 12:46:18 2024 ] 
Training: Epoch [32/120], Step [5499], Loss: 0.025310082361102104, Training Accuracy: 96.13181818181819
[ Thu Jul 11 12:46:18 2024 ] 	Batch(5500/7879) done. Loss: 0.1777  lr:0.000001
[ Thu Jul 11 12:46:41 2024 ] 	Batch(5600/7879) done. Loss: 0.0122  lr:0.000001
[ Thu Jul 11 12:47:03 2024 ] 	Batch(5700/7879) done. Loss: 0.0922  lr:0.000001
[ Thu Jul 11 12:47:26 2024 ] 	Batch(5800/7879) done. Loss: 0.2036  lr:0.000001
[ Thu Jul 11 12:47:49 2024 ] 	Batch(5900/7879) done. Loss: 0.2501  lr:0.000001
[ Thu Jul 11 12:48:11 2024 ] 
Training: Epoch [32/120], Step [5999], Loss: 0.10725469887256622, Training Accuracy: 96.16041666666668
[ Thu Jul 11 12:48:11 2024 ] 	Batch(6000/7879) done. Loss: 0.4578  lr:0.000001
[ Thu Jul 11 12:48:34 2024 ] 	Batch(6100/7879) done. Loss: 0.0690  lr:0.000001
[ Thu Jul 11 12:48:57 2024 ] 	Batch(6200/7879) done. Loss: 0.0211  lr:0.000001
[ Thu Jul 11 12:49:19 2024 ] 	Batch(6300/7879) done. Loss: 0.0009  lr:0.000001
[ Thu Jul 11 12:49:42 2024 ] 	Batch(6400/7879) done. Loss: 0.6385  lr:0.000001
[ Thu Jul 11 12:50:04 2024 ] 
Training: Epoch [32/120], Step [6499], Loss: 0.03435983881354332, Training Accuracy: 96.16153846153847
[ Thu Jul 11 12:50:04 2024 ] 	Batch(6500/7879) done. Loss: 0.9429  lr:0.000001
[ Thu Jul 11 12:50:27 2024 ] 	Batch(6600/7879) done. Loss: 0.0061  lr:0.000001
[ Thu Jul 11 12:50:50 2024 ] 	Batch(6700/7879) done. Loss: 0.0397  lr:0.000001
[ Thu Jul 11 12:51:12 2024 ] 	Batch(6800/7879) done. Loss: 0.0239  lr:0.000001
[ Thu Jul 11 12:51:35 2024 ] 	Batch(6900/7879) done. Loss: 0.0244  lr:0.000001
[ Thu Jul 11 12:51:57 2024 ] 
Training: Epoch [32/120], Step [6999], Loss: 0.04269284009933472, Training Accuracy: 96.12321428571428
[ Thu Jul 11 12:51:58 2024 ] 	Batch(7000/7879) done. Loss: 0.0929  lr:0.000001
[ Thu Jul 11 12:52:20 2024 ] 	Batch(7100/7879) done. Loss: 0.0581  lr:0.000001
[ Thu Jul 11 12:52:43 2024 ] 	Batch(7200/7879) done. Loss: 0.0814  lr:0.000001
[ Thu Jul 11 12:53:05 2024 ] 	Batch(7300/7879) done. Loss: 0.0845  lr:0.000001
[ Thu Jul 11 12:53:28 2024 ] 	Batch(7400/7879) done. Loss: 0.0284  lr:0.000001
[ Thu Jul 11 12:53:50 2024 ] 
Training: Epoch [32/120], Step [7499], Loss: 0.14135906100273132, Training Accuracy: 96.14166666666667
[ Thu Jul 11 12:53:51 2024 ] 	Batch(7500/7879) done. Loss: 0.0644  lr:0.000001
[ Thu Jul 11 12:54:13 2024 ] 	Batch(7600/7879) done. Loss: 0.6804  lr:0.000001
[ Thu Jul 11 12:54:36 2024 ] 	Batch(7700/7879) done. Loss: 0.1741  lr:0.000001
[ Thu Jul 11 12:54:58 2024 ] 	Batch(7800/7879) done. Loss: 0.2303  lr:0.000001
[ Thu Jul 11 12:55:16 2024 ] 	Mean training loss: 0.1434.
[ Thu Jul 11 12:55:16 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 12:55:16 2024 ] Training epoch: 34
[ Thu Jul 11 12:55:17 2024 ] 	Batch(0/7879) done. Loss: 0.1191  lr:0.000001
[ Thu Jul 11 12:55:39 2024 ] 	Batch(100/7879) done. Loss: 0.0643  lr:0.000001
[ Thu Jul 11 12:56:02 2024 ] 	Batch(200/7879) done. Loss: 0.0311  lr:0.000001
[ Thu Jul 11 12:56:25 2024 ] 	Batch(300/7879) done. Loss: 0.0717  lr:0.000001
[ Thu Jul 11 12:56:49 2024 ] 	Batch(400/7879) done. Loss: 0.0336  lr:0.000001
[ Thu Jul 11 12:57:12 2024 ] 
Training: Epoch [33/120], Step [499], Loss: 0.34563136100769043, Training Accuracy: 96.35000000000001
[ Thu Jul 11 12:57:12 2024 ] 	Batch(500/7879) done. Loss: 0.2335  lr:0.000001
[ Thu Jul 11 12:57:35 2024 ] 	Batch(600/7879) done. Loss: 0.2874  lr:0.000001
[ Thu Jul 11 12:57:58 2024 ] 	Batch(700/7879) done. Loss: 0.0882  lr:0.000001
[ Thu Jul 11 12:58:22 2024 ] 	Batch(800/7879) done. Loss: 0.0135  lr:0.000001
[ Thu Jul 11 12:58:45 2024 ] 	Batch(900/7879) done. Loss: 0.4819  lr:0.000001
[ Thu Jul 11 12:59:08 2024 ] 
Training: Epoch [33/120], Step [999], Loss: 0.03495624661445618, Training Accuracy: 96.075
[ Thu Jul 11 12:59:08 2024 ] 	Batch(1000/7879) done. Loss: 0.0166  lr:0.000001
[ Thu Jul 11 12:59:31 2024 ] 	Batch(1100/7879) done. Loss: 0.0880  lr:0.000001
[ Thu Jul 11 12:59:54 2024 ] 	Batch(1200/7879) done. Loss: 0.2901  lr:0.000001
[ Thu Jul 11 13:00:17 2024 ] 	Batch(1300/7879) done. Loss: 0.0976  lr:0.000001
[ Thu Jul 11 13:00:39 2024 ] 	Batch(1400/7879) done. Loss: 0.0841  lr:0.000001
[ Thu Jul 11 13:01:02 2024 ] 
Training: Epoch [33/120], Step [1499], Loss: 0.0021967487409710884, Training Accuracy: 96.14166666666667
[ Thu Jul 11 13:01:02 2024 ] 	Batch(1500/7879) done. Loss: 0.3933  lr:0.000001
[ Thu Jul 11 13:01:25 2024 ] 	Batch(1600/7879) done. Loss: 0.0656  lr:0.000001
[ Thu Jul 11 13:01:48 2024 ] 	Batch(1700/7879) done. Loss: 0.0154  lr:0.000001
[ Thu Jul 11 13:02:11 2024 ] 	Batch(1800/7879) done. Loss: 0.0285  lr:0.000001
[ Thu Jul 11 13:02:34 2024 ] 	Batch(1900/7879) done. Loss: 0.0169  lr:0.000001
[ Thu Jul 11 13:02:56 2024 ] 
Training: Epoch [33/120], Step [1999], Loss: 0.5614186525344849, Training Accuracy: 96.3
[ Thu Jul 11 13:02:57 2024 ] 	Batch(2000/7879) done. Loss: 0.1817  lr:0.000001
[ Thu Jul 11 13:03:20 2024 ] 	Batch(2100/7879) done. Loss: 0.1753  lr:0.000001
[ Thu Jul 11 13:03:43 2024 ] 	Batch(2200/7879) done. Loss: 0.0530  lr:0.000001
[ Thu Jul 11 13:04:06 2024 ] 	Batch(2300/7879) done. Loss: 0.1142  lr:0.000001
[ Thu Jul 11 13:04:29 2024 ] 	Batch(2400/7879) done. Loss: 0.0132  lr:0.000001
[ Thu Jul 11 13:04:53 2024 ] 
Training: Epoch [33/120], Step [2499], Loss: 0.038000766187906265, Training Accuracy: 96.255
[ Thu Jul 11 13:04:53 2024 ] 	Batch(2500/7879) done. Loss: 0.0298  lr:0.000001
[ Thu Jul 11 13:05:17 2024 ] 	Batch(2600/7879) done. Loss: 0.0158  lr:0.000001
[ Thu Jul 11 13:05:41 2024 ] 	Batch(2700/7879) done. Loss: 0.0844  lr:0.000001
[ Thu Jul 11 13:06:04 2024 ] 	Batch(2800/7879) done. Loss: 0.0758  lr:0.000001
[ Thu Jul 11 13:06:28 2024 ] 	Batch(2900/7879) done. Loss: 0.0140  lr:0.000001
[ Thu Jul 11 13:06:52 2024 ] 
Training: Epoch [33/120], Step [2999], Loss: 0.0063612619414925575, Training Accuracy: 96.3
[ Thu Jul 11 13:06:52 2024 ] 	Batch(3000/7879) done. Loss: 0.2073  lr:0.000001
[ Thu Jul 11 13:07:16 2024 ] 	Batch(3100/7879) done. Loss: 0.1068  lr:0.000001
[ Thu Jul 11 13:07:39 2024 ] 	Batch(3200/7879) done. Loss: 0.0076  lr:0.000001
[ Thu Jul 11 13:08:02 2024 ] 	Batch(3300/7879) done. Loss: 0.2019  lr:0.000001
[ Thu Jul 11 13:08:25 2024 ] 	Batch(3400/7879) done. Loss: 0.0967  lr:0.000001
[ Thu Jul 11 13:08:47 2024 ] 
Training: Epoch [33/120], Step [3499], Loss: 0.05997658520936966, Training Accuracy: 96.31785714285715
[ Thu Jul 11 13:08:48 2024 ] 	Batch(3500/7879) done. Loss: 0.9371  lr:0.000001
[ Thu Jul 11 13:09:11 2024 ] 	Batch(3600/7879) done. Loss: 0.0413  lr:0.000001
[ Thu Jul 11 13:09:33 2024 ] 	Batch(3700/7879) done. Loss: 0.0030  lr:0.000001
[ Thu Jul 11 13:09:56 2024 ] 	Batch(3800/7879) done. Loss: 0.2433  lr:0.000001
[ Thu Jul 11 13:10:18 2024 ] 	Batch(3900/7879) done. Loss: 0.0642  lr:0.000001
[ Thu Jul 11 13:10:41 2024 ] 
Training: Epoch [33/120], Step [3999], Loss: 0.03487798199057579, Training Accuracy: 96.309375
[ Thu Jul 11 13:10:41 2024 ] 	Batch(4000/7879) done. Loss: 0.1244  lr:0.000001
[ Thu Jul 11 13:11:04 2024 ] 	Batch(4100/7879) done. Loss: 0.0220  lr:0.000001
[ Thu Jul 11 13:11:26 2024 ] 	Batch(4200/7879) done. Loss: 0.3447  lr:0.000001
[ Thu Jul 11 13:11:49 2024 ] 	Batch(4300/7879) done. Loss: 0.2502  lr:0.000001
[ Thu Jul 11 13:12:12 2024 ] 	Batch(4400/7879) done. Loss: 0.3481  lr:0.000001
[ Thu Jul 11 13:12:34 2024 ] 
Training: Epoch [33/120], Step [4499], Loss: 0.015703778713941574, Training Accuracy: 96.30555555555556
[ Thu Jul 11 13:12:35 2024 ] 	Batch(4500/7879) done. Loss: 0.2629  lr:0.000001
[ Thu Jul 11 13:12:57 2024 ] 	Batch(4600/7879) done. Loss: 0.0172  lr:0.000001
[ Thu Jul 11 13:13:20 2024 ] 	Batch(4700/7879) done. Loss: 0.0090  lr:0.000001
[ Thu Jul 11 13:13:43 2024 ] 	Batch(4800/7879) done. Loss: 0.0059  lr:0.000001
[ Thu Jul 11 13:14:05 2024 ] 	Batch(4900/7879) done. Loss: 0.0704  lr:0.000001
[ Thu Jul 11 13:14:28 2024 ] 
Training: Epoch [33/120], Step [4999], Loss: 0.025750640779733658, Training Accuracy: 96.2975
[ Thu Jul 11 13:14:28 2024 ] 	Batch(5000/7879) done. Loss: 0.2944  lr:0.000001
[ Thu Jul 11 13:14:50 2024 ] 	Batch(5100/7879) done. Loss: 0.0388  lr:0.000001
[ Thu Jul 11 13:15:13 2024 ] 	Batch(5200/7879) done. Loss: 0.1087  lr:0.000001
[ Thu Jul 11 13:15:37 2024 ] 	Batch(5300/7879) done. Loss: 0.2183  lr:0.000001
[ Thu Jul 11 13:16:00 2024 ] 	Batch(5400/7879) done. Loss: 0.0177  lr:0.000001
[ Thu Jul 11 13:16:23 2024 ] 
Training: Epoch [33/120], Step [5499], Loss: 0.2132326364517212, Training Accuracy: 96.29772727272727
[ Thu Jul 11 13:16:23 2024 ] 	Batch(5500/7879) done. Loss: 0.3410  lr:0.000001
[ Thu Jul 11 13:16:46 2024 ] 	Batch(5600/7879) done. Loss: 0.0816  lr:0.000001
[ Thu Jul 11 13:17:08 2024 ] 	Batch(5700/7879) done. Loss: 0.1759  lr:0.000001
[ Thu Jul 11 13:17:31 2024 ] 	Batch(5800/7879) done. Loss: 0.0587  lr:0.000001
[ Thu Jul 11 13:17:53 2024 ] 	Batch(5900/7879) done. Loss: 0.3579  lr:0.000001
[ Thu Jul 11 13:18:16 2024 ] 
Training: Epoch [33/120], Step [5999], Loss: 0.01646696776151657, Training Accuracy: 96.26249999999999
[ Thu Jul 11 13:18:16 2024 ] 	Batch(6000/7879) done. Loss: 0.0617  lr:0.000001
[ Thu Jul 11 13:18:39 2024 ] 	Batch(6100/7879) done. Loss: 0.1827  lr:0.000001
[ Thu Jul 11 13:19:01 2024 ] 	Batch(6200/7879) done. Loss: 0.0345  lr:0.000001
[ Thu Jul 11 13:19:24 2024 ] 	Batch(6300/7879) done. Loss: 0.0426  lr:0.000001
[ Thu Jul 11 13:19:46 2024 ] 	Batch(6400/7879) done. Loss: 0.0723  lr:0.000001
[ Thu Jul 11 13:20:09 2024 ] 
Training: Epoch [33/120], Step [6499], Loss: 0.10645759105682373, Training Accuracy: 96.23461538461538
[ Thu Jul 11 13:20:09 2024 ] 	Batch(6500/7879) done. Loss: 0.0754  lr:0.000001
[ Thu Jul 11 13:20:31 2024 ] 	Batch(6600/7879) done. Loss: 0.0745  lr:0.000001
[ Thu Jul 11 13:20:54 2024 ] 	Batch(6700/7879) done. Loss: 0.0426  lr:0.000001
[ Thu Jul 11 13:21:17 2024 ] 	Batch(6800/7879) done. Loss: 0.0426  lr:0.000001
[ Thu Jul 11 13:21:40 2024 ] 	Batch(6900/7879) done. Loss: 0.0156  lr:0.000001
[ Thu Jul 11 13:22:02 2024 ] 
Training: Epoch [33/120], Step [6999], Loss: 0.06627225130796432, Training Accuracy: 96.21607142857142
[ Thu Jul 11 13:22:02 2024 ] 	Batch(7000/7879) done. Loss: 0.2860  lr:0.000001
[ Thu Jul 11 13:22:25 2024 ] 	Batch(7100/7879) done. Loss: 0.0489  lr:0.000001
[ Thu Jul 11 13:22:48 2024 ] 	Batch(7200/7879) done. Loss: 0.0847  lr:0.000001
[ Thu Jul 11 13:23:10 2024 ] 	Batch(7300/7879) done. Loss: 0.0379  lr:0.000001
[ Thu Jul 11 13:23:33 2024 ] 	Batch(7400/7879) done. Loss: 0.0744  lr:0.000001
[ Thu Jul 11 13:23:55 2024 ] 
Training: Epoch [33/120], Step [7499], Loss: 0.2674611508846283, Training Accuracy: 96.22166666666668
[ Thu Jul 11 13:23:55 2024 ] 	Batch(7500/7879) done. Loss: 0.0029  lr:0.000001
[ Thu Jul 11 13:24:18 2024 ] 	Batch(7600/7879) done. Loss: 0.0184  lr:0.000001
[ Thu Jul 11 13:24:41 2024 ] 	Batch(7700/7879) done. Loss: 0.0107  lr:0.000001
[ Thu Jul 11 13:25:03 2024 ] 	Batch(7800/7879) done. Loss: 0.0506  lr:0.000001
[ Thu Jul 11 13:25:21 2024 ] 	Mean training loss: 0.1407.
[ Thu Jul 11 13:25:21 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 13:25:21 2024 ] Training epoch: 35
[ Thu Jul 11 13:25:22 2024 ] 	Batch(0/7879) done. Loss: 0.0127  lr:0.000001
[ Thu Jul 11 13:25:44 2024 ] 	Batch(100/7879) done. Loss: 0.4585  lr:0.000001
[ Thu Jul 11 13:26:07 2024 ] 	Batch(200/7879) done. Loss: 0.0494  lr:0.000001
[ Thu Jul 11 13:26:29 2024 ] 	Batch(300/7879) done. Loss: 0.1112  lr:0.000001
[ Thu Jul 11 13:26:52 2024 ] 	Batch(400/7879) done. Loss: 0.1244  lr:0.000001
[ Thu Jul 11 13:27:14 2024 ] 
Training: Epoch [34/120], Step [499], Loss: 0.45351916551589966, Training Accuracy: 95.95
[ Thu Jul 11 13:27:14 2024 ] 	Batch(500/7879) done. Loss: 0.0499  lr:0.000001
[ Thu Jul 11 13:27:37 2024 ] 	Batch(600/7879) done. Loss: 0.3805  lr:0.000001
[ Thu Jul 11 13:28:00 2024 ] 	Batch(700/7879) done. Loss: 0.0715  lr:0.000001
[ Thu Jul 11 13:28:22 2024 ] 	Batch(800/7879) done. Loss: 0.0627  lr:0.000001
[ Thu Jul 11 13:28:45 2024 ] 	Batch(900/7879) done. Loss: 0.0391  lr:0.000001
[ Thu Jul 11 13:29:07 2024 ] 
Training: Epoch [34/120], Step [999], Loss: 0.0031079554464668036, Training Accuracy: 96.1
[ Thu Jul 11 13:29:07 2024 ] 	Batch(1000/7879) done. Loss: 0.0214  lr:0.000001
[ Thu Jul 11 13:29:30 2024 ] 	Batch(1100/7879) done. Loss: 0.0034  lr:0.000001
[ Thu Jul 11 13:29:53 2024 ] 	Batch(1200/7879) done. Loss: 0.0083  lr:0.000001
[ Thu Jul 11 13:30:15 2024 ] 	Batch(1300/7879) done. Loss: 0.0063  lr:0.000001
[ Thu Jul 11 13:30:38 2024 ] 	Batch(1400/7879) done. Loss: 0.0481  lr:0.000001
[ Thu Jul 11 13:31:00 2024 ] 
Training: Epoch [34/120], Step [1499], Loss: 0.07957043498754501, Training Accuracy: 96.275
[ Thu Jul 11 13:31:01 2024 ] 	Batch(1500/7879) done. Loss: 0.2238  lr:0.000001
[ Thu Jul 11 13:31:23 2024 ] 	Batch(1600/7879) done. Loss: 0.1060  lr:0.000001
[ Thu Jul 11 13:31:46 2024 ] 	Batch(1700/7879) done. Loss: 0.0148  lr:0.000001
[ Thu Jul 11 13:32:08 2024 ] 	Batch(1800/7879) done. Loss: 0.0266  lr:0.000001
[ Thu Jul 11 13:32:31 2024 ] 	Batch(1900/7879) done. Loss: 0.0993  lr:0.000001
[ Thu Jul 11 13:32:53 2024 ] 
Training: Epoch [34/120], Step [1999], Loss: 0.2841705083847046, Training Accuracy: 96.24374999999999
[ Thu Jul 11 13:32:54 2024 ] 	Batch(2000/7879) done. Loss: 0.4833  lr:0.000001
[ Thu Jul 11 13:33:17 2024 ] 	Batch(2100/7879) done. Loss: 0.2307  lr:0.000001
[ Thu Jul 11 13:33:40 2024 ] 	Batch(2200/7879) done. Loss: 0.3617  lr:0.000001
[ Thu Jul 11 13:34:04 2024 ] 	Batch(2300/7879) done. Loss: 0.1749  lr:0.000001
[ Thu Jul 11 13:34:27 2024 ] 	Batch(2400/7879) done. Loss: 0.1002  lr:0.000001
[ Thu Jul 11 13:34:49 2024 ] 
Training: Epoch [34/120], Step [2499], Loss: 0.053300194442272186, Training Accuracy: 96.33
[ Thu Jul 11 13:34:49 2024 ] 	Batch(2500/7879) done. Loss: 0.2206  lr:0.000001
[ Thu Jul 11 13:35:12 2024 ] 	Batch(2600/7879) done. Loss: 0.0725  lr:0.000001
[ Thu Jul 11 13:35:34 2024 ] 	Batch(2700/7879) done. Loss: 0.1103  lr:0.000001
[ Thu Jul 11 13:35:57 2024 ] 	Batch(2800/7879) done. Loss: 0.0933  lr:0.000001
[ Thu Jul 11 13:36:20 2024 ] 	Batch(2900/7879) done. Loss: 0.1040  lr:0.000001
[ Thu Jul 11 13:36:42 2024 ] 
Training: Epoch [34/120], Step [2999], Loss: 0.015103713609278202, Training Accuracy: 96.28333333333333
[ Thu Jul 11 13:36:42 2024 ] 	Batch(3000/7879) done. Loss: 0.0256  lr:0.000001
[ Thu Jul 11 13:37:05 2024 ] 	Batch(3100/7879) done. Loss: 0.0252  lr:0.000001
[ Thu Jul 11 13:37:28 2024 ] 	Batch(3200/7879) done. Loss: 0.1154  lr:0.000001
[ Thu Jul 11 13:37:51 2024 ] 	Batch(3300/7879) done. Loss: 0.0404  lr:0.000001
[ Thu Jul 11 13:38:14 2024 ] 	Batch(3400/7879) done. Loss: 0.8615  lr:0.000001
[ Thu Jul 11 13:38:37 2024 ] 
Training: Epoch [34/120], Step [3499], Loss: 0.02505679614841938, Training Accuracy: 96.23214285714286
[ Thu Jul 11 13:38:38 2024 ] 	Batch(3500/7879) done. Loss: 0.1235  lr:0.000001
[ Thu Jul 11 13:39:01 2024 ] 	Batch(3600/7879) done. Loss: 0.1204  lr:0.000001
[ Thu Jul 11 13:39:24 2024 ] 	Batch(3700/7879) done. Loss: 0.5502  lr:0.000001
[ Thu Jul 11 13:39:48 2024 ] 	Batch(3800/7879) done. Loss: 0.0501  lr:0.000001
[ Thu Jul 11 13:40:11 2024 ] 	Batch(3900/7879) done. Loss: 0.1327  lr:0.000001
[ Thu Jul 11 13:40:34 2024 ] 
Training: Epoch [34/120], Step [3999], Loss: 0.009934768080711365, Training Accuracy: 96.28750000000001
[ Thu Jul 11 13:40:35 2024 ] 	Batch(4000/7879) done. Loss: 0.0786  lr:0.000001
[ Thu Jul 11 13:40:57 2024 ] 	Batch(4100/7879) done. Loss: 0.0960  lr:0.000001
[ Thu Jul 11 13:41:20 2024 ] 	Batch(4200/7879) done. Loss: 0.1248  lr:0.000001
[ Thu Jul 11 13:41:42 2024 ] 	Batch(4300/7879) done. Loss: 0.0360  lr:0.000001
[ Thu Jul 11 13:42:05 2024 ] 	Batch(4400/7879) done. Loss: 0.0131  lr:0.000001
[ Thu Jul 11 13:42:27 2024 ] 
Training: Epoch [34/120], Step [4499], Loss: 0.042964786291122437, Training Accuracy: 96.25833333333334
[ Thu Jul 11 13:42:27 2024 ] 	Batch(4500/7879) done. Loss: 0.0942  lr:0.000001
[ Thu Jul 11 13:42:50 2024 ] 	Batch(4600/7879) done. Loss: 0.2644  lr:0.000001
[ Thu Jul 11 13:43:13 2024 ] 	Batch(4700/7879) done. Loss: 0.0445  lr:0.000001
[ Thu Jul 11 13:43:35 2024 ] 	Batch(4800/7879) done. Loss: 0.0707  lr:0.000001
[ Thu Jul 11 13:43:58 2024 ] 	Batch(4900/7879) done. Loss: 0.3472  lr:0.000001
[ Thu Jul 11 13:44:20 2024 ] 
Training: Epoch [34/120], Step [4999], Loss: 0.05347735434770584, Training Accuracy: 96.23
[ Thu Jul 11 13:44:20 2024 ] 	Batch(5000/7879) done. Loss: 0.1057  lr:0.000001
[ Thu Jul 11 13:44:43 2024 ] 	Batch(5100/7879) done. Loss: 0.0444  lr:0.000001
[ Thu Jul 11 13:45:06 2024 ] 	Batch(5200/7879) done. Loss: 0.1542  lr:0.000001
[ Thu Jul 11 13:45:29 2024 ] 	Batch(5300/7879) done. Loss: 0.0219  lr:0.000001
[ Thu Jul 11 13:45:52 2024 ] 	Batch(5400/7879) done. Loss: 0.3117  lr:0.000001
[ Thu Jul 11 13:46:15 2024 ] 
Training: Epoch [34/120], Step [5499], Loss: 0.06182269752025604, Training Accuracy: 96.2159090909091
[ Thu Jul 11 13:46:16 2024 ] 	Batch(5500/7879) done. Loss: 0.0265  lr:0.000001
[ Thu Jul 11 13:46:39 2024 ] 	Batch(5600/7879) done. Loss: 0.0244  lr:0.000001
[ Thu Jul 11 13:47:02 2024 ] 	Batch(5700/7879) done. Loss: 0.0145  lr:0.000001
[ Thu Jul 11 13:47:26 2024 ] 	Batch(5800/7879) done. Loss: 0.0044  lr:0.000001
[ Thu Jul 11 13:47:48 2024 ] 	Batch(5900/7879) done. Loss: 0.0508  lr:0.000001
[ Thu Jul 11 13:48:11 2024 ] 
Training: Epoch [34/120], Step [5999], Loss: 0.01030502188950777, Training Accuracy: 96.19375
[ Thu Jul 11 13:48:11 2024 ] 	Batch(6000/7879) done. Loss: 0.0393  lr:0.000001
[ Thu Jul 11 13:48:34 2024 ] 	Batch(6100/7879) done. Loss: 0.3547  lr:0.000001
[ Thu Jul 11 13:48:56 2024 ] 	Batch(6200/7879) done. Loss: 0.0383  lr:0.000001
[ Thu Jul 11 13:49:19 2024 ] 	Batch(6300/7879) done. Loss: 0.1786  lr:0.000001
[ Thu Jul 11 13:49:42 2024 ] 	Batch(6400/7879) done. Loss: 0.0950  lr:0.000001
[ Thu Jul 11 13:50:04 2024 ] 
Training: Epoch [34/120], Step [6499], Loss: 0.05763394013047218, Training Accuracy: 96.18461538461538
[ Thu Jul 11 13:50:04 2024 ] 	Batch(6500/7879) done. Loss: 0.1896  lr:0.000001
[ Thu Jul 11 13:50:27 2024 ] 	Batch(6600/7879) done. Loss: 0.0045  lr:0.000001
[ Thu Jul 11 13:50:49 2024 ] 	Batch(6700/7879) done. Loss: 0.7437  lr:0.000001
[ Thu Jul 11 13:51:12 2024 ] 	Batch(6800/7879) done. Loss: 0.3045  lr:0.000001
[ Thu Jul 11 13:51:35 2024 ] 	Batch(6900/7879) done. Loss: 0.1061  lr:0.000001
[ Thu Jul 11 13:51:57 2024 ] 
Training: Epoch [34/120], Step [6999], Loss: 0.31546348333358765, Training Accuracy: 96.23214285714286
[ Thu Jul 11 13:51:57 2024 ] 	Batch(7000/7879) done. Loss: 0.1638  lr:0.000001
[ Thu Jul 11 13:52:20 2024 ] 	Batch(7100/7879) done. Loss: 0.0382  lr:0.000001
[ Thu Jul 11 13:52:43 2024 ] 	Batch(7200/7879) done. Loss: 0.0020  lr:0.000001
[ Thu Jul 11 13:53:06 2024 ] 	Batch(7300/7879) done. Loss: 0.4640  lr:0.000001
[ Thu Jul 11 13:53:29 2024 ] 	Batch(7400/7879) done. Loss: 0.0347  lr:0.000001
[ Thu Jul 11 13:53:51 2024 ] 
Training: Epoch [34/120], Step [7499], Loss: 0.10450582951307297, Training Accuracy: 96.25333333333333
[ Thu Jul 11 13:53:51 2024 ] 	Batch(7500/7879) done. Loss: 0.0618  lr:0.000001
[ Thu Jul 11 13:54:14 2024 ] 	Batch(7600/7879) done. Loss: 0.0368  lr:0.000001
[ Thu Jul 11 13:54:36 2024 ] 	Batch(7700/7879) done. Loss: 0.0397  lr:0.000001
[ Thu Jul 11 13:54:59 2024 ] 	Batch(7800/7879) done. Loss: 0.1134  lr:0.000001
[ Thu Jul 11 13:55:17 2024 ] 	Mean training loss: 0.1393.
[ Thu Jul 11 13:55:17 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 13:55:17 2024 ] Training epoch: 36
[ Thu Jul 11 13:55:17 2024 ] 	Batch(0/7879) done. Loss: 0.3409  lr:0.000001
[ Thu Jul 11 13:55:40 2024 ] 	Batch(100/7879) done. Loss: 0.1692  lr:0.000001
[ Thu Jul 11 13:56:02 2024 ] 	Batch(200/7879) done. Loss: 0.0506  lr:0.000001
[ Thu Jul 11 13:56:25 2024 ] 	Batch(300/7879) done. Loss: 0.0206  lr:0.000001
[ Thu Jul 11 13:56:48 2024 ] 	Batch(400/7879) done. Loss: 0.6149  lr:0.000001
[ Thu Jul 11 13:57:10 2024 ] 
Training: Epoch [35/120], Step [499], Loss: 0.026416445150971413, Training Accuracy: 96.25
[ Thu Jul 11 13:57:10 2024 ] 	Batch(500/7879) done. Loss: 0.2887  lr:0.000001
[ Thu Jul 11 13:57:33 2024 ] 	Batch(600/7879) done. Loss: 0.0376  lr:0.000001
[ Thu Jul 11 13:57:55 2024 ] 	Batch(700/7879) done. Loss: 0.0759  lr:0.000001
[ Thu Jul 11 13:58:18 2024 ] 	Batch(800/7879) done. Loss: 0.1339  lr:0.000001
[ Thu Jul 11 13:58:41 2024 ] 	Batch(900/7879) done. Loss: 0.0377  lr:0.000001
[ Thu Jul 11 13:59:03 2024 ] 
Training: Epoch [35/120], Step [999], Loss: 0.17841100692749023, Training Accuracy: 96.1625
[ Thu Jul 11 13:59:03 2024 ] 	Batch(1000/7879) done. Loss: 0.0152  lr:0.000001
[ Thu Jul 11 13:59:26 2024 ] 	Batch(1100/7879) done. Loss: 0.0672  lr:0.000001
[ Thu Jul 11 13:59:49 2024 ] 	Batch(1200/7879) done. Loss: 0.2573  lr:0.000001
[ Thu Jul 11 14:00:11 2024 ] 	Batch(1300/7879) done. Loss: 0.2907  lr:0.000001
[ Thu Jul 11 14:00:34 2024 ] 	Batch(1400/7879) done. Loss: 0.1995  lr:0.000001
[ Thu Jul 11 14:00:56 2024 ] 
Training: Epoch [35/120], Step [1499], Loss: 0.04947669059038162, Training Accuracy: 96.26666666666667
[ Thu Jul 11 14:00:56 2024 ] 	Batch(1500/7879) done. Loss: 0.5010  lr:0.000001
[ Thu Jul 11 14:01:19 2024 ] 	Batch(1600/7879) done. Loss: 0.1419  lr:0.000001
[ Thu Jul 11 14:01:41 2024 ] 	Batch(1700/7879) done. Loss: 0.0160  lr:0.000001
[ Thu Jul 11 14:02:04 2024 ] 	Batch(1800/7879) done. Loss: 0.2084  lr:0.000001
[ Thu Jul 11 14:02:27 2024 ] 	Batch(1900/7879) done. Loss: 0.0539  lr:0.000001
[ Thu Jul 11 14:02:49 2024 ] 
Training: Epoch [35/120], Step [1999], Loss: 0.33565157651901245, Training Accuracy: 96.28750000000001
[ Thu Jul 11 14:02:49 2024 ] 	Batch(2000/7879) done. Loss: 0.0125  lr:0.000001
[ Thu Jul 11 14:03:12 2024 ] 	Batch(2100/7879) done. Loss: 0.1323  lr:0.000001
[ Thu Jul 11 14:03:35 2024 ] 	Batch(2200/7879) done. Loss: 0.0492  lr:0.000001
[ Thu Jul 11 14:03:57 2024 ] 	Batch(2300/7879) done. Loss: 0.0569  lr:0.000001
[ Thu Jul 11 14:04:20 2024 ] 	Batch(2400/7879) done. Loss: 0.4965  lr:0.000001
[ Thu Jul 11 14:04:42 2024 ] 
Training: Epoch [35/120], Step [2499], Loss: 0.21869689226150513, Training Accuracy: 96.235
[ Thu Jul 11 14:04:42 2024 ] 	Batch(2500/7879) done. Loss: 0.0043  lr:0.000001
[ Thu Jul 11 14:05:05 2024 ] 	Batch(2600/7879) done. Loss: 0.2654  lr:0.000001
[ Thu Jul 11 14:05:28 2024 ] 	Batch(2700/7879) done. Loss: 0.1867  lr:0.000001
[ Thu Jul 11 14:05:50 2024 ] 	Batch(2800/7879) done. Loss: 0.0227  lr:0.000001
[ Thu Jul 11 14:06:13 2024 ] 	Batch(2900/7879) done. Loss: 0.0129  lr:0.000001
[ Thu Jul 11 14:06:35 2024 ] 
Training: Epoch [35/120], Step [2999], Loss: 0.6083099246025085, Training Accuracy: 96.24166666666667
[ Thu Jul 11 14:06:36 2024 ] 	Batch(3000/7879) done. Loss: 0.1329  lr:0.000001
[ Thu Jul 11 14:06:58 2024 ] 	Batch(3100/7879) done. Loss: 0.6060  lr:0.000001
[ Thu Jul 11 14:07:21 2024 ] 	Batch(3200/7879) done. Loss: 0.0939  lr:0.000001
[ Thu Jul 11 14:07:43 2024 ] 	Batch(3300/7879) done. Loss: 0.1377  lr:0.000001
[ Thu Jul 11 14:08:06 2024 ] 	Batch(3400/7879) done. Loss: 0.1129  lr:0.000001
[ Thu Jul 11 14:08:28 2024 ] 
Training: Epoch [35/120], Step [3499], Loss: 0.04169239476323128, Training Accuracy: 96.31071428571428
[ Thu Jul 11 14:08:29 2024 ] 	Batch(3500/7879) done. Loss: 0.1932  lr:0.000001
[ Thu Jul 11 14:08:51 2024 ] 	Batch(3600/7879) done. Loss: 0.1667  lr:0.000001
[ Thu Jul 11 14:09:14 2024 ] 	Batch(3700/7879) done. Loss: 0.0189  lr:0.000001
[ Thu Jul 11 14:09:36 2024 ] 	Batch(3800/7879) done. Loss: 0.1366  lr:0.000001
[ Thu Jul 11 14:09:59 2024 ] 	Batch(3900/7879) done. Loss: 0.1387  lr:0.000001
[ Thu Jul 11 14:10:21 2024 ] 
Training: Epoch [35/120], Step [3999], Loss: 0.0270529817789793, Training Accuracy: 96.265625
[ Thu Jul 11 14:10:22 2024 ] 	Batch(4000/7879) done. Loss: 0.1821  lr:0.000001
[ Thu Jul 11 14:10:44 2024 ] 	Batch(4100/7879) done. Loss: 0.4174  lr:0.000001
[ Thu Jul 11 14:11:07 2024 ] 	Batch(4200/7879) done. Loss: 0.1314  lr:0.000001
[ Thu Jul 11 14:11:30 2024 ] 	Batch(4300/7879) done. Loss: 0.3193  lr:0.000001
[ Thu Jul 11 14:11:53 2024 ] 	Batch(4400/7879) done. Loss: 0.0123  lr:0.000001
[ Thu Jul 11 14:12:16 2024 ] 
Training: Epoch [35/120], Step [4499], Loss: 0.18228557705879211, Training Accuracy: 96.28888888888889
[ Thu Jul 11 14:12:16 2024 ] 	Batch(4500/7879) done. Loss: 0.0308  lr:0.000001
[ Thu Jul 11 14:12:39 2024 ] 	Batch(4600/7879) done. Loss: 0.4058  lr:0.000001
[ Thu Jul 11 14:13:02 2024 ] 	Batch(4700/7879) done. Loss: 0.1558  lr:0.000001
[ Thu Jul 11 14:13:25 2024 ] 	Batch(4800/7879) done. Loss: 0.0512  lr:0.000001
[ Thu Jul 11 14:13:47 2024 ] 	Batch(4900/7879) done. Loss: 0.6715  lr:0.000001
[ Thu Jul 11 14:14:10 2024 ] 
Training: Epoch [35/120], Step [4999], Loss: 0.018940435722470284, Training Accuracy: 96.2825
[ Thu Jul 11 14:14:10 2024 ] 	Batch(5000/7879) done. Loss: 0.0040  lr:0.000001
[ Thu Jul 11 14:14:33 2024 ] 	Batch(5100/7879) done. Loss: 0.0168  lr:0.000001
[ Thu Jul 11 14:14:55 2024 ] 	Batch(5200/7879) done. Loss: 0.0398  lr:0.000001
[ Thu Jul 11 14:15:18 2024 ] 	Batch(5300/7879) done. Loss: 0.0788  lr:0.000001
[ Thu Jul 11 14:15:40 2024 ] 	Batch(5400/7879) done. Loss: 0.0904  lr:0.000001
[ Thu Jul 11 14:16:03 2024 ] 
Training: Epoch [35/120], Step [5499], Loss: 0.1518569439649582, Training Accuracy: 96.37954545454545
[ Thu Jul 11 14:16:04 2024 ] 	Batch(5500/7879) done. Loss: 0.0880  lr:0.000001
[ Thu Jul 11 14:16:26 2024 ] 	Batch(5600/7879) done. Loss: 0.0971  lr:0.000001
[ Thu Jul 11 14:16:49 2024 ] 	Batch(5700/7879) done. Loss: 0.1654  lr:0.000001
[ Thu Jul 11 14:17:12 2024 ] 	Batch(5800/7879) done. Loss: 0.1713  lr:0.000001
[ Thu Jul 11 14:17:35 2024 ] 	Batch(5900/7879) done. Loss: 0.0172  lr:0.000001
[ Thu Jul 11 14:17:57 2024 ] 
Training: Epoch [35/120], Step [5999], Loss: 0.06996418535709381, Training Accuracy: 96.36875
[ Thu Jul 11 14:17:57 2024 ] 	Batch(6000/7879) done. Loss: 0.0592  lr:0.000001
[ Thu Jul 11 14:18:20 2024 ] 	Batch(6100/7879) done. Loss: 0.1054  lr:0.000001
[ Thu Jul 11 14:18:43 2024 ] 	Batch(6200/7879) done. Loss: 0.0943  lr:0.000001
[ Thu Jul 11 14:19:05 2024 ] 	Batch(6300/7879) done. Loss: 0.2155  lr:0.000001
[ Thu Jul 11 14:19:28 2024 ] 	Batch(6400/7879) done. Loss: 0.0176  lr:0.000001
[ Thu Jul 11 14:19:51 2024 ] 
Training: Epoch [35/120], Step [6499], Loss: 0.13790439069271088, Training Accuracy: 96.38846153846153
[ Thu Jul 11 14:19:51 2024 ] 	Batch(6500/7879) done. Loss: 0.1145  lr:0.000001
[ Thu Jul 11 14:20:14 2024 ] 	Batch(6600/7879) done. Loss: 0.4031  lr:0.000001
[ Thu Jul 11 14:20:36 2024 ] 	Batch(6700/7879) done. Loss: 0.2085  lr:0.000001
[ Thu Jul 11 14:20:59 2024 ] 	Batch(6800/7879) done. Loss: 0.2186  lr:0.000001
[ Thu Jul 11 14:21:22 2024 ] 	Batch(6900/7879) done. Loss: 0.3919  lr:0.000001
[ Thu Jul 11 14:21:44 2024 ] 
Training: Epoch [35/120], Step [6999], Loss: 0.5102523565292358, Training Accuracy: 96.38571428571429
[ Thu Jul 11 14:21:44 2024 ] 	Batch(7000/7879) done. Loss: 0.1458  lr:0.000001
[ Thu Jul 11 14:22:07 2024 ] 	Batch(7100/7879) done. Loss: 0.0017  lr:0.000001
[ Thu Jul 11 14:22:30 2024 ] 	Batch(7200/7879) done. Loss: 0.0582  lr:0.000001
[ Thu Jul 11 14:22:53 2024 ] 	Batch(7300/7879) done. Loss: 0.2553  lr:0.000001
[ Thu Jul 11 14:23:15 2024 ] 	Batch(7400/7879) done. Loss: 0.0153  lr:0.000001
[ Thu Jul 11 14:23:38 2024 ] 
Training: Epoch [35/120], Step [7499], Loss: 0.026919014751911163, Training Accuracy: 96.37333333333333
[ Thu Jul 11 14:23:38 2024 ] 	Batch(7500/7879) done. Loss: 0.0390  lr:0.000001
[ Thu Jul 11 14:24:01 2024 ] 	Batch(7600/7879) done. Loss: 0.2571  lr:0.000001
[ Thu Jul 11 14:24:23 2024 ] 	Batch(7700/7879) done. Loss: 0.0300  lr:0.000001
[ Thu Jul 11 14:24:46 2024 ] 	Batch(7800/7879) done. Loss: 0.1097  lr:0.000001
[ Thu Jul 11 14:25:03 2024 ] 	Mean training loss: 0.1409.
[ Thu Jul 11 14:25:03 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 14:25:04 2024 ] Training epoch: 37
[ Thu Jul 11 14:25:04 2024 ] 	Batch(0/7879) done. Loss: 0.3512  lr:0.000001
[ Thu Jul 11 14:25:27 2024 ] 	Batch(100/7879) done. Loss: 0.0109  lr:0.000001
[ Thu Jul 11 14:25:50 2024 ] 	Batch(200/7879) done. Loss: 0.2196  lr:0.000001
[ Thu Jul 11 14:26:13 2024 ] 	Batch(300/7879) done. Loss: 0.0877  lr:0.000001
[ Thu Jul 11 14:26:36 2024 ] 	Batch(400/7879) done. Loss: 0.1229  lr:0.000001
[ Thu Jul 11 14:26:59 2024 ] 
Training: Epoch [36/120], Step [499], Loss: 0.06465398520231247, Training Accuracy: 96.45
[ Thu Jul 11 14:26:59 2024 ] 	Batch(500/7879) done. Loss: 0.1166  lr:0.000001
[ Thu Jul 11 14:27:22 2024 ] 	Batch(600/7879) done. Loss: 0.1787  lr:0.000001
[ Thu Jul 11 14:27:45 2024 ] 	Batch(700/7879) done. Loss: 0.0193  lr:0.000001
[ Thu Jul 11 14:28:08 2024 ] 	Batch(800/7879) done. Loss: 0.3713  lr:0.000001
[ Thu Jul 11 14:28:31 2024 ] 	Batch(900/7879) done. Loss: 0.1532  lr:0.000001
[ Thu Jul 11 14:28:53 2024 ] 
Training: Epoch [36/120], Step [999], Loss: 0.01070233155041933, Training Accuracy: 96.22500000000001
[ Thu Jul 11 14:28:54 2024 ] 	Batch(1000/7879) done. Loss: 0.0560  lr:0.000001
[ Thu Jul 11 14:29:16 2024 ] 	Batch(1100/7879) done. Loss: 0.1563  lr:0.000001
[ Thu Jul 11 14:29:39 2024 ] 	Batch(1200/7879) done. Loss: 0.0423  lr:0.000001
[ Thu Jul 11 14:30:02 2024 ] 	Batch(1300/7879) done. Loss: 0.2676  lr:0.000001
[ Thu Jul 11 14:30:25 2024 ] 	Batch(1400/7879) done. Loss: 0.1263  lr:0.000001
[ Thu Jul 11 14:30:47 2024 ] 
Training: Epoch [36/120], Step [1499], Loss: 0.09223268926143646, Training Accuracy: 96.125
[ Thu Jul 11 14:30:47 2024 ] 	Batch(1500/7879) done. Loss: 0.0762  lr:0.000001
[ Thu Jul 11 14:31:10 2024 ] 	Batch(1600/7879) done. Loss: 0.0539  lr:0.000001
[ Thu Jul 11 14:31:33 2024 ] 	Batch(1700/7879) done. Loss: 0.1056  lr:0.000001
[ Thu Jul 11 14:31:56 2024 ] 	Batch(1800/7879) done. Loss: 0.0064  lr:0.000001
[ Thu Jul 11 14:32:19 2024 ] 	Batch(1900/7879) done. Loss: 0.1055  lr:0.000001
[ Thu Jul 11 14:32:41 2024 ] 
Training: Epoch [36/120], Step [1999], Loss: 0.1104951947927475, Training Accuracy: 96.1375
[ Thu Jul 11 14:32:41 2024 ] 	Batch(2000/7879) done. Loss: 0.0542  lr:0.000001
[ Thu Jul 11 14:33:04 2024 ] 	Batch(2100/7879) done. Loss: 0.0386  lr:0.000001
[ Thu Jul 11 14:33:27 2024 ] 	Batch(2200/7879) done. Loss: 0.0365  lr:0.000001
[ Thu Jul 11 14:33:50 2024 ] 	Batch(2300/7879) done. Loss: 0.4467  lr:0.000001
[ Thu Jul 11 14:34:13 2024 ] 	Batch(2400/7879) done. Loss: 0.0588  lr:0.000001
[ Thu Jul 11 14:34:35 2024 ] 
Training: Epoch [36/120], Step [2499], Loss: 0.30721792578697205, Training Accuracy: 96.16
[ Thu Jul 11 14:34:35 2024 ] 	Batch(2500/7879) done. Loss: 0.0084  lr:0.000001
[ Thu Jul 11 14:34:58 2024 ] 	Batch(2600/7879) done. Loss: 0.3583  lr:0.000001
[ Thu Jul 11 14:35:21 2024 ] 	Batch(2700/7879) done. Loss: 0.0297  lr:0.000001
[ Thu Jul 11 14:35:44 2024 ] 	Batch(2800/7879) done. Loss: 0.0106  lr:0.000001
[ Thu Jul 11 14:36:06 2024 ] 	Batch(2900/7879) done. Loss: 0.0247  lr:0.000001
[ Thu Jul 11 14:36:29 2024 ] 
Training: Epoch [36/120], Step [2999], Loss: 0.26924678683280945, Training Accuracy: 96.22500000000001
[ Thu Jul 11 14:36:29 2024 ] 	Batch(3000/7879) done. Loss: 0.0327  lr:0.000001
[ Thu Jul 11 14:36:52 2024 ] 	Batch(3100/7879) done. Loss: 0.2545  lr:0.000001
[ Thu Jul 11 14:37:15 2024 ] 	Batch(3200/7879) done. Loss: 0.1273  lr:0.000001
[ Thu Jul 11 14:37:37 2024 ] 	Batch(3300/7879) done. Loss: 0.1941  lr:0.000001
[ Thu Jul 11 14:38:00 2024 ] 	Batch(3400/7879) done. Loss: 0.0345  lr:0.000001
[ Thu Jul 11 14:38:23 2024 ] 
Training: Epoch [36/120], Step [3499], Loss: 0.12627682089805603, Training Accuracy: 96.2
[ Thu Jul 11 14:38:23 2024 ] 	Batch(3500/7879) done. Loss: 0.1025  lr:0.000001
[ Thu Jul 11 14:38:46 2024 ] 	Batch(3600/7879) done. Loss: 1.1219  lr:0.000001
[ Thu Jul 11 14:39:09 2024 ] 	Batch(3700/7879) done. Loss: 0.2340  lr:0.000001
[ Thu Jul 11 14:39:31 2024 ] 	Batch(3800/7879) done. Loss: 0.1930  lr:0.000001
[ Thu Jul 11 14:39:54 2024 ] 	Batch(3900/7879) done. Loss: 0.0314  lr:0.000001
[ Thu Jul 11 14:40:16 2024 ] 
Training: Epoch [36/120], Step [3999], Loss: 0.03164443373680115, Training Accuracy: 96.215625
[ Thu Jul 11 14:40:17 2024 ] 	Batch(4000/7879) done. Loss: 0.0086  lr:0.000001
[ Thu Jul 11 14:40:40 2024 ] 	Batch(4100/7879) done. Loss: 0.1392  lr:0.000001
[ Thu Jul 11 14:41:03 2024 ] 	Batch(4200/7879) done. Loss: 0.1895  lr:0.000001
[ Thu Jul 11 14:41:25 2024 ] 	Batch(4300/7879) done. Loss: 0.0927  lr:0.000001
[ Thu Jul 11 14:41:48 2024 ] 	Batch(4400/7879) done. Loss: 0.1918  lr:0.000001
[ Thu Jul 11 14:42:11 2024 ] 
Training: Epoch [36/120], Step [4499], Loss: 0.019279399886727333, Training Accuracy: 96.31111111111112
[ Thu Jul 11 14:42:11 2024 ] 	Batch(4500/7879) done. Loss: 0.1107  lr:0.000001
[ Thu Jul 11 14:42:34 2024 ] 	Batch(4600/7879) done. Loss: 0.0257  lr:0.000001
[ Thu Jul 11 14:42:56 2024 ] 	Batch(4700/7879) done. Loss: 0.1402  lr:0.000001
[ Thu Jul 11 14:43:20 2024 ] 	Batch(4800/7879) done. Loss: 0.0689  lr:0.000001
[ Thu Jul 11 14:43:43 2024 ] 	Batch(4900/7879) done. Loss: 0.2581  lr:0.000001
[ Thu Jul 11 14:44:06 2024 ] 
Training: Epoch [36/120], Step [4999], Loss: 0.050713419914245605, Training Accuracy: 96.265
[ Thu Jul 11 14:44:06 2024 ] 	Batch(5000/7879) done. Loss: 0.2203  lr:0.000001
[ Thu Jul 11 14:44:29 2024 ] 	Batch(5100/7879) done. Loss: 0.0577  lr:0.000001
[ Thu Jul 11 14:44:53 2024 ] 	Batch(5200/7879) done. Loss: 0.0063  lr:0.000001
[ Thu Jul 11 14:45:16 2024 ] 	Batch(5300/7879) done. Loss: 0.0091  lr:0.000001
[ Thu Jul 11 14:45:39 2024 ] 	Batch(5400/7879) done. Loss: 0.0077  lr:0.000001
[ Thu Jul 11 14:46:02 2024 ] 
Training: Epoch [36/120], Step [5499], Loss: 0.10346479713916779, Training Accuracy: 96.26363636363637
[ Thu Jul 11 14:46:02 2024 ] 	Batch(5500/7879) done. Loss: 0.0723  lr:0.000001
[ Thu Jul 11 14:46:26 2024 ] 	Batch(5600/7879) done. Loss: 0.0201  lr:0.000001
[ Thu Jul 11 14:46:49 2024 ] 	Batch(5700/7879) done. Loss: 0.0266  lr:0.000001
[ Thu Jul 11 14:47:12 2024 ] 	Batch(5800/7879) done. Loss: 0.1627  lr:0.000001
[ Thu Jul 11 14:47:35 2024 ] 	Batch(5900/7879) done. Loss: 0.0397  lr:0.000001
[ Thu Jul 11 14:47:58 2024 ] 
Training: Epoch [36/120], Step [5999], Loss: 0.554288387298584, Training Accuracy: 96.24583333333334
[ Thu Jul 11 14:47:58 2024 ] 	Batch(6000/7879) done. Loss: 0.0222  lr:0.000001
[ Thu Jul 11 14:48:21 2024 ] 	Batch(6100/7879) done. Loss: 0.3170  lr:0.000001
[ Thu Jul 11 14:48:44 2024 ] 	Batch(6200/7879) done. Loss: 0.0158  lr:0.000001
[ Thu Jul 11 14:49:07 2024 ] 	Batch(6300/7879) done. Loss: 0.1550  lr:0.000001
[ Thu Jul 11 14:49:30 2024 ] 	Batch(6400/7879) done. Loss: 0.0533  lr:0.000001
[ Thu Jul 11 14:49:53 2024 ] 
Training: Epoch [36/120], Step [6499], Loss: 0.2202792912721634, Training Accuracy: 96.25769230769231
[ Thu Jul 11 14:49:53 2024 ] 	Batch(6500/7879) done. Loss: 0.1468  lr:0.000001
[ Thu Jul 11 14:50:16 2024 ] 	Batch(6600/7879) done. Loss: 0.0821  lr:0.000001
[ Thu Jul 11 14:50:39 2024 ] 	Batch(6700/7879) done. Loss: 0.3303  lr:0.000001
[ Thu Jul 11 14:51:02 2024 ] 	Batch(6800/7879) done. Loss: 0.1432  lr:0.000001
[ Thu Jul 11 14:51:24 2024 ] 	Batch(6900/7879) done. Loss: 0.3633  lr:0.000001
[ Thu Jul 11 14:51:46 2024 ] 
Training: Epoch [36/120], Step [6999], Loss: 0.16632311046123505, Training Accuracy: 96.26785714285714
[ Thu Jul 11 14:51:47 2024 ] 	Batch(7000/7879) done. Loss: 0.0198  lr:0.000001
[ Thu Jul 11 14:52:09 2024 ] 	Batch(7100/7879) done. Loss: 0.1045  lr:0.000001
[ Thu Jul 11 14:52:32 2024 ] 	Batch(7200/7879) done. Loss: 0.2441  lr:0.000001
[ Thu Jul 11 14:52:56 2024 ] 	Batch(7300/7879) done. Loss: 0.0674  lr:0.000001
[ Thu Jul 11 14:53:19 2024 ] 	Batch(7400/7879) done. Loss: 0.0520  lr:0.000001
[ Thu Jul 11 14:53:42 2024 ] 
Training: Epoch [36/120], Step [7499], Loss: 0.2936398386955261, Training Accuracy: 96.25333333333333
[ Thu Jul 11 14:53:42 2024 ] 	Batch(7500/7879) done. Loss: 0.0031  lr:0.000001
[ Thu Jul 11 14:54:06 2024 ] 	Batch(7600/7879) done. Loss: 0.0978  lr:0.000001
[ Thu Jul 11 14:54:29 2024 ] 	Batch(7700/7879) done. Loss: 0.1282  lr:0.000001
[ Thu Jul 11 14:54:52 2024 ] 	Batch(7800/7879) done. Loss: 0.0935  lr:0.000001
[ Thu Jul 11 14:55:11 2024 ] 	Mean training loss: 0.1391.
[ Thu Jul 11 14:55:11 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 14:55:11 2024 ] Training epoch: 38
[ Thu Jul 11 14:55:11 2024 ] 	Batch(0/7879) done. Loss: 0.1481  lr:0.000001
[ Thu Jul 11 14:55:34 2024 ] 	Batch(100/7879) done. Loss: 0.0425  lr:0.000001
[ Thu Jul 11 14:55:57 2024 ] 	Batch(200/7879) done. Loss: 0.0295  lr:0.000001
[ Thu Jul 11 14:56:19 2024 ] 	Batch(300/7879) done. Loss: 0.0943  lr:0.000001
[ Thu Jul 11 14:56:42 2024 ] 	Batch(400/7879) done. Loss: 0.3374  lr:0.000001
[ Thu Jul 11 14:57:04 2024 ] 
Training: Epoch [37/120], Step [499], Loss: 0.07704130560159683, Training Accuracy: 96.375
[ Thu Jul 11 14:57:04 2024 ] 	Batch(500/7879) done. Loss: 0.0092  lr:0.000001
[ Thu Jul 11 14:57:27 2024 ] 	Batch(600/7879) done. Loss: 0.1117  lr:0.000001
[ Thu Jul 11 14:57:50 2024 ] 	Batch(700/7879) done. Loss: 0.0010  lr:0.000001
[ Thu Jul 11 14:58:12 2024 ] 	Batch(800/7879) done. Loss: 0.0334  lr:0.000001
[ Thu Jul 11 14:58:36 2024 ] 	Batch(900/7879) done. Loss: 0.0062  lr:0.000001
[ Thu Jul 11 14:58:59 2024 ] 
Training: Epoch [37/120], Step [999], Loss: 0.10250405222177505, Training Accuracy: 96.25
[ Thu Jul 11 14:58:59 2024 ] 	Batch(1000/7879) done. Loss: 0.3123  lr:0.000001
[ Thu Jul 11 14:59:22 2024 ] 	Batch(1100/7879) done. Loss: 0.0124  lr:0.000001
[ Thu Jul 11 14:59:46 2024 ] 	Batch(1200/7879) done. Loss: 0.1507  lr:0.000001
[ Thu Jul 11 15:00:09 2024 ] 	Batch(1300/7879) done. Loss: 0.1313  lr:0.000001
[ Thu Jul 11 15:00:32 2024 ] 	Batch(1400/7879) done. Loss: 0.2056  lr:0.000001
[ Thu Jul 11 15:00:54 2024 ] 
Training: Epoch [37/120], Step [1499], Loss: 0.12113258987665176, Training Accuracy: 96.28333333333333
[ Thu Jul 11 15:00:55 2024 ] 	Batch(1500/7879) done. Loss: 0.1813  lr:0.000001
[ Thu Jul 11 15:01:17 2024 ] 	Batch(1600/7879) done. Loss: 0.1236  lr:0.000001
[ Thu Jul 11 15:01:40 2024 ] 	Batch(1700/7879) done. Loss: 0.0581  lr:0.000001
[ Thu Jul 11 15:02:02 2024 ] 	Batch(1800/7879) done. Loss: 0.0293  lr:0.000001
[ Thu Jul 11 15:02:25 2024 ] 	Batch(1900/7879) done. Loss: 0.1710  lr:0.000001
[ Thu Jul 11 15:02:47 2024 ] 
Training: Epoch [37/120], Step [1999], Loss: 0.271578848361969, Training Accuracy: 96.42500000000001
[ Thu Jul 11 15:02:48 2024 ] 	Batch(2000/7879) done. Loss: 0.1635  lr:0.000001
[ Thu Jul 11 15:03:10 2024 ] 	Batch(2100/7879) done. Loss: 0.1913  lr:0.000001
[ Thu Jul 11 15:03:33 2024 ] 	Batch(2200/7879) done. Loss: 0.0264  lr:0.000001
[ Thu Jul 11 15:03:55 2024 ] 	Batch(2300/7879) done. Loss: 0.0739  lr:0.000001
[ Thu Jul 11 15:04:18 2024 ] 	Batch(2400/7879) done. Loss: 0.0331  lr:0.000001
[ Thu Jul 11 15:04:41 2024 ] 
Training: Epoch [37/120], Step [2499], Loss: 0.08657918870449066, Training Accuracy: 96.44500000000001
[ Thu Jul 11 15:04:41 2024 ] 	Batch(2500/7879) done. Loss: 0.1290  lr:0.000001
[ Thu Jul 11 15:05:03 2024 ] 	Batch(2600/7879) done. Loss: 0.0592  lr:0.000001
[ Thu Jul 11 15:05:26 2024 ] 	Batch(2700/7879) done. Loss: 0.1284  lr:0.000001
[ Thu Jul 11 15:05:49 2024 ] 	Batch(2800/7879) done. Loss: 0.1138  lr:0.000001
[ Thu Jul 11 15:06:12 2024 ] 	Batch(2900/7879) done. Loss: 0.0060  lr:0.000001
[ Thu Jul 11 15:06:34 2024 ] 
Training: Epoch [37/120], Step [2999], Loss: 0.1092667430639267, Training Accuracy: 96.44583333333333
[ Thu Jul 11 15:06:34 2024 ] 	Batch(3000/7879) done. Loss: 0.0698  lr:0.000001
[ Thu Jul 11 15:06:57 2024 ] 	Batch(3100/7879) done. Loss: 0.5435  lr:0.000001
[ Thu Jul 11 15:07:19 2024 ] 	Batch(3200/7879) done. Loss: 0.0169  lr:0.000001
[ Thu Jul 11 15:07:42 2024 ] 	Batch(3300/7879) done. Loss: 0.0573  lr:0.000001
[ Thu Jul 11 15:08:05 2024 ] 	Batch(3400/7879) done. Loss: 0.5599  lr:0.000001
[ Thu Jul 11 15:08:27 2024 ] 
Training: Epoch [37/120], Step [3499], Loss: 0.04902862012386322, Training Accuracy: 96.42500000000001
[ Thu Jul 11 15:08:27 2024 ] 	Batch(3500/7879) done. Loss: 0.0669  lr:0.000001
[ Thu Jul 11 15:08:50 2024 ] 	Batch(3600/7879) done. Loss: 0.0171  lr:0.000001
[ Thu Jul 11 15:09:12 2024 ] 	Batch(3700/7879) done. Loss: 0.0109  lr:0.000001
[ Thu Jul 11 15:09:35 2024 ] 	Batch(3800/7879) done. Loss: 0.0862  lr:0.000001
[ Thu Jul 11 15:09:58 2024 ] 	Batch(3900/7879) done. Loss: 0.0420  lr:0.000001
[ Thu Jul 11 15:10:20 2024 ] 
Training: Epoch [37/120], Step [3999], Loss: 0.022683996707201004, Training Accuracy: 96.434375
[ Thu Jul 11 15:10:20 2024 ] 	Batch(4000/7879) done. Loss: 0.0098  lr:0.000001
[ Thu Jul 11 15:10:43 2024 ] 	Batch(4100/7879) done. Loss: 0.0289  lr:0.000001
[ Thu Jul 11 15:11:06 2024 ] 	Batch(4200/7879) done. Loss: 0.4799  lr:0.000001
[ Thu Jul 11 15:11:29 2024 ] 	Batch(4300/7879) done. Loss: 0.0163  lr:0.000001
[ Thu Jul 11 15:11:53 2024 ] 	Batch(4400/7879) done. Loss: 0.0606  lr:0.000001
[ Thu Jul 11 15:12:15 2024 ] 
Training: Epoch [37/120], Step [4499], Loss: 0.09796610474586487, Training Accuracy: 96.42500000000001
[ Thu Jul 11 15:12:15 2024 ] 	Batch(4500/7879) done. Loss: 0.0971  lr:0.000001
[ Thu Jul 11 15:12:38 2024 ] 	Batch(4600/7879) done. Loss: 0.0034  lr:0.000001
[ Thu Jul 11 15:13:00 2024 ] 	Batch(4700/7879) done. Loss: 0.1098  lr:0.000001
[ Thu Jul 11 15:13:23 2024 ] 	Batch(4800/7879) done. Loss: 0.0648  lr:0.000001
[ Thu Jul 11 15:13:46 2024 ] 	Batch(4900/7879) done. Loss: 0.3111  lr:0.000001
[ Thu Jul 11 15:14:08 2024 ] 
Training: Epoch [37/120], Step [4999], Loss: 0.031018711626529694, Training Accuracy: 96.39750000000001
[ Thu Jul 11 15:14:09 2024 ] 	Batch(5000/7879) done. Loss: 0.0356  lr:0.000001
[ Thu Jul 11 15:14:31 2024 ] 	Batch(5100/7879) done. Loss: 0.0866  lr:0.000001
[ Thu Jul 11 15:14:54 2024 ] 	Batch(5200/7879) done. Loss: 0.2215  lr:0.000001
[ Thu Jul 11 15:15:16 2024 ] 	Batch(5300/7879) done. Loss: 0.3100  lr:0.000001
[ Thu Jul 11 15:15:39 2024 ] 	Batch(5400/7879) done. Loss: 0.0809  lr:0.000001
[ Thu Jul 11 15:16:01 2024 ] 
Training: Epoch [37/120], Step [5499], Loss: 0.07786327600479126, Training Accuracy: 96.45
[ Thu Jul 11 15:16:02 2024 ] 	Batch(5500/7879) done. Loss: 0.0148  lr:0.000001
[ Thu Jul 11 15:16:24 2024 ] 	Batch(5600/7879) done. Loss: 0.0810  lr:0.000001
[ Thu Jul 11 15:16:47 2024 ] 	Batch(5700/7879) done. Loss: 0.0785  lr:0.000001
[ Thu Jul 11 15:17:10 2024 ] 	Batch(5800/7879) done. Loss: 0.1698  lr:0.000001
[ Thu Jul 11 15:17:32 2024 ] 	Batch(5900/7879) done. Loss: 0.0713  lr:0.000001
[ Thu Jul 11 15:17:54 2024 ] 
Training: Epoch [37/120], Step [5999], Loss: 0.02908664010465145, Training Accuracy: 96.47291666666666
[ Thu Jul 11 15:17:55 2024 ] 	Batch(6000/7879) done. Loss: 0.0708  lr:0.000001
[ Thu Jul 11 15:18:17 2024 ] 	Batch(6100/7879) done. Loss: 0.0764  lr:0.000001
[ Thu Jul 11 15:18:40 2024 ] 	Batch(6200/7879) done. Loss: 0.2506  lr:0.000001
[ Thu Jul 11 15:19:03 2024 ] 	Batch(6300/7879) done. Loss: 0.3008  lr:0.000001
[ Thu Jul 11 15:19:25 2024 ] 	Batch(6400/7879) done. Loss: 0.1900  lr:0.000001
[ Thu Jul 11 15:19:48 2024 ] 
Training: Epoch [37/120], Step [6499], Loss: 0.11254110187292099, Training Accuracy: 96.44423076923077
[ Thu Jul 11 15:19:48 2024 ] 	Batch(6500/7879) done. Loss: 0.4758  lr:0.000001
[ Thu Jul 11 15:20:10 2024 ] 	Batch(6600/7879) done. Loss: 0.0273  lr:0.000001
[ Thu Jul 11 15:20:33 2024 ] 	Batch(6700/7879) done. Loss: 0.0354  lr:0.000001
[ Thu Jul 11 15:20:56 2024 ] 	Batch(6800/7879) done. Loss: 0.0161  lr:0.000001
[ Thu Jul 11 20:20:20 2024 ] Load weights from prova20/epoch10_model_test.pt.
[ Thu Jul 11 20:20:59 2024 ] Load weights from prova20/epoch10_model_new.pt.
[ Thu Jul 11 20:20:59 2024 ] Eval epoch: 1
[ Thu Jul 11 20:27:05 2024 ] 	Mean test loss of 6365 batches: 1.0055174639569102.
[ Thu Jul 11 20:27:05 2024 ] 	Class1 Precision: 72.84%, Recall: 89.05%
[ Thu Jul 11 20:27:05 2024 ] 	Class2 Precision: 71.12%, Recall: 71.64%
[ Thu Jul 11 20:27:05 2024 ] 	Class3 Precision: 76.82%, Recall: 84.98%
[ Thu Jul 11 20:27:05 2024 ] 	Class4 Precision: 91.57%, Recall: 83.52%
[ Thu Jul 11 20:27:05 2024 ] 	Class5 Precision: 67.27%, Recall: 80.73%
[ Thu Jul 11 20:27:05 2024 ] 	Class6 Precision: 89.19%, Recall: 84.00%
[ Thu Jul 11 20:27:05 2024 ] 	Class7 Precision: 82.31%, Recall: 77.82%
[ Thu Jul 11 20:27:05 2024 ] 	Class8 Precision: 93.01%, Recall: 92.67%
[ Thu Jul 11 20:27:05 2024 ] 	Class9 Precision: 97.43%, Recall: 97.07%
[ Thu Jul 11 20:27:05 2024 ] 	Class10 Precision: 76.95%, Recall: 72.16%
[ Thu Jul 11 20:27:05 2024 ] 	Class11 Precision: 46.01%, Recall: 44.32%
[ Thu Jul 11 20:27:05 2024 ] 	Class12 Precision: 33.85%, Recall: 56.25%
[ Thu Jul 11 20:27:05 2024 ] 	Class13 Precision: 72.04%, Recall: 80.81%
[ Thu Jul 11 20:27:05 2024 ] 	Class14 Precision: 90.14%, Recall: 96.36%
[ Thu Jul 11 20:27:05 2024 ] 	Class15 Precision: 92.91%, Recall: 94.93%
[ Thu Jul 11 20:27:05 2024 ] 	Class16 Precision: 66.16%, Recall: 79.49%
[ Thu Jul 11 20:27:05 2024 ] 	Class17 Precision: 71.00%, Recall: 59.85%
[ Thu Jul 11 20:27:05 2024 ] 	Class18 Precision: 80.34%, Recall: 86.81%
[ Thu Jul 11 20:27:05 2024 ] 	Class19 Precision: 85.61%, Recall: 84.67%
[ Thu Jul 11 20:27:05 2024 ] 	Class20 Precision: 93.51%, Recall: 90.07%
[ Thu Jul 11 20:27:05 2024 ] 	Class21 Precision: 88.77%, Recall: 92.67%
[ Thu Jul 11 20:27:05 2024 ] 	Class22 Precision: 91.84%, Recall: 82.12%
[ Thu Jul 11 20:27:05 2024 ] 	Class23 Precision: 75.08%, Recall: 90.15%
[ Thu Jul 11 20:27:05 2024 ] 	Class24 Precision: 92.22%, Recall: 90.22%
[ Thu Jul 11 20:27:05 2024 ] 	Class25 Precision: 73.11%, Recall: 88.32%
[ Thu Jul 11 20:27:05 2024 ] 	Class26 Precision: 90.31%, Recall: 94.91%
[ Thu Jul 11 20:27:05 2024 ] 	Class27 Precision: 96.42%, Recall: 97.46%
[ Thu Jul 11 20:27:05 2024 ] 	Class28 Precision: 69.28%, Recall: 83.64%
[ Thu Jul 11 20:27:05 2024 ] 	Class29 Precision: 26.67%, Recall: 61.09%
[ Thu Jul 11 20:27:05 2024 ] 	Class30 Precision: 47.88%, Recall: 53.45%
[ Thu Jul 11 20:27:05 2024 ] 	Class31 Precision: 75.34%, Recall: 60.87%
[ Thu Jul 11 20:27:05 2024 ] 	Class32 Precision: 72.17%, Recall: 80.80%
[ Thu Jul 11 20:27:05 2024 ] 	Class33 Precision: 70.36%, Recall: 85.14%
[ Thu Jul 11 20:27:05 2024 ] 	Class34 Precision: 62.50%, Recall: 83.33%
[ Thu Jul 11 20:27:05 2024 ] 	Class35 Precision: 85.56%, Recall: 88.04%
[ Thu Jul 11 20:27:05 2024 ] 	Class36 Precision: 76.85%, Recall: 86.91%
[ Thu Jul 11 20:27:05 2024 ] 	Class37 Precision: 81.27%, Recall: 78.62%
[ Thu Jul 11 20:27:05 2024 ] 	Class38 Precision: 70.03%, Recall: 90.58%
[ Thu Jul 11 20:27:05 2024 ] 	Class39 Precision: 78.39%, Recall: 88.04%
[ Thu Jul 11 20:27:05 2024 ] 	Class40 Precision: 90.73%, Recall: 85.14%
[ Thu Jul 11 20:27:05 2024 ] 	Class41 Precision: 64.44%, Recall: 73.55%
[ Thu Jul 11 20:27:05 2024 ] 	Class42 Precision: 90.66%, Recall: 94.93%
[ Thu Jul 11 20:27:05 2024 ] 	Class43 Precision: 96.15%, Recall: 90.91%
[ Thu Jul 11 20:27:05 2024 ] 	Class44 Precision: 76.21%, Recall: 68.48%
[ Thu Jul 11 20:27:05 2024 ] 	Class45 Precision: 61.56%, Recall: 91.67%
[ Thu Jul 11 20:27:05 2024 ] 	Class46 Precision: 71.82%, Recall: 94.20%
[ Thu Jul 11 20:27:05 2024 ] 	Class47 Precision: 75.16%, Recall: 84.42%
[ Thu Jul 11 20:27:05 2024 ] 	Class48 Precision: 81.18%, Recall: 75.27%
[ Thu Jul 11 20:27:05 2024 ] 	Class49 Precision: 79.86%, Recall: 80.73%
[ Thu Jul 11 20:27:05 2024 ] 	Class50 Precision: 79.69%, Recall: 74.45%
[ Thu Jul 11 20:27:05 2024 ] 	Class51 Precision: 82.76%, Recall: 86.96%
[ Thu Jul 11 20:27:05 2024 ] 	Class52 Precision: 94.80%, Recall: 92.39%
[ Thu Jul 11 20:27:05 2024 ] 	Class53 Precision: 87.18%, Recall: 86.23%
[ Thu Jul 11 20:27:05 2024 ] 	Class54 Precision: 66.29%, Recall: 84.78%
[ Thu Jul 11 20:27:05 2024 ] 	Class55 Precision: 89.24%, Recall: 93.80%
[ Thu Jul 11 20:27:05 2024 ] 	Class56 Precision: 74.53%, Recall: 85.87%
[ Thu Jul 11 20:27:05 2024 ] 	Class57 Precision: 80.07%, Recall: 89.09%
[ Thu Jul 11 20:27:05 2024 ] 	Class58 Precision: 87.06%, Recall: 97.46%
[ Thu Jul 11 20:27:05 2024 ] 	Class59 Precision: 92.93%, Recall: 96.34%
[ Thu Jul 11 20:27:05 2024 ] 	Class60 Precision: 96.91%, Recall: 90.94%
[ Thu Jul 11 20:27:05 2024 ] 	Class61 Precision: 89.87%, Recall: 83.78%
[ Thu Jul 11 20:27:05 2024 ] 	Class62 Precision: 88.95%, Recall: 86.75%
[ Thu Jul 11 20:27:05 2024 ] 	Class63 Precision: 87.45%, Recall: 73.08%
[ Thu Jul 11 20:27:05 2024 ] 	Class64 Precision: 97.37%, Recall: 84.39%
[ Thu Jul 11 20:27:05 2024 ] 	Class65 Precision: 91.68%, Recall: 73.00%
[ Thu Jul 11 20:27:05 2024 ] 	Class66 Precision: 94.92%, Recall: 87.96%
[ Thu Jul 11 20:27:05 2024 ] 	Class67 Precision: 65.65%, Recall: 71.38%
[ Thu Jul 11 20:27:05 2024 ] 	Class68 Precision: 89.06%, Recall: 69.39%
[ Thu Jul 11 20:27:05 2024 ] 	Class69 Precision: 53.43%, Recall: 65.04%
[ Thu Jul 11 20:27:05 2024 ] 	Class70 Precision: 82.58%, Recall: 82.43%
[ Thu Jul 11 20:27:05 2024 ] 	Class71 Precision: 50.44%, Recall: 40.17%
[ Thu Jul 11 20:27:05 2024 ] 	Class72 Precision: 41.68%, Recall: 41.39%
[ Thu Jul 11 20:27:05 2024 ] 	Class73 Precision: 49.33%, Recall: 19.44%
[ Thu Jul 11 20:27:05 2024 ] 	Class74 Precision: 51.08%, Recall: 45.44%
[ Thu Jul 11 20:27:05 2024 ] 	Class75 Precision: 43.04%, Recall: 64.67%
[ Thu Jul 11 20:27:05 2024 ] 	Class76 Precision: 49.45%, Recall: 62.30%
[ Thu Jul 11 20:27:05 2024 ] 	Class77 Precision: 68.54%, Recall: 63.76%
[ Thu Jul 11 20:27:05 2024 ] 	Class78 Precision: 65.42%, Recall: 67.36%
[ Thu Jul 11 20:27:05 2024 ] 	Class79 Precision: 71.75%, Recall: 78.61%
[ Thu Jul 11 20:27:05 2024 ] 	Class80 Precision: 93.33%, Recall: 95.12%
[ Thu Jul 11 20:27:05 2024 ] 	Class81 Precision: 78.84%, Recall: 80.63%
[ Thu Jul 11 20:27:05 2024 ] 	Class82 Precision: 65.93%, Recall: 62.61%
[ Thu Jul 11 20:27:05 2024 ] 	Class83 Precision: 77.68%, Recall: 61.74%
[ Thu Jul 11 20:27:05 2024 ] 	Class84 Precision: 55.73%, Recall: 49.30%
[ Thu Jul 11 20:27:05 2024 ] 	Class85 Precision: 93.79%, Recall: 76.31%
[ Thu Jul 11 20:27:05 2024 ] 	Class86 Precision: 78.78%, Recall: 67.25%
[ Thu Jul 11 20:27:05 2024 ] 	Class87 Precision: 95.85%, Recall: 88.35%
[ Thu Jul 11 20:27:05 2024 ] 	Class88 Precision: 87.57%, Recall: 86.81%
[ Thu Jul 11 20:27:05 2024 ] 	Class89 Precision: 81.78%, Recall: 70.26%
[ Thu Jul 11 20:27:05 2024 ] 	Class90 Precision: 76.30%, Recall: 76.56%
[ Thu Jul 11 20:27:05 2024 ] 	Class91 Precision: 64.15%, Recall: 45.82%
[ Thu Jul 11 20:27:05 2024 ] 	Class92 Precision: 87.36%, Recall: 80.28%
[ Thu Jul 11 20:27:05 2024 ] 	Class93 Precision: 70.81%, Recall: 74.13%
[ Thu Jul 11 20:27:05 2024 ] 	Class94 Precision: 71.13%, Recall: 65.79%
[ Thu Jul 11 20:27:05 2024 ] 	Class95 Precision: 90.80%, Recall: 89.37%
[ Thu Jul 11 20:27:05 2024 ] 	Class96 Precision: 80.30%, Recall: 93.57%
[ Thu Jul 11 20:27:05 2024 ] 	Class97 Precision: 98.41%, Recall: 96.70%
[ Thu Jul 11 20:27:05 2024 ] 	Class98 Precision: 97.01%, Recall: 96.17%
[ Thu Jul 11 20:27:05 2024 ] 	Class99 Precision: 94.01%, Recall: 92.87%
[ Thu Jul 11 20:27:05 2024 ] 	Class100 Precision: 84.34%, Recall: 92.86%
[ Thu Jul 11 20:27:05 2024 ] 	Class101 Precision: 98.90%, Recall: 78.57%
[ Thu Jul 11 20:27:05 2024 ] 	Class102 Precision: 78.35%, Recall: 92.68%
[ Thu Jul 11 20:27:05 2024 ] 	Class103 Precision: 65.03%, Recall: 62.09%
[ Thu Jul 11 20:27:05 2024 ] 	Class104 Precision: 88.95%, Recall: 88.02%
[ Thu Jul 11 20:27:05 2024 ] 	Class105 Precision: 61.08%, Recall: 53.22%
[ Thu Jul 11 20:27:05 2024 ] 	Class106 Precision: 72.34%, Recall: 49.57%
[ Thu Jul 11 20:27:05 2024 ] 	Class107 Precision: 60.71%, Recall: 56.60%
[ Thu Jul 11 20:27:05 2024 ] 	Class108 Precision: 79.25%, Recall: 88.19%
[ Thu Jul 11 20:27:05 2024 ] 	Class109 Precision: 83.80%, Recall: 83.65%
[ Thu Jul 11 20:27:05 2024 ] 	Class110 Precision: 74.60%, Recall: 72.52%
[ Thu Jul 11 20:27:05 2024 ] 	Class111 Precision: 85.59%, Recall: 88.87%
[ Thu Jul 11 20:27:05 2024 ] 	Class112 Precision: 94.13%, Recall: 94.62%
[ Thu Jul 11 20:27:05 2024 ] 	Class113 Precision: 99.27%, Recall: 94.96%
[ Thu Jul 11 20:27:05 2024 ] 	Class114 Precision: 97.40%, Recall: 90.97%
[ Thu Jul 11 20:27:05 2024 ] 	Class115 Precision: 86.26%, Recall: 89.41%
[ Thu Jul 11 20:27:05 2024 ] 	Class116 Precision: 95.04%, Recall: 93.06%
[ Thu Jul 11 20:27:05 2024 ] 	Class117 Precision: 91.99%, Recall: 81.91%
[ Thu Jul 11 20:27:05 2024 ] 	Class118 Precision: 90.48%, Recall: 85.91%
[ Thu Jul 11 20:27:05 2024 ] 	Class119 Precision: 96.73%, Recall: 82.26%
[ Thu Jul 11 20:27:05 2024 ] 	Class120 Precision: 96.39%, Recall: 88.02%
[ Thu Jul 11 20:27:06 2024 ] 	Class1 Top1: 89.05%
[ Thu Jul 11 20:27:06 2024 ] 	Class2 Top1: 71.64%
[ Thu Jul 11 20:27:06 2024 ] 	Class3 Top1: 84.98%
[ Thu Jul 11 20:27:06 2024 ] 	Class4 Top1: 83.52%
[ Thu Jul 11 20:27:06 2024 ] 	Class5 Top1: 80.73%
[ Thu Jul 11 20:27:06 2024 ] 	Class6 Top1: 84.00%
[ Thu Jul 11 20:27:06 2024 ] 	Class7 Top1: 77.82%
[ Thu Jul 11 20:27:06 2024 ] 	Class8 Top1: 92.67%
[ Thu Jul 11 20:27:06 2024 ] 	Class9 Top1: 97.07%
[ Thu Jul 11 20:27:06 2024 ] 	Class10 Top1: 72.16%
[ Thu Jul 11 20:27:06 2024 ] 	Class11 Top1: 44.32%
[ Thu Jul 11 20:27:06 2024 ] 	Class12 Top1: 56.25%
[ Thu Jul 11 20:27:06 2024 ] 	Class13 Top1: 80.81%
[ Thu Jul 11 20:27:06 2024 ] 	Class14 Top1: 96.36%
[ Thu Jul 11 20:27:06 2024 ] 	Class15 Top1: 94.93%
[ Thu Jul 11 20:27:06 2024 ] 	Class16 Top1: 79.49%
[ Thu Jul 11 20:27:06 2024 ] 	Class17 Top1: 59.85%
[ Thu Jul 11 20:27:06 2024 ] 	Class18 Top1: 86.81%
[ Thu Jul 11 20:27:06 2024 ] 	Class19 Top1: 84.67%
[ Thu Jul 11 20:27:06 2024 ] 	Class20 Top1: 90.07%
[ Thu Jul 11 20:27:06 2024 ] 	Class21 Top1: 92.67%
[ Thu Jul 11 20:27:06 2024 ] 	Class22 Top1: 82.12%
[ Thu Jul 11 20:27:06 2024 ] 	Class23 Top1: 90.15%
[ Thu Jul 11 20:27:06 2024 ] 	Class24 Top1: 90.22%
[ Thu Jul 11 20:27:06 2024 ] 	Class25 Top1: 88.32%
[ Thu Jul 11 20:27:06 2024 ] 	Class26 Top1: 94.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class27 Top1: 97.46%
[ Thu Jul 11 20:27:06 2024 ] 	Class28 Top1: 83.64%
[ Thu Jul 11 20:27:06 2024 ] 	Class29 Top1: 61.09%
[ Thu Jul 11 20:27:06 2024 ] 	Class30 Top1: 53.45%
[ Thu Jul 11 20:27:06 2024 ] 	Class31 Top1: 60.87%
[ Thu Jul 11 20:27:06 2024 ] 	Class32 Top1: 80.80%
[ Thu Jul 11 20:27:06 2024 ] 	Class33 Top1: 85.14%
[ Thu Jul 11 20:27:06 2024 ] 	Class34 Top1: 83.33%
[ Thu Jul 11 20:27:06 2024 ] 	Class35 Top1: 88.04%
[ Thu Jul 11 20:27:06 2024 ] 	Class36 Top1: 86.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class37 Top1: 78.62%
[ Thu Jul 11 20:27:06 2024 ] 	Class38 Top1: 90.58%
[ Thu Jul 11 20:27:06 2024 ] 	Class39 Top1: 88.04%
[ Thu Jul 11 20:27:06 2024 ] 	Class40 Top1: 85.14%
[ Thu Jul 11 20:27:06 2024 ] 	Class41 Top1: 73.55%
[ Thu Jul 11 20:27:06 2024 ] 	Class42 Top1: 94.93%
[ Thu Jul 11 20:27:06 2024 ] 	Class43 Top1: 90.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class44 Top1: 68.48%
[ Thu Jul 11 20:27:06 2024 ] 	Class45 Top1: 91.67%
[ Thu Jul 11 20:27:06 2024 ] 	Class46 Top1: 94.20%
[ Thu Jul 11 20:27:06 2024 ] 	Class47 Top1: 84.42%
[ Thu Jul 11 20:27:06 2024 ] 	Class48 Top1: 75.27%
[ Thu Jul 11 20:27:06 2024 ] 	Class49 Top1: 80.73%
[ Thu Jul 11 20:27:06 2024 ] 	Class50 Top1: 74.45%
[ Thu Jul 11 20:27:06 2024 ] 	Class51 Top1: 86.96%
[ Thu Jul 11 20:27:06 2024 ] 	Class52 Top1: 92.39%
[ Thu Jul 11 20:27:06 2024 ] 	Class53 Top1: 86.23%
[ Thu Jul 11 20:27:06 2024 ] 	Class54 Top1: 84.78%
[ Thu Jul 11 20:27:06 2024 ] 	Class55 Top1: 93.80%
[ Thu Jul 11 20:27:06 2024 ] 	Class56 Top1: 85.87%
[ Thu Jul 11 20:27:06 2024 ] 	Class57 Top1: 89.09%
[ Thu Jul 11 20:27:06 2024 ] 	Class58 Top1: 97.46%
[ Thu Jul 11 20:27:06 2024 ] 	Class59 Top1: 96.34%
[ Thu Jul 11 20:27:06 2024 ] 	Class60 Top1: 90.94%
[ Thu Jul 11 20:27:06 2024 ] 	Class61 Top1: 83.78%
[ Thu Jul 11 20:27:06 2024 ] 	Class62 Top1: 86.75%
[ Thu Jul 11 20:27:06 2024 ] 	Class63 Top1: 73.08%
[ Thu Jul 11 20:27:06 2024 ] 	Class64 Top1: 84.39%
[ Thu Jul 11 20:27:06 2024 ] 	Class65 Top1: 73.00%
[ Thu Jul 11 20:27:06 2024 ] 	Class66 Top1: 87.96%
[ Thu Jul 11 20:27:06 2024 ] 	Class67 Top1: 71.38%
[ Thu Jul 11 20:27:06 2024 ] 	Class68 Top1: 69.39%
[ Thu Jul 11 20:27:06 2024 ] 	Class69 Top1: 65.04%
[ Thu Jul 11 20:27:06 2024 ] 	Class70 Top1: 82.43%
[ Thu Jul 11 20:27:06 2024 ] 	Class71 Top1: 40.17%
[ Thu Jul 11 20:27:06 2024 ] 	Class72 Top1: 41.39%
[ Thu Jul 11 20:27:06 2024 ] 	Class73 Top1: 19.44%
[ Thu Jul 11 20:27:06 2024 ] 	Class74 Top1: 45.44%
[ Thu Jul 11 20:27:06 2024 ] 	Class75 Top1: 64.67%
[ Thu Jul 11 20:27:06 2024 ] 	Class76 Top1: 62.30%
[ Thu Jul 11 20:27:06 2024 ] 	Class77 Top1: 63.76%
[ Thu Jul 11 20:27:06 2024 ] 	Class78 Top1: 67.36%
[ Thu Jul 11 20:27:06 2024 ] 	Class79 Top1: 78.61%
[ Thu Jul 11 20:27:06 2024 ] 	Class80 Top1: 95.12%
[ Thu Jul 11 20:27:06 2024 ] 	Class81 Top1: 80.63%
[ Thu Jul 11 20:27:06 2024 ] 	Class82 Top1: 62.61%
[ Thu Jul 11 20:27:06 2024 ] 	Class83 Top1: 61.74%
[ Thu Jul 11 20:27:06 2024 ] 	Class84 Top1: 49.30%
[ Thu Jul 11 20:27:06 2024 ] 	Class85 Top1: 76.31%
[ Thu Jul 11 20:27:06 2024 ] 	Class86 Top1: 67.25%
[ Thu Jul 11 20:27:06 2024 ] 	Class87 Top1: 88.35%
[ Thu Jul 11 20:27:06 2024 ] 	Class88 Top1: 86.81%
[ Thu Jul 11 20:27:06 2024 ] 	Class89 Top1: 70.26%
[ Thu Jul 11 20:27:06 2024 ] 	Class90 Top1: 76.56%
[ Thu Jul 11 20:27:06 2024 ] 	Class91 Top1: 45.82%
[ Thu Jul 11 20:27:06 2024 ] 	Class92 Top1: 80.28%
[ Thu Jul 11 20:27:06 2024 ] 	Class93 Top1: 74.13%
[ Thu Jul 11 20:27:06 2024 ] 	Class94 Top1: 65.79%
[ Thu Jul 11 20:27:06 2024 ] 	Class95 Top1: 89.37%
[ Thu Jul 11 20:27:06 2024 ] 	Class96 Top1: 93.57%
[ Thu Jul 11 20:27:06 2024 ] 	Class97 Top1: 96.70%
[ Thu Jul 11 20:27:06 2024 ] 	Class98 Top1: 96.17%
[ Thu Jul 11 20:27:06 2024 ] 	Class99 Top1: 92.87%
[ Thu Jul 11 20:27:06 2024 ] 	Class100 Top1: 92.86%
[ Thu Jul 11 20:27:06 2024 ] 	Class101 Top1: 78.57%
[ Thu Jul 11 20:27:06 2024 ] 	Class102 Top1: 92.68%
[ Thu Jul 11 20:27:06 2024 ] 	Class103 Top1: 62.09%
[ Thu Jul 11 20:27:06 2024 ] 	Class104 Top1: 88.02%
[ Thu Jul 11 20:27:06 2024 ] 	Class105 Top1: 53.22%
[ Thu Jul 11 20:27:06 2024 ] 	Class106 Top1: 49.57%
[ Thu Jul 11 20:27:06 2024 ] 	Class107 Top1: 56.60%
[ Thu Jul 11 20:27:06 2024 ] 	Class108 Top1: 88.19%
[ Thu Jul 11 20:27:06 2024 ] 	Class109 Top1: 83.65%
[ Thu Jul 11 20:27:06 2024 ] 	Class110 Top1: 72.52%
[ Thu Jul 11 20:27:06 2024 ] 	Class111 Top1: 88.87%
[ Thu Jul 11 20:27:06 2024 ] 	Class112 Top1: 94.62%
[ Thu Jul 11 20:27:06 2024 ] 	Class113 Top1: 94.96%
[ Thu Jul 11 20:27:06 2024 ] 	Class114 Top1: 90.97%
[ Thu Jul 11 20:27:06 2024 ] 	Class115 Top1: 89.41%
[ Thu Jul 11 20:27:06 2024 ] 	Class116 Top1: 93.06%
[ Thu Jul 11 20:27:06 2024 ] 	Class117 Top1: 81.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class118 Top1: 85.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class119 Top1: 82.26%
[ Thu Jul 11 20:27:06 2024 ] 	Class120 Top1: 88.02%
[ Thu Jul 11 20:27:06 2024 ] 	Top1: 79.33%
[ Thu Jul 11 20:27:06 2024 ] 	Class1 Top5: 94.53%
[ Thu Jul 11 20:27:06 2024 ] 	Class2 Top5: 89.09%
[ Thu Jul 11 20:27:06 2024 ] 	Class3 Top5: 95.60%
[ Thu Jul 11 20:27:06 2024 ] 	Class4 Top5: 94.14%
[ Thu Jul 11 20:27:06 2024 ] 	Class5 Top5: 91.27%
[ Thu Jul 11 20:27:06 2024 ] 	Class6 Top5: 98.55%
[ Thu Jul 11 20:27:06 2024 ] 	Class7 Top5: 94.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class8 Top5: 97.44%
[ Thu Jul 11 20:27:06 2024 ] 	Class9 Top5: 98.53%
[ Thu Jul 11 20:27:06 2024 ] 	Class10 Top5: 93.41%
[ Thu Jul 11 20:27:06 2024 ] 	Class11 Top5: 87.55%
[ Thu Jul 11 20:27:06 2024 ] 	Class12 Top5: 94.12%
[ Thu Jul 11 20:27:06 2024 ] 	Class13 Top5: 96.68%
[ Thu Jul 11 20:27:06 2024 ] 	Class14 Top5: 98.18%
[ Thu Jul 11 20:27:06 2024 ] 	Class15 Top5: 98.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class16 Top5: 97.07%
[ Thu Jul 11 20:27:06 2024 ] 	Class17 Top5: 90.51%
[ Thu Jul 11 20:27:06 2024 ] 	Class18 Top5: 95.24%
[ Thu Jul 11 20:27:06 2024 ] 	Class19 Top5: 97.45%
[ Thu Jul 11 20:27:06 2024 ] 	Class20 Top5: 97.79%
[ Thu Jul 11 20:27:06 2024 ] 	Class21 Top5: 97.07%
[ Thu Jul 11 20:27:06 2024 ] 	Class22 Top5: 96.72%
[ Thu Jul 11 20:27:06 2024 ] 	Class23 Top5: 96.72%
[ Thu Jul 11 20:27:06 2024 ] 	Class24 Top5: 97.46%
[ Thu Jul 11 20:27:06 2024 ] 	Class25 Top5: 94.16%
[ Thu Jul 11 20:27:06 2024 ] 	Class26 Top5: 99.27%
[ Thu Jul 11 20:27:06 2024 ] 	Class27 Top5: 99.28%
[ Thu Jul 11 20:27:06 2024 ] 	Class28 Top5: 94.18%
[ Thu Jul 11 20:27:06 2024 ] 	Class29 Top5: 92.36%
[ Thu Jul 11 20:27:06 2024 ] 	Class30 Top5: 91.64%
[ Thu Jul 11 20:27:06 2024 ] 	Class31 Top5: 89.13%
[ Thu Jul 11 20:27:06 2024 ] 	Class32 Top5: 93.12%
[ Thu Jul 11 20:27:06 2024 ] 	Class33 Top5: 93.12%
[ Thu Jul 11 20:27:06 2024 ] 	Class34 Top5: 95.65%
[ Thu Jul 11 20:27:06 2024 ] 	Class35 Top5: 97.10%
[ Thu Jul 11 20:27:06 2024 ] 	Class36 Top5: 94.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class37 Top5: 93.84%
[ Thu Jul 11 20:27:06 2024 ] 	Class38 Top5: 96.74%
[ Thu Jul 11 20:27:06 2024 ] 	Class39 Top5: 98.19%
[ Thu Jul 11 20:27:06 2024 ] 	Class40 Top5: 97.83%
[ Thu Jul 11 20:27:06 2024 ] 	Class41 Top5: 94.57%
[ Thu Jul 11 20:27:06 2024 ] 	Class42 Top5: 99.64%
[ Thu Jul 11 20:27:06 2024 ] 	Class43 Top5: 98.55%
[ Thu Jul 11 20:27:06 2024 ] 	Class44 Top5: 90.94%
[ Thu Jul 11 20:27:06 2024 ] 	Class45 Top5: 98.55%
[ Thu Jul 11 20:27:06 2024 ] 	Class46 Top5: 98.19%
[ Thu Jul 11 20:27:06 2024 ] 	Class47 Top5: 94.20%
[ Thu Jul 11 20:27:06 2024 ] 	Class48 Top5: 98.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class49 Top5: 95.27%
[ Thu Jul 11 20:27:06 2024 ] 	Class50 Top5: 94.53%
[ Thu Jul 11 20:27:06 2024 ] 	Class51 Top5: 96.38%
[ Thu Jul 11 20:27:06 2024 ] 	Class52 Top5: 98.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class53 Top5: 97.10%
[ Thu Jul 11 20:27:06 2024 ] 	Class54 Top5: 95.65%
[ Thu Jul 11 20:27:06 2024 ] 	Class55 Top5: 98.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class56 Top5: 98.55%
[ Thu Jul 11 20:27:06 2024 ] 	Class57 Top5: 98.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class58 Top5: 98.19%
[ Thu Jul 11 20:27:06 2024 ] 	Class59 Top5: 98.90%
[ Thu Jul 11 20:27:06 2024 ] 	Class60 Top5: 97.46%
[ Thu Jul 11 20:27:06 2024 ] 	Class61 Top5: 92.34%
[ Thu Jul 11 20:27:06 2024 ] 	Class62 Top5: 94.70%
[ Thu Jul 11 20:27:06 2024 ] 	Class63 Top5: 91.43%
[ Thu Jul 11 20:27:06 2024 ] 	Class64 Top5: 93.68%
[ Thu Jul 11 20:27:06 2024 ] 	Class65 Top5: 87.28%
[ Thu Jul 11 20:27:06 2024 ] 	Class66 Top5: 94.76%
[ Thu Jul 11 20:27:06 2024 ] 	Class67 Top5: 92.84%
[ Thu Jul 11 20:27:06 2024 ] 	Class68 Top5: 87.65%
[ Thu Jul 11 20:27:06 2024 ] 	Class69 Top5: 90.78%
[ Thu Jul 11 20:27:06 2024 ] 	Class70 Top5: 94.61%
[ Thu Jul 11 20:27:06 2024 ] 	Class71 Top5: 91.48%
[ Thu Jul 11 20:27:06 2024 ] 	Class72 Top5: 92.35%
[ Thu Jul 11 20:27:06 2024 ] 	Class73 Top5: 81.79%
[ Thu Jul 11 20:27:06 2024 ] 	Class74 Top5: 84.04%
[ Thu Jul 11 20:27:06 2024 ] 	Class75 Top5: 93.85%
[ Thu Jul 11 20:27:06 2024 ] 	Class76 Top5: 92.15%
[ Thu Jul 11 20:27:06 2024 ] 	Class77 Top5: 85.02%
[ Thu Jul 11 20:27:06 2024 ] 	Class78 Top5: 89.70%
[ Thu Jul 11 20:27:06 2024 ] 	Class79 Top5: 92.52%
[ Thu Jul 11 20:27:06 2024 ] 	Class80 Top5: 98.08%
[ Thu Jul 11 20:27:06 2024 ] 	Class81 Top5: 92.50%
[ Thu Jul 11 20:27:06 2024 ] 	Class82 Top5: 83.65%
[ Thu Jul 11 20:27:06 2024 ] 	Class83 Top5: 84.70%
[ Thu Jul 11 20:27:06 2024 ] 	Class84 Top5: 87.06%
[ Thu Jul 11 20:27:06 2024 ] 	Class85 Top5: 90.59%
[ Thu Jul 11 20:27:06 2024 ] 	Class86 Top5: 88.68%
[ Thu Jul 11 20:27:06 2024 ] 	Class87 Top5: 96.87%
[ Thu Jul 11 20:27:06 2024 ] 	Class88 Top5: 95.83%
[ Thu Jul 11 20:27:06 2024 ] 	Class89 Top5: 92.70%
[ Thu Jul 11 20:27:06 2024 ] 	Class90 Top5: 93.40%
[ Thu Jul 11 20:27:06 2024 ] 	Class91 Top5: 83.80%
[ Thu Jul 11 20:27:06 2024 ] 	Class92 Top5: 93.31%
[ Thu Jul 11 20:27:06 2024 ] 	Class93 Top5: 90.45%
[ Thu Jul 11 20:27:06 2024 ] 	Class94 Top5: 92.32%
[ Thu Jul 11 20:27:06 2024 ] 	Class95 Top5: 97.74%
[ Thu Jul 11 20:27:06 2024 ] 	Class96 Top5: 98.61%
[ Thu Jul 11 20:27:06 2024 ] 	Class97 Top5: 98.96%
[ Thu Jul 11 20:27:06 2024 ] 	Class98 Top5: 98.95%
[ Thu Jul 11 20:27:06 2024 ] 	Class99 Top5: 97.22%
[ Thu Jul 11 20:27:06 2024 ] 	Class100 Top5: 98.78%
[ Thu Jul 11 20:27:06 2024 ] 	Class101 Top5: 92.86%
[ Thu Jul 11 20:27:06 2024 ] 	Class102 Top5: 97.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class103 Top5: 93.57%
[ Thu Jul 11 20:27:06 2024 ] 	Class104 Top5: 96.53%
[ Thu Jul 11 20:27:06 2024 ] 	Class105 Top5: 90.78%
[ Thu Jul 11 20:27:06 2024 ] 	Class106 Top5: 89.04%
[ Thu Jul 11 20:27:06 2024 ] 	Class107 Top5: 91.32%
[ Thu Jul 11 20:27:06 2024 ] 	Class108 Top5: 96.35%
[ Thu Jul 11 20:27:06 2024 ] 	Class109 Top5: 95.13%
[ Thu Jul 11 20:27:06 2024 ] 	Class110 Top5: 95.30%
[ Thu Jul 11 20:27:06 2024 ] 	Class111 Top5: 96.17%
[ Thu Jul 11 20:27:06 2024 ] 	Class112 Top5: 97.57%
[ Thu Jul 11 20:27:06 2024 ] 	Class113 Top5: 98.09%
[ Thu Jul 11 20:27:06 2024 ] 	Class114 Top5: 97.40%
[ Thu Jul 11 20:27:06 2024 ] 	Class115 Top5: 98.26%
[ Thu Jul 11 20:27:06 2024 ] 	Class116 Top5: 98.78%
[ Thu Jul 11 20:27:06 2024 ] 	Class117 Top5: 95.48%
[ Thu Jul 11 20:27:06 2024 ] 	Class118 Top5: 97.91%
[ Thu Jul 11 20:27:06 2024 ] 	Class119 Top5: 94.09%
[ Thu Jul 11 20:27:06 2024 ] 	Class120 Top5: 95.83%
[ Thu Jul 11 20:27:06 2024 ] 	Top5: 94.48%
[ Thu Jul 11 20:27:06 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': False, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xsub/train_joint_120.npy', 'label_path': 'new_data_processed/xsub/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xsub/val_joint_120.npy', 'label_path': 'new_data_processed/xsub/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': False, 'only_attention': True, 'tcn_attention': True, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': 'prova20/epoch10_model_new.pt', 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': False, 'scheduler': 1, 'base_lr': 1e-06, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Thu Jul 11 20:27:06 2024 ] Training epoch: 1
[ Thu Jul 11 20:27:07 2024 ] 	Batch(0/7879) done. Loss: 0.1203  lr:0.000001
[ Thu Jul 11 20:27:29 2024 ] 	Batch(100/7879) done. Loss: 0.2856  lr:0.000001
[ Thu Jul 11 20:27:51 2024 ] 	Batch(200/7879) done. Loss: 0.0222  lr:0.000001
[ Thu Jul 11 20:28:13 2024 ] 	Batch(300/7879) done. Loss: 0.1456  lr:0.000001
