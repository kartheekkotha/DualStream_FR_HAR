[ Sun Jul 14 13:33:45 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xset/train_joint_120.npy', 'label_path': 'new_data_processed/xset/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xset/val_joint_120.npy', 'label_path': 'new_data_processed/xset/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': False, 'only_attention': True, 'tcn_attention': True, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': False, 'scheduler': 1, 'base_lr': 0.01, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 150, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Sun Jul 14 13:33:45 2024 ] Training epoch: 1
[ Sun Jul 14 13:33:47 2024 ] 	Batch(0/6809) done. Loss: 6.7329  lr:0.010000
[ Sun Jul 14 13:34:09 2024 ] 	Batch(100/6809) done. Loss: 5.3920  lr:0.010000
[ Sun Jul 14 13:34:32 2024 ] 	Batch(200/6809) done. Loss: 5.3783  lr:0.010000
[ Sun Jul 14 13:34:54 2024 ] 	Batch(300/6809) done. Loss: 4.7854  lr:0.010000
[ Sun Jul 14 13:35:16 2024 ] 	Batch(400/6809) done. Loss: 4.4839  lr:0.010000
[ Sun Jul 14 13:35:38 2024 ] 
Training: Epoch [0/150], Step [499], Loss: 5.442300796508789, Training Accuracy: 2.4
[ Sun Jul 14 13:35:38 2024 ] 	Batch(500/6809) done. Loss: 4.5178  lr:0.010000
[ Sun Jul 14 13:36:00 2024 ] 	Batch(600/6809) done. Loss: 4.3760  lr:0.010000
[ Sun Jul 14 13:36:22 2024 ] 	Batch(700/6809) done. Loss: 4.0922  lr:0.010000
[ Sun Jul 14 13:36:45 2024 ] 	Batch(800/6809) done. Loss: 3.7999  lr:0.010000
[ Sun Jul 14 13:37:07 2024 ] 	Batch(900/6809) done. Loss: 5.2714  lr:0.010000
[ Sun Jul 14 13:37:30 2024 ] 
Training: Epoch [0/150], Step [999], Loss: 4.518235206604004, Training Accuracy: 3.2625
[ Sun Jul 14 13:37:30 2024 ] 	Batch(1000/6809) done. Loss: 3.4389  lr:0.010000
[ Sun Jul 14 13:37:53 2024 ] 	Batch(1100/6809) done. Loss: 4.3177  lr:0.010000
[ Sun Jul 14 13:38:15 2024 ] 	Batch(1200/6809) done. Loss: 3.9043  lr:0.010000
[ Sun Jul 14 13:38:37 2024 ] 	Batch(1300/6809) done. Loss: 4.8564  lr:0.010000
[ Sun Jul 14 13:38:59 2024 ] 	Batch(1400/6809) done. Loss: 4.8975  lr:0.010000
[ Sun Jul 14 13:39:21 2024 ] 
Training: Epoch [0/150], Step [1499], Loss: 3.9718775749206543, Training Accuracy: 4.108333333333333
[ Sun Jul 14 13:39:21 2024 ] 	Batch(1500/6809) done. Loss: 4.7079  lr:0.010000
[ Sun Jul 14 13:39:43 2024 ] 	Batch(1600/6809) done. Loss: 3.9210  lr:0.010000
[ Sun Jul 14 13:40:05 2024 ] 	Batch(1700/6809) done. Loss: 4.2930  lr:0.010000
[ Sun Jul 14 13:40:27 2024 ] 	Batch(1800/6809) done. Loss: 3.9271  lr:0.010000
[ Sun Jul 14 13:40:49 2024 ] 	Batch(1900/6809) done. Loss: 3.9497  lr:0.010000
[ Sun Jul 14 13:41:10 2024 ] 
Training: Epoch [0/150], Step [1999], Loss: 3.8420262336730957, Training Accuracy: 5.0687500000000005
[ Sun Jul 14 13:41:11 2024 ] 	Batch(2000/6809) done. Loss: 3.7595  lr:0.010000
[ Sun Jul 14 13:41:32 2024 ] 	Batch(2100/6809) done. Loss: 3.9119  lr:0.010000
[ Sun Jul 14 13:41:54 2024 ] 	Batch(2200/6809) done. Loss: 3.7184  lr:0.010000
[ Sun Jul 14 13:42:16 2024 ] 	Batch(2300/6809) done. Loss: 3.6957  lr:0.010000
[ Sun Jul 14 13:42:38 2024 ] 	Batch(2400/6809) done. Loss: 3.7898  lr:0.010000
[ Sun Jul 14 13:43:00 2024 ] 
Training: Epoch [0/150], Step [2499], Loss: 3.597407579421997, Training Accuracy: 5.965
[ Sun Jul 14 13:43:00 2024 ] 	Batch(2500/6809) done. Loss: 2.9635  lr:0.010000
[ Sun Jul 14 13:43:22 2024 ] 	Batch(2600/6809) done. Loss: 3.8620  lr:0.010000
[ Sun Jul 14 13:43:44 2024 ] 	Batch(2700/6809) done. Loss: 4.5197  lr:0.010000
[ Sun Jul 14 13:44:06 2024 ] 	Batch(2800/6809) done. Loss: 3.5767  lr:0.010000
[ Sun Jul 14 13:44:28 2024 ] 	Batch(2900/6809) done. Loss: 3.6852  lr:0.010000
[ Sun Jul 14 13:44:49 2024 ] 
Training: Epoch [0/150], Step [2999], Loss: 3.379315137863159, Training Accuracy: 6.683333333333333
[ Sun Jul 14 13:44:50 2024 ] 	Batch(3000/6809) done. Loss: 3.4553  lr:0.010000
[ Sun Jul 14 13:45:11 2024 ] 	Batch(3100/6809) done. Loss: 3.9631  lr:0.010000
[ Sun Jul 14 13:45:33 2024 ] 	Batch(3200/6809) done. Loss: 3.1104  lr:0.010000
[ Sun Jul 14 13:45:55 2024 ] 	Batch(3300/6809) done. Loss: 4.2170  lr:0.010000
[ Sun Jul 14 13:46:17 2024 ] 	Batch(3400/6809) done. Loss: 3.6924  lr:0.010000
[ Sun Jul 14 13:46:39 2024 ] 
Training: Epoch [0/150], Step [3499], Loss: 3.0604891777038574, Training Accuracy: 7.621428571428572
[ Sun Jul 14 13:46:39 2024 ] 	Batch(3500/6809) done. Loss: 3.4936  lr:0.010000
[ Sun Jul 14 13:47:01 2024 ] 	Batch(3600/6809) done. Loss: 3.2161  lr:0.010000
[ Sun Jul 14 13:47:23 2024 ] 	Batch(3700/6809) done. Loss: 2.9662  lr:0.010000
[ Sun Jul 14 13:47:45 2024 ] 	Batch(3800/6809) done. Loss: 2.8604  lr:0.010000
[ Sun Jul 14 13:48:07 2024 ] 	Batch(3900/6809) done. Loss: 3.6493  lr:0.010000
[ Sun Jul 14 13:48:28 2024 ] 
Training: Epoch [0/150], Step [3999], Loss: 2.5020015239715576, Training Accuracy: 8.40625
[ Sun Jul 14 13:48:29 2024 ] 	Batch(4000/6809) done. Loss: 2.8503  lr:0.010000
[ Sun Jul 14 13:48:51 2024 ] 	Batch(4100/6809) done. Loss: 2.8939  lr:0.010000
[ Sun Jul 14 13:49:13 2024 ] 	Batch(4200/6809) done. Loss: 3.1909  lr:0.010000
[ Sun Jul 14 13:49:34 2024 ] 	Batch(4300/6809) done. Loss: 2.7294  lr:0.010000
[ Sun Jul 14 13:49:56 2024 ] 	Batch(4400/6809) done. Loss: 2.7730  lr:0.010000
[ Sun Jul 14 13:50:18 2024 ] 
Training: Epoch [0/150], Step [4499], Loss: 2.4725353717803955, Training Accuracy: 9.338888888888889
[ Sun Jul 14 13:50:18 2024 ] 	Batch(4500/6809) done. Loss: 3.7788  lr:0.010000
[ Sun Jul 14 13:50:40 2024 ] 	Batch(4600/6809) done. Loss: 2.9627  lr:0.010000
[ Sun Jul 14 13:51:02 2024 ] 	Batch(4700/6809) done. Loss: 3.0597  lr:0.010000
[ Sun Jul 14 13:51:24 2024 ] 	Batch(4800/6809) done. Loss: 4.4574  lr:0.010000
[ Sun Jul 14 13:51:47 2024 ] 	Batch(4900/6809) done. Loss: 3.4094  lr:0.010000
[ Sun Jul 14 13:52:09 2024 ] 
Training: Epoch [0/150], Step [4999], Loss: 3.075860023498535, Training Accuracy: 10.252500000000001
[ Sun Jul 14 13:52:09 2024 ] 	Batch(5000/6809) done. Loss: 2.7720  lr:0.010000
[ Sun Jul 14 13:52:32 2024 ] 	Batch(5100/6809) done. Loss: 2.5040  lr:0.010000
[ Sun Jul 14 13:52:55 2024 ] 	Batch(5200/6809) done. Loss: 2.5195  lr:0.010000
[ Sun Jul 14 13:53:17 2024 ] 	Batch(5300/6809) done. Loss: 3.8544  lr:0.010000
[ Sun Jul 14 13:53:39 2024 ] 	Batch(5400/6809) done. Loss: 3.9423  lr:0.010000
[ Sun Jul 14 13:54:00 2024 ] 
Training: Epoch [0/150], Step [5499], Loss: 3.7168586254119873, Training Accuracy: 11.175
[ Sun Jul 14 13:54:01 2024 ] 	Batch(5500/6809) done. Loss: 2.6402  lr:0.010000
[ Sun Jul 14 13:54:23 2024 ] 	Batch(5600/6809) done. Loss: 2.4705  lr:0.010000
[ Sun Jul 14 13:54:45 2024 ] 	Batch(5700/6809) done. Loss: 2.5962  lr:0.010000
[ Sun Jul 14 13:55:08 2024 ] 	Batch(5800/6809) done. Loss: 2.4446  lr:0.010000
[ Sun Jul 14 13:55:30 2024 ] 	Batch(5900/6809) done. Loss: 3.5064  lr:0.010000
[ Sun Jul 14 13:55:53 2024 ] 
Training: Epoch [0/150], Step [5999], Loss: 3.1460564136505127, Training Accuracy: 12.112499999999999
[ Sun Jul 14 13:55:53 2024 ] 	Batch(6000/6809) done. Loss: 2.7514  lr:0.010000
[ Sun Jul 14 13:56:15 2024 ] 	Batch(6100/6809) done. Loss: 2.4646  lr:0.010000
[ Sun Jul 14 13:56:37 2024 ] 	Batch(6200/6809) done. Loss: 2.2855  lr:0.010000
[ Sun Jul 14 13:56:59 2024 ] 	Batch(6300/6809) done. Loss: 2.2166  lr:0.010000
[ Sun Jul 14 13:57:21 2024 ] 	Batch(6400/6809) done. Loss: 2.8020  lr:0.010000
[ Sun Jul 14 13:57:44 2024 ] 
Training: Epoch [0/150], Step [6499], Loss: 3.5295827388763428, Training Accuracy: 13.025
[ Sun Jul 14 13:57:44 2024 ] 	Batch(6500/6809) done. Loss: 3.1388  lr:0.010000
[ Sun Jul 14 13:58:06 2024 ] 	Batch(6600/6809) done. Loss: 3.7636  lr:0.010000
[ Sun Jul 14 13:58:29 2024 ] 	Batch(6700/6809) done. Loss: 4.0834  lr:0.010000
[ Sun Jul 14 13:58:52 2024 ] 	Batch(6800/6809) done. Loss: 2.0738  lr:0.010000
[ Sun Jul 14 13:58:54 2024 ] 	Mean training loss: 3.5596.
[ Sun Jul 14 13:58:54 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 13:58:54 2024 ] Training epoch: 2
[ Sun Jul 14 13:58:55 2024 ] 	Batch(0/6809) done. Loss: 3.3382  lr:0.010000
[ Sun Jul 14 13:59:18 2024 ] 	Batch(100/6809) done. Loss: 2.4729  lr:0.010000
[ Sun Jul 14 13:59:41 2024 ] 	Batch(200/6809) done. Loss: 2.5444  lr:0.010000
[ Sun Jul 14 14:00:04 2024 ] 	Batch(300/6809) done. Loss: 2.4990  lr:0.010000
[ Sun Jul 14 14:00:27 2024 ] 	Batch(400/6809) done. Loss: 2.5468  lr:0.010000
[ Sun Jul 14 14:00:50 2024 ] 
Training: Epoch [1/150], Step [499], Loss: 2.5307259559631348, Training Accuracy: 26.125
[ Sun Jul 14 14:00:50 2024 ] 	Batch(500/6809) done. Loss: 3.0520  lr:0.010000
[ Sun Jul 14 14:01:14 2024 ] 	Batch(600/6809) done. Loss: 2.7407  lr:0.010000
[ Sun Jul 14 14:01:37 2024 ] 	Batch(700/6809) done. Loss: 3.0637  lr:0.010000
[ Sun Jul 14 14:02:00 2024 ] 	Batch(800/6809) done. Loss: 3.8698  lr:0.010000
[ Sun Jul 14 14:02:23 2024 ] 	Batch(900/6809) done. Loss: 2.7701  lr:0.010000
[ Sun Jul 14 14:02:46 2024 ] 
Training: Epoch [1/150], Step [999], Loss: 2.5692710876464844, Training Accuracy: 26.8375
[ Sun Jul 14 14:02:46 2024 ] 	Batch(1000/6809) done. Loss: 2.1583  lr:0.010000
[ Sun Jul 14 14:03:09 2024 ] 	Batch(1100/6809) done. Loss: 2.8164  lr:0.010000
[ Sun Jul 14 14:03:32 2024 ] 	Batch(1200/6809) done. Loss: 2.6996  lr:0.010000
[ Sun Jul 14 14:03:55 2024 ] 	Batch(1300/6809) done. Loss: 3.7952  lr:0.010000
[ Sun Jul 14 14:04:19 2024 ] 	Batch(1400/6809) done. Loss: 2.3728  lr:0.010000
[ Sun Jul 14 14:04:41 2024 ] 
Training: Epoch [1/150], Step [1499], Loss: 1.851704716682434, Training Accuracy: 27.425
[ Sun Jul 14 14:04:42 2024 ] 	Batch(1500/6809) done. Loss: 4.1477  lr:0.010000
[ Sun Jul 14 14:05:05 2024 ] 	Batch(1600/6809) done. Loss: 2.9645  lr:0.010000
[ Sun Jul 14 14:05:28 2024 ] 	Batch(1700/6809) done. Loss: 2.3486  lr:0.010000
[ Sun Jul 14 14:05:51 2024 ] 	Batch(1800/6809) done. Loss: 3.7175  lr:0.010000
[ Sun Jul 14 14:06:14 2024 ] 	Batch(1900/6809) done. Loss: 2.0804  lr:0.010000
[ Sun Jul 14 14:06:37 2024 ] 
Training: Epoch [1/150], Step [1999], Loss: 3.8305490016937256, Training Accuracy: 27.95625
[ Sun Jul 14 14:06:37 2024 ] 	Batch(2000/6809) done. Loss: 3.2649  lr:0.010000
[ Sun Jul 14 14:07:00 2024 ] 	Batch(2100/6809) done. Loss: 1.9449  lr:0.010000
[ Sun Jul 14 14:07:23 2024 ] 	Batch(2200/6809) done. Loss: 2.2251  lr:0.010000
[ Sun Jul 14 14:07:46 2024 ] 	Batch(2300/6809) done. Loss: 1.6374  lr:0.010000
[ Sun Jul 14 14:08:10 2024 ] 	Batch(2400/6809) done. Loss: 3.0222  lr:0.010000
[ Sun Jul 14 14:08:32 2024 ] 
Training: Epoch [1/150], Step [2499], Loss: 1.785304307937622, Training Accuracy: 28.275
[ Sun Jul 14 14:08:33 2024 ] 	Batch(2500/6809) done. Loss: 2.8056  lr:0.010000
[ Sun Jul 14 14:08:56 2024 ] 	Batch(2600/6809) done. Loss: 3.7785  lr:0.010000
[ Sun Jul 14 14:09:19 2024 ] 	Batch(2700/6809) done. Loss: 3.2918  lr:0.010000
[ Sun Jul 14 14:09:42 2024 ] 	Batch(2800/6809) done. Loss: 1.8316  lr:0.010000
[ Sun Jul 14 14:10:05 2024 ] 	Batch(2900/6809) done. Loss: 1.9487  lr:0.010000
[ Sun Jul 14 14:10:28 2024 ] 
Training: Epoch [1/150], Step [2999], Loss: 1.6696394681930542, Training Accuracy: 28.566666666666666
[ Sun Jul 14 14:10:28 2024 ] 	Batch(3000/6809) done. Loss: 2.2721  lr:0.010000
[ Sun Jul 14 14:10:52 2024 ] 	Batch(3100/6809) done. Loss: 1.7848  lr:0.010000
[ Sun Jul 14 14:11:15 2024 ] 	Batch(3200/6809) done. Loss: 2.9585  lr:0.010000
[ Sun Jul 14 14:11:38 2024 ] 	Batch(3300/6809) done. Loss: 2.9258  lr:0.010000
[ Sun Jul 14 14:12:01 2024 ] 	Batch(3400/6809) done. Loss: 2.5132  lr:0.010000
[ Sun Jul 14 14:12:23 2024 ] 
Training: Epoch [1/150], Step [3499], Loss: 2.551353931427002, Training Accuracy: 29.103571428571428
[ Sun Jul 14 14:12:24 2024 ] 	Batch(3500/6809) done. Loss: 1.3959  lr:0.010000
[ Sun Jul 14 14:12:47 2024 ] 	Batch(3600/6809) done. Loss: 3.0600  lr:0.010000
[ Sun Jul 14 14:13:10 2024 ] 	Batch(3700/6809) done. Loss: 2.6604  lr:0.010000
[ Sun Jul 14 14:13:33 2024 ] 	Batch(3800/6809) done. Loss: 2.5067  lr:0.010000
[ Sun Jul 14 14:13:56 2024 ] 	Batch(3900/6809) done. Loss: 1.8207  lr:0.010000
[ Sun Jul 14 14:14:18 2024 ] 
Training: Epoch [1/150], Step [3999], Loss: 2.879894733428955, Training Accuracy: 29.75
[ Sun Jul 14 14:14:19 2024 ] 	Batch(4000/6809) done. Loss: 2.4215  lr:0.010000
[ Sun Jul 14 14:14:42 2024 ] 	Batch(4100/6809) done. Loss: 2.3957  lr:0.010000
[ Sun Jul 14 14:15:05 2024 ] 	Batch(4200/6809) done. Loss: 1.6107  lr:0.010000
[ Sun Jul 14 14:15:28 2024 ] 	Batch(4300/6809) done. Loss: 2.7574  lr:0.010000
[ Sun Jul 14 14:15:51 2024 ] 	Batch(4400/6809) done. Loss: 1.9318  lr:0.010000
[ Sun Jul 14 14:16:14 2024 ] 
Training: Epoch [1/150], Step [4499], Loss: 2.06302809715271, Training Accuracy: 30.41388888888889
[ Sun Jul 14 14:16:14 2024 ] 	Batch(4500/6809) done. Loss: 1.6303  lr:0.010000
[ Sun Jul 14 14:16:37 2024 ] 	Batch(4600/6809) done. Loss: 2.8016  lr:0.010000
[ Sun Jul 14 14:17:00 2024 ] 	Batch(4700/6809) done. Loss: 1.4106  lr:0.010000
[ Sun Jul 14 14:17:23 2024 ] 	Batch(4800/6809) done. Loss: 1.9019  lr:0.010000
[ Sun Jul 14 14:17:47 2024 ] 	Batch(4900/6809) done. Loss: 2.4398  lr:0.010000
[ Sun Jul 14 14:18:11 2024 ] 
Training: Epoch [1/150], Step [4999], Loss: 2.2614197731018066, Training Accuracy: 30.9375
[ Sun Jul 14 14:18:11 2024 ] 	Batch(5000/6809) done. Loss: 2.8017  lr:0.010000
[ Sun Jul 14 14:18:34 2024 ] 	Batch(5100/6809) done. Loss: 1.9405  lr:0.010000
[ Sun Jul 14 14:18:58 2024 ] 	Batch(5200/6809) done. Loss: 2.1321  lr:0.010000
[ Sun Jul 14 14:19:21 2024 ] 	Batch(5300/6809) done. Loss: 2.1468  lr:0.010000
[ Sun Jul 14 14:19:43 2024 ] 	Batch(5400/6809) done. Loss: 2.0582  lr:0.010000
[ Sun Jul 14 14:20:06 2024 ] 
Training: Epoch [1/150], Step [5499], Loss: 1.8200857639312744, Training Accuracy: 31.590909090909093
[ Sun Jul 14 14:20:06 2024 ] 	Batch(5500/6809) done. Loss: 1.9655  lr:0.010000
[ Sun Jul 14 14:20:29 2024 ] 	Batch(5600/6809) done. Loss: 1.4144  lr:0.010000
[ Sun Jul 14 14:20:51 2024 ] 	Batch(5700/6809) done. Loss: 3.0569  lr:0.010000
[ Sun Jul 14 14:21:14 2024 ] 	Batch(5800/6809) done. Loss: 2.1458  lr:0.010000
[ Sun Jul 14 14:21:37 2024 ] 	Batch(5900/6809) done. Loss: 2.0852  lr:0.010000
[ Sun Jul 14 14:21:59 2024 ] 
Training: Epoch [1/150], Step [5999], Loss: 1.1568247079849243, Training Accuracy: 32.087500000000006
[ Sun Jul 14 14:21:59 2024 ] 	Batch(6000/6809) done. Loss: 2.1342  lr:0.010000
[ Sun Jul 14 14:22:22 2024 ] 	Batch(6100/6809) done. Loss: 2.3968  lr:0.010000
[ Sun Jul 14 14:22:44 2024 ] 	Batch(6200/6809) done. Loss: 0.8557  lr:0.010000
[ Sun Jul 14 14:23:07 2024 ] 	Batch(6300/6809) done. Loss: 1.7462  lr:0.010000
[ Sun Jul 14 14:23:30 2024 ] 	Batch(6400/6809) done. Loss: 1.6543  lr:0.010000
[ Sun Jul 14 14:23:53 2024 ] 
Training: Epoch [1/150], Step [6499], Loss: 1.5915096998214722, Training Accuracy: 32.496153846153845
[ Sun Jul 14 14:23:53 2024 ] 	Batch(6500/6809) done. Loss: 3.2352  lr:0.010000
[ Sun Jul 14 14:24:17 2024 ] 	Batch(6600/6809) done. Loss: 1.7960  lr:0.010000
[ Sun Jul 14 14:24:40 2024 ] 	Batch(6700/6809) done. Loss: 1.3717  lr:0.010000
[ Sun Jul 14 14:25:04 2024 ] 	Batch(6800/6809) done. Loss: 1.9618  lr:0.010000
[ Sun Jul 14 14:25:06 2024 ] 	Mean training loss: 2.4379.
[ Sun Jul 14 14:25:06 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul 14 14:25:06 2024 ] Training epoch: 3
[ Sun Jul 14 14:25:06 2024 ] 	Batch(0/6809) done. Loss: 1.8823  lr:0.010000
[ Sun Jul 14 14:25:29 2024 ] 	Batch(100/6809) done. Loss: 1.4592  lr:0.010000
[ Sun Jul 14 14:25:52 2024 ] 	Batch(200/6809) done. Loss: 2.0268  lr:0.010000
[ Sun Jul 14 14:26:14 2024 ] 	Batch(300/6809) done. Loss: 2.8966  lr:0.010000
[ Sun Jul 14 14:26:37 2024 ] 	Batch(400/6809) done. Loss: 2.4334  lr:0.010000
[ Sun Jul 14 14:27:00 2024 ] 
Training: Epoch [2/150], Step [499], Loss: 1.5988481044769287, Training Accuracy: 42.125
[ Sun Jul 14 14:27:01 2024 ] 	Batch(500/6809) done. Loss: 2.3385  lr:0.010000
[ Sun Jul 14 14:27:23 2024 ] 	Batch(600/6809) done. Loss: 2.3452  lr:0.010000
[ Sun Jul 14 14:27:46 2024 ] 	Batch(700/6809) done. Loss: 2.4011  lr:0.010000
[ Sun Jul 14 14:28:09 2024 ] 	Batch(800/6809) done. Loss: 2.0105  lr:0.010000
[ Sun Jul 14 14:28:32 2024 ] 	Batch(900/6809) done. Loss: 1.4693  lr:0.010000
[ Sun Jul 14 14:28:54 2024 ] 
Training: Epoch [2/150], Step [999], Loss: 2.7870078086853027, Training Accuracy: 41.6875
[ Sun Jul 14 14:28:54 2024 ] 	Batch(1000/6809) done. Loss: 2.0126  lr:0.010000
[ Sun Jul 14 14:29:17 2024 ] 	Batch(1100/6809) done. Loss: 1.6735  lr:0.010000
[ Sun Jul 14 14:29:40 2024 ] 	Batch(1200/6809) done. Loss: 2.2477  lr:0.010000
[ Sun Jul 14 14:30:02 2024 ] 	Batch(1300/6809) done. Loss: 2.2535  lr:0.010000
[ Sun Jul 14 14:30:25 2024 ] 	Batch(1400/6809) done. Loss: 1.5426  lr:0.010000
[ Sun Jul 14 14:30:48 2024 ] 
Training: Epoch [2/150], Step [1499], Loss: 1.5383422374725342, Training Accuracy: 42.25
[ Sun Jul 14 14:30:48 2024 ] 	Batch(1500/6809) done. Loss: 1.7433  lr:0.010000
[ Sun Jul 14 14:31:11 2024 ] 	Batch(1600/6809) done. Loss: 1.7081  lr:0.010000
[ Sun Jul 14 14:31:33 2024 ] 	Batch(1700/6809) done. Loss: 1.9634  lr:0.010000
[ Sun Jul 14 14:31:56 2024 ] 	Batch(1800/6809) done. Loss: 2.0221  lr:0.010000
[ Sun Jul 14 14:32:19 2024 ] 	Batch(1900/6809) done. Loss: 1.3249  lr:0.010000
[ Sun Jul 14 14:32:41 2024 ] 
Training: Epoch [2/150], Step [1999], Loss: 1.6328043937683105, Training Accuracy: 42.2375
[ Sun Jul 14 14:32:42 2024 ] 	Batch(2000/6809) done. Loss: 2.3289  lr:0.010000
[ Sun Jul 14 14:33:04 2024 ] 	Batch(2100/6809) done. Loss: 2.0214  lr:0.010000
[ Sun Jul 14 14:33:27 2024 ] 	Batch(2200/6809) done. Loss: 1.3831  lr:0.010000
[ Sun Jul 14 14:33:50 2024 ] 	Batch(2300/6809) done. Loss: 1.9946  lr:0.010000
[ Sun Jul 14 14:34:13 2024 ] 	Batch(2400/6809) done. Loss: 2.0459  lr:0.010000
[ Sun Jul 14 14:34:36 2024 ] 
Training: Epoch [2/150], Step [2499], Loss: 1.0092803239822388, Training Accuracy: 42.515
[ Sun Jul 14 14:34:36 2024 ] 	Batch(2500/6809) done. Loss: 2.9463  lr:0.010000
[ Sun Jul 14 14:34:59 2024 ] 	Batch(2600/6809) done. Loss: 2.3216  lr:0.010000
[ Sun Jul 14 14:35:22 2024 ] 	Batch(2700/6809) done. Loss: 0.9271  lr:0.010000
[ Sun Jul 14 14:35:45 2024 ] 	Batch(2800/6809) done. Loss: 1.9428  lr:0.010000
[ Sun Jul 14 14:36:08 2024 ] 	Batch(2900/6809) done. Loss: 1.9093  lr:0.010000
[ Sun Jul 14 14:36:31 2024 ] 
Training: Epoch [2/150], Step [2999], Loss: 2.6797850131988525, Training Accuracy: 42.59166666666667
[ Sun Jul 14 14:36:32 2024 ] 	Batch(3000/6809) done. Loss: 1.2011  lr:0.010000
[ Sun Jul 14 14:36:55 2024 ] 	Batch(3100/6809) done. Loss: 1.5786  lr:0.010000
[ Sun Jul 14 14:37:18 2024 ] 	Batch(3200/6809) done. Loss: 1.9512  lr:0.010000
[ Sun Jul 14 14:37:41 2024 ] 	Batch(3300/6809) done. Loss: 1.4752  lr:0.010000
[ Sun Jul 14 14:38:04 2024 ] 	Batch(3400/6809) done. Loss: 3.1809  lr:0.010000
[ Sun Jul 14 14:38:27 2024 ] 
Training: Epoch [2/150], Step [3499], Loss: 2.024019241333008, Training Accuracy: 43.25714285714285
[ Sun Jul 14 14:38:27 2024 ] 	Batch(3500/6809) done. Loss: 2.0142  lr:0.010000
[ Sun Jul 14 14:38:50 2024 ] 	Batch(3600/6809) done. Loss: 2.7579  lr:0.010000
[ Sun Jul 14 14:39:14 2024 ] 	Batch(3700/6809) done. Loss: 1.7384  lr:0.010000
[ Sun Jul 14 14:39:37 2024 ] 	Batch(3800/6809) done. Loss: 2.0819  lr:0.010000
[ Sun Jul 14 14:40:00 2024 ] 	Batch(3900/6809) done. Loss: 2.1539  lr:0.010000
[ Sun Jul 14 14:40:23 2024 ] 
Training: Epoch [2/150], Step [3999], Loss: 1.64456307888031, Training Accuracy: 43.390625
[ Sun Jul 14 14:40:23 2024 ] 	Batch(4000/6809) done. Loss: 1.8703  lr:0.010000
[ Sun Jul 14 14:40:46 2024 ] 	Batch(4100/6809) done. Loss: 1.9188  lr:0.010000
[ Sun Jul 14 14:41:09 2024 ] 	Batch(4200/6809) done. Loss: 1.3740  lr:0.010000
[ Sun Jul 14 14:41:33 2024 ] 	Batch(4300/6809) done. Loss: 1.0778  lr:0.010000
[ Sun Jul 14 14:41:56 2024 ] 	Batch(4400/6809) done. Loss: 1.1367  lr:0.010000
[ Sun Jul 14 14:42:19 2024 ] 
Training: Epoch [2/150], Step [4499], Loss: 1.324413537979126, Training Accuracy: 43.644444444444446
[ Sun Jul 14 14:42:19 2024 ] 	Batch(4500/6809) done. Loss: 2.6625  lr:0.010000
[ Sun Jul 14 14:42:42 2024 ] 	Batch(4600/6809) done. Loss: 0.4928  lr:0.010000
[ Sun Jul 14 14:43:05 2024 ] 	Batch(4700/6809) done. Loss: 1.7191  lr:0.010000
[ Sun Jul 14 14:43:29 2024 ] 	Batch(4800/6809) done. Loss: 1.6307  lr:0.010000
[ Sun Jul 14 14:43:52 2024 ] 	Batch(4900/6809) done. Loss: 2.4109  lr:0.010000
[ Sun Jul 14 14:44:15 2024 ] 
Training: Epoch [2/150], Step [4999], Loss: 1.8505339622497559, Training Accuracy: 43.9075
[ Sun Jul 14 14:44:15 2024 ] 	Batch(5000/6809) done. Loss: 1.1586  lr:0.010000
[ Sun Jul 14 14:44:38 2024 ] 	Batch(5100/6809) done. Loss: 2.4345  lr:0.010000
[ Sun Jul 14 14:45:01 2024 ] 	Batch(5200/6809) done. Loss: 2.8391  lr:0.010000
[ Sun Jul 14 14:45:24 2024 ] 	Batch(5300/6809) done. Loss: 1.9326  lr:0.010000
[ Sun Jul 14 14:45:47 2024 ] 	Batch(5400/6809) done. Loss: 0.9291  lr:0.010000
[ Sun Jul 14 14:46:10 2024 ] 
Training: Epoch [2/150], Step [5499], Loss: 1.8200700283050537, Training Accuracy: 44.215909090909086
[ Sun Jul 14 14:46:10 2024 ] 	Batch(5500/6809) done. Loss: 1.4323  lr:0.010000
[ Sun Jul 14 14:46:34 2024 ] 	Batch(5600/6809) done. Loss: 2.5151  lr:0.010000
[ Sun Jul 14 14:46:57 2024 ] 	Batch(5700/6809) done. Loss: 1.7823  lr:0.010000
[ Sun Jul 14 14:47:20 2024 ] 	Batch(5800/6809) done. Loss: 3.2192  lr:0.010000
[ Sun Jul 14 14:47:43 2024 ] 	Batch(5900/6809) done. Loss: 2.3330  lr:0.010000
[ Sun Jul 14 14:48:05 2024 ] 
Training: Epoch [2/150], Step [5999], Loss: 1.7413127422332764, Training Accuracy: 44.47291666666667
[ Sun Jul 14 14:48:05 2024 ] 	Batch(6000/6809) done. Loss: 0.8392  lr:0.010000
[ Sun Jul 14 14:48:28 2024 ] 	Batch(6100/6809) done. Loss: 1.7900  lr:0.010000
[ Sun Jul 14 14:48:51 2024 ] 	Batch(6200/6809) done. Loss: 2.1399  lr:0.010000
[ Sun Jul 14 14:49:13 2024 ] 	Batch(6300/6809) done. Loss: 2.3884  lr:0.010000
[ Sun Jul 14 14:49:36 2024 ] 	Batch(6400/6809) done. Loss: 2.4122  lr:0.010000
[ Sun Jul 14 14:49:59 2024 ] 
Training: Epoch [2/150], Step [6499], Loss: 1.3623881340026855, Training Accuracy: 44.68269230769231
[ Sun Jul 14 14:49:59 2024 ] 	Batch(6500/6809) done. Loss: 1.9146  lr:0.010000
[ Sun Jul 14 14:50:22 2024 ] 	Batch(6600/6809) done. Loss: 2.3289  lr:0.010000
[ Sun Jul 14 14:50:45 2024 ] 	Batch(6700/6809) done. Loss: 1.2330  lr:0.010000
[ Sun Jul 14 14:51:07 2024 ] 	Batch(6800/6809) done. Loss: 2.4896  lr:0.010000
[ Sun Jul 14 14:51:09 2024 ] 	Mean training loss: 1.9595.
[ Sun Jul 14 14:51:09 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul 14 14:51:09 2024 ] Training epoch: 4
[ Sun Jul 14 14:51:10 2024 ] 	Batch(0/6809) done. Loss: 1.8428  lr:0.010000
[ Sun Jul 14 14:51:33 2024 ] 	Batch(100/6809) done. Loss: 2.1050  lr:0.010000
[ Sun Jul 14 14:51:56 2024 ] 	Batch(200/6809) done. Loss: 2.5000  lr:0.010000
[ Sun Jul 14 14:52:19 2024 ] 	Batch(300/6809) done. Loss: 1.7356  lr:0.010000
[ Sun Jul 14 14:52:41 2024 ] 	Batch(400/6809) done. Loss: 2.0168  lr:0.010000
[ Sun Jul 14 14:53:04 2024 ] 
Training: Epoch [3/150], Step [499], Loss: 2.8296778202056885, Training Accuracy: 48.4
[ Sun Jul 14 14:53:04 2024 ] 	Batch(500/6809) done. Loss: 1.7001  lr:0.010000
[ Sun Jul 14 14:53:27 2024 ] 	Batch(600/6809) done. Loss: 1.5724  lr:0.010000
[ Sun Jul 14 14:53:49 2024 ] 	Batch(700/6809) done. Loss: 1.3862  lr:0.010000
[ Sun Jul 14 14:54:13 2024 ] 	Batch(800/6809) done. Loss: 1.4749  lr:0.010000
[ Sun Jul 14 14:54:36 2024 ] 	Batch(900/6809) done. Loss: 1.4075  lr:0.010000
[ Sun Jul 14 14:55:00 2024 ] 
Training: Epoch [3/150], Step [999], Loss: 1.4886302947998047, Training Accuracy: 49.512499999999996
[ Sun Jul 14 14:55:00 2024 ] 	Batch(1000/6809) done. Loss: 1.2775  lr:0.010000
[ Sun Jul 14 14:55:23 2024 ] 	Batch(1100/6809) done. Loss: 1.9626  lr:0.010000
[ Sun Jul 14 14:55:47 2024 ] 	Batch(1200/6809) done. Loss: 1.6724  lr:0.010000
[ Sun Jul 14 14:56:11 2024 ] 	Batch(1300/6809) done. Loss: 2.6299  lr:0.010000
[ Sun Jul 14 14:56:33 2024 ] 	Batch(1400/6809) done. Loss: 1.9415  lr:0.010000
[ Sun Jul 14 14:56:56 2024 ] 
Training: Epoch [3/150], Step [1499], Loss: 2.3510241508483887, Training Accuracy: 49.825
[ Sun Jul 14 14:56:56 2024 ] 	Batch(1500/6809) done. Loss: 1.8619  lr:0.010000
[ Sun Jul 14 14:57:19 2024 ] 	Batch(1600/6809) done. Loss: 1.7995  lr:0.010000
[ Sun Jul 14 14:57:42 2024 ] 	Batch(1700/6809) done. Loss: 1.5339  lr:0.010000
[ Sun Jul 14 14:58:05 2024 ] 	Batch(1800/6809) done. Loss: 1.6849  lr:0.010000
[ Sun Jul 14 14:58:29 2024 ] 	Batch(1900/6809) done. Loss: 2.6910  lr:0.010000
[ Sun Jul 14 14:58:52 2024 ] 
Training: Epoch [3/150], Step [1999], Loss: 1.5756641626358032, Training Accuracy: 50.275000000000006
[ Sun Jul 14 14:58:52 2024 ] 	Batch(2000/6809) done. Loss: 1.6859  lr:0.010000
[ Sun Jul 14 14:59:16 2024 ] 	Batch(2100/6809) done. Loss: 2.0792  lr:0.010000
[ Sun Jul 14 14:59:39 2024 ] 	Batch(2200/6809) done. Loss: 1.6716  lr:0.010000
[ Sun Jul 14 15:00:02 2024 ] 	Batch(2300/6809) done. Loss: 0.6698  lr:0.010000
[ Sun Jul 14 15:00:26 2024 ] 	Batch(2400/6809) done. Loss: 1.2346  lr:0.010000
[ Sun Jul 14 15:00:49 2024 ] 
Training: Epoch [3/150], Step [2499], Loss: 0.8052030205726624, Training Accuracy: 50.31
[ Sun Jul 14 15:00:49 2024 ] 	Batch(2500/6809) done. Loss: 1.5881  lr:0.010000
[ Sun Jul 14 15:01:13 2024 ] 	Batch(2600/6809) done. Loss: 1.5240  lr:0.010000
[ Sun Jul 14 15:01:35 2024 ] 	Batch(2700/6809) done. Loss: 1.2209  lr:0.010000
[ Sun Jul 14 15:01:58 2024 ] 	Batch(2800/6809) done. Loss: 1.0591  lr:0.010000
[ Sun Jul 14 15:02:21 2024 ] 	Batch(2900/6809) done. Loss: 1.7022  lr:0.010000
[ Sun Jul 14 15:02:43 2024 ] 
Training: Epoch [3/150], Step [2999], Loss: 2.1296072006225586, Training Accuracy: 50.42916666666667
[ Sun Jul 14 15:02:43 2024 ] 	Batch(3000/6809) done. Loss: 1.3802  lr:0.010000
[ Sun Jul 14 15:03:06 2024 ] 	Batch(3100/6809) done. Loss: 1.4541  lr:0.010000
[ Sun Jul 14 15:03:28 2024 ] 	Batch(3200/6809) done. Loss: 2.9485  lr:0.010000
[ Sun Jul 14 15:03:51 2024 ] 	Batch(3300/6809) done. Loss: 2.4264  lr:0.010000
[ Sun Jul 14 15:04:14 2024 ] 	Batch(3400/6809) done. Loss: 1.4989  lr:0.010000
[ Sun Jul 14 15:04:36 2024 ] 
Training: Epoch [3/150], Step [3499], Loss: 2.011953592300415, Training Accuracy: 50.39642857142857
[ Sun Jul 14 15:04:36 2024 ] 	Batch(3500/6809) done. Loss: 2.2610  lr:0.010000
[ Sun Jul 14 15:04:59 2024 ] 	Batch(3600/6809) done. Loss: 1.8395  lr:0.010000
[ Sun Jul 14 15:05:22 2024 ] 	Batch(3700/6809) done. Loss: 1.0614  lr:0.010000
[ Sun Jul 14 15:05:44 2024 ] 	Batch(3800/6809) done. Loss: 2.4116  lr:0.010000
[ Sun Jul 14 15:06:07 2024 ] 	Batch(3900/6809) done. Loss: 0.4844  lr:0.010000
[ Sun Jul 14 15:06:29 2024 ] 
Training: Epoch [3/150], Step [3999], Loss: 1.8060694932937622, Training Accuracy: 50.525
[ Sun Jul 14 15:06:29 2024 ] 	Batch(4000/6809) done. Loss: 1.9532  lr:0.010000
[ Sun Jul 14 15:06:52 2024 ] 	Batch(4100/6809) done. Loss: 0.8838  lr:0.010000
[ Sun Jul 14 15:07:15 2024 ] 	Batch(4200/6809) done. Loss: 2.1947  lr:0.010000
[ Sun Jul 14 15:07:37 2024 ] 	Batch(4300/6809) done. Loss: 1.8977  lr:0.010000
[ Sun Jul 14 15:08:00 2024 ] 	Batch(4400/6809) done. Loss: 2.3710  lr:0.010000
[ Sun Jul 14 15:08:23 2024 ] 
Training: Epoch [3/150], Step [4499], Loss: 1.316411018371582, Training Accuracy: 50.49722222222223
[ Sun Jul 14 15:08:23 2024 ] 	Batch(4500/6809) done. Loss: 2.8081  lr:0.010000
[ Sun Jul 14 15:08:45 2024 ] 	Batch(4600/6809) done. Loss: 1.9931  lr:0.010000
[ Sun Jul 14 15:09:08 2024 ] 	Batch(4700/6809) done. Loss: 1.5025  lr:0.010000
[ Sun Jul 14 15:09:31 2024 ] 	Batch(4800/6809) done. Loss: 2.4385  lr:0.010000
[ Sun Jul 14 15:09:54 2024 ] 	Batch(4900/6809) done. Loss: 1.1610  lr:0.010000
[ Sun Jul 14 15:10:17 2024 ] 
Training: Epoch [3/150], Step [4999], Loss: 2.8011152744293213, Training Accuracy: 50.612500000000004
[ Sun Jul 14 15:10:17 2024 ] 	Batch(5000/6809) done. Loss: 1.2066  lr:0.010000
[ Sun Jul 14 15:10:40 2024 ] 	Batch(5100/6809) done. Loss: 2.0645  lr:0.010000
[ Sun Jul 14 15:11:02 2024 ] 	Batch(5200/6809) done. Loss: 1.3842  lr:0.010000
[ Sun Jul 14 15:11:25 2024 ] 	Batch(5300/6809) done. Loss: 0.9481  lr:0.010000
[ Sun Jul 14 15:11:47 2024 ] 	Batch(5400/6809) done. Loss: 0.8622  lr:0.010000
[ Sun Jul 14 15:12:10 2024 ] 
Training: Epoch [3/150], Step [5499], Loss: 2.0914714336395264, Training Accuracy: 50.90909090909091
[ Sun Jul 14 15:12:10 2024 ] 	Batch(5500/6809) done. Loss: 1.5325  lr:0.010000
[ Sun Jul 14 15:12:33 2024 ] 	Batch(5600/6809) done. Loss: 0.4890  lr:0.010000
[ Sun Jul 14 15:12:56 2024 ] 	Batch(5700/6809) done. Loss: 2.3005  lr:0.010000
[ Sun Jul 14 15:13:18 2024 ] 	Batch(5800/6809) done. Loss: 1.9028  lr:0.010000
[ Sun Jul 14 15:13:41 2024 ] 	Batch(5900/6809) done. Loss: 2.3123  lr:0.010000
[ Sun Jul 14 15:14:03 2024 ] 
Training: Epoch [3/150], Step [5999], Loss: 2.851689338684082, Training Accuracy: 50.99583333333333
[ Sun Jul 14 15:14:04 2024 ] 	Batch(6000/6809) done. Loss: 1.6639  lr:0.010000
[ Sun Jul 14 15:14:26 2024 ] 	Batch(6100/6809) done. Loss: 1.0263  lr:0.010000
[ Sun Jul 14 15:14:49 2024 ] 	Batch(6200/6809) done. Loss: 1.2850  lr:0.010000
[ Sun Jul 14 15:15:11 2024 ] 	Batch(6300/6809) done. Loss: 1.3626  lr:0.010000
[ Sun Jul 14 15:15:35 2024 ] 	Batch(6400/6809) done. Loss: 1.8202  lr:0.010000
[ Sun Jul 14 15:15:58 2024 ] 
Training: Epoch [3/150], Step [6499], Loss: 1.6012976169586182, Training Accuracy: 51.07307692307692
[ Sun Jul 14 15:15:58 2024 ] 	Batch(6500/6809) done. Loss: 1.4596  lr:0.010000
[ Sun Jul 14 15:16:21 2024 ] 	Batch(6600/6809) done. Loss: 2.4502  lr:0.010000
[ Sun Jul 14 15:16:43 2024 ] 	Batch(6700/6809) done. Loss: 1.8940  lr:0.010000
[ Sun Jul 14 15:17:06 2024 ] 	Batch(6800/6809) done. Loss: 1.5097  lr:0.010000
[ Sun Jul 14 15:17:08 2024 ] 	Mean training loss: 1.7208.
[ Sun Jul 14 15:17:08 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 15:17:08 2024 ] Training epoch: 5
[ Sun Jul 14 15:17:09 2024 ] 	Batch(0/6809) done. Loss: 1.8444  lr:0.010000
[ Sun Jul 14 15:17:32 2024 ] 	Batch(100/6809) done. Loss: 1.0599  lr:0.010000
[ Sun Jul 14 15:17:55 2024 ] 	Batch(200/6809) done. Loss: 2.7585  lr:0.010000
[ Sun Jul 14 15:18:18 2024 ] 	Batch(300/6809) done. Loss: 1.7361  lr:0.010000
[ Sun Jul 14 15:18:41 2024 ] 	Batch(400/6809) done. Loss: 2.1876  lr:0.010000
[ Sun Jul 14 15:19:04 2024 ] 
Training: Epoch [4/150], Step [499], Loss: 1.2004203796386719, Training Accuracy: 53.300000000000004
[ Sun Jul 14 15:19:04 2024 ] 	Batch(500/6809) done. Loss: 1.2101  lr:0.010000
[ Sun Jul 14 15:19:27 2024 ] 	Batch(600/6809) done. Loss: 2.1895  lr:0.010000
[ Sun Jul 14 15:19:50 2024 ] 	Batch(700/6809) done. Loss: 2.5758  lr:0.010000
[ Sun Jul 14 15:20:14 2024 ] 	Batch(800/6809) done. Loss: 2.4002  lr:0.010000
[ Sun Jul 14 15:20:36 2024 ] 	Batch(900/6809) done. Loss: 2.0456  lr:0.010000
[ Sun Jul 14 15:20:59 2024 ] 
Training: Epoch [4/150], Step [999], Loss: 1.1617906093597412, Training Accuracy: 53.075
[ Sun Jul 14 15:20:59 2024 ] 	Batch(1000/6809) done. Loss: 1.3051  lr:0.010000
[ Sun Jul 14 15:21:22 2024 ] 	Batch(1100/6809) done. Loss: 0.9267  lr:0.010000
[ Sun Jul 14 15:21:45 2024 ] 	Batch(1200/6809) done. Loss: 1.2230  lr:0.010000
[ Sun Jul 14 15:22:07 2024 ] 	Batch(1300/6809) done. Loss: 1.1603  lr:0.010000
[ Sun Jul 14 15:22:30 2024 ] 	Batch(1400/6809) done. Loss: 1.7928  lr:0.010000
[ Sun Jul 14 15:22:52 2024 ] 
Training: Epoch [4/150], Step [1499], Loss: 1.518712043762207, Training Accuracy: 53.80833333333334
[ Sun Jul 14 15:22:52 2024 ] 	Batch(1500/6809) done. Loss: 2.3886  lr:0.010000
[ Sun Jul 14 15:23:16 2024 ] 	Batch(1600/6809) done. Loss: 2.1399  lr:0.010000
[ Sun Jul 14 15:23:39 2024 ] 	Batch(1700/6809) done. Loss: 2.2507  lr:0.010000
[ Sun Jul 14 15:24:02 2024 ] 	Batch(1800/6809) done. Loss: 1.2657  lr:0.010000
[ Sun Jul 14 15:24:25 2024 ] 	Batch(1900/6809) done. Loss: 1.8967  lr:0.010000
[ Sun Jul 14 15:24:47 2024 ] 
Training: Epoch [4/150], Step [1999], Loss: 0.9590273499488831, Training Accuracy: 54.231249999999996
[ Sun Jul 14 15:24:48 2024 ] 	Batch(2000/6809) done. Loss: 0.9011  lr:0.010000
[ Sun Jul 14 15:25:11 2024 ] 	Batch(2100/6809) done. Loss: 1.2062  lr:0.010000
[ Sun Jul 14 15:25:34 2024 ] 	Batch(2200/6809) done. Loss: 2.1746  lr:0.010000
[ Sun Jul 14 15:25:57 2024 ] 	Batch(2300/6809) done. Loss: 0.5386  lr:0.010000
[ Sun Jul 14 15:26:20 2024 ] 	Batch(2400/6809) done. Loss: 1.3437  lr:0.010000
[ Sun Jul 14 15:26:43 2024 ] 
Training: Epoch [4/150], Step [2499], Loss: 1.7734049558639526, Training Accuracy: 54.425000000000004
[ Sun Jul 14 15:26:43 2024 ] 	Batch(2500/6809) done. Loss: 1.9417  lr:0.010000
[ Sun Jul 14 15:27:06 2024 ] 	Batch(2600/6809) done. Loss: 1.1816  lr:0.010000
[ Sun Jul 14 15:27:29 2024 ] 	Batch(2700/6809) done. Loss: 1.5195  lr:0.010000
[ Sun Jul 14 15:27:52 2024 ] 	Batch(2800/6809) done. Loss: 2.7614  lr:0.010000
[ Sun Jul 14 15:28:15 2024 ] 	Batch(2900/6809) done. Loss: 1.1314  lr:0.010000
[ Sun Jul 14 15:28:38 2024 ] 
Training: Epoch [4/150], Step [2999], Loss: 1.8822616338729858, Training Accuracy: 54.37916666666667
[ Sun Jul 14 15:28:38 2024 ] 	Batch(3000/6809) done. Loss: 1.6733  lr:0.010000
[ Sun Jul 14 15:29:01 2024 ] 	Batch(3100/6809) done. Loss: 0.7858  lr:0.010000
[ Sun Jul 14 15:29:23 2024 ] 	Batch(3200/6809) done. Loss: 2.1017  lr:0.010000
[ Sun Jul 14 15:29:46 2024 ] 	Batch(3300/6809) done. Loss: 1.2249  lr:0.010000
[ Sun Jul 14 15:30:09 2024 ] 	Batch(3400/6809) done. Loss: 1.6111  lr:0.010000
[ Sun Jul 14 15:30:31 2024 ] 
Training: Epoch [4/150], Step [3499], Loss: 1.6444941759109497, Training Accuracy: 54.589285714285715
[ Sun Jul 14 15:30:31 2024 ] 	Batch(3500/6809) done. Loss: 1.2928  lr:0.010000
[ Sun Jul 14 15:30:54 2024 ] 	Batch(3600/6809) done. Loss: 1.8685  lr:0.010000
[ Sun Jul 14 15:31:18 2024 ] 	Batch(3700/6809) done. Loss: 0.9266  lr:0.010000
[ Sun Jul 14 15:31:41 2024 ] 	Batch(3800/6809) done. Loss: 2.0413  lr:0.010000
[ Sun Jul 14 15:32:05 2024 ] 	Batch(3900/6809) done. Loss: 1.7008  lr:0.010000
[ Sun Jul 14 15:32:28 2024 ] 
Training: Epoch [4/150], Step [3999], Loss: 1.3320071697235107, Training Accuracy: 54.7625
[ Sun Jul 14 15:32:29 2024 ] 	Batch(4000/6809) done. Loss: 2.1432  lr:0.010000
[ Sun Jul 14 15:32:52 2024 ] 	Batch(4100/6809) done. Loss: 0.9354  lr:0.010000
[ Sun Jul 14 15:33:15 2024 ] 	Batch(4200/6809) done. Loss: 0.7566  lr:0.010000
[ Sun Jul 14 15:33:38 2024 ] 	Batch(4300/6809) done. Loss: 1.6755  lr:0.010000
[ Sun Jul 14 15:34:01 2024 ] 	Batch(4400/6809) done. Loss: 1.5451  lr:0.010000
[ Sun Jul 14 15:34:24 2024 ] 
Training: Epoch [4/150], Step [4499], Loss: 1.3141233921051025, Training Accuracy: 54.96666666666666
[ Sun Jul 14 15:34:24 2024 ] 	Batch(4500/6809) done. Loss: 0.9178  lr:0.010000
[ Sun Jul 14 15:34:47 2024 ] 	Batch(4600/6809) done. Loss: 2.5115  lr:0.010000
[ Sun Jul 14 15:35:11 2024 ] 	Batch(4700/6809) done. Loss: 1.0706  lr:0.010000
[ Sun Jul 14 15:35:34 2024 ] 	Batch(4800/6809) done. Loss: 2.4745  lr:0.010000
[ Sun Jul 14 15:35:57 2024 ] 	Batch(4900/6809) done. Loss: 1.9309  lr:0.010000
[ Sun Jul 14 15:36:20 2024 ] 
Training: Epoch [4/150], Step [4999], Loss: 1.5772924423217773, Training Accuracy: 55.144999999999996
[ Sun Jul 14 15:36:20 2024 ] 	Batch(5000/6809) done. Loss: 1.6810  lr:0.010000
[ Sun Jul 14 15:36:43 2024 ] 	Batch(5100/6809) done. Loss: 1.0744  lr:0.010000
[ Sun Jul 14 15:37:06 2024 ] 	Batch(5200/6809) done. Loss: 1.7594  lr:0.010000
[ Sun Jul 14 15:37:30 2024 ] 	Batch(5300/6809) done. Loss: 1.2648  lr:0.010000
[ Sun Jul 14 15:37:53 2024 ] 	Batch(5400/6809) done. Loss: 2.1473  lr:0.010000
[ Sun Jul 14 15:38:16 2024 ] 
Training: Epoch [4/150], Step [5499], Loss: 2.06135892868042, Training Accuracy: 55.31363636363636
[ Sun Jul 14 15:38:16 2024 ] 	Batch(5500/6809) done. Loss: 1.1066  lr:0.010000
[ Sun Jul 14 15:38:40 2024 ] 	Batch(5600/6809) done. Loss: 1.8973  lr:0.010000
[ Sun Jul 14 15:39:03 2024 ] 	Batch(5700/6809) done. Loss: 1.9046  lr:0.010000
[ Sun Jul 14 15:39:26 2024 ] 	Batch(5800/6809) done. Loss: 1.8172  lr:0.010000
[ Sun Jul 14 15:39:49 2024 ] 	Batch(5900/6809) done. Loss: 1.2770  lr:0.010000
[ Sun Jul 14 15:40:11 2024 ] 
Training: Epoch [4/150], Step [5999], Loss: 1.0696967840194702, Training Accuracy: 55.362500000000004
[ Sun Jul 14 15:40:12 2024 ] 	Batch(6000/6809) done. Loss: 2.2687  lr:0.010000
[ Sun Jul 14 15:40:35 2024 ] 	Batch(6100/6809) done. Loss: 1.6436  lr:0.010000
[ Sun Jul 14 15:40:58 2024 ] 	Batch(6200/6809) done. Loss: 1.5589  lr:0.010000
[ Sun Jul 14 15:41:20 2024 ] 	Batch(6300/6809) done. Loss: 0.6209  lr:0.010000
[ Sun Jul 14 15:41:43 2024 ] 	Batch(6400/6809) done. Loss: 1.1013  lr:0.010000
[ Sun Jul 14 15:42:05 2024 ] 
Training: Epoch [4/150], Step [6499], Loss: 1.9200868606567383, Training Accuracy: 55.517307692307696
[ Sun Jul 14 15:42:05 2024 ] 	Batch(6500/6809) done. Loss: 1.5293  lr:0.010000
[ Sun Jul 14 15:42:28 2024 ] 	Batch(6600/6809) done. Loss: 0.3771  lr:0.010000
[ Sun Jul 14 15:42:51 2024 ] 	Batch(6700/6809) done. Loss: 1.8479  lr:0.010000
[ Sun Jul 14 15:43:13 2024 ] 	Batch(6800/6809) done. Loss: 0.9750  lr:0.010000
[ Sun Jul 14 15:43:15 2024 ] 	Mean training loss: 1.5586.
[ Sun Jul 14 15:43:15 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul 14 15:43:15 2024 ] Training epoch: 6
[ Sun Jul 14 15:43:16 2024 ] 	Batch(0/6809) done. Loss: 2.6839  lr:0.010000
[ Sun Jul 14 15:43:39 2024 ] 	Batch(100/6809) done. Loss: 1.7705  lr:0.010000
[ Sun Jul 14 15:44:01 2024 ] 	Batch(200/6809) done. Loss: 1.3484  lr:0.010000
[ Sun Jul 14 15:44:25 2024 ] 	Batch(300/6809) done. Loss: 1.5650  lr:0.010000
[ Sun Jul 14 15:44:48 2024 ] 	Batch(400/6809) done. Loss: 2.3521  lr:0.010000
[ Sun Jul 14 15:45:10 2024 ] 
Training: Epoch [5/150], Step [499], Loss: 1.982977032661438, Training Accuracy: 57.775
[ Sun Jul 14 15:45:10 2024 ] 	Batch(500/6809) done. Loss: 1.5959  lr:0.010000
[ Sun Jul 14 15:45:33 2024 ] 	Batch(600/6809) done. Loss: 2.9540  lr:0.010000
[ Sun Jul 14 15:45:55 2024 ] 	Batch(700/6809) done. Loss: 0.7891  lr:0.010000
[ Sun Jul 14 15:46:18 2024 ] 	Batch(800/6809) done. Loss: 2.7341  lr:0.010000
[ Sun Jul 14 15:46:41 2024 ] 	Batch(900/6809) done. Loss: 1.0955  lr:0.010000
[ Sun Jul 14 15:47:03 2024 ] 
Training: Epoch [5/150], Step [999], Loss: 1.0018198490142822, Training Accuracy: 57.99999999999999
[ Sun Jul 14 15:47:03 2024 ] 	Batch(1000/6809) done. Loss: 0.4079  lr:0.010000
[ Sun Jul 14 15:47:26 2024 ] 	Batch(1100/6809) done. Loss: 1.2726  lr:0.010000
[ Sun Jul 14 15:47:48 2024 ] 	Batch(1200/6809) done. Loss: 0.4768  lr:0.010000
[ Sun Jul 14 15:48:12 2024 ] 	Batch(1300/6809) done. Loss: 1.0441  lr:0.010000
[ Sun Jul 14 15:48:35 2024 ] 	Batch(1400/6809) done. Loss: 0.9365  lr:0.010000
[ Sun Jul 14 15:48:58 2024 ] 
Training: Epoch [5/150], Step [1499], Loss: 2.355376958847046, Training Accuracy: 58.275
[ Sun Jul 14 15:48:58 2024 ] 	Batch(1500/6809) done. Loss: 1.0743  lr:0.010000
[ Sun Jul 14 15:49:21 2024 ] 	Batch(1600/6809) done. Loss: 0.9232  lr:0.010000
[ Sun Jul 14 15:49:44 2024 ] 	Batch(1700/6809) done. Loss: 1.4249  lr:0.010000
[ Sun Jul 14 15:50:07 2024 ] 	Batch(1800/6809) done. Loss: 1.5305  lr:0.010000
[ Sun Jul 14 15:50:31 2024 ] 	Batch(1900/6809) done. Loss: 1.5455  lr:0.010000
[ Sun Jul 14 15:50:53 2024 ] 
Training: Epoch [5/150], Step [1999], Loss: 3.1645965576171875, Training Accuracy: 57.94375000000001
[ Sun Jul 14 15:50:54 2024 ] 	Batch(2000/6809) done. Loss: 1.7603  lr:0.010000
[ Sun Jul 14 15:51:17 2024 ] 	Batch(2100/6809) done. Loss: 1.0502  lr:0.010000
[ Sun Jul 14 15:51:40 2024 ] 	Batch(2200/6809) done. Loss: 0.8429  lr:0.010000
[ Sun Jul 14 15:52:03 2024 ] 	Batch(2300/6809) done. Loss: 1.1559  lr:0.010000
[ Sun Jul 14 15:52:27 2024 ] 	Batch(2400/6809) done. Loss: 0.7065  lr:0.010000
[ Sun Jul 14 15:52:49 2024 ] 
Training: Epoch [5/150], Step [2499], Loss: 0.8894557356834412, Training Accuracy: 57.825
[ Sun Jul 14 15:52:50 2024 ] 	Batch(2500/6809) done. Loss: 1.4025  lr:0.010000
[ Sun Jul 14 15:53:13 2024 ] 	Batch(2600/6809) done. Loss: 1.5268  lr:0.010000
[ Sun Jul 14 15:53:36 2024 ] 	Batch(2700/6809) done. Loss: 1.5578  lr:0.010000
[ Sun Jul 14 15:53:59 2024 ] 	Batch(2800/6809) done. Loss: 1.4358  lr:0.010000
[ Sun Jul 14 15:54:22 2024 ] 	Batch(2900/6809) done. Loss: 1.1916  lr:0.010000
[ Sun Jul 14 15:54:45 2024 ] 
Training: Epoch [5/150], Step [2999], Loss: 1.8576536178588867, Training Accuracy: 58.037499999999994
[ Sun Jul 14 15:54:45 2024 ] 	Batch(3000/6809) done. Loss: 1.5202  lr:0.010000
[ Sun Jul 14 15:55:08 2024 ] 	Batch(3100/6809) done. Loss: 2.9765  lr:0.010000
[ Sun Jul 14 15:55:31 2024 ] 	Batch(3200/6809) done. Loss: 0.7567  lr:0.010000
[ Sun Jul 14 15:55:54 2024 ] 	Batch(3300/6809) done. Loss: 2.1716  lr:0.010000
[ Sun Jul 14 15:56:17 2024 ] 	Batch(3400/6809) done. Loss: 0.9600  lr:0.010000
[ Sun Jul 14 15:56:39 2024 ] 
Training: Epoch [5/150], Step [3499], Loss: 1.0199068784713745, Training Accuracy: 58.339285714285715
[ Sun Jul 14 15:56:39 2024 ] 	Batch(3500/6809) done. Loss: 1.2090  lr:0.010000
[ Sun Jul 14 15:57:02 2024 ] 	Batch(3600/6809) done. Loss: 1.1318  lr:0.010000
[ Sun Jul 14 15:57:25 2024 ] 	Batch(3700/6809) done. Loss: 1.1893  lr:0.010000
[ Sun Jul 14 15:57:48 2024 ] 	Batch(3800/6809) done. Loss: 1.3773  lr:0.010000
[ Sun Jul 14 15:58:10 2024 ] 	Batch(3900/6809) done. Loss: 1.5920  lr:0.010000
[ Sun Jul 14 15:58:33 2024 ] 
Training: Epoch [5/150], Step [3999], Loss: 1.3088648319244385, Training Accuracy: 58.275
[ Sun Jul 14 15:58:33 2024 ] 	Batch(4000/6809) done. Loss: 0.8684  lr:0.010000
[ Sun Jul 14 15:58:56 2024 ] 	Batch(4100/6809) done. Loss: 0.9739  lr:0.010000
[ Sun Jul 14 15:59:19 2024 ] 	Batch(4200/6809) done. Loss: 1.7799  lr:0.010000
[ Sun Jul 14 15:59:41 2024 ] 	Batch(4300/6809) done. Loss: 0.8425  lr:0.010000
[ Sun Jul 14 16:00:04 2024 ] 	Batch(4400/6809) done. Loss: 1.9168  lr:0.010000
[ Sun Jul 14 16:00:27 2024 ] 
Training: Epoch [5/150], Step [4499], Loss: 0.7269244194030762, Training Accuracy: 58.391666666666666
[ Sun Jul 14 16:00:27 2024 ] 	Batch(4500/6809) done. Loss: 0.8968  lr:0.010000
[ Sun Jul 14 16:00:50 2024 ] 	Batch(4600/6809) done. Loss: 1.4941  lr:0.010000
[ Sun Jul 14 16:01:12 2024 ] 	Batch(4700/6809) done. Loss: 1.3963  lr:0.010000
[ Sun Jul 14 16:01:35 2024 ] 	Batch(4800/6809) done. Loss: 1.2116  lr:0.010000
[ Sun Jul 14 16:01:58 2024 ] 	Batch(4900/6809) done. Loss: 1.2327  lr:0.010000
[ Sun Jul 14 16:02:20 2024 ] 
Training: Epoch [5/150], Step [4999], Loss: 0.8030638098716736, Training Accuracy: 58.425000000000004
[ Sun Jul 14 16:02:21 2024 ] 	Batch(5000/6809) done. Loss: 1.2712  lr:0.010000
[ Sun Jul 14 16:02:43 2024 ] 	Batch(5100/6809) done. Loss: 1.5086  lr:0.010000
[ Sun Jul 14 16:03:06 2024 ] 	Batch(5200/6809) done. Loss: 1.8413  lr:0.010000
[ Sun Jul 14 16:03:30 2024 ] 	Batch(5300/6809) done. Loss: 1.3722  lr:0.010000
[ Sun Jul 14 16:03:52 2024 ] 	Batch(5400/6809) done. Loss: 1.7385  lr:0.010000
[ Sun Jul 14 16:04:15 2024 ] 
Training: Epoch [5/150], Step [5499], Loss: 1.2980742454528809, Training Accuracy: 58.49090909090909
[ Sun Jul 14 16:04:15 2024 ] 	Batch(5500/6809) done. Loss: 1.4526  lr:0.010000
[ Sun Jul 14 16:04:38 2024 ] 	Batch(5600/6809) done. Loss: 1.1643  lr:0.010000
[ Sun Jul 14 16:05:01 2024 ] 	Batch(5700/6809) done. Loss: 1.1808  lr:0.010000
[ Sun Jul 14 16:05:23 2024 ] 	Batch(5800/6809) done. Loss: 2.7049  lr:0.010000
[ Sun Jul 14 16:05:47 2024 ] 	Batch(5900/6809) done. Loss: 2.1942  lr:0.010000
[ Sun Jul 14 16:06:09 2024 ] 
Training: Epoch [5/150], Step [5999], Loss: 1.2961328029632568, Training Accuracy: 58.452083333333334
[ Sun Jul 14 16:06:10 2024 ] 	Batch(6000/6809) done. Loss: 1.5519  lr:0.010000
[ Sun Jul 14 16:06:33 2024 ] 	Batch(6100/6809) done. Loss: 1.4109  lr:0.010000
[ Sun Jul 14 16:06:56 2024 ] 	Batch(6200/6809) done. Loss: 1.4263  lr:0.010000
[ Sun Jul 14 16:07:19 2024 ] 	Batch(6300/6809) done. Loss: 1.0341  lr:0.010000
[ Sun Jul 14 16:07:43 2024 ] 	Batch(6400/6809) done. Loss: 1.2002  lr:0.010000
[ Sun Jul 14 16:08:06 2024 ] 
Training: Epoch [5/150], Step [6499], Loss: 1.3730669021606445, Training Accuracy: 58.58461538461538
[ Sun Jul 14 16:08:06 2024 ] 	Batch(6500/6809) done. Loss: 0.7013  lr:0.010000
[ Sun Jul 14 16:08:29 2024 ] 	Batch(6600/6809) done. Loss: 0.8291  lr:0.010000
[ Sun Jul 14 16:08:53 2024 ] 	Batch(6700/6809) done. Loss: 0.8082  lr:0.010000
[ Sun Jul 14 16:09:16 2024 ] 	Batch(6800/6809) done. Loss: 1.4026  lr:0.010000
[ Sun Jul 14 16:09:18 2024 ] 	Mean training loss: 1.4263.
[ Sun Jul 14 16:09:18 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul 14 16:09:18 2024 ] Training epoch: 7
[ Sun Jul 14 16:09:19 2024 ] 	Batch(0/6809) done. Loss: 0.9969  lr:0.010000
[ Sun Jul 14 16:09:42 2024 ] 	Batch(100/6809) done. Loss: 0.6585  lr:0.010000
[ Sun Jul 14 16:10:05 2024 ] 	Batch(200/6809) done. Loss: 1.4279  lr:0.010000
[ Sun Jul 14 16:10:28 2024 ] 	Batch(300/6809) done. Loss: 0.5753  lr:0.010000
[ Sun Jul 14 16:10:51 2024 ] 	Batch(400/6809) done. Loss: 1.8272  lr:0.010000
[ Sun Jul 14 16:11:14 2024 ] 
Training: Epoch [6/150], Step [499], Loss: 1.0456187725067139, Training Accuracy: 61.150000000000006
[ Sun Jul 14 16:11:14 2024 ] 	Batch(500/6809) done. Loss: 2.9921  lr:0.010000
[ Sun Jul 14 16:11:37 2024 ] 	Batch(600/6809) done. Loss: 1.7532  lr:0.010000
[ Sun Jul 14 16:12:00 2024 ] 	Batch(700/6809) done. Loss: 0.6746  lr:0.010000
[ Sun Jul 14 16:12:24 2024 ] 	Batch(800/6809) done. Loss: 1.3940  lr:0.010000
[ Sun Jul 14 16:12:48 2024 ] 	Batch(900/6809) done. Loss: 1.0655  lr:0.010000
[ Sun Jul 14 16:13:11 2024 ] 
Training: Epoch [6/150], Step [999], Loss: 0.8429163098335266, Training Accuracy: 60.75000000000001
[ Sun Jul 14 16:13:11 2024 ] 	Batch(1000/6809) done. Loss: 0.5384  lr:0.010000
[ Sun Jul 14 16:13:35 2024 ] 	Batch(1100/6809) done. Loss: 1.3707  lr:0.010000
[ Sun Jul 14 16:13:58 2024 ] 	Batch(1200/6809) done. Loss: 1.3411  lr:0.010000
[ Sun Jul 14 16:14:22 2024 ] 	Batch(1300/6809) done. Loss: 0.9779  lr:0.010000
[ Sun Jul 14 16:14:46 2024 ] 	Batch(1400/6809) done. Loss: 0.3492  lr:0.010000
[ Sun Jul 14 16:15:10 2024 ] 
Training: Epoch [6/150], Step [1499], Loss: 0.8899303674697876, Training Accuracy: 60.699999999999996
[ Sun Jul 14 16:15:10 2024 ] 	Batch(1500/6809) done. Loss: 1.8807  lr:0.010000
[ Sun Jul 14 16:15:34 2024 ] 	Batch(1600/6809) done. Loss: 1.6953  lr:0.010000
[ Sun Jul 14 16:15:57 2024 ] 	Batch(1700/6809) done. Loss: 0.5862  lr:0.010000
[ Sun Jul 14 16:16:21 2024 ] 	Batch(1800/6809) done. Loss: 0.2984  lr:0.010000
[ Sun Jul 14 16:16:45 2024 ] 	Batch(1900/6809) done. Loss: 2.2260  lr:0.010000
[ Sun Jul 14 16:17:09 2024 ] 
Training: Epoch [6/150], Step [1999], Loss: 1.3382790088653564, Training Accuracy: 60.73125
[ Sun Jul 14 16:17:09 2024 ] 	Batch(2000/6809) done. Loss: 1.4893  lr:0.010000
[ Sun Jul 14 16:17:33 2024 ] 	Batch(2100/6809) done. Loss: 1.0002  lr:0.010000
[ Sun Jul 14 16:17:56 2024 ] 	Batch(2200/6809) done. Loss: 1.3101  lr:0.010000
[ Sun Jul 14 16:18:20 2024 ] 	Batch(2300/6809) done. Loss: 1.5478  lr:0.010000
[ Sun Jul 14 16:18:43 2024 ] 	Batch(2400/6809) done. Loss: 1.6358  lr:0.010000
[ Sun Jul 14 16:19:06 2024 ] 
Training: Epoch [6/150], Step [2499], Loss: 2.2406206130981445, Training Accuracy: 60.75000000000001
[ Sun Jul 14 16:19:06 2024 ] 	Batch(2500/6809) done. Loss: 0.5524  lr:0.010000
[ Sun Jul 14 16:19:29 2024 ] 	Batch(2600/6809) done. Loss: 1.0598  lr:0.010000
[ Sun Jul 14 16:19:52 2024 ] 	Batch(2700/6809) done. Loss: 1.3317  lr:0.010000
[ Sun Jul 14 16:20:15 2024 ] 	Batch(2800/6809) done. Loss: 0.7621  lr:0.010000
[ Sun Jul 14 16:20:39 2024 ] 	Batch(2900/6809) done. Loss: 1.7524  lr:0.010000
[ Sun Jul 14 16:21:02 2024 ] 
Training: Epoch [6/150], Step [2999], Loss: 2.0724830627441406, Training Accuracy: 60.425
[ Sun Jul 14 16:21:02 2024 ] 	Batch(3000/6809) done. Loss: 1.1393  lr:0.010000
[ Sun Jul 14 16:21:25 2024 ] 	Batch(3100/6809) done. Loss: 1.2473  lr:0.010000
[ Sun Jul 14 16:21:48 2024 ] 	Batch(3200/6809) done. Loss: 0.2794  lr:0.010000
[ Sun Jul 14 16:22:11 2024 ] 	Batch(3300/6809) done. Loss: 0.8426  lr:0.010000
[ Sun Jul 14 16:22:34 2024 ] 	Batch(3400/6809) done. Loss: 1.2221  lr:0.010000
[ Sun Jul 14 16:22:56 2024 ] 
Training: Epoch [6/150], Step [3499], Loss: 1.1171612739562988, Training Accuracy: 60.650000000000006
[ Sun Jul 14 16:22:56 2024 ] 	Batch(3500/6809) done. Loss: 0.9568  lr:0.010000
[ Sun Jul 14 16:23:19 2024 ] 	Batch(3600/6809) done. Loss: 0.9026  lr:0.010000
[ Sun Jul 14 16:23:41 2024 ] 	Batch(3700/6809) done. Loss: 1.6749  lr:0.010000
[ Sun Jul 14 16:24:04 2024 ] 	Batch(3800/6809) done. Loss: 0.9058  lr:0.010000
[ Sun Jul 14 16:24:27 2024 ] 	Batch(3900/6809) done. Loss: 1.7138  lr:0.010000
[ Sun Jul 14 16:24:50 2024 ] 
Training: Epoch [6/150], Step [3999], Loss: 1.5116839408874512, Training Accuracy: 60.71875
[ Sun Jul 14 16:24:50 2024 ] 	Batch(4000/6809) done. Loss: 0.6910  lr:0.010000
[ Sun Jul 14 16:25:13 2024 ] 	Batch(4100/6809) done. Loss: 1.5946  lr:0.010000
[ Sun Jul 14 16:25:35 2024 ] 	Batch(4200/6809) done. Loss: 0.9419  lr:0.010000
[ Sun Jul 14 16:25:58 2024 ] 	Batch(4300/6809) done. Loss: 0.8190  lr:0.010000
[ Sun Jul 14 16:26:21 2024 ] 	Batch(4400/6809) done. Loss: 1.7420  lr:0.010000
[ Sun Jul 14 16:26:43 2024 ] 
Training: Epoch [6/150], Step [4499], Loss: 0.7883909940719604, Training Accuracy: 60.836111111111116
[ Sun Jul 14 16:26:44 2024 ] 	Batch(4500/6809) done. Loss: 1.2676  lr:0.010000
[ Sun Jul 14 16:27:07 2024 ] 	Batch(4600/6809) done. Loss: 1.1967  lr:0.010000
[ Sun Jul 14 16:27:30 2024 ] 	Batch(4700/6809) done. Loss: 0.9799  lr:0.010000
[ Sun Jul 14 16:27:53 2024 ] 	Batch(4800/6809) done. Loss: 1.3671  lr:0.010000
[ Sun Jul 14 16:28:15 2024 ] 	Batch(4900/6809) done. Loss: 1.7871  lr:0.010000
[ Sun Jul 14 16:28:38 2024 ] 
Training: Epoch [6/150], Step [4999], Loss: 1.5290101766586304, Training Accuracy: 61.050000000000004
[ Sun Jul 14 16:28:38 2024 ] 	Batch(5000/6809) done. Loss: 1.7031  lr:0.010000
[ Sun Jul 14 16:29:01 2024 ] 	Batch(5100/6809) done. Loss: 1.2265  lr:0.010000
[ Sun Jul 14 16:29:24 2024 ] 	Batch(5200/6809) done. Loss: 1.2538  lr:0.010000
[ Sun Jul 14 16:29:46 2024 ] 	Batch(5300/6809) done. Loss: 1.2626  lr:0.010000
[ Sun Jul 14 16:30:09 2024 ] 	Batch(5400/6809) done. Loss: 0.8537  lr:0.010000
[ Sun Jul 14 16:30:31 2024 ] 
Training: Epoch [6/150], Step [5499], Loss: 1.8912732601165771, Training Accuracy: 61.018181818181816
[ Sun Jul 14 16:30:31 2024 ] 	Batch(5500/6809) done. Loss: 2.3597  lr:0.010000
[ Sun Jul 14 16:30:54 2024 ] 	Batch(5600/6809) done. Loss: 1.1285  lr:0.010000
[ Sun Jul 14 16:31:17 2024 ] 	Batch(5700/6809) done. Loss: 0.9104  lr:0.010000
[ Sun Jul 14 16:31:39 2024 ] 	Batch(5800/6809) done. Loss: 1.1769  lr:0.010000
[ Sun Jul 14 16:32:02 2024 ] 	Batch(5900/6809) done. Loss: 0.8478  lr:0.010000
[ Sun Jul 14 16:32:25 2024 ] 
Training: Epoch [6/150], Step [5999], Loss: 1.7746626138687134, Training Accuracy: 61.16041666666666
[ Sun Jul 14 16:32:25 2024 ] 	Batch(6000/6809) done. Loss: 0.9546  lr:0.010000
[ Sun Jul 14 16:32:49 2024 ] 	Batch(6100/6809) done. Loss: 0.9788  lr:0.010000
[ Sun Jul 14 16:33:11 2024 ] 	Batch(6200/6809) done. Loss: 1.2721  lr:0.010000
[ Sun Jul 14 16:33:34 2024 ] 	Batch(6300/6809) done. Loss: 1.5522  lr:0.010000
[ Sun Jul 14 16:33:58 2024 ] 	Batch(6400/6809) done. Loss: 0.8486  lr:0.010000
[ Sun Jul 14 16:34:20 2024 ] 
Training: Epoch [6/150], Step [6499], Loss: 1.6364854574203491, Training Accuracy: 61.24423076923077
[ Sun Jul 14 16:34:20 2024 ] 	Batch(6500/6809) done. Loss: 1.0959  lr:0.010000
[ Sun Jul 14 16:34:43 2024 ] 	Batch(6600/6809) done. Loss: 1.1268  lr:0.010000
[ Sun Jul 14 16:35:06 2024 ] 	Batch(6700/6809) done. Loss: 1.6853  lr:0.010000
[ Sun Jul 14 16:35:29 2024 ] 	Batch(6800/6809) done. Loss: 0.6120  lr:0.010000
[ Sun Jul 14 16:35:31 2024 ] 	Mean training loss: 1.3405.
[ Sun Jul 14 16:35:31 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul 14 16:35:31 2024 ] Training epoch: 8
[ Sun Jul 14 16:35:31 2024 ] 	Batch(0/6809) done. Loss: 1.9297  lr:0.010000
[ Sun Jul 14 16:35:54 2024 ] 	Batch(100/6809) done. Loss: 1.3895  lr:0.010000
[ Sun Jul 14 16:36:17 2024 ] 	Batch(200/6809) done. Loss: 1.3927  lr:0.010000
[ Sun Jul 14 16:36:40 2024 ] 	Batch(300/6809) done. Loss: 1.5921  lr:0.010000
[ Sun Jul 14 16:37:03 2024 ] 	Batch(400/6809) done. Loss: 1.1837  lr:0.010000
[ Sun Jul 14 16:37:26 2024 ] 
Training: Epoch [7/150], Step [499], Loss: 2.136634349822998, Training Accuracy: 64.1
[ Sun Jul 14 16:37:26 2024 ] 	Batch(500/6809) done. Loss: 1.5608  lr:0.010000
[ Sun Jul 14 16:37:49 2024 ] 	Batch(600/6809) done. Loss: 1.1310  lr:0.010000
[ Sun Jul 14 16:38:12 2024 ] 	Batch(700/6809) done. Loss: 1.0542  lr:0.010000
[ Sun Jul 14 16:38:34 2024 ] 	Batch(800/6809) done. Loss: 2.6707  lr:0.010000
[ Sun Jul 14 16:38:57 2024 ] 	Batch(900/6809) done. Loss: 1.4458  lr:0.010000
[ Sun Jul 14 16:39:19 2024 ] 
Training: Epoch [7/150], Step [999], Loss: 1.1533147096633911, Training Accuracy: 63.14999999999999
[ Sun Jul 14 16:39:19 2024 ] 	Batch(1000/6809) done. Loss: 1.4703  lr:0.010000
[ Sun Jul 14 16:39:42 2024 ] 	Batch(1100/6809) done. Loss: 2.1270  lr:0.010000
[ Sun Jul 14 16:40:05 2024 ] 	Batch(1200/6809) done. Loss: 1.6715  lr:0.010000
[ Sun Jul 14 16:40:27 2024 ] 	Batch(1300/6809) done. Loss: 1.2404  lr:0.010000
[ Sun Jul 14 16:40:50 2024 ] 	Batch(1400/6809) done. Loss: 1.2414  lr:0.010000
[ Sun Jul 14 16:41:12 2024 ] 
Training: Epoch [7/150], Step [1499], Loss: 1.2449662685394287, Training Accuracy: 63.083333333333336
[ Sun Jul 14 16:41:13 2024 ] 	Batch(1500/6809) done. Loss: 1.5033  lr:0.010000
[ Sun Jul 14 16:41:36 2024 ] 	Batch(1600/6809) done. Loss: 0.6884  lr:0.010000
[ Sun Jul 14 16:41:59 2024 ] 	Batch(1700/6809) done. Loss: 1.0406  lr:0.010000
[ Sun Jul 14 16:42:22 2024 ] 	Batch(1800/6809) done. Loss: 1.5133  lr:0.010000
[ Sun Jul 14 16:42:45 2024 ] 	Batch(1900/6809) done. Loss: 1.0803  lr:0.010000
[ Sun Jul 14 16:43:07 2024 ] 
Training: Epoch [7/150], Step [1999], Loss: 1.8702255487442017, Training Accuracy: 63.35625
[ Sun Jul 14 16:43:08 2024 ] 	Batch(2000/6809) done. Loss: 0.8299  lr:0.010000
[ Sun Jul 14 16:43:31 2024 ] 	Batch(2100/6809) done. Loss: 1.4997  lr:0.010000
[ Sun Jul 14 16:43:54 2024 ] 	Batch(2200/6809) done. Loss: 1.2156  lr:0.010000
[ Sun Jul 14 16:44:17 2024 ] 	Batch(2300/6809) done. Loss: 0.9639  lr:0.010000
[ Sun Jul 14 16:44:40 2024 ] 	Batch(2400/6809) done. Loss: 0.9231  lr:0.010000
[ Sun Jul 14 16:45:03 2024 ] 
Training: Epoch [7/150], Step [2499], Loss: 0.6813997626304626, Training Accuracy: 63.53
[ Sun Jul 14 16:45:03 2024 ] 	Batch(2500/6809) done. Loss: 1.9236  lr:0.010000
[ Sun Jul 14 16:45:26 2024 ] 	Batch(2600/6809) done. Loss: 1.0281  lr:0.010000
[ Sun Jul 14 16:45:49 2024 ] 	Batch(2700/6809) done. Loss: 1.9603  lr:0.010000
[ Sun Jul 14 16:46:12 2024 ] 	Batch(2800/6809) done. Loss: 0.5067  lr:0.010000
[ Sun Jul 14 16:46:35 2024 ] 	Batch(2900/6809) done. Loss: 0.4097  lr:0.010000
[ Sun Jul 14 16:46:57 2024 ] 
Training: Epoch [7/150], Step [2999], Loss: 1.3519022464752197, Training Accuracy: 63.68333333333334
[ Sun Jul 14 16:46:57 2024 ] 	Batch(3000/6809) done. Loss: 1.0633  lr:0.010000
[ Sun Jul 14 16:47:20 2024 ] 	Batch(3100/6809) done. Loss: 0.7620  lr:0.010000
[ Sun Jul 14 16:47:43 2024 ] 	Batch(3200/6809) done. Loss: 1.1281  lr:0.010000
[ Sun Jul 14 16:48:05 2024 ] 	Batch(3300/6809) done. Loss: 1.1720  lr:0.010000
[ Sun Jul 14 16:48:28 2024 ] 	Batch(3400/6809) done. Loss: 0.5541  lr:0.010000
[ Sun Jul 14 16:48:51 2024 ] 
Training: Epoch [7/150], Step [3499], Loss: 1.194662094116211, Training Accuracy: 63.68214285714286
[ Sun Jul 14 16:48:51 2024 ] 	Batch(3500/6809) done. Loss: 1.5143  lr:0.010000
[ Sun Jul 14 16:49:14 2024 ] 	Batch(3600/6809) done. Loss: 0.5213  lr:0.010000
[ Sun Jul 14 16:49:37 2024 ] 	Batch(3700/6809) done. Loss: 1.0033  lr:0.010000
[ Sun Jul 14 16:50:01 2024 ] 	Batch(3800/6809) done. Loss: 1.3240  lr:0.010000
[ Sun Jul 14 16:50:24 2024 ] 	Batch(3900/6809) done. Loss: 0.4610  lr:0.010000
[ Sun Jul 14 16:50:47 2024 ] 
Training: Epoch [7/150], Step [3999], Loss: 1.1918258666992188, Training Accuracy: 63.6625
[ Sun Jul 14 16:50:48 2024 ] 	Batch(4000/6809) done. Loss: 0.5563  lr:0.010000
[ Sun Jul 14 16:51:11 2024 ] 	Batch(4100/6809) done. Loss: 0.6206  lr:0.010000
[ Sun Jul 14 16:51:33 2024 ] 	Batch(4200/6809) done. Loss: 1.4640  lr:0.010000
[ Sun Jul 14 16:51:56 2024 ] 	Batch(4300/6809) done. Loss: 0.3118  lr:0.010000
[ Sun Jul 14 16:52:19 2024 ] 	Batch(4400/6809) done. Loss: 2.1565  lr:0.010000
[ Sun Jul 14 16:52:42 2024 ] 
Training: Epoch [7/150], Step [4499], Loss: 1.1509898900985718, Training Accuracy: 63.391666666666666
[ Sun Jul 14 16:52:42 2024 ] 	Batch(4500/6809) done. Loss: 0.7122  lr:0.010000
[ Sun Jul 14 16:53:04 2024 ] 	Batch(4600/6809) done. Loss: 0.6239  lr:0.010000
[ Sun Jul 14 16:53:27 2024 ] 	Batch(4700/6809) done. Loss: 1.5783  lr:0.010000
[ Sun Jul 14 16:53:50 2024 ] 	Batch(4800/6809) done. Loss: 0.8186  lr:0.010000
[ Sun Jul 14 16:54:13 2024 ] 	Batch(4900/6809) done. Loss: 1.0654  lr:0.010000
[ Sun Jul 14 16:54:35 2024 ] 
Training: Epoch [7/150], Step [4999], Loss: 0.6187230348587036, Training Accuracy: 63.4875
[ Sun Jul 14 16:54:35 2024 ] 	Batch(5000/6809) done. Loss: 0.8002  lr:0.010000
[ Sun Jul 14 16:54:58 2024 ] 	Batch(5100/6809) done. Loss: 1.1773  lr:0.010000
[ Sun Jul 14 16:55:21 2024 ] 	Batch(5200/6809) done. Loss: 0.6518  lr:0.010000
[ Sun Jul 14 16:55:44 2024 ] 	Batch(5300/6809) done. Loss: 0.5508  lr:0.010000
[ Sun Jul 14 16:56:08 2024 ] 	Batch(5400/6809) done. Loss: 1.4345  lr:0.010000
[ Sun Jul 14 16:56:31 2024 ] 
Training: Epoch [7/150], Step [5499], Loss: 1.5503671169281006, Training Accuracy: 63.6
[ Sun Jul 14 16:56:31 2024 ] 	Batch(5500/6809) done. Loss: 1.0324  lr:0.010000
[ Sun Jul 14 16:56:55 2024 ] 	Batch(5600/6809) done. Loss: 1.3375  lr:0.010000
[ Sun Jul 14 16:57:18 2024 ] 	Batch(5700/6809) done. Loss: 1.0683  lr:0.010000
[ Sun Jul 14 16:57:41 2024 ] 	Batch(5800/6809) done. Loss: 1.1133  lr:0.010000
[ Sun Jul 14 16:58:03 2024 ] 	Batch(5900/6809) done. Loss: 0.9454  lr:0.010000
[ Sun Jul 14 16:58:26 2024 ] 
Training: Epoch [7/150], Step [5999], Loss: 1.3018088340759277, Training Accuracy: 63.568749999999994
[ Sun Jul 14 16:58:26 2024 ] 	Batch(6000/6809) done. Loss: 1.3995  lr:0.010000
[ Sun Jul 14 16:58:49 2024 ] 	Batch(6100/6809) done. Loss: 1.7802  lr:0.010000
[ Sun Jul 14 16:59:12 2024 ] 	Batch(6200/6809) done. Loss: 0.5598  lr:0.010000
[ Sun Jul 14 16:59:34 2024 ] 	Batch(6300/6809) done. Loss: 1.2826  lr:0.010000
[ Sun Jul 14 16:59:57 2024 ] 	Batch(6400/6809) done. Loss: 0.2473  lr:0.010000
[ Sun Jul 14 17:00:20 2024 ] 
Training: Epoch [7/150], Step [6499], Loss: 1.6067018508911133, Training Accuracy: 63.619230769230775
[ Sun Jul 14 17:00:20 2024 ] 	Batch(6500/6809) done. Loss: 1.7863  lr:0.010000
[ Sun Jul 14 17:00:43 2024 ] 	Batch(6600/6809) done. Loss: 1.2079  lr:0.010000
[ Sun Jul 14 17:01:06 2024 ] 	Batch(6700/6809) done. Loss: 1.3276  lr:0.010000
[ Sun Jul 14 17:01:28 2024 ] 	Batch(6800/6809) done. Loss: 1.1834  lr:0.010000
[ Sun Jul 14 17:01:30 2024 ] 	Mean training loss: 1.2371.
[ Sun Jul 14 17:01:30 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 17:01:30 2024 ] Training epoch: 9
[ Sun Jul 14 17:01:31 2024 ] 	Batch(0/6809) done. Loss: 1.4932  lr:0.010000
[ Sun Jul 14 17:01:54 2024 ] 	Batch(100/6809) done. Loss: 1.0605  lr:0.010000
[ Sun Jul 14 17:02:17 2024 ] 	Batch(200/6809) done. Loss: 2.8841  lr:0.010000
[ Sun Jul 14 17:02:40 2024 ] 	Batch(300/6809) done. Loss: 0.4749  lr:0.010000
[ Sun Jul 14 17:03:03 2024 ] 	Batch(400/6809) done. Loss: 0.8699  lr:0.010000
[ Sun Jul 14 17:03:26 2024 ] 
Training: Epoch [8/150], Step [499], Loss: 0.9976134300231934, Training Accuracy: 66.825
[ Sun Jul 14 17:03:26 2024 ] 	Batch(500/6809) done. Loss: 0.8390  lr:0.010000
[ Sun Jul 14 17:03:49 2024 ] 	Batch(600/6809) done. Loss: 0.8346  lr:0.010000
[ Sun Jul 14 17:04:12 2024 ] 	Batch(700/6809) done. Loss: 1.2684  lr:0.010000
[ Sun Jul 14 17:04:36 2024 ] 	Batch(800/6809) done. Loss: 1.2267  lr:0.010000
[ Sun Jul 14 17:04:59 2024 ] 	Batch(900/6809) done. Loss: 0.4830  lr:0.010000
[ Sun Jul 14 17:05:22 2024 ] 
Training: Epoch [8/150], Step [999], Loss: 1.2282636165618896, Training Accuracy: 66.71249999999999
[ Sun Jul 14 17:05:22 2024 ] 	Batch(1000/6809) done. Loss: 0.6792  lr:0.010000
[ Sun Jul 14 17:05:46 2024 ] 	Batch(1100/6809) done. Loss: 1.6945  lr:0.010000
[ Sun Jul 14 17:06:10 2024 ] 	Batch(1200/6809) done. Loss: 1.0895  lr:0.010000
[ Sun Jul 14 17:06:34 2024 ] 	Batch(1300/6809) done. Loss: 1.7285  lr:0.010000
[ Sun Jul 14 17:06:58 2024 ] 	Batch(1400/6809) done. Loss: 2.1340  lr:0.010000
[ Sun Jul 14 17:07:22 2024 ] 
Training: Epoch [8/150], Step [1499], Loss: 0.44957345724105835, Training Accuracy: 66.49166666666667
[ Sun Jul 14 17:07:22 2024 ] 	Batch(1500/6809) done. Loss: 0.9177  lr:0.010000
[ Sun Jul 14 17:07:46 2024 ] 	Batch(1600/6809) done. Loss: 0.7190  lr:0.010000
[ Sun Jul 14 17:08:09 2024 ] 	Batch(1700/6809) done. Loss: 1.6716  lr:0.010000
[ Sun Jul 14 17:08:32 2024 ] 	Batch(1800/6809) done. Loss: 1.4785  lr:0.010000
[ Sun Jul 14 17:08:55 2024 ] 	Batch(1900/6809) done. Loss: 0.9135  lr:0.010000
[ Sun Jul 14 17:09:18 2024 ] 
Training: Epoch [8/150], Step [1999], Loss: 2.1873302459716797, Training Accuracy: 66.0875
[ Sun Jul 14 17:09:18 2024 ] 	Batch(2000/6809) done. Loss: 1.5859  lr:0.010000
[ Sun Jul 14 17:09:41 2024 ] 	Batch(2100/6809) done. Loss: 0.4580  lr:0.010000
[ Sun Jul 14 17:10:04 2024 ] 	Batch(2200/6809) done. Loss: 0.5584  lr:0.010000
[ Sun Jul 14 17:10:28 2024 ] 	Batch(2300/6809) done. Loss: 0.3452  lr:0.010000
[ Sun Jul 14 17:10:51 2024 ] 	Batch(2400/6809) done. Loss: 0.7794  lr:0.010000
[ Sun Jul 14 17:11:14 2024 ] 
Training: Epoch [8/150], Step [2499], Loss: 0.4379456639289856, Training Accuracy: 65.605
[ Sun Jul 14 17:11:14 2024 ] 	Batch(2500/6809) done. Loss: 1.2786  lr:0.010000
[ Sun Jul 14 17:11:37 2024 ] 	Batch(2600/6809) done. Loss: 1.8853  lr:0.010000
[ Sun Jul 14 17:12:00 2024 ] 	Batch(2700/6809) done. Loss: 1.9841  lr:0.010000
[ Sun Jul 14 17:12:22 2024 ] 	Batch(2800/6809) done. Loss: 1.4866  lr:0.010000
[ Sun Jul 14 17:12:45 2024 ] 	Batch(2900/6809) done. Loss: 1.2901  lr:0.010000
[ Sun Jul 14 17:13:07 2024 ] 
Training: Epoch [8/150], Step [2999], Loss: 1.3482719659805298, Training Accuracy: 65.70833333333333
[ Sun Jul 14 17:13:08 2024 ] 	Batch(3000/6809) done. Loss: 2.1160  lr:0.010000
[ Sun Jul 14 17:13:30 2024 ] 	Batch(3100/6809) done. Loss: 1.6434  lr:0.010000
[ Sun Jul 14 17:13:53 2024 ] 	Batch(3200/6809) done. Loss: 1.9094  lr:0.010000
[ Sun Jul 14 17:14:15 2024 ] 	Batch(3300/6809) done. Loss: 0.6983  lr:0.010000
[ Sun Jul 14 17:14:38 2024 ] 	Batch(3400/6809) done. Loss: 0.8526  lr:0.010000
[ Sun Jul 14 17:15:00 2024 ] 
Training: Epoch [8/150], Step [3499], Loss: 2.768721580505371, Training Accuracy: 65.68214285714285
[ Sun Jul 14 17:15:01 2024 ] 	Batch(3500/6809) done. Loss: 1.5532  lr:0.010000
[ Sun Jul 14 17:15:24 2024 ] 	Batch(3600/6809) done. Loss: 0.8466  lr:0.010000
[ Sun Jul 14 17:15:46 2024 ] 	Batch(3700/6809) done. Loss: 0.7341  lr:0.010000
[ Sun Jul 14 17:16:09 2024 ] 	Batch(3800/6809) done. Loss: 1.7980  lr:0.010000
[ Sun Jul 14 17:16:31 2024 ] 	Batch(3900/6809) done. Loss: 0.7723  lr:0.010000
[ Sun Jul 14 17:16:53 2024 ] 
Training: Epoch [8/150], Step [3999], Loss: 1.3382304906845093, Training Accuracy: 65.53750000000001
[ Sun Jul 14 17:16:54 2024 ] 	Batch(4000/6809) done. Loss: 2.5529  lr:0.010000
[ Sun Jul 14 17:17:16 2024 ] 	Batch(4100/6809) done. Loss: 0.9020  lr:0.010000
[ Sun Jul 14 17:17:39 2024 ] 	Batch(4200/6809) done. Loss: 0.6899  lr:0.010000
[ Sun Jul 14 17:18:01 2024 ] 	Batch(4300/6809) done. Loss: 0.5649  lr:0.010000
[ Sun Jul 14 17:18:24 2024 ] 	Batch(4400/6809) done. Loss: 0.4059  lr:0.010000
[ Sun Jul 14 17:18:47 2024 ] 
Training: Epoch [8/150], Step [4499], Loss: 0.6621754169464111, Training Accuracy: 65.67222222222222
[ Sun Jul 14 17:18:47 2024 ] 	Batch(4500/6809) done. Loss: 2.4311  lr:0.010000
[ Sun Jul 14 17:19:09 2024 ] 	Batch(4600/6809) done. Loss: 0.6855  lr:0.010000
[ Sun Jul 14 17:19:32 2024 ] 	Batch(4700/6809) done. Loss: 0.3506  lr:0.010000
[ Sun Jul 14 17:19:55 2024 ] 	Batch(4800/6809) done. Loss: 1.2033  lr:0.010000
[ Sun Jul 14 17:20:17 2024 ] 	Batch(4900/6809) done. Loss: 1.3455  lr:0.010000
[ Sun Jul 14 17:20:40 2024 ] 
Training: Epoch [8/150], Step [4999], Loss: 0.7460216283798218, Training Accuracy: 65.765
[ Sun Jul 14 17:20:40 2024 ] 	Batch(5000/6809) done. Loss: 0.4850  lr:0.010000
[ Sun Jul 14 17:21:02 2024 ] 	Batch(5100/6809) done. Loss: 0.5810  lr:0.010000
[ Sun Jul 14 17:21:25 2024 ] 	Batch(5200/6809) done. Loss: 1.4396  lr:0.010000
[ Sun Jul 14 17:21:48 2024 ] 	Batch(5300/6809) done. Loss: 1.0227  lr:0.010000
[ Sun Jul 14 17:22:10 2024 ] 	Batch(5400/6809) done. Loss: 1.1850  lr:0.010000
[ Sun Jul 14 17:22:33 2024 ] 
Training: Epoch [8/150], Step [5499], Loss: 1.0232347249984741, Training Accuracy: 65.70227272727273
[ Sun Jul 14 17:22:33 2024 ] 	Batch(5500/6809) done. Loss: 0.2876  lr:0.010000
[ Sun Jul 14 17:22:56 2024 ] 	Batch(5600/6809) done. Loss: 2.1217  lr:0.010000
[ Sun Jul 14 17:23:19 2024 ] 	Batch(5700/6809) done. Loss: 0.8973  lr:0.010000
[ Sun Jul 14 17:23:42 2024 ] 	Batch(5800/6809) done. Loss: 1.8818  lr:0.010000
[ Sun Jul 14 17:24:05 2024 ] 	Batch(5900/6809) done. Loss: 0.4572  lr:0.010000
[ Sun Jul 14 17:24:27 2024 ] 
Training: Epoch [8/150], Step [5999], Loss: 0.8523701429367065, Training Accuracy: 65.64375
[ Sun Jul 14 17:24:28 2024 ] 	Batch(6000/6809) done. Loss: 1.6931  lr:0.010000
[ Sun Jul 14 17:24:50 2024 ] 	Batch(6100/6809) done. Loss: 1.7541  lr:0.010000
[ Sun Jul 14 17:25:13 2024 ] 	Batch(6200/6809) done. Loss: 1.2345  lr:0.010000
[ Sun Jul 14 17:25:36 2024 ] 	Batch(6300/6809) done. Loss: 1.5149  lr:0.010000
[ Sun Jul 14 17:25:59 2024 ] 	Batch(6400/6809) done. Loss: 1.4607  lr:0.010000
[ Sun Jul 14 17:26:22 2024 ] 
Training: Epoch [8/150], Step [6499], Loss: 1.5963433980941772, Training Accuracy: 65.80192307692307
[ Sun Jul 14 17:26:22 2024 ] 	Batch(6500/6809) done. Loss: 0.8615  lr:0.010000
[ Sun Jul 14 17:26:45 2024 ] 	Batch(6600/6809) done. Loss: 1.7151  lr:0.010000
[ Sun Jul 14 17:27:08 2024 ] 	Batch(6700/6809) done. Loss: 1.7631  lr:0.010000
[ Sun Jul 14 17:27:30 2024 ] 	Batch(6800/6809) done. Loss: 0.8491  lr:0.010000
[ Sun Jul 14 17:27:32 2024 ] 	Mean training loss: 1.1710.
[ Sun Jul 14 17:27:32 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul 14 17:27:32 2024 ] Training epoch: 10
[ Sun Jul 14 17:27:33 2024 ] 	Batch(0/6809) done. Loss: 1.4834  lr:0.010000
[ Sun Jul 14 17:27:55 2024 ] 	Batch(100/6809) done. Loss: 1.2901  lr:0.010000
[ Sun Jul 14 17:28:18 2024 ] 	Batch(200/6809) done. Loss: 1.8526  lr:0.010000
[ Sun Jul 14 17:28:41 2024 ] 	Batch(300/6809) done. Loss: 0.6461  lr:0.010000
[ Sun Jul 14 17:29:03 2024 ] 	Batch(400/6809) done. Loss: 1.4652  lr:0.010000
[ Sun Jul 14 17:29:26 2024 ] 
Training: Epoch [9/150], Step [499], Loss: 0.6370993256568909, Training Accuracy: 67.80000000000001
[ Sun Jul 14 17:29:26 2024 ] 	Batch(500/6809) done. Loss: 1.7294  lr:0.010000
[ Sun Jul 14 17:29:48 2024 ] 	Batch(600/6809) done. Loss: 2.0436  lr:0.010000
[ Sun Jul 14 17:30:11 2024 ] 	Batch(700/6809) done. Loss: 0.4356  lr:0.010000
[ Sun Jul 14 17:30:34 2024 ] 	Batch(800/6809) done. Loss: 1.0979  lr:0.010000
[ Sun Jul 14 17:30:56 2024 ] 	Batch(900/6809) done. Loss: 0.6415  lr:0.010000
[ Sun Jul 14 17:31:19 2024 ] 
Training: Epoch [9/150], Step [999], Loss: 1.0172288417816162, Training Accuracy: 66.7875
[ Sun Jul 14 17:31:19 2024 ] 	Batch(1000/6809) done. Loss: 0.7256  lr:0.010000
[ Sun Jul 14 17:31:41 2024 ] 	Batch(1100/6809) done. Loss: 0.9880  lr:0.010000
[ Sun Jul 14 17:32:05 2024 ] 	Batch(1200/6809) done. Loss: 1.5066  lr:0.010000
[ Sun Jul 14 17:32:28 2024 ] 	Batch(1300/6809) done. Loss: 1.3980  lr:0.010000
[ Sun Jul 14 17:32:50 2024 ] 	Batch(1400/6809) done. Loss: 0.7907  lr:0.010000
[ Sun Jul 14 17:33:13 2024 ] 
Training: Epoch [9/150], Step [1499], Loss: 1.2678523063659668, Training Accuracy: 67.08333333333333
[ Sun Jul 14 17:33:13 2024 ] 	Batch(1500/6809) done. Loss: 1.9079  lr:0.010000
[ Sun Jul 14 17:33:36 2024 ] 	Batch(1600/6809) done. Loss: 1.1698  lr:0.010000
[ Sun Jul 14 17:33:59 2024 ] 	Batch(1700/6809) done. Loss: 1.3218  lr:0.010000
[ Sun Jul 14 17:34:22 2024 ] 	Batch(1800/6809) done. Loss: 1.0924  lr:0.010000
[ Sun Jul 14 17:34:45 2024 ] 	Batch(1900/6809) done. Loss: 1.5626  lr:0.010000
[ Sun Jul 14 17:35:08 2024 ] 
Training: Epoch [9/150], Step [1999], Loss: 0.797188937664032, Training Accuracy: 67.48125
[ Sun Jul 14 17:35:08 2024 ] 	Batch(2000/6809) done. Loss: 0.7565  lr:0.010000
[ Sun Jul 14 17:35:31 2024 ] 	Batch(2100/6809) done. Loss: 0.3785  lr:0.010000
[ Sun Jul 14 17:35:54 2024 ] 	Batch(2200/6809) done. Loss: 0.6352  lr:0.010000
[ Sun Jul 14 17:36:17 2024 ] 	Batch(2300/6809) done. Loss: 1.0435  lr:0.010000
[ Sun Jul 14 17:36:40 2024 ] 	Batch(2400/6809) done. Loss: 1.2878  lr:0.010000
[ Sun Jul 14 17:37:03 2024 ] 
Training: Epoch [9/150], Step [2499], Loss: 2.8758413791656494, Training Accuracy: 67.405
[ Sun Jul 14 17:37:03 2024 ] 	Batch(2500/6809) done. Loss: 0.6256  lr:0.010000
[ Sun Jul 14 17:37:26 2024 ] 	Batch(2600/6809) done. Loss: 0.8103  lr:0.010000
[ Sun Jul 14 17:37:49 2024 ] 	Batch(2700/6809) done. Loss: 1.3419  lr:0.010000
[ Sun Jul 14 17:38:12 2024 ] 	Batch(2800/6809) done. Loss: 0.9521  lr:0.010000
[ Sun Jul 14 17:38:35 2024 ] 	Batch(2900/6809) done. Loss: 1.1858  lr:0.010000
[ Sun Jul 14 17:38:58 2024 ] 
Training: Epoch [9/150], Step [2999], Loss: 0.9405234456062317, Training Accuracy: 67.29166666666667
[ Sun Jul 14 17:38:58 2024 ] 	Batch(3000/6809) done. Loss: 0.9408  lr:0.010000
[ Sun Jul 14 17:39:21 2024 ] 	Batch(3100/6809) done. Loss: 0.5571  lr:0.010000
[ Sun Jul 14 17:39:44 2024 ] 	Batch(3200/6809) done. Loss: 1.6413  lr:0.010000
[ Sun Jul 14 17:40:07 2024 ] 	Batch(3300/6809) done. Loss: 0.8989  lr:0.010000
[ Sun Jul 14 17:40:30 2024 ] 	Batch(3400/6809) done. Loss: 2.1388  lr:0.010000
[ Sun Jul 14 17:40:53 2024 ] 
Training: Epoch [9/150], Step [3499], Loss: 1.0899114608764648, Training Accuracy: 67.29642857142856
[ Sun Jul 14 17:40:53 2024 ] 	Batch(3500/6809) done. Loss: 1.1498  lr:0.010000
[ Sun Jul 14 17:41:16 2024 ] 	Batch(3600/6809) done. Loss: 0.7036  lr:0.010000
[ Sun Jul 14 17:41:39 2024 ] 	Batch(3700/6809) done. Loss: 0.7231  lr:0.010000
[ Sun Jul 14 17:42:01 2024 ] 	Batch(3800/6809) done. Loss: 0.8525  lr:0.010000
[ Sun Jul 14 17:42:24 2024 ] 	Batch(3900/6809) done. Loss: 0.7207  lr:0.010000
[ Sun Jul 14 17:42:46 2024 ] 
Training: Epoch [9/150], Step [3999], Loss: 0.8036561012268066, Training Accuracy: 67.33749999999999
[ Sun Jul 14 17:42:47 2024 ] 	Batch(4000/6809) done. Loss: 1.1614  lr:0.010000
[ Sun Jul 14 17:43:09 2024 ] 	Batch(4100/6809) done. Loss: 1.3477  lr:0.010000
[ Sun Jul 14 17:43:32 2024 ] 	Batch(4200/6809) done. Loss: 0.7574  lr:0.010000
[ Sun Jul 14 17:43:54 2024 ] 	Batch(4300/6809) done. Loss: 0.4424  lr:0.010000
[ Sun Jul 14 17:44:17 2024 ] 	Batch(4400/6809) done. Loss: 1.6017  lr:0.010000
[ Sun Jul 14 17:44:39 2024 ] 
Training: Epoch [9/150], Step [4499], Loss: 0.6534433960914612, Training Accuracy: 67.22222222222223
[ Sun Jul 14 17:44:40 2024 ] 	Batch(4500/6809) done. Loss: 0.4805  lr:0.010000
[ Sun Jul 14 17:45:02 2024 ] 	Batch(4600/6809) done. Loss: 1.4267  lr:0.010000
[ Sun Jul 14 17:45:25 2024 ] 	Batch(4700/6809) done. Loss: 1.0814  lr:0.010000
[ Sun Jul 14 17:45:48 2024 ] 	Batch(4800/6809) done. Loss: 1.4889  lr:0.010000
[ Sun Jul 14 17:46:10 2024 ] 	Batch(4900/6809) done. Loss: 1.0137  lr:0.010000
[ Sun Jul 14 17:46:33 2024 ] 
Training: Epoch [9/150], Step [4999], Loss: 1.2026116847991943, Training Accuracy: 67.3575
[ Sun Jul 14 17:46:33 2024 ] 	Batch(5000/6809) done. Loss: 2.2707  lr:0.010000
[ Sun Jul 14 17:46:56 2024 ] 	Batch(5100/6809) done. Loss: 1.2143  lr:0.010000
[ Sun Jul 14 17:47:19 2024 ] 	Batch(5200/6809) done. Loss: 0.8768  lr:0.010000
[ Sun Jul 14 17:47:41 2024 ] 	Batch(5300/6809) done. Loss: 1.0250  lr:0.010000
[ Sun Jul 14 17:48:04 2024 ] 	Batch(5400/6809) done. Loss: 2.1060  lr:0.010000
[ Sun Jul 14 17:48:26 2024 ] 
Training: Epoch [9/150], Step [5499], Loss: 1.0444966554641724, Training Accuracy: 67.38636363636363
[ Sun Jul 14 17:48:27 2024 ] 	Batch(5500/6809) done. Loss: 0.8790  lr:0.010000
[ Sun Jul 14 17:48:50 2024 ] 	Batch(5600/6809) done. Loss: 0.6320  lr:0.010000
[ Sun Jul 14 17:49:12 2024 ] 	Batch(5700/6809) done. Loss: 0.9839  lr:0.010000
[ Sun Jul 14 17:49:35 2024 ] 	Batch(5800/6809) done. Loss: 1.8126  lr:0.010000
[ Sun Jul 14 17:49:58 2024 ] 	Batch(5900/6809) done. Loss: 0.5850  lr:0.010000
[ Sun Jul 14 17:50:20 2024 ] 
Training: Epoch [9/150], Step [5999], Loss: 0.6965762376785278, Training Accuracy: 67.31666666666666
[ Sun Jul 14 17:50:20 2024 ] 	Batch(6000/6809) done. Loss: 0.6098  lr:0.010000
[ Sun Jul 14 17:50:43 2024 ] 	Batch(6100/6809) done. Loss: 0.6271  lr:0.010000
[ Sun Jul 14 17:51:05 2024 ] 	Batch(6200/6809) done. Loss: 0.6678  lr:0.010000
[ Sun Jul 14 17:51:28 2024 ] 	Batch(6300/6809) done. Loss: 0.7620  lr:0.010000
[ Sun Jul 14 17:51:51 2024 ] 	Batch(6400/6809) done. Loss: 0.8783  lr:0.010000
[ Sun Jul 14 17:52:13 2024 ] 
Training: Epoch [9/150], Step [6499], Loss: 1.8970657587051392, Training Accuracy: 67.33461538461538
[ Sun Jul 14 17:52:13 2024 ] 	Batch(6500/6809) done. Loss: 0.5678  lr:0.010000
[ Sun Jul 14 17:52:36 2024 ] 	Batch(6600/6809) done. Loss: 0.7653  lr:0.010000
[ Sun Jul 14 17:52:58 2024 ] 	Batch(6700/6809) done. Loss: 2.0618  lr:0.010000
[ Sun Jul 14 17:53:21 2024 ] 	Batch(6800/6809) done. Loss: 1.2336  lr:0.010000
[ Sun Jul 14 17:53:23 2024 ] 	Mean training loss: 1.0979.
[ Sun Jul 14 17:53:23 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 17:53:23 2024 ] Eval epoch: 10
[ Sun Jul 14 18:00:19 2024 ] 	Mean val loss of 7435 batches: 1.3047245626498787.
[ Sun Jul 14 18:00:19 2024 ] 
Validation: Epoch [9/150], Samples [38596.0/59477], Loss: 1.9843127727508545, Validation Accuracy: 64.89231131361703
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 1 : 311 / 500 = 62 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 2 : 400 / 499 = 80 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 3 : 333 / 500 = 66 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 4 : 420 / 502 = 83 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 5 : 205 / 502 = 40 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 6 : 335 / 502 = 66 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 7 : 441 / 497 = 88 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 8 : 462 / 498 = 92 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 9 : 273 / 500 = 54 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 10 : 89 / 500 = 17 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 11 : 263 / 498 = 52 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 12 : 274 / 499 = 54 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 13 : 461 / 502 = 91 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 14 : 433 / 504 = 85 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 15 : 267 / 502 = 53 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 16 : 202 / 502 = 40 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 17 : 391 / 504 = 77 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 18 : 304 / 504 = 60 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 19 : 269 / 502 = 53 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 20 : 324 / 502 = 64 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 21 : 414 / 503 = 82 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 22 : 267 / 504 = 52 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 23 : 355 / 503 = 70 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 24 : 379 / 504 = 75 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 25 : 402 / 504 = 79 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 26 : 416 / 504 = 82 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 27 : 403 / 501 = 80 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 28 : 301 / 502 = 59 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 29 : 237 / 502 = 47 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 30 : 371 / 501 = 74 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 31 : 393 / 504 = 77 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 32 : 323 / 503 = 64 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 33 : 256 / 503 = 50 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 34 : 446 / 504 = 88 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 35 : 460 / 503 = 91 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 36 : 271 / 502 = 53 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 37 : 352 / 504 = 69 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 38 : 381 / 504 = 75 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 39 : 379 / 498 = 76 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 40 : 295 / 504 = 58 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 41 : 426 / 503 = 84 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 42 : 295 / 504 = 58 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 43 : 318 / 503 = 63 %
[ Sun Jul 14 18:00:19 2024 ] Accuracy of 44 : 267 / 504 = 52 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 45 : 388 / 504 = 76 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 46 : 374 / 504 = 74 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 47 : 319 / 503 = 63 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 48 : 415 / 503 = 82 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 49 : 289 / 499 = 57 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 50 : 321 / 502 = 63 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 51 : 433 / 503 = 86 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 52 : 280 / 504 = 55 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 53 : 436 / 497 = 87 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 54 : 450 / 480 = 93 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 55 : 423 / 504 = 83 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 56 : 398 / 503 = 79 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 57 : 447 / 504 = 88 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 58 : 467 / 499 = 93 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 59 : 460 / 503 = 91 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 60 : 359 / 479 = 74 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 61 : 357 / 484 = 73 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 62 : 274 / 487 = 56 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 63 : 390 / 489 = 79 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 64 : 332 / 488 = 68 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 65 : 359 / 490 = 73 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 66 : 317 / 488 = 64 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 67 : 342 / 490 = 69 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 68 : 149 / 490 = 30 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 69 : 322 / 490 = 65 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 70 : 1 / 490 = 0 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 71 : 381 / 490 = 77 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 72 : 94 / 488 = 19 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 73 : 115 / 486 = 23 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 74 : 122 / 481 = 25 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 75 : 214 / 488 = 43 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 76 : 173 / 489 = 35 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 77 : 265 / 488 = 54 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 78 : 371 / 488 = 76 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 79 : 421 / 490 = 85 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 80 : 282 / 489 = 57 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 81 : 124 / 491 = 25 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 82 : 195 / 491 = 39 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 83 : 136 / 489 = 27 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 84 : 329 / 489 = 67 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 85 : 338 / 489 = 69 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 86 : 335 / 491 = 68 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 87 : 344 / 492 = 69 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 88 : 270 / 491 = 54 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 89 : 232 / 492 = 47 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 90 : 119 / 490 = 24 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 91 : 336 / 482 = 69 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 92 : 300 / 490 = 61 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 93 : 155 / 487 = 31 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 94 : 383 / 489 = 78 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 95 : 366 / 490 = 74 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 96 : 436 / 491 = 88 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 97 : 457 / 490 = 93 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 98 : 407 / 491 = 82 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 99 : 426 / 491 = 86 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 100 : 327 / 491 = 66 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 101 : 391 / 491 = 79 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 102 : 142 / 492 = 28 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 103 : 332 / 492 = 67 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 104 : 135 / 491 = 27 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 105 : 159 / 491 = 32 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 106 : 178 / 492 = 36 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 107 : 232 / 491 = 47 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 108 : 322 / 492 = 65 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 109 : 226 / 490 = 46 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 110 : 363 / 491 = 73 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 111 : 413 / 492 = 83 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 112 : 449 / 492 = 91 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 113 : 353 / 491 = 71 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 114 : 374 / 491 = 76 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 115 : 348 / 492 = 70 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 116 : 377 / 491 = 76 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 117 : 342 / 492 = 69 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 118 : 318 / 490 = 64 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 119 : 437 / 492 = 88 %
[ Sun Jul 14 18:00:20 2024 ] Accuracy of 120 : 286 / 500 = 57 %
[ Sun Jul 14 18:00:20 2024 ] Training epoch: 11
[ Sun Jul 14 18:00:20 2024 ] 	Batch(0/6809) done. Loss: 0.8310  lr:0.010000
[ Sun Jul 14 18:00:43 2024 ] 	Batch(100/6809) done. Loss: 0.6421  lr:0.010000
[ Sun Jul 14 18:01:06 2024 ] 	Batch(200/6809) done. Loss: 0.9499  lr:0.010000
[ Sun Jul 14 18:01:29 2024 ] 	Batch(300/6809) done. Loss: 1.9120  lr:0.010000
[ Sun Jul 14 18:01:52 2024 ] 	Batch(400/6809) done. Loss: 1.1731  lr:0.010000
[ Sun Jul 14 18:02:15 2024 ] 
Training: Epoch [10/150], Step [499], Loss: 0.8713438510894775, Training Accuracy: 67.875
[ Sun Jul 14 18:02:15 2024 ] 	Batch(500/6809) done. Loss: 1.1833  lr:0.010000
[ Sun Jul 14 18:02:38 2024 ] 	Batch(600/6809) done. Loss: 2.1431  lr:0.010000
[ Sun Jul 14 18:03:01 2024 ] 	Batch(700/6809) done. Loss: 0.9818  lr:0.010000
[ Sun Jul 14 18:03:24 2024 ] 	Batch(800/6809) done. Loss: 0.8471  lr:0.010000
[ Sun Jul 14 18:03:47 2024 ] 	Batch(900/6809) done. Loss: 1.7608  lr:0.010000
[ Sun Jul 14 18:04:10 2024 ] 
Training: Epoch [10/150], Step [999], Loss: 1.0470832586288452, Training Accuracy: 68.375
[ Sun Jul 14 18:04:10 2024 ] 	Batch(1000/6809) done. Loss: 0.7361  lr:0.010000
[ Sun Jul 14 18:04:33 2024 ] 	Batch(1100/6809) done. Loss: 0.6913  lr:0.010000
[ Sun Jul 14 18:04:56 2024 ] 	Batch(1200/6809) done. Loss: 0.8373  lr:0.010000
[ Sun Jul 14 18:05:19 2024 ] 	Batch(1300/6809) done. Loss: 1.1923  lr:0.010000
[ Sun Jul 14 18:05:42 2024 ] 	Batch(1400/6809) done. Loss: 0.5633  lr:0.010000
[ Sun Jul 14 18:06:05 2024 ] 
Training: Epoch [10/150], Step [1499], Loss: 0.41777294874191284, Training Accuracy: 68.8
[ Sun Jul 14 18:06:05 2024 ] 	Batch(1500/6809) done. Loss: 0.7757  lr:0.010000
[ Sun Jul 14 18:06:28 2024 ] 	Batch(1600/6809) done. Loss: 1.5293  lr:0.010000
[ Sun Jul 14 18:06:51 2024 ] 	Batch(1700/6809) done. Loss: 1.2056  lr:0.010000
[ Sun Jul 14 18:07:14 2024 ] 	Batch(1800/6809) done. Loss: 1.0577  lr:0.010000
[ Sun Jul 14 18:07:37 2024 ] 	Batch(1900/6809) done. Loss: 1.1801  lr:0.010000
[ Sun Jul 14 18:08:00 2024 ] 
Training: Epoch [10/150], Step [1999], Loss: 0.4443044066429138, Training Accuracy: 68.94375000000001
[ Sun Jul 14 18:08:00 2024 ] 	Batch(2000/6809) done. Loss: 2.1200  lr:0.010000
[ Sun Jul 14 18:08:23 2024 ] 	Batch(2100/6809) done. Loss: 0.5740  lr:0.010000
[ Sun Jul 14 18:08:46 2024 ] 	Batch(2200/6809) done. Loss: 2.0994  lr:0.010000
[ Sun Jul 14 18:09:09 2024 ] 	Batch(2300/6809) done. Loss: 1.0643  lr:0.010000
[ Sun Jul 14 18:09:32 2024 ] 	Batch(2400/6809) done. Loss: 1.3741  lr:0.010000
[ Sun Jul 14 18:09:55 2024 ] 
Training: Epoch [10/150], Step [2499], Loss: 1.8345657587051392, Training Accuracy: 68.95
[ Sun Jul 14 18:09:55 2024 ] 	Batch(2500/6809) done. Loss: 0.4449  lr:0.010000
[ Sun Jul 14 18:10:18 2024 ] 	Batch(2600/6809) done. Loss: 1.6501  lr:0.010000
[ Sun Jul 14 18:10:41 2024 ] 	Batch(2700/6809) done. Loss: 0.9656  lr:0.010000
[ Sun Jul 14 18:11:04 2024 ] 	Batch(2800/6809) done. Loss: 0.3230  lr:0.010000
[ Sun Jul 14 18:11:27 2024 ] 	Batch(2900/6809) done. Loss: 0.4827  lr:0.010000
[ Sun Jul 14 18:11:50 2024 ] 
Training: Epoch [10/150], Step [2999], Loss: 0.7997403740882874, Training Accuracy: 68.92083333333333
[ Sun Jul 14 18:11:50 2024 ] 	Batch(3000/6809) done. Loss: 0.7461  lr:0.010000
[ Sun Jul 14 18:12:13 2024 ] 	Batch(3100/6809) done. Loss: 0.5969  lr:0.010000
[ Sun Jul 14 18:12:36 2024 ] 	Batch(3200/6809) done. Loss: 2.2548  lr:0.010000
[ Sun Jul 14 18:12:59 2024 ] 	Batch(3300/6809) done. Loss: 1.4122  lr:0.010000
[ Sun Jul 14 18:13:22 2024 ] 	Batch(3400/6809) done. Loss: 0.5548  lr:0.010000
[ Sun Jul 14 18:13:45 2024 ] 
Training: Epoch [10/150], Step [3499], Loss: 1.558469295501709, Training Accuracy: 68.925
[ Sun Jul 14 18:13:45 2024 ] 	Batch(3500/6809) done. Loss: 0.8141  lr:0.010000
[ Sun Jul 14 18:14:09 2024 ] 	Batch(3600/6809) done. Loss: 0.5953  lr:0.010000
[ Sun Jul 14 18:14:32 2024 ] 	Batch(3700/6809) done. Loss: 0.5393  lr:0.010000
[ Sun Jul 14 18:14:54 2024 ] 	Batch(3800/6809) done. Loss: 1.3268  lr:0.010000
[ Sun Jul 14 18:15:17 2024 ] 	Batch(3900/6809) done. Loss: 0.6991  lr:0.010000
[ Sun Jul 14 18:15:40 2024 ] 
Training: Epoch [10/150], Step [3999], Loss: 0.3656178414821625, Training Accuracy: 68.778125
[ Sun Jul 14 18:15:40 2024 ] 	Batch(4000/6809) done. Loss: 1.6453  lr:0.010000
[ Sun Jul 14 18:16:03 2024 ] 	Batch(4100/6809) done. Loss: 1.8337  lr:0.010000
[ Sun Jul 14 18:16:25 2024 ] 	Batch(4200/6809) done. Loss: 0.6125  lr:0.010000
[ Sun Jul 14 18:16:48 2024 ] 	Batch(4300/6809) done. Loss: 0.6758  lr:0.010000
[ Sun Jul 14 18:17:11 2024 ] 	Batch(4400/6809) done. Loss: 0.5827  lr:0.010000
[ Sun Jul 14 18:17:35 2024 ] 
Training: Epoch [10/150], Step [4499], Loss: 1.453783392906189, Training Accuracy: 68.75277777777778
[ Sun Jul 14 18:17:35 2024 ] 	Batch(4500/6809) done. Loss: 0.5228  lr:0.010000
[ Sun Jul 14 18:17:58 2024 ] 	Batch(4600/6809) done. Loss: 1.2347  lr:0.010000
[ Sun Jul 14 18:18:21 2024 ] 	Batch(4700/6809) done. Loss: 1.2144  lr:0.010000
[ Sun Jul 14 18:18:44 2024 ] 	Batch(4800/6809) done. Loss: 1.0978  lr:0.010000
[ Sun Jul 14 18:19:07 2024 ] 	Batch(4900/6809) done. Loss: 0.2380  lr:0.010000
[ Sun Jul 14 18:19:30 2024 ] 
Training: Epoch [10/150], Step [4999], Loss: 0.5568263530731201, Training Accuracy: 68.66499999999999
[ Sun Jul 14 18:19:30 2024 ] 	Batch(5000/6809) done. Loss: 0.5920  lr:0.010000
[ Sun Jul 14 18:19:53 2024 ] 	Batch(5100/6809) done. Loss: 0.6817  lr:0.010000
[ Sun Jul 14 18:20:16 2024 ] 	Batch(5200/6809) done. Loss: 0.4244  lr:0.010000
[ Sun Jul 14 18:20:38 2024 ] 	Batch(5300/6809) done. Loss: 0.2187  lr:0.010000
[ Sun Jul 14 18:21:01 2024 ] 	Batch(5400/6809) done. Loss: 1.2910  lr:0.010000
[ Sun Jul 14 18:21:24 2024 ] 
Training: Epoch [10/150], Step [5499], Loss: 0.4133862257003784, Training Accuracy: 68.73636363636363
[ Sun Jul 14 18:21:24 2024 ] 	Batch(5500/6809) done. Loss: 0.9427  lr:0.010000
[ Sun Jul 14 18:21:46 2024 ] 	Batch(5600/6809) done. Loss: 1.1536  lr:0.010000
[ Sun Jul 14 18:22:09 2024 ] 	Batch(5700/6809) done. Loss: 0.9883  lr:0.010000
[ Sun Jul 14 18:22:32 2024 ] 	Batch(5800/6809) done. Loss: 0.3608  lr:0.010000
[ Sun Jul 14 18:22:54 2024 ] 	Batch(5900/6809) done. Loss: 2.2993  lr:0.010000
[ Sun Jul 14 18:23:17 2024 ] 
Training: Epoch [10/150], Step [5999], Loss: 1.6173096895217896, Training Accuracy: 68.71666666666667
[ Sun Jul 14 18:23:17 2024 ] 	Batch(6000/6809) done. Loss: 0.7916  lr:0.010000
[ Sun Jul 14 18:23:40 2024 ] 	Batch(6100/6809) done. Loss: 1.2449  lr:0.010000
[ Sun Jul 14 18:24:02 2024 ] 	Batch(6200/6809) done. Loss: 0.9358  lr:0.010000
[ Sun Jul 14 18:24:25 2024 ] 	Batch(6300/6809) done. Loss: 0.6478  lr:0.010000
[ Sun Jul 14 18:24:48 2024 ] 	Batch(6400/6809) done. Loss: 0.6147  lr:0.010000
[ Sun Jul 14 18:25:10 2024 ] 
Training: Epoch [10/150], Step [6499], Loss: 0.7338171601295471, Training Accuracy: 68.74230769230769
[ Sun Jul 14 18:25:10 2024 ] 	Batch(6500/6809) done. Loss: 1.3582  lr:0.010000
[ Sun Jul 14 18:25:33 2024 ] 	Batch(6600/6809) done. Loss: 0.9822  lr:0.010000
[ Sun Jul 14 18:25:56 2024 ] 	Batch(6700/6809) done. Loss: 0.8365  lr:0.010000
[ Sun Jul 14 18:26:18 2024 ] 	Batch(6800/6809) done. Loss: 1.9200  lr:0.010000
[ Sun Jul 14 18:26:20 2024 ] 	Mean training loss: 1.0530.
[ Sun Jul 14 18:26:20 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul 14 18:26:20 2024 ] Training epoch: 12
[ Sun Jul 14 18:26:21 2024 ] 	Batch(0/6809) done. Loss: 0.8521  lr:0.010000
[ Sun Jul 14 18:26:44 2024 ] 	Batch(100/6809) done. Loss: 1.4539  lr:0.010000
[ Sun Jul 14 18:27:07 2024 ] 	Batch(200/6809) done. Loss: 0.6540  lr:0.010000
[ Sun Jul 14 18:27:30 2024 ] 	Batch(300/6809) done. Loss: 1.0619  lr:0.010000
[ Sun Jul 14 18:27:53 2024 ] 	Batch(400/6809) done. Loss: 1.7357  lr:0.010000
[ Sun Jul 14 18:28:16 2024 ] 
Training: Epoch [11/150], Step [499], Loss: 1.121876835823059, Training Accuracy: 69.475
[ Sun Jul 14 18:28:16 2024 ] 	Batch(500/6809) done. Loss: 0.4946  lr:0.010000
[ Sun Jul 14 18:28:39 2024 ] 	Batch(600/6809) done. Loss: 1.2526  lr:0.010000
[ Sun Jul 14 18:29:02 2024 ] 	Batch(700/6809) done. Loss: 0.8677  lr:0.010000
[ Sun Jul 14 18:29:25 2024 ] 	Batch(800/6809) done. Loss: 0.9025  lr:0.010000
[ Sun Jul 14 18:29:48 2024 ] 	Batch(900/6809) done. Loss: 0.6292  lr:0.010000
[ Sun Jul 14 18:30:11 2024 ] 
Training: Epoch [11/150], Step [999], Loss: 1.7780853509902954, Training Accuracy: 69.8125
[ Sun Jul 14 18:30:11 2024 ] 	Batch(1000/6809) done. Loss: 0.6550  lr:0.010000
[ Sun Jul 14 18:30:34 2024 ] 	Batch(1100/6809) done. Loss: 1.4519  lr:0.010000
[ Sun Jul 14 18:30:57 2024 ] 	Batch(1200/6809) done. Loss: 1.1508  lr:0.010000
[ Sun Jul 14 18:31:20 2024 ] 	Batch(1300/6809) done. Loss: 0.7564  lr:0.010000
[ Sun Jul 14 18:31:43 2024 ] 	Batch(1400/6809) done. Loss: 1.4432  lr:0.010000
[ Sun Jul 14 18:32:06 2024 ] 
Training: Epoch [11/150], Step [1499], Loss: 1.1411455869674683, Training Accuracy: 69.75
[ Sun Jul 14 18:32:06 2024 ] 	Batch(1500/6809) done. Loss: 0.9455  lr:0.010000
[ Sun Jul 14 18:32:29 2024 ] 	Batch(1600/6809) done. Loss: 0.6282  lr:0.010000
[ Sun Jul 14 18:32:52 2024 ] 	Batch(1700/6809) done. Loss: 0.6201  lr:0.010000
[ Sun Jul 14 18:33:15 2024 ] 	Batch(1800/6809) done. Loss: 0.8253  lr:0.010000
[ Sun Jul 14 18:33:38 2024 ] 	Batch(1900/6809) done. Loss: 0.7297  lr:0.010000
[ Sun Jul 14 18:34:01 2024 ] 
Training: Epoch [11/150], Step [1999], Loss: 0.8506301045417786, Training Accuracy: 70.06875000000001
[ Sun Jul 14 18:34:01 2024 ] 	Batch(2000/6809) done. Loss: 0.6763  lr:0.010000
[ Sun Jul 14 18:34:24 2024 ] 	Batch(2100/6809) done. Loss: 0.8983  lr:0.010000
[ Sun Jul 14 18:34:47 2024 ] 	Batch(2200/6809) done. Loss: 0.9937  lr:0.010000
[ Sun Jul 14 18:35:10 2024 ] 	Batch(2300/6809) done. Loss: 1.2493  lr:0.010000
[ Sun Jul 14 18:35:34 2024 ] 	Batch(2400/6809) done. Loss: 0.7894  lr:0.010000
[ Sun Jul 14 18:35:56 2024 ] 
Training: Epoch [11/150], Step [2499], Loss: 0.7153527736663818, Training Accuracy: 70.075
[ Sun Jul 14 18:35:57 2024 ] 	Batch(2500/6809) done. Loss: 0.7673  lr:0.010000
[ Sun Jul 14 18:36:20 2024 ] 	Batch(2600/6809) done. Loss: 0.7245  lr:0.010000
[ Sun Jul 14 18:36:43 2024 ] 	Batch(2700/6809) done. Loss: 0.7551  lr:0.010000
[ Sun Jul 14 18:37:06 2024 ] 	Batch(2800/6809) done. Loss: 1.0379  lr:0.010000
[ Sun Jul 14 18:37:29 2024 ] 	Batch(2900/6809) done. Loss: 1.1312  lr:0.010000
[ Sun Jul 14 18:37:51 2024 ] 
Training: Epoch [11/150], Step [2999], Loss: 0.8703285455703735, Training Accuracy: 70.04583333333333
[ Sun Jul 14 18:37:52 2024 ] 	Batch(3000/6809) done. Loss: 0.4185  lr:0.010000
[ Sun Jul 14 18:38:15 2024 ] 	Batch(3100/6809) done. Loss: 1.7923  lr:0.010000
[ Sun Jul 14 18:38:38 2024 ] 	Batch(3200/6809) done. Loss: 1.8196  lr:0.010000
[ Sun Jul 14 18:39:01 2024 ] 	Batch(3300/6809) done. Loss: 0.7928  lr:0.010000
[ Sun Jul 14 18:39:24 2024 ] 	Batch(3400/6809) done. Loss: 0.5417  lr:0.010000
[ Sun Jul 14 18:39:46 2024 ] 
Training: Epoch [11/150], Step [3499], Loss: 1.7937731742858887, Training Accuracy: 69.95714285714286
[ Sun Jul 14 18:39:47 2024 ] 	Batch(3500/6809) done. Loss: 2.2448  lr:0.010000
[ Sun Jul 14 18:40:10 2024 ] 	Batch(3600/6809) done. Loss: 0.2495  lr:0.010000
[ Sun Jul 14 18:40:33 2024 ] 	Batch(3700/6809) done. Loss: 0.6470  lr:0.010000
[ Sun Jul 14 18:40:56 2024 ] 	Batch(3800/6809) done. Loss: 2.0516  lr:0.010000
[ Sun Jul 14 18:41:19 2024 ] 	Batch(3900/6809) done. Loss: 1.1498  lr:0.010000
[ Sun Jul 14 18:41:41 2024 ] 
Training: Epoch [11/150], Step [3999], Loss: 1.062564730644226, Training Accuracy: 69.984375
[ Sun Jul 14 18:41:42 2024 ] 	Batch(4000/6809) done. Loss: 1.6267  lr:0.010000
[ Sun Jul 14 18:42:05 2024 ] 	Batch(4100/6809) done. Loss: 0.1715  lr:0.010000
[ Sun Jul 14 18:42:27 2024 ] 	Batch(4200/6809) done. Loss: 0.5593  lr:0.010000
[ Sun Jul 14 18:42:50 2024 ] 	Batch(4300/6809) done. Loss: 1.0848  lr:0.010000
[ Sun Jul 14 18:43:13 2024 ] 	Batch(4400/6809) done. Loss: 0.4772  lr:0.010000
[ Sun Jul 14 18:43:35 2024 ] 
Training: Epoch [11/150], Step [4499], Loss: 0.6142247915267944, Training Accuracy: 69.78611111111111
[ Sun Jul 14 18:43:35 2024 ] 	Batch(4500/6809) done. Loss: 1.4076  lr:0.010000
[ Sun Jul 14 18:43:58 2024 ] 	Batch(4600/6809) done. Loss: 1.2072  lr:0.010000
[ Sun Jul 14 18:44:21 2024 ] 	Batch(4700/6809) done. Loss: 1.1196  lr:0.010000
[ Sun Jul 14 18:44:43 2024 ] 	Batch(4800/6809) done. Loss: 0.6008  lr:0.010000
[ Sun Jul 14 18:45:06 2024 ] 	Batch(4900/6809) done. Loss: 1.4325  lr:0.010000
[ Sun Jul 14 18:45:28 2024 ] 
Training: Epoch [11/150], Step [4999], Loss: 1.5651445388793945, Training Accuracy: 69.6275
[ Sun Jul 14 18:45:28 2024 ] 	Batch(5000/6809) done. Loss: 0.3685  lr:0.010000
[ Sun Jul 14 18:45:52 2024 ] 	Batch(5100/6809) done. Loss: 2.4865  lr:0.010000
[ Sun Jul 14 18:46:15 2024 ] 	Batch(5200/6809) done. Loss: 1.4395  lr:0.010000
[ Sun Jul 14 18:46:38 2024 ] 	Batch(5300/6809) done. Loss: 0.9541  lr:0.010000
[ Sun Jul 14 18:47:00 2024 ] 	Batch(5400/6809) done. Loss: 0.9498  lr:0.010000
[ Sun Jul 14 18:47:23 2024 ] 
Training: Epoch [11/150], Step [5499], Loss: 1.9090219736099243, Training Accuracy: 69.61818181818182
[ Sun Jul 14 18:47:23 2024 ] 	Batch(5500/6809) done. Loss: 0.3151  lr:0.010000
[ Sun Jul 14 18:47:46 2024 ] 	Batch(5600/6809) done. Loss: 1.5615  lr:0.010000
[ Sun Jul 14 18:48:08 2024 ] 	Batch(5700/6809) done. Loss: 0.8541  lr:0.010000
[ Sun Jul 14 18:48:31 2024 ] 	Batch(5800/6809) done. Loss: 0.5290  lr:0.010000
[ Sun Jul 14 18:48:53 2024 ] 	Batch(5900/6809) done. Loss: 1.1546  lr:0.010000
[ Sun Jul 14 18:49:16 2024 ] 
Training: Epoch [11/150], Step [5999], Loss: 1.2583754062652588, Training Accuracy: 69.65625
[ Sun Jul 14 18:49:16 2024 ] 	Batch(6000/6809) done. Loss: 0.8650  lr:0.010000
[ Sun Jul 14 18:49:39 2024 ] 	Batch(6100/6809) done. Loss: 0.2176  lr:0.010000
[ Sun Jul 14 18:50:02 2024 ] 	Batch(6200/6809) done. Loss: 0.7452  lr:0.010000
[ Sun Jul 14 18:50:25 2024 ] 	Batch(6300/6809) done. Loss: 1.4455  lr:0.010000
[ Sun Jul 14 18:50:48 2024 ] 	Batch(6400/6809) done. Loss: 0.8848  lr:0.010000
[ Sun Jul 14 18:51:11 2024 ] 
Training: Epoch [11/150], Step [6499], Loss: 1.8367997407913208, Training Accuracy: 69.65384615384616
[ Sun Jul 14 18:51:12 2024 ] 	Batch(6500/6809) done. Loss: 0.6770  lr:0.010000
[ Sun Jul 14 18:51:34 2024 ] 	Batch(6600/6809) done. Loss: 1.9471  lr:0.010000
[ Sun Jul 14 18:51:57 2024 ] 	Batch(6700/6809) done. Loss: 0.4496  lr:0.010000
[ Sun Jul 14 18:52:20 2024 ] 	Batch(6800/6809) done. Loss: 0.5269  lr:0.010000
[ Sun Jul 14 18:52:21 2024 ] 	Mean training loss: 1.0163.
[ Sun Jul 14 18:52:21 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sun Jul 14 18:52:22 2024 ] Training epoch: 13
[ Sun Jul 14 18:52:22 2024 ] 	Batch(0/6809) done. Loss: 0.7533  lr:0.010000
[ Sun Jul 14 18:52:45 2024 ] 	Batch(100/6809) done. Loss: 0.7316  lr:0.010000
[ Sun Jul 14 18:53:07 2024 ] 	Batch(200/6809) done. Loss: 0.2066  lr:0.010000
[ Sun Jul 14 18:53:30 2024 ] 	Batch(300/6809) done. Loss: 1.5233  lr:0.010000
[ Sun Jul 14 18:53:52 2024 ] 	Batch(400/6809) done. Loss: 1.7338  lr:0.010000
[ Sun Jul 14 18:54:15 2024 ] 
Training: Epoch [12/150], Step [499], Loss: 0.8403420448303223, Training Accuracy: 70.3
[ Sun Jul 14 18:54:15 2024 ] 	Batch(500/6809) done. Loss: 1.4740  lr:0.010000
[ Sun Jul 14 18:54:38 2024 ] 	Batch(600/6809) done. Loss: 0.6378  lr:0.010000
[ Sun Jul 14 18:55:00 2024 ] 	Batch(700/6809) done. Loss: 1.7066  lr:0.010000
[ Sun Jul 14 18:55:23 2024 ] 	Batch(800/6809) done. Loss: 1.0843  lr:0.010000
[ Sun Jul 14 18:55:45 2024 ] 	Batch(900/6809) done. Loss: 1.2655  lr:0.010000
[ Sun Jul 14 18:56:08 2024 ] 
Training: Epoch [12/150], Step [999], Loss: 0.7670910954475403, Training Accuracy: 70.8875
[ Sun Jul 14 18:56:08 2024 ] 	Batch(1000/6809) done. Loss: 0.7679  lr:0.010000
[ Sun Jul 14 18:56:30 2024 ] 	Batch(1100/6809) done. Loss: 1.2626  lr:0.010000
[ Sun Jul 14 18:56:53 2024 ] 	Batch(1200/6809) done. Loss: 0.6835  lr:0.010000
[ Sun Jul 14 18:57:17 2024 ] 	Batch(1300/6809) done. Loss: 1.0064  lr:0.010000
[ Sun Jul 14 18:57:40 2024 ] 	Batch(1400/6809) done. Loss: 1.3404  lr:0.010000
[ Sun Jul 14 18:58:03 2024 ] 
Training: Epoch [12/150], Step [1499], Loss: 0.6586582660675049, Training Accuracy: 70.70833333333333
[ Sun Jul 14 18:58:03 2024 ] 	Batch(1500/6809) done. Loss: 0.9324  lr:0.010000
[ Sun Jul 14 18:58:26 2024 ] 	Batch(1600/6809) done. Loss: 1.1510  lr:0.010000
[ Sun Jul 14 18:58:50 2024 ] 	Batch(1700/6809) done. Loss: 3.5161  lr:0.010000
[ Sun Jul 14 18:59:14 2024 ] 	Batch(1800/6809) done. Loss: 1.5310  lr:0.010000
[ Sun Jul 14 18:59:36 2024 ] 	Batch(1900/6809) done. Loss: 0.9311  lr:0.010000
[ Sun Jul 14 18:59:59 2024 ] 
Training: Epoch [12/150], Step [1999], Loss: 0.3926980197429657, Training Accuracy: 71.16875
[ Sun Jul 14 18:59:59 2024 ] 	Batch(2000/6809) done. Loss: 1.5323  lr:0.010000
[ Sun Jul 14 19:00:22 2024 ] 	Batch(2100/6809) done. Loss: 0.3307  lr:0.010000
[ Sun Jul 14 19:00:45 2024 ] 	Batch(2200/6809) done. Loss: 0.6421  lr:0.010000
[ Sun Jul 14 19:01:07 2024 ] 	Batch(2300/6809) done. Loss: 0.8985  lr:0.010000
[ Sun Jul 14 19:01:30 2024 ] 	Batch(2400/6809) done. Loss: 1.7000  lr:0.010000
[ Sun Jul 14 19:01:53 2024 ] 
Training: Epoch [12/150], Step [2499], Loss: 0.610581636428833, Training Accuracy: 71.07
[ Sun Jul 14 19:01:53 2024 ] 	Batch(2500/6809) done. Loss: 1.2447  lr:0.010000
[ Sun Jul 14 19:02:16 2024 ] 	Batch(2600/6809) done. Loss: 1.4698  lr:0.010000
[ Sun Jul 14 19:02:38 2024 ] 	Batch(2700/6809) done. Loss: 0.7577  lr:0.010000
[ Sun Jul 14 19:03:01 2024 ] 	Batch(2800/6809) done. Loss: 0.9510  lr:0.010000
[ Sun Jul 14 19:03:24 2024 ] 	Batch(2900/6809) done. Loss: 1.0338  lr:0.010000
[ Sun Jul 14 19:03:46 2024 ] 
Training: Epoch [12/150], Step [2999], Loss: 0.5368436574935913, Training Accuracy: 71.23333333333333
[ Sun Jul 14 19:03:47 2024 ] 	Batch(3000/6809) done. Loss: 1.1056  lr:0.010000
[ Sun Jul 14 19:04:09 2024 ] 	Batch(3100/6809) done. Loss: 0.6847  lr:0.010000
[ Sun Jul 14 19:04:32 2024 ] 	Batch(3200/6809) done. Loss: 0.7111  lr:0.010000
[ Sun Jul 14 19:04:55 2024 ] 	Batch(3300/6809) done. Loss: 0.9749  lr:0.010000
[ Sun Jul 14 19:05:17 2024 ] 	Batch(3400/6809) done. Loss: 0.5359  lr:0.010000
[ Sun Jul 14 19:05:40 2024 ] 
Training: Epoch [12/150], Step [3499], Loss: 0.8738513588905334, Training Accuracy: 71.33928571428572
[ Sun Jul 14 19:05:40 2024 ] 	Batch(3500/6809) done. Loss: 0.6581  lr:0.010000
[ Sun Jul 14 19:06:03 2024 ] 	Batch(3600/6809) done. Loss: 0.4219  lr:0.010000
[ Sun Jul 14 19:06:26 2024 ] 	Batch(3700/6809) done. Loss: 2.0890  lr:0.010000
[ Sun Jul 14 19:06:48 2024 ] 	Batch(3800/6809) done. Loss: 1.4329  lr:0.010000
[ Sun Jul 14 19:07:11 2024 ] 	Batch(3900/6809) done. Loss: 0.9893  lr:0.010000
[ Sun Jul 14 19:07:34 2024 ] 
Training: Epoch [12/150], Step [3999], Loss: 0.20129534602165222, Training Accuracy: 71.22500000000001
[ Sun Jul 14 19:07:35 2024 ] 	Batch(4000/6809) done. Loss: 0.9460  lr:0.010000
[ Sun Jul 14 19:07:57 2024 ] 	Batch(4100/6809) done. Loss: 0.6806  lr:0.010000
[ Sun Jul 14 19:08:20 2024 ] 	Batch(4200/6809) done. Loss: 1.0766  lr:0.010000
[ Sun Jul 14 19:08:43 2024 ] 	Batch(4300/6809) done. Loss: 0.2024  lr:0.010000
[ Sun Jul 14 19:09:07 2024 ] 	Batch(4400/6809) done. Loss: 0.9028  lr:0.010000
[ Sun Jul 14 19:09:30 2024 ] 
Training: Epoch [12/150], Step [4499], Loss: 1.102552890777588, Training Accuracy: 71.18333333333334
[ Sun Jul 14 19:09:30 2024 ] 	Batch(4500/6809) done. Loss: 0.9882  lr:0.010000
[ Sun Jul 14 19:09:54 2024 ] 	Batch(4600/6809) done. Loss: 0.3198  lr:0.010000
[ Sun Jul 14 19:10:17 2024 ] 	Batch(4700/6809) done. Loss: 0.6849  lr:0.010000
[ Sun Jul 14 19:10:41 2024 ] 	Batch(4800/6809) done. Loss: 1.9929  lr:0.010000
[ Sun Jul 14 19:11:04 2024 ] 	Batch(4900/6809) done. Loss: 1.7375  lr:0.010000
[ Sun Jul 14 19:11:28 2024 ] 
Training: Epoch [12/150], Step [4999], Loss: 0.9177291989326477, Training Accuracy: 71.2075
[ Sun Jul 14 19:11:28 2024 ] 	Batch(5000/6809) done. Loss: 0.2961  lr:0.010000
[ Sun Jul 14 19:11:51 2024 ] 	Batch(5100/6809) done. Loss: 1.8034  lr:0.010000
[ Sun Jul 14 19:12:14 2024 ] 	Batch(5200/6809) done. Loss: 0.8402  lr:0.010000
[ Sun Jul 14 19:12:36 2024 ] 	Batch(5300/6809) done. Loss: 0.8667  lr:0.010000
[ Sun Jul 14 19:12:59 2024 ] 	Batch(5400/6809) done. Loss: 0.0272  lr:0.010000
[ Sun Jul 14 19:13:21 2024 ] 
Training: Epoch [12/150], Step [5499], Loss: 1.1657685041427612, Training Accuracy: 71.29318181818182
[ Sun Jul 14 19:13:22 2024 ] 	Batch(5500/6809) done. Loss: 0.5982  lr:0.010000
[ Sun Jul 14 19:13:44 2024 ] 	Batch(5600/6809) done. Loss: 1.0663  lr:0.010000
[ Sun Jul 14 19:14:07 2024 ] 	Batch(5700/6809) done. Loss: 0.6926  lr:0.010000
[ Sun Jul 14 19:14:30 2024 ] 	Batch(5800/6809) done. Loss: 1.1888  lr:0.010000
[ Sun Jul 14 19:14:52 2024 ] 	Batch(5900/6809) done. Loss: 1.2637  lr:0.010000
[ Sun Jul 14 19:15:15 2024 ] 
Training: Epoch [12/150], Step [5999], Loss: 0.5864599347114563, Training Accuracy: 71.30833333333332
[ Sun Jul 14 19:15:15 2024 ] 	Batch(6000/6809) done. Loss: 1.5123  lr:0.010000
[ Sun Jul 14 19:15:38 2024 ] 	Batch(6100/6809) done. Loss: 1.0741  lr:0.010000
[ Sun Jul 14 19:16:01 2024 ] 	Batch(6200/6809) done. Loss: 1.6953  lr:0.010000
[ Sun Jul 14 19:16:23 2024 ] 	Batch(6300/6809) done. Loss: 1.0941  lr:0.010000
[ Sun Jul 14 19:16:46 2024 ] 	Batch(6400/6809) done. Loss: 1.1980  lr:0.010000
[ Sun Jul 14 19:17:09 2024 ] 
Training: Epoch [12/150], Step [6499], Loss: 0.8149310946464539, Training Accuracy: 71.31346153846154
[ Sun Jul 14 19:17:09 2024 ] 	Batch(6500/6809) done. Loss: 1.7850  lr:0.010000
[ Sun Jul 14 19:17:32 2024 ] 	Batch(6600/6809) done. Loss: 0.9484  lr:0.010000
[ Sun Jul 14 19:17:55 2024 ] 	Batch(6700/6809) done. Loss: 0.7469  lr:0.010000
[ Sun Jul 14 19:18:17 2024 ] 	Batch(6800/6809) done. Loss: 0.9090  lr:0.010000
[ Sun Jul 14 19:18:19 2024 ] 	Mean training loss: 0.9667.
[ Sun Jul 14 19:18:19 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 19:18:19 2024 ] Training epoch: 14
[ Sun Jul 14 19:18:20 2024 ] 	Batch(0/6809) done. Loss: 0.9040  lr:0.010000
[ Sun Jul 14 19:18:43 2024 ] 	Batch(100/6809) done. Loss: 1.7054  lr:0.010000
[ Sun Jul 14 19:19:06 2024 ] 	Batch(200/6809) done. Loss: 1.0219  lr:0.010000
[ Sun Jul 14 19:19:28 2024 ] 	Batch(300/6809) done. Loss: 0.2533  lr:0.010000
[ Sun Jul 14 19:19:51 2024 ] 	Batch(400/6809) done. Loss: 0.3474  lr:0.010000
[ Sun Jul 14 19:20:14 2024 ] 
Training: Epoch [13/150], Step [499], Loss: 0.6489114165306091, Training Accuracy: 72.225
[ Sun Jul 14 19:20:14 2024 ] 	Batch(500/6809) done. Loss: 0.3236  lr:0.010000
[ Sun Jul 14 19:20:37 2024 ] 	Batch(600/6809) done. Loss: 0.7255  lr:0.010000
[ Sun Jul 14 19:20:59 2024 ] 	Batch(700/6809) done. Loss: 0.4357  lr:0.010000
[ Sun Jul 14 19:21:22 2024 ] 	Batch(800/6809) done. Loss: 0.2241  lr:0.010000
[ Sun Jul 14 19:21:45 2024 ] 	Batch(900/6809) done. Loss: 1.4315  lr:0.010000
[ Sun Jul 14 19:22:08 2024 ] 
Training: Epoch [13/150], Step [999], Loss: 1.829620599746704, Training Accuracy: 72.5
[ Sun Jul 14 19:22:08 2024 ] 	Batch(1000/6809) done. Loss: 1.1068  lr:0.010000
[ Sun Jul 14 19:22:31 2024 ] 	Batch(1100/6809) done. Loss: 0.7114  lr:0.010000
[ Sun Jul 14 19:22:55 2024 ] 	Batch(1200/6809) done. Loss: 0.1737  lr:0.010000
[ Sun Jul 14 19:23:18 2024 ] 	Batch(1300/6809) done. Loss: 0.7647  lr:0.010000
[ Sun Jul 14 19:23:41 2024 ] 	Batch(1400/6809) done. Loss: 1.1839  lr:0.010000
[ Sun Jul 14 19:24:04 2024 ] 
Training: Epoch [13/150], Step [1499], Loss: 0.8107207417488098, Training Accuracy: 72.375
[ Sun Jul 14 19:24:04 2024 ] 	Batch(1500/6809) done. Loss: 1.5036  lr:0.010000
[ Sun Jul 14 19:24:27 2024 ] 	Batch(1600/6809) done. Loss: 0.2124  lr:0.010000
[ Sun Jul 14 19:24:50 2024 ] 	Batch(1700/6809) done. Loss: 0.7539  lr:0.010000
[ Sun Jul 14 19:25:13 2024 ] 	Batch(1800/6809) done. Loss: 0.5489  lr:0.010000
[ Sun Jul 14 19:25:36 2024 ] 	Batch(1900/6809) done. Loss: 0.4925  lr:0.010000
[ Sun Jul 14 19:25:59 2024 ] 
Training: Epoch [13/150], Step [1999], Loss: 1.7798069715499878, Training Accuracy: 72.30625
[ Sun Jul 14 19:26:00 2024 ] 	Batch(2000/6809) done. Loss: 0.9535  lr:0.010000
[ Sun Jul 14 19:26:23 2024 ] 	Batch(2100/6809) done. Loss: 1.2493  lr:0.010000
[ Sun Jul 14 19:26:46 2024 ] 	Batch(2200/6809) done. Loss: 1.3371  lr:0.010000
[ Sun Jul 14 19:27:09 2024 ] 	Batch(2300/6809) done. Loss: 1.4406  lr:0.010000
[ Sun Jul 14 19:27:32 2024 ] 	Batch(2400/6809) done. Loss: 1.0142  lr:0.010000
[ Sun Jul 14 19:27:55 2024 ] 
Training: Epoch [13/150], Step [2499], Loss: 2.4756107330322266, Training Accuracy: 72.275
[ Sun Jul 14 19:27:55 2024 ] 	Batch(2500/6809) done. Loss: 0.8428  lr:0.010000
[ Sun Jul 14 19:28:18 2024 ] 	Batch(2600/6809) done. Loss: 1.2256  lr:0.010000
[ Sun Jul 14 19:28:40 2024 ] 	Batch(2700/6809) done. Loss: 1.5687  lr:0.010000
[ Sun Jul 14 19:29:03 2024 ] 	Batch(2800/6809) done. Loss: 0.8878  lr:0.010000
[ Sun Jul 14 19:29:26 2024 ] 	Batch(2900/6809) done. Loss: 0.9997  lr:0.010000
[ Sun Jul 14 19:29:48 2024 ] 
Training: Epoch [13/150], Step [2999], Loss: 0.9137284755706787, Training Accuracy: 72.19166666666666
[ Sun Jul 14 19:29:48 2024 ] 	Batch(3000/6809) done. Loss: 0.7288  lr:0.010000
[ Sun Jul 14 19:30:11 2024 ] 	Batch(3100/6809) done. Loss: 1.1680  lr:0.010000
[ Sun Jul 14 19:30:34 2024 ] 	Batch(3200/6809) done. Loss: 0.8103  lr:0.010000
[ Sun Jul 14 19:30:57 2024 ] 	Batch(3300/6809) done. Loss: 0.6182  lr:0.010000
[ Sun Jul 14 19:31:19 2024 ] 	Batch(3400/6809) done. Loss: 0.7420  lr:0.010000
[ Sun Jul 14 19:31:42 2024 ] 
Training: Epoch [13/150], Step [3499], Loss: 0.6961469054222107, Training Accuracy: 72.04642857142856
[ Sun Jul 14 19:31:42 2024 ] 	Batch(3500/6809) done. Loss: 0.6540  lr:0.010000
[ Sun Jul 14 19:32:05 2024 ] 	Batch(3600/6809) done. Loss: 0.7072  lr:0.010000
[ Sun Jul 14 19:32:28 2024 ] 	Batch(3700/6809) done. Loss: 0.9663  lr:0.010000
[ Sun Jul 14 19:32:50 2024 ] 	Batch(3800/6809) done. Loss: 1.1776  lr:0.010000
[ Sun Jul 14 19:33:13 2024 ] 	Batch(3900/6809) done. Loss: 0.5261  lr:0.010000
[ Sun Jul 14 19:33:36 2024 ] 
Training: Epoch [13/150], Step [3999], Loss: 1.1990134716033936, Training Accuracy: 72.1625
[ Sun Jul 14 19:33:36 2024 ] 	Batch(4000/6809) done. Loss: 1.8680  lr:0.010000
[ Sun Jul 14 19:33:59 2024 ] 	Batch(4100/6809) done. Loss: 1.2162  lr:0.010000
[ Sun Jul 14 19:34:21 2024 ] 	Batch(4200/6809) done. Loss: 0.5380  lr:0.010000
[ Sun Jul 14 19:34:44 2024 ] 	Batch(4300/6809) done. Loss: 1.1355  lr:0.010000
[ Sun Jul 14 19:35:07 2024 ] 	Batch(4400/6809) done. Loss: 0.3115  lr:0.010000
[ Sun Jul 14 19:35:30 2024 ] 
Training: Epoch [13/150], Step [4499], Loss: 0.8006852865219116, Training Accuracy: 72.14722222222221
[ Sun Jul 14 19:35:30 2024 ] 	Batch(4500/6809) done. Loss: 0.7668  lr:0.010000
[ Sun Jul 14 19:35:54 2024 ] 	Batch(4600/6809) done. Loss: 1.2215  lr:0.010000
[ Sun Jul 14 19:36:16 2024 ] 	Batch(4700/6809) done. Loss: 1.1319  lr:0.010000
[ Sun Jul 14 19:36:39 2024 ] 	Batch(4800/6809) done. Loss: 0.7393  lr:0.010000
[ Sun Jul 14 19:37:02 2024 ] 	Batch(4900/6809) done. Loss: 0.9612  lr:0.010000
[ Sun Jul 14 19:37:24 2024 ] 
Training: Epoch [13/150], Step [4999], Loss: 0.5573084950447083, Training Accuracy: 72.1725
[ Sun Jul 14 19:37:25 2024 ] 	Batch(5000/6809) done. Loss: 1.2496  lr:0.010000
[ Sun Jul 14 19:37:47 2024 ] 	Batch(5100/6809) done. Loss: 1.8137  lr:0.010000
[ Sun Jul 14 19:38:10 2024 ] 	Batch(5200/6809) done. Loss: 0.6688  lr:0.010000
[ Sun Jul 14 19:38:34 2024 ] 	Batch(5300/6809) done. Loss: 0.3263  lr:0.010000
[ Sun Jul 14 19:38:57 2024 ] 	Batch(5400/6809) done. Loss: 1.4558  lr:0.010000
[ Sun Jul 14 19:39:21 2024 ] 
Training: Epoch [13/150], Step [5499], Loss: 0.3576027452945709, Training Accuracy: 71.98863636363636
[ Sun Jul 14 19:39:21 2024 ] 	Batch(5500/6809) done. Loss: 0.2834  lr:0.010000
[ Sun Jul 14 19:39:44 2024 ] 	Batch(5600/6809) done. Loss: 0.6565  lr:0.010000
[ Sun Jul 14 19:40:08 2024 ] 	Batch(5700/6809) done. Loss: 1.2658  lr:0.010000
[ Sun Jul 14 19:40:31 2024 ] 	Batch(5800/6809) done. Loss: 1.0497  lr:0.010000
[ Sun Jul 14 19:40:55 2024 ] 	Batch(5900/6809) done. Loss: 0.9349  lr:0.010000
[ Sun Jul 14 19:41:18 2024 ] 
Training: Epoch [13/150], Step [5999], Loss: 0.16690391302108765, Training Accuracy: 72.02291666666667
[ Sun Jul 14 19:41:18 2024 ] 	Batch(6000/6809) done. Loss: 2.2192  lr:0.010000
[ Sun Jul 14 19:41:42 2024 ] 	Batch(6100/6809) done. Loss: 0.7266  lr:0.010000
[ Sun Jul 14 19:42:05 2024 ] 	Batch(6200/6809) done. Loss: 0.4534  lr:0.010000
[ Sun Jul 14 19:42:28 2024 ] 	Batch(6300/6809) done. Loss: 1.1823  lr:0.010000
[ Sun Jul 14 19:42:51 2024 ] 	Batch(6400/6809) done. Loss: 1.9059  lr:0.010000
[ Sun Jul 14 19:43:13 2024 ] 
Training: Epoch [13/150], Step [6499], Loss: 1.018480658531189, Training Accuracy: 72.06153846153846
[ Sun Jul 14 19:43:14 2024 ] 	Batch(6500/6809) done. Loss: 1.5990  lr:0.010000
[ Sun Jul 14 19:43:36 2024 ] 	Batch(6600/6809) done. Loss: 0.8765  lr:0.010000
[ Sun Jul 14 19:43:59 2024 ] 	Batch(6700/6809) done. Loss: 0.9298  lr:0.010000
[ Sun Jul 14 19:44:22 2024 ] 	Batch(6800/6809) done. Loss: 0.1482  lr:0.010000
[ Sun Jul 14 19:44:24 2024 ] 	Mean training loss: 0.9308.
[ Sun Jul 14 19:44:24 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 19:44:24 2024 ] Training epoch: 15
[ Sun Jul 14 19:44:24 2024 ] 	Batch(0/6809) done. Loss: 0.2999  lr:0.010000
[ Sun Jul 14 19:44:47 2024 ] 	Batch(100/6809) done. Loss: 0.6947  lr:0.010000
[ Sun Jul 14 19:45:10 2024 ] 	Batch(200/6809) done. Loss: 0.2288  lr:0.010000
[ Sun Jul 14 19:45:33 2024 ] 	Batch(300/6809) done. Loss: 0.5288  lr:0.010000
[ Sun Jul 14 19:45:55 2024 ] 	Batch(400/6809) done. Loss: 0.3952  lr:0.010000
[ Sun Jul 14 19:46:18 2024 ] 
Training: Epoch [14/150], Step [499], Loss: 0.44886425137519836, Training Accuracy: 72.975
[ Sun Jul 14 19:46:18 2024 ] 	Batch(500/6809) done. Loss: 0.8159  lr:0.010000
[ Sun Jul 14 19:46:41 2024 ] 	Batch(600/6809) done. Loss: 0.5230  lr:0.010000
[ Sun Jul 14 19:47:04 2024 ] 	Batch(700/6809) done. Loss: 0.9952  lr:0.010000
[ Sun Jul 14 19:47:27 2024 ] 	Batch(800/6809) done. Loss: 0.6473  lr:0.010000
[ Sun Jul 14 19:47:49 2024 ] 	Batch(900/6809) done. Loss: 0.1335  lr:0.010000
[ Sun Jul 14 19:48:12 2024 ] 
Training: Epoch [14/150], Step [999], Loss: 0.5440174341201782, Training Accuracy: 72.8625
[ Sun Jul 14 19:48:12 2024 ] 	Batch(1000/6809) done. Loss: 0.2772  lr:0.010000
[ Sun Jul 14 19:48:35 2024 ] 	Batch(1100/6809) done. Loss: 0.6715  lr:0.010000
[ Sun Jul 14 19:48:57 2024 ] 	Batch(1200/6809) done. Loss: 0.7754  lr:0.010000
[ Sun Jul 14 19:49:20 2024 ] 	Batch(1300/6809) done. Loss: 0.5035  lr:0.010000
[ Sun Jul 14 19:49:43 2024 ] 	Batch(1400/6809) done. Loss: 0.2345  lr:0.010000
[ Sun Jul 14 19:50:05 2024 ] 
Training: Epoch [14/150], Step [1499], Loss: 0.7175026535987854, Training Accuracy: 72.78333333333333
[ Sun Jul 14 19:50:05 2024 ] 	Batch(1500/6809) done. Loss: 0.8159  lr:0.010000
[ Sun Jul 14 19:50:28 2024 ] 	Batch(1600/6809) done. Loss: 2.5011  lr:0.010000
[ Sun Jul 14 19:50:52 2024 ] 	Batch(1700/6809) done. Loss: 1.1865  lr:0.010000
[ Sun Jul 14 19:51:15 2024 ] 	Batch(1800/6809) done. Loss: 1.2470  lr:0.010000
[ Sun Jul 14 19:51:39 2024 ] 	Batch(1900/6809) done. Loss: 1.0237  lr:0.010000
[ Sun Jul 14 19:52:01 2024 ] 
Training: Epoch [14/150], Step [1999], Loss: 0.7751402854919434, Training Accuracy: 72.79375
[ Sun Jul 14 19:52:02 2024 ] 	Batch(2000/6809) done. Loss: 0.8432  lr:0.010000
[ Sun Jul 14 19:52:24 2024 ] 	Batch(2100/6809) done. Loss: 1.2417  lr:0.010000
[ Sun Jul 14 19:52:47 2024 ] 	Batch(2200/6809) done. Loss: 0.5251  lr:0.010000
[ Sun Jul 14 19:53:10 2024 ] 	Batch(2300/6809) done. Loss: 1.0188  lr:0.010000
[ Sun Jul 14 19:53:32 2024 ] 	Batch(2400/6809) done. Loss: 1.5910  lr:0.010000
[ Sun Jul 14 19:53:55 2024 ] 
Training: Epoch [14/150], Step [2499], Loss: 1.265442132949829, Training Accuracy: 72.885
[ Sun Jul 14 19:53:55 2024 ] 	Batch(2500/6809) done. Loss: 1.0993  lr:0.010000
[ Sun Jul 14 19:54:18 2024 ] 	Batch(2600/6809) done. Loss: 1.0270  lr:0.010000
[ Sun Jul 14 19:54:40 2024 ] 	Batch(2700/6809) done. Loss: 0.7170  lr:0.010000
[ Sun Jul 14 19:55:03 2024 ] 	Batch(2800/6809) done. Loss: 0.5187  lr:0.010000
[ Sun Jul 14 19:55:26 2024 ] 	Batch(2900/6809) done. Loss: 0.6473  lr:0.010000
[ Sun Jul 14 19:55:49 2024 ] 
Training: Epoch [14/150], Step [2999], Loss: 0.8531211614608765, Training Accuracy: 72.8375
[ Sun Jul 14 19:55:49 2024 ] 	Batch(3000/6809) done. Loss: 0.5780  lr:0.010000
[ Sun Jul 14 19:56:11 2024 ] 	Batch(3100/6809) done. Loss: 0.6298  lr:0.010000
[ Sun Jul 14 19:56:34 2024 ] 	Batch(3200/6809) done. Loss: 0.8929  lr:0.010000
[ Sun Jul 14 19:56:58 2024 ] 	Batch(3300/6809) done. Loss: 1.1078  lr:0.010000
[ Sun Jul 14 19:57:21 2024 ] 	Batch(3400/6809) done. Loss: 1.1163  lr:0.010000
[ Sun Jul 14 19:57:43 2024 ] 
Training: Epoch [14/150], Step [3499], Loss: 1.1439354419708252, Training Accuracy: 72.79642857142858
[ Sun Jul 14 19:57:44 2024 ] 	Batch(3500/6809) done. Loss: 0.9977  lr:0.010000
[ Sun Jul 14 19:58:07 2024 ] 	Batch(3600/6809) done. Loss: 1.1006  lr:0.010000
[ Sun Jul 14 19:58:29 2024 ] 	Batch(3700/6809) done. Loss: 1.0968  lr:0.010000
[ Sun Jul 14 19:58:52 2024 ] 	Batch(3800/6809) done. Loss: 0.9034  lr:0.010000
[ Sun Jul 14 19:59:15 2024 ] 	Batch(3900/6809) done. Loss: 0.3791  lr:0.010000
[ Sun Jul 14 19:59:37 2024 ] 
Training: Epoch [14/150], Step [3999], Loss: 0.5005170702934265, Training Accuracy: 72.725
[ Sun Jul 14 19:59:37 2024 ] 	Batch(4000/6809) done. Loss: 0.2981  lr:0.010000
[ Sun Jul 14 20:00:00 2024 ] 	Batch(4100/6809) done. Loss: 0.9605  lr:0.010000
[ Sun Jul 14 20:00:23 2024 ] 	Batch(4200/6809) done. Loss: 0.4931  lr:0.010000
[ Sun Jul 14 20:00:46 2024 ] 	Batch(4300/6809) done. Loss: 1.0574  lr:0.010000
[ Sun Jul 14 20:01:08 2024 ] 	Batch(4400/6809) done. Loss: 1.3297  lr:0.010000
[ Sun Jul 14 20:01:31 2024 ] 
Training: Epoch [14/150], Step [4499], Loss: 1.8323478698730469, Training Accuracy: 72.74166666666667
[ Sun Jul 14 20:01:31 2024 ] 	Batch(4500/6809) done. Loss: 1.8606  lr:0.010000
[ Sun Jul 14 20:01:54 2024 ] 	Batch(4600/6809) done. Loss: 0.8102  lr:0.010000
[ Sun Jul 14 20:02:16 2024 ] 	Batch(4700/6809) done. Loss: 1.1145  lr:0.010000
[ Sun Jul 14 20:02:39 2024 ] 	Batch(4800/6809) done. Loss: 1.2848  lr:0.010000
[ Sun Jul 14 20:03:02 2024 ] 	Batch(4900/6809) done. Loss: 1.0049  lr:0.010000
[ Sun Jul 14 20:03:25 2024 ] 
Training: Epoch [14/150], Step [4999], Loss: 1.207160472869873, Training Accuracy: 72.775
[ Sun Jul 14 20:03:25 2024 ] 	Batch(5000/6809) done. Loss: 1.5079  lr:0.010000
[ Sun Jul 14 20:03:47 2024 ] 	Batch(5100/6809) done. Loss: 0.1087  lr:0.010000
[ Sun Jul 14 20:04:10 2024 ] 	Batch(5200/6809) done. Loss: 2.0033  lr:0.010000
[ Sun Jul 14 20:04:33 2024 ] 	Batch(5300/6809) done. Loss: 1.5004  lr:0.010000
[ Sun Jul 14 20:04:56 2024 ] 	Batch(5400/6809) done. Loss: 0.3114  lr:0.010000
[ Sun Jul 14 20:05:18 2024 ] 
Training: Epoch [14/150], Step [5499], Loss: 0.9776197671890259, Training Accuracy: 72.9590909090909
[ Sun Jul 14 20:05:18 2024 ] 	Batch(5500/6809) done. Loss: 0.8629  lr:0.010000
[ Sun Jul 14 20:05:41 2024 ] 	Batch(5600/6809) done. Loss: 1.7877  lr:0.010000
[ Sun Jul 14 20:06:04 2024 ] 	Batch(5700/6809) done. Loss: 1.3258  lr:0.010000
[ Sun Jul 14 20:06:27 2024 ] 	Batch(5800/6809) done. Loss: 0.1385  lr:0.010000
[ Sun Jul 14 20:06:49 2024 ] 	Batch(5900/6809) done. Loss: 0.6560  lr:0.010000
[ Sun Jul 14 20:07:12 2024 ] 
Training: Epoch [14/150], Step [5999], Loss: 0.45673051476478577, Training Accuracy: 72.94791666666667
[ Sun Jul 14 20:07:12 2024 ] 	Batch(6000/6809) done. Loss: 0.5561  lr:0.010000
[ Sun Jul 14 20:07:35 2024 ] 	Batch(6100/6809) done. Loss: 0.5760  lr:0.010000
[ Sun Jul 14 20:07:57 2024 ] 	Batch(6200/6809) done. Loss: 0.7463  lr:0.010000
[ Sun Jul 14 20:08:20 2024 ] 	Batch(6300/6809) done. Loss: 1.7813  lr:0.010000
[ Sun Jul 14 20:08:43 2024 ] 	Batch(6400/6809) done. Loss: 1.0627  lr:0.010000
[ Sun Jul 14 20:09:06 2024 ] 
Training: Epoch [14/150], Step [6499], Loss: 1.3491226434707642, Training Accuracy: 72.9673076923077
[ Sun Jul 14 20:09:06 2024 ] 	Batch(6500/6809) done. Loss: 1.0613  lr:0.010000
[ Sun Jul 14 20:09:30 2024 ] 	Batch(6600/6809) done. Loss: 0.8739  lr:0.010000
[ Sun Jul 14 20:09:53 2024 ] 	Batch(6700/6809) done. Loss: 1.6867  lr:0.010000
[ Sun Jul 14 20:10:17 2024 ] 	Batch(6800/6809) done. Loss: 0.1254  lr:0.010000
[ Sun Jul 14 20:10:19 2024 ] 	Mean training loss: 0.9018.
[ Sun Jul 14 20:10:19 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 20:10:19 2024 ] Training epoch: 16
[ Sun Jul 14 20:10:20 2024 ] 	Batch(0/6809) done. Loss: 0.8875  lr:0.010000
[ Sun Jul 14 20:10:43 2024 ] 	Batch(100/6809) done. Loss: 0.5693  lr:0.010000
[ Sun Jul 14 20:11:06 2024 ] 	Batch(200/6809) done. Loss: 0.9981  lr:0.010000
[ Sun Jul 14 20:11:29 2024 ] 	Batch(300/6809) done. Loss: 0.4878  lr:0.010000
[ Sun Jul 14 20:11:52 2024 ] 	Batch(400/6809) done. Loss: 0.7320  lr:0.010000
[ Sun Jul 14 20:12:14 2024 ] 
Training: Epoch [15/150], Step [499], Loss: 0.21558743715286255, Training Accuracy: 74.97500000000001
[ Sun Jul 14 20:12:15 2024 ] 	Batch(500/6809) done. Loss: 1.0436  lr:0.010000
[ Sun Jul 14 20:12:37 2024 ] 	Batch(600/6809) done. Loss: 0.6707  lr:0.010000
[ Sun Jul 14 20:13:00 2024 ] 	Batch(700/6809) done. Loss: 0.7099  lr:0.010000
[ Sun Jul 14 20:13:23 2024 ] 	Batch(800/6809) done. Loss: 1.8976  lr:0.010000
[ Sun Jul 14 20:13:47 2024 ] 	Batch(900/6809) done. Loss: 0.2439  lr:0.010000
[ Sun Jul 14 20:14:10 2024 ] 
Training: Epoch [15/150], Step [999], Loss: 1.0522534847259521, Training Accuracy: 74.3375
[ Sun Jul 14 20:14:10 2024 ] 	Batch(1000/6809) done. Loss: 0.7454  lr:0.010000
[ Sun Jul 14 20:14:34 2024 ] 	Batch(1100/6809) done. Loss: 0.3260  lr:0.010000
[ Sun Jul 14 20:14:57 2024 ] 	Batch(1200/6809) done. Loss: 0.6602  lr:0.010000
[ Sun Jul 14 20:15:21 2024 ] 	Batch(1300/6809) done. Loss: 1.3237  lr:0.010000
[ Sun Jul 14 20:15:44 2024 ] 	Batch(1400/6809) done. Loss: 0.3987  lr:0.010000
[ Sun Jul 14 20:16:08 2024 ] 
Training: Epoch [15/150], Step [1499], Loss: 0.4438069760799408, Training Accuracy: 74.30833333333334
[ Sun Jul 14 20:16:08 2024 ] 	Batch(1500/6809) done. Loss: 0.8716  lr:0.010000
[ Sun Jul 14 20:16:31 2024 ] 	Batch(1600/6809) done. Loss: 0.8191  lr:0.010000
[ Sun Jul 14 20:16:54 2024 ] 	Batch(1700/6809) done. Loss: 0.8632  lr:0.010000
[ Sun Jul 14 20:17:17 2024 ] 	Batch(1800/6809) done. Loss: 0.8697  lr:0.010000
[ Sun Jul 14 20:17:40 2024 ] 	Batch(1900/6809) done. Loss: 0.3230  lr:0.010000
[ Sun Jul 14 20:18:03 2024 ] 
Training: Epoch [15/150], Step [1999], Loss: 1.0386807918548584, Training Accuracy: 74.38125
[ Sun Jul 14 20:18:04 2024 ] 	Batch(2000/6809) done. Loss: 0.6848  lr:0.010000
[ Sun Jul 14 20:18:27 2024 ] 	Batch(2100/6809) done. Loss: 1.7402  lr:0.010000
[ Sun Jul 14 20:18:51 2024 ] 	Batch(2200/6809) done. Loss: 0.2976  lr:0.010000
[ Sun Jul 14 20:19:14 2024 ] 	Batch(2300/6809) done. Loss: 0.6177  lr:0.010000
[ Sun Jul 14 20:19:38 2024 ] 	Batch(2400/6809) done. Loss: 0.8667  lr:0.010000
[ Sun Jul 14 20:20:01 2024 ] 
Training: Epoch [15/150], Step [2499], Loss: 0.470791757106781, Training Accuracy: 74.395
[ Sun Jul 14 20:20:01 2024 ] 	Batch(2500/6809) done. Loss: 0.7149  lr:0.010000
[ Sun Jul 14 20:20:25 2024 ] 	Batch(2600/6809) done. Loss: 1.5063  lr:0.010000
[ Sun Jul 14 20:20:48 2024 ] 	Batch(2700/6809) done. Loss: 0.6614  lr:0.010000
[ Sun Jul 14 20:21:12 2024 ] 	Batch(2800/6809) done. Loss: 1.5388  lr:0.010000
[ Sun Jul 14 20:21:36 2024 ] 	Batch(2900/6809) done. Loss: 0.9552  lr:0.010000
[ Sun Jul 14 20:21:59 2024 ] 
Training: Epoch [15/150], Step [2999], Loss: 0.5368315577507019, Training Accuracy: 74.26666666666667
[ Sun Jul 14 20:21:59 2024 ] 	Batch(3000/6809) done. Loss: 0.1544  lr:0.010000
[ Sun Jul 14 20:22:23 2024 ] 	Batch(3100/6809) done. Loss: 0.3213  lr:0.010000
[ Sun Jul 14 20:22:46 2024 ] 	Batch(3200/6809) done. Loss: 1.5771  lr:0.010000
[ Sun Jul 14 20:23:10 2024 ] 	Batch(3300/6809) done. Loss: 0.8506  lr:0.010000
[ Sun Jul 14 20:23:33 2024 ] 	Batch(3400/6809) done. Loss: 1.3669  lr:0.010000
[ Sun Jul 14 20:23:56 2024 ] 
Training: Epoch [15/150], Step [3499], Loss: 0.6306523680686951, Training Accuracy: 74.26428571428572
[ Sun Jul 14 20:23:56 2024 ] 	Batch(3500/6809) done. Loss: 1.2862  lr:0.010000
[ Sun Jul 14 20:24:20 2024 ] 	Batch(3600/6809) done. Loss: 0.3789  lr:0.010000
[ Sun Jul 14 20:24:43 2024 ] 	Batch(3700/6809) done. Loss: 0.1586  lr:0.010000
[ Sun Jul 14 20:25:06 2024 ] 	Batch(3800/6809) done. Loss: 0.6204  lr:0.010000
[ Sun Jul 14 20:25:28 2024 ] 	Batch(3900/6809) done. Loss: 1.1361  lr:0.010000
[ Sun Jul 14 20:25:51 2024 ] 
Training: Epoch [15/150], Step [3999], Loss: 0.9463220238685608, Training Accuracy: 74.20625
[ Sun Jul 14 20:25:51 2024 ] 	Batch(4000/6809) done. Loss: 1.6840  lr:0.010000
[ Sun Jul 14 20:26:14 2024 ] 	Batch(4100/6809) done. Loss: 0.8615  lr:0.010000
[ Sun Jul 14 20:26:36 2024 ] 	Batch(4200/6809) done. Loss: 1.1568  lr:0.010000
[ Sun Jul 14 20:26:59 2024 ] 	Batch(4300/6809) done. Loss: 0.8303  lr:0.010000
[ Sun Jul 14 20:27:22 2024 ] 	Batch(4400/6809) done. Loss: 0.4551  lr:0.010000
[ Sun Jul 14 20:27:44 2024 ] 
Training: Epoch [15/150], Step [4499], Loss: 1.043621301651001, Training Accuracy: 74.1
[ Sun Jul 14 20:27:44 2024 ] 	Batch(4500/6809) done. Loss: 1.4080  lr:0.010000
[ Sun Jul 14 20:28:07 2024 ] 	Batch(4600/6809) done. Loss: 0.9778  lr:0.010000
[ Sun Jul 14 20:28:29 2024 ] 	Batch(4700/6809) done. Loss: 0.6385  lr:0.010000
[ Sun Jul 14 20:28:52 2024 ] 	Batch(4800/6809) done. Loss: 0.8316  lr:0.010000
[ Sun Jul 14 20:29:15 2024 ] 	Batch(4900/6809) done. Loss: 0.2771  lr:0.010000
[ Sun Jul 14 20:29:37 2024 ] 
Training: Epoch [15/150], Step [4999], Loss: 1.2658960819244385, Training Accuracy: 74.105
[ Sun Jul 14 20:29:37 2024 ] 	Batch(5000/6809) done. Loss: 0.7664  lr:0.010000
[ Sun Jul 14 20:30:00 2024 ] 	Batch(5100/6809) done. Loss: 0.8570  lr:0.010000
[ Sun Jul 14 20:30:23 2024 ] 	Batch(5200/6809) done. Loss: 0.8862  lr:0.010000
[ Sun Jul 14 20:30:45 2024 ] 	Batch(5300/6809) done. Loss: 1.0679  lr:0.010000
[ Sun Jul 14 20:31:08 2024 ] 	Batch(5400/6809) done. Loss: 0.7784  lr:0.010000
[ Sun Jul 14 20:31:30 2024 ] 
Training: Epoch [15/150], Step [5499], Loss: 0.8480457067489624, Training Accuracy: 74.05909090909091
[ Sun Jul 14 20:31:30 2024 ] 	Batch(5500/6809) done. Loss: 0.3966  lr:0.010000
[ Sun Jul 14 20:31:53 2024 ] 	Batch(5600/6809) done. Loss: 0.6494  lr:0.010000
[ Sun Jul 14 20:32:16 2024 ] 	Batch(5700/6809) done. Loss: 0.4094  lr:0.010000
[ Sun Jul 14 20:32:38 2024 ] 	Batch(5800/6809) done. Loss: 0.2731  lr:0.010000
[ Sun Jul 14 20:33:01 2024 ] 	Batch(5900/6809) done. Loss: 1.9533  lr:0.010000
[ Sun Jul 14 20:33:24 2024 ] 
Training: Epoch [15/150], Step [5999], Loss: 0.6627629995346069, Training Accuracy: 74.00833333333333
[ Sun Jul 14 20:33:24 2024 ] 	Batch(6000/6809) done. Loss: 0.6257  lr:0.010000
[ Sun Jul 14 20:33:47 2024 ] 	Batch(6100/6809) done. Loss: 1.2258  lr:0.010000
[ Sun Jul 14 20:34:11 2024 ] 	Batch(6200/6809) done. Loss: 1.3916  lr:0.010000
[ Sun Jul 14 20:34:34 2024 ] 	Batch(6300/6809) done. Loss: 1.6910  lr:0.010000
[ Sun Jul 14 20:34:57 2024 ] 	Batch(6400/6809) done. Loss: 0.5264  lr:0.010000
[ Sun Jul 14 20:35:19 2024 ] 
Training: Epoch [15/150], Step [6499], Loss: 1.2615251541137695, Training Accuracy: 74.00384615384615
[ Sun Jul 14 20:35:19 2024 ] 	Batch(6500/6809) done. Loss: 1.0463  lr:0.010000
[ Sun Jul 14 20:35:42 2024 ] 	Batch(6600/6809) done. Loss: 0.2611  lr:0.010000
[ Sun Jul 14 20:36:04 2024 ] 	Batch(6700/6809) done. Loss: 1.7420  lr:0.010000
[ Sun Jul 14 20:36:27 2024 ] 	Batch(6800/6809) done. Loss: 0.4505  lr:0.010000
[ Sun Jul 14 20:36:29 2024 ] 	Mean training loss: 0.8627.
[ Sun Jul 14 20:36:29 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 20:36:29 2024 ] Training epoch: 17
[ Sun Jul 14 20:36:30 2024 ] 	Batch(0/6809) done. Loss: 0.4580  lr:0.010000
[ Sun Jul 14 20:36:52 2024 ] 	Batch(100/6809) done. Loss: 1.0484  lr:0.010000
[ Sun Jul 14 20:37:15 2024 ] 	Batch(200/6809) done. Loss: 0.5795  lr:0.010000
[ Sun Jul 14 20:37:38 2024 ] 	Batch(300/6809) done. Loss: 1.3585  lr:0.010000
[ Sun Jul 14 20:38:01 2024 ] 	Batch(400/6809) done. Loss: 1.3313  lr:0.010000
[ Sun Jul 14 20:38:23 2024 ] 
Training: Epoch [16/150], Step [499], Loss: 1.0357351303100586, Training Accuracy: 75.05
[ Sun Jul 14 20:38:23 2024 ] 	Batch(500/6809) done. Loss: 1.3192  lr:0.010000
[ Sun Jul 14 20:38:46 2024 ] 	Batch(600/6809) done. Loss: 1.0233  lr:0.010000
[ Sun Jul 14 20:39:09 2024 ] 	Batch(700/6809) done. Loss: 1.1818  lr:0.010000
[ Sun Jul 14 20:39:31 2024 ] 	Batch(800/6809) done. Loss: 0.4647  lr:0.010000
[ Sun Jul 14 20:39:54 2024 ] 	Batch(900/6809) done. Loss: 0.3498  lr:0.010000
[ Sun Jul 14 20:40:17 2024 ] 
Training: Epoch [16/150], Step [999], Loss: 0.7657874822616577, Training Accuracy: 75.4375
[ Sun Jul 14 20:40:17 2024 ] 	Batch(1000/6809) done. Loss: 0.4384  lr:0.010000
[ Sun Jul 14 20:40:40 2024 ] 	Batch(1100/6809) done. Loss: 0.0832  lr:0.010000
[ Sun Jul 14 20:41:03 2024 ] 	Batch(1200/6809) done. Loss: 0.6504  lr:0.010000
[ Sun Jul 14 20:41:26 2024 ] 	Batch(1300/6809) done. Loss: 0.3420  lr:0.010000
[ Sun Jul 14 20:41:48 2024 ] 	Batch(1400/6809) done. Loss: 0.7720  lr:0.010000
[ Sun Jul 14 20:42:11 2024 ] 
Training: Epoch [16/150], Step [1499], Loss: 0.41494131088256836, Training Accuracy: 75.35833333333333
[ Sun Jul 14 20:42:11 2024 ] 	Batch(1500/6809) done. Loss: 0.4415  lr:0.010000
[ Sun Jul 14 20:42:34 2024 ] 	Batch(1600/6809) done. Loss: 0.4722  lr:0.010000
[ Sun Jul 14 20:42:56 2024 ] 	Batch(1700/6809) done. Loss: 0.5196  lr:0.010000
[ Sun Jul 14 20:43:19 2024 ] 	Batch(1800/6809) done. Loss: 1.2007  lr:0.010000
[ Sun Jul 14 20:43:42 2024 ] 	Batch(1900/6809) done. Loss: 1.2303  lr:0.010000
[ Sun Jul 14 20:44:04 2024 ] 
Training: Epoch [16/150], Step [1999], Loss: 0.19551154971122742, Training Accuracy: 74.93125
[ Sun Jul 14 20:44:05 2024 ] 	Batch(2000/6809) done. Loss: 1.0188  lr:0.010000
[ Sun Jul 14 20:44:27 2024 ] 	Batch(2100/6809) done. Loss: 0.7256  lr:0.010000
[ Sun Jul 14 20:44:50 2024 ] 	Batch(2200/6809) done. Loss: 0.8504  lr:0.010000
[ Sun Jul 14 20:45:13 2024 ] 	Batch(2300/6809) done. Loss: 0.7037  lr:0.010000
[ Sun Jul 14 20:45:36 2024 ] 	Batch(2400/6809) done. Loss: 0.1935  lr:0.010000
[ Sun Jul 14 20:45:58 2024 ] 
Training: Epoch [16/150], Step [2499], Loss: 0.9656917452812195, Training Accuracy: 74.86
[ Sun Jul 14 20:45:58 2024 ] 	Batch(2500/6809) done. Loss: 1.1717  lr:0.010000
[ Sun Jul 14 20:46:21 2024 ] 	Batch(2600/6809) done. Loss: 0.3018  lr:0.010000
[ Sun Jul 14 20:46:44 2024 ] 	Batch(2700/6809) done. Loss: 1.2552  lr:0.010000
[ Sun Jul 14 20:47:07 2024 ] 	Batch(2800/6809) done. Loss: 0.7416  lr:0.010000
[ Sun Jul 14 20:47:29 2024 ] 	Batch(2900/6809) done. Loss: 0.4792  lr:0.010000
[ Sun Jul 14 20:47:52 2024 ] 
Training: Epoch [16/150], Step [2999], Loss: 0.3884229063987732, Training Accuracy: 74.90416666666667
[ Sun Jul 14 20:47:52 2024 ] 	Batch(3000/6809) done. Loss: 0.5872  lr:0.010000
[ Sun Jul 14 20:48:16 2024 ] 	Batch(3100/6809) done. Loss: 1.4083  lr:0.010000
[ Sun Jul 14 20:48:39 2024 ] 	Batch(3200/6809) done. Loss: 0.7420  lr:0.010000
[ Sun Jul 14 20:49:02 2024 ] 	Batch(3300/6809) done. Loss: 1.7766  lr:0.010000
[ Sun Jul 14 20:49:25 2024 ] 	Batch(3400/6809) done. Loss: 0.2472  lr:0.010000
[ Sun Jul 14 20:49:48 2024 ] 
Training: Epoch [16/150], Step [3499], Loss: 0.8440384268760681, Training Accuracy: 74.83214285714286
[ Sun Jul 14 20:49:48 2024 ] 	Batch(3500/6809) done. Loss: 0.2077  lr:0.010000
[ Sun Jul 14 20:50:12 2024 ] 	Batch(3600/6809) done. Loss: 0.9753  lr:0.010000
[ Sun Jul 14 20:50:35 2024 ] 	Batch(3700/6809) done. Loss: 1.0102  lr:0.010000
[ Sun Jul 14 20:50:58 2024 ] 	Batch(3800/6809) done. Loss: 0.8050  lr:0.010000
[ Sun Jul 14 20:51:21 2024 ] 	Batch(3900/6809) done. Loss: 0.4155  lr:0.010000
[ Sun Jul 14 20:51:44 2024 ] 
Training: Epoch [16/150], Step [3999], Loss: 0.4609086513519287, Training Accuracy: 74.725
[ Sun Jul 14 20:51:44 2024 ] 	Batch(4000/6809) done. Loss: 0.7490  lr:0.010000
[ Sun Jul 14 20:52:07 2024 ] 	Batch(4100/6809) done. Loss: 0.9592  lr:0.010000
[ Sun Jul 14 20:52:30 2024 ] 	Batch(4200/6809) done. Loss: 1.1306  lr:0.010000
[ Sun Jul 14 20:52:53 2024 ] 	Batch(4300/6809) done. Loss: 1.4766  lr:0.010000
[ Sun Jul 14 20:53:16 2024 ] 	Batch(4400/6809) done. Loss: 0.6186  lr:0.010000
[ Sun Jul 14 20:53:38 2024 ] 
Training: Epoch [16/150], Step [4499], Loss: 0.7415196299552917, Training Accuracy: 74.67777777777778
[ Sun Jul 14 20:53:38 2024 ] 	Batch(4500/6809) done. Loss: 0.9066  lr:0.010000
[ Sun Jul 14 20:54:01 2024 ] 	Batch(4600/6809) done. Loss: 0.6852  lr:0.010000
[ Sun Jul 14 20:54:24 2024 ] 	Batch(4700/6809) done. Loss: 1.0520  lr:0.010000
[ Sun Jul 14 20:54:46 2024 ] 	Batch(4800/6809) done. Loss: 0.7859  lr:0.010000
[ Sun Jul 14 20:55:09 2024 ] 	Batch(4900/6809) done. Loss: 0.3077  lr:0.010000
[ Sun Jul 14 20:55:31 2024 ] 
Training: Epoch [16/150], Step [4999], Loss: 0.7386806011199951, Training Accuracy: 74.645
[ Sun Jul 14 20:55:32 2024 ] 	Batch(5000/6809) done. Loss: 0.9435  lr:0.010000
[ Sun Jul 14 20:55:54 2024 ] 	Batch(5100/6809) done. Loss: 2.7155  lr:0.010000
[ Sun Jul 14 20:56:17 2024 ] 	Batch(5200/6809) done. Loss: 0.8171  lr:0.010000
[ Sun Jul 14 20:56:40 2024 ] 	Batch(5300/6809) done. Loss: 0.6842  lr:0.010000
[ Sun Jul 14 20:57:02 2024 ] 	Batch(5400/6809) done. Loss: 0.4945  lr:0.010000
[ Sun Jul 14 20:57:25 2024 ] 
Training: Epoch [16/150], Step [5499], Loss: 0.39932161569595337, Training Accuracy: 74.68409090909091
[ Sun Jul 14 20:57:25 2024 ] 	Batch(5500/6809) done. Loss: 0.3016  lr:0.010000
[ Sun Jul 14 20:57:48 2024 ] 	Batch(5600/6809) done. Loss: 0.3640  lr:0.010000
[ Sun Jul 14 20:58:11 2024 ] 	Batch(5700/6809) done. Loss: 0.3515  lr:0.010000
[ Sun Jul 14 20:58:34 2024 ] 	Batch(5800/6809) done. Loss: 0.8681  lr:0.010000
[ Sun Jul 14 20:58:57 2024 ] 	Batch(5900/6809) done. Loss: 0.9894  lr:0.010000
[ Sun Jul 14 20:59:19 2024 ] 
Training: Epoch [16/150], Step [5999], Loss: 1.4246374368667603, Training Accuracy: 74.53333333333333
[ Sun Jul 14 20:59:20 2024 ] 	Batch(6000/6809) done. Loss: 0.9483  lr:0.010000
[ Sun Jul 14 20:59:42 2024 ] 	Batch(6100/6809) done. Loss: 0.3728  lr:0.010000
[ Sun Jul 14 21:00:05 2024 ] 	Batch(6200/6809) done. Loss: 0.6673  lr:0.010000
[ Sun Jul 14 21:00:28 2024 ] 	Batch(6300/6809) done. Loss: 0.8281  lr:0.010000
[ Sun Jul 14 21:00:50 2024 ] 	Batch(6400/6809) done. Loss: 0.5524  lr:0.010000
[ Sun Jul 14 21:01:13 2024 ] 
Training: Epoch [16/150], Step [6499], Loss: 0.4234079122543335, Training Accuracy: 74.65961538461539
[ Sun Jul 14 21:01:13 2024 ] 	Batch(6500/6809) done. Loss: 0.3844  lr:0.010000
[ Sun Jul 14 21:01:36 2024 ] 	Batch(6600/6809) done. Loss: 0.6485  lr:0.010000
[ Sun Jul 14 21:01:59 2024 ] 	Batch(6700/6809) done. Loss: 0.9231  lr:0.010000
[ Sun Jul 14 21:02:21 2024 ] 	Batch(6800/6809) done. Loss: 0.5777  lr:0.010000
[ Sun Jul 14 21:02:23 2024 ] 	Mean training loss: 0.8506.
[ Sun Jul 14 21:02:23 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 21:02:23 2024 ] Training epoch: 18
[ Sun Jul 14 21:02:24 2024 ] 	Batch(0/6809) done. Loss: 0.3676  lr:0.010000
[ Sun Jul 14 21:02:47 2024 ] 	Batch(100/6809) done. Loss: 0.8400  lr:0.010000
[ Sun Jul 14 21:03:09 2024 ] 	Batch(200/6809) done. Loss: 0.2890  lr:0.010000
[ Sun Jul 14 21:03:32 2024 ] 	Batch(300/6809) done. Loss: 0.1947  lr:0.010000
[ Sun Jul 14 21:03:55 2024 ] 	Batch(400/6809) done. Loss: 1.8432  lr:0.010000
[ Sun Jul 14 21:04:17 2024 ] 
Training: Epoch [17/150], Step [499], Loss: 0.7466123700141907, Training Accuracy: 75.44999999999999
[ Sun Jul 14 21:04:17 2024 ] 	Batch(500/6809) done. Loss: 0.3974  lr:0.010000
[ Sun Jul 14 21:04:40 2024 ] 	Batch(600/6809) done. Loss: 0.7239  lr:0.010000
[ Sun Jul 14 21:05:03 2024 ] 	Batch(700/6809) done. Loss: 1.2231  lr:0.010000
[ Sun Jul 14 21:05:26 2024 ] 	Batch(800/6809) done. Loss: 1.2969  lr:0.010000
[ Sun Jul 14 21:05:48 2024 ] 	Batch(900/6809) done. Loss: 2.2113  lr:0.010000
[ Sun Jul 14 21:06:11 2024 ] 
Training: Epoch [17/150], Step [999], Loss: 1.2061312198638916, Training Accuracy: 75.675
[ Sun Jul 14 21:06:11 2024 ] 	Batch(1000/6809) done. Loss: 2.0940  lr:0.010000
[ Sun Jul 14 21:06:34 2024 ] 	Batch(1100/6809) done. Loss: 0.2451  lr:0.010000
[ Sun Jul 14 21:06:57 2024 ] 	Batch(1200/6809) done. Loss: 0.3748  lr:0.010000
[ Sun Jul 14 21:07:19 2024 ] 	Batch(1300/6809) done. Loss: 0.3601  lr:0.010000
[ Sun Jul 14 21:07:42 2024 ] 	Batch(1400/6809) done. Loss: 1.7934  lr:0.010000
[ Sun Jul 14 21:08:05 2024 ] 
Training: Epoch [17/150], Step [1499], Loss: 1.012866497039795, Training Accuracy: 75.08333333333333
[ Sun Jul 14 21:08:06 2024 ] 	Batch(1500/6809) done. Loss: 0.8188  lr:0.010000
[ Sun Jul 14 21:08:29 2024 ] 	Batch(1600/6809) done. Loss: 1.7758  lr:0.010000
[ Sun Jul 14 21:08:52 2024 ] 	Batch(1700/6809) done. Loss: 0.9672  lr:0.010000
[ Sun Jul 14 21:09:15 2024 ] 	Batch(1800/6809) done. Loss: 0.3701  lr:0.010000
[ Sun Jul 14 21:09:38 2024 ] 	Batch(1900/6809) done. Loss: 0.5736  lr:0.010000
[ Sun Jul 14 21:10:00 2024 ] 
Training: Epoch [17/150], Step [1999], Loss: 1.6363788843154907, Training Accuracy: 75.38125
[ Sun Jul 14 21:10:01 2024 ] 	Batch(2000/6809) done. Loss: 0.5946  lr:0.010000
[ Sun Jul 14 21:10:23 2024 ] 	Batch(2100/6809) done. Loss: 0.2249  lr:0.010000
[ Sun Jul 14 21:10:46 2024 ] 	Batch(2200/6809) done. Loss: 0.9133  lr:0.010000
[ Sun Jul 14 21:11:08 2024 ] 	Batch(2300/6809) done. Loss: 0.6712  lr:0.010000
[ Sun Jul 14 21:11:31 2024 ] 	Batch(2400/6809) done. Loss: 0.3722  lr:0.010000
[ Sun Jul 14 21:11:54 2024 ] 
Training: Epoch [17/150], Step [2499], Loss: 0.2529538869857788, Training Accuracy: 75.59
[ Sun Jul 14 21:11:54 2024 ] 	Batch(2500/6809) done. Loss: 0.4696  lr:0.010000
[ Sun Jul 14 21:12:18 2024 ] 	Batch(2600/6809) done. Loss: 0.7493  lr:0.010000
[ Sun Jul 14 21:12:41 2024 ] 	Batch(2700/6809) done. Loss: 1.5089  lr:0.010000
[ Sun Jul 14 21:13:05 2024 ] 	Batch(2800/6809) done. Loss: 1.2790  lr:0.010000
[ Sun Jul 14 21:13:27 2024 ] 	Batch(2900/6809) done. Loss: 0.8402  lr:0.010000
[ Sun Jul 14 21:13:50 2024 ] 
Training: Epoch [17/150], Step [2999], Loss: 0.3268485963344574, Training Accuracy: 75.575
[ Sun Jul 14 21:13:50 2024 ] 	Batch(3000/6809) done. Loss: 1.1154  lr:0.010000
[ Sun Jul 14 21:14:12 2024 ] 	Batch(3100/6809) done. Loss: 0.7847  lr:0.010000
[ Sun Jul 14 21:14:35 2024 ] 	Batch(3200/6809) done. Loss: 0.3688  lr:0.010000
[ Sun Jul 14 21:14:58 2024 ] 	Batch(3300/6809) done. Loss: 1.1161  lr:0.010000
[ Sun Jul 14 21:15:20 2024 ] 	Batch(3400/6809) done. Loss: 0.6198  lr:0.010000
[ Sun Jul 14 21:15:43 2024 ] 
Training: Epoch [17/150], Step [3499], Loss: 0.5586817264556885, Training Accuracy: 75.47500000000001
[ Sun Jul 14 21:15:43 2024 ] 	Batch(3500/6809) done. Loss: 0.2419  lr:0.010000
[ Sun Jul 14 21:16:06 2024 ] 	Batch(3600/6809) done. Loss: 0.5867  lr:0.010000
[ Sun Jul 14 21:16:28 2024 ] 	Batch(3700/6809) done. Loss: 0.9249  lr:0.010000
[ Sun Jul 14 21:16:51 2024 ] 	Batch(3800/6809) done. Loss: 0.9898  lr:0.010000
[ Sun Jul 14 21:17:13 2024 ] 	Batch(3900/6809) done. Loss: 0.5025  lr:0.010000
[ Sun Jul 14 21:17:36 2024 ] 
Training: Epoch [17/150], Step [3999], Loss: 0.5240420699119568, Training Accuracy: 75.603125
[ Sun Jul 14 21:17:36 2024 ] 	Batch(4000/6809) done. Loss: 0.5066  lr:0.010000
[ Sun Jul 14 21:17:59 2024 ] 	Batch(4100/6809) done. Loss: 0.1879  lr:0.010000
[ Sun Jul 14 21:18:21 2024 ] 	Batch(4200/6809) done. Loss: 1.7048  lr:0.010000
[ Sun Jul 14 21:18:44 2024 ] 	Batch(4300/6809) done. Loss: 1.0737  lr:0.010000
[ Sun Jul 14 21:19:07 2024 ] 	Batch(4400/6809) done. Loss: 0.5268  lr:0.010000
[ Sun Jul 14 21:19:29 2024 ] 
Training: Epoch [17/150], Step [4499], Loss: 0.2625465393066406, Training Accuracy: 75.49444444444444
[ Sun Jul 14 21:19:29 2024 ] 	Batch(4500/6809) done. Loss: 1.2840  lr:0.010000
[ Sun Jul 14 21:19:52 2024 ] 	Batch(4600/6809) done. Loss: 0.4587  lr:0.010000
[ Sun Jul 14 21:20:14 2024 ] 	Batch(4700/6809) done. Loss: 0.6422  lr:0.010000
[ Sun Jul 14 21:20:37 2024 ] 	Batch(4800/6809) done. Loss: 0.4336  lr:0.010000
[ Sun Jul 14 21:21:00 2024 ] 	Batch(4900/6809) done. Loss: 0.5350  lr:0.010000
[ Sun Jul 14 21:21:22 2024 ] 
Training: Epoch [17/150], Step [4999], Loss: 0.6833046674728394, Training Accuracy: 75.505
[ Sun Jul 14 21:21:22 2024 ] 	Batch(5000/6809) done. Loss: 0.4858  lr:0.010000
[ Sun Jul 14 21:21:45 2024 ] 	Batch(5100/6809) done. Loss: 0.6304  lr:0.010000
[ Sun Jul 14 21:22:08 2024 ] 	Batch(5200/6809) done. Loss: 0.5157  lr:0.010000
[ Sun Jul 14 21:22:30 2024 ] 	Batch(5300/6809) done. Loss: 0.2268  lr:0.010000
[ Sun Jul 14 21:22:53 2024 ] 	Batch(5400/6809) done. Loss: 0.8076  lr:0.010000
[ Sun Jul 14 21:23:15 2024 ] 
Training: Epoch [17/150], Step [5499], Loss: 0.7491495013237, Training Accuracy: 75.35681818181818
[ Sun Jul 14 21:23:15 2024 ] 	Batch(5500/6809) done. Loss: 0.7959  lr:0.010000
[ Sun Jul 14 21:23:38 2024 ] 	Batch(5600/6809) done. Loss: 2.0075  lr:0.010000
[ Sun Jul 14 21:24:01 2024 ] 	Batch(5700/6809) done. Loss: 0.8450  lr:0.010000
[ Sun Jul 14 21:24:23 2024 ] 	Batch(5800/6809) done. Loss: 1.2293  lr:0.010000
[ Sun Jul 14 21:24:46 2024 ] 	Batch(5900/6809) done. Loss: 0.3780  lr:0.010000
[ Sun Jul 14 21:25:10 2024 ] 
Training: Epoch [17/150], Step [5999], Loss: 1.7435016632080078, Training Accuracy: 75.3125
[ Sun Jul 14 21:25:10 2024 ] 	Batch(6000/6809) done. Loss: 0.9787  lr:0.010000
[ Sun Jul 14 21:25:33 2024 ] 	Batch(6100/6809) done. Loss: 0.8753  lr:0.010000
[ Sun Jul 14 21:25:57 2024 ] 	Batch(6200/6809) done. Loss: 0.1773  lr:0.010000
[ Sun Jul 14 21:26:19 2024 ] 	Batch(6300/6809) done. Loss: 0.6306  lr:0.010000
[ Sun Jul 14 21:26:42 2024 ] 	Batch(6400/6809) done. Loss: 0.3728  lr:0.010000
[ Sun Jul 14 21:27:05 2024 ] 
Training: Epoch [17/150], Step [6499], Loss: 0.5064062476158142, Training Accuracy: 75.22307692307693
[ Sun Jul 14 21:27:05 2024 ] 	Batch(6500/6809) done. Loss: 1.0822  lr:0.010000
[ Sun Jul 14 21:27:28 2024 ] 	Batch(6600/6809) done. Loss: 1.7536  lr:0.010000
[ Sun Jul 14 21:27:51 2024 ] 	Batch(6700/6809) done. Loss: 0.7654  lr:0.010000
[ Sun Jul 14 21:28:13 2024 ] 	Batch(6800/6809) done. Loss: 1.0907  lr:0.010000
[ Sun Jul 14 21:28:15 2024 ] 	Mean training loss: 0.7976.
[ Sun Jul 14 21:28:15 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 21:28:15 2024 ] Training epoch: 19
[ Sun Jul 14 21:28:16 2024 ] 	Batch(0/6809) done. Loss: 0.5167  lr:0.010000
[ Sun Jul 14 21:28:39 2024 ] 	Batch(100/6809) done. Loss: 0.6405  lr:0.010000
[ Sun Jul 14 21:29:02 2024 ] 	Batch(200/6809) done. Loss: 0.4245  lr:0.010000
[ Sun Jul 14 21:29:25 2024 ] 	Batch(300/6809) done. Loss: 0.4950  lr:0.010000
[ Sun Jul 14 21:29:48 2024 ] 	Batch(400/6809) done. Loss: 1.1080  lr:0.010000
[ Sun Jul 14 21:30:10 2024 ] 
Training: Epoch [18/150], Step [499], Loss: 0.25041183829307556, Training Accuracy: 77.55
[ Sun Jul 14 21:30:10 2024 ] 	Batch(500/6809) done. Loss: 0.2430  lr:0.010000
[ Sun Jul 14 21:30:33 2024 ] 	Batch(600/6809) done. Loss: 0.7978  lr:0.010000
[ Sun Jul 14 21:30:56 2024 ] 	Batch(700/6809) done. Loss: 0.3049  lr:0.010000
[ Sun Jul 14 21:31:19 2024 ] 	Batch(800/6809) done. Loss: 0.5752  lr:0.010000
[ Sun Jul 14 21:31:42 2024 ] 	Batch(900/6809) done. Loss: 0.6596  lr:0.010000
[ Sun Jul 14 21:32:04 2024 ] 
Training: Epoch [18/150], Step [999], Loss: 2.6275289058685303, Training Accuracy: 76.525
[ Sun Jul 14 21:32:04 2024 ] 	Batch(1000/6809) done. Loss: 0.2288  lr:0.010000
[ Sun Jul 14 21:32:27 2024 ] 	Batch(1100/6809) done. Loss: 1.2687  lr:0.010000
[ Sun Jul 14 21:32:50 2024 ] 	Batch(1200/6809) done. Loss: 0.5035  lr:0.010000
[ Sun Jul 14 21:33:13 2024 ] 	Batch(1300/6809) done. Loss: 0.8494  lr:0.010000
[ Sun Jul 14 21:33:36 2024 ] 	Batch(1400/6809) done. Loss: 0.4658  lr:0.010000
[ Sun Jul 14 21:33:58 2024 ] 
Training: Epoch [18/150], Step [1499], Loss: 0.8242403268814087, Training Accuracy: 76.25833333333333
[ Sun Jul 14 21:33:59 2024 ] 	Batch(1500/6809) done. Loss: 0.6673  lr:0.010000
[ Sun Jul 14 21:34:21 2024 ] 	Batch(1600/6809) done. Loss: 1.1656  lr:0.010000
[ Sun Jul 14 21:34:44 2024 ] 	Batch(1700/6809) done. Loss: 1.0064  lr:0.010000
[ Sun Jul 14 21:35:07 2024 ] 	Batch(1800/6809) done. Loss: 0.1944  lr:0.010000
[ Sun Jul 14 21:35:29 2024 ] 	Batch(1900/6809) done. Loss: 0.7234  lr:0.010000
[ Sun Jul 14 21:35:52 2024 ] 
Training: Epoch [18/150], Step [1999], Loss: 0.6846644282341003, Training Accuracy: 76.35625
[ Sun Jul 14 21:35:52 2024 ] 	Batch(2000/6809) done. Loss: 0.3233  lr:0.010000
[ Sun Jul 14 21:36:15 2024 ] 	Batch(2100/6809) done. Loss: 0.6604  lr:0.010000
[ Sun Jul 14 21:36:38 2024 ] 	Batch(2200/6809) done. Loss: 0.9314  lr:0.010000
[ Sun Jul 14 21:37:00 2024 ] 	Batch(2300/6809) done. Loss: 0.2171  lr:0.010000
[ Sun Jul 14 21:37:23 2024 ] 	Batch(2400/6809) done. Loss: 0.7033  lr:0.010000
[ Sun Jul 14 21:37:45 2024 ] 
Training: Epoch [18/150], Step [2499], Loss: 0.5285767912864685, Training Accuracy: 76.345
[ Sun Jul 14 21:37:46 2024 ] 	Batch(2500/6809) done. Loss: 1.0473  lr:0.010000
[ Sun Jul 14 21:38:08 2024 ] 	Batch(2600/6809) done. Loss: 0.8654  lr:0.010000
[ Sun Jul 14 21:38:31 2024 ] 	Batch(2700/6809) done. Loss: 0.3413  lr:0.010000
[ Sun Jul 14 21:38:54 2024 ] 	Batch(2800/6809) done. Loss: 0.4975  lr:0.010000
[ Sun Jul 14 21:39:16 2024 ] 	Batch(2900/6809) done. Loss: 1.1279  lr:0.010000
[ Sun Jul 14 21:39:39 2024 ] 
Training: Epoch [18/150], Step [2999], Loss: 1.3271609544754028, Training Accuracy: 76.29583333333333
[ Sun Jul 14 21:39:39 2024 ] 	Batch(3000/6809) done. Loss: 0.8810  lr:0.010000
[ Sun Jul 14 21:40:02 2024 ] 	Batch(3100/6809) done. Loss: 1.2360  lr:0.010000
[ Sun Jul 14 21:40:25 2024 ] 	Batch(3200/6809) done. Loss: 0.4715  lr:0.010000
[ Sun Jul 14 21:40:47 2024 ] 	Batch(3300/6809) done. Loss: 0.6618  lr:0.010000
[ Sun Jul 14 21:41:10 2024 ] 	Batch(3400/6809) done. Loss: 1.0799  lr:0.010000
[ Sun Jul 14 21:41:32 2024 ] 
Training: Epoch [18/150], Step [3499], Loss: 0.6168404221534729, Training Accuracy: 76.38214285714285
[ Sun Jul 14 21:41:33 2024 ] 	Batch(3500/6809) done. Loss: 0.4700  lr:0.010000
[ Sun Jul 14 21:41:55 2024 ] 	Batch(3600/6809) done. Loss: 0.2704  lr:0.010000
[ Sun Jul 14 21:42:18 2024 ] 	Batch(3700/6809) done. Loss: 1.1926  lr:0.010000
[ Sun Jul 14 21:42:41 2024 ] 	Batch(3800/6809) done. Loss: 0.6689  lr:0.010000
[ Sun Jul 14 21:43:03 2024 ] 	Batch(3900/6809) done. Loss: 1.4991  lr:0.010000
[ Sun Jul 14 21:43:26 2024 ] 
Training: Epoch [18/150], Step [3999], Loss: 0.8309019804000854, Training Accuracy: 76.315625
[ Sun Jul 14 21:43:26 2024 ] 	Batch(4000/6809) done. Loss: 0.0848  lr:0.010000
[ Sun Jul 14 21:43:49 2024 ] 	Batch(4100/6809) done. Loss: 1.1323  lr:0.010000
[ Sun Jul 14 21:44:12 2024 ] 	Batch(4200/6809) done. Loss: 0.6914  lr:0.010000
[ Sun Jul 14 21:44:34 2024 ] 	Batch(4300/6809) done. Loss: 0.7177  lr:0.010000
[ Sun Jul 14 21:44:57 2024 ] 	Batch(4400/6809) done. Loss: 0.6432  lr:0.010000
[ Sun Jul 14 21:45:20 2024 ] 
Training: Epoch [18/150], Step [4499], Loss: 0.47823166847229004, Training Accuracy: 76.19166666666666
[ Sun Jul 14 21:45:20 2024 ] 	Batch(4500/6809) done. Loss: 1.0767  lr:0.010000
[ Sun Jul 14 21:45:42 2024 ] 	Batch(4600/6809) done. Loss: 0.9631  lr:0.010000
[ Sun Jul 14 21:46:05 2024 ] 	Batch(4700/6809) done. Loss: 0.7219  lr:0.010000
[ Sun Jul 14 21:46:28 2024 ] 	Batch(4800/6809) done. Loss: 0.8655  lr:0.010000
[ Sun Jul 14 21:46:51 2024 ] 	Batch(4900/6809) done. Loss: 1.2832  lr:0.010000
[ Sun Jul 14 21:47:13 2024 ] 
Training: Epoch [18/150], Step [4999], Loss: 0.8570830225944519, Training Accuracy: 76.1225
[ Sun Jul 14 21:47:13 2024 ] 	Batch(5000/6809) done. Loss: 0.8255  lr:0.010000
[ Sun Jul 14 21:47:36 2024 ] 	Batch(5100/6809) done. Loss: 0.9773  lr:0.010000
[ Sun Jul 14 21:47:59 2024 ] 	Batch(5200/6809) done. Loss: 0.6095  lr:0.010000
[ Sun Jul 14 21:48:22 2024 ] 	Batch(5300/6809) done. Loss: 1.6002  lr:0.010000
[ Sun Jul 14 21:48:45 2024 ] 	Batch(5400/6809) done. Loss: 0.5870  lr:0.010000
[ Sun Jul 14 21:49:07 2024 ] 
Training: Epoch [18/150], Step [5499], Loss: 0.8772519826889038, Training Accuracy: 76.07045454545455
[ Sun Jul 14 21:49:07 2024 ] 	Batch(5500/6809) done. Loss: 0.6133  lr:0.010000
[ Sun Jul 14 21:49:30 2024 ] 	Batch(5600/6809) done. Loss: 0.2239  lr:0.010000
[ Sun Jul 14 21:49:53 2024 ] 	Batch(5700/6809) done. Loss: 0.4832  lr:0.010000
[ Sun Jul 14 21:50:16 2024 ] 	Batch(5800/6809) done. Loss: 0.7990  lr:0.010000
[ Sun Jul 14 21:50:38 2024 ] 	Batch(5900/6809) done. Loss: 0.3591  lr:0.010000
[ Sun Jul 14 21:51:01 2024 ] 
Training: Epoch [18/150], Step [5999], Loss: 1.616456389427185, Training Accuracy: 76.06458333333333
[ Sun Jul 14 21:51:01 2024 ] 	Batch(6000/6809) done. Loss: 0.3980  lr:0.010000
[ Sun Jul 14 21:51:24 2024 ] 	Batch(6100/6809) done. Loss: 0.9621  lr:0.010000
[ Sun Jul 14 21:51:47 2024 ] 	Batch(6200/6809) done. Loss: 0.5130  lr:0.010000
[ Sun Jul 14 21:52:10 2024 ] 	Batch(6300/6809) done. Loss: 1.1318  lr:0.010000
[ Sun Jul 14 21:52:33 2024 ] 	Batch(6400/6809) done. Loss: 2.0319  lr:0.010000
[ Sun Jul 14 21:52:55 2024 ] 
Training: Epoch [18/150], Step [6499], Loss: 0.06438815593719482, Training Accuracy: 76.05961538461538
[ Sun Jul 14 21:52:55 2024 ] 	Batch(6500/6809) done. Loss: 0.5677  lr:0.010000
[ Sun Jul 14 21:53:18 2024 ] 	Batch(6600/6809) done. Loss: 1.5662  lr:0.010000
[ Sun Jul 14 21:53:41 2024 ] 	Batch(6700/6809) done. Loss: 1.0586  lr:0.010000
[ Sun Jul 14 21:54:04 2024 ] 	Batch(6800/6809) done. Loss: 0.4069  lr:0.010000
[ Sun Jul 14 21:54:06 2024 ] 	Mean training loss: 0.7954.
[ Sun Jul 14 21:54:06 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 21:54:06 2024 ] Training epoch: 20
[ Sun Jul 14 21:54:07 2024 ] 	Batch(0/6809) done. Loss: 0.4476  lr:0.010000
[ Sun Jul 14 21:54:29 2024 ] 	Batch(100/6809) done. Loss: 0.4697  lr:0.010000
[ Sun Jul 14 21:54:52 2024 ] 	Batch(200/6809) done. Loss: 0.8611  lr:0.010000
[ Sun Jul 14 21:55:15 2024 ] 	Batch(300/6809) done. Loss: 0.5191  lr:0.010000
[ Sun Jul 14 21:55:37 2024 ] 	Batch(400/6809) done. Loss: 0.3952  lr:0.010000
[ Sun Jul 14 21:56:00 2024 ] 
Training: Epoch [19/150], Step [499], Loss: 0.20936745405197144, Training Accuracy: 77.10000000000001
[ Sun Jul 14 21:56:00 2024 ] 	Batch(500/6809) done. Loss: 0.5479  lr:0.010000
[ Sun Jul 14 21:56:23 2024 ] 	Batch(600/6809) done. Loss: 0.4065  lr:0.010000
[ Sun Jul 14 21:56:45 2024 ] 	Batch(700/6809) done. Loss: 1.4134  lr:0.010000
[ Sun Jul 14 21:57:08 2024 ] 	Batch(800/6809) done. Loss: 1.2257  lr:0.010000
[ Sun Jul 14 21:57:31 2024 ] 	Batch(900/6809) done. Loss: 0.4996  lr:0.010000
[ Sun Jul 14 21:57:53 2024 ] 
Training: Epoch [19/150], Step [999], Loss: 1.5192641019821167, Training Accuracy: 77.2625
[ Sun Jul 14 21:57:54 2024 ] 	Batch(1000/6809) done. Loss: 0.6734  lr:0.010000
[ Sun Jul 14 21:58:16 2024 ] 	Batch(1100/6809) done. Loss: 0.6171  lr:0.010000
[ Sun Jul 14 21:58:39 2024 ] 	Batch(1200/6809) done. Loss: 1.5345  lr:0.010000
[ Sun Jul 14 21:59:02 2024 ] 	Batch(1300/6809) done. Loss: 0.3094  lr:0.010000
[ Sun Jul 14 21:59:25 2024 ] 	Batch(1400/6809) done. Loss: 1.1241  lr:0.010000
[ Sun Jul 14 21:59:47 2024 ] 
Training: Epoch [19/150], Step [1499], Loss: 0.5147340893745422, Training Accuracy: 77.14999999999999
[ Sun Jul 14 21:59:48 2024 ] 	Batch(1500/6809) done. Loss: 2.6757  lr:0.010000
[ Sun Jul 14 22:00:10 2024 ] 	Batch(1600/6809) done. Loss: 0.3505  lr:0.010000
[ Sun Jul 14 22:00:33 2024 ] 	Batch(1700/6809) done. Loss: 0.5826  lr:0.010000
[ Sun Jul 14 22:00:56 2024 ] 	Batch(1800/6809) done. Loss: 0.4833  lr:0.010000
[ Sun Jul 14 22:01:18 2024 ] 	Batch(1900/6809) done. Loss: 0.7667  lr:0.010000
[ Sun Jul 14 22:01:41 2024 ] 
Training: Epoch [19/150], Step [1999], Loss: 0.5118552446365356, Training Accuracy: 76.76875
[ Sun Jul 14 22:01:41 2024 ] 	Batch(2000/6809) done. Loss: 0.3739  lr:0.010000
[ Sun Jul 14 22:02:04 2024 ] 	Batch(2100/6809) done. Loss: 0.6449  lr:0.010000
[ Sun Jul 14 22:02:27 2024 ] 	Batch(2200/6809) done. Loss: 1.0535  lr:0.010000
[ Sun Jul 14 22:02:49 2024 ] 	Batch(2300/6809) done. Loss: 0.6116  lr:0.010000
[ Sun Jul 14 22:03:12 2024 ] 	Batch(2400/6809) done. Loss: 0.7754  lr:0.010000
[ Sun Jul 14 22:03:35 2024 ] 
Training: Epoch [19/150], Step [2499], Loss: 0.41369351744651794, Training Accuracy: 76.74
[ Sun Jul 14 22:03:35 2024 ] 	Batch(2500/6809) done. Loss: 0.8987  lr:0.010000
[ Sun Jul 14 22:03:58 2024 ] 	Batch(2600/6809) done. Loss: 0.1572  lr:0.010000
[ Sun Jul 14 22:04:21 2024 ] 	Batch(2700/6809) done. Loss: 0.5217  lr:0.010000
[ Sun Jul 14 22:04:44 2024 ] 	Batch(2800/6809) done. Loss: 0.6718  lr:0.010000
[ Sun Jul 14 22:05:06 2024 ] 	Batch(2900/6809) done. Loss: 0.5621  lr:0.010000
[ Sun Jul 14 22:05:29 2024 ] 
Training: Epoch [19/150], Step [2999], Loss: 0.5440077185630798, Training Accuracy: 76.64999999999999
[ Sun Jul 14 22:05:29 2024 ] 	Batch(3000/6809) done. Loss: 1.5203  lr:0.010000
[ Sun Jul 14 22:05:52 2024 ] 	Batch(3100/6809) done. Loss: 1.1570  lr:0.010000
[ Sun Jul 14 22:06:15 2024 ] 	Batch(3200/6809) done. Loss: 0.5338  lr:0.010000
[ Sun Jul 14 22:06:37 2024 ] 	Batch(3300/6809) done. Loss: 0.5727  lr:0.010000
[ Sun Jul 14 22:07:00 2024 ] 	Batch(3400/6809) done. Loss: 0.4114  lr:0.010000
[ Sun Jul 14 22:07:23 2024 ] 
Training: Epoch [19/150], Step [3499], Loss: 0.5941191911697388, Training Accuracy: 76.68214285714285
[ Sun Jul 14 22:07:23 2024 ] 	Batch(3500/6809) done. Loss: 0.9546  lr:0.010000
[ Sun Jul 14 22:07:46 2024 ] 	Batch(3600/6809) done. Loss: 0.7108  lr:0.010000
[ Sun Jul 14 22:08:09 2024 ] 	Batch(3700/6809) done. Loss: 1.0019  lr:0.010000
[ Sun Jul 14 22:08:31 2024 ] 	Batch(3800/6809) done. Loss: 0.9881  lr:0.010000
[ Sun Jul 14 22:08:54 2024 ] 	Batch(3900/6809) done. Loss: 0.4182  lr:0.010000
[ Sun Jul 14 22:09:17 2024 ] 
Training: Epoch [19/150], Step [3999], Loss: 0.2794817090034485, Training Accuracy: 76.73124999999999
[ Sun Jul 14 22:09:17 2024 ] 	Batch(4000/6809) done. Loss: 0.6059  lr:0.010000
[ Sun Jul 14 22:09:40 2024 ] 	Batch(4100/6809) done. Loss: 0.9238  lr:0.010000
[ Sun Jul 14 22:10:04 2024 ] 	Batch(4200/6809) done. Loss: 0.5164  lr:0.010000
[ Sun Jul 14 22:10:27 2024 ] 	Batch(4300/6809) done. Loss: 1.1692  lr:0.010000
[ Sun Jul 14 22:10:50 2024 ] 	Batch(4400/6809) done. Loss: 0.3026  lr:0.010000
[ Sun Jul 14 22:11:13 2024 ] 
Training: Epoch [19/150], Step [4499], Loss: 0.3754311203956604, Training Accuracy: 76.76111111111112
[ Sun Jul 14 22:11:13 2024 ] 	Batch(4500/6809) done. Loss: 0.9897  lr:0.010000
[ Sun Jul 14 22:11:36 2024 ] 	Batch(4600/6809) done. Loss: 1.7721  lr:0.010000
[ Sun Jul 14 22:11:59 2024 ] 	Batch(4700/6809) done. Loss: 1.2341  lr:0.010000
[ Sun Jul 14 22:12:22 2024 ] 	Batch(4800/6809) done. Loss: 0.7213  lr:0.010000
[ Sun Jul 14 22:12:45 2024 ] 	Batch(4900/6809) done. Loss: 0.1366  lr:0.010000
[ Sun Jul 14 22:13:07 2024 ] 
Training: Epoch [19/150], Step [4999], Loss: 0.7457942962646484, Training Accuracy: 76.61
[ Sun Jul 14 22:13:07 2024 ] 	Batch(5000/6809) done. Loss: 1.1285  lr:0.010000
[ Sun Jul 14 22:13:30 2024 ] 	Batch(5100/6809) done. Loss: 0.8652  lr:0.010000
[ Sun Jul 14 22:13:53 2024 ] 	Batch(5200/6809) done. Loss: 0.0950  lr:0.010000
[ Sun Jul 14 22:14:16 2024 ] 	Batch(5300/6809) done. Loss: 0.8843  lr:0.010000
[ Sun Jul 14 22:14:38 2024 ] 	Batch(5400/6809) done. Loss: 1.4249  lr:0.010000
[ Sun Jul 14 22:15:01 2024 ] 
Training: Epoch [19/150], Step [5499], Loss: 1.2211947441101074, Training Accuracy: 76.50681818181818
[ Sun Jul 14 22:15:01 2024 ] 	Batch(5500/6809) done. Loss: 0.5427  lr:0.010000
[ Sun Jul 14 22:15:24 2024 ] 	Batch(5600/6809) done. Loss: 1.5223  lr:0.010000
[ Sun Jul 14 22:15:46 2024 ] 	Batch(5700/6809) done. Loss: 0.6118  lr:0.010000
[ Sun Jul 14 22:16:09 2024 ] 	Batch(5800/6809) done. Loss: 1.5464  lr:0.010000
[ Sun Jul 14 22:16:32 2024 ] 	Batch(5900/6809) done. Loss: 0.5771  lr:0.010000
[ Sun Jul 14 22:16:54 2024 ] 
Training: Epoch [19/150], Step [5999], Loss: 1.207540512084961, Training Accuracy: 76.47083333333333
[ Sun Jul 14 22:16:55 2024 ] 	Batch(6000/6809) done. Loss: 0.4713  lr:0.010000
[ Sun Jul 14 22:17:17 2024 ] 	Batch(6100/6809) done. Loss: 0.9830  lr:0.010000
[ Sun Jul 14 22:17:40 2024 ] 	Batch(6200/6809) done. Loss: 0.1313  lr:0.010000
[ Sun Jul 14 22:18:03 2024 ] 	Batch(6300/6809) done. Loss: 0.3947  lr:0.010000
[ Sun Jul 14 22:18:26 2024 ] 	Batch(6400/6809) done. Loss: 1.4660  lr:0.010000
[ Sun Jul 14 22:18:49 2024 ] 
Training: Epoch [19/150], Step [6499], Loss: 0.5422550439834595, Training Accuracy: 76.48846153846154
[ Sun Jul 14 22:18:49 2024 ] 	Batch(6500/6809) done. Loss: 0.5434  lr:0.010000
[ Sun Jul 14 22:19:12 2024 ] 	Batch(6600/6809) done. Loss: 0.6390  lr:0.010000
[ Sun Jul 14 22:19:34 2024 ] 	Batch(6700/6809) done. Loss: 1.8740  lr:0.010000
[ Sun Jul 14 22:19:57 2024 ] 	Batch(6800/6809) done. Loss: 1.0472  lr:0.010000
[ Sun Jul 14 22:19:59 2024 ] 	Mean training loss: 0.7709.
[ Sun Jul 14 22:19:59 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 22:19:59 2024 ] Eval epoch: 20
[ Sun Jul 14 22:26:55 2024 ] 	Mean val loss of 7435 batches: 1.0694308056225137.
[ Sun Jul 14 22:26:55 2024 ] 
Validation: Epoch [19/150], Samples [42652.0/59477], Loss: 1.828173279762268, Validation Accuracy: 71.71175412344267
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 1 : 358 / 500 = 71 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 2 : 415 / 499 = 83 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 3 : 346 / 500 = 69 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 4 : 408 / 502 = 81 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 5 : 318 / 502 = 63 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 6 : 388 / 502 = 77 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 7 : 446 / 497 = 89 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 8 : 466 / 498 = 93 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 9 : 254 / 500 = 50 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 10 : 256 / 500 = 51 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 11 : 43 / 498 = 8 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 12 : 369 / 499 = 73 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 13 : 486 / 502 = 96 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 14 : 444 / 504 = 88 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 15 : 381 / 502 = 75 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 16 : 195 / 502 = 38 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 17 : 342 / 504 = 67 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 18 : 385 / 504 = 76 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 19 : 378 / 502 = 75 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 20 : 389 / 502 = 77 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 21 : 474 / 503 = 94 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 22 : 381 / 504 = 75 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 23 : 326 / 503 = 64 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 24 : 438 / 504 = 86 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 25 : 449 / 504 = 89 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 26 : 429 / 504 = 85 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 27 : 406 / 501 = 81 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 28 : 292 / 502 = 58 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 29 : 386 / 502 = 76 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 30 : 371 / 501 = 74 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 31 : 310 / 504 = 61 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 32 : 405 / 503 = 80 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 33 : 419 / 503 = 83 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 34 : 470 / 504 = 93 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 35 : 464 / 503 = 92 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 36 : 390 / 502 = 77 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 37 : 399 / 504 = 79 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 38 : 386 / 504 = 76 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 39 : 436 / 498 = 87 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 40 : 357 / 504 = 70 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 41 : 440 / 503 = 87 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 42 : 422 / 504 = 83 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 43 : 234 / 503 = 46 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 44 : 383 / 504 = 75 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 45 : 408 / 504 = 80 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 46 : 373 / 504 = 74 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 47 : 366 / 503 = 72 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 48 : 416 / 503 = 82 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 49 : 272 / 499 = 54 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 50 : 344 / 502 = 68 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 51 : 442 / 503 = 87 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 52 : 438 / 504 = 86 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 53 : 435 / 497 = 87 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 54 : 449 / 480 = 93 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 55 : 399 / 504 = 79 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 56 : 347 / 503 = 68 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 57 : 475 / 504 = 94 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 58 : 424 / 499 = 84 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 59 : 430 / 503 = 85 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 60 : 391 / 479 = 81 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 61 : 426 / 484 = 88 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 62 : 369 / 487 = 75 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 63 : 412 / 489 = 84 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 64 : 351 / 488 = 71 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 65 : 412 / 490 = 84 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 66 : 284 / 488 = 58 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 67 : 368 / 490 = 75 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 68 : 326 / 490 = 66 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 69 : 399 / 490 = 81 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 70 : 194 / 490 = 39 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 71 : 56 / 490 = 11 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 72 : 127 / 488 = 26 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 73 : 159 / 486 = 32 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 74 : 177 / 481 = 36 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 75 : 278 / 488 = 56 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 76 : 315 / 489 = 64 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 77 : 237 / 488 = 48 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 78 : 320 / 488 = 65 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 79 : 426 / 490 = 86 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 80 : 348 / 489 = 71 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 81 : 192 / 491 = 39 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 82 : 331 / 491 = 67 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 83 : 197 / 489 = 40 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 84 : 294 / 489 = 60 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 85 : 280 / 489 = 57 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 86 : 358 / 491 = 72 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 87 : 311 / 492 = 63 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 88 : 270 / 491 = 54 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 89 : 310 / 492 = 63 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 90 : 216 / 490 = 44 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 91 : 376 / 482 = 78 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 92 : 338 / 490 = 68 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 93 : 288 / 487 = 59 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 94 : 398 / 489 = 81 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 95 : 406 / 490 = 82 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 96 : 452 / 491 = 92 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 97 : 462 / 490 = 94 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 98 : 434 / 491 = 88 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 99 : 434 / 491 = 88 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 100 : 446 / 491 = 90 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 101 : 400 / 491 = 81 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 102 : 283 / 492 = 57 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 103 : 365 / 492 = 74 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 104 : 213 / 491 = 43 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 105 : 256 / 491 = 52 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 106 : 190 / 492 = 38 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 107 : 334 / 491 = 68 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 108 : 320 / 492 = 65 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 109 : 249 / 490 = 50 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 110 : 410 / 491 = 83 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 111 : 461 / 492 = 93 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 112 : 419 / 492 = 85 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 113 : 374 / 491 = 76 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 114 : 368 / 491 = 74 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 115 : 411 / 492 = 83 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 116 : 390 / 491 = 79 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 117 : 359 / 492 = 72 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 118 : 317 / 490 = 64 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 119 : 418 / 492 = 84 %
[ Sun Jul 14 22:26:55 2024 ] Accuracy of 120 : 395 / 500 = 79 %
[ Sun Jul 14 22:26:55 2024 ] Training epoch: 21
[ Sun Jul 14 22:26:56 2024 ] 	Batch(0/6809) done. Loss: 0.4010  lr:0.010000
[ Sun Jul 14 22:27:19 2024 ] 	Batch(100/6809) done. Loss: 0.3861  lr:0.010000
[ Sun Jul 14 22:27:42 2024 ] 	Batch(200/6809) done. Loss: 0.0695  lr:0.010000
[ Sun Jul 14 22:28:04 2024 ] 	Batch(300/6809) done. Loss: 0.0765  lr:0.010000
[ Sun Jul 14 22:28:27 2024 ] 	Batch(400/6809) done. Loss: 0.3202  lr:0.010000
[ Sun Jul 14 22:28:50 2024 ] 
Training: Epoch [20/150], Step [499], Loss: 2.1237196922302246, Training Accuracy: 77.3
[ Sun Jul 14 22:28:50 2024 ] 	Batch(500/6809) done. Loss: 0.4349  lr:0.010000
[ Sun Jul 14 22:29:12 2024 ] 	Batch(600/6809) done. Loss: 0.6299  lr:0.010000
[ Sun Jul 14 22:29:35 2024 ] 	Batch(700/6809) done. Loss: 0.4466  lr:0.010000
[ Sun Jul 14 22:29:58 2024 ] 	Batch(800/6809) done. Loss: 0.5238  lr:0.010000
[ Sun Jul 14 22:30:20 2024 ] 	Batch(900/6809) done. Loss: 0.8169  lr:0.010000
[ Sun Jul 14 22:30:43 2024 ] 
Training: Epoch [20/150], Step [999], Loss: 0.9167451858520508, Training Accuracy: 77.3125
[ Sun Jul 14 22:30:43 2024 ] 	Batch(1000/6809) done. Loss: 0.5361  lr:0.010000
[ Sun Jul 14 22:31:06 2024 ] 	Batch(1100/6809) done. Loss: 0.8395  lr:0.010000
[ Sun Jul 14 22:31:28 2024 ] 	Batch(1200/6809) done. Loss: 0.6737  lr:0.010000
[ Sun Jul 14 22:31:51 2024 ] 	Batch(1300/6809) done. Loss: 0.2153  lr:0.010000
[ Sun Jul 14 22:32:14 2024 ] 	Batch(1400/6809) done. Loss: 0.9560  lr:0.010000
[ Sun Jul 14 22:32:36 2024 ] 
Training: Epoch [20/150], Step [1499], Loss: 0.32128047943115234, Training Accuracy: 77.30833333333334
[ Sun Jul 14 22:32:36 2024 ] 	Batch(1500/6809) done. Loss: 0.3707  lr:0.010000
[ Sun Jul 14 22:32:59 2024 ] 	Batch(1600/6809) done. Loss: 0.5407  lr:0.010000
[ Sun Jul 14 22:33:22 2024 ] 	Batch(1700/6809) done. Loss: 0.2699  lr:0.010000
[ Sun Jul 14 22:33:44 2024 ] 	Batch(1800/6809) done. Loss: 0.6856  lr:0.010000
[ Sun Jul 14 22:34:07 2024 ] 	Batch(1900/6809) done. Loss: 1.4804  lr:0.010000
[ Sun Jul 14 22:34:29 2024 ] 
Training: Epoch [20/150], Step [1999], Loss: 1.7227331399917603, Training Accuracy: 77.1125
[ Sun Jul 14 22:34:30 2024 ] 	Batch(2000/6809) done. Loss: 0.7605  lr:0.010000
[ Sun Jul 14 22:34:52 2024 ] 	Batch(2100/6809) done. Loss: 0.6922  lr:0.010000
[ Sun Jul 14 22:35:15 2024 ] 	Batch(2200/6809) done. Loss: 0.3511  lr:0.010000
[ Sun Jul 14 22:35:37 2024 ] 	Batch(2300/6809) done. Loss: 0.5628  lr:0.010000
[ Sun Jul 14 22:36:00 2024 ] 	Batch(2400/6809) done. Loss: 1.3972  lr:0.010000
[ Sun Jul 14 22:36:22 2024 ] 
Training: Epoch [20/150], Step [2499], Loss: 0.36303940415382385, Training Accuracy: 77.275
[ Sun Jul 14 22:36:23 2024 ] 	Batch(2500/6809) done. Loss: 0.8181  lr:0.010000
[ Sun Jul 14 22:36:45 2024 ] 	Batch(2600/6809) done. Loss: 1.1268  lr:0.010000
[ Sun Jul 14 22:37:08 2024 ] 	Batch(2700/6809) done. Loss: 0.7398  lr:0.010000
[ Sun Jul 14 22:37:31 2024 ] 	Batch(2800/6809) done. Loss: 0.6537  lr:0.010000
[ Sun Jul 14 22:37:53 2024 ] 	Batch(2900/6809) done. Loss: 1.3242  lr:0.010000
[ Sun Jul 14 22:38:16 2024 ] 
Training: Epoch [20/150], Step [2999], Loss: 0.3837721049785614, Training Accuracy: 77.4375
[ Sun Jul 14 22:38:16 2024 ] 	Batch(3000/6809) done. Loss: 0.3567  lr:0.010000
[ Sun Jul 14 22:38:38 2024 ] 	Batch(3100/6809) done. Loss: 0.1968  lr:0.010000
[ Sun Jul 14 22:39:01 2024 ] 	Batch(3200/6809) done. Loss: 0.4138  lr:0.010000
[ Sun Jul 14 22:39:25 2024 ] 	Batch(3300/6809) done. Loss: 1.2085  lr:0.010000
[ Sun Jul 14 22:39:48 2024 ] 	Batch(3400/6809) done. Loss: 0.7647  lr:0.010000
[ Sun Jul 14 22:40:11 2024 ] 
Training: Epoch [20/150], Step [3499], Loss: 0.22011499106884003, Training Accuracy: 77.25
[ Sun Jul 14 22:40:11 2024 ] 	Batch(3500/6809) done. Loss: 0.6567  lr:0.010000
[ Sun Jul 14 22:40:35 2024 ] 	Batch(3600/6809) done. Loss: 0.4146  lr:0.010000
[ Sun Jul 14 22:40:58 2024 ] 	Batch(3700/6809) done. Loss: 1.1578  lr:0.010000
[ Sun Jul 14 22:41:20 2024 ] 	Batch(3800/6809) done. Loss: 0.2900  lr:0.010000
[ Sun Jul 14 22:41:43 2024 ] 	Batch(3900/6809) done. Loss: 1.3622  lr:0.010000
[ Sun Jul 14 22:42:06 2024 ] 
Training: Epoch [20/150], Step [3999], Loss: 1.489130973815918, Training Accuracy: 77.17500000000001
[ Sun Jul 14 22:42:06 2024 ] 	Batch(4000/6809) done. Loss: 1.0696  lr:0.010000
[ Sun Jul 14 22:42:28 2024 ] 	Batch(4100/6809) done. Loss: 0.6940  lr:0.010000
[ Sun Jul 14 22:42:51 2024 ] 	Batch(4200/6809) done. Loss: 1.0992  lr:0.010000
[ Sun Jul 14 22:43:14 2024 ] 	Batch(4300/6809) done. Loss: 0.9300  lr:0.010000
[ Sun Jul 14 22:43:37 2024 ] 	Batch(4400/6809) done. Loss: 0.2408  lr:0.010000
[ Sun Jul 14 22:43:59 2024 ] 
Training: Epoch [20/150], Step [4499], Loss: 0.7258373498916626, Training Accuracy: 77.16388888888889
[ Sun Jul 14 22:43:59 2024 ] 	Batch(4500/6809) done. Loss: 1.1675  lr:0.010000
[ Sun Jul 14 22:44:22 2024 ] 	Batch(4600/6809) done. Loss: 0.1063  lr:0.010000
[ Sun Jul 14 22:44:44 2024 ] 	Batch(4700/6809) done. Loss: 0.6553  lr:0.010000
[ Sun Jul 14 22:45:07 2024 ] 	Batch(4800/6809) done. Loss: 1.0166  lr:0.010000
[ Sun Jul 14 22:45:30 2024 ] 	Batch(4900/6809) done. Loss: 1.2494  lr:0.010000
[ Sun Jul 14 22:45:52 2024 ] 
Training: Epoch [20/150], Step [4999], Loss: 1.0160082578659058, Training Accuracy: 77.22
[ Sun Jul 14 22:45:52 2024 ] 	Batch(5000/6809) done. Loss: 0.7528  lr:0.010000
[ Sun Jul 14 22:46:15 2024 ] 	Batch(5100/6809) done. Loss: 0.1963  lr:0.010000
[ Sun Jul 14 22:46:38 2024 ] 	Batch(5200/6809) done. Loss: 0.2490  lr:0.010000
[ Sun Jul 14 22:47:01 2024 ] 	Batch(5300/6809) done. Loss: 0.6161  lr:0.010000
[ Sun Jul 14 22:47:24 2024 ] 	Batch(5400/6809) done. Loss: 0.8315  lr:0.010000
[ Sun Jul 14 22:47:47 2024 ] 
Training: Epoch [20/150], Step [5499], Loss: 1.5949904918670654, Training Accuracy: 77.19772727272726
[ Sun Jul 14 22:47:47 2024 ] 	Batch(5500/6809) done. Loss: 1.1019  lr:0.010000
[ Sun Jul 14 22:48:10 2024 ] 	Batch(5600/6809) done. Loss: 0.8425  lr:0.010000
[ Sun Jul 14 22:48:32 2024 ] 	Batch(5700/6809) done. Loss: 0.9808  lr:0.010000
[ Sun Jul 14 22:48:55 2024 ] 	Batch(5800/6809) done. Loss: 0.4257  lr:0.010000
[ Sun Jul 14 22:49:18 2024 ] 	Batch(5900/6809) done. Loss: 0.7833  lr:0.010000
[ Sun Jul 14 22:49:40 2024 ] 
Training: Epoch [20/150], Step [5999], Loss: 1.8923195600509644, Training Accuracy: 77.1625
[ Sun Jul 14 22:49:40 2024 ] 	Batch(6000/6809) done. Loss: 0.2378  lr:0.010000
[ Sun Jul 14 22:50:03 2024 ] 	Batch(6100/6809) done. Loss: 0.6192  lr:0.010000
[ Sun Jul 14 22:50:26 2024 ] 	Batch(6200/6809) done. Loss: 0.7420  lr:0.010000
[ Sun Jul 14 22:50:48 2024 ] 	Batch(6300/6809) done. Loss: 0.9051  lr:0.010000
[ Sun Jul 14 22:51:11 2024 ] 	Batch(6400/6809) done. Loss: 0.6904  lr:0.010000
[ Sun Jul 14 22:51:33 2024 ] 
Training: Epoch [20/150], Step [6499], Loss: 0.9395910501480103, Training Accuracy: 77.24423076923077
[ Sun Jul 14 22:51:34 2024 ] 	Batch(6500/6809) done. Loss: 0.2675  lr:0.010000
[ Sun Jul 14 22:51:57 2024 ] 	Batch(6600/6809) done. Loss: 0.7973  lr:0.010000
[ Sun Jul 14 22:52:19 2024 ] 	Batch(6700/6809) done. Loss: 0.7678  lr:0.010000
[ Sun Jul 14 22:52:42 2024 ] 	Batch(6800/6809) done. Loss: 0.9443  lr:0.010000
[ Sun Jul 14 22:52:44 2024 ] 	Mean training loss: 0.7438.
[ Sun Jul 14 22:52:44 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 22:52:44 2024 ] Training epoch: 22
[ Sun Jul 14 22:52:45 2024 ] 	Batch(0/6809) done. Loss: 0.6259  lr:0.010000
[ Sun Jul 14 22:53:08 2024 ] 	Batch(100/6809) done. Loss: 0.8910  lr:0.010000
[ Sun Jul 14 22:53:31 2024 ] 	Batch(200/6809) done. Loss: 1.0384  lr:0.010000
[ Sun Jul 14 22:53:54 2024 ] 	Batch(300/6809) done. Loss: 0.3270  lr:0.010000
[ Sun Jul 14 22:54:16 2024 ] 	Batch(400/6809) done. Loss: 0.6596  lr:0.010000
[ Sun Jul 14 22:54:39 2024 ] 
Training: Epoch [21/150], Step [499], Loss: 0.5669139623641968, Training Accuracy: 77.35
[ Sun Jul 14 22:54:39 2024 ] 	Batch(500/6809) done. Loss: 0.3275  lr:0.010000
[ Sun Jul 14 22:55:02 2024 ] 	Batch(600/6809) done. Loss: 0.1043  lr:0.010000
[ Sun Jul 14 22:55:24 2024 ] 	Batch(700/6809) done. Loss: 0.6118  lr:0.010000
[ Sun Jul 14 22:55:47 2024 ] 	Batch(800/6809) done. Loss: 0.7131  lr:0.010000
[ Sun Jul 14 22:56:10 2024 ] 	Batch(900/6809) done. Loss: 0.3642  lr:0.010000
[ Sun Jul 14 22:56:32 2024 ] 
Training: Epoch [21/150], Step [999], Loss: 1.12101411819458, Training Accuracy: 77.71249999999999
[ Sun Jul 14 22:56:32 2024 ] 	Batch(1000/6809) done. Loss: 0.2604  lr:0.010000
[ Sun Jul 14 22:56:55 2024 ] 	Batch(1100/6809) done. Loss: 0.5793  lr:0.010000
[ Sun Jul 14 22:57:18 2024 ] 	Batch(1200/6809) done. Loss: 0.4627  lr:0.010000
[ Sun Jul 14 22:57:41 2024 ] 	Batch(1300/6809) done. Loss: 0.5973  lr:0.010000
[ Sun Jul 14 22:58:03 2024 ] 	Batch(1400/6809) done. Loss: 0.0631  lr:0.010000
[ Sun Jul 14 22:58:26 2024 ] 
Training: Epoch [21/150], Step [1499], Loss: 0.5956012010574341, Training Accuracy: 78.16666666666666
[ Sun Jul 14 22:58:26 2024 ] 	Batch(1500/6809) done. Loss: 0.2903  lr:0.010000
[ Sun Jul 14 22:58:49 2024 ] 	Batch(1600/6809) done. Loss: 0.4109  lr:0.010000
[ Sun Jul 14 22:59:11 2024 ] 	Batch(1700/6809) done. Loss: 0.2141  lr:0.010000
[ Sun Jul 14 22:59:34 2024 ] 	Batch(1800/6809) done. Loss: 0.9623  lr:0.010000
[ Sun Jul 14 22:59:57 2024 ] 	Batch(1900/6809) done. Loss: 2.3012  lr:0.010000
[ Sun Jul 14 23:00:19 2024 ] 
Training: Epoch [21/150], Step [1999], Loss: 0.8624919652938843, Training Accuracy: 78.03750000000001
[ Sun Jul 14 23:00:20 2024 ] 	Batch(2000/6809) done. Loss: 0.7062  lr:0.010000
[ Sun Jul 14 23:00:42 2024 ] 	Batch(2100/6809) done. Loss: 0.4682  lr:0.010000
[ Sun Jul 14 23:01:05 2024 ] 	Batch(2200/6809) done. Loss: 0.3776  lr:0.010000
[ Sun Jul 14 23:01:28 2024 ] 	Batch(2300/6809) done. Loss: 0.2805  lr:0.010000
[ Sun Jul 14 23:01:50 2024 ] 	Batch(2400/6809) done. Loss: 0.3775  lr:0.010000
[ Sun Jul 14 23:02:13 2024 ] 
Training: Epoch [21/150], Step [2499], Loss: 0.19056853652000427, Training Accuracy: 78.08
[ Sun Jul 14 23:02:13 2024 ] 	Batch(2500/6809) done. Loss: 0.6869  lr:0.010000
[ Sun Jul 14 23:02:36 2024 ] 	Batch(2600/6809) done. Loss: 0.5570  lr:0.010000
[ Sun Jul 14 23:02:58 2024 ] 	Batch(2700/6809) done. Loss: 1.4497  lr:0.010000
[ Sun Jul 14 23:03:21 2024 ] 	Batch(2800/6809) done. Loss: 0.6249  lr:0.010000
[ Sun Jul 14 23:03:44 2024 ] 	Batch(2900/6809) done. Loss: 0.6628  lr:0.010000
[ Sun Jul 14 23:04:06 2024 ] 
Training: Epoch [21/150], Step [2999], Loss: 0.6597512364387512, Training Accuracy: 78.175
[ Sun Jul 14 23:04:06 2024 ] 	Batch(3000/6809) done. Loss: 0.4255  lr:0.010000
[ Sun Jul 14 23:04:29 2024 ] 	Batch(3100/6809) done. Loss: 0.7239  lr:0.010000
[ Sun Jul 14 23:04:52 2024 ] 	Batch(3200/6809) done. Loss: 0.5300  lr:0.010000
[ Sun Jul 14 23:05:15 2024 ] 	Batch(3300/6809) done. Loss: 1.6490  lr:0.010000
[ Sun Jul 14 23:05:37 2024 ] 	Batch(3400/6809) done. Loss: 0.6533  lr:0.010000
[ Sun Jul 14 23:06:00 2024 ] 
Training: Epoch [21/150], Step [3499], Loss: 0.7374839782714844, Training Accuracy: 78.09285714285714
[ Sun Jul 14 23:06:00 2024 ] 	Batch(3500/6809) done. Loss: 0.8721  lr:0.010000
[ Sun Jul 14 23:06:23 2024 ] 	Batch(3600/6809) done. Loss: 0.8216  lr:0.010000
[ Sun Jul 14 23:06:46 2024 ] 	Batch(3700/6809) done. Loss: 0.9516  lr:0.010000
[ Sun Jul 14 23:07:08 2024 ] 	Batch(3800/6809) done. Loss: 0.6490  lr:0.010000
[ Sun Jul 14 23:07:31 2024 ] 	Batch(3900/6809) done. Loss: 1.7452  lr:0.010000
[ Sun Jul 14 23:07:54 2024 ] 
Training: Epoch [21/150], Step [3999], Loss: 0.5866760015487671, Training Accuracy: 78.03750000000001
[ Sun Jul 14 23:07:54 2024 ] 	Batch(4000/6809) done. Loss: 1.5145  lr:0.010000
[ Sun Jul 14 23:08:17 2024 ] 	Batch(4100/6809) done. Loss: 1.0337  lr:0.010000
[ Sun Jul 14 23:08:39 2024 ] 	Batch(4200/6809) done. Loss: 0.9037  lr:0.010000
[ Sun Jul 14 23:09:02 2024 ] 	Batch(4300/6809) done. Loss: 0.5163  lr:0.010000
[ Sun Jul 14 23:09:25 2024 ] 	Batch(4400/6809) done. Loss: 0.2755  lr:0.010000
[ Sun Jul 14 23:09:47 2024 ] 
Training: Epoch [21/150], Step [4499], Loss: 1.0613353252410889, Training Accuracy: 77.83611111111111
[ Sun Jul 14 23:09:47 2024 ] 	Batch(4500/6809) done. Loss: 0.9091  lr:0.010000
[ Sun Jul 14 23:10:10 2024 ] 	Batch(4600/6809) done. Loss: 0.6305  lr:0.010000
[ Sun Jul 14 23:10:33 2024 ] 	Batch(4700/6809) done. Loss: 0.3729  lr:0.010000
[ Sun Jul 14 23:10:56 2024 ] 	Batch(4800/6809) done. Loss: 0.6015  lr:0.010000
[ Sun Jul 14 23:11:19 2024 ] 	Batch(4900/6809) done. Loss: 1.0464  lr:0.010000
[ Sun Jul 14 23:11:42 2024 ] 
Training: Epoch [21/150], Step [4999], Loss: 0.9562931656837463, Training Accuracy: 77.78500000000001
[ Sun Jul 14 23:11:42 2024 ] 	Batch(5000/6809) done. Loss: 0.5085  lr:0.010000
[ Sun Jul 14 23:12:05 2024 ] 	Batch(5100/6809) done. Loss: 0.7805  lr:0.010000
[ Sun Jul 14 23:12:27 2024 ] 	Batch(5200/6809) done. Loss: 1.2979  lr:0.010000
[ Sun Jul 14 23:12:50 2024 ] 	Batch(5300/6809) done. Loss: 1.5479  lr:0.010000
[ Sun Jul 14 23:13:13 2024 ] 	Batch(5400/6809) done. Loss: 0.2028  lr:0.010000
[ Sun Jul 14 23:13:35 2024 ] 
Training: Epoch [21/150], Step [5499], Loss: 0.7565119862556458, Training Accuracy: 77.72954545454546
[ Sun Jul 14 23:13:36 2024 ] 	Batch(5500/6809) done. Loss: 0.4685  lr:0.010000
[ Sun Jul 14 23:13:58 2024 ] 	Batch(5600/6809) done. Loss: 0.9648  lr:0.010000
[ Sun Jul 14 23:14:21 2024 ] 	Batch(5700/6809) done. Loss: 0.7700  lr:0.010000
[ Sun Jul 14 23:14:44 2024 ] 	Batch(5800/6809) done. Loss: 1.4346  lr:0.010000
[ Sun Jul 14 23:15:07 2024 ] 	Batch(5900/6809) done. Loss: 0.6313  lr:0.010000
[ Sun Jul 14 23:15:29 2024 ] 
Training: Epoch [21/150], Step [5999], Loss: 0.4245280921459198, Training Accuracy: 77.81458333333333
[ Sun Jul 14 23:15:29 2024 ] 	Batch(6000/6809) done. Loss: 0.5500  lr:0.010000
[ Sun Jul 14 23:15:52 2024 ] 	Batch(6100/6809) done. Loss: 0.9849  lr:0.010000
[ Sun Jul 14 23:16:15 2024 ] 	Batch(6200/6809) done. Loss: 1.3589  lr:0.010000
[ Sun Jul 14 23:16:37 2024 ] 	Batch(6300/6809) done. Loss: 1.1755  lr:0.010000
[ Sun Jul 14 23:17:00 2024 ] 	Batch(6400/6809) done. Loss: 1.7118  lr:0.010000
[ Sun Jul 14 23:17:23 2024 ] 
Training: Epoch [21/150], Step [6499], Loss: 0.45669108629226685, Training Accuracy: 77.81153846153846
[ Sun Jul 14 23:17:23 2024 ] 	Batch(6500/6809) done. Loss: 0.5523  lr:0.010000
[ Sun Jul 14 23:17:45 2024 ] 	Batch(6600/6809) done. Loss: 0.7800  lr:0.010000
[ Sun Jul 14 23:18:08 2024 ] 	Batch(6700/6809) done. Loss: 0.5949  lr:0.010000
[ Sun Jul 14 23:18:31 2024 ] 	Batch(6800/6809) done. Loss: 0.7006  lr:0.010000
[ Sun Jul 14 23:18:33 2024 ] 	Mean training loss: 0.7286.
[ Sun Jul 14 23:18:33 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 23:18:33 2024 ] Training epoch: 23
[ Sun Jul 14 23:18:33 2024 ] 	Batch(0/6809) done. Loss: 0.8993  lr:0.010000
[ Sun Jul 14 23:18:56 2024 ] 	Batch(100/6809) done. Loss: 0.3362  lr:0.010000
[ Sun Jul 14 23:19:19 2024 ] 	Batch(200/6809) done. Loss: 0.7187  lr:0.010000
[ Sun Jul 14 23:19:42 2024 ] 	Batch(300/6809) done. Loss: 0.6275  lr:0.010000
[ Sun Jul 14 23:20:05 2024 ] 	Batch(400/6809) done. Loss: 0.7057  lr:0.010000
[ Sun Jul 14 23:20:27 2024 ] 
Training: Epoch [22/150], Step [499], Loss: 0.869373083114624, Training Accuracy: 79.35
[ Sun Jul 14 23:20:28 2024 ] 	Batch(500/6809) done. Loss: 0.8786  lr:0.010000
[ Sun Jul 14 23:20:50 2024 ] 	Batch(600/6809) done. Loss: 2.4525  lr:0.010000
[ Sun Jul 14 23:21:13 2024 ] 	Batch(700/6809) done. Loss: 0.5350  lr:0.010000
[ Sun Jul 14 23:21:36 2024 ] 	Batch(800/6809) done. Loss: 0.8966  lr:0.010000
[ Sun Jul 14 23:21:58 2024 ] 	Batch(900/6809) done. Loss: 0.7630  lr:0.010000
[ Sun Jul 14 23:22:21 2024 ] 
Training: Epoch [22/150], Step [999], Loss: 1.0320231914520264, Training Accuracy: 79.0125
[ Sun Jul 14 23:22:21 2024 ] 	Batch(1000/6809) done. Loss: 0.1335  lr:0.010000
[ Sun Jul 14 23:22:44 2024 ] 	Batch(1100/6809) done. Loss: 0.1667  lr:0.010000
[ Sun Jul 14 23:23:06 2024 ] 	Batch(1200/6809) done. Loss: 1.4657  lr:0.010000
[ Sun Jul 14 23:23:29 2024 ] 	Batch(1300/6809) done. Loss: 1.5981  lr:0.010000
[ Sun Jul 14 23:23:52 2024 ] 	Batch(1400/6809) done. Loss: 0.7885  lr:0.010000
[ Sun Jul 14 23:24:14 2024 ] 
Training: Epoch [22/150], Step [1499], Loss: 0.23529770970344543, Training Accuracy: 78.90833333333333
[ Sun Jul 14 23:24:15 2024 ] 	Batch(1500/6809) done. Loss: 0.4757  lr:0.010000
[ Sun Jul 14 23:24:37 2024 ] 	Batch(1600/6809) done. Loss: 0.3008  lr:0.010000
[ Sun Jul 14 23:25:00 2024 ] 	Batch(1700/6809) done. Loss: 0.4995  lr:0.010000
[ Sun Jul 14 23:25:23 2024 ] 	Batch(1800/6809) done. Loss: 0.5257  lr:0.010000
[ Sun Jul 14 23:25:46 2024 ] 	Batch(1900/6809) done. Loss: 0.1404  lr:0.010000
[ Sun Jul 14 23:26:08 2024 ] 
Training: Epoch [22/150], Step [1999], Loss: 0.7874760627746582, Training Accuracy: 78.55
[ Sun Jul 14 23:26:09 2024 ] 	Batch(2000/6809) done. Loss: 0.9709  lr:0.010000
[ Sun Jul 14 23:26:31 2024 ] 	Batch(2100/6809) done. Loss: 0.7639  lr:0.010000
[ Sun Jul 14 23:26:54 2024 ] 	Batch(2200/6809) done. Loss: 0.8158  lr:0.010000
[ Sun Jul 14 23:27:17 2024 ] 	Batch(2300/6809) done. Loss: 0.2892  lr:0.010000
[ Sun Jul 14 23:27:40 2024 ] 	Batch(2400/6809) done. Loss: 2.2567  lr:0.010000
[ Sun Jul 14 23:28:03 2024 ] 
Training: Epoch [22/150], Step [2499], Loss: 1.0116935968399048, Training Accuracy: 78.64
[ Sun Jul 14 23:28:03 2024 ] 	Batch(2500/6809) done. Loss: 0.2890  lr:0.010000
[ Sun Jul 14 23:28:26 2024 ] 	Batch(2600/6809) done. Loss: 0.9108  lr:0.010000
[ Sun Jul 14 23:28:48 2024 ] 	Batch(2700/6809) done. Loss: 0.6040  lr:0.010000
[ Sun Jul 14 23:29:11 2024 ] 	Batch(2800/6809) done. Loss: 0.6592  lr:0.010000
[ Sun Jul 14 23:29:35 2024 ] 	Batch(2900/6809) done. Loss: 0.6262  lr:0.010000
[ Sun Jul 14 23:29:58 2024 ] 
Training: Epoch [22/150], Step [2999], Loss: 0.48019087314605713, Training Accuracy: 78.45
[ Sun Jul 14 23:29:58 2024 ] 	Batch(3000/6809) done. Loss: 1.5033  lr:0.010000
[ Sun Jul 14 23:30:21 2024 ] 	Batch(3100/6809) done. Loss: 1.3897  lr:0.010000
[ Sun Jul 14 23:30:45 2024 ] 	Batch(3200/6809) done. Loss: 1.0068  lr:0.010000
[ Sun Jul 14 23:31:09 2024 ] 	Batch(3300/6809) done. Loss: 0.4259  lr:0.010000
[ Sun Jul 14 23:31:32 2024 ] 	Batch(3400/6809) done. Loss: 0.3326  lr:0.010000
[ Sun Jul 14 23:31:55 2024 ] 
Training: Epoch [22/150], Step [3499], Loss: 0.20419833064079285, Training Accuracy: 78.48214285714286
[ Sun Jul 14 23:31:55 2024 ] 	Batch(3500/6809) done. Loss: 1.1506  lr:0.010000
[ Sun Jul 14 23:32:18 2024 ] 	Batch(3600/6809) done. Loss: 1.4803  lr:0.010000
[ Sun Jul 14 23:32:41 2024 ] 	Batch(3700/6809) done. Loss: 0.7951  lr:0.010000
[ Sun Jul 14 23:33:04 2024 ] 	Batch(3800/6809) done. Loss: 0.4788  lr:0.010000
[ Sun Jul 14 23:33:26 2024 ] 	Batch(3900/6809) done. Loss: 0.3108  lr:0.010000
[ Sun Jul 14 23:33:49 2024 ] 
Training: Epoch [22/150], Step [3999], Loss: 0.6290246844291687, Training Accuracy: 78.396875
[ Sun Jul 14 23:33:49 2024 ] 	Batch(4000/6809) done. Loss: 0.5571  lr:0.010000
[ Sun Jul 14 23:34:12 2024 ] 	Batch(4100/6809) done. Loss: 0.8999  lr:0.010000
[ Sun Jul 14 23:34:35 2024 ] 	Batch(4200/6809) done. Loss: 1.0666  lr:0.010000
[ Sun Jul 14 23:34:57 2024 ] 	Batch(4300/6809) done. Loss: 1.6081  lr:0.010000
[ Sun Jul 14 23:35:20 2024 ] 	Batch(4400/6809) done. Loss: 0.8985  lr:0.010000
[ Sun Jul 14 23:35:42 2024 ] 
Training: Epoch [22/150], Step [4499], Loss: 1.103474736213684, Training Accuracy: 78.33888888888889
[ Sun Jul 14 23:35:43 2024 ] 	Batch(4500/6809) done. Loss: 0.2350  lr:0.010000
[ Sun Jul 14 23:36:05 2024 ] 	Batch(4600/6809) done. Loss: 0.6221  lr:0.010000
[ Sun Jul 14 23:36:28 2024 ] 	Batch(4700/6809) done. Loss: 0.7239  lr:0.010000
[ Sun Jul 14 23:36:51 2024 ] 	Batch(4800/6809) done. Loss: 0.2364  lr:0.010000
[ Sun Jul 14 23:37:14 2024 ] 	Batch(4900/6809) done. Loss: 0.0790  lr:0.010000
[ Sun Jul 14 23:37:37 2024 ] 
Training: Epoch [22/150], Step [4999], Loss: 0.4488997459411621, Training Accuracy: 78.4575
[ Sun Jul 14 23:37:37 2024 ] 	Batch(5000/6809) done. Loss: 0.9453  lr:0.010000
[ Sun Jul 14 23:37:59 2024 ] 	Batch(5100/6809) done. Loss: 0.2691  lr:0.010000
[ Sun Jul 14 23:38:22 2024 ] 	Batch(5200/6809) done. Loss: 0.6415  lr:0.010000
[ Sun Jul 14 23:38:45 2024 ] 	Batch(5300/6809) done. Loss: 0.7056  lr:0.010000
[ Sun Jul 14 23:39:08 2024 ] 	Batch(5400/6809) done. Loss: 0.8895  lr:0.010000
[ Sun Jul 14 23:39:30 2024 ] 
Training: Epoch [22/150], Step [5499], Loss: 0.4234745502471924, Training Accuracy: 78.4
[ Sun Jul 14 23:39:30 2024 ] 	Batch(5500/6809) done. Loss: 0.3795  lr:0.010000
[ Sun Jul 14 23:39:53 2024 ] 	Batch(5600/6809) done. Loss: 1.0255  lr:0.010000
[ Sun Jul 14 23:40:16 2024 ] 	Batch(5700/6809) done. Loss: 0.8442  lr:0.010000
[ Sun Jul 14 23:40:39 2024 ] 	Batch(5800/6809) done. Loss: 0.8129  lr:0.010000
[ Sun Jul 14 23:41:01 2024 ] 	Batch(5900/6809) done. Loss: 0.0511  lr:0.010000
[ Sun Jul 14 23:41:24 2024 ] 
Training: Epoch [22/150], Step [5999], Loss: 0.1769602745771408, Training Accuracy: 78.37916666666666
[ Sun Jul 14 23:41:24 2024 ] 	Batch(6000/6809) done. Loss: 0.2579  lr:0.010000
[ Sun Jul 14 23:41:47 2024 ] 	Batch(6100/6809) done. Loss: 0.4441  lr:0.010000
[ Sun Jul 14 23:42:10 2024 ] 	Batch(6200/6809) done. Loss: 1.3739  lr:0.010000
[ Sun Jul 14 23:42:33 2024 ] 	Batch(6300/6809) done. Loss: 0.1044  lr:0.010000
[ Sun Jul 14 23:42:55 2024 ] 	Batch(6400/6809) done. Loss: 1.0295  lr:0.010000
[ Sun Jul 14 23:43:18 2024 ] 
Training: Epoch [22/150], Step [6499], Loss: 1.211895227432251, Training Accuracy: 78.40576923076922
[ Sun Jul 14 23:43:18 2024 ] 	Batch(6500/6809) done. Loss: 0.1606  lr:0.010000
[ Sun Jul 14 23:43:41 2024 ] 	Batch(6600/6809) done. Loss: 1.2842  lr:0.010000
[ Sun Jul 14 23:44:04 2024 ] 	Batch(6700/6809) done. Loss: 0.5286  lr:0.010000
[ Sun Jul 14 23:44:27 2024 ] 	Batch(6800/6809) done. Loss: 0.2430  lr:0.010000
[ Sun Jul 14 23:44:28 2024 ] 	Mean training loss: 0.7137.
[ Sun Jul 14 23:44:28 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 23:44:29 2024 ] Training epoch: 24
[ Sun Jul 14 23:44:29 2024 ] 	Batch(0/6809) done. Loss: 0.4592  lr:0.010000
[ Sun Jul 14 23:44:52 2024 ] 	Batch(100/6809) done. Loss: 1.0064  lr:0.010000
[ Sun Jul 14 23:45:15 2024 ] 	Batch(200/6809) done. Loss: 0.2693  lr:0.010000
[ Sun Jul 14 23:45:38 2024 ] 	Batch(300/6809) done. Loss: 0.4989  lr:0.010000
[ Sun Jul 14 23:46:01 2024 ] 	Batch(400/6809) done. Loss: 1.0294  lr:0.010000
[ Sun Jul 14 23:46:24 2024 ] 
Training: Epoch [23/150], Step [499], Loss: 0.16078603267669678, Training Accuracy: 80.125
[ Sun Jul 14 23:46:24 2024 ] 	Batch(500/6809) done. Loss: 0.6250  lr:0.010000
[ Sun Jul 14 23:46:47 2024 ] 	Batch(600/6809) done. Loss: 0.3639  lr:0.010000
[ Sun Jul 14 23:47:09 2024 ] 	Batch(700/6809) done. Loss: 0.7348  lr:0.010000
[ Sun Jul 14 23:47:32 2024 ] 	Batch(800/6809) done. Loss: 0.1977  lr:0.010000
[ Sun Jul 14 23:47:54 2024 ] 	Batch(900/6809) done. Loss: 0.5747  lr:0.010000
[ Sun Jul 14 23:48:17 2024 ] 
Training: Epoch [23/150], Step [999], Loss: 0.9246433973312378, Training Accuracy: 79.6125
[ Sun Jul 14 23:48:17 2024 ] 	Batch(1000/6809) done. Loss: 0.8083  lr:0.010000
[ Sun Jul 14 23:48:40 2024 ] 	Batch(1100/6809) done. Loss: 1.0390  lr:0.010000
[ Sun Jul 14 23:49:03 2024 ] 	Batch(1200/6809) done. Loss: 0.1902  lr:0.010000
[ Sun Jul 14 23:49:26 2024 ] 	Batch(1300/6809) done. Loss: 0.2973  lr:0.010000
[ Sun Jul 14 23:49:49 2024 ] 	Batch(1400/6809) done. Loss: 0.8792  lr:0.010000
[ Sun Jul 14 23:50:13 2024 ] 
Training: Epoch [23/150], Step [1499], Loss: 0.9849658608436584, Training Accuracy: 79.44166666666666
[ Sun Jul 14 23:50:13 2024 ] 	Batch(1500/6809) done. Loss: 0.8947  lr:0.010000
[ Sun Jul 14 23:50:36 2024 ] 	Batch(1600/6809) done. Loss: 1.0303  lr:0.010000
[ Sun Jul 14 23:50:59 2024 ] 	Batch(1700/6809) done. Loss: 0.8159  lr:0.010000
[ Sun Jul 14 23:51:22 2024 ] 	Batch(1800/6809) done. Loss: 2.7814  lr:0.010000
[ Sun Jul 14 23:51:45 2024 ] 	Batch(1900/6809) done. Loss: 1.3222  lr:0.010000
[ Sun Jul 14 23:52:08 2024 ] 
Training: Epoch [23/150], Step [1999], Loss: 0.39389967918395996, Training Accuracy: 78.85625
[ Sun Jul 14 23:52:08 2024 ] 	Batch(2000/6809) done. Loss: 0.2430  lr:0.010000
[ Sun Jul 14 23:52:31 2024 ] 	Batch(2100/6809) done. Loss: 1.2528  lr:0.010000
[ Sun Jul 14 23:52:54 2024 ] 	Batch(2200/6809) done. Loss: 0.1719  lr:0.010000
[ Sun Jul 14 23:53:17 2024 ] 	Batch(2300/6809) done. Loss: 0.8450  lr:0.010000
[ Sun Jul 14 23:53:40 2024 ] 	Batch(2400/6809) done. Loss: 0.2863  lr:0.010000
[ Sun Jul 14 23:54:03 2024 ] 
Training: Epoch [23/150], Step [2499], Loss: 0.3523786664009094, Training Accuracy: 78.9
[ Sun Jul 14 23:54:03 2024 ] 	Batch(2500/6809) done. Loss: 1.3788  lr:0.010000
[ Sun Jul 14 23:54:26 2024 ] 	Batch(2600/6809) done. Loss: 1.0375  lr:0.010000
[ Sun Jul 14 23:54:49 2024 ] 	Batch(2700/6809) done. Loss: 0.7641  lr:0.010000
[ Sun Jul 14 23:55:13 2024 ] 	Batch(2800/6809) done. Loss: 0.9882  lr:0.010000
[ Sun Jul 14 23:55:36 2024 ] 	Batch(2900/6809) done. Loss: 0.3911  lr:0.010000
[ Sun Jul 14 23:55:58 2024 ] 
Training: Epoch [23/150], Step [2999], Loss: 0.20586085319519043, Training Accuracy: 78.82083333333333
[ Sun Jul 14 23:55:59 2024 ] 	Batch(3000/6809) done. Loss: 0.3947  lr:0.010000
[ Sun Jul 14 23:56:22 2024 ] 	Batch(3100/6809) done. Loss: 0.6029  lr:0.010000
[ Sun Jul 14 23:56:45 2024 ] 	Batch(3200/6809) done. Loss: 1.0736  lr:0.010000
[ Sun Jul 14 23:57:08 2024 ] 	Batch(3300/6809) done. Loss: 0.4728  lr:0.010000
[ Sun Jul 14 23:57:31 2024 ] 	Batch(3400/6809) done. Loss: 0.1804  lr:0.010000
[ Sun Jul 14 23:57:53 2024 ] 
Training: Epoch [23/150], Step [3499], Loss: 0.9553156495094299, Training Accuracy: 78.49285714285715
[ Sun Jul 14 23:57:54 2024 ] 	Batch(3500/6809) done. Loss: 0.5173  lr:0.010000
[ Sun Jul 14 23:58:17 2024 ] 	Batch(3600/6809) done. Loss: 0.8264  lr:0.010000
[ Sun Jul 14 23:58:40 2024 ] 	Batch(3700/6809) done. Loss: 0.7403  lr:0.010000
[ Sun Jul 14 23:59:03 2024 ] 	Batch(3800/6809) done. Loss: 0.6801  lr:0.010000
[ Sun Jul 14 23:59:26 2024 ] 	Batch(3900/6809) done. Loss: 0.8654  lr:0.010000
[ Sun Jul 14 23:59:48 2024 ] 
Training: Epoch [23/150], Step [3999], Loss: 1.0256565809249878, Training Accuracy: 78.51875
[ Sun Jul 14 23:59:49 2024 ] 	Batch(4000/6809) done. Loss: 0.9169  lr:0.010000
[ Mon Jul 15 00:00:12 2024 ] 	Batch(4100/6809) done. Loss: 0.6009  lr:0.010000
[ Mon Jul 15 00:00:35 2024 ] 	Batch(4200/6809) done. Loss: 0.3046  lr:0.010000
[ Mon Jul 15 00:00:57 2024 ] 	Batch(4300/6809) done. Loss: 0.5232  lr:0.010000
[ Mon Jul 15 00:01:20 2024 ] 	Batch(4400/6809) done. Loss: 0.6761  lr:0.010000
[ Mon Jul 15 00:01:42 2024 ] 
Training: Epoch [23/150], Step [4499], Loss: 0.4483720064163208, Training Accuracy: 78.54166666666667
[ Mon Jul 15 00:01:42 2024 ] 	Batch(4500/6809) done. Loss: 1.1338  lr:0.010000
[ Mon Jul 15 00:02:05 2024 ] 	Batch(4600/6809) done. Loss: 0.8505  lr:0.010000
[ Mon Jul 15 00:02:27 2024 ] 	Batch(4700/6809) done. Loss: 0.8664  lr:0.010000
[ Mon Jul 15 00:02:50 2024 ] 	Batch(4800/6809) done. Loss: 0.7700  lr:0.010000
[ Mon Jul 15 00:03:13 2024 ] 	Batch(4900/6809) done. Loss: 0.3594  lr:0.010000
[ Mon Jul 15 00:03:35 2024 ] 
Training: Epoch [23/150], Step [4999], Loss: 0.9803878664970398, Training Accuracy: 78.4525
[ Mon Jul 15 00:03:35 2024 ] 	Batch(5000/6809) done. Loss: 0.6426  lr:0.010000
[ Mon Jul 15 00:03:58 2024 ] 	Batch(5100/6809) done. Loss: 0.7428  lr:0.010000
[ Mon Jul 15 00:04:21 2024 ] 	Batch(5200/6809) done. Loss: 0.6555  lr:0.010000
[ Mon Jul 15 00:04:43 2024 ] 	Batch(5300/6809) done. Loss: 0.8344  lr:0.010000
[ Mon Jul 15 00:05:06 2024 ] 	Batch(5400/6809) done. Loss: 0.8418  lr:0.010000
[ Mon Jul 15 00:05:29 2024 ] 
Training: Epoch [23/150], Step [5499], Loss: 0.602828323841095, Training Accuracy: 78.44090909090909
[ Mon Jul 15 00:05:29 2024 ] 	Batch(5500/6809) done. Loss: 1.5054  lr:0.010000
[ Mon Jul 15 00:05:53 2024 ] 	Batch(5600/6809) done. Loss: 0.1825  lr:0.010000
[ Mon Jul 15 00:06:16 2024 ] 	Batch(5700/6809) done. Loss: 0.1761  lr:0.010000
[ Mon Jul 15 00:06:39 2024 ] 	Batch(5800/6809) done. Loss: 0.3396  lr:0.010000
[ Mon Jul 15 00:07:02 2024 ] 	Batch(5900/6809) done. Loss: 1.2146  lr:0.010000
[ Mon Jul 15 00:07:25 2024 ] 
Training: Epoch [23/150], Step [5999], Loss: 0.6345651149749756, Training Accuracy: 78.5
[ Mon Jul 15 00:07:25 2024 ] 	Batch(6000/6809) done. Loss: 0.9062  lr:0.010000
[ Mon Jul 15 00:07:48 2024 ] 	Batch(6100/6809) done. Loss: 1.4411  lr:0.010000
[ Mon Jul 15 00:08:11 2024 ] 	Batch(6200/6809) done. Loss: 0.6552  lr:0.010000
[ Mon Jul 15 00:08:34 2024 ] 	Batch(6300/6809) done. Loss: 0.7091  lr:0.010000
[ Mon Jul 15 00:08:57 2024 ] 	Batch(6400/6809) done. Loss: 0.3784  lr:0.010000
[ Mon Jul 15 00:09:20 2024 ] 
Training: Epoch [23/150], Step [6499], Loss: 0.3776513338088989, Training Accuracy: 78.49038461538461
[ Mon Jul 15 00:09:20 2024 ] 	Batch(6500/6809) done. Loss: 0.5059  lr:0.010000
[ Mon Jul 15 00:09:42 2024 ] 	Batch(6600/6809) done. Loss: 0.7553  lr:0.010000
[ Mon Jul 15 00:10:05 2024 ] 	Batch(6700/6809) done. Loss: 1.5764  lr:0.010000
[ Mon Jul 15 00:10:28 2024 ] 	Batch(6800/6809) done. Loss: 0.2542  lr:0.010000
[ Mon Jul 15 00:10:29 2024 ] 	Mean training loss: 0.7031.
[ Mon Jul 15 00:10:29 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 00:10:30 2024 ] Training epoch: 25
[ Mon Jul 15 00:10:30 2024 ] 	Batch(0/6809) done. Loss: 0.4869  lr:0.010000
[ Mon Jul 15 00:10:53 2024 ] 	Batch(100/6809) done. Loss: 0.8315  lr:0.010000
[ Mon Jul 15 00:11:16 2024 ] 	Batch(200/6809) done. Loss: 1.5805  lr:0.010000
[ Mon Jul 15 00:11:39 2024 ] 	Batch(300/6809) done. Loss: 0.1263  lr:0.010000
[ Mon Jul 15 00:12:03 2024 ] 	Batch(400/6809) done. Loss: 1.0176  lr:0.010000
[ Mon Jul 15 00:12:26 2024 ] 
Training: Epoch [24/150], Step [499], Loss: 0.41480550169944763, Training Accuracy: 80.325
[ Mon Jul 15 00:12:26 2024 ] 	Batch(500/6809) done. Loss: 1.8740  lr:0.010000
[ Mon Jul 15 00:12:49 2024 ] 	Batch(600/6809) done. Loss: 0.9720  lr:0.010000
[ Mon Jul 15 00:13:12 2024 ] 	Batch(700/6809) done. Loss: 0.6601  lr:0.010000
[ Mon Jul 15 00:13:34 2024 ] 	Batch(800/6809) done. Loss: 0.1914  lr:0.010000
[ Mon Jul 15 00:13:57 2024 ] 	Batch(900/6809) done. Loss: 1.0719  lr:0.010000
[ Mon Jul 15 00:14:20 2024 ] 
Training: Epoch [24/150], Step [999], Loss: 0.6146335005760193, Training Accuracy: 80.4125
[ Mon Jul 15 00:14:20 2024 ] 	Batch(1000/6809) done. Loss: 0.3210  lr:0.010000
[ Mon Jul 15 00:14:42 2024 ] 	Batch(1100/6809) done. Loss: 0.1004  lr:0.010000
[ Mon Jul 15 00:15:05 2024 ] 	Batch(1200/6809) done. Loss: 1.2568  lr:0.010000
[ Mon Jul 15 00:15:28 2024 ] 	Batch(1300/6809) done. Loss: 0.6934  lr:0.010000
[ Mon Jul 15 00:15:51 2024 ] 	Batch(1400/6809) done. Loss: 0.7888  lr:0.010000
[ Mon Jul 15 00:16:13 2024 ] 
Training: Epoch [24/150], Step [1499], Loss: 1.3032370805740356, Training Accuracy: 80.125
[ Mon Jul 15 00:16:13 2024 ] 	Batch(1500/6809) done. Loss: 0.3098  lr:0.010000
[ Mon Jul 15 00:16:36 2024 ] 	Batch(1600/6809) done. Loss: 0.8908  lr:0.010000
[ Mon Jul 15 00:16:59 2024 ] 	Batch(1700/6809) done. Loss: 0.6813  lr:0.010000
[ Mon Jul 15 00:17:21 2024 ] 	Batch(1800/6809) done. Loss: 0.2424  lr:0.010000
[ Mon Jul 15 00:17:44 2024 ] 	Batch(1900/6809) done. Loss: 0.3444  lr:0.010000
[ Mon Jul 15 00:18:07 2024 ] 
Training: Epoch [24/150], Step [1999], Loss: 0.9261841177940369, Training Accuracy: 79.9625
[ Mon Jul 15 00:18:07 2024 ] 	Batch(2000/6809) done. Loss: 0.7789  lr:0.010000
[ Mon Jul 15 00:18:30 2024 ] 	Batch(2100/6809) done. Loss: 0.1041  lr:0.010000
[ Mon Jul 15 00:18:53 2024 ] 	Batch(2200/6809) done. Loss: 0.0653  lr:0.010000
[ Mon Jul 15 00:19:15 2024 ] 	Batch(2300/6809) done. Loss: 0.4283  lr:0.010000
[ Mon Jul 15 00:19:38 2024 ] 	Batch(2400/6809) done. Loss: 0.4268  lr:0.010000
[ Mon Jul 15 00:20:01 2024 ] 
Training: Epoch [24/150], Step [2499], Loss: 0.8059016466140747, Training Accuracy: 79.905
[ Mon Jul 15 00:20:01 2024 ] 	Batch(2500/6809) done. Loss: 0.8986  lr:0.010000
[ Mon Jul 15 00:20:24 2024 ] 	Batch(2600/6809) done. Loss: 0.9279  lr:0.010000
[ Mon Jul 15 00:20:46 2024 ] 	Batch(2700/6809) done. Loss: 0.6022  lr:0.010000
[ Mon Jul 15 00:21:09 2024 ] 	Batch(2800/6809) done. Loss: 0.2553  lr:0.010000
[ Mon Jul 15 00:21:32 2024 ] 	Batch(2900/6809) done. Loss: 0.8050  lr:0.010000
[ Mon Jul 15 00:21:54 2024 ] 
Training: Epoch [24/150], Step [2999], Loss: 0.10301044583320618, Training Accuracy: 79.87083333333334
[ Mon Jul 15 00:21:55 2024 ] 	Batch(3000/6809) done. Loss: 0.8580  lr:0.010000
[ Mon Jul 15 00:22:17 2024 ] 	Batch(3100/6809) done. Loss: 0.5309  lr:0.010000
[ Mon Jul 15 00:22:40 2024 ] 	Batch(3200/6809) done. Loss: 0.4247  lr:0.010000
[ Mon Jul 15 00:23:03 2024 ] 	Batch(3300/6809) done. Loss: 0.6572  lr:0.010000
[ Mon Jul 15 00:23:26 2024 ] 	Batch(3400/6809) done. Loss: 0.6205  lr:0.010000
[ Mon Jul 15 00:23:48 2024 ] 
Training: Epoch [24/150], Step [3499], Loss: 0.8119497299194336, Training Accuracy: 79.64642857142857
[ Mon Jul 15 00:23:48 2024 ] 	Batch(3500/6809) done. Loss: 0.6276  lr:0.010000
[ Mon Jul 15 00:24:11 2024 ] 	Batch(3600/6809) done. Loss: 0.2858  lr:0.010000
[ Mon Jul 15 00:24:34 2024 ] 	Batch(3700/6809) done. Loss: 0.5901  lr:0.010000
[ Mon Jul 15 00:24:57 2024 ] 	Batch(3800/6809) done. Loss: 0.8560  lr:0.010000
[ Mon Jul 15 00:25:20 2024 ] 	Batch(3900/6809) done. Loss: 0.1565  lr:0.010000
[ Mon Jul 15 00:25:42 2024 ] 
Training: Epoch [24/150], Step [3999], Loss: 0.35092517733573914, Training Accuracy: 79.484375
[ Mon Jul 15 00:25:42 2024 ] 	Batch(4000/6809) done. Loss: 0.3748  lr:0.010000
[ Mon Jul 15 00:26:05 2024 ] 	Batch(4100/6809) done. Loss: 0.6714  lr:0.010000
[ Mon Jul 15 00:26:28 2024 ] 	Batch(4200/6809) done. Loss: 0.5937  lr:0.010000
[ Mon Jul 15 00:26:50 2024 ] 	Batch(4300/6809) done. Loss: 0.7930  lr:0.010000
[ Mon Jul 15 00:27:13 2024 ] 	Batch(4400/6809) done. Loss: 1.6076  lr:0.010000
[ Mon Jul 15 00:27:36 2024 ] 
Training: Epoch [24/150], Step [4499], Loss: 0.03885149583220482, Training Accuracy: 79.34722222222223
[ Mon Jul 15 00:27:36 2024 ] 	Batch(4500/6809) done. Loss: 1.3255  lr:0.010000
[ Mon Jul 15 00:27:59 2024 ] 	Batch(4600/6809) done. Loss: 1.5671  lr:0.010000
[ Mon Jul 15 00:28:21 2024 ] 	Batch(4700/6809) done. Loss: 1.1386  lr:0.010000
[ Mon Jul 15 00:28:44 2024 ] 	Batch(4800/6809) done. Loss: 1.0589  lr:0.010000
[ Mon Jul 15 00:29:07 2024 ] 	Batch(4900/6809) done. Loss: 0.6816  lr:0.010000
[ Mon Jul 15 00:29:29 2024 ] 
Training: Epoch [24/150], Step [4999], Loss: 0.41519397497177124, Training Accuracy: 79.35
[ Mon Jul 15 00:29:29 2024 ] 	Batch(5000/6809) done. Loss: 0.6819  lr:0.010000
[ Mon Jul 15 00:29:52 2024 ] 	Batch(5100/6809) done. Loss: 0.8893  lr:0.010000
[ Mon Jul 15 00:30:15 2024 ] 	Batch(5200/6809) done. Loss: 0.7905  lr:0.010000
[ Mon Jul 15 00:30:38 2024 ] 	Batch(5300/6809) done. Loss: 0.8046  lr:0.010000
[ Mon Jul 15 00:31:01 2024 ] 	Batch(5400/6809) done. Loss: 0.7843  lr:0.010000
[ Mon Jul 15 00:31:24 2024 ] 
Training: Epoch [24/150], Step [5499], Loss: 0.28447625041007996, Training Accuracy: 79.41818181818182
[ Mon Jul 15 00:31:24 2024 ] 	Batch(5500/6809) done. Loss: 0.5080  lr:0.010000
[ Mon Jul 15 00:31:47 2024 ] 	Batch(5600/6809) done. Loss: 0.7846  lr:0.010000
[ Mon Jul 15 00:32:11 2024 ] 	Batch(5700/6809) done. Loss: 0.4423  lr:0.010000
[ Mon Jul 15 00:32:34 2024 ] 	Batch(5800/6809) done. Loss: 1.1366  lr:0.010000
[ Mon Jul 15 00:32:57 2024 ] 	Batch(5900/6809) done. Loss: 0.2889  lr:0.010000
[ Mon Jul 15 00:33:19 2024 ] 
Training: Epoch [24/150], Step [5999], Loss: 0.8879771828651428, Training Accuracy: 79.35
[ Mon Jul 15 00:33:20 2024 ] 	Batch(6000/6809) done. Loss: 0.3547  lr:0.010000
[ Mon Jul 15 00:33:42 2024 ] 	Batch(6100/6809) done. Loss: 0.7795  lr:0.010000
[ Mon Jul 15 00:34:05 2024 ] 	Batch(6200/6809) done. Loss: 0.6176  lr:0.010000
[ Mon Jul 15 00:34:28 2024 ] 	Batch(6300/6809) done. Loss: 0.7195  lr:0.010000
[ Mon Jul 15 00:34:51 2024 ] 	Batch(6400/6809) done. Loss: 0.4872  lr:0.010000
[ Mon Jul 15 00:35:13 2024 ] 
Training: Epoch [24/150], Step [6499], Loss: 0.6673962473869324, Training Accuracy: 79.24423076923077
[ Mon Jul 15 00:35:13 2024 ] 	Batch(6500/6809) done. Loss: 0.6202  lr:0.010000
[ Mon Jul 15 00:35:36 2024 ] 	Batch(6600/6809) done. Loss: 2.5885  lr:0.010000
[ Mon Jul 15 00:35:59 2024 ] 	Batch(6700/6809) done. Loss: 0.9665  lr:0.010000
[ Mon Jul 15 00:36:21 2024 ] 	Batch(6800/6809) done. Loss: 0.3455  lr:0.010000
[ Mon Jul 15 00:36:23 2024 ] 	Mean training loss: 0.6797.
[ Mon Jul 15 00:36:23 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 00:36:23 2024 ] Training epoch: 26
[ Mon Jul 15 00:36:24 2024 ] 	Batch(0/6809) done. Loss: 1.1154  lr:0.010000
[ Mon Jul 15 00:36:47 2024 ] 	Batch(100/6809) done. Loss: 0.6043  lr:0.010000
[ Mon Jul 15 00:37:10 2024 ] 	Batch(200/6809) done. Loss: 0.2675  lr:0.010000
[ Mon Jul 15 00:37:34 2024 ] 	Batch(300/6809) done. Loss: 0.5935  lr:0.010000
[ Mon Jul 15 00:37:57 2024 ] 	Batch(400/6809) done. Loss: 0.3495  lr:0.010000
[ Mon Jul 15 00:38:20 2024 ] 
Training: Epoch [25/150], Step [499], Loss: 0.44199416041374207, Training Accuracy: 79.925
[ Mon Jul 15 00:38:20 2024 ] 	Batch(500/6809) done. Loss: 0.5009  lr:0.010000
[ Mon Jul 15 00:38:43 2024 ] 	Batch(600/6809) done. Loss: 0.8320  lr:0.010000
[ Mon Jul 15 00:39:05 2024 ] 	Batch(700/6809) done. Loss: 0.2563  lr:0.010000
[ Mon Jul 15 00:39:29 2024 ] 	Batch(800/6809) done. Loss: 0.3570  lr:0.010000
[ Mon Jul 15 00:39:52 2024 ] 	Batch(900/6809) done. Loss: 1.1748  lr:0.010000
[ Mon Jul 15 00:40:15 2024 ] 
Training: Epoch [25/150], Step [999], Loss: 0.33181706070899963, Training Accuracy: 80.4125
[ Mon Jul 15 00:40:15 2024 ] 	Batch(1000/6809) done. Loss: 0.4198  lr:0.010000
[ Mon Jul 15 00:40:38 2024 ] 	Batch(1100/6809) done. Loss: 0.4726  lr:0.010000
[ Mon Jul 15 00:41:01 2024 ] 	Batch(1200/6809) done. Loss: 0.3003  lr:0.010000
[ Mon Jul 15 00:41:25 2024 ] 	Batch(1300/6809) done. Loss: 1.0435  lr:0.010000
[ Mon Jul 15 00:41:48 2024 ] 	Batch(1400/6809) done. Loss: 0.8122  lr:0.010000
[ Mon Jul 15 00:42:11 2024 ] 
Training: Epoch [25/150], Step [1499], Loss: 0.8582445383071899, Training Accuracy: 79.88333333333333
[ Mon Jul 15 00:42:11 2024 ] 	Batch(1500/6809) done. Loss: 0.4201  lr:0.010000
[ Mon Jul 15 00:42:34 2024 ] 	Batch(1600/6809) done. Loss: 0.1106  lr:0.010000
[ Mon Jul 15 00:42:57 2024 ] 	Batch(1700/6809) done. Loss: 1.0919  lr:0.010000
[ Mon Jul 15 00:43:20 2024 ] 	Batch(1800/6809) done. Loss: 0.3580  lr:0.010000
[ Mon Jul 15 00:43:43 2024 ] 	Batch(1900/6809) done. Loss: 0.9019  lr:0.010000
[ Mon Jul 15 00:44:06 2024 ] 
Training: Epoch [25/150], Step [1999], Loss: 1.1655604839324951, Training Accuracy: 79.74375
[ Mon Jul 15 00:44:06 2024 ] 	Batch(2000/6809) done. Loss: 0.5172  lr:0.010000
[ Mon Jul 15 00:44:29 2024 ] 	Batch(2100/6809) done. Loss: 0.9043  lr:0.010000
[ Mon Jul 15 00:44:52 2024 ] 	Batch(2200/6809) done. Loss: 0.0409  lr:0.010000
[ Mon Jul 15 00:45:15 2024 ] 	Batch(2300/6809) done. Loss: 0.3796  lr:0.010000
[ Mon Jul 15 00:45:38 2024 ] 	Batch(2400/6809) done. Loss: 0.1187  lr:0.010000
[ Mon Jul 15 00:46:01 2024 ] 
Training: Epoch [25/150], Step [2499], Loss: 1.3231077194213867, Training Accuracy: 79.72
[ Mon Jul 15 00:46:01 2024 ] 	Batch(2500/6809) done. Loss: 0.6233  lr:0.010000
[ Mon Jul 15 00:46:24 2024 ] 	Batch(2600/6809) done. Loss: 0.4832  lr:0.010000
[ Mon Jul 15 00:46:48 2024 ] 	Batch(2700/6809) done. Loss: 0.2037  lr:0.010000
[ Mon Jul 15 00:47:11 2024 ] 	Batch(2800/6809) done. Loss: 0.2794  lr:0.010000
[ Mon Jul 15 00:47:34 2024 ] 	Batch(2900/6809) done. Loss: 0.3982  lr:0.010000
[ Mon Jul 15 00:47:57 2024 ] 
Training: Epoch [25/150], Step [2999], Loss: 0.3398473262786865, Training Accuracy: 79.67083333333333
[ Mon Jul 15 00:47:57 2024 ] 	Batch(3000/6809) done. Loss: 0.1867  lr:0.010000
[ Mon Jul 15 00:48:20 2024 ] 	Batch(3100/6809) done. Loss: 0.0993  lr:0.010000
[ Mon Jul 15 00:48:44 2024 ] 	Batch(3200/6809) done. Loss: 0.3915  lr:0.010000
[ Mon Jul 15 00:49:07 2024 ] 	Batch(3300/6809) done. Loss: 0.9092  lr:0.010000
[ Mon Jul 15 00:49:30 2024 ] 	Batch(3400/6809) done. Loss: 0.2822  lr:0.010000
[ Mon Jul 15 00:49:52 2024 ] 
Training: Epoch [25/150], Step [3499], Loss: 1.0176246166229248, Training Accuracy: 79.64285714285714
[ Mon Jul 15 00:49:52 2024 ] 	Batch(3500/6809) done. Loss: 0.6030  lr:0.010000
[ Mon Jul 15 00:50:16 2024 ] 	Batch(3600/6809) done. Loss: 0.2753  lr:0.010000
[ Mon Jul 15 00:50:39 2024 ] 	Batch(3700/6809) done. Loss: 0.4199  lr:0.010000
[ Mon Jul 15 00:51:01 2024 ] 	Batch(3800/6809) done. Loss: 1.0738  lr:0.010000
[ Mon Jul 15 00:51:24 2024 ] 	Batch(3900/6809) done. Loss: 0.7021  lr:0.010000
[ Mon Jul 15 00:51:47 2024 ] 
Training: Epoch [25/150], Step [3999], Loss: 0.4570714235305786, Training Accuracy: 79.609375
[ Mon Jul 15 00:51:48 2024 ] 	Batch(4000/6809) done. Loss: 0.5414  lr:0.010000
[ Mon Jul 15 00:52:11 2024 ] 	Batch(4100/6809) done. Loss: 0.7016  lr:0.010000
[ Mon Jul 15 00:52:33 2024 ] 	Batch(4200/6809) done. Loss: 0.1527  lr:0.010000
[ Mon Jul 15 00:52:56 2024 ] 	Batch(4300/6809) done. Loss: 0.3735  lr:0.010000
[ Mon Jul 15 00:53:20 2024 ] 	Batch(4400/6809) done. Loss: 0.0507  lr:0.010000
[ Mon Jul 15 00:53:42 2024 ] 
Training: Epoch [25/150], Step [4499], Loss: 0.609760582447052, Training Accuracy: 79.73611111111111
[ Mon Jul 15 00:53:43 2024 ] 	Batch(4500/6809) done. Loss: 0.6111  lr:0.010000
[ Mon Jul 15 00:54:06 2024 ] 	Batch(4600/6809) done. Loss: 1.1416  lr:0.010000
[ Mon Jul 15 00:54:29 2024 ] 	Batch(4700/6809) done. Loss: 0.2038  lr:0.010000
[ Mon Jul 15 00:54:52 2024 ] 	Batch(4800/6809) done. Loss: 0.6359  lr:0.010000
[ Mon Jul 15 00:55:15 2024 ] 	Batch(4900/6809) done. Loss: 0.6584  lr:0.010000
[ Mon Jul 15 00:55:37 2024 ] 
Training: Epoch [25/150], Step [4999], Loss: 1.232710361480713, Training Accuracy: 79.64750000000001
[ Mon Jul 15 00:55:38 2024 ] 	Batch(5000/6809) done. Loss: 0.9257  lr:0.010000
[ Mon Jul 15 00:56:01 2024 ] 	Batch(5100/6809) done. Loss: 1.3450  lr:0.010000
[ Mon Jul 15 00:56:24 2024 ] 	Batch(5200/6809) done. Loss: 1.2622  lr:0.010000
[ Mon Jul 15 00:56:46 2024 ] 	Batch(5300/6809) done. Loss: 0.4926  lr:0.010000
[ Mon Jul 15 00:57:09 2024 ] 	Batch(5400/6809) done. Loss: 0.3951  lr:0.010000
[ Mon Jul 15 00:57:31 2024 ] 
Training: Epoch [25/150], Step [5499], Loss: 1.0264594554901123, Training Accuracy: 79.52954545454546
[ Mon Jul 15 00:57:31 2024 ] 	Batch(5500/6809) done. Loss: 0.5640  lr:0.010000
[ Mon Jul 15 00:57:54 2024 ] 	Batch(5600/6809) done. Loss: 0.3988  lr:0.010000
[ Mon Jul 15 00:58:18 2024 ] 	Batch(5700/6809) done. Loss: 0.5292  lr:0.010000
[ Mon Jul 15 00:58:42 2024 ] 	Batch(5800/6809) done. Loss: 0.5570  lr:0.010000
[ Mon Jul 15 00:59:05 2024 ] 	Batch(5900/6809) done. Loss: 0.2365  lr:0.010000
[ Mon Jul 15 00:59:27 2024 ] 
Training: Epoch [25/150], Step [5999], Loss: 2.424886465072632, Training Accuracy: 79.53750000000001
[ Mon Jul 15 00:59:28 2024 ] 	Batch(6000/6809) done. Loss: 0.1856  lr:0.010000
[ Mon Jul 15 00:59:51 2024 ] 	Batch(6100/6809) done. Loss: 1.2367  lr:0.010000
[ Mon Jul 15 01:00:14 2024 ] 	Batch(6200/6809) done. Loss: 0.8995  lr:0.010000
[ Mon Jul 15 01:00:37 2024 ] 	Batch(6300/6809) done. Loss: 0.9797  lr:0.010000
[ Mon Jul 15 01:00:59 2024 ] 	Batch(6400/6809) done. Loss: 0.9042  lr:0.010000
[ Mon Jul 15 01:01:22 2024 ] 
Training: Epoch [25/150], Step [6499], Loss: 0.9815464615821838, Training Accuracy: 79.48076923076923
[ Mon Jul 15 01:01:22 2024 ] 	Batch(6500/6809) done. Loss: 0.1819  lr:0.010000
[ Mon Jul 15 01:01:44 2024 ] 	Batch(6600/6809) done. Loss: 0.4904  lr:0.010000
[ Mon Jul 15 01:02:07 2024 ] 	Batch(6700/6809) done. Loss: 1.0289  lr:0.010000
[ Mon Jul 15 01:02:30 2024 ] 	Batch(6800/6809) done. Loss: 0.1474  lr:0.010000
[ Mon Jul 15 01:02:31 2024 ] 	Mean training loss: 0.6597.
[ Mon Jul 15 01:02:31 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 01:02:32 2024 ] Training epoch: 27
[ Mon Jul 15 01:02:32 2024 ] 	Batch(0/6809) done. Loss: 1.0042  lr:0.010000
[ Mon Jul 15 01:02:55 2024 ] 	Batch(100/6809) done. Loss: 0.1372  lr:0.010000
[ Mon Jul 15 01:03:18 2024 ] 	Batch(200/6809) done. Loss: 0.4195  lr:0.010000
[ Mon Jul 15 01:03:42 2024 ] 	Batch(300/6809) done. Loss: 1.0269  lr:0.010000
[ Mon Jul 15 01:04:05 2024 ] 	Batch(400/6809) done. Loss: 1.0757  lr:0.010000
[ Mon Jul 15 01:04:28 2024 ] 
Training: Epoch [26/150], Step [499], Loss: 1.0674289464950562, Training Accuracy: 80.675
[ Mon Jul 15 01:04:28 2024 ] 	Batch(500/6809) done. Loss: 1.5219  lr:0.010000
[ Mon Jul 15 01:04:51 2024 ] 	Batch(600/6809) done. Loss: 0.1140  lr:0.010000
[ Mon Jul 15 01:05:14 2024 ] 	Batch(700/6809) done. Loss: 0.6170  lr:0.010000
[ Mon Jul 15 01:05:37 2024 ] 	Batch(800/6809) done. Loss: 0.7125  lr:0.010000
[ Mon Jul 15 01:06:00 2024 ] 	Batch(900/6809) done. Loss: 0.7886  lr:0.010000
[ Mon Jul 15 01:06:22 2024 ] 
Training: Epoch [26/150], Step [999], Loss: 0.4315011501312256, Training Accuracy: 81.13749999999999
[ Mon Jul 15 01:06:23 2024 ] 	Batch(1000/6809) done. Loss: 0.1638  lr:0.010000
[ Mon Jul 15 01:06:45 2024 ] 	Batch(1100/6809) done. Loss: 0.6053  lr:0.010000
[ Mon Jul 15 01:07:08 2024 ] 	Batch(1200/6809) done. Loss: 0.5090  lr:0.010000
[ Mon Jul 15 01:07:31 2024 ] 	Batch(1300/6809) done. Loss: 0.3739  lr:0.010000
[ Mon Jul 15 01:07:53 2024 ] 	Batch(1400/6809) done. Loss: 0.3920  lr:0.010000
[ Mon Jul 15 01:08:16 2024 ] 
Training: Epoch [26/150], Step [1499], Loss: 0.7275835871696472, Training Accuracy: 80.84166666666667
[ Mon Jul 15 01:08:16 2024 ] 	Batch(1500/6809) done. Loss: 0.4379  lr:0.010000
[ Mon Jul 15 01:08:39 2024 ] 	Batch(1600/6809) done. Loss: 0.5087  lr:0.010000
[ Mon Jul 15 01:09:02 2024 ] 	Batch(1700/6809) done. Loss: 0.2908  lr:0.010000
[ Mon Jul 15 01:09:24 2024 ] 	Batch(1800/6809) done. Loss: 0.6365  lr:0.010000
[ Mon Jul 15 01:09:47 2024 ] 	Batch(1900/6809) done. Loss: 0.2401  lr:0.010000
[ Mon Jul 15 01:10:09 2024 ] 
Training: Epoch [26/150], Step [1999], Loss: 0.8907054662704468, Training Accuracy: 81.01875
[ Mon Jul 15 01:10:10 2024 ] 	Batch(2000/6809) done. Loss: 0.5301  lr:0.010000
[ Mon Jul 15 01:10:32 2024 ] 	Batch(2100/6809) done. Loss: 0.7510  lr:0.010000
[ Mon Jul 15 01:10:55 2024 ] 	Batch(2200/6809) done. Loss: 1.2549  lr:0.010000
[ Mon Jul 15 01:11:18 2024 ] 	Batch(2300/6809) done. Loss: 1.3176  lr:0.010000
[ Mon Jul 15 01:11:41 2024 ] 	Batch(2400/6809) done. Loss: 0.6901  lr:0.010000
[ Mon Jul 15 01:12:04 2024 ] 
Training: Epoch [26/150], Step [2499], Loss: 0.4737066328525543, Training Accuracy: 80.72500000000001
[ Mon Jul 15 01:12:04 2024 ] 	Batch(2500/6809) done. Loss: 0.8287  lr:0.010000
[ Mon Jul 15 01:12:27 2024 ] 	Batch(2600/6809) done. Loss: 0.4447  lr:0.010000
[ Mon Jul 15 01:12:51 2024 ] 	Batch(2700/6809) done. Loss: 1.0893  lr:0.010000
[ Mon Jul 15 01:13:14 2024 ] 	Batch(2800/6809) done. Loss: 1.1690  lr:0.010000
[ Mon Jul 15 01:13:37 2024 ] 	Batch(2900/6809) done. Loss: 0.9425  lr:0.010000
[ Mon Jul 15 01:14:00 2024 ] 
Training: Epoch [26/150], Step [2999], Loss: 0.9855177402496338, Training Accuracy: 80.48333333333333
[ Mon Jul 15 01:14:00 2024 ] 	Batch(3000/6809) done. Loss: 1.7983  lr:0.010000
[ Mon Jul 15 01:14:23 2024 ] 	Batch(3100/6809) done. Loss: 1.3811  lr:0.010000
[ Mon Jul 15 01:14:47 2024 ] 	Batch(3200/6809) done. Loss: 0.8848  lr:0.010000
[ Mon Jul 15 01:15:09 2024 ] 	Batch(3300/6809) done. Loss: 0.5272  lr:0.010000
[ Mon Jul 15 01:15:32 2024 ] 	Batch(3400/6809) done. Loss: 0.7149  lr:0.010000
[ Mon Jul 15 01:15:54 2024 ] 
Training: Epoch [26/150], Step [3499], Loss: 0.37175852060317993, Training Accuracy: 80.43571428571428
[ Mon Jul 15 01:15:55 2024 ] 	Batch(3500/6809) done. Loss: 0.3788  lr:0.010000
[ Mon Jul 15 01:16:18 2024 ] 	Batch(3600/6809) done. Loss: 0.1476  lr:0.010000
[ Mon Jul 15 01:16:40 2024 ] 	Batch(3700/6809) done. Loss: 0.8050  lr:0.010000
[ Mon Jul 15 01:17:03 2024 ] 	Batch(3800/6809) done. Loss: 0.3121  lr:0.010000
[ Mon Jul 15 01:17:26 2024 ] 	Batch(3900/6809) done. Loss: 1.5929  lr:0.010000
[ Mon Jul 15 01:17:48 2024 ] 
Training: Epoch [26/150], Step [3999], Loss: 0.3802916407585144, Training Accuracy: 80.21875
[ Mon Jul 15 01:17:48 2024 ] 	Batch(4000/6809) done. Loss: 0.6937  lr:0.010000
[ Mon Jul 15 01:18:11 2024 ] 	Batch(4100/6809) done. Loss: 0.3993  lr:0.010000
[ Mon Jul 15 01:18:34 2024 ] 	Batch(4200/6809) done. Loss: 0.5708  lr:0.010000
[ Mon Jul 15 01:18:56 2024 ] 	Batch(4300/6809) done. Loss: 1.0171  lr:0.010000
[ Mon Jul 15 01:19:19 2024 ] 	Batch(4400/6809) done. Loss: 0.3884  lr:0.010000
[ Mon Jul 15 01:19:42 2024 ] 
Training: Epoch [26/150], Step [4499], Loss: 0.07644671946763992, Training Accuracy: 80.10833333333333
[ Mon Jul 15 01:19:42 2024 ] 	Batch(4500/6809) done. Loss: 1.3374  lr:0.010000
[ Mon Jul 15 01:20:05 2024 ] 	Batch(4600/6809) done. Loss: 0.3907  lr:0.010000
[ Mon Jul 15 01:20:27 2024 ] 	Batch(4700/6809) done. Loss: 1.5538  lr:0.010000
[ Mon Jul 15 01:20:50 2024 ] 	Batch(4800/6809) done. Loss: 0.3085  lr:0.010000
[ Mon Jul 15 01:21:13 2024 ] 	Batch(4900/6809) done. Loss: 0.4724  lr:0.010000
[ Mon Jul 15 01:21:35 2024 ] 
Training: Epoch [26/150], Step [4999], Loss: 0.6736705303192139, Training Accuracy: 80.0975
[ Mon Jul 15 01:21:36 2024 ] 	Batch(5000/6809) done. Loss: 2.1664  lr:0.010000
[ Mon Jul 15 01:21:58 2024 ] 	Batch(5100/6809) done. Loss: 0.5026  lr:0.010000
[ Mon Jul 15 01:22:21 2024 ] 	Batch(5200/6809) done. Loss: 0.4206  lr:0.010000
[ Mon Jul 15 01:22:44 2024 ] 	Batch(5300/6809) done. Loss: 0.7218  lr:0.010000
[ Mon Jul 15 01:23:07 2024 ] 	Batch(5400/6809) done. Loss: 0.3902  lr:0.010000
[ Mon Jul 15 01:23:29 2024 ] 
Training: Epoch [26/150], Step [5499], Loss: 0.7065373659133911, Training Accuracy: 80.03863636363636
[ Mon Jul 15 01:23:29 2024 ] 	Batch(5500/6809) done. Loss: 1.2248  lr:0.010000
[ Mon Jul 15 01:23:52 2024 ] 	Batch(5600/6809) done. Loss: 0.9296  lr:0.010000
[ Mon Jul 15 01:24:15 2024 ] 	Batch(5700/6809) done. Loss: 0.6759  lr:0.010000
[ Mon Jul 15 01:24:37 2024 ] 	Batch(5800/6809) done. Loss: 0.1285  lr:0.010000
[ Mon Jul 15 01:25:00 2024 ] 	Batch(5900/6809) done. Loss: 0.4891  lr:0.010000
[ Mon Jul 15 01:25:23 2024 ] 
Training: Epoch [26/150], Step [5999], Loss: 0.3258667588233948, Training Accuracy: 79.99791666666667
[ Mon Jul 15 01:25:24 2024 ] 	Batch(6000/6809) done. Loss: 0.8806  lr:0.010000
[ Mon Jul 15 01:25:46 2024 ] 	Batch(6100/6809) done. Loss: 0.4659  lr:0.010000
[ Mon Jul 15 01:26:09 2024 ] 	Batch(6200/6809) done. Loss: 0.4805  lr:0.010000
[ Mon Jul 15 01:26:32 2024 ] 	Batch(6300/6809) done. Loss: 0.4673  lr:0.010000
[ Mon Jul 15 01:26:55 2024 ] 	Batch(6400/6809) done. Loss: 0.2005  lr:0.010000
[ Mon Jul 15 01:27:17 2024 ] 
Training: Epoch [26/150], Step [6499], Loss: 1.2121589183807373, Training Accuracy: 80.00384615384615
[ Mon Jul 15 01:27:18 2024 ] 	Batch(6500/6809) done. Loss: 0.6800  lr:0.010000
[ Mon Jul 15 01:27:40 2024 ] 	Batch(6600/6809) done. Loss: 0.4509  lr:0.010000
[ Mon Jul 15 01:28:03 2024 ] 	Batch(6700/6809) done. Loss: 1.1087  lr:0.010000
[ Mon Jul 15 01:28:26 2024 ] 	Batch(6800/6809) done. Loss: 0.2090  lr:0.010000
[ Mon Jul 15 01:28:28 2024 ] 	Mean training loss: 0.6436.
[ Mon Jul 15 01:28:28 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 01:28:28 2024 ] Training epoch: 28
[ Mon Jul 15 01:28:29 2024 ] 	Batch(0/6809) done. Loss: 0.7720  lr:0.010000
[ Mon Jul 15 01:28:52 2024 ] 	Batch(100/6809) done. Loss: 1.3698  lr:0.010000
[ Mon Jul 15 01:29:14 2024 ] 	Batch(200/6809) done. Loss: 1.1566  lr:0.010000
[ Mon Jul 15 01:29:37 2024 ] 	Batch(300/6809) done. Loss: 1.1083  lr:0.010000
[ Mon Jul 15 01:30:01 2024 ] 	Batch(400/6809) done. Loss: 0.2897  lr:0.010000
[ Mon Jul 15 01:30:23 2024 ] 
Training: Epoch [27/150], Step [499], Loss: 0.648507833480835, Training Accuracy: 80.27499999999999
[ Mon Jul 15 01:30:23 2024 ] 	Batch(500/6809) done. Loss: 0.8155  lr:0.010000
[ Mon Jul 15 01:30:46 2024 ] 	Batch(600/6809) done. Loss: 0.3947  lr:0.010000
[ Mon Jul 15 01:31:08 2024 ] 	Batch(700/6809) done. Loss: 0.4316  lr:0.010000
[ Mon Jul 15 01:31:31 2024 ] 	Batch(800/6809) done. Loss: 1.7659  lr:0.010000
[ Mon Jul 15 01:31:53 2024 ] 	Batch(900/6809) done. Loss: 0.6994  lr:0.010000
[ Mon Jul 15 01:32:16 2024 ] 
Training: Epoch [27/150], Step [999], Loss: 0.5735049247741699, Training Accuracy: 80.5875
[ Mon Jul 15 01:32:16 2024 ] 	Batch(1000/6809) done. Loss: 0.4080  lr:0.010000
[ Mon Jul 15 01:32:39 2024 ] 	Batch(1100/6809) done. Loss: 0.3598  lr:0.010000
[ Mon Jul 15 01:33:03 2024 ] 	Batch(1200/6809) done. Loss: 0.1400  lr:0.010000
[ Mon Jul 15 01:33:26 2024 ] 	Batch(1300/6809) done. Loss: 1.6626  lr:0.010000
[ Mon Jul 15 01:33:49 2024 ] 	Batch(1400/6809) done. Loss: 0.3348  lr:0.010000
[ Mon Jul 15 01:34:11 2024 ] 
Training: Epoch [27/150], Step [1499], Loss: 0.9197621941566467, Training Accuracy: 80.44166666666666
[ Mon Jul 15 01:34:11 2024 ] 	Batch(1500/6809) done. Loss: 0.6235  lr:0.010000
[ Mon Jul 15 01:34:35 2024 ] 	Batch(1600/6809) done. Loss: 1.9137  lr:0.010000
[ Mon Jul 15 01:34:57 2024 ] 	Batch(1700/6809) done. Loss: 0.7707  lr:0.010000
[ Mon Jul 15 01:35:20 2024 ] 	Batch(1800/6809) done. Loss: 0.3740  lr:0.010000
[ Mon Jul 15 01:35:42 2024 ] 	Batch(1900/6809) done. Loss: 0.6870  lr:0.010000
[ Mon Jul 15 01:36:05 2024 ] 
Training: Epoch [27/150], Step [1999], Loss: 0.018929332494735718, Training Accuracy: 80.6875
[ Mon Jul 15 01:36:05 2024 ] 	Batch(2000/6809) done. Loss: 0.3324  lr:0.010000
[ Mon Jul 15 01:36:28 2024 ] 	Batch(2100/6809) done. Loss: 0.6443  lr:0.010000
[ Mon Jul 15 01:36:50 2024 ] 	Batch(2200/6809) done. Loss: 0.8015  lr:0.010000
[ Mon Jul 15 01:37:13 2024 ] 	Batch(2300/6809) done. Loss: 1.2382  lr:0.010000
[ Mon Jul 15 01:37:36 2024 ] 	Batch(2400/6809) done. Loss: 1.1394  lr:0.010000
[ Mon Jul 15 01:37:58 2024 ] 
Training: Epoch [27/150], Step [2499], Loss: 0.35191622376441956, Training Accuracy: 80.715
[ Mon Jul 15 01:37:58 2024 ] 	Batch(2500/6809) done. Loss: 0.4808  lr:0.010000
[ Mon Jul 15 01:38:21 2024 ] 	Batch(2600/6809) done. Loss: 0.5592  lr:0.010000
[ Mon Jul 15 01:38:44 2024 ] 	Batch(2700/6809) done. Loss: 1.0258  lr:0.010000
[ Mon Jul 15 01:39:07 2024 ] 	Batch(2800/6809) done. Loss: 1.0868  lr:0.010000
[ Mon Jul 15 01:39:30 2024 ] 	Batch(2900/6809) done. Loss: 0.1832  lr:0.010000
[ Mon Jul 15 01:39:53 2024 ] 
Training: Epoch [27/150], Step [2999], Loss: 0.5792439579963684, Training Accuracy: 80.46666666666667
[ Mon Jul 15 01:39:53 2024 ] 	Batch(3000/6809) done. Loss: 0.9660  lr:0.010000
[ Mon Jul 15 01:40:17 2024 ] 	Batch(3100/6809) done. Loss: 0.1912  lr:0.010000
[ Mon Jul 15 01:40:40 2024 ] 	Batch(3200/6809) done. Loss: 0.3065  lr:0.010000
[ Mon Jul 15 01:41:03 2024 ] 	Batch(3300/6809) done. Loss: 0.2640  lr:0.010000
[ Mon Jul 15 01:41:26 2024 ] 	Batch(3400/6809) done. Loss: 0.1817  lr:0.010000
[ Mon Jul 15 01:41:49 2024 ] 
Training: Epoch [27/150], Step [3499], Loss: 1.2682747840881348, Training Accuracy: 80.49285714285715
[ Mon Jul 15 01:41:49 2024 ] 	Batch(3500/6809) done. Loss: 0.9522  lr:0.010000
[ Mon Jul 15 01:42:12 2024 ] 	Batch(3600/6809) done. Loss: 1.2567  lr:0.010000
[ Mon Jul 15 01:42:35 2024 ] 	Batch(3700/6809) done. Loss: 0.1182  lr:0.010000
[ Mon Jul 15 01:42:58 2024 ] 	Batch(3800/6809) done. Loss: 0.0801  lr:0.010000
[ Mon Jul 15 01:43:21 2024 ] 	Batch(3900/6809) done. Loss: 1.1139  lr:0.010000
[ Mon Jul 15 01:43:44 2024 ] 
Training: Epoch [27/150], Step [3999], Loss: 0.2785555124282837, Training Accuracy: 80.440625
[ Mon Jul 15 01:43:44 2024 ] 	Batch(4000/6809) done. Loss: 0.2128  lr:0.010000
[ Mon Jul 15 01:44:07 2024 ] 	Batch(4100/6809) done. Loss: 0.3772  lr:0.010000
[ Mon Jul 15 01:44:30 2024 ] 	Batch(4200/6809) done. Loss: 0.4319  lr:0.010000
[ Mon Jul 15 01:44:53 2024 ] 	Batch(4300/6809) done. Loss: 0.8322  lr:0.010000
[ Mon Jul 15 01:45:16 2024 ] 	Batch(4400/6809) done. Loss: 0.9464  lr:0.010000
[ Mon Jul 15 01:45:39 2024 ] 
Training: Epoch [27/150], Step [4499], Loss: 0.6272002458572388, Training Accuracy: 80.38055555555556
[ Mon Jul 15 01:45:39 2024 ] 	Batch(4500/6809) done. Loss: 0.3794  lr:0.010000
[ Mon Jul 15 01:46:02 2024 ] 	Batch(4600/6809) done. Loss: 0.3071  lr:0.010000
[ Mon Jul 15 01:46:25 2024 ] 	Batch(4700/6809) done. Loss: 0.2830  lr:0.010000
[ Mon Jul 15 01:46:47 2024 ] 	Batch(4800/6809) done. Loss: 1.0743  lr:0.010000
[ Mon Jul 15 01:47:10 2024 ] 	Batch(4900/6809) done. Loss: 0.5720  lr:0.010000
[ Mon Jul 15 01:47:32 2024 ] 
Training: Epoch [27/150], Step [4999], Loss: 1.2654931545257568, Training Accuracy: 80.22
[ Mon Jul 15 01:47:32 2024 ] 	Batch(5000/6809) done. Loss: 0.3265  lr:0.010000
[ Mon Jul 15 01:47:55 2024 ] 	Batch(5100/6809) done. Loss: 0.6046  lr:0.010000
[ Mon Jul 15 01:48:18 2024 ] 	Batch(5200/6809) done. Loss: 0.7488  lr:0.010000
[ Mon Jul 15 01:48:41 2024 ] 	Batch(5300/6809) done. Loss: 0.1655  lr:0.010000
[ Mon Jul 15 01:49:03 2024 ] 	Batch(5400/6809) done. Loss: 0.4305  lr:0.010000
[ Mon Jul 15 01:49:26 2024 ] 
Training: Epoch [27/150], Step [5499], Loss: 0.47884416580200195, Training Accuracy: 80.2159090909091
[ Mon Jul 15 01:49:26 2024 ] 	Batch(5500/6809) done. Loss: 0.8620  lr:0.010000
[ Mon Jul 15 01:49:50 2024 ] 	Batch(5600/6809) done. Loss: 0.4309  lr:0.010000
[ Mon Jul 15 01:50:13 2024 ] 	Batch(5700/6809) done. Loss: 0.4373  lr:0.010000
[ Mon Jul 15 01:50:35 2024 ] 	Batch(5800/6809) done. Loss: 0.9548  lr:0.010000
[ Mon Jul 15 01:50:58 2024 ] 	Batch(5900/6809) done. Loss: 0.3772  lr:0.010000
[ Mon Jul 15 01:51:20 2024 ] 
Training: Epoch [27/150], Step [5999], Loss: 0.5701071619987488, Training Accuracy: 80.2625
[ Mon Jul 15 01:51:21 2024 ] 	Batch(6000/6809) done. Loss: 0.5774  lr:0.010000
[ Mon Jul 15 01:51:44 2024 ] 	Batch(6100/6809) done. Loss: 1.1995  lr:0.010000
[ Mon Jul 15 01:52:07 2024 ] 	Batch(6200/6809) done. Loss: 0.2841  lr:0.010000
[ Mon Jul 15 01:52:30 2024 ] 	Batch(6300/6809) done. Loss: 0.6745  lr:0.010000
[ Mon Jul 15 01:52:54 2024 ] 	Batch(6400/6809) done. Loss: 0.7662  lr:0.010000
[ Mon Jul 15 01:53:17 2024 ] 
Training: Epoch [27/150], Step [6499], Loss: 0.7080295085906982, Training Accuracy: 80.20961538461539
[ Mon Jul 15 01:53:17 2024 ] 	Batch(6500/6809) done. Loss: 0.1803  lr:0.010000
[ Mon Jul 15 01:53:40 2024 ] 	Batch(6600/6809) done. Loss: 0.1192  lr:0.010000
[ Mon Jul 15 01:54:03 2024 ] 	Batch(6700/6809) done. Loss: 1.3810  lr:0.010000
[ Mon Jul 15 01:54:26 2024 ] 	Batch(6800/6809) done. Loss: 1.2909  lr:0.010000
[ Mon Jul 15 01:54:28 2024 ] 	Mean training loss: 0.6311.
[ Mon Jul 15 01:54:28 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 01:54:28 2024 ] Training epoch: 29
[ Mon Jul 15 01:54:29 2024 ] 	Batch(0/6809) done. Loss: 0.0588  lr:0.010000
[ Mon Jul 15 01:54:52 2024 ] 	Batch(100/6809) done. Loss: 0.2072  lr:0.010000
[ Mon Jul 15 01:55:15 2024 ] 	Batch(200/6809) done. Loss: 0.3865  lr:0.010000
[ Mon Jul 15 01:55:37 2024 ] 	Batch(300/6809) done. Loss: 0.4079  lr:0.010000
[ Mon Jul 15 01:56:00 2024 ] 	Batch(400/6809) done. Loss: 1.1684  lr:0.010000
[ Mon Jul 15 01:56:23 2024 ] 
Training: Epoch [28/150], Step [499], Loss: 0.95027095079422, Training Accuracy: 81.72500000000001
[ Mon Jul 15 01:56:23 2024 ] 	Batch(500/6809) done. Loss: 0.2997  lr:0.010000
[ Mon Jul 15 01:56:46 2024 ] 	Batch(600/6809) done. Loss: 0.3729  lr:0.010000
[ Mon Jul 15 01:57:08 2024 ] 	Batch(700/6809) done. Loss: 0.1588  lr:0.010000
[ Mon Jul 15 01:57:31 2024 ] 	Batch(800/6809) done. Loss: 0.7187  lr:0.010000
[ Mon Jul 15 01:57:55 2024 ] 	Batch(900/6809) done. Loss: 0.4135  lr:0.010000
[ Mon Jul 15 01:58:18 2024 ] 
Training: Epoch [28/150], Step [999], Loss: 0.5556603670120239, Training Accuracy: 81.375
[ Mon Jul 15 01:58:18 2024 ] 	Batch(1000/6809) done. Loss: 0.4760  lr:0.010000
[ Mon Jul 15 01:58:42 2024 ] 	Batch(1100/6809) done. Loss: 0.5159  lr:0.010000
[ Mon Jul 15 01:59:05 2024 ] 	Batch(1200/6809) done. Loss: 0.2596  lr:0.010000
[ Mon Jul 15 01:59:28 2024 ] 	Batch(1300/6809) done. Loss: 0.0657  lr:0.010000
[ Mon Jul 15 01:59:51 2024 ] 	Batch(1400/6809) done. Loss: 0.3263  lr:0.010000
[ Mon Jul 15 02:00:13 2024 ] 
Training: Epoch [28/150], Step [1499], Loss: 0.3254402279853821, Training Accuracy: 81.35
[ Mon Jul 15 02:00:13 2024 ] 	Batch(1500/6809) done. Loss: 0.5795  lr:0.010000
[ Mon Jul 15 02:00:36 2024 ] 	Batch(1600/6809) done. Loss: 0.7685  lr:0.010000
[ Mon Jul 15 02:00:59 2024 ] 	Batch(1700/6809) done. Loss: 0.3723  lr:0.010000
[ Mon Jul 15 02:01:22 2024 ] 	Batch(1800/6809) done. Loss: 0.1989  lr:0.010000
[ Mon Jul 15 02:01:44 2024 ] 	Batch(1900/6809) done. Loss: 0.1276  lr:0.010000
[ Mon Jul 15 02:02:07 2024 ] 
Training: Epoch [28/150], Step [1999], Loss: 0.36173462867736816, Training Accuracy: 81.20625
[ Mon Jul 15 02:02:07 2024 ] 	Batch(2000/6809) done. Loss: 0.9059  lr:0.010000
[ Mon Jul 15 02:02:30 2024 ] 	Batch(2100/6809) done. Loss: 0.3077  lr:0.010000
[ Mon Jul 15 02:02:53 2024 ] 	Batch(2200/6809) done. Loss: 0.3058  lr:0.010000
[ Mon Jul 15 02:03:15 2024 ] 	Batch(2300/6809) done. Loss: 0.6362  lr:0.010000
[ Mon Jul 15 02:03:38 2024 ] 	Batch(2400/6809) done. Loss: 0.3011  lr:0.010000
[ Mon Jul 15 02:04:01 2024 ] 
Training: Epoch [28/150], Step [2499], Loss: 0.9556373357772827, Training Accuracy: 81.25
[ Mon Jul 15 02:04:01 2024 ] 	Batch(2500/6809) done. Loss: 0.0154  lr:0.010000
[ Mon Jul 15 02:04:23 2024 ] 	Batch(2600/6809) done. Loss: 0.3964  lr:0.010000
[ Mon Jul 15 02:04:46 2024 ] 	Batch(2700/6809) done. Loss: 0.7617  lr:0.010000
[ Mon Jul 15 02:05:09 2024 ] 	Batch(2800/6809) done. Loss: 0.2607  lr:0.010000
[ Mon Jul 15 02:05:32 2024 ] 	Batch(2900/6809) done. Loss: 0.2209  lr:0.010000
[ Mon Jul 15 02:05:54 2024 ] 
Training: Epoch [28/150], Step [2999], Loss: 0.34180089831352234, Training Accuracy: 81.1875
[ Mon Jul 15 02:05:54 2024 ] 	Batch(3000/6809) done. Loss: 0.4725  lr:0.010000
[ Mon Jul 15 02:06:17 2024 ] 	Batch(3100/6809) done. Loss: 0.9162  lr:0.010000
[ Mon Jul 15 02:06:40 2024 ] 	Batch(3200/6809) done. Loss: 0.4059  lr:0.010000
[ Mon Jul 15 02:07:03 2024 ] 	Batch(3300/6809) done. Loss: 1.5340  lr:0.010000
[ Mon Jul 15 02:07:25 2024 ] 	Batch(3400/6809) done. Loss: 0.3383  lr:0.010000
[ Mon Jul 15 02:07:48 2024 ] 
Training: Epoch [28/150], Step [3499], Loss: 1.0647778511047363, Training Accuracy: 81.25
[ Mon Jul 15 02:07:48 2024 ] 	Batch(3500/6809) done. Loss: 1.0304  lr:0.010000
[ Mon Jul 15 02:08:11 2024 ] 	Batch(3600/6809) done. Loss: 0.3473  lr:0.010000
[ Mon Jul 15 02:08:34 2024 ] 	Batch(3700/6809) done. Loss: 0.7045  lr:0.010000
[ Mon Jul 15 02:08:56 2024 ] 	Batch(3800/6809) done. Loss: 0.6717  lr:0.010000
[ Mon Jul 15 02:09:19 2024 ] 	Batch(3900/6809) done. Loss: 0.5512  lr:0.010000
[ Mon Jul 15 02:09:41 2024 ] 
Training: Epoch [28/150], Step [3999], Loss: 0.36547040939331055, Training Accuracy: 81.06875
[ Mon Jul 15 02:09:42 2024 ] 	Batch(4000/6809) done. Loss: 0.6196  lr:0.010000
[ Mon Jul 15 02:10:05 2024 ] 	Batch(4100/6809) done. Loss: 0.4065  lr:0.010000
[ Mon Jul 15 02:10:27 2024 ] 	Batch(4200/6809) done. Loss: 0.4229  lr:0.010000
[ Mon Jul 15 02:10:50 2024 ] 	Batch(4300/6809) done. Loss: 0.2454  lr:0.010000
[ Mon Jul 15 02:11:13 2024 ] 	Batch(4400/6809) done. Loss: 0.3621  lr:0.010000
[ Mon Jul 15 02:11:35 2024 ] 
Training: Epoch [28/150], Step [4499], Loss: 0.7508416771888733, Training Accuracy: 81.06944444444444
[ Mon Jul 15 02:11:35 2024 ] 	Batch(4500/6809) done. Loss: 0.1833  lr:0.010000
[ Mon Jul 15 02:11:58 2024 ] 	Batch(4600/6809) done. Loss: 0.7263  lr:0.010000
[ Mon Jul 15 02:12:22 2024 ] 	Batch(4700/6809) done. Loss: 0.7148  lr:0.010000
[ Mon Jul 15 02:12:46 2024 ] 	Batch(4800/6809) done. Loss: 0.1360  lr:0.010000
[ Mon Jul 15 02:13:08 2024 ] 	Batch(4900/6809) done. Loss: 0.9466  lr:0.010000
[ Mon Jul 15 02:13:31 2024 ] 
Training: Epoch [28/150], Step [4999], Loss: 0.547537088394165, Training Accuracy: 80.95
[ Mon Jul 15 02:13:31 2024 ] 	Batch(5000/6809) done. Loss: 1.0316  lr:0.010000
[ Mon Jul 15 02:13:54 2024 ] 	Batch(5100/6809) done. Loss: 1.2283  lr:0.010000
[ Mon Jul 15 02:14:16 2024 ] 	Batch(5200/6809) done. Loss: 0.4828  lr:0.010000
[ Mon Jul 15 02:14:39 2024 ] 	Batch(5300/6809) done. Loss: 0.1586  lr:0.010000
[ Mon Jul 15 02:15:02 2024 ] 	Batch(5400/6809) done. Loss: 0.8019  lr:0.010000
[ Mon Jul 15 02:15:24 2024 ] 
Training: Epoch [28/150], Step [5499], Loss: 0.23611317574977875, Training Accuracy: 80.84772727272728
[ Mon Jul 15 02:15:25 2024 ] 	Batch(5500/6809) done. Loss: 0.7721  lr:0.010000
[ Mon Jul 15 02:15:47 2024 ] 	Batch(5600/6809) done. Loss: 0.4294  lr:0.010000
[ Mon Jul 15 02:16:10 2024 ] 	Batch(5700/6809) done. Loss: 0.0326  lr:0.010000
[ Mon Jul 15 02:16:33 2024 ] 	Batch(5800/6809) done. Loss: 0.8877  lr:0.010000
[ Mon Jul 15 02:16:55 2024 ] 	Batch(5900/6809) done. Loss: 0.8049  lr:0.010000
[ Mon Jul 15 02:17:18 2024 ] 
Training: Epoch [28/150], Step [5999], Loss: 0.8812479972839355, Training Accuracy: 80.83749999999999
[ Mon Jul 15 02:17:18 2024 ] 	Batch(6000/6809) done. Loss: 1.3281  lr:0.010000
[ Mon Jul 15 02:17:41 2024 ] 	Batch(6100/6809) done. Loss: 0.4672  lr:0.010000
[ Mon Jul 15 02:18:04 2024 ] 	Batch(6200/6809) done. Loss: 0.8371  lr:0.010000
[ Mon Jul 15 02:18:26 2024 ] 	Batch(6300/6809) done. Loss: 0.5656  lr:0.010000
[ Mon Jul 15 02:18:49 2024 ] 	Batch(6400/6809) done. Loss: 0.4381  lr:0.010000
[ Mon Jul 15 02:19:12 2024 ] 
Training: Epoch [28/150], Step [6499], Loss: 0.9850850105285645, Training Accuracy: 80.8173076923077
[ Mon Jul 15 02:19:12 2024 ] 	Batch(6500/6809) done. Loss: 0.4523  lr:0.010000
[ Mon Jul 15 02:19:35 2024 ] 	Batch(6600/6809) done. Loss: 0.2224  lr:0.010000
[ Mon Jul 15 02:19:58 2024 ] 	Batch(6700/6809) done. Loss: 1.0619  lr:0.010000
[ Mon Jul 15 02:20:20 2024 ] 	Batch(6800/6809) done. Loss: 0.1761  lr:0.010000
[ Mon Jul 15 02:20:22 2024 ] 	Mean training loss: 0.6150.
[ Mon Jul 15 02:20:22 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 02:20:23 2024 ] Training epoch: 30
[ Mon Jul 15 02:20:23 2024 ] 	Batch(0/6809) done. Loss: 0.2878  lr:0.010000
[ Mon Jul 15 02:20:46 2024 ] 	Batch(100/6809) done. Loss: 0.7661  lr:0.010000
[ Mon Jul 15 02:21:09 2024 ] 	Batch(200/6809) done. Loss: 0.4161  lr:0.010000
[ Mon Jul 15 02:21:32 2024 ] 	Batch(300/6809) done. Loss: 0.2304  lr:0.010000
[ Mon Jul 15 02:21:54 2024 ] 	Batch(400/6809) done. Loss: 0.1540  lr:0.010000
[ Mon Jul 15 02:22:17 2024 ] 
Training: Epoch [29/150], Step [499], Loss: 0.7639122605323792, Training Accuracy: 81.325
[ Mon Jul 15 02:22:17 2024 ] 	Batch(500/6809) done. Loss: 0.2356  lr:0.010000
[ Mon Jul 15 02:22:39 2024 ] 	Batch(600/6809) done. Loss: 0.2422  lr:0.010000
[ Mon Jul 15 02:23:02 2024 ] 	Batch(700/6809) done. Loss: 1.4101  lr:0.010000
[ Mon Jul 15 02:23:25 2024 ] 	Batch(800/6809) done. Loss: 0.3167  lr:0.010000
[ Mon Jul 15 02:23:48 2024 ] 	Batch(900/6809) done. Loss: 1.4588  lr:0.010000
[ Mon Jul 15 02:24:10 2024 ] 
Training: Epoch [29/150], Step [999], Loss: 0.9605827927589417, Training Accuracy: 81.69999999999999
[ Mon Jul 15 02:24:10 2024 ] 	Batch(1000/6809) done. Loss: 0.0782  lr:0.010000
[ Mon Jul 15 02:24:33 2024 ] 	Batch(1100/6809) done. Loss: 0.4724  lr:0.010000
[ Mon Jul 15 02:24:56 2024 ] 	Batch(1200/6809) done. Loss: 1.4150  lr:0.010000
[ Mon Jul 15 02:25:19 2024 ] 	Batch(1300/6809) done. Loss: 0.4538  lr:0.010000
[ Mon Jul 15 02:25:42 2024 ] 	Batch(1400/6809) done. Loss: 0.5224  lr:0.010000
[ Mon Jul 15 02:26:05 2024 ] 
Training: Epoch [29/150], Step [1499], Loss: 0.2896997332572937, Training Accuracy: 81.75833333333333
[ Mon Jul 15 02:26:05 2024 ] 	Batch(1500/6809) done. Loss: 0.8715  lr:0.010000
[ Mon Jul 15 02:26:28 2024 ] 	Batch(1600/6809) done. Loss: 1.5434  lr:0.010000
[ Mon Jul 15 02:26:51 2024 ] 	Batch(1700/6809) done. Loss: 0.0739  lr:0.010000
[ Mon Jul 15 02:27:14 2024 ] 	Batch(1800/6809) done. Loss: 0.3470  lr:0.010000
[ Mon Jul 15 02:27:37 2024 ] 	Batch(1900/6809) done. Loss: 0.8258  lr:0.010000
[ Mon Jul 15 02:28:00 2024 ] 
Training: Epoch [29/150], Step [1999], Loss: 0.08820563554763794, Training Accuracy: 81.44375000000001
[ Mon Jul 15 02:28:00 2024 ] 	Batch(2000/6809) done. Loss: 0.9259  lr:0.010000
[ Mon Jul 15 02:28:23 2024 ] 	Batch(2100/6809) done. Loss: 0.7418  lr:0.010000
[ Mon Jul 15 02:28:46 2024 ] 	Batch(2200/6809) done. Loss: 0.8005  lr:0.010000
[ Mon Jul 15 02:29:09 2024 ] 	Batch(2300/6809) done. Loss: 0.4188  lr:0.010000
[ Mon Jul 15 02:29:32 2024 ] 	Batch(2400/6809) done. Loss: 0.2309  lr:0.010000
[ Mon Jul 15 02:29:55 2024 ] 
Training: Epoch [29/150], Step [2499], Loss: 0.6689040660858154, Training Accuracy: 81.35
[ Mon Jul 15 02:29:55 2024 ] 	Batch(2500/6809) done. Loss: 0.4739  lr:0.010000
[ Mon Jul 15 02:30:18 2024 ] 	Batch(2600/6809) done. Loss: 1.2474  lr:0.010000
[ Mon Jul 15 02:30:41 2024 ] 	Batch(2700/6809) done. Loss: 0.3180  lr:0.010000
[ Mon Jul 15 02:31:04 2024 ] 	Batch(2800/6809) done. Loss: 0.3055  lr:0.010000
[ Mon Jul 15 02:31:28 2024 ] 	Batch(2900/6809) done. Loss: 0.0262  lr:0.010000
[ Mon Jul 15 02:31:51 2024 ] 
Training: Epoch [29/150], Step [2999], Loss: 0.8076002597808838, Training Accuracy: 81.18333333333332
[ Mon Jul 15 02:31:51 2024 ] 	Batch(3000/6809) done. Loss: 0.1985  lr:0.010000
[ Mon Jul 15 02:32:14 2024 ] 	Batch(3100/6809) done. Loss: 0.2731  lr:0.010000
[ Mon Jul 15 02:32:38 2024 ] 	Batch(3200/6809) done. Loss: 1.0185  lr:0.010000
[ Mon Jul 15 02:33:00 2024 ] 	Batch(3300/6809) done. Loss: 0.3035  lr:0.010000
[ Mon Jul 15 02:33:23 2024 ] 	Batch(3400/6809) done. Loss: 1.0273  lr:0.010000
[ Mon Jul 15 02:33:46 2024 ] 
Training: Epoch [29/150], Step [3499], Loss: 0.4191831052303314, Training Accuracy: 81.10714285714286
[ Mon Jul 15 02:33:46 2024 ] 	Batch(3500/6809) done. Loss: 0.4054  lr:0.010000
[ Mon Jul 15 02:34:09 2024 ] 	Batch(3600/6809) done. Loss: 0.1758  lr:0.010000
[ Mon Jul 15 02:34:31 2024 ] 	Batch(3700/6809) done. Loss: 0.3389  lr:0.010000
[ Mon Jul 15 02:34:54 2024 ] 	Batch(3800/6809) done. Loss: 0.1627  lr:0.010000
[ Mon Jul 15 02:35:17 2024 ] 	Batch(3900/6809) done. Loss: 0.5006  lr:0.010000
[ Mon Jul 15 02:35:39 2024 ] 
Training: Epoch [29/150], Step [3999], Loss: 1.5473390817642212, Training Accuracy: 81.196875
[ Mon Jul 15 02:35:39 2024 ] 	Batch(4000/6809) done. Loss: 0.3992  lr:0.010000
[ Mon Jul 15 02:36:02 2024 ] 	Batch(4100/6809) done. Loss: 0.4433  lr:0.010000
[ Mon Jul 15 02:36:25 2024 ] 	Batch(4200/6809) done. Loss: 0.2958  lr:0.010000
[ Mon Jul 15 02:36:47 2024 ] 	Batch(4300/6809) done. Loss: 0.6421  lr:0.010000
[ Mon Jul 15 02:37:10 2024 ] 	Batch(4400/6809) done. Loss: 0.9900  lr:0.010000
[ Mon Jul 15 02:37:33 2024 ] 
Training: Epoch [29/150], Step [4499], Loss: 0.2592906951904297, Training Accuracy: 81.08611111111111
[ Mon Jul 15 02:37:33 2024 ] 	Batch(4500/6809) done. Loss: 0.1806  lr:0.010000
[ Mon Jul 15 02:37:56 2024 ] 	Batch(4600/6809) done. Loss: 0.6174  lr:0.010000
[ Mon Jul 15 02:38:18 2024 ] 	Batch(4700/6809) done. Loss: 0.0423  lr:0.010000
[ Mon Jul 15 02:38:41 2024 ] 	Batch(4800/6809) done. Loss: 0.1315  lr:0.010000
[ Mon Jul 15 02:39:04 2024 ] 	Batch(4900/6809) done. Loss: 1.0356  lr:0.010000
[ Mon Jul 15 02:39:26 2024 ] 
Training: Epoch [29/150], Step [4999], Loss: 0.3851531445980072, Training Accuracy: 81.045
[ Mon Jul 15 02:39:27 2024 ] 	Batch(5000/6809) done. Loss: 0.4845  lr:0.010000
[ Mon Jul 15 02:39:49 2024 ] 	Batch(5100/6809) done. Loss: 0.0647  lr:0.010000
[ Mon Jul 15 02:40:12 2024 ] 	Batch(5200/6809) done. Loss: 0.3604  lr:0.010000
[ Mon Jul 15 02:40:35 2024 ] 	Batch(5300/6809) done. Loss: 0.6495  lr:0.010000
[ Mon Jul 15 02:40:57 2024 ] 	Batch(5400/6809) done. Loss: 0.6192  lr:0.010000
[ Mon Jul 15 02:41:20 2024 ] 
Training: Epoch [29/150], Step [5499], Loss: 0.8494367599487305, Training Accuracy: 81.03181818181818
[ Mon Jul 15 02:41:20 2024 ] 	Batch(5500/6809) done. Loss: 0.7999  lr:0.010000
[ Mon Jul 15 02:41:43 2024 ] 	Batch(5600/6809) done. Loss: 0.2338  lr:0.010000
[ Mon Jul 15 02:42:06 2024 ] 	Batch(5700/6809) done. Loss: 0.6634  lr:0.010000
[ Mon Jul 15 02:42:29 2024 ] 	Batch(5800/6809) done. Loss: 0.7565  lr:0.010000
[ Mon Jul 15 02:42:51 2024 ] 	Batch(5900/6809) done. Loss: 0.2370  lr:0.010000
[ Mon Jul 15 02:43:14 2024 ] 
Training: Epoch [29/150], Step [5999], Loss: 1.7819489240646362, Training Accuracy: 81.03958333333333
[ Mon Jul 15 02:43:14 2024 ] 	Batch(6000/6809) done. Loss: 0.2676  lr:0.010000
[ Mon Jul 15 02:43:37 2024 ] 	Batch(6100/6809) done. Loss: 1.0167  lr:0.010000
[ Mon Jul 15 02:43:59 2024 ] 	Batch(6200/6809) done. Loss: 0.3630  lr:0.010000
[ Mon Jul 15 02:44:22 2024 ] 	Batch(6300/6809) done. Loss: 0.0348  lr:0.010000
[ Mon Jul 15 02:44:45 2024 ] 	Batch(6400/6809) done. Loss: 0.7229  lr:0.010000
[ Mon Jul 15 02:45:08 2024 ] 
Training: Epoch [29/150], Step [6499], Loss: 0.5063910484313965, Training Accuracy: 80.99807692307692
[ Mon Jul 15 02:45:08 2024 ] 	Batch(6500/6809) done. Loss: 0.1765  lr:0.010000
[ Mon Jul 15 02:45:32 2024 ] 	Batch(6600/6809) done. Loss: 0.3799  lr:0.010000
[ Mon Jul 15 02:45:55 2024 ] 	Batch(6700/6809) done. Loss: 0.3858  lr:0.010000
[ Mon Jul 15 02:46:18 2024 ] 	Batch(6800/6809) done. Loss: 0.3458  lr:0.010000
[ Mon Jul 15 02:46:20 2024 ] 	Mean training loss: 0.6035.
[ Mon Jul 15 02:46:20 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 02:46:20 2024 ] Eval epoch: 30
[ Mon Jul 15 02:53:17 2024 ] 	Mean val loss of 7435 batches: 0.9379615793935697.
[ Mon Jul 15 02:53:17 2024 ] 
Validation: Epoch [29/150], Samples [44576.0/59477], Loss: 1.516268014907837, Validation Accuracy: 74.94661802041125
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 1 : 366 / 500 = 73 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 2 : 415 / 499 = 83 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 3 : 434 / 500 = 86 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 4 : 413 / 502 = 82 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 5 : 358 / 502 = 71 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 6 : 427 / 502 = 85 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 7 : 467 / 497 = 93 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 8 : 468 / 498 = 93 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 9 : 304 / 500 = 60 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 10 : 271 / 500 = 54 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 11 : 82 / 498 = 16 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 12 : 383 / 499 = 76 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 13 : 487 / 502 = 97 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 14 : 444 / 504 = 88 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 15 : 346 / 502 = 68 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 16 : 385 / 502 = 76 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 17 : 412 / 504 = 81 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 18 : 459 / 504 = 91 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 19 : 451 / 502 = 89 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 20 : 408 / 502 = 81 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 21 : 448 / 503 = 89 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 22 : 393 / 504 = 77 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 23 : 378 / 503 = 75 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 24 : 415 / 504 = 82 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 25 : 461 / 504 = 91 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 26 : 441 / 504 = 87 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 27 : 411 / 501 = 82 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 28 : 369 / 502 = 73 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 29 : 371 / 502 = 73 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 30 : 355 / 501 = 70 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 31 : 434 / 504 = 86 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 32 : 373 / 503 = 74 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 33 : 350 / 503 = 69 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 34 : 488 / 504 = 96 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 35 : 470 / 503 = 93 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 36 : 363 / 502 = 72 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 37 : 460 / 504 = 91 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 38 : 428 / 504 = 84 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 39 : 425 / 498 = 85 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 40 : 343 / 504 = 68 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 41 : 472 / 503 = 93 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 42 : 408 / 504 = 80 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 43 : 301 / 503 = 59 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 44 : 408 / 504 = 80 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 45 : 425 / 504 = 84 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 46 : 375 / 504 = 74 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 47 : 328 / 503 = 65 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 48 : 376 / 503 = 74 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 49 : 358 / 499 = 71 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 50 : 319 / 502 = 63 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 51 : 456 / 503 = 90 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 52 : 470 / 504 = 93 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 53 : 354 / 497 = 71 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 54 : 411 / 480 = 85 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 55 : 373 / 504 = 74 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 56 : 402 / 503 = 79 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 57 : 469 / 504 = 93 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 58 : 469 / 499 = 93 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 59 : 465 / 503 = 92 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 60 : 406 / 479 = 84 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 61 : 392 / 484 = 80 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 62 : 379 / 487 = 77 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 63 : 425 / 489 = 86 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 64 : 341 / 488 = 69 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 65 : 409 / 490 = 83 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 66 : 347 / 488 = 71 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 67 : 352 / 490 = 71 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 68 : 304 / 490 = 62 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 69 : 370 / 490 = 75 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 70 : 243 / 490 = 49 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 71 : 177 / 490 = 36 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 72 : 198 / 488 = 40 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 73 : 227 / 486 = 46 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 74 : 245 / 481 = 50 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 75 : 193 / 488 = 39 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 76 : 258 / 489 = 52 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 77 : 250 / 488 = 51 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 78 : 314 / 488 = 64 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 79 : 436 / 490 = 88 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 80 : 367 / 489 = 75 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 81 : 215 / 491 = 43 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 82 : 341 / 491 = 69 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 83 : 165 / 489 = 33 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 84 : 350 / 489 = 71 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 85 : 299 / 489 = 61 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 86 : 422 / 491 = 85 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 87 : 393 / 492 = 79 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 88 : 394 / 491 = 80 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 89 : 279 / 492 = 56 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 90 : 186 / 490 = 37 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 91 : 260 / 482 = 53 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 92 : 377 / 490 = 76 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 93 : 340 / 487 = 69 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 94 : 417 / 489 = 85 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 95 : 394 / 490 = 80 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 96 : 456 / 491 = 92 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 97 : 448 / 490 = 91 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 98 : 435 / 491 = 88 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 99 : 446 / 491 = 90 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 100 : 395 / 491 = 80 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 101 : 407 / 491 = 82 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 102 : 260 / 492 = 52 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 103 : 337 / 492 = 68 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 104 : 175 / 491 = 35 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 105 : 333 / 491 = 67 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 106 : 199 / 492 = 40 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 107 : 408 / 491 = 83 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 108 : 336 / 492 = 68 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 109 : 370 / 490 = 75 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 110 : 393 / 491 = 80 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 111 : 433 / 492 = 88 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 112 : 460 / 492 = 93 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 113 : 427 / 491 = 86 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 114 : 374 / 491 = 76 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 115 : 396 / 492 = 80 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 116 : 386 / 491 = 78 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 117 : 431 / 492 = 87 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 118 : 381 / 490 = 77 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 119 : 439 / 492 = 89 %
[ Mon Jul 15 02:53:17 2024 ] Accuracy of 120 : 391 / 500 = 78 %
[ Mon Jul 15 02:53:17 2024 ] Training epoch: 31
[ Mon Jul 15 02:53:17 2024 ] 	Batch(0/6809) done. Loss: 0.2955  lr:0.010000
[ Mon Jul 15 02:53:40 2024 ] 	Batch(100/6809) done. Loss: 0.6919  lr:0.010000
[ Mon Jul 15 02:54:03 2024 ] 	Batch(200/6809) done. Loss: 0.8204  lr:0.010000
[ Mon Jul 15 02:54:25 2024 ] 	Batch(300/6809) done. Loss: 0.1905  lr:0.010000
[ Mon Jul 15 02:54:48 2024 ] 	Batch(400/6809) done. Loss: 0.2151  lr:0.010000
[ Mon Jul 15 02:55:11 2024 ] 
Training: Epoch [30/150], Step [499], Loss: 0.8478880524635315, Training Accuracy: 82.15
[ Mon Jul 15 02:55:11 2024 ] 	Batch(500/6809) done. Loss: 0.7229  lr:0.010000
[ Mon Jul 15 02:55:34 2024 ] 	Batch(600/6809) done. Loss: 0.1541  lr:0.010000
[ Mon Jul 15 02:55:56 2024 ] 	Batch(700/6809) done. Loss: 0.2728  lr:0.010000
[ Mon Jul 15 02:56:19 2024 ] 	Batch(800/6809) done. Loss: 0.1072  lr:0.010000
[ Mon Jul 15 02:56:42 2024 ] 	Batch(900/6809) done. Loss: 1.0140  lr:0.010000
[ Mon Jul 15 02:57:05 2024 ] 
Training: Epoch [30/150], Step [999], Loss: 0.4237763583660126, Training Accuracy: 82.4125
[ Mon Jul 15 02:57:05 2024 ] 	Batch(1000/6809) done. Loss: 0.9690  lr:0.010000
[ Mon Jul 15 02:57:28 2024 ] 	Batch(1100/6809) done. Loss: 0.2828  lr:0.010000
[ Mon Jul 15 02:57:51 2024 ] 	Batch(1200/6809) done. Loss: 0.2227  lr:0.010000
[ Mon Jul 15 02:58:14 2024 ] 	Batch(1300/6809) done. Loss: 0.8312  lr:0.010000
[ Mon Jul 15 02:58:36 2024 ] 	Batch(1400/6809) done. Loss: 0.6129  lr:0.010000
[ Mon Jul 15 02:58:59 2024 ] 
Training: Epoch [30/150], Step [1499], Loss: 0.31699275970458984, Training Accuracy: 82.08333333333333
[ Mon Jul 15 02:58:59 2024 ] 	Batch(1500/6809) done. Loss: 0.6606  lr:0.010000
[ Mon Jul 15 02:59:22 2024 ] 	Batch(1600/6809) done. Loss: 0.2389  lr:0.010000
[ Mon Jul 15 02:59:44 2024 ] 	Batch(1700/6809) done. Loss: 1.3361  lr:0.010000
[ Mon Jul 15 03:00:07 2024 ] 	Batch(1800/6809) done. Loss: 0.2157  lr:0.010000
[ Mon Jul 15 03:00:30 2024 ] 	Batch(1900/6809) done. Loss: 0.7573  lr:0.010000
[ Mon Jul 15 03:00:52 2024 ] 
Training: Epoch [30/150], Step [1999], Loss: 0.22935336828231812, Training Accuracy: 81.64375
[ Mon Jul 15 03:00:53 2024 ] 	Batch(2000/6809) done. Loss: 0.8000  lr:0.010000
[ Mon Jul 15 03:01:15 2024 ] 	Batch(2100/6809) done. Loss: 0.0964  lr:0.010000
[ Mon Jul 15 03:01:38 2024 ] 	Batch(2200/6809) done. Loss: 1.1751  lr:0.010000
[ Mon Jul 15 03:02:01 2024 ] 	Batch(2300/6809) done. Loss: 0.7064  lr:0.010000
[ Mon Jul 15 03:02:24 2024 ] 	Batch(2400/6809) done. Loss: 0.8511  lr:0.010000
[ Mon Jul 15 03:02:46 2024 ] 
Training: Epoch [30/150], Step [2499], Loss: 0.34610965847969055, Training Accuracy: 81.69999999999999
[ Mon Jul 15 03:02:46 2024 ] 	Batch(2500/6809) done. Loss: 0.3008  lr:0.010000
[ Mon Jul 15 03:03:09 2024 ] 	Batch(2600/6809) done. Loss: 0.1978  lr:0.010000
[ Mon Jul 15 03:03:32 2024 ] 	Batch(2700/6809) done. Loss: 1.0527  lr:0.010000
[ Mon Jul 15 03:03:54 2024 ] 	Batch(2800/6809) done. Loss: 1.0650  lr:0.010000
[ Mon Jul 15 03:04:17 2024 ] 	Batch(2900/6809) done. Loss: 0.3658  lr:0.010000
[ Mon Jul 15 03:04:39 2024 ] 
Training: Epoch [30/150], Step [2999], Loss: 0.2214188128709793, Training Accuracy: 81.60416666666667
[ Mon Jul 15 03:04:40 2024 ] 	Batch(3000/6809) done. Loss: 0.2602  lr:0.010000
[ Mon Jul 15 03:05:02 2024 ] 	Batch(3100/6809) done. Loss: 0.3528  lr:0.010000
[ Mon Jul 15 03:05:25 2024 ] 	Batch(3200/6809) done. Loss: 0.7013  lr:0.010000
[ Mon Jul 15 03:05:48 2024 ] 	Batch(3300/6809) done. Loss: 0.7235  lr:0.010000
[ Mon Jul 15 03:06:10 2024 ] 	Batch(3400/6809) done. Loss: 0.1124  lr:0.010000
[ Mon Jul 15 03:06:33 2024 ] 
Training: Epoch [30/150], Step [3499], Loss: 0.4661791920661926, Training Accuracy: 81.575
[ Mon Jul 15 03:06:33 2024 ] 	Batch(3500/6809) done. Loss: 0.8994  lr:0.010000
[ Mon Jul 15 03:06:56 2024 ] 	Batch(3600/6809) done. Loss: 1.0765  lr:0.010000
[ Mon Jul 15 03:07:19 2024 ] 	Batch(3700/6809) done. Loss: 0.6696  lr:0.010000
[ Mon Jul 15 03:07:41 2024 ] 	Batch(3800/6809) done. Loss: 0.2685  lr:0.010000
[ Mon Jul 15 03:08:04 2024 ] 	Batch(3900/6809) done. Loss: 0.3477  lr:0.010000
[ Mon Jul 15 03:08:26 2024 ] 
Training: Epoch [30/150], Step [3999], Loss: 0.49087461829185486, Training Accuracy: 81.646875
[ Mon Jul 15 03:08:27 2024 ] 	Batch(4000/6809) done. Loss: 0.1734  lr:0.010000
[ Mon Jul 15 03:08:49 2024 ] 	Batch(4100/6809) done. Loss: 0.8955  lr:0.010000
[ Mon Jul 15 03:09:12 2024 ] 	Batch(4200/6809) done. Loss: 0.3153  lr:0.010000
[ Mon Jul 15 03:09:35 2024 ] 	Batch(4300/6809) done. Loss: 0.4827  lr:0.010000
[ Mon Jul 15 03:09:57 2024 ] 	Batch(4400/6809) done. Loss: 0.7427  lr:0.010000
[ Mon Jul 15 03:10:20 2024 ] 
Training: Epoch [30/150], Step [4499], Loss: 0.578751802444458, Training Accuracy: 81.59444444444445
[ Mon Jul 15 03:10:20 2024 ] 	Batch(4500/6809) done. Loss: 0.2365  lr:0.010000
[ Mon Jul 15 03:10:43 2024 ] 	Batch(4600/6809) done. Loss: 1.0188  lr:0.010000
[ Mon Jul 15 03:11:05 2024 ] 	Batch(4700/6809) done. Loss: 0.4103  lr:0.010000
[ Mon Jul 15 03:11:28 2024 ] 	Batch(4800/6809) done. Loss: 0.1302  lr:0.010000
[ Mon Jul 15 03:11:51 2024 ] 	Batch(4900/6809) done. Loss: 1.0885  lr:0.010000
[ Mon Jul 15 03:12:13 2024 ] 
Training: Epoch [30/150], Step [4999], Loss: 0.21176670491695404, Training Accuracy: 81.6275
[ Mon Jul 15 03:12:13 2024 ] 	Batch(5000/6809) done. Loss: 0.0453  lr:0.010000
[ Mon Jul 15 03:12:36 2024 ] 	Batch(5100/6809) done. Loss: 0.0517  lr:0.010000
[ Mon Jul 15 03:12:59 2024 ] 	Batch(5200/6809) done. Loss: 0.3821  lr:0.010000
[ Mon Jul 15 03:13:21 2024 ] 	Batch(5300/6809) done. Loss: 0.4140  lr:0.010000
[ Mon Jul 15 03:13:44 2024 ] 	Batch(5400/6809) done. Loss: 0.1489  lr:0.010000
[ Mon Jul 15 03:14:06 2024 ] 
Training: Epoch [30/150], Step [5499], Loss: 0.9390082955360413, Training Accuracy: 81.53181818181818
[ Mon Jul 15 03:14:07 2024 ] 	Batch(5500/6809) done. Loss: 0.2796  lr:0.010000
[ Mon Jul 15 03:14:29 2024 ] 	Batch(5600/6809) done. Loss: 0.1307  lr:0.010000
[ Mon Jul 15 03:14:52 2024 ] 	Batch(5700/6809) done. Loss: 0.1104  lr:0.010000
[ Mon Jul 15 03:15:15 2024 ] 	Batch(5800/6809) done. Loss: 0.8477  lr:0.010000
[ Mon Jul 15 03:15:37 2024 ] 	Batch(5900/6809) done. Loss: 0.2473  lr:0.010000
[ Mon Jul 15 03:16:00 2024 ] 
Training: Epoch [30/150], Step [5999], Loss: 0.11174993216991425, Training Accuracy: 81.58958333333334
[ Mon Jul 15 03:16:00 2024 ] 	Batch(6000/6809) done. Loss: 0.1291  lr:0.010000
[ Mon Jul 15 03:16:23 2024 ] 	Batch(6100/6809) done. Loss: 0.7531  lr:0.010000
[ Mon Jul 15 03:16:45 2024 ] 	Batch(6200/6809) done. Loss: 0.9268  lr:0.010000
[ Mon Jul 15 03:17:08 2024 ] 	Batch(6300/6809) done. Loss: 0.4260  lr:0.010000
[ Mon Jul 15 03:17:31 2024 ] 	Batch(6400/6809) done. Loss: 0.6003  lr:0.010000
[ Mon Jul 15 03:17:53 2024 ] 
Training: Epoch [30/150], Step [6499], Loss: 0.3027580678462982, Training Accuracy: 81.54615384615384
[ Mon Jul 15 03:17:53 2024 ] 	Batch(6500/6809) done. Loss: 1.8191  lr:0.010000
[ Mon Jul 15 03:18:16 2024 ] 	Batch(6600/6809) done. Loss: 0.1413  lr:0.010000
[ Mon Jul 15 03:18:39 2024 ] 	Batch(6700/6809) done. Loss: 0.3547  lr:0.010000
[ Mon Jul 15 03:19:02 2024 ] 	Batch(6800/6809) done. Loss: 0.2610  lr:0.010000
[ Mon Jul 15 03:19:04 2024 ] 	Mean training loss: 0.5992.
[ Mon Jul 15 03:19:04 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 03:19:04 2024 ] Training epoch: 32
[ Mon Jul 15 03:19:04 2024 ] 	Batch(0/6809) done. Loss: 0.1828  lr:0.010000
[ Mon Jul 15 03:19:27 2024 ] 	Batch(100/6809) done. Loss: 0.5768  lr:0.010000
[ Mon Jul 15 03:19:50 2024 ] 	Batch(200/6809) done. Loss: 0.9824  lr:0.010000
[ Mon Jul 15 03:20:13 2024 ] 	Batch(300/6809) done. Loss: 0.4508  lr:0.010000
[ Mon Jul 15 03:20:36 2024 ] 	Batch(400/6809) done. Loss: 0.0611  lr:0.010000
[ Mon Jul 15 03:20:59 2024 ] 
Training: Epoch [31/150], Step [499], Loss: 0.6159189343452454, Training Accuracy: 82.85
[ Mon Jul 15 03:20:59 2024 ] 	Batch(500/6809) done. Loss: 1.1142  lr:0.010000
[ Mon Jul 15 03:21:22 2024 ] 	Batch(600/6809) done. Loss: 0.9071  lr:0.010000
[ Mon Jul 15 03:21:46 2024 ] 	Batch(700/6809) done. Loss: 0.3462  lr:0.010000
[ Mon Jul 15 03:22:09 2024 ] 	Batch(800/6809) done. Loss: 0.2845  lr:0.010000
[ Mon Jul 15 03:22:31 2024 ] 	Batch(900/6809) done. Loss: 0.1613  lr:0.010000
[ Mon Jul 15 03:22:54 2024 ] 
Training: Epoch [31/150], Step [999], Loss: 0.9961915016174316, Training Accuracy: 82.4875
[ Mon Jul 15 03:22:54 2024 ] 	Batch(1000/6809) done. Loss: 0.7601  lr:0.010000
[ Mon Jul 15 03:23:17 2024 ] 	Batch(1100/6809) done. Loss: 0.5945  lr:0.010000
[ Mon Jul 15 03:23:40 2024 ] 	Batch(1200/6809) done. Loss: 0.6686  lr:0.010000
[ Mon Jul 15 03:24:03 2024 ] 	Batch(1300/6809) done. Loss: 0.6772  lr:0.010000
[ Mon Jul 15 03:24:25 2024 ] 	Batch(1400/6809) done. Loss: 1.5486  lr:0.010000
[ Mon Jul 15 03:24:48 2024 ] 
Training: Epoch [31/150], Step [1499], Loss: 0.5759263038635254, Training Accuracy: 82.025
[ Mon Jul 15 03:24:48 2024 ] 	Batch(1500/6809) done. Loss: 0.8862  lr:0.010000
[ Mon Jul 15 03:25:11 2024 ] 	Batch(1600/6809) done. Loss: 0.3833  lr:0.010000
[ Mon Jul 15 03:25:33 2024 ] 	Batch(1700/6809) done. Loss: 0.7811  lr:0.010000
[ Mon Jul 15 03:25:57 2024 ] 	Batch(1800/6809) done. Loss: 1.0279  lr:0.010000
[ Mon Jul 15 03:26:19 2024 ] 	Batch(1900/6809) done. Loss: 0.2981  lr:0.010000
[ Mon Jul 15 03:26:42 2024 ] 
Training: Epoch [31/150], Step [1999], Loss: 0.3310104310512543, Training Accuracy: 81.75625
[ Mon Jul 15 03:26:42 2024 ] 	Batch(2000/6809) done. Loss: 0.3684  lr:0.010000
[ Mon Jul 15 03:27:05 2024 ] 	Batch(2100/6809) done. Loss: 0.3138  lr:0.010000
[ Mon Jul 15 03:27:27 2024 ] 	Batch(2200/6809) done. Loss: 0.5044  lr:0.010000
[ Mon Jul 15 03:27:50 2024 ] 	Batch(2300/6809) done. Loss: 0.2881  lr:0.010000
[ Mon Jul 15 03:28:13 2024 ] 	Batch(2400/6809) done. Loss: 0.7113  lr:0.010000
[ Mon Jul 15 03:28:36 2024 ] 
Training: Epoch [31/150], Step [2499], Loss: 1.2022666931152344, Training Accuracy: 81.88
[ Mon Jul 15 03:28:36 2024 ] 	Batch(2500/6809) done. Loss: 1.2864  lr:0.010000
[ Mon Jul 15 03:28:58 2024 ] 	Batch(2600/6809) done. Loss: 0.9791  lr:0.010000
[ Mon Jul 15 03:29:21 2024 ] 	Batch(2700/6809) done. Loss: 0.6483  lr:0.010000
[ Mon Jul 15 03:29:44 2024 ] 	Batch(2800/6809) done. Loss: 0.4277  lr:0.010000
[ Mon Jul 15 03:30:07 2024 ] 	Batch(2900/6809) done. Loss: 1.7354  lr:0.010000
[ Mon Jul 15 03:30:29 2024 ] 
Training: Epoch [31/150], Step [2999], Loss: 0.8317192792892456, Training Accuracy: 81.79166666666666
[ Mon Jul 15 03:30:29 2024 ] 	Batch(3000/6809) done. Loss: 0.2582  lr:0.010000
[ Mon Jul 15 03:30:52 2024 ] 	Batch(3100/6809) done. Loss: 0.1510  lr:0.010000
[ Mon Jul 15 03:31:15 2024 ] 	Batch(3200/6809) done. Loss: 1.5401  lr:0.010000
[ Mon Jul 15 03:31:38 2024 ] 	Batch(3300/6809) done. Loss: 0.8071  lr:0.010000
[ Mon Jul 15 03:32:01 2024 ] 	Batch(3400/6809) done. Loss: 0.0823  lr:0.010000
[ Mon Jul 15 03:32:23 2024 ] 
Training: Epoch [31/150], Step [3499], Loss: 0.41183042526245117, Training Accuracy: 81.65714285714286
[ Mon Jul 15 03:32:23 2024 ] 	Batch(3500/6809) done. Loss: 0.1589  lr:0.010000
[ Mon Jul 15 03:32:46 2024 ] 	Batch(3600/6809) done. Loss: 0.4669  lr:0.010000
[ Mon Jul 15 03:33:09 2024 ] 	Batch(3700/6809) done. Loss: 0.5332  lr:0.010000
[ Mon Jul 15 03:33:32 2024 ] 	Batch(3800/6809) done. Loss: 0.5319  lr:0.010000
[ Mon Jul 15 03:33:54 2024 ] 	Batch(3900/6809) done. Loss: 1.1990  lr:0.010000
[ Mon Jul 15 03:34:17 2024 ] 
Training: Epoch [31/150], Step [3999], Loss: 0.7060127258300781, Training Accuracy: 81.5375
[ Mon Jul 15 03:34:17 2024 ] 	Batch(4000/6809) done. Loss: 0.4601  lr:0.010000
[ Mon Jul 15 03:34:40 2024 ] 	Batch(4100/6809) done. Loss: 0.5331  lr:0.010000
[ Mon Jul 15 03:35:03 2024 ] 	Batch(4200/6809) done. Loss: 0.8758  lr:0.010000
[ Mon Jul 15 03:35:26 2024 ] 	Batch(4300/6809) done. Loss: 0.0547  lr:0.010000
[ Mon Jul 15 03:35:48 2024 ] 	Batch(4400/6809) done. Loss: 0.8313  lr:0.010000
[ Mon Jul 15 03:36:11 2024 ] 
Training: Epoch [31/150], Step [4499], Loss: 0.46993130445480347, Training Accuracy: 81.59444444444445
[ Mon Jul 15 03:36:11 2024 ] 	Batch(4500/6809) done. Loss: 0.2256  lr:0.010000
[ Mon Jul 15 03:36:35 2024 ] 	Batch(4600/6809) done. Loss: 0.6740  lr:0.010000
[ Mon Jul 15 03:36:58 2024 ] 	Batch(4700/6809) done. Loss: 1.6337  lr:0.010000
[ Mon Jul 15 03:37:21 2024 ] 	Batch(4800/6809) done. Loss: 0.3163  lr:0.010000
[ Mon Jul 15 03:37:44 2024 ] 	Batch(4900/6809) done. Loss: 0.4790  lr:0.010000
[ Mon Jul 15 03:38:07 2024 ] 
Training: Epoch [31/150], Step [4999], Loss: 0.7522954940795898, Training Accuracy: 81.54249999999999
[ Mon Jul 15 03:38:07 2024 ] 	Batch(5000/6809) done. Loss: 0.4553  lr:0.010000
[ Mon Jul 15 03:38:30 2024 ] 	Batch(5100/6809) done. Loss: 0.6878  lr:0.010000
[ Mon Jul 15 03:38:53 2024 ] 	Batch(5200/6809) done. Loss: 0.1485  lr:0.010000
[ Mon Jul 15 03:39:17 2024 ] 	Batch(5300/6809) done. Loss: 0.1658  lr:0.010000
[ Mon Jul 15 03:39:40 2024 ] 	Batch(5400/6809) done. Loss: 0.2926  lr:0.010000
[ Mon Jul 15 03:40:03 2024 ] 
Training: Epoch [31/150], Step [5499], Loss: 0.5617450475692749, Training Accuracy: 81.62727272727273
[ Mon Jul 15 03:40:03 2024 ] 	Batch(5500/6809) done. Loss: 0.9363  lr:0.010000
[ Mon Jul 15 03:40:27 2024 ] 	Batch(5600/6809) done. Loss: 0.3290  lr:0.010000
[ Mon Jul 15 03:40:50 2024 ] 	Batch(5700/6809) done. Loss: 0.3538  lr:0.010000
[ Mon Jul 15 03:41:13 2024 ] 	Batch(5800/6809) done. Loss: 0.1235  lr:0.010000
[ Mon Jul 15 03:41:36 2024 ] 	Batch(5900/6809) done. Loss: 0.4684  lr:0.010000
[ Mon Jul 15 03:41:58 2024 ] 
Training: Epoch [31/150], Step [5999], Loss: 1.203596830368042, Training Accuracy: 81.63333333333334
[ Mon Jul 15 03:41:59 2024 ] 	Batch(6000/6809) done. Loss: 0.3808  lr:0.010000
[ Mon Jul 15 03:42:22 2024 ] 	Batch(6100/6809) done. Loss: 0.1467  lr:0.010000
[ Mon Jul 15 03:42:45 2024 ] 	Batch(6200/6809) done. Loss: 0.6158  lr:0.010000
[ Mon Jul 15 03:43:08 2024 ] 	Batch(6300/6809) done. Loss: 0.2092  lr:0.010000
[ Mon Jul 15 03:43:31 2024 ] 	Batch(6400/6809) done. Loss: 0.6191  lr:0.010000
[ Mon Jul 15 03:43:54 2024 ] 
Training: Epoch [31/150], Step [6499], Loss: 0.48785796761512756, Training Accuracy: 81.64423076923077
[ Mon Jul 15 03:43:54 2024 ] 	Batch(6500/6809) done. Loss: 1.1116  lr:0.010000
[ Mon Jul 15 03:44:17 2024 ] 	Batch(6600/6809) done. Loss: 1.2329  lr:0.010000
[ Mon Jul 15 03:44:40 2024 ] 	Batch(6700/6809) done. Loss: 0.0448  lr:0.010000
[ Mon Jul 15 03:45:03 2024 ] 	Batch(6800/6809) done. Loss: 0.3787  lr:0.010000
[ Mon Jul 15 03:45:05 2024 ] 	Mean training loss: 0.5840.
[ Mon Jul 15 03:45:05 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 03:45:05 2024 ] Training epoch: 33
[ Mon Jul 15 03:45:06 2024 ] 	Batch(0/6809) done. Loss: 0.6179  lr:0.010000
[ Mon Jul 15 03:45:29 2024 ] 	Batch(100/6809) done. Loss: 0.4652  lr:0.010000
[ Mon Jul 15 03:45:52 2024 ] 	Batch(200/6809) done. Loss: 0.4413  lr:0.010000
[ Mon Jul 15 03:46:15 2024 ] 	Batch(300/6809) done. Loss: 0.3334  lr:0.010000
[ Mon Jul 15 03:46:38 2024 ] 	Batch(400/6809) done. Loss: 1.0608  lr:0.010000
[ Mon Jul 15 03:47:00 2024 ] 
Training: Epoch [32/150], Step [499], Loss: 0.33001601696014404, Training Accuracy: 82.92500000000001
[ Mon Jul 15 03:47:01 2024 ] 	Batch(500/6809) done. Loss: 0.4160  lr:0.010000
[ Mon Jul 15 03:47:24 2024 ] 	Batch(600/6809) done. Loss: 0.1626  lr:0.010000
[ Mon Jul 15 03:47:47 2024 ] 	Batch(700/6809) done. Loss: 0.5224  lr:0.010000
[ Mon Jul 15 03:48:10 2024 ] 	Batch(800/6809) done. Loss: 1.2022  lr:0.010000
[ Mon Jul 15 03:48:33 2024 ] 	Batch(900/6809) done. Loss: 0.3135  lr:0.010000
[ Mon Jul 15 03:48:55 2024 ] 
Training: Epoch [32/150], Step [999], Loss: 0.8208860754966736, Training Accuracy: 82.9375
[ Mon Jul 15 03:48:55 2024 ] 	Batch(1000/6809) done. Loss: 1.2739  lr:0.010000
[ Mon Jul 15 03:49:18 2024 ] 	Batch(1100/6809) done. Loss: 0.7861  lr:0.010000
[ Mon Jul 15 03:49:42 2024 ] 	Batch(1200/6809) done. Loss: 2.0937  lr:0.010000
[ Mon Jul 15 03:50:05 2024 ] 	Batch(1300/6809) done. Loss: 0.7273  lr:0.010000
[ Mon Jul 15 03:50:29 2024 ] 	Batch(1400/6809) done. Loss: 0.1492  lr:0.010000
[ Mon Jul 15 03:50:51 2024 ] 
Training: Epoch [32/150], Step [1499], Loss: 0.5235689878463745, Training Accuracy: 82.94166666666666
[ Mon Jul 15 03:50:52 2024 ] 	Batch(1500/6809) done. Loss: 0.5425  lr:0.010000
[ Mon Jul 15 03:51:15 2024 ] 	Batch(1600/6809) done. Loss: 0.4318  lr:0.010000
[ Mon Jul 15 03:51:38 2024 ] 	Batch(1700/6809) done. Loss: 1.1314  lr:0.010000
[ Mon Jul 15 03:52:01 2024 ] 	Batch(1800/6809) done. Loss: 0.3860  lr:0.010000
[ Mon Jul 15 03:52:24 2024 ] 	Batch(1900/6809) done. Loss: 0.7414  lr:0.010000
[ Mon Jul 15 03:52:47 2024 ] 
Training: Epoch [32/150], Step [1999], Loss: 0.16490831971168518, Training Accuracy: 82.75625
[ Mon Jul 15 03:52:48 2024 ] 	Batch(2000/6809) done. Loss: 0.5197  lr:0.010000
[ Mon Jul 15 03:53:11 2024 ] 	Batch(2100/6809) done. Loss: 0.4673  lr:0.010000
[ Mon Jul 15 03:53:33 2024 ] 	Batch(2200/6809) done. Loss: 0.5175  lr:0.010000
[ Mon Jul 15 03:53:56 2024 ] 	Batch(2300/6809) done. Loss: 0.4541  lr:0.010000
[ Mon Jul 15 03:54:19 2024 ] 	Batch(2400/6809) done. Loss: 0.7618  lr:0.010000
[ Mon Jul 15 03:54:42 2024 ] 
Training: Epoch [32/150], Step [2499], Loss: 0.16202712059020996, Training Accuracy: 82.675
[ Mon Jul 15 03:54:42 2024 ] 	Batch(2500/6809) done. Loss: 0.2392  lr:0.010000
[ Mon Jul 15 03:55:06 2024 ] 	Batch(2600/6809) done. Loss: 0.7047  lr:0.010000
[ Mon Jul 15 03:55:29 2024 ] 	Batch(2700/6809) done. Loss: 0.3460  lr:0.010000
[ Mon Jul 15 03:55:53 2024 ] 	Batch(2800/6809) done. Loss: 1.2004  lr:0.010000
[ Mon Jul 15 03:56:15 2024 ] 	Batch(2900/6809) done. Loss: 0.1761  lr:0.010000
[ Mon Jul 15 03:56:38 2024 ] 
Training: Epoch [32/150], Step [2999], Loss: 0.7264123558998108, Training Accuracy: 82.52083333333333
[ Mon Jul 15 03:56:38 2024 ] 	Batch(3000/6809) done. Loss: 0.3739  lr:0.010000
[ Mon Jul 15 03:57:01 2024 ] 	Batch(3100/6809) done. Loss: 0.6502  lr:0.010000
[ Mon Jul 15 03:57:24 2024 ] 	Batch(3200/6809) done. Loss: 0.0761  lr:0.010000
[ Mon Jul 15 03:57:47 2024 ] 	Batch(3300/6809) done. Loss: 0.0643  lr:0.010000
[ Mon Jul 15 03:58:09 2024 ] 	Batch(3400/6809) done. Loss: 2.0879  lr:0.010000
[ Mon Jul 15 03:58:32 2024 ] 
Training: Epoch [32/150], Step [3499], Loss: 0.9400600790977478, Training Accuracy: 82.45357142857142
[ Mon Jul 15 03:58:32 2024 ] 	Batch(3500/6809) done. Loss: 0.6069  lr:0.010000
[ Mon Jul 15 03:58:55 2024 ] 	Batch(3600/6809) done. Loss: 1.3196  lr:0.010000
[ Mon Jul 15 03:59:18 2024 ] 	Batch(3700/6809) done. Loss: 0.8384  lr:0.010000
[ Mon Jul 15 03:59:40 2024 ] 	Batch(3800/6809) done. Loss: 0.2519  lr:0.010000
[ Mon Jul 15 04:00:03 2024 ] 	Batch(3900/6809) done. Loss: 0.7511  lr:0.010000
[ Mon Jul 15 04:00:25 2024 ] 
Training: Epoch [32/150], Step [3999], Loss: 0.7358344793319702, Training Accuracy: 82.5125
[ Mon Jul 15 04:00:26 2024 ] 	Batch(4000/6809) done. Loss: 0.5715  lr:0.010000
[ Mon Jul 15 04:00:49 2024 ] 	Batch(4100/6809) done. Loss: 0.2243  lr:0.010000
[ Mon Jul 15 04:01:11 2024 ] 	Batch(4200/6809) done. Loss: 0.5335  lr:0.010000
[ Mon Jul 15 04:01:34 2024 ] 	Batch(4300/6809) done. Loss: 0.5747  lr:0.010000
[ Mon Jul 15 04:01:57 2024 ] 	Batch(4400/6809) done. Loss: 0.2587  lr:0.010000
[ Mon Jul 15 04:02:20 2024 ] 
Training: Epoch [32/150], Step [4499], Loss: 0.3375793397426605, Training Accuracy: 82.36666666666666
[ Mon Jul 15 04:02:20 2024 ] 	Batch(4500/6809) done. Loss: 0.3227  lr:0.010000
[ Mon Jul 15 04:02:43 2024 ] 	Batch(4600/6809) done. Loss: 0.3548  lr:0.010000
[ Mon Jul 15 04:03:05 2024 ] 	Batch(4700/6809) done. Loss: 1.2843  lr:0.010000
[ Mon Jul 15 04:03:28 2024 ] 	Batch(4800/6809) done. Loss: 0.0060  lr:0.010000
[ Mon Jul 15 04:03:51 2024 ] 	Batch(4900/6809) done. Loss: 1.0332  lr:0.010000
[ Mon Jul 15 04:04:13 2024 ] 
Training: Epoch [32/150], Step [4999], Loss: 1.3165855407714844, Training Accuracy: 82.3125
[ Mon Jul 15 04:04:14 2024 ] 	Batch(5000/6809) done. Loss: 0.6833  lr:0.010000
[ Mon Jul 15 04:04:36 2024 ] 	Batch(5100/6809) done. Loss: 0.6290  lr:0.010000
[ Mon Jul 15 04:04:59 2024 ] 	Batch(5200/6809) done. Loss: 0.5104  lr:0.010000
[ Mon Jul 15 04:05:22 2024 ] 	Batch(5300/6809) done. Loss: 0.2587  lr:0.010000
[ Mon Jul 15 04:05:45 2024 ] 	Batch(5400/6809) done. Loss: 1.6180  lr:0.010000
[ Mon Jul 15 04:06:07 2024 ] 
Training: Epoch [32/150], Step [5499], Loss: 0.5126060247421265, Training Accuracy: 82.28636363636363
[ Mon Jul 15 04:06:07 2024 ] 	Batch(5500/6809) done. Loss: 0.4412  lr:0.010000
[ Mon Jul 15 04:06:30 2024 ] 	Batch(5600/6809) done. Loss: 1.0563  lr:0.010000
[ Mon Jul 15 04:06:54 2024 ] 	Batch(5700/6809) done. Loss: 1.3156  lr:0.010000
[ Mon Jul 15 04:07:17 2024 ] 	Batch(5800/6809) done. Loss: 0.5689  lr:0.010000
[ Mon Jul 15 04:07:40 2024 ] 	Batch(5900/6809) done. Loss: 0.1954  lr:0.010000
[ Mon Jul 15 04:08:03 2024 ] 
Training: Epoch [32/150], Step [5999], Loss: 0.750869870185852, Training Accuracy: 82.21875
[ Mon Jul 15 04:08:03 2024 ] 	Batch(6000/6809) done. Loss: 0.5646  lr:0.010000
[ Mon Jul 15 04:08:26 2024 ] 	Batch(6100/6809) done. Loss: 0.7848  lr:0.010000
[ Mon Jul 15 04:08:49 2024 ] 	Batch(6200/6809) done. Loss: 0.3136  lr:0.010000
[ Mon Jul 15 04:09:12 2024 ] 	Batch(6300/6809) done. Loss: 0.5401  lr:0.010000
[ Mon Jul 15 04:09:35 2024 ] 	Batch(6400/6809) done. Loss: 0.1801  lr:0.010000
[ Mon Jul 15 04:09:57 2024 ] 
Training: Epoch [32/150], Step [6499], Loss: 0.0872425064444542, Training Accuracy: 82.2173076923077
[ Mon Jul 15 04:09:57 2024 ] 	Batch(6500/6809) done. Loss: 0.9273  lr:0.010000
[ Mon Jul 15 04:10:20 2024 ] 	Batch(6600/6809) done. Loss: 0.9037  lr:0.010000
[ Mon Jul 15 04:10:43 2024 ] 	Batch(6700/6809) done. Loss: 0.8666  lr:0.010000
[ Mon Jul 15 04:11:06 2024 ] 	Batch(6800/6809) done. Loss: 1.1201  lr:0.010000
[ Mon Jul 15 04:11:07 2024 ] 	Mean training loss: 0.5724.
[ Mon Jul 15 04:11:07 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 04:11:08 2024 ] Training epoch: 34
[ Mon Jul 15 04:11:08 2024 ] 	Batch(0/6809) done. Loss: 0.0753  lr:0.010000
[ Mon Jul 15 04:11:31 2024 ] 	Batch(100/6809) done. Loss: 1.5728  lr:0.010000
[ Mon Jul 15 04:11:54 2024 ] 	Batch(200/6809) done. Loss: 0.2663  lr:0.010000
[ Mon Jul 15 04:12:16 2024 ] 	Batch(300/6809) done. Loss: 0.1651  lr:0.010000
[ Mon Jul 15 04:12:39 2024 ] 	Batch(400/6809) done. Loss: 0.5486  lr:0.010000
[ Mon Jul 15 04:13:02 2024 ] 
Training: Epoch [33/150], Step [499], Loss: 0.31620702147483826, Training Accuracy: 83.22500000000001
[ Mon Jul 15 04:13:02 2024 ] 	Batch(500/6809) done. Loss: 0.6963  lr:0.010000
[ Mon Jul 15 04:13:26 2024 ] 	Batch(600/6809) done. Loss: 1.0150  lr:0.010000
[ Mon Jul 15 04:13:49 2024 ] 	Batch(700/6809) done. Loss: 1.6192  lr:0.010000
[ Mon Jul 15 04:14:13 2024 ] 	Batch(800/6809) done. Loss: 1.0098  lr:0.010000
[ Mon Jul 15 04:14:36 2024 ] 	Batch(900/6809) done. Loss: 0.4657  lr:0.010000
[ Mon Jul 15 04:14:59 2024 ] 
Training: Epoch [33/150], Step [999], Loss: 0.13354843854904175, Training Accuracy: 83.1875
[ Mon Jul 15 04:15:00 2024 ] 	Batch(1000/6809) done. Loss: 0.0531  lr:0.010000
[ Mon Jul 15 04:15:23 2024 ] 	Batch(1100/6809) done. Loss: 0.4417  lr:0.010000
[ Mon Jul 15 04:15:47 2024 ] 	Batch(1200/6809) done. Loss: 0.8279  lr:0.010000
[ Mon Jul 15 04:16:10 2024 ] 	Batch(1300/6809) done. Loss: 0.0869  lr:0.010000
[ Mon Jul 15 04:16:33 2024 ] 	Batch(1400/6809) done. Loss: 0.8257  lr:0.010000
[ Mon Jul 15 04:16:55 2024 ] 
Training: Epoch [33/150], Step [1499], Loss: 0.8097904920578003, Training Accuracy: 83.125
[ Mon Jul 15 04:16:55 2024 ] 	Batch(1500/6809) done. Loss: 0.2536  lr:0.010000
[ Mon Jul 15 04:17:18 2024 ] 	Batch(1600/6809) done. Loss: 0.4981  lr:0.010000
[ Mon Jul 15 04:17:41 2024 ] 	Batch(1700/6809) done. Loss: 0.8142  lr:0.010000
[ Mon Jul 15 04:18:04 2024 ] 	Batch(1800/6809) done. Loss: 0.6164  lr:0.010000
[ Mon Jul 15 04:18:26 2024 ] 	Batch(1900/6809) done. Loss: 0.6941  lr:0.010000
[ Mon Jul 15 04:18:49 2024 ] 
Training: Epoch [33/150], Step [1999], Loss: 0.5129517316818237, Training Accuracy: 83.26875
[ Mon Jul 15 04:18:49 2024 ] 	Batch(2000/6809) done. Loss: 0.5131  lr:0.010000
[ Mon Jul 15 04:19:12 2024 ] 	Batch(2100/6809) done. Loss: 0.4675  lr:0.010000
[ Mon Jul 15 04:19:34 2024 ] 	Batch(2200/6809) done. Loss: 0.2012  lr:0.010000
[ Mon Jul 15 04:19:57 2024 ] 	Batch(2300/6809) done. Loss: 0.2568  lr:0.010000
[ Mon Jul 15 04:20:20 2024 ] 	Batch(2400/6809) done. Loss: 0.4711  lr:0.010000
[ Mon Jul 15 04:20:42 2024 ] 
Training: Epoch [33/150], Step [2499], Loss: 0.7180851101875305, Training Accuracy: 83.21
[ Mon Jul 15 04:20:43 2024 ] 	Batch(2500/6809) done. Loss: 0.3039  lr:0.010000
[ Mon Jul 15 04:21:05 2024 ] 	Batch(2600/6809) done. Loss: 0.1896  lr:0.010000
[ Mon Jul 15 04:21:28 2024 ] 	Batch(2700/6809) done. Loss: 0.8165  lr:0.010000
[ Mon Jul 15 04:21:51 2024 ] 	Batch(2800/6809) done. Loss: 0.6040  lr:0.010000
[ Mon Jul 15 04:22:14 2024 ] 	Batch(2900/6809) done. Loss: 0.7284  lr:0.010000
[ Mon Jul 15 04:22:36 2024 ] 
Training: Epoch [33/150], Step [2999], Loss: 0.981168270111084, Training Accuracy: 82.96249999999999
[ Mon Jul 15 04:22:36 2024 ] 	Batch(3000/6809) done. Loss: 0.2048  lr:0.010000
[ Mon Jul 15 04:22:59 2024 ] 	Batch(3100/6809) done. Loss: 0.4190  lr:0.010000
[ Mon Jul 15 04:23:22 2024 ] 	Batch(3200/6809) done. Loss: 0.9965  lr:0.010000
[ Mon Jul 15 04:23:44 2024 ] 	Batch(3300/6809) done. Loss: 0.9110  lr:0.010000
[ Mon Jul 15 04:24:07 2024 ] 	Batch(3400/6809) done. Loss: 0.5791  lr:0.010000
[ Mon Jul 15 04:24:30 2024 ] 
Training: Epoch [33/150], Step [3499], Loss: 0.12263656407594681, Training Accuracy: 82.78571428571428
[ Mon Jul 15 04:24:30 2024 ] 	Batch(3500/6809) done. Loss: 0.9031  lr:0.010000
[ Mon Jul 15 04:24:53 2024 ] 	Batch(3600/6809) done. Loss: 0.5770  lr:0.010000
[ Mon Jul 15 04:25:17 2024 ] 	Batch(3700/6809) done. Loss: 0.3834  lr:0.010000
[ Mon Jul 15 04:25:40 2024 ] 	Batch(3800/6809) done. Loss: 0.4880  lr:0.010000
[ Mon Jul 15 04:26:03 2024 ] 	Batch(3900/6809) done. Loss: 0.2522  lr:0.010000
[ Mon Jul 15 04:26:25 2024 ] 
Training: Epoch [33/150], Step [3999], Loss: 0.11833672970533371, Training Accuracy: 82.746875
[ Mon Jul 15 04:26:25 2024 ] 	Batch(4000/6809) done. Loss: 0.3709  lr:0.010000
[ Mon Jul 15 04:26:48 2024 ] 	Batch(4100/6809) done. Loss: 0.0789  lr:0.010000
[ Mon Jul 15 04:27:11 2024 ] 	Batch(4200/6809) done. Loss: 1.7880  lr:0.010000
[ Mon Jul 15 04:27:33 2024 ] 	Batch(4300/6809) done. Loss: 0.0466  lr:0.010000
[ Mon Jul 15 04:27:56 2024 ] 	Batch(4400/6809) done. Loss: 0.4412  lr:0.010000
[ Mon Jul 15 04:28:19 2024 ] 
Training: Epoch [33/150], Step [4499], Loss: 0.6715067625045776, Training Accuracy: 82.57222222222222
[ Mon Jul 15 04:28:19 2024 ] 	Batch(4500/6809) done. Loss: 0.6051  lr:0.010000
[ Mon Jul 15 04:28:42 2024 ] 	Batch(4600/6809) done. Loss: 0.2941  lr:0.010000
[ Mon Jul 15 04:29:05 2024 ] 	Batch(4700/6809) done. Loss: 0.2448  lr:0.010000
[ Mon Jul 15 04:29:27 2024 ] 	Batch(4800/6809) done. Loss: 0.4227  lr:0.010000
[ Mon Jul 15 04:29:50 2024 ] 	Batch(4900/6809) done. Loss: 0.8674  lr:0.010000
[ Mon Jul 15 04:30:13 2024 ] 
Training: Epoch [33/150], Step [4999], Loss: 0.8417365550994873, Training Accuracy: 82.54249999999999
[ Mon Jul 15 04:30:13 2024 ] 	Batch(5000/6809) done. Loss: 0.7689  lr:0.010000
[ Mon Jul 15 04:30:35 2024 ] 	Batch(5100/6809) done. Loss: 0.4978  lr:0.010000
[ Mon Jul 15 04:30:58 2024 ] 	Batch(5200/6809) done. Loss: 0.3185  lr:0.010000
[ Mon Jul 15 04:31:21 2024 ] 	Batch(5300/6809) done. Loss: 0.2677  lr:0.010000
[ Mon Jul 15 04:31:44 2024 ] 	Batch(5400/6809) done. Loss: 0.5605  lr:0.010000
[ Mon Jul 15 04:32:06 2024 ] 
Training: Epoch [33/150], Step [5499], Loss: 0.6805323362350464, Training Accuracy: 82.52727272727273
[ Mon Jul 15 04:32:07 2024 ] 	Batch(5500/6809) done. Loss: 0.6079  lr:0.010000
[ Mon Jul 15 04:32:29 2024 ] 	Batch(5600/6809) done. Loss: 0.0242  lr:0.010000
[ Mon Jul 15 04:32:53 2024 ] 	Batch(5700/6809) done. Loss: 0.4487  lr:0.010000
[ Mon Jul 15 04:33:16 2024 ] 	Batch(5800/6809) done. Loss: 1.1973  lr:0.010000
[ Mon Jul 15 04:33:39 2024 ] 	Batch(5900/6809) done. Loss: 0.5074  lr:0.010000
[ Mon Jul 15 04:34:02 2024 ] 
Training: Epoch [33/150], Step [5999], Loss: 0.2832064926624298, Training Accuracy: 82.5
[ Mon Jul 15 04:34:02 2024 ] 	Batch(6000/6809) done. Loss: 0.6755  lr:0.010000
[ Mon Jul 15 04:34:25 2024 ] 	Batch(6100/6809) done. Loss: 0.9410  lr:0.010000
[ Mon Jul 15 04:34:47 2024 ] 	Batch(6200/6809) done. Loss: 0.7707  lr:0.010000
[ Mon Jul 15 04:35:10 2024 ] 	Batch(6300/6809) done. Loss: 0.2248  lr:0.010000
[ Mon Jul 15 04:35:33 2024 ] 	Batch(6400/6809) done. Loss: 0.1017  lr:0.010000
[ Mon Jul 15 04:35:55 2024 ] 
Training: Epoch [33/150], Step [6499], Loss: 0.7830576300621033, Training Accuracy: 82.43846153846154
[ Mon Jul 15 04:35:55 2024 ] 	Batch(6500/6809) done. Loss: 0.1021  lr:0.010000
[ Mon Jul 15 04:36:18 2024 ] 	Batch(6600/6809) done. Loss: 0.4875  lr:0.010000
[ Mon Jul 15 04:36:41 2024 ] 	Batch(6700/6809) done. Loss: 0.5879  lr:0.010000
[ Mon Jul 15 04:37:04 2024 ] 	Batch(6800/6809) done. Loss: 0.1841  lr:0.010000
[ Mon Jul 15 04:37:06 2024 ] 	Mean training loss: 0.5709.
[ Mon Jul 15 04:37:06 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 04:37:06 2024 ] Training epoch: 35
[ Mon Jul 15 04:37:06 2024 ] 	Batch(0/6809) done. Loss: 0.2619  lr:0.010000
[ Mon Jul 15 04:37:29 2024 ] 	Batch(100/6809) done. Loss: 0.1054  lr:0.010000
[ Mon Jul 15 04:37:52 2024 ] 	Batch(200/6809) done. Loss: 0.3779  lr:0.010000
[ Mon Jul 15 04:38:14 2024 ] 	Batch(300/6809) done. Loss: 0.1195  lr:0.010000
[ Mon Jul 15 04:38:37 2024 ] 	Batch(400/6809) done. Loss: 0.5485  lr:0.010000
[ Mon Jul 15 04:39:00 2024 ] 
Training: Epoch [34/150], Step [499], Loss: 0.22282901406288147, Training Accuracy: 83.825
[ Mon Jul 15 04:39:00 2024 ] 	Batch(500/6809) done. Loss: 0.4565  lr:0.010000
[ Mon Jul 15 04:39:23 2024 ] 	Batch(600/6809) done. Loss: 0.8400  lr:0.010000
[ Mon Jul 15 04:39:45 2024 ] 	Batch(700/6809) done. Loss: 0.5327  lr:0.010000
[ Mon Jul 15 04:40:08 2024 ] 	Batch(800/6809) done. Loss: 0.8333  lr:0.010000
[ Mon Jul 15 04:40:31 2024 ] 	Batch(900/6809) done. Loss: 0.3753  lr:0.010000
[ Mon Jul 15 04:40:53 2024 ] 
Training: Epoch [34/150], Step [999], Loss: 0.28199249505996704, Training Accuracy: 83.28750000000001
[ Mon Jul 15 04:40:53 2024 ] 	Batch(1000/6809) done. Loss: 0.3965  lr:0.010000
[ Mon Jul 15 04:41:16 2024 ] 	Batch(1100/6809) done. Loss: 1.1486  lr:0.010000
[ Mon Jul 15 04:41:39 2024 ] 	Batch(1200/6809) done. Loss: 0.3280  lr:0.010000
[ Mon Jul 15 04:42:02 2024 ] 	Batch(1300/6809) done. Loss: 1.0753  lr:0.010000
[ Mon Jul 15 04:42:24 2024 ] 	Batch(1400/6809) done. Loss: 0.2413  lr:0.010000
[ Mon Jul 15 04:42:47 2024 ] 
Training: Epoch [34/150], Step [1499], Loss: 0.498004674911499, Training Accuracy: 83.16666666666667
[ Mon Jul 15 04:42:47 2024 ] 	Batch(1500/6809) done. Loss: 0.2462  lr:0.010000
[ Mon Jul 15 04:43:10 2024 ] 	Batch(1600/6809) done. Loss: 0.1496  lr:0.010000
[ Mon Jul 15 04:43:34 2024 ] 	Batch(1700/6809) done. Loss: 0.4458  lr:0.010000
[ Mon Jul 15 04:43:57 2024 ] 	Batch(1800/6809) done. Loss: 0.2901  lr:0.010000
[ Mon Jul 15 04:44:21 2024 ] 	Batch(1900/6809) done. Loss: 0.0321  lr:0.010000
[ Mon Jul 15 04:44:44 2024 ] 
Training: Epoch [34/150], Step [1999], Loss: 0.1393532007932663, Training Accuracy: 83.31875
[ Mon Jul 15 04:44:45 2024 ] 	Batch(2000/6809) done. Loss: 0.2646  lr:0.010000
[ Mon Jul 15 04:45:08 2024 ] 	Batch(2100/6809) done. Loss: 0.5308  lr:0.010000
[ Mon Jul 15 04:45:30 2024 ] 	Batch(2200/6809) done. Loss: 0.3914  lr:0.010000
[ Mon Jul 15 04:45:53 2024 ] 	Batch(2300/6809) done. Loss: 0.5470  lr:0.010000
[ Mon Jul 15 04:46:16 2024 ] 	Batch(2400/6809) done. Loss: 0.4791  lr:0.010000
[ Mon Jul 15 04:46:38 2024 ] 
Training: Epoch [34/150], Step [2499], Loss: 0.27999651432037354, Training Accuracy: 83.33500000000001
[ Mon Jul 15 04:46:38 2024 ] 	Batch(2500/6809) done. Loss: 0.3530  lr:0.010000
[ Mon Jul 15 04:47:01 2024 ] 	Batch(2600/6809) done. Loss: 0.5883  lr:0.010000
[ Mon Jul 15 04:47:24 2024 ] 	Batch(2700/6809) done. Loss: 0.4376  lr:0.010000
[ Mon Jul 15 04:47:47 2024 ] 	Batch(2800/6809) done. Loss: 0.7383  lr:0.010000
[ Mon Jul 15 04:48:09 2024 ] 	Batch(2900/6809) done. Loss: 0.2717  lr:0.010000
[ Mon Jul 15 04:48:32 2024 ] 
Training: Epoch [34/150], Step [2999], Loss: 0.2405993938446045, Training Accuracy: 83.4875
[ Mon Jul 15 04:48:32 2024 ] 	Batch(3000/6809) done. Loss: 0.1775  lr:0.010000
[ Mon Jul 15 04:48:55 2024 ] 	Batch(3100/6809) done. Loss: 0.3084  lr:0.010000
[ Mon Jul 15 04:49:18 2024 ] 	Batch(3200/6809) done. Loss: 0.8232  lr:0.010000
[ Mon Jul 15 04:49:40 2024 ] 	Batch(3300/6809) done. Loss: 0.2549  lr:0.010000
[ Mon Jul 15 04:50:03 2024 ] 	Batch(3400/6809) done. Loss: 0.1347  lr:0.010000
[ Mon Jul 15 04:50:25 2024 ] 
Training: Epoch [34/150], Step [3499], Loss: 0.1636018455028534, Training Accuracy: 83.37857142857142
[ Mon Jul 15 04:50:26 2024 ] 	Batch(3500/6809) done. Loss: 0.1443  lr:0.010000
[ Mon Jul 15 04:50:48 2024 ] 	Batch(3600/6809) done. Loss: 0.2634  lr:0.010000
[ Mon Jul 15 04:51:11 2024 ] 	Batch(3700/6809) done. Loss: 0.7323  lr:0.010000
[ Mon Jul 15 04:51:34 2024 ] 	Batch(3800/6809) done. Loss: 0.9969  lr:0.010000
[ Mon Jul 15 04:51:56 2024 ] 	Batch(3900/6809) done. Loss: 0.5057  lr:0.010000
[ Mon Jul 15 04:52:19 2024 ] 
Training: Epoch [34/150], Step [3999], Loss: 0.020687244832515717, Training Accuracy: 83.171875
[ Mon Jul 15 04:52:19 2024 ] 	Batch(4000/6809) done. Loss: 0.3022  lr:0.010000
[ Mon Jul 15 04:52:42 2024 ] 	Batch(4100/6809) done. Loss: 0.4104  lr:0.010000
[ Mon Jul 15 04:53:05 2024 ] 	Batch(4200/6809) done. Loss: 0.4103  lr:0.010000
[ Mon Jul 15 04:53:27 2024 ] 	Batch(4300/6809) done. Loss: 0.6337  lr:0.010000
[ Mon Jul 15 04:53:50 2024 ] 	Batch(4400/6809) done. Loss: 1.0604  lr:0.010000
[ Mon Jul 15 04:54:13 2024 ] 
Training: Epoch [34/150], Step [4499], Loss: 0.47886839509010315, Training Accuracy: 83.06666666666666
[ Mon Jul 15 04:54:13 2024 ] 	Batch(4500/6809) done. Loss: 0.0255  lr:0.010000
[ Mon Jul 15 04:54:36 2024 ] 	Batch(4600/6809) done. Loss: 0.2810  lr:0.010000
[ Mon Jul 15 04:54:59 2024 ] 	Batch(4700/6809) done. Loss: 1.0210  lr:0.010000
[ Mon Jul 15 04:55:22 2024 ] 	Batch(4800/6809) done. Loss: 0.3372  lr:0.010000
[ Mon Jul 15 04:55:45 2024 ] 	Batch(4900/6809) done. Loss: 0.1401  lr:0.010000
[ Mon Jul 15 04:56:07 2024 ] 
Training: Epoch [34/150], Step [4999], Loss: 0.39174091815948486, Training Accuracy: 83.00999999999999
[ Mon Jul 15 04:56:07 2024 ] 	Batch(5000/6809) done. Loss: 0.4584  lr:0.010000
[ Mon Jul 15 04:56:30 2024 ] 	Batch(5100/6809) done. Loss: 0.2857  lr:0.010000
[ Mon Jul 15 04:56:53 2024 ] 	Batch(5200/6809) done. Loss: 0.7258  lr:0.010000
[ Mon Jul 15 04:57:16 2024 ] 	Batch(5300/6809) done. Loss: 0.6003  lr:0.010000
[ Mon Jul 15 04:57:38 2024 ] 	Batch(5400/6809) done. Loss: 0.5475  lr:0.010000
[ Mon Jul 15 04:58:01 2024 ] 
Training: Epoch [34/150], Step [5499], Loss: 0.26235657930374146, Training Accuracy: 82.85909090909091
[ Mon Jul 15 04:58:01 2024 ] 	Batch(5500/6809) done. Loss: 0.6450  lr:0.010000
[ Mon Jul 15 04:58:24 2024 ] 	Batch(5600/6809) done. Loss: 0.3702  lr:0.010000
[ Mon Jul 15 04:58:46 2024 ] 	Batch(5700/6809) done. Loss: 0.3447  lr:0.010000
[ Mon Jul 15 04:59:09 2024 ] 	Batch(5800/6809) done. Loss: 0.4126  lr:0.010000
[ Mon Jul 15 04:59:33 2024 ] 	Batch(5900/6809) done. Loss: 0.0604  lr:0.010000
[ Mon Jul 15 04:59:56 2024 ] 
Training: Epoch [34/150], Step [5999], Loss: 1.1108042001724243, Training Accuracy: 82.72291666666666
[ Mon Jul 15 04:59:56 2024 ] 	Batch(6000/6809) done. Loss: 0.7177  lr:0.010000
[ Mon Jul 15 05:00:19 2024 ] 	Batch(6100/6809) done. Loss: 0.7555  lr:0.010000
[ Mon Jul 15 05:00:42 2024 ] 	Batch(6200/6809) done. Loss: 0.5705  lr:0.010000
[ Mon Jul 15 05:01:04 2024 ] 	Batch(6300/6809) done. Loss: 0.0816  lr:0.010000
[ Mon Jul 15 05:01:27 2024 ] 	Batch(6400/6809) done. Loss: 0.7105  lr:0.010000
[ Mon Jul 15 05:01:50 2024 ] 
Training: Epoch [34/150], Step [6499], Loss: 0.5634911060333252, Training Accuracy: 82.72115384615385
[ Mon Jul 15 05:01:50 2024 ] 	Batch(6500/6809) done. Loss: 0.0969  lr:0.010000
[ Mon Jul 15 05:02:13 2024 ] 	Batch(6600/6809) done. Loss: 0.2494  lr:0.010000
[ Mon Jul 15 05:02:36 2024 ] 	Batch(6700/6809) done. Loss: 0.5731  lr:0.010000
[ Mon Jul 15 05:02:58 2024 ] 	Batch(6800/6809) done. Loss: 0.9117  lr:0.010000
[ Mon Jul 15 05:03:00 2024 ] 	Mean training loss: 0.5538.
[ Mon Jul 15 05:03:00 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 05:03:00 2024 ] Training epoch: 36
[ Mon Jul 15 05:03:01 2024 ] 	Batch(0/6809) done. Loss: 0.8215  lr:0.010000
[ Mon Jul 15 05:03:24 2024 ] 	Batch(100/6809) done. Loss: 0.3422  lr:0.010000
[ Mon Jul 15 05:03:47 2024 ] 	Batch(200/6809) done. Loss: 0.6199  lr:0.010000
[ Mon Jul 15 05:04:10 2024 ] 	Batch(300/6809) done. Loss: 0.3610  lr:0.010000
[ Mon Jul 15 05:04:33 2024 ] 	Batch(400/6809) done. Loss: 0.5657  lr:0.010000
[ Mon Jul 15 05:04:55 2024 ] 
Training: Epoch [35/150], Step [499], Loss: 0.9156153798103333, Training Accuracy: 84.025
[ Mon Jul 15 05:04:56 2024 ] 	Batch(500/6809) done. Loss: 0.5244  lr:0.010000
[ Mon Jul 15 05:05:18 2024 ] 	Batch(600/6809) done. Loss: 0.4030  lr:0.010000
[ Mon Jul 15 05:05:41 2024 ] 	Batch(700/6809) done. Loss: 0.3900  lr:0.010000
[ Mon Jul 15 05:06:04 2024 ] 	Batch(800/6809) done. Loss: 0.6984  lr:0.010000
[ Mon Jul 15 05:06:27 2024 ] 	Batch(900/6809) done. Loss: 0.0518  lr:0.010000
[ Mon Jul 15 05:06:49 2024 ] 
Training: Epoch [35/150], Step [999], Loss: 1.1024863719940186, Training Accuracy: 83.3125
[ Mon Jul 15 05:06:49 2024 ] 	Batch(1000/6809) done. Loss: 0.7276  lr:0.010000
[ Mon Jul 15 05:07:12 2024 ] 	Batch(1100/6809) done. Loss: 0.2299  lr:0.010000
[ Mon Jul 15 05:07:35 2024 ] 	Batch(1200/6809) done. Loss: 0.3516  lr:0.010000
[ Mon Jul 15 05:07:59 2024 ] 	Batch(1300/6809) done. Loss: 0.1452  lr:0.010000
[ Mon Jul 15 05:08:21 2024 ] 	Batch(1400/6809) done. Loss: 0.2767  lr:0.010000
[ Mon Jul 15 05:08:44 2024 ] 
Training: Epoch [35/150], Step [1499], Loss: 1.0293598175048828, Training Accuracy: 83.65833333333333
[ Mon Jul 15 05:08:45 2024 ] 	Batch(1500/6809) done. Loss: 0.1179  lr:0.010000
[ Mon Jul 15 05:09:08 2024 ] 	Batch(1600/6809) done. Loss: 0.4024  lr:0.010000
[ Mon Jul 15 05:09:31 2024 ] 	Batch(1700/6809) done. Loss: 0.2963  lr:0.010000
[ Mon Jul 15 05:09:54 2024 ] 	Batch(1800/6809) done. Loss: 0.1223  lr:0.010000
[ Mon Jul 15 05:10:17 2024 ] 	Batch(1900/6809) done. Loss: 1.5254  lr:0.010000
[ Mon Jul 15 05:10:40 2024 ] 
Training: Epoch [35/150], Step [1999], Loss: 0.2317100316286087, Training Accuracy: 83.71875
[ Mon Jul 15 05:10:40 2024 ] 	Batch(2000/6809) done. Loss: 0.0394  lr:0.010000
[ Mon Jul 15 05:11:03 2024 ] 	Batch(2100/6809) done. Loss: 0.2086  lr:0.010000
[ Mon Jul 15 05:11:26 2024 ] 	Batch(2200/6809) done. Loss: 0.5720  lr:0.010000
[ Mon Jul 15 05:11:49 2024 ] 	Batch(2300/6809) done. Loss: 0.3395  lr:0.010000
[ Mon Jul 15 05:12:12 2024 ] 	Batch(2400/6809) done. Loss: 0.3122  lr:0.010000
[ Mon Jul 15 05:12:35 2024 ] 
Training: Epoch [35/150], Step [2499], Loss: 0.09032540768384933, Training Accuracy: 83.575
[ Mon Jul 15 05:12:35 2024 ] 	Batch(2500/6809) done. Loss: 0.5946  lr:0.010000
[ Mon Jul 15 05:12:57 2024 ] 	Batch(2600/6809) done. Loss: 0.4117  lr:0.010000
[ Mon Jul 15 05:13:21 2024 ] 	Batch(2700/6809) done. Loss: 0.1053  lr:0.010000
[ Mon Jul 15 05:13:44 2024 ] 	Batch(2800/6809) done. Loss: 1.0394  lr:0.010000
[ Mon Jul 15 05:14:07 2024 ] 	Batch(2900/6809) done. Loss: 0.8991  lr:0.010000
[ Mon Jul 15 05:14:29 2024 ] 
Training: Epoch [35/150], Step [2999], Loss: 0.5246157646179199, Training Accuracy: 83.5875
[ Mon Jul 15 05:14:30 2024 ] 	Batch(3000/6809) done. Loss: 0.3175  lr:0.010000
[ Mon Jul 15 05:14:53 2024 ] 	Batch(3100/6809) done. Loss: 0.1993  lr:0.010000
[ Mon Jul 15 05:15:15 2024 ] 	Batch(3200/6809) done. Loss: 0.3382  lr:0.010000
[ Mon Jul 15 05:15:38 2024 ] 	Batch(3300/6809) done. Loss: 0.2354  lr:0.010000
[ Mon Jul 15 05:16:00 2024 ] 	Batch(3400/6809) done. Loss: 0.6638  lr:0.010000
[ Mon Jul 15 05:16:23 2024 ] 
Training: Epoch [35/150], Step [3499], Loss: 0.8145362138748169, Training Accuracy: 83.46785714285714
[ Mon Jul 15 05:16:23 2024 ] 	Batch(3500/6809) done. Loss: 0.7292  lr:0.010000
[ Mon Jul 15 05:16:46 2024 ] 	Batch(3600/6809) done. Loss: 0.0892  lr:0.010000
[ Mon Jul 15 05:17:08 2024 ] 	Batch(3700/6809) done. Loss: 0.2132  lr:0.010000
[ Mon Jul 15 05:17:31 2024 ] 	Batch(3800/6809) done. Loss: 0.4626  lr:0.010000
[ Mon Jul 15 05:17:53 2024 ] 	Batch(3900/6809) done. Loss: 0.3024  lr:0.010000
[ Mon Jul 15 05:18:15 2024 ] 
Training: Epoch [35/150], Step [3999], Loss: 1.2019977569580078, Training Accuracy: 83.35312499999999
[ Mon Jul 15 05:18:16 2024 ] 	Batch(4000/6809) done. Loss: 0.2441  lr:0.010000
[ Mon Jul 15 05:18:38 2024 ] 	Batch(4100/6809) done. Loss: 0.8778  lr:0.010000
[ Mon Jul 15 05:19:01 2024 ] 	Batch(4200/6809) done. Loss: 1.1643  lr:0.010000
[ Mon Jul 15 05:19:23 2024 ] 	Batch(4300/6809) done. Loss: 0.1775  lr:0.010000
[ Mon Jul 15 05:19:46 2024 ] 	Batch(4400/6809) done. Loss: 0.9195  lr:0.010000
[ Mon Jul 15 05:20:09 2024 ] 
Training: Epoch [35/150], Step [4499], Loss: 1.070549726486206, Training Accuracy: 83.24166666666667
[ Mon Jul 15 05:20:09 2024 ] 	Batch(4500/6809) done. Loss: 0.6671  lr:0.010000
[ Mon Jul 15 05:20:32 2024 ] 	Batch(4600/6809) done. Loss: 0.6933  lr:0.010000
[ Mon Jul 15 05:20:55 2024 ] 	Batch(4700/6809) done. Loss: 0.2008  lr:0.010000
[ Mon Jul 15 05:21:18 2024 ] 	Batch(4800/6809) done. Loss: 0.3389  lr:0.010000
[ Mon Jul 15 05:21:40 2024 ] 	Batch(4900/6809) done. Loss: 0.7865  lr:0.010000
[ Mon Jul 15 05:22:03 2024 ] 
Training: Epoch [35/150], Step [4999], Loss: 0.13280132412910461, Training Accuracy: 83.1425
[ Mon Jul 15 05:22:03 2024 ] 	Batch(5000/6809) done. Loss: 1.0012  lr:0.010000
[ Mon Jul 15 05:22:26 2024 ] 	Batch(5100/6809) done. Loss: 2.0635  lr:0.010000
[ Mon Jul 15 05:22:48 2024 ] 	Batch(5200/6809) done. Loss: 0.5071  lr:0.010000
[ Mon Jul 15 05:23:11 2024 ] 	Batch(5300/6809) done. Loss: 0.4095  lr:0.010000
[ Mon Jul 15 05:23:33 2024 ] 	Batch(5400/6809) done. Loss: 0.5250  lr:0.010000
[ Mon Jul 15 05:23:56 2024 ] 
Training: Epoch [35/150], Step [5499], Loss: 0.10573631525039673, Training Accuracy: 83.10909090909091
[ Mon Jul 15 05:23:56 2024 ] 	Batch(5500/6809) done. Loss: 1.2037  lr:0.010000
[ Mon Jul 15 05:24:19 2024 ] 	Batch(5600/6809) done. Loss: 0.4150  lr:0.010000
[ Mon Jul 15 05:24:41 2024 ] 	Batch(5700/6809) done. Loss: 0.6174  lr:0.010000
[ Mon Jul 15 05:25:04 2024 ] 	Batch(5800/6809) done. Loss: 0.6432  lr:0.010000
[ Mon Jul 15 05:25:28 2024 ] 	Batch(5900/6809) done. Loss: 0.5939  lr:0.010000
[ Mon Jul 15 05:25:51 2024 ] 
Training: Epoch [35/150], Step [5999], Loss: 0.4086150527000427, Training Accuracy: 83.06458333333333
[ Mon Jul 15 05:25:51 2024 ] 	Batch(6000/6809) done. Loss: 1.0318  lr:0.010000
[ Mon Jul 15 05:26:14 2024 ] 	Batch(6100/6809) done. Loss: 1.0113  lr:0.010000
[ Mon Jul 15 05:26:37 2024 ] 	Batch(6200/6809) done. Loss: 0.1507  lr:0.010000
[ Mon Jul 15 05:27:00 2024 ] 	Batch(6300/6809) done. Loss: 0.1371  lr:0.010000
[ Mon Jul 15 05:27:23 2024 ] 	Batch(6400/6809) done. Loss: 1.7096  lr:0.010000
[ Mon Jul 15 05:27:46 2024 ] 
Training: Epoch [35/150], Step [6499], Loss: 0.6769068837165833, Training Accuracy: 83.03269230769232
[ Mon Jul 15 05:27:46 2024 ] 	Batch(6500/6809) done. Loss: 0.8394  lr:0.010000
[ Mon Jul 15 05:28:09 2024 ] 	Batch(6600/6809) done. Loss: 0.8136  lr:0.010000
[ Mon Jul 15 05:28:32 2024 ] 	Batch(6700/6809) done. Loss: 1.4002  lr:0.010000
[ Mon Jul 15 05:28:56 2024 ] 	Batch(6800/6809) done. Loss: 0.0243  lr:0.010000
[ Mon Jul 15 05:28:57 2024 ] 	Mean training loss: 0.5524.
[ Mon Jul 15 05:28:57 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 05:28:58 2024 ] Training epoch: 37
[ Mon Jul 15 05:28:58 2024 ] 	Batch(0/6809) done. Loss: 0.3083  lr:0.010000
[ Mon Jul 15 05:29:21 2024 ] 	Batch(100/6809) done. Loss: 0.4928  lr:0.010000
[ Mon Jul 15 05:29:44 2024 ] 	Batch(200/6809) done. Loss: 0.2517  lr:0.010000
[ Mon Jul 15 05:30:07 2024 ] 	Batch(300/6809) done. Loss: 1.4968  lr:0.010000
[ Mon Jul 15 05:30:29 2024 ] 	Batch(400/6809) done. Loss: 0.5884  lr:0.010000
[ Mon Jul 15 05:30:53 2024 ] 
Training: Epoch [36/150], Step [499], Loss: 0.5998531579971313, Training Accuracy: 83.875
[ Mon Jul 15 05:30:53 2024 ] 	Batch(500/6809) done. Loss: 0.1700  lr:0.010000
[ Mon Jul 15 05:31:17 2024 ] 	Batch(600/6809) done. Loss: 0.0494  lr:0.010000
[ Mon Jul 15 05:31:40 2024 ] 	Batch(700/6809) done. Loss: 0.2828  lr:0.010000
[ Mon Jul 15 05:32:03 2024 ] 	Batch(800/6809) done. Loss: 0.9804  lr:0.010000
[ Mon Jul 15 05:32:26 2024 ] 	Batch(900/6809) done. Loss: 0.2673  lr:0.010000
[ Mon Jul 15 05:32:49 2024 ] 
Training: Epoch [36/150], Step [999], Loss: 2.370504856109619, Training Accuracy: 83.6625
[ Mon Jul 15 05:32:49 2024 ] 	Batch(1000/6809) done. Loss: 0.3814  lr:0.010000
[ Mon Jul 15 05:33:12 2024 ] 	Batch(1100/6809) done. Loss: 0.7435  lr:0.010000
[ Mon Jul 15 05:33:36 2024 ] 	Batch(1200/6809) done. Loss: 0.3695  lr:0.010000
[ Mon Jul 15 05:33:59 2024 ] 	Batch(1300/6809) done. Loss: 0.5517  lr:0.010000
[ Mon Jul 15 05:34:22 2024 ] 	Batch(1400/6809) done. Loss: 0.6544  lr:0.010000
[ Mon Jul 15 05:34:45 2024 ] 
Training: Epoch [36/150], Step [1499], Loss: 0.07297798246145248, Training Accuracy: 83.76666666666667
[ Mon Jul 15 05:34:45 2024 ] 	Batch(1500/6809) done. Loss: 0.3330  lr:0.010000
[ Mon Jul 15 05:35:08 2024 ] 	Batch(1600/6809) done. Loss: 0.0982  lr:0.010000
[ Mon Jul 15 05:35:31 2024 ] 	Batch(1700/6809) done. Loss: 0.2552  lr:0.010000
[ Mon Jul 15 05:35:54 2024 ] 	Batch(1800/6809) done. Loss: 0.5488  lr:0.010000
[ Mon Jul 15 05:36:17 2024 ] 	Batch(1900/6809) done. Loss: 0.1883  lr:0.010000
[ Mon Jul 15 05:36:39 2024 ] 
Training: Epoch [36/150], Step [1999], Loss: 1.569649338722229, Training Accuracy: 83.9875
[ Mon Jul 15 05:36:40 2024 ] 	Batch(2000/6809) done. Loss: 1.1187  lr:0.010000
[ Mon Jul 15 05:37:02 2024 ] 	Batch(2100/6809) done. Loss: 0.4387  lr:0.010000
[ Mon Jul 15 05:37:25 2024 ] 	Batch(2200/6809) done. Loss: 0.3063  lr:0.010000
[ Mon Jul 15 05:37:48 2024 ] 	Batch(2300/6809) done. Loss: 0.4969  lr:0.010000
[ Mon Jul 15 05:38:11 2024 ] 	Batch(2400/6809) done. Loss: 1.2141  lr:0.010000
[ Mon Jul 15 05:38:33 2024 ] 
Training: Epoch [36/150], Step [2499], Loss: 0.7533866167068481, Training Accuracy: 83.78
[ Mon Jul 15 05:38:33 2024 ] 	Batch(2500/6809) done. Loss: 0.2995  lr:0.010000
[ Mon Jul 15 05:38:56 2024 ] 	Batch(2600/6809) done. Loss: 0.1926  lr:0.010000
[ Mon Jul 15 05:39:19 2024 ] 	Batch(2700/6809) done. Loss: 0.6315  lr:0.010000
[ Mon Jul 15 05:39:42 2024 ] 	Batch(2800/6809) done. Loss: 0.4868  lr:0.010000
[ Mon Jul 15 05:40:05 2024 ] 	Batch(2900/6809) done. Loss: 0.1158  lr:0.010000
[ Mon Jul 15 05:40:28 2024 ] 
Training: Epoch [36/150], Step [2999], Loss: 0.7224798798561096, Training Accuracy: 83.5625
[ Mon Jul 15 05:40:28 2024 ] 	Batch(3000/6809) done. Loss: 0.1937  lr:0.010000
[ Mon Jul 15 05:40:50 2024 ] 	Batch(3100/6809) done. Loss: 0.0433  lr:0.010000
[ Mon Jul 15 05:41:13 2024 ] 	Batch(3200/6809) done. Loss: 0.6699  lr:0.010000
[ Mon Jul 15 05:41:36 2024 ] 	Batch(3300/6809) done. Loss: 0.0887  lr:0.010000
[ Mon Jul 15 05:41:59 2024 ] 	Batch(3400/6809) done. Loss: 0.6273  lr:0.010000
[ Mon Jul 15 05:42:21 2024 ] 
Training: Epoch [36/150], Step [3499], Loss: 0.04449952766299248, Training Accuracy: 83.40357142857144
[ Mon Jul 15 05:42:21 2024 ] 	Batch(3500/6809) done. Loss: 0.3513  lr:0.010000
[ Mon Jul 15 05:42:44 2024 ] 	Batch(3600/6809) done. Loss: 0.3173  lr:0.010000
[ Mon Jul 15 05:43:07 2024 ] 	Batch(3700/6809) done. Loss: 0.5688  lr:0.010000
[ Mon Jul 15 05:43:30 2024 ] 	Batch(3800/6809) done. Loss: 0.7480  lr:0.010000
[ Mon Jul 15 05:43:52 2024 ] 	Batch(3900/6809) done. Loss: 0.2845  lr:0.010000
[ Mon Jul 15 05:44:15 2024 ] 
Training: Epoch [36/150], Step [3999], Loss: 0.38791537284851074, Training Accuracy: 83.428125
[ Mon Jul 15 05:44:15 2024 ] 	Batch(4000/6809) done. Loss: 0.1674  lr:0.010000
[ Mon Jul 15 05:44:38 2024 ] 	Batch(4100/6809) done. Loss: 0.4034  lr:0.010000
[ Mon Jul 15 05:45:01 2024 ] 	Batch(4200/6809) done. Loss: 0.8859  lr:0.010000
[ Mon Jul 15 05:45:23 2024 ] 	Batch(4300/6809) done. Loss: 0.4614  lr:0.010000
[ Mon Jul 15 05:45:46 2024 ] 	Batch(4400/6809) done. Loss: 0.5998  lr:0.010000
[ Mon Jul 15 05:46:09 2024 ] 
Training: Epoch [36/150], Step [4499], Loss: 1.0871753692626953, Training Accuracy: 83.325
[ Mon Jul 15 05:46:09 2024 ] 	Batch(4500/6809) done. Loss: 0.6231  lr:0.010000
[ Mon Jul 15 05:46:31 2024 ] 	Batch(4600/6809) done. Loss: 1.1936  lr:0.010000
[ Mon Jul 15 05:46:54 2024 ] 	Batch(4700/6809) done. Loss: 0.1527  lr:0.010000
[ Mon Jul 15 05:47:17 2024 ] 	Batch(4800/6809) done. Loss: 0.1687  lr:0.010000
[ Mon Jul 15 05:47:40 2024 ] 	Batch(4900/6809) done. Loss: 0.0879  lr:0.010000
[ Mon Jul 15 05:48:02 2024 ] 
Training: Epoch [36/150], Step [4999], Loss: 1.0140652656555176, Training Accuracy: 83.22
[ Mon Jul 15 05:48:02 2024 ] 	Batch(5000/6809) done. Loss: 0.6318  lr:0.010000
[ Mon Jul 15 05:48:25 2024 ] 	Batch(5100/6809) done. Loss: 0.5412  lr:0.010000
[ Mon Jul 15 05:48:48 2024 ] 	Batch(5200/6809) done. Loss: 0.4789  lr:0.010000
[ Mon Jul 15 05:49:10 2024 ] 	Batch(5300/6809) done. Loss: 0.6730  lr:0.010000
[ Mon Jul 15 05:49:33 2024 ] 	Batch(5400/6809) done. Loss: 0.0459  lr:0.010000
[ Mon Jul 15 05:49:56 2024 ] 
Training: Epoch [36/150], Step [5499], Loss: 0.9823331832885742, Training Accuracy: 83.12954545454545
[ Mon Jul 15 05:49:56 2024 ] 	Batch(5500/6809) done. Loss: 1.3008  lr:0.010000
[ Mon Jul 15 05:50:19 2024 ] 	Batch(5600/6809) done. Loss: 1.2163  lr:0.010000
[ Mon Jul 15 05:50:41 2024 ] 	Batch(5700/6809) done. Loss: 1.3373  lr:0.010000
[ Mon Jul 15 05:51:04 2024 ] 	Batch(5800/6809) done. Loss: 0.8315  lr:0.010000
[ Mon Jul 15 05:51:28 2024 ] 	Batch(5900/6809) done. Loss: 0.3787  lr:0.010000
[ Mon Jul 15 05:51:50 2024 ] 
Training: Epoch [36/150], Step [5999], Loss: 0.307752788066864, Training Accuracy: 83.13333333333334
[ Mon Jul 15 05:51:51 2024 ] 	Batch(6000/6809) done. Loss: 1.6446  lr:0.010000
[ Mon Jul 15 05:52:14 2024 ] 	Batch(6100/6809) done. Loss: 1.2799  lr:0.010000
[ Mon Jul 15 05:52:36 2024 ] 	Batch(6200/6809) done. Loss: 0.1744  lr:0.010000
[ Mon Jul 15 05:52:59 2024 ] 	Batch(6300/6809) done. Loss: 0.9353  lr:0.010000
[ Mon Jul 15 05:53:22 2024 ] 	Batch(6400/6809) done. Loss: 1.0043  lr:0.010000
[ Mon Jul 15 05:53:44 2024 ] 
Training: Epoch [36/150], Step [6499], Loss: 0.24636945128440857, Training Accuracy: 83.03846153846153
[ Mon Jul 15 05:53:45 2024 ] 	Batch(6500/6809) done. Loss: 1.1142  lr:0.010000
[ Mon Jul 15 05:54:07 2024 ] 	Batch(6600/6809) done. Loss: 0.6900  lr:0.010000
[ Mon Jul 15 05:54:30 2024 ] 	Batch(6700/6809) done. Loss: 1.0111  lr:0.010000
[ Mon Jul 15 05:54:53 2024 ] 	Batch(6800/6809) done. Loss: 0.7961  lr:0.010000
[ Mon Jul 15 05:54:55 2024 ] 	Mean training loss: 0.5462.
[ Mon Jul 15 05:54:55 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 05:54:55 2024 ] Training epoch: 38
[ Mon Jul 15 05:54:56 2024 ] 	Batch(0/6809) done. Loss: 0.4250  lr:0.010000
[ Mon Jul 15 05:55:19 2024 ] 	Batch(100/6809) done. Loss: 0.5283  lr:0.010000
[ Mon Jul 15 05:55:41 2024 ] 	Batch(200/6809) done. Loss: 0.1993  lr:0.010000
[ Mon Jul 15 05:56:04 2024 ] 	Batch(300/6809) done. Loss: 0.4662  lr:0.010000
[ Mon Jul 15 05:56:27 2024 ] 	Batch(400/6809) done. Loss: 0.0671  lr:0.010000
[ Mon Jul 15 05:56:49 2024 ] 
Training: Epoch [37/150], Step [499], Loss: 0.3418046832084656, Training Accuracy: 84.45
[ Mon Jul 15 05:56:50 2024 ] 	Batch(500/6809) done. Loss: 0.2287  lr:0.010000
[ Mon Jul 15 05:57:12 2024 ] 	Batch(600/6809) done. Loss: 0.3572  lr:0.010000
[ Mon Jul 15 05:57:35 2024 ] 	Batch(700/6809) done. Loss: 0.2295  lr:0.010000
[ Mon Jul 15 05:57:58 2024 ] 	Batch(800/6809) done. Loss: 0.2705  lr:0.010000
[ Mon Jul 15 05:58:20 2024 ] 	Batch(900/6809) done. Loss: 0.4695  lr:0.010000
[ Mon Jul 15 05:58:43 2024 ] 
Training: Epoch [37/150], Step [999], Loss: 0.282090961933136, Training Accuracy: 84.35000000000001
[ Mon Jul 15 05:58:43 2024 ] 	Batch(1000/6809) done. Loss: 0.0520  lr:0.010000
[ Mon Jul 15 05:59:06 2024 ] 	Batch(1100/6809) done. Loss: 0.4473  lr:0.010000
[ Mon Jul 15 05:59:29 2024 ] 	Batch(1200/6809) done. Loss: 0.2746  lr:0.010000
[ Mon Jul 15 05:59:51 2024 ] 	Batch(1300/6809) done. Loss: 1.6932  lr:0.010000
[ Mon Jul 15 06:00:14 2024 ] 	Batch(1400/6809) done. Loss: 0.0865  lr:0.010000
[ Mon Jul 15 06:00:36 2024 ] 
Training: Epoch [37/150], Step [1499], Loss: 0.16025584936141968, Training Accuracy: 84.45833333333333
[ Mon Jul 15 06:00:37 2024 ] 	Batch(1500/6809) done. Loss: 0.4679  lr:0.010000
[ Mon Jul 15 06:01:00 2024 ] 	Batch(1600/6809) done. Loss: 0.8073  lr:0.010000
[ Mon Jul 15 06:01:22 2024 ] 	Batch(1700/6809) done. Loss: 0.3265  lr:0.010000
[ Mon Jul 15 06:01:45 2024 ] 	Batch(1800/6809) done. Loss: 0.3432  lr:0.010000
[ Mon Jul 15 06:02:08 2024 ] 	Batch(1900/6809) done. Loss: 0.9058  lr:0.010000
[ Mon Jul 15 06:02:30 2024 ] 
Training: Epoch [37/150], Step [1999], Loss: 0.08836549520492554, Training Accuracy: 84.30625
[ Mon Jul 15 06:02:31 2024 ] 	Batch(2000/6809) done. Loss: 0.6040  lr:0.010000
[ Mon Jul 15 06:02:53 2024 ] 	Batch(2100/6809) done. Loss: 0.1554  lr:0.010000
[ Mon Jul 15 06:03:16 2024 ] 	Batch(2200/6809) done. Loss: 0.1342  lr:0.010000
[ Mon Jul 15 06:03:39 2024 ] 	Batch(2300/6809) done. Loss: 0.9120  lr:0.010000
[ Mon Jul 15 06:04:01 2024 ] 	Batch(2400/6809) done. Loss: 0.5831  lr:0.010000
[ Mon Jul 15 06:04:24 2024 ] 
Training: Epoch [37/150], Step [2499], Loss: 0.4611043632030487, Training Accuracy: 84.3
[ Mon Jul 15 06:04:24 2024 ] 	Batch(2500/6809) done. Loss: 0.4757  lr:0.010000
[ Mon Jul 15 06:04:47 2024 ] 	Batch(2600/6809) done. Loss: 0.5283  lr:0.010000
[ Mon Jul 15 06:05:10 2024 ] 	Batch(2700/6809) done. Loss: 0.0509  lr:0.010000
[ Mon Jul 15 06:05:32 2024 ] 	Batch(2800/6809) done. Loss: 0.7714  lr:0.010000
[ Mon Jul 15 06:05:55 2024 ] 	Batch(2900/6809) done. Loss: 0.2077  lr:0.010000
[ Mon Jul 15 06:06:18 2024 ] 
Training: Epoch [37/150], Step [2999], Loss: 0.7307623624801636, Training Accuracy: 84.29583333333333
[ Mon Jul 15 06:06:18 2024 ] 	Batch(3000/6809) done. Loss: 0.2438  lr:0.010000
[ Mon Jul 15 06:06:40 2024 ] 	Batch(3100/6809) done. Loss: 0.0146  lr:0.010000
[ Mon Jul 15 06:07:03 2024 ] 	Batch(3200/6809) done. Loss: 0.2853  lr:0.010000
[ Mon Jul 15 06:07:27 2024 ] 	Batch(3300/6809) done. Loss: 0.6358  lr:0.010000
[ Mon Jul 15 06:07:50 2024 ] 	Batch(3400/6809) done. Loss: 1.0883  lr:0.010000
[ Mon Jul 15 06:08:14 2024 ] 
Training: Epoch [37/150], Step [3499], Loss: 0.3913705050945282, Training Accuracy: 84.06428571428572
[ Mon Jul 15 06:08:14 2024 ] 	Batch(3500/6809) done. Loss: 0.2604  lr:0.010000
[ Mon Jul 15 06:08:37 2024 ] 	Batch(3600/6809) done. Loss: 0.3221  lr:0.010000
[ Mon Jul 15 06:09:00 2024 ] 	Batch(3700/6809) done. Loss: 0.4357  lr:0.010000
[ Mon Jul 15 06:09:23 2024 ] 	Batch(3800/6809) done. Loss: 0.3356  lr:0.010000
[ Mon Jul 15 06:09:45 2024 ] 	Batch(3900/6809) done. Loss: 0.2367  lr:0.010000
[ Mon Jul 15 06:10:08 2024 ] 
Training: Epoch [37/150], Step [3999], Loss: 0.12791624665260315, Training Accuracy: 83.971875
[ Mon Jul 15 06:10:08 2024 ] 	Batch(4000/6809) done. Loss: 0.7179  lr:0.010000
[ Mon Jul 15 06:10:31 2024 ] 	Batch(4100/6809) done. Loss: 0.9110  lr:0.010000
[ Mon Jul 15 06:10:54 2024 ] 	Batch(4200/6809) done. Loss: 0.2325  lr:0.010000
[ Mon Jul 15 06:11:16 2024 ] 	Batch(4300/6809) done. Loss: 0.7956  lr:0.010000
[ Mon Jul 15 06:11:39 2024 ] 	Batch(4400/6809) done. Loss: 0.2977  lr:0.010000
[ Mon Jul 15 06:12:02 2024 ] 
Training: Epoch [37/150], Step [4499], Loss: 0.3898833394050598, Training Accuracy: 83.99722222222222
[ Mon Jul 15 06:12:02 2024 ] 	Batch(4500/6809) done. Loss: 0.5615  lr:0.010000
[ Mon Jul 15 06:12:24 2024 ] 	Batch(4600/6809) done. Loss: 0.7422  lr:0.010000
[ Mon Jul 15 06:12:47 2024 ] 	Batch(4700/6809) done. Loss: 0.2818  lr:0.010000
[ Mon Jul 15 06:13:10 2024 ] 	Batch(4800/6809) done. Loss: 0.2725  lr:0.010000
[ Mon Jul 15 06:13:33 2024 ] 	Batch(4900/6809) done. Loss: 0.0452  lr:0.010000
[ Mon Jul 15 06:13:55 2024 ] 
Training: Epoch [37/150], Step [4999], Loss: 0.6452344655990601, Training Accuracy: 83.76
[ Mon Jul 15 06:13:55 2024 ] 	Batch(5000/6809) done. Loss: 0.4605  lr:0.010000
[ Mon Jul 15 06:14:18 2024 ] 	Batch(5100/6809) done. Loss: 0.5577  lr:0.010000
[ Mon Jul 15 06:14:41 2024 ] 	Batch(5200/6809) done. Loss: 0.8373  lr:0.010000
[ Mon Jul 15 06:15:04 2024 ] 	Batch(5300/6809) done. Loss: 0.0820  lr:0.010000
[ Mon Jul 15 06:15:26 2024 ] 	Batch(5400/6809) done. Loss: 0.3919  lr:0.010000
[ Mon Jul 15 06:15:49 2024 ] 
Training: Epoch [37/150], Step [5499], Loss: 0.5424144268035889, Training Accuracy: 83.68409090909091
[ Mon Jul 15 06:15:49 2024 ] 	Batch(5500/6809) done. Loss: 1.5064  lr:0.010000
[ Mon Jul 15 06:16:13 2024 ] 	Batch(5600/6809) done. Loss: 1.2866  lr:0.010000
[ Mon Jul 15 06:16:36 2024 ] 	Batch(5700/6809) done. Loss: 1.1866  lr:0.010000
[ Mon Jul 15 06:16:59 2024 ] 	Batch(5800/6809) done. Loss: 0.4757  lr:0.010000
[ Mon Jul 15 06:17:22 2024 ] 	Batch(5900/6809) done. Loss: 0.2441  lr:0.010000
[ Mon Jul 15 06:17:45 2024 ] 
Training: Epoch [37/150], Step [5999], Loss: 0.36814549565315247, Training Accuracy: 83.575
[ Mon Jul 15 06:17:45 2024 ] 	Batch(6000/6809) done. Loss: 0.5000  lr:0.010000
[ Mon Jul 15 06:18:09 2024 ] 	Batch(6100/6809) done. Loss: 0.5689  lr:0.010000
[ Mon Jul 15 06:18:32 2024 ] 	Batch(6200/6809) done. Loss: 0.8458  lr:0.010000
[ Mon Jul 15 06:18:55 2024 ] 	Batch(6300/6809) done. Loss: 0.4234  lr:0.010000
[ Mon Jul 15 06:19:18 2024 ] 	Batch(6400/6809) done. Loss: 0.0478  lr:0.010000
[ Mon Jul 15 06:19:41 2024 ] 
Training: Epoch [37/150], Step [6499], Loss: 0.48809072375297546, Training Accuracy: 83.4173076923077
[ Mon Jul 15 06:19:41 2024 ] 	Batch(6500/6809) done. Loss: 0.8233  lr:0.010000
[ Mon Jul 15 06:20:04 2024 ] 	Batch(6600/6809) done. Loss: 0.2177  lr:0.010000
[ Mon Jul 15 06:20:28 2024 ] 	Batch(6700/6809) done. Loss: 0.5679  lr:0.010000
[ Mon Jul 15 06:20:51 2024 ] 	Batch(6800/6809) done. Loss: 1.0663  lr:0.010000
[ Mon Jul 15 06:20:53 2024 ] 	Mean training loss: 0.5299.
[ Mon Jul 15 06:20:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 06:20:53 2024 ] Training epoch: 39
[ Mon Jul 15 06:20:54 2024 ] 	Batch(0/6809) done. Loss: 0.9703  lr:0.010000
[ Mon Jul 15 06:21:17 2024 ] 	Batch(100/6809) done. Loss: 0.5717  lr:0.010000
[ Mon Jul 15 06:21:40 2024 ] 	Batch(200/6809) done. Loss: 0.1405  lr:0.010000
[ Mon Jul 15 06:22:02 2024 ] 	Batch(300/6809) done. Loss: 0.0195  lr:0.010000
[ Mon Jul 15 06:22:26 2024 ] 	Batch(400/6809) done. Loss: 0.6433  lr:0.010000
[ Mon Jul 15 06:22:48 2024 ] 
Training: Epoch [38/150], Step [499], Loss: 0.10922051966190338, Training Accuracy: 84.8
[ Mon Jul 15 06:22:48 2024 ] 	Batch(500/6809) done. Loss: 0.5557  lr:0.010000
[ Mon Jul 15 06:23:11 2024 ] 	Batch(600/6809) done. Loss: 1.1095  lr:0.010000
[ Mon Jul 15 06:23:34 2024 ] 	Batch(700/6809) done. Loss: 0.5358  lr:0.010000
[ Mon Jul 15 06:23:57 2024 ] 	Batch(800/6809) done. Loss: 0.4349  lr:0.010000
[ Mon Jul 15 06:24:20 2024 ] 	Batch(900/6809) done. Loss: 0.3933  lr:0.010000
[ Mon Jul 15 06:24:43 2024 ] 
Training: Epoch [38/150], Step [999], Loss: 0.2795390188694, Training Accuracy: 84.96249999999999
[ Mon Jul 15 06:24:43 2024 ] 	Batch(1000/6809) done. Loss: 0.7436  lr:0.010000
[ Mon Jul 15 06:25:06 2024 ] 	Batch(1100/6809) done. Loss: 0.6892  lr:0.010000
[ Mon Jul 15 06:25:29 2024 ] 	Batch(1200/6809) done. Loss: 0.7346  lr:0.010000
[ Mon Jul 15 06:25:52 2024 ] 	Batch(1300/6809) done. Loss: 0.0147  lr:0.010000
[ Mon Jul 15 06:26:14 2024 ] 	Batch(1400/6809) done. Loss: 1.5215  lr:0.010000
[ Mon Jul 15 06:26:36 2024 ] 
Training: Epoch [38/150], Step [1499], Loss: 0.14383767545223236, Training Accuracy: 84.875
[ Mon Jul 15 06:26:37 2024 ] 	Batch(1500/6809) done. Loss: 0.2391  lr:0.010000
[ Mon Jul 15 06:26:59 2024 ] 	Batch(1600/6809) done. Loss: 0.1015  lr:0.010000
[ Mon Jul 15 06:27:22 2024 ] 	Batch(1700/6809) done. Loss: 0.7109  lr:0.010000
[ Mon Jul 15 06:27:44 2024 ] 	Batch(1800/6809) done. Loss: 0.2412  lr:0.010000
[ Mon Jul 15 06:28:07 2024 ] 	Batch(1900/6809) done. Loss: 0.5050  lr:0.010000
[ Mon Jul 15 06:28:29 2024 ] 
Training: Epoch [38/150], Step [1999], Loss: 0.6446368098258972, Training Accuracy: 84.71875
[ Mon Jul 15 06:28:30 2024 ] 	Batch(2000/6809) done. Loss: 0.5175  lr:0.010000
[ Mon Jul 15 06:28:52 2024 ] 	Batch(2100/6809) done. Loss: 2.2027  lr:0.010000
[ Mon Jul 15 06:29:15 2024 ] 	Batch(2200/6809) done. Loss: 0.2358  lr:0.010000
[ Mon Jul 15 06:29:37 2024 ] 	Batch(2300/6809) done. Loss: 0.7392  lr:0.010000
[ Mon Jul 15 06:30:01 2024 ] 	Batch(2400/6809) done. Loss: 0.2536  lr:0.010000
[ Mon Jul 15 06:30:24 2024 ] 
Training: Epoch [38/150], Step [2499], Loss: 0.2496555894613266, Training Accuracy: 84.495
[ Mon Jul 15 06:30:24 2024 ] 	Batch(2500/6809) done. Loss: 0.3586  lr:0.010000
[ Mon Jul 15 06:30:48 2024 ] 	Batch(2600/6809) done. Loss: 0.7534  lr:0.010000
[ Mon Jul 15 06:31:11 2024 ] 	Batch(2700/6809) done. Loss: 0.5415  lr:0.010000
[ Mon Jul 15 06:31:34 2024 ] 	Batch(2800/6809) done. Loss: 1.0666  lr:0.010000
[ Mon Jul 15 06:31:57 2024 ] 	Batch(2900/6809) done. Loss: 1.1615  lr:0.010000
[ Mon Jul 15 06:32:20 2024 ] 
Training: Epoch [38/150], Step [2999], Loss: 0.1292203962802887, Training Accuracy: 84.51666666666667
[ Mon Jul 15 06:32:20 2024 ] 	Batch(3000/6809) done. Loss: 0.6678  lr:0.010000
[ Mon Jul 15 06:32:43 2024 ] 	Batch(3100/6809) done. Loss: 0.5822  lr:0.010000
[ Mon Jul 15 06:33:06 2024 ] 	Batch(3200/6809) done. Loss: 0.4295  lr:0.010000
[ Mon Jul 15 06:33:29 2024 ] 	Batch(3300/6809) done. Loss: 0.4297  lr:0.010000
[ Mon Jul 15 06:33:52 2024 ] 	Batch(3400/6809) done. Loss: 0.2697  lr:0.010000
[ Mon Jul 15 06:34:14 2024 ] 
Training: Epoch [38/150], Step [3499], Loss: 0.35366135835647583, Training Accuracy: 84.41785714285714
[ Mon Jul 15 06:34:15 2024 ] 	Batch(3500/6809) done. Loss: 0.1884  lr:0.010000
[ Mon Jul 15 06:34:38 2024 ] 	Batch(3600/6809) done. Loss: 0.6549  lr:0.010000
[ Mon Jul 15 06:35:01 2024 ] 	Batch(3700/6809) done. Loss: 1.1377  lr:0.010000
[ Mon Jul 15 06:35:24 2024 ] 	Batch(3800/6809) done. Loss: 0.7554  lr:0.010000
[ Mon Jul 15 06:35:47 2024 ] 	Batch(3900/6809) done. Loss: 1.2286  lr:0.010000
[ Mon Jul 15 06:36:09 2024 ] 
Training: Epoch [38/150], Step [3999], Loss: 0.20911172032356262, Training Accuracy: 84.284375
[ Mon Jul 15 06:36:10 2024 ] 	Batch(4000/6809) done. Loss: 0.0475  lr:0.010000
[ Mon Jul 15 06:36:33 2024 ] 	Batch(4100/6809) done. Loss: 0.2909  lr:0.010000
[ Mon Jul 15 06:36:55 2024 ] 	Batch(4200/6809) done. Loss: 0.7986  lr:0.010000
[ Mon Jul 15 06:37:18 2024 ] 	Batch(4300/6809) done. Loss: 0.5427  lr:0.010000
[ Mon Jul 15 06:37:40 2024 ] 	Batch(4400/6809) done. Loss: 0.2324  lr:0.010000
[ Mon Jul 15 06:38:03 2024 ] 
Training: Epoch [38/150], Step [4499], Loss: 1.0509297847747803, Training Accuracy: 84.14444444444445
[ Mon Jul 15 06:38:03 2024 ] 	Batch(4500/6809) done. Loss: 0.3594  lr:0.010000
[ Mon Jul 15 06:38:26 2024 ] 	Batch(4600/6809) done. Loss: 0.4411  lr:0.010000
[ Mon Jul 15 06:38:49 2024 ] 	Batch(4700/6809) done. Loss: 0.6918  lr:0.010000
[ Mon Jul 15 06:39:13 2024 ] 	Batch(4800/6809) done. Loss: 0.8219  lr:0.010000
[ Mon Jul 15 06:39:36 2024 ] 	Batch(4900/6809) done. Loss: 0.4448  lr:0.010000
[ Mon Jul 15 06:39:59 2024 ] 
Training: Epoch [38/150], Step [4999], Loss: 0.1747455596923828, Training Accuracy: 84.0225
[ Mon Jul 15 06:39:59 2024 ] 	Batch(5000/6809) done. Loss: 0.3081  lr:0.010000
[ Mon Jul 15 06:40:22 2024 ] 	Batch(5100/6809) done. Loss: 0.4989  lr:0.010000
[ Mon Jul 15 06:40:45 2024 ] 	Batch(5200/6809) done. Loss: 0.2331  lr:0.010000
[ Mon Jul 15 06:41:08 2024 ] 	Batch(5300/6809) done. Loss: 0.5929  lr:0.010000
[ Mon Jul 15 06:41:31 2024 ] 	Batch(5400/6809) done. Loss: 0.4726  lr:0.010000
[ Mon Jul 15 06:41:54 2024 ] 
Training: Epoch [38/150], Step [5499], Loss: 0.8513422012329102, Training Accuracy: 83.91818181818181
[ Mon Jul 15 06:41:54 2024 ] 	Batch(5500/6809) done. Loss: 0.2475  lr:0.010000
[ Mon Jul 15 06:42:17 2024 ] 	Batch(5600/6809) done. Loss: 0.0800  lr:0.010000
[ Mon Jul 15 06:42:40 2024 ] 	Batch(5700/6809) done. Loss: 0.5109  lr:0.010000
[ Mon Jul 15 06:43:03 2024 ] 	Batch(5800/6809) done. Loss: 0.3958  lr:0.010000
[ Mon Jul 15 06:43:25 2024 ] 	Batch(5900/6809) done. Loss: 0.1300  lr:0.010000
[ Mon Jul 15 06:43:48 2024 ] 
Training: Epoch [38/150], Step [5999], Loss: 0.099510557949543, Training Accuracy: 83.77708333333334
[ Mon Jul 15 06:43:48 2024 ] 	Batch(6000/6809) done. Loss: 0.4936  lr:0.010000
[ Mon Jul 15 06:44:11 2024 ] 	Batch(6100/6809) done. Loss: 0.8621  lr:0.010000
[ Mon Jul 15 06:44:33 2024 ] 	Batch(6200/6809) done. Loss: 0.6400  lr:0.010000
[ Mon Jul 15 06:44:56 2024 ] 	Batch(6300/6809) done. Loss: 0.7520  lr:0.010000
[ Mon Jul 15 06:45:18 2024 ] 	Batch(6400/6809) done. Loss: 0.2589  lr:0.010000
[ Mon Jul 15 06:45:41 2024 ] 
Training: Epoch [38/150], Step [6499], Loss: 0.6039319038391113, Training Accuracy: 83.64615384615385
[ Mon Jul 15 06:45:41 2024 ] 	Batch(6500/6809) done. Loss: 0.3898  lr:0.010000
[ Mon Jul 15 06:46:03 2024 ] 	Batch(6600/6809) done. Loss: 0.1192  lr:0.010000
[ Mon Jul 15 06:46:26 2024 ] 	Batch(6700/6809) done. Loss: 0.1675  lr:0.010000
[ Mon Jul 15 06:46:49 2024 ] 	Batch(6800/6809) done. Loss: 1.1250  lr:0.010000
[ Mon Jul 15 06:46:51 2024 ] 	Mean training loss: 0.5126.
[ Mon Jul 15 06:46:51 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 06:46:51 2024 ] Training epoch: 40
[ Mon Jul 15 06:46:52 2024 ] 	Batch(0/6809) done. Loss: 0.0868  lr:0.010000
[ Mon Jul 15 06:47:15 2024 ] 	Batch(100/6809) done. Loss: 0.4658  lr:0.010000
[ Mon Jul 15 06:47:38 2024 ] 	Batch(200/6809) done. Loss: 0.0308  lr:0.010000
[ Mon Jul 15 06:48:01 2024 ] 	Batch(300/6809) done. Loss: 1.5049  lr:0.010000
[ Mon Jul 15 06:48:25 2024 ] 	Batch(400/6809) done. Loss: 0.2673  lr:0.010000
[ Mon Jul 15 06:48:47 2024 ] 
Training: Epoch [39/150], Step [499], Loss: 0.08934570848941803, Training Accuracy: 84.875
[ Mon Jul 15 06:48:48 2024 ] 	Batch(500/6809) done. Loss: 0.4598  lr:0.010000
[ Mon Jul 15 06:49:11 2024 ] 	Batch(600/6809) done. Loss: 0.3260  lr:0.010000
[ Mon Jul 15 06:49:34 2024 ] 	Batch(700/6809) done. Loss: 0.2518  lr:0.010000
[ Mon Jul 15 06:49:57 2024 ] 	Batch(800/6809) done. Loss: 0.0511  lr:0.010000
[ Mon Jul 15 06:50:20 2024 ] 	Batch(900/6809) done. Loss: 0.5609  lr:0.010000
[ Mon Jul 15 06:50:42 2024 ] 
Training: Epoch [39/150], Step [999], Loss: 0.12564435601234436, Training Accuracy: 84.725
[ Mon Jul 15 06:50:42 2024 ] 	Batch(1000/6809) done. Loss: 0.0860  lr:0.010000
[ Mon Jul 15 06:51:05 2024 ] 	Batch(1100/6809) done. Loss: 0.2168  lr:0.010000
[ Mon Jul 15 06:51:28 2024 ] 	Batch(1200/6809) done. Loss: 0.3900  lr:0.010000
[ Mon Jul 15 06:51:51 2024 ] 	Batch(1300/6809) done. Loss: 0.2188  lr:0.010000
[ Mon Jul 15 06:52:13 2024 ] 	Batch(1400/6809) done. Loss: 0.4183  lr:0.010000
[ Mon Jul 15 06:52:36 2024 ] 
Training: Epoch [39/150], Step [1499], Loss: 0.05987579748034477, Training Accuracy: 84.56666666666666
[ Mon Jul 15 06:52:36 2024 ] 	Batch(1500/6809) done. Loss: 0.3977  lr:0.010000
[ Mon Jul 15 06:52:59 2024 ] 	Batch(1600/6809) done. Loss: 0.7068  lr:0.010000
[ Mon Jul 15 06:53:21 2024 ] 	Batch(1700/6809) done. Loss: 0.7149  lr:0.010000
[ Mon Jul 15 06:53:44 2024 ] 	Batch(1800/6809) done. Loss: 0.3307  lr:0.010000
[ Mon Jul 15 06:54:07 2024 ] 	Batch(1900/6809) done. Loss: 0.7513  lr:0.010000
[ Mon Jul 15 06:54:29 2024 ] 
Training: Epoch [39/150], Step [1999], Loss: 0.18965651094913483, Training Accuracy: 84.35625
[ Mon Jul 15 06:54:30 2024 ] 	Batch(2000/6809) done. Loss: 0.4954  lr:0.010000
[ Mon Jul 15 06:54:52 2024 ] 	Batch(2100/6809) done. Loss: 0.1893  lr:0.010000
[ Mon Jul 15 06:55:15 2024 ] 	Batch(2200/6809) done. Loss: 0.8835  lr:0.010000
[ Mon Jul 15 06:55:38 2024 ] 	Batch(2300/6809) done. Loss: 0.3116  lr:0.010000
[ Mon Jul 15 06:56:01 2024 ] 	Batch(2400/6809) done. Loss: 0.2376  lr:0.010000
[ Mon Jul 15 06:56:24 2024 ] 
Training: Epoch [39/150], Step [2499], Loss: 0.6055747270584106, Training Accuracy: 84.33500000000001
[ Mon Jul 15 06:56:24 2024 ] 	Batch(2500/6809) done. Loss: 0.7599  lr:0.010000
[ Mon Jul 15 06:56:48 2024 ] 	Batch(2600/6809) done. Loss: 0.1726  lr:0.010000
[ Mon Jul 15 06:57:11 2024 ] 	Batch(2700/6809) done. Loss: 0.0828  lr:0.010000
[ Mon Jul 15 06:57:35 2024 ] 	Batch(2800/6809) done. Loss: 0.6807  lr:0.010000
[ Mon Jul 15 06:57:58 2024 ] 	Batch(2900/6809) done. Loss: 0.7262  lr:0.010000
[ Mon Jul 15 06:58:22 2024 ] 
Training: Epoch [39/150], Step [2999], Loss: 0.32346850633621216, Training Accuracy: 84.27083333333333
[ Mon Jul 15 06:58:22 2024 ] 	Batch(3000/6809) done. Loss: 0.4868  lr:0.010000
[ Mon Jul 15 06:58:45 2024 ] 	Batch(3100/6809) done. Loss: 0.2733  lr:0.010000
[ Mon Jul 15 06:59:09 2024 ] 	Batch(3200/6809) done. Loss: 0.4993  lr:0.010000
[ Mon Jul 15 06:59:32 2024 ] 	Batch(3300/6809) done. Loss: 0.1238  lr:0.010000
[ Mon Jul 15 06:59:56 2024 ] 	Batch(3400/6809) done. Loss: 0.4605  lr:0.010000
[ Mon Jul 15 07:00:20 2024 ] 
Training: Epoch [39/150], Step [3499], Loss: 0.8911659717559814, Training Accuracy: 84.18214285714286
[ Mon Jul 15 07:00:20 2024 ] 	Batch(3500/6809) done. Loss: 0.6951  lr:0.010000
[ Mon Jul 15 07:00:44 2024 ] 	Batch(3600/6809) done. Loss: 1.0507  lr:0.010000
[ Mon Jul 15 07:01:07 2024 ] 	Batch(3700/6809) done. Loss: 0.1114  lr:0.010000
[ Mon Jul 15 07:01:30 2024 ] 	Batch(3800/6809) done. Loss: 0.2635  lr:0.010000
[ Mon Jul 15 07:01:53 2024 ] 	Batch(3900/6809) done. Loss: 0.4313  lr:0.010000
[ Mon Jul 15 07:02:15 2024 ] 
Training: Epoch [39/150], Step [3999], Loss: 0.6924296617507935, Training Accuracy: 84.17812500000001
[ Mon Jul 15 07:02:16 2024 ] 	Batch(4000/6809) done. Loss: 1.3683  lr:0.010000
[ Mon Jul 15 07:02:38 2024 ] 	Batch(4100/6809) done. Loss: 0.9968  lr:0.010000
[ Mon Jul 15 07:03:01 2024 ] 	Batch(4200/6809) done. Loss: 0.5857  lr:0.010000
[ Mon Jul 15 07:03:23 2024 ] 	Batch(4300/6809) done. Loss: 0.7746  lr:0.010000
[ Mon Jul 15 07:03:46 2024 ] 	Batch(4400/6809) done. Loss: 0.3020  lr:0.010000
[ Mon Jul 15 07:04:08 2024 ] 
Training: Epoch [39/150], Step [4499], Loss: 0.10404163599014282, Training Accuracy: 84.05833333333334
[ Mon Jul 15 07:04:08 2024 ] 	Batch(4500/6809) done. Loss: 0.0392  lr:0.010000
[ Mon Jul 15 07:04:31 2024 ] 	Batch(4600/6809) done. Loss: 0.0434  lr:0.010000
[ Mon Jul 15 07:04:54 2024 ] 	Batch(4700/6809) done. Loss: 0.2021  lr:0.010000
[ Mon Jul 15 07:05:16 2024 ] 	Batch(4800/6809) done. Loss: 0.1989  lr:0.010000
[ Mon Jul 15 07:05:39 2024 ] 	Batch(4900/6809) done. Loss: 0.5493  lr:0.010000
[ Mon Jul 15 07:06:01 2024 ] 
Training: Epoch [39/150], Step [4999], Loss: 1.5833897590637207, Training Accuracy: 83.9075
[ Mon Jul 15 07:06:01 2024 ] 	Batch(5000/6809) done. Loss: 0.1272  lr:0.010000
[ Mon Jul 15 07:06:24 2024 ] 	Batch(5100/6809) done. Loss: 0.3114  lr:0.010000
[ Mon Jul 15 07:06:47 2024 ] 	Batch(5200/6809) done. Loss: 0.2851  lr:0.010000
[ Mon Jul 15 07:07:09 2024 ] 	Batch(5300/6809) done. Loss: 0.2604  lr:0.010000
[ Mon Jul 15 07:07:32 2024 ] 	Batch(5400/6809) done. Loss: 0.7593  lr:0.010000
[ Mon Jul 15 07:07:54 2024 ] 
Training: Epoch [39/150], Step [5499], Loss: 0.7549009919166565, Training Accuracy: 83.90227272727273
[ Mon Jul 15 07:07:54 2024 ] 	Batch(5500/6809) done. Loss: 0.0583  lr:0.010000
[ Mon Jul 15 07:08:17 2024 ] 	Batch(5600/6809) done. Loss: 0.2737  lr:0.010000
[ Mon Jul 15 07:08:39 2024 ] 	Batch(5700/6809) done. Loss: 0.4027  lr:0.010000
[ Mon Jul 15 07:09:02 2024 ] 	Batch(5800/6809) done. Loss: 0.7966  lr:0.010000
[ Mon Jul 15 07:09:24 2024 ] 	Batch(5900/6809) done. Loss: 0.7621  lr:0.010000
[ Mon Jul 15 07:09:47 2024 ] 
Training: Epoch [39/150], Step [5999], Loss: 0.6727718114852905, Training Accuracy: 83.85833333333333
[ Mon Jul 15 07:09:47 2024 ] 	Batch(6000/6809) done. Loss: 0.1069  lr:0.010000
[ Mon Jul 15 07:10:10 2024 ] 	Batch(6100/6809) done. Loss: 0.2326  lr:0.010000
[ Mon Jul 15 07:10:32 2024 ] 	Batch(6200/6809) done. Loss: 0.1016  lr:0.010000
[ Mon Jul 15 07:10:55 2024 ] 	Batch(6300/6809) done. Loss: 0.3004  lr:0.010000
[ Mon Jul 15 07:11:17 2024 ] 	Batch(6400/6809) done. Loss: 0.1269  lr:0.010000
[ Mon Jul 15 07:11:40 2024 ] 
Training: Epoch [39/150], Step [6499], Loss: 0.091328926384449, Training Accuracy: 83.79423076923077
[ Mon Jul 15 07:11:40 2024 ] 	Batch(6500/6809) done. Loss: 0.2444  lr:0.010000
[ Mon Jul 15 07:12:03 2024 ] 	Batch(6600/6809) done. Loss: 0.0289  lr:0.010000
[ Mon Jul 15 07:12:26 2024 ] 	Batch(6700/6809) done. Loss: 0.1801  lr:0.010000
[ Mon Jul 15 07:12:49 2024 ] 	Batch(6800/6809) done. Loss: 0.3029  lr:0.010000
[ Mon Jul 15 07:12:51 2024 ] 	Mean training loss: 0.5110.
[ Mon Jul 15 07:12:51 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 07:12:51 2024 ] Eval epoch: 40
[ Mon Jul 15 07:19:47 2024 ] 	Mean val loss of 7435 batches: 1.0590509936882768.
[ Mon Jul 15 07:19:47 2024 ] 
Validation: Epoch [39/150], Samples [43888.0/59477], Loss: 2.4054453372955322, Validation Accuracy: 73.78986835247238
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 1 : 375 / 500 = 75 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 2 : 335 / 499 = 67 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 3 : 391 / 500 = 78 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 4 : 415 / 502 = 82 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 5 : 373 / 502 = 74 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 6 : 436 / 502 = 86 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 7 : 464 / 497 = 93 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 8 : 473 / 498 = 94 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 9 : 325 / 500 = 65 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 10 : 108 / 500 = 21 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 11 : 203 / 498 = 40 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 12 : 379 / 499 = 75 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 13 : 480 / 502 = 95 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 14 : 482 / 504 = 95 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 15 : 391 / 502 = 77 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 16 : 209 / 502 = 41 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 17 : 406 / 504 = 80 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 18 : 383 / 504 = 75 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 19 : 383 / 502 = 76 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 20 : 450 / 502 = 89 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 21 : 449 / 503 = 89 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 22 : 404 / 504 = 80 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 23 : 390 / 503 = 77 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 24 : 435 / 504 = 86 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 25 : 421 / 504 = 83 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 26 : 470 / 504 = 93 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 27 : 424 / 501 = 84 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 28 : 382 / 502 = 76 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 29 : 175 / 502 = 34 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 30 : 370 / 501 = 73 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 31 : 330 / 504 = 65 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 32 : 436 / 503 = 86 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 33 : 259 / 503 = 51 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 34 : 466 / 504 = 92 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 35 : 476 / 503 = 94 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 36 : 313 / 502 = 62 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 37 : 448 / 504 = 88 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 38 : 447 / 504 = 88 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 39 : 446 / 498 = 89 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 40 : 375 / 504 = 74 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 41 : 477 / 503 = 94 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 42 : 416 / 504 = 82 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 43 : 317 / 503 = 63 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 44 : 395 / 504 = 78 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 45 : 416 / 504 = 82 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 46 : 367 / 504 = 72 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 47 : 286 / 503 = 56 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 48 : 382 / 503 = 75 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 49 : 396 / 499 = 79 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 50 : 377 / 502 = 75 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 51 : 457 / 503 = 90 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 52 : 459 / 504 = 91 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 53 : 421 / 497 = 84 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 54 : 444 / 480 = 92 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 55 : 387 / 504 = 76 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 56 : 408 / 503 = 81 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 57 : 461 / 504 = 91 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 58 : 472 / 499 = 94 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 59 : 483 / 503 = 96 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 60 : 412 / 479 = 86 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 61 : 357 / 484 = 73 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 62 : 384 / 487 = 78 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 63 : 396 / 489 = 80 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 64 : 363 / 488 = 74 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 65 : 432 / 490 = 88 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 66 : 280 / 488 = 57 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 67 : 332 / 490 = 67 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 68 : 313 / 490 = 63 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 69 : 399 / 490 = 81 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 70 : 207 / 490 = 42 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 71 : 66 / 490 = 13 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 72 : 185 / 488 = 37 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 73 : 177 / 486 = 36 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 74 : 330 / 481 = 68 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 75 : 127 / 488 = 26 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 76 : 323 / 489 = 66 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 77 : 255 / 488 = 52 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 78 : 366 / 488 = 74 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 79 : 426 / 490 = 86 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 80 : 370 / 489 = 75 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 81 : 248 / 491 = 50 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 82 : 257 / 491 = 52 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 83 : 130 / 489 = 26 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 84 : 301 / 489 = 61 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 85 : 340 / 489 = 69 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 86 : 380 / 491 = 77 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 87 : 394 / 492 = 80 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 88 : 357 / 491 = 72 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 89 : 339 / 492 = 68 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 90 : 173 / 490 = 35 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 91 : 353 / 482 = 73 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 92 : 334 / 490 = 68 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 93 : 291 / 487 = 59 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 94 : 385 / 489 = 78 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 95 : 384 / 490 = 78 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 96 : 441 / 491 = 89 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 97 : 465 / 490 = 94 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 98 : 435 / 491 = 88 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 99 : 434 / 491 = 88 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 100 : 424 / 491 = 86 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 101 : 408 / 491 = 83 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 102 : 263 / 492 = 53 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 103 : 392 / 492 = 79 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 104 : 263 / 491 = 53 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 105 : 289 / 491 = 58 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 106 : 261 / 492 = 53 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 107 : 413 / 491 = 84 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 108 : 337 / 492 = 68 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 109 : 338 / 490 = 68 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 110 : 318 / 491 = 64 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 111 : 410 / 492 = 83 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 112 : 452 / 492 = 91 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 113 : 402 / 491 = 81 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 114 : 348 / 491 = 70 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 115 : 436 / 492 = 88 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 116 : 385 / 491 = 78 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 117 : 418 / 492 = 84 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 118 : 370 / 490 = 75 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 119 : 444 / 492 = 90 %
[ Mon Jul 15 07:19:47 2024 ] Accuracy of 120 : 378 / 500 = 75 %
[ Mon Jul 15 07:19:47 2024 ] Training epoch: 41
[ Mon Jul 15 07:19:47 2024 ] 	Batch(0/6809) done. Loss: 0.0447  lr:0.010000
[ Mon Jul 15 07:20:10 2024 ] 	Batch(100/6809) done. Loss: 1.0788  lr:0.010000
[ Mon Jul 15 07:20:33 2024 ] 	Batch(200/6809) done. Loss: 0.1151  lr:0.010000
[ Mon Jul 15 07:20:56 2024 ] 	Batch(300/6809) done. Loss: 0.4182  lr:0.010000
[ Mon Jul 15 07:21:19 2024 ] 	Batch(400/6809) done. Loss: 0.9237  lr:0.010000
[ Mon Jul 15 07:21:42 2024 ] 
Training: Epoch [40/150], Step [499], Loss: 0.7247889637947083, Training Accuracy: 84.125
[ Mon Jul 15 07:21:42 2024 ] 	Batch(500/6809) done. Loss: 1.6030  lr:0.010000
[ Mon Jul 15 07:22:05 2024 ] 	Batch(600/6809) done. Loss: 0.1633  lr:0.010000
[ Mon Jul 15 07:22:28 2024 ] 	Batch(700/6809) done. Loss: 0.8240  lr:0.010000
[ Mon Jul 15 07:22:50 2024 ] 	Batch(800/6809) done. Loss: 0.6333  lr:0.010000
[ Mon Jul 15 07:23:13 2024 ] 	Batch(900/6809) done. Loss: 0.7310  lr:0.010000
[ Mon Jul 15 07:23:36 2024 ] 
Training: Epoch [40/150], Step [999], Loss: 1.3043725490570068, Training Accuracy: 84.1875
[ Mon Jul 15 07:23:36 2024 ] 	Batch(1000/6809) done. Loss: 0.4183  lr:0.010000
[ Mon Jul 15 07:24:00 2024 ] 	Batch(1100/6809) done. Loss: 0.9612  lr:0.010000
[ Mon Jul 15 07:24:24 2024 ] 	Batch(1200/6809) done. Loss: 0.4026  lr:0.010000
[ Mon Jul 15 07:24:47 2024 ] 	Batch(1300/6809) done. Loss: 0.7013  lr:0.010000
[ Mon Jul 15 07:25:10 2024 ] 	Batch(1400/6809) done. Loss: 0.0457  lr:0.010000
[ Mon Jul 15 07:25:33 2024 ] 
Training: Epoch [40/150], Step [1499], Loss: 0.2241249680519104, Training Accuracy: 84.52499999999999
[ Mon Jul 15 07:25:33 2024 ] 	Batch(1500/6809) done. Loss: 0.4209  lr:0.010000
[ Mon Jul 15 07:25:56 2024 ] 	Batch(1600/6809) done. Loss: 0.5536  lr:0.010000
[ Mon Jul 15 07:26:19 2024 ] 	Batch(1700/6809) done. Loss: 0.3789  lr:0.010000
[ Mon Jul 15 07:26:42 2024 ] 	Batch(1800/6809) done. Loss: 0.3575  lr:0.010000
[ Mon Jul 15 07:27:05 2024 ] 	Batch(1900/6809) done. Loss: 1.1942  lr:0.010000
[ Mon Jul 15 07:27:27 2024 ] 
Training: Epoch [40/150], Step [1999], Loss: 0.1692837029695511, Training Accuracy: 84.41875
[ Mon Jul 15 07:27:27 2024 ] 	Batch(2000/6809) done. Loss: 0.2142  lr:0.010000
[ Mon Jul 15 07:27:51 2024 ] 	Batch(2100/6809) done. Loss: 0.8411  lr:0.010000
[ Mon Jul 15 07:28:14 2024 ] 	Batch(2200/6809) done. Loss: 0.2354  lr:0.010000
[ Mon Jul 15 07:28:37 2024 ] 	Batch(2300/6809) done. Loss: 0.2497  lr:0.010000
[ Mon Jul 15 07:29:00 2024 ] 	Batch(2400/6809) done. Loss: 0.9445  lr:0.010000
[ Mon Jul 15 07:29:23 2024 ] 
Training: Epoch [40/150], Step [2499], Loss: 0.4715750515460968, Training Accuracy: 84.595
[ Mon Jul 15 07:29:23 2024 ] 	Batch(2500/6809) done. Loss: 0.1612  lr:0.010000
[ Mon Jul 15 07:29:46 2024 ] 	Batch(2600/6809) done. Loss: 0.0974  lr:0.010000
[ Mon Jul 15 07:30:09 2024 ] 	Batch(2700/6809) done. Loss: 0.2178  lr:0.010000
[ Mon Jul 15 07:30:32 2024 ] 	Batch(2800/6809) done. Loss: 0.3158  lr:0.010000
[ Mon Jul 15 07:30:55 2024 ] 	Batch(2900/6809) done. Loss: 0.6561  lr:0.010000
[ Mon Jul 15 07:31:18 2024 ] 
Training: Epoch [40/150], Step [2999], Loss: 1.2448383569717407, Training Accuracy: 84.40833333333333
[ Mon Jul 15 07:31:18 2024 ] 	Batch(3000/6809) done. Loss: 0.6819  lr:0.010000
[ Mon Jul 15 07:31:41 2024 ] 	Batch(3100/6809) done. Loss: 0.4433  lr:0.010000
[ Mon Jul 15 07:32:04 2024 ] 	Batch(3200/6809) done. Loss: 0.6666  lr:0.010000
[ Mon Jul 15 07:32:27 2024 ] 	Batch(3300/6809) done. Loss: 0.1407  lr:0.010000
[ Mon Jul 15 07:32:50 2024 ] 	Batch(3400/6809) done. Loss: 0.7649  lr:0.010000
[ Mon Jul 15 07:33:13 2024 ] 
Training: Epoch [40/150], Step [3499], Loss: 0.11826703697443008, Training Accuracy: 84.21428571428572
[ Mon Jul 15 07:33:13 2024 ] 	Batch(3500/6809) done. Loss: 0.4206  lr:0.010000
[ Mon Jul 15 07:33:36 2024 ] 	Batch(3600/6809) done. Loss: 0.4310  lr:0.010000
[ Mon Jul 15 07:33:59 2024 ] 	Batch(3700/6809) done. Loss: 0.2916  lr:0.010000
[ Mon Jul 15 07:34:22 2024 ] 	Batch(3800/6809) done. Loss: 1.0914  lr:0.010000
[ Mon Jul 15 07:34:45 2024 ] 	Batch(3900/6809) done. Loss: 0.5286  lr:0.010000
[ Mon Jul 15 07:35:08 2024 ] 
Training: Epoch [40/150], Step [3999], Loss: 0.5168716311454773, Training Accuracy: 84.25
[ Mon Jul 15 07:35:08 2024 ] 	Batch(4000/6809) done. Loss: 0.3001  lr:0.010000
[ Mon Jul 15 07:35:31 2024 ] 	Batch(4100/6809) done. Loss: 0.1845  lr:0.010000
[ Mon Jul 15 07:35:53 2024 ] 	Batch(4200/6809) done. Loss: 0.3970  lr:0.010000
[ Mon Jul 15 07:36:16 2024 ] 	Batch(4300/6809) done. Loss: 0.3819  lr:0.010000
[ Mon Jul 15 07:36:38 2024 ] 	Batch(4400/6809) done. Loss: 0.7473  lr:0.010000
[ Mon Jul 15 07:37:01 2024 ] 
Training: Epoch [40/150], Step [4499], Loss: 0.5596524477005005, Training Accuracy: 84.1888888888889
[ Mon Jul 15 07:37:01 2024 ] 	Batch(4500/6809) done. Loss: 0.8115  lr:0.010000
[ Mon Jul 15 07:37:23 2024 ] 	Batch(4600/6809) done. Loss: 0.2670  lr:0.010000
[ Mon Jul 15 07:37:46 2024 ] 	Batch(4700/6809) done. Loss: 0.3917  lr:0.010000
[ Mon Jul 15 07:38:10 2024 ] 	Batch(4800/6809) done. Loss: 0.6627  lr:0.010000
[ Mon Jul 15 07:38:33 2024 ] 	Batch(4900/6809) done. Loss: 0.3395  lr:0.010000
[ Mon Jul 15 07:38:56 2024 ] 
Training: Epoch [40/150], Step [4999], Loss: 1.261494517326355, Training Accuracy: 84.2675
[ Mon Jul 15 07:38:56 2024 ] 	Batch(5000/6809) done. Loss: 0.9743  lr:0.010000
[ Mon Jul 15 07:39:19 2024 ] 	Batch(5100/6809) done. Loss: 0.1245  lr:0.010000
[ Mon Jul 15 07:39:42 2024 ] 	Batch(5200/6809) done. Loss: 0.2565  lr:0.010000
[ Mon Jul 15 07:40:05 2024 ] 	Batch(5300/6809) done. Loss: 0.0775  lr:0.010000
[ Mon Jul 15 07:40:28 2024 ] 	Batch(5400/6809) done. Loss: 0.1846  lr:0.010000
[ Mon Jul 15 07:40:51 2024 ] 
Training: Epoch [40/150], Step [5499], Loss: 0.39511793851852417, Training Accuracy: 84.20909090909092
[ Mon Jul 15 07:40:51 2024 ] 	Batch(5500/6809) done. Loss: 0.3259  lr:0.010000
[ Mon Jul 15 07:41:15 2024 ] 	Batch(5600/6809) done. Loss: 0.2284  lr:0.010000
[ Mon Jul 15 07:41:38 2024 ] 	Batch(5700/6809) done. Loss: 0.4117  lr:0.010000
[ Mon Jul 15 07:42:01 2024 ] 	Batch(5800/6809) done. Loss: 0.4617  lr:0.010000
[ Mon Jul 15 07:42:24 2024 ] 	Batch(5900/6809) done. Loss: 0.9998  lr:0.010000
[ Mon Jul 15 07:42:46 2024 ] 
Training: Epoch [40/150], Step [5999], Loss: 0.4268113076686859, Training Accuracy: 84.3
[ Mon Jul 15 07:42:47 2024 ] 	Batch(6000/6809) done. Loss: 0.1263  lr:0.010000
[ Mon Jul 15 07:43:10 2024 ] 	Batch(6100/6809) done. Loss: 0.4098  lr:0.010000
[ Mon Jul 15 07:43:33 2024 ] 	Batch(6200/6809) done. Loss: 0.4998  lr:0.010000
[ Mon Jul 15 07:43:56 2024 ] 	Batch(6300/6809) done. Loss: 0.2733  lr:0.010000
[ Mon Jul 15 07:44:19 2024 ] 	Batch(6400/6809) done. Loss: 0.5880  lr:0.010000
[ Mon Jul 15 07:44:42 2024 ] 
Training: Epoch [40/150], Step [6499], Loss: 0.09842300415039062, Training Accuracy: 84.21923076923076
[ Mon Jul 15 07:44:42 2024 ] 	Batch(6500/6809) done. Loss: 0.2974  lr:0.010000
[ Mon Jul 15 07:45:05 2024 ] 	Batch(6600/6809) done. Loss: 0.2291  lr:0.010000
[ Mon Jul 15 07:45:28 2024 ] 	Batch(6700/6809) done. Loss: 0.3569  lr:0.010000
[ Mon Jul 15 07:45:51 2024 ] 	Batch(6800/6809) done. Loss: 0.4607  lr:0.010000
[ Mon Jul 15 07:45:53 2024 ] 	Mean training loss: 0.5072.
[ Mon Jul 15 07:45:53 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 07:45:53 2024 ] Training epoch: 42
[ Mon Jul 15 07:45:54 2024 ] 	Batch(0/6809) done. Loss: 0.3157  lr:0.010000
[ Mon Jul 15 07:46:17 2024 ] 	Batch(100/6809) done. Loss: 0.3236  lr:0.010000
[ Mon Jul 15 07:46:39 2024 ] 	Batch(200/6809) done. Loss: 0.0148  lr:0.010000
[ Mon Jul 15 07:47:02 2024 ] 	Batch(300/6809) done. Loss: 0.2157  lr:0.010000
[ Mon Jul 15 07:47:25 2024 ] 	Batch(400/6809) done. Loss: 0.3243  lr:0.010000
[ Mon Jul 15 07:47:48 2024 ] 
Training: Epoch [41/150], Step [499], Loss: 0.23259954154491425, Training Accuracy: 85.2
[ Mon Jul 15 07:47:48 2024 ] 	Batch(500/6809) done. Loss: 0.2369  lr:0.010000
[ Mon Jul 15 07:48:10 2024 ] 	Batch(600/6809) done. Loss: 0.1286  lr:0.010000
[ Mon Jul 15 07:48:33 2024 ] 	Batch(700/6809) done. Loss: 0.4606  lr:0.010000
[ Mon Jul 15 07:48:56 2024 ] 	Batch(800/6809) done. Loss: 0.3100  lr:0.010000
[ Mon Jul 15 07:49:18 2024 ] 	Batch(900/6809) done. Loss: 0.3216  lr:0.010000
[ Mon Jul 15 07:49:41 2024 ] 
Training: Epoch [41/150], Step [999], Loss: 0.7116101980209351, Training Accuracy: 84.96249999999999
[ Mon Jul 15 07:49:41 2024 ] 	Batch(1000/6809) done. Loss: 0.2987  lr:0.010000
[ Mon Jul 15 07:50:04 2024 ] 	Batch(1100/6809) done. Loss: 0.2268  lr:0.010000
[ Mon Jul 15 07:50:27 2024 ] 	Batch(1200/6809) done. Loss: 0.3881  lr:0.010000
[ Mon Jul 15 07:50:50 2024 ] 	Batch(1300/6809) done. Loss: 0.2424  lr:0.010000
[ Mon Jul 15 07:51:13 2024 ] 	Batch(1400/6809) done. Loss: 0.2350  lr:0.010000
[ Mon Jul 15 07:51:35 2024 ] 
Training: Epoch [41/150], Step [1499], Loss: 0.6432306170463562, Training Accuracy: 84.575
[ Mon Jul 15 07:51:36 2024 ] 	Batch(1500/6809) done. Loss: 1.1004  lr:0.010000
[ Mon Jul 15 07:51:59 2024 ] 	Batch(1600/6809) done. Loss: 0.6736  lr:0.010000
[ Mon Jul 15 07:52:22 2024 ] 	Batch(1700/6809) done. Loss: 0.4519  lr:0.010000
[ Mon Jul 15 07:52:46 2024 ] 	Batch(1800/6809) done. Loss: 0.5320  lr:0.010000
[ Mon Jul 15 07:53:09 2024 ] 	Batch(1900/6809) done. Loss: 0.0833  lr:0.010000
[ Mon Jul 15 07:53:31 2024 ] 
Training: Epoch [41/150], Step [1999], Loss: 0.4595991373062134, Training Accuracy: 84.7375
[ Mon Jul 15 07:53:32 2024 ] 	Batch(2000/6809) done. Loss: 0.1077  lr:0.010000
[ Mon Jul 15 07:53:55 2024 ] 	Batch(2100/6809) done. Loss: 0.8645  lr:0.010000
[ Mon Jul 15 07:54:18 2024 ] 	Batch(2200/6809) done. Loss: 0.1864  lr:0.010000
[ Mon Jul 15 07:54:41 2024 ] 	Batch(2300/6809) done. Loss: 0.0406  lr:0.010000
[ Mon Jul 15 07:55:03 2024 ] 	Batch(2400/6809) done. Loss: 0.9032  lr:0.010000
[ Mon Jul 15 07:55:26 2024 ] 
Training: Epoch [41/150], Step [2499], Loss: 0.1263856440782547, Training Accuracy: 84.8
[ Mon Jul 15 07:55:26 2024 ] 	Batch(2500/6809) done. Loss: 2.1994  lr:0.010000
[ Mon Jul 15 07:55:48 2024 ] 	Batch(2600/6809) done. Loss: 1.6534  lr:0.010000
[ Mon Jul 15 07:56:11 2024 ] 	Batch(2700/6809) done. Loss: 0.3451  lr:0.010000
[ Mon Jul 15 07:56:34 2024 ] 	Batch(2800/6809) done. Loss: 0.1184  lr:0.010000
[ Mon Jul 15 07:56:56 2024 ] 	Batch(2900/6809) done. Loss: 0.3783  lr:0.010000
[ Mon Jul 15 07:57:18 2024 ] 
Training: Epoch [41/150], Step [2999], Loss: 1.0325698852539062, Training Accuracy: 84.72083333333333
[ Mon Jul 15 07:57:19 2024 ] 	Batch(3000/6809) done. Loss: 0.4305  lr:0.010000
[ Mon Jul 15 07:57:41 2024 ] 	Batch(3100/6809) done. Loss: 0.5660  lr:0.010000
[ Mon Jul 15 07:58:04 2024 ] 	Batch(3200/6809) done. Loss: 0.7002  lr:0.010000
[ Mon Jul 15 07:58:26 2024 ] 	Batch(3300/6809) done. Loss: 1.3028  lr:0.010000
[ Mon Jul 15 07:58:49 2024 ] 	Batch(3400/6809) done. Loss: 0.1353  lr:0.010000
[ Mon Jul 15 07:59:11 2024 ] 
Training: Epoch [41/150], Step [3499], Loss: 0.456432580947876, Training Accuracy: 84.53214285714286
[ Mon Jul 15 07:59:12 2024 ] 	Batch(3500/6809) done. Loss: 0.3204  lr:0.010000
[ Mon Jul 15 07:59:34 2024 ] 	Batch(3600/6809) done. Loss: 0.3270  lr:0.010000
[ Mon Jul 15 07:59:57 2024 ] 	Batch(3700/6809) done. Loss: 0.2739  lr:0.010000
[ Mon Jul 15 08:00:19 2024 ] 	Batch(3800/6809) done. Loss: 0.1770  lr:0.010000
[ Mon Jul 15 08:00:42 2024 ] 	Batch(3900/6809) done. Loss: 0.9360  lr:0.010000
[ Mon Jul 15 08:01:05 2024 ] 
Training: Epoch [41/150], Step [3999], Loss: 0.13758093118667603, Training Accuracy: 84.375
[ Mon Jul 15 08:01:05 2024 ] 	Batch(4000/6809) done. Loss: 1.0510  lr:0.010000
[ Mon Jul 15 08:01:28 2024 ] 	Batch(4100/6809) done. Loss: 0.3317  lr:0.010000
[ Mon Jul 15 08:01:51 2024 ] 	Batch(4200/6809) done. Loss: 0.4391  lr:0.010000
[ Mon Jul 15 08:02:14 2024 ] 	Batch(4300/6809) done. Loss: 0.0438  lr:0.010000
[ Mon Jul 15 08:02:37 2024 ] 	Batch(4400/6809) done. Loss: 1.0975  lr:0.010000
[ Mon Jul 15 08:03:00 2024 ] 
Training: Epoch [41/150], Step [4499], Loss: 0.046642545610666275, Training Accuracy: 84.42777777777778
[ Mon Jul 15 08:03:01 2024 ] 	Batch(4500/6809) done. Loss: 0.1335  lr:0.010000
[ Mon Jul 15 08:03:24 2024 ] 	Batch(4600/6809) done. Loss: 1.0072  lr:0.010000
[ Mon Jul 15 08:03:47 2024 ] 	Batch(4700/6809) done. Loss: 1.2146  lr:0.010000
[ Mon Jul 15 08:04:10 2024 ] 	Batch(4800/6809) done. Loss: 0.6029  lr:0.010000
[ Mon Jul 15 08:04:33 2024 ] 	Batch(4900/6809) done. Loss: 0.6118  lr:0.010000
[ Mon Jul 15 08:04:55 2024 ] 
Training: Epoch [41/150], Step [4999], Loss: 0.195277601480484, Training Accuracy: 84.4025
[ Mon Jul 15 08:04:56 2024 ] 	Batch(5000/6809) done. Loss: 0.6265  lr:0.010000
[ Mon Jul 15 08:05:18 2024 ] 	Batch(5100/6809) done. Loss: 0.2848  lr:0.010000
[ Mon Jul 15 08:05:41 2024 ] 	Batch(5200/6809) done. Loss: 0.3095  lr:0.010000
[ Mon Jul 15 08:06:04 2024 ] 	Batch(5300/6809) done. Loss: 0.0290  lr:0.010000
[ Mon Jul 15 08:06:27 2024 ] 	Batch(5400/6809) done. Loss: 0.5105  lr:0.010000
[ Mon Jul 15 08:06:49 2024 ] 
Training: Epoch [41/150], Step [5499], Loss: 0.9543626308441162, Training Accuracy: 84.39545454545454
[ Mon Jul 15 08:06:49 2024 ] 	Batch(5500/6809) done. Loss: 0.5851  lr:0.010000
[ Mon Jul 15 08:07:12 2024 ] 	Batch(5600/6809) done. Loss: 0.4373  lr:0.010000
[ Mon Jul 15 08:07:35 2024 ] 	Batch(5700/6809) done. Loss: 0.8324  lr:0.010000
[ Mon Jul 15 08:07:58 2024 ] 	Batch(5800/6809) done. Loss: 0.1003  lr:0.010000
[ Mon Jul 15 08:08:20 2024 ] 	Batch(5900/6809) done. Loss: 1.0361  lr:0.010000
[ Mon Jul 15 08:08:43 2024 ] 
Training: Epoch [41/150], Step [5999], Loss: 0.6256643533706665, Training Accuracy: 84.2
[ Mon Jul 15 08:08:43 2024 ] 	Batch(6000/6809) done. Loss: 0.4223  lr:0.010000
[ Mon Jul 15 08:09:06 2024 ] 	Batch(6100/6809) done. Loss: 0.5840  lr:0.010000
[ Mon Jul 15 08:09:29 2024 ] 	Batch(6200/6809) done. Loss: 0.4322  lr:0.010000
[ Mon Jul 15 08:09:52 2024 ] 	Batch(6300/6809) done. Loss: 0.7935  lr:0.010000
[ Mon Jul 15 08:10:15 2024 ] 	Batch(6400/6809) done. Loss: 0.3452  lr:0.010000
[ Mon Jul 15 08:10:38 2024 ] 
Training: Epoch [41/150], Step [6499], Loss: 0.1404053419828415, Training Accuracy: 84.19807692307693
[ Mon Jul 15 08:10:38 2024 ] 	Batch(6500/6809) done. Loss: 1.0122  lr:0.010000
[ Mon Jul 15 08:11:01 2024 ] 	Batch(6600/6809) done. Loss: 1.4148  lr:0.010000
[ Mon Jul 15 08:11:24 2024 ] 	Batch(6700/6809) done. Loss: 0.2934  lr:0.010000
[ Mon Jul 15 08:11:47 2024 ] 	Batch(6800/6809) done. Loss: 0.6684  lr:0.010000
[ Mon Jul 15 08:11:49 2024 ] 	Mean training loss: 0.5045.
[ Mon Jul 15 08:11:49 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 08:11:49 2024 ] Training epoch: 43
[ Mon Jul 15 08:11:50 2024 ] 	Batch(0/6809) done. Loss: 0.1258  lr:0.010000
[ Mon Jul 15 08:12:13 2024 ] 	Batch(100/6809) done. Loss: 0.6236  lr:0.010000
[ Mon Jul 15 08:12:36 2024 ] 	Batch(200/6809) done. Loss: 0.6551  lr:0.010000
[ Mon Jul 15 08:12:59 2024 ] 	Batch(300/6809) done. Loss: 1.0307  lr:0.010000
[ Mon Jul 15 08:13:22 2024 ] 	Batch(400/6809) done. Loss: 0.3290  lr:0.010000
[ Mon Jul 15 08:13:45 2024 ] 
Training: Epoch [42/150], Step [499], Loss: 0.30906015634536743, Training Accuracy: 86.325
[ Mon Jul 15 08:13:45 2024 ] 	Batch(500/6809) done. Loss: 0.2042  lr:0.010000
[ Mon Jul 15 08:14:07 2024 ] 	Batch(600/6809) done. Loss: 0.5778  lr:0.010000
[ Mon Jul 15 08:14:30 2024 ] 	Batch(700/6809) done. Loss: 0.1043  lr:0.010000
[ Mon Jul 15 08:14:53 2024 ] 	Batch(800/6809) done. Loss: 0.1048  lr:0.010000
[ Mon Jul 15 08:15:16 2024 ] 	Batch(900/6809) done. Loss: 0.5984  lr:0.010000
[ Mon Jul 15 08:15:40 2024 ] 
Training: Epoch [42/150], Step [999], Loss: 0.08615896105766296, Training Accuracy: 85.6125
[ Mon Jul 15 08:15:40 2024 ] 	Batch(1000/6809) done. Loss: 0.7210  lr:0.010000
[ Mon Jul 15 08:16:03 2024 ] 	Batch(1100/6809) done. Loss: 0.4596  lr:0.010000
[ Mon Jul 15 08:16:27 2024 ] 	Batch(1200/6809) done. Loss: 1.6694  lr:0.010000
[ Mon Jul 15 08:16:50 2024 ] 	Batch(1300/6809) done. Loss: 1.1889  lr:0.010000
[ Mon Jul 15 08:17:12 2024 ] 	Batch(1400/6809) done. Loss: 0.3623  lr:0.010000
[ Mon Jul 15 08:17:35 2024 ] 
Training: Epoch [42/150], Step [1499], Loss: 0.12703122198581696, Training Accuracy: 85.65
[ Mon Jul 15 08:17:35 2024 ] 	Batch(1500/6809) done. Loss: 0.5040  lr:0.010000
[ Mon Jul 15 08:17:58 2024 ] 	Batch(1600/6809) done. Loss: 0.4103  lr:0.010000
[ Mon Jul 15 08:18:20 2024 ] 	Batch(1700/6809) done. Loss: 0.4212  lr:0.010000
[ Mon Jul 15 08:18:43 2024 ] 	Batch(1800/6809) done. Loss: 0.1219  lr:0.010000
[ Mon Jul 15 08:19:06 2024 ] 	Batch(1900/6809) done. Loss: 1.3365  lr:0.010000
[ Mon Jul 15 08:19:28 2024 ] 
Training: Epoch [42/150], Step [1999], Loss: 1.1432909965515137, Training Accuracy: 85.16875
[ Mon Jul 15 08:19:29 2024 ] 	Batch(2000/6809) done. Loss: 0.4626  lr:0.010000
[ Mon Jul 15 08:19:51 2024 ] 	Batch(2100/6809) done. Loss: 0.2103  lr:0.010000
[ Mon Jul 15 08:20:14 2024 ] 	Batch(2200/6809) done. Loss: 0.4404  lr:0.010000
[ Mon Jul 15 08:20:37 2024 ] 	Batch(2300/6809) done. Loss: 0.6284  lr:0.010000
[ Mon Jul 15 08:20:59 2024 ] 	Batch(2400/6809) done. Loss: 0.4754  lr:0.010000
[ Mon Jul 15 08:21:22 2024 ] 
Training: Epoch [42/150], Step [2499], Loss: 1.0029288530349731, Training Accuracy: 85.055
[ Mon Jul 15 08:21:22 2024 ] 	Batch(2500/6809) done. Loss: 0.9090  lr:0.010000
[ Mon Jul 15 08:21:45 2024 ] 	Batch(2600/6809) done. Loss: 1.0093  lr:0.010000
[ Mon Jul 15 08:22:07 2024 ] 	Batch(2700/6809) done. Loss: 0.3288  lr:0.010000
[ Mon Jul 15 08:22:30 2024 ] 	Batch(2800/6809) done. Loss: 0.2678  lr:0.010000
[ Mon Jul 15 08:22:53 2024 ] 	Batch(2900/6809) done. Loss: 0.4251  lr:0.010000
[ Mon Jul 15 08:23:16 2024 ] 
Training: Epoch [42/150], Step [2999], Loss: 0.8825951218605042, Training Accuracy: 84.92083333333333
[ Mon Jul 15 08:23:16 2024 ] 	Batch(3000/6809) done. Loss: 0.1121  lr:0.010000
[ Mon Jul 15 08:23:39 2024 ] 	Batch(3100/6809) done. Loss: 1.0114  lr:0.010000
[ Mon Jul 15 08:24:02 2024 ] 	Batch(3200/6809) done. Loss: 0.6319  lr:0.010000
[ Mon Jul 15 08:24:25 2024 ] 	Batch(3300/6809) done. Loss: 0.0476  lr:0.010000
[ Mon Jul 15 08:24:48 2024 ] 	Batch(3400/6809) done. Loss: 0.1760  lr:0.010000
[ Mon Jul 15 08:25:10 2024 ] 
Training: Epoch [42/150], Step [3499], Loss: 0.48342883586883545, Training Accuracy: 84.81428571428572
[ Mon Jul 15 08:25:10 2024 ] 	Batch(3500/6809) done. Loss: 1.1328  lr:0.010000
[ Mon Jul 15 08:25:33 2024 ] 	Batch(3600/6809) done. Loss: 0.4778  lr:0.010000
[ Mon Jul 15 08:25:56 2024 ] 	Batch(3700/6809) done. Loss: 0.5426  lr:0.010000
[ Mon Jul 15 08:26:19 2024 ] 	Batch(3800/6809) done. Loss: 0.2803  lr:0.010000
[ Mon Jul 15 08:26:41 2024 ] 	Batch(3900/6809) done. Loss: 1.2508  lr:0.010000
[ Mon Jul 15 08:27:04 2024 ] 
Training: Epoch [42/150], Step [3999], Loss: 0.2034570872783661, Training Accuracy: 84.778125
[ Mon Jul 15 08:27:04 2024 ] 	Batch(4000/6809) done. Loss: 1.1621  lr:0.010000
[ Mon Jul 15 08:27:28 2024 ] 	Batch(4100/6809) done. Loss: 0.5561  lr:0.010000
[ Mon Jul 15 08:27:50 2024 ] 	Batch(4200/6809) done. Loss: 0.3387  lr:0.010000
[ Mon Jul 15 08:28:13 2024 ] 	Batch(4300/6809) done. Loss: 0.4543  lr:0.010000
[ Mon Jul 15 08:28:36 2024 ] 	Batch(4400/6809) done. Loss: 1.1751  lr:0.010000
[ Mon Jul 15 08:28:58 2024 ] 
Training: Epoch [42/150], Step [4499], Loss: 0.3243143558502197, Training Accuracy: 84.65833333333333
[ Mon Jul 15 08:28:58 2024 ] 	Batch(4500/6809) done. Loss: 0.2922  lr:0.010000
[ Mon Jul 15 08:29:21 2024 ] 	Batch(4600/6809) done. Loss: 1.0008  lr:0.010000
[ Mon Jul 15 08:29:44 2024 ] 	Batch(4700/6809) done. Loss: 0.0171  lr:0.010000
[ Mon Jul 15 08:30:07 2024 ] 	Batch(4800/6809) done. Loss: 0.9575  lr:0.010000
[ Mon Jul 15 08:30:30 2024 ] 	Batch(4900/6809) done. Loss: 0.0582  lr:0.010000
[ Mon Jul 15 08:30:53 2024 ] 
Training: Epoch [42/150], Step [4999], Loss: 0.06240643933415413, Training Accuracy: 84.635
[ Mon Jul 15 08:30:53 2024 ] 	Batch(5000/6809) done. Loss: 0.8386  lr:0.010000
[ Mon Jul 15 08:31:17 2024 ] 	Batch(5100/6809) done. Loss: 0.6101  lr:0.010000
[ Mon Jul 15 08:31:40 2024 ] 	Batch(5200/6809) done. Loss: 0.3497  lr:0.010000
[ Mon Jul 15 08:32:03 2024 ] 	Batch(5300/6809) done. Loss: 0.8746  lr:0.010000
[ Mon Jul 15 08:32:26 2024 ] 	Batch(5400/6809) done. Loss: 0.2801  lr:0.010000
[ Mon Jul 15 08:32:48 2024 ] 
Training: Epoch [42/150], Step [5499], Loss: 0.5499118566513062, Training Accuracy: 84.55227272727272
[ Mon Jul 15 08:32:48 2024 ] 	Batch(5500/6809) done. Loss: 0.8062  lr:0.010000
[ Mon Jul 15 08:33:11 2024 ] 	Batch(5600/6809) done. Loss: 0.8974  lr:0.010000
[ Mon Jul 15 08:33:35 2024 ] 	Batch(5700/6809) done. Loss: 0.1273  lr:0.010000
[ Mon Jul 15 08:33:58 2024 ] 	Batch(5800/6809) done. Loss: 0.5555  lr:0.010000
[ Mon Jul 15 08:34:21 2024 ] 	Batch(5900/6809) done. Loss: 0.7086  lr:0.010000
[ Mon Jul 15 08:34:44 2024 ] 
Training: Epoch [42/150], Step [5999], Loss: 0.7515218257904053, Training Accuracy: 84.5375
[ Mon Jul 15 08:34:45 2024 ] 	Batch(6000/6809) done. Loss: 0.7847  lr:0.010000
[ Mon Jul 15 08:35:07 2024 ] 	Batch(6100/6809) done. Loss: 1.1001  lr:0.010000
[ Mon Jul 15 08:35:30 2024 ] 	Batch(6200/6809) done. Loss: 0.4923  lr:0.010000
[ Mon Jul 15 08:35:53 2024 ] 	Batch(6300/6809) done. Loss: 0.1794  lr:0.010000
[ Mon Jul 15 08:36:16 2024 ] 	Batch(6400/6809) done. Loss: 0.1838  lr:0.010000
[ Mon Jul 15 08:36:38 2024 ] 
Training: Epoch [42/150], Step [6499], Loss: 0.05776175856590271, Training Accuracy: 84.53846153846155
[ Mon Jul 15 08:36:38 2024 ] 	Batch(6500/6809) done. Loss: 0.1390  lr:0.010000
[ Mon Jul 15 08:37:01 2024 ] 	Batch(6600/6809) done. Loss: 1.4055  lr:0.010000
[ Mon Jul 15 08:37:24 2024 ] 	Batch(6700/6809) done. Loss: 0.1758  lr:0.010000
[ Mon Jul 15 08:37:46 2024 ] 	Batch(6800/6809) done. Loss: 0.5508  lr:0.010000
[ Mon Jul 15 08:37:48 2024 ] 	Mean training loss: 0.4861.
[ Mon Jul 15 08:37:48 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 08:37:48 2024 ] Training epoch: 44
[ Mon Jul 15 08:37:49 2024 ] 	Batch(0/6809) done. Loss: 0.6841  lr:0.010000
[ Mon Jul 15 08:38:12 2024 ] 	Batch(100/6809) done. Loss: 0.2026  lr:0.010000
[ Mon Jul 15 08:38:35 2024 ] 	Batch(200/6809) done. Loss: 0.3195  lr:0.010000
[ Mon Jul 15 08:38:58 2024 ] 	Batch(300/6809) done. Loss: 0.0295  lr:0.010000
[ Mon Jul 15 08:39:20 2024 ] 	Batch(400/6809) done. Loss: 0.7443  lr:0.010000
[ Mon Jul 15 08:39:43 2024 ] 
Training: Epoch [43/150], Step [499], Loss: 0.5700954794883728, Training Accuracy: 85.975
[ Mon Jul 15 08:39:43 2024 ] 	Batch(500/6809) done. Loss: 1.0131  lr:0.010000
[ Mon Jul 15 08:40:05 2024 ] 	Batch(600/6809) done. Loss: 0.1902  lr:0.010000
[ Mon Jul 15 08:40:28 2024 ] 	Batch(700/6809) done. Loss: 0.2241  lr:0.010000
[ Mon Jul 15 08:40:51 2024 ] 	Batch(800/6809) done. Loss: 0.0071  lr:0.010000
[ Mon Jul 15 08:41:14 2024 ] 	Batch(900/6809) done. Loss: 0.6632  lr:0.010000
[ Mon Jul 15 08:41:37 2024 ] 
Training: Epoch [43/150], Step [999], Loss: 0.5052793622016907, Training Accuracy: 86.1125
[ Mon Jul 15 08:41:37 2024 ] 	Batch(1000/6809) done. Loss: 0.1190  lr:0.010000
[ Mon Jul 15 08:42:00 2024 ] 	Batch(1100/6809) done. Loss: 0.7082  lr:0.010000
[ Mon Jul 15 08:42:23 2024 ] 	Batch(1200/6809) done. Loss: 0.2477  lr:0.010000
[ Mon Jul 15 08:42:46 2024 ] 	Batch(1300/6809) done. Loss: 0.0932  lr:0.010000
[ Mon Jul 15 08:43:09 2024 ] 	Batch(1400/6809) done. Loss: 0.3227  lr:0.010000
[ Mon Jul 15 08:43:32 2024 ] 
Training: Epoch [43/150], Step [1499], Loss: 0.45149102807044983, Training Accuracy: 86.38333333333334
[ Mon Jul 15 08:43:32 2024 ] 	Batch(1500/6809) done. Loss: 0.5311  lr:0.010000
[ Mon Jul 15 08:43:55 2024 ] 	Batch(1600/6809) done. Loss: 0.2184  lr:0.010000
[ Mon Jul 15 08:44:18 2024 ] 	Batch(1700/6809) done. Loss: 0.2677  lr:0.010000
[ Mon Jul 15 08:44:41 2024 ] 	Batch(1800/6809) done. Loss: 0.5329  lr:0.010000
[ Mon Jul 15 08:45:04 2024 ] 	Batch(1900/6809) done. Loss: 0.4140  lr:0.010000
[ Mon Jul 15 08:45:27 2024 ] 
Training: Epoch [43/150], Step [1999], Loss: 0.8278164863586426, Training Accuracy: 85.85625
[ Mon Jul 15 08:45:27 2024 ] 	Batch(2000/6809) done. Loss: 0.3066  lr:0.010000
[ Mon Jul 15 08:45:50 2024 ] 	Batch(2100/6809) done. Loss: 0.6923  lr:0.010000
[ Mon Jul 15 08:46:13 2024 ] 	Batch(2200/6809) done. Loss: 1.1267  lr:0.010000
[ Mon Jul 15 08:46:36 2024 ] 	Batch(2300/6809) done. Loss: 0.0620  lr:0.010000
[ Mon Jul 15 08:46:59 2024 ] 	Batch(2400/6809) done. Loss: 0.7887  lr:0.010000
[ Mon Jul 15 08:47:21 2024 ] 
Training: Epoch [43/150], Step [2499], Loss: 0.2274586409330368, Training Accuracy: 85.795
[ Mon Jul 15 08:47:22 2024 ] 	Batch(2500/6809) done. Loss: 0.0457  lr:0.010000
[ Mon Jul 15 08:47:44 2024 ] 	Batch(2600/6809) done. Loss: 0.0439  lr:0.010000
[ Mon Jul 15 08:48:07 2024 ] 	Batch(2700/6809) done. Loss: 0.2028  lr:0.010000
[ Mon Jul 15 08:48:29 2024 ] 	Batch(2800/6809) done. Loss: 0.4721  lr:0.010000
[ Mon Jul 15 08:48:52 2024 ] 	Batch(2900/6809) done. Loss: 0.0721  lr:0.010000
[ Mon Jul 15 08:49:14 2024 ] 
Training: Epoch [43/150], Step [2999], Loss: 0.3062633275985718, Training Accuracy: 85.44166666666668
[ Mon Jul 15 08:49:14 2024 ] 	Batch(3000/6809) done. Loss: 0.2614  lr:0.010000
[ Mon Jul 15 08:49:37 2024 ] 	Batch(3100/6809) done. Loss: 0.1601  lr:0.010000
[ Mon Jul 15 08:49:59 2024 ] 	Batch(3200/6809) done. Loss: 2.1272  lr:0.010000
[ Mon Jul 15 08:50:22 2024 ] 	Batch(3300/6809) done. Loss: 0.1229  lr:0.010000
[ Mon Jul 15 08:50:44 2024 ] 	Batch(3400/6809) done. Loss: 0.7883  lr:0.010000
[ Mon Jul 15 08:51:07 2024 ] 
Training: Epoch [43/150], Step [3499], Loss: 0.535680890083313, Training Accuracy: 85.13928571428572
[ Mon Jul 15 08:51:07 2024 ] 	Batch(3500/6809) done. Loss: 0.5482  lr:0.010000
[ Mon Jul 15 08:51:30 2024 ] 	Batch(3600/6809) done. Loss: 0.2766  lr:0.010000
[ Mon Jul 15 08:51:53 2024 ] 	Batch(3700/6809) done. Loss: 0.2486  lr:0.010000
[ Mon Jul 15 08:52:15 2024 ] 	Batch(3800/6809) done. Loss: 0.1416  lr:0.010000
[ Mon Jul 15 08:52:38 2024 ] 	Batch(3900/6809) done. Loss: 0.9198  lr:0.010000
[ Mon Jul 15 08:53:02 2024 ] 
Training: Epoch [43/150], Step [3999], Loss: 0.5505083203315735, Training Accuracy: 85.00937499999999
[ Mon Jul 15 08:53:02 2024 ] 	Batch(4000/6809) done. Loss: 0.1119  lr:0.010000
[ Mon Jul 15 08:53:25 2024 ] 	Batch(4100/6809) done. Loss: 0.0927  lr:0.010000
[ Mon Jul 15 08:53:48 2024 ] 	Batch(4200/6809) done. Loss: 0.0276  lr:0.010000
[ Mon Jul 15 08:54:11 2024 ] 	Batch(4300/6809) done. Loss: 0.8894  lr:0.010000
[ Mon Jul 15 08:54:35 2024 ] 	Batch(4400/6809) done. Loss: 0.3403  lr:0.010000
[ Mon Jul 15 08:54:58 2024 ] 
Training: Epoch [43/150], Step [4499], Loss: 1.4200282096862793, Training Accuracy: 84.96111111111111
[ Mon Jul 15 08:54:59 2024 ] 	Batch(4500/6809) done. Loss: 0.3790  lr:0.010000
[ Mon Jul 15 08:55:22 2024 ] 	Batch(4600/6809) done. Loss: 0.1183  lr:0.010000
[ Mon Jul 15 08:55:45 2024 ] 	Batch(4700/6809) done. Loss: 1.7183  lr:0.010000
[ Mon Jul 15 08:56:08 2024 ] 	Batch(4800/6809) done. Loss: 1.3896  lr:0.010000
[ Mon Jul 15 08:56:31 2024 ] 	Batch(4900/6809) done. Loss: 0.6885  lr:0.010000
[ Mon Jul 15 08:56:54 2024 ] 
Training: Epoch [43/150], Step [4999], Loss: 0.3484196960926056, Training Accuracy: 84.8575
[ Mon Jul 15 08:56:54 2024 ] 	Batch(5000/6809) done. Loss: 0.2481  lr:0.010000
[ Mon Jul 15 08:57:17 2024 ] 	Batch(5100/6809) done. Loss: 0.3648  lr:0.010000
[ Mon Jul 15 08:57:40 2024 ] 	Batch(5200/6809) done. Loss: 0.4658  lr:0.010000
[ Mon Jul 15 08:58:03 2024 ] 	Batch(5300/6809) done. Loss: 0.2243  lr:0.010000
[ Mon Jul 15 08:58:26 2024 ] 	Batch(5400/6809) done. Loss: 1.0683  lr:0.010000
[ Mon Jul 15 08:58:49 2024 ] 
Training: Epoch [43/150], Step [5499], Loss: 0.14525310695171356, Training Accuracy: 84.84318181818182
[ Mon Jul 15 08:58:49 2024 ] 	Batch(5500/6809) done. Loss: 0.3077  lr:0.010000
[ Mon Jul 15 08:59:12 2024 ] 	Batch(5600/6809) done. Loss: 0.7326  lr:0.010000
[ Mon Jul 15 08:59:35 2024 ] 	Batch(5700/6809) done. Loss: 0.3178  lr:0.010000
[ Mon Jul 15 08:59:58 2024 ] 	Batch(5800/6809) done. Loss: 1.0103  lr:0.010000
[ Mon Jul 15 09:00:21 2024 ] 	Batch(5900/6809) done. Loss: 0.3634  lr:0.010000
[ Mon Jul 15 09:00:44 2024 ] 
Training: Epoch [43/150], Step [5999], Loss: 0.20521415770053864, Training Accuracy: 84.74166666666667
[ Mon Jul 15 09:00:44 2024 ] 	Batch(6000/6809) done. Loss: 0.3417  lr:0.010000
[ Mon Jul 15 09:01:07 2024 ] 	Batch(6100/6809) done. Loss: 0.0788  lr:0.010000
[ Mon Jul 15 09:01:30 2024 ] 	Batch(6200/6809) done. Loss: 0.3542  lr:0.010000
[ Mon Jul 15 09:01:53 2024 ] 	Batch(6300/6809) done. Loss: 0.6621  lr:0.010000
[ Mon Jul 15 09:02:16 2024 ] 	Batch(6400/6809) done. Loss: 0.0568  lr:0.010000
[ Mon Jul 15 09:02:39 2024 ] 
Training: Epoch [43/150], Step [6499], Loss: 0.8497620820999146, Training Accuracy: 84.71346153846154
[ Mon Jul 15 09:02:39 2024 ] 	Batch(6500/6809) done. Loss: 0.4171  lr:0.010000
[ Mon Jul 15 09:03:02 2024 ] 	Batch(6600/6809) done. Loss: 0.6303  lr:0.010000
[ Mon Jul 15 09:03:25 2024 ] 	Batch(6700/6809) done. Loss: 0.1144  lr:0.010000
[ Mon Jul 15 09:03:48 2024 ] 	Batch(6800/6809) done. Loss: 0.4457  lr:0.010000
[ Mon Jul 15 09:03:50 2024 ] 	Mean training loss: 0.4926.
[ Mon Jul 15 09:03:50 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 09:03:50 2024 ] Training epoch: 45
[ Mon Jul 15 09:03:51 2024 ] 	Batch(0/6809) done. Loss: 0.1525  lr:0.010000
[ Mon Jul 15 09:04:13 2024 ] 	Batch(100/6809) done. Loss: 0.6532  lr:0.010000
[ Mon Jul 15 09:04:36 2024 ] 	Batch(200/6809) done. Loss: 0.6639  lr:0.010000
[ Mon Jul 15 09:04:59 2024 ] 	Batch(300/6809) done. Loss: 0.5814  lr:0.010000
[ Mon Jul 15 09:05:22 2024 ] 	Batch(400/6809) done. Loss: 0.6785  lr:0.010000
[ Mon Jul 15 09:05:44 2024 ] 
Training: Epoch [44/150], Step [499], Loss: 0.5680808424949646, Training Accuracy: 85.9
[ Mon Jul 15 09:05:44 2024 ] 	Batch(500/6809) done. Loss: 0.4931  lr:0.010000
[ Mon Jul 15 09:06:07 2024 ] 	Batch(600/6809) done. Loss: 0.4710  lr:0.010000
[ Mon Jul 15 09:06:31 2024 ] 	Batch(700/6809) done. Loss: 0.3476  lr:0.010000
[ Mon Jul 15 09:06:55 2024 ] 	Batch(800/6809) done. Loss: 0.1339  lr:0.010000
[ Mon Jul 15 09:07:17 2024 ] 	Batch(900/6809) done. Loss: 0.9893  lr:0.010000
[ Mon Jul 15 09:07:40 2024 ] 
Training: Epoch [44/150], Step [999], Loss: 0.26143521070480347, Training Accuracy: 86.2125
[ Mon Jul 15 09:07:40 2024 ] 	Batch(1000/6809) done. Loss: 0.0280  lr:0.010000
[ Mon Jul 15 09:08:02 2024 ] 	Batch(1100/6809) done. Loss: 0.2797  lr:0.010000
[ Mon Jul 15 09:08:25 2024 ] 	Batch(1200/6809) done. Loss: 0.0989  lr:0.010000
[ Mon Jul 15 09:08:48 2024 ] 	Batch(1300/6809) done. Loss: 0.8277  lr:0.010000
[ Mon Jul 15 09:09:10 2024 ] 	Batch(1400/6809) done. Loss: 0.2317  lr:0.010000
[ Mon Jul 15 09:09:32 2024 ] 
Training: Epoch [44/150], Step [1499], Loss: 0.4430684447288513, Training Accuracy: 86.00833333333333
[ Mon Jul 15 09:09:33 2024 ] 	Batch(1500/6809) done. Loss: 0.7867  lr:0.010000
[ Mon Jul 15 09:09:56 2024 ] 	Batch(1600/6809) done. Loss: 0.1390  lr:0.010000
[ Mon Jul 15 09:10:18 2024 ] 	Batch(1700/6809) done. Loss: 0.2098  lr:0.010000
[ Mon Jul 15 09:10:41 2024 ] 	Batch(1800/6809) done. Loss: 0.2036  lr:0.010000
[ Mon Jul 15 09:11:03 2024 ] 	Batch(1900/6809) done. Loss: 0.5120  lr:0.010000
[ Mon Jul 15 09:11:26 2024 ] 
Training: Epoch [44/150], Step [1999], Loss: 0.9839953780174255, Training Accuracy: 85.78750000000001
[ Mon Jul 15 09:11:26 2024 ] 	Batch(2000/6809) done. Loss: 0.0950  lr:0.010000
[ Mon Jul 15 09:11:49 2024 ] 	Batch(2100/6809) done. Loss: 0.4758  lr:0.010000
[ Mon Jul 15 09:12:12 2024 ] 	Batch(2200/6809) done. Loss: 0.2975  lr:0.010000
[ Mon Jul 15 09:12:34 2024 ] 	Batch(2300/6809) done. Loss: 0.6173  lr:0.010000
[ Mon Jul 15 09:12:57 2024 ] 	Batch(2400/6809) done. Loss: 0.4489  lr:0.010000
[ Mon Jul 15 09:13:19 2024 ] 
Training: Epoch [44/150], Step [2499], Loss: 0.6354408264160156, Training Accuracy: 85.71
[ Mon Jul 15 09:13:20 2024 ] 	Batch(2500/6809) done. Loss: 0.6270  lr:0.010000
[ Mon Jul 15 09:13:42 2024 ] 	Batch(2600/6809) done. Loss: 0.9000  lr:0.010000
[ Mon Jul 15 09:14:05 2024 ] 	Batch(2700/6809) done. Loss: 0.1703  lr:0.010000
[ Mon Jul 15 09:14:27 2024 ] 	Batch(2800/6809) done. Loss: 0.3237  lr:0.010000
[ Mon Jul 15 09:14:50 2024 ] 	Batch(2900/6809) done. Loss: 0.0401  lr:0.010000
[ Mon Jul 15 09:15:12 2024 ] 
Training: Epoch [44/150], Step [2999], Loss: 1.0756664276123047, Training Accuracy: 85.67083333333333
[ Mon Jul 15 09:15:12 2024 ] 	Batch(3000/6809) done. Loss: 0.5912  lr:0.010000
[ Mon Jul 15 09:15:36 2024 ] 	Batch(3100/6809) done. Loss: 0.5713  lr:0.010000
[ Mon Jul 15 09:15:59 2024 ] 	Batch(3200/6809) done. Loss: 0.1259  lr:0.010000
[ Mon Jul 15 09:16:22 2024 ] 	Batch(3300/6809) done. Loss: 0.8906  lr:0.010000
[ Mon Jul 15 09:16:45 2024 ] 	Batch(3400/6809) done. Loss: 0.7797  lr:0.010000
[ Mon Jul 15 09:17:08 2024 ] 
Training: Epoch [44/150], Step [3499], Loss: 0.2268255203962326, Training Accuracy: 85.44642857142857
[ Mon Jul 15 09:17:08 2024 ] 	Batch(3500/6809) done. Loss: 0.9923  lr:0.010000
[ Mon Jul 15 09:17:31 2024 ] 	Batch(3600/6809) done. Loss: 0.3463  lr:0.010000
[ Mon Jul 15 09:17:55 2024 ] 	Batch(3700/6809) done. Loss: 0.5365  lr:0.010000
[ Mon Jul 15 09:18:18 2024 ] 	Batch(3800/6809) done. Loss: 0.3981  lr:0.010000
[ Mon Jul 15 09:18:41 2024 ] 	Batch(3900/6809) done. Loss: 0.9105  lr:0.010000
[ Mon Jul 15 09:19:03 2024 ] 
Training: Epoch [44/150], Step [3999], Loss: 0.3851952850818634, Training Accuracy: 85.29375
[ Mon Jul 15 09:19:04 2024 ] 	Batch(4000/6809) done. Loss: 0.0824  lr:0.010000
[ Mon Jul 15 09:19:27 2024 ] 	Batch(4100/6809) done. Loss: 0.6409  lr:0.010000
[ Mon Jul 15 09:19:50 2024 ] 	Batch(4200/6809) done. Loss: 0.4428  lr:0.010000
[ Mon Jul 15 09:20:14 2024 ] 	Batch(4300/6809) done. Loss: 0.0564  lr:0.010000
[ Mon Jul 15 09:20:38 2024 ] 	Batch(4400/6809) done. Loss: 0.0704  lr:0.010000
[ Mon Jul 15 09:21:00 2024 ] 
Training: Epoch [44/150], Step [4499], Loss: 0.7683730125427246, Training Accuracy: 85.15
[ Mon Jul 15 09:21:00 2024 ] 	Batch(4500/6809) done. Loss: 0.1034  lr:0.010000
[ Mon Jul 15 09:21:23 2024 ] 	Batch(4600/6809) done. Loss: 0.2112  lr:0.010000
[ Mon Jul 15 09:21:46 2024 ] 	Batch(4700/6809) done. Loss: 0.6545  lr:0.010000
[ Mon Jul 15 09:22:10 2024 ] 	Batch(4800/6809) done. Loss: 0.6530  lr:0.010000
[ Mon Jul 15 09:22:33 2024 ] 	Batch(4900/6809) done. Loss: 0.6856  lr:0.010000
[ Mon Jul 15 09:22:56 2024 ] 
Training: Epoch [44/150], Step [4999], Loss: 0.4110015630722046, Training Accuracy: 85.1275
[ Mon Jul 15 09:22:56 2024 ] 	Batch(5000/6809) done. Loss: 1.4386  lr:0.010000
[ Mon Jul 15 09:23:20 2024 ] 	Batch(5100/6809) done. Loss: 1.0628  lr:0.010000
[ Mon Jul 15 09:23:43 2024 ] 	Batch(5200/6809) done. Loss: 0.7596  lr:0.010000
[ Mon Jul 15 09:24:06 2024 ] 	Batch(5300/6809) done. Loss: 0.0809  lr:0.010000
[ Mon Jul 15 09:24:29 2024 ] 	Batch(5400/6809) done. Loss: 0.3447  lr:0.010000
[ Mon Jul 15 09:24:52 2024 ] 
Training: Epoch [44/150], Step [5499], Loss: 0.24657967686653137, Training Accuracy: 85.04090909090908
[ Mon Jul 15 09:24:52 2024 ] 	Batch(5500/6809) done. Loss: 0.0363  lr:0.010000
[ Mon Jul 15 09:25:15 2024 ] 	Batch(5600/6809) done. Loss: 0.5370  lr:0.010000
[ Mon Jul 15 09:25:38 2024 ] 	Batch(5700/6809) done. Loss: 0.6493  lr:0.010000
[ Mon Jul 15 09:26:01 2024 ] 	Batch(5800/6809) done. Loss: 0.0571  lr:0.010000
[ Mon Jul 15 09:26:23 2024 ] 	Batch(5900/6809) done. Loss: 0.4247  lr:0.010000
[ Mon Jul 15 09:26:46 2024 ] 
Training: Epoch [44/150], Step [5999], Loss: 0.3873971998691559, Training Accuracy: 84.925
[ Mon Jul 15 09:26:46 2024 ] 	Batch(6000/6809) done. Loss: 0.4366  lr:0.010000
[ Mon Jul 15 09:27:09 2024 ] 	Batch(6100/6809) done. Loss: 0.7716  lr:0.010000
[ Mon Jul 15 09:27:32 2024 ] 	Batch(6200/6809) done. Loss: 0.2003  lr:0.010000
[ Mon Jul 15 09:27:55 2024 ] 	Batch(6300/6809) done. Loss: 0.4830  lr:0.010000
[ Mon Jul 15 09:28:18 2024 ] 	Batch(6400/6809) done. Loss: 0.4943  lr:0.010000
[ Mon Jul 15 09:28:40 2024 ] 
Training: Epoch [44/150], Step [6499], Loss: 0.8502577543258667, Training Accuracy: 84.85769230769232
[ Mon Jul 15 09:28:41 2024 ] 	Batch(6500/6809) done. Loss: 0.2673  lr:0.010000
[ Mon Jul 15 09:29:03 2024 ] 	Batch(6600/6809) done. Loss: 0.3601  lr:0.010000
[ Mon Jul 15 09:29:26 2024 ] 	Batch(6700/6809) done. Loss: 1.3635  lr:0.010000
[ Mon Jul 15 09:29:49 2024 ] 	Batch(6800/6809) done. Loss: 0.0488  lr:0.010000
[ Mon Jul 15 09:29:51 2024 ] 	Mean training loss: 0.4808.
[ Mon Jul 15 09:29:51 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 09:29:52 2024 ] Training epoch: 46
[ Mon Jul 15 09:29:52 2024 ] 	Batch(0/6809) done. Loss: 0.3825  lr:0.010000
[ Mon Jul 15 09:30:15 2024 ] 	Batch(100/6809) done. Loss: 0.1420  lr:0.010000
[ Mon Jul 15 09:30:38 2024 ] 	Batch(200/6809) done. Loss: 0.1141  lr:0.010000
[ Mon Jul 15 09:31:01 2024 ] 	Batch(300/6809) done. Loss: 0.8539  lr:0.010000
[ Mon Jul 15 09:31:24 2024 ] 	Batch(400/6809) done. Loss: 0.3540  lr:0.010000
[ Mon Jul 15 09:31:47 2024 ] 
Training: Epoch [45/150], Step [499], Loss: 0.9880563020706177, Training Accuracy: 86.1
[ Mon Jul 15 09:31:47 2024 ] 	Batch(500/6809) done. Loss: 0.1189  lr:0.010000
[ Mon Jul 15 09:32:11 2024 ] 	Batch(600/6809) done. Loss: 0.3680  lr:0.010000
[ Mon Jul 15 09:32:34 2024 ] 	Batch(700/6809) done. Loss: 0.3961  lr:0.010000
[ Mon Jul 15 09:32:57 2024 ] 	Batch(800/6809) done. Loss: 0.0539  lr:0.010000
[ Mon Jul 15 09:33:20 2024 ] 	Batch(900/6809) done. Loss: 0.1666  lr:0.010000
[ Mon Jul 15 09:33:43 2024 ] 
Training: Epoch [45/150], Step [999], Loss: 0.21820500493049622, Training Accuracy: 85.725
[ Mon Jul 15 09:33:43 2024 ] 	Batch(1000/6809) done. Loss: 0.6461  lr:0.010000
[ Mon Jul 15 09:34:06 2024 ] 	Batch(1100/6809) done. Loss: 0.7947  lr:0.010000
[ Mon Jul 15 09:34:30 2024 ] 	Batch(1200/6809) done. Loss: 0.8649  lr:0.010000
[ Mon Jul 15 09:34:53 2024 ] 	Batch(1300/6809) done. Loss: 0.7893  lr:0.010000
[ Mon Jul 15 09:35:16 2024 ] 	Batch(1400/6809) done. Loss: 1.2284  lr:0.010000
[ Mon Jul 15 09:35:40 2024 ] 
Training: Epoch [45/150], Step [1499], Loss: 0.7756989598274231, Training Accuracy: 85.65
[ Mon Jul 15 09:35:40 2024 ] 	Batch(1500/6809) done. Loss: 0.7606  lr:0.010000
[ Mon Jul 15 09:36:03 2024 ] 	Batch(1600/6809) done. Loss: 0.2849  lr:0.010000
[ Mon Jul 15 09:36:27 2024 ] 	Batch(1700/6809) done. Loss: 0.3176  lr:0.010000
[ Mon Jul 15 09:36:50 2024 ] 	Batch(1800/6809) done. Loss: 0.3083  lr:0.010000
[ Mon Jul 15 09:37:13 2024 ] 	Batch(1900/6809) done. Loss: 0.0536  lr:0.010000
[ Mon Jul 15 09:37:36 2024 ] 
Training: Epoch [45/150], Step [1999], Loss: 0.5743064880371094, Training Accuracy: 85.44375
[ Mon Jul 15 09:37:36 2024 ] 	Batch(2000/6809) done. Loss: 0.6550  lr:0.010000
[ Mon Jul 15 09:37:59 2024 ] 	Batch(2100/6809) done. Loss: 0.8837  lr:0.010000
[ Mon Jul 15 09:38:22 2024 ] 	Batch(2200/6809) done. Loss: 0.2992  lr:0.010000
[ Mon Jul 15 09:38:45 2024 ] 	Batch(2300/6809) done. Loss: 0.0238  lr:0.010000
[ Mon Jul 15 09:39:09 2024 ] 	Batch(2400/6809) done. Loss: 0.4425  lr:0.010000
[ Mon Jul 15 09:39:32 2024 ] 
Training: Epoch [45/150], Step [2499], Loss: 0.9560113549232483, Training Accuracy: 85.30499999999999
[ Mon Jul 15 09:39:32 2024 ] 	Batch(2500/6809) done. Loss: 0.1061  lr:0.010000
[ Mon Jul 15 09:39:55 2024 ] 	Batch(2600/6809) done. Loss: 0.3096  lr:0.010000
[ Mon Jul 15 09:40:19 2024 ] 	Batch(2700/6809) done. Loss: 0.2378  lr:0.010000
[ Mon Jul 15 09:40:43 2024 ] 	Batch(2800/6809) done. Loss: 1.0165  lr:0.010000
[ Mon Jul 15 09:41:07 2024 ] 	Batch(2900/6809) done. Loss: 0.3447  lr:0.010000
[ Mon Jul 15 09:41:31 2024 ] 
Training: Epoch [45/150], Step [2999], Loss: 0.47197258472442627, Training Accuracy: 85.35416666666666
[ Mon Jul 15 09:41:31 2024 ] 	Batch(3000/6809) done. Loss: 1.2027  lr:0.010000
[ Mon Jul 15 09:41:54 2024 ] 	Batch(3100/6809) done. Loss: 0.0294  lr:0.010000
[ Mon Jul 15 09:42:18 2024 ] 	Batch(3200/6809) done. Loss: 0.1726  lr:0.010000
[ Mon Jul 15 09:42:41 2024 ] 	Batch(3300/6809) done. Loss: 0.8714  lr:0.010000
[ Mon Jul 15 09:43:04 2024 ] 	Batch(3400/6809) done. Loss: 0.3112  lr:0.010000
[ Mon Jul 15 09:43:27 2024 ] 
Training: Epoch [45/150], Step [3499], Loss: 0.059408094733953476, Training Accuracy: 85.25357142857143
[ Mon Jul 15 09:43:27 2024 ] 	Batch(3500/6809) done. Loss: 0.1576  lr:0.010000
[ Mon Jul 15 09:43:50 2024 ] 	Batch(3600/6809) done. Loss: 0.6122  lr:0.010000
[ Mon Jul 15 09:44:13 2024 ] 	Batch(3700/6809) done. Loss: 0.6316  lr:0.010000
[ Mon Jul 15 09:44:36 2024 ] 	Batch(3800/6809) done. Loss: 0.5071  lr:0.010000
[ Mon Jul 15 09:44:59 2024 ] 	Batch(3900/6809) done. Loss: 0.2845  lr:0.010000
[ Mon Jul 15 09:45:22 2024 ] 
Training: Epoch [45/150], Step [3999], Loss: 0.945705771446228, Training Accuracy: 85.125
[ Mon Jul 15 09:45:22 2024 ] 	Batch(4000/6809) done. Loss: 1.3253  lr:0.010000
[ Mon Jul 15 09:45:45 2024 ] 	Batch(4100/6809) done. Loss: 1.2525  lr:0.010000
[ Mon Jul 15 09:46:08 2024 ] 	Batch(4200/6809) done. Loss: 0.8098  lr:0.010000
[ Mon Jul 15 09:46:31 2024 ] 	Batch(4300/6809) done. Loss: 0.0903  lr:0.010000
[ Mon Jul 15 09:46:54 2024 ] 	Batch(4400/6809) done. Loss: 0.1919  lr:0.010000
[ Mon Jul 15 09:47:17 2024 ] 
Training: Epoch [45/150], Step [4499], Loss: 0.46724140644073486, Training Accuracy: 85.14722222222221
[ Mon Jul 15 09:47:17 2024 ] 	Batch(4500/6809) done. Loss: 0.4403  lr:0.010000
[ Mon Jul 15 09:47:40 2024 ] 	Batch(4600/6809) done. Loss: 0.7865  lr:0.010000
[ Mon Jul 15 09:48:03 2024 ] 	Batch(4700/6809) done. Loss: 0.1383  lr:0.010000
[ Mon Jul 15 09:48:26 2024 ] 	Batch(4800/6809) done. Loss: 0.2799  lr:0.010000
[ Mon Jul 15 09:48:49 2024 ] 	Batch(4900/6809) done. Loss: 0.1763  lr:0.010000
[ Mon Jul 15 09:49:12 2024 ] 
Training: Epoch [45/150], Step [4999], Loss: 0.11548079550266266, Training Accuracy: 85.2
[ Mon Jul 15 09:49:12 2024 ] 	Batch(5000/6809) done. Loss: 0.0597  lr:0.010000
[ Mon Jul 15 09:49:35 2024 ] 	Batch(5100/6809) done. Loss: 1.2652  lr:0.010000
[ Mon Jul 15 09:49:58 2024 ] 	Batch(5200/6809) done. Loss: 0.2834  lr:0.010000
[ Mon Jul 15 09:50:21 2024 ] 	Batch(5300/6809) done. Loss: 0.8377  lr:0.010000
[ Mon Jul 15 09:50:44 2024 ] 	Batch(5400/6809) done. Loss: 1.3042  lr:0.010000
[ Mon Jul 15 09:51:06 2024 ] 
Training: Epoch [45/150], Step [5499], Loss: 0.13363684713840485, Training Accuracy: 85.05681818181819
[ Mon Jul 15 09:51:06 2024 ] 	Batch(5500/6809) done. Loss: 0.7867  lr:0.010000
[ Mon Jul 15 09:51:29 2024 ] 	Batch(5600/6809) done. Loss: 0.7545  lr:0.010000
[ Mon Jul 15 09:51:52 2024 ] 	Batch(5700/6809) done. Loss: 0.6526  lr:0.010000
[ Mon Jul 15 09:52:14 2024 ] 	Batch(5800/6809) done. Loss: 0.4601  lr:0.010000
[ Mon Jul 15 09:52:37 2024 ] 	Batch(5900/6809) done. Loss: 1.0710  lr:0.010000
[ Mon Jul 15 09:52:59 2024 ] 
Training: Epoch [45/150], Step [5999], Loss: 0.8036100268363953, Training Accuracy: 84.99791666666667
[ Mon Jul 15 09:52:59 2024 ] 	Batch(6000/6809) done. Loss: 0.6319  lr:0.010000
[ Mon Jul 15 09:53:22 2024 ] 	Batch(6100/6809) done. Loss: 0.9945  lr:0.010000
[ Mon Jul 15 09:53:45 2024 ] 	Batch(6200/6809) done. Loss: 0.2150  lr:0.010000
[ Mon Jul 15 09:54:08 2024 ] 	Batch(6300/6809) done. Loss: 0.4234  lr:0.010000
[ Mon Jul 15 09:54:31 2024 ] 	Batch(6400/6809) done. Loss: 0.4799  lr:0.010000
[ Mon Jul 15 09:54:53 2024 ] 
Training: Epoch [45/150], Step [6499], Loss: 0.5536249279975891, Training Accuracy: 84.83653846153845
[ Mon Jul 15 09:54:53 2024 ] 	Batch(6500/6809) done. Loss: 0.6097  lr:0.010000
[ Mon Jul 15 09:55:16 2024 ] 	Batch(6600/6809) done. Loss: 0.7839  lr:0.010000
[ Mon Jul 15 09:55:39 2024 ] 	Batch(6700/6809) done. Loss: 0.7008  lr:0.010000
[ Mon Jul 15 09:56:02 2024 ] 	Batch(6800/6809) done. Loss: 0.8958  lr:0.010000
[ Mon Jul 15 09:56:04 2024 ] 	Mean training loss: 0.4697.
[ Mon Jul 15 09:56:04 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 09:56:04 2024 ] Training epoch: 47
[ Mon Jul 15 09:56:05 2024 ] 	Batch(0/6809) done. Loss: 0.1488  lr:0.010000
[ Mon Jul 15 09:56:28 2024 ] 	Batch(100/6809) done. Loss: 0.4825  lr:0.010000
[ Mon Jul 15 09:56:51 2024 ] 	Batch(200/6809) done. Loss: 0.1241  lr:0.010000
[ Mon Jul 15 09:57:14 2024 ] 	Batch(300/6809) done. Loss: 0.7035  lr:0.010000
[ Mon Jul 15 09:57:37 2024 ] 	Batch(400/6809) done. Loss: 0.0542  lr:0.010000
[ Mon Jul 15 09:58:00 2024 ] 
Training: Epoch [46/150], Step [499], Loss: 0.19175253808498383, Training Accuracy: 85.6
[ Mon Jul 15 09:58:00 2024 ] 	Batch(500/6809) done. Loss: 0.8288  lr:0.010000
[ Mon Jul 15 09:58:23 2024 ] 	Batch(600/6809) done. Loss: 0.3504  lr:0.010000
[ Mon Jul 15 09:58:45 2024 ] 	Batch(700/6809) done. Loss: 0.1009  lr:0.010000
[ Mon Jul 15 09:59:08 2024 ] 	Batch(800/6809) done. Loss: 0.3222  lr:0.010000
[ Mon Jul 15 09:59:31 2024 ] 	Batch(900/6809) done. Loss: 0.4086  lr:0.010000
[ Mon Jul 15 09:59:54 2024 ] 
Training: Epoch [46/150], Step [999], Loss: 0.35498473048210144, Training Accuracy: 85.8625
[ Mon Jul 15 09:59:54 2024 ] 	Batch(1000/6809) done. Loss: 0.2408  lr:0.010000
[ Mon Jul 15 10:00:17 2024 ] 	Batch(1100/6809) done. Loss: 0.2830  lr:0.010000
[ Mon Jul 15 10:00:40 2024 ] 	Batch(1200/6809) done. Loss: 0.1223  lr:0.010000
[ Mon Jul 15 10:01:03 2024 ] 	Batch(1300/6809) done. Loss: 0.0993  lr:0.010000
[ Mon Jul 15 10:01:26 2024 ] 	Batch(1400/6809) done. Loss: 0.1429  lr:0.010000
[ Mon Jul 15 10:01:49 2024 ] 
Training: Epoch [46/150], Step [1499], Loss: 0.12606915831565857, Training Accuracy: 86.425
[ Mon Jul 15 10:01:49 2024 ] 	Batch(1500/6809) done. Loss: 0.4309  lr:0.010000
[ Mon Jul 15 10:02:12 2024 ] 	Batch(1600/6809) done. Loss: 0.2012  lr:0.010000
[ Mon Jul 15 10:02:35 2024 ] 	Batch(1700/6809) done. Loss: 0.2226  lr:0.010000
[ Mon Jul 15 10:02:57 2024 ] 	Batch(1800/6809) done. Loss: 0.7659  lr:0.010000
[ Mon Jul 15 10:03:20 2024 ] 	Batch(1900/6809) done. Loss: 0.1052  lr:0.010000
[ Mon Jul 15 10:03:42 2024 ] 
Training: Epoch [46/150], Step [1999], Loss: 0.04601157084107399, Training Accuracy: 86.325
[ Mon Jul 15 10:03:43 2024 ] 	Batch(2000/6809) done. Loss: 1.4555  lr:0.010000
[ Mon Jul 15 10:04:06 2024 ] 	Batch(2100/6809) done. Loss: 0.8073  lr:0.010000
[ Mon Jul 15 10:04:30 2024 ] 	Batch(2200/6809) done. Loss: 0.5481  lr:0.010000
[ Mon Jul 15 10:04:53 2024 ] 	Batch(2300/6809) done. Loss: 0.1281  lr:0.010000
[ Mon Jul 15 10:05:16 2024 ] 	Batch(2400/6809) done. Loss: 0.3496  lr:0.010000
[ Mon Jul 15 10:05:39 2024 ] 
Training: Epoch [46/150], Step [2499], Loss: 0.2518851161003113, Training Accuracy: 86.15
[ Mon Jul 15 10:05:39 2024 ] 	Batch(2500/6809) done. Loss: 0.0342  lr:0.010000
[ Mon Jul 15 10:06:02 2024 ] 	Batch(2600/6809) done. Loss: 0.2969  lr:0.010000
[ Mon Jul 15 10:06:24 2024 ] 	Batch(2700/6809) done. Loss: 0.2171  lr:0.010000
[ Mon Jul 15 10:06:47 2024 ] 	Batch(2800/6809) done. Loss: 1.6986  lr:0.010000
[ Mon Jul 15 10:07:10 2024 ] 	Batch(2900/6809) done. Loss: 0.4489  lr:0.010000
[ Mon Jul 15 10:07:32 2024 ] 
Training: Epoch [46/150], Step [2999], Loss: 0.8583940863609314, Training Accuracy: 85.91250000000001
[ Mon Jul 15 10:07:33 2024 ] 	Batch(3000/6809) done. Loss: 0.0932  lr:0.010000
[ Mon Jul 15 10:07:55 2024 ] 	Batch(3100/6809) done. Loss: 0.3387  lr:0.010000
[ Mon Jul 15 10:08:18 2024 ] 	Batch(3200/6809) done. Loss: 0.5773  lr:0.010000
[ Mon Jul 15 10:08:41 2024 ] 	Batch(3300/6809) done. Loss: 0.4092  lr:0.010000
[ Mon Jul 15 10:09:04 2024 ] 	Batch(3400/6809) done. Loss: 1.0516  lr:0.010000
[ Mon Jul 15 10:09:26 2024 ] 
Training: Epoch [46/150], Step [3499], Loss: 0.5091711282730103, Training Accuracy: 85.60714285714286
[ Mon Jul 15 10:09:26 2024 ] 	Batch(3500/6809) done. Loss: 0.2288  lr:0.010000
[ Mon Jul 15 10:09:49 2024 ] 	Batch(3600/6809) done. Loss: 0.1100  lr:0.010000
[ Mon Jul 15 10:10:12 2024 ] 	Batch(3700/6809) done. Loss: 0.3251  lr:0.010000
[ Mon Jul 15 10:10:35 2024 ] 	Batch(3800/6809) done. Loss: 1.0796  lr:0.010000
[ Mon Jul 15 10:10:57 2024 ] 	Batch(3900/6809) done. Loss: 0.3834  lr:0.010000
[ Mon Jul 15 10:11:20 2024 ] 
Training: Epoch [46/150], Step [3999], Loss: 0.05408116802573204, Training Accuracy: 85.45937500000001
[ Mon Jul 15 10:11:20 2024 ] 	Batch(4000/6809) done. Loss: 1.0722  lr:0.010000
[ Mon Jul 15 10:11:44 2024 ] 	Batch(4100/6809) done. Loss: 0.3307  lr:0.010000
[ Mon Jul 15 10:12:07 2024 ] 	Batch(4200/6809) done. Loss: 0.3329  lr:0.010000
[ Mon Jul 15 10:12:30 2024 ] 	Batch(4300/6809) done. Loss: 0.1407  lr:0.010000
[ Mon Jul 15 10:12:53 2024 ] 	Batch(4400/6809) done. Loss: 0.3354  lr:0.010000
[ Mon Jul 15 10:13:15 2024 ] 
Training: Epoch [46/150], Step [4499], Loss: 0.2595313787460327, Training Accuracy: 85.40833333333333
[ Mon Jul 15 10:13:16 2024 ] 	Batch(4500/6809) done. Loss: 0.5562  lr:0.010000
[ Mon Jul 15 10:13:38 2024 ] 	Batch(4600/6809) done. Loss: 0.4354  lr:0.010000
[ Mon Jul 15 10:14:01 2024 ] 	Batch(4700/6809) done. Loss: 1.6643  lr:0.010000
[ Mon Jul 15 10:14:24 2024 ] 	Batch(4800/6809) done. Loss: 0.0816  lr:0.010000
[ Mon Jul 15 10:14:47 2024 ] 	Batch(4900/6809) done. Loss: 0.2987  lr:0.010000
[ Mon Jul 15 10:15:09 2024 ] 
Training: Epoch [46/150], Step [4999], Loss: 0.6203466653823853, Training Accuracy: 85.32499999999999
[ Mon Jul 15 10:15:09 2024 ] 	Batch(5000/6809) done. Loss: 0.0345  lr:0.010000
[ Mon Jul 15 10:15:32 2024 ] 	Batch(5100/6809) done. Loss: 0.1431  lr:0.010000
[ Mon Jul 15 10:15:55 2024 ] 	Batch(5200/6809) done. Loss: 0.0737  lr:0.010000
[ Mon Jul 15 10:16:18 2024 ] 	Batch(5300/6809) done. Loss: 0.4849  lr:0.010000
[ Mon Jul 15 10:16:41 2024 ] 	Batch(5400/6809) done. Loss: 0.9320  lr:0.010000
[ Mon Jul 15 10:17:03 2024 ] 
Training: Epoch [46/150], Step [5499], Loss: 0.026423901319503784, Training Accuracy: 85.3409090909091
[ Mon Jul 15 10:17:03 2024 ] 	Batch(5500/6809) done. Loss: 0.3786  lr:0.010000
[ Mon Jul 15 10:17:26 2024 ] 	Batch(5600/6809) done. Loss: 0.7018  lr:0.010000
[ Mon Jul 15 10:17:49 2024 ] 	Batch(5700/6809) done. Loss: 0.4386  lr:0.010000
[ Mon Jul 15 10:18:12 2024 ] 	Batch(5800/6809) done. Loss: 0.7256  lr:0.010000
[ Mon Jul 15 10:18:34 2024 ] 	Batch(5900/6809) done. Loss: 0.1629  lr:0.010000
[ Mon Jul 15 10:18:57 2024 ] 
Training: Epoch [46/150], Step [5999], Loss: 0.2839967906475067, Training Accuracy: 85.32083333333334
[ Mon Jul 15 10:18:57 2024 ] 	Batch(6000/6809) done. Loss: 0.6928  lr:0.010000
[ Mon Jul 15 10:19:20 2024 ] 	Batch(6100/6809) done. Loss: 0.7631  lr:0.010000
[ Mon Jul 15 10:19:43 2024 ] 	Batch(6200/6809) done. Loss: 0.2272  lr:0.010000
[ Mon Jul 15 10:20:05 2024 ] 	Batch(6300/6809) done. Loss: 0.7502  lr:0.010000
[ Mon Jul 15 10:20:28 2024 ] 	Batch(6400/6809) done. Loss: 0.0684  lr:0.010000
[ Mon Jul 15 10:20:50 2024 ] 
Training: Epoch [46/150], Step [6499], Loss: 0.120502769947052, Training Accuracy: 85.21923076923076
[ Mon Jul 15 10:20:51 2024 ] 	Batch(6500/6809) done. Loss: 0.7888  lr:0.010000
[ Mon Jul 15 10:21:13 2024 ] 	Batch(6600/6809) done. Loss: 0.8384  lr:0.010000
[ Mon Jul 15 10:21:36 2024 ] 	Batch(6700/6809) done. Loss: 0.3203  lr:0.010000
[ Mon Jul 15 10:21:59 2024 ] 	Batch(6800/6809) done. Loss: 0.1277  lr:0.010000
[ Mon Jul 15 10:22:01 2024 ] 	Mean training loss: 0.4740.
[ Mon Jul 15 10:22:01 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 10:22:01 2024 ] Training epoch: 48
[ Mon Jul 15 10:22:02 2024 ] 	Batch(0/6809) done. Loss: 0.3889  lr:0.010000
[ Mon Jul 15 10:22:25 2024 ] 	Batch(100/6809) done. Loss: 0.0132  lr:0.010000
[ Mon Jul 15 10:22:47 2024 ] 	Batch(200/6809) done. Loss: 0.6056  lr:0.010000
[ Mon Jul 15 10:23:10 2024 ] 	Batch(300/6809) done. Loss: 0.7124  lr:0.010000
[ Mon Jul 15 10:23:33 2024 ] 	Batch(400/6809) done. Loss: 0.4002  lr:0.010000
[ Mon Jul 15 10:23:56 2024 ] 
Training: Epoch [47/150], Step [499], Loss: 0.12507398426532745, Training Accuracy: 85.52499999999999
[ Mon Jul 15 10:23:56 2024 ] 	Batch(500/6809) done. Loss: 0.1041  lr:0.010000
[ Mon Jul 15 10:24:19 2024 ] 	Batch(600/6809) done. Loss: 0.6513  lr:0.010000
[ Mon Jul 15 10:24:41 2024 ] 	Batch(700/6809) done. Loss: 0.6198  lr:0.010000
[ Mon Jul 15 10:25:04 2024 ] 	Batch(800/6809) done. Loss: 0.7546  lr:0.010000
[ Mon Jul 15 10:25:27 2024 ] 	Batch(900/6809) done. Loss: 0.1738  lr:0.010000
[ Mon Jul 15 10:25:49 2024 ] 
Training: Epoch [47/150], Step [999], Loss: 0.15211805701255798, Training Accuracy: 86.0625
[ Mon Jul 15 10:25:49 2024 ] 	Batch(1000/6809) done. Loss: 0.5838  lr:0.010000
[ Mon Jul 15 10:26:12 2024 ] 	Batch(1100/6809) done. Loss: 0.4332  lr:0.010000
[ Mon Jul 15 10:26:35 2024 ] 	Batch(1200/6809) done. Loss: 0.5588  lr:0.010000
[ Mon Jul 15 10:26:57 2024 ] 	Batch(1300/6809) done. Loss: 0.5440  lr:0.010000
[ Mon Jul 15 10:27:20 2024 ] 	Batch(1400/6809) done. Loss: 0.1440  lr:0.010000
[ Mon Jul 15 10:27:43 2024 ] 
Training: Epoch [47/150], Step [1499], Loss: 0.22564534842967987, Training Accuracy: 85.7
[ Mon Jul 15 10:27:43 2024 ] 	Batch(1500/6809) done. Loss: 0.2105  lr:0.010000
[ Mon Jul 15 10:28:06 2024 ] 	Batch(1600/6809) done. Loss: 0.0530  lr:0.010000
[ Mon Jul 15 10:28:29 2024 ] 	Batch(1700/6809) done. Loss: 0.0594  lr:0.010000
[ Mon Jul 15 10:28:52 2024 ] 	Batch(1800/6809) done. Loss: 0.4105  lr:0.010000
[ Mon Jul 15 10:29:15 2024 ] 	Batch(1900/6809) done. Loss: 0.8034  lr:0.010000
[ Mon Jul 15 10:29:38 2024 ] 
Training: Epoch [47/150], Step [1999], Loss: 0.6770696640014648, Training Accuracy: 85.84375
[ Mon Jul 15 10:29:38 2024 ] 	Batch(2000/6809) done. Loss: 0.0436  lr:0.010000
[ Mon Jul 15 10:30:01 2024 ] 	Batch(2100/6809) done. Loss: 0.7549  lr:0.010000
[ Mon Jul 15 10:30:23 2024 ] 	Batch(2200/6809) done. Loss: 0.4814  lr:0.010000
[ Mon Jul 15 10:30:46 2024 ] 	Batch(2300/6809) done. Loss: 0.3808  lr:0.010000
[ Mon Jul 15 10:31:09 2024 ] 	Batch(2400/6809) done. Loss: 0.8381  lr:0.010000
[ Mon Jul 15 10:31:32 2024 ] 
Training: Epoch [47/150], Step [2499], Loss: 0.6253563165664673, Training Accuracy: 85.64
[ Mon Jul 15 10:31:32 2024 ] 	Batch(2500/6809) done. Loss: 1.2672  lr:0.010000
[ Mon Jul 15 10:31:55 2024 ] 	Batch(2600/6809) done. Loss: 0.5416  lr:0.010000
[ Mon Jul 15 10:32:18 2024 ] 	Batch(2700/6809) done. Loss: 0.0950  lr:0.010000
[ Mon Jul 15 10:32:41 2024 ] 	Batch(2800/6809) done. Loss: 1.0130  lr:0.010000
[ Mon Jul 15 10:33:04 2024 ] 	Batch(2900/6809) done. Loss: 1.2419  lr:0.010000
[ Mon Jul 15 10:33:27 2024 ] 
Training: Epoch [47/150], Step [2999], Loss: 0.7323974370956421, Training Accuracy: 85.62083333333334
[ Mon Jul 15 10:33:27 2024 ] 	Batch(3000/6809) done. Loss: 0.1690  lr:0.010000
[ Mon Jul 15 10:33:50 2024 ] 	Batch(3100/6809) done. Loss: 0.1561  lr:0.010000
[ Mon Jul 15 10:34:13 2024 ] 	Batch(3200/6809) done. Loss: 1.3204  lr:0.010000
[ Mon Jul 15 10:34:36 2024 ] 	Batch(3300/6809) done. Loss: 0.9582  lr:0.010000
[ Mon Jul 15 10:34:59 2024 ] 	Batch(3400/6809) done. Loss: 0.2492  lr:0.010000
[ Mon Jul 15 10:35:22 2024 ] 
Training: Epoch [47/150], Step [3499], Loss: 0.5865297317504883, Training Accuracy: 85.61785714285715
[ Mon Jul 15 10:35:22 2024 ] 	Batch(3500/6809) done. Loss: 0.1814  lr:0.010000
[ Mon Jul 15 10:35:45 2024 ] 	Batch(3600/6809) done. Loss: 1.5648  lr:0.010000
[ Mon Jul 15 10:36:07 2024 ] 	Batch(3700/6809) done. Loss: 0.5490  lr:0.010000
[ Mon Jul 15 10:36:30 2024 ] 	Batch(3800/6809) done. Loss: 0.2794  lr:0.010000
[ Mon Jul 15 10:36:53 2024 ] 	Batch(3900/6809) done. Loss: 0.3328  lr:0.010000
[ Mon Jul 15 10:37:16 2024 ] 
Training: Epoch [47/150], Step [3999], Loss: 0.18830284476280212, Training Accuracy: 85.528125
[ Mon Jul 15 10:37:16 2024 ] 	Batch(4000/6809) done. Loss: 0.3577  lr:0.010000
[ Mon Jul 15 10:37:39 2024 ] 	Batch(4100/6809) done. Loss: 0.0684  lr:0.010000
[ Mon Jul 15 10:38:01 2024 ] 	Batch(4200/6809) done. Loss: 0.4786  lr:0.010000
[ Mon Jul 15 10:38:24 2024 ] 	Batch(4300/6809) done. Loss: 1.0328  lr:0.010000
[ Mon Jul 15 10:38:46 2024 ] 	Batch(4400/6809) done. Loss: 0.1774  lr:0.010000
[ Mon Jul 15 10:39:09 2024 ] 
Training: Epoch [47/150], Step [4499], Loss: 0.4831269681453705, Training Accuracy: 85.60555555555555
[ Mon Jul 15 10:39:09 2024 ] 	Batch(4500/6809) done. Loss: 0.1148  lr:0.010000
[ Mon Jul 15 10:39:32 2024 ] 	Batch(4600/6809) done. Loss: 0.9494  lr:0.010000
[ Mon Jul 15 10:39:54 2024 ] 	Batch(4700/6809) done. Loss: 0.8173  lr:0.010000
[ Mon Jul 15 10:40:17 2024 ] 	Batch(4800/6809) done. Loss: 0.6098  lr:0.010000
[ Mon Jul 15 10:40:39 2024 ] 	Batch(4900/6809) done. Loss: 0.8072  lr:0.010000
[ Mon Jul 15 10:41:02 2024 ] 
Training: Epoch [47/150], Step [4999], Loss: 0.759801983833313, Training Accuracy: 85.595
[ Mon Jul 15 10:41:02 2024 ] 	Batch(5000/6809) done. Loss: 0.5093  lr:0.010000
[ Mon Jul 15 10:41:24 2024 ] 	Batch(5100/6809) done. Loss: 0.1217  lr:0.010000
[ Mon Jul 15 10:41:47 2024 ] 	Batch(5200/6809) done. Loss: 0.8840  lr:0.010000
[ Mon Jul 15 10:42:10 2024 ] 	Batch(5300/6809) done. Loss: 0.3957  lr:0.010000
[ Mon Jul 15 10:42:32 2024 ] 	Batch(5400/6809) done. Loss: 0.1365  lr:0.010000
[ Mon Jul 15 10:42:55 2024 ] 
Training: Epoch [47/150], Step [5499], Loss: 0.6267479658126831, Training Accuracy: 85.53863636363637
[ Mon Jul 15 10:42:55 2024 ] 	Batch(5500/6809) done. Loss: 0.1155  lr:0.010000
[ Mon Jul 15 10:43:18 2024 ] 	Batch(5600/6809) done. Loss: 0.1820  lr:0.010000
[ Mon Jul 15 10:43:41 2024 ] 	Batch(5700/6809) done. Loss: 0.6805  lr:0.010000
[ Mon Jul 15 10:44:04 2024 ] 	Batch(5800/6809) done. Loss: 0.3445  lr:0.010000
[ Mon Jul 15 10:44:28 2024 ] 	Batch(5900/6809) done. Loss: 0.0736  lr:0.010000
[ Mon Jul 15 10:44:51 2024 ] 
Training: Epoch [47/150], Step [5999], Loss: 0.1520998775959015, Training Accuracy: 85.47708333333334
[ Mon Jul 15 10:44:51 2024 ] 	Batch(6000/6809) done. Loss: 0.5661  lr:0.010000
[ Mon Jul 15 10:45:14 2024 ] 	Batch(6100/6809) done. Loss: 0.2722  lr:0.010000
[ Mon Jul 15 10:45:37 2024 ] 	Batch(6200/6809) done. Loss: 0.3737  lr:0.010000
[ Mon Jul 15 10:46:00 2024 ] 	Batch(6300/6809) done. Loss: 0.3129  lr:0.010000
[ Mon Jul 15 10:46:23 2024 ] 	Batch(6400/6809) done. Loss: 0.9624  lr:0.010000
[ Mon Jul 15 10:46:45 2024 ] 
Training: Epoch [47/150], Step [6499], Loss: 0.815194845199585, Training Accuracy: 85.45961538461539
[ Mon Jul 15 10:46:46 2024 ] 	Batch(6500/6809) done. Loss: 0.6285  lr:0.010000
[ Mon Jul 15 10:47:08 2024 ] 	Batch(6600/6809) done. Loss: 0.0892  lr:0.010000
[ Mon Jul 15 10:47:31 2024 ] 	Batch(6700/6809) done. Loss: 0.6493  lr:0.010000
[ Mon Jul 15 10:47:53 2024 ] 	Batch(6800/6809) done. Loss: 0.1189  lr:0.010000
[ Mon Jul 15 10:47:55 2024 ] 	Mean training loss: 0.4675.
[ Mon Jul 15 10:47:55 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 10:47:55 2024 ] Training epoch: 49
[ Mon Jul 15 10:47:56 2024 ] 	Batch(0/6809) done. Loss: 0.0247  lr:0.010000
[ Mon Jul 15 10:48:18 2024 ] 	Batch(100/6809) done. Loss: 0.4092  lr:0.010000
[ Mon Jul 15 10:48:42 2024 ] 	Batch(200/6809) done. Loss: 0.7227  lr:0.010000
[ Mon Jul 15 10:49:05 2024 ] 	Batch(300/6809) done. Loss: 0.1982  lr:0.010000
[ Mon Jul 15 10:49:29 2024 ] 	Batch(400/6809) done. Loss: 0.3571  lr:0.010000
[ Mon Jul 15 10:49:51 2024 ] 
Training: Epoch [48/150], Step [499], Loss: 0.6391825675964355, Training Accuracy: 86.4
[ Mon Jul 15 10:49:52 2024 ] 	Batch(500/6809) done. Loss: 0.2663  lr:0.010000
[ Mon Jul 15 10:50:15 2024 ] 	Batch(600/6809) done. Loss: 0.1444  lr:0.010000
[ Mon Jul 15 10:50:38 2024 ] 	Batch(700/6809) done. Loss: 0.5431  lr:0.010000
[ Mon Jul 15 10:51:01 2024 ] 	Batch(800/6809) done. Loss: 0.4011  lr:0.010000
[ Mon Jul 15 10:51:24 2024 ] 	Batch(900/6809) done. Loss: 0.1649  lr:0.010000
[ Mon Jul 15 10:51:48 2024 ] 
Training: Epoch [48/150], Step [999], Loss: 0.3507099151611328, Training Accuracy: 86.375
[ Mon Jul 15 10:51:48 2024 ] 	Batch(1000/6809) done. Loss: 1.0891  lr:0.010000
[ Mon Jul 15 10:52:12 2024 ] 	Batch(1100/6809) done. Loss: 0.9684  lr:0.010000
[ Mon Jul 15 10:52:36 2024 ] 	Batch(1200/6809) done. Loss: 0.3239  lr:0.010000
[ Mon Jul 15 10:52:58 2024 ] 	Batch(1300/6809) done. Loss: 0.4854  lr:0.010000
[ Mon Jul 15 10:53:21 2024 ] 	Batch(1400/6809) done. Loss: 0.1785  lr:0.010000
[ Mon Jul 15 10:53:43 2024 ] 
Training: Epoch [48/150], Step [1499], Loss: 0.7044423818588257, Training Accuracy: 85.89166666666667
[ Mon Jul 15 10:53:44 2024 ] 	Batch(1500/6809) done. Loss: 0.5039  lr:0.010000
[ Mon Jul 15 10:54:06 2024 ] 	Batch(1600/6809) done. Loss: 0.2163  lr:0.010000
[ Mon Jul 15 10:54:30 2024 ] 	Batch(1700/6809) done. Loss: 0.2141  lr:0.010000
[ Mon Jul 15 10:54:52 2024 ] 	Batch(1800/6809) done. Loss: 0.2050  lr:0.010000
[ Mon Jul 15 10:55:15 2024 ] 	Batch(1900/6809) done. Loss: 0.1490  lr:0.010000
[ Mon Jul 15 10:55:38 2024 ] 
Training: Epoch [48/150], Step [1999], Loss: 0.6092166304588318, Training Accuracy: 85.69375000000001
[ Mon Jul 15 10:55:38 2024 ] 	Batch(2000/6809) done. Loss: 0.7236  lr:0.010000
[ Mon Jul 15 10:56:01 2024 ] 	Batch(2100/6809) done. Loss: 1.0184  lr:0.010000
[ Mon Jul 15 10:56:24 2024 ] 	Batch(2200/6809) done. Loss: 0.6926  lr:0.010000
[ Mon Jul 15 10:56:47 2024 ] 	Batch(2300/6809) done. Loss: 0.0081  lr:0.010000
[ Mon Jul 15 10:57:10 2024 ] 	Batch(2400/6809) done. Loss: 0.6085  lr:0.010000
[ Mon Jul 15 10:57:33 2024 ] 
Training: Epoch [48/150], Step [2499], Loss: 0.4001625180244446, Training Accuracy: 85.78
[ Mon Jul 15 10:57:33 2024 ] 	Batch(2500/6809) done. Loss: 0.6147  lr:0.010000
[ Mon Jul 15 10:57:55 2024 ] 	Batch(2600/6809) done. Loss: 0.2441  lr:0.010000
[ Mon Jul 15 10:58:18 2024 ] 	Batch(2700/6809) done. Loss: 0.5899  lr:0.010000
[ Mon Jul 15 10:58:41 2024 ] 	Batch(2800/6809) done. Loss: 0.7629  lr:0.010000
[ Mon Jul 15 10:59:04 2024 ] 	Batch(2900/6809) done. Loss: 0.6005  lr:0.010000
[ Mon Jul 15 10:59:26 2024 ] 
Training: Epoch [48/150], Step [2999], Loss: 0.701122522354126, Training Accuracy: 85.97083333333333
[ Mon Jul 15 10:59:26 2024 ] 	Batch(3000/6809) done. Loss: 0.0505  lr:0.010000
[ Mon Jul 15 10:59:49 2024 ] 	Batch(3100/6809) done. Loss: 0.7195  lr:0.010000
[ Mon Jul 15 11:00:12 2024 ] 	Batch(3200/6809) done. Loss: 0.3696  lr:0.010000
[ Mon Jul 15 11:00:35 2024 ] 	Batch(3300/6809) done. Loss: 0.3050  lr:0.010000
[ Mon Jul 15 11:00:57 2024 ] 	Batch(3400/6809) done. Loss: 0.2448  lr:0.010000
[ Mon Jul 15 11:01:20 2024 ] 
Training: Epoch [48/150], Step [3499], Loss: 0.27764734625816345, Training Accuracy: 85.8
[ Mon Jul 15 11:01:20 2024 ] 	Batch(3500/6809) done. Loss: 0.3490  lr:0.010000
[ Mon Jul 15 11:01:43 2024 ] 	Batch(3600/6809) done. Loss: 0.5583  lr:0.010000
[ Mon Jul 15 11:02:06 2024 ] 	Batch(3700/6809) done. Loss: 0.7296  lr:0.010000
[ Mon Jul 15 11:02:30 2024 ] 	Batch(3800/6809) done. Loss: 0.5384  lr:0.010000
[ Mon Jul 15 11:02:53 2024 ] 	Batch(3900/6809) done. Loss: 0.3314  lr:0.010000
[ Mon Jul 15 11:03:17 2024 ] 
Training: Epoch [48/150], Step [3999], Loss: 0.17180757224559784, Training Accuracy: 85.71562499999999
[ Mon Jul 15 11:03:17 2024 ] 	Batch(4000/6809) done. Loss: 0.8387  lr:0.010000
[ Mon Jul 15 11:03:40 2024 ] 	Batch(4100/6809) done. Loss: 0.2523  lr:0.010000
[ Mon Jul 15 11:04:04 2024 ] 	Batch(4200/6809) done. Loss: 0.2390  lr:0.010000
[ Mon Jul 15 11:04:27 2024 ] 	Batch(4300/6809) done. Loss: 0.4555  lr:0.010000
[ Mon Jul 15 11:04:50 2024 ] 	Batch(4400/6809) done. Loss: 0.3852  lr:0.010000
[ Mon Jul 15 11:05:12 2024 ] 
Training: Epoch [48/150], Step [4499], Loss: 0.49597254395484924, Training Accuracy: 85.7
[ Mon Jul 15 11:05:12 2024 ] 	Batch(4500/6809) done. Loss: 0.0396  lr:0.010000
[ Mon Jul 15 11:05:35 2024 ] 	Batch(4600/6809) done. Loss: 0.0279  lr:0.010000
[ Mon Jul 15 11:05:58 2024 ] 	Batch(4700/6809) done. Loss: 0.0807  lr:0.010000
[ Mon Jul 15 11:06:21 2024 ] 	Batch(4800/6809) done. Loss: 0.6282  lr:0.010000
[ Mon Jul 15 11:06:43 2024 ] 	Batch(4900/6809) done. Loss: 0.2884  lr:0.010000
[ Mon Jul 15 11:07:06 2024 ] 
Training: Epoch [48/150], Step [4999], Loss: 0.8149504065513611, Training Accuracy: 85.655
[ Mon Jul 15 11:07:06 2024 ] 	Batch(5000/6809) done. Loss: 0.3133  lr:0.010000
[ Mon Jul 15 11:07:29 2024 ] 	Batch(5100/6809) done. Loss: 0.4674  lr:0.010000
[ Mon Jul 15 11:07:52 2024 ] 	Batch(5200/6809) done. Loss: 0.0938  lr:0.010000
[ Mon Jul 15 11:08:14 2024 ] 	Batch(5300/6809) done. Loss: 0.5870  lr:0.010000
[ Mon Jul 15 11:08:37 2024 ] 	Batch(5400/6809) done. Loss: 0.0821  lr:0.010000
[ Mon Jul 15 11:08:59 2024 ] 
Training: Epoch [48/150], Step [5499], Loss: 0.9487116932868958, Training Accuracy: 85.63636363636363
[ Mon Jul 15 11:09:00 2024 ] 	Batch(5500/6809) done. Loss: 1.5861  lr:0.010000
[ Mon Jul 15 11:09:23 2024 ] 	Batch(5600/6809) done. Loss: 0.6484  lr:0.010000
[ Mon Jul 15 11:09:45 2024 ] 	Batch(5700/6809) done. Loss: 0.0955  lr:0.010000
[ Mon Jul 15 11:10:08 2024 ] 	Batch(5800/6809) done. Loss: 0.3123  lr:0.010000
[ Mon Jul 15 11:10:31 2024 ] 	Batch(5900/6809) done. Loss: 0.5935  lr:0.010000
[ Mon Jul 15 11:10:54 2024 ] 
Training: Epoch [48/150], Step [5999], Loss: 0.3990732431411743, Training Accuracy: 85.67291666666667
[ Mon Jul 15 11:10:55 2024 ] 	Batch(6000/6809) done. Loss: 0.1788  lr:0.010000
[ Mon Jul 15 11:11:18 2024 ] 	Batch(6100/6809) done. Loss: 0.1542  lr:0.010000
[ Mon Jul 15 11:11:42 2024 ] 	Batch(6200/6809) done. Loss: 0.8718  lr:0.010000
[ Mon Jul 15 11:12:05 2024 ] 	Batch(6300/6809) done. Loss: 0.4238  lr:0.010000
[ Mon Jul 15 11:12:28 2024 ] 	Batch(6400/6809) done. Loss: 0.5764  lr:0.010000
[ Mon Jul 15 11:12:51 2024 ] 
Training: Epoch [48/150], Step [6499], Loss: 0.6136782169342041, Training Accuracy: 85.65961538461538
[ Mon Jul 15 11:12:51 2024 ] 	Batch(6500/6809) done. Loss: 0.3479  lr:0.010000
[ Mon Jul 15 11:13:14 2024 ] 	Batch(6600/6809) done. Loss: 0.8548  lr:0.010000
[ Mon Jul 15 11:13:36 2024 ] 	Batch(6700/6809) done. Loss: 0.3454  lr:0.010000
[ Mon Jul 15 11:13:59 2024 ] 	Batch(6800/6809) done. Loss: 0.1827  lr:0.010000
[ Mon Jul 15 11:14:01 2024 ] 	Mean training loss: 0.4594.
[ Mon Jul 15 11:14:01 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 11:14:01 2024 ] Training epoch: 50
[ Mon Jul 15 11:14:02 2024 ] 	Batch(0/6809) done. Loss: 0.2868  lr:0.010000
[ Mon Jul 15 11:14:25 2024 ] 	Batch(100/6809) done. Loss: 0.3964  lr:0.010000
[ Mon Jul 15 11:14:49 2024 ] 	Batch(200/6809) done. Loss: 0.5073  lr:0.010000
[ Mon Jul 15 11:15:12 2024 ] 	Batch(300/6809) done. Loss: 0.5471  lr:0.010000
[ Mon Jul 15 11:15:35 2024 ] 	Batch(400/6809) done. Loss: 0.1177  lr:0.010000
[ Mon Jul 15 11:15:57 2024 ] 
Training: Epoch [49/150], Step [499], Loss: 0.0986424908041954, Training Accuracy: 87.825
[ Mon Jul 15 11:15:57 2024 ] 	Batch(500/6809) done. Loss: 0.3692  lr:0.010000
[ Mon Jul 15 11:16:20 2024 ] 	Batch(600/6809) done. Loss: 0.7913  lr:0.010000
[ Mon Jul 15 11:16:42 2024 ] 	Batch(700/6809) done. Loss: 0.3176  lr:0.010000
[ Mon Jul 15 11:17:05 2024 ] 	Batch(800/6809) done. Loss: 0.2827  lr:0.010000
[ Mon Jul 15 11:17:27 2024 ] 	Batch(900/6809) done. Loss: 0.4324  lr:0.010000
[ Mon Jul 15 11:17:50 2024 ] 
Training: Epoch [49/150], Step [999], Loss: 0.07530811429023743, Training Accuracy: 87.6875
[ Mon Jul 15 11:17:50 2024 ] 	Batch(1000/6809) done. Loss: 0.1766  lr:0.010000
[ Mon Jul 15 11:18:12 2024 ] 	Batch(1100/6809) done. Loss: 0.2046  lr:0.010000
[ Mon Jul 15 11:18:35 2024 ] 	Batch(1200/6809) done. Loss: 0.3096  lr:0.010000
[ Mon Jul 15 11:18:58 2024 ] 	Batch(1300/6809) done. Loss: 0.6388  lr:0.010000
[ Mon Jul 15 11:19:21 2024 ] 	Batch(1400/6809) done. Loss: 0.0110  lr:0.010000
[ Mon Jul 15 11:19:43 2024 ] 
Training: Epoch [49/150], Step [1499], Loss: 0.23850196599960327, Training Accuracy: 87.16666666666667
[ Mon Jul 15 11:19:43 2024 ] 	Batch(1500/6809) done. Loss: 1.2275  lr:0.010000
[ Mon Jul 15 11:20:06 2024 ] 	Batch(1600/6809) done. Loss: 0.3925  lr:0.010000
[ Mon Jul 15 11:20:29 2024 ] 	Batch(1700/6809) done. Loss: 0.3745  lr:0.010000
[ Mon Jul 15 11:20:52 2024 ] 	Batch(1800/6809) done. Loss: 0.5225  lr:0.010000
[ Mon Jul 15 11:21:15 2024 ] 	Batch(1900/6809) done. Loss: 1.2214  lr:0.010000
[ Mon Jul 15 11:21:37 2024 ] 
Training: Epoch [49/150], Step [1999], Loss: 0.35208049416542053, Training Accuracy: 86.49375
[ Mon Jul 15 11:21:38 2024 ] 	Batch(2000/6809) done. Loss: 0.4833  lr:0.010000
[ Mon Jul 15 11:22:01 2024 ] 	Batch(2100/6809) done. Loss: 0.0988  lr:0.010000
[ Mon Jul 15 11:22:24 2024 ] 	Batch(2200/6809) done. Loss: 0.3013  lr:0.010000
[ Mon Jul 15 11:22:47 2024 ] 	Batch(2300/6809) done. Loss: 1.1311  lr:0.010000
[ Mon Jul 15 11:23:10 2024 ] 	Batch(2400/6809) done. Loss: 0.9165  lr:0.010000
[ Mon Jul 15 11:23:33 2024 ] 
Training: Epoch [49/150], Step [2499], Loss: 0.33858436346054077, Training Accuracy: 86.31
[ Mon Jul 15 11:23:33 2024 ] 	Batch(2500/6809) done. Loss: 0.2933  lr:0.010000
[ Mon Jul 15 11:23:56 2024 ] 	Batch(2600/6809) done. Loss: 0.1415  lr:0.010000
[ Mon Jul 15 11:24:19 2024 ] 	Batch(2700/6809) done. Loss: 0.4473  lr:0.010000
[ Mon Jul 15 11:24:42 2024 ] 	Batch(2800/6809) done. Loss: 0.3930  lr:0.010000
[ Mon Jul 15 11:25:05 2024 ] 	Batch(2900/6809) done. Loss: 0.6782  lr:0.010000
[ Mon Jul 15 11:25:28 2024 ] 
Training: Epoch [49/150], Step [2999], Loss: 0.8612725734710693, Training Accuracy: 86.1125
[ Mon Jul 15 11:25:28 2024 ] 	Batch(3000/6809) done. Loss: 1.0621  lr:0.010000
[ Mon Jul 15 11:25:51 2024 ] 	Batch(3100/6809) done. Loss: 0.8938  lr:0.010000
[ Mon Jul 15 11:26:14 2024 ] 	Batch(3200/6809) done. Loss: 0.3507  lr:0.010000
[ Mon Jul 15 11:26:37 2024 ] 	Batch(3300/6809) done. Loss: 0.0941  lr:0.010000
[ Mon Jul 15 11:27:00 2024 ] 	Batch(3400/6809) done. Loss: 0.1319  lr:0.010000
[ Mon Jul 15 11:27:22 2024 ] 
Training: Epoch [49/150], Step [3499], Loss: 0.2980227470397949, Training Accuracy: 86.05000000000001
[ Mon Jul 15 11:27:23 2024 ] 	Batch(3500/6809) done. Loss: 0.0765  lr:0.010000
[ Mon Jul 15 11:27:45 2024 ] 	Batch(3600/6809) done. Loss: 0.1327  lr:0.010000
[ Mon Jul 15 11:28:08 2024 ] 	Batch(3700/6809) done. Loss: 0.6322  lr:0.010000
[ Mon Jul 15 11:28:30 2024 ] 	Batch(3800/6809) done. Loss: 0.0869  lr:0.010000
[ Mon Jul 15 11:28:53 2024 ] 	Batch(3900/6809) done. Loss: 0.4141  lr:0.010000
[ Mon Jul 15 11:29:15 2024 ] 
Training: Epoch [49/150], Step [3999], Loss: 0.5858446359634399, Training Accuracy: 86.10624999999999
[ Mon Jul 15 11:29:15 2024 ] 	Batch(4000/6809) done. Loss: 0.0789  lr:0.010000
[ Mon Jul 15 11:29:38 2024 ] 	Batch(4100/6809) done. Loss: 0.6572  lr:0.010000
[ Mon Jul 15 11:30:00 2024 ] 	Batch(4200/6809) done. Loss: 0.4032  lr:0.010000
[ Mon Jul 15 11:30:23 2024 ] 	Batch(4300/6809) done. Loss: 0.5439  lr:0.010000
[ Mon Jul 15 11:30:46 2024 ] 	Batch(4400/6809) done. Loss: 0.6480  lr:0.010000
[ Mon Jul 15 11:31:08 2024 ] 
Training: Epoch [49/150], Step [4499], Loss: 0.8667442798614502, Training Accuracy: 85.99444444444444
[ Mon Jul 15 11:31:08 2024 ] 	Batch(4500/6809) done. Loss: 0.0645  lr:0.010000
[ Mon Jul 15 11:31:31 2024 ] 	Batch(4600/6809) done. Loss: 0.1795  lr:0.010000
[ Mon Jul 15 11:31:54 2024 ] 	Batch(4700/6809) done. Loss: 0.4353  lr:0.010000
[ Mon Jul 15 11:32:17 2024 ] 	Batch(4800/6809) done. Loss: 0.3754  lr:0.010000
[ Mon Jul 15 11:32:40 2024 ] 	Batch(4900/6809) done. Loss: 1.0747  lr:0.010000
[ Mon Jul 15 11:33:02 2024 ] 
Training: Epoch [49/150], Step [4999], Loss: 0.32486552000045776, Training Accuracy: 85.995
[ Mon Jul 15 11:33:03 2024 ] 	Batch(5000/6809) done. Loss: 0.2819  lr:0.010000
[ Mon Jul 15 11:33:26 2024 ] 	Batch(5100/6809) done. Loss: 0.9794  lr:0.010000
[ Mon Jul 15 11:33:49 2024 ] 	Batch(5200/6809) done. Loss: 0.3463  lr:0.010000
[ Mon Jul 15 11:34:12 2024 ] 	Batch(5300/6809) done. Loss: 0.3689  lr:0.010000
[ Mon Jul 15 11:34:35 2024 ] 	Batch(5400/6809) done. Loss: 0.6282  lr:0.010000
[ Mon Jul 15 11:34:57 2024 ] 
Training: Epoch [49/150], Step [5499], Loss: 0.37563085556030273, Training Accuracy: 85.90454545454546
[ Mon Jul 15 11:34:58 2024 ] 	Batch(5500/6809) done. Loss: 0.9953  lr:0.010000
[ Mon Jul 15 11:35:21 2024 ] 	Batch(5600/6809) done. Loss: 0.4236  lr:0.010000
[ Mon Jul 15 11:35:44 2024 ] 	Batch(5700/6809) done. Loss: 0.5347  lr:0.010000
[ Mon Jul 15 11:36:07 2024 ] 	Batch(5800/6809) done. Loss: 0.9059  lr:0.010000
[ Mon Jul 15 11:36:30 2024 ] 	Batch(5900/6809) done. Loss: 0.0709  lr:0.010000
[ Mon Jul 15 11:36:52 2024 ] 
Training: Epoch [49/150], Step [5999], Loss: 1.342808485031128, Training Accuracy: 85.82083333333334
[ Mon Jul 15 11:36:52 2024 ] 	Batch(6000/6809) done. Loss: 0.8030  lr:0.010000
[ Mon Jul 15 11:37:15 2024 ] 	Batch(6100/6809) done. Loss: 0.5486  lr:0.010000
[ Mon Jul 15 11:37:37 2024 ] 	Batch(6200/6809) done. Loss: 0.2039  lr:0.010000
[ Mon Jul 15 11:38:00 2024 ] 	Batch(6300/6809) done. Loss: 0.8521  lr:0.010000
[ Mon Jul 15 11:38:23 2024 ] 	Batch(6400/6809) done. Loss: 0.5051  lr:0.010000
[ Mon Jul 15 11:38:45 2024 ] 
Training: Epoch [49/150], Step [6499], Loss: 0.4336511492729187, Training Accuracy: 85.85961538461538
[ Mon Jul 15 11:38:45 2024 ] 	Batch(6500/6809) done. Loss: 0.1351  lr:0.010000
[ Mon Jul 15 11:39:08 2024 ] 	Batch(6600/6809) done. Loss: 0.7415  lr:0.010000
[ Mon Jul 15 11:39:30 2024 ] 	Batch(6700/6809) done. Loss: 0.1134  lr:0.010000
[ Mon Jul 15 11:39:53 2024 ] 	Batch(6800/6809) done. Loss: 0.2031  lr:0.010000
[ Mon Jul 15 11:39:55 2024 ] 	Mean training loss: 0.4527.
[ Mon Jul 15 11:39:55 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 11:39:55 2024 ] Eval epoch: 50
[ Mon Jul 15 11:46:52 2024 ] 	Mean val loss of 7435 batches: 1.0577058767658767.
[ Mon Jul 15 11:46:52 2024 ] 
Validation: Epoch [49/150], Samples [44717.0/59477], Loss: 2.263326644897461, Validation Accuracy: 75.18368444945105
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 1 : 359 / 500 = 71 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 2 : 389 / 499 = 77 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 3 : 402 / 500 = 80 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 4 : 394 / 502 = 78 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 5 : 373 / 502 = 74 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 6 : 415 / 502 = 82 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 7 : 467 / 497 = 93 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 8 : 482 / 498 = 96 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 9 : 398 / 500 = 79 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 10 : 271 / 500 = 54 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 11 : 357 / 498 = 71 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 12 : 392 / 499 = 78 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 13 : 480 / 502 = 95 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 14 : 451 / 504 = 89 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 15 : 377 / 502 = 75 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 16 : 251 / 502 = 50 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 17 : 317 / 504 = 62 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 18 : 437 / 504 = 86 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 19 : 387 / 502 = 77 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 20 : 407 / 502 = 81 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 21 : 472 / 503 = 93 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 22 : 410 / 504 = 81 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 23 : 414 / 503 = 82 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 24 : 419 / 504 = 83 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 25 : 456 / 504 = 90 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 26 : 447 / 504 = 88 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 27 : 380 / 501 = 75 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 28 : 340 / 502 = 67 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 29 : 178 / 502 = 35 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 30 : 354 / 501 = 70 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 31 : 312 / 504 = 61 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 32 : 427 / 503 = 84 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 33 : 371 / 503 = 73 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 34 : 471 / 504 = 93 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 35 : 457 / 503 = 90 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 36 : 311 / 502 = 61 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 37 : 407 / 504 = 80 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 38 : 406 / 504 = 80 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 39 : 461 / 498 = 92 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 40 : 348 / 504 = 69 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 41 : 476 / 503 = 94 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 42 : 434 / 504 = 86 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 43 : 339 / 503 = 67 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 44 : 398 / 504 = 78 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 45 : 423 / 504 = 83 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 46 : 433 / 504 = 85 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 47 : 408 / 503 = 81 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 48 : 362 / 503 = 71 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 49 : 363 / 499 = 72 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 50 : 362 / 502 = 72 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 51 : 393 / 503 = 78 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 52 : 461 / 504 = 91 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 53 : 406 / 497 = 81 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 54 : 455 / 480 = 94 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 55 : 432 / 504 = 85 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 56 : 376 / 503 = 74 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 57 : 446 / 504 = 88 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 58 : 459 / 499 = 91 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 59 : 475 / 503 = 94 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 60 : 418 / 479 = 87 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 61 : 406 / 484 = 83 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 62 : 387 / 487 = 79 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 63 : 434 / 489 = 88 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 64 : 416 / 488 = 85 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 65 : 432 / 490 = 88 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 66 : 288 / 488 = 59 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 67 : 346 / 490 = 70 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 68 : 196 / 490 = 40 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 69 : 365 / 490 = 74 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 70 : 241 / 490 = 49 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 71 : 241 / 490 = 49 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 72 : 184 / 488 = 37 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 73 : 209 / 486 = 43 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 74 : 155 / 481 = 32 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 75 : 195 / 488 = 39 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 76 : 279 / 489 = 57 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 77 : 214 / 488 = 43 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 78 : 326 / 488 = 66 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 79 : 465 / 490 = 94 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 80 : 361 / 489 = 73 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 81 : 347 / 491 = 70 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 82 : 298 / 491 = 60 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 83 : 169 / 489 = 34 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 84 : 352 / 489 = 71 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 85 : 285 / 489 = 58 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 86 : 388 / 491 = 79 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 87 : 333 / 492 = 67 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 88 : 338 / 491 = 68 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 89 : 355 / 492 = 72 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 90 : 268 / 490 = 54 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 91 : 352 / 482 = 73 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 92 : 324 / 490 = 66 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 93 : 308 / 487 = 63 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 94 : 409 / 489 = 83 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 95 : 404 / 490 = 82 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 96 : 451 / 491 = 91 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 97 : 453 / 490 = 92 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 98 : 447 / 491 = 91 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 99 : 439 / 491 = 89 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 100 : 438 / 491 = 89 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 101 : 387 / 491 = 78 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 102 : 285 / 492 = 57 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 103 : 354 / 492 = 71 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 104 : 301 / 491 = 61 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 105 : 237 / 491 = 48 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 106 : 303 / 492 = 61 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 107 : 411 / 491 = 83 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 108 : 320 / 492 = 65 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 109 : 316 / 490 = 64 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 110 : 426 / 491 = 86 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 111 : 447 / 492 = 90 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 112 : 449 / 492 = 91 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 113 : 443 / 491 = 90 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 114 : 380 / 491 = 77 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 115 : 399 / 492 = 81 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 116 : 393 / 491 = 80 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 117 : 402 / 492 = 81 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 118 : 363 / 490 = 74 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 119 : 443 / 492 = 90 %
[ Mon Jul 15 11:46:52 2024 ] Accuracy of 120 : 394 / 500 = 78 %
[ Mon Jul 15 11:46:52 2024 ] Training epoch: 51
[ Mon Jul 15 11:46:52 2024 ] 	Batch(0/6809) done. Loss: 0.8436  lr:0.010000
[ Mon Jul 15 11:47:16 2024 ] 	Batch(100/6809) done. Loss: 0.7721  lr:0.010000
[ Mon Jul 15 11:47:39 2024 ] 	Batch(200/6809) done. Loss: 0.2418  lr:0.010000
[ Mon Jul 15 11:48:02 2024 ] 	Batch(300/6809) done. Loss: 0.1189  lr:0.010000
[ Mon Jul 15 11:48:25 2024 ] 	Batch(400/6809) done. Loss: 0.7610  lr:0.010000
[ Mon Jul 15 11:48:49 2024 ] 
Training: Epoch [50/150], Step [499], Loss: 0.15332312881946564, Training Accuracy: 86.675
[ Mon Jul 15 11:48:49 2024 ] 	Batch(500/6809) done. Loss: 0.4080  lr:0.010000
[ Mon Jul 15 11:49:13 2024 ] 	Batch(600/6809) done. Loss: 0.4104  lr:0.010000
[ Mon Jul 15 11:49:36 2024 ] 	Batch(700/6809) done. Loss: 0.6386  lr:0.010000
[ Mon Jul 15 11:50:00 2024 ] 	Batch(800/6809) done. Loss: 0.1581  lr:0.010000
[ Mon Jul 15 11:50:23 2024 ] 	Batch(900/6809) done. Loss: 0.3875  lr:0.010000
[ Mon Jul 15 11:50:46 2024 ] 
Training: Epoch [50/150], Step [999], Loss: 0.18950581550598145, Training Accuracy: 86.6875
[ Mon Jul 15 11:50:46 2024 ] 	Batch(1000/6809) done. Loss: 0.3106  lr:0.010000
[ Mon Jul 15 11:51:09 2024 ] 	Batch(1100/6809) done. Loss: 0.3184  lr:0.010000
[ Mon Jul 15 11:51:32 2024 ] 	Batch(1200/6809) done. Loss: 0.0337  lr:0.010000
[ Mon Jul 15 11:51:55 2024 ] 	Batch(1300/6809) done. Loss: 0.5284  lr:0.010000
[ Mon Jul 15 11:52:19 2024 ] 	Batch(1400/6809) done. Loss: 0.6066  lr:0.010000
[ Mon Jul 15 11:52:41 2024 ] 
Training: Epoch [50/150], Step [1499], Loss: 0.08406612277030945, Training Accuracy: 86.83333333333333
[ Mon Jul 15 11:52:42 2024 ] 	Batch(1500/6809) done. Loss: 0.1794  lr:0.010000
[ Mon Jul 15 11:53:05 2024 ] 	Batch(1600/6809) done. Loss: 0.2357  lr:0.010000
[ Mon Jul 15 11:53:29 2024 ] 	Batch(1700/6809) done. Loss: 0.2527  lr:0.010000
[ Mon Jul 15 11:53:52 2024 ] 	Batch(1800/6809) done. Loss: 0.4193  lr:0.010000
[ Mon Jul 15 11:54:15 2024 ] 	Batch(1900/6809) done. Loss: 0.5208  lr:0.010000
[ Mon Jul 15 11:54:38 2024 ] 
Training: Epoch [50/150], Step [1999], Loss: 1.2836145162582397, Training Accuracy: 86.675
[ Mon Jul 15 11:54:38 2024 ] 	Batch(2000/6809) done. Loss: 0.2528  lr:0.010000
[ Mon Jul 15 11:55:01 2024 ] 	Batch(2100/6809) done. Loss: 0.2780  lr:0.010000
[ Mon Jul 15 11:55:24 2024 ] 	Batch(2200/6809) done. Loss: 0.5531  lr:0.010000
[ Mon Jul 15 11:55:47 2024 ] 	Batch(2300/6809) done. Loss: 1.1750  lr:0.010000
[ Mon Jul 15 11:56:11 2024 ] 	Batch(2400/6809) done. Loss: 0.4423  lr:0.010000
[ Mon Jul 15 11:56:35 2024 ] 
Training: Epoch [50/150], Step [2499], Loss: 0.06885440647602081, Training Accuracy: 86.52
[ Mon Jul 15 11:56:35 2024 ] 	Batch(2500/6809) done. Loss: 0.0777  lr:0.010000
[ Mon Jul 15 11:56:59 2024 ] 	Batch(2600/6809) done. Loss: 0.6750  lr:0.010000
[ Mon Jul 15 11:57:23 2024 ] 	Batch(2700/6809) done. Loss: 0.5698  lr:0.010000
[ Mon Jul 15 11:57:47 2024 ] 	Batch(2800/6809) done. Loss: 0.1444  lr:0.010000
[ Mon Jul 15 11:58:10 2024 ] 	Batch(2900/6809) done. Loss: 0.5279  lr:0.010000
[ Mon Jul 15 11:58:33 2024 ] 
Training: Epoch [50/150], Step [2999], Loss: 0.4480176270008087, Training Accuracy: 86.3625
[ Mon Jul 15 11:58:33 2024 ] 	Batch(3000/6809) done. Loss: 0.6763  lr:0.010000
[ Mon Jul 15 11:58:56 2024 ] 	Batch(3100/6809) done. Loss: 0.8586  lr:0.010000
[ Mon Jul 15 11:59:19 2024 ] 	Batch(3200/6809) done. Loss: 0.3463  lr:0.010000
[ Mon Jul 15 11:59:43 2024 ] 	Batch(3300/6809) done. Loss: 0.2320  lr:0.010000
[ Mon Jul 15 12:00:06 2024 ] 	Batch(3400/6809) done. Loss: 0.1528  lr:0.010000
[ Mon Jul 15 12:00:29 2024 ] 
Training: Epoch [50/150], Step [3499], Loss: 0.3462115228176117, Training Accuracy: 86.19642857142857
[ Mon Jul 15 12:00:29 2024 ] 	Batch(3500/6809) done. Loss: 1.1398  lr:0.010000
[ Mon Jul 15 12:00:52 2024 ] 	Batch(3600/6809) done. Loss: 0.3309  lr:0.010000
[ Mon Jul 15 12:01:15 2024 ] 	Batch(3700/6809) done. Loss: 0.1102  lr:0.010000
[ Mon Jul 15 12:01:37 2024 ] 	Batch(3800/6809) done. Loss: 0.1334  lr:0.010000
[ Mon Jul 15 12:02:00 2024 ] 	Batch(3900/6809) done. Loss: 0.0614  lr:0.010000
[ Mon Jul 15 12:02:22 2024 ] 
Training: Epoch [50/150], Step [3999], Loss: 0.2413758635520935, Training Accuracy: 86.15
[ Mon Jul 15 12:02:22 2024 ] 	Batch(4000/6809) done. Loss: 0.4051  lr:0.010000
[ Mon Jul 15 12:02:45 2024 ] 	Batch(4100/6809) done. Loss: 0.2443  lr:0.010000
[ Mon Jul 15 12:03:07 2024 ] 	Batch(4200/6809) done. Loss: 0.5425  lr:0.010000
[ Mon Jul 15 12:03:30 2024 ] 	Batch(4300/6809) done. Loss: 0.5514  lr:0.010000
[ Mon Jul 15 12:03:53 2024 ] 	Batch(4400/6809) done. Loss: 1.2374  lr:0.010000
[ Mon Jul 15 12:04:15 2024 ] 
Training: Epoch [50/150], Step [4499], Loss: 0.47774738073349, Training Accuracy: 86.21944444444445
[ Mon Jul 15 12:04:15 2024 ] 	Batch(4500/6809) done. Loss: 0.1084  lr:0.010000
[ Mon Jul 15 12:04:38 2024 ] 	Batch(4600/6809) done. Loss: 0.7211  lr:0.010000
[ Mon Jul 15 12:05:00 2024 ] 	Batch(4700/6809) done. Loss: 0.1430  lr:0.010000
[ Mon Jul 15 12:05:23 2024 ] 	Batch(4800/6809) done. Loss: 0.7505  lr:0.010000
[ Mon Jul 15 12:05:45 2024 ] 	Batch(4900/6809) done. Loss: 1.1579  lr:0.010000
[ Mon Jul 15 12:06:08 2024 ] 
Training: Epoch [50/150], Step [4999], Loss: 0.9472383260726929, Training Accuracy: 86.115
[ Mon Jul 15 12:06:08 2024 ] 	Batch(5000/6809) done. Loss: 1.1323  lr:0.010000
[ Mon Jul 15 12:06:30 2024 ] 	Batch(5100/6809) done. Loss: 1.1867  lr:0.010000
[ Mon Jul 15 12:06:53 2024 ] 	Batch(5200/6809) done. Loss: 0.1284  lr:0.010000
[ Mon Jul 15 12:07:16 2024 ] 	Batch(5300/6809) done. Loss: 0.0152  lr:0.010000
[ Mon Jul 15 12:07:38 2024 ] 	Batch(5400/6809) done. Loss: 0.5965  lr:0.010000
[ Mon Jul 15 12:08:01 2024 ] 
Training: Epoch [50/150], Step [5499], Loss: 0.7608729004859924, Training Accuracy: 86.075
[ Mon Jul 15 12:08:01 2024 ] 	Batch(5500/6809) done. Loss: 0.6050  lr:0.010000
[ Mon Jul 15 12:08:24 2024 ] 	Batch(5600/6809) done. Loss: 0.5877  lr:0.010000
[ Mon Jul 15 12:08:46 2024 ] 	Batch(5700/6809) done. Loss: 0.2331  lr:0.010000
[ Mon Jul 15 12:09:09 2024 ] 	Batch(5800/6809) done. Loss: 0.0701  lr:0.010000
[ Mon Jul 15 12:09:31 2024 ] 	Batch(5900/6809) done. Loss: 0.8389  lr:0.010000
[ Mon Jul 15 12:09:53 2024 ] 
Training: Epoch [50/150], Step [5999], Loss: 0.1433539092540741, Training Accuracy: 85.97708333333334
[ Mon Jul 15 12:09:54 2024 ] 	Batch(6000/6809) done. Loss: 0.3292  lr:0.010000
[ Mon Jul 15 12:10:16 2024 ] 	Batch(6100/6809) done. Loss: 1.1309  lr:0.010000
[ Mon Jul 15 12:10:39 2024 ] 	Batch(6200/6809) done. Loss: 0.8287  lr:0.010000
[ Mon Jul 15 12:11:01 2024 ] 	Batch(6300/6809) done. Loss: 0.2644  lr:0.010000
[ Mon Jul 15 12:11:24 2024 ] 	Batch(6400/6809) done. Loss: 0.4703  lr:0.010000
[ Mon Jul 15 12:11:47 2024 ] 
Training: Epoch [50/150], Step [6499], Loss: 0.4483644962310791, Training Accuracy: 85.9076923076923
[ Mon Jul 15 12:11:47 2024 ] 	Batch(6500/6809) done. Loss: 0.9816  lr:0.010000
[ Mon Jul 15 12:12:10 2024 ] 	Batch(6600/6809) done. Loss: 0.4451  lr:0.010000
[ Mon Jul 15 12:12:33 2024 ] 	Batch(6700/6809) done. Loss: 0.4935  lr:0.010000
[ Mon Jul 15 12:12:55 2024 ] 	Batch(6800/6809) done. Loss: 0.8269  lr:0.010000
[ Mon Jul 15 12:12:57 2024 ] 	Mean training loss: 0.4493.
[ Mon Jul 15 12:12:57 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 12:12:57 2024 ] Training epoch: 52
[ Mon Jul 15 12:12:58 2024 ] 	Batch(0/6809) done. Loss: 0.1362  lr:0.010000
[ Mon Jul 15 12:13:21 2024 ] 	Batch(100/6809) done. Loss: 0.6753  lr:0.010000
[ Mon Jul 15 12:13:43 2024 ] 	Batch(200/6809) done. Loss: 0.1221  lr:0.010000
[ Mon Jul 15 12:14:06 2024 ] 	Batch(300/6809) done. Loss: 0.1211  lr:0.010000
[ Mon Jul 15 12:14:28 2024 ] 	Batch(400/6809) done. Loss: 0.3400  lr:0.010000
[ Mon Jul 15 12:14:51 2024 ] 
Training: Epoch [51/150], Step [499], Loss: 0.6972671747207642, Training Accuracy: 86.85000000000001
[ Mon Jul 15 12:14:51 2024 ] 	Batch(500/6809) done. Loss: 0.3702  lr:0.010000
[ Mon Jul 15 12:15:13 2024 ] 	Batch(600/6809) done. Loss: 0.5684  lr:0.010000
[ Mon Jul 15 12:15:36 2024 ] 	Batch(700/6809) done. Loss: 0.1139  lr:0.010000
[ Mon Jul 15 12:15:58 2024 ] 	Batch(800/6809) done. Loss: 0.2049  lr:0.010000
[ Mon Jul 15 12:16:22 2024 ] 	Batch(900/6809) done. Loss: 1.0314  lr:0.010000
[ Mon Jul 15 12:16:44 2024 ] 
Training: Epoch [51/150], Step [999], Loss: 0.7661786675453186, Training Accuracy: 87.52499999999999
[ Mon Jul 15 12:16:44 2024 ] 	Batch(1000/6809) done. Loss: 0.3996  lr:0.010000
[ Mon Jul 15 12:17:07 2024 ] 	Batch(1100/6809) done. Loss: 0.7100  lr:0.010000
[ Mon Jul 15 12:17:30 2024 ] 	Batch(1200/6809) done. Loss: 0.5562  lr:0.010000
[ Mon Jul 15 12:17:53 2024 ] 	Batch(1300/6809) done. Loss: 0.2274  lr:0.010000
[ Mon Jul 15 12:18:15 2024 ] 	Batch(1400/6809) done. Loss: 1.0734  lr:0.010000
[ Mon Jul 15 12:18:37 2024 ] 
Training: Epoch [51/150], Step [1499], Loss: 0.5571739673614502, Training Accuracy: 87.15
[ Mon Jul 15 12:18:38 2024 ] 	Batch(1500/6809) done. Loss: 0.6405  lr:0.010000
[ Mon Jul 15 12:19:00 2024 ] 	Batch(1600/6809) done. Loss: 0.0842  lr:0.010000
[ Mon Jul 15 12:19:23 2024 ] 	Batch(1700/6809) done. Loss: 0.3079  lr:0.010000
[ Mon Jul 15 12:19:46 2024 ] 	Batch(1800/6809) done. Loss: 1.0174  lr:0.010000
[ Mon Jul 15 12:20:08 2024 ] 	Batch(1900/6809) done. Loss: 0.0595  lr:0.010000
[ Mon Jul 15 12:20:30 2024 ] 
Training: Epoch [51/150], Step [1999], Loss: 0.4623095691204071, Training Accuracy: 86.64375
[ Mon Jul 15 12:20:31 2024 ] 	Batch(2000/6809) done. Loss: 0.2250  lr:0.010000
[ Mon Jul 15 12:20:53 2024 ] 	Batch(2100/6809) done. Loss: 0.1934  lr:0.010000
[ Mon Jul 15 12:21:17 2024 ] 	Batch(2200/6809) done. Loss: 0.1879  lr:0.010000
[ Mon Jul 15 12:21:39 2024 ] 	Batch(2300/6809) done. Loss: 0.6351  lr:0.010000
[ Mon Jul 15 12:22:02 2024 ] 	Batch(2400/6809) done. Loss: 0.0096  lr:0.010000
[ Mon Jul 15 12:22:24 2024 ] 
Training: Epoch [51/150], Step [2499], Loss: 0.2007976472377777, Training Accuracy: 86.47500000000001
[ Mon Jul 15 12:22:24 2024 ] 	Batch(2500/6809) done. Loss: 0.2524  lr:0.010000
[ Mon Jul 15 12:22:47 2024 ] 	Batch(2600/6809) done. Loss: 0.1199  lr:0.010000
[ Mon Jul 15 12:23:09 2024 ] 	Batch(2700/6809) done. Loss: 0.4728  lr:0.010000
[ Mon Jul 15 12:23:32 2024 ] 	Batch(2800/6809) done. Loss: 0.3450  lr:0.010000
[ Mon Jul 15 12:23:55 2024 ] 	Batch(2900/6809) done. Loss: 0.9469  lr:0.010000
[ Mon Jul 15 12:24:17 2024 ] 
Training: Epoch [51/150], Step [2999], Loss: 0.23429270088672638, Training Accuracy: 86.5375
[ Mon Jul 15 12:24:17 2024 ] 	Batch(3000/6809) done. Loss: 0.2289  lr:0.010000
[ Mon Jul 15 12:24:40 2024 ] 	Batch(3100/6809) done. Loss: 0.1837  lr:0.010000
[ Mon Jul 15 12:25:02 2024 ] 	Batch(3200/6809) done. Loss: 0.2176  lr:0.010000
[ Mon Jul 15 12:25:26 2024 ] 	Batch(3300/6809) done. Loss: 0.2045  lr:0.010000
[ Mon Jul 15 12:25:49 2024 ] 	Batch(3400/6809) done. Loss: 0.2832  lr:0.010000
[ Mon Jul 15 12:26:11 2024 ] 
Training: Epoch [51/150], Step [3499], Loss: 0.3409985303878784, Training Accuracy: 86.47857142857143
[ Mon Jul 15 12:26:12 2024 ] 	Batch(3500/6809) done. Loss: 0.5285  lr:0.010000
[ Mon Jul 15 12:26:35 2024 ] 	Batch(3600/6809) done. Loss: 0.3196  lr:0.010000
[ Mon Jul 15 12:26:57 2024 ] 	Batch(3700/6809) done. Loss: 0.2049  lr:0.010000
[ Mon Jul 15 12:27:20 2024 ] 	Batch(3800/6809) done. Loss: 0.7244  lr:0.010000
[ Mon Jul 15 12:27:43 2024 ] 	Batch(3900/6809) done. Loss: 1.1564  lr:0.010000
[ Mon Jul 15 12:28:05 2024 ] 
Training: Epoch [51/150], Step [3999], Loss: 0.3226715326309204, Training Accuracy: 86.384375
[ Mon Jul 15 12:28:06 2024 ] 	Batch(4000/6809) done. Loss: 0.2900  lr:0.010000
[ Mon Jul 15 12:28:29 2024 ] 	Batch(4100/6809) done. Loss: 0.0341  lr:0.010000
[ Mon Jul 15 12:28:52 2024 ] 	Batch(4200/6809) done. Loss: 0.4237  lr:0.010000
[ Mon Jul 15 12:29:15 2024 ] 	Batch(4300/6809) done. Loss: 0.4453  lr:0.010000
[ Mon Jul 15 12:29:38 2024 ] 	Batch(4400/6809) done. Loss: 0.3196  lr:0.010000
[ Mon Jul 15 12:30:00 2024 ] 
Training: Epoch [51/150], Step [4499], Loss: 1.2300989627838135, Training Accuracy: 86.19444444444444
[ Mon Jul 15 12:30:01 2024 ] 	Batch(4500/6809) done. Loss: 0.3421  lr:0.010000
[ Mon Jul 15 12:30:24 2024 ] 	Batch(4600/6809) done. Loss: 0.7655  lr:0.010000
[ Mon Jul 15 12:30:47 2024 ] 	Batch(4700/6809) done. Loss: 1.4840  lr:0.010000
[ Mon Jul 15 12:31:10 2024 ] 	Batch(4800/6809) done. Loss: 0.3825  lr:0.010000
[ Mon Jul 15 12:31:33 2024 ] 	Batch(4900/6809) done. Loss: 0.7259  lr:0.010000
[ Mon Jul 15 12:31:56 2024 ] 
Training: Epoch [51/150], Step [4999], Loss: 0.8453301191329956, Training Accuracy: 86.16749999999999
[ Mon Jul 15 12:31:56 2024 ] 	Batch(5000/6809) done. Loss: 0.2622  lr:0.010000
[ Mon Jul 15 12:32:19 2024 ] 	Batch(5100/6809) done. Loss: 0.4114  lr:0.010000
[ Mon Jul 15 12:32:42 2024 ] 	Batch(5200/6809) done. Loss: 0.2622  lr:0.010000
[ Mon Jul 15 12:33:04 2024 ] 	Batch(5300/6809) done. Loss: 0.5227  lr:0.010000
[ Mon Jul 15 12:33:27 2024 ] 	Batch(5400/6809) done. Loss: 0.2128  lr:0.010000
[ Mon Jul 15 12:33:49 2024 ] 
Training: Epoch [51/150], Step [5499], Loss: 0.2592688202857971, Training Accuracy: 86.10909090909091
[ Mon Jul 15 12:33:49 2024 ] 	Batch(5500/6809) done. Loss: 0.3188  lr:0.010000
[ Mon Jul 15 12:34:12 2024 ] 	Batch(5600/6809) done. Loss: 0.5970  lr:0.010000
[ Mon Jul 15 12:34:35 2024 ] 	Batch(5700/6809) done. Loss: 0.2839  lr:0.010000
[ Mon Jul 15 12:34:57 2024 ] 	Batch(5800/6809) done. Loss: 0.0800  lr:0.010000
[ Mon Jul 15 12:35:20 2024 ] 	Batch(5900/6809) done. Loss: 0.2456  lr:0.010000
[ Mon Jul 15 12:35:42 2024 ] 
Training: Epoch [51/150], Step [5999], Loss: 0.89093416929245, Training Accuracy: 86.02916666666667
[ Mon Jul 15 12:35:42 2024 ] 	Batch(6000/6809) done. Loss: 1.0642  lr:0.010000
[ Mon Jul 15 12:36:05 2024 ] 	Batch(6100/6809) done. Loss: 0.3408  lr:0.010000
[ Mon Jul 15 12:36:27 2024 ] 	Batch(6200/6809) done. Loss: 1.4711  lr:0.010000
[ Mon Jul 15 12:36:50 2024 ] 	Batch(6300/6809) done. Loss: 0.2770  lr:0.010000
[ Mon Jul 15 12:37:13 2024 ] 	Batch(6400/6809) done. Loss: 0.3314  lr:0.010000
[ Mon Jul 15 12:37:36 2024 ] 
Training: Epoch [51/150], Step [6499], Loss: 0.20067985355854034, Training Accuracy: 85.92692307692307
[ Mon Jul 15 12:37:36 2024 ] 	Batch(6500/6809) done. Loss: 0.5312  lr:0.010000
[ Mon Jul 15 12:37:59 2024 ] 	Batch(6600/6809) done. Loss: 0.3142  lr:0.010000
[ Mon Jul 15 12:38:22 2024 ] 	Batch(6700/6809) done. Loss: 0.4499  lr:0.010000
[ Mon Jul 15 12:38:46 2024 ] 	Batch(6800/6809) done. Loss: 0.8056  lr:0.010000
[ Mon Jul 15 12:38:48 2024 ] 	Mean training loss: 0.4426.
[ Mon Jul 15 12:38:48 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 12:38:48 2024 ] Training epoch: 53
[ Mon Jul 15 12:38:49 2024 ] 	Batch(0/6809) done. Loss: 0.2447  lr:0.010000
[ Mon Jul 15 12:39:12 2024 ] 	Batch(100/6809) done. Loss: 0.0875  lr:0.010000
[ Mon Jul 15 12:39:35 2024 ] 	Batch(200/6809) done. Loss: 0.8918  lr:0.010000
[ Mon Jul 15 12:39:58 2024 ] 	Batch(300/6809) done. Loss: 1.1778  lr:0.010000
[ Mon Jul 15 12:40:21 2024 ] 	Batch(400/6809) done. Loss: 0.1953  lr:0.010000
[ Mon Jul 15 12:40:44 2024 ] 
Training: Epoch [52/150], Step [499], Loss: 0.44974011182785034, Training Accuracy: 87.25
[ Mon Jul 15 12:40:44 2024 ] 	Batch(500/6809) done. Loss: 0.0214  lr:0.010000
[ Mon Jul 15 12:41:07 2024 ] 	Batch(600/6809) done. Loss: 0.6134  lr:0.010000
[ Mon Jul 15 12:41:29 2024 ] 	Batch(700/6809) done. Loss: 0.2008  lr:0.010000
[ Mon Jul 15 12:41:52 2024 ] 	Batch(800/6809) done. Loss: 0.6983  lr:0.010000
[ Mon Jul 15 12:42:15 2024 ] 	Batch(900/6809) done. Loss: 0.1321  lr:0.010000
[ Mon Jul 15 12:42:38 2024 ] 
Training: Epoch [52/150], Step [999], Loss: 0.1406773328781128, Training Accuracy: 87.38749999999999
[ Mon Jul 15 12:42:38 2024 ] 	Batch(1000/6809) done. Loss: 0.2458  lr:0.010000
[ Mon Jul 15 12:43:01 2024 ] 	Batch(1100/6809) done. Loss: 0.0841  lr:0.010000
[ Mon Jul 15 12:43:24 2024 ] 	Batch(1200/6809) done. Loss: 0.6497  lr:0.010000
[ Mon Jul 15 12:43:47 2024 ] 	Batch(1300/6809) done. Loss: 0.4340  lr:0.010000
[ Mon Jul 15 12:44:10 2024 ] 	Batch(1400/6809) done. Loss: 0.6143  lr:0.010000
[ Mon Jul 15 12:44:32 2024 ] 
Training: Epoch [52/150], Step [1499], Loss: 0.2500988245010376, Training Accuracy: 87.125
[ Mon Jul 15 12:44:33 2024 ] 	Batch(1500/6809) done. Loss: 0.2181  lr:0.010000
[ Mon Jul 15 12:44:56 2024 ] 	Batch(1600/6809) done. Loss: 0.0789  lr:0.010000
[ Mon Jul 15 12:45:19 2024 ] 	Batch(1700/6809) done. Loss: 0.0222  lr:0.010000
[ Mon Jul 15 12:45:42 2024 ] 	Batch(1800/6809) done. Loss: 0.1067  lr:0.010000
[ Mon Jul 15 12:46:05 2024 ] 	Batch(1900/6809) done. Loss: 0.4504  lr:0.010000
[ Mon Jul 15 12:46:28 2024 ] 
Training: Epoch [52/150], Step [1999], Loss: 0.20440950989723206, Training Accuracy: 87.15
[ Mon Jul 15 12:46:28 2024 ] 	Batch(2000/6809) done. Loss: 0.6583  lr:0.010000
[ Mon Jul 15 12:46:51 2024 ] 	Batch(2100/6809) done. Loss: 1.2629  lr:0.010000
[ Mon Jul 15 12:47:14 2024 ] 	Batch(2200/6809) done. Loss: 1.2071  lr:0.010000
[ Mon Jul 15 12:47:37 2024 ] 	Batch(2300/6809) done. Loss: 0.3752  lr:0.010000
[ Mon Jul 15 12:48:00 2024 ] 	Batch(2400/6809) done. Loss: 0.4884  lr:0.010000
[ Mon Jul 15 12:48:23 2024 ] 
Training: Epoch [52/150], Step [2499], Loss: 1.3010454177856445, Training Accuracy: 86.77
[ Mon Jul 15 12:48:23 2024 ] 	Batch(2500/6809) done. Loss: 0.6491  lr:0.010000
[ Mon Jul 15 12:48:46 2024 ] 	Batch(2600/6809) done. Loss: 0.2320  lr:0.010000
[ Mon Jul 15 12:49:09 2024 ] 	Batch(2700/6809) done. Loss: 0.6406  lr:0.010000
[ Mon Jul 15 12:49:32 2024 ] 	Batch(2800/6809) done. Loss: 0.6924  lr:0.010000
[ Mon Jul 15 12:49:55 2024 ] 	Batch(2900/6809) done. Loss: 0.7588  lr:0.010000
[ Mon Jul 15 12:50:18 2024 ] 
Training: Epoch [52/150], Step [2999], Loss: 0.8986004590988159, Training Accuracy: 86.5
[ Mon Jul 15 12:50:18 2024 ] 	Batch(3000/6809) done. Loss: 1.1486  lr:0.010000
[ Mon Jul 15 12:50:41 2024 ] 	Batch(3100/6809) done. Loss: 0.2134  lr:0.010000
[ Mon Jul 15 12:51:04 2024 ] 	Batch(3200/6809) done. Loss: 0.1468  lr:0.010000
[ Mon Jul 15 12:51:27 2024 ] 	Batch(3300/6809) done. Loss: 0.3437  lr:0.010000
[ Mon Jul 15 12:51:50 2024 ] 	Batch(3400/6809) done. Loss: 0.1154  lr:0.010000
[ Mon Jul 15 12:52:12 2024 ] 
Training: Epoch [52/150], Step [3499], Loss: 0.2169506549835205, Training Accuracy: 86.49642857142858
[ Mon Jul 15 12:52:13 2024 ] 	Batch(3500/6809) done. Loss: 0.6017  lr:0.010000
[ Mon Jul 15 12:52:36 2024 ] 	Batch(3600/6809) done. Loss: 0.5124  lr:0.010000
[ Mon Jul 15 12:52:59 2024 ] 	Batch(3700/6809) done. Loss: 0.1419  lr:0.010000
[ Mon Jul 15 12:53:21 2024 ] 	Batch(3800/6809) done. Loss: 0.7080  lr:0.010000
[ Mon Jul 15 12:53:44 2024 ] 	Batch(3900/6809) done. Loss: 0.4833  lr:0.010000
[ Mon Jul 15 12:54:06 2024 ] 
Training: Epoch [52/150], Step [3999], Loss: 0.3127131760120392, Training Accuracy: 86.51875
[ Mon Jul 15 12:54:07 2024 ] 	Batch(4000/6809) done. Loss: 0.6201  lr:0.010000
[ Mon Jul 15 12:54:29 2024 ] 	Batch(4100/6809) done. Loss: 0.6365  lr:0.010000
[ Mon Jul 15 12:54:52 2024 ] 	Batch(4200/6809) done. Loss: 0.6801  lr:0.010000
[ Mon Jul 15 12:55:14 2024 ] 	Batch(4300/6809) done. Loss: 0.1074  lr:0.010000
[ Mon Jul 15 12:55:37 2024 ] 	Batch(4400/6809) done. Loss: 0.3807  lr:0.010000
[ Mon Jul 15 12:55:59 2024 ] 
Training: Epoch [52/150], Step [4499], Loss: 0.4908355176448822, Training Accuracy: 86.41666666666666
[ Mon Jul 15 12:55:59 2024 ] 	Batch(4500/6809) done. Loss: 0.2986  lr:0.010000
[ Mon Jul 15 12:56:22 2024 ] 	Batch(4600/6809) done. Loss: 1.0066  lr:0.010000
[ Mon Jul 15 12:56:45 2024 ] 	Batch(4700/6809) done. Loss: 0.2971  lr:0.010000
[ Mon Jul 15 12:57:09 2024 ] 	Batch(4800/6809) done. Loss: 0.4648  lr:0.010000
[ Mon Jul 15 12:57:32 2024 ] 	Batch(4900/6809) done. Loss: 0.9388  lr:0.010000
[ Mon Jul 15 12:57:55 2024 ] 
Training: Epoch [52/150], Step [4999], Loss: 0.6719462871551514, Training Accuracy: 86.30250000000001
[ Mon Jul 15 12:57:55 2024 ] 	Batch(5000/6809) done. Loss: 0.1773  lr:0.010000
[ Mon Jul 15 12:58:18 2024 ] 	Batch(5100/6809) done. Loss: 0.2360  lr:0.010000
[ Mon Jul 15 12:58:41 2024 ] 	Batch(5200/6809) done. Loss: 0.2564  lr:0.010000
[ Mon Jul 15 12:59:04 2024 ] 	Batch(5300/6809) done. Loss: 0.8404  lr:0.010000
[ Mon Jul 15 12:59:27 2024 ] 	Batch(5400/6809) done. Loss: 0.1141  lr:0.010000
[ Mon Jul 15 12:59:50 2024 ] 
Training: Epoch [52/150], Step [5499], Loss: 0.1757502406835556, Training Accuracy: 86.24772727272727
[ Mon Jul 15 12:59:50 2024 ] 	Batch(5500/6809) done. Loss: 0.7255  lr:0.010000
[ Mon Jul 15 13:00:13 2024 ] 	Batch(5600/6809) done. Loss: 0.3990  lr:0.010000
[ Mon Jul 15 13:00:36 2024 ] 	Batch(5700/6809) done. Loss: 0.4000  lr:0.010000
[ Mon Jul 15 13:00:59 2024 ] 	Batch(5800/6809) done. Loss: 0.5988  lr:0.010000
[ Mon Jul 15 13:01:22 2024 ] 	Batch(5900/6809) done. Loss: 0.2896  lr:0.010000
[ Mon Jul 15 13:01:45 2024 ] 
Training: Epoch [52/150], Step [5999], Loss: 0.4996233284473419, Training Accuracy: 86.18541666666667
[ Mon Jul 15 13:01:45 2024 ] 	Batch(6000/6809) done. Loss: 0.8775  lr:0.010000
[ Mon Jul 15 13:02:08 2024 ] 	Batch(6100/6809) done. Loss: 0.4425  lr:0.010000
[ Mon Jul 15 13:02:31 2024 ] 	Batch(6200/6809) done. Loss: 0.6068  lr:0.010000
[ Mon Jul 15 13:02:54 2024 ] 	Batch(6300/6809) done. Loss: 0.1238  lr:0.010000
[ Mon Jul 15 13:03:17 2024 ] 	Batch(6400/6809) done. Loss: 0.4864  lr:0.010000
[ Mon Jul 15 13:03:40 2024 ] 
Training: Epoch [52/150], Step [6499], Loss: 0.48529675602912903, Training Accuracy: 86.17692307692307
[ Mon Jul 15 13:03:40 2024 ] 	Batch(6500/6809) done. Loss: 0.1818  lr:0.010000
[ Mon Jul 15 13:04:03 2024 ] 	Batch(6600/6809) done. Loss: 0.1506  lr:0.010000
[ Mon Jul 15 13:04:26 2024 ] 	Batch(6700/6809) done. Loss: 0.6302  lr:0.010000
[ Mon Jul 15 13:04:49 2024 ] 	Batch(6800/6809) done. Loss: 0.9644  lr:0.010000
[ Mon Jul 15 13:04:51 2024 ] 	Mean training loss: 0.4397.
[ Mon Jul 15 13:04:51 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 13:04:51 2024 ] Training epoch: 54
[ Mon Jul 15 13:04:52 2024 ] 	Batch(0/6809) done. Loss: 0.2069  lr:0.010000
[ Mon Jul 15 13:05:15 2024 ] 	Batch(100/6809) done. Loss: 0.4393  lr:0.010000
[ Mon Jul 15 13:05:38 2024 ] 	Batch(200/6809) done. Loss: 0.4079  lr:0.010000
[ Mon Jul 15 13:06:01 2024 ] 	Batch(300/6809) done. Loss: 0.2426  lr:0.010000
[ Mon Jul 15 13:06:25 2024 ] 	Batch(400/6809) done. Loss: 0.2365  lr:0.010000
[ Mon Jul 15 13:06:47 2024 ] 
Training: Epoch [53/150], Step [499], Loss: 0.42048266530036926, Training Accuracy: 87.35000000000001
[ Mon Jul 15 13:06:47 2024 ] 	Batch(500/6809) done. Loss: 0.1197  lr:0.010000
[ Mon Jul 15 13:07:10 2024 ] 	Batch(600/6809) done. Loss: 0.2986  lr:0.010000
[ Mon Jul 15 13:07:33 2024 ] 	Batch(700/6809) done. Loss: 0.1252  lr:0.010000
[ Mon Jul 15 13:07:55 2024 ] 	Batch(800/6809) done. Loss: 0.4694  lr:0.010000
[ Mon Jul 15 13:08:18 2024 ] 	Batch(900/6809) done. Loss: 0.2268  lr:0.010000
[ Mon Jul 15 13:08:40 2024 ] 
Training: Epoch [53/150], Step [999], Loss: 0.11073872447013855, Training Accuracy: 87.3125
[ Mon Jul 15 13:08:41 2024 ] 	Batch(1000/6809) done. Loss: 0.0786  lr:0.010000
[ Mon Jul 15 13:09:03 2024 ] 	Batch(1100/6809) done. Loss: 0.0954  lr:0.010000
[ Mon Jul 15 13:09:26 2024 ] 	Batch(1200/6809) done. Loss: 0.3720  lr:0.010000
[ Mon Jul 15 13:09:49 2024 ] 	Batch(1300/6809) done. Loss: 0.1385  lr:0.010000
[ Mon Jul 15 13:10:11 2024 ] 	Batch(1400/6809) done. Loss: 0.4452  lr:0.010000
[ Mon Jul 15 13:10:34 2024 ] 
Training: Epoch [53/150], Step [1499], Loss: 0.645606279373169, Training Accuracy: 87.23333333333333
[ Mon Jul 15 13:10:34 2024 ] 	Batch(1500/6809) done. Loss: 0.2583  lr:0.010000
[ Mon Jul 15 13:10:57 2024 ] 	Batch(1600/6809) done. Loss: 0.3124  lr:0.010000
[ Mon Jul 15 13:11:20 2024 ] 	Batch(1700/6809) done. Loss: 0.0701  lr:0.010000
[ Mon Jul 15 13:11:43 2024 ] 	Batch(1800/6809) done. Loss: 0.9227  lr:0.010000
[ Mon Jul 15 13:12:06 2024 ] 	Batch(1900/6809) done. Loss: 0.5067  lr:0.010000
[ Mon Jul 15 13:12:29 2024 ] 
Training: Epoch [53/150], Step [1999], Loss: 0.2833544611930847, Training Accuracy: 87.35000000000001
[ Mon Jul 15 13:12:30 2024 ] 	Batch(2000/6809) done. Loss: 0.3299  lr:0.010000
[ Mon Jul 15 13:12:53 2024 ] 	Batch(2100/6809) done. Loss: 0.4821  lr:0.010000
[ Mon Jul 15 13:13:16 2024 ] 	Batch(2200/6809) done. Loss: 1.1525  lr:0.010000
[ Mon Jul 15 13:13:39 2024 ] 	Batch(2300/6809) done. Loss: 0.0093  lr:0.010000
[ Mon Jul 15 13:14:02 2024 ] 	Batch(2400/6809) done. Loss: 0.2170  lr:0.010000
[ Mon Jul 15 13:14:25 2024 ] 
Training: Epoch [53/150], Step [2499], Loss: 0.36024513840675354, Training Accuracy: 87.125
[ Mon Jul 15 13:14:25 2024 ] 	Batch(2500/6809) done. Loss: 1.4622  lr:0.010000
[ Mon Jul 15 13:14:48 2024 ] 	Batch(2600/6809) done. Loss: 0.2602  lr:0.010000
[ Mon Jul 15 13:15:10 2024 ] 	Batch(2700/6809) done. Loss: 0.1750  lr:0.010000
[ Mon Jul 15 13:15:34 2024 ] 	Batch(2800/6809) done. Loss: 1.0848  lr:0.010000
[ Mon Jul 15 13:15:57 2024 ] 	Batch(2900/6809) done. Loss: 0.4947  lr:0.010000
[ Mon Jul 15 13:16:19 2024 ] 
Training: Epoch [53/150], Step [2999], Loss: 0.6998710036277771, Training Accuracy: 86.98333333333333
[ Mon Jul 15 13:16:19 2024 ] 	Batch(3000/6809) done. Loss: 0.3723  lr:0.010000
[ Mon Jul 15 13:16:42 2024 ] 	Batch(3100/6809) done. Loss: 0.3426  lr:0.010000
[ Mon Jul 15 13:17:04 2024 ] 	Batch(3200/6809) done. Loss: 0.1867  lr:0.010000
[ Mon Jul 15 13:17:27 2024 ] 	Batch(3300/6809) done. Loss: 0.5322  lr:0.010000
[ Mon Jul 15 13:17:50 2024 ] 	Batch(3400/6809) done. Loss: 0.0970  lr:0.010000
[ Mon Jul 15 13:18:13 2024 ] 
Training: Epoch [53/150], Step [3499], Loss: 0.15701663494110107, Training Accuracy: 86.88928571428572
[ Mon Jul 15 13:18:13 2024 ] 	Batch(3500/6809) done. Loss: 0.0372  lr:0.010000
[ Mon Jul 15 13:18:36 2024 ] 	Batch(3600/6809) done. Loss: 0.8931  lr:0.010000
[ Mon Jul 15 13:19:00 2024 ] 	Batch(3700/6809) done. Loss: 0.0147  lr:0.010000
[ Mon Jul 15 13:19:23 2024 ] 	Batch(3800/6809) done. Loss: 0.4755  lr:0.010000
[ Mon Jul 15 13:19:46 2024 ] 	Batch(3900/6809) done. Loss: 0.4411  lr:0.010000
[ Mon Jul 15 13:20:09 2024 ] 
Training: Epoch [53/150], Step [3999], Loss: 0.05098389834165573, Training Accuracy: 86.80624999999999
[ Mon Jul 15 13:20:09 2024 ] 	Batch(4000/6809) done. Loss: 0.6475  lr:0.010000
[ Mon Jul 15 13:20:33 2024 ] 	Batch(4100/6809) done. Loss: 0.1081  lr:0.010000
[ Mon Jul 15 13:20:56 2024 ] 	Batch(4200/6809) done. Loss: 0.4125  lr:0.010000
[ Mon Jul 15 13:21:20 2024 ] 	Batch(4300/6809) done. Loss: 0.3523  lr:0.010000
[ Mon Jul 15 13:21:43 2024 ] 	Batch(4400/6809) done. Loss: 0.1659  lr:0.010000
[ Mon Jul 15 13:22:06 2024 ] 
Training: Epoch [53/150], Step [4499], Loss: 0.1455404907464981, Training Accuracy: 86.6861111111111
[ Mon Jul 15 13:22:06 2024 ] 	Batch(4500/6809) done. Loss: 0.0803  lr:0.010000
[ Mon Jul 15 13:22:29 2024 ] 	Batch(4600/6809) done. Loss: 0.9740  lr:0.010000
[ Mon Jul 15 13:22:52 2024 ] 	Batch(4700/6809) done. Loss: 0.1237  lr:0.010000
[ Mon Jul 15 13:23:15 2024 ] 	Batch(4800/6809) done. Loss: 0.4758  lr:0.010000
[ Mon Jul 15 13:23:38 2024 ] 	Batch(4900/6809) done. Loss: 1.0239  lr:0.010000
[ Mon Jul 15 13:24:00 2024 ] 
Training: Epoch [53/150], Step [4999], Loss: 0.27967092394828796, Training Accuracy: 86.6225
[ Mon Jul 15 13:24:00 2024 ] 	Batch(5000/6809) done. Loss: 0.2529  lr:0.010000
[ Mon Jul 15 13:24:23 2024 ] 	Batch(5100/6809) done. Loss: 0.5433  lr:0.010000
[ Mon Jul 15 13:24:46 2024 ] 	Batch(5200/6809) done. Loss: 1.0450  lr:0.010000
[ Mon Jul 15 13:25:09 2024 ] 	Batch(5300/6809) done. Loss: 0.1759  lr:0.010000
[ Mon Jul 15 13:25:32 2024 ] 	Batch(5400/6809) done. Loss: 0.0863  lr:0.010000
[ Mon Jul 15 13:25:55 2024 ] 
Training: Epoch [53/150], Step [5499], Loss: 0.12981438636779785, Training Accuracy: 86.50909090909092
[ Mon Jul 15 13:25:55 2024 ] 	Batch(5500/6809) done. Loss: 0.1248  lr:0.010000
[ Mon Jul 15 13:26:18 2024 ] 	Batch(5600/6809) done. Loss: 0.1335  lr:0.010000
[ Mon Jul 15 13:26:40 2024 ] 	Batch(5700/6809) done. Loss: 0.3405  lr:0.010000
[ Mon Jul 15 13:27:03 2024 ] 	Batch(5800/6809) done. Loss: 0.2061  lr:0.010000
[ Mon Jul 15 13:27:26 2024 ] 	Batch(5900/6809) done. Loss: 0.1144  lr:0.010000
[ Mon Jul 15 13:27:49 2024 ] 
Training: Epoch [53/150], Step [5999], Loss: 0.6518273949623108, Training Accuracy: 86.46666666666667
[ Mon Jul 15 13:27:49 2024 ] 	Batch(6000/6809) done. Loss: 0.9346  lr:0.010000
[ Mon Jul 15 13:28:12 2024 ] 	Batch(6100/6809) done. Loss: 0.4461  lr:0.010000
[ Mon Jul 15 13:28:35 2024 ] 	Batch(6200/6809) done. Loss: 0.9173  lr:0.010000
[ Mon Jul 15 13:28:58 2024 ] 	Batch(6300/6809) done. Loss: 0.7363  lr:0.010000
[ Mon Jul 15 13:29:21 2024 ] 	Batch(6400/6809) done. Loss: 0.8627  lr:0.010000
[ Mon Jul 15 13:29:44 2024 ] 
Training: Epoch [53/150], Step [6499], Loss: 0.2977859079837799, Training Accuracy: 86.30384615384615
[ Mon Jul 15 13:29:44 2024 ] 	Batch(6500/6809) done. Loss: 0.9862  lr:0.010000
[ Mon Jul 15 13:30:07 2024 ] 	Batch(6600/6809) done. Loss: 0.8068  lr:0.010000
[ Mon Jul 15 13:30:30 2024 ] 	Batch(6700/6809) done. Loss: 0.3744  lr:0.010000
[ Mon Jul 15 13:30:54 2024 ] 	Batch(6800/6809) done. Loss: 0.0844  lr:0.010000
[ Mon Jul 15 13:30:55 2024 ] 	Mean training loss: 0.4279.
[ Mon Jul 15 13:30:55 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 13:30:56 2024 ] Training epoch: 55
[ Mon Jul 15 13:30:56 2024 ] 	Batch(0/6809) done. Loss: 0.7064  lr:0.010000
[ Mon Jul 15 13:31:19 2024 ] 	Batch(100/6809) done. Loss: 0.1984  lr:0.010000
[ Mon Jul 15 13:31:42 2024 ] 	Batch(200/6809) done. Loss: 0.8406  lr:0.010000
[ Mon Jul 15 13:32:05 2024 ] 	Batch(300/6809) done. Loss: 0.0599  lr:0.010000
[ Mon Jul 15 13:32:28 2024 ] 	Batch(400/6809) done. Loss: 0.0031  lr:0.010000
[ Mon Jul 15 13:32:50 2024 ] 
Training: Epoch [54/150], Step [499], Loss: 0.4061383008956909, Training Accuracy: 87.5
[ Mon Jul 15 13:32:50 2024 ] 	Batch(500/6809) done. Loss: 0.2333  lr:0.010000
[ Mon Jul 15 13:33:13 2024 ] 	Batch(600/6809) done. Loss: 0.2180  lr:0.010000
[ Mon Jul 15 13:33:36 2024 ] 	Batch(700/6809) done. Loss: 0.1842  lr:0.010000
[ Mon Jul 15 13:33:59 2024 ] 	Batch(800/6809) done. Loss: 0.6987  lr:0.010000
[ Mon Jul 15 13:34:22 2024 ] 	Batch(900/6809) done. Loss: 0.2771  lr:0.010000
[ Mon Jul 15 13:34:45 2024 ] 
Training: Epoch [54/150], Step [999], Loss: 0.3907388746738434, Training Accuracy: 87.2125
[ Mon Jul 15 13:34:45 2024 ] 	Batch(1000/6809) done. Loss: 0.0507  lr:0.010000
[ Mon Jul 15 13:35:08 2024 ] 	Batch(1100/6809) done. Loss: 0.1957  lr:0.010000
[ Mon Jul 15 13:35:31 2024 ] 	Batch(1200/6809) done. Loss: 0.0532  lr:0.010000
[ Mon Jul 15 13:35:54 2024 ] 	Batch(1300/6809) done. Loss: 0.3110  lr:0.010000
[ Mon Jul 15 13:36:17 2024 ] 	Batch(1400/6809) done. Loss: 0.2887  lr:0.010000
[ Mon Jul 15 13:36:40 2024 ] 
Training: Epoch [54/150], Step [1499], Loss: 0.5508750081062317, Training Accuracy: 87.23333333333333
[ Mon Jul 15 13:36:40 2024 ] 	Batch(1500/6809) done. Loss: 0.5451  lr:0.010000
[ Mon Jul 15 13:37:03 2024 ] 	Batch(1600/6809) done. Loss: 0.0048  lr:0.010000
[ Mon Jul 15 13:37:25 2024 ] 	Batch(1700/6809) done. Loss: 0.2514  lr:0.010000
[ Mon Jul 15 13:37:48 2024 ] 	Batch(1800/6809) done. Loss: 0.1294  lr:0.010000
[ Mon Jul 15 13:38:11 2024 ] 	Batch(1900/6809) done. Loss: 0.3007  lr:0.010000
[ Mon Jul 15 13:38:33 2024 ] 
Training: Epoch [54/150], Step [1999], Loss: 0.04019025340676308, Training Accuracy: 87.30624999999999
[ Mon Jul 15 13:38:33 2024 ] 	Batch(2000/6809) done. Loss: 0.1561  lr:0.010000
[ Mon Jul 15 13:38:56 2024 ] 	Batch(2100/6809) done. Loss: 0.5614  lr:0.010000
[ Mon Jul 15 13:39:19 2024 ] 	Batch(2200/6809) done. Loss: 0.1920  lr:0.010000
[ Mon Jul 15 13:39:42 2024 ] 	Batch(2300/6809) done. Loss: 0.3709  lr:0.010000
[ Mon Jul 15 13:40:05 2024 ] 	Batch(2400/6809) done. Loss: 0.0428  lr:0.010000
[ Mon Jul 15 13:40:27 2024 ] 
Training: Epoch [54/150], Step [2499], Loss: 0.4468415379524231, Training Accuracy: 87.2
[ Mon Jul 15 13:40:28 2024 ] 	Batch(2500/6809) done. Loss: 0.4438  lr:0.010000
[ Mon Jul 15 13:40:50 2024 ] 	Batch(2600/6809) done. Loss: 0.2633  lr:0.010000
[ Mon Jul 15 13:41:13 2024 ] 	Batch(2700/6809) done. Loss: 0.0595  lr:0.010000
[ Mon Jul 15 13:41:35 2024 ] 	Batch(2800/6809) done. Loss: 0.2079  lr:0.010000
[ Mon Jul 15 13:41:58 2024 ] 	Batch(2900/6809) done. Loss: 0.1068  lr:0.010000
[ Mon Jul 15 13:42:20 2024 ] 
Training: Epoch [54/150], Step [2999], Loss: 0.3472033739089966, Training Accuracy: 87.25416666666666
[ Mon Jul 15 13:42:20 2024 ] 	Batch(3000/6809) done. Loss: 0.0346  lr:0.010000
[ Mon Jul 15 13:42:43 2024 ] 	Batch(3100/6809) done. Loss: 0.9527  lr:0.010000
[ Mon Jul 15 13:43:06 2024 ] 	Batch(3200/6809) done. Loss: 0.1499  lr:0.010000
[ Mon Jul 15 13:43:28 2024 ] 	Batch(3300/6809) done. Loss: 0.1291  lr:0.010000
[ Mon Jul 15 13:43:51 2024 ] 	Batch(3400/6809) done. Loss: 0.0150  lr:0.010000
[ Mon Jul 15 13:44:14 2024 ] 
Training: Epoch [54/150], Step [3499], Loss: 0.2284572422504425, Training Accuracy: 87.14642857142857
[ Mon Jul 15 13:44:14 2024 ] 	Batch(3500/6809) done. Loss: 0.0310  lr:0.010000
[ Mon Jul 15 13:44:37 2024 ] 	Batch(3600/6809) done. Loss: 1.4257  lr:0.010000
[ Mon Jul 15 13:45:00 2024 ] 	Batch(3700/6809) done. Loss: 0.3343  lr:0.010000
[ Mon Jul 15 13:45:23 2024 ] 	Batch(3800/6809) done. Loss: 0.0884  lr:0.010000
[ Mon Jul 15 13:45:46 2024 ] 	Batch(3900/6809) done. Loss: 0.4927  lr:0.010000
[ Mon Jul 15 13:46:08 2024 ] 
Training: Epoch [54/150], Step [3999], Loss: 1.4887200593948364, Training Accuracy: 86.99687499999999
[ Mon Jul 15 13:46:09 2024 ] 	Batch(4000/6809) done. Loss: 0.0200  lr:0.010000
[ Mon Jul 15 13:46:31 2024 ] 	Batch(4100/6809) done. Loss: 0.6998  lr:0.010000
[ Mon Jul 15 13:46:54 2024 ] 	Batch(4200/6809) done. Loss: 0.3556  lr:0.010000
[ Mon Jul 15 13:47:17 2024 ] 	Batch(4300/6809) done. Loss: 0.9676  lr:0.010000
[ Mon Jul 15 13:47:40 2024 ] 	Batch(4400/6809) done. Loss: 0.0975  lr:0.010000
[ Mon Jul 15 13:48:03 2024 ] 
Training: Epoch [54/150], Step [4499], Loss: 0.22167174518108368, Training Accuracy: 86.81944444444444
[ Mon Jul 15 13:48:04 2024 ] 	Batch(4500/6809) done. Loss: 1.1831  lr:0.010000
[ Mon Jul 15 13:48:27 2024 ] 	Batch(4600/6809) done. Loss: 0.3221  lr:0.010000
[ Mon Jul 15 13:48:50 2024 ] 	Batch(4700/6809) done. Loss: 0.4921  lr:0.010000
[ Mon Jul 15 13:49:13 2024 ] 	Batch(4800/6809) done. Loss: 0.5896  lr:0.010000
[ Mon Jul 15 13:49:36 2024 ] 	Batch(4900/6809) done. Loss: 0.3753  lr:0.010000
[ Mon Jul 15 13:49:59 2024 ] 
Training: Epoch [54/150], Step [4999], Loss: 1.032928466796875, Training Accuracy: 86.7325
[ Mon Jul 15 13:49:59 2024 ] 	Batch(5000/6809) done. Loss: 0.3884  lr:0.010000
[ Mon Jul 15 13:50:22 2024 ] 	Batch(5100/6809) done. Loss: 0.1334  lr:0.010000
[ Mon Jul 15 13:50:46 2024 ] 	Batch(5200/6809) done. Loss: 0.3074  lr:0.010000
[ Mon Jul 15 13:51:09 2024 ] 	Batch(5300/6809) done. Loss: 0.0760  lr:0.010000
[ Mon Jul 15 13:51:32 2024 ] 	Batch(5400/6809) done. Loss: 1.0322  lr:0.010000
[ Mon Jul 15 13:51:54 2024 ] 
Training: Epoch [54/150], Step [5499], Loss: 0.1872427761554718, Training Accuracy: 86.575
[ Mon Jul 15 13:51:54 2024 ] 	Batch(5500/6809) done. Loss: 0.7892  lr:0.010000
[ Mon Jul 15 13:52:18 2024 ] 	Batch(5600/6809) done. Loss: 0.7075  lr:0.010000
[ Mon Jul 15 13:52:41 2024 ] 	Batch(5700/6809) done. Loss: 0.2265  lr:0.010000
[ Mon Jul 15 13:53:04 2024 ] 	Batch(5800/6809) done. Loss: 0.4601  lr:0.010000
[ Mon Jul 15 13:53:27 2024 ] 	Batch(5900/6809) done. Loss: 0.3158  lr:0.010000
[ Mon Jul 15 13:53:50 2024 ] 
Training: Epoch [54/150], Step [5999], Loss: 0.19360938668251038, Training Accuracy: 86.45625
[ Mon Jul 15 13:53:50 2024 ] 	Batch(6000/6809) done. Loss: 0.7351  lr:0.010000
[ Mon Jul 15 13:54:13 2024 ] 	Batch(6100/6809) done. Loss: 0.2835  lr:0.010000
[ Mon Jul 15 13:54:36 2024 ] 	Batch(6200/6809) done. Loss: 0.8143  lr:0.010000
[ Mon Jul 15 13:54:59 2024 ] 	Batch(6300/6809) done. Loss: 1.0968  lr:0.010000
[ Mon Jul 15 13:55:22 2024 ] 	Batch(6400/6809) done. Loss: 0.7341  lr:0.010000
[ Mon Jul 15 13:55:45 2024 ] 
Training: Epoch [54/150], Step [6499], Loss: 0.06935027241706848, Training Accuracy: 86.35000000000001
[ Mon Jul 15 13:55:45 2024 ] 	Batch(6500/6809) done. Loss: 0.1699  lr:0.010000
[ Mon Jul 15 13:56:08 2024 ] 	Batch(6600/6809) done. Loss: 0.1971  lr:0.010000
[ Mon Jul 15 13:56:31 2024 ] 	Batch(6700/6809) done. Loss: 0.5021  lr:0.010000
[ Mon Jul 15 13:56:54 2024 ] 	Batch(6800/6809) done. Loss: 0.2095  lr:0.010000
[ Mon Jul 15 13:56:56 2024 ] 	Mean training loss: 0.4294.
[ Mon Jul 15 13:56:56 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 13:56:56 2024 ] Training epoch: 56
[ Mon Jul 15 13:56:57 2024 ] 	Batch(0/6809) done. Loss: 1.0266  lr:0.010000
[ Mon Jul 15 13:57:20 2024 ] 	Batch(100/6809) done. Loss: 0.7980  lr:0.010000
[ Mon Jul 15 13:57:42 2024 ] 	Batch(200/6809) done. Loss: 0.1977  lr:0.010000
[ Mon Jul 15 13:58:05 2024 ] 	Batch(300/6809) done. Loss: 0.3992  lr:0.010000
[ Mon Jul 15 13:58:28 2024 ] 	Batch(400/6809) done. Loss: 0.4627  lr:0.010000
[ Mon Jul 15 13:58:51 2024 ] 
Training: Epoch [55/150], Step [499], Loss: 0.47388017177581787, Training Accuracy: 87.47500000000001
[ Mon Jul 15 13:58:51 2024 ] 	Batch(500/6809) done. Loss: 0.6246  lr:0.010000
[ Mon Jul 15 13:59:14 2024 ] 	Batch(600/6809) done. Loss: 0.1386  lr:0.010000
[ Mon Jul 15 13:59:36 2024 ] 	Batch(700/6809) done. Loss: 0.4435  lr:0.010000
[ Mon Jul 15 13:59:59 2024 ] 	Batch(800/6809) done. Loss: 0.3999  lr:0.010000
[ Mon Jul 15 14:00:21 2024 ] 	Batch(900/6809) done. Loss: 0.1322  lr:0.010000
[ Mon Jul 15 14:00:44 2024 ] 
Training: Epoch [55/150], Step [999], Loss: 0.27955982089042664, Training Accuracy: 87.91250000000001
[ Mon Jul 15 14:00:44 2024 ] 	Batch(1000/6809) done. Loss: 0.6263  lr:0.010000
[ Mon Jul 15 14:01:06 2024 ] 	Batch(1100/6809) done. Loss: 0.6603  lr:0.010000
[ Mon Jul 15 14:01:29 2024 ] 	Batch(1200/6809) done. Loss: 0.3256  lr:0.010000
[ Mon Jul 15 14:01:52 2024 ] 	Batch(1300/6809) done. Loss: 0.6022  lr:0.010000
[ Mon Jul 15 14:02:15 2024 ] 	Batch(1400/6809) done. Loss: 0.7989  lr:0.010000
[ Mon Jul 15 14:02:38 2024 ] 
Training: Epoch [55/150], Step [1499], Loss: 0.7058118581771851, Training Accuracy: 87.71666666666667
[ Mon Jul 15 14:02:38 2024 ] 	Batch(1500/6809) done. Loss: 0.4756  lr:0.010000
[ Mon Jul 15 14:03:01 2024 ] 	Batch(1600/6809) done. Loss: 0.1489  lr:0.010000
[ Mon Jul 15 14:03:24 2024 ] 	Batch(1700/6809) done. Loss: 0.0526  lr:0.010000
[ Mon Jul 15 14:03:47 2024 ] 	Batch(1800/6809) done. Loss: 0.1554  lr:0.010000
[ Mon Jul 15 14:04:10 2024 ] 	Batch(1900/6809) done. Loss: 0.6097  lr:0.010000
[ Mon Jul 15 14:04:33 2024 ] 
Training: Epoch [55/150], Step [1999], Loss: 0.1079559475183487, Training Accuracy: 87.60625
[ Mon Jul 15 14:04:33 2024 ] 	Batch(2000/6809) done. Loss: 0.4582  lr:0.010000
[ Mon Jul 15 14:04:57 2024 ] 	Batch(2100/6809) done. Loss: 0.5831  lr:0.010000
[ Mon Jul 15 14:05:20 2024 ] 	Batch(2200/6809) done. Loss: 0.6535  lr:0.010000
[ Mon Jul 15 14:05:44 2024 ] 	Batch(2300/6809) done. Loss: 0.4702  lr:0.010000
[ Mon Jul 15 14:06:07 2024 ] 	Batch(2400/6809) done. Loss: 0.3002  lr:0.010000
[ Mon Jul 15 14:06:30 2024 ] 
Training: Epoch [55/150], Step [2499], Loss: 0.07836563885211945, Training Accuracy: 87.38
[ Mon Jul 15 14:06:30 2024 ] 	Batch(2500/6809) done. Loss: 0.1232  lr:0.010000
[ Mon Jul 15 14:06:53 2024 ] 	Batch(2600/6809) done. Loss: 0.0130  lr:0.010000
[ Mon Jul 15 14:07:16 2024 ] 	Batch(2700/6809) done. Loss: 0.7102  lr:0.010000
[ Mon Jul 15 14:07:39 2024 ] 	Batch(2800/6809) done. Loss: 0.7259  lr:0.010000
[ Mon Jul 15 14:08:02 2024 ] 	Batch(2900/6809) done. Loss: 0.4999  lr:0.010000
[ Mon Jul 15 14:08:24 2024 ] 
Training: Epoch [55/150], Step [2999], Loss: 0.34786367416381836, Training Accuracy: 87.50416666666668
[ Mon Jul 15 14:08:25 2024 ] 	Batch(3000/6809) done. Loss: 0.8222  lr:0.010000
[ Mon Jul 15 14:08:47 2024 ] 	Batch(3100/6809) done. Loss: 0.9511  lr:0.010000
[ Mon Jul 15 14:09:10 2024 ] 	Batch(3200/6809) done. Loss: 0.1725  lr:0.010000
[ Mon Jul 15 14:09:32 2024 ] 	Batch(3300/6809) done. Loss: 0.4426  lr:0.010000
[ Mon Jul 15 14:09:55 2024 ] 	Batch(3400/6809) done. Loss: 0.1935  lr:0.010000
[ Mon Jul 15 14:10:17 2024 ] 
Training: Epoch [55/150], Step [3499], Loss: 0.1594715267419815, Training Accuracy: 87.33571428571429
[ Mon Jul 15 14:10:17 2024 ] 	Batch(3500/6809) done. Loss: 0.1286  lr:0.010000
[ Mon Jul 15 14:10:40 2024 ] 	Batch(3600/6809) done. Loss: 0.0681  lr:0.010000
[ Mon Jul 15 14:11:02 2024 ] 	Batch(3700/6809) done. Loss: 0.3220  lr:0.010000
[ Mon Jul 15 14:11:25 2024 ] 	Batch(3800/6809) done. Loss: 0.2071  lr:0.010000
[ Mon Jul 15 14:11:47 2024 ] 	Batch(3900/6809) done. Loss: 0.1556  lr:0.010000
[ Mon Jul 15 14:12:10 2024 ] 
Training: Epoch [55/150], Step [3999], Loss: 0.8245261311531067, Training Accuracy: 87.2625
[ Mon Jul 15 14:12:10 2024 ] 	Batch(4000/6809) done. Loss: 0.0330  lr:0.010000
[ Mon Jul 15 14:12:33 2024 ] 	Batch(4100/6809) done. Loss: 0.2140  lr:0.010000
[ Mon Jul 15 14:12:55 2024 ] 	Batch(4200/6809) done. Loss: 0.8868  lr:0.010000
[ Mon Jul 15 14:13:18 2024 ] 	Batch(4300/6809) done. Loss: 0.3208  lr:0.010000
[ Mon Jul 15 14:13:41 2024 ] 	Batch(4400/6809) done. Loss: 0.2645  lr:0.010000
[ Mon Jul 15 14:14:03 2024 ] 
Training: Epoch [55/150], Step [4499], Loss: 0.24941836297512054, Training Accuracy: 87.23611111111111
[ Mon Jul 15 14:14:03 2024 ] 	Batch(4500/6809) done. Loss: 0.0341  lr:0.010000
[ Mon Jul 15 14:14:26 2024 ] 	Batch(4600/6809) done. Loss: 0.2074  lr:0.010000
[ Mon Jul 15 14:14:48 2024 ] 	Batch(4700/6809) done. Loss: 0.3436  lr:0.010000
[ Mon Jul 15 14:15:11 2024 ] 	Batch(4800/6809) done. Loss: 0.3224  lr:0.010000
[ Mon Jul 15 14:15:34 2024 ] 	Batch(4900/6809) done. Loss: 1.0100  lr:0.010000
[ Mon Jul 15 14:15:56 2024 ] 
Training: Epoch [55/150], Step [4999], Loss: 0.5801341533660889, Training Accuracy: 87.04499999999999
[ Mon Jul 15 14:15:57 2024 ] 	Batch(5000/6809) done. Loss: 0.6745  lr:0.010000
[ Mon Jul 15 14:16:19 2024 ] 	Batch(5100/6809) done. Loss: 0.6057  lr:0.010000
[ Mon Jul 15 14:16:42 2024 ] 	Batch(5200/6809) done. Loss: 0.2674  lr:0.010000
[ Mon Jul 15 14:17:04 2024 ] 	Batch(5300/6809) done. Loss: 1.1649  lr:0.010000
[ Mon Jul 15 14:17:27 2024 ] 	Batch(5400/6809) done. Loss: 0.4413  lr:0.010000
[ Mon Jul 15 14:17:49 2024 ] 
Training: Epoch [55/150], Step [5499], Loss: 0.9050779342651367, Training Accuracy: 86.92272727272727
[ Mon Jul 15 14:17:49 2024 ] 	Batch(5500/6809) done. Loss: 0.5272  lr:0.010000
[ Mon Jul 15 14:18:12 2024 ] 	Batch(5600/6809) done. Loss: 1.0451  lr:0.010000
[ Mon Jul 15 14:18:35 2024 ] 	Batch(5700/6809) done. Loss: 0.7973  lr:0.010000
[ Mon Jul 15 14:18:57 2024 ] 	Batch(5800/6809) done. Loss: 0.6220  lr:0.010000
[ Mon Jul 15 14:19:20 2024 ] 	Batch(5900/6809) done. Loss: 0.2982  lr:0.010000
[ Mon Jul 15 14:19:43 2024 ] 
Training: Epoch [55/150], Step [5999], Loss: 0.5811337828636169, Training Accuracy: 86.76666666666667
[ Mon Jul 15 14:19:43 2024 ] 	Batch(6000/6809) done. Loss: 0.6987  lr:0.010000
[ Mon Jul 15 14:20:06 2024 ] 	Batch(6100/6809) done. Loss: 0.1111  lr:0.010000
[ Mon Jul 15 14:20:29 2024 ] 	Batch(6200/6809) done. Loss: 0.2524  lr:0.010000
[ Mon Jul 15 14:20:52 2024 ] 	Batch(6300/6809) done. Loss: 0.1051  lr:0.010000
[ Mon Jul 15 14:21:15 2024 ] 	Batch(6400/6809) done. Loss: 0.2655  lr:0.010000
[ Mon Jul 15 14:21:38 2024 ] 
Training: Epoch [55/150], Step [6499], Loss: 0.5531479120254517, Training Accuracy: 86.72307692307693
[ Mon Jul 15 14:21:38 2024 ] 	Batch(6500/6809) done. Loss: 0.3530  lr:0.010000
[ Mon Jul 15 14:22:01 2024 ] 	Batch(6600/6809) done. Loss: 0.4968  lr:0.010000
[ Mon Jul 15 14:22:24 2024 ] 	Batch(6700/6809) done. Loss: 0.0333  lr:0.010000
[ Mon Jul 15 14:22:48 2024 ] 	Batch(6800/6809) done. Loss: 0.0926  lr:0.010000
[ Mon Jul 15 14:22:50 2024 ] 	Mean training loss: 0.4265.
[ Mon Jul 15 14:22:50 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 14:22:50 2024 ] Training epoch: 57
[ Mon Jul 15 14:22:50 2024 ] 	Batch(0/6809) done. Loss: 1.2635  lr:0.010000
[ Mon Jul 15 14:23:13 2024 ] 	Batch(100/6809) done. Loss: 0.2345  lr:0.010000
[ Mon Jul 15 14:23:36 2024 ] 	Batch(200/6809) done. Loss: 0.5572  lr:0.010000
[ Mon Jul 15 14:23:59 2024 ] 	Batch(300/6809) done. Loss: 0.0147  lr:0.010000
[ Mon Jul 15 14:24:22 2024 ] 	Batch(400/6809) done. Loss: 0.4738  lr:0.010000
[ Mon Jul 15 14:24:45 2024 ] 
Training: Epoch [56/150], Step [499], Loss: 0.3049868941307068, Training Accuracy: 87.675
[ Mon Jul 15 14:24:45 2024 ] 	Batch(500/6809) done. Loss: 0.1642  lr:0.010000
[ Mon Jul 15 14:25:08 2024 ] 	Batch(600/6809) done. Loss: 0.1048  lr:0.010000
[ Mon Jul 15 14:25:31 2024 ] 	Batch(700/6809) done. Loss: 0.0220  lr:0.010000
[ Mon Jul 15 14:25:54 2024 ] 	Batch(800/6809) done. Loss: 0.7215  lr:0.010000
[ Mon Jul 15 14:26:17 2024 ] 	Batch(900/6809) done. Loss: 0.0641  lr:0.010000
[ Mon Jul 15 14:26:40 2024 ] 
Training: Epoch [56/150], Step [999], Loss: 0.24603162705898285, Training Accuracy: 87.3125
[ Mon Jul 15 14:26:40 2024 ] 	Batch(1000/6809) done. Loss: 0.2539  lr:0.010000
[ Mon Jul 15 14:27:03 2024 ] 	Batch(1100/6809) done. Loss: 0.3833  lr:0.010000
[ Mon Jul 15 14:27:26 2024 ] 	Batch(1200/6809) done. Loss: 0.1037  lr:0.010000
[ Mon Jul 15 14:27:49 2024 ] 	Batch(1300/6809) done. Loss: 0.2200  lr:0.010000
[ Mon Jul 15 14:28:12 2024 ] 	Batch(1400/6809) done. Loss: 0.1198  lr:0.010000
[ Mon Jul 15 14:28:35 2024 ] 
Training: Epoch [56/150], Step [1499], Loss: 0.09831555932760239, Training Accuracy: 87.325
[ Mon Jul 15 14:28:35 2024 ] 	Batch(1500/6809) done. Loss: 0.4704  lr:0.010000
[ Mon Jul 15 14:28:58 2024 ] 	Batch(1600/6809) done. Loss: 0.8952  lr:0.010000
[ Mon Jul 15 14:29:21 2024 ] 	Batch(1700/6809) done. Loss: 0.0737  lr:0.010000
[ Mon Jul 15 14:29:44 2024 ] 	Batch(1800/6809) done. Loss: 0.3340  lr:0.010000
[ Mon Jul 15 14:30:07 2024 ] 	Batch(1900/6809) done. Loss: 0.2642  lr:0.010000
[ Mon Jul 15 14:30:30 2024 ] 
Training: Epoch [56/150], Step [1999], Loss: 0.05825212597846985, Training Accuracy: 87.4
[ Mon Jul 15 14:30:30 2024 ] 	Batch(2000/6809) done. Loss: 1.0646  lr:0.010000
[ Mon Jul 15 14:30:52 2024 ] 	Batch(2100/6809) done. Loss: 0.3713  lr:0.010000
[ Mon Jul 15 14:31:15 2024 ] 	Batch(2200/6809) done. Loss: 0.2479  lr:0.010000
[ Mon Jul 15 14:31:37 2024 ] 	Batch(2300/6809) done. Loss: 0.1000  lr:0.010000
[ Mon Jul 15 14:32:00 2024 ] 	Batch(2400/6809) done. Loss: 0.1810  lr:0.010000
[ Mon Jul 15 14:32:22 2024 ] 
Training: Epoch [56/150], Step [2499], Loss: 0.614272952079773, Training Accuracy: 87.315
[ Mon Jul 15 14:32:23 2024 ] 	Batch(2500/6809) done. Loss: 0.3464  lr:0.010000
[ Mon Jul 15 14:32:45 2024 ] 	Batch(2600/6809) done. Loss: 0.0316  lr:0.010000
[ Mon Jul 15 14:33:08 2024 ] 	Batch(2700/6809) done. Loss: 0.5855  lr:0.010000
[ Mon Jul 15 14:33:30 2024 ] 	Batch(2800/6809) done. Loss: 1.0711  lr:0.010000
[ Mon Jul 15 14:33:53 2024 ] 	Batch(2900/6809) done. Loss: 0.0908  lr:0.010000
[ Mon Jul 15 14:34:15 2024 ] 
Training: Epoch [56/150], Step [2999], Loss: 0.27685514092445374, Training Accuracy: 87.26666666666667
[ Mon Jul 15 14:34:15 2024 ] 	Batch(3000/6809) done. Loss: 0.1124  lr:0.010000
[ Mon Jul 15 14:34:38 2024 ] 	Batch(3100/6809) done. Loss: 0.3761  lr:0.010000
[ Mon Jul 15 14:35:01 2024 ] 	Batch(3200/6809) done. Loss: 0.6812  lr:0.010000
[ Mon Jul 15 14:35:23 2024 ] 	Batch(3300/6809) done. Loss: 0.9223  lr:0.010000
[ Mon Jul 15 14:35:46 2024 ] 	Batch(3400/6809) done. Loss: 0.4149  lr:0.010000
[ Mon Jul 15 14:36:10 2024 ] 
Training: Epoch [56/150], Step [3499], Loss: 1.1030808687210083, Training Accuracy: 87.12857142857143
[ Mon Jul 15 14:36:10 2024 ] 	Batch(3500/6809) done. Loss: 1.3089  lr:0.010000
[ Mon Jul 15 14:36:33 2024 ] 	Batch(3600/6809) done. Loss: 0.1260  lr:0.010000
[ Mon Jul 15 14:36:57 2024 ] 	Batch(3700/6809) done. Loss: 0.5473  lr:0.010000
[ Mon Jul 15 14:37:20 2024 ] 	Batch(3800/6809) done. Loss: 0.0508  lr:0.010000
[ Mon Jul 15 14:37:43 2024 ] 	Batch(3900/6809) done. Loss: 0.5580  lr:0.010000
[ Mon Jul 15 14:38:06 2024 ] 
Training: Epoch [56/150], Step [3999], Loss: 0.7171968221664429, Training Accuracy: 87.00625
[ Mon Jul 15 14:38:07 2024 ] 	Batch(4000/6809) done. Loss: 0.4663  lr:0.010000
[ Mon Jul 15 14:38:30 2024 ] 	Batch(4100/6809) done. Loss: 0.9414  lr:0.010000
[ Mon Jul 15 14:38:53 2024 ] 	Batch(4200/6809) done. Loss: 0.3617  lr:0.010000
[ Mon Jul 15 14:39:16 2024 ] 	Batch(4300/6809) done. Loss: 0.4737  lr:0.010000
[ Mon Jul 15 14:39:39 2024 ] 	Batch(4400/6809) done. Loss: 0.3444  lr:0.010000
[ Mon Jul 15 14:40:02 2024 ] 
Training: Epoch [56/150], Step [4499], Loss: 0.3679575026035309, Training Accuracy: 86.91111111111111
[ Mon Jul 15 14:40:02 2024 ] 	Batch(4500/6809) done. Loss: 0.2758  lr:0.010000
[ Mon Jul 15 14:40:24 2024 ] 	Batch(4600/6809) done. Loss: 0.2733  lr:0.010000
[ Mon Jul 15 14:40:47 2024 ] 	Batch(4700/6809) done. Loss: 0.2405  lr:0.010000
[ Mon Jul 15 14:41:09 2024 ] 	Batch(4800/6809) done. Loss: 0.5880  lr:0.010000
[ Mon Jul 15 14:41:32 2024 ] 	Batch(4900/6809) done. Loss: 0.4203  lr:0.010000
[ Mon Jul 15 14:41:54 2024 ] 
Training: Epoch [56/150], Step [4999], Loss: 0.41475027799606323, Training Accuracy: 86.8475
[ Mon Jul 15 14:41:55 2024 ] 	Batch(5000/6809) done. Loss: 0.5934  lr:0.010000
[ Mon Jul 15 14:42:17 2024 ] 	Batch(5100/6809) done. Loss: 0.2180  lr:0.010000
[ Mon Jul 15 14:42:40 2024 ] 	Batch(5200/6809) done. Loss: 0.8790  lr:0.010000
[ Mon Jul 15 14:43:02 2024 ] 	Batch(5300/6809) done. Loss: 0.4655  lr:0.010000
[ Mon Jul 15 14:43:25 2024 ] 	Batch(5400/6809) done. Loss: 0.9973  lr:0.010000
[ Mon Jul 15 14:43:47 2024 ] 
Training: Epoch [56/150], Step [5499], Loss: 0.3683468997478485, Training Accuracy: 86.75227272727273
[ Mon Jul 15 14:43:47 2024 ] 	Batch(5500/6809) done. Loss: 0.1464  lr:0.010000
[ Mon Jul 15 14:44:10 2024 ] 	Batch(5600/6809) done. Loss: 0.7558  lr:0.010000
[ Mon Jul 15 14:44:33 2024 ] 	Batch(5700/6809) done. Loss: 0.5504  lr:0.010000
[ Mon Jul 15 14:44:56 2024 ] 	Batch(5800/6809) done. Loss: 0.5693  lr:0.010000
[ Mon Jul 15 14:45:19 2024 ] 	Batch(5900/6809) done. Loss: 0.4638  lr:0.010000
[ Mon Jul 15 14:45:41 2024 ] 
Training: Epoch [56/150], Step [5999], Loss: 0.26912733912467957, Training Accuracy: 86.56458333333333
[ Mon Jul 15 14:45:42 2024 ] 	Batch(6000/6809) done. Loss: 0.3040  lr:0.010000
[ Mon Jul 15 14:46:04 2024 ] 	Batch(6100/6809) done. Loss: 0.2120  lr:0.010000
[ Mon Jul 15 14:46:27 2024 ] 	Batch(6200/6809) done. Loss: 0.1244  lr:0.010000
[ Mon Jul 15 14:46:50 2024 ] 	Batch(6300/6809) done. Loss: 1.3003  lr:0.010000
[ Mon Jul 15 14:47:13 2024 ] 	Batch(6400/6809) done. Loss: 0.3577  lr:0.010000
[ Mon Jul 15 14:47:35 2024 ] 
Training: Epoch [56/150], Step [6499], Loss: 0.698591411113739, Training Accuracy: 86.54807692307692
[ Mon Jul 15 14:47:35 2024 ] 	Batch(6500/6809) done. Loss: 0.1070  lr:0.010000
[ Mon Jul 15 14:47:58 2024 ] 	Batch(6600/6809) done. Loss: 0.1134  lr:0.010000
[ Mon Jul 15 14:48:20 2024 ] 	Batch(6700/6809) done. Loss: 1.3962  lr:0.010000
[ Mon Jul 15 14:48:43 2024 ] 	Batch(6800/6809) done. Loss: 0.5334  lr:0.010000
[ Mon Jul 15 14:48:45 2024 ] 	Mean training loss: 0.4221.
[ Mon Jul 15 14:48:45 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 14:48:45 2024 ] Training epoch: 58
[ Mon Jul 15 14:48:46 2024 ] 	Batch(0/6809) done. Loss: 0.7159  lr:0.010000
[ Mon Jul 15 14:49:08 2024 ] 	Batch(100/6809) done. Loss: 0.2179  lr:0.010000
[ Mon Jul 15 14:49:31 2024 ] 	Batch(200/6809) done. Loss: 0.2934  lr:0.010000
[ Mon Jul 15 14:49:53 2024 ] 	Batch(300/6809) done. Loss: 0.0305  lr:0.010000
[ Mon Jul 15 14:50:17 2024 ] 	Batch(400/6809) done. Loss: 0.5035  lr:0.010000
[ Mon Jul 15 14:50:40 2024 ] 
Training: Epoch [57/150], Step [499], Loss: 0.29993224143981934, Training Accuracy: 87.5
[ Mon Jul 15 14:50:40 2024 ] 	Batch(500/6809) done. Loss: 0.7836  lr:0.010000
[ Mon Jul 15 14:51:03 2024 ] 	Batch(600/6809) done. Loss: 0.1074  lr:0.010000
[ Mon Jul 15 14:51:26 2024 ] 	Batch(700/6809) done. Loss: 0.0915  lr:0.010000
[ Mon Jul 15 14:51:49 2024 ] 	Batch(800/6809) done. Loss: 0.5887  lr:0.010000
[ Mon Jul 15 14:52:12 2024 ] 	Batch(900/6809) done. Loss: 0.0095  lr:0.010000
[ Mon Jul 15 14:52:34 2024 ] 
Training: Epoch [57/150], Step [999], Loss: 0.3020949959754944, Training Accuracy: 87.9875
[ Mon Jul 15 14:52:34 2024 ] 	Batch(1000/6809) done. Loss: 0.2112  lr:0.010000
[ Mon Jul 15 14:52:57 2024 ] 	Batch(1100/6809) done. Loss: 0.2579  lr:0.010000
[ Mon Jul 15 14:53:20 2024 ] 	Batch(1200/6809) done. Loss: 0.0625  lr:0.010000
[ Mon Jul 15 14:53:43 2024 ] 	Batch(1300/6809) done. Loss: 0.1984  lr:0.010000
[ Mon Jul 15 14:54:06 2024 ] 	Batch(1400/6809) done. Loss: 0.8227  lr:0.010000
[ Mon Jul 15 14:54:29 2024 ] 
Training: Epoch [57/150], Step [1499], Loss: 0.03893459215760231, Training Accuracy: 87.65833333333333
[ Mon Jul 15 14:54:29 2024 ] 	Batch(1500/6809) done. Loss: 0.8829  lr:0.010000
[ Mon Jul 15 14:54:53 2024 ] 	Batch(1600/6809) done. Loss: 0.8974  lr:0.010000
[ Mon Jul 15 14:55:16 2024 ] 	Batch(1700/6809) done. Loss: 0.2174  lr:0.010000
[ Mon Jul 15 14:55:39 2024 ] 	Batch(1800/6809) done. Loss: 0.0304  lr:0.010000
[ Mon Jul 15 14:56:03 2024 ] 	Batch(1900/6809) done. Loss: 0.3653  lr:0.010000
[ Mon Jul 15 14:56:25 2024 ] 
Training: Epoch [57/150], Step [1999], Loss: 0.1304352879524231, Training Accuracy: 87.64999999999999
[ Mon Jul 15 14:56:26 2024 ] 	Batch(2000/6809) done. Loss: 0.1874  lr:0.010000
[ Mon Jul 15 14:56:49 2024 ] 	Batch(2100/6809) done. Loss: 0.2784  lr:0.010000
[ Mon Jul 15 14:57:12 2024 ] 	Batch(2200/6809) done. Loss: 0.3949  lr:0.010000
[ Mon Jul 15 14:57:35 2024 ] 	Batch(2300/6809) done. Loss: 0.4188  lr:0.010000
[ Mon Jul 15 14:57:58 2024 ] 	Batch(2400/6809) done. Loss: 0.0357  lr:0.010000
[ Mon Jul 15 14:58:21 2024 ] 
Training: Epoch [57/150], Step [2499], Loss: 0.44346415996551514, Training Accuracy: 87.545
[ Mon Jul 15 14:58:22 2024 ] 	Batch(2500/6809) done. Loss: 0.7216  lr:0.010000
[ Mon Jul 15 14:58:45 2024 ] 	Batch(2600/6809) done. Loss: 1.1974  lr:0.010000
[ Mon Jul 15 14:59:08 2024 ] 	Batch(2700/6809) done. Loss: 0.0788  lr:0.010000
[ Mon Jul 15 14:59:31 2024 ] 	Batch(2800/6809) done. Loss: 0.2212  lr:0.010000
[ Mon Jul 15 14:59:53 2024 ] 	Batch(2900/6809) done. Loss: 0.0273  lr:0.010000
[ Mon Jul 15 15:00:16 2024 ] 
Training: Epoch [57/150], Step [2999], Loss: 0.15043236315250397, Training Accuracy: 87.43333333333332
[ Mon Jul 15 15:00:16 2024 ] 	Batch(3000/6809) done. Loss: 0.6496  lr:0.010000
[ Mon Jul 15 15:00:39 2024 ] 	Batch(3100/6809) done. Loss: 1.0184  lr:0.010000
[ Mon Jul 15 15:01:02 2024 ] 	Batch(3200/6809) done. Loss: 0.0698  lr:0.010000
[ Mon Jul 15 15:01:25 2024 ] 	Batch(3300/6809) done. Loss: 0.4294  lr:0.010000
[ Mon Jul 15 15:01:49 2024 ] 	Batch(3400/6809) done. Loss: 0.5005  lr:0.010000
[ Mon Jul 15 15:02:12 2024 ] 
Training: Epoch [57/150], Step [3499], Loss: 0.6092318892478943, Training Accuracy: 87.25357142857143
[ Mon Jul 15 15:02:12 2024 ] 	Batch(3500/6809) done. Loss: 0.0294  lr:0.010000
[ Mon Jul 15 15:02:35 2024 ] 	Batch(3600/6809) done. Loss: 1.4048  lr:0.010000
[ Mon Jul 15 15:02:58 2024 ] 	Batch(3700/6809) done. Loss: 0.7637  lr:0.010000
[ Mon Jul 15 15:03:20 2024 ] 	Batch(3800/6809) done. Loss: 0.0882  lr:0.010000
[ Mon Jul 15 15:03:43 2024 ] 	Batch(3900/6809) done. Loss: 0.1698  lr:0.010000
[ Mon Jul 15 15:04:06 2024 ] 
Training: Epoch [57/150], Step [3999], Loss: 0.7099961042404175, Training Accuracy: 87.16874999999999
[ Mon Jul 15 15:04:06 2024 ] 	Batch(4000/6809) done. Loss: 0.7488  lr:0.010000
[ Mon Jul 15 15:04:29 2024 ] 	Batch(4100/6809) done. Loss: 0.2265  lr:0.010000
[ Mon Jul 15 15:04:52 2024 ] 	Batch(4200/6809) done. Loss: 0.2217  lr:0.010000
[ Mon Jul 15 15:05:15 2024 ] 	Batch(4300/6809) done. Loss: 1.0588  lr:0.010000
[ Mon Jul 15 15:05:38 2024 ] 	Batch(4400/6809) done. Loss: 0.1017  lr:0.010000
[ Mon Jul 15 15:06:01 2024 ] 
Training: Epoch [57/150], Step [4499], Loss: 1.9303022623062134, Training Accuracy: 87.16666666666667
[ Mon Jul 15 15:06:01 2024 ] 	Batch(4500/6809) done. Loss: 0.6475  lr:0.010000
[ Mon Jul 15 15:06:24 2024 ] 	Batch(4600/6809) done. Loss: 0.2108  lr:0.010000
[ Mon Jul 15 15:06:47 2024 ] 	Batch(4700/6809) done. Loss: 0.1402  lr:0.010000
[ Mon Jul 15 15:07:10 2024 ] 	Batch(4800/6809) done. Loss: 0.7173  lr:0.010000
[ Mon Jul 15 15:07:34 2024 ] 	Batch(4900/6809) done. Loss: 0.2964  lr:0.010000
[ Mon Jul 15 15:07:57 2024 ] 
Training: Epoch [57/150], Step [4999], Loss: 0.11217167973518372, Training Accuracy: 87.08749999999999
[ Mon Jul 15 15:07:58 2024 ] 	Batch(5000/6809) done. Loss: 0.2341  lr:0.010000
[ Mon Jul 15 15:08:21 2024 ] 	Batch(5100/6809) done. Loss: 0.1860  lr:0.010000
[ Mon Jul 15 15:08:44 2024 ] 	Batch(5200/6809) done. Loss: 0.3439  lr:0.010000
[ Mon Jul 15 15:09:07 2024 ] 	Batch(5300/6809) done. Loss: 0.3248  lr:0.010000
[ Mon Jul 15 15:09:30 2024 ] 	Batch(5400/6809) done. Loss: 0.9033  lr:0.010000
[ Mon Jul 15 15:09:53 2024 ] 
Training: Epoch [57/150], Step [5499], Loss: 0.23689614236354828, Training Accuracy: 86.97500000000001
[ Mon Jul 15 15:09:53 2024 ] 	Batch(5500/6809) done. Loss: 0.2512  lr:0.010000
[ Mon Jul 15 15:10:17 2024 ] 	Batch(5600/6809) done. Loss: 0.3844  lr:0.010000
[ Mon Jul 15 15:10:40 2024 ] 	Batch(5700/6809) done. Loss: 0.2332  lr:0.010000
[ Mon Jul 15 15:11:03 2024 ] 	Batch(5800/6809) done. Loss: 0.0836  lr:0.010000
[ Mon Jul 15 15:11:26 2024 ] 	Batch(5900/6809) done. Loss: 0.5536  lr:0.010000
[ Mon Jul 15 15:11:49 2024 ] 
Training: Epoch [57/150], Step [5999], Loss: 0.10229167342185974, Training Accuracy: 86.95208333333333
[ Mon Jul 15 15:11:49 2024 ] 	Batch(6000/6809) done. Loss: 0.4175  lr:0.010000
[ Mon Jul 15 15:12:12 2024 ] 	Batch(6100/6809) done. Loss: 0.0464  lr:0.010000
[ Mon Jul 15 15:12:35 2024 ] 	Batch(6200/6809) done. Loss: 1.0102  lr:0.010000
[ Mon Jul 15 15:12:58 2024 ] 	Batch(6300/6809) done. Loss: 0.7322  lr:0.010000
[ Mon Jul 15 15:13:22 2024 ] 	Batch(6400/6809) done. Loss: 0.3559  lr:0.010000
[ Mon Jul 15 15:13:46 2024 ] 
Training: Epoch [57/150], Step [6499], Loss: 0.16138668358325958, Training Accuracy: 86.85384615384616
[ Mon Jul 15 15:13:46 2024 ] 	Batch(6500/6809) done. Loss: 0.6695  lr:0.010000
[ Mon Jul 15 15:14:10 2024 ] 	Batch(6600/6809) done. Loss: 0.8927  lr:0.010000
[ Mon Jul 15 15:14:34 2024 ] 	Batch(6700/6809) done. Loss: 0.5089  lr:0.010000
[ Mon Jul 15 15:14:58 2024 ] 	Batch(6800/6809) done. Loss: 0.8311  lr:0.010000
[ Mon Jul 15 15:15:00 2024 ] 	Mean training loss: 0.4079.
[ Mon Jul 15 15:15:00 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 15:15:00 2024 ] Training epoch: 59
[ Mon Jul 15 15:15:00 2024 ] 	Batch(0/6809) done. Loss: 0.2550  lr:0.010000
[ Mon Jul 15 15:15:23 2024 ] 	Batch(100/6809) done. Loss: 0.2845  lr:0.010000
[ Mon Jul 15 15:15:47 2024 ] 	Batch(200/6809) done. Loss: 0.2113  lr:0.010000
[ Mon Jul 15 15:16:09 2024 ] 	Batch(300/6809) done. Loss: 0.5245  lr:0.010000
[ Mon Jul 15 15:16:33 2024 ] 	Batch(400/6809) done. Loss: 0.2108  lr:0.010000
[ Mon Jul 15 15:16:55 2024 ] 
Training: Epoch [58/150], Step [499], Loss: 0.9079907536506653, Training Accuracy: 88.75
[ Mon Jul 15 15:16:56 2024 ] 	Batch(500/6809) done. Loss: 0.4773  lr:0.010000
[ Mon Jul 15 15:17:19 2024 ] 	Batch(600/6809) done. Loss: 0.1744  lr:0.010000
[ Mon Jul 15 15:17:42 2024 ] 	Batch(700/6809) done. Loss: 1.3142  lr:0.010000
[ Mon Jul 15 15:18:05 2024 ] 	Batch(800/6809) done. Loss: 0.5780  lr:0.010000
[ Mon Jul 15 15:18:28 2024 ] 	Batch(900/6809) done. Loss: 0.1992  lr:0.010000
[ Mon Jul 15 15:18:51 2024 ] 
Training: Epoch [58/150], Step [999], Loss: 0.7508693933486938, Training Accuracy: 88.0625
[ Mon Jul 15 15:18:51 2024 ] 	Batch(1000/6809) done. Loss: 0.1210  lr:0.010000
[ Mon Jul 15 15:19:14 2024 ] 	Batch(1100/6809) done. Loss: 0.2493  lr:0.010000
[ Mon Jul 15 15:19:38 2024 ] 	Batch(1200/6809) done. Loss: 0.3790  lr:0.010000
[ Mon Jul 15 15:20:01 2024 ] 	Batch(1300/6809) done. Loss: 0.1478  lr:0.010000
[ Mon Jul 15 15:20:24 2024 ] 	Batch(1400/6809) done. Loss: 0.3783  lr:0.010000
[ Mon Jul 15 15:20:46 2024 ] 
Training: Epoch [58/150], Step [1499], Loss: 0.415361613035202, Training Accuracy: 87.75833333333334
[ Mon Jul 15 15:20:47 2024 ] 	Batch(1500/6809) done. Loss: 0.0650  lr:0.010000
[ Mon Jul 15 15:21:10 2024 ] 	Batch(1600/6809) done. Loss: 0.3535  lr:0.010000
[ Mon Jul 15 15:21:33 2024 ] 	Batch(1700/6809) done. Loss: 0.4067  lr:0.010000
[ Mon Jul 15 15:21:56 2024 ] 	Batch(1800/6809) done. Loss: 0.6290  lr:0.010000
[ Mon Jul 15 15:22:19 2024 ] 	Batch(1900/6809) done. Loss: 0.1800  lr:0.010000
[ Mon Jul 15 15:22:42 2024 ] 
Training: Epoch [58/150], Step [1999], Loss: 0.2854563891887665, Training Accuracy: 87.49375
[ Mon Jul 15 15:22:42 2024 ] 	Batch(2000/6809) done. Loss: 0.9019  lr:0.010000
[ Mon Jul 15 15:23:05 2024 ] 	Batch(2100/6809) done. Loss: 0.3308  lr:0.010000
[ Mon Jul 15 15:23:28 2024 ] 	Batch(2200/6809) done. Loss: 0.0812  lr:0.010000
[ Mon Jul 15 15:23:51 2024 ] 	Batch(2300/6809) done. Loss: 0.0606  lr:0.010000
[ Mon Jul 15 15:24:14 2024 ] 	Batch(2400/6809) done. Loss: 0.4253  lr:0.010000
[ Mon Jul 15 15:24:37 2024 ] 
Training: Epoch [58/150], Step [2499], Loss: 0.20613257586956024, Training Accuracy: 87.565
[ Mon Jul 15 15:24:37 2024 ] 	Batch(2500/6809) done. Loss: 0.1138  lr:0.010000
[ Mon Jul 15 15:25:00 2024 ] 	Batch(2600/6809) done. Loss: 0.3274  lr:0.010000
[ Mon Jul 15 15:25:23 2024 ] 	Batch(2700/6809) done. Loss: 0.5975  lr:0.010000
[ Mon Jul 15 15:25:46 2024 ] 	Batch(2800/6809) done. Loss: 1.4710  lr:0.010000
[ Mon Jul 15 15:26:09 2024 ] 	Batch(2900/6809) done. Loss: 0.4873  lr:0.010000
[ Mon Jul 15 15:26:32 2024 ] 
Training: Epoch [58/150], Step [2999], Loss: 0.2105182260274887, Training Accuracy: 87.60416666666667
[ Mon Jul 15 15:26:32 2024 ] 	Batch(3000/6809) done. Loss: 0.0949  lr:0.010000
[ Mon Jul 15 15:26:55 2024 ] 	Batch(3100/6809) done. Loss: 0.3428  lr:0.010000
[ Mon Jul 15 15:27:18 2024 ] 	Batch(3200/6809) done. Loss: 0.7249  lr:0.010000
[ Mon Jul 15 15:27:41 2024 ] 	Batch(3300/6809) done. Loss: 0.0844  lr:0.010000
[ Mon Jul 15 15:28:04 2024 ] 	Batch(3400/6809) done. Loss: 0.0371  lr:0.010000
[ Mon Jul 15 15:28:27 2024 ] 
Training: Epoch [58/150], Step [3499], Loss: 0.11241897940635681, Training Accuracy: 87.53571428571428
[ Mon Jul 15 15:28:27 2024 ] 	Batch(3500/6809) done. Loss: 0.0568  lr:0.010000
[ Mon Jul 15 15:28:50 2024 ] 	Batch(3600/6809) done. Loss: 1.5168  lr:0.010000
[ Mon Jul 15 15:29:12 2024 ] 	Batch(3700/6809) done. Loss: 0.2534  lr:0.010000
[ Mon Jul 15 15:29:35 2024 ] 	Batch(3800/6809) done. Loss: 0.8897  lr:0.010000
[ Mon Jul 15 15:29:57 2024 ] 	Batch(3900/6809) done. Loss: 0.4513  lr:0.010000
[ Mon Jul 15 15:30:20 2024 ] 
Training: Epoch [58/150], Step [3999], Loss: 0.5311488509178162, Training Accuracy: 87.4
[ Mon Jul 15 15:30:20 2024 ] 	Batch(4000/6809) done. Loss: 0.2094  lr:0.010000
[ Mon Jul 15 15:30:42 2024 ] 	Batch(4100/6809) done. Loss: 0.2892  lr:0.010000
[ Mon Jul 15 15:31:05 2024 ] 	Batch(4200/6809) done. Loss: 0.7819  lr:0.010000
[ Mon Jul 15 15:31:28 2024 ] 	Batch(4300/6809) done. Loss: 0.0376  lr:0.010000
[ Mon Jul 15 15:31:50 2024 ] 	Batch(4400/6809) done. Loss: 0.4909  lr:0.010000
[ Mon Jul 15 15:32:13 2024 ] 
Training: Epoch [58/150], Step [4499], Loss: 0.027382439002394676, Training Accuracy: 87.32777777777778
[ Mon Jul 15 15:32:14 2024 ] 	Batch(4500/6809) done. Loss: 0.0820  lr:0.010000
[ Mon Jul 15 15:32:37 2024 ] 	Batch(4600/6809) done. Loss: 0.7102  lr:0.010000
[ Mon Jul 15 15:33:01 2024 ] 	Batch(4700/6809) done. Loss: 0.3120  lr:0.010000
[ Mon Jul 15 15:33:24 2024 ] 	Batch(4800/6809) done. Loss: 0.4564  lr:0.010000
[ Mon Jul 15 15:33:47 2024 ] 	Batch(4900/6809) done. Loss: 0.9801  lr:0.010000
[ Mon Jul 15 15:34:10 2024 ] 
Training: Epoch [58/150], Step [4999], Loss: 0.5516496300697327, Training Accuracy: 87.32
[ Mon Jul 15 15:34:10 2024 ] 	Batch(5000/6809) done. Loss: 0.1218  lr:0.010000
[ Mon Jul 15 15:34:33 2024 ] 	Batch(5100/6809) done. Loss: 0.1280  lr:0.010000
[ Mon Jul 15 15:34:56 2024 ] 	Batch(5200/6809) done. Loss: 0.1731  lr:0.010000
[ Mon Jul 15 15:35:19 2024 ] 	Batch(5300/6809) done. Loss: 0.4519  lr:0.010000
[ Mon Jul 15 15:35:43 2024 ] 	Batch(5400/6809) done. Loss: 0.8204  lr:0.010000
[ Mon Jul 15 15:36:06 2024 ] 
Training: Epoch [58/150], Step [5499], Loss: 0.20866860449314117, Training Accuracy: 87.30454545454546
[ Mon Jul 15 15:36:06 2024 ] 	Batch(5500/6809) done. Loss: 0.0252  lr:0.010000
[ Mon Jul 15 15:36:29 2024 ] 	Batch(5600/6809) done. Loss: 0.2816  lr:0.010000
[ Mon Jul 15 15:36:52 2024 ] 	Batch(5700/6809) done. Loss: 0.6571  lr:0.010000
[ Mon Jul 15 15:37:15 2024 ] 	Batch(5800/6809) done. Loss: 0.0732  lr:0.010000
[ Mon Jul 15 15:37:39 2024 ] 	Batch(5900/6809) done. Loss: 0.9359  lr:0.010000
[ Mon Jul 15 15:38:02 2024 ] 
Training: Epoch [58/150], Step [5999], Loss: 0.5755517482757568, Training Accuracy: 87.20833333333333
[ Mon Jul 15 15:38:02 2024 ] 	Batch(6000/6809) done. Loss: 0.5987  lr:0.010000
[ Mon Jul 15 15:38:25 2024 ] 	Batch(6100/6809) done. Loss: 0.5337  lr:0.010000
[ Mon Jul 15 15:38:48 2024 ] 	Batch(6200/6809) done. Loss: 0.2115  lr:0.010000
[ Mon Jul 15 15:39:11 2024 ] 	Batch(6300/6809) done. Loss: 0.0901  lr:0.010000
[ Mon Jul 15 15:39:35 2024 ] 	Batch(6400/6809) done. Loss: 0.3365  lr:0.010000
[ Mon Jul 15 15:39:58 2024 ] 
Training: Epoch [58/150], Step [6499], Loss: 1.4776642322540283, Training Accuracy: 87.11538461538461
[ Mon Jul 15 15:39:59 2024 ] 	Batch(6500/6809) done. Loss: 0.3587  lr:0.010000
[ Mon Jul 15 15:40:22 2024 ] 	Batch(6600/6809) done. Loss: 0.0421  lr:0.010000
[ Mon Jul 15 15:40:45 2024 ] 	Batch(6700/6809) done. Loss: 0.6853  lr:0.010000
[ Mon Jul 15 15:41:08 2024 ] 	Batch(6800/6809) done. Loss: 0.2074  lr:0.010000
[ Mon Jul 15 15:41:10 2024 ] 	Mean training loss: 0.4114.
[ Mon Jul 15 15:41:10 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 15:41:10 2024 ] Training epoch: 60
[ Mon Jul 15 15:41:11 2024 ] 	Batch(0/6809) done. Loss: 0.2872  lr:0.010000
[ Mon Jul 15 15:41:34 2024 ] 	Batch(100/6809) done. Loss: 0.3040  lr:0.010000
[ Mon Jul 15 15:41:56 2024 ] 	Batch(200/6809) done. Loss: 0.4959  lr:0.010000
[ Mon Jul 15 15:42:19 2024 ] 	Batch(300/6809) done. Loss: 0.0494  lr:0.010000
[ Mon Jul 15 15:42:42 2024 ] 	Batch(400/6809) done. Loss: 0.3551  lr:0.010000
[ Mon Jul 15 15:43:05 2024 ] 
Training: Epoch [59/150], Step [499], Loss: 0.3476068675518036, Training Accuracy: 88.075
[ Mon Jul 15 15:43:05 2024 ] 	Batch(500/6809) done. Loss: 0.1579  lr:0.010000
[ Mon Jul 15 15:43:28 2024 ] 	Batch(600/6809) done. Loss: 0.3190  lr:0.010000
[ Mon Jul 15 15:43:51 2024 ] 	Batch(700/6809) done. Loss: 0.3259  lr:0.010000
[ Mon Jul 15 15:44:14 2024 ] 	Batch(800/6809) done. Loss: 0.0796  lr:0.010000
[ Mon Jul 15 15:44:37 2024 ] 	Batch(900/6809) done. Loss: 0.3203  lr:0.010000
[ Mon Jul 15 15:45:00 2024 ] 
Training: Epoch [59/150], Step [999], Loss: 0.6759403944015503, Training Accuracy: 87.7
[ Mon Jul 15 15:45:00 2024 ] 	Batch(1000/6809) done. Loss: 0.0543  lr:0.010000
[ Mon Jul 15 15:45:24 2024 ] 	Batch(1100/6809) done. Loss: 0.4454  lr:0.010000
[ Mon Jul 15 15:45:48 2024 ] 	Batch(1200/6809) done. Loss: 0.5035  lr:0.010000
[ Mon Jul 15 15:46:11 2024 ] 	Batch(1300/6809) done. Loss: 0.0960  lr:0.010000
[ Mon Jul 15 15:46:34 2024 ] 	Batch(1400/6809) done. Loss: 0.2365  lr:0.010000
[ Mon Jul 15 15:46:57 2024 ] 
Training: Epoch [59/150], Step [1499], Loss: 0.21297022700309753, Training Accuracy: 87.56666666666668
[ Mon Jul 15 15:46:57 2024 ] 	Batch(1500/6809) done. Loss: 0.5282  lr:0.010000
[ Mon Jul 15 15:47:21 2024 ] 	Batch(1600/6809) done. Loss: 0.1594  lr:0.010000
[ Mon Jul 15 15:47:43 2024 ] 	Batch(1700/6809) done. Loss: 0.1922  lr:0.010000
[ Mon Jul 15 15:48:06 2024 ] 	Batch(1800/6809) done. Loss: 0.1178  lr:0.010000
[ Mon Jul 15 15:48:29 2024 ] 	Batch(1900/6809) done. Loss: 0.9963  lr:0.010000
[ Mon Jul 15 15:48:52 2024 ] 
Training: Epoch [59/150], Step [1999], Loss: 0.7251784801483154, Training Accuracy: 87.48125
[ Mon Jul 15 15:48:53 2024 ] 	Batch(2000/6809) done. Loss: 0.6796  lr:0.010000
[ Mon Jul 15 15:49:16 2024 ] 	Batch(2100/6809) done. Loss: 0.6837  lr:0.010000
[ Mon Jul 15 15:49:39 2024 ] 	Batch(2200/6809) done. Loss: 0.0107  lr:0.010000
[ Mon Jul 15 15:50:02 2024 ] 	Batch(2300/6809) done. Loss: 0.2471  lr:0.010000
[ Mon Jul 15 15:50:25 2024 ] 	Batch(2400/6809) done. Loss: 0.7613  lr:0.010000
[ Mon Jul 15 15:50:48 2024 ] 
Training: Epoch [59/150], Step [2499], Loss: 0.9926023483276367, Training Accuracy: 87.515
[ Mon Jul 15 15:50:48 2024 ] 	Batch(2500/6809) done. Loss: 0.0858  lr:0.010000
[ Mon Jul 15 15:51:12 2024 ] 	Batch(2600/6809) done. Loss: 1.3448  lr:0.010000
[ Mon Jul 15 15:51:35 2024 ] 	Batch(2700/6809) done. Loss: 0.0530  lr:0.010000
[ Mon Jul 15 15:51:58 2024 ] 	Batch(2800/6809) done. Loss: 0.9338  lr:0.010000
[ Mon Jul 15 15:52:21 2024 ] 	Batch(2900/6809) done. Loss: 0.1011  lr:0.010000
[ Mon Jul 15 15:52:43 2024 ] 
Training: Epoch [59/150], Step [2999], Loss: 0.9094935059547424, Training Accuracy: 87.40833333333333
[ Mon Jul 15 15:52:43 2024 ] 	Batch(3000/6809) done. Loss: 0.5682  lr:0.010000
[ Mon Jul 15 15:53:06 2024 ] 	Batch(3100/6809) done. Loss: 0.0334  lr:0.010000
[ Mon Jul 15 15:53:29 2024 ] 	Batch(3200/6809) done. Loss: 0.2249  lr:0.010000
[ Mon Jul 15 15:53:52 2024 ] 	Batch(3300/6809) done. Loss: 0.4551  lr:0.010000
[ Mon Jul 15 15:54:14 2024 ] 	Batch(3400/6809) done. Loss: 0.8052  lr:0.010000
[ Mon Jul 15 15:54:37 2024 ] 
Training: Epoch [59/150], Step [3499], Loss: 0.46083542704582214, Training Accuracy: 87.26785714285714
[ Mon Jul 15 15:54:37 2024 ] 	Batch(3500/6809) done. Loss: 0.0103  lr:0.010000
[ Mon Jul 15 15:55:00 2024 ] 	Batch(3600/6809) done. Loss: 0.8224  lr:0.010000
[ Mon Jul 15 15:55:23 2024 ] 	Batch(3700/6809) done. Loss: 0.0385  lr:0.010000
[ Mon Jul 15 15:55:46 2024 ] 	Batch(3800/6809) done. Loss: 0.3938  lr:0.010000
[ Mon Jul 15 15:56:09 2024 ] 	Batch(3900/6809) done. Loss: 0.6322  lr:0.010000
[ Mon Jul 15 15:56:32 2024 ] 
Training: Epoch [59/150], Step [3999], Loss: 0.6861249804496765, Training Accuracy: 87.2375
[ Mon Jul 15 15:56:32 2024 ] 	Batch(4000/6809) done. Loss: 0.1782  lr:0.010000
[ Mon Jul 15 15:56:54 2024 ] 	Batch(4100/6809) done. Loss: 0.0375  lr:0.010000
[ Mon Jul 15 15:57:17 2024 ] 	Batch(4200/6809) done. Loss: 0.3642  lr:0.010000
[ Mon Jul 15 15:57:40 2024 ] 	Batch(4300/6809) done. Loss: 0.3513  lr:0.010000
[ Mon Jul 15 15:58:02 2024 ] 	Batch(4400/6809) done. Loss: 0.0747  lr:0.010000
[ Mon Jul 15 15:58:25 2024 ] 
Training: Epoch [59/150], Step [4499], Loss: 0.09969786554574966, Training Accuracy: 87.17222222222222
[ Mon Jul 15 15:58:25 2024 ] 	Batch(4500/6809) done. Loss: 0.3741  lr:0.010000
[ Mon Jul 15 15:58:47 2024 ] 	Batch(4600/6809) done. Loss: 0.2620  lr:0.010000
[ Mon Jul 15 15:59:10 2024 ] 	Batch(4700/6809) done. Loss: 0.2887  lr:0.010000
[ Mon Jul 15 15:59:32 2024 ] 	Batch(4800/6809) done. Loss: 0.4563  lr:0.010000
[ Mon Jul 15 15:59:55 2024 ] 	Batch(4900/6809) done. Loss: 0.2178  lr:0.010000
[ Mon Jul 15 16:00:17 2024 ] 
Training: Epoch [59/150], Step [4999], Loss: 0.269974023103714, Training Accuracy: 87.155
[ Mon Jul 15 16:00:18 2024 ] 	Batch(5000/6809) done. Loss: 0.0713  lr:0.010000
[ Mon Jul 15 16:00:40 2024 ] 	Batch(5100/6809) done. Loss: 0.5489  lr:0.010000
[ Mon Jul 15 16:01:03 2024 ] 	Batch(5200/6809) done. Loss: 0.0172  lr:0.010000
[ Mon Jul 15 16:01:25 2024 ] 	Batch(5300/6809) done. Loss: 0.1228  lr:0.010000
[ Mon Jul 15 16:01:48 2024 ] 	Batch(5400/6809) done. Loss: 0.2476  lr:0.010000
[ Mon Jul 15 16:02:10 2024 ] 
Training: Epoch [59/150], Step [5499], Loss: 1.1004990339279175, Training Accuracy: 87.06363636363636
[ Mon Jul 15 16:02:10 2024 ] 	Batch(5500/6809) done. Loss: 0.5504  lr:0.010000
[ Mon Jul 15 16:02:33 2024 ] 	Batch(5600/6809) done. Loss: 0.1266  lr:0.010000
[ Mon Jul 15 16:02:56 2024 ] 	Batch(5700/6809) done. Loss: 0.3510  lr:0.010000
[ Mon Jul 15 16:03:19 2024 ] 	Batch(5800/6809) done. Loss: 0.8192  lr:0.010000
[ Mon Jul 15 16:03:42 2024 ] 	Batch(5900/6809) done. Loss: 0.0619  lr:0.010000
[ Mon Jul 15 16:04:06 2024 ] 
Training: Epoch [59/150], Step [5999], Loss: 0.019711392000317574, Training Accuracy: 87.0375
[ Mon Jul 15 16:04:06 2024 ] 	Batch(6000/6809) done. Loss: 0.0765  lr:0.010000
[ Mon Jul 15 16:04:30 2024 ] 	Batch(6100/6809) done. Loss: 0.0852  lr:0.010000
[ Mon Jul 15 16:04:53 2024 ] 	Batch(6200/6809) done. Loss: 0.3936  lr:0.010000
[ Mon Jul 15 16:05:16 2024 ] 	Batch(6300/6809) done. Loss: 0.6658  lr:0.010000
[ Mon Jul 15 16:05:39 2024 ] 	Batch(6400/6809) done. Loss: 0.1935  lr:0.010000
[ Mon Jul 15 16:06:03 2024 ] 
Training: Epoch [59/150], Step [6499], Loss: 0.03840858116745949, Training Accuracy: 87.04038461538461
[ Mon Jul 15 16:06:03 2024 ] 	Batch(6500/6809) done. Loss: 0.0968  lr:0.010000
[ Mon Jul 15 16:06:27 2024 ] 	Batch(6600/6809) done. Loss: 1.0812  lr:0.010000
[ Mon Jul 15 16:06:50 2024 ] 	Batch(6700/6809) done. Loss: 1.1643  lr:0.010000
[ Mon Jul 15 16:07:13 2024 ] 	Batch(6800/6809) done. Loss: 0.1320  lr:0.010000
[ Mon Jul 15 16:07:15 2024 ] 	Mean training loss: 0.4125.
[ Mon Jul 15 16:07:15 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 16:07:15 2024 ] Eval epoch: 60
[ Mon Jul 15 16:14:11 2024 ] 	Mean val loss of 7435 batches: 1.0319916288715398.
[ Mon Jul 15 16:14:11 2024 ] 
Validation: Epoch [59/150], Samples [45111.0/59477], Loss: 3.4428272247314453, Validation Accuracy: 75.84612539300906
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 1 : 380 / 500 = 76 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 2 : 437 / 499 = 87 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 3 : 416 / 500 = 83 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 4 : 426 / 502 = 84 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 5 : 416 / 502 = 82 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 6 : 432 / 502 = 86 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 7 : 473 / 497 = 95 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 8 : 469 / 498 = 94 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 9 : 372 / 500 = 74 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 10 : 246 / 500 = 49 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 11 : 163 / 498 = 32 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 12 : 395 / 499 = 79 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 13 : 472 / 502 = 94 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 14 : 484 / 504 = 96 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 15 : 391 / 502 = 77 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 16 : 324 / 502 = 64 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 17 : 412 / 504 = 81 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 18 : 417 / 504 = 82 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 19 : 439 / 502 = 87 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 20 : 452 / 502 = 90 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 21 : 467 / 503 = 92 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 22 : 399 / 504 = 79 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 23 : 415 / 503 = 82 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 24 : 418 / 504 = 82 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 25 : 449 / 504 = 89 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 26 : 399 / 504 = 79 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 27 : 439 / 501 = 87 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 28 : 350 / 502 = 69 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 29 : 340 / 502 = 67 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 30 : 349 / 501 = 69 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 31 : 315 / 504 = 62 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 32 : 425 / 503 = 84 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 33 : 308 / 503 = 61 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 34 : 483 / 504 = 95 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 35 : 463 / 503 = 92 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 36 : 369 / 502 = 73 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 37 : 431 / 504 = 85 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 38 : 397 / 504 = 78 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 39 : 441 / 498 = 88 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 40 : 424 / 504 = 84 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 41 : 460 / 503 = 91 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 42 : 421 / 504 = 83 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 43 : 332 / 503 = 66 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 44 : 356 / 504 = 70 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 45 : 386 / 504 = 76 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 46 : 370 / 504 = 73 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 47 : 324 / 503 = 64 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 48 : 388 / 503 = 77 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 49 : 422 / 499 = 84 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 50 : 426 / 502 = 84 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 51 : 441 / 503 = 87 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 52 : 419 / 504 = 83 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 53 : 424 / 497 = 85 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 54 : 405 / 480 = 84 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 55 : 408 / 504 = 80 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 56 : 396 / 503 = 78 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 57 : 457 / 504 = 90 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 58 : 459 / 499 = 91 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 59 : 483 / 503 = 96 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 60 : 389 / 479 = 81 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 61 : 392 / 484 = 80 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 62 : 347 / 487 = 71 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 63 : 421 / 489 = 86 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 64 : 387 / 488 = 79 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 65 : 415 / 490 = 84 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 66 : 156 / 488 = 31 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 67 : 258 / 490 = 52 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 68 : 301 / 490 = 61 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 69 : 367 / 490 = 74 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 70 : 101 / 490 = 20 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 71 : 278 / 490 = 56 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 72 : 194 / 488 = 39 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 73 : 260 / 486 = 53 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 74 : 231 / 481 = 48 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 75 : 234 / 488 = 47 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 76 : 342 / 489 = 69 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 77 : 296 / 488 = 60 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 78 : 374 / 488 = 76 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 79 : 459 / 490 = 93 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 80 : 396 / 489 = 80 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 81 : 287 / 491 = 58 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 82 : 317 / 491 = 64 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 83 : 223 / 489 = 45 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 84 : 359 / 489 = 73 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 85 : 342 / 489 = 69 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 86 : 405 / 491 = 82 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 87 : 402 / 492 = 81 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 88 : 394 / 491 = 80 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 89 : 339 / 492 = 68 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 90 : 212 / 490 = 43 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 91 : 360 / 482 = 74 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 92 : 343 / 490 = 70 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 93 : 353 / 487 = 72 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 94 : 428 / 489 = 87 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 95 : 383 / 490 = 78 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 96 : 453 / 491 = 92 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 97 : 455 / 490 = 92 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 98 : 436 / 491 = 88 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 99 : 441 / 491 = 89 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 100 : 410 / 491 = 83 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 101 : 408 / 491 = 83 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 102 : 288 / 492 = 58 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 103 : 334 / 492 = 67 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 104 : 275 / 491 = 56 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 105 : 156 / 491 = 31 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 106 : 312 / 492 = 63 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 107 : 294 / 491 = 59 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 108 : 376 / 492 = 76 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 109 : 353 / 490 = 72 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 110 : 413 / 491 = 84 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 111 : 444 / 492 = 90 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 112 : 453 / 492 = 92 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 113 : 434 / 491 = 88 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 114 : 349 / 491 = 71 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 115 : 404 / 492 = 82 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 116 : 428 / 491 = 87 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 117 : 411 / 492 = 83 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 118 : 371 / 490 = 75 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 119 : 426 / 492 = 86 %
[ Mon Jul 15 16:14:11 2024 ] Accuracy of 120 : 368 / 500 = 73 %
[ Mon Jul 15 16:14:11 2024 ] Training epoch: 61
[ Mon Jul 15 16:14:12 2024 ] 	Batch(0/6809) done. Loss: 0.0758  lr:0.000100
[ Mon Jul 15 16:14:36 2024 ] 	Batch(100/6809) done. Loss: 0.0612  lr:0.000100
[ Mon Jul 15 16:15:00 2024 ] 	Batch(200/6809) done. Loss: 0.2008  lr:0.000100
[ Mon Jul 15 16:15:23 2024 ] 	Batch(300/6809) done. Loss: 0.4271  lr:0.000100
[ Mon Jul 15 16:15:46 2024 ] 	Batch(400/6809) done. Loss: 0.2070  lr:0.000100
[ Mon Jul 15 16:16:09 2024 ] 
Training: Epoch [60/150], Step [499], Loss: 0.3374931514263153, Training Accuracy: 88.44999999999999
[ Mon Jul 15 16:16:09 2024 ] 	Batch(500/6809) done. Loss: 0.0637  lr:0.000100
[ Mon Jul 15 16:16:32 2024 ] 	Batch(600/6809) done. Loss: 0.5058  lr:0.000100
[ Mon Jul 15 16:16:55 2024 ] 	Batch(700/6809) done. Loss: 0.0887  lr:0.000100
[ Mon Jul 15 16:17:19 2024 ] 	Batch(800/6809) done. Loss: 0.9909  lr:0.000100
[ Mon Jul 15 16:17:42 2024 ] 	Batch(900/6809) done. Loss: 0.0949  lr:0.000100
[ Mon Jul 15 16:18:04 2024 ] 
Training: Epoch [60/150], Step [999], Loss: 0.6651166081428528, Training Accuracy: 88.97500000000001
[ Mon Jul 15 16:18:05 2024 ] 	Batch(1000/6809) done. Loss: 0.1896  lr:0.000100
[ Mon Jul 15 16:18:28 2024 ] 	Batch(1100/6809) done. Loss: 0.2782  lr:0.000100
[ Mon Jul 15 16:18:51 2024 ] 	Batch(1200/6809) done. Loss: 0.4724  lr:0.000100
[ Mon Jul 15 16:19:14 2024 ] 	Batch(1300/6809) done. Loss: 1.0080  lr:0.000100
[ Mon Jul 15 16:19:37 2024 ] 	Batch(1400/6809) done. Loss: 0.2049  lr:0.000100
[ Mon Jul 15 16:20:00 2024 ] 
Training: Epoch [60/150], Step [1499], Loss: 0.19990454614162445, Training Accuracy: 89.45
[ Mon Jul 15 16:20:00 2024 ] 	Batch(1500/6809) done. Loss: 0.0718  lr:0.000100
[ Mon Jul 15 16:20:24 2024 ] 	Batch(1600/6809) done. Loss: 0.5681  lr:0.000100
[ Mon Jul 15 16:20:47 2024 ] 	Batch(1700/6809) done. Loss: 0.1337  lr:0.000100
[ Mon Jul 15 16:21:10 2024 ] 	Batch(1800/6809) done. Loss: 0.0167  lr:0.000100
[ Mon Jul 15 16:21:33 2024 ] 	Batch(1900/6809) done. Loss: 0.0743  lr:0.000100
[ Mon Jul 15 16:21:55 2024 ] 
Training: Epoch [60/150], Step [1999], Loss: 0.1747429072856903, Training Accuracy: 89.6125
[ Mon Jul 15 16:21:56 2024 ] 	Batch(2000/6809) done. Loss: 0.1154  lr:0.000100
[ Mon Jul 15 16:22:18 2024 ] 	Batch(2100/6809) done. Loss: 0.5861  lr:0.000100
[ Mon Jul 15 16:22:41 2024 ] 	Batch(2200/6809) done. Loss: 0.0942  lr:0.000100
[ Mon Jul 15 16:23:04 2024 ] 	Batch(2300/6809) done. Loss: 0.7624  lr:0.000100
[ Mon Jul 15 16:23:27 2024 ] 	Batch(2400/6809) done. Loss: 0.0450  lr:0.000100
[ Mon Jul 15 16:23:49 2024 ] 
Training: Epoch [60/150], Step [2499], Loss: 0.15350095927715302, Training Accuracy: 90.095
[ Mon Jul 15 16:23:49 2024 ] 	Batch(2500/6809) done. Loss: 0.0824  lr:0.000100
[ Mon Jul 15 16:24:12 2024 ] 	Batch(2600/6809) done. Loss: 0.5489  lr:0.000100
[ Mon Jul 15 16:24:35 2024 ] 	Batch(2700/6809) done. Loss: 0.1227  lr:0.000100
[ Mon Jul 15 16:24:57 2024 ] 	Batch(2800/6809) done. Loss: 0.9581  lr:0.000100
[ Mon Jul 15 16:25:20 2024 ] 	Batch(2900/6809) done. Loss: 0.6618  lr:0.000100
[ Mon Jul 15 16:25:43 2024 ] 
Training: Epoch [60/150], Step [2999], Loss: 0.07597841322422028, Training Accuracy: 90.19583333333333
[ Mon Jul 15 16:25:43 2024 ] 	Batch(3000/6809) done. Loss: 0.3397  lr:0.000100
[ Mon Jul 15 16:26:05 2024 ] 	Batch(3100/6809) done. Loss: 0.1365  lr:0.000100
[ Mon Jul 15 16:26:28 2024 ] 	Batch(3200/6809) done. Loss: 0.0435  lr:0.000100
[ Mon Jul 15 16:26:51 2024 ] 	Batch(3300/6809) done. Loss: 0.3726  lr:0.000100
[ Mon Jul 15 16:27:15 2024 ] 	Batch(3400/6809) done. Loss: 0.1144  lr:0.000100
[ Mon Jul 15 16:27:37 2024 ] 
Training: Epoch [60/150], Step [3499], Loss: 0.7872382402420044, Training Accuracy: 90.30714285714285
[ Mon Jul 15 16:27:38 2024 ] 	Batch(3500/6809) done. Loss: 0.0893  lr:0.000100
[ Mon Jul 15 16:28:00 2024 ] 	Batch(3600/6809) done. Loss: 0.0168  lr:0.000100
[ Mon Jul 15 16:28:23 2024 ] 	Batch(3700/6809) done. Loss: 0.6839  lr:0.000100
[ Mon Jul 15 16:28:46 2024 ] 	Batch(3800/6809) done. Loss: 0.4565  lr:0.000100
[ Mon Jul 15 16:29:08 2024 ] 	Batch(3900/6809) done. Loss: 0.1194  lr:0.000100
[ Mon Jul 15 16:29:31 2024 ] 
Training: Epoch [60/150], Step [3999], Loss: 0.0642189159989357, Training Accuracy: 90.36874999999999
[ Mon Jul 15 16:29:31 2024 ] 	Batch(4000/6809) done. Loss: 0.1217  lr:0.000100
[ Mon Jul 15 16:29:54 2024 ] 	Batch(4100/6809) done. Loss: 0.7889  lr:0.000100
[ Mon Jul 15 16:30:17 2024 ] 	Batch(4200/6809) done. Loss: 0.7071  lr:0.000100
[ Mon Jul 15 16:30:39 2024 ] 	Batch(4300/6809) done. Loss: 0.2012  lr:0.000100
[ Mon Jul 15 16:31:02 2024 ] 	Batch(4400/6809) done. Loss: 0.0583  lr:0.000100
[ Mon Jul 15 16:31:25 2024 ] 
Training: Epoch [60/150], Step [4499], Loss: 0.19051556289196014, Training Accuracy: 90.38055555555556
[ Mon Jul 15 16:31:25 2024 ] 	Batch(4500/6809) done. Loss: 0.5529  lr:0.000100
[ Mon Jul 15 16:31:48 2024 ] 	Batch(4600/6809) done. Loss: 0.4979  lr:0.000100
[ Mon Jul 15 16:32:11 2024 ] 	Batch(4700/6809) done. Loss: 0.0162  lr:0.000100
[ Mon Jul 15 16:32:34 2024 ] 	Batch(4800/6809) done. Loss: 0.0465  lr:0.000100
[ Mon Jul 15 16:32:57 2024 ] 	Batch(4900/6809) done. Loss: 0.1125  lr:0.000100
[ Mon Jul 15 16:33:20 2024 ] 
Training: Epoch [60/150], Step [4999], Loss: 0.08415906876325607, Training Accuracy: 90.60000000000001
[ Mon Jul 15 16:33:20 2024 ] 	Batch(5000/6809) done. Loss: 0.1146  lr:0.000100
[ Mon Jul 15 16:33:43 2024 ] 	Batch(5100/6809) done. Loss: 0.0400  lr:0.000100
[ Mon Jul 15 16:34:05 2024 ] 	Batch(5200/6809) done. Loss: 0.4036  lr:0.000100
[ Mon Jul 15 16:34:28 2024 ] 	Batch(5300/6809) done. Loss: 0.3425  lr:0.000100
[ Mon Jul 15 16:34:51 2024 ] 	Batch(5400/6809) done. Loss: 0.1871  lr:0.000100
[ Mon Jul 15 16:35:13 2024 ] 
Training: Epoch [60/150], Step [5499], Loss: 0.5058683156967163, Training Accuracy: 90.71136363636364
[ Mon Jul 15 16:35:13 2024 ] 	Batch(5500/6809) done. Loss: 0.0678  lr:0.000100
[ Mon Jul 15 16:35:36 2024 ] 	Batch(5600/6809) done. Loss: 0.1782  lr:0.000100
[ Mon Jul 15 16:35:59 2024 ] 	Batch(5700/6809) done. Loss: 0.3863  lr:0.000100
[ Mon Jul 15 16:36:21 2024 ] 	Batch(5800/6809) done. Loss: 0.1141  lr:0.000100
[ Mon Jul 15 16:36:44 2024 ] 	Batch(5900/6809) done. Loss: 0.0796  lr:0.000100
[ Mon Jul 15 16:37:07 2024 ] 
Training: Epoch [60/150], Step [5999], Loss: 0.17993946373462677, Training Accuracy: 90.88333333333334
[ Mon Jul 15 16:37:07 2024 ] 	Batch(6000/6809) done. Loss: 0.0066  lr:0.000100
[ Mon Jul 15 16:37:30 2024 ] 	Batch(6100/6809) done. Loss: 0.0833  lr:0.000100
[ Mon Jul 15 16:37:52 2024 ] 	Batch(6200/6809) done. Loss: 0.0928  lr:0.000100
[ Mon Jul 15 16:38:15 2024 ] 	Batch(6300/6809) done. Loss: 0.0446  lr:0.000100
[ Mon Jul 15 16:38:38 2024 ] 	Batch(6400/6809) done. Loss: 0.5220  lr:0.000100
[ Mon Jul 15 16:39:01 2024 ] 
Training: Epoch [60/150], Step [6499], Loss: 0.18492542207241058, Training Accuracy: 90.94423076923077
[ Mon Jul 15 16:39:01 2024 ] 	Batch(6500/6809) done. Loss: 0.0716  lr:0.000100
[ Mon Jul 15 16:39:23 2024 ] 	Batch(6600/6809) done. Loss: 0.0621  lr:0.000100
[ Mon Jul 15 16:39:46 2024 ] 	Batch(6700/6809) done. Loss: 0.0195  lr:0.000100
[ Mon Jul 15 16:40:09 2024 ] 	Batch(6800/6809) done. Loss: 0.3392  lr:0.000100
[ Mon Jul 15 16:40:11 2024 ] 	Mean training loss: 0.2966.
[ Mon Jul 15 16:40:11 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 16:40:11 2024 ] Training epoch: 62
[ Mon Jul 15 16:40:12 2024 ] 	Batch(0/6809) done. Loss: 0.0337  lr:0.000100
[ Mon Jul 15 16:40:34 2024 ] 	Batch(100/6809) done. Loss: 0.6647  lr:0.000100
[ Mon Jul 15 16:40:57 2024 ] 	Batch(200/6809) done. Loss: 0.0331  lr:0.000100
[ Mon Jul 15 16:41:20 2024 ] 	Batch(300/6809) done. Loss: 0.0944  lr:0.000100
[ Mon Jul 15 16:41:43 2024 ] 	Batch(400/6809) done. Loss: 0.0455  lr:0.000100
[ Mon Jul 15 16:42:05 2024 ] 
Training: Epoch [61/150], Step [499], Loss: 0.19917134940624237, Training Accuracy: 92.025
[ Mon Jul 15 16:42:06 2024 ] 	Batch(500/6809) done. Loss: 0.2094  lr:0.000100
[ Mon Jul 15 16:42:29 2024 ] 	Batch(600/6809) done. Loss: 0.5077  lr:0.000100
[ Mon Jul 15 16:42:52 2024 ] 	Batch(700/6809) done. Loss: 0.4344  lr:0.000100
[ Mon Jul 15 16:43:16 2024 ] 	Batch(800/6809) done. Loss: 0.0220  lr:0.000100
[ Mon Jul 15 16:43:39 2024 ] 	Batch(900/6809) done. Loss: 0.1210  lr:0.000100
[ Mon Jul 15 16:44:01 2024 ] 
Training: Epoch [61/150], Step [999], Loss: 0.09953022003173828, Training Accuracy: 92.3625
[ Mon Jul 15 16:44:01 2024 ] 	Batch(1000/6809) done. Loss: 0.1697  lr:0.000100
[ Mon Jul 15 16:44:24 2024 ] 	Batch(1100/6809) done. Loss: 0.3141  lr:0.000100
[ Mon Jul 15 16:44:47 2024 ] 	Batch(1200/6809) done. Loss: 0.1338  lr:0.000100
[ Mon Jul 15 16:45:10 2024 ] 	Batch(1300/6809) done. Loss: 0.1736  lr:0.000100
[ Mon Jul 15 16:45:33 2024 ] 	Batch(1400/6809) done. Loss: 0.1383  lr:0.000100
[ Mon Jul 15 16:45:56 2024 ] 
Training: Epoch [61/150], Step [1499], Loss: 0.7649232149124146, Training Accuracy: 92.10833333333333
[ Mon Jul 15 16:45:56 2024 ] 	Batch(1500/6809) done. Loss: 0.0798  lr:0.000100
[ Mon Jul 15 16:46:20 2024 ] 	Batch(1600/6809) done. Loss: 0.5935  lr:0.000100
[ Mon Jul 15 16:46:43 2024 ] 	Batch(1700/6809) done. Loss: 0.1375  lr:0.000100
[ Mon Jul 15 16:47:05 2024 ] 	Batch(1800/6809) done. Loss: 0.0130  lr:0.000100
[ Mon Jul 15 16:47:28 2024 ] 	Batch(1900/6809) done. Loss: 0.0387  lr:0.000100
[ Mon Jul 15 16:47:51 2024 ] 
Training: Epoch [61/150], Step [1999], Loss: 1.1778132915496826, Training Accuracy: 92.4125
[ Mon Jul 15 16:47:51 2024 ] 	Batch(2000/6809) done. Loss: 0.4632  lr:0.000100
[ Mon Jul 15 16:48:14 2024 ] 	Batch(2100/6809) done. Loss: 0.7544  lr:0.000100
[ Mon Jul 15 16:48:36 2024 ] 	Batch(2200/6809) done. Loss: 1.0010  lr:0.000100
[ Mon Jul 15 16:48:59 2024 ] 	Batch(2300/6809) done. Loss: 0.1442  lr:0.000100
[ Mon Jul 15 16:49:22 2024 ] 	Batch(2400/6809) done. Loss: 0.5001  lr:0.000100
[ Mon Jul 15 16:49:44 2024 ] 
Training: Epoch [61/150], Step [2499], Loss: 0.1567169725894928, Training Accuracy: 92.42
[ Mon Jul 15 16:49:45 2024 ] 	Batch(2500/6809) done. Loss: 0.4722  lr:0.000100
[ Mon Jul 15 16:50:07 2024 ] 	Batch(2600/6809) done. Loss: 0.1834  lr:0.000100
[ Mon Jul 15 16:50:30 2024 ] 	Batch(2700/6809) done. Loss: 0.3876  lr:0.000100
[ Mon Jul 15 16:50:53 2024 ] 	Batch(2800/6809) done. Loss: 0.4414  lr:0.000100
[ Mon Jul 15 16:51:15 2024 ] 	Batch(2900/6809) done. Loss: 0.0190  lr:0.000100
[ Mon Jul 15 16:51:38 2024 ] 
Training: Epoch [61/150], Step [2999], Loss: 0.34848782420158386, Training Accuracy: 92.39166666666667
[ Mon Jul 15 16:51:38 2024 ] 	Batch(3000/6809) done. Loss: 0.0261  lr:0.000100
[ Mon Jul 15 16:52:01 2024 ] 	Batch(3100/6809) done. Loss: 0.1990  lr:0.000100
[ Mon Jul 15 16:52:24 2024 ] 	Batch(3200/6809) done. Loss: 0.0699  lr:0.000100
[ Mon Jul 15 16:52:47 2024 ] 	Batch(3300/6809) done. Loss: 0.0117  lr:0.000100
[ Mon Jul 15 16:53:09 2024 ] 	Batch(3400/6809) done. Loss: 0.4162  lr:0.000100
[ Mon Jul 15 16:53:32 2024 ] 
Training: Epoch [61/150], Step [3499], Loss: 0.3464875817298889, Training Accuracy: 92.375
[ Mon Jul 15 16:53:32 2024 ] 	Batch(3500/6809) done. Loss: 0.0717  lr:0.000100
[ Mon Jul 15 16:53:55 2024 ] 	Batch(3600/6809) done. Loss: 0.1713  lr:0.000100
[ Mon Jul 15 16:54:18 2024 ] 	Batch(3700/6809) done. Loss: 0.3626  lr:0.000100
[ Mon Jul 15 16:54:40 2024 ] 	Batch(3800/6809) done. Loss: 0.8864  lr:0.000100
[ Mon Jul 15 16:55:03 2024 ] 	Batch(3900/6809) done. Loss: 0.0610  lr:0.000100
[ Mon Jul 15 16:55:25 2024 ] 
Training: Epoch [61/150], Step [3999], Loss: 0.3453197181224823, Training Accuracy: 92.378125
[ Mon Jul 15 16:55:26 2024 ] 	Batch(4000/6809) done. Loss: 0.1714  lr:0.000100
[ Mon Jul 15 16:55:48 2024 ] 	Batch(4100/6809) done. Loss: 0.0749  lr:0.000100
[ Mon Jul 15 16:56:11 2024 ] 	Batch(4200/6809) done. Loss: 0.7009  lr:0.000100
[ Mon Jul 15 16:56:34 2024 ] 	Batch(4300/6809) done. Loss: 0.1017  lr:0.000100
[ Mon Jul 15 16:56:57 2024 ] 	Batch(4400/6809) done. Loss: 0.6846  lr:0.000100
[ Mon Jul 15 16:57:19 2024 ] 
Training: Epoch [61/150], Step [4499], Loss: 0.014412273652851582, Training Accuracy: 92.48611111111111
[ Mon Jul 15 16:57:19 2024 ] 	Batch(4500/6809) done. Loss: 0.0743  lr:0.000100
[ Mon Jul 15 16:57:42 2024 ] 	Batch(4600/6809) done. Loss: 0.1479  lr:0.000100
[ Mon Jul 15 16:58:05 2024 ] 	Batch(4700/6809) done. Loss: 0.0912  lr:0.000100
[ Mon Jul 15 16:58:27 2024 ] 	Batch(4800/6809) done. Loss: 0.1715  lr:0.000100
[ Mon Jul 15 16:58:50 2024 ] 	Batch(4900/6809) done. Loss: 0.0972  lr:0.000100
[ Mon Jul 15 16:59:13 2024 ] 
Training: Epoch [61/150], Step [4999], Loss: 0.017549294978380203, Training Accuracy: 92.5125
[ Mon Jul 15 16:59:13 2024 ] 	Batch(5000/6809) done. Loss: 0.0445  lr:0.000100
[ Mon Jul 15 16:59:35 2024 ] 	Batch(5100/6809) done. Loss: 0.0294  lr:0.000100
[ Mon Jul 15 16:59:58 2024 ] 	Batch(5200/6809) done. Loss: 0.3514  lr:0.000100
[ Mon Jul 15 17:00:21 2024 ] 	Batch(5300/6809) done. Loss: 0.8779  lr:0.000100
[ Mon Jul 15 17:00:44 2024 ] 	Batch(5400/6809) done. Loss: 0.0472  lr:0.000100
[ Mon Jul 15 17:01:06 2024 ] 
Training: Epoch [61/150], Step [5499], Loss: 0.015190000645816326, Training Accuracy: 92.47272727272727
[ Mon Jul 15 17:01:06 2024 ] 	Batch(5500/6809) done. Loss: 0.1240  lr:0.000100
[ Mon Jul 15 17:01:29 2024 ] 	Batch(5600/6809) done. Loss: 0.4753  lr:0.000100
[ Mon Jul 15 17:01:52 2024 ] 	Batch(5700/6809) done. Loss: 0.5282  lr:0.000100
[ Mon Jul 15 17:02:15 2024 ] 	Batch(5800/6809) done. Loss: 0.1094  lr:0.000100
[ Mon Jul 15 17:02:37 2024 ] 	Batch(5900/6809) done. Loss: 0.1550  lr:0.000100
[ Mon Jul 15 17:03:00 2024 ] 
Training: Epoch [61/150], Step [5999], Loss: 0.05433009937405586, Training Accuracy: 92.54166666666667
[ Mon Jul 15 17:03:00 2024 ] 	Batch(6000/6809) done. Loss: 0.2783  lr:0.000100
[ Mon Jul 15 17:03:23 2024 ] 	Batch(6100/6809) done. Loss: 0.5914  lr:0.000100
[ Mon Jul 15 17:03:46 2024 ] 	Batch(6200/6809) done. Loss: 0.3643  lr:0.000100
[ Mon Jul 15 17:04:08 2024 ] 	Batch(6300/6809) done. Loss: 0.1358  lr:0.000100
[ Mon Jul 15 17:04:31 2024 ] 	Batch(6400/6809) done. Loss: 0.0312  lr:0.000100
[ Mon Jul 15 17:04:54 2024 ] 
Training: Epoch [61/150], Step [6499], Loss: 0.2800767421722412, Training Accuracy: 92.60192307692307
[ Mon Jul 15 17:04:54 2024 ] 	Batch(6500/6809) done. Loss: 0.0797  lr:0.000100
[ Mon Jul 15 17:05:17 2024 ] 	Batch(6600/6809) done. Loss: 0.0128  lr:0.000100
[ Mon Jul 15 17:05:39 2024 ] 	Batch(6700/6809) done. Loss: 0.1332  lr:0.000100
[ Mon Jul 15 17:06:02 2024 ] 	Batch(6800/6809) done. Loss: 0.7324  lr:0.000100
[ Mon Jul 15 17:06:04 2024 ] 	Mean training loss: 0.2555.
[ Mon Jul 15 17:06:04 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 17:06:04 2024 ] Training epoch: 63
[ Mon Jul 15 17:06:05 2024 ] 	Batch(0/6809) done. Loss: 0.2014  lr:0.000100
[ Mon Jul 15 17:06:28 2024 ] 	Batch(100/6809) done. Loss: 0.2641  lr:0.000100
[ Mon Jul 15 17:06:51 2024 ] 	Batch(200/6809) done. Loss: 0.1550  lr:0.000100
[ Mon Jul 15 17:07:14 2024 ] 	Batch(300/6809) done. Loss: 0.0160  lr:0.000100
[ Mon Jul 15 17:07:37 2024 ] 	Batch(400/6809) done. Loss: 0.3842  lr:0.000100
[ Mon Jul 15 17:07:59 2024 ] 
Training: Epoch [62/150], Step [499], Loss: 0.09444328397512436, Training Accuracy: 92.65
[ Mon Jul 15 17:08:00 2024 ] 	Batch(500/6809) done. Loss: 0.1031  lr:0.000100
[ Mon Jul 15 17:08:23 2024 ] 	Batch(600/6809) done. Loss: 0.1050  lr:0.000100
[ Mon Jul 15 17:08:46 2024 ] 	Batch(700/6809) done. Loss: 0.2437  lr:0.000100
[ Mon Jul 15 17:09:08 2024 ] 	Batch(800/6809) done. Loss: 0.2634  lr:0.000100
[ Mon Jul 15 17:09:31 2024 ] 	Batch(900/6809) done. Loss: 0.1995  lr:0.000100
[ Mon Jul 15 17:09:53 2024 ] 
Training: Epoch [62/150], Step [999], Loss: 0.019493810832500458, Training Accuracy: 92.85
[ Mon Jul 15 17:09:53 2024 ] 	Batch(1000/6809) done. Loss: 0.1504  lr:0.000100
[ Mon Jul 15 17:10:16 2024 ] 	Batch(1100/6809) done. Loss: 0.3521  lr:0.000100
[ Mon Jul 15 17:10:39 2024 ] 	Batch(1200/6809) done. Loss: 0.2706  lr:0.000100
[ Mon Jul 15 17:11:01 2024 ] 	Batch(1300/6809) done. Loss: 0.0431  lr:0.000100
[ Mon Jul 15 17:11:23 2024 ] 	Batch(1400/6809) done. Loss: 0.1397  lr:0.000100
[ Mon Jul 15 17:11:46 2024 ] 
Training: Epoch [62/150], Step [1499], Loss: 0.3353055715560913, Training Accuracy: 93.19166666666666
[ Mon Jul 15 17:11:46 2024 ] 	Batch(1500/6809) done. Loss: 0.3714  lr:0.000100
[ Mon Jul 15 17:12:08 2024 ] 	Batch(1600/6809) done. Loss: 0.2730  lr:0.000100
[ Mon Jul 15 17:12:31 2024 ] 	Batch(1700/6809) done. Loss: 0.0508  lr:0.000100
[ Mon Jul 15 17:12:53 2024 ] 	Batch(1800/6809) done. Loss: 0.0375  lr:0.000100
[ Mon Jul 15 17:13:16 2024 ] 	Batch(1900/6809) done. Loss: 0.2833  lr:0.000100
[ Mon Jul 15 17:13:38 2024 ] 
Training: Epoch [62/150], Step [1999], Loss: 0.16429799795150757, Training Accuracy: 93.21249999999999
[ Mon Jul 15 17:13:38 2024 ] 	Batch(2000/6809) done. Loss: 0.0377  lr:0.000100
[ Mon Jul 15 17:14:01 2024 ] 	Batch(2100/6809) done. Loss: 0.2336  lr:0.000100
[ Mon Jul 15 17:14:23 2024 ] 	Batch(2200/6809) done. Loss: 0.0267  lr:0.000100
[ Mon Jul 15 17:14:47 2024 ] 	Batch(2300/6809) done. Loss: 0.5384  lr:0.000100
[ Mon Jul 15 17:15:10 2024 ] 	Batch(2400/6809) done. Loss: 0.1478  lr:0.000100
[ Mon Jul 15 17:15:33 2024 ] 
Training: Epoch [62/150], Step [2499], Loss: 0.14813439548015594, Training Accuracy: 93.215
[ Mon Jul 15 17:15:33 2024 ] 	Batch(2500/6809) done. Loss: 0.0405  lr:0.000100
[ Mon Jul 15 17:15:56 2024 ] 	Batch(2600/6809) done. Loss: 0.3842  lr:0.000100
[ Mon Jul 15 17:16:19 2024 ] 	Batch(2700/6809) done. Loss: 0.0332  lr:0.000100
[ Mon Jul 15 17:16:43 2024 ] 	Batch(2800/6809) done. Loss: 0.1926  lr:0.000100
[ Mon Jul 15 17:17:06 2024 ] 	Batch(2900/6809) done. Loss: 0.0248  lr:0.000100
[ Mon Jul 15 17:17:29 2024 ] 
Training: Epoch [62/150], Step [2999], Loss: 0.014806535094976425, Training Accuracy: 93.11666666666667
[ Mon Jul 15 17:17:29 2024 ] 	Batch(3000/6809) done. Loss: 0.3144  lr:0.000100
[ Mon Jul 15 17:17:52 2024 ] 	Batch(3100/6809) done. Loss: 0.4222  lr:0.000100
[ Mon Jul 15 17:18:16 2024 ] 	Batch(3200/6809) done. Loss: 0.2920  lr:0.000100
[ Mon Jul 15 17:18:39 2024 ] 	Batch(3300/6809) done. Loss: 0.1594  lr:0.000100
[ Mon Jul 15 17:19:02 2024 ] 	Batch(3400/6809) done. Loss: 0.3317  lr:0.000100
[ Mon Jul 15 17:19:25 2024 ] 
Training: Epoch [62/150], Step [3499], Loss: 0.18029846251010895, Training Accuracy: 93.125
[ Mon Jul 15 17:19:25 2024 ] 	Batch(3500/6809) done. Loss: 0.9252  lr:0.000100
[ Mon Jul 15 17:19:48 2024 ] 	Batch(3600/6809) done. Loss: 0.5677  lr:0.000100
[ Mon Jul 15 17:20:12 2024 ] 	Batch(3700/6809) done. Loss: 0.1115  lr:0.000100
[ Mon Jul 15 17:20:36 2024 ] 	Batch(3800/6809) done. Loss: 0.1450  lr:0.000100
[ Mon Jul 15 17:20:59 2024 ] 	Batch(3900/6809) done. Loss: 0.2823  lr:0.000100
[ Mon Jul 15 17:21:22 2024 ] 
Training: Epoch [62/150], Step [3999], Loss: 0.12331964075565338, Training Accuracy: 93.1125
[ Mon Jul 15 17:21:22 2024 ] 	Batch(4000/6809) done. Loss: 0.1733  lr:0.000100
[ Mon Jul 15 17:21:45 2024 ] 	Batch(4100/6809) done. Loss: 0.8723  lr:0.000100
[ Mon Jul 15 17:22:08 2024 ] 	Batch(4200/6809) done. Loss: 0.2425  lr:0.000100
[ Mon Jul 15 17:22:31 2024 ] 	Batch(4300/6809) done. Loss: 0.2415  lr:0.000100
[ Mon Jul 15 17:22:54 2024 ] 	Batch(4400/6809) done. Loss: 0.5453  lr:0.000100
[ Mon Jul 15 17:23:17 2024 ] 
Training: Epoch [62/150], Step [4499], Loss: 0.09833740442991257, Training Accuracy: 93.16666666666666
[ Mon Jul 15 17:23:17 2024 ] 	Batch(4500/6809) done. Loss: 0.1411  lr:0.000100
[ Mon Jul 15 17:23:39 2024 ] 	Batch(4600/6809) done. Loss: 0.4263  lr:0.000100
[ Mon Jul 15 17:24:02 2024 ] 	Batch(4700/6809) done. Loss: 0.0797  lr:0.000100
[ Mon Jul 15 17:24:25 2024 ] 	Batch(4800/6809) done. Loss: 0.3297  lr:0.000100
[ Mon Jul 15 17:24:47 2024 ] 	Batch(4900/6809) done. Loss: 0.2286  lr:0.000100
[ Mon Jul 15 17:25:10 2024 ] 
Training: Epoch [62/150], Step [4999], Loss: 0.0057547688484191895, Training Accuracy: 93.14750000000001
[ Mon Jul 15 17:25:10 2024 ] 	Batch(5000/6809) done. Loss: 0.1949  lr:0.000100
[ Mon Jul 15 17:25:33 2024 ] 	Batch(5100/6809) done. Loss: 0.1075  lr:0.000100
[ Mon Jul 15 17:25:56 2024 ] 	Batch(5200/6809) done. Loss: 0.1968  lr:0.000100
[ Mon Jul 15 17:26:18 2024 ] 	Batch(5300/6809) done. Loss: 0.0095  lr:0.000100
[ Mon Jul 15 17:26:41 2024 ] 	Batch(5400/6809) done. Loss: 0.0338  lr:0.000100
[ Mon Jul 15 17:27:03 2024 ] 
Training: Epoch [62/150], Step [5499], Loss: 0.271752268075943, Training Accuracy: 93.19090909090909
[ Mon Jul 15 17:27:04 2024 ] 	Batch(5500/6809) done. Loss: 0.1963  lr:0.000100
[ Mon Jul 15 17:27:26 2024 ] 	Batch(5600/6809) done. Loss: 0.1426  lr:0.000100
[ Mon Jul 15 17:27:49 2024 ] 	Batch(5700/6809) done. Loss: 0.2418  lr:0.000100
[ Mon Jul 15 17:28:12 2024 ] 	Batch(5800/6809) done. Loss: 0.2417  lr:0.000100
[ Mon Jul 15 17:28:34 2024 ] 	Batch(5900/6809) done. Loss: 0.0907  lr:0.000100
[ Mon Jul 15 17:28:57 2024 ] 
Training: Epoch [62/150], Step [5999], Loss: 0.14824672043323517, Training Accuracy: 93.13333333333334
[ Mon Jul 15 17:28:57 2024 ] 	Batch(6000/6809) done. Loss: 0.0096  lr:0.000100
[ Mon Jul 15 17:29:20 2024 ] 	Batch(6100/6809) done. Loss: 0.0536  lr:0.000100
[ Mon Jul 15 17:29:42 2024 ] 	Batch(6200/6809) done. Loss: 0.2783  lr:0.000100
[ Mon Jul 15 17:30:05 2024 ] 	Batch(6300/6809) done. Loss: 0.7841  lr:0.000100
[ Mon Jul 15 17:30:28 2024 ] 	Batch(6400/6809) done. Loss: 0.1266  lr:0.000100
[ Mon Jul 15 17:30:50 2024 ] 
Training: Epoch [62/150], Step [6499], Loss: 0.3280799388885498, Training Accuracy: 93.16538461538461
[ Mon Jul 15 17:30:50 2024 ] 	Batch(6500/6809) done. Loss: 0.1492  lr:0.000100
[ Mon Jul 15 17:31:13 2024 ] 	Batch(6600/6809) done. Loss: 0.0905  lr:0.000100
[ Mon Jul 15 17:31:36 2024 ] 	Batch(6700/6809) done. Loss: 0.1125  lr:0.000100
[ Mon Jul 15 17:31:59 2024 ] 	Batch(6800/6809) done. Loss: 0.0099  lr:0.000100
[ Mon Jul 15 17:32:00 2024 ] 	Mean training loss: 0.2299.
[ Mon Jul 15 17:32:00 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 17:32:01 2024 ] Training epoch: 64
[ Mon Jul 15 17:32:01 2024 ] 	Batch(0/6809) done. Loss: 0.0810  lr:0.000100
[ Mon Jul 15 17:32:24 2024 ] 	Batch(100/6809) done. Loss: 0.0857  lr:0.000100
[ Mon Jul 15 17:32:47 2024 ] 	Batch(200/6809) done. Loss: 0.1743  lr:0.000100
[ Mon Jul 15 17:33:09 2024 ] 	Batch(300/6809) done. Loss: 0.1343  lr:0.000100
[ Mon Jul 15 17:33:32 2024 ] 	Batch(400/6809) done. Loss: 0.7022  lr:0.000100
[ Mon Jul 15 17:33:55 2024 ] 
Training: Epoch [63/150], Step [499], Loss: 0.02510867826640606, Training Accuracy: 93.0
[ Mon Jul 15 17:33:55 2024 ] 	Batch(500/6809) done. Loss: 0.0858  lr:0.000100
[ Mon Jul 15 17:34:18 2024 ] 	Batch(600/6809) done. Loss: 0.0582  lr:0.000100
[ Mon Jul 15 17:34:40 2024 ] 	Batch(700/6809) done. Loss: 0.0190  lr:0.000100
[ Mon Jul 15 17:35:03 2024 ] 	Batch(800/6809) done. Loss: 0.0339  lr:0.000100
[ Mon Jul 15 17:35:26 2024 ] 	Batch(900/6809) done. Loss: 0.1844  lr:0.000100
[ Mon Jul 15 17:35:48 2024 ] 
Training: Epoch [63/150], Step [999], Loss: 0.015224139206111431, Training Accuracy: 93.05
[ Mon Jul 15 17:35:48 2024 ] 	Batch(1000/6809) done. Loss: 0.1319  lr:0.000100
[ Mon Jul 15 17:36:11 2024 ] 	Batch(1100/6809) done. Loss: 0.0169  lr:0.000100
[ Mon Jul 15 17:36:34 2024 ] 	Batch(1200/6809) done. Loss: 0.2532  lr:0.000100
[ Mon Jul 15 17:36:56 2024 ] 	Batch(1300/6809) done. Loss: 0.1327  lr:0.000100
[ Mon Jul 15 17:37:19 2024 ] 	Batch(1400/6809) done. Loss: 0.1185  lr:0.000100
[ Mon Jul 15 17:37:41 2024 ] 
Training: Epoch [63/150], Step [1499], Loss: 0.091862253844738, Training Accuracy: 93.03333333333333
[ Mon Jul 15 17:37:42 2024 ] 	Batch(1500/6809) done. Loss: 0.4055  lr:0.000100
[ Mon Jul 15 17:38:04 2024 ] 	Batch(1600/6809) done. Loss: 0.1330  lr:0.000100
[ Mon Jul 15 17:38:27 2024 ] 	Batch(1700/6809) done. Loss: 0.3231  lr:0.000100
[ Mon Jul 15 17:38:50 2024 ] 	Batch(1800/6809) done. Loss: 0.1211  lr:0.000100
[ Mon Jul 15 17:39:13 2024 ] 	Batch(1900/6809) done. Loss: 0.6055  lr:0.000100
[ Mon Jul 15 17:39:35 2024 ] 
Training: Epoch [63/150], Step [1999], Loss: 0.08747806400060654, Training Accuracy: 93.35
[ Mon Jul 15 17:39:35 2024 ] 	Batch(2000/6809) done. Loss: 0.0339  lr:0.000100
[ Mon Jul 15 17:39:58 2024 ] 	Batch(2100/6809) done. Loss: 0.4905  lr:0.000100
[ Mon Jul 15 17:40:21 2024 ] 	Batch(2200/6809) done. Loss: 0.0255  lr:0.000100
[ Mon Jul 15 17:40:43 2024 ] 	Batch(2300/6809) done. Loss: 0.1053  lr:0.000100
[ Mon Jul 15 17:41:06 2024 ] 	Batch(2400/6809) done. Loss: 0.3282  lr:0.000100
[ Mon Jul 15 17:41:29 2024 ] 
Training: Epoch [63/150], Step [2499], Loss: 0.07305879890918732, Training Accuracy: 93.265
[ Mon Jul 15 17:41:29 2024 ] 	Batch(2500/6809) done. Loss: 0.2195  lr:0.000100
[ Mon Jul 15 17:41:52 2024 ] 	Batch(2600/6809) done. Loss: 0.5047  lr:0.000100
[ Mon Jul 15 17:42:15 2024 ] 	Batch(2700/6809) done. Loss: 0.0221  lr:0.000100
[ Mon Jul 15 17:42:38 2024 ] 	Batch(2800/6809) done. Loss: 0.0461  lr:0.000100
[ Mon Jul 15 17:43:00 2024 ] 	Batch(2900/6809) done. Loss: 0.3196  lr:0.000100
[ Mon Jul 15 17:43:23 2024 ] 
Training: Epoch [63/150], Step [2999], Loss: 0.24731846153736115, Training Accuracy: 93.24166666666667
[ Mon Jul 15 17:43:23 2024 ] 	Batch(3000/6809) done. Loss: 0.2375  lr:0.000100
[ Mon Jul 15 17:43:46 2024 ] 	Batch(3100/6809) done. Loss: 0.0341  lr:0.000100
[ Mon Jul 15 17:44:09 2024 ] 	Batch(3200/6809) done. Loss: 0.1030  lr:0.000100
[ Mon Jul 15 17:44:32 2024 ] 	Batch(3300/6809) done. Loss: 0.0938  lr:0.000100
[ Mon Jul 15 17:44:54 2024 ] 	Batch(3400/6809) done. Loss: 0.1391  lr:0.000100
[ Mon Jul 15 17:45:17 2024 ] 
Training: Epoch [63/150], Step [3499], Loss: 0.04845806956291199, Training Accuracy: 93.14285714285714
[ Mon Jul 15 17:45:17 2024 ] 	Batch(3500/6809) done. Loss: 0.3182  lr:0.000100
[ Mon Jul 15 17:45:40 2024 ] 	Batch(3600/6809) done. Loss: 0.1581  lr:0.000100
[ Mon Jul 15 17:46:03 2024 ] 	Batch(3700/6809) done. Loss: 0.7503  lr:0.000100
[ Mon Jul 15 17:46:25 2024 ] 	Batch(3800/6809) done. Loss: 0.0103  lr:0.000100
[ Mon Jul 15 17:46:48 2024 ] 	Batch(3900/6809) done. Loss: 0.5138  lr:0.000100
[ Mon Jul 15 17:47:10 2024 ] 
Training: Epoch [63/150], Step [3999], Loss: 0.31146588921546936, Training Accuracy: 93.2
[ Mon Jul 15 17:47:10 2024 ] 	Batch(4000/6809) done. Loss: 0.0296  lr:0.000100
[ Mon Jul 15 17:47:33 2024 ] 	Batch(4100/6809) done. Loss: 0.0892  lr:0.000100
[ Mon Jul 15 17:47:56 2024 ] 	Batch(4200/6809) done. Loss: 0.0693  lr:0.000100
[ Mon Jul 15 17:48:18 2024 ] 	Batch(4300/6809) done. Loss: 0.1769  lr:0.000100
[ Mon Jul 15 17:48:41 2024 ] 	Batch(4400/6809) done. Loss: 0.3346  lr:0.000100
[ Mon Jul 15 17:49:04 2024 ] 
Training: Epoch [63/150], Step [4499], Loss: 0.1860397309064865, Training Accuracy: 93.32222222222222
[ Mon Jul 15 17:49:04 2024 ] 	Batch(4500/6809) done. Loss: 0.0090  lr:0.000100
[ Mon Jul 15 17:49:26 2024 ] 	Batch(4600/6809) done. Loss: 0.0318  lr:0.000100
[ Mon Jul 15 17:49:49 2024 ] 	Batch(4700/6809) done. Loss: 0.0375  lr:0.000100
[ Mon Jul 15 17:50:12 2024 ] 	Batch(4800/6809) done. Loss: 0.0405  lr:0.000100
[ Mon Jul 15 17:50:35 2024 ] 	Batch(4900/6809) done. Loss: 0.0783  lr:0.000100
[ Mon Jul 15 17:50:57 2024 ] 
Training: Epoch [63/150], Step [4999], Loss: 0.15601442754268646, Training Accuracy: 93.3925
[ Mon Jul 15 17:50:58 2024 ] 	Batch(5000/6809) done. Loss: 0.1158  lr:0.000100
[ Mon Jul 15 17:51:20 2024 ] 	Batch(5100/6809) done. Loss: 0.0518  lr:0.000100
[ Mon Jul 15 17:51:43 2024 ] 	Batch(5200/6809) done. Loss: 0.1430  lr:0.000100
[ Mon Jul 15 17:52:05 2024 ] 	Batch(5300/6809) done. Loss: 0.0134  lr:0.000100
[ Mon Jul 15 17:52:28 2024 ] 	Batch(5400/6809) done. Loss: 0.2178  lr:0.000100
[ Mon Jul 15 17:52:50 2024 ] 
Training: Epoch [63/150], Step [5499], Loss: 0.543984591960907, Training Accuracy: 93.41363636363637
[ Mon Jul 15 17:52:51 2024 ] 	Batch(5500/6809) done. Loss: 0.1667  lr:0.000100
[ Mon Jul 15 17:53:13 2024 ] 	Batch(5600/6809) done. Loss: 0.1722  lr:0.000100
[ Mon Jul 15 17:53:36 2024 ] 	Batch(5700/6809) done. Loss: 0.4235  lr:0.000100
[ Mon Jul 15 17:53:59 2024 ] 	Batch(5800/6809) done. Loss: 0.0291  lr:0.000100
[ Mon Jul 15 17:54:21 2024 ] 	Batch(5900/6809) done. Loss: 0.0472  lr:0.000100
[ Mon Jul 15 17:54:44 2024 ] 
Training: Epoch [63/150], Step [5999], Loss: 0.15973986685276031, Training Accuracy: 93.475
[ Mon Jul 15 17:54:44 2024 ] 	Batch(6000/6809) done. Loss: 0.1416  lr:0.000100
[ Mon Jul 15 17:55:06 2024 ] 	Batch(6100/6809) done. Loss: 0.2623  lr:0.000100
[ Mon Jul 15 17:55:29 2024 ] 	Batch(6200/6809) done. Loss: 0.3035  lr:0.000100
[ Mon Jul 15 17:55:52 2024 ] 	Batch(6300/6809) done. Loss: 0.0784  lr:0.000100
[ Mon Jul 15 17:56:14 2024 ] 	Batch(6400/6809) done. Loss: 0.1431  lr:0.000100
[ Mon Jul 15 17:56:37 2024 ] 
Training: Epoch [63/150], Step [6499], Loss: 0.12691374123096466, Training Accuracy: 93.50769230769231
[ Mon Jul 15 17:56:37 2024 ] 	Batch(6500/6809) done. Loss: 0.0204  lr:0.000100
[ Mon Jul 15 17:56:59 2024 ] 	Batch(6600/6809) done. Loss: 0.3748  lr:0.000100
[ Mon Jul 15 17:57:22 2024 ] 	Batch(6700/6809) done. Loss: 0.3308  lr:0.000100
[ Mon Jul 15 17:57:45 2024 ] 	Batch(6800/6809) done. Loss: 0.1334  lr:0.000100
[ Mon Jul 15 17:57:47 2024 ] 	Mean training loss: 0.2259.
[ Mon Jul 15 17:57:47 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 17:57:47 2024 ] Training epoch: 65
[ Mon Jul 15 17:57:48 2024 ] 	Batch(0/6809) done. Loss: 0.3242  lr:0.000100
[ Mon Jul 15 17:58:10 2024 ] 	Batch(100/6809) done. Loss: 0.1810  lr:0.000100
[ Mon Jul 15 17:58:33 2024 ] 	Batch(200/6809) done. Loss: 0.0972  lr:0.000100
[ Mon Jul 15 17:58:55 2024 ] 	Batch(300/6809) done. Loss: 0.2598  lr:0.000100
[ Mon Jul 15 17:59:18 2024 ] 	Batch(400/6809) done. Loss: 0.1583  lr:0.000100
[ Mon Jul 15 17:59:40 2024 ] 
Training: Epoch [64/150], Step [499], Loss: 0.2833625078201294, Training Accuracy: 93.875
[ Mon Jul 15 17:59:40 2024 ] 	Batch(500/6809) done. Loss: 0.2325  lr:0.000100
[ Mon Jul 15 18:00:03 2024 ] 	Batch(600/6809) done. Loss: 0.4218  lr:0.000100
[ Mon Jul 15 18:00:26 2024 ] 	Batch(700/6809) done. Loss: 0.1482  lr:0.000100
[ Mon Jul 15 18:00:48 2024 ] 	Batch(800/6809) done. Loss: 0.0095  lr:0.000100
[ Mon Jul 15 18:01:11 2024 ] 	Batch(900/6809) done. Loss: 0.2051  lr:0.000100
[ Mon Jul 15 18:01:33 2024 ] 
Training: Epoch [64/150], Step [999], Loss: 0.09604954719543457, Training Accuracy: 94.1
[ Mon Jul 15 18:01:33 2024 ] 	Batch(1000/6809) done. Loss: 0.0694  lr:0.000100
[ Mon Jul 15 18:01:56 2024 ] 	Batch(1100/6809) done. Loss: 0.2096  lr:0.000100
[ Mon Jul 15 18:02:19 2024 ] 	Batch(1200/6809) done. Loss: 0.3189  lr:0.000100
[ Mon Jul 15 18:02:41 2024 ] 	Batch(1300/6809) done. Loss: 0.0761  lr:0.000100
[ Mon Jul 15 18:03:04 2024 ] 	Batch(1400/6809) done. Loss: 0.1999  lr:0.000100
[ Mon Jul 15 18:03:26 2024 ] 
Training: Epoch [64/150], Step [1499], Loss: 0.3579419255256653, Training Accuracy: 94.1
[ Mon Jul 15 18:03:27 2024 ] 	Batch(1500/6809) done. Loss: 0.3279  lr:0.000100
[ Mon Jul 15 18:03:49 2024 ] 	Batch(1600/6809) done. Loss: 0.1190  lr:0.000100
[ Mon Jul 15 18:04:12 2024 ] 	Batch(1700/6809) done. Loss: 0.1954  lr:0.000100
[ Mon Jul 15 18:04:35 2024 ] 	Batch(1800/6809) done. Loss: 0.5016  lr:0.000100
[ Mon Jul 15 18:04:58 2024 ] 	Batch(1900/6809) done. Loss: 0.3593  lr:0.000100
[ Mon Jul 15 18:05:20 2024 ] 
Training: Epoch [64/150], Step [1999], Loss: 0.21001367270946503, Training Accuracy: 93.91874999999999
[ Mon Jul 15 18:05:21 2024 ] 	Batch(2000/6809) done. Loss: 0.0644  lr:0.000100
[ Mon Jul 15 18:05:43 2024 ] 	Batch(2100/6809) done. Loss: 0.2904  lr:0.000100
[ Mon Jul 15 18:06:06 2024 ] 	Batch(2200/6809) done. Loss: 0.4239  lr:0.000100
[ Mon Jul 15 18:06:28 2024 ] 	Batch(2300/6809) done. Loss: 0.3708  lr:0.000100
[ Mon Jul 15 18:06:51 2024 ] 	Batch(2400/6809) done. Loss: 0.2529  lr:0.000100
[ Mon Jul 15 18:07:13 2024 ] 
Training: Epoch [64/150], Step [2499], Loss: 0.11534951627254486, Training Accuracy: 93.83
[ Mon Jul 15 18:07:14 2024 ] 	Batch(2500/6809) done. Loss: 0.0912  lr:0.000100
[ Mon Jul 15 18:07:36 2024 ] 	Batch(2600/6809) done. Loss: 0.1501  lr:0.000100
[ Mon Jul 15 18:07:59 2024 ] 	Batch(2700/6809) done. Loss: 0.0111  lr:0.000100
[ Mon Jul 15 18:08:22 2024 ] 	Batch(2800/6809) done. Loss: 0.2338  lr:0.000100
[ Mon Jul 15 18:08:46 2024 ] 	Batch(2900/6809) done. Loss: 0.4231  lr:0.000100
[ Mon Jul 15 18:09:09 2024 ] 
Training: Epoch [64/150], Step [2999], Loss: 0.06643682718276978, Training Accuracy: 93.79583333333333
[ Mon Jul 15 18:09:09 2024 ] 	Batch(3000/6809) done. Loss: 0.1998  lr:0.000100
[ Mon Jul 15 18:09:31 2024 ] 	Batch(3100/6809) done. Loss: 0.1833  lr:0.000100
[ Mon Jul 15 18:09:54 2024 ] 	Batch(3200/6809) done. Loss: 0.8542  lr:0.000100
[ Mon Jul 15 18:10:17 2024 ] 	Batch(3300/6809) done. Loss: 0.0277  lr:0.000100
[ Mon Jul 15 18:10:39 2024 ] 	Batch(3400/6809) done. Loss: 0.1665  lr:0.000100
[ Mon Jul 15 18:11:02 2024 ] 
Training: Epoch [64/150], Step [3499], Loss: 0.7181825041770935, Training Accuracy: 93.68928571428572
[ Mon Jul 15 18:11:02 2024 ] 	Batch(3500/6809) done. Loss: 0.1727  lr:0.000100
[ Mon Jul 15 18:11:25 2024 ] 	Batch(3600/6809) done. Loss: 0.0434  lr:0.000100
[ Mon Jul 15 18:11:47 2024 ] 	Batch(3700/6809) done. Loss: 0.1749  lr:0.000100
[ Mon Jul 15 18:12:10 2024 ] 	Batch(3800/6809) done. Loss: 0.3366  lr:0.000100
[ Mon Jul 15 18:12:32 2024 ] 	Batch(3900/6809) done. Loss: 0.0393  lr:0.000100
[ Mon Jul 15 18:12:55 2024 ] 
Training: Epoch [64/150], Step [3999], Loss: 0.0486304834485054, Training Accuracy: 93.765625
[ Mon Jul 15 18:12:55 2024 ] 	Batch(4000/6809) done. Loss: 0.2747  lr:0.000100
[ Mon Jul 15 18:13:18 2024 ] 	Batch(4100/6809) done. Loss: 0.7399  lr:0.000100
[ Mon Jul 15 18:13:40 2024 ] 	Batch(4200/6809) done. Loss: 0.0059  lr:0.000100
[ Mon Jul 15 18:14:03 2024 ] 	Batch(4300/6809) done. Loss: 0.3401  lr:0.000100
[ Mon Jul 15 18:14:26 2024 ] 	Batch(4400/6809) done. Loss: 0.2399  lr:0.000100
[ Mon Jul 15 18:14:48 2024 ] 
Training: Epoch [64/150], Step [4499], Loss: 0.2932780981063843, Training Accuracy: 93.78888888888889
[ Mon Jul 15 18:14:48 2024 ] 	Batch(4500/6809) done. Loss: 0.2407  lr:0.000100
[ Mon Jul 15 18:15:12 2024 ] 	Batch(4600/6809) done. Loss: 0.1056  lr:0.000100
[ Mon Jul 15 18:15:35 2024 ] 	Batch(4700/6809) done. Loss: 0.1779  lr:0.000100
[ Mon Jul 15 18:15:58 2024 ] 	Batch(4800/6809) done. Loss: 0.0716  lr:0.000100
[ Mon Jul 15 18:16:21 2024 ] 	Batch(4900/6809) done. Loss: 0.2449  lr:0.000100
[ Mon Jul 15 18:16:43 2024 ] 
Training: Epoch [64/150], Step [4999], Loss: 0.23584522306919098, Training Accuracy: 93.8475
[ Mon Jul 15 18:16:44 2024 ] 	Batch(5000/6809) done. Loss: 0.4056  lr:0.000100
[ Mon Jul 15 18:17:06 2024 ] 	Batch(5100/6809) done. Loss: 0.1751  lr:0.000100
[ Mon Jul 15 18:17:29 2024 ] 	Batch(5200/6809) done. Loss: 0.2348  lr:0.000100
[ Mon Jul 15 18:17:51 2024 ] 	Batch(5300/6809) done. Loss: 0.0400  lr:0.000100
[ Mon Jul 15 18:18:14 2024 ] 	Batch(5400/6809) done. Loss: 0.8708  lr:0.000100
[ Mon Jul 15 18:18:36 2024 ] 
Training: Epoch [64/150], Step [5499], Loss: 0.40575268864631653, Training Accuracy: 93.83636363636364
[ Mon Jul 15 18:18:36 2024 ] 	Batch(5500/6809) done. Loss: 0.1212  lr:0.000100
[ Mon Jul 15 18:18:59 2024 ] 	Batch(5600/6809) done. Loss: 0.0742  lr:0.000100
[ Mon Jul 15 18:19:22 2024 ] 	Batch(5700/6809) done. Loss: 0.1421  lr:0.000100
[ Mon Jul 15 18:19:44 2024 ] 	Batch(5800/6809) done. Loss: 0.4453  lr:0.000100
[ Mon Jul 15 18:20:07 2024 ] 	Batch(5900/6809) done. Loss: 0.1001  lr:0.000100
[ Mon Jul 15 18:20:30 2024 ] 
Training: Epoch [64/150], Step [5999], Loss: 0.362568199634552, Training Accuracy: 93.79791666666667
[ Mon Jul 15 18:20:30 2024 ] 	Batch(6000/6809) done. Loss: 0.1840  lr:0.000100
[ Mon Jul 15 18:20:53 2024 ] 	Batch(6100/6809) done. Loss: 0.2974  lr:0.000100
[ Mon Jul 15 18:21:15 2024 ] 	Batch(6200/6809) done. Loss: 0.2591  lr:0.000100
[ Mon Jul 15 18:21:38 2024 ] 	Batch(6300/6809) done. Loss: 0.0338  lr:0.000100
[ Mon Jul 15 18:22:01 2024 ] 	Batch(6400/6809) done. Loss: 0.0570  lr:0.000100
[ Mon Jul 15 18:22:23 2024 ] 
Training: Epoch [64/150], Step [6499], Loss: 0.024400318041443825, Training Accuracy: 93.82692307692308
[ Mon Jul 15 18:22:23 2024 ] 	Batch(6500/6809) done. Loss: 0.1846  lr:0.000100
[ Mon Jul 15 18:22:46 2024 ] 	Batch(6600/6809) done. Loss: 0.0390  lr:0.000100
[ Mon Jul 15 18:23:08 2024 ] 	Batch(6700/6809) done. Loss: 0.4645  lr:0.000100
[ Mon Jul 15 18:23:31 2024 ] 	Batch(6800/6809) done. Loss: 0.1134  lr:0.000100
[ Mon Jul 15 18:23:33 2024 ] 	Mean training loss: 0.2143.
[ Mon Jul 15 18:23:33 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 18:23:33 2024 ] Training epoch: 66
[ Mon Jul 15 18:23:34 2024 ] 	Batch(0/6809) done. Loss: 0.1837  lr:0.000100
[ Mon Jul 15 18:23:56 2024 ] 	Batch(100/6809) done. Loss: 0.0075  lr:0.000100
[ Mon Jul 15 18:24:19 2024 ] 	Batch(200/6809) done. Loss: 0.1093  lr:0.000100
[ Mon Jul 15 18:24:42 2024 ] 	Batch(300/6809) done. Loss: 0.4622  lr:0.000100
[ Mon Jul 15 18:25:04 2024 ] 	Batch(400/6809) done. Loss: 0.0835  lr:0.000100
[ Mon Jul 15 18:25:27 2024 ] 
Training: Epoch [65/150], Step [499], Loss: 0.0175168439745903, Training Accuracy: 93.8
[ Mon Jul 15 18:25:27 2024 ] 	Batch(500/6809) done. Loss: 0.0425  lr:0.000100
[ Mon Jul 15 18:25:49 2024 ] 	Batch(600/6809) done. Loss: 0.1491  lr:0.000100
[ Mon Jul 15 18:26:12 2024 ] 	Batch(700/6809) done. Loss: 0.0831  lr:0.000100
[ Mon Jul 15 18:26:35 2024 ] 	Batch(800/6809) done. Loss: 0.1421  lr:0.000100
[ Mon Jul 15 18:26:58 2024 ] 	Batch(900/6809) done. Loss: 0.0769  lr:0.000100
[ Mon Jul 15 18:27:21 2024 ] 
Training: Epoch [65/150], Step [999], Loss: 0.03684043511748314, Training Accuracy: 93.7125
[ Mon Jul 15 18:27:21 2024 ] 	Batch(1000/6809) done. Loss: 0.3702  lr:0.000100
[ Mon Jul 15 18:27:45 2024 ] 	Batch(1100/6809) done. Loss: 0.0391  lr:0.000100
[ Mon Jul 15 18:28:08 2024 ] 	Batch(1200/6809) done. Loss: 0.5057  lr:0.000100
[ Mon Jul 15 18:28:31 2024 ] 	Batch(1300/6809) done. Loss: 0.4762  lr:0.000100
[ Mon Jul 15 18:28:55 2024 ] 	Batch(1400/6809) done. Loss: 0.0229  lr:0.000100
[ Mon Jul 15 18:29:17 2024 ] 
Training: Epoch [65/150], Step [1499], Loss: 0.013317573815584183, Training Accuracy: 93.65
[ Mon Jul 15 18:29:18 2024 ] 	Batch(1500/6809) done. Loss: 0.0411  lr:0.000100
[ Mon Jul 15 18:29:40 2024 ] 	Batch(1600/6809) done. Loss: 0.3053  lr:0.000100
[ Mon Jul 15 18:30:03 2024 ] 	Batch(1700/6809) done. Loss: 0.0696  lr:0.000100
[ Mon Jul 15 18:30:25 2024 ] 	Batch(1800/6809) done. Loss: 0.2813  lr:0.000100
[ Mon Jul 15 18:30:48 2024 ] 	Batch(1900/6809) done. Loss: 0.1341  lr:0.000100
[ Mon Jul 15 18:31:10 2024 ] 
Training: Epoch [65/150], Step [1999], Loss: 0.027395080775022507, Training Accuracy: 93.78125
[ Mon Jul 15 18:31:11 2024 ] 	Batch(2000/6809) done. Loss: 0.0900  lr:0.000100
[ Mon Jul 15 18:31:33 2024 ] 	Batch(2100/6809) done. Loss: 0.2180  lr:0.000100
[ Mon Jul 15 18:31:56 2024 ] 	Batch(2200/6809) done. Loss: 0.2418  lr:0.000100
[ Mon Jul 15 18:32:18 2024 ] 	Batch(2300/6809) done. Loss: 0.1181  lr:0.000100
[ Mon Jul 15 18:32:42 2024 ] 	Batch(2400/6809) done. Loss: 0.2025  lr:0.000100
[ Mon Jul 15 18:33:04 2024 ] 
Training: Epoch [65/150], Step [2499], Loss: 0.39820989966392517, Training Accuracy: 93.83
[ Mon Jul 15 18:33:04 2024 ] 	Batch(2500/6809) done. Loss: 0.0931  lr:0.000100
[ Mon Jul 15 18:33:27 2024 ] 	Batch(2600/6809) done. Loss: 0.0162  lr:0.000100
[ Mon Jul 15 18:33:50 2024 ] 	Batch(2700/6809) done. Loss: 0.3685  lr:0.000100
[ Mon Jul 15 18:34:14 2024 ] 	Batch(2800/6809) done. Loss: 0.0247  lr:0.000100
[ Mon Jul 15 18:34:37 2024 ] 	Batch(2900/6809) done. Loss: 0.0100  lr:0.000100
[ Mon Jul 15 18:34:59 2024 ] 
Training: Epoch [65/150], Step [2999], Loss: 0.010082552209496498, Training Accuracy: 93.92083333333333
[ Mon Jul 15 18:35:00 2024 ] 	Batch(3000/6809) done. Loss: 0.2697  lr:0.000100
[ Mon Jul 15 18:35:23 2024 ] 	Batch(3100/6809) done. Loss: 0.2634  lr:0.000100
[ Mon Jul 15 18:35:46 2024 ] 	Batch(3200/6809) done. Loss: 0.6318  lr:0.000100
[ Mon Jul 15 18:36:09 2024 ] 	Batch(3300/6809) done. Loss: 0.1221  lr:0.000100
[ Mon Jul 15 18:36:32 2024 ] 	Batch(3400/6809) done. Loss: 0.0179  lr:0.000100
[ Mon Jul 15 18:36:54 2024 ] 
Training: Epoch [65/150], Step [3499], Loss: 0.0838610976934433, Training Accuracy: 93.95714285714286
[ Mon Jul 15 18:36:55 2024 ] 	Batch(3500/6809) done. Loss: 0.3407  lr:0.000100
[ Mon Jul 15 18:37:17 2024 ] 	Batch(3600/6809) done. Loss: 0.0865  lr:0.000100
[ Mon Jul 15 18:37:40 2024 ] 	Batch(3700/6809) done. Loss: 0.2158  lr:0.000100
[ Mon Jul 15 18:38:03 2024 ] 	Batch(3800/6809) done. Loss: 0.3444  lr:0.000100
[ Mon Jul 15 18:38:26 2024 ] 	Batch(3900/6809) done. Loss: 0.1349  lr:0.000100
[ Mon Jul 15 18:38:48 2024 ] 
Training: Epoch [65/150], Step [3999], Loss: 0.6239941716194153, Training Accuracy: 94.025
[ Mon Jul 15 18:38:48 2024 ] 	Batch(4000/6809) done. Loss: 0.1222  lr:0.000100
[ Mon Jul 15 18:39:11 2024 ] 	Batch(4100/6809) done. Loss: 0.1964  lr:0.000100
[ Mon Jul 15 18:39:34 2024 ] 	Batch(4200/6809) done. Loss: 0.2599  lr:0.000100
[ Mon Jul 15 18:39:56 2024 ] 	Batch(4300/6809) done. Loss: 1.3158  lr:0.000100
[ Mon Jul 15 18:40:19 2024 ] 	Batch(4400/6809) done. Loss: 0.0954  lr:0.000100
[ Mon Jul 15 18:40:42 2024 ] 
Training: Epoch [65/150], Step [4499], Loss: 0.1411466896533966, Training Accuracy: 93.96944444444445
[ Mon Jul 15 18:40:42 2024 ] 	Batch(4500/6809) done. Loss: 0.2809  lr:0.000100
[ Mon Jul 15 18:41:05 2024 ] 	Batch(4600/6809) done. Loss: 0.2415  lr:0.000100
[ Mon Jul 15 18:41:28 2024 ] 	Batch(4700/6809) done. Loss: 0.1597  lr:0.000100
[ Mon Jul 15 18:41:50 2024 ] 	Batch(4800/6809) done. Loss: 0.5566  lr:0.000100
[ Mon Jul 15 18:42:13 2024 ] 	Batch(4900/6809) done. Loss: 0.1091  lr:0.000100
[ Mon Jul 15 18:42:36 2024 ] 
Training: Epoch [65/150], Step [4999], Loss: 0.1898241937160492, Training Accuracy: 93.99
[ Mon Jul 15 18:42:36 2024 ] 	Batch(5000/6809) done. Loss: 0.1469  lr:0.000100
[ Mon Jul 15 18:42:58 2024 ] 	Batch(5100/6809) done. Loss: 0.1998  lr:0.000100
[ Mon Jul 15 18:43:21 2024 ] 	Batch(5200/6809) done. Loss: 0.1667  lr:0.000100
[ Mon Jul 15 18:43:44 2024 ] 	Batch(5300/6809) done. Loss: 0.0840  lr:0.000100
[ Mon Jul 15 18:44:07 2024 ] 	Batch(5400/6809) done. Loss: 0.3586  lr:0.000100
[ Mon Jul 15 18:44:29 2024 ] 
Training: Epoch [65/150], Step [5499], Loss: 0.3644804060459137, Training Accuracy: 93.98863636363637
[ Mon Jul 15 18:44:30 2024 ] 	Batch(5500/6809) done. Loss: 0.1349  lr:0.000100
[ Mon Jul 15 18:44:52 2024 ] 	Batch(5600/6809) done. Loss: 0.0885  lr:0.000100
[ Mon Jul 15 18:45:15 2024 ] 	Batch(5700/6809) done. Loss: 0.0906  lr:0.000100
[ Mon Jul 15 18:45:38 2024 ] 	Batch(5800/6809) done. Loss: 0.2172  lr:0.000100
[ Mon Jul 15 18:46:00 2024 ] 	Batch(5900/6809) done. Loss: 0.5060  lr:0.000100
[ Mon Jul 15 18:46:23 2024 ] 
Training: Epoch [65/150], Step [5999], Loss: 0.13684432208538055, Training Accuracy: 94.01666666666667
[ Mon Jul 15 18:46:23 2024 ] 	Batch(6000/6809) done. Loss: 0.0985  lr:0.000100
[ Mon Jul 15 18:46:46 2024 ] 	Batch(6100/6809) done. Loss: 0.1281  lr:0.000100
[ Mon Jul 15 18:47:08 2024 ] 	Batch(6200/6809) done. Loss: 0.0464  lr:0.000100
[ Mon Jul 15 18:47:31 2024 ] 	Batch(6300/6809) done. Loss: 0.6871  lr:0.000100
[ Mon Jul 15 18:47:54 2024 ] 	Batch(6400/6809) done. Loss: 0.0488  lr:0.000100
[ Mon Jul 15 18:48:17 2024 ] 
Training: Epoch [65/150], Step [6499], Loss: 0.030067265033721924, Training Accuracy: 93.9826923076923
[ Mon Jul 15 18:48:17 2024 ] 	Batch(6500/6809) done. Loss: 0.1253  lr:0.000100
[ Mon Jul 15 18:48:40 2024 ] 	Batch(6600/6809) done. Loss: 0.3224  lr:0.000100
[ Mon Jul 15 18:49:02 2024 ] 	Batch(6700/6809) done. Loss: 0.1116  lr:0.000100
[ Mon Jul 15 18:49:26 2024 ] 	Batch(6800/6809) done. Loss: 0.1463  lr:0.000100
[ Mon Jul 15 18:49:28 2024 ] 	Mean training loss: 0.2064.
[ Mon Jul 15 18:49:28 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 18:49:28 2024 ] Training epoch: 67
[ Mon Jul 15 18:49:28 2024 ] 	Batch(0/6809) done. Loss: 0.7070  lr:0.000100
[ Mon Jul 15 18:49:52 2024 ] 	Batch(100/6809) done. Loss: 0.0212  lr:0.000100
[ Mon Jul 15 18:50:15 2024 ] 	Batch(200/6809) done. Loss: 0.0960  lr:0.000100
[ Mon Jul 15 18:50:38 2024 ] 	Batch(300/6809) done. Loss: 0.2609  lr:0.000100
[ Mon Jul 15 18:51:01 2024 ] 	Batch(400/6809) done. Loss: 0.0567  lr:0.000100
[ Mon Jul 15 18:51:24 2024 ] 
Training: Epoch [66/150], Step [499], Loss: 0.27851852774620056, Training Accuracy: 94.55
[ Mon Jul 15 18:51:24 2024 ] 	Batch(500/6809) done. Loss: 0.2672  lr:0.000100
[ Mon Jul 15 18:51:47 2024 ] 	Batch(600/6809) done. Loss: 0.0038  lr:0.000100
[ Mon Jul 15 18:52:10 2024 ] 	Batch(700/6809) done. Loss: 0.3145  lr:0.000100
[ Mon Jul 15 18:52:33 2024 ] 	Batch(800/6809) done. Loss: 0.0622  lr:0.000100
[ Mon Jul 15 18:52:55 2024 ] 	Batch(900/6809) done. Loss: 0.0319  lr:0.000100
[ Mon Jul 15 18:53:18 2024 ] 
Training: Epoch [66/150], Step [999], Loss: 0.041947249323129654, Training Accuracy: 94.6
[ Mon Jul 15 18:53:18 2024 ] 	Batch(1000/6809) done. Loss: 0.2003  lr:0.000100
[ Mon Jul 15 18:53:41 2024 ] 	Batch(1100/6809) done. Loss: 0.0199  lr:0.000100
[ Mon Jul 15 18:54:03 2024 ] 	Batch(1200/6809) done. Loss: 0.0349  lr:0.000100
[ Mon Jul 15 18:54:26 2024 ] 	Batch(1300/6809) done. Loss: 0.0650  lr:0.000100
[ Mon Jul 15 18:54:49 2024 ] 	Batch(1400/6809) done. Loss: 0.1007  lr:0.000100
[ Mon Jul 15 18:55:11 2024 ] 
Training: Epoch [66/150], Step [1499], Loss: 0.2720547020435333, Training Accuracy: 94.54166666666667
[ Mon Jul 15 18:55:11 2024 ] 	Batch(1500/6809) done. Loss: 0.1743  lr:0.000100
[ Mon Jul 15 18:55:34 2024 ] 	Batch(1600/6809) done. Loss: 0.0989  lr:0.000100
[ Mon Jul 15 18:55:57 2024 ] 	Batch(1700/6809) done. Loss: 0.0327  lr:0.000100
[ Mon Jul 15 18:56:20 2024 ] 	Batch(1800/6809) done. Loss: 0.1911  lr:0.000100
[ Mon Jul 15 18:56:42 2024 ] 	Batch(1900/6809) done. Loss: 0.0404  lr:0.000100
[ Mon Jul 15 18:57:05 2024 ] 
Training: Epoch [66/150], Step [1999], Loss: 0.30281782150268555, Training Accuracy: 94.3875
[ Mon Jul 15 18:57:05 2024 ] 	Batch(2000/6809) done. Loss: 0.0156  lr:0.000100
[ Mon Jul 15 18:57:28 2024 ] 	Batch(2100/6809) done. Loss: 0.0772  lr:0.000100
[ Mon Jul 15 18:57:51 2024 ] 	Batch(2200/6809) done. Loss: 0.2922  lr:0.000100
[ Mon Jul 15 18:58:13 2024 ] 	Batch(2300/6809) done. Loss: 0.1351  lr:0.000100
[ Mon Jul 15 18:58:36 2024 ] 	Batch(2400/6809) done. Loss: 0.2510  lr:0.000100
[ Mon Jul 15 18:58:58 2024 ] 
Training: Epoch [66/150], Step [2499], Loss: 0.04574583098292351, Training Accuracy: 94.49
[ Mon Jul 15 18:58:59 2024 ] 	Batch(2500/6809) done. Loss: 0.1072  lr:0.000100
[ Mon Jul 15 18:59:21 2024 ] 	Batch(2600/6809) done. Loss: 0.6207  lr:0.000100
[ Mon Jul 15 18:59:44 2024 ] 	Batch(2700/6809) done. Loss: 0.3868  lr:0.000100
[ Mon Jul 15 19:00:07 2024 ] 	Batch(2800/6809) done. Loss: 0.2252  lr:0.000100
[ Mon Jul 15 19:00:30 2024 ] 	Batch(2900/6809) done. Loss: 0.0663  lr:0.000100
[ Mon Jul 15 19:00:52 2024 ] 
Training: Epoch [66/150], Step [2999], Loss: 0.26652735471725464, Training Accuracy: 94.325
[ Mon Jul 15 19:00:52 2024 ] 	Batch(3000/6809) done. Loss: 0.1187  lr:0.000100
[ Mon Jul 15 19:01:15 2024 ] 	Batch(3100/6809) done. Loss: 0.0910  lr:0.000100
[ Mon Jul 15 19:01:38 2024 ] 	Batch(3200/6809) done. Loss: 0.2045  lr:0.000100
[ Mon Jul 15 19:02:00 2024 ] 	Batch(3300/6809) done. Loss: 0.4031  lr:0.000100
[ Mon Jul 15 19:02:23 2024 ] 	Batch(3400/6809) done. Loss: 0.5344  lr:0.000100
[ Mon Jul 15 19:02:45 2024 ] 
Training: Epoch [66/150], Step [3499], Loss: 0.43099266290664673, Training Accuracy: 94.29285714285714
[ Mon Jul 15 19:02:46 2024 ] 	Batch(3500/6809) done. Loss: 0.1393  lr:0.000100
[ Mon Jul 15 19:03:09 2024 ] 	Batch(3600/6809) done. Loss: 0.1204  lr:0.000100
[ Mon Jul 15 19:03:31 2024 ] 	Batch(3700/6809) done. Loss: 0.2484  lr:0.000100
[ Mon Jul 15 19:03:54 2024 ] 	Batch(3800/6809) done. Loss: 0.0321  lr:0.000100
[ Mon Jul 15 19:04:16 2024 ] 	Batch(3900/6809) done. Loss: 0.5482  lr:0.000100
[ Mon Jul 15 19:04:39 2024 ] 
Training: Epoch [66/150], Step [3999], Loss: 0.1844807118177414, Training Accuracy: 94.28125
[ Mon Jul 15 19:04:39 2024 ] 	Batch(4000/6809) done. Loss: 0.4458  lr:0.000100
[ Mon Jul 15 19:05:02 2024 ] 	Batch(4100/6809) done. Loss: 0.0944  lr:0.000100
[ Mon Jul 15 19:05:25 2024 ] 	Batch(4200/6809) done. Loss: 0.0322  lr:0.000100
[ Mon Jul 15 19:05:47 2024 ] 	Batch(4300/6809) done. Loss: 0.2147  lr:0.000100
[ Mon Jul 15 19:06:10 2024 ] 	Batch(4400/6809) done. Loss: 0.2104  lr:0.000100
[ Mon Jul 15 19:06:32 2024 ] 
Training: Epoch [66/150], Step [4499], Loss: 0.03232419118285179, Training Accuracy: 94.28888888888889
[ Mon Jul 15 19:06:33 2024 ] 	Batch(4500/6809) done. Loss: 0.2559  lr:0.000100
[ Mon Jul 15 19:06:55 2024 ] 	Batch(4600/6809) done. Loss: 0.1096  lr:0.000100
[ Mon Jul 15 19:07:18 2024 ] 	Batch(4700/6809) done. Loss: 0.0234  lr:0.000100
[ Mon Jul 15 19:07:41 2024 ] 	Batch(4800/6809) done. Loss: 0.3965  lr:0.000100
[ Mon Jul 15 19:08:04 2024 ] 	Batch(4900/6809) done. Loss: 0.0383  lr:0.000100
[ Mon Jul 15 19:08:27 2024 ] 
Training: Epoch [66/150], Step [4999], Loss: 0.039955779910087585, Training Accuracy: 94.26
[ Mon Jul 15 19:08:27 2024 ] 	Batch(5000/6809) done. Loss: 0.1787  lr:0.000100
[ Mon Jul 15 19:08:49 2024 ] 	Batch(5100/6809) done. Loss: 0.2634  lr:0.000100
[ Mon Jul 15 19:09:12 2024 ] 	Batch(5200/6809) done. Loss: 0.1454  lr:0.000100
[ Mon Jul 15 19:09:35 2024 ] 	Batch(5300/6809) done. Loss: 0.0071  lr:0.000100
[ Mon Jul 15 19:09:58 2024 ] 	Batch(5400/6809) done. Loss: 0.0449  lr:0.000100
[ Mon Jul 15 19:10:20 2024 ] 
Training: Epoch [66/150], Step [5499], Loss: 0.19278967380523682, Training Accuracy: 94.26818181818182
[ Mon Jul 15 19:10:20 2024 ] 	Batch(5500/6809) done. Loss: 0.4312  lr:0.000100
[ Mon Jul 15 19:10:43 2024 ] 	Batch(5600/6809) done. Loss: 0.3046  lr:0.000100
[ Mon Jul 15 19:11:06 2024 ] 	Batch(5700/6809) done. Loss: 0.1984  lr:0.000100
[ Mon Jul 15 19:11:29 2024 ] 	Batch(5800/6809) done. Loss: 0.0876  lr:0.000100
[ Mon Jul 15 19:11:52 2024 ] 	Batch(5900/6809) done. Loss: 0.0534  lr:0.000100
[ Mon Jul 15 19:12:14 2024 ] 
Training: Epoch [66/150], Step [5999], Loss: 0.10369758307933807, Training Accuracy: 94.22500000000001
[ Mon Jul 15 19:12:15 2024 ] 	Batch(6000/6809) done. Loss: 0.1393  lr:0.000100
[ Mon Jul 15 19:12:38 2024 ] 	Batch(6100/6809) done. Loss: 0.4359  lr:0.000100
[ Mon Jul 15 19:13:01 2024 ] 	Batch(6200/6809) done. Loss: 0.0662  lr:0.000100
[ Mon Jul 15 19:13:24 2024 ] 	Batch(6300/6809) done. Loss: 0.0603  lr:0.000100
[ Mon Jul 15 19:13:47 2024 ] 	Batch(6400/6809) done. Loss: 0.1675  lr:0.000100
[ Mon Jul 15 19:14:10 2024 ] 
Training: Epoch [66/150], Step [6499], Loss: 0.07831452041864395, Training Accuracy: 94.29038461538461
[ Mon Jul 15 19:14:10 2024 ] 	Batch(6500/6809) done. Loss: 0.0455  lr:0.000100
[ Mon Jul 15 19:14:33 2024 ] 	Batch(6600/6809) done. Loss: 0.2122  lr:0.000100
[ Mon Jul 15 19:14:56 2024 ] 	Batch(6700/6809) done. Loss: 0.0747  lr:0.000100
[ Mon Jul 15 19:15:19 2024 ] 	Batch(6800/6809) done. Loss: 0.7673  lr:0.000100
[ Mon Jul 15 19:15:20 2024 ] 	Mean training loss: 0.1950.
[ Mon Jul 15 19:15:20 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 19:15:21 2024 ] Training epoch: 68
[ Mon Jul 15 19:15:21 2024 ] 	Batch(0/6809) done. Loss: 0.0205  lr:0.000100
[ Mon Jul 15 19:15:44 2024 ] 	Batch(100/6809) done. Loss: 0.3375  lr:0.000100
[ Mon Jul 15 19:16:07 2024 ] 	Batch(200/6809) done. Loss: 0.1921  lr:0.000100
[ Mon Jul 15 19:16:30 2024 ] 	Batch(300/6809) done. Loss: 0.1702  lr:0.000100
[ Mon Jul 15 19:16:52 2024 ] 	Batch(400/6809) done. Loss: 0.9953  lr:0.000100
[ Mon Jul 15 19:17:15 2024 ] 
Training: Epoch [67/150], Step [499], Loss: 0.012723174877464771, Training Accuracy: 94.325
[ Mon Jul 15 19:17:15 2024 ] 	Batch(500/6809) done. Loss: 0.1366  lr:0.000100
[ Mon Jul 15 19:17:38 2024 ] 	Batch(600/6809) done. Loss: 0.0777  lr:0.000100
[ Mon Jul 15 19:18:00 2024 ] 	Batch(700/6809) done. Loss: 0.3551  lr:0.000100
[ Mon Jul 15 19:18:23 2024 ] 	Batch(800/6809) done. Loss: 0.2503  lr:0.000100
[ Mon Jul 15 19:18:46 2024 ] 	Batch(900/6809) done. Loss: 0.5399  lr:0.000100
[ Mon Jul 15 19:19:08 2024 ] 
Training: Epoch [67/150], Step [999], Loss: 0.07782040536403656, Training Accuracy: 94.19999999999999
[ Mon Jul 15 19:19:08 2024 ] 	Batch(1000/6809) done. Loss: 0.0128  lr:0.000100
[ Mon Jul 15 19:19:31 2024 ] 	Batch(1100/6809) done. Loss: 0.1669  lr:0.000100
[ Mon Jul 15 19:19:54 2024 ] 	Batch(1200/6809) done. Loss: 0.3502  lr:0.000100
[ Mon Jul 15 19:20:17 2024 ] 	Batch(1300/6809) done. Loss: 0.5541  lr:0.000100
[ Mon Jul 15 19:20:39 2024 ] 	Batch(1400/6809) done. Loss: 0.1130  lr:0.000100
[ Mon Jul 15 19:21:02 2024 ] 
Training: Epoch [67/150], Step [1499], Loss: 0.020985091105103493, Training Accuracy: 94.15
[ Mon Jul 15 19:21:02 2024 ] 	Batch(1500/6809) done. Loss: 0.4221  lr:0.000100
[ Mon Jul 15 19:21:25 2024 ] 	Batch(1600/6809) done. Loss: 0.1079  lr:0.000100
[ Mon Jul 15 19:21:47 2024 ] 	Batch(1700/6809) done. Loss: 0.5362  lr:0.000100
[ Mon Jul 15 19:22:10 2024 ] 	Batch(1800/6809) done. Loss: 0.0492  lr:0.000100
[ Mon Jul 15 19:22:33 2024 ] 	Batch(1900/6809) done. Loss: 0.6528  lr:0.000100
[ Mon Jul 15 19:22:55 2024 ] 
Training: Epoch [67/150], Step [1999], Loss: 0.19526825845241547, Training Accuracy: 94.16875
[ Mon Jul 15 19:22:56 2024 ] 	Batch(2000/6809) done. Loss: 0.2143  lr:0.000100
[ Mon Jul 15 19:23:18 2024 ] 	Batch(2100/6809) done. Loss: 0.2344  lr:0.000100
[ Mon Jul 15 19:23:41 2024 ] 	Batch(2200/6809) done. Loss: 0.1936  lr:0.000100
[ Mon Jul 15 19:24:03 2024 ] 	Batch(2300/6809) done. Loss: 0.0644  lr:0.000100
[ Mon Jul 15 19:24:26 2024 ] 	Batch(2400/6809) done. Loss: 0.0357  lr:0.000100
[ Mon Jul 15 19:24:49 2024 ] 
Training: Epoch [67/150], Step [2499], Loss: 0.4752032160758972, Training Accuracy: 94.265
[ Mon Jul 15 19:24:49 2024 ] 	Batch(2500/6809) done. Loss: 0.6394  lr:0.000100
[ Mon Jul 15 19:25:12 2024 ] 	Batch(2600/6809) done. Loss: 0.2381  lr:0.000100
[ Mon Jul 15 19:25:34 2024 ] 	Batch(2700/6809) done. Loss: 0.0707  lr:0.000100
[ Mon Jul 15 19:25:57 2024 ] 	Batch(2800/6809) done. Loss: 0.2659  lr:0.000100
[ Mon Jul 15 19:26:20 2024 ] 	Batch(2900/6809) done. Loss: 0.6722  lr:0.000100
[ Mon Jul 15 19:26:43 2024 ] 
Training: Epoch [67/150], Step [2999], Loss: 0.2814100682735443, Training Accuracy: 94.25416666666668
[ Mon Jul 15 19:26:43 2024 ] 	Batch(3000/6809) done. Loss: 0.6548  lr:0.000100
[ Mon Jul 15 19:27:06 2024 ] 	Batch(3100/6809) done. Loss: 0.0604  lr:0.000100
[ Mon Jul 15 19:27:29 2024 ] 	Batch(3200/6809) done. Loss: 0.3777  lr:0.000100
[ Mon Jul 15 19:27:51 2024 ] 	Batch(3300/6809) done. Loss: 0.1449  lr:0.000100
[ Mon Jul 15 19:28:14 2024 ] 	Batch(3400/6809) done. Loss: 0.2678  lr:0.000100
[ Mon Jul 15 19:28:36 2024 ] 
Training: Epoch [67/150], Step [3499], Loss: 0.23813562095165253, Training Accuracy: 94.22142857142856
[ Mon Jul 15 19:28:37 2024 ] 	Batch(3500/6809) done. Loss: 0.0023  lr:0.000100
[ Mon Jul 15 19:28:59 2024 ] 	Batch(3600/6809) done. Loss: 0.1543  lr:0.000100
[ Mon Jul 15 19:29:22 2024 ] 	Batch(3700/6809) done. Loss: 0.1545  lr:0.000100
[ Mon Jul 15 19:29:45 2024 ] 	Batch(3800/6809) done. Loss: 0.4286  lr:0.000100
[ Mon Jul 15 19:30:07 2024 ] 	Batch(3900/6809) done. Loss: 0.1505  lr:0.000100
[ Mon Jul 15 19:30:30 2024 ] 
Training: Epoch [67/150], Step [3999], Loss: 0.030804095789790154, Training Accuracy: 94.278125
[ Mon Jul 15 19:30:30 2024 ] 	Batch(4000/6809) done. Loss: 0.0147  lr:0.000100
[ Mon Jul 15 19:30:53 2024 ] 	Batch(4100/6809) done. Loss: 0.1335  lr:0.000100
[ Mon Jul 15 19:31:15 2024 ] 	Batch(4200/6809) done. Loss: 0.0442  lr:0.000100
[ Mon Jul 15 19:31:38 2024 ] 	Batch(4300/6809) done. Loss: 0.0456  lr:0.000100
[ Mon Jul 15 19:32:01 2024 ] 	Batch(4400/6809) done. Loss: 0.0284  lr:0.000100
[ Mon Jul 15 19:32:24 2024 ] 
Training: Epoch [67/150], Step [4499], Loss: 0.05909914895892143, Training Accuracy: 94.24166666666667
[ Mon Jul 15 19:32:24 2024 ] 	Batch(4500/6809) done. Loss: 0.2397  lr:0.000100
[ Mon Jul 15 19:32:47 2024 ] 	Batch(4600/6809) done. Loss: 0.0276  lr:0.000100
[ Mon Jul 15 19:33:10 2024 ] 	Batch(4700/6809) done. Loss: 0.1337  lr:0.000100
[ Mon Jul 15 19:33:33 2024 ] 	Batch(4800/6809) done. Loss: 0.0789  lr:0.000100
[ Mon Jul 15 19:33:56 2024 ] 	Batch(4900/6809) done. Loss: 0.2255  lr:0.000100
[ Mon Jul 15 19:34:18 2024 ] 
Training: Epoch [67/150], Step [4999], Loss: 0.04700860381126404, Training Accuracy: 94.2775
[ Mon Jul 15 19:34:18 2024 ] 	Batch(5000/6809) done. Loss: 0.3033  lr:0.000100
[ Mon Jul 15 19:34:41 2024 ] 	Batch(5100/6809) done. Loss: 0.1409  lr:0.000100
[ Mon Jul 15 19:35:04 2024 ] 	Batch(5200/6809) done. Loss: 0.1336  lr:0.000100
[ Mon Jul 15 19:35:26 2024 ] 	Batch(5300/6809) done. Loss: 0.5527  lr:0.000100
[ Mon Jul 15 19:35:49 2024 ] 	Batch(5400/6809) done. Loss: 0.1121  lr:0.000100
[ Mon Jul 15 19:36:12 2024 ] 
Training: Epoch [67/150], Step [5499], Loss: 0.09193576127290726, Training Accuracy: 94.26590909090909
[ Mon Jul 15 19:36:12 2024 ] 	Batch(5500/6809) done. Loss: 0.3481  lr:0.000100
[ Mon Jul 15 19:36:35 2024 ] 	Batch(5600/6809) done. Loss: 0.0281  lr:0.000100
[ Mon Jul 15 19:36:57 2024 ] 	Batch(5700/6809) done. Loss: 0.1290  lr:0.000100
[ Mon Jul 15 19:37:20 2024 ] 	Batch(5800/6809) done. Loss: 0.0242  lr:0.000100
[ Mon Jul 15 19:37:43 2024 ] 	Batch(5900/6809) done. Loss: 0.0315  lr:0.000100
[ Mon Jul 15 19:38:05 2024 ] 
Training: Epoch [67/150], Step [5999], Loss: 0.024731488898396492, Training Accuracy: 94.29166666666666
[ Mon Jul 15 19:38:05 2024 ] 	Batch(6000/6809) done. Loss: 0.0182  lr:0.000100
[ Mon Jul 15 19:38:28 2024 ] 	Batch(6100/6809) done. Loss: 0.0401  lr:0.000100
[ Mon Jul 15 19:38:51 2024 ] 	Batch(6200/6809) done. Loss: 0.0749  lr:0.000100
[ Mon Jul 15 19:39:13 2024 ] 	Batch(6300/6809) done. Loss: 0.0503  lr:0.000100
[ Mon Jul 15 19:39:36 2024 ] 	Batch(6400/6809) done. Loss: 0.1797  lr:0.000100
[ Mon Jul 15 19:39:59 2024 ] 
Training: Epoch [67/150], Step [6499], Loss: 0.006498269736766815, Training Accuracy: 94.28076923076924
[ Mon Jul 15 19:39:59 2024 ] 	Batch(6500/6809) done. Loss: 0.0617  lr:0.000100
[ Mon Jul 15 19:40:22 2024 ] 	Batch(6600/6809) done. Loss: 0.1618  lr:0.000100
[ Mon Jul 15 19:40:46 2024 ] 	Batch(6700/6809) done. Loss: 0.2416  lr:0.000100
[ Mon Jul 15 19:41:09 2024 ] 	Batch(6800/6809) done. Loss: 0.2068  lr:0.000100
[ Mon Jul 15 19:41:11 2024 ] 	Mean training loss: 0.1976.
[ Mon Jul 15 19:41:11 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 19:41:11 2024 ] Training epoch: 69
[ Mon Jul 15 19:41:12 2024 ] 	Batch(0/6809) done. Loss: 0.4608  lr:0.000100
[ Mon Jul 15 19:41:35 2024 ] 	Batch(100/6809) done. Loss: 0.0305  lr:0.000100
[ Mon Jul 15 19:41:58 2024 ] 	Batch(200/6809) done. Loss: 0.3399  lr:0.000100
[ Mon Jul 15 19:42:21 2024 ] 	Batch(300/6809) done. Loss: 0.0367  lr:0.000100
[ Mon Jul 15 19:42:44 2024 ] 	Batch(400/6809) done. Loss: 0.1944  lr:0.000100
[ Mon Jul 15 19:43:07 2024 ] 
Training: Epoch [68/150], Step [499], Loss: 0.09647256135940552, Training Accuracy: 94.925
[ Mon Jul 15 19:43:07 2024 ] 	Batch(500/6809) done. Loss: 0.2730  lr:0.000100
[ Mon Jul 15 19:43:30 2024 ] 	Batch(600/6809) done. Loss: 0.1006  lr:0.000100
[ Mon Jul 15 19:43:54 2024 ] 	Batch(700/6809) done. Loss: 0.0666  lr:0.000100
[ Mon Jul 15 19:44:17 2024 ] 	Batch(800/6809) done. Loss: 0.1629  lr:0.000100
[ Mon Jul 15 19:44:40 2024 ] 	Batch(900/6809) done. Loss: 0.2493  lr:0.000100
[ Mon Jul 15 19:45:03 2024 ] 
Training: Epoch [68/150], Step [999], Loss: 0.12289798259735107, Training Accuracy: 94.5125
[ Mon Jul 15 19:45:03 2024 ] 	Batch(1000/6809) done. Loss: 0.1204  lr:0.000100
[ Mon Jul 15 19:45:26 2024 ] 	Batch(1100/6809) done. Loss: 0.4645  lr:0.000100
[ Mon Jul 15 19:45:49 2024 ] 	Batch(1200/6809) done. Loss: 0.2655  lr:0.000100
[ Mon Jul 15 19:46:12 2024 ] 	Batch(1300/6809) done. Loss: 0.2175  lr:0.000100
[ Mon Jul 15 19:46:35 2024 ] 	Batch(1400/6809) done. Loss: 0.0192  lr:0.000100
[ Mon Jul 15 19:46:58 2024 ] 
Training: Epoch [68/150], Step [1499], Loss: 0.9776989817619324, Training Accuracy: 94.64166666666667
[ Mon Jul 15 19:46:59 2024 ] 	Batch(1500/6809) done. Loss: 0.2293  lr:0.000100
[ Mon Jul 15 19:47:22 2024 ] 	Batch(1600/6809) done. Loss: 0.0494  lr:0.000100
[ Mon Jul 15 19:47:45 2024 ] 	Batch(1700/6809) done. Loss: 0.0130  lr:0.000100
[ Mon Jul 15 19:48:08 2024 ] 	Batch(1800/6809) done. Loss: 0.0475  lr:0.000100
[ Mon Jul 15 19:48:31 2024 ] 	Batch(1900/6809) done. Loss: 0.0378  lr:0.000100
[ Mon Jul 15 19:48:54 2024 ] 
Training: Epoch [68/150], Step [1999], Loss: 0.012484428472816944, Training Accuracy: 94.74375
[ Mon Jul 15 19:48:54 2024 ] 	Batch(2000/6809) done. Loss: 0.4041  lr:0.000100
[ Mon Jul 15 19:49:17 2024 ] 	Batch(2100/6809) done. Loss: 0.1365  lr:0.000100
[ Mon Jul 15 19:49:41 2024 ] 	Batch(2200/6809) done. Loss: 0.2590  lr:0.000100
[ Mon Jul 15 19:50:04 2024 ] 	Batch(2300/6809) done. Loss: 0.0797  lr:0.000100
[ Mon Jul 15 19:50:26 2024 ] 	Batch(2400/6809) done. Loss: 0.1380  lr:0.000100
[ Mon Jul 15 19:50:49 2024 ] 
Training: Epoch [68/150], Step [2499], Loss: 0.14824192225933075, Training Accuracy: 94.67999999999999
[ Mon Jul 15 19:50:49 2024 ] 	Batch(2500/6809) done. Loss: 0.0229  lr:0.000100
[ Mon Jul 15 19:51:12 2024 ] 	Batch(2600/6809) done. Loss: 0.0427  lr:0.000100
[ Mon Jul 15 19:51:34 2024 ] 	Batch(2700/6809) done. Loss: 0.5006  lr:0.000100
[ Mon Jul 15 19:51:57 2024 ] 	Batch(2800/6809) done. Loss: 0.0722  lr:0.000100
[ Mon Jul 15 19:52:20 2024 ] 	Batch(2900/6809) done. Loss: 0.0825  lr:0.000100
[ Mon Jul 15 19:52:42 2024 ] 
Training: Epoch [68/150], Step [2999], Loss: 0.1366363763809204, Training Accuracy: 94.6
[ Mon Jul 15 19:52:43 2024 ] 	Batch(3000/6809) done. Loss: 0.0152  lr:0.000100
[ Mon Jul 15 19:53:05 2024 ] 	Batch(3100/6809) done. Loss: 0.3835  lr:0.000100
[ Mon Jul 15 19:53:28 2024 ] 	Batch(3200/6809) done. Loss: 0.0021  lr:0.000100
[ Mon Jul 15 19:53:51 2024 ] 	Batch(3300/6809) done. Loss: 0.0301  lr:0.000100
[ Mon Jul 15 19:54:13 2024 ] 	Batch(3400/6809) done. Loss: 0.2298  lr:0.000100
[ Mon Jul 15 19:54:36 2024 ] 
Training: Epoch [68/150], Step [3499], Loss: 0.02894948050379753, Training Accuracy: 94.65357142857142
[ Mon Jul 15 19:54:36 2024 ] 	Batch(3500/6809) done. Loss: 0.3029  lr:0.000100
[ Mon Jul 15 19:54:59 2024 ] 	Batch(3600/6809) done. Loss: 0.1697  lr:0.000100
[ Mon Jul 15 19:55:22 2024 ] 	Batch(3700/6809) done. Loss: 0.0340  lr:0.000100
[ Mon Jul 15 19:55:44 2024 ] 	Batch(3800/6809) done. Loss: 0.1415  lr:0.000100
[ Mon Jul 15 19:56:07 2024 ] 	Batch(3900/6809) done. Loss: 0.8851  lr:0.000100
[ Mon Jul 15 19:56:29 2024 ] 
Training: Epoch [68/150], Step [3999], Loss: 0.07750795781612396, Training Accuracy: 94.58125
[ Mon Jul 15 19:56:30 2024 ] 	Batch(4000/6809) done. Loss: 0.2848  lr:0.000100
[ Mon Jul 15 19:56:53 2024 ] 	Batch(4100/6809) done. Loss: 0.1782  lr:0.000100
[ Mon Jul 15 19:57:16 2024 ] 	Batch(4200/6809) done. Loss: 0.0939  lr:0.000100
[ Mon Jul 15 19:57:39 2024 ] 	Batch(4300/6809) done. Loss: 0.2176  lr:0.000100
[ Mon Jul 15 19:58:02 2024 ] 	Batch(4400/6809) done. Loss: 0.1986  lr:0.000100
[ Mon Jul 15 19:58:25 2024 ] 
Training: Epoch [68/150], Step [4499], Loss: 0.25925493240356445, Training Accuracy: 94.5111111111111
[ Mon Jul 15 19:58:25 2024 ] 	Batch(4500/6809) done. Loss: 0.0436  lr:0.000100
[ Mon Jul 15 19:58:48 2024 ] 	Batch(4600/6809) done. Loss: 0.0147  lr:0.000100
[ Mon Jul 15 19:59:11 2024 ] 	Batch(4700/6809) done. Loss: 0.3194  lr:0.000100
[ Mon Jul 15 19:59:34 2024 ] 	Batch(4800/6809) done. Loss: 0.0820  lr:0.000100
[ Mon Jul 15 19:59:56 2024 ] 	Batch(4900/6809) done. Loss: 0.0251  lr:0.000100
[ Mon Jul 15 20:00:19 2024 ] 
Training: Epoch [68/150], Step [4999], Loss: 0.28597497940063477, Training Accuracy: 94.5275
[ Mon Jul 15 20:00:19 2024 ] 	Batch(5000/6809) done. Loss: 0.3966  lr:0.000100
[ Mon Jul 15 20:00:42 2024 ] 	Batch(5100/6809) done. Loss: 0.2063  lr:0.000100
[ Mon Jul 15 20:01:04 2024 ] 	Batch(5200/6809) done. Loss: 0.5175  lr:0.000100
[ Mon Jul 15 20:01:27 2024 ] 	Batch(5300/6809) done. Loss: 0.1441  lr:0.000100
[ Mon Jul 15 20:01:50 2024 ] 	Batch(5400/6809) done. Loss: 0.0975  lr:0.000100
[ Mon Jul 15 20:02:12 2024 ] 
Training: Epoch [68/150], Step [5499], Loss: 0.10581043362617493, Training Accuracy: 94.50909090909092
[ Mon Jul 15 20:02:12 2024 ] 	Batch(5500/6809) done. Loss: 0.0700  lr:0.000100
[ Mon Jul 15 20:02:35 2024 ] 	Batch(5600/6809) done. Loss: 0.2602  lr:0.000100
[ Mon Jul 15 20:02:58 2024 ] 	Batch(5700/6809) done. Loss: 0.0861  lr:0.000100
[ Mon Jul 15 20:03:21 2024 ] 	Batch(5800/6809) done. Loss: 0.1194  lr:0.000100
[ Mon Jul 15 20:03:43 2024 ] 	Batch(5900/6809) done. Loss: 0.0842  lr:0.000100
[ Mon Jul 15 20:04:06 2024 ] 
Training: Epoch [68/150], Step [5999], Loss: 0.2422153651714325, Training Accuracy: 94.50208333333333
[ Mon Jul 15 20:04:06 2024 ] 	Batch(6000/6809) done. Loss: 0.1029  lr:0.000100
[ Mon Jul 15 20:04:29 2024 ] 	Batch(6100/6809) done. Loss: 0.0776  lr:0.000100
[ Mon Jul 15 20:04:51 2024 ] 	Batch(6200/6809) done. Loss: 0.6616  lr:0.000100
[ Mon Jul 15 20:05:14 2024 ] 	Batch(6300/6809) done. Loss: 0.2664  lr:0.000100
[ Mon Jul 15 20:05:37 2024 ] 	Batch(6400/6809) done. Loss: 0.0782  lr:0.000100
[ Mon Jul 15 20:05:59 2024 ] 
Training: Epoch [68/150], Step [6499], Loss: 0.5658973455429077, Training Accuracy: 94.50192307692308
[ Mon Jul 15 20:06:00 2024 ] 	Batch(6500/6809) done. Loss: 0.0383  lr:0.000100
[ Mon Jul 15 20:06:22 2024 ] 	Batch(6600/6809) done. Loss: 0.0993  lr:0.000100
[ Mon Jul 15 20:06:45 2024 ] 	Batch(6700/6809) done. Loss: 0.2883  lr:0.000100
[ Mon Jul 15 20:07:08 2024 ] 	Batch(6800/6809) done. Loss: 0.1820  lr:0.000100
[ Mon Jul 15 20:07:10 2024 ] 	Mean training loss: 0.1944.
[ Mon Jul 15 20:07:10 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 20:07:10 2024 ] Training epoch: 70
[ Mon Jul 15 20:07:10 2024 ] 	Batch(0/6809) done. Loss: 0.1444  lr:0.000100
[ Mon Jul 15 20:07:33 2024 ] 	Batch(100/6809) done. Loss: 0.1144  lr:0.000100
[ Mon Jul 15 20:07:56 2024 ] 	Batch(200/6809) done. Loss: 0.1688  lr:0.000100
[ Mon Jul 15 20:08:20 2024 ] 	Batch(300/6809) done. Loss: 0.0225  lr:0.000100
[ Mon Jul 15 20:08:43 2024 ] 	Batch(400/6809) done. Loss: 0.0973  lr:0.000100
[ Mon Jul 15 20:09:05 2024 ] 
Training: Epoch [69/150], Step [499], Loss: 0.0427858792245388, Training Accuracy: 94.69999999999999
[ Mon Jul 15 20:09:05 2024 ] 	Batch(500/6809) done. Loss: 0.1521  lr:0.000100
[ Mon Jul 15 20:09:28 2024 ] 	Batch(600/6809) done. Loss: 0.2241  lr:0.000100
[ Mon Jul 15 20:09:51 2024 ] 	Batch(700/6809) done. Loss: 0.0615  lr:0.000100
[ Mon Jul 15 20:10:14 2024 ] 	Batch(800/6809) done. Loss: 0.4965  lr:0.000100
[ Mon Jul 15 20:10:37 2024 ] 	Batch(900/6809) done. Loss: 0.1419  lr:0.000100
[ Mon Jul 15 20:11:00 2024 ] 
Training: Epoch [69/150], Step [999], Loss: 0.025203490629792213, Training Accuracy: 94.69999999999999
[ Mon Jul 15 20:11:00 2024 ] 	Batch(1000/6809) done. Loss: 0.1834  lr:0.000100
[ Mon Jul 15 20:11:24 2024 ] 	Batch(1100/6809) done. Loss: 0.1277  lr:0.000100
[ Mon Jul 15 20:11:47 2024 ] 	Batch(1200/6809) done. Loss: 0.1393  lr:0.000100
[ Mon Jul 15 20:12:10 2024 ] 	Batch(1300/6809) done. Loss: 0.0280  lr:0.000100
[ Mon Jul 15 20:12:33 2024 ] 	Batch(1400/6809) done. Loss: 0.3464  lr:0.000100
[ Mon Jul 15 20:12:56 2024 ] 
Training: Epoch [69/150], Step [1499], Loss: 0.12099207192659378, Training Accuracy: 94.72500000000001
[ Mon Jul 15 20:12:56 2024 ] 	Batch(1500/6809) done. Loss: 0.1763  lr:0.000100
[ Mon Jul 15 20:13:19 2024 ] 	Batch(1600/6809) done. Loss: 0.0567  lr:0.000100
[ Mon Jul 15 20:13:42 2024 ] 	Batch(1700/6809) done. Loss: 0.0535  lr:0.000100
[ Mon Jul 15 20:14:05 2024 ] 	Batch(1800/6809) done. Loss: 0.2879  lr:0.000100
[ Mon Jul 15 20:14:29 2024 ] 	Batch(1900/6809) done. Loss: 0.0362  lr:0.000100
[ Mon Jul 15 20:14:51 2024 ] 
Training: Epoch [69/150], Step [1999], Loss: 0.3852599561214447, Training Accuracy: 94.7125
[ Mon Jul 15 20:14:52 2024 ] 	Batch(2000/6809) done. Loss: 0.3035  lr:0.000100
[ Mon Jul 15 20:15:15 2024 ] 	Batch(2100/6809) done. Loss: 0.0410  lr:0.000100
[ Mon Jul 15 20:15:38 2024 ] 	Batch(2200/6809) done. Loss: 0.1865  lr:0.000100
[ Mon Jul 15 20:16:01 2024 ] 	Batch(2300/6809) done. Loss: 0.0941  lr:0.000100
[ Mon Jul 15 20:16:24 2024 ] 	Batch(2400/6809) done. Loss: 0.3470  lr:0.000100
[ Mon Jul 15 20:16:46 2024 ] 
Training: Epoch [69/150], Step [2499], Loss: 0.02307620830833912, Training Accuracy: 94.63000000000001
[ Mon Jul 15 20:16:46 2024 ] 	Batch(2500/6809) done. Loss: 0.4294  lr:0.000100
[ Mon Jul 15 20:17:09 2024 ] 	Batch(2600/6809) done. Loss: 0.4586  lr:0.000100
[ Mon Jul 15 20:17:32 2024 ] 	Batch(2700/6809) done. Loss: 0.3479  lr:0.000100
[ Mon Jul 15 20:17:54 2024 ] 	Batch(2800/6809) done. Loss: 0.1432  lr:0.000100
[ Mon Jul 15 20:18:17 2024 ] 	Batch(2900/6809) done. Loss: 0.0565  lr:0.000100
[ Mon Jul 15 20:18:39 2024 ] 
Training: Epoch [69/150], Step [2999], Loss: 0.01975788176059723, Training Accuracy: 94.5375
[ Mon Jul 15 20:18:40 2024 ] 	Batch(3000/6809) done. Loss: 0.1481  lr:0.000100
[ Mon Jul 15 20:19:02 2024 ] 	Batch(3100/6809) done. Loss: 0.1954  lr:0.000100
[ Mon Jul 15 20:19:25 2024 ] 	Batch(3200/6809) done. Loss: 0.4703  lr:0.000100
[ Mon Jul 15 20:19:48 2024 ] 	Batch(3300/6809) done. Loss: 0.2175  lr:0.000100
[ Mon Jul 15 20:20:11 2024 ] 	Batch(3400/6809) done. Loss: 0.2060  lr:0.000100
[ Mon Jul 15 20:20:33 2024 ] 
Training: Epoch [69/150], Step [3499], Loss: 0.207074835896492, Training Accuracy: 94.53928571428571
[ Mon Jul 15 20:20:34 2024 ] 	Batch(3500/6809) done. Loss: 0.0553  lr:0.000100
[ Mon Jul 15 20:20:56 2024 ] 	Batch(3600/6809) done. Loss: 0.7184  lr:0.000100
[ Mon Jul 15 20:21:19 2024 ] 	Batch(3700/6809) done. Loss: 0.3887  lr:0.000100
[ Mon Jul 15 20:21:42 2024 ] 	Batch(3800/6809) done. Loss: 0.3098  lr:0.000100
[ Mon Jul 15 20:22:04 2024 ] 	Batch(3900/6809) done. Loss: 0.0254  lr:0.000100
[ Mon Jul 15 20:22:27 2024 ] 
Training: Epoch [69/150], Step [3999], Loss: 0.11610008031129837, Training Accuracy: 94.55
[ Mon Jul 15 20:22:28 2024 ] 	Batch(4000/6809) done. Loss: 0.1690  lr:0.000100
[ Mon Jul 15 20:22:50 2024 ] 	Batch(4100/6809) done. Loss: 0.0784  lr:0.000100
[ Mon Jul 15 20:23:13 2024 ] 	Batch(4200/6809) done. Loss: 0.1178  lr:0.000100
[ Mon Jul 15 20:23:36 2024 ] 	Batch(4300/6809) done. Loss: 0.0556  lr:0.000100
[ Mon Jul 15 20:23:59 2024 ] 	Batch(4400/6809) done. Loss: 0.0639  lr:0.000100
[ Mon Jul 15 20:24:22 2024 ] 
Training: Epoch [69/150], Step [4499], Loss: 0.5961904525756836, Training Accuracy: 94.47777777777779
[ Mon Jul 15 20:24:22 2024 ] 	Batch(4500/6809) done. Loss: 0.0400  lr:0.000100
[ Mon Jul 15 20:24:45 2024 ] 	Batch(4600/6809) done. Loss: 0.4947  lr:0.000100
[ Mon Jul 15 20:25:07 2024 ] 	Batch(4700/6809) done. Loss: 0.0664  lr:0.000100
[ Mon Jul 15 20:25:30 2024 ] 	Batch(4800/6809) done. Loss: 0.1833  lr:0.000100
[ Mon Jul 15 20:25:53 2024 ] 	Batch(4900/6809) done. Loss: 0.0034  lr:0.000100
[ Mon Jul 15 20:26:15 2024 ] 
Training: Epoch [69/150], Step [4999], Loss: 0.020451024174690247, Training Accuracy: 94.47
[ Mon Jul 15 20:26:15 2024 ] 	Batch(5000/6809) done. Loss: 0.1469  lr:0.000100
[ Mon Jul 15 20:26:38 2024 ] 	Batch(5100/6809) done. Loss: 0.3745  lr:0.000100
[ Mon Jul 15 20:27:01 2024 ] 	Batch(5200/6809) done. Loss: 0.0881  lr:0.000100
[ Mon Jul 15 20:27:23 2024 ] 	Batch(5300/6809) done. Loss: 0.0338  lr:0.000100
[ Mon Jul 15 20:27:47 2024 ] 	Batch(5400/6809) done. Loss: 0.0557  lr:0.000100
[ Mon Jul 15 20:28:09 2024 ] 
Training: Epoch [69/150], Step [5499], Loss: 0.10348223149776459, Training Accuracy: 94.46363636363635
[ Mon Jul 15 20:28:09 2024 ] 	Batch(5500/6809) done. Loss: 0.3081  lr:0.000100
[ Mon Jul 15 20:28:32 2024 ] 	Batch(5600/6809) done. Loss: 0.1272  lr:0.000100
[ Mon Jul 15 20:28:55 2024 ] 	Batch(5700/6809) done. Loss: 0.5286  lr:0.000100
[ Mon Jul 15 20:29:18 2024 ] 	Batch(5800/6809) done. Loss: 0.1918  lr:0.000100
[ Mon Jul 15 20:29:40 2024 ] 	Batch(5900/6809) done. Loss: 0.0790  lr:0.000100
[ Mon Jul 15 20:30:03 2024 ] 
Training: Epoch [69/150], Step [5999], Loss: 0.15554215013980865, Training Accuracy: 94.41666666666667
[ Mon Jul 15 20:30:03 2024 ] 	Batch(6000/6809) done. Loss: 0.1096  lr:0.000100
[ Mon Jul 15 20:30:26 2024 ] 	Batch(6100/6809) done. Loss: 0.5222  lr:0.000100
[ Mon Jul 15 20:30:48 2024 ] 	Batch(6200/6809) done. Loss: 0.8783  lr:0.000100
[ Mon Jul 15 20:31:11 2024 ] 	Batch(6300/6809) done. Loss: 0.6980  lr:0.000100
[ Mon Jul 15 20:31:34 2024 ] 	Batch(6400/6809) done. Loss: 0.1655  lr:0.000100
[ Mon Jul 15 20:31:57 2024 ] 
Training: Epoch [69/150], Step [6499], Loss: 0.29401081800460815, Training Accuracy: 94.44615384615385
[ Mon Jul 15 20:31:57 2024 ] 	Batch(6500/6809) done. Loss: 0.0289  lr:0.000100
[ Mon Jul 15 20:32:19 2024 ] 	Batch(6600/6809) done. Loss: 0.1302  lr:0.000100
[ Mon Jul 15 20:32:42 2024 ] 	Batch(6700/6809) done. Loss: 0.5539  lr:0.000100
[ Mon Jul 15 20:33:05 2024 ] 	Batch(6800/6809) done. Loss: 0.0158  lr:0.000100
[ Mon Jul 15 20:33:07 2024 ] 	Mean training loss: 0.1912.
[ Mon Jul 15 20:33:07 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 20:33:07 2024 ] Eval epoch: 70
[ Mon Jul 15 20:40:03 2024 ] 	Mean val loss of 7435 batches: 0.8942806376870659.
[ Mon Jul 15 20:40:03 2024 ] 
Validation: Epoch [69/150], Samples [47195.0/59477], Loss: 2.794826030731201, Validation Accuracy: 79.3500008406611
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 1 : 396 / 500 = 79 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 2 : 417 / 499 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 3 : 409 / 500 = 81 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 4 : 429 / 502 = 85 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 5 : 416 / 502 = 82 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 6 : 417 / 502 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 7 : 464 / 497 = 93 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 8 : 485 / 498 = 97 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 9 : 396 / 500 = 79 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 10 : 318 / 500 = 63 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 11 : 208 / 498 = 41 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 12 : 402 / 499 = 80 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 13 : 482 / 502 = 96 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 14 : 485 / 504 = 96 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 15 : 358 / 502 = 71 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 16 : 377 / 502 = 75 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 17 : 421 / 504 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 18 : 418 / 504 = 82 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 19 : 422 / 502 = 84 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 20 : 442 / 502 = 88 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 21 : 465 / 503 = 92 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 22 : 433 / 504 = 85 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 23 : 422 / 503 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 24 : 434 / 504 = 86 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 25 : 472 / 504 = 93 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 26 : 466 / 504 = 92 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 27 : 434 / 501 = 86 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 28 : 366 / 502 = 72 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 29 : 315 / 502 = 62 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 30 : 383 / 501 = 76 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 31 : 417 / 504 = 82 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 32 : 438 / 503 = 87 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 33 : 408 / 503 = 81 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 34 : 490 / 504 = 97 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 35 : 465 / 503 = 92 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 36 : 386 / 502 = 76 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 37 : 449 / 504 = 89 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 38 : 440 / 504 = 87 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 39 : 460 / 498 = 92 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 40 : 392 / 504 = 77 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 41 : 471 / 503 = 93 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 42 : 450 / 504 = 89 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 43 : 338 / 503 = 67 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 44 : 425 / 504 = 84 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 45 : 418 / 504 = 82 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 46 : 396 / 504 = 78 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 47 : 355 / 503 = 70 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 48 : 417 / 503 = 82 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 49 : 404 / 499 = 80 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 50 : 396 / 502 = 78 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 51 : 459 / 503 = 91 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 52 : 456 / 504 = 90 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 53 : 454 / 497 = 91 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 54 : 446 / 480 = 92 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 55 : 431 / 504 = 85 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 56 : 421 / 503 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 57 : 473 / 504 = 93 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 58 : 481 / 499 = 96 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 59 : 483 / 503 = 96 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 60 : 408 / 479 = 85 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 61 : 402 / 484 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 62 : 390 / 487 = 80 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 63 : 440 / 489 = 89 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 64 : 411 / 488 = 84 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 65 : 433 / 490 = 88 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 66 : 304 / 488 = 62 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 67 : 341 / 490 = 69 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 68 : 305 / 490 = 62 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 69 : 393 / 490 = 80 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 70 : 162 / 490 = 33 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 71 : 291 / 490 = 59 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 72 : 198 / 488 = 40 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 73 : 255 / 486 = 52 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 74 : 255 / 481 = 53 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 75 : 282 / 488 = 57 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 76 : 324 / 489 = 66 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 77 : 315 / 488 = 64 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 78 : 373 / 488 = 76 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 79 : 462 / 490 = 94 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 80 : 394 / 489 = 80 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 81 : 273 / 491 = 55 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 82 : 314 / 491 = 63 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 83 : 248 / 489 = 50 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 84 : 357 / 489 = 73 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 85 : 334 / 489 = 68 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 86 : 408 / 491 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 87 : 411 / 492 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 88 : 349 / 491 = 71 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 89 : 376 / 492 = 76 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 90 : 274 / 490 = 55 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 91 : 362 / 482 = 75 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 92 : 350 / 490 = 71 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 93 : 339 / 487 = 69 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 94 : 432 / 489 = 88 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 95 : 408 / 490 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 96 : 458 / 491 = 93 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 97 : 457 / 490 = 93 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 98 : 438 / 491 = 89 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 99 : 446 / 491 = 90 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 100 : 445 / 491 = 90 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 101 : 417 / 491 = 84 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 102 : 274 / 492 = 55 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 103 : 377 / 492 = 76 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 104 : 296 / 491 = 60 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 105 : 254 / 491 = 51 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 106 : 325 / 492 = 66 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 107 : 413 / 491 = 84 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 108 : 373 / 492 = 75 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 109 : 352 / 490 = 71 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 110 : 407 / 491 = 82 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 111 : 442 / 492 = 89 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 112 : 462 / 492 = 93 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 113 : 435 / 491 = 88 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 114 : 379 / 491 = 77 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 115 : 413 / 492 = 83 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 116 : 419 / 491 = 85 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 117 : 400 / 492 = 81 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 118 : 415 / 490 = 84 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 119 : 448 / 492 = 91 %
[ Mon Jul 15 20:40:03 2024 ] Accuracy of 120 : 406 / 500 = 81 %
[ Mon Jul 15 20:40:03 2024 ] Training epoch: 71
[ Mon Jul 15 20:40:04 2024 ] 	Batch(0/6809) done. Loss: 0.0991  lr:0.000100
[ Mon Jul 15 20:40:26 2024 ] 	Batch(100/6809) done. Loss: 0.0055  lr:0.000100
[ Mon Jul 15 20:40:49 2024 ] 	Batch(200/6809) done. Loss: 0.1102  lr:0.000100
[ Mon Jul 15 20:41:11 2024 ] 	Batch(300/6809) done. Loss: 0.3481  lr:0.000100
[ Mon Jul 15 20:41:34 2024 ] 	Batch(400/6809) done. Loss: 0.0110  lr:0.000100
[ Mon Jul 15 20:41:57 2024 ] 
Training: Epoch [70/150], Step [499], Loss: 0.05397640913724899, Training Accuracy: 93.8
[ Mon Jul 15 20:41:57 2024 ] 	Batch(500/6809) done. Loss: 0.0103  lr:0.000100
[ Mon Jul 15 20:42:20 2024 ] 	Batch(600/6809) done. Loss: 0.0697  lr:0.000100
[ Mon Jul 15 20:42:43 2024 ] 	Batch(700/6809) done. Loss: 0.0410  lr:0.000100
[ Mon Jul 15 20:43:06 2024 ] 	Batch(800/6809) done. Loss: 0.0985  lr:0.000100
[ Mon Jul 15 20:43:29 2024 ] 	Batch(900/6809) done. Loss: 0.0463  lr:0.000100
[ Mon Jul 15 20:43:52 2024 ] 
Training: Epoch [70/150], Step [999], Loss: 0.2978852093219757, Training Accuracy: 93.7875
[ Mon Jul 15 20:43:52 2024 ] 	Batch(1000/6809) done. Loss: 0.0700  lr:0.000100
[ Mon Jul 15 20:44:15 2024 ] 	Batch(1100/6809) done. Loss: 0.2066  lr:0.000100
[ Mon Jul 15 20:44:37 2024 ] 	Batch(1200/6809) done. Loss: 0.3898  lr:0.000100
[ Mon Jul 15 20:45:00 2024 ] 	Batch(1300/6809) done. Loss: 0.2909  lr:0.000100
[ Mon Jul 15 20:45:23 2024 ] 	Batch(1400/6809) done. Loss: 0.4178  lr:0.000100
[ Mon Jul 15 20:45:46 2024 ] 
Training: Epoch [70/150], Step [1499], Loss: 0.04294677823781967, Training Accuracy: 93.86666666666666
[ Mon Jul 15 20:45:46 2024 ] 	Batch(1500/6809) done. Loss: 0.0282  lr:0.000100
[ Mon Jul 15 20:46:09 2024 ] 	Batch(1600/6809) done. Loss: 0.0105  lr:0.000100
[ Mon Jul 15 20:46:31 2024 ] 	Batch(1700/6809) done. Loss: 0.2780  lr:0.000100
[ Mon Jul 15 20:46:54 2024 ] 	Batch(1800/6809) done. Loss: 0.7366  lr:0.000100
[ Mon Jul 15 20:47:17 2024 ] 	Batch(1900/6809) done. Loss: 0.0879  lr:0.000100
[ Mon Jul 15 20:47:39 2024 ] 
Training: Epoch [70/150], Step [1999], Loss: 0.43092790246009827, Training Accuracy: 94.06875
[ Mon Jul 15 20:47:39 2024 ] 	Batch(2000/6809) done. Loss: 0.2362  lr:0.000100
[ Mon Jul 15 20:48:02 2024 ] 	Batch(2100/6809) done. Loss: 0.0727  lr:0.000100
[ Mon Jul 15 20:48:25 2024 ] 	Batch(2200/6809) done. Loss: 0.3290  lr:0.000100
[ Mon Jul 15 20:48:47 2024 ] 	Batch(2300/6809) done. Loss: 0.3609  lr:0.000100
[ Mon Jul 15 20:49:10 2024 ] 	Batch(2400/6809) done. Loss: 0.2078  lr:0.000100
[ Mon Jul 15 20:49:33 2024 ] 
Training: Epoch [70/150], Step [2499], Loss: 0.21572691202163696, Training Accuracy: 94.25
[ Mon Jul 15 20:49:33 2024 ] 	Batch(2500/6809) done. Loss: 0.4703  lr:0.000100
[ Mon Jul 15 20:49:56 2024 ] 	Batch(2600/6809) done. Loss: 0.1434  lr:0.000100
[ Mon Jul 15 20:50:19 2024 ] 	Batch(2700/6809) done. Loss: 0.0975  lr:0.000100
[ Mon Jul 15 20:50:41 2024 ] 	Batch(2800/6809) done. Loss: 0.0112  lr:0.000100
[ Mon Jul 15 20:51:04 2024 ] 	Batch(2900/6809) done. Loss: 0.0105  lr:0.000100
[ Mon Jul 15 20:51:27 2024 ] 
Training: Epoch [70/150], Step [2999], Loss: 0.20886719226837158, Training Accuracy: 94.32083333333333
[ Mon Jul 15 20:51:27 2024 ] 	Batch(3000/6809) done. Loss: 0.0159  lr:0.000100
[ Mon Jul 15 20:51:50 2024 ] 	Batch(3100/6809) done. Loss: 0.5397  lr:0.000100
[ Mon Jul 15 20:52:13 2024 ] 	Batch(3200/6809) done. Loss: 0.3220  lr:0.000100
[ Mon Jul 15 20:52:36 2024 ] 	Batch(3300/6809) done. Loss: 0.0993  lr:0.000100
[ Mon Jul 15 20:52:59 2024 ] 	Batch(3400/6809) done. Loss: 0.5618  lr:0.000100
[ Mon Jul 15 20:53:22 2024 ] 
Training: Epoch [70/150], Step [3499], Loss: 0.33101344108581543, Training Accuracy: 94.31071428571428
[ Mon Jul 15 20:53:22 2024 ] 	Batch(3500/6809) done. Loss: 0.3282  lr:0.000100
[ Mon Jul 15 20:53:45 2024 ] 	Batch(3600/6809) done. Loss: 0.3180  lr:0.000100
[ Mon Jul 15 20:54:08 2024 ] 	Batch(3700/6809) done. Loss: 0.2690  lr:0.000100
[ Mon Jul 15 20:54:31 2024 ] 	Batch(3800/6809) done. Loss: 0.0144  lr:0.000100
[ Mon Jul 15 20:54:54 2024 ] 	Batch(3900/6809) done. Loss: 0.0711  lr:0.000100
[ Mon Jul 15 20:55:17 2024 ] 
Training: Epoch [70/150], Step [3999], Loss: 0.0350668765604496, Training Accuracy: 94.278125
[ Mon Jul 15 20:55:17 2024 ] 	Batch(4000/6809) done. Loss: 0.1019  lr:0.000100
[ Mon Jul 15 20:55:40 2024 ] 	Batch(4100/6809) done. Loss: 0.1196  lr:0.000100
[ Mon Jul 15 20:56:03 2024 ] 	Batch(4200/6809) done. Loss: 0.0780  lr:0.000100
[ Mon Jul 15 20:56:26 2024 ] 	Batch(4300/6809) done. Loss: 0.0259  lr:0.000100
[ Mon Jul 15 20:56:49 2024 ] 	Batch(4400/6809) done. Loss: 0.0582  lr:0.000100
[ Mon Jul 15 20:57:12 2024 ] 
Training: Epoch [70/150], Step [4499], Loss: 0.35289129614830017, Training Accuracy: 94.38055555555556
[ Mon Jul 15 20:57:12 2024 ] 	Batch(4500/6809) done. Loss: 0.0714  lr:0.000100
[ Mon Jul 15 20:57:35 2024 ] 	Batch(4600/6809) done. Loss: 0.1901  lr:0.000100
[ Mon Jul 15 20:57:58 2024 ] 	Batch(4700/6809) done. Loss: 0.0935  lr:0.000100
[ Mon Jul 15 20:58:21 2024 ] 	Batch(4800/6809) done. Loss: 0.5369  lr:0.000100
[ Mon Jul 15 20:58:44 2024 ] 	Batch(4900/6809) done. Loss: 0.0378  lr:0.000100
[ Mon Jul 15 20:59:07 2024 ] 
Training: Epoch [70/150], Step [4999], Loss: 0.18379586935043335, Training Accuracy: 94.39750000000001
[ Mon Jul 15 20:59:07 2024 ] 	Batch(5000/6809) done. Loss: 0.2058  lr:0.000100
[ Mon Jul 15 20:59:30 2024 ] 	Batch(5100/6809) done. Loss: 0.3542  lr:0.000100
[ Mon Jul 15 20:59:54 2024 ] 	Batch(5200/6809) done. Loss: 0.3138  lr:0.000100
[ Mon Jul 15 21:00:16 2024 ] 	Batch(5300/6809) done. Loss: 1.0585  lr:0.000100
[ Mon Jul 15 21:00:39 2024 ] 	Batch(5400/6809) done. Loss: 0.1192  lr:0.000100
[ Mon Jul 15 21:01:02 2024 ] 
Training: Epoch [70/150], Step [5499], Loss: 0.09424835443496704, Training Accuracy: 94.46136363636364
[ Mon Jul 15 21:01:02 2024 ] 	Batch(5500/6809) done. Loss: 0.4171  lr:0.000100
[ Mon Jul 15 21:01:25 2024 ] 	Batch(5600/6809) done. Loss: 0.0533  lr:0.000100
[ Mon Jul 15 21:01:48 2024 ] 	Batch(5700/6809) done. Loss: 0.2758  lr:0.000100
[ Mon Jul 15 21:02:11 2024 ] 	Batch(5800/6809) done. Loss: 0.6537  lr:0.000100
[ Mon Jul 15 21:02:34 2024 ] 	Batch(5900/6809) done. Loss: 0.0087  lr:0.000100
[ Mon Jul 15 21:02:58 2024 ] 
Training: Epoch [70/150], Step [5999], Loss: 0.03386547416448593, Training Accuracy: 94.475
[ Mon Jul 15 21:02:58 2024 ] 	Batch(6000/6809) done. Loss: 0.1206  lr:0.000100
[ Mon Jul 15 21:03:21 2024 ] 	Batch(6100/6809) done. Loss: 0.1127  lr:0.000100
[ Mon Jul 15 21:03:44 2024 ] 	Batch(6200/6809) done. Loss: 0.0435  lr:0.000100
[ Mon Jul 15 21:04:07 2024 ] 	Batch(6300/6809) done. Loss: 0.0597  lr:0.000100
[ Mon Jul 15 21:04:30 2024 ] 	Batch(6400/6809) done. Loss: 0.3007  lr:0.000100
[ Mon Jul 15 21:04:53 2024 ] 
Training: Epoch [70/150], Step [6499], Loss: 0.4802923798561096, Training Accuracy: 94.52115384615385
[ Mon Jul 15 21:04:53 2024 ] 	Batch(6500/6809) done. Loss: 0.3172  lr:0.000100
[ Mon Jul 15 21:05:16 2024 ] 	Batch(6600/6809) done. Loss: 0.0535  lr:0.000100
[ Mon Jul 15 21:05:38 2024 ] 	Batch(6700/6809) done. Loss: 0.1486  lr:0.000100
[ Mon Jul 15 21:06:01 2024 ] 	Batch(6800/6809) done. Loss: 0.0548  lr:0.000100
[ Mon Jul 15 21:06:03 2024 ] 	Mean training loss: 0.1907.
[ Mon Jul 15 21:06:03 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 21:06:03 2024 ] Training epoch: 72
[ Mon Jul 15 21:06:04 2024 ] 	Batch(0/6809) done. Loss: 0.0938  lr:0.000100
[ Mon Jul 15 21:06:26 2024 ] 	Batch(100/6809) done. Loss: 0.1495  lr:0.000100
[ Mon Jul 15 21:06:49 2024 ] 	Batch(200/6809) done. Loss: 0.0397  lr:0.000100
[ Mon Jul 15 21:07:12 2024 ] 	Batch(300/6809) done. Loss: 0.2279  lr:0.000100
[ Mon Jul 15 21:07:35 2024 ] 	Batch(400/6809) done. Loss: 0.2470  lr:0.000100
[ Mon Jul 15 21:07:57 2024 ] 
Training: Epoch [71/150], Step [499], Loss: 0.0321299210190773, Training Accuracy: 95.3
[ Mon Jul 15 21:07:57 2024 ] 	Batch(500/6809) done. Loss: 0.5499  lr:0.000100
[ Mon Jul 15 21:08:20 2024 ] 	Batch(600/6809) done. Loss: 0.8001  lr:0.000100
[ Mon Jul 15 21:08:43 2024 ] 	Batch(700/6809) done. Loss: 0.3804  lr:0.000100
[ Mon Jul 15 21:09:05 2024 ] 	Batch(800/6809) done. Loss: 0.0855  lr:0.000100
[ Mon Jul 15 21:09:28 2024 ] 	Batch(900/6809) done. Loss: 0.0253  lr:0.000100
[ Mon Jul 15 21:09:50 2024 ] 
Training: Epoch [71/150], Step [999], Loss: 0.6890456080436707, Training Accuracy: 95.0
[ Mon Jul 15 21:09:51 2024 ] 	Batch(1000/6809) done. Loss: 0.0469  lr:0.000100
[ Mon Jul 15 21:10:13 2024 ] 	Batch(1100/6809) done. Loss: 0.3390  lr:0.000100
[ Mon Jul 15 21:10:36 2024 ] 	Batch(1200/6809) done. Loss: 0.0076  lr:0.000100
[ Mon Jul 15 21:10:59 2024 ] 	Batch(1300/6809) done. Loss: 0.2198  lr:0.000100
[ Mon Jul 15 21:11:21 2024 ] 	Batch(1400/6809) done. Loss: 0.1611  lr:0.000100
[ Mon Jul 15 21:11:44 2024 ] 
Training: Epoch [71/150], Step [1499], Loss: 0.01000660378485918, Training Accuracy: 94.81666666666668
[ Mon Jul 15 21:11:44 2024 ] 	Batch(1500/6809) done. Loss: 0.2021  lr:0.000100
[ Mon Jul 15 21:12:07 2024 ] 	Batch(1600/6809) done. Loss: 0.0233  lr:0.000100
[ Mon Jul 15 21:12:29 2024 ] 	Batch(1700/6809) done. Loss: 0.0211  lr:0.000100
[ Mon Jul 15 21:12:52 2024 ] 	Batch(1800/6809) done. Loss: 0.0167  lr:0.000100
[ Mon Jul 15 21:13:15 2024 ] 	Batch(1900/6809) done. Loss: 0.0350  lr:0.000100
[ Mon Jul 15 21:13:37 2024 ] 
Training: Epoch [71/150], Step [1999], Loss: 0.11503821611404419, Training Accuracy: 94.64375000000001
[ Mon Jul 15 21:13:37 2024 ] 	Batch(2000/6809) done. Loss: 0.0220  lr:0.000100
[ Mon Jul 15 21:14:00 2024 ] 	Batch(2100/6809) done. Loss: 0.1800  lr:0.000100
[ Mon Jul 15 21:14:23 2024 ] 	Batch(2200/6809) done. Loss: 0.0405  lr:0.000100
[ Mon Jul 15 21:14:45 2024 ] 	Batch(2300/6809) done. Loss: 0.2500  lr:0.000100
[ Mon Jul 15 21:15:08 2024 ] 	Batch(2400/6809) done. Loss: 0.0130  lr:0.000100
[ Mon Jul 15 21:15:30 2024 ] 
Training: Epoch [71/150], Step [2499], Loss: 0.28448158502578735, Training Accuracy: 94.73
[ Mon Jul 15 21:15:31 2024 ] 	Batch(2500/6809) done. Loss: 0.0788  lr:0.000100
[ Mon Jul 15 21:15:53 2024 ] 	Batch(2600/6809) done. Loss: 0.1078  lr:0.000100
[ Mon Jul 15 21:16:16 2024 ] 	Batch(2700/6809) done. Loss: 0.0958  lr:0.000100
[ Mon Jul 15 21:16:39 2024 ] 	Batch(2800/6809) done. Loss: 0.0580  lr:0.000100
[ Mon Jul 15 21:17:02 2024 ] 	Batch(2900/6809) done. Loss: 0.0629  lr:0.000100
[ Mon Jul 15 21:17:24 2024 ] 
Training: Epoch [71/150], Step [2999], Loss: 0.08485100418329239, Training Accuracy: 94.73333333333333
[ Mon Jul 15 21:17:25 2024 ] 	Batch(3000/6809) done. Loss: 0.0437  lr:0.000100
[ Mon Jul 15 21:17:47 2024 ] 	Batch(3100/6809) done. Loss: 0.2842  lr:0.000100
[ Mon Jul 15 21:18:10 2024 ] 	Batch(3200/6809) done. Loss: 0.1253  lr:0.000100
[ Mon Jul 15 21:18:33 2024 ] 	Batch(3300/6809) done. Loss: 0.2438  lr:0.000100
[ Mon Jul 15 21:18:57 2024 ] 	Batch(3400/6809) done. Loss: 0.2449  lr:0.000100
[ Mon Jul 15 21:19:19 2024 ] 
Training: Epoch [71/150], Step [3499], Loss: 0.06822635978460312, Training Accuracy: 94.78571428571428
[ Mon Jul 15 21:19:20 2024 ] 	Batch(3500/6809) done. Loss: 0.0122  lr:0.000100
[ Mon Jul 15 21:19:43 2024 ] 	Batch(3600/6809) done. Loss: 0.0684  lr:0.000100
[ Mon Jul 15 21:20:06 2024 ] 	Batch(3700/6809) done. Loss: 0.1323  lr:0.000100
[ Mon Jul 15 21:20:29 2024 ] 	Batch(3800/6809) done. Loss: 0.1170  lr:0.000100
[ Mon Jul 15 21:20:52 2024 ] 	Batch(3900/6809) done. Loss: 0.0457  lr:0.000100
[ Mon Jul 15 21:21:15 2024 ] 
Training: Epoch [71/150], Step [3999], Loss: 0.15361353754997253, Training Accuracy: 94.821875
[ Mon Jul 15 21:21:15 2024 ] 	Batch(4000/6809) done. Loss: 0.0839  lr:0.000100
[ Mon Jul 15 21:21:38 2024 ] 	Batch(4100/6809) done. Loss: 0.2960  lr:0.000100
[ Mon Jul 15 21:22:01 2024 ] 	Batch(4200/6809) done. Loss: 0.0422  lr:0.000100
[ Mon Jul 15 21:22:24 2024 ] 	Batch(4300/6809) done. Loss: 0.2681  lr:0.000100
[ Mon Jul 15 21:22:46 2024 ] 	Batch(4400/6809) done. Loss: 0.0396  lr:0.000100
[ Mon Jul 15 21:23:09 2024 ] 
Training: Epoch [71/150], Step [4499], Loss: 0.18268650770187378, Training Accuracy: 94.83888888888889
[ Mon Jul 15 21:23:09 2024 ] 	Batch(4500/6809) done. Loss: 0.0213  lr:0.000100
[ Mon Jul 15 21:23:32 2024 ] 	Batch(4600/6809) done. Loss: 0.4219  lr:0.000100
[ Mon Jul 15 21:23:54 2024 ] 	Batch(4700/6809) done. Loss: 0.0765  lr:0.000100
[ Mon Jul 15 21:24:18 2024 ] 	Batch(4800/6809) done. Loss: 0.0693  lr:0.000100
[ Mon Jul 15 21:24:41 2024 ] 	Batch(4900/6809) done. Loss: 0.0437  lr:0.000100
[ Mon Jul 15 21:25:04 2024 ] 
Training: Epoch [71/150], Step [4999], Loss: 0.06183313578367233, Training Accuracy: 94.845
[ Mon Jul 15 21:25:04 2024 ] 	Batch(5000/6809) done. Loss: 0.3538  lr:0.000100
[ Mon Jul 15 21:25:27 2024 ] 	Batch(5100/6809) done. Loss: 0.0267  lr:0.000100
[ Mon Jul 15 21:25:50 2024 ] 	Batch(5200/6809) done. Loss: 0.0597  lr:0.000100
[ Mon Jul 15 21:26:13 2024 ] 	Batch(5300/6809) done. Loss: 0.1007  lr:0.000100
[ Mon Jul 15 21:26:36 2024 ] 	Batch(5400/6809) done. Loss: 0.0692  lr:0.000100
[ Mon Jul 15 21:26:59 2024 ] 
Training: Epoch [71/150], Step [5499], Loss: 0.005857182666659355, Training Accuracy: 94.825
[ Mon Jul 15 21:26:59 2024 ] 	Batch(5500/6809) done. Loss: 0.1198  lr:0.000100
[ Mon Jul 15 21:27:22 2024 ] 	Batch(5600/6809) done. Loss: 0.0920  lr:0.000100
[ Mon Jul 15 21:27:46 2024 ] 	Batch(5700/6809) done. Loss: 0.2452  lr:0.000100
[ Mon Jul 15 21:28:09 2024 ] 	Batch(5800/6809) done. Loss: 0.2437  lr:0.000100
[ Mon Jul 15 21:28:32 2024 ] 	Batch(5900/6809) done. Loss: 0.0298  lr:0.000100
[ Mon Jul 15 21:28:54 2024 ] 
Training: Epoch [71/150], Step [5999], Loss: 0.006374424789100885, Training Accuracy: 94.79791666666667
[ Mon Jul 15 21:28:55 2024 ] 	Batch(6000/6809) done. Loss: 0.0177  lr:0.000100
[ Mon Jul 15 21:29:18 2024 ] 	Batch(6100/6809) done. Loss: 0.4698  lr:0.000100
[ Mon Jul 15 21:29:41 2024 ] 	Batch(6200/6809) done. Loss: 0.3042  lr:0.000100
[ Mon Jul 15 21:30:04 2024 ] 	Batch(6300/6809) done. Loss: 0.5313  lr:0.000100
[ Mon Jul 15 21:30:27 2024 ] 	Batch(6400/6809) done. Loss: 0.4271  lr:0.000100
[ Mon Jul 15 21:30:50 2024 ] 
Training: Epoch [71/150], Step [6499], Loss: 0.5247628688812256, Training Accuracy: 94.7673076923077
[ Mon Jul 15 21:30:50 2024 ] 	Batch(6500/6809) done. Loss: 0.1192  lr:0.000100
[ Mon Jul 15 21:31:14 2024 ] 	Batch(6600/6809) done. Loss: 0.4846  lr:0.000100
[ Mon Jul 15 21:31:37 2024 ] 	Batch(6700/6809) done. Loss: 0.1385  lr:0.000100
[ Mon Jul 15 21:32:00 2024 ] 	Batch(6800/6809) done. Loss: 0.3975  lr:0.000100
[ Mon Jul 15 21:32:02 2024 ] 	Mean training loss: 0.1861.
[ Mon Jul 15 21:32:02 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Mon Jul 15 21:32:02 2024 ] Training epoch: 73
[ Mon Jul 15 21:32:02 2024 ] 	Batch(0/6809) done. Loss: 0.0910  lr:0.000100
[ Mon Jul 15 21:32:25 2024 ] 	Batch(100/6809) done. Loss: 0.4805  lr:0.000100
[ Mon Jul 15 21:32:47 2024 ] 	Batch(200/6809) done. Loss: 0.4448  lr:0.000100
[ Mon Jul 15 21:33:10 2024 ] 	Batch(300/6809) done. Loss: 0.4959  lr:0.000100
[ Mon Jul 15 21:33:33 2024 ] 	Batch(400/6809) done. Loss: 0.2815  lr:0.000100
[ Mon Jul 15 21:33:55 2024 ] 
Training: Epoch [72/150], Step [499], Loss: 0.4690202474594116, Training Accuracy: 94.35
[ Mon Jul 15 21:33:56 2024 ] 	Batch(500/6809) done. Loss: 0.0091  lr:0.000100
[ Mon Jul 15 21:34:19 2024 ] 	Batch(600/6809) done. Loss: 0.1824  lr:0.000100
[ Mon Jul 15 21:34:42 2024 ] 	Batch(700/6809) done. Loss: 0.0852  lr:0.000100
[ Mon Jul 15 21:35:06 2024 ] 	Batch(800/6809) done. Loss: 0.0066  lr:0.000100
[ Mon Jul 15 21:35:29 2024 ] 	Batch(900/6809) done. Loss: 0.0344  lr:0.000100
[ Mon Jul 15 21:35:52 2024 ] 
Training: Epoch [72/150], Step [999], Loss: 0.178056538105011, Training Accuracy: 94.8
[ Mon Jul 15 21:35:52 2024 ] 	Batch(1000/6809) done. Loss: 0.0848  lr:0.000100
[ Mon Jul 15 21:36:15 2024 ] 	Batch(1100/6809) done. Loss: 0.6608  lr:0.000100
[ Mon Jul 15 21:36:38 2024 ] 	Batch(1200/6809) done. Loss: 0.0243  lr:0.000100
[ Mon Jul 15 21:37:01 2024 ] 	Batch(1300/6809) done. Loss: 0.1472  lr:0.000100
[ Mon Jul 15 21:37:24 2024 ] 	Batch(1400/6809) done. Loss: 0.2800  lr:0.000100
[ Mon Jul 15 21:37:46 2024 ] 
Training: Epoch [72/150], Step [1499], Loss: 0.42089131474494934, Training Accuracy: 94.8
[ Mon Jul 15 21:37:46 2024 ] 	Batch(1500/6809) done. Loss: 0.1200  lr:0.000100
[ Mon Jul 15 21:38:09 2024 ] 	Batch(1600/6809) done. Loss: 0.3132  lr:0.000100
[ Mon Jul 15 21:38:32 2024 ] 	Batch(1700/6809) done. Loss: 0.0580  lr:0.000100
[ Mon Jul 15 21:38:55 2024 ] 	Batch(1800/6809) done. Loss: 0.0994  lr:0.000100
[ Mon Jul 15 21:39:17 2024 ] 	Batch(1900/6809) done. Loss: 0.1230  lr:0.000100
[ Mon Jul 15 21:39:40 2024 ] 
Training: Epoch [72/150], Step [1999], Loss: 0.06975514441728592, Training Accuracy: 94.81875
[ Mon Jul 15 21:39:40 2024 ] 	Batch(2000/6809) done. Loss: 0.0181  lr:0.000100
[ Mon Jul 15 21:40:03 2024 ] 	Batch(2100/6809) done. Loss: 0.3980  lr:0.000100
[ Mon Jul 15 21:40:25 2024 ] 	Batch(2200/6809) done. Loss: 0.1049  lr:0.000100
[ Mon Jul 15 21:40:48 2024 ] 	Batch(2300/6809) done. Loss: 0.2065  lr:0.000100
[ Mon Jul 15 21:41:11 2024 ] 	Batch(2400/6809) done. Loss: 0.1083  lr:0.000100
[ Mon Jul 15 21:41:33 2024 ] 
Training: Epoch [72/150], Step [2499], Loss: 0.06694237142801285, Training Accuracy: 94.75
[ Mon Jul 15 21:41:34 2024 ] 	Batch(2500/6809) done. Loss: 0.0842  lr:0.000100
[ Mon Jul 15 21:41:56 2024 ] 	Batch(2600/6809) done. Loss: 0.3147  lr:0.000100
[ Mon Jul 15 21:42:19 2024 ] 	Batch(2700/6809) done. Loss: 0.0522  lr:0.000100
[ Mon Jul 15 21:42:42 2024 ] 	Batch(2800/6809) done. Loss: 0.0126  lr:0.000100
[ Mon Jul 15 21:43:04 2024 ] 	Batch(2900/6809) done. Loss: 0.2209  lr:0.000100
[ Mon Jul 15 21:43:27 2024 ] 
Training: Epoch [72/150], Step [2999], Loss: 0.10640483349561691, Training Accuracy: 94.81666666666668
[ Mon Jul 15 21:43:27 2024 ] 	Batch(3000/6809) done. Loss: 0.0643  lr:0.000100
[ Mon Jul 15 21:43:50 2024 ] 	Batch(3100/6809) done. Loss: 0.0974  lr:0.000100
[ Mon Jul 15 21:44:13 2024 ] 	Batch(3200/6809) done. Loss: 0.0189  lr:0.000100
[ Mon Jul 15 21:44:36 2024 ] 	Batch(3300/6809) done. Loss: 0.0387  lr:0.000100
[ Mon Jul 15 21:44:58 2024 ] 	Batch(3400/6809) done. Loss: 0.0367  lr:0.000100
[ Mon Jul 15 21:45:21 2024 ] 
Training: Epoch [72/150], Step [3499], Loss: 0.22596444189548492, Training Accuracy: 94.80714285714285
[ Mon Jul 15 21:45:21 2024 ] 	Batch(3500/6809) done. Loss: 0.2865  lr:0.000100
[ Mon Jul 15 21:45:44 2024 ] 	Batch(3600/6809) done. Loss: 0.0714  lr:0.000100
[ Mon Jul 15 21:46:06 2024 ] 	Batch(3700/6809) done. Loss: 0.4665  lr:0.000100
[ Mon Jul 15 21:46:29 2024 ] 	Batch(3800/6809) done. Loss: 0.2286  lr:0.000100
[ Mon Jul 15 21:46:52 2024 ] 	Batch(3900/6809) done. Loss: 0.3012  lr:0.000100
[ Mon Jul 15 21:47:14 2024 ] 
Training: Epoch [72/150], Step [3999], Loss: 0.1463603973388672, Training Accuracy: 94.83125
[ Mon Jul 15 21:47:15 2024 ] 	Batch(4000/6809) done. Loss: 0.0809  lr:0.000100
[ Mon Jul 15 21:47:38 2024 ] 	Batch(4100/6809) done. Loss: 0.1351  lr:0.000100
[ Mon Jul 15 21:48:01 2024 ] 	Batch(4200/6809) done. Loss: 0.3925  lr:0.000100
[ Mon Jul 15 21:48:24 2024 ] 	Batch(4300/6809) done. Loss: 0.0800  lr:0.000100
[ Mon Jul 15 21:48:47 2024 ] 	Batch(4400/6809) done. Loss: 0.0864  lr:0.000100
[ Mon Jul 15 21:49:10 2024 ] 
Training: Epoch [72/150], Step [4499], Loss: 0.17660704255104065, Training Accuracy: 94.86666666666666
[ Mon Jul 15 21:49:10 2024 ] 	Batch(4500/6809) done. Loss: 0.2653  lr:0.000100
[ Mon Jul 15 21:49:33 2024 ] 	Batch(4600/6809) done. Loss: 0.1050  lr:0.000100
[ Mon Jul 15 21:49:55 2024 ] 	Batch(4700/6809) done. Loss: 0.0806  lr:0.000100
[ Mon Jul 15 21:50:19 2024 ] 	Batch(4800/6809) done. Loss: 0.0672  lr:0.000100
[ Mon Jul 15 21:50:41 2024 ] 	Batch(4900/6809) done. Loss: 0.0424  lr:0.000100
[ Mon Jul 15 21:51:04 2024 ] 
Training: Epoch [72/150], Step [4999], Loss: 0.04632837325334549, Training Accuracy: 94.845
[ Mon Jul 15 21:51:04 2024 ] 	Batch(5000/6809) done. Loss: 0.2172  lr:0.000100
[ Mon Jul 15 21:51:27 2024 ] 	Batch(5100/6809) done. Loss: 0.0158  lr:0.000100
[ Mon Jul 15 21:51:50 2024 ] 	Batch(5200/6809) done. Loss: 0.0423  lr:0.000100
[ Mon Jul 15 21:52:12 2024 ] 	Batch(5300/6809) done. Loss: 0.0493  lr:0.000100
[ Mon Jul 15 21:52:35 2024 ] 	Batch(5400/6809) done. Loss: 0.1250  lr:0.000100
[ Mon Jul 15 21:52:57 2024 ] 
Training: Epoch [72/150], Step [5499], Loss: 0.13923068344593048, Training Accuracy: 94.84318181818182
[ Mon Jul 15 21:52:58 2024 ] 	Batch(5500/6809) done. Loss: 0.4753  lr:0.000100
[ Mon Jul 15 21:53:20 2024 ] 	Batch(5600/6809) done. Loss: 0.2402  lr:0.000100
[ Mon Jul 15 21:53:43 2024 ] 	Batch(5700/6809) done. Loss: 0.0049  lr:0.000100
[ Mon Jul 15 21:54:06 2024 ] 	Batch(5800/6809) done. Loss: 0.0444  lr:0.000100
[ Mon Jul 15 21:54:29 2024 ] 	Batch(5900/6809) done. Loss: 0.0125  lr:0.000100
[ Mon Jul 15 21:54:51 2024 ] 
Training: Epoch [72/150], Step [5999], Loss: 0.1670878529548645, Training Accuracy: 94.80833333333332
[ Mon Jul 15 21:54:52 2024 ] 	Batch(6000/6809) done. Loss: 0.3061  lr:0.000100
[ Mon Jul 15 21:55:14 2024 ] 	Batch(6100/6809) done. Loss: 0.2048  lr:0.000100
[ Mon Jul 15 21:55:37 2024 ] 	Batch(6200/6809) done. Loss: 1.0611  lr:0.000100
[ Mon Jul 15 21:56:00 2024 ] 	Batch(6300/6809) done. Loss: 0.0247  lr:0.000100
[ Mon Jul 15 21:56:23 2024 ] 	Batch(6400/6809) done. Loss: 0.0829  lr:0.000100
[ Mon Jul 15 21:56:46 2024 ] 
Training: Epoch [72/150], Step [6499], Loss: 0.057033393532037735, Training Accuracy: 94.75961538461539
[ Mon Jul 15 21:56:46 2024 ] 	Batch(6500/6809) done. Loss: 0.0181  lr:0.000100
[ Mon Jul 15 21:57:09 2024 ] 	Batch(6600/6809) done. Loss: 0.0198  lr:0.000100
[ Mon Jul 15 21:57:31 2024 ] 	Batch(6700/6809) done. Loss: 0.3035  lr:0.000100
[ Mon Jul 15 21:57:54 2024 ] 	Batch(6800/6809) done. Loss: 0.1257  lr:0.000100
[ Mon Jul 15 21:57:56 2024 ] 	Mean training loss: 0.1785.
[ Mon Jul 15 21:57:56 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 21:57:56 2024 ] Training epoch: 74
[ Mon Jul 15 21:57:57 2024 ] 	Batch(0/6809) done. Loss: 0.0090  lr:0.000100
[ Mon Jul 15 21:58:20 2024 ] 	Batch(100/6809) done. Loss: 0.0271  lr:0.000100
[ Mon Jul 15 21:58:42 2024 ] 	Batch(200/6809) done. Loss: 0.1992  lr:0.000100
[ Mon Jul 15 21:59:05 2024 ] 	Batch(300/6809) done. Loss: 0.0516  lr:0.000100
[ Mon Jul 15 21:59:28 2024 ] 	Batch(400/6809) done. Loss: 0.7448  lr:0.000100
[ Mon Jul 15 21:59:50 2024 ] 
Training: Epoch [73/150], Step [499], Loss: 0.26096197962760925, Training Accuracy: 94.65
[ Mon Jul 15 21:59:50 2024 ] 	Batch(500/6809) done. Loss: 0.4893  lr:0.000100
[ Mon Jul 15 22:00:13 2024 ] 	Batch(600/6809) done. Loss: 0.1252  lr:0.000100
[ Mon Jul 15 22:00:36 2024 ] 	Batch(700/6809) done. Loss: 0.2979  lr:0.000100
[ Mon Jul 15 22:00:59 2024 ] 	Batch(800/6809) done. Loss: 0.0080  lr:0.000100
[ Mon Jul 15 22:01:22 2024 ] 	Batch(900/6809) done. Loss: 0.1292  lr:0.000100
[ Mon Jul 15 22:01:44 2024 ] 
Training: Epoch [73/150], Step [999], Loss: 0.4850417971611023, Training Accuracy: 94.825
[ Mon Jul 15 22:01:45 2024 ] 	Batch(1000/6809) done. Loss: 0.0112  lr:0.000100
[ Mon Jul 15 22:02:08 2024 ] 	Batch(1100/6809) done. Loss: 0.1132  lr:0.000100
[ Mon Jul 15 22:02:31 2024 ] 	Batch(1200/6809) done. Loss: 0.3680  lr:0.000100
[ Mon Jul 15 22:02:54 2024 ] 	Batch(1300/6809) done. Loss: 0.2789  lr:0.000100
[ Mon Jul 15 22:03:16 2024 ] 	Batch(1400/6809) done. Loss: 0.0518  lr:0.000100
[ Mon Jul 15 22:03:39 2024 ] 
Training: Epoch [73/150], Step [1499], Loss: 0.06954371929168701, Training Accuracy: 94.975
[ Mon Jul 15 22:03:39 2024 ] 	Batch(1500/6809) done. Loss: 0.0956  lr:0.000100
[ Mon Jul 15 22:04:01 2024 ] 	Batch(1600/6809) done. Loss: 0.0463  lr:0.000100
[ Mon Jul 15 22:04:24 2024 ] 	Batch(1700/6809) done. Loss: 0.4249  lr:0.000100
[ Mon Jul 15 22:04:47 2024 ] 	Batch(1800/6809) done. Loss: 0.1089  lr:0.000100
[ Mon Jul 15 22:05:10 2024 ] 	Batch(1900/6809) done. Loss: 0.0094  lr:0.000100
[ Mon Jul 15 22:05:33 2024 ] 
Training: Epoch [73/150], Step [1999], Loss: 0.040144309401512146, Training Accuracy: 94.94375000000001
[ Mon Jul 15 22:05:33 2024 ] 	Batch(2000/6809) done. Loss: 0.0297  lr:0.000100
[ Mon Jul 15 22:05:57 2024 ] 	Batch(2100/6809) done. Loss: 0.1459  lr:0.000100
[ Mon Jul 15 22:06:21 2024 ] 	Batch(2200/6809) done. Loss: 0.0544  lr:0.000100
[ Mon Jul 15 22:06:45 2024 ] 	Batch(2300/6809) done. Loss: 0.3644  lr:0.000100
[ Mon Jul 15 22:07:09 2024 ] 	Batch(2400/6809) done. Loss: 0.1001  lr:0.000100
[ Mon Jul 15 22:07:32 2024 ] 
Training: Epoch [73/150], Step [2499], Loss: 0.12872503697872162, Training Accuracy: 94.86
[ Mon Jul 15 22:07:32 2024 ] 	Batch(2500/6809) done. Loss: 0.0124  lr:0.000100
[ Mon Jul 15 22:07:55 2024 ] 	Batch(2600/6809) done. Loss: 0.2513  lr:0.000100
[ Mon Jul 15 22:08:18 2024 ] 	Batch(2700/6809) done. Loss: 0.2682  lr:0.000100
[ Mon Jul 15 22:08:41 2024 ] 	Batch(2800/6809) done. Loss: 0.0199  lr:0.000100
[ Mon Jul 15 22:09:05 2024 ] 	Batch(2900/6809) done. Loss: 0.1857  lr:0.000100
[ Mon Jul 15 22:09:28 2024 ] 
Training: Epoch [73/150], Step [2999], Loss: 0.11339782923460007, Training Accuracy: 94.90833333333333
[ Mon Jul 15 22:09:28 2024 ] 	Batch(3000/6809) done. Loss: 0.0507  lr:0.000100
[ Mon Jul 15 22:09:51 2024 ] 	Batch(3100/6809) done. Loss: 0.0286  lr:0.000100
[ Mon Jul 15 22:10:15 2024 ] 	Batch(3200/6809) done. Loss: 0.0043  lr:0.000100
[ Mon Jul 15 22:10:38 2024 ] 	Batch(3300/6809) done. Loss: 0.3833  lr:0.000100
[ Mon Jul 15 22:11:01 2024 ] 	Batch(3400/6809) done. Loss: 0.0916  lr:0.000100
[ Mon Jul 15 22:11:24 2024 ] 
Training: Epoch [73/150], Step [3499], Loss: 0.01536788884550333, Training Accuracy: 94.89999999999999
[ Mon Jul 15 22:11:24 2024 ] 	Batch(3500/6809) done. Loss: 0.1409  lr:0.000100
[ Mon Jul 15 22:11:47 2024 ] 	Batch(3600/6809) done. Loss: 0.0543  lr:0.000100
[ Mon Jul 15 22:12:09 2024 ] 	Batch(3700/6809) done. Loss: 0.1193  lr:0.000100
[ Mon Jul 15 22:12:32 2024 ] 	Batch(3800/6809) done. Loss: 0.0095  lr:0.000100
[ Mon Jul 15 22:12:55 2024 ] 	Batch(3900/6809) done. Loss: 0.1992  lr:0.000100
[ Mon Jul 15 22:13:17 2024 ] 
Training: Epoch [73/150], Step [3999], Loss: 0.36558252573013306, Training Accuracy: 94.890625
[ Mon Jul 15 22:13:17 2024 ] 	Batch(4000/6809) done. Loss: 0.2386  lr:0.000100
[ Mon Jul 15 22:13:40 2024 ] 	Batch(4100/6809) done. Loss: 0.1391  lr:0.000100
[ Mon Jul 15 22:14:03 2024 ] 	Batch(4200/6809) done. Loss: 0.1112  lr:0.000100
[ Mon Jul 15 22:14:26 2024 ] 	Batch(4300/6809) done. Loss: 0.0141  lr:0.000100
[ Mon Jul 15 22:14:49 2024 ] 	Batch(4400/6809) done. Loss: 0.1644  lr:0.000100
[ Mon Jul 15 22:15:11 2024 ] 
Training: Epoch [73/150], Step [4499], Loss: 0.31005343794822693, Training Accuracy: 94.95833333333333
[ Mon Jul 15 22:15:11 2024 ] 	Batch(4500/6809) done. Loss: 0.4320  lr:0.000100
[ Mon Jul 15 22:15:34 2024 ] 	Batch(4600/6809) done. Loss: 0.2327  lr:0.000100
[ Mon Jul 15 22:15:57 2024 ] 	Batch(4700/6809) done. Loss: 0.0982  lr:0.000100
[ Mon Jul 15 22:16:19 2024 ] 	Batch(4800/6809) done. Loss: 0.0315  lr:0.000100
[ Mon Jul 15 22:16:42 2024 ] 	Batch(4900/6809) done. Loss: 0.4045  lr:0.000100
[ Mon Jul 15 22:17:04 2024 ] 
Training: Epoch [73/150], Step [4999], Loss: 0.040991052985191345, Training Accuracy: 95.02000000000001
[ Mon Jul 15 22:17:05 2024 ] 	Batch(5000/6809) done. Loss: 0.2208  lr:0.000100
[ Mon Jul 15 22:17:27 2024 ] 	Batch(5100/6809) done. Loss: 0.0954  lr:0.000100
[ Mon Jul 15 22:17:50 2024 ] 	Batch(5200/6809) done. Loss: 0.2345  lr:0.000100
[ Mon Jul 15 22:18:13 2024 ] 	Batch(5300/6809) done. Loss: 0.1567  lr:0.000100
[ Mon Jul 15 22:18:35 2024 ] 	Batch(5400/6809) done. Loss: 0.1755  lr:0.000100
[ Mon Jul 15 22:18:58 2024 ] 
Training: Epoch [73/150], Step [5499], Loss: 0.02736741118133068, Training Accuracy: 95.00227272727273
[ Mon Jul 15 22:18:58 2024 ] 	Batch(5500/6809) done. Loss: 0.0590  lr:0.000100
[ Mon Jul 15 22:19:21 2024 ] 	Batch(5600/6809) done. Loss: 0.0205  lr:0.000100
[ Mon Jul 15 22:19:44 2024 ] 	Batch(5700/6809) done. Loss: 0.0327  lr:0.000100
[ Mon Jul 15 22:20:06 2024 ] 	Batch(5800/6809) done. Loss: 0.2164  lr:0.000100
[ Mon Jul 15 22:20:29 2024 ] 	Batch(5900/6809) done. Loss: 0.1259  lr:0.000100
[ Mon Jul 15 22:20:51 2024 ] 
Training: Epoch [73/150], Step [5999], Loss: 0.09666794538497925, Training Accuracy: 94.97083333333333
[ Mon Jul 15 22:20:52 2024 ] 	Batch(6000/6809) done. Loss: 0.0543  lr:0.000100
[ Mon Jul 15 22:21:14 2024 ] 	Batch(6100/6809) done. Loss: 0.6792  lr:0.000100
[ Mon Jul 15 22:21:37 2024 ] 	Batch(6200/6809) done. Loss: 0.1438  lr:0.000100
[ Mon Jul 15 22:22:00 2024 ] 	Batch(6300/6809) done. Loss: 0.1967  lr:0.000100
[ Mon Jul 15 22:22:23 2024 ] 	Batch(6400/6809) done. Loss: 0.5368  lr:0.000100
[ Mon Jul 15 22:22:46 2024 ] 
Training: Epoch [73/150], Step [6499], Loss: 0.06332012265920639, Training Accuracy: 94.95769230769231
[ Mon Jul 15 22:22:46 2024 ] 	Batch(6500/6809) done. Loss: 0.3172  lr:0.000100
[ Mon Jul 15 22:23:09 2024 ] 	Batch(6600/6809) done. Loss: 0.1058  lr:0.000100
[ Mon Jul 15 22:23:31 2024 ] 	Batch(6700/6809) done. Loss: 0.1756  lr:0.000100
[ Mon Jul 15 22:23:54 2024 ] 	Batch(6800/6809) done. Loss: 0.1186  lr:0.000100
[ Mon Jul 15 22:23:56 2024 ] 	Mean training loss: 0.1826.
[ Mon Jul 15 22:23:56 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 22:23:56 2024 ] Training epoch: 75
[ Mon Jul 15 22:23:57 2024 ] 	Batch(0/6809) done. Loss: 0.0622  lr:0.000100
[ Mon Jul 15 22:24:20 2024 ] 	Batch(100/6809) done. Loss: 0.1917  lr:0.000100
[ Mon Jul 15 22:24:42 2024 ] 	Batch(200/6809) done. Loss: 0.1576  lr:0.000100
[ Mon Jul 15 22:25:05 2024 ] 	Batch(300/6809) done. Loss: 0.3691  lr:0.000100
[ Mon Jul 15 22:25:28 2024 ] 	Batch(400/6809) done. Loss: 0.0587  lr:0.000100
[ Mon Jul 15 22:25:50 2024 ] 
Training: Epoch [74/150], Step [499], Loss: 0.2600533664226532, Training Accuracy: 95.025
[ Mon Jul 15 22:25:50 2024 ] 	Batch(500/6809) done. Loss: 0.0131  lr:0.000100
[ Mon Jul 15 22:26:13 2024 ] 	Batch(600/6809) done. Loss: 0.3244  lr:0.000100
[ Mon Jul 15 22:26:36 2024 ] 	Batch(700/6809) done. Loss: 0.1892  lr:0.000100
[ Mon Jul 15 22:26:59 2024 ] 	Batch(800/6809) done. Loss: 0.3806  lr:0.000100
[ Mon Jul 15 22:27:21 2024 ] 	Batch(900/6809) done. Loss: 0.3686  lr:0.000100
[ Mon Jul 15 22:27:44 2024 ] 
Training: Epoch [74/150], Step [999], Loss: 0.8475514650344849, Training Accuracy: 95.1
[ Mon Jul 15 22:27:44 2024 ] 	Batch(1000/6809) done. Loss: 0.2065  lr:0.000100
[ Mon Jul 15 22:28:06 2024 ] 	Batch(1100/6809) done. Loss: 0.2776  lr:0.000100
[ Mon Jul 15 22:28:29 2024 ] 	Batch(1200/6809) done. Loss: 0.1182  lr:0.000100
[ Mon Jul 15 22:28:52 2024 ] 	Batch(1300/6809) done. Loss: 0.2889  lr:0.000100
[ Mon Jul 15 22:29:15 2024 ] 	Batch(1400/6809) done. Loss: 0.2301  lr:0.000100
[ Mon Jul 15 22:29:37 2024 ] 
Training: Epoch [74/150], Step [1499], Loss: 0.03252333030104637, Training Accuracy: 95.16666666666667
[ Mon Jul 15 22:29:37 2024 ] 	Batch(1500/6809) done. Loss: 0.1171  lr:0.000100
[ Mon Jul 15 22:30:00 2024 ] 	Batch(1600/6809) done. Loss: 0.0772  lr:0.000100
[ Mon Jul 15 22:30:23 2024 ] 	Batch(1700/6809) done. Loss: 0.1339  lr:0.000100
[ Mon Jul 15 22:30:46 2024 ] 	Batch(1800/6809) done. Loss: 0.3032  lr:0.000100
[ Mon Jul 15 22:31:09 2024 ] 	Batch(1900/6809) done. Loss: 0.0765  lr:0.000100
[ Mon Jul 15 22:31:32 2024 ] 
Training: Epoch [74/150], Step [1999], Loss: 0.21795150637626648, Training Accuracy: 95.1125
[ Mon Jul 15 22:31:33 2024 ] 	Batch(2000/6809) done. Loss: 0.1476  lr:0.000100
[ Mon Jul 15 22:31:55 2024 ] 	Batch(2100/6809) done. Loss: 0.1600  lr:0.000100
[ Mon Jul 15 22:32:18 2024 ] 	Batch(2200/6809) done. Loss: 0.5826  lr:0.000100
[ Mon Jul 15 22:32:41 2024 ] 	Batch(2300/6809) done. Loss: 0.1023  lr:0.000100
[ Mon Jul 15 22:33:04 2024 ] 	Batch(2400/6809) done. Loss: 0.0137  lr:0.000100
[ Mon Jul 15 22:33:27 2024 ] 
Training: Epoch [74/150], Step [2499], Loss: 0.03145293891429901, Training Accuracy: 95.12
[ Mon Jul 15 22:33:27 2024 ] 	Batch(2500/6809) done. Loss: 0.2281  lr:0.000100
[ Mon Jul 15 22:33:50 2024 ] 	Batch(2600/6809) done. Loss: 0.4867  lr:0.000100
[ Mon Jul 15 22:34:13 2024 ] 	Batch(2700/6809) done. Loss: 0.0345  lr:0.000100
[ Mon Jul 15 22:34:36 2024 ] 	Batch(2800/6809) done. Loss: 0.0382  lr:0.000100
[ Mon Jul 15 22:34:59 2024 ] 	Batch(2900/6809) done. Loss: 0.7175  lr:0.000100
[ Mon Jul 15 22:35:22 2024 ] 
Training: Epoch [74/150], Step [2999], Loss: 0.27615463733673096, Training Accuracy: 94.97916666666667
[ Mon Jul 15 22:35:22 2024 ] 	Batch(3000/6809) done. Loss: 0.4821  lr:0.000100
[ Mon Jul 15 22:35:45 2024 ] 	Batch(3100/6809) done. Loss: 0.1886  lr:0.000100
[ Mon Jul 15 22:36:08 2024 ] 	Batch(3200/6809) done. Loss: 0.2719  lr:0.000100
[ Mon Jul 15 22:36:31 2024 ] 	Batch(3300/6809) done. Loss: 0.0181  lr:0.000100
[ Mon Jul 15 22:36:53 2024 ] 	Batch(3400/6809) done. Loss: 0.2641  lr:0.000100
[ Mon Jul 15 22:37:16 2024 ] 
Training: Epoch [74/150], Step [3499], Loss: 0.030572369694709778, Training Accuracy: 94.95714285714286
[ Mon Jul 15 22:37:16 2024 ] 	Batch(3500/6809) done. Loss: 0.0906  lr:0.000100
[ Mon Jul 15 22:37:39 2024 ] 	Batch(3600/6809) done. Loss: 0.1671  lr:0.000100
[ Mon Jul 15 22:38:01 2024 ] 	Batch(3700/6809) done. Loss: 0.0603  lr:0.000100
[ Mon Jul 15 22:38:24 2024 ] 	Batch(3800/6809) done. Loss: 0.0151  lr:0.000100
[ Mon Jul 15 22:38:46 2024 ] 	Batch(3900/6809) done. Loss: 0.2177  lr:0.000100
[ Mon Jul 15 22:39:09 2024 ] 
Training: Epoch [74/150], Step [3999], Loss: 0.2731548845767975, Training Accuracy: 94.94687499999999
[ Mon Jul 15 22:39:09 2024 ] 	Batch(4000/6809) done. Loss: 0.0996  lr:0.000100
[ Mon Jul 15 22:39:32 2024 ] 	Batch(4100/6809) done. Loss: 0.3993  lr:0.000100
[ Mon Jul 15 22:39:54 2024 ] 	Batch(4200/6809) done. Loss: 0.0537  lr:0.000100
[ Mon Jul 15 22:40:17 2024 ] 	Batch(4300/6809) done. Loss: 0.4102  lr:0.000100
[ Mon Jul 15 22:40:40 2024 ] 	Batch(4400/6809) done. Loss: 0.2326  lr:0.000100
[ Mon Jul 15 22:41:03 2024 ] 
Training: Epoch [74/150], Step [4499], Loss: 0.09057679027318954, Training Accuracy: 94.98333333333333
[ Mon Jul 15 22:41:03 2024 ] 	Batch(4500/6809) done. Loss: 0.0063  lr:0.000100
[ Mon Jul 15 22:41:26 2024 ] 	Batch(4600/6809) done. Loss: 0.2331  lr:0.000100
[ Mon Jul 15 22:41:50 2024 ] 	Batch(4700/6809) done. Loss: 0.0173  lr:0.000100
[ Mon Jul 15 22:42:13 2024 ] 	Batch(4800/6809) done. Loss: 0.0528  lr:0.000100
[ Mon Jul 15 22:42:36 2024 ] 	Batch(4900/6809) done. Loss: 0.0606  lr:0.000100
[ Mon Jul 15 22:42:58 2024 ] 
Training: Epoch [74/150], Step [4999], Loss: 0.17879891395568848, Training Accuracy: 94.96
[ Mon Jul 15 22:42:58 2024 ] 	Batch(5000/6809) done. Loss: 0.1564  lr:0.000100
[ Mon Jul 15 22:43:21 2024 ] 	Batch(5100/6809) done. Loss: 0.6944  lr:0.000100
[ Mon Jul 15 22:43:44 2024 ] 	Batch(5200/6809) done. Loss: 0.0558  lr:0.000100
[ Mon Jul 15 22:44:06 2024 ] 	Batch(5300/6809) done. Loss: 0.0303  lr:0.000100
[ Mon Jul 15 22:44:29 2024 ] 	Batch(5400/6809) done. Loss: 0.2772  lr:0.000100
[ Mon Jul 15 22:44:51 2024 ] 
Training: Epoch [74/150], Step [5499], Loss: 0.12817564606666565, Training Accuracy: 94.95681818181818
[ Mon Jul 15 22:44:51 2024 ] 	Batch(5500/6809) done. Loss: 0.0191  lr:0.000100
[ Mon Jul 15 22:45:14 2024 ] 	Batch(5600/6809) done. Loss: 0.0076  lr:0.000100
[ Mon Jul 15 22:45:37 2024 ] 	Batch(5700/6809) done. Loss: 0.1087  lr:0.000100
[ Mon Jul 15 22:46:01 2024 ] 	Batch(5800/6809) done. Loss: 0.0905  lr:0.000100
[ Mon Jul 15 22:46:24 2024 ] 	Batch(5900/6809) done. Loss: 0.1941  lr:0.000100
[ Mon Jul 15 22:46:47 2024 ] 
Training: Epoch [74/150], Step [5999], Loss: 0.05617685988545418, Training Accuracy: 94.98124999999999
[ Mon Jul 15 22:46:48 2024 ] 	Batch(6000/6809) done. Loss: 0.4962  lr:0.000100
[ Mon Jul 15 22:47:10 2024 ] 	Batch(6100/6809) done. Loss: 0.2689  lr:0.000100
[ Mon Jul 15 22:47:33 2024 ] 	Batch(6200/6809) done. Loss: 0.1773  lr:0.000100
[ Mon Jul 15 22:47:55 2024 ] 	Batch(6300/6809) done. Loss: 0.1243  lr:0.000100
[ Mon Jul 15 22:48:18 2024 ] 	Batch(6400/6809) done. Loss: 0.3150  lr:0.000100
[ Mon Jul 15 22:48:40 2024 ] 
Training: Epoch [74/150], Step [6499], Loss: 0.06587187945842743, Training Accuracy: 94.9826923076923
[ Mon Jul 15 22:48:41 2024 ] 	Batch(6500/6809) done. Loss: 0.0636  lr:0.000100
[ Mon Jul 15 22:49:03 2024 ] 	Batch(6600/6809) done. Loss: 0.0893  lr:0.000100
[ Mon Jul 15 22:49:26 2024 ] 	Batch(6700/6809) done. Loss: 0.0772  lr:0.000100
[ Mon Jul 15 22:49:48 2024 ] 	Batch(6800/6809) done. Loss: 0.2386  lr:0.000100
[ Mon Jul 15 22:49:50 2024 ] 	Mean training loss: 0.1801.
[ Mon Jul 15 22:49:50 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 22:49:50 2024 ] Training epoch: 76
[ Mon Jul 15 22:49:51 2024 ] 	Batch(0/6809) done. Loss: 0.0133  lr:0.000100
[ Mon Jul 15 22:50:14 2024 ] 	Batch(100/6809) done. Loss: 0.0500  lr:0.000100
[ Mon Jul 15 22:50:36 2024 ] 	Batch(200/6809) done. Loss: 0.1255  lr:0.000100
[ Mon Jul 15 22:50:59 2024 ] 	Batch(300/6809) done. Loss: 0.1813  lr:0.000100
[ Mon Jul 15 22:51:21 2024 ] 	Batch(400/6809) done. Loss: 0.1715  lr:0.000100
[ Mon Jul 15 22:51:44 2024 ] 
Training: Epoch [75/150], Step [499], Loss: 0.1040203794836998, Training Accuracy: 95.575
[ Mon Jul 15 22:51:44 2024 ] 	Batch(500/6809) done. Loss: 0.2904  lr:0.000100
[ Mon Jul 15 22:52:08 2024 ] 	Batch(600/6809) done. Loss: 0.1295  lr:0.000100
[ Mon Jul 15 22:52:31 2024 ] 	Batch(700/6809) done. Loss: 0.1139  lr:0.000100
[ Mon Jul 15 22:52:54 2024 ] 	Batch(800/6809) done. Loss: 0.0619  lr:0.000100
[ Mon Jul 15 22:53:17 2024 ] 	Batch(900/6809) done. Loss: 0.0637  lr:0.000100
[ Mon Jul 15 22:53:39 2024 ] 
Training: Epoch [75/150], Step [999], Loss: 0.09333883970975876, Training Accuracy: 95.46249999999999
[ Mon Jul 15 22:53:40 2024 ] 	Batch(1000/6809) done. Loss: 0.6301  lr:0.000100
[ Mon Jul 15 22:54:02 2024 ] 	Batch(1100/6809) done. Loss: 0.2825  lr:0.000100
[ Mon Jul 15 22:54:25 2024 ] 	Batch(1200/6809) done. Loss: 0.0328  lr:0.000100
[ Mon Jul 15 22:54:48 2024 ] 	Batch(1300/6809) done. Loss: 0.0449  lr:0.000100
[ Mon Jul 15 22:55:11 2024 ] 	Batch(1400/6809) done. Loss: 0.0327  lr:0.000100
[ Mon Jul 15 22:55:34 2024 ] 
Training: Epoch [75/150], Step [1499], Loss: 0.03448399156332016, Training Accuracy: 95.475
[ Mon Jul 15 22:55:34 2024 ] 	Batch(1500/6809) done. Loss: 0.4830  lr:0.000100
[ Mon Jul 15 22:55:57 2024 ] 	Batch(1600/6809) done. Loss: 0.0300  lr:0.000100
[ Mon Jul 15 22:56:20 2024 ] 	Batch(1700/6809) done. Loss: 0.0286  lr:0.000100
[ Mon Jul 15 22:56:42 2024 ] 	Batch(1800/6809) done. Loss: 0.0178  lr:0.000100
[ Mon Jul 15 22:57:06 2024 ] 	Batch(1900/6809) done. Loss: 0.0363  lr:0.000100
[ Mon Jul 15 22:57:28 2024 ] 
Training: Epoch [75/150], Step [1999], Loss: 0.03383663296699524, Training Accuracy: 95.2625
[ Mon Jul 15 22:57:29 2024 ] 	Batch(2000/6809) done. Loss: 0.0206  lr:0.000100
[ Mon Jul 15 22:57:51 2024 ] 	Batch(2100/6809) done. Loss: 0.2149  lr:0.000100
[ Mon Jul 15 22:58:14 2024 ] 	Batch(2200/6809) done. Loss: 0.0294  lr:0.000100
[ Mon Jul 15 22:58:36 2024 ] 	Batch(2300/6809) done. Loss: 0.0546  lr:0.000100
[ Mon Jul 15 22:58:59 2024 ] 	Batch(2400/6809) done. Loss: 0.0799  lr:0.000100
[ Mon Jul 15 22:59:22 2024 ] 
Training: Epoch [75/150], Step [2499], Loss: 0.014049510471522808, Training Accuracy: 95.155
[ Mon Jul 15 22:59:22 2024 ] 	Batch(2500/6809) done. Loss: 0.0914  lr:0.000100
[ Mon Jul 15 22:59:44 2024 ] 	Batch(2600/6809) done. Loss: 0.6090  lr:0.000100
[ Mon Jul 15 23:00:07 2024 ] 	Batch(2700/6809) done. Loss: 0.3866  lr:0.000100
[ Mon Jul 15 23:00:30 2024 ] 	Batch(2800/6809) done. Loss: 0.0593  lr:0.000100
[ Mon Jul 15 23:00:52 2024 ] 	Batch(2900/6809) done. Loss: 0.1917  lr:0.000100
[ Mon Jul 15 23:01:15 2024 ] 
Training: Epoch [75/150], Step [2999], Loss: 0.032692283391952515, Training Accuracy: 95.10416666666667
[ Mon Jul 15 23:01:15 2024 ] 	Batch(3000/6809) done. Loss: 0.0158  lr:0.000100
[ Mon Jul 15 23:01:37 2024 ] 	Batch(3100/6809) done. Loss: 0.1574  lr:0.000100
[ Mon Jul 15 23:02:00 2024 ] 	Batch(3200/6809) done. Loss: 1.0417  lr:0.000100
[ Mon Jul 15 23:02:23 2024 ] 	Batch(3300/6809) done. Loss: 0.0184  lr:0.000100
[ Mon Jul 15 23:02:45 2024 ] 	Batch(3400/6809) done. Loss: 0.0786  lr:0.000100
[ Mon Jul 15 23:03:08 2024 ] 
Training: Epoch [75/150], Step [3499], Loss: 0.10228323191404343, Training Accuracy: 95.01785714285714
[ Mon Jul 15 23:03:08 2024 ] 	Batch(3500/6809) done. Loss: 0.0389  lr:0.000100
[ Mon Jul 15 23:03:31 2024 ] 	Batch(3600/6809) done. Loss: 0.2937  lr:0.000100
[ Mon Jul 15 23:03:54 2024 ] 	Batch(3700/6809) done. Loss: 0.1470  lr:0.000100
[ Mon Jul 15 23:04:17 2024 ] 	Batch(3800/6809) done. Loss: 0.0428  lr:0.000100
[ Mon Jul 15 23:04:41 2024 ] 	Batch(3900/6809) done. Loss: 0.6832  lr:0.000100
[ Mon Jul 15 23:05:03 2024 ] 
Training: Epoch [75/150], Step [3999], Loss: 0.030297815799713135, Training Accuracy: 95.03125
[ Mon Jul 15 23:05:04 2024 ] 	Batch(4000/6809) done. Loss: 0.2745  lr:0.000100
[ Mon Jul 15 23:05:26 2024 ] 	Batch(4100/6809) done. Loss: 0.1053  lr:0.000100
[ Mon Jul 15 23:05:49 2024 ] 	Batch(4200/6809) done. Loss: 0.0213  lr:0.000100
[ Mon Jul 15 23:06:12 2024 ] 	Batch(4300/6809) done. Loss: 0.0477  lr:0.000100
[ Mon Jul 15 23:06:35 2024 ] 	Batch(4400/6809) done. Loss: 0.0749  lr:0.000100
[ Mon Jul 15 23:06:57 2024 ] 
Training: Epoch [75/150], Step [4499], Loss: 0.015492333099246025, Training Accuracy: 95.0
[ Mon Jul 15 23:06:57 2024 ] 	Batch(4500/6809) done. Loss: 0.1221  lr:0.000100
[ Mon Jul 15 23:07:20 2024 ] 	Batch(4600/6809) done. Loss: 0.1698  lr:0.000100
[ Mon Jul 15 23:07:42 2024 ] 	Batch(4700/6809) done. Loss: 0.2229  lr:0.000100
[ Mon Jul 15 23:08:05 2024 ] 	Batch(4800/6809) done. Loss: 0.0786  lr:0.000100
[ Mon Jul 15 23:08:28 2024 ] 	Batch(4900/6809) done. Loss: 0.4180  lr:0.000100
[ Mon Jul 15 23:08:50 2024 ] 
Training: Epoch [75/150], Step [4999], Loss: 0.20919236540794373, Training Accuracy: 95.06750000000001
[ Mon Jul 15 23:08:50 2024 ] 	Batch(5000/6809) done. Loss: 0.1016  lr:0.000100
[ Mon Jul 15 23:09:13 2024 ] 	Batch(5100/6809) done. Loss: 0.0037  lr:0.000100
[ Mon Jul 15 23:09:36 2024 ] 	Batch(5200/6809) done. Loss: 0.1090  lr:0.000100
[ Mon Jul 15 23:09:59 2024 ] 	Batch(5300/6809) done. Loss: 0.5860  lr:0.000100
[ Mon Jul 15 23:10:22 2024 ] 	Batch(5400/6809) done. Loss: 0.0591  lr:0.000100
[ Mon Jul 15 23:10:46 2024 ] 
Training: Epoch [75/150], Step [5499], Loss: 0.06400471180677414, Training Accuracy: 95.08409090909092
[ Mon Jul 15 23:10:46 2024 ] 	Batch(5500/6809) done. Loss: 0.2376  lr:0.000100
[ Mon Jul 15 23:11:09 2024 ] 	Batch(5600/6809) done. Loss: 0.2198  lr:0.000100
[ Mon Jul 15 23:11:31 2024 ] 	Batch(5700/6809) done. Loss: 0.1261  lr:0.000100
[ Mon Jul 15 23:11:54 2024 ] 	Batch(5800/6809) done. Loss: 0.1719  lr:0.000100
[ Mon Jul 15 23:12:16 2024 ] 	Batch(5900/6809) done. Loss: 0.3039  lr:0.000100
[ Mon Jul 15 23:12:39 2024 ] 
Training: Epoch [75/150], Step [5999], Loss: 0.2951677739620209, Training Accuracy: 95.05416666666666
[ Mon Jul 15 23:12:39 2024 ] 	Batch(6000/6809) done. Loss: 0.0610  lr:0.000100
[ Mon Jul 15 23:13:02 2024 ] 	Batch(6100/6809) done. Loss: 0.3801  lr:0.000100
[ Mon Jul 15 23:13:24 2024 ] 	Batch(6200/6809) done. Loss: 0.1064  lr:0.000100
[ Mon Jul 15 23:13:47 2024 ] 	Batch(6300/6809) done. Loss: 0.0943  lr:0.000100
[ Mon Jul 15 23:14:10 2024 ] 	Batch(6400/6809) done. Loss: 0.5527  lr:0.000100
[ Mon Jul 15 23:14:32 2024 ] 
Training: Epoch [75/150], Step [6499], Loss: 0.24488221108913422, Training Accuracy: 95.03076923076922
[ Mon Jul 15 23:14:32 2024 ] 	Batch(6500/6809) done. Loss: 0.0327  lr:0.000100
[ Mon Jul 15 23:14:55 2024 ] 	Batch(6600/6809) done. Loss: 0.0826  lr:0.000100
[ Mon Jul 15 23:15:17 2024 ] 	Batch(6700/6809) done. Loss: 0.3071  lr:0.000100
[ Mon Jul 15 23:15:40 2024 ] 	Batch(6800/6809) done. Loss: 0.2074  lr:0.000100
[ Mon Jul 15 23:15:42 2024 ] 	Mean training loss: 0.1762.
[ Mon Jul 15 23:15:42 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 23:15:42 2024 ] Training epoch: 77
[ Mon Jul 15 23:15:43 2024 ] 	Batch(0/6809) done. Loss: 0.2419  lr:0.000100
[ Mon Jul 15 23:16:05 2024 ] 	Batch(100/6809) done. Loss: 0.0654  lr:0.000100
[ Mon Jul 15 23:16:28 2024 ] 	Batch(200/6809) done. Loss: 0.2201  lr:0.000100
[ Mon Jul 15 23:16:51 2024 ] 	Batch(300/6809) done. Loss: 0.1773  lr:0.000100
[ Mon Jul 15 23:17:14 2024 ] 	Batch(400/6809) done. Loss: 0.0355  lr:0.000100
[ Mon Jul 15 23:17:36 2024 ] 
Training: Epoch [76/150], Step [499], Loss: 0.38202276825904846, Training Accuracy: 95.0
[ Mon Jul 15 23:17:36 2024 ] 	Batch(500/6809) done. Loss: 0.0406  lr:0.000100
[ Mon Jul 15 23:17:59 2024 ] 	Batch(600/6809) done. Loss: 0.2691  lr:0.000100
[ Mon Jul 15 23:18:21 2024 ] 	Batch(700/6809) done. Loss: 0.1611  lr:0.000100
[ Mon Jul 15 23:18:44 2024 ] 	Batch(800/6809) done. Loss: 0.1419  lr:0.000100
[ Mon Jul 15 23:19:07 2024 ] 	Batch(900/6809) done. Loss: 0.0774  lr:0.000100
[ Mon Jul 15 23:19:29 2024 ] 
Training: Epoch [76/150], Step [999], Loss: 0.10974667966365814, Training Accuracy: 95.16250000000001
[ Mon Jul 15 23:19:29 2024 ] 	Batch(1000/6809) done. Loss: 0.0407  lr:0.000100
[ Mon Jul 15 23:19:52 2024 ] 	Batch(1100/6809) done. Loss: 0.3249  lr:0.000100
[ Mon Jul 15 23:20:15 2024 ] 	Batch(1200/6809) done. Loss: 0.2450  lr:0.000100
[ Mon Jul 15 23:20:37 2024 ] 	Batch(1300/6809) done. Loss: 0.2039  lr:0.000100
[ Mon Jul 15 23:21:00 2024 ] 	Batch(1400/6809) done. Loss: 0.1509  lr:0.000100
[ Mon Jul 15 23:21:23 2024 ] 
Training: Epoch [76/150], Step [1499], Loss: 0.12815281748771667, Training Accuracy: 95.25833333333334
[ Mon Jul 15 23:21:23 2024 ] 	Batch(1500/6809) done. Loss: 0.3679  lr:0.000100
[ Mon Jul 15 23:21:47 2024 ] 	Batch(1600/6809) done. Loss: 0.3910  lr:0.000100
[ Mon Jul 15 23:22:09 2024 ] 	Batch(1700/6809) done. Loss: 0.1685  lr:0.000100
[ Mon Jul 15 23:22:32 2024 ] 	Batch(1800/6809) done. Loss: 0.1431  lr:0.000100
[ Mon Jul 15 23:22:55 2024 ] 	Batch(1900/6809) done. Loss: 0.1236  lr:0.000100
[ Mon Jul 15 23:23:18 2024 ] 
Training: Epoch [76/150], Step [1999], Loss: 0.3508881628513336, Training Accuracy: 95.11874999999999
[ Mon Jul 15 23:23:18 2024 ] 	Batch(2000/6809) done. Loss: 0.3179  lr:0.000100
[ Mon Jul 15 23:23:41 2024 ] 	Batch(2100/6809) done. Loss: 0.0566  lr:0.000100
[ Mon Jul 15 23:24:04 2024 ] 	Batch(2200/6809) done. Loss: 0.1778  lr:0.000100
[ Mon Jul 15 23:24:26 2024 ] 	Batch(2300/6809) done. Loss: 0.0228  lr:0.000100
[ Mon Jul 15 23:24:49 2024 ] 	Batch(2400/6809) done. Loss: 0.3000  lr:0.000100
[ Mon Jul 15 23:25:12 2024 ] 
Training: Epoch [76/150], Step [2499], Loss: 0.04654107987880707, Training Accuracy: 95.11500000000001
[ Mon Jul 15 23:25:12 2024 ] 	Batch(2500/6809) done. Loss: 0.1760  lr:0.000100
[ Mon Jul 15 23:25:35 2024 ] 	Batch(2600/6809) done. Loss: 0.0771  lr:0.000100
[ Mon Jul 15 23:25:57 2024 ] 	Batch(2700/6809) done. Loss: 0.1012  lr:0.000100
[ Mon Jul 15 23:26:20 2024 ] 	Batch(2800/6809) done. Loss: 0.0476  lr:0.000100
[ Mon Jul 15 23:26:43 2024 ] 	Batch(2900/6809) done. Loss: 0.0337  lr:0.000100
[ Mon Jul 15 23:27:05 2024 ] 
Training: Epoch [76/150], Step [2999], Loss: 0.058524537831544876, Training Accuracy: 95.12083333333334
[ Mon Jul 15 23:27:05 2024 ] 	Batch(3000/6809) done. Loss: 0.3445  lr:0.000100
[ Mon Jul 15 23:27:28 2024 ] 	Batch(3100/6809) done. Loss: 0.0058  lr:0.000100
[ Mon Jul 15 23:27:51 2024 ] 	Batch(3200/6809) done. Loss: 0.3089  lr:0.000100
[ Mon Jul 15 23:28:14 2024 ] 	Batch(3300/6809) done. Loss: 0.5270  lr:0.000100
[ Mon Jul 15 23:28:37 2024 ] 	Batch(3400/6809) done. Loss: 0.2310  lr:0.000100
[ Mon Jul 15 23:29:00 2024 ] 
Training: Epoch [76/150], Step [3499], Loss: 0.15972895920276642, Training Accuracy: 95.12142857142857
[ Mon Jul 15 23:29:00 2024 ] 	Batch(3500/6809) done. Loss: 0.0580  lr:0.000100
[ Mon Jul 15 23:29:23 2024 ] 	Batch(3600/6809) done. Loss: 0.0484  lr:0.000100
[ Mon Jul 15 23:29:46 2024 ] 	Batch(3700/6809) done. Loss: 0.1861  lr:0.000100
[ Mon Jul 15 23:30:09 2024 ] 	Batch(3800/6809) done. Loss: 0.1753  lr:0.000100
[ Mon Jul 15 23:30:32 2024 ] 	Batch(3900/6809) done. Loss: 0.2857  lr:0.000100
[ Mon Jul 15 23:30:55 2024 ] 
Training: Epoch [76/150], Step [3999], Loss: 0.3875405788421631, Training Accuracy: 95.10625
[ Mon Jul 15 23:30:55 2024 ] 	Batch(4000/6809) done. Loss: 0.2361  lr:0.000100
[ Mon Jul 15 23:31:18 2024 ] 	Batch(4100/6809) done. Loss: 0.0326  lr:0.000100
[ Mon Jul 15 23:31:41 2024 ] 	Batch(4200/6809) done. Loss: 1.0304  lr:0.000100
[ Mon Jul 15 23:32:03 2024 ] 	Batch(4300/6809) done. Loss: 0.3450  lr:0.000100
[ Mon Jul 15 23:32:26 2024 ] 	Batch(4400/6809) done. Loss: 0.2729  lr:0.000100
[ Mon Jul 15 23:32:48 2024 ] 
Training: Epoch [76/150], Step [4499], Loss: 0.27417024970054626, Training Accuracy: 95.12222222222222
[ Mon Jul 15 23:32:49 2024 ] 	Batch(4500/6809) done. Loss: 0.0855  lr:0.000100
[ Mon Jul 15 23:33:11 2024 ] 	Batch(4600/6809) done. Loss: 0.1075  lr:0.000100
[ Mon Jul 15 23:33:34 2024 ] 	Batch(4700/6809) done. Loss: 0.0014  lr:0.000100
[ Mon Jul 15 23:33:57 2024 ] 	Batch(4800/6809) done. Loss: 0.3969  lr:0.000100
[ Mon Jul 15 23:34:19 2024 ] 	Batch(4900/6809) done. Loss: 0.1977  lr:0.000100
[ Mon Jul 15 23:34:42 2024 ] 
Training: Epoch [76/150], Step [4999], Loss: 0.22095656394958496, Training Accuracy: 95.11749999999999
[ Mon Jul 15 23:34:42 2024 ] 	Batch(5000/6809) done. Loss: 0.0176  lr:0.000100
[ Mon Jul 15 23:35:05 2024 ] 	Batch(5100/6809) done. Loss: 0.1769  lr:0.000100
[ Mon Jul 15 23:35:28 2024 ] 	Batch(5200/6809) done. Loss: 0.2097  lr:0.000100
[ Mon Jul 15 23:35:50 2024 ] 	Batch(5300/6809) done. Loss: 0.1386  lr:0.000100
[ Mon Jul 15 23:36:13 2024 ] 	Batch(5400/6809) done. Loss: 0.0111  lr:0.000100
[ Mon Jul 15 23:36:35 2024 ] 
Training: Epoch [76/150], Step [5499], Loss: 0.10945403575897217, Training Accuracy: 95.1
[ Mon Jul 15 23:36:36 2024 ] 	Batch(5500/6809) done. Loss: 0.0830  lr:0.000100
[ Mon Jul 15 23:36:58 2024 ] 	Batch(5600/6809) done. Loss: 0.1993  lr:0.000100
[ Mon Jul 15 23:37:21 2024 ] 	Batch(5700/6809) done. Loss: 0.0615  lr:0.000100
[ Mon Jul 15 23:37:45 2024 ] 	Batch(5800/6809) done. Loss: 0.3571  lr:0.000100
[ Mon Jul 15 23:38:08 2024 ] 	Batch(5900/6809) done. Loss: 0.1588  lr:0.000100
[ Mon Jul 15 23:38:31 2024 ] 
Training: Epoch [76/150], Step [5999], Loss: 0.022474141791462898, Training Accuracy: 95.07916666666667
[ Mon Jul 15 23:38:31 2024 ] 	Batch(6000/6809) done. Loss: 0.4771  lr:0.000100
[ Mon Jul 15 23:38:54 2024 ] 	Batch(6100/6809) done. Loss: 0.0527  lr:0.000100
[ Mon Jul 15 23:39:17 2024 ] 	Batch(6200/6809) done. Loss: 0.0384  lr:0.000100
[ Mon Jul 15 23:39:40 2024 ] 	Batch(6300/6809) done. Loss: 0.0868  lr:0.000100
[ Mon Jul 15 23:40:04 2024 ] 	Batch(6400/6809) done. Loss: 0.1007  lr:0.000100
[ Mon Jul 15 23:40:26 2024 ] 
Training: Epoch [76/150], Step [6499], Loss: 0.021273165941238403, Training Accuracy: 95.07884615384616
[ Mon Jul 15 23:40:27 2024 ] 	Batch(6500/6809) done. Loss: 0.2786  lr:0.000100
[ Mon Jul 15 23:40:50 2024 ] 	Batch(6600/6809) done. Loss: 1.3390  lr:0.000100
[ Mon Jul 15 23:41:12 2024 ] 	Batch(6700/6809) done. Loss: 0.0774  lr:0.000100
[ Mon Jul 15 23:41:35 2024 ] 	Batch(6800/6809) done. Loss: 0.2116  lr:0.000100
[ Mon Jul 15 23:41:37 2024 ] 	Mean training loss: 0.1715.
[ Mon Jul 15 23:41:37 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Mon Jul 15 23:41:37 2024 ] Training epoch: 78
[ Mon Jul 15 23:41:38 2024 ] 	Batch(0/6809) done. Loss: 0.2121  lr:0.000100
[ Mon Jul 15 23:42:01 2024 ] 	Batch(100/6809) done. Loss: 0.3047  lr:0.000100
[ Mon Jul 15 23:42:25 2024 ] 	Batch(200/6809) done. Loss: 0.1558  lr:0.000100
[ Mon Jul 15 23:42:48 2024 ] 	Batch(300/6809) done. Loss: 0.1315  lr:0.000100
[ Mon Jul 15 23:43:12 2024 ] 	Batch(400/6809) done. Loss: 0.0712  lr:0.000100
[ Mon Jul 15 23:43:34 2024 ] 
Training: Epoch [77/150], Step [499], Loss: 0.21553771197795868, Training Accuracy: 95.5
[ Mon Jul 15 23:43:35 2024 ] 	Batch(500/6809) done. Loss: 0.0171  lr:0.000100
[ Mon Jul 15 23:43:57 2024 ] 	Batch(600/6809) done. Loss: 0.0986  lr:0.000100
[ Mon Jul 15 23:44:20 2024 ] 	Batch(700/6809) done. Loss: 0.0026  lr:0.000100
[ Mon Jul 15 23:44:43 2024 ] 	Batch(800/6809) done. Loss: 0.1903  lr:0.000100
[ Mon Jul 15 23:45:05 2024 ] 	Batch(900/6809) done. Loss: 0.1240  lr:0.000100
[ Mon Jul 15 23:45:28 2024 ] 
Training: Epoch [77/150], Step [999], Loss: 0.17764687538146973, Training Accuracy: 95.6125
[ Mon Jul 15 23:45:28 2024 ] 	Batch(1000/6809) done. Loss: 0.0262  lr:0.000100
[ Mon Jul 15 23:45:50 2024 ] 	Batch(1100/6809) done. Loss: 0.5599  lr:0.000100
[ Mon Jul 15 23:46:13 2024 ] 	Batch(1200/6809) done. Loss: 0.0878  lr:0.000100
[ Mon Jul 15 23:46:36 2024 ] 	Batch(1300/6809) done. Loss: 0.0509  lr:0.000100
[ Mon Jul 15 23:46:58 2024 ] 	Batch(1400/6809) done. Loss: 0.1392  lr:0.000100
[ Mon Jul 15 23:47:21 2024 ] 
Training: Epoch [77/150], Step [1499], Loss: 0.1876300722360611, Training Accuracy: 95.28333333333333
[ Mon Jul 15 23:47:21 2024 ] 	Batch(1500/6809) done. Loss: 0.4455  lr:0.000100
[ Mon Jul 15 23:47:44 2024 ] 	Batch(1600/6809) done. Loss: 0.0152  lr:0.000100
[ Mon Jul 15 23:48:06 2024 ] 	Batch(1700/6809) done. Loss: 0.3202  lr:0.000100
[ Mon Jul 15 23:48:29 2024 ] 	Batch(1800/6809) done. Loss: 0.0935  lr:0.000100
[ Mon Jul 15 23:48:52 2024 ] 	Batch(1900/6809) done. Loss: 0.0535  lr:0.000100
[ Mon Jul 15 23:49:14 2024 ] 
Training: Epoch [77/150], Step [1999], Loss: 0.7013182044029236, Training Accuracy: 95.2375
[ Mon Jul 15 23:49:15 2024 ] 	Batch(2000/6809) done. Loss: 0.3727  lr:0.000100
[ Mon Jul 15 23:49:37 2024 ] 	Batch(2100/6809) done. Loss: 0.0899  lr:0.000100
[ Mon Jul 15 23:50:00 2024 ] 	Batch(2200/6809) done. Loss: 0.1596  lr:0.000100
[ Mon Jul 15 23:50:24 2024 ] 	Batch(2300/6809) done. Loss: 0.1092  lr:0.000100
[ Mon Jul 15 23:50:48 2024 ] 	Batch(2400/6809) done. Loss: 0.0233  lr:0.000100
[ Mon Jul 15 23:51:11 2024 ] 
Training: Epoch [77/150], Step [2499], Loss: 0.1644534170627594, Training Accuracy: 95.22
[ Mon Jul 15 23:51:11 2024 ] 	Batch(2500/6809) done. Loss: 0.1508  lr:0.000100
[ Mon Jul 15 23:51:34 2024 ] 	Batch(2600/6809) done. Loss: 0.0102  lr:0.000100
[ Mon Jul 15 23:51:57 2024 ] 	Batch(2700/6809) done. Loss: 0.0906  lr:0.000100
[ Mon Jul 15 23:52:20 2024 ] 	Batch(2800/6809) done. Loss: 0.0181  lr:0.000100
[ Mon Jul 15 23:52:42 2024 ] 	Batch(2900/6809) done. Loss: 0.2319  lr:0.000100
[ Mon Jul 15 23:53:04 2024 ] 
Training: Epoch [77/150], Step [2999], Loss: 0.07391594350337982, Training Accuracy: 95.27916666666667
[ Mon Jul 15 23:53:05 2024 ] 	Batch(3000/6809) done. Loss: 0.0998  lr:0.000100
[ Mon Jul 15 23:53:27 2024 ] 	Batch(3100/6809) done. Loss: 0.0166  lr:0.000100
[ Mon Jul 15 23:53:50 2024 ] 	Batch(3200/6809) done. Loss: 0.0685  lr:0.000100
[ Mon Jul 15 23:54:12 2024 ] 	Batch(3300/6809) done. Loss: 0.0168  lr:0.000100
[ Mon Jul 15 23:54:35 2024 ] 	Batch(3400/6809) done. Loss: 0.0572  lr:0.000100
[ Mon Jul 15 23:54:57 2024 ] 
Training: Epoch [77/150], Step [3499], Loss: 0.4206047058105469, Training Accuracy: 95.22500000000001
[ Mon Jul 15 23:54:58 2024 ] 	Batch(3500/6809) done. Loss: 0.3649  lr:0.000100
[ Mon Jul 15 23:55:20 2024 ] 	Batch(3600/6809) done. Loss: 0.1129  lr:0.000100
[ Mon Jul 15 23:55:43 2024 ] 	Batch(3700/6809) done. Loss: 0.0722  lr:0.000100
[ Mon Jul 15 23:56:05 2024 ] 	Batch(3800/6809) done. Loss: 0.0951  lr:0.000100
[ Mon Jul 15 23:56:28 2024 ] 	Batch(3900/6809) done. Loss: 0.3823  lr:0.000100
[ Mon Jul 15 23:56:50 2024 ] 
Training: Epoch [77/150], Step [3999], Loss: 0.01100401021540165, Training Accuracy: 95.22812499999999
[ Mon Jul 15 23:56:51 2024 ] 	Batch(4000/6809) done. Loss: 0.5125  lr:0.000100
[ Mon Jul 15 23:57:13 2024 ] 	Batch(4100/6809) done. Loss: 0.0652  lr:0.000100
[ Mon Jul 15 23:57:36 2024 ] 	Batch(4200/6809) done. Loss: 0.0150  lr:0.000100
[ Mon Jul 15 23:57:58 2024 ] 	Batch(4300/6809) done. Loss: 0.2029  lr:0.000100
[ Mon Jul 15 23:58:21 2024 ] 	Batch(4400/6809) done. Loss: 0.1338  lr:0.000100
[ Mon Jul 15 23:58:44 2024 ] 
Training: Epoch [77/150], Step [4499], Loss: 0.01196000725030899, Training Accuracy: 95.15277777777777
[ Mon Jul 15 23:58:44 2024 ] 	Batch(4500/6809) done. Loss: 0.4693  lr:0.000100
[ Mon Jul 15 23:59:06 2024 ] 	Batch(4600/6809) done. Loss: 0.5477  lr:0.000100
[ Mon Jul 15 23:59:29 2024 ] 	Batch(4700/6809) done. Loss: 0.2564  lr:0.000100
[ Mon Jul 15 23:59:52 2024 ] 	Batch(4800/6809) done. Loss: 0.3348  lr:0.000100
[ Tue Jul 16 00:00:14 2024 ] 	Batch(4900/6809) done. Loss: 0.0498  lr:0.000100
[ Tue Jul 16 00:00:37 2024 ] 
Training: Epoch [77/150], Step [4999], Loss: 0.04911590367555618, Training Accuracy: 95.1475
[ Tue Jul 16 00:00:37 2024 ] 	Batch(5000/6809) done. Loss: 0.1321  lr:0.000100
[ Tue Jul 16 00:00:59 2024 ] 	Batch(5100/6809) done. Loss: 0.0858  lr:0.000100
[ Tue Jul 16 00:01:22 2024 ] 	Batch(5200/6809) done. Loss: 0.0161  lr:0.000100
[ Tue Jul 16 00:01:45 2024 ] 	Batch(5300/6809) done. Loss: 0.1957  lr:0.000100
[ Tue Jul 16 00:02:07 2024 ] 	Batch(5400/6809) done. Loss: 0.1531  lr:0.000100
[ Tue Jul 16 00:02:30 2024 ] 
Training: Epoch [77/150], Step [5499], Loss: 0.09802128374576569, Training Accuracy: 95.1590909090909
[ Tue Jul 16 00:02:30 2024 ] 	Batch(5500/6809) done. Loss: 0.2987  lr:0.000100
[ Tue Jul 16 00:02:53 2024 ] 	Batch(5600/6809) done. Loss: 0.1469  lr:0.000100
[ Tue Jul 16 00:03:15 2024 ] 	Batch(5700/6809) done. Loss: 0.1311  lr:0.000100
[ Tue Jul 16 00:03:38 2024 ] 	Batch(5800/6809) done. Loss: 0.1166  lr:0.000100
[ Tue Jul 16 00:04:00 2024 ] 	Batch(5900/6809) done. Loss: 0.0750  lr:0.000100
[ Tue Jul 16 00:04:23 2024 ] 
Training: Epoch [77/150], Step [5999], Loss: 0.01456860639154911, Training Accuracy: 95.13125
[ Tue Jul 16 00:04:23 2024 ] 	Batch(6000/6809) done. Loss: 0.2180  lr:0.000100
[ Tue Jul 16 00:04:47 2024 ] 	Batch(6100/6809) done. Loss: 0.0255  lr:0.000100
[ Tue Jul 16 00:05:10 2024 ] 	Batch(6200/6809) done. Loss: 0.1585  lr:0.000100
[ Tue Jul 16 00:05:32 2024 ] 	Batch(6300/6809) done. Loss: 0.0807  lr:0.000100
[ Tue Jul 16 00:05:55 2024 ] 	Batch(6400/6809) done. Loss: 0.2227  lr:0.000100
[ Tue Jul 16 00:06:17 2024 ] 
Training: Epoch [77/150], Step [6499], Loss: 0.2103477567434311, Training Accuracy: 95.13846153846154
[ Tue Jul 16 00:06:18 2024 ] 	Batch(6500/6809) done. Loss: 0.0539  lr:0.000100
[ Tue Jul 16 00:06:40 2024 ] 	Batch(6600/6809) done. Loss: 0.1710  lr:0.000100
[ Tue Jul 16 00:07:03 2024 ] 	Batch(6700/6809) done. Loss: 0.4585  lr:0.000100
[ Tue Jul 16 00:07:26 2024 ] 	Batch(6800/6809) done. Loss: 0.2896  lr:0.000100
[ Tue Jul 16 00:07:27 2024 ] 	Mean training loss: 0.1732.
[ Tue Jul 16 00:07:27 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 00:07:28 2024 ] Training epoch: 79
[ Tue Jul 16 00:07:28 2024 ] 	Batch(0/6809) done. Loss: 0.1361  lr:0.000100
[ Tue Jul 16 00:07:51 2024 ] 	Batch(100/6809) done. Loss: 0.0627  lr:0.000100
[ Tue Jul 16 00:08:13 2024 ] 	Batch(200/6809) done. Loss: 0.1963  lr:0.000100
[ Tue Jul 16 00:08:36 2024 ] 	Batch(300/6809) done. Loss: 0.2819  lr:0.000100
[ Tue Jul 16 00:08:59 2024 ] 	Batch(400/6809) done. Loss: 0.1869  lr:0.000100
[ Tue Jul 16 00:09:21 2024 ] 
Training: Epoch [78/150], Step [499], Loss: 0.2067406177520752, Training Accuracy: 94.975
[ Tue Jul 16 00:09:21 2024 ] 	Batch(500/6809) done. Loss: 0.0901  lr:0.000100
[ Tue Jul 16 00:09:44 2024 ] 	Batch(600/6809) done. Loss: 0.0585  lr:0.000100
[ Tue Jul 16 00:10:06 2024 ] 	Batch(700/6809) done. Loss: 0.3551  lr:0.000100
[ Tue Jul 16 00:10:29 2024 ] 	Batch(800/6809) done. Loss: 0.0191  lr:0.000100
[ Tue Jul 16 00:10:52 2024 ] 	Batch(900/6809) done. Loss: 0.0439  lr:0.000100
[ Tue Jul 16 00:11:14 2024 ] 
Training: Epoch [78/150], Step [999], Loss: 0.14779388904571533, Training Accuracy: 95.3
[ Tue Jul 16 00:11:14 2024 ] 	Batch(1000/6809) done. Loss: 0.1344  lr:0.000100
[ Tue Jul 16 00:11:37 2024 ] 	Batch(1100/6809) done. Loss: 0.0505  lr:0.000100
[ Tue Jul 16 00:11:59 2024 ] 	Batch(1200/6809) done. Loss: 0.4599  lr:0.000100
[ Tue Jul 16 00:12:22 2024 ] 	Batch(1300/6809) done. Loss: 0.3167  lr:0.000100
[ Tue Jul 16 00:12:44 2024 ] 	Batch(1400/6809) done. Loss: 0.0561  lr:0.000100
[ Tue Jul 16 00:13:07 2024 ] 
Training: Epoch [78/150], Step [1499], Loss: 0.39883527159690857, Training Accuracy: 95.21666666666667
[ Tue Jul 16 00:13:07 2024 ] 	Batch(1500/6809) done. Loss: 0.7195  lr:0.000100
[ Tue Jul 16 00:13:30 2024 ] 	Batch(1600/6809) done. Loss: 0.0253  lr:0.000100
[ Tue Jul 16 00:13:52 2024 ] 	Batch(1700/6809) done. Loss: 0.1042  lr:0.000100
[ Tue Jul 16 00:14:15 2024 ] 	Batch(1800/6809) done. Loss: 0.2552  lr:0.000100
[ Tue Jul 16 00:14:38 2024 ] 	Batch(1900/6809) done. Loss: 0.1564  lr:0.000100
[ Tue Jul 16 00:15:00 2024 ] 
Training: Epoch [78/150], Step [1999], Loss: 0.019070807844400406, Training Accuracy: 95.25625
[ Tue Jul 16 00:15:00 2024 ] 	Batch(2000/6809) done. Loss: 0.2276  lr:0.000100
[ Tue Jul 16 00:15:23 2024 ] 	Batch(2100/6809) done. Loss: 0.7635  lr:0.000100
[ Tue Jul 16 00:15:45 2024 ] 	Batch(2200/6809) done. Loss: 0.0634  lr:0.000100
[ Tue Jul 16 00:16:08 2024 ] 	Batch(2300/6809) done. Loss: 0.2511  lr:0.000100
[ Tue Jul 16 00:16:31 2024 ] 	Batch(2400/6809) done. Loss: 0.1690  lr:0.000100
[ Tue Jul 16 00:16:53 2024 ] 
Training: Epoch [78/150], Step [2499], Loss: 0.040472306311130524, Training Accuracy: 95.26
[ Tue Jul 16 00:16:53 2024 ] 	Batch(2500/6809) done. Loss: 0.1091  lr:0.000100
[ Tue Jul 16 00:17:16 2024 ] 	Batch(2600/6809) done. Loss: 0.3900  lr:0.000100
[ Tue Jul 16 00:17:38 2024 ] 	Batch(2700/6809) done. Loss: 0.0219  lr:0.000100
[ Tue Jul 16 00:18:01 2024 ] 	Batch(2800/6809) done. Loss: 0.9873  lr:0.000100
[ Tue Jul 16 00:18:24 2024 ] 	Batch(2900/6809) done. Loss: 0.0382  lr:0.000100
[ Tue Jul 16 00:18:46 2024 ] 
Training: Epoch [78/150], Step [2999], Loss: 0.20626868307590485, Training Accuracy: 95.23333333333333
[ Tue Jul 16 00:18:46 2024 ] 	Batch(3000/6809) done. Loss: 0.2710  lr:0.000100
[ Tue Jul 16 00:19:09 2024 ] 	Batch(3100/6809) done. Loss: 0.0355  lr:0.000100
[ Tue Jul 16 00:19:32 2024 ] 	Batch(3200/6809) done. Loss: 0.0936  lr:0.000100
[ Tue Jul 16 00:19:54 2024 ] 	Batch(3300/6809) done. Loss: 0.2574  lr:0.000100
[ Tue Jul 16 00:20:17 2024 ] 	Batch(3400/6809) done. Loss: 0.0157  lr:0.000100
[ Tue Jul 16 00:20:39 2024 ] 
Training: Epoch [78/150], Step [3499], Loss: 0.23136748373508453, Training Accuracy: 95.26071428571429
[ Tue Jul 16 00:20:39 2024 ] 	Batch(3500/6809) done. Loss: 0.2714  lr:0.000100
[ Tue Jul 16 00:21:02 2024 ] 	Batch(3600/6809) done. Loss: 0.3327  lr:0.000100
[ Tue Jul 16 00:21:25 2024 ] 	Batch(3700/6809) done. Loss: 0.1247  lr:0.000100
[ Tue Jul 16 00:21:48 2024 ] 	Batch(3800/6809) done. Loss: 0.0231  lr:0.000100
[ Tue Jul 16 00:22:11 2024 ] 	Batch(3900/6809) done. Loss: 0.0388  lr:0.000100
[ Tue Jul 16 00:22:33 2024 ] 
Training: Epoch [78/150], Step [3999], Loss: 0.009732687845826149, Training Accuracy: 95.20625
[ Tue Jul 16 00:22:33 2024 ] 	Batch(4000/6809) done. Loss: 0.1586  lr:0.000100
[ Tue Jul 16 00:22:56 2024 ] 	Batch(4100/6809) done. Loss: 0.2493  lr:0.000100
[ Tue Jul 16 00:23:19 2024 ] 	Batch(4200/6809) done. Loss: 0.3524  lr:0.000100
[ Tue Jul 16 00:23:41 2024 ] 	Batch(4300/6809) done. Loss: 0.3618  lr:0.000100
[ Tue Jul 16 00:24:04 2024 ] 	Batch(4400/6809) done. Loss: 0.0581  lr:0.000100
[ Tue Jul 16 00:24:26 2024 ] 
Training: Epoch [78/150], Step [4499], Loss: 0.027744531631469727, Training Accuracy: 95.24166666666667
[ Tue Jul 16 00:24:27 2024 ] 	Batch(4500/6809) done. Loss: 0.0043  lr:0.000100
[ Tue Jul 16 00:24:49 2024 ] 	Batch(4600/6809) done. Loss: 0.0541  lr:0.000100
[ Tue Jul 16 00:25:13 2024 ] 	Batch(4700/6809) done. Loss: 0.2879  lr:0.000100
[ Tue Jul 16 00:25:36 2024 ] 	Batch(4800/6809) done. Loss: 0.1278  lr:0.000100
[ Tue Jul 16 00:25:59 2024 ] 	Batch(4900/6809) done. Loss: 0.1195  lr:0.000100
[ Tue Jul 16 00:26:21 2024 ] 
Training: Epoch [78/150], Step [4999], Loss: 0.261002779006958, Training Accuracy: 95.28999999999999
[ Tue Jul 16 00:26:22 2024 ] 	Batch(5000/6809) done. Loss: 0.1341  lr:0.000100
[ Tue Jul 16 00:26:44 2024 ] 	Batch(5100/6809) done. Loss: 0.0451  lr:0.000100
[ Tue Jul 16 00:27:07 2024 ] 	Batch(5200/6809) done. Loss: 0.0601  lr:0.000100
[ Tue Jul 16 00:27:30 2024 ] 	Batch(5300/6809) done. Loss: 0.4763  lr:0.000100
[ Tue Jul 16 00:27:52 2024 ] 	Batch(5400/6809) done. Loss: 0.0223  lr:0.000100
[ Tue Jul 16 00:28:15 2024 ] 
Training: Epoch [78/150], Step [5499], Loss: 0.08177661895751953, Training Accuracy: 95.29545454545455
[ Tue Jul 16 00:28:15 2024 ] 	Batch(5500/6809) done. Loss: 0.0269  lr:0.000100
[ Tue Jul 16 00:28:38 2024 ] 	Batch(5600/6809) done. Loss: 0.0810  lr:0.000100
[ Tue Jul 16 00:29:00 2024 ] 	Batch(5700/6809) done. Loss: 0.2892  lr:0.000100
[ Tue Jul 16 00:29:23 2024 ] 	Batch(5800/6809) done. Loss: 0.1776  lr:0.000100
[ Tue Jul 16 00:29:45 2024 ] 	Batch(5900/6809) done. Loss: 0.4618  lr:0.000100
[ Tue Jul 16 00:30:08 2024 ] 
Training: Epoch [78/150], Step [5999], Loss: 0.20912092924118042, Training Accuracy: 95.29791666666667
[ Tue Jul 16 00:30:08 2024 ] 	Batch(6000/6809) done. Loss: 0.1579  lr:0.000100
[ Tue Jul 16 00:30:31 2024 ] 	Batch(6100/6809) done. Loss: 0.1069  lr:0.000100
[ Tue Jul 16 00:30:53 2024 ] 	Batch(6200/6809) done. Loss: 0.1493  lr:0.000100
[ Tue Jul 16 00:31:16 2024 ] 	Batch(6300/6809) done. Loss: 0.5534  lr:0.000100
[ Tue Jul 16 00:31:39 2024 ] 	Batch(6400/6809) done. Loss: 0.0093  lr:0.000100
[ Tue Jul 16 00:32:01 2024 ] 
Training: Epoch [78/150], Step [6499], Loss: 0.18281294405460358, Training Accuracy: 95.3173076923077
[ Tue Jul 16 00:32:01 2024 ] 	Batch(6500/6809) done. Loss: 0.0713  lr:0.000100
[ Tue Jul 16 00:32:24 2024 ] 	Batch(6600/6809) done. Loss: 0.5870  lr:0.000100
[ Tue Jul 16 00:32:47 2024 ] 	Batch(6700/6809) done. Loss: 0.4773  lr:0.000100
[ Tue Jul 16 00:33:09 2024 ] 	Batch(6800/6809) done. Loss: 0.0075  lr:0.000100
[ Tue Jul 16 00:33:11 2024 ] 	Mean training loss: 0.1696.
[ Tue Jul 16 00:33:11 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 00:33:11 2024 ] Training epoch: 80
[ Tue Jul 16 00:33:12 2024 ] 	Batch(0/6809) done. Loss: 0.1699  lr:0.000100
[ Tue Jul 16 00:33:35 2024 ] 	Batch(100/6809) done. Loss: 0.0036  lr:0.000100
[ Tue Jul 16 00:33:57 2024 ] 	Batch(200/6809) done. Loss: 0.1319  lr:0.000100
[ Tue Jul 16 00:34:20 2024 ] 	Batch(300/6809) done. Loss: 0.1839  lr:0.000100
[ Tue Jul 16 00:34:42 2024 ] 	Batch(400/6809) done. Loss: 0.0451  lr:0.000100
[ Tue Jul 16 00:35:05 2024 ] 
Training: Epoch [79/150], Step [499], Loss: 0.10664376616477966, Training Accuracy: 94.75
[ Tue Jul 16 00:35:05 2024 ] 	Batch(500/6809) done. Loss: 0.0193  lr:0.000100
[ Tue Jul 16 00:35:27 2024 ] 	Batch(600/6809) done. Loss: 0.0064  lr:0.000100
[ Tue Jul 16 00:35:50 2024 ] 	Batch(700/6809) done. Loss: 0.0756  lr:0.000100
[ Tue Jul 16 00:36:13 2024 ] 	Batch(800/6809) done. Loss: 0.0966  lr:0.000100
[ Tue Jul 16 00:36:36 2024 ] 	Batch(900/6809) done. Loss: 0.2098  lr:0.000100
[ Tue Jul 16 00:36:58 2024 ] 
Training: Epoch [79/150], Step [999], Loss: 0.016485663130879402, Training Accuracy: 95.0
[ Tue Jul 16 00:36:58 2024 ] 	Batch(1000/6809) done. Loss: 0.8946  lr:0.000100
[ Tue Jul 16 00:37:21 2024 ] 	Batch(1100/6809) done. Loss: 0.1988  lr:0.000100
[ Tue Jul 16 00:37:44 2024 ] 	Batch(1200/6809) done. Loss: 0.0137  lr:0.000100
[ Tue Jul 16 00:38:06 2024 ] 	Batch(1300/6809) done. Loss: 0.2809  lr:0.000100
[ Tue Jul 16 00:38:29 2024 ] 	Batch(1400/6809) done. Loss: 0.0433  lr:0.000100
[ Tue Jul 16 00:38:51 2024 ] 
Training: Epoch [79/150], Step [1499], Loss: 0.08200995624065399, Training Accuracy: 95.0
[ Tue Jul 16 00:38:52 2024 ] 	Batch(1500/6809) done. Loss: 0.5436  lr:0.000100
[ Tue Jul 16 00:39:15 2024 ] 	Batch(1600/6809) done. Loss: 0.1603  lr:0.000100
[ Tue Jul 16 00:39:38 2024 ] 	Batch(1700/6809) done. Loss: 0.4064  lr:0.000100
[ Tue Jul 16 00:40:01 2024 ] 	Batch(1800/6809) done. Loss: 0.0048  lr:0.000100
[ Tue Jul 16 00:40:24 2024 ] 	Batch(1900/6809) done. Loss: 0.0865  lr:0.000100
[ Tue Jul 16 00:40:47 2024 ] 
Training: Epoch [79/150], Step [1999], Loss: 0.20688371360301971, Training Accuracy: 95.08125
[ Tue Jul 16 00:40:47 2024 ] 	Batch(2000/6809) done. Loss: 0.0559  lr:0.000100
[ Tue Jul 16 00:41:10 2024 ] 	Batch(2100/6809) done. Loss: 0.4034  lr:0.000100
[ Tue Jul 16 00:41:33 2024 ] 	Batch(2200/6809) done. Loss: 0.0632  lr:0.000100
[ Tue Jul 16 00:41:55 2024 ] 	Batch(2300/6809) done. Loss: 0.1046  lr:0.000100
[ Tue Jul 16 00:42:18 2024 ] 	Batch(2400/6809) done. Loss: 0.1617  lr:0.000100
[ Tue Jul 16 00:42:41 2024 ] 
Training: Epoch [79/150], Step [2499], Loss: 0.14195415377616882, Training Accuracy: 94.93
[ Tue Jul 16 00:42:41 2024 ] 	Batch(2500/6809) done. Loss: 0.0861  lr:0.000100
[ Tue Jul 16 00:43:03 2024 ] 	Batch(2600/6809) done. Loss: 0.3530  lr:0.000100
[ Tue Jul 16 00:43:26 2024 ] 	Batch(2700/6809) done. Loss: 0.3935  lr:0.000100
[ Tue Jul 16 00:43:49 2024 ] 	Batch(2800/6809) done. Loss: 0.0516  lr:0.000100
[ Tue Jul 16 00:44:12 2024 ] 	Batch(2900/6809) done. Loss: 0.0632  lr:0.000100
[ Tue Jul 16 00:44:34 2024 ] 
Training: Epoch [79/150], Step [2999], Loss: 0.7975183725357056, Training Accuracy: 95.0375
[ Tue Jul 16 00:44:34 2024 ] 	Batch(3000/6809) done. Loss: 0.0438  lr:0.000100
[ Tue Jul 16 00:44:57 2024 ] 	Batch(3100/6809) done. Loss: 0.2042  lr:0.000100
[ Tue Jul 16 00:45:20 2024 ] 	Batch(3200/6809) done. Loss: 0.2785  lr:0.000100
[ Tue Jul 16 00:45:43 2024 ] 	Batch(3300/6809) done. Loss: 0.3065  lr:0.000100
[ Tue Jul 16 00:46:05 2024 ] 	Batch(3400/6809) done. Loss: 0.3316  lr:0.000100
[ Tue Jul 16 00:46:28 2024 ] 
Training: Epoch [79/150], Step [3499], Loss: 0.1984034776687622, Training Accuracy: 94.99642857142857
[ Tue Jul 16 00:46:28 2024 ] 	Batch(3500/6809) done. Loss: 0.0685  lr:0.000100
[ Tue Jul 16 00:46:51 2024 ] 	Batch(3600/6809) done. Loss: 0.1829  lr:0.000100
[ Tue Jul 16 00:47:13 2024 ] 	Batch(3700/6809) done. Loss: 0.0980  lr:0.000100
[ Tue Jul 16 00:47:36 2024 ] 	Batch(3800/6809) done. Loss: 0.1594  lr:0.000100
[ Tue Jul 16 00:47:59 2024 ] 	Batch(3900/6809) done. Loss: 0.0931  lr:0.000100
[ Tue Jul 16 00:48:21 2024 ] 
Training: Epoch [79/150], Step [3999], Loss: 0.5192903876304626, Training Accuracy: 95.05624999999999
[ Tue Jul 16 00:48:22 2024 ] 	Batch(4000/6809) done. Loss: 0.0877  lr:0.000100
[ Tue Jul 16 00:48:44 2024 ] 	Batch(4100/6809) done. Loss: 0.0278  lr:0.000100
[ Tue Jul 16 00:49:07 2024 ] 	Batch(4200/6809) done. Loss: 0.2589  lr:0.000100
[ Tue Jul 16 00:49:30 2024 ] 	Batch(4300/6809) done. Loss: 0.6842  lr:0.000100
[ Tue Jul 16 00:49:52 2024 ] 	Batch(4400/6809) done. Loss: 0.0218  lr:0.000100
[ Tue Jul 16 00:50:15 2024 ] 
Training: Epoch [79/150], Step [4499], Loss: 0.014926266856491566, Training Accuracy: 95.07777777777778
[ Tue Jul 16 00:50:15 2024 ] 	Batch(4500/6809) done. Loss: 0.1507  lr:0.000100
[ Tue Jul 16 00:50:38 2024 ] 	Batch(4600/6809) done. Loss: 0.9705  lr:0.000100
[ Tue Jul 16 00:51:00 2024 ] 	Batch(4700/6809) done. Loss: 0.0172  lr:0.000100
[ Tue Jul 16 00:51:23 2024 ] 	Batch(4800/6809) done. Loss: 0.0784  lr:0.000100
[ Tue Jul 16 00:51:46 2024 ] 	Batch(4900/6809) done. Loss: 0.5020  lr:0.000100
[ Tue Jul 16 00:52:09 2024 ] 
Training: Epoch [79/150], Step [4999], Loss: 0.0701555460691452, Training Accuracy: 95.07
[ Tue Jul 16 00:52:09 2024 ] 	Batch(5000/6809) done. Loss: 0.0026  lr:0.000100
[ Tue Jul 16 00:52:31 2024 ] 	Batch(5100/6809) done. Loss: 0.1464  lr:0.000100
[ Tue Jul 16 00:52:54 2024 ] 	Batch(5200/6809) done. Loss: 0.0170  lr:0.000100
[ Tue Jul 16 00:53:17 2024 ] 	Batch(5300/6809) done. Loss: 0.0910  lr:0.000100
[ Tue Jul 16 00:53:40 2024 ] 	Batch(5400/6809) done. Loss: 0.0393  lr:0.000100
[ Tue Jul 16 00:54:03 2024 ] 
Training: Epoch [79/150], Step [5499], Loss: 0.22174610197544098, Training Accuracy: 95.07272727272728
[ Tue Jul 16 00:54:03 2024 ] 	Batch(5500/6809) done. Loss: 0.8465  lr:0.000100
[ Tue Jul 16 00:54:26 2024 ] 	Batch(5600/6809) done. Loss: 0.0539  lr:0.000100
[ Tue Jul 16 00:54:48 2024 ] 	Batch(5700/6809) done. Loss: 0.0625  lr:0.000100
[ Tue Jul 16 00:55:11 2024 ] 	Batch(5800/6809) done. Loss: 0.0493  lr:0.000100
[ Tue Jul 16 00:55:34 2024 ] 	Batch(5900/6809) done. Loss: 0.0236  lr:0.000100
[ Tue Jul 16 00:55:56 2024 ] 
Training: Epoch [79/150], Step [5999], Loss: 0.21318532526493073, Training Accuracy: 95.07291666666666
[ Tue Jul 16 00:55:56 2024 ] 	Batch(6000/6809) done. Loss: 0.2477  lr:0.000100
[ Tue Jul 16 00:56:19 2024 ] 	Batch(6100/6809) done. Loss: 0.0835  lr:0.000100
[ Tue Jul 16 00:56:42 2024 ] 	Batch(6200/6809) done. Loss: 0.4132  lr:0.000100
[ Tue Jul 16 00:57:04 2024 ] 	Batch(6300/6809) done. Loss: 0.0574  lr:0.000100
[ Tue Jul 16 00:57:27 2024 ] 	Batch(6400/6809) done. Loss: 0.0255  lr:0.000100
[ Tue Jul 16 00:57:50 2024 ] 
Training: Epoch [79/150], Step [6499], Loss: 0.20403078198432922, Training Accuracy: 95.11538461538461
[ Tue Jul 16 00:57:50 2024 ] 	Batch(6500/6809) done. Loss: 0.0173  lr:0.000100
[ Tue Jul 16 00:58:13 2024 ] 	Batch(6600/6809) done. Loss: 0.1601  lr:0.000100
[ Tue Jul 16 00:58:36 2024 ] 	Batch(6700/6809) done. Loss: 0.0093  lr:0.000100
[ Tue Jul 16 00:58:59 2024 ] 	Batch(6800/6809) done. Loss: 0.2926  lr:0.000100
[ Tue Jul 16 00:59:01 2024 ] 	Mean training loss: 0.1681.
[ Tue Jul 16 00:59:01 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 00:59:01 2024 ] Eval epoch: 80
[ Tue Jul 16 01:05:56 2024 ] 	Mean val loss of 7435 batches: 0.8852047804014213.
[ Tue Jul 16 01:05:56 2024 ] 
Validation: Epoch [79/150], Samples [47592.0/59477], Loss: 2.619351387023926, Validation Accuracy: 80.01748575079442
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 1 : 386 / 500 = 77 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 2 : 424 / 499 = 84 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 3 : 416 / 500 = 83 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 4 : 426 / 502 = 84 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 5 : 435 / 502 = 86 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 6 : 421 / 502 = 83 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 7 : 460 / 497 = 92 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 8 : 486 / 498 = 97 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 9 : 372 / 500 = 74 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 10 : 313 / 500 = 62 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 11 : 207 / 498 = 41 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 12 : 404 / 499 = 80 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 13 : 488 / 502 = 97 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 14 : 482 / 504 = 95 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 15 : 373 / 502 = 74 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 16 : 365 / 502 = 72 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 17 : 429 / 504 = 85 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 18 : 417 / 504 = 82 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 19 : 433 / 502 = 86 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 20 : 460 / 502 = 91 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 21 : 472 / 503 = 93 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 22 : 431 / 504 = 85 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 23 : 431 / 503 = 85 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 24 : 433 / 504 = 85 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 25 : 481 / 504 = 95 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 26 : 473 / 504 = 93 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 27 : 430 / 501 = 85 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 28 : 343 / 502 = 68 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 29 : 317 / 502 = 63 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 30 : 348 / 501 = 69 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 31 : 437 / 504 = 86 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 32 : 439 / 503 = 87 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 33 : 408 / 503 = 81 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 34 : 487 / 504 = 96 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 35 : 459 / 503 = 91 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 36 : 386 / 502 = 76 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 37 : 453 / 504 = 89 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 38 : 447 / 504 = 88 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 39 : 445 / 498 = 89 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 40 : 392 / 504 = 77 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 41 : 473 / 503 = 94 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 42 : 458 / 504 = 90 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 43 : 367 / 503 = 72 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 44 : 435 / 504 = 86 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 45 : 410 / 504 = 81 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 46 : 397 / 504 = 78 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 47 : 373 / 503 = 74 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 48 : 416 / 503 = 82 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 49 : 391 / 499 = 78 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 50 : 414 / 502 = 82 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 51 : 462 / 503 = 91 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 52 : 459 / 504 = 91 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 53 : 446 / 497 = 89 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 54 : 443 / 480 = 92 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 55 : 434 / 504 = 86 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 56 : 428 / 503 = 85 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 57 : 472 / 504 = 93 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 58 : 481 / 499 = 96 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 59 : 484 / 503 = 96 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 60 : 419 / 479 = 87 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 61 : 415 / 484 = 85 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 62 : 402 / 487 = 82 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 63 : 439 / 489 = 89 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 64 : 400 / 488 = 81 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 65 : 424 / 490 = 86 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 66 : 308 / 488 = 63 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 67 : 353 / 490 = 72 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 68 : 297 / 490 = 60 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 69 : 393 / 490 = 80 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 70 : 179 / 490 = 36 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 71 : 285 / 490 = 58 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 72 : 199 / 488 = 40 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 73 : 241 / 486 = 49 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 74 : 289 / 481 = 60 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 75 : 289 / 488 = 59 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 76 : 318 / 489 = 65 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 77 : 308 / 488 = 63 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 78 : 380 / 488 = 77 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 79 : 463 / 490 = 94 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 80 : 389 / 489 = 79 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 81 : 282 / 491 = 57 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 82 : 317 / 491 = 64 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 83 : 269 / 489 = 55 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 84 : 353 / 489 = 72 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 85 : 366 / 489 = 74 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 86 : 411 / 491 = 83 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 87 : 409 / 492 = 83 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 88 : 351 / 491 = 71 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 89 : 379 / 492 = 77 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 90 : 301 / 490 = 61 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 91 : 365 / 482 = 75 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 92 : 359 / 490 = 73 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 93 : 350 / 487 = 71 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 94 : 441 / 489 = 90 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 95 : 414 / 490 = 84 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 96 : 460 / 491 = 93 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 97 : 457 / 490 = 93 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 98 : 441 / 491 = 89 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 99 : 447 / 491 = 91 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 100 : 445 / 491 = 90 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 101 : 415 / 491 = 84 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 102 : 270 / 492 = 54 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 103 : 390 / 492 = 79 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 104 : 303 / 491 = 61 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 105 : 294 / 491 = 59 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 106 : 314 / 492 = 63 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 107 : 423 / 491 = 86 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 108 : 373 / 492 = 75 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 109 : 358 / 490 = 73 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 110 : 416 / 491 = 84 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 111 : 449 / 492 = 91 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 112 : 459 / 492 = 93 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 113 : 442 / 491 = 90 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 114 : 394 / 491 = 80 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 115 : 425 / 492 = 86 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 116 : 417 / 491 = 84 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 117 : 410 / 492 = 83 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 118 : 430 / 490 = 87 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 119 : 452 / 492 = 91 %
[ Tue Jul 16 01:05:56 2024 ] Accuracy of 120 : 399 / 500 = 79 %
[ Tue Jul 16 01:05:56 2024 ] Training epoch: 81
[ Tue Jul 16 01:05:57 2024 ] 	Batch(0/6809) done. Loss: 0.3773  lr:0.000100
[ Tue Jul 16 01:06:20 2024 ] 	Batch(100/6809) done. Loss: 0.5771  lr:0.000100
[ Tue Jul 16 01:06:43 2024 ] 	Batch(200/6809) done. Loss: 0.3045  lr:0.000100
[ Tue Jul 16 01:07:06 2024 ] 	Batch(300/6809) done. Loss: 0.1266  lr:0.000100
[ Tue Jul 16 01:07:29 2024 ] 	Batch(400/6809) done. Loss: 0.0322  lr:0.000100
[ Tue Jul 16 01:07:51 2024 ] 
Training: Epoch [80/150], Step [499], Loss: 0.552636444568634, Training Accuracy: 94.77499999999999
[ Tue Jul 16 01:07:52 2024 ] 	Batch(500/6809) done. Loss: 0.1765  lr:0.000100
[ Tue Jul 16 01:08:14 2024 ] 	Batch(600/6809) done. Loss: 0.3293  lr:0.000100
[ Tue Jul 16 01:08:37 2024 ] 	Batch(700/6809) done. Loss: 0.1435  lr:0.000100
[ Tue Jul 16 01:09:00 2024 ] 	Batch(800/6809) done. Loss: 0.3396  lr:0.000100
[ Tue Jul 16 01:09:23 2024 ] 	Batch(900/6809) done. Loss: 0.0375  lr:0.000100
[ Tue Jul 16 01:09:45 2024 ] 
Training: Epoch [80/150], Step [999], Loss: 0.3075733482837677, Training Accuracy: 94.825
[ Tue Jul 16 01:09:46 2024 ] 	Batch(1000/6809) done. Loss: 0.0270  lr:0.000100
[ Tue Jul 16 01:10:08 2024 ] 	Batch(1100/6809) done. Loss: 0.1817  lr:0.000100
[ Tue Jul 16 01:10:31 2024 ] 	Batch(1200/6809) done. Loss: 0.2583  lr:0.000100
[ Tue Jul 16 01:10:53 2024 ] 	Batch(1300/6809) done. Loss: 0.0898  lr:0.000100
[ Tue Jul 16 01:11:16 2024 ] 	Batch(1400/6809) done. Loss: 0.1530  lr:0.000100
[ Tue Jul 16 01:11:39 2024 ] 
Training: Epoch [80/150], Step [1499], Loss: 0.01726953499019146, Training Accuracy: 94.925
[ Tue Jul 16 01:11:39 2024 ] 	Batch(1500/6809) done. Loss: 0.0381  lr:0.000100
[ Tue Jul 16 01:12:02 2024 ] 	Batch(1600/6809) done. Loss: 0.1397  lr:0.000100
[ Tue Jul 16 01:12:24 2024 ] 	Batch(1700/6809) done. Loss: 0.2619  lr:0.000100
[ Tue Jul 16 01:12:47 2024 ] 	Batch(1800/6809) done. Loss: 0.3960  lr:0.000100
[ Tue Jul 16 01:13:09 2024 ] 	Batch(1900/6809) done. Loss: 0.0243  lr:0.000100
[ Tue Jul 16 01:13:32 2024 ] 
Training: Epoch [80/150], Step [1999], Loss: 0.7786240577697754, Training Accuracy: 95.01875
[ Tue Jul 16 01:13:32 2024 ] 	Batch(2000/6809) done. Loss: 0.2029  lr:0.000100
[ Tue Jul 16 01:13:55 2024 ] 	Batch(2100/6809) done. Loss: 0.0186  lr:0.000100
[ Tue Jul 16 01:14:17 2024 ] 	Batch(2200/6809) done. Loss: 0.0865  lr:0.000100
[ Tue Jul 16 01:14:40 2024 ] 	Batch(2300/6809) done. Loss: 0.4282  lr:0.000100
[ Tue Jul 16 01:15:03 2024 ] 	Batch(2400/6809) done. Loss: 0.0432  lr:0.000100
[ Tue Jul 16 01:15:26 2024 ] 
Training: Epoch [80/150], Step [2499], Loss: 0.3856644332408905, Training Accuracy: 95.02000000000001
[ Tue Jul 16 01:15:26 2024 ] 	Batch(2500/6809) done. Loss: 0.3046  lr:0.000100
[ Tue Jul 16 01:15:49 2024 ] 	Batch(2600/6809) done. Loss: 0.0476  lr:0.000100
[ Tue Jul 16 01:16:11 2024 ] 	Batch(2700/6809) done. Loss: 0.0328  lr:0.000100
[ Tue Jul 16 01:16:35 2024 ] 	Batch(2800/6809) done. Loss: 0.0840  lr:0.000100
[ Tue Jul 16 01:16:57 2024 ] 	Batch(2900/6809) done. Loss: 0.1860  lr:0.000100
[ Tue Jul 16 01:17:20 2024 ] 
Training: Epoch [80/150], Step [2999], Loss: 0.10906508564949036, Training Accuracy: 95.10833333333333
[ Tue Jul 16 01:17:20 2024 ] 	Batch(3000/6809) done. Loss: 0.0451  lr:0.000100
[ Tue Jul 16 01:17:43 2024 ] 	Batch(3100/6809) done. Loss: 0.1821  lr:0.000100
[ Tue Jul 16 01:18:05 2024 ] 	Batch(3200/6809) done. Loss: 0.3482  lr:0.000100
[ Tue Jul 16 01:18:28 2024 ] 	Batch(3300/6809) done. Loss: 0.2859  lr:0.000100
[ Tue Jul 16 01:18:51 2024 ] 	Batch(3400/6809) done. Loss: 0.2123  lr:0.000100
[ Tue Jul 16 01:19:13 2024 ] 
Training: Epoch [80/150], Step [3499], Loss: 0.08533121645450592, Training Accuracy: 95.175
[ Tue Jul 16 01:19:13 2024 ] 	Batch(3500/6809) done. Loss: 0.0580  lr:0.000100
[ Tue Jul 16 01:19:36 2024 ] 	Batch(3600/6809) done. Loss: 0.2349  lr:0.000100
[ Tue Jul 16 01:19:59 2024 ] 	Batch(3700/6809) done. Loss: 0.2643  lr:0.000100
[ Tue Jul 16 01:20:22 2024 ] 	Batch(3800/6809) done. Loss: 0.0222  lr:0.000100
[ Tue Jul 16 01:20:45 2024 ] 	Batch(3900/6809) done. Loss: 0.3464  lr:0.000100
[ Tue Jul 16 01:21:08 2024 ] 
Training: Epoch [80/150], Step [3999], Loss: 0.03988939896225929, Training Accuracy: 95.203125
[ Tue Jul 16 01:21:08 2024 ] 	Batch(4000/6809) done. Loss: 0.1066  lr:0.000100
[ Tue Jul 16 01:21:31 2024 ] 	Batch(4100/6809) done. Loss: 0.2248  lr:0.000100
[ Tue Jul 16 01:21:54 2024 ] 	Batch(4200/6809) done. Loss: 0.5303  lr:0.000100
[ Tue Jul 16 01:22:17 2024 ] 	Batch(4300/6809) done. Loss: 0.1365  lr:0.000100
[ Tue Jul 16 01:22:40 2024 ] 	Batch(4400/6809) done. Loss: 0.2978  lr:0.000100
[ Tue Jul 16 01:23:03 2024 ] 
Training: Epoch [80/150], Step [4499], Loss: 0.31061434745788574, Training Accuracy: 95.13055555555555
[ Tue Jul 16 01:23:04 2024 ] 	Batch(4500/6809) done. Loss: 0.4044  lr:0.000100
[ Tue Jul 16 01:23:27 2024 ] 	Batch(4600/6809) done. Loss: 0.2953  lr:0.000100
[ Tue Jul 16 01:23:50 2024 ] 	Batch(4700/6809) done. Loss: 0.0977  lr:0.000100
[ Tue Jul 16 01:24:13 2024 ] 	Batch(4800/6809) done. Loss: 0.4168  lr:0.000100
[ Tue Jul 16 01:24:36 2024 ] 	Batch(4900/6809) done. Loss: 0.0705  lr:0.000100
[ Tue Jul 16 01:24:59 2024 ] 
Training: Epoch [80/150], Step [4999], Loss: 0.26614147424697876, Training Accuracy: 95.17
[ Tue Jul 16 01:24:59 2024 ] 	Batch(5000/6809) done. Loss: 0.1404  lr:0.000100
[ Tue Jul 16 01:25:22 2024 ] 	Batch(5100/6809) done. Loss: 0.5457  lr:0.000100
[ Tue Jul 16 01:25:46 2024 ] 	Batch(5200/6809) done. Loss: 0.0110  lr:0.000100
[ Tue Jul 16 01:26:09 2024 ] 	Batch(5300/6809) done. Loss: 0.0163  lr:0.000100
[ Tue Jul 16 01:26:32 2024 ] 	Batch(5400/6809) done. Loss: 0.0517  lr:0.000100
[ Tue Jul 16 01:26:55 2024 ] 
Training: Epoch [80/150], Step [5499], Loss: 0.06415695697069168, Training Accuracy: 95.17045454545455
[ Tue Jul 16 01:26:55 2024 ] 	Batch(5500/6809) done. Loss: 0.1325  lr:0.000100
[ Tue Jul 16 01:27:18 2024 ] 	Batch(5600/6809) done. Loss: 0.1153  lr:0.000100
[ Tue Jul 16 01:27:41 2024 ] 	Batch(5700/6809) done. Loss: 0.2011  lr:0.000100
[ Tue Jul 16 01:28:04 2024 ] 	Batch(5800/6809) done. Loss: 0.2317  lr:0.000100
[ Tue Jul 16 01:28:27 2024 ] 	Batch(5900/6809) done. Loss: 0.0718  lr:0.000100
[ Tue Jul 16 01:28:50 2024 ] 
Training: Epoch [80/150], Step [5999], Loss: 0.0731847882270813, Training Accuracy: 95.17291666666667
[ Tue Jul 16 01:28:51 2024 ] 	Batch(6000/6809) done. Loss: 0.1065  lr:0.000100
[ Tue Jul 16 01:29:14 2024 ] 	Batch(6100/6809) done. Loss: 0.0087  lr:0.000100
[ Tue Jul 16 01:29:37 2024 ] 	Batch(6200/6809) done. Loss: 0.1495  lr:0.000100
[ Tue Jul 16 01:30:00 2024 ] 	Batch(6300/6809) done. Loss: 0.1201  lr:0.000100
[ Tue Jul 16 01:30:23 2024 ] 	Batch(6400/6809) done. Loss: 0.2612  lr:0.000100
[ Tue Jul 16 01:30:46 2024 ] 
Training: Epoch [80/150], Step [6499], Loss: 0.11578896641731262, Training Accuracy: 95.16730769230769
[ Tue Jul 16 01:30:46 2024 ] 	Batch(6500/6809) done. Loss: 0.2184  lr:0.000100
[ Tue Jul 16 01:31:09 2024 ] 	Batch(6600/6809) done. Loss: 0.2421  lr:0.000100
[ Tue Jul 16 01:31:32 2024 ] 	Batch(6700/6809) done. Loss: 0.1527  lr:0.000100
[ Tue Jul 16 01:31:55 2024 ] 	Batch(6800/6809) done. Loss: 0.0018  lr:0.000100
[ Tue Jul 16 01:31:57 2024 ] 	Mean training loss: 0.1663.
[ Tue Jul 16 01:31:57 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 01:31:57 2024 ] Training epoch: 82
[ Tue Jul 16 01:31:58 2024 ] 	Batch(0/6809) done. Loss: 0.0436  lr:0.000100
[ Tue Jul 16 01:32:21 2024 ] 	Batch(100/6809) done. Loss: 0.0068  lr:0.000100
[ Tue Jul 16 01:32:44 2024 ] 	Batch(200/6809) done. Loss: 0.0849  lr:0.000100
[ Tue Jul 16 01:33:07 2024 ] 	Batch(300/6809) done. Loss: 0.0157  lr:0.000100
[ Tue Jul 16 01:33:30 2024 ] 	Batch(400/6809) done. Loss: 0.8649  lr:0.000100
[ Tue Jul 16 01:33:52 2024 ] 
Training: Epoch [81/150], Step [499], Loss: 0.07604788988828659, Training Accuracy: 95.1
[ Tue Jul 16 01:33:53 2024 ] 	Batch(500/6809) done. Loss: 0.3540  lr:0.000100
[ Tue Jul 16 01:34:16 2024 ] 	Batch(600/6809) done. Loss: 0.5114  lr:0.000100
[ Tue Jul 16 01:34:39 2024 ] 	Batch(700/6809) done. Loss: 0.1940  lr:0.000100
[ Tue Jul 16 01:35:02 2024 ] 	Batch(800/6809) done. Loss: 0.0936  lr:0.000100
[ Tue Jul 16 01:35:25 2024 ] 	Batch(900/6809) done. Loss: 0.0842  lr:0.000100
[ Tue Jul 16 01:35:47 2024 ] 
Training: Epoch [81/150], Step [999], Loss: 0.2765406668186188, Training Accuracy: 95.175
[ Tue Jul 16 01:35:47 2024 ] 	Batch(1000/6809) done. Loss: 0.1016  lr:0.000100
[ Tue Jul 16 01:36:10 2024 ] 	Batch(1100/6809) done. Loss: 0.0791  lr:0.000100
[ Tue Jul 16 01:36:33 2024 ] 	Batch(1200/6809) done. Loss: 0.1123  lr:0.000100
[ Tue Jul 16 01:36:56 2024 ] 	Batch(1300/6809) done. Loss: 0.0602  lr:0.000100
[ Tue Jul 16 01:37:18 2024 ] 	Batch(1400/6809) done. Loss: 0.0881  lr:0.000100
[ Tue Jul 16 01:37:41 2024 ] 
Training: Epoch [81/150], Step [1499], Loss: 0.16699430346488953, Training Accuracy: 95.025
[ Tue Jul 16 01:37:41 2024 ] 	Batch(1500/6809) done. Loss: 0.5635  lr:0.000100
[ Tue Jul 16 01:38:04 2024 ] 	Batch(1600/6809) done. Loss: 0.0817  lr:0.000100
[ Tue Jul 16 01:38:26 2024 ] 	Batch(1700/6809) done. Loss: 0.0178  lr:0.000100
[ Tue Jul 16 01:38:49 2024 ] 	Batch(1800/6809) done. Loss: 0.2668  lr:0.000100
[ Tue Jul 16 01:39:12 2024 ] 	Batch(1900/6809) done. Loss: 0.1827  lr:0.000100
[ Tue Jul 16 01:39:34 2024 ] 
Training: Epoch [81/150], Step [1999], Loss: 0.23478101193904877, Training Accuracy: 95.10625
[ Tue Jul 16 01:39:34 2024 ] 	Batch(2000/6809) done. Loss: 0.1318  lr:0.000100
[ Tue Jul 16 01:39:57 2024 ] 	Batch(2100/6809) done. Loss: 0.1980  lr:0.000100
[ Tue Jul 16 01:40:20 2024 ] 	Batch(2200/6809) done. Loss: 0.5446  lr:0.000100
[ Tue Jul 16 01:40:43 2024 ] 	Batch(2300/6809) done. Loss: 0.0147  lr:0.000100
[ Tue Jul 16 01:41:06 2024 ] 	Batch(2400/6809) done. Loss: 0.0118  lr:0.000100
[ Tue Jul 16 01:41:29 2024 ] 
Training: Epoch [81/150], Step [2499], Loss: 0.6183019876480103, Training Accuracy: 95.165
[ Tue Jul 16 01:41:29 2024 ] 	Batch(2500/6809) done. Loss: 0.1717  lr:0.000100
[ Tue Jul 16 01:41:52 2024 ] 	Batch(2600/6809) done. Loss: 0.0495  lr:0.000100
[ Tue Jul 16 01:42:15 2024 ] 	Batch(2700/6809) done. Loss: 0.0709  lr:0.000100
[ Tue Jul 16 01:42:38 2024 ] 	Batch(2800/6809) done. Loss: 0.1745  lr:0.000100
[ Tue Jul 16 01:43:01 2024 ] 	Batch(2900/6809) done. Loss: 0.0199  lr:0.000100
[ Tue Jul 16 01:43:24 2024 ] 
Training: Epoch [81/150], Step [2999], Loss: 0.14304231107234955, Training Accuracy: 95.20416666666667
[ Tue Jul 16 01:43:25 2024 ] 	Batch(3000/6809) done. Loss: 0.0127  lr:0.000100
[ Tue Jul 16 01:43:48 2024 ] 	Batch(3100/6809) done. Loss: 0.1568  lr:0.000100
[ Tue Jul 16 01:44:11 2024 ] 	Batch(3200/6809) done. Loss: 0.0375  lr:0.000100
[ Tue Jul 16 01:44:33 2024 ] 	Batch(3300/6809) done. Loss: 0.0701  lr:0.000100
[ Tue Jul 16 01:44:56 2024 ] 	Batch(3400/6809) done. Loss: 0.0369  lr:0.000100
[ Tue Jul 16 01:45:19 2024 ] 
Training: Epoch [81/150], Step [3499], Loss: 0.2694944143295288, Training Accuracy: 95.22500000000001
[ Tue Jul 16 01:45:19 2024 ] 	Batch(3500/6809) done. Loss: 0.2311  lr:0.000100
[ Tue Jul 16 01:45:42 2024 ] 	Batch(3600/6809) done. Loss: 0.1407  lr:0.000100
[ Tue Jul 16 01:46:04 2024 ] 	Batch(3700/6809) done. Loss: 0.0076  lr:0.000100
[ Tue Jul 16 01:46:27 2024 ] 	Batch(3800/6809) done. Loss: 0.0706  lr:0.000100
[ Tue Jul 16 01:46:50 2024 ] 	Batch(3900/6809) done. Loss: 0.2849  lr:0.000100
[ Tue Jul 16 01:47:12 2024 ] 
Training: Epoch [81/150], Step [3999], Loss: 0.3200131356716156, Training Accuracy: 95.17812500000001
[ Tue Jul 16 01:47:13 2024 ] 	Batch(4000/6809) done. Loss: 0.1046  lr:0.000100
[ Tue Jul 16 01:47:35 2024 ] 	Batch(4100/6809) done. Loss: 0.6987  lr:0.000100
[ Tue Jul 16 01:47:59 2024 ] 	Batch(4200/6809) done. Loss: 0.4151  lr:0.000100
[ Tue Jul 16 01:48:21 2024 ] 	Batch(4300/6809) done. Loss: 0.2252  lr:0.000100
[ Tue Jul 16 01:48:44 2024 ] 	Batch(4400/6809) done. Loss: 0.1132  lr:0.000100
[ Tue Jul 16 01:49:07 2024 ] 
Training: Epoch [81/150], Step [4499], Loss: 0.069514699280262, Training Accuracy: 95.18888888888888
[ Tue Jul 16 01:49:07 2024 ] 	Batch(4500/6809) done. Loss: 0.0480  lr:0.000100
[ Tue Jul 16 01:49:31 2024 ] 	Batch(4600/6809) done. Loss: 0.1797  lr:0.000100
[ Tue Jul 16 01:49:53 2024 ] 	Batch(4700/6809) done. Loss: 0.0764  lr:0.000100
[ Tue Jul 16 01:50:17 2024 ] 	Batch(4800/6809) done. Loss: 0.2556  lr:0.000100
[ Tue Jul 16 01:50:40 2024 ] 	Batch(4900/6809) done. Loss: 0.0957  lr:0.000100
[ Tue Jul 16 01:51:02 2024 ] 
Training: Epoch [81/150], Step [4999], Loss: 0.03339496999979019, Training Accuracy: 95.22500000000001
[ Tue Jul 16 01:51:03 2024 ] 	Batch(5000/6809) done. Loss: 0.6024  lr:0.000100
[ Tue Jul 16 01:51:25 2024 ] 	Batch(5100/6809) done. Loss: 0.0569  lr:0.000100
[ Tue Jul 16 01:51:48 2024 ] 	Batch(5200/6809) done. Loss: 0.0931  lr:0.000100
[ Tue Jul 16 01:52:11 2024 ] 	Batch(5300/6809) done. Loss: 0.0945  lr:0.000100
[ Tue Jul 16 01:52:33 2024 ] 	Batch(5400/6809) done. Loss: 0.0513  lr:0.000100
[ Tue Jul 16 01:52:56 2024 ] 
Training: Epoch [81/150], Step [5499], Loss: 0.18597055971622467, Training Accuracy: 95.26818181818182
[ Tue Jul 16 01:52:56 2024 ] 	Batch(5500/6809) done. Loss: 0.0101  lr:0.000100
[ Tue Jul 16 01:53:19 2024 ] 	Batch(5600/6809) done. Loss: 0.5612  lr:0.000100
[ Tue Jul 16 01:53:41 2024 ] 	Batch(5700/6809) done. Loss: 0.1174  lr:0.000100
[ Tue Jul 16 01:54:04 2024 ] 	Batch(5800/6809) done. Loss: 0.0959  lr:0.000100
[ Tue Jul 16 01:54:27 2024 ] 	Batch(5900/6809) done. Loss: 0.0260  lr:0.000100
[ Tue Jul 16 01:54:49 2024 ] 
Training: Epoch [81/150], Step [5999], Loss: 0.030814718455076218, Training Accuracy: 95.31458333333333
[ Tue Jul 16 01:54:50 2024 ] 	Batch(6000/6809) done. Loss: 0.2306  lr:0.000100
[ Tue Jul 16 01:55:12 2024 ] 	Batch(6100/6809) done. Loss: 0.2525  lr:0.000100
[ Tue Jul 16 01:55:35 2024 ] 	Batch(6200/6809) done. Loss: 0.2942  lr:0.000100
[ Tue Jul 16 01:55:58 2024 ] 	Batch(6300/6809) done. Loss: 0.0557  lr:0.000100
[ Tue Jul 16 01:56:20 2024 ] 	Batch(6400/6809) done. Loss: 0.0786  lr:0.000100
[ Tue Jul 16 01:56:43 2024 ] 
Training: Epoch [81/150], Step [6499], Loss: 0.17943978309631348, Training Accuracy: 95.3
[ Tue Jul 16 01:56:43 2024 ] 	Batch(6500/6809) done. Loss: 0.5737  lr:0.000100
[ Tue Jul 16 01:57:06 2024 ] 	Batch(6600/6809) done. Loss: 0.1586  lr:0.000100
[ Tue Jul 16 01:57:28 2024 ] 	Batch(6700/6809) done. Loss: 0.0244  lr:0.000100
[ Tue Jul 16 01:57:51 2024 ] 	Batch(6800/6809) done. Loss: 0.4903  lr:0.000100
[ Tue Jul 16 01:57:53 2024 ] 	Mean training loss: 0.1648.
[ Tue Jul 16 01:57:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 01:57:54 2024 ] Training epoch: 83
[ Tue Jul 16 01:57:54 2024 ] 	Batch(0/6809) done. Loss: 0.0420  lr:0.000100
[ Tue Jul 16 01:58:17 2024 ] 	Batch(100/6809) done. Loss: 0.1973  lr:0.000100
[ Tue Jul 16 01:58:40 2024 ] 	Batch(200/6809) done. Loss: 0.0071  lr:0.000100
[ Tue Jul 16 01:59:03 2024 ] 	Batch(300/6809) done. Loss: 0.0965  lr:0.000100
[ Tue Jul 16 01:59:26 2024 ] 	Batch(400/6809) done. Loss: 0.1651  lr:0.000100
[ Tue Jul 16 01:59:49 2024 ] 
Training: Epoch [82/150], Step [499], Loss: 0.04847823455929756, Training Accuracy: 94.975
[ Tue Jul 16 01:59:49 2024 ] 	Batch(500/6809) done. Loss: 0.1442  lr:0.000100
[ Tue Jul 16 02:00:12 2024 ] 	Batch(600/6809) done. Loss: 0.0946  lr:0.000100
[ Tue Jul 16 02:00:34 2024 ] 	Batch(700/6809) done. Loss: 0.2900  lr:0.000100
[ Tue Jul 16 02:00:57 2024 ] 	Batch(800/6809) done. Loss: 0.0703  lr:0.000100
[ Tue Jul 16 02:01:20 2024 ] 	Batch(900/6809) done. Loss: 0.1981  lr:0.000100
[ Tue Jul 16 02:01:43 2024 ] 
Training: Epoch [82/150], Step [999], Loss: 0.14419198036193848, Training Accuracy: 95.2625
[ Tue Jul 16 02:01:43 2024 ] 	Batch(1000/6809) done. Loss: 0.1893  lr:0.000100
[ Tue Jul 16 02:02:06 2024 ] 	Batch(1100/6809) done. Loss: 0.3118  lr:0.000100
[ Tue Jul 16 02:02:29 2024 ] 	Batch(1200/6809) done. Loss: 0.0606  lr:0.000100
[ Tue Jul 16 02:02:52 2024 ] 	Batch(1300/6809) done. Loss: 0.2479  lr:0.000100
[ Tue Jul 16 02:03:14 2024 ] 	Batch(1400/6809) done. Loss: 0.0914  lr:0.000100
[ Tue Jul 16 02:03:37 2024 ] 
Training: Epoch [82/150], Step [1499], Loss: 0.13898804783821106, Training Accuracy: 95.34166666666667
[ Tue Jul 16 02:03:37 2024 ] 	Batch(1500/6809) done. Loss: 0.1323  lr:0.000100
[ Tue Jul 16 02:04:00 2024 ] 	Batch(1600/6809) done. Loss: 0.0925  lr:0.000100
[ Tue Jul 16 02:04:23 2024 ] 	Batch(1700/6809) done. Loss: 0.1168  lr:0.000100
[ Tue Jul 16 02:04:46 2024 ] 	Batch(1800/6809) done. Loss: 0.2773  lr:0.000100
[ Tue Jul 16 02:05:08 2024 ] 	Batch(1900/6809) done. Loss: 0.0691  lr:0.000100
[ Tue Jul 16 02:05:32 2024 ] 
Training: Epoch [82/150], Step [1999], Loss: 0.004836707375943661, Training Accuracy: 95.42500000000001
[ Tue Jul 16 02:05:32 2024 ] 	Batch(2000/6809) done. Loss: 0.0366  lr:0.000100
[ Tue Jul 16 02:05:56 2024 ] 	Batch(2100/6809) done. Loss: 0.1279  lr:0.000100
[ Tue Jul 16 02:06:19 2024 ] 	Batch(2200/6809) done. Loss: 0.0087  lr:0.000100
[ Tue Jul 16 02:06:41 2024 ] 	Batch(2300/6809) done. Loss: 0.0348  lr:0.000100
[ Tue Jul 16 02:07:04 2024 ] 	Batch(2400/6809) done. Loss: 0.3425  lr:0.000100
[ Tue Jul 16 02:07:26 2024 ] 
Training: Epoch [82/150], Step [2499], Loss: 0.14327137172222137, Training Accuracy: 95.47
[ Tue Jul 16 02:07:27 2024 ] 	Batch(2500/6809) done. Loss: 0.1036  lr:0.000100
[ Tue Jul 16 02:07:49 2024 ] 	Batch(2600/6809) done. Loss: 0.9736  lr:0.000100
[ Tue Jul 16 02:08:12 2024 ] 	Batch(2700/6809) done. Loss: 0.1192  lr:0.000100
[ Tue Jul 16 02:08:34 2024 ] 	Batch(2800/6809) done. Loss: 0.1108  lr:0.000100
[ Tue Jul 16 02:08:57 2024 ] 	Batch(2900/6809) done. Loss: 0.0371  lr:0.000100
[ Tue Jul 16 02:09:19 2024 ] 
Training: Epoch [82/150], Step [2999], Loss: 0.03203723952174187, Training Accuracy: 95.40833333333333
[ Tue Jul 16 02:09:20 2024 ] 	Batch(3000/6809) done. Loss: 0.0009  lr:0.000100
[ Tue Jul 16 02:09:43 2024 ] 	Batch(3100/6809) done. Loss: 0.7968  lr:0.000100
[ Tue Jul 16 02:10:06 2024 ] 	Batch(3200/6809) done. Loss: 0.1848  lr:0.000100
[ Tue Jul 16 02:10:29 2024 ] 	Batch(3300/6809) done. Loss: 0.1753  lr:0.000100
[ Tue Jul 16 02:10:52 2024 ] 	Batch(3400/6809) done. Loss: 0.0197  lr:0.000100
[ Tue Jul 16 02:11:15 2024 ] 
Training: Epoch [82/150], Step [3499], Loss: 0.02033413015305996, Training Accuracy: 95.40714285714286
[ Tue Jul 16 02:11:15 2024 ] 	Batch(3500/6809) done. Loss: 0.3004  lr:0.000100
[ Tue Jul 16 02:11:38 2024 ] 	Batch(3600/6809) done. Loss: 0.0277  lr:0.000100
[ Tue Jul 16 02:12:00 2024 ] 	Batch(3700/6809) done. Loss: 0.0450  lr:0.000100
[ Tue Jul 16 02:12:23 2024 ] 	Batch(3800/6809) done. Loss: 0.0438  lr:0.000100
[ Tue Jul 16 02:12:46 2024 ] 	Batch(3900/6809) done. Loss: 0.0160  lr:0.000100
[ Tue Jul 16 02:13:08 2024 ] 
Training: Epoch [82/150], Step [3999], Loss: 0.52452552318573, Training Accuracy: 95.39999999999999
[ Tue Jul 16 02:13:08 2024 ] 	Batch(4000/6809) done. Loss: 0.0298  lr:0.000100
[ Tue Jul 16 02:13:31 2024 ] 	Batch(4100/6809) done. Loss: 0.0341  lr:0.000100
[ Tue Jul 16 02:13:55 2024 ] 	Batch(4200/6809) done. Loss: 0.0051  lr:0.000100
[ Tue Jul 16 02:14:18 2024 ] 	Batch(4300/6809) done. Loss: 0.1705  lr:0.000100
[ Tue Jul 16 02:14:41 2024 ] 	Batch(4400/6809) done. Loss: 0.2844  lr:0.000100
[ Tue Jul 16 02:15:04 2024 ] 
Training: Epoch [82/150], Step [4499], Loss: 0.1812094748020172, Training Accuracy: 95.43611111111112
[ Tue Jul 16 02:15:04 2024 ] 	Batch(4500/6809) done. Loss: 0.0502  lr:0.000100
[ Tue Jul 16 02:15:27 2024 ] 	Batch(4600/6809) done. Loss: 0.1243  lr:0.000100
[ Tue Jul 16 02:15:50 2024 ] 	Batch(4700/6809) done. Loss: 0.0649  lr:0.000100
[ Tue Jul 16 02:16:12 2024 ] 	Batch(4800/6809) done. Loss: 0.1482  lr:0.000100
[ Tue Jul 16 02:16:35 2024 ] 	Batch(4900/6809) done. Loss: 0.1059  lr:0.000100
[ Tue Jul 16 02:16:57 2024 ] 
Training: Epoch [82/150], Step [4999], Loss: 0.07673564553260803, Training Accuracy: 95.5225
[ Tue Jul 16 02:16:58 2024 ] 	Batch(5000/6809) done. Loss: 0.2720  lr:0.000100
[ Tue Jul 16 02:17:20 2024 ] 	Batch(5100/6809) done. Loss: 0.0786  lr:0.000100
[ Tue Jul 16 02:17:43 2024 ] 	Batch(5200/6809) done. Loss: 0.0563  lr:0.000100
[ Tue Jul 16 02:18:06 2024 ] 	Batch(5300/6809) done. Loss: 0.1017  lr:0.000100
[ Tue Jul 16 02:18:29 2024 ] 	Batch(5400/6809) done. Loss: 0.4793  lr:0.000100
[ Tue Jul 16 02:18:51 2024 ] 
Training: Epoch [82/150], Step [5499], Loss: 0.1206498071551323, Training Accuracy: 95.50909090909092
[ Tue Jul 16 02:18:52 2024 ] 	Batch(5500/6809) done. Loss: 0.1259  lr:0.000100
[ Tue Jul 16 02:19:15 2024 ] 	Batch(5600/6809) done. Loss: 0.2294  lr:0.000100
[ Tue Jul 16 02:19:38 2024 ] 	Batch(5700/6809) done. Loss: 0.3579  lr:0.000100
[ Tue Jul 16 02:20:01 2024 ] 	Batch(5800/6809) done. Loss: 0.1416  lr:0.000100
[ Tue Jul 16 02:20:24 2024 ] 	Batch(5900/6809) done. Loss: 0.1872  lr:0.000100
[ Tue Jul 16 02:20:47 2024 ] 
Training: Epoch [82/150], Step [5999], Loss: 0.09328550100326538, Training Accuracy: 95.49375
[ Tue Jul 16 02:20:48 2024 ] 	Batch(6000/6809) done. Loss: 0.5211  lr:0.000100
[ Tue Jul 16 02:21:11 2024 ] 	Batch(6100/6809) done. Loss: 0.0185  lr:0.000100
[ Tue Jul 16 02:21:34 2024 ] 	Batch(6200/6809) done. Loss: 0.1007  lr:0.000100
[ Tue Jul 16 02:21:57 2024 ] 	Batch(6300/6809) done. Loss: 0.2054  lr:0.000100
[ Tue Jul 16 02:22:20 2024 ] 	Batch(6400/6809) done. Loss: 0.2222  lr:0.000100
[ Tue Jul 16 02:22:43 2024 ] 
Training: Epoch [82/150], Step [6499], Loss: 0.5437811613082886, Training Accuracy: 95.49230769230769
[ Tue Jul 16 02:22:44 2024 ] 	Batch(6500/6809) done. Loss: 0.0079  lr:0.000100
[ Tue Jul 16 02:23:07 2024 ] 	Batch(6600/6809) done. Loss: 0.1282  lr:0.000100
[ Tue Jul 16 02:23:30 2024 ] 	Batch(6700/6809) done. Loss: 0.0262  lr:0.000100
[ Tue Jul 16 02:23:53 2024 ] 	Batch(6800/6809) done. Loss: 0.3802  lr:0.000100
[ Tue Jul 16 02:23:55 2024 ] 	Mean training loss: 0.1666.
[ Tue Jul 16 02:23:55 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 02:23:55 2024 ] Training epoch: 84
[ Tue Jul 16 02:23:56 2024 ] 	Batch(0/6809) done. Loss: 0.2077  lr:0.000100
[ Tue Jul 16 02:24:19 2024 ] 	Batch(100/6809) done. Loss: 0.0518  lr:0.000100
[ Tue Jul 16 02:24:41 2024 ] 	Batch(200/6809) done. Loss: 0.0910  lr:0.000100
[ Tue Jul 16 02:25:04 2024 ] 	Batch(300/6809) done. Loss: 0.1522  lr:0.000100
[ Tue Jul 16 02:25:28 2024 ] 	Batch(400/6809) done. Loss: 0.0156  lr:0.000100
[ Tue Jul 16 02:25:50 2024 ] 
Training: Epoch [83/150], Step [499], Loss: 0.12320297956466675, Training Accuracy: 94.575
[ Tue Jul 16 02:25:51 2024 ] 	Batch(500/6809) done. Loss: 0.0075  lr:0.000100
[ Tue Jul 16 02:26:14 2024 ] 	Batch(600/6809) done. Loss: 0.0436  lr:0.000100
[ Tue Jul 16 02:26:37 2024 ] 	Batch(700/6809) done. Loss: 0.0256  lr:0.000100
[ Tue Jul 16 02:27:00 2024 ] 	Batch(800/6809) done. Loss: 0.3772  lr:0.000100
[ Tue Jul 16 02:27:23 2024 ] 	Batch(900/6809) done. Loss: 0.0234  lr:0.000100
[ Tue Jul 16 02:27:46 2024 ] 
Training: Epoch [83/150], Step [999], Loss: 0.04586852714419365, Training Accuracy: 95.05
[ Tue Jul 16 02:27:46 2024 ] 	Batch(1000/6809) done. Loss: 0.4799  lr:0.000100
[ Tue Jul 16 02:28:09 2024 ] 	Batch(1100/6809) done. Loss: 0.7422  lr:0.000100
[ Tue Jul 16 02:28:32 2024 ] 	Batch(1200/6809) done. Loss: 0.0877  lr:0.000100
[ Tue Jul 16 02:28:55 2024 ] 	Batch(1300/6809) done. Loss: 0.1305  lr:0.000100
[ Tue Jul 16 02:29:18 2024 ] 	Batch(1400/6809) done. Loss: 0.2687  lr:0.000100
[ Tue Jul 16 02:29:41 2024 ] 
Training: Epoch [83/150], Step [1499], Loss: 0.41253283619880676, Training Accuracy: 95.15
[ Tue Jul 16 02:29:41 2024 ] 	Batch(1500/6809) done. Loss: 0.1456  lr:0.000100
[ Tue Jul 16 02:30:04 2024 ] 	Batch(1600/6809) done. Loss: 0.2479  lr:0.000100
[ Tue Jul 16 02:30:27 2024 ] 	Batch(1700/6809) done. Loss: 0.4275  lr:0.000100
[ Tue Jul 16 02:30:50 2024 ] 	Batch(1800/6809) done. Loss: 0.1319  lr:0.000100
[ Tue Jul 16 02:31:13 2024 ] 	Batch(1900/6809) done. Loss: 0.2848  lr:0.000100
[ Tue Jul 16 02:31:36 2024 ] 
Training: Epoch [83/150], Step [1999], Loss: 0.04364488273859024, Training Accuracy: 95.2375
[ Tue Jul 16 02:31:36 2024 ] 	Batch(2000/6809) done. Loss: 0.0750  lr:0.000100
[ Tue Jul 16 02:31:59 2024 ] 	Batch(2100/6809) done. Loss: 0.0116  lr:0.000100
[ Tue Jul 16 02:32:22 2024 ] 	Batch(2200/6809) done. Loss: 0.1021  lr:0.000100
[ Tue Jul 16 02:32:45 2024 ] 	Batch(2300/6809) done. Loss: 0.4077  lr:0.000100
[ Tue Jul 16 02:33:09 2024 ] 	Batch(2400/6809) done. Loss: 0.1648  lr:0.000100
[ Tue Jul 16 02:33:31 2024 ] 
Training: Epoch [83/150], Step [2499], Loss: 0.040215205401182175, Training Accuracy: 95.165
[ Tue Jul 16 02:33:32 2024 ] 	Batch(2500/6809) done. Loss: 0.4575  lr:0.000100
[ Tue Jul 16 02:33:55 2024 ] 	Batch(2600/6809) done. Loss: 0.0104  lr:0.000100
[ Tue Jul 16 02:34:18 2024 ] 	Batch(2700/6809) done. Loss: 0.0765  lr:0.000100
[ Tue Jul 16 02:34:41 2024 ] 	Batch(2800/6809) done. Loss: 0.0985  lr:0.000100
[ Tue Jul 16 02:35:04 2024 ] 	Batch(2900/6809) done. Loss: 0.1612  lr:0.000100
[ Tue Jul 16 02:35:27 2024 ] 
Training: Epoch [83/150], Step [2999], Loss: 0.11834927648305893, Training Accuracy: 95.22083333333333
[ Tue Jul 16 02:35:27 2024 ] 	Batch(3000/6809) done. Loss: 0.1305  lr:0.000100
[ Tue Jul 16 02:35:50 2024 ] 	Batch(3100/6809) done. Loss: 0.1892  lr:0.000100
[ Tue Jul 16 02:36:13 2024 ] 	Batch(3200/6809) done. Loss: 0.2303  lr:0.000100
[ Tue Jul 16 02:36:37 2024 ] 	Batch(3300/6809) done. Loss: 0.3346  lr:0.000100
[ Tue Jul 16 02:37:01 2024 ] 	Batch(3400/6809) done. Loss: 0.0321  lr:0.000100
[ Tue Jul 16 02:37:24 2024 ] 
Training: Epoch [83/150], Step [3499], Loss: 0.18182215094566345, Training Accuracy: 95.22142857142856
[ Tue Jul 16 02:37:24 2024 ] 	Batch(3500/6809) done. Loss: 0.2187  lr:0.000100
[ Tue Jul 16 02:37:48 2024 ] 	Batch(3600/6809) done. Loss: 0.1452  lr:0.000100
[ Tue Jul 16 02:38:11 2024 ] 	Batch(3700/6809) done. Loss: 0.1573  lr:0.000100
[ Tue Jul 16 02:38:34 2024 ] 	Batch(3800/6809) done. Loss: 0.1419  lr:0.000100
[ Tue Jul 16 02:38:57 2024 ] 	Batch(3900/6809) done. Loss: 0.0129  lr:0.000100
[ Tue Jul 16 02:39:20 2024 ] 
Training: Epoch [83/150], Step [3999], Loss: 0.013999123126268387, Training Accuracy: 95.35
[ Tue Jul 16 02:39:21 2024 ] 	Batch(4000/6809) done. Loss: 0.0394  lr:0.000100
[ Tue Jul 16 02:39:44 2024 ] 	Batch(4100/6809) done. Loss: 0.0769  lr:0.000100
[ Tue Jul 16 02:40:07 2024 ] 	Batch(4200/6809) done. Loss: 0.3068  lr:0.000100
[ Tue Jul 16 02:40:30 2024 ] 	Batch(4300/6809) done. Loss: 0.2091  lr:0.000100
[ Tue Jul 16 02:40:52 2024 ] 	Batch(4400/6809) done. Loss: 0.2066  lr:0.000100
[ Tue Jul 16 02:41:16 2024 ] 
Training: Epoch [83/150], Step [4499], Loss: 0.05869198217988014, Training Accuracy: 95.31111111111112
[ Tue Jul 16 02:41:16 2024 ] 	Batch(4500/6809) done. Loss: 0.4782  lr:0.000100
[ Tue Jul 16 02:41:39 2024 ] 	Batch(4600/6809) done. Loss: 0.0503  lr:0.000100
[ Tue Jul 16 02:42:02 2024 ] 	Batch(4700/6809) done. Loss: 0.8348  lr:0.000100
[ Tue Jul 16 02:42:26 2024 ] 	Batch(4800/6809) done. Loss: 0.2112  lr:0.000100
[ Tue Jul 16 02:42:49 2024 ] 	Batch(4900/6809) done. Loss: 0.2932  lr:0.000100
[ Tue Jul 16 02:43:11 2024 ] 
Training: Epoch [83/150], Step [4999], Loss: 0.020037712529301643, Training Accuracy: 95.30499999999999
[ Tue Jul 16 02:43:11 2024 ] 	Batch(5000/6809) done. Loss: 0.0891  lr:0.000100
[ Tue Jul 16 02:43:34 2024 ] 	Batch(5100/6809) done. Loss: 0.0099  lr:0.000100
[ Tue Jul 16 02:43:57 2024 ] 	Batch(5200/6809) done. Loss: 0.0590  lr:0.000100
[ Tue Jul 16 02:44:19 2024 ] 	Batch(5300/6809) done. Loss: 0.1765  lr:0.000100
[ Tue Jul 16 02:44:42 2024 ] 	Batch(5400/6809) done. Loss: 0.1919  lr:0.000100
[ Tue Jul 16 02:45:04 2024 ] 
Training: Epoch [83/150], Step [5499], Loss: 0.0674724280834198, Training Accuracy: 95.30227272727274
[ Tue Jul 16 02:45:05 2024 ] 	Batch(5500/6809) done. Loss: 0.1600  lr:0.000100
[ Tue Jul 16 02:45:27 2024 ] 	Batch(5600/6809) done. Loss: 0.0578  lr:0.000100
[ Tue Jul 16 02:45:50 2024 ] 	Batch(5700/6809) done. Loss: 0.0305  lr:0.000100
[ Tue Jul 16 02:46:13 2024 ] 	Batch(5800/6809) done. Loss: 0.1734  lr:0.000100
[ Tue Jul 16 02:46:35 2024 ] 	Batch(5900/6809) done. Loss: 0.3808  lr:0.000100
[ Tue Jul 16 02:46:58 2024 ] 
Training: Epoch [83/150], Step [5999], Loss: 0.039681047201156616, Training Accuracy: 95.31458333333333
[ Tue Jul 16 02:46:58 2024 ] 	Batch(6000/6809) done. Loss: 0.1291  lr:0.000100
[ Tue Jul 16 02:47:21 2024 ] 	Batch(6100/6809) done. Loss: 0.0167  lr:0.000100
[ Tue Jul 16 02:47:44 2024 ] 	Batch(6200/6809) done. Loss: 0.0059  lr:0.000100
[ Tue Jul 16 02:48:06 2024 ] 	Batch(6300/6809) done. Loss: 0.1527  lr:0.000100
[ Tue Jul 16 02:48:29 2024 ] 	Batch(6400/6809) done. Loss: 0.0523  lr:0.000100
[ Tue Jul 16 02:48:51 2024 ] 
Training: Epoch [83/150], Step [6499], Loss: 0.037644773721694946, Training Accuracy: 95.32307692307693
[ Tue Jul 16 02:48:52 2024 ] 	Batch(6500/6809) done. Loss: 0.0323  lr:0.000100
[ Tue Jul 16 02:49:14 2024 ] 	Batch(6600/6809) done. Loss: 0.1761  lr:0.000100
[ Tue Jul 16 02:49:37 2024 ] 	Batch(6700/6809) done. Loss: 0.0438  lr:0.000100
[ Tue Jul 16 02:50:00 2024 ] 	Batch(6800/6809) done. Loss: 0.2037  lr:0.000100
[ Tue Jul 16 02:50:01 2024 ] 	Mean training loss: 0.1650.
[ Tue Jul 16 02:50:01 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 02:50:02 2024 ] Training epoch: 85
[ Tue Jul 16 02:50:02 2024 ] 	Batch(0/6809) done. Loss: 0.1234  lr:0.000100
[ Tue Jul 16 02:50:25 2024 ] 	Batch(100/6809) done. Loss: 0.1262  lr:0.000100
[ Tue Jul 16 02:50:49 2024 ] 	Batch(200/6809) done. Loss: 0.1987  lr:0.000100
[ Tue Jul 16 02:51:12 2024 ] 	Batch(300/6809) done. Loss: 0.0388  lr:0.000100
[ Tue Jul 16 02:51:35 2024 ] 	Batch(400/6809) done. Loss: 0.1089  lr:0.000100
[ Tue Jul 16 02:51:58 2024 ] 
Training: Epoch [84/150], Step [499], Loss: 0.0741632729768753, Training Accuracy: 95.35
[ Tue Jul 16 02:51:58 2024 ] 	Batch(500/6809) done. Loss: 0.0199  lr:0.000100
[ Tue Jul 16 02:52:21 2024 ] 	Batch(600/6809) done. Loss: 0.3910  lr:0.000100
[ Tue Jul 16 02:52:44 2024 ] 	Batch(700/6809) done. Loss: 0.5007  lr:0.000100
[ Tue Jul 16 02:53:07 2024 ] 	Batch(800/6809) done. Loss: 0.0698  lr:0.000100
[ Tue Jul 16 02:53:30 2024 ] 	Batch(900/6809) done. Loss: 0.3663  lr:0.000100
[ Tue Jul 16 02:53:53 2024 ] 
Training: Epoch [84/150], Step [999], Loss: 0.08530788123607635, Training Accuracy: 95.4875
[ Tue Jul 16 02:53:53 2024 ] 	Batch(1000/6809) done. Loss: 0.0167  lr:0.000100
[ Tue Jul 16 02:54:16 2024 ] 	Batch(1100/6809) done. Loss: 0.2593  lr:0.000100
[ Tue Jul 16 02:54:40 2024 ] 	Batch(1200/6809) done. Loss: 0.0030  lr:0.000100
[ Tue Jul 16 02:55:03 2024 ] 	Batch(1300/6809) done. Loss: 0.0125  lr:0.000100
[ Tue Jul 16 02:55:26 2024 ] 	Batch(1400/6809) done. Loss: 0.4709  lr:0.000100
[ Tue Jul 16 02:55:48 2024 ] 
Training: Epoch [84/150], Step [1499], Loss: 0.01907026395201683, Training Accuracy: 95.60833333333333
[ Tue Jul 16 02:55:49 2024 ] 	Batch(1500/6809) done. Loss: 0.1529  lr:0.000100
[ Tue Jul 16 02:56:12 2024 ] 	Batch(1600/6809) done. Loss: 0.0371  lr:0.000100
[ Tue Jul 16 02:56:34 2024 ] 	Batch(1700/6809) done. Loss: 0.0716  lr:0.000100
[ Tue Jul 16 02:56:57 2024 ] 	Batch(1800/6809) done. Loss: 0.6057  lr:0.000100
[ Tue Jul 16 02:57:20 2024 ] 	Batch(1900/6809) done. Loss: 0.0452  lr:0.000100
[ Tue Jul 16 02:57:42 2024 ] 
Training: Epoch [84/150], Step [1999], Loss: 0.24539022147655487, Training Accuracy: 95.7125
[ Tue Jul 16 02:57:43 2024 ] 	Batch(2000/6809) done. Loss: 0.0069  lr:0.000100
[ Tue Jul 16 02:58:05 2024 ] 	Batch(2100/6809) done. Loss: 0.3119  lr:0.000100
[ Tue Jul 16 02:58:28 2024 ] 	Batch(2200/6809) done. Loss: 0.3697  lr:0.000100
[ Tue Jul 16 02:58:51 2024 ] 	Batch(2300/6809) done. Loss: 0.0037  lr:0.000100
[ Tue Jul 16 02:59:13 2024 ] 	Batch(2400/6809) done. Loss: 0.0697  lr:0.000100
[ Tue Jul 16 02:59:36 2024 ] 
Training: Epoch [84/150], Step [2499], Loss: 0.3187134563922882, Training Accuracy: 95.695
[ Tue Jul 16 02:59:36 2024 ] 	Batch(2500/6809) done. Loss: 0.0582  lr:0.000100
[ Tue Jul 16 02:59:59 2024 ] 	Batch(2600/6809) done. Loss: 0.9619  lr:0.000100
[ Tue Jul 16 03:00:21 2024 ] 	Batch(2700/6809) done. Loss: 0.1616  lr:0.000100
[ Tue Jul 16 03:00:44 2024 ] 	Batch(2800/6809) done. Loss: 0.1224  lr:0.000100
[ Tue Jul 16 03:01:07 2024 ] 	Batch(2900/6809) done. Loss: 0.1557  lr:0.000100
[ Tue Jul 16 03:01:29 2024 ] 
Training: Epoch [84/150], Step [2999], Loss: 0.1074756383895874, Training Accuracy: 95.6
[ Tue Jul 16 03:01:29 2024 ] 	Batch(3000/6809) done. Loss: 0.0606  lr:0.000100
[ Tue Jul 16 03:01:52 2024 ] 	Batch(3100/6809) done. Loss: 0.1495  lr:0.000100
[ Tue Jul 16 03:02:15 2024 ] 	Batch(3200/6809) done. Loss: 0.0802  lr:0.000100
[ Tue Jul 16 03:02:38 2024 ] 	Batch(3300/6809) done. Loss: 0.1548  lr:0.000100
[ Tue Jul 16 03:03:00 2024 ] 	Batch(3400/6809) done. Loss: 0.1193  lr:0.000100
[ Tue Jul 16 03:03:23 2024 ] 
Training: Epoch [84/150], Step [3499], Loss: 0.11394322663545609, Training Accuracy: 95.56071428571428
[ Tue Jul 16 03:03:23 2024 ] 	Batch(3500/6809) done. Loss: 0.0303  lr:0.000100
[ Tue Jul 16 03:03:46 2024 ] 	Batch(3600/6809) done. Loss: 0.1752  lr:0.000100
[ Tue Jul 16 03:04:09 2024 ] 	Batch(3700/6809) done. Loss: 0.2049  lr:0.000100
[ Tue Jul 16 03:04:32 2024 ] 	Batch(3800/6809) done. Loss: 0.6474  lr:0.000100
[ Tue Jul 16 03:04:54 2024 ] 	Batch(3900/6809) done. Loss: 0.0970  lr:0.000100
[ Tue Jul 16 03:05:17 2024 ] 
Training: Epoch [84/150], Step [3999], Loss: 0.04234723001718521, Training Accuracy: 95.5375
[ Tue Jul 16 03:05:17 2024 ] 	Batch(4000/6809) done. Loss: 0.0545  lr:0.000100
[ Tue Jul 16 03:05:40 2024 ] 	Batch(4100/6809) done. Loss: 0.1253  lr:0.000100
[ Tue Jul 16 03:06:02 2024 ] 	Batch(4200/6809) done. Loss: 0.3538  lr:0.000100
[ Tue Jul 16 03:06:25 2024 ] 	Batch(4300/6809) done. Loss: 0.1101  lr:0.000100
[ Tue Jul 16 03:06:48 2024 ] 	Batch(4400/6809) done. Loss: 0.0097  lr:0.000100
[ Tue Jul 16 03:07:10 2024 ] 
Training: Epoch [84/150], Step [4499], Loss: 0.26936089992523193, Training Accuracy: 95.47777777777777
[ Tue Jul 16 03:07:11 2024 ] 	Batch(4500/6809) done. Loss: 0.1352  lr:0.000100
[ Tue Jul 16 03:07:33 2024 ] 	Batch(4600/6809) done. Loss: 0.1142  lr:0.000100
[ Tue Jul 16 03:07:56 2024 ] 	Batch(4700/6809) done. Loss: 0.0620  lr:0.000100
[ Tue Jul 16 03:08:19 2024 ] 	Batch(4800/6809) done. Loss: 0.3898  lr:0.000100
[ Tue Jul 16 03:08:41 2024 ] 	Batch(4900/6809) done. Loss: 0.0536  lr:0.000100
[ Tue Jul 16 03:09:04 2024 ] 
Training: Epoch [84/150], Step [4999], Loss: 0.08921770751476288, Training Accuracy: 95.4525
[ Tue Jul 16 03:09:04 2024 ] 	Batch(5000/6809) done. Loss: 0.0354  lr:0.000100
[ Tue Jul 16 03:09:27 2024 ] 	Batch(5100/6809) done. Loss: 0.1578  lr:0.000100
[ Tue Jul 16 03:09:50 2024 ] 	Batch(5200/6809) done. Loss: 0.2320  lr:0.000100
[ Tue Jul 16 03:10:12 2024 ] 	Batch(5300/6809) done. Loss: 0.7862  lr:0.000100
[ Tue Jul 16 03:10:35 2024 ] 	Batch(5400/6809) done. Loss: 0.1808  lr:0.000100
[ Tue Jul 16 03:10:58 2024 ] 
Training: Epoch [84/150], Step [5499], Loss: 0.06398855149745941, Training Accuracy: 95.42954545454545
[ Tue Jul 16 03:10:58 2024 ] 	Batch(5500/6809) done. Loss: 0.0773  lr:0.000100
[ Tue Jul 16 03:11:21 2024 ] 	Batch(5600/6809) done. Loss: 0.0541  lr:0.000100
[ Tue Jul 16 03:11:43 2024 ] 	Batch(5700/6809) done. Loss: 0.0554  lr:0.000100
[ Tue Jul 16 03:12:06 2024 ] 	Batch(5800/6809) done. Loss: 0.0148  lr:0.000100
[ Tue Jul 16 03:12:29 2024 ] 	Batch(5900/6809) done. Loss: 0.0626  lr:0.000100
[ Tue Jul 16 03:12:51 2024 ] 
Training: Epoch [84/150], Step [5999], Loss: 0.07112215459346771, Training Accuracy: 95.42916666666666
[ Tue Jul 16 03:12:51 2024 ] 	Batch(6000/6809) done. Loss: 0.0897  lr:0.000100
[ Tue Jul 16 03:13:14 2024 ] 	Batch(6100/6809) done. Loss: 0.1389  lr:0.000100
[ Tue Jul 16 03:13:37 2024 ] 	Batch(6200/6809) done. Loss: 0.0900  lr:0.000100
[ Tue Jul 16 03:14:01 2024 ] 	Batch(6300/6809) done. Loss: 0.0942  lr:0.000100
[ Tue Jul 16 03:14:24 2024 ] 	Batch(6400/6809) done. Loss: 0.1272  lr:0.000100
[ Tue Jul 16 03:14:47 2024 ] 
Training: Epoch [84/150], Step [6499], Loss: 0.039764054119586945, Training Accuracy: 95.38461538461539
[ Tue Jul 16 03:14:47 2024 ] 	Batch(6500/6809) done. Loss: 0.0320  lr:0.000100
[ Tue Jul 16 03:15:10 2024 ] 	Batch(6600/6809) done. Loss: 0.0462  lr:0.000100
[ Tue Jul 16 03:15:32 2024 ] 	Batch(6700/6809) done. Loss: 0.0251  lr:0.000100
[ Tue Jul 16 03:15:55 2024 ] 	Batch(6800/6809) done. Loss: 0.0258  lr:0.000100
[ Tue Jul 16 03:15:57 2024 ] 	Mean training loss: 0.1619.
[ Tue Jul 16 03:15:57 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 03:15:57 2024 ] Training epoch: 86
[ Tue Jul 16 03:15:58 2024 ] 	Batch(0/6809) done. Loss: 0.3107  lr:0.000100
[ Tue Jul 16 03:16:20 2024 ] 	Batch(100/6809) done. Loss: 0.0277  lr:0.000100
[ Tue Jul 16 03:16:43 2024 ] 	Batch(200/6809) done. Loss: 0.1009  lr:0.000100
[ Tue Jul 16 03:17:05 2024 ] 	Batch(300/6809) done. Loss: 0.0160  lr:0.000100
[ Tue Jul 16 03:17:28 2024 ] 	Batch(400/6809) done. Loss: 0.1281  lr:0.000100
[ Tue Jul 16 03:17:50 2024 ] 
Training: Epoch [85/150], Step [499], Loss: 0.014515376649796963, Training Accuracy: 95.325
[ Tue Jul 16 03:17:50 2024 ] 	Batch(500/6809) done. Loss: 0.1240  lr:0.000100
[ Tue Jul 16 03:18:13 2024 ] 	Batch(600/6809) done. Loss: 0.0653  lr:0.000100
[ Tue Jul 16 03:18:35 2024 ] 	Batch(700/6809) done. Loss: 0.1940  lr:0.000100
[ Tue Jul 16 03:18:58 2024 ] 	Batch(800/6809) done. Loss: 0.3738  lr:0.000100
[ Tue Jul 16 03:19:21 2024 ] 	Batch(900/6809) done. Loss: 0.7150  lr:0.000100
[ Tue Jul 16 03:19:43 2024 ] 
Training: Epoch [85/150], Step [999], Loss: 0.1758458912372589, Training Accuracy: 95.15
[ Tue Jul 16 03:19:43 2024 ] 	Batch(1000/6809) done. Loss: 0.2977  lr:0.000100
[ Tue Jul 16 03:20:06 2024 ] 	Batch(1100/6809) done. Loss: 0.1242  lr:0.000100
[ Tue Jul 16 03:20:28 2024 ] 	Batch(1200/6809) done. Loss: 0.4984  lr:0.000100
[ Tue Jul 16 03:20:51 2024 ] 	Batch(1300/6809) done. Loss: 0.1330  lr:0.000100
[ Tue Jul 16 03:21:14 2024 ] 	Batch(1400/6809) done. Loss: 0.0473  lr:0.000100
[ Tue Jul 16 03:21:37 2024 ] 
Training: Epoch [85/150], Step [1499], Loss: 0.03276972472667694, Training Accuracy: 95.22500000000001
[ Tue Jul 16 03:21:37 2024 ] 	Batch(1500/6809) done. Loss: 0.1085  lr:0.000100
[ Tue Jul 16 03:22:00 2024 ] 	Batch(1600/6809) done. Loss: 0.6068  lr:0.000100
[ Tue Jul 16 03:22:22 2024 ] 	Batch(1700/6809) done. Loss: 0.2649  lr:0.000100
[ Tue Jul 16 03:22:45 2024 ] 	Batch(1800/6809) done. Loss: 0.2806  lr:0.000100
[ Tue Jul 16 03:23:07 2024 ] 	Batch(1900/6809) done. Loss: 0.1229  lr:0.000100
[ Tue Jul 16 03:23:30 2024 ] 
Training: Epoch [85/150], Step [1999], Loss: 0.09655282646417618, Training Accuracy: 95.30625
[ Tue Jul 16 03:23:30 2024 ] 	Batch(2000/6809) done. Loss: 0.3398  lr:0.000100
[ Tue Jul 16 03:23:52 2024 ] 	Batch(2100/6809) done. Loss: 0.0157  lr:0.000100
[ Tue Jul 16 03:24:15 2024 ] 	Batch(2200/6809) done. Loss: 0.0319  lr:0.000100
[ Tue Jul 16 03:24:37 2024 ] 	Batch(2300/6809) done. Loss: 0.1380  lr:0.000100
[ Tue Jul 16 03:25:01 2024 ] 	Batch(2400/6809) done. Loss: 0.0580  lr:0.000100
[ Tue Jul 16 03:25:23 2024 ] 
Training: Epoch [85/150], Step [2499], Loss: 0.04618347808718681, Training Accuracy: 95.34
[ Tue Jul 16 03:25:24 2024 ] 	Batch(2500/6809) done. Loss: 0.2480  lr:0.000100
[ Tue Jul 16 03:25:46 2024 ] 	Batch(2600/6809) done. Loss: 0.0195  lr:0.000100
[ Tue Jul 16 03:26:09 2024 ] 	Batch(2700/6809) done. Loss: 0.0189  lr:0.000100
[ Tue Jul 16 03:26:31 2024 ] 	Batch(2800/6809) done. Loss: 0.0218  lr:0.000100
[ Tue Jul 16 03:26:54 2024 ] 	Batch(2900/6809) done. Loss: 0.0412  lr:0.000100
[ Tue Jul 16 03:27:16 2024 ] 
Training: Epoch [85/150], Step [2999], Loss: 0.523432195186615, Training Accuracy: 95.35416666666666
[ Tue Jul 16 03:27:16 2024 ] 	Batch(3000/6809) done. Loss: 0.1762  lr:0.000100
[ Tue Jul 16 03:27:39 2024 ] 	Batch(3100/6809) done. Loss: 0.8272  lr:0.000100
[ Tue Jul 16 03:28:02 2024 ] 	Batch(3200/6809) done. Loss: 0.2508  lr:0.000100
[ Tue Jul 16 03:28:24 2024 ] 	Batch(3300/6809) done. Loss: 0.3969  lr:0.000100
[ Tue Jul 16 03:28:47 2024 ] 	Batch(3400/6809) done. Loss: 0.0449  lr:0.000100
[ Tue Jul 16 03:29:09 2024 ] 
Training: Epoch [85/150], Step [3499], Loss: 0.3450382649898529, Training Accuracy: 95.39999999999999
[ Tue Jul 16 03:29:09 2024 ] 	Batch(3500/6809) done. Loss: 0.0438  lr:0.000100
[ Tue Jul 16 03:29:32 2024 ] 	Batch(3600/6809) done. Loss: 0.0209  lr:0.000100
[ Tue Jul 16 03:29:54 2024 ] 	Batch(3700/6809) done. Loss: 0.0098  lr:0.000100
[ Tue Jul 16 03:30:17 2024 ] 	Batch(3800/6809) done. Loss: 0.3475  lr:0.000100
[ Tue Jul 16 03:30:40 2024 ] 	Batch(3900/6809) done. Loss: 0.0107  lr:0.000100
[ Tue Jul 16 03:31:02 2024 ] 
Training: Epoch [85/150], Step [3999], Loss: 0.11735937744379044, Training Accuracy: 95.434375
[ Tue Jul 16 03:31:02 2024 ] 	Batch(4000/6809) done. Loss: 0.5262  lr:0.000100
[ Tue Jul 16 03:31:25 2024 ] 	Batch(4100/6809) done. Loss: 0.0638  lr:0.000100
[ Tue Jul 16 03:31:48 2024 ] 	Batch(4200/6809) done. Loss: 0.1587  lr:0.000100
[ Tue Jul 16 03:32:10 2024 ] 	Batch(4300/6809) done. Loss: 0.3514  lr:0.000100
[ Tue Jul 16 03:32:33 2024 ] 	Batch(4400/6809) done. Loss: 0.0038  lr:0.000100
[ Tue Jul 16 03:32:57 2024 ] 
Training: Epoch [85/150], Step [4499], Loss: 0.1694371998310089, Training Accuracy: 95.43333333333334
[ Tue Jul 16 03:32:57 2024 ] 	Batch(4500/6809) done. Loss: 0.0463  lr:0.000100
[ Tue Jul 16 03:33:20 2024 ] 	Batch(4600/6809) done. Loss: 0.0393  lr:0.000100
[ Tue Jul 16 03:33:43 2024 ] 	Batch(4700/6809) done. Loss: 0.1647  lr:0.000100
[ Tue Jul 16 03:34:07 2024 ] 	Batch(4800/6809) done. Loss: 0.0794  lr:0.000100
[ Tue Jul 16 03:34:31 2024 ] 	Batch(4900/6809) done. Loss: 0.1012  lr:0.000100
[ Tue Jul 16 03:34:53 2024 ] 
Training: Epoch [85/150], Step [4999], Loss: 0.14844849705696106, Training Accuracy: 95.4375
[ Tue Jul 16 03:34:54 2024 ] 	Batch(5000/6809) done. Loss: 0.0729  lr:0.000100
[ Tue Jul 16 03:35:16 2024 ] 	Batch(5100/6809) done. Loss: 0.8029  lr:0.000100
[ Tue Jul 16 03:35:39 2024 ] 	Batch(5200/6809) done. Loss: 0.5699  lr:0.000100
[ Tue Jul 16 03:36:02 2024 ] 	Batch(5300/6809) done. Loss: 0.2466  lr:0.000100
[ Tue Jul 16 03:36:24 2024 ] 	Batch(5400/6809) done. Loss: 0.0330  lr:0.000100
[ Tue Jul 16 03:36:47 2024 ] 
Training: Epoch [85/150], Step [5499], Loss: 0.8895306587219238, Training Accuracy: 95.44318181818183
[ Tue Jul 16 03:36:47 2024 ] 	Batch(5500/6809) done. Loss: 0.1018  lr:0.000100
[ Tue Jul 16 03:37:09 2024 ] 	Batch(5600/6809) done. Loss: 0.0059  lr:0.000100
[ Tue Jul 16 03:37:32 2024 ] 	Batch(5700/6809) done. Loss: 0.0740  lr:0.000100
[ Tue Jul 16 03:37:55 2024 ] 	Batch(5800/6809) done. Loss: 0.1059  lr:0.000100
[ Tue Jul 16 03:38:17 2024 ] 	Batch(5900/6809) done. Loss: 0.1550  lr:0.000100
[ Tue Jul 16 03:38:40 2024 ] 
Training: Epoch [85/150], Step [5999], Loss: 0.005119780078530312, Training Accuracy: 95.47291666666666
[ Tue Jul 16 03:38:40 2024 ] 	Batch(6000/6809) done. Loss: 0.0099  lr:0.000100
[ Tue Jul 16 03:39:03 2024 ] 	Batch(6100/6809) done. Loss: 0.3324  lr:0.000100
[ Tue Jul 16 03:39:25 2024 ] 	Batch(6200/6809) done. Loss: 0.1075  lr:0.000100
[ Tue Jul 16 03:39:48 2024 ] 	Batch(6300/6809) done. Loss: 0.0026  lr:0.000100
[ Tue Jul 16 03:40:10 2024 ] 	Batch(6400/6809) done. Loss: 0.3558  lr:0.000100
[ Tue Jul 16 03:40:33 2024 ] 
Training: Epoch [85/150], Step [6499], Loss: 0.025217866525053978, Training Accuracy: 95.48076923076924
[ Tue Jul 16 03:40:33 2024 ] 	Batch(6500/6809) done. Loss: 0.1701  lr:0.000100
[ Tue Jul 16 03:40:56 2024 ] 	Batch(6600/6809) done. Loss: 0.1005  lr:0.000100
[ Tue Jul 16 03:41:18 2024 ] 	Batch(6700/6809) done. Loss: 0.2004  lr:0.000100
[ Tue Jul 16 03:41:41 2024 ] 	Batch(6800/6809) done. Loss: 0.1527  lr:0.000100
[ Tue Jul 16 03:41:43 2024 ] 	Mean training loss: 0.1629.
[ Tue Jul 16 03:41:43 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 03:41:43 2024 ] Training epoch: 87
[ Tue Jul 16 03:41:43 2024 ] 	Batch(0/6809) done. Loss: 0.0302  lr:0.000100
[ Tue Jul 16 03:42:06 2024 ] 	Batch(100/6809) done. Loss: 0.0178  lr:0.000100
[ Tue Jul 16 03:42:28 2024 ] 	Batch(200/6809) done. Loss: 0.0788  lr:0.000100
[ Tue Jul 16 03:42:51 2024 ] 	Batch(300/6809) done. Loss: 0.0233  lr:0.000100
[ Tue Jul 16 03:43:14 2024 ] 	Batch(400/6809) done. Loss: 0.1442  lr:0.000100
[ Tue Jul 16 03:43:36 2024 ] 
Training: Epoch [86/150], Step [499], Loss: 0.013954848051071167, Training Accuracy: 95.22500000000001
[ Tue Jul 16 03:43:36 2024 ] 	Batch(500/6809) done. Loss: 0.1784  lr:0.000100
[ Tue Jul 16 03:43:59 2024 ] 	Batch(600/6809) done. Loss: 0.0127  lr:0.000100
[ Tue Jul 16 03:44:21 2024 ] 	Batch(700/6809) done. Loss: 0.0226  lr:0.000100
[ Tue Jul 16 03:44:44 2024 ] 	Batch(800/6809) done. Loss: 0.0138  lr:0.000100
[ Tue Jul 16 03:45:07 2024 ] 	Batch(900/6809) done. Loss: 0.0652  lr:0.000100
[ Tue Jul 16 03:45:29 2024 ] 
Training: Epoch [86/150], Step [999], Loss: 0.07005736231803894, Training Accuracy: 95.16250000000001
[ Tue Jul 16 03:45:29 2024 ] 	Batch(1000/6809) done. Loss: 0.2836  lr:0.000100
[ Tue Jul 16 03:45:52 2024 ] 	Batch(1100/6809) done. Loss: 0.2599  lr:0.000100
[ Tue Jul 16 03:46:15 2024 ] 	Batch(1200/6809) done. Loss: 0.0581  lr:0.000100
[ Tue Jul 16 03:46:38 2024 ] 	Batch(1300/6809) done. Loss: 0.3705  lr:0.000100
[ Tue Jul 16 03:47:01 2024 ] 	Batch(1400/6809) done. Loss: 0.0884  lr:0.000100
[ Tue Jul 16 03:47:24 2024 ] 
Training: Epoch [86/150], Step [1499], Loss: 0.05522402375936508, Training Accuracy: 95.22500000000001
[ Tue Jul 16 03:47:24 2024 ] 	Batch(1500/6809) done. Loss: 0.2090  lr:0.000100
[ Tue Jul 16 03:47:47 2024 ] 	Batch(1600/6809) done. Loss: 0.5681  lr:0.000100
[ Tue Jul 16 03:48:10 2024 ] 	Batch(1700/6809) done. Loss: 0.4278  lr:0.000100
[ Tue Jul 16 03:48:33 2024 ] 	Batch(1800/6809) done. Loss: 0.0420  lr:0.000100
[ Tue Jul 16 03:48:56 2024 ] 	Batch(1900/6809) done. Loss: 0.0254  lr:0.000100
[ Tue Jul 16 03:49:19 2024 ] 
Training: Epoch [86/150], Step [1999], Loss: 0.19861698150634766, Training Accuracy: 95.275
[ Tue Jul 16 03:49:20 2024 ] 	Batch(2000/6809) done. Loss: 0.0339  lr:0.000100
[ Tue Jul 16 03:49:43 2024 ] 	Batch(2100/6809) done. Loss: 0.1187  lr:0.000100
[ Tue Jul 16 03:50:06 2024 ] 	Batch(2200/6809) done. Loss: 0.0902  lr:0.000100
[ Tue Jul 16 03:50:28 2024 ] 	Batch(2300/6809) done. Loss: 0.4442  lr:0.000100
[ Tue Jul 16 03:50:51 2024 ] 	Batch(2400/6809) done. Loss: 0.0526  lr:0.000100
[ Tue Jul 16 03:51:13 2024 ] 
Training: Epoch [86/150], Step [2499], Loss: 0.08038605004549026, Training Accuracy: 95.34
[ Tue Jul 16 03:51:14 2024 ] 	Batch(2500/6809) done. Loss: 0.0359  lr:0.000100
[ Tue Jul 16 03:51:36 2024 ] 	Batch(2600/6809) done. Loss: 0.3424  lr:0.000100
[ Tue Jul 16 03:51:59 2024 ] 	Batch(2700/6809) done. Loss: 0.1685  lr:0.000100
[ Tue Jul 16 03:52:23 2024 ] 	Batch(2800/6809) done. Loss: 0.0930  lr:0.000100
[ Tue Jul 16 03:52:46 2024 ] 	Batch(2900/6809) done. Loss: 0.0537  lr:0.000100
[ Tue Jul 16 03:53:10 2024 ] 
Training: Epoch [86/150], Step [2999], Loss: 0.00900778267532587, Training Accuracy: 95.33749999999999
[ Tue Jul 16 03:53:10 2024 ] 	Batch(3000/6809) done. Loss: 0.1442  lr:0.000100
[ Tue Jul 16 03:53:33 2024 ] 	Batch(3100/6809) done. Loss: 0.0050  lr:0.000100
[ Tue Jul 16 03:53:56 2024 ] 	Batch(3200/6809) done. Loss: 0.2102  lr:0.000100
[ Tue Jul 16 03:54:20 2024 ] 	Batch(3300/6809) done. Loss: 0.0774  lr:0.000100
[ Tue Jul 16 03:54:43 2024 ] 	Batch(3400/6809) done. Loss: 0.0058  lr:0.000100
[ Tue Jul 16 03:55:06 2024 ] 
Training: Epoch [86/150], Step [3499], Loss: 0.01569039560854435, Training Accuracy: 95.37142857142857
[ Tue Jul 16 03:55:06 2024 ] 	Batch(3500/6809) done. Loss: 0.4629  lr:0.000100
[ Tue Jul 16 03:55:29 2024 ] 	Batch(3600/6809) done. Loss: 0.2693  lr:0.000100
[ Tue Jul 16 03:55:52 2024 ] 	Batch(3700/6809) done. Loss: 0.5059  lr:0.000100
[ Tue Jul 16 03:56:15 2024 ] 	Batch(3800/6809) done. Loss: 0.1847  lr:0.000100
[ Tue Jul 16 03:56:37 2024 ] 	Batch(3900/6809) done. Loss: 0.7727  lr:0.000100
[ Tue Jul 16 03:57:00 2024 ] 
Training: Epoch [86/150], Step [3999], Loss: 0.02459203638136387, Training Accuracy: 95.415625
[ Tue Jul 16 03:57:00 2024 ] 	Batch(4000/6809) done. Loss: 0.1130  lr:0.000100
[ Tue Jul 16 03:57:23 2024 ] 	Batch(4100/6809) done. Loss: 0.3414  lr:0.000100
[ Tue Jul 16 03:57:45 2024 ] 	Batch(4200/6809) done. Loss: 0.1243  lr:0.000100
[ Tue Jul 16 03:58:08 2024 ] 	Batch(4300/6809) done. Loss: 0.0318  lr:0.000100
[ Tue Jul 16 03:58:31 2024 ] 	Batch(4400/6809) done. Loss: 0.0668  lr:0.000100
[ Tue Jul 16 03:58:53 2024 ] 
Training: Epoch [86/150], Step [4499], Loss: 0.619644284248352, Training Accuracy: 95.375
[ Tue Jul 16 03:58:53 2024 ] 	Batch(4500/6809) done. Loss: 0.2299  lr:0.000100
[ Tue Jul 16 03:59:16 2024 ] 	Batch(4600/6809) done. Loss: 0.0743  lr:0.000100
[ Tue Jul 16 03:59:38 2024 ] 	Batch(4700/6809) done. Loss: 0.0088  lr:0.000100
[ Tue Jul 16 04:00:01 2024 ] 	Batch(4800/6809) done. Loss: 0.1198  lr:0.000100
[ Tue Jul 16 04:00:24 2024 ] 	Batch(4900/6809) done. Loss: 0.0824  lr:0.000100
[ Tue Jul 16 04:00:46 2024 ] 
Training: Epoch [86/150], Step [4999], Loss: 0.4527633786201477, Training Accuracy: 95.38250000000001
[ Tue Jul 16 04:00:46 2024 ] 	Batch(5000/6809) done. Loss: 0.0052  lr:0.000100
[ Tue Jul 16 04:01:09 2024 ] 	Batch(5100/6809) done. Loss: 0.1386  lr:0.000100
[ Tue Jul 16 04:01:31 2024 ] 	Batch(5200/6809) done. Loss: 0.0505  lr:0.000100
[ Tue Jul 16 04:01:54 2024 ] 	Batch(5300/6809) done. Loss: 0.0975  lr:0.000100
[ Tue Jul 16 04:02:16 2024 ] 	Batch(5400/6809) done. Loss: 0.0415  lr:0.000100
[ Tue Jul 16 04:02:39 2024 ] 
Training: Epoch [86/150], Step [5499], Loss: 0.44499626755714417, Training Accuracy: 95.42500000000001
[ Tue Jul 16 04:02:39 2024 ] 	Batch(5500/6809) done. Loss: 0.1416  lr:0.000100
[ Tue Jul 16 04:03:02 2024 ] 	Batch(5600/6809) done. Loss: 0.1157  lr:0.000100
[ Tue Jul 16 04:03:24 2024 ] 	Batch(5700/6809) done. Loss: 0.1561  lr:0.000100
[ Tue Jul 16 04:03:47 2024 ] 	Batch(5800/6809) done. Loss: 0.1428  lr:0.000100
[ Tue Jul 16 04:04:09 2024 ] 	Batch(5900/6809) done. Loss: 0.3159  lr:0.000100
[ Tue Jul 16 04:04:32 2024 ] 
Training: Epoch [86/150], Step [5999], Loss: 0.03472136706113815, Training Accuracy: 95.46875
[ Tue Jul 16 04:04:32 2024 ] 	Batch(6000/6809) done. Loss: 0.1355  lr:0.000100
[ Tue Jul 16 04:04:55 2024 ] 	Batch(6100/6809) done. Loss: 0.0227  lr:0.000100
[ Tue Jul 16 04:05:17 2024 ] 	Batch(6200/6809) done. Loss: 0.2133  lr:0.000100
[ Tue Jul 16 04:05:40 2024 ] 	Batch(6300/6809) done. Loss: 0.1224  lr:0.000100
[ Tue Jul 16 04:06:02 2024 ] 	Batch(6400/6809) done. Loss: 0.1549  lr:0.000100
[ Tue Jul 16 04:06:25 2024 ] 
Training: Epoch [86/150], Step [6499], Loss: 0.19249498844146729, Training Accuracy: 95.48846153846154
[ Tue Jul 16 04:06:25 2024 ] 	Batch(6500/6809) done. Loss: 0.0649  lr:0.000100
[ Tue Jul 16 04:06:47 2024 ] 	Batch(6600/6809) done. Loss: 0.0732  lr:0.000100
[ Tue Jul 16 04:07:10 2024 ] 	Batch(6700/6809) done. Loss: 0.0207  lr:0.000100
[ Tue Jul 16 04:07:33 2024 ] 	Batch(6800/6809) done. Loss: 0.0645  lr:0.000100
[ Tue Jul 16 04:07:35 2024 ] 	Mean training loss: 0.1571.
[ Tue Jul 16 04:07:35 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 04:07:35 2024 ] Training epoch: 88
[ Tue Jul 16 04:07:35 2024 ] 	Batch(0/6809) done. Loss: 0.0488  lr:0.000100
[ Tue Jul 16 04:07:58 2024 ] 	Batch(100/6809) done. Loss: 0.9528  lr:0.000100
[ Tue Jul 16 04:08:21 2024 ] 	Batch(200/6809) done. Loss: 0.1254  lr:0.000100
[ Tue Jul 16 04:08:45 2024 ] 	Batch(300/6809) done. Loss: 0.0069  lr:0.000100
[ Tue Jul 16 04:09:08 2024 ] 	Batch(400/6809) done. Loss: 0.0650  lr:0.000100
[ Tue Jul 16 04:09:31 2024 ] 
Training: Epoch [87/150], Step [499], Loss: 0.16284190118312836, Training Accuracy: 95.575
[ Tue Jul 16 04:09:31 2024 ] 	Batch(500/6809) done. Loss: 0.2599  lr:0.000100
[ Tue Jul 16 04:09:54 2024 ] 	Batch(600/6809) done. Loss: 0.0337  lr:0.000100
[ Tue Jul 16 04:10:16 2024 ] 	Batch(700/6809) done. Loss: 0.0504  lr:0.000100
[ Tue Jul 16 04:10:39 2024 ] 	Batch(800/6809) done. Loss: 0.1968  lr:0.000100
[ Tue Jul 16 04:11:02 2024 ] 	Batch(900/6809) done. Loss: 0.2013  lr:0.000100
[ Tue Jul 16 04:11:24 2024 ] 
Training: Epoch [87/150], Step [999], Loss: 0.023075034841895103, Training Accuracy: 95.5375
[ Tue Jul 16 04:11:25 2024 ] 	Batch(1000/6809) done. Loss: 0.1096  lr:0.000100
[ Tue Jul 16 04:11:47 2024 ] 	Batch(1100/6809) done. Loss: 0.0540  lr:0.000100
[ Tue Jul 16 04:12:10 2024 ] 	Batch(1200/6809) done. Loss: 0.1955  lr:0.000100
[ Tue Jul 16 04:12:33 2024 ] 	Batch(1300/6809) done. Loss: 0.1505  lr:0.000100
[ Tue Jul 16 04:12:56 2024 ] 	Batch(1400/6809) done. Loss: 0.1891  lr:0.000100
[ Tue Jul 16 04:13:18 2024 ] 
Training: Epoch [87/150], Step [1499], Loss: 0.0640987828373909, Training Accuracy: 95.56666666666666
[ Tue Jul 16 04:13:18 2024 ] 	Batch(1500/6809) done. Loss: 0.1610  lr:0.000100
[ Tue Jul 16 04:13:41 2024 ] 	Batch(1600/6809) done. Loss: 0.1361  lr:0.000100
[ Tue Jul 16 04:14:04 2024 ] 	Batch(1700/6809) done. Loss: 0.0103  lr:0.000100
[ Tue Jul 16 04:14:27 2024 ] 	Batch(1800/6809) done. Loss: 0.1404  lr:0.000100
[ Tue Jul 16 04:14:49 2024 ] 	Batch(1900/6809) done. Loss: 0.2509  lr:0.000100
[ Tue Jul 16 04:15:12 2024 ] 
Training: Epoch [87/150], Step [1999], Loss: 0.4132850170135498, Training Accuracy: 95.5875
[ Tue Jul 16 04:15:12 2024 ] 	Batch(2000/6809) done. Loss: 0.3285  lr:0.000100
[ Tue Jul 16 04:15:35 2024 ] 	Batch(2100/6809) done. Loss: 0.0984  lr:0.000100
[ Tue Jul 16 04:15:57 2024 ] 	Batch(2200/6809) done. Loss: 0.0990  lr:0.000100
[ Tue Jul 16 04:16:20 2024 ] 	Batch(2300/6809) done. Loss: 0.0112  lr:0.000100
[ Tue Jul 16 04:16:43 2024 ] 	Batch(2400/6809) done. Loss: 0.3543  lr:0.000100
[ Tue Jul 16 04:17:05 2024 ] 
Training: Epoch [87/150], Step [2499], Loss: 0.012339039705693722, Training Accuracy: 95.61
[ Tue Jul 16 04:17:06 2024 ] 	Batch(2500/6809) done. Loss: 0.0415  lr:0.000100
[ Tue Jul 16 04:17:28 2024 ] 	Batch(2600/6809) done. Loss: 0.0453  lr:0.000100
[ Tue Jul 16 04:17:51 2024 ] 	Batch(2700/6809) done. Loss: 0.0092  lr:0.000100
[ Tue Jul 16 04:18:14 2024 ] 	Batch(2800/6809) done. Loss: 0.0308  lr:0.000100
[ Tue Jul 16 04:18:37 2024 ] 	Batch(2900/6809) done. Loss: 0.1203  lr:0.000100
[ Tue Jul 16 04:18:59 2024 ] 
Training: Epoch [87/150], Step [2999], Loss: 0.03414943441748619, Training Accuracy: 95.61666666666667
[ Tue Jul 16 04:18:59 2024 ] 	Batch(3000/6809) done. Loss: 0.0111  lr:0.000100
[ Tue Jul 16 04:19:22 2024 ] 	Batch(3100/6809) done. Loss: 0.0227  lr:0.000100
[ Tue Jul 16 04:19:45 2024 ] 	Batch(3200/6809) done. Loss: 0.0792  lr:0.000100
[ Tue Jul 16 04:20:08 2024 ] 	Batch(3300/6809) done. Loss: 0.1574  lr:0.000100
[ Tue Jul 16 04:20:30 2024 ] 	Batch(3400/6809) done. Loss: 0.0042  lr:0.000100
[ Tue Jul 16 04:20:53 2024 ] 
Training: Epoch [87/150], Step [3499], Loss: 0.033361684530973434, Training Accuracy: 95.67142857142858
[ Tue Jul 16 04:20:53 2024 ] 	Batch(3500/6809) done. Loss: 0.2665  lr:0.000100
[ Tue Jul 16 04:21:16 2024 ] 	Batch(3600/6809) done. Loss: 0.1090  lr:0.000100
[ Tue Jul 16 04:21:39 2024 ] 	Batch(3700/6809) done. Loss: 0.4577  lr:0.000100
[ Tue Jul 16 04:22:01 2024 ] 	Batch(3800/6809) done. Loss: 0.0423  lr:0.000100
[ Tue Jul 16 04:22:24 2024 ] 	Batch(3900/6809) done. Loss: 0.0977  lr:0.000100
[ Tue Jul 16 04:22:47 2024 ] 
Training: Epoch [87/150], Step [3999], Loss: 0.04179223254323006, Training Accuracy: 95.659375
[ Tue Jul 16 04:22:47 2024 ] 	Batch(4000/6809) done. Loss: 0.0319  lr:0.000100
[ Tue Jul 16 04:23:10 2024 ] 	Batch(4100/6809) done. Loss: 0.3352  lr:0.000100
[ Tue Jul 16 04:23:32 2024 ] 	Batch(4200/6809) done. Loss: 0.2142  lr:0.000100
[ Tue Jul 16 04:23:55 2024 ] 	Batch(4300/6809) done. Loss: 0.2188  lr:0.000100
[ Tue Jul 16 04:24:18 2024 ] 	Batch(4400/6809) done. Loss: 0.0231  lr:0.000100
[ Tue Jul 16 04:24:40 2024 ] 
Training: Epoch [87/150], Step [4499], Loss: 0.2978544533252716, Training Accuracy: 95.65
[ Tue Jul 16 04:24:41 2024 ] 	Batch(4500/6809) done. Loss: 0.0070  lr:0.000100
[ Tue Jul 16 04:25:04 2024 ] 	Batch(4600/6809) done. Loss: 0.0842  lr:0.000100
[ Tue Jul 16 04:25:28 2024 ] 	Batch(4700/6809) done. Loss: 0.0892  lr:0.000100
[ Tue Jul 16 04:25:51 2024 ] 	Batch(4800/6809) done. Loss: 0.4473  lr:0.000100
[ Tue Jul 16 04:26:15 2024 ] 	Batch(4900/6809) done. Loss: 0.0546  lr:0.000100
[ Tue Jul 16 04:26:38 2024 ] 
Training: Epoch [87/150], Step [4999], Loss: 0.082506462931633, Training Accuracy: 95.645
[ Tue Jul 16 04:26:38 2024 ] 	Batch(5000/6809) done. Loss: 0.2833  lr:0.000100
[ Tue Jul 16 04:27:02 2024 ] 	Batch(5100/6809) done. Loss: 0.3637  lr:0.000100
[ Tue Jul 16 04:27:25 2024 ] 	Batch(5200/6809) done. Loss: 0.4680  lr:0.000100
[ Tue Jul 16 04:27:49 2024 ] 	Batch(5300/6809) done. Loss: 0.0464  lr:0.000100
[ Tue Jul 16 04:28:12 2024 ] 	Batch(5400/6809) done. Loss: 0.2861  lr:0.000100
[ Tue Jul 16 04:28:36 2024 ] 
Training: Epoch [87/150], Step [5499], Loss: 0.2319350242614746, Training Accuracy: 95.62727272727273
[ Tue Jul 16 04:28:36 2024 ] 	Batch(5500/6809) done. Loss: 0.1448  lr:0.000100
[ Tue Jul 16 04:28:59 2024 ] 	Batch(5600/6809) done. Loss: 0.0912  lr:0.000100
[ Tue Jul 16 04:29:22 2024 ] 	Batch(5700/6809) done. Loss: 0.1639  lr:0.000100
[ Tue Jul 16 04:29:45 2024 ] 	Batch(5800/6809) done. Loss: 0.2822  lr:0.000100
[ Tue Jul 16 04:30:07 2024 ] 	Batch(5900/6809) done. Loss: 0.0239  lr:0.000100
[ Tue Jul 16 04:30:30 2024 ] 
Training: Epoch [87/150], Step [5999], Loss: 0.20574195683002472, Training Accuracy: 95.59791666666668
[ Tue Jul 16 04:30:30 2024 ] 	Batch(6000/6809) done. Loss: 0.0053  lr:0.000100
[ Tue Jul 16 04:30:53 2024 ] 	Batch(6100/6809) done. Loss: 0.0240  lr:0.000100
[ Tue Jul 16 04:31:16 2024 ] 	Batch(6200/6809) done. Loss: 0.4260  lr:0.000100
[ Tue Jul 16 04:31:39 2024 ] 	Batch(6300/6809) done. Loss: 0.0088  lr:0.000100
[ Tue Jul 16 04:32:03 2024 ] 	Batch(6400/6809) done. Loss: 0.1723  lr:0.000100
[ Tue Jul 16 04:32:26 2024 ] 
Training: Epoch [87/150], Step [6499], Loss: 0.14307454228401184, Training Accuracy: 95.57692307692308
[ Tue Jul 16 04:32:26 2024 ] 	Batch(6500/6809) done. Loss: 0.0797  lr:0.000100
[ Tue Jul 16 04:32:49 2024 ] 	Batch(6600/6809) done. Loss: 0.0042  lr:0.000100
[ Tue Jul 16 04:33:12 2024 ] 	Batch(6700/6809) done. Loss: 0.0454  lr:0.000100
[ Tue Jul 16 04:33:35 2024 ] 	Batch(6800/6809) done. Loss: 0.2244  lr:0.000100
[ Tue Jul 16 04:33:37 2024 ] 	Mean training loss: 0.1558.
[ Tue Jul 16 04:33:37 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 04:33:37 2024 ] Training epoch: 89
[ Tue Jul 16 04:33:38 2024 ] 	Batch(0/6809) done. Loss: 0.1246  lr:0.000100
[ Tue Jul 16 04:34:01 2024 ] 	Batch(100/6809) done. Loss: 0.4487  lr:0.000100
[ Tue Jul 16 04:34:24 2024 ] 	Batch(200/6809) done. Loss: 0.2458  lr:0.000100
[ Tue Jul 16 04:34:47 2024 ] 	Batch(300/6809) done. Loss: 0.0308  lr:0.000100
[ Tue Jul 16 04:35:10 2024 ] 	Batch(400/6809) done. Loss: 0.2010  lr:0.000100
[ Tue Jul 16 04:35:32 2024 ] 
Training: Epoch [88/150], Step [499], Loss: 0.0689011663198471, Training Accuracy: 95.7
[ Tue Jul 16 04:35:33 2024 ] 	Batch(500/6809) done. Loss: 0.2096  lr:0.000100
[ Tue Jul 16 04:35:55 2024 ] 	Batch(600/6809) done. Loss: 0.0200  lr:0.000100
[ Tue Jul 16 04:36:18 2024 ] 	Batch(700/6809) done. Loss: 0.3965  lr:0.000100
[ Tue Jul 16 04:36:41 2024 ] 	Batch(800/6809) done. Loss: 0.0212  lr:0.000100
[ Tue Jul 16 04:37:04 2024 ] 	Batch(900/6809) done. Loss: 0.0745  lr:0.000100
[ Tue Jul 16 04:37:26 2024 ] 
Training: Epoch [88/150], Step [999], Loss: 0.1796272248029709, Training Accuracy: 95.72500000000001
[ Tue Jul 16 04:37:26 2024 ] 	Batch(1000/6809) done. Loss: 0.2134  lr:0.000100
[ Tue Jul 16 04:37:49 2024 ] 	Batch(1100/6809) done. Loss: 0.2070  lr:0.000100
[ Tue Jul 16 04:38:12 2024 ] 	Batch(1200/6809) done. Loss: 0.0691  lr:0.000100
[ Tue Jul 16 04:38:35 2024 ] 	Batch(1300/6809) done. Loss: 0.1063  lr:0.000100
[ Tue Jul 16 04:38:57 2024 ] 	Batch(1400/6809) done. Loss: 0.0889  lr:0.000100
[ Tue Jul 16 04:39:20 2024 ] 
Training: Epoch [88/150], Step [1499], Loss: 0.38573160767555237, Training Accuracy: 95.69166666666666
[ Tue Jul 16 04:39:20 2024 ] 	Batch(1500/6809) done. Loss: 0.0213  lr:0.000100
[ Tue Jul 16 04:39:43 2024 ] 	Batch(1600/6809) done. Loss: 0.0765  lr:0.000100
[ Tue Jul 16 04:40:05 2024 ] 	Batch(1700/6809) done. Loss: 0.0270  lr:0.000100
[ Tue Jul 16 04:40:28 2024 ] 	Batch(1800/6809) done. Loss: 0.0931  lr:0.000100
[ Tue Jul 16 04:40:51 2024 ] 	Batch(1900/6809) done. Loss: 0.3308  lr:0.000100
[ Tue Jul 16 04:41:14 2024 ] 
Training: Epoch [88/150], Step [1999], Loss: 0.2786671817302704, Training Accuracy: 95.625
[ Tue Jul 16 04:41:14 2024 ] 	Batch(2000/6809) done. Loss: 0.0179  lr:0.000100
[ Tue Jul 16 04:41:37 2024 ] 	Batch(2100/6809) done. Loss: 0.3372  lr:0.000100
[ Tue Jul 16 04:42:00 2024 ] 	Batch(2200/6809) done. Loss: 0.0542  lr:0.000100
[ Tue Jul 16 04:42:23 2024 ] 	Batch(2300/6809) done. Loss: 0.3465  lr:0.000100
[ Tue Jul 16 04:42:46 2024 ] 	Batch(2400/6809) done. Loss: 0.2859  lr:0.000100
[ Tue Jul 16 04:43:08 2024 ] 
Training: Epoch [88/150], Step [2499], Loss: 0.04547412693500519, Training Accuracy: 95.655
[ Tue Jul 16 04:43:09 2024 ] 	Batch(2500/6809) done. Loss: 0.2018  lr:0.000100
[ Tue Jul 16 04:43:31 2024 ] 	Batch(2600/6809) done. Loss: 0.0949  lr:0.000100
[ Tue Jul 16 04:43:54 2024 ] 	Batch(2700/6809) done. Loss: 0.6794  lr:0.000100
[ Tue Jul 16 04:44:17 2024 ] 	Batch(2800/6809) done. Loss: 0.0280  lr:0.000100
[ Tue Jul 16 04:44:41 2024 ] 	Batch(2900/6809) done. Loss: 0.1043  lr:0.000100
[ Tue Jul 16 04:45:04 2024 ] 
Training: Epoch [88/150], Step [2999], Loss: 0.19487294554710388, Training Accuracy: 95.6375
[ Tue Jul 16 04:45:04 2024 ] 	Batch(3000/6809) done. Loss: 0.1846  lr:0.000100
[ Tue Jul 16 04:45:27 2024 ] 	Batch(3100/6809) done. Loss: 0.4083  lr:0.000100
[ Tue Jul 16 04:45:50 2024 ] 	Batch(3200/6809) done. Loss: 0.2083  lr:0.000100
[ Tue Jul 16 04:46:13 2024 ] 	Batch(3300/6809) done. Loss: 0.3628  lr:0.000100
[ Tue Jul 16 04:46:36 2024 ] 	Batch(3400/6809) done. Loss: 0.1471  lr:0.000100
[ Tue Jul 16 04:46:59 2024 ] 
Training: Epoch [88/150], Step [3499], Loss: 0.04496191442012787, Training Accuracy: 95.74285714285715
[ Tue Jul 16 04:47:00 2024 ] 	Batch(3500/6809) done. Loss: 0.1393  lr:0.000100
[ Tue Jul 16 04:47:23 2024 ] 	Batch(3600/6809) done. Loss: 0.2319  lr:0.000100
[ Tue Jul 16 04:47:46 2024 ] 	Batch(3700/6809) done. Loss: 0.0690  lr:0.000100
[ Tue Jul 16 04:48:09 2024 ] 	Batch(3800/6809) done. Loss: 0.0973  lr:0.000100
[ Tue Jul 16 04:48:32 2024 ] 	Batch(3900/6809) done. Loss: 0.1136  lr:0.000100
[ Tue Jul 16 04:48:55 2024 ] 
Training: Epoch [88/150], Step [3999], Loss: 0.17577052116394043, Training Accuracy: 95.79375
[ Tue Jul 16 04:48:55 2024 ] 	Batch(4000/6809) done. Loss: 0.0474  lr:0.000100
[ Tue Jul 16 04:49:19 2024 ] 	Batch(4100/6809) done. Loss: 0.0427  lr:0.000100
[ Tue Jul 16 04:49:42 2024 ] 	Batch(4200/6809) done. Loss: 0.2441  lr:0.000100
[ Tue Jul 16 04:50:05 2024 ] 	Batch(4300/6809) done. Loss: 0.5296  lr:0.000100
[ Tue Jul 16 04:50:28 2024 ] 	Batch(4400/6809) done. Loss: 0.0395  lr:0.000100
[ Tue Jul 16 04:50:51 2024 ] 
Training: Epoch [88/150], Step [4499], Loss: 0.05234915763139725, Training Accuracy: 95.71666666666667
[ Tue Jul 16 04:50:51 2024 ] 	Batch(4500/6809) done. Loss: 0.4569  lr:0.000100
[ Tue Jul 16 04:51:14 2024 ] 	Batch(4600/6809) done. Loss: 0.1398  lr:0.000100
[ Tue Jul 16 04:51:37 2024 ] 	Batch(4700/6809) done. Loss: 0.1394  lr:0.000100
[ Tue Jul 16 04:51:59 2024 ] 	Batch(4800/6809) done. Loss: 0.0271  lr:0.000100
[ Tue Jul 16 04:52:22 2024 ] 	Batch(4900/6809) done. Loss: 0.0081  lr:0.000100
[ Tue Jul 16 04:52:45 2024 ] 
Training: Epoch [88/150], Step [4999], Loss: 0.048944007605314255, Training Accuracy: 95.7325
[ Tue Jul 16 04:52:45 2024 ] 	Batch(5000/6809) done. Loss: 0.0125  lr:0.000100
[ Tue Jul 16 04:53:08 2024 ] 	Batch(5100/6809) done. Loss: 0.0337  lr:0.000100
[ Tue Jul 16 04:53:31 2024 ] 	Batch(5200/6809) done. Loss: 0.2455  lr:0.000100
[ Tue Jul 16 04:53:54 2024 ] 	Batch(5300/6809) done. Loss: 0.2197  lr:0.000100
[ Tue Jul 16 04:54:16 2024 ] 	Batch(5400/6809) done. Loss: 0.0435  lr:0.000100
[ Tue Jul 16 04:54:39 2024 ] 
Training: Epoch [88/150], Step [5499], Loss: 0.056022439152002335, Training Accuracy: 95.7
[ Tue Jul 16 04:54:39 2024 ] 	Batch(5500/6809) done. Loss: 0.0239  lr:0.000100
[ Tue Jul 16 04:55:02 2024 ] 	Batch(5600/6809) done. Loss: 0.0819  lr:0.000100
[ Tue Jul 16 04:55:24 2024 ] 	Batch(5700/6809) done. Loss: 0.0364  lr:0.000100
[ Tue Jul 16 04:55:47 2024 ] 	Batch(5800/6809) done. Loss: 0.1983  lr:0.000100
[ Tue Jul 16 04:56:10 2024 ] 	Batch(5900/6809) done. Loss: 0.0219  lr:0.000100
[ Tue Jul 16 04:56:32 2024 ] 
Training: Epoch [88/150], Step [5999], Loss: 0.270658016204834, Training Accuracy: 95.69166666666666
[ Tue Jul 16 04:56:33 2024 ] 	Batch(6000/6809) done. Loss: 0.1898  lr:0.000100
[ Tue Jul 16 04:56:55 2024 ] 	Batch(6100/6809) done. Loss: 0.0687  lr:0.000100
[ Tue Jul 16 04:57:18 2024 ] 	Batch(6200/6809) done. Loss: 0.0732  lr:0.000100
[ Tue Jul 16 04:57:41 2024 ] 	Batch(6300/6809) done. Loss: 0.0586  lr:0.000100
[ Tue Jul 16 04:58:04 2024 ] 	Batch(6400/6809) done. Loss: 0.3771  lr:0.000100
[ Tue Jul 16 04:58:26 2024 ] 
Training: Epoch [88/150], Step [6499], Loss: 0.013519614934921265, Training Accuracy: 95.63846153846154
[ Tue Jul 16 04:58:27 2024 ] 	Batch(6500/6809) done. Loss: 0.0105  lr:0.000100
[ Tue Jul 16 04:58:50 2024 ] 	Batch(6600/6809) done. Loss: 0.3165  lr:0.000100
[ Tue Jul 16 04:59:13 2024 ] 	Batch(6700/6809) done. Loss: 0.0911  lr:0.000100
[ Tue Jul 16 04:59:36 2024 ] 	Batch(6800/6809) done. Loss: 0.4225  lr:0.000100
[ Tue Jul 16 04:59:38 2024 ] 	Mean training loss: 0.1583.
[ Tue Jul 16 04:59:38 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 04:59:38 2024 ] Training epoch: 90
[ Tue Jul 16 04:59:39 2024 ] 	Batch(0/6809) done. Loss: 0.4507  lr:0.000100
[ Tue Jul 16 05:00:02 2024 ] 	Batch(100/6809) done. Loss: 0.0879  lr:0.000100
[ Tue Jul 16 05:00:25 2024 ] 	Batch(200/6809) done. Loss: 0.0121  lr:0.000100
[ Tue Jul 16 05:00:48 2024 ] 	Batch(300/6809) done. Loss: 0.4330  lr:0.000100
[ Tue Jul 16 05:01:11 2024 ] 	Batch(400/6809) done. Loss: 0.3151  lr:0.000100
[ Tue Jul 16 05:01:33 2024 ] 
Training: Epoch [89/150], Step [499], Loss: 0.3523474335670471, Training Accuracy: 95.25
[ Tue Jul 16 05:01:34 2024 ] 	Batch(500/6809) done. Loss: 0.2002  lr:0.000100
[ Tue Jul 16 05:01:57 2024 ] 	Batch(600/6809) done. Loss: 0.3963  lr:0.000100
[ Tue Jul 16 05:02:20 2024 ] 	Batch(700/6809) done. Loss: 0.3170  lr:0.000100
[ Tue Jul 16 05:02:42 2024 ] 	Batch(800/6809) done. Loss: 0.0632  lr:0.000100
[ Tue Jul 16 05:03:05 2024 ] 	Batch(900/6809) done. Loss: 0.0293  lr:0.000100
[ Tue Jul 16 05:03:28 2024 ] 
Training: Epoch [89/150], Step [999], Loss: 0.1099514439702034, Training Accuracy: 95.39999999999999
[ Tue Jul 16 05:03:28 2024 ] 	Batch(1000/6809) done. Loss: 0.2516  lr:0.000100
[ Tue Jul 16 05:03:51 2024 ] 	Batch(1100/6809) done. Loss: 0.3035  lr:0.000100
[ Tue Jul 16 05:04:14 2024 ] 	Batch(1200/6809) done. Loss: 0.1464  lr:0.000100
[ Tue Jul 16 05:04:36 2024 ] 	Batch(1300/6809) done. Loss: 0.2774  lr:0.000100
[ Tue Jul 16 05:04:59 2024 ] 	Batch(1400/6809) done. Loss: 0.1864  lr:0.000100
[ Tue Jul 16 05:05:23 2024 ] 
Training: Epoch [89/150], Step [1499], Loss: 0.07966364175081253, Training Accuracy: 95.51666666666667
[ Tue Jul 16 05:05:23 2024 ] 	Batch(1500/6809) done. Loss: 0.0656  lr:0.000100
[ Tue Jul 16 05:05:46 2024 ] 	Batch(1600/6809) done. Loss: 0.0659  lr:0.000100
[ Tue Jul 16 05:06:09 2024 ] 	Batch(1700/6809) done. Loss: 0.1810  lr:0.000100
[ Tue Jul 16 05:06:32 2024 ] 	Batch(1800/6809) done. Loss: 0.1706  lr:0.000100
[ Tue Jul 16 05:06:54 2024 ] 	Batch(1900/6809) done. Loss: 0.1286  lr:0.000100
[ Tue Jul 16 05:07:17 2024 ] 
Training: Epoch [89/150], Step [1999], Loss: 0.04016956314444542, Training Accuracy: 95.525
[ Tue Jul 16 05:07:17 2024 ] 	Batch(2000/6809) done. Loss: 0.0861  lr:0.000100
[ Tue Jul 16 05:07:40 2024 ] 	Batch(2100/6809) done. Loss: 0.1495  lr:0.000100
[ Tue Jul 16 05:08:02 2024 ] 	Batch(2200/6809) done. Loss: 0.0049  lr:0.000100
[ Tue Jul 16 05:08:25 2024 ] 	Batch(2300/6809) done. Loss: 0.3001  lr:0.000100
[ Tue Jul 16 05:08:48 2024 ] 	Batch(2400/6809) done. Loss: 0.0145  lr:0.000100
[ Tue Jul 16 05:09:10 2024 ] 
Training: Epoch [89/150], Step [2499], Loss: 0.07300229370594025, Training Accuracy: 95.545
[ Tue Jul 16 05:09:10 2024 ] 	Batch(2500/6809) done. Loss: 0.0255  lr:0.000100
[ Tue Jul 16 05:09:33 2024 ] 	Batch(2600/6809) done. Loss: 0.0198  lr:0.000100
[ Tue Jul 16 05:09:55 2024 ] 	Batch(2700/6809) done. Loss: 0.4593  lr:0.000100
[ Tue Jul 16 05:10:18 2024 ] 	Batch(2800/6809) done. Loss: 0.1075  lr:0.000100
[ Tue Jul 16 05:10:40 2024 ] 	Batch(2900/6809) done. Loss: 0.1081  lr:0.000100
[ Tue Jul 16 05:11:03 2024 ] 
Training: Epoch [89/150], Step [2999], Loss: 0.06313051283359528, Training Accuracy: 95.61666666666667
[ Tue Jul 16 05:11:03 2024 ] 	Batch(3000/6809) done. Loss: 0.0482  lr:0.000100
[ Tue Jul 16 05:11:26 2024 ] 	Batch(3100/6809) done. Loss: 0.0206  lr:0.000100
[ Tue Jul 16 05:11:48 2024 ] 	Batch(3200/6809) done. Loss: 0.2080  lr:0.000100
[ Tue Jul 16 05:12:11 2024 ] 	Batch(3300/6809) done. Loss: 0.0835  lr:0.000100
[ Tue Jul 16 05:12:33 2024 ] 	Batch(3400/6809) done. Loss: 0.0348  lr:0.000100
[ Tue Jul 16 05:12:56 2024 ] 
Training: Epoch [89/150], Step [3499], Loss: 0.07015196979045868, Training Accuracy: 95.625
[ Tue Jul 16 05:12:56 2024 ] 	Batch(3500/6809) done. Loss: 0.3180  lr:0.000100
[ Tue Jul 16 05:13:19 2024 ] 	Batch(3600/6809) done. Loss: 0.1653  lr:0.000100
[ Tue Jul 16 05:13:42 2024 ] 	Batch(3700/6809) done. Loss: 0.0370  lr:0.000100
[ Tue Jul 16 05:14:04 2024 ] 	Batch(3800/6809) done. Loss: 0.2421  lr:0.000100
[ Tue Jul 16 05:14:27 2024 ] 	Batch(3900/6809) done. Loss: 0.2835  lr:0.000100
[ Tue Jul 16 05:14:50 2024 ] 
Training: Epoch [89/150], Step [3999], Loss: 0.00473601883277297, Training Accuracy: 95.675
[ Tue Jul 16 05:14:50 2024 ] 	Batch(4000/6809) done. Loss: 0.1705  lr:0.000100
[ Tue Jul 16 05:15:13 2024 ] 	Batch(4100/6809) done. Loss: 0.4504  lr:0.000100
[ Tue Jul 16 05:15:35 2024 ] 	Batch(4200/6809) done. Loss: 0.0196  lr:0.000100
[ Tue Jul 16 05:15:58 2024 ] 	Batch(4300/6809) done. Loss: 0.0396  lr:0.000100
[ Tue Jul 16 05:16:21 2024 ] 	Batch(4400/6809) done. Loss: 0.0554  lr:0.000100
[ Tue Jul 16 05:16:43 2024 ] 
Training: Epoch [89/150], Step [4499], Loss: 0.028380468487739563, Training Accuracy: 95.63055555555555
[ Tue Jul 16 05:16:43 2024 ] 	Batch(4500/6809) done. Loss: 0.2391  lr:0.000100
[ Tue Jul 16 05:17:06 2024 ] 	Batch(4600/6809) done. Loss: 0.0201  lr:0.000100
[ Tue Jul 16 05:17:28 2024 ] 	Batch(4700/6809) done. Loss: 0.0824  lr:0.000100
[ Tue Jul 16 05:17:51 2024 ] 	Batch(4800/6809) done. Loss: 0.0243  lr:0.000100
[ Tue Jul 16 05:18:14 2024 ] 	Batch(4900/6809) done. Loss: 0.0569  lr:0.000100
[ Tue Jul 16 05:18:36 2024 ] 
Training: Epoch [89/150], Step [4999], Loss: 0.13819965720176697, Training Accuracy: 95.665
[ Tue Jul 16 05:18:36 2024 ] 	Batch(5000/6809) done. Loss: 0.0320  lr:0.000100
[ Tue Jul 16 05:18:59 2024 ] 	Batch(5100/6809) done. Loss: 0.0424  lr:0.000100
[ Tue Jul 16 05:19:22 2024 ] 	Batch(5200/6809) done. Loss: 0.0142  lr:0.000100
[ Tue Jul 16 05:19:44 2024 ] 	Batch(5300/6809) done. Loss: 0.0260  lr:0.000100
[ Tue Jul 16 05:20:07 2024 ] 	Batch(5400/6809) done. Loss: 0.1668  lr:0.000100
[ Tue Jul 16 05:20:30 2024 ] 
Training: Epoch [89/150], Step [5499], Loss: 0.09681180864572525, Training Accuracy: 95.67272727272727
[ Tue Jul 16 05:20:30 2024 ] 	Batch(5500/6809) done. Loss: 0.0851  lr:0.000100
[ Tue Jul 16 05:20:54 2024 ] 	Batch(5600/6809) done. Loss: 0.0398  lr:0.000100
[ Tue Jul 16 05:21:17 2024 ] 	Batch(5700/6809) done. Loss: 0.0812  lr:0.000100
[ Tue Jul 16 05:21:40 2024 ] 	Batch(5800/6809) done. Loss: 0.0867  lr:0.000100
[ Tue Jul 16 05:22:03 2024 ] 	Batch(5900/6809) done. Loss: 0.1392  lr:0.000100
[ Tue Jul 16 05:22:25 2024 ] 
Training: Epoch [89/150], Step [5999], Loss: 0.10673888772726059, Training Accuracy: 95.65
[ Tue Jul 16 05:22:26 2024 ] 	Batch(6000/6809) done. Loss: 0.1726  lr:0.000100
[ Tue Jul 16 05:22:49 2024 ] 	Batch(6100/6809) done. Loss: 0.4410  lr:0.000100
[ Tue Jul 16 05:23:12 2024 ] 	Batch(6200/6809) done. Loss: 0.0936  lr:0.000100
[ Tue Jul 16 05:23:35 2024 ] 	Batch(6300/6809) done. Loss: 0.1862  lr:0.000100
[ Tue Jul 16 05:23:58 2024 ] 	Batch(6400/6809) done. Loss: 0.0556  lr:0.000100
[ Tue Jul 16 05:24:21 2024 ] 
Training: Epoch [89/150], Step [6499], Loss: 0.031181806698441505, Training Accuracy: 95.65769230769232
[ Tue Jul 16 05:24:21 2024 ] 	Batch(6500/6809) done. Loss: 0.0756  lr:0.000100
[ Tue Jul 16 05:24:44 2024 ] 	Batch(6600/6809) done. Loss: 0.0484  lr:0.000100
[ Tue Jul 16 05:25:07 2024 ] 	Batch(6700/6809) done. Loss: 0.2050  lr:0.000100
[ Tue Jul 16 05:25:29 2024 ] 	Batch(6800/6809) done. Loss: 0.2617  lr:0.000100
[ Tue Jul 16 05:25:31 2024 ] 	Mean training loss: 0.1518.
[ Tue Jul 16 05:25:31 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 05:25:31 2024 ] Eval epoch: 90
[ Tue Jul 16 05:32:27 2024 ] 	Mean val loss of 7435 batches: 0.9150178534593466.
[ Tue Jul 16 05:32:27 2024 ] 
Validation: Epoch [89/150], Samples [47252.0/59477], Loss: 2.807328701019287, Validation Accuracy: 79.44583620559207
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 1 : 388 / 500 = 77 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 2 : 433 / 499 = 86 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 3 : 409 / 500 = 81 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 4 : 425 / 502 = 84 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 5 : 420 / 502 = 83 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 6 : 422 / 502 = 84 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 7 : 465 / 497 = 93 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 8 : 487 / 498 = 97 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 9 : 388 / 500 = 77 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 10 : 295 / 500 = 59 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 11 : 208 / 498 = 41 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 12 : 405 / 499 = 81 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 13 : 484 / 502 = 96 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 14 : 483 / 504 = 95 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 15 : 364 / 502 = 72 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 16 : 365 / 502 = 72 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 17 : 437 / 504 = 86 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 18 : 413 / 504 = 81 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 19 : 428 / 502 = 85 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 20 : 458 / 502 = 91 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 21 : 471 / 503 = 93 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 22 : 438 / 504 = 86 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 23 : 428 / 503 = 85 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 24 : 433 / 504 = 85 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 25 : 477 / 504 = 94 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 26 : 463 / 504 = 91 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 27 : 430 / 501 = 85 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 28 : 363 / 502 = 72 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 29 : 335 / 502 = 66 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 30 : 382 / 501 = 76 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 31 : 435 / 504 = 86 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 32 : 435 / 503 = 86 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 33 : 406 / 503 = 80 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 34 : 478 / 504 = 94 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 35 : 459 / 503 = 91 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 36 : 392 / 502 = 78 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 37 : 445 / 504 = 88 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 38 : 440 / 504 = 87 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 39 : 462 / 498 = 92 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 40 : 396 / 504 = 78 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 41 : 476 / 503 = 94 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 42 : 458 / 504 = 90 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 43 : 360 / 503 = 71 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 44 : 427 / 504 = 84 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 45 : 424 / 504 = 84 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 46 : 403 / 504 = 79 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 47 : 343 / 503 = 68 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 48 : 418 / 503 = 83 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 49 : 412 / 499 = 82 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 50 : 405 / 502 = 80 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 51 : 460 / 503 = 91 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 52 : 471 / 504 = 93 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 53 : 459 / 497 = 92 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 54 : 447 / 480 = 93 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 55 : 444 / 504 = 88 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 56 : 414 / 503 = 82 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 57 : 466 / 504 = 92 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 58 : 482 / 499 = 96 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 59 : 484 / 503 = 96 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 60 : 397 / 479 = 82 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 61 : 402 / 484 = 83 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 62 : 395 / 487 = 81 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 63 : 437 / 489 = 89 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 64 : 394 / 488 = 80 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 65 : 426 / 490 = 86 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 66 : 306 / 488 = 62 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 67 : 342 / 490 = 69 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 68 : 298 / 490 = 60 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 69 : 380 / 490 = 77 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 70 : 157 / 490 = 32 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 71 : 296 / 490 = 60 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 72 : 199 / 488 = 40 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 73 : 235 / 486 = 48 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 74 : 273 / 481 = 56 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 75 : 281 / 488 = 57 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 76 : 322 / 489 = 65 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 77 : 317 / 488 = 64 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 78 : 376 / 488 = 77 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 79 : 455 / 490 = 92 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 80 : 383 / 489 = 78 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 81 : 275 / 491 = 56 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 82 : 309 / 491 = 62 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 83 : 255 / 489 = 52 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 84 : 332 / 489 = 67 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 85 : 350 / 489 = 71 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 86 : 419 / 491 = 85 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 87 : 413 / 492 = 83 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 88 : 358 / 491 = 72 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 89 : 359 / 492 = 72 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 90 : 262 / 490 = 53 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 91 : 358 / 482 = 74 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 92 : 359 / 490 = 73 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 93 : 330 / 487 = 67 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 94 : 428 / 489 = 87 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 95 : 406 / 490 = 82 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 96 : 458 / 491 = 93 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 97 : 458 / 490 = 93 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 98 : 437 / 491 = 89 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 99 : 443 / 491 = 90 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 100 : 442 / 491 = 90 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 101 : 411 / 491 = 83 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 102 : 274 / 492 = 55 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 103 : 388 / 492 = 78 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 104 : 307 / 491 = 62 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 105 : 267 / 491 = 54 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 106 : 329 / 492 = 66 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 107 : 419 / 491 = 85 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 108 : 367 / 492 = 74 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 109 : 341 / 490 = 69 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 110 : 399 / 491 = 81 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 111 : 445 / 492 = 90 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 112 : 456 / 492 = 92 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 113 : 441 / 491 = 89 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 114 : 397 / 491 = 80 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 115 : 414 / 492 = 84 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 116 : 419 / 491 = 85 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 117 : 398 / 492 = 80 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 118 : 404 / 490 = 82 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 119 : 447 / 492 = 90 %
[ Tue Jul 16 05:32:27 2024 ] Accuracy of 120 : 409 / 500 = 81 %
[ Tue Jul 16 05:32:27 2024 ] Training epoch: 91
[ Tue Jul 16 05:32:28 2024 ] 	Batch(0/6809) done. Loss: 0.1076  lr:0.000001
[ Tue Jul 16 05:32:51 2024 ] 	Batch(100/6809) done. Loss: 0.0119  lr:0.000001
[ Tue Jul 16 05:33:14 2024 ] 	Batch(200/6809) done. Loss: 0.0881  lr:0.000001
[ Tue Jul 16 05:33:37 2024 ] 	Batch(300/6809) done. Loss: 0.1107  lr:0.000001
[ Tue Jul 16 05:34:01 2024 ] 	Batch(400/6809) done. Loss: 0.1388  lr:0.000001
[ Tue Jul 16 05:34:23 2024 ] 
Training: Epoch [90/150], Step [499], Loss: 0.059458568692207336, Training Accuracy: 95.75
[ Tue Jul 16 05:34:23 2024 ] 	Batch(500/6809) done. Loss: 0.5083  lr:0.000001
[ Tue Jul 16 05:34:46 2024 ] 	Batch(600/6809) done. Loss: 0.1331  lr:0.000001
[ Tue Jul 16 05:35:09 2024 ] 	Batch(700/6809) done. Loss: 0.0962  lr:0.000001
[ Tue Jul 16 05:35:32 2024 ] 	Batch(800/6809) done. Loss: 0.0056  lr:0.000001
[ Tue Jul 16 05:35:55 2024 ] 	Batch(900/6809) done. Loss: 0.7123  lr:0.000001
[ Tue Jul 16 05:36:18 2024 ] 
Training: Epoch [90/150], Step [999], Loss: 0.1884797215461731, Training Accuracy: 96.0625
[ Tue Jul 16 05:36:18 2024 ] 	Batch(1000/6809) done. Loss: 0.4656  lr:0.000001
[ Tue Jul 16 05:36:41 2024 ] 	Batch(1100/6809) done. Loss: 0.1233  lr:0.000001
[ Tue Jul 16 05:37:04 2024 ] 	Batch(1200/6809) done. Loss: 0.0192  lr:0.000001
[ Tue Jul 16 05:37:27 2024 ] 	Batch(1300/6809) done. Loss: 0.0674  lr:0.000001
[ Tue Jul 16 05:37:50 2024 ] 	Batch(1400/6809) done. Loss: 0.0336  lr:0.000001
[ Tue Jul 16 05:38:13 2024 ] 
Training: Epoch [90/150], Step [1499], Loss: 0.40480974316596985, Training Accuracy: 96.05
[ Tue Jul 16 05:38:13 2024 ] 	Batch(1500/6809) done. Loss: 0.0734  lr:0.000001
[ Tue Jul 16 05:38:36 2024 ] 	Batch(1600/6809) done. Loss: 0.1500  lr:0.000001
[ Tue Jul 16 05:38:59 2024 ] 	Batch(1700/6809) done. Loss: 0.0749  lr:0.000001
[ Tue Jul 16 05:39:22 2024 ] 	Batch(1800/6809) done. Loss: 0.1990  lr:0.000001
[ Tue Jul 16 05:39:44 2024 ] 	Batch(1900/6809) done. Loss: 0.2504  lr:0.000001
[ Tue Jul 16 05:40:07 2024 ] 
Training: Epoch [90/150], Step [1999], Loss: 0.15968097746372223, Training Accuracy: 95.89375
[ Tue Jul 16 05:40:07 2024 ] 	Batch(2000/6809) done. Loss: 0.0446  lr:0.000001
[ Tue Jul 16 05:40:29 2024 ] 	Batch(2100/6809) done. Loss: 0.0934  lr:0.000001
[ Tue Jul 16 05:40:52 2024 ] 	Batch(2200/6809) done. Loss: 0.0237  lr:0.000001
[ Tue Jul 16 05:41:15 2024 ] 	Batch(2300/6809) done. Loss: 0.1194  lr:0.000001
[ Tue Jul 16 05:41:38 2024 ] 	Batch(2400/6809) done. Loss: 0.4821  lr:0.000001
[ Tue Jul 16 05:42:00 2024 ] 
Training: Epoch [90/150], Step [2499], Loss: 0.10466508567333221, Training Accuracy: 95.89
[ Tue Jul 16 05:42:01 2024 ] 	Batch(2500/6809) done. Loss: 0.1535  lr:0.000001
[ Tue Jul 16 05:42:23 2024 ] 	Batch(2600/6809) done. Loss: 0.0160  lr:0.000001
[ Tue Jul 16 05:42:46 2024 ] 	Batch(2700/6809) done. Loss: 0.4307  lr:0.000001
[ Tue Jul 16 05:43:09 2024 ] 	Batch(2800/6809) done. Loss: 0.0239  lr:0.000001
[ Tue Jul 16 05:43:31 2024 ] 	Batch(2900/6809) done. Loss: 0.4555  lr:0.000001
[ Tue Jul 16 05:43:54 2024 ] 
Training: Epoch [90/150], Step [2999], Loss: 0.33088141679763794, Training Accuracy: 95.825
[ Tue Jul 16 05:43:54 2024 ] 	Batch(3000/6809) done. Loss: 0.0306  lr:0.000001
[ Tue Jul 16 05:44:17 2024 ] 	Batch(3100/6809) done. Loss: 0.0504  lr:0.000001
[ Tue Jul 16 05:44:40 2024 ] 	Batch(3200/6809) done. Loss: 0.0433  lr:0.000001
[ Tue Jul 16 05:45:03 2024 ] 	Batch(3300/6809) done. Loss: 0.0556  lr:0.000001
[ Tue Jul 16 05:45:25 2024 ] 	Batch(3400/6809) done. Loss: 0.0237  lr:0.000001
[ Tue Jul 16 05:45:48 2024 ] 
Training: Epoch [90/150], Step [3499], Loss: 0.3890976905822754, Training Accuracy: 95.77857142857142
[ Tue Jul 16 05:45:48 2024 ] 	Batch(3500/6809) done. Loss: 0.2626  lr:0.000001
[ Tue Jul 16 05:46:11 2024 ] 	Batch(3600/6809) done. Loss: 0.0526  lr:0.000001
[ Tue Jul 16 05:46:33 2024 ] 	Batch(3700/6809) done. Loss: 0.0163  lr:0.000001
[ Tue Jul 16 05:46:56 2024 ] 	Batch(3800/6809) done. Loss: 0.0495  lr:0.000001
[ Tue Jul 16 05:47:19 2024 ] 	Batch(3900/6809) done. Loss: 0.1912  lr:0.000001
[ Tue Jul 16 05:47:42 2024 ] 
Training: Epoch [90/150], Step [3999], Loss: 0.4169313311576843, Training Accuracy: 95.765625
[ Tue Jul 16 05:47:42 2024 ] 	Batch(4000/6809) done. Loss: 0.0103  lr:0.000001
[ Tue Jul 16 05:48:05 2024 ] 	Batch(4100/6809) done. Loss: 0.1422  lr:0.000001
[ Tue Jul 16 05:48:28 2024 ] 	Batch(4200/6809) done. Loss: 0.0262  lr:0.000001
[ Tue Jul 16 05:48:51 2024 ] 	Batch(4300/6809) done. Loss: 0.2850  lr:0.000001
[ Tue Jul 16 05:49:14 2024 ] 	Batch(4400/6809) done. Loss: 0.0352  lr:0.000001
[ Tue Jul 16 05:49:37 2024 ] 
Training: Epoch [90/150], Step [4499], Loss: 0.3506476879119873, Training Accuracy: 95.74722222222222
[ Tue Jul 16 05:49:37 2024 ] 	Batch(4500/6809) done. Loss: 0.0765  lr:0.000001
[ Tue Jul 16 05:50:00 2024 ] 	Batch(4600/6809) done. Loss: 0.0577  lr:0.000001
[ Tue Jul 16 05:50:23 2024 ] 	Batch(4700/6809) done. Loss: 0.0470  lr:0.000001
[ Tue Jul 16 05:50:46 2024 ] 	Batch(4800/6809) done. Loss: 0.2239  lr:0.000001
[ Tue Jul 16 05:51:09 2024 ] 	Batch(4900/6809) done. Loss: 0.0104  lr:0.000001
[ Tue Jul 16 05:51:32 2024 ] 
Training: Epoch [90/150], Step [4999], Loss: 0.007347926963120699, Training Accuracy: 95.72500000000001
[ Tue Jul 16 05:51:32 2024 ] 	Batch(5000/6809) done. Loss: 0.0637  lr:0.000001
[ Tue Jul 16 05:51:55 2024 ] 	Batch(5100/6809) done. Loss: 0.0460  lr:0.000001
[ Tue Jul 16 05:52:18 2024 ] 	Batch(5200/6809) done. Loss: 0.0088  lr:0.000001
[ Tue Jul 16 05:52:41 2024 ] 	Batch(5300/6809) done. Loss: 0.1779  lr:0.000001
[ Tue Jul 16 05:53:04 2024 ] 	Batch(5400/6809) done. Loss: 0.4825  lr:0.000001
[ Tue Jul 16 05:53:27 2024 ] 
Training: Epoch [90/150], Step [5499], Loss: 0.021248072385787964, Training Accuracy: 95.69772727272728
[ Tue Jul 16 05:53:27 2024 ] 	Batch(5500/6809) done. Loss: 0.2273  lr:0.000001
[ Tue Jul 16 05:53:50 2024 ] 	Batch(5600/6809) done. Loss: 0.0492  lr:0.000001
[ Tue Jul 16 05:54:13 2024 ] 	Batch(5700/6809) done. Loss: 0.3250  lr:0.000001
[ Tue Jul 16 05:54:36 2024 ] 	Batch(5800/6809) done. Loss: 0.1275  lr:0.000001
[ Tue Jul 16 05:54:59 2024 ] 	Batch(5900/6809) done. Loss: 0.3676  lr:0.000001
[ Tue Jul 16 05:55:21 2024 ] 
Training: Epoch [90/150], Step [5999], Loss: 0.04394788667559624, Training Accuracy: 95.7125
[ Tue Jul 16 05:55:22 2024 ] 	Batch(6000/6809) done. Loss: 0.2626  lr:0.000001
[ Tue Jul 16 05:55:44 2024 ] 	Batch(6100/6809) done. Loss: 0.0232  lr:0.000001
[ Tue Jul 16 05:56:07 2024 ] 	Batch(6200/6809) done. Loss: 0.0523  lr:0.000001
[ Tue Jul 16 05:56:29 2024 ] 	Batch(6300/6809) done. Loss: 0.0161  lr:0.000001
[ Tue Jul 16 05:56:52 2024 ] 	Batch(6400/6809) done. Loss: 0.2203  lr:0.000001
[ Tue Jul 16 05:57:14 2024 ] 
Training: Epoch [90/150], Step [6499], Loss: 0.07688590884208679, Training Accuracy: 95.67307692307693
[ Tue Jul 16 05:57:15 2024 ] 	Batch(6500/6809) done. Loss: 0.2577  lr:0.000001
[ Tue Jul 16 05:57:37 2024 ] 	Batch(6600/6809) done. Loss: 0.3060  lr:0.000001
[ Tue Jul 16 05:58:00 2024 ] 	Batch(6700/6809) done. Loss: 0.1786  lr:0.000001
[ Tue Jul 16 05:58:22 2024 ] 	Batch(6800/6809) done. Loss: 0.3874  lr:0.000001
[ Tue Jul 16 05:58:24 2024 ] 	Mean training loss: 0.1579.
[ Tue Jul 16 05:58:24 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 05:58:24 2024 ] Training epoch: 92
[ Tue Jul 16 05:58:25 2024 ] 	Batch(0/6809) done. Loss: 0.1016  lr:0.000001
[ Tue Jul 16 05:58:47 2024 ] 	Batch(100/6809) done. Loss: 0.0638  lr:0.000001
[ Tue Jul 16 05:59:10 2024 ] 	Batch(200/6809) done. Loss: 0.0926  lr:0.000001
[ Tue Jul 16 05:59:32 2024 ] 	Batch(300/6809) done. Loss: 0.1292  lr:0.000001
[ Tue Jul 16 05:59:55 2024 ] 	Batch(400/6809) done. Loss: 0.0348  lr:0.000001
[ Tue Jul 16 06:00:17 2024 ] 
Training: Epoch [91/150], Step [499], Loss: 0.1335630863904953, Training Accuracy: 95.92500000000001
[ Tue Jul 16 06:00:18 2024 ] 	Batch(500/6809) done. Loss: 0.0720  lr:0.000001
[ Tue Jul 16 06:00:41 2024 ] 	Batch(600/6809) done. Loss: 0.0518  lr:0.000001
[ Tue Jul 16 06:01:04 2024 ] 	Batch(700/6809) done. Loss: 0.1281  lr:0.000001
[ Tue Jul 16 06:01:27 2024 ] 	Batch(800/6809) done. Loss: 0.2283  lr:0.000001
[ Tue Jul 16 06:01:50 2024 ] 	Batch(900/6809) done. Loss: 0.0352  lr:0.000001
[ Tue Jul 16 06:02:13 2024 ] 
Training: Epoch [91/150], Step [999], Loss: 0.21191124618053436, Training Accuracy: 95.6
[ Tue Jul 16 06:02:13 2024 ] 	Batch(1000/6809) done. Loss: 0.5657  lr:0.000001
[ Tue Jul 16 06:02:36 2024 ] 	Batch(1100/6809) done. Loss: 0.0422  lr:0.000001
[ Tue Jul 16 06:02:59 2024 ] 	Batch(1200/6809) done. Loss: 0.2041  lr:0.000001
[ Tue Jul 16 06:03:22 2024 ] 	Batch(1300/6809) done. Loss: 0.1788  lr:0.000001
[ Tue Jul 16 06:03:45 2024 ] 	Batch(1400/6809) done. Loss: 0.0822  lr:0.000001
[ Tue Jul 16 06:04:07 2024 ] 
Training: Epoch [91/150], Step [1499], Loss: 0.036571551114320755, Training Accuracy: 95.54166666666667
[ Tue Jul 16 06:04:08 2024 ] 	Batch(1500/6809) done. Loss: 0.0320  lr:0.000001
[ Tue Jul 16 06:04:30 2024 ] 	Batch(1600/6809) done. Loss: 0.0291  lr:0.000001
[ Tue Jul 16 06:04:53 2024 ] 	Batch(1700/6809) done. Loss: 0.0188  lr:0.000001
[ Tue Jul 16 06:05:15 2024 ] 	Batch(1800/6809) done. Loss: 0.1805  lr:0.000001
[ Tue Jul 16 06:05:38 2024 ] 	Batch(1900/6809) done. Loss: 0.0937  lr:0.000001
[ Tue Jul 16 06:06:00 2024 ] 
Training: Epoch [91/150], Step [1999], Loss: 0.47659480571746826, Training Accuracy: 95.55
[ Tue Jul 16 06:06:01 2024 ] 	Batch(2000/6809) done. Loss: 0.3024  lr:0.000001
[ Tue Jul 16 06:06:23 2024 ] 	Batch(2100/6809) done. Loss: 0.1693  lr:0.000001
[ Tue Jul 16 06:06:46 2024 ] 	Batch(2200/6809) done. Loss: 0.0680  lr:0.000001
[ Tue Jul 16 06:07:08 2024 ] 	Batch(2300/6809) done. Loss: 0.3270  lr:0.000001
[ Tue Jul 16 06:07:31 2024 ] 	Batch(2400/6809) done. Loss: 0.0219  lr:0.000001
[ Tue Jul 16 06:07:53 2024 ] 
Training: Epoch [91/150], Step [2499], Loss: 0.12008779495954514, Training Accuracy: 95.50999999999999
[ Tue Jul 16 06:07:54 2024 ] 	Batch(2500/6809) done. Loss: 0.2468  lr:0.000001
[ Tue Jul 16 06:08:16 2024 ] 	Batch(2600/6809) done. Loss: 0.1155  lr:0.000001
[ Tue Jul 16 06:08:39 2024 ] 	Batch(2700/6809) done. Loss: 0.0234  lr:0.000001
[ Tue Jul 16 06:09:01 2024 ] 	Batch(2800/6809) done. Loss: 0.1828  lr:0.000001
[ Tue Jul 16 06:09:24 2024 ] 	Batch(2900/6809) done. Loss: 0.0809  lr:0.000001
[ Tue Jul 16 06:09:46 2024 ] 
Training: Epoch [91/150], Step [2999], Loss: 0.005607672967016697, Training Accuracy: 95.59583333333333
[ Tue Jul 16 06:09:46 2024 ] 	Batch(3000/6809) done. Loss: 0.3355  lr:0.000001
[ Tue Jul 16 06:10:09 2024 ] 	Batch(3100/6809) done. Loss: 0.0188  lr:0.000001
[ Tue Jul 16 06:10:31 2024 ] 	Batch(3200/6809) done. Loss: 0.1742  lr:0.000001
[ Tue Jul 16 06:10:54 2024 ] 	Batch(3300/6809) done. Loss: 0.6083  lr:0.000001
[ Tue Jul 16 06:11:17 2024 ] 	Batch(3400/6809) done. Loss: 0.0358  lr:0.000001
[ Tue Jul 16 06:11:39 2024 ] 
Training: Epoch [91/150], Step [3499], Loss: 0.17769449949264526, Training Accuracy: 95.6
[ Tue Jul 16 06:11:39 2024 ] 	Batch(3500/6809) done. Loss: 0.0215  lr:0.000001
[ Tue Jul 16 06:12:02 2024 ] 	Batch(3600/6809) done. Loss: 0.1379  lr:0.000001
[ Tue Jul 16 06:12:25 2024 ] 	Batch(3700/6809) done. Loss: 0.2504  lr:0.000001
[ Tue Jul 16 06:12:47 2024 ] 	Batch(3800/6809) done. Loss: 0.2200  lr:0.000001
[ Tue Jul 16 06:13:10 2024 ] 	Batch(3900/6809) done. Loss: 0.0411  lr:0.000001
[ Tue Jul 16 06:13:32 2024 ] 
Training: Epoch [91/150], Step [3999], Loss: 0.19401560723781586, Training Accuracy: 95.60625
[ Tue Jul 16 06:13:32 2024 ] 	Batch(4000/6809) done. Loss: 0.0132  lr:0.000001
[ Tue Jul 16 06:13:55 2024 ] 	Batch(4100/6809) done. Loss: 0.0246  lr:0.000001
[ Tue Jul 16 06:14:18 2024 ] 	Batch(4200/6809) done. Loss: 0.0772  lr:0.000001
[ Tue Jul 16 06:14:40 2024 ] 	Batch(4300/6809) done. Loss: 0.0760  lr:0.000001
[ Tue Jul 16 06:15:03 2024 ] 	Batch(4400/6809) done. Loss: 0.1590  lr:0.000001
[ Tue Jul 16 06:15:25 2024 ] 
Training: Epoch [91/150], Step [4499], Loss: 0.37958192825317383, Training Accuracy: 95.63333333333334
[ Tue Jul 16 06:15:26 2024 ] 	Batch(4500/6809) done. Loss: 0.1796  lr:0.000001
[ Tue Jul 16 06:15:49 2024 ] 	Batch(4600/6809) done. Loss: 0.2238  lr:0.000001
[ Tue Jul 16 06:16:12 2024 ] 	Batch(4700/6809) done. Loss: 0.1086  lr:0.000001
[ Tue Jul 16 06:16:35 2024 ] 	Batch(4800/6809) done. Loss: 0.2044  lr:0.000001
[ Tue Jul 16 06:16:57 2024 ] 	Batch(4900/6809) done. Loss: 0.0275  lr:0.000001
[ Tue Jul 16 06:17:19 2024 ] 
Training: Epoch [91/150], Step [4999], Loss: 0.018484069034457207, Training Accuracy: 95.6575
[ Tue Jul 16 06:17:20 2024 ] 	Batch(5000/6809) done. Loss: 0.0116  lr:0.000001
[ Tue Jul 16 06:17:42 2024 ] 	Batch(5100/6809) done. Loss: 0.0497  lr:0.000001
[ Tue Jul 16 06:18:05 2024 ] 	Batch(5200/6809) done. Loss: 0.0242  lr:0.000001
[ Tue Jul 16 06:18:27 2024 ] 	Batch(5300/6809) done. Loss: 0.6308  lr:0.000001
[ Tue Jul 16 06:18:50 2024 ] 	Batch(5400/6809) done. Loss: 0.0864  lr:0.000001
[ Tue Jul 16 06:19:12 2024 ] 
Training: Epoch [91/150], Step [5499], Loss: 0.040705420076847076, Training Accuracy: 95.65454545454546
[ Tue Jul 16 06:19:12 2024 ] 	Batch(5500/6809) done. Loss: 0.0038  lr:0.000001
[ Tue Jul 16 06:19:35 2024 ] 	Batch(5600/6809) done. Loss: 0.4217  lr:0.000001
[ Tue Jul 16 06:19:58 2024 ] 	Batch(5700/6809) done. Loss: 0.0350  lr:0.000001
[ Tue Jul 16 06:20:21 2024 ] 	Batch(5800/6809) done. Loss: 0.0743  lr:0.000001
[ Tue Jul 16 06:20:44 2024 ] 	Batch(5900/6809) done. Loss: 0.1710  lr:0.000001
[ Tue Jul 16 06:21:07 2024 ] 
Training: Epoch [91/150], Step [5999], Loss: 0.26035064458847046, Training Accuracy: 95.64583333333333
[ Tue Jul 16 06:21:08 2024 ] 	Batch(6000/6809) done. Loss: 0.0484  lr:0.000001
[ Tue Jul 16 06:21:31 2024 ] 	Batch(6100/6809) done. Loss: 0.0405  lr:0.000001
[ Tue Jul 16 06:21:54 2024 ] 	Batch(6200/6809) done. Loss: 0.1284  lr:0.000001
[ Tue Jul 16 06:22:17 2024 ] 	Batch(6300/6809) done. Loss: 0.1900  lr:0.000001
[ Tue Jul 16 06:22:41 2024 ] 	Batch(6400/6809) done. Loss: 0.0882  lr:0.000001
[ Tue Jul 16 06:23:03 2024 ] 
Training: Epoch [91/150], Step [6499], Loss: 0.047789204865694046, Training Accuracy: 95.65961538461538
[ Tue Jul 16 06:23:04 2024 ] 	Batch(6500/6809) done. Loss: 0.2538  lr:0.000001
[ Tue Jul 16 06:23:27 2024 ] 	Batch(6600/6809) done. Loss: 0.2660  lr:0.000001
[ Tue Jul 16 06:23:50 2024 ] 	Batch(6700/6809) done. Loss: 0.0264  lr:0.000001
[ Tue Jul 16 06:24:13 2024 ] 	Batch(6800/6809) done. Loss: 0.0587  lr:0.000001
[ Tue Jul 16 06:24:15 2024 ] 	Mean training loss: 0.1545.
[ Tue Jul 16 06:24:15 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 06:24:15 2024 ] Training epoch: 93
[ Tue Jul 16 06:24:16 2024 ] 	Batch(0/6809) done. Loss: 0.0037  lr:0.000001
[ Tue Jul 16 06:24:39 2024 ] 	Batch(100/6809) done. Loss: 0.0015  lr:0.000001
[ Tue Jul 16 06:25:02 2024 ] 	Batch(200/6809) done. Loss: 0.4559  lr:0.000001
[ Tue Jul 16 06:25:25 2024 ] 	Batch(300/6809) done. Loss: 0.0568  lr:0.000001
[ Tue Jul 16 06:25:49 2024 ] 	Batch(400/6809) done. Loss: 0.0954  lr:0.000001
[ Tue Jul 16 06:26:12 2024 ] 
Training: Epoch [92/150], Step [499], Loss: 0.4845463037490845, Training Accuracy: 94.975
[ Tue Jul 16 06:26:12 2024 ] 	Batch(500/6809) done. Loss: 0.0471  lr:0.000001
[ Tue Jul 16 06:26:35 2024 ] 	Batch(600/6809) done. Loss: 0.0353  lr:0.000001
[ Tue Jul 16 06:26:58 2024 ] 	Batch(700/6809) done. Loss: 0.0826  lr:0.000001
[ Tue Jul 16 06:27:21 2024 ] 	Batch(800/6809) done. Loss: 0.0295  lr:0.000001
[ Tue Jul 16 06:27:44 2024 ] 	Batch(900/6809) done. Loss: 0.0580  lr:0.000001
[ Tue Jul 16 06:28:07 2024 ] 
Training: Epoch [92/150], Step [999], Loss: 0.08457084745168686, Training Accuracy: 95.3875
[ Tue Jul 16 06:28:07 2024 ] 	Batch(1000/6809) done. Loss: 0.0405  lr:0.000001
[ Tue Jul 16 06:28:29 2024 ] 	Batch(1100/6809) done. Loss: 0.1320  lr:0.000001
[ Tue Jul 16 06:28:52 2024 ] 	Batch(1200/6809) done. Loss: 0.0243  lr:0.000001
[ Tue Jul 16 06:29:15 2024 ] 	Batch(1300/6809) done. Loss: 0.1721  lr:0.000001
[ Tue Jul 16 06:29:37 2024 ] 	Batch(1400/6809) done. Loss: 0.1345  lr:0.000001
[ Tue Jul 16 06:30:00 2024 ] 
Training: Epoch [92/150], Step [1499], Loss: 0.19502276182174683, Training Accuracy: 95.625
[ Tue Jul 16 06:30:00 2024 ] 	Batch(1500/6809) done. Loss: 0.4603  lr:0.000001
[ Tue Jul 16 06:30:23 2024 ] 	Batch(1600/6809) done. Loss: 0.1503  lr:0.000001
[ Tue Jul 16 06:30:45 2024 ] 	Batch(1700/6809) done. Loss: 0.0892  lr:0.000001
[ Tue Jul 16 06:31:08 2024 ] 	Batch(1800/6809) done. Loss: 0.0880  lr:0.000001
[ Tue Jul 16 06:31:31 2024 ] 	Batch(1900/6809) done. Loss: 0.1481  lr:0.000001
[ Tue Jul 16 06:31:53 2024 ] 
Training: Epoch [92/150], Step [1999], Loss: 0.01141496654599905, Training Accuracy: 95.68124999999999
[ Tue Jul 16 06:31:54 2024 ] 	Batch(2000/6809) done. Loss: 0.4845  lr:0.000001
[ Tue Jul 16 06:32:16 2024 ] 	Batch(2100/6809) done. Loss: 0.0998  lr:0.000001
[ Tue Jul 16 06:32:39 2024 ] 	Batch(2200/6809) done. Loss: 0.0138  lr:0.000001
[ Tue Jul 16 06:33:01 2024 ] 	Batch(2300/6809) done. Loss: 0.0202  lr:0.000001
[ Tue Jul 16 06:33:24 2024 ] 	Batch(2400/6809) done. Loss: 0.0296  lr:0.000001
[ Tue Jul 16 06:33:47 2024 ] 
Training: Epoch [92/150], Step [2499], Loss: 0.013369024731218815, Training Accuracy: 95.645
[ Tue Jul 16 06:33:47 2024 ] 	Batch(2500/6809) done. Loss: 0.1698  lr:0.000001
[ Tue Jul 16 06:34:11 2024 ] 	Batch(2600/6809) done. Loss: 0.0137  lr:0.000001
[ Tue Jul 16 06:34:34 2024 ] 	Batch(2700/6809) done. Loss: 0.0581  lr:0.000001
[ Tue Jul 16 06:34:58 2024 ] 	Batch(2800/6809) done. Loss: 0.2599  lr:0.000001
[ Tue Jul 16 06:35:21 2024 ] 	Batch(2900/6809) done. Loss: 0.1769  lr:0.000001
[ Tue Jul 16 06:35:44 2024 ] 
Training: Epoch [92/150], Step [2999], Loss: 0.3792601227760315, Training Accuracy: 95.61666666666667
[ Tue Jul 16 06:35:44 2024 ] 	Batch(3000/6809) done. Loss: 0.1828  lr:0.000001
[ Tue Jul 16 06:36:07 2024 ] 	Batch(3100/6809) done. Loss: 0.6412  lr:0.000001
[ Tue Jul 16 06:36:30 2024 ] 	Batch(3200/6809) done. Loss: 0.0446  lr:0.000001
[ Tue Jul 16 06:36:52 2024 ] 	Batch(3300/6809) done. Loss: 0.0462  lr:0.000001
[ Tue Jul 16 06:37:15 2024 ] 	Batch(3400/6809) done. Loss: 0.0924  lr:0.000001
[ Tue Jul 16 06:37:37 2024 ] 
Training: Epoch [92/150], Step [3499], Loss: 0.4743157625198364, Training Accuracy: 95.66071428571429
[ Tue Jul 16 06:37:38 2024 ] 	Batch(3500/6809) done. Loss: 0.1911  lr:0.000001
[ Tue Jul 16 06:38:00 2024 ] 	Batch(3600/6809) done. Loss: 0.0386  lr:0.000001
[ Tue Jul 16 06:38:23 2024 ] 	Batch(3700/6809) done. Loss: 0.1328  lr:0.000001
[ Tue Jul 16 06:38:46 2024 ] 	Batch(3800/6809) done. Loss: 0.0456  lr:0.000001
[ Tue Jul 16 06:39:08 2024 ] 	Batch(3900/6809) done. Loss: 0.1407  lr:0.000001
[ Tue Jul 16 06:39:31 2024 ] 
Training: Epoch [92/150], Step [3999], Loss: 0.4228871762752533, Training Accuracy: 95.7
[ Tue Jul 16 06:39:31 2024 ] 	Batch(4000/6809) done. Loss: 0.0370  lr:0.000001
[ Tue Jul 16 06:39:54 2024 ] 	Batch(4100/6809) done. Loss: 0.3161  lr:0.000001
[ Tue Jul 16 06:40:16 2024 ] 	Batch(4200/6809) done. Loss: 0.0879  lr:0.000001
[ Tue Jul 16 06:40:39 2024 ] 	Batch(4300/6809) done. Loss: 0.2100  lr:0.000001
[ Tue Jul 16 06:41:01 2024 ] 	Batch(4400/6809) done. Loss: 0.0124  lr:0.000001
[ Tue Jul 16 06:41:24 2024 ] 
Training: Epoch [92/150], Step [4499], Loss: 0.07750533521175385, Training Accuracy: 95.74722222222222
[ Tue Jul 16 06:41:24 2024 ] 	Batch(4500/6809) done. Loss: 0.0812  lr:0.000001
[ Tue Jul 16 06:41:47 2024 ] 	Batch(4600/6809) done. Loss: 0.1011  lr:0.000001
[ Tue Jul 16 06:42:09 2024 ] 	Batch(4700/6809) done. Loss: 0.0129  lr:0.000001
[ Tue Jul 16 06:42:32 2024 ] 	Batch(4800/6809) done. Loss: 0.1508  lr:0.000001
[ Tue Jul 16 06:42:54 2024 ] 	Batch(4900/6809) done. Loss: 0.2182  lr:0.000001
[ Tue Jul 16 06:43:17 2024 ] 
Training: Epoch [92/150], Step [4999], Loss: 0.008505229838192463, Training Accuracy: 95.72749999999999
[ Tue Jul 16 06:43:17 2024 ] 	Batch(5000/6809) done. Loss: 0.0319  lr:0.000001
[ Tue Jul 16 06:43:40 2024 ] 	Batch(5100/6809) done. Loss: 0.0086  lr:0.000001
[ Tue Jul 16 06:44:02 2024 ] 	Batch(5200/6809) done. Loss: 0.2829  lr:0.000001
[ Tue Jul 16 06:44:25 2024 ] 	Batch(5300/6809) done. Loss: 0.0215  lr:0.000001
[ Tue Jul 16 06:44:48 2024 ] 	Batch(5400/6809) done. Loss: 0.3546  lr:0.000001
[ Tue Jul 16 06:45:10 2024 ] 
Training: Epoch [92/150], Step [5499], Loss: 0.14087727665901184, Training Accuracy: 95.70227272727273
[ Tue Jul 16 06:45:10 2024 ] 	Batch(5500/6809) done. Loss: 0.3294  lr:0.000001
[ Tue Jul 16 06:45:33 2024 ] 	Batch(5600/6809) done. Loss: 0.2464  lr:0.000001
[ Tue Jul 16 06:45:56 2024 ] 	Batch(5700/6809) done. Loss: 0.2096  lr:0.000001
[ Tue Jul 16 06:46:18 2024 ] 	Batch(5800/6809) done. Loss: 0.2331  lr:0.000001
[ Tue Jul 16 06:46:41 2024 ] 	Batch(5900/6809) done. Loss: 0.1650  lr:0.000001
[ Tue Jul 16 06:47:03 2024 ] 
Training: Epoch [92/150], Step [5999], Loss: 0.13906842470169067, Training Accuracy: 95.7125
[ Tue Jul 16 06:47:04 2024 ] 	Batch(6000/6809) done. Loss: 0.2856  lr:0.000001
[ Tue Jul 16 06:47:27 2024 ] 	Batch(6100/6809) done. Loss: 0.1971  lr:0.000001
[ Tue Jul 16 06:47:50 2024 ] 	Batch(6200/6809) done. Loss: 0.6487  lr:0.000001
[ Tue Jul 16 06:48:14 2024 ] 	Batch(6300/6809) done. Loss: 0.4329  lr:0.000001
[ Tue Jul 16 06:48:37 2024 ] 	Batch(6400/6809) done. Loss: 0.0035  lr:0.000001
[ Tue Jul 16 06:49:00 2024 ] 
Training: Epoch [92/150], Step [6499], Loss: 0.3040597438812256, Training Accuracy: 95.72500000000001
[ Tue Jul 16 06:49:00 2024 ] 	Batch(6500/6809) done. Loss: 0.1083  lr:0.000001
[ Tue Jul 16 06:49:22 2024 ] 	Batch(6600/6809) done. Loss: 0.0473  lr:0.000001
[ Tue Jul 16 06:49:45 2024 ] 	Batch(6700/6809) done. Loss: 0.0580  lr:0.000001
[ Tue Jul 16 06:50:08 2024 ] 	Batch(6800/6809) done. Loss: 0.0525  lr:0.000001
[ Tue Jul 16 06:50:10 2024 ] 	Mean training loss: 0.1567.
[ Tue Jul 16 06:50:10 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 06:50:10 2024 ] Training epoch: 94
[ Tue Jul 16 06:50:10 2024 ] 	Batch(0/6809) done. Loss: 0.1515  lr:0.000001
[ Tue Jul 16 06:50:33 2024 ] 	Batch(100/6809) done. Loss: 0.0209  lr:0.000001
[ Tue Jul 16 06:50:56 2024 ] 	Batch(200/6809) done. Loss: 0.3626  lr:0.000001
[ Tue Jul 16 06:51:20 2024 ] 	Batch(300/6809) done. Loss: 0.3521  lr:0.000001
[ Tue Jul 16 06:51:43 2024 ] 	Batch(400/6809) done. Loss: 0.0102  lr:0.000001
[ Tue Jul 16 06:52:05 2024 ] 
Training: Epoch [93/150], Step [499], Loss: 0.04394828900694847, Training Accuracy: 95.475
[ Tue Jul 16 06:52:05 2024 ] 	Batch(500/6809) done. Loss: 0.3519  lr:0.000001
[ Tue Jul 16 06:52:28 2024 ] 	Batch(600/6809) done. Loss: 0.1731  lr:0.000001
[ Tue Jul 16 06:52:51 2024 ] 	Batch(700/6809) done. Loss: 0.1287  lr:0.000001
[ Tue Jul 16 06:53:14 2024 ] 	Batch(800/6809) done. Loss: 0.0143  lr:0.000001
[ Tue Jul 16 06:53:37 2024 ] 	Batch(900/6809) done. Loss: 0.1337  lr:0.000001
[ Tue Jul 16 06:53:59 2024 ] 
Training: Epoch [93/150], Step [999], Loss: 0.09247828274965286, Training Accuracy: 95.4375
[ Tue Jul 16 06:53:59 2024 ] 	Batch(1000/6809) done. Loss: 0.3749  lr:0.000001
[ Tue Jul 16 06:54:22 2024 ] 	Batch(1100/6809) done. Loss: 0.0643  lr:0.000001
[ Tue Jul 16 06:54:45 2024 ] 	Batch(1200/6809) done. Loss: 0.2049  lr:0.000001
[ Tue Jul 16 06:55:07 2024 ] 	Batch(1300/6809) done. Loss: 0.2914  lr:0.000001
[ Tue Jul 16 06:55:30 2024 ] 	Batch(1400/6809) done. Loss: 0.0409  lr:0.000001
[ Tue Jul 16 06:55:52 2024 ] 
Training: Epoch [93/150], Step [1499], Loss: 0.021746868267655373, Training Accuracy: 95.38333333333333
[ Tue Jul 16 06:55:53 2024 ] 	Batch(1500/6809) done. Loss: 0.1135  lr:0.000001
[ Tue Jul 16 06:56:15 2024 ] 	Batch(1600/6809) done. Loss: 0.0565  lr:0.000001
[ Tue Jul 16 06:56:38 2024 ] 	Batch(1700/6809) done. Loss: 0.3681  lr:0.000001
[ Tue Jul 16 06:57:01 2024 ] 	Batch(1800/6809) done. Loss: 0.0031  lr:0.000001
[ Tue Jul 16 06:57:23 2024 ] 	Batch(1900/6809) done. Loss: 0.1072  lr:0.000001
[ Tue Jul 16 06:57:46 2024 ] 
Training: Epoch [93/150], Step [1999], Loss: 0.021212322637438774, Training Accuracy: 95.5375
[ Tue Jul 16 06:57:47 2024 ] 	Batch(2000/6809) done. Loss: 0.3089  lr:0.000001
[ Tue Jul 16 06:58:09 2024 ] 	Batch(2100/6809) done. Loss: 0.1749  lr:0.000001
[ Tue Jul 16 06:58:32 2024 ] 	Batch(2200/6809) done. Loss: 0.0140  lr:0.000001
[ Tue Jul 16 06:58:55 2024 ] 	Batch(2300/6809) done. Loss: 0.0381  lr:0.000001
[ Tue Jul 16 06:59:18 2024 ] 	Batch(2400/6809) done. Loss: 0.0720  lr:0.000001
[ Tue Jul 16 06:59:40 2024 ] 
Training: Epoch [93/150], Step [2499], Loss: 0.07644017040729523, Training Accuracy: 95.675
[ Tue Jul 16 06:59:40 2024 ] 	Batch(2500/6809) done. Loss: 0.5492  lr:0.000001
[ Tue Jul 16 07:00:03 2024 ] 	Batch(2600/6809) done. Loss: 0.3406  lr:0.000001
[ Tue Jul 16 07:00:26 2024 ] 	Batch(2700/6809) done. Loss: 0.1141  lr:0.000001
[ Tue Jul 16 07:00:49 2024 ] 	Batch(2800/6809) done. Loss: 0.0277  lr:0.000001
[ Tue Jul 16 07:01:11 2024 ] 	Batch(2900/6809) done. Loss: 0.1640  lr:0.000001
[ Tue Jul 16 07:01:35 2024 ] 
Training: Epoch [93/150], Step [2999], Loss: 0.40045270323753357, Training Accuracy: 95.71666666666667
[ Tue Jul 16 07:01:35 2024 ] 	Batch(3000/6809) done. Loss: 0.0901  lr:0.000001
[ Tue Jul 16 07:01:57 2024 ] 	Batch(3100/6809) done. Loss: 0.1598  lr:0.000001
[ Tue Jul 16 07:02:20 2024 ] 	Batch(3200/6809) done. Loss: 0.1731  lr:0.000001
[ Tue Jul 16 07:02:43 2024 ] 	Batch(3300/6809) done. Loss: 0.0030  lr:0.000001
[ Tue Jul 16 07:03:06 2024 ] 	Batch(3400/6809) done. Loss: 0.5283  lr:0.000001
[ Tue Jul 16 07:03:28 2024 ] 
Training: Epoch [93/150], Step [3499], Loss: 0.03132818266749382, Training Accuracy: 95.78928571428571
[ Tue Jul 16 07:03:29 2024 ] 	Batch(3500/6809) done. Loss: 0.2387  lr:0.000001
[ Tue Jul 16 07:03:52 2024 ] 	Batch(3600/6809) done. Loss: 0.0610  lr:0.000001
[ Tue Jul 16 07:04:14 2024 ] 	Batch(3700/6809) done. Loss: 0.0974  lr:0.000001
[ Tue Jul 16 07:04:37 2024 ] 	Batch(3800/6809) done. Loss: 0.1235  lr:0.000001
[ Tue Jul 16 07:05:00 2024 ] 	Batch(3900/6809) done. Loss: 0.2839  lr:0.000001
[ Tue Jul 16 07:05:22 2024 ] 
Training: Epoch [93/150], Step [3999], Loss: 0.07871612906455994, Training Accuracy: 95.81562500000001
[ Tue Jul 16 07:05:22 2024 ] 	Batch(4000/6809) done. Loss: 0.1649  lr:0.000001
[ Tue Jul 16 07:05:45 2024 ] 	Batch(4100/6809) done. Loss: 0.0220  lr:0.000001
[ Tue Jul 16 07:06:08 2024 ] 	Batch(4200/6809) done. Loss: 0.3202  lr:0.000001
[ Tue Jul 16 07:06:30 2024 ] 	Batch(4300/6809) done. Loss: 0.0217  lr:0.000001
[ Tue Jul 16 07:06:53 2024 ] 	Batch(4400/6809) done. Loss: 0.5520  lr:0.000001
[ Tue Jul 16 07:07:15 2024 ] 
Training: Epoch [93/150], Step [4499], Loss: 0.3412379324436188, Training Accuracy: 95.74444444444444
[ Tue Jul 16 07:07:16 2024 ] 	Batch(4500/6809) done. Loss: 0.1944  lr:0.000001
[ Tue Jul 16 07:07:38 2024 ] 	Batch(4600/6809) done. Loss: 0.5397  lr:0.000001
[ Tue Jul 16 07:08:01 2024 ] 	Batch(4700/6809) done. Loss: 0.0030  lr:0.000001
[ Tue Jul 16 07:08:24 2024 ] 	Batch(4800/6809) done. Loss: 0.0469  lr:0.000001
[ Tue Jul 16 07:08:46 2024 ] 	Batch(4900/6809) done. Loss: 0.1093  lr:0.000001
[ Tue Jul 16 07:09:09 2024 ] 
Training: Epoch [93/150], Step [4999], Loss: 0.07150809466838837, Training Accuracy: 95.73
[ Tue Jul 16 07:09:09 2024 ] 	Batch(5000/6809) done. Loss: 0.0474  lr:0.000001
[ Tue Jul 16 07:09:32 2024 ] 	Batch(5100/6809) done. Loss: 0.1661  lr:0.000001
[ Tue Jul 16 07:09:55 2024 ] 	Batch(5200/6809) done. Loss: 0.0245  lr:0.000001
[ Tue Jul 16 07:10:17 2024 ] 	Batch(5300/6809) done. Loss: 0.4454  lr:0.000001
[ Tue Jul 16 07:10:40 2024 ] 	Batch(5400/6809) done. Loss: 0.4082  lr:0.000001
[ Tue Jul 16 07:11:03 2024 ] 
Training: Epoch [93/150], Step [5499], Loss: 0.008299523964524269, Training Accuracy: 95.73636363636365
[ Tue Jul 16 07:11:03 2024 ] 	Batch(5500/6809) done. Loss: 0.0129  lr:0.000001
[ Tue Jul 16 07:11:26 2024 ] 	Batch(5600/6809) done. Loss: 0.4874  lr:0.000001
[ Tue Jul 16 07:11:48 2024 ] 	Batch(5700/6809) done. Loss: 0.2518  lr:0.000001
[ Tue Jul 16 07:12:11 2024 ] 	Batch(5800/6809) done. Loss: 0.1091  lr:0.000001
[ Tue Jul 16 07:12:34 2024 ] 	Batch(5900/6809) done. Loss: 0.3239  lr:0.000001
[ Tue Jul 16 07:12:57 2024 ] 
Training: Epoch [93/150], Step [5999], Loss: 0.2689371407032013, Training Accuracy: 95.76458333333333
[ Tue Jul 16 07:12:57 2024 ] 	Batch(6000/6809) done. Loss: 0.1460  lr:0.000001
[ Tue Jul 16 07:13:20 2024 ] 	Batch(6100/6809) done. Loss: 0.0593  lr:0.000001
[ Tue Jul 16 07:13:42 2024 ] 	Batch(6200/6809) done. Loss: 0.0687  lr:0.000001
[ Tue Jul 16 07:14:05 2024 ] 	Batch(6300/6809) done. Loss: 0.1060  lr:0.000001
[ Tue Jul 16 07:14:28 2024 ] 	Batch(6400/6809) done. Loss: 0.0352  lr:0.000001
[ Tue Jul 16 07:14:51 2024 ] 
Training: Epoch [93/150], Step [6499], Loss: 0.4199906587600708, Training Accuracy: 95.73653846153846
[ Tue Jul 16 07:14:51 2024 ] 	Batch(6500/6809) done. Loss: 0.2828  lr:0.000001
[ Tue Jul 16 07:15:13 2024 ] 	Batch(6600/6809) done. Loss: 0.0110  lr:0.000001
[ Tue Jul 16 07:15:36 2024 ] 	Batch(6700/6809) done. Loss: 0.2292  lr:0.000001
[ Tue Jul 16 07:15:59 2024 ] 	Batch(6800/6809) done. Loss: 0.2391  lr:0.000001
[ Tue Jul 16 07:16:01 2024 ] 	Mean training loss: 0.1595.
[ Tue Jul 16 07:16:01 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 07:16:01 2024 ] Training epoch: 95
[ Tue Jul 16 07:16:02 2024 ] 	Batch(0/6809) done. Loss: 0.0958  lr:0.000001
[ Tue Jul 16 07:16:24 2024 ] 	Batch(100/6809) done. Loss: 0.4460  lr:0.000001
[ Tue Jul 16 07:16:47 2024 ] 	Batch(200/6809) done. Loss: 0.0091  lr:0.000001
[ Tue Jul 16 07:17:10 2024 ] 	Batch(300/6809) done. Loss: 0.2031  lr:0.000001
[ Tue Jul 16 07:17:32 2024 ] 	Batch(400/6809) done. Loss: 0.4298  lr:0.000001
[ Tue Jul 16 07:17:55 2024 ] 
Training: Epoch [94/150], Step [499], Loss: 0.1374465525150299, Training Accuracy: 95.92500000000001
[ Tue Jul 16 07:17:55 2024 ] 	Batch(500/6809) done. Loss: 0.2055  lr:0.000001
[ Tue Jul 16 07:18:18 2024 ] 	Batch(600/6809) done. Loss: 0.0073  lr:0.000001
[ Tue Jul 16 07:18:41 2024 ] 	Batch(700/6809) done. Loss: 0.1016  lr:0.000001
[ Tue Jul 16 07:19:04 2024 ] 	Batch(800/6809) done. Loss: 0.1331  lr:0.000001
[ Tue Jul 16 07:19:26 2024 ] 	Batch(900/6809) done. Loss: 0.2450  lr:0.000001
[ Tue Jul 16 07:19:48 2024 ] 
Training: Epoch [94/150], Step [999], Loss: 0.12142176926136017, Training Accuracy: 95.72500000000001
[ Tue Jul 16 07:19:49 2024 ] 	Batch(1000/6809) done. Loss: 0.0875  lr:0.000001
[ Tue Jul 16 07:20:11 2024 ] 	Batch(1100/6809) done. Loss: 0.5553  lr:0.000001
[ Tue Jul 16 07:20:34 2024 ] 	Batch(1200/6809) done. Loss: 0.2737  lr:0.000001
[ Tue Jul 16 07:20:56 2024 ] 	Batch(1300/6809) done. Loss: 0.2408  lr:0.000001
[ Tue Jul 16 07:21:19 2024 ] 	Batch(1400/6809) done. Loss: 0.4304  lr:0.000001
[ Tue Jul 16 07:21:41 2024 ] 
Training: Epoch [94/150], Step [1499], Loss: 0.016061361879110336, Training Accuracy: 95.85000000000001
[ Tue Jul 16 07:21:41 2024 ] 	Batch(1500/6809) done. Loss: 0.0215  lr:0.000001
[ Tue Jul 16 07:22:04 2024 ] 	Batch(1600/6809) done. Loss: 0.1410  lr:0.000001
[ Tue Jul 16 07:22:27 2024 ] 	Batch(1700/6809) done. Loss: 0.0965  lr:0.000001
[ Tue Jul 16 07:22:49 2024 ] 	Batch(1800/6809) done. Loss: 0.3600  lr:0.000001
[ Tue Jul 16 07:23:12 2024 ] 	Batch(1900/6809) done. Loss: 0.0310  lr:0.000001
[ Tue Jul 16 07:23:34 2024 ] 
Training: Epoch [94/150], Step [1999], Loss: 0.06533315777778625, Training Accuracy: 95.94375000000001
[ Tue Jul 16 07:23:34 2024 ] 	Batch(2000/6809) done. Loss: 0.0428  lr:0.000001
[ Tue Jul 16 07:23:57 2024 ] 	Batch(2100/6809) done. Loss: 0.9180  lr:0.000001
[ Tue Jul 16 07:24:19 2024 ] 	Batch(2200/6809) done. Loss: 0.0856  lr:0.000001
[ Tue Jul 16 07:24:42 2024 ] 	Batch(2300/6809) done. Loss: 0.0683  lr:0.000001
[ Tue Jul 16 07:25:04 2024 ] 	Batch(2400/6809) done. Loss: 0.2703  lr:0.000001
[ Tue Jul 16 07:25:27 2024 ] 
Training: Epoch [94/150], Step [2499], Loss: 0.10112147033214569, Training Accuracy: 95.89
[ Tue Jul 16 07:25:27 2024 ] 	Batch(2500/6809) done. Loss: 0.0763  lr:0.000001
[ Tue Jul 16 07:25:50 2024 ] 	Batch(2600/6809) done. Loss: 0.0853  lr:0.000001
[ Tue Jul 16 07:26:12 2024 ] 	Batch(2700/6809) done. Loss: 0.2222  lr:0.000001
[ Tue Jul 16 07:26:35 2024 ] 	Batch(2800/6809) done. Loss: 0.2601  lr:0.000001
[ Tue Jul 16 07:26:57 2024 ] 	Batch(2900/6809) done. Loss: 0.0610  lr:0.000001
[ Tue Jul 16 07:27:20 2024 ] 
Training: Epoch [94/150], Step [2999], Loss: 0.122792087495327, Training Accuracy: 95.85416666666666
[ Tue Jul 16 07:27:20 2024 ] 	Batch(3000/6809) done. Loss: 0.1072  lr:0.000001
[ Tue Jul 16 07:27:42 2024 ] 	Batch(3100/6809) done. Loss: 0.0091  lr:0.000001
[ Tue Jul 16 07:28:05 2024 ] 	Batch(3200/6809) done. Loss: 0.1250  lr:0.000001
[ Tue Jul 16 07:28:28 2024 ] 	Batch(3300/6809) done. Loss: 0.0533  lr:0.000001
[ Tue Jul 16 07:28:50 2024 ] 	Batch(3400/6809) done. Loss: 0.0712  lr:0.000001
[ Tue Jul 16 07:29:13 2024 ] 
Training: Epoch [94/150], Step [3499], Loss: 0.19434146583080292, Training Accuracy: 95.8
[ Tue Jul 16 07:29:13 2024 ] 	Batch(3500/6809) done. Loss: 0.1634  lr:0.000001
[ Tue Jul 16 07:29:36 2024 ] 	Batch(3600/6809) done. Loss: 0.0153  lr:0.000001
[ Tue Jul 16 07:29:58 2024 ] 	Batch(3700/6809) done. Loss: 0.0930  lr:0.000001
[ Tue Jul 16 07:30:21 2024 ] 	Batch(3800/6809) done. Loss: 0.1207  lr:0.000001
[ Tue Jul 16 07:30:43 2024 ] 	Batch(3900/6809) done. Loss: 0.0241  lr:0.000001
[ Tue Jul 16 07:31:06 2024 ] 
Training: Epoch [94/150], Step [3999], Loss: 0.032453346997499466, Training Accuracy: 95.79062499999999
[ Tue Jul 16 07:31:07 2024 ] 	Batch(4000/6809) done. Loss: 0.0236  lr:0.000001
[ Tue Jul 16 07:31:30 2024 ] 	Batch(4100/6809) done. Loss: 0.3762  lr:0.000001
[ Tue Jul 16 07:31:53 2024 ] 	Batch(4200/6809) done. Loss: 0.3473  lr:0.000001
[ Tue Jul 16 07:32:17 2024 ] 	Batch(4300/6809) done. Loss: 0.0506  lr:0.000001
[ Tue Jul 16 07:32:40 2024 ] 	Batch(4400/6809) done. Loss: 0.2262  lr:0.000001
[ Tue Jul 16 07:33:03 2024 ] 
Training: Epoch [94/150], Step [4499], Loss: 0.3029177188873291, Training Accuracy: 95.76388888888889
[ Tue Jul 16 07:33:03 2024 ] 	Batch(4500/6809) done. Loss: 0.4640  lr:0.000001
[ Tue Jul 16 07:33:26 2024 ] 	Batch(4600/6809) done. Loss: 0.0693  lr:0.000001
[ Tue Jul 16 07:33:49 2024 ] 	Batch(4700/6809) done. Loss: 0.1329  lr:0.000001
[ Tue Jul 16 07:34:13 2024 ] 	Batch(4800/6809) done. Loss: 0.1101  lr:0.000001
[ Tue Jul 16 07:34:37 2024 ] 	Batch(4900/6809) done. Loss: 0.1445  lr:0.000001
[ Tue Jul 16 07:35:00 2024 ] 
Training: Epoch [94/150], Step [4999], Loss: 0.040385931730270386, Training Accuracy: 95.74000000000001
[ Tue Jul 16 07:35:00 2024 ] 	Batch(5000/6809) done. Loss: 0.1481  lr:0.000001
[ Tue Jul 16 07:35:24 2024 ] 	Batch(5100/6809) done. Loss: 0.1741  lr:0.000001
[ Tue Jul 16 07:35:48 2024 ] 	Batch(5200/6809) done. Loss: 0.4910  lr:0.000001
[ Tue Jul 16 07:36:11 2024 ] 	Batch(5300/6809) done. Loss: 0.0113  lr:0.000001
[ Tue Jul 16 07:36:35 2024 ] 	Batch(5400/6809) done. Loss: 0.1583  lr:0.000001
[ Tue Jul 16 07:36:57 2024 ] 
Training: Epoch [94/150], Step [5499], Loss: 0.05770726129412651, Training Accuracy: 95.72727272727273
[ Tue Jul 16 07:36:58 2024 ] 	Batch(5500/6809) done. Loss: 0.0263  lr:0.000001
[ Tue Jul 16 07:37:20 2024 ] 	Batch(5600/6809) done. Loss: 0.1569  lr:0.000001
[ Tue Jul 16 07:37:43 2024 ] 	Batch(5700/6809) done. Loss: 0.4877  lr:0.000001
[ Tue Jul 16 07:38:06 2024 ] 	Batch(5800/6809) done. Loss: 0.8081  lr:0.000001
[ Tue Jul 16 07:38:29 2024 ] 	Batch(5900/6809) done. Loss: 0.0479  lr:0.000001
[ Tue Jul 16 07:38:51 2024 ] 
Training: Epoch [94/150], Step [5999], Loss: 0.5908494591712952, Training Accuracy: 95.72083333333333
[ Tue Jul 16 07:38:51 2024 ] 	Batch(6000/6809) done. Loss: 0.1391  lr:0.000001
[ Tue Jul 16 07:39:14 2024 ] 	Batch(6100/6809) done. Loss: 0.0221  lr:0.000001
[ Tue Jul 16 07:39:38 2024 ] 	Batch(6200/6809) done. Loss: 0.1597  lr:0.000001
[ Tue Jul 16 07:40:01 2024 ] 	Batch(6300/6809) done. Loss: 0.4717  lr:0.000001
[ Tue Jul 16 07:40:24 2024 ] 	Batch(6400/6809) done. Loss: 0.1485  lr:0.000001
[ Tue Jul 16 07:40:47 2024 ] 
Training: Epoch [94/150], Step [6499], Loss: 0.34035754203796387, Training Accuracy: 95.68846153846154
[ Tue Jul 16 07:40:47 2024 ] 	Batch(6500/6809) done. Loss: 0.1436  lr:0.000001
[ Tue Jul 16 07:41:10 2024 ] 	Batch(6600/6809) done. Loss: 0.2320  lr:0.000001
[ Tue Jul 16 07:41:33 2024 ] 	Batch(6700/6809) done. Loss: 0.1675  lr:0.000001
[ Tue Jul 16 07:41:55 2024 ] 	Batch(6800/6809) done. Loss: 0.1970  lr:0.000001
[ Tue Jul 16 07:41:57 2024 ] 	Mean training loss: 0.1554.
[ Tue Jul 16 07:41:57 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 07:41:57 2024 ] Training epoch: 96
[ Tue Jul 16 07:41:58 2024 ] 	Batch(0/6809) done. Loss: 0.0420  lr:0.000001
[ Tue Jul 16 07:42:21 2024 ] 	Batch(100/6809) done. Loss: 0.0942  lr:0.000001
[ Tue Jul 16 07:42:44 2024 ] 	Batch(200/6809) done. Loss: 0.2690  lr:0.000001
[ Tue Jul 16 07:43:07 2024 ] 	Batch(300/6809) done. Loss: 0.0278  lr:0.000001
[ Tue Jul 16 07:43:29 2024 ] 	Batch(400/6809) done. Loss: 0.0272  lr:0.000001
[ Tue Jul 16 07:43:52 2024 ] 
Training: Epoch [95/150], Step [499], Loss: 0.15200358629226685, Training Accuracy: 95.72500000000001
[ Tue Jul 16 07:43:52 2024 ] 	Batch(500/6809) done. Loss: 0.0220  lr:0.000001
[ Tue Jul 16 07:44:14 2024 ] 	Batch(600/6809) done. Loss: 0.0227  lr:0.000001
[ Tue Jul 16 07:44:37 2024 ] 	Batch(700/6809) done. Loss: 0.0357  lr:0.000001
[ Tue Jul 16 07:45:01 2024 ] 	Batch(800/6809) done. Loss: 0.2854  lr:0.000001
[ Tue Jul 16 07:45:23 2024 ] 	Batch(900/6809) done. Loss: 0.1156  lr:0.000001
[ Tue Jul 16 07:45:46 2024 ] 
Training: Epoch [95/150], Step [999], Loss: 0.02733846940100193, Training Accuracy: 95.8875
[ Tue Jul 16 07:45:46 2024 ] 	Batch(1000/6809) done. Loss: 0.3856  lr:0.000001
[ Tue Jul 16 07:46:09 2024 ] 	Batch(1100/6809) done. Loss: 0.7000  lr:0.000001
[ Tue Jul 16 07:46:32 2024 ] 	Batch(1200/6809) done. Loss: 0.3668  lr:0.000001
[ Tue Jul 16 07:46:55 2024 ] 	Batch(1300/6809) done. Loss: 0.0370  lr:0.000001
[ Tue Jul 16 07:47:18 2024 ] 	Batch(1400/6809) done. Loss: 0.1676  lr:0.000001
[ Tue Jul 16 07:47:41 2024 ] 
Training: Epoch [95/150], Step [1499], Loss: 0.052950650453567505, Training Accuracy: 95.81666666666668
[ Tue Jul 16 07:47:41 2024 ] 	Batch(1500/6809) done. Loss: 0.0346  lr:0.000001
[ Tue Jul 16 07:48:04 2024 ] 	Batch(1600/6809) done. Loss: 0.2237  lr:0.000001
[ Tue Jul 16 07:48:27 2024 ] 	Batch(1700/6809) done. Loss: 0.1850  lr:0.000001
[ Tue Jul 16 07:48:50 2024 ] 	Batch(1800/6809) done. Loss: 0.2508  lr:0.000001
[ Tue Jul 16 07:49:13 2024 ] 	Batch(1900/6809) done. Loss: 0.3271  lr:0.000001
[ Tue Jul 16 07:49:36 2024 ] 
Training: Epoch [95/150], Step [1999], Loss: 0.16550669074058533, Training Accuracy: 95.675
[ Tue Jul 16 07:49:36 2024 ] 	Batch(2000/6809) done. Loss: 0.1528  lr:0.000001
[ Tue Jul 16 07:49:59 2024 ] 	Batch(2100/6809) done. Loss: 0.2437  lr:0.000001
[ Tue Jul 16 07:50:22 2024 ] 	Batch(2200/6809) done. Loss: 0.0143  lr:0.000001
[ Tue Jul 16 07:50:45 2024 ] 	Batch(2300/6809) done. Loss: 0.3867  lr:0.000001
[ Tue Jul 16 07:51:08 2024 ] 	Batch(2400/6809) done. Loss: 0.0717  lr:0.000001
[ Tue Jul 16 07:51:31 2024 ] 
Training: Epoch [95/150], Step [2499], Loss: 0.05918579548597336, Training Accuracy: 95.76
[ Tue Jul 16 07:51:31 2024 ] 	Batch(2500/6809) done. Loss: 0.0422  lr:0.000001
[ Tue Jul 16 07:51:54 2024 ] 	Batch(2600/6809) done. Loss: 0.2902  lr:0.000001
[ Tue Jul 16 07:52:17 2024 ] 	Batch(2700/6809) done. Loss: 0.1392  lr:0.000001
[ Tue Jul 16 07:52:40 2024 ] 	Batch(2800/6809) done. Loss: 0.2954  lr:0.000001
[ Tue Jul 16 07:53:03 2024 ] 	Batch(2900/6809) done. Loss: 0.0213  lr:0.000001
[ Tue Jul 16 07:53:25 2024 ] 
Training: Epoch [95/150], Step [2999], Loss: 0.10821357369422913, Training Accuracy: 95.78333333333333
[ Tue Jul 16 07:53:25 2024 ] 	Batch(3000/6809) done. Loss: 0.0550  lr:0.000001
[ Tue Jul 16 07:53:48 2024 ] 	Batch(3100/6809) done. Loss: 0.5943  lr:0.000001
[ Tue Jul 16 07:54:11 2024 ] 	Batch(3200/6809) done. Loss: 0.0900  lr:0.000001
[ Tue Jul 16 07:54:34 2024 ] 	Batch(3300/6809) done. Loss: 0.3810  lr:0.000001
[ Tue Jul 16 07:54:58 2024 ] 	Batch(3400/6809) done. Loss: 0.1707  lr:0.000001
[ Tue Jul 16 07:55:21 2024 ] 
Training: Epoch [95/150], Step [3499], Loss: 0.3081282675266266, Training Accuracy: 95.75
[ Tue Jul 16 07:55:21 2024 ] 	Batch(3500/6809) done. Loss: 0.1068  lr:0.000001
[ Tue Jul 16 07:55:45 2024 ] 	Batch(3600/6809) done. Loss: 0.3696  lr:0.000001
[ Tue Jul 16 07:56:08 2024 ] 	Batch(3700/6809) done. Loss: 0.2064  lr:0.000001
[ Tue Jul 16 07:56:31 2024 ] 	Batch(3800/6809) done. Loss: 0.6071  lr:0.000001
[ Tue Jul 16 07:56:54 2024 ] 	Batch(3900/6809) done. Loss: 0.0810  lr:0.000001
[ Tue Jul 16 07:57:16 2024 ] 
Training: Epoch [95/150], Step [3999], Loss: 0.24231290817260742, Training Accuracy: 95.7375
[ Tue Jul 16 07:57:16 2024 ] 	Batch(4000/6809) done. Loss: 0.0048  lr:0.000001
[ Tue Jul 16 07:57:39 2024 ] 	Batch(4100/6809) done. Loss: 0.3380  lr:0.000001
[ Tue Jul 16 07:58:02 2024 ] 	Batch(4200/6809) done. Loss: 0.3933  lr:0.000001
[ Tue Jul 16 07:58:25 2024 ] 	Batch(4300/6809) done. Loss: 0.0348  lr:0.000001
[ Tue Jul 16 07:58:47 2024 ] 	Batch(4400/6809) done. Loss: 0.0474  lr:0.000001
[ Tue Jul 16 07:59:11 2024 ] 
Training: Epoch [95/150], Step [4499], Loss: 0.10534218698740005, Training Accuracy: 95.75555555555556
[ Tue Jul 16 07:59:11 2024 ] 	Batch(4500/6809) done. Loss: 0.0019  lr:0.000001
[ Tue Jul 16 07:59:35 2024 ] 	Batch(4600/6809) done. Loss: 0.3676  lr:0.000001
[ Tue Jul 16 07:59:58 2024 ] 	Batch(4700/6809) done. Loss: 0.0771  lr:0.000001
[ Tue Jul 16 08:00:22 2024 ] 	Batch(4800/6809) done. Loss: 0.0443  lr:0.000001
[ Tue Jul 16 08:00:44 2024 ] 	Batch(4900/6809) done. Loss: 0.0347  lr:0.000001
[ Tue Jul 16 08:01:07 2024 ] 
Training: Epoch [95/150], Step [4999], Loss: 0.15432208776474, Training Accuracy: 95.77
[ Tue Jul 16 08:01:07 2024 ] 	Batch(5000/6809) done. Loss: 0.0296  lr:0.000001
[ Tue Jul 16 08:01:30 2024 ] 	Batch(5100/6809) done. Loss: 0.0879  lr:0.000001
[ Tue Jul 16 08:01:53 2024 ] 	Batch(5200/6809) done. Loss: 0.0572  lr:0.000001
[ Tue Jul 16 08:02:15 2024 ] 	Batch(5300/6809) done. Loss: 0.6321  lr:0.000001
[ Tue Jul 16 08:02:38 2024 ] 	Batch(5400/6809) done. Loss: 0.0188  lr:0.000001
[ Tue Jul 16 08:03:01 2024 ] 
Training: Epoch [95/150], Step [5499], Loss: 0.01795955002307892, Training Accuracy: 95.81363636363636
[ Tue Jul 16 08:03:01 2024 ] 	Batch(5500/6809) done. Loss: 0.0143  lr:0.000001
[ Tue Jul 16 08:03:24 2024 ] 	Batch(5600/6809) done. Loss: 0.0279  lr:0.000001
[ Tue Jul 16 08:03:46 2024 ] 	Batch(5700/6809) done. Loss: 0.0419  lr:0.000001
[ Tue Jul 16 08:04:09 2024 ] 	Batch(5800/6809) done. Loss: 0.2687  lr:0.000001
[ Tue Jul 16 08:04:32 2024 ] 	Batch(5900/6809) done. Loss: 0.0481  lr:0.000001
[ Tue Jul 16 08:04:54 2024 ] 
Training: Epoch [95/150], Step [5999], Loss: 0.038368914276361465, Training Accuracy: 95.82083333333333
[ Tue Jul 16 08:04:55 2024 ] 	Batch(6000/6809) done. Loss: 0.0052  lr:0.000001
[ Tue Jul 16 08:05:18 2024 ] 	Batch(6100/6809) done. Loss: 0.0084  lr:0.000001
[ Tue Jul 16 08:05:40 2024 ] 	Batch(6200/6809) done. Loss: 0.0202  lr:0.000001
[ Tue Jul 16 08:06:03 2024 ] 	Batch(6300/6809) done. Loss: 0.2392  lr:0.000001
[ Tue Jul 16 08:06:26 2024 ] 	Batch(6400/6809) done. Loss: 0.2510  lr:0.000001
[ Tue Jul 16 08:06:48 2024 ] 
Training: Epoch [95/150], Step [6499], Loss: 0.09248580783605576, Training Accuracy: 95.80192307692307
[ Tue Jul 16 08:06:49 2024 ] 	Batch(6500/6809) done. Loss: 0.1241  lr:0.000001
[ Tue Jul 16 08:07:11 2024 ] 	Batch(6600/6809) done. Loss: 0.0363  lr:0.000001
[ Tue Jul 16 08:07:34 2024 ] 	Batch(6700/6809) done. Loss: 0.0653  lr:0.000001
[ Tue Jul 16 08:07:57 2024 ] 	Batch(6800/6809) done. Loss: 0.2183  lr:0.000001
[ Tue Jul 16 08:07:59 2024 ] 	Mean training loss: 0.1545.
[ Tue Jul 16 08:07:59 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 08:07:59 2024 ] Training epoch: 97
[ Tue Jul 16 08:08:00 2024 ] 	Batch(0/6809) done. Loss: 0.0896  lr:0.000001
[ Tue Jul 16 08:08:22 2024 ] 	Batch(100/6809) done. Loss: 0.0102  lr:0.000001
[ Tue Jul 16 08:08:45 2024 ] 	Batch(200/6809) done. Loss: 0.1643  lr:0.000001
[ Tue Jul 16 08:09:08 2024 ] 	Batch(300/6809) done. Loss: 0.5714  lr:0.000001
[ Tue Jul 16 08:09:31 2024 ] 	Batch(400/6809) done. Loss: 0.3346  lr:0.000001
[ Tue Jul 16 08:09:53 2024 ] 
Training: Epoch [96/150], Step [499], Loss: 0.00895700603723526, Training Accuracy: 95.72500000000001
[ Tue Jul 16 08:09:53 2024 ] 	Batch(500/6809) done. Loss: 0.0045  lr:0.000001
[ Tue Jul 16 08:10:16 2024 ] 	Batch(600/6809) done. Loss: 0.0257  lr:0.000001
[ Tue Jul 16 08:10:39 2024 ] 	Batch(700/6809) done. Loss: 0.0551  lr:0.000001
[ Tue Jul 16 08:11:02 2024 ] 	Batch(800/6809) done. Loss: 0.1174  lr:0.000001
[ Tue Jul 16 08:11:25 2024 ] 	Batch(900/6809) done. Loss: 0.0122  lr:0.000001
[ Tue Jul 16 08:11:47 2024 ] 
Training: Epoch [96/150], Step [999], Loss: 0.1893119513988495, Training Accuracy: 95.825
[ Tue Jul 16 08:11:48 2024 ] 	Batch(1000/6809) done. Loss: 0.0476  lr:0.000001
[ Tue Jul 16 08:12:11 2024 ] 	Batch(1100/6809) done. Loss: 0.0064  lr:0.000001
[ Tue Jul 16 08:12:34 2024 ] 	Batch(1200/6809) done. Loss: 0.2684  lr:0.000001
[ Tue Jul 16 08:12:57 2024 ] 	Batch(1300/6809) done. Loss: 0.3455  lr:0.000001
[ Tue Jul 16 08:13:20 2024 ] 	Batch(1400/6809) done. Loss: 0.0390  lr:0.000001
[ Tue Jul 16 08:13:42 2024 ] 
Training: Epoch [96/150], Step [1499], Loss: 0.3284052312374115, Training Accuracy: 95.95833333333333
[ Tue Jul 16 08:13:43 2024 ] 	Batch(1500/6809) done. Loss: 0.0424  lr:0.000001
[ Tue Jul 16 08:14:06 2024 ] 	Batch(1600/6809) done. Loss: 0.3624  lr:0.000001
[ Tue Jul 16 08:14:29 2024 ] 	Batch(1700/6809) done. Loss: 0.0100  lr:0.000001
[ Tue Jul 16 08:14:52 2024 ] 	Batch(1800/6809) done. Loss: 0.0684  lr:0.000001
[ Tue Jul 16 08:15:15 2024 ] 	Batch(1900/6809) done. Loss: 0.1390  lr:0.000001
[ Tue Jul 16 08:15:37 2024 ] 
Training: Epoch [96/150], Step [1999], Loss: 0.10586413741111755, Training Accuracy: 95.95
[ Tue Jul 16 08:15:38 2024 ] 	Batch(2000/6809) done. Loss: 0.1628  lr:0.000001
[ Tue Jul 16 08:16:01 2024 ] 	Batch(2100/6809) done. Loss: 0.0716  lr:0.000001
[ Tue Jul 16 08:16:24 2024 ] 	Batch(2200/6809) done. Loss: 0.0363  lr:0.000001
[ Tue Jul 16 08:16:47 2024 ] 	Batch(2300/6809) done. Loss: 0.1958  lr:0.000001
[ Tue Jul 16 08:17:10 2024 ] 	Batch(2400/6809) done. Loss: 0.0353  lr:0.000001
[ Tue Jul 16 08:17:33 2024 ] 
Training: Epoch [96/150], Step [2499], Loss: 0.05124587565660477, Training Accuracy: 95.89999999999999
[ Tue Jul 16 08:17:33 2024 ] 	Batch(2500/6809) done. Loss: 0.0069  lr:0.000001
[ Tue Jul 16 08:17:56 2024 ] 	Batch(2600/6809) done. Loss: 0.1839  lr:0.000001
[ Tue Jul 16 08:18:19 2024 ] 	Batch(2700/6809) done. Loss: 0.0499  lr:0.000001
[ Tue Jul 16 08:18:42 2024 ] 	Batch(2800/6809) done. Loss: 0.1613  lr:0.000001
[ Tue Jul 16 08:19:05 2024 ] 	Batch(2900/6809) done. Loss: 0.4044  lr:0.000001
[ Tue Jul 16 08:19:28 2024 ] 
Training: Epoch [96/150], Step [2999], Loss: 0.07746735960245132, Training Accuracy: 95.75833333333334
[ Tue Jul 16 08:19:28 2024 ] 	Batch(3000/6809) done. Loss: 0.0823  lr:0.000001
[ Tue Jul 16 08:19:51 2024 ] 	Batch(3100/6809) done. Loss: 0.1162  lr:0.000001
[ Tue Jul 16 08:20:14 2024 ] 	Batch(3200/6809) done. Loss: 0.0468  lr:0.000001
[ Tue Jul 16 08:20:37 2024 ] 	Batch(3300/6809) done. Loss: 0.1528  lr:0.000001
[ Tue Jul 16 08:21:00 2024 ] 	Batch(3400/6809) done. Loss: 0.0928  lr:0.000001
[ Tue Jul 16 08:21:23 2024 ] 
Training: Epoch [96/150], Step [3499], Loss: 0.044963859021663666, Training Accuracy: 95.81071428571428
[ Tue Jul 16 08:21:23 2024 ] 	Batch(3500/6809) done. Loss: 0.0701  lr:0.000001
[ Tue Jul 16 08:21:46 2024 ] 	Batch(3600/6809) done. Loss: 0.3626  lr:0.000001
[ Tue Jul 16 08:22:08 2024 ] 	Batch(3700/6809) done. Loss: 0.0407  lr:0.000001
[ Tue Jul 16 08:22:31 2024 ] 	Batch(3800/6809) done. Loss: 0.2191  lr:0.000001
[ Tue Jul 16 08:22:54 2024 ] 	Batch(3900/6809) done. Loss: 0.1788  lr:0.000001
[ Tue Jul 16 08:23:16 2024 ] 
Training: Epoch [96/150], Step [3999], Loss: 0.10046457499265671, Training Accuracy: 95.81562500000001
[ Tue Jul 16 08:23:16 2024 ] 	Batch(4000/6809) done. Loss: 0.2340  lr:0.000001
[ Tue Jul 16 08:23:39 2024 ] 	Batch(4100/6809) done. Loss: 0.0622  lr:0.000001
[ Tue Jul 16 08:24:02 2024 ] 	Batch(4200/6809) done. Loss: 0.0153  lr:0.000001
[ Tue Jul 16 08:24:24 2024 ] 	Batch(4300/6809) done. Loss: 0.1033  lr:0.000001
[ Tue Jul 16 08:24:47 2024 ] 	Batch(4400/6809) done. Loss: 0.1456  lr:0.000001
[ Tue Jul 16 08:25:09 2024 ] 
Training: Epoch [96/150], Step [4499], Loss: 0.017137479037046432, Training Accuracy: 95.77777777777777
[ Tue Jul 16 08:25:09 2024 ] 	Batch(4500/6809) done. Loss: 0.0924  lr:0.000001
[ Tue Jul 16 08:25:32 2024 ] 	Batch(4600/6809) done. Loss: 0.0756  lr:0.000001
[ Tue Jul 16 08:25:54 2024 ] 	Batch(4700/6809) done. Loss: 0.1177  lr:0.000001
[ Tue Jul 16 08:26:17 2024 ] 	Batch(4800/6809) done. Loss: 0.0153  lr:0.000001
[ Tue Jul 16 08:26:40 2024 ] 	Batch(4900/6809) done. Loss: 0.4071  lr:0.000001
[ Tue Jul 16 08:27:03 2024 ] 
Training: Epoch [96/150], Step [4999], Loss: 0.06813560426235199, Training Accuracy: 95.8075
[ Tue Jul 16 08:27:03 2024 ] 	Batch(5000/6809) done. Loss: 0.0237  lr:0.000001
[ Tue Jul 16 08:27:26 2024 ] 	Batch(5100/6809) done. Loss: 0.1690  lr:0.000001
[ Tue Jul 16 08:27:49 2024 ] 	Batch(5200/6809) done. Loss: 0.0065  lr:0.000001
[ Tue Jul 16 08:28:11 2024 ] 	Batch(5300/6809) done. Loss: 0.1806  lr:0.000001
[ Tue Jul 16 08:28:34 2024 ] 	Batch(5400/6809) done. Loss: 0.1385  lr:0.000001
[ Tue Jul 16 08:28:56 2024 ] 
Training: Epoch [96/150], Step [5499], Loss: 0.004819367080926895, Training Accuracy: 95.80681818181817
[ Tue Jul 16 08:28:56 2024 ] 	Batch(5500/6809) done. Loss: 0.2203  lr:0.000001
[ Tue Jul 16 08:29:19 2024 ] 	Batch(5600/6809) done. Loss: 0.0154  lr:0.000001
[ Tue Jul 16 08:29:42 2024 ] 	Batch(5700/6809) done. Loss: 0.0725  lr:0.000001
[ Tue Jul 16 08:30:05 2024 ] 	Batch(5800/6809) done. Loss: 0.4929  lr:0.000001
[ Tue Jul 16 08:30:29 2024 ] 	Batch(5900/6809) done. Loss: 0.0713  lr:0.000001
[ Tue Jul 16 08:30:51 2024 ] 
Training: Epoch [96/150], Step [5999], Loss: 0.3342607021331787, Training Accuracy: 95.78125
[ Tue Jul 16 08:30:51 2024 ] 	Batch(6000/6809) done. Loss: 0.0730  lr:0.000001
[ Tue Jul 16 08:31:14 2024 ] 	Batch(6100/6809) done. Loss: 0.9416  lr:0.000001
[ Tue Jul 16 08:31:36 2024 ] 	Batch(6200/6809) done. Loss: 0.3101  lr:0.000001
[ Tue Jul 16 08:31:59 2024 ] 	Batch(6300/6809) done. Loss: 0.1973  lr:0.000001
[ Tue Jul 16 08:32:22 2024 ] 	Batch(6400/6809) done. Loss: 0.0704  lr:0.000001
[ Tue Jul 16 08:32:44 2024 ] 
Training: Epoch [96/150], Step [6499], Loss: 0.06883365660905838, Training Accuracy: 95.7826923076923
[ Tue Jul 16 08:32:44 2024 ] 	Batch(6500/6809) done. Loss: 0.0008  lr:0.000001
[ Tue Jul 16 08:33:07 2024 ] 	Batch(6600/6809) done. Loss: 0.3118  lr:0.000001
[ Tue Jul 16 08:33:30 2024 ] 	Batch(6700/6809) done. Loss: 0.1374  lr:0.000001
[ Tue Jul 16 08:33:53 2024 ] 	Batch(6800/6809) done. Loss: 0.0397  lr:0.000001
[ Tue Jul 16 08:33:54 2024 ] 	Mean training loss: 0.1533.
[ Tue Jul 16 08:33:54 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 08:33:55 2024 ] Training epoch: 98
[ Tue Jul 16 08:33:55 2024 ] 	Batch(0/6809) done. Loss: 0.0051  lr:0.000001
[ Tue Jul 16 08:34:18 2024 ] 	Batch(100/6809) done. Loss: 0.2699  lr:0.000001
[ Tue Jul 16 08:34:40 2024 ] 	Batch(200/6809) done. Loss: 0.0244  lr:0.000001
[ Tue Jul 16 08:35:03 2024 ] 	Batch(300/6809) done. Loss: 0.4241  lr:0.000001
[ Tue Jul 16 08:35:25 2024 ] 	Batch(400/6809) done. Loss: 0.5839  lr:0.000001
[ Tue Jul 16 08:35:48 2024 ] 
Training: Epoch [97/150], Step [499], Loss: 0.37671610713005066, Training Accuracy: 96.0
[ Tue Jul 16 08:35:48 2024 ] 	Batch(500/6809) done. Loss: 0.0361  lr:0.000001
[ Tue Jul 16 08:36:10 2024 ] 	Batch(600/6809) done. Loss: 0.1663  lr:0.000001
[ Tue Jul 16 08:36:33 2024 ] 	Batch(700/6809) done. Loss: 0.0160  lr:0.000001
[ Tue Jul 16 08:36:56 2024 ] 	Batch(800/6809) done. Loss: 0.1562  lr:0.000001
[ Tue Jul 16 08:37:18 2024 ] 	Batch(900/6809) done. Loss: 0.2776  lr:0.000001
[ Tue Jul 16 08:37:40 2024 ] 
Training: Epoch [97/150], Step [999], Loss: 0.061458975076675415, Training Accuracy: 95.6875
[ Tue Jul 16 08:37:41 2024 ] 	Batch(1000/6809) done. Loss: 0.0829  lr:0.000001
[ Tue Jul 16 08:38:03 2024 ] 	Batch(1100/6809) done. Loss: 0.1990  lr:0.000001
[ Tue Jul 16 08:38:26 2024 ] 	Batch(1200/6809) done. Loss: 0.0827  lr:0.000001
[ Tue Jul 16 08:38:48 2024 ] 	Batch(1300/6809) done. Loss: 0.0860  lr:0.000001
[ Tue Jul 16 08:39:12 2024 ] 	Batch(1400/6809) done. Loss: 0.3895  lr:0.000001
[ Tue Jul 16 08:39:34 2024 ] 
Training: Epoch [97/150], Step [1499], Loss: 0.016819065436720848, Training Accuracy: 95.85000000000001
[ Tue Jul 16 08:39:35 2024 ] 	Batch(1500/6809) done. Loss: 0.2020  lr:0.000001
[ Tue Jul 16 08:39:58 2024 ] 	Batch(1600/6809) done. Loss: 0.0096  lr:0.000001
[ Tue Jul 16 08:40:21 2024 ] 	Batch(1700/6809) done. Loss: 0.2561  lr:0.000001
[ Tue Jul 16 08:40:43 2024 ] 	Batch(1800/6809) done. Loss: 0.0785  lr:0.000001
[ Tue Jul 16 08:41:06 2024 ] 	Batch(1900/6809) done. Loss: 0.0344  lr:0.000001
[ Tue Jul 16 08:41:28 2024 ] 
Training: Epoch [97/150], Step [1999], Loss: 0.1857726275920868, Training Accuracy: 95.91875
[ Tue Jul 16 08:41:29 2024 ] 	Batch(2000/6809) done. Loss: 0.0280  lr:0.000001
[ Tue Jul 16 08:41:51 2024 ] 	Batch(2100/6809) done. Loss: 0.1247  lr:0.000001
[ Tue Jul 16 08:42:14 2024 ] 	Batch(2200/6809) done. Loss: 0.0066  lr:0.000001
[ Tue Jul 16 08:42:36 2024 ] 	Batch(2300/6809) done. Loss: 0.3082  lr:0.000001
[ Tue Jul 16 08:42:59 2024 ] 	Batch(2400/6809) done. Loss: 0.1458  lr:0.000001
[ Tue Jul 16 08:43:22 2024 ] 
Training: Epoch [97/150], Step [2499], Loss: 0.048054423183202744, Training Accuracy: 95.95
[ Tue Jul 16 08:43:22 2024 ] 	Batch(2500/6809) done. Loss: 0.1519  lr:0.000001
[ Tue Jul 16 08:43:45 2024 ] 	Batch(2600/6809) done. Loss: 0.1794  lr:0.000001
[ Tue Jul 16 08:44:08 2024 ] 	Batch(2700/6809) done. Loss: 0.0141  lr:0.000001
[ Tue Jul 16 08:44:32 2024 ] 	Batch(2800/6809) done. Loss: 0.0300  lr:0.000001
[ Tue Jul 16 08:44:55 2024 ] 	Batch(2900/6809) done. Loss: 0.0775  lr:0.000001
[ Tue Jul 16 08:45:17 2024 ] 
Training: Epoch [97/150], Step [2999], Loss: 0.04407045245170593, Training Accuracy: 95.95833333333333
[ Tue Jul 16 08:45:18 2024 ] 	Batch(3000/6809) done. Loss: 0.3276  lr:0.000001
[ Tue Jul 16 08:45:41 2024 ] 	Batch(3100/6809) done. Loss: 0.0218  lr:0.000001
[ Tue Jul 16 08:46:04 2024 ] 	Batch(3200/6809) done. Loss: 0.2407  lr:0.000001
[ Tue Jul 16 08:46:27 2024 ] 	Batch(3300/6809) done. Loss: 0.0448  lr:0.000001
[ Tue Jul 16 08:46:50 2024 ] 	Batch(3400/6809) done. Loss: 0.1709  lr:0.000001
[ Tue Jul 16 08:47:13 2024 ] 
Training: Epoch [97/150], Step [3499], Loss: 0.12028330564498901, Training Accuracy: 95.88214285714287
[ Tue Jul 16 08:47:13 2024 ] 	Batch(3500/6809) done. Loss: 0.0036  lr:0.000001
[ Tue Jul 16 08:47:36 2024 ] 	Batch(3600/6809) done. Loss: 0.3366  lr:0.000001
[ Tue Jul 16 08:47:59 2024 ] 	Batch(3700/6809) done. Loss: 0.3706  lr:0.000001
[ Tue Jul 16 08:48:22 2024 ] 	Batch(3800/6809) done. Loss: 1.0137  lr:0.000001
[ Tue Jul 16 08:48:45 2024 ] 	Batch(3900/6809) done. Loss: 0.1368  lr:0.000001
[ Tue Jul 16 08:49:08 2024 ] 
Training: Epoch [97/150], Step [3999], Loss: 0.7006890177726746, Training Accuracy: 95.81562500000001
[ Tue Jul 16 08:49:08 2024 ] 	Batch(4000/6809) done. Loss: 0.2171  lr:0.000001
[ Tue Jul 16 08:49:31 2024 ] 	Batch(4100/6809) done. Loss: 0.0825  lr:0.000001
[ Tue Jul 16 08:49:54 2024 ] 	Batch(4200/6809) done. Loss: 0.2013  lr:0.000001
[ Tue Jul 16 08:50:17 2024 ] 	Batch(4300/6809) done. Loss: 0.3063  lr:0.000001
[ Tue Jul 16 08:50:40 2024 ] 	Batch(4400/6809) done. Loss: 0.0856  lr:0.000001
[ Tue Jul 16 08:51:03 2024 ] 
Training: Epoch [97/150], Step [4499], Loss: 0.04456527158617973, Training Accuracy: 95.78055555555555
[ Tue Jul 16 08:51:03 2024 ] 	Batch(4500/6809) done. Loss: 0.1710  lr:0.000001
[ Tue Jul 16 08:51:26 2024 ] 	Batch(4600/6809) done. Loss: 0.0389  lr:0.000001
[ Tue Jul 16 08:51:49 2024 ] 	Batch(4700/6809) done. Loss: 0.1460  lr:0.000001
[ Tue Jul 16 08:52:12 2024 ] 	Batch(4800/6809) done. Loss: 0.3079  lr:0.000001
[ Tue Jul 16 08:52:35 2024 ] 	Batch(4900/6809) done. Loss: 0.0372  lr:0.000001
[ Tue Jul 16 08:52:58 2024 ] 
Training: Epoch [97/150], Step [4999], Loss: 0.40241047739982605, Training Accuracy: 95.78
[ Tue Jul 16 08:52:58 2024 ] 	Batch(5000/6809) done. Loss: 0.0702  lr:0.000001
[ Tue Jul 16 08:53:21 2024 ] 	Batch(5100/6809) done. Loss: 0.6627  lr:0.000001
[ Tue Jul 16 08:53:44 2024 ] 	Batch(5200/6809) done. Loss: 0.0403  lr:0.000001
[ Tue Jul 16 08:54:07 2024 ] 	Batch(5300/6809) done. Loss: 0.2174  lr:0.000001
[ Tue Jul 16 08:54:30 2024 ] 	Batch(5400/6809) done. Loss: 0.6317  lr:0.000001
[ Tue Jul 16 08:54:53 2024 ] 
Training: Epoch [97/150], Step [5499], Loss: 0.05062326043844223, Training Accuracy: 95.72727272727273
[ Tue Jul 16 08:54:53 2024 ] 	Batch(5500/6809) done. Loss: 0.0277  lr:0.000001
[ Tue Jul 16 08:55:16 2024 ] 	Batch(5600/6809) done. Loss: 0.5314  lr:0.000001
[ Tue Jul 16 08:55:39 2024 ] 	Batch(5700/6809) done. Loss: 0.0730  lr:0.000001
[ Tue Jul 16 08:56:02 2024 ] 	Batch(5800/6809) done. Loss: 0.1201  lr:0.000001
[ Tue Jul 16 08:56:25 2024 ] 	Batch(5900/6809) done. Loss: 0.0237  lr:0.000001
[ Tue Jul 16 08:56:48 2024 ] 
Training: Epoch [97/150], Step [5999], Loss: 0.10257411003112793, Training Accuracy: 95.69583333333334
[ Tue Jul 16 08:56:48 2024 ] 	Batch(6000/6809) done. Loss: 0.0101  lr:0.000001
[ Tue Jul 16 08:57:11 2024 ] 	Batch(6100/6809) done. Loss: 0.0087  lr:0.000001
[ Tue Jul 16 08:57:34 2024 ] 	Batch(6200/6809) done. Loss: 0.0611  lr:0.000001
[ Tue Jul 16 08:57:57 2024 ] 	Batch(6300/6809) done. Loss: 0.0497  lr:0.000001
[ Tue Jul 16 08:58:20 2024 ] 	Batch(6400/6809) done. Loss: 0.0216  lr:0.000001
[ Tue Jul 16 08:58:43 2024 ] 
Training: Epoch [97/150], Step [6499], Loss: 0.08989356458187103, Training Accuracy: 95.69807692307693
[ Tue Jul 16 08:58:43 2024 ] 	Batch(6500/6809) done. Loss: 0.2990  lr:0.000001
[ Tue Jul 16 08:59:06 2024 ] 	Batch(6600/6809) done. Loss: 0.0439  lr:0.000001
[ Tue Jul 16 08:59:29 2024 ] 	Batch(6700/6809) done. Loss: 0.0489  lr:0.000001
[ Tue Jul 16 08:59:53 2024 ] 	Batch(6800/6809) done. Loss: 0.0143  lr:0.000001
[ Tue Jul 16 08:59:54 2024 ] 	Mean training loss: 0.1548.
[ Tue Jul 16 08:59:54 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 08:59:55 2024 ] Training epoch: 99
[ Tue Jul 16 08:59:55 2024 ] 	Batch(0/6809) done. Loss: 0.0186  lr:0.000001
[ Tue Jul 16 09:00:18 2024 ] 	Batch(100/6809) done. Loss: 0.0218  lr:0.000001
[ Tue Jul 16 09:00:41 2024 ] 	Batch(200/6809) done. Loss: 0.2365  lr:0.000001
[ Tue Jul 16 09:01:04 2024 ] 	Batch(300/6809) done. Loss: 0.0399  lr:0.000001
[ Tue Jul 16 09:01:28 2024 ] 	Batch(400/6809) done. Loss: 0.2535  lr:0.000001
[ Tue Jul 16 09:01:50 2024 ] 
Training: Epoch [98/150], Step [499], Loss: 0.4001336097717285, Training Accuracy: 95.975
[ Tue Jul 16 09:01:50 2024 ] 	Batch(500/6809) done. Loss: 0.0385  lr:0.000001
[ Tue Jul 16 09:02:13 2024 ] 	Batch(600/6809) done. Loss: 0.4520  lr:0.000001
[ Tue Jul 16 09:02:37 2024 ] 	Batch(700/6809) done. Loss: 0.1650  lr:0.000001
[ Tue Jul 16 09:02:59 2024 ] 	Batch(800/6809) done. Loss: 0.2806  lr:0.000001
[ Tue Jul 16 09:03:23 2024 ] 	Batch(900/6809) done. Loss: 0.0009  lr:0.000001
[ Tue Jul 16 09:03:45 2024 ] 
Training: Epoch [98/150], Step [999], Loss: 0.48578590154647827, Training Accuracy: 95.5
[ Tue Jul 16 09:03:45 2024 ] 	Batch(1000/6809) done. Loss: 0.0268  lr:0.000001
[ Tue Jul 16 09:04:08 2024 ] 	Batch(1100/6809) done. Loss: 0.0506  lr:0.000001
[ Tue Jul 16 09:04:31 2024 ] 	Batch(1200/6809) done. Loss: 0.2816  lr:0.000001
[ Tue Jul 16 09:04:54 2024 ] 	Batch(1300/6809) done. Loss: 0.1207  lr:0.000001
[ Tue Jul 16 09:05:17 2024 ] 	Batch(1400/6809) done. Loss: 0.0639  lr:0.000001
[ Tue Jul 16 09:05:40 2024 ] 
Training: Epoch [98/150], Step [1499], Loss: 0.015285838395357132, Training Accuracy: 95.6
[ Tue Jul 16 09:05:40 2024 ] 	Batch(1500/6809) done. Loss: 0.2170  lr:0.000001
[ Tue Jul 16 09:06:04 2024 ] 	Batch(1600/6809) done. Loss: 0.1783  lr:0.000001
[ Tue Jul 16 09:06:27 2024 ] 	Batch(1700/6809) done. Loss: 0.2236  lr:0.000001
[ Tue Jul 16 09:06:51 2024 ] 	Batch(1800/6809) done. Loss: 0.0391  lr:0.000001
[ Tue Jul 16 09:07:14 2024 ] 	Batch(1900/6809) done. Loss: 0.1707  lr:0.000001
[ Tue Jul 16 09:07:37 2024 ] 
Training: Epoch [98/150], Step [1999], Loss: 0.1109994649887085, Training Accuracy: 95.63125
[ Tue Jul 16 09:07:37 2024 ] 	Batch(2000/6809) done. Loss: 0.0749  lr:0.000001
[ Tue Jul 16 09:08:00 2024 ] 	Batch(2100/6809) done. Loss: 0.0525  lr:0.000001
[ Tue Jul 16 09:08:22 2024 ] 	Batch(2200/6809) done. Loss: 0.1296  lr:0.000001
[ Tue Jul 16 09:08:45 2024 ] 	Batch(2300/6809) done. Loss: 0.4781  lr:0.000001
[ Tue Jul 16 09:09:08 2024 ] 	Batch(2400/6809) done. Loss: 0.2033  lr:0.000001
[ Tue Jul 16 09:09:30 2024 ] 
Training: Epoch [98/150], Step [2499], Loss: 0.011681093834340572, Training Accuracy: 95.57
[ Tue Jul 16 09:09:31 2024 ] 	Batch(2500/6809) done. Loss: 0.2878  lr:0.000001
[ Tue Jul 16 09:09:53 2024 ] 	Batch(2600/6809) done. Loss: 0.0397  lr:0.000001
[ Tue Jul 16 09:10:16 2024 ] 	Batch(2700/6809) done. Loss: 0.6222  lr:0.000001
[ Tue Jul 16 09:10:39 2024 ] 	Batch(2800/6809) done. Loss: 0.2339  lr:0.000001
[ Tue Jul 16 09:11:02 2024 ] 	Batch(2900/6809) done. Loss: 0.3320  lr:0.000001
[ Tue Jul 16 09:11:24 2024 ] 
Training: Epoch [98/150], Step [2999], Loss: 0.007156725972890854, Training Accuracy: 95.6125
[ Tue Jul 16 09:11:24 2024 ] 	Batch(3000/6809) done. Loss: 0.0283  lr:0.000001
[ Tue Jul 16 09:11:47 2024 ] 	Batch(3100/6809) done. Loss: 0.2318  lr:0.000001
[ Tue Jul 16 09:12:10 2024 ] 	Batch(3200/6809) done. Loss: 0.0386  lr:0.000001
[ Tue Jul 16 09:12:33 2024 ] 	Batch(3300/6809) done. Loss: 0.1458  lr:0.000001
[ Tue Jul 16 09:12:55 2024 ] 	Batch(3400/6809) done. Loss: 0.2512  lr:0.000001
[ Tue Jul 16 09:13:18 2024 ] 
Training: Epoch [98/150], Step [3499], Loss: 0.05962270125746727, Training Accuracy: 95.63571428571429
[ Tue Jul 16 09:13:18 2024 ] 	Batch(3500/6809) done. Loss: 0.2084  lr:0.000001
[ Tue Jul 16 09:13:41 2024 ] 	Batch(3600/6809) done. Loss: 0.0243  lr:0.000001
[ Tue Jul 16 09:14:04 2024 ] 	Batch(3700/6809) done. Loss: 0.1667  lr:0.000001
[ Tue Jul 16 09:14:26 2024 ] 	Batch(3800/6809) done. Loss: 0.1942  lr:0.000001
[ Tue Jul 16 09:14:49 2024 ] 	Batch(3900/6809) done. Loss: 0.0927  lr:0.000001
[ Tue Jul 16 09:15:11 2024 ] 
Training: Epoch [98/150], Step [3999], Loss: 0.05541473999619484, Training Accuracy: 95.646875
[ Tue Jul 16 09:15:12 2024 ] 	Batch(4000/6809) done. Loss: 0.1650  lr:0.000001
[ Tue Jul 16 09:15:34 2024 ] 	Batch(4100/6809) done. Loss: 0.0208  lr:0.000001
[ Tue Jul 16 09:15:57 2024 ] 	Batch(4200/6809) done. Loss: 0.2093  lr:0.000001
[ Tue Jul 16 09:16:20 2024 ] 	Batch(4300/6809) done. Loss: 0.1181  lr:0.000001
[ Tue Jul 16 09:16:43 2024 ] 	Batch(4400/6809) done. Loss: 0.1029  lr:0.000001
[ Tue Jul 16 09:17:05 2024 ] 
Training: Epoch [98/150], Step [4499], Loss: 0.01449214480817318, Training Accuracy: 95.62222222222222
[ Tue Jul 16 09:17:05 2024 ] 	Batch(4500/6809) done. Loss: 0.0031  lr:0.000001
[ Tue Jul 16 09:17:28 2024 ] 	Batch(4600/6809) done. Loss: 0.0180  lr:0.000001
[ Tue Jul 16 09:17:51 2024 ] 	Batch(4700/6809) done. Loss: 0.2895  lr:0.000001
[ Tue Jul 16 09:18:13 2024 ] 	Batch(4800/6809) done. Loss: 0.1047  lr:0.000001
[ Tue Jul 16 09:18:36 2024 ] 	Batch(4900/6809) done. Loss: 0.0702  lr:0.000001
[ Tue Jul 16 09:18:59 2024 ] 
Training: Epoch [98/150], Step [4999], Loss: 0.046111028641462326, Training Accuracy: 95.63000000000001
[ Tue Jul 16 09:18:59 2024 ] 	Batch(5000/6809) done. Loss: 0.0053  lr:0.000001
[ Tue Jul 16 09:19:22 2024 ] 	Batch(5100/6809) done. Loss: 0.0483  lr:0.000001
[ Tue Jul 16 09:19:44 2024 ] 	Batch(5200/6809) done. Loss: 0.0716  lr:0.000001
[ Tue Jul 16 09:20:07 2024 ] 	Batch(5300/6809) done. Loss: 0.0560  lr:0.000001
[ Tue Jul 16 09:20:30 2024 ] 	Batch(5400/6809) done. Loss: 0.1422  lr:0.000001
[ Tue Jul 16 09:20:52 2024 ] 
Training: Epoch [98/150], Step [5499], Loss: 0.09162474423646927, Training Accuracy: 95.6340909090909
[ Tue Jul 16 09:20:53 2024 ] 	Batch(5500/6809) done. Loss: 0.0134  lr:0.000001
[ Tue Jul 16 09:21:16 2024 ] 	Batch(5600/6809) done. Loss: 0.5111  lr:0.000001
[ Tue Jul 16 09:21:38 2024 ] 	Batch(5700/6809) done. Loss: 0.0822  lr:0.000001
[ Tue Jul 16 09:22:01 2024 ] 	Batch(5800/6809) done. Loss: 0.0406  lr:0.000001
[ Tue Jul 16 09:22:24 2024 ] 	Batch(5900/6809) done. Loss: 0.0353  lr:0.000001
[ Tue Jul 16 09:22:46 2024 ] 
Training: Epoch [98/150], Step [5999], Loss: 0.0822058916091919, Training Accuracy: 95.65833333333333
[ Tue Jul 16 09:22:47 2024 ] 	Batch(6000/6809) done. Loss: 0.0863  lr:0.000001
[ Tue Jul 16 09:23:09 2024 ] 	Batch(6100/6809) done. Loss: 0.0220  lr:0.000001
[ Tue Jul 16 09:23:33 2024 ] 	Batch(6200/6809) done. Loss: 0.2047  lr:0.000001
[ Tue Jul 16 09:23:56 2024 ] 	Batch(6300/6809) done. Loss: 0.1835  lr:0.000001
[ Tue Jul 16 09:24:19 2024 ] 	Batch(6400/6809) done. Loss: 0.0377  lr:0.000001
[ Tue Jul 16 09:24:42 2024 ] 
Training: Epoch [98/150], Step [6499], Loss: 0.27813929319381714, Training Accuracy: 95.62884615384615
[ Tue Jul 16 09:24:42 2024 ] 	Batch(6500/6809) done. Loss: 0.1061  lr:0.000001
[ Tue Jul 16 09:25:04 2024 ] 	Batch(6600/6809) done. Loss: 0.1187  lr:0.000001
[ Tue Jul 16 09:25:27 2024 ] 	Batch(6700/6809) done. Loss: 0.5026  lr:0.000001
[ Tue Jul 16 09:25:50 2024 ] 	Batch(6800/6809) done. Loss: 0.0990  lr:0.000001
[ Tue Jul 16 09:25:52 2024 ] 	Mean training loss: 0.1577.
[ Tue Jul 16 09:25:52 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 09:25:52 2024 ] Training epoch: 100
[ Tue Jul 16 09:25:53 2024 ] 	Batch(0/6809) done. Loss: 0.0490  lr:0.000001
[ Tue Jul 16 09:26:16 2024 ] 	Batch(100/6809) done. Loss: 0.0682  lr:0.000001
[ Tue Jul 16 09:26:39 2024 ] 	Batch(200/6809) done. Loss: 0.1943  lr:0.000001
[ Tue Jul 16 09:27:02 2024 ] 	Batch(300/6809) done. Loss: 0.1951  lr:0.000001
[ Tue Jul 16 09:27:25 2024 ] 	Batch(400/6809) done. Loss: 0.0117  lr:0.000001
[ Tue Jul 16 09:27:47 2024 ] 
Training: Epoch [99/150], Step [499], Loss: 0.19266802072525024, Training Accuracy: 95.975
[ Tue Jul 16 09:27:48 2024 ] 	Batch(500/6809) done. Loss: 0.5065  lr:0.000001
[ Tue Jul 16 09:28:10 2024 ] 	Batch(600/6809) done. Loss: 0.0717  lr:0.000001
[ Tue Jul 16 09:28:33 2024 ] 	Batch(700/6809) done. Loss: 0.4834  lr:0.000001
[ Tue Jul 16 09:28:56 2024 ] 	Batch(800/6809) done. Loss: 0.3833  lr:0.000001
[ Tue Jul 16 09:29:18 2024 ] 	Batch(900/6809) done. Loss: 0.0016  lr:0.000001
[ Tue Jul 16 09:29:41 2024 ] 
Training: Epoch [99/150], Step [999], Loss: 0.6522403955459595, Training Accuracy: 95.6375
[ Tue Jul 16 09:29:41 2024 ] 	Batch(1000/6809) done. Loss: 0.1262  lr:0.000001
[ Tue Jul 16 09:30:03 2024 ] 	Batch(1100/6809) done. Loss: 0.1294  lr:0.000001
[ Tue Jul 16 09:30:26 2024 ] 	Batch(1200/6809) done. Loss: 0.0151  lr:0.000001
[ Tue Jul 16 09:30:49 2024 ] 	Batch(1300/6809) done. Loss: 0.3742  lr:0.000001
[ Tue Jul 16 09:31:11 2024 ] 	Batch(1400/6809) done. Loss: 0.0481  lr:0.000001
[ Tue Jul 16 09:31:33 2024 ] 
Training: Epoch [99/150], Step [1499], Loss: 0.025639532133936882, Training Accuracy: 95.66666666666667
[ Tue Jul 16 09:31:34 2024 ] 	Batch(1500/6809) done. Loss: 0.0564  lr:0.000001
[ Tue Jul 16 09:31:56 2024 ] 	Batch(1600/6809) done. Loss: 0.0319  lr:0.000001
[ Tue Jul 16 09:32:19 2024 ] 	Batch(1700/6809) done. Loss: 0.2313  lr:0.000001
[ Tue Jul 16 09:32:42 2024 ] 	Batch(1800/6809) done. Loss: 0.0813  lr:0.000001
[ Tue Jul 16 09:33:04 2024 ] 	Batch(1900/6809) done. Loss: 0.0233  lr:0.000001
[ Tue Jul 16 09:33:26 2024 ] 
Training: Epoch [99/150], Step [1999], Loss: 0.4797281324863434, Training Accuracy: 95.675
[ Tue Jul 16 09:33:27 2024 ] 	Batch(2000/6809) done. Loss: 0.2775  lr:0.000001
[ Tue Jul 16 09:33:49 2024 ] 	Batch(2100/6809) done. Loss: 0.2718  lr:0.000001
[ Tue Jul 16 09:34:12 2024 ] 	Batch(2200/6809) done. Loss: 0.0053  lr:0.000001
[ Tue Jul 16 09:34:34 2024 ] 	Batch(2300/6809) done. Loss: 0.0806  lr:0.000001
[ Tue Jul 16 09:34:57 2024 ] 	Batch(2400/6809) done. Loss: 0.0521  lr:0.000001
[ Tue Jul 16 09:35:20 2024 ] 
Training: Epoch [99/150], Step [2499], Loss: 0.09083835780620575, Training Accuracy: 95.67999999999999
[ Tue Jul 16 09:35:20 2024 ] 	Batch(2500/6809) done. Loss: 0.3359  lr:0.000001
[ Tue Jul 16 09:35:42 2024 ] 	Batch(2600/6809) done. Loss: 0.0844  lr:0.000001
[ Tue Jul 16 09:36:05 2024 ] 	Batch(2700/6809) done. Loss: 0.6273  lr:0.000001
[ Tue Jul 16 09:36:28 2024 ] 	Batch(2800/6809) done. Loss: 0.1723  lr:0.000001
[ Tue Jul 16 09:36:50 2024 ] 	Batch(2900/6809) done. Loss: 0.0576  lr:0.000001
[ Tue Jul 16 09:37:12 2024 ] 
Training: Epoch [99/150], Step [2999], Loss: 0.027130242437124252, Training Accuracy: 95.69166666666666
[ Tue Jul 16 09:37:13 2024 ] 	Batch(3000/6809) done. Loss: 0.0303  lr:0.000001
[ Tue Jul 16 09:37:35 2024 ] 	Batch(3100/6809) done. Loss: 0.2461  lr:0.000001
[ Tue Jul 16 09:37:58 2024 ] 	Batch(3200/6809) done. Loss: 0.0470  lr:0.000001
[ Tue Jul 16 09:38:21 2024 ] 	Batch(3300/6809) done. Loss: 0.1279  lr:0.000001
[ Tue Jul 16 09:38:44 2024 ] 	Batch(3400/6809) done. Loss: 0.0259  lr:0.000001
[ Tue Jul 16 09:39:07 2024 ] 
Training: Epoch [99/150], Step [3499], Loss: 0.09201192855834961, Training Accuracy: 95.625
[ Tue Jul 16 09:39:07 2024 ] 	Batch(3500/6809) done. Loss: 0.1018  lr:0.000001
[ Tue Jul 16 09:39:30 2024 ] 	Batch(3600/6809) done. Loss: 0.0073  lr:0.000001
[ Tue Jul 16 09:39:53 2024 ] 	Batch(3700/6809) done. Loss: 0.0044  lr:0.000001
[ Tue Jul 16 09:40:16 2024 ] 	Batch(3800/6809) done. Loss: 0.2039  lr:0.000001
[ Tue Jul 16 09:40:39 2024 ] 	Batch(3900/6809) done. Loss: 0.0353  lr:0.000001
[ Tue Jul 16 09:41:02 2024 ] 
Training: Epoch [99/150], Step [3999], Loss: 0.13498173654079437, Training Accuracy: 95.625
[ Tue Jul 16 09:41:03 2024 ] 	Batch(4000/6809) done. Loss: 0.0552  lr:0.000001
[ Tue Jul 16 09:41:26 2024 ] 	Batch(4100/6809) done. Loss: 0.2038  lr:0.000001
[ Tue Jul 16 09:41:48 2024 ] 	Batch(4200/6809) done. Loss: 0.0465  lr:0.000001
[ Tue Jul 16 09:42:11 2024 ] 	Batch(4300/6809) done. Loss: 0.2340  lr:0.000001
[ Tue Jul 16 09:42:35 2024 ] 	Batch(4400/6809) done. Loss: 0.0682  lr:0.000001
[ Tue Jul 16 09:42:57 2024 ] 
Training: Epoch [99/150], Step [4499], Loss: 0.25005123019218445, Training Accuracy: 95.575
[ Tue Jul 16 09:42:57 2024 ] 	Batch(4500/6809) done. Loss: 0.0101  lr:0.000001
[ Tue Jul 16 09:43:20 2024 ] 	Batch(4600/6809) done. Loss: 0.3351  lr:0.000001
[ Tue Jul 16 09:43:42 2024 ] 	Batch(4700/6809) done. Loss: 0.2703  lr:0.000001
[ Tue Jul 16 09:44:05 2024 ] 	Batch(4800/6809) done. Loss: 0.1082  lr:0.000001
[ Tue Jul 16 09:44:27 2024 ] 	Batch(4900/6809) done. Loss: 0.2590  lr:0.000001
[ Tue Jul 16 09:44:49 2024 ] 
Training: Epoch [99/150], Step [4999], Loss: 0.11201246082782745, Training Accuracy: 95.6175
[ Tue Jul 16 09:44:50 2024 ] 	Batch(5000/6809) done. Loss: 0.1094  lr:0.000001
[ Tue Jul 16 09:45:12 2024 ] 	Batch(5100/6809) done. Loss: 0.0667  lr:0.000001
[ Tue Jul 16 09:45:35 2024 ] 	Batch(5200/6809) done. Loss: 0.0500  lr:0.000001
[ Tue Jul 16 09:45:58 2024 ] 	Batch(5300/6809) done. Loss: 0.0431  lr:0.000001
[ Tue Jul 16 09:46:20 2024 ] 	Batch(5400/6809) done. Loss: 0.1751  lr:0.000001
[ Tue Jul 16 09:46:43 2024 ] 
Training: Epoch [99/150], Step [5499], Loss: 0.3591957688331604, Training Accuracy: 95.60909090909091
[ Tue Jul 16 09:46:43 2024 ] 	Batch(5500/6809) done. Loss: 0.0931  lr:0.000001
[ Tue Jul 16 09:47:06 2024 ] 	Batch(5600/6809) done. Loss: 0.1520  lr:0.000001
[ Tue Jul 16 09:47:29 2024 ] 	Batch(5700/6809) done. Loss: 0.1364  lr:0.000001
[ Tue Jul 16 09:47:51 2024 ] 	Batch(5800/6809) done. Loss: 0.2580  lr:0.000001
[ Tue Jul 16 09:48:14 2024 ] 	Batch(5900/6809) done. Loss: 0.3134  lr:0.000001
[ Tue Jul 16 09:48:37 2024 ] 
Training: Epoch [99/150], Step [5999], Loss: 0.07522599399089813, Training Accuracy: 95.61666666666667
[ Tue Jul 16 09:48:38 2024 ] 	Batch(6000/6809) done. Loss: 0.0617  lr:0.000001
[ Tue Jul 16 09:49:01 2024 ] 	Batch(6100/6809) done. Loss: 0.1919  lr:0.000001
[ Tue Jul 16 09:49:24 2024 ] 	Batch(6200/6809) done. Loss: 0.0155  lr:0.000001
[ Tue Jul 16 09:49:47 2024 ] 	Batch(6300/6809) done. Loss: 0.2710  lr:0.000001
[ Tue Jul 16 09:50:10 2024 ] 	Batch(6400/6809) done. Loss: 0.0756  lr:0.000001
[ Tue Jul 16 09:50:33 2024 ] 
Training: Epoch [99/150], Step [6499], Loss: 0.016918953508138657, Training Accuracy: 95.59807692307693
[ Tue Jul 16 09:50:33 2024 ] 	Batch(6500/6809) done. Loss: 0.2598  lr:0.000001
[ Tue Jul 16 09:50:56 2024 ] 	Batch(6600/6809) done. Loss: 0.1179  lr:0.000001
[ Tue Jul 16 09:51:19 2024 ] 	Batch(6700/6809) done. Loss: 0.0290  lr:0.000001
[ Tue Jul 16 09:51:42 2024 ] 	Batch(6800/6809) done. Loss: 0.0927  lr:0.000001
[ Tue Jul 16 09:51:44 2024 ] 	Mean training loss: 0.1551.
[ Tue Jul 16 09:51:44 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 09:51:44 2024 ] Eval epoch: 100
[ Tue Jul 16 09:58:40 2024 ] 	Mean val loss of 7435 batches: 0.9120192068671886.
[ Tue Jul 16 09:58:40 2024 ] 
Validation: Epoch [99/150], Samples [47305.0/59477], Loss: 2.9832348823547363, Validation Accuracy: 79.53494628175596
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 1 : 389 / 500 = 77 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 2 : 419 / 499 = 83 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 3 : 417 / 500 = 83 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 4 : 433 / 502 = 86 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 5 : 411 / 502 = 81 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 6 : 433 / 502 = 86 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 7 : 465 / 497 = 93 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 8 : 484 / 498 = 97 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 9 : 390 / 500 = 78 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 10 : 314 / 500 = 62 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 11 : 231 / 498 = 46 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 12 : 401 / 499 = 80 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 13 : 487 / 502 = 97 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 14 : 485 / 504 = 96 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 15 : 334 / 502 = 66 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 16 : 381 / 502 = 75 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 17 : 431 / 504 = 85 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 18 : 418 / 504 = 82 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 19 : 442 / 502 = 88 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 20 : 461 / 502 = 91 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 21 : 479 / 503 = 95 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 22 : 437 / 504 = 86 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 23 : 427 / 503 = 84 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 24 : 433 / 504 = 85 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 25 : 481 / 504 = 95 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 26 : 468 / 504 = 92 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 27 : 432 / 501 = 86 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 28 : 369 / 502 = 73 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 29 : 302 / 502 = 60 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 30 : 384 / 501 = 76 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 31 : 429 / 504 = 85 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 32 : 437 / 503 = 86 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 33 : 400 / 503 = 79 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 34 : 484 / 504 = 96 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 35 : 458 / 503 = 91 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 36 : 383 / 502 = 76 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 37 : 446 / 504 = 88 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 38 : 440 / 504 = 87 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 39 : 456 / 498 = 91 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 40 : 408 / 504 = 80 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 41 : 477 / 503 = 94 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 42 : 453 / 504 = 89 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 43 : 374 / 503 = 74 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 44 : 426 / 504 = 84 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 45 : 415 / 504 = 82 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 46 : 402 / 504 = 79 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 47 : 356 / 503 = 70 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 48 : 411 / 503 = 81 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 49 : 410 / 499 = 82 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 50 : 404 / 502 = 80 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 51 : 461 / 503 = 91 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 52 : 461 / 504 = 91 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 53 : 460 / 497 = 92 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 54 : 447 / 480 = 93 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 55 : 445 / 504 = 88 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 56 : 414 / 503 = 82 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 57 : 458 / 504 = 90 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 58 : 479 / 499 = 95 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 59 : 484 / 503 = 96 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 60 : 408 / 479 = 85 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 61 : 394 / 484 = 81 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 62 : 398 / 487 = 81 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 63 : 446 / 489 = 91 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 64 : 403 / 488 = 82 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 65 : 428 / 490 = 87 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 66 : 300 / 488 = 61 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 67 : 345 / 490 = 70 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 68 : 297 / 490 = 60 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 69 : 382 / 490 = 77 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 70 : 162 / 490 = 33 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 71 : 290 / 490 = 59 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 72 : 204 / 488 = 41 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 73 : 260 / 486 = 53 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 74 : 258 / 481 = 53 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 75 : 269 / 488 = 55 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 76 : 322 / 489 = 65 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 77 : 311 / 488 = 63 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 78 : 378 / 488 = 77 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 79 : 458 / 490 = 93 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 80 : 393 / 489 = 80 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 81 : 266 / 491 = 54 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 82 : 315 / 491 = 64 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 83 : 253 / 489 = 51 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 84 : 354 / 489 = 72 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 85 : 332 / 489 = 67 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 86 : 417 / 491 = 84 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 87 : 406 / 492 = 82 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 88 : 359 / 491 = 73 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 89 : 355 / 492 = 72 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 90 : 251 / 490 = 51 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 91 : 341 / 482 = 70 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 92 : 363 / 490 = 74 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 93 : 342 / 487 = 70 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 94 : 428 / 489 = 87 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 95 : 401 / 490 = 81 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 96 : 458 / 491 = 93 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 97 : 458 / 490 = 93 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 98 : 439 / 491 = 89 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 99 : 448 / 491 = 91 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 100 : 449 / 491 = 91 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 101 : 416 / 491 = 84 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 102 : 275 / 492 = 55 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 103 : 382 / 492 = 77 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 104 : 303 / 491 = 61 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 105 : 295 / 491 = 60 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 106 : 301 / 492 = 61 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 107 : 411 / 491 = 83 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 108 : 370 / 492 = 75 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 109 : 352 / 490 = 71 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 110 : 408 / 491 = 83 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 111 : 442 / 492 = 89 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 112 : 454 / 492 = 92 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 113 : 436 / 491 = 88 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 114 : 397 / 491 = 80 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 115 : 420 / 492 = 85 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 116 : 414 / 491 = 84 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 117 : 392 / 492 = 79 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 118 : 415 / 490 = 84 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 119 : 452 / 492 = 91 %
[ Tue Jul 16 09:58:40 2024 ] Accuracy of 120 : 413 / 500 = 82 %
[ Tue Jul 16 09:58:40 2024 ] Training epoch: 101
[ Tue Jul 16 09:58:41 2024 ] 	Batch(0/6809) done. Loss: 0.0237  lr:0.000001
[ Tue Jul 16 09:59:04 2024 ] 	Batch(100/6809) done. Loss: 0.0409  lr:0.000001
[ Tue Jul 16 09:59:27 2024 ] 	Batch(200/6809) done. Loss: 0.0281  lr:0.000001
[ Tue Jul 16 09:59:50 2024 ] 	Batch(300/6809) done. Loss: 0.2181  lr:0.000001
[ Tue Jul 16 10:00:13 2024 ] 	Batch(400/6809) done. Loss: 0.0879  lr:0.000001
[ Tue Jul 16 10:00:35 2024 ] 
Training: Epoch [100/150], Step [499], Loss: 0.2967579960823059, Training Accuracy: 95.92500000000001
[ Tue Jul 16 10:00:36 2024 ] 	Batch(500/6809) done. Loss: 0.0177  lr:0.000001
[ Tue Jul 16 10:00:59 2024 ] 	Batch(600/6809) done. Loss: 0.1333  lr:0.000001
[ Tue Jul 16 10:01:22 2024 ] 	Batch(700/6809) done. Loss: 0.0130  lr:0.000001
[ Tue Jul 16 10:01:45 2024 ] 	Batch(800/6809) done. Loss: 0.0881  lr:0.000001
[ Tue Jul 16 10:02:08 2024 ] 	Batch(900/6809) done. Loss: 0.1873  lr:0.000001
[ Tue Jul 16 10:02:30 2024 ] 
Training: Epoch [100/150], Step [999], Loss: 0.10835310816764832, Training Accuracy: 95.76249999999999
[ Tue Jul 16 10:02:31 2024 ] 	Batch(1000/6809) done. Loss: 0.0203  lr:0.000001
[ Tue Jul 16 10:02:54 2024 ] 	Batch(1100/6809) done. Loss: 0.1472  lr:0.000001
[ Tue Jul 16 10:03:17 2024 ] 	Batch(1200/6809) done. Loss: 0.1008  lr:0.000001
[ Tue Jul 16 10:03:40 2024 ] 	Batch(1300/6809) done. Loss: 0.0488  lr:0.000001
[ Tue Jul 16 10:04:03 2024 ] 	Batch(1400/6809) done. Loss: 0.0187  lr:0.000001
[ Tue Jul 16 10:04:25 2024 ] 
Training: Epoch [100/150], Step [1499], Loss: 0.29975825548171997, Training Accuracy: 95.84166666666667
[ Tue Jul 16 10:04:26 2024 ] 	Batch(1500/6809) done. Loss: 0.0563  lr:0.000001
[ Tue Jul 16 10:04:49 2024 ] 	Batch(1600/6809) done. Loss: 0.0400  lr:0.000001
[ Tue Jul 16 10:05:12 2024 ] 	Batch(1700/6809) done. Loss: 0.3012  lr:0.000001
[ Tue Jul 16 10:05:35 2024 ] 	Batch(1800/6809) done. Loss: 0.2894  lr:0.000001
[ Tue Jul 16 10:05:58 2024 ] 	Batch(1900/6809) done. Loss: 0.0234  lr:0.000001
[ Tue Jul 16 10:06:21 2024 ] 
Training: Epoch [100/150], Step [1999], Loss: 0.3143460750579834, Training Accuracy: 95.68124999999999
[ Tue Jul 16 10:06:21 2024 ] 	Batch(2000/6809) done. Loss: 0.3014  lr:0.000001
[ Tue Jul 16 10:06:44 2024 ] 	Batch(2100/6809) done. Loss: 0.0818  lr:0.000001
[ Tue Jul 16 10:07:07 2024 ] 	Batch(2200/6809) done. Loss: 0.0141  lr:0.000001
[ Tue Jul 16 10:07:30 2024 ] 	Batch(2300/6809) done. Loss: 0.0746  lr:0.000001
[ Tue Jul 16 10:07:53 2024 ] 	Batch(2400/6809) done. Loss: 0.2061  lr:0.000001
[ Tue Jul 16 10:08:16 2024 ] 
Training: Epoch [100/150], Step [2499], Loss: 0.06213804706931114, Training Accuracy: 95.785
[ Tue Jul 16 10:08:16 2024 ] 	Batch(2500/6809) done. Loss: 0.3040  lr:0.000001
[ Tue Jul 16 10:08:39 2024 ] 	Batch(2600/6809) done. Loss: 0.2751  lr:0.000001
[ Tue Jul 16 10:09:02 2024 ] 	Batch(2700/6809) done. Loss: 0.1115  lr:0.000001
[ Tue Jul 16 10:09:25 2024 ] 	Batch(2800/6809) done. Loss: 0.0611  lr:0.000001
[ Tue Jul 16 10:09:48 2024 ] 	Batch(2900/6809) done. Loss: 0.1960  lr:0.000001
[ Tue Jul 16 10:10:11 2024 ] 
Training: Epoch [100/150], Step [2999], Loss: 0.6781619191169739, Training Accuracy: 95.75416666666666
[ Tue Jul 16 10:10:11 2024 ] 	Batch(3000/6809) done. Loss: 0.0293  lr:0.000001
[ Tue Jul 16 10:10:34 2024 ] 	Batch(3100/6809) done. Loss: 0.2745  lr:0.000001
[ Tue Jul 16 10:10:57 2024 ] 	Batch(3200/6809) done. Loss: 0.0126  lr:0.000001
[ Tue Jul 16 10:11:20 2024 ] 	Batch(3300/6809) done. Loss: 0.2318  lr:0.000001
[ Tue Jul 16 10:11:43 2024 ] 	Batch(3400/6809) done. Loss: 0.1046  lr:0.000001
[ Tue Jul 16 10:12:06 2024 ] 
Training: Epoch [100/150], Step [3499], Loss: 0.18605723977088928, Training Accuracy: 95.775
[ Tue Jul 16 10:12:06 2024 ] 	Batch(3500/6809) done. Loss: 0.0887  lr:0.000001
[ Tue Jul 16 10:12:29 2024 ] 	Batch(3600/6809) done. Loss: 0.0679  lr:0.000001
[ Tue Jul 16 10:12:52 2024 ] 	Batch(3700/6809) done. Loss: 0.0036  lr:0.000001
[ Tue Jul 16 10:13:15 2024 ] 	Batch(3800/6809) done. Loss: 0.1587  lr:0.000001
[ Tue Jul 16 10:13:38 2024 ] 	Batch(3900/6809) done. Loss: 0.0469  lr:0.000001
[ Tue Jul 16 10:14:00 2024 ] 
Training: Epoch [100/150], Step [3999], Loss: 0.012781401164829731, Training Accuracy: 95.81875
[ Tue Jul 16 10:14:01 2024 ] 	Batch(4000/6809) done. Loss: 0.0131  lr:0.000001
[ Tue Jul 16 10:14:24 2024 ] 	Batch(4100/6809) done. Loss: 0.0544  lr:0.000001
[ Tue Jul 16 10:14:47 2024 ] 	Batch(4200/6809) done. Loss: 0.1699  lr:0.000001
[ Tue Jul 16 10:15:10 2024 ] 	Batch(4300/6809) done. Loss: 0.0934  lr:0.000001
[ Tue Jul 16 10:15:33 2024 ] 	Batch(4400/6809) done. Loss: 0.0462  lr:0.000001
[ Tue Jul 16 10:15:56 2024 ] 
Training: Epoch [100/150], Step [4499], Loss: 0.09581567347049713, Training Accuracy: 95.82777777777778
[ Tue Jul 16 10:15:56 2024 ] 	Batch(4500/6809) done. Loss: 0.0979  lr:0.000001
[ Tue Jul 16 10:16:19 2024 ] 	Batch(4600/6809) done. Loss: 0.3483  lr:0.000001
[ Tue Jul 16 10:16:42 2024 ] 	Batch(4700/6809) done. Loss: 0.4835  lr:0.000001
[ Tue Jul 16 10:17:05 2024 ] 	Batch(4800/6809) done. Loss: 0.0044  lr:0.000001
[ Tue Jul 16 10:17:28 2024 ] 	Batch(4900/6809) done. Loss: 0.1157  lr:0.000001
[ Tue Jul 16 10:17:51 2024 ] 
Training: Epoch [100/150], Step [4999], Loss: 0.10872168838977814, Training Accuracy: 95.7825
[ Tue Jul 16 10:17:51 2024 ] 	Batch(5000/6809) done. Loss: 0.1340  lr:0.000001
[ Tue Jul 16 10:18:14 2024 ] 	Batch(5100/6809) done. Loss: 0.0725  lr:0.000001
[ Tue Jul 16 10:18:37 2024 ] 	Batch(5200/6809) done. Loss: 0.0609  lr:0.000001
[ Tue Jul 16 10:19:00 2024 ] 	Batch(5300/6809) done. Loss: 0.3214  lr:0.000001
[ Tue Jul 16 10:19:23 2024 ] 	Batch(5400/6809) done. Loss: 0.0722  lr:0.000001
[ Tue Jul 16 10:19:45 2024 ] 
Training: Epoch [100/150], Step [5499], Loss: 0.0030842674896121025, Training Accuracy: 95.75
[ Tue Jul 16 10:19:46 2024 ] 	Batch(5500/6809) done. Loss: 0.0200  lr:0.000001
[ Tue Jul 16 10:20:09 2024 ] 	Batch(5600/6809) done. Loss: 0.0134  lr:0.000001
[ Tue Jul 16 10:20:32 2024 ] 	Batch(5700/6809) done. Loss: 0.1746  lr:0.000001
[ Tue Jul 16 10:20:55 2024 ] 	Batch(5800/6809) done. Loss: 0.0896  lr:0.000001
[ Tue Jul 16 10:21:18 2024 ] 	Batch(5900/6809) done. Loss: 0.2361  lr:0.000001
[ Tue Jul 16 10:21:40 2024 ] 
Training: Epoch [100/150], Step [5999], Loss: 0.24299316108226776, Training Accuracy: 95.7
[ Tue Jul 16 10:21:41 2024 ] 	Batch(6000/6809) done. Loss: 0.0443  lr:0.000001
[ Tue Jul 16 10:22:04 2024 ] 	Batch(6100/6809) done. Loss: 0.3963  lr:0.000001
[ Tue Jul 16 10:22:27 2024 ] 	Batch(6200/6809) done. Loss: 0.0739  lr:0.000001
[ Tue Jul 16 10:22:49 2024 ] 	Batch(6300/6809) done. Loss: 0.2520  lr:0.000001
[ Tue Jul 16 10:23:12 2024 ] 	Batch(6400/6809) done. Loss: 0.2505  lr:0.000001
[ Tue Jul 16 10:23:34 2024 ] 
Training: Epoch [100/150], Step [6499], Loss: 0.01569359377026558, Training Accuracy: 95.69615384615385
[ Tue Jul 16 10:23:35 2024 ] 	Batch(6500/6809) done. Loss: 0.1085  lr:0.000001
[ Tue Jul 16 10:23:58 2024 ] 	Batch(6600/6809) done. Loss: 0.0848  lr:0.000001
[ Tue Jul 16 10:24:21 2024 ] 	Batch(6700/6809) done. Loss: 0.0227  lr:0.000001
[ Tue Jul 16 10:24:45 2024 ] 	Batch(6800/6809) done. Loss: 0.3683  lr:0.000001
[ Tue Jul 16 10:24:47 2024 ] 	Mean training loss: 0.1543.
[ Tue Jul 16 10:24:47 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 10:24:47 2024 ] Training epoch: 102
[ Tue Jul 16 10:24:48 2024 ] 	Batch(0/6809) done. Loss: 0.0260  lr:0.000001
[ Tue Jul 16 10:25:11 2024 ] 	Batch(100/6809) done. Loss: 0.0010  lr:0.000001
[ Tue Jul 16 10:25:34 2024 ] 	Batch(200/6809) done. Loss: 0.0747  lr:0.000001
[ Tue Jul 16 10:25:57 2024 ] 	Batch(300/6809) done. Loss: 0.2060  lr:0.000001
[ Tue Jul 16 10:26:20 2024 ] 	Batch(400/6809) done. Loss: 0.0076  lr:0.000001
[ Tue Jul 16 10:26:42 2024 ] 
Training: Epoch [101/150], Step [499], Loss: 0.0484035387635231, Training Accuracy: 96.05
[ Tue Jul 16 10:26:43 2024 ] 	Batch(500/6809) done. Loss: 0.0985  lr:0.000001
[ Tue Jul 16 10:27:05 2024 ] 	Batch(600/6809) done. Loss: 0.0777  lr:0.000001
[ Tue Jul 16 10:27:28 2024 ] 	Batch(700/6809) done. Loss: 0.3114  lr:0.000001
[ Tue Jul 16 10:27:51 2024 ] 	Batch(800/6809) done. Loss: 0.1023  lr:0.000001
[ Tue Jul 16 10:28:14 2024 ] 	Batch(900/6809) done. Loss: 0.0553  lr:0.000001
[ Tue Jul 16 10:28:36 2024 ] 
Training: Epoch [101/150], Step [999], Loss: 0.021835990250110626, Training Accuracy: 95.9375
[ Tue Jul 16 10:28:37 2024 ] 	Batch(1000/6809) done. Loss: 0.0342  lr:0.000001
[ Tue Jul 16 10:29:00 2024 ] 	Batch(1100/6809) done. Loss: 0.0161  lr:0.000001
[ Tue Jul 16 10:29:23 2024 ] 	Batch(1200/6809) done. Loss: 0.0446  lr:0.000001
[ Tue Jul 16 10:29:47 2024 ] 	Batch(1300/6809) done. Loss: 0.0105  lr:0.000001
[ Tue Jul 16 10:30:09 2024 ] 	Batch(1400/6809) done. Loss: 0.0368  lr:0.000001
[ Tue Jul 16 10:30:32 2024 ] 
Training: Epoch [101/150], Step [1499], Loss: 0.2833268940448761, Training Accuracy: 95.89166666666667
[ Tue Jul 16 10:30:32 2024 ] 	Batch(1500/6809) done. Loss: 0.3815  lr:0.000001
[ Tue Jul 16 10:30:55 2024 ] 	Batch(1600/6809) done. Loss: 0.1227  lr:0.000001
[ Tue Jul 16 10:31:18 2024 ] 	Batch(1700/6809) done. Loss: 0.1522  lr:0.000001
[ Tue Jul 16 10:31:40 2024 ] 	Batch(1800/6809) done. Loss: 0.1539  lr:0.000001
[ Tue Jul 16 10:32:03 2024 ] 	Batch(1900/6809) done. Loss: 0.0438  lr:0.000001
[ Tue Jul 16 10:32:25 2024 ] 
Training: Epoch [101/150], Step [1999], Loss: 0.07481826841831207, Training Accuracy: 95.83125
[ Tue Jul 16 10:32:26 2024 ] 	Batch(2000/6809) done. Loss: 0.3150  lr:0.000001
[ Tue Jul 16 10:32:48 2024 ] 	Batch(2100/6809) done. Loss: 0.0446  lr:0.000001
[ Tue Jul 16 10:33:11 2024 ] 	Batch(2200/6809) done. Loss: 0.0252  lr:0.000001
[ Tue Jul 16 10:33:33 2024 ] 	Batch(2300/6809) done. Loss: 0.1939  lr:0.000001
[ Tue Jul 16 10:33:57 2024 ] 	Batch(2400/6809) done. Loss: 0.3627  lr:0.000001
[ Tue Jul 16 10:34:19 2024 ] 
Training: Epoch [101/150], Step [2499], Loss: 0.18025535345077515, Training Accuracy: 95.76
[ Tue Jul 16 10:34:20 2024 ] 	Batch(2500/6809) done. Loss: 0.2741  lr:0.000001
[ Tue Jul 16 10:34:43 2024 ] 	Batch(2600/6809) done. Loss: 0.2465  lr:0.000001
[ Tue Jul 16 10:35:05 2024 ] 	Batch(2700/6809) done. Loss: 0.1569  lr:0.000001
[ Tue Jul 16 10:35:28 2024 ] 	Batch(2800/6809) done. Loss: 0.0578  lr:0.000001
[ Tue Jul 16 10:35:51 2024 ] 	Batch(2900/6809) done. Loss: 0.3322  lr:0.000001
[ Tue Jul 16 10:36:15 2024 ] 
Training: Epoch [101/150], Step [2999], Loss: 0.06841940432786942, Training Accuracy: 95.76666666666667
[ Tue Jul 16 10:36:15 2024 ] 	Batch(3000/6809) done. Loss: 0.1422  lr:0.000001
[ Tue Jul 16 10:36:38 2024 ] 	Batch(3100/6809) done. Loss: 0.1376  lr:0.000001
[ Tue Jul 16 10:37:02 2024 ] 	Batch(3200/6809) done. Loss: 0.3252  lr:0.000001
[ Tue Jul 16 10:37:26 2024 ] 	Batch(3300/6809) done. Loss: 0.3143  lr:0.000001
[ Tue Jul 16 10:37:49 2024 ] 	Batch(3400/6809) done. Loss: 0.4919  lr:0.000001
[ Tue Jul 16 10:38:13 2024 ] 
Training: Epoch [101/150], Step [3499], Loss: 0.33824265003204346, Training Accuracy: 95.8
[ Tue Jul 16 10:38:13 2024 ] 	Batch(3500/6809) done. Loss: 0.0316  lr:0.000001
[ Tue Jul 16 10:38:37 2024 ] 	Batch(3600/6809) done. Loss: 0.0092  lr:0.000001
[ Tue Jul 16 10:39:00 2024 ] 	Batch(3700/6809) done. Loss: 0.0267  lr:0.000001
[ Tue Jul 16 10:39:23 2024 ] 	Batch(3800/6809) done. Loss: 0.1603  lr:0.000001
[ Tue Jul 16 10:39:46 2024 ] 	Batch(3900/6809) done. Loss: 0.0493  lr:0.000001
[ Tue Jul 16 10:40:09 2024 ] 
Training: Epoch [101/150], Step [3999], Loss: 0.1040201410651207, Training Accuracy: 95.809375
[ Tue Jul 16 10:40:09 2024 ] 	Batch(4000/6809) done. Loss: 0.0262  lr:0.000001
[ Tue Jul 16 10:40:32 2024 ] 	Batch(4100/6809) done. Loss: 0.0788  lr:0.000001
[ Tue Jul 16 10:40:55 2024 ] 	Batch(4200/6809) done. Loss: 0.0766  lr:0.000001
[ Tue Jul 16 10:41:19 2024 ] 	Batch(4300/6809) done. Loss: 0.0277  lr:0.000001
[ Tue Jul 16 10:41:42 2024 ] 	Batch(4400/6809) done. Loss: 0.2286  lr:0.000001
[ Tue Jul 16 10:42:05 2024 ] 
Training: Epoch [101/150], Step [4499], Loss: 0.4020827114582062, Training Accuracy: 95.77222222222223
[ Tue Jul 16 10:42:05 2024 ] 	Batch(4500/6809) done. Loss: 0.1059  lr:0.000001
[ Tue Jul 16 10:42:28 2024 ] 	Batch(4600/6809) done. Loss: 0.1143  lr:0.000001
[ Tue Jul 16 10:42:51 2024 ] 	Batch(4700/6809) done. Loss: 0.0473  lr:0.000001
[ Tue Jul 16 10:43:15 2024 ] 	Batch(4800/6809) done. Loss: 0.4261  lr:0.000001
[ Tue Jul 16 10:43:38 2024 ] 	Batch(4900/6809) done. Loss: 0.0281  lr:0.000001
[ Tue Jul 16 10:44:00 2024 ] 
Training: Epoch [101/150], Step [4999], Loss: 0.15815985202789307, Training Accuracy: 95.7
[ Tue Jul 16 10:44:01 2024 ] 	Batch(5000/6809) done. Loss: 0.0994  lr:0.000001
[ Tue Jul 16 10:44:23 2024 ] 	Batch(5100/6809) done. Loss: 0.1457  lr:0.000001
[ Tue Jul 16 10:44:46 2024 ] 	Batch(5200/6809) done. Loss: 0.0275  lr:0.000001
[ Tue Jul 16 10:45:09 2024 ] 	Batch(5300/6809) done. Loss: 0.3430  lr:0.000001
[ Tue Jul 16 10:45:32 2024 ] 	Batch(5400/6809) done. Loss: 0.0562  lr:0.000001
[ Tue Jul 16 10:45:54 2024 ] 
Training: Epoch [101/150], Step [5499], Loss: 0.3445717692375183, Training Accuracy: 95.70681818181819
[ Tue Jul 16 10:45:54 2024 ] 	Batch(5500/6809) done. Loss: 0.3794  lr:0.000001
[ Tue Jul 16 10:46:18 2024 ] 	Batch(5600/6809) done. Loss: 0.0099  lr:0.000001
[ Tue Jul 16 10:46:41 2024 ] 	Batch(5700/6809) done. Loss: 0.0463  lr:0.000001
[ Tue Jul 16 10:47:04 2024 ] 	Batch(5800/6809) done. Loss: 0.3376  lr:0.000001
[ Tue Jul 16 10:47:27 2024 ] 	Batch(5900/6809) done. Loss: 0.2969  lr:0.000001
[ Tue Jul 16 10:47:49 2024 ] 
Training: Epoch [101/150], Step [5999], Loss: 0.04024996981024742, Training Accuracy: 95.73541666666667
[ Tue Jul 16 10:47:50 2024 ] 	Batch(6000/6809) done. Loss: 0.3306  lr:0.000001
[ Tue Jul 16 10:48:13 2024 ] 	Batch(6100/6809) done. Loss: 0.1073  lr:0.000001
[ Tue Jul 16 10:48:36 2024 ] 	Batch(6200/6809) done. Loss: 0.0114  lr:0.000001
[ Tue Jul 16 10:48:59 2024 ] 	Batch(6300/6809) done. Loss: 0.1455  lr:0.000001
[ Tue Jul 16 10:49:22 2024 ] 	Batch(6400/6809) done. Loss: 0.0141  lr:0.000001
[ Tue Jul 16 10:49:46 2024 ] 
Training: Epoch [101/150], Step [6499], Loss: 0.018825078383088112, Training Accuracy: 95.69423076923077
[ Tue Jul 16 10:49:46 2024 ] 	Batch(6500/6809) done. Loss: 0.0577  lr:0.000001
[ Tue Jul 16 10:50:09 2024 ] 	Batch(6600/6809) done. Loss: 0.2480  lr:0.000001
[ Tue Jul 16 10:50:32 2024 ] 	Batch(6700/6809) done. Loss: 0.4835  lr:0.000001
[ Tue Jul 16 10:50:55 2024 ] 	Batch(6800/6809) done. Loss: 0.1590  lr:0.000001
[ Tue Jul 16 10:50:57 2024 ] 	Mean training loss: 0.1556.
[ Tue Jul 16 10:50:57 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 10:50:57 2024 ] Training epoch: 103
[ Tue Jul 16 10:50:58 2024 ] 	Batch(0/6809) done. Loss: 0.1041  lr:0.000001
[ Tue Jul 16 10:51:21 2024 ] 	Batch(100/6809) done. Loss: 0.0050  lr:0.000001
[ Tue Jul 16 10:51:44 2024 ] 	Batch(200/6809) done. Loss: 0.1536  lr:0.000001
[ Tue Jul 16 10:52:06 2024 ] 	Batch(300/6809) done. Loss: 0.1211  lr:0.000001
[ Tue Jul 16 10:52:29 2024 ] 	Batch(400/6809) done. Loss: 0.4775  lr:0.000001
[ Tue Jul 16 10:52:52 2024 ] 
Training: Epoch [102/150], Step [499], Loss: 0.9061090350151062, Training Accuracy: 95.5
[ Tue Jul 16 10:52:52 2024 ] 	Batch(500/6809) done. Loss: 0.0334  lr:0.000001
[ Tue Jul 16 10:53:15 2024 ] 	Batch(600/6809) done. Loss: 0.0944  lr:0.000001
[ Tue Jul 16 10:53:38 2024 ] 	Batch(700/6809) done. Loss: 0.0137  lr:0.000001
[ Tue Jul 16 10:54:01 2024 ] 	Batch(800/6809) done. Loss: 0.0079  lr:0.000001
[ Tue Jul 16 10:54:23 2024 ] 	Batch(900/6809) done. Loss: 0.1654  lr:0.000001
[ Tue Jul 16 10:54:46 2024 ] 
Training: Epoch [102/150], Step [999], Loss: 0.20962071418762207, Training Accuracy: 95.6375
[ Tue Jul 16 10:54:46 2024 ] 	Batch(1000/6809) done. Loss: 0.0548  lr:0.000001
[ Tue Jul 16 10:55:08 2024 ] 	Batch(1100/6809) done. Loss: 0.0122  lr:0.000001
[ Tue Jul 16 10:55:31 2024 ] 	Batch(1200/6809) done. Loss: 0.0299  lr:0.000001
[ Tue Jul 16 10:55:54 2024 ] 	Batch(1300/6809) done. Loss: 0.0649  lr:0.000001
[ Tue Jul 16 10:56:17 2024 ] 	Batch(1400/6809) done. Loss: 0.1346  lr:0.000001
[ Tue Jul 16 10:56:39 2024 ] 
Training: Epoch [102/150], Step [1499], Loss: 0.02895256131887436, Training Accuracy: 95.84166666666667
[ Tue Jul 16 10:56:39 2024 ] 	Batch(1500/6809) done. Loss: 0.0296  lr:0.000001
[ Tue Jul 16 10:57:02 2024 ] 	Batch(1600/6809) done. Loss: 0.0280  lr:0.000001
[ Tue Jul 16 10:57:25 2024 ] 	Batch(1700/6809) done. Loss: 0.0069  lr:0.000001
[ Tue Jul 16 10:57:48 2024 ] 	Batch(1800/6809) done. Loss: 0.2907  lr:0.000001
[ Tue Jul 16 10:58:11 2024 ] 	Batch(1900/6809) done. Loss: 0.1467  lr:0.000001
[ Tue Jul 16 10:58:34 2024 ] 
Training: Epoch [102/150], Step [1999], Loss: 0.08689674735069275, Training Accuracy: 95.8125
[ Tue Jul 16 10:58:35 2024 ] 	Batch(2000/6809) done. Loss: 0.1473  lr:0.000001
[ Tue Jul 16 10:58:58 2024 ] 	Batch(2100/6809) done. Loss: 0.0571  lr:0.000001
[ Tue Jul 16 10:59:21 2024 ] 	Batch(2200/6809) done. Loss: 0.0860  lr:0.000001
[ Tue Jul 16 10:59:44 2024 ] 	Batch(2300/6809) done. Loss: 0.1234  lr:0.000001
[ Tue Jul 16 11:00:08 2024 ] 	Batch(2400/6809) done. Loss: 0.0604  lr:0.000001
[ Tue Jul 16 11:00:30 2024 ] 
Training: Epoch [102/150], Step [2499], Loss: 0.013390706852078438, Training Accuracy: 95.83500000000001
[ Tue Jul 16 11:00:31 2024 ] 	Batch(2500/6809) done. Loss: 0.0719  lr:0.000001
[ Tue Jul 16 11:00:54 2024 ] 	Batch(2600/6809) done. Loss: 0.2136  lr:0.000001
[ Tue Jul 16 11:01:17 2024 ] 	Batch(2700/6809) done. Loss: 0.4812  lr:0.000001
[ Tue Jul 16 11:01:40 2024 ] 	Batch(2800/6809) done. Loss: 0.1230  lr:0.000001
[ Tue Jul 16 11:02:03 2024 ] 	Batch(2900/6809) done. Loss: 0.4817  lr:0.000001
[ Tue Jul 16 11:02:26 2024 ] 
Training: Epoch [102/150], Step [2999], Loss: 0.20846030116081238, Training Accuracy: 95.82916666666667
[ Tue Jul 16 11:02:26 2024 ] 	Batch(3000/6809) done. Loss: 0.2124  lr:0.000001
[ Tue Jul 16 11:02:50 2024 ] 	Batch(3100/6809) done. Loss: 0.2355  lr:0.000001
[ Tue Jul 16 11:03:13 2024 ] 	Batch(3200/6809) done. Loss: 0.0322  lr:0.000001
[ Tue Jul 16 11:03:36 2024 ] 	Batch(3300/6809) done. Loss: 0.0694  lr:0.000001
[ Tue Jul 16 11:03:59 2024 ] 	Batch(3400/6809) done. Loss: 0.1673  lr:0.000001
[ Tue Jul 16 11:04:22 2024 ] 
Training: Epoch [102/150], Step [3499], Loss: 0.005794713273644447, Training Accuracy: 95.79642857142858
[ Tue Jul 16 11:04:22 2024 ] 	Batch(3500/6809) done. Loss: 0.2736  lr:0.000001
[ Tue Jul 16 11:04:45 2024 ] 	Batch(3600/6809) done. Loss: 0.1219  lr:0.000001
[ Tue Jul 16 11:05:08 2024 ] 	Batch(3700/6809) done. Loss: 0.4596  lr:0.000001
[ Tue Jul 16 11:05:31 2024 ] 	Batch(3800/6809) done. Loss: 0.0145  lr:0.000001
[ Tue Jul 16 11:05:53 2024 ] 	Batch(3900/6809) done. Loss: 0.2434  lr:0.000001
[ Tue Jul 16 11:06:16 2024 ] 
Training: Epoch [102/150], Step [3999], Loss: 0.19865667819976807, Training Accuracy: 95.746875
[ Tue Jul 16 11:06:16 2024 ] 	Batch(4000/6809) done. Loss: 0.0510  lr:0.000001
[ Tue Jul 16 11:06:39 2024 ] 	Batch(4100/6809) done. Loss: 0.2129  lr:0.000001
[ Tue Jul 16 11:07:02 2024 ] 	Batch(4200/6809) done. Loss: 0.0167  lr:0.000001
[ Tue Jul 16 11:07:24 2024 ] 	Batch(4300/6809) done. Loss: 0.0496  lr:0.000001
[ Tue Jul 16 11:07:47 2024 ] 	Batch(4400/6809) done. Loss: 0.1689  lr:0.000001
[ Tue Jul 16 11:08:10 2024 ] 
Training: Epoch [102/150], Step [4499], Loss: 0.0490950308740139, Training Accuracy: 95.71666666666667
[ Tue Jul 16 11:08:10 2024 ] 	Batch(4500/6809) done. Loss: 0.3546  lr:0.000001
[ Tue Jul 16 11:08:33 2024 ] 	Batch(4600/6809) done. Loss: 0.2658  lr:0.000001
[ Tue Jul 16 11:08:56 2024 ] 	Batch(4700/6809) done. Loss: 0.2047  lr:0.000001
[ Tue Jul 16 11:09:18 2024 ] 	Batch(4800/6809) done. Loss: 0.0129  lr:0.000001
[ Tue Jul 16 11:09:41 2024 ] 	Batch(4900/6809) done. Loss: 0.0399  lr:0.000001
[ Tue Jul 16 11:10:03 2024 ] 
Training: Epoch [102/150], Step [4999], Loss: 0.22671756148338318, Training Accuracy: 95.665
[ Tue Jul 16 11:10:04 2024 ] 	Batch(5000/6809) done. Loss: 0.0303  lr:0.000001
[ Tue Jul 16 11:10:26 2024 ] 	Batch(5100/6809) done. Loss: 0.0422  lr:0.000001
[ Tue Jul 16 11:10:49 2024 ] 	Batch(5200/6809) done. Loss: 0.0815  lr:0.000001
[ Tue Jul 16 11:11:12 2024 ] 	Batch(5300/6809) done. Loss: 0.1354  lr:0.000001
[ Tue Jul 16 11:11:35 2024 ] 	Batch(5400/6809) done. Loss: 0.0300  lr:0.000001
[ Tue Jul 16 11:11:57 2024 ] 
Training: Epoch [102/150], Step [5499], Loss: 0.158982515335083, Training Accuracy: 95.69772727272728
[ Tue Jul 16 11:11:57 2024 ] 	Batch(5500/6809) done. Loss: 0.1887  lr:0.000001
[ Tue Jul 16 11:12:20 2024 ] 	Batch(5600/6809) done. Loss: 0.0450  lr:0.000001
[ Tue Jul 16 11:12:43 2024 ] 	Batch(5700/6809) done. Loss: 0.0755  lr:0.000001
[ Tue Jul 16 11:13:06 2024 ] 	Batch(5800/6809) done. Loss: 0.3045  lr:0.000001
[ Tue Jul 16 11:13:28 2024 ] 	Batch(5900/6809) done. Loss: 0.1239  lr:0.000001
[ Tue Jul 16 11:13:51 2024 ] 
Training: Epoch [102/150], Step [5999], Loss: 0.021868057548999786, Training Accuracy: 95.71041666666666
[ Tue Jul 16 11:13:51 2024 ] 	Batch(6000/6809) done. Loss: 0.1718  lr:0.000001
[ Tue Jul 16 11:14:14 2024 ] 	Batch(6100/6809) done. Loss: 0.4954  lr:0.000001
[ Tue Jul 16 11:14:37 2024 ] 	Batch(6200/6809) done. Loss: 0.1006  lr:0.000001
[ Tue Jul 16 11:15:00 2024 ] 	Batch(6300/6809) done. Loss: 0.0866  lr:0.000001
[ Tue Jul 16 11:15:22 2024 ] 	Batch(6400/6809) done. Loss: 0.0252  lr:0.000001
[ Tue Jul 16 11:15:45 2024 ] 
Training: Epoch [102/150], Step [6499], Loss: 0.026244428008794785, Training Accuracy: 95.70576923076923
[ Tue Jul 16 11:15:45 2024 ] 	Batch(6500/6809) done. Loss: 0.1315  lr:0.000001
[ Tue Jul 16 11:16:08 2024 ] 	Batch(6600/6809) done. Loss: 0.0620  lr:0.000001
[ Tue Jul 16 11:16:31 2024 ] 	Batch(6700/6809) done. Loss: 0.0637  lr:0.000001
[ Tue Jul 16 11:16:53 2024 ] 	Batch(6800/6809) done. Loss: 0.3641  lr:0.000001
[ Tue Jul 16 11:16:55 2024 ] 	Mean training loss: 0.1532.
[ Tue Jul 16 11:16:55 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 11:16:56 2024 ] Training epoch: 104
[ Tue Jul 16 11:16:56 2024 ] 	Batch(0/6809) done. Loss: 0.4060  lr:0.000001
[ Tue Jul 16 11:17:19 2024 ] 	Batch(100/6809) done. Loss: 0.0184  lr:0.000001
[ Tue Jul 16 11:17:42 2024 ] 	Batch(200/6809) done. Loss: 0.0203  lr:0.000001
[ Tue Jul 16 11:18:04 2024 ] 	Batch(300/6809) done. Loss: 0.0769  lr:0.000001
[ Tue Jul 16 11:18:27 2024 ] 	Batch(400/6809) done. Loss: 0.0447  lr:0.000001
[ Tue Jul 16 11:18:50 2024 ] 
Training: Epoch [103/150], Step [499], Loss: 0.23441503942012787, Training Accuracy: 95.875
[ Tue Jul 16 11:18:50 2024 ] 	Batch(500/6809) done. Loss: 0.2524  lr:0.000001
[ Tue Jul 16 11:19:13 2024 ] 	Batch(600/6809) done. Loss: 0.0901  lr:0.000001
[ Tue Jul 16 11:19:35 2024 ] 	Batch(700/6809) done. Loss: 0.1235  lr:0.000001
[ Tue Jul 16 11:19:58 2024 ] 	Batch(800/6809) done. Loss: 0.6249  lr:0.000001
[ Tue Jul 16 11:20:21 2024 ] 	Batch(900/6809) done. Loss: 0.0580  lr:0.000001
[ Tue Jul 16 11:20:43 2024 ] 
Training: Epoch [103/150], Step [999], Loss: 0.07927681505680084, Training Accuracy: 95.5625
[ Tue Jul 16 11:20:44 2024 ] 	Batch(1000/6809) done. Loss: 0.0313  lr:0.000001
[ Tue Jul 16 11:21:06 2024 ] 	Batch(1100/6809) done. Loss: 0.2866  lr:0.000001
[ Tue Jul 16 11:21:29 2024 ] 	Batch(1200/6809) done. Loss: 0.0424  lr:0.000001
[ Tue Jul 16 11:21:52 2024 ] 	Batch(1300/6809) done. Loss: 0.1926  lr:0.000001
[ Tue Jul 16 11:22:15 2024 ] 	Batch(1400/6809) done. Loss: 0.1944  lr:0.000001
[ Tue Jul 16 11:22:37 2024 ] 
Training: Epoch [103/150], Step [1499], Loss: 0.04781131446361542, Training Accuracy: 95.825
[ Tue Jul 16 11:22:37 2024 ] 	Batch(1500/6809) done. Loss: 0.0689  lr:0.000001
[ Tue Jul 16 11:23:00 2024 ] 	Batch(1600/6809) done. Loss: 0.0547  lr:0.000001
[ Tue Jul 16 11:23:23 2024 ] 	Batch(1700/6809) done. Loss: 0.1978  lr:0.000001
[ Tue Jul 16 11:23:45 2024 ] 	Batch(1800/6809) done. Loss: 0.1788  lr:0.000001
[ Tue Jul 16 11:24:08 2024 ] 	Batch(1900/6809) done. Loss: 0.0500  lr:0.000001
[ Tue Jul 16 11:24:31 2024 ] 
Training: Epoch [103/150], Step [1999], Loss: 0.004273161757737398, Training Accuracy: 95.825
[ Tue Jul 16 11:24:31 2024 ] 	Batch(2000/6809) done. Loss: 0.2548  lr:0.000001
[ Tue Jul 16 11:24:54 2024 ] 	Batch(2100/6809) done. Loss: 0.0864  lr:0.000001
[ Tue Jul 16 11:25:17 2024 ] 	Batch(2200/6809) done. Loss: 0.0587  lr:0.000001
[ Tue Jul 16 11:25:40 2024 ] 	Batch(2300/6809) done. Loss: 0.1992  lr:0.000001
[ Tue Jul 16 11:26:03 2024 ] 	Batch(2400/6809) done. Loss: 0.1434  lr:0.000001
[ Tue Jul 16 11:26:26 2024 ] 
Training: Epoch [103/150], Step [2499], Loss: 0.021438660100102425, Training Accuracy: 95.815
[ Tue Jul 16 11:26:26 2024 ] 	Batch(2500/6809) done. Loss: 0.0352  lr:0.000001
[ Tue Jul 16 11:26:50 2024 ] 	Batch(2600/6809) done. Loss: 0.1620  lr:0.000001
[ Tue Jul 16 11:27:13 2024 ] 	Batch(2700/6809) done. Loss: 0.0766  lr:0.000001
[ Tue Jul 16 11:27:36 2024 ] 	Batch(2800/6809) done. Loss: 0.0403  lr:0.000001
[ Tue Jul 16 11:27:58 2024 ] 	Batch(2900/6809) done. Loss: 0.7217  lr:0.000001
[ Tue Jul 16 11:28:21 2024 ] 
Training: Epoch [103/150], Step [2999], Loss: 0.15294311940670013, Training Accuracy: 95.79583333333333
[ Tue Jul 16 11:28:21 2024 ] 	Batch(3000/6809) done. Loss: 0.1769  lr:0.000001
[ Tue Jul 16 11:28:44 2024 ] 	Batch(3100/6809) done. Loss: 0.0680  lr:0.000001
[ Tue Jul 16 11:29:07 2024 ] 	Batch(3200/6809) done. Loss: 0.1513  lr:0.000001
[ Tue Jul 16 11:29:29 2024 ] 	Batch(3300/6809) done. Loss: 0.3181  lr:0.000001
[ Tue Jul 16 11:29:52 2024 ] 	Batch(3400/6809) done. Loss: 0.0153  lr:0.000001
[ Tue Jul 16 11:30:14 2024 ] 
Training: Epoch [103/150], Step [3499], Loss: 0.09194822609424591, Training Accuracy: 95.78571428571429
[ Tue Jul 16 11:30:15 2024 ] 	Batch(3500/6809) done. Loss: 0.1943  lr:0.000001
[ Tue Jul 16 11:30:38 2024 ] 	Batch(3600/6809) done. Loss: 0.2717  lr:0.000001
[ Tue Jul 16 11:31:00 2024 ] 	Batch(3700/6809) done. Loss: 0.1440  lr:0.000001
[ Tue Jul 16 11:31:23 2024 ] 	Batch(3800/6809) done. Loss: 0.0127  lr:0.000001
[ Tue Jul 16 11:31:46 2024 ] 	Batch(3900/6809) done. Loss: 0.0801  lr:0.000001
[ Tue Jul 16 11:32:08 2024 ] 
Training: Epoch [103/150], Step [3999], Loss: 0.09851797670125961, Training Accuracy: 95.75625
[ Tue Jul 16 11:32:09 2024 ] 	Batch(4000/6809) done. Loss: 0.0632  lr:0.000001
[ Tue Jul 16 11:32:31 2024 ] 	Batch(4100/6809) done. Loss: 0.1290  lr:0.000001
[ Tue Jul 16 11:32:54 2024 ] 	Batch(4200/6809) done. Loss: 0.0324  lr:0.000001
[ Tue Jul 16 11:33:16 2024 ] 	Batch(4300/6809) done. Loss: 0.0020  lr:0.000001
[ Tue Jul 16 11:33:39 2024 ] 	Batch(4400/6809) done. Loss: 0.0078  lr:0.000001
[ Tue Jul 16 11:34:01 2024 ] 
Training: Epoch [103/150], Step [4499], Loss: 0.009408056735992432, Training Accuracy: 95.76666666666667
[ Tue Jul 16 11:34:02 2024 ] 	Batch(4500/6809) done. Loss: 0.5285  lr:0.000001
[ Tue Jul 16 11:34:24 2024 ] 	Batch(4600/6809) done. Loss: 0.2774  lr:0.000001
[ Tue Jul 16 11:34:47 2024 ] 	Batch(4700/6809) done. Loss: 0.0347  lr:0.000001
[ Tue Jul 16 11:35:09 2024 ] 	Batch(4800/6809) done. Loss: 0.0565  lr:0.000001
[ Tue Jul 16 11:35:32 2024 ] 	Batch(4900/6809) done. Loss: 0.0127  lr:0.000001
[ Tue Jul 16 11:35:54 2024 ] 
Training: Epoch [103/150], Step [4999], Loss: 0.10541922599077225, Training Accuracy: 95.765
[ Tue Jul 16 11:35:55 2024 ] 	Batch(5000/6809) done. Loss: 0.0087  lr:0.000001
[ Tue Jul 16 11:36:17 2024 ] 	Batch(5100/6809) done. Loss: 0.1151  lr:0.000001
[ Tue Jul 16 11:36:40 2024 ] 	Batch(5200/6809) done. Loss: 0.3638  lr:0.000001
[ Tue Jul 16 11:37:04 2024 ] 	Batch(5300/6809) done. Loss: 0.0659  lr:0.000001
[ Tue Jul 16 11:37:27 2024 ] 	Batch(5400/6809) done. Loss: 0.1508  lr:0.000001
[ Tue Jul 16 11:37:50 2024 ] 
Training: Epoch [103/150], Step [5499], Loss: 0.36226388812065125, Training Accuracy: 95.70227272727273
[ Tue Jul 16 11:37:50 2024 ] 	Batch(5500/6809) done. Loss: 0.0224  lr:0.000001
[ Tue Jul 16 11:38:13 2024 ] 	Batch(5600/6809) done. Loss: 0.3174  lr:0.000001
[ Tue Jul 16 11:38:37 2024 ] 	Batch(5700/6809) done. Loss: 0.3930  lr:0.000001
[ Tue Jul 16 11:39:00 2024 ] 	Batch(5800/6809) done. Loss: 0.0132  lr:0.000001
[ Tue Jul 16 11:39:23 2024 ] 	Batch(5900/6809) done. Loss: 0.3222  lr:0.000001
[ Tue Jul 16 11:39:46 2024 ] 
Training: Epoch [103/150], Step [5999], Loss: 0.11409162729978561, Training Accuracy: 95.72916666666667
[ Tue Jul 16 11:39:46 2024 ] 	Batch(6000/6809) done. Loss: 0.0812  lr:0.000001
[ Tue Jul 16 11:40:09 2024 ] 	Batch(6100/6809) done. Loss: 0.0168  lr:0.000001
[ Tue Jul 16 11:40:32 2024 ] 	Batch(6200/6809) done. Loss: 0.0969  lr:0.000001
[ Tue Jul 16 11:40:56 2024 ] 	Batch(6300/6809) done. Loss: 0.0917  lr:0.000001
[ Tue Jul 16 11:41:19 2024 ] 	Batch(6400/6809) done. Loss: 0.0388  lr:0.000001
[ Tue Jul 16 11:41:42 2024 ] 
Training: Epoch [103/150], Step [6499], Loss: 0.06764346361160278, Training Accuracy: 95.75192307692308
[ Tue Jul 16 11:41:42 2024 ] 	Batch(6500/6809) done. Loss: 0.0530  lr:0.000001
[ Tue Jul 16 11:42:05 2024 ] 	Batch(6600/6809) done. Loss: 0.1502  lr:0.000001
[ Tue Jul 16 11:42:28 2024 ] 	Batch(6700/6809) done. Loss: 0.1204  lr:0.000001
[ Tue Jul 16 11:42:52 2024 ] 	Batch(6800/6809) done. Loss: 0.0707  lr:0.000001
[ Tue Jul 16 11:42:54 2024 ] 	Mean training loss: 0.1514.
[ Tue Jul 16 11:42:54 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 11:42:54 2024 ] Training epoch: 105
[ Tue Jul 16 11:42:55 2024 ] 	Batch(0/6809) done. Loss: 0.0768  lr:0.000001
[ Tue Jul 16 11:43:18 2024 ] 	Batch(100/6809) done. Loss: 0.0333  lr:0.000001
[ Tue Jul 16 11:43:41 2024 ] 	Batch(200/6809) done. Loss: 0.0518  lr:0.000001
[ Tue Jul 16 11:44:04 2024 ] 	Batch(300/6809) done. Loss: 0.0495  lr:0.000001
[ Tue Jul 16 11:44:28 2024 ] 	Batch(400/6809) done. Loss: 0.1884  lr:0.000001
[ Tue Jul 16 11:44:50 2024 ] 
Training: Epoch [104/150], Step [499], Loss: 0.4445400536060333, Training Accuracy: 95.175
[ Tue Jul 16 11:44:51 2024 ] 	Batch(500/6809) done. Loss: 0.0190  lr:0.000001
[ Tue Jul 16 11:45:14 2024 ] 	Batch(600/6809) done. Loss: 0.1183  lr:0.000001
[ Tue Jul 16 11:45:37 2024 ] 	Batch(700/6809) done. Loss: 0.0834  lr:0.000001
[ Tue Jul 16 11:46:00 2024 ] 	Batch(800/6809) done. Loss: 0.2486  lr:0.000001
[ Tue Jul 16 11:46:23 2024 ] 	Batch(900/6809) done. Loss: 0.1779  lr:0.000001
[ Tue Jul 16 11:46:46 2024 ] 
Training: Epoch [104/150], Step [999], Loss: 0.18754711747169495, Training Accuracy: 95.72500000000001
[ Tue Jul 16 11:46:46 2024 ] 	Batch(1000/6809) done. Loss: 0.0343  lr:0.000001
[ Tue Jul 16 11:47:09 2024 ] 	Batch(1100/6809) done. Loss: 0.0519  lr:0.000001
[ Tue Jul 16 11:47:32 2024 ] 	Batch(1200/6809) done. Loss: 0.2648  lr:0.000001
[ Tue Jul 16 11:47:55 2024 ] 	Batch(1300/6809) done. Loss: 0.0244  lr:0.000001
[ Tue Jul 16 11:48:18 2024 ] 	Batch(1400/6809) done. Loss: 0.0096  lr:0.000001
[ Tue Jul 16 11:48:41 2024 ] 
Training: Epoch [104/150], Step [1499], Loss: 0.14833368360996246, Training Accuracy: 95.79166666666666
[ Tue Jul 16 11:48:42 2024 ] 	Batch(1500/6809) done. Loss: 0.0236  lr:0.000001
[ Tue Jul 16 11:49:05 2024 ] 	Batch(1600/6809) done. Loss: 0.2816  lr:0.000001
[ Tue Jul 16 11:49:28 2024 ] 	Batch(1700/6809) done. Loss: 0.0765  lr:0.000001
[ Tue Jul 16 11:49:51 2024 ] 	Batch(1800/6809) done. Loss: 0.0966  lr:0.000001
[ Tue Jul 16 11:50:14 2024 ] 	Batch(1900/6809) done. Loss: 0.2764  lr:0.000001
[ Tue Jul 16 11:50:37 2024 ] 
Training: Epoch [104/150], Step [1999], Loss: 0.07082472741603851, Training Accuracy: 95.80625
[ Tue Jul 16 11:50:38 2024 ] 	Batch(2000/6809) done. Loss: 0.0100  lr:0.000001
[ Tue Jul 16 11:51:00 2024 ] 	Batch(2100/6809) done. Loss: 0.4586  lr:0.000001
[ Tue Jul 16 11:51:23 2024 ] 	Batch(2200/6809) done. Loss: 0.3142  lr:0.000001
[ Tue Jul 16 11:51:46 2024 ] 	Batch(2300/6809) done. Loss: 0.0908  lr:0.000001
[ Tue Jul 16 11:52:09 2024 ] 	Batch(2400/6809) done. Loss: 0.0719  lr:0.000001
[ Tue Jul 16 11:52:32 2024 ] 
Training: Epoch [104/150], Step [2499], Loss: 0.016945017501711845, Training Accuracy: 95.77
[ Tue Jul 16 11:52:32 2024 ] 	Batch(2500/6809) done. Loss: 0.0154  lr:0.000001
[ Tue Jul 16 11:52:55 2024 ] 	Batch(2600/6809) done. Loss: 0.2190  lr:0.000001
[ Tue Jul 16 11:53:18 2024 ] 	Batch(2700/6809) done. Loss: 0.0669  lr:0.000001
[ Tue Jul 16 11:53:41 2024 ] 	Batch(2800/6809) done. Loss: 0.0117  lr:0.000001
[ Tue Jul 16 11:54:04 2024 ] 	Batch(2900/6809) done. Loss: 0.0891  lr:0.000001
[ Tue Jul 16 11:54:27 2024 ] 
Training: Epoch [104/150], Step [2999], Loss: 0.013702687807381153, Training Accuracy: 95.79583333333333
[ Tue Jul 16 11:54:27 2024 ] 	Batch(3000/6809) done. Loss: 0.0090  lr:0.000001
[ Tue Jul 16 11:54:50 2024 ] 	Batch(3100/6809) done. Loss: 0.0832  lr:0.000001
[ Tue Jul 16 11:55:14 2024 ] 	Batch(3200/6809) done. Loss: 0.1320  lr:0.000001
[ Tue Jul 16 11:55:37 2024 ] 	Batch(3300/6809) done. Loss: 0.1016  lr:0.000001
[ Tue Jul 16 11:56:00 2024 ] 	Batch(3400/6809) done. Loss: 0.6639  lr:0.000001
[ Tue Jul 16 11:56:23 2024 ] 
Training: Epoch [104/150], Step [3499], Loss: 0.04305816814303398, Training Accuracy: 95.75357142857143
[ Tue Jul 16 11:56:23 2024 ] 	Batch(3500/6809) done. Loss: 0.0610  lr:0.000001
[ Tue Jul 16 11:56:46 2024 ] 	Batch(3600/6809) done. Loss: 0.1584  lr:0.000001
[ Tue Jul 16 11:57:09 2024 ] 	Batch(3700/6809) done. Loss: 0.0229  lr:0.000001
[ Tue Jul 16 11:57:31 2024 ] 	Batch(3800/6809) done. Loss: 0.1204  lr:0.000001
[ Tue Jul 16 11:57:54 2024 ] 	Batch(3900/6809) done. Loss: 0.7059  lr:0.000001
[ Tue Jul 16 11:58:17 2024 ] 
Training: Epoch [104/150], Step [3999], Loss: 0.052521802484989166, Training Accuracy: 95.771875
[ Tue Jul 16 11:58:17 2024 ] 	Batch(4000/6809) done. Loss: 0.0178  lr:0.000001
[ Tue Jul 16 11:58:40 2024 ] 	Batch(4100/6809) done. Loss: 0.0647  lr:0.000001
[ Tue Jul 16 11:59:02 2024 ] 	Batch(4200/6809) done. Loss: 0.0268  lr:0.000001
[ Tue Jul 16 11:59:25 2024 ] 	Batch(4300/6809) done. Loss: 0.0205  lr:0.000001
[ Tue Jul 16 11:59:49 2024 ] 	Batch(4400/6809) done. Loss: 0.0470  lr:0.000001
[ Tue Jul 16 12:00:11 2024 ] 
Training: Epoch [104/150], Step [4499], Loss: 0.21748869121074677, Training Accuracy: 95.75277777777778
[ Tue Jul 16 12:00:12 2024 ] 	Batch(4500/6809) done. Loss: 0.0212  lr:0.000001
[ Tue Jul 16 12:00:35 2024 ] 	Batch(4600/6809) done. Loss: 0.1558  lr:0.000001
[ Tue Jul 16 12:00:58 2024 ] 	Batch(4700/6809) done. Loss: 0.0753  lr:0.000001
[ Tue Jul 16 12:01:21 2024 ] 	Batch(4800/6809) done. Loss: 0.0652  lr:0.000001
[ Tue Jul 16 12:01:44 2024 ] 	Batch(4900/6809) done. Loss: 0.0387  lr:0.000001
[ Tue Jul 16 12:02:07 2024 ] 
Training: Epoch [104/150], Step [4999], Loss: 0.030519399791955948, Training Accuracy: 95.7375
[ Tue Jul 16 12:02:07 2024 ] 	Batch(5000/6809) done. Loss: 0.5425  lr:0.000001
[ Tue Jul 16 12:02:30 2024 ] 	Batch(5100/6809) done. Loss: 0.0352  lr:0.000001
[ Tue Jul 16 12:02:54 2024 ] 	Batch(5200/6809) done. Loss: 0.0274  lr:0.000001
[ Tue Jul 16 12:03:17 2024 ] 	Batch(5300/6809) done. Loss: 0.0020  lr:0.000001
[ Tue Jul 16 12:03:40 2024 ] 	Batch(5400/6809) done. Loss: 0.0385  lr:0.000001
[ Tue Jul 16 12:04:03 2024 ] 
Training: Epoch [104/150], Step [5499], Loss: 0.12303704023361206, Training Accuracy: 95.76818181818182
[ Tue Jul 16 12:04:03 2024 ] 	Batch(5500/6809) done. Loss: 0.2091  lr:0.000001
[ Tue Jul 16 12:04:26 2024 ] 	Batch(5600/6809) done. Loss: 0.0276  lr:0.000001
[ Tue Jul 16 12:04:49 2024 ] 	Batch(5700/6809) done. Loss: 0.2489  lr:0.000001
[ Tue Jul 16 12:05:12 2024 ] 	Batch(5800/6809) done. Loss: 0.2189  lr:0.000001
[ Tue Jul 16 12:05:36 2024 ] 	Batch(5900/6809) done. Loss: 0.0121  lr:0.000001
[ Tue Jul 16 12:05:58 2024 ] 
Training: Epoch [104/150], Step [5999], Loss: 0.23579120635986328, Training Accuracy: 95.7125
[ Tue Jul 16 12:05:59 2024 ] 	Batch(6000/6809) done. Loss: 0.0054  lr:0.000001
[ Tue Jul 16 12:06:22 2024 ] 	Batch(6100/6809) done. Loss: 0.3131  lr:0.000001
[ Tue Jul 16 12:06:45 2024 ] 	Batch(6200/6809) done. Loss: 0.2166  lr:0.000001
[ Tue Jul 16 12:07:08 2024 ] 	Batch(6300/6809) done. Loss: 0.1088  lr:0.000001
[ Tue Jul 16 12:07:31 2024 ] 	Batch(6400/6809) done. Loss: 0.1049  lr:0.000001
[ Tue Jul 16 12:07:54 2024 ] 
Training: Epoch [104/150], Step [6499], Loss: 0.10254402458667755, Training Accuracy: 95.70576923076923
[ Tue Jul 16 12:07:55 2024 ] 	Batch(6500/6809) done. Loss: 0.1746  lr:0.000001
[ Tue Jul 16 12:08:18 2024 ] 	Batch(6600/6809) done. Loss: 0.1496  lr:0.000001
[ Tue Jul 16 12:08:41 2024 ] 	Batch(6700/6809) done. Loss: 0.0920  lr:0.000001
[ Tue Jul 16 12:09:04 2024 ] 	Batch(6800/6809) done. Loss: 0.0911  lr:0.000001
[ Tue Jul 16 12:09:06 2024 ] 	Mean training loss: 0.1545.
[ Tue Jul 16 12:09:06 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 12:09:06 2024 ] Training epoch: 106
[ Tue Jul 16 12:09:07 2024 ] 	Batch(0/6809) done. Loss: 0.0144  lr:0.000001
[ Tue Jul 16 12:09:30 2024 ] 	Batch(100/6809) done. Loss: 0.0902  lr:0.000001
[ Tue Jul 16 12:09:53 2024 ] 	Batch(200/6809) done. Loss: 0.0936  lr:0.000001
[ Tue Jul 16 12:10:16 2024 ] 	Batch(300/6809) done. Loss: 0.0623  lr:0.000001
[ Tue Jul 16 12:10:39 2024 ] 	Batch(400/6809) done. Loss: 0.3234  lr:0.000001
[ Tue Jul 16 12:11:02 2024 ] 
Training: Epoch [105/150], Step [499], Loss: 0.16148865222930908, Training Accuracy: 95.35
[ Tue Jul 16 12:11:02 2024 ] 	Batch(500/6809) done. Loss: 0.1088  lr:0.000001
[ Tue Jul 16 12:11:25 2024 ] 	Batch(600/6809) done. Loss: 0.0450  lr:0.000001
[ Tue Jul 16 12:11:48 2024 ] 	Batch(700/6809) done. Loss: 0.1351  lr:0.000001
[ Tue Jul 16 12:12:12 2024 ] 	Batch(800/6809) done. Loss: 0.1806  lr:0.000001
[ Tue Jul 16 12:12:35 2024 ] 	Batch(900/6809) done. Loss: 0.1245  lr:0.000001
[ Tue Jul 16 12:12:58 2024 ] 
Training: Epoch [105/150], Step [999], Loss: 0.058505021035671234, Training Accuracy: 95.5875
[ Tue Jul 16 12:12:58 2024 ] 	Batch(1000/6809) done. Loss: 0.3054  lr:0.000001
[ Tue Jul 16 12:13:21 2024 ] 	Batch(1100/6809) done. Loss: 0.0989  lr:0.000001
[ Tue Jul 16 12:13:44 2024 ] 	Batch(1200/6809) done. Loss: 0.1691  lr:0.000001
[ Tue Jul 16 12:14:07 2024 ] 	Batch(1300/6809) done. Loss: 0.3109  lr:0.000001
[ Tue Jul 16 12:14:30 2024 ] 	Batch(1400/6809) done. Loss: 0.0469  lr:0.000001
[ Tue Jul 16 12:14:52 2024 ] 
Training: Epoch [105/150], Step [1499], Loss: 0.029281653463840485, Training Accuracy: 95.74166666666667
[ Tue Jul 16 12:14:52 2024 ] 	Batch(1500/6809) done. Loss: 0.4158  lr:0.000001
[ Tue Jul 16 12:15:15 2024 ] 	Batch(1600/6809) done. Loss: 0.1292  lr:0.000001
[ Tue Jul 16 12:15:38 2024 ] 	Batch(1700/6809) done. Loss: 0.1100  lr:0.000001
[ Tue Jul 16 12:16:01 2024 ] 	Batch(1800/6809) done. Loss: 0.3568  lr:0.000001
[ Tue Jul 16 12:16:23 2024 ] 	Batch(1900/6809) done. Loss: 0.0340  lr:0.000001
[ Tue Jul 16 12:16:46 2024 ] 
Training: Epoch [105/150], Step [1999], Loss: 0.006803432013839483, Training Accuracy: 95.66875
[ Tue Jul 16 12:16:46 2024 ] 	Batch(2000/6809) done. Loss: 0.1448  lr:0.000001
[ Tue Jul 16 12:17:09 2024 ] 	Batch(2100/6809) done. Loss: 0.2371  lr:0.000001
[ Tue Jul 16 12:17:31 2024 ] 	Batch(2200/6809) done. Loss: 0.2509  lr:0.000001
[ Tue Jul 16 12:17:54 2024 ] 	Batch(2300/6809) done. Loss: 0.0363  lr:0.000001
[ Tue Jul 16 12:18:17 2024 ] 	Batch(2400/6809) done. Loss: 0.0102  lr:0.000001
[ Tue Jul 16 12:18:39 2024 ] 
Training: Epoch [105/150], Step [2499], Loss: 0.7917840480804443, Training Accuracy: 95.585
[ Tue Jul 16 12:18:40 2024 ] 	Batch(2500/6809) done. Loss: 0.1790  lr:0.000001
[ Tue Jul 16 12:19:02 2024 ] 	Batch(2600/6809) done. Loss: 0.2069  lr:0.000001
[ Tue Jul 16 12:19:25 2024 ] 	Batch(2700/6809) done. Loss: 0.1282  lr:0.000001
[ Tue Jul 16 12:19:48 2024 ] 	Batch(2800/6809) done. Loss: 0.3884  lr:0.000001
[ Tue Jul 16 12:20:10 2024 ] 	Batch(2900/6809) done. Loss: 0.0246  lr:0.000001
[ Tue Jul 16 12:20:33 2024 ] 
Training: Epoch [105/150], Step [2999], Loss: 0.1655033677816391, Training Accuracy: 95.625
[ Tue Jul 16 12:20:33 2024 ] 	Batch(3000/6809) done. Loss: 0.1306  lr:0.000001
[ Tue Jul 16 12:20:56 2024 ] 	Batch(3100/6809) done. Loss: 0.0190  lr:0.000001
[ Tue Jul 16 12:21:19 2024 ] 	Batch(3200/6809) done. Loss: 0.4078  lr:0.000001
[ Tue Jul 16 12:21:41 2024 ] 	Batch(3300/6809) done. Loss: 0.8792  lr:0.000001
[ Tue Jul 16 12:22:04 2024 ] 	Batch(3400/6809) done. Loss: 0.0384  lr:0.000001
[ Tue Jul 16 12:22:26 2024 ] 
Training: Epoch [105/150], Step [3499], Loss: 0.20671182870864868, Training Accuracy: 95.62857142857143
[ Tue Jul 16 12:22:26 2024 ] 	Batch(3500/6809) done. Loss: 0.0545  lr:0.000001
[ Tue Jul 16 12:22:49 2024 ] 	Batch(3600/6809) done. Loss: 0.1876  lr:0.000001
[ Tue Jul 16 12:23:12 2024 ] 	Batch(3700/6809) done. Loss: 0.0921  lr:0.000001
[ Tue Jul 16 12:23:35 2024 ] 	Batch(3800/6809) done. Loss: 0.1131  lr:0.000001
[ Tue Jul 16 12:23:58 2024 ] 	Batch(3900/6809) done. Loss: 0.0115  lr:0.000001
[ Tue Jul 16 12:24:21 2024 ] 
Training: Epoch [105/150], Step [3999], Loss: 0.4715885519981384, Training Accuracy: 95.609375
[ Tue Jul 16 12:24:21 2024 ] 	Batch(4000/6809) done. Loss: 0.0050  lr:0.000001
[ Tue Jul 16 12:24:44 2024 ] 	Batch(4100/6809) done. Loss: 0.3134  lr:0.000001
[ Tue Jul 16 12:25:07 2024 ] 	Batch(4200/6809) done. Loss: 0.2449  lr:0.000001
[ Tue Jul 16 12:25:29 2024 ] 	Batch(4300/6809) done. Loss: 0.2861  lr:0.000001
[ Tue Jul 16 12:25:52 2024 ] 	Batch(4400/6809) done. Loss: 0.2981  lr:0.000001
[ Tue Jul 16 12:26:15 2024 ] 
Training: Epoch [105/150], Step [4499], Loss: 0.7035509347915649, Training Accuracy: 95.63333333333334
[ Tue Jul 16 12:26:15 2024 ] 	Batch(4500/6809) done. Loss: 0.0094  lr:0.000001
[ Tue Jul 16 12:26:38 2024 ] 	Batch(4600/6809) done. Loss: 0.0777  lr:0.000001
[ Tue Jul 16 12:27:00 2024 ] 	Batch(4700/6809) done. Loss: 0.0561  lr:0.000001
[ Tue Jul 16 12:27:23 2024 ] 	Batch(4800/6809) done. Loss: 0.4778  lr:0.000001
[ Tue Jul 16 12:27:46 2024 ] 	Batch(4900/6809) done. Loss: 0.0948  lr:0.000001
[ Tue Jul 16 12:28:08 2024 ] 
Training: Epoch [105/150], Step [4999], Loss: 0.08608666807413101, Training Accuracy: 95.6175
[ Tue Jul 16 12:28:09 2024 ] 	Batch(5000/6809) done. Loss: 0.1028  lr:0.000001
[ Tue Jul 16 12:28:31 2024 ] 	Batch(5100/6809) done. Loss: 0.0092  lr:0.000001
[ Tue Jul 16 12:28:54 2024 ] 	Batch(5200/6809) done. Loss: 0.0274  lr:0.000001
[ Tue Jul 16 12:29:17 2024 ] 	Batch(5300/6809) done. Loss: 0.2150  lr:0.000001
[ Tue Jul 16 12:29:40 2024 ] 	Batch(5400/6809) done. Loss: 0.2916  lr:0.000001
[ Tue Jul 16 12:30:02 2024 ] 
Training: Epoch [105/150], Step [5499], Loss: 0.23431764543056488, Training Accuracy: 95.56363636363636
[ Tue Jul 16 12:30:02 2024 ] 	Batch(5500/6809) done. Loss: 0.0115  lr:0.000001
[ Tue Jul 16 12:30:25 2024 ] 	Batch(5600/6809) done. Loss: 0.0437  lr:0.000001
[ Tue Jul 16 12:30:48 2024 ] 	Batch(5700/6809) done. Loss: 0.6430  lr:0.000001
[ Tue Jul 16 12:31:11 2024 ] 	Batch(5800/6809) done. Loss: 0.1122  lr:0.000001
[ Tue Jul 16 12:31:33 2024 ] 	Batch(5900/6809) done. Loss: 0.0054  lr:0.000001
[ Tue Jul 16 12:31:56 2024 ] 
Training: Epoch [105/150], Step [5999], Loss: 0.1493195742368698, Training Accuracy: 95.55624999999999
[ Tue Jul 16 12:31:56 2024 ] 	Batch(6000/6809) done. Loss: 0.0076  lr:0.000001
[ Tue Jul 16 12:32:19 2024 ] 	Batch(6100/6809) done. Loss: 0.0263  lr:0.000001
[ Tue Jul 16 12:32:41 2024 ] 	Batch(6200/6809) done. Loss: 0.1501  lr:0.000001
[ Tue Jul 16 12:33:04 2024 ] 	Batch(6300/6809) done. Loss: 0.0051  lr:0.000001
[ Tue Jul 16 12:33:26 2024 ] 	Batch(6400/6809) done. Loss: 0.1004  lr:0.000001
[ Tue Jul 16 12:33:49 2024 ] 
Training: Epoch [105/150], Step [6499], Loss: 0.1539464294910431, Training Accuracy: 95.56346153846154
[ Tue Jul 16 12:33:49 2024 ] 	Batch(6500/6809) done. Loss: 0.0101  lr:0.000001
[ Tue Jul 16 12:34:11 2024 ] 	Batch(6600/6809) done. Loss: 0.0128  lr:0.000001
[ Tue Jul 16 12:34:34 2024 ] 	Batch(6700/6809) done. Loss: 0.0307  lr:0.000001
[ Tue Jul 16 12:34:57 2024 ] 	Batch(6800/6809) done. Loss: 0.0660  lr:0.000001
[ Tue Jul 16 12:34:59 2024 ] 	Mean training loss: 0.1573.
[ Tue Jul 16 12:34:59 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 12:34:59 2024 ] Training epoch: 107
[ Tue Jul 16 12:34:59 2024 ] 	Batch(0/6809) done. Loss: 0.1685  lr:0.000001
[ Tue Jul 16 12:35:22 2024 ] 	Batch(100/6809) done. Loss: 0.0447  lr:0.000001
[ Tue Jul 16 12:35:46 2024 ] 	Batch(200/6809) done. Loss: 0.0971  lr:0.000001
[ Tue Jul 16 12:36:09 2024 ] 	Batch(300/6809) done. Loss: 0.1959  lr:0.000001
[ Tue Jul 16 12:36:32 2024 ] 	Batch(400/6809) done. Loss: 0.0072  lr:0.000001
[ Tue Jul 16 12:36:54 2024 ] 
Training: Epoch [106/150], Step [499], Loss: 0.07621059566736221, Training Accuracy: 95.575
[ Tue Jul 16 12:36:54 2024 ] 	Batch(500/6809) done. Loss: 0.1599  lr:0.000001
[ Tue Jul 16 12:37:17 2024 ] 	Batch(600/6809) done. Loss: 0.1495  lr:0.000001
[ Tue Jul 16 12:37:40 2024 ] 	Batch(700/6809) done. Loss: 0.0653  lr:0.000001
[ Tue Jul 16 12:38:03 2024 ] 	Batch(800/6809) done. Loss: 0.0240  lr:0.000001
[ Tue Jul 16 12:38:26 2024 ] 	Batch(900/6809) done. Loss: 0.0975  lr:0.000001
[ Tue Jul 16 12:38:49 2024 ] 
Training: Epoch [106/150], Step [999], Loss: 0.03187396749854088, Training Accuracy: 95.775
[ Tue Jul 16 12:38:49 2024 ] 	Batch(1000/6809) done. Loss: 0.6253  lr:0.000001
[ Tue Jul 16 12:39:11 2024 ] 	Batch(1100/6809) done. Loss: 0.0073  lr:0.000001
[ Tue Jul 16 12:39:34 2024 ] 	Batch(1200/6809) done. Loss: 0.3234  lr:0.000001
[ Tue Jul 16 12:39:57 2024 ] 	Batch(1300/6809) done. Loss: 0.1595  lr:0.000001
[ Tue Jul 16 12:40:19 2024 ] 	Batch(1400/6809) done. Loss: 0.0108  lr:0.000001
[ Tue Jul 16 12:40:42 2024 ] 
Training: Epoch [106/150], Step [1499], Loss: 0.09422138333320618, Training Accuracy: 95.5
[ Tue Jul 16 12:40:42 2024 ] 	Batch(1500/6809) done. Loss: 0.2390  lr:0.000001
[ Tue Jul 16 12:41:05 2024 ] 	Batch(1600/6809) done. Loss: 0.0834  lr:0.000001
[ Tue Jul 16 12:41:27 2024 ] 	Batch(1700/6809) done. Loss: 0.0343  lr:0.000001
[ Tue Jul 16 12:41:50 2024 ] 	Batch(1800/6809) done. Loss: 0.0881  lr:0.000001
[ Tue Jul 16 12:42:13 2024 ] 	Batch(1900/6809) done. Loss: 0.0476  lr:0.000001
[ Tue Jul 16 12:42:35 2024 ] 
Training: Epoch [106/150], Step [1999], Loss: 0.12848764657974243, Training Accuracy: 95.65
[ Tue Jul 16 12:42:36 2024 ] 	Batch(2000/6809) done. Loss: 0.0131  lr:0.000001
[ Tue Jul 16 12:42:58 2024 ] 	Batch(2100/6809) done. Loss: 0.0950  lr:0.000001
[ Tue Jul 16 12:43:21 2024 ] 	Batch(2200/6809) done. Loss: 0.0399  lr:0.000001
[ Tue Jul 16 12:43:44 2024 ] 	Batch(2300/6809) done. Loss: 0.1423  lr:0.000001
[ Tue Jul 16 12:44:06 2024 ] 	Batch(2400/6809) done. Loss: 0.0513  lr:0.000001
[ Tue Jul 16 12:44:29 2024 ] 
Training: Epoch [106/150], Step [2499], Loss: 0.45815253257751465, Training Accuracy: 95.71499999999999
[ Tue Jul 16 12:44:29 2024 ] 	Batch(2500/6809) done. Loss: 0.0648  lr:0.000001
[ Tue Jul 16 12:44:52 2024 ] 	Batch(2600/6809) done. Loss: 0.0492  lr:0.000001
[ Tue Jul 16 12:45:14 2024 ] 	Batch(2700/6809) done. Loss: 0.1915  lr:0.000001
[ Tue Jul 16 12:45:37 2024 ] 	Batch(2800/6809) done. Loss: 0.0441  lr:0.000001
[ Tue Jul 16 12:45:59 2024 ] 	Batch(2900/6809) done. Loss: 0.1542  lr:0.000001
[ Tue Jul 16 12:46:22 2024 ] 
Training: Epoch [106/150], Step [2999], Loss: 0.04742169752717018, Training Accuracy: 95.68333333333334
[ Tue Jul 16 12:46:22 2024 ] 	Batch(3000/6809) done. Loss: 0.1191  lr:0.000001
[ Tue Jul 16 12:46:45 2024 ] 	Batch(3100/6809) done. Loss: 0.0437  lr:0.000001
[ Tue Jul 16 12:47:08 2024 ] 	Batch(3200/6809) done. Loss: 0.0688  lr:0.000001
[ Tue Jul 16 12:47:31 2024 ] 	Batch(3300/6809) done. Loss: 0.3285  lr:0.000001
[ Tue Jul 16 12:47:53 2024 ] 	Batch(3400/6809) done. Loss: 0.5140  lr:0.000001
[ Tue Jul 16 12:48:16 2024 ] 
Training: Epoch [106/150], Step [3499], Loss: 0.0021247745025902987, Training Accuracy: 95.69285714285715
[ Tue Jul 16 12:48:16 2024 ] 	Batch(3500/6809) done. Loss: 0.0669  lr:0.000001
[ Tue Jul 16 12:48:39 2024 ] 	Batch(3600/6809) done. Loss: 0.0590  lr:0.000001
[ Tue Jul 16 12:49:01 2024 ] 	Batch(3700/6809) done. Loss: 0.0268  lr:0.000001
[ Tue Jul 16 12:49:24 2024 ] 	Batch(3800/6809) done. Loss: 0.1095  lr:0.000001
[ Tue Jul 16 12:49:47 2024 ] 	Batch(3900/6809) done. Loss: 0.4656  lr:0.000001
[ Tue Jul 16 12:50:09 2024 ] 
Training: Epoch [106/150], Step [3999], Loss: 0.07244951277971268, Training Accuracy: 95.671875
[ Tue Jul 16 12:50:09 2024 ] 	Batch(4000/6809) done. Loss: 0.0574  lr:0.000001
[ Tue Jul 16 12:50:32 2024 ] 	Batch(4100/6809) done. Loss: 0.3634  lr:0.000001
[ Tue Jul 16 12:50:54 2024 ] 	Batch(4200/6809) done. Loss: 0.4598  lr:0.000001
[ Tue Jul 16 12:51:17 2024 ] 	Batch(4300/6809) done. Loss: 0.2092  lr:0.000001
[ Tue Jul 16 12:51:40 2024 ] 	Batch(4400/6809) done. Loss: 0.0697  lr:0.000001
[ Tue Jul 16 12:52:03 2024 ] 
Training: Epoch [106/150], Step [4499], Loss: 0.34087833762168884, Training Accuracy: 95.67777777777778
[ Tue Jul 16 12:52:03 2024 ] 	Batch(4500/6809) done. Loss: 0.0605  lr:0.000001
[ Tue Jul 16 12:52:26 2024 ] 	Batch(4600/6809) done. Loss: 0.0983  lr:0.000001
[ Tue Jul 16 12:52:49 2024 ] 	Batch(4700/6809) done. Loss: 0.0056  lr:0.000001
[ Tue Jul 16 12:53:13 2024 ] 	Batch(4800/6809) done. Loss: 0.0711  lr:0.000001
[ Tue Jul 16 12:53:36 2024 ] 	Batch(4900/6809) done. Loss: 0.5500  lr:0.000001
[ Tue Jul 16 12:53:59 2024 ] 
Training: Epoch [106/150], Step [4999], Loss: 0.01689102128148079, Training Accuracy: 95.675
[ Tue Jul 16 12:54:00 2024 ] 	Batch(5000/6809) done. Loss: 0.1400  lr:0.000001
[ Tue Jul 16 12:54:23 2024 ] 	Batch(5100/6809) done. Loss: 0.0699  lr:0.000001
[ Tue Jul 16 12:54:46 2024 ] 	Batch(5200/6809) done. Loss: 0.0630  lr:0.000001
[ Tue Jul 16 12:55:10 2024 ] 	Batch(5300/6809) done. Loss: 0.0057  lr:0.000001
[ Tue Jul 16 12:55:33 2024 ] 	Batch(5400/6809) done. Loss: 0.0486  lr:0.000001
[ Tue Jul 16 12:55:56 2024 ] 
Training: Epoch [106/150], Step [5499], Loss: 0.015932496637105942, Training Accuracy: 95.66136363636363
[ Tue Jul 16 12:55:56 2024 ] 	Batch(5500/6809) done. Loss: 0.0563  lr:0.000001
[ Tue Jul 16 12:56:20 2024 ] 	Batch(5600/6809) done. Loss: 0.1945  lr:0.000001
[ Tue Jul 16 12:56:43 2024 ] 	Batch(5700/6809) done. Loss: 0.1172  lr:0.000001
[ Tue Jul 16 12:57:07 2024 ] 	Batch(5800/6809) done. Loss: 0.0216  lr:0.000001
[ Tue Jul 16 12:57:30 2024 ] 	Batch(5900/6809) done. Loss: 0.0438  lr:0.000001
[ Tue Jul 16 12:57:53 2024 ] 
Training: Epoch [106/150], Step [5999], Loss: 0.048991017043590546, Training Accuracy: 95.65
[ Tue Jul 16 12:57:53 2024 ] 	Batch(6000/6809) done. Loss: 0.2721  lr:0.000001
[ Tue Jul 16 12:58:17 2024 ] 	Batch(6100/6809) done. Loss: 0.1796  lr:0.000001
[ Tue Jul 16 12:58:40 2024 ] 	Batch(6200/6809) done. Loss: 0.0356  lr:0.000001
[ Tue Jul 16 12:59:03 2024 ] 	Batch(6300/6809) done. Loss: 0.0109  lr:0.000001
[ Tue Jul 16 12:59:27 2024 ] 	Batch(6400/6809) done. Loss: 0.0015  lr:0.000001
[ Tue Jul 16 12:59:50 2024 ] 
Training: Epoch [106/150], Step [6499], Loss: 0.01453466061502695, Training Accuracy: 95.61153846153846
[ Tue Jul 16 12:59:50 2024 ] 	Batch(6500/6809) done. Loss: 0.0165  lr:0.000001
[ Tue Jul 16 13:00:14 2024 ] 	Batch(6600/6809) done. Loss: 0.3656  lr:0.000001
[ Tue Jul 16 13:00:37 2024 ] 	Batch(6700/6809) done. Loss: 0.3962  lr:0.000001
[ Tue Jul 16 13:01:00 2024 ] 	Batch(6800/6809) done. Loss: 0.0296  lr:0.000001
[ Tue Jul 16 13:01:02 2024 ] 	Mean training loss: 0.1552.
[ Tue Jul 16 13:01:02 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 13:01:03 2024 ] Training epoch: 108
[ Tue Jul 16 13:01:03 2024 ] 	Batch(0/6809) done. Loss: 0.2461  lr:0.000001
[ Tue Jul 16 13:01:26 2024 ] 	Batch(100/6809) done. Loss: 0.0323  lr:0.000001
[ Tue Jul 16 13:01:49 2024 ] 	Batch(200/6809) done. Loss: 0.0189  lr:0.000001
[ Tue Jul 16 13:02:12 2024 ] 	Batch(300/6809) done. Loss: 0.0059  lr:0.000001
[ Tue Jul 16 13:02:35 2024 ] 	Batch(400/6809) done. Loss: 0.0044  lr:0.000001
[ Tue Jul 16 13:02:57 2024 ] 
Training: Epoch [107/150], Step [499], Loss: 0.026847558096051216, Training Accuracy: 95.675
[ Tue Jul 16 13:02:57 2024 ] 	Batch(500/6809) done. Loss: 0.0433  lr:0.000001
[ Tue Jul 16 13:03:20 2024 ] 	Batch(600/6809) done. Loss: 0.1909  lr:0.000001
[ Tue Jul 16 13:03:43 2024 ] 	Batch(700/6809) done. Loss: 0.0640  lr:0.000001
[ Tue Jul 16 13:04:05 2024 ] 	Batch(800/6809) done. Loss: 0.9046  lr:0.000001
[ Tue Jul 16 13:04:28 2024 ] 	Batch(900/6809) done. Loss: 0.1787  lr:0.000001
[ Tue Jul 16 13:04:50 2024 ] 
Training: Epoch [107/150], Step [999], Loss: 0.1482614129781723, Training Accuracy: 95.575
[ Tue Jul 16 13:04:51 2024 ] 	Batch(1000/6809) done. Loss: 0.0750  lr:0.000001
[ Tue Jul 16 13:05:13 2024 ] 	Batch(1100/6809) done. Loss: 0.6799  lr:0.000001
[ Tue Jul 16 13:05:36 2024 ] 	Batch(1200/6809) done. Loss: 0.3298  lr:0.000001
[ Tue Jul 16 13:05:59 2024 ] 	Batch(1300/6809) done. Loss: 0.0736  lr:0.000001
[ Tue Jul 16 13:06:21 2024 ] 	Batch(1400/6809) done. Loss: 0.0239  lr:0.000001
[ Tue Jul 16 13:06:44 2024 ] 
Training: Epoch [107/150], Step [1499], Loss: 0.06674055010080338, Training Accuracy: 95.59166666666667
[ Tue Jul 16 13:06:44 2024 ] 	Batch(1500/6809) done. Loss: 0.1148  lr:0.000001
[ Tue Jul 16 13:07:07 2024 ] 	Batch(1600/6809) done. Loss: 0.0191  lr:0.000001
[ Tue Jul 16 13:07:30 2024 ] 	Batch(1700/6809) done. Loss: 0.0149  lr:0.000001
[ Tue Jul 16 13:07:52 2024 ] 	Batch(1800/6809) done. Loss: 0.4056  lr:0.000001
[ Tue Jul 16 13:08:15 2024 ] 	Batch(1900/6809) done. Loss: 0.2381  lr:0.000001
[ Tue Jul 16 13:08:38 2024 ] 
Training: Epoch [107/150], Step [1999], Loss: 0.37715548276901245, Training Accuracy: 95.575
[ Tue Jul 16 13:08:39 2024 ] 	Batch(2000/6809) done. Loss: 0.0383  lr:0.000001
[ Tue Jul 16 13:09:02 2024 ] 	Batch(2100/6809) done. Loss: 0.0360  lr:0.000001
[ Tue Jul 16 13:09:25 2024 ] 	Batch(2200/6809) done. Loss: 0.2145  lr:0.000001
[ Tue Jul 16 13:09:48 2024 ] 	Batch(2300/6809) done. Loss: 0.0304  lr:0.000001
[ Tue Jul 16 13:10:11 2024 ] 	Batch(2400/6809) done. Loss: 0.0602  lr:0.000001
[ Tue Jul 16 13:10:34 2024 ] 
Training: Epoch [107/150], Step [2499], Loss: 0.21873842179775238, Training Accuracy: 95.675
[ Tue Jul 16 13:10:34 2024 ] 	Batch(2500/6809) done. Loss: 0.1016  lr:0.000001
[ Tue Jul 16 13:10:57 2024 ] 	Batch(2600/6809) done. Loss: 0.1809  lr:0.000001
[ Tue Jul 16 13:11:20 2024 ] 	Batch(2700/6809) done. Loss: 0.3766  lr:0.000001
[ Tue Jul 16 13:11:44 2024 ] 	Batch(2800/6809) done. Loss: 0.1660  lr:0.000001
[ Tue Jul 16 13:12:07 2024 ] 	Batch(2900/6809) done. Loss: 0.0222  lr:0.000001
[ Tue Jul 16 13:12:29 2024 ] 
Training: Epoch [107/150], Step [2999], Loss: 0.016070881858468056, Training Accuracy: 95.6375
[ Tue Jul 16 13:12:30 2024 ] 	Batch(3000/6809) done. Loss: 0.0430  lr:0.000001
[ Tue Jul 16 13:12:52 2024 ] 	Batch(3100/6809) done. Loss: 0.1709  lr:0.000001
[ Tue Jul 16 13:13:15 2024 ] 	Batch(3200/6809) done. Loss: 0.0295  lr:0.000001
[ Tue Jul 16 13:13:38 2024 ] 	Batch(3300/6809) done. Loss: 0.0509  lr:0.000001
[ Tue Jul 16 13:14:00 2024 ] 	Batch(3400/6809) done. Loss: 0.4966  lr:0.000001
[ Tue Jul 16 13:14:23 2024 ] 
Training: Epoch [107/150], Step [3499], Loss: 0.07697068899869919, Training Accuracy: 95.61071428571428
[ Tue Jul 16 13:14:23 2024 ] 	Batch(3500/6809) done. Loss: 0.0124  lr:0.000001
[ Tue Jul 16 13:14:46 2024 ] 	Batch(3600/6809) done. Loss: 0.1431  lr:0.000001
[ Tue Jul 16 13:15:09 2024 ] 	Batch(3700/6809) done. Loss: 0.0236  lr:0.000001
[ Tue Jul 16 13:15:31 2024 ] 	Batch(3800/6809) done. Loss: 0.2828  lr:0.000001
[ Tue Jul 16 13:15:54 2024 ] 	Batch(3900/6809) done. Loss: 0.1290  lr:0.000001
[ Tue Jul 16 13:16:16 2024 ] 
Training: Epoch [107/150], Step [3999], Loss: 0.1257895976305008, Training Accuracy: 95.61874999999999
[ Tue Jul 16 13:16:17 2024 ] 	Batch(4000/6809) done. Loss: 0.0116  lr:0.000001
[ Tue Jul 16 13:16:39 2024 ] 	Batch(4100/6809) done. Loss: 0.1612  lr:0.000001
[ Tue Jul 16 13:17:02 2024 ] 	Batch(4200/6809) done. Loss: 0.0100  lr:0.000001
[ Tue Jul 16 13:17:25 2024 ] 	Batch(4300/6809) done. Loss: 0.2479  lr:0.000001
[ Tue Jul 16 13:17:48 2024 ] 	Batch(4400/6809) done. Loss: 0.0536  lr:0.000001
[ Tue Jul 16 13:18:10 2024 ] 
Training: Epoch [107/150], Step [4499], Loss: 0.02386385388672352, Training Accuracy: 95.625
[ Tue Jul 16 13:18:10 2024 ] 	Batch(4500/6809) done. Loss: 0.1612  lr:0.000001
[ Tue Jul 16 13:18:33 2024 ] 	Batch(4600/6809) done. Loss: 0.1038  lr:0.000001
[ Tue Jul 16 13:18:56 2024 ] 	Batch(4700/6809) done. Loss: 0.0175  lr:0.000001
[ Tue Jul 16 13:19:19 2024 ] 	Batch(4800/6809) done. Loss: 0.4239  lr:0.000001
[ Tue Jul 16 13:19:41 2024 ] 	Batch(4900/6809) done. Loss: 0.0420  lr:0.000001
[ Tue Jul 16 13:20:04 2024 ] 
Training: Epoch [107/150], Step [4999], Loss: 0.014398610219359398, Training Accuracy: 95.6475
[ Tue Jul 16 13:20:04 2024 ] 	Batch(5000/6809) done. Loss: 0.2117  lr:0.000001
[ Tue Jul 16 13:20:27 2024 ] 	Batch(5100/6809) done. Loss: 0.3460  lr:0.000001
[ Tue Jul 16 13:20:49 2024 ] 	Batch(5200/6809) done. Loss: 0.2498  lr:0.000001
[ Tue Jul 16 13:21:12 2024 ] 	Batch(5300/6809) done. Loss: 0.3710  lr:0.000001
[ Tue Jul 16 13:21:35 2024 ] 	Batch(5400/6809) done. Loss: 0.0678  lr:0.000001
[ Tue Jul 16 13:21:57 2024 ] 
Training: Epoch [107/150], Step [5499], Loss: 0.18589183688163757, Training Accuracy: 95.63636363636364
[ Tue Jul 16 13:21:58 2024 ] 	Batch(5500/6809) done. Loss: 0.0282  lr:0.000001
[ Tue Jul 16 13:22:20 2024 ] 	Batch(5600/6809) done. Loss: 0.0348  lr:0.000001
[ Tue Jul 16 13:22:43 2024 ] 	Batch(5700/6809) done. Loss: 0.1057  lr:0.000001
[ Tue Jul 16 13:23:06 2024 ] 	Batch(5800/6809) done. Loss: 0.7227  lr:0.000001
[ Tue Jul 16 13:23:28 2024 ] 	Batch(5900/6809) done. Loss: 0.0466  lr:0.000001
[ Tue Jul 16 13:23:51 2024 ] 
Training: Epoch [107/150], Step [5999], Loss: 0.08132851868867874, Training Accuracy: 95.68541666666667
[ Tue Jul 16 13:23:51 2024 ] 	Batch(6000/6809) done. Loss: 0.5430  lr:0.000001
[ Tue Jul 16 13:24:14 2024 ] 	Batch(6100/6809) done. Loss: 0.0813  lr:0.000001
[ Tue Jul 16 13:24:36 2024 ] 	Batch(6200/6809) done. Loss: 0.1054  lr:0.000001
[ Tue Jul 16 13:24:59 2024 ] 	Batch(6300/6809) done. Loss: 0.0408  lr:0.000001
[ Tue Jul 16 13:25:22 2024 ] 	Batch(6400/6809) done. Loss: 0.1089  lr:0.000001
[ Tue Jul 16 13:25:44 2024 ] 
Training: Epoch [107/150], Step [6499], Loss: 0.5094536542892456, Training Accuracy: 95.71153846153845
[ Tue Jul 16 13:25:44 2024 ] 	Batch(6500/6809) done. Loss: 0.0088  lr:0.000001
[ Tue Jul 16 13:26:07 2024 ] 	Batch(6600/6809) done. Loss: 0.1607  lr:0.000001
[ Tue Jul 16 13:26:30 2024 ] 	Batch(6700/6809) done. Loss: 0.1563  lr:0.000001
[ Tue Jul 16 13:26:52 2024 ] 	Batch(6800/6809) done. Loss: 0.1623  lr:0.000001
[ Tue Jul 16 13:26:54 2024 ] 	Mean training loss: 0.1521.
[ Tue Jul 16 13:26:54 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 13:26:54 2024 ] Training epoch: 109
[ Tue Jul 16 13:26:55 2024 ] 	Batch(0/6809) done. Loss: 0.0256  lr:0.000001
[ Tue Jul 16 13:27:18 2024 ] 	Batch(100/6809) done. Loss: 0.1761  lr:0.000001
[ Tue Jul 16 13:27:40 2024 ] 	Batch(200/6809) done. Loss: 0.2860  lr:0.000001
[ Tue Jul 16 13:28:03 2024 ] 	Batch(300/6809) done. Loss: 0.3825  lr:0.000001
[ Tue Jul 16 13:28:26 2024 ] 	Batch(400/6809) done. Loss: 0.0342  lr:0.000001
[ Tue Jul 16 13:28:48 2024 ] 
Training: Epoch [108/150], Step [499], Loss: 0.017079785466194153, Training Accuracy: 95.825
[ Tue Jul 16 13:28:48 2024 ] 	Batch(500/6809) done. Loss: 0.0378  lr:0.000001
[ Tue Jul 16 13:29:11 2024 ] 	Batch(600/6809) done. Loss: 0.5370  lr:0.000001
[ Tue Jul 16 13:29:34 2024 ] 	Batch(700/6809) done. Loss: 0.1012  lr:0.000001
[ Tue Jul 16 13:29:56 2024 ] 	Batch(800/6809) done. Loss: 0.0946  lr:0.000001
[ Tue Jul 16 13:30:19 2024 ] 	Batch(900/6809) done. Loss: 0.0966  lr:0.000001
[ Tue Jul 16 13:30:42 2024 ] 
Training: Epoch [108/150], Step [999], Loss: 0.016987010836601257, Training Accuracy: 95.75
[ Tue Jul 16 13:30:42 2024 ] 	Batch(1000/6809) done. Loss: 0.3519  lr:0.000001
[ Tue Jul 16 13:31:05 2024 ] 	Batch(1100/6809) done. Loss: 0.0744  lr:0.000001
[ Tue Jul 16 13:31:27 2024 ] 	Batch(1200/6809) done. Loss: 0.1111  lr:0.000001
[ Tue Jul 16 13:31:50 2024 ] 	Batch(1300/6809) done. Loss: 0.0101  lr:0.000001
[ Tue Jul 16 13:32:13 2024 ] 	Batch(1400/6809) done. Loss: 0.3114  lr:0.000001
[ Tue Jul 16 13:32:35 2024 ] 
Training: Epoch [108/150], Step [1499], Loss: 0.060658760368824005, Training Accuracy: 95.89166666666667
[ Tue Jul 16 13:32:35 2024 ] 	Batch(1500/6809) done. Loss: 0.0269  lr:0.000001
[ Tue Jul 16 13:32:58 2024 ] 	Batch(1600/6809) done. Loss: 0.1647  lr:0.000001
[ Tue Jul 16 13:33:21 2024 ] 	Batch(1700/6809) done. Loss: 0.0980  lr:0.000001
[ Tue Jul 16 13:33:43 2024 ] 	Batch(1800/6809) done. Loss: 0.1246  lr:0.000001
[ Tue Jul 16 13:34:06 2024 ] 	Batch(1900/6809) done. Loss: 0.0969  lr:0.000001
[ Tue Jul 16 13:34:29 2024 ] 
Training: Epoch [108/150], Step [1999], Loss: 0.19286717474460602, Training Accuracy: 95.71875
[ Tue Jul 16 13:34:29 2024 ] 	Batch(2000/6809) done. Loss: 0.2161  lr:0.000001
[ Tue Jul 16 13:34:52 2024 ] 	Batch(2100/6809) done. Loss: 0.0282  lr:0.000001
[ Tue Jul 16 13:35:14 2024 ] 	Batch(2200/6809) done. Loss: 0.1796  lr:0.000001
[ Tue Jul 16 13:35:37 2024 ] 	Batch(2300/6809) done. Loss: 0.1813  lr:0.000001
[ Tue Jul 16 13:36:00 2024 ] 	Batch(2400/6809) done. Loss: 0.0456  lr:0.000001
[ Tue Jul 16 13:36:22 2024 ] 
Training: Epoch [108/150], Step [2499], Loss: 0.17192356288433075, Training Accuracy: 95.705
[ Tue Jul 16 13:36:22 2024 ] 	Batch(2500/6809) done. Loss: 0.3036  lr:0.000001
[ Tue Jul 16 13:36:45 2024 ] 	Batch(2600/6809) done. Loss: 0.1271  lr:0.000001
[ Tue Jul 16 13:37:07 2024 ] 	Batch(2700/6809) done. Loss: 0.0620  lr:0.000001
[ Tue Jul 16 13:37:30 2024 ] 	Batch(2800/6809) done. Loss: 0.0936  lr:0.000001
[ Tue Jul 16 13:37:53 2024 ] 	Batch(2900/6809) done. Loss: 0.3996  lr:0.000001
[ Tue Jul 16 13:38:16 2024 ] 
Training: Epoch [108/150], Step [2999], Loss: 0.18215611577033997, Training Accuracy: 95.72500000000001
[ Tue Jul 16 13:38:16 2024 ] 	Batch(3000/6809) done. Loss: 0.0978  lr:0.000001
[ Tue Jul 16 13:38:38 2024 ] 	Batch(3100/6809) done. Loss: 0.0769  lr:0.000001
[ Tue Jul 16 13:39:01 2024 ] 	Batch(3200/6809) done. Loss: 0.2227  lr:0.000001
[ Tue Jul 16 13:39:24 2024 ] 	Batch(3300/6809) done. Loss: 0.5318  lr:0.000001
[ Tue Jul 16 13:39:47 2024 ] 	Batch(3400/6809) done. Loss: 0.0074  lr:0.000001
[ Tue Jul 16 13:40:09 2024 ] 
Training: Epoch [108/150], Step [3499], Loss: 0.11035820841789246, Training Accuracy: 95.76428571428572
[ Tue Jul 16 13:40:09 2024 ] 	Batch(3500/6809) done. Loss: 0.1010  lr:0.000001
[ Tue Jul 16 13:40:32 2024 ] 	Batch(3600/6809) done. Loss: 0.1954  lr:0.000001
[ Tue Jul 16 13:40:55 2024 ] 	Batch(3700/6809) done. Loss: 0.3468  lr:0.000001
[ Tue Jul 16 13:41:17 2024 ] 	Batch(3800/6809) done. Loss: 0.0944  lr:0.000001
[ Tue Jul 16 13:41:40 2024 ] 	Batch(3900/6809) done. Loss: 0.2239  lr:0.000001
[ Tue Jul 16 13:42:02 2024 ] 
Training: Epoch [108/150], Step [3999], Loss: 0.013284758664667606, Training Accuracy: 95.73125
[ Tue Jul 16 13:42:03 2024 ] 	Batch(4000/6809) done. Loss: 0.0195  lr:0.000001
[ Tue Jul 16 13:42:25 2024 ] 	Batch(4100/6809) done. Loss: 0.3152  lr:0.000001
[ Tue Jul 16 13:42:48 2024 ] 	Batch(4200/6809) done. Loss: 0.0565  lr:0.000001
[ Tue Jul 16 13:43:11 2024 ] 	Batch(4300/6809) done. Loss: 0.4939  lr:0.000001
[ Tue Jul 16 13:43:34 2024 ] 	Batch(4400/6809) done. Loss: 0.0403  lr:0.000001
[ Tue Jul 16 13:43:56 2024 ] 
Training: Epoch [108/150], Step [4499], Loss: 0.1848296970129013, Training Accuracy: 95.66666666666667
[ Tue Jul 16 13:43:56 2024 ] 	Batch(4500/6809) done. Loss: 0.3150  lr:0.000001
[ Tue Jul 16 13:44:19 2024 ] 	Batch(4600/6809) done. Loss: 0.5815  lr:0.000001
[ Tue Jul 16 13:44:42 2024 ] 	Batch(4700/6809) done. Loss: 0.0622  lr:0.000001
[ Tue Jul 16 13:45:05 2024 ] 	Batch(4800/6809) done. Loss: 0.0089  lr:0.000001
[ Tue Jul 16 13:45:27 2024 ] 	Batch(4900/6809) done. Loss: 0.0363  lr:0.000001
[ Tue Jul 16 13:45:50 2024 ] 
Training: Epoch [108/150], Step [4999], Loss: 0.25195810198783875, Training Accuracy: 95.6425
[ Tue Jul 16 13:45:50 2024 ] 	Batch(5000/6809) done. Loss: 0.1127  lr:0.000001
[ Tue Jul 16 13:46:13 2024 ] 	Batch(5100/6809) done. Loss: 0.1222  lr:0.000001
[ Tue Jul 16 13:46:35 2024 ] 	Batch(5200/6809) done. Loss: 0.0653  lr:0.000001
[ Tue Jul 16 13:46:58 2024 ] 	Batch(5300/6809) done. Loss: 0.1453  lr:0.000001
[ Tue Jul 16 13:47:21 2024 ] 	Batch(5400/6809) done. Loss: 0.0995  lr:0.000001
[ Tue Jul 16 13:47:43 2024 ] 
Training: Epoch [108/150], Step [5499], Loss: 0.15323863923549652, Training Accuracy: 95.68409090909091
[ Tue Jul 16 13:47:43 2024 ] 	Batch(5500/6809) done. Loss: 0.0788  lr:0.000001
[ Tue Jul 16 13:48:06 2024 ] 	Batch(5600/6809) done. Loss: 0.0750  lr:0.000001
[ Tue Jul 16 13:48:29 2024 ] 	Batch(5700/6809) done. Loss: 0.0519  lr:0.000001
[ Tue Jul 16 13:48:52 2024 ] 	Batch(5800/6809) done. Loss: 0.2473  lr:0.000001
[ Tue Jul 16 13:49:15 2024 ] 	Batch(5900/6809) done. Loss: 0.0786  lr:0.000001
[ Tue Jul 16 13:49:37 2024 ] 
Training: Epoch [108/150], Step [5999], Loss: 0.027096066623926163, Training Accuracy: 95.69375
[ Tue Jul 16 13:49:37 2024 ] 	Batch(6000/6809) done. Loss: 0.1205  lr:0.000001
[ Tue Jul 16 13:50:00 2024 ] 	Batch(6100/6809) done. Loss: 0.0723  lr:0.000001
[ Tue Jul 16 13:50:23 2024 ] 	Batch(6200/6809) done. Loss: 0.0429  lr:0.000001
[ Tue Jul 16 13:50:46 2024 ] 	Batch(6300/6809) done. Loss: 0.1321  lr:0.000001
[ Tue Jul 16 13:51:08 2024 ] 	Batch(6400/6809) done. Loss: 0.0791  lr:0.000001
[ Tue Jul 16 13:51:31 2024 ] 
Training: Epoch [108/150], Step [6499], Loss: 0.27058127522468567, Training Accuracy: 95.70192307692308
[ Tue Jul 16 13:51:31 2024 ] 	Batch(6500/6809) done. Loss: 0.0416  lr:0.000001
[ Tue Jul 16 13:51:54 2024 ] 	Batch(6600/6809) done. Loss: 0.0580  lr:0.000001
[ Tue Jul 16 13:52:16 2024 ] 	Batch(6700/6809) done. Loss: 0.0875  lr:0.000001
[ Tue Jul 16 13:52:39 2024 ] 	Batch(6800/6809) done. Loss: 0.1134  lr:0.000001
[ Tue Jul 16 13:52:41 2024 ] 	Mean training loss: 0.1518.
[ Tue Jul 16 13:52:41 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 13:52:41 2024 ] Training epoch: 110
[ Tue Jul 16 13:52:42 2024 ] 	Batch(0/6809) done. Loss: 0.0758  lr:0.000001
[ Tue Jul 16 13:53:05 2024 ] 	Batch(100/6809) done. Loss: 0.2213  lr:0.000001
[ Tue Jul 16 13:53:27 2024 ] 	Batch(200/6809) done. Loss: 0.1821  lr:0.000001
[ Tue Jul 16 13:53:50 2024 ] 	Batch(300/6809) done. Loss: 0.1515  lr:0.000001
[ Tue Jul 16 13:54:13 2024 ] 	Batch(400/6809) done. Loss: 0.2635  lr:0.000001
[ Tue Jul 16 13:54:35 2024 ] 
Training: Epoch [109/150], Step [499], Loss: 0.02930113859474659, Training Accuracy: 95.92500000000001
[ Tue Jul 16 13:54:35 2024 ] 	Batch(500/6809) done. Loss: 0.1978  lr:0.000001
[ Tue Jul 16 13:54:58 2024 ] 	Batch(600/6809) done. Loss: 0.0033  lr:0.000001
[ Tue Jul 16 13:55:21 2024 ] 	Batch(700/6809) done. Loss: 0.1336  lr:0.000001
[ Tue Jul 16 13:55:44 2024 ] 	Batch(800/6809) done. Loss: 0.1652  lr:0.000001
[ Tue Jul 16 13:56:06 2024 ] 	Batch(900/6809) done. Loss: 0.1197  lr:0.000001
[ Tue Jul 16 13:56:29 2024 ] 
Training: Epoch [109/150], Step [999], Loss: 0.05051424354314804, Training Accuracy: 95.5625
[ Tue Jul 16 13:56:29 2024 ] 	Batch(1000/6809) done. Loss: 0.0086  lr:0.000001
[ Tue Jul 16 13:56:52 2024 ] 	Batch(1100/6809) done. Loss: 0.0160  lr:0.000001
[ Tue Jul 16 13:57:15 2024 ] 	Batch(1200/6809) done. Loss: 0.3498  lr:0.000001
[ Tue Jul 16 13:57:37 2024 ] 	Batch(1300/6809) done. Loss: 0.0682  lr:0.000001
[ Tue Jul 16 13:58:01 2024 ] 	Batch(1400/6809) done. Loss: 0.2244  lr:0.000001
[ Tue Jul 16 13:58:23 2024 ] 
Training: Epoch [109/150], Step [1499], Loss: 0.8420202136039734, Training Accuracy: 95.75833333333334
[ Tue Jul 16 13:58:24 2024 ] 	Batch(1500/6809) done. Loss: 0.2741  lr:0.000001
[ Tue Jul 16 13:58:47 2024 ] 	Batch(1600/6809) done. Loss: 0.0413  lr:0.000001
[ Tue Jul 16 13:59:10 2024 ] 	Batch(1700/6809) done. Loss: 0.2430  lr:0.000001
[ Tue Jul 16 13:59:33 2024 ] 	Batch(1800/6809) done. Loss: 0.0685  lr:0.000001
[ Tue Jul 16 13:59:56 2024 ] 	Batch(1900/6809) done. Loss: 0.1129  lr:0.000001
[ Tue Jul 16 14:00:19 2024 ] 
Training: Epoch [109/150], Step [1999], Loss: 0.846419632434845, Training Accuracy: 95.75
[ Tue Jul 16 14:00:19 2024 ] 	Batch(2000/6809) done. Loss: 0.3254  lr:0.000001
[ Tue Jul 16 14:00:43 2024 ] 	Batch(2100/6809) done. Loss: 0.1897  lr:0.000001
[ Tue Jul 16 14:01:06 2024 ] 	Batch(2200/6809) done. Loss: 0.0313  lr:0.000001
[ Tue Jul 16 14:01:29 2024 ] 	Batch(2300/6809) done. Loss: 0.3653  lr:0.000001
[ Tue Jul 16 14:01:52 2024 ] 	Batch(2400/6809) done. Loss: 0.0644  lr:0.000001
[ Tue Jul 16 14:02:15 2024 ] 
Training: Epoch [109/150], Step [2499], Loss: 0.1049143373966217, Training Accuracy: 95.895
[ Tue Jul 16 14:02:15 2024 ] 	Batch(2500/6809) done. Loss: 0.0978  lr:0.000001
[ Tue Jul 16 14:02:38 2024 ] 	Batch(2600/6809) done. Loss: 0.0692  lr:0.000001
[ Tue Jul 16 14:03:01 2024 ] 	Batch(2700/6809) done. Loss: 0.2469  lr:0.000001
[ Tue Jul 16 14:03:25 2024 ] 	Batch(2800/6809) done. Loss: 0.4452  lr:0.000001
[ Tue Jul 16 14:03:48 2024 ] 	Batch(2900/6809) done. Loss: 0.0146  lr:0.000001
[ Tue Jul 16 14:04:11 2024 ] 
Training: Epoch [109/150], Step [2999], Loss: 0.025054138153791428, Training Accuracy: 95.86666666666666
[ Tue Jul 16 14:04:11 2024 ] 	Batch(3000/6809) done. Loss: 0.1445  lr:0.000001
[ Tue Jul 16 14:04:34 2024 ] 	Batch(3100/6809) done. Loss: 0.0433  lr:0.000001
[ Tue Jul 16 14:04:57 2024 ] 	Batch(3200/6809) done. Loss: 0.0624  lr:0.000001
[ Tue Jul 16 14:05:20 2024 ] 	Batch(3300/6809) done. Loss: 0.1834  lr:0.000001
[ Tue Jul 16 14:05:43 2024 ] 	Batch(3400/6809) done. Loss: 0.1020  lr:0.000001
[ Tue Jul 16 14:06:06 2024 ] 
Training: Epoch [109/150], Step [3499], Loss: 0.10025529563426971, Training Accuracy: 95.85714285714285
[ Tue Jul 16 14:06:06 2024 ] 	Batch(3500/6809) done. Loss: 0.1368  lr:0.000001
[ Tue Jul 16 14:06:30 2024 ] 	Batch(3600/6809) done. Loss: 0.1699  lr:0.000001
[ Tue Jul 16 14:06:53 2024 ] 	Batch(3700/6809) done. Loss: 0.1696  lr:0.000001
[ Tue Jul 16 14:07:16 2024 ] 	Batch(3800/6809) done. Loss: 0.0736  lr:0.000001
[ Tue Jul 16 14:07:39 2024 ] 	Batch(3900/6809) done. Loss: 0.2000  lr:0.000001
[ Tue Jul 16 14:08:02 2024 ] 
Training: Epoch [109/150], Step [3999], Loss: 0.10653279721736908, Training Accuracy: 95.821875
[ Tue Jul 16 14:08:02 2024 ] 	Batch(4000/6809) done. Loss: 0.1776  lr:0.000001
[ Tue Jul 16 14:08:25 2024 ] 	Batch(4100/6809) done. Loss: 0.0745  lr:0.000001
[ Tue Jul 16 14:08:48 2024 ] 	Batch(4200/6809) done. Loss: 0.0073  lr:0.000001
[ Tue Jul 16 14:09:11 2024 ] 	Batch(4300/6809) done. Loss: 0.0256  lr:0.000001
[ Tue Jul 16 14:09:34 2024 ] 	Batch(4400/6809) done. Loss: 0.3998  lr:0.000001
[ Tue Jul 16 14:09:56 2024 ] 
Training: Epoch [109/150], Step [4499], Loss: 0.4619537591934204, Training Accuracy: 95.80277777777778
[ Tue Jul 16 14:09:56 2024 ] 	Batch(4500/6809) done. Loss: 0.0515  lr:0.000001
[ Tue Jul 16 14:10:19 2024 ] 	Batch(4600/6809) done. Loss: 0.2871  lr:0.000001
[ Tue Jul 16 14:10:42 2024 ] 	Batch(4700/6809) done. Loss: 0.2378  lr:0.000001
[ Tue Jul 16 14:11:05 2024 ] 	Batch(4800/6809) done. Loss: 0.0147  lr:0.000001
[ Tue Jul 16 14:11:27 2024 ] 	Batch(4900/6809) done. Loss: 0.0418  lr:0.000001
[ Tue Jul 16 14:11:50 2024 ] 
Training: Epoch [109/150], Step [4999], Loss: 0.38626495003700256, Training Accuracy: 95.77
[ Tue Jul 16 14:11:50 2024 ] 	Batch(5000/6809) done. Loss: 0.3926  lr:0.000001
[ Tue Jul 16 14:12:13 2024 ] 	Batch(5100/6809) done. Loss: 0.2277  lr:0.000001
[ Tue Jul 16 14:12:35 2024 ] 	Batch(5200/6809) done. Loss: 0.1748  lr:0.000001
[ Tue Jul 16 14:12:59 2024 ] 	Batch(5300/6809) done. Loss: 0.1103  lr:0.000001
[ Tue Jul 16 14:13:21 2024 ] 	Batch(5400/6809) done. Loss: 0.1411  lr:0.000001
[ Tue Jul 16 14:13:44 2024 ] 
Training: Epoch [109/150], Step [5499], Loss: 0.020840061828494072, Training Accuracy: 95.78636363636363
[ Tue Jul 16 14:13:44 2024 ] 	Batch(5500/6809) done. Loss: 0.0204  lr:0.000001
[ Tue Jul 16 14:14:07 2024 ] 	Batch(5600/6809) done. Loss: 0.0394  lr:0.000001
[ Tue Jul 16 14:14:30 2024 ] 	Batch(5700/6809) done. Loss: 0.0780  lr:0.000001
[ Tue Jul 16 14:14:53 2024 ] 	Batch(5800/6809) done. Loss: 0.2861  lr:0.000001
[ Tue Jul 16 14:15:16 2024 ] 	Batch(5900/6809) done. Loss: 0.0706  lr:0.000001
[ Tue Jul 16 14:15:39 2024 ] 
Training: Epoch [109/150], Step [5999], Loss: 0.00597008503973484, Training Accuracy: 95.78541666666666
[ Tue Jul 16 14:15:39 2024 ] 	Batch(6000/6809) done. Loss: 0.1278  lr:0.000001
[ Tue Jul 16 14:16:02 2024 ] 	Batch(6100/6809) done. Loss: 0.2549  lr:0.000001
[ Tue Jul 16 14:16:25 2024 ] 	Batch(6200/6809) done. Loss: 0.2782  lr:0.000001
[ Tue Jul 16 14:16:47 2024 ] 	Batch(6300/6809) done. Loss: 0.0173  lr:0.000001
[ Tue Jul 16 14:17:10 2024 ] 	Batch(6400/6809) done. Loss: 0.1392  lr:0.000001
[ Tue Jul 16 14:17:33 2024 ] 
Training: Epoch [109/150], Step [6499], Loss: 0.05425107479095459, Training Accuracy: 95.70961538461539
[ Tue Jul 16 14:17:33 2024 ] 	Batch(6500/6809) done. Loss: 0.0267  lr:0.000001
[ Tue Jul 16 14:17:56 2024 ] 	Batch(6600/6809) done. Loss: 0.0551  lr:0.000001
[ Tue Jul 16 14:18:18 2024 ] 	Batch(6700/6809) done. Loss: 0.1324  lr:0.000001
[ Tue Jul 16 14:18:41 2024 ] 	Batch(6800/6809) done. Loss: 0.4692  lr:0.000001
[ Tue Jul 16 14:18:43 2024 ] 	Mean training loss: 0.1545.
[ Tue Jul 16 14:18:43 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 14:18:43 2024 ] Eval epoch: 110
[ Tue Jul 16 14:25:39 2024 ] 	Mean val loss of 7435 batches: 0.8694652053402374.
[ Tue Jul 16 14:25:39 2024 ] 
Validation: Epoch [109/150], Samples [47687.0/59477], Loss: 2.700906276702881, Validation Accuracy: 80.17721135901272
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 1 : 391 / 500 = 78 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 2 : 422 / 499 = 84 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 3 : 425 / 500 = 85 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 4 : 431 / 502 = 85 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 5 : 428 / 502 = 85 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 6 : 429 / 502 = 85 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 7 : 465 / 497 = 93 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 8 : 487 / 498 = 97 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 9 : 393 / 500 = 78 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 10 : 321 / 500 = 64 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 11 : 226 / 498 = 45 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 12 : 400 / 499 = 80 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 13 : 488 / 502 = 97 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 14 : 482 / 504 = 95 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 15 : 381 / 502 = 75 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 16 : 377 / 502 = 75 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 17 : 429 / 504 = 85 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 18 : 418 / 504 = 82 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 19 : 441 / 502 = 87 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 20 : 458 / 502 = 91 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 21 : 466 / 503 = 92 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 22 : 439 / 504 = 87 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 23 : 435 / 503 = 86 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 24 : 434 / 504 = 86 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 25 : 486 / 504 = 96 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 26 : 472 / 504 = 93 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 27 : 429 / 501 = 85 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 28 : 355 / 502 = 70 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 29 : 311 / 502 = 61 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 30 : 378 / 501 = 75 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 31 : 439 / 504 = 87 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 32 : 439 / 503 = 87 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 33 : 399 / 503 = 79 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 34 : 487 / 504 = 96 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 35 : 462 / 503 = 91 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 36 : 388 / 502 = 77 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 37 : 453 / 504 = 89 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 38 : 443 / 504 = 87 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 39 : 453 / 498 = 90 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 40 : 397 / 504 = 78 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 41 : 469 / 503 = 93 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 42 : 456 / 504 = 90 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 43 : 365 / 503 = 72 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 44 : 421 / 504 = 83 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 45 : 420 / 504 = 83 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 46 : 385 / 504 = 76 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 47 : 346 / 503 = 68 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 48 : 419 / 503 = 83 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 49 : 410 / 499 = 82 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 50 : 412 / 502 = 82 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 51 : 463 / 503 = 92 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 52 : 462 / 504 = 91 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 53 : 451 / 497 = 90 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 54 : 449 / 480 = 93 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 55 : 437 / 504 = 86 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 56 : 421 / 503 = 83 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 57 : 469 / 504 = 93 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 58 : 483 / 499 = 96 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 59 : 484 / 503 = 96 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 60 : 413 / 479 = 86 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 61 : 399 / 484 = 82 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 62 : 400 / 487 = 82 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 63 : 447 / 489 = 91 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 64 : 409 / 488 = 83 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 65 : 431 / 490 = 87 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 66 : 306 / 488 = 62 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 67 : 359 / 490 = 73 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 68 : 308 / 490 = 62 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 69 : 379 / 490 = 77 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 70 : 164 / 490 = 33 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 71 : 275 / 490 = 56 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 72 : 194 / 488 = 39 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 73 : 261 / 486 = 53 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 74 : 273 / 481 = 56 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 75 : 297 / 488 = 60 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 76 : 322 / 489 = 65 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 77 : 315 / 488 = 64 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 78 : 380 / 488 = 77 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 79 : 459 / 490 = 93 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 80 : 388 / 489 = 79 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 81 : 289 / 491 = 58 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 82 : 312 / 491 = 63 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 83 : 253 / 489 = 51 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 84 : 362 / 489 = 74 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 85 : 331 / 489 = 67 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 86 : 425 / 491 = 86 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 87 : 419 / 492 = 85 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 88 : 363 / 491 = 73 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 89 : 376 / 492 = 76 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 90 : 290 / 490 = 59 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 91 : 367 / 482 = 76 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 92 : 367 / 490 = 74 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 93 : 367 / 487 = 75 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 94 : 439 / 489 = 89 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 95 : 404 / 490 = 82 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 96 : 459 / 491 = 93 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 97 : 459 / 490 = 93 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 98 : 441 / 491 = 89 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 99 : 449 / 491 = 91 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 100 : 449 / 491 = 91 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 101 : 419 / 491 = 85 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 102 : 288 / 492 = 58 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 103 : 389 / 492 = 79 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 104 : 293 / 491 = 59 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 105 : 274 / 491 = 55 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 106 : 321 / 492 = 65 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 107 : 419 / 491 = 85 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 108 : 371 / 492 = 75 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 109 : 359 / 490 = 73 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 110 : 415 / 491 = 84 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 111 : 445 / 492 = 90 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 112 : 461 / 492 = 93 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 113 : 443 / 491 = 90 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 114 : 397 / 491 = 80 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 115 : 416 / 492 = 84 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 116 : 414 / 491 = 84 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 117 : 405 / 492 = 82 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 118 : 415 / 490 = 84 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 119 : 449 / 492 = 91 %
[ Tue Jul 16 14:25:39 2024 ] Accuracy of 120 : 415 / 500 = 83 %
[ Tue Jul 16 14:25:39 2024 ] Training epoch: 111
[ Tue Jul 16 14:25:40 2024 ] 	Batch(0/6809) done. Loss: 0.1029  lr:0.000001
[ Tue Jul 16 14:26:03 2024 ] 	Batch(100/6809) done. Loss: 0.0063  lr:0.000001
[ Tue Jul 16 14:26:26 2024 ] 	Batch(200/6809) done. Loss: 0.0336  lr:0.000001
[ Tue Jul 16 14:26:49 2024 ] 	Batch(300/6809) done. Loss: 0.0134  lr:0.000001
[ Tue Jul 16 14:27:13 2024 ] 	Batch(400/6809) done. Loss: 0.3375  lr:0.000001
[ Tue Jul 16 14:27:36 2024 ] 
Training: Epoch [110/150], Step [499], Loss: 0.2858380675315857, Training Accuracy: 95.6
[ Tue Jul 16 14:27:36 2024 ] 	Batch(500/6809) done. Loss: 0.2225  lr:0.000001
[ Tue Jul 16 14:27:59 2024 ] 	Batch(600/6809) done. Loss: 0.1604  lr:0.000001
[ Tue Jul 16 14:28:22 2024 ] 	Batch(700/6809) done. Loss: 0.1393  lr:0.000001
[ Tue Jul 16 14:28:45 2024 ] 	Batch(800/6809) done. Loss: 0.1946  lr:0.000001
[ Tue Jul 16 14:29:08 2024 ] 	Batch(900/6809) done. Loss: 0.3399  lr:0.000001
[ Tue Jul 16 14:29:31 2024 ] 
Training: Epoch [110/150], Step [999], Loss: 0.07986534386873245, Training Accuracy: 95.7125
[ Tue Jul 16 14:29:31 2024 ] 	Batch(1000/6809) done. Loss: 0.6424  lr:0.000001
[ Tue Jul 16 14:29:54 2024 ] 	Batch(1100/6809) done. Loss: 0.0413  lr:0.000001
[ Tue Jul 16 14:30:17 2024 ] 	Batch(1200/6809) done. Loss: 0.2334  lr:0.000001
[ Tue Jul 16 14:30:40 2024 ] 	Batch(1300/6809) done. Loss: 0.1628  lr:0.000001
[ Tue Jul 16 14:31:03 2024 ] 	Batch(1400/6809) done. Loss: 0.1474  lr:0.000001
[ Tue Jul 16 14:31:26 2024 ] 
Training: Epoch [110/150], Step [1499], Loss: 0.0324096642434597, Training Accuracy: 95.625
[ Tue Jul 16 14:31:26 2024 ] 	Batch(1500/6809) done. Loss: 0.0805  lr:0.000001
[ Tue Jul 16 14:31:50 2024 ] 	Batch(1600/6809) done. Loss: 0.0938  lr:0.000001
[ Tue Jul 16 14:32:13 2024 ] 	Batch(1700/6809) done. Loss: 0.1934  lr:0.000001
[ Tue Jul 16 14:32:36 2024 ] 	Batch(1800/6809) done. Loss: 0.2356  lr:0.000001
[ Tue Jul 16 14:32:59 2024 ] 	Batch(1900/6809) done. Loss: 0.4821  lr:0.000001
[ Tue Jul 16 14:33:22 2024 ] 
Training: Epoch [110/150], Step [1999], Loss: 0.5141966938972473, Training Accuracy: 95.60625
[ Tue Jul 16 14:33:22 2024 ] 	Batch(2000/6809) done. Loss: 0.0154  lr:0.000001
[ Tue Jul 16 14:33:45 2024 ] 	Batch(2100/6809) done. Loss: 0.0723  lr:0.000001
[ Tue Jul 16 14:34:08 2024 ] 	Batch(2200/6809) done. Loss: 0.0550  lr:0.000001
[ Tue Jul 16 14:34:31 2024 ] 	Batch(2300/6809) done. Loss: 0.0063  lr:0.000001
[ Tue Jul 16 14:34:54 2024 ] 	Batch(2400/6809) done. Loss: 0.0336  lr:0.000001
[ Tue Jul 16 14:35:17 2024 ] 
Training: Epoch [110/150], Step [2499], Loss: 0.02457861602306366, Training Accuracy: 95.67
[ Tue Jul 16 14:35:17 2024 ] 	Batch(2500/6809) done. Loss: 0.0261  lr:0.000001
[ Tue Jul 16 14:35:40 2024 ] 	Batch(2600/6809) done. Loss: 0.0408  lr:0.000001
[ Tue Jul 16 14:36:02 2024 ] 	Batch(2700/6809) done. Loss: 0.0233  lr:0.000001
[ Tue Jul 16 14:36:26 2024 ] 	Batch(2800/6809) done. Loss: 0.7101  lr:0.000001
[ Tue Jul 16 14:36:49 2024 ] 	Batch(2900/6809) done. Loss: 0.1091  lr:0.000001
[ Tue Jul 16 14:37:12 2024 ] 
Training: Epoch [110/150], Step [2999], Loss: 0.025627605617046356, Training Accuracy: 95.72083333333333
[ Tue Jul 16 14:37:12 2024 ] 	Batch(3000/6809) done. Loss: 0.1209  lr:0.000001
[ Tue Jul 16 14:37:35 2024 ] 	Batch(3100/6809) done. Loss: 0.2388  lr:0.000001
[ Tue Jul 16 14:37:58 2024 ] 	Batch(3200/6809) done. Loss: 0.0553  lr:0.000001
[ Tue Jul 16 14:38:21 2024 ] 	Batch(3300/6809) done. Loss: 0.0094  lr:0.000001
[ Tue Jul 16 14:38:43 2024 ] 	Batch(3400/6809) done. Loss: 0.0638  lr:0.000001
[ Tue Jul 16 14:39:06 2024 ] 
Training: Epoch [110/150], Step [3499], Loss: 0.0912761464715004, Training Accuracy: 95.73214285714286
[ Tue Jul 16 14:39:06 2024 ] 	Batch(3500/6809) done. Loss: 0.1004  lr:0.000001
[ Tue Jul 16 14:39:29 2024 ] 	Batch(3600/6809) done. Loss: 0.3896  lr:0.000001
[ Tue Jul 16 14:39:52 2024 ] 	Batch(3700/6809) done. Loss: 0.2021  lr:0.000001
[ Tue Jul 16 14:40:14 2024 ] 	Batch(3800/6809) done. Loss: 0.4718  lr:0.000001
[ Tue Jul 16 14:40:37 2024 ] 	Batch(3900/6809) done. Loss: 0.1309  lr:0.000001
[ Tue Jul 16 14:40:59 2024 ] 
Training: Epoch [110/150], Step [3999], Loss: 0.0034103470388799906, Training Accuracy: 95.71875
[ Tue Jul 16 14:41:00 2024 ] 	Batch(4000/6809) done. Loss: 0.0642  lr:0.000001
[ Tue Jul 16 14:41:22 2024 ] 	Batch(4100/6809) done. Loss: 0.5975  lr:0.000001
[ Tue Jul 16 14:41:45 2024 ] 	Batch(4200/6809) done. Loss: 0.1544  lr:0.000001
[ Tue Jul 16 14:42:08 2024 ] 	Batch(4300/6809) done. Loss: 0.0171  lr:0.000001
[ Tue Jul 16 14:42:31 2024 ] 	Batch(4400/6809) done. Loss: 0.0233  lr:0.000001
[ Tue Jul 16 14:42:53 2024 ] 
Training: Epoch [110/150], Step [4499], Loss: 0.12445726990699768, Training Accuracy: 95.71111111111111
[ Tue Jul 16 14:42:53 2024 ] 	Batch(4500/6809) done. Loss: 0.0364  lr:0.000001
[ Tue Jul 16 14:43:16 2024 ] 	Batch(4600/6809) done. Loss: 0.0609  lr:0.000001
[ Tue Jul 16 14:43:38 2024 ] 	Batch(4700/6809) done. Loss: 0.0090  lr:0.000001
[ Tue Jul 16 14:44:01 2024 ] 	Batch(4800/6809) done. Loss: 0.0029  lr:0.000001
[ Tue Jul 16 14:44:24 2024 ] 	Batch(4900/6809) done. Loss: 0.1489  lr:0.000001
[ Tue Jul 16 14:44:46 2024 ] 
Training: Epoch [110/150], Step [4999], Loss: 0.09829022735357285, Training Accuracy: 95.72749999999999
[ Tue Jul 16 14:44:47 2024 ] 	Batch(5000/6809) done. Loss: 0.0311  lr:0.000001
[ Tue Jul 16 14:45:09 2024 ] 	Batch(5100/6809) done. Loss: 0.1807  lr:0.000001
[ Tue Jul 16 14:45:32 2024 ] 	Batch(5200/6809) done. Loss: 0.0862  lr:0.000001
[ Tue Jul 16 14:45:55 2024 ] 	Batch(5300/6809) done. Loss: 0.0366  lr:0.000001
[ Tue Jul 16 14:46:18 2024 ] 	Batch(5400/6809) done. Loss: 0.1155  lr:0.000001
[ Tue Jul 16 14:46:40 2024 ] 
Training: Epoch [110/150], Step [5499], Loss: 0.17111840844154358, Training Accuracy: 95.8
[ Tue Jul 16 14:46:41 2024 ] 	Batch(5500/6809) done. Loss: 0.2783  lr:0.000001
[ Tue Jul 16 14:47:03 2024 ] 	Batch(5600/6809) done. Loss: 0.0784  lr:0.000001
[ Tue Jul 16 14:47:26 2024 ] 	Batch(5700/6809) done. Loss: 0.3328  lr:0.000001
[ Tue Jul 16 14:47:48 2024 ] 	Batch(5800/6809) done. Loss: 0.0055  lr:0.000001
[ Tue Jul 16 14:48:11 2024 ] 	Batch(5900/6809) done. Loss: 0.2861  lr:0.000001
[ Tue Jul 16 14:48:33 2024 ] 
Training: Epoch [110/150], Step [5999], Loss: 0.5751103162765503, Training Accuracy: 95.77916666666667
[ Tue Jul 16 14:48:34 2024 ] 	Batch(6000/6809) done. Loss: 0.2019  lr:0.000001
[ Tue Jul 16 14:48:56 2024 ] 	Batch(6100/6809) done. Loss: 0.1981  lr:0.000001
[ Tue Jul 16 14:49:19 2024 ] 	Batch(6200/6809) done. Loss: 0.4525  lr:0.000001
[ Tue Jul 16 14:49:41 2024 ] 	Batch(6300/6809) done. Loss: 0.2329  lr:0.000001
[ Tue Jul 16 14:50:04 2024 ] 	Batch(6400/6809) done. Loss: 0.0376  lr:0.000001
[ Tue Jul 16 14:50:26 2024 ] 
Training: Epoch [110/150], Step [6499], Loss: 0.0063549429178237915, Training Accuracy: 95.77884615384616
[ Tue Jul 16 14:50:27 2024 ] 	Batch(6500/6809) done. Loss: 0.1860  lr:0.000001
[ Tue Jul 16 14:50:50 2024 ] 	Batch(6600/6809) done. Loss: 0.1892  lr:0.000001
[ Tue Jul 16 14:51:12 2024 ] 	Batch(6700/6809) done. Loss: 0.2306  lr:0.000001
[ Tue Jul 16 14:51:35 2024 ] 	Batch(6800/6809) done. Loss: 0.0516  lr:0.000001
[ Tue Jul 16 14:51:37 2024 ] 	Mean training loss: 0.1541.
[ Tue Jul 16 14:51:37 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 14:51:37 2024 ] Training epoch: 112
[ Tue Jul 16 14:51:38 2024 ] 	Batch(0/6809) done. Loss: 0.0393  lr:0.000001
[ Tue Jul 16 14:52:01 2024 ] 	Batch(100/6809) done. Loss: 0.3598  lr:0.000001
[ Tue Jul 16 14:52:24 2024 ] 	Batch(200/6809) done. Loss: 0.1940  lr:0.000001
[ Tue Jul 16 14:52:47 2024 ] 	Batch(300/6809) done. Loss: 0.3250  lr:0.000001
[ Tue Jul 16 14:53:11 2024 ] 	Batch(400/6809) done. Loss: 0.0417  lr:0.000001
[ Tue Jul 16 14:53:34 2024 ] 
Training: Epoch [111/150], Step [499], Loss: 0.03102647326886654, Training Accuracy: 95.75
[ Tue Jul 16 14:53:34 2024 ] 	Batch(500/6809) done. Loss: 0.0103  lr:0.000001
[ Tue Jul 16 14:53:57 2024 ] 	Batch(600/6809) done. Loss: 0.1307  lr:0.000001
[ Tue Jul 16 14:54:21 2024 ] 	Batch(700/6809) done. Loss: 0.0697  lr:0.000001
[ Tue Jul 16 14:54:44 2024 ] 	Batch(800/6809) done. Loss: 0.0204  lr:0.000001
[ Tue Jul 16 14:55:07 2024 ] 	Batch(900/6809) done. Loss: 0.0091  lr:0.000001
[ Tue Jul 16 14:55:30 2024 ] 
Training: Epoch [111/150], Step [999], Loss: 0.6945595145225525, Training Accuracy: 95.78750000000001
[ Tue Jul 16 14:55:31 2024 ] 	Batch(1000/6809) done. Loss: 0.0805  lr:0.000001
[ Tue Jul 16 14:55:54 2024 ] 	Batch(1100/6809) done. Loss: 0.0673  lr:0.000001
[ Tue Jul 16 14:56:17 2024 ] 	Batch(1200/6809) done. Loss: 0.1248  lr:0.000001
[ Tue Jul 16 14:56:40 2024 ] 	Batch(1300/6809) done. Loss: 0.0331  lr:0.000001
[ Tue Jul 16 14:57:03 2024 ] 	Batch(1400/6809) done. Loss: 0.0020  lr:0.000001
[ Tue Jul 16 14:57:26 2024 ] 
Training: Epoch [111/150], Step [1499], Loss: 0.05498135834932327, Training Accuracy: 95.775
[ Tue Jul 16 14:57:26 2024 ] 	Batch(1500/6809) done. Loss: 0.0300  lr:0.000001
[ Tue Jul 16 14:57:49 2024 ] 	Batch(1600/6809) done. Loss: 0.2595  lr:0.000001
[ Tue Jul 16 14:58:12 2024 ] 	Batch(1700/6809) done. Loss: 0.0339  lr:0.000001
[ Tue Jul 16 14:58:35 2024 ] 	Batch(1800/6809) done. Loss: 0.2775  lr:0.000001
[ Tue Jul 16 14:58:58 2024 ] 	Batch(1900/6809) done. Loss: 0.2477  lr:0.000001
[ Tue Jul 16 14:59:21 2024 ] 
Training: Epoch [111/150], Step [1999], Loss: 0.008053997531533241, Training Accuracy: 95.80625
[ Tue Jul 16 14:59:21 2024 ] 	Batch(2000/6809) done. Loss: 0.1572  lr:0.000001
[ Tue Jul 16 14:59:44 2024 ] 	Batch(2100/6809) done. Loss: 0.0375  lr:0.000001
[ Tue Jul 16 15:00:06 2024 ] 	Batch(2200/6809) done. Loss: 0.0818  lr:0.000001
[ Tue Jul 16 15:00:29 2024 ] 	Batch(2300/6809) done. Loss: 0.1307  lr:0.000001
[ Tue Jul 16 15:00:52 2024 ] 	Batch(2400/6809) done. Loss: 0.2153  lr:0.000001
[ Tue Jul 16 15:01:14 2024 ] 
Training: Epoch [111/150], Step [2499], Loss: 0.2457619607448578, Training Accuracy: 95.71
[ Tue Jul 16 15:01:14 2024 ] 	Batch(2500/6809) done. Loss: 0.0213  lr:0.000001
[ Tue Jul 16 15:01:37 2024 ] 	Batch(2600/6809) done. Loss: 0.0747  lr:0.000001
[ Tue Jul 16 15:02:00 2024 ] 	Batch(2700/6809) done. Loss: 0.0389  lr:0.000001
[ Tue Jul 16 15:02:22 2024 ] 	Batch(2800/6809) done. Loss: 0.0416  lr:0.000001
[ Tue Jul 16 15:02:46 2024 ] 	Batch(2900/6809) done. Loss: 0.1119  lr:0.000001
[ Tue Jul 16 15:03:09 2024 ] 
Training: Epoch [111/150], Step [2999], Loss: 0.43341249227523804, Training Accuracy: 95.65833333333333
[ Tue Jul 16 15:03:09 2024 ] 	Batch(3000/6809) done. Loss: 0.0020  lr:0.000001
[ Tue Jul 16 15:03:32 2024 ] 	Batch(3100/6809) done. Loss: 1.1141  lr:0.000001
[ Tue Jul 16 15:03:55 2024 ] 	Batch(3200/6809) done. Loss: 0.2175  lr:0.000001
[ Tue Jul 16 15:04:17 2024 ] 	Batch(3300/6809) done. Loss: 0.1182  lr:0.000001
[ Tue Jul 16 15:04:40 2024 ] 	Batch(3400/6809) done. Loss: 0.0857  lr:0.000001
[ Tue Jul 16 15:05:02 2024 ] 
Training: Epoch [111/150], Step [3499], Loss: 0.09893448650836945, Training Accuracy: 95.52857142857142
[ Tue Jul 16 15:05:02 2024 ] 	Batch(3500/6809) done. Loss: 0.0572  lr:0.000001
[ Tue Jul 16 15:05:26 2024 ] 	Batch(3600/6809) done. Loss: 0.0085  lr:0.000001
[ Tue Jul 16 15:05:48 2024 ] 	Batch(3700/6809) done. Loss: 0.1178  lr:0.000001
[ Tue Jul 16 15:06:11 2024 ] 	Batch(3800/6809) done. Loss: 0.2377  lr:0.000001
[ Tue Jul 16 15:06:34 2024 ] 	Batch(3900/6809) done. Loss: 0.0891  lr:0.000001
[ Tue Jul 16 15:06:56 2024 ] 
Training: Epoch [111/150], Step [3999], Loss: 0.22713829576969147, Training Accuracy: 95.6375
[ Tue Jul 16 15:06:56 2024 ] 	Batch(4000/6809) done. Loss: 0.0369  lr:0.000001
[ Tue Jul 16 15:07:19 2024 ] 	Batch(4100/6809) done. Loss: 0.2189  lr:0.000001
[ Tue Jul 16 15:07:42 2024 ] 	Batch(4200/6809) done. Loss: 0.0168  lr:0.000001
[ Tue Jul 16 15:08:05 2024 ] 	Batch(4300/6809) done. Loss: 0.0127  lr:0.000001
[ Tue Jul 16 15:08:28 2024 ] 	Batch(4400/6809) done. Loss: 0.0108  lr:0.000001
[ Tue Jul 16 15:08:50 2024 ] 
Training: Epoch [111/150], Step [4499], Loss: 0.08532062917947769, Training Accuracy: 95.63055555555555
[ Tue Jul 16 15:08:50 2024 ] 	Batch(4500/6809) done. Loss: 0.4145  lr:0.000001
[ Tue Jul 16 15:09:13 2024 ] 	Batch(4600/6809) done. Loss: 0.3322  lr:0.000001
[ Tue Jul 16 15:09:36 2024 ] 	Batch(4700/6809) done. Loss: 0.3410  lr:0.000001
[ Tue Jul 16 15:09:59 2024 ] 	Batch(4800/6809) done. Loss: 0.2529  lr:0.000001
[ Tue Jul 16 15:10:21 2024 ] 	Batch(4900/6809) done. Loss: 0.1672  lr:0.000001
[ Tue Jul 16 15:10:44 2024 ] 
Training: Epoch [111/150], Step [4999], Loss: 0.20649713277816772, Training Accuracy: 95.685
[ Tue Jul 16 15:10:44 2024 ] 	Batch(5000/6809) done. Loss: 0.2563  lr:0.000001
[ Tue Jul 16 15:11:07 2024 ] 	Batch(5100/6809) done. Loss: 0.1121  lr:0.000001
[ Tue Jul 16 15:11:30 2024 ] 	Batch(5200/6809) done. Loss: 0.0059  lr:0.000001
[ Tue Jul 16 15:11:52 2024 ] 	Batch(5300/6809) done. Loss: 0.7602  lr:0.000001
[ Tue Jul 16 15:12:15 2024 ] 	Batch(5400/6809) done. Loss: 0.2667  lr:0.000001
[ Tue Jul 16 15:12:37 2024 ] 
Training: Epoch [111/150], Step [5499], Loss: 0.003089308040216565, Training Accuracy: 95.68409090909091
[ Tue Jul 16 15:12:38 2024 ] 	Batch(5500/6809) done. Loss: 0.0730  lr:0.000001
[ Tue Jul 16 15:13:01 2024 ] 	Batch(5600/6809) done. Loss: 0.0932  lr:0.000001
[ Tue Jul 16 15:13:23 2024 ] 	Batch(5700/6809) done. Loss: 0.0668  lr:0.000001
[ Tue Jul 16 15:13:46 2024 ] 	Batch(5800/6809) done. Loss: 0.4911  lr:0.000001
[ Tue Jul 16 15:14:09 2024 ] 	Batch(5900/6809) done. Loss: 0.5033  lr:0.000001
[ Tue Jul 16 15:14:32 2024 ] 
Training: Epoch [111/150], Step [5999], Loss: 0.4775714576244354, Training Accuracy: 95.68333333333334
[ Tue Jul 16 15:14:32 2024 ] 	Batch(6000/6809) done. Loss: 0.7694  lr:0.000001
[ Tue Jul 16 15:14:55 2024 ] 	Batch(6100/6809) done. Loss: 0.0302  lr:0.000001
[ Tue Jul 16 15:15:18 2024 ] 	Batch(6200/6809) done. Loss: 0.1021  lr:0.000001
[ Tue Jul 16 15:15:42 2024 ] 	Batch(6300/6809) done. Loss: 0.1937  lr:0.000001
[ Tue Jul 16 15:16:06 2024 ] 	Batch(6400/6809) done. Loss: 0.0279  lr:0.000001
[ Tue Jul 16 15:16:28 2024 ] 
Training: Epoch [111/150], Step [6499], Loss: 0.40771961212158203, Training Accuracy: 95.6923076923077
[ Tue Jul 16 15:16:28 2024 ] 	Batch(6500/6809) done. Loss: 0.0704  lr:0.000001
[ Tue Jul 16 15:16:52 2024 ] 	Batch(6600/6809) done. Loss: 0.4228  lr:0.000001
[ Tue Jul 16 15:17:15 2024 ] 	Batch(6700/6809) done. Loss: 0.1670  lr:0.000001
[ Tue Jul 16 15:17:38 2024 ] 	Batch(6800/6809) done. Loss: 0.1411  lr:0.000001
[ Tue Jul 16 15:17:40 2024 ] 	Mean training loss: 0.1567.
[ Tue Jul 16 15:17:40 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 15:17:40 2024 ] Training epoch: 113
[ Tue Jul 16 15:17:41 2024 ] 	Batch(0/6809) done. Loss: 0.0046  lr:0.000001
[ Tue Jul 16 15:18:04 2024 ] 	Batch(100/6809) done. Loss: 0.0472  lr:0.000001
[ Tue Jul 16 15:18:27 2024 ] 	Batch(200/6809) done. Loss: 0.2265  lr:0.000001
[ Tue Jul 16 15:18:50 2024 ] 	Batch(300/6809) done. Loss: 0.7809  lr:0.000001
[ Tue Jul 16 15:19:13 2024 ] 	Batch(400/6809) done. Loss: 0.5997  lr:0.000001
[ Tue Jul 16 15:19:36 2024 ] 
Training: Epoch [112/150], Step [499], Loss: 0.4986743628978729, Training Accuracy: 95.92500000000001
[ Tue Jul 16 15:19:36 2024 ] 	Batch(500/6809) done. Loss: 0.4075  lr:0.000001
[ Tue Jul 16 15:19:59 2024 ] 	Batch(600/6809) done. Loss: 0.2366  lr:0.000001
[ Tue Jul 16 15:20:22 2024 ] 	Batch(700/6809) done. Loss: 0.2713  lr:0.000001
[ Tue Jul 16 15:20:44 2024 ] 	Batch(800/6809) done. Loss: 0.2455  lr:0.000001
[ Tue Jul 16 15:21:07 2024 ] 	Batch(900/6809) done. Loss: 0.0181  lr:0.000001
[ Tue Jul 16 15:21:30 2024 ] 
Training: Epoch [112/150], Step [999], Loss: 0.4335349202156067, Training Accuracy: 95.7
[ Tue Jul 16 15:21:30 2024 ] 	Batch(1000/6809) done. Loss: 0.1295  lr:0.000001
[ Tue Jul 16 15:21:53 2024 ] 	Batch(1100/6809) done. Loss: 0.1949  lr:0.000001
[ Tue Jul 16 15:22:16 2024 ] 	Batch(1200/6809) done. Loss: 0.3332  lr:0.000001
[ Tue Jul 16 15:22:38 2024 ] 	Batch(1300/6809) done. Loss: 0.0947  lr:0.000001
[ Tue Jul 16 15:23:01 2024 ] 	Batch(1400/6809) done. Loss: 0.0227  lr:0.000001
[ Tue Jul 16 15:23:24 2024 ] 
Training: Epoch [112/150], Step [1499], Loss: 0.21109892427921295, Training Accuracy: 95.75833333333334
[ Tue Jul 16 15:23:24 2024 ] 	Batch(1500/6809) done. Loss: 0.1301  lr:0.000001
[ Tue Jul 16 15:23:47 2024 ] 	Batch(1600/6809) done. Loss: 0.0794  lr:0.000001
[ Tue Jul 16 15:24:10 2024 ] 	Batch(1700/6809) done. Loss: 0.1383  lr:0.000001
[ Tue Jul 16 15:24:33 2024 ] 	Batch(1800/6809) done. Loss: 0.3148  lr:0.000001
[ Tue Jul 16 15:24:56 2024 ] 	Batch(1900/6809) done. Loss: 1.0170  lr:0.000001
[ Tue Jul 16 15:25:18 2024 ] 
Training: Epoch [112/150], Step [1999], Loss: 0.229045107960701, Training Accuracy: 95.76249999999999
[ Tue Jul 16 15:25:19 2024 ] 	Batch(2000/6809) done. Loss: 0.1899  lr:0.000001
[ Tue Jul 16 15:25:42 2024 ] 	Batch(2100/6809) done. Loss: 0.0669  lr:0.000001
[ Tue Jul 16 15:26:05 2024 ] 	Batch(2200/6809) done. Loss: 0.1161  lr:0.000001
[ Tue Jul 16 15:26:28 2024 ] 	Batch(2300/6809) done. Loss: 0.0157  lr:0.000001
[ Tue Jul 16 15:26:51 2024 ] 	Batch(2400/6809) done. Loss: 0.0968  lr:0.000001
[ Tue Jul 16 15:27:14 2024 ] 
Training: Epoch [112/150], Step [2499], Loss: 0.09998602420091629, Training Accuracy: 95.755
[ Tue Jul 16 15:27:14 2024 ] 	Batch(2500/6809) done. Loss: 0.2357  lr:0.000001
[ Tue Jul 16 15:27:37 2024 ] 	Batch(2600/6809) done. Loss: 0.1489  lr:0.000001
[ Tue Jul 16 15:27:59 2024 ] 	Batch(2700/6809) done. Loss: 0.0440  lr:0.000001
[ Tue Jul 16 15:28:22 2024 ] 	Batch(2800/6809) done. Loss: 0.0542  lr:0.000001
[ Tue Jul 16 15:28:45 2024 ] 	Batch(2900/6809) done. Loss: 0.3529  lr:0.000001
[ Tue Jul 16 15:29:08 2024 ] 
Training: Epoch [112/150], Step [2999], Loss: 0.015464771538972855, Training Accuracy: 95.72916666666667
[ Tue Jul 16 15:29:08 2024 ] 	Batch(3000/6809) done. Loss: 0.0225  lr:0.000001
[ Tue Jul 16 15:29:31 2024 ] 	Batch(3100/6809) done. Loss: 0.0572  lr:0.000001
[ Tue Jul 16 15:29:54 2024 ] 	Batch(3200/6809) done. Loss: 0.1008  lr:0.000001
[ Tue Jul 16 15:30:17 2024 ] 	Batch(3300/6809) done. Loss: 0.4081  lr:0.000001
[ Tue Jul 16 15:30:40 2024 ] 	Batch(3400/6809) done. Loss: 0.0350  lr:0.000001
[ Tue Jul 16 15:31:02 2024 ] 
Training: Epoch [112/150], Step [3499], Loss: 0.04517829045653343, Training Accuracy: 95.76071428571429
[ Tue Jul 16 15:31:02 2024 ] 	Batch(3500/6809) done. Loss: 0.1449  lr:0.000001
[ Tue Jul 16 15:31:25 2024 ] 	Batch(3600/6809) done. Loss: 0.1218  lr:0.000001
[ Tue Jul 16 15:31:48 2024 ] 	Batch(3700/6809) done. Loss: 0.1153  lr:0.000001
[ Tue Jul 16 15:32:11 2024 ] 	Batch(3800/6809) done. Loss: 0.2444  lr:0.000001
[ Tue Jul 16 15:32:34 2024 ] 	Batch(3900/6809) done. Loss: 0.2161  lr:0.000001
[ Tue Jul 16 15:32:56 2024 ] 
Training: Epoch [112/150], Step [3999], Loss: 0.1296221762895584, Training Accuracy: 95.721875
[ Tue Jul 16 15:32:56 2024 ] 	Batch(4000/6809) done. Loss: 0.0059  lr:0.000001
[ Tue Jul 16 15:33:19 2024 ] 	Batch(4100/6809) done. Loss: 0.0742  lr:0.000001
[ Tue Jul 16 15:33:42 2024 ] 	Batch(4200/6809) done. Loss: 0.1403  lr:0.000001
[ Tue Jul 16 15:34:04 2024 ] 	Batch(4300/6809) done. Loss: 0.2840  lr:0.000001
[ Tue Jul 16 15:34:27 2024 ] 	Batch(4400/6809) done. Loss: 0.0713  lr:0.000001
[ Tue Jul 16 15:34:50 2024 ] 
Training: Epoch [112/150], Step [4499], Loss: 0.2286432981491089, Training Accuracy: 95.75
[ Tue Jul 16 15:34:50 2024 ] 	Batch(4500/6809) done. Loss: 0.1376  lr:0.000001
[ Tue Jul 16 15:35:13 2024 ] 	Batch(4600/6809) done. Loss: 0.1813  lr:0.000001
[ Tue Jul 16 15:35:36 2024 ] 	Batch(4700/6809) done. Loss: 0.1465  lr:0.000001
[ Tue Jul 16 15:35:58 2024 ] 	Batch(4800/6809) done. Loss: 0.1399  lr:0.000001
[ Tue Jul 16 15:36:21 2024 ] 	Batch(4900/6809) done. Loss: 0.0310  lr:0.000001
[ Tue Jul 16 15:36:43 2024 ] 
Training: Epoch [112/150], Step [4999], Loss: 0.23761394619941711, Training Accuracy: 95.77250000000001
[ Tue Jul 16 15:36:44 2024 ] 	Batch(5000/6809) done. Loss: 0.1122  lr:0.000001
[ Tue Jul 16 15:37:06 2024 ] 	Batch(5100/6809) done. Loss: 0.0111  lr:0.000001
[ Tue Jul 16 15:37:29 2024 ] 	Batch(5200/6809) done. Loss: 0.0707  lr:0.000001
[ Tue Jul 16 15:37:52 2024 ] 	Batch(5300/6809) done. Loss: 0.3572  lr:0.000001
[ Tue Jul 16 15:38:15 2024 ] 	Batch(5400/6809) done. Loss: 0.1350  lr:0.000001
[ Tue Jul 16 15:38:37 2024 ] 
Training: Epoch [112/150], Step [5499], Loss: 0.04207085445523262, Training Accuracy: 95.72954545454544
[ Tue Jul 16 15:38:37 2024 ] 	Batch(5500/6809) done. Loss: 0.0961  lr:0.000001
[ Tue Jul 16 15:39:00 2024 ] 	Batch(5600/6809) done. Loss: 0.1567  lr:0.000001
[ Tue Jul 16 15:39:23 2024 ] 	Batch(5700/6809) done. Loss: 0.0937  lr:0.000001
[ Tue Jul 16 15:39:46 2024 ] 	Batch(5800/6809) done. Loss: 0.4099  lr:0.000001
[ Tue Jul 16 15:40:08 2024 ] 	Batch(5900/6809) done. Loss: 0.1471  lr:0.000001
[ Tue Jul 16 15:40:31 2024 ] 
Training: Epoch [112/150], Step [5999], Loss: 0.5504084825515747, Training Accuracy: 95.74375
[ Tue Jul 16 15:40:31 2024 ] 	Batch(6000/6809) done. Loss: 0.0980  lr:0.000001
[ Tue Jul 16 15:40:54 2024 ] 	Batch(6100/6809) done. Loss: 0.1176  lr:0.000001
[ Tue Jul 16 15:41:17 2024 ] 	Batch(6200/6809) done. Loss: 0.0197  lr:0.000001
[ Tue Jul 16 15:41:39 2024 ] 	Batch(6300/6809) done. Loss: 0.0268  lr:0.000001
[ Tue Jul 16 15:42:02 2024 ] 	Batch(6400/6809) done. Loss: 0.7584  lr:0.000001
[ Tue Jul 16 15:42:25 2024 ] 
Training: Epoch [112/150], Step [6499], Loss: 0.04720619320869446, Training Accuracy: 95.71923076923076
[ Tue Jul 16 15:42:25 2024 ] 	Batch(6500/6809) done. Loss: 0.0276  lr:0.000001
[ Tue Jul 16 15:42:47 2024 ] 	Batch(6600/6809) done. Loss: 0.0990  lr:0.000001
[ Tue Jul 16 15:43:10 2024 ] 	Batch(6700/6809) done. Loss: 0.2840  lr:0.000001
[ Tue Jul 16 15:43:33 2024 ] 	Batch(6800/6809) done. Loss: 0.1566  lr:0.000001
[ Tue Jul 16 15:43:35 2024 ] 	Mean training loss: 0.1520.
[ Tue Jul 16 15:43:35 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 15:43:35 2024 ] Training epoch: 114
[ Tue Jul 16 15:43:36 2024 ] 	Batch(0/6809) done. Loss: 0.2386  lr:0.000001
[ Tue Jul 16 15:43:58 2024 ] 	Batch(100/6809) done. Loss: 0.0643  lr:0.000001
[ Tue Jul 16 15:44:21 2024 ] 	Batch(200/6809) done. Loss: 0.0345  lr:0.000001
[ Tue Jul 16 15:44:43 2024 ] 	Batch(300/6809) done. Loss: 0.1340  lr:0.000001
[ Tue Jul 16 15:45:06 2024 ] 	Batch(400/6809) done. Loss: 0.1989  lr:0.000001
[ Tue Jul 16 15:45:29 2024 ] 
Training: Epoch [113/150], Step [499], Loss: 0.04121428728103638, Training Accuracy: 95.55
[ Tue Jul 16 15:45:29 2024 ] 	Batch(500/6809) done. Loss: 0.0036  lr:0.000001
[ Tue Jul 16 15:45:52 2024 ] 	Batch(600/6809) done. Loss: 0.0681  lr:0.000001
[ Tue Jul 16 15:46:15 2024 ] 	Batch(700/6809) done. Loss: 0.2057  lr:0.000001
[ Tue Jul 16 15:46:38 2024 ] 	Batch(800/6809) done. Loss: 0.0336  lr:0.000001
[ Tue Jul 16 15:47:01 2024 ] 	Batch(900/6809) done. Loss: 0.0846  lr:0.000001
[ Tue Jul 16 15:47:24 2024 ] 
Training: Epoch [113/150], Step [999], Loss: 0.1199386715888977, Training Accuracy: 95.6875
[ Tue Jul 16 15:47:24 2024 ] 	Batch(1000/6809) done. Loss: 0.0719  lr:0.000001
[ Tue Jul 16 15:47:48 2024 ] 	Batch(1100/6809) done. Loss: 0.1102  lr:0.000001
[ Tue Jul 16 15:48:11 2024 ] 	Batch(1200/6809) done. Loss: 0.0141  lr:0.000001
[ Tue Jul 16 15:48:35 2024 ] 	Batch(1300/6809) done. Loss: 0.0066  lr:0.000001
[ Tue Jul 16 15:48:58 2024 ] 	Batch(1400/6809) done. Loss: 0.1347  lr:0.000001
[ Tue Jul 16 15:49:21 2024 ] 
Training: Epoch [113/150], Step [1499], Loss: 0.17678241431713104, Training Accuracy: 95.79166666666666
[ Tue Jul 16 15:49:21 2024 ] 	Batch(1500/6809) done. Loss: 0.0226  lr:0.000001
[ Tue Jul 16 15:49:44 2024 ] 	Batch(1600/6809) done. Loss: 0.0104  lr:0.000001
[ Tue Jul 16 15:50:08 2024 ] 	Batch(1700/6809) done. Loss: 0.0558  lr:0.000001
[ Tue Jul 16 15:50:32 2024 ] 	Batch(1800/6809) done. Loss: 0.0651  lr:0.000001
[ Tue Jul 16 15:50:56 2024 ] 	Batch(1900/6809) done. Loss: 0.0635  lr:0.000001
[ Tue Jul 16 15:51:19 2024 ] 
Training: Epoch [113/150], Step [1999], Loss: 0.005959529895335436, Training Accuracy: 95.78125
[ Tue Jul 16 15:51:19 2024 ] 	Batch(2000/6809) done. Loss: 0.8731  lr:0.000001
[ Tue Jul 16 15:51:42 2024 ] 	Batch(2100/6809) done. Loss: 0.0032  lr:0.000001
[ Tue Jul 16 15:52:05 2024 ] 	Batch(2200/6809) done. Loss: 0.0097  lr:0.000001
[ Tue Jul 16 15:52:29 2024 ] 	Batch(2300/6809) done. Loss: 0.3844  lr:0.000001
[ Tue Jul 16 15:52:52 2024 ] 	Batch(2400/6809) done. Loss: 0.2419  lr:0.000001
[ Tue Jul 16 15:53:15 2024 ] 
Training: Epoch [113/150], Step [2499], Loss: 0.6077336668968201, Training Accuracy: 95.77
[ Tue Jul 16 15:53:15 2024 ] 	Batch(2500/6809) done. Loss: 0.2186  lr:0.000001
[ Tue Jul 16 15:53:38 2024 ] 	Batch(2600/6809) done. Loss: 0.1271  lr:0.000001
[ Tue Jul 16 15:54:01 2024 ] 	Batch(2700/6809) done. Loss: 0.0404  lr:0.000001
[ Tue Jul 16 15:54:24 2024 ] 	Batch(2800/6809) done. Loss: 0.0378  lr:0.000001
[ Tue Jul 16 15:54:48 2024 ] 	Batch(2900/6809) done. Loss: 0.1653  lr:0.000001
[ Tue Jul 16 15:55:10 2024 ] 
Training: Epoch [113/150], Step [2999], Loss: 0.14535021781921387, Training Accuracy: 95.77916666666667
[ Tue Jul 16 15:55:11 2024 ] 	Batch(3000/6809) done. Loss: 0.0244  lr:0.000001
[ Tue Jul 16 15:55:34 2024 ] 	Batch(3100/6809) done. Loss: 0.1901  lr:0.000001
[ Tue Jul 16 15:55:57 2024 ] 	Batch(3200/6809) done. Loss: 0.0933  lr:0.000001
[ Tue Jul 16 15:56:20 2024 ] 	Batch(3300/6809) done. Loss: 0.0504  lr:0.000001
[ Tue Jul 16 15:56:43 2024 ] 	Batch(3400/6809) done. Loss: 0.3701  lr:0.000001
[ Tue Jul 16 15:57:05 2024 ] 
Training: Epoch [113/150], Step [3499], Loss: 0.05653585121035576, Training Accuracy: 95.71785714285714
[ Tue Jul 16 15:57:06 2024 ] 	Batch(3500/6809) done. Loss: 0.0097  lr:0.000001
[ Tue Jul 16 15:57:29 2024 ] 	Batch(3600/6809) done. Loss: 0.0119  lr:0.000001
[ Tue Jul 16 15:57:52 2024 ] 	Batch(3700/6809) done. Loss: 0.1339  lr:0.000001
[ Tue Jul 16 15:58:15 2024 ] 	Batch(3800/6809) done. Loss: 0.2372  lr:0.000001
[ Tue Jul 16 15:58:39 2024 ] 	Batch(3900/6809) done. Loss: 0.1149  lr:0.000001
[ Tue Jul 16 15:59:02 2024 ] 
Training: Epoch [113/150], Step [3999], Loss: 0.004836270119994879, Training Accuracy: 95.759375
[ Tue Jul 16 15:59:02 2024 ] 	Batch(4000/6809) done. Loss: 0.0461  lr:0.000001
[ Tue Jul 16 15:59:25 2024 ] 	Batch(4100/6809) done. Loss: 0.2312  lr:0.000001
[ Tue Jul 16 15:59:48 2024 ] 	Batch(4200/6809) done. Loss: 0.0407  lr:0.000001
[ Tue Jul 16 16:00:11 2024 ] 	Batch(4300/6809) done. Loss: 0.2802  lr:0.000001
[ Tue Jul 16 16:00:34 2024 ] 	Batch(4400/6809) done. Loss: 0.3451  lr:0.000001
[ Tue Jul 16 16:00:57 2024 ] 
Training: Epoch [113/150], Step [4499], Loss: 0.02140969969332218, Training Accuracy: 95.75833333333334
[ Tue Jul 16 16:00:57 2024 ] 	Batch(4500/6809) done. Loss: 0.0036  lr:0.000001
[ Tue Jul 16 16:01:20 2024 ] 	Batch(4600/6809) done. Loss: 0.1844  lr:0.000001
[ Tue Jul 16 16:01:43 2024 ] 	Batch(4700/6809) done. Loss: 0.3287  lr:0.000001
[ Tue Jul 16 16:02:06 2024 ] 	Batch(4800/6809) done. Loss: 0.1160  lr:0.000001
[ Tue Jul 16 16:02:29 2024 ] 	Batch(4900/6809) done. Loss: 0.0371  lr:0.000001
[ Tue Jul 16 16:02:52 2024 ] 
Training: Epoch [113/150], Step [4999], Loss: 0.08747214078903198, Training Accuracy: 95.735
[ Tue Jul 16 16:02:52 2024 ] 	Batch(5000/6809) done. Loss: 0.0177  lr:0.000001
[ Tue Jul 16 16:03:15 2024 ] 	Batch(5100/6809) done. Loss: 0.0190  lr:0.000001
[ Tue Jul 16 16:03:38 2024 ] 	Batch(5200/6809) done. Loss: 0.6584  lr:0.000001
[ Tue Jul 16 16:04:01 2024 ] 	Batch(5300/6809) done. Loss: 0.0005  lr:0.000001
[ Tue Jul 16 16:04:24 2024 ] 	Batch(5400/6809) done. Loss: 0.5048  lr:0.000001
[ Tue Jul 16 16:04:47 2024 ] 
Training: Epoch [113/150], Step [5499], Loss: 0.9660428762435913, Training Accuracy: 95.73863636363636
[ Tue Jul 16 16:04:47 2024 ] 	Batch(5500/6809) done. Loss: 0.3197  lr:0.000001
[ Tue Jul 16 16:05:10 2024 ] 	Batch(5600/6809) done. Loss: 0.2506  lr:0.000001
[ Tue Jul 16 16:05:33 2024 ] 	Batch(5700/6809) done. Loss: 0.2089  lr:0.000001
[ Tue Jul 16 16:05:55 2024 ] 	Batch(5800/6809) done. Loss: 0.0152  lr:0.000001
[ Tue Jul 16 16:06:18 2024 ] 	Batch(5900/6809) done. Loss: 0.1135  lr:0.000001
[ Tue Jul 16 16:06:41 2024 ] 
Training: Epoch [113/150], Step [5999], Loss: 0.24468287825584412, Training Accuracy: 95.75
[ Tue Jul 16 16:06:42 2024 ] 	Batch(6000/6809) done. Loss: 0.0089  lr:0.000001
[ Tue Jul 16 16:07:05 2024 ] 	Batch(6100/6809) done. Loss: 0.0055  lr:0.000001
[ Tue Jul 16 16:07:28 2024 ] 	Batch(6200/6809) done. Loss: 0.1198  lr:0.000001
[ Tue Jul 16 16:07:51 2024 ] 	Batch(6300/6809) done. Loss: 0.1598  lr:0.000001
[ Tue Jul 16 16:08:14 2024 ] 	Batch(6400/6809) done. Loss: 0.0085  lr:0.000001
[ Tue Jul 16 16:08:36 2024 ] 
Training: Epoch [113/150], Step [6499], Loss: 0.04245875030755997, Training Accuracy: 95.775
[ Tue Jul 16 16:08:37 2024 ] 	Batch(6500/6809) done. Loss: 0.0610  lr:0.000001
[ Tue Jul 16 16:09:00 2024 ] 	Batch(6600/6809) done. Loss: 0.3060  lr:0.000001
[ Tue Jul 16 16:09:22 2024 ] 	Batch(6700/6809) done. Loss: 0.0210  lr:0.000001
[ Tue Jul 16 16:09:46 2024 ] 	Batch(6800/6809) done. Loss: 0.3712  lr:0.000001
[ Tue Jul 16 16:09:48 2024 ] 	Mean training loss: 0.1546.
[ Tue Jul 16 16:09:48 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 16:09:48 2024 ] Training epoch: 115
[ Tue Jul 16 16:09:48 2024 ] 	Batch(0/6809) done. Loss: 0.2052  lr:0.000001
[ Tue Jul 16 16:10:11 2024 ] 	Batch(100/6809) done. Loss: 0.1208  lr:0.000001
[ Tue Jul 16 16:10:34 2024 ] 	Batch(200/6809) done. Loss: 0.1428  lr:0.000001
[ Tue Jul 16 16:10:57 2024 ] 	Batch(300/6809) done. Loss: 0.0432  lr:0.000001
[ Tue Jul 16 16:11:21 2024 ] 	Batch(400/6809) done. Loss: 0.0572  lr:0.000001
[ Tue Jul 16 16:11:44 2024 ] 
Training: Epoch [114/150], Step [499], Loss: 0.16366063058376312, Training Accuracy: 95.95
[ Tue Jul 16 16:11:44 2024 ] 	Batch(500/6809) done. Loss: 0.0229  lr:0.000001
[ Tue Jul 16 16:12:07 2024 ] 	Batch(600/6809) done. Loss: 0.0177  lr:0.000001
[ Tue Jul 16 16:12:29 2024 ] 	Batch(700/6809) done. Loss: 0.1704  lr:0.000001
[ Tue Jul 16 16:12:52 2024 ] 	Batch(800/6809) done. Loss: 0.2375  lr:0.000001
[ Tue Jul 16 16:13:15 2024 ] 	Batch(900/6809) done. Loss: 0.0652  lr:0.000001
[ Tue Jul 16 16:13:37 2024 ] 
Training: Epoch [114/150], Step [999], Loss: 0.25015464425086975, Training Accuracy: 95.7125
[ Tue Jul 16 16:13:38 2024 ] 	Batch(1000/6809) done. Loss: 0.0245  lr:0.000001
[ Tue Jul 16 16:14:00 2024 ] 	Batch(1100/6809) done. Loss: 0.0167  lr:0.000001
[ Tue Jul 16 16:14:23 2024 ] 	Batch(1200/6809) done. Loss: 0.0024  lr:0.000001
[ Tue Jul 16 16:14:46 2024 ] 	Batch(1300/6809) done. Loss: 0.1319  lr:0.000001
[ Tue Jul 16 16:15:09 2024 ] 	Batch(1400/6809) done. Loss: 0.1792  lr:0.000001
[ Tue Jul 16 16:15:31 2024 ] 
Training: Epoch [114/150], Step [1499], Loss: 0.0061103468760848045, Training Accuracy: 95.61666666666667
[ Tue Jul 16 16:15:32 2024 ] 	Batch(1500/6809) done. Loss: 0.1731  lr:0.000001
[ Tue Jul 16 16:15:54 2024 ] 	Batch(1600/6809) done. Loss: 0.0969  lr:0.000001
[ Tue Jul 16 16:16:17 2024 ] 	Batch(1700/6809) done. Loss: 0.0188  lr:0.000001
[ Tue Jul 16 16:16:40 2024 ] 	Batch(1800/6809) done. Loss: 0.2683  lr:0.000001
[ Tue Jul 16 16:17:02 2024 ] 	Batch(1900/6809) done. Loss: 0.0631  lr:0.000001
[ Tue Jul 16 16:17:25 2024 ] 
Training: Epoch [114/150], Step [1999], Loss: 0.08408760279417038, Training Accuracy: 95.6625
[ Tue Jul 16 16:17:25 2024 ] 	Batch(2000/6809) done. Loss: 0.0108  lr:0.000001
[ Tue Jul 16 16:17:48 2024 ] 	Batch(2100/6809) done. Loss: 0.0064  lr:0.000001
[ Tue Jul 16 16:18:11 2024 ] 	Batch(2200/6809) done. Loss: 0.1341  lr:0.000001
[ Tue Jul 16 16:18:33 2024 ] 	Batch(2300/6809) done. Loss: 0.0263  lr:0.000001
[ Tue Jul 16 16:18:56 2024 ] 	Batch(2400/6809) done. Loss: 0.0500  lr:0.000001
[ Tue Jul 16 16:19:19 2024 ] 
Training: Epoch [114/150], Step [2499], Loss: 0.3050374686717987, Training Accuracy: 95.645
[ Tue Jul 16 16:19:19 2024 ] 	Batch(2500/6809) done. Loss: 0.0210  lr:0.000001
[ Tue Jul 16 16:19:41 2024 ] 	Batch(2600/6809) done. Loss: 0.0796  lr:0.000001
[ Tue Jul 16 16:20:04 2024 ] 	Batch(2700/6809) done. Loss: 0.2168  lr:0.000001
[ Tue Jul 16 16:20:27 2024 ] 	Batch(2800/6809) done. Loss: 0.1390  lr:0.000001
[ Tue Jul 16 16:20:50 2024 ] 	Batch(2900/6809) done. Loss: 0.1059  lr:0.000001
[ Tue Jul 16 16:21:12 2024 ] 
Training: Epoch [114/150], Step [2999], Loss: 0.6042181253433228, Training Accuracy: 95.76249999999999
[ Tue Jul 16 16:21:12 2024 ] 	Batch(3000/6809) done. Loss: 0.0048  lr:0.000001
[ Tue Jul 16 16:21:35 2024 ] 	Batch(3100/6809) done. Loss: 0.1528  lr:0.000001
[ Tue Jul 16 16:21:58 2024 ] 	Batch(3200/6809) done. Loss: 0.5071  lr:0.000001
[ Tue Jul 16 16:22:21 2024 ] 	Batch(3300/6809) done. Loss: 0.0111  lr:0.000001
[ Tue Jul 16 16:22:43 2024 ] 	Batch(3400/6809) done. Loss: 0.0351  lr:0.000001
[ Tue Jul 16 16:23:06 2024 ] 
Training: Epoch [114/150], Step [3499], Loss: 0.2400684356689453, Training Accuracy: 95.81071428571428
[ Tue Jul 16 16:23:06 2024 ] 	Batch(3500/6809) done. Loss: 0.3179  lr:0.000001
[ Tue Jul 16 16:23:29 2024 ] 	Batch(3600/6809) done. Loss: 0.8645  lr:0.000001
[ Tue Jul 16 16:23:51 2024 ] 	Batch(3700/6809) done. Loss: 0.0028  lr:0.000001
[ Tue Jul 16 16:24:14 2024 ] 	Batch(3800/6809) done. Loss: 0.0528  lr:0.000001
[ Tue Jul 16 16:24:37 2024 ] 	Batch(3900/6809) done. Loss: 0.0936  lr:0.000001
[ Tue Jul 16 16:24:59 2024 ] 
Training: Epoch [114/150], Step [3999], Loss: 0.0326416902244091, Training Accuracy: 95.740625
[ Tue Jul 16 16:25:00 2024 ] 	Batch(4000/6809) done. Loss: 0.0356  lr:0.000001
[ Tue Jul 16 16:25:22 2024 ] 	Batch(4100/6809) done. Loss: 0.3870  lr:0.000001
[ Tue Jul 16 16:25:45 2024 ] 	Batch(4200/6809) done. Loss: 0.0043  lr:0.000001
[ Tue Jul 16 16:26:08 2024 ] 	Batch(4300/6809) done. Loss: 0.0727  lr:0.000001
[ Tue Jul 16 16:26:31 2024 ] 	Batch(4400/6809) done. Loss: 0.1493  lr:0.000001
[ Tue Jul 16 16:26:53 2024 ] 
Training: Epoch [114/150], Step [4499], Loss: 0.46536120772361755, Training Accuracy: 95.72222222222221
[ Tue Jul 16 16:26:53 2024 ] 	Batch(4500/6809) done. Loss: 0.1421  lr:0.000001
[ Tue Jul 16 16:27:16 2024 ] 	Batch(4600/6809) done. Loss: 0.0902  lr:0.000001
[ Tue Jul 16 16:27:39 2024 ] 	Batch(4700/6809) done. Loss: 0.0453  lr:0.000001
[ Tue Jul 16 16:28:02 2024 ] 	Batch(4800/6809) done. Loss: 0.0260  lr:0.000001
[ Tue Jul 16 16:28:25 2024 ] 	Batch(4900/6809) done. Loss: 0.0113  lr:0.000001
[ Tue Jul 16 16:28:47 2024 ] 
Training: Epoch [114/150], Step [4999], Loss: 0.19193218648433685, Training Accuracy: 95.74249999999999
[ Tue Jul 16 16:28:47 2024 ] 	Batch(5000/6809) done. Loss: 0.4034  lr:0.000001
[ Tue Jul 16 16:29:10 2024 ] 	Batch(5100/6809) done. Loss: 0.2129  lr:0.000001
[ Tue Jul 16 16:29:33 2024 ] 	Batch(5200/6809) done. Loss: 0.4859  lr:0.000001
[ Tue Jul 16 16:29:55 2024 ] 	Batch(5300/6809) done. Loss: 0.0094  lr:0.000001
[ Tue Jul 16 16:30:18 2024 ] 	Batch(5400/6809) done. Loss: 0.1159  lr:0.000001
[ Tue Jul 16 16:30:40 2024 ] 
Training: Epoch [114/150], Step [5499], Loss: 0.5219814777374268, Training Accuracy: 95.69545454545455
[ Tue Jul 16 16:30:40 2024 ] 	Batch(5500/6809) done. Loss: 0.0692  lr:0.000001
[ Tue Jul 16 16:31:03 2024 ] 	Batch(5600/6809) done. Loss: 0.1695  lr:0.000001
[ Tue Jul 16 16:31:26 2024 ] 	Batch(5700/6809) done. Loss: 0.2047  lr:0.000001
[ Tue Jul 16 16:31:49 2024 ] 	Batch(5800/6809) done. Loss: 0.0079  lr:0.000001
[ Tue Jul 16 16:32:11 2024 ] 	Batch(5900/6809) done. Loss: 0.0878  lr:0.000001
[ Tue Jul 16 16:32:34 2024 ] 
Training: Epoch [114/150], Step [5999], Loss: 0.05513531342148781, Training Accuracy: 95.74583333333334
[ Tue Jul 16 16:32:34 2024 ] 	Batch(6000/6809) done. Loss: 0.1034  lr:0.000001
[ Tue Jul 16 16:32:57 2024 ] 	Batch(6100/6809) done. Loss: 0.0246  lr:0.000001
[ Tue Jul 16 16:33:20 2024 ] 	Batch(6200/6809) done. Loss: 0.4614  lr:0.000001
[ Tue Jul 16 16:33:42 2024 ] 	Batch(6300/6809) done. Loss: 0.0225  lr:0.000001
[ Tue Jul 16 16:34:05 2024 ] 	Batch(6400/6809) done. Loss: 0.1183  lr:0.000001
[ Tue Jul 16 16:34:27 2024 ] 
Training: Epoch [114/150], Step [6499], Loss: 0.03662296012043953, Training Accuracy: 95.71923076923076
[ Tue Jul 16 16:34:28 2024 ] 	Batch(6500/6809) done. Loss: 0.2085  lr:0.000001
[ Tue Jul 16 16:34:50 2024 ] 	Batch(6600/6809) done. Loss: 0.0070  lr:0.000001
[ Tue Jul 16 16:35:13 2024 ] 	Batch(6700/6809) done. Loss: 0.1965  lr:0.000001
[ Tue Jul 16 16:35:36 2024 ] 	Batch(6800/6809) done. Loss: 0.2390  lr:0.000001
[ Tue Jul 16 16:35:38 2024 ] 	Mean training loss: 0.1517.
[ Tue Jul 16 16:35:38 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 16:35:38 2024 ] Training epoch: 116
[ Tue Jul 16 16:35:39 2024 ] 	Batch(0/6809) done. Loss: 0.0211  lr:0.000001
[ Tue Jul 16 16:36:02 2024 ] 	Batch(100/6809) done. Loss: 0.2154  lr:0.000001
[ Tue Jul 16 16:36:25 2024 ] 	Batch(200/6809) done. Loss: 0.0838  lr:0.000001
[ Tue Jul 16 16:36:48 2024 ] 	Batch(300/6809) done. Loss: 0.2657  lr:0.000001
[ Tue Jul 16 16:37:10 2024 ] 	Batch(400/6809) done. Loss: 0.2412  lr:0.000001
[ Tue Jul 16 16:37:33 2024 ] 
Training: Epoch [115/150], Step [499], Loss: 0.14974874258041382, Training Accuracy: 95.525
[ Tue Jul 16 16:37:33 2024 ] 	Batch(500/6809) done. Loss: 0.1065  lr:0.000001
[ Tue Jul 16 16:37:55 2024 ] 	Batch(600/6809) done. Loss: 0.0792  lr:0.000001
[ Tue Jul 16 16:38:18 2024 ] 	Batch(700/6809) done. Loss: 0.0495  lr:0.000001
[ Tue Jul 16 16:38:41 2024 ] 	Batch(800/6809) done. Loss: 0.0986  lr:0.000001
[ Tue Jul 16 16:39:03 2024 ] 	Batch(900/6809) done. Loss: 0.2218  lr:0.000001
[ Tue Jul 16 16:39:25 2024 ] 
Training: Epoch [115/150], Step [999], Loss: 0.018415305763483047, Training Accuracy: 95.6375
[ Tue Jul 16 16:39:26 2024 ] 	Batch(1000/6809) done. Loss: 0.0766  lr:0.000001
[ Tue Jul 16 16:39:48 2024 ] 	Batch(1100/6809) done. Loss: 0.0837  lr:0.000001
[ Tue Jul 16 16:40:11 2024 ] 	Batch(1200/6809) done. Loss: 0.1071  lr:0.000001
[ Tue Jul 16 16:40:33 2024 ] 	Batch(1300/6809) done. Loss: 0.3573  lr:0.000001
[ Tue Jul 16 16:40:56 2024 ] 	Batch(1400/6809) done. Loss: 0.0267  lr:0.000001
[ Tue Jul 16 16:41:18 2024 ] 
Training: Epoch [115/150], Step [1499], Loss: 0.143803209066391, Training Accuracy: 95.75
[ Tue Jul 16 16:41:19 2024 ] 	Batch(1500/6809) done. Loss: 0.1594  lr:0.000001
[ Tue Jul 16 16:41:41 2024 ] 	Batch(1600/6809) done. Loss: 0.4472  lr:0.000001
[ Tue Jul 16 16:42:04 2024 ] 	Batch(1700/6809) done. Loss: 0.0377  lr:0.000001
[ Tue Jul 16 16:42:27 2024 ] 	Batch(1800/6809) done. Loss: 0.3713  lr:0.000001
[ Tue Jul 16 16:42:49 2024 ] 	Batch(1900/6809) done. Loss: 0.0543  lr:0.000001
[ Tue Jul 16 16:43:12 2024 ] 
Training: Epoch [115/150], Step [1999], Loss: 0.04792880266904831, Training Accuracy: 95.79375
[ Tue Jul 16 16:43:12 2024 ] 	Batch(2000/6809) done. Loss: 0.2595  lr:0.000001
[ Tue Jul 16 16:43:34 2024 ] 	Batch(2100/6809) done. Loss: 0.0611  lr:0.000001
[ Tue Jul 16 16:43:57 2024 ] 	Batch(2200/6809) done. Loss: 0.4271  lr:0.000001
[ Tue Jul 16 16:44:19 2024 ] 	Batch(2300/6809) done. Loss: 0.0197  lr:0.000001
[ Tue Jul 16 16:44:42 2024 ] 	Batch(2400/6809) done. Loss: 0.1221  lr:0.000001
[ Tue Jul 16 16:45:04 2024 ] 
Training: Epoch [115/150], Step [2499], Loss: 0.13888028264045715, Training Accuracy: 95.77
[ Tue Jul 16 16:45:05 2024 ] 	Batch(2500/6809) done. Loss: 0.0713  lr:0.000001
[ Tue Jul 16 16:45:27 2024 ] 	Batch(2600/6809) done. Loss: 0.0504  lr:0.000001
[ Tue Jul 16 16:45:50 2024 ] 	Batch(2700/6809) done. Loss: 0.0475  lr:0.000001
[ Tue Jul 16 16:46:13 2024 ] 	Batch(2800/6809) done. Loss: 0.4737  lr:0.000001
[ Tue Jul 16 16:46:36 2024 ] 	Batch(2900/6809) done. Loss: 0.1036  lr:0.000001
[ Tue Jul 16 16:46:59 2024 ] 
Training: Epoch [115/150], Step [2999], Loss: 0.07972858846187592, Training Accuracy: 95.83333333333334
[ Tue Jul 16 16:46:59 2024 ] 	Batch(3000/6809) done. Loss: 0.0747  lr:0.000001
[ Tue Jul 16 16:47:22 2024 ] 	Batch(3100/6809) done. Loss: 0.2969  lr:0.000001
[ Tue Jul 16 16:47:45 2024 ] 	Batch(3200/6809) done. Loss: 0.0157  lr:0.000001
[ Tue Jul 16 16:48:07 2024 ] 	Batch(3300/6809) done. Loss: 0.0423  lr:0.000001
[ Tue Jul 16 16:48:30 2024 ] 	Batch(3400/6809) done. Loss: 0.0161  lr:0.000001
[ Tue Jul 16 16:48:52 2024 ] 
Training: Epoch [115/150], Step [3499], Loss: 0.43590840697288513, Training Accuracy: 95.83928571428572
[ Tue Jul 16 16:48:52 2024 ] 	Batch(3500/6809) done. Loss: 0.2492  lr:0.000001
[ Tue Jul 16 16:49:15 2024 ] 	Batch(3600/6809) done. Loss: 0.0784  lr:0.000001
[ Tue Jul 16 16:49:38 2024 ] 	Batch(3700/6809) done. Loss: 0.1252  lr:0.000001
[ Tue Jul 16 16:50:01 2024 ] 	Batch(3800/6809) done. Loss: 0.1247  lr:0.000001
[ Tue Jul 16 16:50:24 2024 ] 	Batch(3900/6809) done. Loss: 0.2267  lr:0.000001
[ Tue Jul 16 16:50:47 2024 ] 
Training: Epoch [115/150], Step [3999], Loss: 0.00926409475505352, Training Accuracy: 95.78750000000001
[ Tue Jul 16 16:50:47 2024 ] 	Batch(4000/6809) done. Loss: 0.0278  lr:0.000001
[ Tue Jul 16 16:51:10 2024 ] 	Batch(4100/6809) done. Loss: 0.1621  lr:0.000001
[ Tue Jul 16 16:51:33 2024 ] 	Batch(4200/6809) done. Loss: 0.1043  lr:0.000001
[ Tue Jul 16 16:51:56 2024 ] 	Batch(4300/6809) done. Loss: 0.0110  lr:0.000001
[ Tue Jul 16 16:52:19 2024 ] 	Batch(4400/6809) done. Loss: 0.0360  lr:0.000001
[ Tue Jul 16 16:52:42 2024 ] 
Training: Epoch [115/150], Step [4499], Loss: 0.014874939806759357, Training Accuracy: 95.78055555555555
[ Tue Jul 16 16:52:42 2024 ] 	Batch(4500/6809) done. Loss: 0.0244  lr:0.000001
[ Tue Jul 16 16:53:05 2024 ] 	Batch(4600/6809) done. Loss: 0.0979  lr:0.000001
[ Tue Jul 16 16:53:27 2024 ] 	Batch(4700/6809) done. Loss: 0.1280  lr:0.000001
[ Tue Jul 16 16:53:50 2024 ] 	Batch(4800/6809) done. Loss: 0.0561  lr:0.000001
[ Tue Jul 16 16:54:13 2024 ] 	Batch(4900/6809) done. Loss: 0.0236  lr:0.000001
[ Tue Jul 16 16:54:35 2024 ] 
Training: Epoch [115/150], Step [4999], Loss: 0.04021935537457466, Training Accuracy: 95.77250000000001
[ Tue Jul 16 16:54:35 2024 ] 	Batch(5000/6809) done. Loss: 0.2949  lr:0.000001
[ Tue Jul 16 16:54:58 2024 ] 	Batch(5100/6809) done. Loss: 0.2736  lr:0.000001
[ Tue Jul 16 16:55:20 2024 ] 	Batch(5200/6809) done. Loss: 0.3977  lr:0.000001
[ Tue Jul 16 16:55:43 2024 ] 	Batch(5300/6809) done. Loss: 0.2049  lr:0.000001
[ Tue Jul 16 16:56:06 2024 ] 	Batch(5400/6809) done. Loss: 0.2198  lr:0.000001
[ Tue Jul 16 16:56:28 2024 ] 
Training: Epoch [115/150], Step [5499], Loss: 0.23493674397468567, Training Accuracy: 95.82045454545455
[ Tue Jul 16 16:56:28 2024 ] 	Batch(5500/6809) done. Loss: 0.0355  lr:0.000001
[ Tue Jul 16 16:56:51 2024 ] 	Batch(5600/6809) done. Loss: 0.1222  lr:0.000001
[ Tue Jul 16 16:57:14 2024 ] 	Batch(5700/6809) done. Loss: 0.6273  lr:0.000001
[ Tue Jul 16 16:57:36 2024 ] 	Batch(5800/6809) done. Loss: 0.3382  lr:0.000001
[ Tue Jul 16 16:57:59 2024 ] 	Batch(5900/6809) done. Loss: 0.0458  lr:0.000001
[ Tue Jul 16 16:58:21 2024 ] 
Training: Epoch [115/150], Step [5999], Loss: 0.10514751821756363, Training Accuracy: 95.82916666666667
[ Tue Jul 16 16:58:21 2024 ] 	Batch(6000/6809) done. Loss: 0.7246  lr:0.000001
[ Tue Jul 16 16:58:44 2024 ] 	Batch(6100/6809) done. Loss: 0.1132  lr:0.000001
[ Tue Jul 16 16:59:07 2024 ] 	Batch(6200/6809) done. Loss: 0.0497  lr:0.000001
[ Tue Jul 16 16:59:29 2024 ] 	Batch(6300/6809) done. Loss: 0.0095  lr:0.000001
[ Tue Jul 16 16:59:52 2024 ] 	Batch(6400/6809) done. Loss: 0.0390  lr:0.000001
[ Tue Jul 16 17:00:14 2024 ] 
Training: Epoch [115/150], Step [6499], Loss: 0.1454845368862152, Training Accuracy: 95.79807692307692
[ Tue Jul 16 17:00:15 2024 ] 	Batch(6500/6809) done. Loss: 0.4051  lr:0.000001
[ Tue Jul 16 17:00:37 2024 ] 	Batch(6600/6809) done. Loss: 0.2176  lr:0.000001
[ Tue Jul 16 17:01:00 2024 ] 	Batch(6700/6809) done. Loss: 0.1459  lr:0.000001
[ Tue Jul 16 17:01:22 2024 ] 	Batch(6800/6809) done. Loss: 0.4723  lr:0.000001
[ Tue Jul 16 17:01:24 2024 ] 	Mean training loss: 0.1554.
[ Tue Jul 16 17:01:24 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 17:01:24 2024 ] Training epoch: 117
[ Tue Jul 16 17:01:25 2024 ] 	Batch(0/6809) done. Loss: 0.8859  lr:0.000001
[ Tue Jul 16 17:01:48 2024 ] 	Batch(100/6809) done. Loss: 0.1162  lr:0.000001
[ Tue Jul 16 17:02:10 2024 ] 	Batch(200/6809) done. Loss: 0.0724  lr:0.000001
[ Tue Jul 16 17:02:33 2024 ] 	Batch(300/6809) done. Loss: 0.1693  lr:0.000001
[ Tue Jul 16 17:02:56 2024 ] 	Batch(400/6809) done. Loss: 0.0162  lr:0.000001
[ Tue Jul 16 17:03:18 2024 ] 
Training: Epoch [116/150], Step [499], Loss: 0.0596705786883831, Training Accuracy: 96.175
[ Tue Jul 16 17:03:18 2024 ] 	Batch(500/6809) done. Loss: 0.0763  lr:0.000001
[ Tue Jul 16 17:03:41 2024 ] 	Batch(600/6809) done. Loss: 0.0778  lr:0.000001
[ Tue Jul 16 17:04:03 2024 ] 	Batch(700/6809) done. Loss: 0.0275  lr:0.000001
[ Tue Jul 16 17:04:26 2024 ] 	Batch(800/6809) done. Loss: 0.0315  lr:0.000001
[ Tue Jul 16 17:04:49 2024 ] 	Batch(900/6809) done. Loss: 0.2919  lr:0.000001
[ Tue Jul 16 17:05:11 2024 ] 
Training: Epoch [116/150], Step [999], Loss: 0.15354926884174347, Training Accuracy: 95.9125
[ Tue Jul 16 17:05:11 2024 ] 	Batch(1000/6809) done. Loss: 0.2162  lr:0.000001
[ Tue Jul 16 17:05:34 2024 ] 	Batch(1100/6809) done. Loss: 0.1190  lr:0.000001
[ Tue Jul 16 17:05:56 2024 ] 	Batch(1200/6809) done. Loss: 0.2332  lr:0.000001
[ Tue Jul 16 17:06:19 2024 ] 	Batch(1300/6809) done. Loss: 0.3042  lr:0.000001
[ Tue Jul 16 17:06:41 2024 ] 	Batch(1400/6809) done. Loss: 0.0963  lr:0.000001
[ Tue Jul 16 17:07:04 2024 ] 
Training: Epoch [116/150], Step [1499], Loss: 0.055645301938056946, Training Accuracy: 95.74166666666667
[ Tue Jul 16 17:07:04 2024 ] 	Batch(1500/6809) done. Loss: 0.1456  lr:0.000001
[ Tue Jul 16 17:07:27 2024 ] 	Batch(1600/6809) done. Loss: 0.0861  lr:0.000001
[ Tue Jul 16 17:07:50 2024 ] 	Batch(1700/6809) done. Loss: 0.3275  lr:0.000001
[ Tue Jul 16 17:08:12 2024 ] 	Batch(1800/6809) done. Loss: 0.0460  lr:0.000001
[ Tue Jul 16 17:08:35 2024 ] 	Batch(1900/6809) done. Loss: 0.0747  lr:0.000001
[ Tue Jul 16 17:08:57 2024 ] 
Training: Epoch [116/150], Step [1999], Loss: 0.23497876524925232, Training Accuracy: 95.69375
[ Tue Jul 16 17:08:57 2024 ] 	Batch(2000/6809) done. Loss: 0.0448  lr:0.000001
[ Tue Jul 16 17:09:20 2024 ] 	Batch(2100/6809) done. Loss: 0.2697  lr:0.000001
[ Tue Jul 16 17:09:42 2024 ] 	Batch(2200/6809) done. Loss: 0.2385  lr:0.000001
[ Tue Jul 16 17:10:05 2024 ] 	Batch(2300/6809) done. Loss: 0.3090  lr:0.000001
[ Tue Jul 16 17:10:28 2024 ] 	Batch(2400/6809) done. Loss: 0.1450  lr:0.000001
[ Tue Jul 16 17:10:50 2024 ] 
Training: Epoch [116/150], Step [2499], Loss: 0.15265491604804993, Training Accuracy: 95.685
[ Tue Jul 16 17:10:50 2024 ] 	Batch(2500/6809) done. Loss: 0.0103  lr:0.000001
[ Tue Jul 16 17:11:13 2024 ] 	Batch(2600/6809) done. Loss: 0.1712  lr:0.000001
[ Tue Jul 16 17:11:35 2024 ] 	Batch(2700/6809) done. Loss: 0.0815  lr:0.000001
[ Tue Jul 16 17:11:58 2024 ] 	Batch(2800/6809) done. Loss: 0.2283  lr:0.000001
[ Tue Jul 16 17:12:21 2024 ] 	Batch(2900/6809) done. Loss: 0.0556  lr:0.000001
[ Tue Jul 16 17:12:44 2024 ] 
Training: Epoch [116/150], Step [2999], Loss: 0.5892624855041504, Training Accuracy: 95.68333333333334
[ Tue Jul 16 17:12:44 2024 ] 	Batch(3000/6809) done. Loss: 0.1267  lr:0.000001
[ Tue Jul 16 17:13:07 2024 ] 	Batch(3100/6809) done. Loss: 0.1986  lr:0.000001
[ Tue Jul 16 17:13:31 2024 ] 	Batch(3200/6809) done. Loss: 0.0113  lr:0.000001
[ Tue Jul 16 17:13:53 2024 ] 	Batch(3300/6809) done. Loss: 0.0031  lr:0.000001
[ Tue Jul 16 17:14:16 2024 ] 	Batch(3400/6809) done. Loss: 0.0179  lr:0.000001
[ Tue Jul 16 17:14:38 2024 ] 
Training: Epoch [116/150], Step [3499], Loss: 0.22689849138259888, Training Accuracy: 95.64285714285714
[ Tue Jul 16 17:14:39 2024 ] 	Batch(3500/6809) done. Loss: 0.2325  lr:0.000001
[ Tue Jul 16 17:15:01 2024 ] 	Batch(3600/6809) done. Loss: 0.0307  lr:0.000001
[ Tue Jul 16 17:15:24 2024 ] 	Batch(3700/6809) done. Loss: 0.2997  lr:0.000001
[ Tue Jul 16 17:15:48 2024 ] 	Batch(3800/6809) done. Loss: 0.0094  lr:0.000001
[ Tue Jul 16 17:16:11 2024 ] 	Batch(3900/6809) done. Loss: 0.0929  lr:0.000001
[ Tue Jul 16 17:16:34 2024 ] 
Training: Epoch [116/150], Step [3999], Loss: 0.27041855454444885, Training Accuracy: 95.6875
[ Tue Jul 16 17:16:34 2024 ] 	Batch(4000/6809) done. Loss: 0.0510  lr:0.000001
[ Tue Jul 16 17:16:57 2024 ] 	Batch(4100/6809) done. Loss: 0.1767  lr:0.000001
[ Tue Jul 16 17:17:20 2024 ] 	Batch(4200/6809) done. Loss: 0.3270  lr:0.000001
[ Tue Jul 16 17:17:43 2024 ] 	Batch(4300/6809) done. Loss: 0.0040  lr:0.000001
[ Tue Jul 16 17:18:07 2024 ] 	Batch(4400/6809) done. Loss: 0.1750  lr:0.000001
[ Tue Jul 16 17:18:30 2024 ] 
Training: Epoch [116/150], Step [4499], Loss: 0.016475483775138855, Training Accuracy: 95.68611111111112
[ Tue Jul 16 17:18:30 2024 ] 	Batch(4500/6809) done. Loss: 0.0440  lr:0.000001
[ Tue Jul 16 17:18:53 2024 ] 	Batch(4600/6809) done. Loss: 0.0210  lr:0.000001
[ Tue Jul 16 17:19:16 2024 ] 	Batch(4700/6809) done. Loss: 0.2141  lr:0.000001
[ Tue Jul 16 17:19:40 2024 ] 	Batch(4800/6809) done. Loss: 0.1793  lr:0.000001
[ Tue Jul 16 17:20:03 2024 ] 	Batch(4900/6809) done. Loss: 0.0550  lr:0.000001
[ Tue Jul 16 17:20:26 2024 ] 
Training: Epoch [116/150], Step [4999], Loss: 0.048919834196567535, Training Accuracy: 95.65249999999999
[ Tue Jul 16 17:20:26 2024 ] 	Batch(5000/6809) done. Loss: 0.2858  lr:0.000001
[ Tue Jul 16 17:20:49 2024 ] 	Batch(5100/6809) done. Loss: 0.0244  lr:0.000001
[ Tue Jul 16 17:21:13 2024 ] 	Batch(5200/6809) done. Loss: 0.0161  lr:0.000001
[ Tue Jul 16 17:21:36 2024 ] 	Batch(5300/6809) done. Loss: 0.0488  lr:0.000001
[ Tue Jul 16 17:22:00 2024 ] 	Batch(5400/6809) done. Loss: 0.1807  lr:0.000001
[ Tue Jul 16 17:22:22 2024 ] 
Training: Epoch [116/150], Step [5499], Loss: 0.17450107634067535, Training Accuracy: 95.67727272727272
[ Tue Jul 16 17:22:22 2024 ] 	Batch(5500/6809) done. Loss: 0.2637  lr:0.000001
[ Tue Jul 16 17:22:45 2024 ] 	Batch(5600/6809) done. Loss: 0.8716  lr:0.000001
[ Tue Jul 16 17:23:08 2024 ] 	Batch(5700/6809) done. Loss: 0.0942  lr:0.000001
[ Tue Jul 16 17:23:31 2024 ] 	Batch(5800/6809) done. Loss: 0.0788  lr:0.000001
[ Tue Jul 16 17:23:53 2024 ] 	Batch(5900/6809) done. Loss: 0.3126  lr:0.000001
[ Tue Jul 16 17:24:16 2024 ] 
Training: Epoch [116/150], Step [5999], Loss: 0.3560253977775574, Training Accuracy: 95.62916666666666
[ Tue Jul 16 17:24:16 2024 ] 	Batch(6000/6809) done. Loss: 0.0495  lr:0.000001
[ Tue Jul 16 17:24:39 2024 ] 	Batch(6100/6809) done. Loss: 0.2330  lr:0.000001
[ Tue Jul 16 17:25:02 2024 ] 	Batch(6200/6809) done. Loss: 0.0239  lr:0.000001
[ Tue Jul 16 17:25:24 2024 ] 	Batch(6300/6809) done. Loss: 0.1968  lr:0.000001
[ Tue Jul 16 17:25:47 2024 ] 	Batch(6400/6809) done. Loss: 0.0585  lr:0.000001
[ Tue Jul 16 17:26:10 2024 ] 
Training: Epoch [116/150], Step [6499], Loss: 0.19809749722480774, Training Accuracy: 95.62692307692308
[ Tue Jul 16 17:26:10 2024 ] 	Batch(6500/6809) done. Loss: 0.0269  lr:0.000001
[ Tue Jul 16 17:26:33 2024 ] 	Batch(6600/6809) done. Loss: 0.4834  lr:0.000001
[ Tue Jul 16 17:26:55 2024 ] 	Batch(6700/6809) done. Loss: 0.1831  lr:0.000001
[ Tue Jul 16 17:27:18 2024 ] 	Batch(6800/6809) done. Loss: 0.0416  lr:0.000001
[ Tue Jul 16 17:27:20 2024 ] 	Mean training loss: 0.1538.
[ Tue Jul 16 17:27:20 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 17:27:20 2024 ] Training epoch: 118
[ Tue Jul 16 17:27:21 2024 ] 	Batch(0/6809) done. Loss: 0.1013  lr:0.000001
[ Tue Jul 16 17:27:44 2024 ] 	Batch(100/6809) done. Loss: 0.2450  lr:0.000001
[ Tue Jul 16 17:28:06 2024 ] 	Batch(200/6809) done. Loss: 0.1251  lr:0.000001
[ Tue Jul 16 17:28:29 2024 ] 	Batch(300/6809) done. Loss: 0.0017  lr:0.000001
[ Tue Jul 16 17:28:52 2024 ] 	Batch(400/6809) done. Loss: 0.1420  lr:0.000001
[ Tue Jul 16 17:29:15 2024 ] 
Training: Epoch [117/150], Step [499], Loss: 0.020632661879062653, Training Accuracy: 96.1
[ Tue Jul 16 17:29:15 2024 ] 	Batch(500/6809) done. Loss: 0.0346  lr:0.000001
[ Tue Jul 16 17:29:38 2024 ] 	Batch(600/6809) done. Loss: 0.0061  lr:0.000001
[ Tue Jul 16 17:30:01 2024 ] 	Batch(700/6809) done. Loss: 0.2746  lr:0.000001
[ Tue Jul 16 17:30:25 2024 ] 	Batch(800/6809) done. Loss: 0.3103  lr:0.000001
[ Tue Jul 16 17:30:47 2024 ] 	Batch(900/6809) done. Loss: 0.0261  lr:0.000001
[ Tue Jul 16 17:31:10 2024 ] 
Training: Epoch [117/150], Step [999], Loss: 0.3235688805580139, Training Accuracy: 96.0
[ Tue Jul 16 17:31:10 2024 ] 	Batch(1000/6809) done. Loss: 0.0558  lr:0.000001
[ Tue Jul 16 17:31:33 2024 ] 	Batch(1100/6809) done. Loss: 0.1708  lr:0.000001
[ Tue Jul 16 17:31:57 2024 ] 	Batch(1200/6809) done. Loss: 0.0489  lr:0.000001
[ Tue Jul 16 17:32:20 2024 ] 	Batch(1300/6809) done. Loss: 0.1583  lr:0.000001
[ Tue Jul 16 17:32:43 2024 ] 	Batch(1400/6809) done. Loss: 0.0330  lr:0.000001
[ Tue Jul 16 17:33:05 2024 ] 
Training: Epoch [117/150], Step [1499], Loss: 0.15352605283260345, Training Accuracy: 96.05833333333334
[ Tue Jul 16 17:33:06 2024 ] 	Batch(1500/6809) done. Loss: 0.1035  lr:0.000001
[ Tue Jul 16 17:33:29 2024 ] 	Batch(1600/6809) done. Loss: 0.1745  lr:0.000001
[ Tue Jul 16 17:33:52 2024 ] 	Batch(1700/6809) done. Loss: 0.0010  lr:0.000001
[ Tue Jul 16 17:34:15 2024 ] 	Batch(1800/6809) done. Loss: 0.1321  lr:0.000001
[ Tue Jul 16 17:34:38 2024 ] 	Batch(1900/6809) done. Loss: 0.1331  lr:0.000001
[ Tue Jul 16 17:35:00 2024 ] 
Training: Epoch [117/150], Step [1999], Loss: 0.10753367096185684, Training Accuracy: 96.175
[ Tue Jul 16 17:35:00 2024 ] 	Batch(2000/6809) done. Loss: 0.2971  lr:0.000001
[ Tue Jul 16 17:35:23 2024 ] 	Batch(2100/6809) done. Loss: 0.3444  lr:0.000001
[ Tue Jul 16 17:35:45 2024 ] 	Batch(2200/6809) done. Loss: 0.0067  lr:0.000001
[ Tue Jul 16 17:36:08 2024 ] 	Batch(2300/6809) done. Loss: 0.0041  lr:0.000001
[ Tue Jul 16 17:36:31 2024 ] 	Batch(2400/6809) done. Loss: 0.0951  lr:0.000001
[ Tue Jul 16 17:36:53 2024 ] 
Training: Epoch [117/150], Step [2499], Loss: 0.08016662299633026, Training Accuracy: 96.045
[ Tue Jul 16 17:36:53 2024 ] 	Batch(2500/6809) done. Loss: 0.0949  lr:0.000001
[ Tue Jul 16 17:37:16 2024 ] 	Batch(2600/6809) done. Loss: 0.0445  lr:0.000001
[ Tue Jul 16 17:37:38 2024 ] 	Batch(2700/6809) done. Loss: 0.3712  lr:0.000001
[ Tue Jul 16 17:38:01 2024 ] 	Batch(2800/6809) done. Loss: 0.4271  lr:0.000001
[ Tue Jul 16 17:38:24 2024 ] 	Batch(2900/6809) done. Loss: 0.1646  lr:0.000001
[ Tue Jul 16 17:38:46 2024 ] 
Training: Epoch [117/150], Step [2999], Loss: 0.1952192187309265, Training Accuracy: 95.92916666666666
[ Tue Jul 16 17:38:46 2024 ] 	Batch(3000/6809) done. Loss: 0.3320  lr:0.000001
[ Tue Jul 16 17:39:09 2024 ] 	Batch(3100/6809) done. Loss: 0.2171  lr:0.000001
[ Tue Jul 16 17:39:31 2024 ] 	Batch(3200/6809) done. Loss: 0.0981  lr:0.000001
[ Tue Jul 16 17:39:54 2024 ] 	Batch(3300/6809) done. Loss: 0.1162  lr:0.000001
[ Tue Jul 16 17:40:17 2024 ] 	Batch(3400/6809) done. Loss: 0.1337  lr:0.000001
[ Tue Jul 16 17:40:40 2024 ] 
Training: Epoch [117/150], Step [3499], Loss: 0.026152560487389565, Training Accuracy: 95.85000000000001
[ Tue Jul 16 17:40:40 2024 ] 	Batch(3500/6809) done. Loss: 0.0907  lr:0.000001
[ Tue Jul 16 17:41:04 2024 ] 	Batch(3600/6809) done. Loss: 0.0170  lr:0.000001
[ Tue Jul 16 17:41:26 2024 ] 	Batch(3700/6809) done. Loss: 0.0169  lr:0.000001
[ Tue Jul 16 17:41:49 2024 ] 	Batch(3800/6809) done. Loss: 0.0446  lr:0.000001
[ Tue Jul 16 17:42:12 2024 ] 	Batch(3900/6809) done. Loss: 0.0215  lr:0.000001
[ Tue Jul 16 17:42:34 2024 ] 
Training: Epoch [117/150], Step [3999], Loss: 0.16528397798538208, Training Accuracy: 95.759375
[ Tue Jul 16 17:42:34 2024 ] 	Batch(4000/6809) done. Loss: 0.3030  lr:0.000001
[ Tue Jul 16 17:42:57 2024 ] 	Batch(4100/6809) done. Loss: 0.0900  lr:0.000001
[ Tue Jul 16 17:43:19 2024 ] 	Batch(4200/6809) done. Loss: 0.0266  lr:0.000001
[ Tue Jul 16 17:43:42 2024 ] 	Batch(4300/6809) done. Loss: 0.1531  lr:0.000001
[ Tue Jul 16 17:44:04 2024 ] 	Batch(4400/6809) done. Loss: 0.0069  lr:0.000001
[ Tue Jul 16 17:44:27 2024 ] 
Training: Epoch [117/150], Step [4499], Loss: 0.24128808081150055, Training Accuracy: 95.66944444444444
[ Tue Jul 16 17:44:27 2024 ] 	Batch(4500/6809) done. Loss: 0.1407  lr:0.000001
[ Tue Jul 16 17:44:49 2024 ] 	Batch(4600/6809) done. Loss: 0.0061  lr:0.000001
[ Tue Jul 16 17:45:12 2024 ] 	Batch(4700/6809) done. Loss: 0.3225  lr:0.000001
[ Tue Jul 16 17:45:35 2024 ] 	Batch(4800/6809) done. Loss: 0.1111  lr:0.000001
[ Tue Jul 16 17:45:57 2024 ] 	Batch(4900/6809) done. Loss: 0.4827  lr:0.000001
[ Tue Jul 16 17:46:20 2024 ] 
Training: Epoch [117/150], Step [4999], Loss: 0.006923042703419924, Training Accuracy: 95.65
[ Tue Jul 16 17:46:20 2024 ] 	Batch(5000/6809) done. Loss: 0.1336  lr:0.000001
[ Tue Jul 16 17:46:42 2024 ] 	Batch(5100/6809) done. Loss: 0.1266  lr:0.000001
[ Tue Jul 16 17:47:05 2024 ] 	Batch(5200/6809) done. Loss: 0.1564  lr:0.000001
[ Tue Jul 16 17:47:28 2024 ] 	Batch(5300/6809) done. Loss: 0.0277  lr:0.000001
[ Tue Jul 16 17:47:51 2024 ] 	Batch(5400/6809) done. Loss: 0.1551  lr:0.000001
[ Tue Jul 16 17:48:14 2024 ] 
Training: Epoch [117/150], Step [5499], Loss: 0.33373314142227173, Training Accuracy: 95.67272727272727
[ Tue Jul 16 17:48:14 2024 ] 	Batch(5500/6809) done. Loss: 0.3277  lr:0.000001
[ Tue Jul 16 17:48:37 2024 ] 	Batch(5600/6809) done. Loss: 0.2170  lr:0.000001
[ Tue Jul 16 17:49:00 2024 ] 	Batch(5700/6809) done. Loss: 0.0228  lr:0.000001
[ Tue Jul 16 17:49:23 2024 ] 	Batch(5800/6809) done. Loss: 0.2984  lr:0.000001
[ Tue Jul 16 17:49:45 2024 ] 	Batch(5900/6809) done. Loss: 0.2396  lr:0.000001
[ Tue Jul 16 17:50:08 2024 ] 
Training: Epoch [117/150], Step [5999], Loss: 0.2306922823190689, Training Accuracy: 95.66458333333333
[ Tue Jul 16 17:50:08 2024 ] 	Batch(6000/6809) done. Loss: 0.0916  lr:0.000001
[ Tue Jul 16 17:50:31 2024 ] 	Batch(6100/6809) done. Loss: 0.3964  lr:0.000001
[ Tue Jul 16 17:50:53 2024 ] 	Batch(6200/6809) done. Loss: 0.0725  lr:0.000001
[ Tue Jul 16 17:51:16 2024 ] 	Batch(6300/6809) done. Loss: 0.3204  lr:0.000001
[ Tue Jul 16 17:51:39 2024 ] 	Batch(6400/6809) done. Loss: 0.3436  lr:0.000001
[ Tue Jul 16 17:52:01 2024 ] 
Training: Epoch [117/150], Step [6499], Loss: 0.13437986373901367, Training Accuracy: 95.68653846153846
[ Tue Jul 16 17:52:01 2024 ] 	Batch(6500/6809) done. Loss: 0.0600  lr:0.000001
[ Tue Jul 16 17:52:24 2024 ] 	Batch(6600/6809) done. Loss: 0.0743  lr:0.000001
[ Tue Jul 16 17:52:46 2024 ] 	Batch(6700/6809) done. Loss: 0.0963  lr:0.000001
[ Tue Jul 16 17:53:09 2024 ] 	Batch(6800/6809) done. Loss: 0.0393  lr:0.000001
[ Tue Jul 16 17:53:11 2024 ] 	Mean training loss: 0.1531.
[ Tue Jul 16 17:53:11 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 17:53:11 2024 ] Training epoch: 119
[ Tue Jul 16 17:53:12 2024 ] 	Batch(0/6809) done. Loss: 0.2407  lr:0.000001
[ Tue Jul 16 17:53:35 2024 ] 	Batch(100/6809) done. Loss: 0.0332  lr:0.000001
[ Tue Jul 16 17:53:59 2024 ] 	Batch(200/6809) done. Loss: 0.2320  lr:0.000001
[ Tue Jul 16 17:54:22 2024 ] 	Batch(300/6809) done. Loss: 0.1533  lr:0.000001
[ Tue Jul 16 17:54:45 2024 ] 	Batch(400/6809) done. Loss: 0.0312  lr:0.000001
[ Tue Jul 16 17:55:08 2024 ] 
Training: Epoch [118/150], Step [499], Loss: 0.1380559653043747, Training Accuracy: 95.675
[ Tue Jul 16 17:55:08 2024 ] 	Batch(500/6809) done. Loss: 0.3052  lr:0.000001
[ Tue Jul 16 17:55:31 2024 ] 	Batch(600/6809) done. Loss: 0.2218  lr:0.000001
[ Tue Jul 16 17:55:54 2024 ] 	Batch(700/6809) done. Loss: 0.1267  lr:0.000001
[ Tue Jul 16 17:56:17 2024 ] 	Batch(800/6809) done. Loss: 0.0397  lr:0.000001
[ Tue Jul 16 17:56:40 2024 ] 	Batch(900/6809) done. Loss: 0.2127  lr:0.000001
[ Tue Jul 16 17:57:03 2024 ] 
Training: Epoch [118/150], Step [999], Loss: 0.18315540254116058, Training Accuracy: 95.775
[ Tue Jul 16 17:57:03 2024 ] 	Batch(1000/6809) done. Loss: 0.0294  lr:0.000001
[ Tue Jul 16 17:57:26 2024 ] 	Batch(1100/6809) done. Loss: 0.0988  lr:0.000001
[ Tue Jul 16 17:57:49 2024 ] 	Batch(1200/6809) done. Loss: 0.2034  lr:0.000001
[ Tue Jul 16 17:58:12 2024 ] 	Batch(1300/6809) done. Loss: 0.5276  lr:0.000001
[ Tue Jul 16 17:58:35 2024 ] 	Batch(1400/6809) done. Loss: 0.0278  lr:0.000001
[ Tue Jul 16 17:58:58 2024 ] 
Training: Epoch [118/150], Step [1499], Loss: 0.07622502744197845, Training Accuracy: 95.59166666666667
[ Tue Jul 16 17:58:58 2024 ] 	Batch(1500/6809) done. Loss: 0.1078  lr:0.000001
[ Tue Jul 16 17:59:21 2024 ] 	Batch(1600/6809) done. Loss: 0.1992  lr:0.000001
[ Tue Jul 16 17:59:44 2024 ] 	Batch(1700/6809) done. Loss: 0.0339  lr:0.000001
[ Tue Jul 16 18:00:07 2024 ] 	Batch(1800/6809) done. Loss: 0.0921  lr:0.000001
[ Tue Jul 16 18:00:30 2024 ] 	Batch(1900/6809) done. Loss: 0.0372  lr:0.000001
[ Tue Jul 16 18:00:52 2024 ] 
Training: Epoch [118/150], Step [1999], Loss: 0.04920249059796333, Training Accuracy: 95.66875
[ Tue Jul 16 18:00:53 2024 ] 	Batch(2000/6809) done. Loss: 0.0574  lr:0.000001
[ Tue Jul 16 18:01:15 2024 ] 	Batch(2100/6809) done. Loss: 0.1767  lr:0.000001
[ Tue Jul 16 18:01:38 2024 ] 	Batch(2200/6809) done. Loss: 0.0211  lr:0.000001
[ Tue Jul 16 18:02:01 2024 ] 	Batch(2300/6809) done. Loss: 0.0169  lr:0.000001
[ Tue Jul 16 18:02:24 2024 ] 	Batch(2400/6809) done. Loss: 0.1607  lr:0.000001
[ Tue Jul 16 18:02:46 2024 ] 
Training: Epoch [118/150], Step [2499], Loss: 0.04487739875912666, Training Accuracy: 95.67
[ Tue Jul 16 18:02:46 2024 ] 	Batch(2500/6809) done. Loss: 0.0164  lr:0.000001
[ Tue Jul 16 18:03:09 2024 ] 	Batch(2600/6809) done. Loss: 0.0066  lr:0.000001
[ Tue Jul 16 18:03:32 2024 ] 	Batch(2700/6809) done. Loss: 0.1025  lr:0.000001
[ Tue Jul 16 18:03:54 2024 ] 	Batch(2800/6809) done. Loss: 0.0833  lr:0.000001
[ Tue Jul 16 18:04:17 2024 ] 	Batch(2900/6809) done. Loss: 0.0335  lr:0.000001
[ Tue Jul 16 18:04:40 2024 ] 
Training: Epoch [118/150], Step [2999], Loss: 0.1073346808552742, Training Accuracy: 95.6375
[ Tue Jul 16 18:04:40 2024 ] 	Batch(3000/6809) done. Loss: 0.0128  lr:0.000001
[ Tue Jul 16 18:05:03 2024 ] 	Batch(3100/6809) done. Loss: 0.0482  lr:0.000001
[ Tue Jul 16 18:05:27 2024 ] 	Batch(3200/6809) done. Loss: 0.0876  lr:0.000001
[ Tue Jul 16 18:05:50 2024 ] 	Batch(3300/6809) done. Loss: 0.2273  lr:0.000001
[ Tue Jul 16 18:06:14 2024 ] 	Batch(3400/6809) done. Loss: 0.2489  lr:0.000001
[ Tue Jul 16 18:06:36 2024 ] 
Training: Epoch [118/150], Step [3499], Loss: 0.04687020927667618, Training Accuracy: 95.65357142857142
[ Tue Jul 16 18:06:37 2024 ] 	Batch(3500/6809) done. Loss: 0.1958  lr:0.000001
[ Tue Jul 16 18:06:59 2024 ] 	Batch(3600/6809) done. Loss: 0.0618  lr:0.000001
[ Tue Jul 16 18:07:23 2024 ] 	Batch(3700/6809) done. Loss: 0.0811  lr:0.000001
[ Tue Jul 16 18:07:46 2024 ] 	Batch(3800/6809) done. Loss: 0.2208  lr:0.000001
[ Tue Jul 16 18:08:09 2024 ] 	Batch(3900/6809) done. Loss: 0.0383  lr:0.000001
[ Tue Jul 16 18:08:31 2024 ] 
Training: Epoch [118/150], Step [3999], Loss: 0.25642964243888855, Training Accuracy: 95.61874999999999
[ Tue Jul 16 18:08:32 2024 ] 	Batch(4000/6809) done. Loss: 0.0335  lr:0.000001
[ Tue Jul 16 18:08:54 2024 ] 	Batch(4100/6809) done. Loss: 0.0173  lr:0.000001
[ Tue Jul 16 18:09:17 2024 ] 	Batch(4200/6809) done. Loss: 0.1553  lr:0.000001
[ Tue Jul 16 18:09:40 2024 ] 	Batch(4300/6809) done. Loss: 0.2995  lr:0.000001
[ Tue Jul 16 18:10:03 2024 ] 	Batch(4400/6809) done. Loss: 0.0381  lr:0.000001
[ Tue Jul 16 18:10:25 2024 ] 
Training: Epoch [118/150], Step [4499], Loss: 0.11202803254127502, Training Accuracy: 95.56666666666666
[ Tue Jul 16 18:10:25 2024 ] 	Batch(4500/6809) done. Loss: 0.0319  lr:0.000001
[ Tue Jul 16 18:10:48 2024 ] 	Batch(4600/6809) done. Loss: 0.7022  lr:0.000001
[ Tue Jul 16 18:11:11 2024 ] 	Batch(4700/6809) done. Loss: 0.0893  lr:0.000001
[ Tue Jul 16 18:11:34 2024 ] 	Batch(4800/6809) done. Loss: 0.1638  lr:0.000001
[ Tue Jul 16 18:11:56 2024 ] 	Batch(4900/6809) done. Loss: 0.0876  lr:0.000001
[ Tue Jul 16 18:12:19 2024 ] 
Training: Epoch [118/150], Step [4999], Loss: 0.4117491841316223, Training Accuracy: 95.575
[ Tue Jul 16 18:12:19 2024 ] 	Batch(5000/6809) done. Loss: 0.1012  lr:0.000001
[ Tue Jul 16 18:12:42 2024 ] 	Batch(5100/6809) done. Loss: 0.1906  lr:0.000001
[ Tue Jul 16 18:13:04 2024 ] 	Batch(5200/6809) done. Loss: 0.2376  lr:0.000001
[ Tue Jul 16 18:13:27 2024 ] 	Batch(5300/6809) done. Loss: 0.3404  lr:0.000001
[ Tue Jul 16 18:13:50 2024 ] 	Batch(5400/6809) done. Loss: 0.2008  lr:0.000001
[ Tue Jul 16 18:14:12 2024 ] 
Training: Epoch [118/150], Step [5499], Loss: 0.04080471396446228, Training Accuracy: 95.62045454545455
[ Tue Jul 16 18:14:12 2024 ] 	Batch(5500/6809) done. Loss: 0.0683  lr:0.000001
[ Tue Jul 16 18:14:35 2024 ] 	Batch(5600/6809) done. Loss: 0.0782  lr:0.000001
[ Tue Jul 16 18:14:58 2024 ] 	Batch(5700/6809) done. Loss: 0.0527  lr:0.000001
[ Tue Jul 16 18:15:21 2024 ] 	Batch(5800/6809) done. Loss: 0.1323  lr:0.000001
[ Tue Jul 16 18:15:44 2024 ] 	Batch(5900/6809) done. Loss: 0.0243  lr:0.000001
[ Tue Jul 16 18:16:06 2024 ] 
Training: Epoch [118/150], Step [5999], Loss: 0.028302356600761414, Training Accuracy: 95.61458333333334
[ Tue Jul 16 18:16:07 2024 ] 	Batch(6000/6809) done. Loss: 0.0284  lr:0.000001
[ Tue Jul 16 18:16:29 2024 ] 	Batch(6100/6809) done. Loss: 0.0371  lr:0.000001
[ Tue Jul 16 18:16:52 2024 ] 	Batch(6200/6809) done. Loss: 0.0692  lr:0.000001
[ Tue Jul 16 18:17:15 2024 ] 	Batch(6300/6809) done. Loss: 0.2313  lr:0.000001
[ Tue Jul 16 18:17:38 2024 ] 	Batch(6400/6809) done. Loss: 0.1009  lr:0.000001
[ Tue Jul 16 18:18:00 2024 ] 
Training: Epoch [118/150], Step [6499], Loss: 0.30475977063179016, Training Accuracy: 95.63076923076923
[ Tue Jul 16 18:18:01 2024 ] 	Batch(6500/6809) done. Loss: 0.0415  lr:0.000001
[ Tue Jul 16 18:18:23 2024 ] 	Batch(6600/6809) done. Loss: 0.1830  lr:0.000001
[ Tue Jul 16 18:18:46 2024 ] 	Batch(6700/6809) done. Loss: 0.1271  lr:0.000001
[ Tue Jul 16 18:19:09 2024 ] 	Batch(6800/6809) done. Loss: 0.3434  lr:0.000001
[ Tue Jul 16 18:19:10 2024 ] 	Mean training loss: 0.1549.
[ Tue Jul 16 18:19:10 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 18:19:11 2024 ] Training epoch: 120
[ Tue Jul 16 18:19:11 2024 ] 	Batch(0/6809) done. Loss: 0.1959  lr:0.000001
[ Tue Jul 16 18:19:34 2024 ] 	Batch(100/6809) done. Loss: 0.3570  lr:0.000001
[ Tue Jul 16 18:19:56 2024 ] 	Batch(200/6809) done. Loss: 0.2345  lr:0.000001
[ Tue Jul 16 18:20:19 2024 ] 	Batch(300/6809) done. Loss: 0.2905  lr:0.000001
[ Tue Jul 16 18:20:42 2024 ] 	Batch(400/6809) done. Loss: 0.0851  lr:0.000001
[ Tue Jul 16 18:21:04 2024 ] 
Training: Epoch [119/150], Step [499], Loss: 0.05791088938713074, Training Accuracy: 95.875
[ Tue Jul 16 18:21:04 2024 ] 	Batch(500/6809) done. Loss: 0.1424  lr:0.000001
[ Tue Jul 16 18:21:27 2024 ] 	Batch(600/6809) done. Loss: 0.5080  lr:0.000001
[ Tue Jul 16 18:21:49 2024 ] 	Batch(700/6809) done. Loss: 0.5541  lr:0.000001
[ Tue Jul 16 18:22:12 2024 ] 	Batch(800/6809) done. Loss: 0.0567  lr:0.000001
[ Tue Jul 16 18:22:35 2024 ] 	Batch(900/6809) done. Loss: 0.0868  lr:0.000001
[ Tue Jul 16 18:22:57 2024 ] 
Training: Epoch [119/150], Step [999], Loss: 0.04227375611662865, Training Accuracy: 95.85000000000001
[ Tue Jul 16 18:22:57 2024 ] 	Batch(1000/6809) done. Loss: 0.0339  lr:0.000001
[ Tue Jul 16 18:23:20 2024 ] 	Batch(1100/6809) done. Loss: 0.1157  lr:0.000001
[ Tue Jul 16 18:23:43 2024 ] 	Batch(1200/6809) done. Loss: 0.2519  lr:0.000001
[ Tue Jul 16 18:24:05 2024 ] 	Batch(1300/6809) done. Loss: 0.0984  lr:0.000001
[ Tue Jul 16 18:24:28 2024 ] 	Batch(1400/6809) done. Loss: 0.2474  lr:0.000001
[ Tue Jul 16 18:24:50 2024 ] 
Training: Epoch [119/150], Step [1499], Loss: 0.6466001272201538, Training Accuracy: 95.89166666666667
[ Tue Jul 16 18:24:50 2024 ] 	Batch(1500/6809) done. Loss: 0.2369  lr:0.000001
[ Tue Jul 16 18:25:13 2024 ] 	Batch(1600/6809) done. Loss: 0.2736  lr:0.000001
[ Tue Jul 16 18:25:36 2024 ] 	Batch(1700/6809) done. Loss: 0.0401  lr:0.000001
[ Tue Jul 16 18:25:58 2024 ] 	Batch(1800/6809) done. Loss: 0.0788  lr:0.000001
[ Tue Jul 16 18:26:21 2024 ] 	Batch(1900/6809) done. Loss: 0.2167  lr:0.000001
[ Tue Jul 16 18:26:43 2024 ] 
Training: Epoch [119/150], Step [1999], Loss: 0.1385086327791214, Training Accuracy: 95.81875
[ Tue Jul 16 18:26:43 2024 ] 	Batch(2000/6809) done. Loss: 0.0336  lr:0.000001
[ Tue Jul 16 18:27:06 2024 ] 	Batch(2100/6809) done. Loss: 0.2541  lr:0.000001
[ Tue Jul 16 18:27:29 2024 ] 	Batch(2200/6809) done. Loss: 0.3855  lr:0.000001
[ Tue Jul 16 18:27:52 2024 ] 	Batch(2300/6809) done. Loss: 0.3144  lr:0.000001
[ Tue Jul 16 18:28:14 2024 ] 	Batch(2400/6809) done. Loss: 0.1038  lr:0.000001
[ Tue Jul 16 18:28:37 2024 ] 
Training: Epoch [119/150], Step [2499], Loss: 0.34288591146469116, Training Accuracy: 95.83500000000001
[ Tue Jul 16 18:28:37 2024 ] 	Batch(2500/6809) done. Loss: 0.0305  lr:0.000001
[ Tue Jul 16 18:29:00 2024 ] 	Batch(2600/6809) done. Loss: 0.1819  lr:0.000001
[ Tue Jul 16 18:29:22 2024 ] 	Batch(2700/6809) done. Loss: 0.0154  lr:0.000001
[ Tue Jul 16 18:29:46 2024 ] 	Batch(2800/6809) done. Loss: 0.0584  lr:0.000001
[ Tue Jul 16 18:30:08 2024 ] 	Batch(2900/6809) done. Loss: 0.0556  lr:0.000001
[ Tue Jul 16 18:30:31 2024 ] 
Training: Epoch [119/150], Step [2999], Loss: 0.08914022892713547, Training Accuracy: 95.75416666666666
[ Tue Jul 16 18:30:31 2024 ] 	Batch(3000/6809) done. Loss: 0.1322  lr:0.000001
[ Tue Jul 16 18:30:53 2024 ] 	Batch(3100/6809) done. Loss: 0.3118  lr:0.000001
[ Tue Jul 16 18:31:16 2024 ] 	Batch(3200/6809) done. Loss: 0.0316  lr:0.000001
[ Tue Jul 16 18:31:39 2024 ] 	Batch(3300/6809) done. Loss: 0.0790  lr:0.000001
[ Tue Jul 16 18:32:01 2024 ] 	Batch(3400/6809) done. Loss: 0.3512  lr:0.000001
[ Tue Jul 16 18:32:24 2024 ] 
Training: Epoch [119/150], Step [3499], Loss: 0.9636871218681335, Training Accuracy: 95.76071428571429
[ Tue Jul 16 18:32:24 2024 ] 	Batch(3500/6809) done. Loss: 0.1884  lr:0.000001
[ Tue Jul 16 18:32:47 2024 ] 	Batch(3600/6809) done. Loss: 0.7295  lr:0.000001
[ Tue Jul 16 18:33:10 2024 ] 	Batch(3700/6809) done. Loss: 0.0098  lr:0.000001
[ Tue Jul 16 18:33:32 2024 ] 	Batch(3800/6809) done. Loss: 0.0130  lr:0.000001
[ Tue Jul 16 18:33:55 2024 ] 	Batch(3900/6809) done. Loss: 0.1874  lr:0.000001
[ Tue Jul 16 18:34:17 2024 ] 
Training: Epoch [119/150], Step [3999], Loss: 0.008783679455518723, Training Accuracy: 95.75625
[ Tue Jul 16 18:34:18 2024 ] 	Batch(4000/6809) done. Loss: 0.0230  lr:0.000001
[ Tue Jul 16 18:34:40 2024 ] 	Batch(4100/6809) done. Loss: 0.2174  lr:0.000001
[ Tue Jul 16 18:35:03 2024 ] 	Batch(4200/6809) done. Loss: 0.2076  lr:0.000001
[ Tue Jul 16 18:35:26 2024 ] 	Batch(4300/6809) done. Loss: 0.0926  lr:0.000001
[ Tue Jul 16 18:35:48 2024 ] 	Batch(4400/6809) done. Loss: 0.4471  lr:0.000001
[ Tue Jul 16 18:36:12 2024 ] 
Training: Epoch [119/150], Step [4499], Loss: 0.2164059281349182, Training Accuracy: 95.75833333333334
[ Tue Jul 16 18:36:12 2024 ] 	Batch(4500/6809) done. Loss: 0.1049  lr:0.000001
[ Tue Jul 16 18:36:35 2024 ] 	Batch(4600/6809) done. Loss: 0.0953  lr:0.000001
[ Tue Jul 16 18:36:58 2024 ] 	Batch(4700/6809) done. Loss: 0.0095  lr:0.000001
[ Tue Jul 16 18:37:22 2024 ] 	Batch(4800/6809) done. Loss: 0.1955  lr:0.000001
[ Tue Jul 16 18:37:45 2024 ] 	Batch(4900/6809) done. Loss: 0.1426  lr:0.000001
[ Tue Jul 16 18:38:08 2024 ] 
Training: Epoch [119/150], Step [4999], Loss: 0.0882529467344284, Training Accuracy: 95.78
[ Tue Jul 16 18:38:08 2024 ] 	Batch(5000/6809) done. Loss: 0.0723  lr:0.000001
[ Tue Jul 16 18:38:30 2024 ] 	Batch(5100/6809) done. Loss: 0.1444  lr:0.000001
[ Tue Jul 16 18:38:53 2024 ] 	Batch(5200/6809) done. Loss: 0.0047  lr:0.000001
[ Tue Jul 16 18:39:16 2024 ] 	Batch(5300/6809) done. Loss: 0.0470  lr:0.000001
[ Tue Jul 16 18:39:38 2024 ] 	Batch(5400/6809) done. Loss: 0.2297  lr:0.000001
[ Tue Jul 16 18:40:01 2024 ] 
Training: Epoch [119/150], Step [5499], Loss: 0.10358124226331711, Training Accuracy: 95.7840909090909
[ Tue Jul 16 18:40:01 2024 ] 	Batch(5500/6809) done. Loss: 0.0256  lr:0.000001
[ Tue Jul 16 18:40:24 2024 ] 	Batch(5600/6809) done. Loss: 0.0401  lr:0.000001
[ Tue Jul 16 18:40:46 2024 ] 	Batch(5700/6809) done. Loss: 0.0328  lr:0.000001
[ Tue Jul 16 18:41:09 2024 ] 	Batch(5800/6809) done. Loss: 0.0059  lr:0.000001
[ Tue Jul 16 18:41:33 2024 ] 	Batch(5900/6809) done. Loss: 0.0314  lr:0.000001
[ Tue Jul 16 18:41:55 2024 ] 
Training: Epoch [119/150], Step [5999], Loss: 0.25196802616119385, Training Accuracy: 95.79166666666666
[ Tue Jul 16 18:41:56 2024 ] 	Batch(6000/6809) done. Loss: 0.7602  lr:0.000001
[ Tue Jul 16 18:42:19 2024 ] 	Batch(6100/6809) done. Loss: 0.0348  lr:0.000001
[ Tue Jul 16 18:42:42 2024 ] 	Batch(6200/6809) done. Loss: 0.2476  lr:0.000001
[ Tue Jul 16 18:43:05 2024 ] 	Batch(6300/6809) done. Loss: 0.1588  lr:0.000001
[ Tue Jul 16 18:43:29 2024 ] 	Batch(6400/6809) done. Loss: 0.1338  lr:0.000001
[ Tue Jul 16 18:43:52 2024 ] 
Training: Epoch [119/150], Step [6499], Loss: 0.04700007662177086, Training Accuracy: 95.73846153846154
[ Tue Jul 16 18:43:52 2024 ] 	Batch(6500/6809) done. Loss: 0.0277  lr:0.000001
[ Tue Jul 16 18:44:15 2024 ] 	Batch(6600/6809) done. Loss: 0.0521  lr:0.000001
[ Tue Jul 16 18:44:38 2024 ] 	Batch(6700/6809) done. Loss: 0.1428  lr:0.000001
[ Tue Jul 16 18:45:01 2024 ] 	Batch(6800/6809) done. Loss: 0.4977  lr:0.000001
[ Tue Jul 16 18:45:03 2024 ] 	Mean training loss: 0.1559.
[ Tue Jul 16 18:45:03 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 18:45:03 2024 ] Eval epoch: 120
[ Tue Jul 16 18:51:59 2024 ] 	Mean val loss of 7435 batches: 0.9037136077564115.
[ Tue Jul 16 18:51:59 2024 ] 
Validation: Epoch [119/150], Samples [47440.0/59477], Loss: 2.694901943206787, Validation Accuracy: 79.76192477764515
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 1 : 393 / 500 = 78 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 2 : 422 / 499 = 84 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 3 : 417 / 500 = 83 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 4 : 420 / 502 = 83 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 5 : 433 / 502 = 86 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 6 : 427 / 502 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 7 : 463 / 497 = 93 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 8 : 487 / 498 = 97 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 9 : 386 / 500 = 77 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 10 : 303 / 500 = 60 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 11 : 200 / 498 = 40 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 12 : 402 / 499 = 80 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 13 : 480 / 502 = 95 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 14 : 486 / 504 = 96 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 15 : 360 / 502 = 71 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 16 : 378 / 502 = 75 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 17 : 430 / 504 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 18 : 411 / 504 = 81 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 19 : 432 / 502 = 86 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 20 : 452 / 502 = 90 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 21 : 476 / 503 = 94 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 22 : 429 / 504 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 23 : 419 / 503 = 83 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 24 : 441 / 504 = 87 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 25 : 481 / 504 = 95 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 26 : 466 / 504 = 92 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 27 : 429 / 501 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 28 : 348 / 502 = 69 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 29 : 315 / 502 = 62 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 30 : 374 / 501 = 74 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 31 : 428 / 504 = 84 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 32 : 429 / 503 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 33 : 400 / 503 = 79 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 34 : 485 / 504 = 96 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 35 : 451 / 503 = 89 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 36 : 380 / 502 = 75 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 37 : 446 / 504 = 88 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 38 : 444 / 504 = 88 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 39 : 455 / 498 = 91 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 40 : 392 / 504 = 77 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 41 : 473 / 503 = 94 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 42 : 455 / 504 = 90 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 43 : 353 / 503 = 70 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 44 : 428 / 504 = 84 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 45 : 426 / 504 = 84 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 46 : 396 / 504 = 78 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 47 : 360 / 503 = 71 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 48 : 422 / 503 = 83 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 49 : 414 / 499 = 82 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 50 : 395 / 502 = 78 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 51 : 463 / 503 = 92 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 52 : 461 / 504 = 91 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 53 : 446 / 497 = 89 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 54 : 451 / 480 = 93 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 55 : 435 / 504 = 86 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 56 : 420 / 503 = 83 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 57 : 466 / 504 = 92 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 58 : 483 / 499 = 96 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 59 : 485 / 503 = 96 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 60 : 418 / 479 = 87 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 61 : 415 / 484 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 62 : 402 / 487 = 82 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 63 : 441 / 489 = 90 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 64 : 405 / 488 = 82 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 65 : 427 / 490 = 87 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 66 : 317 / 488 = 64 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 67 : 350 / 490 = 71 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 68 : 300 / 490 = 61 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 69 : 389 / 490 = 79 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 70 : 163 / 490 = 33 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 71 : 284 / 490 = 57 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 72 : 197 / 488 = 40 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 73 : 252 / 486 = 51 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 74 : 275 / 481 = 57 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 75 : 302 / 488 = 61 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 76 : 309 / 489 = 63 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 77 : 322 / 488 = 65 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 78 : 370 / 488 = 75 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 79 : 458 / 490 = 93 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 80 : 395 / 489 = 80 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 81 : 284 / 491 = 57 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 82 : 305 / 491 = 62 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 83 : 247 / 489 = 50 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 84 : 358 / 489 = 73 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 85 : 356 / 489 = 72 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 86 : 418 / 491 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 87 : 415 / 492 = 84 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 88 : 357 / 491 = 72 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 89 : 370 / 492 = 75 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 90 : 263 / 490 = 53 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 91 : 353 / 482 = 73 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 92 : 369 / 490 = 75 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 93 : 350 / 487 = 71 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 94 : 429 / 489 = 87 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 95 : 414 / 490 = 84 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 96 : 459 / 491 = 93 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 97 : 457 / 490 = 93 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 98 : 439 / 491 = 89 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 99 : 446 / 491 = 90 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 100 : 450 / 491 = 91 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 101 : 418 / 491 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 102 : 290 / 492 = 58 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 103 : 391 / 492 = 79 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 104 : 301 / 491 = 61 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 105 : 265 / 491 = 53 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 106 : 322 / 492 = 65 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 107 : 430 / 491 = 87 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 108 : 371 / 492 = 75 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 109 : 362 / 490 = 73 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 110 : 408 / 491 = 83 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 111 : 447 / 492 = 90 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 112 : 460 / 492 = 93 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 113 : 439 / 491 = 89 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 114 : 393 / 491 = 80 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 115 : 411 / 492 = 83 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 116 : 420 / 491 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 117 : 406 / 492 = 82 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 118 : 421 / 490 = 85 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 119 : 449 / 492 = 91 %
[ Tue Jul 16 18:51:59 2024 ] Accuracy of 120 : 404 / 500 = 80 %
[ Tue Jul 16 18:51:59 2024 ] Training epoch: 121
[ Tue Jul 16 18:51:59 2024 ] 	Batch(0/6809) done. Loss: 0.1688  lr:0.000001
[ Tue Jul 16 18:52:22 2024 ] 	Batch(100/6809) done. Loss: 0.1106  lr:0.000001
[ Tue Jul 16 18:52:44 2024 ] 	Batch(200/6809) done. Loss: 0.2439  lr:0.000001
[ Tue Jul 16 18:53:07 2024 ] 	Batch(300/6809) done. Loss: 0.0775  lr:0.000001
[ Tue Jul 16 18:53:30 2024 ] 	Batch(400/6809) done. Loss: 0.1050  lr:0.000001
[ Tue Jul 16 18:53:52 2024 ] 
Training: Epoch [120/150], Step [499], Loss: 0.012602747417986393, Training Accuracy: 95.975
[ Tue Jul 16 18:53:52 2024 ] 	Batch(500/6809) done. Loss: 0.1381  lr:0.000001
[ Tue Jul 16 18:54:15 2024 ] 	Batch(600/6809) done. Loss: 0.0438  lr:0.000001
[ Tue Jul 16 18:54:37 2024 ] 	Batch(700/6809) done. Loss: 0.0910  lr:0.000001
[ Tue Jul 16 18:55:00 2024 ] 	Batch(800/6809) done. Loss: 0.0990  lr:0.000001
[ Tue Jul 16 18:55:23 2024 ] 	Batch(900/6809) done. Loss: 0.0841  lr:0.000001
[ Tue Jul 16 18:55:46 2024 ] 
Training: Epoch [120/150], Step [999], Loss: 0.07491357624530792, Training Accuracy: 95.8
[ Tue Jul 16 18:55:47 2024 ] 	Batch(1000/6809) done. Loss: 0.0680  lr:0.000001
[ Tue Jul 16 18:56:09 2024 ] 	Batch(1100/6809) done. Loss: 0.4109  lr:0.000001
[ Tue Jul 16 18:56:32 2024 ] 	Batch(1200/6809) done. Loss: 0.2830  lr:0.000001
[ Tue Jul 16 18:56:54 2024 ] 	Batch(1300/6809) done. Loss: 0.2888  lr:0.000001
[ Tue Jul 16 18:57:17 2024 ] 	Batch(1400/6809) done. Loss: 0.1179  lr:0.000001
[ Tue Jul 16 18:57:39 2024 ] 
Training: Epoch [120/150], Step [1499], Loss: 0.15069052577018738, Training Accuracy: 95.80833333333332
[ Tue Jul 16 18:57:40 2024 ] 	Batch(1500/6809) done. Loss: 0.0166  lr:0.000001
[ Tue Jul 16 18:58:02 2024 ] 	Batch(1600/6809) done. Loss: 0.0680  lr:0.000001
[ Tue Jul 16 18:58:25 2024 ] 	Batch(1700/6809) done. Loss: 0.2495  lr:0.000001
[ Tue Jul 16 18:58:48 2024 ] 	Batch(1800/6809) done. Loss: 0.2275  lr:0.000001
[ Tue Jul 16 18:59:10 2024 ] 	Batch(1900/6809) done. Loss: 0.1067  lr:0.000001
[ Tue Jul 16 18:59:33 2024 ] 
Training: Epoch [120/150], Step [1999], Loss: 0.04507097601890564, Training Accuracy: 95.7125
[ Tue Jul 16 18:59:33 2024 ] 	Batch(2000/6809) done. Loss: 0.2460  lr:0.000001
[ Tue Jul 16 18:59:56 2024 ] 	Batch(2100/6809) done. Loss: 0.1835  lr:0.000001
[ Tue Jul 16 19:00:19 2024 ] 	Batch(2200/6809) done. Loss: 0.0386  lr:0.000001
[ Tue Jul 16 19:00:42 2024 ] 	Batch(2300/6809) done. Loss: 0.0803  lr:0.000001
[ Tue Jul 16 19:01:05 2024 ] 	Batch(2400/6809) done. Loss: 0.0687  lr:0.000001
[ Tue Jul 16 19:01:28 2024 ] 
Training: Epoch [120/150], Step [2499], Loss: 0.02714710310101509, Training Accuracy: 95.735
[ Tue Jul 16 19:01:29 2024 ] 	Batch(2500/6809) done. Loss: 0.0543  lr:0.000001
[ Tue Jul 16 19:01:52 2024 ] 	Batch(2600/6809) done. Loss: 0.0084  lr:0.000001
[ Tue Jul 16 19:02:15 2024 ] 	Batch(2700/6809) done. Loss: 0.5778  lr:0.000001
[ Tue Jul 16 19:02:37 2024 ] 	Batch(2800/6809) done. Loss: 0.0294  lr:0.000001
[ Tue Jul 16 19:03:00 2024 ] 	Batch(2900/6809) done. Loss: 0.1643  lr:0.000001
[ Tue Jul 16 19:03:22 2024 ] 
Training: Epoch [120/150], Step [2999], Loss: 0.07222439348697662, Training Accuracy: 95.72083333333333
[ Tue Jul 16 19:03:23 2024 ] 	Batch(3000/6809) done. Loss: 0.4343  lr:0.000001
[ Tue Jul 16 19:03:45 2024 ] 	Batch(3100/6809) done. Loss: 0.0993  lr:0.000001
[ Tue Jul 16 19:04:08 2024 ] 	Batch(3200/6809) done. Loss: 0.2329  lr:0.000001
[ Tue Jul 16 19:04:31 2024 ] 	Batch(3300/6809) done. Loss: 0.2526  lr:0.000001
[ Tue Jul 16 19:04:54 2024 ] 	Batch(3400/6809) done. Loss: 0.1270  lr:0.000001
[ Tue Jul 16 19:05:17 2024 ] 
Training: Epoch [120/150], Step [3499], Loss: 0.4007176160812378, Training Accuracy: 95.77142857142857
[ Tue Jul 16 19:05:17 2024 ] 	Batch(3500/6809) done. Loss: 0.0279  lr:0.000001
[ Tue Jul 16 19:05:41 2024 ] 	Batch(3600/6809) done. Loss: 0.0457  lr:0.000001
[ Tue Jul 16 19:06:04 2024 ] 	Batch(3700/6809) done. Loss: 0.7409  lr:0.000001
[ Tue Jul 16 19:06:27 2024 ] 	Batch(3800/6809) done. Loss: 0.2122  lr:0.000001
[ Tue Jul 16 19:06:50 2024 ] 	Batch(3900/6809) done. Loss: 0.0301  lr:0.000001
[ Tue Jul 16 19:07:13 2024 ] 
Training: Epoch [120/150], Step [3999], Loss: 0.2010742872953415, Training Accuracy: 95.74375
[ Tue Jul 16 19:07:13 2024 ] 	Batch(4000/6809) done. Loss: 0.1041  lr:0.000001
[ Tue Jul 16 19:07:36 2024 ] 	Batch(4100/6809) done. Loss: 0.3348  lr:0.000001
[ Tue Jul 16 19:07:59 2024 ] 	Batch(4200/6809) done. Loss: 0.2701  lr:0.000001
[ Tue Jul 16 19:08:22 2024 ] 	Batch(4300/6809) done. Loss: 0.1218  lr:0.000001
[ Tue Jul 16 19:08:46 2024 ] 	Batch(4400/6809) done. Loss: 0.1368  lr:0.000001
[ Tue Jul 16 19:09:09 2024 ] 
Training: Epoch [120/150], Step [4499], Loss: 0.24804848432540894, Training Accuracy: 95.66111111111111
[ Tue Jul 16 19:09:09 2024 ] 	Batch(4500/6809) done. Loss: 0.0720  lr:0.000001
[ Tue Jul 16 19:09:32 2024 ] 	Batch(4600/6809) done. Loss: 0.0855  lr:0.000001
[ Tue Jul 16 19:09:55 2024 ] 	Batch(4700/6809) done. Loss: 0.2548  lr:0.000001
[ Tue Jul 16 19:10:18 2024 ] 	Batch(4800/6809) done. Loss: 0.0642  lr:0.000001
[ Tue Jul 16 19:10:41 2024 ] 	Batch(4900/6809) done. Loss: 0.2008  lr:0.000001
[ Tue Jul 16 19:11:04 2024 ] 
Training: Epoch [120/150], Step [4999], Loss: 0.2842562198638916, Training Accuracy: 95.67750000000001
[ Tue Jul 16 19:11:04 2024 ] 	Batch(5000/6809) done. Loss: 0.1182  lr:0.000001
[ Tue Jul 16 19:11:27 2024 ] 	Batch(5100/6809) done. Loss: 0.0568  lr:0.000001
[ Tue Jul 16 19:11:51 2024 ] 	Batch(5200/6809) done. Loss: 0.0495  lr:0.000001
[ Tue Jul 16 19:12:14 2024 ] 	Batch(5300/6809) done. Loss: 0.0553  lr:0.000001
[ Tue Jul 16 19:12:37 2024 ] 	Batch(5400/6809) done. Loss: 0.0627  lr:0.000001
[ Tue Jul 16 19:13:00 2024 ] 
Training: Epoch [120/150], Step [5499], Loss: 0.033343810588121414, Training Accuracy: 95.71818181818182
[ Tue Jul 16 19:13:00 2024 ] 	Batch(5500/6809) done. Loss: 0.1502  lr:0.000001
[ Tue Jul 16 19:13:24 2024 ] 	Batch(5600/6809) done. Loss: 0.0311  lr:0.000001
[ Tue Jul 16 19:13:47 2024 ] 	Batch(5700/6809) done. Loss: 0.0300  lr:0.000001
[ Tue Jul 16 19:14:10 2024 ] 	Batch(5800/6809) done. Loss: 0.0731  lr:0.000001
[ Tue Jul 16 19:14:33 2024 ] 	Batch(5900/6809) done. Loss: 0.1143  lr:0.000001
[ Tue Jul 16 19:14:56 2024 ] 
Training: Epoch [120/150], Step [5999], Loss: 0.15252740681171417, Training Accuracy: 95.71041666666666
[ Tue Jul 16 19:14:57 2024 ] 	Batch(6000/6809) done. Loss: 0.2583  lr:0.000001
[ Tue Jul 16 19:15:20 2024 ] 	Batch(6100/6809) done. Loss: 0.0123  lr:0.000001
[ Tue Jul 16 19:15:43 2024 ] 	Batch(6200/6809) done. Loss: 0.3359  lr:0.000001
[ Tue Jul 16 19:16:07 2024 ] 	Batch(6300/6809) done. Loss: 0.0071  lr:0.000001
[ Tue Jul 16 19:16:30 2024 ] 	Batch(6400/6809) done. Loss: 0.6550  lr:0.000001
[ Tue Jul 16 19:16:53 2024 ] 
Training: Epoch [120/150], Step [6499], Loss: 0.1449274867773056, Training Accuracy: 95.69423076923077
[ Tue Jul 16 19:16:53 2024 ] 	Batch(6500/6809) done. Loss: 0.6808  lr:0.000001
[ Tue Jul 16 19:17:17 2024 ] 	Batch(6600/6809) done. Loss: 0.3982  lr:0.000001
[ Tue Jul 16 19:17:40 2024 ] 	Batch(6700/6809) done. Loss: 0.0280  lr:0.000001
[ Tue Jul 16 19:18:03 2024 ] 	Batch(6800/6809) done. Loss: 0.0828  lr:0.000001
[ Tue Jul 16 19:18:05 2024 ] 	Mean training loss: 0.1548.
[ Tue Jul 16 19:18:05 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 19:18:06 2024 ] Training epoch: 122
[ Tue Jul 16 19:18:06 2024 ] 	Batch(0/6809) done. Loss: 0.1754  lr:0.000001
[ Tue Jul 16 19:18:29 2024 ] 	Batch(100/6809) done. Loss: 0.0322  lr:0.000001
[ Tue Jul 16 19:18:53 2024 ] 	Batch(200/6809) done. Loss: 0.0122  lr:0.000001
[ Tue Jul 16 19:19:16 2024 ] 	Batch(300/6809) done. Loss: 0.1771  lr:0.000001
[ Tue Jul 16 19:19:39 2024 ] 	Batch(400/6809) done. Loss: 0.0088  lr:0.000001
[ Tue Jul 16 19:20:02 2024 ] 
Training: Epoch [121/150], Step [499], Loss: 0.43252286314964294, Training Accuracy: 95.25
[ Tue Jul 16 19:20:03 2024 ] 	Batch(500/6809) done. Loss: 0.0833  lr:0.000001
[ Tue Jul 16 19:20:26 2024 ] 	Batch(600/6809) done. Loss: 0.0081  lr:0.000001
[ Tue Jul 16 19:20:50 2024 ] 	Batch(700/6809) done. Loss: 0.4103  lr:0.000001
[ Tue Jul 16 19:21:13 2024 ] 	Batch(800/6809) done. Loss: 0.0542  lr:0.000001
[ Tue Jul 16 19:21:36 2024 ] 	Batch(900/6809) done. Loss: 0.0214  lr:0.000001
[ Tue Jul 16 19:21:58 2024 ] 
Training: Epoch [121/150], Step [999], Loss: 0.16974914073944092, Training Accuracy: 95.375
[ Tue Jul 16 19:21:58 2024 ] 	Batch(1000/6809) done. Loss: 0.0191  lr:0.000001
[ Tue Jul 16 19:22:21 2024 ] 	Batch(1100/6809) done. Loss: 0.0872  lr:0.000001
[ Tue Jul 16 19:22:44 2024 ] 	Batch(1200/6809) done. Loss: 0.1923  lr:0.000001
[ Tue Jul 16 19:23:06 2024 ] 	Batch(1300/6809) done. Loss: 0.0534  lr:0.000001
[ Tue Jul 16 19:23:29 2024 ] 	Batch(1400/6809) done. Loss: 0.1814  lr:0.000001
[ Tue Jul 16 19:23:51 2024 ] 
Training: Epoch [121/150], Step [1499], Loss: 0.10603342950344086, Training Accuracy: 95.54166666666667
[ Tue Jul 16 19:23:51 2024 ] 	Batch(1500/6809) done. Loss: 0.1288  lr:0.000001
[ Tue Jul 16 19:24:14 2024 ] 	Batch(1600/6809) done. Loss: 0.1126  lr:0.000001
[ Tue Jul 16 19:24:37 2024 ] 	Batch(1700/6809) done. Loss: 0.0290  lr:0.000001
[ Tue Jul 16 19:24:59 2024 ] 	Batch(1800/6809) done. Loss: 0.7698  lr:0.000001
[ Tue Jul 16 19:25:22 2024 ] 	Batch(1900/6809) done. Loss: 0.6236  lr:0.000001
[ Tue Jul 16 19:25:44 2024 ] 
Training: Epoch [121/150], Step [1999], Loss: 0.0062753669917583466, Training Accuracy: 95.46249999999999
[ Tue Jul 16 19:25:45 2024 ] 	Batch(2000/6809) done. Loss: 0.0326  lr:0.000001
[ Tue Jul 16 19:26:07 2024 ] 	Batch(2100/6809) done. Loss: 0.0759  lr:0.000001
[ Tue Jul 16 19:26:30 2024 ] 	Batch(2200/6809) done. Loss: 0.0332  lr:0.000001
[ Tue Jul 16 19:26:52 2024 ] 	Batch(2300/6809) done. Loss: 0.0284  lr:0.000001
[ Tue Jul 16 19:27:15 2024 ] 	Batch(2400/6809) done. Loss: 0.0675  lr:0.000001
[ Tue Jul 16 19:27:38 2024 ] 
Training: Epoch [121/150], Step [2499], Loss: 0.3917481303215027, Training Accuracy: 95.5
[ Tue Jul 16 19:27:38 2024 ] 	Batch(2500/6809) done. Loss: 0.2517  lr:0.000001
[ Tue Jul 16 19:28:01 2024 ] 	Batch(2600/6809) done. Loss: 0.2826  lr:0.000001
[ Tue Jul 16 19:28:24 2024 ] 	Batch(2700/6809) done. Loss: 0.2137  lr:0.000001
[ Tue Jul 16 19:28:47 2024 ] 	Batch(2800/6809) done. Loss: 0.1747  lr:0.000001
[ Tue Jul 16 19:29:10 2024 ] 	Batch(2900/6809) done. Loss: 0.1320  lr:0.000001
[ Tue Jul 16 19:29:33 2024 ] 
Training: Epoch [121/150], Step [2999], Loss: 0.20106543600559235, Training Accuracy: 95.52083333333333
[ Tue Jul 16 19:29:33 2024 ] 	Batch(3000/6809) done. Loss: 0.0086  lr:0.000001
[ Tue Jul 16 19:29:56 2024 ] 	Batch(3100/6809) done. Loss: 0.0465  lr:0.000001
[ Tue Jul 16 19:30:19 2024 ] 	Batch(3200/6809) done. Loss: 0.0518  lr:0.000001
[ Tue Jul 16 19:30:42 2024 ] 	Batch(3300/6809) done. Loss: 0.1473  lr:0.000001
[ Tue Jul 16 19:31:05 2024 ] 	Batch(3400/6809) done. Loss: 0.0740  lr:0.000001
[ Tue Jul 16 19:31:28 2024 ] 
Training: Epoch [121/150], Step [3499], Loss: 0.023534908890724182, Training Accuracy: 95.57857142857142
[ Tue Jul 16 19:31:28 2024 ] 	Batch(3500/6809) done. Loss: 0.1025  lr:0.000001
[ Tue Jul 16 19:31:51 2024 ] 	Batch(3600/6809) done. Loss: 0.1531  lr:0.000001
[ Tue Jul 16 19:32:15 2024 ] 	Batch(3700/6809) done. Loss: 0.0607  lr:0.000001
[ Tue Jul 16 19:32:38 2024 ] 	Batch(3800/6809) done. Loss: 0.0875  lr:0.000001
[ Tue Jul 16 19:33:01 2024 ] 	Batch(3900/6809) done. Loss: 0.0701  lr:0.000001
[ Tue Jul 16 19:33:24 2024 ] 
Training: Epoch [121/150], Step [3999], Loss: 0.0221157968044281, Training Accuracy: 95.55624999999999
[ Tue Jul 16 19:33:25 2024 ] 	Batch(4000/6809) done. Loss: 0.3499  lr:0.000001
[ Tue Jul 16 19:33:48 2024 ] 	Batch(4100/6809) done. Loss: 0.0571  lr:0.000001
[ Tue Jul 16 19:34:12 2024 ] 	Batch(4200/6809) done. Loss: 0.1548  lr:0.000001
[ Tue Jul 16 19:34:36 2024 ] 	Batch(4300/6809) done. Loss: 0.0459  lr:0.000001
[ Tue Jul 16 19:35:00 2024 ] 	Batch(4400/6809) done. Loss: 0.1261  lr:0.000001
[ Tue Jul 16 19:35:23 2024 ] 
Training: Epoch [121/150], Step [4499], Loss: 0.35281091928482056, Training Accuracy: 95.60555555555555
[ Tue Jul 16 19:35:23 2024 ] 	Batch(4500/6809) done. Loss: 0.0492  lr:0.000001
[ Tue Jul 16 19:35:46 2024 ] 	Batch(4600/6809) done. Loss: 0.1461  lr:0.000001
[ Tue Jul 16 19:36:09 2024 ] 	Batch(4700/6809) done. Loss: 0.0102  lr:0.000001
[ Tue Jul 16 19:36:32 2024 ] 	Batch(4800/6809) done. Loss: 0.0538  lr:0.000001
[ Tue Jul 16 19:36:55 2024 ] 	Batch(4900/6809) done. Loss: 0.1221  lr:0.000001
[ Tue Jul 16 19:37:18 2024 ] 
Training: Epoch [121/150], Step [4999], Loss: 0.16596458852291107, Training Accuracy: 95.625
[ Tue Jul 16 19:37:18 2024 ] 	Batch(5000/6809) done. Loss: 0.3572  lr:0.000001
[ Tue Jul 16 19:37:41 2024 ] 	Batch(5100/6809) done. Loss: 0.3708  lr:0.000001
[ Tue Jul 16 19:38:04 2024 ] 	Batch(5200/6809) done. Loss: 0.0427  lr:0.000001
[ Tue Jul 16 19:38:27 2024 ] 	Batch(5300/6809) done. Loss: 0.1792  lr:0.000001
[ Tue Jul 16 19:38:50 2024 ] 	Batch(5400/6809) done. Loss: 0.0115  lr:0.000001
[ Tue Jul 16 19:39:13 2024 ] 
Training: Epoch [121/150], Step [5499], Loss: 0.05593735724687576, Training Accuracy: 95.65454545454546
[ Tue Jul 16 19:39:13 2024 ] 	Batch(5500/6809) done. Loss: 0.1329  lr:0.000001
[ Tue Jul 16 19:39:36 2024 ] 	Batch(5600/6809) done. Loss: 0.3874  lr:0.000001
[ Tue Jul 16 19:39:59 2024 ] 	Batch(5700/6809) done. Loss: 0.2438  lr:0.000001
[ Tue Jul 16 19:40:21 2024 ] 	Batch(5800/6809) done. Loss: 0.3081  lr:0.000001
[ Tue Jul 16 19:40:44 2024 ] 	Batch(5900/6809) done. Loss: 0.1151  lr:0.000001
[ Tue Jul 16 19:41:06 2024 ] 
Training: Epoch [121/150], Step [5999], Loss: 0.013184026814997196, Training Accuracy: 95.61041666666667
[ Tue Jul 16 19:41:07 2024 ] 	Batch(6000/6809) done. Loss: 0.0075  lr:0.000001
[ Tue Jul 16 19:41:29 2024 ] 	Batch(6100/6809) done. Loss: 0.0654  lr:0.000001
[ Tue Jul 16 19:41:52 2024 ] 	Batch(6200/6809) done. Loss: 0.3605  lr:0.000001
[ Tue Jul 16 19:42:15 2024 ] 	Batch(6300/6809) done. Loss: 0.0389  lr:0.000001
[ Tue Jul 16 19:42:39 2024 ] 	Batch(6400/6809) done. Loss: 0.4064  lr:0.000001
[ Tue Jul 16 19:43:02 2024 ] 
Training: Epoch [121/150], Step [6499], Loss: 0.7256516218185425, Training Accuracy: 95.62307692307692
[ Tue Jul 16 19:43:02 2024 ] 	Batch(6500/6809) done. Loss: 0.0189  lr:0.000001
[ Tue Jul 16 19:43:25 2024 ] 	Batch(6600/6809) done. Loss: 0.0424  lr:0.000001
[ Tue Jul 16 19:43:48 2024 ] 	Batch(6700/6809) done. Loss: 0.0735  lr:0.000001
[ Tue Jul 16 19:44:11 2024 ] 	Batch(6800/6809) done. Loss: 0.4799  lr:0.000001
[ Tue Jul 16 19:44:13 2024 ] 	Mean training loss: 0.1570.
[ Tue Jul 16 19:44:13 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 19:44:13 2024 ] Training epoch: 123
[ Tue Jul 16 19:44:14 2024 ] 	Batch(0/6809) done. Loss: 0.0676  lr:0.000001
[ Tue Jul 16 19:44:37 2024 ] 	Batch(100/6809) done. Loss: 0.0649  lr:0.000001
[ Tue Jul 16 19:45:00 2024 ] 	Batch(200/6809) done. Loss: 0.1618  lr:0.000001
[ Tue Jul 16 19:45:23 2024 ] 	Batch(300/6809) done. Loss: 0.1788  lr:0.000001
[ Tue Jul 16 19:45:47 2024 ] 	Batch(400/6809) done. Loss: 0.0183  lr:0.000001
[ Tue Jul 16 19:46:09 2024 ] 
Training: Epoch [122/150], Step [499], Loss: 0.06258691847324371, Training Accuracy: 95.85000000000001
[ Tue Jul 16 19:46:10 2024 ] 	Batch(500/6809) done. Loss: 0.1932  lr:0.000001
[ Tue Jul 16 19:46:33 2024 ] 	Batch(600/6809) done. Loss: 0.0840  lr:0.000001
[ Tue Jul 16 19:46:56 2024 ] 	Batch(700/6809) done. Loss: 0.2747  lr:0.000001
[ Tue Jul 16 19:47:19 2024 ] 	Batch(800/6809) done. Loss: 0.1026  lr:0.000001
[ Tue Jul 16 19:47:42 2024 ] 	Batch(900/6809) done. Loss: 0.0123  lr:0.000001
[ Tue Jul 16 19:48:05 2024 ] 
Training: Epoch [122/150], Step [999], Loss: 0.3424532413482666, Training Accuracy: 95.8875
[ Tue Jul 16 19:48:05 2024 ] 	Batch(1000/6809) done. Loss: 0.2225  lr:0.000001
[ Tue Jul 16 19:48:29 2024 ] 	Batch(1100/6809) done. Loss: 0.0576  lr:0.000001
[ Tue Jul 16 19:48:52 2024 ] 	Batch(1200/6809) done. Loss: 0.6015  lr:0.000001
[ Tue Jul 16 19:49:15 2024 ] 	Batch(1300/6809) done. Loss: 0.0708  lr:0.000001
[ Tue Jul 16 19:49:38 2024 ] 	Batch(1400/6809) done. Loss: 0.0472  lr:0.000001
[ Tue Jul 16 19:50:01 2024 ] 
Training: Epoch [122/150], Step [1499], Loss: 0.07546510547399521, Training Accuracy: 95.73333333333333
[ Tue Jul 16 19:50:01 2024 ] 	Batch(1500/6809) done. Loss: 0.3556  lr:0.000001
[ Tue Jul 16 19:50:24 2024 ] 	Batch(1600/6809) done. Loss: 0.0148  lr:0.000001
[ Tue Jul 16 19:50:47 2024 ] 	Batch(1700/6809) done. Loss: 0.2747  lr:0.000001
[ Tue Jul 16 19:51:10 2024 ] 	Batch(1800/6809) done. Loss: 0.2311  lr:0.000001
[ Tue Jul 16 19:51:32 2024 ] 	Batch(1900/6809) done. Loss: 0.0761  lr:0.000001
[ Tue Jul 16 19:51:55 2024 ] 
Training: Epoch [122/150], Step [1999], Loss: 0.08482979983091354, Training Accuracy: 95.66875
[ Tue Jul 16 19:51:55 2024 ] 	Batch(2000/6809) done. Loss: 0.0073  lr:0.000001
[ Tue Jul 16 19:52:18 2024 ] 	Batch(2100/6809) done. Loss: 0.0060  lr:0.000001
[ Tue Jul 16 19:52:41 2024 ] 	Batch(2200/6809) done. Loss: 0.0525  lr:0.000001
[ Tue Jul 16 19:53:04 2024 ] 	Batch(2300/6809) done. Loss: 0.0710  lr:0.000001
[ Tue Jul 16 19:53:28 2024 ] 	Batch(2400/6809) done. Loss: 0.0554  lr:0.000001
[ Tue Jul 16 19:53:50 2024 ] 
Training: Epoch [122/150], Step [2499], Loss: 0.5253133773803711, Training Accuracy: 95.685
[ Tue Jul 16 19:53:50 2024 ] 	Batch(2500/6809) done. Loss: 0.0891  lr:0.000001
[ Tue Jul 16 19:54:13 2024 ] 	Batch(2600/6809) done. Loss: 0.3064  lr:0.000001
[ Tue Jul 16 19:54:35 2024 ] 	Batch(2700/6809) done. Loss: 0.0614  lr:0.000001
[ Tue Jul 16 19:54:58 2024 ] 	Batch(2800/6809) done. Loss: 0.0109  lr:0.000001
[ Tue Jul 16 19:55:20 2024 ] 	Batch(2900/6809) done. Loss: 0.0256  lr:0.000001
[ Tue Jul 16 19:55:43 2024 ] 
Training: Epoch [122/150], Step [2999], Loss: 0.017950767651200294, Training Accuracy: 95.6625
[ Tue Jul 16 19:55:43 2024 ] 	Batch(3000/6809) done. Loss: 0.0957  lr:0.000001
[ Tue Jul 16 19:56:06 2024 ] 	Batch(3100/6809) done. Loss: 0.1704  lr:0.000001
[ Tue Jul 16 19:56:28 2024 ] 	Batch(3200/6809) done. Loss: 0.0094  lr:0.000001
[ Tue Jul 16 19:56:51 2024 ] 	Batch(3300/6809) done. Loss: 0.0595  lr:0.000001
[ Tue Jul 16 19:57:13 2024 ] 	Batch(3400/6809) done. Loss: 0.1552  lr:0.000001
[ Tue Jul 16 19:57:36 2024 ] 
Training: Epoch [122/150], Step [3499], Loss: 0.10972075164318085, Training Accuracy: 95.68214285714286
[ Tue Jul 16 19:57:36 2024 ] 	Batch(3500/6809) done. Loss: 0.0086  lr:0.000001
[ Tue Jul 16 19:57:59 2024 ] 	Batch(3600/6809) done. Loss: 0.1385  lr:0.000001
[ Tue Jul 16 19:58:21 2024 ] 	Batch(3700/6809) done. Loss: 0.0661  lr:0.000001
[ Tue Jul 16 19:58:44 2024 ] 	Batch(3800/6809) done. Loss: 0.2165  lr:0.000001
[ Tue Jul 16 19:59:07 2024 ] 	Batch(3900/6809) done. Loss: 0.0164  lr:0.000001
[ Tue Jul 16 19:59:29 2024 ] 
Training: Epoch [122/150], Step [3999], Loss: 0.10189217329025269, Training Accuracy: 95.69375
[ Tue Jul 16 19:59:30 2024 ] 	Batch(4000/6809) done. Loss: 0.4415  lr:0.000001
[ Tue Jul 16 19:59:53 2024 ] 	Batch(4100/6809) done. Loss: 0.0537  lr:0.000001
[ Tue Jul 16 20:00:16 2024 ] 	Batch(4200/6809) done. Loss: 0.0064  lr:0.000001
[ Tue Jul 16 20:00:38 2024 ] 	Batch(4300/6809) done. Loss: 0.0977  lr:0.000001
[ Tue Jul 16 20:01:01 2024 ] 	Batch(4400/6809) done. Loss: 0.0883  lr:0.000001
[ Tue Jul 16 20:01:24 2024 ] 
Training: Epoch [122/150], Step [4499], Loss: 0.09106073528528214, Training Accuracy: 95.64444444444445
[ Tue Jul 16 20:01:24 2024 ] 	Batch(4500/6809) done. Loss: 0.4379  lr:0.000001
[ Tue Jul 16 20:01:47 2024 ] 	Batch(4600/6809) done. Loss: 0.0426  lr:0.000001
[ Tue Jul 16 20:02:09 2024 ] 	Batch(4700/6809) done. Loss: 0.3136  lr:0.000001
[ Tue Jul 16 20:02:32 2024 ] 	Batch(4800/6809) done. Loss: 0.6125  lr:0.000001
[ Tue Jul 16 20:02:55 2024 ] 	Batch(4900/6809) done. Loss: 0.0213  lr:0.000001
[ Tue Jul 16 20:03:17 2024 ] 
Training: Epoch [122/150], Step [4999], Loss: 0.009536458179354668, Training Accuracy: 95.60499999999999
[ Tue Jul 16 20:03:17 2024 ] 	Batch(5000/6809) done. Loss: 0.0453  lr:0.000001
[ Tue Jul 16 20:03:40 2024 ] 	Batch(5100/6809) done. Loss: 0.0253  lr:0.000001
[ Tue Jul 16 20:04:03 2024 ] 	Batch(5200/6809) done. Loss: 0.1346  lr:0.000001
[ Tue Jul 16 20:04:26 2024 ] 	Batch(5300/6809) done. Loss: 0.0245  lr:0.000001
[ Tue Jul 16 20:04:48 2024 ] 	Batch(5400/6809) done. Loss: 0.7190  lr:0.000001
[ Tue Jul 16 20:05:11 2024 ] 
Training: Epoch [122/150], Step [5499], Loss: 0.19482167065143585, Training Accuracy: 95.57954545454545
[ Tue Jul 16 20:05:11 2024 ] 	Batch(5500/6809) done. Loss: 0.2032  lr:0.000001
[ Tue Jul 16 20:05:34 2024 ] 	Batch(5600/6809) done. Loss: 0.1374  lr:0.000001
[ Tue Jul 16 20:05:57 2024 ] 	Batch(5700/6809) done. Loss: 0.5381  lr:0.000001
[ Tue Jul 16 20:06:20 2024 ] 	Batch(5800/6809) done. Loss: 0.0169  lr:0.000001
[ Tue Jul 16 20:06:43 2024 ] 	Batch(5900/6809) done. Loss: 0.3464  lr:0.000001
[ Tue Jul 16 20:07:05 2024 ] 
Training: Epoch [122/150], Step [5999], Loss: 0.0166446752846241, Training Accuracy: 95.59166666666667
[ Tue Jul 16 20:07:06 2024 ] 	Batch(6000/6809) done. Loss: 0.3060  lr:0.000001
[ Tue Jul 16 20:07:28 2024 ] 	Batch(6100/6809) done. Loss: 0.0955  lr:0.000001
[ Tue Jul 16 20:07:51 2024 ] 	Batch(6200/6809) done. Loss: 0.2874  lr:0.000001
[ Tue Jul 16 20:08:14 2024 ] 	Batch(6300/6809) done. Loss: 0.1612  lr:0.000001
[ Tue Jul 16 20:08:37 2024 ] 	Batch(6400/6809) done. Loss: 0.0127  lr:0.000001
[ Tue Jul 16 20:08:59 2024 ] 
Training: Epoch [122/150], Step [6499], Loss: 0.030792785808444023, Training Accuracy: 95.60192307692307
[ Tue Jul 16 20:08:59 2024 ] 	Batch(6500/6809) done. Loss: 0.0218  lr:0.000001
[ Tue Jul 16 20:09:22 2024 ] 	Batch(6600/6809) done. Loss: 0.1006  lr:0.000001
[ Tue Jul 16 20:09:45 2024 ] 	Batch(6700/6809) done. Loss: 0.2361  lr:0.000001
[ Tue Jul 16 20:10:08 2024 ] 	Batch(6800/6809) done. Loss: 0.5439  lr:0.000001
[ Tue Jul 16 20:10:10 2024 ] 	Mean training loss: 0.1588.
[ Tue Jul 16 20:10:10 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 20:10:10 2024 ] Training epoch: 124
[ Tue Jul 16 20:10:10 2024 ] 	Batch(0/6809) done. Loss: 0.2430  lr:0.000001
[ Tue Jul 16 20:10:33 2024 ] 	Batch(100/6809) done. Loss: 0.0143  lr:0.000001
[ Tue Jul 16 20:10:56 2024 ] 	Batch(200/6809) done. Loss: 0.0428  lr:0.000001
[ Tue Jul 16 20:11:19 2024 ] 	Batch(300/6809) done. Loss: 0.0277  lr:0.000001
[ Tue Jul 16 20:11:42 2024 ] 	Batch(400/6809) done. Loss: 0.0807  lr:0.000001
[ Tue Jul 16 20:12:05 2024 ] 
Training: Epoch [123/150], Step [499], Loss: 0.06494563817977905, Training Accuracy: 95.475
[ Tue Jul 16 20:12:05 2024 ] 	Batch(500/6809) done. Loss: 0.0103  lr:0.000001
[ Tue Jul 16 20:12:28 2024 ] 	Batch(600/6809) done. Loss: 0.1261  lr:0.000001
[ Tue Jul 16 20:12:51 2024 ] 	Batch(700/6809) done. Loss: 0.0571  lr:0.000001
[ Tue Jul 16 20:13:15 2024 ] 	Batch(800/6809) done. Loss: 0.0604  lr:0.000001
[ Tue Jul 16 20:13:38 2024 ] 	Batch(900/6809) done. Loss: 0.2058  lr:0.000001
[ Tue Jul 16 20:14:00 2024 ] 
Training: Epoch [123/150], Step [999], Loss: 0.2565445601940155, Training Accuracy: 95.8875
[ Tue Jul 16 20:14:00 2024 ] 	Batch(1000/6809) done. Loss: 0.1611  lr:0.000001
[ Tue Jul 16 20:14:23 2024 ] 	Batch(1100/6809) done. Loss: 0.1392  lr:0.000001
[ Tue Jul 16 20:14:47 2024 ] 	Batch(1200/6809) done. Loss: 0.1746  lr:0.000001
[ Tue Jul 16 20:15:10 2024 ] 	Batch(1300/6809) done. Loss: 0.0623  lr:0.000001
[ Tue Jul 16 20:15:33 2024 ] 	Batch(1400/6809) done. Loss: 0.1061  lr:0.000001
[ Tue Jul 16 20:15:56 2024 ] 
Training: Epoch [123/150], Step [1499], Loss: 0.10969024151563644, Training Accuracy: 95.89999999999999
[ Tue Jul 16 20:15:56 2024 ] 	Batch(1500/6809) done. Loss: 0.0483  lr:0.000001
[ Tue Jul 16 20:16:19 2024 ] 	Batch(1600/6809) done. Loss: 0.1490  lr:0.000001
[ Tue Jul 16 20:16:43 2024 ] 	Batch(1700/6809) done. Loss: 0.0750  lr:0.000001
[ Tue Jul 16 20:17:06 2024 ] 	Batch(1800/6809) done. Loss: 0.1573  lr:0.000001
[ Tue Jul 16 20:17:28 2024 ] 	Batch(1900/6809) done. Loss: 0.1969  lr:0.000001
[ Tue Jul 16 20:17:51 2024 ] 
Training: Epoch [123/150], Step [1999], Loss: 0.07850652188062668, Training Accuracy: 95.90625
[ Tue Jul 16 20:17:51 2024 ] 	Batch(2000/6809) done. Loss: 0.1099  lr:0.000001
[ Tue Jul 16 20:18:14 2024 ] 	Batch(2100/6809) done. Loss: 0.2112  lr:0.000001
[ Tue Jul 16 20:18:37 2024 ] 	Batch(2200/6809) done. Loss: 0.0354  lr:0.000001
[ Tue Jul 16 20:19:00 2024 ] 	Batch(2300/6809) done. Loss: 0.0922  lr:0.000001
[ Tue Jul 16 20:19:24 2024 ] 	Batch(2400/6809) done. Loss: 0.0227  lr:0.000001
[ Tue Jul 16 20:19:46 2024 ] 
Training: Epoch [123/150], Step [2499], Loss: 0.08303003013134003, Training Accuracy: 95.85000000000001
[ Tue Jul 16 20:19:47 2024 ] 	Batch(2500/6809) done. Loss: 0.0168  lr:0.000001
[ Tue Jul 16 20:20:09 2024 ] 	Batch(2600/6809) done. Loss: 0.1441  lr:0.000001
[ Tue Jul 16 20:20:32 2024 ] 	Batch(2700/6809) done. Loss: 0.0399  lr:0.000001
[ Tue Jul 16 20:20:55 2024 ] 	Batch(2800/6809) done. Loss: 0.0338  lr:0.000001
[ Tue Jul 16 20:21:17 2024 ] 	Batch(2900/6809) done. Loss: 0.0142  lr:0.000001
[ Tue Jul 16 20:21:40 2024 ] 
Training: Epoch [123/150], Step [2999], Loss: 0.06508007645606995, Training Accuracy: 95.85833333333333
[ Tue Jul 16 20:21:40 2024 ] 	Batch(3000/6809) done. Loss: 0.0292  lr:0.000001
[ Tue Jul 16 20:22:03 2024 ] 	Batch(3100/6809) done. Loss: 0.2140  lr:0.000001
[ Tue Jul 16 20:22:26 2024 ] 	Batch(3200/6809) done. Loss: 0.0117  lr:0.000001
[ Tue Jul 16 20:22:49 2024 ] 	Batch(3300/6809) done. Loss: 0.0473  lr:0.000001
[ Tue Jul 16 20:23:12 2024 ] 	Batch(3400/6809) done. Loss: 0.8392  lr:0.000001
[ Tue Jul 16 20:23:34 2024 ] 
Training: Epoch [123/150], Step [3499], Loss: 0.2587391436100006, Training Accuracy: 95.81071428571428
[ Tue Jul 16 20:23:34 2024 ] 	Batch(3500/6809) done. Loss: 0.1936  lr:0.000001
[ Tue Jul 16 20:23:57 2024 ] 	Batch(3600/6809) done. Loss: 0.0401  lr:0.000001
[ Tue Jul 16 20:24:21 2024 ] 	Batch(3700/6809) done. Loss: 0.0170  lr:0.000001
[ Tue Jul 16 20:24:44 2024 ] 	Batch(3800/6809) done. Loss: 0.0646  lr:0.000001
[ Tue Jul 16 20:25:07 2024 ] 	Batch(3900/6809) done. Loss: 0.0434  lr:0.000001
[ Tue Jul 16 20:25:29 2024 ] 
Training: Epoch [123/150], Step [3999], Loss: 0.1415625512599945, Training Accuracy: 95.828125
[ Tue Jul 16 20:25:29 2024 ] 	Batch(4000/6809) done. Loss: 0.2358  lr:0.000001
[ Tue Jul 16 20:25:52 2024 ] 	Batch(4100/6809) done. Loss: 0.6590  lr:0.000001
[ Tue Jul 16 20:26:14 2024 ] 	Batch(4200/6809) done. Loss: 0.1550  lr:0.000001
[ Tue Jul 16 20:26:37 2024 ] 	Batch(4300/6809) done. Loss: 0.0348  lr:0.000001
[ Tue Jul 16 20:27:00 2024 ] 	Batch(4400/6809) done. Loss: 0.0537  lr:0.000001
[ Tue Jul 16 20:27:22 2024 ] 
Training: Epoch [123/150], Step [4499], Loss: 0.010944388806819916, Training Accuracy: 95.79166666666666
[ Tue Jul 16 20:27:23 2024 ] 	Batch(4500/6809) done. Loss: 0.0817  lr:0.000001
[ Tue Jul 16 20:27:45 2024 ] 	Batch(4600/6809) done. Loss: 0.0157  lr:0.000001
[ Tue Jul 16 20:28:08 2024 ] 	Batch(4700/6809) done. Loss: 0.1799  lr:0.000001
[ Tue Jul 16 20:28:31 2024 ] 	Batch(4800/6809) done. Loss: 0.1535  lr:0.000001
[ Tue Jul 16 20:28:53 2024 ] 	Batch(4900/6809) done. Loss: 0.0644  lr:0.000001
[ Tue Jul 16 20:29:16 2024 ] 
Training: Epoch [123/150], Step [4999], Loss: 0.052351027727127075, Training Accuracy: 95.8075
[ Tue Jul 16 20:29:16 2024 ] 	Batch(5000/6809) done. Loss: 0.2162  lr:0.000001
[ Tue Jul 16 20:29:39 2024 ] 	Batch(5100/6809) done. Loss: 0.0054  lr:0.000001
[ Tue Jul 16 20:30:02 2024 ] 	Batch(5200/6809) done. Loss: 0.0452  lr:0.000001
[ Tue Jul 16 20:30:24 2024 ] 	Batch(5300/6809) done. Loss: 0.0802  lr:0.000001
[ Tue Jul 16 20:30:47 2024 ] 	Batch(5400/6809) done. Loss: 0.0901  lr:0.000001
[ Tue Jul 16 20:31:11 2024 ] 
Training: Epoch [123/150], Step [5499], Loss: 0.15186138451099396, Training Accuracy: 95.8659090909091
[ Tue Jul 16 20:31:11 2024 ] 	Batch(5500/6809) done. Loss: 0.2677  lr:0.000001
[ Tue Jul 16 20:31:34 2024 ] 	Batch(5600/6809) done. Loss: 0.0379  lr:0.000001
[ Tue Jul 16 20:31:57 2024 ] 	Batch(5700/6809) done. Loss: 0.1194  lr:0.000001
[ Tue Jul 16 20:32:20 2024 ] 	Batch(5800/6809) done. Loss: 0.0207  lr:0.000001
[ Tue Jul 16 20:32:42 2024 ] 	Batch(5900/6809) done. Loss: 0.0138  lr:0.000001
[ Tue Jul 16 20:33:05 2024 ] 
Training: Epoch [123/150], Step [5999], Loss: 0.12292711436748505, Training Accuracy: 95.80833333333332
[ Tue Jul 16 20:33:05 2024 ] 	Batch(6000/6809) done. Loss: 0.0958  lr:0.000001
[ Tue Jul 16 20:33:28 2024 ] 	Batch(6100/6809) done. Loss: 0.0035  lr:0.000001
[ Tue Jul 16 20:33:51 2024 ] 	Batch(6200/6809) done. Loss: 0.0817  lr:0.000001
[ Tue Jul 16 20:34:13 2024 ] 	Batch(6300/6809) done. Loss: 0.2628  lr:0.000001
[ Tue Jul 16 20:34:37 2024 ] 	Batch(6400/6809) done. Loss: 0.2576  lr:0.000001
[ Tue Jul 16 20:35:00 2024 ] 
Training: Epoch [123/150], Step [6499], Loss: 0.07267298549413681, Training Accuracy: 95.78653846153846
[ Tue Jul 16 20:35:00 2024 ] 	Batch(6500/6809) done. Loss: 0.1067  lr:0.000001
[ Tue Jul 16 20:35:22 2024 ] 	Batch(6600/6809) done. Loss: 0.0621  lr:0.000001
[ Tue Jul 16 20:35:45 2024 ] 	Batch(6700/6809) done. Loss: 0.1523  lr:0.000001
[ Tue Jul 16 20:36:08 2024 ] 	Batch(6800/6809) done. Loss: 0.2403  lr:0.000001
[ Tue Jul 16 20:36:10 2024 ] 	Mean training loss: 0.1591.
[ Tue Jul 16 20:36:10 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 20:36:10 2024 ] Training epoch: 125
[ Tue Jul 16 20:36:11 2024 ] 	Batch(0/6809) done. Loss: 0.6320  lr:0.000001
[ Tue Jul 16 20:36:33 2024 ] 	Batch(100/6809) done. Loss: 0.4393  lr:0.000001
[ Tue Jul 16 20:36:56 2024 ] 	Batch(200/6809) done. Loss: 0.0641  lr:0.000001
[ Tue Jul 16 20:37:19 2024 ] 	Batch(300/6809) done. Loss: 0.0782  lr:0.000001
[ Tue Jul 16 20:37:41 2024 ] 	Batch(400/6809) done. Loss: 0.0294  lr:0.000001
[ Tue Jul 16 20:38:04 2024 ] 
Training: Epoch [124/150], Step [499], Loss: 0.44109630584716797, Training Accuracy: 96.05
[ Tue Jul 16 20:38:04 2024 ] 	Batch(500/6809) done. Loss: 0.0125  lr:0.000001
[ Tue Jul 16 20:38:27 2024 ] 	Batch(600/6809) done. Loss: 0.0690  lr:0.000001
[ Tue Jul 16 20:38:49 2024 ] 	Batch(700/6809) done. Loss: 0.2115  lr:0.000001
[ Tue Jul 16 20:39:12 2024 ] 	Batch(800/6809) done. Loss: 0.0052  lr:0.000001
[ Tue Jul 16 20:39:35 2024 ] 	Batch(900/6809) done. Loss: 0.1524  lr:0.000001
[ Tue Jul 16 20:39:57 2024 ] 
Training: Epoch [124/150], Step [999], Loss: 0.10140193998813629, Training Accuracy: 95.8
[ Tue Jul 16 20:39:57 2024 ] 	Batch(1000/6809) done. Loss: 0.3790  lr:0.000001
[ Tue Jul 16 20:40:20 2024 ] 	Batch(1100/6809) done. Loss: 0.1276  lr:0.000001
[ Tue Jul 16 20:40:43 2024 ] 	Batch(1200/6809) done. Loss: 0.0024  lr:0.000001
[ Tue Jul 16 20:41:06 2024 ] 	Batch(1300/6809) done. Loss: 0.0426  lr:0.000001
[ Tue Jul 16 20:41:30 2024 ] 	Batch(1400/6809) done. Loss: 0.0174  lr:0.000001
[ Tue Jul 16 20:41:52 2024 ] 
Training: Epoch [124/150], Step [1499], Loss: 0.1855727732181549, Training Accuracy: 95.90833333333333
[ Tue Jul 16 20:41:52 2024 ] 	Batch(1500/6809) done. Loss: 0.1422  lr:0.000001
[ Tue Jul 16 20:42:15 2024 ] 	Batch(1600/6809) done. Loss: 0.0991  lr:0.000001
[ Tue Jul 16 20:42:38 2024 ] 	Batch(1700/6809) done. Loss: 0.0461  lr:0.000001
[ Tue Jul 16 20:43:01 2024 ] 	Batch(1800/6809) done. Loss: 0.0813  lr:0.000001
[ Tue Jul 16 20:43:24 2024 ] 	Batch(1900/6809) done. Loss: 0.0478  lr:0.000001
[ Tue Jul 16 20:43:46 2024 ] 
Training: Epoch [124/150], Step [1999], Loss: 0.008340016007423401, Training Accuracy: 95.9125
[ Tue Jul 16 20:43:47 2024 ] 	Batch(2000/6809) done. Loss: 0.1120  lr:0.000001
[ Tue Jul 16 20:44:09 2024 ] 	Batch(2100/6809) done. Loss: 0.4083  lr:0.000001
[ Tue Jul 16 20:44:32 2024 ] 	Batch(2200/6809) done. Loss: 0.1036  lr:0.000001
[ Tue Jul 16 20:44:54 2024 ] 	Batch(2300/6809) done. Loss: 0.4827  lr:0.000001
[ Tue Jul 16 20:45:17 2024 ] 	Batch(2400/6809) done. Loss: 0.0228  lr:0.000001
[ Tue Jul 16 20:45:40 2024 ] 
Training: Epoch [124/150], Step [2499], Loss: 0.0033312710002064705, Training Accuracy: 95.75
[ Tue Jul 16 20:45:40 2024 ] 	Batch(2500/6809) done. Loss: 0.1105  lr:0.000001
[ Tue Jul 16 20:46:02 2024 ] 	Batch(2600/6809) done. Loss: 0.0251  lr:0.000001
[ Tue Jul 16 20:46:25 2024 ] 	Batch(2700/6809) done. Loss: 0.0517  lr:0.000001
[ Tue Jul 16 20:46:49 2024 ] 	Batch(2800/6809) done. Loss: 0.0458  lr:0.000001
[ Tue Jul 16 20:47:12 2024 ] 	Batch(2900/6809) done. Loss: 0.4659  lr:0.000001
[ Tue Jul 16 20:47:35 2024 ] 
Training: Epoch [124/150], Step [2999], Loss: 0.02844943106174469, Training Accuracy: 95.77916666666667
[ Tue Jul 16 20:47:35 2024 ] 	Batch(3000/6809) done. Loss: 0.0759  lr:0.000001
[ Tue Jul 16 20:47:58 2024 ] 	Batch(3100/6809) done. Loss: 0.0874  lr:0.000001
[ Tue Jul 16 20:48:21 2024 ] 	Batch(3200/6809) done. Loss: 0.2607  lr:0.000001
[ Tue Jul 16 20:48:44 2024 ] 	Batch(3300/6809) done. Loss: 0.0393  lr:0.000001
[ Tue Jul 16 20:49:07 2024 ] 	Batch(3400/6809) done. Loss: 0.2160  lr:0.000001
[ Tue Jul 16 20:49:30 2024 ] 
Training: Epoch [124/150], Step [3499], Loss: 0.1817515790462494, Training Accuracy: 95.82142857142857
[ Tue Jul 16 20:49:30 2024 ] 	Batch(3500/6809) done. Loss: 0.0189  lr:0.000001
[ Tue Jul 16 20:49:53 2024 ] 	Batch(3600/6809) done. Loss: 0.2121  lr:0.000001
[ Tue Jul 16 20:50:16 2024 ] 	Batch(3700/6809) done. Loss: 0.0580  lr:0.000001
[ Tue Jul 16 20:50:39 2024 ] 	Batch(3800/6809) done. Loss: 0.1255  lr:0.000001
[ Tue Jul 16 20:51:02 2024 ] 	Batch(3900/6809) done. Loss: 0.2869  lr:0.000001
[ Tue Jul 16 20:51:25 2024 ] 
Training: Epoch [124/150], Step [3999], Loss: 0.006505854427814484, Training Accuracy: 95.78125
[ Tue Jul 16 20:51:25 2024 ] 	Batch(4000/6809) done. Loss: 0.0019  lr:0.000001
[ Tue Jul 16 20:51:48 2024 ] 	Batch(4100/6809) done. Loss: 0.0302  lr:0.000001
[ Tue Jul 16 20:52:11 2024 ] 	Batch(4200/6809) done. Loss: 0.2776  lr:0.000001
[ Tue Jul 16 20:52:34 2024 ] 	Batch(4300/6809) done. Loss: 0.0160  lr:0.000001
[ Tue Jul 16 20:52:57 2024 ] 	Batch(4400/6809) done. Loss: 0.1567  lr:0.000001
[ Tue Jul 16 20:53:20 2024 ] 
Training: Epoch [124/150], Step [4499], Loss: 0.08051366358995438, Training Accuracy: 95.82222222222222
[ Tue Jul 16 20:53:20 2024 ] 	Batch(4500/6809) done. Loss: 0.0982  lr:0.000001
[ Tue Jul 16 20:53:43 2024 ] 	Batch(4600/6809) done. Loss: 0.7572  lr:0.000001
[ Tue Jul 16 20:54:06 2024 ] 	Batch(4700/6809) done. Loss: 0.3173  lr:0.000001
[ Tue Jul 16 20:54:29 2024 ] 	Batch(4800/6809) done. Loss: 0.1960  lr:0.000001
[ Tue Jul 16 20:54:52 2024 ] 	Batch(4900/6809) done. Loss: 0.1549  lr:0.000001
[ Tue Jul 16 20:55:15 2024 ] 
Training: Epoch [124/150], Step [4999], Loss: 0.01148371770977974, Training Accuracy: 95.76750000000001
[ Tue Jul 16 20:55:15 2024 ] 	Batch(5000/6809) done. Loss: 0.0320  lr:0.000001
[ Tue Jul 16 20:55:38 2024 ] 	Batch(5100/6809) done. Loss: 0.1653  lr:0.000001
[ Tue Jul 16 20:56:01 2024 ] 	Batch(5200/6809) done. Loss: 0.0065  lr:0.000001
[ Tue Jul 16 20:56:24 2024 ] 	Batch(5300/6809) done. Loss: 0.1737  lr:0.000001
[ Tue Jul 16 20:56:47 2024 ] 	Batch(5400/6809) done. Loss: 0.1728  lr:0.000001
[ Tue Jul 16 20:57:10 2024 ] 
Training: Epoch [124/150], Step [5499], Loss: 0.05960709601640701, Training Accuracy: 95.76818181818182
[ Tue Jul 16 20:57:10 2024 ] 	Batch(5500/6809) done. Loss: 0.0543  lr:0.000001
[ Tue Jul 16 20:57:33 2024 ] 	Batch(5600/6809) done. Loss: 0.0540  lr:0.000001
[ Tue Jul 16 20:57:56 2024 ] 	Batch(5700/6809) done. Loss: 0.1473  lr:0.000001
[ Tue Jul 16 20:58:19 2024 ] 	Batch(5800/6809) done. Loss: 0.3172  lr:0.000001
[ Tue Jul 16 20:58:42 2024 ] 	Batch(5900/6809) done. Loss: 0.0088  lr:0.000001
[ Tue Jul 16 20:59:05 2024 ] 
Training: Epoch [124/150], Step [5999], Loss: 0.20585127174854279, Training Accuracy: 95.73125
[ Tue Jul 16 20:59:05 2024 ] 	Batch(6000/6809) done. Loss: 0.3316  lr:0.000001
[ Tue Jul 16 20:59:28 2024 ] 	Batch(6100/6809) done. Loss: 0.1561  lr:0.000001
[ Tue Jul 16 20:59:51 2024 ] 	Batch(6200/6809) done. Loss: 0.1283  lr:0.000001
[ Tue Jul 16 21:00:14 2024 ] 	Batch(6300/6809) done. Loss: 0.1909  lr:0.000001
[ Tue Jul 16 21:00:37 2024 ] 	Batch(6400/6809) done. Loss: 0.0384  lr:0.000001
[ Tue Jul 16 21:01:00 2024 ] 
Training: Epoch [124/150], Step [6499], Loss: 0.06774886697530746, Training Accuracy: 95.7076923076923
[ Tue Jul 16 21:01:00 2024 ] 	Batch(6500/6809) done. Loss: 0.4913  lr:0.000001
[ Tue Jul 16 21:01:23 2024 ] 	Batch(6600/6809) done. Loss: 0.2134  lr:0.000001
[ Tue Jul 16 21:01:46 2024 ] 	Batch(6700/6809) done. Loss: 0.2472  lr:0.000001
[ Tue Jul 16 21:02:08 2024 ] 	Batch(6800/6809) done. Loss: 0.1500  lr:0.000001
[ Tue Jul 16 21:02:10 2024 ] 	Mean training loss: 0.1525.
[ Tue Jul 16 21:02:10 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 21:02:10 2024 ] Training epoch: 126
[ Tue Jul 16 21:02:11 2024 ] 	Batch(0/6809) done. Loss: 0.0736  lr:0.000001
[ Tue Jul 16 21:02:33 2024 ] 	Batch(100/6809) done. Loss: 0.0333  lr:0.000001
[ Tue Jul 16 21:02:56 2024 ] 	Batch(200/6809) done. Loss: 0.1001  lr:0.000001
[ Tue Jul 16 21:03:18 2024 ] 	Batch(300/6809) done. Loss: 0.3111  lr:0.000001
[ Tue Jul 16 21:03:41 2024 ] 	Batch(400/6809) done. Loss: 0.3718  lr:0.000001
[ Tue Jul 16 21:04:03 2024 ] 
Training: Epoch [125/150], Step [499], Loss: 0.05862303823232651, Training Accuracy: 95.775
[ Tue Jul 16 21:04:04 2024 ] 	Batch(500/6809) done. Loss: 0.6040  lr:0.000001
[ Tue Jul 16 21:04:26 2024 ] 	Batch(600/6809) done. Loss: 0.5181  lr:0.000001
[ Tue Jul 16 21:04:49 2024 ] 	Batch(700/6809) done. Loss: 0.3055  lr:0.000001
[ Tue Jul 16 21:05:11 2024 ] 	Batch(800/6809) done. Loss: 0.0343  lr:0.000001
[ Tue Jul 16 21:05:34 2024 ] 	Batch(900/6809) done. Loss: 0.4635  lr:0.000001
[ Tue Jul 16 21:05:57 2024 ] 
Training: Epoch [125/150], Step [999], Loss: 0.1033671647310257, Training Accuracy: 95.6
[ Tue Jul 16 21:05:57 2024 ] 	Batch(1000/6809) done. Loss: 0.0162  lr:0.000001
[ Tue Jul 16 21:06:20 2024 ] 	Batch(1100/6809) done. Loss: 0.0212  lr:0.000001
[ Tue Jul 16 21:06:43 2024 ] 	Batch(1200/6809) done. Loss: 0.2033  lr:0.000001
[ Tue Jul 16 21:07:06 2024 ] 	Batch(1300/6809) done. Loss: 0.0142  lr:0.000001
[ Tue Jul 16 21:07:29 2024 ] 	Batch(1400/6809) done. Loss: 0.3280  lr:0.000001
[ Tue Jul 16 21:07:52 2024 ] 
Training: Epoch [125/150], Step [1499], Loss: 0.6670804023742676, Training Accuracy: 95.56666666666666
[ Tue Jul 16 21:07:52 2024 ] 	Batch(1500/6809) done. Loss: 0.1323  lr:0.000001
[ Tue Jul 16 21:08:15 2024 ] 	Batch(1600/6809) done. Loss: 0.4161  lr:0.000001
[ Tue Jul 16 21:08:38 2024 ] 	Batch(1700/6809) done. Loss: 0.4178  lr:0.000001
[ Tue Jul 16 21:09:01 2024 ] 	Batch(1800/6809) done. Loss: 0.2433  lr:0.000001
[ Tue Jul 16 21:09:23 2024 ] 	Batch(1900/6809) done. Loss: 0.1063  lr:0.000001
[ Tue Jul 16 21:09:46 2024 ] 
Training: Epoch [125/150], Step [1999], Loss: 0.20938050746917725, Training Accuracy: 95.43125
[ Tue Jul 16 21:09:46 2024 ] 	Batch(2000/6809) done. Loss: 0.6903  lr:0.000001
[ Tue Jul 16 21:10:09 2024 ] 	Batch(2100/6809) done. Loss: 0.0207  lr:0.000001
[ Tue Jul 16 21:10:31 2024 ] 	Batch(2200/6809) done. Loss: 0.0904  lr:0.000001
[ Tue Jul 16 21:10:54 2024 ] 	Batch(2300/6809) done. Loss: 0.4409  lr:0.000001
[ Tue Jul 16 21:11:16 2024 ] 	Batch(2400/6809) done. Loss: 0.0290  lr:0.000001
[ Tue Jul 16 21:11:39 2024 ] 
Training: Epoch [125/150], Step [2499], Loss: 0.06344433128833771, Training Accuracy: 95.56
[ Tue Jul 16 21:11:39 2024 ] 	Batch(2500/6809) done. Loss: 0.0787  lr:0.000001
[ Tue Jul 16 21:12:02 2024 ] 	Batch(2600/6809) done. Loss: 0.0723  lr:0.000001
[ Tue Jul 16 21:12:24 2024 ] 	Batch(2700/6809) done. Loss: 0.1701  lr:0.000001
[ Tue Jul 16 21:12:47 2024 ] 	Batch(2800/6809) done. Loss: 0.0810  lr:0.000001
[ Tue Jul 16 21:13:10 2024 ] 	Batch(2900/6809) done. Loss: 0.0912  lr:0.000001
[ Tue Jul 16 21:13:32 2024 ] 
Training: Epoch [125/150], Step [2999], Loss: 0.21040545403957367, Training Accuracy: 95.57083333333334
[ Tue Jul 16 21:13:32 2024 ] 	Batch(3000/6809) done. Loss: 0.0250  lr:0.000001
[ Tue Jul 16 21:13:55 2024 ] 	Batch(3100/6809) done. Loss: 0.0313  lr:0.000001
[ Tue Jul 16 21:14:17 2024 ] 	Batch(3200/6809) done. Loss: 0.0890  lr:0.000001
[ Tue Jul 16 21:14:41 2024 ] 	Batch(3300/6809) done. Loss: 0.0146  lr:0.000001
[ Tue Jul 16 21:15:04 2024 ] 	Batch(3400/6809) done. Loss: 0.9712  lr:0.000001
[ Tue Jul 16 21:15:27 2024 ] 
Training: Epoch [125/150], Step [3499], Loss: 0.13617654144763947, Training Accuracy: 95.66428571428571
[ Tue Jul 16 21:15:28 2024 ] 	Batch(3500/6809) done. Loss: 0.0060  lr:0.000001
[ Tue Jul 16 21:15:51 2024 ] 	Batch(3600/6809) done. Loss: 0.1872  lr:0.000001
[ Tue Jul 16 21:16:14 2024 ] 	Batch(3700/6809) done. Loss: 0.3719  lr:0.000001
[ Tue Jul 16 21:16:37 2024 ] 	Batch(3800/6809) done. Loss: 0.1562  lr:0.000001
[ Tue Jul 16 21:17:00 2024 ] 	Batch(3900/6809) done. Loss: 0.0438  lr:0.000001
[ Tue Jul 16 21:17:24 2024 ] 
Training: Epoch [125/150], Step [3999], Loss: 0.39550071954727173, Training Accuracy: 95.61874999999999
[ Tue Jul 16 21:17:24 2024 ] 	Batch(4000/6809) done. Loss: 0.0777  lr:0.000001
[ Tue Jul 16 21:17:47 2024 ] 	Batch(4100/6809) done. Loss: 0.1026  lr:0.000001
[ Tue Jul 16 21:18:09 2024 ] 	Batch(4200/6809) done. Loss: 0.0139  lr:0.000001
[ Tue Jul 16 21:18:32 2024 ] 	Batch(4300/6809) done. Loss: 0.1065  lr:0.000001
[ Tue Jul 16 21:18:55 2024 ] 	Batch(4400/6809) done. Loss: 0.0246  lr:0.000001
[ Tue Jul 16 21:19:17 2024 ] 
Training: Epoch [125/150], Step [4499], Loss: 0.23390543460845947, Training Accuracy: 95.6888888888889
[ Tue Jul 16 21:19:17 2024 ] 	Batch(4500/6809) done. Loss: 0.0863  lr:0.000001
[ Tue Jul 16 21:19:40 2024 ] 	Batch(4600/6809) done. Loss: 0.1646  lr:0.000001
[ Tue Jul 16 21:20:02 2024 ] 	Batch(4700/6809) done. Loss: 0.0654  lr:0.000001
[ Tue Jul 16 21:20:25 2024 ] 	Batch(4800/6809) done. Loss: 0.1176  lr:0.000001
[ Tue Jul 16 21:20:48 2024 ] 	Batch(4900/6809) done. Loss: 0.1969  lr:0.000001
[ Tue Jul 16 21:21:10 2024 ] 
Training: Epoch [125/150], Step [4999], Loss: 0.21083693206310272, Training Accuracy: 95.72
[ Tue Jul 16 21:21:10 2024 ] 	Batch(5000/6809) done. Loss: 0.0274  lr:0.000001
[ Tue Jul 16 21:21:33 2024 ] 	Batch(5100/6809) done. Loss: 0.1894  lr:0.000001
[ Tue Jul 16 21:21:56 2024 ] 	Batch(5200/6809) done. Loss: 0.0740  lr:0.000001
[ Tue Jul 16 21:22:19 2024 ] 	Batch(5300/6809) done. Loss: 0.1448  lr:0.000001
[ Tue Jul 16 21:22:41 2024 ] 	Batch(5400/6809) done. Loss: 0.0306  lr:0.000001
[ Tue Jul 16 21:23:04 2024 ] 
Training: Epoch [125/150], Step [5499], Loss: 0.12385562807321548, Training Accuracy: 95.72954545454544
[ Tue Jul 16 21:23:04 2024 ] 	Batch(5500/6809) done. Loss: 0.0754  lr:0.000001
[ Tue Jul 16 21:23:27 2024 ] 	Batch(5600/6809) done. Loss: 0.2435  lr:0.000001
[ Tue Jul 16 21:23:50 2024 ] 	Batch(5700/6809) done. Loss: 0.3981  lr:0.000001
[ Tue Jul 16 21:24:12 2024 ] 	Batch(5800/6809) done. Loss: 0.0376  lr:0.000001
[ Tue Jul 16 21:24:35 2024 ] 	Batch(5900/6809) done. Loss: 0.3916  lr:0.000001
[ Tue Jul 16 21:24:58 2024 ] 
Training: Epoch [125/150], Step [5999], Loss: 0.11152659356594086, Training Accuracy: 95.76041666666667
[ Tue Jul 16 21:24:58 2024 ] 	Batch(6000/6809) done. Loss: 0.1691  lr:0.000001
[ Tue Jul 16 21:25:21 2024 ] 	Batch(6100/6809) done. Loss: 0.0046  lr:0.000001
[ Tue Jul 16 21:25:44 2024 ] 	Batch(6200/6809) done. Loss: 0.1455  lr:0.000001
[ Tue Jul 16 21:26:06 2024 ] 	Batch(6300/6809) done. Loss: 0.1690  lr:0.000001
[ Tue Jul 16 21:26:29 2024 ] 	Batch(6400/6809) done. Loss: 0.1493  lr:0.000001
[ Tue Jul 16 21:26:52 2024 ] 
Training: Epoch [125/150], Step [6499], Loss: 0.048279374837875366, Training Accuracy: 95.75961538461539
[ Tue Jul 16 21:26:52 2024 ] 	Batch(6500/6809) done. Loss: 0.1517  lr:0.000001
[ Tue Jul 16 21:27:15 2024 ] 	Batch(6600/6809) done. Loss: 0.0456  lr:0.000001
[ Tue Jul 16 21:27:37 2024 ] 	Batch(6700/6809) done. Loss: 0.0238  lr:0.000001
[ Tue Jul 16 21:28:00 2024 ] 	Batch(6800/6809) done. Loss: 0.4048  lr:0.000001
[ Tue Jul 16 21:28:02 2024 ] 	Mean training loss: 0.1533.
[ Tue Jul 16 21:28:02 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 21:28:02 2024 ] Training epoch: 127
[ Tue Jul 16 21:28:03 2024 ] 	Batch(0/6809) done. Loss: 0.2701  lr:0.000001
[ Tue Jul 16 21:28:26 2024 ] 	Batch(100/6809) done. Loss: 0.0408  lr:0.000001
[ Tue Jul 16 21:28:49 2024 ] 	Batch(200/6809) done. Loss: 0.1170  lr:0.000001
[ Tue Jul 16 21:29:12 2024 ] 	Batch(300/6809) done. Loss: 0.0474  lr:0.000001
[ Tue Jul 16 21:29:35 2024 ] 	Batch(400/6809) done. Loss: 0.1731  lr:0.000001
[ Tue Jul 16 21:29:58 2024 ] 
Training: Epoch [126/150], Step [499], Loss: 0.1099756509065628, Training Accuracy: 96.175
[ Tue Jul 16 21:29:58 2024 ] 	Batch(500/6809) done. Loss: 0.3699  lr:0.000001
[ Tue Jul 16 21:30:21 2024 ] 	Batch(600/6809) done. Loss: 0.0735  lr:0.000001
[ Tue Jul 16 21:30:43 2024 ] 	Batch(700/6809) done. Loss: 0.0461  lr:0.000001
[ Tue Jul 16 21:31:06 2024 ] 	Batch(800/6809) done. Loss: 0.0636  lr:0.000001
[ Tue Jul 16 21:31:28 2024 ] 	Batch(900/6809) done. Loss: 0.3566  lr:0.000001
[ Tue Jul 16 21:31:51 2024 ] 
Training: Epoch [126/150], Step [999], Loss: 0.028091249987483025, Training Accuracy: 95.8875
[ Tue Jul 16 21:31:51 2024 ] 	Batch(1000/6809) done. Loss: 0.0466  lr:0.000001
[ Tue Jul 16 21:32:13 2024 ] 	Batch(1100/6809) done. Loss: 0.0347  lr:0.000001
[ Tue Jul 16 21:32:36 2024 ] 	Batch(1200/6809) done. Loss: 0.1919  lr:0.000001
[ Tue Jul 16 21:32:59 2024 ] 	Batch(1300/6809) done. Loss: 0.1282  lr:0.000001
[ Tue Jul 16 21:33:21 2024 ] 	Batch(1400/6809) done. Loss: 0.0410  lr:0.000001
[ Tue Jul 16 21:33:44 2024 ] 
Training: Epoch [126/150], Step [1499], Loss: 0.0260559543967247, Training Accuracy: 95.91666666666666
[ Tue Jul 16 21:33:44 2024 ] 	Batch(1500/6809) done. Loss: 0.6606  lr:0.000001
[ Tue Jul 16 21:34:06 2024 ] 	Batch(1600/6809) done. Loss: 0.3393  lr:0.000001
[ Tue Jul 16 21:34:29 2024 ] 	Batch(1700/6809) done. Loss: 0.5506  lr:0.000001
[ Tue Jul 16 21:34:52 2024 ] 	Batch(1800/6809) done. Loss: 0.0361  lr:0.000001
[ Tue Jul 16 21:35:14 2024 ] 	Batch(1900/6809) done. Loss: 0.2809  lr:0.000001
[ Tue Jul 16 21:35:37 2024 ] 
Training: Epoch [126/150], Step [1999], Loss: 0.07326009124517441, Training Accuracy: 95.95625
[ Tue Jul 16 21:35:37 2024 ] 	Batch(2000/6809) done. Loss: 0.1938  lr:0.000001
[ Tue Jul 16 21:36:00 2024 ] 	Batch(2100/6809) done. Loss: 0.2440  lr:0.000001
[ Tue Jul 16 21:36:22 2024 ] 	Batch(2200/6809) done. Loss: 0.0630  lr:0.000001
[ Tue Jul 16 21:36:45 2024 ] 	Batch(2300/6809) done. Loss: 0.0199  lr:0.000001
[ Tue Jul 16 21:37:08 2024 ] 	Batch(2400/6809) done. Loss: 0.0527  lr:0.000001
[ Tue Jul 16 21:37:31 2024 ] 
Training: Epoch [126/150], Step [2499], Loss: 0.5614729523658752, Training Accuracy: 95.94
[ Tue Jul 16 21:37:31 2024 ] 	Batch(2500/6809) done. Loss: 0.4042  lr:0.000001
[ Tue Jul 16 21:37:54 2024 ] 	Batch(2600/6809) done. Loss: 0.7322  lr:0.000001
[ Tue Jul 16 21:38:17 2024 ] 	Batch(2700/6809) done. Loss: 0.0208  lr:0.000001
[ Tue Jul 16 21:38:40 2024 ] 	Batch(2800/6809) done. Loss: 0.1004  lr:0.000001
[ Tue Jul 16 21:39:03 2024 ] 	Batch(2900/6809) done. Loss: 0.0708  lr:0.000001
[ Tue Jul 16 21:39:26 2024 ] 
Training: Epoch [126/150], Step [2999], Loss: 0.1723995804786682, Training Accuracy: 95.9375
[ Tue Jul 16 21:39:26 2024 ] 	Batch(3000/6809) done. Loss: 0.0111  lr:0.000001
[ Tue Jul 16 21:39:49 2024 ] 	Batch(3100/6809) done. Loss: 0.4470  lr:0.000001
[ Tue Jul 16 21:40:11 2024 ] 	Batch(3200/6809) done. Loss: 0.1565  lr:0.000001
[ Tue Jul 16 21:40:34 2024 ] 	Batch(3300/6809) done. Loss: 0.0255  lr:0.000001
[ Tue Jul 16 21:40:56 2024 ] 	Batch(3400/6809) done. Loss: 0.0558  lr:0.000001
[ Tue Jul 16 21:41:19 2024 ] 
Training: Epoch [126/150], Step [3499], Loss: 0.0012636793544515967, Training Accuracy: 95.83214285714286
[ Tue Jul 16 21:41:19 2024 ] 	Batch(3500/6809) done. Loss: 1.0691  lr:0.000001
[ Tue Jul 16 21:41:42 2024 ] 	Batch(3600/6809) done. Loss: 0.1121  lr:0.000001
[ Tue Jul 16 21:42:04 2024 ] 	Batch(3700/6809) done. Loss: 0.2909  lr:0.000001
[ Tue Jul 16 21:42:27 2024 ] 	Batch(3800/6809) done. Loss: 0.0757  lr:0.000001
[ Tue Jul 16 21:42:49 2024 ] 	Batch(3900/6809) done. Loss: 0.0835  lr:0.000001
[ Tue Jul 16 21:43:12 2024 ] 
Training: Epoch [126/150], Step [3999], Loss: 0.09987359493970871, Training Accuracy: 95.809375
[ Tue Jul 16 21:43:12 2024 ] 	Batch(4000/6809) done. Loss: 0.1591  lr:0.000001
[ Tue Jul 16 21:43:35 2024 ] 	Batch(4100/6809) done. Loss: 0.0818  lr:0.000001
[ Tue Jul 16 21:43:57 2024 ] 	Batch(4200/6809) done. Loss: 0.0384  lr:0.000001
[ Tue Jul 16 21:44:20 2024 ] 	Batch(4300/6809) done. Loss: 0.2642  lr:0.000001
[ Tue Jul 16 21:44:42 2024 ] 	Batch(4400/6809) done. Loss: 0.0279  lr:0.000001
[ Tue Jul 16 21:45:05 2024 ] 
Training: Epoch [126/150], Step [4499], Loss: 0.0441712960600853, Training Accuracy: 95.81944444444444
[ Tue Jul 16 21:45:05 2024 ] 	Batch(4500/6809) done. Loss: 0.0957  lr:0.000001
[ Tue Jul 16 21:45:27 2024 ] 	Batch(4600/6809) done. Loss: 0.1217  lr:0.000001
[ Tue Jul 16 21:45:50 2024 ] 	Batch(4700/6809) done. Loss: 0.6861  lr:0.000001
[ Tue Jul 16 21:46:13 2024 ] 	Batch(4800/6809) done. Loss: 0.0156  lr:0.000001
[ Tue Jul 16 21:46:35 2024 ] 	Batch(4900/6809) done. Loss: 0.3184  lr:0.000001
[ Tue Jul 16 21:46:58 2024 ] 
Training: Epoch [126/150], Step [4999], Loss: 0.006678104866296053, Training Accuracy: 95.8025
[ Tue Jul 16 21:46:58 2024 ] 	Batch(5000/6809) done. Loss: 0.3187  lr:0.000001
[ Tue Jul 16 21:47:21 2024 ] 	Batch(5100/6809) done. Loss: 0.1992  lr:0.000001
[ Tue Jul 16 21:47:45 2024 ] 	Batch(5200/6809) done. Loss: 0.0557  lr:0.000001
[ Tue Jul 16 21:48:08 2024 ] 	Batch(5300/6809) done. Loss: 0.0093  lr:0.000001
[ Tue Jul 16 21:48:31 2024 ] 	Batch(5400/6809) done. Loss: 0.2756  lr:0.000001
[ Tue Jul 16 21:48:54 2024 ] 
Training: Epoch [126/150], Step [5499], Loss: 0.08430137485265732, Training Accuracy: 95.76136363636364
[ Tue Jul 16 21:48:54 2024 ] 	Batch(5500/6809) done. Loss: 0.2933  lr:0.000001
[ Tue Jul 16 21:49:17 2024 ] 	Batch(5600/6809) done. Loss: 0.0268  lr:0.000001
[ Tue Jul 16 21:49:40 2024 ] 	Batch(5700/6809) done. Loss: 0.0527  lr:0.000001
[ Tue Jul 16 21:50:03 2024 ] 	Batch(5800/6809) done. Loss: 0.2680  lr:0.000001
[ Tue Jul 16 21:50:26 2024 ] 	Batch(5900/6809) done. Loss: 0.2688  lr:0.000001
[ Tue Jul 16 21:50:49 2024 ] 
Training: Epoch [126/150], Step [5999], Loss: 0.28065726161003113, Training Accuracy: 95.74583333333334
[ Tue Jul 16 21:50:49 2024 ] 	Batch(6000/6809) done. Loss: 0.3766  lr:0.000001
[ Tue Jul 16 21:51:12 2024 ] 	Batch(6100/6809) done. Loss: 0.1706  lr:0.000001
[ Tue Jul 16 21:51:35 2024 ] 	Batch(6200/6809) done. Loss: 0.1200  lr:0.000001
[ Tue Jul 16 21:51:58 2024 ] 	Batch(6300/6809) done. Loss: 0.0701  lr:0.000001
[ Tue Jul 16 21:52:21 2024 ] 	Batch(6400/6809) done. Loss: 0.1971  lr:0.000001
[ Tue Jul 16 21:52:43 2024 ] 
Training: Epoch [126/150], Step [6499], Loss: 0.30461233854293823, Training Accuracy: 95.76346153846154
[ Tue Jul 16 21:52:44 2024 ] 	Batch(6500/6809) done. Loss: 0.1248  lr:0.000001
[ Tue Jul 16 21:53:06 2024 ] 	Batch(6600/6809) done. Loss: 0.0508  lr:0.000001
[ Tue Jul 16 21:53:29 2024 ] 	Batch(6700/6809) done. Loss: 0.0716  lr:0.000001
[ Tue Jul 16 21:53:51 2024 ] 	Batch(6800/6809) done. Loss: 0.1483  lr:0.000001
[ Tue Jul 16 21:53:53 2024 ] 	Mean training loss: 0.1527.
[ Tue Jul 16 21:53:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 21:53:53 2024 ] Training epoch: 128
[ Tue Jul 16 21:53:54 2024 ] 	Batch(0/6809) done. Loss: 0.0323  lr:0.000001
[ Tue Jul 16 21:54:17 2024 ] 	Batch(100/6809) done. Loss: 0.1362  lr:0.000001
[ Tue Jul 16 21:54:40 2024 ] 	Batch(200/6809) done. Loss: 0.0551  lr:0.000001
[ Tue Jul 16 21:55:03 2024 ] 	Batch(300/6809) done. Loss: 0.1326  lr:0.000001
[ Tue Jul 16 21:55:25 2024 ] 	Batch(400/6809) done. Loss: 0.0781  lr:0.000001
[ Tue Jul 16 21:55:48 2024 ] 
Training: Epoch [127/150], Step [499], Loss: 0.07886979728937149, Training Accuracy: 96.025
[ Tue Jul 16 21:55:48 2024 ] 	Batch(500/6809) done. Loss: 0.1357  lr:0.000001
[ Tue Jul 16 21:56:11 2024 ] 	Batch(600/6809) done. Loss: 0.3516  lr:0.000001
[ Tue Jul 16 21:56:33 2024 ] 	Batch(700/6809) done. Loss: 0.2466  lr:0.000001
[ Tue Jul 16 21:56:56 2024 ] 	Batch(800/6809) done. Loss: 0.0576  lr:0.000001
[ Tue Jul 16 21:57:18 2024 ] 	Batch(900/6809) done. Loss: 0.0331  lr:0.000001
[ Tue Jul 16 21:57:41 2024 ] 
Training: Epoch [127/150], Step [999], Loss: 0.10242922604084015, Training Accuracy: 95.5
[ Tue Jul 16 21:57:41 2024 ] 	Batch(1000/6809) done. Loss: 0.1817  lr:0.000001
[ Tue Jul 16 21:58:03 2024 ] 	Batch(1100/6809) done. Loss: 0.0770  lr:0.000001
[ Tue Jul 16 21:58:26 2024 ] 	Batch(1200/6809) done. Loss: 0.2892  lr:0.000001
[ Tue Jul 16 21:58:49 2024 ] 	Batch(1300/6809) done. Loss: 0.3978  lr:0.000001
[ Tue Jul 16 21:59:11 2024 ] 	Batch(1400/6809) done. Loss: 0.1124  lr:0.000001
[ Tue Jul 16 21:59:34 2024 ] 
Training: Epoch [127/150], Step [1499], Loss: 0.006760654971003532, Training Accuracy: 95.56666666666666
[ Tue Jul 16 21:59:34 2024 ] 	Batch(1500/6809) done. Loss: 0.0584  lr:0.000001
[ Tue Jul 16 21:59:56 2024 ] 	Batch(1600/6809) done. Loss: 0.0125  lr:0.000001
[ Tue Jul 16 22:00:19 2024 ] 	Batch(1700/6809) done. Loss: 0.0407  lr:0.000001
[ Tue Jul 16 22:00:42 2024 ] 	Batch(1800/6809) done. Loss: 0.0641  lr:0.000001
[ Tue Jul 16 22:01:04 2024 ] 	Batch(1900/6809) done. Loss: 0.0814  lr:0.000001
[ Tue Jul 16 22:01:26 2024 ] 
Training: Epoch [127/150], Step [1999], Loss: 0.03867455944418907, Training Accuracy: 95.575
[ Tue Jul 16 22:01:27 2024 ] 	Batch(2000/6809) done. Loss: 0.1775  lr:0.000001
[ Tue Jul 16 22:01:49 2024 ] 	Batch(2100/6809) done. Loss: 0.3389  lr:0.000001
[ Tue Jul 16 22:02:12 2024 ] 	Batch(2200/6809) done. Loss: 0.1950  lr:0.000001
[ Tue Jul 16 22:02:34 2024 ] 	Batch(2300/6809) done. Loss: 0.1646  lr:0.000001
[ Tue Jul 16 22:02:57 2024 ] 	Batch(2400/6809) done. Loss: 0.0727  lr:0.000001
[ Tue Jul 16 22:03:20 2024 ] 
Training: Epoch [127/150], Step [2499], Loss: 0.023475827649235725, Training Accuracy: 95.655
[ Tue Jul 16 22:03:20 2024 ] 	Batch(2500/6809) done. Loss: 0.0391  lr:0.000001
[ Tue Jul 16 22:03:43 2024 ] 	Batch(2600/6809) done. Loss: 0.0658  lr:0.000001
[ Tue Jul 16 22:04:06 2024 ] 	Batch(2700/6809) done. Loss: 0.1303  lr:0.000001
[ Tue Jul 16 22:04:29 2024 ] 	Batch(2800/6809) done. Loss: 0.1325  lr:0.000001
[ Tue Jul 16 22:04:53 2024 ] 	Batch(2900/6809) done. Loss: 0.1111  lr:0.000001
[ Tue Jul 16 22:05:16 2024 ] 
Training: Epoch [127/150], Step [2999], Loss: 0.4880504608154297, Training Accuracy: 95.66666666666667
[ Tue Jul 16 22:05:17 2024 ] 	Batch(3000/6809) done. Loss: 0.4428  lr:0.000001
[ Tue Jul 16 22:05:40 2024 ] 	Batch(3100/6809) done. Loss: 0.0126  lr:0.000001
[ Tue Jul 16 22:06:03 2024 ] 	Batch(3200/6809) done. Loss: 0.0081  lr:0.000001
[ Tue Jul 16 22:06:26 2024 ] 	Batch(3300/6809) done. Loss: 0.5777  lr:0.000001
[ Tue Jul 16 22:06:49 2024 ] 	Batch(3400/6809) done. Loss: 1.0567  lr:0.000001
[ Tue Jul 16 22:07:12 2024 ] 
Training: Epoch [127/150], Step [3499], Loss: 0.07701494544744492, Training Accuracy: 95.69285714285715
[ Tue Jul 16 22:07:12 2024 ] 	Batch(3500/6809) done. Loss: 0.2393  lr:0.000001
[ Tue Jul 16 22:07:34 2024 ] 	Batch(3600/6809) done. Loss: 0.0292  lr:0.000001
[ Tue Jul 16 22:07:57 2024 ] 	Batch(3700/6809) done. Loss: 0.1103  lr:0.000001
[ Tue Jul 16 22:08:20 2024 ] 	Batch(3800/6809) done. Loss: 0.0624  lr:0.000001
[ Tue Jul 16 22:08:42 2024 ] 	Batch(3900/6809) done. Loss: 0.1028  lr:0.000001
[ Tue Jul 16 22:09:04 2024 ] 
Training: Epoch [127/150], Step [3999], Loss: 0.36570581793785095, Training Accuracy: 95.67812500000001
[ Tue Jul 16 22:09:05 2024 ] 	Batch(4000/6809) done. Loss: 0.0347  lr:0.000001
[ Tue Jul 16 22:09:28 2024 ] 	Batch(4100/6809) done. Loss: 0.0429  lr:0.000001
[ Tue Jul 16 22:09:51 2024 ] 	Batch(4200/6809) done. Loss: 0.6965  lr:0.000001
[ Tue Jul 16 22:10:14 2024 ] 	Batch(4300/6809) done. Loss: 0.0163  lr:0.000001
[ Tue Jul 16 22:10:37 2024 ] 	Batch(4400/6809) done. Loss: 0.1732  lr:0.000001
[ Tue Jul 16 22:11:00 2024 ] 
Training: Epoch [127/150], Step [4499], Loss: 0.06271909177303314, Training Accuracy: 95.675
[ Tue Jul 16 22:11:00 2024 ] 	Batch(4500/6809) done. Loss: 0.1720  lr:0.000001
[ Tue Jul 16 22:11:23 2024 ] 	Batch(4600/6809) done. Loss: 0.1322  lr:0.000001
[ Tue Jul 16 22:11:46 2024 ] 	Batch(4700/6809) done. Loss: 0.0174  lr:0.000001
[ Tue Jul 16 22:12:09 2024 ] 	Batch(4800/6809) done. Loss: 0.2750  lr:0.000001
[ Tue Jul 16 22:12:32 2024 ] 	Batch(4900/6809) done. Loss: 0.1936  lr:0.000001
[ Tue Jul 16 22:12:55 2024 ] 
Training: Epoch [127/150], Step [4999], Loss: 0.018734823912382126, Training Accuracy: 95.7075
[ Tue Jul 16 22:12:55 2024 ] 	Batch(5000/6809) done. Loss: 0.0467  lr:0.000001
[ Tue Jul 16 22:13:18 2024 ] 	Batch(5100/6809) done. Loss: 0.0036  lr:0.000001
[ Tue Jul 16 22:13:41 2024 ] 	Batch(5200/6809) done. Loss: 0.2928  lr:0.000001
[ Tue Jul 16 22:14:04 2024 ] 	Batch(5300/6809) done. Loss: 0.0104  lr:0.000001
[ Tue Jul 16 22:14:27 2024 ] 	Batch(5400/6809) done. Loss: 0.0404  lr:0.000001
[ Tue Jul 16 22:14:50 2024 ] 
Training: Epoch [127/150], Step [5499], Loss: 0.019565369933843613, Training Accuracy: 95.71363636363637
[ Tue Jul 16 22:14:50 2024 ] 	Batch(5500/6809) done. Loss: 0.1677  lr:0.000001
[ Tue Jul 16 22:15:13 2024 ] 	Batch(5600/6809) done. Loss: 0.1849  lr:0.000001
[ Tue Jul 16 22:15:36 2024 ] 	Batch(5700/6809) done. Loss: 0.0932  lr:0.000001
[ Tue Jul 16 22:15:59 2024 ] 	Batch(5800/6809) done. Loss: 0.0209  lr:0.000001
[ Tue Jul 16 22:16:22 2024 ] 	Batch(5900/6809) done. Loss: 0.0543  lr:0.000001
[ Tue Jul 16 22:16:45 2024 ] 
Training: Epoch [127/150], Step [5999], Loss: 0.09454743564128876, Training Accuracy: 95.68541666666667
[ Tue Jul 16 22:16:45 2024 ] 	Batch(6000/6809) done. Loss: 0.2757  lr:0.000001
[ Tue Jul 16 22:17:09 2024 ] 	Batch(6100/6809) done. Loss: 0.2422  lr:0.000001
[ Tue Jul 16 22:17:33 2024 ] 	Batch(6200/6809) done. Loss: 0.1451  lr:0.000001
[ Tue Jul 16 22:17:57 2024 ] 	Batch(6300/6809) done. Loss: 0.2652  lr:0.000001
[ Tue Jul 16 22:18:21 2024 ] 	Batch(6400/6809) done. Loss: 0.0510  lr:0.000001
[ Tue Jul 16 22:18:45 2024 ] 
Training: Epoch [127/150], Step [6499], Loss: 0.04343556612730026, Training Accuracy: 95.6423076923077
[ Tue Jul 16 22:18:45 2024 ] 	Batch(6500/6809) done. Loss: 0.7782  lr:0.000001
[ Tue Jul 16 22:19:08 2024 ] 	Batch(6600/6809) done. Loss: 0.0796  lr:0.000001
[ Tue Jul 16 22:19:31 2024 ] 	Batch(6700/6809) done. Loss: 0.0588  lr:0.000001
[ Tue Jul 16 22:19:54 2024 ] 	Batch(6800/6809) done. Loss: 0.0093  lr:0.000001
[ Tue Jul 16 22:19:56 2024 ] 	Mean training loss: 0.1554.
[ Tue Jul 16 22:19:56 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 22:19:56 2024 ] Training epoch: 129
[ Tue Jul 16 22:19:57 2024 ] 	Batch(0/6809) done. Loss: 0.0065  lr:0.000001
[ Tue Jul 16 22:20:20 2024 ] 	Batch(100/6809) done. Loss: 0.0732  lr:0.000001
[ Tue Jul 16 22:20:42 2024 ] 	Batch(200/6809) done. Loss: 0.0109  lr:0.000001
[ Tue Jul 16 22:21:05 2024 ] 	Batch(300/6809) done. Loss: 0.0493  lr:0.000001
[ Tue Jul 16 22:21:28 2024 ] 	Batch(400/6809) done. Loss: 0.1838  lr:0.000001
[ Tue Jul 16 22:21:50 2024 ] 
Training: Epoch [128/150], Step [499], Loss: 0.0744287371635437, Training Accuracy: 96.72500000000001
[ Tue Jul 16 22:21:50 2024 ] 	Batch(500/6809) done. Loss: 0.2840  lr:0.000001
[ Tue Jul 16 22:22:13 2024 ] 	Batch(600/6809) done. Loss: 0.0333  lr:0.000001
[ Tue Jul 16 22:22:35 2024 ] 	Batch(700/6809) done. Loss: 0.0777  lr:0.000001
[ Tue Jul 16 22:22:58 2024 ] 	Batch(800/6809) done. Loss: 0.2611  lr:0.000001
[ Tue Jul 16 22:23:21 2024 ] 	Batch(900/6809) done. Loss: 0.0145  lr:0.000001
[ Tue Jul 16 22:23:44 2024 ] 
Training: Epoch [128/150], Step [999], Loss: 0.007727668154984713, Training Accuracy: 96.26249999999999
[ Tue Jul 16 22:23:44 2024 ] 	Batch(1000/6809) done. Loss: 0.2362  lr:0.000001
[ Tue Jul 16 22:24:07 2024 ] 	Batch(1100/6809) done. Loss: 0.0267  lr:0.000001
[ Tue Jul 16 22:24:30 2024 ] 	Batch(1200/6809) done. Loss: 0.1972  lr:0.000001
[ Tue Jul 16 22:24:53 2024 ] 	Batch(1300/6809) done. Loss: 0.0319  lr:0.000001
[ Tue Jul 16 22:25:16 2024 ] 	Batch(1400/6809) done. Loss: 0.0175  lr:0.000001
[ Tue Jul 16 22:25:39 2024 ] 
Training: Epoch [128/150], Step [1499], Loss: 0.05496558919548988, Training Accuracy: 96.05833333333334
[ Tue Jul 16 22:25:39 2024 ] 	Batch(1500/6809) done. Loss: 0.0627  lr:0.000001
[ Tue Jul 16 22:26:02 2024 ] 	Batch(1600/6809) done. Loss: 0.0295  lr:0.000001
[ Tue Jul 16 22:26:25 2024 ] 	Batch(1700/6809) done. Loss: 0.1901  lr:0.000001
[ Tue Jul 16 22:26:48 2024 ] 	Batch(1800/6809) done. Loss: 0.0109  lr:0.000001
[ Tue Jul 16 22:27:11 2024 ] 	Batch(1900/6809) done. Loss: 0.2064  lr:0.000001
[ Tue Jul 16 22:27:34 2024 ] 
Training: Epoch [128/150], Step [1999], Loss: 0.12307574599981308, Training Accuracy: 95.95
[ Tue Jul 16 22:27:34 2024 ] 	Batch(2000/6809) done. Loss: 0.0282  lr:0.000001
[ Tue Jul 16 22:27:57 2024 ] 	Batch(2100/6809) done. Loss: 0.1100  lr:0.000001
[ Tue Jul 16 22:28:21 2024 ] 	Batch(2200/6809) done. Loss: 0.0020  lr:0.000001
[ Tue Jul 16 22:28:44 2024 ] 	Batch(2300/6809) done. Loss: 0.0416  lr:0.000001
[ Tue Jul 16 22:29:07 2024 ] 	Batch(2400/6809) done. Loss: 0.0198  lr:0.000001
[ Tue Jul 16 22:29:30 2024 ] 
Training: Epoch [128/150], Step [2499], Loss: 0.10000816732645035, Training Accuracy: 95.91
[ Tue Jul 16 22:29:30 2024 ] 	Batch(2500/6809) done. Loss: 0.0522  lr:0.000001
[ Tue Jul 16 22:29:53 2024 ] 	Batch(2600/6809) done. Loss: 0.0242  lr:0.000001
[ Tue Jul 16 22:30:17 2024 ] 	Batch(2700/6809) done. Loss: 0.2284  lr:0.000001
[ Tue Jul 16 22:30:41 2024 ] 	Batch(2800/6809) done. Loss: 0.1095  lr:0.000001
[ Tue Jul 16 22:31:05 2024 ] 	Batch(2900/6809) done. Loss: 0.7441  lr:0.000001
[ Tue Jul 16 22:31:29 2024 ] 
Training: Epoch [128/150], Step [2999], Loss: 0.29591235518455505, Training Accuracy: 95.90833333333333
[ Tue Jul 16 22:31:29 2024 ] 	Batch(3000/6809) done. Loss: 0.2081  lr:0.000001
[ Tue Jul 16 22:31:52 2024 ] 	Batch(3100/6809) done. Loss: 0.3432  lr:0.000001
[ Tue Jul 16 22:32:15 2024 ] 	Batch(3200/6809) done. Loss: 0.0226  lr:0.000001
[ Tue Jul 16 22:32:38 2024 ] 	Batch(3300/6809) done. Loss: 0.1266  lr:0.000001
[ Tue Jul 16 22:33:01 2024 ] 	Batch(3400/6809) done. Loss: 0.0939  lr:0.000001
[ Tue Jul 16 22:33:24 2024 ] 
Training: Epoch [128/150], Step [3499], Loss: 0.4038640558719635, Training Accuracy: 95.81071428571428
[ Tue Jul 16 22:33:25 2024 ] 	Batch(3500/6809) done. Loss: 0.4346  lr:0.000001
[ Tue Jul 16 22:33:48 2024 ] 	Batch(3600/6809) done. Loss: 0.0604  lr:0.000001
[ Tue Jul 16 22:34:11 2024 ] 	Batch(3700/6809) done. Loss: 0.1672  lr:0.000001
[ Tue Jul 16 22:34:35 2024 ] 	Batch(3800/6809) done. Loss: 0.0426  lr:0.000001
[ Tue Jul 16 22:34:59 2024 ] 	Batch(3900/6809) done. Loss: 0.0095  lr:0.000001
[ Tue Jul 16 22:35:22 2024 ] 
Training: Epoch [128/150], Step [3999], Loss: 0.07111099362373352, Training Accuracy: 95.75625
[ Tue Jul 16 22:35:23 2024 ] 	Batch(4000/6809) done. Loss: 0.0225  lr:0.000001
[ Tue Jul 16 22:35:46 2024 ] 	Batch(4100/6809) done. Loss: 0.0187  lr:0.000001
[ Tue Jul 16 22:36:09 2024 ] 	Batch(4200/6809) done. Loss: 0.0988  lr:0.000001
[ Tue Jul 16 22:36:32 2024 ] 	Batch(4300/6809) done. Loss: 0.3036  lr:0.000001
[ Tue Jul 16 22:36:55 2024 ] 	Batch(4400/6809) done. Loss: 0.2819  lr:0.000001
[ Tue Jul 16 22:37:18 2024 ] 
Training: Epoch [128/150], Step [4499], Loss: 0.07403493672609329, Training Accuracy: 95.78055555555555
[ Tue Jul 16 22:37:18 2024 ] 	Batch(4500/6809) done. Loss: 0.0733  lr:0.000001
[ Tue Jul 16 22:37:41 2024 ] 	Batch(4600/6809) done. Loss: 0.3324  lr:0.000001
[ Tue Jul 16 22:38:04 2024 ] 	Batch(4700/6809) done. Loss: 0.0438  lr:0.000001
[ Tue Jul 16 22:38:27 2024 ] 	Batch(4800/6809) done. Loss: 0.0867  lr:0.000001
[ Tue Jul 16 22:38:50 2024 ] 	Batch(4900/6809) done. Loss: 0.3738  lr:0.000001
[ Tue Jul 16 22:39:13 2024 ] 
Training: Epoch [128/150], Step [4999], Loss: 0.4114924669265747, Training Accuracy: 95.7375
[ Tue Jul 16 22:39:14 2024 ] 	Batch(5000/6809) done. Loss: 0.1306  lr:0.000001
[ Tue Jul 16 22:39:37 2024 ] 	Batch(5100/6809) done. Loss: 0.2010  lr:0.000001
[ Tue Jul 16 22:40:00 2024 ] 	Batch(5200/6809) done. Loss: 0.0108  lr:0.000001
[ Tue Jul 16 22:40:23 2024 ] 	Batch(5300/6809) done. Loss: 0.0692  lr:0.000001
[ Tue Jul 16 22:40:46 2024 ] 	Batch(5400/6809) done. Loss: 0.2094  lr:0.000001
[ Tue Jul 16 22:41:09 2024 ] 
Training: Epoch [128/150], Step [5499], Loss: 0.10624387115240097, Training Accuracy: 95.72045454545454
[ Tue Jul 16 22:41:09 2024 ] 	Batch(5500/6809) done. Loss: 0.0496  lr:0.000001
[ Tue Jul 16 22:41:32 2024 ] 	Batch(5600/6809) done. Loss: 0.0565  lr:0.000001
[ Tue Jul 16 22:41:55 2024 ] 	Batch(5700/6809) done. Loss: 0.3647  lr:0.000001
[ Tue Jul 16 22:42:18 2024 ] 	Batch(5800/6809) done. Loss: 0.5612  lr:0.000001
[ Tue Jul 16 22:42:41 2024 ] 	Batch(5900/6809) done. Loss: 0.0094  lr:0.000001
[ Tue Jul 16 22:43:04 2024 ] 
Training: Epoch [128/150], Step [5999], Loss: 0.05491665378212929, Training Accuracy: 95.73541666666667
[ Tue Jul 16 22:43:04 2024 ] 	Batch(6000/6809) done. Loss: 0.3849  lr:0.000001
[ Tue Jul 16 22:43:27 2024 ] 	Batch(6100/6809) done. Loss: 0.0401  lr:0.000001
[ Tue Jul 16 22:43:50 2024 ] 	Batch(6200/6809) done. Loss: 0.2562  lr:0.000001
[ Tue Jul 16 22:44:13 2024 ] 	Batch(6300/6809) done. Loss: 0.0930  lr:0.000001
[ Tue Jul 16 22:44:36 2024 ] 	Batch(6400/6809) done. Loss: 0.1609  lr:0.000001
[ Tue Jul 16 22:44:58 2024 ] 
Training: Epoch [128/150], Step [6499], Loss: 0.575546145439148, Training Accuracy: 95.71923076923076
[ Tue Jul 16 22:44:58 2024 ] 	Batch(6500/6809) done. Loss: 0.1945  lr:0.000001
[ Tue Jul 16 22:45:21 2024 ] 	Batch(6600/6809) done. Loss: 0.0450  lr:0.000001
[ Tue Jul 16 22:45:43 2024 ] 	Batch(6700/6809) done. Loss: 0.2412  lr:0.000001
[ Tue Jul 16 22:46:06 2024 ] 	Batch(6800/6809) done. Loss: 0.0743  lr:0.000001
[ Tue Jul 16 22:46:08 2024 ] 	Mean training loss: 0.1559.
[ Tue Jul 16 22:46:08 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Tue Jul 16 22:46:08 2024 ] Training epoch: 130
[ Tue Jul 16 22:46:09 2024 ] 	Batch(0/6809) done. Loss: 0.0787  lr:0.000001
[ Tue Jul 16 22:46:32 2024 ] 	Batch(100/6809) done. Loss: 0.4618  lr:0.000001
[ Tue Jul 16 22:46:55 2024 ] 	Batch(200/6809) done. Loss: 0.0734  lr:0.000001
[ Tue Jul 16 22:47:18 2024 ] 	Batch(300/6809) done. Loss: 0.1971  lr:0.000001
[ Tue Jul 16 22:47:41 2024 ] 	Batch(400/6809) done. Loss: 0.1220  lr:0.000001
[ Tue Jul 16 22:48:03 2024 ] 
Training: Epoch [129/150], Step [499], Loss: 0.08643695712089539, Training Accuracy: 96.1
[ Tue Jul 16 22:48:04 2024 ] 	Batch(500/6809) done. Loss: 0.2895  lr:0.000001
[ Tue Jul 16 22:48:26 2024 ] 	Batch(600/6809) done. Loss: 0.4218  lr:0.000001
[ Tue Jul 16 22:48:49 2024 ] 	Batch(700/6809) done. Loss: 0.1833  lr:0.000001
[ Tue Jul 16 22:49:12 2024 ] 	Batch(800/6809) done. Loss: 0.0129  lr:0.000001
[ Tue Jul 16 22:49:35 2024 ] 	Batch(900/6809) done. Loss: 0.0299  lr:0.000001
[ Tue Jul 16 22:49:58 2024 ] 
Training: Epoch [129/150], Step [999], Loss: 0.09229541569948196, Training Accuracy: 96.1625
[ Tue Jul 16 22:49:59 2024 ] 	Batch(1000/6809) done. Loss: 0.3655  lr:0.000001
[ Tue Jul 16 22:50:22 2024 ] 	Batch(1100/6809) done. Loss: 0.0188  lr:0.000001
[ Tue Jul 16 22:50:45 2024 ] 	Batch(1200/6809) done. Loss: 0.0950  lr:0.000001
[ Tue Jul 16 22:51:08 2024 ] 	Batch(1300/6809) done. Loss: 0.2959  lr:0.000001
[ Tue Jul 16 22:51:32 2024 ] 	Batch(1400/6809) done. Loss: 0.0257  lr:0.000001
[ Tue Jul 16 22:51:55 2024 ] 
Training: Epoch [129/150], Step [1499], Loss: 0.062178149819374084, Training Accuracy: 96.0
[ Tue Jul 16 22:51:55 2024 ] 	Batch(1500/6809) done. Loss: 0.0592  lr:0.000001
[ Tue Jul 16 22:52:18 2024 ] 	Batch(1600/6809) done. Loss: 0.0328  lr:0.000001
[ Tue Jul 16 22:52:41 2024 ] 	Batch(1700/6809) done. Loss: 0.2294  lr:0.000001
[ Tue Jul 16 22:53:04 2024 ] 	Batch(1800/6809) done. Loss: 0.0051  lr:0.000001
[ Tue Jul 16 22:53:27 2024 ] 	Batch(1900/6809) done. Loss: 0.0076  lr:0.000001
[ Tue Jul 16 22:53:50 2024 ] 
Training: Epoch [129/150], Step [1999], Loss: 0.03951261565089226, Training Accuracy: 96.14375
[ Tue Jul 16 22:53:50 2024 ] 	Batch(2000/6809) done. Loss: 0.0344  lr:0.000001
[ Tue Jul 16 22:54:13 2024 ] 	Batch(2100/6809) done. Loss: 0.1174  lr:0.000001
[ Tue Jul 16 22:54:37 2024 ] 	Batch(2200/6809) done. Loss: 0.1825  lr:0.000001
[ Tue Jul 16 22:55:00 2024 ] 	Batch(2300/6809) done. Loss: 0.0187  lr:0.000001
[ Tue Jul 16 22:55:23 2024 ] 	Batch(2400/6809) done. Loss: 0.0308  lr:0.000001
[ Tue Jul 16 22:55:45 2024 ] 
Training: Epoch [129/150], Step [2499], Loss: 0.11896607279777527, Training Accuracy: 96.135
[ Tue Jul 16 22:55:45 2024 ] 	Batch(2500/6809) done. Loss: 0.2447  lr:0.000001
[ Tue Jul 16 22:56:08 2024 ] 	Batch(2600/6809) done. Loss: 0.0058  lr:0.000001
[ Tue Jul 16 22:56:31 2024 ] 	Batch(2700/6809) done. Loss: 0.0330  lr:0.000001
[ Tue Jul 16 22:56:53 2024 ] 	Batch(2800/6809) done. Loss: 0.2122  lr:0.000001
[ Tue Jul 16 22:57:16 2024 ] 	Batch(2900/6809) done. Loss: 0.3836  lr:0.000001
[ Tue Jul 16 22:57:38 2024 ] 
Training: Epoch [129/150], Step [2999], Loss: 0.48256149888038635, Training Accuracy: 96.00416666666666
[ Tue Jul 16 22:57:38 2024 ] 	Batch(3000/6809) done. Loss: 0.0641  lr:0.000001
[ Tue Jul 16 22:58:01 2024 ] 	Batch(3100/6809) done. Loss: 0.2334  lr:0.000001
[ Tue Jul 16 22:58:24 2024 ] 	Batch(3200/6809) done. Loss: 0.1515  lr:0.000001
[ Tue Jul 16 22:58:46 2024 ] 	Batch(3300/6809) done. Loss: 0.1448  lr:0.000001
[ Tue Jul 16 22:59:09 2024 ] 	Batch(3400/6809) done. Loss: 0.0278  lr:0.000001
[ Tue Jul 16 22:59:31 2024 ] 
Training: Epoch [129/150], Step [3499], Loss: 0.1232251226902008, Training Accuracy: 95.95714285714286
[ Tue Jul 16 22:59:32 2024 ] 	Batch(3500/6809) done. Loss: 0.0899  lr:0.000001
[ Tue Jul 16 22:59:54 2024 ] 	Batch(3600/6809) done. Loss: 0.4181  lr:0.000001
[ Tue Jul 16 23:00:17 2024 ] 	Batch(3700/6809) done. Loss: 0.0635  lr:0.000001
[ Tue Jul 16 23:00:40 2024 ] 	Batch(3800/6809) done. Loss: 0.1616  lr:0.000001
[ Tue Jul 16 23:01:02 2024 ] 	Batch(3900/6809) done. Loss: 0.0569  lr:0.000001
[ Tue Jul 16 23:01:25 2024 ] 
Training: Epoch [129/150], Step [3999], Loss: 0.28089088201522827, Training Accuracy: 95.95
[ Tue Jul 16 23:01:25 2024 ] 	Batch(4000/6809) done. Loss: 0.0487  lr:0.000001
[ Tue Jul 16 23:01:48 2024 ] 	Batch(4100/6809) done. Loss: 0.3719  lr:0.000001
[ Tue Jul 16 23:02:10 2024 ] 	Batch(4200/6809) done. Loss: 0.0174  lr:0.000001
[ Tue Jul 16 23:02:33 2024 ] 	Batch(4300/6809) done. Loss: 0.0206  lr:0.000001
[ Tue Jul 16 23:02:56 2024 ] 	Batch(4400/6809) done. Loss: 0.0982  lr:0.000001
[ Tue Jul 16 23:03:18 2024 ] 
Training: Epoch [129/150], Step [4499], Loss: 0.15055863559246063, Training Accuracy: 95.93333333333334
[ Tue Jul 16 23:03:18 2024 ] 	Batch(4500/6809) done. Loss: 0.1389  lr:0.000001
[ Tue Jul 16 23:03:41 2024 ] 	Batch(4600/6809) done. Loss: 0.3845  lr:0.000001
[ Tue Jul 16 23:04:03 2024 ] 	Batch(4700/6809) done. Loss: 0.2093  lr:0.000001
[ Tue Jul 16 23:04:26 2024 ] 	Batch(4800/6809) done. Loss: 0.2550  lr:0.000001
[ Tue Jul 16 23:04:49 2024 ] 	Batch(4900/6809) done. Loss: 0.3638  lr:0.000001
[ Tue Jul 16 23:05:11 2024 ] 
Training: Epoch [129/150], Step [4999], Loss: 0.26695412397384644, Training Accuracy: 95.8675
[ Tue Jul 16 23:05:11 2024 ] 	Batch(5000/6809) done. Loss: 0.0208  lr:0.000001
[ Tue Jul 16 23:05:34 2024 ] 	Batch(5100/6809) done. Loss: 0.0494  lr:0.000001
[ Tue Jul 16 23:05:57 2024 ] 	Batch(5200/6809) done. Loss: 0.4808  lr:0.000001
[ Tue Jul 16 23:06:20 2024 ] 	Batch(5300/6809) done. Loss: 0.0388  lr:0.000001
[ Tue Jul 16 23:06:42 2024 ] 	Batch(5400/6809) done. Loss: 0.0911  lr:0.000001
[ Tue Jul 16 23:07:05 2024 ] 
Training: Epoch [129/150], Step [5499], Loss: 0.3391600549221039, Training Accuracy: 95.86363636363636
[ Tue Jul 16 23:07:05 2024 ] 	Batch(5500/6809) done. Loss: 0.0421  lr:0.000001
[ Tue Jul 16 23:07:28 2024 ] 	Batch(5600/6809) done. Loss: 0.4407  lr:0.000001
[ Tue Jul 16 23:07:51 2024 ] 	Batch(5700/6809) done. Loss: 0.0395  lr:0.000001
[ Tue Jul 16 23:08:13 2024 ] 	Batch(5800/6809) done. Loss: 0.3851  lr:0.000001
[ Tue Jul 16 23:08:36 2024 ] 	Batch(5900/6809) done. Loss: 0.0220  lr:0.000001
[ Tue Jul 16 23:08:59 2024 ] 
Training: Epoch [129/150], Step [5999], Loss: 0.18604791164398193, Training Accuracy: 95.79583333333333
[ Tue Jul 16 23:08:59 2024 ] 	Batch(6000/6809) done. Loss: 0.1626  lr:0.000001
[ Tue Jul 16 23:09:22 2024 ] 	Batch(6100/6809) done. Loss: 0.3474  lr:0.000001
[ Tue Jul 16 23:09:44 2024 ] 	Batch(6200/6809) done. Loss: 0.1443  lr:0.000001
[ Tue Jul 16 23:10:07 2024 ] 	Batch(6300/6809) done. Loss: 0.3926  lr:0.000001
[ Tue Jul 16 23:10:30 2024 ] 	Batch(6400/6809) done. Loss: 0.1348  lr:0.000001
[ Tue Jul 16 23:10:52 2024 ] 
Training: Epoch [129/150], Step [6499], Loss: 0.11394448578357697, Training Accuracy: 95.74230769230769
[ Tue Jul 16 23:10:53 2024 ] 	Batch(6500/6809) done. Loss: 0.1445  lr:0.000001
[ Tue Jul 16 23:11:15 2024 ] 	Batch(6600/6809) done. Loss: 0.5992  lr:0.000001
[ Tue Jul 16 23:11:38 2024 ] 	Batch(6700/6809) done. Loss: 0.1119  lr:0.000001
[ Tue Jul 16 23:12:01 2024 ] 	Batch(6800/6809) done. Loss: 0.4190  lr:0.000001
[ Tue Jul 16 23:12:03 2024 ] 	Mean training loss: 0.1531.
[ Tue Jul 16 23:12:03 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 23:12:03 2024 ] Eval epoch: 130
[ Tue Jul 16 23:18:58 2024 ] 	Mean val loss of 7435 batches: 0.9052314630564235.
[ Tue Jul 16 23:18:58 2024 ] 
Validation: Epoch [129/150], Samples [47389.0/59477], Loss: 2.862614870071411, Validation Accuracy: 79.67617734586479
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 1 : 387 / 500 = 77 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 2 : 418 / 499 = 83 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 3 : 408 / 500 = 81 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 4 : 429 / 502 = 85 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 5 : 426 / 502 = 84 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 6 : 414 / 502 = 82 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 7 : 465 / 497 = 93 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 8 : 486 / 498 = 97 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 9 : 366 / 500 = 73 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 10 : 302 / 500 = 60 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 11 : 223 / 498 = 44 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 12 : 399 / 499 = 79 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 13 : 485 / 502 = 96 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 14 : 482 / 504 = 95 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 15 : 349 / 502 = 69 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 16 : 380 / 502 = 75 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 17 : 434 / 504 = 86 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 18 : 406 / 504 = 80 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 19 : 439 / 502 = 87 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 20 : 459 / 502 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 21 : 472 / 503 = 93 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 22 : 436 / 504 = 86 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 23 : 430 / 503 = 85 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 24 : 437 / 504 = 86 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 25 : 476 / 504 = 94 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 26 : 465 / 504 = 92 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 27 : 430 / 501 = 85 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 28 : 349 / 502 = 69 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 29 : 318 / 502 = 63 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 30 : 384 / 501 = 76 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 31 : 426 / 504 = 84 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 32 : 440 / 503 = 87 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 33 : 400 / 503 = 79 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 34 : 485 / 504 = 96 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 35 : 458 / 503 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 36 : 381 / 502 = 75 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 37 : 451 / 504 = 89 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 38 : 446 / 504 = 88 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 39 : 455 / 498 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 40 : 389 / 504 = 77 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 41 : 478 / 503 = 95 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 42 : 460 / 504 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 43 : 338 / 503 = 67 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 44 : 422 / 504 = 83 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 45 : 419 / 504 = 83 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 46 : 401 / 504 = 79 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 47 : 359 / 503 = 71 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 48 : 417 / 503 = 82 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 49 : 410 / 499 = 82 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 50 : 412 / 502 = 82 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 51 : 465 / 503 = 92 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 52 : 460 / 504 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 53 : 456 / 497 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 54 : 444 / 480 = 92 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 55 : 441 / 504 = 87 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 56 : 416 / 503 = 82 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 57 : 459 / 504 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 58 : 482 / 499 = 96 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 59 : 484 / 503 = 96 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 60 : 414 / 479 = 86 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 61 : 405 / 484 = 83 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 62 : 401 / 487 = 82 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 63 : 444 / 489 = 90 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 64 : 411 / 488 = 84 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 65 : 430 / 490 = 87 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 66 : 300 / 488 = 61 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 67 : 348 / 490 = 71 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 68 : 299 / 490 = 61 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 69 : 383 / 490 = 78 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 70 : 184 / 490 = 37 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 71 : 280 / 490 = 57 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 72 : 218 / 488 = 44 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 73 : 232 / 486 = 47 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 74 : 282 / 481 = 58 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 75 : 285 / 488 = 58 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 76 : 305 / 489 = 62 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 77 : 320 / 488 = 65 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 78 : 377 / 488 = 77 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 79 : 457 / 490 = 93 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 80 : 394 / 489 = 80 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 81 : 268 / 491 = 54 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 82 : 321 / 491 = 65 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 83 : 251 / 489 = 51 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 84 : 367 / 489 = 75 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 85 : 356 / 489 = 72 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 86 : 409 / 491 = 83 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 87 : 421 / 492 = 85 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 88 : 347 / 491 = 70 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 89 : 360 / 492 = 73 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 90 : 271 / 490 = 55 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 91 : 382 / 482 = 79 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 92 : 355 / 490 = 72 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 93 : 346 / 487 = 71 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 94 : 436 / 489 = 89 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 95 : 401 / 490 = 81 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 96 : 458 / 491 = 93 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 97 : 457 / 490 = 93 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 98 : 434 / 491 = 88 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 99 : 450 / 491 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 100 : 446 / 491 = 90 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 101 : 414 / 491 = 84 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 102 : 296 / 492 = 60 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 103 : 388 / 492 = 78 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 104 : 298 / 491 = 60 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 105 : 251 / 491 = 51 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 106 : 324 / 492 = 65 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 107 : 415 / 491 = 84 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 108 : 368 / 492 = 74 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 109 : 356 / 490 = 72 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 110 : 402 / 491 = 81 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 111 : 443 / 492 = 90 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 112 : 459 / 492 = 93 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 113 : 447 / 491 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 114 : 395 / 491 = 80 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 115 : 414 / 492 = 84 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 116 : 423 / 491 = 86 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 117 : 399 / 492 = 81 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 118 : 417 / 490 = 85 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 119 : 448 / 492 = 91 %
[ Tue Jul 16 23:18:58 2024 ] Accuracy of 120 : 419 / 500 = 83 %
[ Tue Jul 16 23:18:58 2024 ] Training epoch: 131
[ Tue Jul 16 23:18:59 2024 ] 	Batch(0/6809) done. Loss: 0.2069  lr:0.000001
[ Tue Jul 16 23:19:22 2024 ] 	Batch(100/6809) done. Loss: 0.0367  lr:0.000001
[ Tue Jul 16 23:19:44 2024 ] 	Batch(200/6809) done. Loss: 0.1610  lr:0.000001
[ Tue Jul 16 23:20:07 2024 ] 	Batch(300/6809) done. Loss: 0.2279  lr:0.000001
[ Tue Jul 16 23:20:30 2024 ] 	Batch(400/6809) done. Loss: 0.0143  lr:0.000001
[ Tue Jul 16 23:20:52 2024 ] 
Training: Epoch [130/150], Step [499], Loss: 0.010798229835927486, Training Accuracy: 95.92500000000001
[ Tue Jul 16 23:20:52 2024 ] 	Batch(500/6809) done. Loss: 0.1464  lr:0.000001
[ Tue Jul 16 23:21:15 2024 ] 	Batch(600/6809) done. Loss: 0.1042  lr:0.000001
[ Tue Jul 16 23:21:38 2024 ] 	Batch(700/6809) done. Loss: 0.0989  lr:0.000001
[ Tue Jul 16 23:22:00 2024 ] 	Batch(800/6809) done. Loss: 0.1087  lr:0.000001
[ Tue Jul 16 23:22:23 2024 ] 	Batch(900/6809) done. Loss: 0.7418  lr:0.000001
[ Tue Jul 16 23:22:45 2024 ] 
Training: Epoch [130/150], Step [999], Loss: 0.0800480917096138, Training Accuracy: 95.625
[ Tue Jul 16 23:22:45 2024 ] 	Batch(1000/6809) done. Loss: 0.8499  lr:0.000001
[ Tue Jul 16 23:23:08 2024 ] 	Batch(1100/6809) done. Loss: 0.0069  lr:0.000001
[ Tue Jul 16 23:23:31 2024 ] 	Batch(1200/6809) done. Loss: 0.0692  lr:0.000001
[ Tue Jul 16 23:23:53 2024 ] 	Batch(1300/6809) done. Loss: 0.1277  lr:0.000001
[ Tue Jul 16 23:24:16 2024 ] 	Batch(1400/6809) done. Loss: 0.1316  lr:0.000001
[ Tue Jul 16 23:24:38 2024 ] 
Training: Epoch [130/150], Step [1499], Loss: 0.4267444610595703, Training Accuracy: 95.70833333333333
[ Tue Jul 16 23:24:38 2024 ] 	Batch(1500/6809) done. Loss: 0.0032  lr:0.000001
[ Tue Jul 16 23:25:01 2024 ] 	Batch(1600/6809) done. Loss: 0.0822  lr:0.000001
[ Tue Jul 16 23:25:24 2024 ] 	Batch(1700/6809) done. Loss: 0.2665  lr:0.000001
[ Tue Jul 16 23:25:47 2024 ] 	Batch(1800/6809) done. Loss: 0.0173  lr:0.000001
[ Tue Jul 16 23:26:09 2024 ] 	Batch(1900/6809) done. Loss: 0.0388  lr:0.000001
[ Tue Jul 16 23:26:32 2024 ] 
Training: Epoch [130/150], Step [1999], Loss: 0.1395132690668106, Training Accuracy: 95.76875
[ Tue Jul 16 23:26:32 2024 ] 	Batch(2000/6809) done. Loss: 0.0281  lr:0.000001
[ Tue Jul 16 23:26:55 2024 ] 	Batch(2100/6809) done. Loss: 0.0616  lr:0.000001
[ Tue Jul 16 23:27:17 2024 ] 	Batch(2200/6809) done. Loss: 0.1179  lr:0.000001
[ Tue Jul 16 23:27:40 2024 ] 	Batch(2300/6809) done. Loss: 0.0622  lr:0.000001
[ Tue Jul 16 23:28:04 2024 ] 	Batch(2400/6809) done. Loss: 0.1618  lr:0.000001
[ Tue Jul 16 23:28:27 2024 ] 
Training: Epoch [130/150], Step [2499], Loss: 0.04429902881383896, Training Accuracy: 95.765
[ Tue Jul 16 23:28:27 2024 ] 	Batch(2500/6809) done. Loss: 0.0086  lr:0.000001
[ Tue Jul 16 23:28:50 2024 ] 	Batch(2600/6809) done. Loss: 0.6215  lr:0.000001
[ Tue Jul 16 23:29:13 2024 ] 	Batch(2700/6809) done. Loss: 0.1544  lr:0.000001
[ Tue Jul 16 23:29:37 2024 ] 	Batch(2800/6809) done. Loss: 0.1915  lr:0.000001
[ Tue Jul 16 23:29:59 2024 ] 	Batch(2900/6809) done. Loss: 0.3249  lr:0.000001
[ Tue Jul 16 23:30:22 2024 ] 
Training: Epoch [130/150], Step [2999], Loss: 0.01909913681447506, Training Accuracy: 95.72916666666667
[ Tue Jul 16 23:30:22 2024 ] 	Batch(3000/6809) done. Loss: 0.0399  lr:0.000001
[ Tue Jul 16 23:30:45 2024 ] 	Batch(3100/6809) done. Loss: 0.0644  lr:0.000001
[ Tue Jul 16 23:31:08 2024 ] 	Batch(3200/6809) done. Loss: 0.0732  lr:0.000001
[ Tue Jul 16 23:31:31 2024 ] 	Batch(3300/6809) done. Loss: 0.0111  lr:0.000001
[ Tue Jul 16 23:31:54 2024 ] 	Batch(3400/6809) done. Loss: 0.0641  lr:0.000001
[ Tue Jul 16 23:32:17 2024 ] 
Training: Epoch [130/150], Step [3499], Loss: 0.12102801352739334, Training Accuracy: 95.77142857142857
[ Tue Jul 16 23:32:17 2024 ] 	Batch(3500/6809) done. Loss: 0.0591  lr:0.000001
[ Tue Jul 16 23:32:40 2024 ] 	Batch(3600/6809) done. Loss: 0.3842  lr:0.000001
[ Tue Jul 16 23:33:03 2024 ] 	Batch(3700/6809) done. Loss: 0.2515  lr:0.000001
[ Tue Jul 16 23:33:26 2024 ] 	Batch(3800/6809) done. Loss: 0.1491  lr:0.000001
[ Tue Jul 16 23:33:49 2024 ] 	Batch(3900/6809) done. Loss: 0.0417  lr:0.000001
[ Tue Jul 16 23:34:12 2024 ] 
Training: Epoch [130/150], Step [3999], Loss: 0.46238672733306885, Training Accuracy: 95.7
[ Tue Jul 16 23:34:12 2024 ] 	Batch(4000/6809) done. Loss: 0.1764  lr:0.000001
[ Tue Jul 16 23:34:35 2024 ] 	Batch(4100/6809) done. Loss: 0.0569  lr:0.000001
[ Tue Jul 16 23:34:58 2024 ] 	Batch(4200/6809) done. Loss: 0.2459  lr:0.000001
[ Tue Jul 16 23:35:21 2024 ] 	Batch(4300/6809) done. Loss: 0.0343  lr:0.000001
[ Tue Jul 16 23:35:44 2024 ] 	Batch(4400/6809) done. Loss: 0.1102  lr:0.000001
[ Tue Jul 16 23:36:06 2024 ] 
Training: Epoch [130/150], Step [4499], Loss: 0.11168748140335083, Training Accuracy: 95.67222222222223
[ Tue Jul 16 23:36:06 2024 ] 	Batch(4500/6809) done. Loss: 0.1230  lr:0.000001
[ Tue Jul 16 23:36:29 2024 ] 	Batch(4600/6809) done. Loss: 0.1608  lr:0.000001
[ Tue Jul 16 23:36:51 2024 ] 	Batch(4700/6809) done. Loss: 0.0179  lr:0.000001
[ Tue Jul 16 23:37:14 2024 ] 	Batch(4800/6809) done. Loss: 0.0581  lr:0.000001
[ Tue Jul 16 23:37:37 2024 ] 	Batch(4900/6809) done. Loss: 0.2310  lr:0.000001
[ Tue Jul 16 23:38:00 2024 ] 
Training: Epoch [130/150], Step [4999], Loss: 0.024247944355010986, Training Accuracy: 95.69
[ Tue Jul 16 23:38:01 2024 ] 	Batch(5000/6809) done. Loss: 0.2488  lr:0.000001
[ Tue Jul 16 23:38:23 2024 ] 	Batch(5100/6809) done. Loss: 0.0346  lr:0.000001
[ Tue Jul 16 23:38:47 2024 ] 	Batch(5200/6809) done. Loss: 0.4451  lr:0.000001
[ Tue Jul 16 23:39:10 2024 ] 	Batch(5300/6809) done. Loss: 0.0962  lr:0.000001
[ Tue Jul 16 23:39:33 2024 ] 	Batch(5400/6809) done. Loss: 0.1162  lr:0.000001
[ Tue Jul 16 23:39:56 2024 ] 
Training: Epoch [130/150], Step [5499], Loss: 0.22134479880332947, Training Accuracy: 95.73409090909091
[ Tue Jul 16 23:39:56 2024 ] 	Batch(5500/6809) done. Loss: 0.6423  lr:0.000001
[ Tue Jul 16 23:40:19 2024 ] 	Batch(5600/6809) done. Loss: 0.2238  lr:0.000001
[ Tue Jul 16 23:40:42 2024 ] 	Batch(5700/6809) done. Loss: 0.0186  lr:0.000001
[ Tue Jul 16 23:41:05 2024 ] 	Batch(5800/6809) done. Loss: 0.2009  lr:0.000001
[ Tue Jul 16 23:41:28 2024 ] 	Batch(5900/6809) done. Loss: 0.0345  lr:0.000001
[ Tue Jul 16 23:41:50 2024 ] 
Training: Epoch [130/150], Step [5999], Loss: 0.3349556624889374, Training Accuracy: 95.70208333333333
[ Tue Jul 16 23:41:51 2024 ] 	Batch(6000/6809) done. Loss: 0.2154  lr:0.000001
[ Tue Jul 16 23:42:13 2024 ] 	Batch(6100/6809) done. Loss: 0.0301  lr:0.000001
[ Tue Jul 16 23:42:36 2024 ] 	Batch(6200/6809) done. Loss: 0.0956  lr:0.000001
[ Tue Jul 16 23:42:59 2024 ] 	Batch(6300/6809) done. Loss: 0.0066  lr:0.000001
[ Tue Jul 16 23:43:22 2024 ] 	Batch(6400/6809) done. Loss: 0.2900  lr:0.000001
[ Tue Jul 16 23:43:44 2024 ] 
Training: Epoch [130/150], Step [6499], Loss: 0.16394028067588806, Training Accuracy: 95.70384615384614
[ Tue Jul 16 23:43:44 2024 ] 	Batch(6500/6809) done. Loss: 0.0653  lr:0.000001
[ Tue Jul 16 23:44:07 2024 ] 	Batch(6600/6809) done. Loss: 0.0504  lr:0.000001
[ Tue Jul 16 23:44:29 2024 ] 	Batch(6700/6809) done. Loss: 0.2908  lr:0.000001
[ Tue Jul 16 23:44:52 2024 ] 	Batch(6800/6809) done. Loss: 0.0758  lr:0.000001
[ Tue Jul 16 23:44:54 2024 ] 	Mean training loss: 0.1544.
[ Tue Jul 16 23:44:54 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Tue Jul 16 23:44:54 2024 ] Training epoch: 132
[ Tue Jul 16 23:44:55 2024 ] 	Batch(0/6809) done. Loss: 0.0540  lr:0.000001
[ Tue Jul 16 23:45:18 2024 ] 	Batch(100/6809) done. Loss: 0.0307  lr:0.000001
[ Tue Jul 16 23:45:41 2024 ] 	Batch(200/6809) done. Loss: 0.1074  lr:0.000001
[ Tue Jul 16 23:46:04 2024 ] 	Batch(300/6809) done. Loss: 0.1770  lr:0.000001
[ Tue Jul 16 23:46:27 2024 ] 	Batch(400/6809) done. Loss: 0.0276  lr:0.000001
[ Tue Jul 16 23:46:50 2024 ] 
Training: Epoch [131/150], Step [499], Loss: 0.04792669042944908, Training Accuracy: 96.2
[ Tue Jul 16 23:46:50 2024 ] 	Batch(500/6809) done. Loss: 0.4140  lr:0.000001
[ Tue Jul 16 23:47:12 2024 ] 	Batch(600/6809) done. Loss: 0.0784  lr:0.000001
[ Tue Jul 16 23:47:35 2024 ] 	Batch(700/6809) done. Loss: 0.1836  lr:0.000001
[ Tue Jul 16 23:47:58 2024 ] 	Batch(800/6809) done. Loss: 0.0755  lr:0.000001
[ Tue Jul 16 23:48:21 2024 ] 	Batch(900/6809) done. Loss: 0.0320  lr:0.000001
[ Tue Jul 16 23:48:43 2024 ] 
Training: Epoch [131/150], Step [999], Loss: 0.006916038691997528, Training Accuracy: 95.875
[ Tue Jul 16 23:48:43 2024 ] 	Batch(1000/6809) done. Loss: 0.2081  lr:0.000001
[ Tue Jul 16 23:49:06 2024 ] 	Batch(1100/6809) done. Loss: 0.1187  lr:0.000001
[ Tue Jul 16 23:49:29 2024 ] 	Batch(1200/6809) done. Loss: 0.0581  lr:0.000001
[ Tue Jul 16 23:49:52 2024 ] 	Batch(1300/6809) done. Loss: 0.5216  lr:0.000001
[ Tue Jul 16 23:50:15 2024 ] 	Batch(1400/6809) done. Loss: 0.1274  lr:0.000001
[ Tue Jul 16 23:50:37 2024 ] 
Training: Epoch [131/150], Step [1499], Loss: 0.29458481073379517, Training Accuracy: 95.675
[ Tue Jul 16 23:50:37 2024 ] 	Batch(1500/6809) done. Loss: 0.0535  lr:0.000001
[ Tue Jul 16 23:51:00 2024 ] 	Batch(1600/6809) done. Loss: 0.5247  lr:0.000001
[ Tue Jul 16 23:51:23 2024 ] 	Batch(1700/6809) done. Loss: 0.1896  lr:0.000001
[ Tue Jul 16 23:51:46 2024 ] 	Batch(1800/6809) done. Loss: 0.0127  lr:0.000001
[ Tue Jul 16 23:52:09 2024 ] 	Batch(1900/6809) done. Loss: 0.0394  lr:0.000001
[ Tue Jul 16 23:52:31 2024 ] 
Training: Epoch [131/150], Step [1999], Loss: 0.010981583036482334, Training Accuracy: 95.69375
[ Tue Jul 16 23:52:32 2024 ] 	Batch(2000/6809) done. Loss: 0.1473  lr:0.000001
[ Tue Jul 16 23:52:55 2024 ] 	Batch(2100/6809) done. Loss: 0.0472  lr:0.000001
[ Tue Jul 16 23:53:18 2024 ] 	Batch(2200/6809) done. Loss: 0.0020  lr:0.000001
[ Tue Jul 16 23:53:41 2024 ] 	Batch(2300/6809) done. Loss: 0.1444  lr:0.000001
[ Tue Jul 16 23:54:03 2024 ] 	Batch(2400/6809) done. Loss: 0.1231  lr:0.000001
[ Tue Jul 16 23:54:26 2024 ] 
Training: Epoch [131/150], Step [2499], Loss: 0.3010310232639313, Training Accuracy: 95.735
[ Tue Jul 16 23:54:26 2024 ] 	Batch(2500/6809) done. Loss: 0.0539  lr:0.000001
[ Tue Jul 16 23:54:49 2024 ] 	Batch(2600/6809) done. Loss: 0.0155  lr:0.000001
[ Tue Jul 16 23:55:11 2024 ] 	Batch(2700/6809) done. Loss: 0.0541  lr:0.000001
[ Tue Jul 16 23:55:34 2024 ] 	Batch(2800/6809) done. Loss: 0.0407  lr:0.000001
[ Tue Jul 16 23:55:57 2024 ] 	Batch(2900/6809) done. Loss: 0.1721  lr:0.000001
[ Tue Jul 16 23:56:20 2024 ] 
Training: Epoch [131/150], Step [2999], Loss: 0.21137526631355286, Training Accuracy: 95.78750000000001
[ Tue Jul 16 23:56:20 2024 ] 	Batch(3000/6809) done. Loss: 0.5828  lr:0.000001
[ Tue Jul 16 23:56:43 2024 ] 	Batch(3100/6809) done. Loss: 0.1621  lr:0.000001
[ Tue Jul 16 23:57:06 2024 ] 	Batch(3200/6809) done. Loss: 0.0444  lr:0.000001
[ Tue Jul 16 23:57:29 2024 ] 	Batch(3300/6809) done. Loss: 0.4141  lr:0.000001
[ Tue Jul 16 23:57:53 2024 ] 	Batch(3400/6809) done. Loss: 0.0090  lr:0.000001
[ Tue Jul 16 23:58:15 2024 ] 
Training: Epoch [131/150], Step [3499], Loss: 0.0032052635215222836, Training Accuracy: 95.76785714285714
[ Tue Jul 16 23:58:16 2024 ] 	Batch(3500/6809) done. Loss: 0.0280  lr:0.000001
[ Tue Jul 16 23:58:39 2024 ] 	Batch(3600/6809) done. Loss: 0.0384  lr:0.000001
[ Tue Jul 16 23:59:02 2024 ] 	Batch(3700/6809) done. Loss: 0.0205  lr:0.000001
[ Tue Jul 16 23:59:25 2024 ] 	Batch(3800/6809) done. Loss: 0.0770  lr:0.000001
[ Tue Jul 16 23:59:48 2024 ] 	Batch(3900/6809) done. Loss: 0.1948  lr:0.000001
[ Wed Jul 17 00:00:11 2024 ] 
Training: Epoch [131/150], Step [3999], Loss: 0.1387961357831955, Training Accuracy: 95.765625
[ Wed Jul 17 00:00:11 2024 ] 	Batch(4000/6809) done. Loss: 0.3482  lr:0.000001
[ Wed Jul 17 00:00:35 2024 ] 	Batch(4100/6809) done. Loss: 0.0242  lr:0.000001
[ Wed Jul 17 00:00:58 2024 ] 	Batch(4200/6809) done. Loss: 0.1546  lr:0.000001
[ Wed Jul 17 00:01:21 2024 ] 	Batch(4300/6809) done. Loss: 0.0613  lr:0.000001
[ Wed Jul 17 00:01:44 2024 ] 	Batch(4400/6809) done. Loss: 0.0727  lr:0.000001
[ Wed Jul 17 00:02:06 2024 ] 
Training: Epoch [131/150], Step [4499], Loss: 0.4724459648132324, Training Accuracy: 95.71666666666667
[ Wed Jul 17 00:02:07 2024 ] 	Batch(4500/6809) done. Loss: 0.1122  lr:0.000001
[ Wed Jul 17 00:02:29 2024 ] 	Batch(4600/6809) done. Loss: 0.0205  lr:0.000001
[ Wed Jul 17 00:02:52 2024 ] 	Batch(4700/6809) done. Loss: 0.0841  lr:0.000001
[ Wed Jul 17 00:03:14 2024 ] 	Batch(4800/6809) done. Loss: 0.0235  lr:0.000001
[ Wed Jul 17 00:03:37 2024 ] 	Batch(4900/6809) done. Loss: 0.2371  lr:0.000001
[ Wed Jul 17 00:03:59 2024 ] 
Training: Epoch [131/150], Step [4999], Loss: 0.25419023633003235, Training Accuracy: 95.7175
[ Wed Jul 17 00:03:59 2024 ] 	Batch(5000/6809) done. Loss: 0.0588  lr:0.000001
[ Wed Jul 17 00:04:22 2024 ] 	Batch(5100/6809) done. Loss: 0.0473  lr:0.000001
[ Wed Jul 17 00:04:45 2024 ] 	Batch(5200/6809) done. Loss: 0.0491  lr:0.000001
[ Wed Jul 17 00:05:07 2024 ] 	Batch(5300/6809) done. Loss: 0.1410  lr:0.000001
[ Wed Jul 17 00:05:30 2024 ] 	Batch(5400/6809) done. Loss: 0.0248  lr:0.000001
[ Wed Jul 17 00:05:52 2024 ] 
Training: Epoch [131/150], Step [5499], Loss: 0.5219165682792664, Training Accuracy: 95.72727272727273
[ Wed Jul 17 00:05:53 2024 ] 	Batch(5500/6809) done. Loss: 0.0140  lr:0.000001
[ Wed Jul 17 00:06:15 2024 ] 	Batch(5600/6809) done. Loss: 0.0941  lr:0.000001
[ Wed Jul 17 00:06:38 2024 ] 	Batch(5700/6809) done. Loss: 0.0113  lr:0.000001
[ Wed Jul 17 00:07:01 2024 ] 	Batch(5800/6809) done. Loss: 0.1588  lr:0.000001
[ Wed Jul 17 00:07:24 2024 ] 	Batch(5900/6809) done. Loss: 0.2960  lr:0.000001
[ Wed Jul 17 00:07:47 2024 ] 
Training: Epoch [131/150], Step [5999], Loss: 0.07436403632164001, Training Accuracy: 95.75208333333333
[ Wed Jul 17 00:07:48 2024 ] 	Batch(6000/6809) done. Loss: 0.1664  lr:0.000001
[ Wed Jul 17 00:08:11 2024 ] 	Batch(6100/6809) done. Loss: 0.1092  lr:0.000001
[ Wed Jul 17 00:08:34 2024 ] 	Batch(6200/6809) done. Loss: 0.2269  lr:0.000001
[ Wed Jul 17 00:08:57 2024 ] 	Batch(6300/6809) done. Loss: 0.2086  lr:0.000001
[ Wed Jul 17 00:09:20 2024 ] 	Batch(6400/6809) done. Loss: 0.0870  lr:0.000001
[ Wed Jul 17 00:09:43 2024 ] 
Training: Epoch [131/150], Step [6499], Loss: 0.33077478408813477, Training Accuracy: 95.75
[ Wed Jul 17 00:09:43 2024 ] 	Batch(6500/6809) done. Loss: 0.0500  lr:0.000001
[ Wed Jul 17 00:10:06 2024 ] 	Batch(6600/6809) done. Loss: 0.2710  lr:0.000001
[ Wed Jul 17 00:10:29 2024 ] 	Batch(6700/6809) done. Loss: 0.0392  lr:0.000001
[ Wed Jul 17 00:10:53 2024 ] 	Batch(6800/6809) done. Loss: 0.0469  lr:0.000001
[ Wed Jul 17 00:10:54 2024 ] 	Mean training loss: 0.1526.
[ Wed Jul 17 00:10:54 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 00:10:55 2024 ] Training epoch: 133
[ Wed Jul 17 00:10:55 2024 ] 	Batch(0/6809) done. Loss: 0.3630  lr:0.000001
[ Wed Jul 17 00:11:18 2024 ] 	Batch(100/6809) done. Loss: 0.2504  lr:0.000001
[ Wed Jul 17 00:11:42 2024 ] 	Batch(200/6809) done. Loss: 0.0784  lr:0.000001
[ Wed Jul 17 00:12:05 2024 ] 	Batch(300/6809) done. Loss: 0.1887  lr:0.000001
[ Wed Jul 17 00:12:28 2024 ] 	Batch(400/6809) done. Loss: 0.0785  lr:0.000001
[ Wed Jul 17 00:12:51 2024 ] 
Training: Epoch [132/150], Step [499], Loss: 0.24750222265720367, Training Accuracy: 95.825
[ Wed Jul 17 00:12:51 2024 ] 	Batch(500/6809) done. Loss: 0.2542  lr:0.000001
[ Wed Jul 17 00:13:14 2024 ] 	Batch(600/6809) done. Loss: 0.5132  lr:0.000001
[ Wed Jul 17 00:13:36 2024 ] 	Batch(700/6809) done. Loss: 0.1048  lr:0.000001
[ Wed Jul 17 00:13:59 2024 ] 	Batch(800/6809) done. Loss: 0.4186  lr:0.000001
[ Wed Jul 17 00:14:22 2024 ] 	Batch(900/6809) done. Loss: 0.7647  lr:0.000001
[ Wed Jul 17 00:14:44 2024 ] 
Training: Epoch [132/150], Step [999], Loss: 0.10811919718980789, Training Accuracy: 95.5625
[ Wed Jul 17 00:14:44 2024 ] 	Batch(1000/6809) done. Loss: 0.3000  lr:0.000001
[ Wed Jul 17 00:15:07 2024 ] 	Batch(1100/6809) done. Loss: 0.0815  lr:0.000001
[ Wed Jul 17 00:15:30 2024 ] 	Batch(1200/6809) done. Loss: 0.0055  lr:0.000001
[ Wed Jul 17 00:15:52 2024 ] 	Batch(1300/6809) done. Loss: 0.3331  lr:0.000001
[ Wed Jul 17 00:16:15 2024 ] 	Batch(1400/6809) done. Loss: 0.1010  lr:0.000001
[ Wed Jul 17 00:16:37 2024 ] 
Training: Epoch [132/150], Step [1499], Loss: 0.6230698823928833, Training Accuracy: 95.525
[ Wed Jul 17 00:16:38 2024 ] 	Batch(1500/6809) done. Loss: 0.3786  lr:0.000001
[ Wed Jul 17 00:17:01 2024 ] 	Batch(1600/6809) done. Loss: 0.1180  lr:0.000001
[ Wed Jul 17 00:17:23 2024 ] 	Batch(1700/6809) done. Loss: 0.1324  lr:0.000001
[ Wed Jul 17 00:17:46 2024 ] 	Batch(1800/6809) done. Loss: 0.0606  lr:0.000001
[ Wed Jul 17 00:18:08 2024 ] 	Batch(1900/6809) done. Loss: 0.1773  lr:0.000001
[ Wed Jul 17 00:18:31 2024 ] 
Training: Epoch [132/150], Step [1999], Loss: 0.027413757517933846, Training Accuracy: 95.60625
[ Wed Jul 17 00:18:31 2024 ] 	Batch(2000/6809) done. Loss: 0.1117  lr:0.000001
[ Wed Jul 17 00:18:54 2024 ] 	Batch(2100/6809) done. Loss: 0.0156  lr:0.000001
[ Wed Jul 17 00:19:16 2024 ] 	Batch(2200/6809) done. Loss: 0.1313  lr:0.000001
[ Wed Jul 17 00:19:39 2024 ] 	Batch(2300/6809) done. Loss: 0.0051  lr:0.000001
[ Wed Jul 17 00:20:03 2024 ] 	Batch(2400/6809) done. Loss: 0.6918  lr:0.000001
[ Wed Jul 17 00:20:26 2024 ] 
Training: Epoch [132/150], Step [2499], Loss: 0.11376194655895233, Training Accuracy: 95.575
[ Wed Jul 17 00:20:26 2024 ] 	Batch(2500/6809) done. Loss: 0.0587  lr:0.000001
[ Wed Jul 17 00:20:49 2024 ] 	Batch(2600/6809) done. Loss: 0.0997  lr:0.000001
[ Wed Jul 17 00:21:12 2024 ] 	Batch(2700/6809) done. Loss: 0.1613  lr:0.000001
[ Wed Jul 17 00:21:35 2024 ] 	Batch(2800/6809) done. Loss: 0.4140  lr:0.000001
[ Wed Jul 17 00:21:59 2024 ] 	Batch(2900/6809) done. Loss: 0.0071  lr:0.000001
[ Wed Jul 17 00:22:21 2024 ] 
Training: Epoch [132/150], Step [2999], Loss: 0.11108595877885818, Training Accuracy: 95.65
[ Wed Jul 17 00:22:22 2024 ] 	Batch(3000/6809) done. Loss: 0.0094  lr:0.000001
[ Wed Jul 17 00:22:45 2024 ] 	Batch(3100/6809) done. Loss: 0.3894  lr:0.000001
[ Wed Jul 17 00:23:08 2024 ] 	Batch(3200/6809) done. Loss: 0.3644  lr:0.000001
[ Wed Jul 17 00:23:31 2024 ] 	Batch(3300/6809) done. Loss: 0.0425  lr:0.000001
[ Wed Jul 17 00:23:54 2024 ] 	Batch(3400/6809) done. Loss: 0.5430  lr:0.000001
[ Wed Jul 17 00:24:17 2024 ] 
Training: Epoch [132/150], Step [3499], Loss: 0.05058685690164566, Training Accuracy: 95.66071428571429
[ Wed Jul 17 00:24:17 2024 ] 	Batch(3500/6809) done. Loss: 0.7421  lr:0.000001
[ Wed Jul 17 00:24:41 2024 ] 	Batch(3600/6809) done. Loss: 0.0686  lr:0.000001
[ Wed Jul 17 00:25:04 2024 ] 	Batch(3700/6809) done. Loss: 0.0069  lr:0.000001
[ Wed Jul 17 00:25:27 2024 ] 	Batch(3800/6809) done. Loss: 0.2456  lr:0.000001
[ Wed Jul 17 00:25:50 2024 ] 	Batch(3900/6809) done. Loss: 0.1931  lr:0.000001
[ Wed Jul 17 00:26:13 2024 ] 
Training: Epoch [132/150], Step [3999], Loss: 0.07900550216436386, Training Accuracy: 95.671875
[ Wed Jul 17 00:26:13 2024 ] 	Batch(4000/6809) done. Loss: 0.2692  lr:0.000001
[ Wed Jul 17 00:26:37 2024 ] 	Batch(4100/6809) done. Loss: 0.1051  lr:0.000001
[ Wed Jul 17 00:27:00 2024 ] 	Batch(4200/6809) done. Loss: 0.2452  lr:0.000001
[ Wed Jul 17 00:27:23 2024 ] 	Batch(4300/6809) done. Loss: 0.0351  lr:0.000001
[ Wed Jul 17 00:27:46 2024 ] 	Batch(4400/6809) done. Loss: 0.2484  lr:0.000001
[ Wed Jul 17 00:28:09 2024 ] 
Training: Epoch [132/150], Step [4499], Loss: 0.27896901965141296, Training Accuracy: 95.70277777777778
[ Wed Jul 17 00:28:10 2024 ] 	Batch(4500/6809) done. Loss: 0.0842  lr:0.000001
[ Wed Jul 17 00:28:33 2024 ] 	Batch(4600/6809) done. Loss: 0.0354  lr:0.000001
[ Wed Jul 17 00:28:56 2024 ] 	Batch(4700/6809) done. Loss: 0.3233  lr:0.000001
[ Wed Jul 17 00:29:19 2024 ] 	Batch(4800/6809) done. Loss: 0.0952  lr:0.000001
[ Wed Jul 17 00:29:42 2024 ] 	Batch(4900/6809) done. Loss: 0.0726  lr:0.000001
[ Wed Jul 17 00:30:05 2024 ] 
Training: Epoch [132/150], Step [4999], Loss: 0.013181285001337528, Training Accuracy: 95.67750000000001
[ Wed Jul 17 00:30:05 2024 ] 	Batch(5000/6809) done. Loss: 0.5763  lr:0.000001
[ Wed Jul 17 00:30:29 2024 ] 	Batch(5100/6809) done. Loss: 0.3024  lr:0.000001
[ Wed Jul 17 00:30:53 2024 ] 	Batch(5200/6809) done. Loss: 0.1962  lr:0.000001
[ Wed Jul 17 00:31:16 2024 ] 	Batch(5300/6809) done. Loss: 0.1490  lr:0.000001
[ Wed Jul 17 00:31:40 2024 ] 	Batch(5400/6809) done. Loss: 0.0273  lr:0.000001
[ Wed Jul 17 00:32:03 2024 ] 
Training: Epoch [132/150], Step [5499], Loss: 0.03591257706284523, Training Accuracy: 95.67727272727272
[ Wed Jul 17 00:32:03 2024 ] 	Batch(5500/6809) done. Loss: 0.3380  lr:0.000001
[ Wed Jul 17 00:32:26 2024 ] 	Batch(5600/6809) done. Loss: 0.1591  lr:0.000001
[ Wed Jul 17 00:32:49 2024 ] 	Batch(5700/6809) done. Loss: 0.1454  lr:0.000001
[ Wed Jul 17 00:33:12 2024 ] 	Batch(5800/6809) done. Loss: 0.0455  lr:0.000001
[ Wed Jul 17 00:33:35 2024 ] 	Batch(5900/6809) done. Loss: 0.5712  lr:0.000001
[ Wed Jul 17 00:33:58 2024 ] 
Training: Epoch [132/150], Step [5999], Loss: 0.14004652202129364, Training Accuracy: 95.675
[ Wed Jul 17 00:33:58 2024 ] 	Batch(6000/6809) done. Loss: 0.0291  lr:0.000001
[ Wed Jul 17 00:34:21 2024 ] 	Batch(6100/6809) done. Loss: 0.1056  lr:0.000001
[ Wed Jul 17 00:34:44 2024 ] 	Batch(6200/6809) done. Loss: 0.4986  lr:0.000001
[ Wed Jul 17 00:35:07 2024 ] 	Batch(6300/6809) done. Loss: 0.2282  lr:0.000001
[ Wed Jul 17 00:35:30 2024 ] 	Batch(6400/6809) done. Loss: 0.0521  lr:0.000001
[ Wed Jul 17 00:35:53 2024 ] 
Training: Epoch [132/150], Step [6499], Loss: 0.54850834608078, Training Accuracy: 95.65961538461538
[ Wed Jul 17 00:35:53 2024 ] 	Batch(6500/6809) done. Loss: 0.0646  lr:0.000001
[ Wed Jul 17 00:36:16 2024 ] 	Batch(6600/6809) done. Loss: 0.1813  lr:0.000001
[ Wed Jul 17 00:36:38 2024 ] 	Batch(6700/6809) done. Loss: 0.0641  lr:0.000001
[ Wed Jul 17 00:37:02 2024 ] 	Batch(6800/6809) done. Loss: 0.1232  lr:0.000001
[ Wed Jul 17 00:37:04 2024 ] 	Mean training loss: 0.1552.
[ Wed Jul 17 00:37:04 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Wed Jul 17 00:37:04 2024 ] Training epoch: 134
[ Wed Jul 17 00:37:05 2024 ] 	Batch(0/6809) done. Loss: 0.0118  lr:0.000001
[ Wed Jul 17 00:37:27 2024 ] 	Batch(100/6809) done. Loss: 0.1633  lr:0.000001
[ Wed Jul 17 00:37:50 2024 ] 	Batch(200/6809) done. Loss: 0.1178  lr:0.000001
[ Wed Jul 17 00:38:12 2024 ] 	Batch(300/6809) done. Loss: 0.0369  lr:0.000001
[ Wed Jul 17 00:38:35 2024 ] 	Batch(400/6809) done. Loss: 0.1147  lr:0.000001
[ Wed Jul 17 00:38:57 2024 ] 
Training: Epoch [133/150], Step [499], Loss: 0.7645379304885864, Training Accuracy: 95.8
[ Wed Jul 17 00:38:57 2024 ] 	Batch(500/6809) done. Loss: 0.1399  lr:0.000001
[ Wed Jul 17 00:39:20 2024 ] 	Batch(600/6809) done. Loss: 0.0179  lr:0.000001
[ Wed Jul 17 00:39:43 2024 ] 	Batch(700/6809) done. Loss: 0.0228  lr:0.000001
[ Wed Jul 17 00:40:05 2024 ] 	Batch(800/6809) done. Loss: 0.1308  lr:0.000001
[ Wed Jul 17 00:40:28 2024 ] 	Batch(900/6809) done. Loss: 0.0059  lr:0.000001
[ Wed Jul 17 00:40:50 2024 ] 
Training: Epoch [133/150], Step [999], Loss: 0.09459580481052399, Training Accuracy: 95.95
[ Wed Jul 17 00:40:50 2024 ] 	Batch(1000/6809) done. Loss: 0.0759  lr:0.000001
[ Wed Jul 17 00:41:13 2024 ] 	Batch(1100/6809) done. Loss: 0.0217  lr:0.000001
[ Wed Jul 17 00:41:36 2024 ] 	Batch(1200/6809) done. Loss: 0.0530  lr:0.000001
[ Wed Jul 17 00:41:58 2024 ] 	Batch(1300/6809) done. Loss: 0.0829  lr:0.000001
[ Wed Jul 17 00:42:21 2024 ] 	Batch(1400/6809) done. Loss: 0.1660  lr:0.000001
[ Wed Jul 17 00:42:44 2024 ] 
Training: Epoch [133/150], Step [1499], Loss: 0.025772346183657646, Training Accuracy: 95.76666666666667
[ Wed Jul 17 00:42:44 2024 ] 	Batch(1500/6809) done. Loss: 0.0431  lr:0.000001
[ Wed Jul 17 00:43:07 2024 ] 	Batch(1600/6809) done. Loss: 0.0553  lr:0.000001
[ Wed Jul 17 00:43:30 2024 ] 	Batch(1700/6809) done. Loss: 0.0603  lr:0.000001
[ Wed Jul 17 00:43:53 2024 ] 	Batch(1800/6809) done. Loss: 0.0274  lr:0.000001
[ Wed Jul 17 00:44:16 2024 ] 	Batch(1900/6809) done. Loss: 0.5945  lr:0.000001
[ Wed Jul 17 00:44:39 2024 ] 
Training: Epoch [133/150], Step [1999], Loss: 0.09384915232658386, Training Accuracy: 95.75
[ Wed Jul 17 00:44:39 2024 ] 	Batch(2000/6809) done. Loss: 0.1431  lr:0.000001
[ Wed Jul 17 00:45:02 2024 ] 	Batch(2100/6809) done. Loss: 0.0372  lr:0.000001
[ Wed Jul 17 00:45:26 2024 ] 	Batch(2200/6809) done. Loss: 0.0628  lr:0.000001
[ Wed Jul 17 00:45:49 2024 ] 	Batch(2300/6809) done. Loss: 0.2826  lr:0.000001
[ Wed Jul 17 00:46:12 2024 ] 	Batch(2400/6809) done. Loss: 0.0837  lr:0.000001
[ Wed Jul 17 00:46:35 2024 ] 
Training: Epoch [133/150], Step [2499], Loss: 0.19546382129192352, Training Accuracy: 95.74000000000001
[ Wed Jul 17 00:46:35 2024 ] 	Batch(2500/6809) done. Loss: 0.0892  lr:0.000001
[ Wed Jul 17 00:46:58 2024 ] 	Batch(2600/6809) done. Loss: 0.0241  lr:0.000001
[ Wed Jul 17 00:47:21 2024 ] 	Batch(2700/6809) done. Loss: 0.1947  lr:0.000001
[ Wed Jul 17 00:47:44 2024 ] 	Batch(2800/6809) done. Loss: 0.0814  lr:0.000001
[ Wed Jul 17 00:48:07 2024 ] 	Batch(2900/6809) done. Loss: 0.0502  lr:0.000001
[ Wed Jul 17 00:48:29 2024 ] 
Training: Epoch [133/150], Step [2999], Loss: 0.2682815492153168, Training Accuracy: 95.6875
[ Wed Jul 17 00:48:30 2024 ] 	Batch(3000/6809) done. Loss: 0.1638  lr:0.000001
[ Wed Jul 17 00:48:52 2024 ] 	Batch(3100/6809) done. Loss: 0.6108  lr:0.000001
[ Wed Jul 17 00:49:15 2024 ] 	Batch(3200/6809) done. Loss: 0.3481  lr:0.000001
[ Wed Jul 17 00:49:37 2024 ] 	Batch(3300/6809) done. Loss: 0.0906  lr:0.000001
[ Wed Jul 17 00:50:00 2024 ] 	Batch(3400/6809) done. Loss: 0.0332  lr:0.000001
[ Wed Jul 17 00:50:24 2024 ] 
Training: Epoch [133/150], Step [3499], Loss: 0.0335129089653492, Training Accuracy: 95.64642857142857
[ Wed Jul 17 00:50:24 2024 ] 	Batch(3500/6809) done. Loss: 0.5217  lr:0.000001
[ Wed Jul 17 00:50:47 2024 ] 	Batch(3600/6809) done. Loss: 0.1055  lr:0.000001
[ Wed Jul 17 00:51:10 2024 ] 	Batch(3700/6809) done. Loss: 0.0368  lr:0.000001
[ Wed Jul 17 00:51:33 2024 ] 	Batch(3800/6809) done. Loss: 0.0131  lr:0.000001
[ Wed Jul 17 00:51:55 2024 ] 	Batch(3900/6809) done. Loss: 0.2685  lr:0.000001
[ Wed Jul 17 00:52:18 2024 ] 
Training: Epoch [133/150], Step [3999], Loss: 0.05294407159090042, Training Accuracy: 95.7
[ Wed Jul 17 00:52:18 2024 ] 	Batch(4000/6809) done. Loss: 0.0564  lr:0.000001
[ Wed Jul 17 00:52:40 2024 ] 	Batch(4100/6809) done. Loss: 0.1950  lr:0.000001
[ Wed Jul 17 00:53:03 2024 ] 	Batch(4200/6809) done. Loss: 0.6257  lr:0.000001
[ Wed Jul 17 00:53:27 2024 ] 	Batch(4300/6809) done. Loss: 0.0188  lr:0.000001
[ Wed Jul 17 00:53:50 2024 ] 	Batch(4400/6809) done. Loss: 0.0154  lr:0.000001
[ Wed Jul 17 00:54:12 2024 ] 
Training: Epoch [133/150], Step [4499], Loss: 0.4109145998954773, Training Accuracy: 95.65555555555557
[ Wed Jul 17 00:54:12 2024 ] 	Batch(4500/6809) done. Loss: 0.0249  lr:0.000001
[ Wed Jul 17 00:54:35 2024 ] 	Batch(4600/6809) done. Loss: 0.2713  lr:0.000001
[ Wed Jul 17 00:54:58 2024 ] 	Batch(4700/6809) done. Loss: 0.1826  lr:0.000001
[ Wed Jul 17 00:55:21 2024 ] 	Batch(4800/6809) done. Loss: 0.1526  lr:0.000001
[ Wed Jul 17 00:55:44 2024 ] 	Batch(4900/6809) done. Loss: 0.3910  lr:0.000001
[ Wed Jul 17 00:56:07 2024 ] 
Training: Epoch [133/150], Step [4999], Loss: 0.034802697598934174, Training Accuracy: 95.67999999999999
[ Wed Jul 17 00:56:07 2024 ] 	Batch(5000/6809) done. Loss: 0.2277  lr:0.000001
[ Wed Jul 17 00:56:29 2024 ] 	Batch(5100/6809) done. Loss: 0.1734  lr:0.000001
[ Wed Jul 17 00:56:52 2024 ] 	Batch(5200/6809) done. Loss: 0.0419  lr:0.000001
[ Wed Jul 17 00:57:15 2024 ] 	Batch(5300/6809) done. Loss: 0.1169  lr:0.000001
[ Wed Jul 17 00:57:38 2024 ] 	Batch(5400/6809) done. Loss: 0.2547  lr:0.000001
[ Wed Jul 17 00:58:01 2024 ] 
Training: Epoch [133/150], Step [5499], Loss: 0.2544686794281006, Training Accuracy: 95.61818181818181
[ Wed Jul 17 00:58:01 2024 ] 	Batch(5500/6809) done. Loss: 0.0926  lr:0.000001
[ Wed Jul 17 00:58:24 2024 ] 	Batch(5600/6809) done. Loss: 0.1248  lr:0.000001
[ Wed Jul 17 00:58:47 2024 ] 	Batch(5700/6809) done. Loss: 0.0011  lr:0.000001
[ Wed Jul 17 00:59:10 2024 ] 	Batch(5800/6809) done. Loss: 0.2229  lr:0.000001
[ Wed Jul 17 00:59:33 2024 ] 	Batch(5900/6809) done. Loss: 0.0637  lr:0.000001
[ Wed Jul 17 00:59:56 2024 ] 
Training: Epoch [133/150], Step [5999], Loss: 0.07569040358066559, Training Accuracy: 95.67083333333333
[ Wed Jul 17 00:59:56 2024 ] 	Batch(6000/6809) done. Loss: 0.0557  lr:0.000001
[ Wed Jul 17 01:00:19 2024 ] 	Batch(6100/6809) done. Loss: 0.0771  lr:0.000001
[ Wed Jul 17 01:00:42 2024 ] 	Batch(6200/6809) done. Loss: 0.2998  lr:0.000001
[ Wed Jul 17 01:01:05 2024 ] 	Batch(6300/6809) done. Loss: 0.0076  lr:0.000001
[ Wed Jul 17 01:01:29 2024 ] 	Batch(6400/6809) done. Loss: 0.0160  lr:0.000001
[ Wed Jul 17 01:01:51 2024 ] 
Training: Epoch [133/150], Step [6499], Loss: 0.0622098334133625, Training Accuracy: 95.675
[ Wed Jul 17 01:01:52 2024 ] 	Batch(6500/6809) done. Loss: 0.2223  lr:0.000001
[ Wed Jul 17 01:02:15 2024 ] 	Batch(6600/6809) done. Loss: 0.2191  lr:0.000001
[ Wed Jul 17 01:02:37 2024 ] 	Batch(6700/6809) done. Loss: 0.3701  lr:0.000001
[ Wed Jul 17 01:03:01 2024 ] 	Batch(6800/6809) done. Loss: 0.2173  lr:0.000001
[ Wed Jul 17 01:03:03 2024 ] 	Mean training loss: 0.1571.
[ Wed Jul 17 01:03:03 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Wed Jul 17 01:03:03 2024 ] Training epoch: 135
[ Wed Jul 17 01:03:03 2024 ] 	Batch(0/6809) done. Loss: 0.2689  lr:0.000001
[ Wed Jul 17 01:03:26 2024 ] 	Batch(100/6809) done. Loss: 0.0767  lr:0.000001
[ Wed Jul 17 01:03:49 2024 ] 	Batch(200/6809) done. Loss: 0.0896  lr:0.000001
[ Wed Jul 17 01:04:12 2024 ] 	Batch(300/6809) done. Loss: 0.2527  lr:0.000001
[ Wed Jul 17 01:04:35 2024 ] 	Batch(400/6809) done. Loss: 0.0739  lr:0.000001
[ Wed Jul 17 01:04:58 2024 ] 
Training: Epoch [134/150], Step [499], Loss: 0.01045126374810934, Training Accuracy: 95.5
[ Wed Jul 17 01:04:58 2024 ] 	Batch(500/6809) done. Loss: 0.1469  lr:0.000001
[ Wed Jul 17 01:05:21 2024 ] 	Batch(600/6809) done. Loss: 0.0352  lr:0.000001
[ Wed Jul 17 01:05:44 2024 ] 	Batch(700/6809) done. Loss: 0.0789  lr:0.000001
[ Wed Jul 17 01:06:07 2024 ] 	Batch(800/6809) done. Loss: 0.5061  lr:0.000001
[ Wed Jul 17 01:06:31 2024 ] 	Batch(900/6809) done. Loss: 0.9372  lr:0.000001
[ Wed Jul 17 01:06:54 2024 ] 
Training: Epoch [134/150], Step [999], Loss: 0.11690427362918854, Training Accuracy: 95.55
[ Wed Jul 17 01:06:54 2024 ] 	Batch(1000/6809) done. Loss: 0.0136  lr:0.000001
[ Wed Jul 17 01:07:17 2024 ] 	Batch(1100/6809) done. Loss: 0.0621  lr:0.000001
[ Wed Jul 17 01:07:40 2024 ] 	Batch(1200/6809) done. Loss: 0.0075  lr:0.000001
[ Wed Jul 17 01:08:03 2024 ] 	Batch(1300/6809) done. Loss: 0.1028  lr:0.000001
[ Wed Jul 17 01:08:26 2024 ] 	Batch(1400/6809) done. Loss: 0.0131  lr:0.000001
[ Wed Jul 17 01:08:48 2024 ] 
Training: Epoch [134/150], Step [1499], Loss: 0.19389605522155762, Training Accuracy: 95.625
[ Wed Jul 17 01:08:49 2024 ] 	Batch(1500/6809) done. Loss: 0.0289  lr:0.000001
[ Wed Jul 17 01:09:11 2024 ] 	Batch(1600/6809) done. Loss: 0.1484  lr:0.000001
[ Wed Jul 17 01:09:35 2024 ] 	Batch(1700/6809) done. Loss: 0.5805  lr:0.000001
[ Wed Jul 17 01:09:57 2024 ] 	Batch(1800/6809) done. Loss: 0.0547  lr:0.000001
[ Wed Jul 17 01:10:20 2024 ] 	Batch(1900/6809) done. Loss: 0.0600  lr:0.000001
[ Wed Jul 17 01:10:43 2024 ] 
Training: Epoch [134/150], Step [1999], Loss: 0.00418294221162796, Training Accuracy: 95.56875000000001
[ Wed Jul 17 01:10:43 2024 ] 	Batch(2000/6809) done. Loss: 0.6435  lr:0.000001
[ Wed Jul 17 01:11:07 2024 ] 	Batch(2100/6809) done. Loss: 0.0157  lr:0.000001
[ Wed Jul 17 01:11:30 2024 ] 	Batch(2200/6809) done. Loss: 0.0180  lr:0.000001
[ Wed Jul 17 01:11:54 2024 ] 	Batch(2300/6809) done. Loss: 0.0050  lr:0.000001
[ Wed Jul 17 01:12:17 2024 ] 	Batch(2400/6809) done. Loss: 0.1593  lr:0.000001
[ Wed Jul 17 01:12:40 2024 ] 
Training: Epoch [134/150], Step [2499], Loss: 0.16301921010017395, Training Accuracy: 95.655
[ Wed Jul 17 01:12:40 2024 ] 	Batch(2500/6809) done. Loss: 0.1594  lr:0.000001
[ Wed Jul 17 01:13:03 2024 ] 	Batch(2600/6809) done. Loss: 0.4982  lr:0.000001
[ Wed Jul 17 01:13:26 2024 ] 	Batch(2700/6809) done. Loss: 0.3034  lr:0.000001
[ Wed Jul 17 01:13:49 2024 ] 	Batch(2800/6809) done. Loss: 0.1551  lr:0.000001
[ Wed Jul 17 01:14:12 2024 ] 	Batch(2900/6809) done. Loss: 0.1922  lr:0.000001
[ Wed Jul 17 01:14:34 2024 ] 
Training: Epoch [134/150], Step [2999], Loss: 0.055854447185993195, Training Accuracy: 95.62083333333334
[ Wed Jul 17 01:14:34 2024 ] 	Batch(3000/6809) done. Loss: 0.1426  lr:0.000001
[ Wed Jul 17 01:14:57 2024 ] 	Batch(3100/6809) done. Loss: 0.1301  lr:0.000001
[ Wed Jul 17 01:15:20 2024 ] 	Batch(3200/6809) done. Loss: 0.4133  lr:0.000001
[ Wed Jul 17 01:15:43 2024 ] 	Batch(3300/6809) done. Loss: 0.0243  lr:0.000001
[ Wed Jul 17 01:16:06 2024 ] 	Batch(3400/6809) done. Loss: 0.2335  lr:0.000001
[ Wed Jul 17 01:16:28 2024 ] 
Training: Epoch [134/150], Step [3499], Loss: 0.048493511974811554, Training Accuracy: 95.675
[ Wed Jul 17 01:16:28 2024 ] 	Batch(3500/6809) done. Loss: 0.1478  lr:0.000001
[ Wed Jul 17 01:16:51 2024 ] 	Batch(3600/6809) done. Loss: 0.1476  lr:0.000001
[ Wed Jul 17 01:17:15 2024 ] 	Batch(3700/6809) done. Loss: 0.3482  lr:0.000001
[ Wed Jul 17 01:17:37 2024 ] 	Batch(3800/6809) done. Loss: 0.0397  lr:0.000001
[ Wed Jul 17 01:18:00 2024 ] 	Batch(3900/6809) done. Loss: 0.0327  lr:0.000001
[ Wed Jul 17 01:18:23 2024 ] 
Training: Epoch [134/150], Step [3999], Loss: 0.004246829077601433, Training Accuracy: 95.7
[ Wed Jul 17 01:18:23 2024 ] 	Batch(4000/6809) done. Loss: 0.0209  lr:0.000001
[ Wed Jul 17 01:18:46 2024 ] 	Batch(4100/6809) done. Loss: 0.1996  lr:0.000001
[ Wed Jul 17 01:19:10 2024 ] 	Batch(4200/6809) done. Loss: 0.0096  lr:0.000001
[ Wed Jul 17 01:19:33 2024 ] 	Batch(4300/6809) done. Loss: 0.0066  lr:0.000001
[ Wed Jul 17 01:19:55 2024 ] 	Batch(4400/6809) done. Loss: 0.0205  lr:0.000001
[ Wed Jul 17 01:20:18 2024 ] 
Training: Epoch [134/150], Step [4499], Loss: 0.16072721779346466, Training Accuracy: 95.69166666666666
[ Wed Jul 17 01:20:18 2024 ] 	Batch(4500/6809) done. Loss: 0.2134  lr:0.000001
[ Wed Jul 17 01:20:41 2024 ] 	Batch(4600/6809) done. Loss: 0.2009  lr:0.000001
[ Wed Jul 17 01:21:03 2024 ] 	Batch(4700/6809) done. Loss: 0.0417  lr:0.000001
[ Wed Jul 17 01:21:26 2024 ] 	Batch(4800/6809) done. Loss: 0.0713  lr:0.000001
[ Wed Jul 17 01:21:49 2024 ] 	Batch(4900/6809) done. Loss: 0.7178  lr:0.000001
[ Wed Jul 17 01:22:11 2024 ] 
Training: Epoch [134/150], Step [4999], Loss: 0.028356757014989853, Training Accuracy: 95.66
[ Wed Jul 17 01:22:11 2024 ] 	Batch(5000/6809) done. Loss: 0.1067  lr:0.000001
[ Wed Jul 17 01:22:34 2024 ] 	Batch(5100/6809) done. Loss: 0.0338  lr:0.000001
[ Wed Jul 17 01:22:56 2024 ] 	Batch(5200/6809) done. Loss: 0.0250  lr:0.000001
[ Wed Jul 17 01:23:19 2024 ] 	Batch(5300/6809) done. Loss: 0.1367  lr:0.000001
[ Wed Jul 17 01:23:42 2024 ] 	Batch(5400/6809) done. Loss: 0.2232  lr:0.000001
[ Wed Jul 17 01:24:04 2024 ] 
Training: Epoch [134/150], Step [5499], Loss: 0.15775468945503235, Training Accuracy: 95.69545454545455
[ Wed Jul 17 01:24:04 2024 ] 	Batch(5500/6809) done. Loss: 0.5373  lr:0.000001
[ Wed Jul 17 01:24:27 2024 ] 	Batch(5600/6809) done. Loss: 0.2176  lr:0.000001
[ Wed Jul 17 01:24:50 2024 ] 	Batch(5700/6809) done. Loss: 0.1810  lr:0.000001
[ Wed Jul 17 01:25:14 2024 ] 	Batch(5800/6809) done. Loss: 0.2444  lr:0.000001
[ Wed Jul 17 01:25:37 2024 ] 	Batch(5900/6809) done. Loss: 0.0198  lr:0.000001
[ Wed Jul 17 01:26:00 2024 ] 
Training: Epoch [134/150], Step [5999], Loss: 0.20872467756271362, Training Accuracy: 95.68124999999999
[ Wed Jul 17 01:26:01 2024 ] 	Batch(6000/6809) done. Loss: 0.0922  lr:0.000001
[ Wed Jul 17 01:26:24 2024 ] 	Batch(6100/6809) done. Loss: 0.0218  lr:0.000001
[ Wed Jul 17 01:26:47 2024 ] 	Batch(6200/6809) done. Loss: 0.0103  lr:0.000001
[ Wed Jul 17 01:27:10 2024 ] 	Batch(6300/6809) done. Loss: 0.1113  lr:0.000001
[ Wed Jul 17 01:27:33 2024 ] 	Batch(6400/6809) done. Loss: 0.1513  lr:0.000001
[ Wed Jul 17 01:27:56 2024 ] 
Training: Epoch [134/150], Step [6499], Loss: 0.00537900673225522, Training Accuracy: 95.65961538461538
[ Wed Jul 17 01:27:56 2024 ] 	Batch(6500/6809) done. Loss: 0.0475  lr:0.000001
[ Wed Jul 17 01:28:19 2024 ] 	Batch(6600/6809) done. Loss: 0.0447  lr:0.000001
[ Wed Jul 17 01:28:43 2024 ] 	Batch(6700/6809) done. Loss: 0.3963  lr:0.000001
[ Wed Jul 17 01:29:06 2024 ] 	Batch(6800/6809) done. Loss: 0.4145  lr:0.000001
[ Wed Jul 17 01:29:08 2024 ] 	Mean training loss: 0.1523.
[ Wed Jul 17 01:29:08 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 01:29:08 2024 ] Training epoch: 136
[ Wed Jul 17 01:29:08 2024 ] 	Batch(0/6809) done. Loss: 0.1500  lr:0.000001
[ Wed Jul 17 01:29:31 2024 ] 	Batch(100/6809) done. Loss: 0.1605  lr:0.000001
[ Wed Jul 17 01:29:54 2024 ] 	Batch(200/6809) done. Loss: 0.2364  lr:0.000001
[ Wed Jul 17 01:30:17 2024 ] 	Batch(300/6809) done. Loss: 0.2162  lr:0.000001
[ Wed Jul 17 01:30:40 2024 ] 	Batch(400/6809) done. Loss: 0.0661  lr:0.000001
[ Wed Jul 17 01:31:03 2024 ] 
Training: Epoch [135/150], Step [499], Loss: 0.076320581138134, Training Accuracy: 95.89999999999999
[ Wed Jul 17 01:31:03 2024 ] 	Batch(500/6809) done. Loss: 0.0300  lr:0.000001
[ Wed Jul 17 01:31:26 2024 ] 	Batch(600/6809) done. Loss: 0.0213  lr:0.000001
[ Wed Jul 17 01:31:49 2024 ] 	Batch(700/6809) done. Loss: 0.3690  lr:0.000001
[ Wed Jul 17 01:32:12 2024 ] 	Batch(800/6809) done. Loss: 0.2778  lr:0.000001
[ Wed Jul 17 01:32:35 2024 ] 	Batch(900/6809) done. Loss: 0.0940  lr:0.000001
[ Wed Jul 17 01:32:57 2024 ] 
Training: Epoch [135/150], Step [999], Loss: 0.31816211342811584, Training Accuracy: 96.0625
[ Wed Jul 17 01:32:58 2024 ] 	Batch(1000/6809) done. Loss: 0.0010  lr:0.000001
[ Wed Jul 17 01:33:20 2024 ] 	Batch(1100/6809) done. Loss: 0.1231  lr:0.000001
[ Wed Jul 17 01:33:43 2024 ] 	Batch(1200/6809) done. Loss: 0.3777  lr:0.000001
[ Wed Jul 17 01:34:05 2024 ] 	Batch(1300/6809) done. Loss: 0.0204  lr:0.000001
[ Wed Jul 17 01:34:28 2024 ] 	Batch(1400/6809) done. Loss: 0.3753  lr:0.000001
[ Wed Jul 17 01:34:50 2024 ] 
Training: Epoch [135/150], Step [1499], Loss: 0.023021001368761063, Training Accuracy: 96.125
[ Wed Jul 17 01:34:50 2024 ] 	Batch(1500/6809) done. Loss: 0.1111  lr:0.000001
[ Wed Jul 17 01:35:13 2024 ] 	Batch(1600/6809) done. Loss: 0.2874  lr:0.000001
[ Wed Jul 17 01:35:36 2024 ] 	Batch(1700/6809) done. Loss: 0.0354  lr:0.000001
[ Wed Jul 17 01:35:58 2024 ] 	Batch(1800/6809) done. Loss: 0.3963  lr:0.000001
[ Wed Jul 17 01:36:21 2024 ] 	Batch(1900/6809) done. Loss: 0.0122  lr:0.000001
[ Wed Jul 17 01:36:43 2024 ] 
Training: Epoch [135/150], Step [1999], Loss: 0.22519917786121368, Training Accuracy: 96.19375
[ Wed Jul 17 01:36:44 2024 ] 	Batch(2000/6809) done. Loss: 0.5677  lr:0.000001
[ Wed Jul 17 01:37:06 2024 ] 	Batch(2100/6809) done. Loss: 0.0046  lr:0.000001
[ Wed Jul 17 01:37:29 2024 ] 	Batch(2200/6809) done. Loss: 0.0474  lr:0.000001
[ Wed Jul 17 01:37:51 2024 ] 	Batch(2300/6809) done. Loss: 0.0015  lr:0.000001
[ Wed Jul 17 01:38:14 2024 ] 	Batch(2400/6809) done. Loss: 0.0395  lr:0.000001
[ Wed Jul 17 01:38:36 2024 ] 
Training: Epoch [135/150], Step [2499], Loss: 0.32774415612220764, Training Accuracy: 96.05
[ Wed Jul 17 01:38:37 2024 ] 	Batch(2500/6809) done. Loss: 0.0163  lr:0.000001
[ Wed Jul 17 01:38:59 2024 ] 	Batch(2600/6809) done. Loss: 0.0137  lr:0.000001
[ Wed Jul 17 01:39:22 2024 ] 	Batch(2700/6809) done. Loss: 0.7010  lr:0.000001
[ Wed Jul 17 01:39:45 2024 ] 	Batch(2800/6809) done. Loss: 0.1076  lr:0.000001
[ Wed Jul 17 01:40:07 2024 ] 	Batch(2900/6809) done. Loss: 0.0509  lr:0.000001
[ Wed Jul 17 01:40:30 2024 ] 
Training: Epoch [135/150], Step [2999], Loss: 0.001771113951690495, Training Accuracy: 95.99583333333334
[ Wed Jul 17 01:40:30 2024 ] 	Batch(3000/6809) done. Loss: 0.0647  lr:0.000001
[ Wed Jul 17 01:40:52 2024 ] 	Batch(3100/6809) done. Loss: 0.3973  lr:0.000001
[ Wed Jul 17 01:41:15 2024 ] 	Batch(3200/6809) done. Loss: 0.0437  lr:0.000001
[ Wed Jul 17 01:41:38 2024 ] 	Batch(3300/6809) done. Loss: 0.1356  lr:0.000001
[ Wed Jul 17 01:42:00 2024 ] 	Batch(3400/6809) done. Loss: 0.0908  lr:0.000001
[ Wed Jul 17 01:42:22 2024 ] 
Training: Epoch [135/150], Step [3499], Loss: 0.814306378364563, Training Accuracy: 95.99285714285715
[ Wed Jul 17 01:42:23 2024 ] 	Batch(3500/6809) done. Loss: 0.1928  lr:0.000001
[ Wed Jul 17 01:42:45 2024 ] 	Batch(3600/6809) done. Loss: 0.2584  lr:0.000001
[ Wed Jul 17 01:43:08 2024 ] 	Batch(3700/6809) done. Loss: 0.0662  lr:0.000001
[ Wed Jul 17 01:43:31 2024 ] 	Batch(3800/6809) done. Loss: 0.6833  lr:0.000001
[ Wed Jul 17 01:43:53 2024 ] 	Batch(3900/6809) done. Loss: 0.0590  lr:0.000001
[ Wed Jul 17 01:44:16 2024 ] 
Training: Epoch [135/150], Step [3999], Loss: 0.025972889736294746, Training Accuracy: 95.928125
[ Wed Jul 17 01:44:16 2024 ] 	Batch(4000/6809) done. Loss: 0.0808  lr:0.000001
[ Wed Jul 17 01:44:38 2024 ] 	Batch(4100/6809) done. Loss: 0.0591  lr:0.000001
[ Wed Jul 17 01:45:01 2024 ] 	Batch(4200/6809) done. Loss: 0.3333  lr:0.000001
[ Wed Jul 17 01:45:24 2024 ] 	Batch(4300/6809) done. Loss: 0.1616  lr:0.000001
[ Wed Jul 17 01:45:46 2024 ] 	Batch(4400/6809) done. Loss: 0.4644  lr:0.000001
[ Wed Jul 17 01:46:09 2024 ] 
Training: Epoch [135/150], Step [4499], Loss: 0.030119892209768295, Training Accuracy: 95.83611111111111
[ Wed Jul 17 01:46:09 2024 ] 	Batch(4500/6809) done. Loss: 0.0157  lr:0.000001
[ Wed Jul 17 01:46:32 2024 ] 	Batch(4600/6809) done. Loss: 0.0416  lr:0.000001
[ Wed Jul 17 01:46:54 2024 ] 	Batch(4700/6809) done. Loss: 0.1115  lr:0.000001
[ Wed Jul 17 01:47:17 2024 ] 	Batch(4800/6809) done. Loss: 0.1978  lr:0.000001
[ Wed Jul 17 01:47:40 2024 ] 	Batch(4900/6809) done. Loss: 0.0629  lr:0.000001
[ Wed Jul 17 01:48:02 2024 ] 
Training: Epoch [135/150], Step [4999], Loss: 0.07317177951335907, Training Accuracy: 95.7925
[ Wed Jul 17 01:48:02 2024 ] 	Batch(5000/6809) done. Loss: 0.1272  lr:0.000001
[ Wed Jul 17 01:48:25 2024 ] 	Batch(5100/6809) done. Loss: 0.0471  lr:0.000001
[ Wed Jul 17 01:48:48 2024 ] 	Batch(5200/6809) done. Loss: 0.0446  lr:0.000001
[ Wed Jul 17 01:49:10 2024 ] 	Batch(5300/6809) done. Loss: 0.0296  lr:0.000001
[ Wed Jul 17 01:49:33 2024 ] 	Batch(5400/6809) done. Loss: 0.2328  lr:0.000001
[ Wed Jul 17 01:49:55 2024 ] 
Training: Epoch [135/150], Step [5499], Loss: 0.4952889084815979, Training Accuracy: 95.80454545454545
[ Wed Jul 17 01:49:55 2024 ] 	Batch(5500/6809) done. Loss: 0.1768  lr:0.000001
[ Wed Jul 17 01:50:18 2024 ] 	Batch(5600/6809) done. Loss: 0.0187  lr:0.000001
[ Wed Jul 17 01:50:41 2024 ] 	Batch(5700/6809) done. Loss: 0.1164  lr:0.000001
[ Wed Jul 17 01:51:03 2024 ] 	Batch(5800/6809) done. Loss: 0.0222  lr:0.000001
[ Wed Jul 17 01:51:26 2024 ] 	Batch(5900/6809) done. Loss: 0.1487  lr:0.000001
[ Wed Jul 17 01:51:48 2024 ] 
Training: Epoch [135/150], Step [5999], Loss: 0.17918428778648376, Training Accuracy: 95.8
[ Wed Jul 17 01:51:49 2024 ] 	Batch(6000/6809) done. Loss: 0.0768  lr:0.000001
[ Wed Jul 17 01:52:11 2024 ] 	Batch(6100/6809) done. Loss: 0.0702  lr:0.000001
[ Wed Jul 17 01:52:34 2024 ] 	Batch(6200/6809) done. Loss: 0.2103  lr:0.000001
[ Wed Jul 17 01:52:56 2024 ] 	Batch(6300/6809) done. Loss: 0.0554  lr:0.000001
[ Wed Jul 17 01:53:19 2024 ] 	Batch(6400/6809) done. Loss: 0.3361  lr:0.000001
[ Wed Jul 17 01:53:42 2024 ] 
Training: Epoch [135/150], Step [6499], Loss: 0.08033790439367294, Training Accuracy: 95.80576923076923
[ Wed Jul 17 01:53:42 2024 ] 	Batch(6500/6809) done. Loss: 0.2188  lr:0.000001
[ Wed Jul 17 01:54:04 2024 ] 	Batch(6600/6809) done. Loss: 0.3929  lr:0.000001
[ Wed Jul 17 01:54:27 2024 ] 	Batch(6700/6809) done. Loss: 0.0875  lr:0.000001
[ Wed Jul 17 01:54:50 2024 ] 	Batch(6800/6809) done. Loss: 0.2101  lr:0.000001
[ Wed Jul 17 01:54:52 2024 ] 	Mean training loss: 0.1546.
[ Wed Jul 17 01:54:52 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 01:54:52 2024 ] Training epoch: 137
[ Wed Jul 17 01:54:52 2024 ] 	Batch(0/6809) done. Loss: 0.1343  lr:0.000001
[ Wed Jul 17 01:55:16 2024 ] 	Batch(100/6809) done. Loss: 0.0378  lr:0.000001
[ Wed Jul 17 01:55:39 2024 ] 	Batch(200/6809) done. Loss: 0.0605  lr:0.000001
[ Wed Jul 17 01:56:03 2024 ] 	Batch(300/6809) done. Loss: 0.1192  lr:0.000001
[ Wed Jul 17 01:56:26 2024 ] 	Batch(400/6809) done. Loss: 0.0545  lr:0.000001
[ Wed Jul 17 01:56:49 2024 ] 
Training: Epoch [136/150], Step [499], Loss: 0.16310130059719086, Training Accuracy: 95.75
[ Wed Jul 17 01:56:49 2024 ] 	Batch(500/6809) done. Loss: 0.0384  lr:0.000001
[ Wed Jul 17 01:57:13 2024 ] 	Batch(600/6809) done. Loss: 0.2772  lr:0.000001
[ Wed Jul 17 01:57:36 2024 ] 	Batch(700/6809) done. Loss: 0.2928  lr:0.000001
[ Wed Jul 17 01:57:59 2024 ] 	Batch(800/6809) done. Loss: 0.0469  lr:0.000001
[ Wed Jul 17 01:58:21 2024 ] 	Batch(900/6809) done. Loss: 0.1430  lr:0.000001
[ Wed Jul 17 01:58:44 2024 ] 
Training: Epoch [136/150], Step [999], Loss: 0.297303169965744, Training Accuracy: 95.675
[ Wed Jul 17 01:58:44 2024 ] 	Batch(1000/6809) done. Loss: 0.0551  lr:0.000001
[ Wed Jul 17 01:59:07 2024 ] 	Batch(1100/6809) done. Loss: 0.2208  lr:0.000001
[ Wed Jul 17 01:59:29 2024 ] 	Batch(1200/6809) done. Loss: 0.1355  lr:0.000001
[ Wed Jul 17 01:59:52 2024 ] 	Batch(1300/6809) done. Loss: 0.0772  lr:0.000001
[ Wed Jul 17 02:00:15 2024 ] 	Batch(1400/6809) done. Loss: 0.1155  lr:0.000001
[ Wed Jul 17 02:00:37 2024 ] 
Training: Epoch [136/150], Step [1499], Loss: 0.243486225605011, Training Accuracy: 95.60833333333333
[ Wed Jul 17 02:00:38 2024 ] 	Batch(1500/6809) done. Loss: 0.0900  lr:0.000001
[ Wed Jul 17 02:01:00 2024 ] 	Batch(1600/6809) done. Loss: 0.0150  lr:0.000001
[ Wed Jul 17 02:01:24 2024 ] 	Batch(1700/6809) done. Loss: 0.3038  lr:0.000001
[ Wed Jul 17 02:01:47 2024 ] 	Batch(1800/6809) done. Loss: 0.0227  lr:0.000001
[ Wed Jul 17 02:02:10 2024 ] 	Batch(1900/6809) done. Loss: 0.1391  lr:0.000001
[ Wed Jul 17 02:02:32 2024 ] 
Training: Epoch [136/150], Step [1999], Loss: 0.26340076327323914, Training Accuracy: 95.675
[ Wed Jul 17 02:02:33 2024 ] 	Batch(2000/6809) done. Loss: 0.0587  lr:0.000001
[ Wed Jul 17 02:02:56 2024 ] 	Batch(2100/6809) done. Loss: 0.0840  lr:0.000001
[ Wed Jul 17 02:03:19 2024 ] 	Batch(2200/6809) done. Loss: 0.0633  lr:0.000001
[ Wed Jul 17 02:03:41 2024 ] 	Batch(2300/6809) done. Loss: 0.3486  lr:0.000001
[ Wed Jul 17 02:04:04 2024 ] 	Batch(2400/6809) done. Loss: 0.4717  lr:0.000001
[ Wed Jul 17 02:04:27 2024 ] 
Training: Epoch [136/150], Step [2499], Loss: 0.27360910177230835, Training Accuracy: 95.66
[ Wed Jul 17 02:04:27 2024 ] 	Batch(2500/6809) done. Loss: 0.0037  lr:0.000001
[ Wed Jul 17 02:04:49 2024 ] 	Batch(2600/6809) done. Loss: 0.3094  lr:0.000001
[ Wed Jul 17 02:05:12 2024 ] 	Batch(2700/6809) done. Loss: 0.0987  lr:0.000001
[ Wed Jul 17 02:05:35 2024 ] 	Batch(2800/6809) done. Loss: 0.1768  lr:0.000001
[ Wed Jul 17 02:05:58 2024 ] 	Batch(2900/6809) done. Loss: 0.8866  lr:0.000001
[ Wed Jul 17 02:06:21 2024 ] 
Training: Epoch [136/150], Step [2999], Loss: 0.04984406381845474, Training Accuracy: 95.61666666666667
[ Wed Jul 17 02:06:21 2024 ] 	Batch(3000/6809) done. Loss: 0.0338  lr:0.000001
[ Wed Jul 17 02:06:43 2024 ] 	Batch(3100/6809) done. Loss: 0.1946  lr:0.000001
[ Wed Jul 17 02:07:07 2024 ] 	Batch(3200/6809) done. Loss: 0.0565  lr:0.000001
[ Wed Jul 17 02:07:30 2024 ] 	Batch(3300/6809) done. Loss: 0.0488  lr:0.000001
[ Wed Jul 17 02:07:52 2024 ] 	Batch(3400/6809) done. Loss: 0.2886  lr:0.000001
[ Wed Jul 17 02:08:15 2024 ] 
Training: Epoch [136/150], Step [3499], Loss: 0.01876126602292061, Training Accuracy: 95.61071428571428
[ Wed Jul 17 02:08:15 2024 ] 	Batch(3500/6809) done. Loss: 0.0982  lr:0.000001
[ Wed Jul 17 02:08:38 2024 ] 	Batch(3600/6809) done. Loss: 0.0833  lr:0.000001
[ Wed Jul 17 02:09:00 2024 ] 	Batch(3700/6809) done. Loss: 0.1791  lr:0.000001
[ Wed Jul 17 02:09:23 2024 ] 	Batch(3800/6809) done. Loss: 0.3913  lr:0.000001
[ Wed Jul 17 02:09:46 2024 ] 	Batch(3900/6809) done. Loss: 0.2327  lr:0.000001
[ Wed Jul 17 02:10:08 2024 ] 
Training: Epoch [136/150], Step [3999], Loss: 0.5047000050544739, Training Accuracy: 95.59375
[ Wed Jul 17 02:10:08 2024 ] 	Batch(4000/6809) done. Loss: 0.2938  lr:0.000001
[ Wed Jul 17 02:10:31 2024 ] 	Batch(4100/6809) done. Loss: 0.0318  lr:0.000001
[ Wed Jul 17 02:10:53 2024 ] 	Batch(4200/6809) done. Loss: 0.0492  lr:0.000001
[ Wed Jul 17 02:11:16 2024 ] 	Batch(4300/6809) done. Loss: 0.0830  lr:0.000001
[ Wed Jul 17 02:11:39 2024 ] 	Batch(4400/6809) done. Loss: 0.2735  lr:0.000001
[ Wed Jul 17 02:12:02 2024 ] 
Training: Epoch [136/150], Step [4499], Loss: 0.1366426646709442, Training Accuracy: 95.58055555555556
[ Wed Jul 17 02:12:02 2024 ] 	Batch(4500/6809) done. Loss: 0.4295  lr:0.000001
[ Wed Jul 17 02:12:25 2024 ] 	Batch(4600/6809) done. Loss: 0.1529  lr:0.000001
[ Wed Jul 17 02:12:49 2024 ] 	Batch(4700/6809) done. Loss: 0.0462  lr:0.000001
[ Wed Jul 17 02:13:12 2024 ] 	Batch(4800/6809) done. Loss: 0.5170  lr:0.000001
[ Wed Jul 17 02:13:35 2024 ] 	Batch(4900/6809) done. Loss: 0.0992  lr:0.000001
[ Wed Jul 17 02:13:57 2024 ] 
Training: Epoch [136/150], Step [4999], Loss: 0.2913666367530823, Training Accuracy: 95.58
[ Wed Jul 17 02:13:57 2024 ] 	Batch(5000/6809) done. Loss: 0.1113  lr:0.000001
[ Wed Jul 17 02:14:20 2024 ] 	Batch(5100/6809) done. Loss: 0.0471  lr:0.000001
[ Wed Jul 17 02:14:43 2024 ] 	Batch(5200/6809) done. Loss: 0.2533  lr:0.000001
[ Wed Jul 17 02:15:05 2024 ] 	Batch(5300/6809) done. Loss: 0.2487  lr:0.000001
[ Wed Jul 17 02:15:28 2024 ] 	Batch(5400/6809) done. Loss: 0.0012  lr:0.000001
[ Wed Jul 17 02:15:50 2024 ] 
Training: Epoch [136/150], Step [5499], Loss: 0.2262599766254425, Training Accuracy: 95.63863636363637
[ Wed Jul 17 02:15:51 2024 ] 	Batch(5500/6809) done. Loss: 0.1217  lr:0.000001
[ Wed Jul 17 02:16:13 2024 ] 	Batch(5600/6809) done. Loss: 0.0585  lr:0.000001
[ Wed Jul 17 02:16:36 2024 ] 	Batch(5700/6809) done. Loss: 0.6143  lr:0.000001
[ Wed Jul 17 02:16:58 2024 ] 	Batch(5800/6809) done. Loss: 0.1070  lr:0.000001
[ Wed Jul 17 02:17:21 2024 ] 	Batch(5900/6809) done. Loss: 0.3157  lr:0.000001
[ Wed Jul 17 02:17:44 2024 ] 
Training: Epoch [136/150], Step [5999], Loss: 0.16671012341976166, Training Accuracy: 95.65416666666667
[ Wed Jul 17 02:17:44 2024 ] 	Batch(6000/6809) done. Loss: 0.7272  lr:0.000001
[ Wed Jul 17 02:18:07 2024 ] 	Batch(6100/6809) done. Loss: 0.2550  lr:0.000001
[ Wed Jul 17 02:18:29 2024 ] 	Batch(6200/6809) done. Loss: 0.0058  lr:0.000001
[ Wed Jul 17 02:18:52 2024 ] 	Batch(6300/6809) done. Loss: 0.0048  lr:0.000001
[ Wed Jul 17 02:19:15 2024 ] 	Batch(6400/6809) done. Loss: 0.1797  lr:0.000001
[ Wed Jul 17 02:19:38 2024 ] 
Training: Epoch [136/150], Step [6499], Loss: 0.02789197862148285, Training Accuracy: 95.68076923076923
[ Wed Jul 17 02:19:38 2024 ] 	Batch(6500/6809) done. Loss: 0.1700  lr:0.000001
[ Wed Jul 17 02:20:00 2024 ] 	Batch(6600/6809) done. Loss: 0.2605  lr:0.000001
[ Wed Jul 17 02:20:23 2024 ] 	Batch(6700/6809) done. Loss: 0.2174  lr:0.000001
[ Wed Jul 17 02:20:46 2024 ] 	Batch(6800/6809) done. Loss: 0.1787  lr:0.000001
[ Wed Jul 17 02:20:48 2024 ] 	Mean training loss: 0.1513.
[ Wed Jul 17 02:20:48 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 02:20:48 2024 ] Training epoch: 138
[ Wed Jul 17 02:20:49 2024 ] 	Batch(0/6809) done. Loss: 0.6277  lr:0.000001
[ Wed Jul 17 02:21:11 2024 ] 	Batch(100/6809) done. Loss: 0.1661  lr:0.000001
[ Wed Jul 17 02:21:34 2024 ] 	Batch(200/6809) done. Loss: 0.1978  lr:0.000001
[ Wed Jul 17 02:21:56 2024 ] 	Batch(300/6809) done. Loss: 0.0263  lr:0.000001
[ Wed Jul 17 02:22:19 2024 ] 	Batch(400/6809) done. Loss: 0.0142  lr:0.000001
[ Wed Jul 17 02:22:41 2024 ] 
Training: Epoch [137/150], Step [499], Loss: 0.24927197396755219, Training Accuracy: 95.575
[ Wed Jul 17 02:22:42 2024 ] 	Batch(500/6809) done. Loss: 0.0594  lr:0.000001
[ Wed Jul 17 02:23:04 2024 ] 	Batch(600/6809) done. Loss: 0.0023  lr:0.000001
[ Wed Jul 17 02:23:27 2024 ] 	Batch(700/6809) done. Loss: 0.2799  lr:0.000001
[ Wed Jul 17 02:23:50 2024 ] 	Batch(800/6809) done. Loss: 0.1512  lr:0.000001
[ Wed Jul 17 02:24:13 2024 ] 	Batch(900/6809) done. Loss: 0.3954  lr:0.000001
[ Wed Jul 17 02:24:36 2024 ] 
Training: Epoch [137/150], Step [999], Loss: 0.054771777242422104, Training Accuracy: 95.6375
[ Wed Jul 17 02:24:36 2024 ] 	Batch(1000/6809) done. Loss: 0.1117  lr:0.000001
[ Wed Jul 17 02:24:59 2024 ] 	Batch(1100/6809) done. Loss: 0.0963  lr:0.000001
[ Wed Jul 17 02:25:22 2024 ] 	Batch(1200/6809) done. Loss: 0.1989  lr:0.000001
[ Wed Jul 17 02:25:45 2024 ] 	Batch(1300/6809) done. Loss: 0.5322  lr:0.000001
[ Wed Jul 17 02:26:08 2024 ] 	Batch(1400/6809) done. Loss: 0.0505  lr:0.000001
[ Wed Jul 17 02:26:30 2024 ] 
Training: Epoch [137/150], Step [1499], Loss: 0.4261924624443054, Training Accuracy: 95.70833333333333
[ Wed Jul 17 02:26:31 2024 ] 	Batch(1500/6809) done. Loss: 0.0260  lr:0.000001
[ Wed Jul 17 02:26:53 2024 ] 	Batch(1600/6809) done. Loss: 0.0204  lr:0.000001
[ Wed Jul 17 02:27:16 2024 ] 	Batch(1700/6809) done. Loss: 0.4233  lr:0.000001
[ Wed Jul 17 02:27:39 2024 ] 	Batch(1800/6809) done. Loss: 0.0243  lr:0.000001
[ Wed Jul 17 02:28:01 2024 ] 	Batch(1900/6809) done. Loss: 0.0548  lr:0.000001
[ Wed Jul 17 02:28:24 2024 ] 
Training: Epoch [137/150], Step [1999], Loss: 0.014005676843225956, Training Accuracy: 95.76249999999999
[ Wed Jul 17 02:28:24 2024 ] 	Batch(2000/6809) done. Loss: 0.5064  lr:0.000001
[ Wed Jul 17 02:28:47 2024 ] 	Batch(2100/6809) done. Loss: 0.0211  lr:0.000001
[ Wed Jul 17 02:29:10 2024 ] 	Batch(2200/6809) done. Loss: 0.3128  lr:0.000001
[ Wed Jul 17 02:29:32 2024 ] 	Batch(2300/6809) done. Loss: 0.1704  lr:0.000001
[ Wed Jul 17 02:29:55 2024 ] 	Batch(2400/6809) done. Loss: 0.0258  lr:0.000001
[ Wed Jul 17 02:30:18 2024 ] 
Training: Epoch [137/150], Step [2499], Loss: 0.3565775454044342, Training Accuracy: 95.71499999999999
[ Wed Jul 17 02:30:18 2024 ] 	Batch(2500/6809) done. Loss: 0.0494  lr:0.000001
[ Wed Jul 17 02:30:41 2024 ] 	Batch(2600/6809) done. Loss: 0.0104  lr:0.000001
[ Wed Jul 17 02:31:04 2024 ] 	Batch(2700/6809) done. Loss: 0.1024  lr:0.000001
[ Wed Jul 17 02:31:27 2024 ] 	Batch(2800/6809) done. Loss: 0.1213  lr:0.000001
[ Wed Jul 17 02:31:50 2024 ] 	Batch(2900/6809) done. Loss: 0.0145  lr:0.000001
[ Wed Jul 17 02:32:13 2024 ] 
Training: Epoch [137/150], Step [2999], Loss: 0.07637248188257217, Training Accuracy: 95.77083333333334
[ Wed Jul 17 02:32:13 2024 ] 	Batch(3000/6809) done. Loss: 0.0703  lr:0.000001
[ Wed Jul 17 02:32:36 2024 ] 	Batch(3100/6809) done. Loss: 0.2457  lr:0.000001
[ Wed Jul 17 02:32:59 2024 ] 	Batch(3200/6809) done. Loss: 0.1244  lr:0.000001
[ Wed Jul 17 02:33:22 2024 ] 	Batch(3300/6809) done. Loss: 0.1071  lr:0.000001
[ Wed Jul 17 02:33:45 2024 ] 	Batch(3400/6809) done. Loss: 0.0604  lr:0.000001
[ Wed Jul 17 02:34:08 2024 ] 
Training: Epoch [137/150], Step [3499], Loss: 0.01809956133365631, Training Accuracy: 95.72142857142858
[ Wed Jul 17 02:34:09 2024 ] 	Batch(3500/6809) done. Loss: 0.0316  lr:0.000001
[ Wed Jul 17 02:34:32 2024 ] 	Batch(3600/6809) done. Loss: 0.2374  lr:0.000001
[ Wed Jul 17 02:34:55 2024 ] 	Batch(3700/6809) done. Loss: 0.1031  lr:0.000001
[ Wed Jul 17 02:35:18 2024 ] 	Batch(3800/6809) done. Loss: 0.3409  lr:0.000001
[ Wed Jul 17 02:35:41 2024 ] 	Batch(3900/6809) done. Loss: 0.0466  lr:0.000001
[ Wed Jul 17 02:36:04 2024 ] 
Training: Epoch [137/150], Step [3999], Loss: 0.11516795307397842, Training Accuracy: 95.70625
[ Wed Jul 17 02:36:04 2024 ] 	Batch(4000/6809) done. Loss: 0.0578  lr:0.000001
[ Wed Jul 17 02:36:27 2024 ] 	Batch(4100/6809) done. Loss: 0.4515  lr:0.000001
[ Wed Jul 17 02:36:50 2024 ] 	Batch(4200/6809) done. Loss: 0.1631  lr:0.000001
[ Wed Jul 17 02:37:13 2024 ] 	Batch(4300/6809) done. Loss: 0.0104  lr:0.000001
[ Wed Jul 17 02:37:35 2024 ] 	Batch(4400/6809) done. Loss: 0.2689  lr:0.000001
[ Wed Jul 17 02:37:58 2024 ] 
Training: Epoch [137/150], Step [4499], Loss: 0.010179510340094566, Training Accuracy: 95.63888888888889
[ Wed Jul 17 02:37:58 2024 ] 	Batch(4500/6809) done. Loss: 0.1129  lr:0.000001
[ Wed Jul 17 02:38:21 2024 ] 	Batch(4600/6809) done. Loss: 0.1057  lr:0.000001
[ Wed Jul 17 02:38:45 2024 ] 	Batch(4700/6809) done. Loss: 0.4414  lr:0.000001
[ Wed Jul 17 02:39:09 2024 ] 	Batch(4800/6809) done. Loss: 0.1395  lr:0.000001
[ Wed Jul 17 02:39:32 2024 ] 	Batch(4900/6809) done. Loss: 0.5589  lr:0.000001
[ Wed Jul 17 02:39:55 2024 ] 
Training: Epoch [137/150], Step [4999], Loss: 0.028382442891597748, Training Accuracy: 95.61500000000001
[ Wed Jul 17 02:39:55 2024 ] 	Batch(5000/6809) done. Loss: 0.2154  lr:0.000001
[ Wed Jul 17 02:40:18 2024 ] 	Batch(5100/6809) done. Loss: 0.0307  lr:0.000001
[ Wed Jul 17 02:40:41 2024 ] 	Batch(5200/6809) done. Loss: 0.2509  lr:0.000001
[ Wed Jul 17 02:41:03 2024 ] 	Batch(5300/6809) done. Loss: 0.1266  lr:0.000001
[ Wed Jul 17 02:41:26 2024 ] 	Batch(5400/6809) done. Loss: 0.3541  lr:0.000001
[ Wed Jul 17 02:41:49 2024 ] 
Training: Epoch [137/150], Step [5499], Loss: 0.3340509831905365, Training Accuracy: 95.62045454545455
[ Wed Jul 17 02:41:49 2024 ] 	Batch(5500/6809) done. Loss: 0.2166  lr:0.000001
[ Wed Jul 17 02:42:12 2024 ] 	Batch(5600/6809) done. Loss: 0.4922  lr:0.000001
[ Wed Jul 17 02:42:34 2024 ] 	Batch(5700/6809) done. Loss: 0.1273  lr:0.000001
[ Wed Jul 17 02:42:57 2024 ] 	Batch(5800/6809) done. Loss: 0.0100  lr:0.000001
[ Wed Jul 17 02:43:20 2024 ] 	Batch(5900/6809) done. Loss: 0.0872  lr:0.000001
[ Wed Jul 17 02:43:42 2024 ] 
Training: Epoch [137/150], Step [5999], Loss: 0.17129501700401306, Training Accuracy: 95.63541666666666
[ Wed Jul 17 02:43:42 2024 ] 	Batch(6000/6809) done. Loss: 0.0981  lr:0.000001
[ Wed Jul 17 02:44:05 2024 ] 	Batch(6100/6809) done. Loss: 0.4545  lr:0.000001
[ Wed Jul 17 02:44:28 2024 ] 	Batch(6200/6809) done. Loss: 0.4699  lr:0.000001
[ Wed Jul 17 02:44:50 2024 ] 	Batch(6300/6809) done. Loss: 0.2662  lr:0.000001
[ Wed Jul 17 02:45:13 2024 ] 	Batch(6400/6809) done. Loss: 0.0824  lr:0.000001
[ Wed Jul 17 02:45:36 2024 ] 
Training: Epoch [137/150], Step [6499], Loss: 0.2154036909341812, Training Accuracy: 95.60961538461538
[ Wed Jul 17 02:45:36 2024 ] 	Batch(6500/6809) done. Loss: 0.1597  lr:0.000001
[ Wed Jul 17 02:45:59 2024 ] 	Batch(6600/6809) done. Loss: 0.1395  lr:0.000001
[ Wed Jul 17 02:46:21 2024 ] 	Batch(6700/6809) done. Loss: 0.1936  lr:0.000001
[ Wed Jul 17 02:46:44 2024 ] 	Batch(6800/6809) done. Loss: 0.2251  lr:0.000001
[ Wed Jul 17 02:46:46 2024 ] 	Mean training loss: 0.1535.
[ Wed Jul 17 02:46:46 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 02:46:46 2024 ] Training epoch: 139
[ Wed Jul 17 02:46:47 2024 ] 	Batch(0/6809) done. Loss: 0.0684  lr:0.000001
[ Wed Jul 17 02:47:10 2024 ] 	Batch(100/6809) done. Loss: 0.0093  lr:0.000001
[ Wed Jul 17 02:47:32 2024 ] 	Batch(200/6809) done. Loss: 0.2702  lr:0.000001
[ Wed Jul 17 02:47:55 2024 ] 	Batch(300/6809) done. Loss: 0.1070  lr:0.000001
[ Wed Jul 17 02:48:18 2024 ] 	Batch(400/6809) done. Loss: 0.0069  lr:0.000001
[ Wed Jul 17 02:48:40 2024 ] 
Training: Epoch [138/150], Step [499], Loss: 0.004278175067156553, Training Accuracy: 95.95
[ Wed Jul 17 02:48:41 2024 ] 	Batch(500/6809) done. Loss: 0.3032  lr:0.000001
[ Wed Jul 17 02:49:03 2024 ] 	Batch(600/6809) done. Loss: 0.1423  lr:0.000001
[ Wed Jul 17 02:49:26 2024 ] 	Batch(700/6809) done. Loss: 0.4872  lr:0.000001
[ Wed Jul 17 02:49:49 2024 ] 	Batch(800/6809) done. Loss: 0.1993  lr:0.000001
[ Wed Jul 17 02:50:13 2024 ] 	Batch(900/6809) done. Loss: 0.0636  lr:0.000001
[ Wed Jul 17 02:50:36 2024 ] 
Training: Epoch [138/150], Step [999], Loss: 0.06123199313879013, Training Accuracy: 95.875
[ Wed Jul 17 02:50:36 2024 ] 	Batch(1000/6809) done. Loss: 0.1193  lr:0.000001
[ Wed Jul 17 02:50:59 2024 ] 	Batch(1100/6809) done. Loss: 0.0517  lr:0.000001
[ Wed Jul 17 02:51:22 2024 ] 	Batch(1200/6809) done. Loss: 0.0138  lr:0.000001
[ Wed Jul 17 02:51:45 2024 ] 	Batch(1300/6809) done. Loss: 0.0070  lr:0.000001
[ Wed Jul 17 02:52:07 2024 ] 	Batch(1400/6809) done. Loss: 0.2801  lr:0.000001
[ Wed Jul 17 02:52:30 2024 ] 
Training: Epoch [138/150], Step [1499], Loss: 0.024838117882609367, Training Accuracy: 95.6
[ Wed Jul 17 02:52:30 2024 ] 	Batch(1500/6809) done. Loss: 0.0560  lr:0.000001
[ Wed Jul 17 02:52:53 2024 ] 	Batch(1600/6809) done. Loss: 0.2559  lr:0.000001
[ Wed Jul 17 02:53:15 2024 ] 	Batch(1700/6809) done. Loss: 0.0110  lr:0.000001
[ Wed Jul 17 02:53:38 2024 ] 	Batch(1800/6809) done. Loss: 0.0356  lr:0.000001
[ Wed Jul 17 02:54:01 2024 ] 	Batch(1900/6809) done. Loss: 0.2201  lr:0.000001
[ Wed Jul 17 02:54:23 2024 ] 
Training: Epoch [138/150], Step [1999], Loss: 0.010539066977798939, Training Accuracy: 95.63125
[ Wed Jul 17 02:54:23 2024 ] 	Batch(2000/6809) done. Loss: 0.5847  lr:0.000001
[ Wed Jul 17 02:54:47 2024 ] 	Batch(2100/6809) done. Loss: 0.0489  lr:0.000001
[ Wed Jul 17 02:55:10 2024 ] 	Batch(2200/6809) done. Loss: 0.1088  lr:0.000001
[ Wed Jul 17 02:55:34 2024 ] 	Batch(2300/6809) done. Loss: 0.0074  lr:0.000001
[ Wed Jul 17 02:55:57 2024 ] 	Batch(2400/6809) done. Loss: 0.1472  lr:0.000001
[ Wed Jul 17 02:56:19 2024 ] 
Training: Epoch [138/150], Step [2499], Loss: 0.024557573720812798, Training Accuracy: 95.73
[ Wed Jul 17 02:56:20 2024 ] 	Batch(2500/6809) done. Loss: 0.0220  lr:0.000001
[ Wed Jul 17 02:56:42 2024 ] 	Batch(2600/6809) done. Loss: 0.1914  lr:0.000001
[ Wed Jul 17 02:57:05 2024 ] 	Batch(2700/6809) done. Loss: 0.0404  lr:0.000001
[ Wed Jul 17 02:57:28 2024 ] 	Batch(2800/6809) done. Loss: 0.1672  lr:0.000001
[ Wed Jul 17 02:57:51 2024 ] 	Batch(2900/6809) done. Loss: 0.2042  lr:0.000001
[ Wed Jul 17 02:58:13 2024 ] 
Training: Epoch [138/150], Step [2999], Loss: 0.06562396883964539, Training Accuracy: 95.66666666666667
[ Wed Jul 17 02:58:13 2024 ] 	Batch(3000/6809) done. Loss: 0.6660  lr:0.000001
[ Wed Jul 17 02:58:36 2024 ] 	Batch(3100/6809) done. Loss: 0.0053  lr:0.000001
[ Wed Jul 17 02:58:59 2024 ] 	Batch(3200/6809) done. Loss: 0.0472  lr:0.000001
[ Wed Jul 17 02:59:22 2024 ] 	Batch(3300/6809) done. Loss: 0.1637  lr:0.000001
[ Wed Jul 17 02:59:46 2024 ] 	Batch(3400/6809) done. Loss: 0.0971  lr:0.000001
[ Wed Jul 17 03:00:09 2024 ] 
Training: Epoch [138/150], Step [3499], Loss: 0.04273682087659836, Training Accuracy: 95.69285714285715
[ Wed Jul 17 03:00:09 2024 ] 	Batch(3500/6809) done. Loss: 0.0463  lr:0.000001
[ Wed Jul 17 03:00:33 2024 ] 	Batch(3600/6809) done. Loss: 0.0689  lr:0.000001
[ Wed Jul 17 03:00:55 2024 ] 	Batch(3700/6809) done. Loss: 0.1619  lr:0.000001
[ Wed Jul 17 03:01:18 2024 ] 	Batch(3800/6809) done. Loss: 0.0367  lr:0.000001
[ Wed Jul 17 03:01:40 2024 ] 	Batch(3900/6809) done. Loss: 0.3458  lr:0.000001
[ Wed Jul 17 03:02:03 2024 ] 
Training: Epoch [138/150], Step [3999], Loss: 0.3670504689216614, Training Accuracy: 95.684375
[ Wed Jul 17 03:02:03 2024 ] 	Batch(4000/6809) done. Loss: 0.1690  lr:0.000001
[ Wed Jul 17 03:02:27 2024 ] 	Batch(4100/6809) done. Loss: 0.0928  lr:0.000001
[ Wed Jul 17 03:02:50 2024 ] 	Batch(4200/6809) done. Loss: 0.0330  lr:0.000001
[ Wed Jul 17 03:03:14 2024 ] 	Batch(4300/6809) done. Loss: 0.5236  lr:0.000001
[ Wed Jul 17 03:03:37 2024 ] 	Batch(4400/6809) done. Loss: 0.0399  lr:0.000001
[ Wed Jul 17 03:04:00 2024 ] 
Training: Epoch [138/150], Step [4499], Loss: 0.051692523062229156, Training Accuracy: 95.67777777777778
[ Wed Jul 17 03:04:00 2024 ] 	Batch(4500/6809) done. Loss: 0.1805  lr:0.000001
[ Wed Jul 17 03:04:24 2024 ] 	Batch(4600/6809) done. Loss: 0.1262  lr:0.000001
[ Wed Jul 17 03:04:46 2024 ] 	Batch(4700/6809) done. Loss: 0.3879  lr:0.000001
[ Wed Jul 17 03:05:09 2024 ] 	Batch(4800/6809) done. Loss: 0.0389  lr:0.000001
[ Wed Jul 17 03:05:32 2024 ] 	Batch(4900/6809) done. Loss: 0.3706  lr:0.000001
[ Wed Jul 17 03:05:54 2024 ] 
Training: Epoch [138/150], Step [4999], Loss: 0.16178672015666962, Training Accuracy: 95.705
[ Wed Jul 17 03:05:54 2024 ] 	Batch(5000/6809) done. Loss: 0.0481  lr:0.000001
[ Wed Jul 17 03:06:17 2024 ] 	Batch(5100/6809) done. Loss: 0.0997  lr:0.000001
[ Wed Jul 17 03:06:39 2024 ] 	Batch(5200/6809) done. Loss: 0.0142  lr:0.000001
[ Wed Jul 17 03:07:02 2024 ] 	Batch(5300/6809) done. Loss: 0.5077  lr:0.000001
[ Wed Jul 17 03:07:25 2024 ] 	Batch(5400/6809) done. Loss: 0.0160  lr:0.000001
[ Wed Jul 17 03:07:47 2024 ] 
Training: Epoch [138/150], Step [5499], Loss: 0.08191853761672974, Training Accuracy: 95.65227272727273
[ Wed Jul 17 03:07:47 2024 ] 	Batch(5500/6809) done. Loss: 0.2334  lr:0.000001
[ Wed Jul 17 03:08:10 2024 ] 	Batch(5600/6809) done. Loss: 0.0590  lr:0.000001
[ Wed Jul 17 03:08:32 2024 ] 	Batch(5700/6809) done. Loss: 0.1013  lr:0.000001
[ Wed Jul 17 03:08:55 2024 ] 	Batch(5800/6809) done. Loss: 0.0826  lr:0.000001
[ Wed Jul 17 03:09:18 2024 ] 	Batch(5900/6809) done. Loss: 0.0437  lr:0.000001
[ Wed Jul 17 03:09:40 2024 ] 
Training: Epoch [138/150], Step [5999], Loss: 0.2155674546957016, Training Accuracy: 95.65
[ Wed Jul 17 03:09:40 2024 ] 	Batch(6000/6809) done. Loss: 0.6686  lr:0.000001
[ Wed Jul 17 03:10:03 2024 ] 	Batch(6100/6809) done. Loss: 0.0342  lr:0.000001
[ Wed Jul 17 03:10:26 2024 ] 	Batch(6200/6809) done. Loss: 0.1176  lr:0.000001
[ Wed Jul 17 03:10:48 2024 ] 	Batch(6300/6809) done. Loss: 0.1899  lr:0.000001
[ Wed Jul 17 03:11:11 2024 ] 	Batch(6400/6809) done. Loss: 0.1907  lr:0.000001
[ Wed Jul 17 03:11:33 2024 ] 
Training: Epoch [138/150], Step [6499], Loss: 0.1407649666070938, Training Accuracy: 95.65384615384616
[ Wed Jul 17 03:11:33 2024 ] 	Batch(6500/6809) done. Loss: 0.0851  lr:0.000001
[ Wed Jul 17 03:11:56 2024 ] 	Batch(6600/6809) done. Loss: 0.1409  lr:0.000001
[ Wed Jul 17 03:12:18 2024 ] 	Batch(6700/6809) done. Loss: 0.1610  lr:0.000001
[ Wed Jul 17 03:12:41 2024 ] 	Batch(6800/6809) done. Loss: 0.0664  lr:0.000001
[ Wed Jul 17 03:12:43 2024 ] 	Mean training loss: 0.1523.
[ Wed Jul 17 03:12:43 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 03:12:43 2024 ] Training epoch: 140
[ Wed Jul 17 03:12:44 2024 ] 	Batch(0/6809) done. Loss: 0.0344  lr:0.000001
[ Wed Jul 17 03:13:06 2024 ] 	Batch(100/6809) done. Loss: 0.0069  lr:0.000001
[ Wed Jul 17 03:13:29 2024 ] 	Batch(200/6809) done. Loss: 0.0038  lr:0.000001
[ Wed Jul 17 03:13:52 2024 ] 	Batch(300/6809) done. Loss: 0.0910  lr:0.000001
[ Wed Jul 17 03:14:14 2024 ] 	Batch(400/6809) done. Loss: 0.0128  lr:0.000001
[ Wed Jul 17 03:14:37 2024 ] 
Training: Epoch [139/150], Step [499], Loss: 0.20368298888206482, Training Accuracy: 95.95
[ Wed Jul 17 03:14:37 2024 ] 	Batch(500/6809) done. Loss: 0.0186  lr:0.000001
[ Wed Jul 17 03:15:00 2024 ] 	Batch(600/6809) done. Loss: 0.2704  lr:0.000001
[ Wed Jul 17 03:15:22 2024 ] 	Batch(700/6809) done. Loss: 0.5698  lr:0.000001
[ Wed Jul 17 03:15:45 2024 ] 	Batch(800/6809) done. Loss: 0.0206  lr:0.000001
[ Wed Jul 17 03:16:08 2024 ] 	Batch(900/6809) done. Loss: 0.1679  lr:0.000001
[ Wed Jul 17 03:16:30 2024 ] 
Training: Epoch [139/150], Step [999], Loss: 0.028298653662204742, Training Accuracy: 95.76249999999999
[ Wed Jul 17 03:16:30 2024 ] 	Batch(1000/6809) done. Loss: 0.0574  lr:0.000001
[ Wed Jul 17 03:16:53 2024 ] 	Batch(1100/6809) done. Loss: 0.2264  lr:0.000001
[ Wed Jul 17 03:17:15 2024 ] 	Batch(1200/6809) done. Loss: 0.1045  lr:0.000001
[ Wed Jul 17 03:17:39 2024 ] 	Batch(1300/6809) done. Loss: 0.4320  lr:0.000001
[ Wed Jul 17 03:18:02 2024 ] 	Batch(1400/6809) done. Loss: 0.0350  lr:0.000001
[ Wed Jul 17 03:18:25 2024 ] 
Training: Epoch [139/150], Step [1499], Loss: 0.1182454526424408, Training Accuracy: 95.74166666666667
[ Wed Jul 17 03:18:26 2024 ] 	Batch(1500/6809) done. Loss: 0.0120  lr:0.000001
[ Wed Jul 17 03:18:49 2024 ] 	Batch(1600/6809) done. Loss: 0.1376  lr:0.000001
[ Wed Jul 17 03:19:11 2024 ] 	Batch(1700/6809) done. Loss: 0.0905  lr:0.000001
[ Wed Jul 17 03:19:34 2024 ] 	Batch(1800/6809) done. Loss: 0.1242  lr:0.000001
[ Wed Jul 17 03:19:57 2024 ] 	Batch(1900/6809) done. Loss: 0.2099  lr:0.000001
[ Wed Jul 17 03:20:19 2024 ] 
Training: Epoch [139/150], Step [1999], Loss: 0.1951676458120346, Training Accuracy: 95.7375
[ Wed Jul 17 03:20:19 2024 ] 	Batch(2000/6809) done. Loss: 0.0762  lr:0.000001
[ Wed Jul 17 03:20:42 2024 ] 	Batch(2100/6809) done. Loss: 0.1744  lr:0.000001
[ Wed Jul 17 03:21:04 2024 ] 	Batch(2200/6809) done. Loss: 0.0431  lr:0.000001
[ Wed Jul 17 03:21:27 2024 ] 	Batch(2300/6809) done. Loss: 0.0698  lr:0.000001
[ Wed Jul 17 03:21:50 2024 ] 	Batch(2400/6809) done. Loss: 0.3899  lr:0.000001
[ Wed Jul 17 03:22:13 2024 ] 
Training: Epoch [139/150], Step [2499], Loss: 0.031091731041669846, Training Accuracy: 95.78
[ Wed Jul 17 03:22:13 2024 ] 	Batch(2500/6809) done. Loss: 0.2998  lr:0.000001
[ Wed Jul 17 03:22:37 2024 ] 	Batch(2600/6809) done. Loss: 0.1290  lr:0.000001
[ Wed Jul 17 03:23:00 2024 ] 	Batch(2700/6809) done. Loss: 0.0225  lr:0.000001
[ Wed Jul 17 03:23:23 2024 ] 	Batch(2800/6809) done. Loss: 0.0292  lr:0.000001
[ Wed Jul 17 03:23:45 2024 ] 	Batch(2900/6809) done. Loss: 0.0057  lr:0.000001
[ Wed Jul 17 03:24:08 2024 ] 
Training: Epoch [139/150], Step [2999], Loss: 0.20175188779830933, Training Accuracy: 95.8
[ Wed Jul 17 03:24:08 2024 ] 	Batch(3000/6809) done. Loss: 0.0028  lr:0.000001
[ Wed Jul 17 03:24:30 2024 ] 	Batch(3100/6809) done. Loss: 0.0467  lr:0.000001
[ Wed Jul 17 03:24:53 2024 ] 	Batch(3200/6809) done. Loss: 0.0719  lr:0.000001
[ Wed Jul 17 03:25:16 2024 ] 	Batch(3300/6809) done. Loss: 0.0157  lr:0.000001
[ Wed Jul 17 03:25:38 2024 ] 	Batch(3400/6809) done. Loss: 0.1842  lr:0.000001
[ Wed Jul 17 03:26:01 2024 ] 
Training: Epoch [139/150], Step [3499], Loss: 0.12081857025623322, Training Accuracy: 95.77857142857142
[ Wed Jul 17 03:26:01 2024 ] 	Batch(3500/6809) done. Loss: 0.0465  lr:0.000001
[ Wed Jul 17 03:26:24 2024 ] 	Batch(3600/6809) done. Loss: 0.2051  lr:0.000001
[ Wed Jul 17 03:26:47 2024 ] 	Batch(3700/6809) done. Loss: 0.1018  lr:0.000001
[ Wed Jul 17 03:27:09 2024 ] 	Batch(3800/6809) done. Loss: 0.5888  lr:0.000001
[ Wed Jul 17 03:27:32 2024 ] 	Batch(3900/6809) done. Loss: 0.0663  lr:0.000001
[ Wed Jul 17 03:27:54 2024 ] 
Training: Epoch [139/150], Step [3999], Loss: 0.06107185781002045, Training Accuracy: 95.8125
[ Wed Jul 17 03:27:55 2024 ] 	Batch(4000/6809) done. Loss: 0.0621  lr:0.000001
[ Wed Jul 17 03:28:17 2024 ] 	Batch(4100/6809) done. Loss: 0.0552  lr:0.000001
[ Wed Jul 17 03:28:40 2024 ] 	Batch(4200/6809) done. Loss: 0.1763  lr:0.000001
[ Wed Jul 17 03:29:02 2024 ] 	Batch(4300/6809) done. Loss: 0.0270  lr:0.000001
[ Wed Jul 17 03:29:25 2024 ] 	Batch(4400/6809) done. Loss: 0.0135  lr:0.000001
[ Wed Jul 17 03:29:48 2024 ] 
Training: Epoch [139/150], Step [4499], Loss: 0.08336877822875977, Training Accuracy: 95.75833333333334
[ Wed Jul 17 03:29:48 2024 ] 	Batch(4500/6809) done. Loss: 0.0825  lr:0.000001
[ Wed Jul 17 03:30:10 2024 ] 	Batch(4600/6809) done. Loss: 0.1571  lr:0.000001
[ Wed Jul 17 03:30:33 2024 ] 	Batch(4700/6809) done. Loss: 0.2212  lr:0.000001
[ Wed Jul 17 03:30:56 2024 ] 	Batch(4800/6809) done. Loss: 0.5259  lr:0.000001
[ Wed Jul 17 03:31:18 2024 ] 	Batch(4900/6809) done. Loss: 0.0643  lr:0.000001
[ Wed Jul 17 03:31:41 2024 ] 
Training: Epoch [139/150], Step [4999], Loss: 0.0798506960272789, Training Accuracy: 95.7
[ Wed Jul 17 03:31:41 2024 ] 	Batch(5000/6809) done. Loss: 0.3698  lr:0.000001
[ Wed Jul 17 03:32:04 2024 ] 	Batch(5100/6809) done. Loss: 0.0028  lr:0.000001
[ Wed Jul 17 03:32:28 2024 ] 	Batch(5200/6809) done. Loss: 0.0861  lr:0.000001
[ Wed Jul 17 03:32:51 2024 ] 	Batch(5300/6809) done. Loss: 0.1950  lr:0.000001
[ Wed Jul 17 03:33:13 2024 ] 	Batch(5400/6809) done. Loss: 0.1229  lr:0.000001
[ Wed Jul 17 03:33:36 2024 ] 
Training: Epoch [139/150], Step [5499], Loss: 0.08161220699548721, Training Accuracy: 95.73181818181818
[ Wed Jul 17 03:33:36 2024 ] 	Batch(5500/6809) done. Loss: 0.0716  lr:0.000001
[ Wed Jul 17 03:33:59 2024 ] 	Batch(5600/6809) done. Loss: 0.3365  lr:0.000001
[ Wed Jul 17 03:34:22 2024 ] 	Batch(5700/6809) done. Loss: 0.2774  lr:0.000001
[ Wed Jul 17 03:34:45 2024 ] 	Batch(5800/6809) done. Loss: 0.0158  lr:0.000001
[ Wed Jul 17 03:35:08 2024 ] 	Batch(5900/6809) done. Loss: 0.0937  lr:0.000001
[ Wed Jul 17 03:35:30 2024 ] 
Training: Epoch [139/150], Step [5999], Loss: 0.460159569978714, Training Accuracy: 95.67291666666667
[ Wed Jul 17 03:35:31 2024 ] 	Batch(6000/6809) done. Loss: 0.1202  lr:0.000001
[ Wed Jul 17 03:35:53 2024 ] 	Batch(6100/6809) done. Loss: 0.0665  lr:0.000001
[ Wed Jul 17 03:36:16 2024 ] 	Batch(6200/6809) done. Loss: 0.1436  lr:0.000001
[ Wed Jul 17 03:36:39 2024 ] 	Batch(6300/6809) done. Loss: 0.0444  lr:0.000001
[ Wed Jul 17 03:37:01 2024 ] 	Batch(6400/6809) done. Loss: 0.1659  lr:0.000001
[ Wed Jul 17 03:37:24 2024 ] 
Training: Epoch [139/150], Step [6499], Loss: 0.0915151834487915, Training Accuracy: 95.68076923076923
[ Wed Jul 17 03:37:24 2024 ] 	Batch(6500/6809) done. Loss: 0.1741  lr:0.000001
[ Wed Jul 17 03:37:46 2024 ] 	Batch(6600/6809) done. Loss: 0.0117  lr:0.000001
[ Wed Jul 17 03:38:09 2024 ] 	Batch(6700/6809) done. Loss: 0.4027  lr:0.000001
[ Wed Jul 17 03:38:32 2024 ] 	Batch(6800/6809) done. Loss: 0.1190  lr:0.000001
[ Wed Jul 17 03:38:34 2024 ] 	Mean training loss: 0.1516.
[ Wed Jul 17 03:38:34 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 03:38:34 2024 ] Eval epoch: 140
[ Wed Jul 17 03:45:29 2024 ] 	Mean val loss of 7435 batches: 0.8799607266425887.
[ Wed Jul 17 03:45:29 2024 ] 
Validation: Epoch [139/150], Samples [47538.0/59477], Loss: 2.894594669342041, Validation Accuracy: 79.92669435243876
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 1 : 385 / 500 = 77 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 2 : 412 / 499 = 82 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 3 : 426 / 500 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 4 : 426 / 502 = 84 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 5 : 429 / 502 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 6 : 419 / 502 = 83 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 7 : 464 / 497 = 93 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 8 : 487 / 498 = 97 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 9 : 381 / 500 = 76 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 10 : 301 / 500 = 60 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 11 : 218 / 498 = 43 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 12 : 391 / 499 = 78 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 13 : 486 / 502 = 96 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 14 : 482 / 504 = 95 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 15 : 379 / 502 = 75 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 16 : 361 / 502 = 71 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 17 : 428 / 504 = 84 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 18 : 409 / 504 = 81 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 19 : 432 / 502 = 86 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 20 : 457 / 502 = 91 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 21 : 475 / 503 = 94 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 22 : 429 / 504 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 23 : 431 / 503 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 24 : 437 / 504 = 86 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 25 : 480 / 504 = 95 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 26 : 462 / 504 = 91 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 27 : 429 / 501 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 28 : 364 / 502 = 72 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 29 : 310 / 502 = 61 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 30 : 377 / 501 = 75 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 31 : 426 / 504 = 84 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 32 : 432 / 503 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 33 : 391 / 503 = 77 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 34 : 486 / 504 = 96 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 35 : 461 / 503 = 91 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 36 : 391 / 502 = 77 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 37 : 442 / 504 = 87 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 38 : 450 / 504 = 89 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 39 : 449 / 498 = 90 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 40 : 400 / 504 = 79 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 41 : 481 / 503 = 95 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 42 : 463 / 504 = 91 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 43 : 352 / 503 = 69 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 44 : 433 / 504 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 45 : 419 / 504 = 83 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 46 : 396 / 504 = 78 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 47 : 360 / 503 = 71 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 48 : 422 / 503 = 83 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 49 : 399 / 499 = 79 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 50 : 417 / 502 = 83 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 51 : 459 / 503 = 91 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 52 : 451 / 504 = 89 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 53 : 443 / 497 = 89 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 54 : 447 / 480 = 93 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 55 : 439 / 504 = 87 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 56 : 428 / 503 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 57 : 471 / 504 = 93 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 58 : 477 / 499 = 95 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 59 : 485 / 503 = 96 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 60 : 413 / 479 = 86 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 61 : 413 / 484 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 62 : 402 / 487 = 82 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 63 : 451 / 489 = 92 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 64 : 396 / 488 = 81 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 65 : 431 / 490 = 87 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 66 : 313 / 488 = 64 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 67 : 353 / 490 = 72 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 68 : 321 / 490 = 65 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 69 : 376 / 490 = 76 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 70 : 176 / 490 = 35 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 71 : 274 / 490 = 55 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 72 : 198 / 488 = 40 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 73 : 256 / 486 = 52 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 74 : 272 / 481 = 56 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 75 : 283 / 488 = 57 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 76 : 310 / 489 = 63 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 77 : 316 / 488 = 64 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 78 : 378 / 488 = 77 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 79 : 455 / 490 = 92 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 80 : 398 / 489 = 81 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 81 : 271 / 491 = 55 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 82 : 321 / 491 = 65 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 83 : 260 / 489 = 53 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 84 : 362 / 489 = 74 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 85 : 357 / 489 = 73 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 86 : 416 / 491 = 84 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 87 : 419 / 492 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 88 : 368 / 491 = 74 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 89 : 364 / 492 = 73 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 90 : 279 / 490 = 56 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 91 : 381 / 482 = 79 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 92 : 369 / 490 = 75 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 93 : 361 / 487 = 74 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 94 : 432 / 489 = 88 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 95 : 408 / 490 = 83 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 96 : 459 / 491 = 93 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 97 : 457 / 490 = 93 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 98 : 439 / 491 = 89 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 99 : 449 / 491 = 91 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 100 : 444 / 491 = 90 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 101 : 419 / 491 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 102 : 290 / 492 = 58 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 103 : 390 / 492 = 79 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 104 : 298 / 491 = 60 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 105 : 250 / 491 = 50 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 106 : 324 / 492 = 65 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 107 : 414 / 491 = 84 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 108 : 364 / 492 = 73 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 109 : 366 / 490 = 74 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 110 : 409 / 491 = 83 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 111 : 448 / 492 = 91 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 112 : 462 / 492 = 93 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 113 : 443 / 491 = 90 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 114 : 396 / 491 = 80 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 115 : 420 / 492 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 116 : 421 / 491 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 117 : 405 / 492 = 82 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 118 : 417 / 490 = 85 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 119 : 450 / 492 = 91 %
[ Wed Jul 17 03:45:29 2024 ] Accuracy of 120 : 414 / 500 = 82 %
[ Wed Jul 17 03:45:29 2024 ] Training epoch: 141
[ Wed Jul 17 03:45:30 2024 ] 	Batch(0/6809) done. Loss: 0.5792  lr:0.000001
[ Wed Jul 17 03:45:54 2024 ] 	Batch(100/6809) done. Loss: 0.0081  lr:0.000001
[ Wed Jul 17 03:46:17 2024 ] 	Batch(200/6809) done. Loss: 0.1641  lr:0.000001
[ Wed Jul 17 03:46:40 2024 ] 	Batch(300/6809) done. Loss: 0.0230  lr:0.000001
[ Wed Jul 17 03:47:03 2024 ] 	Batch(400/6809) done. Loss: 0.1267  lr:0.000001
[ Wed Jul 17 03:47:26 2024 ] 
Training: Epoch [140/150], Step [499], Loss: 0.17470017075538635, Training Accuracy: 95.525
[ Wed Jul 17 03:47:26 2024 ] 	Batch(500/6809) done. Loss: 0.1576  lr:0.000001
[ Wed Jul 17 03:47:49 2024 ] 	Batch(600/6809) done. Loss: 0.0926  lr:0.000001
[ Wed Jul 17 03:48:12 2024 ] 	Batch(700/6809) done. Loss: 0.0843  lr:0.000001
[ Wed Jul 17 03:48:36 2024 ] 	Batch(800/6809) done. Loss: 0.0917  lr:0.000001
[ Wed Jul 17 03:48:59 2024 ] 	Batch(900/6809) done. Loss: 0.0696  lr:0.000001
[ Wed Jul 17 03:49:22 2024 ] 
Training: Epoch [140/150], Step [999], Loss: 0.1085071936249733, Training Accuracy: 95.5125
[ Wed Jul 17 03:49:22 2024 ] 	Batch(1000/6809) done. Loss: 0.0927  lr:0.000001
[ Wed Jul 17 03:49:45 2024 ] 	Batch(1100/6809) done. Loss: 0.0037  lr:0.000001
[ Wed Jul 17 03:50:09 2024 ] 	Batch(1200/6809) done. Loss: 0.0524  lr:0.000001
[ Wed Jul 17 03:50:31 2024 ] 	Batch(1300/6809) done. Loss: 0.0764  lr:0.000001
[ Wed Jul 17 03:50:54 2024 ] 	Batch(1400/6809) done. Loss: 0.3482  lr:0.000001
[ Wed Jul 17 03:51:17 2024 ] 
Training: Epoch [140/150], Step [1499], Loss: 0.01763957552611828, Training Accuracy: 95.48333333333333
[ Wed Jul 17 03:51:17 2024 ] 	Batch(1500/6809) done. Loss: 0.0521  lr:0.000001
[ Wed Jul 17 03:51:40 2024 ] 	Batch(1600/6809) done. Loss: 0.0546  lr:0.000001
[ Wed Jul 17 03:52:03 2024 ] 	Batch(1700/6809) done. Loss: 0.6809  lr:0.000001
[ Wed Jul 17 03:52:25 2024 ] 	Batch(1800/6809) done. Loss: 0.0750  lr:0.000001
[ Wed Jul 17 03:52:48 2024 ] 	Batch(1900/6809) done. Loss: 0.0081  lr:0.000001
[ Wed Jul 17 03:53:11 2024 ] 
Training: Epoch [140/150], Step [1999], Loss: 0.03461635857820511, Training Accuracy: 95.64375000000001
[ Wed Jul 17 03:53:11 2024 ] 	Batch(2000/6809) done. Loss: 0.3942  lr:0.000001
[ Wed Jul 17 03:53:34 2024 ] 	Batch(2100/6809) done. Loss: 0.0286  lr:0.000001
[ Wed Jul 17 03:53:56 2024 ] 	Batch(2200/6809) done. Loss: 0.5289  lr:0.000001
[ Wed Jul 17 03:54:19 2024 ] 	Batch(2300/6809) done. Loss: 0.2184  lr:0.000001
[ Wed Jul 17 03:54:42 2024 ] 	Batch(2400/6809) done. Loss: 0.1767  lr:0.000001
[ Wed Jul 17 03:55:05 2024 ] 
Training: Epoch [140/150], Step [2499], Loss: 0.02660597674548626, Training Accuracy: 95.755
[ Wed Jul 17 03:55:05 2024 ] 	Batch(2500/6809) done. Loss: 0.0801  lr:0.000001
[ Wed Jul 17 03:55:28 2024 ] 	Batch(2600/6809) done. Loss: 0.0169  lr:0.000001
[ Wed Jul 17 03:55:50 2024 ] 	Batch(2700/6809) done. Loss: 0.1910  lr:0.000001
[ Wed Jul 17 03:56:13 2024 ] 	Batch(2800/6809) done. Loss: 0.1837  lr:0.000001
[ Wed Jul 17 03:56:36 2024 ] 	Batch(2900/6809) done. Loss: 0.0904  lr:0.000001
[ Wed Jul 17 03:56:59 2024 ] 
Training: Epoch [140/150], Step [2999], Loss: 0.022763434797525406, Training Accuracy: 95.78750000000001
[ Wed Jul 17 03:56:59 2024 ] 	Batch(3000/6809) done. Loss: 0.0970  lr:0.000001
[ Wed Jul 17 03:57:22 2024 ] 	Batch(3100/6809) done. Loss: 0.0711  lr:0.000001
[ Wed Jul 17 03:57:45 2024 ] 	Batch(3200/6809) done. Loss: 0.1097  lr:0.000001
[ Wed Jul 17 03:58:07 2024 ] 	Batch(3300/6809) done. Loss: 0.0028  lr:0.000001
[ Wed Jul 17 03:58:30 2024 ] 	Batch(3400/6809) done. Loss: 0.2186  lr:0.000001
[ Wed Jul 17 03:58:53 2024 ] 
Training: Epoch [140/150], Step [3499], Loss: 0.3714410662651062, Training Accuracy: 95.8
[ Wed Jul 17 03:58:53 2024 ] 	Batch(3500/6809) done. Loss: 0.2800  lr:0.000001
[ Wed Jul 17 03:59:16 2024 ] 	Batch(3600/6809) done. Loss: 0.3504  lr:0.000001
[ Wed Jul 17 03:59:39 2024 ] 	Batch(3700/6809) done. Loss: 0.0876  lr:0.000001
[ Wed Jul 17 04:00:01 2024 ] 	Batch(3800/6809) done. Loss: 0.0739  lr:0.000001
[ Wed Jul 17 04:00:24 2024 ] 	Batch(3900/6809) done. Loss: 0.1469  lr:0.000001
[ Wed Jul 17 04:00:47 2024 ] 
Training: Epoch [140/150], Step [3999], Loss: 0.4934968054294586, Training Accuracy: 95.746875
[ Wed Jul 17 04:00:47 2024 ] 	Batch(4000/6809) done. Loss: 0.0033  lr:0.000001
[ Wed Jul 17 04:01:10 2024 ] 	Batch(4100/6809) done. Loss: 0.5857  lr:0.000001
[ Wed Jul 17 04:01:33 2024 ] 	Batch(4200/6809) done. Loss: 0.0341  lr:0.000001
[ Wed Jul 17 04:01:56 2024 ] 	Batch(4300/6809) done. Loss: 0.4277  lr:0.000001
[ Wed Jul 17 04:02:19 2024 ] 	Batch(4400/6809) done. Loss: 0.1999  lr:0.000001
[ Wed Jul 17 04:02:42 2024 ] 
Training: Epoch [140/150], Step [4499], Loss: 0.043257612735033035, Training Accuracy: 95.72222222222221
[ Wed Jul 17 04:02:42 2024 ] 	Batch(4500/6809) done. Loss: 0.1920  lr:0.000001
[ Wed Jul 17 04:03:05 2024 ] 	Batch(4600/6809) done. Loss: 0.0218  lr:0.000001
[ Wed Jul 17 04:03:28 2024 ] 	Batch(4700/6809) done. Loss: 0.0389  lr:0.000001
[ Wed Jul 17 04:03:51 2024 ] 	Batch(4800/6809) done. Loss: 0.0740  lr:0.000001
[ Wed Jul 17 04:04:14 2024 ] 	Batch(4900/6809) done. Loss: 0.5868  lr:0.000001
[ Wed Jul 17 04:04:36 2024 ] 
Training: Epoch [140/150], Step [4999], Loss: 0.03294667601585388, Training Accuracy: 95.7325
[ Wed Jul 17 04:04:37 2024 ] 	Batch(5000/6809) done. Loss: 0.3304  lr:0.000001
[ Wed Jul 17 04:04:59 2024 ] 	Batch(5100/6809) done. Loss: 0.1722  lr:0.000001
[ Wed Jul 17 04:05:22 2024 ] 	Batch(5200/6809) done. Loss: 0.1089  lr:0.000001
[ Wed Jul 17 04:05:45 2024 ] 	Batch(5300/6809) done. Loss: 0.1261  lr:0.000001
[ Wed Jul 17 04:06:08 2024 ] 	Batch(5400/6809) done. Loss: 0.2074  lr:0.000001
[ Wed Jul 17 04:06:31 2024 ] 
Training: Epoch [140/150], Step [5499], Loss: 0.07000729441642761, Training Accuracy: 95.76363636363637
[ Wed Jul 17 04:06:31 2024 ] 	Batch(5500/6809) done. Loss: 0.5907  lr:0.000001
[ Wed Jul 17 04:06:54 2024 ] 	Batch(5600/6809) done. Loss: 0.2708  lr:0.000001
[ Wed Jul 17 04:07:17 2024 ] 	Batch(5700/6809) done. Loss: 0.0219  lr:0.000001
[ Wed Jul 17 04:07:39 2024 ] 	Batch(5800/6809) done. Loss: 0.2362  lr:0.000001
[ Wed Jul 17 04:08:02 2024 ] 	Batch(5900/6809) done. Loss: 0.0458  lr:0.000001
[ Wed Jul 17 04:08:25 2024 ] 
Training: Epoch [140/150], Step [5999], Loss: 0.04371367394924164, Training Accuracy: 95.74166666666667
[ Wed Jul 17 04:08:25 2024 ] 	Batch(6000/6809) done. Loss: 0.0574  lr:0.000001
[ Wed Jul 17 04:08:48 2024 ] 	Batch(6100/6809) done. Loss: 0.1457  lr:0.000001
[ Wed Jul 17 04:09:10 2024 ] 	Batch(6200/6809) done. Loss: 0.3919  lr:0.000001
[ Wed Jul 17 04:09:33 2024 ] 	Batch(6300/6809) done. Loss: 0.0965  lr:0.000001
[ Wed Jul 17 04:09:56 2024 ] 	Batch(6400/6809) done. Loss: 0.0172  lr:0.000001
[ Wed Jul 17 04:10:19 2024 ] 
Training: Epoch [140/150], Step [6499], Loss: 0.021103497594594955, Training Accuracy: 95.72115384615385
[ Wed Jul 17 04:10:19 2024 ] 	Batch(6500/6809) done. Loss: 0.1701  lr:0.000001
[ Wed Jul 17 04:10:42 2024 ] 	Batch(6600/6809) done. Loss: 0.0034  lr:0.000001
[ Wed Jul 17 04:11:05 2024 ] 	Batch(6700/6809) done. Loss: 0.0057  lr:0.000001
[ Wed Jul 17 04:11:28 2024 ] 	Batch(6800/6809) done. Loss: 0.0119  lr:0.000001
[ Wed Jul 17 04:11:30 2024 ] 	Mean training loss: 0.1550.
[ Wed Jul 17 04:11:30 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 04:11:30 2024 ] Training epoch: 142
[ Wed Jul 17 04:11:31 2024 ] 	Batch(0/6809) done. Loss: 0.3532  lr:0.000001
[ Wed Jul 17 04:11:54 2024 ] 	Batch(100/6809) done. Loss: 0.1527  lr:0.000001
[ Wed Jul 17 04:12:17 2024 ] 	Batch(200/6809) done. Loss: 0.1526  lr:0.000001
[ Wed Jul 17 04:12:40 2024 ] 	Batch(300/6809) done. Loss: 0.0219  lr:0.000001
[ Wed Jul 17 04:13:03 2024 ] 	Batch(400/6809) done. Loss: 0.1924  lr:0.000001
[ Wed Jul 17 04:13:26 2024 ] 
Training: Epoch [141/150], Step [499], Loss: 0.17286819219589233, Training Accuracy: 95.6
[ Wed Jul 17 04:13:27 2024 ] 	Batch(500/6809) done. Loss: 0.0551  lr:0.000001
[ Wed Jul 17 04:13:50 2024 ] 	Batch(600/6809) done. Loss: 0.1292  lr:0.000001
[ Wed Jul 17 04:14:13 2024 ] 	Batch(700/6809) done. Loss: 0.0374  lr:0.000001
[ Wed Jul 17 04:14:36 2024 ] 	Batch(800/6809) done. Loss: 0.0558  lr:0.000001
[ Wed Jul 17 04:14:59 2024 ] 	Batch(900/6809) done. Loss: 0.0422  lr:0.000001
[ Wed Jul 17 04:15:22 2024 ] 
Training: Epoch [141/150], Step [999], Loss: 0.3578025698661804, Training Accuracy: 95.6375
[ Wed Jul 17 04:15:23 2024 ] 	Batch(1000/6809) done. Loss: 0.0600  lr:0.000001
[ Wed Jul 17 04:15:46 2024 ] 	Batch(1100/6809) done. Loss: 0.0730  lr:0.000001
[ Wed Jul 17 04:16:09 2024 ] 	Batch(1200/6809) done. Loss: 0.0110  lr:0.000001
[ Wed Jul 17 04:16:32 2024 ] 	Batch(1300/6809) done. Loss: 0.1610  lr:0.000001
[ Wed Jul 17 04:16:55 2024 ] 	Batch(1400/6809) done. Loss: 0.0024  lr:0.000001
[ Wed Jul 17 04:17:18 2024 ] 
Training: Epoch [141/150], Step [1499], Loss: 0.06508079916238785, Training Accuracy: 95.79166666666666
[ Wed Jul 17 04:17:18 2024 ] 	Batch(1500/6809) done. Loss: 0.0037  lr:0.000001
[ Wed Jul 17 04:17:41 2024 ] 	Batch(1600/6809) done. Loss: 0.0322  lr:0.000001
[ Wed Jul 17 04:18:04 2024 ] 	Batch(1700/6809) done. Loss: 0.2712  lr:0.000001
[ Wed Jul 17 04:18:27 2024 ] 	Batch(1800/6809) done. Loss: 0.0089  lr:0.000001
[ Wed Jul 17 04:18:49 2024 ] 	Batch(1900/6809) done. Loss: 0.3109  lr:0.000001
[ Wed Jul 17 04:19:12 2024 ] 
Training: Epoch [141/150], Step [1999], Loss: 0.022274935618042946, Training Accuracy: 95.8875
[ Wed Jul 17 04:19:12 2024 ] 	Batch(2000/6809) done. Loss: 0.0894  lr:0.000001
[ Wed Jul 17 04:19:35 2024 ] 	Batch(2100/6809) done. Loss: 0.0263  lr:0.000001
[ Wed Jul 17 04:19:58 2024 ] 	Batch(2200/6809) done. Loss: 0.1693  lr:0.000001
[ Wed Jul 17 04:20:20 2024 ] 	Batch(2300/6809) done. Loss: 0.3823  lr:0.000001
[ Wed Jul 17 04:20:43 2024 ] 	Batch(2400/6809) done. Loss: 0.4479  lr:0.000001
[ Wed Jul 17 04:21:06 2024 ] 
Training: Epoch [141/150], Step [2499], Loss: 0.16452237963676453, Training Accuracy: 95.83500000000001
[ Wed Jul 17 04:21:06 2024 ] 	Batch(2500/6809) done. Loss: 0.2741  lr:0.000001
[ Wed Jul 17 04:21:29 2024 ] 	Batch(2600/6809) done. Loss: 0.2495  lr:0.000001
[ Wed Jul 17 04:21:52 2024 ] 	Batch(2700/6809) done. Loss: 0.1128  lr:0.000001
[ Wed Jul 17 04:22:15 2024 ] 	Batch(2800/6809) done. Loss: 0.0857  lr:0.000001
[ Wed Jul 17 04:22:38 2024 ] 	Batch(2900/6809) done. Loss: 0.1762  lr:0.000001
[ Wed Jul 17 04:23:01 2024 ] 
Training: Epoch [141/150], Step [2999], Loss: 0.010371387004852295, Training Accuracy: 95.775
[ Wed Jul 17 04:23:01 2024 ] 	Batch(3000/6809) done. Loss: 0.1736  lr:0.000001
[ Wed Jul 17 04:23:24 2024 ] 	Batch(3100/6809) done. Loss: 0.0890  lr:0.000001
[ Wed Jul 17 04:23:48 2024 ] 	Batch(3200/6809) done. Loss: 0.9887  lr:0.000001
[ Wed Jul 17 04:24:11 2024 ] 	Batch(3300/6809) done. Loss: 0.0084  lr:0.000001
[ Wed Jul 17 04:24:34 2024 ] 	Batch(3400/6809) done. Loss: 0.0827  lr:0.000001
[ Wed Jul 17 04:24:56 2024 ] 
Training: Epoch [141/150], Step [3499], Loss: 0.04696715250611305, Training Accuracy: 95.75714285714285
[ Wed Jul 17 04:24:56 2024 ] 	Batch(3500/6809) done. Loss: 0.0436  lr:0.000001
[ Wed Jul 17 04:25:19 2024 ] 	Batch(3600/6809) done. Loss: 0.0614  lr:0.000001
[ Wed Jul 17 04:25:42 2024 ] 	Batch(3700/6809) done. Loss: 0.0205  lr:0.000001
[ Wed Jul 17 04:26:05 2024 ] 	Batch(3800/6809) done. Loss: 0.4691  lr:0.000001
[ Wed Jul 17 04:26:28 2024 ] 	Batch(3900/6809) done. Loss: 0.2885  lr:0.000001
[ Wed Jul 17 04:26:50 2024 ] 
Training: Epoch [141/150], Step [3999], Loss: 0.04222454875707626, Training Accuracy: 95.765625
[ Wed Jul 17 04:26:51 2024 ] 	Batch(4000/6809) done. Loss: 0.0771  lr:0.000001
[ Wed Jul 17 04:27:14 2024 ] 	Batch(4100/6809) done. Loss: 0.4787  lr:0.000001
[ Wed Jul 17 04:27:38 2024 ] 	Batch(4200/6809) done. Loss: 0.3035  lr:0.000001
[ Wed Jul 17 04:28:02 2024 ] 	Batch(4300/6809) done. Loss: 0.3256  lr:0.000001
[ Wed Jul 17 04:28:26 2024 ] 	Batch(4400/6809) done. Loss: 0.2076  lr:0.000001
[ Wed Jul 17 04:28:49 2024 ] 
Training: Epoch [141/150], Step [4499], Loss: 0.060352228581905365, Training Accuracy: 95.73888888888888
[ Wed Jul 17 04:28:49 2024 ] 	Batch(4500/6809) done. Loss: 0.0356  lr:0.000001
[ Wed Jul 17 04:29:12 2024 ] 	Batch(4600/6809) done. Loss: 0.1207  lr:0.000001
[ Wed Jul 17 04:29:35 2024 ] 	Batch(4700/6809) done. Loss: 0.1362  lr:0.000001
[ Wed Jul 17 04:29:58 2024 ] 	Batch(4800/6809) done. Loss: 0.1756  lr:0.000001
[ Wed Jul 17 04:30:21 2024 ] 	Batch(4900/6809) done. Loss: 0.2840  lr:0.000001
[ Wed Jul 17 04:30:44 2024 ] 
Training: Epoch [141/150], Step [4999], Loss: 0.2639526128768921, Training Accuracy: 95.74249999999999
[ Wed Jul 17 04:30:44 2024 ] 	Batch(5000/6809) done. Loss: 0.1503  lr:0.000001
[ Wed Jul 17 04:31:07 2024 ] 	Batch(5100/6809) done. Loss: 0.2145  lr:0.000001
[ Wed Jul 17 04:31:30 2024 ] 	Batch(5200/6809) done. Loss: 0.7194  lr:0.000001
[ Wed Jul 17 04:31:52 2024 ] 	Batch(5300/6809) done. Loss: 0.0073  lr:0.000001
[ Wed Jul 17 04:32:15 2024 ] 	Batch(5400/6809) done. Loss: 0.1822  lr:0.000001
[ Wed Jul 17 04:32:37 2024 ] 
Training: Epoch [141/150], Step [5499], Loss: 0.0421537421643734, Training Accuracy: 95.76136363636364
[ Wed Jul 17 04:32:38 2024 ] 	Batch(5500/6809) done. Loss: 0.0236  lr:0.000001
[ Wed Jul 17 04:33:00 2024 ] 	Batch(5600/6809) done. Loss: 0.1123  lr:0.000001
[ Wed Jul 17 04:33:23 2024 ] 	Batch(5700/6809) done. Loss: 0.1955  lr:0.000001
[ Wed Jul 17 04:33:45 2024 ] 	Batch(5800/6809) done. Loss: 0.0362  lr:0.000001
[ Wed Jul 17 04:34:08 2024 ] 	Batch(5900/6809) done. Loss: 0.0704  lr:0.000001
[ Wed Jul 17 04:34:31 2024 ] 
Training: Epoch [141/150], Step [5999], Loss: 0.2655256986618042, Training Accuracy: 95.82291666666667
[ Wed Jul 17 04:34:31 2024 ] 	Batch(6000/6809) done. Loss: 0.0080  lr:0.000001
[ Wed Jul 17 04:34:53 2024 ] 	Batch(6100/6809) done. Loss: 0.0118  lr:0.000001
[ Wed Jul 17 04:35:16 2024 ] 	Batch(6200/6809) done. Loss: 0.1116  lr:0.000001
[ Wed Jul 17 04:35:39 2024 ] 	Batch(6300/6809) done. Loss: 0.0977  lr:0.000001
[ Wed Jul 17 04:36:02 2024 ] 	Batch(6400/6809) done. Loss: 0.1276  lr:0.000001
[ Wed Jul 17 04:36:24 2024 ] 
Training: Epoch [141/150], Step [6499], Loss: 0.19352301955223083, Training Accuracy: 95.79807692307692
[ Wed Jul 17 04:36:24 2024 ] 	Batch(6500/6809) done. Loss: 0.1252  lr:0.000001
[ Wed Jul 17 04:36:47 2024 ] 	Batch(6600/6809) done. Loss: 0.0849  lr:0.000001
[ Wed Jul 17 04:37:10 2024 ] 	Batch(6700/6809) done. Loss: 0.3465  lr:0.000001
[ Wed Jul 17 04:37:33 2024 ] 	Batch(6800/6809) done. Loss: 0.6226  lr:0.000001
[ Wed Jul 17 04:37:35 2024 ] 	Mean training loss: 0.1551.
[ Wed Jul 17 04:37:35 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 04:37:35 2024 ] Training epoch: 143
[ Wed Jul 17 04:37:35 2024 ] 	Batch(0/6809) done. Loss: 0.0288  lr:0.000001
[ Wed Jul 17 04:37:58 2024 ] 	Batch(100/6809) done. Loss: 0.1931  lr:0.000001
[ Wed Jul 17 04:38:21 2024 ] 	Batch(200/6809) done. Loss: 0.0245  lr:0.000001
[ Wed Jul 17 04:38:44 2024 ] 	Batch(300/6809) done. Loss: 0.1325  lr:0.000001
[ Wed Jul 17 04:39:07 2024 ] 	Batch(400/6809) done. Loss: 0.2304  lr:0.000001
[ Wed Jul 17 04:39:30 2024 ] 
Training: Epoch [142/150], Step [499], Loss: 0.12398193031549454, Training Accuracy: 96.05
[ Wed Jul 17 04:39:30 2024 ] 	Batch(500/6809) done. Loss: 0.0019  lr:0.000001
[ Wed Jul 17 04:39:53 2024 ] 	Batch(600/6809) done. Loss: 0.0482  lr:0.000001
[ Wed Jul 17 04:40:15 2024 ] 	Batch(700/6809) done. Loss: 0.0416  lr:0.000001
[ Wed Jul 17 04:40:38 2024 ] 	Batch(800/6809) done. Loss: 0.0250  lr:0.000001
[ Wed Jul 17 04:41:01 2024 ] 	Batch(900/6809) done. Loss: 0.2412  lr:0.000001
[ Wed Jul 17 04:41:23 2024 ] 
Training: Epoch [142/150], Step [999], Loss: 0.4315181076526642, Training Accuracy: 95.96249999999999
[ Wed Jul 17 04:41:23 2024 ] 	Batch(1000/6809) done. Loss: 0.0695  lr:0.000001
[ Wed Jul 17 04:41:46 2024 ] 	Batch(1100/6809) done. Loss: 0.0315  lr:0.000001
[ Wed Jul 17 04:42:08 2024 ] 	Batch(1200/6809) done. Loss: 0.0385  lr:0.000001
[ Wed Jul 17 04:42:31 2024 ] 	Batch(1300/6809) done. Loss: 0.1867  lr:0.000001
[ Wed Jul 17 04:42:53 2024 ] 	Batch(1400/6809) done. Loss: 0.2193  lr:0.000001
[ Wed Jul 17 04:43:16 2024 ] 
Training: Epoch [142/150], Step [1499], Loss: 0.21319185197353363, Training Accuracy: 95.89166666666667
[ Wed Jul 17 04:43:16 2024 ] 	Batch(1500/6809) done. Loss: 0.0695  lr:0.000001
[ Wed Jul 17 04:43:39 2024 ] 	Batch(1600/6809) done. Loss: 0.3769  lr:0.000001
[ Wed Jul 17 04:44:01 2024 ] 	Batch(1700/6809) done. Loss: 0.1700  lr:0.000001
[ Wed Jul 17 04:44:24 2024 ] 	Batch(1800/6809) done. Loss: 0.0091  lr:0.000001
[ Wed Jul 17 04:44:46 2024 ] 	Batch(1900/6809) done. Loss: 0.0267  lr:0.000001
[ Wed Jul 17 04:45:08 2024 ] 
Training: Epoch [142/150], Step [1999], Loss: 0.041844189167022705, Training Accuracy: 95.75625
[ Wed Jul 17 04:45:09 2024 ] 	Batch(2000/6809) done. Loss: 0.1425  lr:0.000001
[ Wed Jul 17 04:45:31 2024 ] 	Batch(2100/6809) done. Loss: 0.1140  lr:0.000001
[ Wed Jul 17 04:45:54 2024 ] 	Batch(2200/6809) done. Loss: 0.1982  lr:0.000001
[ Wed Jul 17 04:46:16 2024 ] 	Batch(2300/6809) done. Loss: 0.1589  lr:0.000001
[ Wed Jul 17 04:46:39 2024 ] 	Batch(2400/6809) done. Loss: 0.8323  lr:0.000001
[ Wed Jul 17 04:47:01 2024 ] 
Training: Epoch [142/150], Step [2499], Loss: 0.08513498306274414, Training Accuracy: 95.7
[ Wed Jul 17 04:47:02 2024 ] 	Batch(2500/6809) done. Loss: 0.0326  lr:0.000001
[ Wed Jul 17 04:47:25 2024 ] 	Batch(2600/6809) done. Loss: 0.0429  lr:0.000001
[ Wed Jul 17 04:47:48 2024 ] 	Batch(2700/6809) done. Loss: 0.1698  lr:0.000001
[ Wed Jul 17 04:48:10 2024 ] 	Batch(2800/6809) done. Loss: 0.3391  lr:0.000001
[ Wed Jul 17 04:48:33 2024 ] 	Batch(2900/6809) done. Loss: 0.1920  lr:0.000001
[ Wed Jul 17 04:48:55 2024 ] 
Training: Epoch [142/150], Step [2999], Loss: 0.6787192821502686, Training Accuracy: 95.66666666666667
[ Wed Jul 17 04:48:55 2024 ] 	Batch(3000/6809) done. Loss: 0.0159  lr:0.000001
[ Wed Jul 17 04:49:18 2024 ] 	Batch(3100/6809) done. Loss: 0.1010  lr:0.000001
[ Wed Jul 17 04:49:40 2024 ] 	Batch(3200/6809) done. Loss: 0.3773  lr:0.000001
[ Wed Jul 17 04:50:04 2024 ] 	Batch(3300/6809) done. Loss: 0.0241  lr:0.000001
[ Wed Jul 17 04:50:28 2024 ] 	Batch(3400/6809) done. Loss: 0.0045  lr:0.000001
[ Wed Jul 17 04:50:51 2024 ] 
Training: Epoch [142/150], Step [3499], Loss: 0.20317420363426208, Training Accuracy: 95.61071428571428
[ Wed Jul 17 04:50:51 2024 ] 	Batch(3500/6809) done. Loss: 0.3245  lr:0.000001
[ Wed Jul 17 04:51:14 2024 ] 	Batch(3600/6809) done. Loss: 0.1125  lr:0.000001
[ Wed Jul 17 04:51:38 2024 ] 	Batch(3700/6809) done. Loss: 0.0422  lr:0.000001
[ Wed Jul 17 04:52:01 2024 ] 	Batch(3800/6809) done. Loss: 0.1568  lr:0.000001
[ Wed Jul 17 04:52:24 2024 ] 	Batch(3900/6809) done. Loss: 0.1261  lr:0.000001
[ Wed Jul 17 04:52:47 2024 ] 
Training: Epoch [142/150], Step [3999], Loss: 0.06225203722715378, Training Accuracy: 95.578125
[ Wed Jul 17 04:52:48 2024 ] 	Batch(4000/6809) done. Loss: 0.0942  lr:0.000001
[ Wed Jul 17 04:53:11 2024 ] 	Batch(4100/6809) done. Loss: 0.0582  lr:0.000001
[ Wed Jul 17 04:53:34 2024 ] 	Batch(4200/6809) done. Loss: 0.1130  lr:0.000001
[ Wed Jul 17 04:53:57 2024 ] 	Batch(4300/6809) done. Loss: 0.0301  lr:0.000001
[ Wed Jul 17 04:54:20 2024 ] 	Batch(4400/6809) done. Loss: 0.9887  lr:0.000001
[ Wed Jul 17 04:54:42 2024 ] 
Training: Epoch [142/150], Step [4499], Loss: 0.04360206425189972, Training Accuracy: 95.5638888888889
[ Wed Jul 17 04:54:43 2024 ] 	Batch(4500/6809) done. Loss: 0.0317  lr:0.000001
[ Wed Jul 17 04:55:06 2024 ] 	Batch(4600/6809) done. Loss: 0.3682  lr:0.000001
[ Wed Jul 17 04:55:29 2024 ] 	Batch(4700/6809) done. Loss: 0.0363  lr:0.000001
[ Wed Jul 17 04:55:53 2024 ] 	Batch(4800/6809) done. Loss: 0.0762  lr:0.000001
[ Wed Jul 17 04:56:16 2024 ] 	Batch(4900/6809) done. Loss: 0.0970  lr:0.000001
[ Wed Jul 17 04:56:39 2024 ] 
Training: Epoch [142/150], Step [4999], Loss: 0.6399422883987427, Training Accuracy: 95.555
[ Wed Jul 17 04:56:39 2024 ] 	Batch(5000/6809) done. Loss: 0.1470  lr:0.000001
[ Wed Jul 17 04:57:02 2024 ] 	Batch(5100/6809) done. Loss: 0.7949  lr:0.000001
[ Wed Jul 17 04:57:26 2024 ] 	Batch(5200/6809) done. Loss: 0.0332  lr:0.000001
[ Wed Jul 17 04:57:49 2024 ] 	Batch(5300/6809) done. Loss: 0.3914  lr:0.000001
[ Wed Jul 17 04:58:12 2024 ] 	Batch(5400/6809) done. Loss: 0.0035  lr:0.000001
[ Wed Jul 17 04:58:35 2024 ] 
Training: Epoch [142/150], Step [5499], Loss: 0.14284853637218475, Training Accuracy: 95.57272727272728
[ Wed Jul 17 04:58:35 2024 ] 	Batch(5500/6809) done. Loss: 0.5745  lr:0.000001
[ Wed Jul 17 04:58:58 2024 ] 	Batch(5600/6809) done. Loss: 0.2382  lr:0.000001
[ Wed Jul 17 04:59:21 2024 ] 	Batch(5700/6809) done. Loss: 0.1110  lr:0.000001
[ Wed Jul 17 04:59:43 2024 ] 	Batch(5800/6809) done. Loss: 0.0560  lr:0.000001
[ Wed Jul 17 05:00:06 2024 ] 	Batch(5900/6809) done. Loss: 0.0413  lr:0.000001
[ Wed Jul 17 05:00:28 2024 ] 
Training: Epoch [142/150], Step [5999], Loss: 0.13802219927310944, Training Accuracy: 95.58541666666667
[ Wed Jul 17 05:00:29 2024 ] 	Batch(6000/6809) done. Loss: 0.2011  lr:0.000001
[ Wed Jul 17 05:00:51 2024 ] 	Batch(6100/6809) done. Loss: 0.0419  lr:0.000001
[ Wed Jul 17 05:01:14 2024 ] 	Batch(6200/6809) done. Loss: 0.4226  lr:0.000001
[ Wed Jul 17 05:01:37 2024 ] 	Batch(6300/6809) done. Loss: 0.0918  lr:0.000001
[ Wed Jul 17 05:02:00 2024 ] 	Batch(6400/6809) done. Loss: 0.1349  lr:0.000001
[ Wed Jul 17 05:02:22 2024 ] 
Training: Epoch [142/150], Step [6499], Loss: 0.5268721580505371, Training Accuracy: 95.58461538461539
[ Wed Jul 17 05:02:22 2024 ] 	Batch(6500/6809) done. Loss: 0.0186  lr:0.000001
[ Wed Jul 17 05:02:45 2024 ] 	Batch(6600/6809) done. Loss: 0.0656  lr:0.000001
[ Wed Jul 17 05:03:08 2024 ] 	Batch(6700/6809) done. Loss: 0.0079  lr:0.000001
[ Wed Jul 17 05:03:30 2024 ] 	Batch(6800/6809) done. Loss: 0.1207  lr:0.000001
[ Wed Jul 17 05:03:32 2024 ] 	Mean training loss: 0.1554.
[ Wed Jul 17 05:03:32 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 05:03:32 2024 ] Training epoch: 144
[ Wed Jul 17 05:03:33 2024 ] 	Batch(0/6809) done. Loss: 0.0972  lr:0.000001
[ Wed Jul 17 05:03:56 2024 ] 	Batch(100/6809) done. Loss: 0.1651  lr:0.000001
[ Wed Jul 17 05:04:18 2024 ] 	Batch(200/6809) done. Loss: 0.2614  lr:0.000001
[ Wed Jul 17 05:04:41 2024 ] 	Batch(300/6809) done. Loss: 0.5710  lr:0.000001
[ Wed Jul 17 05:05:03 2024 ] 	Batch(400/6809) done. Loss: 0.0061  lr:0.000001
[ Wed Jul 17 05:05:26 2024 ] 
Training: Epoch [143/150], Step [499], Loss: 0.0333593375980854, Training Accuracy: 95.8
[ Wed Jul 17 05:05:26 2024 ] 	Batch(500/6809) done. Loss: 0.0114  lr:0.000001
[ Wed Jul 17 05:05:49 2024 ] 	Batch(600/6809) done. Loss: 0.0186  lr:0.000001
[ Wed Jul 17 05:06:11 2024 ] 	Batch(700/6809) done. Loss: 0.0042  lr:0.000001
[ Wed Jul 17 05:06:34 2024 ] 	Batch(800/6809) done. Loss: 0.0617  lr:0.000001
[ Wed Jul 17 05:06:57 2024 ] 	Batch(900/6809) done. Loss: 0.0689  lr:0.000001
[ Wed Jul 17 05:07:19 2024 ] 
Training: Epoch [143/150], Step [999], Loss: 0.07466886937618256, Training Accuracy: 95.6125
[ Wed Jul 17 05:07:19 2024 ] 	Batch(1000/6809) done. Loss: 0.3523  lr:0.000001
[ Wed Jul 17 05:07:42 2024 ] 	Batch(1100/6809) done. Loss: 0.0048  lr:0.000001
[ Wed Jul 17 05:08:04 2024 ] 	Batch(1200/6809) done. Loss: 0.0084  lr:0.000001
[ Wed Jul 17 05:08:27 2024 ] 	Batch(1300/6809) done. Loss: 0.0040  lr:0.000001
[ Wed Jul 17 05:08:50 2024 ] 	Batch(1400/6809) done. Loss: 0.0287  lr:0.000001
[ Wed Jul 17 05:09:12 2024 ] 
Training: Epoch [143/150], Step [1499], Loss: 0.17022593319416046, Training Accuracy: 95.775
[ Wed Jul 17 05:09:12 2024 ] 	Batch(1500/6809) done. Loss: 0.1483  lr:0.000001
[ Wed Jul 17 05:09:35 2024 ] 	Batch(1600/6809) done. Loss: 0.0305  lr:0.000001
[ Wed Jul 17 05:09:58 2024 ] 	Batch(1700/6809) done. Loss: 0.1788  lr:0.000001
[ Wed Jul 17 05:10:20 2024 ] 	Batch(1800/6809) done. Loss: 0.0337  lr:0.000001
[ Wed Jul 17 05:10:43 2024 ] 	Batch(1900/6809) done. Loss: 0.0205  lr:0.000001
[ Wed Jul 17 05:11:05 2024 ] 
Training: Epoch [143/150], Step [1999], Loss: 0.056681569665670395, Training Accuracy: 95.7125
[ Wed Jul 17 05:11:05 2024 ] 	Batch(2000/6809) done. Loss: 0.3605  lr:0.000001
[ Wed Jul 17 05:11:28 2024 ] 	Batch(2100/6809) done. Loss: 0.3133  lr:0.000001
[ Wed Jul 17 05:11:51 2024 ] 	Batch(2200/6809) done. Loss: 0.0192  lr:0.000001
[ Wed Jul 17 05:12:13 2024 ] 	Batch(2300/6809) done. Loss: 0.2873  lr:0.000001
[ Wed Jul 17 05:12:36 2024 ] 	Batch(2400/6809) done. Loss: 0.1323  lr:0.000001
[ Wed Jul 17 05:12:59 2024 ] 
Training: Epoch [143/150], Step [2499], Loss: 0.028269978240132332, Training Accuracy: 95.66
[ Wed Jul 17 05:12:59 2024 ] 	Batch(2500/6809) done. Loss: 0.4187  lr:0.000001
[ Wed Jul 17 05:13:22 2024 ] 	Batch(2600/6809) done. Loss: 0.1900  lr:0.000001
[ Wed Jul 17 05:13:45 2024 ] 	Batch(2700/6809) done. Loss: 0.0481  lr:0.000001
[ Wed Jul 17 05:14:08 2024 ] 	Batch(2800/6809) done. Loss: 0.0706  lr:0.000001
[ Wed Jul 17 05:14:31 2024 ] 	Batch(2900/6809) done. Loss: 0.0354  lr:0.000001
[ Wed Jul 17 05:14:54 2024 ] 
Training: Epoch [143/150], Step [2999], Loss: 0.08158020675182343, Training Accuracy: 95.65416666666667
[ Wed Jul 17 05:14:54 2024 ] 	Batch(3000/6809) done. Loss: 0.0299  lr:0.000001
[ Wed Jul 17 05:15:17 2024 ] 	Batch(3100/6809) done. Loss: 0.1773  lr:0.000001
[ Wed Jul 17 05:15:40 2024 ] 	Batch(3200/6809) done. Loss: 0.0939  lr:0.000001
[ Wed Jul 17 05:16:03 2024 ] 	Batch(3300/6809) done. Loss: 0.0621  lr:0.000001
[ Wed Jul 17 05:16:26 2024 ] 	Batch(3400/6809) done. Loss: 0.4274  lr:0.000001
[ Wed Jul 17 05:16:49 2024 ] 
Training: Epoch [143/150], Step [3499], Loss: 0.009408852085471153, Training Accuracy: 95.63928571428572
[ Wed Jul 17 05:16:49 2024 ] 	Batch(3500/6809) done. Loss: 0.2532  lr:0.000001
[ Wed Jul 17 05:17:13 2024 ] 	Batch(3600/6809) done. Loss: 0.2463  lr:0.000001
[ Wed Jul 17 05:17:36 2024 ] 	Batch(3700/6809) done. Loss: 0.2849  lr:0.000001
[ Wed Jul 17 05:17:59 2024 ] 	Batch(3800/6809) done. Loss: 0.1489  lr:0.000001
[ Wed Jul 17 05:18:22 2024 ] 	Batch(3900/6809) done. Loss: 0.7253  lr:0.000001
[ Wed Jul 17 05:18:44 2024 ] 
Training: Epoch [143/150], Step [3999], Loss: 0.039129555225372314, Training Accuracy: 95.6625
[ Wed Jul 17 05:18:45 2024 ] 	Batch(4000/6809) done. Loss: 0.1363  lr:0.000001
[ Wed Jul 17 05:19:08 2024 ] 	Batch(4100/6809) done. Loss: 0.0949  lr:0.000001
[ Wed Jul 17 05:19:31 2024 ] 	Batch(4200/6809) done. Loss: 0.2439  lr:0.000001
[ Wed Jul 17 05:19:54 2024 ] 	Batch(4300/6809) done. Loss: 0.0313  lr:0.000001
[ Wed Jul 17 05:20:17 2024 ] 	Batch(4400/6809) done. Loss: 0.1022  lr:0.000001
[ Wed Jul 17 05:20:40 2024 ] 
Training: Epoch [143/150], Step [4499], Loss: 0.19374555349349976, Training Accuracy: 95.75
[ Wed Jul 17 05:20:41 2024 ] 	Batch(4500/6809) done. Loss: 0.1617  lr:0.000001
[ Wed Jul 17 05:21:04 2024 ] 	Batch(4600/6809) done. Loss: 0.2202  lr:0.000001
[ Wed Jul 17 05:21:27 2024 ] 	Batch(4700/6809) done. Loss: 0.0274  lr:0.000001
[ Wed Jul 17 05:21:51 2024 ] 	Batch(4800/6809) done. Loss: 0.1143  lr:0.000001
[ Wed Jul 17 05:22:14 2024 ] 	Batch(4900/6809) done. Loss: 0.1117  lr:0.000001
[ Wed Jul 17 05:22:37 2024 ] 
Training: Epoch [143/150], Step [4999], Loss: 0.09571409225463867, Training Accuracy: 95.78750000000001
[ Wed Jul 17 05:22:38 2024 ] 	Batch(5000/6809) done. Loss: 0.0892  lr:0.000001
[ Wed Jul 17 05:23:01 2024 ] 	Batch(5100/6809) done. Loss: 0.0785  lr:0.000001
[ Wed Jul 17 05:23:25 2024 ] 	Batch(5200/6809) done. Loss: 0.0443  lr:0.000001
[ Wed Jul 17 05:23:48 2024 ] 	Batch(5300/6809) done. Loss: 0.0740  lr:0.000001
[ Wed Jul 17 05:24:11 2024 ] 	Batch(5400/6809) done. Loss: 0.0758  lr:0.000001
[ Wed Jul 17 05:24:34 2024 ] 
Training: Epoch [143/150], Step [5499], Loss: 0.5206215381622314, Training Accuracy: 95.76818181818182
[ Wed Jul 17 05:24:35 2024 ] 	Batch(5500/6809) done. Loss: 0.4153  lr:0.000001
[ Wed Jul 17 05:24:58 2024 ] 	Batch(5600/6809) done. Loss: 0.2905  lr:0.000001
[ Wed Jul 17 05:25:21 2024 ] 	Batch(5700/6809) done. Loss: 0.3681  lr:0.000001
[ Wed Jul 17 05:25:43 2024 ] 	Batch(5800/6809) done. Loss: 0.0971  lr:0.000001
[ Wed Jul 17 05:26:06 2024 ] 	Batch(5900/6809) done. Loss: 0.4414  lr:0.000001
[ Wed Jul 17 05:26:28 2024 ] 
Training: Epoch [143/150], Step [5999], Loss: 0.12641598284244537, Training Accuracy: 95.75416666666666
[ Wed Jul 17 05:26:29 2024 ] 	Batch(6000/6809) done. Loss: 0.0099  lr:0.000001
[ Wed Jul 17 05:26:51 2024 ] 	Batch(6100/6809) done. Loss: 0.0417  lr:0.000001
[ Wed Jul 17 05:27:14 2024 ] 	Batch(6200/6809) done. Loss: 0.4142  lr:0.000001
[ Wed Jul 17 05:27:37 2024 ] 	Batch(6300/6809) done. Loss: 0.0437  lr:0.000001
[ Wed Jul 17 05:27:59 2024 ] 	Batch(6400/6809) done. Loss: 0.1005  lr:0.000001
[ Wed Jul 17 05:28:22 2024 ] 
Training: Epoch [143/150], Step [6499], Loss: 0.14370371401309967, Training Accuracy: 95.75769230769231
[ Wed Jul 17 05:28:22 2024 ] 	Batch(6500/6809) done. Loss: 0.0038  lr:0.000001
[ Wed Jul 17 05:28:45 2024 ] 	Batch(6600/6809) done. Loss: 0.3016  lr:0.000001
[ Wed Jul 17 05:29:07 2024 ] 	Batch(6700/6809) done. Loss: 0.0161  lr:0.000001
[ Wed Jul 17 05:29:30 2024 ] 	Batch(6800/6809) done. Loss: 0.0314  lr:0.000001
[ Wed Jul 17 05:29:32 2024 ] 	Mean training loss: 0.1559.
[ Wed Jul 17 05:29:32 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 05:29:32 2024 ] Training epoch: 145
[ Wed Jul 17 05:29:33 2024 ] 	Batch(0/6809) done. Loss: 0.1530  lr:0.000001
[ Wed Jul 17 05:29:55 2024 ] 	Batch(100/6809) done. Loss: 0.0046  lr:0.000001
[ Wed Jul 17 05:30:18 2024 ] 	Batch(200/6809) done. Loss: 0.2290  lr:0.000001
[ Wed Jul 17 05:30:40 2024 ] 	Batch(300/6809) done. Loss: 0.0618  lr:0.000001
[ Wed Jul 17 05:31:03 2024 ] 	Batch(400/6809) done. Loss: 0.2173  lr:0.000001
[ Wed Jul 17 05:31:26 2024 ] 
Training: Epoch [144/150], Step [499], Loss: 0.13970722258090973, Training Accuracy: 95.625
[ Wed Jul 17 05:31:26 2024 ] 	Batch(500/6809) done. Loss: 0.0062  lr:0.000001
[ Wed Jul 17 05:31:48 2024 ] 	Batch(600/6809) done. Loss: 0.0213  lr:0.000001
[ Wed Jul 17 05:32:11 2024 ] 	Batch(700/6809) done. Loss: 0.0164  lr:0.000001
[ Wed Jul 17 05:32:34 2024 ] 	Batch(800/6809) done. Loss: 0.1218  lr:0.000001
[ Wed Jul 17 05:32:56 2024 ] 	Batch(900/6809) done. Loss: 0.2706  lr:0.000001
[ Wed Jul 17 05:33:19 2024 ] 
Training: Epoch [144/150], Step [999], Loss: 0.056705448776483536, Training Accuracy: 95.83749999999999
[ Wed Jul 17 05:33:19 2024 ] 	Batch(1000/6809) done. Loss: 0.2054  lr:0.000001
[ Wed Jul 17 05:33:42 2024 ] 	Batch(1100/6809) done. Loss: 0.0622  lr:0.000001
[ Wed Jul 17 05:34:05 2024 ] 	Batch(1200/6809) done. Loss: 0.0199  lr:0.000001
[ Wed Jul 17 05:34:29 2024 ] 	Batch(1300/6809) done. Loss: 0.1613  lr:0.000001
[ Wed Jul 17 05:34:52 2024 ] 	Batch(1400/6809) done. Loss: 0.0924  lr:0.000001
[ Wed Jul 17 05:35:15 2024 ] 
Training: Epoch [144/150], Step [1499], Loss: 0.21637624502182007, Training Accuracy: 95.575
[ Wed Jul 17 05:35:15 2024 ] 	Batch(1500/6809) done. Loss: 0.2695  lr:0.000001
[ Wed Jul 17 05:35:39 2024 ] 	Batch(1600/6809) done. Loss: 0.0063  lr:0.000001
[ Wed Jul 17 05:36:02 2024 ] 	Batch(1700/6809) done. Loss: 0.0744  lr:0.000001
[ Wed Jul 17 05:36:26 2024 ] 	Batch(1800/6809) done. Loss: 0.0842  lr:0.000001
[ Wed Jul 17 05:36:49 2024 ] 	Batch(1900/6809) done. Loss: 0.1633  lr:0.000001
[ Wed Jul 17 05:37:12 2024 ] 
Training: Epoch [144/150], Step [1999], Loss: 0.05872807651758194, Training Accuracy: 95.72500000000001
[ Wed Jul 17 05:37:13 2024 ] 	Batch(2000/6809) done. Loss: 0.4448  lr:0.000001
[ Wed Jul 17 05:37:36 2024 ] 	Batch(2100/6809) done. Loss: 0.1630  lr:0.000001
[ Wed Jul 17 05:37:59 2024 ] 	Batch(2200/6809) done. Loss: 0.2193  lr:0.000001
[ Wed Jul 17 05:38:23 2024 ] 	Batch(2300/6809) done. Loss: 0.1339  lr:0.000001
[ Wed Jul 17 05:38:46 2024 ] 	Batch(2400/6809) done. Loss: 0.0295  lr:0.000001
[ Wed Jul 17 05:39:08 2024 ] 
Training: Epoch [144/150], Step [2499], Loss: 0.2148127406835556, Training Accuracy: 95.7
[ Wed Jul 17 05:39:09 2024 ] 	Batch(2500/6809) done. Loss: 0.1325  lr:0.000001
[ Wed Jul 17 05:39:31 2024 ] 	Batch(2600/6809) done. Loss: 0.1178  lr:0.000001
[ Wed Jul 17 05:39:54 2024 ] 	Batch(2700/6809) done. Loss: 0.0780  lr:0.000001
[ Wed Jul 17 05:40:16 2024 ] 	Batch(2800/6809) done. Loss: 0.0210  lr:0.000001
[ Wed Jul 17 05:40:39 2024 ] 	Batch(2900/6809) done. Loss: 0.0878  lr:0.000001
[ Wed Jul 17 05:41:01 2024 ] 
Training: Epoch [144/150], Step [2999], Loss: 0.23286911845207214, Training Accuracy: 95.65416666666667
[ Wed Jul 17 05:41:02 2024 ] 	Batch(3000/6809) done. Loss: 0.0266  lr:0.000001
[ Wed Jul 17 05:41:24 2024 ] 	Batch(3100/6809) done. Loss: 0.0071  lr:0.000001
[ Wed Jul 17 05:41:47 2024 ] 	Batch(3200/6809) done. Loss: 0.4077  lr:0.000001
[ Wed Jul 17 05:42:10 2024 ] 	Batch(3300/6809) done. Loss: 0.2189  lr:0.000001
[ Wed Jul 17 05:42:32 2024 ] 	Batch(3400/6809) done. Loss: 0.0298  lr:0.000001
[ Wed Jul 17 05:42:55 2024 ] 
Training: Epoch [144/150], Step [3499], Loss: 0.3880313038825989, Training Accuracy: 95.58928571428571
[ Wed Jul 17 05:42:55 2024 ] 	Batch(3500/6809) done. Loss: 0.1521  lr:0.000001
[ Wed Jul 17 05:43:18 2024 ] 	Batch(3600/6809) done. Loss: 0.0917  lr:0.000001
[ Wed Jul 17 05:43:40 2024 ] 	Batch(3700/6809) done. Loss: 0.0497  lr:0.000001
[ Wed Jul 17 05:44:03 2024 ] 	Batch(3800/6809) done. Loss: 0.3124  lr:0.000001
[ Wed Jul 17 05:44:25 2024 ] 	Batch(3900/6809) done. Loss: 0.0162  lr:0.000001
[ Wed Jul 17 05:44:48 2024 ] 
Training: Epoch [144/150], Step [3999], Loss: 0.24983718991279602, Training Accuracy: 95.64375000000001
[ Wed Jul 17 05:44:48 2024 ] 	Batch(4000/6809) done. Loss: 0.0038  lr:0.000001
[ Wed Jul 17 05:45:11 2024 ] 	Batch(4100/6809) done. Loss: 0.0184  lr:0.000001
[ Wed Jul 17 05:45:33 2024 ] 	Batch(4200/6809) done. Loss: 0.6351  lr:0.000001
[ Wed Jul 17 05:45:56 2024 ] 	Batch(4300/6809) done. Loss: 0.1583  lr:0.000001
[ Wed Jul 17 05:46:19 2024 ] 	Batch(4400/6809) done. Loss: 0.0637  lr:0.000001
[ Wed Jul 17 05:46:42 2024 ] 
Training: Epoch [144/150], Step [4499], Loss: 0.06437650322914124, Training Accuracy: 95.60555555555555
[ Wed Jul 17 05:46:42 2024 ] 	Batch(4500/6809) done. Loss: 0.0280  lr:0.000001
[ Wed Jul 17 05:47:05 2024 ] 	Batch(4600/6809) done. Loss: 0.0579  lr:0.000001
[ Wed Jul 17 05:47:27 2024 ] 	Batch(4700/6809) done. Loss: 0.1762  lr:0.000001
[ Wed Jul 17 05:47:50 2024 ] 	Batch(4800/6809) done. Loss: 0.0026  lr:0.000001
[ Wed Jul 17 05:48:13 2024 ] 	Batch(4900/6809) done. Loss: 0.0528  lr:0.000001
[ Wed Jul 17 05:48:36 2024 ] 
Training: Epoch [144/150], Step [4999], Loss: 0.09227669984102249, Training Accuracy: 95.6475
[ Wed Jul 17 05:48:36 2024 ] 	Batch(5000/6809) done. Loss: 0.0442  lr:0.000001
[ Wed Jul 17 05:48:59 2024 ] 	Batch(5100/6809) done. Loss: 0.0472  lr:0.000001
[ Wed Jul 17 05:49:23 2024 ] 	Batch(5200/6809) done. Loss: 0.0303  lr:0.000001
[ Wed Jul 17 05:49:46 2024 ] 	Batch(5300/6809) done. Loss: 0.1988  lr:0.000001
[ Wed Jul 17 05:50:10 2024 ] 	Batch(5400/6809) done. Loss: 0.0882  lr:0.000001
[ Wed Jul 17 05:50:33 2024 ] 
Training: Epoch [144/150], Step [5499], Loss: 0.10913021117448807, Training Accuracy: 95.67954545454546
[ Wed Jul 17 05:50:33 2024 ] 	Batch(5500/6809) done. Loss: 0.1676  lr:0.000001
[ Wed Jul 17 05:50:57 2024 ] 	Batch(5600/6809) done. Loss: 0.2131  lr:0.000001
[ Wed Jul 17 05:51:19 2024 ] 	Batch(5700/6809) done. Loss: 0.1591  lr:0.000001
[ Wed Jul 17 05:51:42 2024 ] 	Batch(5800/6809) done. Loss: 0.0464  lr:0.000001
[ Wed Jul 17 05:52:04 2024 ] 	Batch(5900/6809) done. Loss: 0.6069  lr:0.000001
[ Wed Jul 17 05:52:27 2024 ] 
Training: Epoch [144/150], Step [5999], Loss: 0.16311606764793396, Training Accuracy: 95.67916666666667
[ Wed Jul 17 05:52:27 2024 ] 	Batch(6000/6809) done. Loss: 0.0563  lr:0.000001
[ Wed Jul 17 05:52:50 2024 ] 	Batch(6100/6809) done. Loss: 0.3609  lr:0.000001
[ Wed Jul 17 05:53:12 2024 ] 	Batch(6200/6809) done. Loss: 0.3546  lr:0.000001
[ Wed Jul 17 05:53:35 2024 ] 	Batch(6300/6809) done. Loss: 0.1429  lr:0.000001
[ Wed Jul 17 05:53:58 2024 ] 	Batch(6400/6809) done. Loss: 0.2663  lr:0.000001
[ Wed Jul 17 05:54:20 2024 ] 
Training: Epoch [144/150], Step [6499], Loss: 0.09760977327823639, Training Accuracy: 95.68076923076923
[ Wed Jul 17 05:54:20 2024 ] 	Batch(6500/6809) done. Loss: 0.2564  lr:0.000001
[ Wed Jul 17 05:54:43 2024 ] 	Batch(6600/6809) done. Loss: 0.0307  lr:0.000001
[ Wed Jul 17 05:55:05 2024 ] 	Batch(6700/6809) done. Loss: 0.1782  lr:0.000001
[ Wed Jul 17 05:55:28 2024 ] 	Batch(6800/6809) done. Loss: 0.1772  lr:0.000001
[ Wed Jul 17 05:55:30 2024 ] 	Mean training loss: 0.1592.
[ Wed Jul 17 05:55:30 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 05:55:30 2024 ] Training epoch: 146
[ Wed Jul 17 05:55:31 2024 ] 	Batch(0/6809) done. Loss: 0.0417  lr:0.000001
[ Wed Jul 17 05:55:53 2024 ] 	Batch(100/6809) done. Loss: 0.2114  lr:0.000001
[ Wed Jul 17 05:56:16 2024 ] 	Batch(200/6809) done. Loss: 0.0110  lr:0.000001
[ Wed Jul 17 05:56:39 2024 ] 	Batch(300/6809) done. Loss: 0.9638  lr:0.000001
[ Wed Jul 17 05:57:02 2024 ] 	Batch(400/6809) done. Loss: 0.1638  lr:0.000001
[ Wed Jul 17 05:57:25 2024 ] 
Training: Epoch [145/150], Step [499], Loss: 0.011763670481741428, Training Accuracy: 95.075
[ Wed Jul 17 05:57:25 2024 ] 	Batch(500/6809) done. Loss: 0.0454  lr:0.000001
[ Wed Jul 17 05:57:48 2024 ] 	Batch(600/6809) done. Loss: 0.2710  lr:0.000001
[ Wed Jul 17 05:58:11 2024 ] 	Batch(700/6809) done. Loss: 0.0599  lr:0.000001
[ Wed Jul 17 05:58:34 2024 ] 	Batch(800/6809) done. Loss: 0.0733  lr:0.000001
[ Wed Jul 17 05:58:56 2024 ] 	Batch(900/6809) done. Loss: 0.1708  lr:0.000001
[ Wed Jul 17 05:59:19 2024 ] 
Training: Epoch [145/150], Step [999], Loss: 0.06854322552680969, Training Accuracy: 95.4875
[ Wed Jul 17 05:59:19 2024 ] 	Batch(1000/6809) done. Loss: 0.1428  lr:0.000001
[ Wed Jul 17 05:59:42 2024 ] 	Batch(1100/6809) done. Loss: 0.0283  lr:0.000001
[ Wed Jul 17 06:00:04 2024 ] 	Batch(1200/6809) done. Loss: 0.1180  lr:0.000001
[ Wed Jul 17 06:00:27 2024 ] 	Batch(1300/6809) done. Loss: 0.2447  lr:0.000001
[ Wed Jul 17 06:00:50 2024 ] 	Batch(1400/6809) done. Loss: 0.0068  lr:0.000001
[ Wed Jul 17 06:01:12 2024 ] 
Training: Epoch [145/150], Step [1499], Loss: 0.07693295180797577, Training Accuracy: 95.58333333333333
[ Wed Jul 17 06:01:12 2024 ] 	Batch(1500/6809) done. Loss: 0.3102  lr:0.000001
[ Wed Jul 17 06:01:35 2024 ] 	Batch(1600/6809) done. Loss: 0.2863  lr:0.000001
[ Wed Jul 17 06:01:58 2024 ] 	Batch(1700/6809) done. Loss: 0.0042  lr:0.000001
[ Wed Jul 17 06:02:21 2024 ] 	Batch(1800/6809) done. Loss: 0.3328  lr:0.000001
[ Wed Jul 17 06:02:43 2024 ] 	Batch(1900/6809) done. Loss: 0.0092  lr:0.000001
[ Wed Jul 17 06:03:06 2024 ] 
Training: Epoch [145/150], Step [1999], Loss: 0.018941165879368782, Training Accuracy: 95.575
[ Wed Jul 17 06:03:06 2024 ] 	Batch(2000/6809) done. Loss: 0.0174  lr:0.000001
[ Wed Jul 17 06:03:29 2024 ] 	Batch(2100/6809) done. Loss: 0.0873  lr:0.000001
[ Wed Jul 17 06:03:52 2024 ] 	Batch(2200/6809) done. Loss: 0.0321  lr:0.000001
[ Wed Jul 17 06:04:15 2024 ] 	Batch(2300/6809) done. Loss: 0.1756  lr:0.000001
[ Wed Jul 17 06:04:38 2024 ] 	Batch(2400/6809) done. Loss: 0.0318  lr:0.000001
[ Wed Jul 17 06:05:00 2024 ] 
Training: Epoch [145/150], Step [2499], Loss: 0.08295473456382751, Training Accuracy: 95.575
[ Wed Jul 17 06:05:01 2024 ] 	Batch(2500/6809) done. Loss: 0.0274  lr:0.000001
[ Wed Jul 17 06:05:23 2024 ] 	Batch(2600/6809) done. Loss: 0.0355  lr:0.000001
[ Wed Jul 17 06:05:46 2024 ] 	Batch(2700/6809) done. Loss: 0.0493  lr:0.000001
[ Wed Jul 17 06:06:09 2024 ] 	Batch(2800/6809) done. Loss: 0.1561  lr:0.000001
[ Wed Jul 17 06:06:32 2024 ] 	Batch(2900/6809) done. Loss: 0.0141  lr:0.000001
[ Wed Jul 17 06:06:54 2024 ] 
Training: Epoch [145/150], Step [2999], Loss: 0.6052305102348328, Training Accuracy: 95.56666666666666
[ Wed Jul 17 06:06:54 2024 ] 	Batch(3000/6809) done. Loss: 0.0169  lr:0.000001
[ Wed Jul 17 06:07:17 2024 ] 	Batch(3100/6809) done. Loss: 0.1713  lr:0.000001
[ Wed Jul 17 06:07:40 2024 ] 	Batch(3200/6809) done. Loss: 0.0187  lr:0.000001
[ Wed Jul 17 06:08:02 2024 ] 	Batch(3300/6809) done. Loss: 0.1516  lr:0.000001
[ Wed Jul 17 06:08:25 2024 ] 	Batch(3400/6809) done. Loss: 0.0844  lr:0.000001
[ Wed Jul 17 06:08:47 2024 ] 
Training: Epoch [145/150], Step [3499], Loss: 0.14279042184352875, Training Accuracy: 95.54285714285714
[ Wed Jul 17 06:08:48 2024 ] 	Batch(3500/6809) done. Loss: 0.0602  lr:0.000001
[ Wed Jul 17 06:09:11 2024 ] 	Batch(3600/6809) done. Loss: 0.3813  lr:0.000001
[ Wed Jul 17 06:09:33 2024 ] 	Batch(3700/6809) done. Loss: 0.1162  lr:0.000001
[ Wed Jul 17 06:09:56 2024 ] 	Batch(3800/6809) done. Loss: 0.0988  lr:0.000001
[ Wed Jul 17 06:10:19 2024 ] 	Batch(3900/6809) done. Loss: 0.0153  lr:0.000001
[ Wed Jul 17 06:10:42 2024 ] 
Training: Epoch [145/150], Step [3999], Loss: 0.024412833154201508, Training Accuracy: 95.596875
[ Wed Jul 17 06:10:42 2024 ] 	Batch(4000/6809) done. Loss: 0.2098  lr:0.000001
[ Wed Jul 17 06:11:05 2024 ] 	Batch(4100/6809) done. Loss: 0.5117  lr:0.000001
[ Wed Jul 17 06:11:28 2024 ] 	Batch(4200/6809) done. Loss: 0.1032  lr:0.000001
[ Wed Jul 17 06:11:51 2024 ] 	Batch(4300/6809) done. Loss: 0.0244  lr:0.000001
[ Wed Jul 17 06:12:14 2024 ] 	Batch(4400/6809) done. Loss: 0.2624  lr:0.000001
[ Wed Jul 17 06:12:36 2024 ] 
Training: Epoch [145/150], Step [4499], Loss: 0.04916747286915779, Training Accuracy: 95.58333333333333
[ Wed Jul 17 06:12:36 2024 ] 	Batch(4500/6809) done. Loss: 0.0505  lr:0.000001
[ Wed Jul 17 06:12:59 2024 ] 	Batch(4600/6809) done. Loss: 0.2995  lr:0.000001
[ Wed Jul 17 06:13:21 2024 ] 	Batch(4700/6809) done. Loss: 0.1453  lr:0.000001
[ Wed Jul 17 06:13:44 2024 ] 	Batch(4800/6809) done. Loss: 0.2086  lr:0.000001
[ Wed Jul 17 06:14:07 2024 ] 	Batch(4900/6809) done. Loss: 0.0671  lr:0.000001
[ Wed Jul 17 06:14:30 2024 ] 
Training: Epoch [145/150], Step [4999], Loss: 0.11107594519853592, Training Accuracy: 95.6225
[ Wed Jul 17 06:14:30 2024 ] 	Batch(5000/6809) done. Loss: 0.0731  lr:0.000001
[ Wed Jul 17 06:14:53 2024 ] 	Batch(5100/6809) done. Loss: 0.1223  lr:0.000001
[ Wed Jul 17 06:15:16 2024 ] 	Batch(5200/6809) done. Loss: 0.0906  lr:0.000001
[ Wed Jul 17 06:15:39 2024 ] 	Batch(5300/6809) done. Loss: 0.1442  lr:0.000001
[ Wed Jul 17 06:16:02 2024 ] 	Batch(5400/6809) done. Loss: 0.0216  lr:0.000001
[ Wed Jul 17 06:16:25 2024 ] 
Training: Epoch [145/150], Step [5499], Loss: 0.17150120437145233, Training Accuracy: 95.65454545454546
[ Wed Jul 17 06:16:25 2024 ] 	Batch(5500/6809) done. Loss: 0.1661  lr:0.000001
[ Wed Jul 17 06:16:48 2024 ] 	Batch(5600/6809) done. Loss: 0.1597  lr:0.000001
[ Wed Jul 17 06:17:11 2024 ] 	Batch(5700/6809) done. Loss: 0.0303  lr:0.000001
[ Wed Jul 17 06:17:34 2024 ] 	Batch(5800/6809) done. Loss: 0.0838  lr:0.000001
[ Wed Jul 17 06:17:57 2024 ] 	Batch(5900/6809) done. Loss: 0.4377  lr:0.000001
[ Wed Jul 17 06:18:20 2024 ] 
Training: Epoch [145/150], Step [5999], Loss: 0.28184422850608826, Training Accuracy: 95.68541666666667
[ Wed Jul 17 06:18:20 2024 ] 	Batch(6000/6809) done. Loss: 0.0488  lr:0.000001
[ Wed Jul 17 06:18:43 2024 ] 	Batch(6100/6809) done. Loss: 0.0442  lr:0.000001
[ Wed Jul 17 06:19:06 2024 ] 	Batch(6200/6809) done. Loss: 0.0108  lr:0.000001
[ Wed Jul 17 06:19:29 2024 ] 	Batch(6300/6809) done. Loss: 0.2216  lr:0.000001
[ Wed Jul 17 06:19:52 2024 ] 	Batch(6400/6809) done. Loss: 0.0649  lr:0.000001
[ Wed Jul 17 06:20:15 2024 ] 
Training: Epoch [145/150], Step [6499], Loss: 0.07978790253400803, Training Accuracy: 95.70192307692308
[ Wed Jul 17 06:20:15 2024 ] 	Batch(6500/6809) done. Loss: 0.0157  lr:0.000001
[ Wed Jul 17 06:20:38 2024 ] 	Batch(6600/6809) done. Loss: 0.1462  lr:0.000001
[ Wed Jul 17 06:21:01 2024 ] 	Batch(6700/6809) done. Loss: 0.0705  lr:0.000001
[ Wed Jul 17 06:21:24 2024 ] 	Batch(6800/6809) done. Loss: 0.0146  lr:0.000001
[ Wed Jul 17 06:21:26 2024 ] 	Mean training loss: 0.1555.
[ Wed Jul 17 06:21:26 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 06:21:26 2024 ] Training epoch: 147
[ Wed Jul 17 06:21:27 2024 ] 	Batch(0/6809) done. Loss: 0.1087  lr:0.000001
[ Wed Jul 17 06:21:50 2024 ] 	Batch(100/6809) done. Loss: 0.3226  lr:0.000001
[ Wed Jul 17 06:22:13 2024 ] 	Batch(200/6809) done. Loss: 0.0425  lr:0.000001
[ Wed Jul 17 06:22:36 2024 ] 	Batch(300/6809) done. Loss: 0.0635  lr:0.000001
[ Wed Jul 17 06:22:59 2024 ] 	Batch(400/6809) done. Loss: 0.0083  lr:0.000001
[ Wed Jul 17 06:23:22 2024 ] 
Training: Epoch [146/150], Step [499], Loss: 0.07399720698595047, Training Accuracy: 95.775
[ Wed Jul 17 06:23:23 2024 ] 	Batch(500/6809) done. Loss: 0.0264  lr:0.000001
[ Wed Jul 17 06:23:46 2024 ] 	Batch(600/6809) done. Loss: 0.0397  lr:0.000001
[ Wed Jul 17 06:24:09 2024 ] 	Batch(700/6809) done. Loss: 0.4681  lr:0.000001
[ Wed Jul 17 06:24:33 2024 ] 	Batch(800/6809) done. Loss: 0.0279  lr:0.000001
[ Wed Jul 17 06:24:56 2024 ] 	Batch(900/6809) done. Loss: 0.0209  lr:0.000001
[ Wed Jul 17 06:25:19 2024 ] 
Training: Epoch [146/150], Step [999], Loss: 0.1532173603773117, Training Accuracy: 95.75
[ Wed Jul 17 06:25:19 2024 ] 	Batch(1000/6809) done. Loss: 0.0128  lr:0.000001
[ Wed Jul 17 06:25:43 2024 ] 	Batch(1100/6809) done. Loss: 0.0429  lr:0.000001
[ Wed Jul 17 06:26:07 2024 ] 	Batch(1200/6809) done. Loss: 0.2753  lr:0.000001
[ Wed Jul 17 06:26:30 2024 ] 	Batch(1300/6809) done. Loss: 0.1100  lr:0.000001
[ Wed Jul 17 06:26:52 2024 ] 	Batch(1400/6809) done. Loss: 0.5394  lr:0.000001
[ Wed Jul 17 06:27:15 2024 ] 
Training: Epoch [146/150], Step [1499], Loss: 0.014862420037388802, Training Accuracy: 95.525
[ Wed Jul 17 06:27:15 2024 ] 	Batch(1500/6809) done. Loss: 0.0349  lr:0.000001
[ Wed Jul 17 06:27:39 2024 ] 	Batch(1600/6809) done. Loss: 0.2653  lr:0.000001
[ Wed Jul 17 06:28:01 2024 ] 	Batch(1700/6809) done. Loss: 0.0648  lr:0.000001
[ Wed Jul 17 06:28:24 2024 ] 	Batch(1800/6809) done. Loss: 0.5060  lr:0.000001
[ Wed Jul 17 06:28:46 2024 ] 	Batch(1900/6809) done. Loss: 0.0841  lr:0.000001
[ Wed Jul 17 06:29:09 2024 ] 
Training: Epoch [146/150], Step [1999], Loss: 0.053107019513845444, Training Accuracy: 95.5875
[ Wed Jul 17 06:29:09 2024 ] 	Batch(2000/6809) done. Loss: 0.1726  lr:0.000001
[ Wed Jul 17 06:29:32 2024 ] 	Batch(2100/6809) done. Loss: 0.0312  lr:0.000001
[ Wed Jul 17 06:29:54 2024 ] 	Batch(2200/6809) done. Loss: 0.0390  lr:0.000001
[ Wed Jul 17 06:30:17 2024 ] 	Batch(2300/6809) done. Loss: 0.4183  lr:0.000001
[ Wed Jul 17 06:30:39 2024 ] 	Batch(2400/6809) done. Loss: 0.2249  lr:0.000001
[ Wed Jul 17 06:31:02 2024 ] 
Training: Epoch [146/150], Step [2499], Loss: 0.23367366194725037, Training Accuracy: 95.59
[ Wed Jul 17 06:31:02 2024 ] 	Batch(2500/6809) done. Loss: 0.8320  lr:0.000001
[ Wed Jul 17 06:31:25 2024 ] 	Batch(2600/6809) done. Loss: 0.3103  lr:0.000001
[ Wed Jul 17 06:31:47 2024 ] 	Batch(2700/6809) done. Loss: 0.0317  lr:0.000001
[ Wed Jul 17 06:32:10 2024 ] 	Batch(2800/6809) done. Loss: 0.0601  lr:0.000001
[ Wed Jul 17 06:32:33 2024 ] 	Batch(2900/6809) done. Loss: 0.4624  lr:0.000001
[ Wed Jul 17 06:32:55 2024 ] 
Training: Epoch [146/150], Step [2999], Loss: 0.017966771498322487, Training Accuracy: 95.64166666666667
[ Wed Jul 17 06:32:55 2024 ] 	Batch(3000/6809) done. Loss: 0.0393  lr:0.000001
[ Wed Jul 17 06:33:18 2024 ] 	Batch(3100/6809) done. Loss: 0.0246  lr:0.000001
[ Wed Jul 17 06:33:41 2024 ] 	Batch(3200/6809) done. Loss: 0.0572  lr:0.000001
[ Wed Jul 17 06:34:04 2024 ] 	Batch(3300/6809) done. Loss: 0.0150  lr:0.000001
[ Wed Jul 17 06:34:27 2024 ] 	Batch(3400/6809) done. Loss: 0.3805  lr:0.000001
[ Wed Jul 17 06:34:50 2024 ] 
Training: Epoch [146/150], Step [3499], Loss: 0.06916346400976181, Training Accuracy: 95.61071428571428
[ Wed Jul 17 06:34:51 2024 ] 	Batch(3500/6809) done. Loss: 0.1576  lr:0.000001
[ Wed Jul 17 06:35:14 2024 ] 	Batch(3600/6809) done. Loss: 0.0472  lr:0.000001
[ Wed Jul 17 06:35:36 2024 ] 	Batch(3700/6809) done. Loss: 0.0402  lr:0.000001
[ Wed Jul 17 06:35:59 2024 ] 	Batch(3800/6809) done. Loss: 0.0126  lr:0.000001
[ Wed Jul 17 06:36:21 2024 ] 	Batch(3900/6809) done. Loss: 0.0348  lr:0.000001
[ Wed Jul 17 06:36:44 2024 ] 
Training: Epoch [146/150], Step [3999], Loss: 0.09045626223087311, Training Accuracy: 95.625
[ Wed Jul 17 06:36:44 2024 ] 	Batch(4000/6809) done. Loss: 0.3041  lr:0.000001
[ Wed Jul 17 06:37:07 2024 ] 	Batch(4100/6809) done. Loss: 0.0193  lr:0.000001
[ Wed Jul 17 06:37:31 2024 ] 	Batch(4200/6809) done. Loss: 0.5980  lr:0.000001
[ Wed Jul 17 06:37:53 2024 ] 	Batch(4300/6809) done. Loss: 0.0621  lr:0.000001
[ Wed Jul 17 06:38:16 2024 ] 	Batch(4400/6809) done. Loss: 0.3471  lr:0.000001
[ Wed Jul 17 06:38:38 2024 ] 
Training: Epoch [146/150], Step [4499], Loss: 0.04317646101117134, Training Accuracy: 95.66944444444444
[ Wed Jul 17 06:38:39 2024 ] 	Batch(4500/6809) done. Loss: 0.0621  lr:0.000001
[ Wed Jul 17 06:39:01 2024 ] 	Batch(4600/6809) done. Loss: 0.0376  lr:0.000001
[ Wed Jul 17 06:39:24 2024 ] 	Batch(4700/6809) done. Loss: 0.1584  lr:0.000001
[ Wed Jul 17 06:39:46 2024 ] 	Batch(4800/6809) done. Loss: 0.1838  lr:0.000001
[ Wed Jul 17 06:40:09 2024 ] 	Batch(4900/6809) done. Loss: 0.3951  lr:0.000001
[ Wed Jul 17 06:40:31 2024 ] 
Training: Epoch [146/150], Step [4999], Loss: 0.03870149329304695, Training Accuracy: 95.675
[ Wed Jul 17 06:40:32 2024 ] 	Batch(5000/6809) done. Loss: 0.0218  lr:0.000001
[ Wed Jul 17 06:40:54 2024 ] 	Batch(5100/6809) done. Loss: 0.1031  lr:0.000001
[ Wed Jul 17 06:41:18 2024 ] 	Batch(5200/6809) done. Loss: 0.0668  lr:0.000001
[ Wed Jul 17 06:41:40 2024 ] 	Batch(5300/6809) done. Loss: 0.0201  lr:0.000001
[ Wed Jul 17 06:42:03 2024 ] 	Batch(5400/6809) done. Loss: 0.1751  lr:0.000001
[ Wed Jul 17 06:42:25 2024 ] 
Training: Epoch [146/150], Step [5499], Loss: 0.013995652087032795, Training Accuracy: 95.71136363636363
[ Wed Jul 17 06:42:26 2024 ] 	Batch(5500/6809) done. Loss: 0.2286  lr:0.000001
[ Wed Jul 17 06:42:48 2024 ] 	Batch(5600/6809) done. Loss: 0.1944  lr:0.000001
[ Wed Jul 17 06:43:11 2024 ] 	Batch(5700/6809) done. Loss: 0.0910  lr:0.000001
[ Wed Jul 17 06:43:33 2024 ] 	Batch(5800/6809) done. Loss: 0.0341  lr:0.000001
[ Wed Jul 17 06:43:56 2024 ] 	Batch(5900/6809) done. Loss: 0.2280  lr:0.000001
[ Wed Jul 17 06:44:18 2024 ] 
Training: Epoch [146/150], Step [5999], Loss: 0.005634327419102192, Training Accuracy: 95.70833333333333
[ Wed Jul 17 06:44:19 2024 ] 	Batch(6000/6809) done. Loss: 0.0024  lr:0.000001
[ Wed Jul 17 06:44:41 2024 ] 	Batch(6100/6809) done. Loss: 0.5129  lr:0.000001
[ Wed Jul 17 06:45:04 2024 ] 	Batch(6200/6809) done. Loss: 0.1058  lr:0.000001
[ Wed Jul 17 06:45:26 2024 ] 	Batch(6300/6809) done. Loss: 0.2864  lr:0.000001
[ Wed Jul 17 06:45:49 2024 ] 	Batch(6400/6809) done. Loss: 0.0209  lr:0.000001
[ Wed Jul 17 06:46:12 2024 ] 
Training: Epoch [146/150], Step [6499], Loss: 0.4028327167034149, Training Accuracy: 95.69807692307693
[ Wed Jul 17 06:46:12 2024 ] 	Batch(6500/6809) done. Loss: 0.0129  lr:0.000001
[ Wed Jul 17 06:46:36 2024 ] 	Batch(6600/6809) done. Loss: 0.0258  lr:0.000001
[ Wed Jul 17 06:46:59 2024 ] 	Batch(6700/6809) done. Loss: 0.0302  lr:0.000001
[ Wed Jul 17 06:47:23 2024 ] 	Batch(6800/6809) done. Loss: 0.0909  lr:0.000001
[ Wed Jul 17 06:47:25 2024 ] 	Mean training loss: 0.1521.
[ Wed Jul 17 06:47:25 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 06:47:25 2024 ] Training epoch: 148
[ Wed Jul 17 06:47:25 2024 ] 	Batch(0/6809) done. Loss: 0.0045  lr:0.000001
[ Wed Jul 17 06:47:48 2024 ] 	Batch(100/6809) done. Loss: 0.1414  lr:0.000001
[ Wed Jul 17 06:48:11 2024 ] 	Batch(200/6809) done. Loss: 0.0106  lr:0.000001
[ Wed Jul 17 06:48:34 2024 ] 	Batch(300/6809) done. Loss: 0.0779  lr:0.000001
[ Wed Jul 17 06:48:58 2024 ] 	Batch(400/6809) done. Loss: 0.3736  lr:0.000001
[ Wed Jul 17 06:49:21 2024 ] 
Training: Epoch [147/150], Step [499], Loss: 0.010550688952207565, Training Accuracy: 95.675
[ Wed Jul 17 06:49:21 2024 ] 	Batch(500/6809) done. Loss: 0.0039  lr:0.000001
[ Wed Jul 17 06:49:44 2024 ] 	Batch(600/6809) done. Loss: 0.0360  lr:0.000001
[ Wed Jul 17 06:50:07 2024 ] 	Batch(700/6809) done. Loss: 0.1205  lr:0.000001
[ Wed Jul 17 06:50:30 2024 ] 	Batch(800/6809) done. Loss: 0.0149  lr:0.000001
[ Wed Jul 17 06:50:53 2024 ] 	Batch(900/6809) done. Loss: 0.1126  lr:0.000001
[ Wed Jul 17 06:51:15 2024 ] 
Training: Epoch [147/150], Step [999], Loss: 0.13725358247756958, Training Accuracy: 95.2375
[ Wed Jul 17 06:51:16 2024 ] 	Batch(1000/6809) done. Loss: 0.5984  lr:0.000001
[ Wed Jul 17 06:51:39 2024 ] 	Batch(1100/6809) done. Loss: 0.0184  lr:0.000001
[ Wed Jul 17 06:52:02 2024 ] 	Batch(1200/6809) done. Loss: 0.7725  lr:0.000001
[ Wed Jul 17 06:52:25 2024 ] 	Batch(1300/6809) done. Loss: 0.0443  lr:0.000001
[ Wed Jul 17 06:52:48 2024 ] 	Batch(1400/6809) done. Loss: 0.0201  lr:0.000001
[ Wed Jul 17 06:53:10 2024 ] 
Training: Epoch [147/150], Step [1499], Loss: 0.005150491837412119, Training Accuracy: 95.50833333333333
[ Wed Jul 17 06:53:11 2024 ] 	Batch(1500/6809) done. Loss: 0.2735  lr:0.000001
[ Wed Jul 17 06:53:34 2024 ] 	Batch(1600/6809) done. Loss: 0.0779  lr:0.000001
[ Wed Jul 17 06:53:57 2024 ] 	Batch(1700/6809) done. Loss: 0.2703  lr:0.000001
[ Wed Jul 17 06:54:19 2024 ] 	Batch(1800/6809) done. Loss: 0.0920  lr:0.000001
[ Wed Jul 17 06:54:42 2024 ] 	Batch(1900/6809) done. Loss: 0.0412  lr:0.000001
[ Wed Jul 17 06:55:04 2024 ] 
Training: Epoch [147/150], Step [1999], Loss: 0.08991064876317978, Training Accuracy: 95.625
[ Wed Jul 17 06:55:05 2024 ] 	Batch(2000/6809) done. Loss: 0.3986  lr:0.000001
[ Wed Jul 17 06:55:27 2024 ] 	Batch(2100/6809) done. Loss: 0.1654  lr:0.000001
[ Wed Jul 17 06:55:50 2024 ] 	Batch(2200/6809) done. Loss: 0.0111  lr:0.000001
[ Wed Jul 17 06:56:12 2024 ] 	Batch(2300/6809) done. Loss: 0.2313  lr:0.000001
[ Wed Jul 17 06:56:35 2024 ] 	Batch(2400/6809) done. Loss: 0.4323  lr:0.000001
[ Wed Jul 17 06:56:57 2024 ] 
Training: Epoch [147/150], Step [2499], Loss: 0.06080496683716774, Training Accuracy: 95.78
[ Wed Jul 17 06:56:57 2024 ] 	Batch(2500/6809) done. Loss: 0.0660  lr:0.000001
[ Wed Jul 17 06:57:20 2024 ] 	Batch(2600/6809) done. Loss: 0.2457  lr:0.000001
[ Wed Jul 17 06:57:42 2024 ] 	Batch(2700/6809) done. Loss: 0.1877  lr:0.000001
[ Wed Jul 17 06:58:05 2024 ] 	Batch(2800/6809) done. Loss: 0.0400  lr:0.000001
[ Wed Jul 17 06:58:29 2024 ] 	Batch(2900/6809) done. Loss: 0.3281  lr:0.000001
[ Wed Jul 17 06:58:51 2024 ] 
Training: Epoch [147/150], Step [2999], Loss: 0.04157007485628128, Training Accuracy: 95.775
[ Wed Jul 17 06:58:51 2024 ] 	Batch(3000/6809) done. Loss: 0.2082  lr:0.000001
[ Wed Jul 17 06:59:14 2024 ] 	Batch(3100/6809) done. Loss: 0.1422  lr:0.000001
[ Wed Jul 17 06:59:37 2024 ] 	Batch(3200/6809) done. Loss: 0.3354  lr:0.000001
[ Wed Jul 17 06:59:59 2024 ] 	Batch(3300/6809) done. Loss: 0.0062  lr:0.000001
[ Wed Jul 17 07:00:22 2024 ] 	Batch(3400/6809) done. Loss: 0.1646  lr:0.000001
[ Wed Jul 17 07:00:44 2024 ] 
Training: Epoch [147/150], Step [3499], Loss: 0.3537955582141876, Training Accuracy: 95.79285714285714
[ Wed Jul 17 07:00:44 2024 ] 	Batch(3500/6809) done. Loss: 0.0060  lr:0.000001
[ Wed Jul 17 07:01:07 2024 ] 	Batch(3600/6809) done. Loss: 0.0667  lr:0.000001
[ Wed Jul 17 07:01:29 2024 ] 	Batch(3700/6809) done. Loss: 0.2549  lr:0.000001
[ Wed Jul 17 07:01:52 2024 ] 	Batch(3800/6809) done. Loss: 0.0059  lr:0.000001
[ Wed Jul 17 07:02:14 2024 ] 	Batch(3900/6809) done. Loss: 0.0221  lr:0.000001
[ Wed Jul 17 07:02:37 2024 ] 
Training: Epoch [147/150], Step [3999], Loss: 0.027532408013939857, Training Accuracy: 95.72812499999999
[ Wed Jul 17 07:02:37 2024 ] 	Batch(4000/6809) done. Loss: 0.0318  lr:0.000001
[ Wed Jul 17 07:03:00 2024 ] 	Batch(4100/6809) done. Loss: 0.1201  lr:0.000001
[ Wed Jul 17 07:03:22 2024 ] 	Batch(4200/6809) done. Loss: 0.1377  lr:0.000001
[ Wed Jul 17 07:03:45 2024 ] 	Batch(4300/6809) done. Loss: 0.1318  lr:0.000001
[ Wed Jul 17 07:04:07 2024 ] 	Batch(4400/6809) done. Loss: 0.0495  lr:0.000001
[ Wed Jul 17 07:04:30 2024 ] 
Training: Epoch [147/150], Step [4499], Loss: 0.011553009040653706, Training Accuracy: 95.75277777777778
[ Wed Jul 17 07:04:30 2024 ] 	Batch(4500/6809) done. Loss: 0.0411  lr:0.000001
[ Wed Jul 17 07:04:54 2024 ] 	Batch(4600/6809) done. Loss: 0.3946  lr:0.000001
[ Wed Jul 17 07:05:17 2024 ] 	Batch(4700/6809) done. Loss: 0.0596  lr:0.000001
[ Wed Jul 17 07:05:40 2024 ] 	Batch(4800/6809) done. Loss: 0.0553  lr:0.000001
[ Wed Jul 17 07:06:03 2024 ] 	Batch(4900/6809) done. Loss: 0.1548  lr:0.000001
[ Wed Jul 17 07:06:26 2024 ] 
Training: Epoch [147/150], Step [4999], Loss: 0.15731850266456604, Training Accuracy: 95.7375
[ Wed Jul 17 07:06:27 2024 ] 	Batch(5000/6809) done. Loss: 0.0388  lr:0.000001
[ Wed Jul 17 07:06:50 2024 ] 	Batch(5100/6809) done. Loss: 0.1051  lr:0.000001
[ Wed Jul 17 07:07:13 2024 ] 	Batch(5200/6809) done. Loss: 0.1438  lr:0.000001
[ Wed Jul 17 07:07:36 2024 ] 	Batch(5300/6809) done. Loss: 0.0609  lr:0.000001
[ Wed Jul 17 07:07:59 2024 ] 	Batch(5400/6809) done. Loss: 0.0929  lr:0.000001
[ Wed Jul 17 07:08:22 2024 ] 
Training: Epoch [147/150], Step [5499], Loss: 0.2796573340892792, Training Accuracy: 95.7590909090909
[ Wed Jul 17 07:08:22 2024 ] 	Batch(5500/6809) done. Loss: 0.2997  lr:0.000001
[ Wed Jul 17 07:08:46 2024 ] 	Batch(5600/6809) done. Loss: 0.1612  lr:0.000001
[ Wed Jul 17 07:09:09 2024 ] 	Batch(5700/6809) done. Loss: 0.0297  lr:0.000001
[ Wed Jul 17 07:09:32 2024 ] 	Batch(5800/6809) done. Loss: 0.0119  lr:0.000001
[ Wed Jul 17 07:09:55 2024 ] 	Batch(5900/6809) done. Loss: 0.5670  lr:0.000001
[ Wed Jul 17 07:10:17 2024 ] 
Training: Epoch [147/150], Step [5999], Loss: 0.10673896223306656, Training Accuracy: 95.72083333333333
[ Wed Jul 17 07:10:17 2024 ] 	Batch(6000/6809) done. Loss: 0.0154  lr:0.000001
[ Wed Jul 17 07:10:40 2024 ] 	Batch(6100/6809) done. Loss: 0.0295  lr:0.000001
[ Wed Jul 17 07:11:03 2024 ] 	Batch(6200/6809) done. Loss: 0.1429  lr:0.000001
[ Wed Jul 17 07:11:25 2024 ] 	Batch(6300/6809) done. Loss: 0.1541  lr:0.000001
[ Wed Jul 17 07:11:48 2024 ] 	Batch(6400/6809) done. Loss: 0.0100  lr:0.000001
[ Wed Jul 17 07:12:11 2024 ] 
Training: Epoch [147/150], Step [6499], Loss: 0.1449369341135025, Training Accuracy: 95.7
[ Wed Jul 17 07:12:11 2024 ] 	Batch(6500/6809) done. Loss: 0.0604  lr:0.000001
[ Wed Jul 17 07:12:33 2024 ] 	Batch(6600/6809) done. Loss: 0.0680  lr:0.000001
[ Wed Jul 17 07:12:56 2024 ] 	Batch(6700/6809) done. Loss: 0.0503  lr:0.000001
[ Wed Jul 17 07:13:19 2024 ] 	Batch(6800/6809) done. Loss: 0.0933  lr:0.000001
[ Wed Jul 17 07:13:21 2024 ] 	Mean training loss: 0.1527.
[ Wed Jul 17 07:13:21 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 07:13:21 2024 ] Training epoch: 149
[ Wed Jul 17 07:13:22 2024 ] 	Batch(0/6809) done. Loss: 0.0126  lr:0.000001
[ Wed Jul 17 07:13:45 2024 ] 	Batch(100/6809) done. Loss: 0.3455  lr:0.000001
[ Wed Jul 17 07:14:09 2024 ] 	Batch(200/6809) done. Loss: 0.0526  lr:0.000001
[ Wed Jul 17 07:14:32 2024 ] 	Batch(300/6809) done. Loss: 0.2672  lr:0.000001
[ Wed Jul 17 07:14:55 2024 ] 	Batch(400/6809) done. Loss: 0.0431  lr:0.000001
[ Wed Jul 17 07:15:17 2024 ] 
Training: Epoch [148/150], Step [499], Loss: 0.18568091094493866, Training Accuracy: 95.72500000000001
[ Wed Jul 17 07:15:17 2024 ] 	Batch(500/6809) done. Loss: 0.4874  lr:0.000001
[ Wed Jul 17 07:15:40 2024 ] 	Batch(600/6809) done. Loss: 0.0241  lr:0.000001
[ Wed Jul 17 07:16:03 2024 ] 	Batch(700/6809) done. Loss: 0.1520  lr:0.000001
[ Wed Jul 17 07:16:26 2024 ] 	Batch(800/6809) done. Loss: 0.3569  lr:0.000001
[ Wed Jul 17 07:16:49 2024 ] 	Batch(900/6809) done. Loss: 0.1566  lr:0.000001
[ Wed Jul 17 07:17:12 2024 ] 
Training: Epoch [148/150], Step [999], Loss: 0.06741883605718613, Training Accuracy: 95.825
[ Wed Jul 17 07:17:12 2024 ] 	Batch(1000/6809) done. Loss: 0.2373  lr:0.000001
[ Wed Jul 17 07:17:35 2024 ] 	Batch(1100/6809) done. Loss: 0.4795  lr:0.000001
[ Wed Jul 17 07:17:58 2024 ] 	Batch(1200/6809) done. Loss: 0.2408  lr:0.000001
[ Wed Jul 17 07:18:22 2024 ] 	Batch(1300/6809) done. Loss: 0.3295  lr:0.000001
[ Wed Jul 17 07:18:45 2024 ] 	Batch(1400/6809) done. Loss: 0.0144  lr:0.000001
[ Wed Jul 17 07:19:07 2024 ] 
Training: Epoch [148/150], Step [1499], Loss: 0.024817058816552162, Training Accuracy: 95.7
[ Wed Jul 17 07:19:08 2024 ] 	Batch(1500/6809) done. Loss: 0.2962  lr:0.000001
[ Wed Jul 17 07:19:31 2024 ] 	Batch(1600/6809) done. Loss: 0.2075  lr:0.000001
[ Wed Jul 17 07:19:54 2024 ] 	Batch(1700/6809) done. Loss: 0.2598  lr:0.000001
[ Wed Jul 17 07:20:17 2024 ] 	Batch(1800/6809) done. Loss: 0.2449  lr:0.000001
[ Wed Jul 17 07:20:39 2024 ] 	Batch(1900/6809) done. Loss: 0.6171  lr:0.000001
[ Wed Jul 17 07:21:02 2024 ] 
Training: Epoch [148/150], Step [1999], Loss: 0.16725486516952515, Training Accuracy: 95.65625
[ Wed Jul 17 07:21:02 2024 ] 	Batch(2000/6809) done. Loss: 0.1774  lr:0.000001
[ Wed Jul 17 07:21:25 2024 ] 	Batch(2100/6809) done. Loss: 0.2140  lr:0.000001
[ Wed Jul 17 07:21:48 2024 ] 	Batch(2200/6809) done. Loss: 0.0886  lr:0.000001
[ Wed Jul 17 07:22:10 2024 ] 	Batch(2300/6809) done. Loss: 0.0113  lr:0.000001
[ Wed Jul 17 07:22:33 2024 ] 	Batch(2400/6809) done. Loss: 0.0592  lr:0.000001
[ Wed Jul 17 07:22:55 2024 ] 
Training: Epoch [148/150], Step [2499], Loss: 0.38216620683670044, Training Accuracy: 95.72
[ Wed Jul 17 07:22:56 2024 ] 	Batch(2500/6809) done. Loss: 0.0657  lr:0.000001
[ Wed Jul 17 07:23:18 2024 ] 	Batch(2600/6809) done. Loss: 0.1442  lr:0.000001
[ Wed Jul 17 07:23:41 2024 ] 	Batch(2700/6809) done. Loss: 0.1155  lr:0.000001
[ Wed Jul 17 07:24:04 2024 ] 	Batch(2800/6809) done. Loss: 0.0159  lr:0.000001
[ Wed Jul 17 07:24:26 2024 ] 	Batch(2900/6809) done. Loss: 0.2631  lr:0.000001
[ Wed Jul 17 07:24:48 2024 ] 
Training: Epoch [148/150], Step [2999], Loss: 0.26165127754211426, Training Accuracy: 95.67916666666667
[ Wed Jul 17 07:24:49 2024 ] 	Batch(3000/6809) done. Loss: 0.0081  lr:0.000001
[ Wed Jul 17 07:25:11 2024 ] 	Batch(3100/6809) done. Loss: 0.4135  lr:0.000001
[ Wed Jul 17 07:25:34 2024 ] 	Batch(3200/6809) done. Loss: 0.1867  lr:0.000001
[ Wed Jul 17 07:25:56 2024 ] 	Batch(3300/6809) done. Loss: 0.1783  lr:0.000001
[ Wed Jul 17 07:26:19 2024 ] 	Batch(3400/6809) done. Loss: 0.0653  lr:0.000001
[ Wed Jul 17 07:26:41 2024 ] 
Training: Epoch [148/150], Step [3499], Loss: 0.06602409482002258, Training Accuracy: 95.58214285714286
[ Wed Jul 17 07:26:41 2024 ] 	Batch(3500/6809) done. Loss: 0.2259  lr:0.000001
[ Wed Jul 17 07:27:04 2024 ] 	Batch(3600/6809) done. Loss: 0.0926  lr:0.000001
[ Wed Jul 17 07:27:27 2024 ] 	Batch(3700/6809) done. Loss: 0.0456  lr:0.000001
[ Wed Jul 17 07:27:49 2024 ] 	Batch(3800/6809) done. Loss: 0.0351  lr:0.000001
[ Wed Jul 17 07:28:12 2024 ] 	Batch(3900/6809) done. Loss: 0.0403  lr:0.000001
[ Wed Jul 17 07:28:34 2024 ] 
Training: Epoch [148/150], Step [3999], Loss: 0.15593945980072021, Training Accuracy: 95.59062499999999
[ Wed Jul 17 07:28:34 2024 ] 	Batch(4000/6809) done. Loss: 0.4522  lr:0.000001
[ Wed Jul 17 07:28:57 2024 ] 	Batch(4100/6809) done. Loss: 0.0977  lr:0.000001
[ Wed Jul 17 07:29:20 2024 ] 	Batch(4200/6809) done. Loss: 0.1837  lr:0.000001
[ Wed Jul 17 07:29:42 2024 ] 	Batch(4300/6809) done. Loss: 0.0772  lr:0.000001
[ Wed Jul 17 07:30:05 2024 ] 	Batch(4400/6809) done. Loss: 0.1292  lr:0.000001
[ Wed Jul 17 07:30:28 2024 ] 
Training: Epoch [148/150], Step [4499], Loss: 0.03044920414686203, Training Accuracy: 95.58333333333333
[ Wed Jul 17 07:30:28 2024 ] 	Batch(4500/6809) done. Loss: 0.0079  lr:0.000001
[ Wed Jul 17 07:30:51 2024 ] 	Batch(4600/6809) done. Loss: 0.0050  lr:0.000001
[ Wed Jul 17 07:31:14 2024 ] 	Batch(4700/6809) done. Loss: 0.2053  lr:0.000001
[ Wed Jul 17 07:31:37 2024 ] 	Batch(4800/6809) done. Loss: 0.8036  lr:0.000001
[ Wed Jul 17 07:32:00 2024 ] 	Batch(4900/6809) done. Loss: 0.0042  lr:0.000001
[ Wed Jul 17 07:32:22 2024 ] 
Training: Epoch [148/150], Step [4999], Loss: 0.24731674790382385, Training Accuracy: 95.6075
[ Wed Jul 17 07:32:23 2024 ] 	Batch(5000/6809) done. Loss: 0.0499  lr:0.000001
[ Wed Jul 17 07:32:45 2024 ] 	Batch(5100/6809) done. Loss: 0.2823  lr:0.000001
[ Wed Jul 17 07:33:08 2024 ] 	Batch(5200/6809) done. Loss: 0.0937  lr:0.000001
[ Wed Jul 17 07:33:30 2024 ] 	Batch(5300/6809) done. Loss: 0.2043  lr:0.000001
[ Wed Jul 17 07:33:53 2024 ] 	Batch(5400/6809) done. Loss: 0.0960  lr:0.000001
[ Wed Jul 17 07:34:15 2024 ] 
Training: Epoch [148/150], Step [5499], Loss: 0.2627509534358978, Training Accuracy: 95.62272727272727
[ Wed Jul 17 07:34:16 2024 ] 	Batch(5500/6809) done. Loss: 0.8355  lr:0.000001
[ Wed Jul 17 07:34:38 2024 ] 	Batch(5600/6809) done. Loss: 0.0835  lr:0.000001
[ Wed Jul 17 07:35:01 2024 ] 	Batch(5700/6809) done. Loss: 0.0325  lr:0.000001
[ Wed Jul 17 07:35:23 2024 ] 	Batch(5800/6809) done. Loss: 0.1804  lr:0.000001
[ Wed Jul 17 07:35:46 2024 ] 	Batch(5900/6809) done. Loss: 0.1413  lr:0.000001
[ Wed Jul 17 07:36:08 2024 ] 
Training: Epoch [148/150], Step [5999], Loss: 0.5832781791687012, Training Accuracy: 95.62708333333333
[ Wed Jul 17 07:36:09 2024 ] 	Batch(6000/6809) done. Loss: 0.2333  lr:0.000001
[ Wed Jul 17 07:36:31 2024 ] 	Batch(6100/6809) done. Loss: 0.4631  lr:0.000001
[ Wed Jul 17 07:36:54 2024 ] 	Batch(6200/6809) done. Loss: 0.1153  lr:0.000001
[ Wed Jul 17 07:37:16 2024 ] 	Batch(6300/6809) done. Loss: 0.0989  lr:0.000001
[ Wed Jul 17 07:37:39 2024 ] 	Batch(6400/6809) done. Loss: 0.0839  lr:0.000001
[ Wed Jul 17 07:38:01 2024 ] 
Training: Epoch [148/150], Step [6499], Loss: 0.06571297347545624, Training Accuracy: 95.6
[ Wed Jul 17 07:38:01 2024 ] 	Batch(6500/6809) done. Loss: 0.0586  lr:0.000001
[ Wed Jul 17 07:38:24 2024 ] 	Batch(6600/6809) done. Loss: 0.0047  lr:0.000001
[ Wed Jul 17 07:38:46 2024 ] 	Batch(6700/6809) done. Loss: 0.0956  lr:0.000001
[ Wed Jul 17 07:39:09 2024 ] 	Batch(6800/6809) done. Loss: 0.1464  lr:0.000001
[ Wed Jul 17 07:39:11 2024 ] 	Mean training loss: 0.1550.
[ Wed Jul 17 07:39:11 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 07:39:11 2024 ] Training epoch: 150
[ Wed Jul 17 07:39:12 2024 ] 	Batch(0/6809) done. Loss: 0.1958  lr:0.000001
[ Wed Jul 17 07:39:34 2024 ] 	Batch(100/6809) done. Loss: 0.1025  lr:0.000001
[ Wed Jul 17 07:39:57 2024 ] 	Batch(200/6809) done. Loss: 0.0623  lr:0.000001
[ Wed Jul 17 07:40:19 2024 ] 	Batch(300/6809) done. Loss: 0.6615  lr:0.000001
[ Wed Jul 17 07:40:42 2024 ] 	Batch(400/6809) done. Loss: 0.1152  lr:0.000001
[ Wed Jul 17 07:41:04 2024 ] 
Training: Epoch [149/150], Step [499], Loss: 0.018706046044826508, Training Accuracy: 95.8
[ Wed Jul 17 07:41:05 2024 ] 	Batch(500/6809) done. Loss: 0.2392  lr:0.000001
[ Wed Jul 17 07:41:27 2024 ] 	Batch(600/6809) done. Loss: 0.1327  lr:0.000001
[ Wed Jul 17 07:41:50 2024 ] 	Batch(700/6809) done. Loss: 0.0916  lr:0.000001
[ Wed Jul 17 07:42:12 2024 ] 	Batch(800/6809) done. Loss: 0.0631  lr:0.000001
[ Wed Jul 17 07:42:35 2024 ] 	Batch(900/6809) done. Loss: 0.0667  lr:0.000001
[ Wed Jul 17 07:42:57 2024 ] 
Training: Epoch [149/150], Step [999], Loss: 0.09127860516309738, Training Accuracy: 95.85000000000001
[ Wed Jul 17 07:42:58 2024 ] 	Batch(1000/6809) done. Loss: 0.0715  lr:0.000001
[ Wed Jul 17 07:43:20 2024 ] 	Batch(1100/6809) done. Loss: 0.2252  lr:0.000001
[ Wed Jul 17 07:43:43 2024 ] 	Batch(1200/6809) done. Loss: 0.0020  lr:0.000001
[ Wed Jul 17 07:44:05 2024 ] 	Batch(1300/6809) done. Loss: 0.0200  lr:0.000001
[ Wed Jul 17 07:44:28 2024 ] 	Batch(1400/6809) done. Loss: 0.1182  lr:0.000001
[ Wed Jul 17 07:44:50 2024 ] 
Training: Epoch [149/150], Step [1499], Loss: 0.20708389580249786, Training Accuracy: 95.83333333333334
[ Wed Jul 17 07:44:51 2024 ] 	Batch(1500/6809) done. Loss: 0.3573  lr:0.000001
[ Wed Jul 17 07:45:13 2024 ] 	Batch(1600/6809) done. Loss: 0.0034  lr:0.000001
[ Wed Jul 17 07:45:36 2024 ] 	Batch(1700/6809) done. Loss: 0.1649  lr:0.000001
[ Wed Jul 17 07:45:59 2024 ] 	Batch(1800/6809) done. Loss: 0.0955  lr:0.000001
[ Wed Jul 17 07:46:22 2024 ] 	Batch(1900/6809) done. Loss: 0.1938  lr:0.000001
[ Wed Jul 17 07:46:44 2024 ] 
Training: Epoch [149/150], Step [1999], Loss: 0.08625057339668274, Training Accuracy: 95.575
[ Wed Jul 17 07:46:45 2024 ] 	Batch(2000/6809) done. Loss: 0.1982  lr:0.000001
[ Wed Jul 17 07:47:07 2024 ] 	Batch(2100/6809) done. Loss: 0.1955  lr:0.000001
[ Wed Jul 17 07:47:30 2024 ] 	Batch(2200/6809) done. Loss: 0.1219  lr:0.000001
[ Wed Jul 17 07:47:53 2024 ] 	Batch(2300/6809) done. Loss: 0.0778  lr:0.000001
[ Wed Jul 17 07:48:16 2024 ] 	Batch(2400/6809) done. Loss: 0.0784  lr:0.000001
[ Wed Jul 17 07:48:39 2024 ] 
Training: Epoch [149/150], Step [2499], Loss: 0.10348507016897202, Training Accuracy: 95.65
[ Wed Jul 17 07:48:39 2024 ] 	Batch(2500/6809) done. Loss: 0.0277  lr:0.000001
[ Wed Jul 17 07:49:02 2024 ] 	Batch(2600/6809) done. Loss: 0.0701  lr:0.000001
[ Wed Jul 17 07:49:25 2024 ] 	Batch(2700/6809) done. Loss: 0.3892  lr:0.000001
[ Wed Jul 17 07:49:48 2024 ] 	Batch(2800/6809) done. Loss: 0.1557  lr:0.000001
[ Wed Jul 17 07:50:10 2024 ] 	Batch(2900/6809) done. Loss: 0.0322  lr:0.000001
[ Wed Jul 17 07:50:33 2024 ] 
Training: Epoch [149/150], Step [2999], Loss: 0.1176525130867958, Training Accuracy: 95.61666666666667
[ Wed Jul 17 07:50:33 2024 ] 	Batch(3000/6809) done. Loss: 0.6487  lr:0.000001
[ Wed Jul 17 07:50:56 2024 ] 	Batch(3100/6809) done. Loss: 0.0644  lr:0.000001
[ Wed Jul 17 07:51:19 2024 ] 	Batch(3200/6809) done. Loss: 0.3591  lr:0.000001
[ Wed Jul 17 07:51:41 2024 ] 	Batch(3300/6809) done. Loss: 0.1409  lr:0.000001
[ Wed Jul 17 07:52:04 2024 ] 	Batch(3400/6809) done. Loss: 0.5014  lr:0.000001
[ Wed Jul 17 07:52:27 2024 ] 
Training: Epoch [149/150], Step [3499], Loss: 0.02843647263944149, Training Accuracy: 95.67142857142858
[ Wed Jul 17 07:52:27 2024 ] 	Batch(3500/6809) done. Loss: 0.1381  lr:0.000001
[ Wed Jul 17 07:52:50 2024 ] 	Batch(3600/6809) done. Loss: 0.4382  lr:0.000001
[ Wed Jul 17 07:53:13 2024 ] 	Batch(3700/6809) done. Loss: 0.0501  lr:0.000001
[ Wed Jul 17 07:53:35 2024 ] 	Batch(3800/6809) done. Loss: 0.1688  lr:0.000001
[ Wed Jul 17 07:53:58 2024 ] 	Batch(3900/6809) done. Loss: 0.3681  lr:0.000001
[ Wed Jul 17 07:54:21 2024 ] 
Training: Epoch [149/150], Step [3999], Loss: 0.4260980486869812, Training Accuracy: 95.69375
[ Wed Jul 17 07:54:21 2024 ] 	Batch(4000/6809) done. Loss: 0.3033  lr:0.000001
[ Wed Jul 17 07:54:44 2024 ] 	Batch(4100/6809) done. Loss: 0.2702  lr:0.000001
[ Wed Jul 17 07:55:06 2024 ] 	Batch(4200/6809) done. Loss: 0.1094  lr:0.000001
[ Wed Jul 17 07:55:29 2024 ] 	Batch(4300/6809) done. Loss: 0.1271  lr:0.000001
[ Wed Jul 17 07:55:52 2024 ] 	Batch(4400/6809) done. Loss: 0.1036  lr:0.000001
[ Wed Jul 17 07:56:14 2024 ] 
Training: Epoch [149/150], Step [4499], Loss: 0.1400865614414215, Training Accuracy: 95.69166666666666
[ Wed Jul 17 07:56:15 2024 ] 	Batch(4500/6809) done. Loss: 0.1528  lr:0.000001
[ Wed Jul 17 07:56:37 2024 ] 	Batch(4600/6809) done. Loss: 0.2887  lr:0.000001
[ Wed Jul 17 07:57:00 2024 ] 	Batch(4700/6809) done. Loss: 0.0210  lr:0.000001
[ Wed Jul 17 07:57:23 2024 ] 	Batch(4800/6809) done. Loss: 0.0823  lr:0.000001
[ Wed Jul 17 07:57:45 2024 ] 	Batch(4900/6809) done. Loss: 0.1102  lr:0.000001
[ Wed Jul 17 07:58:08 2024 ] 
Training: Epoch [149/150], Step [4999], Loss: 0.023587165400385857, Training Accuracy: 95.67750000000001
[ Wed Jul 17 07:58:08 2024 ] 	Batch(5000/6809) done. Loss: 0.1420  lr:0.000001
[ Wed Jul 17 07:58:31 2024 ] 	Batch(5100/6809) done. Loss: 0.1058  lr:0.000001
[ Wed Jul 17 07:58:54 2024 ] 	Batch(5200/6809) done. Loss: 0.0061  lr:0.000001
[ Wed Jul 17 07:59:16 2024 ] 	Batch(5300/6809) done. Loss: 0.0395  lr:0.000001
[ Wed Jul 17 07:59:40 2024 ] 	Batch(5400/6809) done. Loss: 0.0216  lr:0.000001
[ Wed Jul 17 08:00:02 2024 ] 
Training: Epoch [149/150], Step [5499], Loss: 0.036420173943042755, Training Accuracy: 95.64090909090909
[ Wed Jul 17 08:00:03 2024 ] 	Batch(5500/6809) done. Loss: 0.1010  lr:0.000001
[ Wed Jul 17 08:00:26 2024 ] 	Batch(5600/6809) done. Loss: 0.1179  lr:0.000001
[ Wed Jul 17 08:00:48 2024 ] 	Batch(5700/6809) done. Loss: 0.3408  lr:0.000001
[ Wed Jul 17 08:01:11 2024 ] 	Batch(5800/6809) done. Loss: 0.0688  lr:0.000001
[ Wed Jul 17 08:01:34 2024 ] 	Batch(5900/6809) done. Loss: 0.0055  lr:0.000001
[ Wed Jul 17 08:01:57 2024 ] 
Training: Epoch [149/150], Step [5999], Loss: 0.008434639312326908, Training Accuracy: 95.64583333333333
[ Wed Jul 17 08:01:57 2024 ] 	Batch(6000/6809) done. Loss: 0.3658  lr:0.000001
[ Wed Jul 17 08:02:20 2024 ] 	Batch(6100/6809) done. Loss: 0.2108  lr:0.000001
[ Wed Jul 17 08:02:43 2024 ] 	Batch(6200/6809) done. Loss: 0.2662  lr:0.000001
[ Wed Jul 17 08:03:06 2024 ] 	Batch(6300/6809) done. Loss: 0.0111  lr:0.000001
[ Wed Jul 17 08:03:28 2024 ] 	Batch(6400/6809) done. Loss: 0.0114  lr:0.000001
[ Wed Jul 17 08:03:52 2024 ] 
Training: Epoch [149/150], Step [6499], Loss: 0.10543288290500641, Training Accuracy: 95.65192307692307
[ Wed Jul 17 08:03:52 2024 ] 	Batch(6500/6809) done. Loss: 0.0361  lr:0.000001
[ Wed Jul 17 08:04:15 2024 ] 	Batch(6600/6809) done. Loss: 0.0110  lr:0.000001
[ Wed Jul 17 08:04:38 2024 ] 	Batch(6700/6809) done. Loss: 0.1405  lr:0.000001
[ Wed Jul 17 08:05:01 2024 ] 	Batch(6800/6809) done. Loss: 0.1599  lr:0.000001
[ Wed Jul 17 08:05:03 2024 ] 	Mean training loss: 0.1488.
[ Wed Jul 17 08:05:03 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Wed Jul 17 08:05:03 2024 ] Eval epoch: 150
[ Wed Jul 17 08:11:58 2024 ] 	Mean val loss of 7435 batches: 0.8821505426299723.
[ Wed Jul 17 08:11:58 2024 ] 
Validation: Epoch [149/150], Samples [47465.0/59477], Loss: 2.674675464630127, Validation Accuracy: 79.80395783243944
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 1 : 396 / 500 = 79 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 2 : 420 / 499 = 84 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 3 : 416 / 500 = 83 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 4 : 427 / 502 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 5 : 430 / 502 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 6 : 410 / 502 = 81 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 7 : 458 / 497 = 92 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 8 : 487 / 498 = 97 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 9 : 386 / 500 = 77 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 10 : 291 / 500 = 58 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 11 : 213 / 498 = 42 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 12 : 404 / 499 = 80 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 13 : 485 / 502 = 96 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 14 : 480 / 504 = 95 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 15 : 363 / 502 = 72 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 16 : 381 / 502 = 75 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 17 : 431 / 504 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 18 : 410 / 504 = 81 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 19 : 422 / 502 = 84 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 20 : 450 / 502 = 89 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 21 : 473 / 503 = 94 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 22 : 437 / 504 = 86 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 23 : 423 / 503 = 84 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 24 : 438 / 504 = 86 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 25 : 476 / 504 = 94 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 26 : 463 / 504 = 91 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 27 : 428 / 501 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 28 : 351 / 502 = 69 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 29 : 315 / 502 = 62 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 30 : 367 / 501 = 73 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 31 : 430 / 504 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 32 : 441 / 503 = 87 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 33 : 400 / 503 = 79 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 34 : 484 / 504 = 96 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 35 : 460 / 503 = 91 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 36 : 383 / 502 = 76 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 37 : 454 / 504 = 90 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 38 : 446 / 504 = 88 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 39 : 459 / 498 = 92 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 40 : 405 / 504 = 80 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 41 : 478 / 503 = 95 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 42 : 460 / 504 = 91 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 43 : 338 / 503 = 67 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 44 : 431 / 504 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 45 : 416 / 504 = 82 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 46 : 399 / 504 = 79 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 47 : 334 / 503 = 66 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 48 : 407 / 503 = 80 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 49 : 408 / 499 = 81 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 50 : 404 / 502 = 80 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 51 : 463 / 503 = 92 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 52 : 460 / 504 = 91 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 53 : 447 / 497 = 89 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 54 : 444 / 480 = 92 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 55 : 437 / 504 = 86 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 56 : 418 / 503 = 83 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 57 : 464 / 504 = 92 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 58 : 482 / 499 = 96 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 59 : 484 / 503 = 96 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 60 : 417 / 479 = 87 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 61 : 414 / 484 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 62 : 399 / 487 = 81 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 63 : 444 / 489 = 90 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 64 : 399 / 488 = 81 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 65 : 427 / 490 = 87 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 66 : 311 / 488 = 63 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 67 : 345 / 490 = 70 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 68 : 295 / 490 = 60 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 69 : 398 / 490 = 81 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 70 : 168 / 490 = 34 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 71 : 275 / 490 = 56 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 72 : 202 / 488 = 41 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 73 : 257 / 486 = 52 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 74 : 282 / 481 = 58 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 75 : 289 / 488 = 59 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 76 : 321 / 489 = 65 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 77 : 330 / 488 = 67 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 78 : 388 / 488 = 79 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 79 : 462 / 490 = 94 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 80 : 391 / 489 = 79 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 81 : 278 / 491 = 56 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 82 : 317 / 491 = 64 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 83 : 238 / 489 = 48 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 84 : 354 / 489 = 72 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 85 : 352 / 489 = 71 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 86 : 422 / 491 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 87 : 427 / 492 = 86 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 88 : 361 / 491 = 73 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 89 : 380 / 492 = 77 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 90 : 283 / 490 = 57 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 91 : 360 / 482 = 74 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 92 : 371 / 490 = 75 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 93 : 323 / 487 = 66 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 94 : 435 / 489 = 88 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 95 : 418 / 490 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 96 : 460 / 491 = 93 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 97 : 458 / 490 = 93 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 98 : 440 / 491 = 89 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 99 : 444 / 491 = 90 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 100 : 443 / 491 = 90 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 101 : 416 / 491 = 84 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 102 : 280 / 492 = 56 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 103 : 387 / 492 = 78 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 104 : 300 / 491 = 61 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 105 : 272 / 491 = 55 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 106 : 302 / 492 = 61 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 107 : 422 / 491 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 108 : 382 / 492 = 77 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 109 : 356 / 490 = 72 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 110 : 422 / 491 = 85 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 111 : 448 / 492 = 91 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 112 : 459 / 492 = 93 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 113 : 445 / 491 = 90 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 114 : 404 / 491 = 82 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 115 : 413 / 492 = 83 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 116 : 426 / 491 = 86 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 117 : 400 / 492 = 81 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 118 : 416 / 490 = 84 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 119 : 450 / 492 = 91 %
[ Wed Jul 17 08:11:58 2024 ] Accuracy of 120 : 420 / 500 = 84 %
[ Wed Jul 17 08:11:58 2024 ] Load weights from ./prova20/epoch149_model.pt.
[ Wed Jul 17 21:02:46 2024 ] Load weights from prova20/epoch110_model.pt.
[ Wed Jul 17 21:02:46 2024 ] Eval epoch: 1
[ Wed Jul 17 21:09:54 2024 ] 	Mean test loss of 7435 batches: 0.8694652053402374.
[ Wed Jul 17 21:09:54 2024 ] 	Class1 Precision: 83.17%, Recall: 83.00%
[ Wed Jul 17 21:09:54 2024 ] 	Class2 Precision: 77.27%, Recall: 78.20%
[ Wed Jul 17 21:09:54 2024 ] 	Class3 Precision: 79.03%, Recall: 84.57%
[ Wed Jul 17 21:09:54 2024 ] 	Class4 Precision: 92.59%, Recall: 85.00%
[ Wed Jul 17 21:09:54 2024 ] 	Class5 Precision: 77.24%, Recall: 85.86%
[ Wed Jul 17 21:09:54 2024 ] 	Class6 Precision: 96.83%, Recall: 85.26%
[ Wed Jul 17 21:09:54 2024 ] 	Class7 Precision: 91.47%, Recall: 85.46%
[ Wed Jul 17 21:09:54 2024 ] 	Class8 Precision: 94.32%, Recall: 93.56%
[ Wed Jul 17 21:09:54 2024 ] 	Class9 Precision: 97.21%, Recall: 97.79%
[ Wed Jul 17 21:09:54 2024 ] 	Class10 Precision: 71.98%, Recall: 78.60%
[ Wed Jul 17 21:09:54 2024 ] 	Class11 Precision: 48.20%, Recall: 64.20%
[ Wed Jul 17 21:09:54 2024 ] 	Class12 Precision: 39.37%, Recall: 45.38%
[ Wed Jul 17 21:09:54 2024 ] 	Class13 Precision: 83.51%, Recall: 80.16%
[ Wed Jul 17 21:09:54 2024 ] 	Class14 Precision: 97.21%, Recall: 97.21%
[ Wed Jul 17 21:09:54 2024 ] 	Class15 Precision: 91.29%, Recall: 95.63%
[ Wed Jul 17 21:09:54 2024 ] 	Class16 Precision: 79.71%, Recall: 75.90%
[ Wed Jul 17 21:09:54 2024 ] 	Class17 Precision: 74.65%, Recall: 75.10%
[ Wed Jul 17 21:09:54 2024 ] 	Class18 Precision: 82.82%, Recall: 85.12%
[ Wed Jul 17 21:09:54 2024 ] 	Class19 Precision: 81.64%, Recall: 82.94%
[ Wed Jul 17 21:09:54 2024 ] 	Class20 Precision: 96.71%, Recall: 87.85%
[ Wed Jul 17 21:09:54 2024 ] 	Class21 Precision: 90.51%, Recall: 91.24%
[ Wed Jul 17 21:09:54 2024 ] 	Class22 Precision: 92.64%, Recall: 92.64%
[ Wed Jul 17 21:09:54 2024 ] 	Class23 Precision: 85.08%, Recall: 87.10%
[ Wed Jul 17 21:09:54 2024 ] 	Class24 Precision: 92.75%, Recall: 86.48%
[ Wed Jul 17 21:09:54 2024 ] 	Class25 Precision: 83.14%, Recall: 86.11%
[ Wed Jul 17 21:09:54 2024 ] 	Class26 Precision: 91.70%, Recall: 96.43%
[ Wed Jul 17 21:09:54 2024 ] 	Class27 Precision: 97.32%, Recall: 93.65%
[ Wed Jul 17 21:09:54 2024 ] 	Class28 Precision: 72.10%, Recall: 85.63%
[ Wed Jul 17 21:09:54 2024 ] 	Class29 Precision: 54.36%, Recall: 70.72%
[ Wed Jul 17 21:09:54 2024 ] 	Class30 Precision: 57.27%, Recall: 61.95%
[ Wed Jul 17 21:09:54 2024 ] 	Class31 Precision: 73.11%, Recall: 75.45%
[ Wed Jul 17 21:09:54 2024 ] 	Class32 Precision: 80.11%, Recall: 87.10%
[ Wed Jul 17 21:09:54 2024 ] 	Class33 Precision: 79.39%, Recall: 87.28%
[ Wed Jul 17 21:09:54 2024 ] 	Class34 Precision: 75.00%, Recall: 79.32%
[ Wed Jul 17 21:09:54 2024 ] 	Class35 Precision: 89.03%, Recall: 96.63%
[ Wed Jul 17 21:09:54 2024 ] 	Class36 Precision: 76.11%, Recall: 91.85%
[ Wed Jul 17 21:09:54 2024 ] 	Class37 Precision: 82.91%, Recall: 77.29%
[ Wed Jul 17 21:09:54 2024 ] 	Class38 Precision: 72.48%, Recall: 89.88%
[ Wed Jul 17 21:09:54 2024 ] 	Class39 Precision: 80.11%, Recall: 87.90%
[ Wed Jul 17 21:09:54 2024 ] 	Class40 Precision: 91.15%, Recall: 90.96%
[ Wed Jul 17 21:09:54 2024 ] 	Class41 Precision: 70.89%, Recall: 78.77%
[ Wed Jul 17 21:09:54 2024 ] 	Class42 Precision: 94.37%, Recall: 93.24%
[ Wed Jul 17 21:09:54 2024 ] 	Class43 Precision: 93.25%, Recall: 90.48%
[ Wed Jul 17 21:09:54 2024 ] 	Class44 Precision: 78.83%, Recall: 72.56%
[ Wed Jul 17 21:09:54 2024 ] 	Class45 Precision: 82.07%, Recall: 83.53%
[ Wed Jul 17 21:09:54 2024 ] 	Class46 Precision: 82.19%, Recall: 83.33%
[ Wed Jul 17 21:09:54 2024 ] 	Class47 Precision: 91.23%, Recall: 76.39%
[ Wed Jul 17 21:09:54 2024 ] 	Class48 Precision: 88.95%, Recall: 68.79%
[ Wed Jul 17 21:09:54 2024 ] 	Class49 Precision: 85.86%, Recall: 83.30%
[ Wed Jul 17 21:09:54 2024 ] 	Class50 Precision: 85.42%, Recall: 82.16%
[ Wed Jul 17 21:09:54 2024 ] 	Class51 Precision: 95.59%, Recall: 82.07%
[ Wed Jul 17 21:09:54 2024 ] 	Class52 Precision: 92.60%, Recall: 92.05%
[ Wed Jul 17 21:09:54 2024 ] 	Class53 Precision: 80.49%, Recall: 91.67%
[ Wed Jul 17 21:09:54 2024 ] 	Class54 Precision: 74.18%, Recall: 90.74%
[ Wed Jul 17 21:09:54 2024 ] 	Class55 Precision: 96.56%, Recall: 93.54%
[ Wed Jul 17 21:09:54 2024 ] 	Class56 Precision: 82.61%, Recall: 86.71%
[ Wed Jul 17 21:09:54 2024 ] 	Class57 Precision: 84.03%, Recall: 83.70%
[ Wed Jul 17 21:09:54 2024 ] 	Class58 Precision: 93.99%, Recall: 93.06%
[ Wed Jul 17 21:09:54 2024 ] 	Class59 Precision: 93.60%, Recall: 96.79%
[ Wed Jul 17 21:09:54 2024 ] 	Class60 Precision: 95.28%, Recall: 96.22%
[ Wed Jul 17 21:09:54 2024 ] 	Class61 Precision: 73.75%, Recall: 86.22%
[ Wed Jul 17 21:09:54 2024 ] 	Class62 Precision: 86.18%, Recall: 82.44%
[ Wed Jul 17 21:09:54 2024 ] 	Class63 Precision: 83.51%, Recall: 82.14%
[ Wed Jul 17 21:09:54 2024 ] 	Class64 Precision: 93.12%, Recall: 91.41%
[ Wed Jul 17 21:09:54 2024 ] 	Class65 Precision: 76.59%, Recall: 83.81%
[ Wed Jul 17 21:09:54 2024 ] 	Class66 Precision: 78.79%, Recall: 87.96%
[ Wed Jul 17 21:09:54 2024 ] 	Class67 Precision: 67.25%, Recall: 62.70%
[ Wed Jul 17 21:09:54 2024 ] 	Class68 Precision: 85.48%, Recall: 73.27%
[ Wed Jul 17 21:09:54 2024 ] 	Class69 Precision: 53.29%, Recall: 62.86%
[ Wed Jul 17 21:09:54 2024 ] 	Class70 Precision: 75.65%, Recall: 77.35%
[ Wed Jul 17 21:09:54 2024 ] 	Class71 Precision: 49.70%, Recall: 33.47%
[ Wed Jul 17 21:09:54 2024 ] 	Class72 Precision: 38.84%, Recall: 56.12%
[ Wed Jul 17 21:09:54 2024 ] 	Class73 Precision: 44.70%, Recall: 39.75%
[ Wed Jul 17 21:09:54 2024 ] 	Class74 Precision: 53.81%, Recall: 53.70%
[ Wed Jul 17 21:09:54 2024 ] 	Class75 Precision: 62.47%, Recall: 56.76%
[ Wed Jul 17 21:09:54 2024 ] 	Class76 Precision: 40.46%, Recall: 60.86%
[ Wed Jul 17 21:09:54 2024 ] 	Class77 Precision: 63.39%, Recall: 65.85%
[ Wed Jul 17 21:09:54 2024 ] 	Class78 Precision: 65.49%, Recall: 64.55%
[ Wed Jul 17 21:09:54 2024 ] 	Class79 Precision: 71.97%, Recall: 77.87%
[ Wed Jul 17 21:09:54 2024 ] 	Class80 Precision: 87.26%, Recall: 93.67%
[ Wed Jul 17 21:09:54 2024 ] 	Class81 Precision: 77.14%, Recall: 79.35%
[ Wed Jul 17 21:09:54 2024 ] 	Class82 Precision: 73.72%, Recall: 58.86%
[ Wed Jul 17 21:09:54 2024 ] 	Class83 Precision: 72.73%, Recall: 63.54%
[ Wed Jul 17 21:09:54 2024 ] 	Class84 Precision: 59.11%, Recall: 51.74%
[ Wed Jul 17 21:09:54 2024 ] 	Class85 Precision: 85.18%, Recall: 74.03%
[ Wed Jul 17 21:09:54 2024 ] 	Class86 Precision: 80.93%, Recall: 67.69%
[ Wed Jul 17 21:09:54 2024 ] 	Class87 Precision: 95.08%, Recall: 86.56%
[ Wed Jul 17 21:09:54 2024 ] 	Class88 Precision: 91.48%, Recall: 85.16%
[ Wed Jul 17 21:09:54 2024 ] 	Class89 Precision: 76.26%, Recall: 73.93%
[ Wed Jul 17 21:09:54 2024 ] 	Class90 Precision: 82.28%, Recall: 76.42%
[ Wed Jul 17 21:09:54 2024 ] 	Class91 Precision: 64.88%, Recall: 59.18%
[ Wed Jul 17 21:09:54 2024 ] 	Class92 Precision: 90.17%, Recall: 76.14%
[ Wed Jul 17 21:09:54 2024 ] 	Class93 Precision: 72.10%, Recall: 74.90%
[ Wed Jul 17 21:09:54 2024 ] 	Class94 Precision: 76.78%, Recall: 75.36%
[ Wed Jul 17 21:09:54 2024 ] 	Class95 Precision: 80.11%, Recall: 89.78%
[ Wed Jul 17 21:09:54 2024 ] 	Class96 Precision: 91.61%, Recall: 82.45%
[ Wed Jul 17 21:09:54 2024 ] 	Class97 Precision: 99.14%, Recall: 93.48%
[ Wed Jul 17 21:09:54 2024 ] 	Class98 Precision: 96.63%, Recall: 93.67%
[ Wed Jul 17 21:09:54 2024 ] 	Class99 Precision: 96.08%, Recall: 89.82%
[ Wed Jul 17 21:09:54 2024 ] 	Class100 Precision: 93.54%, Recall: 91.45%
[ Wed Jul 17 21:09:54 2024 ] 	Class101 Precision: 92.96%, Recall: 91.45%
[ Wed Jul 17 21:09:54 2024 ] 	Class102 Precision: 91.68%, Recall: 85.34%
[ Wed Jul 17 21:09:54 2024 ] 	Class103 Precision: 69.06%, Recall: 58.54%
[ Wed Jul 17 21:09:54 2024 ] 	Class104 Precision: 91.31%, Recall: 79.07%
[ Wed Jul 17 21:09:54 2024 ] 	Class105 Precision: 65.11%, Recall: 59.67%
[ Wed Jul 17 21:09:54 2024 ] 	Class106 Precision: 67.82%, Recall: 55.80%
[ Wed Jul 17 21:09:54 2024 ] 	Class107 Precision: 55.34%, Recall: 65.24%
[ Wed Jul 17 21:09:54 2024 ] 	Class108 Precision: 84.99%, Recall: 85.34%
[ Wed Jul 17 21:09:54 2024 ] 	Class109 Precision: 87.91%, Recall: 75.41%
[ Wed Jul 17 21:09:54 2024 ] 	Class110 Precision: 77.37%, Recall: 73.27%
[ Wed Jul 17 21:09:54 2024 ] 	Class111 Precision: 82.50%, Recall: 84.52%
[ Wed Jul 17 21:09:54 2024 ] 	Class112 Precision: 94.48%, Recall: 90.45%
[ Wed Jul 17 21:09:54 2024 ] 	Class113 Precision: 94.86%, Recall: 93.70%
[ Wed Jul 17 21:09:54 2024 ] 	Class114 Precision: 96.10%, Recall: 90.22%
[ Wed Jul 17 21:09:54 2024 ] 	Class115 Precision: 91.47%, Recall: 80.86%
[ Wed Jul 17 21:09:54 2024 ] 	Class116 Precision: 97.42%, Recall: 84.55%
[ Wed Jul 17 21:09:54 2024 ] 	Class117 Precision: 88.27%, Recall: 84.32%
[ Wed Jul 17 21:09:54 2024 ] 	Class118 Precision: 89.60%, Recall: 82.32%
[ Wed Jul 17 21:09:54 2024 ] 	Class119 Precision: 95.18%, Recall: 84.69%
[ Wed Jul 17 21:09:54 2024 ] 	Class120 Precision: 92.39%, Recall: 91.26%
[ Wed Jul 17 21:09:55 2024 ] 	Class1 Top1: 83.00%
[ Wed Jul 17 21:09:55 2024 ] 	Class2 Top1: 78.20%
[ Wed Jul 17 21:09:55 2024 ] 	Class3 Top1: 84.57%
[ Wed Jul 17 21:09:55 2024 ] 	Class4 Top1: 85.00%
[ Wed Jul 17 21:09:55 2024 ] 	Class5 Top1: 85.86%
[ Wed Jul 17 21:09:55 2024 ] 	Class6 Top1: 85.26%
[ Wed Jul 17 21:09:55 2024 ] 	Class7 Top1: 85.46%
[ Wed Jul 17 21:09:55 2024 ] 	Class8 Top1: 93.56%
[ Wed Jul 17 21:09:55 2024 ] 	Class9 Top1: 97.79%
[ Wed Jul 17 21:09:55 2024 ] 	Class10 Top1: 78.60%
[ Wed Jul 17 21:09:55 2024 ] 	Class11 Top1: 64.20%
[ Wed Jul 17 21:09:55 2024 ] 	Class12 Top1: 45.38%
[ Wed Jul 17 21:09:55 2024 ] 	Class13 Top1: 80.16%
[ Wed Jul 17 21:09:55 2024 ] 	Class14 Top1: 97.21%
[ Wed Jul 17 21:09:55 2024 ] 	Class15 Top1: 95.63%
[ Wed Jul 17 21:09:55 2024 ] 	Class16 Top1: 75.90%
[ Wed Jul 17 21:09:55 2024 ] 	Class17 Top1: 75.10%
[ Wed Jul 17 21:09:55 2024 ] 	Class18 Top1: 85.12%
[ Wed Jul 17 21:09:55 2024 ] 	Class19 Top1: 82.94%
[ Wed Jul 17 21:09:55 2024 ] 	Class20 Top1: 87.85%
[ Wed Jul 17 21:09:55 2024 ] 	Class21 Top1: 91.24%
[ Wed Jul 17 21:09:55 2024 ] 	Class22 Top1: 92.64%
[ Wed Jul 17 21:09:55 2024 ] 	Class23 Top1: 87.10%
[ Wed Jul 17 21:09:55 2024 ] 	Class24 Top1: 86.48%
[ Wed Jul 17 21:09:55 2024 ] 	Class25 Top1: 86.11%
[ Wed Jul 17 21:09:55 2024 ] 	Class26 Top1: 96.43%
[ Wed Jul 17 21:09:55 2024 ] 	Class27 Top1: 93.65%
[ Wed Jul 17 21:09:55 2024 ] 	Class28 Top1: 85.63%
[ Wed Jul 17 21:09:55 2024 ] 	Class29 Top1: 70.72%
[ Wed Jul 17 21:09:55 2024 ] 	Class30 Top1: 61.95%
[ Wed Jul 17 21:09:55 2024 ] 	Class31 Top1: 75.45%
[ Wed Jul 17 21:09:55 2024 ] 	Class32 Top1: 87.10%
[ Wed Jul 17 21:09:55 2024 ] 	Class33 Top1: 87.28%
[ Wed Jul 17 21:09:55 2024 ] 	Class34 Top1: 79.32%
[ Wed Jul 17 21:09:55 2024 ] 	Class35 Top1: 96.63%
[ Wed Jul 17 21:09:55 2024 ] 	Class36 Top1: 91.85%
[ Wed Jul 17 21:09:55 2024 ] 	Class37 Top1: 77.29%
[ Wed Jul 17 21:09:55 2024 ] 	Class38 Top1: 89.88%
[ Wed Jul 17 21:09:55 2024 ] 	Class39 Top1: 87.90%
[ Wed Jul 17 21:09:55 2024 ] 	Class40 Top1: 90.96%
[ Wed Jul 17 21:09:55 2024 ] 	Class41 Top1: 78.77%
[ Wed Jul 17 21:09:55 2024 ] 	Class42 Top1: 93.24%
[ Wed Jul 17 21:09:55 2024 ] 	Class43 Top1: 90.48%
[ Wed Jul 17 21:09:55 2024 ] 	Class44 Top1: 72.56%
[ Wed Jul 17 21:09:55 2024 ] 	Class45 Top1: 83.53%
[ Wed Jul 17 21:09:55 2024 ] 	Class46 Top1: 83.33%
[ Wed Jul 17 21:09:55 2024 ] 	Class47 Top1: 76.39%
[ Wed Jul 17 21:09:55 2024 ] 	Class48 Top1: 68.79%
[ Wed Jul 17 21:09:55 2024 ] 	Class49 Top1: 83.30%
[ Wed Jul 17 21:09:55 2024 ] 	Class50 Top1: 82.16%
[ Wed Jul 17 21:09:55 2024 ] 	Class51 Top1: 82.07%
[ Wed Jul 17 21:09:55 2024 ] 	Class52 Top1: 92.05%
[ Wed Jul 17 21:09:55 2024 ] 	Class53 Top1: 91.67%
[ Wed Jul 17 21:09:55 2024 ] 	Class54 Top1: 90.74%
[ Wed Jul 17 21:09:55 2024 ] 	Class55 Top1: 93.54%
[ Wed Jul 17 21:09:55 2024 ] 	Class56 Top1: 86.71%
[ Wed Jul 17 21:09:55 2024 ] 	Class57 Top1: 83.70%
[ Wed Jul 17 21:09:55 2024 ] 	Class58 Top1: 93.06%
[ Wed Jul 17 21:09:55 2024 ] 	Class59 Top1: 96.79%
[ Wed Jul 17 21:09:55 2024 ] 	Class60 Top1: 96.22%
[ Wed Jul 17 21:09:55 2024 ] 	Class61 Top1: 86.22%
[ Wed Jul 17 21:09:55 2024 ] 	Class62 Top1: 82.44%
[ Wed Jul 17 21:09:55 2024 ] 	Class63 Top1: 82.14%
[ Wed Jul 17 21:09:55 2024 ] 	Class64 Top1: 91.41%
[ Wed Jul 17 21:09:55 2024 ] 	Class65 Top1: 83.81%
[ Wed Jul 17 21:09:55 2024 ] 	Class66 Top1: 87.96%
[ Wed Jul 17 21:09:55 2024 ] 	Class67 Top1: 62.70%
[ Wed Jul 17 21:09:55 2024 ] 	Class68 Top1: 73.27%
[ Wed Jul 17 21:09:55 2024 ] 	Class69 Top1: 62.86%
[ Wed Jul 17 21:09:55 2024 ] 	Class70 Top1: 77.35%
[ Wed Jul 17 21:09:55 2024 ] 	Class71 Top1: 33.47%
[ Wed Jul 17 21:09:55 2024 ] 	Class72 Top1: 56.12%
[ Wed Jul 17 21:09:55 2024 ] 	Class73 Top1: 39.75%
[ Wed Jul 17 21:09:55 2024 ] 	Class74 Top1: 53.70%
[ Wed Jul 17 21:09:55 2024 ] 	Class75 Top1: 56.76%
[ Wed Jul 17 21:09:55 2024 ] 	Class76 Top1: 60.86%
[ Wed Jul 17 21:09:55 2024 ] 	Class77 Top1: 65.85%
[ Wed Jul 17 21:09:55 2024 ] 	Class78 Top1: 64.55%
[ Wed Jul 17 21:09:55 2024 ] 	Class79 Top1: 77.87%
[ Wed Jul 17 21:09:55 2024 ] 	Class80 Top1: 93.67%
[ Wed Jul 17 21:09:55 2024 ] 	Class81 Top1: 79.35%
[ Wed Jul 17 21:09:55 2024 ] 	Class82 Top1: 58.86%
[ Wed Jul 17 21:09:55 2024 ] 	Class83 Top1: 63.54%
[ Wed Jul 17 21:09:55 2024 ] 	Class84 Top1: 51.74%
[ Wed Jul 17 21:09:55 2024 ] 	Class85 Top1: 74.03%
[ Wed Jul 17 21:09:55 2024 ] 	Class86 Top1: 67.69%
[ Wed Jul 17 21:09:55 2024 ] 	Class87 Top1: 86.56%
[ Wed Jul 17 21:09:55 2024 ] 	Class88 Top1: 85.16%
[ Wed Jul 17 21:09:55 2024 ] 	Class89 Top1: 73.93%
[ Wed Jul 17 21:09:55 2024 ] 	Class90 Top1: 76.42%
[ Wed Jul 17 21:09:55 2024 ] 	Class91 Top1: 59.18%
[ Wed Jul 17 21:09:55 2024 ] 	Class92 Top1: 76.14%
[ Wed Jul 17 21:09:55 2024 ] 	Class93 Top1: 74.90%
[ Wed Jul 17 21:09:55 2024 ] 	Class94 Top1: 75.36%
[ Wed Jul 17 21:09:55 2024 ] 	Class95 Top1: 89.78%
[ Wed Jul 17 21:09:55 2024 ] 	Class96 Top1: 82.45%
[ Wed Jul 17 21:09:55 2024 ] 	Class97 Top1: 93.48%
[ Wed Jul 17 21:09:55 2024 ] 	Class98 Top1: 93.67%
[ Wed Jul 17 21:09:55 2024 ] 	Class99 Top1: 89.82%
[ Wed Jul 17 21:09:55 2024 ] 	Class100 Top1: 91.45%
[ Wed Jul 17 21:09:55 2024 ] 	Class101 Top1: 91.45%
[ Wed Jul 17 21:09:55 2024 ] 	Class102 Top1: 85.34%
[ Wed Jul 17 21:09:55 2024 ] 	Class103 Top1: 58.54%
[ Wed Jul 17 21:09:55 2024 ] 	Class104 Top1: 79.07%
[ Wed Jul 17 21:09:55 2024 ] 	Class105 Top1: 59.67%
[ Wed Jul 17 21:09:55 2024 ] 	Class106 Top1: 55.80%
[ Wed Jul 17 21:09:55 2024 ] 	Class107 Top1: 65.24%
[ Wed Jul 17 21:09:55 2024 ] 	Class108 Top1: 85.34%
[ Wed Jul 17 21:09:55 2024 ] 	Class109 Top1: 75.41%
[ Wed Jul 17 21:09:55 2024 ] 	Class110 Top1: 73.27%
[ Wed Jul 17 21:09:55 2024 ] 	Class111 Top1: 84.52%
[ Wed Jul 17 21:09:55 2024 ] 	Class112 Top1: 90.45%
[ Wed Jul 17 21:09:55 2024 ] 	Class113 Top1: 93.70%
[ Wed Jul 17 21:09:55 2024 ] 	Class114 Top1: 90.22%
[ Wed Jul 17 21:09:55 2024 ] 	Class115 Top1: 80.86%
[ Wed Jul 17 21:09:55 2024 ] 	Class116 Top1: 84.55%
[ Wed Jul 17 21:09:55 2024 ] 	Class117 Top1: 84.32%
[ Wed Jul 17 21:09:55 2024 ] 	Class118 Top1: 82.32%
[ Wed Jul 17 21:09:55 2024 ] 	Class119 Top1: 84.69%
[ Wed Jul 17 21:09:55 2024 ] 	Class120 Top1: 91.26%
[ Wed Jul 17 21:09:55 2024 ] 	Top1: 80.11%
[ Wed Jul 17 21:09:55 2024 ] 	Class1 Top5: 96.00%
[ Wed Jul 17 21:09:55 2024 ] 	Class2 Top5: 91.60%
[ Wed Jul 17 21:09:55 2024 ] 	Class3 Top5: 94.59%
[ Wed Jul 17 21:09:55 2024 ] 	Class4 Top5: 94.80%
[ Wed Jul 17 21:09:55 2024 ] 	Class5 Top5: 93.63%
[ Wed Jul 17 21:09:55 2024 ] 	Class6 Top5: 97.81%
[ Wed Jul 17 21:09:55 2024 ] 	Class7 Top5: 96.22%
[ Wed Jul 17 21:09:55 2024 ] 	Class8 Top5: 97.79%
[ Wed Jul 17 21:09:55 2024 ] 	Class9 Top5: 97.99%
[ Wed Jul 17 21:09:55 2024 ] 	Class10 Top5: 95.60%
[ Wed Jul 17 21:09:55 2024 ] 	Class11 Top5: 91.60%
[ Wed Jul 17 21:09:55 2024 ] 	Class12 Top5: 93.37%
[ Wed Jul 17 21:09:55 2024 ] 	Class13 Top5: 91.98%
[ Wed Jul 17 21:09:55 2024 ] 	Class14 Top5: 99.60%
[ Wed Jul 17 21:09:55 2024 ] 	Class15 Top5: 98.21%
[ Wed Jul 17 21:09:55 2024 ] 	Class16 Top5: 95.82%
[ Wed Jul 17 21:09:55 2024 ] 	Class17 Top5: 96.81%
[ Wed Jul 17 21:09:55 2024 ] 	Class18 Top5: 95.63%
[ Wed Jul 17 21:09:55 2024 ] 	Class19 Top5: 95.44%
[ Wed Jul 17 21:09:55 2024 ] 	Class20 Top5: 96.41%
[ Wed Jul 17 21:09:55 2024 ] 	Class21 Top5: 97.61%
[ Wed Jul 17 21:09:55 2024 ] 	Class22 Top5: 98.61%
[ Wed Jul 17 21:09:55 2024 ] 	Class23 Top5: 96.03%
[ Wed Jul 17 21:09:55 2024 ] 	Class24 Top5: 96.02%
[ Wed Jul 17 21:09:55 2024 ] 	Class25 Top5: 93.65%
[ Wed Jul 17 21:09:55 2024 ] 	Class26 Top5: 97.42%
[ Wed Jul 17 21:09:55 2024 ] 	Class27 Top5: 98.21%
[ Wed Jul 17 21:09:55 2024 ] 	Class28 Top5: 94.41%
[ Wed Jul 17 21:09:55 2024 ] 	Class29 Top5: 92.83%
[ Wed Jul 17 21:09:55 2024 ] 	Class30 Top5: 93.43%
[ Wed Jul 17 21:09:55 2024 ] 	Class31 Top5: 93.21%
[ Wed Jul 17 21:09:55 2024 ] 	Class32 Top5: 96.83%
[ Wed Jul 17 21:09:55 2024 ] 	Class33 Top5: 95.43%
[ Wed Jul 17 21:09:55 2024 ] 	Class34 Top5: 94.83%
[ Wed Jul 17 21:09:55 2024 ] 	Class35 Top5: 99.01%
[ Wed Jul 17 21:09:55 2024 ] 	Class36 Top5: 95.83%
[ Wed Jul 17 21:09:55 2024 ] 	Class37 Top5: 92.43%
[ Wed Jul 17 21:09:55 2024 ] 	Class38 Top5: 97.02%
[ Wed Jul 17 21:09:55 2024 ] 	Class39 Top5: 97.02%
[ Wed Jul 17 21:09:55 2024 ] 	Class40 Top5: 98.39%
[ Wed Jul 17 21:09:55 2024 ] 	Class41 Top5: 95.83%
[ Wed Jul 17 21:09:55 2024 ] 	Class42 Top5: 99.60%
[ Wed Jul 17 21:09:55 2024 ] 	Class43 Top5: 99.01%
[ Wed Jul 17 21:09:55 2024 ] 	Class44 Top5: 95.23%
[ Wed Jul 17 21:09:55 2024 ] 	Class45 Top5: 94.64%
[ Wed Jul 17 21:09:55 2024 ] 	Class46 Top5: 92.86%
[ Wed Jul 17 21:09:55 2024 ] 	Class47 Top5: 91.87%
[ Wed Jul 17 21:09:55 2024 ] 	Class48 Top5: 97.61%
[ Wed Jul 17 21:09:55 2024 ] 	Class49 Top5: 95.03%
[ Wed Jul 17 21:09:55 2024 ] 	Class50 Top5: 96.39%
[ Wed Jul 17 21:09:55 2024 ] 	Class51 Top5: 97.61%
[ Wed Jul 17 21:09:55 2024 ] 	Class52 Top5: 97.02%
[ Wed Jul 17 21:09:55 2024 ] 	Class53 Top5: 98.81%
[ Wed Jul 17 21:09:55 2024 ] 	Class54 Top5: 99.20%
[ Wed Jul 17 21:09:55 2024 ] 	Class55 Top5: 99.38%
[ Wed Jul 17 21:09:55 2024 ] 	Class56 Top5: 98.21%
[ Wed Jul 17 21:09:55 2024 ] 	Class57 Top5: 97.42%
[ Wed Jul 17 21:09:55 2024 ] 	Class58 Top5: 99.01%
[ Wed Jul 17 21:09:55 2024 ] 	Class59 Top5: 99.20%
[ Wed Jul 17 21:09:55 2024 ] 	Class60 Top5: 99.40%
[ Wed Jul 17 21:09:55 2024 ] 	Class61 Top5: 95.62%
[ Wed Jul 17 21:09:55 2024 ] 	Class62 Top5: 94.01%
[ Wed Jul 17 21:09:55 2024 ] 	Class63 Top5: 93.22%
[ Wed Jul 17 21:09:55 2024 ] 	Class64 Top5: 95.91%
[ Wed Jul 17 21:09:55 2024 ] 	Class65 Top5: 92.21%
[ Wed Jul 17 21:09:55 2024 ] 	Class66 Top5: 95.51%
[ Wed Jul 17 21:09:55 2024 ] 	Class67 Top5: 88.52%
[ Wed Jul 17 21:09:55 2024 ] 	Class68 Top5: 88.16%
[ Wed Jul 17 21:09:55 2024 ] 	Class69 Top5: 92.65%
[ Wed Jul 17 21:09:55 2024 ] 	Class70 Top5: 93.67%
[ Wed Jul 17 21:09:55 2024 ] 	Class71 Top5: 92.86%
[ Wed Jul 17 21:09:55 2024 ] 	Class72 Top5: 93.27%
[ Wed Jul 17 21:09:55 2024 ] 	Class73 Top5: 89.55%
[ Wed Jul 17 21:09:55 2024 ] 	Class74 Top5: 88.89%
[ Wed Jul 17 21:09:55 2024 ] 	Class75 Top5: 90.02%
[ Wed Jul 17 21:09:55 2024 ] 	Class76 Top5: 92.62%
[ Wed Jul 17 21:09:55 2024 ] 	Class77 Top5: 86.71%
[ Wed Jul 17 21:09:55 2024 ] 	Class78 Top5: 83.20%
[ Wed Jul 17 21:09:55 2024 ] 	Class79 Top5: 89.96%
[ Wed Jul 17 21:09:55 2024 ] 	Class80 Top5: 97.14%
[ Wed Jul 17 21:09:55 2024 ] 	Class81 Top5: 91.00%
[ Wed Jul 17 21:09:55 2024 ] 	Class82 Top5: 86.35%
[ Wed Jul 17 21:09:55 2024 ] 	Class83 Top5: 84.11%
[ Wed Jul 17 21:09:55 2024 ] 	Class84 Top5: 83.03%
[ Wed Jul 17 21:09:55 2024 ] 	Class85 Top5: 89.57%
[ Wed Jul 17 21:09:55 2024 ] 	Class86 Top5: 89.37%
[ Wed Jul 17 21:09:55 2024 ] 	Class87 Top5: 95.72%
[ Wed Jul 17 21:09:55 2024 ] 	Class88 Top5: 95.53%
[ Wed Jul 17 21:09:55 2024 ] 	Class89 Top5: 90.84%
[ Wed Jul 17 21:09:55 2024 ] 	Class90 Top5: 93.50%
[ Wed Jul 17 21:09:55 2024 ] 	Class91 Top5: 87.14%
[ Wed Jul 17 21:09:55 2024 ] 	Class92 Top5: 94.19%
[ Wed Jul 17 21:09:55 2024 ] 	Class93 Top5: 91.63%
[ Wed Jul 17 21:09:55 2024 ] 	Class94 Top5: 93.84%
[ Wed Jul 17 21:09:55 2024 ] 	Class95 Top5: 96.11%
[ Wed Jul 17 21:09:55 2024 ] 	Class96 Top5: 91.43%
[ Wed Jul 17 21:09:55 2024 ] 	Class97 Top5: 96.13%
[ Wed Jul 17 21:09:55 2024 ] 	Class98 Top5: 95.71%
[ Wed Jul 17 21:09:55 2024 ] 	Class99 Top5: 92.67%
[ Wed Jul 17 21:09:55 2024 ] 	Class100 Top5: 95.11%
[ Wed Jul 17 21:09:55 2024 ] 	Class101 Top5: 96.13%
[ Wed Jul 17 21:09:55 2024 ] 	Class102 Top5: 90.84%
[ Wed Jul 17 21:09:55 2024 ] 	Class103 Top5: 88.01%
[ Wed Jul 17 21:09:55 2024 ] 	Class104 Top5: 93.09%
[ Wed Jul 17 21:09:55 2024 ] 	Class105 Top5: 87.58%
[ Wed Jul 17 21:09:55 2024 ] 	Class106 Top5: 93.28%
[ Wed Jul 17 21:09:55 2024 ] 	Class107 Top5: 91.87%
[ Wed Jul 17 21:09:55 2024 ] 	Class108 Top5: 96.33%
[ Wed Jul 17 21:09:55 2024 ] 	Class109 Top5: 93.29%
[ Wed Jul 17 21:09:55 2024 ] 	Class110 Top5: 92.65%
[ Wed Jul 17 21:09:55 2024 ] 	Class111 Top5: 94.91%
[ Wed Jul 17 21:09:55 2024 ] 	Class112 Top5: 96.34%
[ Wed Jul 17 21:09:55 2024 ] 	Class113 Top5: 98.17%
[ Wed Jul 17 21:09:55 2024 ] 	Class114 Top5: 97.15%
[ Wed Jul 17 21:09:55 2024 ] 	Class115 Top5: 94.50%
[ Wed Jul 17 21:09:55 2024 ] 	Class116 Top5: 95.53%
[ Wed Jul 17 21:09:55 2024 ] 	Class117 Top5: 92.87%
[ Wed Jul 17 21:09:55 2024 ] 	Class118 Top5: 97.56%
[ Wed Jul 17 21:09:55 2024 ] 	Class119 Top5: 95.71%
[ Wed Jul 17 21:09:55 2024 ] 	Class120 Top5: 95.93%
[ Wed Jul 17 21:09:55 2024 ] 	Top5: 94.34%
