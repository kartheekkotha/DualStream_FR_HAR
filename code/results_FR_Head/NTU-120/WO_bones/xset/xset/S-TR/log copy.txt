[ Thu Jul 11 20:55:51 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xset/train_joint_120.npy', 'label_path': 'new_data_processed/xset/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xset/val_joint_120.npy', 'label_path': 'new_data_processed/xset/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': True, 'only_attention': True, 'tcn_attention': False, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': None, 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': True, 'scheduler': 1, 'base_lr': 0.01, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 120, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Thu Jul 11 20:55:51 2024 ] Training epoch: 1
[ Thu Jul 11 20:55:53 2024 ] 	Batch(0/6809) done. Loss: 7.2058  lr:0.010000
[ Thu Jul 11 20:56:11 2024 ] 	Batch(100/6809) done. Loss: 4.9263  lr:0.010000
[ Thu Jul 11 20:56:28 2024 ] 	Batch(200/6809) done. Loss: 5.0548  lr:0.010000
[ Thu Jul 11 20:56:45 2024 ] 	Batch(300/6809) done. Loss: 4.4394  lr:0.010000
[ Thu Jul 11 20:57:03 2024 ] 	Batch(400/6809) done. Loss: 5.3794  lr:0.010000
[ Thu Jul 11 20:57:20 2024 ] 
Training: Epoch [0/120], Step [499], Loss: 4.689647197723389, Training Accuracy: 2.1999999999999997
[ Thu Jul 11 20:57:20 2024 ] 	Batch(500/6809) done. Loss: 4.5815  lr:0.010000
[ Thu Jul 11 20:57:37 2024 ] 	Batch(600/6809) done. Loss: 4.8785  lr:0.010000
[ Thu Jul 11 20:57:55 2024 ] 	Batch(700/6809) done. Loss: 4.6337  lr:0.010000
[ Thu Jul 11 20:58:12 2024 ] 	Batch(800/6809) done. Loss: 4.6588  lr:0.010000
[ Thu Jul 11 20:58:30 2024 ] 	Batch(900/6809) done. Loss: 4.4499  lr:0.010000
[ Thu Jul 11 20:58:47 2024 ] 
Training: Epoch [0/120], Step [999], Loss: 3.771055221557617, Training Accuracy: 2.9749999999999996
[ Thu Jul 11 20:58:47 2024 ] 	Batch(1000/6809) done. Loss: 4.4913  lr:0.010000
[ Thu Jul 11 20:59:04 2024 ] 	Batch(1100/6809) done. Loss: 4.8928  lr:0.010000
[ Thu Jul 11 20:59:22 2024 ] 	Batch(1200/6809) done. Loss: 3.9913  lr:0.010000
[ Thu Jul 11 20:59:39 2024 ] 	Batch(1300/6809) done. Loss: 4.2976  lr:0.010000
[ Thu Jul 11 20:59:56 2024 ] 	Batch(1400/6809) done. Loss: 3.0902  lr:0.010000
[ Thu Jul 11 21:00:13 2024 ] 
Training: Epoch [0/120], Step [1499], Loss: 4.039412498474121, Training Accuracy: 3.9
[ Thu Jul 11 21:00:13 2024 ] 	Batch(1500/6809) done. Loss: 4.4226  lr:0.010000
[ Thu Jul 11 21:00:31 2024 ] 	Batch(1600/6809) done. Loss: 4.5461  lr:0.010000
[ Thu Jul 11 21:00:48 2024 ] 	Batch(1700/6809) done. Loss: 3.8603  lr:0.010000
[ Thu Jul 11 21:01:05 2024 ] 	Batch(1800/6809) done. Loss: 3.5835  lr:0.010000
[ Thu Jul 11 21:01:23 2024 ] 	Batch(1900/6809) done. Loss: 4.2487  lr:0.010000
[ Thu Jul 11 21:01:40 2024 ] 
Training: Epoch [0/120], Step [1999], Loss: 3.352724313735962, Training Accuracy: 4.75625
[ Thu Jul 11 21:01:40 2024 ] 	Batch(2000/6809) done. Loss: 3.8464  lr:0.010000
[ Thu Jul 11 21:01:57 2024 ] 	Batch(2100/6809) done. Loss: 4.2868  lr:0.010000
[ Thu Jul 11 21:02:14 2024 ] 	Batch(2200/6809) done. Loss: 3.8238  lr:0.010000
[ Thu Jul 11 21:02:32 2024 ] 	Batch(2300/6809) done. Loss: 4.1996  lr:0.010000
[ Thu Jul 11 21:02:49 2024 ] 	Batch(2400/6809) done. Loss: 3.5974  lr:0.010000
[ Thu Jul 11 21:03:06 2024 ] 
Training: Epoch [0/120], Step [2499], Loss: 3.4944448471069336, Training Accuracy: 5.785
[ Thu Jul 11 21:03:06 2024 ] 	Batch(2500/6809) done. Loss: 3.4506  lr:0.010000
[ Thu Jul 11 21:03:24 2024 ] 	Batch(2600/6809) done. Loss: 4.2897  lr:0.010000
[ Thu Jul 11 21:03:41 2024 ] 	Batch(2700/6809) done. Loss: 3.0042  lr:0.010000
[ Thu Jul 11 21:03:58 2024 ] 	Batch(2800/6809) done. Loss: 3.6880  lr:0.010000
[ Thu Jul 11 21:04:15 2024 ] 	Batch(2900/6809) done. Loss: 2.7611  lr:0.010000
[ Thu Jul 11 21:04:32 2024 ] 
Training: Epoch [0/120], Step [2999], Loss: 2.8616347312927246, Training Accuracy: 6.816666666666666
[ Thu Jul 11 21:04:33 2024 ] 	Batch(3000/6809) done. Loss: 2.4838  lr:0.010000
[ Thu Jul 11 21:04:50 2024 ] 	Batch(3100/6809) done. Loss: 4.3460  lr:0.010000
[ Thu Jul 11 21:05:07 2024 ] 	Batch(3200/6809) done. Loss: 3.6192  lr:0.010000
[ Thu Jul 11 21:05:24 2024 ] 	Batch(3300/6809) done. Loss: 3.3956  lr:0.010000
[ Thu Jul 11 21:05:41 2024 ] 	Batch(3400/6809) done. Loss: 2.9955  lr:0.010000
[ Thu Jul 11 21:05:58 2024 ] 
Training: Epoch [0/120], Step [3499], Loss: 4.351346015930176, Training Accuracy: 7.7857142857142865
[ Thu Jul 11 21:05:59 2024 ] 	Batch(3500/6809) done. Loss: 3.6018  lr:0.010000
[ Thu Jul 11 21:06:16 2024 ] 	Batch(3600/6809) done. Loss: 2.9311  lr:0.010000
[ Thu Jul 11 21:06:34 2024 ] 	Batch(3700/6809) done. Loss: 2.2320  lr:0.010000
[ Thu Jul 11 21:06:52 2024 ] 	Batch(3800/6809) done. Loss: 3.5314  lr:0.010000
[ Thu Jul 11 21:07:09 2024 ] 	Batch(3900/6809) done. Loss: 2.5344  lr:0.010000
[ Thu Jul 11 21:07:27 2024 ] 
Training: Epoch [0/120], Step [3999], Loss: 2.996748924255371, Training Accuracy: 8.759375
[ Thu Jul 11 21:07:27 2024 ] 	Batch(4000/6809) done. Loss: 3.0828  lr:0.010000
[ Thu Jul 11 21:07:45 2024 ] 	Batch(4100/6809) done. Loss: 2.8758  lr:0.010000
[ Thu Jul 11 21:08:02 2024 ] 	Batch(4200/6809) done. Loss: 3.4893  lr:0.010000
[ Thu Jul 11 21:08:19 2024 ] 	Batch(4300/6809) done. Loss: 3.4675  lr:0.010000
[ Thu Jul 11 21:08:36 2024 ] 	Batch(4400/6809) done. Loss: 3.0771  lr:0.010000
[ Thu Jul 11 21:08:53 2024 ] 
Training: Epoch [0/120], Step [4499], Loss: 3.614553689956665, Training Accuracy: 9.855555555555554
[ Thu Jul 11 21:08:54 2024 ] 	Batch(4500/6809) done. Loss: 2.9760  lr:0.010000
[ Thu Jul 11 21:09:11 2024 ] 	Batch(4600/6809) done. Loss: 2.2898  lr:0.010000
[ Thu Jul 11 21:09:29 2024 ] 	Batch(4700/6809) done. Loss: 1.9729  lr:0.010000
[ Thu Jul 11 21:09:46 2024 ] 	Batch(4800/6809) done. Loss: 2.1007  lr:0.010000
[ Thu Jul 11 21:10:04 2024 ] 	Batch(4900/6809) done. Loss: 2.4296  lr:0.010000
[ Thu Jul 11 21:10:21 2024 ] 
Training: Epoch [0/120], Step [4999], Loss: 3.077613353729248, Training Accuracy: 10.9275
[ Thu Jul 11 21:10:21 2024 ] 	Batch(5000/6809) done. Loss: 2.8163  lr:0.010000
[ Thu Jul 11 21:10:39 2024 ] 	Batch(5100/6809) done. Loss: 2.5520  lr:0.010000
[ Thu Jul 11 21:10:56 2024 ] 	Batch(5200/6809) done. Loss: 1.9848  lr:0.010000
[ Thu Jul 11 21:11:14 2024 ] 	Batch(5300/6809) done. Loss: 2.1866  lr:0.010000
[ Thu Jul 11 21:11:31 2024 ] 	Batch(5400/6809) done. Loss: 2.2292  lr:0.010000
[ Thu Jul 11 21:11:48 2024 ] 
Training: Epoch [0/120], Step [5499], Loss: 2.30865216255188, Training Accuracy: 11.922727272727272
[ Thu Jul 11 21:11:49 2024 ] 	Batch(5500/6809) done. Loss: 3.2153  lr:0.010000
[ Thu Jul 11 21:12:06 2024 ] 	Batch(5600/6809) done. Loss: 3.5914  lr:0.010000
[ Thu Jul 11 21:12:24 2024 ] 	Batch(5700/6809) done. Loss: 2.8051  lr:0.010000
[ Thu Jul 11 21:12:41 2024 ] 	Batch(5800/6809) done. Loss: 2.3549  lr:0.010000
[ Thu Jul 11 21:12:59 2024 ] 	Batch(5900/6809) done. Loss: 2.4847  lr:0.010000
[ Thu Jul 11 21:13:16 2024 ] 
Training: Epoch [0/120], Step [5999], Loss: 3.066218852996826, Training Accuracy: 12.960416666666665
[ Thu Jul 11 21:13:16 2024 ] 	Batch(6000/6809) done. Loss: 2.8687  lr:0.010000
[ Thu Jul 11 21:13:34 2024 ] 	Batch(6100/6809) done. Loss: 2.4845  lr:0.010000
[ Thu Jul 11 21:13:52 2024 ] 	Batch(6200/6809) done. Loss: 2.8546  lr:0.010000
[ Thu Jul 11 21:14:10 2024 ] 	Batch(6300/6809) done. Loss: 2.9375  lr:0.010000
[ Thu Jul 11 21:14:28 2024 ] 	Batch(6400/6809) done. Loss: 3.6317  lr:0.010000
[ Thu Jul 11 21:14:46 2024 ] 
Training: Epoch [0/120], Step [6499], Loss: 2.6352272033691406, Training Accuracy: 13.848076923076924
[ Thu Jul 11 21:14:46 2024 ] 	Batch(6500/6809) done. Loss: 1.8454  lr:0.010000
[ Thu Jul 11 21:15:03 2024 ] 	Batch(6600/6809) done. Loss: 2.8287  lr:0.010000
[ Thu Jul 11 21:15:21 2024 ] 	Batch(6700/6809) done. Loss: 2.5229  lr:0.010000
[ Thu Jul 11 21:15:38 2024 ] 	Batch(6800/6809) done. Loss: 2.1493  lr:0.010000
[ Thu Jul 11 21:15:40 2024 ] 	Mean training loss: 3.5352.
[ Thu Jul 11 21:15:40 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul 11 21:15:40 2024 ] Training epoch: 2
[ Thu Jul 11 21:15:41 2024 ] 	Batch(0/6809) done. Loss: 1.9172  lr:0.010000
[ Thu Jul 11 21:15:59 2024 ] 	Batch(100/6809) done. Loss: 2.7286  lr:0.010000
[ Thu Jul 11 21:16:17 2024 ] 	Batch(200/6809) done. Loss: 2.6481  lr:0.010000
[ Thu Jul 11 21:16:36 2024 ] 	Batch(300/6809) done. Loss: 3.2859  lr:0.010000
[ Thu Jul 11 21:16:54 2024 ] 	Batch(400/6809) done. Loss: 2.7107  lr:0.010000
[ Thu Jul 11 21:17:13 2024 ] 
Training: Epoch [1/120], Step [499], Loss: 2.255373954772949, Training Accuracy: 27.1
[ Thu Jul 11 21:17:13 2024 ] 	Batch(500/6809) done. Loss: 2.2912  lr:0.010000
[ Thu Jul 11 21:17:31 2024 ] 	Batch(600/6809) done. Loss: 2.6475  lr:0.010000
[ Thu Jul 11 21:17:50 2024 ] 	Batch(700/6809) done. Loss: 2.7922  lr:0.010000
[ Thu Jul 11 21:18:09 2024 ] 	Batch(800/6809) done. Loss: 1.7458  lr:0.010000
[ Thu Jul 11 21:18:26 2024 ] 	Batch(900/6809) done. Loss: 2.3286  lr:0.010000
[ Thu Jul 11 21:18:44 2024 ] 
Training: Epoch [1/120], Step [999], Loss: 2.6949236392974854, Training Accuracy: 28.15
[ Thu Jul 11 21:18:44 2024 ] 	Batch(1000/6809) done. Loss: 1.8527  lr:0.010000
[ Thu Jul 11 21:19:02 2024 ] 	Batch(1100/6809) done. Loss: 3.1396  lr:0.010000
[ Thu Jul 11 21:19:20 2024 ] 	Batch(1200/6809) done. Loss: 2.5090  lr:0.010000
[ Thu Jul 11 21:19:38 2024 ] 	Batch(1300/6809) done. Loss: 3.0972  lr:0.010000
[ Thu Jul 11 21:19:56 2024 ] 	Batch(1400/6809) done. Loss: 1.9040  lr:0.010000
[ Thu Jul 11 21:20:14 2024 ] 
Training: Epoch [1/120], Step [1499], Loss: 3.6147167682647705, Training Accuracy: 28.749999999999996
[ Thu Jul 11 21:20:14 2024 ] 	Batch(1500/6809) done. Loss: 2.9281  lr:0.010000
[ Thu Jul 11 21:20:32 2024 ] 	Batch(1600/6809) done. Loss: 2.9794  lr:0.010000
[ Thu Jul 11 21:20:50 2024 ] 	Batch(1700/6809) done. Loss: 2.1744  lr:0.010000
[ Thu Jul 11 21:21:08 2024 ] 	Batch(1800/6809) done. Loss: 2.5753  lr:0.010000
[ Thu Jul 11 21:21:26 2024 ] 	Batch(1900/6809) done. Loss: 2.3856  lr:0.010000
[ Thu Jul 11 21:21:43 2024 ] 
Training: Epoch [1/120], Step [1999], Loss: 2.540550708770752, Training Accuracy: 29.65625
[ Thu Jul 11 21:21:44 2024 ] 	Batch(2000/6809) done. Loss: 3.2525  lr:0.010000
[ Thu Jul 11 21:22:02 2024 ] 	Batch(2100/6809) done. Loss: 3.7155  lr:0.010000
[ Thu Jul 11 21:22:21 2024 ] 	Batch(2200/6809) done. Loss: 3.4742  lr:0.010000
[ Thu Jul 11 21:22:39 2024 ] 	Batch(2300/6809) done. Loss: 1.8146  lr:0.010000
[ Thu Jul 11 21:22:58 2024 ] 	Batch(2400/6809) done. Loss: 1.5854  lr:0.010000
[ Thu Jul 11 21:23:16 2024 ] 
Training: Epoch [1/120], Step [2499], Loss: 2.887269973754883, Training Accuracy: 30.37
[ Thu Jul 11 21:23:16 2024 ] 	Batch(2500/6809) done. Loss: 2.2711  lr:0.010000
[ Thu Jul 11 21:23:34 2024 ] 	Batch(2600/6809) done. Loss: 3.5585  lr:0.010000
[ Thu Jul 11 21:23:52 2024 ] 	Batch(2700/6809) done. Loss: 2.6448  lr:0.010000
[ Thu Jul 11 21:24:10 2024 ] 	Batch(2800/6809) done. Loss: 2.5053  lr:0.010000
[ Thu Jul 11 21:24:28 2024 ] 	Batch(2900/6809) done. Loss: 3.1286  lr:0.010000
[ Thu Jul 11 21:24:47 2024 ] 
Training: Epoch [1/120], Step [2999], Loss: 3.354377508163452, Training Accuracy: 31.09166666666667
[ Thu Jul 11 21:24:47 2024 ] 	Batch(3000/6809) done. Loss: 1.9423  lr:0.010000
[ Thu Jul 11 21:25:05 2024 ] 	Batch(3100/6809) done. Loss: 2.4880  lr:0.010000
[ Thu Jul 11 21:25:24 2024 ] 	Batch(3200/6809) done. Loss: 1.8616  lr:0.010000
[ Thu Jul 11 21:25:42 2024 ] 	Batch(3300/6809) done. Loss: 2.8552  lr:0.010000
[ Thu Jul 11 21:26:00 2024 ] 	Batch(3400/6809) done. Loss: 3.4635  lr:0.010000
[ Thu Jul 11 21:26:18 2024 ] 
Training: Epoch [1/120], Step [3499], Loss: 3.2451579570770264, Training Accuracy: 31.775
[ Thu Jul 11 21:26:18 2024 ] 	Batch(3500/6809) done. Loss: 2.0954  lr:0.010000
[ Thu Jul 11 21:26:36 2024 ] 	Batch(3600/6809) done. Loss: 2.7842  lr:0.010000
[ Thu Jul 11 21:26:55 2024 ] 	Batch(3700/6809) done. Loss: 2.4681  lr:0.010000
[ Thu Jul 11 21:27:13 2024 ] 	Batch(3800/6809) done. Loss: 2.5681  lr:0.010000
[ Thu Jul 11 21:27:32 2024 ] 	Batch(3900/6809) done. Loss: 3.8947  lr:0.010000
[ Thu Jul 11 21:27:50 2024 ] 
Training: Epoch [1/120], Step [3999], Loss: 2.6481235027313232, Training Accuracy: 32.1375
[ Thu Jul 11 21:27:51 2024 ] 	Batch(4000/6809) done. Loss: 2.2087  lr:0.010000
[ Thu Jul 11 21:28:09 2024 ] 	Batch(4100/6809) done. Loss: 1.5953  lr:0.010000
[ Thu Jul 11 21:28:27 2024 ] 	Batch(4200/6809) done. Loss: 2.5026  lr:0.010000
[ Thu Jul 11 21:28:45 2024 ] 	Batch(4300/6809) done. Loss: 3.0120  lr:0.010000
[ Thu Jul 11 21:29:03 2024 ] 	Batch(4400/6809) done. Loss: 1.9466  lr:0.010000
[ Thu Jul 11 21:29:21 2024 ] 
Training: Epoch [1/120], Step [4499], Loss: 1.8793728351593018, Training Accuracy: 32.81666666666666
[ Thu Jul 11 21:29:21 2024 ] 	Batch(4500/6809) done. Loss: 2.4837  lr:0.010000
[ Thu Jul 11 21:29:39 2024 ] 	Batch(4600/6809) done. Loss: 2.3680  lr:0.010000
[ Thu Jul 11 21:29:57 2024 ] 	Batch(4700/6809) done. Loss: 2.6872  lr:0.010000
[ Thu Jul 11 21:30:15 2024 ] 	Batch(4800/6809) done. Loss: 3.0777  lr:0.010000
[ Thu Jul 11 21:30:32 2024 ] 	Batch(4900/6809) done. Loss: 2.6176  lr:0.010000
[ Thu Jul 11 21:30:50 2024 ] 
Training: Epoch [1/120], Step [4999], Loss: 2.4190657138824463, Training Accuracy: 33.4725
[ Thu Jul 11 21:30:50 2024 ] 	Batch(5000/6809) done. Loss: 2.1359  lr:0.010000
[ Thu Jul 11 21:31:08 2024 ] 	Batch(5100/6809) done. Loss: 2.0765  lr:0.010000
[ Thu Jul 11 21:31:26 2024 ] 	Batch(5200/6809) done. Loss: 2.0472  lr:0.010000
[ Thu Jul 11 21:31:45 2024 ] 	Batch(5300/6809) done. Loss: 3.4595  lr:0.010000
[ Thu Jul 11 21:32:03 2024 ] 	Batch(5400/6809) done. Loss: 2.7517  lr:0.010000
[ Thu Jul 11 21:32:22 2024 ] 
Training: Epoch [1/120], Step [5499], Loss: 2.5943727493286133, Training Accuracy: 34.09318181818182
[ Thu Jul 11 21:32:22 2024 ] 	Batch(5500/6809) done. Loss: 3.3139  lr:0.010000
[ Thu Jul 11 21:32:41 2024 ] 	Batch(5600/6809) done. Loss: 2.7007  lr:0.010000
[ Thu Jul 11 21:32:58 2024 ] 	Batch(5700/6809) done. Loss: 1.4558  lr:0.010000
[ Thu Jul 11 21:33:16 2024 ] 	Batch(5800/6809) done. Loss: 1.5496  lr:0.010000
[ Thu Jul 11 21:33:34 2024 ] 	Batch(5900/6809) done. Loss: 2.4336  lr:0.010000
[ Thu Jul 11 21:33:52 2024 ] 
Training: Epoch [1/120], Step [5999], Loss: 2.5902657508850098, Training Accuracy: 34.75
[ Thu Jul 11 21:33:52 2024 ] 	Batch(6000/6809) done. Loss: 2.9424  lr:0.010000
[ Thu Jul 11 21:34:10 2024 ] 	Batch(6100/6809) done. Loss: 3.5735  lr:0.010000
[ Thu Jul 11 21:34:28 2024 ] 	Batch(6200/6809) done. Loss: 1.3858  lr:0.010000
[ Thu Jul 11 21:34:46 2024 ] 	Batch(6300/6809) done. Loss: 1.6533  lr:0.010000
[ Thu Jul 11 21:35:04 2024 ] 	Batch(6400/6809) done. Loss: 1.8297  lr:0.010000
[ Thu Jul 11 21:35:21 2024 ] 
Training: Epoch [1/120], Step [6499], Loss: 3.1939311027526855, Training Accuracy: 35.323076923076925
[ Thu Jul 11 21:35:22 2024 ] 	Batch(6500/6809) done. Loss: 1.4624  lr:0.010000
[ Thu Jul 11 21:35:39 2024 ] 	Batch(6600/6809) done. Loss: 1.5453  lr:0.010000
[ Thu Jul 11 21:35:57 2024 ] 	Batch(6700/6809) done. Loss: 2.6864  lr:0.010000
[ Thu Jul 11 21:36:15 2024 ] 	Batch(6800/6809) done. Loss: 1.6040  lr:0.010000
[ Thu Jul 11 21:36:17 2024 ] 	Mean training loss: 2.3542.
[ Thu Jul 11 21:36:17 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul 11 21:36:17 2024 ] Training epoch: 3
[ Thu Jul 11 21:36:18 2024 ] 	Batch(0/6809) done. Loss: 2.1478  lr:0.010000
[ Thu Jul 11 21:36:36 2024 ] 	Batch(100/6809) done. Loss: 1.3003  lr:0.010000
[ Thu Jul 11 21:36:54 2024 ] 	Batch(200/6809) done. Loss: 2.7094  lr:0.010000
[ Thu Jul 11 21:37:13 2024 ] 	Batch(300/6809) done. Loss: 1.6172  lr:0.010000
[ Thu Jul 11 21:37:31 2024 ] 	Batch(400/6809) done. Loss: 2.9742  lr:0.010000
[ Thu Jul 11 21:37:49 2024 ] 
Training: Epoch [2/120], Step [499], Loss: 0.9913197755813599, Training Accuracy: 43.775
[ Thu Jul 11 21:37:49 2024 ] 	Batch(500/6809) done. Loss: 2.4751  lr:0.010000
[ Thu Jul 11 21:38:08 2024 ] 	Batch(600/6809) done. Loss: 2.9342  lr:0.010000
[ Thu Jul 11 21:38:26 2024 ] 	Batch(700/6809) done. Loss: 1.3569  lr:0.010000
[ Thu Jul 11 21:38:45 2024 ] 	Batch(800/6809) done. Loss: 3.1931  lr:0.010000
[ Thu Jul 11 21:39:03 2024 ] 	Batch(900/6809) done. Loss: 3.2469  lr:0.010000
[ Thu Jul 11 21:39:21 2024 ] 
Training: Epoch [2/120], Step [999], Loss: 1.3324477672576904, Training Accuracy: 43.85
[ Thu Jul 11 21:39:21 2024 ] 	Batch(1000/6809) done. Loss: 2.6798  lr:0.010000
[ Thu Jul 11 21:39:39 2024 ] 	Batch(1100/6809) done. Loss: 2.3943  lr:0.010000
[ Thu Jul 11 21:39:58 2024 ] 	Batch(1200/6809) done. Loss: 2.3430  lr:0.010000
[ Thu Jul 11 21:40:16 2024 ] 	Batch(1300/6809) done. Loss: 2.5033  lr:0.010000
[ Thu Jul 11 21:40:35 2024 ] 	Batch(1400/6809) done. Loss: 0.9305  lr:0.010000
[ Thu Jul 11 21:40:53 2024 ] 
Training: Epoch [2/120], Step [1499], Loss: 2.8488528728485107, Training Accuracy: 44.4
[ Thu Jul 11 21:40:53 2024 ] 	Batch(1500/6809) done. Loss: 2.5110  lr:0.010000
[ Thu Jul 11 21:41:11 2024 ] 	Batch(1600/6809) done. Loss: 1.8872  lr:0.010000
[ Thu Jul 11 21:41:30 2024 ] 	Batch(1700/6809) done. Loss: 2.0961  lr:0.010000
[ Thu Jul 11 21:41:48 2024 ] 	Batch(1800/6809) done. Loss: 2.5184  lr:0.010000
[ Thu Jul 11 21:42:06 2024 ] 	Batch(1900/6809) done. Loss: 3.0175  lr:0.010000
[ Thu Jul 11 21:42:25 2024 ] 
Training: Epoch [2/120], Step [1999], Loss: 1.4513270854949951, Training Accuracy: 44.56875
[ Thu Jul 11 21:42:25 2024 ] 	Batch(2000/6809) done. Loss: 1.6596  lr:0.010000
[ Thu Jul 11 21:42:43 2024 ] 	Batch(2100/6809) done. Loss: 1.4691  lr:0.010000
[ Thu Jul 11 21:43:02 2024 ] 	Batch(2200/6809) done. Loss: 0.8388  lr:0.010000
[ Thu Jul 11 21:43:20 2024 ] 	Batch(2300/6809) done. Loss: 1.9258  lr:0.010000
[ Thu Jul 11 21:43:38 2024 ] 	Batch(2400/6809) done. Loss: 3.0845  lr:0.010000
[ Thu Jul 11 21:43:57 2024 ] 
Training: Epoch [2/120], Step [2499], Loss: 1.260538101196289, Training Accuracy: 44.92
[ Thu Jul 11 21:43:57 2024 ] 	Batch(2500/6809) done. Loss: 1.2430  lr:0.010000
[ Thu Jul 11 21:44:15 2024 ] 	Batch(2600/6809) done. Loss: 1.8163  lr:0.010000
[ Thu Jul 11 21:44:33 2024 ] 	Batch(2700/6809) done. Loss: 1.6622  lr:0.010000
[ Thu Jul 11 21:44:52 2024 ] 	Batch(2800/6809) done. Loss: 2.2380  lr:0.010000
[ Thu Jul 11 21:45:10 2024 ] 	Batch(2900/6809) done. Loss: 0.9624  lr:0.010000
[ Thu Jul 11 21:45:28 2024 ] 
Training: Epoch [2/120], Step [2999], Loss: 1.417570948600769, Training Accuracy: 45.637499999999996
[ Thu Jul 11 21:45:28 2024 ] 	Batch(3000/6809) done. Loss: 3.2457  lr:0.010000
[ Thu Jul 11 21:45:46 2024 ] 	Batch(3100/6809) done. Loss: 1.4171  lr:0.010000
[ Thu Jul 11 21:46:04 2024 ] 	Batch(3200/6809) done. Loss: 1.6409  lr:0.010000
[ Thu Jul 11 21:46:22 2024 ] 	Batch(3300/6809) done. Loss: 1.5286  lr:0.010000
[ Thu Jul 11 21:46:41 2024 ] 	Batch(3400/6809) done. Loss: 1.7612  lr:0.010000
[ Thu Jul 11 21:46:59 2024 ] 
Training: Epoch [2/120], Step [3499], Loss: 2.9074645042419434, Training Accuracy: 45.871428571428574
[ Thu Jul 11 21:46:59 2024 ] 	Batch(3500/6809) done. Loss: 1.2048  lr:0.010000
[ Thu Jul 11 21:47:17 2024 ] 	Batch(3600/6809) done. Loss: 2.3029  lr:0.010000
[ Thu Jul 11 21:47:35 2024 ] 	Batch(3700/6809) done. Loss: 1.1678  lr:0.010000
[ Thu Jul 11 21:47:54 2024 ] 	Batch(3800/6809) done. Loss: 2.0396  lr:0.010000
[ Thu Jul 11 21:48:12 2024 ] 	Batch(3900/6809) done. Loss: 2.7363  lr:0.010000
[ Thu Jul 11 21:48:30 2024 ] 
Training: Epoch [2/120], Step [3999], Loss: 1.5850757360458374, Training Accuracy: 46.234375
[ Thu Jul 11 21:48:30 2024 ] 	Batch(4000/6809) done. Loss: 1.4223  lr:0.010000
[ Thu Jul 11 21:48:48 2024 ] 	Batch(4100/6809) done. Loss: 2.3471  lr:0.010000
[ Thu Jul 11 21:49:06 2024 ] 	Batch(4200/6809) done. Loss: 1.1093  lr:0.010000
[ Thu Jul 11 21:49:24 2024 ] 	Batch(4300/6809) done. Loss: 2.4339  lr:0.010000
[ Thu Jul 11 21:49:43 2024 ] 	Batch(4400/6809) done. Loss: 2.5812  lr:0.010000
[ Thu Jul 11 21:50:01 2024 ] 
Training: Epoch [2/120], Step [4499], Loss: 0.705280065536499, Training Accuracy: 46.67777777777778
[ Thu Jul 11 21:50:01 2024 ] 	Batch(4500/6809) done. Loss: 0.9358  lr:0.010000
[ Thu Jul 11 21:50:19 2024 ] 	Batch(4600/6809) done. Loss: 1.0091  lr:0.010000
[ Thu Jul 11 21:50:37 2024 ] 	Batch(4700/6809) done. Loss: 2.6675  lr:0.010000
[ Thu Jul 11 21:50:55 2024 ] 	Batch(4800/6809) done. Loss: 1.2297  lr:0.010000
[ Thu Jul 11 21:51:13 2024 ] 	Batch(4900/6809) done. Loss: 2.0184  lr:0.010000
[ Thu Jul 11 21:51:31 2024 ] 
Training: Epoch [2/120], Step [4999], Loss: 1.7298963069915771, Training Accuracy: 46.8575
[ Thu Jul 11 21:51:31 2024 ] 	Batch(5000/6809) done. Loss: 2.2278  lr:0.010000
[ Thu Jul 11 21:51:49 2024 ] 	Batch(5100/6809) done. Loss: 1.2682  lr:0.010000
[ Thu Jul 11 21:52:07 2024 ] 	Batch(5200/6809) done. Loss: 1.6239  lr:0.010000
[ Thu Jul 11 21:52:26 2024 ] 	Batch(5300/6809) done. Loss: 0.6912  lr:0.010000
[ Thu Jul 11 21:52:44 2024 ] 	Batch(5400/6809) done. Loss: 1.5645  lr:0.010000
[ Thu Jul 11 21:53:02 2024 ] 
Training: Epoch [2/120], Step [5499], Loss: 1.635248064994812, Training Accuracy: 47.21818181818182
[ Thu Jul 11 21:53:02 2024 ] 	Batch(5500/6809) done. Loss: 2.4665  lr:0.010000
[ Thu Jul 11 21:53:20 2024 ] 	Batch(5600/6809) done. Loss: 3.0543  lr:0.010000
[ Thu Jul 11 21:53:38 2024 ] 	Batch(5700/6809) done. Loss: 1.6470  lr:0.010000
[ Thu Jul 11 21:53:56 2024 ] 	Batch(5800/6809) done. Loss: 2.4323  lr:0.010000
[ Thu Jul 11 21:54:14 2024 ] 	Batch(5900/6809) done. Loss: 1.8901  lr:0.010000
[ Thu Jul 11 21:54:32 2024 ] 
Training: Epoch [2/120], Step [5999], Loss: 1.8883901834487915, Training Accuracy: 47.479166666666664
[ Thu Jul 11 21:54:32 2024 ] 	Batch(6000/6809) done. Loss: 1.6808  lr:0.010000
[ Thu Jul 11 21:54:50 2024 ] 	Batch(6100/6809) done. Loss: 2.2170  lr:0.010000
[ Thu Jul 11 21:55:08 2024 ] 	Batch(6200/6809) done. Loss: 1.1545  lr:0.010000
[ Thu Jul 11 21:55:27 2024 ] 	Batch(6300/6809) done. Loss: 2.5467  lr:0.010000
[ Thu Jul 11 21:55:46 2024 ] 	Batch(6400/6809) done. Loss: 1.5794  lr:0.010000
[ Thu Jul 11 21:56:03 2024 ] 
Training: Epoch [2/120], Step [6499], Loss: 2.327857494354248, Training Accuracy: 47.761538461538464
[ Thu Jul 11 21:56:04 2024 ] 	Batch(6500/6809) done. Loss: 1.9939  lr:0.010000
[ Thu Jul 11 21:56:21 2024 ] 	Batch(6600/6809) done. Loss: 1.2039  lr:0.010000
[ Thu Jul 11 21:56:39 2024 ] 	Batch(6700/6809) done. Loss: 1.1216  lr:0.010000
[ Thu Jul 11 21:56:57 2024 ] 	Batch(6800/6809) done. Loss: 2.4124  lr:0.010000
[ Thu Jul 11 21:56:59 2024 ] 	Mean training loss: 1.8557.
[ Thu Jul 11 21:56:59 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 21:56:59 2024 ] Training epoch: 4
[ Thu Jul 11 21:57:00 2024 ] 	Batch(0/6809) done. Loss: 2.0670  lr:0.010000
[ Thu Jul 11 21:57:18 2024 ] 	Batch(100/6809) done. Loss: 1.0548  lr:0.010000
[ Thu Jul 11 21:57:36 2024 ] 	Batch(200/6809) done. Loss: 1.6416  lr:0.010000
[ Thu Jul 11 21:57:54 2024 ] 	Batch(300/6809) done. Loss: 3.2454  lr:0.010000
[ Thu Jul 11 21:58:13 2024 ] 	Batch(400/6809) done. Loss: 2.4768  lr:0.010000
[ Thu Jul 11 21:58:31 2024 ] 
Training: Epoch [3/120], Step [499], Loss: 1.5723202228546143, Training Accuracy: 52.2
[ Thu Jul 11 21:58:32 2024 ] 	Batch(500/6809) done. Loss: 1.6491  lr:0.010000
[ Thu Jul 11 21:58:50 2024 ] 	Batch(600/6809) done. Loss: 0.8748  lr:0.010000
[ Thu Jul 11 21:59:09 2024 ] 	Batch(700/6809) done. Loss: 2.6286  lr:0.010000
[ Thu Jul 11 21:59:27 2024 ] 	Batch(800/6809) done. Loss: 2.0480  lr:0.010000
[ Thu Jul 11 21:59:46 2024 ] 	Batch(900/6809) done. Loss: 1.9838  lr:0.010000
[ Thu Jul 11 22:00:04 2024 ] 
Training: Epoch [3/120], Step [999], Loss: 1.95404052734375, Training Accuracy: 52.7
[ Thu Jul 11 22:00:04 2024 ] 	Batch(1000/6809) done. Loss: 1.1737  lr:0.010000
[ Thu Jul 11 22:00:23 2024 ] 	Batch(1100/6809) done. Loss: 2.2540  lr:0.010000
[ Thu Jul 11 22:00:41 2024 ] 	Batch(1200/6809) done. Loss: 0.9060  lr:0.010000
[ Thu Jul 11 22:00:59 2024 ] 	Batch(1300/6809) done. Loss: 1.0067  lr:0.010000
[ Thu Jul 11 22:01:18 2024 ] 	Batch(1400/6809) done. Loss: 2.3884  lr:0.010000
[ Thu Jul 11 22:01:36 2024 ] 
Training: Epoch [3/120], Step [1499], Loss: 1.2878772020339966, Training Accuracy: 52.76666666666666
[ Thu Jul 11 22:01:36 2024 ] 	Batch(1500/6809) done. Loss: 1.8356  lr:0.010000
[ Thu Jul 11 22:01:55 2024 ] 	Batch(1600/6809) done. Loss: 1.4008  lr:0.010000
[ Thu Jul 11 22:02:13 2024 ] 	Batch(1700/6809) done. Loss: 1.7929  lr:0.010000
[ Thu Jul 11 22:02:31 2024 ] 	Batch(1800/6809) done. Loss: 1.8297  lr:0.010000
[ Thu Jul 11 22:02:49 2024 ] 	Batch(1900/6809) done. Loss: 1.1217  lr:0.010000
[ Thu Jul 11 22:03:06 2024 ] 
Training: Epoch [3/120], Step [1999], Loss: 1.5062546730041504, Training Accuracy: 53.068749999999994
[ Thu Jul 11 22:03:07 2024 ] 	Batch(2000/6809) done. Loss: 2.0875  lr:0.010000
[ Thu Jul 11 22:03:25 2024 ] 	Batch(2100/6809) done. Loss: 1.2586  lr:0.010000
[ Thu Jul 11 22:03:43 2024 ] 	Batch(2200/6809) done. Loss: 1.1527  lr:0.010000
[ Thu Jul 11 22:04:00 2024 ] 	Batch(2300/6809) done. Loss: 2.5539  lr:0.010000
[ Thu Jul 11 22:04:18 2024 ] 	Batch(2400/6809) done. Loss: 0.9399  lr:0.010000
[ Thu Jul 11 22:04:37 2024 ] 
Training: Epoch [3/120], Step [2499], Loss: 0.8549867868423462, Training Accuracy: 53.364999999999995
[ Thu Jul 11 22:04:37 2024 ] 	Batch(2500/6809) done. Loss: 3.0319  lr:0.010000
[ Thu Jul 11 22:04:56 2024 ] 	Batch(2600/6809) done. Loss: 0.7180  lr:0.010000
[ Thu Jul 11 22:05:14 2024 ] 	Batch(2700/6809) done. Loss: 1.8390  lr:0.010000
[ Thu Jul 11 22:05:33 2024 ] 	Batch(2800/6809) done. Loss: 1.1457  lr:0.010000
[ Thu Jul 11 22:05:51 2024 ] 	Batch(2900/6809) done. Loss: 2.0839  lr:0.010000
[ Thu Jul 11 22:06:08 2024 ] 
Training: Epoch [3/120], Step [2999], Loss: 1.0650181770324707, Training Accuracy: 53.625
[ Thu Jul 11 22:06:09 2024 ] 	Batch(3000/6809) done. Loss: 1.0531  lr:0.010000
[ Thu Jul 11 22:06:27 2024 ] 	Batch(3100/6809) done. Loss: 1.9539  lr:0.010000
[ Thu Jul 11 22:06:45 2024 ] 	Batch(3200/6809) done. Loss: 1.9075  lr:0.010000
[ Thu Jul 11 22:07:02 2024 ] 	Batch(3300/6809) done. Loss: 1.6436  lr:0.010000
[ Thu Jul 11 22:07:20 2024 ] 	Batch(3400/6809) done. Loss: 1.6513  lr:0.010000
[ Thu Jul 11 22:07:38 2024 ] 
Training: Epoch [3/120], Step [3499], Loss: 1.5587196350097656, Training Accuracy: 53.91785714285714
[ Thu Jul 11 22:07:38 2024 ] 	Batch(3500/6809) done. Loss: 1.0805  lr:0.010000
[ Thu Jul 11 22:07:56 2024 ] 	Batch(3600/6809) done. Loss: 1.2993  lr:0.010000
[ Thu Jul 11 22:08:14 2024 ] 	Batch(3700/6809) done. Loss: 1.6158  lr:0.010000
[ Thu Jul 11 22:08:32 2024 ] 	Batch(3800/6809) done. Loss: 2.0571  lr:0.010000
[ Thu Jul 11 22:08:50 2024 ] 	Batch(3900/6809) done. Loss: 2.8110  lr:0.010000
[ Thu Jul 11 22:09:08 2024 ] 
Training: Epoch [3/120], Step [3999], Loss: 2.083190679550171, Training Accuracy: 54.090625
[ Thu Jul 11 22:09:08 2024 ] 	Batch(4000/6809) done. Loss: 2.2945  lr:0.010000
[ Thu Jul 11 22:09:27 2024 ] 	Batch(4100/6809) done. Loss: 1.3418  lr:0.010000
[ Thu Jul 11 22:09:45 2024 ] 	Batch(4200/6809) done. Loss: 1.5250  lr:0.010000
[ Thu Jul 11 22:10:04 2024 ] 	Batch(4300/6809) done. Loss: 2.6297  lr:0.010000
[ Thu Jul 11 22:10:22 2024 ] 	Batch(4400/6809) done. Loss: 0.9417  lr:0.010000
[ Thu Jul 11 22:10:41 2024 ] 
Training: Epoch [3/120], Step [4499], Loss: 1.0969687700271606, Training Accuracy: 54.21944444444444
[ Thu Jul 11 22:10:41 2024 ] 	Batch(4500/6809) done. Loss: 1.5293  lr:0.010000
[ Thu Jul 11 22:10:59 2024 ] 	Batch(4600/6809) done. Loss: 1.8198  lr:0.010000
[ Thu Jul 11 22:11:17 2024 ] 	Batch(4700/6809) done. Loss: 0.6291  lr:0.010000
[ Thu Jul 11 22:11:35 2024 ] 	Batch(4800/6809) done. Loss: 1.0867  lr:0.010000
[ Thu Jul 11 22:11:53 2024 ] 	Batch(4900/6809) done. Loss: 0.8907  lr:0.010000
[ Thu Jul 11 22:12:11 2024 ] 
Training: Epoch [3/120], Step [4999], Loss: 1.3457647562026978, Training Accuracy: 54.295
[ Thu Jul 11 22:12:11 2024 ] 	Batch(5000/6809) done. Loss: 1.6685  lr:0.010000
[ Thu Jul 11 22:12:29 2024 ] 	Batch(5100/6809) done. Loss: 1.6206  lr:0.010000
[ Thu Jul 11 22:12:47 2024 ] 	Batch(5200/6809) done. Loss: 1.9328  lr:0.010000
[ Thu Jul 11 22:13:05 2024 ] 	Batch(5300/6809) done. Loss: 1.3699  lr:0.010000
[ Thu Jul 11 22:13:23 2024 ] 	Batch(5400/6809) done. Loss: 1.3930  lr:0.010000
[ Thu Jul 11 22:13:40 2024 ] 
Training: Epoch [3/120], Step [5499], Loss: 3.0126290321350098, Training Accuracy: 54.44772727272728
[ Thu Jul 11 22:13:41 2024 ] 	Batch(5500/6809) done. Loss: 1.2084  lr:0.010000
[ Thu Jul 11 22:13:59 2024 ] 	Batch(5600/6809) done. Loss: 1.8853  lr:0.010000
[ Thu Jul 11 22:14:17 2024 ] 	Batch(5700/6809) done. Loss: 1.1279  lr:0.010000
[ Thu Jul 11 22:14:34 2024 ] 	Batch(5800/6809) done. Loss: 1.9315  lr:0.010000
[ Thu Jul 11 22:14:52 2024 ] 	Batch(5900/6809) done. Loss: 0.5458  lr:0.010000
[ Thu Jul 11 22:15:10 2024 ] 
Training: Epoch [3/120], Step [5999], Loss: 1.7947049140930176, Training Accuracy: 54.645833333333336
[ Thu Jul 11 22:15:10 2024 ] 	Batch(6000/6809) done. Loss: 1.7791  lr:0.010000
[ Thu Jul 11 22:15:29 2024 ] 	Batch(6100/6809) done. Loss: 1.1598  lr:0.010000
[ Thu Jul 11 22:15:47 2024 ] 	Batch(6200/6809) done. Loss: 1.8772  lr:0.010000
[ Thu Jul 11 22:16:05 2024 ] 	Batch(6300/6809) done. Loss: 1.4247  lr:0.010000
[ Thu Jul 11 22:16:24 2024 ] 	Batch(6400/6809) done. Loss: 1.6461  lr:0.010000
[ Thu Jul 11 22:16:42 2024 ] 
Training: Epoch [3/120], Step [6499], Loss: 1.3225674629211426, Training Accuracy: 54.81730769230769
[ Thu Jul 11 22:16:42 2024 ] 	Batch(6500/6809) done. Loss: 2.0879  lr:0.010000
[ Thu Jul 11 22:17:00 2024 ] 	Batch(6600/6809) done. Loss: 1.0055  lr:0.010000
[ Thu Jul 11 22:17:18 2024 ] 	Batch(6700/6809) done. Loss: 1.7389  lr:0.010000
[ Thu Jul 11 22:17:36 2024 ] 	Batch(6800/6809) done. Loss: 1.4942  lr:0.010000
[ Thu Jul 11 22:17:38 2024 ] 	Mean training loss: 1.5648.
[ Thu Jul 11 22:17:38 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul 11 22:17:38 2024 ] Training epoch: 5
[ Thu Jul 11 22:17:38 2024 ] 	Batch(0/6809) done. Loss: 2.2084  lr:0.010000
[ Thu Jul 11 22:17:56 2024 ] 	Batch(100/6809) done. Loss: 1.6099  lr:0.010000
[ Thu Jul 11 22:18:14 2024 ] 	Batch(200/6809) done. Loss: 1.0098  lr:0.010000
[ Thu Jul 11 22:18:32 2024 ] 	Batch(300/6809) done. Loss: 1.1571  lr:0.010000
[ Thu Jul 11 22:18:50 2024 ] 	Batch(400/6809) done. Loss: 1.5818  lr:0.010000
[ Thu Jul 11 22:19:09 2024 ] 
Training: Epoch [4/120], Step [499], Loss: 1.0074427127838135, Training Accuracy: 56.75
[ Thu Jul 11 22:19:09 2024 ] 	Batch(500/6809) done. Loss: 0.8351  lr:0.010000
[ Thu Jul 11 22:19:27 2024 ] 	Batch(600/6809) done. Loss: 1.2742  lr:0.010000
[ Thu Jul 11 22:19:45 2024 ] 	Batch(700/6809) done. Loss: 1.8970  lr:0.010000
[ Thu Jul 11 22:20:04 2024 ] 	Batch(800/6809) done. Loss: 2.2619  lr:0.010000
[ Thu Jul 11 22:20:22 2024 ] 	Batch(900/6809) done. Loss: 1.3554  lr:0.010000
[ Thu Jul 11 22:20:41 2024 ] 
Training: Epoch [4/120], Step [999], Loss: 1.1074626445770264, Training Accuracy: 57.9125
[ Thu Jul 11 22:20:41 2024 ] 	Batch(1000/6809) done. Loss: 1.2880  lr:0.010000
[ Thu Jul 11 22:20:59 2024 ] 	Batch(1100/6809) done. Loss: 0.8143  lr:0.010000
[ Thu Jul 11 22:21:18 2024 ] 	Batch(1200/6809) done. Loss: 1.1218  lr:0.010000
[ Thu Jul 11 22:21:36 2024 ] 	Batch(1300/6809) done. Loss: 1.5132  lr:0.010000
[ Thu Jul 11 22:21:54 2024 ] 	Batch(1400/6809) done. Loss: 0.9830  lr:0.010000
[ Thu Jul 11 22:22:12 2024 ] 
Training: Epoch [4/120], Step [1499], Loss: 0.39994752407073975, Training Accuracy: 58.25833333333333
[ Thu Jul 11 22:22:12 2024 ] 	Batch(1500/6809) done. Loss: 1.6779  lr:0.010000
[ Thu Jul 11 22:22:30 2024 ] 	Batch(1600/6809) done. Loss: 1.0288  lr:0.010000
[ Thu Jul 11 22:22:48 2024 ] 	Batch(1700/6809) done. Loss: 0.8395  lr:0.010000
[ Thu Jul 11 22:23:06 2024 ] 	Batch(1800/6809) done. Loss: 0.4727  lr:0.010000
[ Thu Jul 11 22:23:24 2024 ] 	Batch(1900/6809) done. Loss: 1.9079  lr:0.010000
[ Thu Jul 11 22:23:41 2024 ] 
Training: Epoch [4/120], Step [1999], Loss: 1.6681545972824097, Training Accuracy: 58.39375
[ Thu Jul 11 22:23:42 2024 ] 	Batch(2000/6809) done. Loss: 1.6794  lr:0.010000
[ Thu Jul 11 22:24:00 2024 ] 	Batch(2100/6809) done. Loss: 1.5733  lr:0.010000
[ Thu Jul 11 22:24:17 2024 ] 	Batch(2200/6809) done. Loss: 1.2655  lr:0.010000
[ Thu Jul 11 22:24:35 2024 ] 	Batch(2300/6809) done. Loss: 1.3105  lr:0.010000
[ Thu Jul 11 22:24:53 2024 ] 	Batch(2400/6809) done. Loss: 1.4772  lr:0.010000
[ Thu Jul 11 22:25:11 2024 ] 
Training: Epoch [4/120], Step [2499], Loss: 0.6171281337738037, Training Accuracy: 58.635000000000005
[ Thu Jul 11 22:25:11 2024 ] 	Batch(2500/6809) done. Loss: 4.3622  lr:0.010000
[ Thu Jul 11 22:25:29 2024 ] 	Batch(2600/6809) done. Loss: 0.9446  lr:0.010000
[ Thu Jul 11 22:25:47 2024 ] 	Batch(2700/6809) done. Loss: 1.1200  lr:0.010000
[ Thu Jul 11 22:26:05 2024 ] 	Batch(2800/6809) done. Loss: 1.2738  lr:0.010000
[ Thu Jul 11 22:26:23 2024 ] 	Batch(2900/6809) done. Loss: 0.7942  lr:0.010000
[ Thu Jul 11 22:26:41 2024 ] 
Training: Epoch [4/120], Step [2999], Loss: 1.4113197326660156, Training Accuracy: 58.55416666666666
[ Thu Jul 11 22:26:41 2024 ] 	Batch(3000/6809) done. Loss: 0.6816  lr:0.010000
[ Thu Jul 11 22:26:59 2024 ] 	Batch(3100/6809) done. Loss: 1.0488  lr:0.010000
[ Thu Jul 11 22:27:17 2024 ] 	Batch(3200/6809) done. Loss: 1.1477  lr:0.010000
[ Thu Jul 11 22:27:35 2024 ] 	Batch(3300/6809) done. Loss: 0.5719  lr:0.010000
[ Thu Jul 11 22:27:53 2024 ] 	Batch(3400/6809) done. Loss: 1.2269  lr:0.010000
[ Thu Jul 11 22:28:10 2024 ] 
Training: Epoch [4/120], Step [3499], Loss: 0.8342573046684265, Training Accuracy: 58.79642857142857
[ Thu Jul 11 22:28:10 2024 ] 	Batch(3500/6809) done. Loss: 0.9753  lr:0.010000
[ Thu Jul 11 22:28:29 2024 ] 	Batch(3600/6809) done. Loss: 1.1518  lr:0.010000
[ Thu Jul 11 22:28:46 2024 ] 	Batch(3700/6809) done. Loss: 0.7537  lr:0.010000
[ Thu Jul 11 22:29:04 2024 ] 	Batch(3800/6809) done. Loss: 0.8539  lr:0.010000
[ Thu Jul 11 22:29:22 2024 ] 	Batch(3900/6809) done. Loss: 0.7674  lr:0.010000
[ Thu Jul 11 22:29:40 2024 ] 
Training: Epoch [4/120], Step [3999], Loss: 1.3885024785995483, Training Accuracy: 59.03125
[ Thu Jul 11 22:29:40 2024 ] 	Batch(4000/6809) done. Loss: 1.1670  lr:0.010000
[ Thu Jul 11 22:29:59 2024 ] 	Batch(4100/6809) done. Loss: 1.2044  lr:0.010000
[ Thu Jul 11 22:30:17 2024 ] 	Batch(4200/6809) done. Loss: 1.3820  lr:0.010000
[ Thu Jul 11 22:30:36 2024 ] 	Batch(4300/6809) done. Loss: 1.0736  lr:0.010000
[ Thu Jul 11 22:30:55 2024 ] 	Batch(4400/6809) done. Loss: 0.9711  lr:0.010000
[ Thu Jul 11 22:31:13 2024 ] 
Training: Epoch [4/120], Step [4499], Loss: 0.9156457185745239, Training Accuracy: 59.169444444444444
[ Thu Jul 11 22:31:13 2024 ] 	Batch(4500/6809) done. Loss: 0.9863  lr:0.010000
[ Thu Jul 11 22:31:32 2024 ] 	Batch(4600/6809) done. Loss: 0.6841  lr:0.010000
[ Thu Jul 11 22:31:50 2024 ] 	Batch(4700/6809) done. Loss: 1.2832  lr:0.010000
[ Thu Jul 11 22:32:09 2024 ] 	Batch(4800/6809) done. Loss: 1.2163  lr:0.010000
[ Thu Jul 11 22:32:27 2024 ] 	Batch(4900/6809) done. Loss: 2.2533  lr:0.010000
[ Thu Jul 11 22:32:46 2024 ] 
Training: Epoch [4/120], Step [4999], Loss: 0.9471080899238586, Training Accuracy: 59.162499999999994
[ Thu Jul 11 22:32:46 2024 ] 	Batch(5000/6809) done. Loss: 1.1020  lr:0.010000
[ Thu Jul 11 22:33:04 2024 ] 	Batch(5100/6809) done. Loss: 1.4206  lr:0.010000
[ Thu Jul 11 22:33:23 2024 ] 	Batch(5200/6809) done. Loss: 1.2756  lr:0.010000
[ Thu Jul 11 22:33:42 2024 ] 	Batch(5300/6809) done. Loss: 1.3007  lr:0.010000
[ Thu Jul 11 22:34:00 2024 ] 	Batch(5400/6809) done. Loss: 1.2480  lr:0.010000
[ Thu Jul 11 22:34:19 2024 ] 
Training: Epoch [4/120], Step [5499], Loss: 1.253072738647461, Training Accuracy: 59.28181818181818
[ Thu Jul 11 22:34:19 2024 ] 	Batch(5500/6809) done. Loss: 1.1176  lr:0.010000
[ Thu Jul 11 22:34:37 2024 ] 	Batch(5600/6809) done. Loss: 1.1002  lr:0.010000
[ Thu Jul 11 22:34:56 2024 ] 	Batch(5700/6809) done. Loss: 2.0811  lr:0.010000
[ Thu Jul 11 22:35:14 2024 ] 	Batch(5800/6809) done. Loss: 0.7763  lr:0.010000
[ Thu Jul 11 22:35:33 2024 ] 	Batch(5900/6809) done. Loss: 1.7877  lr:0.010000
[ Thu Jul 11 22:35:51 2024 ] 
Training: Epoch [4/120], Step [5999], Loss: 1.3295015096664429, Training Accuracy: 59.43333333333334
[ Thu Jul 11 22:35:52 2024 ] 	Batch(6000/6809) done. Loss: 0.5985  lr:0.010000
[ Thu Jul 11 22:36:10 2024 ] 	Batch(6100/6809) done. Loss: 1.8870  lr:0.010000
[ Thu Jul 11 22:36:29 2024 ] 	Batch(6200/6809) done. Loss: 0.8488  lr:0.010000
[ Thu Jul 11 22:36:47 2024 ] 	Batch(6300/6809) done. Loss: 2.3318  lr:0.010000
[ Thu Jul 11 22:37:06 2024 ] 	Batch(6400/6809) done. Loss: 1.3030  lr:0.010000
[ Thu Jul 11 22:37:24 2024 ] 
Training: Epoch [4/120], Step [6499], Loss: 1.1059280633926392, Training Accuracy: 59.63846153846154
[ Thu Jul 11 22:37:25 2024 ] 	Batch(6500/6809) done. Loss: 1.2346  lr:0.010000
[ Thu Jul 11 22:37:43 2024 ] 	Batch(6600/6809) done. Loss: 0.8668  lr:0.010000
[ Thu Jul 11 22:38:02 2024 ] 	Batch(6700/6809) done. Loss: 1.1718  lr:0.010000
[ Thu Jul 11 22:38:20 2024 ] 	Batch(6800/6809) done. Loss: 2.0699  lr:0.010000
[ Thu Jul 11 22:38:22 2024 ] 	Mean training loss: 1.3885.
[ Thu Jul 11 22:38:22 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 22:38:22 2024 ] Training epoch: 6
[ Thu Jul 11 22:38:23 2024 ] 	Batch(0/6809) done. Loss: 1.1025  lr:0.010000
[ Thu Jul 11 22:38:41 2024 ] 	Batch(100/6809) done. Loss: 0.7421  lr:0.010000
[ Thu Jul 11 22:38:59 2024 ] 	Batch(200/6809) done. Loss: 0.4197  lr:0.010000
[ Thu Jul 11 22:39:17 2024 ] 	Batch(300/6809) done. Loss: 1.3618  lr:0.010000
[ Thu Jul 11 22:39:36 2024 ] 	Batch(400/6809) done. Loss: 1.0312  lr:0.010000
[ Thu Jul 11 22:39:54 2024 ] 
Training: Epoch [5/120], Step [499], Loss: 0.8886690139770508, Training Accuracy: 62.425
[ Thu Jul 11 22:39:54 2024 ] 	Batch(500/6809) done. Loss: 0.7403  lr:0.010000
[ Thu Jul 11 22:40:13 2024 ] 	Batch(600/6809) done. Loss: 0.7661  lr:0.010000
[ Thu Jul 11 22:40:31 2024 ] 	Batch(700/6809) done. Loss: 1.7273  lr:0.010000
[ Thu Jul 11 22:40:49 2024 ] 	Batch(800/6809) done. Loss: 1.3508  lr:0.010000
[ Thu Jul 11 22:41:07 2024 ] 	Batch(900/6809) done. Loss: 2.6330  lr:0.010000
[ Thu Jul 11 22:41:25 2024 ] 
Training: Epoch [5/120], Step [999], Loss: 1.0841296911239624, Training Accuracy: 62.9625
[ Thu Jul 11 22:41:25 2024 ] 	Batch(1000/6809) done. Loss: 1.0023  lr:0.010000
[ Thu Jul 11 22:41:43 2024 ] 	Batch(1100/6809) done. Loss: 3.2303  lr:0.010000
[ Thu Jul 11 22:42:01 2024 ] 	Batch(1200/6809) done. Loss: 1.6800  lr:0.010000
[ Thu Jul 11 22:42:20 2024 ] 	Batch(1300/6809) done. Loss: 0.9523  lr:0.010000
[ Thu Jul 11 22:42:38 2024 ] 	Batch(1400/6809) done. Loss: 1.4314  lr:0.010000
[ Thu Jul 11 22:42:57 2024 ] 
Training: Epoch [5/120], Step [1499], Loss: 0.9419031739234924, Training Accuracy: 63.13333333333333
[ Thu Jul 11 22:42:57 2024 ] 	Batch(1500/6809) done. Loss: 0.7470  lr:0.010000
[ Thu Jul 11 22:43:16 2024 ] 	Batch(1600/6809) done. Loss: 0.7342  lr:0.010000
[ Thu Jul 11 22:43:34 2024 ] 	Batch(1700/6809) done. Loss: 2.3480  lr:0.010000
[ Thu Jul 11 22:43:52 2024 ] 	Batch(1800/6809) done. Loss: 1.2014  lr:0.010000
[ Thu Jul 11 22:44:09 2024 ] 	Batch(1900/6809) done. Loss: 0.3610  lr:0.010000
[ Thu Jul 11 22:44:27 2024 ] 
Training: Epoch [5/120], Step [1999], Loss: 1.0126394033432007, Training Accuracy: 62.8625
[ Thu Jul 11 22:44:27 2024 ] 	Batch(2000/6809) done. Loss: 1.6789  lr:0.010000
[ Thu Jul 11 22:44:45 2024 ] 	Batch(2100/6809) done. Loss: 0.6279  lr:0.010000
[ Thu Jul 11 22:45:03 2024 ] 	Batch(2200/6809) done. Loss: 0.7303  lr:0.010000
[ Thu Jul 11 22:45:21 2024 ] 	Batch(2300/6809) done. Loss: 1.2259  lr:0.010000
[ Thu Jul 11 22:45:39 2024 ] 	Batch(2400/6809) done. Loss: 1.3748  lr:0.010000
[ Thu Jul 11 22:45:57 2024 ] 
Training: Epoch [5/120], Step [2499], Loss: 1.1419522762298584, Training Accuracy: 62.739999999999995
[ Thu Jul 11 22:45:57 2024 ] 	Batch(2500/6809) done. Loss: 1.2557  lr:0.010000
[ Thu Jul 11 22:46:15 2024 ] 	Batch(2600/6809) done. Loss: 1.1814  lr:0.010000
[ Thu Jul 11 22:46:33 2024 ] 	Batch(2700/6809) done. Loss: 0.4770  lr:0.010000
[ Thu Jul 11 22:46:51 2024 ] 	Batch(2800/6809) done. Loss: 1.7331  lr:0.010000
[ Thu Jul 11 22:47:09 2024 ] 	Batch(2900/6809) done. Loss: 0.6403  lr:0.010000
[ Thu Jul 11 22:47:28 2024 ] 
Training: Epoch [5/120], Step [2999], Loss: 0.7423160076141357, Training Accuracy: 63.0125
[ Thu Jul 11 22:47:28 2024 ] 	Batch(3000/6809) done. Loss: 0.7758  lr:0.010000
[ Thu Jul 11 22:47:46 2024 ] 	Batch(3100/6809) done. Loss: 0.8175  lr:0.010000
[ Thu Jul 11 22:48:05 2024 ] 	Batch(3200/6809) done. Loss: 1.1212  lr:0.010000
[ Thu Jul 11 22:48:23 2024 ] 	Batch(3300/6809) done. Loss: 1.8078  lr:0.010000
[ Thu Jul 11 22:48:41 2024 ] 	Batch(3400/6809) done. Loss: 0.7849  lr:0.010000
[ Thu Jul 11 22:48:59 2024 ] 
Training: Epoch [5/120], Step [3499], Loss: 1.2879019975662231, Training Accuracy: 62.96071428571428
[ Thu Jul 11 22:48:59 2024 ] 	Batch(3500/6809) done. Loss: 0.7920  lr:0.010000
[ Thu Jul 11 22:49:17 2024 ] 	Batch(3600/6809) done. Loss: 0.8061  lr:0.010000
[ Thu Jul 11 22:49:35 2024 ] 	Batch(3700/6809) done. Loss: 2.1846  lr:0.010000
[ Thu Jul 11 22:49:52 2024 ] 	Batch(3800/6809) done. Loss: 1.1450  lr:0.010000
[ Thu Jul 11 22:50:10 2024 ] 	Batch(3900/6809) done. Loss: 0.8424  lr:0.010000
[ Thu Jul 11 22:50:28 2024 ] 
Training: Epoch [5/120], Step [3999], Loss: 1.415665864944458, Training Accuracy: 62.878125
[ Thu Jul 11 22:50:28 2024 ] 	Batch(4000/6809) done. Loss: 0.7852  lr:0.010000
[ Thu Jul 11 22:50:46 2024 ] 	Batch(4100/6809) done. Loss: 0.6808  lr:0.010000
[ Thu Jul 11 22:51:04 2024 ] 	Batch(4200/6809) done. Loss: 1.1242  lr:0.010000
[ Thu Jul 11 22:51:22 2024 ] 	Batch(4300/6809) done. Loss: 0.4818  lr:0.010000
[ Thu Jul 11 22:51:40 2024 ] 	Batch(4400/6809) done. Loss: 1.6432  lr:0.010000
[ Thu Jul 11 22:51:58 2024 ] 
Training: Epoch [5/120], Step [4499], Loss: 1.5599946975708008, Training Accuracy: 62.84444444444445
[ Thu Jul 11 22:51:59 2024 ] 	Batch(4500/6809) done. Loss: 1.4927  lr:0.010000
[ Thu Jul 11 22:52:17 2024 ] 	Batch(4600/6809) done. Loss: 0.6090  lr:0.010000
[ Thu Jul 11 22:52:35 2024 ] 	Batch(4700/6809) done. Loss: 0.7485  lr:0.010000
[ Thu Jul 11 22:52:53 2024 ] 	Batch(4800/6809) done. Loss: 0.8134  lr:0.010000
[ Thu Jul 11 22:53:11 2024 ] 	Batch(4900/6809) done. Loss: 1.4734  lr:0.010000
[ Thu Jul 11 22:53:30 2024 ] 
Training: Epoch [5/120], Step [4999], Loss: 1.8661493062973022, Training Accuracy: 62.96000000000001
[ Thu Jul 11 22:53:30 2024 ] 	Batch(5000/6809) done. Loss: 0.6371  lr:0.010000
[ Thu Jul 11 22:53:48 2024 ] 	Batch(5100/6809) done. Loss: 1.4044  lr:0.010000
[ Thu Jul 11 22:54:06 2024 ] 	Batch(5200/6809) done. Loss: 1.1953  lr:0.010000
[ Thu Jul 11 22:54:24 2024 ] 	Batch(5300/6809) done. Loss: 2.3376  lr:0.010000
[ Thu Jul 11 22:54:42 2024 ] 	Batch(5400/6809) done. Loss: 1.1410  lr:0.010000
[ Thu Jul 11 22:55:00 2024 ] 
Training: Epoch [5/120], Step [5499], Loss: 0.8398723006248474, Training Accuracy: 62.970454545454544
[ Thu Jul 11 22:55:00 2024 ] 	Batch(5500/6809) done. Loss: 1.8273  lr:0.010000
[ Thu Jul 11 22:55:18 2024 ] 	Batch(5600/6809) done. Loss: 1.4439  lr:0.010000
[ Thu Jul 11 22:55:36 2024 ] 	Batch(5700/6809) done. Loss: 1.6995  lr:0.010000
[ Thu Jul 11 22:55:54 2024 ] 	Batch(5800/6809) done. Loss: 0.7665  lr:0.010000
[ Thu Jul 11 22:56:11 2024 ] 	Batch(5900/6809) done. Loss: 0.4957  lr:0.010000
[ Thu Jul 11 22:56:29 2024 ] 
Training: Epoch [5/120], Step [5999], Loss: 0.6359761953353882, Training Accuracy: 63.145833333333336
[ Thu Jul 11 22:56:29 2024 ] 	Batch(6000/6809) done. Loss: 1.3295  lr:0.010000
[ Thu Jul 11 22:56:47 2024 ] 	Batch(6100/6809) done. Loss: 1.1899  lr:0.010000
[ Thu Jul 11 22:57:05 2024 ] 	Batch(6200/6809) done. Loss: 1.5279  lr:0.010000
[ Thu Jul 11 22:57:23 2024 ] 	Batch(6300/6809) done. Loss: 1.3970  lr:0.010000
[ Thu Jul 11 22:57:41 2024 ] 	Batch(6400/6809) done. Loss: 1.0241  lr:0.010000
[ Thu Jul 11 22:57:59 2024 ] 
Training: Epoch [5/120], Step [6499], Loss: 0.8523377180099487, Training Accuracy: 63.14807692307692
[ Thu Jul 11 22:57:59 2024 ] 	Batch(6500/6809) done. Loss: 0.9828  lr:0.010000
[ Thu Jul 11 22:58:18 2024 ] 	Batch(6600/6809) done. Loss: 0.5786  lr:0.010000
[ Thu Jul 11 22:58:36 2024 ] 	Batch(6700/6809) done. Loss: 1.1799  lr:0.010000
[ Thu Jul 11 22:58:55 2024 ] 	Batch(6800/6809) done. Loss: 1.0200  lr:0.010000
[ Thu Jul 11 22:58:57 2024 ] 	Mean training loss: 1.2517.
[ Thu Jul 11 22:58:57 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul 11 22:58:57 2024 ] Training epoch: 7
[ Thu Jul 11 22:58:57 2024 ] 	Batch(0/6809) done. Loss: 1.7014  lr:0.010000
[ Thu Jul 11 22:59:16 2024 ] 	Batch(100/6809) done. Loss: 2.1526  lr:0.010000
[ Thu Jul 11 22:59:34 2024 ] 	Batch(200/6809) done. Loss: 1.1228  lr:0.010000
[ Thu Jul 11 22:59:52 2024 ] 	Batch(300/6809) done. Loss: 3.0282  lr:0.010000
[ Thu Jul 11 23:00:11 2024 ] 	Batch(400/6809) done. Loss: 1.7201  lr:0.010000
[ Thu Jul 11 23:00:29 2024 ] 
Training: Epoch [6/120], Step [499], Loss: 1.4706519842147827, Training Accuracy: 65.10000000000001
[ Thu Jul 11 23:00:29 2024 ] 	Batch(500/6809) done. Loss: 2.1439  lr:0.010000
[ Thu Jul 11 23:00:47 2024 ] 	Batch(600/6809) done. Loss: 1.1666  lr:0.010000
[ Thu Jul 11 23:01:05 2024 ] 	Batch(700/6809) done. Loss: 1.9167  lr:0.010000
[ Thu Jul 11 23:01:23 2024 ] 	Batch(800/6809) done. Loss: 1.2217  lr:0.010000
[ Thu Jul 11 23:01:41 2024 ] 	Batch(900/6809) done. Loss: 0.6609  lr:0.010000
[ Thu Jul 11 23:01:58 2024 ] 
Training: Epoch [6/120], Step [999], Loss: 1.2218072414398193, Training Accuracy: 65.275
[ Thu Jul 11 23:01:59 2024 ] 	Batch(1000/6809) done. Loss: 2.2424  lr:0.010000
[ Thu Jul 11 23:02:16 2024 ] 	Batch(1100/6809) done. Loss: 1.2767  lr:0.010000
[ Thu Jul 11 23:02:35 2024 ] 	Batch(1200/6809) done. Loss: 1.5473  lr:0.010000
[ Thu Jul 11 23:02:53 2024 ] 	Batch(1300/6809) done. Loss: 0.8978  lr:0.010000
[ Thu Jul 11 23:03:12 2024 ] 	Batch(1400/6809) done. Loss: 1.3761  lr:0.010000
[ Thu Jul 11 23:03:31 2024 ] 
Training: Epoch [6/120], Step [1499], Loss: 0.9898250699043274, Training Accuracy: 65.2
[ Thu Jul 11 23:03:31 2024 ] 	Batch(1500/6809) done. Loss: 1.5401  lr:0.010000
[ Thu Jul 11 23:03:50 2024 ] 	Batch(1600/6809) done. Loss: 1.1975  lr:0.010000
[ Thu Jul 11 23:04:08 2024 ] 	Batch(1700/6809) done. Loss: 0.9953  lr:0.010000
[ Thu Jul 11 23:04:27 2024 ] 	Batch(1800/6809) done. Loss: 0.8630  lr:0.010000
[ Thu Jul 11 23:04:45 2024 ] 	Batch(1900/6809) done. Loss: 1.2653  lr:0.010000
[ Thu Jul 11 23:05:04 2024 ] 
Training: Epoch [6/120], Step [1999], Loss: 0.6403835415840149, Training Accuracy: 64.875
[ Thu Jul 11 23:05:04 2024 ] 	Batch(2000/6809) done. Loss: 0.9437  lr:0.010000
[ Thu Jul 11 23:05:22 2024 ] 	Batch(2100/6809) done. Loss: 0.9460  lr:0.010000
[ Thu Jul 11 23:05:40 2024 ] 	Batch(2200/6809) done. Loss: 2.0853  lr:0.010000
[ Thu Jul 11 23:05:58 2024 ] 	Batch(2300/6809) done. Loss: 0.8117  lr:0.010000
[ Thu Jul 11 23:06:16 2024 ] 	Batch(2400/6809) done. Loss: 1.2664  lr:0.010000
[ Thu Jul 11 23:06:33 2024 ] 
Training: Epoch [6/120], Step [2499], Loss: 1.1820429563522339, Training Accuracy: 65.05499999999999
[ Thu Jul 11 23:06:34 2024 ] 	Batch(2500/6809) done. Loss: 2.0070  lr:0.010000
[ Thu Jul 11 23:06:51 2024 ] 	Batch(2600/6809) done. Loss: 1.4919  lr:0.010000
[ Thu Jul 11 23:07:09 2024 ] 	Batch(2700/6809) done. Loss: 1.0924  lr:0.010000
[ Thu Jul 11 23:07:27 2024 ] 	Batch(2800/6809) done. Loss: 3.5425  lr:0.010000
[ Thu Jul 11 23:07:45 2024 ] 	Batch(2900/6809) done. Loss: 1.9693  lr:0.010000
[ Thu Jul 11 23:08:03 2024 ] 
Training: Epoch [6/120], Step [2999], Loss: 1.3008346557617188, Training Accuracy: 64.97500000000001
[ Thu Jul 11 23:08:04 2024 ] 	Batch(3000/6809) done. Loss: 1.0643  lr:0.010000
[ Thu Jul 11 23:08:22 2024 ] 	Batch(3100/6809) done. Loss: 0.9103  lr:0.010000
[ Thu Jul 11 23:08:40 2024 ] 	Batch(3200/6809) done. Loss: 1.3063  lr:0.010000
[ Thu Jul 11 23:08:58 2024 ] 	Batch(3300/6809) done. Loss: 0.8725  lr:0.010000
[ Thu Jul 11 23:09:16 2024 ] 	Batch(3400/6809) done. Loss: 1.6639  lr:0.010000
[ Thu Jul 11 23:09:33 2024 ] 
Training: Epoch [6/120], Step [3499], Loss: 1.6043193340301514, Training Accuracy: 65.02142857142857
[ Thu Jul 11 23:09:34 2024 ] 	Batch(3500/6809) done. Loss: 1.3164  lr:0.010000
[ Thu Jul 11 23:09:52 2024 ] 	Batch(3600/6809) done. Loss: 0.8049  lr:0.010000
[ Thu Jul 11 23:10:10 2024 ] 	Batch(3700/6809) done. Loss: 1.1822  lr:0.010000
[ Thu Jul 11 23:10:27 2024 ] 	Batch(3800/6809) done. Loss: 0.7377  lr:0.010000
[ Thu Jul 11 23:10:45 2024 ] 	Batch(3900/6809) done. Loss: 0.3346  lr:0.010000
[ Thu Jul 11 23:11:03 2024 ] 
Training: Epoch [6/120], Step [3999], Loss: 0.8635636568069458, Training Accuracy: 65.203125
[ Thu Jul 11 23:11:03 2024 ] 	Batch(4000/6809) done. Loss: 1.3814  lr:0.010000
[ Thu Jul 11 23:11:22 2024 ] 	Batch(4100/6809) done. Loss: 1.7710  lr:0.010000
[ Thu Jul 11 23:11:40 2024 ] 	Batch(4200/6809) done. Loss: 0.8316  lr:0.010000
[ Thu Jul 11 23:11:58 2024 ] 	Batch(4300/6809) done. Loss: 0.0872  lr:0.010000
[ Thu Jul 11 23:12:16 2024 ] 	Batch(4400/6809) done. Loss: 1.2567  lr:0.010000
[ Thu Jul 11 23:12:33 2024 ] 
Training: Epoch [6/120], Step [4499], Loss: 0.6648305058479309, Training Accuracy: 65.34722222222223
[ Thu Jul 11 23:12:33 2024 ] 	Batch(4500/6809) done. Loss: 0.5958  lr:0.010000
[ Thu Jul 11 23:12:52 2024 ] 	Batch(4600/6809) done. Loss: 0.9618  lr:0.010000
[ Thu Jul 11 23:13:11 2024 ] 	Batch(4700/6809) done. Loss: 2.5804  lr:0.010000
[ Thu Jul 11 23:13:29 2024 ] 	Batch(4800/6809) done. Loss: 1.0893  lr:0.010000
[ Thu Jul 11 23:13:47 2024 ] 	Batch(4900/6809) done. Loss: 0.4415  lr:0.010000
[ Thu Jul 11 23:14:05 2024 ] 
Training: Epoch [6/120], Step [4999], Loss: 1.4423898458480835, Training Accuracy: 65.405
[ Thu Jul 11 23:14:05 2024 ] 	Batch(5000/6809) done. Loss: 0.6969  lr:0.010000
[ Thu Jul 11 23:14:23 2024 ] 	Batch(5100/6809) done. Loss: 1.4706  lr:0.010000
[ Thu Jul 11 23:14:41 2024 ] 	Batch(5200/6809) done. Loss: 1.5207  lr:0.010000
[ Thu Jul 11 23:14:59 2024 ] 	Batch(5300/6809) done. Loss: 0.5494  lr:0.010000
[ Thu Jul 11 23:15:17 2024 ] 	Batch(5400/6809) done. Loss: 0.5739  lr:0.010000
[ Thu Jul 11 23:15:35 2024 ] 
Training: Epoch [6/120], Step [5499], Loss: 0.34147173166275024, Training Accuracy: 65.44772727272728
[ Thu Jul 11 23:15:35 2024 ] 	Batch(5500/6809) done. Loss: 0.7245  lr:0.010000
[ Thu Jul 11 23:15:53 2024 ] 	Batch(5600/6809) done. Loss: 1.9487  lr:0.010000
[ Thu Jul 11 23:16:11 2024 ] 	Batch(5700/6809) done. Loss: 2.2015  lr:0.010000
[ Thu Jul 11 23:16:30 2024 ] 	Batch(5800/6809) done. Loss: 1.0167  lr:0.010000
[ Thu Jul 11 23:16:48 2024 ] 	Batch(5900/6809) done. Loss: 0.6640  lr:0.010000
[ Thu Jul 11 23:17:07 2024 ] 
Training: Epoch [6/120], Step [5999], Loss: 1.540276050567627, Training Accuracy: 65.55
[ Thu Jul 11 23:17:07 2024 ] 	Batch(6000/6809) done. Loss: 1.0410  lr:0.010000
[ Thu Jul 11 23:17:25 2024 ] 	Batch(6100/6809) done. Loss: 0.9160  lr:0.010000
[ Thu Jul 11 23:17:43 2024 ] 	Batch(6200/6809) done. Loss: 1.6867  lr:0.010000
[ Thu Jul 11 23:18:01 2024 ] 	Batch(6300/6809) done. Loss: 1.8198  lr:0.010000
[ Thu Jul 11 23:18:19 2024 ] 	Batch(6400/6809) done. Loss: 1.4763  lr:0.010000
[ Thu Jul 11 23:18:36 2024 ] 
Training: Epoch [6/120], Step [6499], Loss: 1.5907635688781738, Training Accuracy: 65.56538461538462
[ Thu Jul 11 23:18:36 2024 ] 	Batch(6500/6809) done. Loss: 1.4102  lr:0.010000
[ Thu Jul 11 23:18:54 2024 ] 	Batch(6600/6809) done. Loss: 0.5409  lr:0.010000
[ Thu Jul 11 23:19:12 2024 ] 	Batch(6700/6809) done. Loss: 1.0130  lr:0.010000
[ Thu Jul 11 23:19:31 2024 ] 	Batch(6800/6809) done. Loss: 1.0614  lr:0.010000
[ Thu Jul 11 23:19:32 2024 ] 	Mean training loss: 1.1553.
[ Thu Jul 11 23:19:32 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Thu Jul 11 23:19:32 2024 ] Training epoch: 8
[ Thu Jul 11 23:19:33 2024 ] 	Batch(0/6809) done. Loss: 1.0953  lr:0.010000
[ Thu Jul 11 23:19:51 2024 ] 	Batch(100/6809) done. Loss: 1.8938  lr:0.010000
[ Thu Jul 11 23:20:08 2024 ] 	Batch(200/6809) done. Loss: 1.7691  lr:0.010000
[ Thu Jul 11 23:20:26 2024 ] 	Batch(300/6809) done. Loss: 1.3114  lr:0.010000
[ Thu Jul 11 23:20:44 2024 ] 	Batch(400/6809) done. Loss: 0.3434  lr:0.010000
[ Thu Jul 11 23:21:02 2024 ] 
Training: Epoch [7/120], Step [499], Loss: 1.0333330631256104, Training Accuracy: 68.22500000000001
[ Thu Jul 11 23:21:02 2024 ] 	Batch(500/6809) done. Loss: 0.7181  lr:0.010000
[ Thu Jul 11 23:21:20 2024 ] 	Batch(600/6809) done. Loss: 0.2751  lr:0.010000
[ Thu Jul 11 23:21:38 2024 ] 	Batch(700/6809) done. Loss: 1.4033  lr:0.010000
[ Thu Jul 11 23:21:56 2024 ] 	Batch(800/6809) done. Loss: 1.1116  lr:0.010000
[ Thu Jul 11 23:22:14 2024 ] 	Batch(900/6809) done. Loss: 1.2075  lr:0.010000
[ Thu Jul 11 23:22:32 2024 ] 
Training: Epoch [7/120], Step [999], Loss: 0.6802242994308472, Training Accuracy: 67.825
[ Thu Jul 11 23:22:32 2024 ] 	Batch(1000/6809) done. Loss: 1.6362  lr:0.010000
[ Thu Jul 11 23:22:50 2024 ] 	Batch(1100/6809) done. Loss: 0.8191  lr:0.010000
[ Thu Jul 11 23:23:08 2024 ] 	Batch(1200/6809) done. Loss: 1.3751  lr:0.010000
[ Thu Jul 11 23:23:26 2024 ] 	Batch(1300/6809) done. Loss: 0.7120  lr:0.010000
[ Thu Jul 11 23:23:43 2024 ] 	Batch(1400/6809) done. Loss: 0.7642  lr:0.010000
[ Thu Jul 11 23:24:01 2024 ] 
Training: Epoch [7/120], Step [1499], Loss: 0.44376039505004883, Training Accuracy: 67.71666666666667
[ Thu Jul 11 23:24:01 2024 ] 	Batch(1500/6809) done. Loss: 2.3287  lr:0.010000
[ Thu Jul 11 23:24:19 2024 ] 	Batch(1600/6809) done. Loss: 1.4462  lr:0.010000
[ Thu Jul 11 23:24:37 2024 ] 	Batch(1700/6809) done. Loss: 1.1797  lr:0.010000
[ Thu Jul 11 23:24:55 2024 ] 	Batch(1800/6809) done. Loss: 0.9175  lr:0.010000
[ Thu Jul 11 23:25:13 2024 ] 	Batch(1900/6809) done. Loss: 0.8006  lr:0.010000
[ Thu Jul 11 23:25:31 2024 ] 
Training: Epoch [7/120], Step [1999], Loss: 2.2594680786132812, Training Accuracy: 67.30625
[ Thu Jul 11 23:25:31 2024 ] 	Batch(2000/6809) done. Loss: 2.5555  lr:0.010000
[ Thu Jul 11 23:25:49 2024 ] 	Batch(2100/6809) done. Loss: 1.6093  lr:0.010000
[ Thu Jul 11 23:26:07 2024 ] 	Batch(2200/6809) done. Loss: 0.6378  lr:0.010000
[ Thu Jul 11 23:26:25 2024 ] 	Batch(2300/6809) done. Loss: 1.0383  lr:0.010000
[ Thu Jul 11 23:26:43 2024 ] 	Batch(2400/6809) done. Loss: 0.7284  lr:0.010000
[ Thu Jul 11 23:27:01 2024 ] 
Training: Epoch [7/120], Step [2499], Loss: 1.2068744897842407, Training Accuracy: 67.325
[ Thu Jul 11 23:27:01 2024 ] 	Batch(2500/6809) done. Loss: 0.2238  lr:0.010000
[ Thu Jul 11 23:27:19 2024 ] 	Batch(2600/6809) done. Loss: 0.4259  lr:0.010000
[ Thu Jul 11 23:27:37 2024 ] 	Batch(2700/6809) done. Loss: 0.7558  lr:0.010000
[ Thu Jul 11 23:27:55 2024 ] 	Batch(2800/6809) done. Loss: 1.7349  lr:0.010000
[ Thu Jul 11 23:28:13 2024 ] 	Batch(2900/6809) done. Loss: 0.2675  lr:0.010000
[ Thu Jul 11 23:28:31 2024 ] 
Training: Epoch [7/120], Step [2999], Loss: 0.6273476481437683, Training Accuracy: 67.37083333333334
[ Thu Jul 11 23:28:32 2024 ] 	Batch(3000/6809) done. Loss: 0.8011  lr:0.010000
[ Thu Jul 11 23:28:50 2024 ] 	Batch(3100/6809) done. Loss: 0.8049  lr:0.010000
[ Thu Jul 11 23:29:09 2024 ] 	Batch(3200/6809) done. Loss: 0.6867  lr:0.010000
[ Thu Jul 11 23:29:27 2024 ] 	Batch(3300/6809) done. Loss: 1.0268  lr:0.010000
[ Thu Jul 11 23:29:45 2024 ] 	Batch(3400/6809) done. Loss: 2.6107  lr:0.010000
[ Thu Jul 11 23:30:03 2024 ] 
Training: Epoch [7/120], Step [3499], Loss: 0.9027171730995178, Training Accuracy: 67.52142857142857
[ Thu Jul 11 23:30:03 2024 ] 	Batch(3500/6809) done. Loss: 1.1006  lr:0.010000
[ Thu Jul 11 23:30:21 2024 ] 	Batch(3600/6809) done. Loss: 0.5863  lr:0.010000
[ Thu Jul 11 23:30:39 2024 ] 	Batch(3700/6809) done. Loss: 0.4252  lr:0.010000
[ Thu Jul 11 23:30:57 2024 ] 	Batch(3800/6809) done. Loss: 1.1175  lr:0.010000
[ Thu Jul 11 23:31:15 2024 ] 	Batch(3900/6809) done. Loss: 0.3743  lr:0.010000
[ Thu Jul 11 23:31:33 2024 ] 
Training: Epoch [7/120], Step [3999], Loss: 0.8448868989944458, Training Accuracy: 67.5125
[ Thu Jul 11 23:31:33 2024 ] 	Batch(4000/6809) done. Loss: 0.5098  lr:0.010000
[ Thu Jul 11 23:31:51 2024 ] 	Batch(4100/6809) done. Loss: 1.4693  lr:0.010000
[ Thu Jul 11 23:32:09 2024 ] 	Batch(4200/6809) done. Loss: 0.6679  lr:0.010000
[ Thu Jul 11 23:32:27 2024 ] 	Batch(4300/6809) done. Loss: 0.7049  lr:0.010000
[ Thu Jul 11 23:32:45 2024 ] 	Batch(4400/6809) done. Loss: 1.4743  lr:0.010000
[ Thu Jul 11 23:33:03 2024 ] 
Training: Epoch [7/120], Step [4499], Loss: 1.266953468322754, Training Accuracy: 67.70277777777778
[ Thu Jul 11 23:33:03 2024 ] 	Batch(4500/6809) done. Loss: 1.9660  lr:0.010000
[ Thu Jul 11 23:33:22 2024 ] 	Batch(4600/6809) done. Loss: 1.3192  lr:0.010000
[ Thu Jul 11 23:33:40 2024 ] 	Batch(4700/6809) done. Loss: 2.4514  lr:0.010000
[ Thu Jul 11 23:33:59 2024 ] 	Batch(4800/6809) done. Loss: 1.2031  lr:0.010000
[ Thu Jul 11 23:34:18 2024 ] 	Batch(4900/6809) done. Loss: 2.0304  lr:0.010000
[ Thu Jul 11 23:34:36 2024 ] 
Training: Epoch [7/120], Step [4999], Loss: 0.9452147483825684, Training Accuracy: 67.705
[ Thu Jul 11 23:34:36 2024 ] 	Batch(5000/6809) done. Loss: 1.8847  lr:0.010000
[ Thu Jul 11 23:34:55 2024 ] 	Batch(5100/6809) done. Loss: 0.6378  lr:0.010000
[ Thu Jul 11 23:35:13 2024 ] 	Batch(5200/6809) done. Loss: 1.7404  lr:0.010000
[ Thu Jul 11 23:35:31 2024 ] 	Batch(5300/6809) done. Loss: 1.0791  lr:0.010000
[ Thu Jul 11 23:35:49 2024 ] 	Batch(5400/6809) done. Loss: 0.4261  lr:0.010000
[ Thu Jul 11 23:36:06 2024 ] 
Training: Epoch [7/120], Step [5499], Loss: 0.5903925895690918, Training Accuracy: 67.76136363636364
[ Thu Jul 11 23:36:06 2024 ] 	Batch(5500/6809) done. Loss: 0.7562  lr:0.010000
[ Thu Jul 11 23:36:25 2024 ] 	Batch(5600/6809) done. Loss: 0.9297  lr:0.010000
[ Thu Jul 11 23:36:43 2024 ] 	Batch(5700/6809) done. Loss: 1.2017  lr:0.010000
[ Thu Jul 11 23:37:01 2024 ] 	Batch(5800/6809) done. Loss: 0.8509  lr:0.010000
[ Thu Jul 11 23:37:19 2024 ] 	Batch(5900/6809) done. Loss: 0.4916  lr:0.010000
[ Thu Jul 11 23:37:36 2024 ] 
Training: Epoch [7/120], Step [5999], Loss: 0.6161407232284546, Training Accuracy: 67.77083333333334
[ Thu Jul 11 23:37:37 2024 ] 	Batch(6000/6809) done. Loss: 1.9483  lr:0.010000
[ Thu Jul 11 23:37:55 2024 ] 	Batch(6100/6809) done. Loss: 0.9968  lr:0.010000
[ Thu Jul 11 23:38:13 2024 ] 	Batch(6200/6809) done. Loss: 0.7336  lr:0.010000
[ Thu Jul 11 23:38:30 2024 ] 	Batch(6300/6809) done. Loss: 1.4110  lr:0.010000
[ Thu Jul 11 23:38:49 2024 ] 	Batch(6400/6809) done. Loss: 2.1682  lr:0.010000
[ Thu Jul 11 23:39:07 2024 ] 
Training: Epoch [7/120], Step [6499], Loss: 1.3115891218185425, Training Accuracy: 67.8576923076923
[ Thu Jul 11 23:39:07 2024 ] 	Batch(6500/6809) done. Loss: 1.4650  lr:0.010000
[ Thu Jul 11 23:39:26 2024 ] 	Batch(6600/6809) done. Loss: 0.2143  lr:0.010000
[ Thu Jul 11 23:39:44 2024 ] 	Batch(6700/6809) done. Loss: 0.8694  lr:0.010000
[ Thu Jul 11 23:40:03 2024 ] 	Batch(6800/6809) done. Loss: 2.1421  lr:0.010000
[ Thu Jul 11 23:40:04 2024 ] 	Mean training loss: 1.0884.
[ Thu Jul 11 23:40:04 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Thu Jul 11 23:40:04 2024 ] Training epoch: 9
[ Thu Jul 11 23:40:05 2024 ] 	Batch(0/6809) done. Loss: 0.3911  lr:0.010000
[ Thu Jul 11 23:40:23 2024 ] 	Batch(100/6809) done. Loss: 0.7301  lr:0.010000
[ Thu Jul 11 23:40:42 2024 ] 	Batch(200/6809) done. Loss: 1.5559  lr:0.010000
[ Thu Jul 11 23:41:00 2024 ] 	Batch(300/6809) done. Loss: 0.4329  lr:0.010000
[ Thu Jul 11 23:41:18 2024 ] 	Batch(400/6809) done. Loss: 0.7357  lr:0.010000
[ Thu Jul 11 23:41:36 2024 ] 
Training: Epoch [8/120], Step [499], Loss: 0.656460165977478, Training Accuracy: 70.025
[ Thu Jul 11 23:41:36 2024 ] 	Batch(500/6809) done. Loss: 0.4946  lr:0.010000
[ Thu Jul 11 23:41:54 2024 ] 	Batch(600/6809) done. Loss: 0.4726  lr:0.010000
[ Thu Jul 11 23:42:12 2024 ] 	Batch(700/6809) done. Loss: 1.5770  lr:0.010000
[ Thu Jul 11 23:42:30 2024 ] 	Batch(800/6809) done. Loss: 1.1784  lr:0.010000
[ Thu Jul 11 23:42:48 2024 ] 	Batch(900/6809) done. Loss: 1.2475  lr:0.010000
[ Thu Jul 11 23:43:07 2024 ] 
Training: Epoch [8/120], Step [999], Loss: 0.5468077063560486, Training Accuracy: 69.5
[ Thu Jul 11 23:43:07 2024 ] 	Batch(1000/6809) done. Loss: 1.1352  lr:0.010000
[ Thu Jul 11 23:43:25 2024 ] 	Batch(1100/6809) done. Loss: 0.3846  lr:0.010000
[ Thu Jul 11 23:43:43 2024 ] 	Batch(1200/6809) done. Loss: 0.7827  lr:0.010000
[ Thu Jul 11 23:44:01 2024 ] 	Batch(1300/6809) done. Loss: 1.2581  lr:0.010000
[ Thu Jul 11 23:44:20 2024 ] 	Batch(1400/6809) done. Loss: 1.3982  lr:0.010000
[ Thu Jul 11 23:44:38 2024 ] 
Training: Epoch [8/120], Step [1499], Loss: 1.150426983833313, Training Accuracy: 69.425
[ Thu Jul 11 23:44:38 2024 ] 	Batch(1500/6809) done. Loss: 1.2262  lr:0.010000
[ Thu Jul 11 23:44:57 2024 ] 	Batch(1600/6809) done. Loss: 0.9629  lr:0.010000
[ Thu Jul 11 23:45:16 2024 ] 	Batch(1700/6809) done. Loss: 1.7914  lr:0.010000
[ Thu Jul 11 23:45:34 2024 ] 	Batch(1800/6809) done. Loss: 1.7217  lr:0.010000
[ Thu Jul 11 23:45:53 2024 ] 	Batch(1900/6809) done. Loss: 1.1916  lr:0.010000
[ Thu Jul 11 23:46:11 2024 ] 
Training: Epoch [8/120], Step [1999], Loss: 0.735284149646759, Training Accuracy: 69.4125
[ Thu Jul 11 23:46:11 2024 ] 	Batch(2000/6809) done. Loss: 1.4089  lr:0.010000
[ Thu Jul 11 23:46:29 2024 ] 	Batch(2100/6809) done. Loss: 0.4367  lr:0.010000
[ Thu Jul 11 23:46:47 2024 ] 	Batch(2200/6809) done. Loss: 0.5019  lr:0.010000
[ Thu Jul 11 23:47:05 2024 ] 	Batch(2300/6809) done. Loss: 1.1184  lr:0.010000
[ Thu Jul 11 23:47:23 2024 ] 	Batch(2400/6809) done. Loss: 0.4922  lr:0.010000
[ Thu Jul 11 23:47:41 2024 ] 
Training: Epoch [8/120], Step [2499], Loss: 1.8885763883590698, Training Accuracy: 69.415
[ Thu Jul 11 23:47:41 2024 ] 	Batch(2500/6809) done. Loss: 1.5798  lr:0.010000
[ Thu Jul 11 23:47:59 2024 ] 	Batch(2600/6809) done. Loss: 0.8976  lr:0.010000
[ Thu Jul 11 23:48:17 2024 ] 	Batch(2700/6809) done. Loss: 0.6540  lr:0.010000
[ Thu Jul 11 23:48:35 2024 ] 	Batch(2800/6809) done. Loss: 1.6242  lr:0.010000
[ Thu Jul 11 23:48:53 2024 ] 	Batch(2900/6809) done. Loss: 1.2077  lr:0.010000
[ Thu Jul 11 23:49:10 2024 ] 
Training: Epoch [8/120], Step [2999], Loss: 0.4752116799354553, Training Accuracy: 69.41666666666667
[ Thu Jul 11 23:49:11 2024 ] 	Batch(3000/6809) done. Loss: 1.3871  lr:0.010000
[ Thu Jul 11 23:49:28 2024 ] 	Batch(3100/6809) done. Loss: 1.6447  lr:0.010000
[ Thu Jul 11 23:49:47 2024 ] 	Batch(3200/6809) done. Loss: 2.0250  lr:0.010000
[ Thu Jul 11 23:50:05 2024 ] 	Batch(3300/6809) done. Loss: 0.4015  lr:0.010000
[ Thu Jul 11 23:50:24 2024 ] 	Batch(3400/6809) done. Loss: 1.7241  lr:0.010000
[ Thu Jul 11 23:50:42 2024 ] 
Training: Epoch [8/120], Step [3499], Loss: 1.001039743423462, Training Accuracy: 69.66428571428571
[ Thu Jul 11 23:50:42 2024 ] 	Batch(3500/6809) done. Loss: 2.5024  lr:0.010000
[ Thu Jul 11 23:51:00 2024 ] 	Batch(3600/6809) done. Loss: 0.4258  lr:0.010000
[ Thu Jul 11 23:51:18 2024 ] 	Batch(3700/6809) done. Loss: 1.4948  lr:0.010000
[ Thu Jul 11 23:51:36 2024 ] 	Batch(3800/6809) done. Loss: 0.6551  lr:0.010000
[ Thu Jul 11 23:51:54 2024 ] 	Batch(3900/6809) done. Loss: 0.8054  lr:0.010000
[ Thu Jul 11 23:52:12 2024 ] 
Training: Epoch [8/120], Step [3999], Loss: 0.5481645464897156, Training Accuracy: 69.75312500000001
[ Thu Jul 11 23:52:12 2024 ] 	Batch(4000/6809) done. Loss: 1.1966  lr:0.010000
[ Thu Jul 11 23:52:30 2024 ] 	Batch(4100/6809) done. Loss: 0.4608  lr:0.010000
[ Thu Jul 11 23:52:48 2024 ] 	Batch(4200/6809) done. Loss: 0.4334  lr:0.010000
[ Thu Jul 11 23:53:06 2024 ] 	Batch(4300/6809) done. Loss: 1.0178  lr:0.010000
[ Thu Jul 11 23:53:24 2024 ] 	Batch(4400/6809) done. Loss: 0.3596  lr:0.010000
[ Thu Jul 11 23:53:42 2024 ] 
Training: Epoch [8/120], Step [4499], Loss: 0.9806627035140991, Training Accuracy: 69.81944444444444
[ Thu Jul 11 23:53:42 2024 ] 	Batch(4500/6809) done. Loss: 1.0283  lr:0.010000
[ Thu Jul 11 23:54:00 2024 ] 	Batch(4600/6809) done. Loss: 1.0765  lr:0.010000
[ Thu Jul 11 23:54:18 2024 ] 	Batch(4700/6809) done. Loss: 2.6692  lr:0.010000
[ Thu Jul 11 23:54:36 2024 ] 	Batch(4800/6809) done. Loss: 0.9810  lr:0.010000
[ Thu Jul 11 23:54:54 2024 ] 	Batch(4900/6809) done. Loss: 2.4740  lr:0.010000
[ Thu Jul 11 23:55:11 2024 ] 
Training: Epoch [8/120], Step [4999], Loss: 1.211626648902893, Training Accuracy: 69.83500000000001
[ Thu Jul 11 23:55:11 2024 ] 	Batch(5000/6809) done. Loss: 0.4076  lr:0.010000
[ Thu Jul 11 23:55:29 2024 ] 	Batch(5100/6809) done. Loss: 0.2833  lr:0.010000
[ Thu Jul 11 23:55:47 2024 ] 	Batch(5200/6809) done. Loss: 1.0516  lr:0.010000
[ Thu Jul 11 23:56:05 2024 ] 	Batch(5300/6809) done. Loss: 0.5777  lr:0.010000
[ Thu Jul 11 23:56:23 2024 ] 	Batch(5400/6809) done. Loss: 0.8170  lr:0.010000
[ Thu Jul 11 23:56:41 2024 ] 
Training: Epoch [8/120], Step [5499], Loss: 0.8806964159011841, Training Accuracy: 69.93863636363636
[ Thu Jul 11 23:56:41 2024 ] 	Batch(5500/6809) done. Loss: 0.5591  lr:0.010000
[ Thu Jul 11 23:56:59 2024 ] 	Batch(5600/6809) done. Loss: 1.0111  lr:0.010000
[ Thu Jul 11 23:57:17 2024 ] 	Batch(5700/6809) done. Loss: 1.3269  lr:0.010000
[ Thu Jul 11 23:57:35 2024 ] 	Batch(5800/6809) done. Loss: 1.0533  lr:0.010000
[ Thu Jul 11 23:57:53 2024 ] 	Batch(5900/6809) done. Loss: 1.5568  lr:0.010000
[ Thu Jul 11 23:58:11 2024 ] 
Training: Epoch [8/120], Step [5999], Loss: 1.3067023754119873, Training Accuracy: 69.96666666666667
[ Thu Jul 11 23:58:11 2024 ] 	Batch(6000/6809) done. Loss: 1.0285  lr:0.010000
[ Thu Jul 11 23:58:29 2024 ] 	Batch(6100/6809) done. Loss: 0.3635  lr:0.010000
[ Thu Jul 11 23:58:47 2024 ] 	Batch(6200/6809) done. Loss: 0.2784  lr:0.010000
[ Thu Jul 11 23:59:05 2024 ] 	Batch(6300/6809) done. Loss: 0.5691  lr:0.010000
[ Thu Jul 11 23:59:23 2024 ] 	Batch(6400/6809) done. Loss: 0.8400  lr:0.010000
[ Thu Jul 11 23:59:41 2024 ] 
Training: Epoch [8/120], Step [6499], Loss: 0.6497472524642944, Training Accuracy: 69.97307692307693
[ Thu Jul 11 23:59:41 2024 ] 	Batch(6500/6809) done. Loss: 1.4082  lr:0.010000
[ Thu Jul 11 23:59:59 2024 ] 	Batch(6600/6809) done. Loss: 1.0207  lr:0.010000
[ Fri Jul 12 00:00:17 2024 ] 	Batch(6700/6809) done. Loss: 0.8569  lr:0.010000
[ Fri Jul 12 00:00:35 2024 ] 	Batch(6800/6809) done. Loss: 0.8326  lr:0.010000
[ Fri Jul 12 00:00:37 2024 ] 	Mean training loss: 1.0092.
[ Fri Jul 12 00:00:37 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 00:00:37 2024 ] Training epoch: 10
[ Fri Jul 12 00:00:37 2024 ] 	Batch(0/6809) done. Loss: 0.6144  lr:0.010000
[ Fri Jul 12 00:00:56 2024 ] 	Batch(100/6809) done. Loss: 0.8607  lr:0.010000
[ Fri Jul 12 00:01:14 2024 ] 	Batch(200/6809) done. Loss: 1.1936  lr:0.010000
[ Fri Jul 12 00:01:32 2024 ] 	Batch(300/6809) done. Loss: 0.8101  lr:0.010000
[ Fri Jul 12 00:01:51 2024 ] 	Batch(400/6809) done. Loss: 0.4545  lr:0.010000
[ Fri Jul 12 00:02:09 2024 ] 
Training: Epoch [9/120], Step [499], Loss: 0.6820991039276123, Training Accuracy: 73.65
[ Fri Jul 12 00:02:09 2024 ] 	Batch(500/6809) done. Loss: 1.2126  lr:0.010000
[ Fri Jul 12 00:02:28 2024 ] 	Batch(600/6809) done. Loss: 1.9222  lr:0.010000
[ Fri Jul 12 00:02:46 2024 ] 	Batch(700/6809) done. Loss: 0.5132  lr:0.010000
[ Fri Jul 12 00:03:04 2024 ] 	Batch(800/6809) done. Loss: 0.4394  lr:0.010000
[ Fri Jul 12 00:03:23 2024 ] 	Batch(900/6809) done. Loss: 1.1511  lr:0.010000
[ Fri Jul 12 00:03:41 2024 ] 
Training: Epoch [9/120], Step [999], Loss: 1.369422197341919, Training Accuracy: 72.075
[ Fri Jul 12 00:03:41 2024 ] 	Batch(1000/6809) done. Loss: 0.4912  lr:0.010000
[ Fri Jul 12 00:03:59 2024 ] 	Batch(1100/6809) done. Loss: 0.9210  lr:0.010000
[ Fri Jul 12 00:04:18 2024 ] 	Batch(1200/6809) done. Loss: 0.9858  lr:0.010000
[ Fri Jul 12 00:04:36 2024 ] 	Batch(1300/6809) done. Loss: 0.6306  lr:0.010000
[ Fri Jul 12 00:04:54 2024 ] 	Batch(1400/6809) done. Loss: 1.0598  lr:0.010000
[ Fri Jul 12 00:05:11 2024 ] 
Training: Epoch [9/120], Step [1499], Loss: 0.49615341424942017, Training Accuracy: 71.79166666666667
[ Fri Jul 12 00:05:12 2024 ] 	Batch(1500/6809) done. Loss: 1.1420  lr:0.010000
[ Fri Jul 12 00:05:30 2024 ] 	Batch(1600/6809) done. Loss: 0.3789  lr:0.010000
[ Fri Jul 12 00:05:48 2024 ] 	Batch(1700/6809) done. Loss: 1.1388  lr:0.010000
[ Fri Jul 12 00:06:07 2024 ] 	Batch(1800/6809) done. Loss: 0.8382  lr:0.010000
[ Fri Jul 12 00:06:25 2024 ] 	Batch(1900/6809) done. Loss: 1.5859  lr:0.010000
[ Fri Jul 12 00:06:44 2024 ] 
Training: Epoch [9/120], Step [1999], Loss: 1.3467798233032227, Training Accuracy: 71.40625
[ Fri Jul 12 00:06:44 2024 ] 	Batch(2000/6809) done. Loss: 4.1989  lr:0.010000
[ Fri Jul 12 00:07:03 2024 ] 	Batch(2100/6809) done. Loss: 0.9988  lr:0.010000
[ Fri Jul 12 00:07:21 2024 ] 	Batch(2200/6809) done. Loss: 1.2165  lr:0.010000
[ Fri Jul 12 00:07:40 2024 ] 	Batch(2300/6809) done. Loss: 2.0796  lr:0.010000
[ Fri Jul 12 00:07:58 2024 ] 	Batch(2400/6809) done. Loss: 0.8072  lr:0.010000
[ Fri Jul 12 00:08:17 2024 ] 
Training: Epoch [9/120], Step [2499], Loss: 0.6427676677703857, Training Accuracy: 71.5
[ Fri Jul 12 00:08:17 2024 ] 	Batch(2500/6809) done. Loss: 0.3935  lr:0.010000
[ Fri Jul 12 00:08:35 2024 ] 	Batch(2600/6809) done. Loss: 0.7344  lr:0.010000
[ Fri Jul 12 00:08:54 2024 ] 	Batch(2700/6809) done. Loss: 0.5217  lr:0.010000
[ Fri Jul 12 00:09:13 2024 ] 	Batch(2800/6809) done. Loss: 0.5109  lr:0.010000
[ Fri Jul 12 00:09:30 2024 ] 	Batch(2900/6809) done. Loss: 1.7550  lr:0.010000
[ Fri Jul 12 00:09:48 2024 ] 
Training: Epoch [9/120], Step [2999], Loss: 0.8581162095069885, Training Accuracy: 71.55
[ Fri Jul 12 00:09:48 2024 ] 	Batch(3000/6809) done. Loss: 0.7194  lr:0.010000
[ Fri Jul 12 00:10:06 2024 ] 	Batch(3100/6809) done. Loss: 0.4931  lr:0.010000
[ Fri Jul 12 00:10:24 2024 ] 	Batch(3200/6809) done. Loss: 0.8799  lr:0.010000
[ Fri Jul 12 00:10:42 2024 ] 	Batch(3300/6809) done. Loss: 0.8344  lr:0.010000
[ Fri Jul 12 00:11:00 2024 ] 	Batch(3400/6809) done. Loss: 0.7171  lr:0.010000
[ Fri Jul 12 00:11:18 2024 ] 
Training: Epoch [9/120], Step [3499], Loss: 1.2623662948608398, Training Accuracy: 71.56785714285714
[ Fri Jul 12 00:11:18 2024 ] 	Batch(3500/6809) done. Loss: 0.5246  lr:0.010000
[ Fri Jul 12 00:11:36 2024 ] 	Batch(3600/6809) done. Loss: 0.9459  lr:0.010000
[ Fri Jul 12 00:11:54 2024 ] 	Batch(3700/6809) done. Loss: 0.4165  lr:0.010000
[ Fri Jul 12 00:12:12 2024 ] 	Batch(3800/6809) done. Loss: 1.1585  lr:0.010000
[ Fri Jul 12 00:12:30 2024 ] 	Batch(3900/6809) done. Loss: 1.4450  lr:0.010000
[ Fri Jul 12 00:12:48 2024 ] 
Training: Epoch [9/120], Step [3999], Loss: 1.7005823850631714, Training Accuracy: 71.55
[ Fri Jul 12 00:12:48 2024 ] 	Batch(4000/6809) done. Loss: 0.4538  lr:0.010000
[ Fri Jul 12 00:13:06 2024 ] 	Batch(4100/6809) done. Loss: 2.3536  lr:0.010000
[ Fri Jul 12 00:13:24 2024 ] 	Batch(4200/6809) done. Loss: 1.5839  lr:0.010000
[ Fri Jul 12 00:13:41 2024 ] 	Batch(4300/6809) done. Loss: 0.4046  lr:0.010000
[ Fri Jul 12 00:14:00 2024 ] 	Batch(4400/6809) done. Loss: 1.1563  lr:0.010000
[ Fri Jul 12 00:14:17 2024 ] 
Training: Epoch [9/120], Step [4499], Loss: 0.9845179319381714, Training Accuracy: 71.39999999999999
[ Fri Jul 12 00:14:17 2024 ] 	Batch(4500/6809) done. Loss: 0.3024  lr:0.010000
[ Fri Jul 12 00:14:35 2024 ] 	Batch(4600/6809) done. Loss: 0.9175  lr:0.010000
[ Fri Jul 12 00:14:53 2024 ] 	Batch(4700/6809) done. Loss: 0.4873  lr:0.010000
[ Fri Jul 12 00:15:11 2024 ] 	Batch(4800/6809) done. Loss: 0.4052  lr:0.010000
[ Fri Jul 12 00:15:30 2024 ] 	Batch(4900/6809) done. Loss: 1.8397  lr:0.010000
[ Fri Jul 12 00:15:48 2024 ] 
Training: Epoch [9/120], Step [4999], Loss: 0.3563048541545868, Training Accuracy: 71.33500000000001
[ Fri Jul 12 00:15:48 2024 ] 	Batch(5000/6809) done. Loss: 1.4383  lr:0.010000
[ Fri Jul 12 00:16:07 2024 ] 	Batch(5100/6809) done. Loss: 0.2641  lr:0.010000
[ Fri Jul 12 00:16:25 2024 ] 	Batch(5200/6809) done. Loss: 0.5611  lr:0.010000
[ Fri Jul 12 00:16:44 2024 ] 	Batch(5300/6809) done. Loss: 1.1724  lr:0.010000
[ Fri Jul 12 00:17:03 2024 ] 	Batch(5400/6809) done. Loss: 0.4393  lr:0.010000
[ Fri Jul 12 00:17:21 2024 ] 
Training: Epoch [9/120], Step [5499], Loss: 1.205434799194336, Training Accuracy: 71.27045454545454
[ Fri Jul 12 00:17:21 2024 ] 	Batch(5500/6809) done. Loss: 1.5194  lr:0.010000
[ Fri Jul 12 00:17:40 2024 ] 	Batch(5600/6809) done. Loss: 1.2722  lr:0.010000
[ Fri Jul 12 00:17:58 2024 ] 	Batch(5700/6809) done. Loss: 0.8427  lr:0.010000
[ Fri Jul 12 00:18:16 2024 ] 	Batch(5800/6809) done. Loss: 1.2834  lr:0.010000
[ Fri Jul 12 00:18:34 2024 ] 	Batch(5900/6809) done. Loss: 0.5624  lr:0.010000
[ Fri Jul 12 00:18:51 2024 ] 
Training: Epoch [9/120], Step [5999], Loss: 0.94424968957901, Training Accuracy: 71.28541666666666
[ Fri Jul 12 00:18:52 2024 ] 	Batch(6000/6809) done. Loss: 0.6809  lr:0.010000
[ Fri Jul 12 00:19:10 2024 ] 	Batch(6100/6809) done. Loss: 0.4124  lr:0.010000
[ Fri Jul 12 00:19:28 2024 ] 	Batch(6200/6809) done. Loss: 1.5388  lr:0.010000
[ Fri Jul 12 00:19:46 2024 ] 	Batch(6300/6809) done. Loss: 0.5796  lr:0.010000
[ Fri Jul 12 00:20:04 2024 ] 	Batch(6400/6809) done. Loss: 1.2851  lr:0.010000
[ Fri Jul 12 00:20:22 2024 ] 
Training: Epoch [9/120], Step [6499], Loss: 0.8117395639419556, Training Accuracy: 71.30576923076923
[ Fri Jul 12 00:20:22 2024 ] 	Batch(6500/6809) done. Loss: 0.8290  lr:0.010000
[ Fri Jul 12 00:20:40 2024 ] 	Batch(6600/6809) done. Loss: 0.8818  lr:0.010000
[ Fri Jul 12 00:20:58 2024 ] 	Batch(6700/6809) done. Loss: 1.9248  lr:0.010000
[ Fri Jul 12 00:21:16 2024 ] 	Batch(6800/6809) done. Loss: 0.9740  lr:0.010000
[ Fri Jul 12 00:21:17 2024 ] 	Mean training loss: 0.9673.
[ Fri Jul 12 00:21:17 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 00:21:17 2024 ] Eval epoch: 10
[ Fri Jul 12 00:26:52 2024 ] 	Mean val loss of 7435 batches: 1.410900782139934.
[ Fri Jul 12 00:26:52 2024 ] 
Validation: Epoch [9/120], Samples [40620.0/59477], Loss: 0.5783987045288086, Validation Accuracy: 68.29530742976276
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 1 : 266 / 500 = 53 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 2 : 328 / 499 = 65 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 3 : 332 / 500 = 66 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 4 : 403 / 502 = 80 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 5 : 445 / 502 = 88 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 6 : 420 / 502 = 83 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 7 : 474 / 497 = 95 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 8 : 477 / 498 = 95 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 9 : 443 / 500 = 88 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 10 : 29 / 500 = 5 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 11 : 88 / 498 = 17 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 12 : 345 / 499 = 69 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 13 : 460 / 502 = 91 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 14 : 468 / 504 = 92 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 15 : 358 / 502 = 71 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 16 : 245 / 502 = 48 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 17 : 348 / 504 = 69 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 18 : 407 / 504 = 80 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 19 : 437 / 502 = 87 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 20 : 400 / 502 = 79 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 21 : 436 / 503 = 86 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 22 : 332 / 504 = 65 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 23 : 419 / 503 = 83 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 24 : 377 / 504 = 74 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 25 : 457 / 504 = 90 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 26 : 473 / 504 = 93 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 27 : 354 / 501 = 70 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 28 : 327 / 502 = 65 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 29 : 348 / 502 = 69 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 30 : 317 / 501 = 63 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 31 : 392 / 504 = 77 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 32 : 360 / 503 = 71 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 33 : 37 / 503 = 7 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 34 : 464 / 504 = 92 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 35 : 377 / 503 = 74 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 36 : 368 / 502 = 73 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 37 : 352 / 504 = 69 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 38 : 370 / 504 = 73 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 39 : 467 / 498 = 93 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 40 : 393 / 504 = 77 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 41 : 403 / 503 = 80 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 42 : 388 / 504 = 76 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 43 : 247 / 503 = 49 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 44 : 397 / 504 = 78 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 45 : 369 / 504 = 73 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 46 : 281 / 504 = 55 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 47 : 279 / 503 = 55 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 48 : 272 / 503 = 54 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 49 : 316 / 499 = 63 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 50 : 344 / 502 = 68 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 51 : 443 / 503 = 88 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 52 : 319 / 504 = 63 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 53 : 353 / 497 = 71 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 54 : 431 / 480 = 89 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 55 : 339 / 504 = 67 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 56 : 396 / 503 = 78 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 57 : 461 / 504 = 91 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 58 : 485 / 499 = 97 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 59 : 463 / 503 = 92 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 60 : 384 / 479 = 80 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 61 : 336 / 484 = 69 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 62 : 368 / 487 = 75 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 63 : 421 / 489 = 86 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 64 : 331 / 488 = 67 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 65 : 396 / 490 = 80 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 66 : 201 / 488 = 41 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 67 : 304 / 490 = 62 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 68 : 170 / 490 = 34 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 69 : 384 / 490 = 78 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 70 : 110 / 490 = 22 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 71 : 297 / 490 = 60 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 72 : 59 / 488 = 12 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 73 : 250 / 486 = 51 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 74 : 129 / 481 = 26 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 75 : 156 / 488 = 31 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 76 : 223 / 489 = 45 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 77 : 155 / 488 = 31 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 78 : 278 / 488 = 56 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 79 : 422 / 490 = 86 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 80 : 377 / 489 = 77 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 81 : 267 / 491 = 54 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 82 : 265 / 491 = 53 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 83 : 92 / 489 = 18 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 84 : 365 / 489 = 74 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 85 : 208 / 489 = 42 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 86 : 405 / 491 = 82 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 87 : 399 / 492 = 81 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 88 : 327 / 491 = 66 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 89 : 309 / 492 = 62 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 90 : 60 / 490 = 12 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 91 : 315 / 482 = 65 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 92 : 285 / 490 = 58 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 93 : 319 / 487 = 65 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 94 : 422 / 489 = 86 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 95 : 420 / 490 = 85 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 96 : 460 / 491 = 93 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 97 : 461 / 490 = 94 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 98 : 439 / 491 = 89 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 99 : 429 / 491 = 87 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 100 : 411 / 491 = 83 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 101 : 362 / 491 = 73 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 102 : 216 / 492 = 43 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 103 : 350 / 492 = 71 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 104 : 114 / 491 = 23 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 105 : 253 / 491 = 51 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 106 : 176 / 492 = 35 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 107 : 324 / 491 = 65 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 108 : 379 / 492 = 77 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 109 : 305 / 490 = 62 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 110 : 338 / 491 = 68 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 111 : 396 / 492 = 80 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 112 : 428 / 492 = 86 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 113 : 395 / 491 = 80 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 114 : 371 / 491 = 75 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 115 : 353 / 492 = 71 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 116 : 343 / 491 = 69 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 117 : 401 / 492 = 81 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 118 : 414 / 490 = 84 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 119 : 426 / 492 = 86 %
[ Fri Jul 12 00:26:52 2024 ] Accuracy of 120 : 288 / 500 = 57 %
[ Fri Jul 12 00:26:52 2024 ] Training epoch: 11
[ Fri Jul 12 00:26:53 2024 ] 	Batch(0/6809) done. Loss: 0.6983  lr:0.010000
[ Fri Jul 12 00:27:11 2024 ] 	Batch(100/6809) done. Loss: 0.7708  lr:0.010000
[ Fri Jul 12 00:27:29 2024 ] 	Batch(200/6809) done. Loss: 0.5216  lr:0.010000
[ Fri Jul 12 00:27:47 2024 ] 	Batch(300/6809) done. Loss: 1.4916  lr:0.010000
[ Fri Jul 12 00:28:05 2024 ] 	Batch(400/6809) done. Loss: 0.5804  lr:0.010000
[ Fri Jul 12 00:28:22 2024 ] 
Training: Epoch [10/120], Step [499], Loss: 0.18467597663402557, Training Accuracy: 72.225
[ Fri Jul 12 00:28:23 2024 ] 	Batch(500/6809) done. Loss: 1.0120  lr:0.010000
[ Fri Jul 12 00:28:40 2024 ] 	Batch(600/6809) done. Loss: 1.2553  lr:0.010000
[ Fri Jul 12 00:28:58 2024 ] 	Batch(700/6809) done. Loss: 1.0099  lr:0.010000
[ Fri Jul 12 00:29:16 2024 ] 	Batch(800/6809) done. Loss: 1.2408  lr:0.010000
[ Fri Jul 12 00:29:34 2024 ] 	Batch(900/6809) done. Loss: 0.5317  lr:0.010000
[ Fri Jul 12 00:29:52 2024 ] 
Training: Epoch [10/120], Step [999], Loss: 1.0838462114334106, Training Accuracy: 71.98750000000001
[ Fri Jul 12 00:29:52 2024 ] 	Batch(1000/6809) done. Loss: 0.4510  lr:0.010000
[ Fri Jul 12 00:30:10 2024 ] 	Batch(1100/6809) done. Loss: 0.6412  lr:0.010000
[ Fri Jul 12 00:30:28 2024 ] 	Batch(1200/6809) done. Loss: 0.5191  lr:0.010000
[ Fri Jul 12 00:30:46 2024 ] 	Batch(1300/6809) done. Loss: 0.7739  lr:0.010000
[ Fri Jul 12 00:31:04 2024 ] 	Batch(1400/6809) done. Loss: 0.9018  lr:0.010000
[ Fri Jul 12 00:31:22 2024 ] 
Training: Epoch [10/120], Step [1499], Loss: 0.4083191454410553, Training Accuracy: 72.31666666666666
[ Fri Jul 12 00:31:22 2024 ] 	Batch(1500/6809) done. Loss: 0.3202  lr:0.010000
[ Fri Jul 12 00:31:40 2024 ] 	Batch(1600/6809) done. Loss: 0.6175  lr:0.010000
[ Fri Jul 12 00:31:58 2024 ] 	Batch(1700/6809) done. Loss: 0.4807  lr:0.010000
[ Fri Jul 12 00:32:16 2024 ] 	Batch(1800/6809) done. Loss: 0.4620  lr:0.010000
[ Fri Jul 12 00:32:33 2024 ] 	Batch(1900/6809) done. Loss: 1.1058  lr:0.010000
[ Fri Jul 12 00:32:51 2024 ] 
Training: Epoch [10/120], Step [1999], Loss: 0.297807514667511, Training Accuracy: 72.3125
[ Fri Jul 12 00:32:51 2024 ] 	Batch(2000/6809) done. Loss: 1.5785  lr:0.010000
[ Fri Jul 12 00:33:09 2024 ] 	Batch(2100/6809) done. Loss: 0.4364  lr:0.010000
[ Fri Jul 12 00:33:27 2024 ] 	Batch(2200/6809) done. Loss: 0.9253  lr:0.010000
[ Fri Jul 12 00:33:45 2024 ] 	Batch(2300/6809) done. Loss: 0.7806  lr:0.010000
[ Fri Jul 12 00:34:03 2024 ] 	Batch(2400/6809) done. Loss: 1.2908  lr:0.010000
[ Fri Jul 12 00:34:21 2024 ] 
Training: Epoch [10/120], Step [2499], Loss: 1.031219244003296, Training Accuracy: 72.265
[ Fri Jul 12 00:34:21 2024 ] 	Batch(2500/6809) done. Loss: 0.2355  lr:0.010000
[ Fri Jul 12 00:34:39 2024 ] 	Batch(2600/6809) done. Loss: 1.2431  lr:0.010000
[ Fri Jul 12 00:34:57 2024 ] 	Batch(2700/6809) done. Loss: 0.3268  lr:0.010000
[ Fri Jul 12 00:35:15 2024 ] 	Batch(2800/6809) done. Loss: 0.5362  lr:0.010000
[ Fri Jul 12 00:35:33 2024 ] 	Batch(2900/6809) done. Loss: 0.5418  lr:0.010000
[ Fri Jul 12 00:35:50 2024 ] 
Training: Epoch [10/120], Step [2999], Loss: 0.2797964811325073, Training Accuracy: 72.32083333333334
[ Fri Jul 12 00:35:50 2024 ] 	Batch(3000/6809) done. Loss: 1.1925  lr:0.010000
[ Fri Jul 12 00:36:08 2024 ] 	Batch(3100/6809) done. Loss: 1.1582  lr:0.010000
[ Fri Jul 12 00:36:26 2024 ] 	Batch(3200/6809) done. Loss: 0.7708  lr:0.010000
[ Fri Jul 12 00:36:45 2024 ] 	Batch(3300/6809) done. Loss: 0.5457  lr:0.010000
[ Fri Jul 12 00:37:03 2024 ] 	Batch(3400/6809) done. Loss: 0.5831  lr:0.010000
[ Fri Jul 12 00:37:22 2024 ] 
Training: Epoch [10/120], Step [3499], Loss: 1.0405651330947876, Training Accuracy: 72.46071428571429
[ Fri Jul 12 00:37:22 2024 ] 	Batch(3500/6809) done. Loss: 1.0752  lr:0.010000
[ Fri Jul 12 00:37:41 2024 ] 	Batch(3600/6809) done. Loss: 0.4380  lr:0.010000
[ Fri Jul 12 00:37:59 2024 ] 	Batch(3700/6809) done. Loss: 0.9312  lr:0.010000
[ Fri Jul 12 00:38:16 2024 ] 	Batch(3800/6809) done. Loss: 0.9106  lr:0.010000
[ Fri Jul 12 00:38:34 2024 ] 	Batch(3900/6809) done. Loss: 0.7102  lr:0.010000
[ Fri Jul 12 00:38:52 2024 ] 
Training: Epoch [10/120], Step [3999], Loss: 0.8407074213027954, Training Accuracy: 72.475
[ Fri Jul 12 00:38:52 2024 ] 	Batch(4000/6809) done. Loss: 0.7374  lr:0.010000
[ Fri Jul 12 00:39:10 2024 ] 	Batch(4100/6809) done. Loss: 0.7859  lr:0.010000
[ Fri Jul 12 00:39:28 2024 ] 	Batch(4200/6809) done. Loss: 0.1154  lr:0.010000
[ Fri Jul 12 00:39:46 2024 ] 	Batch(4300/6809) done. Loss: 1.8689  lr:0.010000
[ Fri Jul 12 00:40:04 2024 ] 	Batch(4400/6809) done. Loss: 0.7627  lr:0.010000
[ Fri Jul 12 00:40:22 2024 ] 
Training: Epoch [10/120], Step [4499], Loss: 1.0309710502624512, Training Accuracy: 72.45833333333334
[ Fri Jul 12 00:40:22 2024 ] 	Batch(4500/6809) done. Loss: 0.8674  lr:0.010000
[ Fri Jul 12 00:40:40 2024 ] 	Batch(4600/6809) done. Loss: 0.1808  lr:0.010000
[ Fri Jul 12 00:40:58 2024 ] 	Batch(4700/6809) done. Loss: 1.4598  lr:0.010000
[ Fri Jul 12 00:41:16 2024 ] 	Batch(4800/6809) done. Loss: 0.6778  lr:0.010000
[ Fri Jul 12 00:41:34 2024 ] 	Batch(4900/6809) done. Loss: 2.2549  lr:0.010000
[ Fri Jul 12 00:41:51 2024 ] 
Training: Epoch [10/120], Step [4999], Loss: 0.6888803839683533, Training Accuracy: 72.50750000000001
[ Fri Jul 12 00:41:51 2024 ] 	Batch(5000/6809) done. Loss: 0.4561  lr:0.010000
[ Fri Jul 12 00:42:09 2024 ] 	Batch(5100/6809) done. Loss: 0.6109  lr:0.010000
[ Fri Jul 12 00:42:27 2024 ] 	Batch(5200/6809) done. Loss: 0.7416  lr:0.010000
[ Fri Jul 12 00:42:45 2024 ] 	Batch(5300/6809) done. Loss: 1.2778  lr:0.010000
[ Fri Jul 12 00:43:03 2024 ] 	Batch(5400/6809) done. Loss: 1.2829  lr:0.010000
[ Fri Jul 12 00:43:21 2024 ] 
Training: Epoch [10/120], Step [5499], Loss: 0.7450631260871887, Training Accuracy: 72.49545454545454
[ Fri Jul 12 00:43:21 2024 ] 	Batch(5500/6809) done. Loss: 0.8581  lr:0.010000
[ Fri Jul 12 00:43:39 2024 ] 	Batch(5600/6809) done. Loss: 0.4401  lr:0.010000
[ Fri Jul 12 00:43:57 2024 ] 	Batch(5700/6809) done. Loss: 0.9376  lr:0.010000
[ Fri Jul 12 00:44:16 2024 ] 	Batch(5800/6809) done. Loss: 0.5404  lr:0.010000
[ Fri Jul 12 00:44:34 2024 ] 	Batch(5900/6809) done. Loss: 1.0105  lr:0.010000
[ Fri Jul 12 00:44:52 2024 ] 
Training: Epoch [10/120], Step [5999], Loss: 0.9665408134460449, Training Accuracy: 72.41875
[ Fri Jul 12 00:44:52 2024 ] 	Batch(6000/6809) done. Loss: 0.6683  lr:0.010000
[ Fri Jul 12 00:45:10 2024 ] 	Batch(6100/6809) done. Loss: 0.6050  lr:0.010000
[ Fri Jul 12 00:45:28 2024 ] 	Batch(6200/6809) done. Loss: 1.4196  lr:0.010000
[ Fri Jul 12 00:45:46 2024 ] 	Batch(6300/6809) done. Loss: 1.9246  lr:0.010000
[ Fri Jul 12 00:46:04 2024 ] 	Batch(6400/6809) done. Loss: 0.8501  lr:0.010000
[ Fri Jul 12 00:46:22 2024 ] 
Training: Epoch [10/120], Step [6499], Loss: 0.9315292835235596, Training Accuracy: 72.43653846153846
[ Fri Jul 12 00:46:23 2024 ] 	Batch(6500/6809) done. Loss: 1.0184  lr:0.010000
[ Fri Jul 12 00:46:41 2024 ] 	Batch(6600/6809) done. Loss: 1.1921  lr:0.010000
[ Fri Jul 12 00:47:00 2024 ] 	Batch(6700/6809) done. Loss: 0.4610  lr:0.010000
[ Fri Jul 12 00:47:18 2024 ] 	Batch(6800/6809) done. Loss: 1.3161  lr:0.010000
[ Fri Jul 12 00:47:20 2024 ] 	Mean training loss: 0.9244.
[ Fri Jul 12 00:47:20 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 00:47:20 2024 ] Training epoch: 12
[ Fri Jul 12 00:47:20 2024 ] 	Batch(0/6809) done. Loss: 1.3103  lr:0.010000
[ Fri Jul 12 00:47:39 2024 ] 	Batch(100/6809) done. Loss: 1.4457  lr:0.010000
[ Fri Jul 12 00:47:57 2024 ] 	Batch(200/6809) done. Loss: 0.4691  lr:0.010000
[ Fri Jul 12 00:48:15 2024 ] 	Batch(300/6809) done. Loss: 1.1650  lr:0.010000
[ Fri Jul 12 00:48:33 2024 ] 	Batch(400/6809) done. Loss: 0.6073  lr:0.010000
[ Fri Jul 12 00:48:50 2024 ] 
Training: Epoch [11/120], Step [499], Loss: 0.18810173869132996, Training Accuracy: 73.925
[ Fri Jul 12 00:48:51 2024 ] 	Batch(500/6809) done. Loss: 0.8136  lr:0.010000
[ Fri Jul 12 00:49:09 2024 ] 	Batch(600/6809) done. Loss: 1.0944  lr:0.010000
[ Fri Jul 12 00:49:26 2024 ] 	Batch(700/6809) done. Loss: 1.6722  lr:0.010000
[ Fri Jul 12 00:49:45 2024 ] 	Batch(800/6809) done. Loss: 0.4783  lr:0.010000
[ Fri Jul 12 00:50:02 2024 ] 	Batch(900/6809) done. Loss: 0.4030  lr:0.010000
[ Fri Jul 12 00:50:20 2024 ] 
Training: Epoch [11/120], Step [999], Loss: 1.608407735824585, Training Accuracy: 73.925
[ Fri Jul 12 00:50:20 2024 ] 	Batch(1000/6809) done. Loss: 0.1029  lr:0.010000
[ Fri Jul 12 00:50:38 2024 ] 	Batch(1100/6809) done. Loss: 0.8525  lr:0.010000
[ Fri Jul 12 00:50:56 2024 ] 	Batch(1200/6809) done. Loss: 0.7165  lr:0.010000
[ Fri Jul 12 00:51:14 2024 ] 	Batch(1300/6809) done. Loss: 0.6975  lr:0.010000
[ Fri Jul 12 00:51:32 2024 ] 	Batch(1400/6809) done. Loss: 0.7279  lr:0.010000
[ Fri Jul 12 00:51:50 2024 ] 
Training: Epoch [11/120], Step [1499], Loss: 0.15652650594711304, Training Accuracy: 74.09166666666667
[ Fri Jul 12 00:51:50 2024 ] 	Batch(1500/6809) done. Loss: 0.0771  lr:0.010000
[ Fri Jul 12 00:52:08 2024 ] 	Batch(1600/6809) done. Loss: 0.4908  lr:0.010000
[ Fri Jul 12 00:52:26 2024 ] 	Batch(1700/6809) done. Loss: 0.8205  lr:0.010000
[ Fri Jul 12 00:52:45 2024 ] 	Batch(1800/6809) done. Loss: 0.3605  lr:0.010000
[ Fri Jul 12 00:53:03 2024 ] 	Batch(1900/6809) done. Loss: 0.7957  lr:0.010000
[ Fri Jul 12 00:53:22 2024 ] 
Training: Epoch [11/120], Step [1999], Loss: 1.2910237312316895, Training Accuracy: 74.425
[ Fri Jul 12 00:53:22 2024 ] 	Batch(2000/6809) done. Loss: 0.3386  lr:0.010000
[ Fri Jul 12 00:53:41 2024 ] 	Batch(2100/6809) done. Loss: 0.0942  lr:0.010000
[ Fri Jul 12 00:53:59 2024 ] 	Batch(2200/6809) done. Loss: 1.0985  lr:0.010000
[ Fri Jul 12 00:54:18 2024 ] 	Batch(2300/6809) done. Loss: 1.5494  lr:0.010000
[ Fri Jul 12 00:54:36 2024 ] 	Batch(2400/6809) done. Loss: 1.3093  lr:0.010000
[ Fri Jul 12 00:54:54 2024 ] 
Training: Epoch [11/120], Step [2499], Loss: 0.6284997463226318, Training Accuracy: 74.065
[ Fri Jul 12 00:54:54 2024 ] 	Batch(2500/6809) done. Loss: 0.3468  lr:0.010000
[ Fri Jul 12 00:55:12 2024 ] 	Batch(2600/6809) done. Loss: 1.1622  lr:0.010000
[ Fri Jul 12 00:55:30 2024 ] 	Batch(2700/6809) done. Loss: 0.6669  lr:0.010000
[ Fri Jul 12 00:55:48 2024 ] 	Batch(2800/6809) done. Loss: 0.4234  lr:0.010000
[ Fri Jul 12 00:56:06 2024 ] 	Batch(2900/6809) done. Loss: 0.7724  lr:0.010000
[ Fri Jul 12 00:56:24 2024 ] 
Training: Epoch [11/120], Step [2999], Loss: 1.3590317964553833, Training Accuracy: 73.925
[ Fri Jul 12 00:56:24 2024 ] 	Batch(3000/6809) done. Loss: 2.2974  lr:0.010000
[ Fri Jul 12 00:56:42 2024 ] 	Batch(3100/6809) done. Loss: 1.1120  lr:0.010000
[ Fri Jul 12 00:57:01 2024 ] 	Batch(3200/6809) done. Loss: 1.0209  lr:0.010000
[ Fri Jul 12 00:57:19 2024 ] 	Batch(3300/6809) done. Loss: 0.4147  lr:0.010000
[ Fri Jul 12 00:57:36 2024 ] 	Batch(3400/6809) done. Loss: 1.0215  lr:0.010000
[ Fri Jul 12 00:57:54 2024 ] 
Training: Epoch [11/120], Step [3499], Loss: 0.3119336664676666, Training Accuracy: 73.72142857142858
[ Fri Jul 12 00:57:54 2024 ] 	Batch(3500/6809) done. Loss: 0.4830  lr:0.010000
[ Fri Jul 12 00:58:12 2024 ] 	Batch(3600/6809) done. Loss: 0.6769  lr:0.010000
[ Fri Jul 12 00:58:31 2024 ] 	Batch(3700/6809) done. Loss: 0.7021  lr:0.010000
[ Fri Jul 12 00:58:49 2024 ] 	Batch(3800/6809) done. Loss: 0.8848  lr:0.010000
[ Fri Jul 12 00:59:08 2024 ] 	Batch(3900/6809) done. Loss: 0.7409  lr:0.010000
[ Fri Jul 12 00:59:26 2024 ] 
Training: Epoch [11/120], Step [3999], Loss: 1.0820558071136475, Training Accuracy: 73.784375
[ Fri Jul 12 00:59:27 2024 ] 	Batch(4000/6809) done. Loss: 0.6658  lr:0.010000
[ Fri Jul 12 00:59:45 2024 ] 	Batch(4100/6809) done. Loss: 0.9497  lr:0.010000
[ Fri Jul 12 01:00:02 2024 ] 	Batch(4200/6809) done. Loss: 0.6823  lr:0.010000
[ Fri Jul 12 01:00:20 2024 ] 	Batch(4300/6809) done. Loss: 1.5448  lr:0.010000
[ Fri Jul 12 01:00:38 2024 ] 	Batch(4400/6809) done. Loss: 1.6244  lr:0.010000
[ Fri Jul 12 01:00:57 2024 ] 
Training: Epoch [11/120], Step [4499], Loss: 0.21450631320476532, Training Accuracy: 73.78611111111111
[ Fri Jul 12 01:00:57 2024 ] 	Batch(4500/6809) done. Loss: 1.2895  lr:0.010000
[ Fri Jul 12 01:01:15 2024 ] 	Batch(4600/6809) done. Loss: 1.8840  lr:0.010000
[ Fri Jul 12 01:01:34 2024 ] 	Batch(4700/6809) done. Loss: 0.3613  lr:0.010000
[ Fri Jul 12 01:01:52 2024 ] 	Batch(4800/6809) done. Loss: 0.3578  lr:0.010000
[ Fri Jul 12 01:02:10 2024 ] 	Batch(4900/6809) done. Loss: 0.7085  lr:0.010000
[ Fri Jul 12 01:02:28 2024 ] 
Training: Epoch [11/120], Step [4999], Loss: 0.478651762008667, Training Accuracy: 73.785
[ Fri Jul 12 01:02:28 2024 ] 	Batch(5000/6809) done. Loss: 1.4718  lr:0.010000
[ Fri Jul 12 01:02:46 2024 ] 	Batch(5100/6809) done. Loss: 0.3873  lr:0.010000
[ Fri Jul 12 01:03:04 2024 ] 	Batch(5200/6809) done. Loss: 0.9797  lr:0.010000
[ Fri Jul 12 01:03:23 2024 ] 	Batch(5300/6809) done. Loss: 1.2840  lr:0.010000
[ Fri Jul 12 01:03:41 2024 ] 	Batch(5400/6809) done. Loss: 1.2639  lr:0.010000
[ Fri Jul 12 01:04:00 2024 ] 
Training: Epoch [11/120], Step [5499], Loss: 0.6814846396446228, Training Accuracy: 73.80909090909091
[ Fri Jul 12 01:04:00 2024 ] 	Batch(5500/6809) done. Loss: 1.1836  lr:0.010000
[ Fri Jul 12 01:04:18 2024 ] 	Batch(5600/6809) done. Loss: 1.6421  lr:0.010000
[ Fri Jul 12 01:04:36 2024 ] 	Batch(5700/6809) done. Loss: 0.1787  lr:0.010000
[ Fri Jul 12 01:04:54 2024 ] 	Batch(5800/6809) done. Loss: 0.7217  lr:0.010000
[ Fri Jul 12 01:05:12 2024 ] 	Batch(5900/6809) done. Loss: 1.6886  lr:0.010000
[ Fri Jul 12 01:05:30 2024 ] 
Training: Epoch [11/120], Step [5999], Loss: 0.41208308935165405, Training Accuracy: 73.81458333333333
[ Fri Jul 12 01:05:30 2024 ] 	Batch(6000/6809) done. Loss: 0.5945  lr:0.010000
[ Fri Jul 12 01:05:48 2024 ] 	Batch(6100/6809) done. Loss: 1.5595  lr:0.010000
[ Fri Jul 12 01:06:06 2024 ] 	Batch(6200/6809) done. Loss: 1.4121  lr:0.010000
[ Fri Jul 12 01:06:23 2024 ] 	Batch(6300/6809) done. Loss: 0.6519  lr:0.010000
[ Fri Jul 12 01:06:42 2024 ] 	Batch(6400/6809) done. Loss: 1.3548  lr:0.010000
[ Fri Jul 12 01:06:59 2024 ] 
Training: Epoch [11/120], Step [6499], Loss: 1.4839277267456055, Training Accuracy: 73.77307692307691
[ Fri Jul 12 01:06:59 2024 ] 	Batch(6500/6809) done. Loss: 1.5205  lr:0.010000
[ Fri Jul 12 01:07:17 2024 ] 	Batch(6600/6809) done. Loss: 1.0166  lr:0.010000
[ Fri Jul 12 01:07:35 2024 ] 	Batch(6700/6809) done. Loss: 1.3273  lr:0.010000
[ Fri Jul 12 01:07:53 2024 ] 	Batch(6800/6809) done. Loss: 0.5224  lr:0.010000
[ Fri Jul 12 01:07:55 2024 ] 	Mean training loss: 0.8772.
[ Fri Jul 12 01:07:55 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 01:07:55 2024 ] Training epoch: 13
[ Fri Jul 12 01:07:55 2024 ] 	Batch(0/6809) done. Loss: 0.4397  lr:0.010000
[ Fri Jul 12 01:08:13 2024 ] 	Batch(100/6809) done. Loss: 1.6335  lr:0.010000
[ Fri Jul 12 01:08:32 2024 ] 	Batch(200/6809) done. Loss: 1.2401  lr:0.010000
[ Fri Jul 12 01:08:50 2024 ] 	Batch(300/6809) done. Loss: 0.3143  lr:0.010000
[ Fri Jul 12 01:09:08 2024 ] 	Batch(400/6809) done. Loss: 0.5341  lr:0.010000
[ Fri Jul 12 01:09:27 2024 ] 
Training: Epoch [12/120], Step [499], Loss: 0.31707578897476196, Training Accuracy: 75.825
[ Fri Jul 12 01:09:27 2024 ] 	Batch(500/6809) done. Loss: 0.3459  lr:0.010000
[ Fri Jul 12 01:09:45 2024 ] 	Batch(600/6809) done. Loss: 1.0414  lr:0.010000
[ Fri Jul 12 01:10:03 2024 ] 	Batch(700/6809) done. Loss: 0.8209  lr:0.010000
[ Fri Jul 12 01:10:21 2024 ] 	Batch(800/6809) done. Loss: 0.5046  lr:0.010000
[ Fri Jul 12 01:10:39 2024 ] 	Batch(900/6809) done. Loss: 1.5179  lr:0.010000
[ Fri Jul 12 01:10:57 2024 ] 
Training: Epoch [12/120], Step [999], Loss: 1.6581922769546509, Training Accuracy: 75.44999999999999
[ Fri Jul 12 01:10:57 2024 ] 	Batch(1000/6809) done. Loss: 0.4217  lr:0.010000
[ Fri Jul 12 01:11:15 2024 ] 	Batch(1100/6809) done. Loss: 1.3511  lr:0.010000
[ Fri Jul 12 01:11:33 2024 ] 	Batch(1200/6809) done. Loss: 1.2828  lr:0.010000
[ Fri Jul 12 01:11:51 2024 ] 	Batch(1300/6809) done. Loss: 0.5505  lr:0.010000
[ Fri Jul 12 01:12:09 2024 ] 	Batch(1400/6809) done. Loss: 1.0847  lr:0.010000
[ Fri Jul 12 01:12:27 2024 ] 
Training: Epoch [12/120], Step [1499], Loss: 0.7818079590797424, Training Accuracy: 75.34166666666667
[ Fri Jul 12 01:12:27 2024 ] 	Batch(1500/6809) done. Loss: 0.6085  lr:0.010000
[ Fri Jul 12 01:12:45 2024 ] 	Batch(1600/6809) done. Loss: 1.2680  lr:0.010000
[ Fri Jul 12 01:13:03 2024 ] 	Batch(1700/6809) done. Loss: 0.1095  lr:0.010000
[ Fri Jul 12 01:13:21 2024 ] 	Batch(1800/6809) done. Loss: 1.5331  lr:0.010000
[ Fri Jul 12 01:13:39 2024 ] 	Batch(1900/6809) done. Loss: 0.8213  lr:0.010000
[ Fri Jul 12 01:13:57 2024 ] 
Training: Epoch [12/120], Step [1999], Loss: 1.1217844486236572, Training Accuracy: 74.97500000000001
[ Fri Jul 12 01:13:57 2024 ] 	Batch(2000/6809) done. Loss: 1.3631  lr:0.010000
[ Fri Jul 12 01:14:15 2024 ] 	Batch(2100/6809) done. Loss: 1.1241  lr:0.010000
[ Fri Jul 12 01:14:34 2024 ] 	Batch(2200/6809) done. Loss: 0.8838  lr:0.010000
[ Fri Jul 12 01:14:52 2024 ] 	Batch(2300/6809) done. Loss: 0.9443  lr:0.010000
[ Fri Jul 12 01:15:11 2024 ] 	Batch(2400/6809) done. Loss: 0.7352  lr:0.010000
[ Fri Jul 12 01:15:30 2024 ] 
Training: Epoch [12/120], Step [2499], Loss: 0.7555568218231201, Training Accuracy: 74.905
[ Fri Jul 12 01:15:30 2024 ] 	Batch(2500/6809) done. Loss: 0.4489  lr:0.010000
[ Fri Jul 12 01:15:48 2024 ] 	Batch(2600/6809) done. Loss: 0.7249  lr:0.010000
[ Fri Jul 12 01:16:07 2024 ] 	Batch(2700/6809) done. Loss: 0.9043  lr:0.010000
[ Fri Jul 12 01:16:25 2024 ] 	Batch(2800/6809) done. Loss: 0.7544  lr:0.010000
[ Fri Jul 12 01:16:43 2024 ] 	Batch(2900/6809) done. Loss: 0.5149  lr:0.010000
[ Fri Jul 12 01:17:01 2024 ] 
Training: Epoch [12/120], Step [2999], Loss: 0.5848122835159302, Training Accuracy: 74.875
[ Fri Jul 12 01:17:01 2024 ] 	Batch(3000/6809) done. Loss: 0.7175  lr:0.010000
[ Fri Jul 12 01:17:19 2024 ] 	Batch(3100/6809) done. Loss: 0.8499  lr:0.010000
[ Fri Jul 12 01:17:37 2024 ] 	Batch(3200/6809) done. Loss: 0.9528  lr:0.010000
[ Fri Jul 12 01:17:55 2024 ] 	Batch(3300/6809) done. Loss: 0.7242  lr:0.010000
[ Fri Jul 12 01:18:13 2024 ] 	Batch(3400/6809) done. Loss: 0.3740  lr:0.010000
[ Fri Jul 12 01:18:31 2024 ] 
Training: Epoch [12/120], Step [3499], Loss: 1.6300339698791504, Training Accuracy: 74.83214285714286
[ Fri Jul 12 01:18:31 2024 ] 	Batch(3500/6809) done. Loss: 0.5210  lr:0.010000
[ Fri Jul 12 01:18:49 2024 ] 	Batch(3600/6809) done. Loss: 0.8748  lr:0.010000
[ Fri Jul 12 01:19:07 2024 ] 	Batch(3700/6809) done. Loss: 0.6515  lr:0.010000
[ Fri Jul 12 01:19:25 2024 ] 	Batch(3800/6809) done. Loss: 0.4455  lr:0.010000
[ Fri Jul 12 01:19:43 2024 ] 	Batch(3900/6809) done. Loss: 0.8207  lr:0.010000
[ Fri Jul 12 01:20:00 2024 ] 
Training: Epoch [12/120], Step [3999], Loss: 0.6209062337875366, Training Accuracy: 74.790625
[ Fri Jul 12 01:20:01 2024 ] 	Batch(4000/6809) done. Loss: 0.3355  lr:0.010000
[ Fri Jul 12 01:20:19 2024 ] 	Batch(4100/6809) done. Loss: 0.4570  lr:0.010000
[ Fri Jul 12 01:20:37 2024 ] 	Batch(4200/6809) done. Loss: 0.7853  lr:0.010000
[ Fri Jul 12 01:20:54 2024 ] 	Batch(4300/6809) done. Loss: 0.4124  lr:0.010000
[ Fri Jul 12 01:21:13 2024 ] 	Batch(4400/6809) done. Loss: 0.3654  lr:0.010000
[ Fri Jul 12 01:21:31 2024 ] 
Training: Epoch [12/120], Step [4499], Loss: 1.1661850214004517, Training Accuracy: 74.79166666666667
[ Fri Jul 12 01:21:31 2024 ] 	Batch(4500/6809) done. Loss: 1.3332  lr:0.010000
[ Fri Jul 12 01:21:50 2024 ] 	Batch(4600/6809) done. Loss: 1.1199  lr:0.010000
[ Fri Jul 12 01:22:08 2024 ] 	Batch(4700/6809) done. Loss: 0.5543  lr:0.010000
[ Fri Jul 12 01:22:27 2024 ] 	Batch(4800/6809) done. Loss: 0.8789  lr:0.010000
[ Fri Jul 12 01:22:45 2024 ] 	Batch(4900/6809) done. Loss: 0.5529  lr:0.010000
[ Fri Jul 12 01:23:02 2024 ] 
Training: Epoch [12/120], Step [4999], Loss: 0.28801801800727844, Training Accuracy: 74.6675
[ Fri Jul 12 01:23:03 2024 ] 	Batch(5000/6809) done. Loss: 0.6496  lr:0.010000
[ Fri Jul 12 01:23:20 2024 ] 	Batch(5100/6809) done. Loss: 1.3918  lr:0.010000
[ Fri Jul 12 01:23:38 2024 ] 	Batch(5200/6809) done. Loss: 1.1894  lr:0.010000
[ Fri Jul 12 01:23:56 2024 ] 	Batch(5300/6809) done. Loss: 0.5742  lr:0.010000
[ Fri Jul 12 01:24:14 2024 ] 	Batch(5400/6809) done. Loss: 0.6632  lr:0.010000
[ Fri Jul 12 01:24:32 2024 ] 
Training: Epoch [12/120], Step [5499], Loss: 0.6199151873588562, Training Accuracy: 74.52499999999999
[ Fri Jul 12 01:24:32 2024 ] 	Batch(5500/6809) done. Loss: 0.6178  lr:0.010000
[ Fri Jul 12 01:24:50 2024 ] 	Batch(5600/6809) done. Loss: 0.5089  lr:0.010000
[ Fri Jul 12 01:25:08 2024 ] 	Batch(5700/6809) done. Loss: 0.6484  lr:0.010000
[ Fri Jul 12 01:25:26 2024 ] 	Batch(5800/6809) done. Loss: 0.9145  lr:0.010000
[ Fri Jul 12 01:25:44 2024 ] 	Batch(5900/6809) done. Loss: 1.0590  lr:0.010000
[ Fri Jul 12 01:26:02 2024 ] 
Training: Epoch [12/120], Step [5999], Loss: 0.5635642409324646, Training Accuracy: 74.50208333333333
[ Fri Jul 12 01:26:02 2024 ] 	Batch(6000/6809) done. Loss: 0.6312  lr:0.010000
[ Fri Jul 12 01:26:20 2024 ] 	Batch(6100/6809) done. Loss: 0.4363  lr:0.010000
[ Fri Jul 12 01:26:38 2024 ] 	Batch(6200/6809) done. Loss: 0.2001  lr:0.010000
[ Fri Jul 12 01:26:56 2024 ] 	Batch(6300/6809) done. Loss: 1.5283  lr:0.010000
[ Fri Jul 12 01:27:14 2024 ] 	Batch(6400/6809) done. Loss: 1.3526  lr:0.010000
[ Fri Jul 12 01:27:31 2024 ] 
Training: Epoch [12/120], Step [6499], Loss: 0.5724347829818726, Training Accuracy: 74.5423076923077
[ Fri Jul 12 01:27:32 2024 ] 	Batch(6500/6809) done. Loss: 0.5375  lr:0.010000
[ Fri Jul 12 01:27:50 2024 ] 	Batch(6600/6809) done. Loss: 0.7541  lr:0.010000
[ Fri Jul 12 01:28:07 2024 ] 	Batch(6700/6809) done. Loss: 0.8907  lr:0.010000
[ Fri Jul 12 01:28:25 2024 ] 	Batch(6800/6809) done. Loss: 0.5795  lr:0.010000
[ Fri Jul 12 01:28:27 2024 ] 	Mean training loss: 0.8422.
[ Fri Jul 12 01:28:27 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 01:28:27 2024 ] Training epoch: 14
[ Fri Jul 12 01:28:28 2024 ] 	Batch(0/6809) done. Loss: 1.3757  lr:0.010000
[ Fri Jul 12 01:28:46 2024 ] 	Batch(100/6809) done. Loss: 0.5046  lr:0.010000
[ Fri Jul 12 01:29:03 2024 ] 	Batch(200/6809) done. Loss: 0.9108  lr:0.010000
[ Fri Jul 12 01:29:21 2024 ] 	Batch(300/6809) done. Loss: 1.0386  lr:0.010000
[ Fri Jul 12 01:29:39 2024 ] 	Batch(400/6809) done. Loss: 1.6277  lr:0.010000
[ Fri Jul 12 01:29:57 2024 ] 
Training: Epoch [13/120], Step [499], Loss: 1.4508943557739258, Training Accuracy: 75.725
[ Fri Jul 12 01:29:57 2024 ] 	Batch(500/6809) done. Loss: 0.5829  lr:0.010000
[ Fri Jul 12 01:30:15 2024 ] 	Batch(600/6809) done. Loss: 1.5234  lr:0.010000
[ Fri Jul 12 01:30:33 2024 ] 	Batch(700/6809) done. Loss: 0.6546  lr:0.010000
[ Fri Jul 12 01:30:51 2024 ] 	Batch(800/6809) done. Loss: 0.2672  lr:0.010000
[ Fri Jul 12 01:31:09 2024 ] 	Batch(900/6809) done. Loss: 0.4507  lr:0.010000
[ Fri Jul 12 01:31:27 2024 ] 
Training: Epoch [13/120], Step [999], Loss: 0.4682564437389374, Training Accuracy: 75.75
[ Fri Jul 12 01:31:27 2024 ] 	Batch(1000/6809) done. Loss: 0.7468  lr:0.010000
[ Fri Jul 12 01:31:45 2024 ] 	Batch(1100/6809) done. Loss: 0.3442  lr:0.010000
[ Fri Jul 12 01:32:04 2024 ] 	Batch(1200/6809) done. Loss: 0.6946  lr:0.010000
[ Fri Jul 12 01:32:21 2024 ] 	Batch(1300/6809) done. Loss: 0.7892  lr:0.010000
[ Fri Jul 12 01:32:39 2024 ] 	Batch(1400/6809) done. Loss: 1.5736  lr:0.010000
[ Fri Jul 12 01:32:57 2024 ] 
Training: Epoch [13/120], Step [1499], Loss: 1.4058184623718262, Training Accuracy: 75.30833333333334
[ Fri Jul 12 01:32:57 2024 ] 	Batch(1500/6809) done. Loss: 0.2767  lr:0.010000
[ Fri Jul 12 01:33:15 2024 ] 	Batch(1600/6809) done. Loss: 0.6087  lr:0.010000
[ Fri Jul 12 01:33:33 2024 ] 	Batch(1700/6809) done. Loss: 1.2848  lr:0.010000
[ Fri Jul 12 01:33:51 2024 ] 	Batch(1800/6809) done. Loss: 1.0430  lr:0.010000
[ Fri Jul 12 01:34:09 2024 ] 	Batch(1900/6809) done. Loss: 1.2350  lr:0.010000
[ Fri Jul 12 01:34:27 2024 ] 
Training: Epoch [13/120], Step [1999], Loss: 1.7390378713607788, Training Accuracy: 75.46875
[ Fri Jul 12 01:34:27 2024 ] 	Batch(2000/6809) done. Loss: 0.8393  lr:0.010000
[ Fri Jul 12 01:34:46 2024 ] 	Batch(2100/6809) done. Loss: 0.4824  lr:0.010000
[ Fri Jul 12 01:35:04 2024 ] 	Batch(2200/6809) done. Loss: 1.3446  lr:0.010000
[ Fri Jul 12 01:35:22 2024 ] 	Batch(2300/6809) done. Loss: 1.3120  lr:0.010000
[ Fri Jul 12 01:35:40 2024 ] 	Batch(2400/6809) done. Loss: 0.6025  lr:0.010000
[ Fri Jul 12 01:35:58 2024 ] 
Training: Epoch [13/120], Step [2499], Loss: 0.34430262446403503, Training Accuracy: 75.535
[ Fri Jul 12 01:35:58 2024 ] 	Batch(2500/6809) done. Loss: 0.7784  lr:0.010000
[ Fri Jul 12 01:36:16 2024 ] 	Batch(2600/6809) done. Loss: 1.1382  lr:0.010000
[ Fri Jul 12 01:36:33 2024 ] 	Batch(2700/6809) done. Loss: 0.6129  lr:0.010000
[ Fri Jul 12 01:36:52 2024 ] 	Batch(2800/6809) done. Loss: 1.0094  lr:0.010000
[ Fri Jul 12 01:37:09 2024 ] 	Batch(2900/6809) done. Loss: 0.3113  lr:0.010000
[ Fri Jul 12 01:37:27 2024 ] 
Training: Epoch [13/120], Step [2999], Loss: 0.9550191164016724, Training Accuracy: 75.425
[ Fri Jul 12 01:37:27 2024 ] 	Batch(3000/6809) done. Loss: 0.5085  lr:0.010000
[ Fri Jul 12 01:37:45 2024 ] 	Batch(3100/6809) done. Loss: 0.7219  lr:0.010000
[ Fri Jul 12 01:38:03 2024 ] 	Batch(3200/6809) done. Loss: 0.8662  lr:0.010000
[ Fri Jul 12 01:38:21 2024 ] 	Batch(3300/6809) done. Loss: 0.8298  lr:0.010000
[ Fri Jul 12 01:38:39 2024 ] 	Batch(3400/6809) done. Loss: 0.6453  lr:0.010000
[ Fri Jul 12 01:38:57 2024 ] 
Training: Epoch [13/120], Step [3499], Loss: 1.0418448448181152, Training Accuracy: 75.35
[ Fri Jul 12 01:38:57 2024 ] 	Batch(3500/6809) done. Loss: 1.0435  lr:0.010000
[ Fri Jul 12 01:39:15 2024 ] 	Batch(3600/6809) done. Loss: 1.1910  lr:0.010000
[ Fri Jul 12 01:39:33 2024 ] 	Batch(3700/6809) done. Loss: 0.5669  lr:0.010000
[ Fri Jul 12 01:39:51 2024 ] 	Batch(3800/6809) done. Loss: 0.0997  lr:0.010000
[ Fri Jul 12 01:40:09 2024 ] 	Batch(3900/6809) done. Loss: 0.5505  lr:0.010000
[ Fri Jul 12 01:40:26 2024 ] 
Training: Epoch [13/120], Step [3999], Loss: 2.7787091732025146, Training Accuracy: 75.3
[ Fri Jul 12 01:40:27 2024 ] 	Batch(4000/6809) done. Loss: 1.2107  lr:0.010000
[ Fri Jul 12 01:40:45 2024 ] 	Batch(4100/6809) done. Loss: 0.4742  lr:0.010000
[ Fri Jul 12 01:41:03 2024 ] 	Batch(4200/6809) done. Loss: 1.0439  lr:0.010000
[ Fri Jul 12 01:41:21 2024 ] 	Batch(4300/6809) done. Loss: 0.4087  lr:0.010000
[ Fri Jul 12 01:41:39 2024 ] 	Batch(4400/6809) done. Loss: 0.5108  lr:0.010000
[ Fri Jul 12 01:41:56 2024 ] 
Training: Epoch [13/120], Step [4499], Loss: 0.531091034412384, Training Accuracy: 75.39166666666667
[ Fri Jul 12 01:41:57 2024 ] 	Batch(4500/6809) done. Loss: 1.4184  lr:0.010000
[ Fri Jul 12 01:42:15 2024 ] 	Batch(4600/6809) done. Loss: 0.3553  lr:0.010000
[ Fri Jul 12 01:42:32 2024 ] 	Batch(4700/6809) done. Loss: 0.7935  lr:0.010000
[ Fri Jul 12 01:42:50 2024 ] 	Batch(4800/6809) done. Loss: 1.4528  lr:0.010000
[ Fri Jul 12 01:43:09 2024 ] 	Batch(4900/6809) done. Loss: 1.0754  lr:0.010000
[ Fri Jul 12 01:43:27 2024 ] 
Training: Epoch [13/120], Step [4999], Loss: 0.6069729924201965, Training Accuracy: 75.42750000000001
[ Fri Jul 12 01:43:27 2024 ] 	Batch(5000/6809) done. Loss: 1.3307  lr:0.010000
[ Fri Jul 12 01:43:46 2024 ] 	Batch(5100/6809) done. Loss: 0.6653  lr:0.010000
[ Fri Jul 12 01:44:05 2024 ] 	Batch(5200/6809) done. Loss: 0.3895  lr:0.010000
[ Fri Jul 12 01:44:23 2024 ] 	Batch(5300/6809) done. Loss: 0.9366  lr:0.010000
[ Fri Jul 12 01:44:40 2024 ] 	Batch(5400/6809) done. Loss: 0.2801  lr:0.010000
[ Fri Jul 12 01:44:58 2024 ] 
Training: Epoch [13/120], Step [5499], Loss: 1.372583031654358, Training Accuracy: 75.41136363636363
[ Fri Jul 12 01:44:58 2024 ] 	Batch(5500/6809) done. Loss: 0.5151  lr:0.010000
[ Fri Jul 12 01:45:16 2024 ] 	Batch(5600/6809) done. Loss: 0.9102  lr:0.010000
[ Fri Jul 12 01:45:35 2024 ] 	Batch(5700/6809) done. Loss: 0.7210  lr:0.010000
[ Fri Jul 12 01:45:54 2024 ] 	Batch(5800/6809) done. Loss: 1.2871  lr:0.010000
[ Fri Jul 12 01:46:12 2024 ] 	Batch(5900/6809) done. Loss: 0.8782  lr:0.010000
[ Fri Jul 12 01:46:30 2024 ] 
Training: Epoch [13/120], Step [5999], Loss: 0.8422619700431824, Training Accuracy: 75.48333333333333
[ Fri Jul 12 01:46:31 2024 ] 	Batch(6000/6809) done. Loss: 0.1664  lr:0.010000
[ Fri Jul 12 01:46:49 2024 ] 	Batch(6100/6809) done. Loss: 1.0156  lr:0.010000
[ Fri Jul 12 01:47:08 2024 ] 	Batch(6200/6809) done. Loss: 1.4456  lr:0.010000
[ Fri Jul 12 01:47:26 2024 ] 	Batch(6300/6809) done. Loss: 0.4662  lr:0.010000
[ Fri Jul 12 01:47:45 2024 ] 	Batch(6400/6809) done. Loss: 0.1611  lr:0.010000
[ Fri Jul 12 01:48:03 2024 ] 
Training: Epoch [13/120], Step [6499], Loss: 1.006922960281372, Training Accuracy: 75.49230769230769
[ Fri Jul 12 01:48:04 2024 ] 	Batch(6500/6809) done. Loss: 1.8042  lr:0.010000
[ Fri Jul 12 01:48:22 2024 ] 	Batch(6600/6809) done. Loss: 0.3792  lr:0.010000
[ Fri Jul 12 01:48:41 2024 ] 	Batch(6700/6809) done. Loss: 0.6844  lr:0.010000
[ Fri Jul 12 01:48:59 2024 ] 	Batch(6800/6809) done. Loss: 0.7881  lr:0.010000
[ Fri Jul 12 01:49:01 2024 ] 	Mean training loss: 0.7996.
[ Fri Jul 12 01:49:01 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 01:49:01 2024 ] Training epoch: 15
[ Fri Jul 12 01:49:02 2024 ] 	Batch(0/6809) done. Loss: 0.4570  lr:0.010000
[ Fri Jul 12 01:49:20 2024 ] 	Batch(100/6809) done. Loss: 0.0630  lr:0.010000
[ Fri Jul 12 01:49:37 2024 ] 	Batch(200/6809) done. Loss: 0.8026  lr:0.010000
[ Fri Jul 12 01:49:55 2024 ] 	Batch(300/6809) done. Loss: 0.3142  lr:0.010000
[ Fri Jul 12 01:50:13 2024 ] 	Batch(400/6809) done. Loss: 1.2510  lr:0.010000
[ Fri Jul 12 01:50:31 2024 ] 
Training: Epoch [14/120], Step [499], Loss: 0.7282886505126953, Training Accuracy: 76.925
[ Fri Jul 12 01:50:31 2024 ] 	Batch(500/6809) done. Loss: 1.2045  lr:0.010000
[ Fri Jul 12 01:50:49 2024 ] 	Batch(600/6809) done. Loss: 0.9066  lr:0.010000
[ Fri Jul 12 01:51:07 2024 ] 	Batch(700/6809) done. Loss: 0.9993  lr:0.010000
[ Fri Jul 12 01:51:25 2024 ] 	Batch(800/6809) done. Loss: 0.0858  lr:0.010000
[ Fri Jul 12 01:51:43 2024 ] 	Batch(900/6809) done. Loss: 0.5014  lr:0.010000
[ Fri Jul 12 01:52:01 2024 ] 
Training: Epoch [14/120], Step [999], Loss: 0.8439264893531799, Training Accuracy: 77.2
[ Fri Jul 12 01:52:01 2024 ] 	Batch(1000/6809) done. Loss: 0.6923  lr:0.010000
[ Fri Jul 12 01:52:19 2024 ] 	Batch(1100/6809) done. Loss: 0.0573  lr:0.010000
[ Fri Jul 12 01:52:37 2024 ] 	Batch(1200/6809) done. Loss: 0.9970  lr:0.010000
[ Fri Jul 12 01:52:55 2024 ] 	Batch(1300/6809) done. Loss: 0.1815  lr:0.010000
[ Fri Jul 12 01:53:13 2024 ] 	Batch(1400/6809) done. Loss: 1.7009  lr:0.010000
[ Fri Jul 12 01:53:30 2024 ] 
Training: Epoch [14/120], Step [1499], Loss: 0.8986712694168091, Training Accuracy: 77.23333333333333
[ Fri Jul 12 01:53:30 2024 ] 	Batch(1500/6809) done. Loss: 0.8861  lr:0.010000
[ Fri Jul 12 01:53:49 2024 ] 	Batch(1600/6809) done. Loss: 0.1471  lr:0.010000
[ Fri Jul 12 01:54:07 2024 ] 	Batch(1700/6809) done. Loss: 1.0635  lr:0.010000
[ Fri Jul 12 01:54:24 2024 ] 	Batch(1800/6809) done. Loss: 0.9145  lr:0.010000
[ Fri Jul 12 01:54:42 2024 ] 	Batch(1900/6809) done. Loss: 0.9351  lr:0.010000
[ Fri Jul 12 01:55:00 2024 ] 
Training: Epoch [14/120], Step [1999], Loss: 1.2309077978134155, Training Accuracy: 77.11875
[ Fri Jul 12 01:55:00 2024 ] 	Batch(2000/6809) done. Loss: 0.5741  lr:0.010000
[ Fri Jul 12 01:55:18 2024 ] 	Batch(2100/6809) done. Loss: 0.1171  lr:0.010000
[ Fri Jul 12 01:55:36 2024 ] 	Batch(2200/6809) done. Loss: 0.7802  lr:0.010000
[ Fri Jul 12 01:55:54 2024 ] 	Batch(2300/6809) done. Loss: 0.7092  lr:0.010000
[ Fri Jul 12 01:56:12 2024 ] 	Batch(2400/6809) done. Loss: 1.3253  lr:0.010000
[ Fri Jul 12 01:56:30 2024 ] 
Training: Epoch [14/120], Step [2499], Loss: 0.4506777822971344, Training Accuracy: 77.095
[ Fri Jul 12 01:56:30 2024 ] 	Batch(2500/6809) done. Loss: 1.2551  lr:0.010000
[ Fri Jul 12 01:56:48 2024 ] 	Batch(2600/6809) done. Loss: 0.3560  lr:0.010000
[ Fri Jul 12 01:57:06 2024 ] 	Batch(2700/6809) done. Loss: 0.4324  lr:0.010000
[ Fri Jul 12 01:57:24 2024 ] 	Batch(2800/6809) done. Loss: 1.2236  lr:0.010000
[ Fri Jul 12 01:57:42 2024 ] 	Batch(2900/6809) done. Loss: 0.1881  lr:0.010000
[ Fri Jul 12 01:57:59 2024 ] 
Training: Epoch [14/120], Step [2999], Loss: 0.4581835865974426, Training Accuracy: 76.90833333333333
[ Fri Jul 12 01:57:59 2024 ] 	Batch(3000/6809) done. Loss: 1.0560  lr:0.010000
[ Fri Jul 12 01:58:17 2024 ] 	Batch(3100/6809) done. Loss: 0.7176  lr:0.010000
[ Fri Jul 12 01:58:35 2024 ] 	Batch(3200/6809) done. Loss: 0.5768  lr:0.010000
[ Fri Jul 12 01:58:54 2024 ] 	Batch(3300/6809) done. Loss: 0.6266  lr:0.010000
[ Fri Jul 12 01:59:12 2024 ] 	Batch(3400/6809) done. Loss: 0.5016  lr:0.010000
[ Fri Jul 12 01:59:31 2024 ] 
Training: Epoch [14/120], Step [3499], Loss: 0.932803750038147, Training Accuracy: 76.71071428571429
[ Fri Jul 12 01:59:31 2024 ] 	Batch(3500/6809) done. Loss: 0.4933  lr:0.010000
[ Fri Jul 12 01:59:50 2024 ] 	Batch(3600/6809) done. Loss: 1.5022  lr:0.010000
[ Fri Jul 12 02:00:08 2024 ] 	Batch(3700/6809) done. Loss: 1.0534  lr:0.010000
[ Fri Jul 12 02:00:27 2024 ] 	Batch(3800/6809) done. Loss: 0.2784  lr:0.010000
[ Fri Jul 12 02:00:45 2024 ] 	Batch(3900/6809) done. Loss: 0.1533  lr:0.010000
[ Fri Jul 12 02:01:04 2024 ] 
Training: Epoch [14/120], Step [3999], Loss: 0.6602240204811096, Training Accuracy: 76.690625
[ Fri Jul 12 02:01:04 2024 ] 	Batch(4000/6809) done. Loss: 0.1444  lr:0.010000
[ Fri Jul 12 02:01:22 2024 ] 	Batch(4100/6809) done. Loss: 1.2223  lr:0.010000
[ Fri Jul 12 02:01:41 2024 ] 	Batch(4200/6809) done. Loss: 1.5827  lr:0.010000
[ Fri Jul 12 02:01:59 2024 ] 	Batch(4300/6809) done. Loss: 0.2729  lr:0.010000
[ Fri Jul 12 02:02:18 2024 ] 	Batch(4400/6809) done. Loss: 0.7379  lr:0.010000
[ Fri Jul 12 02:02:35 2024 ] 
Training: Epoch [14/120], Step [4499], Loss: 0.426320880651474, Training Accuracy: 76.70833333333333
[ Fri Jul 12 02:02:36 2024 ] 	Batch(4500/6809) done. Loss: 0.5618  lr:0.010000
[ Fri Jul 12 02:02:53 2024 ] 	Batch(4600/6809) done. Loss: 1.0467  lr:0.010000
[ Fri Jul 12 02:03:11 2024 ] 	Batch(4700/6809) done. Loss: 0.5866  lr:0.010000
[ Fri Jul 12 02:03:29 2024 ] 	Batch(4800/6809) done. Loss: 1.1926  lr:0.010000
[ Fri Jul 12 02:03:47 2024 ] 	Batch(4900/6809) done. Loss: 0.7936  lr:0.010000
[ Fri Jul 12 02:04:05 2024 ] 
Training: Epoch [14/120], Step [4999], Loss: 0.32836854457855225, Training Accuracy: 76.6375
[ Fri Jul 12 02:04:05 2024 ] 	Batch(5000/6809) done. Loss: 0.2981  lr:0.010000
[ Fri Jul 12 02:04:23 2024 ] 	Batch(5100/6809) done. Loss: 0.1015  lr:0.010000
[ Fri Jul 12 02:04:41 2024 ] 	Batch(5200/6809) done. Loss: 0.2348  lr:0.010000
[ Fri Jul 12 02:04:59 2024 ] 	Batch(5300/6809) done. Loss: 1.5019  lr:0.010000
[ Fri Jul 12 02:05:17 2024 ] 	Batch(5400/6809) done. Loss: 0.4760  lr:0.010000
[ Fri Jul 12 02:05:34 2024 ] 
Training: Epoch [14/120], Step [5499], Loss: 1.8972429037094116, Training Accuracy: 76.55681818181819
[ Fri Jul 12 02:05:35 2024 ] 	Batch(5500/6809) done. Loss: 1.0247  lr:0.010000
[ Fri Jul 12 02:05:53 2024 ] 	Batch(5600/6809) done. Loss: 0.7230  lr:0.010000
[ Fri Jul 12 02:06:11 2024 ] 	Batch(5700/6809) done. Loss: 0.3328  lr:0.010000
[ Fri Jul 12 02:06:28 2024 ] 	Batch(5800/6809) done. Loss: 0.3069  lr:0.010000
[ Fri Jul 12 02:06:46 2024 ] 	Batch(5900/6809) done. Loss: 0.8496  lr:0.010000
[ Fri Jul 12 02:07:04 2024 ] 
Training: Epoch [14/120], Step [5999], Loss: 0.35415804386138916, Training Accuracy: 76.53541666666666
[ Fri Jul 12 02:07:04 2024 ] 	Batch(6000/6809) done. Loss: 1.0569  lr:0.010000
[ Fri Jul 12 02:07:23 2024 ] 	Batch(6100/6809) done. Loss: 1.4380  lr:0.010000
[ Fri Jul 12 02:07:41 2024 ] 	Batch(6200/6809) done. Loss: 1.1412  lr:0.010000
[ Fri Jul 12 02:08:00 2024 ] 	Batch(6300/6809) done. Loss: 1.0224  lr:0.010000
[ Fri Jul 12 02:08:18 2024 ] 	Batch(6400/6809) done. Loss: 0.1305  lr:0.010000
[ Fri Jul 12 02:08:37 2024 ] 
Training: Epoch [14/120], Step [6499], Loss: 0.4142182171344757, Training Accuracy: 76.56153846153846
[ Fri Jul 12 02:08:37 2024 ] 	Batch(6500/6809) done. Loss: 1.9359  lr:0.010000
[ Fri Jul 12 02:08:55 2024 ] 	Batch(6600/6809) done. Loss: 0.3340  lr:0.010000
[ Fri Jul 12 02:09:14 2024 ] 	Batch(6700/6809) done. Loss: 1.5167  lr:0.010000
[ Fri Jul 12 02:09:33 2024 ] 	Batch(6800/6809) done. Loss: 1.2324  lr:0.010000
[ Fri Jul 12 02:09:34 2024 ] 	Mean training loss: 0.7746.
[ Fri Jul 12 02:09:34 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 02:09:34 2024 ] Training epoch: 16
[ Fri Jul 12 02:09:35 2024 ] 	Batch(0/6809) done. Loss: 0.8842  lr:0.010000
[ Fri Jul 12 02:09:53 2024 ] 	Batch(100/6809) done. Loss: 0.8258  lr:0.010000
[ Fri Jul 12 02:10:11 2024 ] 	Batch(200/6809) done. Loss: 0.2889  lr:0.010000
[ Fri Jul 12 02:10:29 2024 ] 	Batch(300/6809) done. Loss: 1.1782  lr:0.010000
[ Fri Jul 12 02:10:47 2024 ] 	Batch(400/6809) done. Loss: 0.4732  lr:0.010000
[ Fri Jul 12 02:11:04 2024 ] 
Training: Epoch [15/120], Step [499], Loss: 1.9248863458633423, Training Accuracy: 77.725
[ Fri Jul 12 02:11:04 2024 ] 	Batch(500/6809) done. Loss: 0.4709  lr:0.010000
[ Fri Jul 12 02:11:22 2024 ] 	Batch(600/6809) done. Loss: 0.2590  lr:0.010000
[ Fri Jul 12 02:11:40 2024 ] 	Batch(700/6809) done. Loss: 0.3280  lr:0.010000
[ Fri Jul 12 02:11:58 2024 ] 	Batch(800/6809) done. Loss: 1.2000  lr:0.010000
[ Fri Jul 12 02:12:16 2024 ] 	Batch(900/6809) done. Loss: 0.2695  lr:0.010000
[ Fri Jul 12 02:12:34 2024 ] 
Training: Epoch [15/120], Step [999], Loss: 1.0922696590423584, Training Accuracy: 77.7
[ Fri Jul 12 02:12:34 2024 ] 	Batch(1000/6809) done. Loss: 0.6017  lr:0.010000
[ Fri Jul 12 02:12:52 2024 ] 	Batch(1100/6809) done. Loss: 0.5903  lr:0.010000
[ Fri Jul 12 02:13:10 2024 ] 	Batch(1200/6809) done. Loss: 0.6119  lr:0.010000
[ Fri Jul 12 02:13:28 2024 ] 	Batch(1300/6809) done. Loss: 0.0839  lr:0.010000
[ Fri Jul 12 02:13:46 2024 ] 	Batch(1400/6809) done. Loss: 0.3695  lr:0.010000
[ Fri Jul 12 02:14:04 2024 ] 
Training: Epoch [15/120], Step [1499], Loss: 0.22234772145748138, Training Accuracy: 77.71666666666667
[ Fri Jul 12 02:14:04 2024 ] 	Batch(1500/6809) done. Loss: 0.3192  lr:0.010000
[ Fri Jul 12 02:14:23 2024 ] 	Batch(1600/6809) done. Loss: 0.6627  lr:0.010000
[ Fri Jul 12 02:14:41 2024 ] 	Batch(1700/6809) done. Loss: 1.2359  lr:0.010000
[ Fri Jul 12 02:15:00 2024 ] 	Batch(1800/6809) done. Loss: 0.6887  lr:0.010000
[ Fri Jul 12 02:15:18 2024 ] 	Batch(1900/6809) done. Loss: 0.6445  lr:0.010000
[ Fri Jul 12 02:15:36 2024 ] 
Training: Epoch [15/120], Step [1999], Loss: 0.7157194018363953, Training Accuracy: 77.9
[ Fri Jul 12 02:15:37 2024 ] 	Batch(2000/6809) done. Loss: 0.8301  lr:0.010000
[ Fri Jul 12 02:15:55 2024 ] 	Batch(2100/6809) done. Loss: 0.1740  lr:0.010000
[ Fri Jul 12 02:16:13 2024 ] 	Batch(2200/6809) done. Loss: 0.8473  lr:0.010000
[ Fri Jul 12 02:16:30 2024 ] 	Batch(2300/6809) done. Loss: 0.5233  lr:0.010000
[ Fri Jul 12 02:16:49 2024 ] 	Batch(2400/6809) done. Loss: 0.7421  lr:0.010000
[ Fri Jul 12 02:17:06 2024 ] 
Training: Epoch [15/120], Step [2499], Loss: 0.5223902463912964, Training Accuracy: 77.86
[ Fri Jul 12 02:17:06 2024 ] 	Batch(2500/6809) done. Loss: 0.5586  lr:0.010000
[ Fri Jul 12 02:17:24 2024 ] 	Batch(2600/6809) done. Loss: 0.6032  lr:0.010000
[ Fri Jul 12 02:17:42 2024 ] 	Batch(2700/6809) done. Loss: 0.7349  lr:0.010000
[ Fri Jul 12 02:18:00 2024 ] 	Batch(2800/6809) done. Loss: 0.1477  lr:0.010000
[ Fri Jul 12 02:18:18 2024 ] 	Batch(2900/6809) done. Loss: 1.5291  lr:0.010000
[ Fri Jul 12 02:18:36 2024 ] 
Training: Epoch [15/120], Step [2999], Loss: 0.15241853892803192, Training Accuracy: 77.56666666666666
[ Fri Jul 12 02:18:36 2024 ] 	Batch(3000/6809) done. Loss: 0.6533  lr:0.010000
[ Fri Jul 12 02:18:54 2024 ] 	Batch(3100/6809) done. Loss: 0.6125  lr:0.010000
[ Fri Jul 12 02:19:12 2024 ] 	Batch(3200/6809) done. Loss: 0.8954  lr:0.010000
[ Fri Jul 12 02:19:30 2024 ] 	Batch(3300/6809) done. Loss: 0.1466  lr:0.010000
[ Fri Jul 12 02:19:48 2024 ] 	Batch(3400/6809) done. Loss: 0.5189  lr:0.010000
[ Fri Jul 12 02:20:06 2024 ] 
Training: Epoch [15/120], Step [3499], Loss: 0.6738728880882263, Training Accuracy: 77.56428571428572
[ Fri Jul 12 02:20:06 2024 ] 	Batch(3500/6809) done. Loss: 1.0288  lr:0.010000
[ Fri Jul 12 02:20:24 2024 ] 	Batch(3600/6809) done. Loss: 0.3997  lr:0.010000
[ Fri Jul 12 02:20:42 2024 ] 	Batch(3700/6809) done. Loss: 0.8959  lr:0.010000
[ Fri Jul 12 02:21:01 2024 ] 	Batch(3800/6809) done. Loss: 0.9131  lr:0.010000
[ Fri Jul 12 02:21:19 2024 ] 	Batch(3900/6809) done. Loss: 0.5302  lr:0.010000
[ Fri Jul 12 02:21:37 2024 ] 
Training: Epoch [15/120], Step [3999], Loss: 0.5931267738342285, Training Accuracy: 77.734375
[ Fri Jul 12 02:21:38 2024 ] 	Batch(4000/6809) done. Loss: 0.3447  lr:0.010000
[ Fri Jul 12 02:21:56 2024 ] 	Batch(4100/6809) done. Loss: 1.5290  lr:0.010000
[ Fri Jul 12 02:22:14 2024 ] 	Batch(4200/6809) done. Loss: 1.0117  lr:0.010000
[ Fri Jul 12 02:22:31 2024 ] 	Batch(4300/6809) done. Loss: 0.5057  lr:0.010000
[ Fri Jul 12 02:22:50 2024 ] 	Batch(4400/6809) done. Loss: 0.7793  lr:0.010000
[ Fri Jul 12 02:23:08 2024 ] 
Training: Epoch [15/120], Step [4499], Loss: 0.6683856248855591, Training Accuracy: 77.61666666666667
[ Fri Jul 12 02:23:08 2024 ] 	Batch(4500/6809) done. Loss: 1.8073  lr:0.010000
[ Fri Jul 12 02:23:26 2024 ] 	Batch(4600/6809) done. Loss: 0.9969  lr:0.010000
[ Fri Jul 12 02:23:44 2024 ] 	Batch(4700/6809) done. Loss: 0.5708  lr:0.010000
[ Fri Jul 12 02:24:02 2024 ] 	Batch(4800/6809) done. Loss: 1.4774  lr:0.010000
[ Fri Jul 12 02:24:20 2024 ] 	Batch(4900/6809) done. Loss: 0.3394  lr:0.010000
[ Fri Jul 12 02:24:37 2024 ] 
Training: Epoch [15/120], Step [4999], Loss: 0.4425811469554901, Training Accuracy: 77.595
[ Fri Jul 12 02:24:37 2024 ] 	Batch(5000/6809) done. Loss: 0.3661  lr:0.010000
[ Fri Jul 12 02:24:55 2024 ] 	Batch(5100/6809) done. Loss: 0.1055  lr:0.010000
[ Fri Jul 12 02:25:13 2024 ] 	Batch(5200/6809) done. Loss: 0.2183  lr:0.010000
[ Fri Jul 12 02:25:32 2024 ] 	Batch(5300/6809) done. Loss: 0.7677  lr:0.010000
[ Fri Jul 12 02:25:50 2024 ] 	Batch(5400/6809) done. Loss: 0.6963  lr:0.010000
[ Fri Jul 12 02:26:07 2024 ] 
Training: Epoch [15/120], Step [5499], Loss: 0.6122565269470215, Training Accuracy: 77.64772727272728
[ Fri Jul 12 02:26:08 2024 ] 	Batch(5500/6809) done. Loss: 0.5801  lr:0.010000
[ Fri Jul 12 02:26:26 2024 ] 	Batch(5600/6809) done. Loss: 0.6515  lr:0.010000
[ Fri Jul 12 02:26:44 2024 ] 	Batch(5700/6809) done. Loss: 1.2370  lr:0.010000
[ Fri Jul 12 02:27:03 2024 ] 	Batch(5800/6809) done. Loss: 0.6276  lr:0.010000
[ Fri Jul 12 02:27:21 2024 ] 	Batch(5900/6809) done. Loss: 0.7444  lr:0.010000
[ Fri Jul 12 02:27:40 2024 ] 
Training: Epoch [15/120], Step [5999], Loss: 1.6394187211990356, Training Accuracy: 77.7
[ Fri Jul 12 02:27:40 2024 ] 	Batch(6000/6809) done. Loss: 1.0223  lr:0.010000
[ Fri Jul 12 02:27:58 2024 ] 	Batch(6100/6809) done. Loss: 0.7617  lr:0.010000
[ Fri Jul 12 02:28:16 2024 ] 	Batch(6200/6809) done. Loss: 1.0504  lr:0.010000
[ Fri Jul 12 02:28:34 2024 ] 	Batch(6300/6809) done. Loss: 0.7579  lr:0.010000
[ Fri Jul 12 02:28:52 2024 ] 	Batch(6400/6809) done. Loss: 0.6063  lr:0.010000
[ Fri Jul 12 02:29:09 2024 ] 
Training: Epoch [15/120], Step [6499], Loss: 0.8061201572418213, Training Accuracy: 77.60384615384616
[ Fri Jul 12 02:29:10 2024 ] 	Batch(6500/6809) done. Loss: 0.5519  lr:0.010000
[ Fri Jul 12 02:29:27 2024 ] 	Batch(6600/6809) done. Loss: 1.0195  lr:0.010000
[ Fri Jul 12 02:29:45 2024 ] 	Batch(6700/6809) done. Loss: 0.8424  lr:0.010000
[ Fri Jul 12 02:30:03 2024 ] 	Batch(6800/6809) done. Loss: 0.1970  lr:0.010000
[ Fri Jul 12 02:30:05 2024 ] 	Mean training loss: 0.7404.
[ Fri Jul 12 02:30:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 02:30:05 2024 ] Training epoch: 17
[ Fri Jul 12 02:30:06 2024 ] 	Batch(0/6809) done. Loss: 0.8294  lr:0.010000
[ Fri Jul 12 02:30:24 2024 ] 	Batch(100/6809) done. Loss: 0.5014  lr:0.010000
[ Fri Jul 12 02:30:42 2024 ] 	Batch(200/6809) done. Loss: 0.5607  lr:0.010000
[ Fri Jul 12 02:31:01 2024 ] 	Batch(300/6809) done. Loss: 0.4392  lr:0.010000
[ Fri Jul 12 02:31:19 2024 ] 	Batch(400/6809) done. Loss: 0.8189  lr:0.010000
[ Fri Jul 12 02:31:37 2024 ] 
Training: Epoch [16/120], Step [499], Loss: 0.918372631072998, Training Accuracy: 78.64999999999999
[ Fri Jul 12 02:31:37 2024 ] 	Batch(500/6809) done. Loss: 1.4387  lr:0.010000
[ Fri Jul 12 02:31:56 2024 ] 	Batch(600/6809) done. Loss: 1.2089  lr:0.010000
[ Fri Jul 12 02:32:14 2024 ] 	Batch(700/6809) done. Loss: 0.6171  lr:0.010000
[ Fri Jul 12 02:32:33 2024 ] 	Batch(800/6809) done. Loss: 0.3747  lr:0.010000
[ Fri Jul 12 02:32:51 2024 ] 	Batch(900/6809) done. Loss: 0.5403  lr:0.010000
[ Fri Jul 12 02:33:09 2024 ] 
Training: Epoch [16/120], Step [999], Loss: 0.3773849904537201, Training Accuracy: 78.73750000000001
[ Fri Jul 12 02:33:09 2024 ] 	Batch(1000/6809) done. Loss: 0.5513  lr:0.010000
[ Fri Jul 12 02:33:27 2024 ] 	Batch(1100/6809) done. Loss: 2.1951  lr:0.010000
[ Fri Jul 12 02:33:45 2024 ] 	Batch(1200/6809) done. Loss: 0.2529  lr:0.010000
[ Fri Jul 12 02:34:03 2024 ] 	Batch(1300/6809) done. Loss: 0.3080  lr:0.010000
[ Fri Jul 12 02:34:20 2024 ] 	Batch(1400/6809) done. Loss: 0.3060  lr:0.010000
[ Fri Jul 12 02:34:38 2024 ] 
Training: Epoch [16/120], Step [1499], Loss: 0.25209763646125793, Training Accuracy: 78.68333333333334
[ Fri Jul 12 02:34:38 2024 ] 	Batch(1500/6809) done. Loss: 1.1070  lr:0.010000
[ Fri Jul 12 02:34:56 2024 ] 	Batch(1600/6809) done. Loss: 0.3566  lr:0.010000
[ Fri Jul 12 02:35:14 2024 ] 	Batch(1700/6809) done. Loss: 0.1895  lr:0.010000
[ Fri Jul 12 02:35:32 2024 ] 	Batch(1800/6809) done. Loss: 0.2164  lr:0.010000
[ Fri Jul 12 02:35:50 2024 ] 	Batch(1900/6809) done. Loss: 0.1822  lr:0.010000
[ Fri Jul 12 02:36:08 2024 ] 
Training: Epoch [16/120], Step [1999], Loss: 0.9604114294052124, Training Accuracy: 78.71249999999999
[ Fri Jul 12 02:36:08 2024 ] 	Batch(2000/6809) done. Loss: 0.6037  lr:0.010000
[ Fri Jul 12 02:36:26 2024 ] 	Batch(2100/6809) done. Loss: 0.3653  lr:0.010000
[ Fri Jul 12 02:36:44 2024 ] 	Batch(2200/6809) done. Loss: 0.3759  lr:0.010000
[ Fri Jul 12 02:37:02 2024 ] 	Batch(2300/6809) done. Loss: 0.5753  lr:0.010000
[ Fri Jul 12 02:37:20 2024 ] 	Batch(2400/6809) done. Loss: 0.1889  lr:0.010000
[ Fri Jul 12 02:37:37 2024 ] 
Training: Epoch [16/120], Step [2499], Loss: 1.0352098941802979, Training Accuracy: 78.48
[ Fri Jul 12 02:37:38 2024 ] 	Batch(2500/6809) done. Loss: 0.9180  lr:0.010000
[ Fri Jul 12 02:37:56 2024 ] 	Batch(2600/6809) done. Loss: 0.1281  lr:0.010000
[ Fri Jul 12 02:38:13 2024 ] 	Batch(2700/6809) done. Loss: 1.3313  lr:0.010000
[ Fri Jul 12 02:38:32 2024 ] 	Batch(2800/6809) done. Loss: 0.9176  lr:0.010000
[ Fri Jul 12 02:38:49 2024 ] 	Batch(2900/6809) done. Loss: 0.4390  lr:0.010000
[ Fri Jul 12 02:39:07 2024 ] 
Training: Epoch [16/120], Step [2999], Loss: 0.5535907745361328, Training Accuracy: 78.32916666666667
[ Fri Jul 12 02:39:07 2024 ] 	Batch(3000/6809) done. Loss: 0.5134  lr:0.010000
[ Fri Jul 12 02:39:25 2024 ] 	Batch(3100/6809) done. Loss: 0.5313  lr:0.010000
[ Fri Jul 12 02:39:43 2024 ] 	Batch(3200/6809) done. Loss: 1.1152  lr:0.010000
[ Fri Jul 12 02:40:01 2024 ] 	Batch(3300/6809) done. Loss: 1.1076  lr:0.010000
[ Fri Jul 12 02:40:19 2024 ] 	Batch(3400/6809) done. Loss: 0.9387  lr:0.010000
[ Fri Jul 12 02:40:37 2024 ] 
Training: Epoch [16/120], Step [3499], Loss: 1.1847928762435913, Training Accuracy: 78.19285714285714
[ Fri Jul 12 02:40:37 2024 ] 	Batch(3500/6809) done. Loss: 0.4534  lr:0.010000
[ Fri Jul 12 02:40:55 2024 ] 	Batch(3600/6809) done. Loss: 0.3118  lr:0.010000
[ Fri Jul 12 02:41:13 2024 ] 	Batch(3700/6809) done. Loss: 0.6276  lr:0.010000
[ Fri Jul 12 02:41:31 2024 ] 	Batch(3800/6809) done. Loss: 0.7919  lr:0.010000
[ Fri Jul 12 02:41:49 2024 ] 	Batch(3900/6809) done. Loss: 0.9890  lr:0.010000
[ Fri Jul 12 02:42:06 2024 ] 
Training: Epoch [16/120], Step [3999], Loss: 0.44486480951309204, Training Accuracy: 78.246875
[ Fri Jul 12 02:42:07 2024 ] 	Batch(4000/6809) done. Loss: 1.1511  lr:0.010000
[ Fri Jul 12 02:42:25 2024 ] 	Batch(4100/6809) done. Loss: 0.7853  lr:0.010000
[ Fri Jul 12 02:42:42 2024 ] 	Batch(4200/6809) done. Loss: 0.0436  lr:0.010000
[ Fri Jul 12 02:43:00 2024 ] 	Batch(4300/6809) done. Loss: 0.6537  lr:0.010000
[ Fri Jul 12 02:43:18 2024 ] 	Batch(4400/6809) done. Loss: 1.2493  lr:0.010000
[ Fri Jul 12 02:43:37 2024 ] 
Training: Epoch [16/120], Step [4499], Loss: 0.23503300547599792, Training Accuracy: 78.18888888888888
[ Fri Jul 12 02:43:37 2024 ] 	Batch(4500/6809) done. Loss: 0.5816  lr:0.010000
[ Fri Jul 12 02:43:55 2024 ] 	Batch(4600/6809) done. Loss: 0.6401  lr:0.010000
[ Fri Jul 12 02:44:13 2024 ] 	Batch(4700/6809) done. Loss: 0.3511  lr:0.010000
[ Fri Jul 12 02:44:31 2024 ] 	Batch(4800/6809) done. Loss: 0.9741  lr:0.010000
[ Fri Jul 12 02:44:48 2024 ] 	Batch(4900/6809) done. Loss: 1.0994  lr:0.010000
[ Fri Jul 12 02:45:06 2024 ] 
Training: Epoch [16/120], Step [4999], Loss: 0.3541373312473297, Training Accuracy: 78.14999999999999
[ Fri Jul 12 02:45:06 2024 ] 	Batch(5000/6809) done. Loss: 0.2814  lr:0.010000
[ Fri Jul 12 02:45:24 2024 ] 	Batch(5100/6809) done. Loss: 0.8989  lr:0.010000
[ Fri Jul 12 02:45:42 2024 ] 	Batch(5200/6809) done. Loss: 1.4424  lr:0.010000
[ Fri Jul 12 02:46:01 2024 ] 	Batch(5300/6809) done. Loss: 1.3710  lr:0.010000
[ Fri Jul 12 02:46:18 2024 ] 	Batch(5400/6809) done. Loss: 0.1645  lr:0.010000
[ Fri Jul 12 02:46:36 2024 ] 
Training: Epoch [16/120], Step [5499], Loss: 0.4234364628791809, Training Accuracy: 78.17727272727272
[ Fri Jul 12 02:46:36 2024 ] 	Batch(5500/6809) done. Loss: 0.9249  lr:0.010000
[ Fri Jul 12 02:46:54 2024 ] 	Batch(5600/6809) done. Loss: 0.7762  lr:0.010000
[ Fri Jul 12 02:47:12 2024 ] 	Batch(5700/6809) done. Loss: 0.6681  lr:0.010000
[ Fri Jul 12 02:47:30 2024 ] 	Batch(5800/6809) done. Loss: 0.6688  lr:0.010000
[ Fri Jul 12 02:47:48 2024 ] 	Batch(5900/6809) done. Loss: 0.7429  lr:0.010000
[ Fri Jul 12 02:48:06 2024 ] 
Training: Epoch [16/120], Step [5999], Loss: 0.45185035467147827, Training Accuracy: 78.16875
[ Fri Jul 12 02:48:06 2024 ] 	Batch(6000/6809) done. Loss: 0.5665  lr:0.010000
[ Fri Jul 12 02:48:24 2024 ] 	Batch(6100/6809) done. Loss: 0.1289  lr:0.010000
[ Fri Jul 12 02:48:42 2024 ] 	Batch(6200/6809) done. Loss: 0.1306  lr:0.010000
[ Fri Jul 12 02:49:00 2024 ] 	Batch(6300/6809) done. Loss: 0.9384  lr:0.010000
[ Fri Jul 12 02:49:18 2024 ] 	Batch(6400/6809) done. Loss: 0.8176  lr:0.010000
[ Fri Jul 12 02:49:35 2024 ] 
Training: Epoch [16/120], Step [6499], Loss: 2.008770704269409, Training Accuracy: 78.18846153846154
[ Fri Jul 12 02:49:36 2024 ] 	Batch(6500/6809) done. Loss: 0.8141  lr:0.010000
[ Fri Jul 12 02:49:53 2024 ] 	Batch(6600/6809) done. Loss: 0.9902  lr:0.010000
[ Fri Jul 12 02:50:11 2024 ] 	Batch(6700/6809) done. Loss: 0.3569  lr:0.010000
[ Fri Jul 12 02:50:29 2024 ] 	Batch(6800/6809) done. Loss: 1.1910  lr:0.010000
[ Fri Jul 12 02:50:31 2024 ] 	Mean training loss: 0.7173.
[ Fri Jul 12 02:50:31 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 02:50:31 2024 ] Training epoch: 18
[ Fri Jul 12 02:50:32 2024 ] 	Batch(0/6809) done. Loss: 0.0254  lr:0.010000
[ Fri Jul 12 02:50:50 2024 ] 	Batch(100/6809) done. Loss: 0.3435  lr:0.010000
[ Fri Jul 12 02:51:08 2024 ] 	Batch(200/6809) done. Loss: 1.0085  lr:0.010000
[ Fri Jul 12 02:51:26 2024 ] 	Batch(300/6809) done. Loss: 0.3962  lr:0.010000
[ Fri Jul 12 02:51:44 2024 ] 	Batch(400/6809) done. Loss: 0.0539  lr:0.010000
[ Fri Jul 12 02:52:02 2024 ] 
Training: Epoch [17/120], Step [499], Loss: 0.27836909890174866, Training Accuracy: 78.95
[ Fri Jul 12 02:52:02 2024 ] 	Batch(500/6809) done. Loss: 0.0900  lr:0.010000
[ Fri Jul 12 02:52:20 2024 ] 	Batch(600/6809) done. Loss: 0.9081  lr:0.010000
[ Fri Jul 12 02:52:38 2024 ] 	Batch(700/6809) done. Loss: 0.5702  lr:0.010000
[ Fri Jul 12 02:52:56 2024 ] 	Batch(800/6809) done. Loss: 0.3255  lr:0.010000
[ Fri Jul 12 02:53:14 2024 ] 	Batch(900/6809) done. Loss: 1.2121  lr:0.010000
[ Fri Jul 12 02:53:31 2024 ] 
Training: Epoch [17/120], Step [999], Loss: 0.6335641741752625, Training Accuracy: 78.9375
[ Fri Jul 12 02:53:32 2024 ] 	Batch(1000/6809) done. Loss: 0.4012  lr:0.010000
[ Fri Jul 12 02:53:49 2024 ] 	Batch(1100/6809) done. Loss: 0.3572  lr:0.010000
[ Fri Jul 12 02:54:08 2024 ] 	Batch(1200/6809) done. Loss: 0.7120  lr:0.010000
[ Fri Jul 12 02:54:25 2024 ] 	Batch(1300/6809) done. Loss: 0.1237  lr:0.010000
[ Fri Jul 12 02:54:43 2024 ] 	Batch(1400/6809) done. Loss: 0.0727  lr:0.010000
[ Fri Jul 12 02:55:01 2024 ] 
Training: Epoch [17/120], Step [1499], Loss: 0.686582088470459, Training Accuracy: 79.025
[ Fri Jul 12 02:55:01 2024 ] 	Batch(1500/6809) done. Loss: 0.8052  lr:0.010000
[ Fri Jul 12 02:55:19 2024 ] 	Batch(1600/6809) done. Loss: 0.5797  lr:0.010000
[ Fri Jul 12 02:55:37 2024 ] 	Batch(1700/6809) done. Loss: 0.3878  lr:0.010000
[ Fri Jul 12 02:55:55 2024 ] 	Batch(1800/6809) done. Loss: 0.1476  lr:0.010000
[ Fri Jul 12 02:56:13 2024 ] 	Batch(1900/6809) done. Loss: 0.2976  lr:0.010000
[ Fri Jul 12 02:56:31 2024 ] 
Training: Epoch [17/120], Step [1999], Loss: 0.3861958682537079, Training Accuracy: 78.99374999999999
[ Fri Jul 12 02:56:31 2024 ] 	Batch(2000/6809) done. Loss: 0.3825  lr:0.010000
[ Fri Jul 12 02:56:50 2024 ] 	Batch(2100/6809) done. Loss: 0.9251  lr:0.010000
[ Fri Jul 12 02:57:08 2024 ] 	Batch(2200/6809) done. Loss: 1.0003  lr:0.010000
[ Fri Jul 12 02:57:27 2024 ] 	Batch(2300/6809) done. Loss: 1.0213  lr:0.010000
[ Fri Jul 12 02:57:46 2024 ] 	Batch(2400/6809) done. Loss: 0.2648  lr:0.010000
[ Fri Jul 12 02:58:03 2024 ] 
Training: Epoch [17/120], Step [2499], Loss: 1.4828567504882812, Training Accuracy: 78.69
[ Fri Jul 12 02:58:03 2024 ] 	Batch(2500/6809) done. Loss: 0.7560  lr:0.010000
[ Fri Jul 12 02:58:21 2024 ] 	Batch(2600/6809) done. Loss: 0.9830  lr:0.010000
[ Fri Jul 12 02:58:39 2024 ] 	Batch(2700/6809) done. Loss: 0.2955  lr:0.010000
[ Fri Jul 12 02:58:58 2024 ] 	Batch(2800/6809) done. Loss: 0.7224  lr:0.010000
[ Fri Jul 12 02:59:16 2024 ] 	Batch(2900/6809) done. Loss: 0.3764  lr:0.010000
[ Fri Jul 12 02:59:34 2024 ] 
Training: Epoch [17/120], Step [2999], Loss: 0.8180573582649231, Training Accuracy: 78.61666666666667
[ Fri Jul 12 02:59:35 2024 ] 	Batch(3000/6809) done. Loss: 0.7870  lr:0.010000
[ Fri Jul 12 02:59:53 2024 ] 	Batch(3100/6809) done. Loss: 0.3912  lr:0.010000
[ Fri Jul 12 03:00:12 2024 ] 	Batch(3200/6809) done. Loss: 0.3982  lr:0.010000
[ Fri Jul 12 03:00:30 2024 ] 	Batch(3300/6809) done. Loss: 0.4733  lr:0.010000
[ Fri Jul 12 03:00:48 2024 ] 	Batch(3400/6809) done. Loss: 0.8434  lr:0.010000
[ Fri Jul 12 03:01:05 2024 ] 
Training: Epoch [17/120], Step [3499], Loss: 0.2939226031303406, Training Accuracy: 78.55357142857143
[ Fri Jul 12 03:01:06 2024 ] 	Batch(3500/6809) done. Loss: 0.2333  lr:0.010000
[ Fri Jul 12 03:01:24 2024 ] 	Batch(3600/6809) done. Loss: 0.9735  lr:0.010000
[ Fri Jul 12 03:01:42 2024 ] 	Batch(3700/6809) done. Loss: 0.3960  lr:0.010000
[ Fri Jul 12 03:01:59 2024 ] 	Batch(3800/6809) done. Loss: 0.5046  lr:0.010000
[ Fri Jul 12 03:02:17 2024 ] 	Batch(3900/6809) done. Loss: 0.4064  lr:0.010000
[ Fri Jul 12 03:02:35 2024 ] 
Training: Epoch [17/120], Step [3999], Loss: 0.36164143681526184, Training Accuracy: 78.5625
[ Fri Jul 12 03:02:35 2024 ] 	Batch(4000/6809) done. Loss: 0.7625  lr:0.010000
[ Fri Jul 12 03:02:53 2024 ] 	Batch(4100/6809) done. Loss: 0.4311  lr:0.010000
[ Fri Jul 12 03:03:11 2024 ] 	Batch(4200/6809) done. Loss: 1.4498  lr:0.010000
[ Fri Jul 12 03:03:29 2024 ] 	Batch(4300/6809) done. Loss: 0.9094  lr:0.010000
[ Fri Jul 12 03:03:47 2024 ] 	Batch(4400/6809) done. Loss: 1.4295  lr:0.010000
[ Fri Jul 12 03:04:05 2024 ] 
Training: Epoch [17/120], Step [4499], Loss: 1.0830745697021484, Training Accuracy: 78.65833333333333
[ Fri Jul 12 03:04:05 2024 ] 	Batch(4500/6809) done. Loss: 0.6787  lr:0.010000
[ Fri Jul 12 03:04:23 2024 ] 	Batch(4600/6809) done. Loss: 0.2938  lr:0.010000
[ Fri Jul 12 03:04:41 2024 ] 	Batch(4700/6809) done. Loss: 0.4944  lr:0.010000
[ Fri Jul 12 03:04:59 2024 ] 	Batch(4800/6809) done. Loss: 0.9368  lr:0.010000
[ Fri Jul 12 03:05:17 2024 ] 	Batch(4900/6809) done. Loss: 0.7952  lr:0.010000
[ Fri Jul 12 03:05:35 2024 ] 
Training: Epoch [17/120], Step [4999], Loss: 0.4067283868789673, Training Accuracy: 78.71249999999999
[ Fri Jul 12 03:05:35 2024 ] 	Batch(5000/6809) done. Loss: 1.2276  lr:0.010000
[ Fri Jul 12 03:05:53 2024 ] 	Batch(5100/6809) done. Loss: 1.0277  lr:0.010000
[ Fri Jul 12 03:06:11 2024 ] 	Batch(5200/6809) done. Loss: 0.2034  lr:0.010000
[ Fri Jul 12 03:06:29 2024 ] 	Batch(5300/6809) done. Loss: 0.9230  lr:0.010000
[ Fri Jul 12 03:06:47 2024 ] 	Batch(5400/6809) done. Loss: 0.4048  lr:0.010000
[ Fri Jul 12 03:07:04 2024 ] 
Training: Epoch [17/120], Step [5499], Loss: 0.9636759757995605, Training Accuracy: 78.74545454545454
[ Fri Jul 12 03:07:05 2024 ] 	Batch(5500/6809) done. Loss: 0.7754  lr:0.010000
[ Fri Jul 12 03:07:23 2024 ] 	Batch(5600/6809) done. Loss: 0.5473  lr:0.010000
[ Fri Jul 12 03:07:40 2024 ] 	Batch(5700/6809) done. Loss: 0.2194  lr:0.010000
[ Fri Jul 12 03:07:58 2024 ] 	Batch(5800/6809) done. Loss: 0.5380  lr:0.010000
[ Fri Jul 12 03:08:16 2024 ] 	Batch(5900/6809) done. Loss: 1.3352  lr:0.010000
[ Fri Jul 12 03:08:34 2024 ] 
Training: Epoch [17/120], Step [5999], Loss: 0.5452051162719727, Training Accuracy: 78.63125
[ Fri Jul 12 03:08:34 2024 ] 	Batch(6000/6809) done. Loss: 0.1077  lr:0.010000
[ Fri Jul 12 03:08:52 2024 ] 	Batch(6100/6809) done. Loss: 0.1678  lr:0.010000
[ Fri Jul 12 03:09:10 2024 ] 	Batch(6200/6809) done. Loss: 0.3869  lr:0.010000
[ Fri Jul 12 03:09:28 2024 ] 	Batch(6300/6809) done. Loss: 1.1944  lr:0.010000
[ Fri Jul 12 03:09:46 2024 ] 	Batch(6400/6809) done. Loss: 0.8191  lr:0.010000
[ Fri Jul 12 03:10:04 2024 ] 
Training: Epoch [17/120], Step [6499], Loss: 0.5937974452972412, Training Accuracy: 78.59807692307692
[ Fri Jul 12 03:10:04 2024 ] 	Batch(6500/6809) done. Loss: 0.1787  lr:0.010000
[ Fri Jul 12 03:10:22 2024 ] 	Batch(6600/6809) done. Loss: 0.7669  lr:0.010000
[ Fri Jul 12 03:10:40 2024 ] 	Batch(6700/6809) done. Loss: 0.1667  lr:0.010000
[ Fri Jul 12 03:10:58 2024 ] 	Batch(6800/6809) done. Loss: 0.6904  lr:0.010000
[ Fri Jul 12 03:10:59 2024 ] 	Mean training loss: 0.7052.
[ Fri Jul 12 03:10:59 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 03:10:59 2024 ] Training epoch: 19
[ Fri Jul 12 03:11:00 2024 ] 	Batch(0/6809) done. Loss: 0.6834  lr:0.010000
[ Fri Jul 12 03:11:18 2024 ] 	Batch(100/6809) done. Loss: 0.6686  lr:0.010000
[ Fri Jul 12 03:11:37 2024 ] 	Batch(200/6809) done. Loss: 1.4406  lr:0.010000
[ Fri Jul 12 03:11:55 2024 ] 	Batch(300/6809) done. Loss: 0.6463  lr:0.010000
[ Fri Jul 12 03:12:13 2024 ] 	Batch(400/6809) done. Loss: 1.3260  lr:0.010000
[ Fri Jul 12 03:12:32 2024 ] 
Training: Epoch [18/120], Step [499], Loss: 0.2909359037876129, Training Accuracy: 80.025
[ Fri Jul 12 03:12:32 2024 ] 	Batch(500/6809) done. Loss: 0.7880  lr:0.010000
[ Fri Jul 12 03:12:50 2024 ] 	Batch(600/6809) done. Loss: 0.5984  lr:0.010000
[ Fri Jul 12 03:13:08 2024 ] 	Batch(700/6809) done. Loss: 0.2593  lr:0.010000
[ Fri Jul 12 03:13:27 2024 ] 	Batch(800/6809) done. Loss: 0.1846  lr:0.010000
[ Fri Jul 12 03:13:45 2024 ] 	Batch(900/6809) done. Loss: 0.6126  lr:0.010000
[ Fri Jul 12 03:14:03 2024 ] 
Training: Epoch [18/120], Step [999], Loss: 0.6507769227027893, Training Accuracy: 80.175
[ Fri Jul 12 03:14:04 2024 ] 	Batch(1000/6809) done. Loss: 0.4882  lr:0.010000
[ Fri Jul 12 03:14:22 2024 ] 	Batch(1100/6809) done. Loss: 0.1337  lr:0.010000
[ Fri Jul 12 03:14:40 2024 ] 	Batch(1200/6809) done. Loss: 0.5399  lr:0.010000
[ Fri Jul 12 03:14:59 2024 ] 	Batch(1300/6809) done. Loss: 0.6783  lr:0.010000
[ Fri Jul 12 03:15:17 2024 ] 	Batch(1400/6809) done. Loss: 0.2471  lr:0.010000
[ Fri Jul 12 03:15:35 2024 ] 
Training: Epoch [18/120], Step [1499], Loss: 0.8699389696121216, Training Accuracy: 80.5
[ Fri Jul 12 03:15:35 2024 ] 	Batch(1500/6809) done. Loss: 0.8622  lr:0.010000
[ Fri Jul 12 03:15:54 2024 ] 	Batch(1600/6809) done. Loss: 1.0016  lr:0.010000
[ Fri Jul 12 03:16:12 2024 ] 	Batch(1700/6809) done. Loss: 0.2355  lr:0.010000
[ Fri Jul 12 03:16:30 2024 ] 	Batch(1800/6809) done. Loss: 0.6498  lr:0.010000
[ Fri Jul 12 03:16:49 2024 ] 	Batch(1900/6809) done. Loss: 0.6301  lr:0.010000
[ Fri Jul 12 03:17:07 2024 ] 
Training: Epoch [18/120], Step [1999], Loss: 0.25772297382354736, Training Accuracy: 80.125
[ Fri Jul 12 03:17:07 2024 ] 	Batch(2000/6809) done. Loss: 0.4270  lr:0.010000
[ Fri Jul 12 03:17:26 2024 ] 	Batch(2100/6809) done. Loss: 0.3920  lr:0.010000
[ Fri Jul 12 03:17:44 2024 ] 	Batch(2200/6809) done. Loss: 1.8959  lr:0.010000
[ Fri Jul 12 03:18:02 2024 ] 	Batch(2300/6809) done. Loss: 0.2739  lr:0.010000
[ Fri Jul 12 03:18:21 2024 ] 	Batch(2400/6809) done. Loss: 0.4236  lr:0.010000
[ Fri Jul 12 03:18:39 2024 ] 
Training: Epoch [18/120], Step [2499], Loss: 0.27087047696113586, Training Accuracy: 79.71000000000001
[ Fri Jul 12 03:18:39 2024 ] 	Batch(2500/6809) done. Loss: 0.1550  lr:0.010000
[ Fri Jul 12 03:18:58 2024 ] 	Batch(2600/6809) done. Loss: 0.1363  lr:0.010000
[ Fri Jul 12 03:19:16 2024 ] 	Batch(2700/6809) done. Loss: 0.0557  lr:0.010000
[ Fri Jul 12 03:19:34 2024 ] 	Batch(2800/6809) done. Loss: 0.9202  lr:0.010000
[ Fri Jul 12 03:19:53 2024 ] 	Batch(2900/6809) done. Loss: 1.0524  lr:0.010000
[ Fri Jul 12 03:20:11 2024 ] 
Training: Epoch [18/120], Step [2999], Loss: 0.9316611289978027, Training Accuracy: 79.45833333333333
[ Fri Jul 12 03:20:11 2024 ] 	Batch(3000/6809) done. Loss: 0.3541  lr:0.010000
[ Fri Jul 12 03:20:29 2024 ] 	Batch(3100/6809) done. Loss: 0.4486  lr:0.010000
[ Fri Jul 12 03:20:48 2024 ] 	Batch(3200/6809) done. Loss: 0.5334  lr:0.010000
[ Fri Jul 12 03:21:06 2024 ] 	Batch(3300/6809) done. Loss: 0.1838  lr:0.010000
[ Fri Jul 12 03:21:24 2024 ] 	Batch(3400/6809) done. Loss: 0.1037  lr:0.010000
[ Fri Jul 12 03:21:43 2024 ] 
Training: Epoch [18/120], Step [3499], Loss: 1.3338818550109863, Training Accuracy: 79.45357142857142
[ Fri Jul 12 03:21:43 2024 ] 	Batch(3500/6809) done. Loss: 0.0464  lr:0.010000
[ Fri Jul 12 03:22:01 2024 ] 	Batch(3600/6809) done. Loss: 0.5476  lr:0.010000
[ Fri Jul 12 03:22:20 2024 ] 	Batch(3700/6809) done. Loss: 0.8279  lr:0.010000
[ Fri Jul 12 03:22:38 2024 ] 	Batch(3800/6809) done. Loss: 0.7444  lr:0.010000
[ Fri Jul 12 03:22:56 2024 ] 	Batch(3900/6809) done. Loss: 0.0153  lr:0.010000
[ Fri Jul 12 03:23:14 2024 ] 
Training: Epoch [18/120], Step [3999], Loss: 0.12803104519844055, Training Accuracy: 79.315625
[ Fri Jul 12 03:23:15 2024 ] 	Batch(4000/6809) done. Loss: 0.0945  lr:0.010000
[ Fri Jul 12 03:23:33 2024 ] 	Batch(4100/6809) done. Loss: 0.8293  lr:0.010000
[ Fri Jul 12 03:23:52 2024 ] 	Batch(4200/6809) done. Loss: 0.4787  lr:0.010000
[ Fri Jul 12 03:24:10 2024 ] 	Batch(4300/6809) done. Loss: 0.4177  lr:0.010000
[ Fri Jul 12 03:24:29 2024 ] 	Batch(4400/6809) done. Loss: 0.3719  lr:0.010000
[ Fri Jul 12 03:24:47 2024 ] 
Training: Epoch [18/120], Step [4499], Loss: 0.3745811879634857, Training Accuracy: 79.31944444444444
[ Fri Jul 12 03:24:48 2024 ] 	Batch(4500/6809) done. Loss: 0.7716  lr:0.010000
[ Fri Jul 12 03:25:06 2024 ] 	Batch(4600/6809) done. Loss: 0.1617  lr:0.010000
[ Fri Jul 12 03:25:25 2024 ] 	Batch(4700/6809) done. Loss: 0.7500  lr:0.010000
[ Fri Jul 12 03:25:43 2024 ] 	Batch(4800/6809) done. Loss: 0.8214  lr:0.010000
[ Fri Jul 12 03:26:01 2024 ] 	Batch(4900/6809) done. Loss: 0.1765  lr:0.010000
[ Fri Jul 12 03:26:18 2024 ] 
Training: Epoch [18/120], Step [4999], Loss: 0.34387117624282837, Training Accuracy: 79.23249999999999
[ Fri Jul 12 03:26:19 2024 ] 	Batch(5000/6809) done. Loss: 0.2286  lr:0.010000
[ Fri Jul 12 03:26:36 2024 ] 	Batch(5100/6809) done. Loss: 0.1769  lr:0.010000
[ Fri Jul 12 03:26:54 2024 ] 	Batch(5200/6809) done. Loss: 1.2846  lr:0.010000
[ Fri Jul 12 03:27:12 2024 ] 	Batch(5300/6809) done. Loss: 0.9756  lr:0.010000
[ Fri Jul 12 03:27:30 2024 ] 	Batch(5400/6809) done. Loss: 0.5216  lr:0.010000
[ Fri Jul 12 03:27:48 2024 ] 
Training: Epoch [18/120], Step [5499], Loss: 1.0423626899719238, Training Accuracy: 79.19772727272726
[ Fri Jul 12 03:27:48 2024 ] 	Batch(5500/6809) done. Loss: 1.4587  lr:0.010000
[ Fri Jul 12 03:28:06 2024 ] 	Batch(5600/6809) done. Loss: 0.6442  lr:0.010000
[ Fri Jul 12 03:28:24 2024 ] 	Batch(5700/6809) done. Loss: 0.7861  lr:0.010000
[ Fri Jul 12 03:28:42 2024 ] 	Batch(5800/6809) done. Loss: 0.0879  lr:0.010000
[ Fri Jul 12 03:29:00 2024 ] 	Batch(5900/6809) done. Loss: 0.6570  lr:0.010000
[ Fri Jul 12 03:29:18 2024 ] 
Training: Epoch [18/120], Step [5999], Loss: 0.7754532098770142, Training Accuracy: 79.19583333333333
[ Fri Jul 12 03:29:18 2024 ] 	Batch(6000/6809) done. Loss: 0.5936  lr:0.010000
[ Fri Jul 12 03:29:36 2024 ] 	Batch(6100/6809) done. Loss: 0.5094  lr:0.010000
[ Fri Jul 12 03:29:54 2024 ] 	Batch(6200/6809) done. Loss: 1.2824  lr:0.010000
[ Fri Jul 12 03:30:12 2024 ] 	Batch(6300/6809) done. Loss: 0.3612  lr:0.010000
[ Fri Jul 12 03:30:30 2024 ] 	Batch(6400/6809) done. Loss: 0.1128  lr:0.010000
[ Fri Jul 12 03:30:47 2024 ] 
Training: Epoch [18/120], Step [6499], Loss: 0.4037797749042511, Training Accuracy: 79.21923076923076
[ Fri Jul 12 03:30:48 2024 ] 	Batch(6500/6809) done. Loss: 1.2211  lr:0.010000
[ Fri Jul 12 03:31:06 2024 ] 	Batch(6600/6809) done. Loss: 0.7164  lr:0.010000
[ Fri Jul 12 03:31:23 2024 ] 	Batch(6700/6809) done. Loss: 1.0957  lr:0.010000
[ Fri Jul 12 03:31:41 2024 ] 	Batch(6800/6809) done. Loss: 0.6342  lr:0.010000
[ Fri Jul 12 03:31:43 2024 ] 	Mean training loss: 0.6758.
[ Fri Jul 12 03:31:43 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 03:31:43 2024 ] Training epoch: 20
[ Fri Jul 12 03:31:44 2024 ] 	Batch(0/6809) done. Loss: 0.7398  lr:0.010000
[ Fri Jul 12 03:32:02 2024 ] 	Batch(100/6809) done. Loss: 1.2994  lr:0.010000
[ Fri Jul 12 03:32:21 2024 ] 	Batch(200/6809) done. Loss: 1.3958  lr:0.010000
[ Fri Jul 12 03:32:39 2024 ] 	Batch(300/6809) done. Loss: 0.1168  lr:0.010000
[ Fri Jul 12 03:32:57 2024 ] 	Batch(400/6809) done. Loss: 0.4002  lr:0.010000
[ Fri Jul 12 03:33:15 2024 ] 
Training: Epoch [19/120], Step [499], Loss: 0.8913159370422363, Training Accuracy: 80.05
[ Fri Jul 12 03:33:15 2024 ] 	Batch(500/6809) done. Loss: 0.4165  lr:0.010000
[ Fri Jul 12 03:33:33 2024 ] 	Batch(600/6809) done. Loss: 0.1137  lr:0.010000
[ Fri Jul 12 03:33:51 2024 ] 	Batch(700/6809) done. Loss: 0.7432  lr:0.010000
[ Fri Jul 12 03:34:10 2024 ] 	Batch(800/6809) done. Loss: 0.2083  lr:0.010000
[ Fri Jul 12 03:34:28 2024 ] 	Batch(900/6809) done. Loss: 1.0632  lr:0.010000
[ Fri Jul 12 03:34:47 2024 ] 
Training: Epoch [19/120], Step [999], Loss: 0.22830425202846527, Training Accuracy: 79.83749999999999
[ Fri Jul 12 03:34:47 2024 ] 	Batch(1000/6809) done. Loss: 0.2996  lr:0.010000
[ Fri Jul 12 03:35:05 2024 ] 	Batch(1100/6809) done. Loss: 1.3695  lr:0.010000
[ Fri Jul 12 03:35:23 2024 ] 	Batch(1200/6809) done. Loss: 0.2445  lr:0.010000
[ Fri Jul 12 03:35:41 2024 ] 	Batch(1300/6809) done. Loss: 0.3065  lr:0.010000
[ Fri Jul 12 03:35:59 2024 ] 	Batch(1400/6809) done. Loss: 0.4076  lr:0.010000
[ Fri Jul 12 03:36:17 2024 ] 
Training: Epoch [19/120], Step [1499], Loss: 0.6650123000144958, Training Accuracy: 79.78333333333333
[ Fri Jul 12 03:36:17 2024 ] 	Batch(1500/6809) done. Loss: 0.1676  lr:0.010000
[ Fri Jul 12 03:36:35 2024 ] 	Batch(1600/6809) done. Loss: 0.5211  lr:0.010000
[ Fri Jul 12 03:36:53 2024 ] 	Batch(1700/6809) done. Loss: 0.3226  lr:0.010000
[ Fri Jul 12 03:37:11 2024 ] 	Batch(1800/6809) done. Loss: 0.0577  lr:0.010000
[ Fri Jul 12 03:37:29 2024 ] 	Batch(1900/6809) done. Loss: 0.1936  lr:0.010000
[ Fri Jul 12 03:37:47 2024 ] 
Training: Epoch [19/120], Step [1999], Loss: 0.07740974426269531, Training Accuracy: 80.0375
[ Fri Jul 12 03:37:47 2024 ] 	Batch(2000/6809) done. Loss: 0.4742  lr:0.010000
[ Fri Jul 12 03:38:06 2024 ] 	Batch(2100/6809) done. Loss: 0.6975  lr:0.010000
[ Fri Jul 12 03:38:24 2024 ] 	Batch(2200/6809) done. Loss: 0.7229  lr:0.010000
[ Fri Jul 12 03:38:43 2024 ] 	Batch(2300/6809) done. Loss: 0.3215  lr:0.010000
[ Fri Jul 12 03:39:01 2024 ] 	Batch(2400/6809) done. Loss: 0.0834  lr:0.010000
[ Fri Jul 12 03:39:19 2024 ] 
Training: Epoch [19/120], Step [2499], Loss: 0.533257007598877, Training Accuracy: 80.05
[ Fri Jul 12 03:39:19 2024 ] 	Batch(2500/6809) done. Loss: 0.8720  lr:0.010000
[ Fri Jul 12 03:39:37 2024 ] 	Batch(2600/6809) done. Loss: 0.5567  lr:0.010000
[ Fri Jul 12 03:39:55 2024 ] 	Batch(2700/6809) done. Loss: 0.1227  lr:0.010000
[ Fri Jul 12 03:40:13 2024 ] 	Batch(2800/6809) done. Loss: 0.8153  lr:0.010000
[ Fri Jul 12 03:40:31 2024 ] 	Batch(2900/6809) done. Loss: 0.4864  lr:0.010000
[ Fri Jul 12 03:40:49 2024 ] 
Training: Epoch [19/120], Step [2999], Loss: 1.4240540266036987, Training Accuracy: 80.01666666666667
[ Fri Jul 12 03:40:49 2024 ] 	Batch(3000/6809) done. Loss: 0.0517  lr:0.010000
[ Fri Jul 12 03:41:07 2024 ] 	Batch(3100/6809) done. Loss: 0.7318  lr:0.010000
[ Fri Jul 12 03:41:25 2024 ] 	Batch(3200/6809) done. Loss: 0.2692  lr:0.010000
[ Fri Jul 12 03:41:43 2024 ] 	Batch(3300/6809) done. Loss: 0.8557  lr:0.010000
[ Fri Jul 12 03:42:01 2024 ] 	Batch(3400/6809) done. Loss: 0.7847  lr:0.010000
[ Fri Jul 12 03:42:19 2024 ] 
Training: Epoch [19/120], Step [3499], Loss: 0.808972954750061, Training Accuracy: 80.23214285714285
[ Fri Jul 12 03:42:19 2024 ] 	Batch(3500/6809) done. Loss: 0.7003  lr:0.010000
[ Fri Jul 12 03:42:38 2024 ] 	Batch(3600/6809) done. Loss: 0.2073  lr:0.010000
[ Fri Jul 12 03:42:56 2024 ] 	Batch(3700/6809) done. Loss: 0.0577  lr:0.010000
[ Fri Jul 12 03:43:14 2024 ] 	Batch(3800/6809) done. Loss: 0.6644  lr:0.010000
[ Fri Jul 12 03:43:32 2024 ] 	Batch(3900/6809) done. Loss: 0.7178  lr:0.010000
[ Fri Jul 12 03:43:51 2024 ] 
Training: Epoch [19/120], Step [3999], Loss: 1.141892433166504, Training Accuracy: 80.10312499999999
[ Fri Jul 12 03:43:51 2024 ] 	Batch(4000/6809) done. Loss: 0.7148  lr:0.010000
[ Fri Jul 12 03:44:09 2024 ] 	Batch(4100/6809) done. Loss: 0.7721  lr:0.010000
[ Fri Jul 12 03:44:27 2024 ] 	Batch(4200/6809) done. Loss: 0.6760  lr:0.010000
[ Fri Jul 12 03:44:46 2024 ] 	Batch(4300/6809) done. Loss: 0.5979  lr:0.010000
[ Fri Jul 12 03:45:04 2024 ] 	Batch(4400/6809) done. Loss: 0.7669  lr:0.010000
[ Fri Jul 12 03:45:22 2024 ] 
Training: Epoch [19/120], Step [4499], Loss: 1.5227205753326416, Training Accuracy: 80.01388888888889
[ Fri Jul 12 03:45:22 2024 ] 	Batch(4500/6809) done. Loss: 1.0720  lr:0.010000
[ Fri Jul 12 03:45:40 2024 ] 	Batch(4600/6809) done. Loss: 0.8059  lr:0.010000
[ Fri Jul 12 03:45:58 2024 ] 	Batch(4700/6809) done. Loss: 0.9446  lr:0.010000
[ Fri Jul 12 03:46:17 2024 ] 	Batch(4800/6809) done. Loss: 0.3915  lr:0.010000
[ Fri Jul 12 03:46:35 2024 ] 	Batch(4900/6809) done. Loss: 0.3495  lr:0.010000
[ Fri Jul 12 03:46:53 2024 ] 
Training: Epoch [19/120], Step [4999], Loss: 0.48069193959236145, Training Accuracy: 80.0425
[ Fri Jul 12 03:46:53 2024 ] 	Batch(5000/6809) done. Loss: 0.7737  lr:0.010000
[ Fri Jul 12 03:47:11 2024 ] 	Batch(5100/6809) done. Loss: 0.8951  lr:0.010000
[ Fri Jul 12 03:47:30 2024 ] 	Batch(5200/6809) done. Loss: 0.0880  lr:0.010000
[ Fri Jul 12 03:47:48 2024 ] 	Batch(5300/6809) done. Loss: 0.8514  lr:0.010000
[ Fri Jul 12 03:48:06 2024 ] 	Batch(5400/6809) done. Loss: 0.3594  lr:0.010000
[ Fri Jul 12 03:48:24 2024 ] 
Training: Epoch [19/120], Step [5499], Loss: 1.1889348030090332, Training Accuracy: 79.95454545454545
[ Fri Jul 12 03:48:25 2024 ] 	Batch(5500/6809) done. Loss: 0.1535  lr:0.010000
[ Fri Jul 12 03:48:43 2024 ] 	Batch(5600/6809) done. Loss: 0.7708  lr:0.010000
[ Fri Jul 12 03:49:01 2024 ] 	Batch(5700/6809) done. Loss: 0.5473  lr:0.010000
[ Fri Jul 12 03:49:19 2024 ] 	Batch(5800/6809) done. Loss: 0.8074  lr:0.010000
[ Fri Jul 12 03:49:38 2024 ] 	Batch(5900/6809) done. Loss: 0.2060  lr:0.010000
[ Fri Jul 12 03:49:56 2024 ] 
Training: Epoch [19/120], Step [5999], Loss: 0.4039210379123688, Training Accuracy: 79.96666666666667
[ Fri Jul 12 03:49:56 2024 ] 	Batch(6000/6809) done. Loss: 1.5282  lr:0.010000
[ Fri Jul 12 03:50:14 2024 ] 	Batch(6100/6809) done. Loss: 0.5703  lr:0.010000
[ Fri Jul 12 03:50:32 2024 ] 	Batch(6200/6809) done. Loss: 0.7962  lr:0.010000
[ Fri Jul 12 03:50:50 2024 ] 	Batch(6300/6809) done. Loss: 1.4899  lr:0.010000
[ Fri Jul 12 03:51:08 2024 ] 	Batch(6400/6809) done. Loss: 0.0904  lr:0.010000
[ Fri Jul 12 03:51:26 2024 ] 
Training: Epoch [19/120], Step [6499], Loss: 0.9166197180747986, Training Accuracy: 79.99615384615385
[ Fri Jul 12 03:51:26 2024 ] 	Batch(6500/6809) done. Loss: 0.5698  lr:0.010000
[ Fri Jul 12 03:51:44 2024 ] 	Batch(6600/6809) done. Loss: 0.2324  lr:0.010000
[ Fri Jul 12 03:52:02 2024 ] 	Batch(6700/6809) done. Loss: 0.4596  lr:0.010000
[ Fri Jul 12 03:52:20 2024 ] 	Batch(6800/6809) done. Loss: 1.2793  lr:0.010000
[ Fri Jul 12 03:52:21 2024 ] 	Mean training loss: 0.6574.
[ Fri Jul 12 03:52:21 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 03:52:22 2024 ] Eval epoch: 20
[ Fri Jul 12 03:57:56 2024 ] 	Mean val loss of 7435 batches: 1.090553649546388.
[ Fri Jul 12 03:57:56 2024 ] 
Validation: Epoch [19/120], Samples [44454.0/59477], Loss: 0.529739499092102, Validation Accuracy: 74.74149671301511
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 1 : 339 / 500 = 67 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 2 : 367 / 499 = 73 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 3 : 363 / 500 = 72 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 4 : 411 / 502 = 81 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 5 : 469 / 502 = 93 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 6 : 426 / 502 = 84 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 7 : 439 / 497 = 88 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 8 : 480 / 498 = 96 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 9 : 303 / 500 = 60 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 10 : 101 / 500 = 20 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 11 : 189 / 498 = 37 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 12 : 407 / 499 = 81 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 13 : 474 / 502 = 94 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 14 : 463 / 504 = 91 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 15 : 427 / 502 = 85 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 16 : 348 / 502 = 69 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 17 : 406 / 504 = 80 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 18 : 391 / 504 = 77 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 19 : 454 / 502 = 90 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 20 : 459 / 502 = 91 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 21 : 452 / 503 = 89 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 22 : 395 / 504 = 78 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 23 : 411 / 503 = 81 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 24 : 425 / 504 = 84 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 25 : 472 / 504 = 93 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 26 : 462 / 504 = 91 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 27 : 405 / 501 = 80 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 28 : 342 / 502 = 68 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 29 : 179 / 502 = 35 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 30 : 375 / 501 = 74 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 31 : 378 / 504 = 75 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 32 : 403 / 503 = 80 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 33 : 409 / 503 = 81 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 34 : 476 / 504 = 94 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 35 : 444 / 503 = 88 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 36 : 363 / 502 = 72 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 37 : 395 / 504 = 78 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 38 : 428 / 504 = 84 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 39 : 445 / 498 = 89 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 40 : 349 / 504 = 69 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 41 : 481 / 503 = 95 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 42 : 450 / 504 = 89 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 43 : 262 / 503 = 52 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 44 : 426 / 504 = 84 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 45 : 405 / 504 = 80 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 46 : 404 / 504 = 80 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 47 : 384 / 503 = 76 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 48 : 411 / 503 = 81 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 49 : 296 / 499 = 59 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 50 : 285 / 502 = 56 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 51 : 462 / 503 = 91 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 52 : 356 / 504 = 70 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 53 : 383 / 497 = 77 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 54 : 421 / 480 = 87 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 55 : 290 / 504 = 57 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 56 : 422 / 503 = 83 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 57 : 447 / 504 = 88 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 58 : 480 / 499 = 96 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 59 : 463 / 503 = 92 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 60 : 367 / 479 = 76 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 61 : 357 / 484 = 73 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 62 : 390 / 487 = 80 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 63 : 450 / 489 = 92 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 64 : 387 / 488 = 79 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 65 : 448 / 490 = 91 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 66 : 295 / 488 = 60 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 67 : 348 / 490 = 71 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 68 : 253 / 490 = 51 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 69 : 371 / 490 = 75 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 70 : 208 / 490 = 42 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 71 : 150 / 490 = 30 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 72 : 199 / 488 = 40 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 73 : 288 / 486 = 59 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 74 : 249 / 481 = 51 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 75 : 65 / 488 = 13 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 76 : 266 / 489 = 54 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 77 : 211 / 488 = 43 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 78 : 331 / 488 = 67 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 79 : 453 / 490 = 92 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 80 : 388 / 489 = 79 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 81 : 273 / 491 = 55 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 82 : 294 / 491 = 59 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 83 : 130 / 489 = 26 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 84 : 366 / 489 = 74 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 85 : 338 / 489 = 69 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 86 : 434 / 491 = 88 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 87 : 443 / 492 = 90 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 88 : 346 / 491 = 70 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 89 : 365 / 492 = 74 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 90 : 174 / 490 = 35 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 91 : 380 / 482 = 78 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 92 : 373 / 490 = 76 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 93 : 316 / 487 = 64 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 94 : 418 / 489 = 85 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 95 : 411 / 490 = 83 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 96 : 460 / 491 = 93 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 97 : 460 / 490 = 93 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 98 : 440 / 491 = 89 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 99 : 442 / 491 = 90 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 100 : 467 / 491 = 95 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 101 : 400 / 491 = 81 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 102 : 305 / 492 = 61 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 103 : 355 / 492 = 72 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 104 : 219 / 491 = 44 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 105 : 347 / 491 = 70 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 106 : 132 / 492 = 26 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 107 : 357 / 491 = 72 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 108 : 388 / 492 = 78 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 109 : 316 / 490 = 64 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 110 : 446 / 491 = 90 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 111 : 446 / 492 = 90 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 112 : 451 / 492 = 91 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 113 : 440 / 491 = 89 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 114 : 383 / 491 = 78 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 115 : 446 / 492 = 90 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 116 : 320 / 491 = 65 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 117 : 443 / 492 = 90 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 118 : 354 / 490 = 72 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 119 : 447 / 492 = 90 %
[ Fri Jul 12 03:57:56 2024 ] Accuracy of 120 : 403 / 500 = 80 %
[ Fri Jul 12 03:57:56 2024 ] Training epoch: 21
[ Fri Jul 12 03:57:57 2024 ] 	Batch(0/6809) done. Loss: 1.0215  lr:0.010000
[ Fri Jul 12 03:58:15 2024 ] 	Batch(100/6809) done. Loss: 0.7576  lr:0.010000
[ Fri Jul 12 03:58:33 2024 ] 	Batch(200/6809) done. Loss: 0.7507  lr:0.010000
[ Fri Jul 12 03:58:51 2024 ] 	Batch(300/6809) done. Loss: 0.4405  lr:0.010000
[ Fri Jul 12 03:59:09 2024 ] 	Batch(400/6809) done. Loss: 0.9587  lr:0.010000
[ Fri Jul 12 03:59:28 2024 ] 
Training: Epoch [20/120], Step [499], Loss: 0.2780894637107849, Training Accuracy: 81.05
[ Fri Jul 12 03:59:28 2024 ] 	Batch(500/6809) done. Loss: 0.8360  lr:0.010000
[ Fri Jul 12 03:59:46 2024 ] 	Batch(600/6809) done. Loss: 0.8453  lr:0.010000
[ Fri Jul 12 04:00:05 2024 ] 	Batch(700/6809) done. Loss: 0.5596  lr:0.010000
[ Fri Jul 12 04:00:23 2024 ] 	Batch(800/6809) done. Loss: 0.9417  lr:0.010000
[ Fri Jul 12 04:00:42 2024 ] 	Batch(900/6809) done. Loss: 0.4480  lr:0.010000
[ Fri Jul 12 04:00:59 2024 ] 
Training: Epoch [20/120], Step [999], Loss: 0.6240124106407166, Training Accuracy: 81.27499999999999
[ Fri Jul 12 04:00:59 2024 ] 	Batch(1000/6809) done. Loss: 0.8688  lr:0.010000
[ Fri Jul 12 04:01:17 2024 ] 	Batch(1100/6809) done. Loss: 0.3494  lr:0.010000
[ Fri Jul 12 04:01:35 2024 ] 	Batch(1200/6809) done. Loss: 1.3148  lr:0.010000
[ Fri Jul 12 04:01:53 2024 ] 	Batch(1300/6809) done. Loss: 0.3301  lr:0.010000
[ Fri Jul 12 04:02:11 2024 ] 	Batch(1400/6809) done. Loss: 0.5521  lr:0.010000
[ Fri Jul 12 04:02:29 2024 ] 
Training: Epoch [20/120], Step [1499], Loss: 0.6857606172561646, Training Accuracy: 81.13333333333334
[ Fri Jul 12 04:02:29 2024 ] 	Batch(1500/6809) done. Loss: 0.5717  lr:0.010000
[ Fri Jul 12 04:02:47 2024 ] 	Batch(1600/6809) done. Loss: 0.6812  lr:0.010000
[ Fri Jul 12 04:03:05 2024 ] 	Batch(1700/6809) done. Loss: 0.4720  lr:0.010000
[ Fri Jul 12 04:03:23 2024 ] 	Batch(1800/6809) done. Loss: 0.2777  lr:0.010000
[ Fri Jul 12 04:03:40 2024 ] 	Batch(1900/6809) done. Loss: 0.9500  lr:0.010000
[ Fri Jul 12 04:03:58 2024 ] 
Training: Epoch [20/120], Step [1999], Loss: 0.131434828042984, Training Accuracy: 81.01875
[ Fri Jul 12 04:03:58 2024 ] 	Batch(2000/6809) done. Loss: 0.4317  lr:0.010000
[ Fri Jul 12 04:04:16 2024 ] 	Batch(2100/6809) done. Loss: 0.3233  lr:0.010000
[ Fri Jul 12 04:04:34 2024 ] 	Batch(2200/6809) done. Loss: 0.3119  lr:0.010000
[ Fri Jul 12 04:04:52 2024 ] 	Batch(2300/6809) done. Loss: 0.1079  lr:0.010000
[ Fri Jul 12 04:05:10 2024 ] 	Batch(2400/6809) done. Loss: 1.1989  lr:0.010000
[ Fri Jul 12 04:05:28 2024 ] 
Training: Epoch [20/120], Step [2499], Loss: 0.20779527723789215, Training Accuracy: 80.905
[ Fri Jul 12 04:05:28 2024 ] 	Batch(2500/6809) done. Loss: 0.8336  lr:0.010000
[ Fri Jul 12 04:05:46 2024 ] 	Batch(2600/6809) done. Loss: 0.9487  lr:0.010000
[ Fri Jul 12 04:06:04 2024 ] 	Batch(2700/6809) done. Loss: 0.9758  lr:0.010000
[ Fri Jul 12 04:06:22 2024 ] 	Batch(2800/6809) done. Loss: 0.2022  lr:0.010000
[ Fri Jul 12 04:06:40 2024 ] 	Batch(2900/6809) done. Loss: 0.4083  lr:0.010000
[ Fri Jul 12 04:06:57 2024 ] 
Training: Epoch [20/120], Step [2999], Loss: 0.7119442820549011, Training Accuracy: 80.77083333333334
[ Fri Jul 12 04:06:58 2024 ] 	Batch(3000/6809) done. Loss: 0.7188  lr:0.010000
[ Fri Jul 12 04:07:15 2024 ] 	Batch(3100/6809) done. Loss: 0.7116  lr:0.010000
[ Fri Jul 12 04:07:33 2024 ] 	Batch(3200/6809) done. Loss: 0.7572  lr:0.010000
[ Fri Jul 12 04:07:52 2024 ] 	Batch(3300/6809) done. Loss: 1.5098  lr:0.010000
[ Fri Jul 12 04:08:10 2024 ] 	Batch(3400/6809) done. Loss: 0.5957  lr:0.010000
[ Fri Jul 12 04:08:29 2024 ] 
Training: Epoch [20/120], Step [3499], Loss: 0.7536566853523254, Training Accuracy: 80.66071428571429
[ Fri Jul 12 04:08:29 2024 ] 	Batch(3500/6809) done. Loss: 0.0905  lr:0.010000
[ Fri Jul 12 04:08:48 2024 ] 	Batch(3600/6809) done. Loss: 0.4995  lr:0.010000
[ Fri Jul 12 04:09:06 2024 ] 	Batch(3700/6809) done. Loss: 0.2260  lr:0.010000
[ Fri Jul 12 04:09:23 2024 ] 	Batch(3800/6809) done. Loss: 0.2879  lr:0.010000
[ Fri Jul 12 04:09:41 2024 ] 	Batch(3900/6809) done. Loss: 0.7126  lr:0.010000
[ Fri Jul 12 04:09:59 2024 ] 
Training: Epoch [20/120], Step [3999], Loss: 0.28629323840141296, Training Accuracy: 80.571875
[ Fri Jul 12 04:09:59 2024 ] 	Batch(4000/6809) done. Loss: 0.3407  lr:0.010000
[ Fri Jul 12 04:10:17 2024 ] 	Batch(4100/6809) done. Loss: 1.1205  lr:0.010000
[ Fri Jul 12 04:10:35 2024 ] 	Batch(4200/6809) done. Loss: 0.5745  lr:0.010000
[ Fri Jul 12 04:10:53 2024 ] 	Batch(4300/6809) done. Loss: 0.2939  lr:0.010000
[ Fri Jul 12 04:11:11 2024 ] 	Batch(4400/6809) done. Loss: 0.6977  lr:0.010000
[ Fri Jul 12 04:11:29 2024 ] 
Training: Epoch [20/120], Step [4499], Loss: 0.6554387211799622, Training Accuracy: 80.37777777777778
[ Fri Jul 12 04:11:29 2024 ] 	Batch(4500/6809) done. Loss: 0.3968  lr:0.010000
[ Fri Jul 12 04:11:48 2024 ] 	Batch(4600/6809) done. Loss: 1.1573  lr:0.010000
[ Fri Jul 12 04:12:06 2024 ] 	Batch(4700/6809) done. Loss: 0.3041  lr:0.010000
[ Fri Jul 12 04:12:24 2024 ] 	Batch(4800/6809) done. Loss: 1.0761  lr:0.010000
[ Fri Jul 12 04:12:42 2024 ] 	Batch(4900/6809) done. Loss: 0.2479  lr:0.010000
[ Fri Jul 12 04:12:59 2024 ] 
Training: Epoch [20/120], Step [4999], Loss: 0.5954656004905701, Training Accuracy: 80.405
[ Fri Jul 12 04:13:00 2024 ] 	Batch(5000/6809) done. Loss: 0.9807  lr:0.010000
[ Fri Jul 12 04:13:17 2024 ] 	Batch(5100/6809) done. Loss: 0.8512  lr:0.010000
[ Fri Jul 12 04:13:35 2024 ] 	Batch(5200/6809) done. Loss: 1.4552  lr:0.010000
[ Fri Jul 12 04:13:54 2024 ] 	Batch(5300/6809) done. Loss: 0.8173  lr:0.010000
[ Fri Jul 12 04:14:13 2024 ] 	Batch(5400/6809) done. Loss: 0.6930  lr:0.010000
[ Fri Jul 12 04:14:31 2024 ] 
Training: Epoch [20/120], Step [5499], Loss: 0.4120767414569855, Training Accuracy: 80.41363636363637
[ Fri Jul 12 04:14:31 2024 ] 	Batch(5500/6809) done. Loss: 0.2555  lr:0.010000
[ Fri Jul 12 04:14:50 2024 ] 	Batch(5600/6809) done. Loss: 0.9234  lr:0.010000
[ Fri Jul 12 04:15:08 2024 ] 	Batch(5700/6809) done. Loss: 0.6169  lr:0.010000
[ Fri Jul 12 04:15:27 2024 ] 	Batch(5800/6809) done. Loss: 0.8200  lr:0.010000
[ Fri Jul 12 04:15:45 2024 ] 	Batch(5900/6809) done. Loss: 1.8018  lr:0.010000
[ Fri Jul 12 04:16:03 2024 ] 
Training: Epoch [20/120], Step [5999], Loss: 0.6039817929267883, Training Accuracy: 80.45208333333333
[ Fri Jul 12 04:16:03 2024 ] 	Batch(6000/6809) done. Loss: 0.1267  lr:0.010000
[ Fri Jul 12 04:16:21 2024 ] 	Batch(6100/6809) done. Loss: 0.7174  lr:0.010000
[ Fri Jul 12 04:16:39 2024 ] 	Batch(6200/6809) done. Loss: 0.8248  lr:0.010000
[ Fri Jul 12 04:16:57 2024 ] 	Batch(6300/6809) done. Loss: 0.8761  lr:0.010000
[ Fri Jul 12 04:17:15 2024 ] 	Batch(6400/6809) done. Loss: 0.4991  lr:0.010000
[ Fri Jul 12 04:17:33 2024 ] 
Training: Epoch [20/120], Step [6499], Loss: 0.8877601027488708, Training Accuracy: 80.32307692307693
[ Fri Jul 12 04:17:33 2024 ] 	Batch(6500/6809) done. Loss: 1.1016  lr:0.010000
[ Fri Jul 12 04:17:52 2024 ] 	Batch(6600/6809) done. Loss: 2.3499  lr:0.010000
[ Fri Jul 12 04:18:10 2024 ] 	Batch(6700/6809) done. Loss: 0.7550  lr:0.010000
[ Fri Jul 12 04:18:29 2024 ] 	Batch(6800/6809) done. Loss: 0.4083  lr:0.010000
[ Fri Jul 12 04:18:31 2024 ] 	Mean training loss: 0.6423.
[ Fri Jul 12 04:18:31 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 04:18:31 2024 ] Training epoch: 22
[ Fri Jul 12 04:18:32 2024 ] 	Batch(0/6809) done. Loss: 0.3009  lr:0.010000
[ Fri Jul 12 04:18:50 2024 ] 	Batch(100/6809) done. Loss: 0.0946  lr:0.010000
[ Fri Jul 12 04:19:08 2024 ] 	Batch(200/6809) done. Loss: 0.4328  lr:0.010000
[ Fri Jul 12 04:19:25 2024 ] 	Batch(300/6809) done. Loss: 0.2689  lr:0.010000
[ Fri Jul 12 04:19:44 2024 ] 	Batch(400/6809) done. Loss: 0.8278  lr:0.010000
[ Fri Jul 12 04:20:02 2024 ] 
Training: Epoch [21/120], Step [499], Loss: 1.0113776922225952, Training Accuracy: 80.95
[ Fri Jul 12 04:20:02 2024 ] 	Batch(500/6809) done. Loss: 0.4125  lr:0.010000
[ Fri Jul 12 04:20:20 2024 ] 	Batch(600/6809) done. Loss: 0.6281  lr:0.010000
[ Fri Jul 12 04:20:38 2024 ] 	Batch(700/6809) done. Loss: 0.4176  lr:0.010000
[ Fri Jul 12 04:20:57 2024 ] 	Batch(800/6809) done. Loss: 0.8114  lr:0.010000
[ Fri Jul 12 04:21:15 2024 ] 	Batch(900/6809) done. Loss: 1.0607  lr:0.010000
[ Fri Jul 12 04:21:33 2024 ] 
Training: Epoch [21/120], Step [999], Loss: 0.27761563658714294, Training Accuracy: 81.6
[ Fri Jul 12 04:21:34 2024 ] 	Batch(1000/6809) done. Loss: 0.2254  lr:0.010000
[ Fri Jul 12 04:21:52 2024 ] 	Batch(1100/6809) done. Loss: 0.3009  lr:0.010000
[ Fri Jul 12 04:22:10 2024 ] 	Batch(1200/6809) done. Loss: 0.2498  lr:0.010000
[ Fri Jul 12 04:22:29 2024 ] 	Batch(1300/6809) done. Loss: 0.3113  lr:0.010000
[ Fri Jul 12 04:22:47 2024 ] 	Batch(1400/6809) done. Loss: 0.5136  lr:0.010000
[ Fri Jul 12 04:23:05 2024 ] 
Training: Epoch [21/120], Step [1499], Loss: 1.590580940246582, Training Accuracy: 81.06666666666666
[ Fri Jul 12 04:23:05 2024 ] 	Batch(1500/6809) done. Loss: 0.4020  lr:0.010000
[ Fri Jul 12 04:23:23 2024 ] 	Batch(1600/6809) done. Loss: 0.4723  lr:0.010000
[ Fri Jul 12 04:23:41 2024 ] 	Batch(1700/6809) done. Loss: 1.0963  lr:0.010000
[ Fri Jul 12 04:23:59 2024 ] 	Batch(1800/6809) done. Loss: 0.3366  lr:0.010000
[ Fri Jul 12 04:24:17 2024 ] 	Batch(1900/6809) done. Loss: 0.3883  lr:0.010000
[ Fri Jul 12 04:24:34 2024 ] 
Training: Epoch [21/120], Step [1999], Loss: 0.7841868996620178, Training Accuracy: 81.1875
[ Fri Jul 12 04:24:35 2024 ] 	Batch(2000/6809) done. Loss: 0.8696  lr:0.010000
[ Fri Jul 12 04:24:53 2024 ] 	Batch(2100/6809) done. Loss: 0.8697  lr:0.010000
[ Fri Jul 12 04:25:11 2024 ] 	Batch(2200/6809) done. Loss: 0.0778  lr:0.010000
[ Fri Jul 12 04:25:29 2024 ] 	Batch(2300/6809) done. Loss: 1.1121  lr:0.010000
[ Fri Jul 12 04:25:47 2024 ] 	Batch(2400/6809) done. Loss: 1.8778  lr:0.010000
[ Fri Jul 12 04:26:05 2024 ] 
Training: Epoch [21/120], Step [2499], Loss: 0.48874542117118835, Training Accuracy: 81.07
[ Fri Jul 12 04:26:05 2024 ] 	Batch(2500/6809) done. Loss: 0.7802  lr:0.010000
[ Fri Jul 12 04:26:23 2024 ] 	Batch(2600/6809) done. Loss: 0.2901  lr:0.010000
[ Fri Jul 12 04:26:41 2024 ] 	Batch(2700/6809) done. Loss: 0.9294  lr:0.010000
[ Fri Jul 12 04:26:59 2024 ] 	Batch(2800/6809) done. Loss: 0.7030  lr:0.010000
[ Fri Jul 12 04:27:17 2024 ] 	Batch(2900/6809) done. Loss: 0.5544  lr:0.010000
[ Fri Jul 12 04:27:35 2024 ] 
Training: Epoch [21/120], Step [2999], Loss: 0.39625707268714905, Training Accuracy: 80.92916666666666
[ Fri Jul 12 04:27:35 2024 ] 	Batch(3000/6809) done. Loss: 0.9487  lr:0.010000
[ Fri Jul 12 04:27:53 2024 ] 	Batch(3100/6809) done. Loss: 0.9700  lr:0.010000
[ Fri Jul 12 04:28:11 2024 ] 	Batch(3200/6809) done. Loss: 0.1297  lr:0.010000
[ Fri Jul 12 04:28:29 2024 ] 	Batch(3300/6809) done. Loss: 0.6121  lr:0.010000
[ Fri Jul 12 04:28:47 2024 ] 	Batch(3400/6809) done. Loss: 0.8130  lr:0.010000
[ Fri Jul 12 04:29:04 2024 ] 
Training: Epoch [21/120], Step [3499], Loss: 0.5946230888366699, Training Accuracy: 80.875
[ Fri Jul 12 04:29:05 2024 ] 	Batch(3500/6809) done. Loss: 0.2383  lr:0.010000
[ Fri Jul 12 04:29:23 2024 ] 	Batch(3600/6809) done. Loss: 0.1880  lr:0.010000
[ Fri Jul 12 04:29:41 2024 ] 	Batch(3700/6809) done. Loss: 0.3626  lr:0.010000
[ Fri Jul 12 04:30:00 2024 ] 	Batch(3800/6809) done. Loss: 0.6477  lr:0.010000
[ Fri Jul 12 04:30:18 2024 ] 	Batch(3900/6809) done. Loss: 0.4269  lr:0.010000
[ Fri Jul 12 04:30:37 2024 ] 
Training: Epoch [21/120], Step [3999], Loss: 0.6785053610801697, Training Accuracy: 80.715625
[ Fri Jul 12 04:30:37 2024 ] 	Batch(4000/6809) done. Loss: 1.4913  lr:0.010000
[ Fri Jul 12 04:30:55 2024 ] 	Batch(4100/6809) done. Loss: 0.3914  lr:0.010000
[ Fri Jul 12 04:31:14 2024 ] 	Batch(4200/6809) done. Loss: 0.2823  lr:0.010000
[ Fri Jul 12 04:31:32 2024 ] 	Batch(4300/6809) done. Loss: 0.0280  lr:0.010000
[ Fri Jul 12 04:31:51 2024 ] 	Batch(4400/6809) done. Loss: 0.5968  lr:0.010000
[ Fri Jul 12 04:32:09 2024 ] 
Training: Epoch [21/120], Step [4499], Loss: 0.2917634844779968, Training Accuracy: 80.68888888888888
[ Fri Jul 12 04:32:09 2024 ] 	Batch(4500/6809) done. Loss: 0.2538  lr:0.010000
[ Fri Jul 12 04:32:28 2024 ] 	Batch(4600/6809) done. Loss: 0.7264  lr:0.010000
[ Fri Jul 12 04:32:46 2024 ] 	Batch(4700/6809) done. Loss: 0.5496  lr:0.010000
[ Fri Jul 12 04:33:05 2024 ] 	Batch(4800/6809) done. Loss: 0.4814  lr:0.010000
[ Fri Jul 12 04:33:23 2024 ] 	Batch(4900/6809) done. Loss: 0.7329  lr:0.010000
[ Fri Jul 12 04:33:42 2024 ] 
Training: Epoch [21/120], Step [4999], Loss: 0.19835112988948822, Training Accuracy: 80.625
[ Fri Jul 12 04:33:42 2024 ] 	Batch(5000/6809) done. Loss: 0.1271  lr:0.010000
[ Fri Jul 12 04:34:00 2024 ] 	Batch(5100/6809) done. Loss: 0.9412  lr:0.010000
[ Fri Jul 12 04:34:19 2024 ] 	Batch(5200/6809) done. Loss: 0.3067  lr:0.010000
[ Fri Jul 12 04:34:36 2024 ] 	Batch(5300/6809) done. Loss: 0.8844  lr:0.010000
[ Fri Jul 12 04:34:54 2024 ] 	Batch(5400/6809) done. Loss: 0.8410  lr:0.010000
[ Fri Jul 12 04:35:12 2024 ] 
Training: Epoch [21/120], Step [5499], Loss: 0.26822930574417114, Training Accuracy: 80.60000000000001
[ Fri Jul 12 04:35:12 2024 ] 	Batch(5500/6809) done. Loss: 0.6159  lr:0.010000
[ Fri Jul 12 04:35:30 2024 ] 	Batch(5600/6809) done. Loss: 0.9071  lr:0.010000
[ Fri Jul 12 04:35:48 2024 ] 	Batch(5700/6809) done. Loss: 0.3599  lr:0.010000
[ Fri Jul 12 04:36:06 2024 ] 	Batch(5800/6809) done. Loss: 0.6960  lr:0.010000
[ Fri Jul 12 04:36:24 2024 ] 	Batch(5900/6809) done. Loss: 0.4370  lr:0.010000
[ Fri Jul 12 04:36:42 2024 ] 
Training: Epoch [21/120], Step [5999], Loss: 0.5526118278503418, Training Accuracy: 80.66458333333333
[ Fri Jul 12 04:36:42 2024 ] 	Batch(6000/6809) done. Loss: 0.6165  lr:0.010000
[ Fri Jul 12 04:37:00 2024 ] 	Batch(6100/6809) done. Loss: 0.6173  lr:0.010000
[ Fri Jul 12 04:37:18 2024 ] 	Batch(6200/6809) done. Loss: 0.4416  lr:0.010000
[ Fri Jul 12 04:37:36 2024 ] 	Batch(6300/6809) done. Loss: 1.2938  lr:0.010000
[ Fri Jul 12 04:37:54 2024 ] 	Batch(6400/6809) done. Loss: 1.0128  lr:0.010000
[ Fri Jul 12 04:38:11 2024 ] 
Training: Epoch [21/120], Step [6499], Loss: 0.5331395268440247, Training Accuracy: 80.61346153846154
[ Fri Jul 12 04:38:11 2024 ] 	Batch(6500/6809) done. Loss: 0.2333  lr:0.010000
[ Fri Jul 12 04:38:29 2024 ] 	Batch(6600/6809) done. Loss: 0.6660  lr:0.010000
[ Fri Jul 12 04:38:47 2024 ] 	Batch(6700/6809) done. Loss: 0.8862  lr:0.010000
[ Fri Jul 12 04:39:05 2024 ] 	Batch(6800/6809) done. Loss: 0.5329  lr:0.010000
[ Fri Jul 12 04:39:07 2024 ] 	Mean training loss: 0.6263.
[ Fri Jul 12 04:39:07 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 04:39:07 2024 ] Training epoch: 23
[ Fri Jul 12 04:39:07 2024 ] 	Batch(0/6809) done. Loss: 0.2799  lr:0.010000
[ Fri Jul 12 04:39:25 2024 ] 	Batch(100/6809) done. Loss: 0.5302  lr:0.010000
[ Fri Jul 12 04:39:43 2024 ] 	Batch(200/6809) done. Loss: 0.4983  lr:0.010000
[ Fri Jul 12 04:40:01 2024 ] 	Batch(300/6809) done. Loss: 0.6415  lr:0.010000
[ Fri Jul 12 04:40:19 2024 ] 	Batch(400/6809) done. Loss: 0.4123  lr:0.010000
[ Fri Jul 12 04:40:37 2024 ] 
Training: Epoch [22/120], Step [499], Loss: 0.07216697931289673, Training Accuracy: 82.3
[ Fri Jul 12 04:40:37 2024 ] 	Batch(500/6809) done. Loss: 0.4019  lr:0.010000
[ Fri Jul 12 04:40:55 2024 ] 	Batch(600/6809) done. Loss: 0.6641  lr:0.010000
[ Fri Jul 12 04:41:13 2024 ] 	Batch(700/6809) done. Loss: 0.1899  lr:0.010000
[ Fri Jul 12 04:41:31 2024 ] 	Batch(800/6809) done. Loss: 0.2068  lr:0.010000
[ Fri Jul 12 04:41:49 2024 ] 	Batch(900/6809) done. Loss: 0.0735  lr:0.010000
[ Fri Jul 12 04:42:08 2024 ] 
Training: Epoch [22/120], Step [999], Loss: 0.3963145911693573, Training Accuracy: 81.5625
[ Fri Jul 12 04:42:08 2024 ] 	Batch(1000/6809) done. Loss: 0.0723  lr:0.010000
[ Fri Jul 12 04:42:26 2024 ] 	Batch(1100/6809) done. Loss: 0.1311  lr:0.010000
[ Fri Jul 12 04:42:45 2024 ] 	Batch(1200/6809) done. Loss: 0.2452  lr:0.010000
[ Fri Jul 12 04:43:04 2024 ] 	Batch(1300/6809) done. Loss: 0.4148  lr:0.010000
[ Fri Jul 12 04:43:22 2024 ] 	Batch(1400/6809) done. Loss: 0.5800  lr:0.010000
[ Fri Jul 12 04:43:40 2024 ] 
Training: Epoch [22/120], Step [1499], Loss: 0.6131696105003357, Training Accuracy: 81.68333333333334
[ Fri Jul 12 04:43:41 2024 ] 	Batch(1500/6809) done. Loss: 0.4184  lr:0.010000
[ Fri Jul 12 04:43:59 2024 ] 	Batch(1600/6809) done. Loss: 0.1759  lr:0.010000
[ Fri Jul 12 04:44:17 2024 ] 	Batch(1700/6809) done. Loss: 0.4958  lr:0.010000
[ Fri Jul 12 04:44:35 2024 ] 	Batch(1800/6809) done. Loss: 0.5495  lr:0.010000
[ Fri Jul 12 04:44:53 2024 ] 	Batch(1900/6809) done. Loss: 0.8337  lr:0.010000
[ Fri Jul 12 04:45:11 2024 ] 
Training: Epoch [22/120], Step [1999], Loss: 0.4041239321231842, Training Accuracy: 81.6375
[ Fri Jul 12 04:45:11 2024 ] 	Batch(2000/6809) done. Loss: 1.4749  lr:0.010000
[ Fri Jul 12 04:45:29 2024 ] 	Batch(2100/6809) done. Loss: 0.1402  lr:0.010000
[ Fri Jul 12 04:45:47 2024 ] 	Batch(2200/6809) done. Loss: 0.5982  lr:0.010000
[ Fri Jul 12 04:46:05 2024 ] 	Batch(2300/6809) done. Loss: 0.9494  lr:0.010000
[ Fri Jul 12 04:46:23 2024 ] 	Batch(2400/6809) done. Loss: 0.8775  lr:0.010000
[ Fri Jul 12 04:46:41 2024 ] 
Training: Epoch [22/120], Step [2499], Loss: 0.8850716948509216, Training Accuracy: 81.66499999999999
[ Fri Jul 12 04:46:41 2024 ] 	Batch(2500/6809) done. Loss: 0.2771  lr:0.010000
[ Fri Jul 12 04:47:00 2024 ] 	Batch(2600/6809) done. Loss: 0.5242  lr:0.010000
[ Fri Jul 12 04:47:18 2024 ] 	Batch(2700/6809) done. Loss: 0.0118  lr:0.010000
[ Fri Jul 12 04:47:37 2024 ] 	Batch(2800/6809) done. Loss: 0.6419  lr:0.010000
[ Fri Jul 12 04:47:55 2024 ] 	Batch(2900/6809) done. Loss: 0.8001  lr:0.010000
[ Fri Jul 12 04:48:12 2024 ] 
Training: Epoch [22/120], Step [2999], Loss: 0.706071674823761, Training Accuracy: 81.74166666666667
[ Fri Jul 12 04:48:13 2024 ] 	Batch(3000/6809) done. Loss: 0.4411  lr:0.010000
[ Fri Jul 12 04:48:30 2024 ] 	Batch(3100/6809) done. Loss: 1.0458  lr:0.010000
[ Fri Jul 12 04:48:49 2024 ] 	Batch(3200/6809) done. Loss: 0.8824  lr:0.010000
[ Fri Jul 12 04:49:07 2024 ] 	Batch(3300/6809) done. Loss: 1.1122  lr:0.010000
[ Fri Jul 12 04:49:26 2024 ] 	Batch(3400/6809) done. Loss: 0.4902  lr:0.010000
[ Fri Jul 12 04:49:44 2024 ] 
Training: Epoch [22/120], Step [3499], Loss: 0.5280646681785583, Training Accuracy: 81.52142857142857
[ Fri Jul 12 04:49:44 2024 ] 	Batch(3500/6809) done. Loss: 0.8193  lr:0.010000
[ Fri Jul 12 04:50:03 2024 ] 	Batch(3600/6809) done. Loss: 0.3509  lr:0.010000
[ Fri Jul 12 04:50:21 2024 ] 	Batch(3700/6809) done. Loss: 0.1224  lr:0.010000
[ Fri Jul 12 04:50:38 2024 ] 	Batch(3800/6809) done. Loss: 0.7009  lr:0.010000
[ Fri Jul 12 04:50:56 2024 ] 	Batch(3900/6809) done. Loss: 0.5115  lr:0.010000
[ Fri Jul 12 04:51:14 2024 ] 
Training: Epoch [22/120], Step [3999], Loss: 0.8509230017662048, Training Accuracy: 81.440625
[ Fri Jul 12 04:51:14 2024 ] 	Batch(4000/6809) done. Loss: 0.6739  lr:0.010000
[ Fri Jul 12 04:51:32 2024 ] 	Batch(4100/6809) done. Loss: 0.3824  lr:0.010000
[ Fri Jul 12 04:51:50 2024 ] 	Batch(4200/6809) done. Loss: 0.5666  lr:0.010000
[ Fri Jul 12 04:52:08 2024 ] 	Batch(4300/6809) done. Loss: 0.6856  lr:0.010000
[ Fri Jul 12 04:52:26 2024 ] 	Batch(4400/6809) done. Loss: 0.7864  lr:0.010000
[ Fri Jul 12 04:52:44 2024 ] 
Training: Epoch [22/120], Step [4499], Loss: 0.02544947899878025, Training Accuracy: 81.49166666666666
[ Fri Jul 12 04:52:44 2024 ] 	Batch(4500/6809) done. Loss: 0.6676  lr:0.010000
[ Fri Jul 12 04:53:02 2024 ] 	Batch(4600/6809) done. Loss: 0.6979  lr:0.010000
[ Fri Jul 12 04:53:19 2024 ] 	Batch(4700/6809) done. Loss: 0.6178  lr:0.010000
[ Fri Jul 12 04:53:37 2024 ] 	Batch(4800/6809) done. Loss: 1.0700  lr:0.010000
[ Fri Jul 12 04:53:56 2024 ] 	Batch(4900/6809) done. Loss: 0.9238  lr:0.010000
[ Fri Jul 12 04:54:14 2024 ] 
Training: Epoch [22/120], Step [4999], Loss: 0.4677058756351471, Training Accuracy: 81.515
[ Fri Jul 12 04:54:14 2024 ] 	Batch(5000/6809) done. Loss: 0.0267  lr:0.010000
[ Fri Jul 12 04:54:33 2024 ] 	Batch(5100/6809) done. Loss: 0.2690  lr:0.010000
[ Fri Jul 12 04:54:52 2024 ] 	Batch(5200/6809) done. Loss: 1.1129  lr:0.010000
[ Fri Jul 12 04:55:10 2024 ] 	Batch(5300/6809) done. Loss: 0.2888  lr:0.010000
[ Fri Jul 12 04:55:29 2024 ] 	Batch(5400/6809) done. Loss: 1.1494  lr:0.010000
[ Fri Jul 12 04:55:47 2024 ] 
Training: Epoch [22/120], Step [5499], Loss: 0.7794814705848694, Training Accuracy: 81.39318181818182
[ Fri Jul 12 04:55:47 2024 ] 	Batch(5500/6809) done. Loss: 0.7630  lr:0.010000
[ Fri Jul 12 04:56:05 2024 ] 	Batch(5600/6809) done. Loss: 0.4156  lr:0.010000
[ Fri Jul 12 04:56:24 2024 ] 	Batch(5700/6809) done. Loss: 1.8077  lr:0.010000
[ Fri Jul 12 04:56:43 2024 ] 	Batch(5800/6809) done. Loss: 1.0350  lr:0.010000
[ Fri Jul 12 04:57:01 2024 ] 	Batch(5900/6809) done. Loss: 0.2561  lr:0.010000
[ Fri Jul 12 04:57:19 2024 ] 
Training: Epoch [22/120], Step [5999], Loss: 0.7582013607025146, Training Accuracy: 81.34166666666667
[ Fri Jul 12 04:57:20 2024 ] 	Batch(6000/6809) done. Loss: 0.4467  lr:0.010000
[ Fri Jul 12 04:57:38 2024 ] 	Batch(6100/6809) done. Loss: 0.1711  lr:0.010000
[ Fri Jul 12 04:57:55 2024 ] 	Batch(6200/6809) done. Loss: 0.4725  lr:0.010000
[ Fri Jul 12 04:58:13 2024 ] 	Batch(6300/6809) done. Loss: 0.6734  lr:0.010000
[ Fri Jul 12 04:58:31 2024 ] 	Batch(6400/6809) done. Loss: 0.7908  lr:0.010000
[ Fri Jul 12 04:58:50 2024 ] 
Training: Epoch [22/120], Step [6499], Loss: 1.5687575340270996, Training Accuracy: 81.37884615384615
[ Fri Jul 12 04:58:50 2024 ] 	Batch(6500/6809) done. Loss: 0.6966  lr:0.010000
[ Fri Jul 12 04:59:08 2024 ] 	Batch(6600/6809) done. Loss: 0.7487  lr:0.010000
[ Fri Jul 12 04:59:27 2024 ] 	Batch(6700/6809) done. Loss: 0.3323  lr:0.010000
[ Fri Jul 12 04:59:46 2024 ] 	Batch(6800/6809) done. Loss: 1.3799  lr:0.010000
[ Fri Jul 12 04:59:47 2024 ] 	Mean training loss: 0.6040.
[ Fri Jul 12 04:59:47 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 04:59:47 2024 ] Training epoch: 24
[ Fri Jul 12 04:59:48 2024 ] 	Batch(0/6809) done. Loss: 0.3751  lr:0.010000
[ Fri Jul 12 05:00:06 2024 ] 	Batch(100/6809) done. Loss: 0.5640  lr:0.010000
[ Fri Jul 12 05:00:24 2024 ] 	Batch(200/6809) done. Loss: 0.1717  lr:0.010000
[ Fri Jul 12 05:00:42 2024 ] 	Batch(300/6809) done. Loss: 0.6778  lr:0.010000
[ Fri Jul 12 05:01:00 2024 ] 	Batch(400/6809) done. Loss: 0.8034  lr:0.010000
[ Fri Jul 12 05:01:17 2024 ] 
Training: Epoch [23/120], Step [499], Loss: 0.23101414740085602, Training Accuracy: 81.3
[ Fri Jul 12 05:01:18 2024 ] 	Batch(500/6809) done. Loss: 0.6023  lr:0.010000
[ Fri Jul 12 05:01:35 2024 ] 	Batch(600/6809) done. Loss: 0.0788  lr:0.010000
[ Fri Jul 12 05:01:53 2024 ] 	Batch(700/6809) done. Loss: 0.1608  lr:0.010000
[ Fri Jul 12 05:02:11 2024 ] 	Batch(800/6809) done. Loss: 0.5006  lr:0.010000
[ Fri Jul 12 05:02:29 2024 ] 	Batch(900/6809) done. Loss: 0.7650  lr:0.010000
[ Fri Jul 12 05:02:47 2024 ] 
Training: Epoch [23/120], Step [999], Loss: 0.6248166561126709, Training Accuracy: 81.5125
[ Fri Jul 12 05:02:47 2024 ] 	Batch(1000/6809) done. Loss: 0.4739  lr:0.010000
[ Fri Jul 12 05:03:05 2024 ] 	Batch(1100/6809) done. Loss: 0.9191  lr:0.010000
[ Fri Jul 12 05:03:23 2024 ] 	Batch(1200/6809) done. Loss: 0.2202  lr:0.010000
[ Fri Jul 12 05:03:42 2024 ] 	Batch(1300/6809) done. Loss: 0.7772  lr:0.010000
[ Fri Jul 12 05:04:00 2024 ] 	Batch(1400/6809) done. Loss: 0.7520  lr:0.010000
[ Fri Jul 12 05:04:18 2024 ] 
Training: Epoch [23/120], Step [1499], Loss: 1.073830008506775, Training Accuracy: 82.08333333333333
[ Fri Jul 12 05:04:19 2024 ] 	Batch(1500/6809) done. Loss: 0.5201  lr:0.010000
[ Fri Jul 12 05:04:37 2024 ] 	Batch(1600/6809) done. Loss: 0.3816  lr:0.010000
[ Fri Jul 12 05:04:56 2024 ] 	Batch(1700/6809) done. Loss: 0.5543  lr:0.010000
[ Fri Jul 12 05:05:14 2024 ] 	Batch(1800/6809) done. Loss: 0.6185  lr:0.010000
[ Fri Jul 12 05:05:33 2024 ] 	Batch(1900/6809) done. Loss: 0.4999  lr:0.010000
[ Fri Jul 12 05:05:51 2024 ] 
Training: Epoch [23/120], Step [1999], Loss: 0.3339356780052185, Training Accuracy: 82.125
[ Fri Jul 12 05:05:52 2024 ] 	Batch(2000/6809) done. Loss: 0.7859  lr:0.010000
[ Fri Jul 12 05:06:10 2024 ] 	Batch(2100/6809) done. Loss: 0.4031  lr:0.010000
[ Fri Jul 12 05:06:29 2024 ] 	Batch(2200/6809) done. Loss: 0.7394  lr:0.010000
[ Fri Jul 12 05:06:47 2024 ] 	Batch(2300/6809) done. Loss: 0.7897  lr:0.010000
[ Fri Jul 12 05:07:06 2024 ] 	Batch(2400/6809) done. Loss: 0.0624  lr:0.010000
[ Fri Jul 12 05:07:24 2024 ] 
Training: Epoch [23/120], Step [2499], Loss: 0.7129920721054077, Training Accuracy: 81.965
[ Fri Jul 12 05:07:24 2024 ] 	Batch(2500/6809) done. Loss: 0.2128  lr:0.010000
[ Fri Jul 12 05:07:43 2024 ] 	Batch(2600/6809) done. Loss: 1.6374  lr:0.010000
[ Fri Jul 12 05:08:01 2024 ] 	Batch(2700/6809) done. Loss: 1.2073  lr:0.010000
[ Fri Jul 12 05:08:20 2024 ] 	Batch(2800/6809) done. Loss: 0.7594  lr:0.010000
[ Fri Jul 12 05:08:39 2024 ] 	Batch(2900/6809) done. Loss: 0.5718  lr:0.010000
[ Fri Jul 12 05:08:57 2024 ] 
Training: Epoch [23/120], Step [2999], Loss: 0.2766778767108917, Training Accuracy: 81.89166666666667
[ Fri Jul 12 05:08:57 2024 ] 	Batch(3000/6809) done. Loss: 0.4143  lr:0.010000
[ Fri Jul 12 05:09:15 2024 ] 	Batch(3100/6809) done. Loss: 0.5098  lr:0.010000
[ Fri Jul 12 05:09:33 2024 ] 	Batch(3200/6809) done. Loss: 0.3201  lr:0.010000
[ Fri Jul 12 05:09:51 2024 ] 	Batch(3300/6809) done. Loss: 0.2812  lr:0.010000
[ Fri Jul 12 05:10:10 2024 ] 	Batch(3400/6809) done. Loss: 0.6966  lr:0.010000
[ Fri Jul 12 05:10:27 2024 ] 
Training: Epoch [23/120], Step [3499], Loss: 0.9718477725982666, Training Accuracy: 81.96785714285714
[ Fri Jul 12 05:10:27 2024 ] 	Batch(3500/6809) done. Loss: 0.1314  lr:0.010000
[ Fri Jul 12 05:10:45 2024 ] 	Batch(3600/6809) done. Loss: 0.7390  lr:0.010000
[ Fri Jul 12 05:11:03 2024 ] 	Batch(3700/6809) done. Loss: 0.5977  lr:0.010000
[ Fri Jul 12 05:11:21 2024 ] 	Batch(3800/6809) done. Loss: 0.8891  lr:0.010000
[ Fri Jul 12 05:11:39 2024 ] 	Batch(3900/6809) done. Loss: 0.1065  lr:0.010000
[ Fri Jul 12 05:11:57 2024 ] 
Training: Epoch [23/120], Step [3999], Loss: 0.6883026957511902, Training Accuracy: 81.89999999999999
[ Fri Jul 12 05:11:57 2024 ] 	Batch(4000/6809) done. Loss: 0.9363  lr:0.010000
[ Fri Jul 12 05:12:15 2024 ] 	Batch(4100/6809) done. Loss: 0.4709  lr:0.010000
[ Fri Jul 12 05:12:33 2024 ] 	Batch(4200/6809) done. Loss: 0.5690  lr:0.010000
[ Fri Jul 12 05:12:51 2024 ] 	Batch(4300/6809) done. Loss: 0.2835  lr:0.010000
[ Fri Jul 12 05:13:09 2024 ] 	Batch(4400/6809) done. Loss: 0.4801  lr:0.010000
[ Fri Jul 12 05:13:27 2024 ] 
Training: Epoch [23/120], Step [4499], Loss: 0.4453233480453491, Training Accuracy: 81.8638888888889
[ Fri Jul 12 05:13:27 2024 ] 	Batch(4500/6809) done. Loss: 1.0202  lr:0.010000
[ Fri Jul 12 05:13:45 2024 ] 	Batch(4600/6809) done. Loss: 0.4351  lr:0.010000
[ Fri Jul 12 05:14:03 2024 ] 	Batch(4700/6809) done. Loss: 0.4365  lr:0.010000
[ Fri Jul 12 05:14:21 2024 ] 	Batch(4800/6809) done. Loss: 0.8287  lr:0.010000
[ Fri Jul 12 05:14:38 2024 ] 	Batch(4900/6809) done. Loss: 0.8015  lr:0.010000
[ Fri Jul 12 05:14:56 2024 ] 
Training: Epoch [23/120], Step [4999], Loss: 0.5129903554916382, Training Accuracy: 81.7775
[ Fri Jul 12 05:14:56 2024 ] 	Batch(5000/6809) done. Loss: 1.0826  lr:0.010000
[ Fri Jul 12 05:15:14 2024 ] 	Batch(5100/6809) done. Loss: 0.2949  lr:0.010000
[ Fri Jul 12 05:15:32 2024 ] 	Batch(5200/6809) done. Loss: 0.2188  lr:0.010000
[ Fri Jul 12 05:15:50 2024 ] 	Batch(5300/6809) done. Loss: 1.9458  lr:0.010000
[ Fri Jul 12 05:16:08 2024 ] 	Batch(5400/6809) done. Loss: 1.0220  lr:0.010000
[ Fri Jul 12 05:16:26 2024 ] 
Training: Epoch [23/120], Step [5499], Loss: 0.1824611872434616, Training Accuracy: 81.70681818181819
[ Fri Jul 12 05:16:26 2024 ] 	Batch(5500/6809) done. Loss: 0.6257  lr:0.010000
[ Fri Jul 12 05:16:44 2024 ] 	Batch(5600/6809) done. Loss: 0.2670  lr:0.010000
[ Fri Jul 12 05:17:02 2024 ] 	Batch(5700/6809) done. Loss: 1.0180  lr:0.010000
[ Fri Jul 12 05:17:20 2024 ] 	Batch(5800/6809) done. Loss: 0.8400  lr:0.010000
[ Fri Jul 12 05:17:38 2024 ] 	Batch(5900/6809) done. Loss: 0.4207  lr:0.010000
[ Fri Jul 12 05:17:55 2024 ] 
Training: Epoch [23/120], Step [5999], Loss: 1.1295826435089111, Training Accuracy: 81.69583333333334
[ Fri Jul 12 05:17:56 2024 ] 	Batch(6000/6809) done. Loss: 1.2102  lr:0.010000
[ Fri Jul 12 05:18:15 2024 ] 	Batch(6100/6809) done. Loss: 0.9810  lr:0.010000
[ Fri Jul 12 05:18:33 2024 ] 	Batch(6200/6809) done. Loss: 1.7593  lr:0.010000
[ Fri Jul 12 05:18:51 2024 ] 	Batch(6300/6809) done. Loss: 0.9777  lr:0.010000
[ Fri Jul 12 05:19:09 2024 ] 	Batch(6400/6809) done. Loss: 0.6892  lr:0.010000
[ Fri Jul 12 05:19:26 2024 ] 
Training: Epoch [23/120], Step [6499], Loss: 0.5771744847297668, Training Accuracy: 81.59615384615385
[ Fri Jul 12 05:19:26 2024 ] 	Batch(6500/6809) done. Loss: 0.4413  lr:0.010000
[ Fri Jul 12 05:19:44 2024 ] 	Batch(6600/6809) done. Loss: 0.4483  lr:0.010000
[ Fri Jul 12 05:20:02 2024 ] 	Batch(6700/6809) done. Loss: 0.4851  lr:0.010000
[ Fri Jul 12 05:20:20 2024 ] 	Batch(6800/6809) done. Loss: 0.8465  lr:0.010000
[ Fri Jul 12 05:20:22 2024 ] 	Mean training loss: 0.5945.
[ Fri Jul 12 05:20:22 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 05:20:22 2024 ] Training epoch: 25
[ Fri Jul 12 05:20:23 2024 ] 	Batch(0/6809) done. Loss: 0.5240  lr:0.010000
[ Fri Jul 12 05:20:40 2024 ] 	Batch(100/6809) done. Loss: 0.1150  lr:0.010000
[ Fri Jul 12 05:20:58 2024 ] 	Batch(200/6809) done. Loss: 0.5009  lr:0.010000
[ Fri Jul 12 05:21:16 2024 ] 	Batch(300/6809) done. Loss: 0.4188  lr:0.010000
[ Fri Jul 12 05:21:34 2024 ] 	Batch(400/6809) done. Loss: 0.6205  lr:0.010000
[ Fri Jul 12 05:21:52 2024 ] 
Training: Epoch [24/120], Step [499], Loss: 0.8762427568435669, Training Accuracy: 82.25
[ Fri Jul 12 05:21:52 2024 ] 	Batch(500/6809) done. Loss: 0.4178  lr:0.010000
[ Fri Jul 12 05:22:10 2024 ] 	Batch(600/6809) done. Loss: 0.0527  lr:0.010000
[ Fri Jul 12 05:22:28 2024 ] 	Batch(700/6809) done. Loss: 0.1456  lr:0.010000
[ Fri Jul 12 05:22:46 2024 ] 	Batch(800/6809) done. Loss: 0.1737  lr:0.010000
[ Fri Jul 12 05:23:04 2024 ] 	Batch(900/6809) done. Loss: 0.4299  lr:0.010000
[ Fri Jul 12 05:23:21 2024 ] 
Training: Epoch [24/120], Step [999], Loss: 0.4146515727043152, Training Accuracy: 82.78750000000001
[ Fri Jul 12 05:23:21 2024 ] 	Batch(1000/6809) done. Loss: 0.7935  lr:0.010000
[ Fri Jul 12 05:23:39 2024 ] 	Batch(1100/6809) done. Loss: 0.4250  lr:0.010000
[ Fri Jul 12 05:23:57 2024 ] 	Batch(1200/6809) done. Loss: 0.7388  lr:0.010000
[ Fri Jul 12 05:24:15 2024 ] 	Batch(1300/6809) done. Loss: 0.4405  lr:0.010000
[ Fri Jul 12 05:24:33 2024 ] 	Batch(1400/6809) done. Loss: 1.3305  lr:0.010000
[ Fri Jul 12 05:24:51 2024 ] 
Training: Epoch [24/120], Step [1499], Loss: 0.07416518777608871, Training Accuracy: 82.525
[ Fri Jul 12 05:24:51 2024 ] 	Batch(1500/6809) done. Loss: 0.4404  lr:0.010000
[ Fri Jul 12 05:25:09 2024 ] 	Batch(1600/6809) done. Loss: 0.2569  lr:0.010000
[ Fri Jul 12 05:25:28 2024 ] 	Batch(1700/6809) done. Loss: 0.3810  lr:0.010000
[ Fri Jul 12 05:25:46 2024 ] 	Batch(1800/6809) done. Loss: 0.4798  lr:0.010000
[ Fri Jul 12 05:26:05 2024 ] 	Batch(1900/6809) done. Loss: 0.2008  lr:0.010000
[ Fri Jul 12 05:26:23 2024 ] 
Training: Epoch [24/120], Step [1999], Loss: 0.3874188959598541, Training Accuracy: 82.5
[ Fri Jul 12 05:26:24 2024 ] 	Batch(2000/6809) done. Loss: 0.4873  lr:0.010000
[ Fri Jul 12 05:26:42 2024 ] 	Batch(2100/6809) done. Loss: 0.5084  lr:0.010000
[ Fri Jul 12 05:27:01 2024 ] 	Batch(2200/6809) done. Loss: 0.3811  lr:0.010000
[ Fri Jul 12 05:27:19 2024 ] 	Batch(2300/6809) done. Loss: 0.8603  lr:0.010000
[ Fri Jul 12 05:27:38 2024 ] 	Batch(2400/6809) done. Loss: 0.8001  lr:0.010000
[ Fri Jul 12 05:27:56 2024 ] 
Training: Epoch [24/120], Step [2499], Loss: 0.06614546477794647, Training Accuracy: 82.62
[ Fri Jul 12 05:27:56 2024 ] 	Batch(2500/6809) done. Loss: 1.4992  lr:0.010000
[ Fri Jul 12 05:28:14 2024 ] 	Batch(2600/6809) done. Loss: 0.4598  lr:0.010000
[ Fri Jul 12 05:28:31 2024 ] 	Batch(2700/6809) done. Loss: 0.7931  lr:0.010000
[ Fri Jul 12 05:28:49 2024 ] 	Batch(2800/6809) done. Loss: 0.5686  lr:0.010000
[ Fri Jul 12 05:29:07 2024 ] 	Batch(2900/6809) done. Loss: 0.3786  lr:0.010000
[ Fri Jul 12 05:29:25 2024 ] 
Training: Epoch [24/120], Step [2999], Loss: 0.3890886902809143, Training Accuracy: 82.425
[ Fri Jul 12 05:29:25 2024 ] 	Batch(3000/6809) done. Loss: 0.0811  lr:0.010000
[ Fri Jul 12 05:29:43 2024 ] 	Batch(3100/6809) done. Loss: 1.2486  lr:0.010000
[ Fri Jul 12 05:30:01 2024 ] 	Batch(3200/6809) done. Loss: 0.2821  lr:0.010000
[ Fri Jul 12 05:30:19 2024 ] 	Batch(3300/6809) done. Loss: 0.4862  lr:0.010000
[ Fri Jul 12 05:30:37 2024 ] 	Batch(3400/6809) done. Loss: 0.3987  lr:0.010000
[ Fri Jul 12 05:30:55 2024 ] 
Training: Epoch [24/120], Step [3499], Loss: 0.7317503094673157, Training Accuracy: 82.40357142857142
[ Fri Jul 12 05:30:55 2024 ] 	Batch(3500/6809) done. Loss: 0.5476  lr:0.010000
[ Fri Jul 12 05:31:13 2024 ] 	Batch(3600/6809) done. Loss: 0.0943  lr:0.010000
[ Fri Jul 12 05:31:31 2024 ] 	Batch(3700/6809) done. Loss: 0.5007  lr:0.010000
[ Fri Jul 12 05:31:49 2024 ] 	Batch(3800/6809) done. Loss: 0.5582  lr:0.010000
[ Fri Jul 12 05:32:06 2024 ] 	Batch(3900/6809) done. Loss: 1.1103  lr:0.010000
[ Fri Jul 12 05:32:24 2024 ] 
Training: Epoch [24/120], Step [3999], Loss: 0.21724633872509003, Training Accuracy: 82.521875
[ Fri Jul 12 05:32:25 2024 ] 	Batch(4000/6809) done. Loss: 0.6526  lr:0.010000
[ Fri Jul 12 05:32:42 2024 ] 	Batch(4100/6809) done. Loss: 0.8417  lr:0.010000
[ Fri Jul 12 05:33:00 2024 ] 	Batch(4200/6809) done. Loss: 0.4496  lr:0.010000
[ Fri Jul 12 05:33:18 2024 ] 	Batch(4300/6809) done. Loss: 0.1550  lr:0.010000
[ Fri Jul 12 05:33:36 2024 ] 	Batch(4400/6809) done. Loss: 0.1936  lr:0.010000
[ Fri Jul 12 05:33:54 2024 ] 
Training: Epoch [24/120], Step [4499], Loss: 1.0948920249938965, Training Accuracy: 82.525
[ Fri Jul 12 05:33:54 2024 ] 	Batch(4500/6809) done. Loss: 0.2839  lr:0.010000
[ Fri Jul 12 05:34:12 2024 ] 	Batch(4600/6809) done. Loss: 0.6443  lr:0.010000
[ Fri Jul 12 05:34:30 2024 ] 	Batch(4700/6809) done. Loss: 0.4766  lr:0.010000
[ Fri Jul 12 05:34:48 2024 ] 	Batch(4800/6809) done. Loss: 0.2401  lr:0.010000
[ Fri Jul 12 05:35:06 2024 ] 	Batch(4900/6809) done. Loss: 0.6499  lr:0.010000
[ Fri Jul 12 05:35:23 2024 ] 
Training: Epoch [24/120], Step [4999], Loss: 0.5374807119369507, Training Accuracy: 82.4875
[ Fri Jul 12 05:35:24 2024 ] 	Batch(5000/6809) done. Loss: 0.3365  lr:0.010000
[ Fri Jul 12 05:35:41 2024 ] 	Batch(5100/6809) done. Loss: 0.6257  lr:0.010000
[ Fri Jul 12 05:36:00 2024 ] 	Batch(5200/6809) done. Loss: 1.1350  lr:0.010000
[ Fri Jul 12 05:36:17 2024 ] 	Batch(5300/6809) done. Loss: 1.0837  lr:0.010000
[ Fri Jul 12 05:36:35 2024 ] 	Batch(5400/6809) done. Loss: 0.4492  lr:0.010000
[ Fri Jul 12 05:36:53 2024 ] 
Training: Epoch [24/120], Step [5499], Loss: 0.5056033134460449, Training Accuracy: 82.47727272727273
[ Fri Jul 12 05:36:53 2024 ] 	Batch(5500/6809) done. Loss: 0.3633  lr:0.010000
[ Fri Jul 12 05:37:11 2024 ] 	Batch(5600/6809) done. Loss: 0.4642  lr:0.010000
[ Fri Jul 12 05:37:29 2024 ] 	Batch(5700/6809) done. Loss: 1.7853  lr:0.010000
[ Fri Jul 12 05:37:47 2024 ] 	Batch(5800/6809) done. Loss: 1.2792  lr:0.010000
[ Fri Jul 12 05:38:05 2024 ] 	Batch(5900/6809) done. Loss: 0.4967  lr:0.010000
[ Fri Jul 12 05:38:23 2024 ] 
Training: Epoch [24/120], Step [5999], Loss: 0.4715151786804199, Training Accuracy: 82.50833333333333
[ Fri Jul 12 05:38:23 2024 ] 	Batch(6000/6809) done. Loss: 1.4304  lr:0.010000
[ Fri Jul 12 05:38:41 2024 ] 	Batch(6100/6809) done. Loss: 0.8174  lr:0.010000
[ Fri Jul 12 05:38:59 2024 ] 	Batch(6200/6809) done. Loss: 0.1671  lr:0.010000
[ Fri Jul 12 05:39:17 2024 ] 	Batch(6300/6809) done. Loss: 0.6059  lr:0.010000
[ Fri Jul 12 05:39:35 2024 ] 	Batch(6400/6809) done. Loss: 0.8036  lr:0.010000
[ Fri Jul 12 05:39:53 2024 ] 
Training: Epoch [24/120], Step [6499], Loss: 0.23503372073173523, Training Accuracy: 82.43653846153846
[ Fri Jul 12 05:39:53 2024 ] 	Batch(6500/6809) done. Loss: 0.7126  lr:0.010000
[ Fri Jul 12 05:40:11 2024 ] 	Batch(6600/6809) done. Loss: 0.3454  lr:0.010000
[ Fri Jul 12 05:40:29 2024 ] 	Batch(6700/6809) done. Loss: 0.4663  lr:0.010000
[ Fri Jul 12 05:40:47 2024 ] 	Batch(6800/6809) done. Loss: 0.3187  lr:0.010000
[ Fri Jul 12 05:40:48 2024 ] 	Mean training loss: 0.5714.
[ Fri Jul 12 05:40:48 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 05:40:48 2024 ] Training epoch: 26
[ Fri Jul 12 05:40:49 2024 ] 	Batch(0/6809) done. Loss: 0.6338  lr:0.010000
[ Fri Jul 12 05:41:07 2024 ] 	Batch(100/6809) done. Loss: 0.1392  lr:0.010000
[ Fri Jul 12 05:41:26 2024 ] 	Batch(200/6809) done. Loss: 0.1639  lr:0.010000
[ Fri Jul 12 05:41:44 2024 ] 	Batch(300/6809) done. Loss: 1.1894  lr:0.010000
[ Fri Jul 12 05:42:02 2024 ] 	Batch(400/6809) done. Loss: 0.1306  lr:0.010000
[ Fri Jul 12 05:42:21 2024 ] 
Training: Epoch [25/120], Step [499], Loss: 0.7657716870307922, Training Accuracy: 83.175
[ Fri Jul 12 05:42:21 2024 ] 	Batch(500/6809) done. Loss: 0.3029  lr:0.010000
[ Fri Jul 12 05:42:39 2024 ] 	Batch(600/6809) done. Loss: 0.4583  lr:0.010000
[ Fri Jul 12 05:42:57 2024 ] 	Batch(700/6809) done. Loss: 0.2847  lr:0.010000
[ Fri Jul 12 05:43:16 2024 ] 	Batch(800/6809) done. Loss: 0.3146  lr:0.010000
[ Fri Jul 12 05:43:34 2024 ] 	Batch(900/6809) done. Loss: 0.5780  lr:0.010000
[ Fri Jul 12 05:43:52 2024 ] 
Training: Epoch [25/120], Step [999], Loss: 0.31755632162094116, Training Accuracy: 83.3875
[ Fri Jul 12 05:43:52 2024 ] 	Batch(1000/6809) done. Loss: 0.1556  lr:0.010000
[ Fri Jul 12 05:44:11 2024 ] 	Batch(1100/6809) done. Loss: 0.4638  lr:0.010000
[ Fri Jul 12 05:44:29 2024 ] 	Batch(1200/6809) done. Loss: 0.2940  lr:0.010000
[ Fri Jul 12 05:44:48 2024 ] 	Batch(1300/6809) done. Loss: 0.3992  lr:0.010000
[ Fri Jul 12 05:45:06 2024 ] 	Batch(1400/6809) done. Loss: 0.4488  lr:0.010000
[ Fri Jul 12 05:45:24 2024 ] 
Training: Epoch [25/120], Step [1499], Loss: 1.312023401260376, Training Accuracy: 83.45
[ Fri Jul 12 05:45:24 2024 ] 	Batch(1500/6809) done. Loss: 0.5246  lr:0.010000
[ Fri Jul 12 05:45:43 2024 ] 	Batch(1600/6809) done. Loss: 1.2669  lr:0.010000
[ Fri Jul 12 05:46:01 2024 ] 	Batch(1700/6809) done. Loss: 0.1902  lr:0.010000
[ Fri Jul 12 05:46:19 2024 ] 	Batch(1800/6809) done. Loss: 0.8457  lr:0.010000
[ Fri Jul 12 05:46:37 2024 ] 	Batch(1900/6809) done. Loss: 0.4629  lr:0.010000
[ Fri Jul 12 05:46:55 2024 ] 
Training: Epoch [25/120], Step [1999], Loss: 0.10257584601640701, Training Accuracy: 83.0875
[ Fri Jul 12 05:46:56 2024 ] 	Batch(2000/6809) done. Loss: 0.4353  lr:0.010000
[ Fri Jul 12 05:47:14 2024 ] 	Batch(2100/6809) done. Loss: 0.4748  lr:0.010000
[ Fri Jul 12 05:47:32 2024 ] 	Batch(2200/6809) done. Loss: 0.1168  lr:0.010000
[ Fri Jul 12 05:47:50 2024 ] 	Batch(2300/6809) done. Loss: 0.4830  lr:0.010000
[ Fri Jul 12 05:48:08 2024 ] 	Batch(2400/6809) done. Loss: 0.1831  lr:0.010000
[ Fri Jul 12 05:48:26 2024 ] 
Training: Epoch [25/120], Step [2499], Loss: 0.6201356053352356, Training Accuracy: 83.025
[ Fri Jul 12 05:48:27 2024 ] 	Batch(2500/6809) done. Loss: 0.2882  lr:0.010000
[ Fri Jul 12 05:48:45 2024 ] 	Batch(2600/6809) done. Loss: 0.3500  lr:0.010000
[ Fri Jul 12 05:49:03 2024 ] 	Batch(2700/6809) done. Loss: 1.0016  lr:0.010000
[ Fri Jul 12 05:49:21 2024 ] 	Batch(2800/6809) done. Loss: 0.6381  lr:0.010000
[ Fri Jul 12 05:49:39 2024 ] 	Batch(2900/6809) done. Loss: 0.1939  lr:0.010000
[ Fri Jul 12 05:49:57 2024 ] 
Training: Epoch [25/120], Step [2999], Loss: 0.6387967467308044, Training Accuracy: 82.87083333333334
[ Fri Jul 12 05:49:57 2024 ] 	Batch(3000/6809) done. Loss: 0.1875  lr:0.010000
[ Fri Jul 12 05:50:15 2024 ] 	Batch(3100/6809) done. Loss: 0.3019  lr:0.010000
[ Fri Jul 12 05:50:33 2024 ] 	Batch(3200/6809) done. Loss: 0.3962  lr:0.010000
[ Fri Jul 12 05:50:51 2024 ] 	Batch(3300/6809) done. Loss: 0.4850  lr:0.010000
[ Fri Jul 12 05:51:09 2024 ] 	Batch(3400/6809) done. Loss: 0.6760  lr:0.010000
[ Fri Jul 12 05:51:26 2024 ] 
Training: Epoch [25/120], Step [3499], Loss: 0.021649125963449478, Training Accuracy: 82.85
[ Fri Jul 12 05:51:26 2024 ] 	Batch(3500/6809) done. Loss: 1.2404  lr:0.010000
[ Fri Jul 12 05:51:45 2024 ] 	Batch(3600/6809) done. Loss: 0.0395  lr:0.010000
[ Fri Jul 12 05:52:03 2024 ] 	Batch(3700/6809) done. Loss: 1.0462  lr:0.010000
[ Fri Jul 12 05:52:22 2024 ] 	Batch(3800/6809) done. Loss: 0.1037  lr:0.010000
[ Fri Jul 12 05:52:40 2024 ] 	Batch(3900/6809) done. Loss: 0.9136  lr:0.010000
[ Fri Jul 12 05:52:58 2024 ] 
Training: Epoch [25/120], Step [3999], Loss: 1.462315320968628, Training Accuracy: 82.90625
[ Fri Jul 12 05:52:59 2024 ] 	Batch(4000/6809) done. Loss: 0.5819  lr:0.010000
[ Fri Jul 12 05:53:17 2024 ] 	Batch(4100/6809) done. Loss: 0.7125  lr:0.010000
[ Fri Jul 12 05:53:36 2024 ] 	Batch(4200/6809) done. Loss: 0.1574  lr:0.010000
[ Fri Jul 12 05:53:54 2024 ] 	Batch(4300/6809) done. Loss: 0.2387  lr:0.010000
[ Fri Jul 12 05:54:13 2024 ] 	Batch(4400/6809) done. Loss: 0.5638  lr:0.010000
[ Fri Jul 12 05:54:31 2024 ] 
Training: Epoch [25/120], Step [4499], Loss: 0.7304153442382812, Training Accuracy: 82.91388888888889
[ Fri Jul 12 05:54:32 2024 ] 	Batch(4500/6809) done. Loss: 0.0318  lr:0.010000
[ Fri Jul 12 05:54:50 2024 ] 	Batch(4600/6809) done. Loss: 0.0729  lr:0.010000
[ Fri Jul 12 05:55:09 2024 ] 	Batch(4700/6809) done. Loss: 0.8375  lr:0.010000
[ Fri Jul 12 05:55:27 2024 ] 	Batch(4800/6809) done. Loss: 1.5537  lr:0.010000
[ Fri Jul 12 05:55:45 2024 ] 	Batch(4900/6809) done. Loss: 0.1948  lr:0.010000
[ Fri Jul 12 05:56:03 2024 ] 
Training: Epoch [25/120], Step [4999], Loss: 0.4461073875427246, Training Accuracy: 82.7325
[ Fri Jul 12 05:56:03 2024 ] 	Batch(5000/6809) done. Loss: 0.1363  lr:0.010000
[ Fri Jul 12 05:56:21 2024 ] 	Batch(5100/6809) done. Loss: 0.3323  lr:0.010000
[ Fri Jul 12 05:56:39 2024 ] 	Batch(5200/6809) done. Loss: 0.4673  lr:0.010000
[ Fri Jul 12 05:56:57 2024 ] 	Batch(5300/6809) done. Loss: 0.1628  lr:0.010000
[ Fri Jul 12 05:57:15 2024 ] 	Batch(5400/6809) done. Loss: 0.3701  lr:0.010000
[ Fri Jul 12 05:57:32 2024 ] 
Training: Epoch [25/120], Step [5499], Loss: 1.3078347444534302, Training Accuracy: 82.63181818181819
[ Fri Jul 12 05:57:33 2024 ] 	Batch(5500/6809) done. Loss: 0.2025  lr:0.010000
[ Fri Jul 12 05:57:51 2024 ] 	Batch(5600/6809) done. Loss: 0.1608  lr:0.010000
[ Fri Jul 12 05:58:09 2024 ] 	Batch(5700/6809) done. Loss: 0.7539  lr:0.010000
[ Fri Jul 12 05:58:26 2024 ] 	Batch(5800/6809) done. Loss: 0.3587  lr:0.010000
[ Fri Jul 12 05:58:44 2024 ] 	Batch(5900/6809) done. Loss: 1.0180  lr:0.010000
[ Fri Jul 12 05:59:02 2024 ] 
Training: Epoch [25/120], Step [5999], Loss: 1.2451951503753662, Training Accuracy: 82.64791666666666
[ Fri Jul 12 05:59:02 2024 ] 	Batch(6000/6809) done. Loss: 0.2450  lr:0.010000
[ Fri Jul 12 05:59:21 2024 ] 	Batch(6100/6809) done. Loss: 0.5444  lr:0.010000
[ Fri Jul 12 05:59:39 2024 ] 	Batch(6200/6809) done. Loss: 1.3817  lr:0.010000
[ Fri Jul 12 05:59:58 2024 ] 	Batch(6300/6809) done. Loss: 1.2842  lr:0.010000
[ Fri Jul 12 06:00:17 2024 ] 	Batch(6400/6809) done. Loss: 0.4408  lr:0.010000
[ Fri Jul 12 06:00:35 2024 ] 
Training: Epoch [25/120], Step [6499], Loss: 0.5705956220626831, Training Accuracy: 82.56153846153846
[ Fri Jul 12 06:00:35 2024 ] 	Batch(6500/6809) done. Loss: 0.2879  lr:0.010000
[ Fri Jul 12 06:00:54 2024 ] 	Batch(6600/6809) done. Loss: 1.0511  lr:0.010000
[ Fri Jul 12 06:01:12 2024 ] 	Batch(6700/6809) done. Loss: 0.6251  lr:0.010000
[ Fri Jul 12 06:01:31 2024 ] 	Batch(6800/6809) done. Loss: 0.4317  lr:0.010000
[ Fri Jul 12 06:01:32 2024 ] 	Mean training loss: 0.5630.
[ Fri Jul 12 06:01:32 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 06:01:32 2024 ] Training epoch: 27
[ Fri Jul 12 06:01:33 2024 ] 	Batch(0/6809) done. Loss: 0.3422  lr:0.010000
[ Fri Jul 12 06:01:51 2024 ] 	Batch(100/6809) done. Loss: 0.3838  lr:0.010000
[ Fri Jul 12 06:02:10 2024 ] 	Batch(200/6809) done. Loss: 1.5314  lr:0.010000
[ Fri Jul 12 06:02:28 2024 ] 	Batch(300/6809) done. Loss: 0.4512  lr:0.010000
[ Fri Jul 12 06:02:46 2024 ] 	Batch(400/6809) done. Loss: 0.4892  lr:0.010000
[ Fri Jul 12 06:03:04 2024 ] 
Training: Epoch [26/120], Step [499], Loss: 0.36363163590431213, Training Accuracy: 84.82499999999999
[ Fri Jul 12 06:03:05 2024 ] 	Batch(500/6809) done. Loss: 0.6713  lr:0.010000
[ Fri Jul 12 06:03:23 2024 ] 	Batch(600/6809) done. Loss: 0.2657  lr:0.010000
[ Fri Jul 12 06:03:41 2024 ] 	Batch(700/6809) done. Loss: 0.2180  lr:0.010000
[ Fri Jul 12 06:04:00 2024 ] 	Batch(800/6809) done. Loss: 0.6042  lr:0.010000
[ Fri Jul 12 06:04:18 2024 ] 	Batch(900/6809) done. Loss: 0.1852  lr:0.010000
[ Fri Jul 12 06:04:36 2024 ] 
Training: Epoch [26/120], Step [999], Loss: 0.10142624378204346, Training Accuracy: 83.76249999999999
[ Fri Jul 12 06:04:36 2024 ] 	Batch(1000/6809) done. Loss: 0.2127  lr:0.010000
[ Fri Jul 12 06:04:55 2024 ] 	Batch(1100/6809) done. Loss: 0.3719  lr:0.010000
[ Fri Jul 12 06:05:13 2024 ] 	Batch(1200/6809) done. Loss: 0.9749  lr:0.010000
[ Fri Jul 12 06:05:31 2024 ] 	Batch(1300/6809) done. Loss: 1.0615  lr:0.010000
[ Fri Jul 12 06:05:49 2024 ] 	Batch(1400/6809) done. Loss: 0.0343  lr:0.010000
[ Fri Jul 12 06:06:08 2024 ] 
Training: Epoch [26/120], Step [1499], Loss: 0.609247624874115, Training Accuracy: 83.29166666666666
[ Fri Jul 12 06:06:08 2024 ] 	Batch(1500/6809) done. Loss: 0.3799  lr:0.010000
[ Fri Jul 12 06:06:26 2024 ] 	Batch(1600/6809) done. Loss: 0.3212  lr:0.010000
[ Fri Jul 12 06:06:45 2024 ] 	Batch(1700/6809) done. Loss: 1.1273  lr:0.010000
[ Fri Jul 12 06:07:03 2024 ] 	Batch(1800/6809) done. Loss: 0.6272  lr:0.010000
[ Fri Jul 12 06:07:22 2024 ] 	Batch(1900/6809) done. Loss: 0.6224  lr:0.010000
[ Fri Jul 12 06:07:40 2024 ] 
Training: Epoch [26/120], Step [1999], Loss: 0.6866461634635925, Training Accuracy: 83.05
[ Fri Jul 12 06:07:41 2024 ] 	Batch(2000/6809) done. Loss: 0.2503  lr:0.010000
[ Fri Jul 12 06:07:58 2024 ] 	Batch(2100/6809) done. Loss: 0.4034  lr:0.010000
[ Fri Jul 12 06:08:16 2024 ] 	Batch(2200/6809) done. Loss: 1.9419  lr:0.010000
[ Fri Jul 12 06:08:34 2024 ] 	Batch(2300/6809) done. Loss: 0.0714  lr:0.010000
[ Fri Jul 12 06:08:52 2024 ] 	Batch(2400/6809) done. Loss: 0.5726  lr:0.010000
[ Fri Jul 12 06:09:10 2024 ] 
Training: Epoch [26/120], Step [2499], Loss: 0.5909686088562012, Training Accuracy: 82.99499999999999
[ Fri Jul 12 06:09:10 2024 ] 	Batch(2500/6809) done. Loss: 0.0152  lr:0.010000
[ Fri Jul 12 06:09:28 2024 ] 	Batch(2600/6809) done. Loss: 1.5007  lr:0.010000
[ Fri Jul 12 06:09:46 2024 ] 	Batch(2700/6809) done. Loss: 0.5115  lr:0.010000
[ Fri Jul 12 06:10:04 2024 ] 	Batch(2800/6809) done. Loss: 0.5000  lr:0.010000
[ Fri Jul 12 06:10:22 2024 ] 	Batch(2900/6809) done. Loss: 0.3970  lr:0.010000
[ Fri Jul 12 06:10:40 2024 ] 
Training: Epoch [26/120], Step [2999], Loss: 0.9634323120117188, Training Accuracy: 83.04583333333333
[ Fri Jul 12 06:10:40 2024 ] 	Batch(3000/6809) done. Loss: 0.8088  lr:0.010000
[ Fri Jul 12 06:10:58 2024 ] 	Batch(3100/6809) done. Loss: 0.1166  lr:0.010000
[ Fri Jul 12 06:11:16 2024 ] 	Batch(3200/6809) done. Loss: 0.5215  lr:0.010000
[ Fri Jul 12 06:11:34 2024 ] 	Batch(3300/6809) done. Loss: 0.7522  lr:0.010000
[ Fri Jul 12 06:11:52 2024 ] 	Batch(3400/6809) done. Loss: 0.7296  lr:0.010000
[ Fri Jul 12 06:12:09 2024 ] 
Training: Epoch [26/120], Step [3499], Loss: 0.7183666229248047, Training Accuracy: 83.025
[ Fri Jul 12 06:12:10 2024 ] 	Batch(3500/6809) done. Loss: 0.4841  lr:0.010000
[ Fri Jul 12 06:12:28 2024 ] 	Batch(3600/6809) done. Loss: 0.4033  lr:0.010000
[ Fri Jul 12 06:12:46 2024 ] 	Batch(3700/6809) done. Loss: 0.0672  lr:0.010000
[ Fri Jul 12 06:13:03 2024 ] 	Batch(3800/6809) done. Loss: 0.5501  lr:0.010000
[ Fri Jul 12 06:13:21 2024 ] 	Batch(3900/6809) done. Loss: 0.8095  lr:0.010000
[ Fri Jul 12 06:13:39 2024 ] 
Training: Epoch [26/120], Step [3999], Loss: 0.08766429871320724, Training Accuracy: 83.09062499999999
[ Fri Jul 12 06:13:39 2024 ] 	Batch(4000/6809) done. Loss: 0.3904  lr:0.010000
[ Fri Jul 12 06:13:57 2024 ] 	Batch(4100/6809) done. Loss: 1.2412  lr:0.010000
[ Fri Jul 12 06:14:15 2024 ] 	Batch(4200/6809) done. Loss: 0.0229  lr:0.010000
[ Fri Jul 12 06:14:33 2024 ] 	Batch(4300/6809) done. Loss: 0.4741  lr:0.010000
[ Fri Jul 12 06:14:51 2024 ] 	Batch(4400/6809) done. Loss: 0.2021  lr:0.010000
[ Fri Jul 12 06:15:09 2024 ] 
Training: Epoch [26/120], Step [4499], Loss: 0.895877480506897, Training Accuracy: 83.02777777777777
[ Fri Jul 12 06:15:09 2024 ] 	Batch(4500/6809) done. Loss: 0.6797  lr:0.010000
[ Fri Jul 12 06:15:27 2024 ] 	Batch(4600/6809) done. Loss: 0.5751  lr:0.010000
[ Fri Jul 12 06:15:45 2024 ] 	Batch(4700/6809) done. Loss: 0.8734  lr:0.010000
[ Fri Jul 12 06:16:03 2024 ] 	Batch(4800/6809) done. Loss: 0.2296  lr:0.010000
[ Fri Jul 12 06:16:21 2024 ] 	Batch(4900/6809) done. Loss: 0.5815  lr:0.010000
[ Fri Jul 12 06:16:39 2024 ] 
Training: Epoch [26/120], Step [4999], Loss: 0.30891743302345276, Training Accuracy: 82.97
[ Fri Jul 12 06:16:39 2024 ] 	Batch(5000/6809) done. Loss: 1.2383  lr:0.010000
[ Fri Jul 12 06:16:57 2024 ] 	Batch(5100/6809) done. Loss: 1.5713  lr:0.010000
[ Fri Jul 12 06:17:15 2024 ] 	Batch(5200/6809) done. Loss: 0.5182  lr:0.010000
[ Fri Jul 12 06:17:33 2024 ] 	Batch(5300/6809) done. Loss: 0.1969  lr:0.010000
[ Fri Jul 12 06:17:52 2024 ] 	Batch(5400/6809) done. Loss: 0.2249  lr:0.010000
[ Fri Jul 12 06:18:10 2024 ] 
Training: Epoch [26/120], Step [5499], Loss: 0.47916078567504883, Training Accuracy: 82.91363636363637
[ Fri Jul 12 06:18:10 2024 ] 	Batch(5500/6809) done. Loss: 1.1256  lr:0.010000
[ Fri Jul 12 06:18:29 2024 ] 	Batch(5600/6809) done. Loss: 0.7930  lr:0.010000
[ Fri Jul 12 06:18:47 2024 ] 	Batch(5700/6809) done. Loss: 0.0609  lr:0.010000
[ Fri Jul 12 06:19:06 2024 ] 	Batch(5800/6809) done. Loss: 0.4068  lr:0.010000
[ Fri Jul 12 06:19:25 2024 ] 	Batch(5900/6809) done. Loss: 1.6360  lr:0.010000
[ Fri Jul 12 06:19:43 2024 ] 
Training: Epoch [26/120], Step [5999], Loss: 0.3794727325439453, Training Accuracy: 82.78750000000001
[ Fri Jul 12 06:19:43 2024 ] 	Batch(6000/6809) done. Loss: 0.4548  lr:0.010000
[ Fri Jul 12 06:20:01 2024 ] 	Batch(6100/6809) done. Loss: 0.4584  lr:0.010000
[ Fri Jul 12 06:20:19 2024 ] 	Batch(6200/6809) done. Loss: 0.1516  lr:0.010000
[ Fri Jul 12 06:20:37 2024 ] 	Batch(6300/6809) done. Loss: 0.5888  lr:0.010000
[ Fri Jul 12 06:20:55 2024 ] 	Batch(6400/6809) done. Loss: 0.6432  lr:0.010000
[ Fri Jul 12 06:21:12 2024 ] 
Training: Epoch [26/120], Step [6499], Loss: 0.7482262849807739, Training Accuracy: 82.80192307692307
[ Fri Jul 12 06:21:13 2024 ] 	Batch(6500/6809) done. Loss: 0.1543  lr:0.010000
[ Fri Jul 12 06:21:31 2024 ] 	Batch(6600/6809) done. Loss: 0.1693  lr:0.010000
[ Fri Jul 12 06:21:48 2024 ] 	Batch(6700/6809) done. Loss: 0.1172  lr:0.010000
[ Fri Jul 12 06:22:06 2024 ] 	Batch(6800/6809) done. Loss: 1.0779  lr:0.010000
[ Fri Jul 12 06:22:08 2024 ] 	Mean training loss: 0.5544.
[ Fri Jul 12 06:22:08 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 06:22:08 2024 ] Training epoch: 28
[ Fri Jul 12 06:22:09 2024 ] 	Batch(0/6809) done. Loss: 0.9638  lr:0.010000
[ Fri Jul 12 06:22:27 2024 ] 	Batch(100/6809) done. Loss: 0.2265  lr:0.010000
[ Fri Jul 12 06:22:45 2024 ] 	Batch(200/6809) done. Loss: 0.5125  lr:0.010000
[ Fri Jul 12 06:23:04 2024 ] 	Batch(300/6809) done. Loss: 0.6503  lr:0.010000
[ Fri Jul 12 06:23:22 2024 ] 	Batch(400/6809) done. Loss: 0.2393  lr:0.010000
[ Fri Jul 12 06:23:40 2024 ] 
Training: Epoch [27/120], Step [499], Loss: 0.4296247959136963, Training Accuracy: 84.65
[ Fri Jul 12 06:23:40 2024 ] 	Batch(500/6809) done. Loss: 0.5550  lr:0.010000
[ Fri Jul 12 06:23:59 2024 ] 	Batch(600/6809) done. Loss: 0.4640  lr:0.010000
[ Fri Jul 12 06:24:17 2024 ] 	Batch(700/6809) done. Loss: 0.5029  lr:0.010000
[ Fri Jul 12 06:24:36 2024 ] 	Batch(800/6809) done. Loss: 0.2637  lr:0.010000
[ Fri Jul 12 06:24:54 2024 ] 	Batch(900/6809) done. Loss: 0.7890  lr:0.010000
[ Fri Jul 12 06:25:12 2024 ] 
Training: Epoch [27/120], Step [999], Loss: 0.3547857403755188, Training Accuracy: 84.05
[ Fri Jul 12 06:25:12 2024 ] 	Batch(1000/6809) done. Loss: 0.2815  lr:0.010000
[ Fri Jul 12 06:25:30 2024 ] 	Batch(1100/6809) done. Loss: 0.8444  lr:0.010000
[ Fri Jul 12 06:25:49 2024 ] 	Batch(1200/6809) done. Loss: 0.7754  lr:0.010000
[ Fri Jul 12 06:26:07 2024 ] 	Batch(1300/6809) done. Loss: 0.0309  lr:0.010000
[ Fri Jul 12 06:26:26 2024 ] 	Batch(1400/6809) done. Loss: 0.8739  lr:0.010000
[ Fri Jul 12 06:26:44 2024 ] 
Training: Epoch [27/120], Step [1499], Loss: 0.5722130537033081, Training Accuracy: 83.65
[ Fri Jul 12 06:26:44 2024 ] 	Batch(1500/6809) done. Loss: 0.9685  lr:0.010000
[ Fri Jul 12 06:27:02 2024 ] 	Batch(1600/6809) done. Loss: 0.0610  lr:0.010000
[ Fri Jul 12 06:27:21 2024 ] 	Batch(1700/6809) done. Loss: 0.8534  lr:0.010000
[ Fri Jul 12 06:27:40 2024 ] 	Batch(1800/6809) done. Loss: 0.1948  lr:0.010000
[ Fri Jul 12 06:27:58 2024 ] 	Batch(1900/6809) done. Loss: 0.9702  lr:0.010000
[ Fri Jul 12 06:28:16 2024 ] 
Training: Epoch [27/120], Step [1999], Loss: 0.5370712280273438, Training Accuracy: 83.6375
[ Fri Jul 12 06:28:17 2024 ] 	Batch(2000/6809) done. Loss: 0.2349  lr:0.010000
[ Fri Jul 12 06:28:35 2024 ] 	Batch(2100/6809) done. Loss: 1.3637  lr:0.010000
[ Fri Jul 12 06:28:53 2024 ] 	Batch(2200/6809) done. Loss: 0.4616  lr:0.010000
[ Fri Jul 12 06:29:11 2024 ] 	Batch(2300/6809) done. Loss: 0.5879  lr:0.010000
[ Fri Jul 12 06:29:29 2024 ] 	Batch(2400/6809) done. Loss: 0.2895  lr:0.010000
[ Fri Jul 12 06:29:47 2024 ] 
Training: Epoch [27/120], Step [2499], Loss: 0.757289707660675, Training Accuracy: 83.695
[ Fri Jul 12 06:29:47 2024 ] 	Batch(2500/6809) done. Loss: 0.3871  lr:0.010000
[ Fri Jul 12 06:30:05 2024 ] 	Batch(2600/6809) done. Loss: 0.2261  lr:0.010000
[ Fri Jul 12 06:30:23 2024 ] 	Batch(2700/6809) done. Loss: 0.1940  lr:0.010000
[ Fri Jul 12 06:30:41 2024 ] 	Batch(2800/6809) done. Loss: 0.1331  lr:0.010000
[ Fri Jul 12 06:30:59 2024 ] 	Batch(2900/6809) done. Loss: 0.4441  lr:0.010000
[ Fri Jul 12 06:31:17 2024 ] 
Training: Epoch [27/120], Step [2999], Loss: 0.3307722806930542, Training Accuracy: 83.65833333333333
[ Fri Jul 12 06:31:17 2024 ] 	Batch(3000/6809) done. Loss: 0.8992  lr:0.010000
[ Fri Jul 12 06:31:35 2024 ] 	Batch(3100/6809) done. Loss: 0.0479  lr:0.010000
[ Fri Jul 12 06:31:53 2024 ] 	Batch(3200/6809) done. Loss: 0.4681  lr:0.010000
[ Fri Jul 12 06:32:11 2024 ] 	Batch(3300/6809) done. Loss: 0.1302  lr:0.010000
[ Fri Jul 12 06:32:28 2024 ] 	Batch(3400/6809) done. Loss: 1.0033  lr:0.010000
[ Fri Jul 12 06:32:46 2024 ] 
Training: Epoch [27/120], Step [3499], Loss: 0.15657511353492737, Training Accuracy: 83.66071428571429
[ Fri Jul 12 06:32:46 2024 ] 	Batch(3500/6809) done. Loss: 0.2867  lr:0.010000
[ Fri Jul 12 06:33:04 2024 ] 	Batch(3600/6809) done. Loss: 0.9333  lr:0.010000
[ Fri Jul 12 06:33:22 2024 ] 	Batch(3700/6809) done. Loss: 0.9788  lr:0.010000
[ Fri Jul 12 06:33:40 2024 ] 	Batch(3800/6809) done. Loss: 0.2733  lr:0.010000
[ Fri Jul 12 06:33:59 2024 ] 	Batch(3900/6809) done. Loss: 0.1474  lr:0.010000
[ Fri Jul 12 06:34:17 2024 ] 
Training: Epoch [27/120], Step [3999], Loss: 0.7699383497238159, Training Accuracy: 83.640625
[ Fri Jul 12 06:34:17 2024 ] 	Batch(4000/6809) done. Loss: 0.2610  lr:0.010000
[ Fri Jul 12 06:34:36 2024 ] 	Batch(4100/6809) done. Loss: 0.4586  lr:0.010000
[ Fri Jul 12 06:34:54 2024 ] 	Batch(4200/6809) done. Loss: 0.8955  lr:0.010000
[ Fri Jul 12 06:35:13 2024 ] 	Batch(4300/6809) done. Loss: 1.3409  lr:0.010000
[ Fri Jul 12 06:35:32 2024 ] 	Batch(4400/6809) done. Loss: 0.4263  lr:0.010000
[ Fri Jul 12 06:35:49 2024 ] 
Training: Epoch [27/120], Step [4499], Loss: 0.29976552724838257, Training Accuracy: 83.58333333333333
[ Fri Jul 12 06:35:49 2024 ] 	Batch(4500/6809) done. Loss: 0.4573  lr:0.010000
[ Fri Jul 12 06:36:07 2024 ] 	Batch(4600/6809) done. Loss: 0.3812  lr:0.010000
[ Fri Jul 12 06:36:25 2024 ] 	Batch(4700/6809) done. Loss: 0.7244  lr:0.010000
[ Fri Jul 12 06:36:43 2024 ] 	Batch(4800/6809) done. Loss: 0.6197  lr:0.010000
[ Fri Jul 12 06:37:01 2024 ] 	Batch(4900/6809) done. Loss: 0.8713  lr:0.010000
[ Fri Jul 12 06:37:19 2024 ] 
Training: Epoch [27/120], Step [4999], Loss: 1.2697142362594604, Training Accuracy: 83.5625
[ Fri Jul 12 06:37:19 2024 ] 	Batch(5000/6809) done. Loss: 0.3223  lr:0.010000
[ Fri Jul 12 06:37:37 2024 ] 	Batch(5100/6809) done. Loss: 0.5225  lr:0.010000
[ Fri Jul 12 06:37:55 2024 ] 	Batch(5200/6809) done. Loss: 0.0989  lr:0.010000
[ Fri Jul 12 06:38:13 2024 ] 	Batch(5300/6809) done. Loss: 0.5210  lr:0.010000
[ Fri Jul 12 06:38:31 2024 ] 	Batch(5400/6809) done. Loss: 0.4946  lr:0.010000
[ Fri Jul 12 06:38:49 2024 ] 
Training: Epoch [27/120], Step [5499], Loss: 0.8009474277496338, Training Accuracy: 83.44772727272726
[ Fri Jul 12 06:38:49 2024 ] 	Batch(5500/6809) done. Loss: 0.6990  lr:0.010000
[ Fri Jul 12 06:39:07 2024 ] 	Batch(5600/6809) done. Loss: 1.3115  lr:0.010000
[ Fri Jul 12 06:39:25 2024 ] 	Batch(5700/6809) done. Loss: 0.1159  lr:0.010000
[ Fri Jul 12 06:39:43 2024 ] 	Batch(5800/6809) done. Loss: 0.2711  lr:0.010000
[ Fri Jul 12 06:40:01 2024 ] 	Batch(5900/6809) done. Loss: 0.4467  lr:0.010000
[ Fri Jul 12 06:40:18 2024 ] 
Training: Epoch [27/120], Step [5999], Loss: 0.4120334982872009, Training Accuracy: 83.31666666666668
[ Fri Jul 12 06:40:19 2024 ] 	Batch(6000/6809) done. Loss: 0.2748  lr:0.010000
[ Fri Jul 12 06:40:37 2024 ] 	Batch(6100/6809) done. Loss: 0.7307  lr:0.010000
[ Fri Jul 12 06:40:54 2024 ] 	Batch(6200/6809) done. Loss: 0.2001  lr:0.010000
[ Fri Jul 12 06:41:12 2024 ] 	Batch(6300/6809) done. Loss: 0.4779  lr:0.010000
[ Fri Jul 12 06:41:30 2024 ] 	Batch(6400/6809) done. Loss: 0.3776  lr:0.010000
[ Fri Jul 12 06:41:48 2024 ] 
Training: Epoch [27/120], Step [6499], Loss: 0.40867894887924194, Training Accuracy: 83.33461538461539
[ Fri Jul 12 06:41:49 2024 ] 	Batch(6500/6809) done. Loss: 0.6281  lr:0.010000
[ Fri Jul 12 06:42:06 2024 ] 	Batch(6600/6809) done. Loss: 0.2549  lr:0.010000
[ Fri Jul 12 06:42:24 2024 ] 	Batch(6700/6809) done. Loss: 0.3351  lr:0.010000
[ Fri Jul 12 06:42:42 2024 ] 	Batch(6800/6809) done. Loss: 1.0781  lr:0.010000
[ Fri Jul 12 06:42:44 2024 ] 	Mean training loss: 0.5312.
[ Fri Jul 12 06:42:44 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 06:42:44 2024 ] Training epoch: 29
[ Fri Jul 12 06:42:45 2024 ] 	Batch(0/6809) done. Loss: 0.2208  lr:0.010000
[ Fri Jul 12 06:43:02 2024 ] 	Batch(100/6809) done. Loss: 0.3285  lr:0.010000
[ Fri Jul 12 06:43:20 2024 ] 	Batch(200/6809) done. Loss: 0.3685  lr:0.010000
[ Fri Jul 12 06:43:38 2024 ] 	Batch(300/6809) done. Loss: 0.7591  lr:0.010000
[ Fri Jul 12 06:43:56 2024 ] 	Batch(400/6809) done. Loss: 0.4354  lr:0.010000
[ Fri Jul 12 06:44:14 2024 ] 
Training: Epoch [28/120], Step [499], Loss: 1.391397476196289, Training Accuracy: 84.1
[ Fri Jul 12 06:44:14 2024 ] 	Batch(500/6809) done. Loss: 1.1244  lr:0.010000
[ Fri Jul 12 06:44:32 2024 ] 	Batch(600/6809) done. Loss: 0.3126  lr:0.010000
[ Fri Jul 12 06:44:50 2024 ] 	Batch(700/6809) done. Loss: 0.3016  lr:0.010000
[ Fri Jul 12 06:45:08 2024 ] 	Batch(800/6809) done. Loss: 0.8330  lr:0.010000
[ Fri Jul 12 06:45:26 2024 ] 	Batch(900/6809) done. Loss: 0.5906  lr:0.010000
[ Fri Jul 12 06:45:44 2024 ] 
Training: Epoch [28/120], Step [999], Loss: 0.08195210248231888, Training Accuracy: 84.0125
[ Fri Jul 12 06:45:44 2024 ] 	Batch(1000/6809) done. Loss: 0.0488  lr:0.010000
[ Fri Jul 12 06:46:02 2024 ] 	Batch(1100/6809) done. Loss: 1.1165  lr:0.010000
[ Fri Jul 12 06:46:20 2024 ] 	Batch(1200/6809) done. Loss: 1.1959  lr:0.010000
[ Fri Jul 12 06:46:39 2024 ] 	Batch(1300/6809) done. Loss: 0.4329  lr:0.010000
[ Fri Jul 12 06:46:57 2024 ] 	Batch(1400/6809) done. Loss: 0.7169  lr:0.010000
[ Fri Jul 12 06:47:16 2024 ] 
Training: Epoch [28/120], Step [1499], Loss: 0.3717358708381653, Training Accuracy: 84.025
[ Fri Jul 12 06:47:16 2024 ] 	Batch(1500/6809) done. Loss: 0.5748  lr:0.010000
[ Fri Jul 12 06:47:34 2024 ] 	Batch(1600/6809) done. Loss: 0.1335  lr:0.010000
[ Fri Jul 12 06:47:53 2024 ] 	Batch(1700/6809) done. Loss: 0.2698  lr:0.010000
[ Fri Jul 12 06:48:12 2024 ] 	Batch(1800/6809) done. Loss: 0.1217  lr:0.010000
[ Fri Jul 12 06:48:30 2024 ] 	Batch(1900/6809) done. Loss: 0.5117  lr:0.010000
[ Fri Jul 12 06:48:48 2024 ] 
Training: Epoch [28/120], Step [1999], Loss: 0.17988638579845428, Training Accuracy: 84.125
[ Fri Jul 12 06:48:49 2024 ] 	Batch(2000/6809) done. Loss: 0.4615  lr:0.010000
[ Fri Jul 12 06:49:08 2024 ] 	Batch(2100/6809) done. Loss: 1.2223  lr:0.010000
[ Fri Jul 12 06:49:26 2024 ] 	Batch(2200/6809) done. Loss: 0.6592  lr:0.010000
[ Fri Jul 12 06:49:43 2024 ] 	Batch(2300/6809) done. Loss: 1.0977  lr:0.010000
[ Fri Jul 12 06:50:02 2024 ] 	Batch(2400/6809) done. Loss: 0.1908  lr:0.010000
[ Fri Jul 12 06:50:19 2024 ] 
Training: Epoch [28/120], Step [2499], Loss: 1.570521354675293, Training Accuracy: 83.885
[ Fri Jul 12 06:50:19 2024 ] 	Batch(2500/6809) done. Loss: 0.8309  lr:0.010000
[ Fri Jul 12 06:50:37 2024 ] 	Batch(2600/6809) done. Loss: 0.1929  lr:0.010000
[ Fri Jul 12 06:50:55 2024 ] 	Batch(2700/6809) done. Loss: 0.6563  lr:0.010000
[ Fri Jul 12 06:51:13 2024 ] 	Batch(2800/6809) done. Loss: 0.6307  lr:0.010000
[ Fri Jul 12 06:51:31 2024 ] 	Batch(2900/6809) done. Loss: 2.1972  lr:0.010000
[ Fri Jul 12 06:51:49 2024 ] 
Training: Epoch [28/120], Step [2999], Loss: 1.003645658493042, Training Accuracy: 83.79166666666666
[ Fri Jul 12 06:51:49 2024 ] 	Batch(3000/6809) done. Loss: 0.5052  lr:0.010000
[ Fri Jul 12 06:52:07 2024 ] 	Batch(3100/6809) done. Loss: 0.5238  lr:0.010000
[ Fri Jul 12 06:52:25 2024 ] 	Batch(3200/6809) done. Loss: 0.5423  lr:0.010000
[ Fri Jul 12 06:52:43 2024 ] 	Batch(3300/6809) done. Loss: 0.4408  lr:0.010000
[ Fri Jul 12 06:53:01 2024 ] 	Batch(3400/6809) done. Loss: 0.7334  lr:0.010000
[ Fri Jul 12 06:53:18 2024 ] 
Training: Epoch [28/120], Step [3499], Loss: 0.2245507538318634, Training Accuracy: 83.76785714285714
[ Fri Jul 12 06:53:18 2024 ] 	Batch(3500/6809) done. Loss: 0.2667  lr:0.010000
[ Fri Jul 12 06:53:36 2024 ] 	Batch(3600/6809) done. Loss: 1.1738  lr:0.010000
[ Fri Jul 12 06:53:54 2024 ] 	Batch(3700/6809) done. Loss: 0.3461  lr:0.010000
[ Fri Jul 12 06:54:12 2024 ] 	Batch(3800/6809) done. Loss: 0.1075  lr:0.010000
[ Fri Jul 12 06:54:30 2024 ] 	Batch(3900/6809) done. Loss: 1.3599  lr:0.010000
[ Fri Jul 12 06:54:48 2024 ] 
Training: Epoch [28/120], Step [3999], Loss: 0.7234277129173279, Training Accuracy: 83.65312499999999
[ Fri Jul 12 06:54:48 2024 ] 	Batch(4000/6809) done. Loss: 0.1734  lr:0.010000
[ Fri Jul 12 06:55:06 2024 ] 	Batch(4100/6809) done. Loss: 0.6458  lr:0.010000
[ Fri Jul 12 06:55:24 2024 ] 	Batch(4200/6809) done. Loss: 1.3239  lr:0.010000
[ Fri Jul 12 06:55:42 2024 ] 	Batch(4300/6809) done. Loss: 0.4109  lr:0.010000
[ Fri Jul 12 06:56:00 2024 ] 	Batch(4400/6809) done. Loss: 1.3383  lr:0.010000
[ Fri Jul 12 06:56:18 2024 ] 
Training: Epoch [28/120], Step [4499], Loss: 0.32043978571891785, Training Accuracy: 83.53333333333333
[ Fri Jul 12 06:56:18 2024 ] 	Batch(4500/6809) done. Loss: 0.1499  lr:0.010000
[ Fri Jul 12 06:56:36 2024 ] 	Batch(4600/6809) done. Loss: 0.4055  lr:0.010000
[ Fri Jul 12 06:56:54 2024 ] 	Batch(4700/6809) done. Loss: 0.0982  lr:0.010000
[ Fri Jul 12 06:57:12 2024 ] 	Batch(4800/6809) done. Loss: 1.1926  lr:0.010000
[ Fri Jul 12 06:57:30 2024 ] 	Batch(4900/6809) done. Loss: 0.3378  lr:0.010000
[ Fri Jul 12 06:57:47 2024 ] 
Training: Epoch [28/120], Step [4999], Loss: 0.3731890320777893, Training Accuracy: 83.3925
[ Fri Jul 12 06:57:47 2024 ] 	Batch(5000/6809) done. Loss: 0.3341  lr:0.010000
[ Fri Jul 12 06:58:05 2024 ] 	Batch(5100/6809) done. Loss: 0.0619  lr:0.010000
[ Fri Jul 12 06:58:23 2024 ] 	Batch(5200/6809) done. Loss: 1.3547  lr:0.010000
[ Fri Jul 12 06:58:41 2024 ] 	Batch(5300/6809) done. Loss: 1.1536  lr:0.010000
[ Fri Jul 12 06:58:59 2024 ] 	Batch(5400/6809) done. Loss: 0.0254  lr:0.010000
[ Fri Jul 12 06:59:17 2024 ] 
Training: Epoch [28/120], Step [5499], Loss: 0.9030581116676331, Training Accuracy: 83.39772727272728
[ Fri Jul 12 06:59:17 2024 ] 	Batch(5500/6809) done. Loss: 0.4111  lr:0.010000
[ Fri Jul 12 06:59:35 2024 ] 	Batch(5600/6809) done. Loss: 1.4412  lr:0.010000
[ Fri Jul 12 06:59:53 2024 ] 	Batch(5700/6809) done. Loss: 0.2185  lr:0.010000
[ Fri Jul 12 07:00:11 2024 ] 	Batch(5800/6809) done. Loss: 0.8451  lr:0.010000
[ Fri Jul 12 07:00:29 2024 ] 	Batch(5900/6809) done. Loss: 0.1634  lr:0.010000
[ Fri Jul 12 07:00:47 2024 ] 
Training: Epoch [28/120], Step [5999], Loss: 0.4181506037712097, Training Accuracy: 83.31458333333333
[ Fri Jul 12 07:00:47 2024 ] 	Batch(6000/6809) done. Loss: 0.3277  lr:0.010000
[ Fri Jul 12 07:01:05 2024 ] 	Batch(6100/6809) done. Loss: 0.3400  lr:0.010000
[ Fri Jul 12 07:01:23 2024 ] 	Batch(6200/6809) done. Loss: 0.2691  lr:0.010000
[ Fri Jul 12 07:01:41 2024 ] 	Batch(6300/6809) done. Loss: 0.5671  lr:0.010000
[ Fri Jul 12 07:01:59 2024 ] 	Batch(6400/6809) done. Loss: 1.0203  lr:0.010000
[ Fri Jul 12 07:02:17 2024 ] 
Training: Epoch [28/120], Step [6499], Loss: 0.033073436468839645, Training Accuracy: 83.25769230769231
[ Fri Jul 12 07:02:17 2024 ] 	Batch(6500/6809) done. Loss: 0.3000  lr:0.010000
[ Fri Jul 12 07:02:35 2024 ] 	Batch(6600/6809) done. Loss: 0.0942  lr:0.010000
[ Fri Jul 12 07:02:53 2024 ] 	Batch(6700/6809) done. Loss: 0.7919  lr:0.010000
[ Fri Jul 12 07:03:11 2024 ] 	Batch(6800/6809) done. Loss: 0.5943  lr:0.010000
[ Fri Jul 12 07:03:12 2024 ] 	Mean training loss: 0.5292.
[ Fri Jul 12 07:03:12 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 07:03:12 2024 ] Training epoch: 30
[ Fri Jul 12 07:03:13 2024 ] 	Batch(0/6809) done. Loss: 0.1859  lr:0.010000
[ Fri Jul 12 07:03:31 2024 ] 	Batch(100/6809) done. Loss: 0.2692  lr:0.010000
[ Fri Jul 12 07:03:49 2024 ] 	Batch(200/6809) done. Loss: 0.4698  lr:0.010000
[ Fri Jul 12 07:04:06 2024 ] 	Batch(300/6809) done. Loss: 0.1426  lr:0.010000
[ Fri Jul 12 07:04:24 2024 ] 	Batch(400/6809) done. Loss: 0.5687  lr:0.010000
[ Fri Jul 12 07:04:42 2024 ] 
Training: Epoch [29/120], Step [499], Loss: 0.7655503153800964, Training Accuracy: 85.02499999999999
[ Fri Jul 12 07:04:42 2024 ] 	Batch(500/6809) done. Loss: 0.0520  lr:0.010000
[ Fri Jul 12 07:05:00 2024 ] 	Batch(600/6809) done. Loss: 0.8216  lr:0.010000
[ Fri Jul 12 07:05:18 2024 ] 	Batch(700/6809) done. Loss: 0.2525  lr:0.010000
[ Fri Jul 12 07:05:36 2024 ] 	Batch(800/6809) done. Loss: 0.8272  lr:0.010000
[ Fri Jul 12 07:05:54 2024 ] 	Batch(900/6809) done. Loss: 0.3919  lr:0.010000
[ Fri Jul 12 07:06:12 2024 ] 
Training: Epoch [29/120], Step [999], Loss: 0.2558993995189667, Training Accuracy: 85.02499999999999
[ Fri Jul 12 07:06:12 2024 ] 	Batch(1000/6809) done. Loss: 0.3242  lr:0.010000
[ Fri Jul 12 07:06:30 2024 ] 	Batch(1100/6809) done. Loss: 0.6224  lr:0.010000
[ Fri Jul 12 07:06:48 2024 ] 	Batch(1200/6809) done. Loss: 0.1477  lr:0.010000
[ Fri Jul 12 07:07:06 2024 ] 	Batch(1300/6809) done. Loss: 0.0383  lr:0.010000
[ Fri Jul 12 07:07:24 2024 ] 	Batch(1400/6809) done. Loss: 0.2311  lr:0.010000
[ Fri Jul 12 07:07:41 2024 ] 
Training: Epoch [29/120], Step [1499], Loss: 0.34994885325431824, Training Accuracy: 84.98333333333333
[ Fri Jul 12 07:07:42 2024 ] 	Batch(1500/6809) done. Loss: 1.0965  lr:0.010000
[ Fri Jul 12 07:08:00 2024 ] 	Batch(1600/6809) done. Loss: 0.2312  lr:0.010000
[ Fri Jul 12 07:08:18 2024 ] 	Batch(1700/6809) done. Loss: 0.1210  lr:0.010000
[ Fri Jul 12 07:08:37 2024 ] 	Batch(1800/6809) done. Loss: 0.1657  lr:0.010000
[ Fri Jul 12 07:08:55 2024 ] 	Batch(1900/6809) done. Loss: 1.0966  lr:0.010000
[ Fri Jul 12 07:09:13 2024 ] 
Training: Epoch [29/120], Step [1999], Loss: 1.7323397397994995, Training Accuracy: 84.79375
[ Fri Jul 12 07:09:13 2024 ] 	Batch(2000/6809) done. Loss: 0.5705  lr:0.010000
[ Fri Jul 12 07:09:31 2024 ] 	Batch(2100/6809) done. Loss: 1.2115  lr:0.010000
[ Fri Jul 12 07:09:49 2024 ] 	Batch(2200/6809) done. Loss: 0.4447  lr:0.010000
[ Fri Jul 12 07:10:07 2024 ] 	Batch(2300/6809) done. Loss: 0.5316  lr:0.010000
[ Fri Jul 12 07:10:25 2024 ] 	Batch(2400/6809) done. Loss: 0.4518  lr:0.010000
[ Fri Jul 12 07:10:44 2024 ] 
Training: Epoch [29/120], Step [2499], Loss: 0.18243633210659027, Training Accuracy: 84.635
[ Fri Jul 12 07:10:44 2024 ] 	Batch(2500/6809) done. Loss: 1.4522  lr:0.010000
[ Fri Jul 12 07:11:02 2024 ] 	Batch(2600/6809) done. Loss: 0.4343  lr:0.010000
[ Fri Jul 12 07:11:21 2024 ] 	Batch(2700/6809) done. Loss: 0.5993  lr:0.010000
[ Fri Jul 12 07:11:40 2024 ] 	Batch(2800/6809) done. Loss: 0.2032  lr:0.010000
[ Fri Jul 12 07:11:57 2024 ] 	Batch(2900/6809) done. Loss: 0.1366  lr:0.010000
[ Fri Jul 12 07:12:15 2024 ] 
Training: Epoch [29/120], Step [2999], Loss: 0.22317826747894287, Training Accuracy: 84.4125
[ Fri Jul 12 07:12:15 2024 ] 	Batch(3000/6809) done. Loss: 0.5190  lr:0.010000
[ Fri Jul 12 07:12:33 2024 ] 	Batch(3100/6809) done. Loss: 0.5319  lr:0.010000
[ Fri Jul 12 07:12:51 2024 ] 	Batch(3200/6809) done. Loss: 0.9421  lr:0.010000
[ Fri Jul 12 07:13:09 2024 ] 	Batch(3300/6809) done. Loss: 1.0980  lr:0.010000
[ Fri Jul 12 07:13:27 2024 ] 	Batch(3400/6809) done. Loss: 0.0439  lr:0.010000
[ Fri Jul 12 07:13:45 2024 ] 
Training: Epoch [29/120], Step [3499], Loss: 0.5733305215835571, Training Accuracy: 84.29642857142858
[ Fri Jul 12 07:13:45 2024 ] 	Batch(3500/6809) done. Loss: 0.3739  lr:0.010000
[ Fri Jul 12 07:14:03 2024 ] 	Batch(3600/6809) done. Loss: 0.4499  lr:0.010000
[ Fri Jul 12 07:14:21 2024 ] 	Batch(3700/6809) done. Loss: 0.7452  lr:0.010000
[ Fri Jul 12 07:14:39 2024 ] 	Batch(3800/6809) done. Loss: 0.2251  lr:0.010000
[ Fri Jul 12 07:14:57 2024 ] 	Batch(3900/6809) done. Loss: 1.0723  lr:0.010000
[ Fri Jul 12 07:15:15 2024 ] 
Training: Epoch [29/120], Step [3999], Loss: 0.49332207441329956, Training Accuracy: 84.234375
[ Fri Jul 12 07:15:15 2024 ] 	Batch(4000/6809) done. Loss: 0.3992  lr:0.010000
[ Fri Jul 12 07:15:33 2024 ] 	Batch(4100/6809) done. Loss: 0.3765  lr:0.010000
[ Fri Jul 12 07:15:51 2024 ] 	Batch(4200/6809) done. Loss: 0.7264  lr:0.010000
[ Fri Jul 12 07:16:09 2024 ] 	Batch(4300/6809) done. Loss: 0.5105  lr:0.010000
[ Fri Jul 12 07:16:27 2024 ] 	Batch(4400/6809) done. Loss: 0.6351  lr:0.010000
[ Fri Jul 12 07:16:44 2024 ] 
Training: Epoch [29/120], Step [4499], Loss: 0.11654706299304962, Training Accuracy: 84.08055555555556
[ Fri Jul 12 07:16:45 2024 ] 	Batch(4500/6809) done. Loss: 0.2077  lr:0.010000
[ Fri Jul 12 07:17:02 2024 ] 	Batch(4600/6809) done. Loss: 0.0399  lr:0.010000
[ Fri Jul 12 07:17:20 2024 ] 	Batch(4700/6809) done. Loss: 0.3043  lr:0.010000
[ Fri Jul 12 07:17:38 2024 ] 	Batch(4800/6809) done. Loss: 0.1755  lr:0.010000
[ Fri Jul 12 07:17:56 2024 ] 	Batch(4900/6809) done. Loss: 1.2128  lr:0.010000
[ Fri Jul 12 07:18:14 2024 ] 
Training: Epoch [29/120], Step [4999], Loss: 0.30306658148765564, Training Accuracy: 83.9375
[ Fri Jul 12 07:18:14 2024 ] 	Batch(5000/6809) done. Loss: 0.1713  lr:0.010000
[ Fri Jul 12 07:18:32 2024 ] 	Batch(5100/6809) done. Loss: 0.1074  lr:0.010000
[ Fri Jul 12 07:18:50 2024 ] 	Batch(5200/6809) done. Loss: 0.7796  lr:0.010000
[ Fri Jul 12 07:19:08 2024 ] 	Batch(5300/6809) done. Loss: 0.8609  lr:0.010000
[ Fri Jul 12 07:19:26 2024 ] 	Batch(5400/6809) done. Loss: 0.3599  lr:0.010000
[ Fri Jul 12 07:19:44 2024 ] 
Training: Epoch [29/120], Step [5499], Loss: 0.2913767695426941, Training Accuracy: 83.96818181818182
[ Fri Jul 12 07:19:44 2024 ] 	Batch(5500/6809) done. Loss: 0.1674  lr:0.010000
[ Fri Jul 12 07:20:02 2024 ] 	Batch(5600/6809) done. Loss: 2.4152  lr:0.010000
[ Fri Jul 12 07:20:21 2024 ] 	Batch(5700/6809) done. Loss: 0.1317  lr:0.010000
[ Fri Jul 12 07:20:39 2024 ] 	Batch(5800/6809) done. Loss: 0.5012  lr:0.010000
[ Fri Jul 12 07:20:57 2024 ] 	Batch(5900/6809) done. Loss: 0.8262  lr:0.010000
[ Fri Jul 12 07:21:15 2024 ] 
Training: Epoch [29/120], Step [5999], Loss: 0.2439662516117096, Training Accuracy: 83.89583333333334
[ Fri Jul 12 07:21:15 2024 ] 	Batch(6000/6809) done. Loss: 0.5728  lr:0.010000
[ Fri Jul 12 07:21:34 2024 ] 	Batch(6100/6809) done. Loss: 0.5293  lr:0.010000
[ Fri Jul 12 07:21:52 2024 ] 	Batch(6200/6809) done. Loss: 0.2970  lr:0.010000
[ Fri Jul 12 07:22:10 2024 ] 	Batch(6300/6809) done. Loss: 0.8261  lr:0.010000
[ Fri Jul 12 07:22:28 2024 ] 	Batch(6400/6809) done. Loss: 0.5037  lr:0.010000
[ Fri Jul 12 07:22:46 2024 ] 
Training: Epoch [29/120], Step [6499], Loss: 1.1178635358810425, Training Accuracy: 83.86923076923077
[ Fri Jul 12 07:22:46 2024 ] 	Batch(6500/6809) done. Loss: 0.2767  lr:0.010000
[ Fri Jul 12 07:23:04 2024 ] 	Batch(6600/6809) done. Loss: 0.3519  lr:0.010000
[ Fri Jul 12 07:23:22 2024 ] 	Batch(6700/6809) done. Loss: 1.0681  lr:0.010000
[ Fri Jul 12 07:23:40 2024 ] 	Batch(6800/6809) done. Loss: 0.0819  lr:0.010000
[ Fri Jul 12 07:23:41 2024 ] 	Mean training loss: 0.5069.
[ Fri Jul 12 07:23:41 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 07:23:41 2024 ] Eval epoch: 30
[ Fri Jul 12 07:29:16 2024 ] 	Mean val loss of 7435 batches: 1.0051550882830795.
[ Fri Jul 12 07:29:16 2024 ] 
Validation: Epoch [29/120], Samples [45200.0/59477], Loss: 0.3965224623680115, Validation Accuracy: 75.99576306807674
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 1 : 353 / 500 = 70 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 2 : 377 / 499 = 75 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 3 : 354 / 500 = 70 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 4 : 420 / 502 = 83 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 5 : 460 / 502 = 91 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 6 : 368 / 502 = 73 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 7 : 456 / 497 = 91 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 8 : 476 / 498 = 95 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 9 : 347 / 500 = 69 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 10 : 268 / 500 = 53 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 11 : 183 / 498 = 36 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 12 : 368 / 499 = 73 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 13 : 482 / 502 = 96 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 14 : 466 / 504 = 92 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 15 : 418 / 502 = 83 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 16 : 346 / 502 = 68 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 17 : 390 / 504 = 77 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 18 : 379 / 504 = 75 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 19 : 443 / 502 = 88 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 20 : 466 / 502 = 92 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 21 : 454 / 503 = 90 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 22 : 370 / 504 = 73 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 23 : 442 / 503 = 87 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 24 : 434 / 504 = 86 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 25 : 486 / 504 = 96 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 26 : 485 / 504 = 96 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 27 : 397 / 501 = 79 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 28 : 285 / 502 = 56 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 29 : 284 / 502 = 56 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 30 : 234 / 501 = 46 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 31 : 359 / 504 = 71 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 32 : 404 / 503 = 80 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 33 : 379 / 503 = 75 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 34 : 445 / 504 = 88 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 35 : 459 / 503 = 91 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 36 : 409 / 502 = 81 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 37 : 441 / 504 = 87 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 38 : 444 / 504 = 88 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 39 : 416 / 498 = 83 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 40 : 340 / 504 = 67 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 41 : 442 / 503 = 87 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 42 : 449 / 504 = 89 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 43 : 341 / 503 = 67 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 44 : 395 / 504 = 78 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 45 : 376 / 504 = 74 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 46 : 422 / 504 = 83 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 47 : 392 / 503 = 77 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 48 : 414 / 503 = 82 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 49 : 354 / 499 = 70 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 50 : 399 / 502 = 79 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 51 : 467 / 503 = 92 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 52 : 386 / 504 = 76 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 53 : 445 / 497 = 89 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 54 : 430 / 480 = 89 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 55 : 358 / 504 = 71 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 56 : 363 / 503 = 72 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 57 : 477 / 504 = 94 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 58 : 469 / 499 = 93 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 59 : 488 / 503 = 97 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 60 : 413 / 479 = 86 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 61 : 436 / 484 = 90 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 62 : 381 / 487 = 78 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 63 : 447 / 489 = 91 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 64 : 356 / 488 = 72 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 65 : 433 / 490 = 88 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 66 : 270 / 488 = 55 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 67 : 318 / 490 = 64 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 68 : 293 / 490 = 59 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 69 : 348 / 490 = 71 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 70 : 105 / 490 = 21 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 71 : 266 / 490 = 54 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 72 : 219 / 488 = 44 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 73 : 228 / 486 = 46 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 74 : 289 / 481 = 60 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 75 : 199 / 488 = 40 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 76 : 226 / 489 = 46 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 77 : 259 / 488 = 53 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 78 : 336 / 488 = 68 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 79 : 449 / 490 = 91 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 80 : 383 / 489 = 78 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 81 : 181 / 491 = 36 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 82 : 225 / 491 = 45 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 83 : 233 / 489 = 47 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 84 : 384 / 489 = 78 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 85 : 400 / 489 = 81 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 86 : 377 / 491 = 76 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 87 : 437 / 492 = 88 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 88 : 339 / 491 = 69 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 89 : 360 / 492 = 73 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 90 : 262 / 490 = 53 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 91 : 389 / 482 = 80 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 92 : 357 / 490 = 72 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 93 : 392 / 487 = 80 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 94 : 407 / 489 = 83 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 95 : 385 / 490 = 78 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 96 : 455 / 491 = 92 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 97 : 464 / 490 = 94 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 98 : 420 / 491 = 85 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 99 : 434 / 491 = 88 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 100 : 436 / 491 = 88 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 101 : 412 / 491 = 83 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 102 : 269 / 492 = 54 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 103 : 411 / 492 = 83 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 104 : 281 / 491 = 57 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 105 : 325 / 491 = 66 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 106 : 169 / 492 = 34 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 107 : 408 / 491 = 83 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 108 : 407 / 492 = 82 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 109 : 290 / 490 = 59 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 110 : 421 / 491 = 85 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 111 : 440 / 492 = 89 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 112 : 444 / 492 = 90 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 113 : 460 / 491 = 93 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 114 : 379 / 491 = 77 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 115 : 437 / 492 = 88 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 116 : 348 / 491 = 70 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 117 : 449 / 492 = 91 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 118 : 424 / 490 = 86 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 119 : 443 / 492 = 90 %
[ Fri Jul 12 07:29:16 2024 ] Accuracy of 120 : 338 / 500 = 67 %
[ Fri Jul 12 07:29:16 2024 ] Training epoch: 31
[ Fri Jul 12 07:29:16 2024 ] 	Batch(0/6809) done. Loss: 0.7737  lr:0.010000
[ Fri Jul 12 07:29:34 2024 ] 	Batch(100/6809) done. Loss: 0.3114  lr:0.010000
[ Fri Jul 12 07:29:52 2024 ] 	Batch(200/6809) done. Loss: 0.1872  lr:0.010000
[ Fri Jul 12 07:30:10 2024 ] 	Batch(300/6809) done. Loss: 0.4570  lr:0.010000
[ Fri Jul 12 07:30:28 2024 ] 	Batch(400/6809) done. Loss: 0.3336  lr:0.010000
[ Fri Jul 12 07:30:45 2024 ] 
Training: Epoch [30/120], Step [499], Loss: 0.8024802207946777, Training Accuracy: 85.1
[ Fri Jul 12 07:30:46 2024 ] 	Batch(500/6809) done. Loss: 0.1514  lr:0.010000
[ Fri Jul 12 07:31:03 2024 ] 	Batch(600/6809) done. Loss: 0.2397  lr:0.010000
[ Fri Jul 12 07:31:21 2024 ] 	Batch(700/6809) done. Loss: 1.5008  lr:0.010000
[ Fri Jul 12 07:31:39 2024 ] 	Batch(800/6809) done. Loss: 0.1522  lr:0.010000
[ Fri Jul 12 07:31:57 2024 ] 	Batch(900/6809) done. Loss: 0.9190  lr:0.010000
[ Fri Jul 12 07:32:15 2024 ] 
Training: Epoch [30/120], Step [999], Loss: 0.8675308227539062, Training Accuracy: 85.1125
[ Fri Jul 12 07:32:15 2024 ] 	Batch(1000/6809) done. Loss: 0.7027  lr:0.010000
[ Fri Jul 12 07:32:33 2024 ] 	Batch(1100/6809) done. Loss: 0.5162  lr:0.010000
[ Fri Jul 12 07:32:52 2024 ] 	Batch(1200/6809) done. Loss: 0.5642  lr:0.010000
[ Fri Jul 12 07:33:10 2024 ] 	Batch(1300/6809) done. Loss: 0.1997  lr:0.010000
[ Fri Jul 12 07:33:29 2024 ] 	Batch(1400/6809) done. Loss: 0.2165  lr:0.010000
[ Fri Jul 12 07:33:47 2024 ] 
Training: Epoch [30/120], Step [1499], Loss: 0.02895728498697281, Training Accuracy: 84.98333333333333
[ Fri Jul 12 07:33:48 2024 ] 	Batch(1500/6809) done. Loss: 0.0505  lr:0.010000
[ Fri Jul 12 07:34:06 2024 ] 	Batch(1600/6809) done. Loss: 0.3053  lr:0.010000
[ Fri Jul 12 07:34:25 2024 ] 	Batch(1700/6809) done. Loss: 0.7290  lr:0.010000
[ Fri Jul 12 07:34:43 2024 ] 	Batch(1800/6809) done. Loss: 0.0442  lr:0.010000
[ Fri Jul 12 07:35:02 2024 ] 	Batch(1900/6809) done. Loss: 0.4060  lr:0.010000
[ Fri Jul 12 07:35:20 2024 ] 
Training: Epoch [30/120], Step [1999], Loss: 0.12544217705726624, Training Accuracy: 85.16875
[ Fri Jul 12 07:35:21 2024 ] 	Batch(2000/6809) done. Loss: 0.5464  lr:0.010000
[ Fri Jul 12 07:35:39 2024 ] 	Batch(2100/6809) done. Loss: 0.2973  lr:0.010000
[ Fri Jul 12 07:35:57 2024 ] 	Batch(2200/6809) done. Loss: 1.2237  lr:0.010000
[ Fri Jul 12 07:36:14 2024 ] 	Batch(2300/6809) done. Loss: 0.2255  lr:0.010000
[ Fri Jul 12 07:36:33 2024 ] 	Batch(2400/6809) done. Loss: 1.2014  lr:0.010000
[ Fri Jul 12 07:36:51 2024 ] 
Training: Epoch [30/120], Step [2499], Loss: 0.7743351459503174, Training Accuracy: 84.88
[ Fri Jul 12 07:36:51 2024 ] 	Batch(2500/6809) done. Loss: 1.2783  lr:0.010000
[ Fri Jul 12 07:37:09 2024 ] 	Batch(2600/6809) done. Loss: 0.2666  lr:0.010000
[ Fri Jul 12 07:37:27 2024 ] 	Batch(2700/6809) done. Loss: 0.3858  lr:0.010000
[ Fri Jul 12 07:37:45 2024 ] 	Batch(2800/6809) done. Loss: 0.0407  lr:0.010000
[ Fri Jul 12 07:38:03 2024 ] 	Batch(2900/6809) done. Loss: 0.4523  lr:0.010000
[ Fri Jul 12 07:38:21 2024 ] 
Training: Epoch [30/120], Step [2999], Loss: 0.12752872705459595, Training Accuracy: 84.84583333333333
[ Fri Jul 12 07:38:22 2024 ] 	Batch(3000/6809) done. Loss: 0.5875  lr:0.010000
[ Fri Jul 12 07:38:40 2024 ] 	Batch(3100/6809) done. Loss: 0.2578  lr:0.010000
[ Fri Jul 12 07:38:58 2024 ] 	Batch(3200/6809) done. Loss: 1.2309  lr:0.010000
[ Fri Jul 12 07:39:17 2024 ] 	Batch(3300/6809) done. Loss: 0.1454  lr:0.010000
[ Fri Jul 12 07:39:36 2024 ] 	Batch(3400/6809) done. Loss: 0.5574  lr:0.010000
[ Fri Jul 12 07:39:55 2024 ] 
Training: Epoch [30/120], Step [3499], Loss: 1.0838425159454346, Training Accuracy: 84.87857142857142
[ Fri Jul 12 07:39:55 2024 ] 	Batch(3500/6809) done. Loss: 0.6947  lr:0.010000
[ Fri Jul 12 07:40:14 2024 ] 	Batch(3600/6809) done. Loss: 0.8479  lr:0.010000
[ Fri Jul 12 07:40:32 2024 ] 	Batch(3700/6809) done. Loss: 0.7581  lr:0.010000
[ Fri Jul 12 07:40:50 2024 ] 	Batch(3800/6809) done. Loss: 0.7711  lr:0.010000
[ Fri Jul 12 07:41:08 2024 ] 	Batch(3900/6809) done. Loss: 0.0528  lr:0.010000
[ Fri Jul 12 07:41:26 2024 ] 
Training: Epoch [30/120], Step [3999], Loss: 0.957979679107666, Training Accuracy: 84.78125
[ Fri Jul 12 07:41:27 2024 ] 	Batch(4000/6809) done. Loss: 0.5201  lr:0.010000
[ Fri Jul 12 07:41:45 2024 ] 	Batch(4100/6809) done. Loss: 0.4374  lr:0.010000
[ Fri Jul 12 07:42:03 2024 ] 	Batch(4200/6809) done. Loss: 0.2775  lr:0.010000
[ Fri Jul 12 07:42:20 2024 ] 	Batch(4300/6809) done. Loss: 0.5313  lr:0.010000
[ Fri Jul 12 07:42:38 2024 ] 	Batch(4400/6809) done. Loss: 0.9442  lr:0.010000
[ Fri Jul 12 07:42:57 2024 ] 
Training: Epoch [30/120], Step [4499], Loss: 1.183345913887024, Training Accuracy: 84.62777777777778
[ Fri Jul 12 07:42:57 2024 ] 	Batch(4500/6809) done. Loss: 0.9802  lr:0.010000
[ Fri Jul 12 07:43:15 2024 ] 	Batch(4600/6809) done. Loss: 0.2289  lr:0.010000
[ Fri Jul 12 07:43:34 2024 ] 	Batch(4700/6809) done. Loss: 0.9537  lr:0.010000
[ Fri Jul 12 07:43:53 2024 ] 	Batch(4800/6809) done. Loss: 0.1578  lr:0.010000
[ Fri Jul 12 07:44:10 2024 ] 	Batch(4900/6809) done. Loss: 0.7815  lr:0.010000
[ Fri Jul 12 07:44:28 2024 ] 
Training: Epoch [30/120], Step [4999], Loss: 0.8489681482315063, Training Accuracy: 84.45
[ Fri Jul 12 07:44:28 2024 ] 	Batch(5000/6809) done. Loss: 0.3425  lr:0.010000
[ Fri Jul 12 07:44:46 2024 ] 	Batch(5100/6809) done. Loss: 0.4215  lr:0.010000
[ Fri Jul 12 07:45:04 2024 ] 	Batch(5200/6809) done. Loss: 0.4797  lr:0.010000
[ Fri Jul 12 07:45:23 2024 ] 	Batch(5300/6809) done. Loss: 0.1111  lr:0.010000
[ Fri Jul 12 07:45:41 2024 ] 	Batch(5400/6809) done. Loss: 0.4429  lr:0.010000
[ Fri Jul 12 07:45:59 2024 ] 
Training: Epoch [30/120], Step [5499], Loss: 0.448516845703125, Training Accuracy: 84.28181818181818
[ Fri Jul 12 07:45:59 2024 ] 	Batch(5500/6809) done. Loss: 0.0780  lr:0.010000
[ Fri Jul 12 07:46:17 2024 ] 	Batch(5600/6809) done. Loss: 0.0561  lr:0.010000
[ Fri Jul 12 07:46:35 2024 ] 	Batch(5700/6809) done. Loss: 0.9965  lr:0.010000
[ Fri Jul 12 07:46:53 2024 ] 	Batch(5800/6809) done. Loss: 0.2910  lr:0.010000
[ Fri Jul 12 07:47:11 2024 ] 	Batch(5900/6809) done. Loss: 0.1098  lr:0.010000
[ Fri Jul 12 07:47:28 2024 ] 
Training: Epoch [30/120], Step [5999], Loss: 0.375163733959198, Training Accuracy: 84.3375
[ Fri Jul 12 07:47:29 2024 ] 	Batch(6000/6809) done. Loss: 0.2853  lr:0.010000
[ Fri Jul 12 07:47:47 2024 ] 	Batch(6100/6809) done. Loss: 0.3507  lr:0.010000
[ Fri Jul 12 07:48:05 2024 ] 	Batch(6200/6809) done. Loss: 0.0777  lr:0.010000
[ Fri Jul 12 07:48:22 2024 ] 	Batch(6300/6809) done. Loss: 0.0374  lr:0.010000
[ Fri Jul 12 07:48:40 2024 ] 	Batch(6400/6809) done. Loss: 0.4814  lr:0.010000
[ Fri Jul 12 07:48:58 2024 ] 
Training: Epoch [30/120], Step [6499], Loss: 0.6614605784416199, Training Accuracy: 84.31346153846154
[ Fri Jul 12 07:48:58 2024 ] 	Batch(6500/6809) done. Loss: 0.4936  lr:0.010000
[ Fri Jul 12 07:49:16 2024 ] 	Batch(6600/6809) done. Loss: 0.3160  lr:0.010000
[ Fri Jul 12 07:49:34 2024 ] 	Batch(6700/6809) done. Loss: 0.2068  lr:0.010000
[ Fri Jul 12 07:49:52 2024 ] 	Batch(6800/6809) done. Loss: 0.5499  lr:0.010000
[ Fri Jul 12 07:49:54 2024 ] 	Mean training loss: 0.5066.
[ Fri Jul 12 07:49:54 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 07:49:54 2024 ] Training epoch: 32
[ Fri Jul 12 07:49:54 2024 ] 	Batch(0/6809) done. Loss: 1.3578  lr:0.010000
[ Fri Jul 12 07:50:12 2024 ] 	Batch(100/6809) done. Loss: 0.0748  lr:0.010000
[ Fri Jul 12 07:50:30 2024 ] 	Batch(200/6809) done. Loss: 0.3350  lr:0.010000
[ Fri Jul 12 07:50:48 2024 ] 	Batch(300/6809) done. Loss: 0.8353  lr:0.010000
[ Fri Jul 12 07:51:06 2024 ] 	Batch(400/6809) done. Loss: 0.9551  lr:0.010000
[ Fri Jul 12 07:51:24 2024 ] 
Training: Epoch [31/120], Step [499], Loss: 0.22683575749397278, Training Accuracy: 85.275
[ Fri Jul 12 07:51:24 2024 ] 	Batch(500/6809) done. Loss: 0.8208  lr:0.010000
[ Fri Jul 12 07:51:42 2024 ] 	Batch(600/6809) done. Loss: 0.0645  lr:0.010000
[ Fri Jul 12 07:52:00 2024 ] 	Batch(700/6809) done. Loss: 0.2248  lr:0.010000
[ Fri Jul 12 07:52:18 2024 ] 	Batch(800/6809) done. Loss: 0.1609  lr:0.010000
[ Fri Jul 12 07:52:36 2024 ] 	Batch(900/6809) done. Loss: 0.2936  lr:0.010000
[ Fri Jul 12 07:52:54 2024 ] 
Training: Epoch [31/120], Step [999], Loss: 0.04673426225781441, Training Accuracy: 85.55
[ Fri Jul 12 07:52:54 2024 ] 	Batch(1000/6809) done. Loss: 0.1125  lr:0.010000
[ Fri Jul 12 07:53:12 2024 ] 	Batch(1100/6809) done. Loss: 0.0414  lr:0.010000
[ Fri Jul 12 07:53:30 2024 ] 	Batch(1200/6809) done. Loss: 0.8363  lr:0.010000
[ Fri Jul 12 07:53:48 2024 ] 	Batch(1300/6809) done. Loss: 0.3987  lr:0.010000
[ Fri Jul 12 07:54:06 2024 ] 	Batch(1400/6809) done. Loss: 0.3900  lr:0.010000
[ Fri Jul 12 07:54:24 2024 ] 
Training: Epoch [31/120], Step [1499], Loss: 0.6363992691040039, Training Accuracy: 85.5
[ Fri Jul 12 07:54:24 2024 ] 	Batch(1500/6809) done. Loss: 0.4693  lr:0.010000
[ Fri Jul 12 07:54:42 2024 ] 	Batch(1600/6809) done. Loss: 0.3179  lr:0.010000
[ Fri Jul 12 07:55:01 2024 ] 	Batch(1700/6809) done. Loss: 0.5862  lr:0.010000
[ Fri Jul 12 07:55:19 2024 ] 	Batch(1800/6809) done. Loss: 0.3327  lr:0.010000
[ Fri Jul 12 07:55:38 2024 ] 	Batch(1900/6809) done. Loss: 0.1596  lr:0.010000
[ Fri Jul 12 07:55:56 2024 ] 
Training: Epoch [31/120], Step [1999], Loss: 0.39201849699020386, Training Accuracy: 85.31875
[ Fri Jul 12 07:55:56 2024 ] 	Batch(2000/6809) done. Loss: 0.4205  lr:0.010000
[ Fri Jul 12 07:56:15 2024 ] 	Batch(2100/6809) done. Loss: 0.2846  lr:0.010000
[ Fri Jul 12 07:56:34 2024 ] 	Batch(2200/6809) done. Loss: 0.2699  lr:0.010000
[ Fri Jul 12 07:56:52 2024 ] 	Batch(2300/6809) done. Loss: 0.4110  lr:0.010000
[ Fri Jul 12 07:57:11 2024 ] 	Batch(2400/6809) done. Loss: 1.0887  lr:0.010000
[ Fri Jul 12 07:57:29 2024 ] 
Training: Epoch [31/120], Step [2499], Loss: 0.6173851490020752, Training Accuracy: 85.05
[ Fri Jul 12 07:57:29 2024 ] 	Batch(2500/6809) done. Loss: 0.7024  lr:0.010000
[ Fri Jul 12 07:57:48 2024 ] 	Batch(2600/6809) done. Loss: 0.2134  lr:0.010000
[ Fri Jul 12 07:58:06 2024 ] 	Batch(2700/6809) done. Loss: 0.3370  lr:0.010000
[ Fri Jul 12 07:58:25 2024 ] 	Batch(2800/6809) done. Loss: 0.2538  lr:0.010000
[ Fri Jul 12 07:58:43 2024 ] 	Batch(2900/6809) done. Loss: 1.1527  lr:0.010000
[ Fri Jul 12 07:59:00 2024 ] 
Training: Epoch [31/120], Step [2999], Loss: 0.6703712940216064, Training Accuracy: 84.97916666666666
[ Fri Jul 12 07:59:01 2024 ] 	Batch(3000/6809) done. Loss: 0.4575  lr:0.010000
[ Fri Jul 12 07:59:18 2024 ] 	Batch(3100/6809) done. Loss: 0.5475  lr:0.010000
[ Fri Jul 12 07:59:36 2024 ] 	Batch(3200/6809) done. Loss: 0.2828  lr:0.010000
[ Fri Jul 12 07:59:54 2024 ] 	Batch(3300/6809) done. Loss: 0.8330  lr:0.010000
[ Fri Jul 12 08:00:12 2024 ] 	Batch(3400/6809) done. Loss: 0.1808  lr:0.010000
[ Fri Jul 12 08:00:30 2024 ] 
Training: Epoch [31/120], Step [3499], Loss: 0.43044179677963257, Training Accuracy: 84.83928571428572
[ Fri Jul 12 08:00:30 2024 ] 	Batch(3500/6809) done. Loss: 0.3936  lr:0.010000
[ Fri Jul 12 08:00:48 2024 ] 	Batch(3600/6809) done. Loss: 0.5158  lr:0.010000
[ Fri Jul 12 08:01:06 2024 ] 	Batch(3700/6809) done. Loss: 0.8377  lr:0.010000
[ Fri Jul 12 08:01:24 2024 ] 	Batch(3800/6809) done. Loss: 1.5279  lr:0.010000
[ Fri Jul 12 08:01:42 2024 ] 	Batch(3900/6809) done. Loss: 0.2562  lr:0.010000
[ Fri Jul 12 08:01:59 2024 ] 
Training: Epoch [31/120], Step [3999], Loss: 0.15121233463287354, Training Accuracy: 84.79375
[ Fri Jul 12 08:02:00 2024 ] 	Batch(4000/6809) done. Loss: 0.2753  lr:0.010000
[ Fri Jul 12 08:02:18 2024 ] 	Batch(4100/6809) done. Loss: 0.0959  lr:0.010000
[ Fri Jul 12 08:02:36 2024 ] 	Batch(4200/6809) done. Loss: 0.1269  lr:0.010000
[ Fri Jul 12 08:02:54 2024 ] 	Batch(4300/6809) done. Loss: 0.6881  lr:0.010000
[ Fri Jul 12 08:03:12 2024 ] 	Batch(4400/6809) done. Loss: 0.3764  lr:0.010000
[ Fri Jul 12 08:03:30 2024 ] 
Training: Epoch [31/120], Step [4499], Loss: 0.4624292552471161, Training Accuracy: 84.65555555555555
[ Fri Jul 12 08:03:30 2024 ] 	Batch(4500/6809) done. Loss: 0.4040  lr:0.010000
[ Fri Jul 12 08:03:48 2024 ] 	Batch(4600/6809) done. Loss: 0.5385  lr:0.010000
[ Fri Jul 12 08:04:06 2024 ] 	Batch(4700/6809) done. Loss: 0.1935  lr:0.010000
[ Fri Jul 12 08:04:24 2024 ] 	Batch(4800/6809) done. Loss: 0.0329  lr:0.010000
[ Fri Jul 12 08:04:42 2024 ] 	Batch(4900/6809) done. Loss: 0.0860  lr:0.010000
[ Fri Jul 12 08:05:01 2024 ] 
Training: Epoch [31/120], Step [4999], Loss: 1.052153468132019, Training Accuracy: 84.665
[ Fri Jul 12 08:05:01 2024 ] 	Batch(5000/6809) done. Loss: 0.5946  lr:0.010000
[ Fri Jul 12 08:05:19 2024 ] 	Batch(5100/6809) done. Loss: 0.0908  lr:0.010000
[ Fri Jul 12 08:05:38 2024 ] 	Batch(5200/6809) done. Loss: 0.2911  lr:0.010000
[ Fri Jul 12 08:05:57 2024 ] 	Batch(5300/6809) done. Loss: 0.3445  lr:0.010000
[ Fri Jul 12 08:06:15 2024 ] 	Batch(5400/6809) done. Loss: 0.3828  lr:0.010000
[ Fri Jul 12 08:06:33 2024 ] 
Training: Epoch [31/120], Step [5499], Loss: 0.5891733169555664, Training Accuracy: 84.63181818181819
[ Fri Jul 12 08:06:34 2024 ] 	Batch(5500/6809) done. Loss: 0.1629  lr:0.010000
[ Fri Jul 12 08:06:52 2024 ] 	Batch(5600/6809) done. Loss: 0.4996  lr:0.010000
[ Fri Jul 12 08:07:10 2024 ] 	Batch(5700/6809) done. Loss: 1.4045  lr:0.010000
[ Fri Jul 12 08:07:28 2024 ] 	Batch(5800/6809) done. Loss: 0.5484  lr:0.010000
[ Fri Jul 12 08:07:46 2024 ] 	Batch(5900/6809) done. Loss: 0.0995  lr:0.010000
[ Fri Jul 12 08:08:04 2024 ] 
Training: Epoch [31/120], Step [5999], Loss: 0.27737632393836975, Training Accuracy: 84.53333333333333
[ Fri Jul 12 08:08:04 2024 ] 	Batch(6000/6809) done. Loss: 0.2568  lr:0.010000
[ Fri Jul 12 08:08:22 2024 ] 	Batch(6100/6809) done. Loss: 0.3695  lr:0.010000
[ Fri Jul 12 08:08:40 2024 ] 	Batch(6200/6809) done. Loss: 0.3368  lr:0.010000
[ Fri Jul 12 08:08:58 2024 ] 	Batch(6300/6809) done. Loss: 0.5835  lr:0.010000
[ Fri Jul 12 08:09:16 2024 ] 	Batch(6400/6809) done. Loss: 0.1203  lr:0.010000
[ Fri Jul 12 08:09:34 2024 ] 
Training: Epoch [31/120], Step [6499], Loss: 0.13328172266483307, Training Accuracy: 84.43846153846154
[ Fri Jul 12 08:09:34 2024 ] 	Batch(6500/6809) done. Loss: 0.3826  lr:0.010000
[ Fri Jul 12 08:09:53 2024 ] 	Batch(6600/6809) done. Loss: 0.8620  lr:0.010000
[ Fri Jul 12 08:10:11 2024 ] 	Batch(6700/6809) done. Loss: 0.0448  lr:0.010000
[ Fri Jul 12 08:10:30 2024 ] 	Batch(6800/6809) done. Loss: 0.7141  lr:0.010000
[ Fri Jul 12 08:10:31 2024 ] 	Mean training loss: 0.4894.
[ Fri Jul 12 08:10:31 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 08:10:32 2024 ] Training epoch: 33
[ Fri Jul 12 08:10:32 2024 ] 	Batch(0/6809) done. Loss: 0.1064  lr:0.010000
[ Fri Jul 12 08:10:50 2024 ] 	Batch(100/6809) done. Loss: 0.8329  lr:0.010000
[ Fri Jul 12 08:11:08 2024 ] 	Batch(200/6809) done. Loss: 0.3738  lr:0.010000
[ Fri Jul 12 08:11:26 2024 ] 	Batch(300/6809) done. Loss: 0.5710  lr:0.010000
[ Fri Jul 12 08:11:44 2024 ] 	Batch(400/6809) done. Loss: 0.3945  lr:0.010000
[ Fri Jul 12 08:12:01 2024 ] 
Training: Epoch [32/120], Step [499], Loss: 0.38864627480506897, Training Accuracy: 86.625
[ Fri Jul 12 08:12:02 2024 ] 	Batch(500/6809) done. Loss: 0.2590  lr:0.010000
[ Fri Jul 12 08:12:19 2024 ] 	Batch(600/6809) done. Loss: 0.3327  lr:0.010000
[ Fri Jul 12 08:12:37 2024 ] 	Batch(700/6809) done. Loss: 0.0134  lr:0.010000
[ Fri Jul 12 08:12:55 2024 ] 	Batch(800/6809) done. Loss: 0.0703  lr:0.010000
[ Fri Jul 12 08:13:14 2024 ] 	Batch(900/6809) done. Loss: 0.2110  lr:0.010000
[ Fri Jul 12 08:13:32 2024 ] 
Training: Epoch [32/120], Step [999], Loss: 0.062141600996255875, Training Accuracy: 86.4375
[ Fri Jul 12 08:13:32 2024 ] 	Batch(1000/6809) done. Loss: 0.3286  lr:0.010000
[ Fri Jul 12 08:13:51 2024 ] 	Batch(1100/6809) done. Loss: 0.2562  lr:0.010000
[ Fri Jul 12 08:14:10 2024 ] 	Batch(1200/6809) done. Loss: 0.3052  lr:0.010000
[ Fri Jul 12 08:14:28 2024 ] 	Batch(1300/6809) done. Loss: 0.2369  lr:0.010000
[ Fri Jul 12 08:14:47 2024 ] 	Batch(1400/6809) done. Loss: 0.3011  lr:0.010000
[ Fri Jul 12 08:15:05 2024 ] 
Training: Epoch [32/120], Step [1499], Loss: 0.23321816325187683, Training Accuracy: 85.81666666666666
[ Fri Jul 12 08:15:05 2024 ] 	Batch(1500/6809) done. Loss: 0.1748  lr:0.010000
[ Fri Jul 12 08:15:23 2024 ] 	Batch(1600/6809) done. Loss: 0.2053  lr:0.010000
[ Fri Jul 12 08:15:41 2024 ] 	Batch(1700/6809) done. Loss: 0.3160  lr:0.010000
[ Fri Jul 12 08:15:59 2024 ] 	Batch(1800/6809) done. Loss: 0.5265  lr:0.010000
[ Fri Jul 12 08:16:17 2024 ] 	Batch(1900/6809) done. Loss: 0.1062  lr:0.010000
[ Fri Jul 12 08:16:35 2024 ] 
Training: Epoch [32/120], Step [1999], Loss: 0.4374167025089264, Training Accuracy: 85.75
[ Fri Jul 12 08:16:35 2024 ] 	Batch(2000/6809) done. Loss: 0.7975  lr:0.010000
[ Fri Jul 12 08:16:53 2024 ] 	Batch(2100/6809) done. Loss: 0.5243  lr:0.010000
[ Fri Jul 12 08:17:11 2024 ] 	Batch(2200/6809) done. Loss: 0.2473  lr:0.010000
[ Fri Jul 12 08:17:30 2024 ] 	Batch(2300/6809) done. Loss: 0.5866  lr:0.010000
[ Fri Jul 12 08:17:49 2024 ] 	Batch(2400/6809) done. Loss: 1.0747  lr:0.010000
[ Fri Jul 12 08:18:06 2024 ] 
Training: Epoch [32/120], Step [2499], Loss: 0.4952203333377838, Training Accuracy: 85.425
[ Fri Jul 12 08:18:07 2024 ] 	Batch(2500/6809) done. Loss: 0.3282  lr:0.010000
[ Fri Jul 12 08:18:25 2024 ] 	Batch(2600/6809) done. Loss: 0.4960  lr:0.010000
[ Fri Jul 12 08:18:43 2024 ] 	Batch(2700/6809) done. Loss: 1.3696  lr:0.010000
[ Fri Jul 12 08:19:01 2024 ] 	Batch(2800/6809) done. Loss: 0.4439  lr:0.010000
[ Fri Jul 12 08:19:20 2024 ] 	Batch(2900/6809) done. Loss: 0.8192  lr:0.010000
[ Fri Jul 12 08:19:38 2024 ] 
Training: Epoch [32/120], Step [2999], Loss: 0.04281458258628845, Training Accuracy: 85.3625
[ Fri Jul 12 08:19:38 2024 ] 	Batch(3000/6809) done. Loss: 0.4160  lr:0.010000
[ Fri Jul 12 08:19:57 2024 ] 	Batch(3100/6809) done. Loss: 0.7389  lr:0.010000
[ Fri Jul 12 08:20:15 2024 ] 	Batch(3200/6809) done. Loss: 0.3977  lr:0.010000
[ Fri Jul 12 08:20:34 2024 ] 	Batch(3300/6809) done. Loss: 0.6087  lr:0.010000
[ Fri Jul 12 08:20:53 2024 ] 	Batch(3400/6809) done. Loss: 0.6190  lr:0.010000
[ Fri Jul 12 08:21:11 2024 ] 
Training: Epoch [32/120], Step [3499], Loss: 0.35647207498550415, Training Accuracy: 85.22142857142858
[ Fri Jul 12 08:21:11 2024 ] 	Batch(3500/6809) done. Loss: 0.7476  lr:0.010000
[ Fri Jul 12 08:21:30 2024 ] 	Batch(3600/6809) done. Loss: 0.1877  lr:0.010000
[ Fri Jul 12 08:21:48 2024 ] 	Batch(3700/6809) done. Loss: 0.6241  lr:0.010000
[ Fri Jul 12 08:22:07 2024 ] 	Batch(3800/6809) done. Loss: 0.3472  lr:0.010000
[ Fri Jul 12 08:22:25 2024 ] 	Batch(3900/6809) done. Loss: 0.0309  lr:0.010000
[ Fri Jul 12 08:22:44 2024 ] 
Training: Epoch [32/120], Step [3999], Loss: 0.1265082210302353, Training Accuracy: 85.278125
[ Fri Jul 12 08:22:44 2024 ] 	Batch(4000/6809) done. Loss: 0.8495  lr:0.010000
[ Fri Jul 12 08:23:03 2024 ] 	Batch(4100/6809) done. Loss: 0.7772  lr:0.010000
[ Fri Jul 12 08:23:21 2024 ] 	Batch(4200/6809) done. Loss: 0.0756  lr:0.010000
[ Fri Jul 12 08:23:40 2024 ] 	Batch(4300/6809) done. Loss: 0.4457  lr:0.010000
[ Fri Jul 12 08:23:58 2024 ] 	Batch(4400/6809) done. Loss: 0.0981  lr:0.010000
[ Fri Jul 12 08:24:16 2024 ] 
Training: Epoch [32/120], Step [4499], Loss: 0.5892397165298462, Training Accuracy: 85.21666666666667
[ Fri Jul 12 08:24:16 2024 ] 	Batch(4500/6809) done. Loss: 0.5345  lr:0.010000
[ Fri Jul 12 08:24:34 2024 ] 	Batch(4600/6809) done. Loss: 0.1626  lr:0.010000
[ Fri Jul 12 08:24:52 2024 ] 	Batch(4700/6809) done. Loss: 0.3469  lr:0.010000
[ Fri Jul 12 08:25:10 2024 ] 	Batch(4800/6809) done. Loss: 1.2613  lr:0.010000
[ Fri Jul 12 08:25:28 2024 ] 	Batch(4900/6809) done. Loss: 0.4470  lr:0.010000
[ Fri Jul 12 08:25:46 2024 ] 
Training: Epoch [32/120], Step [4999], Loss: 0.2862128019332886, Training Accuracy: 85.095
[ Fri Jul 12 08:25:46 2024 ] 	Batch(5000/6809) done. Loss: 0.3049  lr:0.010000
[ Fri Jul 12 08:26:04 2024 ] 	Batch(5100/6809) done. Loss: 0.4643  lr:0.010000
[ Fri Jul 12 08:26:22 2024 ] 	Batch(5200/6809) done. Loss: 0.0823  lr:0.010000
[ Fri Jul 12 08:26:40 2024 ] 	Batch(5300/6809) done. Loss: 0.8740  lr:0.010000
[ Fri Jul 12 08:26:58 2024 ] 	Batch(5400/6809) done. Loss: 0.2147  lr:0.010000
[ Fri Jul 12 08:27:16 2024 ] 
Training: Epoch [32/120], Step [5499], Loss: 0.51078200340271, Training Accuracy: 85.02272727272727
[ Fri Jul 12 08:27:16 2024 ] 	Batch(5500/6809) done. Loss: 0.0600  lr:0.010000
[ Fri Jul 12 08:27:34 2024 ] 	Batch(5600/6809) done. Loss: 0.0239  lr:0.010000
[ Fri Jul 12 08:27:52 2024 ] 	Batch(5700/6809) done. Loss: 0.2353  lr:0.010000
[ Fri Jul 12 08:28:10 2024 ] 	Batch(5800/6809) done. Loss: 0.6530  lr:0.010000
[ Fri Jul 12 08:28:28 2024 ] 	Batch(5900/6809) done. Loss: 0.8043  lr:0.010000
[ Fri Jul 12 08:28:46 2024 ] 
Training: Epoch [32/120], Step [5999], Loss: 1.0275954008102417, Training Accuracy: 84.96041666666667
[ Fri Jul 12 08:28:46 2024 ] 	Batch(6000/6809) done. Loss: 0.4018  lr:0.010000
[ Fri Jul 12 08:29:04 2024 ] 	Batch(6100/6809) done. Loss: 0.5729  lr:0.010000
[ Fri Jul 12 08:29:22 2024 ] 	Batch(6200/6809) done. Loss: 0.1458  lr:0.010000
[ Fri Jul 12 08:29:40 2024 ] 	Batch(6300/6809) done. Loss: 0.3053  lr:0.010000
[ Fri Jul 12 08:29:58 2024 ] 	Batch(6400/6809) done. Loss: 0.0711  lr:0.010000
[ Fri Jul 12 08:30:16 2024 ] 
Training: Epoch [32/120], Step [6499], Loss: 1.7009549140930176, Training Accuracy: 84.91346153846155
[ Fri Jul 12 08:30:16 2024 ] 	Batch(6500/6809) done. Loss: 0.5205  lr:0.010000
[ Fri Jul 12 08:30:34 2024 ] 	Batch(6600/6809) done. Loss: 0.3708  lr:0.010000
[ Fri Jul 12 08:30:52 2024 ] 	Batch(6700/6809) done. Loss: 0.7555  lr:0.010000
[ Fri Jul 12 08:31:10 2024 ] 	Batch(6800/6809) done. Loss: 0.7017  lr:0.010000
[ Fri Jul 12 08:31:11 2024 ] 	Mean training loss: 0.4744.
[ Fri Jul 12 08:31:11 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 08:31:11 2024 ] Training epoch: 34
[ Fri Jul 12 08:31:12 2024 ] 	Batch(0/6809) done. Loss: 0.0466  lr:0.010000
[ Fri Jul 12 08:31:30 2024 ] 	Batch(100/6809) done. Loss: 0.2887  lr:0.010000
[ Fri Jul 12 08:31:48 2024 ] 	Batch(200/6809) done. Loss: 0.4096  lr:0.010000
[ Fri Jul 12 08:32:06 2024 ] 	Batch(300/6809) done. Loss: 0.5381  lr:0.010000
[ Fri Jul 12 08:32:24 2024 ] 	Batch(400/6809) done. Loss: 0.2740  lr:0.010000
[ Fri Jul 12 08:32:41 2024 ] 
Training: Epoch [33/120], Step [499], Loss: 0.5034632682800293, Training Accuracy: 85.8
[ Fri Jul 12 08:32:42 2024 ] 	Batch(500/6809) done. Loss: 0.3948  lr:0.010000
[ Fri Jul 12 08:32:59 2024 ] 	Batch(600/6809) done. Loss: 0.2244  lr:0.010000
[ Fri Jul 12 08:33:17 2024 ] 	Batch(700/6809) done. Loss: 1.0165  lr:0.010000
[ Fri Jul 12 08:33:36 2024 ] 	Batch(800/6809) done. Loss: 0.2501  lr:0.010000
[ Fri Jul 12 08:33:53 2024 ] 	Batch(900/6809) done. Loss: 0.3960  lr:0.010000
[ Fri Jul 12 08:34:11 2024 ] 
Training: Epoch [33/120], Step [999], Loss: 0.36455997824668884, Training Accuracy: 85.975
[ Fri Jul 12 08:34:11 2024 ] 	Batch(1000/6809) done. Loss: 0.4069  lr:0.010000
[ Fri Jul 12 08:34:29 2024 ] 	Batch(1100/6809) done. Loss: 0.1166  lr:0.010000
[ Fri Jul 12 08:34:47 2024 ] 	Batch(1200/6809) done. Loss: 0.4072  lr:0.010000
[ Fri Jul 12 08:35:05 2024 ] 	Batch(1300/6809) done. Loss: 0.3689  lr:0.010000
[ Fri Jul 12 08:35:23 2024 ] 	Batch(1400/6809) done. Loss: 0.7655  lr:0.010000
[ Fri Jul 12 08:35:41 2024 ] 
Training: Epoch [33/120], Step [1499], Loss: 0.2704624831676483, Training Accuracy: 85.9
[ Fri Jul 12 08:35:41 2024 ] 	Batch(1500/6809) done. Loss: 0.4147  lr:0.010000
[ Fri Jul 12 08:35:59 2024 ] 	Batch(1600/6809) done. Loss: 0.4503  lr:0.010000
[ Fri Jul 12 08:36:17 2024 ] 	Batch(1700/6809) done. Loss: 0.6797  lr:0.010000
[ Fri Jul 12 08:36:35 2024 ] 	Batch(1800/6809) done. Loss: 0.7110  lr:0.010000
[ Fri Jul 12 08:36:53 2024 ] 	Batch(1900/6809) done. Loss: 0.6881  lr:0.010000
[ Fri Jul 12 08:37:10 2024 ] 
Training: Epoch [33/120], Step [1999], Loss: 0.4255043566226959, Training Accuracy: 85.8
[ Fri Jul 12 08:37:11 2024 ] 	Batch(2000/6809) done. Loss: 2.6755  lr:0.010000
[ Fri Jul 12 08:37:29 2024 ] 	Batch(2100/6809) done. Loss: 1.1446  lr:0.010000
[ Fri Jul 12 08:37:46 2024 ] 	Batch(2200/6809) done. Loss: 0.3774  lr:0.010000
[ Fri Jul 12 08:38:04 2024 ] 	Batch(2300/6809) done. Loss: 0.0456  lr:0.010000
[ Fri Jul 12 08:38:22 2024 ] 	Batch(2400/6809) done. Loss: 0.5539  lr:0.010000
[ Fri Jul 12 08:38:41 2024 ] 
Training: Epoch [33/120], Step [2499], Loss: 0.4161403477191925, Training Accuracy: 85.665
[ Fri Jul 12 08:38:41 2024 ] 	Batch(2500/6809) done. Loss: 0.2086  lr:0.010000
[ Fri Jul 12 08:38:59 2024 ] 	Batch(2600/6809) done. Loss: 0.3262  lr:0.010000
[ Fri Jul 12 08:39:18 2024 ] 	Batch(2700/6809) done. Loss: 0.4101  lr:0.010000
[ Fri Jul 12 08:39:37 2024 ] 	Batch(2800/6809) done. Loss: 0.9622  lr:0.010000
[ Fri Jul 12 08:39:55 2024 ] 	Batch(2900/6809) done. Loss: 0.3443  lr:0.010000
[ Fri Jul 12 08:40:14 2024 ] 
Training: Epoch [33/120], Step [2999], Loss: 0.07177479565143585, Training Accuracy: 85.55833333333334
[ Fri Jul 12 08:40:14 2024 ] 	Batch(3000/6809) done. Loss: 0.4298  lr:0.010000
[ Fri Jul 12 08:40:32 2024 ] 	Batch(3100/6809) done. Loss: 0.8084  lr:0.010000
[ Fri Jul 12 08:40:51 2024 ] 	Batch(3200/6809) done. Loss: 0.1964  lr:0.010000
[ Fri Jul 12 08:41:09 2024 ] 	Batch(3300/6809) done. Loss: 0.1164  lr:0.010000
[ Fri Jul 12 08:41:28 2024 ] 	Batch(3400/6809) done. Loss: 0.4151  lr:0.010000
[ Fri Jul 12 08:41:46 2024 ] 
Training: Epoch [33/120], Step [3499], Loss: 0.8957186937332153, Training Accuracy: 85.6
[ Fri Jul 12 08:41:47 2024 ] 	Batch(3500/6809) done. Loss: 0.2524  lr:0.010000
[ Fri Jul 12 08:42:05 2024 ] 	Batch(3600/6809) done. Loss: 0.6270  lr:0.010000
[ Fri Jul 12 08:42:24 2024 ] 	Batch(3700/6809) done. Loss: 0.3613  lr:0.010000
[ Fri Jul 12 08:42:42 2024 ] 	Batch(3800/6809) done. Loss: 0.9123  lr:0.010000
[ Fri Jul 12 08:43:01 2024 ] 	Batch(3900/6809) done. Loss: 1.2526  lr:0.010000
[ Fri Jul 12 08:43:19 2024 ] 
Training: Epoch [33/120], Step [3999], Loss: 0.37559378147125244, Training Accuracy: 85.52499999999999
[ Fri Jul 12 08:43:19 2024 ] 	Batch(4000/6809) done. Loss: 0.2655  lr:0.010000
[ Fri Jul 12 08:43:37 2024 ] 	Batch(4100/6809) done. Loss: 0.2108  lr:0.010000
[ Fri Jul 12 08:43:55 2024 ] 	Batch(4200/6809) done. Loss: 0.4343  lr:0.010000
[ Fri Jul 12 08:44:13 2024 ] 	Batch(4300/6809) done. Loss: 0.2880  lr:0.010000
[ Fri Jul 12 08:44:31 2024 ] 	Batch(4400/6809) done. Loss: 1.1295  lr:0.010000
[ Fri Jul 12 08:44:49 2024 ] 
Training: Epoch [33/120], Step [4499], Loss: 0.4240284860134125, Training Accuracy: 85.44166666666668
[ Fri Jul 12 08:44:49 2024 ] 	Batch(4500/6809) done. Loss: 0.4240  lr:0.010000
[ Fri Jul 12 08:45:07 2024 ] 	Batch(4600/6809) done. Loss: 0.6281  lr:0.010000
[ Fri Jul 12 08:45:25 2024 ] 	Batch(4700/6809) done. Loss: 1.1277  lr:0.010000
[ Fri Jul 12 08:45:43 2024 ] 	Batch(4800/6809) done. Loss: 0.6540  lr:0.010000
[ Fri Jul 12 08:46:01 2024 ] 	Batch(4900/6809) done. Loss: 0.3935  lr:0.010000
[ Fri Jul 12 08:46:18 2024 ] 
Training: Epoch [33/120], Step [4999], Loss: 0.7291596531867981, Training Accuracy: 85.295
[ Fri Jul 12 08:46:19 2024 ] 	Batch(5000/6809) done. Loss: 0.3024  lr:0.010000
[ Fri Jul 12 08:46:36 2024 ] 	Batch(5100/6809) done. Loss: 0.5017  lr:0.010000
[ Fri Jul 12 08:46:55 2024 ] 	Batch(5200/6809) done. Loss: 0.4851  lr:0.010000
[ Fri Jul 12 08:47:13 2024 ] 	Batch(5300/6809) done. Loss: 0.0413  lr:0.010000
[ Fri Jul 12 08:47:31 2024 ] 	Batch(5400/6809) done. Loss: 1.6267  lr:0.010000
[ Fri Jul 12 08:47:48 2024 ] 
Training: Epoch [33/120], Step [5499], Loss: 0.2117440402507782, Training Accuracy: 85.35909090909091
[ Fri Jul 12 08:47:48 2024 ] 	Batch(5500/6809) done. Loss: 0.9532  lr:0.010000
[ Fri Jul 12 08:48:07 2024 ] 	Batch(5600/6809) done. Loss: 0.2071  lr:0.010000
[ Fri Jul 12 08:48:24 2024 ] 	Batch(5700/6809) done. Loss: 0.5867  lr:0.010000
[ Fri Jul 12 08:48:42 2024 ] 	Batch(5800/6809) done. Loss: 0.6008  lr:0.010000
[ Fri Jul 12 08:49:00 2024 ] 	Batch(5900/6809) done. Loss: 0.5587  lr:0.010000
[ Fri Jul 12 08:49:18 2024 ] 
Training: Epoch [33/120], Step [5999], Loss: 0.3357028067111969, Training Accuracy: 85.25416666666666
[ Fri Jul 12 08:49:18 2024 ] 	Batch(6000/6809) done. Loss: 0.9300  lr:0.010000
[ Fri Jul 12 08:49:36 2024 ] 	Batch(6100/6809) done. Loss: 1.0636  lr:0.010000
[ Fri Jul 12 08:49:54 2024 ] 	Batch(6200/6809) done. Loss: 1.2163  lr:0.010000
[ Fri Jul 12 08:50:12 2024 ] 	Batch(6300/6809) done. Loss: 0.5228  lr:0.010000
[ Fri Jul 12 08:50:30 2024 ] 	Batch(6400/6809) done. Loss: 0.6151  lr:0.010000
[ Fri Jul 12 08:50:48 2024 ] 
Training: Epoch [33/120], Step [6499], Loss: 0.32283884286880493, Training Accuracy: 85.12115384615385
[ Fri Jul 12 08:50:48 2024 ] 	Batch(6500/6809) done. Loss: 0.3796  lr:0.010000
[ Fri Jul 12 08:51:06 2024 ] 	Batch(6600/6809) done. Loss: 0.9857  lr:0.010000
[ Fri Jul 12 08:51:24 2024 ] 	Batch(6700/6809) done. Loss: 0.0967  lr:0.010000
[ Fri Jul 12 08:51:42 2024 ] 	Batch(6800/6809) done. Loss: 1.1194  lr:0.010000
[ Fri Jul 12 08:51:43 2024 ] 	Mean training loss: 0.4574.
[ Fri Jul 12 08:51:43 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 08:51:43 2024 ] Training epoch: 35
[ Fri Jul 12 08:51:44 2024 ] 	Batch(0/6809) done. Loss: 0.4286  lr:0.010000
[ Fri Jul 12 08:52:02 2024 ] 	Batch(100/6809) done. Loss: 1.2351  lr:0.010000
[ Fri Jul 12 08:52:21 2024 ] 	Batch(200/6809) done. Loss: 0.1883  lr:0.010000
[ Fri Jul 12 08:52:39 2024 ] 	Batch(300/6809) done. Loss: 0.7097  lr:0.010000
[ Fri Jul 12 08:52:57 2024 ] 	Batch(400/6809) done. Loss: 0.4586  lr:0.010000
[ Fri Jul 12 08:53:14 2024 ] 
Training: Epoch [34/120], Step [499], Loss: 0.7512966990470886, Training Accuracy: 85.75
[ Fri Jul 12 08:53:15 2024 ] 	Batch(500/6809) done. Loss: 0.6095  lr:0.010000
[ Fri Jul 12 08:53:32 2024 ] 	Batch(600/6809) done. Loss: 0.2678  lr:0.010000
[ Fri Jul 12 08:53:50 2024 ] 	Batch(700/6809) done. Loss: 0.2654  lr:0.010000
[ Fri Jul 12 08:54:08 2024 ] 	Batch(800/6809) done. Loss: 0.8499  lr:0.010000
[ Fri Jul 12 08:54:26 2024 ] 	Batch(900/6809) done. Loss: 0.3505  lr:0.010000
[ Fri Jul 12 08:54:44 2024 ] 
Training: Epoch [34/120], Step [999], Loss: 0.6699074506759644, Training Accuracy: 86.075
[ Fri Jul 12 08:54:44 2024 ] 	Batch(1000/6809) done. Loss: 0.2326  lr:0.010000
[ Fri Jul 12 08:55:02 2024 ] 	Batch(1100/6809) done. Loss: 0.3315  lr:0.010000
[ Fri Jul 12 08:55:20 2024 ] 	Batch(1200/6809) done. Loss: 0.7633  lr:0.010000
[ Fri Jul 12 08:55:39 2024 ] 	Batch(1300/6809) done. Loss: 0.1488  lr:0.010000
[ Fri Jul 12 08:55:58 2024 ] 	Batch(1400/6809) done. Loss: 0.6734  lr:0.010000
[ Fri Jul 12 08:56:16 2024 ] 
Training: Epoch [34/120], Step [1499], Loss: 0.07222654670476913, Training Accuracy: 85.96666666666667
[ Fri Jul 12 08:56:16 2024 ] 	Batch(1500/6809) done. Loss: 0.3731  lr:0.010000
[ Fri Jul 12 08:56:35 2024 ] 	Batch(1600/6809) done. Loss: 0.1111  lr:0.010000
[ Fri Jul 12 08:56:53 2024 ] 	Batch(1700/6809) done. Loss: 0.3343  lr:0.010000
[ Fri Jul 12 08:57:11 2024 ] 	Batch(1800/6809) done. Loss: 0.0523  lr:0.010000
[ Fri Jul 12 08:57:29 2024 ] 	Batch(1900/6809) done. Loss: 0.3597  lr:0.010000
[ Fri Jul 12 08:57:46 2024 ] 
Training: Epoch [34/120], Step [1999], Loss: 0.5362945795059204, Training Accuracy: 85.99375
[ Fri Jul 12 08:57:47 2024 ] 	Batch(2000/6809) done. Loss: 0.2164  lr:0.010000
[ Fri Jul 12 08:58:05 2024 ] 	Batch(2100/6809) done. Loss: 0.2106  lr:0.010000
[ Fri Jul 12 08:58:23 2024 ] 	Batch(2200/6809) done. Loss: 0.4731  lr:0.010000
[ Fri Jul 12 08:58:41 2024 ] 	Batch(2300/6809) done. Loss: 0.1166  lr:0.010000
[ Fri Jul 12 08:58:59 2024 ] 	Batch(2400/6809) done. Loss: 0.3101  lr:0.010000
[ Fri Jul 12 08:59:16 2024 ] 
Training: Epoch [34/120], Step [2499], Loss: 0.15739482641220093, Training Accuracy: 85.91499999999999
[ Fri Jul 12 08:59:17 2024 ] 	Batch(2500/6809) done. Loss: 0.0764  lr:0.010000
[ Fri Jul 12 08:59:34 2024 ] 	Batch(2600/6809) done. Loss: 0.3041  lr:0.010000
[ Fri Jul 12 08:59:52 2024 ] 	Batch(2700/6809) done. Loss: 0.3640  lr:0.010000
[ Fri Jul 12 09:00:10 2024 ] 	Batch(2800/6809) done. Loss: 0.3681  lr:0.010000
[ Fri Jul 12 09:00:28 2024 ] 	Batch(2900/6809) done. Loss: 0.8715  lr:0.010000
[ Fri Jul 12 09:00:46 2024 ] 
Training: Epoch [34/120], Step [2999], Loss: 0.6832697987556458, Training Accuracy: 85.8875
[ Fri Jul 12 09:00:46 2024 ] 	Batch(3000/6809) done. Loss: 0.9525  lr:0.010000
[ Fri Jul 12 09:01:04 2024 ] 	Batch(3100/6809) done. Loss: 0.2692  lr:0.010000
[ Fri Jul 12 09:01:22 2024 ] 	Batch(3200/6809) done. Loss: 0.3244  lr:0.010000
[ Fri Jul 12 09:01:41 2024 ] 	Batch(3300/6809) done. Loss: 0.1309  lr:0.010000
[ Fri Jul 12 09:01:59 2024 ] 	Batch(3400/6809) done. Loss: 0.1893  lr:0.010000
[ Fri Jul 12 09:02:18 2024 ] 
Training: Epoch [34/120], Step [3499], Loss: 0.5099201202392578, Training Accuracy: 85.78571428571429
[ Fri Jul 12 09:02:18 2024 ] 	Batch(3500/6809) done. Loss: 1.5588  lr:0.010000
[ Fri Jul 12 09:02:36 2024 ] 	Batch(3600/6809) done. Loss: 1.1948  lr:0.010000
[ Fri Jul 12 09:02:55 2024 ] 	Batch(3700/6809) done. Loss: 0.3481  lr:0.010000
[ Fri Jul 12 09:03:13 2024 ] 	Batch(3800/6809) done. Loss: 1.3430  lr:0.010000
[ Fri Jul 12 09:03:32 2024 ] 	Batch(3900/6809) done. Loss: 0.7383  lr:0.010000
[ Fri Jul 12 09:03:50 2024 ] 
Training: Epoch [34/120], Step [3999], Loss: 0.29566216468811035, Training Accuracy: 85.63437499999999
[ Fri Jul 12 09:03:50 2024 ] 	Batch(4000/6809) done. Loss: 0.5706  lr:0.010000
[ Fri Jul 12 09:04:08 2024 ] 	Batch(4100/6809) done. Loss: 0.4323  lr:0.010000
[ Fri Jul 12 09:04:26 2024 ] 	Batch(4200/6809) done. Loss: 0.3419  lr:0.010000
[ Fri Jul 12 09:04:44 2024 ] 	Batch(4300/6809) done. Loss: 0.1794  lr:0.010000
[ Fri Jul 12 09:05:02 2024 ] 	Batch(4400/6809) done. Loss: 0.6635  lr:0.010000
[ Fri Jul 12 09:05:20 2024 ] 
Training: Epoch [34/120], Step [4499], Loss: 0.37169894576072693, Training Accuracy: 85.58333333333333
[ Fri Jul 12 09:05:20 2024 ] 	Batch(4500/6809) done. Loss: 0.6850  lr:0.010000
[ Fri Jul 12 09:05:38 2024 ] 	Batch(4600/6809) done. Loss: 0.6485  lr:0.010000
[ Fri Jul 12 09:05:56 2024 ] 	Batch(4700/6809) done. Loss: 0.5774  lr:0.010000
[ Fri Jul 12 09:06:14 2024 ] 	Batch(4800/6809) done. Loss: 0.7169  lr:0.010000
[ Fri Jul 12 09:06:32 2024 ] 	Batch(4900/6809) done. Loss: 0.1767  lr:0.010000
[ Fri Jul 12 09:06:49 2024 ] 
Training: Epoch [34/120], Step [4999], Loss: 0.2989290654659271, Training Accuracy: 85.36500000000001
[ Fri Jul 12 09:06:49 2024 ] 	Batch(5000/6809) done. Loss: 0.0194  lr:0.010000
[ Fri Jul 12 09:07:07 2024 ] 	Batch(5100/6809) done. Loss: 0.9687  lr:0.010000
[ Fri Jul 12 09:07:25 2024 ] 	Batch(5200/6809) done. Loss: 0.0820  lr:0.010000
[ Fri Jul 12 09:07:43 2024 ] 	Batch(5300/6809) done. Loss: 0.1795  lr:0.010000
[ Fri Jul 12 09:08:01 2024 ] 	Batch(5400/6809) done. Loss: 0.7909  lr:0.010000
[ Fri Jul 12 09:08:19 2024 ] 
Training: Epoch [34/120], Step [5499], Loss: 0.12256679683923721, Training Accuracy: 85.28636363636363
[ Fri Jul 12 09:08:19 2024 ] 	Batch(5500/6809) done. Loss: 0.3812  lr:0.010000
[ Fri Jul 12 09:08:37 2024 ] 	Batch(5600/6809) done. Loss: 0.7589  lr:0.010000
[ Fri Jul 12 09:08:55 2024 ] 	Batch(5700/6809) done. Loss: 0.2891  lr:0.010000
[ Fri Jul 12 09:09:13 2024 ] 	Batch(5800/6809) done. Loss: 0.3862  lr:0.010000
[ Fri Jul 12 09:09:31 2024 ] 	Batch(5900/6809) done. Loss: 0.4839  lr:0.010000
[ Fri Jul 12 09:09:48 2024 ] 
Training: Epoch [34/120], Step [5999], Loss: 0.5940713882446289, Training Accuracy: 85.21666666666667
[ Fri Jul 12 09:09:49 2024 ] 	Batch(6000/6809) done. Loss: 0.4199  lr:0.010000
[ Fri Jul 12 09:10:07 2024 ] 	Batch(6100/6809) done. Loss: 0.0977  lr:0.010000
[ Fri Jul 12 09:10:26 2024 ] 	Batch(6200/6809) done. Loss: 0.9391  lr:0.010000
[ Fri Jul 12 09:10:44 2024 ] 	Batch(6300/6809) done. Loss: 0.1477  lr:0.010000
[ Fri Jul 12 09:11:03 2024 ] 	Batch(6400/6809) done. Loss: 0.5565  lr:0.010000
[ Fri Jul 12 09:11:21 2024 ] 
Training: Epoch [34/120], Step [6499], Loss: 0.07847423851490021, Training Accuracy: 85.20769230769231
[ Fri Jul 12 09:11:22 2024 ] 	Batch(6500/6809) done. Loss: 0.2091  lr:0.010000
[ Fri Jul 12 09:11:40 2024 ] 	Batch(6600/6809) done. Loss: 0.2897  lr:0.010000
[ Fri Jul 12 09:11:59 2024 ] 	Batch(6700/6809) done. Loss: 0.9227  lr:0.010000
[ Fri Jul 12 09:12:17 2024 ] 	Batch(6800/6809) done. Loss: 0.8533  lr:0.010000
[ Fri Jul 12 09:12:19 2024 ] 	Mean training loss: 0.4592.
[ Fri Jul 12 09:12:19 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 09:12:19 2024 ] Training epoch: 36
[ Fri Jul 12 09:12:20 2024 ] 	Batch(0/6809) done. Loss: 0.3461  lr:0.010000
[ Fri Jul 12 09:12:37 2024 ] 	Batch(100/6809) done. Loss: 0.4594  lr:0.010000
[ Fri Jul 12 09:12:55 2024 ] 	Batch(200/6809) done. Loss: 0.3169  lr:0.010000
[ Fri Jul 12 09:13:13 2024 ] 	Batch(300/6809) done. Loss: 1.9769  lr:0.010000
[ Fri Jul 12 09:13:31 2024 ] 	Batch(400/6809) done. Loss: 0.0695  lr:0.010000
[ Fri Jul 12 09:13:49 2024 ] 
Training: Epoch [35/120], Step [499], Loss: 0.3714928925037384, Training Accuracy: 86.575
[ Fri Jul 12 09:13:49 2024 ] 	Batch(500/6809) done. Loss: 1.2594  lr:0.010000
[ Fri Jul 12 09:14:07 2024 ] 	Batch(600/6809) done. Loss: 0.2803  lr:0.010000
[ Fri Jul 12 09:14:25 2024 ] 	Batch(700/6809) done. Loss: 0.0126  lr:0.010000
[ Fri Jul 12 09:14:43 2024 ] 	Batch(800/6809) done. Loss: 0.3301  lr:0.010000
[ Fri Jul 12 09:15:01 2024 ] 	Batch(900/6809) done. Loss: 0.5593  lr:0.010000
[ Fri Jul 12 09:15:18 2024 ] 
Training: Epoch [35/120], Step [999], Loss: 0.48505809903144836, Training Accuracy: 86.35000000000001
[ Fri Jul 12 09:15:19 2024 ] 	Batch(1000/6809) done. Loss: 0.2637  lr:0.010000
[ Fri Jul 12 09:15:37 2024 ] 	Batch(1100/6809) done. Loss: 0.3661  lr:0.010000
[ Fri Jul 12 09:15:55 2024 ] 	Batch(1200/6809) done. Loss: 0.6124  lr:0.010000
[ Fri Jul 12 09:16:12 2024 ] 	Batch(1300/6809) done. Loss: 0.3889  lr:0.010000
[ Fri Jul 12 09:16:30 2024 ] 	Batch(1400/6809) done. Loss: 0.5093  lr:0.010000
[ Fri Jul 12 09:16:48 2024 ] 
Training: Epoch [35/120], Step [1499], Loss: 0.204514279961586, Training Accuracy: 86.31666666666666
[ Fri Jul 12 09:16:48 2024 ] 	Batch(1500/6809) done. Loss: 0.2370  lr:0.010000
[ Fri Jul 12 09:17:06 2024 ] 	Batch(1600/6809) done. Loss: 0.3284  lr:0.010000
[ Fri Jul 12 09:17:24 2024 ] 	Batch(1700/6809) done. Loss: 0.1045  lr:0.010000
[ Fri Jul 12 09:17:42 2024 ] 	Batch(1800/6809) done. Loss: 1.3154  lr:0.010000
[ Fri Jul 12 09:18:00 2024 ] 	Batch(1900/6809) done. Loss: 0.6954  lr:0.010000
[ Fri Jul 12 09:18:18 2024 ] 
Training: Epoch [35/120], Step [1999], Loss: 0.7877179384231567, Training Accuracy: 86.275
[ Fri Jul 12 09:18:18 2024 ] 	Batch(2000/6809) done. Loss: 0.6198  lr:0.010000
[ Fri Jul 12 09:18:36 2024 ] 	Batch(2100/6809) done. Loss: 0.0261  lr:0.010000
[ Fri Jul 12 09:18:54 2024 ] 	Batch(2200/6809) done. Loss: 0.2651  lr:0.010000
[ Fri Jul 12 09:19:12 2024 ] 	Batch(2300/6809) done. Loss: 0.2510  lr:0.010000
[ Fri Jul 12 09:19:30 2024 ] 	Batch(2400/6809) done. Loss: 0.4536  lr:0.010000
[ Fri Jul 12 09:19:48 2024 ] 
Training: Epoch [35/120], Step [2499], Loss: 0.35918739438056946, Training Accuracy: 86.04
[ Fri Jul 12 09:19:48 2024 ] 	Batch(2500/6809) done. Loss: 0.2735  lr:0.010000
[ Fri Jul 12 09:20:06 2024 ] 	Batch(2600/6809) done. Loss: 0.8558  lr:0.010000
[ Fri Jul 12 09:20:24 2024 ] 	Batch(2700/6809) done. Loss: 0.1883  lr:0.010000
[ Fri Jul 12 09:20:42 2024 ] 	Batch(2800/6809) done. Loss: 0.5558  lr:0.010000
[ Fri Jul 12 09:21:00 2024 ] 	Batch(2900/6809) done. Loss: 0.0373  lr:0.010000
[ Fri Jul 12 09:21:17 2024 ] 
Training: Epoch [35/120], Step [2999], Loss: 0.5693941116333008, Training Accuracy: 86.08333333333333
[ Fri Jul 12 09:21:17 2024 ] 	Batch(3000/6809) done. Loss: 0.3877  lr:0.010000
[ Fri Jul 12 09:21:35 2024 ] 	Batch(3100/6809) done. Loss: 0.5707  lr:0.010000
[ Fri Jul 12 09:21:53 2024 ] 	Batch(3200/6809) done. Loss: 0.4327  lr:0.010000
[ Fri Jul 12 09:22:12 2024 ] 	Batch(3300/6809) done. Loss: 0.6932  lr:0.010000
[ Fri Jul 12 09:22:31 2024 ] 	Batch(3400/6809) done. Loss: 0.0442  lr:0.010000
[ Fri Jul 12 09:22:49 2024 ] 
Training: Epoch [35/120], Step [3499], Loss: 0.52171790599823, Training Accuracy: 86.01785714285714
[ Fri Jul 12 09:22:49 2024 ] 	Batch(3500/6809) done. Loss: 0.1229  lr:0.010000
[ Fri Jul 12 09:23:08 2024 ] 	Batch(3600/6809) done. Loss: 0.4137  lr:0.010000
[ Fri Jul 12 09:23:26 2024 ] 	Batch(3700/6809) done. Loss: 0.6786  lr:0.010000
[ Fri Jul 12 09:23:44 2024 ] 	Batch(3800/6809) done. Loss: 0.8272  lr:0.010000
[ Fri Jul 12 09:24:01 2024 ] 	Batch(3900/6809) done. Loss: 0.7670  lr:0.010000
[ Fri Jul 12 09:24:19 2024 ] 
Training: Epoch [35/120], Step [3999], Loss: 0.10577405244112015, Training Accuracy: 85.8
[ Fri Jul 12 09:24:19 2024 ] 	Batch(4000/6809) done. Loss: 0.2772  lr:0.010000
[ Fri Jul 12 09:24:37 2024 ] 	Batch(4100/6809) done. Loss: 0.6192  lr:0.010000
[ Fri Jul 12 09:24:55 2024 ] 	Batch(4200/6809) done. Loss: 0.3567  lr:0.010000
[ Fri Jul 12 09:25:13 2024 ] 	Batch(4300/6809) done. Loss: 0.4192  lr:0.010000
[ Fri Jul 12 09:25:31 2024 ] 	Batch(4400/6809) done. Loss: 0.1555  lr:0.010000
[ Fri Jul 12 09:25:49 2024 ] 
Training: Epoch [35/120], Step [4499], Loss: 1.2206426858901978, Training Accuracy: 85.79444444444444
[ Fri Jul 12 09:25:49 2024 ] 	Batch(4500/6809) done. Loss: 0.6227  lr:0.010000
[ Fri Jul 12 09:26:07 2024 ] 	Batch(4600/6809) done. Loss: 0.3648  lr:0.010000
[ Fri Jul 12 09:26:25 2024 ] 	Batch(4700/6809) done. Loss: 0.1623  lr:0.010000
[ Fri Jul 12 09:26:43 2024 ] 	Batch(4800/6809) done. Loss: 0.4328  lr:0.010000
[ Fri Jul 12 09:27:01 2024 ] 	Batch(4900/6809) done. Loss: 0.5229  lr:0.010000
[ Fri Jul 12 09:27:18 2024 ] 
Training: Epoch [35/120], Step [4999], Loss: 0.2456999272108078, Training Accuracy: 85.705
[ Fri Jul 12 09:27:19 2024 ] 	Batch(5000/6809) done. Loss: 0.8255  lr:0.010000
[ Fri Jul 12 09:27:36 2024 ] 	Batch(5100/6809) done. Loss: 0.8974  lr:0.010000
[ Fri Jul 12 09:27:55 2024 ] 	Batch(5200/6809) done. Loss: 0.0985  lr:0.010000
[ Fri Jul 12 09:28:12 2024 ] 	Batch(5300/6809) done. Loss: 0.1573  lr:0.010000
[ Fri Jul 12 09:28:30 2024 ] 	Batch(5400/6809) done. Loss: 0.4203  lr:0.010000
[ Fri Jul 12 09:28:48 2024 ] 
Training: Epoch [35/120], Step [5499], Loss: 0.530735433101654, Training Accuracy: 85.66818181818182
[ Fri Jul 12 09:28:48 2024 ] 	Batch(5500/6809) done. Loss: 0.7900  lr:0.010000
[ Fri Jul 12 09:29:06 2024 ] 	Batch(5600/6809) done. Loss: 0.7428  lr:0.010000
[ Fri Jul 12 09:29:25 2024 ] 	Batch(5700/6809) done. Loss: 0.2461  lr:0.010000
[ Fri Jul 12 09:29:43 2024 ] 	Batch(5800/6809) done. Loss: 0.8185  lr:0.010000
[ Fri Jul 12 09:30:02 2024 ] 	Batch(5900/6809) done. Loss: 0.7154  lr:0.010000
[ Fri Jul 12 09:30:20 2024 ] 
Training: Epoch [35/120], Step [5999], Loss: 0.2614724338054657, Training Accuracy: 85.6625
[ Fri Jul 12 09:30:20 2024 ] 	Batch(6000/6809) done. Loss: 0.2467  lr:0.010000
[ Fri Jul 12 09:30:39 2024 ] 	Batch(6100/6809) done. Loss: 0.7302  lr:0.010000
[ Fri Jul 12 09:30:57 2024 ] 	Batch(6200/6809) done. Loss: 0.1603  lr:0.010000
[ Fri Jul 12 09:31:15 2024 ] 	Batch(6300/6809) done. Loss: 0.1395  lr:0.010000
[ Fri Jul 12 09:31:33 2024 ] 	Batch(6400/6809) done. Loss: 0.2267  lr:0.010000
[ Fri Jul 12 09:31:51 2024 ] 
Training: Epoch [35/120], Step [6499], Loss: 0.25904586911201477, Training Accuracy: 85.61730769230769
[ Fri Jul 12 09:31:51 2024 ] 	Batch(6500/6809) done. Loss: 0.2411  lr:0.010000
[ Fri Jul 12 09:32:09 2024 ] 	Batch(6600/6809) done. Loss: 0.0868  lr:0.010000
[ Fri Jul 12 09:32:27 2024 ] 	Batch(6700/6809) done. Loss: 0.6755  lr:0.010000
[ Fri Jul 12 09:32:45 2024 ] 	Batch(6800/6809) done. Loss: 0.0584  lr:0.010000
[ Fri Jul 12 09:32:46 2024 ] 	Mean training loss: 0.4601.
[ Fri Jul 12 09:32:46 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 09:32:46 2024 ] Training epoch: 37
[ Fri Jul 12 09:32:47 2024 ] 	Batch(0/6809) done. Loss: 0.4642  lr:0.010000
[ Fri Jul 12 09:33:05 2024 ] 	Batch(100/6809) done. Loss: 0.1086  lr:0.010000
[ Fri Jul 12 09:33:23 2024 ] 	Batch(200/6809) done. Loss: 0.2539  lr:0.010000
[ Fri Jul 12 09:33:41 2024 ] 	Batch(300/6809) done. Loss: 0.0934  lr:0.010000
[ Fri Jul 12 09:33:59 2024 ] 	Batch(400/6809) done. Loss: 0.7009  lr:0.010000
[ Fri Jul 12 09:34:17 2024 ] 
Training: Epoch [36/120], Step [499], Loss: 0.6713149547576904, Training Accuracy: 88.2
[ Fri Jul 12 09:34:17 2024 ] 	Batch(500/6809) done. Loss: 0.0811  lr:0.010000
[ Fri Jul 12 09:34:35 2024 ] 	Batch(600/6809) done. Loss: 0.7163  lr:0.010000
[ Fri Jul 12 09:34:53 2024 ] 	Batch(700/6809) done. Loss: 0.6697  lr:0.010000
[ Fri Jul 12 09:35:11 2024 ] 	Batch(800/6809) done. Loss: 0.3646  lr:0.010000
[ Fri Jul 12 09:35:29 2024 ] 	Batch(900/6809) done. Loss: 0.7930  lr:0.010000
[ Fri Jul 12 09:35:46 2024 ] 
Training: Epoch [36/120], Step [999], Loss: 1.2660136222839355, Training Accuracy: 87.47500000000001
[ Fri Jul 12 09:35:46 2024 ] 	Batch(1000/6809) done. Loss: 0.1094  lr:0.010000
[ Fri Jul 12 09:36:04 2024 ] 	Batch(1100/6809) done. Loss: 0.3887  lr:0.010000
[ Fri Jul 12 09:36:22 2024 ] 	Batch(1200/6809) done. Loss: 0.0963  lr:0.010000
[ Fri Jul 12 09:36:41 2024 ] 	Batch(1300/6809) done. Loss: 0.3355  lr:0.010000
[ Fri Jul 12 09:36:59 2024 ] 	Batch(1400/6809) done. Loss: 0.4583  lr:0.010000
[ Fri Jul 12 09:37:17 2024 ] 
Training: Epoch [36/120], Step [1499], Loss: 0.36950916051864624, Training Accuracy: 87.10833333333333
[ Fri Jul 12 09:37:17 2024 ] 	Batch(1500/6809) done. Loss: 0.3312  lr:0.010000
[ Fri Jul 12 09:37:35 2024 ] 	Batch(1600/6809) done. Loss: 0.8782  lr:0.010000
[ Fri Jul 12 09:37:53 2024 ] 	Batch(1700/6809) done. Loss: 0.2556  lr:0.010000
[ Fri Jul 12 09:38:11 2024 ] 	Batch(1800/6809) done. Loss: 0.2947  lr:0.010000
[ Fri Jul 12 09:38:29 2024 ] 	Batch(1900/6809) done. Loss: 0.6339  lr:0.010000
[ Fri Jul 12 09:38:47 2024 ] 
Training: Epoch [36/120], Step [1999], Loss: 0.12673595547676086, Training Accuracy: 86.85625
[ Fri Jul 12 09:38:47 2024 ] 	Batch(2000/6809) done. Loss: 0.1220  lr:0.010000
[ Fri Jul 12 09:39:05 2024 ] 	Batch(2100/6809) done. Loss: 0.7174  lr:0.010000
[ Fri Jul 12 09:39:23 2024 ] 	Batch(2200/6809) done. Loss: 0.0471  lr:0.010000
[ Fri Jul 12 09:39:41 2024 ] 	Batch(2300/6809) done. Loss: 1.1000  lr:0.010000
[ Fri Jul 12 09:39:59 2024 ] 	Batch(2400/6809) done. Loss: 0.3800  lr:0.010000
[ Fri Jul 12 09:40:17 2024 ] 
Training: Epoch [36/120], Step [2499], Loss: 1.032891035079956, Training Accuracy: 86.72999999999999
[ Fri Jul 12 09:40:18 2024 ] 	Batch(2500/6809) done. Loss: 0.0136  lr:0.010000
[ Fri Jul 12 09:40:36 2024 ] 	Batch(2600/6809) done. Loss: 0.2149  lr:0.010000
[ Fri Jul 12 09:40:55 2024 ] 	Batch(2700/6809) done. Loss: 0.5663  lr:0.010000
[ Fri Jul 12 09:41:13 2024 ] 	Batch(2800/6809) done. Loss: 0.2433  lr:0.010000
[ Fri Jul 12 09:41:32 2024 ] 	Batch(2900/6809) done. Loss: 0.8210  lr:0.010000
[ Fri Jul 12 09:41:50 2024 ] 
Training: Epoch [36/120], Step [2999], Loss: 0.5869097709655762, Training Accuracy: 86.59583333333333
[ Fri Jul 12 09:41:50 2024 ] 	Batch(3000/6809) done. Loss: 0.7043  lr:0.010000
[ Fri Jul 12 09:42:09 2024 ] 	Batch(3100/6809) done. Loss: 0.1683  lr:0.010000
[ Fri Jul 12 09:42:28 2024 ] 	Batch(3200/6809) done. Loss: 0.1967  lr:0.010000
[ Fri Jul 12 09:42:46 2024 ] 	Batch(3300/6809) done. Loss: 0.3069  lr:0.010000
[ Fri Jul 12 09:43:04 2024 ] 	Batch(3400/6809) done. Loss: 1.7023  lr:0.010000
[ Fri Jul 12 09:43:22 2024 ] 
Training: Epoch [36/120], Step [3499], Loss: 0.285330206155777, Training Accuracy: 86.51071428571429
[ Fri Jul 12 09:43:22 2024 ] 	Batch(3500/6809) done. Loss: 0.9819  lr:0.010000
[ Fri Jul 12 09:43:40 2024 ] 	Batch(3600/6809) done. Loss: 0.8787  lr:0.010000
[ Fri Jul 12 09:43:58 2024 ] 	Batch(3700/6809) done. Loss: 0.0810  lr:0.010000
[ Fri Jul 12 09:44:16 2024 ] 	Batch(3800/6809) done. Loss: 0.1547  lr:0.010000
[ Fri Jul 12 09:44:33 2024 ] 	Batch(3900/6809) done. Loss: 0.1893  lr:0.010000
[ Fri Jul 12 09:44:51 2024 ] 
Training: Epoch [36/120], Step [3999], Loss: 0.2837280333042145, Training Accuracy: 86.39375
[ Fri Jul 12 09:44:51 2024 ] 	Batch(4000/6809) done. Loss: 0.4124  lr:0.010000
[ Fri Jul 12 09:45:09 2024 ] 	Batch(4100/6809) done. Loss: 0.5189  lr:0.010000
[ Fri Jul 12 09:45:27 2024 ] 	Batch(4200/6809) done. Loss: 0.3426  lr:0.010000
[ Fri Jul 12 09:45:45 2024 ] 	Batch(4300/6809) done. Loss: 0.0884  lr:0.010000
[ Fri Jul 12 09:46:03 2024 ] 	Batch(4400/6809) done. Loss: 0.2022  lr:0.010000
[ Fri Jul 12 09:46:21 2024 ] 
Training: Epoch [36/120], Step [4499], Loss: 1.10335373878479, Training Accuracy: 86.1888888888889
[ Fri Jul 12 09:46:21 2024 ] 	Batch(4500/6809) done. Loss: 0.0764  lr:0.010000
[ Fri Jul 12 09:46:39 2024 ] 	Batch(4600/6809) done. Loss: 0.0817  lr:0.010000
[ Fri Jul 12 09:46:57 2024 ] 	Batch(4700/6809) done. Loss: 0.2191  lr:0.010000
[ Fri Jul 12 09:47:15 2024 ] 	Batch(4800/6809) done. Loss: 0.3058  lr:0.010000
[ Fri Jul 12 09:47:33 2024 ] 	Batch(4900/6809) done. Loss: 0.0107  lr:0.010000
[ Fri Jul 12 09:47:51 2024 ] 
Training: Epoch [36/120], Step [4999], Loss: 0.4087863862514496, Training Accuracy: 85.975
[ Fri Jul 12 09:47:51 2024 ] 	Batch(5000/6809) done. Loss: 0.0916  lr:0.010000
[ Fri Jul 12 09:48:10 2024 ] 	Batch(5100/6809) done. Loss: 1.4404  lr:0.010000
[ Fri Jul 12 09:48:28 2024 ] 	Batch(5200/6809) done. Loss: 0.1221  lr:0.010000
[ Fri Jul 12 09:48:46 2024 ] 	Batch(5300/6809) done. Loss: 0.7913  lr:0.010000
[ Fri Jul 12 09:49:05 2024 ] 	Batch(5400/6809) done. Loss: 1.1977  lr:0.010000
[ Fri Jul 12 09:49:23 2024 ] 
Training: Epoch [36/120], Step [5499], Loss: 0.285796582698822, Training Accuracy: 85.98636363636363
[ Fri Jul 12 09:49:24 2024 ] 	Batch(5500/6809) done. Loss: 0.7256  lr:0.010000
[ Fri Jul 12 09:49:42 2024 ] 	Batch(5600/6809) done. Loss: 0.8368  lr:0.010000
[ Fri Jul 12 09:50:01 2024 ] 	Batch(5700/6809) done. Loss: 0.2055  lr:0.010000
[ Fri Jul 12 09:50:19 2024 ] 	Batch(5800/6809) done. Loss: 0.7211  lr:0.010000
[ Fri Jul 12 09:50:37 2024 ] 	Batch(5900/6809) done. Loss: 0.3346  lr:0.010000
[ Fri Jul 12 09:50:55 2024 ] 
Training: Epoch [36/120], Step [5999], Loss: 0.7327415943145752, Training Accuracy: 85.95416666666667
[ Fri Jul 12 09:50:55 2024 ] 	Batch(6000/6809) done. Loss: 0.9086  lr:0.010000
[ Fri Jul 12 09:51:13 2024 ] 	Batch(6100/6809) done. Loss: 0.3070  lr:0.010000
[ Fri Jul 12 09:51:31 2024 ] 	Batch(6200/6809) done. Loss: 0.0618  lr:0.010000
[ Fri Jul 12 09:51:49 2024 ] 	Batch(6300/6809) done. Loss: 0.3278  lr:0.010000
[ Fri Jul 12 09:52:07 2024 ] 	Batch(6400/6809) done. Loss: 0.3030  lr:0.010000
[ Fri Jul 12 09:52:24 2024 ] 
Training: Epoch [36/120], Step [6499], Loss: 0.8987554311752319, Training Accuracy: 85.86730769230769
[ Fri Jul 12 09:52:25 2024 ] 	Batch(6500/6809) done. Loss: 0.4933  lr:0.010000
[ Fri Jul 12 09:52:42 2024 ] 	Batch(6600/6809) done. Loss: 0.2386  lr:0.010000
[ Fri Jul 12 09:53:00 2024 ] 	Batch(6700/6809) done. Loss: 0.0134  lr:0.010000
[ Fri Jul 12 09:53:18 2024 ] 	Batch(6800/6809) done. Loss: 0.6779  lr:0.010000
[ Fri Jul 12 09:53:20 2024 ] 	Mean training loss: 0.4473.
[ Fri Jul 12 09:53:20 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 09:53:20 2024 ] Training epoch: 38
[ Fri Jul 12 09:53:21 2024 ] 	Batch(0/6809) done. Loss: 0.7326  lr:0.010000
[ Fri Jul 12 09:53:39 2024 ] 	Batch(100/6809) done. Loss: 0.7331  lr:0.010000
[ Fri Jul 12 09:53:57 2024 ] 	Batch(200/6809) done. Loss: 0.1983  lr:0.010000
[ Fri Jul 12 09:54:15 2024 ] 	Batch(300/6809) done. Loss: 0.3199  lr:0.010000
[ Fri Jul 12 09:54:34 2024 ] 	Batch(400/6809) done. Loss: 0.0874  lr:0.010000
[ Fri Jul 12 09:54:52 2024 ] 
Training: Epoch [37/120], Step [499], Loss: 0.2376376986503601, Training Accuracy: 86.3
[ Fri Jul 12 09:54:52 2024 ] 	Batch(500/6809) done. Loss: 0.2673  lr:0.010000
[ Fri Jul 12 09:55:10 2024 ] 	Batch(600/6809) done. Loss: 0.2869  lr:0.010000
[ Fri Jul 12 09:55:29 2024 ] 	Batch(700/6809) done. Loss: 0.2508  lr:0.010000
[ Fri Jul 12 09:55:47 2024 ] 	Batch(800/6809) done. Loss: 0.9260  lr:0.010000
[ Fri Jul 12 09:56:05 2024 ] 	Batch(900/6809) done. Loss: 0.4579  lr:0.010000
[ Fri Jul 12 09:56:24 2024 ] 
Training: Epoch [37/120], Step [999], Loss: 0.9103221297264099, Training Accuracy: 86.45
[ Fri Jul 12 09:56:24 2024 ] 	Batch(1000/6809) done. Loss: 0.5738  lr:0.010000
[ Fri Jul 12 09:56:42 2024 ] 	Batch(1100/6809) done. Loss: 0.5116  lr:0.010000
[ Fri Jul 12 09:57:00 2024 ] 	Batch(1200/6809) done. Loss: 0.2909  lr:0.010000
[ Fri Jul 12 09:57:18 2024 ] 	Batch(1300/6809) done. Loss: 0.6020  lr:0.010000
[ Fri Jul 12 09:57:36 2024 ] 	Batch(1400/6809) done. Loss: 0.3419  lr:0.010000
[ Fri Jul 12 09:57:54 2024 ] 
Training: Epoch [37/120], Step [1499], Loss: 0.5308904647827148, Training Accuracy: 86.53333333333333
[ Fri Jul 12 09:57:54 2024 ] 	Batch(1500/6809) done. Loss: 0.2608  lr:0.010000
[ Fri Jul 12 09:58:12 2024 ] 	Batch(1600/6809) done. Loss: 0.1158  lr:0.010000
[ Fri Jul 12 09:58:30 2024 ] 	Batch(1700/6809) done. Loss: 0.7716  lr:0.010000
[ Fri Jul 12 09:58:49 2024 ] 	Batch(1800/6809) done. Loss: 1.1018  lr:0.010000
[ Fri Jul 12 09:59:07 2024 ] 	Batch(1900/6809) done. Loss: 0.2036  lr:0.010000
[ Fri Jul 12 09:59:25 2024 ] 
Training: Epoch [37/120], Step [1999], Loss: 0.3021867275238037, Training Accuracy: 86.5625
[ Fri Jul 12 09:59:25 2024 ] 	Batch(2000/6809) done. Loss: 0.2896  lr:0.010000
[ Fri Jul 12 09:59:43 2024 ] 	Batch(2100/6809) done. Loss: 0.7138  lr:0.010000
[ Fri Jul 12 10:00:01 2024 ] 	Batch(2200/6809) done. Loss: 0.7880  lr:0.010000
[ Fri Jul 12 10:00:19 2024 ] 	Batch(2300/6809) done. Loss: 0.3152  lr:0.010000
[ Fri Jul 12 10:00:37 2024 ] 	Batch(2400/6809) done. Loss: 0.3656  lr:0.010000
[ Fri Jul 12 10:00:55 2024 ] 
Training: Epoch [37/120], Step [2499], Loss: 0.281338095664978, Training Accuracy: 86.56
[ Fri Jul 12 10:00:55 2024 ] 	Batch(2500/6809) done. Loss: 0.1870  lr:0.010000
[ Fri Jul 12 10:01:13 2024 ] 	Batch(2600/6809) done. Loss: 0.5207  lr:0.010000
[ Fri Jul 12 10:01:31 2024 ] 	Batch(2700/6809) done. Loss: 0.7695  lr:0.010000
[ Fri Jul 12 10:01:49 2024 ] 	Batch(2800/6809) done. Loss: 0.4237  lr:0.010000
[ Fri Jul 12 10:02:07 2024 ] 	Batch(2900/6809) done. Loss: 0.9221  lr:0.010000
[ Fri Jul 12 10:02:24 2024 ] 
Training: Epoch [37/120], Step [2999], Loss: 0.14172838628292084, Training Accuracy: 86.47500000000001
[ Fri Jul 12 10:02:24 2024 ] 	Batch(3000/6809) done. Loss: 0.3599  lr:0.010000
[ Fri Jul 12 10:02:42 2024 ] 	Batch(3100/6809) done. Loss: 0.3803  lr:0.010000
[ Fri Jul 12 10:03:00 2024 ] 	Batch(3200/6809) done. Loss: 0.3533  lr:0.010000
[ Fri Jul 12 10:03:19 2024 ] 	Batch(3300/6809) done. Loss: 0.5640  lr:0.010000
[ Fri Jul 12 10:03:38 2024 ] 	Batch(3400/6809) done. Loss: 0.2803  lr:0.010000
[ Fri Jul 12 10:03:56 2024 ] 
Training: Epoch [37/120], Step [3499], Loss: 0.2952442765235901, Training Accuracy: 86.375
[ Fri Jul 12 10:03:56 2024 ] 	Batch(3500/6809) done. Loss: 0.2142  lr:0.010000
[ Fri Jul 12 10:04:14 2024 ] 	Batch(3600/6809) done. Loss: 1.1473  lr:0.010000
[ Fri Jul 12 10:04:32 2024 ] 	Batch(3700/6809) done. Loss: 0.0984  lr:0.010000
[ Fri Jul 12 10:04:50 2024 ] 	Batch(3800/6809) done. Loss: 0.6561  lr:0.010000
[ Fri Jul 12 10:05:08 2024 ] 	Batch(3900/6809) done. Loss: 0.5097  lr:0.010000
[ Fri Jul 12 10:05:26 2024 ] 
Training: Epoch [37/120], Step [3999], Loss: 0.4157305955886841, Training Accuracy: 86.325
[ Fri Jul 12 10:05:26 2024 ] 	Batch(4000/6809) done. Loss: 1.0819  lr:0.010000
[ Fri Jul 12 10:05:45 2024 ] 	Batch(4100/6809) done. Loss: 0.7882  lr:0.010000
[ Fri Jul 12 10:06:03 2024 ] 	Batch(4200/6809) done. Loss: 0.4831  lr:0.010000
[ Fri Jul 12 10:06:22 2024 ] 	Batch(4300/6809) done. Loss: 0.1353  lr:0.010000
[ Fri Jul 12 10:06:41 2024 ] 	Batch(4400/6809) done. Loss: 0.0630  lr:0.010000
[ Fri Jul 12 10:06:59 2024 ] 
Training: Epoch [37/120], Step [4499], Loss: 0.13730640709400177, Training Accuracy: 86.27777777777777
[ Fri Jul 12 10:06:59 2024 ] 	Batch(4500/6809) done. Loss: 0.2646  lr:0.010000
[ Fri Jul 12 10:07:17 2024 ] 	Batch(4600/6809) done. Loss: 0.4908  lr:0.010000
[ Fri Jul 12 10:07:35 2024 ] 	Batch(4700/6809) done. Loss: 0.6731  lr:0.010000
[ Fri Jul 12 10:07:53 2024 ] 	Batch(4800/6809) done. Loss: 0.0096  lr:0.010000
[ Fri Jul 12 10:08:11 2024 ] 	Batch(4900/6809) done. Loss: 0.2633  lr:0.010000
[ Fri Jul 12 10:08:29 2024 ] 
Training: Epoch [37/120], Step [4999], Loss: 0.6067298054695129, Training Accuracy: 86.1775
[ Fri Jul 12 10:08:29 2024 ] 	Batch(5000/6809) done. Loss: 0.5772  lr:0.010000
[ Fri Jul 12 10:08:47 2024 ] 	Batch(5100/6809) done. Loss: 0.5706  lr:0.010000
[ Fri Jul 12 10:09:05 2024 ] 	Batch(5200/6809) done. Loss: 0.1761  lr:0.010000
[ Fri Jul 12 10:09:23 2024 ] 	Batch(5300/6809) done. Loss: 0.2363  lr:0.010000
[ Fri Jul 12 10:09:40 2024 ] 	Batch(5400/6809) done. Loss: 0.1050  lr:0.010000
[ Fri Jul 12 10:09:58 2024 ] 
Training: Epoch [37/120], Step [5499], Loss: 0.18550941348075867, Training Accuracy: 86.26136363636364
[ Fri Jul 12 10:09:58 2024 ] 	Batch(5500/6809) done. Loss: 0.7227  lr:0.010000
[ Fri Jul 12 10:10:16 2024 ] 	Batch(5600/6809) done. Loss: 0.5969  lr:0.010000
[ Fri Jul 12 10:10:34 2024 ] 	Batch(5700/6809) done. Loss: 0.3353  lr:0.010000
[ Fri Jul 12 10:10:52 2024 ] 	Batch(5800/6809) done. Loss: 0.0356  lr:0.010000
[ Fri Jul 12 10:11:10 2024 ] 	Batch(5900/6809) done. Loss: 0.1015  lr:0.010000
[ Fri Jul 12 10:11:28 2024 ] 
Training: Epoch [37/120], Step [5999], Loss: 0.2428353875875473, Training Accuracy: 86.28125
[ Fri Jul 12 10:11:28 2024 ] 	Batch(6000/6809) done. Loss: 1.1534  lr:0.010000
[ Fri Jul 12 10:11:46 2024 ] 	Batch(6100/6809) done. Loss: 0.0742  lr:0.010000
[ Fri Jul 12 10:12:04 2024 ] 	Batch(6200/6809) done. Loss: 0.9103  lr:0.010000
[ Fri Jul 12 10:12:22 2024 ] 	Batch(6300/6809) done. Loss: 0.0995  lr:0.010000
[ Fri Jul 12 10:12:40 2024 ] 	Batch(6400/6809) done. Loss: 0.0448  lr:0.010000
[ Fri Jul 12 10:12:57 2024 ] 
Training: Epoch [37/120], Step [6499], Loss: 0.8155229091644287, Training Accuracy: 86.20192307692308
[ Fri Jul 12 10:12:58 2024 ] 	Batch(6500/6809) done. Loss: 0.1553  lr:0.010000
[ Fri Jul 12 10:13:15 2024 ] 	Batch(6600/6809) done. Loss: 0.5690  lr:0.010000
[ Fri Jul 12 10:13:33 2024 ] 	Batch(6700/6809) done. Loss: 0.1469  lr:0.010000
[ Fri Jul 12 10:13:51 2024 ] 	Batch(6800/6809) done. Loss: 0.4488  lr:0.010000
[ Fri Jul 12 10:13:53 2024 ] 	Mean training loss: 0.4338.
[ Fri Jul 12 10:13:53 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 10:13:53 2024 ] Training epoch: 39
[ Fri Jul 12 10:13:53 2024 ] 	Batch(0/6809) done. Loss: 0.0891  lr:0.010000
[ Fri Jul 12 10:14:11 2024 ] 	Batch(100/6809) done. Loss: 0.5665  lr:0.010000
[ Fri Jul 12 10:14:29 2024 ] 	Batch(200/6809) done. Loss: 0.3939  lr:0.010000
[ Fri Jul 12 10:14:47 2024 ] 	Batch(300/6809) done. Loss: 0.2112  lr:0.010000
[ Fri Jul 12 10:15:05 2024 ] 	Batch(400/6809) done. Loss: 0.3539  lr:0.010000
[ Fri Jul 12 10:15:23 2024 ] 
Training: Epoch [38/120], Step [499], Loss: 0.1349736452102661, Training Accuracy: 86.625
[ Fri Jul 12 10:15:23 2024 ] 	Batch(500/6809) done. Loss: 0.1439  lr:0.010000
[ Fri Jul 12 10:15:41 2024 ] 	Batch(600/6809) done. Loss: 0.8741  lr:0.010000
[ Fri Jul 12 10:15:59 2024 ] 	Batch(700/6809) done. Loss: 0.0714  lr:0.010000
[ Fri Jul 12 10:16:17 2024 ] 	Batch(800/6809) done. Loss: 0.7865  lr:0.010000
[ Fri Jul 12 10:16:35 2024 ] 	Batch(900/6809) done. Loss: 0.0383  lr:0.010000
[ Fri Jul 12 10:16:52 2024 ] 
Training: Epoch [38/120], Step [999], Loss: 0.5979794263839722, Training Accuracy: 86.9875
[ Fri Jul 12 10:16:53 2024 ] 	Batch(1000/6809) done. Loss: 0.1066  lr:0.010000
[ Fri Jul 12 10:17:11 2024 ] 	Batch(1100/6809) done. Loss: 0.1481  lr:0.010000
[ Fri Jul 12 10:17:29 2024 ] 	Batch(1200/6809) done. Loss: 0.6965  lr:0.010000
[ Fri Jul 12 10:17:46 2024 ] 	Batch(1300/6809) done. Loss: 0.9211  lr:0.010000
[ Fri Jul 12 10:18:04 2024 ] 	Batch(1400/6809) done. Loss: 0.3368  lr:0.010000
[ Fri Jul 12 10:18:22 2024 ] 
Training: Epoch [38/120], Step [1499], Loss: 0.020305708050727844, Training Accuracy: 87.125
[ Fri Jul 12 10:18:22 2024 ] 	Batch(1500/6809) done. Loss: 0.9725  lr:0.010000
[ Fri Jul 12 10:18:40 2024 ] 	Batch(1600/6809) done. Loss: 0.6015  lr:0.010000
[ Fri Jul 12 10:18:58 2024 ] 	Batch(1700/6809) done. Loss: 0.3313  lr:0.010000
[ Fri Jul 12 10:19:16 2024 ] 	Batch(1800/6809) done. Loss: 0.4364  lr:0.010000
[ Fri Jul 12 10:19:34 2024 ] 	Batch(1900/6809) done. Loss: 0.7201  lr:0.010000
[ Fri Jul 12 10:19:52 2024 ] 
Training: Epoch [38/120], Step [1999], Loss: 1.1181552410125732, Training Accuracy: 87.26875
[ Fri Jul 12 10:19:52 2024 ] 	Batch(2000/6809) done. Loss: 0.2995  lr:0.010000
[ Fri Jul 12 10:20:10 2024 ] 	Batch(2100/6809) done. Loss: 0.4907  lr:0.010000
[ Fri Jul 12 10:20:28 2024 ] 	Batch(2200/6809) done. Loss: 0.1775  lr:0.010000
[ Fri Jul 12 10:20:46 2024 ] 	Batch(2300/6809) done. Loss: 0.4362  lr:0.010000
[ Fri Jul 12 10:21:04 2024 ] 	Batch(2400/6809) done. Loss: 0.2152  lr:0.010000
[ Fri Jul 12 10:21:21 2024 ] 
Training: Epoch [38/120], Step [2499], Loss: 0.26783859729766846, Training Accuracy: 87.115
[ Fri Jul 12 10:21:21 2024 ] 	Batch(2500/6809) done. Loss: 0.6345  lr:0.010000
[ Fri Jul 12 10:21:39 2024 ] 	Batch(2600/6809) done. Loss: 0.3706  lr:0.010000
[ Fri Jul 12 10:21:57 2024 ] 	Batch(2700/6809) done. Loss: 0.2381  lr:0.010000
[ Fri Jul 12 10:22:15 2024 ] 	Batch(2800/6809) done. Loss: 0.0940  lr:0.010000
[ Fri Jul 12 10:22:33 2024 ] 	Batch(2900/6809) done. Loss: 0.3886  lr:0.010000
[ Fri Jul 12 10:22:51 2024 ] 
Training: Epoch [38/120], Step [2999], Loss: 0.36768341064453125, Training Accuracy: 86.97083333333333
[ Fri Jul 12 10:22:51 2024 ] 	Batch(3000/6809) done. Loss: 0.3131  lr:0.010000
[ Fri Jul 12 10:23:09 2024 ] 	Batch(3100/6809) done. Loss: 0.0601  lr:0.010000
[ Fri Jul 12 10:23:27 2024 ] 	Batch(3200/6809) done. Loss: 0.1121  lr:0.010000
[ Fri Jul 12 10:23:46 2024 ] 	Batch(3300/6809) done. Loss: 0.1798  lr:0.010000
[ Fri Jul 12 10:24:04 2024 ] 	Batch(3400/6809) done. Loss: 0.0405  lr:0.010000
[ Fri Jul 12 10:24:23 2024 ] 
Training: Epoch [38/120], Step [3499], Loss: 0.27928993105888367, Training Accuracy: 86.91785714285714
[ Fri Jul 12 10:24:23 2024 ] 	Batch(3500/6809) done. Loss: 0.3607  lr:0.010000
[ Fri Jul 12 10:24:42 2024 ] 	Batch(3600/6809) done. Loss: 0.6348  lr:0.010000
[ Fri Jul 12 10:25:00 2024 ] 	Batch(3700/6809) done. Loss: 0.3871  lr:0.010000
[ Fri Jul 12 10:25:19 2024 ] 	Batch(3800/6809) done. Loss: 0.0686  lr:0.010000
[ Fri Jul 12 10:25:37 2024 ] 	Batch(3900/6809) done. Loss: 0.4472  lr:0.010000
[ Fri Jul 12 10:25:56 2024 ] 
Training: Epoch [38/120], Step [3999], Loss: 0.5240074992179871, Training Accuracy: 86.790625
[ Fri Jul 12 10:25:56 2024 ] 	Batch(4000/6809) done. Loss: 0.2749  lr:0.010000
[ Fri Jul 12 10:26:14 2024 ] 	Batch(4100/6809) done. Loss: 1.5366  lr:0.010000
[ Fri Jul 12 10:26:32 2024 ] 	Batch(4200/6809) done. Loss: 1.2042  lr:0.010000
[ Fri Jul 12 10:26:49 2024 ] 	Batch(4300/6809) done. Loss: 0.4004  lr:0.010000
[ Fri Jul 12 10:27:08 2024 ] 	Batch(4400/6809) done. Loss: 0.3766  lr:0.010000
[ Fri Jul 12 10:27:26 2024 ] 
Training: Epoch [38/120], Step [4499], Loss: 0.4359016418457031, Training Accuracy: 86.69722222222222
[ Fri Jul 12 10:27:26 2024 ] 	Batch(4500/6809) done. Loss: 0.5638  lr:0.010000
[ Fri Jul 12 10:27:45 2024 ] 	Batch(4600/6809) done. Loss: 0.4422  lr:0.010000
[ Fri Jul 12 10:28:03 2024 ] 	Batch(4700/6809) done. Loss: 0.3363  lr:0.010000
[ Fri Jul 12 10:28:22 2024 ] 	Batch(4800/6809) done. Loss: 0.4992  lr:0.010000
[ Fri Jul 12 10:28:40 2024 ] 	Batch(4900/6809) done. Loss: 0.0597  lr:0.010000
[ Fri Jul 12 10:28:59 2024 ] 
Training: Epoch [38/120], Step [4999], Loss: 0.36129921674728394, Training Accuracy: 86.57249999999999
[ Fri Jul 12 10:28:59 2024 ] 	Batch(5000/6809) done. Loss: 0.3927  lr:0.010000
[ Fri Jul 12 10:29:17 2024 ] 	Batch(5100/6809) done. Loss: 0.7497  lr:0.010000
[ Fri Jul 12 10:29:36 2024 ] 	Batch(5200/6809) done. Loss: 0.1921  lr:0.010000
[ Fri Jul 12 10:29:55 2024 ] 	Batch(5300/6809) done. Loss: 0.1409  lr:0.010000
[ Fri Jul 12 10:30:13 2024 ] 	Batch(5400/6809) done. Loss: 0.3547  lr:0.010000
[ Fri Jul 12 10:30:31 2024 ] 
Training: Epoch [38/120], Step [5499], Loss: 0.11595354974269867, Training Accuracy: 86.52499999999999
[ Fri Jul 12 10:30:31 2024 ] 	Batch(5500/6809) done. Loss: 0.5652  lr:0.010000
[ Fri Jul 12 10:30:49 2024 ] 	Batch(5600/6809) done. Loss: 0.8059  lr:0.010000
[ Fri Jul 12 10:31:07 2024 ] 	Batch(5700/6809) done. Loss: 0.9805  lr:0.010000
[ Fri Jul 12 10:31:25 2024 ] 	Batch(5800/6809) done. Loss: 0.3545  lr:0.010000
[ Fri Jul 12 10:31:43 2024 ] 	Batch(5900/6809) done. Loss: 0.5731  lr:0.010000
[ Fri Jul 12 10:32:01 2024 ] 
Training: Epoch [38/120], Step [5999], Loss: 0.07425568997859955, Training Accuracy: 86.47708333333334
[ Fri Jul 12 10:32:01 2024 ] 	Batch(6000/6809) done. Loss: 0.0292  lr:0.010000
[ Fri Jul 12 10:32:19 2024 ] 	Batch(6100/6809) done. Loss: 0.2596  lr:0.010000
[ Fri Jul 12 10:32:37 2024 ] 	Batch(6200/6809) done. Loss: 0.1191  lr:0.010000
[ Fri Jul 12 10:32:55 2024 ] 	Batch(6300/6809) done. Loss: 0.3979  lr:0.010000
[ Fri Jul 12 10:33:13 2024 ] 	Batch(6400/6809) done. Loss: 1.0569  lr:0.010000
[ Fri Jul 12 10:33:31 2024 ] 
Training: Epoch [38/120], Step [6499], Loss: 0.3336998522281647, Training Accuracy: 86.39615384615385
[ Fri Jul 12 10:33:31 2024 ] 	Batch(6500/6809) done. Loss: 0.5202  lr:0.010000
[ Fri Jul 12 10:33:50 2024 ] 	Batch(6600/6809) done. Loss: 0.1293  lr:0.010000
[ Fri Jul 12 10:34:08 2024 ] 	Batch(6700/6809) done. Loss: 0.8510  lr:0.010000
[ Fri Jul 12 10:34:27 2024 ] 	Batch(6800/6809) done. Loss: 0.3264  lr:0.010000
[ Fri Jul 12 10:34:28 2024 ] 	Mean training loss: 0.4350.
[ Fri Jul 12 10:34:28 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 10:34:28 2024 ] Training epoch: 40
[ Fri Jul 12 10:34:29 2024 ] 	Batch(0/6809) done. Loss: 0.4222  lr:0.010000
[ Fri Jul 12 10:34:47 2024 ] 	Batch(100/6809) done. Loss: 0.2477  lr:0.010000
[ Fri Jul 12 10:35:05 2024 ] 	Batch(200/6809) done. Loss: 0.7660  lr:0.010000
[ Fri Jul 12 10:35:23 2024 ] 	Batch(300/6809) done. Loss: 0.1890  lr:0.010000
[ Fri Jul 12 10:35:41 2024 ] 	Batch(400/6809) done. Loss: 0.5395  lr:0.010000
[ Fri Jul 12 10:35:58 2024 ] 
Training: Epoch [39/120], Step [499], Loss: 0.2551189959049225, Training Accuracy: 87.5
[ Fri Jul 12 10:35:59 2024 ] 	Batch(500/6809) done. Loss: 0.4130  lr:0.010000
[ Fri Jul 12 10:36:17 2024 ] 	Batch(600/6809) done. Loss: 0.1172  lr:0.010000
[ Fri Jul 12 10:36:34 2024 ] 	Batch(700/6809) done. Loss: 0.2672  lr:0.010000
[ Fri Jul 12 10:36:53 2024 ] 	Batch(800/6809) done. Loss: 0.2505  lr:0.010000
[ Fri Jul 12 10:37:10 2024 ] 	Batch(900/6809) done. Loss: 0.4649  lr:0.010000
[ Fri Jul 12 10:37:28 2024 ] 
Training: Epoch [39/120], Step [999], Loss: 0.615791380405426, Training Accuracy: 87.3
[ Fri Jul 12 10:37:28 2024 ] 	Batch(1000/6809) done. Loss: 0.1031  lr:0.010000
[ Fri Jul 12 10:37:46 2024 ] 	Batch(1100/6809) done. Loss: 0.1784  lr:0.010000
[ Fri Jul 12 10:38:04 2024 ] 	Batch(1200/6809) done. Loss: 0.1516  lr:0.010000
[ Fri Jul 12 10:38:23 2024 ] 	Batch(1300/6809) done. Loss: 0.4011  lr:0.010000
[ Fri Jul 12 10:38:41 2024 ] 	Batch(1400/6809) done. Loss: 0.2181  lr:0.010000
[ Fri Jul 12 10:39:00 2024 ] 
Training: Epoch [39/120], Step [1499], Loss: 0.12906570732593536, Training Accuracy: 87.45
[ Fri Jul 12 10:39:00 2024 ] 	Batch(1500/6809) done. Loss: 0.1785  lr:0.010000
[ Fri Jul 12 10:39:19 2024 ] 	Batch(1600/6809) done. Loss: 0.1763  lr:0.010000
[ Fri Jul 12 10:39:37 2024 ] 	Batch(1700/6809) done. Loss: 0.3964  lr:0.010000
[ Fri Jul 12 10:39:55 2024 ] 	Batch(1800/6809) done. Loss: 0.2250  lr:0.010000
[ Fri Jul 12 10:40:13 2024 ] 	Batch(1900/6809) done. Loss: 0.2385  lr:0.010000
[ Fri Jul 12 10:40:31 2024 ] 
Training: Epoch [39/120], Step [1999], Loss: 0.18474173545837402, Training Accuracy: 87.53125
[ Fri Jul 12 10:40:31 2024 ] 	Batch(2000/6809) done. Loss: 0.3990  lr:0.010000
[ Fri Jul 12 10:40:49 2024 ] 	Batch(2100/6809) done. Loss: 0.1735  lr:0.010000
[ Fri Jul 12 10:41:07 2024 ] 	Batch(2200/6809) done. Loss: 0.4388  lr:0.010000
[ Fri Jul 12 10:41:25 2024 ] 	Batch(2300/6809) done. Loss: 0.1895  lr:0.010000
[ Fri Jul 12 10:41:43 2024 ] 	Batch(2400/6809) done. Loss: 0.7095  lr:0.010000
[ Fri Jul 12 10:42:01 2024 ] 
Training: Epoch [39/120], Step [2499], Loss: 0.5265149474143982, Training Accuracy: 87.195
[ Fri Jul 12 10:42:02 2024 ] 	Batch(2500/6809) done. Loss: 0.0328  lr:0.010000
[ Fri Jul 12 10:42:20 2024 ] 	Batch(2600/6809) done. Loss: 1.7700  lr:0.010000
[ Fri Jul 12 10:42:38 2024 ] 	Batch(2700/6809) done. Loss: 0.0599  lr:0.010000
[ Fri Jul 12 10:42:56 2024 ] 	Batch(2800/6809) done. Loss: 0.3837  lr:0.010000
[ Fri Jul 12 10:43:15 2024 ] 	Batch(2900/6809) done. Loss: 0.1784  lr:0.010000
[ Fri Jul 12 10:43:33 2024 ] 
Training: Epoch [39/120], Step [2999], Loss: 0.594825804233551, Training Accuracy: 86.9375
[ Fri Jul 12 10:43:33 2024 ] 	Batch(3000/6809) done. Loss: 0.4322  lr:0.010000
[ Fri Jul 12 10:43:52 2024 ] 	Batch(3100/6809) done. Loss: 0.2805  lr:0.010000
[ Fri Jul 12 10:44:10 2024 ] 	Batch(3200/6809) done. Loss: 0.1755  lr:0.010000
[ Fri Jul 12 10:44:28 2024 ] 	Batch(3300/6809) done. Loss: 0.2010  lr:0.010000
[ Fri Jul 12 10:44:46 2024 ] 	Batch(3400/6809) done. Loss: 0.2890  lr:0.010000
[ Fri Jul 12 10:45:04 2024 ] 
Training: Epoch [39/120], Step [3499], Loss: 0.40999123454093933, Training Accuracy: 86.82142857142857
[ Fri Jul 12 10:45:04 2024 ] 	Batch(3500/6809) done. Loss: 0.5945  lr:0.010000
[ Fri Jul 12 10:45:22 2024 ] 	Batch(3600/6809) done. Loss: 0.1180  lr:0.010000
[ Fri Jul 12 10:45:40 2024 ] 	Batch(3700/6809) done. Loss: 0.9304  lr:0.010000
[ Fri Jul 12 10:45:58 2024 ] 	Batch(3800/6809) done. Loss: 0.9318  lr:0.010000
[ Fri Jul 12 10:46:16 2024 ] 	Batch(3900/6809) done. Loss: 0.0670  lr:0.010000
[ Fri Jul 12 10:46:34 2024 ] 
Training: Epoch [39/120], Step [3999], Loss: 0.38954561948776245, Training Accuracy: 86.684375
[ Fri Jul 12 10:46:35 2024 ] 	Batch(4000/6809) done. Loss: 0.4017  lr:0.010000
[ Fri Jul 12 10:46:53 2024 ] 	Batch(4100/6809) done. Loss: 0.4803  lr:0.010000
[ Fri Jul 12 10:47:10 2024 ] 	Batch(4200/6809) done. Loss: 0.0436  lr:0.010000
[ Fri Jul 12 10:47:28 2024 ] 	Batch(4300/6809) done. Loss: 0.9845  lr:0.010000
[ Fri Jul 12 10:47:46 2024 ] 	Batch(4400/6809) done. Loss: 0.1347  lr:0.010000
[ Fri Jul 12 10:48:04 2024 ] 
Training: Epoch [39/120], Step [4499], Loss: 0.42896223068237305, Training Accuracy: 86.61666666666666
[ Fri Jul 12 10:48:04 2024 ] 	Batch(4500/6809) done. Loss: 0.3575  lr:0.010000
[ Fri Jul 12 10:48:22 2024 ] 	Batch(4600/6809) done. Loss: 0.0603  lr:0.010000
[ Fri Jul 12 10:48:40 2024 ] 	Batch(4700/6809) done. Loss: 0.4781  lr:0.010000
[ Fri Jul 12 10:48:58 2024 ] 	Batch(4800/6809) done. Loss: 0.0679  lr:0.010000
[ Fri Jul 12 10:49:16 2024 ] 	Batch(4900/6809) done. Loss: 0.0493  lr:0.010000
[ Fri Jul 12 10:49:34 2024 ] 
Training: Epoch [39/120], Step [4999], Loss: 0.30283305048942566, Training Accuracy: 86.63
[ Fri Jul 12 10:49:34 2024 ] 	Batch(5000/6809) done. Loss: 0.1254  lr:0.010000
[ Fri Jul 12 10:49:52 2024 ] 	Batch(5100/6809) done. Loss: 0.2149  lr:0.010000
[ Fri Jul 12 10:50:10 2024 ] 	Batch(5200/6809) done. Loss: 0.7932  lr:0.010000
[ Fri Jul 12 10:50:28 2024 ] 	Batch(5300/6809) done. Loss: 0.2393  lr:0.010000
[ Fri Jul 12 10:50:45 2024 ] 	Batch(5400/6809) done. Loss: 0.2134  lr:0.010000
[ Fri Jul 12 10:51:03 2024 ] 
Training: Epoch [39/120], Step [5499], Loss: 0.4407363533973694, Training Accuracy: 86.56590909090909
[ Fri Jul 12 10:51:03 2024 ] 	Batch(5500/6809) done. Loss: 0.2302  lr:0.010000
[ Fri Jul 12 10:51:21 2024 ] 	Batch(5600/6809) done. Loss: 0.6402  lr:0.010000
[ Fri Jul 12 10:51:39 2024 ] 	Batch(5700/6809) done. Loss: 0.5814  lr:0.010000
[ Fri Jul 12 10:51:57 2024 ] 	Batch(5800/6809) done. Loss: 0.3122  lr:0.010000
[ Fri Jul 12 10:52:15 2024 ] 	Batch(5900/6809) done. Loss: 0.1255  lr:0.010000
[ Fri Jul 12 10:52:33 2024 ] 
Training: Epoch [39/120], Step [5999], Loss: 1.5814861059188843, Training Accuracy: 86.5
[ Fri Jul 12 10:52:33 2024 ] 	Batch(6000/6809) done. Loss: 1.1519  lr:0.010000
[ Fri Jul 12 10:52:52 2024 ] 	Batch(6100/6809) done. Loss: 0.5538  lr:0.010000
[ Fri Jul 12 10:53:09 2024 ] 	Batch(6200/6809) done. Loss: 0.1324  lr:0.010000
[ Fri Jul 12 10:53:27 2024 ] 	Batch(6300/6809) done. Loss: 0.2366  lr:0.010000
[ Fri Jul 12 10:53:45 2024 ] 	Batch(6400/6809) done. Loss: 0.7040  lr:0.010000
[ Fri Jul 12 10:54:03 2024 ] 
Training: Epoch [39/120], Step [6499], Loss: 0.563869297504425, Training Accuracy: 86.40961538461538
[ Fri Jul 12 10:54:03 2024 ] 	Batch(6500/6809) done. Loss: 0.3844  lr:0.010000
[ Fri Jul 12 10:54:21 2024 ] 	Batch(6600/6809) done. Loss: 0.2417  lr:0.010000
[ Fri Jul 12 10:54:39 2024 ] 	Batch(6700/6809) done. Loss: 0.7623  lr:0.010000
[ Fri Jul 12 10:54:57 2024 ] 	Batch(6800/6809) done. Loss: 0.5865  lr:0.010000
[ Fri Jul 12 10:54:59 2024 ] 	Mean training loss: 0.4280.
[ Fri Jul 12 10:54:59 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 10:54:59 2024 ] Eval epoch: 40
[ Fri Jul 12 11:00:33 2024 ] 	Mean val loss of 7435 batches: 1.0635790112030987.
[ Fri Jul 12 11:00:33 2024 ] 
Validation: Epoch [39/120], Samples [45529.0/59477], Loss: 0.37601375579833984, Validation Accuracy: 76.54891806916959
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 1 : 365 / 500 = 73 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 2 : 331 / 499 = 66 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 3 : 418 / 500 = 83 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 4 : 406 / 502 = 80 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 5 : 463 / 502 = 92 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 6 : 410 / 502 = 81 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 7 : 472 / 497 = 94 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 8 : 470 / 498 = 94 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 9 : 386 / 500 = 77 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 10 : 242 / 500 = 48 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 11 : 243 / 498 = 48 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 12 : 423 / 499 = 84 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 13 : 449 / 502 = 89 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 14 : 455 / 504 = 90 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 15 : 314 / 502 = 62 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 16 : 423 / 502 = 84 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 17 : 418 / 504 = 82 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 18 : 405 / 504 = 80 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 19 : 443 / 502 = 88 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 20 : 419 / 502 = 83 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 21 : 454 / 503 = 90 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 22 : 423 / 504 = 83 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 23 : 428 / 503 = 85 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 24 : 407 / 504 = 80 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 25 : 479 / 504 = 95 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 26 : 479 / 504 = 95 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 27 : 402 / 501 = 80 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 28 : 311 / 502 = 61 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 29 : 315 / 502 = 62 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 30 : 344 / 501 = 68 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 31 : 400 / 504 = 79 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 32 : 441 / 503 = 87 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 33 : 419 / 503 = 83 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 34 : 482 / 504 = 95 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 35 : 433 / 503 = 86 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 36 : 389 / 502 = 77 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 37 : 353 / 504 = 70 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 38 : 394 / 504 = 78 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 39 : 404 / 498 = 81 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 40 : 372 / 504 = 73 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 41 : 480 / 503 = 95 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 42 : 477 / 504 = 94 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 43 : 362 / 503 = 71 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 44 : 419 / 504 = 83 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 45 : 413 / 504 = 81 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 46 : 398 / 504 = 78 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 47 : 398 / 503 = 79 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 48 : 396 / 503 = 78 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 49 : 375 / 499 = 75 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 50 : 370 / 502 = 73 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 51 : 464 / 503 = 92 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 52 : 427 / 504 = 84 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 53 : 431 / 497 = 86 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 54 : 422 / 480 = 87 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 55 : 379 / 504 = 75 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 56 : 372 / 503 = 73 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 57 : 483 / 504 = 95 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 58 : 483 / 499 = 96 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 59 : 473 / 503 = 94 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 60 : 411 / 479 = 85 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 61 : 387 / 484 = 79 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 62 : 383 / 487 = 78 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 63 : 451 / 489 = 92 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 64 : 403 / 488 = 82 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 65 : 423 / 490 = 86 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 66 : 369 / 488 = 75 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 67 : 346 / 490 = 70 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 68 : 218 / 490 = 44 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 69 : 357 / 490 = 72 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 70 : 204 / 490 = 41 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 71 : 248 / 490 = 50 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 72 : 44 / 488 = 9 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 73 : 210 / 486 = 43 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 74 : 222 / 481 = 46 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 75 : 268 / 488 = 54 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 76 : 309 / 489 = 63 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 77 : 331 / 488 = 67 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 78 : 343 / 488 = 70 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 79 : 437 / 490 = 89 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 80 : 423 / 489 = 86 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 81 : 359 / 491 = 73 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 82 : 295 / 491 = 60 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 83 : 307 / 489 = 62 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 84 : 326 / 489 = 66 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 85 : 299 / 489 = 61 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 86 : 401 / 491 = 81 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 87 : 413 / 492 = 83 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 88 : 343 / 491 = 69 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 89 : 356 / 492 = 72 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 90 : 167 / 490 = 34 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 91 : 365 / 482 = 75 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 92 : 292 / 490 = 59 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 93 : 311 / 487 = 63 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 94 : 390 / 489 = 79 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 95 : 405 / 490 = 82 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 96 : 466 / 491 = 94 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 97 : 457 / 490 = 93 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 98 : 445 / 491 = 90 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 99 : 435 / 491 = 88 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 100 : 448 / 491 = 91 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 101 : 414 / 491 = 84 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 102 : 239 / 492 = 48 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 103 : 373 / 492 = 75 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 104 : 128 / 491 = 26 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 105 : 271 / 491 = 55 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 106 : 214 / 492 = 43 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 107 : 367 / 491 = 74 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 108 : 348 / 492 = 70 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 109 : 277 / 490 = 56 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 110 : 393 / 491 = 80 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 111 : 456 / 492 = 92 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 112 : 458 / 492 = 93 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 113 : 447 / 491 = 91 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 114 : 416 / 491 = 84 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 115 : 383 / 492 = 77 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 116 : 400 / 491 = 81 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 117 : 445 / 492 = 90 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 118 : 434 / 490 = 88 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 119 : 444 / 492 = 90 %
[ Fri Jul 12 11:00:33 2024 ] Accuracy of 120 : 449 / 500 = 89 %
[ Fri Jul 12 11:00:33 2024 ] Training epoch: 41
[ Fri Jul 12 11:00:33 2024 ] 	Batch(0/6809) done. Loss: 0.3426  lr:0.010000
[ Fri Jul 12 11:00:51 2024 ] 	Batch(100/6809) done. Loss: 0.2471  lr:0.010000
[ Fri Jul 12 11:01:09 2024 ] 	Batch(200/6809) done. Loss: 0.9960  lr:0.010000
[ Fri Jul 12 11:01:27 2024 ] 	Batch(300/6809) done. Loss: 0.1214  lr:0.010000
[ Fri Jul 12 11:01:45 2024 ] 	Batch(400/6809) done. Loss: 0.6479  lr:0.010000
[ Fri Jul 12 11:02:03 2024 ] 
Training: Epoch [40/120], Step [499], Loss: 0.18913128972053528, Training Accuracy: 87.6
[ Fri Jul 12 11:02:03 2024 ] 	Batch(500/6809) done. Loss: 0.5905  lr:0.010000
[ Fri Jul 12 11:02:21 2024 ] 	Batch(600/6809) done. Loss: 0.0499  lr:0.010000
[ Fri Jul 12 11:02:39 2024 ] 	Batch(700/6809) done. Loss: 0.9020  lr:0.010000
[ Fri Jul 12 11:02:57 2024 ] 	Batch(800/6809) done. Loss: 1.0817  lr:0.010000
[ Fri Jul 12 11:03:15 2024 ] 	Batch(900/6809) done. Loss: 0.0451  lr:0.010000
[ Fri Jul 12 11:03:33 2024 ] 
Training: Epoch [40/120], Step [999], Loss: 0.07316333800554276, Training Accuracy: 88.0125
[ Fri Jul 12 11:03:33 2024 ] 	Batch(1000/6809) done. Loss: 0.5569  lr:0.010000
[ Fri Jul 12 11:03:51 2024 ] 	Batch(1100/6809) done. Loss: 0.0117  lr:0.010000
[ Fri Jul 12 11:04:09 2024 ] 	Batch(1200/6809) done. Loss: 0.2903  lr:0.010000
[ Fri Jul 12 11:04:27 2024 ] 	Batch(1300/6809) done. Loss: 0.2070  lr:0.010000
[ Fri Jul 12 11:04:46 2024 ] 	Batch(1400/6809) done. Loss: 0.2598  lr:0.010000
[ Fri Jul 12 11:05:04 2024 ] 
Training: Epoch [40/120], Step [1499], Loss: 0.5002940893173218, Training Accuracy: 87.9
[ Fri Jul 12 11:05:04 2024 ] 	Batch(1500/6809) done. Loss: 0.1649  lr:0.010000
[ Fri Jul 12 11:05:23 2024 ] 	Batch(1600/6809) done. Loss: 0.4926  lr:0.010000
[ Fri Jul 12 11:05:42 2024 ] 	Batch(1700/6809) done. Loss: 0.2244  lr:0.010000
[ Fri Jul 12 11:06:00 2024 ] 	Batch(1800/6809) done. Loss: 0.8583  lr:0.010000
[ Fri Jul 12 11:06:19 2024 ] 	Batch(1900/6809) done. Loss: 0.1874  lr:0.010000
[ Fri Jul 12 11:06:37 2024 ] 
Training: Epoch [40/120], Step [1999], Loss: 0.507064938545227, Training Accuracy: 87.60625
[ Fri Jul 12 11:06:37 2024 ] 	Batch(2000/6809) done. Loss: 1.0397  lr:0.010000
[ Fri Jul 12 11:06:55 2024 ] 	Batch(2100/6809) done. Loss: 0.6493  lr:0.010000
[ Fri Jul 12 11:07:13 2024 ] 	Batch(2200/6809) done. Loss: 0.1737  lr:0.010000
[ Fri Jul 12 11:07:31 2024 ] 	Batch(2300/6809) done. Loss: 0.6466  lr:0.010000
[ Fri Jul 12 11:07:49 2024 ] 	Batch(2400/6809) done. Loss: 0.2988  lr:0.010000
[ Fri Jul 12 11:08:07 2024 ] 
Training: Epoch [40/120], Step [2499], Loss: 0.6103091239929199, Training Accuracy: 87.445
[ Fri Jul 12 11:08:07 2024 ] 	Batch(2500/6809) done. Loss: 0.2420  lr:0.010000
[ Fri Jul 12 11:08:26 2024 ] 	Batch(2600/6809) done. Loss: 0.1117  lr:0.010000
[ Fri Jul 12 11:08:44 2024 ] 	Batch(2700/6809) done. Loss: 0.0812  lr:0.010000
[ Fri Jul 12 11:09:03 2024 ] 	Batch(2800/6809) done. Loss: 0.3963  lr:0.010000
[ Fri Jul 12 11:09:22 2024 ] 	Batch(2900/6809) done. Loss: 0.1406  lr:0.010000
[ Fri Jul 12 11:09:40 2024 ] 
Training: Epoch [40/120], Step [2999], Loss: 0.08413566648960114, Training Accuracy: 87.34166666666667
[ Fri Jul 12 11:09:40 2024 ] 	Batch(3000/6809) done. Loss: 0.0023  lr:0.010000
[ Fri Jul 12 11:09:59 2024 ] 	Batch(3100/6809) done. Loss: 0.1562  lr:0.010000
[ Fri Jul 12 11:10:17 2024 ] 	Batch(3200/6809) done. Loss: 0.4193  lr:0.010000
[ Fri Jul 12 11:10:36 2024 ] 	Batch(3300/6809) done. Loss: 0.0410  lr:0.010000
[ Fri Jul 12 11:10:54 2024 ] 	Batch(3400/6809) done. Loss: 1.0172  lr:0.010000
[ Fri Jul 12 11:11:13 2024 ] 
Training: Epoch [40/120], Step [3499], Loss: 0.09980212897062302, Training Accuracy: 87.35357142857143
[ Fri Jul 12 11:11:13 2024 ] 	Batch(3500/6809) done. Loss: 0.2286  lr:0.010000
[ Fri Jul 12 11:11:32 2024 ] 	Batch(3600/6809) done. Loss: 0.3964  lr:0.010000
[ Fri Jul 12 11:11:50 2024 ] 	Batch(3700/6809) done. Loss: 0.1219  lr:0.010000
[ Fri Jul 12 11:12:09 2024 ] 	Batch(3800/6809) done. Loss: 0.1901  lr:0.010000
[ Fri Jul 12 11:12:27 2024 ] 	Batch(3900/6809) done. Loss: 0.1702  lr:0.010000
[ Fri Jul 12 11:12:46 2024 ] 
Training: Epoch [40/120], Step [3999], Loss: 0.4327335059642792, Training Accuracy: 87.228125
[ Fri Jul 12 11:12:46 2024 ] 	Batch(4000/6809) done. Loss: 0.2588  lr:0.010000
[ Fri Jul 12 11:13:04 2024 ] 	Batch(4100/6809) done. Loss: 0.4905  lr:0.010000
[ Fri Jul 12 11:13:22 2024 ] 	Batch(4200/6809) done. Loss: 0.1935  lr:0.010000
[ Fri Jul 12 11:13:40 2024 ] 	Batch(4300/6809) done. Loss: 0.1811  lr:0.010000
[ Fri Jul 12 11:13:58 2024 ] 	Batch(4400/6809) done. Loss: 0.1823  lr:0.010000
[ Fri Jul 12 11:14:15 2024 ] 
Training: Epoch [40/120], Step [4499], Loss: 0.08179837465286255, Training Accuracy: 87.11944444444445
[ Fri Jul 12 11:14:15 2024 ] 	Batch(4500/6809) done. Loss: 0.9382  lr:0.010000
[ Fri Jul 12 11:14:33 2024 ] 	Batch(4600/6809) done. Loss: 0.0484  lr:0.010000
[ Fri Jul 12 11:14:51 2024 ] 	Batch(4700/6809) done. Loss: 0.7432  lr:0.010000
[ Fri Jul 12 11:15:09 2024 ] 	Batch(4800/6809) done. Loss: 0.6492  lr:0.010000
[ Fri Jul 12 11:15:27 2024 ] 	Batch(4900/6809) done. Loss: 0.2350  lr:0.010000
[ Fri Jul 12 11:15:45 2024 ] 
Training: Epoch [40/120], Step [4999], Loss: 0.16554902493953705, Training Accuracy: 87.035
[ Fri Jul 12 11:15:45 2024 ] 	Batch(5000/6809) done. Loss: 1.2845  lr:0.010000
[ Fri Jul 12 11:16:03 2024 ] 	Batch(5100/6809) done. Loss: 0.7137  lr:0.010000
[ Fri Jul 12 11:16:21 2024 ] 	Batch(5200/6809) done. Loss: 0.4072  lr:0.010000
[ Fri Jul 12 11:16:39 2024 ] 	Batch(5300/6809) done. Loss: 0.7465  lr:0.010000
[ Fri Jul 12 11:16:57 2024 ] 	Batch(5400/6809) done. Loss: 0.0307  lr:0.010000
[ Fri Jul 12 11:17:15 2024 ] 
Training: Epoch [40/120], Step [5499], Loss: 0.07567527145147324, Training Accuracy: 86.93181818181817
[ Fri Jul 12 11:17:15 2024 ] 	Batch(5500/6809) done. Loss: 0.2938  lr:0.010000
[ Fri Jul 12 11:17:33 2024 ] 	Batch(5600/6809) done. Loss: 0.3225  lr:0.010000
[ Fri Jul 12 11:17:51 2024 ] 	Batch(5700/6809) done. Loss: 1.4261  lr:0.010000
[ Fri Jul 12 11:18:09 2024 ] 	Batch(5800/6809) done. Loss: 0.3494  lr:0.010000
[ Fri Jul 12 11:18:27 2024 ] 	Batch(5900/6809) done. Loss: 0.3270  lr:0.010000
[ Fri Jul 12 11:18:44 2024 ] 
Training: Epoch [40/120], Step [5999], Loss: 0.5177093148231506, Training Accuracy: 86.825
[ Fri Jul 12 11:18:45 2024 ] 	Batch(6000/6809) done. Loss: 0.4515  lr:0.010000
[ Fri Jul 12 11:19:03 2024 ] 	Batch(6100/6809) done. Loss: 0.3236  lr:0.010000
[ Fri Jul 12 11:19:21 2024 ] 	Batch(6200/6809) done. Loss: 0.3695  lr:0.010000
[ Fri Jul 12 11:19:39 2024 ] 	Batch(6300/6809) done. Loss: 0.7694  lr:0.010000
[ Fri Jul 12 11:19:57 2024 ] 	Batch(6400/6809) done. Loss: 0.1157  lr:0.010000
[ Fri Jul 12 11:20:14 2024 ] 
Training: Epoch [40/120], Step [6499], Loss: 0.2055657058954239, Training Accuracy: 86.75384615384615
[ Fri Jul 12 11:20:15 2024 ] 	Batch(6500/6809) done. Loss: 0.4992  lr:0.010000
[ Fri Jul 12 11:20:32 2024 ] 	Batch(6600/6809) done. Loss: 0.4831  lr:0.010000
[ Fri Jul 12 11:20:50 2024 ] 	Batch(6700/6809) done. Loss: 0.2800  lr:0.010000
[ Fri Jul 12 11:21:08 2024 ] 	Batch(6800/6809) done. Loss: 0.8712  lr:0.010000
[ Fri Jul 12 11:21:10 2024 ] 	Mean training loss: 0.4231.
[ Fri Jul 12 11:21:10 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 11:21:10 2024 ] Training epoch: 42
[ Fri Jul 12 11:21:11 2024 ] 	Batch(0/6809) done. Loss: 0.0922  lr:0.010000
[ Fri Jul 12 11:21:28 2024 ] 	Batch(100/6809) done. Loss: 0.2369  lr:0.010000
[ Fri Jul 12 11:21:46 2024 ] 	Batch(200/6809) done. Loss: 0.0316  lr:0.010000
[ Fri Jul 12 11:22:04 2024 ] 	Batch(300/6809) done. Loss: 0.0911  lr:0.010000
[ Fri Jul 12 11:22:22 2024 ] 	Batch(400/6809) done. Loss: 0.5118  lr:0.010000
[ Fri Jul 12 11:22:40 2024 ] 
Training: Epoch [41/120], Step [499], Loss: 0.09086647629737854, Training Accuracy: 87.02499999999999
[ Fri Jul 12 11:22:40 2024 ] 	Batch(500/6809) done. Loss: 0.1246  lr:0.010000
[ Fri Jul 12 11:22:58 2024 ] 	Batch(600/6809) done. Loss: 0.8190  lr:0.010000
[ Fri Jul 12 11:23:16 2024 ] 	Batch(700/6809) done. Loss: 0.1520  lr:0.010000
[ Fri Jul 12 11:23:34 2024 ] 	Batch(800/6809) done. Loss: 0.4317  lr:0.010000
[ Fri Jul 12 11:23:52 2024 ] 	Batch(900/6809) done. Loss: 0.5867  lr:0.010000
[ Fri Jul 12 11:24:09 2024 ] 
Training: Epoch [41/120], Step [999], Loss: 0.24352295696735382, Training Accuracy: 87.775
[ Fri Jul 12 11:24:10 2024 ] 	Batch(1000/6809) done. Loss: 0.3864  lr:0.010000
[ Fri Jul 12 11:24:28 2024 ] 	Batch(1100/6809) done. Loss: 0.9273  lr:0.010000
[ Fri Jul 12 11:24:46 2024 ] 	Batch(1200/6809) done. Loss: 0.7285  lr:0.010000
[ Fri Jul 12 11:25:03 2024 ] 	Batch(1300/6809) done. Loss: 0.2814  lr:0.010000
[ Fri Jul 12 11:25:21 2024 ] 	Batch(1400/6809) done. Loss: 0.0384  lr:0.010000
[ Fri Jul 12 11:25:39 2024 ] 
Training: Epoch [41/120], Step [1499], Loss: 0.10967326164245605, Training Accuracy: 87.86666666666667
[ Fri Jul 12 11:25:39 2024 ] 	Batch(1500/6809) done. Loss: 0.8614  lr:0.010000
[ Fri Jul 12 11:25:57 2024 ] 	Batch(1600/6809) done. Loss: 0.3542  lr:0.010000
[ Fri Jul 12 11:26:16 2024 ] 	Batch(1700/6809) done. Loss: 0.2105  lr:0.010000
[ Fri Jul 12 11:26:34 2024 ] 	Batch(1800/6809) done. Loss: 0.3816  lr:0.010000
[ Fri Jul 12 11:26:53 2024 ] 	Batch(1900/6809) done. Loss: 0.0858  lr:0.010000
[ Fri Jul 12 11:27:10 2024 ] 
Training: Epoch [41/120], Step [1999], Loss: 0.20276543498039246, Training Accuracy: 87.7625
[ Fri Jul 12 11:27:11 2024 ] 	Batch(2000/6809) done. Loss: 0.5379  lr:0.010000
[ Fri Jul 12 11:27:29 2024 ] 	Batch(2100/6809) done. Loss: 0.1217  lr:0.010000
[ Fri Jul 12 11:27:48 2024 ] 	Batch(2200/6809) done. Loss: 0.0836  lr:0.010000
[ Fri Jul 12 11:28:06 2024 ] 	Batch(2300/6809) done. Loss: 0.2624  lr:0.010000
[ Fri Jul 12 11:28:25 2024 ] 	Batch(2400/6809) done. Loss: 0.0647  lr:0.010000
[ Fri Jul 12 11:28:43 2024 ] 
Training: Epoch [41/120], Step [2499], Loss: 0.06868183612823486, Training Accuracy: 87.75
[ Fri Jul 12 11:28:43 2024 ] 	Batch(2500/6809) done. Loss: 0.1929  lr:0.010000
[ Fri Jul 12 11:29:01 2024 ] 	Batch(2600/6809) done. Loss: 0.3084  lr:0.010000
[ Fri Jul 12 11:29:19 2024 ] 	Batch(2700/6809) done. Loss: 0.8579  lr:0.010000
[ Fri Jul 12 11:29:37 2024 ] 	Batch(2800/6809) done. Loss: 0.2340  lr:0.010000
[ Fri Jul 12 11:29:55 2024 ] 	Batch(2900/6809) done. Loss: 0.1258  lr:0.010000
[ Fri Jul 12 11:30:13 2024 ] 
Training: Epoch [41/120], Step [2999], Loss: 0.6391214728355408, Training Accuracy: 87.61250000000001
[ Fri Jul 12 11:30:13 2024 ] 	Batch(3000/6809) done. Loss: 0.2521  lr:0.010000
[ Fri Jul 12 11:30:31 2024 ] 	Batch(3100/6809) done. Loss: 0.3932  lr:0.010000
[ Fri Jul 12 11:30:49 2024 ] 	Batch(3200/6809) done. Loss: 0.1986  lr:0.010000
[ Fri Jul 12 11:31:07 2024 ] 	Batch(3300/6809) done. Loss: 0.4438  lr:0.010000
[ Fri Jul 12 11:31:24 2024 ] 	Batch(3400/6809) done. Loss: 0.2282  lr:0.010000
[ Fri Jul 12 11:31:42 2024 ] 
Training: Epoch [41/120], Step [3499], Loss: 0.09426283091306686, Training Accuracy: 87.46785714285714
[ Fri Jul 12 11:31:42 2024 ] 	Batch(3500/6809) done. Loss: 0.4720  lr:0.010000
[ Fri Jul 12 11:32:00 2024 ] 	Batch(3600/6809) done. Loss: 0.5143  lr:0.010000
[ Fri Jul 12 11:32:18 2024 ] 	Batch(3700/6809) done. Loss: 0.4883  lr:0.010000
[ Fri Jul 12 11:32:36 2024 ] 	Batch(3800/6809) done. Loss: 0.1305  lr:0.010000
[ Fri Jul 12 11:32:54 2024 ] 	Batch(3900/6809) done. Loss: 0.0308  lr:0.010000
[ Fri Jul 12 11:33:12 2024 ] 
Training: Epoch [41/120], Step [3999], Loss: 0.8577373623847961, Training Accuracy: 87.503125
[ Fri Jul 12 11:33:12 2024 ] 	Batch(4000/6809) done. Loss: 0.1091  lr:0.010000
[ Fri Jul 12 11:33:30 2024 ] 	Batch(4100/6809) done. Loss: 0.9386  lr:0.010000
[ Fri Jul 12 11:33:48 2024 ] 	Batch(4200/6809) done. Loss: 0.3278  lr:0.010000
[ Fri Jul 12 11:34:06 2024 ] 	Batch(4300/6809) done. Loss: 0.2345  lr:0.010000
[ Fri Jul 12 11:34:24 2024 ] 	Batch(4400/6809) done. Loss: 0.5742  lr:0.010000
[ Fri Jul 12 11:34:42 2024 ] 
Training: Epoch [41/120], Step [4499], Loss: 0.16802076995372772, Training Accuracy: 87.40555555555557
[ Fri Jul 12 11:34:42 2024 ] 	Batch(4500/6809) done. Loss: 0.0725  lr:0.010000
[ Fri Jul 12 11:35:00 2024 ] 	Batch(4600/6809) done. Loss: 0.7727  lr:0.010000
[ Fri Jul 12 11:35:18 2024 ] 	Batch(4700/6809) done. Loss: 0.0278  lr:0.010000
[ Fri Jul 12 11:35:36 2024 ] 	Batch(4800/6809) done. Loss: 0.0901  lr:0.010000
[ Fri Jul 12 11:35:54 2024 ] 	Batch(4900/6809) done. Loss: 0.2888  lr:0.010000
[ Fri Jul 12 11:36:11 2024 ] 
Training: Epoch [41/120], Step [4999], Loss: 0.8025990128517151, Training Accuracy: 87.35000000000001
[ Fri Jul 12 11:36:11 2024 ] 	Batch(5000/6809) done. Loss: 0.6687  lr:0.010000
[ Fri Jul 12 11:36:29 2024 ] 	Batch(5100/6809) done. Loss: 1.2760  lr:0.010000
[ Fri Jul 12 11:36:47 2024 ] 	Batch(5200/6809) done. Loss: 0.4515  lr:0.010000
[ Fri Jul 12 11:37:06 2024 ] 	Batch(5300/6809) done. Loss: 0.4009  lr:0.010000
[ Fri Jul 12 11:37:24 2024 ] 	Batch(5400/6809) done. Loss: 0.7083  lr:0.010000
[ Fri Jul 12 11:37:43 2024 ] 
Training: Epoch [41/120], Step [5499], Loss: 0.7102154493331909, Training Accuracy: 87.14318181818182
[ Fri Jul 12 11:37:43 2024 ] 	Batch(5500/6809) done. Loss: 0.3022  lr:0.010000
[ Fri Jul 12 11:38:02 2024 ] 	Batch(5600/6809) done. Loss: 0.2351  lr:0.010000
[ Fri Jul 12 11:38:20 2024 ] 	Batch(5700/6809) done. Loss: 0.1068  lr:0.010000
[ Fri Jul 12 11:38:39 2024 ] 	Batch(5800/6809) done. Loss: 0.2956  lr:0.010000
[ Fri Jul 12 11:38:57 2024 ] 	Batch(5900/6809) done. Loss: 0.4364  lr:0.010000
[ Fri Jul 12 11:39:16 2024 ] 
Training: Epoch [41/120], Step [5999], Loss: 0.3690769672393799, Training Accuracy: 87.0375
[ Fri Jul 12 11:39:16 2024 ] 	Batch(6000/6809) done. Loss: 0.0124  lr:0.010000
[ Fri Jul 12 11:39:35 2024 ] 	Batch(6100/6809) done. Loss: 0.3456  lr:0.010000
[ Fri Jul 12 11:39:53 2024 ] 	Batch(6200/6809) done. Loss: 0.1692  lr:0.010000
[ Fri Jul 12 11:40:12 2024 ] 	Batch(6300/6809) done. Loss: 0.5904  lr:0.010000
[ Fri Jul 12 11:40:30 2024 ] 	Batch(6400/6809) done. Loss: 0.1761  lr:0.010000
[ Fri Jul 12 11:40:48 2024 ] 
Training: Epoch [41/120], Step [6499], Loss: 0.652908980846405, Training Accuracy: 86.89807692307693
[ Fri Jul 12 11:40:48 2024 ] 	Batch(6500/6809) done. Loss: 0.5388  lr:0.010000
[ Fri Jul 12 11:41:06 2024 ] 	Batch(6600/6809) done. Loss: 0.0345  lr:0.010000
[ Fri Jul 12 11:41:24 2024 ] 	Batch(6700/6809) done. Loss: 0.2489  lr:0.010000
[ Fri Jul 12 11:41:42 2024 ] 	Batch(6800/6809) done. Loss: 0.1032  lr:0.010000
[ Fri Jul 12 11:41:44 2024 ] 	Mean training loss: 0.4105.
[ Fri Jul 12 11:41:44 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 11:41:44 2024 ] Training epoch: 43
[ Fri Jul 12 11:41:44 2024 ] 	Batch(0/6809) done. Loss: 0.3675  lr:0.010000
[ Fri Jul 12 11:42:03 2024 ] 	Batch(100/6809) done. Loss: 0.5951  lr:0.010000
[ Fri Jul 12 11:42:21 2024 ] 	Batch(200/6809) done. Loss: 0.5135  lr:0.010000
[ Fri Jul 12 11:42:39 2024 ] 	Batch(300/6809) done. Loss: 0.2626  lr:0.010000
[ Fri Jul 12 11:42:57 2024 ] 	Batch(400/6809) done. Loss: 0.4651  lr:0.010000
[ Fri Jul 12 11:43:14 2024 ] 
Training: Epoch [42/120], Step [499], Loss: 0.10785095393657684, Training Accuracy: 87.925
[ Fri Jul 12 11:43:15 2024 ] 	Batch(500/6809) done. Loss: 0.1348  lr:0.010000
[ Fri Jul 12 11:43:32 2024 ] 	Batch(600/6809) done. Loss: 0.5737  lr:0.010000
[ Fri Jul 12 11:43:50 2024 ] 	Batch(700/6809) done. Loss: 0.2656  lr:0.010000
[ Fri Jul 12 11:44:08 2024 ] 	Batch(800/6809) done. Loss: 0.4528  lr:0.010000
[ Fri Jul 12 11:44:26 2024 ] 	Batch(900/6809) done. Loss: 0.3423  lr:0.010000
[ Fri Jul 12 11:44:44 2024 ] 
Training: Epoch [42/120], Step [999], Loss: 0.14505383372306824, Training Accuracy: 88.0625
[ Fri Jul 12 11:44:44 2024 ] 	Batch(1000/6809) done. Loss: 0.4303  lr:0.010000
[ Fri Jul 12 11:45:02 2024 ] 	Batch(1100/6809) done. Loss: 1.4631  lr:0.010000
[ Fri Jul 12 11:45:20 2024 ] 	Batch(1200/6809) done. Loss: 0.0687  lr:0.010000
[ Fri Jul 12 11:45:38 2024 ] 	Batch(1300/6809) done. Loss: 0.3595  lr:0.010000
[ Fri Jul 12 11:45:56 2024 ] 	Batch(1400/6809) done. Loss: 0.0034  lr:0.010000
[ Fri Jul 12 11:46:14 2024 ] 
Training: Epoch [42/120], Step [1499], Loss: 0.2126353532075882, Training Accuracy: 88.18333333333334
[ Fri Jul 12 11:46:14 2024 ] 	Batch(1500/6809) done. Loss: 0.4238  lr:0.010000
[ Fri Jul 12 11:46:32 2024 ] 	Batch(1600/6809) done. Loss: 0.3192  lr:0.010000
[ Fri Jul 12 11:46:50 2024 ] 	Batch(1700/6809) done. Loss: 0.0382  lr:0.010000
[ Fri Jul 12 11:47:08 2024 ] 	Batch(1800/6809) done. Loss: 0.4143  lr:0.010000
[ Fri Jul 12 11:47:26 2024 ] 	Batch(1900/6809) done. Loss: 0.2668  lr:0.010000
[ Fri Jul 12 11:47:43 2024 ] 
Training: Epoch [42/120], Step [1999], Loss: 0.030539048835635185, Training Accuracy: 88.25
[ Fri Jul 12 11:47:44 2024 ] 	Batch(2000/6809) done. Loss: 0.5307  lr:0.010000
[ Fri Jul 12 11:48:02 2024 ] 	Batch(2100/6809) done. Loss: 0.3023  lr:0.010000
[ Fri Jul 12 11:48:20 2024 ] 	Batch(2200/6809) done. Loss: 0.3676  lr:0.010000
[ Fri Jul 12 11:48:37 2024 ] 	Batch(2300/6809) done. Loss: 0.1136  lr:0.010000
[ Fri Jul 12 11:48:56 2024 ] 	Batch(2400/6809) done. Loss: 0.4558  lr:0.010000
[ Fri Jul 12 11:49:14 2024 ] 
Training: Epoch [42/120], Step [2499], Loss: 0.1473558396100998, Training Accuracy: 88.11
[ Fri Jul 12 11:49:14 2024 ] 	Batch(2500/6809) done. Loss: 0.4031  lr:0.010000
[ Fri Jul 12 11:49:33 2024 ] 	Batch(2600/6809) done. Loss: 0.1072  lr:0.010000
[ Fri Jul 12 11:49:51 2024 ] 	Batch(2700/6809) done. Loss: 1.2168  lr:0.010000
[ Fri Jul 12 11:50:10 2024 ] 	Batch(2800/6809) done. Loss: 0.1463  lr:0.010000
[ Fri Jul 12 11:50:28 2024 ] 	Batch(2900/6809) done. Loss: 0.2437  lr:0.010000
[ Fri Jul 12 11:50:46 2024 ] 
Training: Epoch [42/120], Step [2999], Loss: 0.1540948897600174, Training Accuracy: 88.04166666666666
[ Fri Jul 12 11:50:46 2024 ] 	Batch(3000/6809) done. Loss: 0.0723  lr:0.010000
[ Fri Jul 12 11:51:04 2024 ] 	Batch(3100/6809) done. Loss: 0.3863  lr:0.010000
[ Fri Jul 12 11:51:22 2024 ] 	Batch(3200/6809) done. Loss: 0.2719  lr:0.010000
[ Fri Jul 12 11:51:40 2024 ] 	Batch(3300/6809) done. Loss: 0.7238  lr:0.010000
[ Fri Jul 12 11:51:57 2024 ] 	Batch(3400/6809) done. Loss: 0.5786  lr:0.010000
[ Fri Jul 12 11:52:15 2024 ] 
Training: Epoch [42/120], Step [3499], Loss: 0.03407222777605057, Training Accuracy: 87.80357142857143
[ Fri Jul 12 11:52:15 2024 ] 	Batch(3500/6809) done. Loss: 0.2864  lr:0.010000
[ Fri Jul 12 11:52:33 2024 ] 	Batch(3600/6809) done. Loss: 0.0840  lr:0.010000
[ Fri Jul 12 11:52:51 2024 ] 	Batch(3700/6809) done. Loss: 0.5191  lr:0.010000
[ Fri Jul 12 11:53:09 2024 ] 	Batch(3800/6809) done. Loss: 0.2380  lr:0.010000
[ Fri Jul 12 11:53:27 2024 ] 	Batch(3900/6809) done. Loss: 1.1589  lr:0.010000
[ Fri Jul 12 11:53:45 2024 ] 
Training: Epoch [42/120], Step [3999], Loss: 0.3173646628856659, Training Accuracy: 87.52499999999999
[ Fri Jul 12 11:53:45 2024 ] 	Batch(4000/6809) done. Loss: 0.1934  lr:0.010000
[ Fri Jul 12 11:54:03 2024 ] 	Batch(4100/6809) done. Loss: 0.2435  lr:0.010000
[ Fri Jul 12 11:54:21 2024 ] 	Batch(4200/6809) done. Loss: 0.0197  lr:0.010000
[ Fri Jul 12 11:54:39 2024 ] 	Batch(4300/6809) done. Loss: 0.1792  lr:0.010000
[ Fri Jul 12 11:54:57 2024 ] 	Batch(4400/6809) done. Loss: 0.7032  lr:0.010000
[ Fri Jul 12 11:55:15 2024 ] 
Training: Epoch [42/120], Step [4499], Loss: 0.02255559153854847, Training Accuracy: 87.42777777777778
[ Fri Jul 12 11:55:16 2024 ] 	Batch(4500/6809) done. Loss: 0.6689  lr:0.010000
[ Fri Jul 12 11:55:34 2024 ] 	Batch(4600/6809) done. Loss: 0.2167  lr:0.010000
[ Fri Jul 12 11:55:51 2024 ] 	Batch(4700/6809) done. Loss: 0.6529  lr:0.010000
[ Fri Jul 12 11:56:10 2024 ] 	Batch(4800/6809) done. Loss: 0.1081  lr:0.010000
[ Fri Jul 12 11:56:27 2024 ] 	Batch(4900/6809) done. Loss: 0.3697  lr:0.010000
[ Fri Jul 12 11:56:45 2024 ] 
Training: Epoch [42/120], Step [4999], Loss: 0.11501799523830414, Training Accuracy: 87.4175
[ Fri Jul 12 11:56:45 2024 ] 	Batch(5000/6809) done. Loss: 0.3907  lr:0.010000
[ Fri Jul 12 11:57:03 2024 ] 	Batch(5100/6809) done. Loss: 0.5513  lr:0.010000
[ Fri Jul 12 11:57:21 2024 ] 	Batch(5200/6809) done. Loss: 1.0915  lr:0.010000
[ Fri Jul 12 11:57:39 2024 ] 	Batch(5300/6809) done. Loss: 0.0414  lr:0.010000
[ Fri Jul 12 11:57:57 2024 ] 	Batch(5400/6809) done. Loss: 0.3366  lr:0.010000
[ Fri Jul 12 11:58:15 2024 ] 
Training: Epoch [42/120], Step [5499], Loss: 0.7426350712776184, Training Accuracy: 87.36136363636363
[ Fri Jul 12 11:58:15 2024 ] 	Batch(5500/6809) done. Loss: 0.2071  lr:0.010000
[ Fri Jul 12 11:58:33 2024 ] 	Batch(5600/6809) done. Loss: 1.1878  lr:0.010000
[ Fri Jul 12 11:58:51 2024 ] 	Batch(5700/6809) done. Loss: 0.1728  lr:0.010000
[ Fri Jul 12 11:59:09 2024 ] 	Batch(5800/6809) done. Loss: 0.3006  lr:0.010000
[ Fri Jul 12 11:59:27 2024 ] 	Batch(5900/6809) done. Loss: 0.4076  lr:0.010000
[ Fri Jul 12 11:59:44 2024 ] 
Training: Epoch [42/120], Step [5999], Loss: 0.9281755685806274, Training Accuracy: 87.30624999999999
[ Fri Jul 12 11:59:45 2024 ] 	Batch(6000/6809) done. Loss: 0.4051  lr:0.010000
[ Fri Jul 12 12:00:03 2024 ] 	Batch(6100/6809) done. Loss: 0.0833  lr:0.010000
[ Fri Jul 12 12:00:20 2024 ] 	Batch(6200/6809) done. Loss: 0.2820  lr:0.010000
[ Fri Jul 12 12:00:38 2024 ] 	Batch(6300/6809) done. Loss: 0.0851  lr:0.010000
[ Fri Jul 12 12:00:56 2024 ] 	Batch(6400/6809) done. Loss: 0.6236  lr:0.010000
[ Fri Jul 12 12:01:14 2024 ] 
Training: Epoch [42/120], Step [6499], Loss: 0.14097027480602264, Training Accuracy: 87.27115384615385
[ Fri Jul 12 12:01:14 2024 ] 	Batch(6500/6809) done. Loss: 0.2680  lr:0.010000
[ Fri Jul 12 12:01:32 2024 ] 	Batch(6600/6809) done. Loss: 0.1463  lr:0.010000
[ Fri Jul 12 12:01:50 2024 ] 	Batch(6700/6809) done. Loss: 0.7750  lr:0.010000
[ Fri Jul 12 12:02:08 2024 ] 	Batch(6800/6809) done. Loss: 0.0795  lr:0.010000
[ Fri Jul 12 12:02:10 2024 ] 	Mean training loss: 0.4011.
[ Fri Jul 12 12:02:10 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 12:02:10 2024 ] Training epoch: 44
[ Fri Jul 12 12:02:10 2024 ] 	Batch(0/6809) done. Loss: 0.0885  lr:0.010000
[ Fri Jul 12 12:02:28 2024 ] 	Batch(100/6809) done. Loss: 0.0771  lr:0.010000
[ Fri Jul 12 12:02:46 2024 ] 	Batch(200/6809) done. Loss: 0.0953  lr:0.010000
[ Fri Jul 12 12:03:04 2024 ] 	Batch(300/6809) done. Loss: 0.2210  lr:0.010000
[ Fri Jul 12 12:03:22 2024 ] 	Batch(400/6809) done. Loss: 0.0661  lr:0.010000
[ Fri Jul 12 12:03:40 2024 ] 
Training: Epoch [43/120], Step [499], Loss: 0.7947100400924683, Training Accuracy: 88.875
[ Fri Jul 12 12:03:40 2024 ] 	Batch(500/6809) done. Loss: 0.4695  lr:0.010000
[ Fri Jul 12 12:03:58 2024 ] 	Batch(600/6809) done. Loss: 0.2735  lr:0.010000
[ Fri Jul 12 12:04:16 2024 ] 	Batch(700/6809) done. Loss: 0.6760  lr:0.010000
[ Fri Jul 12 12:04:34 2024 ] 	Batch(800/6809) done. Loss: 0.8936  lr:0.010000
[ Fri Jul 12 12:04:52 2024 ] 	Batch(900/6809) done. Loss: 0.0589  lr:0.010000
[ Fri Jul 12 12:05:10 2024 ] 
Training: Epoch [43/120], Step [999], Loss: 0.18643063306808472, Training Accuracy: 88.1625
[ Fri Jul 12 12:05:10 2024 ] 	Batch(1000/6809) done. Loss: 0.3465  lr:0.010000
[ Fri Jul 12 12:05:28 2024 ] 	Batch(1100/6809) done. Loss: 0.4669  lr:0.010000
[ Fri Jul 12 12:05:46 2024 ] 	Batch(1200/6809) done. Loss: 0.4907  lr:0.010000
[ Fri Jul 12 12:06:05 2024 ] 	Batch(1300/6809) done. Loss: 0.2735  lr:0.010000
[ Fri Jul 12 12:06:23 2024 ] 	Batch(1400/6809) done. Loss: 0.2580  lr:0.010000
[ Fri Jul 12 12:06:40 2024 ] 
Training: Epoch [43/120], Step [1499], Loss: 0.03422413393855095, Training Accuracy: 88.36666666666667
[ Fri Jul 12 12:06:40 2024 ] 	Batch(1500/6809) done. Loss: 0.2260  lr:0.010000
[ Fri Jul 12 12:06:58 2024 ] 	Batch(1600/6809) done. Loss: 0.5959  lr:0.010000
[ Fri Jul 12 12:07:16 2024 ] 	Batch(1700/6809) done. Loss: 0.3825  lr:0.010000
[ Fri Jul 12 12:07:34 2024 ] 	Batch(1800/6809) done. Loss: 0.2244  lr:0.010000
[ Fri Jul 12 12:07:52 2024 ] 	Batch(1900/6809) done. Loss: 0.5537  lr:0.010000
[ Fri Jul 12 12:08:10 2024 ] 
Training: Epoch [43/120], Step [1999], Loss: 0.10584460198879242, Training Accuracy: 88.47500000000001
[ Fri Jul 12 12:08:10 2024 ] 	Batch(2000/6809) done. Loss: 0.6724  lr:0.010000
[ Fri Jul 12 12:08:28 2024 ] 	Batch(2100/6809) done. Loss: 0.6059  lr:0.010000
[ Fri Jul 12 12:08:46 2024 ] 	Batch(2200/6809) done. Loss: 0.3286  lr:0.010000
[ Fri Jul 12 12:09:04 2024 ] 	Batch(2300/6809) done. Loss: 0.9851  lr:0.010000
[ Fri Jul 12 12:09:22 2024 ] 	Batch(2400/6809) done. Loss: 0.1621  lr:0.010000
[ Fri Jul 12 12:09:40 2024 ] 
Training: Epoch [43/120], Step [2499], Loss: 0.20222318172454834, Training Accuracy: 88.13
[ Fri Jul 12 12:09:40 2024 ] 	Batch(2500/6809) done. Loss: 0.0361  lr:0.010000
[ Fri Jul 12 12:09:58 2024 ] 	Batch(2600/6809) done. Loss: 0.0977  lr:0.010000
[ Fri Jul 12 12:10:15 2024 ] 	Batch(2700/6809) done. Loss: 0.5328  lr:0.010000
[ Fri Jul 12 12:10:34 2024 ] 	Batch(2800/6809) done. Loss: 0.8581  lr:0.010000
[ Fri Jul 12 12:10:51 2024 ] 	Batch(2900/6809) done. Loss: 0.3351  lr:0.010000
[ Fri Jul 12 12:11:09 2024 ] 
Training: Epoch [43/120], Step [2999], Loss: 0.21168595552444458, Training Accuracy: 87.80833333333334
[ Fri Jul 12 12:11:09 2024 ] 	Batch(3000/6809) done. Loss: 0.7595  lr:0.010000
[ Fri Jul 12 12:11:27 2024 ] 	Batch(3100/6809) done. Loss: 0.6977  lr:0.010000
[ Fri Jul 12 12:11:45 2024 ] 	Batch(3200/6809) done. Loss: 0.1197  lr:0.010000
[ Fri Jul 12 12:12:03 2024 ] 	Batch(3300/6809) done. Loss: 0.1519  lr:0.010000
[ Fri Jul 12 12:12:21 2024 ] 	Batch(3400/6809) done. Loss: 0.1096  lr:0.010000
[ Fri Jul 12 12:12:39 2024 ] 
Training: Epoch [43/120], Step [3499], Loss: 0.27903831005096436, Training Accuracy: 87.72857142857143
[ Fri Jul 12 12:12:39 2024 ] 	Batch(3500/6809) done. Loss: 0.0553  lr:0.010000
[ Fri Jul 12 12:12:57 2024 ] 	Batch(3600/6809) done. Loss: 0.1957  lr:0.010000
[ Fri Jul 12 12:13:15 2024 ] 	Batch(3700/6809) done. Loss: 0.5597  lr:0.010000
[ Fri Jul 12 12:13:33 2024 ] 	Batch(3800/6809) done. Loss: 0.2554  lr:0.010000
[ Fri Jul 12 12:13:51 2024 ] 	Batch(3900/6809) done. Loss: 0.2868  lr:0.010000
[ Fri Jul 12 12:14:08 2024 ] 
Training: Epoch [43/120], Step [3999], Loss: 1.3670430183410645, Training Accuracy: 87.728125
[ Fri Jul 12 12:14:09 2024 ] 	Batch(4000/6809) done. Loss: 0.1292  lr:0.010000
[ Fri Jul 12 12:14:26 2024 ] 	Batch(4100/6809) done. Loss: 0.0668  lr:0.010000
[ Fri Jul 12 12:14:44 2024 ] 	Batch(4200/6809) done. Loss: 0.7551  lr:0.010000
[ Fri Jul 12 12:15:02 2024 ] 	Batch(4300/6809) done. Loss: 0.1838  lr:0.010000
[ Fri Jul 12 12:15:20 2024 ] 	Batch(4400/6809) done. Loss: 0.1471  lr:0.010000
[ Fri Jul 12 12:15:39 2024 ] 
Training: Epoch [43/120], Step [4499], Loss: 0.11604195088148117, Training Accuracy: 87.67222222222222
[ Fri Jul 12 12:15:39 2024 ] 	Batch(4500/6809) done. Loss: 0.1302  lr:0.010000
[ Fri Jul 12 12:15:57 2024 ] 	Batch(4600/6809) done. Loss: 0.1319  lr:0.010000
[ Fri Jul 12 12:16:15 2024 ] 	Batch(4700/6809) done. Loss: 0.2343  lr:0.010000
[ Fri Jul 12 12:16:33 2024 ] 	Batch(4800/6809) done. Loss: 0.0595  lr:0.010000
[ Fri Jul 12 12:16:51 2024 ] 	Batch(4900/6809) done. Loss: 0.2838  lr:0.010000
[ Fri Jul 12 12:17:09 2024 ] 
Training: Epoch [43/120], Step [4999], Loss: 0.258735328912735, Training Accuracy: 87.62
[ Fri Jul 12 12:17:09 2024 ] 	Batch(5000/6809) done. Loss: 0.4341  lr:0.010000
[ Fri Jul 12 12:17:27 2024 ] 	Batch(5100/6809) done. Loss: 0.8822  lr:0.010000
[ Fri Jul 12 12:17:45 2024 ] 	Batch(5200/6809) done. Loss: 0.9683  lr:0.010000
[ Fri Jul 12 12:18:03 2024 ] 	Batch(5300/6809) done. Loss: 0.4431  lr:0.010000
[ Fri Jul 12 12:18:21 2024 ] 	Batch(5400/6809) done. Loss: 0.5050  lr:0.010000
[ Fri Jul 12 12:18:39 2024 ] 
Training: Epoch [43/120], Step [5499], Loss: 0.793601930141449, Training Accuracy: 87.49545454545455
[ Fri Jul 12 12:18:39 2024 ] 	Batch(5500/6809) done. Loss: 0.1859  lr:0.010000
[ Fri Jul 12 12:18:57 2024 ] 	Batch(5600/6809) done. Loss: 0.4201  lr:0.010000
[ Fri Jul 12 12:19:15 2024 ] 	Batch(5700/6809) done. Loss: 0.7249  lr:0.010000
[ Fri Jul 12 12:19:34 2024 ] 	Batch(5800/6809) done. Loss: 0.0594  lr:0.010000
[ Fri Jul 12 12:19:53 2024 ] 	Batch(5900/6809) done. Loss: 0.3626  lr:0.010000
[ Fri Jul 12 12:20:11 2024 ] 
Training: Epoch [43/120], Step [5999], Loss: 0.10863213986158371, Training Accuracy: 87.50416666666668
[ Fri Jul 12 12:20:11 2024 ] 	Batch(6000/6809) done. Loss: 0.2455  lr:0.010000
[ Fri Jul 12 12:20:30 2024 ] 	Batch(6100/6809) done. Loss: 0.1815  lr:0.010000
[ Fri Jul 12 12:20:48 2024 ] 	Batch(6200/6809) done. Loss: 0.8571  lr:0.010000
[ Fri Jul 12 12:21:05 2024 ] 	Batch(6300/6809) done. Loss: 0.1318  lr:0.010000
[ Fri Jul 12 12:21:23 2024 ] 	Batch(6400/6809) done. Loss: 0.9795  lr:0.010000
[ Fri Jul 12 12:21:41 2024 ] 
Training: Epoch [43/120], Step [6499], Loss: 0.11860861629247665, Training Accuracy: 87.39423076923077
[ Fri Jul 12 12:21:41 2024 ] 	Batch(6500/6809) done. Loss: 0.1617  lr:0.010000
[ Fri Jul 12 12:21:59 2024 ] 	Batch(6600/6809) done. Loss: 1.5728  lr:0.010000
[ Fri Jul 12 12:22:17 2024 ] 	Batch(6700/6809) done. Loss: 0.1402  lr:0.010000
[ Fri Jul 12 12:22:35 2024 ] 	Batch(6800/6809) done. Loss: 0.3213  lr:0.010000
[ Fri Jul 12 12:22:37 2024 ] 	Mean training loss: 0.4010.
[ Fri Jul 12 12:22:37 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 12:22:37 2024 ] Training epoch: 45
[ Fri Jul 12 12:22:37 2024 ] 	Batch(0/6809) done. Loss: 0.2303  lr:0.010000
[ Fri Jul 12 12:22:55 2024 ] 	Batch(100/6809) done. Loss: 0.2067  lr:0.010000
[ Fri Jul 12 12:23:14 2024 ] 	Batch(200/6809) done. Loss: 0.3322  lr:0.010000
[ Fri Jul 12 12:23:32 2024 ] 	Batch(300/6809) done. Loss: 0.0593  lr:0.010000
[ Fri Jul 12 12:23:51 2024 ] 	Batch(400/6809) done. Loss: 0.3720  lr:0.010000
[ Fri Jul 12 12:24:09 2024 ] 
Training: Epoch [44/120], Step [499], Loss: 0.1421060562133789, Training Accuracy: 88.925
[ Fri Jul 12 12:24:09 2024 ] 	Batch(500/6809) done. Loss: 0.0600  lr:0.010000
[ Fri Jul 12 12:24:27 2024 ] 	Batch(600/6809) done. Loss: 0.0263  lr:0.010000
[ Fri Jul 12 12:24:46 2024 ] 	Batch(700/6809) done. Loss: 0.0099  lr:0.010000
[ Fri Jul 12 12:25:04 2024 ] 	Batch(800/6809) done. Loss: 0.0762  lr:0.010000
[ Fri Jul 12 12:25:22 2024 ] 	Batch(900/6809) done. Loss: 0.4412  lr:0.010000
[ Fri Jul 12 12:25:40 2024 ] 
Training: Epoch [44/120], Step [999], Loss: 0.3085044026374817, Training Accuracy: 88.725
[ Fri Jul 12 12:25:41 2024 ] 	Batch(1000/6809) done. Loss: 0.1322  lr:0.010000
[ Fri Jul 12 12:25:59 2024 ] 	Batch(1100/6809) done. Loss: 0.3485  lr:0.010000
[ Fri Jul 12 12:26:18 2024 ] 	Batch(1200/6809) done. Loss: 0.3945  lr:0.010000
[ Fri Jul 12 12:26:36 2024 ] 	Batch(1300/6809) done. Loss: 0.1011  lr:0.010000
[ Fri Jul 12 12:26:54 2024 ] 	Batch(1400/6809) done. Loss: 0.6055  lr:0.010000
[ Fri Jul 12 12:27:12 2024 ] 
Training: Epoch [44/120], Step [1499], Loss: 0.5530819296836853, Training Accuracy: 88.275
[ Fri Jul 12 12:27:12 2024 ] 	Batch(1500/6809) done. Loss: 0.2291  lr:0.010000
[ Fri Jul 12 12:27:30 2024 ] 	Batch(1600/6809) done. Loss: 0.1548  lr:0.010000
[ Fri Jul 12 12:27:48 2024 ] 	Batch(1700/6809) done. Loss: 0.2946  lr:0.010000
[ Fri Jul 12 12:28:06 2024 ] 	Batch(1800/6809) done. Loss: 0.3317  lr:0.010000
[ Fri Jul 12 12:28:24 2024 ] 	Batch(1900/6809) done. Loss: 0.1358  lr:0.010000
[ Fri Jul 12 12:28:41 2024 ] 
Training: Epoch [44/120], Step [1999], Loss: 0.3001764118671417, Training Accuracy: 88.05
[ Fri Jul 12 12:28:42 2024 ] 	Batch(2000/6809) done. Loss: 0.3734  lr:0.010000
[ Fri Jul 12 12:29:00 2024 ] 	Batch(2100/6809) done. Loss: 0.3333  lr:0.010000
[ Fri Jul 12 12:29:17 2024 ] 	Batch(2200/6809) done. Loss: 0.9447  lr:0.010000
[ Fri Jul 12 12:29:35 2024 ] 	Batch(2300/6809) done. Loss: 0.1087  lr:0.010000
[ Fri Jul 12 12:29:53 2024 ] 	Batch(2400/6809) done. Loss: 0.5945  lr:0.010000
[ Fri Jul 12 12:30:12 2024 ] 
Training: Epoch [44/120], Step [2499], Loss: 0.7245604991912842, Training Accuracy: 88.055
[ Fri Jul 12 12:30:12 2024 ] 	Batch(2500/6809) done. Loss: 0.6592  lr:0.010000
[ Fri Jul 12 12:30:30 2024 ] 	Batch(2600/6809) done. Loss: 0.3593  lr:0.010000
[ Fri Jul 12 12:30:48 2024 ] 	Batch(2700/6809) done. Loss: 0.0884  lr:0.010000
[ Fri Jul 12 12:31:06 2024 ] 	Batch(2800/6809) done. Loss: 0.2569  lr:0.010000
[ Fri Jul 12 12:31:24 2024 ] 	Batch(2900/6809) done. Loss: 0.6002  lr:0.010000
[ Fri Jul 12 12:31:41 2024 ] 
Training: Epoch [44/120], Step [2999], Loss: 0.39908498525619507, Training Accuracy: 87.92916666666667
[ Fri Jul 12 12:31:42 2024 ] 	Batch(3000/6809) done. Loss: 0.0532  lr:0.010000
[ Fri Jul 12 12:31:59 2024 ] 	Batch(3100/6809) done. Loss: 1.1680  lr:0.010000
[ Fri Jul 12 12:32:18 2024 ] 	Batch(3200/6809) done. Loss: 0.9287  lr:0.010000
[ Fri Jul 12 12:32:36 2024 ] 	Batch(3300/6809) done. Loss: 0.3831  lr:0.010000
[ Fri Jul 12 12:32:54 2024 ] 	Batch(3400/6809) done. Loss: 0.1058  lr:0.010000
[ Fri Jul 12 12:33:12 2024 ] 
Training: Epoch [44/120], Step [3499], Loss: 0.7714945673942566, Training Accuracy: 87.89285714285714
[ Fri Jul 12 12:33:12 2024 ] 	Batch(3500/6809) done. Loss: 0.4670  lr:0.010000
[ Fri Jul 12 12:33:30 2024 ] 	Batch(3600/6809) done. Loss: 0.0470  lr:0.010000
[ Fri Jul 12 12:33:48 2024 ] 	Batch(3700/6809) done. Loss: 0.0967  lr:0.010000
[ Fri Jul 12 12:34:06 2024 ] 	Batch(3800/6809) done. Loss: 0.1754  lr:0.010000
[ Fri Jul 12 12:34:23 2024 ] 	Batch(3900/6809) done. Loss: 0.0454  lr:0.010000
[ Fri Jul 12 12:34:41 2024 ] 
Training: Epoch [44/120], Step [3999], Loss: 0.451036274433136, Training Accuracy: 87.7875
[ Fri Jul 12 12:34:42 2024 ] 	Batch(4000/6809) done. Loss: 0.2757  lr:0.010000
[ Fri Jul 12 12:34:59 2024 ] 	Batch(4100/6809) done. Loss: 1.2183  lr:0.010000
[ Fri Jul 12 12:35:17 2024 ] 	Batch(4200/6809) done. Loss: 0.1441  lr:0.010000
[ Fri Jul 12 12:35:35 2024 ] 	Batch(4300/6809) done. Loss: 0.8623  lr:0.010000
[ Fri Jul 12 12:35:53 2024 ] 	Batch(4400/6809) done. Loss: 1.0442  lr:0.010000
[ Fri Jul 12 12:36:11 2024 ] 
Training: Epoch [44/120], Step [4499], Loss: 0.1023474708199501, Training Accuracy: 87.66666666666667
[ Fri Jul 12 12:36:11 2024 ] 	Batch(4500/6809) done. Loss: 0.6943  lr:0.010000
[ Fri Jul 12 12:36:29 2024 ] 	Batch(4600/6809) done. Loss: 0.0258  lr:0.010000
[ Fri Jul 12 12:36:47 2024 ] 	Batch(4700/6809) done. Loss: 0.7265  lr:0.010000
[ Fri Jul 12 12:37:05 2024 ] 	Batch(4800/6809) done. Loss: 0.3697  lr:0.010000
[ Fri Jul 12 12:37:23 2024 ] 	Batch(4900/6809) done. Loss: 0.4389  lr:0.010000
[ Fri Jul 12 12:37:41 2024 ] 
Training: Epoch [44/120], Step [4999], Loss: 1.344956874847412, Training Accuracy: 87.5825
[ Fri Jul 12 12:37:41 2024 ] 	Batch(5000/6809) done. Loss: 0.6077  lr:0.010000
[ Fri Jul 12 12:37:59 2024 ] 	Batch(5100/6809) done. Loss: 0.9066  lr:0.010000
[ Fri Jul 12 12:38:17 2024 ] 	Batch(5200/6809) done. Loss: 0.1067  lr:0.010000
[ Fri Jul 12 12:38:34 2024 ] 	Batch(5300/6809) done. Loss: 0.9302  lr:0.010000
[ Fri Jul 12 12:38:52 2024 ] 	Batch(5400/6809) done. Loss: 0.2439  lr:0.010000
[ Fri Jul 12 12:39:10 2024 ] 
Training: Epoch [44/120], Step [5499], Loss: 0.06234198808670044, Training Accuracy: 87.5840909090909
[ Fri Jul 12 12:39:10 2024 ] 	Batch(5500/6809) done. Loss: 0.1967  lr:0.010000
[ Fri Jul 12 12:39:28 2024 ] 	Batch(5600/6809) done. Loss: 0.1115  lr:0.010000
[ Fri Jul 12 12:39:46 2024 ] 	Batch(5700/6809) done. Loss: 0.8640  lr:0.010000
[ Fri Jul 12 12:40:04 2024 ] 	Batch(5800/6809) done. Loss: 0.2914  lr:0.010000
[ Fri Jul 12 12:40:22 2024 ] 	Batch(5900/6809) done. Loss: 0.4190  lr:0.010000
[ Fri Jul 12 12:40:40 2024 ] 
Training: Epoch [44/120], Step [5999], Loss: 0.02021394856274128, Training Accuracy: 87.5375
[ Fri Jul 12 12:40:40 2024 ] 	Batch(6000/6809) done. Loss: 0.5550  lr:0.010000
[ Fri Jul 12 12:40:58 2024 ] 	Batch(6100/6809) done. Loss: 0.2378  lr:0.010000
[ Fri Jul 12 12:41:16 2024 ] 	Batch(6200/6809) done. Loss: 0.1376  lr:0.010000
[ Fri Jul 12 12:41:34 2024 ] 	Batch(6300/6809) done. Loss: 0.8302  lr:0.010000
[ Fri Jul 12 12:41:52 2024 ] 	Batch(6400/6809) done. Loss: 0.0824  lr:0.010000
[ Fri Jul 12 12:42:09 2024 ] 
Training: Epoch [44/120], Step [6499], Loss: 0.45814818143844604, Training Accuracy: 87.52115384615384
[ Fri Jul 12 12:42:09 2024 ] 	Batch(6500/6809) done. Loss: 0.2213  lr:0.010000
[ Fri Jul 12 12:42:27 2024 ] 	Batch(6600/6809) done. Loss: 0.2541  lr:0.010000
[ Fri Jul 12 12:42:45 2024 ] 	Batch(6700/6809) done. Loss: 1.1060  lr:0.010000
[ Fri Jul 12 12:43:04 2024 ] 	Batch(6800/6809) done. Loss: 0.3390  lr:0.010000
[ Fri Jul 12 12:43:05 2024 ] 	Mean training loss: 0.3885.
[ Fri Jul 12 12:43:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 12:43:05 2024 ] Training epoch: 46
[ Fri Jul 12 12:43:06 2024 ] 	Batch(0/6809) done. Loss: 0.0428  lr:0.010000
[ Fri Jul 12 12:43:24 2024 ] 	Batch(100/6809) done. Loss: 0.2239  lr:0.010000
[ Fri Jul 12 12:43:42 2024 ] 	Batch(200/6809) done. Loss: 1.0297  lr:0.010000
[ Fri Jul 12 12:44:00 2024 ] 	Batch(300/6809) done. Loss: 0.4672  lr:0.010000
[ Fri Jul 12 12:44:18 2024 ] 	Batch(400/6809) done. Loss: 0.3890  lr:0.010000
[ Fri Jul 12 12:44:36 2024 ] 
Training: Epoch [45/120], Step [499], Loss: 0.06808817386627197, Training Accuracy: 88.64999999999999
[ Fri Jul 12 12:44:36 2024 ] 	Batch(500/6809) done. Loss: 0.3025  lr:0.010000
[ Fri Jul 12 12:44:54 2024 ] 	Batch(600/6809) done. Loss: 0.3687  lr:0.010000
[ Fri Jul 12 12:45:12 2024 ] 	Batch(700/6809) done. Loss: 0.2076  lr:0.010000
[ Fri Jul 12 12:45:30 2024 ] 	Batch(800/6809) done. Loss: 0.1075  lr:0.010000
[ Fri Jul 12 12:45:49 2024 ] 	Batch(900/6809) done. Loss: 0.5459  lr:0.010000
[ Fri Jul 12 12:46:07 2024 ] 
Training: Epoch [45/120], Step [999], Loss: 0.5888495445251465, Training Accuracy: 89.0
[ Fri Jul 12 12:46:08 2024 ] 	Batch(1000/6809) done. Loss: 0.4271  lr:0.010000
[ Fri Jul 12 12:46:26 2024 ] 	Batch(1100/6809) done. Loss: 0.8401  lr:0.010000
[ Fri Jul 12 12:46:45 2024 ] 	Batch(1200/6809) done. Loss: 0.8131  lr:0.010000
[ Fri Jul 12 12:47:03 2024 ] 	Batch(1300/6809) done. Loss: 0.4519  lr:0.010000
[ Fri Jul 12 12:47:22 2024 ] 	Batch(1400/6809) done. Loss: 0.8439  lr:0.010000
[ Fri Jul 12 12:47:40 2024 ] 
Training: Epoch [45/120], Step [1499], Loss: 0.7655631899833679, Training Accuracy: 88.81666666666666
[ Fri Jul 12 12:47:40 2024 ] 	Batch(1500/6809) done. Loss: 0.4122  lr:0.010000
[ Fri Jul 12 12:47:58 2024 ] 	Batch(1600/6809) done. Loss: 0.2150  lr:0.010000
[ Fri Jul 12 12:48:16 2024 ] 	Batch(1700/6809) done. Loss: 0.1531  lr:0.010000
[ Fri Jul 12 12:48:34 2024 ] 	Batch(1800/6809) done. Loss: 0.1541  lr:0.010000
[ Fri Jul 12 12:48:52 2024 ] 	Batch(1900/6809) done. Loss: 0.5897  lr:0.010000
[ Fri Jul 12 12:49:09 2024 ] 
Training: Epoch [45/120], Step [1999], Loss: 0.4455513656139374, Training Accuracy: 88.58125
[ Fri Jul 12 12:49:10 2024 ] 	Batch(2000/6809) done. Loss: 1.6007  lr:0.010000
[ Fri Jul 12 12:49:28 2024 ] 	Batch(2100/6809) done. Loss: 0.1342  lr:0.010000
[ Fri Jul 12 12:49:46 2024 ] 	Batch(2200/6809) done. Loss: 0.4017  lr:0.010000
[ Fri Jul 12 12:50:03 2024 ] 	Batch(2300/6809) done. Loss: 0.2235  lr:0.010000
[ Fri Jul 12 12:50:22 2024 ] 	Batch(2400/6809) done. Loss: 0.2765  lr:0.010000
[ Fri Jul 12 12:50:40 2024 ] 
Training: Epoch [45/120], Step [2499], Loss: 0.6270124316215515, Training Accuracy: 88.545
[ Fri Jul 12 12:50:40 2024 ] 	Batch(2500/6809) done. Loss: 1.1539  lr:0.010000
[ Fri Jul 12 12:50:58 2024 ] 	Batch(2600/6809) done. Loss: 0.7938  lr:0.010000
[ Fri Jul 12 12:51:16 2024 ] 	Batch(2700/6809) done. Loss: 0.1752  lr:0.010000
[ Fri Jul 12 12:51:34 2024 ] 	Batch(2800/6809) done. Loss: 0.3468  lr:0.010000
[ Fri Jul 12 12:51:52 2024 ] 	Batch(2900/6809) done. Loss: 0.6495  lr:0.010000
[ Fri Jul 12 12:52:10 2024 ] 
Training: Epoch [45/120], Step [2999], Loss: 0.5811061263084412, Training Accuracy: 88.41250000000001
[ Fri Jul 12 12:52:10 2024 ] 	Batch(3000/6809) done. Loss: 0.0709  lr:0.010000
[ Fri Jul 12 12:52:28 2024 ] 	Batch(3100/6809) done. Loss: 0.0939  lr:0.010000
[ Fri Jul 12 12:52:46 2024 ] 	Batch(3200/6809) done. Loss: 0.0472  lr:0.010000
[ Fri Jul 12 12:53:04 2024 ] 	Batch(3300/6809) done. Loss: 0.7302  lr:0.010000
[ Fri Jul 12 12:53:22 2024 ] 	Batch(3400/6809) done. Loss: 1.2966  lr:0.010000
[ Fri Jul 12 12:53:39 2024 ] 
Training: Epoch [45/120], Step [3499], Loss: 0.6322786808013916, Training Accuracy: 88.16071428571428
[ Fri Jul 12 12:53:40 2024 ] 	Batch(3500/6809) done. Loss: 0.2870  lr:0.010000
[ Fri Jul 12 12:53:58 2024 ] 	Batch(3600/6809) done. Loss: 0.2906  lr:0.010000
[ Fri Jul 12 12:54:15 2024 ] 	Batch(3700/6809) done. Loss: 0.1251  lr:0.010000
[ Fri Jul 12 12:54:33 2024 ] 	Batch(3800/6809) done. Loss: 0.2719  lr:0.010000
[ Fri Jul 12 12:54:51 2024 ] 	Batch(3900/6809) done. Loss: 0.3557  lr:0.010000
[ Fri Jul 12 12:55:09 2024 ] 
Training: Epoch [45/120], Step [3999], Loss: 0.04638371989130974, Training Accuracy: 88.090625
[ Fri Jul 12 12:55:09 2024 ] 	Batch(4000/6809) done. Loss: 0.7101  lr:0.010000
[ Fri Jul 12 12:55:27 2024 ] 	Batch(4100/6809) done. Loss: 0.0662  lr:0.010000
[ Fri Jul 12 12:55:45 2024 ] 	Batch(4200/6809) done. Loss: 0.5170  lr:0.010000
[ Fri Jul 12 12:56:03 2024 ] 	Batch(4300/6809) done. Loss: 0.1713  lr:0.010000
[ Fri Jul 12 12:56:21 2024 ] 	Batch(4400/6809) done. Loss: 0.4001  lr:0.010000
[ Fri Jul 12 12:56:39 2024 ] 
Training: Epoch [45/120], Step [4499], Loss: 0.08102066069841385, Training Accuracy: 88.1138888888889
[ Fri Jul 12 12:56:39 2024 ] 	Batch(4500/6809) done. Loss: 0.0493  lr:0.010000
[ Fri Jul 12 12:56:57 2024 ] 	Batch(4600/6809) done. Loss: 0.4520  lr:0.010000
[ Fri Jul 12 12:57:15 2024 ] 	Batch(4700/6809) done. Loss: 0.4023  lr:0.010000
[ Fri Jul 12 12:57:33 2024 ] 	Batch(4800/6809) done. Loss: 0.2198  lr:0.010000
[ Fri Jul 12 12:57:51 2024 ] 	Batch(4900/6809) done. Loss: 0.2480  lr:0.010000
[ Fri Jul 12 12:58:08 2024 ] 
Training: Epoch [45/120], Step [4999], Loss: 0.2893436849117279, Training Accuracy: 87.97749999999999
[ Fri Jul 12 12:58:09 2024 ] 	Batch(5000/6809) done. Loss: 0.4243  lr:0.010000
[ Fri Jul 12 12:58:27 2024 ] 	Batch(5100/6809) done. Loss: 0.2073  lr:0.010000
[ Fri Jul 12 12:58:45 2024 ] 	Batch(5200/6809) done. Loss: 0.3400  lr:0.010000
[ Fri Jul 12 12:59:03 2024 ] 	Batch(5300/6809) done. Loss: 0.2742  lr:0.010000
[ Fri Jul 12 12:59:21 2024 ] 	Batch(5400/6809) done. Loss: 0.1682  lr:0.010000
[ Fri Jul 12 12:59:38 2024 ] 
Training: Epoch [45/120], Step [5499], Loss: 0.34958526492118835, Training Accuracy: 87.90909090909092
[ Fri Jul 12 12:59:38 2024 ] 	Batch(5500/6809) done. Loss: 0.1238  lr:0.010000
[ Fri Jul 12 12:59:56 2024 ] 	Batch(5600/6809) done. Loss: 0.0288  lr:0.010000
[ Fri Jul 12 13:00:14 2024 ] 	Batch(5700/6809) done. Loss: 0.8247  lr:0.010000
[ Fri Jul 12 13:00:32 2024 ] 	Batch(5800/6809) done. Loss: 0.3899  lr:0.010000
[ Fri Jul 12 13:00:50 2024 ] 	Batch(5900/6809) done. Loss: 0.3502  lr:0.010000
[ Fri Jul 12 13:01:08 2024 ] 
Training: Epoch [45/120], Step [5999], Loss: 0.7685037851333618, Training Accuracy: 87.8875
[ Fri Jul 12 13:01:08 2024 ] 	Batch(6000/6809) done. Loss: 0.7137  lr:0.010000
[ Fri Jul 12 13:01:26 2024 ] 	Batch(6100/6809) done. Loss: 0.2117  lr:0.010000
[ Fri Jul 12 13:01:44 2024 ] 	Batch(6200/6809) done. Loss: 0.3124  lr:0.010000
[ Fri Jul 12 13:02:02 2024 ] 	Batch(6300/6809) done. Loss: 0.0590  lr:0.010000
[ Fri Jul 12 13:02:20 2024 ] 	Batch(6400/6809) done. Loss: 1.0234  lr:0.010000
[ Fri Jul 12 13:02:38 2024 ] 
Training: Epoch [45/120], Step [6499], Loss: 0.4539833068847656, Training Accuracy: 87.83076923076923
[ Fri Jul 12 13:02:38 2024 ] 	Batch(6500/6809) done. Loss: 0.0318  lr:0.010000
[ Fri Jul 12 13:02:57 2024 ] 	Batch(6600/6809) done. Loss: 0.1216  lr:0.010000
[ Fri Jul 12 13:03:16 2024 ] 	Batch(6700/6809) done. Loss: 0.1731  lr:0.010000
[ Fri Jul 12 13:03:34 2024 ] 	Batch(6800/6809) done. Loss: 0.2296  lr:0.010000
[ Fri Jul 12 13:03:36 2024 ] 	Mean training loss: 0.3809.
[ Fri Jul 12 13:03:36 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 13:03:36 2024 ] Training epoch: 47
[ Fri Jul 12 13:03:36 2024 ] 	Batch(0/6809) done. Loss: 0.1404  lr:0.010000
[ Fri Jul 12 13:03:54 2024 ] 	Batch(100/6809) done. Loss: 0.0789  lr:0.010000
[ Fri Jul 12 13:04:12 2024 ] 	Batch(200/6809) done. Loss: 0.0181  lr:0.010000
[ Fri Jul 12 13:04:30 2024 ] 	Batch(300/6809) done. Loss: 0.2818  lr:0.010000
[ Fri Jul 12 13:04:48 2024 ] 	Batch(400/6809) done. Loss: 0.0326  lr:0.010000
[ Fri Jul 12 13:05:06 2024 ] 
Training: Epoch [46/120], Step [499], Loss: 0.26101580262184143, Training Accuracy: 89.25
[ Fri Jul 12 13:05:06 2024 ] 	Batch(500/6809) done. Loss: 0.3590  lr:0.010000
[ Fri Jul 12 13:05:24 2024 ] 	Batch(600/6809) done. Loss: 0.2293  lr:0.010000
[ Fri Jul 12 13:05:42 2024 ] 	Batch(700/6809) done. Loss: 0.6403  lr:0.010000
[ Fri Jul 12 13:06:00 2024 ] 	Batch(800/6809) done. Loss: 0.3084  lr:0.010000
[ Fri Jul 12 13:06:18 2024 ] 	Batch(900/6809) done. Loss: 0.1778  lr:0.010000
[ Fri Jul 12 13:06:35 2024 ] 
Training: Epoch [46/120], Step [999], Loss: 0.371121883392334, Training Accuracy: 89.45
[ Fri Jul 12 13:06:35 2024 ] 	Batch(1000/6809) done. Loss: 0.3981  lr:0.010000
[ Fri Jul 12 13:06:53 2024 ] 	Batch(1100/6809) done. Loss: 0.0585  lr:0.010000
[ Fri Jul 12 13:07:11 2024 ] 	Batch(1200/6809) done. Loss: 0.2325  lr:0.010000
[ Fri Jul 12 13:07:30 2024 ] 	Batch(1300/6809) done. Loss: 1.0755  lr:0.010000
[ Fri Jul 12 13:07:49 2024 ] 	Batch(1400/6809) done. Loss: 0.5057  lr:0.010000
[ Fri Jul 12 13:08:07 2024 ] 
Training: Epoch [46/120], Step [1499], Loss: 0.1384885162115097, Training Accuracy: 89.3
[ Fri Jul 12 13:08:07 2024 ] 	Batch(1500/6809) done. Loss: 0.9581  lr:0.010000
[ Fri Jul 12 13:08:26 2024 ] 	Batch(1600/6809) done. Loss: 0.0456  lr:0.010000
[ Fri Jul 12 13:08:44 2024 ] 	Batch(1700/6809) done. Loss: 0.8645  lr:0.010000
[ Fri Jul 12 13:09:02 2024 ] 	Batch(1800/6809) done. Loss: 0.4417  lr:0.010000
[ Fri Jul 12 13:09:20 2024 ] 	Batch(1900/6809) done. Loss: 0.6348  lr:0.010000
[ Fri Jul 12 13:09:37 2024 ] 
Training: Epoch [46/120], Step [1999], Loss: 0.17831173539161682, Training Accuracy: 89.20625
[ Fri Jul 12 13:09:38 2024 ] 	Batch(2000/6809) done. Loss: 0.7820  lr:0.010000
[ Fri Jul 12 13:09:56 2024 ] 	Batch(2100/6809) done. Loss: 0.1462  lr:0.010000
[ Fri Jul 12 13:10:13 2024 ] 	Batch(2200/6809) done. Loss: 0.0826  lr:0.010000
[ Fri Jul 12 13:10:31 2024 ] 	Batch(2300/6809) done. Loss: 0.3796  lr:0.010000
[ Fri Jul 12 13:10:49 2024 ] 	Batch(2400/6809) done. Loss: 0.8139  lr:0.010000
[ Fri Jul 12 13:11:07 2024 ] 
Training: Epoch [46/120], Step [2499], Loss: 0.3901538550853729, Training Accuracy: 89.08
[ Fri Jul 12 13:11:07 2024 ] 	Batch(2500/6809) done. Loss: 0.1686  lr:0.010000
[ Fri Jul 12 13:11:25 2024 ] 	Batch(2600/6809) done. Loss: 0.5137  lr:0.010000
[ Fri Jul 12 13:11:43 2024 ] 	Batch(2700/6809) done. Loss: 0.3471  lr:0.010000
[ Fri Jul 12 13:12:01 2024 ] 	Batch(2800/6809) done. Loss: 0.1863  lr:0.010000
[ Fri Jul 12 13:12:19 2024 ] 	Batch(2900/6809) done. Loss: 0.3880  lr:0.010000
[ Fri Jul 12 13:12:37 2024 ] 
Training: Epoch [46/120], Step [2999], Loss: 0.1814458668231964, Training Accuracy: 88.7875
[ Fri Jul 12 13:12:37 2024 ] 	Batch(3000/6809) done. Loss: 0.0116  lr:0.010000
[ Fri Jul 12 13:12:55 2024 ] 	Batch(3100/6809) done. Loss: 0.5538  lr:0.010000
[ Fri Jul 12 13:13:13 2024 ] 	Batch(3200/6809) done. Loss: 0.0144  lr:0.010000
[ Fri Jul 12 13:13:32 2024 ] 	Batch(3300/6809) done. Loss: 0.1747  lr:0.010000
[ Fri Jul 12 13:13:50 2024 ] 	Batch(3400/6809) done. Loss: 0.4386  lr:0.010000
[ Fri Jul 12 13:14:08 2024 ] 
Training: Epoch [46/120], Step [3499], Loss: 0.0456114299595356, Training Accuracy: 88.61071428571428
[ Fri Jul 12 13:14:08 2024 ] 	Batch(3500/6809) done. Loss: 0.6448  lr:0.010000
[ Fri Jul 12 13:14:27 2024 ] 	Batch(3600/6809) done. Loss: 0.3341  lr:0.010000
[ Fri Jul 12 13:14:44 2024 ] 	Batch(3700/6809) done. Loss: 0.2202  lr:0.010000
[ Fri Jul 12 13:15:02 2024 ] 	Batch(3800/6809) done. Loss: 0.2570  lr:0.010000
[ Fri Jul 12 13:15:20 2024 ] 	Batch(3900/6809) done. Loss: 0.3286  lr:0.010000
[ Fri Jul 12 13:15:38 2024 ] 
Training: Epoch [46/120], Step [3999], Loss: 0.312374472618103, Training Accuracy: 88.49375
[ Fri Jul 12 13:15:38 2024 ] 	Batch(4000/6809) done. Loss: 0.2206  lr:0.010000
[ Fri Jul 12 13:15:56 2024 ] 	Batch(4100/6809) done. Loss: 0.2474  lr:0.010000
[ Fri Jul 12 13:16:14 2024 ] 	Batch(4200/6809) done. Loss: 0.7111  lr:0.010000
[ Fri Jul 12 13:16:32 2024 ] 	Batch(4300/6809) done. Loss: 0.5255  lr:0.010000
[ Fri Jul 12 13:16:50 2024 ] 	Batch(4400/6809) done. Loss: 0.2144  lr:0.010000
[ Fri Jul 12 13:17:08 2024 ] 
Training: Epoch [46/120], Step [4499], Loss: 1.0963759422302246, Training Accuracy: 88.44444444444444
[ Fri Jul 12 13:17:08 2024 ] 	Batch(4500/6809) done. Loss: 0.2253  lr:0.010000
[ Fri Jul 12 13:17:26 2024 ] 	Batch(4600/6809) done. Loss: 0.0748  lr:0.010000
[ Fri Jul 12 13:17:44 2024 ] 	Batch(4700/6809) done. Loss: 0.0942  lr:0.010000
[ Fri Jul 12 13:18:02 2024 ] 	Batch(4800/6809) done. Loss: 0.3771  lr:0.010000
[ Fri Jul 12 13:18:20 2024 ] 	Batch(4900/6809) done. Loss: 0.1383  lr:0.010000
[ Fri Jul 12 13:18:38 2024 ] 
Training: Epoch [46/120], Step [4999], Loss: 1.031787633895874, Training Accuracy: 88.385
[ Fri Jul 12 13:18:38 2024 ] 	Batch(5000/6809) done. Loss: 0.0857  lr:0.010000
[ Fri Jul 12 13:18:56 2024 ] 	Batch(5100/6809) done. Loss: 0.0360  lr:0.010000
[ Fri Jul 12 13:19:14 2024 ] 	Batch(5200/6809) done. Loss: 0.4552  lr:0.010000
[ Fri Jul 12 13:19:32 2024 ] 	Batch(5300/6809) done. Loss: 0.2280  lr:0.010000
[ Fri Jul 12 13:19:50 2024 ] 	Batch(5400/6809) done. Loss: 0.4181  lr:0.010000
[ Fri Jul 12 13:20:08 2024 ] 
Training: Epoch [46/120], Step [5499], Loss: 0.5937639474868774, Training Accuracy: 88.32272727272728
[ Fri Jul 12 13:20:08 2024 ] 	Batch(5500/6809) done. Loss: 0.6653  lr:0.010000
[ Fri Jul 12 13:20:26 2024 ] 	Batch(5600/6809) done. Loss: 0.4465  lr:0.010000
[ Fri Jul 12 13:20:44 2024 ] 	Batch(5700/6809) done. Loss: 0.5618  lr:0.010000
[ Fri Jul 12 13:21:02 2024 ] 	Batch(5800/6809) done. Loss: 0.8449  lr:0.010000
[ Fri Jul 12 13:21:20 2024 ] 	Batch(5900/6809) done. Loss: 0.1308  lr:0.010000
[ Fri Jul 12 13:21:37 2024 ] 
Training: Epoch [46/120], Step [5999], Loss: 0.6868937611579895, Training Accuracy: 88.23958333333334
[ Fri Jul 12 13:21:38 2024 ] 	Batch(6000/6809) done. Loss: 0.0061  lr:0.010000
[ Fri Jul 12 13:21:56 2024 ] 	Batch(6100/6809) done. Loss: 0.0802  lr:0.010000
[ Fri Jul 12 13:22:15 2024 ] 	Batch(6200/6809) done. Loss: 0.1405  lr:0.010000
[ Fri Jul 12 13:22:33 2024 ] 	Batch(6300/6809) done. Loss: 0.3225  lr:0.010000
[ Fri Jul 12 13:22:52 2024 ] 	Batch(6400/6809) done. Loss: 0.4788  lr:0.010000
[ Fri Jul 12 13:23:10 2024 ] 
Training: Epoch [46/120], Step [6499], Loss: 1.2575476169586182, Training Accuracy: 88.10961538461538
[ Fri Jul 12 13:23:10 2024 ] 	Batch(6500/6809) done. Loss: 0.2326  lr:0.010000
[ Fri Jul 12 13:23:28 2024 ] 	Batch(6600/6809) done. Loss: 0.1324  lr:0.010000
[ Fri Jul 12 13:23:46 2024 ] 	Batch(6700/6809) done. Loss: 0.2652  lr:0.010000
[ Fri Jul 12 13:24:04 2024 ] 	Batch(6800/6809) done. Loss: 0.8304  lr:0.010000
[ Fri Jul 12 13:24:05 2024 ] 	Mean training loss: 0.3696.
[ Fri Jul 12 13:24:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 13:24:06 2024 ] Training epoch: 48
[ Fri Jul 12 13:24:06 2024 ] 	Batch(0/6809) done. Loss: 0.6783  lr:0.010000
[ Fri Jul 12 13:24:24 2024 ] 	Batch(100/6809) done. Loss: 0.7279  lr:0.010000
[ Fri Jul 12 13:24:43 2024 ] 	Batch(200/6809) done. Loss: 0.2064  lr:0.010000
[ Fri Jul 12 13:25:01 2024 ] 	Batch(300/6809) done. Loss: 0.2739  lr:0.010000
[ Fri Jul 12 13:25:19 2024 ] 	Batch(400/6809) done. Loss: 0.0265  lr:0.010000
[ Fri Jul 12 13:25:38 2024 ] 
Training: Epoch [47/120], Step [499], Loss: 0.1097426787018776, Training Accuracy: 89.47500000000001
[ Fri Jul 12 13:25:38 2024 ] 	Batch(500/6809) done. Loss: 0.2265  lr:0.010000
[ Fri Jul 12 13:25:56 2024 ] 	Batch(600/6809) done. Loss: 0.0588  lr:0.010000
[ Fri Jul 12 13:26:14 2024 ] 	Batch(700/6809) done. Loss: 0.1714  lr:0.010000
[ Fri Jul 12 13:26:33 2024 ] 	Batch(800/6809) done. Loss: 0.2050  lr:0.010000
[ Fri Jul 12 13:26:51 2024 ] 	Batch(900/6809) done. Loss: 0.3220  lr:0.010000
[ Fri Jul 12 13:27:09 2024 ] 
Training: Epoch [47/120], Step [999], Loss: 1.1541019678115845, Training Accuracy: 89.075
[ Fri Jul 12 13:27:09 2024 ] 	Batch(1000/6809) done. Loss: 0.1502  lr:0.010000
[ Fri Jul 12 13:27:28 2024 ] 	Batch(1100/6809) done. Loss: 0.0416  lr:0.010000
[ Fri Jul 12 13:27:46 2024 ] 	Batch(1200/6809) done. Loss: 0.8992  lr:0.010000
[ Fri Jul 12 13:28:04 2024 ] 	Batch(1300/6809) done. Loss: 0.1760  lr:0.010000
[ Fri Jul 12 13:28:23 2024 ] 	Batch(1400/6809) done. Loss: 0.8137  lr:0.010000
[ Fri Jul 12 13:28:41 2024 ] 
Training: Epoch [47/120], Step [1499], Loss: 0.3876303732395172, Training Accuracy: 89.13333333333333
[ Fri Jul 12 13:28:41 2024 ] 	Batch(1500/6809) done. Loss: 0.0590  lr:0.010000
[ Fri Jul 12 13:29:00 2024 ] 	Batch(1600/6809) done. Loss: 0.3584  lr:0.010000
[ Fri Jul 12 13:29:18 2024 ] 	Batch(1700/6809) done. Loss: 0.5569  lr:0.010000
[ Fri Jul 12 13:29:36 2024 ] 	Batch(1800/6809) done. Loss: 1.2408  lr:0.010000
[ Fri Jul 12 13:29:55 2024 ] 	Batch(1900/6809) done. Loss: 0.7920  lr:0.010000
[ Fri Jul 12 13:30:13 2024 ] 
Training: Epoch [47/120], Step [1999], Loss: 0.15938597917556763, Training Accuracy: 88.95625
[ Fri Jul 12 13:30:13 2024 ] 	Batch(2000/6809) done. Loss: 0.1005  lr:0.010000
[ Fri Jul 12 13:30:31 2024 ] 	Batch(2100/6809) done. Loss: 0.0684  lr:0.010000
[ Fri Jul 12 13:30:50 2024 ] 	Batch(2200/6809) done. Loss: 0.0465  lr:0.010000
[ Fri Jul 12 13:31:08 2024 ] 	Batch(2300/6809) done. Loss: 0.4096  lr:0.010000
[ Fri Jul 12 13:31:27 2024 ] 	Batch(2400/6809) done. Loss: 0.2817  lr:0.010000
[ Fri Jul 12 13:31:45 2024 ] 
Training: Epoch [47/120], Step [2499], Loss: 0.48938724398612976, Training Accuracy: 88.905
[ Fri Jul 12 13:31:45 2024 ] 	Batch(2500/6809) done. Loss: 0.2106  lr:0.010000
[ Fri Jul 12 13:32:03 2024 ] 	Batch(2600/6809) done. Loss: 0.3474  lr:0.010000
[ Fri Jul 12 13:32:21 2024 ] 	Batch(2700/6809) done. Loss: 1.0041  lr:0.010000
[ Fri Jul 12 13:32:40 2024 ] 	Batch(2800/6809) done. Loss: 0.1206  lr:0.010000
[ Fri Jul 12 13:32:58 2024 ] 	Batch(2900/6809) done. Loss: 0.2249  lr:0.010000
[ Fri Jul 12 13:33:16 2024 ] 
Training: Epoch [47/120], Step [2999], Loss: 0.5871785283088684, Training Accuracy: 88.8625
[ Fri Jul 12 13:33:17 2024 ] 	Batch(3000/6809) done. Loss: 0.5664  lr:0.010000
[ Fri Jul 12 13:33:35 2024 ] 	Batch(3100/6809) done. Loss: 0.0496  lr:0.010000
[ Fri Jul 12 13:33:54 2024 ] 	Batch(3200/6809) done. Loss: 1.0629  lr:0.010000
[ Fri Jul 12 13:34:12 2024 ] 	Batch(3300/6809) done. Loss: 0.1009  lr:0.010000
[ Fri Jul 12 13:34:30 2024 ] 	Batch(3400/6809) done. Loss: 0.3535  lr:0.010000
[ Fri Jul 12 13:34:48 2024 ] 
Training: Epoch [47/120], Step [3499], Loss: 0.8562393188476562, Training Accuracy: 88.75714285714285
[ Fri Jul 12 13:34:48 2024 ] 	Batch(3500/6809) done. Loss: 0.1860  lr:0.010000
[ Fri Jul 12 13:35:06 2024 ] 	Batch(3600/6809) done. Loss: 0.0691  lr:0.010000
[ Fri Jul 12 13:35:24 2024 ] 	Batch(3700/6809) done. Loss: 0.6039  lr:0.010000
[ Fri Jul 12 13:35:42 2024 ] 	Batch(3800/6809) done. Loss: 0.1417  lr:0.010000
[ Fri Jul 12 13:36:00 2024 ] 	Batch(3900/6809) done. Loss: 0.0111  lr:0.010000
[ Fri Jul 12 13:36:18 2024 ] 
Training: Epoch [47/120], Step [3999], Loss: 0.2998824119567871, Training Accuracy: 88.746875
[ Fri Jul 12 13:36:18 2024 ] 	Batch(4000/6809) done. Loss: 0.1393  lr:0.010000
[ Fri Jul 12 13:36:36 2024 ] 	Batch(4100/6809) done. Loss: 0.6866  lr:0.010000
[ Fri Jul 12 13:36:54 2024 ] 	Batch(4200/6809) done. Loss: 0.1212  lr:0.010000
[ Fri Jul 12 13:37:12 2024 ] 	Batch(4300/6809) done. Loss: 0.2963  lr:0.010000
[ Fri Jul 12 13:37:30 2024 ] 	Batch(4400/6809) done. Loss: 0.2576  lr:0.010000
[ Fri Jul 12 13:37:48 2024 ] 
Training: Epoch [47/120], Step [4499], Loss: 0.276371031999588, Training Accuracy: 88.67222222222222
[ Fri Jul 12 13:37:48 2024 ] 	Batch(4500/6809) done. Loss: 0.4743  lr:0.010000
[ Fri Jul 12 13:38:06 2024 ] 	Batch(4600/6809) done. Loss: 0.0297  lr:0.010000
[ Fri Jul 12 13:38:24 2024 ] 	Batch(4700/6809) done. Loss: 1.1691  lr:0.010000
[ Fri Jul 12 13:38:42 2024 ] 	Batch(4800/6809) done. Loss: 0.5202  lr:0.010000
[ Fri Jul 12 13:39:00 2024 ] 	Batch(4900/6809) done. Loss: 0.1043  lr:0.010000
[ Fri Jul 12 13:39:18 2024 ] 
Training: Epoch [47/120], Step [4999], Loss: 0.02264433726668358, Training Accuracy: 88.63250000000001
[ Fri Jul 12 13:39:18 2024 ] 	Batch(5000/6809) done. Loss: 1.0828  lr:0.010000
[ Fri Jul 12 13:39:36 2024 ] 	Batch(5100/6809) done. Loss: 0.0369  lr:0.010000
[ Fri Jul 12 13:39:54 2024 ] 	Batch(5200/6809) done. Loss: 0.1872  lr:0.010000
[ Fri Jul 12 13:40:12 2024 ] 	Batch(5300/6809) done. Loss: 0.2175  lr:0.010000
[ Fri Jul 12 13:40:30 2024 ] 	Batch(5400/6809) done. Loss: 0.3254  lr:0.010000
[ Fri Jul 12 13:40:47 2024 ] 
Training: Epoch [47/120], Step [5499], Loss: 0.36850640177726746, Training Accuracy: 88.49772727272727
[ Fri Jul 12 13:40:48 2024 ] 	Batch(5500/6809) done. Loss: 0.4940  lr:0.010000
[ Fri Jul 12 13:41:06 2024 ] 	Batch(5600/6809) done. Loss: 0.0056  lr:0.010000
[ Fri Jul 12 13:41:24 2024 ] 	Batch(5700/6809) done. Loss: 0.3861  lr:0.010000
[ Fri Jul 12 13:41:42 2024 ] 	Batch(5800/6809) done. Loss: 0.5341  lr:0.010000
[ Fri Jul 12 13:42:00 2024 ] 	Batch(5900/6809) done. Loss: 0.1600  lr:0.010000
[ Fri Jul 12 13:42:17 2024 ] 
Training: Epoch [47/120], Step [5999], Loss: 0.04253517463803291, Training Accuracy: 88.43124999999999
[ Fri Jul 12 13:42:18 2024 ] 	Batch(6000/6809) done. Loss: 0.0923  lr:0.010000
[ Fri Jul 12 13:42:36 2024 ] 	Batch(6100/6809) done. Loss: 0.4233  lr:0.010000
[ Fri Jul 12 13:42:53 2024 ] 	Batch(6200/6809) done. Loss: 0.0804  lr:0.010000
[ Fri Jul 12 13:43:11 2024 ] 	Batch(6300/6809) done. Loss: 0.9711  lr:0.010000
[ Fri Jul 12 13:43:29 2024 ] 	Batch(6400/6809) done. Loss: 1.1688  lr:0.010000
[ Fri Jul 12 13:43:47 2024 ] 
Training: Epoch [47/120], Step [6499], Loss: 0.3650919795036316, Training Accuracy: 88.29038461538462
[ Fri Jul 12 13:43:47 2024 ] 	Batch(6500/6809) done. Loss: 0.0147  lr:0.010000
[ Fri Jul 12 13:44:05 2024 ] 	Batch(6600/6809) done. Loss: 0.4380  lr:0.010000
[ Fri Jul 12 13:44:23 2024 ] 	Batch(6700/6809) done. Loss: 0.5374  lr:0.010000
[ Fri Jul 12 13:44:41 2024 ] 	Batch(6800/6809) done. Loss: 1.2176  lr:0.010000
[ Fri Jul 12 13:44:43 2024 ] 	Mean training loss: 0.3719.
[ Fri Jul 12 13:44:43 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 13:44:43 2024 ] Training epoch: 49
[ Fri Jul 12 13:44:43 2024 ] 	Batch(0/6809) done. Loss: 0.4559  lr:0.010000
[ Fri Jul 12 13:45:02 2024 ] 	Batch(100/6809) done. Loss: 0.0365  lr:0.010000
[ Fri Jul 12 13:45:20 2024 ] 	Batch(200/6809) done. Loss: 0.1859  lr:0.010000
[ Fri Jul 12 13:45:38 2024 ] 	Batch(300/6809) done. Loss: 0.1204  lr:0.010000
[ Fri Jul 12 13:45:56 2024 ] 	Batch(400/6809) done. Loss: 0.1726  lr:0.010000
[ Fri Jul 12 13:46:14 2024 ] 
Training: Epoch [48/120], Step [499], Loss: 0.38220784068107605, Training Accuracy: 88.75
[ Fri Jul 12 13:46:14 2024 ] 	Batch(500/6809) done. Loss: 0.3568  lr:0.010000
[ Fri Jul 12 13:46:32 2024 ] 	Batch(600/6809) done. Loss: 0.3995  lr:0.010000
[ Fri Jul 12 13:46:50 2024 ] 	Batch(700/6809) done. Loss: 0.0307  lr:0.010000
[ Fri Jul 12 13:47:08 2024 ] 	Batch(800/6809) done. Loss: 0.1819  lr:0.010000
[ Fri Jul 12 13:47:26 2024 ] 	Batch(900/6809) done. Loss: 0.1864  lr:0.010000
[ Fri Jul 12 13:47:45 2024 ] 
Training: Epoch [48/120], Step [999], Loss: 0.5672944784164429, Training Accuracy: 88.775
[ Fri Jul 12 13:47:45 2024 ] 	Batch(1000/6809) done. Loss: 0.0555  lr:0.010000
[ Fri Jul 12 13:48:03 2024 ] 	Batch(1100/6809) done. Loss: 0.4288  lr:0.010000
[ Fri Jul 12 13:48:21 2024 ] 	Batch(1200/6809) done. Loss: 0.0903  lr:0.010000
[ Fri Jul 12 13:48:39 2024 ] 	Batch(1300/6809) done. Loss: 1.1150  lr:0.010000
[ Fri Jul 12 13:48:57 2024 ] 	Batch(1400/6809) done. Loss: 0.5481  lr:0.010000
[ Fri Jul 12 13:49:15 2024 ] 
Training: Epoch [48/120], Step [1499], Loss: 0.06662847101688385, Training Accuracy: 89.15833333333333
[ Fri Jul 12 13:49:15 2024 ] 	Batch(1500/6809) done. Loss: 0.4648  lr:0.010000
[ Fri Jul 12 13:49:33 2024 ] 	Batch(1600/6809) done. Loss: 0.7465  lr:0.010000
[ Fri Jul 12 13:49:52 2024 ] 	Batch(1700/6809) done. Loss: 1.1144  lr:0.010000
[ Fri Jul 12 13:50:10 2024 ] 	Batch(1800/6809) done. Loss: 0.4799  lr:0.010000
[ Fri Jul 12 13:50:27 2024 ] 	Batch(1900/6809) done. Loss: 1.0857  lr:0.010000
[ Fri Jul 12 13:50:45 2024 ] 
Training: Epoch [48/120], Step [1999], Loss: 0.4387213885784149, Training Accuracy: 89.01875
[ Fri Jul 12 13:50:45 2024 ] 	Batch(2000/6809) done. Loss: 0.0458  lr:0.010000
[ Fri Jul 12 13:51:04 2024 ] 	Batch(2100/6809) done. Loss: 0.0973  lr:0.010000
[ Fri Jul 12 13:51:22 2024 ] 	Batch(2200/6809) done. Loss: 0.4166  lr:0.010000
[ Fri Jul 12 13:51:40 2024 ] 	Batch(2300/6809) done. Loss: 0.2280  lr:0.010000
[ Fri Jul 12 13:51:58 2024 ] 	Batch(2400/6809) done. Loss: 0.4845  lr:0.010000
[ Fri Jul 12 13:52:15 2024 ] 
Training: Epoch [48/120], Step [2499], Loss: 0.49150434136390686, Training Accuracy: 88.81
[ Fri Jul 12 13:52:16 2024 ] 	Batch(2500/6809) done. Loss: 0.5974  lr:0.010000
[ Fri Jul 12 13:52:34 2024 ] 	Batch(2600/6809) done. Loss: 0.7015  lr:0.010000
[ Fri Jul 12 13:52:51 2024 ] 	Batch(2700/6809) done. Loss: 0.4499  lr:0.010000
[ Fri Jul 12 13:53:10 2024 ] 	Batch(2800/6809) done. Loss: 0.0753  lr:0.010000
[ Fri Jul 12 13:53:27 2024 ] 	Batch(2900/6809) done. Loss: 0.2660  lr:0.010000
[ Fri Jul 12 13:53:45 2024 ] 
Training: Epoch [48/120], Step [2999], Loss: 0.09943725913763046, Training Accuracy: 88.69166666666666
[ Fri Jul 12 13:53:45 2024 ] 	Batch(3000/6809) done. Loss: 1.2632  lr:0.010000
[ Fri Jul 12 13:54:03 2024 ] 	Batch(3100/6809) done. Loss: 0.2467  lr:0.010000
[ Fri Jul 12 13:54:21 2024 ] 	Batch(3200/6809) done. Loss: 1.0549  lr:0.010000
[ Fri Jul 12 13:54:40 2024 ] 	Batch(3300/6809) done. Loss: 0.2714  lr:0.010000
[ Fri Jul 12 13:54:58 2024 ] 	Batch(3400/6809) done. Loss: 0.2535  lr:0.010000
[ Fri Jul 12 13:55:17 2024 ] 
Training: Epoch [48/120], Step [3499], Loss: 0.19555573165416718, Training Accuracy: 88.775
[ Fri Jul 12 13:55:17 2024 ] 	Batch(3500/6809) done. Loss: 0.3498  lr:0.010000
[ Fri Jul 12 13:55:36 2024 ] 	Batch(3600/6809) done. Loss: 0.5635  lr:0.010000
[ Fri Jul 12 13:55:54 2024 ] 	Batch(3700/6809) done. Loss: 0.4065  lr:0.010000
[ Fri Jul 12 13:56:13 2024 ] 	Batch(3800/6809) done. Loss: 0.0277  lr:0.010000
[ Fri Jul 12 13:56:31 2024 ] 	Batch(3900/6809) done. Loss: 0.3614  lr:0.010000
[ Fri Jul 12 13:56:49 2024 ] 
Training: Epoch [48/120], Step [3999], Loss: 0.32569971680641174, Training Accuracy: 88.6375
[ Fri Jul 12 13:56:50 2024 ] 	Batch(4000/6809) done. Loss: 0.0888  lr:0.010000
[ Fri Jul 12 13:57:07 2024 ] 	Batch(4100/6809) done. Loss: 0.1325  lr:0.010000
[ Fri Jul 12 13:57:25 2024 ] 	Batch(4200/6809) done. Loss: 0.7287  lr:0.010000
[ Fri Jul 12 13:57:43 2024 ] 	Batch(4300/6809) done. Loss: 0.1615  lr:0.010000
[ Fri Jul 12 13:58:01 2024 ] 	Batch(4400/6809) done. Loss: 0.8526  lr:0.010000
[ Fri Jul 12 13:58:19 2024 ] 
Training: Epoch [48/120], Step [4499], Loss: 0.03249894827604294, Training Accuracy: 88.53333333333333
[ Fri Jul 12 13:58:19 2024 ] 	Batch(4500/6809) done. Loss: 0.3969  lr:0.010000
[ Fri Jul 12 13:58:37 2024 ] 	Batch(4600/6809) done. Loss: 0.1524  lr:0.010000
[ Fri Jul 12 13:58:55 2024 ] 	Batch(4700/6809) done. Loss: 0.2200  lr:0.010000
[ Fri Jul 12 13:59:13 2024 ] 	Batch(4800/6809) done. Loss: 0.0767  lr:0.010000
[ Fri Jul 12 13:59:31 2024 ] 	Batch(4900/6809) done. Loss: 0.5424  lr:0.010000
[ Fri Jul 12 13:59:49 2024 ] 
Training: Epoch [48/120], Step [4999], Loss: 0.6256673336029053, Training Accuracy: 88.5325
[ Fri Jul 12 13:59:49 2024 ] 	Batch(5000/6809) done. Loss: 0.3438  lr:0.010000
[ Fri Jul 12 14:00:07 2024 ] 	Batch(5100/6809) done. Loss: 0.3727  lr:0.010000
[ Fri Jul 12 14:00:26 2024 ] 	Batch(5200/6809) done. Loss: 0.0209  lr:0.010000
[ Fri Jul 12 14:00:44 2024 ] 	Batch(5300/6809) done. Loss: 0.4177  lr:0.010000
[ Fri Jul 12 14:01:03 2024 ] 	Batch(5400/6809) done. Loss: 0.4566  lr:0.010000
[ Fri Jul 12 14:01:21 2024 ] 
Training: Epoch [48/120], Step [5499], Loss: 0.2634677588939667, Training Accuracy: 88.32954545454545
[ Fri Jul 12 14:01:22 2024 ] 	Batch(5500/6809) done. Loss: 0.1512  lr:0.010000
[ Fri Jul 12 14:01:40 2024 ] 	Batch(5600/6809) done. Loss: 0.4534  lr:0.010000
[ Fri Jul 12 14:01:59 2024 ] 	Batch(5700/6809) done. Loss: 0.3752  lr:0.010000
[ Fri Jul 12 14:02:17 2024 ] 	Batch(5800/6809) done. Loss: 0.4155  lr:0.010000
[ Fri Jul 12 14:02:36 2024 ] 	Batch(5900/6809) done. Loss: 0.0167  lr:0.010000
[ Fri Jul 12 14:02:54 2024 ] 
Training: Epoch [48/120], Step [5999], Loss: 0.0532265305519104, Training Accuracy: 88.37291666666667
[ Fri Jul 12 14:02:55 2024 ] 	Batch(6000/6809) done. Loss: 0.1383  lr:0.010000
[ Fri Jul 12 14:03:13 2024 ] 	Batch(6100/6809) done. Loss: 0.0176  lr:0.010000
[ Fri Jul 12 14:03:31 2024 ] 	Batch(6200/6809) done. Loss: 0.2382  lr:0.010000
[ Fri Jul 12 14:03:49 2024 ] 	Batch(6300/6809) done. Loss: 0.4827  lr:0.010000
[ Fri Jul 12 14:04:07 2024 ] 	Batch(6400/6809) done. Loss: 0.3624  lr:0.010000
[ Fri Jul 12 14:04:25 2024 ] 
Training: Epoch [48/120], Step [6499], Loss: 0.5186067223548889, Training Accuracy: 88.26346153846154
[ Fri Jul 12 14:04:25 2024 ] 	Batch(6500/6809) done. Loss: 0.7935  lr:0.010000
[ Fri Jul 12 14:04:43 2024 ] 	Batch(6600/6809) done. Loss: 0.2589  lr:0.010000
[ Fri Jul 12 14:05:01 2024 ] 	Batch(6700/6809) done. Loss: 0.4603  lr:0.010000
[ Fri Jul 12 14:05:19 2024 ] 	Batch(6800/6809) done. Loss: 0.5820  lr:0.010000
[ Fri Jul 12 14:05:20 2024 ] 	Mean training loss: 0.3708.
[ Fri Jul 12 14:05:20 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 14:05:20 2024 ] Training epoch: 50
[ Fri Jul 12 14:05:21 2024 ] 	Batch(0/6809) done. Loss: 0.4835  lr:0.010000
[ Fri Jul 12 14:05:39 2024 ] 	Batch(100/6809) done. Loss: 0.0749  lr:0.010000
[ Fri Jul 12 14:05:56 2024 ] 	Batch(200/6809) done. Loss: 0.2838  lr:0.010000
[ Fri Jul 12 14:06:14 2024 ] 	Batch(300/6809) done. Loss: 0.7386  lr:0.010000
[ Fri Jul 12 14:06:32 2024 ] 	Batch(400/6809) done. Loss: 0.6985  lr:0.010000
[ Fri Jul 12 14:06:50 2024 ] 
Training: Epoch [49/120], Step [499], Loss: 0.6782461404800415, Training Accuracy: 88.64999999999999
[ Fri Jul 12 14:06:50 2024 ] 	Batch(500/6809) done. Loss: 0.2572  lr:0.010000
[ Fri Jul 12 14:07:08 2024 ] 	Batch(600/6809) done. Loss: 1.1679  lr:0.010000
[ Fri Jul 12 14:07:26 2024 ] 	Batch(700/6809) done. Loss: 0.2619  lr:0.010000
[ Fri Jul 12 14:07:45 2024 ] 	Batch(800/6809) done. Loss: 0.9633  lr:0.010000
[ Fri Jul 12 14:08:03 2024 ] 	Batch(900/6809) done. Loss: 0.1726  lr:0.010000
[ Fri Jul 12 14:08:21 2024 ] 
Training: Epoch [49/120], Step [999], Loss: 0.6531658172607422, Training Accuracy: 89.0625
[ Fri Jul 12 14:08:21 2024 ] 	Batch(1000/6809) done. Loss: 0.5420  lr:0.010000
[ Fri Jul 12 14:08:39 2024 ] 	Batch(1100/6809) done. Loss: 0.6075  lr:0.010000
[ Fri Jul 12 14:08:57 2024 ] 	Batch(1200/6809) done. Loss: 0.2458  lr:0.010000
[ Fri Jul 12 14:09:15 2024 ] 	Batch(1300/6809) done. Loss: 0.0810  lr:0.010000
[ Fri Jul 12 14:09:33 2024 ] 	Batch(1400/6809) done. Loss: 0.1942  lr:0.010000
[ Fri Jul 12 14:09:50 2024 ] 
Training: Epoch [49/120], Step [1499], Loss: 0.16159693896770477, Training Accuracy: 89.19166666666666
[ Fri Jul 12 14:09:50 2024 ] 	Batch(1500/6809) done. Loss: 0.2060  lr:0.010000
[ Fri Jul 12 14:10:08 2024 ] 	Batch(1600/6809) done. Loss: 1.4600  lr:0.010000
[ Fri Jul 12 14:10:26 2024 ] 	Batch(1700/6809) done. Loss: 0.4261  lr:0.010000
[ Fri Jul 12 14:10:44 2024 ] 	Batch(1800/6809) done. Loss: 0.3023  lr:0.010000
[ Fri Jul 12 14:11:02 2024 ] 	Batch(1900/6809) done. Loss: 0.1792  lr:0.010000
[ Fri Jul 12 14:11:20 2024 ] 
Training: Epoch [49/120], Step [1999], Loss: 0.20316995680332184, Training Accuracy: 89.20625
[ Fri Jul 12 14:11:20 2024 ] 	Batch(2000/6809) done. Loss: 0.3315  lr:0.010000
[ Fri Jul 12 14:11:38 2024 ] 	Batch(2100/6809) done. Loss: 0.9498  lr:0.010000
[ Fri Jul 12 14:11:56 2024 ] 	Batch(2200/6809) done. Loss: 0.5072  lr:0.010000
[ Fri Jul 12 14:12:14 2024 ] 	Batch(2300/6809) done. Loss: 0.4605  lr:0.010000
[ Fri Jul 12 14:12:32 2024 ] 	Batch(2400/6809) done. Loss: 0.1517  lr:0.010000
[ Fri Jul 12 14:12:50 2024 ] 
Training: Epoch [49/120], Step [2499], Loss: 0.6892821788787842, Training Accuracy: 89.105
[ Fri Jul 12 14:12:50 2024 ] 	Batch(2500/6809) done. Loss: 0.4508  lr:0.010000
[ Fri Jul 12 14:13:08 2024 ] 	Batch(2600/6809) done. Loss: 0.1003  lr:0.010000
[ Fri Jul 12 14:13:25 2024 ] 	Batch(2700/6809) done. Loss: 0.4288  lr:0.010000
[ Fri Jul 12 14:13:43 2024 ] 	Batch(2800/6809) done. Loss: 0.5287  lr:0.010000
[ Fri Jul 12 14:14:01 2024 ] 	Batch(2900/6809) done. Loss: 0.0590  lr:0.010000
[ Fri Jul 12 14:14:19 2024 ] 
Training: Epoch [49/120], Step [2999], Loss: 0.2820036709308624, Training Accuracy: 88.99583333333332
[ Fri Jul 12 14:14:19 2024 ] 	Batch(3000/6809) done. Loss: 0.3163  lr:0.010000
[ Fri Jul 12 14:14:37 2024 ] 	Batch(3100/6809) done. Loss: 0.1982  lr:0.010000
[ Fri Jul 12 14:14:55 2024 ] 	Batch(3200/6809) done. Loss: 0.3824  lr:0.010000
[ Fri Jul 12 14:15:13 2024 ] 	Batch(3300/6809) done. Loss: 0.0810  lr:0.010000
[ Fri Jul 12 14:15:31 2024 ] 	Batch(3400/6809) done. Loss: 0.0978  lr:0.010000
[ Fri Jul 12 14:15:49 2024 ] 
Training: Epoch [49/120], Step [3499], Loss: 0.6719738245010376, Training Accuracy: 89.04285714285714
[ Fri Jul 12 14:15:49 2024 ] 	Batch(3500/6809) done. Loss: 0.1940  lr:0.010000
[ Fri Jul 12 14:16:07 2024 ] 	Batch(3600/6809) done. Loss: 0.3938  lr:0.010000
[ Fri Jul 12 14:16:26 2024 ] 	Batch(3700/6809) done. Loss: 0.0596  lr:0.010000
[ Fri Jul 12 14:16:44 2024 ] 	Batch(3800/6809) done. Loss: 0.5039  lr:0.010000
[ Fri Jul 12 14:17:02 2024 ] 	Batch(3900/6809) done. Loss: 0.7439  lr:0.010000
[ Fri Jul 12 14:17:20 2024 ] 
Training: Epoch [49/120], Step [3999], Loss: 0.6832864284515381, Training Accuracy: 88.93124999999999
[ Fri Jul 12 14:17:20 2024 ] 	Batch(4000/6809) done. Loss: 0.7425  lr:0.010000
[ Fri Jul 12 14:17:39 2024 ] 	Batch(4100/6809) done. Loss: 0.0389  lr:0.010000
[ Fri Jul 12 14:17:57 2024 ] 	Batch(4200/6809) done. Loss: 0.2666  lr:0.010000
[ Fri Jul 12 14:18:16 2024 ] 	Batch(4300/6809) done. Loss: 0.2544  lr:0.010000
[ Fri Jul 12 14:18:34 2024 ] 	Batch(4400/6809) done. Loss: 0.1098  lr:0.010000
[ Fri Jul 12 14:18:53 2024 ] 
Training: Epoch [49/120], Step [4499], Loss: 0.3248782455921173, Training Accuracy: 88.74444444444445
[ Fri Jul 12 14:18:53 2024 ] 	Batch(4500/6809) done. Loss: 0.3414  lr:0.010000
[ Fri Jul 12 14:19:11 2024 ] 	Batch(4600/6809) done. Loss: 0.2830  lr:0.010000
[ Fri Jul 12 14:19:30 2024 ] 	Batch(4700/6809) done. Loss: 0.1173  lr:0.010000
[ Fri Jul 12 14:19:48 2024 ] 	Batch(4800/6809) done. Loss: 0.0800  lr:0.010000
[ Fri Jul 12 14:20:06 2024 ] 	Batch(4900/6809) done. Loss: 0.2158  lr:0.010000
[ Fri Jul 12 14:20:24 2024 ] 
Training: Epoch [49/120], Step [4999], Loss: 0.24485012888908386, Training Accuracy: 88.5725
[ Fri Jul 12 14:20:24 2024 ] 	Batch(5000/6809) done. Loss: 0.1620  lr:0.010000
[ Fri Jul 12 14:20:42 2024 ] 	Batch(5100/6809) done. Loss: 0.3955  lr:0.010000
[ Fri Jul 12 14:21:00 2024 ] 	Batch(5200/6809) done. Loss: 0.2374  lr:0.010000
[ Fri Jul 12 14:21:18 2024 ] 	Batch(5300/6809) done. Loss: 0.2655  lr:0.010000
[ Fri Jul 12 14:21:36 2024 ] 	Batch(5400/6809) done. Loss: 0.0686  lr:0.010000
[ Fri Jul 12 14:21:55 2024 ] 
Training: Epoch [49/120], Step [5499], Loss: 0.032423730939626694, Training Accuracy: 88.46818181818182
[ Fri Jul 12 14:21:55 2024 ] 	Batch(5500/6809) done. Loss: 0.7070  lr:0.010000
[ Fri Jul 12 14:22:14 2024 ] 	Batch(5600/6809) done. Loss: 1.3203  lr:0.010000
[ Fri Jul 12 14:22:31 2024 ] 	Batch(5700/6809) done. Loss: 0.5629  lr:0.010000
[ Fri Jul 12 14:22:49 2024 ] 	Batch(5800/6809) done. Loss: 0.2063  lr:0.010000
[ Fri Jul 12 14:23:07 2024 ] 	Batch(5900/6809) done. Loss: 1.0429  lr:0.010000
[ Fri Jul 12 14:23:25 2024 ] 
Training: Epoch [49/120], Step [5999], Loss: 0.08388236910104752, Training Accuracy: 88.45208333333333
[ Fri Jul 12 14:23:25 2024 ] 	Batch(6000/6809) done. Loss: 0.1255  lr:0.010000
[ Fri Jul 12 14:23:43 2024 ] 	Batch(6100/6809) done. Loss: 0.1872  lr:0.010000
[ Fri Jul 12 14:24:01 2024 ] 	Batch(6200/6809) done. Loss: 0.0090  lr:0.010000
[ Fri Jul 12 14:24:19 2024 ] 	Batch(6300/6809) done. Loss: 0.0789  lr:0.010000
[ Fri Jul 12 14:24:37 2024 ] 	Batch(6400/6809) done. Loss: 0.2000  lr:0.010000
[ Fri Jul 12 14:24:55 2024 ] 
Training: Epoch [49/120], Step [6499], Loss: 0.02518545091152191, Training Accuracy: 88.51538461538462
[ Fri Jul 12 14:24:55 2024 ] 	Batch(6500/6809) done. Loss: 1.1810  lr:0.010000
[ Fri Jul 12 14:25:13 2024 ] 	Batch(6600/6809) done. Loss: 0.5536  lr:0.010000
[ Fri Jul 12 14:25:31 2024 ] 	Batch(6700/6809) done. Loss: 0.1396  lr:0.010000
[ Fri Jul 12 14:25:49 2024 ] 	Batch(6800/6809) done. Loss: 0.3403  lr:0.010000
[ Fri Jul 12 14:25:51 2024 ] 	Mean training loss: 0.3548.
[ Fri Jul 12 14:25:51 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 14:25:51 2024 ] Eval epoch: 50
[ Fri Jul 12 14:31:25 2024 ] 	Mean val loss of 7435 batches: 1.1271008379227596.
[ Fri Jul 12 14:31:25 2024 ] 
Validation: Epoch [49/120], Samples [45631.0/59477], Loss: 0.02753886580467224, Validation Accuracy: 76.7204129327303
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 1 : 297 / 500 = 59 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 2 : 358 / 499 = 71 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 3 : 392 / 500 = 78 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 4 : 409 / 502 = 81 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 5 : 431 / 502 = 85 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 6 : 344 / 502 = 68 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 7 : 458 / 497 = 92 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 8 : 478 / 498 = 95 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 9 : 354 / 500 = 70 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 10 : 233 / 500 = 46 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 11 : 128 / 498 = 25 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 12 : 359 / 499 = 71 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 13 : 471 / 502 = 93 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 14 : 467 / 504 = 92 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 15 : 445 / 502 = 88 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 16 : 314 / 502 = 62 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 17 : 435 / 504 = 86 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 18 : 373 / 504 = 74 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 19 : 459 / 502 = 91 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 20 : 467 / 502 = 93 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 21 : 479 / 503 = 95 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 22 : 370 / 504 = 73 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 23 : 423 / 503 = 84 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 24 : 348 / 504 = 69 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 25 : 479 / 504 = 95 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 26 : 463 / 504 = 91 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 27 : 406 / 501 = 81 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 28 : 281 / 502 = 55 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 29 : 321 / 502 = 63 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 30 : 235 / 501 = 46 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 31 : 438 / 504 = 86 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 32 : 401 / 503 = 79 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 33 : 376 / 503 = 74 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 34 : 472 / 504 = 93 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 35 : 436 / 503 = 86 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 36 : 353 / 502 = 70 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 37 : 436 / 504 = 86 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 38 : 433 / 504 = 85 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 39 : 433 / 498 = 86 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 40 : 338 / 504 = 67 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 41 : 369 / 503 = 73 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 42 : 455 / 504 = 90 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 43 : 347 / 503 = 68 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 44 : 471 / 504 = 93 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 45 : 392 / 504 = 77 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 46 : 414 / 504 = 82 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 47 : 361 / 503 = 71 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 48 : 375 / 503 = 74 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 49 : 377 / 499 = 75 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 50 : 419 / 502 = 83 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 51 : 461 / 503 = 91 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 52 : 347 / 504 = 68 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 53 : 386 / 497 = 77 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 54 : 446 / 480 = 92 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 55 : 332 / 504 = 65 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 56 : 389 / 503 = 77 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 57 : 479 / 504 = 95 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 58 : 478 / 499 = 95 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 59 : 487 / 503 = 96 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 60 : 411 / 479 = 85 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 61 : 420 / 484 = 86 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 62 : 394 / 487 = 80 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 63 : 444 / 489 = 90 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 64 : 369 / 488 = 75 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 65 : 444 / 490 = 90 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 66 : 349 / 488 = 71 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 67 : 394 / 490 = 80 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 68 : 303 / 490 = 61 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 69 : 388 / 490 = 79 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 70 : 235 / 490 = 47 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 71 : 146 / 490 = 29 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 72 : 120 / 488 = 24 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 73 : 210 / 486 = 43 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 74 : 276 / 481 = 57 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 75 : 338 / 488 = 69 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 76 : 292 / 489 = 59 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 77 : 330 / 488 = 67 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 78 : 369 / 488 = 75 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 79 : 434 / 490 = 88 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 80 : 397 / 489 = 81 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 81 : 295 / 491 = 60 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 82 : 301 / 491 = 61 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 83 : 308 / 489 = 62 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 84 : 391 / 489 = 79 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 85 : 371 / 489 = 75 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 86 : 428 / 491 = 87 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 87 : 440 / 492 = 89 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 88 : 303 / 491 = 61 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 89 : 412 / 492 = 83 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 90 : 286 / 490 = 58 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 91 : 392 / 482 = 81 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 92 : 355 / 490 = 72 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 93 : 326 / 487 = 66 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 94 : 405 / 489 = 82 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 95 : 402 / 490 = 82 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 96 : 463 / 491 = 94 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 97 : 459 / 490 = 93 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 98 : 454 / 491 = 92 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 99 : 438 / 491 = 89 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 100 : 458 / 491 = 93 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 101 : 413 / 491 = 84 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 102 : 263 / 492 = 53 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 103 : 387 / 492 = 78 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 104 : 232 / 491 = 47 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 105 : 265 / 491 = 53 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 106 : 211 / 492 = 42 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 107 : 424 / 491 = 86 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 108 : 394 / 492 = 80 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 109 : 354 / 490 = 72 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 110 : 354 / 491 = 72 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 111 : 439 / 492 = 89 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 112 : 453 / 492 = 92 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 113 : 432 / 491 = 87 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 114 : 370 / 491 = 75 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 115 : 408 / 492 = 82 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 116 : 373 / 491 = 75 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 117 : 455 / 492 = 92 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 118 : 421 / 490 = 85 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 119 : 429 / 492 = 87 %
[ Fri Jul 12 14:31:25 2024 ] Accuracy of 120 : 426 / 500 = 85 %
[ Fri Jul 12 14:31:25 2024 ] Training epoch: 51
[ Fri Jul 12 14:31:26 2024 ] 	Batch(0/6809) done. Loss: 0.4431  lr:0.010000
[ Fri Jul 12 14:31:44 2024 ] 	Batch(100/6809) done. Loss: 0.1939  lr:0.010000
[ Fri Jul 12 14:32:02 2024 ] 	Batch(200/6809) done. Loss: 0.2545  lr:0.010000
[ Fri Jul 12 14:32:20 2024 ] 	Batch(300/6809) done. Loss: 0.2493  lr:0.010000
[ Fri Jul 12 14:32:38 2024 ] 	Batch(400/6809) done. Loss: 0.2144  lr:0.010000
[ Fri Jul 12 14:32:55 2024 ] 
Training: Epoch [50/120], Step [499], Loss: 0.24293488264083862, Training Accuracy: 89.425
[ Fri Jul 12 14:32:56 2024 ] 	Batch(500/6809) done. Loss: 0.3533  lr:0.010000
[ Fri Jul 12 14:33:13 2024 ] 	Batch(600/6809) done. Loss: 0.3251  lr:0.010000
[ Fri Jul 12 14:33:31 2024 ] 	Batch(700/6809) done. Loss: 0.0656  lr:0.010000
[ Fri Jul 12 14:33:49 2024 ] 	Batch(800/6809) done. Loss: 0.1433  lr:0.010000
[ Fri Jul 12 14:34:07 2024 ] 	Batch(900/6809) done. Loss: 0.3725  lr:0.010000
[ Fri Jul 12 14:34:25 2024 ] 
Training: Epoch [50/120], Step [999], Loss: 0.35164451599121094, Training Accuracy: 88.9875
[ Fri Jul 12 14:34:25 2024 ] 	Batch(1000/6809) done. Loss: 0.0704  lr:0.010000
[ Fri Jul 12 14:34:43 2024 ] 	Batch(1100/6809) done. Loss: 0.3694  lr:0.010000
[ Fri Jul 12 14:35:01 2024 ] 	Batch(1200/6809) done. Loss: 0.0054  lr:0.010000
[ Fri Jul 12 14:35:19 2024 ] 	Batch(1300/6809) done. Loss: 0.3245  lr:0.010000
[ Fri Jul 12 14:35:38 2024 ] 	Batch(1400/6809) done. Loss: 0.1701  lr:0.010000
[ Fri Jul 12 14:35:56 2024 ] 
Training: Epoch [50/120], Step [1499], Loss: 0.5219154357910156, Training Accuracy: 89.58333333333334
[ Fri Jul 12 14:35:57 2024 ] 	Batch(1500/6809) done. Loss: 0.2127  lr:0.010000
[ Fri Jul 12 14:36:15 2024 ] 	Batch(1600/6809) done. Loss: 0.5069  lr:0.010000
[ Fri Jul 12 14:36:33 2024 ] 	Batch(1700/6809) done. Loss: 0.7557  lr:0.010000
[ Fri Jul 12 14:36:51 2024 ] 	Batch(1800/6809) done. Loss: 0.6908  lr:0.010000
[ Fri Jul 12 14:37:09 2024 ] 	Batch(1900/6809) done. Loss: 0.2594  lr:0.010000
[ Fri Jul 12 14:37:27 2024 ] 
Training: Epoch [50/120], Step [1999], Loss: 0.17956848442554474, Training Accuracy: 89.34375
[ Fri Jul 12 14:37:27 2024 ] 	Batch(2000/6809) done. Loss: 0.0386  lr:0.010000
[ Fri Jul 12 14:37:45 2024 ] 	Batch(2100/6809) done. Loss: 0.5573  lr:0.010000
[ Fri Jul 12 14:38:03 2024 ] 	Batch(2200/6809) done. Loss: 0.2123  lr:0.010000
[ Fri Jul 12 14:38:21 2024 ] 	Batch(2300/6809) done. Loss: 0.0625  lr:0.010000
[ Fri Jul 12 14:38:39 2024 ] 	Batch(2400/6809) done. Loss: 0.4128  lr:0.010000
[ Fri Jul 12 14:38:56 2024 ] 
Training: Epoch [50/120], Step [2499], Loss: 0.22612707316875458, Training Accuracy: 89.205
[ Fri Jul 12 14:38:56 2024 ] 	Batch(2500/6809) done. Loss: 0.9204  lr:0.010000
[ Fri Jul 12 14:39:14 2024 ] 	Batch(2600/6809) done. Loss: 0.1699  lr:0.010000
[ Fri Jul 12 14:39:32 2024 ] 	Batch(2700/6809) done. Loss: 0.2158  lr:0.010000
[ Fri Jul 12 14:39:50 2024 ] 	Batch(2800/6809) done. Loss: 1.4827  lr:0.010000
[ Fri Jul 12 14:40:08 2024 ] 	Batch(2900/6809) done. Loss: 0.3098  lr:0.010000
[ Fri Jul 12 14:40:26 2024 ] 
Training: Epoch [50/120], Step [2999], Loss: 0.4528030753135681, Training Accuracy: 89.1125
[ Fri Jul 12 14:40:26 2024 ] 	Batch(3000/6809) done. Loss: 0.0677  lr:0.010000
[ Fri Jul 12 14:40:44 2024 ] 	Batch(3100/6809) done. Loss: 0.5902  lr:0.010000
[ Fri Jul 12 14:41:02 2024 ] 	Batch(3200/6809) done. Loss: 0.0193  lr:0.010000
[ Fri Jul 12 14:41:19 2024 ] 	Batch(3300/6809) done. Loss: 0.7873  lr:0.010000
[ Fri Jul 12 14:41:37 2024 ] 	Batch(3400/6809) done. Loss: 0.2318  lr:0.010000
[ Fri Jul 12 14:41:55 2024 ] 
Training: Epoch [50/120], Step [3499], Loss: 1.0118268728256226, Training Accuracy: 89.10714285714286
[ Fri Jul 12 14:41:55 2024 ] 	Batch(3500/6809) done. Loss: 0.3107  lr:0.010000
[ Fri Jul 12 14:42:13 2024 ] 	Batch(3600/6809) done. Loss: 0.2494  lr:0.010000
[ Fri Jul 12 14:42:31 2024 ] 	Batch(3700/6809) done. Loss: 0.0476  lr:0.010000
[ Fri Jul 12 14:42:49 2024 ] 	Batch(3800/6809) done. Loss: 0.2987  lr:0.010000
[ Fri Jul 12 14:43:06 2024 ] 	Batch(3900/6809) done. Loss: 0.1224  lr:0.010000
[ Fri Jul 12 14:43:24 2024 ] 
Training: Epoch [50/120], Step [3999], Loss: 0.013764334842562675, Training Accuracy: 88.91562499999999
[ Fri Jul 12 14:43:24 2024 ] 	Batch(4000/6809) done. Loss: 0.0866  lr:0.010000
[ Fri Jul 12 14:43:42 2024 ] 	Batch(4100/6809) done. Loss: 0.4016  lr:0.010000
[ Fri Jul 12 14:44:00 2024 ] 	Batch(4200/6809) done. Loss: 0.3085  lr:0.010000
[ Fri Jul 12 14:44:18 2024 ] 	Batch(4300/6809) done. Loss: 0.0907  lr:0.010000
[ Fri Jul 12 14:44:36 2024 ] 	Batch(4400/6809) done. Loss: 0.1095  lr:0.010000
[ Fri Jul 12 14:44:54 2024 ] 
Training: Epoch [50/120], Step [4499], Loss: 0.28680741786956787, Training Accuracy: 88.97222222222221
[ Fri Jul 12 14:44:54 2024 ] 	Batch(4500/6809) done. Loss: 0.0722  lr:0.010000
[ Fri Jul 12 14:45:12 2024 ] 	Batch(4600/6809) done. Loss: 0.2114  lr:0.010000
[ Fri Jul 12 14:45:30 2024 ] 	Batch(4700/6809) done. Loss: 0.4828  lr:0.010000
[ Fri Jul 12 14:45:48 2024 ] 	Batch(4800/6809) done. Loss: 0.5404  lr:0.010000
[ Fri Jul 12 14:46:06 2024 ] 	Batch(4900/6809) done. Loss: 0.2790  lr:0.010000
[ Fri Jul 12 14:46:23 2024 ] 
Training: Epoch [50/120], Step [4999], Loss: 2.106337308883667, Training Accuracy: 88.90249999999999
[ Fri Jul 12 14:46:24 2024 ] 	Batch(5000/6809) done. Loss: 0.0183  lr:0.010000
[ Fri Jul 12 14:46:41 2024 ] 	Batch(5100/6809) done. Loss: 0.4293  lr:0.010000
[ Fri Jul 12 14:46:59 2024 ] 	Batch(5200/6809) done. Loss: 0.1347  lr:0.010000
[ Fri Jul 12 14:47:17 2024 ] 	Batch(5300/6809) done. Loss: 0.0294  lr:0.010000
[ Fri Jul 12 14:47:35 2024 ] 	Batch(5400/6809) done. Loss: 0.1377  lr:0.010000
[ Fri Jul 12 14:47:53 2024 ] 
Training: Epoch [50/120], Step [5499], Loss: 0.34077906608581543, Training Accuracy: 88.80681818181819
[ Fri Jul 12 14:47:53 2024 ] 	Batch(5500/6809) done. Loss: 0.4120  lr:0.010000
[ Fri Jul 12 14:48:11 2024 ] 	Batch(5600/6809) done. Loss: 0.3415  lr:0.010000
[ Fri Jul 12 14:48:29 2024 ] 	Batch(5700/6809) done. Loss: 0.0997  lr:0.010000
[ Fri Jul 12 14:48:47 2024 ] 	Batch(5800/6809) done. Loss: 0.3897  lr:0.010000
[ Fri Jul 12 14:49:05 2024 ] 	Batch(5900/6809) done. Loss: 0.2058  lr:0.010000
[ Fri Jul 12 14:49:23 2024 ] 
Training: Epoch [50/120], Step [5999], Loss: 0.19125190377235413, Training Accuracy: 88.79583333333333
[ Fri Jul 12 14:49:23 2024 ] 	Batch(6000/6809) done. Loss: 0.3201  lr:0.010000
[ Fri Jul 12 14:49:41 2024 ] 	Batch(6100/6809) done. Loss: 0.6527  lr:0.010000
[ Fri Jul 12 14:49:59 2024 ] 	Batch(6200/6809) done. Loss: 0.6642  lr:0.010000
[ Fri Jul 12 14:50:17 2024 ] 	Batch(6300/6809) done. Loss: 0.1070  lr:0.010000
[ Fri Jul 12 14:50:35 2024 ] 	Batch(6400/6809) done. Loss: 0.1541  lr:0.010000
[ Fri Jul 12 14:50:53 2024 ] 
Training: Epoch [50/120], Step [6499], Loss: 0.0430811382830143, Training Accuracy: 88.66923076923077
[ Fri Jul 12 14:50:53 2024 ] 	Batch(6500/6809) done. Loss: 0.2462  lr:0.010000
[ Fri Jul 12 14:51:11 2024 ] 	Batch(6600/6809) done. Loss: 0.6179  lr:0.010000
[ Fri Jul 12 14:51:29 2024 ] 	Batch(6700/6809) done. Loss: 0.2626  lr:0.010000
[ Fri Jul 12 14:51:47 2024 ] 	Batch(6800/6809) done. Loss: 0.6438  lr:0.010000
[ Fri Jul 12 14:51:48 2024 ] 	Mean training loss: 0.3518.
[ Fri Jul 12 14:51:48 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 14:51:48 2024 ] Training epoch: 52
[ Fri Jul 12 14:51:49 2024 ] 	Batch(0/6809) done. Loss: 0.4317  lr:0.010000
[ Fri Jul 12 14:52:07 2024 ] 	Batch(100/6809) done. Loss: 0.5387  lr:0.010000
[ Fri Jul 12 14:52:25 2024 ] 	Batch(200/6809) done. Loss: 0.0607  lr:0.010000
[ Fri Jul 12 14:52:42 2024 ] 	Batch(300/6809) done. Loss: 0.8871  lr:0.010000
[ Fri Jul 12 14:53:00 2024 ] 	Batch(400/6809) done. Loss: 0.4121  lr:0.010000
[ Fri Jul 12 14:53:18 2024 ] 
Training: Epoch [51/120], Step [499], Loss: 0.4115138053894043, Training Accuracy: 89.075
[ Fri Jul 12 14:53:18 2024 ] 	Batch(500/6809) done. Loss: 0.0106  lr:0.010000
[ Fri Jul 12 14:53:36 2024 ] 	Batch(600/6809) done. Loss: 0.4355  lr:0.010000
[ Fri Jul 12 14:53:54 2024 ] 	Batch(700/6809) done. Loss: 0.3439  lr:0.010000
[ Fri Jul 12 14:54:12 2024 ] 	Batch(800/6809) done. Loss: 0.0298  lr:0.010000
[ Fri Jul 12 14:54:30 2024 ] 	Batch(900/6809) done. Loss: 0.7729  lr:0.010000
[ Fri Jul 12 14:54:48 2024 ] 
Training: Epoch [51/120], Step [999], Loss: 0.026075661182403564, Training Accuracy: 89.45
[ Fri Jul 12 14:54:48 2024 ] 	Batch(1000/6809) done. Loss: 0.0886  lr:0.010000
[ Fri Jul 12 14:55:06 2024 ] 	Batch(1100/6809) done. Loss: 0.3196  lr:0.010000
[ Fri Jul 12 14:55:24 2024 ] 	Batch(1200/6809) done. Loss: 0.8544  lr:0.010000
[ Fri Jul 12 14:55:42 2024 ] 	Batch(1300/6809) done. Loss: 0.2535  lr:0.010000
[ Fri Jul 12 14:56:00 2024 ] 	Batch(1400/6809) done. Loss: 0.0768  lr:0.010000
[ Fri Jul 12 14:56:17 2024 ] 
Training: Epoch [51/120], Step [1499], Loss: 0.5835387110710144, Training Accuracy: 89.525
[ Fri Jul 12 14:56:18 2024 ] 	Batch(1500/6809) done. Loss: 0.2265  lr:0.010000
[ Fri Jul 12 14:56:36 2024 ] 	Batch(1600/6809) done. Loss: 0.1876  lr:0.010000
[ Fri Jul 12 14:56:54 2024 ] 	Batch(1700/6809) done. Loss: 0.2998  lr:0.010000
[ Fri Jul 12 14:57:12 2024 ] 	Batch(1800/6809) done. Loss: 0.3021  lr:0.010000
[ Fri Jul 12 14:57:29 2024 ] 	Batch(1900/6809) done. Loss: 0.1382  lr:0.010000
[ Fri Jul 12 14:57:47 2024 ] 
Training: Epoch [51/120], Step [1999], Loss: 0.4268815815448761, Training Accuracy: 89.7375
[ Fri Jul 12 14:57:48 2024 ] 	Batch(2000/6809) done. Loss: 0.4548  lr:0.010000
[ Fri Jul 12 14:58:05 2024 ] 	Batch(2100/6809) done. Loss: 0.0156  lr:0.010000
[ Fri Jul 12 14:58:23 2024 ] 	Batch(2200/6809) done. Loss: 0.0515  lr:0.010000
[ Fri Jul 12 14:58:41 2024 ] 	Batch(2300/6809) done. Loss: 0.3546  lr:0.010000
[ Fri Jul 12 14:58:59 2024 ] 	Batch(2400/6809) done. Loss: 0.5076  lr:0.010000
[ Fri Jul 12 14:59:17 2024 ] 
Training: Epoch [51/120], Step [2499], Loss: 0.3763619065284729, Training Accuracy: 89.66499999999999
[ Fri Jul 12 14:59:17 2024 ] 	Batch(2500/6809) done. Loss: 0.2186  lr:0.010000
[ Fri Jul 12 14:59:35 2024 ] 	Batch(2600/6809) done. Loss: 0.1744  lr:0.010000
[ Fri Jul 12 14:59:53 2024 ] 	Batch(2700/6809) done. Loss: 0.0271  lr:0.010000
[ Fri Jul 12 15:00:11 2024 ] 	Batch(2800/6809) done. Loss: 0.0464  lr:0.010000
[ Fri Jul 12 15:00:29 2024 ] 	Batch(2900/6809) done. Loss: 0.4652  lr:0.010000
[ Fri Jul 12 15:00:47 2024 ] 
Training: Epoch [51/120], Step [2999], Loss: 0.7044498324394226, Training Accuracy: 89.5
[ Fri Jul 12 15:00:47 2024 ] 	Batch(3000/6809) done. Loss: 0.8461  lr:0.010000
[ Fri Jul 12 15:01:05 2024 ] 	Batch(3100/6809) done. Loss: 0.7876  lr:0.010000
[ Fri Jul 12 15:01:23 2024 ] 	Batch(3200/6809) done. Loss: 0.1254  lr:0.010000
[ Fri Jul 12 15:01:41 2024 ] 	Batch(3300/6809) done. Loss: 0.5523  lr:0.010000
[ Fri Jul 12 15:01:59 2024 ] 	Batch(3400/6809) done. Loss: 0.0876  lr:0.010000
[ Fri Jul 12 15:02:17 2024 ] 
Training: Epoch [51/120], Step [3499], Loss: 0.19824162125587463, Training Accuracy: 89.26428571428572
[ Fri Jul 12 15:02:17 2024 ] 	Batch(3500/6809) done. Loss: 0.1924  lr:0.010000
[ Fri Jul 12 15:02:35 2024 ] 	Batch(3600/6809) done. Loss: 0.3236  lr:0.010000
[ Fri Jul 12 15:02:53 2024 ] 	Batch(3700/6809) done. Loss: 0.2345  lr:0.010000
[ Fri Jul 12 15:03:11 2024 ] 	Batch(3800/6809) done. Loss: 0.5637  lr:0.010000
[ Fri Jul 12 15:03:29 2024 ] 	Batch(3900/6809) done. Loss: 0.2467  lr:0.010000
[ Fri Jul 12 15:03:47 2024 ] 
Training: Epoch [51/120], Step [3999], Loss: 0.37067919969558716, Training Accuracy: 89.090625
[ Fri Jul 12 15:03:47 2024 ] 	Batch(4000/6809) done. Loss: 0.5495  lr:0.010000
[ Fri Jul 12 15:04:05 2024 ] 	Batch(4100/6809) done. Loss: 0.5888  lr:0.010000
[ Fri Jul 12 15:04:23 2024 ] 	Batch(4200/6809) done. Loss: 0.9443  lr:0.010000
[ Fri Jul 12 15:04:41 2024 ] 	Batch(4300/6809) done. Loss: 0.3410  lr:0.010000
[ Fri Jul 12 15:04:59 2024 ] 	Batch(4400/6809) done. Loss: 0.3079  lr:0.010000
[ Fri Jul 12 15:05:17 2024 ] 
Training: Epoch [51/120], Step [4499], Loss: 0.165559321641922, Training Accuracy: 89.06666666666668
[ Fri Jul 12 15:05:17 2024 ] 	Batch(4500/6809) done. Loss: 0.2034  lr:0.010000
[ Fri Jul 12 15:05:35 2024 ] 	Batch(4600/6809) done. Loss: 0.3475  lr:0.010000
[ Fri Jul 12 15:05:52 2024 ] 	Batch(4700/6809) done. Loss: 0.0406  lr:0.010000
[ Fri Jul 12 15:06:10 2024 ] 	Batch(4800/6809) done. Loss: 0.9968  lr:0.010000
[ Fri Jul 12 15:06:28 2024 ] 	Batch(4900/6809) done. Loss: 0.0471  lr:0.010000
[ Fri Jul 12 15:06:46 2024 ] 
Training: Epoch [51/120], Step [4999], Loss: 0.6826117634773254, Training Accuracy: 89.0675
[ Fri Jul 12 15:06:46 2024 ] 	Batch(5000/6809) done. Loss: 0.1490  lr:0.010000
[ Fri Jul 12 15:07:04 2024 ] 	Batch(5100/6809) done. Loss: 1.1690  lr:0.010000
[ Fri Jul 12 15:07:22 2024 ] 	Batch(5200/6809) done. Loss: 0.6114  lr:0.010000
[ Fri Jul 12 15:07:41 2024 ] 	Batch(5300/6809) done. Loss: 0.4915  lr:0.010000
[ Fri Jul 12 15:07:58 2024 ] 	Batch(5400/6809) done. Loss: 0.6556  lr:0.010000
[ Fri Jul 12 15:08:16 2024 ] 
Training: Epoch [51/120], Step [5499], Loss: 0.10458394885063171, Training Accuracy: 88.96136363636363
[ Fri Jul 12 15:08:16 2024 ] 	Batch(5500/6809) done. Loss: 0.1164  lr:0.010000
[ Fri Jul 12 15:08:34 2024 ] 	Batch(5600/6809) done. Loss: 0.3833  lr:0.010000
[ Fri Jul 12 15:08:52 2024 ] 	Batch(5700/6809) done. Loss: 0.5044  lr:0.010000
[ Fri Jul 12 15:09:10 2024 ] 	Batch(5800/6809) done. Loss: 0.2767  lr:0.010000
[ Fri Jul 12 15:09:28 2024 ] 	Batch(5900/6809) done. Loss: 0.2746  lr:0.010000
[ Fri Jul 12 15:09:46 2024 ] 
Training: Epoch [51/120], Step [5999], Loss: 0.5437379479408264, Training Accuracy: 88.88541666666666
[ Fri Jul 12 15:09:46 2024 ] 	Batch(6000/6809) done. Loss: 0.3096  lr:0.010000
[ Fri Jul 12 15:10:04 2024 ] 	Batch(6100/6809) done. Loss: 0.3459  lr:0.010000
[ Fri Jul 12 15:10:22 2024 ] 	Batch(6200/6809) done. Loss: 0.0923  lr:0.010000
[ Fri Jul 12 15:10:40 2024 ] 	Batch(6300/6809) done. Loss: 0.7863  lr:0.010000
[ Fri Jul 12 15:10:58 2024 ] 	Batch(6400/6809) done. Loss: 1.1483  lr:0.010000
[ Fri Jul 12 15:11:16 2024 ] 
Training: Epoch [51/120], Step [6499], Loss: 1.2980071306228638, Training Accuracy: 88.78653846153847
[ Fri Jul 12 15:11:16 2024 ] 	Batch(6500/6809) done. Loss: 0.1044  lr:0.010000
[ Fri Jul 12 15:11:34 2024 ] 	Batch(6600/6809) done. Loss: 0.3887  lr:0.010000
[ Fri Jul 12 15:11:52 2024 ] 	Batch(6700/6809) done. Loss: 0.0912  lr:0.010000
[ Fri Jul 12 15:12:10 2024 ] 	Batch(6800/6809) done. Loss: 0.1355  lr:0.010000
[ Fri Jul 12 15:12:11 2024 ] 	Mean training loss: 0.3534.
[ Fri Jul 12 15:12:11 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 15:12:12 2024 ] Training epoch: 53
[ Fri Jul 12 15:12:12 2024 ] 	Batch(0/6809) done. Loss: 0.3041  lr:0.010000
[ Fri Jul 12 15:12:30 2024 ] 	Batch(100/6809) done. Loss: 0.1478  lr:0.010000
[ Fri Jul 12 15:12:48 2024 ] 	Batch(200/6809) done. Loss: 0.5107  lr:0.010000
[ Fri Jul 12 15:13:06 2024 ] 	Batch(300/6809) done. Loss: 0.1563  lr:0.010000
[ Fri Jul 12 15:13:24 2024 ] 	Batch(400/6809) done. Loss: 0.2952  lr:0.010000
[ Fri Jul 12 15:13:42 2024 ] 
Training: Epoch [52/120], Step [499], Loss: 0.04263070225715637, Training Accuracy: 90.3
[ Fri Jul 12 15:13:42 2024 ] 	Batch(500/6809) done. Loss: 0.1725  lr:0.010000
[ Fri Jul 12 15:14:00 2024 ] 	Batch(600/6809) done. Loss: 0.3825  lr:0.010000
[ Fri Jul 12 15:14:18 2024 ] 	Batch(700/6809) done. Loss: 0.1312  lr:0.010000
[ Fri Jul 12 15:14:36 2024 ] 	Batch(800/6809) done. Loss: 0.2887  lr:0.010000
[ Fri Jul 12 15:14:54 2024 ] 	Batch(900/6809) done. Loss: 0.0211  lr:0.010000
[ Fri Jul 12 15:15:13 2024 ] 
Training: Epoch [52/120], Step [999], Loss: 0.0868096873164177, Training Accuracy: 90.01249999999999
[ Fri Jul 12 15:15:13 2024 ] 	Batch(1000/6809) done. Loss: 0.2637  lr:0.010000
[ Fri Jul 12 15:15:31 2024 ] 	Batch(1100/6809) done. Loss: 0.9217  lr:0.010000
[ Fri Jul 12 15:15:50 2024 ] 	Batch(1200/6809) done. Loss: 0.1858  lr:0.010000
[ Fri Jul 12 15:16:09 2024 ] 	Batch(1300/6809) done. Loss: 0.2898  lr:0.010000
[ Fri Jul 12 15:16:27 2024 ] 	Batch(1400/6809) done. Loss: 0.0035  lr:0.010000
[ Fri Jul 12 15:16:45 2024 ] 
Training: Epoch [52/120], Step [1499], Loss: 0.37246471643447876, Training Accuracy: 90.06666666666666
[ Fri Jul 12 15:16:46 2024 ] 	Batch(1500/6809) done. Loss: 0.5288  lr:0.010000
[ Fri Jul 12 15:17:04 2024 ] 	Batch(1600/6809) done. Loss: 0.1685  lr:0.010000
[ Fri Jul 12 15:17:23 2024 ] 	Batch(1700/6809) done. Loss: 0.1967  lr:0.010000
[ Fri Jul 12 15:17:41 2024 ] 	Batch(1800/6809) done. Loss: 0.0140  lr:0.010000
[ Fri Jul 12 15:18:00 2024 ] 	Batch(1900/6809) done. Loss: 0.1318  lr:0.010000
[ Fri Jul 12 15:18:18 2024 ] 
Training: Epoch [52/120], Step [1999], Loss: 0.24147050082683563, Training Accuracy: 89.625
[ Fri Jul 12 15:18:19 2024 ] 	Batch(2000/6809) done. Loss: 0.7870  lr:0.010000
[ Fri Jul 12 15:18:36 2024 ] 	Batch(2100/6809) done. Loss: 0.0125  lr:0.010000
[ Fri Jul 12 15:18:54 2024 ] 	Batch(2200/6809) done. Loss: 0.1266  lr:0.010000
[ Fri Jul 12 15:19:12 2024 ] 	Batch(2300/6809) done. Loss: 1.2634  lr:0.010000
[ Fri Jul 12 15:19:30 2024 ] 	Batch(2400/6809) done. Loss: 0.0239  lr:0.010000
[ Fri Jul 12 15:19:48 2024 ] 
Training: Epoch [52/120], Step [2499], Loss: 0.8590217232704163, Training Accuracy: 89.415
[ Fri Jul 12 15:19:48 2024 ] 	Batch(2500/6809) done. Loss: 0.2038  lr:0.010000
[ Fri Jul 12 15:20:06 2024 ] 	Batch(2600/6809) done. Loss: 0.2782  lr:0.010000
[ Fri Jul 12 15:20:24 2024 ] 	Batch(2700/6809) done. Loss: 1.7570  lr:0.010000
[ Fri Jul 12 15:20:42 2024 ] 	Batch(2800/6809) done. Loss: 1.1376  lr:0.010000
[ Fri Jul 12 15:21:00 2024 ] 	Batch(2900/6809) done. Loss: 0.5433  lr:0.010000
[ Fri Jul 12 15:21:18 2024 ] 
Training: Epoch [52/120], Step [2999], Loss: 0.12415526807308197, Training Accuracy: 89.19166666666666
[ Fri Jul 12 15:21:18 2024 ] 	Batch(3000/6809) done. Loss: 0.4299  lr:0.010000
[ Fri Jul 12 15:21:36 2024 ] 	Batch(3100/6809) done. Loss: 0.0222  lr:0.010000
[ Fri Jul 12 15:21:54 2024 ] 	Batch(3200/6809) done. Loss: 0.3492  lr:0.010000
[ Fri Jul 12 15:22:12 2024 ] 	Batch(3300/6809) done. Loss: 0.1340  lr:0.010000
[ Fri Jul 12 15:22:30 2024 ] 	Batch(3400/6809) done. Loss: 0.1325  lr:0.010000
[ Fri Jul 12 15:22:47 2024 ] 
Training: Epoch [52/120], Step [3499], Loss: 0.6508592963218689, Training Accuracy: 89.21071428571429
[ Fri Jul 12 15:22:48 2024 ] 	Batch(3500/6809) done. Loss: 0.8051  lr:0.010000
[ Fri Jul 12 15:23:06 2024 ] 	Batch(3600/6809) done. Loss: 0.2335  lr:0.010000
[ Fri Jul 12 15:23:24 2024 ] 	Batch(3700/6809) done. Loss: 0.4335  lr:0.010000
[ Fri Jul 12 15:23:42 2024 ] 	Batch(3800/6809) done. Loss: 0.1990  lr:0.010000
[ Fri Jul 12 15:24:00 2024 ] 	Batch(3900/6809) done. Loss: 0.2085  lr:0.010000
[ Fri Jul 12 15:24:17 2024 ] 
Training: Epoch [52/120], Step [3999], Loss: 0.027116477489471436, Training Accuracy: 89.16875
[ Fri Jul 12 15:24:18 2024 ] 	Batch(4000/6809) done. Loss: 0.0978  lr:0.010000
[ Fri Jul 12 15:24:36 2024 ] 	Batch(4100/6809) done. Loss: 0.5331  lr:0.010000
[ Fri Jul 12 15:24:55 2024 ] 	Batch(4200/6809) done. Loss: 0.1770  lr:0.010000
[ Fri Jul 12 15:25:13 2024 ] 	Batch(4300/6809) done. Loss: 0.1839  lr:0.010000
[ Fri Jul 12 15:25:31 2024 ] 	Batch(4400/6809) done. Loss: 0.4061  lr:0.010000
[ Fri Jul 12 15:25:49 2024 ] 
Training: Epoch [52/120], Step [4499], Loss: 0.15077190101146698, Training Accuracy: 89.17777777777778
[ Fri Jul 12 15:25:49 2024 ] 	Batch(4500/6809) done. Loss: 0.3801  lr:0.010000
[ Fri Jul 12 15:26:07 2024 ] 	Batch(4600/6809) done. Loss: 0.1303  lr:0.010000
[ Fri Jul 12 15:26:25 2024 ] 	Batch(4700/6809) done. Loss: 0.5824  lr:0.010000
[ Fri Jul 12 15:26:43 2024 ] 	Batch(4800/6809) done. Loss: 0.1373  lr:0.010000
[ Fri Jul 12 15:27:01 2024 ] 	Batch(4900/6809) done. Loss: 0.9124  lr:0.010000
[ Fri Jul 12 15:27:18 2024 ] 
Training: Epoch [52/120], Step [4999], Loss: 0.01638876087963581, Training Accuracy: 89.02
[ Fri Jul 12 15:27:19 2024 ] 	Batch(5000/6809) done. Loss: 0.5460  lr:0.010000
[ Fri Jul 12 15:27:36 2024 ] 	Batch(5100/6809) done. Loss: 0.2472  lr:0.010000
[ Fri Jul 12 15:27:54 2024 ] 	Batch(5200/6809) done. Loss: 0.2500  lr:0.010000
[ Fri Jul 12 15:28:12 2024 ] 	Batch(5300/6809) done. Loss: 0.1798  lr:0.010000
[ Fri Jul 12 15:28:30 2024 ] 	Batch(5400/6809) done. Loss: 0.0355  lr:0.010000
[ Fri Jul 12 15:28:48 2024 ] 
Training: Epoch [52/120], Step [5499], Loss: 0.022401247173547745, Training Accuracy: 88.92954545454546
[ Fri Jul 12 15:28:48 2024 ] 	Batch(5500/6809) done. Loss: 0.3711  lr:0.010000
[ Fri Jul 12 15:29:06 2024 ] 	Batch(5600/6809) done. Loss: 0.1724  lr:0.010000
[ Fri Jul 12 15:29:24 2024 ] 	Batch(5700/6809) done. Loss: 0.0266  lr:0.010000
[ Fri Jul 12 15:29:42 2024 ] 	Batch(5800/6809) done. Loss: 0.1111  lr:0.010000
[ Fri Jul 12 15:30:00 2024 ] 	Batch(5900/6809) done. Loss: 0.7526  lr:0.010000
[ Fri Jul 12 15:30:17 2024 ] 
Training: Epoch [52/120], Step [5999], Loss: 0.3673142194747925, Training Accuracy: 88.93958333333333
[ Fri Jul 12 15:30:18 2024 ] 	Batch(6000/6809) done. Loss: 0.1364  lr:0.010000
[ Fri Jul 12 15:30:36 2024 ] 	Batch(6100/6809) done. Loss: 0.2760  lr:0.010000
[ Fri Jul 12 15:30:54 2024 ] 	Batch(6200/6809) done. Loss: 0.8385  lr:0.010000
[ Fri Jul 12 15:31:11 2024 ] 	Batch(6300/6809) done. Loss: 0.1144  lr:0.010000
[ Fri Jul 12 15:31:30 2024 ] 	Batch(6400/6809) done. Loss: 0.0750  lr:0.010000
[ Fri Jul 12 15:31:47 2024 ] 
Training: Epoch [52/120], Step [6499], Loss: 0.2686086893081665, Training Accuracy: 88.83269230769231
[ Fri Jul 12 15:31:47 2024 ] 	Batch(6500/6809) done. Loss: 0.8238  lr:0.010000
[ Fri Jul 12 15:32:05 2024 ] 	Batch(6600/6809) done. Loss: 1.5972  lr:0.010000
[ Fri Jul 12 15:32:23 2024 ] 	Batch(6700/6809) done. Loss: 0.1888  lr:0.010000
[ Fri Jul 12 15:32:41 2024 ] 	Batch(6800/6809) done. Loss: 1.2835  lr:0.010000
[ Fri Jul 12 15:32:43 2024 ] 	Mean training loss: 0.3472.
[ Fri Jul 12 15:32:43 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 15:32:43 2024 ] Training epoch: 54
[ Fri Jul 12 15:32:43 2024 ] 	Batch(0/6809) done. Loss: 0.5295  lr:0.010000
[ Fri Jul 12 15:33:01 2024 ] 	Batch(100/6809) done. Loss: 0.0388  lr:0.010000
[ Fri Jul 12 15:33:19 2024 ] 	Batch(200/6809) done. Loss: 0.6169  lr:0.010000
[ Fri Jul 12 15:33:37 2024 ] 	Batch(300/6809) done. Loss: 0.1506  lr:0.010000
[ Fri Jul 12 15:33:55 2024 ] 	Batch(400/6809) done. Loss: 0.1662  lr:0.010000
[ Fri Jul 12 15:34:13 2024 ] 
Training: Epoch [53/120], Step [499], Loss: 0.0160322654992342, Training Accuracy: 90.675
[ Fri Jul 12 15:34:13 2024 ] 	Batch(500/6809) done. Loss: 0.3450  lr:0.010000
[ Fri Jul 12 15:34:31 2024 ] 	Batch(600/6809) done. Loss: 0.1611  lr:0.010000
[ Fri Jul 12 15:34:49 2024 ] 	Batch(700/6809) done. Loss: 0.2161  lr:0.010000
[ Fri Jul 12 15:35:07 2024 ] 	Batch(800/6809) done. Loss: 0.2040  lr:0.010000
[ Fri Jul 12 15:35:25 2024 ] 	Batch(900/6809) done. Loss: 0.1455  lr:0.010000
[ Fri Jul 12 15:35:43 2024 ] 
Training: Epoch [53/120], Step [999], Loss: 0.2150009274482727, Training Accuracy: 90.725
[ Fri Jul 12 15:35:43 2024 ] 	Batch(1000/6809) done. Loss: 0.4082  lr:0.010000
[ Fri Jul 12 15:36:01 2024 ] 	Batch(1100/6809) done. Loss: 0.1827  lr:0.010000
[ Fri Jul 12 15:36:19 2024 ] 	Batch(1200/6809) done. Loss: 0.2062  lr:0.010000
[ Fri Jul 12 15:36:37 2024 ] 	Batch(1300/6809) done. Loss: 1.0299  lr:0.010000
[ Fri Jul 12 15:36:55 2024 ] 	Batch(1400/6809) done. Loss: 0.5033  lr:0.010000
[ Fri Jul 12 15:37:12 2024 ] 
Training: Epoch [53/120], Step [1499], Loss: 0.03610143065452576, Training Accuracy: 90.60833333333333
[ Fri Jul 12 15:37:12 2024 ] 	Batch(1500/6809) done. Loss: 0.5046  lr:0.010000
[ Fri Jul 12 15:37:30 2024 ] 	Batch(1600/6809) done. Loss: 0.6361  lr:0.010000
[ Fri Jul 12 15:37:48 2024 ] 	Batch(1700/6809) done. Loss: 0.0913  lr:0.010000
[ Fri Jul 12 15:38:06 2024 ] 	Batch(1800/6809) done. Loss: 0.2363  lr:0.010000
[ Fri Jul 12 15:38:24 2024 ] 	Batch(1900/6809) done. Loss: 0.0998  lr:0.010000
[ Fri Jul 12 15:38:42 2024 ] 
Training: Epoch [53/120], Step [1999], Loss: 0.6397414207458496, Training Accuracy: 90.275
[ Fri Jul 12 15:38:42 2024 ] 	Batch(2000/6809) done. Loss: 0.1878  lr:0.010000
[ Fri Jul 12 15:39:00 2024 ] 	Batch(2100/6809) done. Loss: 0.1905  lr:0.010000
[ Fri Jul 12 15:39:18 2024 ] 	Batch(2200/6809) done. Loss: 0.9932  lr:0.010000
[ Fri Jul 12 15:39:36 2024 ] 	Batch(2300/6809) done. Loss: 0.2192  lr:0.010000
[ Fri Jul 12 15:39:54 2024 ] 	Batch(2400/6809) done. Loss: 0.1852  lr:0.010000
[ Fri Jul 12 15:40:12 2024 ] 
Training: Epoch [53/120], Step [2499], Loss: 0.09798890352249146, Training Accuracy: 90.16499999999999
[ Fri Jul 12 15:40:12 2024 ] 	Batch(2500/6809) done. Loss: 0.1294  lr:0.010000
[ Fri Jul 12 15:40:30 2024 ] 	Batch(2600/6809) done. Loss: 0.0047  lr:0.010000
[ Fri Jul 12 15:40:48 2024 ] 	Batch(2700/6809) done. Loss: 0.8843  lr:0.010000
[ Fri Jul 12 15:41:07 2024 ] 	Batch(2800/6809) done. Loss: 0.3927  lr:0.010000
[ Fri Jul 12 15:41:24 2024 ] 	Batch(2900/6809) done. Loss: 0.1624  lr:0.010000
[ Fri Jul 12 15:41:42 2024 ] 
Training: Epoch [53/120], Step [2999], Loss: 0.49559125304222107, Training Accuracy: 90.07083333333334
[ Fri Jul 12 15:41:42 2024 ] 	Batch(3000/6809) done. Loss: 0.5126  lr:0.010000
[ Fri Jul 12 15:42:00 2024 ] 	Batch(3100/6809) done. Loss: 0.7387  lr:0.010000
[ Fri Jul 12 15:42:18 2024 ] 	Batch(3200/6809) done. Loss: 0.8131  lr:0.010000
[ Fri Jul 12 15:42:36 2024 ] 	Batch(3300/6809) done. Loss: 0.0260  lr:0.010000
[ Fri Jul 12 15:42:54 2024 ] 	Batch(3400/6809) done. Loss: 0.4125  lr:0.010000
[ Fri Jul 12 15:43:12 2024 ] 
Training: Epoch [53/120], Step [3499], Loss: 0.4538903832435608, Training Accuracy: 89.95357142857144
[ Fri Jul 12 15:43:12 2024 ] 	Batch(3500/6809) done. Loss: 0.4070  lr:0.010000
[ Fri Jul 12 15:43:30 2024 ] 	Batch(3600/6809) done. Loss: 0.4658  lr:0.010000
[ Fri Jul 12 15:43:48 2024 ] 	Batch(3700/6809) done. Loss: 0.8585  lr:0.010000
[ Fri Jul 12 15:44:06 2024 ] 	Batch(3800/6809) done. Loss: 0.4408  lr:0.010000
[ Fri Jul 12 15:44:24 2024 ] 	Batch(3900/6809) done. Loss: 0.7141  lr:0.010000
[ Fri Jul 12 15:44:41 2024 ] 
Training: Epoch [53/120], Step [3999], Loss: 0.8548609614372253, Training Accuracy: 89.84375
[ Fri Jul 12 15:44:42 2024 ] 	Batch(4000/6809) done. Loss: 0.5508  lr:0.010000
[ Fri Jul 12 15:44:59 2024 ] 	Batch(4100/6809) done. Loss: 0.0768  lr:0.010000
[ Fri Jul 12 15:45:17 2024 ] 	Batch(4200/6809) done. Loss: 0.5456  lr:0.010000
[ Fri Jul 12 15:45:35 2024 ] 	Batch(4300/6809) done. Loss: 0.0232  lr:0.010000
[ Fri Jul 12 15:45:53 2024 ] 	Batch(4400/6809) done. Loss: 0.1743  lr:0.010000
[ Fri Jul 12 15:46:11 2024 ] 
Training: Epoch [53/120], Step [4499], Loss: 0.7766339778900146, Training Accuracy: 89.71111111111111
[ Fri Jul 12 15:46:11 2024 ] 	Batch(4500/6809) done. Loss: 0.0703  lr:0.010000
[ Fri Jul 12 15:46:29 2024 ] 	Batch(4600/6809) done. Loss: 0.2596  lr:0.010000
[ Fri Jul 12 15:46:47 2024 ] 	Batch(4700/6809) done. Loss: 0.5400  lr:0.010000
[ Fri Jul 12 15:47:05 2024 ] 	Batch(4800/6809) done. Loss: 0.0222  lr:0.010000
[ Fri Jul 12 15:47:23 2024 ] 	Batch(4900/6809) done. Loss: 0.0042  lr:0.010000
[ Fri Jul 12 15:47:41 2024 ] 
Training: Epoch [53/120], Step [4999], Loss: 0.20169344544410706, Training Accuracy: 89.60249999999999
[ Fri Jul 12 15:47:41 2024 ] 	Batch(5000/6809) done. Loss: 0.0095  lr:0.010000
[ Fri Jul 12 15:47:59 2024 ] 	Batch(5100/6809) done. Loss: 0.5213  lr:0.010000
[ Fri Jul 12 15:48:17 2024 ] 	Batch(5200/6809) done. Loss: 0.5540  lr:0.010000
[ Fri Jul 12 15:48:35 2024 ] 	Batch(5300/6809) done. Loss: 0.1270  lr:0.010000
[ Fri Jul 12 15:48:53 2024 ] 	Batch(5400/6809) done. Loss: 0.0667  lr:0.010000
[ Fri Jul 12 15:49:11 2024 ] 
Training: Epoch [53/120], Step [5499], Loss: 0.5343677401542664, Training Accuracy: 89.42272727272727
[ Fri Jul 12 15:49:11 2024 ] 	Batch(5500/6809) done. Loss: 0.3139  lr:0.010000
[ Fri Jul 12 15:49:29 2024 ] 	Batch(5600/6809) done. Loss: 0.4717  lr:0.010000
[ Fri Jul 12 15:49:47 2024 ] 	Batch(5700/6809) done. Loss: 0.0416  lr:0.010000
[ Fri Jul 12 15:50:05 2024 ] 	Batch(5800/6809) done. Loss: 0.5721  lr:0.010000
[ Fri Jul 12 15:50:23 2024 ] 	Batch(5900/6809) done. Loss: 0.3680  lr:0.010000
[ Fri Jul 12 15:50:41 2024 ] 
Training: Epoch [53/120], Step [5999], Loss: 0.6809319257736206, Training Accuracy: 89.40416666666667
[ Fri Jul 12 15:50:41 2024 ] 	Batch(6000/6809) done. Loss: 0.1084  lr:0.010000
[ Fri Jul 12 15:51:00 2024 ] 	Batch(6100/6809) done. Loss: 0.2333  lr:0.010000
[ Fri Jul 12 15:51:18 2024 ] 	Batch(6200/6809) done. Loss: 0.5594  lr:0.010000
[ Fri Jul 12 15:51:37 2024 ] 	Batch(6300/6809) done. Loss: 0.3000  lr:0.010000
[ Fri Jul 12 15:51:55 2024 ] 	Batch(6400/6809) done. Loss: 0.4353  lr:0.010000
[ Fri Jul 12 15:52:14 2024 ] 
Training: Epoch [53/120], Step [6499], Loss: 0.2992086410522461, Training Accuracy: 89.28653846153846
[ Fri Jul 12 15:52:14 2024 ] 	Batch(6500/6809) done. Loss: 0.3346  lr:0.010000
[ Fri Jul 12 15:52:32 2024 ] 	Batch(6600/6809) done. Loss: 0.2135  lr:0.010000
[ Fri Jul 12 15:52:50 2024 ] 	Batch(6700/6809) done. Loss: 0.0538  lr:0.010000
[ Fri Jul 12 15:53:08 2024 ] 	Batch(6800/6809) done. Loss: 1.8493  lr:0.010000
[ Fri Jul 12 15:53:10 2024 ] 	Mean training loss: 0.3314.
[ Fri Jul 12 15:53:10 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 15:53:10 2024 ] Training epoch: 55
[ Fri Jul 12 15:53:10 2024 ] 	Batch(0/6809) done. Loss: 0.6538  lr:0.010000
[ Fri Jul 12 15:53:28 2024 ] 	Batch(100/6809) done. Loss: 0.7973  lr:0.010000
[ Fri Jul 12 15:53:46 2024 ] 	Batch(200/6809) done. Loss: 0.0945  lr:0.010000
[ Fri Jul 12 15:54:04 2024 ] 	Batch(300/6809) done. Loss: 0.8883  lr:0.010000
[ Fri Jul 12 15:54:22 2024 ] 	Batch(400/6809) done. Loss: 0.1477  lr:0.010000
[ Fri Jul 12 15:54:40 2024 ] 
Training: Epoch [54/120], Step [499], Loss: 0.42990443110466003, Training Accuracy: 90.525
[ Fri Jul 12 15:54:40 2024 ] 	Batch(500/6809) done. Loss: 0.5538  lr:0.010000
[ Fri Jul 12 15:54:58 2024 ] 	Batch(600/6809) done. Loss: 0.8014  lr:0.010000
[ Fri Jul 12 15:55:15 2024 ] 	Batch(700/6809) done. Loss: 0.0563  lr:0.010000
[ Fri Jul 12 15:55:33 2024 ] 	Batch(800/6809) done. Loss: 0.2273  lr:0.010000
[ Fri Jul 12 15:55:51 2024 ] 	Batch(900/6809) done. Loss: 1.1266  lr:0.010000
[ Fri Jul 12 15:56:09 2024 ] 
Training: Epoch [54/120], Step [999], Loss: 0.09156979620456696, Training Accuracy: 90.4375
[ Fri Jul 12 15:56:09 2024 ] 	Batch(1000/6809) done. Loss: 0.2124  lr:0.010000
[ Fri Jul 12 15:56:27 2024 ] 	Batch(1100/6809) done. Loss: 0.6673  lr:0.010000
[ Fri Jul 12 15:56:45 2024 ] 	Batch(1200/6809) done. Loss: 0.3258  lr:0.010000
[ Fri Jul 12 15:57:03 2024 ] 	Batch(1300/6809) done. Loss: 0.0044  lr:0.010000
[ Fri Jul 12 15:57:21 2024 ] 	Batch(1400/6809) done. Loss: 0.4592  lr:0.010000
[ Fri Jul 12 15:57:39 2024 ] 
Training: Epoch [54/120], Step [1499], Loss: 0.4859839975833893, Training Accuracy: 90.325
[ Fri Jul 12 15:57:39 2024 ] 	Batch(1500/6809) done. Loss: 0.3582  lr:0.010000
[ Fri Jul 12 15:57:57 2024 ] 	Batch(1600/6809) done. Loss: 0.7320  lr:0.010000
[ Fri Jul 12 15:58:15 2024 ] 	Batch(1700/6809) done. Loss: 0.0129  lr:0.010000
[ Fri Jul 12 15:58:33 2024 ] 	Batch(1800/6809) done. Loss: 0.0954  lr:0.010000
[ Fri Jul 12 15:58:51 2024 ] 	Batch(1900/6809) done. Loss: 0.9734  lr:0.010000
[ Fri Jul 12 15:59:08 2024 ] 
Training: Epoch [54/120], Step [1999], Loss: 0.4998184144496918, Training Accuracy: 90.35625
[ Fri Jul 12 15:59:09 2024 ] 	Batch(2000/6809) done. Loss: 0.0926  lr:0.010000
[ Fri Jul 12 15:59:26 2024 ] 	Batch(2100/6809) done. Loss: 0.5113  lr:0.010000
[ Fri Jul 12 15:59:44 2024 ] 	Batch(2200/6809) done. Loss: 0.4978  lr:0.010000
[ Fri Jul 12 16:00:02 2024 ] 	Batch(2300/6809) done. Loss: 0.2664  lr:0.010000
[ Fri Jul 12 16:00:20 2024 ] 	Batch(2400/6809) done. Loss: 0.2562  lr:0.010000
[ Fri Jul 12 16:00:38 2024 ] 
Training: Epoch [54/120], Step [2499], Loss: 0.4782482981681824, Training Accuracy: 90.03999999999999
[ Fri Jul 12 16:00:38 2024 ] 	Batch(2500/6809) done. Loss: 0.0298  lr:0.010000
[ Fri Jul 12 16:00:56 2024 ] 	Batch(2600/6809) done. Loss: 0.0931  lr:0.010000
[ Fri Jul 12 16:01:14 2024 ] 	Batch(2700/6809) done. Loss: 0.3679  lr:0.010000
[ Fri Jul 12 16:01:32 2024 ] 	Batch(2800/6809) done. Loss: 0.1491  lr:0.010000
[ Fri Jul 12 16:01:50 2024 ] 	Batch(2900/6809) done. Loss: 0.1163  lr:0.010000
[ Fri Jul 12 16:02:07 2024 ] 
Training: Epoch [54/120], Step [2999], Loss: 0.17257028818130493, Training Accuracy: 89.9375
[ Fri Jul 12 16:02:08 2024 ] 	Batch(3000/6809) done. Loss: 0.2778  lr:0.010000
[ Fri Jul 12 16:02:26 2024 ] 	Batch(3100/6809) done. Loss: 0.2565  lr:0.010000
[ Fri Jul 12 16:02:44 2024 ] 	Batch(3200/6809) done. Loss: 0.0794  lr:0.010000
[ Fri Jul 12 16:03:01 2024 ] 	Batch(3300/6809) done. Loss: 0.5178  lr:0.010000
[ Fri Jul 12 16:03:19 2024 ] 	Batch(3400/6809) done. Loss: 0.1395  lr:0.010000
[ Fri Jul 12 16:03:37 2024 ] 
Training: Epoch [54/120], Step [3499], Loss: 0.19854740798473358, Training Accuracy: 89.85357142857143
[ Fri Jul 12 16:03:37 2024 ] 	Batch(3500/6809) done. Loss: 0.9539  lr:0.010000
[ Fri Jul 12 16:03:55 2024 ] 	Batch(3600/6809) done. Loss: 0.3008  lr:0.010000
[ Fri Jul 12 16:04:13 2024 ] 	Batch(3700/6809) done. Loss: 0.2992  lr:0.010000
[ Fri Jul 12 16:04:31 2024 ] 	Batch(3800/6809) done. Loss: 0.0152  lr:0.010000
[ Fri Jul 12 16:04:49 2024 ] 	Batch(3900/6809) done. Loss: 0.5322  lr:0.010000
[ Fri Jul 12 16:05:06 2024 ] 
Training: Epoch [54/120], Step [3999], Loss: 1.6607413291931152, Training Accuracy: 89.775
[ Fri Jul 12 16:05:07 2024 ] 	Batch(4000/6809) done. Loss: 0.4699  lr:0.010000
[ Fri Jul 12 16:05:25 2024 ] 	Batch(4100/6809) done. Loss: 0.5017  lr:0.010000
[ Fri Jul 12 16:05:44 2024 ] 	Batch(4200/6809) done. Loss: 0.8518  lr:0.010000
[ Fri Jul 12 16:06:02 2024 ] 	Batch(4300/6809) done. Loss: 0.2617  lr:0.010000
[ Fri Jul 12 16:06:21 2024 ] 	Batch(4400/6809) done. Loss: 0.6016  lr:0.010000
[ Fri Jul 12 16:06:40 2024 ] 
Training: Epoch [54/120], Step [4499], Loss: 0.6342732906341553, Training Accuracy: 89.725
[ Fri Jul 12 16:06:40 2024 ] 	Batch(4500/6809) done. Loss: 0.6244  lr:0.010000
[ Fri Jul 12 16:06:58 2024 ] 	Batch(4600/6809) done. Loss: 0.0279  lr:0.010000
[ Fri Jul 12 16:07:17 2024 ] 	Batch(4700/6809) done. Loss: 0.3541  lr:0.010000
[ Fri Jul 12 16:07:35 2024 ] 	Batch(4800/6809) done. Loss: 0.0744  lr:0.010000
[ Fri Jul 12 16:07:53 2024 ] 	Batch(4900/6809) done. Loss: 0.8658  lr:0.010000
[ Fri Jul 12 16:08:11 2024 ] 
Training: Epoch [54/120], Step [4999], Loss: 0.1699216663837433, Training Accuracy: 89.6675
[ Fri Jul 12 16:08:11 2024 ] 	Batch(5000/6809) done. Loss: 0.1204  lr:0.010000
[ Fri Jul 12 16:08:29 2024 ] 	Batch(5100/6809) done. Loss: 0.0295  lr:0.010000
[ Fri Jul 12 16:08:47 2024 ] 	Batch(5200/6809) done. Loss: 0.2197  lr:0.010000
[ Fri Jul 12 16:09:05 2024 ] 	Batch(5300/6809) done. Loss: 0.3882  lr:0.010000
[ Fri Jul 12 16:09:23 2024 ] 	Batch(5400/6809) done. Loss: 0.1056  lr:0.010000
[ Fri Jul 12 16:09:41 2024 ] 
Training: Epoch [54/120], Step [5499], Loss: 0.22204652428627014, Training Accuracy: 89.49090909090908
[ Fri Jul 12 16:09:41 2024 ] 	Batch(5500/6809) done. Loss: 0.3422  lr:0.010000
[ Fri Jul 12 16:09:59 2024 ] 	Batch(5600/6809) done. Loss: 0.4693  lr:0.010000
[ Fri Jul 12 16:10:17 2024 ] 	Batch(5700/6809) done. Loss: 0.0761  lr:0.010000
[ Fri Jul 12 16:10:35 2024 ] 	Batch(5800/6809) done. Loss: 0.7151  lr:0.010000
[ Fri Jul 12 16:10:52 2024 ] 	Batch(5900/6809) done. Loss: 0.6174  lr:0.010000
[ Fri Jul 12 16:11:10 2024 ] 
Training: Epoch [54/120], Step [5999], Loss: 0.057959429919719696, Training Accuracy: 89.38333333333334
[ Fri Jul 12 16:11:10 2024 ] 	Batch(6000/6809) done. Loss: 0.1174  lr:0.010000
[ Fri Jul 12 16:11:29 2024 ] 	Batch(6100/6809) done. Loss: 0.9705  lr:0.010000
[ Fri Jul 12 16:11:48 2024 ] 	Batch(6200/6809) done. Loss: 0.7203  lr:0.010000
[ Fri Jul 12 16:12:06 2024 ] 	Batch(6300/6809) done. Loss: 0.1407  lr:0.010000
[ Fri Jul 12 16:12:25 2024 ] 	Batch(6400/6809) done. Loss: 0.2001  lr:0.010000
[ Fri Jul 12 16:12:43 2024 ] 
Training: Epoch [54/120], Step [6499], Loss: 0.298600971698761, Training Accuracy: 89.30576923076923
[ Fri Jul 12 16:12:43 2024 ] 	Batch(6500/6809) done. Loss: 0.2419  lr:0.010000
[ Fri Jul 12 16:13:01 2024 ] 	Batch(6600/6809) done. Loss: 0.2325  lr:0.010000
[ Fri Jul 12 16:13:19 2024 ] 	Batch(6700/6809) done. Loss: 0.0913  lr:0.010000
[ Fri Jul 12 16:13:37 2024 ] 	Batch(6800/6809) done. Loss: 0.3897  lr:0.010000
[ Fri Jul 12 16:13:38 2024 ] 	Mean training loss: 0.3366.
[ Fri Jul 12 16:13:38 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 16:13:38 2024 ] Training epoch: 56
[ Fri Jul 12 16:13:39 2024 ] 	Batch(0/6809) done. Loss: 0.5218  lr:0.010000
[ Fri Jul 12 16:13:57 2024 ] 	Batch(100/6809) done. Loss: 0.1418  lr:0.010000
[ Fri Jul 12 16:14:15 2024 ] 	Batch(200/6809) done. Loss: 0.1588  lr:0.010000
[ Fri Jul 12 16:14:32 2024 ] 	Batch(300/6809) done. Loss: 0.0114  lr:0.010000
[ Fri Jul 12 16:14:51 2024 ] 	Batch(400/6809) done. Loss: 0.4990  lr:0.010000
[ Fri Jul 12 16:15:08 2024 ] 
Training: Epoch [55/120], Step [499], Loss: 0.18701881170272827, Training Accuracy: 90.75
[ Fri Jul 12 16:15:08 2024 ] 	Batch(500/6809) done. Loss: 0.3300  lr:0.010000
[ Fri Jul 12 16:15:26 2024 ] 	Batch(600/6809) done. Loss: 0.0893  lr:0.010000
[ Fri Jul 12 16:15:44 2024 ] 	Batch(700/6809) done. Loss: 0.5482  lr:0.010000
[ Fri Jul 12 16:16:02 2024 ] 	Batch(800/6809) done. Loss: 0.5471  lr:0.010000
[ Fri Jul 12 16:16:21 2024 ] 	Batch(900/6809) done. Loss: 0.0388  lr:0.010000
[ Fri Jul 12 16:16:39 2024 ] 
Training: Epoch [55/120], Step [999], Loss: 0.44002020359039307, Training Accuracy: 90.625
[ Fri Jul 12 16:16:39 2024 ] 	Batch(1000/6809) done. Loss: 0.1003  lr:0.010000
[ Fri Jul 12 16:16:58 2024 ] 	Batch(1100/6809) done. Loss: 0.3163  lr:0.010000
[ Fri Jul 12 16:17:16 2024 ] 	Batch(1200/6809) done. Loss: 0.0673  lr:0.010000
[ Fri Jul 12 16:17:34 2024 ] 	Batch(1300/6809) done. Loss: 0.0449  lr:0.010000
[ Fri Jul 12 16:17:53 2024 ] 	Batch(1400/6809) done. Loss: 0.0313  lr:0.010000
[ Fri Jul 12 16:18:11 2024 ] 
Training: Epoch [55/120], Step [1499], Loss: 0.12891225516796112, Training Accuracy: 90.64166666666667
[ Fri Jul 12 16:18:11 2024 ] 	Batch(1500/6809) done. Loss: 0.1675  lr:0.010000
[ Fri Jul 12 16:18:30 2024 ] 	Batch(1600/6809) done. Loss: 0.3706  lr:0.010000
[ Fri Jul 12 16:18:48 2024 ] 	Batch(1700/6809) done. Loss: 0.4401  lr:0.010000
[ Fri Jul 12 16:19:06 2024 ] 	Batch(1800/6809) done. Loss: 0.4310  lr:0.010000
[ Fri Jul 12 16:19:24 2024 ] 	Batch(1900/6809) done. Loss: 0.2169  lr:0.010000
[ Fri Jul 12 16:19:41 2024 ] 
Training: Epoch [55/120], Step [1999], Loss: 0.20346787571907043, Training Accuracy: 90.35
[ Fri Jul 12 16:19:42 2024 ] 	Batch(2000/6809) done. Loss: 0.8984  lr:0.010000
[ Fri Jul 12 16:19:59 2024 ] 	Batch(2100/6809) done. Loss: 0.7414  lr:0.010000
[ Fri Jul 12 16:20:17 2024 ] 	Batch(2200/6809) done. Loss: 0.0112  lr:0.010000
[ Fri Jul 12 16:20:35 2024 ] 	Batch(2300/6809) done. Loss: 0.3043  lr:0.010000
[ Fri Jul 12 16:20:53 2024 ] 	Batch(2400/6809) done. Loss: 0.1113  lr:0.010000
[ Fri Jul 12 16:21:11 2024 ] 
Training: Epoch [55/120], Step [2499], Loss: 1.0941365957260132, Training Accuracy: 90.225
[ Fri Jul 12 16:21:11 2024 ] 	Batch(2500/6809) done. Loss: 0.0385  lr:0.010000
[ Fri Jul 12 16:21:29 2024 ] 	Batch(2600/6809) done. Loss: 0.2855  lr:0.010000
[ Fri Jul 12 16:21:47 2024 ] 	Batch(2700/6809) done. Loss: 0.7254  lr:0.010000
[ Fri Jul 12 16:22:05 2024 ] 	Batch(2800/6809) done. Loss: 0.1867  lr:0.010000
[ Fri Jul 12 16:22:23 2024 ] 	Batch(2900/6809) done. Loss: 0.3072  lr:0.010000
[ Fri Jul 12 16:22:41 2024 ] 
Training: Epoch [55/120], Step [2999], Loss: 0.2048446089029312, Training Accuracy: 90.01666666666667
[ Fri Jul 12 16:22:41 2024 ] 	Batch(3000/6809) done. Loss: 0.2865  lr:0.010000
[ Fri Jul 12 16:22:59 2024 ] 	Batch(3100/6809) done. Loss: 0.0353  lr:0.010000
[ Fri Jul 12 16:23:17 2024 ] 	Batch(3200/6809) done. Loss: 0.2043  lr:0.010000
[ Fri Jul 12 16:23:35 2024 ] 	Batch(3300/6809) done. Loss: 0.0105  lr:0.010000
[ Fri Jul 12 16:23:52 2024 ] 	Batch(3400/6809) done. Loss: 0.4323  lr:0.010000
[ Fri Jul 12 16:24:10 2024 ] 
Training: Epoch [55/120], Step [3499], Loss: 0.28176209330558777, Training Accuracy: 89.92142857142858
[ Fri Jul 12 16:24:10 2024 ] 	Batch(3500/6809) done. Loss: 0.2383  lr:0.010000
[ Fri Jul 12 16:24:28 2024 ] 	Batch(3600/6809) done. Loss: 0.1821  lr:0.010000
[ Fri Jul 12 16:24:47 2024 ] 	Batch(3700/6809) done. Loss: 0.2755  lr:0.010000
[ Fri Jul 12 16:25:05 2024 ] 	Batch(3800/6809) done. Loss: 0.2445  lr:0.010000
[ Fri Jul 12 16:25:23 2024 ] 	Batch(3900/6809) done. Loss: 0.1963  lr:0.010000
[ Fri Jul 12 16:25:41 2024 ] 
Training: Epoch [55/120], Step [3999], Loss: 0.02367962896823883, Training Accuracy: 89.896875
[ Fri Jul 12 16:25:41 2024 ] 	Batch(4000/6809) done. Loss: 0.0525  lr:0.010000
[ Fri Jul 12 16:25:59 2024 ] 	Batch(4100/6809) done. Loss: 0.1048  lr:0.010000
[ Fri Jul 12 16:26:17 2024 ] 	Batch(4200/6809) done. Loss: 0.3556  lr:0.010000
[ Fri Jul 12 16:26:35 2024 ] 	Batch(4300/6809) done. Loss: 0.4002  lr:0.010000
[ Fri Jul 12 16:26:53 2024 ] 	Batch(4400/6809) done. Loss: 0.1424  lr:0.010000
[ Fri Jul 12 16:27:11 2024 ] 
Training: Epoch [55/120], Step [4499], Loss: 0.10221236199140549, Training Accuracy: 89.90277777777777
[ Fri Jul 12 16:27:11 2024 ] 	Batch(4500/6809) done. Loss: 0.6087  lr:0.010000
[ Fri Jul 12 16:27:29 2024 ] 	Batch(4600/6809) done. Loss: 0.6955  lr:0.010000
[ Fri Jul 12 16:27:47 2024 ] 	Batch(4700/6809) done. Loss: 0.1702  lr:0.010000
[ Fri Jul 12 16:28:05 2024 ] 	Batch(4800/6809) done. Loss: 0.4830  lr:0.010000
[ Fri Jul 12 16:28:23 2024 ] 	Batch(4900/6809) done. Loss: 0.3996  lr:0.010000
[ Fri Jul 12 16:28:42 2024 ] 
Training: Epoch [55/120], Step [4999], Loss: 0.3701240122318268, Training Accuracy: 89.77000000000001
[ Fri Jul 12 16:28:42 2024 ] 	Batch(5000/6809) done. Loss: 0.0858  lr:0.010000
[ Fri Jul 12 16:29:00 2024 ] 	Batch(5100/6809) done. Loss: 0.2884  lr:0.010000
[ Fri Jul 12 16:29:19 2024 ] 	Batch(5200/6809) done. Loss: 0.2037  lr:0.010000
[ Fri Jul 12 16:29:38 2024 ] 	Batch(5300/6809) done. Loss: 0.4078  lr:0.010000
[ Fri Jul 12 16:29:56 2024 ] 	Batch(5400/6809) done. Loss: 0.0608  lr:0.010000
[ Fri Jul 12 16:30:14 2024 ] 
Training: Epoch [55/120], Step [5499], Loss: 0.05725354701280594, Training Accuracy: 89.71818181818182
[ Fri Jul 12 16:30:15 2024 ] 	Batch(5500/6809) done. Loss: 0.2939  lr:0.010000
[ Fri Jul 12 16:30:33 2024 ] 	Batch(5600/6809) done. Loss: 0.0480  lr:0.010000
[ Fri Jul 12 16:30:51 2024 ] 	Batch(5700/6809) done. Loss: 0.0489  lr:0.010000
[ Fri Jul 12 16:31:09 2024 ] 	Batch(5800/6809) done. Loss: 0.0612  lr:0.010000
[ Fri Jul 12 16:31:27 2024 ] 	Batch(5900/6809) done. Loss: 0.5928  lr:0.010000
[ Fri Jul 12 16:31:45 2024 ] 
Training: Epoch [55/120], Step [5999], Loss: 0.13896575570106506, Training Accuracy: 89.57916666666667
[ Fri Jul 12 16:31:45 2024 ] 	Batch(6000/6809) done. Loss: 0.5042  lr:0.010000
[ Fri Jul 12 16:32:04 2024 ] 	Batch(6100/6809) done. Loss: 0.1272  lr:0.010000
[ Fri Jul 12 16:32:22 2024 ] 	Batch(6200/6809) done. Loss: 0.3098  lr:0.010000
[ Fri Jul 12 16:32:41 2024 ] 	Batch(6300/6809) done. Loss: 0.2846  lr:0.010000
[ Fri Jul 12 16:32:59 2024 ] 	Batch(6400/6809) done. Loss: 0.3395  lr:0.010000
[ Fri Jul 12 16:33:18 2024 ] 
Training: Epoch [55/120], Step [6499], Loss: 0.14341570436954498, Training Accuracy: 89.47115384615385
[ Fri Jul 12 16:33:18 2024 ] 	Batch(6500/6809) done. Loss: 0.0094  lr:0.010000
[ Fri Jul 12 16:33:37 2024 ] 	Batch(6600/6809) done. Loss: 0.3808  lr:0.010000
[ Fri Jul 12 16:33:55 2024 ] 	Batch(6700/6809) done. Loss: 0.5025  lr:0.010000
[ Fri Jul 12 16:34:14 2024 ] 	Batch(6800/6809) done. Loss: 0.2937  lr:0.010000
[ Fri Jul 12 16:34:15 2024 ] 	Mean training loss: 0.3279.
[ Fri Jul 12 16:34:15 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 16:34:15 2024 ] Training epoch: 57
[ Fri Jul 12 16:34:16 2024 ] 	Batch(0/6809) done. Loss: 0.5303  lr:0.010000
[ Fri Jul 12 16:34:34 2024 ] 	Batch(100/6809) done. Loss: 0.0884  lr:0.010000
[ Fri Jul 12 16:34:51 2024 ] 	Batch(200/6809) done. Loss: 0.3190  lr:0.010000
[ Fri Jul 12 16:35:09 2024 ] 	Batch(300/6809) done. Loss: 0.0245  lr:0.010000
[ Fri Jul 12 16:35:28 2024 ] 	Batch(400/6809) done. Loss: 0.1337  lr:0.010000
[ Fri Jul 12 16:35:45 2024 ] 
Training: Epoch [56/120], Step [499], Loss: 0.2081817388534546, Training Accuracy: 90.45
[ Fri Jul 12 16:35:45 2024 ] 	Batch(500/6809) done. Loss: 0.2424  lr:0.010000
[ Fri Jul 12 16:36:03 2024 ] 	Batch(600/6809) done. Loss: 0.2399  lr:0.010000
[ Fri Jul 12 16:36:21 2024 ] 	Batch(700/6809) done. Loss: 0.2255  lr:0.010000
[ Fri Jul 12 16:36:39 2024 ] 	Batch(800/6809) done. Loss: 0.2039  lr:0.010000
[ Fri Jul 12 16:36:57 2024 ] 	Batch(900/6809) done. Loss: 0.9730  lr:0.010000
[ Fri Jul 12 16:37:15 2024 ] 
Training: Epoch [56/120], Step [999], Loss: 0.8017877340316772, Training Accuracy: 90.0625
[ Fri Jul 12 16:37:15 2024 ] 	Batch(1000/6809) done. Loss: 0.2957  lr:0.010000
[ Fri Jul 12 16:37:33 2024 ] 	Batch(1100/6809) done. Loss: 0.4907  lr:0.010000
[ Fri Jul 12 16:37:51 2024 ] 	Batch(1200/6809) done. Loss: 0.6666  lr:0.010000
[ Fri Jul 12 16:38:09 2024 ] 	Batch(1300/6809) done. Loss: 0.0705  lr:0.010000
[ Fri Jul 12 16:38:28 2024 ] 	Batch(1400/6809) done. Loss: 0.1399  lr:0.010000
[ Fri Jul 12 16:38:46 2024 ] 
Training: Epoch [56/120], Step [1499], Loss: 0.12571145594120026, Training Accuracy: 90.00833333333334
[ Fri Jul 12 16:38:46 2024 ] 	Batch(1500/6809) done. Loss: 0.1992  lr:0.010000
[ Fri Jul 12 16:39:05 2024 ] 	Batch(1600/6809) done. Loss: 0.2247  lr:0.010000
[ Fri Jul 12 16:39:23 2024 ] 	Batch(1700/6809) done. Loss: 1.2986  lr:0.010000
[ Fri Jul 12 16:39:40 2024 ] 	Batch(1800/6809) done. Loss: 0.2140  lr:0.010000
[ Fri Jul 12 16:39:58 2024 ] 	Batch(1900/6809) done. Loss: 0.3846  lr:0.010000
[ Fri Jul 12 16:40:16 2024 ] 
Training: Epoch [56/120], Step [1999], Loss: 0.18852317333221436, Training Accuracy: 89.94375
[ Fri Jul 12 16:40:16 2024 ] 	Batch(2000/6809) done. Loss: 0.4313  lr:0.010000
[ Fri Jul 12 16:40:35 2024 ] 	Batch(2100/6809) done. Loss: 0.2154  lr:0.010000
[ Fri Jul 12 16:40:54 2024 ] 	Batch(2200/6809) done. Loss: 0.0341  lr:0.010000
[ Fri Jul 12 16:41:12 2024 ] 	Batch(2300/6809) done. Loss: 0.7926  lr:0.010000
[ Fri Jul 12 16:41:30 2024 ] 	Batch(2400/6809) done. Loss: 0.1208  lr:0.010000
[ Fri Jul 12 16:41:47 2024 ] 
Training: Epoch [56/120], Step [2499], Loss: 0.0028881910257041454, Training Accuracy: 89.77000000000001
[ Fri Jul 12 16:41:47 2024 ] 	Batch(2500/6809) done. Loss: 0.4777  lr:0.010000
[ Fri Jul 12 16:42:05 2024 ] 	Batch(2600/6809) done. Loss: 0.3320  lr:0.010000
[ Fri Jul 12 16:42:23 2024 ] 	Batch(2700/6809) done. Loss: 0.0419  lr:0.010000
[ Fri Jul 12 16:42:41 2024 ] 	Batch(2800/6809) done. Loss: 0.1356  lr:0.010000
[ Fri Jul 12 16:42:59 2024 ] 	Batch(2900/6809) done. Loss: 0.2158  lr:0.010000
[ Fri Jul 12 16:43:17 2024 ] 
Training: Epoch [56/120], Step [2999], Loss: 0.4197879433631897, Training Accuracy: 89.70416666666667
[ Fri Jul 12 16:43:17 2024 ] 	Batch(3000/6809) done. Loss: 0.3738  lr:0.010000
[ Fri Jul 12 16:43:35 2024 ] 	Batch(3100/6809) done. Loss: 1.6620  lr:0.010000
[ Fri Jul 12 16:43:53 2024 ] 	Batch(3200/6809) done. Loss: 0.0030  lr:0.010000
[ Fri Jul 12 16:44:11 2024 ] 	Batch(3300/6809) done. Loss: 0.1063  lr:0.010000
[ Fri Jul 12 16:44:29 2024 ] 	Batch(3400/6809) done. Loss: 0.2188  lr:0.010000
[ Fri Jul 12 16:44:47 2024 ] 
Training: Epoch [56/120], Step [3499], Loss: 0.2020011842250824, Training Accuracy: 89.86071428571428
[ Fri Jul 12 16:44:47 2024 ] 	Batch(3500/6809) done. Loss: 0.3173  lr:0.010000
[ Fri Jul 12 16:45:05 2024 ] 	Batch(3600/6809) done. Loss: 0.8602  lr:0.010000
[ Fri Jul 12 16:45:23 2024 ] 	Batch(3700/6809) done. Loss: 0.1236  lr:0.010000
[ Fri Jul 12 16:45:41 2024 ] 	Batch(3800/6809) done. Loss: 0.0869  lr:0.010000
[ Fri Jul 12 16:45:59 2024 ] 	Batch(3900/6809) done. Loss: 0.6797  lr:0.010000
[ Fri Jul 12 16:46:16 2024 ] 
Training: Epoch [56/120], Step [3999], Loss: 0.14789989590644836, Training Accuracy: 89.640625
[ Fri Jul 12 16:46:17 2024 ] 	Batch(4000/6809) done. Loss: 0.3607  lr:0.010000
[ Fri Jul 12 16:46:35 2024 ] 	Batch(4100/6809) done. Loss: 0.8322  lr:0.010000
[ Fri Jul 12 16:46:53 2024 ] 	Batch(4200/6809) done. Loss: 0.1689  lr:0.010000
[ Fri Jul 12 16:47:10 2024 ] 	Batch(4300/6809) done. Loss: 0.4018  lr:0.010000
[ Fri Jul 12 16:47:29 2024 ] 	Batch(4400/6809) done. Loss: 0.0570  lr:0.010000
[ Fri Jul 12 16:47:47 2024 ] 
Training: Epoch [56/120], Step [4499], Loss: 0.24017193913459778, Training Accuracy: 89.51944444444445
[ Fri Jul 12 16:47:47 2024 ] 	Batch(4500/6809) done. Loss: 0.2186  lr:0.010000
[ Fri Jul 12 16:48:05 2024 ] 	Batch(4600/6809) done. Loss: 0.1276  lr:0.010000
[ Fri Jul 12 16:48:23 2024 ] 	Batch(4700/6809) done. Loss: 0.2596  lr:0.010000
[ Fri Jul 12 16:48:41 2024 ] 	Batch(4800/6809) done. Loss: 0.2020  lr:0.010000
[ Fri Jul 12 16:48:58 2024 ] 	Batch(4900/6809) done. Loss: 0.5062  lr:0.010000
[ Fri Jul 12 16:49:16 2024 ] 
Training: Epoch [56/120], Step [4999], Loss: 0.15653634071350098, Training Accuracy: 89.4575
[ Fri Jul 12 16:49:16 2024 ] 	Batch(5000/6809) done. Loss: 0.9123  lr:0.010000
[ Fri Jul 12 16:49:34 2024 ] 	Batch(5100/6809) done. Loss: 0.1562  lr:0.010000
[ Fri Jul 12 16:49:52 2024 ] 	Batch(5200/6809) done. Loss: 0.3399  lr:0.010000
[ Fri Jul 12 16:50:10 2024 ] 	Batch(5300/6809) done. Loss: 0.3610  lr:0.010000
[ Fri Jul 12 16:50:28 2024 ] 	Batch(5400/6809) done. Loss: 0.4586  lr:0.010000
[ Fri Jul 12 16:50:46 2024 ] 
Training: Epoch [56/120], Step [5499], Loss: 0.09786687046289444, Training Accuracy: 89.37727272727273
[ Fri Jul 12 16:50:46 2024 ] 	Batch(5500/6809) done. Loss: 0.5910  lr:0.010000
[ Fri Jul 12 16:51:04 2024 ] 	Batch(5600/6809) done. Loss: 0.5456  lr:0.010000
[ Fri Jul 12 16:51:22 2024 ] 	Batch(5700/6809) done. Loss: 0.3078  lr:0.010000
[ Fri Jul 12 16:51:40 2024 ] 	Batch(5800/6809) done. Loss: 0.3940  lr:0.010000
[ Fri Jul 12 16:51:58 2024 ] 	Batch(5900/6809) done. Loss: 0.0136  lr:0.010000
[ Fri Jul 12 16:52:16 2024 ] 
Training: Epoch [56/120], Step [5999], Loss: 0.9174160361289978, Training Accuracy: 89.41875
[ Fri Jul 12 16:52:16 2024 ] 	Batch(6000/6809) done. Loss: 0.6068  lr:0.010000
[ Fri Jul 12 16:52:34 2024 ] 	Batch(6100/6809) done. Loss: 0.2511  lr:0.010000
[ Fri Jul 12 16:52:52 2024 ] 	Batch(6200/6809) done. Loss: 0.2836  lr:0.010000
[ Fri Jul 12 16:53:10 2024 ] 	Batch(6300/6809) done. Loss: 0.3340  lr:0.010000
[ Fri Jul 12 16:53:28 2024 ] 	Batch(6400/6809) done. Loss: 0.2046  lr:0.010000
[ Fri Jul 12 16:53:45 2024 ] 
Training: Epoch [56/120], Step [6499], Loss: 0.4255939722061157, Training Accuracy: 89.39423076923076
[ Fri Jul 12 16:53:46 2024 ] 	Batch(6500/6809) done. Loss: 0.1500  lr:0.010000
[ Fri Jul 12 16:54:04 2024 ] 	Batch(6600/6809) done. Loss: 0.0208  lr:0.010000
[ Fri Jul 12 16:54:21 2024 ] 	Batch(6700/6809) done. Loss: 0.0365  lr:0.010000
[ Fri Jul 12 16:54:39 2024 ] 	Batch(6800/6809) done. Loss: 0.2966  lr:0.010000
[ Fri Jul 12 16:54:41 2024 ] 	Mean training loss: 0.3302.
[ Fri Jul 12 16:54:41 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 16:54:41 2024 ] Training epoch: 58
[ Fri Jul 12 16:54:42 2024 ] 	Batch(0/6809) done. Loss: 0.1090  lr:0.010000
[ Fri Jul 12 16:55:00 2024 ] 	Batch(100/6809) done. Loss: 0.1423  lr:0.010000
[ Fri Jul 12 16:55:18 2024 ] 	Batch(200/6809) done. Loss: 0.0454  lr:0.010000
[ Fri Jul 12 16:55:36 2024 ] 	Batch(300/6809) done. Loss: 0.0310  lr:0.010000
[ Fri Jul 12 16:55:55 2024 ] 	Batch(400/6809) done. Loss: 0.1530  lr:0.010000
[ Fri Jul 12 16:56:13 2024 ] 
Training: Epoch [57/120], Step [499], Loss: 0.09326274693012238, Training Accuracy: 91.57499999999999
[ Fri Jul 12 16:56:13 2024 ] 	Batch(500/6809) done. Loss: 0.3305  lr:0.010000
[ Fri Jul 12 16:56:32 2024 ] 	Batch(600/6809) done. Loss: 0.5035  lr:0.010000
[ Fri Jul 12 16:56:50 2024 ] 	Batch(700/6809) done. Loss: 0.2114  lr:0.010000
[ Fri Jul 12 16:57:08 2024 ] 	Batch(800/6809) done. Loss: 0.0082  lr:0.010000
[ Fri Jul 12 16:57:26 2024 ] 	Batch(900/6809) done. Loss: 0.0140  lr:0.010000
[ Fri Jul 12 16:57:44 2024 ] 
Training: Epoch [57/120], Step [999], Loss: 0.17620804905891418, Training Accuracy: 91.07499999999999
[ Fri Jul 12 16:57:44 2024 ] 	Batch(1000/6809) done. Loss: 0.4505  lr:0.010000
[ Fri Jul 12 16:58:02 2024 ] 	Batch(1100/6809) done. Loss: 0.6098  lr:0.010000
[ Fri Jul 12 16:58:20 2024 ] 	Batch(1200/6809) done. Loss: 0.0913  lr:0.010000
[ Fri Jul 12 16:58:38 2024 ] 	Batch(1300/6809) done. Loss: 0.0368  lr:0.010000
[ Fri Jul 12 16:58:56 2024 ] 	Batch(1400/6809) done. Loss: 0.3745  lr:0.010000
[ Fri Jul 12 16:59:13 2024 ] 
Training: Epoch [57/120], Step [1499], Loss: 0.15401773154735565, Training Accuracy: 91.025
[ Fri Jul 12 16:59:14 2024 ] 	Batch(1500/6809) done. Loss: 0.1408  lr:0.010000
[ Fri Jul 12 16:59:32 2024 ] 	Batch(1600/6809) done. Loss: 0.3790  lr:0.010000
[ Fri Jul 12 16:59:50 2024 ] 	Batch(1700/6809) done. Loss: 0.4162  lr:0.010000
[ Fri Jul 12 17:00:09 2024 ] 	Batch(1800/6809) done. Loss: 1.0732  lr:0.010000
[ Fri Jul 12 17:00:27 2024 ] 	Batch(1900/6809) done. Loss: 0.7023  lr:0.010000
[ Fri Jul 12 17:00:46 2024 ] 
Training: Epoch [57/120], Step [1999], Loss: 0.40625596046447754, Training Accuracy: 90.71875
[ Fri Jul 12 17:00:46 2024 ] 	Batch(2000/6809) done. Loss: 0.0570  lr:0.010000
[ Fri Jul 12 17:01:04 2024 ] 	Batch(2100/6809) done. Loss: 0.5383  lr:0.010000
[ Fri Jul 12 17:01:23 2024 ] 	Batch(2200/6809) done. Loss: 0.0803  lr:0.010000
[ Fri Jul 12 17:01:42 2024 ] 	Batch(2300/6809) done. Loss: 0.5208  lr:0.010000
[ Fri Jul 12 17:02:00 2024 ] 	Batch(2400/6809) done. Loss: 0.0458  lr:0.010000
[ Fri Jul 12 17:02:19 2024 ] 
Training: Epoch [57/120], Step [2499], Loss: 0.36951425671577454, Training Accuracy: 90.635
[ Fri Jul 12 17:02:19 2024 ] 	Batch(2500/6809) done. Loss: 0.7926  lr:0.010000
[ Fri Jul 12 17:02:37 2024 ] 	Batch(2600/6809) done. Loss: 0.0097  lr:0.010000
[ Fri Jul 12 17:02:56 2024 ] 	Batch(2700/6809) done. Loss: 0.0582  lr:0.010000
[ Fri Jul 12 17:03:15 2024 ] 	Batch(2800/6809) done. Loss: 0.0852  lr:0.010000
[ Fri Jul 12 17:03:32 2024 ] 	Batch(2900/6809) done. Loss: 0.2706  lr:0.010000
[ Fri Jul 12 17:03:50 2024 ] 
Training: Epoch [57/120], Step [2999], Loss: 0.24902065098285675, Training Accuracy: 90.72916666666667
[ Fri Jul 12 17:03:50 2024 ] 	Batch(3000/6809) done. Loss: 0.1522  lr:0.010000
[ Fri Jul 12 17:04:08 2024 ] 	Batch(3100/6809) done. Loss: 0.2196  lr:0.010000
[ Fri Jul 12 17:04:27 2024 ] 	Batch(3200/6809) done. Loss: 0.3100  lr:0.010000
[ Fri Jul 12 17:04:45 2024 ] 	Batch(3300/6809) done. Loss: 0.1004  lr:0.010000
[ Fri Jul 12 17:05:04 2024 ] 	Batch(3400/6809) done. Loss: 0.3558  lr:0.010000
[ Fri Jul 12 17:05:22 2024 ] 
Training: Epoch [57/120], Step [3499], Loss: 0.13642655313014984, Training Accuracy: 90.76071428571429
[ Fri Jul 12 17:05:22 2024 ] 	Batch(3500/6809) done. Loss: 0.2718  lr:0.010000
[ Fri Jul 12 17:05:41 2024 ] 	Batch(3600/6809) done. Loss: 0.3583  lr:0.010000
[ Fri Jul 12 17:06:00 2024 ] 	Batch(3700/6809) done. Loss: 0.2797  lr:0.010000
[ Fri Jul 12 17:06:18 2024 ] 	Batch(3800/6809) done. Loss: 0.0648  lr:0.010000
[ Fri Jul 12 17:06:35 2024 ] 	Batch(3900/6809) done. Loss: 0.0293  lr:0.010000
[ Fri Jul 12 17:06:53 2024 ] 
Training: Epoch [57/120], Step [3999], Loss: 0.20791035890579224, Training Accuracy: 90.703125
[ Fri Jul 12 17:06:53 2024 ] 	Batch(4000/6809) done. Loss: 0.0876  lr:0.010000
[ Fri Jul 12 17:07:11 2024 ] 	Batch(4100/6809) done. Loss: 0.3689  lr:0.010000
[ Fri Jul 12 17:07:29 2024 ] 	Batch(4200/6809) done. Loss: 0.1106  lr:0.010000
[ Fri Jul 12 17:07:47 2024 ] 	Batch(4300/6809) done. Loss: 0.5256  lr:0.010000
[ Fri Jul 12 17:08:05 2024 ] 	Batch(4400/6809) done. Loss: 0.8270  lr:0.010000
[ Fri Jul 12 17:08:23 2024 ] 
Training: Epoch [57/120], Step [4499], Loss: 0.061666712164878845, Training Accuracy: 90.55277777777778
[ Fri Jul 12 17:08:23 2024 ] 	Batch(4500/6809) done. Loss: 0.0122  lr:0.010000
[ Fri Jul 12 17:08:41 2024 ] 	Batch(4600/6809) done. Loss: 0.3717  lr:0.010000
[ Fri Jul 12 17:08:59 2024 ] 	Batch(4700/6809) done. Loss: 0.8030  lr:0.010000
[ Fri Jul 12 17:09:17 2024 ] 	Batch(4800/6809) done. Loss: 1.2197  lr:0.010000
[ Fri Jul 12 17:09:36 2024 ] 	Batch(4900/6809) done. Loss: 0.1603  lr:0.010000
[ Fri Jul 12 17:09:54 2024 ] 
Training: Epoch [57/120], Step [4999], Loss: 0.22592058777809143, Training Accuracy: 90.3575
[ Fri Jul 12 17:09:54 2024 ] 	Batch(5000/6809) done. Loss: 0.1437  lr:0.010000
[ Fri Jul 12 17:10:13 2024 ] 	Batch(5100/6809) done. Loss: 0.1681  lr:0.010000
[ Fri Jul 12 17:10:31 2024 ] 	Batch(5200/6809) done. Loss: 1.3401  lr:0.010000
[ Fri Jul 12 17:10:49 2024 ] 	Batch(5300/6809) done. Loss: 0.3763  lr:0.010000
[ Fri Jul 12 17:11:07 2024 ] 	Batch(5400/6809) done. Loss: 0.3651  lr:0.010000
[ Fri Jul 12 17:11:25 2024 ] 
Training: Epoch [57/120], Step [5499], Loss: 0.7649154663085938, Training Accuracy: 90.23863636363636
[ Fri Jul 12 17:11:25 2024 ] 	Batch(5500/6809) done. Loss: 0.0383  lr:0.010000
[ Fri Jul 12 17:11:43 2024 ] 	Batch(5600/6809) done. Loss: 0.7768  lr:0.010000
[ Fri Jul 12 17:12:01 2024 ] 	Batch(5700/6809) done. Loss: 0.1925  lr:0.010000
[ Fri Jul 12 17:12:19 2024 ] 	Batch(5800/6809) done. Loss: 0.3593  lr:0.010000
[ Fri Jul 12 17:12:37 2024 ] 	Batch(5900/6809) done. Loss: 0.3521  lr:0.010000
[ Fri Jul 12 17:12:55 2024 ] 
Training: Epoch [57/120], Step [5999], Loss: 0.0740923061966896, Training Accuracy: 90.16041666666666
[ Fri Jul 12 17:12:55 2024 ] 	Batch(6000/6809) done. Loss: 0.2102  lr:0.010000
[ Fri Jul 12 17:13:13 2024 ] 	Batch(6100/6809) done. Loss: 0.1530  lr:0.010000
[ Fri Jul 12 17:13:31 2024 ] 	Batch(6200/6809) done. Loss: 0.1891  lr:0.010000
[ Fri Jul 12 17:13:48 2024 ] 	Batch(6300/6809) done. Loss: 0.4059  lr:0.010000
[ Fri Jul 12 17:14:06 2024 ] 	Batch(6400/6809) done. Loss: 1.3217  lr:0.010000
[ Fri Jul 12 17:14:24 2024 ] 
Training: Epoch [57/120], Step [6499], Loss: 0.7942730784416199, Training Accuracy: 90.06538461538462
[ Fri Jul 12 17:14:24 2024 ] 	Batch(6500/6809) done. Loss: 1.5344  lr:0.010000
[ Fri Jul 12 17:14:42 2024 ] 	Batch(6600/6809) done. Loss: 0.7265  lr:0.010000
[ Fri Jul 12 17:15:00 2024 ] 	Batch(6700/6809) done. Loss: 0.9799  lr:0.010000
[ Fri Jul 12 17:15:18 2024 ] 	Batch(6800/6809) done. Loss: 0.4030  lr:0.010000
[ Fri Jul 12 17:15:20 2024 ] 	Mean training loss: 0.3176.
[ Fri Jul 12 17:15:20 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 17:15:20 2024 ] Training epoch: 59
[ Fri Jul 12 17:15:20 2024 ] 	Batch(0/6809) done. Loss: 0.5752  lr:0.010000
[ Fri Jul 12 17:15:39 2024 ] 	Batch(100/6809) done. Loss: 0.1499  lr:0.010000
[ Fri Jul 12 17:15:57 2024 ] 	Batch(200/6809) done. Loss: 0.1224  lr:0.010000
[ Fri Jul 12 17:16:15 2024 ] 	Batch(300/6809) done. Loss: 0.1025  lr:0.010000
[ Fri Jul 12 17:16:34 2024 ] 	Batch(400/6809) done. Loss: 0.4536  lr:0.010000
[ Fri Jul 12 17:16:52 2024 ] 
Training: Epoch [58/120], Step [499], Loss: 0.07804721593856812, Training Accuracy: 90.125
[ Fri Jul 12 17:16:52 2024 ] 	Batch(500/6809) done. Loss: 0.5521  lr:0.010000
[ Fri Jul 12 17:17:10 2024 ] 	Batch(600/6809) done. Loss: 0.9152  lr:0.010000
[ Fri Jul 12 17:17:29 2024 ] 	Batch(700/6809) done. Loss: 0.0483  lr:0.010000
[ Fri Jul 12 17:17:47 2024 ] 	Batch(800/6809) done. Loss: 0.0280  lr:0.010000
[ Fri Jul 12 17:18:06 2024 ] 	Batch(900/6809) done. Loss: 0.0606  lr:0.010000
[ Fri Jul 12 17:18:24 2024 ] 
Training: Epoch [58/120], Step [999], Loss: 0.5027512311935425, Training Accuracy: 90.525
[ Fri Jul 12 17:18:24 2024 ] 	Batch(1000/6809) done. Loss: 0.3012  lr:0.010000
[ Fri Jul 12 17:18:42 2024 ] 	Batch(1100/6809) done. Loss: 0.2014  lr:0.010000
[ Fri Jul 12 17:19:00 2024 ] 	Batch(1200/6809) done. Loss: 1.1686  lr:0.010000
[ Fri Jul 12 17:19:18 2024 ] 	Batch(1300/6809) done. Loss: 0.2472  lr:0.010000
[ Fri Jul 12 17:19:36 2024 ] 	Batch(1400/6809) done. Loss: 0.0505  lr:0.010000
[ Fri Jul 12 17:19:54 2024 ] 
Training: Epoch [58/120], Step [1499], Loss: 0.01995692029595375, Training Accuracy: 90.10833333333333
[ Fri Jul 12 17:19:54 2024 ] 	Batch(1500/6809) done. Loss: 0.1809  lr:0.010000
[ Fri Jul 12 17:20:12 2024 ] 	Batch(1600/6809) done. Loss: 0.1106  lr:0.010000
[ Fri Jul 12 17:20:30 2024 ] 	Batch(1700/6809) done. Loss: 0.0345  lr:0.010000
[ Fri Jul 12 17:20:48 2024 ] 	Batch(1800/6809) done. Loss: 0.1658  lr:0.010000
[ Fri Jul 12 17:21:06 2024 ] 	Batch(1900/6809) done. Loss: 0.1084  lr:0.010000
[ Fri Jul 12 17:21:23 2024 ] 
Training: Epoch [58/120], Step [1999], Loss: 0.1619282215833664, Training Accuracy: 90.2875
[ Fri Jul 12 17:21:24 2024 ] 	Batch(2000/6809) done. Loss: 0.2635  lr:0.010000
[ Fri Jul 12 17:21:42 2024 ] 	Batch(2100/6809) done. Loss: 0.1367  lr:0.010000
[ Fri Jul 12 17:22:00 2024 ] 	Batch(2200/6809) done. Loss: 0.0957  lr:0.010000
[ Fri Jul 12 17:22:18 2024 ] 	Batch(2300/6809) done. Loss: 0.3425  lr:0.010000
[ Fri Jul 12 17:22:36 2024 ] 	Batch(2400/6809) done. Loss: 0.3502  lr:0.010000
[ Fri Jul 12 17:22:53 2024 ] 
Training: Epoch [58/120], Step [2499], Loss: 0.6364791393280029, Training Accuracy: 90.05499999999999
[ Fri Jul 12 17:22:54 2024 ] 	Batch(2500/6809) done. Loss: 0.2436  lr:0.010000
[ Fri Jul 12 17:23:12 2024 ] 	Batch(2600/6809) done. Loss: 0.2293  lr:0.010000
[ Fri Jul 12 17:23:30 2024 ] 	Batch(2700/6809) done. Loss: 0.3673  lr:0.010000
[ Fri Jul 12 17:23:48 2024 ] 	Batch(2800/6809) done. Loss: 0.2077  lr:0.010000
[ Fri Jul 12 17:24:06 2024 ] 	Batch(2900/6809) done. Loss: 0.9021  lr:0.010000
[ Fri Jul 12 17:24:23 2024 ] 
Training: Epoch [58/120], Step [2999], Loss: 0.026963822543621063, Training Accuracy: 89.97083333333333
[ Fri Jul 12 17:24:24 2024 ] 	Batch(3000/6809) done. Loss: 0.2798  lr:0.010000
[ Fri Jul 12 17:24:42 2024 ] 	Batch(3100/6809) done. Loss: 0.4156  lr:0.010000
[ Fri Jul 12 17:25:00 2024 ] 	Batch(3200/6809) done. Loss: 0.5320  lr:0.010000
[ Fri Jul 12 17:25:18 2024 ] 	Batch(3300/6809) done. Loss: 0.0312  lr:0.010000
[ Fri Jul 12 17:25:35 2024 ] 	Batch(3400/6809) done. Loss: 0.5352  lr:0.010000
[ Fri Jul 12 17:25:53 2024 ] 
Training: Epoch [58/120], Step [3499], Loss: 0.015253531746566296, Training Accuracy: 90.00357142857143
[ Fri Jul 12 17:25:53 2024 ] 	Batch(3500/6809) done. Loss: 0.0209  lr:0.010000
[ Fri Jul 12 17:26:11 2024 ] 	Batch(3600/6809) done. Loss: 0.0318  lr:0.010000
[ Fri Jul 12 17:26:29 2024 ] 	Batch(3700/6809) done. Loss: 0.0374  lr:0.010000
[ Fri Jul 12 17:26:47 2024 ] 	Batch(3800/6809) done. Loss: 0.0204  lr:0.010000
[ Fri Jul 12 17:27:05 2024 ] 	Batch(3900/6809) done. Loss: 0.1768  lr:0.010000
[ Fri Jul 12 17:27:23 2024 ] 
Training: Epoch [58/120], Step [3999], Loss: 0.28126177191734314, Training Accuracy: 90.01249999999999
[ Fri Jul 12 17:27:23 2024 ] 	Batch(4000/6809) done. Loss: 0.2633  lr:0.010000
[ Fri Jul 12 17:27:41 2024 ] 	Batch(4100/6809) done. Loss: 0.0391  lr:0.010000
[ Fri Jul 12 17:27:59 2024 ] 	Batch(4200/6809) done. Loss: 0.2764  lr:0.010000
[ Fri Jul 12 17:28:17 2024 ] 	Batch(4300/6809) done. Loss: 0.7828  lr:0.010000
[ Fri Jul 12 17:28:35 2024 ] 	Batch(4400/6809) done. Loss: 0.1088  lr:0.010000
[ Fri Jul 12 17:28:53 2024 ] 
Training: Epoch [58/120], Step [4499], Loss: 0.1695321649312973, Training Accuracy: 89.89166666666667
[ Fri Jul 12 17:28:53 2024 ] 	Batch(4500/6809) done. Loss: 0.0513  lr:0.010000
[ Fri Jul 12 17:29:11 2024 ] 	Batch(4600/6809) done. Loss: 0.2436  lr:0.010000
[ Fri Jul 12 17:29:29 2024 ] 	Batch(4700/6809) done. Loss: 0.4583  lr:0.010000
[ Fri Jul 12 17:29:47 2024 ] 	Batch(4800/6809) done. Loss: 0.5419  lr:0.010000
[ Fri Jul 12 17:30:05 2024 ] 	Batch(4900/6809) done. Loss: 0.2246  lr:0.010000
[ Fri Jul 12 17:30:22 2024 ] 
Training: Epoch [58/120], Step [4999], Loss: 0.30936554074287415, Training Accuracy: 89.7875
[ Fri Jul 12 17:30:23 2024 ] 	Batch(5000/6809) done. Loss: 0.5988  lr:0.010000
[ Fri Jul 12 17:30:40 2024 ] 	Batch(5100/6809) done. Loss: 0.3408  lr:0.010000
[ Fri Jul 12 17:30:59 2024 ] 	Batch(5200/6809) done. Loss: 0.4475  lr:0.010000
[ Fri Jul 12 17:31:16 2024 ] 	Batch(5300/6809) done. Loss: 0.4364  lr:0.010000
[ Fri Jul 12 17:31:34 2024 ] 	Batch(5400/6809) done. Loss: 0.2842  lr:0.010000
[ Fri Jul 12 17:31:52 2024 ] 
Training: Epoch [58/120], Step [5499], Loss: 0.0897618755698204, Training Accuracy: 89.59545454545454
[ Fri Jul 12 17:31:52 2024 ] 	Batch(5500/6809) done. Loss: 0.1279  lr:0.010000
[ Fri Jul 12 17:32:10 2024 ] 	Batch(5600/6809) done. Loss: 0.7437  lr:0.010000
[ Fri Jul 12 17:32:28 2024 ] 	Batch(5700/6809) done. Loss: 0.1038  lr:0.010000
[ Fri Jul 12 17:32:46 2024 ] 	Batch(5800/6809) done. Loss: 0.2282  lr:0.010000
[ Fri Jul 12 17:33:04 2024 ] 	Batch(5900/6809) done. Loss: 0.1519  lr:0.010000
[ Fri Jul 12 17:33:22 2024 ] 
Training: Epoch [58/120], Step [5999], Loss: 0.8029481768608093, Training Accuracy: 89.55833333333332
[ Fri Jul 12 17:33:22 2024 ] 	Batch(6000/6809) done. Loss: 0.1472  lr:0.010000
[ Fri Jul 12 17:33:40 2024 ] 	Batch(6100/6809) done. Loss: 0.3871  lr:0.010000
[ Fri Jul 12 17:33:58 2024 ] 	Batch(6200/6809) done. Loss: 0.9772  lr:0.010000
[ Fri Jul 12 17:34:16 2024 ] 	Batch(6300/6809) done. Loss: 0.4570  lr:0.010000
[ Fri Jul 12 17:34:34 2024 ] 	Batch(6400/6809) done. Loss: 0.2981  lr:0.010000
[ Fri Jul 12 17:34:52 2024 ] 
Training: Epoch [58/120], Step [6499], Loss: 0.05956493690609932, Training Accuracy: 89.52115384615385
[ Fri Jul 12 17:34:52 2024 ] 	Batch(6500/6809) done. Loss: 0.1500  lr:0.010000
[ Fri Jul 12 17:35:10 2024 ] 	Batch(6600/6809) done. Loss: 0.6279  lr:0.010000
[ Fri Jul 12 17:35:28 2024 ] 	Batch(6700/6809) done. Loss: 0.6421  lr:0.010000
[ Fri Jul 12 17:35:46 2024 ] 	Batch(6800/6809) done. Loss: 0.9764  lr:0.010000
[ Fri Jul 12 17:35:47 2024 ] 	Mean training loss: 0.3257.
[ Fri Jul 12 17:35:47 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 17:35:47 2024 ] Training epoch: 60
[ Fri Jul 12 17:35:48 2024 ] 	Batch(0/6809) done. Loss: 0.1204  lr:0.010000
[ Fri Jul 12 17:36:06 2024 ] 	Batch(100/6809) done. Loss: 0.1075  lr:0.010000
[ Fri Jul 12 17:36:24 2024 ] 	Batch(200/6809) done. Loss: 0.7461  lr:0.010000
[ Fri Jul 12 17:36:43 2024 ] 	Batch(300/6809) done. Loss: 0.4648  lr:0.010000
[ Fri Jul 12 17:37:01 2024 ] 	Batch(400/6809) done. Loss: 0.4257  lr:0.010000
[ Fri Jul 12 17:37:19 2024 ] 
Training: Epoch [59/120], Step [499], Loss: 0.23017625510692596, Training Accuracy: 91.375
[ Fri Jul 12 17:37:19 2024 ] 	Batch(500/6809) done. Loss: 0.2256  lr:0.010000
[ Fri Jul 12 17:37:38 2024 ] 	Batch(600/6809) done. Loss: 0.7024  lr:0.010000
[ Fri Jul 12 17:37:56 2024 ] 	Batch(700/6809) done. Loss: 0.0345  lr:0.010000
[ Fri Jul 12 17:38:14 2024 ] 	Batch(800/6809) done. Loss: 0.1469  lr:0.010000
[ Fri Jul 12 17:38:32 2024 ] 	Batch(900/6809) done. Loss: 0.3667  lr:0.010000
[ Fri Jul 12 17:38:51 2024 ] 
Training: Epoch [59/120], Step [999], Loss: 0.28569090366363525, Training Accuracy: 91.35
[ Fri Jul 12 17:38:51 2024 ] 	Batch(1000/6809) done. Loss: 0.2143  lr:0.010000
[ Fri Jul 12 17:39:09 2024 ] 	Batch(1100/6809) done. Loss: 0.0837  lr:0.010000
[ Fri Jul 12 17:39:27 2024 ] 	Batch(1200/6809) done. Loss: 0.3813  lr:0.010000
[ Fri Jul 12 17:39:45 2024 ] 	Batch(1300/6809) done. Loss: 0.7015  lr:0.010000
[ Fri Jul 12 17:40:04 2024 ] 	Batch(1400/6809) done. Loss: 0.6591  lr:0.010000
[ Fri Jul 12 17:40:22 2024 ] 
Training: Epoch [59/120], Step [1499], Loss: 0.7253937721252441, Training Accuracy: 91.29166666666667
[ Fri Jul 12 17:40:22 2024 ] 	Batch(1500/6809) done. Loss: 0.0141  lr:0.010000
[ Fri Jul 12 17:40:40 2024 ] 	Batch(1600/6809) done. Loss: 0.5522  lr:0.010000
[ Fri Jul 12 17:40:59 2024 ] 	Batch(1700/6809) done. Loss: 0.4227  lr:0.010000
[ Fri Jul 12 17:41:17 2024 ] 	Batch(1800/6809) done. Loss: 0.3675  lr:0.010000
[ Fri Jul 12 17:41:35 2024 ] 	Batch(1900/6809) done. Loss: 0.3400  lr:0.010000
[ Fri Jul 12 17:41:54 2024 ] 
Training: Epoch [59/120], Step [1999], Loss: 0.3621017336845398, Training Accuracy: 91.09375
[ Fri Jul 12 17:41:54 2024 ] 	Batch(2000/6809) done. Loss: 0.2710  lr:0.010000
[ Fri Jul 12 17:42:12 2024 ] 	Batch(2100/6809) done. Loss: 0.4536  lr:0.010000
[ Fri Jul 12 17:42:30 2024 ] 	Batch(2200/6809) done. Loss: 0.5174  lr:0.010000
[ Fri Jul 12 17:42:48 2024 ] 	Batch(2300/6809) done. Loss: 0.2815  lr:0.010000
[ Fri Jul 12 17:43:06 2024 ] 	Batch(2400/6809) done. Loss: 0.1874  lr:0.010000
[ Fri Jul 12 17:43:24 2024 ] 
Training: Epoch [59/120], Step [2499], Loss: 0.1219140887260437, Training Accuracy: 91.095
[ Fri Jul 12 17:43:24 2024 ] 	Batch(2500/6809) done. Loss: 0.3428  lr:0.010000
[ Fri Jul 12 17:43:42 2024 ] 	Batch(2600/6809) done. Loss: 0.2636  lr:0.010000
[ Fri Jul 12 17:43:59 2024 ] 	Batch(2700/6809) done. Loss: 0.0146  lr:0.010000
[ Fri Jul 12 17:44:17 2024 ] 	Batch(2800/6809) done. Loss: 0.3297  lr:0.010000
[ Fri Jul 12 17:44:35 2024 ] 	Batch(2900/6809) done. Loss: 0.1400  lr:0.010000
[ Fri Jul 12 17:44:53 2024 ] 
Training: Epoch [59/120], Step [2999], Loss: 0.023015886545181274, Training Accuracy: 90.8875
[ Fri Jul 12 17:44:53 2024 ] 	Batch(3000/6809) done. Loss: 0.1132  lr:0.010000
[ Fri Jul 12 17:45:11 2024 ] 	Batch(3100/6809) done. Loss: 0.5489  lr:0.010000
[ Fri Jul 12 17:45:29 2024 ] 	Batch(3200/6809) done. Loss: 0.2875  lr:0.010000
[ Fri Jul 12 17:45:47 2024 ] 	Batch(3300/6809) done. Loss: 0.1429  lr:0.010000
[ Fri Jul 12 17:46:05 2024 ] 	Batch(3400/6809) done. Loss: 0.0774  lr:0.010000
[ Fri Jul 12 17:46:23 2024 ] 
Training: Epoch [59/120], Step [3499], Loss: 0.10756116360425949, Training Accuracy: 90.81428571428572
[ Fri Jul 12 17:46:23 2024 ] 	Batch(3500/6809) done. Loss: 0.0787  lr:0.010000
[ Fri Jul 12 17:46:41 2024 ] 	Batch(3600/6809) done. Loss: 0.1472  lr:0.010000
[ Fri Jul 12 17:46:59 2024 ] 	Batch(3700/6809) done. Loss: 0.2534  lr:0.010000
[ Fri Jul 12 17:47:17 2024 ] 	Batch(3800/6809) done. Loss: 0.0112  lr:0.010000
[ Fri Jul 12 17:47:35 2024 ] 	Batch(3900/6809) done. Loss: 1.1565  lr:0.010000
[ Fri Jul 12 17:47:52 2024 ] 
Training: Epoch [59/120], Step [3999], Loss: 0.14864091575145721, Training Accuracy: 90.753125
[ Fri Jul 12 17:47:53 2024 ] 	Batch(4000/6809) done. Loss: 0.0491  lr:0.010000
[ Fri Jul 12 17:48:10 2024 ] 	Batch(4100/6809) done. Loss: 0.0868  lr:0.010000
[ Fri Jul 12 17:48:28 2024 ] 	Batch(4200/6809) done. Loss: 0.0128  lr:0.010000
[ Fri Jul 12 17:48:46 2024 ] 	Batch(4300/6809) done. Loss: 0.4108  lr:0.010000
[ Fri Jul 12 17:49:04 2024 ] 	Batch(4400/6809) done. Loss: 0.2272  lr:0.010000
[ Fri Jul 12 17:49:22 2024 ] 
Training: Epoch [59/120], Step [4499], Loss: 0.03251715376973152, Training Accuracy: 90.60000000000001
[ Fri Jul 12 17:49:22 2024 ] 	Batch(4500/6809) done. Loss: 0.1338  lr:0.010000
[ Fri Jul 12 17:49:40 2024 ] 	Batch(4600/6809) done. Loss: 0.7138  lr:0.010000
[ Fri Jul 12 17:49:58 2024 ] 	Batch(4700/6809) done. Loss: 0.3834  lr:0.010000
[ Fri Jul 12 17:50:16 2024 ] 	Batch(4800/6809) done. Loss: 0.0201  lr:0.010000
[ Fri Jul 12 17:50:34 2024 ] 	Batch(4900/6809) done. Loss: 0.3306  lr:0.010000
[ Fri Jul 12 17:50:52 2024 ] 
Training: Epoch [59/120], Step [4999], Loss: 0.38024821877479553, Training Accuracy: 90.5025
[ Fri Jul 12 17:50:52 2024 ] 	Batch(5000/6809) done. Loss: 0.4525  lr:0.010000
[ Fri Jul 12 17:51:10 2024 ] 	Batch(5100/6809) done. Loss: 1.1226  lr:0.010000
[ Fri Jul 12 17:51:28 2024 ] 	Batch(5200/6809) done. Loss: 0.1114  lr:0.010000
[ Fri Jul 12 17:51:46 2024 ] 	Batch(5300/6809) done. Loss: 0.0397  lr:0.010000
[ Fri Jul 12 17:52:04 2024 ] 	Batch(5400/6809) done. Loss: 0.2425  lr:0.010000
[ Fri Jul 12 17:52:22 2024 ] 
Training: Epoch [59/120], Step [5499], Loss: 0.07480476051568985, Training Accuracy: 90.47272727272727
[ Fri Jul 12 17:52:22 2024 ] 	Batch(5500/6809) done. Loss: 1.2828  lr:0.010000
[ Fri Jul 12 17:52:40 2024 ] 	Batch(5600/6809) done. Loss: 0.1725  lr:0.010000
[ Fri Jul 12 17:52:58 2024 ] 	Batch(5700/6809) done. Loss: 1.1989  lr:0.010000
[ Fri Jul 12 17:53:16 2024 ] 	Batch(5800/6809) done. Loss: 0.1652  lr:0.010000
[ Fri Jul 12 17:53:33 2024 ] 	Batch(5900/6809) done. Loss: 0.6443  lr:0.010000
[ Fri Jul 12 17:53:51 2024 ] 
Training: Epoch [59/120], Step [5999], Loss: 0.09858939051628113, Training Accuracy: 90.31458333333333
[ Fri Jul 12 17:53:52 2024 ] 	Batch(6000/6809) done. Loss: 0.7952  lr:0.010000
[ Fri Jul 12 17:54:09 2024 ] 	Batch(6100/6809) done. Loss: 0.0593  lr:0.010000
[ Fri Jul 12 17:54:27 2024 ] 	Batch(6200/6809) done. Loss: 0.1254  lr:0.010000
[ Fri Jul 12 17:54:45 2024 ] 	Batch(6300/6809) done. Loss: 0.7369  lr:0.010000
[ Fri Jul 12 17:55:03 2024 ] 	Batch(6400/6809) done. Loss: 0.4865  lr:0.010000
[ Fri Jul 12 17:55:21 2024 ] 
Training: Epoch [59/120], Step [6499], Loss: 0.47643983364105225, Training Accuracy: 90.23846153846154
[ Fri Jul 12 17:55:21 2024 ] 	Batch(6500/6809) done. Loss: 0.4357  lr:0.010000
[ Fri Jul 12 17:55:39 2024 ] 	Batch(6600/6809) done. Loss: 0.0531  lr:0.010000
[ Fri Jul 12 17:55:57 2024 ] 	Batch(6700/6809) done. Loss: 0.1997  lr:0.010000
[ Fri Jul 12 17:56:15 2024 ] 	Batch(6800/6809) done. Loss: 0.9490  lr:0.010000
[ Fri Jul 12 17:56:17 2024 ] 	Mean training loss: 0.3106.
[ Fri Jul 12 17:56:17 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 17:56:17 2024 ] Eval epoch: 60
[ Fri Jul 12 18:01:51 2024 ] 	Mean val loss of 7435 batches: 1.2193241917877438.
[ Fri Jul 12 18:01:51 2024 ] 
Validation: Epoch [59/120], Samples [45585.0/59477], Loss: 1.1894556283950806, Validation Accuracy: 76.64307211190881
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 1 : 365 / 500 = 73 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 2 : 387 / 499 = 77 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 3 : 394 / 500 = 78 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 4 : 407 / 502 = 81 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 5 : 464 / 502 = 92 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 6 : 420 / 502 = 83 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 7 : 465 / 497 = 93 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 8 : 467 / 498 = 93 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 9 : 386 / 500 = 77 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 10 : 175 / 500 = 35 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 11 : 197 / 498 = 39 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 12 : 410 / 499 = 82 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 13 : 399 / 502 = 79 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 14 : 475 / 504 = 94 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 15 : 445 / 502 = 88 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 16 : 313 / 502 = 62 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 17 : 446 / 504 = 88 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 18 : 424 / 504 = 84 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 19 : 431 / 502 = 85 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 20 : 445 / 502 = 88 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 21 : 453 / 503 = 90 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 22 : 423 / 504 = 83 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 23 : 447 / 503 = 88 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 24 : 360 / 504 = 71 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 25 : 480 / 504 = 95 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 26 : 469 / 504 = 93 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 27 : 419 / 501 = 83 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 28 : 334 / 502 = 66 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 29 : 338 / 502 = 67 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 30 : 305 / 501 = 60 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 31 : 413 / 504 = 81 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 32 : 392 / 503 = 77 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 33 : 417 / 503 = 82 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 34 : 463 / 504 = 91 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 35 : 465 / 503 = 92 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 36 : 394 / 502 = 78 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 37 : 428 / 504 = 84 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 38 : 415 / 504 = 82 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 39 : 457 / 498 = 91 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 40 : 371 / 504 = 73 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 41 : 479 / 503 = 95 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 42 : 471 / 504 = 93 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 43 : 303 / 503 = 60 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 44 : 427 / 504 = 84 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 45 : 429 / 504 = 85 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 46 : 432 / 504 = 85 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 47 : 301 / 503 = 59 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 48 : 381 / 503 = 75 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 49 : 388 / 499 = 77 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 50 : 390 / 502 = 77 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 51 : 456 / 503 = 90 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 52 : 454 / 504 = 90 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 53 : 406 / 497 = 81 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 54 : 460 / 480 = 95 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 55 : 394 / 504 = 78 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 56 : 314 / 503 = 62 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 57 : 479 / 504 = 95 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 58 : 473 / 499 = 94 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 59 : 489 / 503 = 97 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 60 : 411 / 479 = 85 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 61 : 382 / 484 = 78 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 62 : 358 / 487 = 73 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 63 : 447 / 489 = 91 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 64 : 348 / 488 = 71 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 65 : 450 / 490 = 91 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 66 : 289 / 488 = 59 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 67 : 330 / 490 = 67 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 68 : 197 / 490 = 40 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 69 : 312 / 490 = 63 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 70 : 316 / 490 = 64 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 71 : 151 / 490 = 30 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 72 : 78 / 488 = 15 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 73 : 189 / 486 = 38 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 74 : 205 / 481 = 42 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 75 : 232 / 488 = 47 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 76 : 296 / 489 = 60 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 77 : 297 / 488 = 60 %
[ Fri Jul 12 18:01:51 2024 ] Accuracy of 78 : 343 / 488 = 70 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 79 : 436 / 490 = 88 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 80 : 386 / 489 = 78 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 81 : 341 / 491 = 69 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 82 : 277 / 491 = 56 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 83 : 314 / 489 = 64 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 84 : 363 / 489 = 74 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 85 : 336 / 489 = 68 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 86 : 444 / 491 = 90 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 87 : 422 / 492 = 85 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 88 : 377 / 491 = 76 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 89 : 380 / 492 = 77 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 90 : 203 / 490 = 41 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 91 : 356 / 482 = 73 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 92 : 300 / 490 = 61 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 93 : 345 / 487 = 70 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 94 : 424 / 489 = 86 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 95 : 403 / 490 = 82 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 96 : 461 / 491 = 93 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 97 : 451 / 490 = 92 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 98 : 453 / 491 = 92 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 99 : 445 / 491 = 90 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 100 : 432 / 491 = 87 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 101 : 417 / 491 = 84 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 102 : 236 / 492 = 47 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 103 : 407 / 492 = 82 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 104 : 249 / 491 = 50 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 105 : 253 / 491 = 51 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 106 : 238 / 492 = 48 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 107 : 411 / 491 = 83 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 108 : 399 / 492 = 81 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 109 : 276 / 490 = 56 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 110 : 374 / 491 = 76 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 111 : 447 / 492 = 90 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 112 : 461 / 492 = 93 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 113 : 432 / 491 = 87 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 114 : 406 / 491 = 82 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 115 : 429 / 492 = 87 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 116 : 372 / 491 = 75 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 117 : 415 / 492 = 84 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 118 : 434 / 490 = 88 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 119 : 443 / 492 = 90 %
[ Fri Jul 12 18:01:52 2024 ] Accuracy of 120 : 422 / 500 = 84 %
[ Fri Jul 12 18:01:52 2024 ] Training epoch: 61
[ Fri Jul 12 18:01:52 2024 ] 	Batch(0/6809) done. Loss: 0.2019  lr:0.000100
[ Fri Jul 12 18:02:10 2024 ] 	Batch(100/6809) done. Loss: 0.3444  lr:0.000100
[ Fri Jul 12 18:02:28 2024 ] 	Batch(200/6809) done. Loss: 0.7672  lr:0.000100
[ Fri Jul 12 18:02:46 2024 ] 	Batch(300/6809) done. Loss: 0.0206  lr:0.000100
[ Fri Jul 12 18:03:04 2024 ] 	Batch(400/6809) done. Loss: 0.1445  lr:0.000100
[ Fri Jul 12 18:03:22 2024 ] 
Training: Epoch [60/120], Step [499], Loss: 0.04982389882206917, Training Accuracy: 92.10000000000001
[ Fri Jul 12 18:03:23 2024 ] 	Batch(500/6809) done. Loss: 0.2602  lr:0.000100
[ Fri Jul 12 18:03:41 2024 ] 	Batch(600/6809) done. Loss: 0.0350  lr:0.000100
[ Fri Jul 12 18:03:59 2024 ] 	Batch(700/6809) done. Loss: 0.0799  lr:0.000100
[ Fri Jul 12 18:04:17 2024 ] 	Batch(800/6809) done. Loss: 0.1020  lr:0.000100
[ Fri Jul 12 18:04:35 2024 ] 	Batch(900/6809) done. Loss: 0.0553  lr:0.000100
[ Fri Jul 12 18:04:52 2024 ] 
Training: Epoch [60/120], Step [999], Loss: 0.2353283166885376, Training Accuracy: 92.45
[ Fri Jul 12 18:04:52 2024 ] 	Batch(1000/6809) done. Loss: 0.1157  lr:0.000100
[ Fri Jul 12 18:05:10 2024 ] 	Batch(1100/6809) done. Loss: 0.0680  lr:0.000100
[ Fri Jul 12 18:05:29 2024 ] 	Batch(1200/6809) done. Loss: 0.4150  lr:0.000100
[ Fri Jul 12 18:05:47 2024 ] 	Batch(1300/6809) done. Loss: 0.1969  lr:0.000100
[ Fri Jul 12 18:06:05 2024 ] 	Batch(1400/6809) done. Loss: 0.3215  lr:0.000100
[ Fri Jul 12 18:06:23 2024 ] 
Training: Epoch [60/120], Step [1499], Loss: 0.11618229746818542, Training Accuracy: 92.375
[ Fri Jul 12 18:06:23 2024 ] 	Batch(1500/6809) done. Loss: 0.2094  lr:0.000100
[ Fri Jul 12 18:06:41 2024 ] 	Batch(1600/6809) done. Loss: 0.1519  lr:0.000100
[ Fri Jul 12 18:06:59 2024 ] 	Batch(1700/6809) done. Loss: 0.0936  lr:0.000100
[ Fri Jul 12 18:07:17 2024 ] 	Batch(1800/6809) done. Loss: 0.0768  lr:0.000100
[ Fri Jul 12 18:07:35 2024 ] 	Batch(1900/6809) done. Loss: 0.1659  lr:0.000100
[ Fri Jul 12 18:07:52 2024 ] 
Training: Epoch [60/120], Step [1999], Loss: 0.054032664746046066, Training Accuracy: 92.55624999999999
[ Fri Jul 12 18:07:53 2024 ] 	Batch(2000/6809) done. Loss: 0.4018  lr:0.000100
[ Fri Jul 12 18:08:11 2024 ] 	Batch(2100/6809) done. Loss: 0.5727  lr:0.000100
[ Fri Jul 12 18:08:29 2024 ] 	Batch(2200/6809) done. Loss: 0.0571  lr:0.000100
[ Fri Jul 12 18:08:46 2024 ] 	Batch(2300/6809) done. Loss: 0.0192  lr:0.000100
[ Fri Jul 12 18:09:04 2024 ] 	Batch(2400/6809) done. Loss: 0.1366  lr:0.000100
[ Fri Jul 12 18:09:22 2024 ] 
Training: Epoch [60/120], Step [2499], Loss: 0.5629860758781433, Training Accuracy: 92.78
[ Fri Jul 12 18:09:22 2024 ] 	Batch(2500/6809) done. Loss: 0.0516  lr:0.000100
[ Fri Jul 12 18:09:40 2024 ] 	Batch(2600/6809) done. Loss: 0.1739  lr:0.000100
[ Fri Jul 12 18:09:58 2024 ] 	Batch(2700/6809) done. Loss: 0.1978  lr:0.000100
[ Fri Jul 12 18:10:16 2024 ] 	Batch(2800/6809) done. Loss: 0.3188  lr:0.000100
[ Fri Jul 12 18:10:35 2024 ] 	Batch(2900/6809) done. Loss: 0.1659  lr:0.000100
[ Fri Jul 12 18:10:53 2024 ] 
Training: Epoch [60/120], Step [2999], Loss: 0.06737986207008362, Training Accuracy: 92.9375
[ Fri Jul 12 18:10:53 2024 ] 	Batch(3000/6809) done. Loss: 0.1556  lr:0.000100
[ Fri Jul 12 18:11:12 2024 ] 	Batch(3100/6809) done. Loss: 0.5323  lr:0.000100
[ Fri Jul 12 18:11:30 2024 ] 	Batch(3200/6809) done. Loss: 0.2123  lr:0.000100
[ Fri Jul 12 18:11:48 2024 ] 	Batch(3300/6809) done. Loss: 0.2576  lr:0.000100
[ Fri Jul 12 18:12:06 2024 ] 	Batch(3400/6809) done. Loss: 0.0596  lr:0.000100
[ Fri Jul 12 18:12:24 2024 ] 
Training: Epoch [60/120], Step [3499], Loss: 0.2331027388572693, Training Accuracy: 93.09285714285714
[ Fri Jul 12 18:12:24 2024 ] 	Batch(3500/6809) done. Loss: 0.3045  lr:0.000100
[ Fri Jul 12 18:12:42 2024 ] 	Batch(3600/6809) done. Loss: 1.0826  lr:0.000100
[ Fri Jul 12 18:13:00 2024 ] 	Batch(3700/6809) done. Loss: 0.1937  lr:0.000100
[ Fri Jul 12 18:13:18 2024 ] 	Batch(3800/6809) done. Loss: 0.5898  lr:0.000100
[ Fri Jul 12 18:13:36 2024 ] 	Batch(3900/6809) done. Loss: 0.2002  lr:0.000100
[ Fri Jul 12 18:13:53 2024 ] 
Training: Epoch [60/120], Step [3999], Loss: 0.06792326271533966, Training Accuracy: 93.234375
[ Fri Jul 12 18:13:54 2024 ] 	Batch(4000/6809) done. Loss: 0.2276  lr:0.000100
[ Fri Jul 12 18:14:12 2024 ] 	Batch(4100/6809) done. Loss: 0.0342  lr:0.000100
[ Fri Jul 12 18:14:29 2024 ] 	Batch(4200/6809) done. Loss: 0.2934  lr:0.000100
[ Fri Jul 12 18:14:47 2024 ] 	Batch(4300/6809) done. Loss: 0.0333  lr:0.000100
[ Fri Jul 12 18:15:05 2024 ] 	Batch(4400/6809) done. Loss: 0.1019  lr:0.000100
[ Fri Jul 12 18:15:23 2024 ] 
Training: Epoch [60/120], Step [4499], Loss: 0.262002557516098, Training Accuracy: 93.39722222222223
[ Fri Jul 12 18:15:23 2024 ] 	Batch(4500/6809) done. Loss: 0.1290  lr:0.000100
[ Fri Jul 12 18:15:41 2024 ] 	Batch(4600/6809) done. Loss: 0.0439  lr:0.000100
[ Fri Jul 12 18:15:59 2024 ] 	Batch(4700/6809) done. Loss: 0.0103  lr:0.000100
[ Fri Jul 12 18:16:17 2024 ] 	Batch(4800/6809) done. Loss: 0.2127  lr:0.000100
[ Fri Jul 12 18:16:36 2024 ] 	Batch(4900/6809) done. Loss: 0.1614  lr:0.000100
[ Fri Jul 12 18:16:54 2024 ] 
Training: Epoch [60/120], Step [4999], Loss: 0.1920730173587799, Training Accuracy: 93.4675
[ Fri Jul 12 18:16:54 2024 ] 	Batch(5000/6809) done. Loss: 0.0684  lr:0.000100
[ Fri Jul 12 18:17:12 2024 ] 	Batch(5100/6809) done. Loss: 0.1199  lr:0.000100
[ Fri Jul 12 18:17:30 2024 ] 	Batch(5200/6809) done. Loss: 0.0865  lr:0.000100
[ Fri Jul 12 18:17:48 2024 ] 	Batch(5300/6809) done. Loss: 0.1902  lr:0.000100
[ Fri Jul 12 18:18:06 2024 ] 	Batch(5400/6809) done. Loss: 0.0598  lr:0.000100
[ Fri Jul 12 18:18:24 2024 ] 
Training: Epoch [60/120], Step [5499], Loss: 1.0107052326202393, Training Accuracy: 93.52954545454546
[ Fri Jul 12 18:18:24 2024 ] 	Batch(5500/6809) done. Loss: 0.0414  lr:0.000100
[ Fri Jul 12 18:18:42 2024 ] 	Batch(5600/6809) done. Loss: 0.1307  lr:0.000100
[ Fri Jul 12 18:19:00 2024 ] 	Batch(5700/6809) done. Loss: 0.4345  lr:0.000100
[ Fri Jul 12 18:19:17 2024 ] 	Batch(5800/6809) done. Loss: 0.0234  lr:0.000100
[ Fri Jul 12 18:19:35 2024 ] 	Batch(5900/6809) done. Loss: 0.0536  lr:0.000100
[ Fri Jul 12 18:19:53 2024 ] 
Training: Epoch [60/120], Step [5999], Loss: 0.00862156506627798, Training Accuracy: 93.59375
[ Fri Jul 12 18:19:53 2024 ] 	Batch(6000/6809) done. Loss: 0.0562  lr:0.000100
[ Fri Jul 12 18:20:11 2024 ] 	Batch(6100/6809) done. Loss: 0.0547  lr:0.000100
[ Fri Jul 12 18:20:29 2024 ] 	Batch(6200/6809) done. Loss: 0.2037  lr:0.000100
[ Fri Jul 12 18:20:47 2024 ] 	Batch(6300/6809) done. Loss: 0.1759  lr:0.000100
[ Fri Jul 12 18:21:05 2024 ] 	Batch(6400/6809) done. Loss: 0.2329  lr:0.000100
[ Fri Jul 12 18:21:23 2024 ] 
Training: Epoch [60/120], Step [6499], Loss: 0.09343693405389786, Training Accuracy: 93.63461538461539
[ Fri Jul 12 18:21:24 2024 ] 	Batch(6500/6809) done. Loss: 0.0368  lr:0.000100
[ Fri Jul 12 18:21:42 2024 ] 	Batch(6600/6809) done. Loss: 0.0148  lr:0.000100
[ Fri Jul 12 18:22:00 2024 ] 	Batch(6700/6809) done. Loss: 0.4325  lr:0.000100
[ Fri Jul 12 18:22:18 2024 ] 	Batch(6800/6809) done. Loss: 0.1607  lr:0.000100
[ Fri Jul 12 18:22:20 2024 ] 	Mean training loss: 0.2040.
[ Fri Jul 12 18:22:20 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 18:22:20 2024 ] Training epoch: 62
[ Fri Jul 12 18:22:20 2024 ] 	Batch(0/6809) done. Loss: 0.0603  lr:0.000100
[ Fri Jul 12 18:22:38 2024 ] 	Batch(100/6809) done. Loss: 0.1282  lr:0.000100
[ Fri Jul 12 18:22:56 2024 ] 	Batch(200/6809) done. Loss: 0.2474  lr:0.000100
[ Fri Jul 12 18:23:14 2024 ] 	Batch(300/6809) done. Loss: 0.0521  lr:0.000100
[ Fri Jul 12 18:23:32 2024 ] 	Batch(400/6809) done. Loss: 0.0301  lr:0.000100
[ Fri Jul 12 18:23:50 2024 ] 
Training: Epoch [61/120], Step [499], Loss: 0.12564049661159515, Training Accuracy: 95.35
[ Fri Jul 12 18:23:50 2024 ] 	Batch(500/6809) done. Loss: 0.0332  lr:0.000100
[ Fri Jul 12 18:24:08 2024 ] 	Batch(600/6809) done. Loss: 0.4002  lr:0.000100
[ Fri Jul 12 18:24:26 2024 ] 	Batch(700/6809) done. Loss: 0.2063  lr:0.000100
[ Fri Jul 12 18:24:44 2024 ] 	Batch(800/6809) done. Loss: 0.0635  lr:0.000100
[ Fri Jul 12 18:25:02 2024 ] 	Batch(900/6809) done. Loss: 0.0079  lr:0.000100
[ Fri Jul 12 18:25:19 2024 ] 
Training: Epoch [61/120], Step [999], Loss: 0.3980073630809784, Training Accuracy: 94.875
[ Fri Jul 12 18:25:20 2024 ] 	Batch(1000/6809) done. Loss: 0.0778  lr:0.000100
[ Fri Jul 12 18:25:37 2024 ] 	Batch(1100/6809) done. Loss: 0.0467  lr:0.000100
[ Fri Jul 12 18:25:56 2024 ] 	Batch(1200/6809) done. Loss: 0.2421  lr:0.000100
[ Fri Jul 12 18:26:14 2024 ] 	Batch(1300/6809) done. Loss: 0.2940  lr:0.000100
[ Fri Jul 12 18:26:33 2024 ] 	Batch(1400/6809) done. Loss: 0.7559  lr:0.000100
[ Fri Jul 12 18:26:51 2024 ] 
Training: Epoch [61/120], Step [1499], Loss: 0.23182281851768494, Training Accuracy: 94.91666666666667
[ Fri Jul 12 18:26:51 2024 ] 	Batch(1500/6809) done. Loss: 0.3336  lr:0.000100
[ Fri Jul 12 18:27:10 2024 ] 	Batch(1600/6809) done. Loss: 0.0764  lr:0.000100
[ Fri Jul 12 18:27:28 2024 ] 	Batch(1700/6809) done. Loss: 0.1516  lr:0.000100
[ Fri Jul 12 18:27:47 2024 ] 	Batch(1800/6809) done. Loss: 0.0635  lr:0.000100
[ Fri Jul 12 18:28:05 2024 ] 	Batch(1900/6809) done. Loss: 0.0169  lr:0.000100
[ Fri Jul 12 18:28:23 2024 ] 
Training: Epoch [61/120], Step [1999], Loss: 0.005202682688832283, Training Accuracy: 95.03125
[ Fri Jul 12 18:28:23 2024 ] 	Batch(2000/6809) done. Loss: 0.4893  lr:0.000100
[ Fri Jul 12 18:28:41 2024 ] 	Batch(2100/6809) done. Loss: 0.0582  lr:0.000100
[ Fri Jul 12 18:28:59 2024 ] 	Batch(2200/6809) done. Loss: 0.1280  lr:0.000100
[ Fri Jul 12 18:29:17 2024 ] 	Batch(2300/6809) done. Loss: 0.0431  lr:0.000100
[ Fri Jul 12 18:29:35 2024 ] 	Batch(2400/6809) done. Loss: 0.1227  lr:0.000100
[ Fri Jul 12 18:29:53 2024 ] 
Training: Epoch [61/120], Step [2499], Loss: 0.07591290771961212, Training Accuracy: 94.87
[ Fri Jul 12 18:29:53 2024 ] 	Batch(2500/6809) done. Loss: 0.2419  lr:0.000100
[ Fri Jul 12 18:30:11 2024 ] 	Batch(2600/6809) done. Loss: 0.2313  lr:0.000100
[ Fri Jul 12 18:30:28 2024 ] 	Batch(2700/6809) done. Loss: 0.3615  lr:0.000100
[ Fri Jul 12 18:30:47 2024 ] 	Batch(2800/6809) done. Loss: 0.0151  lr:0.000100
[ Fri Jul 12 18:31:04 2024 ] 	Batch(2900/6809) done. Loss: 0.1609  lr:0.000100
[ Fri Jul 12 18:31:22 2024 ] 
Training: Epoch [61/120], Step [2999], Loss: 0.1839582473039627, Training Accuracy: 94.89166666666667
[ Fri Jul 12 18:31:22 2024 ] 	Batch(3000/6809) done. Loss: 0.0577  lr:0.000100
[ Fri Jul 12 18:31:40 2024 ] 	Batch(3100/6809) done. Loss: 0.1316  lr:0.000100
[ Fri Jul 12 18:31:58 2024 ] 	Batch(3200/6809) done. Loss: 0.0899  lr:0.000100
[ Fri Jul 12 18:32:17 2024 ] 	Batch(3300/6809) done. Loss: 0.0984  lr:0.000100
[ Fri Jul 12 18:32:35 2024 ] 	Batch(3400/6809) done. Loss: 0.0693  lr:0.000100
[ Fri Jul 12 18:32:54 2024 ] 
Training: Epoch [61/120], Step [3499], Loss: 0.028559504076838493, Training Accuracy: 94.98214285714286
[ Fri Jul 12 18:32:54 2024 ] 	Batch(3500/6809) done. Loss: 0.0494  lr:0.000100
[ Fri Jul 12 18:33:12 2024 ] 	Batch(3600/6809) done. Loss: 0.0563  lr:0.000100
[ Fri Jul 12 18:33:30 2024 ] 	Batch(3700/6809) done. Loss: 0.0467  lr:0.000100
[ Fri Jul 12 18:33:49 2024 ] 	Batch(3800/6809) done. Loss: 0.1826  lr:0.000100
[ Fri Jul 12 18:34:07 2024 ] 	Batch(3900/6809) done. Loss: 0.0180  lr:0.000100
[ Fri Jul 12 18:34:25 2024 ] 
Training: Epoch [61/120], Step [3999], Loss: 0.0847231075167656, Training Accuracy: 94.91875
[ Fri Jul 12 18:34:26 2024 ] 	Batch(4000/6809) done. Loss: 0.0304  lr:0.000100
[ Fri Jul 12 18:34:44 2024 ] 	Batch(4100/6809) done. Loss: 0.0020  lr:0.000100
[ Fri Jul 12 18:35:03 2024 ] 	Batch(4200/6809) done. Loss: 0.0913  lr:0.000100
[ Fri Jul 12 18:35:21 2024 ] 	Batch(4300/6809) done. Loss: 0.4543  lr:0.000100
[ Fri Jul 12 18:35:39 2024 ] 	Batch(4400/6809) done. Loss: 0.1592  lr:0.000100
[ Fri Jul 12 18:35:57 2024 ] 
Training: Epoch [61/120], Step [4499], Loss: 0.015175403095781803, Training Accuracy: 94.97222222222223
[ Fri Jul 12 18:35:58 2024 ] 	Batch(4500/6809) done. Loss: 0.0268  lr:0.000100
[ Fri Jul 12 18:36:16 2024 ] 	Batch(4600/6809) done. Loss: 0.0173  lr:0.000100
[ Fri Jul 12 18:36:35 2024 ] 	Batch(4700/6809) done. Loss: 0.0729  lr:0.000100
[ Fri Jul 12 18:36:53 2024 ] 	Batch(4800/6809) done. Loss: 0.1895  lr:0.000100
[ Fri Jul 12 18:37:11 2024 ] 	Batch(4900/6809) done. Loss: 0.1027  lr:0.000100
[ Fri Jul 12 18:37:29 2024 ] 
Training: Epoch [61/120], Step [4999], Loss: 0.19795933365821838, Training Accuracy: 94.9675
[ Fri Jul 12 18:37:29 2024 ] 	Batch(5000/6809) done. Loss: 0.2712  lr:0.000100
[ Fri Jul 12 18:37:47 2024 ] 	Batch(5100/6809) done. Loss: 0.2992  lr:0.000100
[ Fri Jul 12 18:38:05 2024 ] 	Batch(5200/6809) done. Loss: 0.0864  lr:0.000100
[ Fri Jul 12 18:38:24 2024 ] 	Batch(5300/6809) done. Loss: 0.0459  lr:0.000100
[ Fri Jul 12 18:38:42 2024 ] 	Batch(5400/6809) done. Loss: 0.0824  lr:0.000100
[ Fri Jul 12 18:39:01 2024 ] 
Training: Epoch [61/120], Step [5499], Loss: 0.03769604489207268, Training Accuracy: 95.00454545454545
[ Fri Jul 12 18:39:01 2024 ] 	Batch(5500/6809) done. Loss: 0.0761  lr:0.000100
[ Fri Jul 12 18:39:19 2024 ] 	Batch(5600/6809) done. Loss: 0.0233  lr:0.000100
[ Fri Jul 12 18:39:37 2024 ] 	Batch(5700/6809) done. Loss: 0.2550  lr:0.000100
[ Fri Jul 12 18:39:55 2024 ] 	Batch(5800/6809) done. Loss: 0.0024  lr:0.000100
[ Fri Jul 12 18:40:13 2024 ] 	Batch(5900/6809) done. Loss: 0.1938  lr:0.000100
[ Fri Jul 12 18:40:31 2024 ] 
Training: Epoch [61/120], Step [5999], Loss: 0.29071447253227234, Training Accuracy: 95.04166666666667
[ Fri Jul 12 18:40:31 2024 ] 	Batch(6000/6809) done. Loss: 0.2662  lr:0.000100
[ Fri Jul 12 18:40:49 2024 ] 	Batch(6100/6809) done. Loss: 0.0097  lr:0.000100
[ Fri Jul 12 18:41:07 2024 ] 	Batch(6200/6809) done. Loss: 0.1103  lr:0.000100
[ Fri Jul 12 18:41:25 2024 ] 	Batch(6300/6809) done. Loss: 0.4873  lr:0.000100
[ Fri Jul 12 18:41:43 2024 ] 	Batch(6400/6809) done. Loss: 0.0570  lr:0.000100
[ Fri Jul 12 18:42:01 2024 ] 
Training: Epoch [61/120], Step [6499], Loss: 0.12692983448505402, Training Accuracy: 95.06923076923077
[ Fri Jul 12 18:42:01 2024 ] 	Batch(6500/6809) done. Loss: 0.5305  lr:0.000100
[ Fri Jul 12 18:42:19 2024 ] 	Batch(6600/6809) done. Loss: 0.1846  lr:0.000100
[ Fri Jul 12 18:42:38 2024 ] 	Batch(6700/6809) done. Loss: 0.0567  lr:0.000100
[ Fri Jul 12 18:42:56 2024 ] 	Batch(6800/6809) done. Loss: 0.0630  lr:0.000100
[ Fri Jul 12 18:42:57 2024 ] 	Mean training loss: 0.1711.
[ Fri Jul 12 18:42:57 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 18:42:58 2024 ] Training epoch: 63
[ Fri Jul 12 18:42:58 2024 ] 	Batch(0/6809) done. Loss: 0.0084  lr:0.000100
[ Fri Jul 12 18:43:16 2024 ] 	Batch(100/6809) done. Loss: 0.2464  lr:0.000100
[ Fri Jul 12 18:43:34 2024 ] 	Batch(200/6809) done. Loss: 0.0315  lr:0.000100
[ Fri Jul 12 18:43:52 2024 ] 	Batch(300/6809) done. Loss: 0.0321  lr:0.000100
[ Fri Jul 12 18:44:10 2024 ] 	Batch(400/6809) done. Loss: 0.1789  lr:0.000100
[ Fri Jul 12 18:44:28 2024 ] 
Training: Epoch [62/120], Step [499], Loss: 0.028508450835943222, Training Accuracy: 95.55
[ Fri Jul 12 18:44:28 2024 ] 	Batch(500/6809) done. Loss: 0.0526  lr:0.000100
[ Fri Jul 12 18:44:46 2024 ] 	Batch(600/6809) done. Loss: 0.1807  lr:0.000100
[ Fri Jul 12 18:45:04 2024 ] 	Batch(700/6809) done. Loss: 0.1209  lr:0.000100
[ Fri Jul 12 18:45:22 2024 ] 	Batch(800/6809) done. Loss: 0.1804  lr:0.000100
[ Fri Jul 12 18:45:40 2024 ] 	Batch(900/6809) done. Loss: 0.1176  lr:0.000100
[ Fri Jul 12 18:45:58 2024 ] 
Training: Epoch [62/120], Step [999], Loss: 0.06838896125555038, Training Accuracy: 95.72500000000001
[ Fri Jul 12 18:45:59 2024 ] 	Batch(1000/6809) done. Loss: 0.1788  lr:0.000100
[ Fri Jul 12 18:46:17 2024 ] 	Batch(1100/6809) done. Loss: 0.0259  lr:0.000100
[ Fri Jul 12 18:46:36 2024 ] 	Batch(1200/6809) done. Loss: 0.1182  lr:0.000100
[ Fri Jul 12 18:46:54 2024 ] 	Batch(1300/6809) done. Loss: 0.4677  lr:0.000100
[ Fri Jul 12 18:47:12 2024 ] 	Batch(1400/6809) done. Loss: 0.0717  lr:0.000100
[ Fri Jul 12 18:47:29 2024 ] 
Training: Epoch [62/120], Step [1499], Loss: 0.054305996745824814, Training Accuracy: 95.48333333333333
[ Fri Jul 12 18:47:30 2024 ] 	Batch(1500/6809) done. Loss: 0.0854  lr:0.000100
[ Fri Jul 12 18:47:48 2024 ] 	Batch(1600/6809) done. Loss: 0.0427  lr:0.000100
[ Fri Jul 12 18:48:05 2024 ] 	Batch(1700/6809) done. Loss: 0.0096  lr:0.000100
[ Fri Jul 12 18:48:23 2024 ] 	Batch(1800/6809) done. Loss: 0.3210  lr:0.000100
[ Fri Jul 12 18:48:41 2024 ] 	Batch(1900/6809) done. Loss: 0.3415  lr:0.000100
[ Fri Jul 12 18:48:59 2024 ] 
Training: Epoch [62/120], Step [1999], Loss: 0.008521826937794685, Training Accuracy: 95.46875
[ Fri Jul 12 18:49:00 2024 ] 	Batch(2000/6809) done. Loss: 0.0555  lr:0.000100
[ Fri Jul 12 18:49:18 2024 ] 	Batch(2100/6809) done. Loss: 0.0243  lr:0.000100
[ Fri Jul 12 18:49:35 2024 ] 	Batch(2200/6809) done. Loss: 0.0076  lr:0.000100
[ Fri Jul 12 18:49:53 2024 ] 	Batch(2300/6809) done. Loss: 0.2636  lr:0.000100
[ Fri Jul 12 18:50:11 2024 ] 	Batch(2400/6809) done. Loss: 0.2169  lr:0.000100
[ Fri Jul 12 18:50:29 2024 ] 
Training: Epoch [62/120], Step [2499], Loss: 0.0023448443971574306, Training Accuracy: 95.555
[ Fri Jul 12 18:50:29 2024 ] 	Batch(2500/6809) done. Loss: 0.2642  lr:0.000100
[ Fri Jul 12 18:50:47 2024 ] 	Batch(2600/6809) done. Loss: 0.0898  lr:0.000100
[ Fri Jul 12 18:51:05 2024 ] 	Batch(2700/6809) done. Loss: 0.1442  lr:0.000100
[ Fri Jul 12 18:51:23 2024 ] 	Batch(2800/6809) done. Loss: 0.1126  lr:0.000100
[ Fri Jul 12 18:51:41 2024 ] 	Batch(2900/6809) done. Loss: 0.0626  lr:0.000100
[ Fri Jul 12 18:51:59 2024 ] 
Training: Epoch [62/120], Step [2999], Loss: 0.16998350620269775, Training Accuracy: 95.47083333333333
[ Fri Jul 12 18:51:59 2024 ] 	Batch(3000/6809) done. Loss: 0.0566  lr:0.000100
[ Fri Jul 12 18:52:17 2024 ] 	Batch(3100/6809) done. Loss: 0.1369  lr:0.000100
[ Fri Jul 12 18:52:35 2024 ] 	Batch(3200/6809) done. Loss: 0.0147  lr:0.000100
[ Fri Jul 12 18:52:53 2024 ] 	Batch(3300/6809) done. Loss: 0.0755  lr:0.000100
[ Fri Jul 12 18:53:11 2024 ] 	Batch(3400/6809) done. Loss: 0.0297  lr:0.000100
[ Fri Jul 12 18:53:29 2024 ] 
Training: Epoch [62/120], Step [3499], Loss: 0.011072924360632896, Training Accuracy: 95.41785714285714
[ Fri Jul 12 18:53:29 2024 ] 	Batch(3500/6809) done. Loss: 0.2206  lr:0.000100
[ Fri Jul 12 18:53:47 2024 ] 	Batch(3600/6809) done. Loss: 0.0159  lr:0.000100
[ Fri Jul 12 18:54:05 2024 ] 	Batch(3700/6809) done. Loss: 0.0199  lr:0.000100
[ Fri Jul 12 18:54:23 2024 ] 	Batch(3800/6809) done. Loss: 0.0477  lr:0.000100
[ Fri Jul 12 18:54:41 2024 ] 	Batch(3900/6809) done. Loss: 0.0122  lr:0.000100
[ Fri Jul 12 18:54:58 2024 ] 
Training: Epoch [62/120], Step [3999], Loss: 0.13148808479309082, Training Accuracy: 95.44375000000001
[ Fri Jul 12 18:54:59 2024 ] 	Batch(4000/6809) done. Loss: 0.5614  lr:0.000100
[ Fri Jul 12 18:55:17 2024 ] 	Batch(4100/6809) done. Loss: 0.3370  lr:0.000100
[ Fri Jul 12 18:55:34 2024 ] 	Batch(4200/6809) done. Loss: 0.0275  lr:0.000100
[ Fri Jul 12 18:55:52 2024 ] 	Batch(4300/6809) done. Loss: 0.0614  lr:0.000100
[ Fri Jul 12 18:56:10 2024 ] 	Batch(4400/6809) done. Loss: 0.0290  lr:0.000100
[ Fri Jul 12 18:56:29 2024 ] 
Training: Epoch [62/120], Step [4499], Loss: 0.3971811830997467, Training Accuracy: 95.45
[ Fri Jul 12 18:56:29 2024 ] 	Batch(4500/6809) done. Loss: 0.7235  lr:0.000100
[ Fri Jul 12 18:56:47 2024 ] 	Batch(4600/6809) done. Loss: 0.2640  lr:0.000100
[ Fri Jul 12 18:57:05 2024 ] 	Batch(4700/6809) done. Loss: 0.0143  lr:0.000100
[ Fri Jul 12 18:57:23 2024 ] 	Batch(4800/6809) done. Loss: 0.7922  lr:0.000100
[ Fri Jul 12 18:57:41 2024 ] 	Batch(4900/6809) done. Loss: 0.0308  lr:0.000100
[ Fri Jul 12 18:57:59 2024 ] 
Training: Epoch [62/120], Step [4999], Loss: 0.01384760532528162, Training Accuracy: 95.4425
[ Fri Jul 12 18:57:59 2024 ] 	Batch(5000/6809) done. Loss: 0.1041  lr:0.000100
[ Fri Jul 12 18:58:17 2024 ] 	Batch(5100/6809) done. Loss: 0.0041  lr:0.000100
[ Fri Jul 12 18:58:35 2024 ] 	Batch(5200/6809) done. Loss: 0.2802  lr:0.000100
[ Fri Jul 12 18:58:53 2024 ] 	Batch(5300/6809) done. Loss: 0.3199  lr:0.000100
[ Fri Jul 12 18:59:11 2024 ] 	Batch(5400/6809) done. Loss: 0.2241  lr:0.000100
[ Fri Jul 12 18:59:29 2024 ] 
Training: Epoch [62/120], Step [5499], Loss: 0.011887948960065842, Training Accuracy: 95.45
[ Fri Jul 12 18:59:29 2024 ] 	Batch(5500/6809) done. Loss: 0.3287  lr:0.000100
[ Fri Jul 12 18:59:47 2024 ] 	Batch(5600/6809) done. Loss: 0.0249  lr:0.000100
[ Fri Jul 12 19:00:05 2024 ] 	Batch(5700/6809) done. Loss: 0.0119  lr:0.000100
[ Fri Jul 12 19:00:23 2024 ] 	Batch(5800/6809) done. Loss: 0.4518  lr:0.000100
[ Fri Jul 12 19:00:40 2024 ] 	Batch(5900/6809) done. Loss: 0.2635  lr:0.000100
[ Fri Jul 12 19:00:58 2024 ] 
Training: Epoch [62/120], Step [5999], Loss: 0.02691991999745369, Training Accuracy: 95.49375
[ Fri Jul 12 19:00:58 2024 ] 	Batch(6000/6809) done. Loss: 0.1966  lr:0.000100
[ Fri Jul 12 19:01:16 2024 ] 	Batch(6100/6809) done. Loss: 0.1368  lr:0.000100
[ Fri Jul 12 19:01:34 2024 ] 	Batch(6200/6809) done. Loss: 0.3775  lr:0.000100
[ Fri Jul 12 19:01:52 2024 ] 	Batch(6300/6809) done. Loss: 0.0054  lr:0.000100
[ Fri Jul 12 19:02:10 2024 ] 	Batch(6400/6809) done. Loss: 0.1717  lr:0.000100
[ Fri Jul 12 19:02:28 2024 ] 
Training: Epoch [62/120], Step [6499], Loss: 0.0806538537144661, Training Accuracy: 95.47884615384615
[ Fri Jul 12 19:02:29 2024 ] 	Batch(6500/6809) done. Loss: 0.0281  lr:0.000100
[ Fri Jul 12 19:02:47 2024 ] 	Batch(6600/6809) done. Loss: 0.2396  lr:0.000100
[ Fri Jul 12 19:03:05 2024 ] 	Batch(6700/6809) done. Loss: 0.2442  lr:0.000100
[ Fri Jul 12 19:03:23 2024 ] 	Batch(6800/6809) done. Loss: 0.0209  lr:0.000100
[ Fri Jul 12 19:03:25 2024 ] 	Mean training loss: 0.1599.
[ Fri Jul 12 19:03:25 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 19:03:25 2024 ] Training epoch: 64
[ Fri Jul 12 19:03:25 2024 ] 	Batch(0/6809) done. Loss: 0.0275  lr:0.000100
[ Fri Jul 12 19:03:43 2024 ] 	Batch(100/6809) done. Loss: 0.1696  lr:0.000100
[ Fri Jul 12 19:04:01 2024 ] 	Batch(200/6809) done. Loss: 0.0331  lr:0.000100
[ Fri Jul 12 19:04:19 2024 ] 	Batch(300/6809) done. Loss: 0.0284  lr:0.000100
[ Fri Jul 12 19:04:37 2024 ] 	Batch(400/6809) done. Loss: 0.1092  lr:0.000100
[ Fri Jul 12 19:04:55 2024 ] 
Training: Epoch [63/120], Step [499], Loss: 0.16125109791755676, Training Accuracy: 95.475
[ Fri Jul 12 19:04:55 2024 ] 	Batch(500/6809) done. Loss: 0.0780  lr:0.000100
[ Fri Jul 12 19:05:13 2024 ] 	Batch(600/6809) done. Loss: 0.1788  lr:0.000100
[ Fri Jul 12 19:05:31 2024 ] 	Batch(700/6809) done. Loss: 0.0139  lr:0.000100
[ Fri Jul 12 19:05:49 2024 ] 	Batch(800/6809) done. Loss: 0.0601  lr:0.000100
[ Fri Jul 12 19:06:07 2024 ] 	Batch(900/6809) done. Loss: 0.0456  lr:0.000100
[ Fri Jul 12 19:06:24 2024 ] 
Training: Epoch [63/120], Step [999], Loss: 0.2623960077762604, Training Accuracy: 95.775
[ Fri Jul 12 19:06:24 2024 ] 	Batch(1000/6809) done. Loss: 0.0144  lr:0.000100
[ Fri Jul 12 19:06:42 2024 ] 	Batch(1100/6809) done. Loss: 0.0755  lr:0.000100
[ Fri Jul 12 19:07:01 2024 ] 	Batch(1200/6809) done. Loss: 0.0851  lr:0.000100
[ Fri Jul 12 19:07:18 2024 ] 	Batch(1300/6809) done. Loss: 0.4971  lr:0.000100
[ Fri Jul 12 19:07:36 2024 ] 	Batch(1400/6809) done. Loss: 0.2007  lr:0.000100
[ Fri Jul 12 19:07:54 2024 ] 
Training: Epoch [63/120], Step [1499], Loss: 0.15730008482933044, Training Accuracy: 95.76666666666667
[ Fri Jul 12 19:07:54 2024 ] 	Batch(1500/6809) done. Loss: 0.0125  lr:0.000100
[ Fri Jul 12 19:08:12 2024 ] 	Batch(1600/6809) done. Loss: 0.0184  lr:0.000100
[ Fri Jul 12 19:08:30 2024 ] 	Batch(1700/6809) done. Loss: 0.3660  lr:0.000100
[ Fri Jul 12 19:08:48 2024 ] 	Batch(1800/6809) done. Loss: 0.2752  lr:0.000100
[ Fri Jul 12 19:09:06 2024 ] 	Batch(1900/6809) done. Loss: 0.0361  lr:0.000100
[ Fri Jul 12 19:09:24 2024 ] 
Training: Epoch [63/120], Step [1999], Loss: 0.23404641449451447, Training Accuracy: 95.60625
[ Fri Jul 12 19:09:24 2024 ] 	Batch(2000/6809) done. Loss: 0.0062  lr:0.000100
[ Fri Jul 12 19:09:42 2024 ] 	Batch(2100/6809) done. Loss: 0.2468  lr:0.000100
[ Fri Jul 12 19:10:00 2024 ] 	Batch(2200/6809) done. Loss: 0.2051  lr:0.000100
[ Fri Jul 12 19:10:17 2024 ] 	Batch(2300/6809) done. Loss: 0.0087  lr:0.000100
[ Fri Jul 12 19:10:35 2024 ] 	Batch(2400/6809) done. Loss: 0.1141  lr:0.000100
[ Fri Jul 12 19:10:53 2024 ] 
Training: Epoch [63/120], Step [2499], Loss: 0.34389641880989075, Training Accuracy: 95.645
[ Fri Jul 12 19:10:53 2024 ] 	Batch(2500/6809) done. Loss: 0.1450  lr:0.000100
[ Fri Jul 12 19:11:11 2024 ] 	Batch(2600/6809) done. Loss: 0.1454  lr:0.000100
[ Fri Jul 12 19:11:29 2024 ] 	Batch(2700/6809) done. Loss: 0.0776  lr:0.000100
[ Fri Jul 12 19:11:47 2024 ] 	Batch(2800/6809) done. Loss: 0.0489  lr:0.000100
[ Fri Jul 12 19:12:05 2024 ] 	Batch(2900/6809) done. Loss: 0.0795  lr:0.000100
[ Fri Jul 12 19:12:23 2024 ] 
Training: Epoch [63/120], Step [2999], Loss: 0.14840933680534363, Training Accuracy: 95.62916666666666
[ Fri Jul 12 19:12:23 2024 ] 	Batch(3000/6809) done. Loss: 0.0680  lr:0.000100
[ Fri Jul 12 19:12:41 2024 ] 	Batch(3100/6809) done. Loss: 0.0626  lr:0.000100
[ Fri Jul 12 19:12:59 2024 ] 	Batch(3200/6809) done. Loss: 0.0048  lr:0.000100
[ Fri Jul 12 19:13:17 2024 ] 	Batch(3300/6809) done. Loss: 0.2591  lr:0.000100
[ Fri Jul 12 19:13:35 2024 ] 	Batch(3400/6809) done. Loss: 0.1288  lr:0.000100
[ Fri Jul 12 19:13:52 2024 ] 
Training: Epoch [63/120], Step [3499], Loss: 0.03152891993522644, Training Accuracy: 95.75357142857143
[ Fri Jul 12 19:13:53 2024 ] 	Batch(3500/6809) done. Loss: 0.0913  lr:0.000100
[ Fri Jul 12 19:14:11 2024 ] 	Batch(3600/6809) done. Loss: 0.0430  lr:0.000100
[ Fri Jul 12 19:14:28 2024 ] 	Batch(3700/6809) done. Loss: 0.0228  lr:0.000100
[ Fri Jul 12 19:14:46 2024 ] 	Batch(3800/6809) done. Loss: 0.1013  lr:0.000100
[ Fri Jul 12 19:15:04 2024 ] 	Batch(3900/6809) done. Loss: 0.1202  lr:0.000100
[ Fri Jul 12 19:15:22 2024 ] 
Training: Epoch [63/120], Step [3999], Loss: 0.07530196011066437, Training Accuracy: 95.8
[ Fri Jul 12 19:15:22 2024 ] 	Batch(4000/6809) done. Loss: 0.0694  lr:0.000100
[ Fri Jul 12 19:15:40 2024 ] 	Batch(4100/6809) done. Loss: 0.1473  lr:0.000100
[ Fri Jul 12 19:15:58 2024 ] 	Batch(4200/6809) done. Loss: 0.0741  lr:0.000100
[ Fri Jul 12 19:16:16 2024 ] 	Batch(4300/6809) done. Loss: 0.0477  lr:0.000100
[ Fri Jul 12 19:16:34 2024 ] 	Batch(4400/6809) done. Loss: 0.0035  lr:0.000100
[ Fri Jul 12 19:16:52 2024 ] 
Training: Epoch [63/120], Step [4499], Loss: 0.2343316674232483, Training Accuracy: 95.81111111111112
[ Fri Jul 12 19:16:52 2024 ] 	Batch(4500/6809) done. Loss: 0.0964  lr:0.000100
[ Fri Jul 12 19:17:11 2024 ] 	Batch(4600/6809) done. Loss: 0.2931  lr:0.000100
[ Fri Jul 12 19:17:29 2024 ] 	Batch(4700/6809) done. Loss: 0.2857  lr:0.000100
[ Fri Jul 12 19:17:47 2024 ] 	Batch(4800/6809) done. Loss: 0.2449  lr:0.000100
[ Fri Jul 12 19:18:05 2024 ] 	Batch(4900/6809) done. Loss: 0.0408  lr:0.000100
[ Fri Jul 12 19:18:23 2024 ] 
Training: Epoch [63/120], Step [4999], Loss: 0.037069305777549744, Training Accuracy: 95.78750000000001
[ Fri Jul 12 19:18:23 2024 ] 	Batch(5000/6809) done. Loss: 0.0317  lr:0.000100
[ Fri Jul 12 19:18:41 2024 ] 	Batch(5100/6809) done. Loss: 0.0915  lr:0.000100
[ Fri Jul 12 19:18:59 2024 ] 	Batch(5200/6809) done. Loss: 0.1811  lr:0.000100
[ Fri Jul 12 19:19:17 2024 ] 	Batch(5300/6809) done. Loss: 0.1347  lr:0.000100
[ Fri Jul 12 19:19:34 2024 ] 	Batch(5400/6809) done. Loss: 0.0969  lr:0.000100
[ Fri Jul 12 19:19:52 2024 ] 
Training: Epoch [63/120], Step [5499], Loss: 0.36303192377090454, Training Accuracy: 95.81136363636364
[ Fri Jul 12 19:19:52 2024 ] 	Batch(5500/6809) done. Loss: 0.0208  lr:0.000100
[ Fri Jul 12 19:20:10 2024 ] 	Batch(5600/6809) done. Loss: 0.3735  lr:0.000100
[ Fri Jul 12 19:20:29 2024 ] 	Batch(5700/6809) done. Loss: 0.0616  lr:0.000100
[ Fri Jul 12 19:20:47 2024 ] 	Batch(5800/6809) done. Loss: 0.0823  lr:0.000100
[ Fri Jul 12 19:21:06 2024 ] 	Batch(5900/6809) done. Loss: 0.0916  lr:0.000100
[ Fri Jul 12 19:21:24 2024 ] 
Training: Epoch [63/120], Step [5999], Loss: 0.019094470888376236, Training Accuracy: 95.80208333333333
[ Fri Jul 12 19:21:24 2024 ] 	Batch(6000/6809) done. Loss: 0.0181  lr:0.000100
[ Fri Jul 12 19:21:42 2024 ] 	Batch(6100/6809) done. Loss: 0.1342  lr:0.000100
[ Fri Jul 12 19:22:00 2024 ] 	Batch(6200/6809) done. Loss: 0.0615  lr:0.000100
[ Fri Jul 12 19:22:17 2024 ] 	Batch(6300/6809) done. Loss: 0.0705  lr:0.000100
[ Fri Jul 12 19:22:35 2024 ] 	Batch(6400/6809) done. Loss: 0.1232  lr:0.000100
[ Fri Jul 12 19:22:53 2024 ] 
Training: Epoch [63/120], Step [6499], Loss: 0.027346497401595116, Training Accuracy: 95.80192307692307
[ Fri Jul 12 19:22:53 2024 ] 	Batch(6500/6809) done. Loss: 0.0998  lr:0.000100
[ Fri Jul 12 19:23:11 2024 ] 	Batch(6600/6809) done. Loss: 0.1764  lr:0.000100
[ Fri Jul 12 19:23:29 2024 ] 	Batch(6700/6809) done. Loss: 0.0112  lr:0.000100
[ Fri Jul 12 19:23:47 2024 ] 	Batch(6800/6809) done. Loss: 0.1929  lr:0.000100
[ Fri Jul 12 19:23:49 2024 ] 	Mean training loss: 0.1524.
[ Fri Jul 12 19:23:49 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 19:23:49 2024 ] Training epoch: 65
[ Fri Jul 12 19:23:49 2024 ] 	Batch(0/6809) done. Loss: 0.6446  lr:0.000100
[ Fri Jul 12 19:24:07 2024 ] 	Batch(100/6809) done. Loss: 0.3082  lr:0.000100
[ Fri Jul 12 19:24:25 2024 ] 	Batch(200/6809) done. Loss: 0.2423  lr:0.000100
[ Fri Jul 12 19:24:43 2024 ] 	Batch(300/6809) done. Loss: 0.0958  lr:0.000100
[ Fri Jul 12 19:25:01 2024 ] 	Batch(400/6809) done. Loss: 0.0297  lr:0.000100
[ Fri Jul 12 19:25:18 2024 ] 
Training: Epoch [64/120], Step [499], Loss: 0.03590982407331467, Training Accuracy: 95.625
[ Fri Jul 12 19:25:19 2024 ] 	Batch(500/6809) done. Loss: 0.0022  lr:0.000100
[ Fri Jul 12 19:25:36 2024 ] 	Batch(600/6809) done. Loss: 0.1037  lr:0.000100
[ Fri Jul 12 19:25:54 2024 ] 	Batch(700/6809) done. Loss: 0.0040  lr:0.000100
[ Fri Jul 12 19:26:12 2024 ] 	Batch(800/6809) done. Loss: 0.0022  lr:0.000100
[ Fri Jul 12 19:26:31 2024 ] 	Batch(900/6809) done. Loss: 0.0908  lr:0.000100
[ Fri Jul 12 19:26:49 2024 ] 
Training: Epoch [64/120], Step [999], Loss: 0.042952761054039, Training Accuracy: 95.8625
[ Fri Jul 12 19:26:49 2024 ] 	Batch(1000/6809) done. Loss: 0.0221  lr:0.000100
[ Fri Jul 12 19:27:07 2024 ] 	Batch(1100/6809) done. Loss: 0.0366  lr:0.000100
[ Fri Jul 12 19:27:25 2024 ] 	Batch(1200/6809) done. Loss: 0.3104  lr:0.000100
[ Fri Jul 12 19:27:43 2024 ] 	Batch(1300/6809) done. Loss: 0.0479  lr:0.000100
[ Fri Jul 12 19:28:01 2024 ] 	Batch(1400/6809) done. Loss: 0.0634  lr:0.000100
[ Fri Jul 12 19:28:19 2024 ] 
Training: Epoch [64/120], Step [1499], Loss: 0.02691526710987091, Training Accuracy: 95.93333333333334
[ Fri Jul 12 19:28:19 2024 ] 	Batch(1500/6809) done. Loss: 0.0640  lr:0.000100
[ Fri Jul 12 19:28:37 2024 ] 	Batch(1600/6809) done. Loss: 0.0905  lr:0.000100
[ Fri Jul 12 19:28:55 2024 ] 	Batch(1700/6809) done. Loss: 0.1513  lr:0.000100
[ Fri Jul 12 19:29:13 2024 ] 	Batch(1800/6809) done. Loss: 0.0425  lr:0.000100
[ Fri Jul 12 19:29:30 2024 ] 	Batch(1900/6809) done. Loss: 0.2780  lr:0.000100
[ Fri Jul 12 19:29:48 2024 ] 
Training: Epoch [64/120], Step [1999], Loss: 0.046319421380758286, Training Accuracy: 96.0625
[ Fri Jul 12 19:29:48 2024 ] 	Batch(2000/6809) done. Loss: 0.1084  lr:0.000100
[ Fri Jul 12 19:30:06 2024 ] 	Batch(2100/6809) done. Loss: 0.0321  lr:0.000100
[ Fri Jul 12 19:30:24 2024 ] 	Batch(2200/6809) done. Loss: 0.0051  lr:0.000100
[ Fri Jul 12 19:30:42 2024 ] 	Batch(2300/6809) done. Loss: 0.2762  lr:0.000100
[ Fri Jul 12 19:31:00 2024 ] 	Batch(2400/6809) done. Loss: 0.0172  lr:0.000100
[ Fri Jul 12 19:31:18 2024 ] 
Training: Epoch [64/120], Step [2499], Loss: 0.26518285274505615, Training Accuracy: 96.10499999999999
[ Fri Jul 12 19:31:19 2024 ] 	Batch(2500/6809) done. Loss: 0.0347  lr:0.000100
[ Fri Jul 12 19:31:37 2024 ] 	Batch(2600/6809) done. Loss: 0.0742  lr:0.000100
[ Fri Jul 12 19:31:55 2024 ] 	Batch(2700/6809) done. Loss: 0.0064  lr:0.000100
[ Fri Jul 12 19:32:13 2024 ] 	Batch(2800/6809) done. Loss: 0.0926  lr:0.000100
[ Fri Jul 12 19:32:31 2024 ] 	Batch(2900/6809) done. Loss: 0.1671  lr:0.000100
[ Fri Jul 12 19:32:49 2024 ] 
Training: Epoch [64/120], Step [2999], Loss: 0.06104842573404312, Training Accuracy: 96.25
[ Fri Jul 12 19:32:49 2024 ] 	Batch(3000/6809) done. Loss: 0.2589  lr:0.000100
[ Fri Jul 12 19:33:07 2024 ] 	Batch(3100/6809) done. Loss: 0.0366  lr:0.000100
[ Fri Jul 12 19:33:25 2024 ] 	Batch(3200/6809) done. Loss: 0.0877  lr:0.000100
[ Fri Jul 12 19:33:43 2024 ] 	Batch(3300/6809) done. Loss: 0.3367  lr:0.000100
[ Fri Jul 12 19:34:01 2024 ] 	Batch(3400/6809) done. Loss: 0.0227  lr:0.000100
[ Fri Jul 12 19:34:18 2024 ] 
Training: Epoch [64/120], Step [3499], Loss: 0.14412938058376312, Training Accuracy: 96.12142857142857
[ Fri Jul 12 19:34:18 2024 ] 	Batch(3500/6809) done. Loss: 0.2114  lr:0.000100
[ Fri Jul 12 19:34:36 2024 ] 	Batch(3600/6809) done. Loss: 0.1314  lr:0.000100
[ Fri Jul 12 19:34:54 2024 ] 	Batch(3700/6809) done. Loss: 0.3777  lr:0.000100
[ Fri Jul 12 19:35:12 2024 ] 	Batch(3800/6809) done. Loss: 0.1874  lr:0.000100
[ Fri Jul 12 19:35:30 2024 ] 	Batch(3900/6809) done. Loss: 0.2008  lr:0.000100
[ Fri Jul 12 19:35:48 2024 ] 
Training: Epoch [64/120], Step [3999], Loss: 0.017481930553913116, Training Accuracy: 96.109375
[ Fri Jul 12 19:35:48 2024 ] 	Batch(4000/6809) done. Loss: 0.0328  lr:0.000100
[ Fri Jul 12 19:36:07 2024 ] 	Batch(4100/6809) done. Loss: 0.5479  lr:0.000100
[ Fri Jul 12 19:36:25 2024 ] 	Batch(4200/6809) done. Loss: 0.1802  lr:0.000100
[ Fri Jul 12 19:36:44 2024 ] 	Batch(4300/6809) done. Loss: 0.2832  lr:0.000100
[ Fri Jul 12 19:37:02 2024 ] 	Batch(4400/6809) done. Loss: 0.0314  lr:0.000100
[ Fri Jul 12 19:37:19 2024 ] 
Training: Epoch [64/120], Step [4499], Loss: 0.4500102698802948, Training Accuracy: 96.11666666666666
[ Fri Jul 12 19:37:20 2024 ] 	Batch(4500/6809) done. Loss: 0.0436  lr:0.000100
[ Fri Jul 12 19:37:37 2024 ] 	Batch(4600/6809) done. Loss: 0.0997  lr:0.000100
[ Fri Jul 12 19:37:55 2024 ] 	Batch(4700/6809) done. Loss: 0.0177  lr:0.000100
[ Fri Jul 12 19:38:13 2024 ] 	Batch(4800/6809) done. Loss: 0.0415  lr:0.000100
[ Fri Jul 12 19:38:32 2024 ] 	Batch(4900/6809) done. Loss: 0.0196  lr:0.000100
[ Fri Jul 12 19:38:50 2024 ] 
Training: Epoch [64/120], Step [4999], Loss: 0.042783595621585846, Training Accuracy: 96.115
[ Fri Jul 12 19:38:50 2024 ] 	Batch(5000/6809) done. Loss: 0.0064  lr:0.000100
[ Fri Jul 12 19:39:09 2024 ] 	Batch(5100/6809) done. Loss: 0.1675  lr:0.000100
[ Fri Jul 12 19:39:28 2024 ] 	Batch(5200/6809) done. Loss: 0.0172  lr:0.000100
[ Fri Jul 12 19:39:46 2024 ] 	Batch(5300/6809) done. Loss: 0.0172  lr:0.000100
[ Fri Jul 12 19:40:03 2024 ] 	Batch(5400/6809) done. Loss: 0.0656  lr:0.000100
[ Fri Jul 12 19:40:21 2024 ] 
Training: Epoch [64/120], Step [5499], Loss: 0.0064090583473443985, Training Accuracy: 96.10681818181817
[ Fri Jul 12 19:40:21 2024 ] 	Batch(5500/6809) done. Loss: 0.0424  lr:0.000100
[ Fri Jul 12 19:40:39 2024 ] 	Batch(5600/6809) done. Loss: 0.2238  lr:0.000100
[ Fri Jul 12 19:40:58 2024 ] 	Batch(5700/6809) done. Loss: 0.4977  lr:0.000100
[ Fri Jul 12 19:41:16 2024 ] 	Batch(5800/6809) done. Loss: 0.0532  lr:0.000100
[ Fri Jul 12 19:41:35 2024 ] 	Batch(5900/6809) done. Loss: 0.1138  lr:0.000100
[ Fri Jul 12 19:41:53 2024 ] 
Training: Epoch [64/120], Step [5999], Loss: 0.02571711130440235, Training Accuracy: 96.1
[ Fri Jul 12 19:41:54 2024 ] 	Batch(6000/6809) done. Loss: 0.0521  lr:0.000100
[ Fri Jul 12 19:42:11 2024 ] 	Batch(6100/6809) done. Loss: 0.0600  lr:0.000100
[ Fri Jul 12 19:42:29 2024 ] 	Batch(6200/6809) done. Loss: 0.0095  lr:0.000100
[ Fri Jul 12 19:42:47 2024 ] 	Batch(6300/6809) done. Loss: 0.0652  lr:0.000100
[ Fri Jul 12 19:43:05 2024 ] 	Batch(6400/6809) done. Loss: 0.0943  lr:0.000100
[ Fri Jul 12 19:43:24 2024 ] 
Training: Epoch [64/120], Step [6499], Loss: 0.01756187528371811, Training Accuracy: 96.10192307692309
[ Fri Jul 12 19:43:24 2024 ] 	Batch(6500/6809) done. Loss: 0.0417  lr:0.000100
[ Fri Jul 12 19:43:42 2024 ] 	Batch(6600/6809) done. Loss: 0.0564  lr:0.000100
[ Fri Jul 12 19:44:01 2024 ] 	Batch(6700/6809) done. Loss: 0.3155  lr:0.000100
[ Fri Jul 12 19:44:20 2024 ] 	Batch(6800/6809) done. Loss: 0.0114  lr:0.000100
[ Fri Jul 12 19:44:21 2024 ] 	Mean training loss: 0.1397.
[ Fri Jul 12 19:44:21 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 19:44:21 2024 ] Training epoch: 66
[ Fri Jul 12 19:44:22 2024 ] 	Batch(0/6809) done. Loss: 0.0030  lr:0.000100
[ Fri Jul 12 19:44:40 2024 ] 	Batch(100/6809) done. Loss: 0.0320  lr:0.000100
[ Fri Jul 12 19:44:57 2024 ] 	Batch(200/6809) done. Loss: 0.0033  lr:0.000100
[ Fri Jul 12 19:45:16 2024 ] 	Batch(300/6809) done. Loss: 0.0979  lr:0.000100
[ Fri Jul 12 19:45:34 2024 ] 	Batch(400/6809) done. Loss: 0.0046  lr:0.000100
[ Fri Jul 12 19:45:53 2024 ] 
Training: Epoch [65/120], Step [499], Loss: 0.12767434120178223, Training Accuracy: 96.525
[ Fri Jul 12 19:45:53 2024 ] 	Batch(500/6809) done. Loss: 0.3780  lr:0.000100
[ Fri Jul 12 19:46:11 2024 ] 	Batch(600/6809) done. Loss: 0.1418  lr:0.000100
[ Fri Jul 12 19:46:30 2024 ] 	Batch(700/6809) done. Loss: 0.0305  lr:0.000100
[ Fri Jul 12 19:46:48 2024 ] 	Batch(800/6809) done. Loss: 0.5415  lr:0.000100
[ Fri Jul 12 19:47:06 2024 ] 	Batch(900/6809) done. Loss: 0.7011  lr:0.000100
[ Fri Jul 12 19:47:23 2024 ] 
Training: Epoch [65/120], Step [999], Loss: 0.14607033133506775, Training Accuracy: 96.39999999999999
[ Fri Jul 12 19:47:23 2024 ] 	Batch(1000/6809) done. Loss: 0.0849  lr:0.000100
[ Fri Jul 12 19:47:41 2024 ] 	Batch(1100/6809) done. Loss: 0.1103  lr:0.000100
[ Fri Jul 12 19:47:59 2024 ] 	Batch(1200/6809) done. Loss: 0.0050  lr:0.000100
[ Fri Jul 12 19:48:17 2024 ] 	Batch(1300/6809) done. Loss: 0.1426  lr:0.000100
[ Fri Jul 12 19:48:35 2024 ] 	Batch(1400/6809) done. Loss: 0.0061  lr:0.000100
[ Fri Jul 12 19:48:53 2024 ] 
Training: Epoch [65/120], Step [1499], Loss: 0.3696106970310211, Training Accuracy: 96.35833333333333
[ Fri Jul 12 19:48:53 2024 ] 	Batch(1500/6809) done. Loss: 0.1137  lr:0.000100
[ Fri Jul 12 19:49:12 2024 ] 	Batch(1600/6809) done. Loss: 0.0224  lr:0.000100
[ Fri Jul 12 19:49:31 2024 ] 	Batch(1700/6809) done. Loss: 0.0044  lr:0.000100
[ Fri Jul 12 19:49:49 2024 ] 	Batch(1800/6809) done. Loss: 0.2533  lr:0.000100
[ Fri Jul 12 19:50:08 2024 ] 	Batch(1900/6809) done. Loss: 0.0703  lr:0.000100
[ Fri Jul 12 19:50:26 2024 ] 
Training: Epoch [65/120], Step [1999], Loss: 0.08387377858161926, Training Accuracy: 96.3125
[ Fri Jul 12 19:50:26 2024 ] 	Batch(2000/6809) done. Loss: 0.0889  lr:0.000100
[ Fri Jul 12 19:50:45 2024 ] 	Batch(2100/6809) done. Loss: 0.0566  lr:0.000100
[ Fri Jul 12 19:51:03 2024 ] 	Batch(2200/6809) done. Loss: 0.0160  lr:0.000100
[ Fri Jul 12 19:51:21 2024 ] 	Batch(2300/6809) done. Loss: 0.2502  lr:0.000100
[ Fri Jul 12 19:51:39 2024 ] 	Batch(2400/6809) done. Loss: 0.0190  lr:0.000100
[ Fri Jul 12 19:51:57 2024 ] 
Training: Epoch [65/120], Step [2499], Loss: 0.07380092889070511, Training Accuracy: 96.265
[ Fri Jul 12 19:51:57 2024 ] 	Batch(2500/6809) done. Loss: 0.1235  lr:0.000100
[ Fri Jul 12 19:52:15 2024 ] 	Batch(2600/6809) done. Loss: 0.0435  lr:0.000100
[ Fri Jul 12 19:52:33 2024 ] 	Batch(2700/6809) done. Loss: 0.0312  lr:0.000100
[ Fri Jul 12 19:52:51 2024 ] 	Batch(2800/6809) done. Loss: 0.0881  lr:0.000100
[ Fri Jul 12 19:53:08 2024 ] 	Batch(2900/6809) done. Loss: 0.1613  lr:0.000100
[ Fri Jul 12 19:53:26 2024 ] 
Training: Epoch [65/120], Step [2999], Loss: 0.012156465090811253, Training Accuracy: 96.3
[ Fri Jul 12 19:53:26 2024 ] 	Batch(3000/6809) done. Loss: 0.0989  lr:0.000100
[ Fri Jul 12 19:53:44 2024 ] 	Batch(3100/6809) done. Loss: 0.0161  lr:0.000100
[ Fri Jul 12 19:54:02 2024 ] 	Batch(3200/6809) done. Loss: 0.0893  lr:0.000100
[ Fri Jul 12 19:54:21 2024 ] 	Batch(3300/6809) done. Loss: 0.1753  lr:0.000100
[ Fri Jul 12 19:54:39 2024 ] 	Batch(3400/6809) done. Loss: 0.1318  lr:0.000100
[ Fri Jul 12 19:54:58 2024 ] 
Training: Epoch [65/120], Step [3499], Loss: 0.12720488011837006, Training Accuracy: 96.29285714285714
[ Fri Jul 12 19:54:58 2024 ] 	Batch(3500/6809) done. Loss: 0.1892  lr:0.000100
[ Fri Jul 12 19:55:17 2024 ] 	Batch(3600/6809) done. Loss: 0.1025  lr:0.000100
[ Fri Jul 12 19:55:35 2024 ] 	Batch(3700/6809) done. Loss: 0.0549  lr:0.000100
[ Fri Jul 12 19:55:54 2024 ] 	Batch(3800/6809) done. Loss: 0.1030  lr:0.000100
[ Fri Jul 12 19:56:11 2024 ] 	Batch(3900/6809) done. Loss: 0.0902  lr:0.000100
[ Fri Jul 12 19:56:29 2024 ] 
Training: Epoch [65/120], Step [3999], Loss: 0.156263530254364, Training Accuracy: 96.25
[ Fri Jul 12 19:56:30 2024 ] 	Batch(4000/6809) done. Loss: 0.0133  lr:0.000100
[ Fri Jul 12 19:56:48 2024 ] 	Batch(4100/6809) done. Loss: 0.0679  lr:0.000100
[ Fri Jul 12 19:57:06 2024 ] 	Batch(4200/6809) done. Loss: 0.0133  lr:0.000100
[ Fri Jul 12 19:57:24 2024 ] 	Batch(4300/6809) done. Loss: 0.0178  lr:0.000100
[ Fri Jul 12 19:57:42 2024 ] 	Batch(4400/6809) done. Loss: 0.1381  lr:0.000100
[ Fri Jul 12 19:58:00 2024 ] 
Training: Epoch [65/120], Step [4499], Loss: 0.3256192207336426, Training Accuracy: 96.28611111111111
[ Fri Jul 12 19:58:00 2024 ] 	Batch(4500/6809) done. Loss: 0.0264  lr:0.000100
[ Fri Jul 12 19:58:18 2024 ] 	Batch(4600/6809) done. Loss: 0.4088  lr:0.000100
[ Fri Jul 12 19:58:36 2024 ] 	Batch(4700/6809) done. Loss: 0.0667  lr:0.000100
[ Fri Jul 12 19:58:54 2024 ] 	Batch(4800/6809) done. Loss: 0.1367  lr:0.000100
[ Fri Jul 12 19:59:12 2024 ] 	Batch(4900/6809) done. Loss: 0.0202  lr:0.000100
[ Fri Jul 12 19:59:29 2024 ] 
Training: Epoch [65/120], Step [4999], Loss: 0.13221561908721924, Training Accuracy: 96.2575
[ Fri Jul 12 19:59:29 2024 ] 	Batch(5000/6809) done. Loss: 0.0211  lr:0.000100
[ Fri Jul 12 19:59:47 2024 ] 	Batch(5100/6809) done. Loss: 0.1379  lr:0.000100
[ Fri Jul 12 20:00:05 2024 ] 	Batch(5200/6809) done. Loss: 0.0588  lr:0.000100
[ Fri Jul 12 20:00:23 2024 ] 	Batch(5300/6809) done. Loss: 0.1514  lr:0.000100
[ Fri Jul 12 20:00:41 2024 ] 	Batch(5400/6809) done. Loss: 0.3301  lr:0.000100
[ Fri Jul 12 20:00:59 2024 ] 
Training: Epoch [65/120], Step [5499], Loss: 0.5927189588546753, Training Accuracy: 96.20909090909092
[ Fri Jul 12 20:00:59 2024 ] 	Batch(5500/6809) done. Loss: 0.2045  lr:0.000100
[ Fri Jul 12 20:01:17 2024 ] 	Batch(5600/6809) done. Loss: 0.3984  lr:0.000100
[ Fri Jul 12 20:01:35 2024 ] 	Batch(5700/6809) done. Loss: 0.0502  lr:0.000100
[ Fri Jul 12 20:01:54 2024 ] 	Batch(5800/6809) done. Loss: 0.0035  lr:0.000100
[ Fri Jul 12 20:02:12 2024 ] 	Batch(5900/6809) done. Loss: 0.1062  lr:0.000100
[ Fri Jul 12 20:02:29 2024 ] 
Training: Epoch [65/120], Step [5999], Loss: 0.06118287891149521, Training Accuracy: 96.16875
[ Fri Jul 12 20:02:30 2024 ] 	Batch(6000/6809) done. Loss: 0.0651  lr:0.000100
[ Fri Jul 12 20:02:48 2024 ] 	Batch(6100/6809) done. Loss: 0.0302  lr:0.000100
[ Fri Jul 12 20:03:06 2024 ] 	Batch(6200/6809) done. Loss: 0.0457  lr:0.000100
[ Fri Jul 12 20:03:24 2024 ] 	Batch(6300/6809) done. Loss: 0.1263  lr:0.000100
[ Fri Jul 12 20:03:42 2024 ] 	Batch(6400/6809) done. Loss: 0.0133  lr:0.000100
[ Fri Jul 12 20:04:01 2024 ] 
Training: Epoch [65/120], Step [6499], Loss: 0.03935393691062927, Training Accuracy: 96.175
[ Fri Jul 12 20:04:01 2024 ] 	Batch(6500/6809) done. Loss: 0.2600  lr:0.000100
[ Fri Jul 12 20:04:19 2024 ] 	Batch(6600/6809) done. Loss: 0.0296  lr:0.000100
[ Fri Jul 12 20:04:38 2024 ] 	Batch(6700/6809) done. Loss: 0.2172  lr:0.000100
[ Fri Jul 12 20:04:57 2024 ] 	Batch(6800/6809) done. Loss: 0.8713  lr:0.000100
[ Fri Jul 12 20:04:58 2024 ] 	Mean training loss: 0.1375.
[ Fri Jul 12 20:04:58 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 20:04:58 2024 ] Training epoch: 67
[ Fri Jul 12 20:04:59 2024 ] 	Batch(0/6809) done. Loss: 0.0790  lr:0.000100
[ Fri Jul 12 20:05:17 2024 ] 	Batch(100/6809) done. Loss: 0.3020  lr:0.000100
[ Fri Jul 12 20:05:35 2024 ] 	Batch(200/6809) done. Loss: 0.0229  lr:0.000100
[ Fri Jul 12 20:05:54 2024 ] 	Batch(300/6809) done. Loss: 0.0276  lr:0.000100
[ Fri Jul 12 20:06:12 2024 ] 	Batch(400/6809) done. Loss: 0.0970  lr:0.000100
[ Fri Jul 12 20:06:30 2024 ] 
Training: Epoch [66/120], Step [499], Loss: 0.0773654505610466, Training Accuracy: 96.175
[ Fri Jul 12 20:06:30 2024 ] 	Batch(500/6809) done. Loss: 0.4306  lr:0.000100
[ Fri Jul 12 20:06:49 2024 ] 	Batch(600/6809) done. Loss: 0.1080  lr:0.000100
[ Fri Jul 12 20:07:07 2024 ] 	Batch(700/6809) done. Loss: 0.4494  lr:0.000100
[ Fri Jul 12 20:07:25 2024 ] 	Batch(800/6809) done. Loss: 0.1806  lr:0.000100
[ Fri Jul 12 20:07:44 2024 ] 	Batch(900/6809) done. Loss: 0.1313  lr:0.000100
[ Fri Jul 12 20:08:02 2024 ] 
Training: Epoch [66/120], Step [999], Loss: 0.27022677659988403, Training Accuracy: 96.1875
[ Fri Jul 12 20:08:02 2024 ] 	Batch(1000/6809) done. Loss: 0.2498  lr:0.000100
[ Fri Jul 12 20:08:20 2024 ] 	Batch(1100/6809) done. Loss: 0.0325  lr:0.000100
[ Fri Jul 12 20:08:39 2024 ] 	Batch(1200/6809) done. Loss: 0.0580  lr:0.000100
[ Fri Jul 12 20:08:57 2024 ] 	Batch(1300/6809) done. Loss: 0.2861  lr:0.000100
[ Fri Jul 12 20:09:16 2024 ] 	Batch(1400/6809) done. Loss: 0.2689  lr:0.000100
[ Fri Jul 12 20:09:34 2024 ] 
Training: Epoch [66/120], Step [1499], Loss: 0.002505821641534567, Training Accuracy: 96.14166666666667
[ Fri Jul 12 20:09:34 2024 ] 	Batch(1500/6809) done. Loss: 0.2837  lr:0.000100
[ Fri Jul 12 20:09:52 2024 ] 	Batch(1600/6809) done. Loss: 0.0103  lr:0.000100
[ Fri Jul 12 20:10:11 2024 ] 	Batch(1700/6809) done. Loss: 0.0102  lr:0.000100
[ Fri Jul 12 20:10:29 2024 ] 	Batch(1800/6809) done. Loss: 0.3368  lr:0.000100
[ Fri Jul 12 20:10:47 2024 ] 	Batch(1900/6809) done. Loss: 0.0245  lr:0.000100
[ Fri Jul 12 20:11:05 2024 ] 
Training: Epoch [66/120], Step [1999], Loss: 0.054827846586704254, Training Accuracy: 96.3
[ Fri Jul 12 20:11:06 2024 ] 	Batch(2000/6809) done. Loss: 0.0393  lr:0.000100
[ Fri Jul 12 20:11:24 2024 ] 	Batch(2100/6809) done. Loss: 0.0128  lr:0.000100
[ Fri Jul 12 20:11:42 2024 ] 	Batch(2200/6809) done. Loss: 0.1882  lr:0.000100
[ Fri Jul 12 20:12:00 2024 ] 	Batch(2300/6809) done. Loss: 0.0090  lr:0.000100
[ Fri Jul 12 20:12:18 2024 ] 	Batch(2400/6809) done. Loss: 0.0182  lr:0.000100
[ Fri Jul 12 20:12:37 2024 ] 
Training: Epoch [66/120], Step [2499], Loss: 0.1583210974931717, Training Accuracy: 96.31
[ Fri Jul 12 20:12:37 2024 ] 	Batch(2500/6809) done. Loss: 0.0104  lr:0.000100
[ Fri Jul 12 20:12:55 2024 ] 	Batch(2600/6809) done. Loss: 0.0143  lr:0.000100
[ Fri Jul 12 20:13:14 2024 ] 	Batch(2700/6809) done. Loss: 0.0260  lr:0.000100
[ Fri Jul 12 20:13:33 2024 ] 	Batch(2800/6809) done. Loss: 0.2341  lr:0.000100
[ Fri Jul 12 20:13:50 2024 ] 	Batch(2900/6809) done. Loss: 0.0069  lr:0.000100
[ Fri Jul 12 20:14:08 2024 ] 
Training: Epoch [66/120], Step [2999], Loss: 0.06196632981300354, Training Accuracy: 96.39166666666667
[ Fri Jul 12 20:14:08 2024 ] 	Batch(3000/6809) done. Loss: 0.0160  lr:0.000100
[ Fri Jul 12 20:14:26 2024 ] 	Batch(3100/6809) done. Loss: 0.6983  lr:0.000100
[ Fri Jul 12 20:14:44 2024 ] 	Batch(3200/6809) done. Loss: 0.5065  lr:0.000100
[ Fri Jul 12 20:15:02 2024 ] 	Batch(3300/6809) done. Loss: 0.1417  lr:0.000100
[ Fri Jul 12 20:15:20 2024 ] 	Batch(3400/6809) done. Loss: 0.2527  lr:0.000100
[ Fri Jul 12 20:15:38 2024 ] 
Training: Epoch [66/120], Step [3499], Loss: 0.03564298525452614, Training Accuracy: 96.39285714285715
[ Fri Jul 12 20:15:38 2024 ] 	Batch(3500/6809) done. Loss: 0.0423  lr:0.000100
[ Fri Jul 12 20:15:56 2024 ] 	Batch(3600/6809) done. Loss: 0.0167  lr:0.000100
[ Fri Jul 12 20:16:14 2024 ] 	Batch(3700/6809) done. Loss: 0.3135  lr:0.000100
[ Fri Jul 12 20:16:32 2024 ] 	Batch(3800/6809) done. Loss: 0.1748  lr:0.000100
[ Fri Jul 12 20:16:50 2024 ] 	Batch(3900/6809) done. Loss: 0.1468  lr:0.000100
[ Fri Jul 12 20:17:07 2024 ] 
Training: Epoch [66/120], Step [3999], Loss: 0.02102084830403328, Training Accuracy: 96.3625
[ Fri Jul 12 20:17:08 2024 ] 	Batch(4000/6809) done. Loss: 0.0586  lr:0.000100
[ Fri Jul 12 20:17:26 2024 ] 	Batch(4100/6809) done. Loss: 0.3577  lr:0.000100
[ Fri Jul 12 20:17:45 2024 ] 	Batch(4200/6809) done. Loss: 0.0464  lr:0.000100
[ Fri Jul 12 20:18:03 2024 ] 	Batch(4300/6809) done. Loss: 0.1186  lr:0.000100
[ Fri Jul 12 20:18:22 2024 ] 	Batch(4400/6809) done. Loss: 0.1162  lr:0.000100
[ Fri Jul 12 20:18:40 2024 ] 
Training: Epoch [66/120], Step [4499], Loss: 0.4706505537033081, Training Accuracy: 96.39999999999999
[ Fri Jul 12 20:18:40 2024 ] 	Batch(4500/6809) done. Loss: 0.0185  lr:0.000100
[ Fri Jul 12 20:18:58 2024 ] 	Batch(4600/6809) done. Loss: 0.0137  lr:0.000100
[ Fri Jul 12 20:19:17 2024 ] 	Batch(4700/6809) done. Loss: 0.1744  lr:0.000100
[ Fri Jul 12 20:19:35 2024 ] 	Batch(4800/6809) done. Loss: 0.4112  lr:0.000100
[ Fri Jul 12 20:19:53 2024 ] 	Batch(4900/6809) done. Loss: 0.2118  lr:0.000100
[ Fri Jul 12 20:20:10 2024 ] 
Training: Epoch [66/120], Step [4999], Loss: 0.4761044979095459, Training Accuracy: 96.375
[ Fri Jul 12 20:20:10 2024 ] 	Batch(5000/6809) done. Loss: 0.0678  lr:0.000100
[ Fri Jul 12 20:20:28 2024 ] 	Batch(5100/6809) done. Loss: 0.1590  lr:0.000100
[ Fri Jul 12 20:20:46 2024 ] 	Batch(5200/6809) done. Loss: 0.2007  lr:0.000100
[ Fri Jul 12 20:21:04 2024 ] 	Batch(5300/6809) done. Loss: 0.0197  lr:0.000100
[ Fri Jul 12 20:21:22 2024 ] 	Batch(5400/6809) done. Loss: 0.1415  lr:0.000100
[ Fri Jul 12 20:21:40 2024 ] 
Training: Epoch [66/120], Step [5499], Loss: 0.034640636295080185, Training Accuracy: 96.38863636363637
[ Fri Jul 12 20:21:40 2024 ] 	Batch(5500/6809) done. Loss: 0.2414  lr:0.000100
[ Fri Jul 12 20:21:58 2024 ] 	Batch(5600/6809) done. Loss: 0.3049  lr:0.000100
[ Fri Jul 12 20:22:16 2024 ] 	Batch(5700/6809) done. Loss: 0.0569  lr:0.000100
[ Fri Jul 12 20:22:35 2024 ] 	Batch(5800/6809) done. Loss: 0.3478  lr:0.000100
[ Fri Jul 12 20:22:53 2024 ] 	Batch(5900/6809) done. Loss: 0.0158  lr:0.000100
[ Fri Jul 12 20:23:10 2024 ] 
Training: Epoch [66/120], Step [5999], Loss: 0.006543624214828014, Training Accuracy: 96.40625
[ Fri Jul 12 20:23:11 2024 ] 	Batch(6000/6809) done. Loss: 0.0327  lr:0.000100
[ Fri Jul 12 20:23:29 2024 ] 	Batch(6100/6809) done. Loss: 0.0329  lr:0.000100
[ Fri Jul 12 20:23:46 2024 ] 	Batch(6200/6809) done. Loss: 0.0730  lr:0.000100
[ Fri Jul 12 20:24:04 2024 ] 	Batch(6300/6809) done. Loss: 0.0261  lr:0.000100
[ Fri Jul 12 20:24:22 2024 ] 	Batch(6400/6809) done. Loss: 0.2402  lr:0.000100
[ Fri Jul 12 20:24:40 2024 ] 
Training: Epoch [66/120], Step [6499], Loss: 0.2681988477706909, Training Accuracy: 96.36730769230769
[ Fri Jul 12 20:24:40 2024 ] 	Batch(6500/6809) done. Loss: 0.0061  lr:0.000100
[ Fri Jul 12 20:24:58 2024 ] 	Batch(6600/6809) done. Loss: 0.1692  lr:0.000100
[ Fri Jul 12 20:25:16 2024 ] 	Batch(6700/6809) done. Loss: 0.1066  lr:0.000100
[ Fri Jul 12 20:25:34 2024 ] 	Batch(6800/6809) done. Loss: 0.0614  lr:0.000100
[ Fri Jul 12 20:25:36 2024 ] 	Mean training loss: 0.1313.
[ Fri Jul 12 20:25:36 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Fri Jul 12 20:25:36 2024 ] Training epoch: 68
[ Fri Jul 12 20:25:36 2024 ] 	Batch(0/6809) done. Loss: 0.1514  lr:0.000100
[ Fri Jul 12 20:25:54 2024 ] 	Batch(100/6809) done. Loss: 0.2379  lr:0.000100
[ Fri Jul 12 20:26:12 2024 ] 	Batch(200/6809) done. Loss: 0.0166  lr:0.000100
[ Fri Jul 12 20:26:30 2024 ] 	Batch(300/6809) done. Loss: 0.1044  lr:0.000100
[ Fri Jul 12 20:26:48 2024 ] 	Batch(400/6809) done. Loss: 0.1555  lr:0.000100
[ Fri Jul 12 20:27:06 2024 ] 
Training: Epoch [67/120], Step [499], Loss: 0.002113746013492346, Training Accuracy: 96.975
[ Fri Jul 12 20:27:06 2024 ] 	Batch(500/6809) done. Loss: 0.0063  lr:0.000100
[ Fri Jul 12 20:27:24 2024 ] 	Batch(600/6809) done. Loss: 0.1275  lr:0.000100
[ Fri Jul 12 20:27:42 2024 ] 	Batch(700/6809) done. Loss: 0.4191  lr:0.000100
[ Fri Jul 12 20:28:00 2024 ] 	Batch(800/6809) done. Loss: 0.0066  lr:0.000100
[ Fri Jul 12 20:28:18 2024 ] 	Batch(900/6809) done. Loss: 0.0513  lr:0.000100
[ Fri Jul 12 20:28:36 2024 ] 
Training: Epoch [67/120], Step [999], Loss: 0.07716946303844452, Training Accuracy: 96.8125
[ Fri Jul 12 20:28:36 2024 ] 	Batch(1000/6809) done. Loss: 0.0284  lr:0.000100
[ Fri Jul 12 20:28:54 2024 ] 	Batch(1100/6809) done. Loss: 0.1919  lr:0.000100
[ Fri Jul 12 20:29:12 2024 ] 	Batch(1200/6809) done. Loss: 0.5221  lr:0.000100
[ Fri Jul 12 20:29:30 2024 ] 	Batch(1300/6809) done. Loss: 0.2358  lr:0.000100
[ Fri Jul 12 20:29:48 2024 ] 	Batch(1400/6809) done. Loss: 0.0603  lr:0.000100
[ Fri Jul 12 20:30:06 2024 ] 
Training: Epoch [67/120], Step [1499], Loss: 0.06237514689564705, Training Accuracy: 96.85000000000001
[ Fri Jul 12 20:30:06 2024 ] 	Batch(1500/6809) done. Loss: 0.3699  lr:0.000100
[ Fri Jul 12 20:30:24 2024 ] 	Batch(1600/6809) done. Loss: 0.1803  lr:0.000100
[ Fri Jul 12 20:30:42 2024 ] 	Batch(1700/6809) done. Loss: 0.0739  lr:0.000100
[ Fri Jul 12 20:31:00 2024 ] 	Batch(1800/6809) done. Loss: 0.1665  lr:0.000100
[ Fri Jul 12 20:31:18 2024 ] 	Batch(1900/6809) done. Loss: 0.2420  lr:0.000100
[ Fri Jul 12 20:31:36 2024 ] 
Training: Epoch [67/120], Step [1999], Loss: 0.038473621010780334, Training Accuracy: 96.89999999999999
[ Fri Jul 12 20:31:37 2024 ] 	Batch(2000/6809) done. Loss: 0.0113  lr:0.000100
[ Fri Jul 12 20:31:55 2024 ] 	Batch(2100/6809) done. Loss: 0.0370  lr:0.000100
[ Fri Jul 12 20:32:13 2024 ] 	Batch(2200/6809) done. Loss: 0.1926  lr:0.000100
[ Fri Jul 12 20:32:31 2024 ] 	Batch(2300/6809) done. Loss: 0.0100  lr:0.000100
[ Fri Jul 12 20:32:49 2024 ] 	Batch(2400/6809) done. Loss: 0.0910  lr:0.000100
[ Fri Jul 12 20:33:07 2024 ] 
Training: Epoch [67/120], Step [2499], Loss: 0.20164033770561218, Training Accuracy: 96.84
[ Fri Jul 12 20:33:07 2024 ] 	Batch(2500/6809) done. Loss: 0.0317  lr:0.000100
[ Fri Jul 12 20:33:26 2024 ] 	Batch(2600/6809) done. Loss: 0.4838  lr:0.000100
[ Fri Jul 12 20:33:44 2024 ] 	Batch(2700/6809) done. Loss: 0.0387  lr:0.000100
[ Fri Jul 12 20:34:02 2024 ] 	Batch(2800/6809) done. Loss: 0.0655  lr:0.000100
[ Fri Jul 12 20:34:20 2024 ] 	Batch(2900/6809) done. Loss: 0.0501  lr:0.000100
[ Fri Jul 12 20:34:38 2024 ] 
Training: Epoch [67/120], Step [2999], Loss: 0.0701138824224472, Training Accuracy: 96.78333333333333
[ Fri Jul 12 20:34:38 2024 ] 	Batch(3000/6809) done. Loss: 0.0977  lr:0.000100
[ Fri Jul 12 20:34:56 2024 ] 	Batch(3100/6809) done. Loss: 0.1736  lr:0.000100
[ Fri Jul 12 20:35:14 2024 ] 	Batch(3200/6809) done. Loss: 0.0812  lr:0.000100
[ Fri Jul 12 20:35:33 2024 ] 	Batch(3300/6809) done. Loss: 0.0309  lr:0.000100
[ Fri Jul 12 20:35:52 2024 ] 	Batch(3400/6809) done. Loss: 0.0059  lr:0.000100
[ Fri Jul 12 20:36:10 2024 ] 
Training: Epoch [67/120], Step [3499], Loss: 0.07761052250862122, Training Accuracy: 96.72142857142858
[ Fri Jul 12 20:36:10 2024 ] 	Batch(3500/6809) done. Loss: 0.0564  lr:0.000100
[ Fri Jul 12 20:36:29 2024 ] 	Batch(3600/6809) done. Loss: 0.1289  lr:0.000100
[ Fri Jul 12 20:36:48 2024 ] 	Batch(3700/6809) done. Loss: 0.1446  lr:0.000100
[ Fri Jul 12 20:37:06 2024 ] 	Batch(3800/6809) done. Loss: 0.6955  lr:0.000100
[ Fri Jul 12 20:37:24 2024 ] 	Batch(3900/6809) done. Loss: 0.0180  lr:0.000100
[ Fri Jul 12 20:37:42 2024 ] 
Training: Epoch [67/120], Step [3999], Loss: 0.2543366849422455, Training Accuracy: 96.72812499999999
[ Fri Jul 12 20:37:42 2024 ] 	Batch(4000/6809) done. Loss: 0.0598  lr:0.000100
[ Fri Jul 12 20:38:01 2024 ] 	Batch(4100/6809) done. Loss: 0.0450  lr:0.000100
[ Fri Jul 12 20:38:19 2024 ] 	Batch(4200/6809) done. Loss: 0.0040  lr:0.000100
[ Fri Jul 12 20:38:37 2024 ] 	Batch(4300/6809) done. Loss: 0.1453  lr:0.000100
[ Fri Jul 12 20:38:56 2024 ] 	Batch(4400/6809) done. Loss: 0.2083  lr:0.000100
[ Fri Jul 12 20:39:14 2024 ] 
Training: Epoch [67/120], Step [4499], Loss: 0.14977163076400757, Training Accuracy: 96.71944444444445
[ Fri Jul 12 20:39:14 2024 ] 	Batch(4500/6809) done. Loss: 0.0903  lr:0.000100
[ Fri Jul 12 20:39:33 2024 ] 	Batch(4600/6809) done. Loss: 0.0566  lr:0.000100
[ Fri Jul 12 20:39:51 2024 ] 	Batch(4700/6809) done. Loss: 0.0312  lr:0.000100
[ Fri Jul 12 20:40:09 2024 ] 	Batch(4800/6809) done. Loss: 0.0991  lr:0.000100
[ Fri Jul 12 20:40:28 2024 ] 	Batch(4900/6809) done. Loss: 0.3091  lr:0.000100
[ Fri Jul 12 20:40:45 2024 ] 
Training: Epoch [67/120], Step [4999], Loss: 0.0035338031593710184, Training Accuracy: 96.6875
[ Fri Jul 12 20:40:46 2024 ] 	Batch(5000/6809) done. Loss: 0.1430  lr:0.000100
[ Fri Jul 12 20:41:03 2024 ] 	Batch(5100/6809) done. Loss: 0.2747  lr:0.000100
[ Fri Jul 12 20:41:21 2024 ] 	Batch(5200/6809) done. Loss: 0.0398  lr:0.000100
[ Fri Jul 12 20:41:39 2024 ] 	Batch(5300/6809) done. Loss: 0.2286  lr:0.000100
[ Fri Jul 12 20:41:57 2024 ] 	Batch(5400/6809) done. Loss: 0.0043  lr:0.000100
[ Fri Jul 12 20:42:15 2024 ] 
Training: Epoch [67/120], Step [5499], Loss: 0.12142965197563171, Training Accuracy: 96.70454545454545
[ Fri Jul 12 20:42:15 2024 ] 	Batch(5500/6809) done. Loss: 0.1751  lr:0.000100
[ Fri Jul 12 20:42:33 2024 ] 	Batch(5600/6809) done. Loss: 0.0586  lr:0.000100
[ Fri Jul 12 20:42:51 2024 ] 	Batch(5700/6809) done. Loss: 0.6348  lr:0.000100
[ Fri Jul 12 20:43:09 2024 ] 	Batch(5800/6809) done. Loss: 0.0095  lr:0.000100
[ Fri Jul 12 20:43:27 2024 ] 	Batch(5900/6809) done. Loss: 0.0876  lr:0.000100
[ Fri Jul 12 20:43:45 2024 ] 
Training: Epoch [67/120], Step [5999], Loss: 0.023112528026103973, Training Accuracy: 96.66041666666668
[ Fri Jul 12 20:43:45 2024 ] 	Batch(6000/6809) done. Loss: 0.0601  lr:0.000100
[ Fri Jul 12 20:44:03 2024 ] 	Batch(6100/6809) done. Loss: 0.0081  lr:0.000100
[ Fri Jul 12 20:44:21 2024 ] 	Batch(6200/6809) done. Loss: 0.0213  lr:0.000100
[ Fri Jul 12 20:44:39 2024 ] 	Batch(6300/6809) done. Loss: 0.0417  lr:0.000100
[ Fri Jul 12 20:44:57 2024 ] 	Batch(6400/6809) done. Loss: 0.0073  lr:0.000100
[ Fri Jul 12 20:45:15 2024 ] 
Training: Epoch [67/120], Step [6499], Loss: 0.42965203523635864, Training Accuracy: 96.63653846153846
[ Fri Jul 12 20:45:15 2024 ] 	Batch(6500/6809) done. Loss: 0.0659  lr:0.000100
[ Fri Jul 12 20:45:34 2024 ] 	Batch(6600/6809) done. Loss: 0.1042  lr:0.000100
[ Fri Jul 12 20:45:52 2024 ] 	Batch(6700/6809) done. Loss: 0.0517  lr:0.000100
[ Fri Jul 12 20:46:11 2024 ] 	Batch(6800/6809) done. Loss: 0.3418  lr:0.000100
[ Fri Jul 12 20:46:12 2024 ] 	Mean training loss: 0.1261.
[ Fri Jul 12 20:46:12 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 20:46:13 2024 ] Training epoch: 69
[ Fri Jul 12 20:46:13 2024 ] 	Batch(0/6809) done. Loss: 0.0119  lr:0.000100
[ Fri Jul 12 20:46:31 2024 ] 	Batch(100/6809) done. Loss: 0.0564  lr:0.000100
[ Fri Jul 12 20:46:50 2024 ] 	Batch(200/6809) done. Loss: 0.5160  lr:0.000100
[ Fri Jul 12 20:47:08 2024 ] 	Batch(300/6809) done. Loss: 0.0436  lr:0.000100
[ Fri Jul 12 20:47:26 2024 ] 	Batch(400/6809) done. Loss: 0.2512  lr:0.000100
[ Fri Jul 12 20:47:44 2024 ] 
Training: Epoch [68/120], Step [499], Loss: 0.5638379454612732, Training Accuracy: 96.025
[ Fri Jul 12 20:47:45 2024 ] 	Batch(500/6809) done. Loss: 0.1478  lr:0.000100
[ Fri Jul 12 20:48:03 2024 ] 	Batch(600/6809) done. Loss: 0.0500  lr:0.000100
[ Fri Jul 12 20:48:22 2024 ] 	Batch(700/6809) done. Loss: 0.3008  lr:0.000100
[ Fri Jul 12 20:48:40 2024 ] 	Batch(800/6809) done. Loss: 0.1652  lr:0.000100
[ Fri Jul 12 20:48:58 2024 ] 	Batch(900/6809) done. Loss: 0.0044  lr:0.000100
[ Fri Jul 12 20:49:16 2024 ] 
Training: Epoch [68/120], Step [999], Loss: 0.20101293921470642, Training Accuracy: 96.42500000000001
[ Fri Jul 12 20:49:16 2024 ] 	Batch(1000/6809) done. Loss: 0.0323  lr:0.000100
[ Fri Jul 12 20:49:34 2024 ] 	Batch(1100/6809) done. Loss: 0.2415  lr:0.000100
[ Fri Jul 12 20:49:52 2024 ] 	Batch(1200/6809) done. Loss: 0.2172  lr:0.000100
[ Fri Jul 12 20:50:10 2024 ] 	Batch(1300/6809) done. Loss: 0.2442  lr:0.000100
[ Fri Jul 12 20:50:29 2024 ] 	Batch(1400/6809) done. Loss: 0.0436  lr:0.000100
[ Fri Jul 12 20:50:46 2024 ] 
Training: Epoch [68/120], Step [1499], Loss: 0.08914381265640259, Training Accuracy: 96.39166666666667
[ Fri Jul 12 20:50:46 2024 ] 	Batch(1500/6809) done. Loss: 0.0877  lr:0.000100
[ Fri Jul 12 20:51:04 2024 ] 	Batch(1600/6809) done. Loss: 0.0201  lr:0.000100
[ Fri Jul 12 20:51:23 2024 ] 	Batch(1700/6809) done. Loss: 0.0063  lr:0.000100
[ Fri Jul 12 20:51:41 2024 ] 	Batch(1800/6809) done. Loss: 0.0023  lr:0.000100
[ Fri Jul 12 20:52:00 2024 ] 	Batch(1900/6809) done. Loss: 0.2170  lr:0.000100
[ Fri Jul 12 20:52:18 2024 ] 
Training: Epoch [68/120], Step [1999], Loss: 0.003234975039958954, Training Accuracy: 96.41875
[ Fri Jul 12 20:52:19 2024 ] 	Batch(2000/6809) done. Loss: 0.0150  lr:0.000100
[ Fri Jul 12 20:52:37 2024 ] 	Batch(2100/6809) done. Loss: 0.0143  lr:0.000100
[ Fri Jul 12 20:52:56 2024 ] 	Batch(2200/6809) done. Loss: 0.0860  lr:0.000100
[ Fri Jul 12 20:53:13 2024 ] 	Batch(2300/6809) done. Loss: 0.0355  lr:0.000100
[ Fri Jul 12 20:53:31 2024 ] 	Batch(2400/6809) done. Loss: 0.2118  lr:0.000100
[ Fri Jul 12 20:53:49 2024 ] 
Training: Epoch [68/120], Step [2499], Loss: 0.04691688343882561, Training Accuracy: 96.37
[ Fri Jul 12 20:53:49 2024 ] 	Batch(2500/6809) done. Loss: 0.0033  lr:0.000100
[ Fri Jul 12 20:54:07 2024 ] 	Batch(2600/6809) done. Loss: 0.2502  lr:0.000100
[ Fri Jul 12 20:54:25 2024 ] 	Batch(2700/6809) done. Loss: 0.0274  lr:0.000100
[ Fri Jul 12 20:54:43 2024 ] 	Batch(2800/6809) done. Loss: 0.0280  lr:0.000100
[ Fri Jul 12 20:55:01 2024 ] 	Batch(2900/6809) done. Loss: 0.1269  lr:0.000100
[ Fri Jul 12 20:55:19 2024 ] 
Training: Epoch [68/120], Step [2999], Loss: 0.12416863441467285, Training Accuracy: 96.52083333333333
[ Fri Jul 12 20:55:19 2024 ] 	Batch(3000/6809) done. Loss: 0.1355  lr:0.000100
[ Fri Jul 12 20:55:37 2024 ] 	Batch(3100/6809) done. Loss: 0.0869  lr:0.000100
[ Fri Jul 12 20:55:55 2024 ] 	Batch(3200/6809) done. Loss: 0.1799  lr:0.000100
[ Fri Jul 12 20:56:13 2024 ] 	Batch(3300/6809) done. Loss: 0.0098  lr:0.000100
[ Fri Jul 12 20:56:31 2024 ] 	Batch(3400/6809) done. Loss: 0.3592  lr:0.000100
[ Fri Jul 12 20:56:48 2024 ] 
Training: Epoch [68/120], Step [3499], Loss: 0.1361239105463028, Training Accuracy: 96.54285714285714
[ Fri Jul 12 20:56:48 2024 ] 	Batch(3500/6809) done. Loss: 0.1388  lr:0.000100
[ Fri Jul 12 20:57:06 2024 ] 	Batch(3600/6809) done. Loss: 0.0065  lr:0.000100
[ Fri Jul 12 20:57:24 2024 ] 	Batch(3700/6809) done. Loss: 0.0069  lr:0.000100
[ Fri Jul 12 20:57:42 2024 ] 	Batch(3800/6809) done. Loss: 0.0298  lr:0.000100
[ Fri Jul 12 20:58:00 2024 ] 	Batch(3900/6809) done. Loss: 0.0313  lr:0.000100
[ Fri Jul 12 20:58:18 2024 ] 
Training: Epoch [68/120], Step [3999], Loss: 0.025974754244089127, Training Accuracy: 96.559375
[ Fri Jul 12 20:58:18 2024 ] 	Batch(4000/6809) done. Loss: 0.0965  lr:0.000100
[ Fri Jul 12 20:58:37 2024 ] 	Batch(4100/6809) done. Loss: 0.0148  lr:0.000100
[ Fri Jul 12 20:58:55 2024 ] 	Batch(4200/6809) done. Loss: 0.1637  lr:0.000100
[ Fri Jul 12 20:59:14 2024 ] 	Batch(4300/6809) done. Loss: 0.0278  lr:0.000100
[ Fri Jul 12 20:59:32 2024 ] 	Batch(4400/6809) done. Loss: 0.0495  lr:0.000100
[ Fri Jul 12 20:59:51 2024 ] 
Training: Epoch [68/120], Step [4499], Loss: 0.18475684523582458, Training Accuracy: 96.54722222222222
[ Fri Jul 12 20:59:51 2024 ] 	Batch(4500/6809) done. Loss: 0.0348  lr:0.000100
[ Fri Jul 12 21:00:09 2024 ] 	Batch(4600/6809) done. Loss: 0.0430  lr:0.000100
[ Fri Jul 12 21:00:27 2024 ] 	Batch(4700/6809) done. Loss: 0.0377  lr:0.000100
[ Fri Jul 12 21:00:45 2024 ] 	Batch(4800/6809) done. Loss: 0.0143  lr:0.000100
[ Fri Jul 12 21:01:04 2024 ] 	Batch(4900/6809) done. Loss: 0.0160  lr:0.000100
[ Fri Jul 12 21:01:22 2024 ] 
Training: Epoch [68/120], Step [4999], Loss: 0.2866036593914032, Training Accuracy: 96.58
[ Fri Jul 12 21:01:22 2024 ] 	Batch(5000/6809) done. Loss: 0.0028  lr:0.000100
[ Fri Jul 12 21:01:40 2024 ] 	Batch(5100/6809) done. Loss: 0.9320  lr:0.000100
[ Fri Jul 12 21:01:58 2024 ] 	Batch(5200/6809) done. Loss: 0.0750  lr:0.000100
[ Fri Jul 12 21:02:16 2024 ] 	Batch(5300/6809) done. Loss: 0.1348  lr:0.000100
[ Fri Jul 12 21:02:34 2024 ] 	Batch(5400/6809) done. Loss: 0.3915  lr:0.000100
[ Fri Jul 12 21:02:51 2024 ] 
Training: Epoch [68/120], Step [5499], Loss: 0.03225577250123024, Training Accuracy: 96.625
[ Fri Jul 12 21:02:52 2024 ] 	Batch(5500/6809) done. Loss: 0.1461  lr:0.000100
[ Fri Jul 12 21:03:10 2024 ] 	Batch(5600/6809) done. Loss: 0.0888  lr:0.000100
[ Fri Jul 12 21:03:28 2024 ] 	Batch(5700/6809) done. Loss: 0.1309  lr:0.000100
[ Fri Jul 12 21:03:45 2024 ] 	Batch(5800/6809) done. Loss: 0.0144  lr:0.000100
[ Fri Jul 12 21:04:03 2024 ] 	Batch(5900/6809) done. Loss: 0.0066  lr:0.000100
[ Fri Jul 12 21:04:21 2024 ] 
Training: Epoch [68/120], Step [5999], Loss: 0.12970590591430664, Training Accuracy: 96.625
[ Fri Jul 12 21:04:21 2024 ] 	Batch(6000/6809) done. Loss: 0.2983  lr:0.000100
[ Fri Jul 12 21:04:39 2024 ] 	Batch(6100/6809) done. Loss: 0.4530  lr:0.000100
[ Fri Jul 12 21:04:57 2024 ] 	Batch(6200/6809) done. Loss: 0.1407  lr:0.000100
[ Fri Jul 12 21:05:16 2024 ] 	Batch(6300/6809) done. Loss: 0.0227  lr:0.000100
[ Fri Jul 12 21:05:34 2024 ] 	Batch(6400/6809) done. Loss: 0.1411  lr:0.000100
[ Fri Jul 12 21:05:52 2024 ] 
Training: Epoch [68/120], Step [6499], Loss: 0.5596244931221008, Training Accuracy: 96.6076923076923
[ Fri Jul 12 21:05:52 2024 ] 	Batch(6500/6809) done. Loss: 0.0718  lr:0.000100
[ Fri Jul 12 21:06:10 2024 ] 	Batch(6600/6809) done. Loss: 0.1138  lr:0.000100
[ Fri Jul 12 21:06:28 2024 ] 	Batch(6700/6809) done. Loss: 0.0197  lr:0.000100
[ Fri Jul 12 21:06:46 2024 ] 	Batch(6800/6809) done. Loss: 0.0691  lr:0.000100
[ Fri Jul 12 21:06:47 2024 ] 	Mean training loss: 0.1240.
[ Fri Jul 12 21:06:47 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 21:06:47 2024 ] Training epoch: 70
[ Fri Jul 12 21:06:48 2024 ] 	Batch(0/6809) done. Loss: 0.0477  lr:0.000100
[ Fri Jul 12 21:07:06 2024 ] 	Batch(100/6809) done. Loss: 0.0977  lr:0.000100
[ Fri Jul 12 21:07:24 2024 ] 	Batch(200/6809) done. Loss: 0.0011  lr:0.000100
[ Fri Jul 12 21:07:42 2024 ] 	Batch(300/6809) done. Loss: 0.0989  lr:0.000100
[ Fri Jul 12 21:08:00 2024 ] 	Batch(400/6809) done. Loss: 0.0272  lr:0.000100
[ Fri Jul 12 21:08:18 2024 ] 
Training: Epoch [69/120], Step [499], Loss: 0.2737657427787781, Training Accuracy: 96.025
[ Fri Jul 12 21:08:19 2024 ] 	Batch(500/6809) done. Loss: 0.1289  lr:0.000100
[ Fri Jul 12 21:08:37 2024 ] 	Batch(600/6809) done. Loss: 0.0282  lr:0.000100
[ Fri Jul 12 21:08:54 2024 ] 	Batch(700/6809) done. Loss: 0.0685  lr:0.000100
[ Fri Jul 12 21:09:12 2024 ] 	Batch(800/6809) done. Loss: 0.6518  lr:0.000100
[ Fri Jul 12 21:09:30 2024 ] 	Batch(900/6809) done. Loss: 0.2427  lr:0.000100
[ Fri Jul 12 21:09:48 2024 ] 
Training: Epoch [69/120], Step [999], Loss: 0.04205623269081116, Training Accuracy: 96.3875
[ Fri Jul 12 21:09:48 2024 ] 	Batch(1000/6809) done. Loss: 0.0444  lr:0.000100
[ Fri Jul 12 21:10:06 2024 ] 	Batch(1100/6809) done. Loss: 0.2316  lr:0.000100
[ Fri Jul 12 21:10:24 2024 ] 	Batch(1200/6809) done. Loss: 0.3259  lr:0.000100
[ Fri Jul 12 21:10:42 2024 ] 	Batch(1300/6809) done. Loss: 0.0373  lr:0.000100
[ Fri Jul 12 21:11:00 2024 ] 	Batch(1400/6809) done. Loss: 0.0083  lr:0.000100
[ Fri Jul 12 21:11:18 2024 ] 
Training: Epoch [69/120], Step [1499], Loss: 0.011062701232731342, Training Accuracy: 96.36666666666667
[ Fri Jul 12 21:11:18 2024 ] 	Batch(1500/6809) done. Loss: 0.0062  lr:0.000100
[ Fri Jul 12 21:11:36 2024 ] 	Batch(1600/6809) done. Loss: 0.0221  lr:0.000100
[ Fri Jul 12 21:11:54 2024 ] 	Batch(1700/6809) done. Loss: 0.0877  lr:0.000100
[ Fri Jul 12 21:12:12 2024 ] 	Batch(1800/6809) done. Loss: 0.0159  lr:0.000100
[ Fri Jul 12 21:12:30 2024 ] 	Batch(1900/6809) done. Loss: 0.0136  lr:0.000100
[ Fri Jul 12 21:12:47 2024 ] 
Training: Epoch [69/120], Step [1999], Loss: 0.05490369349718094, Training Accuracy: 96.45625
[ Fri Jul 12 21:12:48 2024 ] 	Batch(2000/6809) done. Loss: 0.0100  lr:0.000100
[ Fri Jul 12 21:13:05 2024 ] 	Batch(2100/6809) done. Loss: 0.0037  lr:0.000100
[ Fri Jul 12 21:13:23 2024 ] 	Batch(2200/6809) done. Loss: 0.0828  lr:0.000100
[ Fri Jul 12 21:13:41 2024 ] 	Batch(2300/6809) done. Loss: 0.0695  lr:0.000100
[ Fri Jul 12 21:13:59 2024 ] 	Batch(2400/6809) done. Loss: 0.1346  lr:0.000100
[ Fri Jul 12 21:14:17 2024 ] 
Training: Epoch [69/120], Step [2499], Loss: 0.11361268907785416, Training Accuracy: 96.41999999999999
[ Fri Jul 12 21:14:17 2024 ] 	Batch(2500/6809) done. Loss: 0.0333  lr:0.000100
[ Fri Jul 12 21:14:35 2024 ] 	Batch(2600/6809) done. Loss: 0.0869  lr:0.000100
[ Fri Jul 12 21:14:53 2024 ] 	Batch(2700/6809) done. Loss: 0.0321  lr:0.000100
[ Fri Jul 12 21:15:11 2024 ] 	Batch(2800/6809) done. Loss: 0.1417  lr:0.000100
[ Fri Jul 12 21:15:29 2024 ] 	Batch(2900/6809) done. Loss: 0.1808  lr:0.000100
[ Fri Jul 12 21:15:47 2024 ] 
Training: Epoch [69/120], Step [2999], Loss: 0.22224073112010956, Training Accuracy: 96.49166666666666
[ Fri Jul 12 21:15:47 2024 ] 	Batch(3000/6809) done. Loss: 0.5190  lr:0.000100
[ Fri Jul 12 21:16:05 2024 ] 	Batch(3100/6809) done. Loss: 0.6255  lr:0.000100
[ Fri Jul 12 21:16:23 2024 ] 	Batch(3200/6809) done. Loss: 0.0381  lr:0.000100
[ Fri Jul 12 21:16:41 2024 ] 	Batch(3300/6809) done. Loss: 1.2171  lr:0.000100
[ Fri Jul 12 21:16:58 2024 ] 	Batch(3400/6809) done. Loss: 0.1286  lr:0.000100
[ Fri Jul 12 21:17:16 2024 ] 
Training: Epoch [69/120], Step [3499], Loss: 0.20045627653598785, Training Accuracy: 96.46071428571429
[ Fri Jul 12 21:17:16 2024 ] 	Batch(3500/6809) done. Loss: 0.1151  lr:0.000100
[ Fri Jul 12 21:17:34 2024 ] 	Batch(3600/6809) done. Loss: 0.0175  lr:0.000100
[ Fri Jul 12 21:17:52 2024 ] 	Batch(3700/6809) done. Loss: 0.0704  lr:0.000100
[ Fri Jul 12 21:18:10 2024 ] 	Batch(3800/6809) done. Loss: 0.0745  lr:0.000100
[ Fri Jul 12 21:18:28 2024 ] 	Batch(3900/6809) done. Loss: 0.0847  lr:0.000100
[ Fri Jul 12 21:18:46 2024 ] 
Training: Epoch [69/120], Step [3999], Loss: 0.06535313278436661, Training Accuracy: 96.49375
[ Fri Jul 12 21:18:46 2024 ] 	Batch(4000/6809) done. Loss: 0.1609  lr:0.000100
[ Fri Jul 12 21:19:04 2024 ] 	Batch(4100/6809) done. Loss: 0.1536  lr:0.000100
[ Fri Jul 12 21:19:22 2024 ] 	Batch(4200/6809) done. Loss: 0.0703  lr:0.000100
[ Fri Jul 12 21:19:40 2024 ] 	Batch(4300/6809) done. Loss: 0.0970  lr:0.000100
[ Fri Jul 12 21:19:59 2024 ] 	Batch(4400/6809) done. Loss: 0.0312  lr:0.000100
[ Fri Jul 12 21:20:17 2024 ] 
Training: Epoch [69/120], Step [4499], Loss: 0.038327641785144806, Training Accuracy: 96.54166666666667
[ Fri Jul 12 21:20:17 2024 ] 	Batch(4500/6809) done. Loss: 0.0862  lr:0.000100
[ Fri Jul 12 21:20:36 2024 ] 	Batch(4600/6809) done. Loss: 0.0444  lr:0.000100
[ Fri Jul 12 21:20:54 2024 ] 	Batch(4700/6809) done. Loss: 0.1458  lr:0.000100
[ Fri Jul 12 21:21:13 2024 ] 	Batch(4800/6809) done. Loss: 0.1767  lr:0.000100
[ Fri Jul 12 21:21:31 2024 ] 	Batch(4900/6809) done. Loss: 0.0311  lr:0.000100
[ Fri Jul 12 21:21:50 2024 ] 
Training: Epoch [69/120], Step [4999], Loss: 0.11618900299072266, Training Accuracy: 96.57249999999999
[ Fri Jul 12 21:21:50 2024 ] 	Batch(5000/6809) done. Loss: 0.0670  lr:0.000100
[ Fri Jul 12 21:22:08 2024 ] 	Batch(5100/6809) done. Loss: 0.0126  lr:0.000100
[ Fri Jul 12 21:22:27 2024 ] 	Batch(5200/6809) done. Loss: 0.1650  lr:0.000100
[ Fri Jul 12 21:22:45 2024 ] 	Batch(5300/6809) done. Loss: 0.0138  lr:0.000100
[ Fri Jul 12 21:23:03 2024 ] 	Batch(5400/6809) done. Loss: 0.0368  lr:0.000100
[ Fri Jul 12 21:23:21 2024 ] 
Training: Epoch [69/120], Step [5499], Loss: 0.037951644510030746, Training Accuracy: 96.58409090909092
[ Fri Jul 12 21:23:21 2024 ] 	Batch(5500/6809) done. Loss: 0.0999  lr:0.000100
[ Fri Jul 12 21:23:40 2024 ] 	Batch(5600/6809) done. Loss: 0.0016  lr:0.000100
[ Fri Jul 12 21:23:58 2024 ] 	Batch(5700/6809) done. Loss: 0.3749  lr:0.000100
[ Fri Jul 12 21:24:17 2024 ] 	Batch(5800/6809) done. Loss: 0.0416  lr:0.000100
[ Fri Jul 12 21:24:35 2024 ] 	Batch(5900/6809) done. Loss: 0.0734  lr:0.000100
[ Fri Jul 12 21:24:54 2024 ] 
Training: Epoch [69/120], Step [5999], Loss: 0.0055359043180942535, Training Accuracy: 96.56666666666666
[ Fri Jul 12 21:24:54 2024 ] 	Batch(6000/6809) done. Loss: 0.0103  lr:0.000100
[ Fri Jul 12 21:25:13 2024 ] 	Batch(6100/6809) done. Loss: 0.1116  lr:0.000100
[ Fri Jul 12 21:25:31 2024 ] 	Batch(6200/6809) done. Loss: 0.0382  lr:0.000100
[ Fri Jul 12 21:25:50 2024 ] 	Batch(6300/6809) done. Loss: 0.0322  lr:0.000100
[ Fri Jul 12 21:26:08 2024 ] 	Batch(6400/6809) done. Loss: 0.0244  lr:0.000100
[ Fri Jul 12 21:26:26 2024 ] 
Training: Epoch [69/120], Step [6499], Loss: 0.02526700310409069, Training Accuracy: 96.63269230769231
[ Fri Jul 12 21:26:26 2024 ] 	Batch(6500/6809) done. Loss: 0.0643  lr:0.000100
[ Fri Jul 12 21:26:44 2024 ] 	Batch(6600/6809) done. Loss: 0.0242  lr:0.000100
[ Fri Jul 12 21:27:02 2024 ] 	Batch(6700/6809) done. Loss: 0.0319  lr:0.000100
[ Fri Jul 12 21:27:20 2024 ] 	Batch(6800/6809) done. Loss: 0.2055  lr:0.000100
[ Fri Jul 12 21:27:21 2024 ] 	Mean training loss: 0.1247.
[ Fri Jul 12 21:27:21 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 21:27:22 2024 ] Eval epoch: 70
[ Fri Jul 12 21:32:55 2024 ] 	Mean val loss of 7435 batches: 1.0555679200453552.
[ Fri Jul 12 21:32:55 2024 ] 
Validation: Epoch [69/120], Samples [47482.0/59477], Loss: 0.821562647819519, Validation Accuracy: 79.83254030969955
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 1 : 374 / 500 = 74 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 2 : 427 / 499 = 85 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 3 : 395 / 500 = 79 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 4 : 403 / 502 = 80 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 5 : 461 / 502 = 91 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 6 : 410 / 502 = 81 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 7 : 469 / 497 = 94 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 8 : 481 / 498 = 96 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 9 : 384 / 500 = 76 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 10 : 171 / 500 = 34 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 11 : 195 / 498 = 39 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 12 : 393 / 499 = 78 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 13 : 476 / 502 = 94 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 14 : 476 / 504 = 94 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 15 : 435 / 502 = 86 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 16 : 374 / 502 = 74 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 17 : 437 / 504 = 86 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 18 : 405 / 504 = 80 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 19 : 464 / 502 = 92 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 20 : 454 / 502 = 90 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 21 : 472 / 503 = 93 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 22 : 426 / 504 = 84 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 23 : 439 / 503 = 87 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 24 : 408 / 504 = 80 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 25 : 486 / 504 = 96 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 26 : 473 / 504 = 93 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 27 : 410 / 501 = 81 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 28 : 351 / 502 = 69 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 29 : 315 / 502 = 62 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 30 : 324 / 501 = 64 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 31 : 419 / 504 = 83 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 32 : 415 / 503 = 82 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 33 : 408 / 503 = 81 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 34 : 481 / 504 = 95 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 35 : 466 / 503 = 92 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 36 : 410 / 502 = 81 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 37 : 438 / 504 = 86 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 38 : 428 / 504 = 84 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 39 : 449 / 498 = 90 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 40 : 386 / 504 = 76 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 41 : 481 / 503 = 95 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 42 : 457 / 504 = 90 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 43 : 329 / 503 = 65 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 44 : 441 / 504 = 87 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 45 : 427 / 504 = 84 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 46 : 403 / 504 = 79 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 47 : 410 / 503 = 81 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 48 : 428 / 503 = 85 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 49 : 375 / 499 = 75 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 50 : 416 / 502 = 82 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 51 : 466 / 503 = 92 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 52 : 450 / 504 = 89 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 53 : 425 / 497 = 85 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 54 : 455 / 480 = 94 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 55 : 375 / 504 = 74 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 56 : 409 / 503 = 81 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 57 : 486 / 504 = 96 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 58 : 478 / 499 = 95 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 59 : 491 / 503 = 97 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 60 : 420 / 479 = 87 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 61 : 427 / 484 = 88 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 62 : 400 / 487 = 82 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 63 : 446 / 489 = 91 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 64 : 371 / 488 = 76 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 65 : 453 / 490 = 92 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 66 : 322 / 488 = 65 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 67 : 381 / 490 = 77 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 68 : 277 / 490 = 56 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 69 : 375 / 490 = 76 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 70 : 206 / 490 = 42 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 71 : 198 / 490 = 40 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 72 : 187 / 488 = 38 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 73 : 287 / 486 = 59 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 74 : 298 / 481 = 61 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 75 : 255 / 488 = 52 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 76 : 324 / 489 = 66 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 77 : 325 / 488 = 66 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 78 : 370 / 488 = 75 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 79 : 447 / 490 = 91 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 80 : 416 / 489 = 85 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 81 : 313 / 491 = 63 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 82 : 307 / 491 = 62 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 83 : 261 / 489 = 53 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 84 : 395 / 489 = 80 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 85 : 382 / 489 = 78 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 86 : 430 / 491 = 87 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 87 : 436 / 492 = 88 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 88 : 372 / 491 = 75 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 89 : 399 / 492 = 81 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 90 : 262 / 490 = 53 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 91 : 400 / 482 = 82 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 92 : 368 / 490 = 75 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 93 : 363 / 487 = 74 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 94 : 412 / 489 = 84 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 95 : 413 / 490 = 84 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 96 : 466 / 491 = 94 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 97 : 463 / 490 = 94 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 98 : 449 / 491 = 91 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 99 : 442 / 491 = 90 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 100 : 448 / 491 = 91 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 101 : 438 / 491 = 89 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 102 : 293 / 492 = 59 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 103 : 391 / 492 = 79 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 104 : 285 / 491 = 58 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 105 : 274 / 491 = 55 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 106 : 263 / 492 = 53 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 107 : 417 / 491 = 84 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 108 : 396 / 492 = 80 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 109 : 323 / 490 = 65 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 110 : 412 / 491 = 83 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 111 : 458 / 492 = 93 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 112 : 456 / 492 = 92 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 113 : 439 / 491 = 89 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 114 : 409 / 491 = 83 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 115 : 439 / 492 = 89 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 116 : 391 / 491 = 79 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 117 : 428 / 492 = 86 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 118 : 442 / 490 = 90 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 119 : 451 / 492 = 91 %
[ Fri Jul 12 21:32:55 2024 ] Accuracy of 120 : 421 / 500 = 84 %
[ Fri Jul 12 21:32:55 2024 ] Training epoch: 71
[ Fri Jul 12 21:32:56 2024 ] 	Batch(0/6809) done. Loss: 0.0888  lr:0.000100
[ Fri Jul 12 21:33:14 2024 ] 	Batch(100/6809) done. Loss: 0.0190  lr:0.000100
[ Fri Jul 12 21:33:32 2024 ] 	Batch(200/6809) done. Loss: 0.0303  lr:0.000100
[ Fri Jul 12 21:33:50 2024 ] 	Batch(300/6809) done. Loss: 0.3007  lr:0.000100
[ Fri Jul 12 21:34:08 2024 ] 	Batch(400/6809) done. Loss: 0.0572  lr:0.000100
[ Fri Jul 12 21:34:25 2024 ] 
Training: Epoch [70/120], Step [499], Loss: 0.035313934087753296, Training Accuracy: 96.675
[ Fri Jul 12 21:34:25 2024 ] 	Batch(500/6809) done. Loss: 0.0115  lr:0.000100
[ Fri Jul 12 21:34:43 2024 ] 	Batch(600/6809) done. Loss: 0.0484  lr:0.000100
[ Fri Jul 12 21:35:01 2024 ] 	Batch(700/6809) done. Loss: 0.1175  lr:0.000100
[ Fri Jul 12 21:35:20 2024 ] 	Batch(800/6809) done. Loss: 0.0403  lr:0.000100
[ Fri Jul 12 21:35:38 2024 ] 	Batch(900/6809) done. Loss: 0.0054  lr:0.000100
[ Fri Jul 12 21:35:57 2024 ] 
Training: Epoch [70/120], Step [999], Loss: 0.25020167231559753, Training Accuracy: 96.775
[ Fri Jul 12 21:35:57 2024 ] 	Batch(1000/6809) done. Loss: 0.1682  lr:0.000100
[ Fri Jul 12 21:36:15 2024 ] 	Batch(1100/6809) done. Loss: 0.0634  lr:0.000100
[ Fri Jul 12 21:36:34 2024 ] 	Batch(1200/6809) done. Loss: 0.1314  lr:0.000100
[ Fri Jul 12 21:36:52 2024 ] 	Batch(1300/6809) done. Loss: 0.0308  lr:0.000100
[ Fri Jul 12 21:37:10 2024 ] 	Batch(1400/6809) done. Loss: 0.1564  lr:0.000100
[ Fri Jul 12 21:37:28 2024 ] 
Training: Epoch [70/120], Step [1499], Loss: 0.014282372780144215, Training Accuracy: 96.65833333333333
[ Fri Jul 12 21:37:28 2024 ] 	Batch(1500/6809) done. Loss: 0.1350  lr:0.000100
[ Fri Jul 12 21:37:46 2024 ] 	Batch(1600/6809) done. Loss: 0.0169  lr:0.000100
[ Fri Jul 12 21:38:04 2024 ] 	Batch(1700/6809) done. Loss: 0.0090  lr:0.000100
[ Fri Jul 12 21:38:21 2024 ] 	Batch(1800/6809) done. Loss: 0.0682  lr:0.000100
[ Fri Jul 12 21:38:39 2024 ] 	Batch(1900/6809) done. Loss: 0.0500  lr:0.000100
[ Fri Jul 12 21:38:57 2024 ] 
Training: Epoch [70/120], Step [1999], Loss: 0.0600927397608757, Training Accuracy: 96.64375
[ Fri Jul 12 21:38:57 2024 ] 	Batch(2000/6809) done. Loss: 0.0607  lr:0.000100
[ Fri Jul 12 21:39:15 2024 ] 	Batch(2100/6809) done. Loss: 0.2219  lr:0.000100
[ Fri Jul 12 21:39:33 2024 ] 	Batch(2200/6809) done. Loss: 0.0747  lr:0.000100
[ Fri Jul 12 21:39:51 2024 ] 	Batch(2300/6809) done. Loss: 0.2564  lr:0.000100
[ Fri Jul 12 21:40:09 2024 ] 	Batch(2400/6809) done. Loss: 0.1350  lr:0.000100
[ Fri Jul 12 21:40:28 2024 ] 
Training: Epoch [70/120], Step [2499], Loss: 0.08079009503126144, Training Accuracy: 96.67
[ Fri Jul 12 21:40:28 2024 ] 	Batch(2500/6809) done. Loss: 0.0097  lr:0.000100
[ Fri Jul 12 21:40:46 2024 ] 	Batch(2600/6809) done. Loss: 0.1307  lr:0.000100
[ Fri Jul 12 21:41:05 2024 ] 	Batch(2700/6809) done. Loss: 0.0056  lr:0.000100
[ Fri Jul 12 21:41:24 2024 ] 	Batch(2800/6809) done. Loss: 0.0289  lr:0.000100
[ Fri Jul 12 21:41:41 2024 ] 	Batch(2900/6809) done. Loss: 0.0775  lr:0.000100
[ Fri Jul 12 21:41:59 2024 ] 
Training: Epoch [70/120], Step [2999], Loss: 0.1220252588391304, Training Accuracy: 96.65416666666667
[ Fri Jul 12 21:41:59 2024 ] 	Batch(3000/6809) done. Loss: 0.1265  lr:0.000100
[ Fri Jul 12 21:42:17 2024 ] 	Batch(3100/6809) done. Loss: 0.0240  lr:0.000100
[ Fri Jul 12 21:42:35 2024 ] 	Batch(3200/6809) done. Loss: 0.0069  lr:0.000100
[ Fri Jul 12 21:42:53 2024 ] 	Batch(3300/6809) done. Loss: 0.0717  lr:0.000100
[ Fri Jul 12 21:43:11 2024 ] 	Batch(3400/6809) done. Loss: 0.5043  lr:0.000100
[ Fri Jul 12 21:43:29 2024 ] 
Training: Epoch [70/120], Step [3499], Loss: 0.01168636791408062, Training Accuracy: 96.67857142857143
[ Fri Jul 12 21:43:29 2024 ] 	Batch(3500/6809) done. Loss: 0.1837  lr:0.000100
[ Fri Jul 12 21:43:47 2024 ] 	Batch(3600/6809) done. Loss: 0.1941  lr:0.000100
[ Fri Jul 12 21:44:05 2024 ] 	Batch(3700/6809) done. Loss: 0.0997  lr:0.000100
[ Fri Jul 12 21:44:22 2024 ] 	Batch(3800/6809) done. Loss: 0.8433  lr:0.000100
[ Fri Jul 12 21:44:40 2024 ] 	Batch(3900/6809) done. Loss: 0.0481  lr:0.000100
[ Fri Jul 12 21:44:58 2024 ] 
Training: Epoch [70/120], Step [3999], Loss: 0.2923874855041504, Training Accuracy: 96.71875
[ Fri Jul 12 21:44:58 2024 ] 	Batch(4000/6809) done. Loss: 0.0264  lr:0.000100
[ Fri Jul 12 21:45:16 2024 ] 	Batch(4100/6809) done. Loss: 0.2385  lr:0.000100
[ Fri Jul 12 21:45:34 2024 ] 	Batch(4200/6809) done. Loss: 0.0175  lr:0.000100
[ Fri Jul 12 21:45:52 2024 ] 	Batch(4300/6809) done. Loss: 0.0344  lr:0.000100
[ Fri Jul 12 21:46:10 2024 ] 	Batch(4400/6809) done. Loss: 0.1099  lr:0.000100
[ Fri Jul 12 21:46:28 2024 ] 
Training: Epoch [70/120], Step [4499], Loss: 0.03607737645506859, Training Accuracy: 96.69722222222222
[ Fri Jul 12 21:46:29 2024 ] 	Batch(4500/6809) done. Loss: 0.1302  lr:0.000100
[ Fri Jul 12 21:46:46 2024 ] 	Batch(4600/6809) done. Loss: 0.4076  lr:0.000100
[ Fri Jul 12 21:47:04 2024 ] 	Batch(4700/6809) done. Loss: 0.0261  lr:0.000100
[ Fri Jul 12 21:47:22 2024 ] 	Batch(4800/6809) done. Loss: 0.0907  lr:0.000100
[ Fri Jul 12 21:47:40 2024 ] 	Batch(4900/6809) done. Loss: 0.5075  lr:0.000100
[ Fri Jul 12 21:47:58 2024 ] 
Training: Epoch [70/120], Step [4999], Loss: 0.08026295155286789, Training Accuracy: 96.695
[ Fri Jul 12 21:47:58 2024 ] 	Batch(5000/6809) done. Loss: 0.0217  lr:0.000100
[ Fri Jul 12 21:48:16 2024 ] 	Batch(5100/6809) done. Loss: 0.0203  lr:0.000100
[ Fri Jul 12 21:48:34 2024 ] 	Batch(5200/6809) done. Loss: 0.1764  lr:0.000100
[ Fri Jul 12 21:48:52 2024 ] 	Batch(5300/6809) done. Loss: 0.1770  lr:0.000100
[ Fri Jul 12 21:49:10 2024 ] 	Batch(5400/6809) done. Loss: 0.4910  lr:0.000100
[ Fri Jul 12 21:49:27 2024 ] 
Training: Epoch [70/120], Step [5499], Loss: 0.018075283616781235, Training Accuracy: 96.67727272727272
[ Fri Jul 12 21:49:28 2024 ] 	Batch(5500/6809) done. Loss: 0.3664  lr:0.000100
[ Fri Jul 12 21:49:46 2024 ] 	Batch(5600/6809) done. Loss: 0.0106  lr:0.000100
[ Fri Jul 12 21:50:04 2024 ] 	Batch(5700/6809) done. Loss: 0.0837  lr:0.000100
[ Fri Jul 12 21:50:23 2024 ] 	Batch(5800/6809) done. Loss: 0.1144  lr:0.000100
[ Fri Jul 12 21:50:40 2024 ] 	Batch(5900/6809) done. Loss: 0.0125  lr:0.000100
[ Fri Jul 12 21:50:58 2024 ] 
Training: Epoch [70/120], Step [5999], Loss: 0.03499404713511467, Training Accuracy: 96.75625
[ Fri Jul 12 21:50:59 2024 ] 	Batch(6000/6809) done. Loss: 0.0082  lr:0.000100
[ Fri Jul 12 21:51:17 2024 ] 	Batch(6100/6809) done. Loss: 0.0088  lr:0.000100
[ Fri Jul 12 21:51:36 2024 ] 	Batch(6200/6809) done. Loss: 0.0031  lr:0.000100
[ Fri Jul 12 21:51:54 2024 ] 	Batch(6300/6809) done. Loss: 0.2569  lr:0.000100
[ Fri Jul 12 21:52:13 2024 ] 	Batch(6400/6809) done. Loss: 0.0166  lr:0.000100
[ Fri Jul 12 21:52:30 2024 ] 
Training: Epoch [70/120], Step [6499], Loss: 0.09345626831054688, Training Accuracy: 96.79423076923077
[ Fri Jul 12 21:52:31 2024 ] 	Batch(6500/6809) done. Loss: 0.0041  lr:0.000100
[ Fri Jul 12 21:52:48 2024 ] 	Batch(6600/6809) done. Loss: 0.0676  lr:0.000100
[ Fri Jul 12 21:53:06 2024 ] 	Batch(6700/6809) done. Loss: 0.1327  lr:0.000100
[ Fri Jul 12 21:53:24 2024 ] 	Batch(6800/6809) done. Loss: 0.1291  lr:0.000100
[ Fri Jul 12 21:53:26 2024 ] 	Mean training loss: 0.1232.
[ Fri Jul 12 21:53:26 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 21:53:26 2024 ] Training epoch: 72
[ Fri Jul 12 21:53:27 2024 ] 	Batch(0/6809) done. Loss: 0.2009  lr:0.000100
[ Fri Jul 12 21:53:44 2024 ] 	Batch(100/6809) done. Loss: 0.0051  lr:0.000100
[ Fri Jul 12 21:54:02 2024 ] 	Batch(200/6809) done. Loss: 0.0504  lr:0.000100
[ Fri Jul 12 21:54:20 2024 ] 	Batch(300/6809) done. Loss: 0.0699  lr:0.000100
[ Fri Jul 12 21:54:38 2024 ] 	Batch(400/6809) done. Loss: 0.2186  lr:0.000100
[ Fri Jul 12 21:54:56 2024 ] 
Training: Epoch [71/120], Step [499], Loss: 0.019311802461743355, Training Accuracy: 96.275
[ Fri Jul 12 21:54:56 2024 ] 	Batch(500/6809) done. Loss: 0.2523  lr:0.000100
[ Fri Jul 12 21:55:14 2024 ] 	Batch(600/6809) done. Loss: 0.0271  lr:0.000100
[ Fri Jul 12 21:55:32 2024 ] 	Batch(700/6809) done. Loss: 0.0044  lr:0.000100
[ Fri Jul 12 21:55:50 2024 ] 	Batch(800/6809) done. Loss: 0.0503  lr:0.000100
[ Fri Jul 12 21:56:07 2024 ] 	Batch(900/6809) done. Loss: 0.0776  lr:0.000100
[ Fri Jul 12 21:56:25 2024 ] 
Training: Epoch [71/120], Step [999], Loss: 0.049538299441337585, Training Accuracy: 96.65
[ Fri Jul 12 21:56:25 2024 ] 	Batch(1000/6809) done. Loss: 0.2509  lr:0.000100
[ Fri Jul 12 21:56:43 2024 ] 	Batch(1100/6809) done. Loss: 0.0858  lr:0.000100
[ Fri Jul 12 21:57:01 2024 ] 	Batch(1200/6809) done. Loss: 0.0235  lr:0.000100
[ Fri Jul 12 21:57:19 2024 ] 	Batch(1300/6809) done. Loss: 0.0298  lr:0.000100
[ Fri Jul 12 21:57:37 2024 ] 	Batch(1400/6809) done. Loss: 0.0166  lr:0.000100
[ Fri Jul 12 21:57:55 2024 ] 
Training: Epoch [71/120], Step [1499], Loss: 0.13989470899105072, Training Accuracy: 96.775
[ Fri Jul 12 21:57:55 2024 ] 	Batch(1500/6809) done. Loss: 0.0382  lr:0.000100
[ Fri Jul 12 21:58:13 2024 ] 	Batch(1600/6809) done. Loss: 0.1206  lr:0.000100
[ Fri Jul 12 21:58:31 2024 ] 	Batch(1700/6809) done. Loss: 0.1371  lr:0.000100
[ Fri Jul 12 21:58:49 2024 ] 	Batch(1800/6809) done. Loss: 0.0045  lr:0.000100
[ Fri Jul 12 21:59:06 2024 ] 	Batch(1900/6809) done. Loss: 0.1334  lr:0.000100
[ Fri Jul 12 21:59:24 2024 ] 
Training: Epoch [71/120], Step [1999], Loss: 0.09369522333145142, Training Accuracy: 96.76249999999999
[ Fri Jul 12 21:59:24 2024 ] 	Batch(2000/6809) done. Loss: 0.0413  lr:0.000100
[ Fri Jul 12 21:59:42 2024 ] 	Batch(2100/6809) done. Loss: 0.0193  lr:0.000100
[ Fri Jul 12 22:00:00 2024 ] 	Batch(2200/6809) done. Loss: 0.1216  lr:0.000100
[ Fri Jul 12 22:00:18 2024 ] 	Batch(2300/6809) done. Loss: 0.4376  lr:0.000100
[ Fri Jul 12 22:00:36 2024 ] 	Batch(2400/6809) done. Loss: 0.0090  lr:0.000100
[ Fri Jul 12 22:00:54 2024 ] 
Training: Epoch [71/120], Step [2499], Loss: 0.050306178629398346, Training Accuracy: 96.88
[ Fri Jul 12 22:00:54 2024 ] 	Batch(2500/6809) done. Loss: 0.0155  lr:0.000100
[ Fri Jul 12 22:01:12 2024 ] 	Batch(2600/6809) done. Loss: 0.0774  lr:0.000100
[ Fri Jul 12 22:01:30 2024 ] 	Batch(2700/6809) done. Loss: 0.1575  lr:0.000100
[ Fri Jul 12 22:01:48 2024 ] 	Batch(2800/6809) done. Loss: 0.0247  lr:0.000100
[ Fri Jul 12 22:02:06 2024 ] 	Batch(2900/6809) done. Loss: 0.2686  lr:0.000100
[ Fri Jul 12 22:02:24 2024 ] 
Training: Epoch [71/120], Step [2999], Loss: 0.030504755675792694, Training Accuracy: 96.8375
[ Fri Jul 12 22:02:24 2024 ] 	Batch(3000/6809) done. Loss: 0.0897  lr:0.000100
[ Fri Jul 12 22:02:42 2024 ] 	Batch(3100/6809) done. Loss: 0.3947  lr:0.000100
[ Fri Jul 12 22:03:00 2024 ] 	Batch(3200/6809) done. Loss: 0.1314  lr:0.000100
[ Fri Jul 12 22:03:18 2024 ] 	Batch(3300/6809) done. Loss: 0.2116  lr:0.000100
[ Fri Jul 12 22:03:36 2024 ] 	Batch(3400/6809) done. Loss: 0.0155  lr:0.000100
[ Fri Jul 12 22:03:53 2024 ] 
Training: Epoch [71/120], Step [3499], Loss: 0.13239049911499023, Training Accuracy: 96.85714285714285
[ Fri Jul 12 22:03:54 2024 ] 	Batch(3500/6809) done. Loss: 0.1767  lr:0.000100
[ Fri Jul 12 22:04:12 2024 ] 	Batch(3600/6809) done. Loss: 0.1309  lr:0.000100
[ Fri Jul 12 22:04:30 2024 ] 	Batch(3700/6809) done. Loss: 0.0129  lr:0.000100
[ Fri Jul 12 22:04:47 2024 ] 	Batch(3800/6809) done. Loss: 0.0488  lr:0.000100
[ Fri Jul 12 22:05:05 2024 ] 	Batch(3900/6809) done. Loss: 0.0856  lr:0.000100
[ Fri Jul 12 22:05:23 2024 ] 
Training: Epoch [71/120], Step [3999], Loss: 0.3593722879886627, Training Accuracy: 96.89687500000001
[ Fri Jul 12 22:05:23 2024 ] 	Batch(4000/6809) done. Loss: 0.3376  lr:0.000100
[ Fri Jul 12 22:05:42 2024 ] 	Batch(4100/6809) done. Loss: 0.1840  lr:0.000100
[ Fri Jul 12 22:06:00 2024 ] 	Batch(4200/6809) done. Loss: 0.1003  lr:0.000100
[ Fri Jul 12 22:06:19 2024 ] 	Batch(4300/6809) done. Loss: 0.0271  lr:0.000100
[ Fri Jul 12 22:06:38 2024 ] 	Batch(4400/6809) done. Loss: 0.0082  lr:0.000100
[ Fri Jul 12 22:06:56 2024 ] 
Training: Epoch [71/120], Step [4499], Loss: 0.02035822719335556, Training Accuracy: 96.87777777777778
[ Fri Jul 12 22:06:56 2024 ] 	Batch(4500/6809) done. Loss: 0.0237  lr:0.000100
[ Fri Jul 12 22:07:15 2024 ] 	Batch(4600/6809) done. Loss: 0.0842  lr:0.000100
[ Fri Jul 12 22:07:33 2024 ] 	Batch(4700/6809) done. Loss: 0.1074  lr:0.000100
[ Fri Jul 12 22:07:52 2024 ] 	Batch(4800/6809) done. Loss: 0.0380  lr:0.000100
[ Fri Jul 12 22:08:11 2024 ] 	Batch(4900/6809) done. Loss: 0.0156  lr:0.000100
[ Fri Jul 12 22:08:29 2024 ] 
Training: Epoch [71/120], Step [4999], Loss: 0.03785227984189987, Training Accuracy: 96.85000000000001
[ Fri Jul 12 22:08:29 2024 ] 	Batch(5000/6809) done. Loss: 0.4354  lr:0.000100
[ Fri Jul 12 22:08:48 2024 ] 	Batch(5100/6809) done. Loss: 0.0502  lr:0.000100
[ Fri Jul 12 22:09:07 2024 ] 	Batch(5200/6809) done. Loss: 0.1889  lr:0.000100
[ Fri Jul 12 22:09:25 2024 ] 	Batch(5300/6809) done. Loss: 0.1109  lr:0.000100
[ Fri Jul 12 22:09:44 2024 ] 	Batch(5400/6809) done. Loss: 0.0326  lr:0.000100
[ Fri Jul 12 22:10:02 2024 ] 
Training: Epoch [71/120], Step [5499], Loss: 0.01266616489738226, Training Accuracy: 96.80681818181817
[ Fri Jul 12 22:10:02 2024 ] 	Batch(5500/6809) done. Loss: 0.1486  lr:0.000100
[ Fri Jul 12 22:10:21 2024 ] 	Batch(5600/6809) done. Loss: 0.2460  lr:0.000100
[ Fri Jul 12 22:10:39 2024 ] 	Batch(5700/6809) done. Loss: 0.6681  lr:0.000100
[ Fri Jul 12 22:10:57 2024 ] 	Batch(5800/6809) done. Loss: 0.6894  lr:0.000100
[ Fri Jul 12 22:11:15 2024 ] 	Batch(5900/6809) done. Loss: 0.1012  lr:0.000100
[ Fri Jul 12 22:11:32 2024 ] 
Training: Epoch [71/120], Step [5999], Loss: 0.06881145387887955, Training Accuracy: 96.77708333333334
[ Fri Jul 12 22:11:33 2024 ] 	Batch(6000/6809) done. Loss: 0.2653  lr:0.000100
[ Fri Jul 12 22:11:51 2024 ] 	Batch(6100/6809) done. Loss: 0.0195  lr:0.000100
[ Fri Jul 12 22:12:10 2024 ] 	Batch(6200/6809) done. Loss: 0.0348  lr:0.000100
[ Fri Jul 12 22:12:28 2024 ] 	Batch(6300/6809) done. Loss: 0.0132  lr:0.000100
[ Fri Jul 12 22:12:47 2024 ] 	Batch(6400/6809) done. Loss: 0.0676  lr:0.000100
[ Fri Jul 12 22:13:05 2024 ] 
Training: Epoch [71/120], Step [6499], Loss: 0.011069884523749352, Training Accuracy: 96.78846153846153
[ Fri Jul 12 22:13:05 2024 ] 	Batch(6500/6809) done. Loss: 0.2435  lr:0.000100
[ Fri Jul 12 22:13:23 2024 ] 	Batch(6600/6809) done. Loss: 0.0329  lr:0.000100
[ Fri Jul 12 22:13:41 2024 ] 	Batch(6700/6809) done. Loss: 0.1311  lr:0.000100
[ Fri Jul 12 22:13:59 2024 ] 	Batch(6800/6809) done. Loss: 0.1065  lr:0.000100
[ Fri Jul 12 22:14:00 2024 ] 	Mean training loss: 0.1190.
[ Fri Jul 12 22:14:00 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 22:14:00 2024 ] Training epoch: 73
[ Fri Jul 12 22:14:01 2024 ] 	Batch(0/6809) done. Loss: 0.0178  lr:0.000100
[ Fri Jul 12 22:14:19 2024 ] 	Batch(100/6809) done. Loss: 0.0550  lr:0.000100
[ Fri Jul 12 22:14:37 2024 ] 	Batch(200/6809) done. Loss: 0.0699  lr:0.000100
[ Fri Jul 12 22:14:55 2024 ] 	Batch(300/6809) done. Loss: 0.0501  lr:0.000100
[ Fri Jul 12 22:15:13 2024 ] 	Batch(400/6809) done. Loss: 0.0005  lr:0.000100
[ Fri Jul 12 22:15:30 2024 ] 
Training: Epoch [72/120], Step [499], Loss: 0.1795567274093628, Training Accuracy: 97.0
[ Fri Jul 12 22:15:31 2024 ] 	Batch(500/6809) done. Loss: 0.0692  lr:0.000100
[ Fri Jul 12 22:15:48 2024 ] 	Batch(600/6809) done. Loss: 0.1670  lr:0.000100
[ Fri Jul 12 22:16:06 2024 ] 	Batch(700/6809) done. Loss: 0.1342  lr:0.000100
[ Fri Jul 12 22:16:24 2024 ] 	Batch(800/6809) done. Loss: 0.0408  lr:0.000100
[ Fri Jul 12 22:16:42 2024 ] 	Batch(900/6809) done. Loss: 0.2202  lr:0.000100
[ Fri Jul 12 22:17:00 2024 ] 
Training: Epoch [72/120], Step [999], Loss: 0.01894252561032772, Training Accuracy: 96.925
[ Fri Jul 12 22:17:00 2024 ] 	Batch(1000/6809) done. Loss: 0.2772  lr:0.000100
[ Fri Jul 12 22:17:18 2024 ] 	Batch(1100/6809) done. Loss: 0.5574  lr:0.000100
[ Fri Jul 12 22:17:36 2024 ] 	Batch(1200/6809) done. Loss: 0.0205  lr:0.000100
[ Fri Jul 12 22:17:54 2024 ] 	Batch(1300/6809) done. Loss: 0.0353  lr:0.000100
[ Fri Jul 12 22:18:13 2024 ] 	Batch(1400/6809) done. Loss: 0.2885  lr:0.000100
[ Fri Jul 12 22:18:31 2024 ] 
Training: Epoch [72/120], Step [1499], Loss: 0.37082529067993164, Training Accuracy: 97.00833333333333
[ Fri Jul 12 22:18:31 2024 ] 	Batch(1500/6809) done. Loss: 0.0101  lr:0.000100
[ Fri Jul 12 22:18:49 2024 ] 	Batch(1600/6809) done. Loss: 0.1744  lr:0.000100
[ Fri Jul 12 22:19:07 2024 ] 	Batch(1700/6809) done. Loss: 0.0535  lr:0.000100
[ Fri Jul 12 22:19:25 2024 ] 	Batch(1800/6809) done. Loss: 0.0810  lr:0.000100
[ Fri Jul 12 22:19:43 2024 ] 	Batch(1900/6809) done. Loss: 0.0060  lr:0.000100
[ Fri Jul 12 22:20:01 2024 ] 
Training: Epoch [72/120], Step [1999], Loss: 0.04217114299535751, Training Accuracy: 97.02499999999999
[ Fri Jul 12 22:20:01 2024 ] 	Batch(2000/6809) done. Loss: 0.0915  lr:0.000100
[ Fri Jul 12 22:20:19 2024 ] 	Batch(2100/6809) done. Loss: 0.0018  lr:0.000100
[ Fri Jul 12 22:20:37 2024 ] 	Batch(2200/6809) done. Loss: 0.1482  lr:0.000100
[ Fri Jul 12 22:20:55 2024 ] 	Batch(2300/6809) done. Loss: 0.0392  lr:0.000100
[ Fri Jul 12 22:21:13 2024 ] 	Batch(2400/6809) done. Loss: 0.0373  lr:0.000100
[ Fri Jul 12 22:21:30 2024 ] 
Training: Epoch [72/120], Step [2499], Loss: 0.03830733522772789, Training Accuracy: 96.99
[ Fri Jul 12 22:21:31 2024 ] 	Batch(2500/6809) done. Loss: 0.0160  lr:0.000100
[ Fri Jul 12 22:21:49 2024 ] 	Batch(2600/6809) done. Loss: 0.0113  lr:0.000100
[ Fri Jul 12 22:22:06 2024 ] 	Batch(2700/6809) done. Loss: 0.2173  lr:0.000100
[ Fri Jul 12 22:22:25 2024 ] 	Batch(2800/6809) done. Loss: 0.1705  lr:0.000100
[ Fri Jul 12 22:22:43 2024 ] 	Batch(2900/6809) done. Loss: 0.0210  lr:0.000100
[ Fri Jul 12 22:23:00 2024 ] 
Training: Epoch [72/120], Step [2999], Loss: 0.07739662379026413, Training Accuracy: 96.85833333333333
[ Fri Jul 12 22:23:00 2024 ] 	Batch(3000/6809) done. Loss: 0.0970  lr:0.000100
[ Fri Jul 12 22:23:18 2024 ] 	Batch(3100/6809) done. Loss: 0.1110  lr:0.000100
[ Fri Jul 12 22:23:36 2024 ] 	Batch(3200/6809) done. Loss: 0.2119  lr:0.000100
[ Fri Jul 12 22:23:54 2024 ] 	Batch(3300/6809) done. Loss: 0.0100  lr:0.000100
[ Fri Jul 12 22:24:12 2024 ] 	Batch(3400/6809) done. Loss: 0.1032  lr:0.000100
[ Fri Jul 12 22:24:30 2024 ] 
Training: Epoch [72/120], Step [3499], Loss: 0.18861675262451172, Training Accuracy: 96.91428571428573
[ Fri Jul 12 22:24:30 2024 ] 	Batch(3500/6809) done. Loss: 0.1455  lr:0.000100
[ Fri Jul 12 22:24:48 2024 ] 	Batch(3600/6809) done. Loss: 0.4080  lr:0.000100
[ Fri Jul 12 22:25:06 2024 ] 	Batch(3700/6809) done. Loss: 0.3148  lr:0.000100
[ Fri Jul 12 22:25:24 2024 ] 	Batch(3800/6809) done. Loss: 0.0355  lr:0.000100
[ Fri Jul 12 22:25:42 2024 ] 	Batch(3900/6809) done. Loss: 0.1148  lr:0.000100
[ Fri Jul 12 22:26:00 2024 ] 
Training: Epoch [72/120], Step [3999], Loss: 0.16522468626499176, Training Accuracy: 96.9125
[ Fri Jul 12 22:26:00 2024 ] 	Batch(4000/6809) done. Loss: 0.6358  lr:0.000100
[ Fri Jul 12 22:26:18 2024 ] 	Batch(4100/6809) done. Loss: 0.0978  lr:0.000100
[ Fri Jul 12 22:26:36 2024 ] 	Batch(4200/6809) done. Loss: 0.0911  lr:0.000100
[ Fri Jul 12 22:26:54 2024 ] 	Batch(4300/6809) done. Loss: 0.0866  lr:0.000100
[ Fri Jul 12 22:27:12 2024 ] 	Batch(4400/6809) done. Loss: 0.0216  lr:0.000100
[ Fri Jul 12 22:27:30 2024 ] 
Training: Epoch [72/120], Step [4499], Loss: 0.2240598499774933, Training Accuracy: 96.86111111111111
[ Fri Jul 12 22:27:30 2024 ] 	Batch(4500/6809) done. Loss: 0.0104  lr:0.000100
[ Fri Jul 12 22:27:49 2024 ] 	Batch(4600/6809) done. Loss: 0.0559  lr:0.000100
[ Fri Jul 12 22:28:08 2024 ] 	Batch(4700/6809) done. Loss: 0.1191  lr:0.000100
[ Fri Jul 12 22:28:26 2024 ] 	Batch(4800/6809) done. Loss: 0.0144  lr:0.000100
[ Fri Jul 12 22:28:45 2024 ] 	Batch(4900/6809) done. Loss: 0.0016  lr:0.000100
[ Fri Jul 12 22:29:03 2024 ] 
Training: Epoch [72/120], Step [4999], Loss: 0.11560603976249695, Training Accuracy: 96.905
[ Fri Jul 12 22:29:03 2024 ] 	Batch(5000/6809) done. Loss: 0.1499  lr:0.000100
[ Fri Jul 12 22:29:22 2024 ] 	Batch(5100/6809) done. Loss: 0.0101  lr:0.000100
[ Fri Jul 12 22:29:40 2024 ] 	Batch(5200/6809) done. Loss: 0.0131  lr:0.000100
[ Fri Jul 12 22:29:58 2024 ] 	Batch(5300/6809) done. Loss: 0.0098  lr:0.000100
[ Fri Jul 12 22:30:16 2024 ] 	Batch(5400/6809) done. Loss: 0.1395  lr:0.000100
[ Fri Jul 12 22:30:33 2024 ] 
Training: Epoch [72/120], Step [5499], Loss: 0.02694859728217125, Training Accuracy: 96.9090909090909
[ Fri Jul 12 22:30:33 2024 ] 	Batch(5500/6809) done. Loss: 0.0460  lr:0.000100
[ Fri Jul 12 22:30:51 2024 ] 	Batch(5600/6809) done. Loss: 0.0059  lr:0.000100
[ Fri Jul 12 22:31:09 2024 ] 	Batch(5700/6809) done. Loss: 0.1542  lr:0.000100
[ Fri Jul 12 22:31:27 2024 ] 	Batch(5800/6809) done. Loss: 0.2326  lr:0.000100
[ Fri Jul 12 22:31:45 2024 ] 	Batch(5900/6809) done. Loss: 0.2185  lr:0.000100
[ Fri Jul 12 22:32:03 2024 ] 
Training: Epoch [72/120], Step [5999], Loss: 0.13661682605743408, Training Accuracy: 96.9375
[ Fri Jul 12 22:32:03 2024 ] 	Batch(6000/6809) done. Loss: 0.1340  lr:0.000100
[ Fri Jul 12 22:32:22 2024 ] 	Batch(6100/6809) done. Loss: 0.1467  lr:0.000100
[ Fri Jul 12 22:32:41 2024 ] 	Batch(6200/6809) done. Loss: 0.1066  lr:0.000100
[ Fri Jul 12 22:32:59 2024 ] 	Batch(6300/6809) done. Loss: 0.0064  lr:0.000100
[ Fri Jul 12 22:33:18 2024 ] 	Batch(6400/6809) done. Loss: 0.1438  lr:0.000100
[ Fri Jul 12 22:33:36 2024 ] 
Training: Epoch [72/120], Step [6499], Loss: 0.04664980620145798, Training Accuracy: 96.91923076923077
[ Fri Jul 12 22:33:36 2024 ] 	Batch(6500/6809) done. Loss: 0.1256  lr:0.000100
[ Fri Jul 12 22:33:54 2024 ] 	Batch(6600/6809) done. Loss: 0.0415  lr:0.000100
[ Fri Jul 12 22:34:12 2024 ] 	Batch(6700/6809) done. Loss: 0.0167  lr:0.000100
[ Fri Jul 12 22:34:30 2024 ] 	Batch(6800/6809) done. Loss: 0.0750  lr:0.000100
[ Fri Jul 12 22:34:31 2024 ] 	Mean training loss: 0.1182.
[ Fri Jul 12 22:34:31 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 22:34:32 2024 ] Training epoch: 74
[ Fri Jul 12 22:34:32 2024 ] 	Batch(0/6809) done. Loss: 0.1950  lr:0.000100
[ Fri Jul 12 22:34:50 2024 ] 	Batch(100/6809) done. Loss: 0.0875  lr:0.000100
[ Fri Jul 12 22:35:08 2024 ] 	Batch(200/6809) done. Loss: 0.0569  lr:0.000100
[ Fri Jul 12 22:35:26 2024 ] 	Batch(300/6809) done. Loss: 0.0198  lr:0.000100
[ Fri Jul 12 22:35:44 2024 ] 	Batch(400/6809) done. Loss: 0.0191  lr:0.000100
[ Fri Jul 12 22:36:01 2024 ] 
Training: Epoch [73/120], Step [499], Loss: 0.19542084634304047, Training Accuracy: 96.875
[ Fri Jul 12 22:36:02 2024 ] 	Batch(500/6809) done. Loss: 0.2361  lr:0.000100
[ Fri Jul 12 22:36:20 2024 ] 	Batch(600/6809) done. Loss: 0.0231  lr:0.000100
[ Fri Jul 12 22:36:37 2024 ] 	Batch(700/6809) done. Loss: 0.4649  lr:0.000100
[ Fri Jul 12 22:36:55 2024 ] 	Batch(800/6809) done. Loss: 0.2471  lr:0.000100
[ Fri Jul 12 22:37:13 2024 ] 	Batch(900/6809) done. Loss: 0.0831  lr:0.000100
[ Fri Jul 12 22:37:31 2024 ] 
Training: Epoch [73/120], Step [999], Loss: 0.11543696373701096, Training Accuracy: 96.875
[ Fri Jul 12 22:37:31 2024 ] 	Batch(1000/6809) done. Loss: 0.0254  lr:0.000100
[ Fri Jul 12 22:37:49 2024 ] 	Batch(1100/6809) done. Loss: 0.0064  lr:0.000100
[ Fri Jul 12 22:38:07 2024 ] 	Batch(1200/6809) done. Loss: 0.1107  lr:0.000100
[ Fri Jul 12 22:38:25 2024 ] 	Batch(1300/6809) done. Loss: 0.1517  lr:0.000100
[ Fri Jul 12 22:38:43 2024 ] 	Batch(1400/6809) done. Loss: 0.2300  lr:0.000100
[ Fri Jul 12 22:39:00 2024 ] 
Training: Epoch [73/120], Step [1499], Loss: 0.22431132197380066, Training Accuracy: 97.05833333333334
[ Fri Jul 12 22:39:01 2024 ] 	Batch(1500/6809) done. Loss: 0.4324  lr:0.000100
[ Fri Jul 12 22:39:19 2024 ] 	Batch(1600/6809) done. Loss: 0.2845  lr:0.000100
[ Fri Jul 12 22:39:37 2024 ] 	Batch(1700/6809) done. Loss: 0.0758  lr:0.000100
[ Fri Jul 12 22:39:55 2024 ] 	Batch(1800/6809) done. Loss: 0.0134  lr:0.000100
[ Fri Jul 12 22:40:12 2024 ] 	Batch(1900/6809) done. Loss: 0.3468  lr:0.000100
[ Fri Jul 12 22:40:30 2024 ] 
Training: Epoch [73/120], Step [1999], Loss: 0.017503837123513222, Training Accuracy: 97.03125
[ Fri Jul 12 22:40:30 2024 ] 	Batch(2000/6809) done. Loss: 0.0228  lr:0.000100
[ Fri Jul 12 22:40:48 2024 ] 	Batch(2100/6809) done. Loss: 0.0713  lr:0.000100
[ Fri Jul 12 22:41:06 2024 ] 	Batch(2200/6809) done. Loss: 0.0718  lr:0.000100
[ Fri Jul 12 22:41:24 2024 ] 	Batch(2300/6809) done. Loss: 0.2334  lr:0.000100
[ Fri Jul 12 22:41:42 2024 ] 	Batch(2400/6809) done. Loss: 0.0446  lr:0.000100
[ Fri Jul 12 22:42:00 2024 ] 
Training: Epoch [73/120], Step [2499], Loss: 0.006768574472516775, Training Accuracy: 97.06
[ Fri Jul 12 22:42:00 2024 ] 	Batch(2500/6809) done. Loss: 0.0023  lr:0.000100
[ Fri Jul 12 22:42:18 2024 ] 	Batch(2600/6809) done. Loss: 0.1200  lr:0.000100
[ Fri Jul 12 22:42:36 2024 ] 	Batch(2700/6809) done. Loss: 0.3329  lr:0.000100
[ Fri Jul 12 22:42:54 2024 ] 	Batch(2800/6809) done. Loss: 0.0774  lr:0.000100
[ Fri Jul 12 22:43:12 2024 ] 	Batch(2900/6809) done. Loss: 0.0114  lr:0.000100
[ Fri Jul 12 22:43:29 2024 ] 
Training: Epoch [73/120], Step [2999], Loss: 0.04062773287296295, Training Accuracy: 96.975
[ Fri Jul 12 22:43:30 2024 ] 	Batch(3000/6809) done. Loss: 0.1228  lr:0.000100
[ Fri Jul 12 22:43:47 2024 ] 	Batch(3100/6809) done. Loss: 0.0212  lr:0.000100
[ Fri Jul 12 22:44:05 2024 ] 	Batch(3200/6809) done. Loss: 0.0812  lr:0.000100
[ Fri Jul 12 22:44:24 2024 ] 	Batch(3300/6809) done. Loss: 0.1029  lr:0.000100
[ Fri Jul 12 22:44:43 2024 ] 	Batch(3400/6809) done. Loss: 0.6182  lr:0.000100
[ Fri Jul 12 22:45:01 2024 ] 
Training: Epoch [73/120], Step [3499], Loss: 0.007601223886013031, Training Accuracy: 96.89285714285715
[ Fri Jul 12 22:45:01 2024 ] 	Batch(3500/6809) done. Loss: 0.1667  lr:0.000100
[ Fri Jul 12 22:45:20 2024 ] 	Batch(3600/6809) done. Loss: 0.0992  lr:0.000100
[ Fri Jul 12 22:45:38 2024 ] 	Batch(3700/6809) done. Loss: 0.3120  lr:0.000100
[ Fri Jul 12 22:45:55 2024 ] 	Batch(3800/6809) done. Loss: 0.0063  lr:0.000100
[ Fri Jul 12 22:46:13 2024 ] 	Batch(3900/6809) done. Loss: 0.2531  lr:0.000100
[ Fri Jul 12 22:46:31 2024 ] 
Training: Epoch [73/120], Step [3999], Loss: 0.03690674155950546, Training Accuracy: 96.965625
[ Fri Jul 12 22:46:31 2024 ] 	Batch(4000/6809) done. Loss: 0.0214  lr:0.000100
[ Fri Jul 12 22:46:50 2024 ] 	Batch(4100/6809) done. Loss: 0.0050  lr:0.000100
[ Fri Jul 12 22:47:09 2024 ] 	Batch(4200/6809) done. Loss: 0.0108  lr:0.000100
[ Fri Jul 12 22:47:27 2024 ] 	Batch(4300/6809) done. Loss: 0.0731  lr:0.000100
[ Fri Jul 12 22:47:46 2024 ] 	Batch(4400/6809) done. Loss: 0.0645  lr:0.000100
[ Fri Jul 12 22:48:04 2024 ] 
Training: Epoch [73/120], Step [4499], Loss: 0.020864350721240044, Training Accuracy: 96.98055555555555
[ Fri Jul 12 22:48:04 2024 ] 	Batch(4500/6809) done. Loss: 0.0368  lr:0.000100
[ Fri Jul 12 22:48:23 2024 ] 	Batch(4600/6809) done. Loss: 0.5201  lr:0.000100
[ Fri Jul 12 22:48:41 2024 ] 	Batch(4700/6809) done. Loss: 0.1548  lr:0.000100
[ Fri Jul 12 22:49:00 2024 ] 	Batch(4800/6809) done. Loss: 0.2245  lr:0.000100
[ Fri Jul 12 22:49:18 2024 ] 	Batch(4900/6809) done. Loss: 0.0235  lr:0.000100
[ Fri Jul 12 22:49:36 2024 ] 
Training: Epoch [73/120], Step [4999], Loss: 0.621613621711731, Training Accuracy: 96.9975
[ Fri Jul 12 22:49:36 2024 ] 	Batch(5000/6809) done. Loss: 0.2516  lr:0.000100
[ Fri Jul 12 22:49:54 2024 ] 	Batch(5100/6809) done. Loss: 0.0409  lr:0.000100
[ Fri Jul 12 22:50:12 2024 ] 	Batch(5200/6809) done. Loss: 0.1713  lr:0.000100
[ Fri Jul 12 22:50:31 2024 ] 	Batch(5300/6809) done. Loss: 0.1077  lr:0.000100
[ Fri Jul 12 22:50:50 2024 ] 	Batch(5400/6809) done. Loss: 0.0333  lr:0.000100
[ Fri Jul 12 22:51:08 2024 ] 
Training: Epoch [73/120], Step [5499], Loss: 0.007860423065721989, Training Accuracy: 96.99772727272727
[ Fri Jul 12 22:51:08 2024 ] 	Batch(5500/6809) done. Loss: 0.3937  lr:0.000100
[ Fri Jul 12 22:51:27 2024 ] 	Batch(5600/6809) done. Loss: 0.0361  lr:0.000100
[ Fri Jul 12 22:51:45 2024 ] 	Batch(5700/6809) done. Loss: 0.0182  lr:0.000100
[ Fri Jul 12 22:52:03 2024 ] 	Batch(5800/6809) done. Loss: 0.1429  lr:0.000100
[ Fri Jul 12 22:52:21 2024 ] 	Batch(5900/6809) done. Loss: 0.1420  lr:0.000100
[ Fri Jul 12 22:52:39 2024 ] 
Training: Epoch [73/120], Step [5999], Loss: 0.021190715953707695, Training Accuracy: 97.00416666666666
[ Fri Jul 12 22:52:39 2024 ] 	Batch(6000/6809) done. Loss: 0.0923  lr:0.000100
[ Fri Jul 12 22:52:57 2024 ] 	Batch(6100/6809) done. Loss: 0.1696  lr:0.000100
[ Fri Jul 12 22:53:15 2024 ] 	Batch(6200/6809) done. Loss: 0.0637  lr:0.000100
[ Fri Jul 12 22:53:33 2024 ] 	Batch(6300/6809) done. Loss: 0.0022  lr:0.000100
[ Fri Jul 12 22:53:51 2024 ] 	Batch(6400/6809) done. Loss: 0.0131  lr:0.000100
[ Fri Jul 12 22:54:09 2024 ] 
Training: Epoch [73/120], Step [6499], Loss: 0.1395394206047058, Training Accuracy: 97.00961538461539
[ Fri Jul 12 22:54:09 2024 ] 	Batch(6500/6809) done. Loss: 0.0615  lr:0.000100
[ Fri Jul 12 22:54:28 2024 ] 	Batch(6600/6809) done. Loss: 0.0097  lr:0.000100
[ Fri Jul 12 22:54:46 2024 ] 	Batch(6700/6809) done. Loss: 0.4625  lr:0.000100
[ Fri Jul 12 22:55:05 2024 ] 	Batch(6800/6809) done. Loss: 0.0057  lr:0.000100
[ Fri Jul 12 22:55:07 2024 ] 	Mean training loss: 0.1129.
[ Fri Jul 12 22:55:07 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 22:55:07 2024 ] Training epoch: 75
[ Fri Jul 12 22:55:07 2024 ] 	Batch(0/6809) done. Loss: 0.1004  lr:0.000100
[ Fri Jul 12 22:55:25 2024 ] 	Batch(100/6809) done. Loss: 0.1069  lr:0.000100
[ Fri Jul 12 22:55:43 2024 ] 	Batch(200/6809) done. Loss: 0.0064  lr:0.000100
[ Fri Jul 12 22:56:01 2024 ] 	Batch(300/6809) done. Loss: 0.0606  lr:0.000100
[ Fri Jul 12 22:56:19 2024 ] 	Batch(400/6809) done. Loss: 0.0086  lr:0.000100
[ Fri Jul 12 22:56:37 2024 ] 
Training: Epoch [74/120], Step [499], Loss: 0.14100296795368195, Training Accuracy: 96.925
[ Fri Jul 12 22:56:37 2024 ] 	Batch(500/6809) done. Loss: 0.0064  lr:0.000100
[ Fri Jul 12 22:56:55 2024 ] 	Batch(600/6809) done. Loss: 0.1792  lr:0.000100
[ Fri Jul 12 22:57:13 2024 ] 	Batch(700/6809) done. Loss: 0.0194  lr:0.000100
[ Fri Jul 12 22:57:31 2024 ] 	Batch(800/6809) done. Loss: 0.0830  lr:0.000100
[ Fri Jul 12 22:57:49 2024 ] 	Batch(900/6809) done. Loss: 0.0951  lr:0.000100
[ Fri Jul 12 22:58:07 2024 ] 
Training: Epoch [74/120], Step [999], Loss: 0.7442725896835327, Training Accuracy: 96.925
[ Fri Jul 12 22:58:08 2024 ] 	Batch(1000/6809) done. Loss: 0.0285  lr:0.000100
[ Fri Jul 12 22:58:26 2024 ] 	Batch(1100/6809) done. Loss: 0.0057  lr:0.000100
[ Fri Jul 12 22:58:45 2024 ] 	Batch(1200/6809) done. Loss: 0.0018  lr:0.000100
[ Fri Jul 12 22:59:03 2024 ] 	Batch(1300/6809) done. Loss: 0.0304  lr:0.000100
[ Fri Jul 12 22:59:22 2024 ] 	Batch(1400/6809) done. Loss: 0.0402  lr:0.000100
[ Fri Jul 12 22:59:40 2024 ] 
Training: Epoch [74/120], Step [1499], Loss: 0.6367806792259216, Training Accuracy: 96.94166666666668
[ Fri Jul 12 22:59:40 2024 ] 	Batch(1500/6809) done. Loss: 0.0229  lr:0.000100
[ Fri Jul 12 22:59:58 2024 ] 	Batch(1600/6809) done. Loss: 0.0493  lr:0.000100
[ Fri Jul 12 23:00:16 2024 ] 	Batch(1700/6809) done. Loss: 0.0672  lr:0.000100
[ Fri Jul 12 23:00:34 2024 ] 	Batch(1800/6809) done. Loss: 0.1365  lr:0.000100
[ Fri Jul 12 23:00:51 2024 ] 	Batch(1900/6809) done. Loss: 0.0016  lr:0.000100
[ Fri Jul 12 23:01:09 2024 ] 
Training: Epoch [74/120], Step [1999], Loss: 0.06904852390289307, Training Accuracy: 96.84375
[ Fri Jul 12 23:01:10 2024 ] 	Batch(2000/6809) done. Loss: 0.0519  lr:0.000100
[ Fri Jul 12 23:01:28 2024 ] 	Batch(2100/6809) done. Loss: 0.0132  lr:0.000100
[ Fri Jul 12 23:01:45 2024 ] 	Batch(2200/6809) done. Loss: 0.0764  lr:0.000100
[ Fri Jul 12 23:02:03 2024 ] 	Batch(2300/6809) done. Loss: 0.0478  lr:0.000100
[ Fri Jul 12 23:02:21 2024 ] 	Batch(2400/6809) done. Loss: 0.0273  lr:0.000100
[ Fri Jul 12 23:02:39 2024 ] 
Training: Epoch [74/120], Step [2499], Loss: 0.02175050601363182, Training Accuracy: 96.965
[ Fri Jul 12 23:02:39 2024 ] 	Batch(2500/6809) done. Loss: 0.1268  lr:0.000100
[ Fri Jul 12 23:02:57 2024 ] 	Batch(2600/6809) done. Loss: 0.2695  lr:0.000100
[ Fri Jul 12 23:03:15 2024 ] 	Batch(2700/6809) done. Loss: 0.0058  lr:0.000100
[ Fri Jul 12 23:03:33 2024 ] 	Batch(2800/6809) done. Loss: 0.0074  lr:0.000100
[ Fri Jul 12 23:03:51 2024 ] 	Batch(2900/6809) done. Loss: 1.0632  lr:0.000100
[ Fri Jul 12 23:04:09 2024 ] 
Training: Epoch [74/120], Step [2999], Loss: 0.5486538410186768, Training Accuracy: 96.94166666666668
[ Fri Jul 12 23:04:09 2024 ] 	Batch(3000/6809) done. Loss: 0.1280  lr:0.000100
[ Fri Jul 12 23:04:27 2024 ] 	Batch(3100/6809) done. Loss: 0.1244  lr:0.000100
[ Fri Jul 12 23:04:45 2024 ] 	Batch(3200/6809) done. Loss: 0.0113  lr:0.000100
[ Fri Jul 12 23:05:03 2024 ] 	Batch(3300/6809) done. Loss: 0.0075  lr:0.000100
[ Fri Jul 12 23:05:20 2024 ] 	Batch(3400/6809) done. Loss: 0.1521  lr:0.000100
[ Fri Jul 12 23:05:38 2024 ] 
Training: Epoch [74/120], Step [3499], Loss: 0.3430693745613098, Training Accuracy: 96.95357142857142
[ Fri Jul 12 23:05:38 2024 ] 	Batch(3500/6809) done. Loss: 0.1149  lr:0.000100
[ Fri Jul 12 23:05:56 2024 ] 	Batch(3600/6809) done. Loss: 0.0581  lr:0.000100
[ Fri Jul 12 23:06:14 2024 ] 	Batch(3700/6809) done. Loss: 0.2156  lr:0.000100
[ Fri Jul 12 23:06:32 2024 ] 	Batch(3800/6809) done. Loss: 0.0494  lr:0.000100
[ Fri Jul 12 23:06:50 2024 ] 	Batch(3900/6809) done. Loss: 0.0450  lr:0.000100
[ Fri Jul 12 23:07:08 2024 ] 
Training: Epoch [74/120], Step [3999], Loss: 0.09704882651567459, Training Accuracy: 96.93125
[ Fri Jul 12 23:07:08 2024 ] 	Batch(4000/6809) done. Loss: 0.8087  lr:0.000100
[ Fri Jul 12 23:07:26 2024 ] 	Batch(4100/6809) done. Loss: 0.0259  lr:0.000100
[ Fri Jul 12 23:07:44 2024 ] 	Batch(4200/6809) done. Loss: 0.0489  lr:0.000100
[ Fri Jul 12 23:08:02 2024 ] 	Batch(4300/6809) done. Loss: 0.0206  lr:0.000100
[ Fri Jul 12 23:08:20 2024 ] 	Batch(4400/6809) done. Loss: 0.0184  lr:0.000100
[ Fri Jul 12 23:08:38 2024 ] 
Training: Epoch [74/120], Step [4499], Loss: 0.12347681820392609, Training Accuracy: 96.94722222222222
[ Fri Jul 12 23:08:38 2024 ] 	Batch(4500/6809) done. Loss: 0.0028  lr:0.000100
[ Fri Jul 12 23:08:56 2024 ] 	Batch(4600/6809) done. Loss: 0.0484  lr:0.000100
[ Fri Jul 12 23:09:14 2024 ] 	Batch(4700/6809) done. Loss: 0.1610  lr:0.000100
[ Fri Jul 12 23:09:32 2024 ] 	Batch(4800/6809) done. Loss: 0.1527  lr:0.000100
[ Fri Jul 12 23:09:50 2024 ] 	Batch(4900/6809) done. Loss: 0.0078  lr:0.000100
[ Fri Jul 12 23:10:07 2024 ] 
Training: Epoch [74/120], Step [4999], Loss: 0.05175518989562988, Training Accuracy: 96.91499999999999
[ Fri Jul 12 23:10:08 2024 ] 	Batch(5000/6809) done. Loss: 0.0049  lr:0.000100
[ Fri Jul 12 23:10:26 2024 ] 	Batch(5100/6809) done. Loss: 0.0768  lr:0.000100
[ Fri Jul 12 23:10:44 2024 ] 	Batch(5200/6809) done. Loss: 0.0220  lr:0.000100
[ Fri Jul 12 23:11:01 2024 ] 	Batch(5300/6809) done. Loss: 0.0938  lr:0.000100
[ Fri Jul 12 23:11:19 2024 ] 	Batch(5400/6809) done. Loss: 0.0022  lr:0.000100
[ Fri Jul 12 23:11:37 2024 ] 
Training: Epoch [74/120], Step [5499], Loss: 0.010781988501548767, Training Accuracy: 96.92727272727272
[ Fri Jul 12 23:11:37 2024 ] 	Batch(5500/6809) done. Loss: 0.1699  lr:0.000100
[ Fri Jul 12 23:11:55 2024 ] 	Batch(5600/6809) done. Loss: 0.0021  lr:0.000100
[ Fri Jul 12 23:12:13 2024 ] 	Batch(5700/6809) done. Loss: 0.2774  lr:0.000100
[ Fri Jul 12 23:12:31 2024 ] 	Batch(5800/6809) done. Loss: 0.0752  lr:0.000100
[ Fri Jul 12 23:12:49 2024 ] 	Batch(5900/6809) done. Loss: 0.0145  lr:0.000100
[ Fri Jul 12 23:13:06 2024 ] 
Training: Epoch [74/120], Step [5999], Loss: 0.012059187516570091, Training Accuracy: 96.975
[ Fri Jul 12 23:13:07 2024 ] 	Batch(6000/6809) done. Loss: 0.0346  lr:0.000100
[ Fri Jul 12 23:13:25 2024 ] 	Batch(6100/6809) done. Loss: 0.0370  lr:0.000100
[ Fri Jul 12 23:13:44 2024 ] 	Batch(6200/6809) done. Loss: 0.0323  lr:0.000100
[ Fri Jul 12 23:14:01 2024 ] 	Batch(6300/6809) done. Loss: 0.1363  lr:0.000100
[ Fri Jul 12 23:14:19 2024 ] 	Batch(6400/6809) done. Loss: 0.0063  lr:0.000100
[ Fri Jul 12 23:14:37 2024 ] 
Training: Epoch [74/120], Step [6499], Loss: 0.025455085560679436, Training Accuracy: 96.97884615384615
[ Fri Jul 12 23:14:37 2024 ] 	Batch(6500/6809) done. Loss: 0.0454  lr:0.000100
[ Fri Jul 12 23:14:55 2024 ] 	Batch(6600/6809) done. Loss: 0.1578  lr:0.000100
[ Fri Jul 12 23:15:13 2024 ] 	Batch(6700/6809) done. Loss: 0.1157  lr:0.000100
[ Fri Jul 12 23:15:31 2024 ] 	Batch(6800/6809) done. Loss: 0.0365  lr:0.000100
[ Fri Jul 12 23:15:33 2024 ] 	Mean training loss: 0.1121.
[ Fri Jul 12 23:15:33 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 23:15:33 2024 ] Training epoch: 76
[ Fri Jul 12 23:15:33 2024 ] 	Batch(0/6809) done. Loss: 0.3273  lr:0.000100
[ Fri Jul 12 23:15:51 2024 ] 	Batch(100/6809) done. Loss: 0.3144  lr:0.000100
[ Fri Jul 12 23:16:09 2024 ] 	Batch(200/6809) done. Loss: 0.3091  lr:0.000100
[ Fri Jul 12 23:16:27 2024 ] 	Batch(300/6809) done. Loss: 0.6325  lr:0.000100
[ Fri Jul 12 23:16:45 2024 ] 	Batch(400/6809) done. Loss: 0.1355  lr:0.000100
[ Fri Jul 12 23:17:03 2024 ] 
Training: Epoch [75/120], Step [499], Loss: 0.027232713997364044, Training Accuracy: 97.125
[ Fri Jul 12 23:17:03 2024 ] 	Batch(500/6809) done. Loss: 0.4389  lr:0.000100
[ Fri Jul 12 23:17:21 2024 ] 	Batch(600/6809) done. Loss: 0.3512  lr:0.000100
[ Fri Jul 12 23:17:39 2024 ] 	Batch(700/6809) done. Loss: 0.0031  lr:0.000100
[ Fri Jul 12 23:17:57 2024 ] 	Batch(800/6809) done. Loss: 0.1850  lr:0.000100
[ Fri Jul 12 23:18:15 2024 ] 	Batch(900/6809) done. Loss: 0.0502  lr:0.000100
[ Fri Jul 12 23:18:34 2024 ] 
Training: Epoch [75/120], Step [999], Loss: 0.03217877820134163, Training Accuracy: 97.225
[ Fri Jul 12 23:18:34 2024 ] 	Batch(1000/6809) done. Loss: 0.0164  lr:0.000100
[ Fri Jul 12 23:18:52 2024 ] 	Batch(1100/6809) done. Loss: 0.0135  lr:0.000100
[ Fri Jul 12 23:19:11 2024 ] 	Batch(1200/6809) done. Loss: 0.0255  lr:0.000100
[ Fri Jul 12 23:19:29 2024 ] 	Batch(1300/6809) done. Loss: 0.0063  lr:0.000100
[ Fri Jul 12 23:19:47 2024 ] 	Batch(1400/6809) done. Loss: 0.0503  lr:0.000100
[ Fri Jul 12 23:20:05 2024 ] 
Training: Epoch [75/120], Step [1499], Loss: 0.006031031254678965, Training Accuracy: 97.19166666666666
[ Fri Jul 12 23:20:05 2024 ] 	Batch(1500/6809) done. Loss: 0.0122  lr:0.000100
[ Fri Jul 12 23:20:23 2024 ] 	Batch(1600/6809) done. Loss: 0.0725  lr:0.000100
[ Fri Jul 12 23:20:41 2024 ] 	Batch(1700/6809) done. Loss: 0.1118  lr:0.000100
[ Fri Jul 12 23:20:59 2024 ] 	Batch(1800/6809) done. Loss: 0.2029  lr:0.000100
[ Fri Jul 12 23:21:17 2024 ] 	Batch(1900/6809) done. Loss: 0.1099  lr:0.000100
[ Fri Jul 12 23:21:34 2024 ] 
Training: Epoch [75/120], Step [1999], Loss: 0.31373709440231323, Training Accuracy: 97.14375
[ Fri Jul 12 23:21:35 2024 ] 	Batch(2000/6809) done. Loss: 0.0179  lr:0.000100
[ Fri Jul 12 23:21:52 2024 ] 	Batch(2100/6809) done. Loss: 0.1922  lr:0.000100
[ Fri Jul 12 23:22:10 2024 ] 	Batch(2200/6809) done. Loss: 0.0165  lr:0.000100
[ Fri Jul 12 23:22:28 2024 ] 	Batch(2300/6809) done. Loss: 0.1387  lr:0.000100
[ Fri Jul 12 23:22:46 2024 ] 	Batch(2400/6809) done. Loss: 0.0889  lr:0.000100
[ Fri Jul 12 23:23:04 2024 ] 
Training: Epoch [75/120], Step [2499], Loss: 0.6822195649147034, Training Accuracy: 97.175
[ Fri Jul 12 23:23:05 2024 ] 	Batch(2500/6809) done. Loss: 0.0499  lr:0.000100
[ Fri Jul 12 23:23:23 2024 ] 	Batch(2600/6809) done. Loss: 0.0059  lr:0.000100
[ Fri Jul 12 23:23:41 2024 ] 	Batch(2700/6809) done. Loss: 0.0652  lr:0.000100
[ Fri Jul 12 23:23:59 2024 ] 	Batch(2800/6809) done. Loss: 0.0671  lr:0.000100
[ Fri Jul 12 23:24:17 2024 ] 	Batch(2900/6809) done. Loss: 0.0927  lr:0.000100
[ Fri Jul 12 23:24:35 2024 ] 
Training: Epoch [75/120], Step [2999], Loss: 0.020269576460123062, Training Accuracy: 97.15
[ Fri Jul 12 23:24:35 2024 ] 	Batch(3000/6809) done. Loss: 0.0144  lr:0.000100
[ Fri Jul 12 23:24:53 2024 ] 	Batch(3100/6809) done. Loss: 0.1232  lr:0.000100
[ Fri Jul 12 23:25:12 2024 ] 	Batch(3200/6809) done. Loss: 0.0264  lr:0.000100
[ Fri Jul 12 23:25:30 2024 ] 	Batch(3300/6809) done. Loss: 0.0362  lr:0.000100
[ Fri Jul 12 23:25:48 2024 ] 	Batch(3400/6809) done. Loss: 0.4019  lr:0.000100
[ Fri Jul 12 23:26:06 2024 ] 
Training: Epoch [75/120], Step [3499], Loss: 0.022336646914482117, Training Accuracy: 97.13571428571429
[ Fri Jul 12 23:26:06 2024 ] 	Batch(3500/6809) done. Loss: 1.0611  lr:0.000100
[ Fri Jul 12 23:26:25 2024 ] 	Batch(3600/6809) done. Loss: 0.0349  lr:0.000100
[ Fri Jul 12 23:26:43 2024 ] 	Batch(3700/6809) done. Loss: 0.0925  lr:0.000100
[ Fri Jul 12 23:27:00 2024 ] 	Batch(3800/6809) done. Loss: 0.0134  lr:0.000100
[ Fri Jul 12 23:27:18 2024 ] 	Batch(3900/6809) done. Loss: 0.0324  lr:0.000100
[ Fri Jul 12 23:27:36 2024 ] 
Training: Epoch [75/120], Step [3999], Loss: 0.01679016463458538, Training Accuracy: 97.15312499999999
[ Fri Jul 12 23:27:36 2024 ] 	Batch(4000/6809) done. Loss: 0.0101  lr:0.000100
[ Fri Jul 12 23:27:54 2024 ] 	Batch(4100/6809) done. Loss: 0.0475  lr:0.000100
[ Fri Jul 12 23:28:12 2024 ] 	Batch(4200/6809) done. Loss: 0.0654  lr:0.000100
[ Fri Jul 12 23:28:30 2024 ] 	Batch(4300/6809) done. Loss: 0.2579  lr:0.000100
[ Fri Jul 12 23:28:48 2024 ] 	Batch(4400/6809) done. Loss: 0.0360  lr:0.000100
[ Fri Jul 12 23:29:06 2024 ] 
Training: Epoch [75/120], Step [4499], Loss: 0.11647932976484299, Training Accuracy: 97.15
[ Fri Jul 12 23:29:06 2024 ] 	Batch(4500/6809) done. Loss: 0.0790  lr:0.000100
[ Fri Jul 12 23:29:24 2024 ] 	Batch(4600/6809) done. Loss: 0.0865  lr:0.000100
[ Fri Jul 12 23:29:42 2024 ] 	Batch(4700/6809) done. Loss: 0.0671  lr:0.000100
[ Fri Jul 12 23:30:00 2024 ] 	Batch(4800/6809) done. Loss: 0.0132  lr:0.000100
[ Fri Jul 12 23:30:17 2024 ] 	Batch(4900/6809) done. Loss: 0.3907  lr:0.000100
[ Fri Jul 12 23:30:35 2024 ] 
Training: Epoch [75/120], Step [4999], Loss: 0.15407100319862366, Training Accuracy: 97.1325
[ Fri Jul 12 23:30:35 2024 ] 	Batch(5000/6809) done. Loss: 0.1730  lr:0.000100
[ Fri Jul 12 23:30:53 2024 ] 	Batch(5100/6809) done. Loss: 0.0738  lr:0.000100
[ Fri Jul 12 23:31:11 2024 ] 	Batch(5200/6809) done. Loss: 0.0510  lr:0.000100
[ Fri Jul 12 23:31:29 2024 ] 	Batch(5300/6809) done. Loss: 0.0794  lr:0.000100
[ Fri Jul 12 23:31:47 2024 ] 	Batch(5400/6809) done. Loss: 0.1884  lr:0.000100
[ Fri Jul 12 23:32:05 2024 ] 
Training: Epoch [75/120], Step [5499], Loss: 0.20011471211910248, Training Accuracy: 97.19090909090909
[ Fri Jul 12 23:32:05 2024 ] 	Batch(5500/6809) done. Loss: 0.0582  lr:0.000100
[ Fri Jul 12 23:32:23 2024 ] 	Batch(5600/6809) done. Loss: 0.0901  lr:0.000100
[ Fri Jul 12 23:32:42 2024 ] 	Batch(5700/6809) done. Loss: 0.0315  lr:0.000100
[ Fri Jul 12 23:33:00 2024 ] 	Batch(5800/6809) done. Loss: 0.0650  lr:0.000100
[ Fri Jul 12 23:33:19 2024 ] 	Batch(5900/6809) done. Loss: 0.0930  lr:0.000100
[ Fri Jul 12 23:33:37 2024 ] 
Training: Epoch [75/120], Step [5999], Loss: 0.23924335837364197, Training Accuracy: 97.19583333333334
[ Fri Jul 12 23:33:37 2024 ] 	Batch(6000/6809) done. Loss: 0.1284  lr:0.000100
[ Fri Jul 12 23:33:56 2024 ] 	Batch(6100/6809) done. Loss: 0.0262  lr:0.000100
[ Fri Jul 12 23:34:14 2024 ] 	Batch(6200/6809) done. Loss: 0.2605  lr:0.000100
[ Fri Jul 12 23:34:33 2024 ] 	Batch(6300/6809) done. Loss: 0.0565  lr:0.000100
[ Fri Jul 12 23:34:52 2024 ] 	Batch(6400/6809) done. Loss: 0.1975  lr:0.000100
[ Fri Jul 12 23:35:10 2024 ] 
Training: Epoch [75/120], Step [6499], Loss: 0.3994413912296295, Training Accuracy: 97.16730769230769
[ Fri Jul 12 23:35:10 2024 ] 	Batch(6500/6809) done. Loss: 0.0511  lr:0.000100
[ Fri Jul 12 23:35:29 2024 ] 	Batch(6600/6809) done. Loss: 0.2584  lr:0.000100
[ Fri Jul 12 23:35:47 2024 ] 	Batch(6700/6809) done. Loss: 0.0173  lr:0.000100
[ Fri Jul 12 23:36:06 2024 ] 	Batch(6800/6809) done. Loss: 0.1164  lr:0.000100
[ Fri Jul 12 23:36:07 2024 ] 	Mean training loss: 0.1102.
[ Fri Jul 12 23:36:07 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 23:36:07 2024 ] Training epoch: 77
[ Fri Jul 12 23:36:08 2024 ] 	Batch(0/6809) done. Loss: 0.0124  lr:0.000100
[ Fri Jul 12 23:36:26 2024 ] 	Batch(100/6809) done. Loss: 0.0101  lr:0.000100
[ Fri Jul 12 23:36:44 2024 ] 	Batch(200/6809) done. Loss: 0.0263  lr:0.000100
[ Fri Jul 12 23:37:02 2024 ] 	Batch(300/6809) done. Loss: 0.2861  lr:0.000100
[ Fri Jul 12 23:37:20 2024 ] 	Batch(400/6809) done. Loss: 0.1751  lr:0.000100
[ Fri Jul 12 23:37:38 2024 ] 
Training: Epoch [76/120], Step [499], Loss: 0.09596481919288635, Training Accuracy: 97.075
[ Fri Jul 12 23:37:38 2024 ] 	Batch(500/6809) done. Loss: 0.3844  lr:0.000100
[ Fri Jul 12 23:37:56 2024 ] 	Batch(600/6809) done. Loss: 0.1206  lr:0.000100
[ Fri Jul 12 23:38:14 2024 ] 	Batch(700/6809) done. Loss: 0.0756  lr:0.000100
[ Fri Jul 12 23:38:32 2024 ] 	Batch(800/6809) done. Loss: 0.0140  lr:0.000100
[ Fri Jul 12 23:38:51 2024 ] 	Batch(900/6809) done. Loss: 0.0178  lr:0.000100
[ Fri Jul 12 23:39:09 2024 ] 
Training: Epoch [76/120], Step [999], Loss: 0.13165408372879028, Training Accuracy: 96.925
[ Fri Jul 12 23:39:09 2024 ] 	Batch(1000/6809) done. Loss: 0.2396  lr:0.000100
[ Fri Jul 12 23:39:27 2024 ] 	Batch(1100/6809) done. Loss: 0.0218  lr:0.000100
[ Fri Jul 12 23:39:45 2024 ] 	Batch(1200/6809) done. Loss: 0.0304  lr:0.000100
[ Fri Jul 12 23:40:03 2024 ] 	Batch(1300/6809) done. Loss: 0.0447  lr:0.000100
[ Fri Jul 12 23:40:21 2024 ] 	Batch(1400/6809) done. Loss: 0.0254  lr:0.000100
[ Fri Jul 12 23:40:39 2024 ] 
Training: Epoch [76/120], Step [1499], Loss: 0.05977198854088783, Training Accuracy: 96.81666666666666
[ Fri Jul 12 23:40:39 2024 ] 	Batch(1500/6809) done. Loss: 0.2363  lr:0.000100
[ Fri Jul 12 23:40:57 2024 ] 	Batch(1600/6809) done. Loss: 0.0354  lr:0.000100
[ Fri Jul 12 23:41:15 2024 ] 	Batch(1700/6809) done. Loss: 0.0626  lr:0.000100
[ Fri Jul 12 23:41:33 2024 ] 	Batch(1800/6809) done. Loss: 0.0765  lr:0.000100
[ Fri Jul 12 23:41:51 2024 ] 	Batch(1900/6809) done. Loss: 0.0544  lr:0.000100
[ Fri Jul 12 23:42:08 2024 ] 
Training: Epoch [76/120], Step [1999], Loss: 0.008448934182524681, Training Accuracy: 96.81875
[ Fri Jul 12 23:42:09 2024 ] 	Batch(2000/6809) done. Loss: 0.0426  lr:0.000100
[ Fri Jul 12 23:42:27 2024 ] 	Batch(2100/6809) done. Loss: 0.1074  lr:0.000100
[ Fri Jul 12 23:42:45 2024 ] 	Batch(2200/6809) done. Loss: 0.1939  lr:0.000100
[ Fri Jul 12 23:43:02 2024 ] 	Batch(2300/6809) done. Loss: 0.1031  lr:0.000100
[ Fri Jul 12 23:43:20 2024 ] 	Batch(2400/6809) done. Loss: 0.0513  lr:0.000100
[ Fri Jul 12 23:43:38 2024 ] 
Training: Epoch [76/120], Step [2499], Loss: 0.12696516513824463, Training Accuracy: 96.89
[ Fri Jul 12 23:43:38 2024 ] 	Batch(2500/6809) done. Loss: 0.2798  lr:0.000100
[ Fri Jul 12 23:43:56 2024 ] 	Batch(2600/6809) done. Loss: 0.0881  lr:0.000100
[ Fri Jul 12 23:44:14 2024 ] 	Batch(2700/6809) done. Loss: 0.3258  lr:0.000100
[ Fri Jul 12 23:44:32 2024 ] 	Batch(2800/6809) done. Loss: 0.0249  lr:0.000100
[ Fri Jul 12 23:44:50 2024 ] 	Batch(2900/6809) done. Loss: 0.1832  lr:0.000100
[ Fri Jul 12 23:45:08 2024 ] 
Training: Epoch [76/120], Step [2999], Loss: 0.1946157068014145, Training Accuracy: 96.89166666666667
[ Fri Jul 12 23:45:08 2024 ] 	Batch(3000/6809) done. Loss: 0.0404  lr:0.000100
[ Fri Jul 12 23:45:26 2024 ] 	Batch(3100/6809) done. Loss: 0.0347  lr:0.000100
[ Fri Jul 12 23:45:44 2024 ] 	Batch(3200/6809) done. Loss: 0.0331  lr:0.000100
[ Fri Jul 12 23:46:03 2024 ] 	Batch(3300/6809) done. Loss: 0.2327  lr:0.000100
[ Fri Jul 12 23:46:21 2024 ] 	Batch(3400/6809) done. Loss: 0.0157  lr:0.000100
[ Fri Jul 12 23:46:40 2024 ] 
Training: Epoch [76/120], Step [3499], Loss: 0.14597320556640625, Training Accuracy: 96.875
[ Fri Jul 12 23:46:40 2024 ] 	Batch(3500/6809) done. Loss: 0.1523  lr:0.000100
[ Fri Jul 12 23:46:59 2024 ] 	Batch(3600/6809) done. Loss: 0.2604  lr:0.000100
[ Fri Jul 12 23:47:16 2024 ] 	Batch(3700/6809) done. Loss: 0.1306  lr:0.000100
[ Fri Jul 12 23:47:34 2024 ] 	Batch(3800/6809) done. Loss: 0.0275  lr:0.000100
[ Fri Jul 12 23:47:52 2024 ] 	Batch(3900/6809) done. Loss: 0.0325  lr:0.000100
[ Fri Jul 12 23:48:11 2024 ] 
Training: Epoch [76/120], Step [3999], Loss: 0.05607328936457634, Training Accuracy: 96.9125
[ Fri Jul 12 23:48:11 2024 ] 	Batch(4000/6809) done. Loss: 0.0068  lr:0.000100
[ Fri Jul 12 23:48:29 2024 ] 	Batch(4100/6809) done. Loss: 0.1133  lr:0.000100
[ Fri Jul 12 23:48:47 2024 ] 	Batch(4200/6809) done. Loss: 0.1594  lr:0.000100
[ Fri Jul 12 23:49:05 2024 ] 	Batch(4300/6809) done. Loss: 0.4381  lr:0.000100
[ Fri Jul 12 23:49:23 2024 ] 	Batch(4400/6809) done. Loss: 0.0044  lr:0.000100
[ Fri Jul 12 23:49:41 2024 ] 
Training: Epoch [76/120], Step [4499], Loss: 0.06628517061471939, Training Accuracy: 96.91666666666666
[ Fri Jul 12 23:49:41 2024 ] 	Batch(4500/6809) done. Loss: 0.0022  lr:0.000100
[ Fri Jul 12 23:49:59 2024 ] 	Batch(4600/6809) done. Loss: 0.0648  lr:0.000100
[ Fri Jul 12 23:50:16 2024 ] 	Batch(4700/6809) done. Loss: 0.0211  lr:0.000100
[ Fri Jul 12 23:50:34 2024 ] 	Batch(4800/6809) done. Loss: 0.4554  lr:0.000100
[ Fri Jul 12 23:50:52 2024 ] 	Batch(4900/6809) done. Loss: 0.0909  lr:0.000100
[ Fri Jul 12 23:51:10 2024 ] 
Training: Epoch [76/120], Step [4999], Loss: 0.15407173335552216, Training Accuracy: 96.93
[ Fri Jul 12 23:51:10 2024 ] 	Batch(5000/6809) done. Loss: 0.1625  lr:0.000100
[ Fri Jul 12 23:51:28 2024 ] 	Batch(5100/6809) done. Loss: 0.0100  lr:0.000100
[ Fri Jul 12 23:51:46 2024 ] 	Batch(5200/6809) done. Loss: 0.2827  lr:0.000100
[ Fri Jul 12 23:52:04 2024 ] 	Batch(5300/6809) done. Loss: 0.0627  lr:0.000100
[ Fri Jul 12 23:52:22 2024 ] 	Batch(5400/6809) done. Loss: 0.0493  lr:0.000100
[ Fri Jul 12 23:52:40 2024 ] 
Training: Epoch [76/120], Step [5499], Loss: 0.30941224098205566, Training Accuracy: 96.92727272727272
[ Fri Jul 12 23:52:40 2024 ] 	Batch(5500/6809) done. Loss: 0.3112  lr:0.000100
[ Fri Jul 12 23:52:58 2024 ] 	Batch(5600/6809) done. Loss: 0.0341  lr:0.000100
[ Fri Jul 12 23:53:16 2024 ] 	Batch(5700/6809) done. Loss: 0.0305  lr:0.000100
[ Fri Jul 12 23:53:35 2024 ] 	Batch(5800/6809) done. Loss: 0.0446  lr:0.000100
[ Fri Jul 12 23:53:53 2024 ] 	Batch(5900/6809) done. Loss: 0.0684  lr:0.000100
[ Fri Jul 12 23:54:12 2024 ] 
Training: Epoch [76/120], Step [5999], Loss: 0.04160730913281441, Training Accuracy: 96.93541666666667
[ Fri Jul 12 23:54:12 2024 ] 	Batch(6000/6809) done. Loss: 0.1140  lr:0.000100
[ Fri Jul 12 23:54:31 2024 ] 	Batch(6100/6809) done. Loss: 0.0788  lr:0.000100
[ Fri Jul 12 23:54:49 2024 ] 	Batch(6200/6809) done. Loss: 0.0579  lr:0.000100
[ Fri Jul 12 23:55:08 2024 ] 	Batch(6300/6809) done. Loss: 0.0779  lr:0.000100
[ Fri Jul 12 23:55:26 2024 ] 	Batch(6400/6809) done. Loss: 0.0166  lr:0.000100
[ Fri Jul 12 23:55:45 2024 ] 
Training: Epoch [76/120], Step [6499], Loss: 0.14693216979503632, Training Accuracy: 96.95192307692308
[ Fri Jul 12 23:55:45 2024 ] 	Batch(6500/6809) done. Loss: 0.0140  lr:0.000100
[ Fri Jul 12 23:56:03 2024 ] 	Batch(6600/6809) done. Loss: 0.0407  lr:0.000100
[ Fri Jul 12 23:56:21 2024 ] 	Batch(6700/6809) done. Loss: 0.0411  lr:0.000100
[ Fri Jul 12 23:56:39 2024 ] 	Batch(6800/6809) done. Loss: 0.0121  lr:0.000100
[ Fri Jul 12 23:56:41 2024 ] 	Mean training loss: 0.1103.
[ Fri Jul 12 23:56:41 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Fri Jul 12 23:56:41 2024 ] Training epoch: 78
[ Fri Jul 12 23:56:41 2024 ] 	Batch(0/6809) done. Loss: 0.0914  lr:0.000100
[ Fri Jul 12 23:56:59 2024 ] 	Batch(100/6809) done. Loss: 0.0452  lr:0.000100
[ Fri Jul 12 23:57:17 2024 ] 	Batch(200/6809) done. Loss: 0.3617  lr:0.000100
[ Fri Jul 12 23:57:35 2024 ] 	Batch(300/6809) done. Loss: 0.0625  lr:0.000100
[ Fri Jul 12 23:57:53 2024 ] 	Batch(400/6809) done. Loss: 0.1510  lr:0.000100
[ Fri Jul 12 23:58:11 2024 ] 
Training: Epoch [77/120], Step [499], Loss: 0.012246470898389816, Training Accuracy: 97.275
[ Fri Jul 12 23:58:11 2024 ] 	Batch(500/6809) done. Loss: 0.0789  lr:0.000100
[ Fri Jul 12 23:58:29 2024 ] 	Batch(600/6809) done. Loss: 0.1308  lr:0.000100
[ Fri Jul 12 23:58:46 2024 ] 	Batch(700/6809) done. Loss: 0.0512  lr:0.000100
[ Fri Jul 12 23:59:05 2024 ] 	Batch(800/6809) done. Loss: 0.2523  lr:0.000100
[ Fri Jul 12 23:59:22 2024 ] 	Batch(900/6809) done. Loss: 0.0318  lr:0.000100
[ Fri Jul 12 23:59:40 2024 ] 
Training: Epoch [77/120], Step [999], Loss: 0.027311934158205986, Training Accuracy: 97.3875
[ Fri Jul 12 23:59:40 2024 ] 	Batch(1000/6809) done. Loss: 0.1297  lr:0.000100
[ Fri Jul 12 23:59:58 2024 ] 	Batch(1100/6809) done. Loss: 0.1203  lr:0.000100
[ Sat Jul 13 00:00:16 2024 ] 	Batch(1200/6809) done. Loss: 0.1154  lr:0.000100
[ Sat Jul 13 00:00:34 2024 ] 	Batch(1300/6809) done. Loss: 0.0447  lr:0.000100
[ Sat Jul 13 00:00:52 2024 ] 	Batch(1400/6809) done. Loss: 0.2747  lr:0.000100
[ Sat Jul 13 00:01:10 2024 ] 
Training: Epoch [77/120], Step [1499], Loss: 0.08037003874778748, Training Accuracy: 97.31666666666666
[ Sat Jul 13 00:01:10 2024 ] 	Batch(1500/6809) done. Loss: 0.5746  lr:0.000100
[ Sat Jul 13 00:01:28 2024 ] 	Batch(1600/6809) done. Loss: 0.0021  lr:0.000100
[ Sat Jul 13 00:01:46 2024 ] 	Batch(1700/6809) done. Loss: 0.3835  lr:0.000100
[ Sat Jul 13 00:02:03 2024 ] 	Batch(1800/6809) done. Loss: 0.0044  lr:0.000100
[ Sat Jul 13 00:02:21 2024 ] 	Batch(1900/6809) done. Loss: 0.0319  lr:0.000100
[ Sat Jul 13 00:02:39 2024 ] 
Training: Epoch [77/120], Step [1999], Loss: 0.4986947774887085, Training Accuracy: 97.3625
[ Sat Jul 13 00:02:39 2024 ] 	Batch(2000/6809) done. Loss: 0.0514  lr:0.000100
[ Sat Jul 13 00:02:57 2024 ] 	Batch(2100/6809) done. Loss: 0.0263  lr:0.000100
[ Sat Jul 13 00:03:15 2024 ] 	Batch(2200/6809) done. Loss: 0.5067  lr:0.000100
[ Sat Jul 13 00:03:33 2024 ] 	Batch(2300/6809) done. Loss: 0.0047  lr:0.000100
[ Sat Jul 13 00:03:51 2024 ] 	Batch(2400/6809) done. Loss: 0.0213  lr:0.000100
[ Sat Jul 13 00:04:09 2024 ] 
Training: Epoch [77/120], Step [2499], Loss: 0.0323638916015625, Training Accuracy: 97.25
[ Sat Jul 13 00:04:09 2024 ] 	Batch(2500/6809) done. Loss: 0.2525  lr:0.000100
[ Sat Jul 13 00:04:27 2024 ] 	Batch(2600/6809) done. Loss: 0.3038  lr:0.000100
[ Sat Jul 13 00:04:45 2024 ] 	Batch(2700/6809) done. Loss: 0.0232  lr:0.000100
[ Sat Jul 13 00:05:03 2024 ] 	Batch(2800/6809) done. Loss: 0.0016  lr:0.000100
[ Sat Jul 13 00:05:21 2024 ] 	Batch(2900/6809) done. Loss: 0.0769  lr:0.000100
[ Sat Jul 13 00:05:39 2024 ] 
Training: Epoch [77/120], Step [2999], Loss: 0.04316394776105881, Training Accuracy: 97.27916666666667
[ Sat Jul 13 00:05:39 2024 ] 	Batch(3000/6809) done. Loss: 0.2274  lr:0.000100
[ Sat Jul 13 00:05:57 2024 ] 	Batch(3100/6809) done. Loss: 0.0779  lr:0.000100
[ Sat Jul 13 00:06:15 2024 ] 	Batch(3200/6809) done. Loss: 0.0440  lr:0.000100
[ Sat Jul 13 00:06:33 2024 ] 	Batch(3300/6809) done. Loss: 0.0525  lr:0.000100
[ Sat Jul 13 00:06:51 2024 ] 	Batch(3400/6809) done. Loss: 0.1185  lr:0.000100
[ Sat Jul 13 00:07:08 2024 ] 
Training: Epoch [77/120], Step [3499], Loss: 0.0020379263442009687, Training Accuracy: 97.29642857142858
[ Sat Jul 13 00:07:08 2024 ] 	Batch(3500/6809) done. Loss: 0.0128  lr:0.000100
[ Sat Jul 13 00:07:27 2024 ] 	Batch(3600/6809) done. Loss: 0.1831  lr:0.000100
[ Sat Jul 13 00:07:45 2024 ] 	Batch(3700/6809) done. Loss: 0.0503  lr:0.000100
[ Sat Jul 13 00:08:04 2024 ] 	Batch(3800/6809) done. Loss: 0.1300  lr:0.000100
[ Sat Jul 13 00:08:22 2024 ] 	Batch(3900/6809) done. Loss: 0.1665  lr:0.000100
[ Sat Jul 13 00:08:41 2024 ] 
Training: Epoch [77/120], Step [3999], Loss: 0.29609084129333496, Training Accuracy: 97.28125
[ Sat Jul 13 00:08:41 2024 ] 	Batch(4000/6809) done. Loss: 0.0282  lr:0.000100
[ Sat Jul 13 00:08:59 2024 ] 	Batch(4100/6809) done. Loss: 0.0182  lr:0.000100
[ Sat Jul 13 00:09:18 2024 ] 	Batch(4200/6809) done. Loss: 0.0276  lr:0.000100
[ Sat Jul 13 00:09:36 2024 ] 	Batch(4300/6809) done. Loss: 0.1322  lr:0.000100
[ Sat Jul 13 00:09:54 2024 ] 	Batch(4400/6809) done. Loss: 0.2447  lr:0.000100
[ Sat Jul 13 00:10:12 2024 ] 
Training: Epoch [77/120], Step [4499], Loss: 0.3091636598110199, Training Accuracy: 97.28333333333333
[ Sat Jul 13 00:10:12 2024 ] 	Batch(4500/6809) done. Loss: 0.0264  lr:0.000100
[ Sat Jul 13 00:10:30 2024 ] 	Batch(4600/6809) done. Loss: 0.0535  lr:0.000100
[ Sat Jul 13 00:10:48 2024 ] 	Batch(4700/6809) done. Loss: 0.0689  lr:0.000100
[ Sat Jul 13 00:11:06 2024 ] 	Batch(4800/6809) done. Loss: 0.1581  lr:0.000100
[ Sat Jul 13 00:11:24 2024 ] 	Batch(4900/6809) done. Loss: 0.0771  lr:0.000100
[ Sat Jul 13 00:11:42 2024 ] 
Training: Epoch [77/120], Step [4999], Loss: 0.2146511822938919, Training Accuracy: 97.2275
[ Sat Jul 13 00:11:42 2024 ] 	Batch(5000/6809) done. Loss: 0.2382  lr:0.000100
[ Sat Jul 13 00:12:00 2024 ] 	Batch(5100/6809) done. Loss: 0.2083  lr:0.000100
[ Sat Jul 13 00:12:18 2024 ] 	Batch(5200/6809) done. Loss: 0.1351  lr:0.000100
[ Sat Jul 13 00:12:36 2024 ] 	Batch(5300/6809) done. Loss: 0.0216  lr:0.000100
[ Sat Jul 13 00:12:53 2024 ] 	Batch(5400/6809) done. Loss: 0.1205  lr:0.000100
[ Sat Jul 13 00:13:11 2024 ] 
Training: Epoch [77/120], Step [5499], Loss: 0.015939194709062576, Training Accuracy: 97.22272727272727
[ Sat Jul 13 00:13:11 2024 ] 	Batch(5500/6809) done. Loss: 0.1365  lr:0.000100
[ Sat Jul 13 00:13:29 2024 ] 	Batch(5600/6809) done. Loss: 0.0670  lr:0.000100
[ Sat Jul 13 00:13:47 2024 ] 	Batch(5700/6809) done. Loss: 0.0961  lr:0.000100
[ Sat Jul 13 00:14:05 2024 ] 	Batch(5800/6809) done. Loss: 0.3589  lr:0.000100
[ Sat Jul 13 00:14:23 2024 ] 	Batch(5900/6809) done. Loss: 0.1347  lr:0.000100
[ Sat Jul 13 00:14:41 2024 ] 
Training: Epoch [77/120], Step [5999], Loss: 0.05859946832060814, Training Accuracy: 97.19375
[ Sat Jul 13 00:14:41 2024 ] 	Batch(6000/6809) done. Loss: 0.0485  lr:0.000100
[ Sat Jul 13 00:15:00 2024 ] 	Batch(6100/6809) done. Loss: 0.1219  lr:0.000100
[ Sat Jul 13 00:15:18 2024 ] 	Batch(6200/6809) done. Loss: 0.1535  lr:0.000100
[ Sat Jul 13 00:15:36 2024 ] 	Batch(6300/6809) done. Loss: 0.0405  lr:0.000100
[ Sat Jul 13 00:15:55 2024 ] 	Batch(6400/6809) done. Loss: 0.2328  lr:0.000100
[ Sat Jul 13 00:16:13 2024 ] 
Training: Epoch [77/120], Step [6499], Loss: 0.011559713631868362, Training Accuracy: 97.21346153846154
[ Sat Jul 13 00:16:13 2024 ] 	Batch(6500/6809) done. Loss: 1.1287  lr:0.000100
[ Sat Jul 13 00:16:31 2024 ] 	Batch(6600/6809) done. Loss: 0.0136  lr:0.000100
[ Sat Jul 13 00:16:48 2024 ] 	Batch(6700/6809) done. Loss: 0.1498  lr:0.000100
[ Sat Jul 13 00:17:07 2024 ] 	Batch(6800/6809) done. Loss: 0.1358  lr:0.000100
[ Sat Jul 13 00:17:08 2024 ] 	Mean training loss: 0.1083.
[ Sat Jul 13 00:17:08 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 00:17:08 2024 ] Training epoch: 79
[ Sat Jul 13 00:17:09 2024 ] 	Batch(0/6809) done. Loss: 0.1860  lr:0.000100
[ Sat Jul 13 00:17:27 2024 ] 	Batch(100/6809) done. Loss: 0.0844  lr:0.000100
[ Sat Jul 13 00:17:44 2024 ] 	Batch(200/6809) done. Loss: 0.1522  lr:0.000100
[ Sat Jul 13 00:18:02 2024 ] 	Batch(300/6809) done. Loss: 0.1230  lr:0.000100
[ Sat Jul 13 00:18:20 2024 ] 	Batch(400/6809) done. Loss: 0.0659  lr:0.000100
[ Sat Jul 13 00:18:38 2024 ] 
Training: Epoch [78/120], Step [499], Loss: 0.017972031608223915, Training Accuracy: 97.1
[ Sat Jul 13 00:18:38 2024 ] 	Batch(500/6809) done. Loss: 0.0116  lr:0.000100
[ Sat Jul 13 00:18:56 2024 ] 	Batch(600/6809) done. Loss: 0.0049  lr:0.000100
[ Sat Jul 13 00:19:14 2024 ] 	Batch(700/6809) done. Loss: 0.3714  lr:0.000100
[ Sat Jul 13 00:19:32 2024 ] 	Batch(800/6809) done. Loss: 0.0144  lr:0.000100
[ Sat Jul 13 00:19:50 2024 ] 	Batch(900/6809) done. Loss: 0.0921  lr:0.000100
[ Sat Jul 13 00:20:08 2024 ] 
Training: Epoch [78/120], Step [999], Loss: 0.06948184967041016, Training Accuracy: 97.3375
[ Sat Jul 13 00:20:08 2024 ] 	Batch(1000/6809) done. Loss: 0.1247  lr:0.000100
[ Sat Jul 13 00:20:26 2024 ] 	Batch(1100/6809) done. Loss: 0.2630  lr:0.000100
[ Sat Jul 13 00:20:44 2024 ] 	Batch(1200/6809) done. Loss: 0.1859  lr:0.000100
[ Sat Jul 13 00:21:02 2024 ] 	Batch(1300/6809) done. Loss: 0.0658  lr:0.000100
[ Sat Jul 13 00:21:19 2024 ] 	Batch(1400/6809) done. Loss: 0.0082  lr:0.000100
[ Sat Jul 13 00:21:37 2024 ] 
Training: Epoch [78/120], Step [1499], Loss: 0.046975597739219666, Training Accuracy: 97.29166666666667
[ Sat Jul 13 00:21:37 2024 ] 	Batch(1500/6809) done. Loss: 0.1844  lr:0.000100
[ Sat Jul 13 00:21:55 2024 ] 	Batch(1600/6809) done. Loss: 0.1005  lr:0.000100
[ Sat Jul 13 00:22:13 2024 ] 	Batch(1700/6809) done. Loss: 0.1126  lr:0.000100
[ Sat Jul 13 00:22:31 2024 ] 	Batch(1800/6809) done. Loss: 0.0329  lr:0.000100
[ Sat Jul 13 00:22:49 2024 ] 	Batch(1900/6809) done. Loss: 0.1222  lr:0.000100
[ Sat Jul 13 00:23:07 2024 ] 
Training: Epoch [78/120], Step [1999], Loss: 0.14819489419460297, Training Accuracy: 97.275
[ Sat Jul 13 00:23:07 2024 ] 	Batch(2000/6809) done. Loss: 0.0720  lr:0.000100
[ Sat Jul 13 00:23:25 2024 ] 	Batch(2100/6809) done. Loss: 0.3437  lr:0.000100
[ Sat Jul 13 00:23:43 2024 ] 	Batch(2200/6809) done. Loss: 0.0693  lr:0.000100
[ Sat Jul 13 00:24:01 2024 ] 	Batch(2300/6809) done. Loss: 0.0602  lr:0.000100
[ Sat Jul 13 00:24:19 2024 ] 	Batch(2400/6809) done. Loss: 0.1288  lr:0.000100
[ Sat Jul 13 00:24:37 2024 ] 
Training: Epoch [78/120], Step [2499], Loss: 0.030036117881536484, Training Accuracy: 97.345
[ Sat Jul 13 00:24:37 2024 ] 	Batch(2500/6809) done. Loss: 0.0780  lr:0.000100
[ Sat Jul 13 00:24:55 2024 ] 	Batch(2600/6809) done. Loss: 0.0365  lr:0.000100
[ Sat Jul 13 00:25:12 2024 ] 	Batch(2700/6809) done. Loss: 0.0318  lr:0.000100
[ Sat Jul 13 00:25:30 2024 ] 	Batch(2800/6809) done. Loss: 0.0329  lr:0.000100
[ Sat Jul 13 00:25:48 2024 ] 	Batch(2900/6809) done. Loss: 0.0910  lr:0.000100
[ Sat Jul 13 00:26:06 2024 ] 
Training: Epoch [78/120], Step [2999], Loss: 0.028520381078124046, Training Accuracy: 97.32916666666667
[ Sat Jul 13 00:26:07 2024 ] 	Batch(3000/6809) done. Loss: 0.1026  lr:0.000100
[ Sat Jul 13 00:26:25 2024 ] 	Batch(3100/6809) done. Loss: 0.1412  lr:0.000100
[ Sat Jul 13 00:26:44 2024 ] 	Batch(3200/6809) done. Loss: 0.0249  lr:0.000100
[ Sat Jul 13 00:27:02 2024 ] 	Batch(3300/6809) done. Loss: 0.0112  lr:0.000100
[ Sat Jul 13 00:27:20 2024 ] 	Batch(3400/6809) done. Loss: 0.0365  lr:0.000100
[ Sat Jul 13 00:27:37 2024 ] 
Training: Epoch [78/120], Step [3499], Loss: 0.7331571578979492, Training Accuracy: 97.26785714285714
[ Sat Jul 13 00:27:37 2024 ] 	Batch(3500/6809) done. Loss: 0.4216  lr:0.000100
[ Sat Jul 13 00:27:55 2024 ] 	Batch(3600/6809) done. Loss: 0.0058  lr:0.000100
[ Sat Jul 13 00:28:13 2024 ] 	Batch(3700/6809) done. Loss: 0.2962  lr:0.000100
[ Sat Jul 13 00:28:31 2024 ] 	Batch(3800/6809) done. Loss: 0.1239  lr:0.000100
[ Sat Jul 13 00:28:49 2024 ] 	Batch(3900/6809) done. Loss: 0.1726  lr:0.000100
[ Sat Jul 13 00:29:07 2024 ] 
Training: Epoch [78/120], Step [3999], Loss: 0.030794739723205566, Training Accuracy: 97.234375
[ Sat Jul 13 00:29:07 2024 ] 	Batch(4000/6809) done. Loss: 0.3042  lr:0.000100
[ Sat Jul 13 00:29:25 2024 ] 	Batch(4100/6809) done. Loss: 0.0644  lr:0.000100
[ Sat Jul 13 00:29:43 2024 ] 	Batch(4200/6809) done. Loss: 0.0613  lr:0.000100
[ Sat Jul 13 00:30:01 2024 ] 	Batch(4300/6809) done. Loss: 0.0354  lr:0.000100
[ Sat Jul 13 00:30:19 2024 ] 	Batch(4400/6809) done. Loss: 0.0148  lr:0.000100
[ Sat Jul 13 00:30:37 2024 ] 
Training: Epoch [78/120], Step [4499], Loss: 0.23066000640392303, Training Accuracy: 97.22222222222221
[ Sat Jul 13 00:30:37 2024 ] 	Batch(4500/6809) done. Loss: 0.0352  lr:0.000100
[ Sat Jul 13 00:30:55 2024 ] 	Batch(4600/6809) done. Loss: 0.0167  lr:0.000100
[ Sat Jul 13 00:31:13 2024 ] 	Batch(4700/6809) done. Loss: 0.1305  lr:0.000100
[ Sat Jul 13 00:31:31 2024 ] 	Batch(4800/6809) done. Loss: 0.0073  lr:0.000100
[ Sat Jul 13 00:31:49 2024 ] 	Batch(4900/6809) done. Loss: 0.4593  lr:0.000100
[ Sat Jul 13 00:32:06 2024 ] 
Training: Epoch [78/120], Step [4999], Loss: 0.004952130373567343, Training Accuracy: 97.2225
[ Sat Jul 13 00:32:07 2024 ] 	Batch(5000/6809) done. Loss: 0.0042  lr:0.000100
[ Sat Jul 13 00:32:25 2024 ] 	Batch(5100/6809) done. Loss: 0.0587  lr:0.000100
[ Sat Jul 13 00:32:43 2024 ] 	Batch(5200/6809) done. Loss: 0.1066  lr:0.000100
[ Sat Jul 13 00:33:01 2024 ] 	Batch(5300/6809) done. Loss: 0.2677  lr:0.000100
[ Sat Jul 13 00:33:20 2024 ] 	Batch(5400/6809) done. Loss: 0.1027  lr:0.000100
[ Sat Jul 13 00:33:38 2024 ] 
Training: Epoch [78/120], Step [5499], Loss: 0.3990854024887085, Training Accuracy: 97.23636363636363
[ Sat Jul 13 00:33:38 2024 ] 	Batch(5500/6809) done. Loss: 0.1130  lr:0.000100
[ Sat Jul 13 00:33:57 2024 ] 	Batch(5600/6809) done. Loss: 0.1222  lr:0.000100
[ Sat Jul 13 00:34:15 2024 ] 	Batch(5700/6809) done. Loss: 0.1716  lr:0.000100
[ Sat Jul 13 00:34:33 2024 ] 	Batch(5800/6809) done. Loss: 0.0635  lr:0.000100
[ Sat Jul 13 00:34:50 2024 ] 	Batch(5900/6809) done. Loss: 0.0430  lr:0.000100
[ Sat Jul 13 00:35:08 2024 ] 
Training: Epoch [78/120], Step [5999], Loss: 0.01422223262488842, Training Accuracy: 97.20208333333333
[ Sat Jul 13 00:35:09 2024 ] 	Batch(6000/6809) done. Loss: 0.0018  lr:0.000100
[ Sat Jul 13 00:35:27 2024 ] 	Batch(6100/6809) done. Loss: 0.1288  lr:0.000100
[ Sat Jul 13 00:35:44 2024 ] 	Batch(6200/6809) done. Loss: 0.0347  lr:0.000100
[ Sat Jul 13 00:36:02 2024 ] 	Batch(6300/6809) done. Loss: 0.1314  lr:0.000100
[ Sat Jul 13 00:36:20 2024 ] 	Batch(6400/6809) done. Loss: 0.4863  lr:0.000100
[ Sat Jul 13 00:36:38 2024 ] 
Training: Epoch [78/120], Step [6499], Loss: 0.1866590529680252, Training Accuracy: 97.225
[ Sat Jul 13 00:36:38 2024 ] 	Batch(6500/6809) done. Loss: 0.0393  lr:0.000100
[ Sat Jul 13 00:36:56 2024 ] 	Batch(6600/6809) done. Loss: 0.1204  lr:0.000100
[ Sat Jul 13 00:37:14 2024 ] 	Batch(6700/6809) done. Loss: 0.0166  lr:0.000100
[ Sat Jul 13 00:37:32 2024 ] 	Batch(6800/6809) done. Loss: 0.0674  lr:0.000100
[ Sat Jul 13 00:37:34 2024 ] 	Mean training loss: 0.1040.
[ Sat Jul 13 00:37:34 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 00:37:34 2024 ] Training epoch: 80
[ Sat Jul 13 00:37:34 2024 ] 	Batch(0/6809) done. Loss: 0.1302  lr:0.000100
[ Sat Jul 13 00:37:52 2024 ] 	Batch(100/6809) done. Loss: 0.0208  lr:0.000100
[ Sat Jul 13 00:38:10 2024 ] 	Batch(200/6809) done. Loss: 0.0590  lr:0.000100
[ Sat Jul 13 00:38:28 2024 ] 	Batch(300/6809) done. Loss: 0.0143  lr:0.000100
[ Sat Jul 13 00:38:46 2024 ] 	Batch(400/6809) done. Loss: 0.1755  lr:0.000100
[ Sat Jul 13 00:39:04 2024 ] 
Training: Epoch [79/120], Step [499], Loss: 0.03672381862998009, Training Accuracy: 97.5
[ Sat Jul 13 00:39:04 2024 ] 	Batch(500/6809) done. Loss: 0.0879  lr:0.000100
[ Sat Jul 13 00:39:22 2024 ] 	Batch(600/6809) done. Loss: 0.2736  lr:0.000100
[ Sat Jul 13 00:39:40 2024 ] 	Batch(700/6809) done. Loss: 0.0287  lr:0.000100
[ Sat Jul 13 00:39:58 2024 ] 	Batch(800/6809) done. Loss: 0.0789  lr:0.000100
[ Sat Jul 13 00:40:16 2024 ] 	Batch(900/6809) done. Loss: 0.0690  lr:0.000100
[ Sat Jul 13 00:40:33 2024 ] 
Training: Epoch [79/120], Step [999], Loss: 0.21478931605815887, Training Accuracy: 97.2125
[ Sat Jul 13 00:40:34 2024 ] 	Batch(1000/6809) done. Loss: 0.0227  lr:0.000100
[ Sat Jul 13 00:40:51 2024 ] 	Batch(1100/6809) done. Loss: 0.0734  lr:0.000100
[ Sat Jul 13 00:41:09 2024 ] 	Batch(1200/6809) done. Loss: 0.0137  lr:0.000100
[ Sat Jul 13 00:41:27 2024 ] 	Batch(1300/6809) done. Loss: 0.0033  lr:0.000100
[ Sat Jul 13 00:41:45 2024 ] 	Batch(1400/6809) done. Loss: 0.2350  lr:0.000100
[ Sat Jul 13 00:42:03 2024 ] 
Training: Epoch [79/120], Step [1499], Loss: 0.0369877889752388, Training Accuracy: 97.2
[ Sat Jul 13 00:42:03 2024 ] 	Batch(1500/6809) done. Loss: 0.2000  lr:0.000100
[ Sat Jul 13 00:42:21 2024 ] 	Batch(1600/6809) done. Loss: 0.0038  lr:0.000100
[ Sat Jul 13 00:42:39 2024 ] 	Batch(1700/6809) done. Loss: 0.1269  lr:0.000100
[ Sat Jul 13 00:42:57 2024 ] 	Batch(1800/6809) done. Loss: 0.1775  lr:0.000100
[ Sat Jul 13 00:43:15 2024 ] 	Batch(1900/6809) done. Loss: 0.0123  lr:0.000100
[ Sat Jul 13 00:43:33 2024 ] 
Training: Epoch [79/120], Step [1999], Loss: 0.02012796327471733, Training Accuracy: 97.3
[ Sat Jul 13 00:43:33 2024 ] 	Batch(2000/6809) done. Loss: 0.0584  lr:0.000100
[ Sat Jul 13 00:43:51 2024 ] 	Batch(2100/6809) done. Loss: 0.0222  lr:0.000100
[ Sat Jul 13 00:44:09 2024 ] 	Batch(2200/6809) done. Loss: 0.0074  lr:0.000100
[ Sat Jul 13 00:44:27 2024 ] 	Batch(2300/6809) done. Loss: 0.1677  lr:0.000100
[ Sat Jul 13 00:44:45 2024 ] 	Batch(2400/6809) done. Loss: 0.0101  lr:0.000100
[ Sat Jul 13 00:45:03 2024 ] 
Training: Epoch [79/120], Step [2499], Loss: 0.07893360406160355, Training Accuracy: 97.32499999999999
[ Sat Jul 13 00:45:03 2024 ] 	Batch(2500/6809) done. Loss: 0.0207  lr:0.000100
[ Sat Jul 13 00:45:21 2024 ] 	Batch(2600/6809) done. Loss: 0.4527  lr:0.000100
[ Sat Jul 13 00:45:39 2024 ] 	Batch(2700/6809) done. Loss: 0.0921  lr:0.000100
[ Sat Jul 13 00:45:57 2024 ] 	Batch(2800/6809) done. Loss: 0.0368  lr:0.000100
[ Sat Jul 13 00:46:16 2024 ] 	Batch(2900/6809) done. Loss: 0.0929  lr:0.000100
[ Sat Jul 13 00:46:34 2024 ] 
Training: Epoch [79/120], Step [2999], Loss: 0.1612059473991394, Training Accuracy: 97.28333333333333
[ Sat Jul 13 00:46:34 2024 ] 	Batch(3000/6809) done. Loss: 0.3639  lr:0.000100
[ Sat Jul 13 00:46:53 2024 ] 	Batch(3100/6809) done. Loss: 0.0183  lr:0.000100
[ Sat Jul 13 00:47:11 2024 ] 	Batch(3200/6809) done. Loss: 0.0805  lr:0.000100
[ Sat Jul 13 00:47:29 2024 ] 	Batch(3300/6809) done. Loss: 0.1265  lr:0.000100
[ Sat Jul 13 00:47:47 2024 ] 	Batch(3400/6809) done. Loss: 0.1029  lr:0.000100
[ Sat Jul 13 00:48:05 2024 ] 
Training: Epoch [79/120], Step [3499], Loss: 0.14002814888954163, Training Accuracy: 97.29285714285714
[ Sat Jul 13 00:48:05 2024 ] 	Batch(3500/6809) done. Loss: 0.0489  lr:0.000100
[ Sat Jul 13 00:48:23 2024 ] 	Batch(3600/6809) done. Loss: 0.0981  lr:0.000100
[ Sat Jul 13 00:48:41 2024 ] 	Batch(3700/6809) done. Loss: 0.0167  lr:0.000100
[ Sat Jul 13 00:48:59 2024 ] 	Batch(3800/6809) done. Loss: 0.1594  lr:0.000100
[ Sat Jul 13 00:49:17 2024 ] 	Batch(3900/6809) done. Loss: 0.0037  lr:0.000100
[ Sat Jul 13 00:49:34 2024 ] 
Training: Epoch [79/120], Step [3999], Loss: 0.0071700881235301495, Training Accuracy: 97.2625
[ Sat Jul 13 00:49:35 2024 ] 	Batch(4000/6809) done. Loss: 0.0219  lr:0.000100
[ Sat Jul 13 00:49:53 2024 ] 	Batch(4100/6809) done. Loss: 0.0359  lr:0.000100
[ Sat Jul 13 00:50:12 2024 ] 	Batch(4200/6809) done. Loss: 0.1937  lr:0.000100
[ Sat Jul 13 00:50:30 2024 ] 	Batch(4300/6809) done. Loss: 0.0207  lr:0.000100
[ Sat Jul 13 00:50:49 2024 ] 	Batch(4400/6809) done. Loss: 0.1172  lr:0.000100
[ Sat Jul 13 00:51:07 2024 ] 
Training: Epoch [79/120], Step [4499], Loss: 0.018897371366620064, Training Accuracy: 97.22222222222221
[ Sat Jul 13 00:51:08 2024 ] 	Batch(4500/6809) done. Loss: 0.0334  lr:0.000100
[ Sat Jul 13 00:51:26 2024 ] 	Batch(4600/6809) done. Loss: 0.0052  lr:0.000100
[ Sat Jul 13 00:51:45 2024 ] 	Batch(4700/6809) done. Loss: 0.0356  lr:0.000100
[ Sat Jul 13 00:52:03 2024 ] 	Batch(4800/6809) done. Loss: 0.0148  lr:0.000100
[ Sat Jul 13 00:52:21 2024 ] 	Batch(4900/6809) done. Loss: 0.0174  lr:0.000100
[ Sat Jul 13 00:52:39 2024 ] 
Training: Epoch [79/120], Step [4999], Loss: 0.11891176551580429, Training Accuracy: 97.23
[ Sat Jul 13 00:52:39 2024 ] 	Batch(5000/6809) done. Loss: 0.0123  lr:0.000100
[ Sat Jul 13 00:52:57 2024 ] 	Batch(5100/6809) done. Loss: 0.1115  lr:0.000100
[ Sat Jul 13 00:53:15 2024 ] 	Batch(5200/6809) done. Loss: 0.0231  lr:0.000100
[ Sat Jul 13 00:53:33 2024 ] 	Batch(5300/6809) done. Loss: 0.0338  lr:0.000100
[ Sat Jul 13 00:53:51 2024 ] 	Batch(5400/6809) done. Loss: 0.1979  lr:0.000100
[ Sat Jul 13 00:54:08 2024 ] 
Training: Epoch [79/120], Step [5499], Loss: 0.39262863993644714, Training Accuracy: 97.18181818181819
[ Sat Jul 13 00:54:09 2024 ] 	Batch(5500/6809) done. Loss: 0.4251  lr:0.000100
[ Sat Jul 13 00:54:27 2024 ] 	Batch(5600/6809) done. Loss: 0.0661  lr:0.000100
[ Sat Jul 13 00:54:45 2024 ] 	Batch(5700/6809) done. Loss: 0.2057  lr:0.000100
[ Sat Jul 13 00:55:02 2024 ] 	Batch(5800/6809) done. Loss: 0.0684  lr:0.000100
[ Sat Jul 13 00:55:20 2024 ] 	Batch(5900/6809) done. Loss: 0.1568  lr:0.000100
[ Sat Jul 13 00:55:38 2024 ] 
Training: Epoch [79/120], Step [5999], Loss: 0.07511075586080551, Training Accuracy: 97.21041666666666
[ Sat Jul 13 00:55:38 2024 ] 	Batch(6000/6809) done. Loss: 0.1891  lr:0.000100
[ Sat Jul 13 00:55:56 2024 ] 	Batch(6100/6809) done. Loss: 0.1196  lr:0.000100
[ Sat Jul 13 00:56:14 2024 ] 	Batch(6200/6809) done. Loss: 0.2126  lr:0.000100
[ Sat Jul 13 00:56:32 2024 ] 	Batch(6300/6809) done. Loss: 0.1161  lr:0.000100
[ Sat Jul 13 00:56:50 2024 ] 	Batch(6400/6809) done. Loss: 0.1381  lr:0.000100
[ Sat Jul 13 00:57:08 2024 ] 
Training: Epoch [79/120], Step [6499], Loss: 0.005095948465168476, Training Accuracy: 97.23653846153846
[ Sat Jul 13 00:57:08 2024 ] 	Batch(6500/6809) done. Loss: 0.6922  lr:0.000100
[ Sat Jul 13 00:57:26 2024 ] 	Batch(6600/6809) done. Loss: 0.0166  lr:0.000100
[ Sat Jul 13 00:57:44 2024 ] 	Batch(6700/6809) done. Loss: 0.0346  lr:0.000100
[ Sat Jul 13 00:58:02 2024 ] 	Batch(6800/6809) done. Loss: 0.0229  lr:0.000100
[ Sat Jul 13 00:58:03 2024 ] 	Mean training loss: 0.1061.
[ Sat Jul 13 00:58:03 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 00:58:04 2024 ] Eval epoch: 80
[ Sat Jul 13 01:03:38 2024 ] 	Mean val loss of 7435 batches: 1.0362929846293554.
[ Sat Jul 13 01:03:38 2024 ] 
Validation: Epoch [79/120], Samples [47863.0/59477], Loss: 0.4252505898475647, Validation Accuracy: 80.47312406476453
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 1 : 375 / 500 = 75 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 2 : 427 / 499 = 85 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 3 : 402 / 500 = 80 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 4 : 413 / 502 = 82 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 5 : 463 / 502 = 92 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 6 : 431 / 502 = 85 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 7 : 466 / 497 = 93 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 8 : 481 / 498 = 96 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 9 : 381 / 500 = 76 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 10 : 210 / 500 = 42 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 11 : 197 / 498 = 39 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 12 : 417 / 499 = 83 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 13 : 476 / 502 = 94 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 14 : 479 / 504 = 95 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 15 : 427 / 502 = 85 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 16 : 384 / 502 = 76 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 17 : 446 / 504 = 88 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 18 : 423 / 504 = 83 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 19 : 459 / 502 = 91 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 20 : 454 / 502 = 90 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 21 : 475 / 503 = 94 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 22 : 438 / 504 = 86 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 23 : 450 / 503 = 89 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 24 : 410 / 504 = 81 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 25 : 491 / 504 = 97 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 26 : 471 / 504 = 93 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 27 : 416 / 501 = 83 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 28 : 354 / 502 = 70 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 29 : 317 / 502 = 63 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 30 : 321 / 501 = 64 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 31 : 414 / 504 = 82 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 32 : 421 / 503 = 83 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 33 : 414 / 503 = 82 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 34 : 480 / 504 = 95 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 35 : 467 / 503 = 92 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 36 : 404 / 502 = 80 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 37 : 435 / 504 = 86 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 38 : 433 / 504 = 85 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 39 : 454 / 498 = 91 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 40 : 397 / 504 = 78 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 41 : 476 / 503 = 94 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 42 : 459 / 504 = 91 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 43 : 329 / 503 = 65 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 44 : 438 / 504 = 86 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 45 : 422 / 504 = 83 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 46 : 415 / 504 = 82 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 47 : 413 / 503 = 82 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 48 : 434 / 503 = 86 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 49 : 379 / 499 = 75 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 50 : 431 / 502 = 85 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 51 : 466 / 503 = 92 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 52 : 449 / 504 = 89 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 53 : 433 / 497 = 87 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 54 : 449 / 480 = 93 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 55 : 374 / 504 = 74 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 56 : 414 / 503 = 82 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 57 : 483 / 504 = 95 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 58 : 481 / 499 = 96 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 59 : 493 / 503 = 98 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 60 : 422 / 479 = 88 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 61 : 420 / 484 = 86 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 62 : 393 / 487 = 80 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 63 : 448 / 489 = 91 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 64 : 369 / 488 = 75 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 65 : 457 / 490 = 93 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 66 : 325 / 488 = 66 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 67 : 373 / 490 = 76 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 68 : 281 / 490 = 57 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 69 : 376 / 490 = 76 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 70 : 212 / 490 = 43 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 71 : 207 / 490 = 42 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 72 : 203 / 488 = 41 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 73 : 278 / 486 = 57 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 74 : 298 / 481 = 61 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 75 : 269 / 488 = 55 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 76 : 334 / 489 = 68 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 77 : 338 / 488 = 69 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 78 : 374 / 488 = 76 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 79 : 454 / 490 = 92 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 80 : 403 / 489 = 82 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 81 : 311 / 491 = 63 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 82 : 314 / 491 = 63 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 83 : 259 / 489 = 52 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 84 : 392 / 489 = 80 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 85 : 386 / 489 = 78 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 86 : 440 / 491 = 89 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 87 : 439 / 492 = 89 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 88 : 376 / 491 = 76 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 89 : 409 / 492 = 83 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 90 : 263 / 490 = 53 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 91 : 400 / 482 = 82 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 92 : 376 / 490 = 76 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 93 : 364 / 487 = 74 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 94 : 424 / 489 = 86 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 95 : 417 / 490 = 85 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 96 : 467 / 491 = 95 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 97 : 464 / 490 = 94 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 98 : 451 / 491 = 91 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 99 : 445 / 491 = 90 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 100 : 450 / 491 = 91 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 101 : 436 / 491 = 88 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 102 : 291 / 492 = 59 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 103 : 387 / 492 = 78 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 104 : 301 / 491 = 61 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 105 : 272 / 491 = 55 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 106 : 278 / 492 = 56 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 107 : 409 / 491 = 83 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 108 : 398 / 492 = 80 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 109 : 332 / 490 = 67 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 110 : 415 / 491 = 84 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 111 : 463 / 492 = 94 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 112 : 455 / 492 = 92 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 113 : 447 / 491 = 91 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 114 : 412 / 491 = 83 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 115 : 429 / 492 = 87 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 116 : 401 / 491 = 81 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 117 : 440 / 492 = 89 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 118 : 437 / 490 = 89 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 119 : 451 / 492 = 91 %
[ Sat Jul 13 01:03:38 2024 ] Accuracy of 120 : 417 / 500 = 83 %
[ Sat Jul 13 01:03:38 2024 ] Training epoch: 81
[ Sat Jul 13 01:03:38 2024 ] 	Batch(0/6809) done. Loss: 0.0100  lr:0.000100
[ Sat Jul 13 01:03:57 2024 ] 	Batch(100/6809) done. Loss: 0.0182  lr:0.000100
[ Sat Jul 13 01:04:15 2024 ] 	Batch(200/6809) done. Loss: 0.0220  lr:0.000100
[ Sat Jul 13 01:04:34 2024 ] 	Batch(300/6809) done. Loss: 0.0203  lr:0.000100
[ Sat Jul 13 01:04:52 2024 ] 	Batch(400/6809) done. Loss: 0.0609  lr:0.000100
[ Sat Jul 13 01:05:11 2024 ] 
Training: Epoch [80/120], Step [499], Loss: 0.037280045449733734, Training Accuracy: 97.275
[ Sat Jul 13 01:05:11 2024 ] 	Batch(500/6809) done. Loss: 0.0129  lr:0.000100
[ Sat Jul 13 01:05:29 2024 ] 	Batch(600/6809) done. Loss: 0.0570  lr:0.000100
[ Sat Jul 13 01:05:48 2024 ] 	Batch(700/6809) done. Loss: 0.2106  lr:0.000100
[ Sat Jul 13 01:06:07 2024 ] 	Batch(800/6809) done. Loss: 0.0052  lr:0.000100
[ Sat Jul 13 01:06:25 2024 ] 	Batch(900/6809) done. Loss: 0.1892  lr:0.000100
[ Sat Jul 13 01:06:44 2024 ] 
Training: Epoch [80/120], Step [999], Loss: 0.010459436103701591, Training Accuracy: 97.3375
[ Sat Jul 13 01:06:44 2024 ] 	Batch(1000/6809) done. Loss: 0.0056  lr:0.000100
[ Sat Jul 13 01:07:02 2024 ] 	Batch(1100/6809) done. Loss: 0.3180  lr:0.000100
[ Sat Jul 13 01:07:21 2024 ] 	Batch(1200/6809) done. Loss: 0.0091  lr:0.000100
[ Sat Jul 13 01:07:38 2024 ] 	Batch(1300/6809) done. Loss: 0.4074  lr:0.000100
[ Sat Jul 13 01:07:56 2024 ] 	Batch(1400/6809) done. Loss: 0.1800  lr:0.000100
[ Sat Jul 13 01:08:14 2024 ] 
Training: Epoch [80/120], Step [1499], Loss: 0.18058890104293823, Training Accuracy: 97.38333333333333
[ Sat Jul 13 01:08:14 2024 ] 	Batch(1500/6809) done. Loss: 0.3383  lr:0.000100
[ Sat Jul 13 01:08:32 2024 ] 	Batch(1600/6809) done. Loss: 0.0688  lr:0.000100
[ Sat Jul 13 01:08:50 2024 ] 	Batch(1700/6809) done. Loss: 0.1491  lr:0.000100
[ Sat Jul 13 01:09:08 2024 ] 	Batch(1800/6809) done. Loss: 0.0539  lr:0.000100
[ Sat Jul 13 01:09:26 2024 ] 	Batch(1900/6809) done. Loss: 0.1442  lr:0.000100
[ Sat Jul 13 01:09:44 2024 ] 
Training: Epoch [80/120], Step [1999], Loss: 0.08292587101459503, Training Accuracy: 97.28750000000001
[ Sat Jul 13 01:09:44 2024 ] 	Batch(2000/6809) done. Loss: 0.0568  lr:0.000100
[ Sat Jul 13 01:10:02 2024 ] 	Batch(2100/6809) done. Loss: 0.1799  lr:0.000100
[ Sat Jul 13 01:10:20 2024 ] 	Batch(2200/6809) done. Loss: 0.0385  lr:0.000100
[ Sat Jul 13 01:10:38 2024 ] 	Batch(2300/6809) done. Loss: 0.0522  lr:0.000100
[ Sat Jul 13 01:10:56 2024 ] 	Batch(2400/6809) done. Loss: 0.1081  lr:0.000100
[ Sat Jul 13 01:11:13 2024 ] 
Training: Epoch [80/120], Step [2499], Loss: 0.08534076809883118, Training Accuracy: 97.265
[ Sat Jul 13 01:11:13 2024 ] 	Batch(2500/6809) done. Loss: 0.0189  lr:0.000100
[ Sat Jul 13 01:11:31 2024 ] 	Batch(2600/6809) done. Loss: 0.0238  lr:0.000100
[ Sat Jul 13 01:11:49 2024 ] 	Batch(2700/6809) done. Loss: 0.0599  lr:0.000100
[ Sat Jul 13 01:12:07 2024 ] 	Batch(2800/6809) done. Loss: 0.0158  lr:0.000100
[ Sat Jul 13 01:12:26 2024 ] 	Batch(2900/6809) done. Loss: 0.0630  lr:0.000100
[ Sat Jul 13 01:12:44 2024 ] 
Training: Epoch [80/120], Step [2999], Loss: 0.20674419403076172, Training Accuracy: 97.19583333333334
[ Sat Jul 13 01:12:44 2024 ] 	Batch(3000/6809) done. Loss: 0.0848  lr:0.000100
[ Sat Jul 13 01:13:03 2024 ] 	Batch(3100/6809) done. Loss: 0.1117  lr:0.000100
[ Sat Jul 13 01:13:21 2024 ] 	Batch(3200/6809) done. Loss: 0.0014  lr:0.000100
[ Sat Jul 13 01:13:39 2024 ] 	Batch(3300/6809) done. Loss: 0.0230  lr:0.000100
[ Sat Jul 13 01:13:57 2024 ] 	Batch(3400/6809) done. Loss: 0.0168  lr:0.000100
[ Sat Jul 13 01:14:15 2024 ] 
Training: Epoch [80/120], Step [3499], Loss: 0.09071051329374313, Training Accuracy: 97.23928571428571
[ Sat Jul 13 01:14:15 2024 ] 	Batch(3500/6809) done. Loss: 0.0301  lr:0.000100
[ Sat Jul 13 01:14:33 2024 ] 	Batch(3600/6809) done. Loss: 0.0293  lr:0.000100
[ Sat Jul 13 01:14:51 2024 ] 	Batch(3700/6809) done. Loss: 0.0228  lr:0.000100
[ Sat Jul 13 01:15:09 2024 ] 	Batch(3800/6809) done. Loss: 0.1432  lr:0.000100
[ Sat Jul 13 01:15:26 2024 ] 	Batch(3900/6809) done. Loss: 0.0862  lr:0.000100
[ Sat Jul 13 01:15:44 2024 ] 
Training: Epoch [80/120], Step [3999], Loss: 0.07501301914453506, Training Accuracy: 97.24374999999999
[ Sat Jul 13 01:15:45 2024 ] 	Batch(4000/6809) done. Loss: 0.0475  lr:0.000100
[ Sat Jul 13 01:16:02 2024 ] 	Batch(4100/6809) done. Loss: 0.0053  lr:0.000100
[ Sat Jul 13 01:16:20 2024 ] 	Batch(4200/6809) done. Loss: 0.3401  lr:0.000100
[ Sat Jul 13 01:16:38 2024 ] 	Batch(4300/6809) done. Loss: 0.0773  lr:0.000100
[ Sat Jul 13 01:16:56 2024 ] 	Batch(4400/6809) done. Loss: 0.0592  lr:0.000100
[ Sat Jul 13 01:17:14 2024 ] 
Training: Epoch [80/120], Step [4499], Loss: 0.06396108120679855, Training Accuracy: 97.24166666666667
[ Sat Jul 13 01:17:14 2024 ] 	Batch(4500/6809) done. Loss: 0.1407  lr:0.000100
[ Sat Jul 13 01:17:32 2024 ] 	Batch(4600/6809) done. Loss: 0.0301  lr:0.000100
[ Sat Jul 13 01:17:50 2024 ] 	Batch(4700/6809) done. Loss: 0.1634  lr:0.000100
[ Sat Jul 13 01:18:08 2024 ] 	Batch(4800/6809) done. Loss: 0.1129  lr:0.000100
[ Sat Jul 13 01:18:26 2024 ] 	Batch(4900/6809) done. Loss: 0.0227  lr:0.000100
[ Sat Jul 13 01:18:43 2024 ] 
Training: Epoch [80/120], Step [4999], Loss: 0.11472459137439728, Training Accuracy: 97.2525
[ Sat Jul 13 01:18:44 2024 ] 	Batch(5000/6809) done. Loss: 0.0192  lr:0.000100
[ Sat Jul 13 01:19:02 2024 ] 	Batch(5100/6809) done. Loss: 0.0302  lr:0.000100
[ Sat Jul 13 01:19:20 2024 ] 	Batch(5200/6809) done. Loss: 0.0514  lr:0.000100
[ Sat Jul 13 01:19:37 2024 ] 	Batch(5300/6809) done. Loss: 0.0119  lr:0.000100
[ Sat Jul 13 01:19:55 2024 ] 	Batch(5400/6809) done. Loss: 0.0188  lr:0.000100
[ Sat Jul 13 01:20:13 2024 ] 
Training: Epoch [80/120], Step [5499], Loss: 0.2333880066871643, Training Accuracy: 97.275
[ Sat Jul 13 01:20:13 2024 ] 	Batch(5500/6809) done. Loss: 0.0246  lr:0.000100
[ Sat Jul 13 01:20:31 2024 ] 	Batch(5600/6809) done. Loss: 0.0082  lr:0.000100
[ Sat Jul 13 01:20:50 2024 ] 	Batch(5700/6809) done. Loss: 0.0335  lr:0.000100
[ Sat Jul 13 01:21:08 2024 ] 	Batch(5800/6809) done. Loss: 0.2603  lr:0.000100
[ Sat Jul 13 01:21:27 2024 ] 	Batch(5900/6809) done. Loss: 0.0074  lr:0.000100
[ Sat Jul 13 01:21:45 2024 ] 
Training: Epoch [80/120], Step [5999], Loss: 0.01865449734032154, Training Accuracy: 97.27291666666666
[ Sat Jul 13 01:21:46 2024 ] 	Batch(6000/6809) done. Loss: 0.0422  lr:0.000100
[ Sat Jul 13 01:22:04 2024 ] 	Batch(6100/6809) done. Loss: 0.0758  lr:0.000100
[ Sat Jul 13 01:22:23 2024 ] 	Batch(6200/6809) done. Loss: 0.1644  lr:0.000100
[ Sat Jul 13 01:22:41 2024 ] 	Batch(6300/6809) done. Loss: 0.0079  lr:0.000100
[ Sat Jul 13 01:23:00 2024 ] 	Batch(6400/6809) done. Loss: 0.0799  lr:0.000100
[ Sat Jul 13 01:23:18 2024 ] 
Training: Epoch [80/120], Step [6499], Loss: 0.1714017242193222, Training Accuracy: 97.26153846153846
[ Sat Jul 13 01:23:18 2024 ] 	Batch(6500/6809) done. Loss: 0.0213  lr:0.000100
[ Sat Jul 13 01:23:37 2024 ] 	Batch(6600/6809) done. Loss: 0.0379  lr:0.000100
[ Sat Jul 13 01:23:55 2024 ] 	Batch(6700/6809) done. Loss: 0.0751  lr:0.000100
[ Sat Jul 13 01:24:14 2024 ] 	Batch(6800/6809) done. Loss: 0.0278  lr:0.000100
[ Sat Jul 13 01:24:16 2024 ] 	Mean training loss: 0.1070.
[ Sat Jul 13 01:24:16 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 01:24:16 2024 ] Training epoch: 82
[ Sat Jul 13 01:24:16 2024 ] 	Batch(0/6809) done. Loss: 0.0391  lr:0.000100
[ Sat Jul 13 01:24:35 2024 ] 	Batch(100/6809) done. Loss: 0.0256  lr:0.000100
[ Sat Jul 13 01:24:53 2024 ] 	Batch(200/6809) done. Loss: 0.2734  lr:0.000100
[ Sat Jul 13 01:25:11 2024 ] 	Batch(300/6809) done. Loss: 0.1051  lr:0.000100
[ Sat Jul 13 01:25:29 2024 ] 	Batch(400/6809) done. Loss: 0.1028  lr:0.000100
[ Sat Jul 13 01:25:47 2024 ] 
Training: Epoch [81/120], Step [499], Loss: 0.39765626192092896, Training Accuracy: 97.45
[ Sat Jul 13 01:25:47 2024 ] 	Batch(500/6809) done. Loss: 0.2273  lr:0.000100
[ Sat Jul 13 01:26:05 2024 ] 	Batch(600/6809) done. Loss: 0.0048  lr:0.000100
[ Sat Jul 13 01:26:23 2024 ] 	Batch(700/6809) done. Loss: 0.0110  lr:0.000100
[ Sat Jul 13 01:26:41 2024 ] 	Batch(800/6809) done. Loss: 0.0410  lr:0.000100
[ Sat Jul 13 01:26:59 2024 ] 	Batch(900/6809) done. Loss: 0.0870  lr:0.000100
[ Sat Jul 13 01:27:17 2024 ] 
Training: Epoch [81/120], Step [999], Loss: 0.025852331891655922, Training Accuracy: 97.55
[ Sat Jul 13 01:27:17 2024 ] 	Batch(1000/6809) done. Loss: 0.0526  lr:0.000100
[ Sat Jul 13 01:27:35 2024 ] 	Batch(1100/6809) done. Loss: 0.1721  lr:0.000100
[ Sat Jul 13 01:27:53 2024 ] 	Batch(1200/6809) done. Loss: 0.4216  lr:0.000100
[ Sat Jul 13 01:28:11 2024 ] 	Batch(1300/6809) done. Loss: 0.0141  lr:0.000100
[ Sat Jul 13 01:28:29 2024 ] 	Batch(1400/6809) done. Loss: 0.0203  lr:0.000100
[ Sat Jul 13 01:28:46 2024 ] 
Training: Epoch [81/120], Step [1499], Loss: 0.007349349092692137, Training Accuracy: 97.56666666666666
[ Sat Jul 13 01:28:46 2024 ] 	Batch(1500/6809) done. Loss: 0.0213  lr:0.000100
[ Sat Jul 13 01:29:04 2024 ] 	Batch(1600/6809) done. Loss: 0.0623  lr:0.000100
[ Sat Jul 13 01:29:23 2024 ] 	Batch(1700/6809) done. Loss: 0.1638  lr:0.000100
[ Sat Jul 13 01:29:41 2024 ] 	Batch(1800/6809) done. Loss: 0.2042  lr:0.000100
[ Sat Jul 13 01:30:00 2024 ] 	Batch(1900/6809) done. Loss: 0.0141  lr:0.000100
[ Sat Jul 13 01:30:18 2024 ] 
Training: Epoch [81/120], Step [1999], Loss: 0.1942540854215622, Training Accuracy: 97.425
[ Sat Jul 13 01:30:19 2024 ] 	Batch(2000/6809) done. Loss: 0.0088  lr:0.000100
[ Sat Jul 13 01:30:36 2024 ] 	Batch(2100/6809) done. Loss: 0.0557  lr:0.000100
[ Sat Jul 13 01:30:54 2024 ] 	Batch(2200/6809) done. Loss: 0.0741  lr:0.000100
[ Sat Jul 13 01:31:12 2024 ] 	Batch(2300/6809) done. Loss: 0.0888  lr:0.000100
[ Sat Jul 13 01:31:30 2024 ] 	Batch(2400/6809) done. Loss: 0.1765  lr:0.000100
[ Sat Jul 13 01:31:48 2024 ] 
Training: Epoch [81/120], Step [2499], Loss: 0.21974530816078186, Training Accuracy: 97.32499999999999
[ Sat Jul 13 01:31:48 2024 ] 	Batch(2500/6809) done. Loss: 0.0368  lr:0.000100
[ Sat Jul 13 01:32:06 2024 ] 	Batch(2600/6809) done. Loss: 0.0118  lr:0.000100
[ Sat Jul 13 01:32:24 2024 ] 	Batch(2700/6809) done. Loss: 0.0482  lr:0.000100
[ Sat Jul 13 01:32:42 2024 ] 	Batch(2800/6809) done. Loss: 0.0443  lr:0.000100
[ Sat Jul 13 01:33:00 2024 ] 	Batch(2900/6809) done. Loss: 0.0271  lr:0.000100
[ Sat Jul 13 01:33:19 2024 ] 
Training: Epoch [81/120], Step [2999], Loss: 0.02912876196205616, Training Accuracy: 97.27083333333333
[ Sat Jul 13 01:33:19 2024 ] 	Batch(3000/6809) done. Loss: 0.6282  lr:0.000100
[ Sat Jul 13 01:33:37 2024 ] 	Batch(3100/6809) done. Loss: 0.4164  lr:0.000100
[ Sat Jul 13 01:33:55 2024 ] 	Batch(3200/6809) done. Loss: 0.3640  lr:0.000100
[ Sat Jul 13 01:34:13 2024 ] 	Batch(3300/6809) done. Loss: 0.2119  lr:0.000100
[ Sat Jul 13 01:34:31 2024 ] 	Batch(3400/6809) done. Loss: 0.0436  lr:0.000100
[ Sat Jul 13 01:34:49 2024 ] 
Training: Epoch [81/120], Step [3499], Loss: 0.09721101075410843, Training Accuracy: 97.35714285714285
[ Sat Jul 13 01:34:49 2024 ] 	Batch(3500/6809) done. Loss: 0.0562  lr:0.000100
[ Sat Jul 13 01:35:07 2024 ] 	Batch(3600/6809) done. Loss: 0.0196  lr:0.000100
[ Sat Jul 13 01:35:25 2024 ] 	Batch(3700/6809) done. Loss: 0.1152  lr:0.000100
[ Sat Jul 13 01:35:43 2024 ] 	Batch(3800/6809) done. Loss: 0.0322  lr:0.000100
[ Sat Jul 13 01:36:01 2024 ] 	Batch(3900/6809) done. Loss: 0.0097  lr:0.000100
[ Sat Jul 13 01:36:19 2024 ] 
Training: Epoch [81/120], Step [3999], Loss: 0.28306907415390015, Training Accuracy: 97.35312499999999
[ Sat Jul 13 01:36:19 2024 ] 	Batch(4000/6809) done. Loss: 0.0982  lr:0.000100
[ Sat Jul 13 01:36:37 2024 ] 	Batch(4100/6809) done. Loss: 0.0256  lr:0.000100
[ Sat Jul 13 01:36:55 2024 ] 	Batch(4200/6809) done. Loss: 0.0421  lr:0.000100
[ Sat Jul 13 01:37:13 2024 ] 	Batch(4300/6809) done. Loss: 0.1143  lr:0.000100
[ Sat Jul 13 01:37:31 2024 ] 	Batch(4400/6809) done. Loss: 0.2533  lr:0.000100
[ Sat Jul 13 01:37:50 2024 ] 
Training: Epoch [81/120], Step [4499], Loss: 0.03091745637357235, Training Accuracy: 97.34722222222221
[ Sat Jul 13 01:37:50 2024 ] 	Batch(4500/6809) done. Loss: 0.0033  lr:0.000100
[ Sat Jul 13 01:38:08 2024 ] 	Batch(4600/6809) done. Loss: 0.0045  lr:0.000100
[ Sat Jul 13 01:38:27 2024 ] 	Batch(4700/6809) done. Loss: 0.0900  lr:0.000100
[ Sat Jul 13 01:38:46 2024 ] 	Batch(4800/6809) done. Loss: 0.2903  lr:0.000100
[ Sat Jul 13 01:39:03 2024 ] 	Batch(4900/6809) done. Loss: 0.0338  lr:0.000100
[ Sat Jul 13 01:39:21 2024 ] 
Training: Epoch [81/120], Step [4999], Loss: 0.10084867477416992, Training Accuracy: 97.3875
[ Sat Jul 13 01:39:21 2024 ] 	Batch(5000/6809) done. Loss: 0.0495  lr:0.000100
[ Sat Jul 13 01:39:39 2024 ] 	Batch(5100/6809) done. Loss: 0.0118  lr:0.000100
[ Sat Jul 13 01:39:57 2024 ] 	Batch(5200/6809) done. Loss: 0.0253  lr:0.000100
[ Sat Jul 13 01:40:15 2024 ] 	Batch(5300/6809) done. Loss: 0.0496  lr:0.000100
[ Sat Jul 13 01:40:33 2024 ] 	Batch(5400/6809) done. Loss: 0.0393  lr:0.000100
[ Sat Jul 13 01:40:51 2024 ] 
Training: Epoch [81/120], Step [5499], Loss: 0.03899344056844711, Training Accuracy: 97.39318181818182
[ Sat Jul 13 01:40:51 2024 ] 	Batch(5500/6809) done. Loss: 0.0155  lr:0.000100
[ Sat Jul 13 01:41:09 2024 ] 	Batch(5600/6809) done. Loss: 0.2868  lr:0.000100
[ Sat Jul 13 01:41:27 2024 ] 	Batch(5700/6809) done. Loss: 0.0823  lr:0.000100
[ Sat Jul 13 01:41:45 2024 ] 	Batch(5800/6809) done. Loss: 0.3594  lr:0.000100
[ Sat Jul 13 01:42:03 2024 ] 	Batch(5900/6809) done. Loss: 0.0390  lr:0.000100
[ Sat Jul 13 01:42:20 2024 ] 
Training: Epoch [81/120], Step [5999], Loss: 0.22018715739250183, Training Accuracy: 97.36041666666667
[ Sat Jul 13 01:42:21 2024 ] 	Batch(6000/6809) done. Loss: 0.2911  lr:0.000100
[ Sat Jul 13 01:42:39 2024 ] 	Batch(6100/6809) done. Loss: 0.0470  lr:0.000100
[ Sat Jul 13 01:42:58 2024 ] 	Batch(6200/6809) done. Loss: 0.1022  lr:0.000100
[ Sat Jul 13 01:43:16 2024 ] 	Batch(6300/6809) done. Loss: 0.0238  lr:0.000100
[ Sat Jul 13 01:43:35 2024 ] 	Batch(6400/6809) done. Loss: 0.2203  lr:0.000100
[ Sat Jul 13 01:43:53 2024 ] 
Training: Epoch [81/120], Step [6499], Loss: 0.6499446630477905, Training Accuracy: 97.35384615384616
[ Sat Jul 13 01:43:53 2024 ] 	Batch(6500/6809) done. Loss: 0.1021  lr:0.000100
[ Sat Jul 13 01:44:12 2024 ] 	Batch(6600/6809) done. Loss: 0.0140  lr:0.000100
[ Sat Jul 13 01:44:31 2024 ] 	Batch(6700/6809) done. Loss: 0.0029  lr:0.000100
[ Sat Jul 13 01:44:49 2024 ] 	Batch(6800/6809) done. Loss: 0.0725  lr:0.000100
[ Sat Jul 13 01:44:51 2024 ] 	Mean training loss: 0.1044.
[ Sat Jul 13 01:44:51 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 01:44:51 2024 ] Training epoch: 83
[ Sat Jul 13 01:44:51 2024 ] 	Batch(0/6809) done. Loss: 0.1122  lr:0.000100
[ Sat Jul 13 01:45:10 2024 ] 	Batch(100/6809) done. Loss: 0.0044  lr:0.000100
[ Sat Jul 13 01:45:28 2024 ] 	Batch(200/6809) done. Loss: 0.0400  lr:0.000100
[ Sat Jul 13 01:45:46 2024 ] 	Batch(300/6809) done. Loss: 0.2223  lr:0.000100
[ Sat Jul 13 01:46:05 2024 ] 	Batch(400/6809) done. Loss: 0.3280  lr:0.000100
[ Sat Jul 13 01:46:23 2024 ] 
Training: Epoch [82/120], Step [499], Loss: 0.235334113240242, Training Accuracy: 97.25
[ Sat Jul 13 01:46:23 2024 ] 	Batch(500/6809) done. Loss: 0.1927  lr:0.000100
[ Sat Jul 13 01:46:42 2024 ] 	Batch(600/6809) done. Loss: 0.0334  lr:0.000100
[ Sat Jul 13 01:47:00 2024 ] 	Batch(700/6809) done. Loss: 0.0089  lr:0.000100
[ Sat Jul 13 01:47:18 2024 ] 	Batch(800/6809) done. Loss: 0.4847  lr:0.000100
[ Sat Jul 13 01:47:37 2024 ] 	Batch(900/6809) done. Loss: 0.0511  lr:0.000100
[ Sat Jul 13 01:47:55 2024 ] 
Training: Epoch [82/120], Step [999], Loss: 0.013292823918163776, Training Accuracy: 97.125
[ Sat Jul 13 01:47:55 2024 ] 	Batch(1000/6809) done. Loss: 0.0338  lr:0.000100
[ Sat Jul 13 01:48:13 2024 ] 	Batch(1100/6809) done. Loss: 0.0121  lr:0.000100
[ Sat Jul 13 01:48:32 2024 ] 	Batch(1200/6809) done. Loss: 0.0617  lr:0.000100
[ Sat Jul 13 01:48:50 2024 ] 	Batch(1300/6809) done. Loss: 0.0523  lr:0.000100
[ Sat Jul 13 01:49:08 2024 ] 	Batch(1400/6809) done. Loss: 0.0351  lr:0.000100
[ Sat Jul 13 01:49:26 2024 ] 
Training: Epoch [82/120], Step [1499], Loss: 0.10627231746912003, Training Accuracy: 97.34166666666667
[ Sat Jul 13 01:49:27 2024 ] 	Batch(1500/6809) done. Loss: 0.0323  lr:0.000100
[ Sat Jul 13 01:49:45 2024 ] 	Batch(1600/6809) done. Loss: 0.0592  lr:0.000100
[ Sat Jul 13 01:50:03 2024 ] 	Batch(1700/6809) done. Loss: 0.1283  lr:0.000100
[ Sat Jul 13 01:50:22 2024 ] 	Batch(1800/6809) done. Loss: 0.0903  lr:0.000100
[ Sat Jul 13 01:50:40 2024 ] 	Batch(1900/6809) done. Loss: 0.0915  lr:0.000100
[ Sat Jul 13 01:50:58 2024 ] 
Training: Epoch [82/120], Step [1999], Loss: 0.02083783783018589, Training Accuracy: 97.38125
[ Sat Jul 13 01:50:59 2024 ] 	Batch(2000/6809) done. Loss: 0.0223  lr:0.000100
[ Sat Jul 13 01:51:17 2024 ] 	Batch(2100/6809) done. Loss: 0.0444  lr:0.000100
[ Sat Jul 13 01:51:35 2024 ] 	Batch(2200/6809) done. Loss: 0.0683  lr:0.000100
[ Sat Jul 13 01:51:54 2024 ] 	Batch(2300/6809) done. Loss: 0.0505  lr:0.000100
[ Sat Jul 13 01:52:12 2024 ] 	Batch(2400/6809) done. Loss: 0.0460  lr:0.000100
[ Sat Jul 13 01:52:30 2024 ] 
Training: Epoch [82/120], Step [2499], Loss: 0.025824911892414093, Training Accuracy: 97.38
[ Sat Jul 13 01:52:30 2024 ] 	Batch(2500/6809) done. Loss: 0.0068  lr:0.000100
[ Sat Jul 13 01:52:49 2024 ] 	Batch(2600/6809) done. Loss: 0.0928  lr:0.000100
[ Sat Jul 13 01:53:07 2024 ] 	Batch(2700/6809) done. Loss: 0.0243  lr:0.000100
[ Sat Jul 13 01:53:25 2024 ] 	Batch(2800/6809) done. Loss: 0.0401  lr:0.000100
[ Sat Jul 13 01:53:44 2024 ] 	Batch(2900/6809) done. Loss: 0.2321  lr:0.000100
[ Sat Jul 13 01:54:02 2024 ] 
Training: Epoch [82/120], Step [2999], Loss: 0.1575944423675537, Training Accuracy: 97.3125
[ Sat Jul 13 01:54:02 2024 ] 	Batch(3000/6809) done. Loss: 0.0042  lr:0.000100
[ Sat Jul 13 01:54:21 2024 ] 	Batch(3100/6809) done. Loss: 0.2971  lr:0.000100
[ Sat Jul 13 01:54:39 2024 ] 	Batch(3200/6809) done. Loss: 0.0317  lr:0.000100
[ Sat Jul 13 01:54:58 2024 ] 	Batch(3300/6809) done. Loss: 0.0581  lr:0.000100
[ Sat Jul 13 01:55:16 2024 ] 	Batch(3400/6809) done. Loss: 0.0529  lr:0.000100
[ Sat Jul 13 01:55:34 2024 ] 
Training: Epoch [82/120], Step [3499], Loss: 0.32751598954200745, Training Accuracy: 97.30714285714286
[ Sat Jul 13 01:55:35 2024 ] 	Batch(3500/6809) done. Loss: 0.0050  lr:0.000100
[ Sat Jul 13 01:55:53 2024 ] 	Batch(3600/6809) done. Loss: 0.0081  lr:0.000100
[ Sat Jul 13 01:56:12 2024 ] 	Batch(3700/6809) done. Loss: 0.0742  lr:0.000100
[ Sat Jul 13 01:56:30 2024 ] 	Batch(3800/6809) done. Loss: 0.0599  lr:0.000100
[ Sat Jul 13 01:56:49 2024 ] 	Batch(3900/6809) done. Loss: 0.0459  lr:0.000100
[ Sat Jul 13 01:57:07 2024 ] 
Training: Epoch [82/120], Step [3999], Loss: 0.21725261211395264, Training Accuracy: 97.346875
[ Sat Jul 13 01:57:08 2024 ] 	Batch(4000/6809) done. Loss: 0.0213  lr:0.000100
[ Sat Jul 13 01:57:26 2024 ] 	Batch(4100/6809) done. Loss: 0.0448  lr:0.000100
[ Sat Jul 13 01:57:45 2024 ] 	Batch(4200/6809) done. Loss: 0.0126  lr:0.000100
[ Sat Jul 13 01:58:03 2024 ] 	Batch(4300/6809) done. Loss: 0.2706  lr:0.000100
[ Sat Jul 13 01:58:22 2024 ] 	Batch(4400/6809) done. Loss: 0.1822  lr:0.000100
[ Sat Jul 13 01:58:40 2024 ] 
Training: Epoch [82/120], Step [4499], Loss: 0.0200169887393713, Training Accuracy: 97.37777777777778
[ Sat Jul 13 01:58:40 2024 ] 	Batch(4500/6809) done. Loss: 0.0202  lr:0.000100
[ Sat Jul 13 01:58:58 2024 ] 	Batch(4600/6809) done. Loss: 0.0146  lr:0.000100
[ Sat Jul 13 01:59:16 2024 ] 	Batch(4700/6809) done. Loss: 0.0211  lr:0.000100
[ Sat Jul 13 01:59:34 2024 ] 	Batch(4800/6809) done. Loss: 0.1565  lr:0.000100
[ Sat Jul 13 01:59:51 2024 ] 	Batch(4900/6809) done. Loss: 0.0257  lr:0.000100
[ Sat Jul 13 02:00:09 2024 ] 
Training: Epoch [82/120], Step [4999], Loss: 0.04895992577075958, Training Accuracy: 97.38250000000001
[ Sat Jul 13 02:00:09 2024 ] 	Batch(5000/6809) done. Loss: 0.3183  lr:0.000100
[ Sat Jul 13 02:00:27 2024 ] 	Batch(5100/6809) done. Loss: 0.0259  lr:0.000100
[ Sat Jul 13 02:00:45 2024 ] 	Batch(5200/6809) done. Loss: 0.3120  lr:0.000100
[ Sat Jul 13 02:01:04 2024 ] 	Batch(5300/6809) done. Loss: 0.1146  lr:0.000100
[ Sat Jul 13 02:01:22 2024 ] 	Batch(5400/6809) done. Loss: 0.0669  lr:0.000100
[ Sat Jul 13 02:01:39 2024 ] 
Training: Epoch [82/120], Step [5499], Loss: 0.10885924100875854, Training Accuracy: 97.35454545454544
[ Sat Jul 13 02:01:40 2024 ] 	Batch(5500/6809) done. Loss: 0.0074  lr:0.000100
[ Sat Jul 13 02:01:58 2024 ] 	Batch(5600/6809) done. Loss: 0.0130  lr:0.000100
[ Sat Jul 13 02:02:16 2024 ] 	Batch(5700/6809) done. Loss: 0.1069  lr:0.000100
[ Sat Jul 13 02:02:33 2024 ] 	Batch(5800/6809) done. Loss: 0.0968  lr:0.000100
[ Sat Jul 13 02:02:51 2024 ] 	Batch(5900/6809) done. Loss: 0.2458  lr:0.000100
[ Sat Jul 13 02:03:09 2024 ] 
Training: Epoch [82/120], Step [5999], Loss: 0.12697386741638184, Training Accuracy: 97.37083333333332
[ Sat Jul 13 02:03:09 2024 ] 	Batch(6000/6809) done. Loss: 0.0137  lr:0.000100
[ Sat Jul 13 02:03:28 2024 ] 	Batch(6100/6809) done. Loss: 0.4067  lr:0.000100
[ Sat Jul 13 02:03:46 2024 ] 	Batch(6200/6809) done. Loss: 0.1043  lr:0.000100
[ Sat Jul 13 02:04:04 2024 ] 	Batch(6300/6809) done. Loss: 0.1512  lr:0.000100
[ Sat Jul 13 02:04:22 2024 ] 	Batch(6400/6809) done. Loss: 0.1462  lr:0.000100
[ Sat Jul 13 02:04:39 2024 ] 
Training: Epoch [82/120], Step [6499], Loss: 0.021682705730199814, Training Accuracy: 97.3326923076923
[ Sat Jul 13 02:04:40 2024 ] 	Batch(6500/6809) done. Loss: 0.5578  lr:0.000100
[ Sat Jul 13 02:04:58 2024 ] 	Batch(6600/6809) done. Loss: 0.0054  lr:0.000100
[ Sat Jul 13 02:05:15 2024 ] 	Batch(6700/6809) done. Loss: 0.2141  lr:0.000100
[ Sat Jul 13 02:05:33 2024 ] 	Batch(6800/6809) done. Loss: 0.0045  lr:0.000100
[ Sat Jul 13 02:05:35 2024 ] 	Mean training loss: 0.1018.
[ Sat Jul 13 02:05:35 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 02:05:35 2024 ] Training epoch: 84
[ Sat Jul 13 02:05:36 2024 ] 	Batch(0/6809) done. Loss: 0.0339  lr:0.000100
[ Sat Jul 13 02:05:54 2024 ] 	Batch(100/6809) done. Loss: 0.0154  lr:0.000100
[ Sat Jul 13 02:06:12 2024 ] 	Batch(200/6809) done. Loss: 0.6144  lr:0.000100
[ Sat Jul 13 02:06:31 2024 ] 	Batch(300/6809) done. Loss: 0.1608  lr:0.000100
[ Sat Jul 13 02:06:49 2024 ] 	Batch(400/6809) done. Loss: 0.0592  lr:0.000100
[ Sat Jul 13 02:07:07 2024 ] 
Training: Epoch [83/120], Step [499], Loss: 0.2399267554283142, Training Accuracy: 97.2
[ Sat Jul 13 02:07:07 2024 ] 	Batch(500/6809) done. Loss: 0.0339  lr:0.000100
[ Sat Jul 13 02:07:25 2024 ] 	Batch(600/6809) done. Loss: 0.1238  lr:0.000100
[ Sat Jul 13 02:07:44 2024 ] 	Batch(700/6809) done. Loss: 0.6857  lr:0.000100
[ Sat Jul 13 02:08:02 2024 ] 	Batch(800/6809) done. Loss: 0.0771  lr:0.000100
[ Sat Jul 13 02:08:20 2024 ] 	Batch(900/6809) done. Loss: 0.0191  lr:0.000100
[ Sat Jul 13 02:08:39 2024 ] 
Training: Epoch [83/120], Step [999], Loss: 0.08710004389286041, Training Accuracy: 97.0375
[ Sat Jul 13 02:08:39 2024 ] 	Batch(1000/6809) done. Loss: 0.0029  lr:0.000100
[ Sat Jul 13 02:08:57 2024 ] 	Batch(1100/6809) done. Loss: 0.0242  lr:0.000100
[ Sat Jul 13 02:09:16 2024 ] 	Batch(1200/6809) done. Loss: 0.0063  lr:0.000100
[ Sat Jul 13 02:09:35 2024 ] 	Batch(1300/6809) done. Loss: 0.0585  lr:0.000100
[ Sat Jul 13 02:09:53 2024 ] 	Batch(1400/6809) done. Loss: 0.5220  lr:0.000100
[ Sat Jul 13 02:10:11 2024 ] 
Training: Epoch [83/120], Step [1499], Loss: 0.30892109870910645, Training Accuracy: 97.2
[ Sat Jul 13 02:10:11 2024 ] 	Batch(1500/6809) done. Loss: 0.0540  lr:0.000100
[ Sat Jul 13 02:10:29 2024 ] 	Batch(1600/6809) done. Loss: 0.0349  lr:0.000100
[ Sat Jul 13 02:10:47 2024 ] 	Batch(1700/6809) done. Loss: 0.0044  lr:0.000100
[ Sat Jul 13 02:11:06 2024 ] 	Batch(1800/6809) done. Loss: 0.0996  lr:0.000100
[ Sat Jul 13 02:11:24 2024 ] 	Batch(1900/6809) done. Loss: 0.1678  lr:0.000100
[ Sat Jul 13 02:11:43 2024 ] 
Training: Epoch [83/120], Step [1999], Loss: 0.1217510923743248, Training Accuracy: 97.3
[ Sat Jul 13 02:11:43 2024 ] 	Batch(2000/6809) done. Loss: 0.0605  lr:0.000100
[ Sat Jul 13 02:12:02 2024 ] 	Batch(2100/6809) done. Loss: 0.0165  lr:0.000100
[ Sat Jul 13 02:12:20 2024 ] 	Batch(2200/6809) done. Loss: 0.2395  lr:0.000100
[ Sat Jul 13 02:12:38 2024 ] 	Batch(2300/6809) done. Loss: 0.1109  lr:0.000100
[ Sat Jul 13 02:12:56 2024 ] 	Batch(2400/6809) done. Loss: 0.0842  lr:0.000100
[ Sat Jul 13 02:13:13 2024 ] 
Training: Epoch [83/120], Step [2499], Loss: 0.05860354006290436, Training Accuracy: 97.32499999999999
[ Sat Jul 13 02:13:13 2024 ] 	Batch(2500/6809) done. Loss: 0.0208  lr:0.000100
[ Sat Jul 13 02:13:31 2024 ] 	Batch(2600/6809) done. Loss: 0.0414  lr:0.000100
[ Sat Jul 13 02:13:50 2024 ] 	Batch(2700/6809) done. Loss: 0.0141  lr:0.000100
[ Sat Jul 13 02:14:09 2024 ] 	Batch(2800/6809) done. Loss: 0.0039  lr:0.000100
[ Sat Jul 13 02:14:27 2024 ] 	Batch(2900/6809) done. Loss: 0.0067  lr:0.000100
[ Sat Jul 13 02:14:45 2024 ] 
Training: Epoch [83/120], Step [2999], Loss: 0.0988144725561142, Training Accuracy: 97.28750000000001
[ Sat Jul 13 02:14:45 2024 ] 	Batch(3000/6809) done. Loss: 0.4996  lr:0.000100
[ Sat Jul 13 02:15:03 2024 ] 	Batch(3100/6809) done. Loss: 0.0279  lr:0.000100
[ Sat Jul 13 02:15:21 2024 ] 	Batch(3200/6809) done. Loss: 0.1410  lr:0.000100
[ Sat Jul 13 02:15:39 2024 ] 	Batch(3300/6809) done. Loss: 0.1953  lr:0.000100
[ Sat Jul 13 02:15:57 2024 ] 	Batch(3400/6809) done. Loss: 0.6029  lr:0.000100
[ Sat Jul 13 02:16:15 2024 ] 
Training: Epoch [83/120], Step [3499], Loss: 0.013357644900679588, Training Accuracy: 97.26428571428572
[ Sat Jul 13 02:16:15 2024 ] 	Batch(3500/6809) done. Loss: 0.0078  lr:0.000100
[ Sat Jul 13 02:16:33 2024 ] 	Batch(3600/6809) done. Loss: 0.2298  lr:0.000100
[ Sat Jul 13 02:16:51 2024 ] 	Batch(3700/6809) done. Loss: 0.1117  lr:0.000100
[ Sat Jul 13 02:17:10 2024 ] 	Batch(3800/6809) done. Loss: 0.2643  lr:0.000100
[ Sat Jul 13 02:17:28 2024 ] 	Batch(3900/6809) done. Loss: 0.0032  lr:0.000100
[ Sat Jul 13 02:17:47 2024 ] 
Training: Epoch [83/120], Step [3999], Loss: 0.014056814834475517, Training Accuracy: 97.271875
[ Sat Jul 13 02:17:47 2024 ] 	Batch(4000/6809) done. Loss: 0.0310  lr:0.000100
[ Sat Jul 13 02:18:06 2024 ] 	Batch(4100/6809) done. Loss: 0.0081  lr:0.000100
[ Sat Jul 13 02:18:24 2024 ] 	Batch(4200/6809) done. Loss: 0.1078  lr:0.000100
[ Sat Jul 13 02:18:43 2024 ] 	Batch(4300/6809) done. Loss: 0.3201  lr:0.000100
[ Sat Jul 13 02:19:01 2024 ] 	Batch(4400/6809) done. Loss: 0.0182  lr:0.000100
[ Sat Jul 13 02:19:20 2024 ] 
Training: Epoch [83/120], Step [4499], Loss: 0.10274743288755417, Training Accuracy: 97.30555555555556
[ Sat Jul 13 02:19:20 2024 ] 	Batch(4500/6809) done. Loss: 0.0340  lr:0.000100
[ Sat Jul 13 02:19:38 2024 ] 	Batch(4600/6809) done. Loss: 0.4665  lr:0.000100
[ Sat Jul 13 02:19:56 2024 ] 	Batch(4700/6809) done. Loss: 0.0035  lr:0.000100
[ Sat Jul 13 02:20:14 2024 ] 	Batch(4800/6809) done. Loss: 0.0171  lr:0.000100
[ Sat Jul 13 02:20:32 2024 ] 	Batch(4900/6809) done. Loss: 0.1274  lr:0.000100
[ Sat Jul 13 02:20:49 2024 ] 
Training: Epoch [83/120], Step [4999], Loss: 0.14506717026233673, Training Accuracy: 97.3075
[ Sat Jul 13 02:20:49 2024 ] 	Batch(5000/6809) done. Loss: 0.1826  lr:0.000100
[ Sat Jul 13 02:21:07 2024 ] 	Batch(5100/6809) done. Loss: 0.0519  lr:0.000100
[ Sat Jul 13 02:21:25 2024 ] 	Batch(5200/6809) done. Loss: 0.0315  lr:0.000100
[ Sat Jul 13 02:21:43 2024 ] 	Batch(5300/6809) done. Loss: 0.0213  lr:0.000100
[ Sat Jul 13 02:22:01 2024 ] 	Batch(5400/6809) done. Loss: 0.0386  lr:0.000100
[ Sat Jul 13 02:22:19 2024 ] 
Training: Epoch [83/120], Step [5499], Loss: 0.13421598076820374, Training Accuracy: 97.31136363636364
[ Sat Jul 13 02:22:20 2024 ] 	Batch(5500/6809) done. Loss: 0.0642  lr:0.000100
[ Sat Jul 13 02:22:38 2024 ] 	Batch(5600/6809) done. Loss: 0.0394  lr:0.000100
[ Sat Jul 13 02:22:55 2024 ] 	Batch(5700/6809) done. Loss: 0.4018  lr:0.000100
[ Sat Jul 13 02:23:13 2024 ] 	Batch(5800/6809) done. Loss: 0.0341  lr:0.000100
[ Sat Jul 13 02:23:31 2024 ] 	Batch(5900/6809) done. Loss: 0.0276  lr:0.000100
[ Sat Jul 13 02:23:49 2024 ] 
Training: Epoch [83/120], Step [5999], Loss: 0.0340331494808197, Training Accuracy: 97.31041666666667
[ Sat Jul 13 02:23:49 2024 ] 	Batch(6000/6809) done. Loss: 0.1240  lr:0.000100
[ Sat Jul 13 02:24:08 2024 ] 	Batch(6100/6809) done. Loss: 0.1406  lr:0.000100
[ Sat Jul 13 02:24:26 2024 ] 	Batch(6200/6809) done. Loss: 0.0528  lr:0.000100
[ Sat Jul 13 02:24:45 2024 ] 	Batch(6300/6809) done. Loss: 0.2367  lr:0.000100
[ Sat Jul 13 02:25:04 2024 ] 	Batch(6400/6809) done. Loss: 0.0441  lr:0.000100
[ Sat Jul 13 02:25:21 2024 ] 
Training: Epoch [83/120], Step [6499], Loss: 0.053713779896497726, Training Accuracy: 97.32115384615385
[ Sat Jul 13 02:25:21 2024 ] 	Batch(6500/6809) done. Loss: 0.0045  lr:0.000100
[ Sat Jul 13 02:25:39 2024 ] 	Batch(6600/6809) done. Loss: 0.0230  lr:0.000100
[ Sat Jul 13 02:25:57 2024 ] 	Batch(6700/6809) done. Loss: 0.1208  lr:0.000100
[ Sat Jul 13 02:26:15 2024 ] 	Batch(6800/6809) done. Loss: 0.0477  lr:0.000100
[ Sat Jul 13 02:26:17 2024 ] 	Mean training loss: 0.1005.
[ Sat Jul 13 02:26:17 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 02:26:17 2024 ] Training epoch: 85
[ Sat Jul 13 02:26:17 2024 ] 	Batch(0/6809) done. Loss: 0.1604  lr:0.000100
[ Sat Jul 13 02:26:36 2024 ] 	Batch(100/6809) done. Loss: 0.1286  lr:0.000100
[ Sat Jul 13 02:26:54 2024 ] 	Batch(200/6809) done. Loss: 0.1143  lr:0.000100
[ Sat Jul 13 02:27:13 2024 ] 	Batch(300/6809) done. Loss: 0.0108  lr:0.000100
[ Sat Jul 13 02:27:31 2024 ] 	Batch(400/6809) done. Loss: 0.0303  lr:0.000100
[ Sat Jul 13 02:27:49 2024 ] 
Training: Epoch [84/120], Step [499], Loss: 0.031402379274368286, Training Accuracy: 97.225
[ Sat Jul 13 02:27:49 2024 ] 	Batch(500/6809) done. Loss: 0.0725  lr:0.000100
[ Sat Jul 13 02:28:08 2024 ] 	Batch(600/6809) done. Loss: 0.1989  lr:0.000100
[ Sat Jul 13 02:28:26 2024 ] 	Batch(700/6809) done. Loss: 0.0734  lr:0.000100
[ Sat Jul 13 02:28:45 2024 ] 	Batch(800/6809) done. Loss: 0.1522  lr:0.000100
[ Sat Jul 13 02:29:03 2024 ] 	Batch(900/6809) done. Loss: 0.4720  lr:0.000100
[ Sat Jul 13 02:29:21 2024 ] 
Training: Epoch [84/120], Step [999], Loss: 0.005954182706773281, Training Accuracy: 97.3625
[ Sat Jul 13 02:29:21 2024 ] 	Batch(1000/6809) done. Loss: 0.3465  lr:0.000100
[ Sat Jul 13 02:29:39 2024 ] 	Batch(1100/6809) done. Loss: 0.1899  lr:0.000100
[ Sat Jul 13 02:29:58 2024 ] 	Batch(1200/6809) done. Loss: 0.4925  lr:0.000100
[ Sat Jul 13 02:30:16 2024 ] 	Batch(1300/6809) done. Loss: 0.2211  lr:0.000100
[ Sat Jul 13 02:30:35 2024 ] 	Batch(1400/6809) done. Loss: 0.0812  lr:0.000100
[ Sat Jul 13 02:30:53 2024 ] 
Training: Epoch [84/120], Step [1499], Loss: 0.21024960279464722, Training Accuracy: 97.35833333333333
[ Sat Jul 13 02:30:53 2024 ] 	Batch(1500/6809) done. Loss: 0.0229  lr:0.000100
[ Sat Jul 13 02:31:12 2024 ] 	Batch(1600/6809) done. Loss: 0.2302  lr:0.000100
[ Sat Jul 13 02:31:30 2024 ] 	Batch(1700/6809) done. Loss: 0.0890  lr:0.000100
[ Sat Jul 13 02:31:49 2024 ] 	Batch(1800/6809) done. Loss: 0.1067  lr:0.000100
[ Sat Jul 13 02:32:07 2024 ] 	Batch(1900/6809) done. Loss: 0.1707  lr:0.000100
[ Sat Jul 13 02:32:26 2024 ] 
Training: Epoch [84/120], Step [1999], Loss: 0.13432826101779938, Training Accuracy: 97.39375
[ Sat Jul 13 02:32:26 2024 ] 	Batch(2000/6809) done. Loss: 0.0219  lr:0.000100
[ Sat Jul 13 02:32:44 2024 ] 	Batch(2100/6809) done. Loss: 0.1990  lr:0.000100
[ Sat Jul 13 02:33:03 2024 ] 	Batch(2200/6809) done. Loss: 0.0308  lr:0.000100
[ Sat Jul 13 02:33:21 2024 ] 	Batch(2300/6809) done. Loss: 0.0685  lr:0.000100
[ Sat Jul 13 02:33:39 2024 ] 	Batch(2400/6809) done. Loss: 0.0150  lr:0.000100
[ Sat Jul 13 02:33:57 2024 ] 
Training: Epoch [84/120], Step [2499], Loss: 0.011996730230748653, Training Accuracy: 97.47
[ Sat Jul 13 02:33:57 2024 ] 	Batch(2500/6809) done. Loss: 0.0006  lr:0.000100
[ Sat Jul 13 02:34:16 2024 ] 	Batch(2600/6809) done. Loss: 0.1401  lr:0.000100
[ Sat Jul 13 02:34:35 2024 ] 	Batch(2700/6809) done. Loss: 0.3820  lr:0.000100
[ Sat Jul 13 02:34:53 2024 ] 	Batch(2800/6809) done. Loss: 0.0057  lr:0.000100
[ Sat Jul 13 02:35:12 2024 ] 	Batch(2900/6809) done. Loss: 0.2771  lr:0.000100
[ Sat Jul 13 02:35:30 2024 ] 
Training: Epoch [84/120], Step [2999], Loss: 0.009324196726083755, Training Accuracy: 97.46249999999999
[ Sat Jul 13 02:35:30 2024 ] 	Batch(3000/6809) done. Loss: 0.3032  lr:0.000100
[ Sat Jul 13 02:35:48 2024 ] 	Batch(3100/6809) done. Loss: 0.0096  lr:0.000100
[ Sat Jul 13 02:36:06 2024 ] 	Batch(3200/6809) done. Loss: 0.1001  lr:0.000100
[ Sat Jul 13 02:36:24 2024 ] 	Batch(3300/6809) done. Loss: 0.1112  lr:0.000100
[ Sat Jul 13 02:36:42 2024 ] 	Batch(3400/6809) done. Loss: 0.0077  lr:0.000100
[ Sat Jul 13 02:37:00 2024 ] 
Training: Epoch [84/120], Step [3499], Loss: 0.003088282886892557, Training Accuracy: 97.42857142857143
[ Sat Jul 13 02:37:00 2024 ] 	Batch(3500/6809) done. Loss: 0.0278  lr:0.000100
[ Sat Jul 13 02:37:18 2024 ] 	Batch(3600/6809) done. Loss: 0.4482  lr:0.000100
[ Sat Jul 13 02:37:36 2024 ] 	Batch(3700/6809) done. Loss: 0.0290  lr:0.000100
[ Sat Jul 13 02:37:53 2024 ] 	Batch(3800/6809) done. Loss: 0.0284  lr:0.000100
[ Sat Jul 13 02:38:11 2024 ] 	Batch(3900/6809) done. Loss: 0.0988  lr:0.000100
[ Sat Jul 13 02:38:29 2024 ] 
Training: Epoch [84/120], Step [3999], Loss: 0.20993579924106598, Training Accuracy: 97.38125
[ Sat Jul 13 02:38:29 2024 ] 	Batch(4000/6809) done. Loss: 0.1324  lr:0.000100
[ Sat Jul 13 02:38:48 2024 ] 	Batch(4100/6809) done. Loss: 0.1376  lr:0.000100
[ Sat Jul 13 02:39:06 2024 ] 	Batch(4200/6809) done. Loss: 0.0223  lr:0.000100
[ Sat Jul 13 02:39:25 2024 ] 	Batch(4300/6809) done. Loss: 0.4322  lr:0.000100
[ Sat Jul 13 02:39:44 2024 ] 	Batch(4400/6809) done. Loss: 0.0596  lr:0.000100
[ Sat Jul 13 02:40:01 2024 ] 
Training: Epoch [84/120], Step [4499], Loss: 0.16131781041622162, Training Accuracy: 97.3638888888889
[ Sat Jul 13 02:40:01 2024 ] 	Batch(4500/6809) done. Loss: 0.0938  lr:0.000100
[ Sat Jul 13 02:40:19 2024 ] 	Batch(4600/6809) done. Loss: 0.1675  lr:0.000100
[ Sat Jul 13 02:40:37 2024 ] 	Batch(4700/6809) done. Loss: 0.1510  lr:0.000100
[ Sat Jul 13 02:40:55 2024 ] 	Batch(4800/6809) done. Loss: 0.1328  lr:0.000100
[ Sat Jul 13 02:41:13 2024 ] 	Batch(4900/6809) done. Loss: 0.0141  lr:0.000100
[ Sat Jul 13 02:41:31 2024 ] 
Training: Epoch [84/120], Step [4999], Loss: 0.10240348428487778, Training Accuracy: 97.37
[ Sat Jul 13 02:41:31 2024 ] 	Batch(5000/6809) done. Loss: 0.0111  lr:0.000100
[ Sat Jul 13 02:41:49 2024 ] 	Batch(5100/6809) done. Loss: 0.0380  lr:0.000100
[ Sat Jul 13 02:42:07 2024 ] 	Batch(5200/6809) done. Loss: 0.0541  lr:0.000100
[ Sat Jul 13 02:42:25 2024 ] 	Batch(5300/6809) done. Loss: 0.0301  lr:0.000100
[ Sat Jul 13 02:42:43 2024 ] 	Batch(5400/6809) done. Loss: 0.2091  lr:0.000100
[ Sat Jul 13 02:43:00 2024 ] 
Training: Epoch [84/120], Step [5499], Loss: 0.07256963104009628, Training Accuracy: 97.38863636363637
[ Sat Jul 13 02:43:01 2024 ] 	Batch(5500/6809) done. Loss: 0.0213  lr:0.000100
[ Sat Jul 13 02:43:19 2024 ] 	Batch(5600/6809) done. Loss: 0.1409  lr:0.000100
[ Sat Jul 13 02:43:37 2024 ] 	Batch(5700/6809) done. Loss: 0.0452  lr:0.000100
[ Sat Jul 13 02:43:54 2024 ] 	Batch(5800/6809) done. Loss: 0.0118  lr:0.000100
[ Sat Jul 13 02:44:12 2024 ] 	Batch(5900/6809) done. Loss: 0.1549  lr:0.000100
[ Sat Jul 13 02:44:30 2024 ] 
Training: Epoch [84/120], Step [5999], Loss: 0.03148122876882553, Training Accuracy: 97.40208333333334
[ Sat Jul 13 02:44:30 2024 ] 	Batch(6000/6809) done. Loss: 0.2332  lr:0.000100
[ Sat Jul 13 02:44:48 2024 ] 	Batch(6100/6809) done. Loss: 0.0373  lr:0.000100
[ Sat Jul 13 02:45:06 2024 ] 	Batch(6200/6809) done. Loss: 0.4833  lr:0.000100
[ Sat Jul 13 02:45:25 2024 ] 	Batch(6300/6809) done. Loss: 0.2373  lr:0.000100
[ Sat Jul 13 02:45:43 2024 ] 	Batch(6400/6809) done. Loss: 0.0993  lr:0.000100
[ Sat Jul 13 02:46:01 2024 ] 
Training: Epoch [84/120], Step [6499], Loss: 0.3266483247280121, Training Accuracy: 97.42692307692307
[ Sat Jul 13 02:46:01 2024 ] 	Batch(6500/6809) done. Loss: 0.5085  lr:0.000100
[ Sat Jul 13 02:46:20 2024 ] 	Batch(6600/6809) done. Loss: 0.0195  lr:0.000100
[ Sat Jul 13 02:46:38 2024 ] 	Batch(6700/6809) done. Loss: 0.0016  lr:0.000100
[ Sat Jul 13 02:46:56 2024 ] 	Batch(6800/6809) done. Loss: 0.2055  lr:0.000100
[ Sat Jul 13 02:46:57 2024 ] 	Mean training loss: 0.0987.
[ Sat Jul 13 02:46:57 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 02:46:58 2024 ] Training epoch: 86
[ Sat Jul 13 02:46:58 2024 ] 	Batch(0/6809) done. Loss: 0.0130  lr:0.000100
[ Sat Jul 13 02:47:16 2024 ] 	Batch(100/6809) done. Loss: 0.2306  lr:0.000100
[ Sat Jul 13 02:47:34 2024 ] 	Batch(200/6809) done. Loss: 0.0985  lr:0.000100
[ Sat Jul 13 02:47:52 2024 ] 	Batch(300/6809) done. Loss: 0.0285  lr:0.000100
[ Sat Jul 13 02:48:10 2024 ] 	Batch(400/6809) done. Loss: 0.0556  lr:0.000100
[ Sat Jul 13 02:48:28 2024 ] 
Training: Epoch [85/120], Step [499], Loss: 0.006881104316562414, Training Accuracy: 97.25
[ Sat Jul 13 02:48:28 2024 ] 	Batch(500/6809) done. Loss: 0.0060  lr:0.000100
[ Sat Jul 13 02:48:46 2024 ] 	Batch(600/6809) done. Loss: 0.0205  lr:0.000100
[ Sat Jul 13 02:49:03 2024 ] 	Batch(700/6809) done. Loss: 0.2798  lr:0.000100
[ Sat Jul 13 02:49:21 2024 ] 	Batch(800/6809) done. Loss: 0.1557  lr:0.000100
[ Sat Jul 13 02:49:39 2024 ] 	Batch(900/6809) done. Loss: 0.2204  lr:0.000100
[ Sat Jul 13 02:49:57 2024 ] 
Training: Epoch [85/120], Step [999], Loss: 0.17836299538612366, Training Accuracy: 97.1375
[ Sat Jul 13 02:49:57 2024 ] 	Batch(1000/6809) done. Loss: 0.0270  lr:0.000100
[ Sat Jul 13 02:50:15 2024 ] 	Batch(1100/6809) done. Loss: 0.1012  lr:0.000100
[ Sat Jul 13 02:50:33 2024 ] 	Batch(1200/6809) done. Loss: 0.0321  lr:0.000100
[ Sat Jul 13 02:50:51 2024 ] 	Batch(1300/6809) done. Loss: 0.0195  lr:0.000100
[ Sat Jul 13 02:51:09 2024 ] 	Batch(1400/6809) done. Loss: 0.0774  lr:0.000100
[ Sat Jul 13 02:51:27 2024 ] 
Training: Epoch [85/120], Step [1499], Loss: 0.01237238198518753, Training Accuracy: 97.19166666666666
[ Sat Jul 13 02:51:27 2024 ] 	Batch(1500/6809) done. Loss: 0.7857  lr:0.000100
[ Sat Jul 13 02:51:45 2024 ] 	Batch(1600/6809) done. Loss: 0.0831  lr:0.000100
[ Sat Jul 13 02:52:03 2024 ] 	Batch(1700/6809) done. Loss: 0.1472  lr:0.000100
[ Sat Jul 13 02:52:21 2024 ] 	Batch(1800/6809) done. Loss: 0.0130  lr:0.000100
[ Sat Jul 13 02:52:38 2024 ] 	Batch(1900/6809) done. Loss: 0.0701  lr:0.000100
[ Sat Jul 13 02:52:56 2024 ] 
Training: Epoch [85/120], Step [1999], Loss: 0.04498952999711037, Training Accuracy: 97.25625
[ Sat Jul 13 02:52:56 2024 ] 	Batch(2000/6809) done. Loss: 0.0861  lr:0.000100
[ Sat Jul 13 02:53:15 2024 ] 	Batch(2100/6809) done. Loss: 0.0096  lr:0.000100
[ Sat Jul 13 02:53:33 2024 ] 	Batch(2200/6809) done. Loss: 0.0289  lr:0.000100
[ Sat Jul 13 02:53:51 2024 ] 	Batch(2300/6809) done. Loss: 0.0469  lr:0.000100
[ Sat Jul 13 02:54:09 2024 ] 	Batch(2400/6809) done. Loss: 0.0404  lr:0.000100
[ Sat Jul 13 02:54:27 2024 ] 
Training: Epoch [85/120], Step [2499], Loss: 0.0814007967710495, Training Accuracy: 97.315
[ Sat Jul 13 02:54:28 2024 ] 	Batch(2500/6809) done. Loss: 0.1573  lr:0.000100
[ Sat Jul 13 02:54:46 2024 ] 	Batch(2600/6809) done. Loss: 0.0475  lr:0.000100
[ Sat Jul 13 02:55:05 2024 ] 	Batch(2700/6809) done. Loss: 0.1695  lr:0.000100
[ Sat Jul 13 02:55:23 2024 ] 	Batch(2800/6809) done. Loss: 0.0182  lr:0.000100
[ Sat Jul 13 02:55:41 2024 ] 	Batch(2900/6809) done. Loss: 0.0740  lr:0.000100
[ Sat Jul 13 02:55:59 2024 ] 
Training: Epoch [85/120], Step [2999], Loss: 0.03148314729332924, Training Accuracy: 97.32916666666667
[ Sat Jul 13 02:55:59 2024 ] 	Batch(3000/6809) done. Loss: 0.0346  lr:0.000100
[ Sat Jul 13 02:56:17 2024 ] 	Batch(3100/6809) done. Loss: 0.1452  lr:0.000100
[ Sat Jul 13 02:56:35 2024 ] 	Batch(3200/6809) done. Loss: 0.0380  lr:0.000100
[ Sat Jul 13 02:56:54 2024 ] 	Batch(3300/6809) done. Loss: 0.0013  lr:0.000100
[ Sat Jul 13 02:57:12 2024 ] 	Batch(3400/6809) done. Loss: 0.2295  lr:0.000100
[ Sat Jul 13 02:57:31 2024 ] 
Training: Epoch [85/120], Step [3499], Loss: 0.003585799364373088, Training Accuracy: 97.3607142857143
[ Sat Jul 13 02:57:31 2024 ] 	Batch(3500/6809) done. Loss: 0.6225  lr:0.000100
[ Sat Jul 13 02:57:49 2024 ] 	Batch(3600/6809) done. Loss: 0.0257  lr:0.000100
[ Sat Jul 13 02:58:08 2024 ] 	Batch(3700/6809) done. Loss: 0.0173  lr:0.000100
[ Sat Jul 13 02:58:26 2024 ] 	Batch(3800/6809) done. Loss: 0.0485  lr:0.000100
[ Sat Jul 13 02:58:44 2024 ] 	Batch(3900/6809) done. Loss: 0.1774  lr:0.000100
[ Sat Jul 13 02:59:01 2024 ] 
Training: Epoch [85/120], Step [3999], Loss: 0.16530267894268036, Training Accuracy: 97.31875
[ Sat Jul 13 02:59:02 2024 ] 	Batch(4000/6809) done. Loss: 0.0754  lr:0.000100
[ Sat Jul 13 02:59:19 2024 ] 	Batch(4100/6809) done. Loss: 0.1106  lr:0.000100
[ Sat Jul 13 02:59:37 2024 ] 	Batch(4200/6809) done. Loss: 0.0194  lr:0.000100
[ Sat Jul 13 02:59:55 2024 ] 	Batch(4300/6809) done. Loss: 0.1192  lr:0.000100
[ Sat Jul 13 03:00:13 2024 ] 	Batch(4400/6809) done. Loss: 0.0320  lr:0.000100
[ Sat Jul 13 03:00:31 2024 ] 
Training: Epoch [85/120], Step [4499], Loss: 0.016367826610803604, Training Accuracy: 97.29722222222222
[ Sat Jul 13 03:00:31 2024 ] 	Batch(4500/6809) done. Loss: 0.1230  lr:0.000100
[ Sat Jul 13 03:00:49 2024 ] 	Batch(4600/6809) done. Loss: 0.0299  lr:0.000100
[ Sat Jul 13 03:01:07 2024 ] 	Batch(4700/6809) done. Loss: 0.0064  lr:0.000100
[ Sat Jul 13 03:01:25 2024 ] 	Batch(4800/6809) done. Loss: 0.0159  lr:0.000100
[ Sat Jul 13 03:01:43 2024 ] 	Batch(4900/6809) done. Loss: 0.1142  lr:0.000100
[ Sat Jul 13 03:02:01 2024 ] 
Training: Epoch [85/120], Step [4999], Loss: 0.022411806508898735, Training Accuracy: 97.27749999999999
[ Sat Jul 13 03:02:01 2024 ] 	Batch(5000/6809) done. Loss: 0.1682  lr:0.000100
[ Sat Jul 13 03:02:19 2024 ] 	Batch(5100/6809) done. Loss: 0.0169  lr:0.000100
[ Sat Jul 13 03:02:37 2024 ] 	Batch(5200/6809) done. Loss: 0.0560  lr:0.000100
[ Sat Jul 13 03:02:55 2024 ] 	Batch(5300/6809) done. Loss: 0.3351  lr:0.000100
[ Sat Jul 13 03:03:14 2024 ] 	Batch(5400/6809) done. Loss: 0.0369  lr:0.000100
[ Sat Jul 13 03:03:32 2024 ] 
Training: Epoch [85/120], Step [5499], Loss: 0.01020904816687107, Training Accuracy: 97.28863636363636
[ Sat Jul 13 03:03:32 2024 ] 	Batch(5500/6809) done. Loss: 0.0139  lr:0.000100
[ Sat Jul 13 03:03:51 2024 ] 	Batch(5600/6809) done. Loss: 0.0217  lr:0.000100
[ Sat Jul 13 03:04:10 2024 ] 	Batch(5700/6809) done. Loss: 0.0403  lr:0.000100
[ Sat Jul 13 03:04:29 2024 ] 	Batch(5800/6809) done. Loss: 0.1730  lr:0.000100
[ Sat Jul 13 03:04:47 2024 ] 	Batch(5900/6809) done. Loss: 0.0258  lr:0.000100
[ Sat Jul 13 03:05:05 2024 ] 
Training: Epoch [85/120], Step [5999], Loss: 0.0074920752085745335, Training Accuracy: 97.31458333333333
[ Sat Jul 13 03:05:05 2024 ] 	Batch(6000/6809) done. Loss: 0.0650  lr:0.000100
[ Sat Jul 13 03:05:23 2024 ] 	Batch(6100/6809) done. Loss: 0.0544  lr:0.000100
[ Sat Jul 13 03:05:41 2024 ] 	Batch(6200/6809) done. Loss: 0.1558  lr:0.000100
[ Sat Jul 13 03:05:59 2024 ] 	Batch(6300/6809) done. Loss: 0.0216  lr:0.000100
[ Sat Jul 13 03:06:17 2024 ] 	Batch(6400/6809) done. Loss: 0.0480  lr:0.000100
[ Sat Jul 13 03:06:35 2024 ] 
Training: Epoch [85/120], Step [6499], Loss: 0.16916772723197937, Training Accuracy: 97.3
[ Sat Jul 13 03:06:35 2024 ] 	Batch(6500/6809) done. Loss: 0.0093  lr:0.000100
[ Sat Jul 13 03:06:53 2024 ] 	Batch(6600/6809) done. Loss: 0.0736  lr:0.000100
[ Sat Jul 13 03:07:11 2024 ] 	Batch(6700/6809) done. Loss: 0.1746  lr:0.000100
[ Sat Jul 13 03:07:29 2024 ] 	Batch(6800/6809) done. Loss: 0.0315  lr:0.000100
[ Sat Jul 13 03:07:30 2024 ] 	Mean training loss: 0.1016.
[ Sat Jul 13 03:07:30 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 03:07:31 2024 ] Training epoch: 87
[ Sat Jul 13 03:07:31 2024 ] 	Batch(0/6809) done. Loss: 0.0867  lr:0.000100
[ Sat Jul 13 03:07:49 2024 ] 	Batch(100/6809) done. Loss: 0.0566  lr:0.000100
[ Sat Jul 13 03:08:07 2024 ] 	Batch(200/6809) done. Loss: 0.0821  lr:0.000100
[ Sat Jul 13 03:08:25 2024 ] 	Batch(300/6809) done. Loss: 0.0506  lr:0.000100
[ Sat Jul 13 03:08:43 2024 ] 	Batch(400/6809) done. Loss: 0.5972  lr:0.000100
[ Sat Jul 13 03:09:01 2024 ] 
Training: Epoch [86/120], Step [499], Loss: 0.06065747141838074, Training Accuracy: 97.25
[ Sat Jul 13 03:09:01 2024 ] 	Batch(500/6809) done. Loss: 0.4083  lr:0.000100
[ Sat Jul 13 03:09:19 2024 ] 	Batch(600/6809) done. Loss: 0.0044  lr:0.000100
[ Sat Jul 13 03:09:37 2024 ] 	Batch(700/6809) done. Loss: 0.0201  lr:0.000100
[ Sat Jul 13 03:09:55 2024 ] 	Batch(800/6809) done. Loss: 0.0391  lr:0.000100
[ Sat Jul 13 03:10:13 2024 ] 	Batch(900/6809) done. Loss: 0.4940  lr:0.000100
[ Sat Jul 13 03:10:31 2024 ] 
Training: Epoch [86/120], Step [999], Loss: 0.05882149189710617, Training Accuracy: 97.2
[ Sat Jul 13 03:10:31 2024 ] 	Batch(1000/6809) done. Loss: 0.0503  lr:0.000100
[ Sat Jul 13 03:10:49 2024 ] 	Batch(1100/6809) done. Loss: 0.0058  lr:0.000100
[ Sat Jul 13 03:11:07 2024 ] 	Batch(1200/6809) done. Loss: 0.0661  lr:0.000100
[ Sat Jul 13 03:11:25 2024 ] 	Batch(1300/6809) done. Loss: 0.0243  lr:0.000100
[ Sat Jul 13 03:11:43 2024 ] 	Batch(1400/6809) done. Loss: 0.0258  lr:0.000100
[ Sat Jul 13 03:12:00 2024 ] 
Training: Epoch [86/120], Step [1499], Loss: 0.032494936138391495, Training Accuracy: 97.39166666666667
[ Sat Jul 13 03:12:01 2024 ] 	Batch(1500/6809) done. Loss: 0.1810  lr:0.000100
[ Sat Jul 13 03:12:19 2024 ] 	Batch(1600/6809) done. Loss: 0.2115  lr:0.000100
[ Sat Jul 13 03:12:37 2024 ] 	Batch(1700/6809) done. Loss: 0.0984  lr:0.000100
[ Sat Jul 13 03:12:54 2024 ] 	Batch(1800/6809) done. Loss: 0.0501  lr:0.000100
[ Sat Jul 13 03:13:12 2024 ] 	Batch(1900/6809) done. Loss: 0.0350  lr:0.000100
[ Sat Jul 13 03:13:30 2024 ] 
Training: Epoch [86/120], Step [1999], Loss: 0.010678142309188843, Training Accuracy: 97.3625
[ Sat Jul 13 03:13:30 2024 ] 	Batch(2000/6809) done. Loss: 0.0461  lr:0.000100
[ Sat Jul 13 03:13:48 2024 ] 	Batch(2100/6809) done. Loss: 0.0137  lr:0.000100
[ Sat Jul 13 03:14:06 2024 ] 	Batch(2200/6809) done. Loss: 0.5378  lr:0.000100
[ Sat Jul 13 03:14:24 2024 ] 	Batch(2300/6809) done. Loss: 0.0051  lr:0.000100
[ Sat Jul 13 03:14:42 2024 ] 	Batch(2400/6809) done. Loss: 0.1359  lr:0.000100
[ Sat Jul 13 03:15:00 2024 ] 
Training: Epoch [86/120], Step [2499], Loss: 0.2535184621810913, Training Accuracy: 97.35000000000001
[ Sat Jul 13 03:15:00 2024 ] 	Batch(2500/6809) done. Loss: 0.0694  lr:0.000100
[ Sat Jul 13 03:15:18 2024 ] 	Batch(2600/6809) done. Loss: 0.0902  lr:0.000100
[ Sat Jul 13 03:15:36 2024 ] 	Batch(2700/6809) done. Loss: 0.1751  lr:0.000100
[ Sat Jul 13 03:15:54 2024 ] 	Batch(2800/6809) done. Loss: 0.0883  lr:0.000100
[ Sat Jul 13 03:16:13 2024 ] 	Batch(2900/6809) done. Loss: 0.1557  lr:0.000100
[ Sat Jul 13 03:16:31 2024 ] 
Training: Epoch [86/120], Step [2999], Loss: 0.0010078748455271125, Training Accuracy: 97.35000000000001
[ Sat Jul 13 03:16:31 2024 ] 	Batch(3000/6809) done. Loss: 0.2424  lr:0.000100
[ Sat Jul 13 03:16:49 2024 ] 	Batch(3100/6809) done. Loss: 0.0383  lr:0.000100
[ Sat Jul 13 03:17:07 2024 ] 	Batch(3200/6809) done. Loss: 0.0427  lr:0.000100
[ Sat Jul 13 03:17:25 2024 ] 	Batch(3300/6809) done. Loss: 0.0375  lr:0.000100
[ Sat Jul 13 03:17:42 2024 ] 	Batch(3400/6809) done. Loss: 0.0043  lr:0.000100
[ Sat Jul 13 03:18:00 2024 ] 
Training: Epoch [86/120], Step [3499], Loss: 0.009773754514753819, Training Accuracy: 97.30357142857143
[ Sat Jul 13 03:18:00 2024 ] 	Batch(3500/6809) done. Loss: 0.1944  lr:0.000100
[ Sat Jul 13 03:18:18 2024 ] 	Batch(3600/6809) done. Loss: 0.0161  lr:0.000100
[ Sat Jul 13 03:18:37 2024 ] 	Batch(3700/6809) done. Loss: 0.2285  lr:0.000100
[ Sat Jul 13 03:18:55 2024 ] 	Batch(3800/6809) done. Loss: 0.2199  lr:0.000100
[ Sat Jul 13 03:19:14 2024 ] 	Batch(3900/6809) done. Loss: 0.1836  lr:0.000100
[ Sat Jul 13 03:19:32 2024 ] 
Training: Epoch [86/120], Step [3999], Loss: 0.21774117648601532, Training Accuracy: 97.3375
[ Sat Jul 13 03:19:33 2024 ] 	Batch(4000/6809) done. Loss: 0.0196  lr:0.000100
[ Sat Jul 13 03:19:51 2024 ] 	Batch(4100/6809) done. Loss: 0.2294  lr:0.000100
[ Sat Jul 13 03:20:08 2024 ] 	Batch(4200/6809) done. Loss: 0.0033  lr:0.000100
[ Sat Jul 13 03:20:26 2024 ] 	Batch(4300/6809) done. Loss: 0.0045  lr:0.000100
[ Sat Jul 13 03:20:44 2024 ] 	Batch(4400/6809) done. Loss: 0.1133  lr:0.000100
[ Sat Jul 13 03:21:02 2024 ] 
Training: Epoch [86/120], Step [4499], Loss: 0.019377753138542175, Training Accuracy: 97.39444444444445
[ Sat Jul 13 03:21:02 2024 ] 	Batch(4500/6809) done. Loss: 0.0718  lr:0.000100
[ Sat Jul 13 03:21:20 2024 ] 	Batch(4600/6809) done. Loss: 0.2074  lr:0.000100
[ Sat Jul 13 03:21:38 2024 ] 	Batch(4700/6809) done. Loss: 0.0518  lr:0.000100
[ Sat Jul 13 03:21:57 2024 ] 	Batch(4800/6809) done. Loss: 0.0820  lr:0.000100
[ Sat Jul 13 03:22:15 2024 ] 	Batch(4900/6809) done. Loss: 0.0599  lr:0.000100
[ Sat Jul 13 03:22:32 2024 ] 
Training: Epoch [86/120], Step [4999], Loss: 0.02875867486000061, Training Accuracy: 97.4725
[ Sat Jul 13 03:22:32 2024 ] 	Batch(5000/6809) done. Loss: 0.2639  lr:0.000100
[ Sat Jul 13 03:22:50 2024 ] 	Batch(5100/6809) done. Loss: 0.0539  lr:0.000100
[ Sat Jul 13 03:23:08 2024 ] 	Batch(5200/6809) done. Loss: 0.0336  lr:0.000100
[ Sat Jul 13 03:23:27 2024 ] 	Batch(5300/6809) done. Loss: 0.2622  lr:0.000100
[ Sat Jul 13 03:23:45 2024 ] 	Batch(5400/6809) done. Loss: 0.0242  lr:0.000100
[ Sat Jul 13 03:24:03 2024 ] 
Training: Epoch [86/120], Step [5499], Loss: 0.13364723324775696, Training Accuracy: 97.4659090909091
[ Sat Jul 13 03:24:04 2024 ] 	Batch(5500/6809) done. Loss: 0.0238  lr:0.000100
[ Sat Jul 13 03:24:22 2024 ] 	Batch(5600/6809) done. Loss: 0.0099  lr:0.000100
[ Sat Jul 13 03:24:40 2024 ] 	Batch(5700/6809) done. Loss: 0.0372  lr:0.000100
[ Sat Jul 13 03:24:58 2024 ] 	Batch(5800/6809) done. Loss: 0.0196  lr:0.000100
[ Sat Jul 13 03:25:16 2024 ] 	Batch(5900/6809) done. Loss: 0.2083  lr:0.000100
[ Sat Jul 13 03:25:33 2024 ] 
Training: Epoch [86/120], Step [5999], Loss: 0.06765280663967133, Training Accuracy: 97.49583333333334
[ Sat Jul 13 03:25:34 2024 ] 	Batch(6000/6809) done. Loss: 0.0416  lr:0.000100
[ Sat Jul 13 03:25:52 2024 ] 	Batch(6100/6809) done. Loss: 0.1348  lr:0.000100
[ Sat Jul 13 03:26:11 2024 ] 	Batch(6200/6809) done. Loss: 0.0844  lr:0.000100
[ Sat Jul 13 03:26:29 2024 ] 	Batch(6300/6809) done. Loss: 0.0301  lr:0.000100
[ Sat Jul 13 03:26:47 2024 ] 	Batch(6400/6809) done. Loss: 0.0294  lr:0.000100
[ Sat Jul 13 03:27:04 2024 ] 
Training: Epoch [86/120], Step [6499], Loss: 0.009659267030656338, Training Accuracy: 97.49807692307692
[ Sat Jul 13 03:27:05 2024 ] 	Batch(6500/6809) done. Loss: 0.0509  lr:0.000100
[ Sat Jul 13 03:27:23 2024 ] 	Batch(6600/6809) done. Loss: 0.0609  lr:0.000100
[ Sat Jul 13 03:27:41 2024 ] 	Batch(6700/6809) done. Loss: 0.0858  lr:0.000100
[ Sat Jul 13 03:27:59 2024 ] 	Batch(6800/6809) done. Loss: 0.0967  lr:0.000100
[ Sat Jul 13 03:28:00 2024 ] 	Mean training loss: 0.0989.
[ Sat Jul 13 03:28:00 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 03:28:00 2024 ] Training epoch: 88
[ Sat Jul 13 03:28:01 2024 ] 	Batch(0/6809) done. Loss: 0.2571  lr:0.000100
[ Sat Jul 13 03:28:19 2024 ] 	Batch(100/6809) done. Loss: 0.2202  lr:0.000100
[ Sat Jul 13 03:28:37 2024 ] 	Batch(200/6809) done. Loss: 0.0023  lr:0.000100
[ Sat Jul 13 03:28:56 2024 ] 	Batch(300/6809) done. Loss: 0.0541  lr:0.000100
[ Sat Jul 13 03:29:14 2024 ] 	Batch(400/6809) done. Loss: 0.2353  lr:0.000100
[ Sat Jul 13 03:29:32 2024 ] 
Training: Epoch [87/120], Step [499], Loss: 0.556197464466095, Training Accuracy: 97.475
[ Sat Jul 13 03:29:32 2024 ] 	Batch(500/6809) done. Loss: 0.1654  lr:0.000100
[ Sat Jul 13 03:29:51 2024 ] 	Batch(600/6809) done. Loss: 0.0132  lr:0.000100
[ Sat Jul 13 03:30:09 2024 ] 	Batch(700/6809) done. Loss: 0.0226  lr:0.000100
[ Sat Jul 13 03:30:28 2024 ] 	Batch(800/6809) done. Loss: 0.1627  lr:0.000100
[ Sat Jul 13 03:30:46 2024 ] 	Batch(900/6809) done. Loss: 0.2844  lr:0.000100
[ Sat Jul 13 03:31:04 2024 ] 
Training: Epoch [87/120], Step [999], Loss: 0.005052670836448669, Training Accuracy: 97.5125
[ Sat Jul 13 03:31:04 2024 ] 	Batch(1000/6809) done. Loss: 0.3124  lr:0.000100
[ Sat Jul 13 03:31:23 2024 ] 	Batch(1100/6809) done. Loss: 0.0170  lr:0.000100
[ Sat Jul 13 03:31:41 2024 ] 	Batch(1200/6809) done. Loss: 0.0715  lr:0.000100
[ Sat Jul 13 03:31:59 2024 ] 	Batch(1300/6809) done. Loss: 0.0135  lr:0.000100
[ Sat Jul 13 03:32:18 2024 ] 	Batch(1400/6809) done. Loss: 0.0172  lr:0.000100
[ Sat Jul 13 03:32:36 2024 ] 
Training: Epoch [87/120], Step [1499], Loss: 0.1552211195230484, Training Accuracy: 97.40833333333333
[ Sat Jul 13 03:32:37 2024 ] 	Batch(1500/6809) done. Loss: 0.1144  lr:0.000100
[ Sat Jul 13 03:32:55 2024 ] 	Batch(1600/6809) done. Loss: 0.2816  lr:0.000100
[ Sat Jul 13 03:33:14 2024 ] 	Batch(1700/6809) done. Loss: 0.1133  lr:0.000100
[ Sat Jul 13 03:33:32 2024 ] 	Batch(1800/6809) done. Loss: 0.0654  lr:0.000100
[ Sat Jul 13 03:33:50 2024 ] 	Batch(1900/6809) done. Loss: 0.1195  lr:0.000100
[ Sat Jul 13 03:34:08 2024 ] 
Training: Epoch [87/120], Step [1999], Loss: 0.14603498578071594, Training Accuracy: 97.45
[ Sat Jul 13 03:34:08 2024 ] 	Batch(2000/6809) done. Loss: 0.0050  lr:0.000100
[ Sat Jul 13 03:34:26 2024 ] 	Batch(2100/6809) done. Loss: 0.0237  lr:0.000100
[ Sat Jul 13 03:34:44 2024 ] 	Batch(2200/6809) done. Loss: 0.1652  lr:0.000100
[ Sat Jul 13 03:35:02 2024 ] 	Batch(2300/6809) done. Loss: 0.1027  lr:0.000100
[ Sat Jul 13 03:35:20 2024 ] 	Batch(2400/6809) done. Loss: 0.1001  lr:0.000100
[ Sat Jul 13 03:35:38 2024 ] 
Training: Epoch [87/120], Step [2499], Loss: 0.008782834745943546, Training Accuracy: 97.465
[ Sat Jul 13 03:35:38 2024 ] 	Batch(2500/6809) done. Loss: 0.0495  lr:0.000100
[ Sat Jul 13 03:35:57 2024 ] 	Batch(2600/6809) done. Loss: 0.2133  lr:0.000100
[ Sat Jul 13 03:36:16 2024 ] 	Batch(2700/6809) done. Loss: 0.0145  lr:0.000100
[ Sat Jul 13 03:36:34 2024 ] 	Batch(2800/6809) done. Loss: 0.0256  lr:0.000100
[ Sat Jul 13 03:36:53 2024 ] 	Batch(2900/6809) done. Loss: 0.0620  lr:0.000100
[ Sat Jul 13 03:37:11 2024 ] 
Training: Epoch [87/120], Step [2999], Loss: 0.03783191740512848, Training Accuracy: 97.39583333333334
[ Sat Jul 13 03:37:11 2024 ] 	Batch(3000/6809) done. Loss: 0.0744  lr:0.000100
[ Sat Jul 13 03:37:30 2024 ] 	Batch(3100/6809) done. Loss: 0.2097  lr:0.000100
[ Sat Jul 13 03:37:49 2024 ] 	Batch(3200/6809) done. Loss: 0.0230  lr:0.000100
[ Sat Jul 13 03:38:07 2024 ] 	Batch(3300/6809) done. Loss: 0.0107  lr:0.000100
[ Sat Jul 13 03:38:26 2024 ] 	Batch(3400/6809) done. Loss: 0.0990  lr:0.000100
[ Sat Jul 13 03:38:44 2024 ] 
Training: Epoch [87/120], Step [3499], Loss: 0.2198878675699234, Training Accuracy: 97.35357142857143
[ Sat Jul 13 03:38:44 2024 ] 	Batch(3500/6809) done. Loss: 0.0357  lr:0.000100
[ Sat Jul 13 03:39:03 2024 ] 	Batch(3600/6809) done. Loss: 0.0088  lr:0.000100
[ Sat Jul 13 03:39:21 2024 ] 	Batch(3700/6809) done. Loss: 0.0190  lr:0.000100
[ Sat Jul 13 03:39:40 2024 ] 	Batch(3800/6809) done. Loss: 0.0380  lr:0.000100
[ Sat Jul 13 03:39:58 2024 ] 	Batch(3900/6809) done. Loss: 0.2383  lr:0.000100
[ Sat Jul 13 03:40:17 2024 ] 
Training: Epoch [87/120], Step [3999], Loss: 0.05124685540795326, Training Accuracy: 97.35625
[ Sat Jul 13 03:40:17 2024 ] 	Batch(4000/6809) done. Loss: 0.2397  lr:0.000100
[ Sat Jul 13 03:40:35 2024 ] 	Batch(4100/6809) done. Loss: 0.0329  lr:0.000100
[ Sat Jul 13 03:40:53 2024 ] 	Batch(4200/6809) done. Loss: 0.0639  lr:0.000100
[ Sat Jul 13 03:41:11 2024 ] 	Batch(4300/6809) done. Loss: 0.0128  lr:0.000100
[ Sat Jul 13 03:41:29 2024 ] 	Batch(4400/6809) done. Loss: 0.1089  lr:0.000100
[ Sat Jul 13 03:41:47 2024 ] 
Training: Epoch [87/120], Step [4499], Loss: 0.20861536264419556, Training Accuracy: 97.39999999999999
[ Sat Jul 13 03:41:47 2024 ] 	Batch(4500/6809) done. Loss: 0.0501  lr:0.000100
[ Sat Jul 13 03:42:05 2024 ] 	Batch(4600/6809) done. Loss: 0.0732  lr:0.000100
[ Sat Jul 13 03:42:23 2024 ] 	Batch(4700/6809) done. Loss: 0.0134  lr:0.000100
[ Sat Jul 13 03:42:41 2024 ] 	Batch(4800/6809) done. Loss: 0.0231  lr:0.000100
[ Sat Jul 13 03:42:59 2024 ] 	Batch(4900/6809) done. Loss: 0.0898  lr:0.000100
[ Sat Jul 13 03:43:17 2024 ] 
Training: Epoch [87/120], Step [4999], Loss: 0.005089249927550554, Training Accuracy: 97.3875
[ Sat Jul 13 03:43:17 2024 ] 	Batch(5000/6809) done. Loss: 0.0044  lr:0.000100
[ Sat Jul 13 03:43:35 2024 ] 	Batch(5100/6809) done. Loss: 0.0597  lr:0.000100
[ Sat Jul 13 03:43:53 2024 ] 	Batch(5200/6809) done. Loss: 0.0185  lr:0.000100
[ Sat Jul 13 03:44:11 2024 ] 	Batch(5300/6809) done. Loss: 0.0681  lr:0.000100
[ Sat Jul 13 03:44:29 2024 ] 	Batch(5400/6809) done. Loss: 0.0092  lr:0.000100
[ Sat Jul 13 03:44:47 2024 ] 
Training: Epoch [87/120], Step [5499], Loss: 0.007503498811274767, Training Accuracy: 97.39090909090909
[ Sat Jul 13 03:44:47 2024 ] 	Batch(5500/6809) done. Loss: 0.1214  lr:0.000100
[ Sat Jul 13 03:45:05 2024 ] 	Batch(5600/6809) done. Loss: 0.2065  lr:0.000100
[ Sat Jul 13 03:45:23 2024 ] 	Batch(5700/6809) done. Loss: 0.1215  lr:0.000100
[ Sat Jul 13 03:45:41 2024 ] 	Batch(5800/6809) done. Loss: 0.0249  lr:0.000100
[ Sat Jul 13 03:45:59 2024 ] 	Batch(5900/6809) done. Loss: 0.1573  lr:0.000100
[ Sat Jul 13 03:46:16 2024 ] 
Training: Epoch [87/120], Step [5999], Loss: 0.009661995805799961, Training Accuracy: 97.36875
[ Sat Jul 13 03:46:17 2024 ] 	Batch(6000/6809) done. Loss: 0.0909  lr:0.000100
[ Sat Jul 13 03:46:35 2024 ] 	Batch(6100/6809) done. Loss: 0.0150  lr:0.000100
[ Sat Jul 13 03:46:54 2024 ] 	Batch(6200/6809) done. Loss: 0.1428  lr:0.000100
[ Sat Jul 13 03:47:12 2024 ] 	Batch(6300/6809) done. Loss: 0.1554  lr:0.000100
[ Sat Jul 13 03:47:31 2024 ] 	Batch(6400/6809) done. Loss: 0.0879  lr:0.000100
[ Sat Jul 13 03:47:49 2024 ] 
Training: Epoch [87/120], Step [6499], Loss: 0.01149436179548502, Training Accuracy: 97.38461538461539
[ Sat Jul 13 03:47:49 2024 ] 	Batch(6500/6809) done. Loss: 0.0505  lr:0.000100
[ Sat Jul 13 03:48:07 2024 ] 	Batch(6600/6809) done. Loss: 0.0030  lr:0.000100
[ Sat Jul 13 03:48:25 2024 ] 	Batch(6700/6809) done. Loss: 0.1029  lr:0.000100
[ Sat Jul 13 03:48:43 2024 ] 	Batch(6800/6809) done. Loss: 0.0095  lr:0.000100
[ Sat Jul 13 03:48:45 2024 ] 	Mean training loss: 0.0980.
[ Sat Jul 13 03:48:45 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 03:48:45 2024 ] Training epoch: 89
[ Sat Jul 13 03:48:45 2024 ] 	Batch(0/6809) done. Loss: 0.0723  lr:0.000100
[ Sat Jul 13 03:49:03 2024 ] 	Batch(100/6809) done. Loss: 0.0829  lr:0.000100
[ Sat Jul 13 03:49:22 2024 ] 	Batch(200/6809) done. Loss: 0.3148  lr:0.000100
[ Sat Jul 13 03:49:40 2024 ] 	Batch(300/6809) done. Loss: 0.1555  lr:0.000100
[ Sat Jul 13 03:49:58 2024 ] 	Batch(400/6809) done. Loss: 0.0316  lr:0.000100
[ Sat Jul 13 03:50:15 2024 ] 
Training: Epoch [88/120], Step [499], Loss: 0.01117832399904728, Training Accuracy: 97.375
[ Sat Jul 13 03:50:15 2024 ] 	Batch(500/6809) done. Loss: 0.0926  lr:0.000100
[ Sat Jul 13 03:50:33 2024 ] 	Batch(600/6809) done. Loss: 0.0712  lr:0.000100
[ Sat Jul 13 03:50:51 2024 ] 	Batch(700/6809) done. Loss: 0.1658  lr:0.000100
[ Sat Jul 13 03:51:09 2024 ] 	Batch(800/6809) done. Loss: 0.0249  lr:0.000100
[ Sat Jul 13 03:51:28 2024 ] 	Batch(900/6809) done. Loss: 0.0655  lr:0.000100
[ Sat Jul 13 03:51:46 2024 ] 
Training: Epoch [88/120], Step [999], Loss: 0.06340725719928741, Training Accuracy: 97.52499999999999
[ Sat Jul 13 03:51:46 2024 ] 	Batch(1000/6809) done. Loss: 0.0185  lr:0.000100
[ Sat Jul 13 03:52:05 2024 ] 	Batch(1100/6809) done. Loss: 0.2332  lr:0.000100
[ Sat Jul 13 03:52:24 2024 ] 	Batch(1200/6809) done. Loss: 0.1523  lr:0.000100
[ Sat Jul 13 03:52:42 2024 ] 	Batch(1300/6809) done. Loss: 0.1669  lr:0.000100
[ Sat Jul 13 03:53:00 2024 ] 	Batch(1400/6809) done. Loss: 0.2334  lr:0.000100
[ Sat Jul 13 03:53:18 2024 ] 
Training: Epoch [88/120], Step [1499], Loss: 0.10163979977369308, Training Accuracy: 97.39999999999999
[ Sat Jul 13 03:53:18 2024 ] 	Batch(1500/6809) done. Loss: 0.0275  lr:0.000100
[ Sat Jul 13 03:53:36 2024 ] 	Batch(1600/6809) done. Loss: 0.0703  lr:0.000100
[ Sat Jul 13 03:53:54 2024 ] 	Batch(1700/6809) done. Loss: 0.0136  lr:0.000100
[ Sat Jul 13 03:54:12 2024 ] 	Batch(1800/6809) done. Loss: 0.0016  lr:0.000100
[ Sat Jul 13 03:54:30 2024 ] 	Batch(1900/6809) done. Loss: 0.0494  lr:0.000100
[ Sat Jul 13 03:54:48 2024 ] 
Training: Epoch [88/120], Step [1999], Loss: 0.08296015858650208, Training Accuracy: 97.45625
[ Sat Jul 13 03:54:48 2024 ] 	Batch(2000/6809) done. Loss: 0.0161  lr:0.000100
[ Sat Jul 13 03:55:06 2024 ] 	Batch(2100/6809) done. Loss: 0.3120  lr:0.000100
[ Sat Jul 13 03:55:24 2024 ] 	Batch(2200/6809) done. Loss: 0.0248  lr:0.000100
[ Sat Jul 13 03:55:42 2024 ] 	Batch(2300/6809) done. Loss: 0.0636  lr:0.000100
[ Sat Jul 13 03:56:00 2024 ] 	Batch(2400/6809) done. Loss: 0.0766  lr:0.000100
[ Sat Jul 13 03:56:18 2024 ] 
Training: Epoch [88/120], Step [2499], Loss: 0.0050921193324029446, Training Accuracy: 97.41
[ Sat Jul 13 03:56:18 2024 ] 	Batch(2500/6809) done. Loss: 0.2402  lr:0.000100
[ Sat Jul 13 03:56:36 2024 ] 	Batch(2600/6809) done. Loss: 0.0039  lr:0.000100
[ Sat Jul 13 03:56:53 2024 ] 	Batch(2700/6809) done. Loss: 0.0039  lr:0.000100
[ Sat Jul 13 03:57:11 2024 ] 	Batch(2800/6809) done. Loss: 0.0992  lr:0.000100
[ Sat Jul 13 03:57:29 2024 ] 	Batch(2900/6809) done. Loss: 0.0467  lr:0.000100
[ Sat Jul 13 03:57:47 2024 ] 
Training: Epoch [88/120], Step [2999], Loss: 0.16357296705245972, Training Accuracy: 97.40416666666667
[ Sat Jul 13 03:57:47 2024 ] 	Batch(3000/6809) done. Loss: 0.3814  lr:0.000100
[ Sat Jul 13 03:58:05 2024 ] 	Batch(3100/6809) done. Loss: 0.1479  lr:0.000100
[ Sat Jul 13 03:58:23 2024 ] 	Batch(3200/6809) done. Loss: 0.0308  lr:0.000100
[ Sat Jul 13 03:58:41 2024 ] 	Batch(3300/6809) done. Loss: 0.0268  lr:0.000100
[ Sat Jul 13 03:58:59 2024 ] 	Batch(3400/6809) done. Loss: 0.0283  lr:0.000100
[ Sat Jul 13 03:59:17 2024 ] 
Training: Epoch [88/120], Step [3499], Loss: 0.009756607003509998, Training Accuracy: 97.41428571428571
[ Sat Jul 13 03:59:17 2024 ] 	Batch(3500/6809) done. Loss: 0.0918  lr:0.000100
[ Sat Jul 13 03:59:35 2024 ] 	Batch(3600/6809) done. Loss: 0.0975  lr:0.000100
[ Sat Jul 13 03:59:53 2024 ] 	Batch(3700/6809) done. Loss: 0.0165  lr:0.000100
[ Sat Jul 13 04:00:11 2024 ] 	Batch(3800/6809) done. Loss: 0.2512  lr:0.000100
[ Sat Jul 13 04:00:29 2024 ] 	Batch(3900/6809) done. Loss: 0.0456  lr:0.000100
[ Sat Jul 13 04:00:47 2024 ] 
Training: Epoch [88/120], Step [3999], Loss: 0.08946749567985535, Training Accuracy: 97.403125
[ Sat Jul 13 04:00:47 2024 ] 	Batch(4000/6809) done. Loss: 0.3368  lr:0.000100
[ Sat Jul 13 04:01:05 2024 ] 	Batch(4100/6809) done. Loss: 0.0106  lr:0.000100
[ Sat Jul 13 04:01:23 2024 ] 	Batch(4200/6809) done. Loss: 0.0243  lr:0.000100
[ Sat Jul 13 04:01:41 2024 ] 	Batch(4300/6809) done. Loss: 0.0801  lr:0.000100
[ Sat Jul 13 04:01:59 2024 ] 	Batch(4400/6809) done. Loss: 0.0946  lr:0.000100
[ Sat Jul 13 04:02:16 2024 ] 
Training: Epoch [88/120], Step [4499], Loss: 0.11784093081951141, Training Accuracy: 97.40555555555555
[ Sat Jul 13 04:02:16 2024 ] 	Batch(4500/6809) done. Loss: 0.1243  lr:0.000100
[ Sat Jul 13 04:02:34 2024 ] 	Batch(4600/6809) done. Loss: 0.1487  lr:0.000100
[ Sat Jul 13 04:02:52 2024 ] 	Batch(4700/6809) done. Loss: 0.0066  lr:0.000100
[ Sat Jul 13 04:03:11 2024 ] 	Batch(4800/6809) done. Loss: 0.0062  lr:0.000100
[ Sat Jul 13 04:03:29 2024 ] 	Batch(4900/6809) done. Loss: 0.0205  lr:0.000100
[ Sat Jul 13 04:03:46 2024 ] 
Training: Epoch [88/120], Step [4999], Loss: 0.05272144824266434, Training Accuracy: 97.42
[ Sat Jul 13 04:03:47 2024 ] 	Batch(5000/6809) done. Loss: 0.0120  lr:0.000100
[ Sat Jul 13 04:04:05 2024 ] 	Batch(5100/6809) done. Loss: 0.0099  lr:0.000100
[ Sat Jul 13 04:04:23 2024 ] 	Batch(5200/6809) done. Loss: 0.0453  lr:0.000100
[ Sat Jul 13 04:04:41 2024 ] 	Batch(5300/6809) done. Loss: 0.1068  lr:0.000100
[ Sat Jul 13 04:05:00 2024 ] 	Batch(5400/6809) done. Loss: 0.1883  lr:0.000100
[ Sat Jul 13 04:05:18 2024 ] 
Training: Epoch [88/120], Step [5499], Loss: 0.008557161316275597, Training Accuracy: 97.4659090909091
[ Sat Jul 13 04:05:18 2024 ] 	Batch(5500/6809) done. Loss: 0.0588  lr:0.000100
[ Sat Jul 13 04:05:37 2024 ] 	Batch(5600/6809) done. Loss: 0.1644  lr:0.000100
[ Sat Jul 13 04:05:55 2024 ] 	Batch(5700/6809) done. Loss: 0.0389  lr:0.000100
[ Sat Jul 13 04:06:13 2024 ] 	Batch(5800/6809) done. Loss: 0.1652  lr:0.000100
[ Sat Jul 13 04:06:31 2024 ] 	Batch(5900/6809) done. Loss: 0.0325  lr:0.000100
[ Sat Jul 13 04:06:48 2024 ] 
Training: Epoch [88/120], Step [5999], Loss: 0.07525935769081116, Training Accuracy: 97.48750000000001
[ Sat Jul 13 04:06:49 2024 ] 	Batch(6000/6809) done. Loss: 0.1013  lr:0.000100
[ Sat Jul 13 04:07:06 2024 ] 	Batch(6100/6809) done. Loss: 0.0230  lr:0.000100
[ Sat Jul 13 04:07:24 2024 ] 	Batch(6200/6809) done. Loss: 0.0153  lr:0.000100
[ Sat Jul 13 04:07:42 2024 ] 	Batch(6300/6809) done. Loss: 0.1424  lr:0.000100
[ Sat Jul 13 04:08:00 2024 ] 	Batch(6400/6809) done. Loss: 0.1424  lr:0.000100
[ Sat Jul 13 04:08:18 2024 ] 
Training: Epoch [88/120], Step [6499], Loss: 0.4707634747028351, Training Accuracy: 97.47692307692307
[ Sat Jul 13 04:08:18 2024 ] 	Batch(6500/6809) done. Loss: 0.0372  lr:0.000100
[ Sat Jul 13 04:08:36 2024 ] 	Batch(6600/6809) done. Loss: 0.0871  lr:0.000100
[ Sat Jul 13 04:08:54 2024 ] 	Batch(6700/6809) done. Loss: 0.0196  lr:0.000100
[ Sat Jul 13 04:09:12 2024 ] 	Batch(6800/6809) done. Loss: 0.0101  lr:0.000100
[ Sat Jul 13 04:09:14 2024 ] 	Mean training loss: 0.0980.
[ Sat Jul 13 04:09:14 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 04:09:14 2024 ] Training epoch: 90
[ Sat Jul 13 04:09:14 2024 ] 	Batch(0/6809) done. Loss: 0.0122  lr:0.000100
[ Sat Jul 13 04:09:33 2024 ] 	Batch(100/6809) done. Loss: 0.0034  lr:0.000100
[ Sat Jul 13 04:09:51 2024 ] 	Batch(200/6809) done. Loss: 0.0045  lr:0.000100
[ Sat Jul 13 04:10:09 2024 ] 	Batch(300/6809) done. Loss: 0.0193  lr:0.000100
[ Sat Jul 13 04:10:27 2024 ] 	Batch(400/6809) done. Loss: 0.0199  lr:0.000100
[ Sat Jul 13 04:10:45 2024 ] 
Training: Epoch [89/120], Step [499], Loss: 0.16544251143932343, Training Accuracy: 97.95
[ Sat Jul 13 04:10:45 2024 ] 	Batch(500/6809) done. Loss: 0.0839  lr:0.000100
[ Sat Jul 13 04:11:03 2024 ] 	Batch(600/6809) done. Loss: 0.0200  lr:0.000100
[ Sat Jul 13 04:11:21 2024 ] 	Batch(700/6809) done. Loss: 0.1778  lr:0.000100
[ Sat Jul 13 04:11:39 2024 ] 	Batch(800/6809) done. Loss: 0.0142  lr:0.000100
[ Sat Jul 13 04:11:57 2024 ] 	Batch(900/6809) done. Loss: 0.0103  lr:0.000100
[ Sat Jul 13 04:12:16 2024 ] 
Training: Epoch [89/120], Step [999], Loss: 0.039918091148138046, Training Accuracy: 97.65
[ Sat Jul 13 04:12:16 2024 ] 	Batch(1000/6809) done. Loss: 0.2222  lr:0.000100
[ Sat Jul 13 04:12:34 2024 ] 	Batch(1100/6809) done. Loss: 0.0799  lr:0.000100
[ Sat Jul 13 04:12:53 2024 ] 	Batch(1200/6809) done. Loss: 0.0026  lr:0.000100
[ Sat Jul 13 04:13:12 2024 ] 	Batch(1300/6809) done. Loss: 0.0410  lr:0.000100
[ Sat Jul 13 04:13:30 2024 ] 	Batch(1400/6809) done. Loss: 0.0352  lr:0.000100
[ Sat Jul 13 04:13:47 2024 ] 
Training: Epoch [89/120], Step [1499], Loss: 0.020114783197641373, Training Accuracy: 97.45833333333334
[ Sat Jul 13 04:13:47 2024 ] 	Batch(1500/6809) done. Loss: 0.0792  lr:0.000100
[ Sat Jul 13 04:14:06 2024 ] 	Batch(1600/6809) done. Loss: 0.0121  lr:0.000100
[ Sat Jul 13 04:14:23 2024 ] 	Batch(1700/6809) done. Loss: 0.0314  lr:0.000100
[ Sat Jul 13 04:14:41 2024 ] 	Batch(1800/6809) done. Loss: 0.0515  lr:0.000100
[ Sat Jul 13 04:14:59 2024 ] 	Batch(1900/6809) done. Loss: 0.0190  lr:0.000100
[ Sat Jul 13 04:15:17 2024 ] 
Training: Epoch [89/120], Step [1999], Loss: 0.06372608244419098, Training Accuracy: 97.53125
[ Sat Jul 13 04:15:17 2024 ] 	Batch(2000/6809) done. Loss: 0.0353  lr:0.000100
[ Sat Jul 13 04:15:35 2024 ] 	Batch(2100/6809) done. Loss: 0.2746  lr:0.000100
[ Sat Jul 13 04:15:53 2024 ] 	Batch(2200/6809) done. Loss: 0.0116  lr:0.000100
[ Sat Jul 13 04:16:12 2024 ] 	Batch(2300/6809) done. Loss: 0.3417  lr:0.000100
[ Sat Jul 13 04:16:30 2024 ] 	Batch(2400/6809) done. Loss: 0.0316  lr:0.000100
[ Sat Jul 13 04:16:48 2024 ] 
Training: Epoch [89/120], Step [2499], Loss: 0.256888747215271, Training Accuracy: 97.53500000000001
[ Sat Jul 13 04:16:48 2024 ] 	Batch(2500/6809) done. Loss: 0.0049  lr:0.000100
[ Sat Jul 13 04:17:06 2024 ] 	Batch(2600/6809) done. Loss: 0.0630  lr:0.000100
[ Sat Jul 13 04:17:24 2024 ] 	Batch(2700/6809) done. Loss: 0.0916  lr:0.000100
[ Sat Jul 13 04:17:42 2024 ] 	Batch(2800/6809) done. Loss: 0.0130  lr:0.000100
[ Sat Jul 13 04:18:00 2024 ] 	Batch(2900/6809) done. Loss: 0.0978  lr:0.000100
[ Sat Jul 13 04:18:18 2024 ] 
Training: Epoch [89/120], Step [2999], Loss: 0.08444055169820786, Training Accuracy: 97.575
[ Sat Jul 13 04:18:18 2024 ] 	Batch(3000/6809) done. Loss: 0.0679  lr:0.000100
[ Sat Jul 13 04:18:36 2024 ] 	Batch(3100/6809) done. Loss: 0.0308  lr:0.000100
[ Sat Jul 13 04:18:54 2024 ] 	Batch(3200/6809) done. Loss: 0.0123  lr:0.000100
[ Sat Jul 13 04:19:12 2024 ] 	Batch(3300/6809) done. Loss: 0.2020  lr:0.000100
[ Sat Jul 13 04:19:30 2024 ] 	Batch(3400/6809) done. Loss: 0.0076  lr:0.000100
[ Sat Jul 13 04:19:48 2024 ] 
Training: Epoch [89/120], Step [3499], Loss: 0.17841234803199768, Training Accuracy: 97.54642857142856
[ Sat Jul 13 04:19:48 2024 ] 	Batch(3500/6809) done. Loss: 0.1018  lr:0.000100
[ Sat Jul 13 04:20:06 2024 ] 	Batch(3600/6809) done. Loss: 0.0065  lr:0.000100
[ Sat Jul 13 04:20:24 2024 ] 	Batch(3700/6809) done. Loss: 0.0698  lr:0.000100
[ Sat Jul 13 04:20:42 2024 ] 	Batch(3800/6809) done. Loss: 0.1553  lr:0.000100
[ Sat Jul 13 04:21:00 2024 ] 	Batch(3900/6809) done. Loss: 0.0010  lr:0.000100
[ Sat Jul 13 04:21:18 2024 ] 
Training: Epoch [89/120], Step [3999], Loss: 0.18083466589450836, Training Accuracy: 97.471875
[ Sat Jul 13 04:21:18 2024 ] 	Batch(4000/6809) done. Loss: 0.1462  lr:0.000100
[ Sat Jul 13 04:21:36 2024 ] 	Batch(4100/6809) done. Loss: 0.0172  lr:0.000100
[ Sat Jul 13 04:21:54 2024 ] 	Batch(4200/6809) done. Loss: 0.0107  lr:0.000100
[ Sat Jul 13 04:22:12 2024 ] 	Batch(4300/6809) done. Loss: 0.0808  lr:0.000100
[ Sat Jul 13 04:22:30 2024 ] 	Batch(4400/6809) done. Loss: 0.0838  lr:0.000100
[ Sat Jul 13 04:22:48 2024 ] 
Training: Epoch [89/120], Step [4499], Loss: 0.002774841384962201, Training Accuracy: 97.51111111111112
[ Sat Jul 13 04:22:48 2024 ] 	Batch(4500/6809) done. Loss: 0.0944  lr:0.000100
[ Sat Jul 13 04:23:07 2024 ] 	Batch(4600/6809) done. Loss: 0.0286  lr:0.000100
[ Sat Jul 13 04:23:25 2024 ] 	Batch(4700/6809) done. Loss: 0.0673  lr:0.000100
[ Sat Jul 13 04:23:43 2024 ] 	Batch(4800/6809) done. Loss: 0.0362  lr:0.000100
[ Sat Jul 13 04:24:01 2024 ] 	Batch(4900/6809) done. Loss: 0.0224  lr:0.000100
[ Sat Jul 13 04:24:20 2024 ] 
Training: Epoch [89/120], Step [4999], Loss: 0.1520872712135315, Training Accuracy: 97.53500000000001
[ Sat Jul 13 04:24:20 2024 ] 	Batch(5000/6809) done. Loss: 0.0037  lr:0.000100
[ Sat Jul 13 04:24:38 2024 ] 	Batch(5100/6809) done. Loss: 0.0163  lr:0.000100
[ Sat Jul 13 04:24:57 2024 ] 	Batch(5200/6809) done. Loss: 0.5242  lr:0.000100
[ Sat Jul 13 04:25:15 2024 ] 	Batch(5300/6809) done. Loss: 0.0038  lr:0.000100
[ Sat Jul 13 04:25:32 2024 ] 	Batch(5400/6809) done. Loss: 0.0186  lr:0.000100
[ Sat Jul 13 04:25:50 2024 ] 
Training: Epoch [89/120], Step [5499], Loss: 0.008361139334738255, Training Accuracy: 97.55909090909091
[ Sat Jul 13 04:25:51 2024 ] 	Batch(5500/6809) done. Loss: 0.0109  lr:0.000100
[ Sat Jul 13 04:26:09 2024 ] 	Batch(5600/6809) done. Loss: 0.3341  lr:0.000100
[ Sat Jul 13 04:26:27 2024 ] 	Batch(5700/6809) done. Loss: 0.0383  lr:0.000100
[ Sat Jul 13 04:26:45 2024 ] 	Batch(5800/6809) done. Loss: 0.1369  lr:0.000100
[ Sat Jul 13 04:27:03 2024 ] 	Batch(5900/6809) done. Loss: 0.0817  lr:0.000100
[ Sat Jul 13 04:27:20 2024 ] 
Training: Epoch [89/120], Step [5999], Loss: 0.29708969593048096, Training Accuracy: 97.58125
[ Sat Jul 13 04:27:21 2024 ] 	Batch(6000/6809) done. Loss: 0.0535  lr:0.000100
[ Sat Jul 13 04:27:39 2024 ] 	Batch(6100/6809) done. Loss: 0.1898  lr:0.000100
[ Sat Jul 13 04:27:56 2024 ] 	Batch(6200/6809) done. Loss: 0.1023  lr:0.000100
[ Sat Jul 13 04:28:14 2024 ] 	Batch(6300/6809) done. Loss: 0.0040  lr:0.000100
[ Sat Jul 13 04:28:32 2024 ] 	Batch(6400/6809) done. Loss: 0.0352  lr:0.000100
[ Sat Jul 13 04:28:51 2024 ] 
Training: Epoch [89/120], Step [6499], Loss: 0.01667078025639057, Training Accuracy: 97.56346153846154
[ Sat Jul 13 04:28:51 2024 ] 	Batch(6500/6809) done. Loss: 0.0607  lr:0.000100
[ Sat Jul 13 04:29:09 2024 ] 	Batch(6600/6809) done. Loss: 0.0604  lr:0.000100
[ Sat Jul 13 04:29:27 2024 ] 	Batch(6700/6809) done. Loss: 0.0192  lr:0.000100
[ Sat Jul 13 04:29:45 2024 ] 	Batch(6800/6809) done. Loss: 0.0024  lr:0.000100
[ Sat Jul 13 04:29:46 2024 ] 	Mean training loss: 0.0950.
[ Sat Jul 13 04:29:46 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 04:29:46 2024 ] Eval epoch: 90
[ Sat Jul 13 04:35:21 2024 ] 	Mean val loss of 7435 batches: 1.0897110741703568.
[ Sat Jul 13 04:35:21 2024 ] 
Validation: Epoch [89/120], Samples [47823.0/59477], Loss: 0.6404718160629272, Validation Accuracy: 80.40587117709367
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 1 : 373 / 500 = 74 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 2 : 439 / 499 = 87 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 3 : 399 / 500 = 79 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 4 : 413 / 502 = 82 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 5 : 467 / 502 = 93 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 6 : 414 / 502 = 82 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 7 : 466 / 497 = 93 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 8 : 483 / 498 = 96 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 9 : 389 / 500 = 77 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 10 : 203 / 500 = 40 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 11 : 199 / 498 = 39 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 12 : 424 / 499 = 84 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 13 : 478 / 502 = 95 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 14 : 480 / 504 = 95 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 15 : 437 / 502 = 87 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 16 : 369 / 502 = 73 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 17 : 449 / 504 = 89 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 18 : 434 / 504 = 86 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 19 : 466 / 502 = 92 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 20 : 458 / 502 = 91 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 21 : 472 / 503 = 93 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 22 : 426 / 504 = 84 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 23 : 449 / 503 = 89 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 24 : 413 / 504 = 81 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 25 : 489 / 504 = 97 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 26 : 472 / 504 = 93 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 27 : 418 / 501 = 83 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 28 : 349 / 502 = 69 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 29 : 309 / 502 = 61 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 30 : 322 / 501 = 64 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 31 : 413 / 504 = 81 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 32 : 421 / 503 = 83 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 33 : 417 / 503 = 82 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 34 : 480 / 504 = 95 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 35 : 464 / 503 = 92 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 36 : 418 / 502 = 83 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 37 : 437 / 504 = 86 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 38 : 432 / 504 = 85 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 39 : 451 / 498 = 90 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 40 : 384 / 504 = 76 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 41 : 473 / 503 = 94 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 42 : 460 / 504 = 91 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 43 : 336 / 503 = 66 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 44 : 443 / 504 = 87 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 45 : 419 / 504 = 83 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 46 : 409 / 504 = 81 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 47 : 431 / 503 = 85 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 48 : 429 / 503 = 85 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 49 : 375 / 499 = 75 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 50 : 428 / 502 = 85 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 51 : 475 / 503 = 94 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 52 : 441 / 504 = 87 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 53 : 427 / 497 = 85 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 54 : 451 / 480 = 93 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 55 : 385 / 504 = 76 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 56 : 410 / 503 = 81 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 57 : 484 / 504 = 96 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 58 : 482 / 499 = 96 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 59 : 492 / 503 = 97 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 60 : 420 / 479 = 87 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 61 : 406 / 484 = 83 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 62 : 400 / 487 = 82 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 63 : 453 / 489 = 92 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 64 : 374 / 488 = 76 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 65 : 458 / 490 = 93 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 66 : 320 / 488 = 65 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 67 : 375 / 490 = 76 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 68 : 296 / 490 = 60 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 69 : 375 / 490 = 76 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 70 : 206 / 490 = 42 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 71 : 210 / 490 = 42 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 72 : 185 / 488 = 37 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 73 : 262 / 486 = 53 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 74 : 280 / 481 = 58 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 75 : 277 / 488 = 56 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 76 : 333 / 489 = 68 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 77 : 349 / 488 = 71 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 78 : 376 / 488 = 77 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 79 : 449 / 490 = 91 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 80 : 414 / 489 = 84 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 81 : 311 / 491 = 63 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 82 : 328 / 491 = 66 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 83 : 269 / 489 = 55 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 84 : 385 / 489 = 78 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 85 : 375 / 489 = 76 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 86 : 443 / 491 = 90 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 87 : 441 / 492 = 89 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 88 : 384 / 491 = 78 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 89 : 405 / 492 = 82 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 90 : 264 / 490 = 53 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 91 : 410 / 482 = 85 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 92 : 365 / 490 = 74 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 93 : 361 / 487 = 74 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 94 : 414 / 489 = 84 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 95 : 406 / 490 = 82 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 96 : 467 / 491 = 95 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 97 : 463 / 490 = 94 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 98 : 449 / 491 = 91 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 99 : 449 / 491 = 91 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 100 : 445 / 491 = 90 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 101 : 433 / 491 = 88 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 102 : 282 / 492 = 57 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 103 : 399 / 492 = 81 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 104 : 315 / 491 = 64 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 105 : 277 / 491 = 56 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 106 : 261 / 492 = 53 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 107 : 413 / 491 = 84 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 108 : 393 / 492 = 79 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 109 : 330 / 490 = 67 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 110 : 418 / 491 = 85 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 111 : 453 / 492 = 92 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 112 : 453 / 492 = 92 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 113 : 435 / 491 = 88 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 114 : 403 / 491 = 82 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 115 : 431 / 492 = 87 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 116 : 406 / 491 = 82 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 117 : 428 / 492 = 86 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 118 : 444 / 490 = 90 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 119 : 447 / 492 = 90 %
[ Sat Jul 13 04:35:21 2024 ] Accuracy of 120 : 424 / 500 = 84 %
[ Sat Jul 13 04:35:21 2024 ] Training epoch: 91
[ Sat Jul 13 04:35:22 2024 ] 	Batch(0/6809) done. Loss: 0.0479  lr:0.000001
[ Sat Jul 13 04:35:40 2024 ] 	Batch(100/6809) done. Loss: 0.2004  lr:0.000001
[ Sat Jul 13 04:35:57 2024 ] 	Batch(200/6809) done. Loss: 0.1789  lr:0.000001
[ Sat Jul 13 04:36:15 2024 ] 	Batch(300/6809) done. Loss: 0.0577  lr:0.000001
[ Sat Jul 13 04:36:33 2024 ] 	Batch(400/6809) done. Loss: 0.3379  lr:0.000001
[ Sat Jul 13 04:36:51 2024 ] 
Training: Epoch [90/120], Step [499], Loss: 0.6872089505195618, Training Accuracy: 97.6
[ Sat Jul 13 04:36:51 2024 ] 	Batch(500/6809) done. Loss: 0.0427  lr:0.000001
[ Sat Jul 13 04:37:10 2024 ] 	Batch(600/6809) done. Loss: 0.1577  lr:0.000001
[ Sat Jul 13 04:37:28 2024 ] 	Batch(700/6809) done. Loss: 0.0272  lr:0.000001
[ Sat Jul 13 04:37:47 2024 ] 	Batch(800/6809) done. Loss: 0.0050  lr:0.000001
[ Sat Jul 13 04:38:05 2024 ] 	Batch(900/6809) done. Loss: 0.0113  lr:0.000001
[ Sat Jul 13 04:38:23 2024 ] 
Training: Epoch [90/120], Step [999], Loss: 0.27395862340927124, Training Accuracy: 97.6
[ Sat Jul 13 04:38:23 2024 ] 	Batch(1000/6809) done. Loss: 0.0108  lr:0.000001
[ Sat Jul 13 04:38:41 2024 ] 	Batch(1100/6809) done. Loss: 0.1240  lr:0.000001
[ Sat Jul 13 04:38:59 2024 ] 	Batch(1200/6809) done. Loss: 0.1187  lr:0.000001
[ Sat Jul 13 04:39:17 2024 ] 	Batch(1300/6809) done. Loss: 0.0255  lr:0.000001
[ Sat Jul 13 04:39:35 2024 ] 	Batch(1400/6809) done. Loss: 0.0065  lr:0.000001
[ Sat Jul 13 04:39:52 2024 ] 
Training: Epoch [90/120], Step [1499], Loss: 0.19668631255626678, Training Accuracy: 97.575
[ Sat Jul 13 04:39:52 2024 ] 	Batch(1500/6809) done. Loss: 0.0746  lr:0.000001
[ Sat Jul 13 04:40:10 2024 ] 	Batch(1600/6809) done. Loss: 0.5054  lr:0.000001
[ Sat Jul 13 04:40:28 2024 ] 	Batch(1700/6809) done. Loss: 0.0086  lr:0.000001
[ Sat Jul 13 04:40:46 2024 ] 	Batch(1800/6809) done. Loss: 0.0397  lr:0.000001
[ Sat Jul 13 04:41:04 2024 ] 	Batch(1900/6809) done. Loss: 0.0050  lr:0.000001
[ Sat Jul 13 04:41:22 2024 ] 
Training: Epoch [90/120], Step [1999], Loss: 0.07421036809682846, Training Accuracy: 97.46875
[ Sat Jul 13 04:41:22 2024 ] 	Batch(2000/6809) done. Loss: 0.0214  lr:0.000001
[ Sat Jul 13 04:41:40 2024 ] 	Batch(2100/6809) done. Loss: 0.0173  lr:0.000001
[ Sat Jul 13 04:41:58 2024 ] 	Batch(2200/6809) done. Loss: 0.0099  lr:0.000001
[ Sat Jul 13 04:42:16 2024 ] 	Batch(2300/6809) done. Loss: 0.0126  lr:0.000001
[ Sat Jul 13 04:42:34 2024 ] 	Batch(2400/6809) done. Loss: 0.0074  lr:0.000001
[ Sat Jul 13 04:42:51 2024 ] 
Training: Epoch [90/120], Step [2499], Loss: 0.15162038803100586, Training Accuracy: 97.50999999999999
[ Sat Jul 13 04:42:52 2024 ] 	Batch(2500/6809) done. Loss: 0.0329  lr:0.000001
[ Sat Jul 13 04:43:09 2024 ] 	Batch(2600/6809) done. Loss: 0.0267  lr:0.000001
[ Sat Jul 13 04:43:27 2024 ] 	Batch(2700/6809) done. Loss: 0.0199  lr:0.000001
[ Sat Jul 13 04:43:45 2024 ] 	Batch(2800/6809) done. Loss: 0.0265  lr:0.000001
[ Sat Jul 13 04:44:03 2024 ] 	Batch(2900/6809) done. Loss: 0.0154  lr:0.000001
[ Sat Jul 13 04:44:21 2024 ] 
Training: Epoch [90/120], Step [2999], Loss: 0.003213528310880065, Training Accuracy: 97.54583333333333
[ Sat Jul 13 04:44:21 2024 ] 	Batch(3000/6809) done. Loss: 0.1033  lr:0.000001
[ Sat Jul 13 04:44:39 2024 ] 	Batch(3100/6809) done. Loss: 0.0084  lr:0.000001
[ Sat Jul 13 04:44:57 2024 ] 	Batch(3200/6809) done. Loss: 0.0134  lr:0.000001
[ Sat Jul 13 04:45:15 2024 ] 	Batch(3300/6809) done. Loss: 0.0306  lr:0.000001
[ Sat Jul 13 04:45:33 2024 ] 	Batch(3400/6809) done. Loss: 0.0378  lr:0.000001
[ Sat Jul 13 04:45:52 2024 ] 
Training: Epoch [90/120], Step [3499], Loss: 0.017128877341747284, Training Accuracy: 97.53571428571428
[ Sat Jul 13 04:45:52 2024 ] 	Batch(3500/6809) done. Loss: 0.0224  lr:0.000001
[ Sat Jul 13 04:46:11 2024 ] 	Batch(3600/6809) done. Loss: 0.1676  lr:0.000001
[ Sat Jul 13 04:46:29 2024 ] 	Batch(3700/6809) done. Loss: 0.0176  lr:0.000001
[ Sat Jul 13 04:46:48 2024 ] 	Batch(3800/6809) done. Loss: 0.2850  lr:0.000001
[ Sat Jul 13 04:47:06 2024 ] 	Batch(3900/6809) done. Loss: 0.0268  lr:0.000001
[ Sat Jul 13 04:47:24 2024 ] 
Training: Epoch [90/120], Step [3999], Loss: 0.11587108671665192, Training Accuracy: 97.5125
[ Sat Jul 13 04:47:25 2024 ] 	Batch(4000/6809) done. Loss: 0.0438  lr:0.000001
[ Sat Jul 13 04:47:43 2024 ] 	Batch(4100/6809) done. Loss: 0.0419  lr:0.000001
[ Sat Jul 13 04:48:00 2024 ] 	Batch(4200/6809) done. Loss: 0.0821  lr:0.000001
[ Sat Jul 13 04:48:18 2024 ] 	Batch(4300/6809) done. Loss: 0.0203  lr:0.000001
[ Sat Jul 13 04:48:36 2024 ] 	Batch(4400/6809) done. Loss: 0.0171  lr:0.000001
[ Sat Jul 13 04:48:54 2024 ] 
Training: Epoch [90/120], Step [4499], Loss: 0.29800525307655334, Training Accuracy: 97.53611111111111
[ Sat Jul 13 04:48:54 2024 ] 	Batch(4500/6809) done. Loss: 0.0559  lr:0.000001
[ Sat Jul 13 04:49:12 2024 ] 	Batch(4600/6809) done. Loss: 0.3185  lr:0.000001
[ Sat Jul 13 04:49:30 2024 ] 	Batch(4700/6809) done. Loss: 0.2176  lr:0.000001
[ Sat Jul 13 04:49:48 2024 ] 	Batch(4800/6809) done. Loss: 0.0114  lr:0.000001
[ Sat Jul 13 04:50:06 2024 ] 	Batch(4900/6809) done. Loss: 0.3254  lr:0.000001
[ Sat Jul 13 04:50:24 2024 ] 
Training: Epoch [90/120], Step [4999], Loss: 0.03765218332409859, Training Accuracy: 97.5325
[ Sat Jul 13 04:50:24 2024 ] 	Batch(5000/6809) done. Loss: 0.0142  lr:0.000001
[ Sat Jul 13 04:50:42 2024 ] 	Batch(5100/6809) done. Loss: 0.0775  lr:0.000001
[ Sat Jul 13 04:51:00 2024 ] 	Batch(5200/6809) done. Loss: 0.0211  lr:0.000001
[ Sat Jul 13 04:51:18 2024 ] 	Batch(5300/6809) done. Loss: 0.0126  lr:0.000001
[ Sat Jul 13 04:51:36 2024 ] 	Batch(5400/6809) done. Loss: 0.0368  lr:0.000001
[ Sat Jul 13 04:51:53 2024 ] 
Training: Epoch [90/120], Step [5499], Loss: 0.5628739595413208, Training Accuracy: 97.54090909090908
[ Sat Jul 13 04:51:53 2024 ] 	Batch(5500/6809) done. Loss: 0.0018  lr:0.000001
[ Sat Jul 13 04:52:12 2024 ] 	Batch(5600/6809) done. Loss: 0.0156  lr:0.000001
[ Sat Jul 13 04:52:30 2024 ] 	Batch(5700/6809) done. Loss: 0.1251  lr:0.000001
[ Sat Jul 13 04:52:49 2024 ] 	Batch(5800/6809) done. Loss: 0.1506  lr:0.000001
[ Sat Jul 13 04:53:07 2024 ] 	Batch(5900/6809) done. Loss: 0.0609  lr:0.000001
[ Sat Jul 13 04:53:26 2024 ] 
Training: Epoch [90/120], Step [5999], Loss: 0.25484633445739746, Training Accuracy: 97.53541666666666
[ Sat Jul 13 04:53:26 2024 ] 	Batch(6000/6809) done. Loss: 0.0606  lr:0.000001
[ Sat Jul 13 04:53:44 2024 ] 	Batch(6100/6809) done. Loss: 0.0052  lr:0.000001
[ Sat Jul 13 04:54:02 2024 ] 	Batch(6200/6809) done. Loss: 0.1369  lr:0.000001
[ Sat Jul 13 04:54:20 2024 ] 	Batch(6300/6809) done. Loss: 0.0124  lr:0.000001
[ Sat Jul 13 04:54:38 2024 ] 	Batch(6400/6809) done. Loss: 0.0654  lr:0.000001
[ Sat Jul 13 04:54:56 2024 ] 
Training: Epoch [90/120], Step [6499], Loss: 0.1763274371623993, Training Accuracy: 97.52499999999999
[ Sat Jul 13 04:54:56 2024 ] 	Batch(6500/6809) done. Loss: 0.0124  lr:0.000001
[ Sat Jul 13 04:55:14 2024 ] 	Batch(6600/6809) done. Loss: 0.0324  lr:0.000001
[ Sat Jul 13 04:55:32 2024 ] 	Batch(6700/6809) done. Loss: 0.1620  lr:0.000001
[ Sat Jul 13 04:55:50 2024 ] 	Batch(6800/6809) done. Loss: 0.1576  lr:0.000001
[ Sat Jul 13 04:55:52 2024 ] 	Mean training loss: 0.0970.
[ Sat Jul 13 04:55:52 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 04:55:52 2024 ] Training epoch: 92
[ Sat Jul 13 04:55:52 2024 ] 	Batch(0/6809) done. Loss: 0.0330  lr:0.000001
[ Sat Jul 13 04:56:11 2024 ] 	Batch(100/6809) done. Loss: 0.0205  lr:0.000001
[ Sat Jul 13 04:56:29 2024 ] 	Batch(200/6809) done. Loss: 0.0225  lr:0.000001
[ Sat Jul 13 04:56:47 2024 ] 	Batch(300/6809) done. Loss: 0.0073  lr:0.000001
[ Sat Jul 13 04:57:05 2024 ] 	Batch(400/6809) done. Loss: 0.1724  lr:0.000001
[ Sat Jul 13 04:57:22 2024 ] 
Training: Epoch [91/120], Step [499], Loss: 0.0055839912965893745, Training Accuracy: 97.35000000000001
[ Sat Jul 13 04:57:23 2024 ] 	Batch(500/6809) done. Loss: 0.0048  lr:0.000001
[ Sat Jul 13 04:57:41 2024 ] 	Batch(600/6809) done. Loss: 0.2099  lr:0.000001
[ Sat Jul 13 04:57:58 2024 ] 	Batch(700/6809) done. Loss: 0.0348  lr:0.000001
[ Sat Jul 13 04:58:17 2024 ] 	Batch(800/6809) done. Loss: 0.0721  lr:0.000001
[ Sat Jul 13 04:58:35 2024 ] 	Batch(900/6809) done. Loss: 0.0004  lr:0.000001
[ Sat Jul 13 04:58:53 2024 ] 
Training: Epoch [91/120], Step [999], Loss: 0.06563496589660645, Training Accuracy: 97.5875
[ Sat Jul 13 04:58:54 2024 ] 	Batch(1000/6809) done. Loss: 0.0400  lr:0.000001
[ Sat Jul 13 04:59:12 2024 ] 	Batch(1100/6809) done. Loss: 0.0060  lr:0.000001
[ Sat Jul 13 04:59:31 2024 ] 	Batch(1200/6809) done. Loss: 0.0508  lr:0.000001
[ Sat Jul 13 04:59:49 2024 ] 	Batch(1300/6809) done. Loss: 0.0130  lr:0.000001
[ Sat Jul 13 05:00:08 2024 ] 	Batch(1400/6809) done. Loss: 0.0446  lr:0.000001
[ Sat Jul 13 05:00:26 2024 ] 
Training: Epoch [91/120], Step [1499], Loss: 0.037849754095077515, Training Accuracy: 97.65
[ Sat Jul 13 05:00:26 2024 ] 	Batch(1500/6809) done. Loss: 0.0278  lr:0.000001
[ Sat Jul 13 05:00:45 2024 ] 	Batch(1600/6809) done. Loss: 0.0006  lr:0.000001
[ Sat Jul 13 05:01:03 2024 ] 	Batch(1700/6809) done. Loss: 0.0646  lr:0.000001
[ Sat Jul 13 05:01:22 2024 ] 	Batch(1800/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 05:01:41 2024 ] 	Batch(1900/6809) done. Loss: 0.3605  lr:0.000001
[ Sat Jul 13 05:01:59 2024 ] 
Training: Epoch [91/120], Step [1999], Loss: 0.05059174448251724, Training Accuracy: 97.74374999999999
[ Sat Jul 13 05:01:59 2024 ] 	Batch(2000/6809) done. Loss: 0.0286  lr:0.000001
[ Sat Jul 13 05:02:17 2024 ] 	Batch(2100/6809) done. Loss: 0.0238  lr:0.000001
[ Sat Jul 13 05:02:35 2024 ] 	Batch(2200/6809) done. Loss: 0.0172  lr:0.000001
[ Sat Jul 13 05:02:53 2024 ] 	Batch(2300/6809) done. Loss: 0.0108  lr:0.000001
[ Sat Jul 13 05:03:11 2024 ] 	Batch(2400/6809) done. Loss: 0.0232  lr:0.000001
[ Sat Jul 13 05:03:29 2024 ] 
Training: Epoch [91/120], Step [2499], Loss: 0.006218665279448032, Training Accuracy: 97.595
[ Sat Jul 13 05:03:30 2024 ] 	Batch(2500/6809) done. Loss: 0.0205  lr:0.000001
[ Sat Jul 13 05:03:48 2024 ] 	Batch(2600/6809) done. Loss: 0.2177  lr:0.000001
[ Sat Jul 13 05:04:05 2024 ] 	Batch(2700/6809) done. Loss: 0.0236  lr:0.000001
[ Sat Jul 13 05:04:23 2024 ] 	Batch(2800/6809) done. Loss: 0.0060  lr:0.000001
[ Sat Jul 13 05:04:41 2024 ] 	Batch(2900/6809) done. Loss: 0.2329  lr:0.000001
[ Sat Jul 13 05:04:59 2024 ] 
Training: Epoch [91/120], Step [2999], Loss: 0.022031378000974655, Training Accuracy: 97.62916666666666
[ Sat Jul 13 05:04:59 2024 ] 	Batch(3000/6809) done. Loss: 0.3559  lr:0.000001
[ Sat Jul 13 05:05:17 2024 ] 	Batch(3100/6809) done. Loss: 0.0130  lr:0.000001
[ Sat Jul 13 05:05:35 2024 ] 	Batch(3200/6809) done. Loss: 0.0755  lr:0.000001
[ Sat Jul 13 05:05:53 2024 ] 	Batch(3300/6809) done. Loss: 0.0720  lr:0.000001
[ Sat Jul 13 05:06:11 2024 ] 	Batch(3400/6809) done. Loss: 0.0272  lr:0.000001
[ Sat Jul 13 05:06:29 2024 ] 
Training: Epoch [91/120], Step [3499], Loss: 0.05220629274845123, Training Accuracy: 97.69642857142857
[ Sat Jul 13 05:06:29 2024 ] 	Batch(3500/6809) done. Loss: 0.0075  lr:0.000001
[ Sat Jul 13 05:06:47 2024 ] 	Batch(3600/6809) done. Loss: 0.0452  lr:0.000001
[ Sat Jul 13 05:07:05 2024 ] 	Batch(3700/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 05:07:23 2024 ] 	Batch(3800/6809) done. Loss: 0.2225  lr:0.000001
[ Sat Jul 13 05:07:40 2024 ] 	Batch(3900/6809) done. Loss: 0.0334  lr:0.000001
[ Sat Jul 13 05:07:58 2024 ] 
Training: Epoch [91/120], Step [3999], Loss: 0.1031087189912796, Training Accuracy: 97.68124999999999
[ Sat Jul 13 05:07:58 2024 ] 	Batch(4000/6809) done. Loss: 0.0538  lr:0.000001
[ Sat Jul 13 05:08:16 2024 ] 	Batch(4100/6809) done. Loss: 0.0462  lr:0.000001
[ Sat Jul 13 05:08:34 2024 ] 	Batch(4200/6809) done. Loss: 0.0323  lr:0.000001
[ Sat Jul 13 05:08:52 2024 ] 	Batch(4300/6809) done. Loss: 0.0211  lr:0.000001
[ Sat Jul 13 05:09:10 2024 ] 	Batch(4400/6809) done. Loss: 0.0521  lr:0.000001
[ Sat Jul 13 05:09:28 2024 ] 
Training: Epoch [91/120], Step [4499], Loss: 0.001104583265259862, Training Accuracy: 97.65833333333333
[ Sat Jul 13 05:09:28 2024 ] 	Batch(4500/6809) done. Loss: 0.0464  lr:0.000001
[ Sat Jul 13 05:09:46 2024 ] 	Batch(4600/6809) done. Loss: 0.0158  lr:0.000001
[ Sat Jul 13 05:10:04 2024 ] 	Batch(4700/6809) done. Loss: 0.3086  lr:0.000001
[ Sat Jul 13 05:10:22 2024 ] 	Batch(4800/6809) done. Loss: 0.0552  lr:0.000001
[ Sat Jul 13 05:10:40 2024 ] 	Batch(4900/6809) done. Loss: 0.1110  lr:0.000001
[ Sat Jul 13 05:10:57 2024 ] 
Training: Epoch [91/120], Step [4999], Loss: 0.1395343393087387, Training Accuracy: 97.665
[ Sat Jul 13 05:10:57 2024 ] 	Batch(5000/6809) done. Loss: 0.0300  lr:0.000001
[ Sat Jul 13 05:11:15 2024 ] 	Batch(5100/6809) done. Loss: 0.0431  lr:0.000001
[ Sat Jul 13 05:11:33 2024 ] 	Batch(5200/6809) done. Loss: 0.0148  lr:0.000001
[ Sat Jul 13 05:11:52 2024 ] 	Batch(5300/6809) done. Loss: 0.2468  lr:0.000001
[ Sat Jul 13 05:12:11 2024 ] 	Batch(5400/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 05:12:29 2024 ] 
Training: Epoch [91/120], Step [5499], Loss: 0.12178774923086166, Training Accuracy: 97.64090909090909
[ Sat Jul 13 05:12:29 2024 ] 	Batch(5500/6809) done. Loss: 0.0059  lr:0.000001
[ Sat Jul 13 05:12:48 2024 ] 	Batch(5600/6809) done. Loss: 0.0171  lr:0.000001
[ Sat Jul 13 05:13:06 2024 ] 	Batch(5700/6809) done. Loss: 0.1403  lr:0.000001
[ Sat Jul 13 05:13:24 2024 ] 	Batch(5800/6809) done. Loss: 0.1687  lr:0.000001
[ Sat Jul 13 05:13:41 2024 ] 	Batch(5900/6809) done. Loss: 0.0729  lr:0.000001
[ Sat Jul 13 05:13:59 2024 ] 
Training: Epoch [91/120], Step [5999], Loss: 0.07601689547300339, Training Accuracy: 97.59583333333333
[ Sat Jul 13 05:13:59 2024 ] 	Batch(6000/6809) done. Loss: 0.0172  lr:0.000001
[ Sat Jul 13 05:14:17 2024 ] 	Batch(6100/6809) done. Loss: 0.1711  lr:0.000001
[ Sat Jul 13 05:14:35 2024 ] 	Batch(6200/6809) done. Loss: 0.0762  lr:0.000001
[ Sat Jul 13 05:14:53 2024 ] 	Batch(6300/6809) done. Loss: 0.0563  lr:0.000001
[ Sat Jul 13 05:15:11 2024 ] 	Batch(6400/6809) done. Loss: 0.2043  lr:0.000001
[ Sat Jul 13 05:15:29 2024 ] 
Training: Epoch [91/120], Step [6499], Loss: 0.009926141239702702, Training Accuracy: 97.58461538461538
[ Sat Jul 13 05:15:29 2024 ] 	Batch(6500/6809) done. Loss: 0.0059  lr:0.000001
[ Sat Jul 13 05:15:47 2024 ] 	Batch(6600/6809) done. Loss: 0.0316  lr:0.000001
[ Sat Jul 13 05:16:05 2024 ] 	Batch(6700/6809) done. Loss: 0.0966  lr:0.000001
[ Sat Jul 13 05:16:23 2024 ] 	Batch(6800/6809) done. Loss: 0.0131  lr:0.000001
[ Sat Jul 13 05:16:25 2024 ] 	Mean training loss: 0.0960.
[ Sat Jul 13 05:16:25 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 05:16:25 2024 ] Training epoch: 93
[ Sat Jul 13 05:16:26 2024 ] 	Batch(0/6809) done. Loss: 0.0230  lr:0.000001
[ Sat Jul 13 05:16:43 2024 ] 	Batch(100/6809) done. Loss: 0.1757  lr:0.000001
[ Sat Jul 13 05:17:01 2024 ] 	Batch(200/6809) done. Loss: 0.0230  lr:0.000001
[ Sat Jul 13 05:17:19 2024 ] 	Batch(300/6809) done. Loss: 0.0464  lr:0.000001
[ Sat Jul 13 05:17:37 2024 ] 	Batch(400/6809) done. Loss: 0.1561  lr:0.000001
[ Sat Jul 13 05:17:55 2024 ] 
Training: Epoch [92/120], Step [499], Loss: 0.15016639232635498, Training Accuracy: 97.775
[ Sat Jul 13 05:17:55 2024 ] 	Batch(500/6809) done. Loss: 0.0568  lr:0.000001
[ Sat Jul 13 05:18:13 2024 ] 	Batch(600/6809) done. Loss: 0.2930  lr:0.000001
[ Sat Jul 13 05:18:31 2024 ] 	Batch(700/6809) done. Loss: 0.0361  lr:0.000001
[ Sat Jul 13 05:18:49 2024 ] 	Batch(800/6809) done. Loss: 0.0161  lr:0.000001
[ Sat Jul 13 05:19:07 2024 ] 	Batch(900/6809) done. Loss: 0.1404  lr:0.000001
[ Sat Jul 13 05:19:24 2024 ] 
Training: Epoch [92/120], Step [999], Loss: 0.13970060646533966, Training Accuracy: 97.65
[ Sat Jul 13 05:19:25 2024 ] 	Batch(1000/6809) done. Loss: 0.1313  lr:0.000001
[ Sat Jul 13 05:19:43 2024 ] 	Batch(1100/6809) done. Loss: 0.0567  lr:0.000001
[ Sat Jul 13 05:20:01 2024 ] 	Batch(1200/6809) done. Loss: 0.0169  lr:0.000001
[ Sat Jul 13 05:20:19 2024 ] 	Batch(1300/6809) done. Loss: 0.1874  lr:0.000001
[ Sat Jul 13 05:20:37 2024 ] 	Batch(1400/6809) done. Loss: 0.0203  lr:0.000001
[ Sat Jul 13 05:20:54 2024 ] 
Training: Epoch [92/120], Step [1499], Loss: 0.015332159586250782, Training Accuracy: 97.65833333333333
[ Sat Jul 13 05:20:54 2024 ] 	Batch(1500/6809) done. Loss: 0.1775  lr:0.000001
[ Sat Jul 13 05:21:12 2024 ] 	Batch(1600/6809) done. Loss: 0.0157  lr:0.000001
[ Sat Jul 13 05:21:31 2024 ] 	Batch(1700/6809) done. Loss: 0.0663  lr:0.000001
[ Sat Jul 13 05:21:49 2024 ] 	Batch(1800/6809) done. Loss: 0.1780  lr:0.000001
[ Sat Jul 13 05:22:07 2024 ] 	Batch(1900/6809) done. Loss: 0.0215  lr:0.000001
[ Sat Jul 13 05:22:25 2024 ] 
Training: Epoch [92/120], Step [1999], Loss: 0.18478171527385712, Training Accuracy: 97.675
[ Sat Jul 13 05:22:25 2024 ] 	Batch(2000/6809) done. Loss: 0.0562  lr:0.000001
[ Sat Jul 13 05:22:43 2024 ] 	Batch(2100/6809) done. Loss: 0.0902  lr:0.000001
[ Sat Jul 13 05:23:01 2024 ] 	Batch(2200/6809) done. Loss: 0.0107  lr:0.000001
[ Sat Jul 13 05:23:19 2024 ] 	Batch(2300/6809) done. Loss: 0.0071  lr:0.000001
[ Sat Jul 13 05:23:37 2024 ] 	Batch(2400/6809) done. Loss: 0.0782  lr:0.000001
[ Sat Jul 13 05:23:54 2024 ] 
Training: Epoch [92/120], Step [2499], Loss: 0.15161265432834625, Training Accuracy: 97.7
[ Sat Jul 13 05:23:55 2024 ] 	Batch(2500/6809) done. Loss: 0.0115  lr:0.000001
[ Sat Jul 13 05:24:12 2024 ] 	Batch(2600/6809) done. Loss: 0.1139  lr:0.000001
[ Sat Jul 13 05:24:30 2024 ] 	Batch(2700/6809) done. Loss: 0.0363  lr:0.000001
[ Sat Jul 13 05:24:48 2024 ] 	Batch(2800/6809) done. Loss: 0.0663  lr:0.000001
[ Sat Jul 13 05:25:07 2024 ] 	Batch(2900/6809) done. Loss: 0.0480  lr:0.000001
[ Sat Jul 13 05:25:25 2024 ] 
Training: Epoch [92/120], Step [2999], Loss: 0.06114732846617699, Training Accuracy: 97.65
[ Sat Jul 13 05:25:25 2024 ] 	Batch(3000/6809) done. Loss: 0.0181  lr:0.000001
[ Sat Jul 13 05:25:44 2024 ] 	Batch(3100/6809) done. Loss: 0.0072  lr:0.000001
[ Sat Jul 13 05:26:03 2024 ] 	Batch(3200/6809) done. Loss: 0.0234  lr:0.000001
[ Sat Jul 13 05:26:21 2024 ] 	Batch(3300/6809) done. Loss: 0.1676  lr:0.000001
[ Sat Jul 13 05:26:40 2024 ] 	Batch(3400/6809) done. Loss: 0.0237  lr:0.000001
[ Sat Jul 13 05:26:58 2024 ] 
Training: Epoch [92/120], Step [3499], Loss: 0.08970235288143158, Training Accuracy: 97.66785714285714
[ Sat Jul 13 05:26:58 2024 ] 	Batch(3500/6809) done. Loss: 0.1234  lr:0.000001
[ Sat Jul 13 05:27:16 2024 ] 	Batch(3600/6809) done. Loss: 0.0236  lr:0.000001
[ Sat Jul 13 05:27:35 2024 ] 	Batch(3700/6809) done. Loss: 0.0398  lr:0.000001
[ Sat Jul 13 05:27:53 2024 ] 	Batch(3800/6809) done. Loss: 0.2293  lr:0.000001
[ Sat Jul 13 05:28:12 2024 ] 	Batch(3900/6809) done. Loss: 0.0796  lr:0.000001
[ Sat Jul 13 05:28:30 2024 ] 
Training: Epoch [92/120], Step [3999], Loss: 0.014835227280855179, Training Accuracy: 97.63437499999999
[ Sat Jul 13 05:28:30 2024 ] 	Batch(4000/6809) done. Loss: 0.4121  lr:0.000001
[ Sat Jul 13 05:28:49 2024 ] 	Batch(4100/6809) done. Loss: 0.4590  lr:0.000001
[ Sat Jul 13 05:29:07 2024 ] 	Batch(4200/6809) done. Loss: 0.1305  lr:0.000001
[ Sat Jul 13 05:29:26 2024 ] 	Batch(4300/6809) done. Loss: 0.0284  lr:0.000001
[ Sat Jul 13 05:29:45 2024 ] 	Batch(4400/6809) done. Loss: 0.2490  lr:0.000001
[ Sat Jul 13 05:30:02 2024 ] 
Training: Epoch [92/120], Step [4499], Loss: 0.050891581922769547, Training Accuracy: 97.6138888888889
[ Sat Jul 13 05:30:02 2024 ] 	Batch(4500/6809) done. Loss: 0.0166  lr:0.000001
[ Sat Jul 13 05:30:20 2024 ] 	Batch(4600/6809) done. Loss: 0.0332  lr:0.000001
[ Sat Jul 13 05:30:38 2024 ] 	Batch(4700/6809) done. Loss: 0.0051  lr:0.000001
[ Sat Jul 13 05:30:56 2024 ] 	Batch(4800/6809) done. Loss: 0.0734  lr:0.000001
[ Sat Jul 13 05:31:14 2024 ] 	Batch(4900/6809) done. Loss: 0.3585  lr:0.000001
[ Sat Jul 13 05:31:32 2024 ] 
Training: Epoch [92/120], Step [4999], Loss: 0.0663611963391304, Training Accuracy: 97.625
[ Sat Jul 13 05:31:32 2024 ] 	Batch(5000/6809) done. Loss: 0.0663  lr:0.000001
[ Sat Jul 13 05:31:50 2024 ] 	Batch(5100/6809) done. Loss: 0.0573  lr:0.000001
[ Sat Jul 13 05:32:08 2024 ] 	Batch(5200/6809) done. Loss: 0.0717  lr:0.000001
[ Sat Jul 13 05:32:26 2024 ] 	Batch(5300/6809) done. Loss: 0.0328  lr:0.000001
[ Sat Jul 13 05:32:44 2024 ] 	Batch(5400/6809) done. Loss: 0.1890  lr:0.000001
[ Sat Jul 13 05:33:02 2024 ] 
Training: Epoch [92/120], Step [5499], Loss: 0.018035996705293655, Training Accuracy: 97.61363636363637
[ Sat Jul 13 05:33:02 2024 ] 	Batch(5500/6809) done. Loss: 0.0498  lr:0.000001
[ Sat Jul 13 05:33:20 2024 ] 	Batch(5600/6809) done. Loss: 0.0007  lr:0.000001
[ Sat Jul 13 05:33:38 2024 ] 	Batch(5700/6809) done. Loss: 0.1107  lr:0.000001
[ Sat Jul 13 05:33:57 2024 ] 	Batch(5800/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 05:34:14 2024 ] 	Batch(5900/6809) done. Loss: 0.0216  lr:0.000001
[ Sat Jul 13 05:34:32 2024 ] 
Training: Epoch [92/120], Step [5999], Loss: 0.029699495062232018, Training Accuracy: 97.6125
[ Sat Jul 13 05:34:33 2024 ] 	Batch(6000/6809) done. Loss: 0.6842  lr:0.000001
[ Sat Jul 13 05:34:50 2024 ] 	Batch(6100/6809) done. Loss: 0.0253  lr:0.000001
[ Sat Jul 13 05:35:08 2024 ] 	Batch(6200/6809) done. Loss: 0.0178  lr:0.000001
[ Sat Jul 13 05:35:26 2024 ] 	Batch(6300/6809) done. Loss: 0.0664  lr:0.000001
[ Sat Jul 13 05:35:44 2024 ] 	Batch(6400/6809) done. Loss: 0.0624  lr:0.000001
[ Sat Jul 13 05:36:02 2024 ] 
Training: Epoch [92/120], Step [6499], Loss: 0.020368725061416626, Training Accuracy: 97.60384615384615
[ Sat Jul 13 05:36:02 2024 ] 	Batch(6500/6809) done. Loss: 0.0941  lr:0.000001
[ Sat Jul 13 05:36:20 2024 ] 	Batch(6600/6809) done. Loss: 0.0697  lr:0.000001
[ Sat Jul 13 05:36:38 2024 ] 	Batch(6700/6809) done. Loss: 0.1089  lr:0.000001
[ Sat Jul 13 05:36:56 2024 ] 	Batch(6800/6809) done. Loss: 0.0430  lr:0.000001
[ Sat Jul 13 05:36:58 2024 ] 	Mean training loss: 0.0974.
[ Sat Jul 13 05:36:58 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 05:36:58 2024 ] Training epoch: 94
[ Sat Jul 13 05:36:58 2024 ] 	Batch(0/6809) done. Loss: 0.0823  lr:0.000001
[ Sat Jul 13 05:37:17 2024 ] 	Batch(100/6809) done. Loss: 0.0027  lr:0.000001
[ Sat Jul 13 05:37:35 2024 ] 	Batch(200/6809) done. Loss: 0.0189  lr:0.000001
[ Sat Jul 13 05:37:53 2024 ] 	Batch(300/6809) done. Loss: 0.0274  lr:0.000001
[ Sat Jul 13 05:38:12 2024 ] 	Batch(400/6809) done. Loss: 0.1100  lr:0.000001
[ Sat Jul 13 05:38:29 2024 ] 
Training: Epoch [93/120], Step [499], Loss: 0.017552703619003296, Training Accuracy: 97.675
[ Sat Jul 13 05:38:30 2024 ] 	Batch(500/6809) done. Loss: 0.0155  lr:0.000001
[ Sat Jul 13 05:38:47 2024 ] 	Batch(600/6809) done. Loss: 0.0207  lr:0.000001
[ Sat Jul 13 05:39:05 2024 ] 	Batch(700/6809) done. Loss: 0.0552  lr:0.000001
[ Sat Jul 13 05:39:23 2024 ] 	Batch(800/6809) done. Loss: 0.0080  lr:0.000001
[ Sat Jul 13 05:39:41 2024 ] 	Batch(900/6809) done. Loss: 0.0069  lr:0.000001
[ Sat Jul 13 05:39:59 2024 ] 
Training: Epoch [93/120], Step [999], Loss: 0.2624472379684448, Training Accuracy: 97.55
[ Sat Jul 13 05:39:59 2024 ] 	Batch(1000/6809) done. Loss: 0.0973  lr:0.000001
[ Sat Jul 13 05:40:17 2024 ] 	Batch(1100/6809) done. Loss: 0.0468  lr:0.000001
[ Sat Jul 13 05:40:35 2024 ] 	Batch(1200/6809) done. Loss: 0.0311  lr:0.000001
[ Sat Jul 13 05:40:53 2024 ] 	Batch(1300/6809) done. Loss: 0.0023  lr:0.000001
[ Sat Jul 13 05:41:12 2024 ] 	Batch(1400/6809) done. Loss: 0.0233  lr:0.000001
[ Sat Jul 13 05:41:30 2024 ] 
Training: Epoch [93/120], Step [1499], Loss: 0.08356638252735138, Training Accuracy: 97.5
[ Sat Jul 13 05:41:30 2024 ] 	Batch(1500/6809) done. Loss: 0.0816  lr:0.000001
[ Sat Jul 13 05:41:49 2024 ] 	Batch(1600/6809) done. Loss: 0.0543  lr:0.000001
[ Sat Jul 13 05:42:08 2024 ] 	Batch(1700/6809) done. Loss: 0.0337  lr:0.000001
[ Sat Jul 13 05:42:26 2024 ] 	Batch(1800/6809) done. Loss: 0.0090  lr:0.000001
[ Sat Jul 13 05:42:45 2024 ] 	Batch(1900/6809) done. Loss: 0.0248  lr:0.000001
[ Sat Jul 13 05:43:03 2024 ] 
Training: Epoch [93/120], Step [1999], Loss: 0.06829331815242767, Training Accuracy: 97.55625
[ Sat Jul 13 05:43:03 2024 ] 	Batch(2000/6809) done. Loss: 0.0030  lr:0.000001
[ Sat Jul 13 05:43:21 2024 ] 	Batch(2100/6809) done. Loss: 0.1030  lr:0.000001
[ Sat Jul 13 05:43:39 2024 ] 	Batch(2200/6809) done. Loss: 0.2327  lr:0.000001
[ Sat Jul 13 05:43:57 2024 ] 	Batch(2300/6809) done. Loss: 0.0191  lr:0.000001
[ Sat Jul 13 05:44:15 2024 ] 	Batch(2400/6809) done. Loss: 0.0606  lr:0.000001
[ Sat Jul 13 05:44:33 2024 ] 
Training: Epoch [93/120], Step [2499], Loss: 0.21759510040283203, Training Accuracy: 97.6
[ Sat Jul 13 05:44:33 2024 ] 	Batch(2500/6809) done. Loss: 0.2519  lr:0.000001
[ Sat Jul 13 05:44:51 2024 ] 	Batch(2600/6809) done. Loss: 0.2048  lr:0.000001
[ Sat Jul 13 05:45:09 2024 ] 	Batch(2700/6809) done. Loss: 0.0843  lr:0.000001
[ Sat Jul 13 05:45:27 2024 ] 	Batch(2800/6809) done. Loss: 0.0655  lr:0.000001
[ Sat Jul 13 05:45:45 2024 ] 	Batch(2900/6809) done. Loss: 0.0380  lr:0.000001
[ Sat Jul 13 05:46:02 2024 ] 
Training: Epoch [93/120], Step [2999], Loss: 0.010479690507054329, Training Accuracy: 97.59583333333333
[ Sat Jul 13 05:46:02 2024 ] 	Batch(3000/6809) done. Loss: 0.1743  lr:0.000001
[ Sat Jul 13 05:46:20 2024 ] 	Batch(3100/6809) done. Loss: 0.0895  lr:0.000001
[ Sat Jul 13 05:46:38 2024 ] 	Batch(3200/6809) done. Loss: 0.0115  lr:0.000001
[ Sat Jul 13 05:46:56 2024 ] 	Batch(3300/6809) done. Loss: 0.0583  lr:0.000001
[ Sat Jul 13 05:47:14 2024 ] 	Batch(3400/6809) done. Loss: 0.0171  lr:0.000001
[ Sat Jul 13 05:47:32 2024 ] 
Training: Epoch [93/120], Step [3499], Loss: 0.08970490097999573, Training Accuracy: 97.575
[ Sat Jul 13 05:47:32 2024 ] 	Batch(3500/6809) done. Loss: 0.1268  lr:0.000001
[ Sat Jul 13 05:47:50 2024 ] 	Batch(3600/6809) done. Loss: 0.0024  lr:0.000001
[ Sat Jul 13 05:48:08 2024 ] 	Batch(3700/6809) done. Loss: 0.0374  lr:0.000001
[ Sat Jul 13 05:48:26 2024 ] 	Batch(3800/6809) done. Loss: 0.4200  lr:0.000001
[ Sat Jul 13 05:48:44 2024 ] 	Batch(3900/6809) done. Loss: 0.0822  lr:0.000001
[ Sat Jul 13 05:49:01 2024 ] 
Training: Epoch [93/120], Step [3999], Loss: 0.16728276014328003, Training Accuracy: 97.603125
[ Sat Jul 13 05:49:02 2024 ] 	Batch(4000/6809) done. Loss: 0.3994  lr:0.000001
[ Sat Jul 13 05:49:20 2024 ] 	Batch(4100/6809) done. Loss: 0.0807  lr:0.000001
[ Sat Jul 13 05:49:39 2024 ] 	Batch(4200/6809) done. Loss: 0.0967  lr:0.000001
[ Sat Jul 13 05:49:57 2024 ] 	Batch(4300/6809) done. Loss: 0.2501  lr:0.000001
[ Sat Jul 13 05:50:15 2024 ] 	Batch(4400/6809) done. Loss: 0.1080  lr:0.000001
[ Sat Jul 13 05:50:33 2024 ] 
Training: Epoch [93/120], Step [4499], Loss: 0.07523377984762192, Training Accuracy: 97.56666666666666
[ Sat Jul 13 05:50:33 2024 ] 	Batch(4500/6809) done. Loss: 0.0613  lr:0.000001
[ Sat Jul 13 05:50:52 2024 ] 	Batch(4600/6809) done. Loss: 0.0548  lr:0.000001
[ Sat Jul 13 05:51:10 2024 ] 	Batch(4700/6809) done. Loss: 0.0434  lr:0.000001
[ Sat Jul 13 05:51:29 2024 ] 	Batch(4800/6809) done. Loss: 0.0713  lr:0.000001
[ Sat Jul 13 05:51:46 2024 ] 	Batch(4900/6809) done. Loss: 0.0321  lr:0.000001
[ Sat Jul 13 05:52:04 2024 ] 
Training: Epoch [93/120], Step [4999], Loss: 0.15233121812343597, Training Accuracy: 97.5775
[ Sat Jul 13 05:52:04 2024 ] 	Batch(5000/6809) done. Loss: 0.0805  lr:0.000001
[ Sat Jul 13 05:52:22 2024 ] 	Batch(5100/6809) done. Loss: 0.0053  lr:0.000001
[ Sat Jul 13 05:52:40 2024 ] 	Batch(5200/6809) done. Loss: 0.0271  lr:0.000001
[ Sat Jul 13 05:52:58 2024 ] 	Batch(5300/6809) done. Loss: 0.0492  lr:0.000001
[ Sat Jul 13 05:53:16 2024 ] 	Batch(5400/6809) done. Loss: 0.0770  lr:0.000001
[ Sat Jul 13 05:53:34 2024 ] 
Training: Epoch [93/120], Step [5499], Loss: 0.051096588373184204, Training Accuracy: 97.61136363636363
[ Sat Jul 13 05:53:34 2024 ] 	Batch(5500/6809) done. Loss: 0.1365  lr:0.000001
[ Sat Jul 13 05:53:52 2024 ] 	Batch(5600/6809) done. Loss: 0.0306  lr:0.000001
[ Sat Jul 13 05:54:10 2024 ] 	Batch(5700/6809) done. Loss: 0.2718  lr:0.000001
[ Sat Jul 13 05:54:29 2024 ] 	Batch(5800/6809) done. Loss: 0.0019  lr:0.000001
[ Sat Jul 13 05:54:47 2024 ] 	Batch(5900/6809) done. Loss: 0.8969  lr:0.000001
[ Sat Jul 13 05:55:06 2024 ] 
Training: Epoch [93/120], Step [5999], Loss: 0.5317299962043762, Training Accuracy: 97.57291666666667
[ Sat Jul 13 05:55:06 2024 ] 	Batch(6000/6809) done. Loss: 0.1501  lr:0.000001
[ Sat Jul 13 05:55:25 2024 ] 	Batch(6100/6809) done. Loss: 0.0331  lr:0.000001
[ Sat Jul 13 05:55:43 2024 ] 	Batch(6200/6809) done. Loss: 0.0149  lr:0.000001
[ Sat Jul 13 05:56:02 2024 ] 	Batch(6300/6809) done. Loss: 0.1209  lr:0.000001
[ Sat Jul 13 05:56:20 2024 ] 	Batch(6400/6809) done. Loss: 0.0528  lr:0.000001
[ Sat Jul 13 05:56:39 2024 ] 
Training: Epoch [93/120], Step [6499], Loss: 0.04604129120707512, Training Accuracy: 97.55192307692307
[ Sat Jul 13 05:56:39 2024 ] 	Batch(6500/6809) done. Loss: 0.0156  lr:0.000001
[ Sat Jul 13 05:56:57 2024 ] 	Batch(6600/6809) done. Loss: 0.0507  lr:0.000001
[ Sat Jul 13 05:57:15 2024 ] 	Batch(6700/6809) done. Loss: 0.1539  lr:0.000001
[ Sat Jul 13 05:57:33 2024 ] 	Batch(6800/6809) done. Loss: 0.0317  lr:0.000001
[ Sat Jul 13 05:57:34 2024 ] 	Mean training loss: 0.0982.
[ Sat Jul 13 05:57:34 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 05:57:34 2024 ] Training epoch: 95
[ Sat Jul 13 05:57:35 2024 ] 	Batch(0/6809) done. Loss: 0.0406  lr:0.000001
[ Sat Jul 13 05:57:53 2024 ] 	Batch(100/6809) done. Loss: 0.0520  lr:0.000001
[ Sat Jul 13 05:58:11 2024 ] 	Batch(200/6809) done. Loss: 0.3788  lr:0.000001
[ Sat Jul 13 05:58:28 2024 ] 	Batch(300/6809) done. Loss: 0.0360  lr:0.000001
[ Sat Jul 13 05:58:46 2024 ] 	Batch(400/6809) done. Loss: 0.4105  lr:0.000001
[ Sat Jul 13 05:59:04 2024 ] 
Training: Epoch [94/120], Step [499], Loss: 0.03248704597353935, Training Accuracy: 97.15
[ Sat Jul 13 05:59:04 2024 ] 	Batch(500/6809) done. Loss: 0.0502  lr:0.000001
[ Sat Jul 13 05:59:22 2024 ] 	Batch(600/6809) done. Loss: 0.1918  lr:0.000001
[ Sat Jul 13 05:59:40 2024 ] 	Batch(700/6809) done. Loss: 0.1975  lr:0.000001
[ Sat Jul 13 05:59:58 2024 ] 	Batch(800/6809) done. Loss: 0.0043  lr:0.000001
[ Sat Jul 13 06:00:17 2024 ] 	Batch(900/6809) done. Loss: 0.0224  lr:0.000001
[ Sat Jul 13 06:00:35 2024 ] 
Training: Epoch [94/120], Step [999], Loss: 0.024403750896453857, Training Accuracy: 97.25
[ Sat Jul 13 06:00:35 2024 ] 	Batch(1000/6809) done. Loss: 0.4066  lr:0.000001
[ Sat Jul 13 06:00:53 2024 ] 	Batch(1100/6809) done. Loss: 0.0328  lr:0.000001
[ Sat Jul 13 06:01:11 2024 ] 	Batch(1200/6809) done. Loss: 0.1426  lr:0.000001
[ Sat Jul 13 06:01:29 2024 ] 	Batch(1300/6809) done. Loss: 0.0481  lr:0.000001
[ Sat Jul 13 06:01:47 2024 ] 	Batch(1400/6809) done. Loss: 0.1108  lr:0.000001
[ Sat Jul 13 06:02:05 2024 ] 
Training: Epoch [94/120], Step [1499], Loss: 0.07837709784507751, Training Accuracy: 97.43333333333334
[ Sat Jul 13 06:02:05 2024 ] 	Batch(1500/6809) done. Loss: 0.1417  lr:0.000001
[ Sat Jul 13 06:02:23 2024 ] 	Batch(1600/6809) done. Loss: 0.0327  lr:0.000001
[ Sat Jul 13 06:02:42 2024 ] 	Batch(1700/6809) done. Loss: 0.0046  lr:0.000001
[ Sat Jul 13 06:03:00 2024 ] 	Batch(1800/6809) done. Loss: 0.0599  lr:0.000001
[ Sat Jul 13 06:03:19 2024 ] 	Batch(1900/6809) done. Loss: 0.0207  lr:0.000001
[ Sat Jul 13 06:03:37 2024 ] 
Training: Epoch [94/120], Step [1999], Loss: 0.01441210601478815, Training Accuracy: 97.46249999999999
[ Sat Jul 13 06:03:37 2024 ] 	Batch(2000/6809) done. Loss: 0.0049  lr:0.000001
[ Sat Jul 13 06:03:56 2024 ] 	Batch(2100/6809) done. Loss: 0.0290  lr:0.000001
[ Sat Jul 13 06:04:14 2024 ] 	Batch(2200/6809) done. Loss: 0.2866  lr:0.000001
[ Sat Jul 13 06:04:32 2024 ] 	Batch(2300/6809) done. Loss: 0.0713  lr:0.000001
[ Sat Jul 13 06:04:50 2024 ] 	Batch(2400/6809) done. Loss: 0.0081  lr:0.000001
[ Sat Jul 13 06:05:08 2024 ] 
Training: Epoch [94/120], Step [2499], Loss: 0.3479391932487488, Training Accuracy: 97.475
[ Sat Jul 13 06:05:08 2024 ] 	Batch(2500/6809) done. Loss: 0.3828  lr:0.000001
[ Sat Jul 13 06:05:26 2024 ] 	Batch(2600/6809) done. Loss: 0.0142  lr:0.000001
[ Sat Jul 13 06:05:44 2024 ] 	Batch(2700/6809) done. Loss: 0.2732  lr:0.000001
[ Sat Jul 13 06:06:02 2024 ] 	Batch(2800/6809) done. Loss: 0.0141  lr:0.000001
[ Sat Jul 13 06:06:19 2024 ] 	Batch(2900/6809) done. Loss: 0.2571  lr:0.000001
[ Sat Jul 13 06:06:37 2024 ] 
Training: Epoch [94/120], Step [2999], Loss: 0.0023475466296076775, Training Accuracy: 97.49166666666666
[ Sat Jul 13 06:06:37 2024 ] 	Batch(3000/6809) done. Loss: 0.0109  lr:0.000001
[ Sat Jul 13 06:06:55 2024 ] 	Batch(3100/6809) done. Loss: 0.0494  lr:0.000001
[ Sat Jul 13 06:07:13 2024 ] 	Batch(3200/6809) done. Loss: 0.1600  lr:0.000001
[ Sat Jul 13 06:07:32 2024 ] 	Batch(3300/6809) done. Loss: 0.0377  lr:0.000001
[ Sat Jul 13 06:07:50 2024 ] 	Batch(3400/6809) done. Loss: 0.0368  lr:0.000001
[ Sat Jul 13 06:08:09 2024 ] 
Training: Epoch [94/120], Step [3499], Loss: 0.06497544050216675, Training Accuracy: 97.53214285714286
[ Sat Jul 13 06:08:09 2024 ] 	Batch(3500/6809) done. Loss: 0.0150  lr:0.000001
[ Sat Jul 13 06:08:27 2024 ] 	Batch(3600/6809) done. Loss: 0.0125  lr:0.000001
[ Sat Jul 13 06:08:46 2024 ] 	Batch(3700/6809) done. Loss: 0.0547  lr:0.000001
[ Sat Jul 13 06:09:04 2024 ] 	Batch(3800/6809) done. Loss: 0.0167  lr:0.000001
[ Sat Jul 13 06:09:22 2024 ] 	Batch(3900/6809) done. Loss: 0.0560  lr:0.000001
[ Sat Jul 13 06:09:39 2024 ] 
Training: Epoch [94/120], Step [3999], Loss: 0.007764026522636414, Training Accuracy: 97.546875
[ Sat Jul 13 06:09:40 2024 ] 	Batch(4000/6809) done. Loss: 0.0418  lr:0.000001
[ Sat Jul 13 06:09:58 2024 ] 	Batch(4100/6809) done. Loss: 0.0537  lr:0.000001
[ Sat Jul 13 06:10:15 2024 ] 	Batch(4200/6809) done. Loss: 0.2262  lr:0.000001
[ Sat Jul 13 06:10:33 2024 ] 	Batch(4300/6809) done. Loss: 0.0251  lr:0.000001
[ Sat Jul 13 06:10:51 2024 ] 	Batch(4400/6809) done. Loss: 0.0476  lr:0.000001
[ Sat Jul 13 06:11:09 2024 ] 
Training: Epoch [94/120], Step [4499], Loss: 0.2702189087867737, Training Accuracy: 97.54166666666667
[ Sat Jul 13 06:11:09 2024 ] 	Batch(4500/6809) done. Loss: 0.0228  lr:0.000001
[ Sat Jul 13 06:11:27 2024 ] 	Batch(4600/6809) done. Loss: 0.0129  lr:0.000001
[ Sat Jul 13 06:11:45 2024 ] 	Batch(4700/6809) done. Loss: 0.0306  lr:0.000001
[ Sat Jul 13 06:12:03 2024 ] 	Batch(4800/6809) done. Loss: 0.0101  lr:0.000001
[ Sat Jul 13 06:12:21 2024 ] 	Batch(4900/6809) done. Loss: 0.1338  lr:0.000001
[ Sat Jul 13 06:12:40 2024 ] 
Training: Epoch [94/120], Step [4999], Loss: 0.02700066938996315, Training Accuracy: 97.58
[ Sat Jul 13 06:12:40 2024 ] 	Batch(5000/6809) done. Loss: 0.0679  lr:0.000001
[ Sat Jul 13 06:12:58 2024 ] 	Batch(5100/6809) done. Loss: 0.0123  lr:0.000001
[ Sat Jul 13 06:13:16 2024 ] 	Batch(5200/6809) done. Loss: 0.3774  lr:0.000001
[ Sat Jul 13 06:13:35 2024 ] 	Batch(5300/6809) done. Loss: 0.0217  lr:0.000001
[ Sat Jul 13 06:13:53 2024 ] 	Batch(5400/6809) done. Loss: 0.0676  lr:0.000001
[ Sat Jul 13 06:14:11 2024 ] 
Training: Epoch [94/120], Step [5499], Loss: 0.0825347825884819, Training Accuracy: 97.55227272727272
[ Sat Jul 13 06:14:12 2024 ] 	Batch(5500/6809) done. Loss: 0.0451  lr:0.000001
[ Sat Jul 13 06:14:30 2024 ] 	Batch(5600/6809) done. Loss: 0.0118  lr:0.000001
[ Sat Jul 13 06:14:49 2024 ] 	Batch(5700/6809) done. Loss: 0.2148  lr:0.000001
[ Sat Jul 13 06:15:07 2024 ] 	Batch(5800/6809) done. Loss: 0.0333  lr:0.000001
[ Sat Jul 13 06:15:25 2024 ] 	Batch(5900/6809) done. Loss: 0.0169  lr:0.000001
[ Sat Jul 13 06:15:42 2024 ] 
Training: Epoch [94/120], Step [5999], Loss: 0.0330134816467762, Training Accuracy: 97.59791666666666
[ Sat Jul 13 06:15:43 2024 ] 	Batch(6000/6809) done. Loss: 0.0500  lr:0.000001
[ Sat Jul 13 06:16:00 2024 ] 	Batch(6100/6809) done. Loss: 0.0202  lr:0.000001
[ Sat Jul 13 06:16:18 2024 ] 	Batch(6200/6809) done. Loss: 0.1160  lr:0.000001
[ Sat Jul 13 06:16:36 2024 ] 	Batch(6300/6809) done. Loss: 0.0159  lr:0.000001
[ Sat Jul 13 06:16:54 2024 ] 	Batch(6400/6809) done. Loss: 0.0544  lr:0.000001
[ Sat Jul 13 06:17:12 2024 ] 
Training: Epoch [94/120], Step [6499], Loss: 0.14978598058223724, Training Accuracy: 97.59038461538462
[ Sat Jul 13 06:17:12 2024 ] 	Batch(6500/6809) done. Loss: 0.0355  lr:0.000001
[ Sat Jul 13 06:17:30 2024 ] 	Batch(6600/6809) done. Loss: 0.1523  lr:0.000001
[ Sat Jul 13 06:17:48 2024 ] 	Batch(6700/6809) done. Loss: 0.0479  lr:0.000001
[ Sat Jul 13 06:18:06 2024 ] 	Batch(6800/6809) done. Loss: 0.0390  lr:0.000001
[ Sat Jul 13 06:18:07 2024 ] 	Mean training loss: 0.0955.
[ Sat Jul 13 06:18:07 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 06:18:08 2024 ] Training epoch: 96
[ Sat Jul 13 06:18:08 2024 ] 	Batch(0/6809) done. Loss: 0.3489  lr:0.000001
[ Sat Jul 13 06:18:26 2024 ] 	Batch(100/6809) done. Loss: 0.2094  lr:0.000001
[ Sat Jul 13 06:18:44 2024 ] 	Batch(200/6809) done. Loss: 0.0283  lr:0.000001
[ Sat Jul 13 06:19:02 2024 ] 	Batch(300/6809) done. Loss: 0.7909  lr:0.000001
[ Sat Jul 13 06:19:20 2024 ] 	Batch(400/6809) done. Loss: 0.0215  lr:0.000001
[ Sat Jul 13 06:19:38 2024 ] 
Training: Epoch [95/120], Step [499], Loss: 0.16471774876117706, Training Accuracy: 97.8
[ Sat Jul 13 06:19:38 2024 ] 	Batch(500/6809) done. Loss: 0.0927  lr:0.000001
[ Sat Jul 13 06:19:56 2024 ] 	Batch(600/6809) done. Loss: 0.1103  lr:0.000001
[ Sat Jul 13 06:20:13 2024 ] 	Batch(700/6809) done. Loss: 0.2282  lr:0.000001
[ Sat Jul 13 06:20:31 2024 ] 	Batch(800/6809) done. Loss: 0.2728  lr:0.000001
[ Sat Jul 13 06:20:49 2024 ] 	Batch(900/6809) done. Loss: 0.0276  lr:0.000001
[ Sat Jul 13 06:21:07 2024 ] 
Training: Epoch [95/120], Step [999], Loss: 0.19175170361995697, Training Accuracy: 97.6
[ Sat Jul 13 06:21:07 2024 ] 	Batch(1000/6809) done. Loss: 0.1357  lr:0.000001
[ Sat Jul 13 06:21:25 2024 ] 	Batch(1100/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 06:21:43 2024 ] 	Batch(1200/6809) done. Loss: 0.1164  lr:0.000001
[ Sat Jul 13 06:22:01 2024 ] 	Batch(1300/6809) done. Loss: 0.0557  lr:0.000001
[ Sat Jul 13 06:22:19 2024 ] 	Batch(1400/6809) done. Loss: 0.2250  lr:0.000001
[ Sat Jul 13 06:22:37 2024 ] 
Training: Epoch [95/120], Step [1499], Loss: 0.06174376979470253, Training Accuracy: 97.625
[ Sat Jul 13 06:22:37 2024 ] 	Batch(1500/6809) done. Loss: 0.0258  lr:0.000001
[ Sat Jul 13 06:22:55 2024 ] 	Batch(1600/6809) done. Loss: 0.2686  lr:0.000001
[ Sat Jul 13 06:23:13 2024 ] 	Batch(1700/6809) done. Loss: 0.0233  lr:0.000001
[ Sat Jul 13 06:23:31 2024 ] 	Batch(1800/6809) done. Loss: 0.0018  lr:0.000001
[ Sat Jul 13 06:23:48 2024 ] 	Batch(1900/6809) done. Loss: 0.1977  lr:0.000001
[ Sat Jul 13 06:24:06 2024 ] 
Training: Epoch [95/120], Step [1999], Loss: 0.07816464453935623, Training Accuracy: 97.49375
[ Sat Jul 13 06:24:07 2024 ] 	Batch(2000/6809) done. Loss: 0.1767  lr:0.000001
[ Sat Jul 13 06:24:24 2024 ] 	Batch(2100/6809) done. Loss: 0.0601  lr:0.000001
[ Sat Jul 13 06:24:42 2024 ] 	Batch(2200/6809) done. Loss: 0.0141  lr:0.000001
[ Sat Jul 13 06:25:00 2024 ] 	Batch(2300/6809) done. Loss: 0.0031  lr:0.000001
[ Sat Jul 13 06:25:18 2024 ] 	Batch(2400/6809) done. Loss: 0.0535  lr:0.000001
[ Sat Jul 13 06:25:36 2024 ] 
Training: Epoch [95/120], Step [2499], Loss: 0.07176023721694946, Training Accuracy: 97.49
[ Sat Jul 13 06:25:36 2024 ] 	Batch(2500/6809) done. Loss: 0.0176  lr:0.000001
[ Sat Jul 13 06:25:54 2024 ] 	Batch(2600/6809) done. Loss: 0.0121  lr:0.000001
[ Sat Jul 13 06:26:12 2024 ] 	Batch(2700/6809) done. Loss: 0.5082  lr:0.000001
[ Sat Jul 13 06:26:30 2024 ] 	Batch(2800/6809) done. Loss: 0.1196  lr:0.000001
[ Sat Jul 13 06:26:48 2024 ] 	Batch(2900/6809) done. Loss: 0.0233  lr:0.000001
[ Sat Jul 13 06:27:06 2024 ] 
Training: Epoch [95/120], Step [2999], Loss: 0.330557644367218, Training Accuracy: 97.48750000000001
[ Sat Jul 13 06:27:06 2024 ] 	Batch(3000/6809) done. Loss: 0.4338  lr:0.000001
[ Sat Jul 13 06:27:24 2024 ] 	Batch(3100/6809) done. Loss: 0.1258  lr:0.000001
[ Sat Jul 13 06:27:42 2024 ] 	Batch(3200/6809) done. Loss: 0.1330  lr:0.000001
[ Sat Jul 13 06:28:00 2024 ] 	Batch(3300/6809) done. Loss: 0.2035  lr:0.000001
[ Sat Jul 13 06:28:19 2024 ] 	Batch(3400/6809) done. Loss: 0.0167  lr:0.000001
[ Sat Jul 13 06:28:37 2024 ] 
Training: Epoch [95/120], Step [3499], Loss: 0.041201092302799225, Training Accuracy: 97.45357142857142
[ Sat Jul 13 06:28:37 2024 ] 	Batch(3500/6809) done. Loss: 0.4225  lr:0.000001
[ Sat Jul 13 06:28:55 2024 ] 	Batch(3600/6809) done. Loss: 0.0864  lr:0.000001
[ Sat Jul 13 06:29:14 2024 ] 	Batch(3700/6809) done. Loss: 0.0114  lr:0.000001
[ Sat Jul 13 06:29:32 2024 ] 	Batch(3800/6809) done. Loss: 0.0061  lr:0.000001
[ Sat Jul 13 06:29:50 2024 ] 	Batch(3900/6809) done. Loss: 0.0654  lr:0.000001
[ Sat Jul 13 06:30:08 2024 ] 
Training: Epoch [95/120], Step [3999], Loss: 0.0027915718965232372, Training Accuracy: 97.453125
[ Sat Jul 13 06:30:08 2024 ] 	Batch(4000/6809) done. Loss: 0.0444  lr:0.000001
[ Sat Jul 13 06:30:27 2024 ] 	Batch(4100/6809) done. Loss: 0.1413  lr:0.000001
[ Sat Jul 13 06:30:45 2024 ] 	Batch(4200/6809) done. Loss: 0.1190  lr:0.000001
[ Sat Jul 13 06:31:04 2024 ] 	Batch(4300/6809) done. Loss: 0.0615  lr:0.000001
[ Sat Jul 13 06:31:22 2024 ] 	Batch(4400/6809) done. Loss: 0.0207  lr:0.000001
[ Sat Jul 13 06:31:41 2024 ] 
Training: Epoch [95/120], Step [4499], Loss: 0.046527665108442307, Training Accuracy: 97.44444444444444
[ Sat Jul 13 06:31:41 2024 ] 	Batch(4500/6809) done. Loss: 0.0209  lr:0.000001
[ Sat Jul 13 06:31:59 2024 ] 	Batch(4600/6809) done. Loss: 0.0191  lr:0.000001
[ Sat Jul 13 06:32:18 2024 ] 	Batch(4700/6809) done. Loss: 0.1320  lr:0.000001
[ Sat Jul 13 06:32:37 2024 ] 	Batch(4800/6809) done. Loss: 0.1520  lr:0.000001
[ Sat Jul 13 06:32:54 2024 ] 	Batch(4900/6809) done. Loss: 0.1015  lr:0.000001
[ Sat Jul 13 06:33:12 2024 ] 
Training: Epoch [95/120], Step [4999], Loss: 0.15982657670974731, Training Accuracy: 97.47749999999999
[ Sat Jul 13 06:33:12 2024 ] 	Batch(5000/6809) done. Loss: 0.1106  lr:0.000001
[ Sat Jul 13 06:33:30 2024 ] 	Batch(5100/6809) done. Loss: 0.0080  lr:0.000001
[ Sat Jul 13 06:33:48 2024 ] 	Batch(5200/6809) done. Loss: 0.1984  lr:0.000001
[ Sat Jul 13 06:34:06 2024 ] 	Batch(5300/6809) done. Loss: 0.0114  lr:0.000001
[ Sat Jul 13 06:34:24 2024 ] 	Batch(5400/6809) done. Loss: 0.0260  lr:0.000001
[ Sat Jul 13 06:34:42 2024 ] 
Training: Epoch [95/120], Step [5499], Loss: 0.12750981748104095, Training Accuracy: 97.49545454545454
[ Sat Jul 13 06:34:42 2024 ] 	Batch(5500/6809) done. Loss: 0.0764  lr:0.000001
[ Sat Jul 13 06:35:00 2024 ] 	Batch(5600/6809) done. Loss: 0.0436  lr:0.000001
[ Sat Jul 13 06:35:18 2024 ] 	Batch(5700/6809) done. Loss: 0.0585  lr:0.000001
[ Sat Jul 13 06:35:37 2024 ] 	Batch(5800/6809) done. Loss: 0.1319  lr:0.000001
[ Sat Jul 13 06:35:56 2024 ] 	Batch(5900/6809) done. Loss: 0.0208  lr:0.000001
[ Sat Jul 13 06:36:14 2024 ] 
Training: Epoch [95/120], Step [5999], Loss: 0.03856074437499046, Training Accuracy: 97.51666666666667
[ Sat Jul 13 06:36:14 2024 ] 	Batch(6000/6809) done. Loss: 0.2951  lr:0.000001
[ Sat Jul 13 06:36:33 2024 ] 	Batch(6100/6809) done. Loss: 0.0655  lr:0.000001
[ Sat Jul 13 06:36:51 2024 ] 	Batch(6200/6809) done. Loss: 0.5399  lr:0.000001
[ Sat Jul 13 06:37:10 2024 ] 	Batch(6300/6809) done. Loss: 0.0166  lr:0.000001
[ Sat Jul 13 06:37:28 2024 ] 	Batch(6400/6809) done. Loss: 0.0010  lr:0.000001
[ Sat Jul 13 06:37:47 2024 ] 
Training: Epoch [95/120], Step [6499], Loss: 0.11046510189771652, Training Accuracy: 97.52499999999999
[ Sat Jul 13 06:37:47 2024 ] 	Batch(6500/6809) done. Loss: 0.0238  lr:0.000001
[ Sat Jul 13 06:38:05 2024 ] 	Batch(6600/6809) done. Loss: 0.1496  lr:0.000001
[ Sat Jul 13 06:38:24 2024 ] 	Batch(6700/6809) done. Loss: 0.0608  lr:0.000001
[ Sat Jul 13 06:38:42 2024 ] 	Batch(6800/6809) done. Loss: 0.0879  lr:0.000001
[ Sat Jul 13 06:38:44 2024 ] 	Mean training loss: 0.0972.
[ Sat Jul 13 06:38:44 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 06:38:44 2024 ] Training epoch: 97
[ Sat Jul 13 06:38:45 2024 ] 	Batch(0/6809) done. Loss: 0.0223  lr:0.000001
[ Sat Jul 13 06:39:03 2024 ] 	Batch(100/6809) done. Loss: 0.0897  lr:0.000001
[ Sat Jul 13 06:39:21 2024 ] 	Batch(200/6809) done. Loss: 0.2799  lr:0.000001
[ Sat Jul 13 06:39:39 2024 ] 	Batch(300/6809) done. Loss: 0.0951  lr:0.000001
[ Sat Jul 13 06:39:57 2024 ] 	Batch(400/6809) done. Loss: 0.1165  lr:0.000001
[ Sat Jul 13 06:40:15 2024 ] 
Training: Epoch [96/120], Step [499], Loss: 0.0006210882565937936, Training Accuracy: 97.725
[ Sat Jul 13 06:40:15 2024 ] 	Batch(500/6809) done. Loss: 0.0771  lr:0.000001
[ Sat Jul 13 06:40:33 2024 ] 	Batch(600/6809) done. Loss: 0.3340  lr:0.000001
[ Sat Jul 13 06:40:51 2024 ] 	Batch(700/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 06:41:09 2024 ] 	Batch(800/6809) done. Loss: 0.0426  lr:0.000001
[ Sat Jul 13 06:41:27 2024 ] 	Batch(900/6809) done. Loss: 0.0481  lr:0.000001
[ Sat Jul 13 06:41:46 2024 ] 
Training: Epoch [96/120], Step [999], Loss: 0.0902007669210434, Training Accuracy: 97.75
[ Sat Jul 13 06:41:46 2024 ] 	Batch(1000/6809) done. Loss: 0.2423  lr:0.000001
[ Sat Jul 13 06:42:04 2024 ] 	Batch(1100/6809) done. Loss: 0.0487  lr:0.000001
[ Sat Jul 13 06:42:23 2024 ] 	Batch(1200/6809) done. Loss: 0.0700  lr:0.000001
[ Sat Jul 13 06:42:41 2024 ] 	Batch(1300/6809) done. Loss: 0.0528  lr:0.000001
[ Sat Jul 13 06:42:59 2024 ] 	Batch(1400/6809) done. Loss: 0.0708  lr:0.000001
[ Sat Jul 13 06:43:17 2024 ] 
Training: Epoch [96/120], Step [1499], Loss: 0.14012297987937927, Training Accuracy: 97.775
[ Sat Jul 13 06:43:17 2024 ] 	Batch(1500/6809) done. Loss: 0.0710  lr:0.000001
[ Sat Jul 13 06:43:35 2024 ] 	Batch(1600/6809) done. Loss: 0.2270  lr:0.000001
[ Sat Jul 13 06:43:53 2024 ] 	Batch(1700/6809) done. Loss: 0.0117  lr:0.000001
[ Sat Jul 13 06:44:12 2024 ] 	Batch(1800/6809) done. Loss: 0.0380  lr:0.000001
[ Sat Jul 13 06:44:30 2024 ] 	Batch(1900/6809) done. Loss: 0.0669  lr:0.000001
[ Sat Jul 13 06:44:49 2024 ] 
Training: Epoch [96/120], Step [1999], Loss: 0.0035961396060884, Training Accuracy: 97.73125
[ Sat Jul 13 06:44:49 2024 ] 	Batch(2000/6809) done. Loss: 0.0311  lr:0.000001
[ Sat Jul 13 06:45:07 2024 ] 	Batch(2100/6809) done. Loss: 0.0295  lr:0.000001
[ Sat Jul 13 06:45:25 2024 ] 	Batch(2200/6809) done. Loss: 0.0223  lr:0.000001
[ Sat Jul 13 06:45:43 2024 ] 	Batch(2300/6809) done. Loss: 0.1467  lr:0.000001
[ Sat Jul 13 06:46:01 2024 ] 	Batch(2400/6809) done. Loss: 0.0748  lr:0.000001
[ Sat Jul 13 06:46:19 2024 ] 
Training: Epoch [96/120], Step [2499], Loss: 0.01454134751111269, Training Accuracy: 97.68
[ Sat Jul 13 06:46:19 2024 ] 	Batch(2500/6809) done. Loss: 0.3163  lr:0.000001
[ Sat Jul 13 06:46:37 2024 ] 	Batch(2600/6809) done. Loss: 0.0460  lr:0.000001
[ Sat Jul 13 06:46:55 2024 ] 	Batch(2700/6809) done. Loss: 0.0605  lr:0.000001
[ Sat Jul 13 06:47:13 2024 ] 	Batch(2800/6809) done. Loss: 0.0118  lr:0.000001
[ Sat Jul 13 06:47:30 2024 ] 	Batch(2900/6809) done. Loss: 0.0670  lr:0.000001
[ Sat Jul 13 06:47:48 2024 ] 
Training: Epoch [96/120], Step [2999], Loss: 0.027079708874225616, Training Accuracy: 97.64583333333333
[ Sat Jul 13 06:47:48 2024 ] 	Batch(3000/6809) done. Loss: 0.2706  lr:0.000001
[ Sat Jul 13 06:48:07 2024 ] 	Batch(3100/6809) done. Loss: 0.1011  lr:0.000001
[ Sat Jul 13 06:48:25 2024 ] 	Batch(3200/6809) done. Loss: 0.0630  lr:0.000001
[ Sat Jul 13 06:48:43 2024 ] 	Batch(3300/6809) done. Loss: 0.0076  lr:0.000001
[ Sat Jul 13 06:49:02 2024 ] 	Batch(3400/6809) done. Loss: 0.3077  lr:0.000001
[ Sat Jul 13 06:49:20 2024 ] 
Training: Epoch [96/120], Step [3499], Loss: 0.05357637256383896, Training Accuracy: 97.61785714285715
[ Sat Jul 13 06:49:20 2024 ] 	Batch(3500/6809) done. Loss: 0.2001  lr:0.000001
[ Sat Jul 13 06:49:39 2024 ] 	Batch(3600/6809) done. Loss: 0.0378  lr:0.000001
[ Sat Jul 13 06:49:57 2024 ] 	Batch(3700/6809) done. Loss: 0.7787  lr:0.000001
[ Sat Jul 13 06:50:15 2024 ] 	Batch(3800/6809) done. Loss: 0.0035  lr:0.000001
[ Sat Jul 13 06:50:33 2024 ] 	Batch(3900/6809) done. Loss: 0.0279  lr:0.000001
[ Sat Jul 13 06:50:50 2024 ] 
Training: Epoch [96/120], Step [3999], Loss: 0.03968743979930878, Training Accuracy: 97.63437499999999
[ Sat Jul 13 06:50:51 2024 ] 	Batch(4000/6809) done. Loss: 0.0128  lr:0.000001
[ Sat Jul 13 06:51:09 2024 ] 	Batch(4100/6809) done. Loss: 0.0286  lr:0.000001
[ Sat Jul 13 06:51:27 2024 ] 	Batch(4200/6809) done. Loss: 0.2382  lr:0.000001
[ Sat Jul 13 06:51:44 2024 ] 	Batch(4300/6809) done. Loss: 0.4078  lr:0.000001
[ Sat Jul 13 06:52:02 2024 ] 	Batch(4400/6809) done. Loss: 0.0360  lr:0.000001
[ Sat Jul 13 06:52:20 2024 ] 
Training: Epoch [96/120], Step [4499], Loss: 0.04333055019378662, Training Accuracy: 97.64166666666667
[ Sat Jul 13 06:52:20 2024 ] 	Batch(4500/6809) done. Loss: 0.0703  lr:0.000001
[ Sat Jul 13 06:52:38 2024 ] 	Batch(4600/6809) done. Loss: 0.0749  lr:0.000001
[ Sat Jul 13 06:52:56 2024 ] 	Batch(4700/6809) done. Loss: 0.0027  lr:0.000001
[ Sat Jul 13 06:53:14 2024 ] 	Batch(4800/6809) done. Loss: 0.0255  lr:0.000001
[ Sat Jul 13 06:53:32 2024 ] 	Batch(4900/6809) done. Loss: 0.0491  lr:0.000001
[ Sat Jul 13 06:53:50 2024 ] 
Training: Epoch [96/120], Step [4999], Loss: 0.4371984601020813, Training Accuracy: 97.6025
[ Sat Jul 13 06:53:50 2024 ] 	Batch(5000/6809) done. Loss: 0.0150  lr:0.000001
[ Sat Jul 13 06:54:08 2024 ] 	Batch(5100/6809) done. Loss: 0.0818  lr:0.000001
[ Sat Jul 13 06:54:26 2024 ] 	Batch(5200/6809) done. Loss: 0.0687  lr:0.000001
[ Sat Jul 13 06:54:44 2024 ] 	Batch(5300/6809) done. Loss: 0.0072  lr:0.000001
[ Sat Jul 13 06:55:02 2024 ] 	Batch(5400/6809) done. Loss: 0.1025  lr:0.000001
[ Sat Jul 13 06:55:19 2024 ] 
Training: Epoch [96/120], Step [5499], Loss: 0.24818137288093567, Training Accuracy: 97.62954545454545
[ Sat Jul 13 06:55:19 2024 ] 	Batch(5500/6809) done. Loss: 0.0255  lr:0.000001
[ Sat Jul 13 06:55:37 2024 ] 	Batch(5600/6809) done. Loss: 0.0030  lr:0.000001
[ Sat Jul 13 06:55:56 2024 ] 	Batch(5700/6809) done. Loss: 0.0504  lr:0.000001
[ Sat Jul 13 06:56:14 2024 ] 	Batch(5800/6809) done. Loss: 0.1726  lr:0.000001
[ Sat Jul 13 06:56:33 2024 ] 	Batch(5900/6809) done. Loss: 0.5546  lr:0.000001
[ Sat Jul 13 06:56:51 2024 ] 
Training: Epoch [96/120], Step [5999], Loss: 0.05360300838947296, Training Accuracy: 97.67083333333333
[ Sat Jul 13 06:56:52 2024 ] 	Batch(6000/6809) done. Loss: 0.0837  lr:0.000001
[ Sat Jul 13 06:57:10 2024 ] 	Batch(6100/6809) done. Loss: 0.0019  lr:0.000001
[ Sat Jul 13 06:57:27 2024 ] 	Batch(6200/6809) done. Loss: 0.1907  lr:0.000001
[ Sat Jul 13 06:57:45 2024 ] 	Batch(6300/6809) done. Loss: 0.1409  lr:0.000001
[ Sat Jul 13 06:58:03 2024 ] 	Batch(6400/6809) done. Loss: 0.0532  lr:0.000001
[ Sat Jul 13 06:58:21 2024 ] 
Training: Epoch [96/120], Step [6499], Loss: 0.02625899948179722, Training Accuracy: 97.65192307692307
[ Sat Jul 13 06:58:21 2024 ] 	Batch(6500/6809) done. Loss: 0.0492  lr:0.000001
[ Sat Jul 13 06:58:39 2024 ] 	Batch(6600/6809) done. Loss: 0.0338  lr:0.000001
[ Sat Jul 13 06:58:57 2024 ] 	Batch(6700/6809) done. Loss: 0.0334  lr:0.000001
[ Sat Jul 13 06:59:15 2024 ] 	Batch(6800/6809) done. Loss: 0.2241  lr:0.000001
[ Sat Jul 13 06:59:16 2024 ] 	Mean training loss: 0.0949.
[ Sat Jul 13 06:59:16 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 06:59:17 2024 ] Training epoch: 98
[ Sat Jul 13 06:59:17 2024 ] 	Batch(0/6809) done. Loss: 0.1209  lr:0.000001
[ Sat Jul 13 06:59:35 2024 ] 	Batch(100/6809) done. Loss: 0.0086  lr:0.000001
[ Sat Jul 13 06:59:54 2024 ] 	Batch(200/6809) done. Loss: 0.0243  lr:0.000001
[ Sat Jul 13 07:00:12 2024 ] 	Batch(300/6809) done. Loss: 0.0538  lr:0.000001
[ Sat Jul 13 07:00:31 2024 ] 	Batch(400/6809) done. Loss: 0.0277  lr:0.000001
[ Sat Jul 13 07:00:49 2024 ] 
Training: Epoch [97/120], Step [499], Loss: 0.0061724078841507435, Training Accuracy: 97.475
[ Sat Jul 13 07:00:49 2024 ] 	Batch(500/6809) done. Loss: 0.0553  lr:0.000001
[ Sat Jul 13 07:01:07 2024 ] 	Batch(600/6809) done. Loss: 0.0929  lr:0.000001
[ Sat Jul 13 07:01:25 2024 ] 	Batch(700/6809) done. Loss: 0.0641  lr:0.000001
[ Sat Jul 13 07:01:43 2024 ] 	Batch(800/6809) done. Loss: 0.2829  lr:0.000001
[ Sat Jul 13 07:02:01 2024 ] 	Batch(900/6809) done. Loss: 0.0637  lr:0.000001
[ Sat Jul 13 07:02:19 2024 ] 
Training: Epoch [97/120], Step [999], Loss: 0.07720817625522614, Training Accuracy: 97.55
[ Sat Jul 13 07:02:19 2024 ] 	Batch(1000/6809) done. Loss: 0.1492  lr:0.000001
[ Sat Jul 13 07:02:37 2024 ] 	Batch(1100/6809) done. Loss: 0.0047  lr:0.000001
[ Sat Jul 13 07:02:55 2024 ] 	Batch(1200/6809) done. Loss: 0.0108  lr:0.000001
[ Sat Jul 13 07:03:14 2024 ] 	Batch(1300/6809) done. Loss: 0.0785  lr:0.000001
[ Sat Jul 13 07:03:32 2024 ] 	Batch(1400/6809) done. Loss: 0.0662  lr:0.000001
[ Sat Jul 13 07:03:51 2024 ] 
Training: Epoch [97/120], Step [1499], Loss: 0.007930106483399868, Training Accuracy: 97.51666666666667
[ Sat Jul 13 07:03:51 2024 ] 	Batch(1500/6809) done. Loss: 0.1940  lr:0.000001
[ Sat Jul 13 07:04:09 2024 ] 	Batch(1600/6809) done. Loss: 0.0259  lr:0.000001
[ Sat Jul 13 07:04:27 2024 ] 	Batch(1700/6809) done. Loss: 0.0230  lr:0.000001
[ Sat Jul 13 07:04:45 2024 ] 	Batch(1800/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 07:05:03 2024 ] 	Batch(1900/6809) done. Loss: 0.1511  lr:0.000001
[ Sat Jul 13 07:05:20 2024 ] 
Training: Epoch [97/120], Step [1999], Loss: 0.009686565957963467, Training Accuracy: 97.55625
[ Sat Jul 13 07:05:21 2024 ] 	Batch(2000/6809) done. Loss: 0.0020  lr:0.000001
[ Sat Jul 13 07:05:39 2024 ] 	Batch(2100/6809) done. Loss: 0.0873  lr:0.000001
[ Sat Jul 13 07:05:56 2024 ] 	Batch(2200/6809) done. Loss: 0.0825  lr:0.000001
[ Sat Jul 13 07:06:14 2024 ] 	Batch(2300/6809) done. Loss: 0.0180  lr:0.000001
[ Sat Jul 13 07:06:33 2024 ] 	Batch(2400/6809) done. Loss: 0.0287  lr:0.000001
[ Sat Jul 13 07:06:50 2024 ] 
Training: Epoch [97/120], Step [2499], Loss: 0.017403028905391693, Training Accuracy: 97.57000000000001
[ Sat Jul 13 07:06:50 2024 ] 	Batch(2500/6809) done. Loss: 0.0316  lr:0.000001
[ Sat Jul 13 07:07:08 2024 ] 	Batch(2600/6809) done. Loss: 0.1952  lr:0.000001
[ Sat Jul 13 07:07:26 2024 ] 	Batch(2700/6809) done. Loss: 0.0440  lr:0.000001
[ Sat Jul 13 07:07:44 2024 ] 	Batch(2800/6809) done. Loss: 0.3329  lr:0.000001
[ Sat Jul 13 07:08:02 2024 ] 	Batch(2900/6809) done. Loss: 0.0165  lr:0.000001
[ Sat Jul 13 07:08:20 2024 ] 
Training: Epoch [97/120], Step [2999], Loss: 0.06890963017940521, Training Accuracy: 97.60833333333333
[ Sat Jul 13 07:08:20 2024 ] 	Batch(3000/6809) done. Loss: 0.0384  lr:0.000001
[ Sat Jul 13 07:08:38 2024 ] 	Batch(3100/6809) done. Loss: 0.1751  lr:0.000001
[ Sat Jul 13 07:08:56 2024 ] 	Batch(3200/6809) done. Loss: 0.0586  lr:0.000001
[ Sat Jul 13 07:09:14 2024 ] 	Batch(3300/6809) done. Loss: 0.0096  lr:0.000001
[ Sat Jul 13 07:09:33 2024 ] 	Batch(3400/6809) done. Loss: 0.0729  lr:0.000001
[ Sat Jul 13 07:09:51 2024 ] 
Training: Epoch [97/120], Step [3499], Loss: 0.01977257803082466, Training Accuracy: 97.60714285714286
[ Sat Jul 13 07:09:52 2024 ] 	Batch(3500/6809) done. Loss: 0.0936  lr:0.000001
[ Sat Jul 13 07:10:10 2024 ] 	Batch(3600/6809) done. Loss: 0.0047  lr:0.000001
[ Sat Jul 13 07:10:29 2024 ] 	Batch(3700/6809) done. Loss: 0.0706  lr:0.000001
[ Sat Jul 13 07:10:47 2024 ] 	Batch(3800/6809) done. Loss: 0.0779  lr:0.000001
[ Sat Jul 13 07:11:06 2024 ] 	Batch(3900/6809) done. Loss: 0.1992  lr:0.000001
[ Sat Jul 13 07:11:24 2024 ] 
Training: Epoch [97/120], Step [3999], Loss: 0.0330151729285717, Training Accuracy: 97.565625
[ Sat Jul 13 07:11:24 2024 ] 	Batch(4000/6809) done. Loss: 0.0136  lr:0.000001
[ Sat Jul 13 07:11:43 2024 ] 	Batch(4100/6809) done. Loss: 0.2544  lr:0.000001
[ Sat Jul 13 07:12:01 2024 ] 	Batch(4200/6809) done. Loss: 0.0609  lr:0.000001
[ Sat Jul 13 07:12:20 2024 ] 	Batch(4300/6809) done. Loss: 0.1253  lr:0.000001
[ Sat Jul 13 07:12:39 2024 ] 	Batch(4400/6809) done. Loss: 0.0915  lr:0.000001
[ Sat Jul 13 07:12:57 2024 ] 
Training: Epoch [97/120], Step [4499], Loss: 0.03189995884895325, Training Accuracy: 97.58333333333333
[ Sat Jul 13 07:12:57 2024 ] 	Batch(4500/6809) done. Loss: 0.2093  lr:0.000001
[ Sat Jul 13 07:13:15 2024 ] 	Batch(4600/6809) done. Loss: 0.0710  lr:0.000001
[ Sat Jul 13 07:13:33 2024 ] 	Batch(4700/6809) done. Loss: 0.1130  lr:0.000001
[ Sat Jul 13 07:13:51 2024 ] 	Batch(4800/6809) done. Loss: 0.1304  lr:0.000001
[ Sat Jul 13 07:14:09 2024 ] 	Batch(4900/6809) done. Loss: 0.0613  lr:0.000001
[ Sat Jul 13 07:14:26 2024 ] 
Training: Epoch [97/120], Step [4999], Loss: 0.28145110607147217, Training Accuracy: 97.565
[ Sat Jul 13 07:14:27 2024 ] 	Batch(5000/6809) done. Loss: 0.0792  lr:0.000001
[ Sat Jul 13 07:14:45 2024 ] 	Batch(5100/6809) done. Loss: 0.2341  lr:0.000001
[ Sat Jul 13 07:15:03 2024 ] 	Batch(5200/6809) done. Loss: 0.0654  lr:0.000001
[ Sat Jul 13 07:15:21 2024 ] 	Batch(5300/6809) done. Loss: 0.0441  lr:0.000001
[ Sat Jul 13 07:15:39 2024 ] 	Batch(5400/6809) done. Loss: 0.0423  lr:0.000001
[ Sat Jul 13 07:15:57 2024 ] 
Training: Epoch [97/120], Step [5499], Loss: 0.02110426127910614, Training Accuracy: 97.55227272727272
[ Sat Jul 13 07:15:57 2024 ] 	Batch(5500/6809) done. Loss: 0.0487  lr:0.000001
[ Sat Jul 13 07:16:15 2024 ] 	Batch(5600/6809) done. Loss: 0.0491  lr:0.000001
[ Sat Jul 13 07:16:33 2024 ] 	Batch(5700/6809) done. Loss: 0.0637  lr:0.000001
[ Sat Jul 13 07:16:51 2024 ] 	Batch(5800/6809) done. Loss: 0.0365  lr:0.000001
[ Sat Jul 13 07:17:09 2024 ] 	Batch(5900/6809) done. Loss: 0.0053  lr:0.000001
[ Sat Jul 13 07:17:27 2024 ] 
Training: Epoch [97/120], Step [5999], Loss: 0.1663631647825241, Training Accuracy: 97.53333333333333
[ Sat Jul 13 07:17:27 2024 ] 	Batch(6000/6809) done. Loss: 0.0973  lr:0.000001
[ Sat Jul 13 07:17:45 2024 ] 	Batch(6100/6809) done. Loss: 0.1239  lr:0.000001
[ Sat Jul 13 07:18:03 2024 ] 	Batch(6200/6809) done. Loss: 0.0085  lr:0.000001
[ Sat Jul 13 07:18:20 2024 ] 	Batch(6300/6809) done. Loss: 0.0182  lr:0.000001
[ Sat Jul 13 07:18:39 2024 ] 	Batch(6400/6809) done. Loss: 0.0327  lr:0.000001
[ Sat Jul 13 07:18:57 2024 ] 
Training: Epoch [97/120], Step [6499], Loss: 0.053228121250867844, Training Accuracy: 97.55
[ Sat Jul 13 07:18:57 2024 ] 	Batch(6500/6809) done. Loss: 0.0505  lr:0.000001
[ Sat Jul 13 07:19:16 2024 ] 	Batch(6600/6809) done. Loss: 0.0697  lr:0.000001
[ Sat Jul 13 07:19:34 2024 ] 	Batch(6700/6809) done. Loss: 0.0346  lr:0.000001
[ Sat Jul 13 07:19:53 2024 ] 	Batch(6800/6809) done. Loss: 0.2939  lr:0.000001
[ Sat Jul 13 07:19:54 2024 ] 	Mean training loss: 0.0963.
[ Sat Jul 13 07:19:54 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 07:19:54 2024 ] Training epoch: 99
[ Sat Jul 13 07:19:55 2024 ] 	Batch(0/6809) done. Loss: 0.0591  lr:0.000001
[ Sat Jul 13 07:20:13 2024 ] 	Batch(100/6809) done. Loss: 0.0112  lr:0.000001
[ Sat Jul 13 07:20:32 2024 ] 	Batch(200/6809) done. Loss: 0.0916  lr:0.000001
[ Sat Jul 13 07:20:50 2024 ] 	Batch(300/6809) done. Loss: 0.1760  lr:0.000001
[ Sat Jul 13 07:21:08 2024 ] 	Batch(400/6809) done. Loss: 0.1048  lr:0.000001
[ Sat Jul 13 07:21:27 2024 ] 
Training: Epoch [98/120], Step [499], Loss: 0.008278314024209976, Training Accuracy: 97.65
[ Sat Jul 13 07:21:27 2024 ] 	Batch(500/6809) done. Loss: 0.0569  lr:0.000001
[ Sat Jul 13 07:21:45 2024 ] 	Batch(600/6809) done. Loss: 0.0286  lr:0.000001
[ Sat Jul 13 07:22:03 2024 ] 	Batch(700/6809) done. Loss: 0.0498  lr:0.000001
[ Sat Jul 13 07:22:22 2024 ] 	Batch(800/6809) done. Loss: 0.3041  lr:0.000001
[ Sat Jul 13 07:22:40 2024 ] 	Batch(900/6809) done. Loss: 0.0062  lr:0.000001
[ Sat Jul 13 07:22:58 2024 ] 
Training: Epoch [98/120], Step [999], Loss: 0.03384826332330704, Training Accuracy: 97.5
[ Sat Jul 13 07:22:58 2024 ] 	Batch(1000/6809) done. Loss: 0.4897  lr:0.000001
[ Sat Jul 13 07:23:16 2024 ] 	Batch(1100/6809) done. Loss: 0.2158  lr:0.000001
[ Sat Jul 13 07:23:34 2024 ] 	Batch(1200/6809) done. Loss: 0.0055  lr:0.000001
[ Sat Jul 13 07:23:52 2024 ] 	Batch(1300/6809) done. Loss: 0.0081  lr:0.000001
[ Sat Jul 13 07:24:10 2024 ] 	Batch(1400/6809) done. Loss: 0.1829  lr:0.000001
[ Sat Jul 13 07:24:27 2024 ] 
Training: Epoch [98/120], Step [1499], Loss: 0.04864213615655899, Training Accuracy: 97.425
[ Sat Jul 13 07:24:27 2024 ] 	Batch(1500/6809) done. Loss: 0.0089  lr:0.000001
[ Sat Jul 13 07:24:46 2024 ] 	Batch(1600/6809) done. Loss: 0.0220  lr:0.000001
[ Sat Jul 13 07:25:03 2024 ] 	Batch(1700/6809) done. Loss: 0.1235  lr:0.000001
[ Sat Jul 13 07:25:21 2024 ] 	Batch(1800/6809) done. Loss: 0.2334  lr:0.000001
[ Sat Jul 13 07:25:39 2024 ] 	Batch(1900/6809) done. Loss: 0.2318  lr:0.000001
[ Sat Jul 13 07:25:57 2024 ] 
Training: Epoch [98/120], Step [1999], Loss: 0.07613438367843628, Training Accuracy: 97.46249999999999
[ Sat Jul 13 07:25:57 2024 ] 	Batch(2000/6809) done. Loss: 0.1593  lr:0.000001
[ Sat Jul 13 07:26:15 2024 ] 	Batch(2100/6809) done. Loss: 0.0451  lr:0.000001
[ Sat Jul 13 07:26:33 2024 ] 	Batch(2200/6809) done. Loss: 0.0838  lr:0.000001
[ Sat Jul 13 07:26:51 2024 ] 	Batch(2300/6809) done. Loss: 0.0677  lr:0.000001
[ Sat Jul 13 07:27:09 2024 ] 	Batch(2400/6809) done. Loss: 0.0067  lr:0.000001
[ Sat Jul 13 07:27:27 2024 ] 
Training: Epoch [98/120], Step [2499], Loss: 0.08338545262813568, Training Accuracy: 97.485
[ Sat Jul 13 07:27:27 2024 ] 	Batch(2500/6809) done. Loss: 0.1939  lr:0.000001
[ Sat Jul 13 07:27:45 2024 ] 	Batch(2600/6809) done. Loss: 0.0208  lr:0.000001
[ Sat Jul 13 07:28:03 2024 ] 	Batch(2700/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 07:28:21 2024 ] 	Batch(2800/6809) done. Loss: 0.0849  lr:0.000001
[ Sat Jul 13 07:28:40 2024 ] 	Batch(2900/6809) done. Loss: 0.4107  lr:0.000001
[ Sat Jul 13 07:28:58 2024 ] 
Training: Epoch [98/120], Step [2999], Loss: 0.02152271755039692, Training Accuracy: 97.5125
[ Sat Jul 13 07:28:58 2024 ] 	Batch(3000/6809) done. Loss: 0.0241  lr:0.000001
[ Sat Jul 13 07:29:16 2024 ] 	Batch(3100/6809) done. Loss: 0.0647  lr:0.000001
[ Sat Jul 13 07:29:34 2024 ] 	Batch(3200/6809) done. Loss: 0.1499  lr:0.000001
[ Sat Jul 13 07:29:53 2024 ] 	Batch(3300/6809) done. Loss: 0.0172  lr:0.000001
[ Sat Jul 13 07:30:11 2024 ] 	Batch(3400/6809) done. Loss: 0.0431  lr:0.000001
[ Sat Jul 13 07:30:30 2024 ] 
Training: Epoch [98/120], Step [3499], Loss: 0.010553013533353806, Training Accuracy: 97.48571428571428
[ Sat Jul 13 07:30:30 2024 ] 	Batch(3500/6809) done. Loss: 0.0064  lr:0.000001
[ Sat Jul 13 07:30:49 2024 ] 	Batch(3600/6809) done. Loss: 0.0771  lr:0.000001
[ Sat Jul 13 07:31:07 2024 ] 	Batch(3700/6809) done. Loss: 0.0187  lr:0.000001
[ Sat Jul 13 07:31:25 2024 ] 	Batch(3800/6809) done. Loss: 0.0181  lr:0.000001
[ Sat Jul 13 07:31:42 2024 ] 	Batch(3900/6809) done. Loss: 0.2021  lr:0.000001
[ Sat Jul 13 07:32:00 2024 ] 
Training: Epoch [98/120], Step [3999], Loss: 0.010055975057184696, Training Accuracy: 97.465625
[ Sat Jul 13 07:32:01 2024 ] 	Batch(4000/6809) done. Loss: 0.0055  lr:0.000001
[ Sat Jul 13 07:32:18 2024 ] 	Batch(4100/6809) done. Loss: 0.0185  lr:0.000001
[ Sat Jul 13 07:32:36 2024 ] 	Batch(4200/6809) done. Loss: 0.0034  lr:0.000001
[ Sat Jul 13 07:32:54 2024 ] 	Batch(4300/6809) done. Loss: 0.0210  lr:0.000001
[ Sat Jul 13 07:33:12 2024 ] 	Batch(4400/6809) done. Loss: 0.1666  lr:0.000001
[ Sat Jul 13 07:33:30 2024 ] 
Training: Epoch [98/120], Step [4499], Loss: 0.014353950507938862, Training Accuracy: 97.49166666666666
[ Sat Jul 13 07:33:30 2024 ] 	Batch(4500/6809) done. Loss: 0.3767  lr:0.000001
[ Sat Jul 13 07:33:48 2024 ] 	Batch(4600/6809) done. Loss: 0.0358  lr:0.000001
[ Sat Jul 13 07:34:06 2024 ] 	Batch(4700/6809) done. Loss: 0.1868  lr:0.000001
[ Sat Jul 13 07:34:24 2024 ] 	Batch(4800/6809) done. Loss: 0.1338  lr:0.000001
[ Sat Jul 13 07:34:42 2024 ] 	Batch(4900/6809) done. Loss: 0.0406  lr:0.000001
[ Sat Jul 13 07:35:00 2024 ] 
Training: Epoch [98/120], Step [4999], Loss: 0.004421491175889969, Training Accuracy: 97.465
[ Sat Jul 13 07:35:00 2024 ] 	Batch(5000/6809) done. Loss: 0.2206  lr:0.000001
[ Sat Jul 13 07:35:18 2024 ] 	Batch(5100/6809) done. Loss: 0.2809  lr:0.000001
[ Sat Jul 13 07:35:36 2024 ] 	Batch(5200/6809) done. Loss: 0.0660  lr:0.000001
[ Sat Jul 13 07:35:55 2024 ] 	Batch(5300/6809) done. Loss: 0.2802  lr:0.000001
[ Sat Jul 13 07:36:13 2024 ] 	Batch(5400/6809) done. Loss: 0.0535  lr:0.000001
[ Sat Jul 13 07:36:32 2024 ] 
Training: Epoch [98/120], Step [5499], Loss: 0.09726126492023468, Training Accuracy: 97.46363636363637
[ Sat Jul 13 07:36:32 2024 ] 	Batch(5500/6809) done. Loss: 0.0142  lr:0.000001
[ Sat Jul 13 07:36:50 2024 ] 	Batch(5600/6809) done. Loss: 0.0567  lr:0.000001
[ Sat Jul 13 07:37:08 2024 ] 	Batch(5700/6809) done. Loss: 0.3284  lr:0.000001
[ Sat Jul 13 07:37:26 2024 ] 	Batch(5800/6809) done. Loss: 0.0972  lr:0.000001
[ Sat Jul 13 07:37:44 2024 ] 	Batch(5900/6809) done. Loss: 0.0946  lr:0.000001
[ Sat Jul 13 07:38:02 2024 ] 
Training: Epoch [98/120], Step [5999], Loss: 0.1834355890750885, Training Accuracy: 97.47708333333334
[ Sat Jul 13 07:38:02 2024 ] 	Batch(6000/6809) done. Loss: 0.0223  lr:0.000001
[ Sat Jul 13 07:38:20 2024 ] 	Batch(6100/6809) done. Loss: 0.1176  lr:0.000001
[ Sat Jul 13 07:38:39 2024 ] 	Batch(6200/6809) done. Loss: 0.0177  lr:0.000001
[ Sat Jul 13 07:38:58 2024 ] 	Batch(6300/6809) done. Loss: 0.0091  lr:0.000001
[ Sat Jul 13 07:39:16 2024 ] 	Batch(6400/6809) done. Loss: 0.0405  lr:0.000001
[ Sat Jul 13 07:39:34 2024 ] 
Training: Epoch [98/120], Step [6499], Loss: 0.0462670661509037, Training Accuracy: 97.53461538461539
[ Sat Jul 13 07:39:34 2024 ] 	Batch(6500/6809) done. Loss: 0.0056  lr:0.000001
[ Sat Jul 13 07:39:52 2024 ] 	Batch(6600/6809) done. Loss: 0.0149  lr:0.000001
[ Sat Jul 13 07:40:10 2024 ] 	Batch(6700/6809) done. Loss: 0.2463  lr:0.000001
[ Sat Jul 13 07:40:28 2024 ] 	Batch(6800/6809) done. Loss: 0.0355  lr:0.000001
[ Sat Jul 13 07:40:29 2024 ] 	Mean training loss: 0.0952.
[ Sat Jul 13 07:40:29 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 07:40:29 2024 ] Training epoch: 100
[ Sat Jul 13 07:40:30 2024 ] 	Batch(0/6809) done. Loss: 0.0332  lr:0.000001
[ Sat Jul 13 07:40:48 2024 ] 	Batch(100/6809) done. Loss: 0.0594  lr:0.000001
[ Sat Jul 13 07:41:07 2024 ] 	Batch(200/6809) done. Loss: 0.1216  lr:0.000001
[ Sat Jul 13 07:41:25 2024 ] 	Batch(300/6809) done. Loss: 0.0280  lr:0.000001
[ Sat Jul 13 07:41:43 2024 ] 	Batch(400/6809) done. Loss: 0.0112  lr:0.000001
[ Sat Jul 13 07:42:01 2024 ] 
Training: Epoch [99/120], Step [499], Loss: 0.2892788350582123, Training Accuracy: 97.32499999999999
[ Sat Jul 13 07:42:02 2024 ] 	Batch(500/6809) done. Loss: 0.1301  lr:0.000001
[ Sat Jul 13 07:42:20 2024 ] 	Batch(600/6809) done. Loss: 0.4210  lr:0.000001
[ Sat Jul 13 07:42:38 2024 ] 	Batch(700/6809) done. Loss: 0.0438  lr:0.000001
[ Sat Jul 13 07:42:57 2024 ] 	Batch(800/6809) done. Loss: 0.2939  lr:0.000001
[ Sat Jul 13 07:43:15 2024 ] 	Batch(900/6809) done. Loss: 0.0679  lr:0.000001
[ Sat Jul 13 07:43:33 2024 ] 
Training: Epoch [99/120], Step [999], Loss: 0.14048130810260773, Training Accuracy: 97.3375
[ Sat Jul 13 07:43:33 2024 ] 	Batch(1000/6809) done. Loss: 0.0139  lr:0.000001
[ Sat Jul 13 07:43:51 2024 ] 	Batch(1100/6809) done. Loss: 0.0931  lr:0.000001
[ Sat Jul 13 07:44:09 2024 ] 	Batch(1200/6809) done. Loss: 0.5323  lr:0.000001
[ Sat Jul 13 07:44:27 2024 ] 	Batch(1300/6809) done. Loss: 0.0174  lr:0.000001
[ Sat Jul 13 07:44:45 2024 ] 	Batch(1400/6809) done. Loss: 0.0324  lr:0.000001
[ Sat Jul 13 07:45:03 2024 ] 
Training: Epoch [99/120], Step [1499], Loss: 0.004985483828932047, Training Accuracy: 97.36666666666667
[ Sat Jul 13 07:45:03 2024 ] 	Batch(1500/6809) done. Loss: 0.0301  lr:0.000001
[ Sat Jul 13 07:45:22 2024 ] 	Batch(1600/6809) done. Loss: 0.0853  lr:0.000001
[ Sat Jul 13 07:45:40 2024 ] 	Batch(1700/6809) done. Loss: 0.2919  lr:0.000001
[ Sat Jul 13 07:45:58 2024 ] 	Batch(1800/6809) done. Loss: 0.2266  lr:0.000001
[ Sat Jul 13 07:46:16 2024 ] 	Batch(1900/6809) done. Loss: 0.0087  lr:0.000001
[ Sat Jul 13 07:46:34 2024 ] 
Training: Epoch [99/120], Step [1999], Loss: 0.05925087258219719, Training Accuracy: 97.52499999999999
[ Sat Jul 13 07:46:35 2024 ] 	Batch(2000/6809) done. Loss: 0.0134  lr:0.000001
[ Sat Jul 13 07:46:53 2024 ] 	Batch(2100/6809) done. Loss: 0.6719  lr:0.000001
[ Sat Jul 13 07:47:11 2024 ] 	Batch(2200/6809) done. Loss: 0.1499  lr:0.000001
[ Sat Jul 13 07:47:30 2024 ] 	Batch(2300/6809) done. Loss: 0.2382  lr:0.000001
[ Sat Jul 13 07:47:48 2024 ] 	Batch(2400/6809) done. Loss: 0.2711  lr:0.000001
[ Sat Jul 13 07:48:06 2024 ] 
Training: Epoch [99/120], Step [2499], Loss: 0.1721878945827484, Training Accuracy: 97.58
[ Sat Jul 13 07:48:06 2024 ] 	Batch(2500/6809) done. Loss: 0.0068  lr:0.000001
[ Sat Jul 13 07:48:25 2024 ] 	Batch(2600/6809) done. Loss: 0.0781  lr:0.000001
[ Sat Jul 13 07:48:43 2024 ] 	Batch(2700/6809) done. Loss: 0.0238  lr:0.000001
[ Sat Jul 13 07:49:02 2024 ] 	Batch(2800/6809) done. Loss: 0.1162  lr:0.000001
[ Sat Jul 13 07:49:20 2024 ] 	Batch(2900/6809) done. Loss: 0.0207  lr:0.000001
[ Sat Jul 13 07:49:39 2024 ] 
Training: Epoch [99/120], Step [2999], Loss: 0.10625925660133362, Training Accuracy: 97.57916666666667
[ Sat Jul 13 07:49:39 2024 ] 	Batch(3000/6809) done. Loss: 0.0557  lr:0.000001
[ Sat Jul 13 07:49:57 2024 ] 	Batch(3100/6809) done. Loss: 0.1890  lr:0.000001
[ Sat Jul 13 07:50:16 2024 ] 	Batch(3200/6809) done. Loss: 0.1894  lr:0.000001
[ Sat Jul 13 07:50:35 2024 ] 	Batch(3300/6809) done. Loss: 0.0204  lr:0.000001
[ Sat Jul 13 07:50:53 2024 ] 	Batch(3400/6809) done. Loss: 0.0975  lr:0.000001
[ Sat Jul 13 07:51:11 2024 ] 
Training: Epoch [99/120], Step [3499], Loss: 0.007312275934964418, Training Accuracy: 97.63214285714285
[ Sat Jul 13 07:51:12 2024 ] 	Batch(3500/6809) done. Loss: 0.0135  lr:0.000001
[ Sat Jul 13 07:51:30 2024 ] 	Batch(3600/6809) done. Loss: 0.0941  lr:0.000001
[ Sat Jul 13 07:51:49 2024 ] 	Batch(3700/6809) done. Loss: 0.0356  lr:0.000001
[ Sat Jul 13 07:52:07 2024 ] 	Batch(3800/6809) done. Loss: 0.0279  lr:0.000001
[ Sat Jul 13 07:52:24 2024 ] 	Batch(3900/6809) done. Loss: 0.0299  lr:0.000001
[ Sat Jul 13 07:52:42 2024 ] 
Training: Epoch [99/120], Step [3999], Loss: 0.13589614629745483, Training Accuracy: 97.675
[ Sat Jul 13 07:52:43 2024 ] 	Batch(4000/6809) done. Loss: 0.0138  lr:0.000001
[ Sat Jul 13 07:53:01 2024 ] 	Batch(4100/6809) done. Loss: 0.0276  lr:0.000001
[ Sat Jul 13 07:53:20 2024 ] 	Batch(4200/6809) done. Loss: 0.0095  lr:0.000001
[ Sat Jul 13 07:53:38 2024 ] 	Batch(4300/6809) done. Loss: 0.0791  lr:0.000001
[ Sat Jul 13 07:53:57 2024 ] 	Batch(4400/6809) done. Loss: 0.0763  lr:0.000001
[ Sat Jul 13 07:54:15 2024 ] 
Training: Epoch [99/120], Step [4499], Loss: 0.1409180462360382, Training Accuracy: 97.65
[ Sat Jul 13 07:54:15 2024 ] 	Batch(4500/6809) done. Loss: 0.0157  lr:0.000001
[ Sat Jul 13 07:54:34 2024 ] 	Batch(4600/6809) done. Loss: 0.0113  lr:0.000001
[ Sat Jul 13 07:54:52 2024 ] 	Batch(4700/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 07:55:11 2024 ] 	Batch(4800/6809) done. Loss: 0.0093  lr:0.000001
[ Sat Jul 13 07:55:30 2024 ] 	Batch(4900/6809) done. Loss: 0.0649  lr:0.000001
[ Sat Jul 13 07:55:48 2024 ] 
Training: Epoch [99/120], Step [4999], Loss: 0.014590967446565628, Training Accuracy: 97.6275
[ Sat Jul 13 07:55:48 2024 ] 	Batch(5000/6809) done. Loss: 0.0050  lr:0.000001
[ Sat Jul 13 07:56:07 2024 ] 	Batch(5100/6809) done. Loss: 0.0286  lr:0.000001
[ Sat Jul 13 07:56:25 2024 ] 	Batch(5200/6809) done. Loss: 0.0066  lr:0.000001
[ Sat Jul 13 07:56:44 2024 ] 	Batch(5300/6809) done. Loss: 0.0065  lr:0.000001
[ Sat Jul 13 07:57:02 2024 ] 	Batch(5400/6809) done. Loss: 0.0219  lr:0.000001
[ Sat Jul 13 07:57:21 2024 ] 
Training: Epoch [99/120], Step [5499], Loss: 0.0017179122660309076, Training Accuracy: 97.64090909090909
[ Sat Jul 13 07:57:21 2024 ] 	Batch(5500/6809) done. Loss: 0.0191  lr:0.000001
[ Sat Jul 13 07:57:40 2024 ] 	Batch(5600/6809) done. Loss: 0.2609  lr:0.000001
[ Sat Jul 13 07:57:57 2024 ] 	Batch(5700/6809) done. Loss: 0.0450  lr:0.000001
[ Sat Jul 13 07:58:15 2024 ] 	Batch(5800/6809) done. Loss: 0.0491  lr:0.000001
[ Sat Jul 13 07:58:33 2024 ] 	Batch(5900/6809) done. Loss: 0.0698  lr:0.000001
[ Sat Jul 13 07:58:51 2024 ] 
Training: Epoch [99/120], Step [5999], Loss: 0.010491329245269299, Training Accuracy: 97.65208333333332
[ Sat Jul 13 07:58:51 2024 ] 	Batch(6000/6809) done. Loss: 0.1501  lr:0.000001
[ Sat Jul 13 07:59:09 2024 ] 	Batch(6100/6809) done. Loss: 0.0539  lr:0.000001
[ Sat Jul 13 07:59:27 2024 ] 	Batch(6200/6809) done. Loss: 0.0115  lr:0.000001
[ Sat Jul 13 07:59:45 2024 ] 	Batch(6300/6809) done. Loss: 0.0240  lr:0.000001
[ Sat Jul 13 08:00:03 2024 ] 	Batch(6400/6809) done. Loss: 0.1252  lr:0.000001
[ Sat Jul 13 08:00:21 2024 ] 
Training: Epoch [99/120], Step [6499], Loss: 0.09509465843439102, Training Accuracy: 97.66153846153847
[ Sat Jul 13 08:00:22 2024 ] 	Batch(6500/6809) done. Loss: 0.6853  lr:0.000001
[ Sat Jul 13 08:00:40 2024 ] 	Batch(6600/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 08:00:59 2024 ] 	Batch(6700/6809) done. Loss: 0.1743  lr:0.000001
[ Sat Jul 13 08:01:17 2024 ] 	Batch(6800/6809) done. Loss: 0.2560  lr:0.000001
[ Sat Jul 13 08:01:19 2024 ] 	Mean training loss: 0.0950.
[ Sat Jul 13 08:01:19 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 08:01:19 2024 ] Eval epoch: 100
[ Sat Jul 13 08:06:53 2024 ] 	Mean val loss of 7435 batches: 1.0649212128198913.
[ Sat Jul 13 08:06:53 2024 ] 
Validation: Epoch [99/120], Samples [47834.0/59477], Loss: 0.459749311208725, Validation Accuracy: 80.42436572120315
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 1 : 385 / 500 = 77 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 2 : 424 / 499 = 84 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 3 : 394 / 500 = 78 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 4 : 408 / 502 = 81 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 5 : 468 / 502 = 93 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 6 : 422 / 502 = 84 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 7 : 468 / 497 = 94 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 8 : 482 / 498 = 96 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 9 : 386 / 500 = 77 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 10 : 214 / 500 = 42 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 11 : 184 / 498 = 36 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 12 : 414 / 499 = 82 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 13 : 477 / 502 = 95 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 14 : 482 / 504 = 95 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 15 : 432 / 502 = 86 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 16 : 382 / 502 = 76 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 17 : 441 / 504 = 87 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 18 : 415 / 504 = 82 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 19 : 462 / 502 = 92 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 20 : 451 / 502 = 89 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 21 : 468 / 503 = 93 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 22 : 430 / 504 = 85 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 23 : 436 / 503 = 86 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 24 : 401 / 504 = 79 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 25 : 491 / 504 = 97 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 26 : 471 / 504 = 93 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 27 : 420 / 501 = 83 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 28 : 346 / 502 = 68 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 29 : 316 / 502 = 62 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 30 : 325 / 501 = 64 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 31 : 418 / 504 = 82 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 32 : 421 / 503 = 83 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 33 : 407 / 503 = 80 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 34 : 484 / 504 = 96 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 35 : 464 / 503 = 92 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 36 : 403 / 502 = 80 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 37 : 431 / 504 = 85 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 38 : 424 / 504 = 84 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 39 : 456 / 498 = 91 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 40 : 366 / 504 = 72 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 41 : 476 / 503 = 94 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 42 : 450 / 504 = 89 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 43 : 326 / 503 = 64 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 44 : 451 / 504 = 89 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 45 : 422 / 504 = 83 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 46 : 420 / 504 = 83 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 47 : 423 / 503 = 84 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 48 : 426 / 503 = 84 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 49 : 373 / 499 = 74 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 50 : 425 / 502 = 84 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 51 : 466 / 503 = 92 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 52 : 449 / 504 = 89 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 53 : 422 / 497 = 84 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 54 : 455 / 480 = 94 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 55 : 361 / 504 = 71 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 56 : 412 / 503 = 81 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 57 : 485 / 504 = 96 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 58 : 483 / 499 = 96 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 59 : 490 / 503 = 97 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 60 : 420 / 479 = 87 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 61 : 416 / 484 = 85 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 62 : 399 / 487 = 81 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 63 : 454 / 489 = 92 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 64 : 372 / 488 = 76 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 65 : 458 / 490 = 93 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 66 : 327 / 488 = 67 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 67 : 375 / 490 = 76 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 68 : 277 / 490 = 56 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 69 : 367 / 490 = 74 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 70 : 232 / 490 = 47 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 71 : 186 / 490 = 37 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 72 : 200 / 488 = 40 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 73 : 297 / 486 = 61 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 74 : 283 / 481 = 58 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 75 : 280 / 488 = 57 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 76 : 318 / 489 = 65 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 77 : 322 / 488 = 65 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 78 : 366 / 488 = 74 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 79 : 453 / 490 = 92 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 80 : 402 / 489 = 82 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 81 : 324 / 491 = 65 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 82 : 337 / 491 = 68 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 83 : 257 / 489 = 52 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 84 : 391 / 489 = 79 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 85 : 377 / 489 = 77 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 86 : 447 / 491 = 91 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 87 : 434 / 492 = 88 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 88 : 374 / 491 = 76 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 89 : 403 / 492 = 81 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 90 : 285 / 490 = 58 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 91 : 413 / 482 = 85 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 92 : 381 / 490 = 77 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 93 : 378 / 487 = 77 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 94 : 416 / 489 = 85 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 95 : 420 / 490 = 85 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 96 : 467 / 491 = 95 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 97 : 464 / 490 = 94 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 98 : 449 / 491 = 91 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 99 : 449 / 491 = 91 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 100 : 448 / 491 = 91 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 101 : 438 / 491 = 89 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 102 : 304 / 492 = 61 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 103 : 404 / 492 = 82 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 104 : 295 / 491 = 60 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 105 : 271 / 491 = 55 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 106 : 280 / 492 = 56 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 107 : 403 / 491 = 82 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 108 : 397 / 492 = 80 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 109 : 335 / 490 = 68 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 110 : 418 / 491 = 85 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 111 : 459 / 492 = 93 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 112 : 456 / 492 = 92 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 113 : 449 / 491 = 91 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 114 : 404 / 491 = 82 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 115 : 439 / 492 = 89 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 116 : 406 / 491 = 82 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 117 : 451 / 492 = 91 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 118 : 447 / 490 = 91 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 119 : 451 / 492 = 91 %
[ Sat Jul 13 08:06:53 2024 ] Accuracy of 120 : 425 / 500 = 85 %
[ Sat Jul 13 08:06:53 2024 ] Training epoch: 101
[ Sat Jul 13 08:06:54 2024 ] 	Batch(0/6809) done. Loss: 0.0546  lr:0.000001
[ Sat Jul 13 08:07:12 2024 ] 	Batch(100/6809) done. Loss: 0.0269  lr:0.000001
[ Sat Jul 13 08:07:30 2024 ] 	Batch(200/6809) done. Loss: 0.0522  lr:0.000001
[ Sat Jul 13 08:07:48 2024 ] 	Batch(300/6809) done. Loss: 0.1244  lr:0.000001
[ Sat Jul 13 08:08:06 2024 ] 	Batch(400/6809) done. Loss: 0.0548  lr:0.000001
[ Sat Jul 13 08:08:23 2024 ] 
Training: Epoch [100/120], Step [499], Loss: 0.33101484179496765, Training Accuracy: 97.475
[ Sat Jul 13 08:08:24 2024 ] 	Batch(500/6809) done. Loss: 0.1863  lr:0.000001
[ Sat Jul 13 08:08:41 2024 ] 	Batch(600/6809) done. Loss: 0.0765  lr:0.000001
[ Sat Jul 13 08:08:59 2024 ] 	Batch(700/6809) done. Loss: 0.0105  lr:0.000001
[ Sat Jul 13 08:09:17 2024 ] 	Batch(800/6809) done. Loss: 0.0887  lr:0.000001
[ Sat Jul 13 08:09:35 2024 ] 	Batch(900/6809) done. Loss: 0.1887  lr:0.000001
[ Sat Jul 13 08:09:53 2024 ] 
Training: Epoch [100/120], Step [999], Loss: 0.014424920082092285, Training Accuracy: 97.45
[ Sat Jul 13 08:09:53 2024 ] 	Batch(1000/6809) done. Loss: 0.0805  lr:0.000001
[ Sat Jul 13 08:10:11 2024 ] 	Batch(1100/6809) done. Loss: 0.0095  lr:0.000001
[ Sat Jul 13 08:10:29 2024 ] 	Batch(1200/6809) done. Loss: 0.0983  lr:0.000001
[ Sat Jul 13 08:10:47 2024 ] 	Batch(1300/6809) done. Loss: 0.0547  lr:0.000001
[ Sat Jul 13 08:11:05 2024 ] 	Batch(1400/6809) done. Loss: 0.0126  lr:0.000001
[ Sat Jul 13 08:11:23 2024 ] 
Training: Epoch [100/120], Step [1499], Loss: 0.018307453021407127, Training Accuracy: 97.55
[ Sat Jul 13 08:11:23 2024 ] 	Batch(1500/6809) done. Loss: 0.0742  lr:0.000001
[ Sat Jul 13 08:11:42 2024 ] 	Batch(1600/6809) done. Loss: 0.0405  lr:0.000001
[ Sat Jul 13 08:12:00 2024 ] 	Batch(1700/6809) done. Loss: 0.0043  lr:0.000001
[ Sat Jul 13 08:12:19 2024 ] 	Batch(1800/6809) done. Loss: 0.0322  lr:0.000001
[ Sat Jul 13 08:12:37 2024 ] 	Batch(1900/6809) done. Loss: 0.0053  lr:0.000001
[ Sat Jul 13 08:12:55 2024 ] 
Training: Epoch [100/120], Step [1999], Loss: 0.3991332948207855, Training Accuracy: 97.53125
[ Sat Jul 13 08:12:56 2024 ] 	Batch(2000/6809) done. Loss: 0.1520  lr:0.000001
[ Sat Jul 13 08:13:14 2024 ] 	Batch(2100/6809) done. Loss: 0.0252  lr:0.000001
[ Sat Jul 13 08:13:33 2024 ] 	Batch(2200/6809) done. Loss: 0.1789  lr:0.000001
[ Sat Jul 13 08:13:51 2024 ] 	Batch(2300/6809) done. Loss: 0.0495  lr:0.000001
[ Sat Jul 13 08:14:10 2024 ] 	Batch(2400/6809) done. Loss: 0.3286  lr:0.000001
[ Sat Jul 13 08:14:28 2024 ] 
Training: Epoch [100/120], Step [2499], Loss: 0.06349025666713715, Training Accuracy: 97.47
[ Sat Jul 13 08:14:28 2024 ] 	Batch(2500/6809) done. Loss: 0.0368  lr:0.000001
[ Sat Jul 13 08:14:46 2024 ] 	Batch(2600/6809) done. Loss: 0.0557  lr:0.000001
[ Sat Jul 13 08:15:04 2024 ] 	Batch(2700/6809) done. Loss: 0.1727  lr:0.000001
[ Sat Jul 13 08:15:22 2024 ] 	Batch(2800/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 08:15:39 2024 ] 	Batch(2900/6809) done. Loss: 0.1857  lr:0.000001
[ Sat Jul 13 08:15:57 2024 ] 
Training: Epoch [100/120], Step [2999], Loss: 0.016724977642297745, Training Accuracy: 97.45416666666667
[ Sat Jul 13 08:15:57 2024 ] 	Batch(3000/6809) done. Loss: 0.0060  lr:0.000001
[ Sat Jul 13 08:16:15 2024 ] 	Batch(3100/6809) done. Loss: 0.1481  lr:0.000001
[ Sat Jul 13 08:16:33 2024 ] 	Batch(3200/6809) done. Loss: 0.2520  lr:0.000001
[ Sat Jul 13 08:16:52 2024 ] 	Batch(3300/6809) done. Loss: 0.1553  lr:0.000001
[ Sat Jul 13 08:17:10 2024 ] 	Batch(3400/6809) done. Loss: 0.0934  lr:0.000001
[ Sat Jul 13 08:17:29 2024 ] 
Training: Epoch [100/120], Step [3499], Loss: 0.01339163538068533, Training Accuracy: 97.425
[ Sat Jul 13 08:17:29 2024 ] 	Batch(3500/6809) done. Loss: 0.0539  lr:0.000001
[ Sat Jul 13 08:17:47 2024 ] 	Batch(3600/6809) done. Loss: 0.0305  lr:0.000001
[ Sat Jul 13 08:18:06 2024 ] 	Batch(3700/6809) done. Loss: 0.0088  lr:0.000001
[ Sat Jul 13 08:18:24 2024 ] 	Batch(3800/6809) done. Loss: 0.0083  lr:0.000001
[ Sat Jul 13 08:18:43 2024 ] 	Batch(3900/6809) done. Loss: 0.0217  lr:0.000001
[ Sat Jul 13 08:19:01 2024 ] 
Training: Epoch [100/120], Step [3999], Loss: 0.008874508552253246, Training Accuracy: 97.503125
[ Sat Jul 13 08:19:02 2024 ] 	Batch(4000/6809) done. Loss: 0.0129  lr:0.000001
[ Sat Jul 13 08:19:20 2024 ] 	Batch(4100/6809) done. Loss: 0.0595  lr:0.000001
[ Sat Jul 13 08:19:38 2024 ] 	Batch(4200/6809) done. Loss: 0.0912  lr:0.000001
[ Sat Jul 13 08:19:56 2024 ] 	Batch(4300/6809) done. Loss: 0.1570  lr:0.000001
[ Sat Jul 13 08:20:15 2024 ] 	Batch(4400/6809) done. Loss: 0.1333  lr:0.000001
[ Sat Jul 13 08:20:32 2024 ] 
Training: Epoch [100/120], Step [4499], Loss: 0.008374269120395184, Training Accuracy: 97.52777777777779
[ Sat Jul 13 08:20:32 2024 ] 	Batch(4500/6809) done. Loss: 0.0239  lr:0.000001
[ Sat Jul 13 08:20:50 2024 ] 	Batch(4600/6809) done. Loss: 0.0433  lr:0.000001
[ Sat Jul 13 08:21:08 2024 ] 	Batch(4700/6809) done. Loss: 0.0507  lr:0.000001
[ Sat Jul 13 08:21:26 2024 ] 	Batch(4800/6809) done. Loss: 0.0198  lr:0.000001
[ Sat Jul 13 08:21:44 2024 ] 	Batch(4900/6809) done. Loss: 0.4286  lr:0.000001
[ Sat Jul 13 08:22:02 2024 ] 
Training: Epoch [100/120], Step [4999], Loss: 0.11144063621759415, Training Accuracy: 97.52250000000001
[ Sat Jul 13 08:22:02 2024 ] 	Batch(5000/6809) done. Loss: 0.0204  lr:0.000001
[ Sat Jul 13 08:22:20 2024 ] 	Batch(5100/6809) done. Loss: 0.0207  lr:0.000001
[ Sat Jul 13 08:22:38 2024 ] 	Batch(5200/6809) done. Loss: 0.1193  lr:0.000001
[ Sat Jul 13 08:22:56 2024 ] 	Batch(5300/6809) done. Loss: 0.0172  lr:0.000001
[ Sat Jul 13 08:23:14 2024 ] 	Batch(5400/6809) done. Loss: 0.0108  lr:0.000001
[ Sat Jul 13 08:23:31 2024 ] 
Training: Epoch [100/120], Step [5499], Loss: 0.4429820775985718, Training Accuracy: 97.54318181818182
[ Sat Jul 13 08:23:31 2024 ] 	Batch(5500/6809) done. Loss: 0.0264  lr:0.000001
[ Sat Jul 13 08:23:50 2024 ] 	Batch(5600/6809) done. Loss: 0.2294  lr:0.000001
[ Sat Jul 13 08:24:08 2024 ] 	Batch(5700/6809) done. Loss: 0.1733  lr:0.000001
[ Sat Jul 13 08:24:27 2024 ] 	Batch(5800/6809) done. Loss: 0.0286  lr:0.000001
[ Sat Jul 13 08:24:45 2024 ] 	Batch(5900/6809) done. Loss: 0.0502  lr:0.000001
[ Sat Jul 13 08:25:04 2024 ] 
Training: Epoch [100/120], Step [5999], Loss: 0.0034940859768539667, Training Accuracy: 97.54166666666667
[ Sat Jul 13 08:25:04 2024 ] 	Batch(6000/6809) done. Loss: 0.3442  lr:0.000001
[ Sat Jul 13 08:25:22 2024 ] 	Batch(6100/6809) done. Loss: 0.0247  lr:0.000001
[ Sat Jul 13 08:25:41 2024 ] 	Batch(6200/6809) done. Loss: 0.0305  lr:0.000001
[ Sat Jul 13 08:25:59 2024 ] 	Batch(6300/6809) done. Loss: 0.6256  lr:0.000001
[ Sat Jul 13 08:26:18 2024 ] 	Batch(6400/6809) done. Loss: 0.1428  lr:0.000001
[ Sat Jul 13 08:26:37 2024 ] 
Training: Epoch [100/120], Step [6499], Loss: 0.22481024265289307, Training Accuracy: 97.54038461538461
[ Sat Jul 13 08:26:37 2024 ] 	Batch(6500/6809) done. Loss: 0.0337  lr:0.000001
[ Sat Jul 13 08:26:55 2024 ] 	Batch(6600/6809) done. Loss: 0.1012  lr:0.000001
[ Sat Jul 13 08:27:14 2024 ] 	Batch(6700/6809) done. Loss: 0.0104  lr:0.000001
[ Sat Jul 13 08:27:32 2024 ] 	Batch(6800/6809) done. Loss: 0.0509  lr:0.000001
[ Sat Jul 13 08:27:34 2024 ] 	Mean training loss: 0.0967.
[ Sat Jul 13 08:27:34 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 08:27:34 2024 ] Training epoch: 102
[ Sat Jul 13 08:27:35 2024 ] 	Batch(0/6809) done. Loss: 0.0129  lr:0.000001
[ Sat Jul 13 08:27:53 2024 ] 	Batch(100/6809) done. Loss: 0.0233  lr:0.000001
[ Sat Jul 13 08:28:10 2024 ] 	Batch(200/6809) done. Loss: 0.0074  lr:0.000001
[ Sat Jul 13 08:28:28 2024 ] 	Batch(300/6809) done. Loss: 0.0146  lr:0.000001
[ Sat Jul 13 08:28:46 2024 ] 	Batch(400/6809) done. Loss: 0.0367  lr:0.000001
[ Sat Jul 13 08:29:04 2024 ] 
Training: Epoch [101/120], Step [499], Loss: 0.13817907869815826, Training Accuracy: 97.675
[ Sat Jul 13 08:29:04 2024 ] 	Batch(500/6809) done. Loss: 0.0036  lr:0.000001
[ Sat Jul 13 08:29:22 2024 ] 	Batch(600/6809) done. Loss: 0.2929  lr:0.000001
[ Sat Jul 13 08:29:40 2024 ] 	Batch(700/6809) done. Loss: 0.1617  lr:0.000001
[ Sat Jul 13 08:29:58 2024 ] 	Batch(800/6809) done. Loss: 0.0539  lr:0.000001
[ Sat Jul 13 08:30:16 2024 ] 	Batch(900/6809) done. Loss: 0.2574  lr:0.000001
[ Sat Jul 13 08:30:34 2024 ] 
Training: Epoch [101/120], Step [999], Loss: 0.00761263445019722, Training Accuracy: 97.65
[ Sat Jul 13 08:30:34 2024 ] 	Batch(1000/6809) done. Loss: 0.1012  lr:0.000001
[ Sat Jul 13 08:30:52 2024 ] 	Batch(1100/6809) done. Loss: 0.3931  lr:0.000001
[ Sat Jul 13 08:31:10 2024 ] 	Batch(1200/6809) done. Loss: 0.0353  lr:0.000001
[ Sat Jul 13 08:31:28 2024 ] 	Batch(1300/6809) done. Loss: 0.0552  lr:0.000001
[ Sat Jul 13 08:31:45 2024 ] 	Batch(1400/6809) done. Loss: 0.1325  lr:0.000001
[ Sat Jul 13 08:32:03 2024 ] 
Training: Epoch [101/120], Step [1499], Loss: 0.09130093455314636, Training Accuracy: 97.58333333333333
[ Sat Jul 13 08:32:03 2024 ] 	Batch(1500/6809) done. Loss: 0.0449  lr:0.000001
[ Sat Jul 13 08:32:21 2024 ] 	Batch(1600/6809) done. Loss: 0.0595  lr:0.000001
[ Sat Jul 13 08:32:39 2024 ] 	Batch(1700/6809) done. Loss: 0.0019  lr:0.000001
[ Sat Jul 13 08:32:57 2024 ] 	Batch(1800/6809) done. Loss: 0.0796  lr:0.000001
[ Sat Jul 13 08:33:15 2024 ] 	Batch(1900/6809) done. Loss: 0.0208  lr:0.000001
[ Sat Jul 13 08:33:33 2024 ] 
Training: Epoch [101/120], Step [1999], Loss: 0.09649376571178436, Training Accuracy: 97.6125
[ Sat Jul 13 08:33:33 2024 ] 	Batch(2000/6809) done. Loss: 0.1945  lr:0.000001
[ Sat Jul 13 08:33:51 2024 ] 	Batch(2100/6809) done. Loss: 0.0560  lr:0.000001
[ Sat Jul 13 08:34:09 2024 ] 	Batch(2200/6809) done. Loss: 0.0169  lr:0.000001
[ Sat Jul 13 08:34:27 2024 ] 	Batch(2300/6809) done. Loss: 0.0112  lr:0.000001
[ Sat Jul 13 08:34:45 2024 ] 	Batch(2400/6809) done. Loss: 0.0176  lr:0.000001
[ Sat Jul 13 08:35:02 2024 ] 
Training: Epoch [101/120], Step [2499], Loss: 0.46973729133605957, Training Accuracy: 97.605
[ Sat Jul 13 08:35:03 2024 ] 	Batch(2500/6809) done. Loss: 0.2777  lr:0.000001
[ Sat Jul 13 08:35:20 2024 ] 	Batch(2600/6809) done. Loss: 0.0762  lr:0.000001
[ Sat Jul 13 08:35:38 2024 ] 	Batch(2700/6809) done. Loss: 0.0690  lr:0.000001
[ Sat Jul 13 08:35:56 2024 ] 	Batch(2800/6809) done. Loss: 0.0207  lr:0.000001
[ Sat Jul 13 08:36:14 2024 ] 	Batch(2900/6809) done. Loss: 0.3517  lr:0.000001
[ Sat Jul 13 08:36:32 2024 ] 
Training: Epoch [101/120], Step [2999], Loss: 0.17260615527629852, Training Accuracy: 97.53333333333333
[ Sat Jul 13 08:36:32 2024 ] 	Batch(3000/6809) done. Loss: 0.0025  lr:0.000001
[ Sat Jul 13 08:36:50 2024 ] 	Batch(3100/6809) done. Loss: 0.0656  lr:0.000001
[ Sat Jul 13 08:37:08 2024 ] 	Batch(3200/6809) done. Loss: 0.2620  lr:0.000001
[ Sat Jul 13 08:37:26 2024 ] 	Batch(3300/6809) done. Loss: 0.0526  lr:0.000001
[ Sat Jul 13 08:37:44 2024 ] 	Batch(3400/6809) done. Loss: 0.0647  lr:0.000001
[ Sat Jul 13 08:38:01 2024 ] 
Training: Epoch [101/120], Step [3499], Loss: 0.11216083914041519, Training Accuracy: 97.55
[ Sat Jul 13 08:38:02 2024 ] 	Batch(3500/6809) done. Loss: 0.0123  lr:0.000001
[ Sat Jul 13 08:38:20 2024 ] 	Batch(3600/6809) done. Loss: 0.1203  lr:0.000001
[ Sat Jul 13 08:38:38 2024 ] 	Batch(3700/6809) done. Loss: 0.0977  lr:0.000001
[ Sat Jul 13 08:38:57 2024 ] 	Batch(3800/6809) done. Loss: 0.0179  lr:0.000001
[ Sat Jul 13 08:39:15 2024 ] 	Batch(3900/6809) done. Loss: 0.0788  lr:0.000001
[ Sat Jul 13 08:39:34 2024 ] 
Training: Epoch [101/120], Step [3999], Loss: 0.03494982421398163, Training Accuracy: 97.609375
[ Sat Jul 13 08:39:34 2024 ] 	Batch(4000/6809) done. Loss: 0.0125  lr:0.000001
[ Sat Jul 13 08:39:52 2024 ] 	Batch(4100/6809) done. Loss: 0.0786  lr:0.000001
[ Sat Jul 13 08:40:10 2024 ] 	Batch(4200/6809) done. Loss: 0.0666  lr:0.000001
[ Sat Jul 13 08:40:28 2024 ] 	Batch(4300/6809) done. Loss: 0.0143  lr:0.000001
[ Sat Jul 13 08:40:46 2024 ] 	Batch(4400/6809) done. Loss: 0.0031  lr:0.000001
[ Sat Jul 13 08:41:04 2024 ] 
Training: Epoch [101/120], Step [4499], Loss: 0.35358983278274536, Training Accuracy: 97.64166666666667
[ Sat Jul 13 08:41:04 2024 ] 	Batch(4500/6809) done. Loss: 0.0771  lr:0.000001
[ Sat Jul 13 08:41:22 2024 ] 	Batch(4600/6809) done. Loss: 0.0616  lr:0.000001
[ Sat Jul 13 08:41:40 2024 ] 	Batch(4700/6809) done. Loss: 0.1030  lr:0.000001
[ Sat Jul 13 08:41:58 2024 ] 	Batch(4800/6809) done. Loss: 0.1033  lr:0.000001
[ Sat Jul 13 08:42:16 2024 ] 	Batch(4900/6809) done. Loss: 0.0658  lr:0.000001
[ Sat Jul 13 08:42:34 2024 ] 
Training: Epoch [101/120], Step [4999], Loss: 0.023128507658839226, Training Accuracy: 97.66
[ Sat Jul 13 08:42:34 2024 ] 	Batch(5000/6809) done. Loss: 0.0649  lr:0.000001
[ Sat Jul 13 08:42:52 2024 ] 	Batch(5100/6809) done. Loss: 0.0086  lr:0.000001
[ Sat Jul 13 08:43:10 2024 ] 	Batch(5200/6809) done. Loss: 0.1090  lr:0.000001
[ Sat Jul 13 08:43:28 2024 ] 	Batch(5300/6809) done. Loss: 0.0822  lr:0.000001
[ Sat Jul 13 08:43:46 2024 ] 	Batch(5400/6809) done. Loss: 0.0148  lr:0.000001
[ Sat Jul 13 08:44:05 2024 ] 
Training: Epoch [101/120], Step [5499], Loss: 0.04091253876686096, Training Accuracy: 97.68863636363636
[ Sat Jul 13 08:44:05 2024 ] 	Batch(5500/6809) done. Loss: 0.0217  lr:0.000001
[ Sat Jul 13 08:44:24 2024 ] 	Batch(5600/6809) done. Loss: 0.1370  lr:0.000001
[ Sat Jul 13 08:44:42 2024 ] 	Batch(5700/6809) done. Loss: 0.0005  lr:0.000001
[ Sat Jul 13 08:45:01 2024 ] 	Batch(5800/6809) done. Loss: 0.0010  lr:0.000001
[ Sat Jul 13 08:45:19 2024 ] 	Batch(5900/6809) done. Loss: 0.1017  lr:0.000001
[ Sat Jul 13 08:45:37 2024 ] 
Training: Epoch [101/120], Step [5999], Loss: 0.005543889943510294, Training Accuracy: 97.66458333333333
[ Sat Jul 13 08:45:37 2024 ] 	Batch(6000/6809) done. Loss: 0.1317  lr:0.000001
[ Sat Jul 13 08:45:55 2024 ] 	Batch(6100/6809) done. Loss: 0.0159  lr:0.000001
[ Sat Jul 13 08:46:13 2024 ] 	Batch(6200/6809) done. Loss: 0.1315  lr:0.000001
[ Sat Jul 13 08:46:31 2024 ] 	Batch(6300/6809) done. Loss: 0.2122  lr:0.000001
[ Sat Jul 13 08:46:50 2024 ] 	Batch(6400/6809) done. Loss: 0.0655  lr:0.000001
[ Sat Jul 13 08:47:08 2024 ] 
Training: Epoch [101/120], Step [6499], Loss: 0.18232637643814087, Training Accuracy: 97.6423076923077
[ Sat Jul 13 08:47:08 2024 ] 	Batch(6500/6809) done. Loss: 0.1353  lr:0.000001
[ Sat Jul 13 08:47:27 2024 ] 	Batch(6600/6809) done. Loss: 0.1670  lr:0.000001
[ Sat Jul 13 08:47:45 2024 ] 	Batch(6700/6809) done. Loss: 0.0649  lr:0.000001
[ Sat Jul 13 08:48:03 2024 ] 	Batch(6800/6809) done. Loss: 0.0127  lr:0.000001
[ Sat Jul 13 08:48:05 2024 ] 	Mean training loss: 0.0974.
[ Sat Jul 13 08:48:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 08:48:05 2024 ] Training epoch: 103
[ Sat Jul 13 08:48:06 2024 ] 	Batch(0/6809) done. Loss: 0.0308  lr:0.000001
[ Sat Jul 13 08:48:23 2024 ] 	Batch(100/6809) done. Loss: 0.4542  lr:0.000001
[ Sat Jul 13 08:48:41 2024 ] 	Batch(200/6809) done. Loss: 0.0916  lr:0.000001
[ Sat Jul 13 08:48:59 2024 ] 	Batch(300/6809) done. Loss: 0.0197  lr:0.000001
[ Sat Jul 13 08:49:17 2024 ] 	Batch(400/6809) done. Loss: 0.0522  lr:0.000001
[ Sat Jul 13 08:49:35 2024 ] 
Training: Epoch [102/120], Step [499], Loss: 0.005882208235561848, Training Accuracy: 97.35000000000001
[ Sat Jul 13 08:49:35 2024 ] 	Batch(500/6809) done. Loss: 0.1979  lr:0.000001
[ Sat Jul 13 08:49:53 2024 ] 	Batch(600/6809) done. Loss: 0.0057  lr:0.000001
[ Sat Jul 13 08:50:11 2024 ] 	Batch(700/6809) done. Loss: 0.1413  lr:0.000001
[ Sat Jul 13 08:50:29 2024 ] 	Batch(800/6809) done. Loss: 0.2679  lr:0.000001
[ Sat Jul 13 08:50:47 2024 ] 	Batch(900/6809) done. Loss: 0.0455  lr:0.000001
[ Sat Jul 13 08:51:04 2024 ] 
Training: Epoch [102/120], Step [999], Loss: 0.006734264548867941, Training Accuracy: 97.3625
[ Sat Jul 13 08:51:05 2024 ] 	Batch(1000/6809) done. Loss: 0.0530  lr:0.000001
[ Sat Jul 13 08:51:22 2024 ] 	Batch(1100/6809) done. Loss: 0.0224  lr:0.000001
[ Sat Jul 13 08:51:40 2024 ] 	Batch(1200/6809) done. Loss: 0.0182  lr:0.000001
[ Sat Jul 13 08:51:59 2024 ] 	Batch(1300/6809) done. Loss: 0.1479  lr:0.000001
[ Sat Jul 13 08:52:18 2024 ] 	Batch(1400/6809) done. Loss: 0.0700  lr:0.000001
[ Sat Jul 13 08:52:36 2024 ] 
Training: Epoch [102/120], Step [1499], Loss: 0.06853249669075012, Training Accuracy: 97.46666666666667
[ Sat Jul 13 08:52:36 2024 ] 	Batch(1500/6809) done. Loss: 0.0923  lr:0.000001
[ Sat Jul 13 08:52:55 2024 ] 	Batch(1600/6809) done. Loss: 0.0032  lr:0.000001
[ Sat Jul 13 08:53:13 2024 ] 	Batch(1700/6809) done. Loss: 0.2110  lr:0.000001
[ Sat Jul 13 08:53:32 2024 ] 	Batch(1800/6809) done. Loss: 0.0203  lr:0.000001
[ Sat Jul 13 08:53:51 2024 ] 	Batch(1900/6809) done. Loss: 0.0248  lr:0.000001
[ Sat Jul 13 08:54:09 2024 ] 
Training: Epoch [102/120], Step [1999], Loss: 0.020127302035689354, Training Accuracy: 97.46875
[ Sat Jul 13 08:54:09 2024 ] 	Batch(2000/6809) done. Loss: 0.2835  lr:0.000001
[ Sat Jul 13 08:54:27 2024 ] 	Batch(2100/6809) done. Loss: 0.0057  lr:0.000001
[ Sat Jul 13 08:54:45 2024 ] 	Batch(2200/6809) done. Loss: 0.0716  lr:0.000001
[ Sat Jul 13 08:55:03 2024 ] 	Batch(2300/6809) done. Loss: 0.3346  lr:0.000001
[ Sat Jul 13 08:55:21 2024 ] 	Batch(2400/6809) done. Loss: 0.0228  lr:0.000001
[ Sat Jul 13 08:55:39 2024 ] 
Training: Epoch [102/120], Step [2499], Loss: 0.1728527545928955, Training Accuracy: 97.41
[ Sat Jul 13 08:55:39 2024 ] 	Batch(2500/6809) done. Loss: 0.0361  lr:0.000001
[ Sat Jul 13 08:55:57 2024 ] 	Batch(2600/6809) done. Loss: 0.2962  lr:0.000001
[ Sat Jul 13 08:56:15 2024 ] 	Batch(2700/6809) done. Loss: 0.0391  lr:0.000001
[ Sat Jul 13 08:56:33 2024 ] 	Batch(2800/6809) done. Loss: 0.0899  lr:0.000001
[ Sat Jul 13 08:56:50 2024 ] 	Batch(2900/6809) done. Loss: 0.0196  lr:0.000001
[ Sat Jul 13 08:57:08 2024 ] 
Training: Epoch [102/120], Step [2999], Loss: 0.03241799399256706, Training Accuracy: 97.45416666666667
[ Sat Jul 13 08:57:08 2024 ] 	Batch(3000/6809) done. Loss: 0.0420  lr:0.000001
[ Sat Jul 13 08:57:26 2024 ] 	Batch(3100/6809) done. Loss: 0.1173  lr:0.000001
[ Sat Jul 13 08:57:44 2024 ] 	Batch(3200/6809) done. Loss: 0.0053  lr:0.000001
[ Sat Jul 13 08:58:03 2024 ] 	Batch(3300/6809) done. Loss: 0.1825  lr:0.000001
[ Sat Jul 13 08:58:21 2024 ] 	Batch(3400/6809) done. Loss: 0.1485  lr:0.000001
[ Sat Jul 13 08:58:40 2024 ] 
Training: Epoch [102/120], Step [3499], Loss: 0.11234648525714874, Training Accuracy: 97.50357142857143
[ Sat Jul 13 08:58:40 2024 ] 	Batch(3500/6809) done. Loss: 0.3423  lr:0.000001
[ Sat Jul 13 08:58:59 2024 ] 	Batch(3600/6809) done. Loss: 0.0392  lr:0.000001
[ Sat Jul 13 08:59:16 2024 ] 	Batch(3700/6809) done. Loss: 0.2288  lr:0.000001
[ Sat Jul 13 08:59:34 2024 ] 	Batch(3800/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 08:59:52 2024 ] 	Batch(3900/6809) done. Loss: 0.0056  lr:0.000001
[ Sat Jul 13 09:00:10 2024 ] 
Training: Epoch [102/120], Step [3999], Loss: 0.036388467997312546, Training Accuracy: 97.490625
[ Sat Jul 13 09:00:10 2024 ] 	Batch(4000/6809) done. Loss: 0.0065  lr:0.000001
[ Sat Jul 13 09:00:28 2024 ] 	Batch(4100/6809) done. Loss: 0.0815  lr:0.000001
[ Sat Jul 13 09:00:46 2024 ] 	Batch(4200/6809) done. Loss: 0.0876  lr:0.000001
[ Sat Jul 13 09:01:04 2024 ] 	Batch(4300/6809) done. Loss: 0.0018  lr:0.000001
[ Sat Jul 13 09:01:22 2024 ] 	Batch(4400/6809) done. Loss: 0.0749  lr:0.000001
[ Sat Jul 13 09:01:40 2024 ] 
Training: Epoch [102/120], Step [4499], Loss: 0.01352018490433693, Training Accuracy: 97.54722222222222
[ Sat Jul 13 09:01:40 2024 ] 	Batch(4500/6809) done. Loss: 0.0323  lr:0.000001
[ Sat Jul 13 09:01:58 2024 ] 	Batch(4600/6809) done. Loss: 0.2511  lr:0.000001
[ Sat Jul 13 09:02:16 2024 ] 	Batch(4700/6809) done. Loss: 0.2145  lr:0.000001
[ Sat Jul 13 09:02:34 2024 ] 	Batch(4800/6809) done. Loss: 0.0580  lr:0.000001
[ Sat Jul 13 09:02:51 2024 ] 	Batch(4900/6809) done. Loss: 0.0845  lr:0.000001
[ Sat Jul 13 09:03:09 2024 ] 
Training: Epoch [102/120], Step [4999], Loss: 0.2775163948535919, Training Accuracy: 97.52499999999999
[ Sat Jul 13 09:03:09 2024 ] 	Batch(5000/6809) done. Loss: 0.0238  lr:0.000001
[ Sat Jul 13 09:03:27 2024 ] 	Batch(5100/6809) done. Loss: 0.1592  lr:0.000001
[ Sat Jul 13 09:03:45 2024 ] 	Batch(5200/6809) done. Loss: 0.0860  lr:0.000001
[ Sat Jul 13 09:04:03 2024 ] 	Batch(5300/6809) done. Loss: 0.0899  lr:0.000001
[ Sat Jul 13 09:04:21 2024 ] 	Batch(5400/6809) done. Loss: 0.0076  lr:0.000001
[ Sat Jul 13 09:04:39 2024 ] 
Training: Epoch [102/120], Step [5499], Loss: 0.023917125537991524, Training Accuracy: 97.5
[ Sat Jul 13 09:04:39 2024 ] 	Batch(5500/6809) done. Loss: 0.0446  lr:0.000001
[ Sat Jul 13 09:04:57 2024 ] 	Batch(5600/6809) done. Loss: 0.0602  lr:0.000001
[ Sat Jul 13 09:05:15 2024 ] 	Batch(5700/6809) done. Loss: 0.1178  lr:0.000001
[ Sat Jul 13 09:05:33 2024 ] 	Batch(5800/6809) done. Loss: 0.0198  lr:0.000001
[ Sat Jul 13 09:05:51 2024 ] 	Batch(5900/6809) done. Loss: 0.0305  lr:0.000001
[ Sat Jul 13 09:06:08 2024 ] 
Training: Epoch [102/120], Step [5999], Loss: 0.1895027905702591, Training Accuracy: 97.50833333333333
[ Sat Jul 13 09:06:09 2024 ] 	Batch(6000/6809) done. Loss: 0.0049  lr:0.000001
[ Sat Jul 13 09:06:26 2024 ] 	Batch(6100/6809) done. Loss: 0.0743  lr:0.000001
[ Sat Jul 13 09:06:44 2024 ] 	Batch(6200/6809) done. Loss: 0.0580  lr:0.000001
[ Sat Jul 13 09:07:02 2024 ] 	Batch(6300/6809) done. Loss: 0.2408  lr:0.000001
[ Sat Jul 13 09:07:20 2024 ] 	Batch(6400/6809) done. Loss: 0.1127  lr:0.000001
[ Sat Jul 13 09:07:38 2024 ] 
Training: Epoch [102/120], Step [6499], Loss: 0.1589493751525879, Training Accuracy: 97.5173076923077
[ Sat Jul 13 09:07:38 2024 ] 	Batch(6500/6809) done. Loss: 0.2279  lr:0.000001
[ Sat Jul 13 09:07:56 2024 ] 	Batch(6600/6809) done. Loss: 0.2208  lr:0.000001
[ Sat Jul 13 09:08:14 2024 ] 	Batch(6700/6809) done. Loss: 0.0080  lr:0.000001
[ Sat Jul 13 09:08:32 2024 ] 	Batch(6800/6809) done. Loss: 0.1088  lr:0.000001
[ Sat Jul 13 09:08:34 2024 ] 	Mean training loss: 0.0942.
[ Sat Jul 13 09:08:34 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 09:08:34 2024 ] Training epoch: 104
[ Sat Jul 13 09:08:34 2024 ] 	Batch(0/6809) done. Loss: 0.0118  lr:0.000001
[ Sat Jul 13 09:08:53 2024 ] 	Batch(100/6809) done. Loss: 0.1289  lr:0.000001
[ Sat Jul 13 09:09:11 2024 ] 	Batch(200/6809) done. Loss: 0.2622  lr:0.000001
[ Sat Jul 13 09:09:29 2024 ] 	Batch(300/6809) done. Loss: 0.0153  lr:0.000001
[ Sat Jul 13 09:09:48 2024 ] 	Batch(400/6809) done. Loss: 0.0047  lr:0.000001
[ Sat Jul 13 09:10:06 2024 ] 
Training: Epoch [103/120], Step [499], Loss: 0.020579440519213676, Training Accuracy: 97.52499999999999
[ Sat Jul 13 09:10:06 2024 ] 	Batch(500/6809) done. Loss: 0.0610  lr:0.000001
[ Sat Jul 13 09:10:24 2024 ] 	Batch(600/6809) done. Loss: 0.0767  lr:0.000001
[ Sat Jul 13 09:10:43 2024 ] 	Batch(700/6809) done. Loss: 0.0790  lr:0.000001
[ Sat Jul 13 09:11:01 2024 ] 	Batch(800/6809) done. Loss: 0.1822  lr:0.000001
[ Sat Jul 13 09:11:20 2024 ] 	Batch(900/6809) done. Loss: 0.2878  lr:0.000001
[ Sat Jul 13 09:11:38 2024 ] 
Training: Epoch [103/120], Step [999], Loss: 0.030792148783802986, Training Accuracy: 97.85000000000001
[ Sat Jul 13 09:11:38 2024 ] 	Batch(1000/6809) done. Loss: 0.0703  lr:0.000001
[ Sat Jul 13 09:11:56 2024 ] 	Batch(1100/6809) done. Loss: 0.4304  lr:0.000001
[ Sat Jul 13 09:12:15 2024 ] 	Batch(1200/6809) done. Loss: 0.0016  lr:0.000001
[ Sat Jul 13 09:12:33 2024 ] 	Batch(1300/6809) done. Loss: 0.0135  lr:0.000001
[ Sat Jul 13 09:12:51 2024 ] 	Batch(1400/6809) done. Loss: 0.0257  lr:0.000001
[ Sat Jul 13 09:13:10 2024 ] 
Training: Epoch [103/120], Step [1499], Loss: 0.11158652603626251, Training Accuracy: 97.80833333333334
[ Sat Jul 13 09:13:10 2024 ] 	Batch(1500/6809) done. Loss: 0.0118  lr:0.000001
[ Sat Jul 13 09:13:28 2024 ] 	Batch(1600/6809) done. Loss: 0.0019  lr:0.000001
[ Sat Jul 13 09:13:47 2024 ] 	Batch(1700/6809) done. Loss: 0.4947  lr:0.000001
[ Sat Jul 13 09:14:05 2024 ] 	Batch(1800/6809) done. Loss: 0.0960  lr:0.000001
[ Sat Jul 13 09:14:23 2024 ] 	Batch(1900/6809) done. Loss: 0.1071  lr:0.000001
[ Sat Jul 13 09:14:42 2024 ] 
Training: Epoch [103/120], Step [1999], Loss: 0.014877622947096825, Training Accuracy: 97.7
[ Sat Jul 13 09:14:42 2024 ] 	Batch(2000/6809) done. Loss: 0.0142  lr:0.000001
[ Sat Jul 13 09:15:00 2024 ] 	Batch(2100/6809) done. Loss: 0.0073  lr:0.000001
[ Sat Jul 13 09:15:19 2024 ] 	Batch(2200/6809) done. Loss: 0.0036  lr:0.000001
[ Sat Jul 13 09:15:37 2024 ] 	Batch(2300/6809) done. Loss: 0.0813  lr:0.000001
[ Sat Jul 13 09:15:56 2024 ] 	Batch(2400/6809) done. Loss: 0.0107  lr:0.000001
[ Sat Jul 13 09:16:14 2024 ] 
Training: Epoch [103/120], Step [2499], Loss: 0.007064910605549812, Training Accuracy: 97.69
[ Sat Jul 13 09:16:14 2024 ] 	Batch(2500/6809) done. Loss: 0.0876  lr:0.000001
[ Sat Jul 13 09:16:32 2024 ] 	Batch(2600/6809) done. Loss: 0.0185  lr:0.000001
[ Sat Jul 13 09:16:50 2024 ] 	Batch(2700/6809) done. Loss: 0.0302  lr:0.000001
[ Sat Jul 13 09:17:09 2024 ] 	Batch(2800/6809) done. Loss: 0.0349  lr:0.000001
[ Sat Jul 13 09:17:27 2024 ] 	Batch(2900/6809) done. Loss: 0.0436  lr:0.000001
[ Sat Jul 13 09:17:45 2024 ] 
Training: Epoch [103/120], Step [2999], Loss: 0.08838880062103271, Training Accuracy: 97.6875
[ Sat Jul 13 09:17:45 2024 ] 	Batch(3000/6809) done. Loss: 0.5053  lr:0.000001
[ Sat Jul 13 09:18:03 2024 ] 	Batch(3100/6809) done. Loss: 0.0150  lr:0.000001
[ Sat Jul 13 09:18:22 2024 ] 	Batch(3200/6809) done. Loss: 0.0967  lr:0.000001
[ Sat Jul 13 09:18:40 2024 ] 	Batch(3300/6809) done. Loss: 0.1424  lr:0.000001
[ Sat Jul 13 09:18:58 2024 ] 	Batch(3400/6809) done. Loss: 0.1960  lr:0.000001
[ Sat Jul 13 09:19:16 2024 ] 
Training: Epoch [103/120], Step [3499], Loss: 0.01885594241321087, Training Accuracy: 97.68928571428572
[ Sat Jul 13 09:19:16 2024 ] 	Batch(3500/6809) done. Loss: 0.0588  lr:0.000001
[ Sat Jul 13 09:19:35 2024 ] 	Batch(3600/6809) done. Loss: 0.1171  lr:0.000001
[ Sat Jul 13 09:19:53 2024 ] 	Batch(3700/6809) done. Loss: 0.1152  lr:0.000001
[ Sat Jul 13 09:20:11 2024 ] 	Batch(3800/6809) done. Loss: 0.0451  lr:0.000001
[ Sat Jul 13 09:20:29 2024 ] 	Batch(3900/6809) done. Loss: 0.0178  lr:0.000001
[ Sat Jul 13 09:20:47 2024 ] 
Training: Epoch [103/120], Step [3999], Loss: 0.01691768504679203, Training Accuracy: 97.628125
[ Sat Jul 13 09:20:48 2024 ] 	Batch(4000/6809) done. Loss: 0.0610  lr:0.000001
[ Sat Jul 13 09:21:06 2024 ] 	Batch(4100/6809) done. Loss: 0.0170  lr:0.000001
[ Sat Jul 13 09:21:24 2024 ] 	Batch(4200/6809) done. Loss: 0.0695  lr:0.000001
[ Sat Jul 13 09:21:42 2024 ] 	Batch(4300/6809) done. Loss: 0.1129  lr:0.000001
[ Sat Jul 13 09:22:01 2024 ] 	Batch(4400/6809) done. Loss: 0.0808  lr:0.000001
[ Sat Jul 13 09:22:19 2024 ] 
Training: Epoch [103/120], Step [4499], Loss: 0.003299430478364229, Training Accuracy: 97.63888888888889
[ Sat Jul 13 09:22:19 2024 ] 	Batch(4500/6809) done. Loss: 0.0195  lr:0.000001
[ Sat Jul 13 09:22:37 2024 ] 	Batch(4600/6809) done. Loss: 0.0346  lr:0.000001
[ Sat Jul 13 09:22:55 2024 ] 	Batch(4700/6809) done. Loss: 0.2064  lr:0.000001
[ Sat Jul 13 09:23:14 2024 ] 	Batch(4800/6809) done. Loss: 0.1310  lr:0.000001
[ Sat Jul 13 09:23:32 2024 ] 	Batch(4900/6809) done. Loss: 0.0016  lr:0.000001
[ Sat Jul 13 09:23:50 2024 ] 
Training: Epoch [103/120], Step [4999], Loss: 0.0966661274433136, Training Accuracy: 97.635
[ Sat Jul 13 09:23:50 2024 ] 	Batch(5000/6809) done. Loss: 0.0517  lr:0.000001
[ Sat Jul 13 09:24:08 2024 ] 	Batch(5100/6809) done. Loss: 0.0427  lr:0.000001
[ Sat Jul 13 09:24:27 2024 ] 	Batch(5200/6809) done. Loss: 0.0422  lr:0.000001
[ Sat Jul 13 09:24:46 2024 ] 	Batch(5300/6809) done. Loss: 0.0286  lr:0.000001
[ Sat Jul 13 09:25:05 2024 ] 	Batch(5400/6809) done. Loss: 0.1621  lr:0.000001
[ Sat Jul 13 09:25:23 2024 ] 
Training: Epoch [103/120], Step [5499], Loss: 0.01301696989685297, Training Accuracy: 97.63863636363637
[ Sat Jul 13 09:25:24 2024 ] 	Batch(5500/6809) done. Loss: 0.1024  lr:0.000001
[ Sat Jul 13 09:25:43 2024 ] 	Batch(5600/6809) done. Loss: 0.0845  lr:0.000001
[ Sat Jul 13 09:26:01 2024 ] 	Batch(5700/6809) done. Loss: 0.0075  lr:0.000001
[ Sat Jul 13 09:26:20 2024 ] 	Batch(5800/6809) done. Loss: 0.0864  lr:0.000001
[ Sat Jul 13 09:26:39 2024 ] 	Batch(5900/6809) done. Loss: 0.0272  lr:0.000001
[ Sat Jul 13 09:26:57 2024 ] 
Training: Epoch [103/120], Step [5999], Loss: 0.0007205853471532464, Training Accuracy: 97.61875
[ Sat Jul 13 09:26:57 2024 ] 	Batch(6000/6809) done. Loss: 0.1783  lr:0.000001
[ Sat Jul 13 09:27:16 2024 ] 	Batch(6100/6809) done. Loss: 0.0350  lr:0.000001
[ Sat Jul 13 09:27:34 2024 ] 	Batch(6200/6809) done. Loss: 0.0175  lr:0.000001
[ Sat Jul 13 09:27:52 2024 ] 	Batch(6300/6809) done. Loss: 0.0050  lr:0.000001
[ Sat Jul 13 09:28:10 2024 ] 	Batch(6400/6809) done. Loss: 0.2986  lr:0.000001
[ Sat Jul 13 09:28:29 2024 ] 
Training: Epoch [103/120], Step [6499], Loss: 0.17593804001808167, Training Accuracy: 97.59615384615384
[ Sat Jul 13 09:28:29 2024 ] 	Batch(6500/6809) done. Loss: 0.0064  lr:0.000001
[ Sat Jul 13 09:28:48 2024 ] 	Batch(6600/6809) done. Loss: 0.0648  lr:0.000001
[ Sat Jul 13 09:29:07 2024 ] 	Batch(6700/6809) done. Loss: 0.0337  lr:0.000001
[ Sat Jul 13 09:29:26 2024 ] 	Batch(6800/6809) done. Loss: 0.1472  lr:0.000001
[ Sat Jul 13 09:29:28 2024 ] 	Mean training loss: 0.0946.
[ Sat Jul 13 09:29:28 2024 ] 	Time consumption: [Data]01%, [Network]88%
[ Sat Jul 13 09:29:28 2024 ] Training epoch: 105
[ Sat Jul 13 09:29:28 2024 ] 	Batch(0/6809) done. Loss: 0.1187  lr:0.000001
[ Sat Jul 13 09:29:47 2024 ] 	Batch(100/6809) done. Loss: 0.0492  lr:0.000001
[ Sat Jul 13 09:30:05 2024 ] 	Batch(200/6809) done. Loss: 0.0433  lr:0.000001
[ Sat Jul 13 09:30:23 2024 ] 	Batch(300/6809) done. Loss: 0.0133  lr:0.000001
[ Sat Jul 13 09:30:41 2024 ] 	Batch(400/6809) done. Loss: 0.0957  lr:0.000001
[ Sat Jul 13 09:31:00 2024 ] 
Training: Epoch [104/120], Step [499], Loss: 0.1296476572751999, Training Accuracy: 97.3
[ Sat Jul 13 09:31:00 2024 ] 	Batch(500/6809) done. Loss: 0.0096  lr:0.000001
[ Sat Jul 13 09:31:18 2024 ] 	Batch(600/6809) done. Loss: 0.0399  lr:0.000001
[ Sat Jul 13 09:31:36 2024 ] 	Batch(700/6809) done. Loss: 0.0085  lr:0.000001
[ Sat Jul 13 09:31:55 2024 ] 	Batch(800/6809) done. Loss: 0.1465  lr:0.000001
[ Sat Jul 13 09:32:13 2024 ] 	Batch(900/6809) done. Loss: 0.0053  lr:0.000001
[ Sat Jul 13 09:32:31 2024 ] 
Training: Epoch [104/120], Step [999], Loss: 0.038082677870988846, Training Accuracy: 97.6125
[ Sat Jul 13 09:32:31 2024 ] 	Batch(1000/6809) done. Loss: 0.0997  lr:0.000001
[ Sat Jul 13 09:32:49 2024 ] 	Batch(1100/6809) done. Loss: 0.0593  lr:0.000001
[ Sat Jul 13 09:33:07 2024 ] 	Batch(1200/6809) done. Loss: 0.0278  lr:0.000001
[ Sat Jul 13 09:33:25 2024 ] 	Batch(1300/6809) done. Loss: 0.0401  lr:0.000001
[ Sat Jul 13 09:33:43 2024 ] 	Batch(1400/6809) done. Loss: 0.0337  lr:0.000001
[ Sat Jul 13 09:34:00 2024 ] 
Training: Epoch [104/120], Step [1499], Loss: 0.006029786542057991, Training Accuracy: 97.68333333333334
[ Sat Jul 13 09:34:01 2024 ] 	Batch(1500/6809) done. Loss: 0.0917  lr:0.000001
[ Sat Jul 13 09:34:19 2024 ] 	Batch(1600/6809) done. Loss: 0.0042  lr:0.000001
[ Sat Jul 13 09:34:36 2024 ] 	Batch(1700/6809) done. Loss: 0.0564  lr:0.000001
[ Sat Jul 13 09:34:54 2024 ] 	Batch(1800/6809) done. Loss: 0.0519  lr:0.000001
[ Sat Jul 13 09:35:12 2024 ] 	Batch(1900/6809) done. Loss: 0.0206  lr:0.000001
[ Sat Jul 13 09:35:30 2024 ] 
Training: Epoch [104/120], Step [1999], Loss: 0.04827234894037247, Training Accuracy: 97.5
[ Sat Jul 13 09:35:30 2024 ] 	Batch(2000/6809) done. Loss: 0.0525  lr:0.000001
[ Sat Jul 13 09:35:48 2024 ] 	Batch(2100/6809) done. Loss: 0.0845  lr:0.000001
[ Sat Jul 13 09:36:06 2024 ] 	Batch(2200/6809) done. Loss: 0.0024  lr:0.000001
[ Sat Jul 13 09:36:24 2024 ] 	Batch(2300/6809) done. Loss: 0.0282  lr:0.000001
[ Sat Jul 13 09:36:42 2024 ] 	Batch(2400/6809) done. Loss: 0.6489  lr:0.000001
[ Sat Jul 13 09:37:01 2024 ] 
Training: Epoch [104/120], Step [2499], Loss: 0.045685987919569016, Training Accuracy: 97.385
[ Sat Jul 13 09:37:01 2024 ] 	Batch(2500/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 09:37:19 2024 ] 	Batch(2600/6809) done. Loss: 0.0511  lr:0.000001
[ Sat Jul 13 09:37:36 2024 ] 	Batch(2700/6809) done. Loss: 0.0317  lr:0.000001
[ Sat Jul 13 09:37:55 2024 ] 	Batch(2800/6809) done. Loss: 0.0107  lr:0.000001
[ Sat Jul 13 09:38:13 2024 ] 	Batch(2900/6809) done. Loss: 0.0157  lr:0.000001
[ Sat Jul 13 09:38:31 2024 ] 
Training: Epoch [104/120], Step [2999], Loss: 0.014177752658724785, Training Accuracy: 97.42916666666666
[ Sat Jul 13 09:38:31 2024 ] 	Batch(3000/6809) done. Loss: 0.0114  lr:0.000001
[ Sat Jul 13 09:38:49 2024 ] 	Batch(3100/6809) done. Loss: 0.1798  lr:0.000001
[ Sat Jul 13 09:39:07 2024 ] 	Batch(3200/6809) done. Loss: 0.3847  lr:0.000001
[ Sat Jul 13 09:39:25 2024 ] 	Batch(3300/6809) done. Loss: 0.0298  lr:0.000001
[ Sat Jul 13 09:39:44 2024 ] 	Batch(3400/6809) done. Loss: 0.0945  lr:0.000001
[ Sat Jul 13 09:40:02 2024 ] 
Training: Epoch [104/120], Step [3499], Loss: 0.051916833966970444, Training Accuracy: 97.47857142857143
[ Sat Jul 13 09:40:02 2024 ] 	Batch(3500/6809) done. Loss: 0.0114  lr:0.000001
[ Sat Jul 13 09:40:20 2024 ] 	Batch(3600/6809) done. Loss: 0.0123  lr:0.000001
[ Sat Jul 13 09:40:38 2024 ] 	Batch(3700/6809) done. Loss: 0.0277  lr:0.000001
[ Sat Jul 13 09:40:56 2024 ] 	Batch(3800/6809) done. Loss: 0.0347  lr:0.000001
[ Sat Jul 13 09:41:14 2024 ] 	Batch(3900/6809) done. Loss: 0.4958  lr:0.000001
[ Sat Jul 13 09:41:31 2024 ] 
Training: Epoch [104/120], Step [3999], Loss: 0.011293943040072918, Training Accuracy: 97.528125
[ Sat Jul 13 09:41:32 2024 ] 	Batch(4000/6809) done. Loss: 0.0608  lr:0.000001
[ Sat Jul 13 09:41:50 2024 ] 	Batch(4100/6809) done. Loss: 0.0893  lr:0.000001
[ Sat Jul 13 09:42:08 2024 ] 	Batch(4200/6809) done. Loss: 0.0856  lr:0.000001
[ Sat Jul 13 09:42:25 2024 ] 	Batch(4300/6809) done. Loss: 0.0406  lr:0.000001
[ Sat Jul 13 09:42:43 2024 ] 	Batch(4400/6809) done. Loss: 0.0290  lr:0.000001
[ Sat Jul 13 09:43:01 2024 ] 
Training: Epoch [104/120], Step [4499], Loss: 0.03057934157550335, Training Accuracy: 97.53888888888889
[ Sat Jul 13 09:43:01 2024 ] 	Batch(4500/6809) done. Loss: 0.3666  lr:0.000001
[ Sat Jul 13 09:43:19 2024 ] 	Batch(4600/6809) done. Loss: 0.0644  lr:0.000001
[ Sat Jul 13 09:43:37 2024 ] 	Batch(4700/6809) done. Loss: 0.3414  lr:0.000001
[ Sat Jul 13 09:43:55 2024 ] 	Batch(4800/6809) done. Loss: 0.2542  lr:0.000001
[ Sat Jul 13 09:44:13 2024 ] 	Batch(4900/6809) done. Loss: 0.1685  lr:0.000001
[ Sat Jul 13 09:44:31 2024 ] 
Training: Epoch [104/120], Step [4999], Loss: 0.09347129613161087, Training Accuracy: 97.5475
[ Sat Jul 13 09:44:31 2024 ] 	Batch(5000/6809) done. Loss: 0.0581  lr:0.000001
[ Sat Jul 13 09:44:49 2024 ] 	Batch(5100/6809) done. Loss: 0.0064  lr:0.000001
[ Sat Jul 13 09:45:07 2024 ] 	Batch(5200/6809) done. Loss: 0.0675  lr:0.000001
[ Sat Jul 13 09:45:25 2024 ] 	Batch(5300/6809) done. Loss: 0.0059  lr:0.000001
[ Sat Jul 13 09:45:43 2024 ] 	Batch(5400/6809) done. Loss: 0.0482  lr:0.000001
[ Sat Jul 13 09:46:01 2024 ] 
Training: Epoch [104/120], Step [5499], Loss: 0.2509554326534271, Training Accuracy: 97.52499999999999
[ Sat Jul 13 09:46:01 2024 ] 	Batch(5500/6809) done. Loss: 0.1305  lr:0.000001
[ Sat Jul 13 09:46:19 2024 ] 	Batch(5600/6809) done. Loss: 0.0518  lr:0.000001
[ Sat Jul 13 09:46:37 2024 ] 	Batch(5700/6809) done. Loss: 0.0601  lr:0.000001
[ Sat Jul 13 09:46:55 2024 ] 	Batch(5800/6809) done. Loss: 0.0218  lr:0.000001
[ Sat Jul 13 09:47:13 2024 ] 	Batch(5900/6809) done. Loss: 0.1149  lr:0.000001
[ Sat Jul 13 09:47:31 2024 ] 
Training: Epoch [104/120], Step [5999], Loss: 0.007379072718322277, Training Accuracy: 97.5
[ Sat Jul 13 09:47:31 2024 ] 	Batch(6000/6809) done. Loss: 0.0229  lr:0.000001
[ Sat Jul 13 09:47:49 2024 ] 	Batch(6100/6809) done. Loss: 0.0496  lr:0.000001
[ Sat Jul 13 09:48:08 2024 ] 	Batch(6200/6809) done. Loss: 0.0366  lr:0.000001
[ Sat Jul 13 09:48:26 2024 ] 	Batch(6300/6809) done. Loss: 0.0157  lr:0.000001
[ Sat Jul 13 09:48:45 2024 ] 	Batch(6400/6809) done. Loss: 0.0137  lr:0.000001
[ Sat Jul 13 09:49:03 2024 ] 
Training: Epoch [104/120], Step [6499], Loss: 0.1245562955737114, Training Accuracy: 97.50769230769231
[ Sat Jul 13 09:49:03 2024 ] 	Batch(6500/6809) done. Loss: 0.1184  lr:0.000001
[ Sat Jul 13 09:49:21 2024 ] 	Batch(6600/6809) done. Loss: 0.0573  lr:0.000001
[ Sat Jul 13 09:49:39 2024 ] 	Batch(6700/6809) done. Loss: 0.0426  lr:0.000001
[ Sat Jul 13 09:49:57 2024 ] 	Batch(6800/6809) done. Loss: 0.0087  lr:0.000001
[ Sat Jul 13 09:49:59 2024 ] 	Mean training loss: 0.0968.
[ Sat Jul 13 09:49:59 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 09:49:59 2024 ] Training epoch: 106
[ Sat Jul 13 09:49:59 2024 ] 	Batch(0/6809) done. Loss: 0.0393  lr:0.000001
[ Sat Jul 13 09:50:17 2024 ] 	Batch(100/6809) done. Loss: 0.0591  lr:0.000001
[ Sat Jul 13 09:50:36 2024 ] 	Batch(200/6809) done. Loss: 0.0068  lr:0.000001
[ Sat Jul 13 09:50:54 2024 ] 	Batch(300/6809) done. Loss: 0.0170  lr:0.000001
[ Sat Jul 13 09:51:12 2024 ] 	Batch(400/6809) done. Loss: 0.1373  lr:0.000001
[ Sat Jul 13 09:51:30 2024 ] 
Training: Epoch [105/120], Step [499], Loss: 0.0807107537984848, Training Accuracy: 97.875
[ Sat Jul 13 09:51:30 2024 ] 	Batch(500/6809) done. Loss: 0.0301  lr:0.000001
[ Sat Jul 13 09:51:48 2024 ] 	Batch(600/6809) done. Loss: 0.1770  lr:0.000001
[ Sat Jul 13 09:52:06 2024 ] 	Batch(700/6809) done. Loss: 0.8388  lr:0.000001
[ Sat Jul 13 09:52:24 2024 ] 	Batch(800/6809) done. Loss: 0.0074  lr:0.000001
[ Sat Jul 13 09:52:42 2024 ] 	Batch(900/6809) done. Loss: 0.0919  lr:0.000001
[ Sat Jul 13 09:53:00 2024 ] 
Training: Epoch [105/120], Step [999], Loss: 0.03298589959740639, Training Accuracy: 97.6875
[ Sat Jul 13 09:53:00 2024 ] 	Batch(1000/6809) done. Loss: 0.0005  lr:0.000001
[ Sat Jul 13 09:53:18 2024 ] 	Batch(1100/6809) done. Loss: 0.2786  lr:0.000001
[ Sat Jul 13 09:53:36 2024 ] 	Batch(1200/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 09:53:55 2024 ] 	Batch(1300/6809) done. Loss: 0.0467  lr:0.000001
[ Sat Jul 13 09:54:13 2024 ] 	Batch(1400/6809) done. Loss: 0.0830  lr:0.000001
[ Sat Jul 13 09:54:32 2024 ] 
Training: Epoch [105/120], Step [1499], Loss: 0.09713304787874222, Training Accuracy: 97.575
[ Sat Jul 13 09:54:32 2024 ] 	Batch(1500/6809) done. Loss: 0.0299  lr:0.000001
[ Sat Jul 13 09:54:51 2024 ] 	Batch(1600/6809) done. Loss: 0.0525  lr:0.000001
[ Sat Jul 13 09:55:09 2024 ] 	Batch(1700/6809) done. Loss: 0.5584  lr:0.000001
[ Sat Jul 13 09:55:28 2024 ] 	Batch(1800/6809) done. Loss: 0.0116  lr:0.000001
[ Sat Jul 13 09:55:46 2024 ] 	Batch(1900/6809) done. Loss: 0.0366  lr:0.000001
[ Sat Jul 13 09:56:05 2024 ] 
Training: Epoch [105/120], Step [1999], Loss: 0.10351888090372086, Training Accuracy: 97.63125000000001
[ Sat Jul 13 09:56:05 2024 ] 	Batch(2000/6809) done. Loss: 0.0246  lr:0.000001
[ Sat Jul 13 09:56:23 2024 ] 	Batch(2100/6809) done. Loss: 0.1861  lr:0.000001
[ Sat Jul 13 09:56:41 2024 ] 	Batch(2200/6809) done. Loss: 0.2147  lr:0.000001
[ Sat Jul 13 09:56:59 2024 ] 	Batch(2300/6809) done. Loss: 0.0487  lr:0.000001
[ Sat Jul 13 09:57:17 2024 ] 	Batch(2400/6809) done. Loss: 0.0658  lr:0.000001
[ Sat Jul 13 09:57:34 2024 ] 
Training: Epoch [105/120], Step [2499], Loss: 0.22489432990550995, Training Accuracy: 97.595
[ Sat Jul 13 09:57:35 2024 ] 	Batch(2500/6809) done. Loss: 0.0190  lr:0.000001
[ Sat Jul 13 09:57:53 2024 ] 	Batch(2600/6809) done. Loss: 0.0203  lr:0.000001
[ Sat Jul 13 09:58:10 2024 ] 	Batch(2700/6809) done. Loss: 0.0146  lr:0.000001
[ Sat Jul 13 09:58:28 2024 ] 	Batch(2800/6809) done. Loss: 0.0094  lr:0.000001
[ Sat Jul 13 09:58:46 2024 ] 	Batch(2900/6809) done. Loss: 0.0955  lr:0.000001
[ Sat Jul 13 09:59:04 2024 ] 
Training: Epoch [105/120], Step [2999], Loss: 0.0496131032705307, Training Accuracy: 97.54583333333333
[ Sat Jul 13 09:59:04 2024 ] 	Batch(3000/6809) done. Loss: 0.0852  lr:0.000001
[ Sat Jul 13 09:59:22 2024 ] 	Batch(3100/6809) done. Loss: 0.9633  lr:0.000001
[ Sat Jul 13 09:59:40 2024 ] 	Batch(3200/6809) done. Loss: 0.1205  lr:0.000001
[ Sat Jul 13 09:59:59 2024 ] 	Batch(3300/6809) done. Loss: 0.0214  lr:0.000001
[ Sat Jul 13 10:00:18 2024 ] 	Batch(3400/6809) done. Loss: 0.2756  lr:0.000001
[ Sat Jul 13 10:00:36 2024 ] 
Training: Epoch [105/120], Step [3499], Loss: 0.008581905625760555, Training Accuracy: 97.54285714285714
[ Sat Jul 13 10:00:36 2024 ] 	Batch(3500/6809) done. Loss: 0.0231  lr:0.000001
[ Sat Jul 13 10:00:55 2024 ] 	Batch(3600/6809) done. Loss: 0.0913  lr:0.000001
[ Sat Jul 13 10:01:13 2024 ] 	Batch(3700/6809) done. Loss: 0.0352  lr:0.000001
[ Sat Jul 13 10:01:31 2024 ] 	Batch(3800/6809) done. Loss: 0.0059  lr:0.000001
[ Sat Jul 13 10:01:49 2024 ] 	Batch(3900/6809) done. Loss: 0.0262  lr:0.000001
[ Sat Jul 13 10:02:06 2024 ] 
Training: Epoch [105/120], Step [3999], Loss: 0.006697558332234621, Training Accuracy: 97.5625
[ Sat Jul 13 10:02:07 2024 ] 	Batch(4000/6809) done. Loss: 0.0222  lr:0.000001
[ Sat Jul 13 10:02:25 2024 ] 	Batch(4100/6809) done. Loss: 0.2246  lr:0.000001
[ Sat Jul 13 10:02:43 2024 ] 	Batch(4200/6809) done. Loss: 0.0650  lr:0.000001
[ Sat Jul 13 10:03:02 2024 ] 	Batch(4300/6809) done. Loss: 0.0056  lr:0.000001
[ Sat Jul 13 10:03:21 2024 ] 	Batch(4400/6809) done. Loss: 0.0464  lr:0.000001
[ Sat Jul 13 10:03:39 2024 ] 
Training: Epoch [105/120], Step [4499], Loss: 0.27498671412467957, Training Accuracy: 97.53333333333333
[ Sat Jul 13 10:03:39 2024 ] 	Batch(4500/6809) done. Loss: 0.0142  lr:0.000001
[ Sat Jul 13 10:03:58 2024 ] 	Batch(4600/6809) done. Loss: 0.0452  lr:0.000001
[ Sat Jul 13 10:04:16 2024 ] 	Batch(4700/6809) done. Loss: 0.1241  lr:0.000001
[ Sat Jul 13 10:04:35 2024 ] 	Batch(4800/6809) done. Loss: 0.0212  lr:0.000001
[ Sat Jul 13 10:04:54 2024 ] 	Batch(4900/6809) done. Loss: 0.1397  lr:0.000001
[ Sat Jul 13 10:05:12 2024 ] 
Training: Epoch [105/120], Step [4999], Loss: 0.18278908729553223, Training Accuracy: 97.57249999999999
[ Sat Jul 13 10:05:12 2024 ] 	Batch(5000/6809) done. Loss: 0.0244  lr:0.000001
[ Sat Jul 13 10:05:31 2024 ] 	Batch(5100/6809) done. Loss: 0.0374  lr:0.000001
[ Sat Jul 13 10:05:49 2024 ] 	Batch(5200/6809) done. Loss: 0.4303  lr:0.000001
[ Sat Jul 13 10:06:07 2024 ] 	Batch(5300/6809) done. Loss: 0.0273  lr:0.000001
[ Sat Jul 13 10:06:25 2024 ] 	Batch(5400/6809) done. Loss: 0.0341  lr:0.000001
[ Sat Jul 13 10:06:43 2024 ] 
Training: Epoch [105/120], Step [5499], Loss: 0.10609067976474762, Training Accuracy: 97.57272727272728
[ Sat Jul 13 10:06:43 2024 ] 	Batch(5500/6809) done. Loss: 0.0126  lr:0.000001
[ Sat Jul 13 10:07:01 2024 ] 	Batch(5600/6809) done. Loss: 0.0308  lr:0.000001
[ Sat Jul 13 10:07:19 2024 ] 	Batch(5700/6809) done. Loss: 0.2293  lr:0.000001
[ Sat Jul 13 10:07:37 2024 ] 	Batch(5800/6809) done. Loss: 0.0496  lr:0.000001
[ Sat Jul 13 10:07:55 2024 ] 	Batch(5900/6809) done. Loss: 0.4170  lr:0.000001
[ Sat Jul 13 10:08:13 2024 ] 
Training: Epoch [105/120], Step [5999], Loss: 0.14302729070186615, Training Accuracy: 97.54374999999999
[ Sat Jul 13 10:08:13 2024 ] 	Batch(6000/6809) done. Loss: 0.3474  lr:0.000001
[ Sat Jul 13 10:08:31 2024 ] 	Batch(6100/6809) done. Loss: 0.3412  lr:0.000001
[ Sat Jul 13 10:08:49 2024 ] 	Batch(6200/6809) done. Loss: 0.0473  lr:0.000001
[ Sat Jul 13 10:09:07 2024 ] 	Batch(6300/6809) done. Loss: 0.3419  lr:0.000001
[ Sat Jul 13 10:09:25 2024 ] 	Batch(6400/6809) done. Loss: 0.0094  lr:0.000001
[ Sat Jul 13 10:09:42 2024 ] 
Training: Epoch [105/120], Step [6499], Loss: 0.07669666409492493, Training Accuracy: 97.50961538461539
[ Sat Jul 13 10:09:43 2024 ] 	Batch(6500/6809) done. Loss: 0.0715  lr:0.000001
[ Sat Jul 13 10:10:01 2024 ] 	Batch(6600/6809) done. Loss: 0.0441  lr:0.000001
[ Sat Jul 13 10:10:18 2024 ] 	Batch(6700/6809) done. Loss: 0.0694  lr:0.000001
[ Sat Jul 13 10:10:37 2024 ] 	Batch(6800/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 10:10:38 2024 ] 	Mean training loss: 0.0963.
[ Sat Jul 13 10:10:38 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 10:10:38 2024 ] Training epoch: 107
[ Sat Jul 13 10:10:39 2024 ] 	Batch(0/6809) done. Loss: 0.0073  lr:0.000001
[ Sat Jul 13 10:10:57 2024 ] 	Batch(100/6809) done. Loss: 0.0221  lr:0.000001
[ Sat Jul 13 10:11:15 2024 ] 	Batch(200/6809) done. Loss: 0.0345  lr:0.000001
[ Sat Jul 13 10:11:33 2024 ] 	Batch(300/6809) done. Loss: 0.0229  lr:0.000001
[ Sat Jul 13 10:11:51 2024 ] 	Batch(400/6809) done. Loss: 0.0254  lr:0.000001
[ Sat Jul 13 10:12:09 2024 ] 
Training: Epoch [106/120], Step [499], Loss: 0.04963972419500351, Training Accuracy: 97.25
[ Sat Jul 13 10:12:09 2024 ] 	Batch(500/6809) done. Loss: 0.1133  lr:0.000001
[ Sat Jul 13 10:12:27 2024 ] 	Batch(600/6809) done. Loss: 0.2530  lr:0.000001
[ Sat Jul 13 10:12:45 2024 ] 	Batch(700/6809) done. Loss: 0.0047  lr:0.000001
[ Sat Jul 13 10:13:03 2024 ] 	Batch(800/6809) done. Loss: 0.0366  lr:0.000001
[ Sat Jul 13 10:13:21 2024 ] 	Batch(900/6809) done. Loss: 0.0556  lr:0.000001
[ Sat Jul 13 10:13:38 2024 ] 
Training: Epoch [106/120], Step [999], Loss: 0.003468928625807166, Training Accuracy: 97.48750000000001
[ Sat Jul 13 10:13:39 2024 ] 	Batch(1000/6809) done. Loss: 0.0153  lr:0.000001
[ Sat Jul 13 10:13:56 2024 ] 	Batch(1100/6809) done. Loss: 0.0071  lr:0.000001
[ Sat Jul 13 10:14:15 2024 ] 	Batch(1200/6809) done. Loss: 0.0416  lr:0.000001
[ Sat Jul 13 10:14:32 2024 ] 	Batch(1300/6809) done. Loss: 0.1302  lr:0.000001
[ Sat Jul 13 10:14:50 2024 ] 	Batch(1400/6809) done. Loss: 0.1462  lr:0.000001
[ Sat Jul 13 10:15:08 2024 ] 
Training: Epoch [106/120], Step [1499], Loss: 0.023450935259461403, Training Accuracy: 97.5
[ Sat Jul 13 10:15:08 2024 ] 	Batch(1500/6809) done. Loss: 0.0147  lr:0.000001
[ Sat Jul 13 10:15:26 2024 ] 	Batch(1600/6809) done. Loss: 0.0303  lr:0.000001
[ Sat Jul 13 10:15:44 2024 ] 	Batch(1700/6809) done. Loss: 0.3042  lr:0.000001
[ Sat Jul 13 10:16:02 2024 ] 	Batch(1800/6809) done. Loss: 0.0297  lr:0.000001
[ Sat Jul 13 10:16:20 2024 ] 	Batch(1900/6809) done. Loss: 0.0067  lr:0.000001
[ Sat Jul 13 10:16:38 2024 ] 
Training: Epoch [106/120], Step [1999], Loss: 0.023470964282751083, Training Accuracy: 97.46875
[ Sat Jul 13 10:16:38 2024 ] 	Batch(2000/6809) done. Loss: 0.0308  lr:0.000001
[ Sat Jul 13 10:16:57 2024 ] 	Batch(2100/6809) done. Loss: 0.0140  lr:0.000001
[ Sat Jul 13 10:17:15 2024 ] 	Batch(2200/6809) done. Loss: 0.0660  lr:0.000001
[ Sat Jul 13 10:17:34 2024 ] 	Batch(2300/6809) done. Loss: 0.0887  lr:0.000001
[ Sat Jul 13 10:17:53 2024 ] 	Batch(2400/6809) done. Loss: 0.0230  lr:0.000001
[ Sat Jul 13 10:18:11 2024 ] 
Training: Epoch [106/120], Step [2499], Loss: 0.055145714432001114, Training Accuracy: 97.605
[ Sat Jul 13 10:18:11 2024 ] 	Batch(2500/6809) done. Loss: 0.0296  lr:0.000001
[ Sat Jul 13 10:18:30 2024 ] 	Batch(2600/6809) done. Loss: 0.0161  lr:0.000001
[ Sat Jul 13 10:18:48 2024 ] 	Batch(2700/6809) done. Loss: 0.1000  lr:0.000001
[ Sat Jul 13 10:19:07 2024 ] 	Batch(2800/6809) done. Loss: 0.0007  lr:0.000001
[ Sat Jul 13 10:19:25 2024 ] 	Batch(2900/6809) done. Loss: 0.0735  lr:0.000001
[ Sat Jul 13 10:19:44 2024 ] 
Training: Epoch [106/120], Step [2999], Loss: 0.11644665151834488, Training Accuracy: 97.48750000000001
[ Sat Jul 13 10:19:44 2024 ] 	Batch(3000/6809) done. Loss: 0.0170  lr:0.000001
[ Sat Jul 13 10:20:03 2024 ] 	Batch(3100/6809) done. Loss: 0.1350  lr:0.000001
[ Sat Jul 13 10:20:21 2024 ] 	Batch(3200/6809) done. Loss: 0.0657  lr:0.000001
[ Sat Jul 13 10:20:40 2024 ] 	Batch(3300/6809) done. Loss: 0.1629  lr:0.000001
[ Sat Jul 13 10:20:58 2024 ] 	Batch(3400/6809) done. Loss: 0.0313  lr:0.000001
[ Sat Jul 13 10:21:17 2024 ] 
Training: Epoch [106/120], Step [3499], Loss: 0.04050711914896965, Training Accuracy: 97.5142857142857
[ Sat Jul 13 10:21:17 2024 ] 	Batch(3500/6809) done. Loss: 0.0219  lr:0.000001
[ Sat Jul 13 10:21:36 2024 ] 	Batch(3600/6809) done. Loss: 0.0047  lr:0.000001
[ Sat Jul 13 10:21:53 2024 ] 	Batch(3700/6809) done. Loss: 0.1475  lr:0.000001
[ Sat Jul 13 10:22:11 2024 ] 	Batch(3800/6809) done. Loss: 0.2605  lr:0.000001
[ Sat Jul 13 10:22:29 2024 ] 	Batch(3900/6809) done. Loss: 0.1569  lr:0.000001
[ Sat Jul 13 10:22:47 2024 ] 
Training: Epoch [106/120], Step [3999], Loss: 0.10102260112762451, Training Accuracy: 97.484375
[ Sat Jul 13 10:22:47 2024 ] 	Batch(4000/6809) done. Loss: 0.0814  lr:0.000001
[ Sat Jul 13 10:23:06 2024 ] 	Batch(4100/6809) done. Loss: 0.1387  lr:0.000001
[ Sat Jul 13 10:23:25 2024 ] 	Batch(4200/6809) done. Loss: 0.0507  lr:0.000001
[ Sat Jul 13 10:23:44 2024 ] 	Batch(4300/6809) done. Loss: 0.1191  lr:0.000001
[ Sat Jul 13 10:24:03 2024 ] 	Batch(4400/6809) done. Loss: 0.0102  lr:0.000001
[ Sat Jul 13 10:24:21 2024 ] 
Training: Epoch [106/120], Step [4499], Loss: 0.056784823536872864, Training Accuracy: 97.49722222222222
[ Sat Jul 13 10:24:21 2024 ] 	Batch(4500/6809) done. Loss: 0.0471  lr:0.000001
[ Sat Jul 13 10:24:40 2024 ] 	Batch(4600/6809) done. Loss: 0.0301  lr:0.000001
[ Sat Jul 13 10:24:58 2024 ] 	Batch(4700/6809) done. Loss: 0.1803  lr:0.000001
[ Sat Jul 13 10:25:16 2024 ] 	Batch(4800/6809) done. Loss: 0.0125  lr:0.000001
[ Sat Jul 13 10:25:34 2024 ] 	Batch(4900/6809) done. Loss: 0.0398  lr:0.000001
[ Sat Jul 13 10:25:51 2024 ] 
Training: Epoch [106/120], Step [4999], Loss: 0.07057459652423859, Training Accuracy: 97.52
[ Sat Jul 13 10:25:52 2024 ] 	Batch(5000/6809) done. Loss: 0.0098  lr:0.000001
[ Sat Jul 13 10:26:09 2024 ] 	Batch(5100/6809) done. Loss: 0.9303  lr:0.000001
[ Sat Jul 13 10:26:28 2024 ] 	Batch(5200/6809) done. Loss: 0.0154  lr:0.000001
[ Sat Jul 13 10:26:45 2024 ] 	Batch(5300/6809) done. Loss: 0.0330  lr:0.000001
[ Sat Jul 13 10:27:03 2024 ] 	Batch(5400/6809) done. Loss: 0.1384  lr:0.000001
[ Sat Jul 13 10:27:21 2024 ] 
Training: Epoch [106/120], Step [5499], Loss: 0.35387128591537476, Training Accuracy: 97.54545454545455
[ Sat Jul 13 10:27:21 2024 ] 	Batch(5500/6809) done. Loss: 0.0994  lr:0.000001
[ Sat Jul 13 10:27:39 2024 ] 	Batch(5600/6809) done. Loss: 0.0132  lr:0.000001
[ Sat Jul 13 10:27:58 2024 ] 	Batch(5700/6809) done. Loss: 0.0664  lr:0.000001
[ Sat Jul 13 10:28:16 2024 ] 	Batch(5800/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 10:28:35 2024 ] 	Batch(5900/6809) done. Loss: 0.1521  lr:0.000001
[ Sat Jul 13 10:28:53 2024 ] 
Training: Epoch [106/120], Step [5999], Loss: 0.1473538875579834, Training Accuracy: 97.55833333333334
[ Sat Jul 13 10:28:54 2024 ] 	Batch(6000/6809) done. Loss: 0.0167  lr:0.000001
[ Sat Jul 13 10:29:12 2024 ] 	Batch(6100/6809) done. Loss: 0.0541  lr:0.000001
[ Sat Jul 13 10:29:29 2024 ] 	Batch(6200/6809) done. Loss: 0.0767  lr:0.000001
[ Sat Jul 13 10:29:47 2024 ] 	Batch(6300/6809) done. Loss: 0.0228  lr:0.000001
[ Sat Jul 13 10:30:05 2024 ] 	Batch(6400/6809) done. Loss: 0.0465  lr:0.000001
[ Sat Jul 13 10:30:24 2024 ] 
Training: Epoch [106/120], Step [6499], Loss: 0.022449633106589317, Training Accuracy: 97.55384615384615
[ Sat Jul 13 10:30:24 2024 ] 	Batch(6500/6809) done. Loss: 0.1920  lr:0.000001
[ Sat Jul 13 10:30:42 2024 ] 	Batch(6600/6809) done. Loss: 0.0433  lr:0.000001
[ Sat Jul 13 10:31:01 2024 ] 	Batch(6700/6809) done. Loss: 0.0210  lr:0.000001
[ Sat Jul 13 10:31:19 2024 ] 	Batch(6800/6809) done. Loss: 0.0150  lr:0.000001
[ Sat Jul 13 10:31:20 2024 ] 	Mean training loss: 0.0948.
[ Sat Jul 13 10:31:20 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 10:31:21 2024 ] Training epoch: 108
[ Sat Jul 13 10:31:21 2024 ] 	Batch(0/6809) done. Loss: 0.0948  lr:0.000001
[ Sat Jul 13 10:31:39 2024 ] 	Batch(100/6809) done. Loss: 0.0388  lr:0.000001
[ Sat Jul 13 10:31:57 2024 ] 	Batch(200/6809) done. Loss: 0.0925  lr:0.000001
[ Sat Jul 13 10:32:15 2024 ] 	Batch(300/6809) done. Loss: 0.1265  lr:0.000001
[ Sat Jul 13 10:32:33 2024 ] 	Batch(400/6809) done. Loss: 0.0457  lr:0.000001
[ Sat Jul 13 10:32:51 2024 ] 
Training: Epoch [107/120], Step [499], Loss: 0.02258063480257988, Training Accuracy: 97.3
[ Sat Jul 13 10:32:51 2024 ] 	Batch(500/6809) done. Loss: 0.0258  lr:0.000001
[ Sat Jul 13 10:33:09 2024 ] 	Batch(600/6809) done. Loss: 0.0928  lr:0.000001
[ Sat Jul 13 10:33:27 2024 ] 	Batch(700/6809) done. Loss: 0.0125  lr:0.000001
[ Sat Jul 13 10:33:45 2024 ] 	Batch(800/6809) done. Loss: 0.0797  lr:0.000001
[ Sat Jul 13 10:34:03 2024 ] 	Batch(900/6809) done. Loss: 0.0716  lr:0.000001
[ Sat Jul 13 10:34:21 2024 ] 
Training: Epoch [107/120], Step [999], Loss: 0.049283720552921295, Training Accuracy: 97.41250000000001
[ Sat Jul 13 10:34:21 2024 ] 	Batch(1000/6809) done. Loss: 0.0872  lr:0.000001
[ Sat Jul 13 10:34:40 2024 ] 	Batch(1100/6809) done. Loss: 0.9219  lr:0.000001
[ Sat Jul 13 10:34:58 2024 ] 	Batch(1200/6809) done. Loss: 0.0650  lr:0.000001
[ Sat Jul 13 10:35:16 2024 ] 	Batch(1300/6809) done. Loss: 0.1584  lr:0.000001
[ Sat Jul 13 10:35:34 2024 ] 	Batch(1400/6809) done. Loss: 0.0229  lr:0.000001
[ Sat Jul 13 10:35:51 2024 ] 
Training: Epoch [107/120], Step [1499], Loss: 0.1971207708120346, Training Accuracy: 97.44166666666668
[ Sat Jul 13 10:35:52 2024 ] 	Batch(1500/6809) done. Loss: 0.0241  lr:0.000001
[ Sat Jul 13 10:36:10 2024 ] 	Batch(1600/6809) done. Loss: 0.4747  lr:0.000001
[ Sat Jul 13 10:36:27 2024 ] 	Batch(1700/6809) done. Loss: 0.2564  lr:0.000001
[ Sat Jul 13 10:36:45 2024 ] 	Batch(1800/6809) done. Loss: 0.2635  lr:0.000001
[ Sat Jul 13 10:37:03 2024 ] 	Batch(1900/6809) done. Loss: 0.0503  lr:0.000001
[ Sat Jul 13 10:37:21 2024 ] 
Training: Epoch [107/120], Step [1999], Loss: 0.0054907239973545074, Training Accuracy: 97.52499999999999
[ Sat Jul 13 10:37:21 2024 ] 	Batch(2000/6809) done. Loss: 0.0026  lr:0.000001
[ Sat Jul 13 10:37:39 2024 ] 	Batch(2100/6809) done. Loss: 0.1140  lr:0.000001
[ Sat Jul 13 10:37:57 2024 ] 	Batch(2200/6809) done. Loss: 0.0290  lr:0.000001
[ Sat Jul 13 10:38:15 2024 ] 	Batch(2300/6809) done. Loss: 0.0213  lr:0.000001
[ Sat Jul 13 10:38:33 2024 ] 	Batch(2400/6809) done. Loss: 0.1313  lr:0.000001
[ Sat Jul 13 10:38:51 2024 ] 
Training: Epoch [107/120], Step [2499], Loss: 0.06060784310102463, Training Accuracy: 97.505
[ Sat Jul 13 10:38:51 2024 ] 	Batch(2500/6809) done. Loss: 0.0627  lr:0.000001
[ Sat Jul 13 10:39:09 2024 ] 	Batch(2600/6809) done. Loss: 0.0268  lr:0.000001
[ Sat Jul 13 10:39:27 2024 ] 	Batch(2700/6809) done. Loss: 0.2217  lr:0.000001
[ Sat Jul 13 10:39:45 2024 ] 	Batch(2800/6809) done. Loss: 0.1211  lr:0.000001
[ Sat Jul 13 10:40:03 2024 ] 	Batch(2900/6809) done. Loss: 0.5681  lr:0.000001
[ Sat Jul 13 10:40:20 2024 ] 
Training: Epoch [107/120], Step [2999], Loss: 0.03507262095808983, Training Accuracy: 97.5
[ Sat Jul 13 10:40:21 2024 ] 	Batch(3000/6809) done. Loss: 0.0277  lr:0.000001
[ Sat Jul 13 10:40:39 2024 ] 	Batch(3100/6809) done. Loss: 0.0193  lr:0.000001
[ Sat Jul 13 10:40:57 2024 ] 	Batch(3200/6809) done. Loss: 0.0540  lr:0.000001
[ Sat Jul 13 10:41:15 2024 ] 	Batch(3300/6809) done. Loss: 0.0212  lr:0.000001
[ Sat Jul 13 10:41:33 2024 ] 	Batch(3400/6809) done. Loss: 0.0353  lr:0.000001
[ Sat Jul 13 10:41:50 2024 ] 
Training: Epoch [107/120], Step [3499], Loss: 0.26949557662010193, Training Accuracy: 97.53928571428571
[ Sat Jul 13 10:41:50 2024 ] 	Batch(3500/6809) done. Loss: 0.0366  lr:0.000001
[ Sat Jul 13 10:42:09 2024 ] 	Batch(3600/6809) done. Loss: 0.0470  lr:0.000001
[ Sat Jul 13 10:42:27 2024 ] 	Batch(3700/6809) done. Loss: 0.2281  lr:0.000001
[ Sat Jul 13 10:42:46 2024 ] 	Batch(3800/6809) done. Loss: 0.5483  lr:0.000001
[ Sat Jul 13 10:43:04 2024 ] 	Batch(3900/6809) done. Loss: 0.2442  lr:0.000001
[ Sat Jul 13 10:43:23 2024 ] 
Training: Epoch [107/120], Step [3999], Loss: 0.015693770721554756, Training Accuracy: 97.53125
[ Sat Jul 13 10:43:23 2024 ] 	Batch(4000/6809) done. Loss: 0.1318  lr:0.000001
[ Sat Jul 13 10:43:41 2024 ] 	Batch(4100/6809) done. Loss: 0.0061  lr:0.000001
[ Sat Jul 13 10:43:59 2024 ] 	Batch(4200/6809) done. Loss: 0.0213  lr:0.000001
[ Sat Jul 13 10:44:17 2024 ] 	Batch(4300/6809) done. Loss: 0.0095  lr:0.000001
[ Sat Jul 13 10:44:35 2024 ] 	Batch(4400/6809) done. Loss: 0.0059  lr:0.000001
[ Sat Jul 13 10:44:53 2024 ] 
Training: Epoch [107/120], Step [4499], Loss: 0.4174805283546448, Training Accuracy: 97.54166666666667
[ Sat Jul 13 10:44:53 2024 ] 	Batch(4500/6809) done. Loss: 0.0747  lr:0.000001
[ Sat Jul 13 10:45:11 2024 ] 	Batch(4600/6809) done. Loss: 0.0221  lr:0.000001
[ Sat Jul 13 10:45:29 2024 ] 	Batch(4700/6809) done. Loss: 0.0407  lr:0.000001
[ Sat Jul 13 10:45:47 2024 ] 	Batch(4800/6809) done. Loss: 0.0105  lr:0.000001
[ Sat Jul 13 10:46:05 2024 ] 	Batch(4900/6809) done. Loss: 0.0165  lr:0.000001
[ Sat Jul 13 10:46:22 2024 ] 
Training: Epoch [107/120], Step [4999], Loss: 0.03405892848968506, Training Accuracy: 97.5125
[ Sat Jul 13 10:46:23 2024 ] 	Batch(5000/6809) done. Loss: 0.0281  lr:0.000001
[ Sat Jul 13 10:46:41 2024 ] 	Batch(5100/6809) done. Loss: 0.0181  lr:0.000001
[ Sat Jul 13 10:46:59 2024 ] 	Batch(5200/6809) done. Loss: 0.0199  lr:0.000001
[ Sat Jul 13 10:47:17 2024 ] 	Batch(5300/6809) done. Loss: 0.1350  lr:0.000001
[ Sat Jul 13 10:47:36 2024 ] 	Batch(5400/6809) done. Loss: 0.0254  lr:0.000001
[ Sat Jul 13 10:47:54 2024 ] 
Training: Epoch [107/120], Step [5499], Loss: 0.018394462764263153, Training Accuracy: 97.52954545454546
[ Sat Jul 13 10:47:54 2024 ] 	Batch(5500/6809) done. Loss: 0.1748  lr:0.000001
[ Sat Jul 13 10:48:13 2024 ] 	Batch(5600/6809) done. Loss: 0.0408  lr:0.000001
[ Sat Jul 13 10:48:31 2024 ] 	Batch(5700/6809) done. Loss: 0.0347  lr:0.000001
[ Sat Jul 13 10:48:48 2024 ] 	Batch(5800/6809) done. Loss: 0.0157  lr:0.000001
[ Sat Jul 13 10:49:06 2024 ] 	Batch(5900/6809) done. Loss: 0.0703  lr:0.000001
[ Sat Jul 13 10:49:24 2024 ] 
Training: Epoch [107/120], Step [5999], Loss: 0.16503357887268066, Training Accuracy: 97.52708333333334
[ Sat Jul 13 10:49:24 2024 ] 	Batch(6000/6809) done. Loss: 0.1289  lr:0.000001
[ Sat Jul 13 10:49:42 2024 ] 	Batch(6100/6809) done. Loss: 0.0127  lr:0.000001
[ Sat Jul 13 10:50:00 2024 ] 	Batch(6200/6809) done. Loss: 0.0327  lr:0.000001
[ Sat Jul 13 10:50:18 2024 ] 	Batch(6300/6809) done. Loss: 0.2484  lr:0.000001
[ Sat Jul 13 10:50:36 2024 ] 	Batch(6400/6809) done. Loss: 0.0193  lr:0.000001
[ Sat Jul 13 10:50:54 2024 ] 
Training: Epoch [107/120], Step [6499], Loss: 0.19239990413188934, Training Accuracy: 97.52884615384615
[ Sat Jul 13 10:50:54 2024 ] 	Batch(6500/6809) done. Loss: 0.0432  lr:0.000001
[ Sat Jul 13 10:51:12 2024 ] 	Batch(6600/6809) done. Loss: 0.0164  lr:0.000001
[ Sat Jul 13 10:51:30 2024 ] 	Batch(6700/6809) done. Loss: 0.1252  lr:0.000001
[ Sat Jul 13 10:51:48 2024 ] 	Batch(6800/6809) done. Loss: 0.1091  lr:0.000001
[ Sat Jul 13 10:51:50 2024 ] 	Mean training loss: 0.0975.
[ Sat Jul 13 10:51:50 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 10:51:50 2024 ] Training epoch: 109
[ Sat Jul 13 10:51:50 2024 ] 	Batch(0/6809) done. Loss: 0.5792  lr:0.000001
[ Sat Jul 13 10:52:08 2024 ] 	Batch(100/6809) done. Loss: 0.4053  lr:0.000001
[ Sat Jul 13 10:52:26 2024 ] 	Batch(200/6809) done. Loss: 0.1260  lr:0.000001
[ Sat Jul 13 10:52:44 2024 ] 	Batch(300/6809) done. Loss: 0.0189  lr:0.000001
[ Sat Jul 13 10:53:02 2024 ] 	Batch(400/6809) done. Loss: 0.2234  lr:0.000001
[ Sat Jul 13 10:53:20 2024 ] 
Training: Epoch [108/120], Step [499], Loss: 0.18413181602954865, Training Accuracy: 97.35000000000001
[ Sat Jul 13 10:53:20 2024 ] 	Batch(500/6809) done. Loss: 0.0099  lr:0.000001
[ Sat Jul 13 10:53:38 2024 ] 	Batch(600/6809) done. Loss: 0.0702  lr:0.000001
[ Sat Jul 13 10:53:56 2024 ] 	Batch(700/6809) done. Loss: 0.0107  lr:0.000001
[ Sat Jul 13 10:54:14 2024 ] 	Batch(800/6809) done. Loss: 0.2374  lr:0.000001
[ Sat Jul 13 10:54:32 2024 ] 	Batch(900/6809) done. Loss: 0.1463  lr:0.000001
[ Sat Jul 13 10:54:50 2024 ] 
Training: Epoch [108/120], Step [999], Loss: 0.16367438435554504, Training Accuracy: 97.625
[ Sat Jul 13 10:54:50 2024 ] 	Batch(1000/6809) done. Loss: 0.0059  lr:0.000001
[ Sat Jul 13 10:55:08 2024 ] 	Batch(1100/6809) done. Loss: 0.1407  lr:0.000001
[ Sat Jul 13 10:55:26 2024 ] 	Batch(1200/6809) done. Loss: 0.0714  lr:0.000001
[ Sat Jul 13 10:55:44 2024 ] 	Batch(1300/6809) done. Loss: 0.0677  lr:0.000001
[ Sat Jul 13 10:56:02 2024 ] 	Batch(1400/6809) done. Loss: 0.0278  lr:0.000001
[ Sat Jul 13 10:56:20 2024 ] 
Training: Epoch [108/120], Step [1499], Loss: 0.17542089521884918, Training Accuracy: 97.475
[ Sat Jul 13 10:56:20 2024 ] 	Batch(1500/6809) done. Loss: 0.0735  lr:0.000001
[ Sat Jul 13 10:56:38 2024 ] 	Batch(1600/6809) done. Loss: 0.0888  lr:0.000001
[ Sat Jul 13 10:56:56 2024 ] 	Batch(1700/6809) done. Loss: 0.0020  lr:0.000001
[ Sat Jul 13 10:57:14 2024 ] 	Batch(1800/6809) done. Loss: 0.0215  lr:0.000001
[ Sat Jul 13 10:57:31 2024 ] 	Batch(1900/6809) done. Loss: 0.0308  lr:0.000001
[ Sat Jul 13 10:57:49 2024 ] 
Training: Epoch [108/120], Step [1999], Loss: 0.06038712337613106, Training Accuracy: 97.52499999999999
[ Sat Jul 13 10:57:49 2024 ] 	Batch(2000/6809) done. Loss: 0.0268  lr:0.000001
[ Sat Jul 13 10:58:07 2024 ] 	Batch(2100/6809) done. Loss: 0.0088  lr:0.000001
[ Sat Jul 13 10:58:25 2024 ] 	Batch(2200/6809) done. Loss: 0.0373  lr:0.000001
[ Sat Jul 13 10:58:43 2024 ] 	Batch(2300/6809) done. Loss: 0.0781  lr:0.000001
[ Sat Jul 13 10:59:01 2024 ] 	Batch(2400/6809) done. Loss: 0.1767  lr:0.000001
[ Sat Jul 13 10:59:19 2024 ] 
Training: Epoch [108/120], Step [2499], Loss: 0.013743402436375618, Training Accuracy: 97.595
[ Sat Jul 13 10:59:19 2024 ] 	Batch(2500/6809) done. Loss: 0.0023  lr:0.000001
[ Sat Jul 13 10:59:37 2024 ] 	Batch(2600/6809) done. Loss: 0.3328  lr:0.000001
[ Sat Jul 13 10:59:55 2024 ] 	Batch(2700/6809) done. Loss: 0.0222  lr:0.000001
[ Sat Jul 13 11:00:13 2024 ] 	Batch(2800/6809) done. Loss: 0.1902  lr:0.000001
[ Sat Jul 13 11:00:31 2024 ] 	Batch(2900/6809) done. Loss: 0.0776  lr:0.000001
[ Sat Jul 13 11:00:50 2024 ] 
Training: Epoch [108/120], Step [2999], Loss: 0.010679285041987896, Training Accuracy: 97.57916666666667
[ Sat Jul 13 11:00:50 2024 ] 	Batch(3000/6809) done. Loss: 0.0683  lr:0.000001
[ Sat Jul 13 11:01:09 2024 ] 	Batch(3100/6809) done. Loss: 0.1700  lr:0.000001
[ Sat Jul 13 11:01:27 2024 ] 	Batch(3200/6809) done. Loss: 0.0254  lr:0.000001
[ Sat Jul 13 11:01:46 2024 ] 	Batch(3300/6809) done. Loss: 0.2557  lr:0.000001
[ Sat Jul 13 11:02:04 2024 ] 	Batch(3400/6809) done. Loss: 0.0208  lr:0.000001
[ Sat Jul 13 11:02:23 2024 ] 
Training: Epoch [108/120], Step [3499], Loss: 0.054081179201602936, Training Accuracy: 97.55714285714285
[ Sat Jul 13 11:02:23 2024 ] 	Batch(3500/6809) done. Loss: 0.1066  lr:0.000001
[ Sat Jul 13 11:02:42 2024 ] 	Batch(3600/6809) done. Loss: 0.0047  lr:0.000001
[ Sat Jul 13 11:03:00 2024 ] 	Batch(3700/6809) done. Loss: 0.0171  lr:0.000001
[ Sat Jul 13 11:03:17 2024 ] 	Batch(3800/6809) done. Loss: 0.0849  lr:0.000001
[ Sat Jul 13 11:03:35 2024 ] 	Batch(3900/6809) done. Loss: 0.1327  lr:0.000001
[ Sat Jul 13 11:03:53 2024 ] 
Training: Epoch [108/120], Step [3999], Loss: 0.2165181040763855, Training Accuracy: 97.53125
[ Sat Jul 13 11:03:54 2024 ] 	Batch(4000/6809) done. Loss: 0.1756  lr:0.000001
[ Sat Jul 13 11:04:11 2024 ] 	Batch(4100/6809) done. Loss: 0.0546  lr:0.000001
[ Sat Jul 13 11:04:29 2024 ] 	Batch(4200/6809) done. Loss: 0.0039  lr:0.000001
[ Sat Jul 13 11:04:47 2024 ] 	Batch(4300/6809) done. Loss: 0.0966  lr:0.000001
[ Sat Jul 13 11:05:05 2024 ] 	Batch(4400/6809) done. Loss: 0.0150  lr:0.000001
[ Sat Jul 13 11:05:23 2024 ] 
Training: Epoch [108/120], Step [4499], Loss: 0.18628990650177002, Training Accuracy: 97.58055555555556
[ Sat Jul 13 11:05:23 2024 ] 	Batch(4500/6809) done. Loss: 0.0919  lr:0.000001
[ Sat Jul 13 11:05:41 2024 ] 	Batch(4600/6809) done. Loss: 0.0312  lr:0.000001
[ Sat Jul 13 11:05:59 2024 ] 	Batch(4700/6809) done. Loss: 0.0319  lr:0.000001
[ Sat Jul 13 11:06:17 2024 ] 	Batch(4800/6809) done. Loss: 0.0271  lr:0.000001
[ Sat Jul 13 11:06:36 2024 ] 	Batch(4900/6809) done. Loss: 0.0851  lr:0.000001
[ Sat Jul 13 11:06:54 2024 ] 
Training: Epoch [108/120], Step [4999], Loss: 0.017756251618266106, Training Accuracy: 97.575
[ Sat Jul 13 11:06:54 2024 ] 	Batch(5000/6809) done. Loss: 0.3379  lr:0.000001
[ Sat Jul 13 11:07:12 2024 ] 	Batch(5100/6809) done. Loss: 0.0161  lr:0.000001
[ Sat Jul 13 11:07:30 2024 ] 	Batch(5200/6809) done. Loss: 0.0356  lr:0.000001
[ Sat Jul 13 11:07:49 2024 ] 	Batch(5300/6809) done. Loss: 0.2568  lr:0.000001
[ Sat Jul 13 11:08:07 2024 ] 	Batch(5400/6809) done. Loss: 0.0492  lr:0.000001
[ Sat Jul 13 11:08:26 2024 ] 
Training: Epoch [108/120], Step [5499], Loss: 0.42710772156715393, Training Accuracy: 97.56818181818183
[ Sat Jul 13 11:08:26 2024 ] 	Batch(5500/6809) done. Loss: 0.2077  lr:0.000001
[ Sat Jul 13 11:08:44 2024 ] 	Batch(5600/6809) done. Loss: 0.0818  lr:0.000001
[ Sat Jul 13 11:09:02 2024 ] 	Batch(5700/6809) done. Loss: 0.0885  lr:0.000001
[ Sat Jul 13 11:09:20 2024 ] 	Batch(5800/6809) done. Loss: 0.0341  lr:0.000001
[ Sat Jul 13 11:09:38 2024 ] 	Batch(5900/6809) done. Loss: 0.0527  lr:0.000001
[ Sat Jul 13 11:09:56 2024 ] 
Training: Epoch [108/120], Step [5999], Loss: 0.004378680605441332, Training Accuracy: 97.58333333333333
[ Sat Jul 13 11:09:56 2024 ] 	Batch(6000/6809) done. Loss: 0.0314  lr:0.000001
[ Sat Jul 13 11:10:14 2024 ] 	Batch(6100/6809) done. Loss: 0.2080  lr:0.000001
[ Sat Jul 13 11:10:32 2024 ] 	Batch(6200/6809) done. Loss: 0.0454  lr:0.000001
[ Sat Jul 13 11:10:50 2024 ] 	Batch(6300/6809) done. Loss: 0.0202  lr:0.000001
[ Sat Jul 13 11:11:08 2024 ] 	Batch(6400/6809) done. Loss: 0.0778  lr:0.000001
[ Sat Jul 13 11:11:26 2024 ] 
Training: Epoch [108/120], Step [6499], Loss: 0.038252923637628555, Training Accuracy: 97.59038461538462
[ Sat Jul 13 11:11:26 2024 ] 	Batch(6500/6809) done. Loss: 0.0058  lr:0.000001
[ Sat Jul 13 11:11:45 2024 ] 	Batch(6600/6809) done. Loss: 0.0399  lr:0.000001
[ Sat Jul 13 11:12:04 2024 ] 	Batch(6700/6809) done. Loss: 0.0155  lr:0.000001
[ Sat Jul 13 11:12:22 2024 ] 	Batch(6800/6809) done. Loss: 0.0231  lr:0.000001
[ Sat Jul 13 11:12:24 2024 ] 	Mean training loss: 0.0944.
[ Sat Jul 13 11:12:24 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 11:12:24 2024 ] Training epoch: 110
[ Sat Jul 13 11:12:25 2024 ] 	Batch(0/6809) done. Loss: 0.0281  lr:0.000001
[ Sat Jul 13 11:12:42 2024 ] 	Batch(100/6809) done. Loss: 0.0104  lr:0.000001
[ Sat Jul 13 11:13:00 2024 ] 	Batch(200/6809) done. Loss: 0.0115  lr:0.000001
[ Sat Jul 13 11:13:18 2024 ] 	Batch(300/6809) done. Loss: 0.0420  lr:0.000001
[ Sat Jul 13 11:13:36 2024 ] 	Batch(400/6809) done. Loss: 0.0146  lr:0.000001
[ Sat Jul 13 11:13:54 2024 ] 
Training: Epoch [109/120], Step [499], Loss: 0.026586750522255898, Training Accuracy: 97.45
[ Sat Jul 13 11:13:54 2024 ] 	Batch(500/6809) done. Loss: 0.0640  lr:0.000001
[ Sat Jul 13 11:14:12 2024 ] 	Batch(600/6809) done. Loss: 0.1443  lr:0.000001
[ Sat Jul 13 11:14:30 2024 ] 	Batch(700/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 11:14:48 2024 ] 	Batch(800/6809) done. Loss: 0.1601  lr:0.000001
[ Sat Jul 13 11:15:06 2024 ] 	Batch(900/6809) done. Loss: 0.0161  lr:0.000001
[ Sat Jul 13 11:15:23 2024 ] 
Training: Epoch [109/120], Step [999], Loss: 0.06177481263875961, Training Accuracy: 97.675
[ Sat Jul 13 11:15:24 2024 ] 	Batch(1000/6809) done. Loss: 0.2489  lr:0.000001
[ Sat Jul 13 11:15:41 2024 ] 	Batch(1100/6809) done. Loss: 0.0037  lr:0.000001
[ Sat Jul 13 11:16:00 2024 ] 	Batch(1200/6809) done. Loss: 0.1245  lr:0.000001
[ Sat Jul 13 11:16:17 2024 ] 	Batch(1300/6809) done. Loss: 0.0545  lr:0.000001
[ Sat Jul 13 11:16:35 2024 ] 	Batch(1400/6809) done. Loss: 0.2263  lr:0.000001
[ Sat Jul 13 11:16:53 2024 ] 
Training: Epoch [109/120], Step [1499], Loss: 0.04866372048854828, Training Accuracy: 97.625
[ Sat Jul 13 11:16:53 2024 ] 	Batch(1500/6809) done. Loss: 0.6490  lr:0.000001
[ Sat Jul 13 11:17:11 2024 ] 	Batch(1600/6809) done. Loss: 0.0149  lr:0.000001
[ Sat Jul 13 11:17:29 2024 ] 	Batch(1700/6809) done. Loss: 0.0073  lr:0.000001
[ Sat Jul 13 11:17:47 2024 ] 	Batch(1800/6809) done. Loss: 0.0707  lr:0.000001
[ Sat Jul 13 11:18:05 2024 ] 	Batch(1900/6809) done. Loss: 0.0737  lr:0.000001
[ Sat Jul 13 11:18:23 2024 ] 
Training: Epoch [109/120], Step [1999], Loss: 0.18829286098480225, Training Accuracy: 97.55
[ Sat Jul 13 11:18:23 2024 ] 	Batch(2000/6809) done. Loss: 0.1235  lr:0.000001
[ Sat Jul 13 11:18:41 2024 ] 	Batch(2100/6809) done. Loss: 0.3219  lr:0.000001
[ Sat Jul 13 11:18:59 2024 ] 	Batch(2200/6809) done. Loss: 0.2402  lr:0.000001
[ Sat Jul 13 11:19:17 2024 ] 	Batch(2300/6809) done. Loss: 0.1737  lr:0.000001
[ Sat Jul 13 11:19:35 2024 ] 	Batch(2400/6809) done. Loss: 0.0330  lr:0.000001
[ Sat Jul 13 11:19:52 2024 ] 
Training: Epoch [109/120], Step [2499], Loss: 0.026241328567266464, Training Accuracy: 97.575
[ Sat Jul 13 11:19:53 2024 ] 	Batch(2500/6809) done. Loss: 0.5186  lr:0.000001
[ Sat Jul 13 11:20:11 2024 ] 	Batch(2600/6809) done. Loss: 0.0708  lr:0.000001
[ Sat Jul 13 11:20:28 2024 ] 	Batch(2700/6809) done. Loss: 0.3704  lr:0.000001
[ Sat Jul 13 11:20:46 2024 ] 	Batch(2800/6809) done. Loss: 0.2280  lr:0.000001
[ Sat Jul 13 11:21:04 2024 ] 	Batch(2900/6809) done. Loss: 0.0372  lr:0.000001
[ Sat Jul 13 11:21:22 2024 ] 
Training: Epoch [109/120], Step [2999], Loss: 0.07919777929782867, Training Accuracy: 97.60833333333333
[ Sat Jul 13 11:21:22 2024 ] 	Batch(3000/6809) done. Loss: 0.1288  lr:0.000001
[ Sat Jul 13 11:21:40 2024 ] 	Batch(3100/6809) done. Loss: 0.0081  lr:0.000001
[ Sat Jul 13 11:21:58 2024 ] 	Batch(3200/6809) done. Loss: 0.0238  lr:0.000001
[ Sat Jul 13 11:22:16 2024 ] 	Batch(3300/6809) done. Loss: 0.0404  lr:0.000001
[ Sat Jul 13 11:22:34 2024 ] 	Batch(3400/6809) done. Loss: 0.3154  lr:0.000001
[ Sat Jul 13 11:22:52 2024 ] 
Training: Epoch [109/120], Step [3499], Loss: 0.03958631679415703, Training Accuracy: 97.63571428571429
[ Sat Jul 13 11:22:52 2024 ] 	Batch(3500/6809) done. Loss: 0.0268  lr:0.000001
[ Sat Jul 13 11:23:10 2024 ] 	Batch(3600/6809) done. Loss: 0.0035  lr:0.000001
[ Sat Jul 13 11:23:28 2024 ] 	Batch(3700/6809) done. Loss: 0.1160  lr:0.000001
[ Sat Jul 13 11:23:46 2024 ] 	Batch(3800/6809) done. Loss: 0.0369  lr:0.000001
[ Sat Jul 13 11:24:04 2024 ] 	Batch(3900/6809) done. Loss: 0.0609  lr:0.000001
[ Sat Jul 13 11:24:22 2024 ] 
Training: Epoch [109/120], Step [3999], Loss: 0.046102944761514664, Training Accuracy: 97.578125
[ Sat Jul 13 11:24:22 2024 ] 	Batch(4000/6809) done. Loss: 0.0149  lr:0.000001
[ Sat Jul 13 11:24:40 2024 ] 	Batch(4100/6809) done. Loss: 0.0517  lr:0.000001
[ Sat Jul 13 11:24:58 2024 ] 	Batch(4200/6809) done. Loss: 0.0417  lr:0.000001
[ Sat Jul 13 11:25:16 2024 ] 	Batch(4300/6809) done. Loss: 0.1663  lr:0.000001
[ Sat Jul 13 11:25:34 2024 ] 	Batch(4400/6809) done. Loss: 0.0129  lr:0.000001
[ Sat Jul 13 11:25:51 2024 ] 
Training: Epoch [109/120], Step [4499], Loss: 0.09198247641324997, Training Accuracy: 97.57777777777777
[ Sat Jul 13 11:25:52 2024 ] 	Batch(4500/6809) done. Loss: 0.1321  lr:0.000001
[ Sat Jul 13 11:26:09 2024 ] 	Batch(4600/6809) done. Loss: 0.0213  lr:0.000001
[ Sat Jul 13 11:26:27 2024 ] 	Batch(4700/6809) done. Loss: 0.0182  lr:0.000001
[ Sat Jul 13 11:26:45 2024 ] 	Batch(4800/6809) done. Loss: 0.3220  lr:0.000001
[ Sat Jul 13 11:27:03 2024 ] 	Batch(4900/6809) done. Loss: 0.2102  lr:0.000001
[ Sat Jul 13 11:27:21 2024 ] 
Training: Epoch [109/120], Step [4999], Loss: 0.00400688499212265, Training Accuracy: 97.6
[ Sat Jul 13 11:27:21 2024 ] 	Batch(5000/6809) done. Loss: 0.0091  lr:0.000001
[ Sat Jul 13 11:27:39 2024 ] 	Batch(5100/6809) done. Loss: 0.0047  lr:0.000001
[ Sat Jul 13 11:27:57 2024 ] 	Batch(5200/6809) done. Loss: 0.0059  lr:0.000001
[ Sat Jul 13 11:28:15 2024 ] 	Batch(5300/6809) done. Loss: 0.0629  lr:0.000001
[ Sat Jul 13 11:28:33 2024 ] 	Batch(5400/6809) done. Loss: 0.0114  lr:0.000001
[ Sat Jul 13 11:28:51 2024 ] 
Training: Epoch [109/120], Step [5499], Loss: 0.11178132146596909, Training Accuracy: 97.60454545454546
[ Sat Jul 13 11:28:51 2024 ] 	Batch(5500/6809) done. Loss: 0.1240  lr:0.000001
[ Sat Jul 13 11:29:09 2024 ] 	Batch(5600/6809) done. Loss: 0.1214  lr:0.000001
[ Sat Jul 13 11:29:27 2024 ] 	Batch(5700/6809) done. Loss: 0.0601  lr:0.000001
[ Sat Jul 13 11:29:45 2024 ] 	Batch(5800/6809) done. Loss: 0.1090  lr:0.000001
[ Sat Jul 13 11:30:03 2024 ] 	Batch(5900/6809) done. Loss: 0.0881  lr:0.000001
[ Sat Jul 13 11:30:21 2024 ] 
Training: Epoch [109/120], Step [5999], Loss: 0.0486137792468071, Training Accuracy: 97.62291666666667
[ Sat Jul 13 11:30:21 2024 ] 	Batch(6000/6809) done. Loss: 0.0067  lr:0.000001
[ Sat Jul 13 11:30:39 2024 ] 	Batch(6100/6809) done. Loss: 0.0495  lr:0.000001
[ Sat Jul 13 11:30:57 2024 ] 	Batch(6200/6809) done. Loss: 0.2240  lr:0.000001
[ Sat Jul 13 11:31:15 2024 ] 	Batch(6300/6809) done. Loss: 0.0351  lr:0.000001
[ Sat Jul 13 11:31:33 2024 ] 	Batch(6400/6809) done. Loss: 0.1533  lr:0.000001
[ Sat Jul 13 11:31:50 2024 ] 
Training: Epoch [109/120], Step [6499], Loss: 0.13547982275485992, Training Accuracy: 97.62115384615385
[ Sat Jul 13 11:31:50 2024 ] 	Batch(6500/6809) done. Loss: 0.0830  lr:0.000001
[ Sat Jul 13 11:32:08 2024 ] 	Batch(6600/6809) done. Loss: 0.1043  lr:0.000001
[ Sat Jul 13 11:32:27 2024 ] 	Batch(6700/6809) done. Loss: 0.0220  lr:0.000001
[ Sat Jul 13 11:32:46 2024 ] 	Batch(6800/6809) done. Loss: 0.3669  lr:0.000001
[ Sat Jul 13 11:32:47 2024 ] 	Mean training loss: 0.0964.
[ Sat Jul 13 11:32:47 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 11:32:47 2024 ] Eval epoch: 110
[ Sat Jul 13 11:38:22 2024 ] 	Mean val loss of 7435 batches: 1.054042993605163.
[ Sat Jul 13 11:38:22 2024 ] 
Validation: Epoch [109/120], Samples [47854.0/59477], Loss: 0.32764944434165955, Validation Accuracy: 80.45799216503859
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 1 : 370 / 500 = 74 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 2 : 431 / 499 = 86 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 3 : 416 / 500 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 4 : 410 / 502 = 81 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 5 : 469 / 502 = 93 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 6 : 416 / 502 = 82 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 7 : 465 / 497 = 93 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 8 : 482 / 498 = 96 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 9 : 388 / 500 = 77 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 10 : 215 / 500 = 43 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 11 : 208 / 498 = 41 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 12 : 413 / 499 = 82 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 13 : 478 / 502 = 95 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 14 : 480 / 504 = 95 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 15 : 431 / 502 = 85 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 16 : 382 / 502 = 76 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 17 : 448 / 504 = 88 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 18 : 418 / 504 = 82 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 19 : 465 / 502 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 20 : 464 / 502 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 21 : 480 / 503 = 95 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 22 : 432 / 504 = 85 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 23 : 441 / 503 = 87 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 24 : 401 / 504 = 79 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 25 : 491 / 504 = 97 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 26 : 473 / 504 = 93 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 27 : 420 / 501 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 28 : 355 / 502 = 70 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 29 : 307 / 502 = 61 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 30 : 318 / 501 = 63 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 31 : 422 / 504 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 32 : 418 / 503 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 33 : 418 / 503 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 34 : 482 / 504 = 95 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 35 : 463 / 503 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 36 : 397 / 502 = 79 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 37 : 432 / 504 = 85 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 38 : 423 / 504 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 39 : 460 / 498 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 40 : 374 / 504 = 74 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 41 : 472 / 503 = 93 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 42 : 460 / 504 = 91 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 43 : 350 / 503 = 69 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 44 : 430 / 504 = 85 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 45 : 413 / 504 = 81 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 46 : 407 / 504 = 80 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 47 : 433 / 503 = 86 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 48 : 445 / 503 = 88 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 49 : 381 / 499 = 76 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 50 : 429 / 502 = 85 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 51 : 467 / 503 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 52 : 451 / 504 = 89 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 53 : 415 / 497 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 54 : 453 / 480 = 94 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 55 : 378 / 504 = 75 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 56 : 411 / 503 = 81 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 57 : 484 / 504 = 96 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 58 : 481 / 499 = 96 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 59 : 492 / 503 = 97 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 60 : 418 / 479 = 87 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 61 : 419 / 484 = 86 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 62 : 409 / 487 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 63 : 457 / 489 = 93 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 64 : 374 / 488 = 76 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 65 : 457 / 490 = 93 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 66 : 319 / 488 = 65 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 67 : 378 / 490 = 77 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 68 : 275 / 490 = 56 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 69 : 366 / 490 = 74 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 70 : 219 / 490 = 44 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 71 : 197 / 490 = 40 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 72 : 185 / 488 = 37 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 73 : 293 / 486 = 60 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 74 : 263 / 481 = 54 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 75 : 290 / 488 = 59 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 76 : 321 / 489 = 65 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 77 : 340 / 488 = 69 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 78 : 381 / 488 = 78 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 79 : 453 / 490 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 80 : 403 / 489 = 82 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 81 : 321 / 491 = 65 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 82 : 314 / 491 = 63 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 83 : 258 / 489 = 52 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 84 : 389 / 489 = 79 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 85 : 372 / 489 = 76 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 86 : 445 / 491 = 90 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 87 : 435 / 492 = 88 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 88 : 379 / 491 = 77 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 89 : 401 / 492 = 81 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 90 : 267 / 490 = 54 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 91 : 406 / 482 = 84 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 92 : 379 / 490 = 77 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 93 : 374 / 487 = 76 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 94 : 421 / 489 = 86 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 95 : 408 / 490 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 96 : 467 / 491 = 95 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 97 : 463 / 490 = 94 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 98 : 448 / 491 = 91 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 99 : 452 / 491 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 100 : 454 / 491 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 101 : 438 / 491 = 89 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 102 : 288 / 492 = 58 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 103 : 395 / 492 = 80 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 104 : 299 / 491 = 60 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 105 : 277 / 491 = 56 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 106 : 273 / 492 = 55 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 107 : 407 / 491 = 82 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 108 : 398 / 492 = 80 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 109 : 318 / 490 = 64 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 110 : 421 / 491 = 85 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 111 : 453 / 492 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 112 : 455 / 492 = 92 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 113 : 442 / 491 = 90 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 114 : 411 / 491 = 83 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 115 : 440 / 492 = 89 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 116 : 404 / 491 = 82 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 117 : 438 / 492 = 89 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 118 : 441 / 490 = 89 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 119 : 449 / 492 = 91 %
[ Sat Jul 13 11:38:22 2024 ] Accuracy of 120 : 429 / 500 = 85 %
[ Sat Jul 13 11:38:22 2024 ] Training epoch: 111
[ Sat Jul 13 11:38:22 2024 ] 	Batch(0/6809) done. Loss: 0.0260  lr:0.000001
[ Sat Jul 13 11:38:41 2024 ] 	Batch(100/6809) done. Loss: 0.4564  lr:0.000001
[ Sat Jul 13 11:38:59 2024 ] 	Batch(200/6809) done. Loss: 0.3048  lr:0.000001
[ Sat Jul 13 11:39:18 2024 ] 	Batch(300/6809) done. Loss: 0.0366  lr:0.000001
[ Sat Jul 13 11:39:37 2024 ] 	Batch(400/6809) done. Loss: 0.0802  lr:0.000001
[ Sat Jul 13 11:39:55 2024 ] 
Training: Epoch [110/120], Step [499], Loss: 0.007140096742659807, Training Accuracy: 97.725
[ Sat Jul 13 11:39:55 2024 ] 	Batch(500/6809) done. Loss: 0.1072  lr:0.000001
[ Sat Jul 13 11:40:14 2024 ] 	Batch(600/6809) done. Loss: 0.2545  lr:0.000001
[ Sat Jul 13 11:40:32 2024 ] 	Batch(700/6809) done. Loss: 0.0693  lr:0.000001
[ Sat Jul 13 11:40:51 2024 ] 	Batch(800/6809) done. Loss: 0.0306  lr:0.000001
[ Sat Jul 13 11:41:09 2024 ] 	Batch(900/6809) done. Loss: 0.4802  lr:0.000001
[ Sat Jul 13 11:41:27 2024 ] 
Training: Epoch [110/120], Step [999], Loss: 0.13581234216690063, Training Accuracy: 97.575
[ Sat Jul 13 11:41:27 2024 ] 	Batch(1000/6809) done. Loss: 0.3041  lr:0.000001
[ Sat Jul 13 11:41:45 2024 ] 	Batch(1100/6809) done. Loss: 0.0484  lr:0.000001
[ Sat Jul 13 11:42:03 2024 ] 	Batch(1200/6809) done. Loss: 0.0118  lr:0.000001
[ Sat Jul 13 11:42:21 2024 ] 	Batch(1300/6809) done. Loss: 0.0011  lr:0.000001
[ Sat Jul 13 11:42:39 2024 ] 	Batch(1400/6809) done. Loss: 0.0620  lr:0.000001
[ Sat Jul 13 11:42:57 2024 ] 
Training: Epoch [110/120], Step [1499], Loss: 0.35518741607666016, Training Accuracy: 97.53333333333333
[ Sat Jul 13 11:42:57 2024 ] 	Batch(1500/6809) done. Loss: 0.0166  lr:0.000001
[ Sat Jul 13 11:43:15 2024 ] 	Batch(1600/6809) done. Loss: 0.5094  lr:0.000001
[ Sat Jul 13 11:43:33 2024 ] 	Batch(1700/6809) done. Loss: 0.0384  lr:0.000001
[ Sat Jul 13 11:43:51 2024 ] 	Batch(1800/6809) done. Loss: 0.0221  lr:0.000001
[ Sat Jul 13 11:44:08 2024 ] 	Batch(1900/6809) done. Loss: 0.0546  lr:0.000001
[ Sat Jul 13 11:44:26 2024 ] 
Training: Epoch [110/120], Step [1999], Loss: 0.017035210505127907, Training Accuracy: 97.5125
[ Sat Jul 13 11:44:26 2024 ] 	Batch(2000/6809) done. Loss: 0.0048  lr:0.000001
[ Sat Jul 13 11:44:44 2024 ] 	Batch(2100/6809) done. Loss: 0.0387  lr:0.000001
[ Sat Jul 13 11:45:02 2024 ] 	Batch(2200/6809) done. Loss: 0.0245  lr:0.000001
[ Sat Jul 13 11:45:20 2024 ] 	Batch(2300/6809) done. Loss: 0.0503  lr:0.000001
[ Sat Jul 13 11:45:38 2024 ] 	Batch(2400/6809) done. Loss: 0.1193  lr:0.000001
[ Sat Jul 13 11:45:56 2024 ] 
Training: Epoch [110/120], Step [2499], Loss: 0.05693301558494568, Training Accuracy: 97.485
[ Sat Jul 13 11:45:56 2024 ] 	Batch(2500/6809) done. Loss: 0.0966  lr:0.000001
[ Sat Jul 13 11:46:14 2024 ] 	Batch(2600/6809) done. Loss: 0.1170  lr:0.000001
[ Sat Jul 13 11:46:32 2024 ] 	Batch(2700/6809) done. Loss: 0.0030  lr:0.000001
[ Sat Jul 13 11:46:50 2024 ] 	Batch(2800/6809) done. Loss: 0.0070  lr:0.000001
[ Sat Jul 13 11:47:08 2024 ] 	Batch(2900/6809) done. Loss: 0.0503  lr:0.000001
[ Sat Jul 13 11:47:25 2024 ] 
Training: Epoch [110/120], Step [2999], Loss: 0.03340243548154831, Training Accuracy: 97.41666666666666
[ Sat Jul 13 11:47:26 2024 ] 	Batch(3000/6809) done. Loss: 0.0274  lr:0.000001
[ Sat Jul 13 11:47:44 2024 ] 	Batch(3100/6809) done. Loss: 0.0373  lr:0.000001
[ Sat Jul 13 11:48:02 2024 ] 	Batch(3200/6809) done. Loss: 0.0685  lr:0.000001
[ Sat Jul 13 11:48:19 2024 ] 	Batch(3300/6809) done. Loss: 0.0570  lr:0.000001
[ Sat Jul 13 11:48:37 2024 ] 	Batch(3400/6809) done. Loss: 0.0309  lr:0.000001
[ Sat Jul 13 11:48:55 2024 ] 
Training: Epoch [110/120], Step [3499], Loss: 0.015293513424694538, Training Accuracy: 97.43928571428572
[ Sat Jul 13 11:48:55 2024 ] 	Batch(3500/6809) done. Loss: 0.0100  lr:0.000001
[ Sat Jul 13 11:49:13 2024 ] 	Batch(3600/6809) done. Loss: 0.0219  lr:0.000001
[ Sat Jul 13 11:49:32 2024 ] 	Batch(3700/6809) done. Loss: 0.0183  lr:0.000001
[ Sat Jul 13 11:49:50 2024 ] 	Batch(3800/6809) done. Loss: 0.0362  lr:0.000001
[ Sat Jul 13 11:50:09 2024 ] 	Batch(3900/6809) done. Loss: 0.0051  lr:0.000001
[ Sat Jul 13 11:50:27 2024 ] 
Training: Epoch [110/120], Step [3999], Loss: 0.39463913440704346, Training Accuracy: 97.421875
[ Sat Jul 13 11:50:28 2024 ] 	Batch(4000/6809) done. Loss: 0.0565  lr:0.000001
[ Sat Jul 13 11:50:46 2024 ] 	Batch(4100/6809) done. Loss: 0.0070  lr:0.000001
[ Sat Jul 13 11:51:05 2024 ] 	Batch(4200/6809) done. Loss: 0.1159  lr:0.000001
[ Sat Jul 13 11:51:23 2024 ] 	Batch(4300/6809) done. Loss: 0.0626  lr:0.000001
[ Sat Jul 13 11:51:42 2024 ] 	Batch(4400/6809) done. Loss: 0.0515  lr:0.000001
[ Sat Jul 13 11:52:00 2024 ] 
Training: Epoch [110/120], Step [4499], Loss: 0.02607591822743416, Training Accuracy: 97.41944444444445
[ Sat Jul 13 11:52:00 2024 ] 	Batch(4500/6809) done. Loss: 0.0336  lr:0.000001
[ Sat Jul 13 11:52:18 2024 ] 	Batch(4600/6809) done. Loss: 0.0131  lr:0.000001
[ Sat Jul 13 11:52:36 2024 ] 	Batch(4700/6809) done. Loss: 0.1281  lr:0.000001
[ Sat Jul 13 11:52:54 2024 ] 	Batch(4800/6809) done. Loss: 0.0073  lr:0.000001
[ Sat Jul 13 11:53:12 2024 ] 	Batch(4900/6809) done. Loss: 0.1028  lr:0.000001
[ Sat Jul 13 11:53:31 2024 ] 
Training: Epoch [110/120], Step [4999], Loss: 0.013626440428197384, Training Accuracy: 97.425
[ Sat Jul 13 11:53:31 2024 ] 	Batch(5000/6809) done. Loss: 0.0090  lr:0.000001
[ Sat Jul 13 11:53:49 2024 ] 	Batch(5100/6809) done. Loss: 0.0345  lr:0.000001
[ Sat Jul 13 11:54:08 2024 ] 	Batch(5200/6809) done. Loss: 0.0615  lr:0.000001
[ Sat Jul 13 11:54:26 2024 ] 	Batch(5300/6809) done. Loss: 0.0429  lr:0.000001
[ Sat Jul 13 11:54:45 2024 ] 	Batch(5400/6809) done. Loss: 0.2002  lr:0.000001
[ Sat Jul 13 11:55:02 2024 ] 
Training: Epoch [110/120], Step [5499], Loss: 0.17492322623729706, Training Accuracy: 97.44772727272726
[ Sat Jul 13 11:55:02 2024 ] 	Batch(5500/6809) done. Loss: 0.1808  lr:0.000001
[ Sat Jul 13 11:55:20 2024 ] 	Batch(5600/6809) done. Loss: 0.0312  lr:0.000001
[ Sat Jul 13 11:55:38 2024 ] 	Batch(5700/6809) done. Loss: 0.0284  lr:0.000001
[ Sat Jul 13 11:55:56 2024 ] 	Batch(5800/6809) done. Loss: 0.1097  lr:0.000001
[ Sat Jul 13 11:56:14 2024 ] 	Batch(5900/6809) done. Loss: 0.0302  lr:0.000001
[ Sat Jul 13 11:56:32 2024 ] 
Training: Epoch [110/120], Step [5999], Loss: 0.05856645852327347, Training Accuracy: 97.46458333333334
[ Sat Jul 13 11:56:32 2024 ] 	Batch(6000/6809) done. Loss: 0.7121  lr:0.000001
[ Sat Jul 13 11:56:50 2024 ] 	Batch(6100/6809) done. Loss: 0.0060  lr:0.000001
[ Sat Jul 13 11:57:08 2024 ] 	Batch(6200/6809) done. Loss: 0.2709  lr:0.000001
[ Sat Jul 13 11:57:26 2024 ] 	Batch(6300/6809) done. Loss: 0.2536  lr:0.000001
[ Sat Jul 13 11:57:44 2024 ] 	Batch(6400/6809) done. Loss: 0.0148  lr:0.000001
[ Sat Jul 13 11:58:02 2024 ] 
Training: Epoch [110/120], Step [6499], Loss: 0.0074401237070560455, Training Accuracy: 97.46153846153847
[ Sat Jul 13 11:58:03 2024 ] 	Batch(6500/6809) done. Loss: 0.0728  lr:0.000001
[ Sat Jul 13 11:58:21 2024 ] 	Batch(6600/6809) done. Loss: 0.0719  lr:0.000001
[ Sat Jul 13 11:58:40 2024 ] 	Batch(6700/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 11:58:58 2024 ] 	Batch(6800/6809) done. Loss: 0.0028  lr:0.000001
[ Sat Jul 13 11:59:00 2024 ] 	Mean training loss: 0.0983.
[ Sat Jul 13 11:59:00 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 11:59:00 2024 ] Training epoch: 112
[ Sat Jul 13 11:59:01 2024 ] 	Batch(0/6809) done. Loss: 0.0129  lr:0.000001
[ Sat Jul 13 11:59:19 2024 ] 	Batch(100/6809) done. Loss: 0.0200  lr:0.000001
[ Sat Jul 13 11:59:36 2024 ] 	Batch(200/6809) done. Loss: 0.0946  lr:0.000001
[ Sat Jul 13 11:59:54 2024 ] 	Batch(300/6809) done. Loss: 0.3043  lr:0.000001
[ Sat Jul 13 12:00:12 2024 ] 	Batch(400/6809) done. Loss: 0.0110  lr:0.000001
[ Sat Jul 13 12:00:30 2024 ] 
Training: Epoch [111/120], Step [499], Loss: 0.15843316912651062, Training Accuracy: 97.75
[ Sat Jul 13 12:00:30 2024 ] 	Batch(500/6809) done. Loss: 0.0410  lr:0.000001
[ Sat Jul 13 12:00:48 2024 ] 	Batch(600/6809) done. Loss: 0.2363  lr:0.000001
[ Sat Jul 13 12:01:06 2024 ] 	Batch(700/6809) done. Loss: 0.1505  lr:0.000001
[ Sat Jul 13 12:01:24 2024 ] 	Batch(800/6809) done. Loss: 0.0482  lr:0.000001
[ Sat Jul 13 12:01:43 2024 ] 	Batch(900/6809) done. Loss: 0.1448  lr:0.000001
[ Sat Jul 13 12:02:01 2024 ] 
Training: Epoch [111/120], Step [999], Loss: 0.4234007000923157, Training Accuracy: 97.8375
[ Sat Jul 13 12:02:01 2024 ] 	Batch(1000/6809) done. Loss: 0.0609  lr:0.000001
[ Sat Jul 13 12:02:20 2024 ] 	Batch(1100/6809) done. Loss: 0.0376  lr:0.000001
[ Sat Jul 13 12:02:38 2024 ] 	Batch(1200/6809) done. Loss: 0.0443  lr:0.000001
[ Sat Jul 13 12:02:57 2024 ] 	Batch(1300/6809) done. Loss: 0.3326  lr:0.000001
[ Sat Jul 13 12:03:15 2024 ] 	Batch(1400/6809) done. Loss: 0.0078  lr:0.000001
[ Sat Jul 13 12:03:33 2024 ] 
Training: Epoch [111/120], Step [1499], Loss: 0.1346816122531891, Training Accuracy: 97.7
[ Sat Jul 13 12:03:33 2024 ] 	Batch(1500/6809) done. Loss: 0.1638  lr:0.000001
[ Sat Jul 13 12:03:51 2024 ] 	Batch(1600/6809) done. Loss: 0.0173  lr:0.000001
[ Sat Jul 13 12:04:09 2024 ] 	Batch(1700/6809) done. Loss: 0.0486  lr:0.000001
[ Sat Jul 13 12:04:27 2024 ] 	Batch(1800/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 12:04:45 2024 ] 	Batch(1900/6809) done. Loss: 0.0781  lr:0.000001
[ Sat Jul 13 12:05:03 2024 ] 
Training: Epoch [111/120], Step [1999], Loss: 0.03496929630637169, Training Accuracy: 97.66875
[ Sat Jul 13 12:05:03 2024 ] 	Batch(2000/6809) done. Loss: 0.1712  lr:0.000001
[ Sat Jul 13 12:05:21 2024 ] 	Batch(2100/6809) done. Loss: 0.0507  lr:0.000001
[ Sat Jul 13 12:05:39 2024 ] 	Batch(2200/6809) done. Loss: 0.0639  lr:0.000001
[ Sat Jul 13 12:05:56 2024 ] 	Batch(2300/6809) done. Loss: 0.1017  lr:0.000001
[ Sat Jul 13 12:06:15 2024 ] 	Batch(2400/6809) done. Loss: 0.1037  lr:0.000001
[ Sat Jul 13 12:06:32 2024 ] 
Training: Epoch [111/120], Step [2499], Loss: 0.014154771342873573, Training Accuracy: 97.675
[ Sat Jul 13 12:06:32 2024 ] 	Batch(2500/6809) done. Loss: 0.0423  lr:0.000001
[ Sat Jul 13 12:06:50 2024 ] 	Batch(2600/6809) done. Loss: 0.4550  lr:0.000001
[ Sat Jul 13 12:07:08 2024 ] 	Batch(2700/6809) done. Loss: 0.1570  lr:0.000001
[ Sat Jul 13 12:07:26 2024 ] 	Batch(2800/6809) done. Loss: 0.0507  lr:0.000001
[ Sat Jul 13 12:07:45 2024 ] 	Batch(2900/6809) done. Loss: 0.5045  lr:0.000001
[ Sat Jul 13 12:08:03 2024 ] 
Training: Epoch [111/120], Step [2999], Loss: 0.09667856991291046, Training Accuracy: 97.6125
[ Sat Jul 13 12:08:03 2024 ] 	Batch(3000/6809) done. Loss: 0.0120  lr:0.000001
[ Sat Jul 13 12:08:22 2024 ] 	Batch(3100/6809) done. Loss: 0.2551  lr:0.000001
[ Sat Jul 13 12:08:40 2024 ] 	Batch(3200/6809) done. Loss: 0.0235  lr:0.000001
[ Sat Jul 13 12:08:59 2024 ] 	Batch(3300/6809) done. Loss: 0.0028  lr:0.000001
[ Sat Jul 13 12:09:17 2024 ] 	Batch(3400/6809) done. Loss: 0.4152  lr:0.000001
[ Sat Jul 13 12:09:36 2024 ] 
Training: Epoch [111/120], Step [3499], Loss: 0.0023244512267410755, Training Accuracy: 97.62142857142857
[ Sat Jul 13 12:09:36 2024 ] 	Batch(3500/6809) done. Loss: 0.0037  lr:0.000001
[ Sat Jul 13 12:09:55 2024 ] 	Batch(3600/6809) done. Loss: 0.0181  lr:0.000001
[ Sat Jul 13 12:10:13 2024 ] 	Batch(3700/6809) done. Loss: 0.1275  lr:0.000001
[ Sat Jul 13 12:10:32 2024 ] 	Batch(3800/6809) done. Loss: 0.0487  lr:0.000001
[ Sat Jul 13 12:10:50 2024 ] 	Batch(3900/6809) done. Loss: 0.0912  lr:0.000001
[ Sat Jul 13 12:11:09 2024 ] 
Training: Epoch [111/120], Step [3999], Loss: 0.18820171058177948, Training Accuracy: 97.584375
[ Sat Jul 13 12:11:09 2024 ] 	Batch(4000/6809) done. Loss: 0.0149  lr:0.000001
[ Sat Jul 13 12:11:27 2024 ] 	Batch(4100/6809) done. Loss: 0.0141  lr:0.000001
[ Sat Jul 13 12:11:46 2024 ] 	Batch(4200/6809) done. Loss: 0.0036  lr:0.000001
[ Sat Jul 13 12:12:04 2024 ] 	Batch(4300/6809) done. Loss: 0.2294  lr:0.000001
[ Sat Jul 13 12:12:23 2024 ] 	Batch(4400/6809) done. Loss: 0.1296  lr:0.000001
[ Sat Jul 13 12:12:41 2024 ] 
Training: Epoch [111/120], Step [4499], Loss: 0.006148584187030792, Training Accuracy: 97.52777777777779
[ Sat Jul 13 12:12:42 2024 ] 	Batch(4500/6809) done. Loss: 0.0328  lr:0.000001
[ Sat Jul 13 12:13:00 2024 ] 	Batch(4600/6809) done. Loss: 0.0232  lr:0.000001
[ Sat Jul 13 12:13:19 2024 ] 	Batch(4700/6809) done. Loss: 0.0029  lr:0.000001
[ Sat Jul 13 12:13:37 2024 ] 	Batch(4800/6809) done. Loss: 0.0361  lr:0.000001
[ Sat Jul 13 12:13:55 2024 ] 	Batch(4900/6809) done. Loss: 0.0806  lr:0.000001
[ Sat Jul 13 12:14:13 2024 ] 
Training: Epoch [111/120], Step [4999], Loss: 0.010430889204144478, Training Accuracy: 97.565
[ Sat Jul 13 12:14:13 2024 ] 	Batch(5000/6809) done. Loss: 0.0061  lr:0.000001
[ Sat Jul 13 12:14:31 2024 ] 	Batch(5100/6809) done. Loss: 0.0159  lr:0.000001
[ Sat Jul 13 12:14:49 2024 ] 	Batch(5200/6809) done. Loss: 0.2229  lr:0.000001
[ Sat Jul 13 12:15:07 2024 ] 	Batch(5300/6809) done. Loss: 0.0047  lr:0.000001
[ Sat Jul 13 12:15:25 2024 ] 	Batch(5400/6809) done. Loss: 0.2481  lr:0.000001
[ Sat Jul 13 12:15:43 2024 ] 
Training: Epoch [111/120], Step [5499], Loss: 0.11301790922880173, Training Accuracy: 97.57727272727273
[ Sat Jul 13 12:15:43 2024 ] 	Batch(5500/6809) done. Loss: 0.0641  lr:0.000001
[ Sat Jul 13 12:16:01 2024 ] 	Batch(5600/6809) done. Loss: 0.1869  lr:0.000001
[ Sat Jul 13 12:16:19 2024 ] 	Batch(5700/6809) done. Loss: 0.0128  lr:0.000001
[ Sat Jul 13 12:16:37 2024 ] 	Batch(5800/6809) done. Loss: 0.0347  lr:0.000001
[ Sat Jul 13 12:16:55 2024 ] 	Batch(5900/6809) done. Loss: 0.0846  lr:0.000001
[ Sat Jul 13 12:17:12 2024 ] 
Training: Epoch [111/120], Step [5999], Loss: 0.012650316581130028, Training Accuracy: 97.56666666666666
[ Sat Jul 13 12:17:13 2024 ] 	Batch(6000/6809) done. Loss: 0.1803  lr:0.000001
[ Sat Jul 13 12:17:31 2024 ] 	Batch(6100/6809) done. Loss: 0.0729  lr:0.000001
[ Sat Jul 13 12:17:48 2024 ] 	Batch(6200/6809) done. Loss: 0.0335  lr:0.000001
[ Sat Jul 13 12:18:06 2024 ] 	Batch(6300/6809) done. Loss: 0.1862  lr:0.000001
[ Sat Jul 13 12:18:24 2024 ] 	Batch(6400/6809) done. Loss: 0.0101  lr:0.000001
[ Sat Jul 13 12:18:42 2024 ] 
Training: Epoch [111/120], Step [6499], Loss: 0.3518017828464508, Training Accuracy: 97.58461538461538
[ Sat Jul 13 12:18:42 2024 ] 	Batch(6500/6809) done. Loss: 0.0039  lr:0.000001
[ Sat Jul 13 12:19:00 2024 ] 	Batch(6600/6809) done. Loss: 0.1377  lr:0.000001
[ Sat Jul 13 12:19:18 2024 ] 	Batch(6700/6809) done. Loss: 0.0229  lr:0.000001
[ Sat Jul 13 12:19:36 2024 ] 	Batch(6800/6809) done. Loss: 0.3166  lr:0.000001
[ Sat Jul 13 12:19:38 2024 ] 	Mean training loss: 0.0959.
[ Sat Jul 13 12:19:38 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 12:19:38 2024 ] Training epoch: 113
[ Sat Jul 13 12:19:38 2024 ] 	Batch(0/6809) done. Loss: 0.0194  lr:0.000001
[ Sat Jul 13 12:19:56 2024 ] 	Batch(100/6809) done. Loss: 0.0621  lr:0.000001
[ Sat Jul 13 12:20:14 2024 ] 	Batch(200/6809) done. Loss: 0.0350  lr:0.000001
[ Sat Jul 13 12:20:32 2024 ] 	Batch(300/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 12:20:50 2024 ] 	Batch(400/6809) done. Loss: 0.1208  lr:0.000001
[ Sat Jul 13 12:21:08 2024 ] 
Training: Epoch [112/120], Step [499], Loss: 0.5215985774993896, Training Accuracy: 97.3
[ Sat Jul 13 12:21:08 2024 ] 	Batch(500/6809) done. Loss: 0.1698  lr:0.000001
[ Sat Jul 13 12:21:26 2024 ] 	Batch(600/6809) done. Loss: 0.1055  lr:0.000001
[ Sat Jul 13 12:21:44 2024 ] 	Batch(700/6809) done. Loss: 0.4497  lr:0.000001
[ Sat Jul 13 12:22:02 2024 ] 	Batch(800/6809) done. Loss: 0.0279  lr:0.000001
[ Sat Jul 13 12:22:19 2024 ] 	Batch(900/6809) done. Loss: 0.0199  lr:0.000001
[ Sat Jul 13 12:22:37 2024 ] 
Training: Epoch [112/120], Step [999], Loss: 0.047481831163167953, Training Accuracy: 97.5
[ Sat Jul 13 12:22:37 2024 ] 	Batch(1000/6809) done. Loss: 0.0386  lr:0.000001
[ Sat Jul 13 12:22:55 2024 ] 	Batch(1100/6809) done. Loss: 0.0766  lr:0.000001
[ Sat Jul 13 12:23:13 2024 ] 	Batch(1200/6809) done. Loss: 0.1129  lr:0.000001
[ Sat Jul 13 12:23:31 2024 ] 	Batch(1300/6809) done. Loss: 0.0092  lr:0.000001
[ Sat Jul 13 12:23:49 2024 ] 	Batch(1400/6809) done. Loss: 0.1523  lr:0.000001
[ Sat Jul 13 12:24:07 2024 ] 
Training: Epoch [112/120], Step [1499], Loss: 0.2547285854816437, Training Accuracy: 97.41666666666666
[ Sat Jul 13 12:24:07 2024 ] 	Batch(1500/6809) done. Loss: 0.0775  lr:0.000001
[ Sat Jul 13 12:24:25 2024 ] 	Batch(1600/6809) done. Loss: 0.0316  lr:0.000001
[ Sat Jul 13 12:24:43 2024 ] 	Batch(1700/6809) done. Loss: 0.1954  lr:0.000001
[ Sat Jul 13 12:25:01 2024 ] 	Batch(1800/6809) done. Loss: 0.0356  lr:0.000001
[ Sat Jul 13 12:25:19 2024 ] 	Batch(1900/6809) done. Loss: 0.0779  lr:0.000001
[ Sat Jul 13 12:25:37 2024 ] 
Training: Epoch [112/120], Step [1999], Loss: 0.15320169925689697, Training Accuracy: 97.39999999999999
[ Sat Jul 13 12:25:37 2024 ] 	Batch(2000/6809) done. Loss: 0.0792  lr:0.000001
[ Sat Jul 13 12:25:55 2024 ] 	Batch(2100/6809) done. Loss: 0.1923  lr:0.000001
[ Sat Jul 13 12:26:14 2024 ] 	Batch(2200/6809) done. Loss: 0.0066  lr:0.000001
[ Sat Jul 13 12:26:32 2024 ] 	Batch(2300/6809) done. Loss: 0.0715  lr:0.000001
[ Sat Jul 13 12:26:50 2024 ] 	Batch(2400/6809) done. Loss: 0.1105  lr:0.000001
[ Sat Jul 13 12:27:07 2024 ] 
Training: Epoch [112/120], Step [2499], Loss: 0.1709468960762024, Training Accuracy: 97.41499999999999
[ Sat Jul 13 12:27:08 2024 ] 	Batch(2500/6809) done. Loss: 0.0189  lr:0.000001
[ Sat Jul 13 12:27:26 2024 ] 	Batch(2600/6809) done. Loss: 0.0464  lr:0.000001
[ Sat Jul 13 12:27:43 2024 ] 	Batch(2700/6809) done. Loss: 0.0380  lr:0.000001
[ Sat Jul 13 12:28:01 2024 ] 	Batch(2800/6809) done. Loss: 0.0377  lr:0.000001
[ Sat Jul 13 12:28:19 2024 ] 	Batch(2900/6809) done. Loss: 0.0471  lr:0.000001
[ Sat Jul 13 12:28:37 2024 ] 
Training: Epoch [112/120], Step [2999], Loss: 0.07451571524143219, Training Accuracy: 97.40416666666667
[ Sat Jul 13 12:28:37 2024 ] 	Batch(3000/6809) done. Loss: 0.0108  lr:0.000001
[ Sat Jul 13 12:28:55 2024 ] 	Batch(3100/6809) done. Loss: 0.0036  lr:0.000001
[ Sat Jul 13 12:29:13 2024 ] 	Batch(3200/6809) done. Loss: 0.0178  lr:0.000001
[ Sat Jul 13 12:29:31 2024 ] 	Batch(3300/6809) done. Loss: 0.0694  lr:0.000001
[ Sat Jul 13 12:29:49 2024 ] 	Batch(3400/6809) done. Loss: 0.2014  lr:0.000001
[ Sat Jul 13 12:30:07 2024 ] 
Training: Epoch [112/120], Step [3499], Loss: 0.20284028351306915, Training Accuracy: 97.49285714285715
[ Sat Jul 13 12:30:07 2024 ] 	Batch(3500/6809) done. Loss: 0.3482  lr:0.000001
[ Sat Jul 13 12:30:25 2024 ] 	Batch(3600/6809) done. Loss: 0.1439  lr:0.000001
[ Sat Jul 13 12:30:43 2024 ] 	Batch(3700/6809) done. Loss: 0.0811  lr:0.000001
[ Sat Jul 13 12:31:01 2024 ] 	Batch(3800/6809) done. Loss: 0.1058  lr:0.000001
[ Sat Jul 13 12:31:19 2024 ] 	Batch(3900/6809) done. Loss: 0.0314  lr:0.000001
[ Sat Jul 13 12:31:37 2024 ] 
Training: Epoch [112/120], Step [3999], Loss: 0.08970878273248672, Training Accuracy: 97.45625
[ Sat Jul 13 12:31:37 2024 ] 	Batch(4000/6809) done. Loss: 0.0278  lr:0.000001
[ Sat Jul 13 12:31:55 2024 ] 	Batch(4100/6809) done. Loss: 0.0199  lr:0.000001
[ Sat Jul 13 12:32:13 2024 ] 	Batch(4200/6809) done. Loss: 0.2067  lr:0.000001
[ Sat Jul 13 12:32:31 2024 ] 	Batch(4300/6809) done. Loss: 0.0984  lr:0.000001
[ Sat Jul 13 12:32:49 2024 ] 	Batch(4400/6809) done. Loss: 0.0102  lr:0.000001
[ Sat Jul 13 12:33:07 2024 ] 
Training: Epoch [112/120], Step [4499], Loss: 0.018003730103373528, Training Accuracy: 97.475
[ Sat Jul 13 12:33:07 2024 ] 	Batch(4500/6809) done. Loss: 0.0082  lr:0.000001
[ Sat Jul 13 12:33:26 2024 ] 	Batch(4600/6809) done. Loss: 0.0260  lr:0.000001
[ Sat Jul 13 12:33:44 2024 ] 	Batch(4700/6809) done. Loss: 0.0276  lr:0.000001
[ Sat Jul 13 12:34:02 2024 ] 	Batch(4800/6809) done. Loss: 0.0971  lr:0.000001
[ Sat Jul 13 12:34:20 2024 ] 	Batch(4900/6809) done. Loss: 0.0041  lr:0.000001
[ Sat Jul 13 12:34:38 2024 ] 
Training: Epoch [112/120], Step [4999], Loss: 0.0634731724858284, Training Accuracy: 97.52250000000001
[ Sat Jul 13 12:34:38 2024 ] 	Batch(5000/6809) done. Loss: 0.0740  lr:0.000001
[ Sat Jul 13 12:34:56 2024 ] 	Batch(5100/6809) done. Loss: 0.2865  lr:0.000001
[ Sat Jul 13 12:35:14 2024 ] 	Batch(5200/6809) done. Loss: 0.0042  lr:0.000001
[ Sat Jul 13 12:35:32 2024 ] 	Batch(5300/6809) done. Loss: 0.0038  lr:0.000001
[ Sat Jul 13 12:35:50 2024 ] 	Batch(5400/6809) done. Loss: 0.0875  lr:0.000001
[ Sat Jul 13 12:36:07 2024 ] 
Training: Epoch [112/120], Step [5499], Loss: 0.08292539417743683, Training Accuracy: 97.53181818181818
[ Sat Jul 13 12:36:07 2024 ] 	Batch(5500/6809) done. Loss: 0.1736  lr:0.000001
[ Sat Jul 13 12:36:26 2024 ] 	Batch(5600/6809) done. Loss: 0.0049  lr:0.000001
[ Sat Jul 13 12:36:45 2024 ] 	Batch(5700/6809) done. Loss: 0.0374  lr:0.000001
[ Sat Jul 13 12:37:03 2024 ] 	Batch(5800/6809) done. Loss: 0.0245  lr:0.000001
[ Sat Jul 13 12:37:22 2024 ] 	Batch(5900/6809) done. Loss: 0.0188  lr:0.000001
[ Sat Jul 13 12:37:40 2024 ] 
Training: Epoch [112/120], Step [5999], Loss: 0.017658783122897148, Training Accuracy: 97.54791666666667
[ Sat Jul 13 12:37:40 2024 ] 	Batch(6000/6809) done. Loss: 0.1213  lr:0.000001
[ Sat Jul 13 12:37:58 2024 ] 	Batch(6100/6809) done. Loss: 0.0523  lr:0.000001
[ Sat Jul 13 12:38:16 2024 ] 	Batch(6200/6809) done. Loss: 0.0301  lr:0.000001
[ Sat Jul 13 12:38:34 2024 ] 	Batch(6300/6809) done. Loss: 0.1735  lr:0.000001
[ Sat Jul 13 12:38:52 2024 ] 	Batch(6400/6809) done. Loss: 0.5217  lr:0.000001
[ Sat Jul 13 12:39:10 2024 ] 
Training: Epoch [112/120], Step [6499], Loss: 0.2270243763923645, Training Accuracy: 97.5423076923077
[ Sat Jul 13 12:39:10 2024 ] 	Batch(6500/6809) done. Loss: 0.1926  lr:0.000001
[ Sat Jul 13 12:39:29 2024 ] 	Batch(6600/6809) done. Loss: 0.0155  lr:0.000001
[ Sat Jul 13 12:39:48 2024 ] 	Batch(6700/6809) done. Loss: 0.0072  lr:0.000001
[ Sat Jul 13 12:40:06 2024 ] 	Batch(6800/6809) done. Loss: 0.0139  lr:0.000001
[ Sat Jul 13 12:40:08 2024 ] 	Mean training loss: 0.0967.
[ Sat Jul 13 12:40:08 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 12:40:08 2024 ] Training epoch: 114
[ Sat Jul 13 12:40:09 2024 ] 	Batch(0/6809) done. Loss: 0.0275  lr:0.000001
[ Sat Jul 13 12:40:26 2024 ] 	Batch(100/6809) done. Loss: 0.0147  lr:0.000001
[ Sat Jul 13 12:40:44 2024 ] 	Batch(200/6809) done. Loss: 0.0916  lr:0.000001
[ Sat Jul 13 12:41:02 2024 ] 	Batch(300/6809) done. Loss: 0.0414  lr:0.000001
[ Sat Jul 13 12:41:20 2024 ] 	Batch(400/6809) done. Loss: 0.0219  lr:0.000001
[ Sat Jul 13 12:41:38 2024 ] 
Training: Epoch [113/120], Step [499], Loss: 0.08141592890024185, Training Accuracy: 97.675
[ Sat Jul 13 12:41:38 2024 ] 	Batch(500/6809) done. Loss: 0.0605  lr:0.000001
[ Sat Jul 13 12:41:56 2024 ] 	Batch(600/6809) done. Loss: 0.0140  lr:0.000001
[ Sat Jul 13 12:42:14 2024 ] 	Batch(700/6809) done. Loss: 0.0466  lr:0.000001
[ Sat Jul 13 12:42:32 2024 ] 	Batch(800/6809) done. Loss: 0.0942  lr:0.000001
[ Sat Jul 13 12:42:50 2024 ] 	Batch(900/6809) done. Loss: 0.0391  lr:0.000001
[ Sat Jul 13 12:43:08 2024 ] 
Training: Epoch [113/120], Step [999], Loss: 0.02413170412182808, Training Accuracy: 97.725
[ Sat Jul 13 12:43:08 2024 ] 	Batch(1000/6809) done. Loss: 0.0941  lr:0.000001
[ Sat Jul 13 12:43:26 2024 ] 	Batch(1100/6809) done. Loss: 0.0368  lr:0.000001
[ Sat Jul 13 12:43:44 2024 ] 	Batch(1200/6809) done. Loss: 0.0476  lr:0.000001
[ Sat Jul 13 12:44:02 2024 ] 	Batch(1300/6809) done. Loss: 0.0225  lr:0.000001
[ Sat Jul 13 12:44:20 2024 ] 	Batch(1400/6809) done. Loss: 0.0387  lr:0.000001
[ Sat Jul 13 12:44:38 2024 ] 
Training: Epoch [113/120], Step [1499], Loss: 0.054177138954401016, Training Accuracy: 97.63333333333334
[ Sat Jul 13 12:44:38 2024 ] 	Batch(1500/6809) done. Loss: 0.0097  lr:0.000001
[ Sat Jul 13 12:44:56 2024 ] 	Batch(1600/6809) done. Loss: 0.0112  lr:0.000001
[ Sat Jul 13 12:45:14 2024 ] 	Batch(1700/6809) done. Loss: 0.0180  lr:0.000001
[ Sat Jul 13 12:45:32 2024 ] 	Batch(1800/6809) done. Loss: 0.2433  lr:0.000001
[ Sat Jul 13 12:45:50 2024 ] 	Batch(1900/6809) done. Loss: 0.0143  lr:0.000001
[ Sat Jul 13 12:46:07 2024 ] 
Training: Epoch [113/120], Step [1999], Loss: 0.05278405174612999, Training Accuracy: 97.69375
[ Sat Jul 13 12:46:08 2024 ] 	Batch(2000/6809) done. Loss: 1.3304  lr:0.000001
[ Sat Jul 13 12:46:26 2024 ] 	Batch(2100/6809) done. Loss: 0.0010  lr:0.000001
[ Sat Jul 13 12:46:44 2024 ] 	Batch(2200/6809) done. Loss: 0.0595  lr:0.000001
[ Sat Jul 13 12:47:02 2024 ] 	Batch(2300/6809) done. Loss: 0.2512  lr:0.000001
[ Sat Jul 13 12:47:20 2024 ] 	Batch(2400/6809) done. Loss: 0.0487  lr:0.000001
[ Sat Jul 13 12:47:38 2024 ] 
Training: Epoch [113/120], Step [2499], Loss: 0.03487181290984154, Training Accuracy: 97.69
[ Sat Jul 13 12:47:38 2024 ] 	Batch(2500/6809) done. Loss: 0.0625  lr:0.000001
[ Sat Jul 13 12:47:56 2024 ] 	Batch(2600/6809) done. Loss: 0.0619  lr:0.000001
[ Sat Jul 13 12:48:14 2024 ] 	Batch(2700/6809) done. Loss: 0.1038  lr:0.000001
[ Sat Jul 13 12:48:32 2024 ] 	Batch(2800/6809) done. Loss: 0.0579  lr:0.000001
[ Sat Jul 13 12:48:50 2024 ] 	Batch(2900/6809) done. Loss: 0.0093  lr:0.000001
[ Sat Jul 13 12:49:08 2024 ] 
Training: Epoch [113/120], Step [2999], Loss: 0.10648135840892792, Training Accuracy: 97.575
[ Sat Jul 13 12:49:08 2024 ] 	Batch(3000/6809) done. Loss: 0.1423  lr:0.000001
[ Sat Jul 13 12:49:26 2024 ] 	Batch(3100/6809) done. Loss: 0.1501  lr:0.000001
[ Sat Jul 13 12:49:45 2024 ] 	Batch(3200/6809) done. Loss: 0.1673  lr:0.000001
[ Sat Jul 13 12:50:03 2024 ] 	Batch(3300/6809) done. Loss: 0.0248  lr:0.000001
[ Sat Jul 13 12:50:20 2024 ] 	Batch(3400/6809) done. Loss: 0.0017  lr:0.000001
[ Sat Jul 13 12:50:38 2024 ] 
Training: Epoch [113/120], Step [3499], Loss: 0.5129942297935486, Training Accuracy: 97.55357142857143
[ Sat Jul 13 12:50:38 2024 ] 	Batch(3500/6809) done. Loss: 0.1638  lr:0.000001
[ Sat Jul 13 12:50:56 2024 ] 	Batch(3600/6809) done. Loss: 0.0523  lr:0.000001
[ Sat Jul 13 12:51:15 2024 ] 	Batch(3700/6809) done. Loss: 0.0906  lr:0.000001
[ Sat Jul 13 12:51:34 2024 ] 	Batch(3800/6809) done. Loss: 0.0916  lr:0.000001
[ Sat Jul 13 12:51:52 2024 ] 	Batch(3900/6809) done. Loss: 0.4101  lr:0.000001
[ Sat Jul 13 12:52:10 2024 ] 
Training: Epoch [113/120], Step [3999], Loss: 0.15920567512512207, Training Accuracy: 97.55
[ Sat Jul 13 12:52:10 2024 ] 	Batch(4000/6809) done. Loss: 0.0435  lr:0.000001
[ Sat Jul 13 12:52:29 2024 ] 	Batch(4100/6809) done. Loss: 0.0055  lr:0.000001
[ Sat Jul 13 12:52:48 2024 ] 	Batch(4200/6809) done. Loss: 0.1013  lr:0.000001
[ Sat Jul 13 12:53:06 2024 ] 	Batch(4300/6809) done. Loss: 0.0012  lr:0.000001
[ Sat Jul 13 12:53:25 2024 ] 	Batch(4400/6809) done. Loss: 0.1481  lr:0.000001
[ Sat Jul 13 12:53:43 2024 ] 
Training: Epoch [113/120], Step [4499], Loss: 0.09562258422374725, Training Accuracy: 97.57777777777777
[ Sat Jul 13 12:53:43 2024 ] 	Batch(4500/6809) done. Loss: 0.7263  lr:0.000001
[ Sat Jul 13 12:54:01 2024 ] 	Batch(4600/6809) done. Loss: 0.0505  lr:0.000001
[ Sat Jul 13 12:54:19 2024 ] 	Batch(4700/6809) done. Loss: 0.0023  lr:0.000001
[ Sat Jul 13 12:54:37 2024 ] 	Batch(4800/6809) done. Loss: 0.0401  lr:0.000001
[ Sat Jul 13 12:54:55 2024 ] 	Batch(4900/6809) done. Loss: 0.0617  lr:0.000001
[ Sat Jul 13 12:55:12 2024 ] 
Training: Epoch [113/120], Step [4999], Loss: 0.03984851390123367, Training Accuracy: 97.565
[ Sat Jul 13 12:55:13 2024 ] 	Batch(5000/6809) done. Loss: 0.0103  lr:0.000001
[ Sat Jul 13 12:55:31 2024 ] 	Batch(5100/6809) done. Loss: 0.0442  lr:0.000001
[ Sat Jul 13 12:55:49 2024 ] 	Batch(5200/6809) done. Loss: 0.0745  lr:0.000001
[ Sat Jul 13 12:56:07 2024 ] 	Batch(5300/6809) done. Loss: 0.0724  lr:0.000001
[ Sat Jul 13 12:56:24 2024 ] 	Batch(5400/6809) done. Loss: 0.0533  lr:0.000001
[ Sat Jul 13 12:56:42 2024 ] 
Training: Epoch [113/120], Step [5499], Loss: 0.009805169887840748, Training Accuracy: 97.53636363636363
[ Sat Jul 13 12:56:42 2024 ] 	Batch(5500/6809) done. Loss: 0.0314  lr:0.000001
[ Sat Jul 13 12:57:00 2024 ] 	Batch(5600/6809) done. Loss: 0.0447  lr:0.000001
[ Sat Jul 13 12:57:19 2024 ] 	Batch(5700/6809) done. Loss: 0.0093  lr:0.000001
[ Sat Jul 13 12:57:38 2024 ] 	Batch(5800/6809) done. Loss: 0.3056  lr:0.000001
[ Sat Jul 13 12:57:56 2024 ] 	Batch(5900/6809) done. Loss: 0.1917  lr:0.000001
[ Sat Jul 13 12:58:14 2024 ] 
Training: Epoch [113/120], Step [5999], Loss: 0.03007570095360279, Training Accuracy: 97.55
[ Sat Jul 13 12:58:15 2024 ] 	Batch(6000/6809) done. Loss: 0.0187  lr:0.000001
[ Sat Jul 13 12:58:33 2024 ] 	Batch(6100/6809) done. Loss: 0.0149  lr:0.000001
[ Sat Jul 13 12:58:51 2024 ] 	Batch(6200/6809) done. Loss: 0.2262  lr:0.000001
[ Sat Jul 13 12:59:09 2024 ] 	Batch(6300/6809) done. Loss: 0.0055  lr:0.000001
[ Sat Jul 13 12:59:27 2024 ] 	Batch(6400/6809) done. Loss: 0.0586  lr:0.000001
[ Sat Jul 13 12:59:45 2024 ] 
Training: Epoch [113/120], Step [6499], Loss: 0.0818883553147316, Training Accuracy: 97.53461538461539
[ Sat Jul 13 12:59:45 2024 ] 	Batch(6500/6809) done. Loss: 0.0480  lr:0.000001
[ Sat Jul 13 13:00:03 2024 ] 	Batch(6600/6809) done. Loss: 0.0066  lr:0.000001
[ Sat Jul 13 13:00:21 2024 ] 	Batch(6700/6809) done. Loss: 0.0629  lr:0.000001
[ Sat Jul 13 13:00:39 2024 ] 	Batch(6800/6809) done. Loss: 0.0071  lr:0.000001
[ Sat Jul 13 13:00:40 2024 ] 	Mean training loss: 0.0920.
[ Sat Jul 13 13:00:40 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 13:00:41 2024 ] Training epoch: 115
[ Sat Jul 13 13:00:41 2024 ] 	Batch(0/6809) done. Loss: 0.0353  lr:0.000001
[ Sat Jul 13 13:00:59 2024 ] 	Batch(100/6809) done. Loss: 0.2398  lr:0.000001
[ Sat Jul 13 13:01:18 2024 ] 	Batch(200/6809) done. Loss: 0.0557  lr:0.000001
[ Sat Jul 13 13:01:36 2024 ] 	Batch(300/6809) done. Loss: 0.0273  lr:0.000001
[ Sat Jul 13 13:01:54 2024 ] 	Batch(400/6809) done. Loss: 0.1706  lr:0.000001
[ Sat Jul 13 13:02:12 2024 ] 
Training: Epoch [114/120], Step [499], Loss: 0.010888856835663319, Training Accuracy: 97.675
[ Sat Jul 13 13:02:12 2024 ] 	Batch(500/6809) done. Loss: 0.0217  lr:0.000001
[ Sat Jul 13 13:02:30 2024 ] 	Batch(600/6809) done. Loss: 0.0042  lr:0.000001
[ Sat Jul 13 13:02:48 2024 ] 	Batch(700/6809) done. Loss: 0.0256  lr:0.000001
[ Sat Jul 13 13:03:06 2024 ] 	Batch(800/6809) done. Loss: 0.0402  lr:0.000001
[ Sat Jul 13 13:03:24 2024 ] 	Batch(900/6809) done. Loss: 0.2728  lr:0.000001
[ Sat Jul 13 13:03:42 2024 ] 
Training: Epoch [114/120], Step [999], Loss: 0.016420243307948112, Training Accuracy: 97.675
[ Sat Jul 13 13:03:42 2024 ] 	Batch(1000/6809) done. Loss: 0.0570  lr:0.000001
[ Sat Jul 13 13:04:00 2024 ] 	Batch(1100/6809) done. Loss: 0.0180  lr:0.000001
[ Sat Jul 13 13:04:18 2024 ] 	Batch(1200/6809) done. Loss: 0.0274  lr:0.000001
[ Sat Jul 13 13:04:37 2024 ] 	Batch(1300/6809) done. Loss: 0.3970  lr:0.000001
[ Sat Jul 13 13:04:55 2024 ] 	Batch(1400/6809) done. Loss: 0.0364  lr:0.000001
[ Sat Jul 13 13:05:13 2024 ] 
Training: Epoch [114/120], Step [1499], Loss: 0.3648468255996704, Training Accuracy: 97.50833333333333
[ Sat Jul 13 13:05:13 2024 ] 	Batch(1500/6809) done. Loss: 0.0694  lr:0.000001
[ Sat Jul 13 13:05:31 2024 ] 	Batch(1600/6809) done. Loss: 0.0416  lr:0.000001
[ Sat Jul 13 13:05:49 2024 ] 	Batch(1700/6809) done. Loss: 0.0026  lr:0.000001
[ Sat Jul 13 13:06:07 2024 ] 	Batch(1800/6809) done. Loss: 0.0033  lr:0.000001
[ Sat Jul 13 13:06:25 2024 ] 	Batch(1900/6809) done. Loss: 0.1012  lr:0.000001
[ Sat Jul 13 13:06:42 2024 ] 
Training: Epoch [114/120], Step [1999], Loss: 0.19596900045871735, Training Accuracy: 97.46875
[ Sat Jul 13 13:06:43 2024 ] 	Batch(2000/6809) done. Loss: 0.1337  lr:0.000001
[ Sat Jul 13 13:07:01 2024 ] 	Batch(2100/6809) done. Loss: 0.0062  lr:0.000001
[ Sat Jul 13 13:07:19 2024 ] 	Batch(2200/6809) done. Loss: 0.0540  lr:0.000001
[ Sat Jul 13 13:07:37 2024 ] 	Batch(2300/6809) done. Loss: 0.0488  lr:0.000001
[ Sat Jul 13 13:07:55 2024 ] 	Batch(2400/6809) done. Loss: 0.1841  lr:0.000001
[ Sat Jul 13 13:08:13 2024 ] 
Training: Epoch [114/120], Step [2499], Loss: 0.020187264308333397, Training Accuracy: 97.58
[ Sat Jul 13 13:08:13 2024 ] 	Batch(2500/6809) done. Loss: 0.0927  lr:0.000001
[ Sat Jul 13 13:08:31 2024 ] 	Batch(2600/6809) done. Loss: 0.1299  lr:0.000001
[ Sat Jul 13 13:08:49 2024 ] 	Batch(2700/6809) done. Loss: 0.0125  lr:0.000001
[ Sat Jul 13 13:09:07 2024 ] 	Batch(2800/6809) done. Loss: 0.0985  lr:0.000001
[ Sat Jul 13 13:09:25 2024 ] 	Batch(2900/6809) done. Loss: 0.0240  lr:0.000001
[ Sat Jul 13 13:09:43 2024 ] 
Training: Epoch [114/120], Step [2999], Loss: 0.015658918768167496, Training Accuracy: 97.55416666666666
[ Sat Jul 13 13:09:43 2024 ] 	Batch(3000/6809) done. Loss: 0.1268  lr:0.000001
[ Sat Jul 13 13:10:01 2024 ] 	Batch(3100/6809) done. Loss: 0.0556  lr:0.000001
[ Sat Jul 13 13:10:19 2024 ] 	Batch(3200/6809) done. Loss: 0.0411  lr:0.000001
[ Sat Jul 13 13:10:37 2024 ] 	Batch(3300/6809) done. Loss: 0.0271  lr:0.000001
[ Sat Jul 13 13:10:55 2024 ] 	Batch(3400/6809) done. Loss: 0.2300  lr:0.000001
[ Sat Jul 13 13:11:13 2024 ] 
Training: Epoch [114/120], Step [3499], Loss: 0.12621229887008667, Training Accuracy: 97.53571428571428
[ Sat Jul 13 13:11:13 2024 ] 	Batch(3500/6809) done. Loss: 0.3930  lr:0.000001
[ Sat Jul 13 13:11:31 2024 ] 	Batch(3600/6809) done. Loss: 0.1184  lr:0.000001
[ Sat Jul 13 13:11:49 2024 ] 	Batch(3700/6809) done. Loss: 0.0161  lr:0.000001
[ Sat Jul 13 13:12:07 2024 ] 	Batch(3800/6809) done. Loss: 0.0020  lr:0.000001
[ Sat Jul 13 13:12:25 2024 ] 	Batch(3900/6809) done. Loss: 0.0188  lr:0.000001
[ Sat Jul 13 13:12:42 2024 ] 
Training: Epoch [114/120], Step [3999], Loss: 0.30385079979896545, Training Accuracy: 97.53125
[ Sat Jul 13 13:12:43 2024 ] 	Batch(4000/6809) done. Loss: 0.0113  lr:0.000001
[ Sat Jul 13 13:13:01 2024 ] 	Batch(4100/6809) done. Loss: 0.1115  lr:0.000001
[ Sat Jul 13 13:13:19 2024 ] 	Batch(4200/6809) done. Loss: 0.3305  lr:0.000001
[ Sat Jul 13 13:13:36 2024 ] 	Batch(4300/6809) done. Loss: 0.0130  lr:0.000001
[ Sat Jul 13 13:13:54 2024 ] 	Batch(4400/6809) done. Loss: 0.2816  lr:0.000001
[ Sat Jul 13 13:14:13 2024 ] 
Training: Epoch [114/120], Step [4499], Loss: 0.005091366358101368, Training Accuracy: 97.50833333333333
[ Sat Jul 13 13:14:13 2024 ] 	Batch(4500/6809) done. Loss: 0.0221  lr:0.000001
[ Sat Jul 13 13:14:32 2024 ] 	Batch(4600/6809) done. Loss: 0.1234  lr:0.000001
[ Sat Jul 13 13:14:50 2024 ] 	Batch(4700/6809) done. Loss: 0.0284  lr:0.000001
[ Sat Jul 13 13:15:09 2024 ] 	Batch(4800/6809) done. Loss: 0.0440  lr:0.000001
[ Sat Jul 13 13:15:27 2024 ] 	Batch(4900/6809) done. Loss: 0.0810  lr:0.000001
[ Sat Jul 13 13:15:46 2024 ] 
Training: Epoch [114/120], Step [4999], Loss: 0.006739126518368721, Training Accuracy: 97.5675
[ Sat Jul 13 13:15:46 2024 ] 	Batch(5000/6809) done. Loss: 0.0129  lr:0.000001
[ Sat Jul 13 13:16:04 2024 ] 	Batch(5100/6809) done. Loss: 0.1477  lr:0.000001
[ Sat Jul 13 13:16:22 2024 ] 	Batch(5200/6809) done. Loss: 0.1227  lr:0.000001
[ Sat Jul 13 13:16:40 2024 ] 	Batch(5300/6809) done. Loss: 0.0243  lr:0.000001
[ Sat Jul 13 13:16:58 2024 ] 	Batch(5400/6809) done. Loss: 0.0108  lr:0.000001
[ Sat Jul 13 13:17:16 2024 ] 
Training: Epoch [114/120], Step [5499], Loss: 0.005182966124266386, Training Accuracy: 97.56590909090909
[ Sat Jul 13 13:17:16 2024 ] 	Batch(5500/6809) done. Loss: 0.0526  lr:0.000001
[ Sat Jul 13 13:17:34 2024 ] 	Batch(5600/6809) done. Loss: 0.0727  lr:0.000001
[ Sat Jul 13 13:17:52 2024 ] 	Batch(5700/6809) done. Loss: 0.3007  lr:0.000001
[ Sat Jul 13 13:18:10 2024 ] 	Batch(5800/6809) done. Loss: 0.0788  lr:0.000001
[ Sat Jul 13 13:18:28 2024 ] 	Batch(5900/6809) done. Loss: 0.0403  lr:0.000001
[ Sat Jul 13 13:18:45 2024 ] 
Training: Epoch [114/120], Step [5999], Loss: 0.1546984165906906, Training Accuracy: 97.58125
[ Sat Jul 13 13:18:46 2024 ] 	Batch(6000/6809) done. Loss: 0.0249  lr:0.000001
[ Sat Jul 13 13:19:04 2024 ] 	Batch(6100/6809) done. Loss: 0.1282  lr:0.000001
[ Sat Jul 13 13:19:23 2024 ] 	Batch(6200/6809) done. Loss: 0.0214  lr:0.000001
[ Sat Jul 13 13:19:41 2024 ] 	Batch(6300/6809) done. Loss: 0.1185  lr:0.000001
[ Sat Jul 13 13:20:00 2024 ] 	Batch(6400/6809) done. Loss: 0.1653  lr:0.000001
[ Sat Jul 13 13:20:19 2024 ] 
Training: Epoch [114/120], Step [6499], Loss: 0.015145612880587578, Training Accuracy: 97.59807692307693
[ Sat Jul 13 13:20:19 2024 ] 	Batch(6500/6809) done. Loss: 0.1855  lr:0.000001
[ Sat Jul 13 13:20:37 2024 ] 	Batch(6600/6809) done. Loss: 0.4353  lr:0.000001
[ Sat Jul 13 13:20:56 2024 ] 	Batch(6700/6809) done. Loss: 0.0993  lr:0.000001
[ Sat Jul 13 13:21:15 2024 ] 	Batch(6800/6809) done. Loss: 0.1995  lr:0.000001
[ Sat Jul 13 13:21:16 2024 ] 	Mean training loss: 0.0960.
[ Sat Jul 13 13:21:16 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 13:21:16 2024 ] Training epoch: 116
[ Sat Jul 13 13:21:17 2024 ] 	Batch(0/6809) done. Loss: 0.2352  lr:0.000001
[ Sat Jul 13 13:21:35 2024 ] 	Batch(100/6809) done. Loss: 0.1088  lr:0.000001
[ Sat Jul 13 13:21:53 2024 ] 	Batch(200/6809) done. Loss: 0.2814  lr:0.000001
[ Sat Jul 13 13:22:11 2024 ] 	Batch(300/6809) done. Loss: 0.1829  lr:0.000001
[ Sat Jul 13 13:22:29 2024 ] 	Batch(400/6809) done. Loss: 0.0025  lr:0.000001
[ Sat Jul 13 13:22:46 2024 ] 
Training: Epoch [115/120], Step [499], Loss: 0.40042293071746826, Training Accuracy: 97.725
[ Sat Jul 13 13:22:46 2024 ] 	Batch(500/6809) done. Loss: 0.1737  lr:0.000001
[ Sat Jul 13 13:23:04 2024 ] 	Batch(600/6809) done. Loss: 0.0041  lr:0.000001
[ Sat Jul 13 13:23:22 2024 ] 	Batch(700/6809) done. Loss: 0.2099  lr:0.000001
[ Sat Jul 13 13:23:40 2024 ] 	Batch(800/6809) done. Loss: 0.3378  lr:0.000001
[ Sat Jul 13 13:23:58 2024 ] 	Batch(900/6809) done. Loss: 0.0869  lr:0.000001
[ Sat Jul 13 13:24:16 2024 ] 
Training: Epoch [115/120], Step [999], Loss: 0.18410561978816986, Training Accuracy: 97.7375
[ Sat Jul 13 13:24:16 2024 ] 	Batch(1000/6809) done. Loss: 0.0017  lr:0.000001
[ Sat Jul 13 13:24:34 2024 ] 	Batch(1100/6809) done. Loss: 0.0169  lr:0.000001
[ Sat Jul 13 13:24:52 2024 ] 	Batch(1200/6809) done. Loss: 0.0939  lr:0.000001
[ Sat Jul 13 13:25:10 2024 ] 	Batch(1300/6809) done. Loss: 0.0578  lr:0.000001
[ Sat Jul 13 13:25:28 2024 ] 	Batch(1400/6809) done. Loss: 0.4329  lr:0.000001
[ Sat Jul 13 13:25:46 2024 ] 
Training: Epoch [115/120], Step [1499], Loss: 0.007672411855310202, Training Accuracy: 97.75
[ Sat Jul 13 13:25:46 2024 ] 	Batch(1500/6809) done. Loss: 0.0050  lr:0.000001
[ Sat Jul 13 13:26:04 2024 ] 	Batch(1600/6809) done. Loss: 0.0046  lr:0.000001
[ Sat Jul 13 13:26:22 2024 ] 	Batch(1700/6809) done. Loss: 0.0346  lr:0.000001
[ Sat Jul 13 13:26:40 2024 ] 	Batch(1800/6809) done. Loss: 0.0784  lr:0.000001
[ Sat Jul 13 13:26:58 2024 ] 	Batch(1900/6809) done. Loss: 0.0610  lr:0.000001
[ Sat Jul 13 13:27:15 2024 ] 
Training: Epoch [115/120], Step [1999], Loss: 0.01842309907078743, Training Accuracy: 97.70625
[ Sat Jul 13 13:27:16 2024 ] 	Batch(2000/6809) done. Loss: 0.0888  lr:0.000001
[ Sat Jul 13 13:27:34 2024 ] 	Batch(2100/6809) done. Loss: 0.0102  lr:0.000001
[ Sat Jul 13 13:27:51 2024 ] 	Batch(2200/6809) done. Loss: 0.1790  lr:0.000001
[ Sat Jul 13 13:28:09 2024 ] 	Batch(2300/6809) done. Loss: 0.0069  lr:0.000001
[ Sat Jul 13 13:28:27 2024 ] 	Batch(2400/6809) done. Loss: 0.0521  lr:0.000001
[ Sat Jul 13 13:28:45 2024 ] 
Training: Epoch [115/120], Step [2499], Loss: 0.058850161731243134, Training Accuracy: 97.695
[ Sat Jul 13 13:28:45 2024 ] 	Batch(2500/6809) done. Loss: 0.0255  lr:0.000001
[ Sat Jul 13 13:29:03 2024 ] 	Batch(2600/6809) done. Loss: 0.0906  lr:0.000001
[ Sat Jul 13 13:29:21 2024 ] 	Batch(2700/6809) done. Loss: 0.0142  lr:0.000001
[ Sat Jul 13 13:29:39 2024 ] 	Batch(2800/6809) done. Loss: 0.1069  lr:0.000001
[ Sat Jul 13 13:29:57 2024 ] 	Batch(2900/6809) done. Loss: 0.1390  lr:0.000001
[ Sat Jul 13 13:30:15 2024 ] 
Training: Epoch [115/120], Step [2999], Loss: 0.07949574291706085, Training Accuracy: 97.72083333333333
[ Sat Jul 13 13:30:15 2024 ] 	Batch(3000/6809) done. Loss: 0.0684  lr:0.000001
[ Sat Jul 13 13:30:33 2024 ] 	Batch(3100/6809) done. Loss: 0.0227  lr:0.000001
[ Sat Jul 13 13:30:51 2024 ] 	Batch(3200/6809) done. Loss: 0.5465  lr:0.000001
[ Sat Jul 13 13:31:09 2024 ] 	Batch(3300/6809) done. Loss: 0.0287  lr:0.000001
[ Sat Jul 13 13:31:26 2024 ] 	Batch(3400/6809) done. Loss: 0.0282  lr:0.000001
[ Sat Jul 13 13:31:44 2024 ] 
Training: Epoch [115/120], Step [3499], Loss: 0.16885319352149963, Training Accuracy: 97.775
[ Sat Jul 13 13:31:44 2024 ] 	Batch(3500/6809) done. Loss: 0.0753  lr:0.000001
[ Sat Jul 13 13:32:02 2024 ] 	Batch(3600/6809) done. Loss: 0.0604  lr:0.000001
[ Sat Jul 13 13:32:20 2024 ] 	Batch(3700/6809) done. Loss: 0.0181  lr:0.000001
[ Sat Jul 13 13:32:38 2024 ] 	Batch(3800/6809) done. Loss: 0.0484  lr:0.000001
[ Sat Jul 13 13:32:56 2024 ] 	Batch(3900/6809) done. Loss: 0.0116  lr:0.000001
[ Sat Jul 13 13:33:14 2024 ] 
Training: Epoch [115/120], Step [3999], Loss: 0.010263804346323013, Training Accuracy: 97.734375
[ Sat Jul 13 13:33:14 2024 ] 	Batch(4000/6809) done. Loss: 0.0965  lr:0.000001
[ Sat Jul 13 13:33:33 2024 ] 	Batch(4100/6809) done. Loss: 0.0096  lr:0.000001
[ Sat Jul 13 13:33:51 2024 ] 	Batch(4200/6809) done. Loss: 0.0077  lr:0.000001
[ Sat Jul 13 13:34:09 2024 ] 	Batch(4300/6809) done. Loss: 0.2294  lr:0.000001
[ Sat Jul 13 13:34:27 2024 ] 	Batch(4400/6809) done. Loss: 0.0627  lr:0.000001
[ Sat Jul 13 13:34:45 2024 ] 
Training: Epoch [115/120], Step [4499], Loss: 0.05838694050908089, Training Accuracy: 97.68333333333334
[ Sat Jul 13 13:34:45 2024 ] 	Batch(4500/6809) done. Loss: 0.0368  lr:0.000001
[ Sat Jul 13 13:35:04 2024 ] 	Batch(4600/6809) done. Loss: 0.0060  lr:0.000001
[ Sat Jul 13 13:35:22 2024 ] 	Batch(4700/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 13:35:41 2024 ] 	Batch(4800/6809) done. Loss: 0.0402  lr:0.000001
[ Sat Jul 13 13:35:59 2024 ] 	Batch(4900/6809) done. Loss: 0.0051  lr:0.000001
[ Sat Jul 13 13:36:18 2024 ] 
Training: Epoch [115/120], Step [4999], Loss: 0.04604000598192215, Training Accuracy: 97.6225
[ Sat Jul 13 13:36:18 2024 ] 	Batch(5000/6809) done. Loss: 0.1076  lr:0.000001
[ Sat Jul 13 13:36:36 2024 ] 	Batch(5100/6809) done. Loss: 0.0240  lr:0.000001
[ Sat Jul 13 13:36:54 2024 ] 	Batch(5200/6809) done. Loss: 0.0120  lr:0.000001
[ Sat Jul 13 13:37:12 2024 ] 	Batch(5300/6809) done. Loss: 0.1033  lr:0.000001
[ Sat Jul 13 13:37:30 2024 ] 	Batch(5400/6809) done. Loss: 0.1477  lr:0.000001
[ Sat Jul 13 13:37:48 2024 ] 
Training: Epoch [115/120], Step [5499], Loss: 0.021055908873677254, Training Accuracy: 97.62954545454545
[ Sat Jul 13 13:37:48 2024 ] 	Batch(5500/6809) done. Loss: 0.0160  lr:0.000001
[ Sat Jul 13 13:38:06 2024 ] 	Batch(5600/6809) done. Loss: 0.2809  lr:0.000001
[ Sat Jul 13 13:38:24 2024 ] 	Batch(5700/6809) done. Loss: 0.0058  lr:0.000001
[ Sat Jul 13 13:38:41 2024 ] 	Batch(5800/6809) done. Loss: 0.0950  lr:0.000001
[ Sat Jul 13 13:38:59 2024 ] 	Batch(5900/6809) done. Loss: 0.0727  lr:0.000001
[ Sat Jul 13 13:39:17 2024 ] 
Training: Epoch [115/120], Step [5999], Loss: 0.016509221866726875, Training Accuracy: 97.61875
[ Sat Jul 13 13:39:17 2024 ] 	Batch(6000/6809) done. Loss: 0.0331  lr:0.000001
[ Sat Jul 13 13:39:35 2024 ] 	Batch(6100/6809) done. Loss: 0.1284  lr:0.000001
[ Sat Jul 13 13:39:53 2024 ] 	Batch(6200/6809) done. Loss: 0.2510  lr:0.000001
[ Sat Jul 13 13:40:11 2024 ] 	Batch(6300/6809) done. Loss: 0.0088  lr:0.000001
[ Sat Jul 13 13:40:29 2024 ] 	Batch(6400/6809) done. Loss: 0.1972  lr:0.000001
[ Sat Jul 13 13:40:47 2024 ] 
Training: Epoch [115/120], Step [6499], Loss: 0.16974234580993652, Training Accuracy: 97.61923076923077
[ Sat Jul 13 13:40:47 2024 ] 	Batch(6500/6809) done. Loss: 0.0213  lr:0.000001
[ Sat Jul 13 13:41:06 2024 ] 	Batch(6600/6809) done. Loss: 0.0902  lr:0.000001
[ Sat Jul 13 13:41:25 2024 ] 	Batch(6700/6809) done. Loss: 0.2075  lr:0.000001
[ Sat Jul 13 13:41:43 2024 ] 	Batch(6800/6809) done. Loss: 0.0190  lr:0.000001
[ Sat Jul 13 13:41:45 2024 ] 	Mean training loss: 0.0936.
[ Sat Jul 13 13:41:45 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 13:41:45 2024 ] Training epoch: 117
[ Sat Jul 13 13:41:45 2024 ] 	Batch(0/6809) done. Loss: 0.1861  lr:0.000001
[ Sat Jul 13 13:42:04 2024 ] 	Batch(100/6809) done. Loss: 0.0668  lr:0.000001
[ Sat Jul 13 13:42:22 2024 ] 	Batch(200/6809) done. Loss: 0.0408  lr:0.000001
[ Sat Jul 13 13:42:40 2024 ] 	Batch(300/6809) done. Loss: 0.0237  lr:0.000001
[ Sat Jul 13 13:42:59 2024 ] 	Batch(400/6809) done. Loss: 0.0227  lr:0.000001
[ Sat Jul 13 13:43:17 2024 ] 
Training: Epoch [116/120], Step [499], Loss: 0.0024140586610883474, Training Accuracy: 97.95
[ Sat Jul 13 13:43:17 2024 ] 	Batch(500/6809) done. Loss: 0.0371  lr:0.000001
[ Sat Jul 13 13:43:35 2024 ] 	Batch(600/6809) done. Loss: 0.0825  lr:0.000001
[ Sat Jul 13 13:43:53 2024 ] 	Batch(700/6809) done. Loss: 0.1222  lr:0.000001
[ Sat Jul 13 13:44:12 2024 ] 	Batch(800/6809) done. Loss: 0.2822  lr:0.000001
[ Sat Jul 13 13:44:30 2024 ] 	Batch(900/6809) done. Loss: 0.1490  lr:0.000001
[ Sat Jul 13 13:44:48 2024 ] 
Training: Epoch [116/120], Step [999], Loss: 0.38898706436157227, Training Accuracy: 97.625
[ Sat Jul 13 13:44:49 2024 ] 	Batch(1000/6809) done. Loss: 0.0158  lr:0.000001
[ Sat Jul 13 13:45:07 2024 ] 	Batch(1100/6809) done. Loss: 0.0189  lr:0.000001
[ Sat Jul 13 13:45:25 2024 ] 	Batch(1200/6809) done. Loss: 0.1442  lr:0.000001
[ Sat Jul 13 13:45:43 2024 ] 	Batch(1300/6809) done. Loss: 0.0066  lr:0.000001
[ Sat Jul 13 13:46:01 2024 ] 	Batch(1400/6809) done. Loss: 0.1863  lr:0.000001
[ Sat Jul 13 13:46:19 2024 ] 
Training: Epoch [116/120], Step [1499], Loss: 0.030957894399762154, Training Accuracy: 97.7
[ Sat Jul 13 13:46:19 2024 ] 	Batch(1500/6809) done. Loss: 0.0021  lr:0.000001
[ Sat Jul 13 13:46:37 2024 ] 	Batch(1600/6809) done. Loss: 0.0637  lr:0.000001
[ Sat Jul 13 13:46:55 2024 ] 	Batch(1700/6809) done. Loss: 0.1864  lr:0.000001
[ Sat Jul 13 13:47:13 2024 ] 	Batch(1800/6809) done. Loss: 0.1710  lr:0.000001
[ Sat Jul 13 13:47:30 2024 ] 	Batch(1900/6809) done. Loss: 0.0049  lr:0.000001
[ Sat Jul 13 13:47:48 2024 ] 
Training: Epoch [116/120], Step [1999], Loss: 0.36494988203048706, Training Accuracy: 97.64375
[ Sat Jul 13 13:47:49 2024 ] 	Batch(2000/6809) done. Loss: 0.1870  lr:0.000001
[ Sat Jul 13 13:48:07 2024 ] 	Batch(2100/6809) done. Loss: 0.0137  lr:0.000001
[ Sat Jul 13 13:48:26 2024 ] 	Batch(2200/6809) done. Loss: 0.0157  lr:0.000001
[ Sat Jul 13 13:48:44 2024 ] 	Batch(2300/6809) done. Loss: 0.1968  lr:0.000001
[ Sat Jul 13 13:49:03 2024 ] 	Batch(2400/6809) done. Loss: 0.2620  lr:0.000001
[ Sat Jul 13 13:49:21 2024 ] 
Training: Epoch [116/120], Step [2499], Loss: 0.03033435344696045, Training Accuracy: 97.595
[ Sat Jul 13 13:49:21 2024 ] 	Batch(2500/6809) done. Loss: 0.0610  lr:0.000001
[ Sat Jul 13 13:49:40 2024 ] 	Batch(2600/6809) done. Loss: 0.0139  lr:0.000001
[ Sat Jul 13 13:49:58 2024 ] 	Batch(2700/6809) done. Loss: 0.0035  lr:0.000001
[ Sat Jul 13 13:50:16 2024 ] 	Batch(2800/6809) done. Loss: 0.2301  lr:0.000001
[ Sat Jul 13 13:50:34 2024 ] 	Batch(2900/6809) done. Loss: 0.0102  lr:0.000001
[ Sat Jul 13 13:50:51 2024 ] 
Training: Epoch [116/120], Step [2999], Loss: 0.2515299916267395, Training Accuracy: 97.6375
[ Sat Jul 13 13:50:52 2024 ] 	Batch(3000/6809) done. Loss: 0.0180  lr:0.000001
[ Sat Jul 13 13:51:09 2024 ] 	Batch(3100/6809) done. Loss: 0.1934  lr:0.000001
[ Sat Jul 13 13:51:27 2024 ] 	Batch(3200/6809) done. Loss: 0.0322  lr:0.000001
[ Sat Jul 13 13:51:45 2024 ] 	Batch(3300/6809) done. Loss: 0.0250  lr:0.000001
[ Sat Jul 13 13:52:03 2024 ] 	Batch(3400/6809) done. Loss: 0.0670  lr:0.000001
[ Sat Jul 13 13:52:21 2024 ] 
Training: Epoch [116/120], Step [3499], Loss: 0.009924065321683884, Training Accuracy: 97.58928571428571
[ Sat Jul 13 13:52:21 2024 ] 	Batch(3500/6809) done. Loss: 0.0767  lr:0.000001
[ Sat Jul 13 13:52:39 2024 ] 	Batch(3600/6809) done. Loss: 0.1812  lr:0.000001
[ Sat Jul 13 13:52:57 2024 ] 	Batch(3700/6809) done. Loss: 0.0885  lr:0.000001
[ Sat Jul 13 13:53:15 2024 ] 	Batch(3800/6809) done. Loss: 0.0422  lr:0.000001
[ Sat Jul 13 13:53:33 2024 ] 	Batch(3900/6809) done. Loss: 0.2050  lr:0.000001
[ Sat Jul 13 13:53:50 2024 ] 
Training: Epoch [116/120], Step [3999], Loss: 0.20155125856399536, Training Accuracy: 97.575
[ Sat Jul 13 13:53:51 2024 ] 	Batch(4000/6809) done. Loss: 0.0819  lr:0.000001
[ Sat Jul 13 13:54:09 2024 ] 	Batch(4100/6809) done. Loss: 0.0219  lr:0.000001
[ Sat Jul 13 13:54:26 2024 ] 	Batch(4200/6809) done. Loss: 0.0172  lr:0.000001
[ Sat Jul 13 13:54:44 2024 ] 	Batch(4300/6809) done. Loss: 0.0665  lr:0.000001
[ Sat Jul 13 13:55:02 2024 ] 	Batch(4400/6809) done. Loss: 0.1333  lr:0.000001
[ Sat Jul 13 13:55:20 2024 ] 
Training: Epoch [116/120], Step [4499], Loss: 0.051171887665987015, Training Accuracy: 97.58333333333333
[ Sat Jul 13 13:55:20 2024 ] 	Batch(4500/6809) done. Loss: 0.1485  lr:0.000001
[ Sat Jul 13 13:55:38 2024 ] 	Batch(4600/6809) done. Loss: 0.0274  lr:0.000001
[ Sat Jul 13 13:55:56 2024 ] 	Batch(4700/6809) done. Loss: 0.0397  lr:0.000001
[ Sat Jul 13 13:56:15 2024 ] 	Batch(4800/6809) done. Loss: 0.1265  lr:0.000001
[ Sat Jul 13 13:56:33 2024 ] 	Batch(4900/6809) done. Loss: 0.1518  lr:0.000001
[ Sat Jul 13 13:56:51 2024 ] 
Training: Epoch [116/120], Step [4999], Loss: 0.057315267622470856, Training Accuracy: 97.58250000000001
[ Sat Jul 13 13:56:51 2024 ] 	Batch(5000/6809) done. Loss: 0.0460  lr:0.000001
[ Sat Jul 13 13:57:09 2024 ] 	Batch(5100/6809) done. Loss: 0.0204  lr:0.000001
[ Sat Jul 13 13:57:27 2024 ] 	Batch(5200/6809) done. Loss: 0.0970  lr:0.000001
[ Sat Jul 13 13:57:45 2024 ] 	Batch(5300/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 13:58:03 2024 ] 	Batch(5400/6809) done. Loss: 0.1579  lr:0.000001
[ Sat Jul 13 13:58:21 2024 ] 
Training: Epoch [116/120], Step [5499], Loss: 0.03836100921034813, Training Accuracy: 97.56363636363636
[ Sat Jul 13 13:58:21 2024 ] 	Batch(5500/6809) done. Loss: 0.0658  lr:0.000001
[ Sat Jul 13 13:58:39 2024 ] 	Batch(5600/6809) done. Loss: 0.0091  lr:0.000001
[ Sat Jul 13 13:58:57 2024 ] 	Batch(5700/6809) done. Loss: 0.0856  lr:0.000001
[ Sat Jul 13 13:59:15 2024 ] 	Batch(5800/6809) done. Loss: 0.1235  lr:0.000001
[ Sat Jul 13 13:59:33 2024 ] 	Batch(5900/6809) done. Loss: 0.0299  lr:0.000001
[ Sat Jul 13 13:59:50 2024 ] 
Training: Epoch [116/120], Step [5999], Loss: 0.007402638904750347, Training Accuracy: 97.55416666666666
[ Sat Jul 13 13:59:51 2024 ] 	Batch(6000/6809) done. Loss: 0.1292  lr:0.000001
[ Sat Jul 13 14:00:08 2024 ] 	Batch(6100/6809) done. Loss: 0.0180  lr:0.000001
[ Sat Jul 13 14:00:26 2024 ] 	Batch(6200/6809) done. Loss: 0.0952  lr:0.000001
[ Sat Jul 13 14:00:44 2024 ] 	Batch(6300/6809) done. Loss: 0.2145  lr:0.000001
[ Sat Jul 13 14:01:02 2024 ] 	Batch(6400/6809) done. Loss: 0.0060  lr:0.000001
[ Sat Jul 13 14:01:21 2024 ] 
Training: Epoch [116/120], Step [6499], Loss: 0.0882696807384491, Training Accuracy: 97.52884615384615
[ Sat Jul 13 14:01:21 2024 ] 	Batch(6500/6809) done. Loss: 0.0685  lr:0.000001
[ Sat Jul 13 14:01:39 2024 ] 	Batch(6600/6809) done. Loss: 0.0032  lr:0.000001
[ Sat Jul 13 14:01:57 2024 ] 	Batch(6700/6809) done. Loss: 0.1047  lr:0.000001
[ Sat Jul 13 14:02:15 2024 ] 	Batch(6800/6809) done. Loss: 0.0113  lr:0.000001
[ Sat Jul 13 14:02:16 2024 ] 	Mean training loss: 0.0953.
[ Sat Jul 13 14:02:16 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 14:02:16 2024 ] Training epoch: 118
[ Sat Jul 13 14:02:17 2024 ] 	Batch(0/6809) done. Loss: 0.4745  lr:0.000001
[ Sat Jul 13 14:02:35 2024 ] 	Batch(100/6809) done. Loss: 0.1207  lr:0.000001
[ Sat Jul 13 14:02:53 2024 ] 	Batch(200/6809) done. Loss: 0.4617  lr:0.000001
[ Sat Jul 13 14:03:10 2024 ] 	Batch(300/6809) done. Loss: 0.0893  lr:0.000001
[ Sat Jul 13 14:03:29 2024 ] 	Batch(400/6809) done. Loss: 0.4578  lr:0.000001
[ Sat Jul 13 14:03:47 2024 ] 
Training: Epoch [117/120], Step [499], Loss: 0.413283109664917, Training Accuracy: 97.75
[ Sat Jul 13 14:03:47 2024 ] 	Batch(500/6809) done. Loss: 0.1475  lr:0.000001
[ Sat Jul 13 14:04:06 2024 ] 	Batch(600/6809) done. Loss: 0.0516  lr:0.000001
[ Sat Jul 13 14:04:24 2024 ] 	Batch(700/6809) done. Loss: 0.0989  lr:0.000001
[ Sat Jul 13 14:04:43 2024 ] 	Batch(800/6809) done. Loss: 0.3024  lr:0.000001
[ Sat Jul 13 14:05:01 2024 ] 	Batch(900/6809) done. Loss: 0.0130  lr:0.000001
[ Sat Jul 13 14:05:20 2024 ] 
Training: Epoch [117/120], Step [999], Loss: 0.015554316341876984, Training Accuracy: 97.75
[ Sat Jul 13 14:05:20 2024 ] 	Batch(1000/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 14:05:38 2024 ] 	Batch(1100/6809) done. Loss: 0.0063  lr:0.000001
[ Sat Jul 13 14:05:57 2024 ] 	Batch(1200/6809) done. Loss: 0.0480  lr:0.000001
[ Sat Jul 13 14:06:16 2024 ] 	Batch(1300/6809) done. Loss: 0.0496  lr:0.000001
[ Sat Jul 13 14:06:34 2024 ] 	Batch(1400/6809) done. Loss: 0.2460  lr:0.000001
[ Sat Jul 13 14:06:52 2024 ] 
Training: Epoch [117/120], Step [1499], Loss: 0.3089272975921631, Training Accuracy: 97.55833333333334
[ Sat Jul 13 14:06:53 2024 ] 	Batch(1500/6809) done. Loss: 0.0238  lr:0.000001
[ Sat Jul 13 14:07:11 2024 ] 	Batch(1600/6809) done. Loss: 0.0099  lr:0.000001
[ Sat Jul 13 14:07:30 2024 ] 	Batch(1700/6809) done. Loss: 0.0101  lr:0.000001
[ Sat Jul 13 14:07:48 2024 ] 	Batch(1800/6809) done. Loss: 0.0997  lr:0.000001
[ Sat Jul 13 14:08:06 2024 ] 	Batch(1900/6809) done. Loss: 0.4613  lr:0.000001
[ Sat Jul 13 14:08:24 2024 ] 
Training: Epoch [117/120], Step [1999], Loss: 0.02115379273891449, Training Accuracy: 97.5
[ Sat Jul 13 14:08:24 2024 ] 	Batch(2000/6809) done. Loss: 0.1435  lr:0.000001
[ Sat Jul 13 14:08:42 2024 ] 	Batch(2100/6809) done. Loss: 0.4395  lr:0.000001
[ Sat Jul 13 14:09:00 2024 ] 	Batch(2200/6809) done. Loss: 0.1410  lr:0.000001
[ Sat Jul 13 14:09:18 2024 ] 	Batch(2300/6809) done. Loss: 0.2016  lr:0.000001
[ Sat Jul 13 14:09:36 2024 ] 	Batch(2400/6809) done. Loss: 0.3683  lr:0.000001
[ Sat Jul 13 14:09:53 2024 ] 
Training: Epoch [117/120], Step [2499], Loss: 0.1731518805027008, Training Accuracy: 97.50999999999999
[ Sat Jul 13 14:09:54 2024 ] 	Batch(2500/6809) done. Loss: 0.0841  lr:0.000001
[ Sat Jul 13 14:10:11 2024 ] 	Batch(2600/6809) done. Loss: 0.0376  lr:0.000001
[ Sat Jul 13 14:10:29 2024 ] 	Batch(2700/6809) done. Loss: 0.1097  lr:0.000001
[ Sat Jul 13 14:10:47 2024 ] 	Batch(2800/6809) done. Loss: 0.0509  lr:0.000001
[ Sat Jul 13 14:11:05 2024 ] 	Batch(2900/6809) done. Loss: 0.1023  lr:0.000001
[ Sat Jul 13 14:11:23 2024 ] 
Training: Epoch [117/120], Step [2999], Loss: 0.011353119276463985, Training Accuracy: 97.55
[ Sat Jul 13 14:11:23 2024 ] 	Batch(3000/6809) done. Loss: 0.0481  lr:0.000001
[ Sat Jul 13 14:11:41 2024 ] 	Batch(3100/6809) done. Loss: 0.2694  lr:0.000001
[ Sat Jul 13 14:12:00 2024 ] 	Batch(3200/6809) done. Loss: 0.0372  lr:0.000001
[ Sat Jul 13 14:12:18 2024 ] 	Batch(3300/6809) done. Loss: 0.0115  lr:0.000001
[ Sat Jul 13 14:12:36 2024 ] 	Batch(3400/6809) done. Loss: 0.2455  lr:0.000001
[ Sat Jul 13 14:12:54 2024 ] 
Training: Epoch [117/120], Step [3499], Loss: 0.0007443941431120038, Training Accuracy: 97.55
[ Sat Jul 13 14:12:54 2024 ] 	Batch(3500/6809) done. Loss: 0.0677  lr:0.000001
[ Sat Jul 13 14:13:12 2024 ] 	Batch(3600/6809) done. Loss: 0.0160  lr:0.000001
[ Sat Jul 13 14:13:30 2024 ] 	Batch(3700/6809) done. Loss: 0.0123  lr:0.000001
[ Sat Jul 13 14:13:48 2024 ] 	Batch(3800/6809) done. Loss: 0.0502  lr:0.000001
[ Sat Jul 13 14:14:06 2024 ] 	Batch(3900/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 14:14:23 2024 ] 
Training: Epoch [117/120], Step [3999], Loss: 0.028667835518717766, Training Accuracy: 97.50937499999999
[ Sat Jul 13 14:14:24 2024 ] 	Batch(4000/6809) done. Loss: 0.0940  lr:0.000001
[ Sat Jul 13 14:14:42 2024 ] 	Batch(4100/6809) done. Loss: 0.0400  lr:0.000001
[ Sat Jul 13 14:14:59 2024 ] 	Batch(4200/6809) done. Loss: 0.0299  lr:0.000001
[ Sat Jul 13 14:15:17 2024 ] 	Batch(4300/6809) done. Loss: 0.0303  lr:0.000001
[ Sat Jul 13 14:15:35 2024 ] 	Batch(4400/6809) done. Loss: 0.1040  lr:0.000001
[ Sat Jul 13 14:15:54 2024 ] 
Training: Epoch [117/120], Step [4499], Loss: 0.12444882094860077, Training Accuracy: 97.52499999999999
[ Sat Jul 13 14:15:54 2024 ] 	Batch(4500/6809) done. Loss: 0.0208  lr:0.000001
[ Sat Jul 13 14:16:12 2024 ] 	Batch(4600/6809) done. Loss: 0.0488  lr:0.000001
[ Sat Jul 13 14:16:30 2024 ] 	Batch(4700/6809) done. Loss: 0.0183  lr:0.000001
[ Sat Jul 13 14:16:48 2024 ] 	Batch(4800/6809) done. Loss: 0.0923  lr:0.000001
[ Sat Jul 13 14:17:07 2024 ] 	Batch(4900/6809) done. Loss: 0.1536  lr:0.000001
[ Sat Jul 13 14:17:25 2024 ] 
Training: Epoch [117/120], Step [4999], Loss: 0.0064723496325314045, Training Accuracy: 97.56
[ Sat Jul 13 14:17:25 2024 ] 	Batch(5000/6809) done. Loss: 0.0034  lr:0.000001
[ Sat Jul 13 14:17:44 2024 ] 	Batch(5100/6809) done. Loss: 0.0675  lr:0.000001
[ Sat Jul 13 14:18:02 2024 ] 	Batch(5200/6809) done. Loss: 0.0335  lr:0.000001
[ Sat Jul 13 14:18:21 2024 ] 	Batch(5300/6809) done. Loss: 0.2754  lr:0.000001
[ Sat Jul 13 14:18:38 2024 ] 	Batch(5400/6809) done. Loss: 0.1340  lr:0.000001
[ Sat Jul 13 14:18:56 2024 ] 
Training: Epoch [117/120], Step [5499], Loss: 0.007040345575660467, Training Accuracy: 97.54772727272727
[ Sat Jul 13 14:18:56 2024 ] 	Batch(5500/6809) done. Loss: 0.0529  lr:0.000001
[ Sat Jul 13 14:19:14 2024 ] 	Batch(5600/6809) done. Loss: 0.0031  lr:0.000001
[ Sat Jul 13 14:19:32 2024 ] 	Batch(5700/6809) done. Loss: 0.1628  lr:0.000001
[ Sat Jul 13 14:19:50 2024 ] 	Batch(5800/6809) done. Loss: 0.1397  lr:0.000001
[ Sat Jul 13 14:20:08 2024 ] 	Batch(5900/6809) done. Loss: 0.1314  lr:0.000001
[ Sat Jul 13 14:20:26 2024 ] 
Training: Epoch [117/120], Step [5999], Loss: 0.09164329618215561, Training Accuracy: 97.59583333333333
[ Sat Jul 13 14:20:26 2024 ] 	Batch(6000/6809) done. Loss: 0.2555  lr:0.000001
[ Sat Jul 13 14:20:45 2024 ] 	Batch(6100/6809) done. Loss: 0.0337  lr:0.000001
[ Sat Jul 13 14:21:03 2024 ] 	Batch(6200/6809) done. Loss: 0.0118  lr:0.000001
[ Sat Jul 13 14:21:21 2024 ] 	Batch(6300/6809) done. Loss: 0.0306  lr:0.000001
[ Sat Jul 13 14:21:39 2024 ] 	Batch(6400/6809) done. Loss: 0.1222  lr:0.000001
[ Sat Jul 13 14:21:58 2024 ] 
Training: Epoch [117/120], Step [6499], Loss: 0.013486403971910477, Training Accuracy: 97.61923076923077
[ Sat Jul 13 14:21:58 2024 ] 	Batch(6500/6809) done. Loss: 0.1067  lr:0.000001
[ Sat Jul 13 14:22:16 2024 ] 	Batch(6600/6809) done. Loss: 0.0097  lr:0.000001
[ Sat Jul 13 14:22:35 2024 ] 	Batch(6700/6809) done. Loss: 0.0446  lr:0.000001
[ Sat Jul 13 14:22:53 2024 ] 	Batch(6800/6809) done. Loss: 0.1767  lr:0.000001
[ Sat Jul 13 14:22:55 2024 ] 	Mean training loss: 0.0955.
[ Sat Jul 13 14:22:55 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 14:22:55 2024 ] Training epoch: 119
[ Sat Jul 13 14:22:55 2024 ] 	Batch(0/6809) done. Loss: 0.0749  lr:0.000001
[ Sat Jul 13 14:23:14 2024 ] 	Batch(100/6809) done. Loss: 0.0946  lr:0.000001
[ Sat Jul 13 14:23:32 2024 ] 	Batch(200/6809) done. Loss: 0.0422  lr:0.000001
[ Sat Jul 13 14:23:49 2024 ] 	Batch(300/6809) done. Loss: 0.0436  lr:0.000001
[ Sat Jul 13 14:24:07 2024 ] 	Batch(400/6809) done. Loss: 0.0083  lr:0.000001
[ Sat Jul 13 14:24:25 2024 ] 
Training: Epoch [118/120], Step [499], Loss: 0.08463970571756363, Training Accuracy: 97.575
[ Sat Jul 13 14:24:25 2024 ] 	Batch(500/6809) done. Loss: 0.0263  lr:0.000001
[ Sat Jul 13 14:24:43 2024 ] 	Batch(600/6809) done. Loss: 0.1730  lr:0.000001
[ Sat Jul 13 14:25:01 2024 ] 	Batch(700/6809) done. Loss: 0.0253  lr:0.000001
[ Sat Jul 13 14:25:19 2024 ] 	Batch(800/6809) done. Loss: 0.0149  lr:0.000001
[ Sat Jul 13 14:25:37 2024 ] 	Batch(900/6809) done. Loss: 0.3648  lr:0.000001
[ Sat Jul 13 14:25:55 2024 ] 
Training: Epoch [118/120], Step [999], Loss: 0.6111279129981995, Training Accuracy: 97.6375
[ Sat Jul 13 14:25:55 2024 ] 	Batch(1000/6809) done. Loss: 0.0090  lr:0.000001
[ Sat Jul 13 14:26:13 2024 ] 	Batch(1100/6809) done. Loss: 0.1018  lr:0.000001
[ Sat Jul 13 14:26:31 2024 ] 	Batch(1200/6809) done. Loss: 0.0011  lr:0.000001
[ Sat Jul 13 14:26:49 2024 ] 	Batch(1300/6809) done. Loss: 0.2143  lr:0.000001
[ Sat Jul 13 14:27:06 2024 ] 	Batch(1400/6809) done. Loss: 0.0079  lr:0.000001
[ Sat Jul 13 14:27:24 2024 ] 
Training: Epoch [118/120], Step [1499], Loss: 0.04110048711299896, Training Accuracy: 97.53333333333333
[ Sat Jul 13 14:27:24 2024 ] 	Batch(1500/6809) done. Loss: 0.0131  lr:0.000001
[ Sat Jul 13 14:27:42 2024 ] 	Batch(1600/6809) done. Loss: 0.0038  lr:0.000001
[ Sat Jul 13 14:28:01 2024 ] 	Batch(1700/6809) done. Loss: 0.2013  lr:0.000001
[ Sat Jul 13 14:28:19 2024 ] 	Batch(1800/6809) done. Loss: 0.0179  lr:0.000001
[ Sat Jul 13 14:28:36 2024 ] 	Batch(1900/6809) done. Loss: 0.3838  lr:0.000001
[ Sat Jul 13 14:28:54 2024 ] 
Training: Epoch [118/120], Step [1999], Loss: 0.04864858090877533, Training Accuracy: 97.51875
[ Sat Jul 13 14:28:55 2024 ] 	Batch(2000/6809) done. Loss: 0.1087  lr:0.000001
[ Sat Jul 13 14:29:13 2024 ] 	Batch(2100/6809) done. Loss: 0.0637  lr:0.000001
[ Sat Jul 13 14:29:32 2024 ] 	Batch(2200/6809) done. Loss: 0.0400  lr:0.000001
[ Sat Jul 13 14:29:50 2024 ] 	Batch(2300/6809) done. Loss: 0.0155  lr:0.000001
[ Sat Jul 13 14:30:09 2024 ] 	Batch(2400/6809) done. Loss: 0.2013  lr:0.000001
[ Sat Jul 13 14:30:27 2024 ] 
Training: Epoch [118/120], Step [2499], Loss: 0.06961387395858765, Training Accuracy: 97.505
[ Sat Jul 13 14:30:27 2024 ] 	Batch(2500/6809) done. Loss: 0.2099  lr:0.000001
[ Sat Jul 13 14:30:46 2024 ] 	Batch(2600/6809) done. Loss: 0.1381  lr:0.000001
[ Sat Jul 13 14:31:04 2024 ] 	Batch(2700/6809) done. Loss: 0.1003  lr:0.000001
[ Sat Jul 13 14:31:23 2024 ] 	Batch(2800/6809) done. Loss: 0.1341  lr:0.000001
[ Sat Jul 13 14:31:41 2024 ] 	Batch(2900/6809) done. Loss: 0.0193  lr:0.000001
[ Sat Jul 13 14:31:59 2024 ] 
Training: Epoch [118/120], Step [2999], Loss: 0.021914087235927582, Training Accuracy: 97.59583333333333
[ Sat Jul 13 14:31:59 2024 ] 	Batch(3000/6809) done. Loss: 0.0022  lr:0.000001
[ Sat Jul 13 14:32:17 2024 ] 	Batch(3100/6809) done. Loss: 0.0636  lr:0.000001
[ Sat Jul 13 14:32:35 2024 ] 	Batch(3200/6809) done. Loss: 0.0892  lr:0.000001
[ Sat Jul 13 14:32:53 2024 ] 	Batch(3300/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 14:33:12 2024 ] 	Batch(3400/6809) done. Loss: 0.0038  lr:0.000001
[ Sat Jul 13 14:33:30 2024 ] 
Training: Epoch [118/120], Step [3499], Loss: 0.19312989711761475, Training Accuracy: 97.66071428571429
[ Sat Jul 13 14:33:30 2024 ] 	Batch(3500/6809) done. Loss: 0.0510  lr:0.000001
[ Sat Jul 13 14:33:49 2024 ] 	Batch(3600/6809) done. Loss: 0.1651  lr:0.000001
[ Sat Jul 13 14:34:07 2024 ] 	Batch(3700/6809) done. Loss: 0.1303  lr:0.000001
[ Sat Jul 13 14:34:25 2024 ] 	Batch(3800/6809) done. Loss: 0.0234  lr:0.000001
[ Sat Jul 13 14:34:43 2024 ] 	Batch(3900/6809) done. Loss: 0.0027  lr:0.000001
[ Sat Jul 13 14:35:00 2024 ] 
Training: Epoch [118/120], Step [3999], Loss: 0.031035199761390686, Training Accuracy: 97.625
[ Sat Jul 13 14:35:01 2024 ] 	Batch(4000/6809) done. Loss: 0.1258  lr:0.000001
[ Sat Jul 13 14:35:18 2024 ] 	Batch(4100/6809) done. Loss: 0.0059  lr:0.000001
[ Sat Jul 13 14:35:36 2024 ] 	Batch(4200/6809) done. Loss: 0.0016  lr:0.000001
[ Sat Jul 13 14:35:54 2024 ] 	Batch(4300/6809) done. Loss: 0.0569  lr:0.000001
[ Sat Jul 13 14:36:12 2024 ] 	Batch(4400/6809) done. Loss: 0.1510  lr:0.000001
[ Sat Jul 13 14:36:30 2024 ] 
Training: Epoch [118/120], Step [4499], Loss: 0.12033499777317047, Training Accuracy: 97.62222222222222
[ Sat Jul 13 14:36:30 2024 ] 	Batch(4500/6809) done. Loss: 0.0177  lr:0.000001
[ Sat Jul 13 14:36:48 2024 ] 	Batch(4600/6809) done. Loss: 0.4037  lr:0.000001
[ Sat Jul 13 14:37:06 2024 ] 	Batch(4700/6809) done. Loss: 0.0997  lr:0.000001
[ Sat Jul 13 14:37:24 2024 ] 	Batch(4800/6809) done. Loss: 0.0762  lr:0.000001
[ Sat Jul 13 14:37:43 2024 ] 	Batch(4900/6809) done. Loss: 0.2431  lr:0.000001
[ Sat Jul 13 14:38:01 2024 ] 
Training: Epoch [118/120], Step [4999], Loss: 0.0030619394965469837, Training Accuracy: 97.6125
[ Sat Jul 13 14:38:01 2024 ] 	Batch(5000/6809) done. Loss: 0.1576  lr:0.000001
[ Sat Jul 13 14:38:20 2024 ] 	Batch(5100/6809) done. Loss: 0.2984  lr:0.000001
[ Sat Jul 13 14:38:38 2024 ] 	Batch(5200/6809) done. Loss: 0.0191  lr:0.000001
[ Sat Jul 13 14:38:57 2024 ] 	Batch(5300/6809) done. Loss: 0.0383  lr:0.000001
[ Sat Jul 13 14:39:16 2024 ] 	Batch(5400/6809) done. Loss: 0.0093  lr:0.000001
[ Sat Jul 13 14:39:34 2024 ] 
Training: Epoch [118/120], Step [5499], Loss: 0.006821146700531244, Training Accuracy: 97.57727272727273
[ Sat Jul 13 14:39:34 2024 ] 	Batch(5500/6809) done. Loss: 0.0962  lr:0.000001
[ Sat Jul 13 14:39:53 2024 ] 	Batch(5600/6809) done. Loss: 0.0880  lr:0.000001
[ Sat Jul 13 14:40:11 2024 ] 	Batch(5700/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 14:40:30 2024 ] 	Batch(5800/6809) done. Loss: 0.0786  lr:0.000001
[ Sat Jul 13 14:40:48 2024 ] 	Batch(5900/6809) done. Loss: 0.0317  lr:0.000001
[ Sat Jul 13 14:41:07 2024 ] 
Training: Epoch [118/120], Step [5999], Loss: 0.01964551955461502, Training Accuracy: 97.55625
[ Sat Jul 13 14:41:07 2024 ] 	Batch(6000/6809) done. Loss: 0.2013  lr:0.000001
[ Sat Jul 13 14:41:26 2024 ] 	Batch(6100/6809) done. Loss: 0.1155  lr:0.000001
[ Sat Jul 13 14:41:44 2024 ] 	Batch(6200/6809) done. Loss: 0.0022  lr:0.000001
[ Sat Jul 13 14:42:03 2024 ] 	Batch(6300/6809) done. Loss: 0.2046  lr:0.000001
[ Sat Jul 13 14:42:21 2024 ] 	Batch(6400/6809) done. Loss: 0.1530  lr:0.000001
[ Sat Jul 13 14:42:40 2024 ] 
Training: Epoch [118/120], Step [6499], Loss: 0.01722482405602932, Training Accuracy: 97.56538461538462
[ Sat Jul 13 14:42:40 2024 ] 	Batch(6500/6809) done. Loss: 0.0492  lr:0.000001
[ Sat Jul 13 14:42:58 2024 ] 	Batch(6600/6809) done. Loss: 0.0528  lr:0.000001
[ Sat Jul 13 14:43:15 2024 ] 	Batch(6700/6809) done. Loss: 0.2381  lr:0.000001
[ Sat Jul 13 14:43:33 2024 ] 	Batch(6800/6809) done. Loss: 0.0522  lr:0.000001
[ Sat Jul 13 14:43:35 2024 ] 	Mean training loss: 0.0954.
[ Sat Jul 13 14:43:35 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 14:43:35 2024 ] Training epoch: 120
[ Sat Jul 13 14:43:36 2024 ] 	Batch(0/6809) done. Loss: 0.0558  lr:0.000001
[ Sat Jul 13 14:43:54 2024 ] 	Batch(100/6809) done. Loss: 0.0346  lr:0.000001
[ Sat Jul 13 14:44:12 2024 ] 	Batch(200/6809) done. Loss: 0.0042  lr:0.000001
[ Sat Jul 13 14:44:31 2024 ] 	Batch(300/6809) done. Loss: 0.0099  lr:0.000001
[ Sat Jul 13 14:44:49 2024 ] 	Batch(400/6809) done. Loss: 0.0078  lr:0.000001
[ Sat Jul 13 14:45:07 2024 ] 
Training: Epoch [119/120], Step [499], Loss: 0.25971195101737976, Training Accuracy: 97.225
[ Sat Jul 13 14:45:07 2024 ] 	Batch(500/6809) done. Loss: 0.0127  lr:0.000001
[ Sat Jul 13 14:45:25 2024 ] 	Batch(600/6809) done. Loss: 0.1474  lr:0.000001
[ Sat Jul 13 14:45:43 2024 ] 	Batch(700/6809) done. Loss: 0.1956  lr:0.000001
[ Sat Jul 13 14:46:01 2024 ] 	Batch(800/6809) done. Loss: 0.0210  lr:0.000001
[ Sat Jul 13 14:46:19 2024 ] 	Batch(900/6809) done. Loss: 0.1144  lr:0.000001
[ Sat Jul 13 14:46:38 2024 ] 
Training: Epoch [119/120], Step [999], Loss: 0.014128828421235085, Training Accuracy: 97.275
[ Sat Jul 13 14:46:38 2024 ] 	Batch(1000/6809) done. Loss: 0.0431  lr:0.000001
[ Sat Jul 13 14:46:56 2024 ] 	Batch(1100/6809) done. Loss: 0.4472  lr:0.000001
[ Sat Jul 13 14:47:15 2024 ] 	Batch(1200/6809) done. Loss: 0.0517  lr:0.000001
[ Sat Jul 13 14:47:34 2024 ] 	Batch(1300/6809) done. Loss: 0.1689  lr:0.000001
[ Sat Jul 13 14:47:52 2024 ] 	Batch(1400/6809) done. Loss: 0.0333  lr:0.000001
[ Sat Jul 13 14:48:11 2024 ] 
Training: Epoch [119/120], Step [1499], Loss: 0.0449899286031723, Training Accuracy: 97.39166666666667
[ Sat Jul 13 14:48:11 2024 ] 	Batch(1500/6809) done. Loss: 0.0352  lr:0.000001
[ Sat Jul 13 14:48:30 2024 ] 	Batch(1600/6809) done. Loss: 0.0114  lr:0.000001
[ Sat Jul 13 14:48:48 2024 ] 	Batch(1700/6809) done. Loss: 0.1031  lr:0.000001
[ Sat Jul 13 14:49:07 2024 ] 	Batch(1800/6809) done. Loss: 0.2647  lr:0.000001
[ Sat Jul 13 14:49:25 2024 ] 	Batch(1900/6809) done. Loss: 0.0097  lr:0.000001
[ Sat Jul 13 14:49:44 2024 ] 
Training: Epoch [119/120], Step [1999], Loss: 0.13295616209506989, Training Accuracy: 97.375
[ Sat Jul 13 14:49:44 2024 ] 	Batch(2000/6809) done. Loss: 0.0203  lr:0.000001
[ Sat Jul 13 14:50:02 2024 ] 	Batch(2100/6809) done. Loss: 0.0292  lr:0.000001
[ Sat Jul 13 14:50:20 2024 ] 	Batch(2200/6809) done. Loss: 0.0026  lr:0.000001
[ Sat Jul 13 14:50:38 2024 ] 	Batch(2300/6809) done. Loss: 0.1888  lr:0.000001
[ Sat Jul 13 14:50:56 2024 ] 	Batch(2400/6809) done. Loss: 0.2000  lr:0.000001
[ Sat Jul 13 14:51:13 2024 ] 
Training: Epoch [119/120], Step [2499], Loss: 0.15919773280620575, Training Accuracy: 97.435
[ Sat Jul 13 14:51:14 2024 ] 	Batch(2500/6809) done. Loss: 0.3519  lr:0.000001
[ Sat Jul 13 14:51:31 2024 ] 	Batch(2600/6809) done. Loss: 0.0129  lr:0.000001
[ Sat Jul 13 14:51:49 2024 ] 	Batch(2700/6809) done. Loss: 0.0529  lr:0.000001
[ Sat Jul 13 14:52:07 2024 ] 	Batch(2800/6809) done. Loss: 0.1417  lr:0.000001
[ Sat Jul 13 14:52:25 2024 ] 	Batch(2900/6809) done. Loss: 0.0612  lr:0.000001
[ Sat Jul 13 14:52:43 2024 ] 
Training: Epoch [119/120], Step [2999], Loss: 0.11249776184558868, Training Accuracy: 97.47083333333333
[ Sat Jul 13 14:52:43 2024 ] 	Batch(3000/6809) done. Loss: 0.0424  lr:0.000001
[ Sat Jul 13 14:53:01 2024 ] 	Batch(3100/6809) done. Loss: 0.0210  lr:0.000001
[ Sat Jul 13 14:53:20 2024 ] 	Batch(3200/6809) done. Loss: 0.2918  lr:0.000001
[ Sat Jul 13 14:53:37 2024 ] 	Batch(3300/6809) done. Loss: 0.1743  lr:0.000001
[ Sat Jul 13 14:53:55 2024 ] 	Batch(3400/6809) done. Loss: 0.2407  lr:0.000001
[ Sat Jul 13 14:54:13 2024 ] 
Training: Epoch [119/120], Step [3499], Loss: 0.0090758902952075, Training Accuracy: 97.43214285714285
[ Sat Jul 13 14:54:13 2024 ] 	Batch(3500/6809) done. Loss: 0.0149  lr:0.000001
[ Sat Jul 13 14:54:31 2024 ] 	Batch(3600/6809) done. Loss: 0.0845  lr:0.000001
[ Sat Jul 13 14:54:50 2024 ] 	Batch(3700/6809) done. Loss: 0.0521  lr:0.000001
[ Sat Jul 13 14:55:08 2024 ] 	Batch(3800/6809) done. Loss: 0.0271  lr:0.000001
[ Sat Jul 13 14:55:27 2024 ] 	Batch(3900/6809) done. Loss: 0.0812  lr:0.000001
[ Sat Jul 13 14:55:45 2024 ] 
Training: Epoch [119/120], Step [3999], Loss: 0.2143014371395111, Training Accuracy: 97.39999999999999
[ Sat Jul 13 14:55:45 2024 ] 	Batch(4000/6809) done. Loss: 0.0514  lr:0.000001
[ Sat Jul 13 14:56:03 2024 ] 	Batch(4100/6809) done. Loss: 0.0324  lr:0.000001
[ Sat Jul 13 14:56:21 2024 ] 	Batch(4200/6809) done. Loss: 0.0934  lr:0.000001
[ Sat Jul 13 14:56:39 2024 ] 	Batch(4300/6809) done. Loss: 0.1105  lr:0.000001
[ Sat Jul 13 14:56:57 2024 ] 	Batch(4400/6809) done. Loss: 0.1482  lr:0.000001
[ Sat Jul 13 14:57:15 2024 ] 
Training: Epoch [119/120], Step [4499], Loss: 0.25017818808555603, Training Accuracy: 97.39722222222223
[ Sat Jul 13 14:57:15 2024 ] 	Batch(4500/6809) done. Loss: 0.1280  lr:0.000001
[ Sat Jul 13 14:57:33 2024 ] 	Batch(4600/6809) done. Loss: 0.0586  lr:0.000001
[ Sat Jul 13 14:57:51 2024 ] 	Batch(4700/6809) done. Loss: 0.0938  lr:0.000001
[ Sat Jul 13 14:58:09 2024 ] 	Batch(4800/6809) done. Loss: 0.0961  lr:0.000001
[ Sat Jul 13 14:58:27 2024 ] 	Batch(4900/6809) done. Loss: 0.0324  lr:0.000001
[ Sat Jul 13 14:58:45 2024 ] 
Training: Epoch [119/120], Step [4999], Loss: 0.009581323713064194, Training Accuracy: 97.44
[ Sat Jul 13 14:58:45 2024 ] 	Batch(5000/6809) done. Loss: 0.1204  lr:0.000001
[ Sat Jul 13 14:59:03 2024 ] 	Batch(5100/6809) done. Loss: 0.2789  lr:0.000001
[ Sat Jul 13 14:59:21 2024 ] 	Batch(5200/6809) done. Loss: 0.0318  lr:0.000001
[ Sat Jul 13 14:59:39 2024 ] 	Batch(5300/6809) done. Loss: 0.3186  lr:0.000001
[ Sat Jul 13 14:59:56 2024 ] 	Batch(5400/6809) done. Loss: 0.0588  lr:0.000001
[ Sat Jul 13 15:00:14 2024 ] 
Training: Epoch [119/120], Step [5499], Loss: 0.1748727560043335, Training Accuracy: 97.49545454545454
[ Sat Jul 13 15:00:14 2024 ] 	Batch(5500/6809) done. Loss: 0.1155  lr:0.000001
[ Sat Jul 13 15:00:33 2024 ] 	Batch(5600/6809) done. Loss: 0.4436  lr:0.000001
[ Sat Jul 13 15:00:52 2024 ] 	Batch(5700/6809) done. Loss: 0.0407  lr:0.000001
[ Sat Jul 13 15:01:10 2024 ] 	Batch(5800/6809) done. Loss: 0.1003  lr:0.000001
[ Sat Jul 13 15:01:29 2024 ] 	Batch(5900/6809) done. Loss: 0.0371  lr:0.000001
[ Sat Jul 13 15:01:47 2024 ] 
Training: Epoch [119/120], Step [5999], Loss: 0.004685364663600922, Training Accuracy: 97.48750000000001
[ Sat Jul 13 15:01:47 2024 ] 	Batch(6000/6809) done. Loss: 0.0965  lr:0.000001
[ Sat Jul 13 15:02:06 2024 ] 	Batch(6100/6809) done. Loss: 0.0750  lr:0.000001
[ Sat Jul 13 15:02:24 2024 ] 	Batch(6200/6809) done. Loss: 0.0617  lr:0.000001
[ Sat Jul 13 15:02:43 2024 ] 	Batch(6300/6809) done. Loss: 0.0654  lr:0.000001
[ Sat Jul 13 15:03:02 2024 ] 	Batch(6400/6809) done. Loss: 0.1603  lr:0.000001
[ Sat Jul 13 15:03:19 2024 ] 
Training: Epoch [119/120], Step [6499], Loss: 0.13223481178283691, Training Accuracy: 97.49423076923077
[ Sat Jul 13 15:03:19 2024 ] 	Batch(6500/6809) done. Loss: 0.0294  lr:0.000001
[ Sat Jul 13 15:03:37 2024 ] 	Batch(6600/6809) done. Loss: 0.0966  lr:0.000001
[ Sat Jul 13 15:03:55 2024 ] 	Batch(6700/6809) done. Loss: 0.2326  lr:0.000001
[ Sat Jul 13 15:04:13 2024 ] 	Batch(6800/6809) done. Loss: 0.1594  lr:0.000001
[ Sat Jul 13 15:04:15 2024 ] 	Mean training loss: 0.0951.
[ Sat Jul 13 15:04:15 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 15:04:15 2024 ] Eval epoch: 120
[ Sat Jul 13 15:09:49 2024 ] 	Mean val loss of 7435 batches: 1.0660926158305946.
[ Sat Jul 13 15:09:49 2024 ] 
Validation: Epoch [119/120], Samples [47877.0/59477], Loss: 0.42068034410476685, Validation Accuracy: 80.49666257544934
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 1 : 373 / 500 = 74 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 2 : 430 / 499 = 86 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 3 : 396 / 500 = 79 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 4 : 416 / 502 = 82 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 5 : 465 / 502 = 92 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 6 : 418 / 502 = 83 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 7 : 464 / 497 = 93 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 8 : 482 / 498 = 96 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 9 : 389 / 500 = 77 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 10 : 206 / 500 = 41 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 11 : 194 / 498 = 38 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 12 : 413 / 499 = 82 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 13 : 484 / 502 = 96 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 14 : 477 / 504 = 94 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 15 : 440 / 502 = 87 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 16 : 368 / 502 = 73 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 17 : 446 / 504 = 88 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 18 : 414 / 504 = 82 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 19 : 469 / 502 = 93 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 20 : 455 / 502 = 90 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 21 : 476 / 503 = 94 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 22 : 435 / 504 = 86 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 23 : 448 / 503 = 89 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 24 : 418 / 504 = 82 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 25 : 493 / 504 = 97 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 26 : 466 / 504 = 92 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 27 : 422 / 501 = 84 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 28 : 365 / 502 = 72 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 29 : 334 / 502 = 66 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 30 : 327 / 501 = 65 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 31 : 424 / 504 = 84 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 32 : 426 / 503 = 84 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 33 : 422 / 503 = 83 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 34 : 482 / 504 = 95 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 35 : 471 / 503 = 93 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 36 : 418 / 502 = 83 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 37 : 440 / 504 = 87 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 38 : 422 / 504 = 83 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 39 : 455 / 498 = 91 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 40 : 390 / 504 = 77 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 41 : 478 / 503 = 95 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 42 : 462 / 504 = 91 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 43 : 328 / 503 = 65 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 44 : 437 / 504 = 86 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 45 : 420 / 504 = 83 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 46 : 420 / 504 = 83 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 47 : 415 / 503 = 82 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 48 : 439 / 503 = 87 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 49 : 380 / 499 = 76 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 50 : 428 / 502 = 85 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 51 : 471 / 503 = 93 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 52 : 452 / 504 = 89 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 53 : 419 / 497 = 84 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 54 : 452 / 480 = 94 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 55 : 380 / 504 = 75 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 56 : 414 / 503 = 82 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 57 : 487 / 504 = 96 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 58 : 481 / 499 = 96 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 59 : 491 / 503 = 97 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 60 : 416 / 479 = 86 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 61 : 418 / 484 = 86 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 62 : 400 / 487 = 82 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 63 : 453 / 489 = 92 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 64 : 378 / 488 = 77 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 65 : 461 / 490 = 94 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 66 : 334 / 488 = 68 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 67 : 370 / 490 = 75 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 68 : 295 / 490 = 60 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 69 : 374 / 490 = 76 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 70 : 201 / 490 = 41 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 71 : 212 / 490 = 43 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 72 : 199 / 488 = 40 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 73 : 285 / 486 = 58 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 74 : 266 / 481 = 55 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 75 : 259 / 488 = 53 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 76 : 316 / 489 = 64 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 77 : 323 / 488 = 66 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 78 : 372 / 488 = 76 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 79 : 452 / 490 = 92 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 80 : 409 / 489 = 83 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 81 : 298 / 491 = 60 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 82 : 329 / 491 = 67 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 83 : 252 / 489 = 51 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 84 : 392 / 489 = 80 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 85 : 377 / 489 = 77 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 86 : 442 / 491 = 90 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 87 : 437 / 492 = 88 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 88 : 376 / 491 = 76 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 89 : 407 / 492 = 82 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 90 : 274 / 490 = 55 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 91 : 402 / 482 = 83 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 92 : 379 / 490 = 77 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 93 : 363 / 487 = 74 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 94 : 413 / 489 = 84 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 95 : 413 / 490 = 84 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 96 : 467 / 491 = 95 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 97 : 464 / 490 = 94 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 98 : 447 / 491 = 91 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 99 : 450 / 491 = 91 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 100 : 453 / 491 = 92 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 101 : 434 / 491 = 88 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 102 : 287 / 492 = 58 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 103 : 398 / 492 = 80 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 104 : 305 / 491 = 62 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 105 : 271 / 491 = 55 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 106 : 278 / 492 = 56 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 107 : 411 / 491 = 83 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 108 : 402 / 492 = 81 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 109 : 326 / 490 = 66 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 110 : 417 / 491 = 84 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 111 : 453 / 492 = 92 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 112 : 453 / 492 = 92 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 113 : 443 / 491 = 90 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 114 : 398 / 491 = 81 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 115 : 429 / 492 = 87 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 116 : 403 / 491 = 82 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 117 : 436 / 492 = 88 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 118 : 440 / 490 = 89 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 119 : 452 / 492 = 91 %
[ Sat Jul 13 15:09:49 2024 ] Accuracy of 120 : 426 / 500 = 85 %
[ Sat Jul 13 15:09:49 2024 ] Load weights from ./prova20/epoch119_model.pt.
[ Sat Jul 13 17:09:45 2024 ] Load weights from prova20/epoch119_model.pt.
[ Sat Jul 13 17:09:45 2024 ] Parameters:
{'val_split': 0.2, 'data_dir': None, 'log_dir': './checkpoints/prova20', 'exp_name': 'prova20', 'num_workers': 10, 'clip_grad_norm': 0.5, 'writer_enabled': True, 'gcn0_flag': False, 'scheduling_lr': True, 'complete': True, 'bn_flag': True, 'accumulating_gradients': True, 'optimize_every': 2, 'clip': False, 'validation_split': False, 'data_mirroring': False, 'local_rank': 0, 'work_dir': './prova20', 'config': 'config/st_gcn/nturgbd/train.yaml', 'phase': 'train', 'save_score': True, 'seed': 13696642, 'training': True, 'log_interval': 100, 'save_interval': 1, 'eval_interval': 10, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'feeder_augmented': 'st_gcn.feeder.FeederAugmented', 'num_worker': 10, 'train_feeder_args': {'data_path': 'new_data_processed/xset/train_joint_120.npy', 'label_path': 'new_data_processed/xset/train_label_120.pkl', 'random_choose': False, 'random_shift': False, 'random_move': False, 'window_size': -1, 'normalization': False, 'mirroring': False}, 'test_feeder_args': {'data_path': 'new_data_processed/xset/val_joint_120.npy', 'label_path': 'new_data_processed/xset/val_label_120.pkl'}, 'train_feeder_args_new': {}, 'test_feeder_args_new': {}, 'model': 'st_gcn.net.ST_GCN', 'model_args': {'num_class': 120, 'channel': 3, 'window_size': 300, 'num_point': 25, 'num_person': 2, 'mask_learning': True, 'use_data_bn': True, 'attention': True, 'only_attention': True, 'tcn_attention': False, 'data_normalization': True, 'skip_conn': True, 'weight_matrix': 2, 'only_temporal_attention': False, 'bn_flag': True, 'attention_3': False, 'kernel_temporal': 9, 'more_channels': False, 'double_channel': False, 'drop_connect': True, 'concat_original': True, 'all_layers': False, 'adjacency': False, 'agcn': False, 'dv': 0.25, 'dk': 0.25, 'Nh': 8, 'n': 4, 'dim_block1': 10, 'dim_block2': 30, 'dim_block3': 75, 'relative': False, 'graph': 'st_gcn.graph.NTU_RGB_D', 'visualization': False, 'graph_args': {'labeling_mode': 'spatial'}}, 'weights': 'prova20/epoch119_model.pt', 'ignore_weights': [], 'cl_mode': 'ST-Multi-Level', 'cl_version': 'V0', 'w_multi_cl_loss': [0.1, 0.2, 0.5, 1], 'w_cl_loss': 0.1, 'complete_cl_loss': False, 'spatial_only_loss': True, 'scheduler': 1, 'base_lr': 1e-06, 'step': [60, 90], 'device': [0, 1], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 8, 'test_batch_size': 8, 'start_epoch': 0, 'start_cl_epoch': -1, 'num_epoch': 50, 'weight_decay': 0.0001, 'display_by_category': True, 'display_recall_precision': True}

[ Sat Jul 13 17:09:45 2024 ] Training epoch: 1
[ Sat Jul 13 17:09:47 2024 ] 	Batch(0/6809) done. Loss: 0.0662  lr:0.000001
[ Sat Jul 13 17:10:04 2024 ] 	Batch(100/6809) done. Loss: 0.0136  lr:0.000001
[ Sat Jul 13 17:10:22 2024 ] 	Batch(200/6809) done. Loss: 0.0130  lr:0.000001
[ Sat Jul 13 17:10:39 2024 ] 	Batch(300/6809) done. Loss: 0.0454  lr:0.000001
[ Sat Jul 13 17:10:57 2024 ] 	Batch(400/6809) done. Loss: 0.0350  lr:0.000001
[ Sat Jul 13 17:11:14 2024 ] 
Training: Epoch [0/50], Step [499], Loss: 0.012278998270630836, Training Accuracy: 97.82499999999999
[ Sat Jul 13 17:11:14 2024 ] 	Batch(500/6809) done. Loss: 0.1365  lr:0.000001
[ Sat Jul 13 17:11:31 2024 ] 	Batch(600/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 17:11:49 2024 ] 	Batch(700/6809) done. Loss: 0.3341  lr:0.000001
[ Sat Jul 13 17:12:06 2024 ] 	Batch(800/6809) done. Loss: 0.1725  lr:0.000001
[ Sat Jul 13 17:12:24 2024 ] 	Batch(900/6809) done. Loss: 0.0357  lr:0.000001
[ Sat Jul 13 17:12:41 2024 ] 
Training: Epoch [0/50], Step [999], Loss: 0.0071817804127931595, Training Accuracy: 97.91250000000001
[ Sat Jul 13 17:12:41 2024 ] 	Batch(1000/6809) done. Loss: 0.2461  lr:0.000001
[ Sat Jul 13 17:12:58 2024 ] 	Batch(1100/6809) done. Loss: 0.0981  lr:0.000001
[ Sat Jul 13 17:13:16 2024 ] 	Batch(1200/6809) done. Loss: 0.0038  lr:0.000001
[ Sat Jul 13 17:13:33 2024 ] 	Batch(1300/6809) done. Loss: 0.2169  lr:0.000001
[ Sat Jul 13 17:13:50 2024 ] 	Batch(1400/6809) done. Loss: 0.0814  lr:0.000001
[ Sat Jul 13 17:14:07 2024 ] 
Training: Epoch [0/50], Step [1499], Loss: 0.006137317977845669, Training Accuracy: 97.79166666666667
[ Sat Jul 13 17:14:07 2024 ] 	Batch(1500/6809) done. Loss: 0.0808  lr:0.000001
[ Sat Jul 13 17:14:25 2024 ] 	Batch(1600/6809) done. Loss: 0.3167  lr:0.000001
[ Sat Jul 13 17:14:43 2024 ] 	Batch(1700/6809) done. Loss: 0.0705  lr:0.000001
[ Sat Jul 13 17:15:00 2024 ] 	Batch(1800/6809) done. Loss: 0.1478  lr:0.000001
[ Sat Jul 13 17:15:17 2024 ] 	Batch(1900/6809) done. Loss: 0.1044  lr:0.000001
[ Sat Jul 13 17:15:34 2024 ] 
Training: Epoch [0/50], Step [1999], Loss: 0.028706109151244164, Training Accuracy: 97.65625
[ Sat Jul 13 17:15:35 2024 ] 	Batch(2000/6809) done. Loss: 0.0620  lr:0.000001
[ Sat Jul 13 17:15:52 2024 ] 	Batch(2100/6809) done. Loss: 0.0776  lr:0.000001
[ Sat Jul 13 17:16:09 2024 ] 	Batch(2200/6809) done. Loss: 0.0925  lr:0.000001
[ Sat Jul 13 17:16:26 2024 ] 	Batch(2300/6809) done. Loss: 0.0074  lr:0.000001
[ Sat Jul 13 17:16:44 2024 ] 	Batch(2400/6809) done. Loss: 0.3838  lr:0.000001
[ Sat Jul 13 17:17:01 2024 ] 
Training: Epoch [0/50], Step [2499], Loss: 0.09892570972442627, Training Accuracy: 97.615
[ Sat Jul 13 17:17:01 2024 ] 	Batch(2500/6809) done. Loss: 0.0013  lr:0.000001
[ Sat Jul 13 17:17:18 2024 ] 	Batch(2600/6809) done. Loss: 0.0992  lr:0.000001
[ Sat Jul 13 17:17:35 2024 ] 	Batch(2700/6809) done. Loss: 0.0558  lr:0.000001
[ Sat Jul 13 17:17:53 2024 ] 	Batch(2800/6809) done. Loss: 0.2080  lr:0.000001
[ Sat Jul 13 17:18:10 2024 ] 	Batch(2900/6809) done. Loss: 0.0643  lr:0.000001
[ Sat Jul 13 17:18:27 2024 ] 
Training: Epoch [0/50], Step [2999], Loss: 0.2832755446434021, Training Accuracy: 97.59583333333333
[ Sat Jul 13 17:18:27 2024 ] 	Batch(3000/6809) done. Loss: 0.0229  lr:0.000001
[ Sat Jul 13 17:18:44 2024 ] 	Batch(3100/6809) done. Loss: 0.0330  lr:0.000001
[ Sat Jul 13 17:19:02 2024 ] 	Batch(3200/6809) done. Loss: 0.0845  lr:0.000001
[ Sat Jul 13 17:19:19 2024 ] 	Batch(3300/6809) done. Loss: 0.0794  lr:0.000001
[ Sat Jul 13 17:19:36 2024 ] 	Batch(3400/6809) done. Loss: 0.1092  lr:0.000001
[ Sat Jul 13 17:19:53 2024 ] 
Training: Epoch [0/50], Step [3499], Loss: 0.01974605768918991, Training Accuracy: 97.63214285714285
[ Sat Jul 13 17:19:53 2024 ] 	Batch(3500/6809) done. Loss: 0.0079  lr:0.000001
[ Sat Jul 13 17:20:11 2024 ] 	Batch(3600/6809) done. Loss: 0.0253  lr:0.000001
[ Sat Jul 13 17:20:28 2024 ] 	Batch(3700/6809) done. Loss: 0.1554  lr:0.000001
[ Sat Jul 13 17:20:45 2024 ] 	Batch(3800/6809) done. Loss: 0.0620  lr:0.000001
[ Sat Jul 13 17:21:02 2024 ] 	Batch(3900/6809) done. Loss: 0.0285  lr:0.000001
[ Sat Jul 13 17:21:19 2024 ] 
Training: Epoch [0/50], Step [3999], Loss: 0.10809744894504547, Training Accuracy: 97.603125
[ Sat Jul 13 17:21:20 2024 ] 	Batch(4000/6809) done. Loss: 0.0102  lr:0.000001
[ Sat Jul 13 17:21:37 2024 ] 	Batch(4100/6809) done. Loss: 0.0459  lr:0.000001
[ Sat Jul 13 17:21:54 2024 ] 	Batch(4200/6809) done. Loss: 0.0576  lr:0.000001
[ Sat Jul 13 17:22:11 2024 ] 	Batch(4300/6809) done. Loss: 0.0446  lr:0.000001
[ Sat Jul 13 17:22:28 2024 ] 	Batch(4400/6809) done. Loss: 0.0110  lr:0.000001
[ Sat Jul 13 17:22:45 2024 ] 
Training: Epoch [0/50], Step [4499], Loss: 0.4001745283603668, Training Accuracy: 97.6361111111111
[ Sat Jul 13 17:22:46 2024 ] 	Batch(4500/6809) done. Loss: 0.0087  lr:0.000001
[ Sat Jul 13 17:23:03 2024 ] 	Batch(4600/6809) done. Loss: 0.0038  lr:0.000001
[ Sat Jul 13 17:23:20 2024 ] 	Batch(4700/6809) done. Loss: 0.0158  lr:0.000001
[ Sat Jul 13 17:23:37 2024 ] 	Batch(4800/6809) done. Loss: 0.2201  lr:0.000001
[ Sat Jul 13 17:23:54 2024 ] 	Batch(4900/6809) done. Loss: 0.0066  lr:0.000001
[ Sat Jul 13 17:24:11 2024 ] 
Training: Epoch [0/50], Step [4999], Loss: 0.02266131341457367, Training Accuracy: 97.64500000000001
[ Sat Jul 13 17:24:12 2024 ] 	Batch(5000/6809) done. Loss: 0.0229  lr:0.000001
[ Sat Jul 13 17:24:29 2024 ] 	Batch(5100/6809) done. Loss: 0.0307  lr:0.000001
[ Sat Jul 13 17:24:46 2024 ] 	Batch(5200/6809) done. Loss: 0.1319  lr:0.000001
[ Sat Jul 13 17:25:04 2024 ] 	Batch(5300/6809) done. Loss: 0.1696  lr:0.000001
[ Sat Jul 13 17:25:22 2024 ] 	Batch(5400/6809) done. Loss: 0.0128  lr:0.000001
[ Sat Jul 13 17:25:39 2024 ] 
Training: Epoch [0/50], Step [5499], Loss: 0.2068198174238205, Training Accuracy: 97.62272727272727
[ Sat Jul 13 17:25:40 2024 ] 	Batch(5500/6809) done. Loss: 0.2353  lr:0.000001
[ Sat Jul 13 17:25:58 2024 ] 	Batch(5600/6809) done. Loss: 0.1080  lr:0.000001
[ Sat Jul 13 17:26:15 2024 ] 	Batch(5700/6809) done. Loss: 0.0381  lr:0.000001
[ Sat Jul 13 17:26:32 2024 ] 	Batch(5800/6809) done. Loss: 0.1875  lr:0.000001
[ Sat Jul 13 17:26:49 2024 ] 	Batch(5900/6809) done. Loss: 0.1010  lr:0.000001
[ Sat Jul 13 17:27:06 2024 ] 
Training: Epoch [0/50], Step [5999], Loss: 0.022504236549139023, Training Accuracy: 97.63333333333334
[ Sat Jul 13 17:27:06 2024 ] 	Batch(6000/6809) done. Loss: 0.0010  lr:0.000001
[ Sat Jul 13 17:27:24 2024 ] 	Batch(6100/6809) done. Loss: 0.0608  lr:0.000001
[ Sat Jul 13 17:27:41 2024 ] 	Batch(6200/6809) done. Loss: 0.0231  lr:0.000001
[ Sat Jul 13 17:27:58 2024 ] 	Batch(6300/6809) done. Loss: 0.0055  lr:0.000001
[ Sat Jul 13 17:28:15 2024 ] 	Batch(6400/6809) done. Loss: 0.0693  lr:0.000001
[ Sat Jul 13 17:28:33 2024 ] 
Training: Epoch [0/50], Step [6499], Loss: 0.01409488171339035, Training Accuracy: 97.62307692307692
[ Sat Jul 13 17:28:33 2024 ] 	Batch(6500/6809) done. Loss: 0.0012  lr:0.000001
[ Sat Jul 13 17:28:50 2024 ] 	Batch(6600/6809) done. Loss: 0.1526  lr:0.000001
[ Sat Jul 13 17:29:08 2024 ] 	Batch(6700/6809) done. Loss: 0.0539  lr:0.000001
[ Sat Jul 13 17:29:25 2024 ] 	Batch(6800/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 17:29:27 2024 ] 	Mean training loss: 0.0940.
[ Sat Jul 13 17:29:27 2024 ] 	Time consumption: [Data]01%, [Network]91%
[ Sat Jul 13 17:29:27 2024 ] Training epoch: 2
[ Sat Jul 13 17:29:27 2024 ] 	Batch(0/6809) done. Loss: 0.1695  lr:0.000001
[ Sat Jul 13 17:29:45 2024 ] 	Batch(100/6809) done. Loss: 0.0936  lr:0.000001
[ Sat Jul 13 17:30:03 2024 ] 	Batch(200/6809) done. Loss: 0.0825  lr:0.000001
[ Sat Jul 13 17:30:21 2024 ] 	Batch(300/6809) done. Loss: 0.0081  lr:0.000001
[ Sat Jul 13 17:30:39 2024 ] 	Batch(400/6809) done. Loss: 0.4204  lr:0.000001
[ Sat Jul 13 17:30:57 2024 ] 
Training: Epoch [1/50], Step [499], Loss: 0.14038603007793427, Training Accuracy: 97.45
[ Sat Jul 13 17:30:57 2024 ] 	Batch(500/6809) done. Loss: 0.0337  lr:0.000001
[ Sat Jul 13 17:31:15 2024 ] 	Batch(600/6809) done. Loss: 0.4671  lr:0.000001
[ Sat Jul 13 17:31:34 2024 ] 	Batch(700/6809) done. Loss: 0.0168  lr:0.000001
[ Sat Jul 13 17:31:52 2024 ] 	Batch(800/6809) done. Loss: 0.0623  lr:0.000001
[ Sat Jul 13 17:32:10 2024 ] 	Batch(900/6809) done. Loss: 0.0252  lr:0.000001
[ Sat Jul 13 17:32:28 2024 ] 
Training: Epoch [1/50], Step [999], Loss: 0.1367717683315277, Training Accuracy: 97.28750000000001
[ Sat Jul 13 17:32:28 2024 ] 	Batch(1000/6809) done. Loss: 0.2116  lr:0.000001
[ Sat Jul 13 17:32:46 2024 ] 	Batch(1100/6809) done. Loss: 0.1167  lr:0.000001
[ Sat Jul 13 17:33:04 2024 ] 	Batch(1200/6809) done. Loss: 0.3672  lr:0.000001
[ Sat Jul 13 17:33:22 2024 ] 	Batch(1300/6809) done. Loss: 0.0822  lr:0.000001
[ Sat Jul 13 17:33:40 2024 ] 	Batch(1400/6809) done. Loss: 0.0016  lr:0.000001
[ Sat Jul 13 17:33:57 2024 ] 
Training: Epoch [1/50], Step [1499], Loss: 0.07944752275943756, Training Accuracy: 97.375
[ Sat Jul 13 17:33:57 2024 ] 	Batch(1500/6809) done. Loss: 0.2086  lr:0.000001
[ Sat Jul 13 17:34:15 2024 ] 	Batch(1600/6809) done. Loss: 0.0851  lr:0.000001
[ Sat Jul 13 17:34:34 2024 ] 	Batch(1700/6809) done. Loss: 0.0771  lr:0.000001
[ Sat Jul 13 17:34:53 2024 ] 	Batch(1800/6809) done. Loss: 0.0069  lr:0.000001
[ Sat Jul 13 17:35:11 2024 ] 	Batch(1900/6809) done. Loss: 0.1123  lr:0.000001
[ Sat Jul 13 17:35:29 2024 ] 
Training: Epoch [1/50], Step [1999], Loss: 0.02750697359442711, Training Accuracy: 97.54374999999999
[ Sat Jul 13 17:35:30 2024 ] 	Batch(2000/6809) done. Loss: 0.4362  lr:0.000001
[ Sat Jul 13 17:35:48 2024 ] 	Batch(2100/6809) done. Loss: 0.0680  lr:0.000001
[ Sat Jul 13 17:36:07 2024 ] 	Batch(2200/6809) done. Loss: 0.2351  lr:0.000001
[ Sat Jul 13 17:36:25 2024 ] 	Batch(2300/6809) done. Loss: 0.0687  lr:0.000001
[ Sat Jul 13 17:36:44 2024 ] 	Batch(2400/6809) done. Loss: 0.0255  lr:0.000001
[ Sat Jul 13 17:37:02 2024 ] 
Training: Epoch [1/50], Step [2499], Loss: 0.019871607422828674, Training Accuracy: 97.53500000000001
[ Sat Jul 13 17:37:02 2024 ] 	Batch(2500/6809) done. Loss: 0.1315  lr:0.000001
[ Sat Jul 13 17:37:20 2024 ] 	Batch(2600/6809) done. Loss: 0.1045  lr:0.000001
[ Sat Jul 13 17:37:38 2024 ] 	Batch(2700/6809) done. Loss: 0.0032  lr:0.000001
[ Sat Jul 13 17:37:56 2024 ] 	Batch(2800/6809) done. Loss: 0.0169  lr:0.000001
[ Sat Jul 13 17:38:14 2024 ] 	Batch(2900/6809) done. Loss: 0.1727  lr:0.000001
[ Sat Jul 13 17:38:32 2024 ] 
Training: Epoch [1/50], Step [2999], Loss: 0.14874114096164703, Training Accuracy: 97.51666666666667
[ Sat Jul 13 17:38:32 2024 ] 	Batch(3000/6809) done. Loss: 0.0199  lr:0.000001
[ Sat Jul 13 17:38:50 2024 ] 	Batch(3100/6809) done. Loss: 0.0118  lr:0.000001
[ Sat Jul 13 17:39:08 2024 ] 	Batch(3200/6809) done. Loss: 0.0116  lr:0.000001
[ Sat Jul 13 17:39:26 2024 ] 	Batch(3300/6809) done. Loss: 0.7458  lr:0.000001
[ Sat Jul 13 17:39:43 2024 ] 	Batch(3400/6809) done. Loss: 0.0471  lr:0.000001
[ Sat Jul 13 17:40:01 2024 ] 
Training: Epoch [1/50], Step [3499], Loss: 0.2753850519657135, Training Accuracy: 97.57142857142857
[ Sat Jul 13 17:40:01 2024 ] 	Batch(3500/6809) done. Loss: 0.0282  lr:0.000001
[ Sat Jul 13 17:40:19 2024 ] 	Batch(3600/6809) done. Loss: 0.0780  lr:0.000001
[ Sat Jul 13 17:40:37 2024 ] 	Batch(3700/6809) done. Loss: 0.0554  lr:0.000001
[ Sat Jul 13 17:40:55 2024 ] 	Batch(3800/6809) done. Loss: 0.0710  lr:0.000001
[ Sat Jul 13 17:41:13 2024 ] 	Batch(3900/6809) done. Loss: 0.9352  lr:0.000001
[ Sat Jul 13 17:41:30 2024 ] 
Training: Epoch [1/50], Step [3999], Loss: 0.03978944197297096, Training Accuracy: 97.61875
[ Sat Jul 13 17:41:31 2024 ] 	Batch(4000/6809) done. Loss: 0.0311  lr:0.000001
[ Sat Jul 13 17:41:49 2024 ] 	Batch(4100/6809) done. Loss: 0.0090  lr:0.000001
[ Sat Jul 13 17:42:07 2024 ] 	Batch(4200/6809) done. Loss: 0.0801  lr:0.000001
[ Sat Jul 13 17:42:25 2024 ] 	Batch(4300/6809) done. Loss: 0.0235  lr:0.000001
[ Sat Jul 13 17:42:43 2024 ] 	Batch(4400/6809) done. Loss: 0.0140  lr:0.000001
[ Sat Jul 13 17:43:00 2024 ] 
Training: Epoch [1/50], Step [4499], Loss: 0.04606730490922928, Training Accuracy: 97.54166666666667
[ Sat Jul 13 17:43:00 2024 ] 	Batch(4500/6809) done. Loss: 0.0894  lr:0.000001
[ Sat Jul 13 17:43:18 2024 ] 	Batch(4600/6809) done. Loss: 0.2680  lr:0.000001
[ Sat Jul 13 17:43:36 2024 ] 	Batch(4700/6809) done. Loss: 0.0546  lr:0.000001
[ Sat Jul 13 17:43:54 2024 ] 	Batch(4800/6809) done. Loss: 0.0487  lr:0.000001
[ Sat Jul 13 17:44:12 2024 ] 	Batch(4900/6809) done. Loss: 0.2067  lr:0.000001
[ Sat Jul 13 17:44:30 2024 ] 
Training: Epoch [1/50], Step [4999], Loss: 0.040725525468587875, Training Accuracy: 97.5475
[ Sat Jul 13 17:44:30 2024 ] 	Batch(5000/6809) done. Loss: 0.0276  lr:0.000001
[ Sat Jul 13 17:44:48 2024 ] 	Batch(5100/6809) done. Loss: 0.1085  lr:0.000001
[ Sat Jul 13 17:45:06 2024 ] 	Batch(5200/6809) done. Loss: 0.3224  lr:0.000001
[ Sat Jul 13 17:45:23 2024 ] 	Batch(5300/6809) done. Loss: 0.1425  lr:0.000001
[ Sat Jul 13 17:45:42 2024 ] 	Batch(5400/6809) done. Loss: 0.0634  lr:0.000001
[ Sat Jul 13 17:46:00 2024 ] 
Training: Epoch [1/50], Step [5499], Loss: 0.0075616659596562386, Training Accuracy: 97.51590909090909
[ Sat Jul 13 17:46:00 2024 ] 	Batch(5500/6809) done. Loss: 0.1163  lr:0.000001
[ Sat Jul 13 17:46:18 2024 ] 	Batch(5600/6809) done. Loss: 0.0173  lr:0.000001
[ Sat Jul 13 17:46:36 2024 ] 	Batch(5700/6809) done. Loss: 0.0042  lr:0.000001
[ Sat Jul 13 17:46:55 2024 ] 	Batch(5800/6809) done. Loss: 0.0090  lr:0.000001
[ Sat Jul 13 17:47:13 2024 ] 	Batch(5900/6809) done. Loss: 0.1733  lr:0.000001
[ Sat Jul 13 17:47:30 2024 ] 
Training: Epoch [1/50], Step [5999], Loss: 0.60962975025177, Training Accuracy: 97.53125
[ Sat Jul 13 17:47:31 2024 ] 	Batch(6000/6809) done. Loss: 0.1758  lr:0.000001
[ Sat Jul 13 17:47:49 2024 ] 	Batch(6100/6809) done. Loss: 0.0590  lr:0.000001
[ Sat Jul 13 17:48:06 2024 ] 	Batch(6200/6809) done. Loss: 0.0041  lr:0.000001
[ Sat Jul 13 17:48:24 2024 ] 	Batch(6300/6809) done. Loss: 0.2217  lr:0.000001
[ Sat Jul 13 17:48:42 2024 ] 	Batch(6400/6809) done. Loss: 0.0302  lr:0.000001
[ Sat Jul 13 17:49:00 2024 ] 
Training: Epoch [1/50], Step [6499], Loss: 0.06463367491960526, Training Accuracy: 97.59230769230768
[ Sat Jul 13 17:49:00 2024 ] 	Batch(6500/6809) done. Loss: 0.0814  lr:0.000001
[ Sat Jul 13 17:49:18 2024 ] 	Batch(6600/6809) done. Loss: 0.0313  lr:0.000001
[ Sat Jul 13 17:49:36 2024 ] 	Batch(6700/6809) done. Loss: 0.0578  lr:0.000001
[ Sat Jul 13 17:49:54 2024 ] 	Batch(6800/6809) done. Loss: 0.0237  lr:0.000001
[ Sat Jul 13 17:49:55 2024 ] 	Mean training loss: 0.0952.
[ Sat Jul 13 17:49:55 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 17:49:55 2024 ] Training epoch: 3
[ Sat Jul 13 17:49:56 2024 ] 	Batch(0/6809) done. Loss: 0.0716  lr:0.000001
[ Sat Jul 13 17:50:14 2024 ] 	Batch(100/6809) done. Loss: 0.0190  lr:0.000001
[ Sat Jul 13 17:50:32 2024 ] 	Batch(200/6809) done. Loss: 0.1821  lr:0.000001
[ Sat Jul 13 17:50:50 2024 ] 	Batch(300/6809) done. Loss: 0.1628  lr:0.000001
[ Sat Jul 13 17:51:08 2024 ] 	Batch(400/6809) done. Loss: 0.2054  lr:0.000001
[ Sat Jul 13 17:51:25 2024 ] 
Training: Epoch [2/50], Step [499], Loss: 0.06102406978607178, Training Accuracy: 97.575
[ Sat Jul 13 17:51:25 2024 ] 	Batch(500/6809) done. Loss: 0.0299  lr:0.000001
[ Sat Jul 13 17:51:43 2024 ] 	Batch(600/6809) done. Loss: 0.3212  lr:0.000001
[ Sat Jul 13 17:52:01 2024 ] 	Batch(700/6809) done. Loss: 0.0360  lr:0.000001
[ Sat Jul 13 17:52:19 2024 ] 	Batch(800/6809) done. Loss: 0.2673  lr:0.000001
[ Sat Jul 13 17:52:38 2024 ] 	Batch(900/6809) done. Loss: 0.3016  lr:0.000001
[ Sat Jul 13 17:52:55 2024 ] 
Training: Epoch [2/50], Step [999], Loss: 0.0030173659324645996, Training Accuracy: 97.28750000000001
[ Sat Jul 13 17:52:56 2024 ] 	Batch(1000/6809) done. Loss: 0.0198  lr:0.000001
[ Sat Jul 13 17:53:13 2024 ] 	Batch(1100/6809) done. Loss: 0.3779  lr:0.000001
[ Sat Jul 13 17:53:31 2024 ] 	Batch(1200/6809) done. Loss: 0.1043  lr:0.000001
[ Sat Jul 13 17:53:50 2024 ] 	Batch(1300/6809) done. Loss: 0.2184  lr:0.000001
[ Sat Jul 13 17:54:09 2024 ] 	Batch(1400/6809) done. Loss: 0.0009  lr:0.000001
[ Sat Jul 13 17:54:27 2024 ] 
Training: Epoch [2/50], Step [1499], Loss: 0.01539091020822525, Training Accuracy: 97.39999999999999
[ Sat Jul 13 17:54:27 2024 ] 	Batch(1500/6809) done. Loss: 0.3590  lr:0.000001
[ Sat Jul 13 17:54:46 2024 ] 	Batch(1600/6809) done. Loss: 0.0256  lr:0.000001
[ Sat Jul 13 17:55:04 2024 ] 	Batch(1700/6809) done. Loss: 0.0584  lr:0.000001
[ Sat Jul 13 17:55:23 2024 ] 	Batch(1800/6809) done. Loss: 0.0063  lr:0.000001
[ Sat Jul 13 17:55:41 2024 ] 	Batch(1900/6809) done. Loss: 0.1168  lr:0.000001
[ Sat Jul 13 17:56:00 2024 ] 
Training: Epoch [2/50], Step [1999], Loss: 0.017268069088459015, Training Accuracy: 97.425
[ Sat Jul 13 17:56:00 2024 ] 	Batch(2000/6809) done. Loss: 0.0246  lr:0.000001
[ Sat Jul 13 17:56:18 2024 ] 	Batch(2100/6809) done. Loss: 0.1318  lr:0.000001
[ Sat Jul 13 17:56:36 2024 ] 	Batch(2200/6809) done. Loss: 0.0289  lr:0.000001
[ Sat Jul 13 17:56:54 2024 ] 	Batch(2300/6809) done. Loss: 0.1037  lr:0.000001
[ Sat Jul 13 17:57:12 2024 ] 	Batch(2400/6809) done. Loss: 0.1562  lr:0.000001
[ Sat Jul 13 17:57:30 2024 ] 
Training: Epoch [2/50], Step [2499], Loss: 0.15702685713768005, Training Accuracy: 97.36
[ Sat Jul 13 17:57:30 2024 ] 	Batch(2500/6809) done. Loss: 0.0135  lr:0.000001
[ Sat Jul 13 17:57:48 2024 ] 	Batch(2600/6809) done. Loss: 0.0393  lr:0.000001
[ Sat Jul 13 17:58:06 2024 ] 	Batch(2700/6809) done. Loss: 0.0167  lr:0.000001
[ Sat Jul 13 17:58:24 2024 ] 	Batch(2800/6809) done. Loss: 0.0819  lr:0.000001
[ Sat Jul 13 17:58:42 2024 ] 	Batch(2900/6809) done. Loss: 0.0025  lr:0.000001
[ Sat Jul 13 17:59:00 2024 ] 
Training: Epoch [2/50], Step [2999], Loss: 0.01848626136779785, Training Accuracy: 97.425
[ Sat Jul 13 17:59:00 2024 ] 	Batch(3000/6809) done. Loss: 0.4147  lr:0.000001
[ Sat Jul 13 17:59:18 2024 ] 	Batch(3100/6809) done. Loss: 0.0091  lr:0.000001
[ Sat Jul 13 17:59:36 2024 ] 	Batch(3200/6809) done. Loss: 0.0198  lr:0.000001
[ Sat Jul 13 17:59:55 2024 ] 	Batch(3300/6809) done. Loss: 0.0869  lr:0.000001
[ Sat Jul 13 18:00:13 2024 ] 	Batch(3400/6809) done. Loss: 0.0292  lr:0.000001
[ Sat Jul 13 18:00:32 2024 ] 
Training: Epoch [2/50], Step [3499], Loss: 0.38933080434799194, Training Accuracy: 97.45714285714286
[ Sat Jul 13 18:00:32 2024 ] 	Batch(3500/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 18:00:50 2024 ] 	Batch(3600/6809) done. Loss: 0.1957  lr:0.000001
[ Sat Jul 13 18:01:08 2024 ] 	Batch(3700/6809) done. Loss: 0.0882  lr:0.000001
[ Sat Jul 13 18:01:26 2024 ] 	Batch(3800/6809) done. Loss: 0.1696  lr:0.000001
[ Sat Jul 13 18:01:44 2024 ] 	Batch(3900/6809) done. Loss: 0.1578  lr:0.000001
[ Sat Jul 13 18:02:02 2024 ] 
Training: Epoch [2/50], Step [3999], Loss: 0.05199570953845978, Training Accuracy: 97.421875
[ Sat Jul 13 18:02:02 2024 ] 	Batch(4000/6809) done. Loss: 0.0537  lr:0.000001
[ Sat Jul 13 18:02:20 2024 ] 	Batch(4100/6809) done. Loss: 0.1089  lr:0.000001
[ Sat Jul 13 18:02:38 2024 ] 	Batch(4200/6809) done. Loss: 0.0081  lr:0.000001
[ Sat Jul 13 18:02:56 2024 ] 	Batch(4300/6809) done. Loss: 0.0510  lr:0.000001
[ Sat Jul 13 18:03:14 2024 ] 	Batch(4400/6809) done. Loss: 0.1011  lr:0.000001
[ Sat Jul 13 18:03:32 2024 ] 
Training: Epoch [2/50], Step [4499], Loss: 0.016111334785819054, Training Accuracy: 97.45277777777778
[ Sat Jul 13 18:03:32 2024 ] 	Batch(4500/6809) done. Loss: 0.0808  lr:0.000001
[ Sat Jul 13 18:03:50 2024 ] 	Batch(4600/6809) done. Loss: 0.0322  lr:0.000001
[ Sat Jul 13 18:04:08 2024 ] 	Batch(4700/6809) done. Loss: 0.2600  lr:0.000001
[ Sat Jul 13 18:04:26 2024 ] 	Batch(4800/6809) done. Loss: 0.0365  lr:0.000001
[ Sat Jul 13 18:04:44 2024 ] 	Batch(4900/6809) done. Loss: 0.3974  lr:0.000001
[ Sat Jul 13 18:05:01 2024 ] 
Training: Epoch [2/50], Step [4999], Loss: 0.02344459667801857, Training Accuracy: 97.495
[ Sat Jul 13 18:05:02 2024 ] 	Batch(5000/6809) done. Loss: 0.0173  lr:0.000001
[ Sat Jul 13 18:05:20 2024 ] 	Batch(5100/6809) done. Loss: 0.0121  lr:0.000001
[ Sat Jul 13 18:05:38 2024 ] 	Batch(5200/6809) done. Loss: 0.0499  lr:0.000001
[ Sat Jul 13 18:05:56 2024 ] 	Batch(5300/6809) done. Loss: 0.0187  lr:0.000001
[ Sat Jul 13 18:06:13 2024 ] 	Batch(5400/6809) done. Loss: 0.0293  lr:0.000001
[ Sat Jul 13 18:06:31 2024 ] 
Training: Epoch [2/50], Step [5499], Loss: 0.015107657760381699, Training Accuracy: 97.49318181818182
[ Sat Jul 13 18:06:31 2024 ] 	Batch(5500/6809) done. Loss: 0.0496  lr:0.000001
[ Sat Jul 13 18:06:49 2024 ] 	Batch(5600/6809) done. Loss: 0.0100  lr:0.000001
[ Sat Jul 13 18:07:08 2024 ] 	Batch(5700/6809) done. Loss: 0.0964  lr:0.000001
[ Sat Jul 13 18:07:25 2024 ] 	Batch(5800/6809) done. Loss: 0.0469  lr:0.000001
[ Sat Jul 13 18:07:43 2024 ] 	Batch(5900/6809) done. Loss: 0.1507  lr:0.000001
[ Sat Jul 13 18:08:01 2024 ] 
Training: Epoch [2/50], Step [5999], Loss: 0.23396800458431244, Training Accuracy: 97.52083333333333
[ Sat Jul 13 18:08:01 2024 ] 	Batch(6000/6809) done. Loss: 0.1860  lr:0.000001
[ Sat Jul 13 18:08:20 2024 ] 	Batch(6100/6809) done. Loss: 0.0314  lr:0.000001
[ Sat Jul 13 18:08:38 2024 ] 	Batch(6200/6809) done. Loss: 0.0307  lr:0.000001
[ Sat Jul 13 18:08:56 2024 ] 	Batch(6300/6809) done. Loss: 0.3363  lr:0.000001
[ Sat Jul 13 18:09:14 2024 ] 	Batch(6400/6809) done. Loss: 0.0905  lr:0.000001
[ Sat Jul 13 18:09:32 2024 ] 
Training: Epoch [2/50], Step [6499], Loss: 0.014299269765615463, Training Accuracy: 97.53461538461539
[ Sat Jul 13 18:09:32 2024 ] 	Batch(6500/6809) done. Loss: 0.0690  lr:0.000001
[ Sat Jul 13 18:09:50 2024 ] 	Batch(6600/6809) done. Loss: 0.0677  lr:0.000001
[ Sat Jul 13 18:10:08 2024 ] 	Batch(6700/6809) done. Loss: 0.0187  lr:0.000001
[ Sat Jul 13 18:10:26 2024 ] 	Batch(6800/6809) done. Loss: 0.2214  lr:0.000001
[ Sat Jul 13 18:10:28 2024 ] 	Mean training loss: 0.0945.
[ Sat Jul 13 18:10:28 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 18:10:28 2024 ] Training epoch: 4
[ Sat Jul 13 18:10:28 2024 ] 	Batch(0/6809) done. Loss: 0.0426  lr:0.000001
[ Sat Jul 13 18:10:46 2024 ] 	Batch(100/6809) done. Loss: 0.0060  lr:0.000001
[ Sat Jul 13 18:11:04 2024 ] 	Batch(200/6809) done. Loss: 0.0222  lr:0.000001
[ Sat Jul 13 18:11:22 2024 ] 	Batch(300/6809) done. Loss: 0.2063  lr:0.000001
[ Sat Jul 13 18:11:40 2024 ] 	Batch(400/6809) done. Loss: 0.1139  lr:0.000001
[ Sat Jul 13 18:11:58 2024 ] 
Training: Epoch [3/50], Step [499], Loss: 0.3690703809261322, Training Accuracy: 97.625
[ Sat Jul 13 18:11:58 2024 ] 	Batch(500/6809) done. Loss: 0.0896  lr:0.000001
[ Sat Jul 13 18:12:16 2024 ] 	Batch(600/6809) done. Loss: 0.0475  lr:0.000001
[ Sat Jul 13 18:12:34 2024 ] 	Batch(700/6809) done. Loss: 0.0150  lr:0.000001
[ Sat Jul 13 18:12:52 2024 ] 	Batch(800/6809) done. Loss: 0.0484  lr:0.000001
[ Sat Jul 13 18:13:10 2024 ] 	Batch(900/6809) done. Loss: 0.4992  lr:0.000001
[ Sat Jul 13 18:13:28 2024 ] 
Training: Epoch [3/50], Step [999], Loss: 0.02033662050962448, Training Accuracy: 97.5125
[ Sat Jul 13 18:13:28 2024 ] 	Batch(1000/6809) done. Loss: 0.1348  lr:0.000001
[ Sat Jul 13 18:13:46 2024 ] 	Batch(1100/6809) done. Loss: 0.0896  lr:0.000001
[ Sat Jul 13 18:14:04 2024 ] 	Batch(1200/6809) done. Loss: 0.1763  lr:0.000001
[ Sat Jul 13 18:14:22 2024 ] 	Batch(1300/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 18:14:40 2024 ] 	Batch(1400/6809) done. Loss: 0.0780  lr:0.000001
[ Sat Jul 13 18:14:58 2024 ] 
Training: Epoch [3/50], Step [1499], Loss: 0.03748159110546112, Training Accuracy: 97.64166666666667
[ Sat Jul 13 18:14:58 2024 ] 	Batch(1500/6809) done. Loss: 0.2337  lr:0.000001
[ Sat Jul 13 18:15:16 2024 ] 	Batch(1600/6809) done. Loss: 0.1211  lr:0.000001
[ Sat Jul 13 18:15:34 2024 ] 	Batch(1700/6809) done. Loss: 0.2042  lr:0.000001
[ Sat Jul 13 18:15:52 2024 ] 	Batch(1800/6809) done. Loss: 0.1084  lr:0.000001
[ Sat Jul 13 18:16:10 2024 ] 	Batch(1900/6809) done. Loss: 0.0792  lr:0.000001
[ Sat Jul 13 18:16:28 2024 ] 
Training: Epoch [3/50], Step [1999], Loss: 0.0027542756870388985, Training Accuracy: 97.59375
[ Sat Jul 13 18:16:28 2024 ] 	Batch(2000/6809) done. Loss: 0.0937  lr:0.000001
[ Sat Jul 13 18:16:47 2024 ] 	Batch(2100/6809) done. Loss: 0.0911  lr:0.000001
[ Sat Jul 13 18:17:05 2024 ] 	Batch(2200/6809) done. Loss: 0.0747  lr:0.000001
[ Sat Jul 13 18:17:23 2024 ] 	Batch(2300/6809) done. Loss: 0.2386  lr:0.000001
[ Sat Jul 13 18:17:41 2024 ] 	Batch(2400/6809) done. Loss: 0.1350  lr:0.000001
[ Sat Jul 13 18:17:59 2024 ] 
Training: Epoch [3/50], Step [2499], Loss: 0.0409516841173172, Training Accuracy: 97.625
[ Sat Jul 13 18:17:59 2024 ] 	Batch(2500/6809) done. Loss: 0.1891  lr:0.000001
[ Sat Jul 13 18:18:17 2024 ] 	Batch(2600/6809) done. Loss: 0.0153  lr:0.000001
[ Sat Jul 13 18:18:34 2024 ] 	Batch(2700/6809) done. Loss: 0.2850  lr:0.000001
[ Sat Jul 13 18:18:52 2024 ] 	Batch(2800/6809) done. Loss: 0.0529  lr:0.000001
[ Sat Jul 13 18:19:10 2024 ] 	Batch(2900/6809) done. Loss: 0.0094  lr:0.000001
[ Sat Jul 13 18:19:28 2024 ] 
Training: Epoch [3/50], Step [2999], Loss: 0.002500491449609399, Training Accuracy: 97.6625
[ Sat Jul 13 18:19:28 2024 ] 	Batch(3000/6809) done. Loss: 0.0548  lr:0.000001
[ Sat Jul 13 18:19:46 2024 ] 	Batch(3100/6809) done. Loss: 0.1330  lr:0.000001
[ Sat Jul 13 18:20:04 2024 ] 	Batch(3200/6809) done. Loss: 0.0764  lr:0.000001
[ Sat Jul 13 18:20:23 2024 ] 	Batch(3300/6809) done. Loss: 0.0229  lr:0.000001
[ Sat Jul 13 18:20:41 2024 ] 	Batch(3400/6809) done. Loss: 0.0571  lr:0.000001
[ Sat Jul 13 18:20:59 2024 ] 
Training: Epoch [3/50], Step [3499], Loss: 0.041447076946496964, Training Accuracy: 97.65714285714286
[ Sat Jul 13 18:20:59 2024 ] 	Batch(3500/6809) done. Loss: 0.0211  lr:0.000001
[ Sat Jul 13 18:21:17 2024 ] 	Batch(3600/6809) done. Loss: 0.0252  lr:0.000001
[ Sat Jul 13 18:21:35 2024 ] 	Batch(3700/6809) done. Loss: 0.2229  lr:0.000001
[ Sat Jul 13 18:21:53 2024 ] 	Batch(3800/6809) done. Loss: 0.1048  lr:0.000001
[ Sat Jul 13 18:22:11 2024 ] 	Batch(3900/6809) done. Loss: 0.0519  lr:0.000001
[ Sat Jul 13 18:22:29 2024 ] 
Training: Epoch [3/50], Step [3999], Loss: 0.019122658297419548, Training Accuracy: 97.6875
[ Sat Jul 13 18:22:29 2024 ] 	Batch(4000/6809) done. Loss: 0.2028  lr:0.000001
[ Sat Jul 13 18:22:47 2024 ] 	Batch(4100/6809) done. Loss: 0.0549  lr:0.000001
[ Sat Jul 13 18:23:05 2024 ] 	Batch(4200/6809) done. Loss: 0.1232  lr:0.000001
[ Sat Jul 13 18:23:23 2024 ] 	Batch(4300/6809) done. Loss: 0.1745  lr:0.000001
[ Sat Jul 13 18:23:41 2024 ] 	Batch(4400/6809) done. Loss: 0.0082  lr:0.000001
[ Sat Jul 13 18:23:59 2024 ] 
Training: Epoch [3/50], Step [4499], Loss: 0.23971618711948395, Training Accuracy: 97.65555555555555
[ Sat Jul 13 18:23:59 2024 ] 	Batch(4500/6809) done. Loss: 0.0062  lr:0.000001
[ Sat Jul 13 18:24:17 2024 ] 	Batch(4600/6809) done. Loss: 0.2776  lr:0.000001
[ Sat Jul 13 18:24:35 2024 ] 	Batch(4700/6809) done. Loss: 0.0053  lr:0.000001
[ Sat Jul 13 18:24:53 2024 ] 	Batch(4800/6809) done. Loss: 0.0349  lr:0.000001
[ Sat Jul 13 18:25:11 2024 ] 	Batch(4900/6809) done. Loss: 0.0163  lr:0.000001
[ Sat Jul 13 18:25:29 2024 ] 
Training: Epoch [3/50], Step [4999], Loss: 0.17970812320709229, Training Accuracy: 97.67
[ Sat Jul 13 18:25:29 2024 ] 	Batch(5000/6809) done. Loss: 0.0898  lr:0.000001
[ Sat Jul 13 18:25:47 2024 ] 	Batch(5100/6809) done. Loss: 0.0393  lr:0.000001
[ Sat Jul 13 18:26:05 2024 ] 	Batch(5200/6809) done. Loss: 0.2063  lr:0.000001
[ Sat Jul 13 18:26:23 2024 ] 	Batch(5300/6809) done. Loss: 0.1421  lr:0.000001
[ Sat Jul 13 18:26:40 2024 ] 	Batch(5400/6809) done. Loss: 0.2161  lr:0.000001
[ Sat Jul 13 18:26:58 2024 ] 
Training: Epoch [3/50], Step [5499], Loss: 0.13576170802116394, Training Accuracy: 97.66363636363636
[ Sat Jul 13 18:26:58 2024 ] 	Batch(5500/6809) done. Loss: 0.2575  lr:0.000001
[ Sat Jul 13 18:27:16 2024 ] 	Batch(5600/6809) done. Loss: 0.0354  lr:0.000001
[ Sat Jul 13 18:27:35 2024 ] 	Batch(5700/6809) done. Loss: 0.0048  lr:0.000001
[ Sat Jul 13 18:27:54 2024 ] 	Batch(5800/6809) done. Loss: 0.0426  lr:0.000001
[ Sat Jul 13 18:28:12 2024 ] 	Batch(5900/6809) done. Loss: 0.0179  lr:0.000001
[ Sat Jul 13 18:28:30 2024 ] 
Training: Epoch [3/50], Step [5999], Loss: 0.14799439907073975, Training Accuracy: 97.66458333333333
[ Sat Jul 13 18:28:31 2024 ] 	Batch(6000/6809) done. Loss: 0.0175  lr:0.000001
[ Sat Jul 13 18:28:49 2024 ] 	Batch(6100/6809) done. Loss: 0.0770  lr:0.000001
[ Sat Jul 13 18:29:08 2024 ] 	Batch(6200/6809) done. Loss: 0.0306  lr:0.000001
[ Sat Jul 13 18:29:26 2024 ] 	Batch(6300/6809) done. Loss: 0.0197  lr:0.000001
[ Sat Jul 13 18:29:45 2024 ] 	Batch(6400/6809) done. Loss: 0.0966  lr:0.000001
[ Sat Jul 13 18:30:03 2024 ] 
Training: Epoch [3/50], Step [6499], Loss: 0.03438464552164078, Training Accuracy: 97.63461538461539
[ Sat Jul 13 18:30:03 2024 ] 	Batch(6500/6809) done. Loss: 0.0108  lr:0.000001
[ Sat Jul 13 18:30:22 2024 ] 	Batch(6600/6809) done. Loss: 0.0486  lr:0.000001
[ Sat Jul 13 18:30:40 2024 ] 	Batch(6700/6809) done. Loss: 0.1496  lr:0.000001
[ Sat Jul 13 18:30:59 2024 ] 	Batch(6800/6809) done. Loss: 0.0044  lr:0.000001
[ Sat Jul 13 18:31:01 2024 ] 	Mean training loss: 0.0961.
[ Sat Jul 13 18:31:01 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 18:31:01 2024 ] Training epoch: 5
[ Sat Jul 13 18:31:01 2024 ] 	Batch(0/6809) done. Loss: 0.3339  lr:0.000001
[ Sat Jul 13 18:31:19 2024 ] 	Batch(100/6809) done. Loss: 0.2354  lr:0.000001
[ Sat Jul 13 18:31:37 2024 ] 	Batch(200/6809) done. Loss: 0.1504  lr:0.000001
[ Sat Jul 13 18:31:55 2024 ] 	Batch(300/6809) done. Loss: 0.0949  lr:0.000001
[ Sat Jul 13 18:32:13 2024 ] 	Batch(400/6809) done. Loss: 0.0067  lr:0.000001
[ Sat Jul 13 18:32:31 2024 ] 
Training: Epoch [4/50], Step [499], Loss: 0.00725482776761055, Training Accuracy: 97.2
[ Sat Jul 13 18:32:31 2024 ] 	Batch(500/6809) done. Loss: 0.0078  lr:0.000001
[ Sat Jul 13 18:32:49 2024 ] 	Batch(600/6809) done. Loss: 0.0134  lr:0.000001
[ Sat Jul 13 18:33:07 2024 ] 	Batch(700/6809) done. Loss: 0.3434  lr:0.000001
[ Sat Jul 13 18:33:25 2024 ] 	Batch(800/6809) done. Loss: 0.2615  lr:0.000001
[ Sat Jul 13 18:33:43 2024 ] 	Batch(900/6809) done. Loss: 0.0739  lr:0.000001
[ Sat Jul 13 18:34:01 2024 ] 
Training: Epoch [4/50], Step [999], Loss: 0.009035991504788399, Training Accuracy: 97.475
[ Sat Jul 13 18:34:01 2024 ] 	Batch(1000/6809) done. Loss: 0.0416  lr:0.000001
[ Sat Jul 13 18:34:19 2024 ] 	Batch(1100/6809) done. Loss: 0.1059  lr:0.000001
[ Sat Jul 13 18:34:37 2024 ] 	Batch(1200/6809) done. Loss: 0.0081  lr:0.000001
[ Sat Jul 13 18:34:55 2024 ] 	Batch(1300/6809) done. Loss: 0.2585  lr:0.000001
[ Sat Jul 13 18:35:13 2024 ] 	Batch(1400/6809) done. Loss: 0.0236  lr:0.000001
[ Sat Jul 13 18:35:31 2024 ] 
Training: Epoch [4/50], Step [1499], Loss: 0.0009631104185245931, Training Accuracy: 97.53333333333333
[ Sat Jul 13 18:35:31 2024 ] 	Batch(1500/6809) done. Loss: 0.4699  lr:0.000001
[ Sat Jul 13 18:35:49 2024 ] 	Batch(1600/6809) done. Loss: 0.1361  lr:0.000001
[ Sat Jul 13 18:36:07 2024 ] 	Batch(1700/6809) done. Loss: 0.0313  lr:0.000001
[ Sat Jul 13 18:36:25 2024 ] 	Batch(1800/6809) done. Loss: 0.0101  lr:0.000001
[ Sat Jul 13 18:36:42 2024 ] 	Batch(1900/6809) done. Loss: 0.0497  lr:0.000001
[ Sat Jul 13 18:37:00 2024 ] 
Training: Epoch [4/50], Step [1999], Loss: 0.30713731050491333, Training Accuracy: 97.45
[ Sat Jul 13 18:37:01 2024 ] 	Batch(2000/6809) done. Loss: 0.2162  lr:0.000001
[ Sat Jul 13 18:37:18 2024 ] 	Batch(2100/6809) done. Loss: 0.0531  lr:0.000001
[ Sat Jul 13 18:37:36 2024 ] 	Batch(2200/6809) done. Loss: 0.0411  lr:0.000001
[ Sat Jul 13 18:37:54 2024 ] 	Batch(2300/6809) done. Loss: 0.0917  lr:0.000001
[ Sat Jul 13 18:38:12 2024 ] 	Batch(2400/6809) done. Loss: 0.0056  lr:0.000001
[ Sat Jul 13 18:38:30 2024 ] 
Training: Epoch [4/50], Step [2499], Loss: 0.022868124768137932, Training Accuracy: 97.43
[ Sat Jul 13 18:38:30 2024 ] 	Batch(2500/6809) done. Loss: 0.9410  lr:0.000001
[ Sat Jul 13 18:38:48 2024 ] 	Batch(2600/6809) done. Loss: 0.1705  lr:0.000001
[ Sat Jul 13 18:39:06 2024 ] 	Batch(2700/6809) done. Loss: 0.0915  lr:0.000001
[ Sat Jul 13 18:39:24 2024 ] 	Batch(2800/6809) done. Loss: 0.0335  lr:0.000001
[ Sat Jul 13 18:39:42 2024 ] 	Batch(2900/6809) done. Loss: 0.0058  lr:0.000001
[ Sat Jul 13 18:40:01 2024 ] 
Training: Epoch [4/50], Step [2999], Loss: 0.059231627732515335, Training Accuracy: 97.41666666666666
[ Sat Jul 13 18:40:01 2024 ] 	Batch(3000/6809) done. Loss: 0.3330  lr:0.000001
[ Sat Jul 13 18:40:19 2024 ] 	Batch(3100/6809) done. Loss: 0.0507  lr:0.000001
[ Sat Jul 13 18:40:37 2024 ] 	Batch(3200/6809) done. Loss: 0.0187  lr:0.000001
[ Sat Jul 13 18:40:55 2024 ] 	Batch(3300/6809) done. Loss: 0.0313  lr:0.000001
[ Sat Jul 13 18:41:14 2024 ] 	Batch(3400/6809) done. Loss: 0.0322  lr:0.000001
[ Sat Jul 13 18:41:32 2024 ] 
Training: Epoch [4/50], Step [3499], Loss: 0.0025346772745251656, Training Accuracy: 97.48928571428571
[ Sat Jul 13 18:41:32 2024 ] 	Batch(3500/6809) done. Loss: 0.0259  lr:0.000001
[ Sat Jul 13 18:41:50 2024 ] 	Batch(3600/6809) done. Loss: 0.0731  lr:0.000001
[ Sat Jul 13 18:42:08 2024 ] 	Batch(3700/6809) done. Loss: 0.0039  lr:0.000001
[ Sat Jul 13 18:42:26 2024 ] 	Batch(3800/6809) done. Loss: 0.0096  lr:0.000001
[ Sat Jul 13 18:42:44 2024 ] 	Batch(3900/6809) done. Loss: 0.0582  lr:0.000001
[ Sat Jul 13 18:43:02 2024 ] 
Training: Epoch [4/50], Step [3999], Loss: 0.048774223774671555, Training Accuracy: 97.496875
[ Sat Jul 13 18:43:02 2024 ] 	Batch(4000/6809) done. Loss: 0.0225  lr:0.000001
[ Sat Jul 13 18:43:20 2024 ] 	Batch(4100/6809) done. Loss: 0.0187  lr:0.000001
[ Sat Jul 13 18:43:38 2024 ] 	Batch(4200/6809) done. Loss: 0.1483  lr:0.000001
[ Sat Jul 13 18:43:56 2024 ] 	Batch(4300/6809) done. Loss: 0.0151  lr:0.000001
[ Sat Jul 13 18:44:14 2024 ] 	Batch(4400/6809) done. Loss: 0.0885  lr:0.000001
[ Sat Jul 13 18:44:31 2024 ] 
Training: Epoch [4/50], Step [4499], Loss: 0.05179843679070473, Training Accuracy: 97.49166666666666
[ Sat Jul 13 18:44:32 2024 ] 	Batch(4500/6809) done. Loss: 0.2248  lr:0.000001
[ Sat Jul 13 18:44:49 2024 ] 	Batch(4600/6809) done. Loss: 0.0434  lr:0.000001
[ Sat Jul 13 18:45:07 2024 ] 	Batch(4700/6809) done. Loss: 0.2183  lr:0.000001
[ Sat Jul 13 18:45:25 2024 ] 	Batch(4800/6809) done. Loss: 0.0156  lr:0.000001
[ Sat Jul 13 18:45:44 2024 ] 	Batch(4900/6809) done. Loss: 0.3341  lr:0.000001
[ Sat Jul 13 18:46:03 2024 ] 
Training: Epoch [4/50], Step [4999], Loss: 0.10269446671009064, Training Accuracy: 97.4575
[ Sat Jul 13 18:46:03 2024 ] 	Batch(5000/6809) done. Loss: 0.0239  lr:0.000001
[ Sat Jul 13 18:46:21 2024 ] 	Batch(5100/6809) done. Loss: 0.1118  lr:0.000001
[ Sat Jul 13 18:46:40 2024 ] 	Batch(5200/6809) done. Loss: 0.0119  lr:0.000001
[ Sat Jul 13 18:46:58 2024 ] 	Batch(5300/6809) done. Loss: 0.0236  lr:0.000001
[ Sat Jul 13 18:47:17 2024 ] 	Batch(5400/6809) done. Loss: 0.0179  lr:0.000001
[ Sat Jul 13 18:47:35 2024 ] 
Training: Epoch [4/50], Step [5499], Loss: 0.15011245012283325, Training Accuracy: 97.45454545454545
[ Sat Jul 13 18:47:36 2024 ] 	Batch(5500/6809) done. Loss: 0.0513  lr:0.000001
[ Sat Jul 13 18:47:54 2024 ] 	Batch(5600/6809) done. Loss: 0.1253  lr:0.000001
[ Sat Jul 13 18:48:13 2024 ] 	Batch(5700/6809) done. Loss: 0.0547  lr:0.000001
[ Sat Jul 13 18:48:31 2024 ] 	Batch(5800/6809) done. Loss: 0.0072  lr:0.000001
[ Sat Jul 13 18:48:50 2024 ] 	Batch(5900/6809) done. Loss: 0.1026  lr:0.000001
[ Sat Jul 13 18:49:08 2024 ] 
Training: Epoch [4/50], Step [5999], Loss: 0.052310049533843994, Training Accuracy: 97.46249999999999
[ Sat Jul 13 18:49:08 2024 ] 	Batch(6000/6809) done. Loss: 0.0095  lr:0.000001
[ Sat Jul 13 18:49:26 2024 ] 	Batch(6100/6809) done. Loss: 0.1520  lr:0.000001
[ Sat Jul 13 18:49:44 2024 ] 	Batch(6200/6809) done. Loss: 0.0259  lr:0.000001
[ Sat Jul 13 18:50:02 2024 ] 	Batch(6300/6809) done. Loss: 0.2644  lr:0.000001
[ Sat Jul 13 18:50:20 2024 ] 	Batch(6400/6809) done. Loss: 0.2592  lr:0.000001
[ Sat Jul 13 18:50:38 2024 ] 
Training: Epoch [4/50], Step [6499], Loss: 0.03673573210835457, Training Accuracy: 97.43076923076923
[ Sat Jul 13 18:50:38 2024 ] 	Batch(6500/6809) done. Loss: 0.1084  lr:0.000001
[ Sat Jul 13 18:50:56 2024 ] 	Batch(6600/6809) done. Loss: 0.0704  lr:0.000001
[ Sat Jul 13 18:51:14 2024 ] 	Batch(6700/6809) done. Loss: 0.1100  lr:0.000001
[ Sat Jul 13 18:51:32 2024 ] 	Batch(6800/6809) done. Loss: 0.0273  lr:0.000001
[ Sat Jul 13 18:51:33 2024 ] 	Mean training loss: 0.0987.
[ Sat Jul 13 18:51:33 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 18:51:33 2024 ] Training epoch: 6
[ Sat Jul 13 18:51:34 2024 ] 	Batch(0/6809) done. Loss: 0.0649  lr:0.000001
[ Sat Jul 13 18:51:52 2024 ] 	Batch(100/6809) done. Loss: 0.0024  lr:0.000001
[ Sat Jul 13 18:52:10 2024 ] 	Batch(200/6809) done. Loss: 0.0131  lr:0.000001
[ Sat Jul 13 18:52:28 2024 ] 	Batch(300/6809) done. Loss: 0.0816  lr:0.000001
[ Sat Jul 13 18:52:46 2024 ] 	Batch(400/6809) done. Loss: 0.1297  lr:0.000001
[ Sat Jul 13 18:53:03 2024 ] 
Training: Epoch [5/50], Step [499], Loss: 0.035290006548166275, Training Accuracy: 97.75
[ Sat Jul 13 18:53:04 2024 ] 	Batch(500/6809) done. Loss: 0.0114  lr:0.000001
[ Sat Jul 13 18:53:22 2024 ] 	Batch(600/6809) done. Loss: 0.0579  lr:0.000001
[ Sat Jul 13 18:53:39 2024 ] 	Batch(700/6809) done. Loss: 0.0725  lr:0.000001
[ Sat Jul 13 18:53:57 2024 ] 	Batch(800/6809) done. Loss: 0.0084  lr:0.000001
[ Sat Jul 13 18:54:16 2024 ] 	Batch(900/6809) done. Loss: 0.0478  lr:0.000001
[ Sat Jul 13 18:54:34 2024 ] 
Training: Epoch [5/50], Step [999], Loss: 0.009633874520659447, Training Accuracy: 97.6375
[ Sat Jul 13 18:54:35 2024 ] 	Batch(1000/6809) done. Loss: 0.0031  lr:0.000001
[ Sat Jul 13 18:54:53 2024 ] 	Batch(1100/6809) done. Loss: 0.2679  lr:0.000001
[ Sat Jul 13 18:55:12 2024 ] 	Batch(1200/6809) done. Loss: 0.0541  lr:0.000001
[ Sat Jul 13 18:55:30 2024 ] 	Batch(1300/6809) done. Loss: 0.0089  lr:0.000001
[ Sat Jul 13 18:55:48 2024 ] 	Batch(1400/6809) done. Loss: 0.2358  lr:0.000001
[ Sat Jul 13 18:56:06 2024 ] 
Training: Epoch [5/50], Step [1499], Loss: 0.019377343356609344, Training Accuracy: 97.675
[ Sat Jul 13 18:56:06 2024 ] 	Batch(1500/6809) done. Loss: 0.0569  lr:0.000001
[ Sat Jul 13 18:56:24 2024 ] 	Batch(1600/6809) done. Loss: 0.1312  lr:0.000001
[ Sat Jul 13 18:56:42 2024 ] 	Batch(1700/6809) done. Loss: 0.2041  lr:0.000001
[ Sat Jul 13 18:57:00 2024 ] 	Batch(1800/6809) done. Loss: 0.0449  lr:0.000001
[ Sat Jul 13 18:57:18 2024 ] 	Batch(1900/6809) done. Loss: 0.0036  lr:0.000001
[ Sat Jul 13 18:57:36 2024 ] 
Training: Epoch [5/50], Step [1999], Loss: 0.041688427329063416, Training Accuracy: 97.64375
[ Sat Jul 13 18:57:36 2024 ] 	Batch(2000/6809) done. Loss: 0.4597  lr:0.000001
[ Sat Jul 13 18:57:54 2024 ] 	Batch(2100/6809) done. Loss: 0.0080  lr:0.000001
[ Sat Jul 13 18:58:12 2024 ] 	Batch(2200/6809) done. Loss: 0.0174  lr:0.000001
[ Sat Jul 13 18:58:30 2024 ] 	Batch(2300/6809) done. Loss: 0.0182  lr:0.000001
[ Sat Jul 13 18:58:48 2024 ] 	Batch(2400/6809) done. Loss: 0.1472  lr:0.000001
[ Sat Jul 13 18:59:06 2024 ] 
Training: Epoch [5/50], Step [2499], Loss: 0.0257780272513628, Training Accuracy: 97.66
[ Sat Jul 13 18:59:06 2024 ] 	Batch(2500/6809) done. Loss: 0.3154  lr:0.000001
[ Sat Jul 13 18:59:25 2024 ] 	Batch(2600/6809) done. Loss: 0.0573  lr:0.000001
[ Sat Jul 13 18:59:43 2024 ] 	Batch(2700/6809) done. Loss: 0.0692  lr:0.000001
[ Sat Jul 13 19:00:02 2024 ] 	Batch(2800/6809) done. Loss: 0.0217  lr:0.000001
[ Sat Jul 13 19:00:21 2024 ] 	Batch(2900/6809) done. Loss: 0.1602  lr:0.000001
[ Sat Jul 13 19:00:39 2024 ] 
Training: Epoch [5/50], Step [2999], Loss: 0.01662658341228962, Training Accuracy: 97.72083333333333
[ Sat Jul 13 19:00:39 2024 ] 	Batch(3000/6809) done. Loss: 0.0747  lr:0.000001
[ Sat Jul 13 19:00:58 2024 ] 	Batch(3100/6809) done. Loss: 0.0027  lr:0.000001
[ Sat Jul 13 19:01:16 2024 ] 	Batch(3200/6809) done. Loss: 0.0509  lr:0.000001
[ Sat Jul 13 19:01:34 2024 ] 	Batch(3300/6809) done. Loss: 0.1811  lr:0.000001
[ Sat Jul 13 19:01:52 2024 ] 	Batch(3400/6809) done. Loss: 0.0350  lr:0.000001
[ Sat Jul 13 19:02:09 2024 ] 
Training: Epoch [5/50], Step [3499], Loss: 0.1627480834722519, Training Accuracy: 97.69285714285715
[ Sat Jul 13 19:02:10 2024 ] 	Batch(3500/6809) done. Loss: 0.0842  lr:0.000001
[ Sat Jul 13 19:02:28 2024 ] 	Batch(3600/6809) done. Loss: 0.0066  lr:0.000001
[ Sat Jul 13 19:02:46 2024 ] 	Batch(3700/6809) done. Loss: 0.2348  lr:0.000001
[ Sat Jul 13 19:03:03 2024 ] 	Batch(3800/6809) done. Loss: 0.2746  lr:0.000001
[ Sat Jul 13 19:03:21 2024 ] 	Batch(3900/6809) done. Loss: 0.1128  lr:0.000001
[ Sat Jul 13 19:03:39 2024 ] 
Training: Epoch [5/50], Step [3999], Loss: 0.14175893366336823, Training Accuracy: 97.66875
[ Sat Jul 13 19:03:39 2024 ] 	Batch(4000/6809) done. Loss: 0.0101  lr:0.000001
[ Sat Jul 13 19:03:57 2024 ] 	Batch(4100/6809) done. Loss: 0.0383  lr:0.000001
[ Sat Jul 13 19:04:15 2024 ] 	Batch(4200/6809) done. Loss: 0.1992  lr:0.000001
[ Sat Jul 13 19:04:33 2024 ] 	Batch(4300/6809) done. Loss: 0.0470  lr:0.000001
[ Sat Jul 13 19:04:51 2024 ] 	Batch(4400/6809) done. Loss: 0.0537  lr:0.000001
[ Sat Jul 13 19:05:09 2024 ] 
Training: Epoch [5/50], Step [4499], Loss: 0.14371883869171143, Training Accuracy: 97.66666666666667
[ Sat Jul 13 19:05:09 2024 ] 	Batch(4500/6809) done. Loss: 0.2639  lr:0.000001
[ Sat Jul 13 19:05:27 2024 ] 	Batch(4600/6809) done. Loss: 0.1430  lr:0.000001
[ Sat Jul 13 19:05:45 2024 ] 	Batch(4700/6809) done. Loss: 0.0175  lr:0.000001
[ Sat Jul 13 19:06:03 2024 ] 	Batch(4800/6809) done. Loss: 0.0033  lr:0.000001
[ Sat Jul 13 19:06:21 2024 ] 	Batch(4900/6809) done. Loss: 0.2419  lr:0.000001
[ Sat Jul 13 19:06:39 2024 ] 
Training: Epoch [5/50], Step [4999], Loss: 0.0043901102617383, Training Accuracy: 97.625
[ Sat Jul 13 19:06:39 2024 ] 	Batch(5000/6809) done. Loss: 0.0243  lr:0.000001
[ Sat Jul 13 19:06:57 2024 ] 	Batch(5100/6809) done. Loss: 0.0206  lr:0.000001
[ Sat Jul 13 19:07:15 2024 ] 	Batch(5200/6809) done. Loss: 0.0751  lr:0.000001
[ Sat Jul 13 19:07:32 2024 ] 	Batch(5300/6809) done. Loss: 0.0917  lr:0.000001
[ Sat Jul 13 19:07:50 2024 ] 	Batch(5400/6809) done. Loss: 0.1247  lr:0.000001
[ Sat Jul 13 19:08:08 2024 ] 
Training: Epoch [5/50], Step [5499], Loss: 0.014363196678459644, Training Accuracy: 97.60454545454546
[ Sat Jul 13 19:08:08 2024 ] 	Batch(5500/6809) done. Loss: 0.2588  lr:0.000001
[ Sat Jul 13 19:08:26 2024 ] 	Batch(5600/6809) done. Loss: 0.0200  lr:0.000001
[ Sat Jul 13 19:08:44 2024 ] 	Batch(5700/6809) done. Loss: 0.2583  lr:0.000001
[ Sat Jul 13 19:09:02 2024 ] 	Batch(5800/6809) done. Loss: 0.1610  lr:0.000001
[ Sat Jul 13 19:09:20 2024 ] 	Batch(5900/6809) done. Loss: 0.0286  lr:0.000001
[ Sat Jul 13 19:09:38 2024 ] 
Training: Epoch [5/50], Step [5999], Loss: 0.044060830026865005, Training Accuracy: 97.60416666666667
[ Sat Jul 13 19:09:38 2024 ] 	Batch(6000/6809) done. Loss: 0.1296  lr:0.000001
[ Sat Jul 13 19:09:56 2024 ] 	Batch(6100/6809) done. Loss: 0.0491  lr:0.000001
[ Sat Jul 13 19:10:14 2024 ] 	Batch(6200/6809) done. Loss: 0.1165  lr:0.000001
[ Sat Jul 13 19:10:33 2024 ] 	Batch(6300/6809) done. Loss: 0.4324  lr:0.000001
[ Sat Jul 13 19:10:51 2024 ] 	Batch(6400/6809) done. Loss: 0.0133  lr:0.000001
[ Sat Jul 13 19:11:09 2024 ] 
Training: Epoch [5/50], Step [6499], Loss: 0.04845244437456131, Training Accuracy: 97.5673076923077
[ Sat Jul 13 19:11:10 2024 ] 	Batch(6500/6809) done. Loss: 0.0383  lr:0.000001
[ Sat Jul 13 19:11:28 2024 ] 	Batch(6600/6809) done. Loss: 0.0329  lr:0.000001
[ Sat Jul 13 19:11:45 2024 ] 	Batch(6700/6809) done. Loss: 0.0857  lr:0.000001
[ Sat Jul 13 19:12:03 2024 ] 	Batch(6800/6809) done. Loss: 0.1354  lr:0.000001
[ Sat Jul 13 19:12:05 2024 ] 	Mean training loss: 0.0953.
[ Sat Jul 13 19:12:05 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 19:12:05 2024 ] Training epoch: 7
[ Sat Jul 13 19:12:06 2024 ] 	Batch(0/6809) done. Loss: 0.1141  lr:0.000001
[ Sat Jul 13 19:12:24 2024 ] 	Batch(100/6809) done. Loss: 0.3790  lr:0.000001
[ Sat Jul 13 19:12:42 2024 ] 	Batch(200/6809) done. Loss: 0.0091  lr:0.000001
[ Sat Jul 13 19:13:01 2024 ] 	Batch(300/6809) done. Loss: 0.1294  lr:0.000001
[ Sat Jul 13 19:13:19 2024 ] 	Batch(400/6809) done. Loss: 0.0573  lr:0.000001
[ Sat Jul 13 19:13:37 2024 ] 
Training: Epoch [6/50], Step [499], Loss: 0.1441720873117447, Training Accuracy: 97.6
[ Sat Jul 13 19:13:37 2024 ] 	Batch(500/6809) done. Loss: 0.7090  lr:0.000001
[ Sat Jul 13 19:13:55 2024 ] 	Batch(600/6809) done. Loss: 0.6348  lr:0.000001
[ Sat Jul 13 19:14:13 2024 ] 	Batch(700/6809) done. Loss: 0.0056  lr:0.000001
[ Sat Jul 13 19:14:31 2024 ] 	Batch(800/6809) done. Loss: 0.1646  lr:0.000001
[ Sat Jul 13 19:14:49 2024 ] 	Batch(900/6809) done. Loss: 0.0116  lr:0.000001
[ Sat Jul 13 19:15:07 2024 ] 
Training: Epoch [6/50], Step [999], Loss: 0.015813950449228287, Training Accuracy: 97.6
[ Sat Jul 13 19:15:07 2024 ] 	Batch(1000/6809) done. Loss: 0.0130  lr:0.000001
[ Sat Jul 13 19:15:25 2024 ] 	Batch(1100/6809) done. Loss: 0.0651  lr:0.000001
[ Sat Jul 13 19:15:44 2024 ] 	Batch(1200/6809) done. Loss: 0.3161  lr:0.000001
[ Sat Jul 13 19:16:02 2024 ] 	Batch(1300/6809) done. Loss: 0.0467  lr:0.000001
[ Sat Jul 13 19:16:20 2024 ] 	Batch(1400/6809) done. Loss: 0.1014  lr:0.000001
[ Sat Jul 13 19:16:37 2024 ] 
Training: Epoch [6/50], Step [1499], Loss: 0.08180341124534607, Training Accuracy: 97.76666666666667
[ Sat Jul 13 19:16:38 2024 ] 	Batch(1500/6809) done. Loss: 0.0988  lr:0.000001
[ Sat Jul 13 19:16:56 2024 ] 	Batch(1600/6809) done. Loss: 0.1437  lr:0.000001
[ Sat Jul 13 19:17:14 2024 ] 	Batch(1700/6809) done. Loss: 0.0846  lr:0.000001
[ Sat Jul 13 19:17:32 2024 ] 	Batch(1800/6809) done. Loss: 0.0666  lr:0.000001
[ Sat Jul 13 19:17:50 2024 ] 	Batch(1900/6809) done. Loss: 0.0230  lr:0.000001
[ Sat Jul 13 19:18:08 2024 ] 
Training: Epoch [6/50], Step [1999], Loss: 0.056779325008392334, Training Accuracy: 97.7125
[ Sat Jul 13 19:18:08 2024 ] 	Batch(2000/6809) done. Loss: 0.2165  lr:0.000001
[ Sat Jul 13 19:18:26 2024 ] 	Batch(2100/6809) done. Loss: 0.0239  lr:0.000001
[ Sat Jul 13 19:18:44 2024 ] 	Batch(2200/6809) done. Loss: 0.1939  lr:0.000001
[ Sat Jul 13 19:19:02 2024 ] 	Batch(2300/6809) done. Loss: 0.0390  lr:0.000001
[ Sat Jul 13 19:19:20 2024 ] 	Batch(2400/6809) done. Loss: 0.0616  lr:0.000001
[ Sat Jul 13 19:19:38 2024 ] 
Training: Epoch [6/50], Step [2499], Loss: 0.06961376219987869, Training Accuracy: 97.68
[ Sat Jul 13 19:19:38 2024 ] 	Batch(2500/6809) done. Loss: 0.3607  lr:0.000001
[ Sat Jul 13 19:19:56 2024 ] 	Batch(2600/6809) done. Loss: 0.0456  lr:0.000001
[ Sat Jul 13 19:20:14 2024 ] 	Batch(2700/6809) done. Loss: 0.0473  lr:0.000001
[ Sat Jul 13 19:20:32 2024 ] 	Batch(2800/6809) done. Loss: 0.3613  lr:0.000001
[ Sat Jul 13 19:20:51 2024 ] 	Batch(2900/6809) done. Loss: 0.1726  lr:0.000001
[ Sat Jul 13 19:21:09 2024 ] 
Training: Epoch [6/50], Step [2999], Loss: 0.04006947576999664, Training Accuracy: 97.70833333333333
[ Sat Jul 13 19:21:09 2024 ] 	Batch(3000/6809) done. Loss: 0.0297  lr:0.000001
[ Sat Jul 13 19:21:27 2024 ] 	Batch(3100/6809) done. Loss: 0.0734  lr:0.000001
[ Sat Jul 13 19:21:45 2024 ] 	Batch(3200/6809) done. Loss: 0.0239  lr:0.000001
[ Sat Jul 13 19:22:03 2024 ] 	Batch(3300/6809) done. Loss: 0.0271  lr:0.000001
[ Sat Jul 13 19:22:21 2024 ] 	Batch(3400/6809) done. Loss: 0.0117  lr:0.000001
[ Sat Jul 13 19:22:39 2024 ] 
Training: Epoch [6/50], Step [3499], Loss: 0.004986948799341917, Training Accuracy: 97.66071428571429
[ Sat Jul 13 19:22:39 2024 ] 	Batch(3500/6809) done. Loss: 0.1306  lr:0.000001
[ Sat Jul 13 19:22:57 2024 ] 	Batch(3600/6809) done. Loss: 0.0304  lr:0.000001
[ Sat Jul 13 19:23:16 2024 ] 	Batch(3700/6809) done. Loss: 0.0759  lr:0.000001
[ Sat Jul 13 19:23:34 2024 ] 	Batch(3800/6809) done. Loss: 0.1424  lr:0.000001
[ Sat Jul 13 19:23:52 2024 ] 	Batch(3900/6809) done. Loss: 0.0450  lr:0.000001
[ Sat Jul 13 19:24:10 2024 ] 
Training: Epoch [6/50], Step [3999], Loss: 0.02142610028386116, Training Accuracy: 97.6625
[ Sat Jul 13 19:24:10 2024 ] 	Batch(4000/6809) done. Loss: 0.2551  lr:0.000001
[ Sat Jul 13 19:24:28 2024 ] 	Batch(4100/6809) done. Loss: 0.0386  lr:0.000001
[ Sat Jul 13 19:24:46 2024 ] 	Batch(4200/6809) done. Loss: 0.1558  lr:0.000001
[ Sat Jul 13 19:25:04 2024 ] 	Batch(4300/6809) done. Loss: 0.0008  lr:0.000001
[ Sat Jul 13 19:25:22 2024 ] 	Batch(4400/6809) done. Loss: 0.1510  lr:0.000001
[ Sat Jul 13 19:25:40 2024 ] 
Training: Epoch [6/50], Step [4499], Loss: 0.029387282207608223, Training Accuracy: 97.60833333333333
[ Sat Jul 13 19:25:40 2024 ] 	Batch(4500/6809) done. Loss: 0.0030  lr:0.000001
[ Sat Jul 13 19:25:58 2024 ] 	Batch(4600/6809) done. Loss: 0.0417  lr:0.000001
[ Sat Jul 13 19:26:16 2024 ] 	Batch(4700/6809) done. Loss: 0.1044  lr:0.000001
[ Sat Jul 13 19:26:34 2024 ] 	Batch(4800/6809) done. Loss: 0.0746  lr:0.000001
[ Sat Jul 13 19:26:53 2024 ] 	Batch(4900/6809) done. Loss: 0.0223  lr:0.000001
[ Sat Jul 13 19:27:10 2024 ] 
Training: Epoch [6/50], Step [4999], Loss: 0.345646470785141, Training Accuracy: 97.6
[ Sat Jul 13 19:27:11 2024 ] 	Batch(5000/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 19:27:28 2024 ] 	Batch(5100/6809) done. Loss: 0.0455  lr:0.000001
[ Sat Jul 13 19:27:47 2024 ] 	Batch(5200/6809) done. Loss: 0.0229  lr:0.000001
[ Sat Jul 13 19:28:05 2024 ] 	Batch(5300/6809) done. Loss: 0.0187  lr:0.000001
[ Sat Jul 13 19:28:24 2024 ] 	Batch(5400/6809) done. Loss: 0.0510  lr:0.000001
[ Sat Jul 13 19:28:42 2024 ] 
Training: Epoch [6/50], Step [5499], Loss: 0.19529880583286285, Training Accuracy: 97.61136363636363
[ Sat Jul 13 19:28:42 2024 ] 	Batch(5500/6809) done. Loss: 0.0878  lr:0.000001
[ Sat Jul 13 19:29:00 2024 ] 	Batch(5600/6809) done. Loss: 0.1925  lr:0.000001
[ Sat Jul 13 19:29:18 2024 ] 	Batch(5700/6809) done. Loss: 0.1914  lr:0.000001
[ Sat Jul 13 19:29:36 2024 ] 	Batch(5800/6809) done. Loss: 0.0969  lr:0.000001
[ Sat Jul 13 19:29:54 2024 ] 	Batch(5900/6809) done. Loss: 0.3074  lr:0.000001
[ Sat Jul 13 19:30:11 2024 ] 
Training: Epoch [6/50], Step [5999], Loss: 0.04902496933937073, Training Accuracy: 97.57708333333333
[ Sat Jul 13 19:30:12 2024 ] 	Batch(6000/6809) done. Loss: 0.1805  lr:0.000001
[ Sat Jul 13 19:30:30 2024 ] 	Batch(6100/6809) done. Loss: 0.0687  lr:0.000001
[ Sat Jul 13 19:30:48 2024 ] 	Batch(6200/6809) done. Loss: 0.0339  lr:0.000001
[ Sat Jul 13 19:31:05 2024 ] 	Batch(6300/6809) done. Loss: 0.0260  lr:0.000001
[ Sat Jul 13 19:31:24 2024 ] 	Batch(6400/6809) done. Loss: 0.0799  lr:0.000001
[ Sat Jul 13 19:31:42 2024 ] 
Training: Epoch [6/50], Step [6499], Loss: 0.15214069187641144, Training Accuracy: 97.57115384615385
[ Sat Jul 13 19:31:42 2024 ] 	Batch(6500/6809) done. Loss: 0.2523  lr:0.000001
[ Sat Jul 13 19:32:00 2024 ] 	Batch(6600/6809) done. Loss: 0.1147  lr:0.000001
[ Sat Jul 13 19:32:18 2024 ] 	Batch(6700/6809) done. Loss: 0.2085  lr:0.000001
[ Sat Jul 13 19:32:36 2024 ] 	Batch(6800/6809) done. Loss: 0.0649  lr:0.000001
[ Sat Jul 13 19:32:38 2024 ] 	Mean training loss: 0.0939.
[ Sat Jul 13 19:32:38 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 19:32:38 2024 ] Training epoch: 8
[ Sat Jul 13 19:32:38 2024 ] 	Batch(0/6809) done. Loss: 0.2400  lr:0.000001
[ Sat Jul 13 19:32:56 2024 ] 	Batch(100/6809) done. Loss: 0.0382  lr:0.000001
[ Sat Jul 13 19:33:15 2024 ] 	Batch(200/6809) done. Loss: 0.2049  lr:0.000001
[ Sat Jul 13 19:33:32 2024 ] 	Batch(300/6809) done. Loss: 0.0120  lr:0.000001
[ Sat Jul 13 19:33:50 2024 ] 	Batch(400/6809) done. Loss: 0.1015  lr:0.000001
[ Sat Jul 13 19:34:08 2024 ] 
Training: Epoch [7/50], Step [499], Loss: 0.018654845654964447, Training Accuracy: 97.65
[ Sat Jul 13 19:34:08 2024 ] 	Batch(500/6809) done. Loss: 0.2018  lr:0.000001
[ Sat Jul 13 19:34:26 2024 ] 	Batch(600/6809) done. Loss: 0.0786  lr:0.000001
[ Sat Jul 13 19:34:45 2024 ] 	Batch(700/6809) done. Loss: 0.0827  lr:0.000001
[ Sat Jul 13 19:35:03 2024 ] 	Batch(800/6809) done. Loss: 0.0535  lr:0.000001
[ Sat Jul 13 19:35:21 2024 ] 	Batch(900/6809) done. Loss: 0.0559  lr:0.000001
[ Sat Jul 13 19:35:40 2024 ] 
Training: Epoch [7/50], Step [999], Loss: 0.007866709493100643, Training Accuracy: 97.7625
[ Sat Jul 13 19:35:40 2024 ] 	Batch(1000/6809) done. Loss: 0.3206  lr:0.000001
[ Sat Jul 13 19:35:58 2024 ] 	Batch(1100/6809) done. Loss: 0.0144  lr:0.000001
[ Sat Jul 13 19:36:17 2024 ] 	Batch(1200/6809) done. Loss: 0.0968  lr:0.000001
[ Sat Jul 13 19:36:35 2024 ] 	Batch(1300/6809) done. Loss: 0.1074  lr:0.000001
[ Sat Jul 13 19:36:53 2024 ] 	Batch(1400/6809) done. Loss: 0.0509  lr:0.000001
[ Sat Jul 13 19:37:11 2024 ] 
Training: Epoch [7/50], Step [1499], Loss: 0.06299000978469849, Training Accuracy: 97.70833333333333
[ Sat Jul 13 19:37:11 2024 ] 	Batch(1500/6809) done. Loss: 0.2126  lr:0.000001
[ Sat Jul 13 19:37:30 2024 ] 	Batch(1600/6809) done. Loss: 0.0994  lr:0.000001
[ Sat Jul 13 19:37:48 2024 ] 	Batch(1700/6809) done. Loss: 0.0372  lr:0.000001
[ Sat Jul 13 19:38:06 2024 ] 	Batch(1800/6809) done. Loss: 0.0256  lr:0.000001
[ Sat Jul 13 19:38:24 2024 ] 	Batch(1900/6809) done. Loss: 0.0265  lr:0.000001
[ Sat Jul 13 19:38:41 2024 ] 
Training: Epoch [7/50], Step [1999], Loss: 0.2566602826118469, Training Accuracy: 97.675
[ Sat Jul 13 19:38:42 2024 ] 	Batch(2000/6809) done. Loss: 0.2840  lr:0.000001
[ Sat Jul 13 19:39:00 2024 ] 	Batch(2100/6809) done. Loss: 0.0834  lr:0.000001
[ Sat Jul 13 19:39:18 2024 ] 	Batch(2200/6809) done. Loss: 0.0034  lr:0.000001
[ Sat Jul 13 19:39:36 2024 ] 	Batch(2300/6809) done. Loss: 0.0074  lr:0.000001
[ Sat Jul 13 19:39:54 2024 ] 	Batch(2400/6809) done. Loss: 0.0539  lr:0.000001
[ Sat Jul 13 19:40:12 2024 ] 
Training: Epoch [7/50], Step [2499], Loss: 0.26048731803894043, Training Accuracy: 97.65
[ Sat Jul 13 19:40:12 2024 ] 	Batch(2500/6809) done. Loss: 0.0006  lr:0.000001
[ Sat Jul 13 19:40:30 2024 ] 	Batch(2600/6809) done. Loss: 0.0057  lr:0.000001
[ Sat Jul 13 19:40:48 2024 ] 	Batch(2700/6809) done. Loss: 0.0462  lr:0.000001
[ Sat Jul 13 19:41:06 2024 ] 	Batch(2800/6809) done. Loss: 0.0125  lr:0.000001
[ Sat Jul 13 19:41:24 2024 ] 	Batch(2900/6809) done. Loss: 0.0879  lr:0.000001
[ Sat Jul 13 19:41:42 2024 ] 
Training: Epoch [7/50], Step [2999], Loss: 0.009611903689801693, Training Accuracy: 97.57083333333333
[ Sat Jul 13 19:41:43 2024 ] 	Batch(3000/6809) done. Loss: 0.1940  lr:0.000001
[ Sat Jul 13 19:42:01 2024 ] 	Batch(3100/6809) done. Loss: 0.0098  lr:0.000001
[ Sat Jul 13 19:42:19 2024 ] 	Batch(3200/6809) done. Loss: 0.0411  lr:0.000001
[ Sat Jul 13 19:42:37 2024 ] 	Batch(3300/6809) done. Loss: 0.0749  lr:0.000001
[ Sat Jul 13 19:42:55 2024 ] 	Batch(3400/6809) done. Loss: 0.1731  lr:0.000001
[ Sat Jul 13 19:43:13 2024 ] 
Training: Epoch [7/50], Step [3499], Loss: 0.07673322409391403, Training Accuracy: 97.57142857142857
[ Sat Jul 13 19:43:13 2024 ] 	Batch(3500/6809) done. Loss: 0.0738  lr:0.000001
[ Sat Jul 13 19:43:31 2024 ] 	Batch(3600/6809) done. Loss: 0.0090  lr:0.000001
[ Sat Jul 13 19:43:49 2024 ] 	Batch(3700/6809) done. Loss: 0.0065  lr:0.000001
[ Sat Jul 13 19:44:07 2024 ] 	Batch(3800/6809) done. Loss: 0.1157  lr:0.000001
[ Sat Jul 13 19:44:25 2024 ] 	Batch(3900/6809) done. Loss: 0.0151  lr:0.000001
[ Sat Jul 13 19:44:43 2024 ] 
Training: Epoch [7/50], Step [3999], Loss: 0.15824055671691895, Training Accuracy: 97.59375
[ Sat Jul 13 19:44:44 2024 ] 	Batch(4000/6809) done. Loss: 0.0235  lr:0.000001
[ Sat Jul 13 19:45:02 2024 ] 	Batch(4100/6809) done. Loss: 0.0768  lr:0.000001
[ Sat Jul 13 19:45:20 2024 ] 	Batch(4200/6809) done. Loss: 0.0241  lr:0.000001
[ Sat Jul 13 19:45:38 2024 ] 	Batch(4300/6809) done. Loss: 0.0273  lr:0.000001
[ Sat Jul 13 19:45:56 2024 ] 	Batch(4400/6809) done. Loss: 0.0585  lr:0.000001
[ Sat Jul 13 19:46:14 2024 ] 
Training: Epoch [7/50], Step [4499], Loss: 0.08418477326631546, Training Accuracy: 97.63055555555556
[ Sat Jul 13 19:46:14 2024 ] 	Batch(4500/6809) done. Loss: 0.1524  lr:0.000001
[ Sat Jul 13 19:46:32 2024 ] 	Batch(4600/6809) done. Loss: 0.0397  lr:0.000001
[ Sat Jul 13 19:46:51 2024 ] 	Batch(4700/6809) done. Loss: 0.2686  lr:0.000001
[ Sat Jul 13 19:47:09 2024 ] 	Batch(4800/6809) done. Loss: 0.0320  lr:0.000001
[ Sat Jul 13 19:47:27 2024 ] 	Batch(4900/6809) done. Loss: 0.0841  lr:0.000001
[ Sat Jul 13 19:47:45 2024 ] 
Training: Epoch [7/50], Step [4999], Loss: 0.08708160370588303, Training Accuracy: 97.6075
[ Sat Jul 13 19:47:45 2024 ] 	Batch(5000/6809) done. Loss: 0.1449  lr:0.000001
[ Sat Jul 13 19:48:03 2024 ] 	Batch(5100/6809) done. Loss: 0.0226  lr:0.000001
[ Sat Jul 13 19:48:21 2024 ] 	Batch(5200/6809) done. Loss: 0.1693  lr:0.000001
[ Sat Jul 13 19:48:39 2024 ] 	Batch(5300/6809) done. Loss: 0.0252  lr:0.000001
[ Sat Jul 13 19:48:57 2024 ] 	Batch(5400/6809) done. Loss: 0.0424  lr:0.000001
[ Sat Jul 13 19:49:15 2024 ] 
Training: Epoch [7/50], Step [5499], Loss: 0.011774104088544846, Training Accuracy: 97.61136363636363
[ Sat Jul 13 19:49:15 2024 ] 	Batch(5500/6809) done. Loss: 0.1653  lr:0.000001
[ Sat Jul 13 19:49:33 2024 ] 	Batch(5600/6809) done. Loss: 0.0332  lr:0.000001
[ Sat Jul 13 19:49:51 2024 ] 	Batch(5700/6809) done. Loss: 0.0447  lr:0.000001
[ Sat Jul 13 19:50:09 2024 ] 	Batch(5800/6809) done. Loss: 0.1656  lr:0.000001
[ Sat Jul 13 19:50:28 2024 ] 	Batch(5900/6809) done. Loss: 0.0969  lr:0.000001
[ Sat Jul 13 19:50:46 2024 ] 
Training: Epoch [7/50], Step [5999], Loss: 0.009265652857720852, Training Accuracy: 97.56458333333333
[ Sat Jul 13 19:50:46 2024 ] 	Batch(6000/6809) done. Loss: 0.0314  lr:0.000001
[ Sat Jul 13 19:51:04 2024 ] 	Batch(6100/6809) done. Loss: 0.0072  lr:0.000001
[ Sat Jul 13 19:51:22 2024 ] 	Batch(6200/6809) done. Loss: 0.0137  lr:0.000001
[ Sat Jul 13 19:51:41 2024 ] 	Batch(6300/6809) done. Loss: 0.2600  lr:0.000001
[ Sat Jul 13 19:51:59 2024 ] 	Batch(6400/6809) done. Loss: 0.1701  lr:0.000001
[ Sat Jul 13 19:52:16 2024 ] 
Training: Epoch [7/50], Step [6499], Loss: 0.03313538059592247, Training Accuracy: 97.575
[ Sat Jul 13 19:52:17 2024 ] 	Batch(6500/6809) done. Loss: 0.0675  lr:0.000001
[ Sat Jul 13 19:52:35 2024 ] 	Batch(6600/6809) done. Loss: 0.0979  lr:0.000001
[ Sat Jul 13 19:52:53 2024 ] 	Batch(6700/6809) done. Loss: 0.0058  lr:0.000001
[ Sat Jul 13 19:53:11 2024 ] 	Batch(6800/6809) done. Loss: 0.0110  lr:0.000001
[ Sat Jul 13 19:53:12 2024 ] 	Mean training loss: 0.0959.
[ Sat Jul 13 19:53:12 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 19:53:12 2024 ] Training epoch: 9
[ Sat Jul 13 19:53:13 2024 ] 	Batch(0/6809) done. Loss: 0.0147  lr:0.000001
[ Sat Jul 13 19:53:31 2024 ] 	Batch(100/6809) done. Loss: 0.0906  lr:0.000001
[ Sat Jul 13 19:53:49 2024 ] 	Batch(200/6809) done. Loss: 0.0354  lr:0.000001
[ Sat Jul 13 19:54:07 2024 ] 	Batch(300/6809) done. Loss: 0.0730  lr:0.000001
[ Sat Jul 13 19:54:25 2024 ] 	Batch(400/6809) done. Loss: 0.0027  lr:0.000001
[ Sat Jul 13 19:54:43 2024 ] 
Training: Epoch [8/50], Step [499], Loss: 0.042563389986753464, Training Accuracy: 98.0
[ Sat Jul 13 19:54:43 2024 ] 	Batch(500/6809) done. Loss: 0.0761  lr:0.000001
[ Sat Jul 13 19:55:01 2024 ] 	Batch(600/6809) done. Loss: 0.0171  lr:0.000001
[ Sat Jul 13 19:55:19 2024 ] 	Batch(700/6809) done. Loss: 0.0173  lr:0.000001
[ Sat Jul 13 19:55:37 2024 ] 	Batch(800/6809) done. Loss: 0.0748  lr:0.000001
[ Sat Jul 13 19:55:55 2024 ] 	Batch(900/6809) done. Loss: 0.1436  lr:0.000001
[ Sat Jul 13 19:56:14 2024 ] 
Training: Epoch [8/50], Step [999], Loss: 0.03327605500817299, Training Accuracy: 97.85000000000001
[ Sat Jul 13 19:56:14 2024 ] 	Batch(1000/6809) done. Loss: 0.0983  lr:0.000001
[ Sat Jul 13 19:56:32 2024 ] 	Batch(1100/6809) done. Loss: 0.0042  lr:0.000001
[ Sat Jul 13 19:56:51 2024 ] 	Batch(1200/6809) done. Loss: 0.0924  lr:0.000001
[ Sat Jul 13 19:57:09 2024 ] 	Batch(1300/6809) done. Loss: 0.0209  lr:0.000001
[ Sat Jul 13 19:57:27 2024 ] 	Batch(1400/6809) done. Loss: 0.0848  lr:0.000001
[ Sat Jul 13 19:57:45 2024 ] 
Training: Epoch [8/50], Step [1499], Loss: 0.06567452847957611, Training Accuracy: 97.70833333333333
[ Sat Jul 13 19:57:45 2024 ] 	Batch(1500/6809) done. Loss: 0.0318  lr:0.000001
[ Sat Jul 13 19:58:03 2024 ] 	Batch(1600/6809) done. Loss: 0.1057  lr:0.000001
[ Sat Jul 13 19:58:21 2024 ] 	Batch(1700/6809) done. Loss: 0.3027  lr:0.000001
[ Sat Jul 13 19:58:39 2024 ] 	Batch(1800/6809) done. Loss: 0.3363  lr:0.000001
[ Sat Jul 13 19:58:57 2024 ] 	Batch(1900/6809) done. Loss: 0.1017  lr:0.000001
[ Sat Jul 13 19:59:15 2024 ] 
Training: Epoch [8/50], Step [1999], Loss: 0.2045697271823883, Training Accuracy: 97.675
[ Sat Jul 13 19:59:15 2024 ] 	Batch(2000/6809) done. Loss: 0.4486  lr:0.000001
[ Sat Jul 13 19:59:33 2024 ] 	Batch(2100/6809) done. Loss: 0.0186  lr:0.000001
[ Sat Jul 13 19:59:51 2024 ] 	Batch(2200/6809) done. Loss: 0.0811  lr:0.000001
[ Sat Jul 13 20:00:09 2024 ] 	Batch(2300/6809) done. Loss: 0.0740  lr:0.000001
[ Sat Jul 13 20:00:27 2024 ] 	Batch(2400/6809) done. Loss: 0.0353  lr:0.000001
[ Sat Jul 13 20:00:45 2024 ] 
Training: Epoch [8/50], Step [2499], Loss: 0.12953972816467285, Training Accuracy: 97.68
[ Sat Jul 13 20:00:45 2024 ] 	Batch(2500/6809) done. Loss: 0.2354  lr:0.000001
[ Sat Jul 13 20:01:03 2024 ] 	Batch(2600/6809) done. Loss: 0.1531  lr:0.000001
[ Sat Jul 13 20:01:20 2024 ] 	Batch(2700/6809) done. Loss: 0.0643  lr:0.000001
[ Sat Jul 13 20:01:38 2024 ] 	Batch(2800/6809) done. Loss: 0.2607  lr:0.000001
[ Sat Jul 13 20:01:56 2024 ] 	Batch(2900/6809) done. Loss: 0.0490  lr:0.000001
[ Sat Jul 13 20:02:14 2024 ] 
Training: Epoch [8/50], Step [2999], Loss: 0.002688384149223566, Training Accuracy: 97.64166666666667
[ Sat Jul 13 20:02:14 2024 ] 	Batch(3000/6809) done. Loss: 0.0156  lr:0.000001
[ Sat Jul 13 20:02:32 2024 ] 	Batch(3100/6809) done. Loss: 0.0399  lr:0.000001
[ Sat Jul 13 20:02:50 2024 ] 	Batch(3200/6809) done. Loss: 0.0270  lr:0.000001
[ Sat Jul 13 20:03:08 2024 ] 	Batch(3300/6809) done. Loss: 0.0314  lr:0.000001
[ Sat Jul 13 20:03:26 2024 ] 	Batch(3400/6809) done. Loss: 0.1337  lr:0.000001
[ Sat Jul 13 20:03:44 2024 ] 
Training: Epoch [8/50], Step [3499], Loss: 0.021695226430892944, Training Accuracy: 97.64642857142857
[ Sat Jul 13 20:03:44 2024 ] 	Batch(3500/6809) done. Loss: 0.0409  lr:0.000001
[ Sat Jul 13 20:04:02 2024 ] 	Batch(3600/6809) done. Loss: 0.0026  lr:0.000001
[ Sat Jul 13 20:04:21 2024 ] 	Batch(3700/6809) done. Loss: 0.0240  lr:0.000001
[ Sat Jul 13 20:04:39 2024 ] 	Batch(3800/6809) done. Loss: 0.0604  lr:0.000001
[ Sat Jul 13 20:04:57 2024 ] 	Batch(3900/6809) done. Loss: 0.0372  lr:0.000001
[ Sat Jul 13 20:05:15 2024 ] 
Training: Epoch [8/50], Step [3999], Loss: 0.028528574854135513, Training Accuracy: 97.675
[ Sat Jul 13 20:05:15 2024 ] 	Batch(4000/6809) done. Loss: 0.0362  lr:0.000001
[ Sat Jul 13 20:05:33 2024 ] 	Batch(4100/6809) done. Loss: 0.0090  lr:0.000001
[ Sat Jul 13 20:05:51 2024 ] 	Batch(4200/6809) done. Loss: 0.0898  lr:0.000001
[ Sat Jul 13 20:06:09 2024 ] 	Batch(4300/6809) done. Loss: 0.0223  lr:0.000001
[ Sat Jul 13 20:06:27 2024 ] 	Batch(4400/6809) done. Loss: 0.0266  lr:0.000001
[ Sat Jul 13 20:06:45 2024 ] 
Training: Epoch [8/50], Step [4499], Loss: 0.12937802076339722, Training Accuracy: 97.67777777777778
[ Sat Jul 13 20:06:45 2024 ] 	Batch(4500/6809) done. Loss: 0.1320  lr:0.000001
[ Sat Jul 13 20:07:03 2024 ] 	Batch(4600/6809) done. Loss: 0.2953  lr:0.000001
[ Sat Jul 13 20:07:21 2024 ] 	Batch(4700/6809) done. Loss: 0.4097  lr:0.000001
[ Sat Jul 13 20:07:39 2024 ] 	Batch(4800/6809) done. Loss: 0.0625  lr:0.000001
[ Sat Jul 13 20:07:57 2024 ] 	Batch(4900/6809) done. Loss: 0.3417  lr:0.000001
[ Sat Jul 13 20:08:16 2024 ] 
Training: Epoch [8/50], Step [4999], Loss: 0.3393126428127289, Training Accuracy: 97.6775
[ Sat Jul 13 20:08:16 2024 ] 	Batch(5000/6809) done. Loss: 0.0275  lr:0.000001
[ Sat Jul 13 20:08:34 2024 ] 	Batch(5100/6809) done. Loss: 0.0224  lr:0.000001
[ Sat Jul 13 20:08:53 2024 ] 	Batch(5200/6809) done. Loss: 0.0141  lr:0.000001
[ Sat Jul 13 20:09:11 2024 ] 	Batch(5300/6809) done. Loss: 0.0098  lr:0.000001
[ Sat Jul 13 20:09:29 2024 ] 	Batch(5400/6809) done. Loss: 0.1582  lr:0.000001
[ Sat Jul 13 20:09:46 2024 ] 
Training: Epoch [8/50], Step [5499], Loss: 0.03454504534602165, Training Accuracy: 97.66818181818182
[ Sat Jul 13 20:09:47 2024 ] 	Batch(5500/6809) done. Loss: 0.0526  lr:0.000001
[ Sat Jul 13 20:10:05 2024 ] 	Batch(5600/6809) done. Loss: 0.0415  lr:0.000001
[ Sat Jul 13 20:10:23 2024 ] 	Batch(5700/6809) done. Loss: 0.0351  lr:0.000001
[ Sat Jul 13 20:10:40 2024 ] 	Batch(5800/6809) done. Loss: 0.2573  lr:0.000001
[ Sat Jul 13 20:10:58 2024 ] 	Batch(5900/6809) done. Loss: 0.2364  lr:0.000001
[ Sat Jul 13 20:11:16 2024 ] 
Training: Epoch [8/50], Step [5999], Loss: 0.02331889234483242, Training Accuracy: 97.6625
[ Sat Jul 13 20:11:16 2024 ] 	Batch(6000/6809) done. Loss: 0.1304  lr:0.000001
[ Sat Jul 13 20:11:35 2024 ] 	Batch(6100/6809) done. Loss: 0.0843  lr:0.000001
[ Sat Jul 13 20:11:53 2024 ] 	Batch(6200/6809) done. Loss: 0.0190  lr:0.000001
[ Sat Jul 13 20:12:12 2024 ] 	Batch(6300/6809) done. Loss: 0.0347  lr:0.000001
[ Sat Jul 13 20:12:31 2024 ] 	Batch(6400/6809) done. Loss: 0.0300  lr:0.000001
[ Sat Jul 13 20:12:48 2024 ] 
Training: Epoch [8/50], Step [6499], Loss: 0.10713299363851547, Training Accuracy: 97.67115384615384
[ Sat Jul 13 20:12:49 2024 ] 	Batch(6500/6809) done. Loss: 0.0764  lr:0.000001
[ Sat Jul 13 20:13:06 2024 ] 	Batch(6600/6809) done. Loss: 0.1129  lr:0.000001
[ Sat Jul 13 20:13:24 2024 ] 	Batch(6700/6809) done. Loss: 0.0523  lr:0.000001
[ Sat Jul 13 20:13:42 2024 ] 	Batch(6800/6809) done. Loss: 0.0697  lr:0.000001
[ Sat Jul 13 20:13:44 2024 ] 	Mean training loss: 0.0922.
[ Sat Jul 13 20:13:44 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 20:13:44 2024 ] Training epoch: 10
[ Sat Jul 13 20:13:45 2024 ] 	Batch(0/6809) done. Loss: 0.0134  lr:0.000001
[ Sat Jul 13 20:14:03 2024 ] 	Batch(100/6809) done. Loss: 0.0107  lr:0.000001
[ Sat Jul 13 20:14:22 2024 ] 	Batch(200/6809) done. Loss: 0.2450  lr:0.000001
[ Sat Jul 13 20:14:40 2024 ] 	Batch(300/6809) done. Loss: 0.1471  lr:0.000001
[ Sat Jul 13 20:14:59 2024 ] 	Batch(400/6809) done. Loss: 0.0025  lr:0.000001
[ Sat Jul 13 20:15:17 2024 ] 
Training: Epoch [9/50], Step [499], Loss: 0.0050791422836482525, Training Accuracy: 97.875
[ Sat Jul 13 20:15:17 2024 ] 	Batch(500/6809) done. Loss: 0.3620  lr:0.000001
[ Sat Jul 13 20:15:36 2024 ] 	Batch(600/6809) done. Loss: 0.1145  lr:0.000001
[ Sat Jul 13 20:15:54 2024 ] 	Batch(700/6809) done. Loss: 0.0580  lr:0.000001
[ Sat Jul 13 20:16:12 2024 ] 	Batch(800/6809) done. Loss: 0.0511  lr:0.000001
[ Sat Jul 13 20:16:30 2024 ] 	Batch(900/6809) done. Loss: 0.0130  lr:0.000001
[ Sat Jul 13 20:16:47 2024 ] 
Training: Epoch [9/50], Step [999], Loss: 0.05298978090286255, Training Accuracy: 97.925
[ Sat Jul 13 20:16:48 2024 ] 	Batch(1000/6809) done. Loss: 0.0299  lr:0.000001
[ Sat Jul 13 20:17:05 2024 ] 	Batch(1100/6809) done. Loss: 0.1034  lr:0.000001
[ Sat Jul 13 20:17:23 2024 ] 	Batch(1200/6809) done. Loss: 0.0171  lr:0.000001
[ Sat Jul 13 20:17:42 2024 ] 	Batch(1300/6809) done. Loss: 0.0276  lr:0.000001
[ Sat Jul 13 20:18:00 2024 ] 	Batch(1400/6809) done. Loss: 0.0750  lr:0.000001
[ Sat Jul 13 20:18:19 2024 ] 
Training: Epoch [9/50], Step [1499], Loss: 0.012554666958749294, Training Accuracy: 97.69166666666666
[ Sat Jul 13 20:18:19 2024 ] 	Batch(1500/6809) done. Loss: 0.1034  lr:0.000001
[ Sat Jul 13 20:18:37 2024 ] 	Batch(1600/6809) done. Loss: 0.0221  lr:0.000001
[ Sat Jul 13 20:18:55 2024 ] 	Batch(1700/6809) done. Loss: 0.1946  lr:0.000001
[ Sat Jul 13 20:19:13 2024 ] 	Batch(1800/6809) done. Loss: 0.0795  lr:0.000001
[ Sat Jul 13 20:19:31 2024 ] 	Batch(1900/6809) done. Loss: 0.0618  lr:0.000001
[ Sat Jul 13 20:19:49 2024 ] 
Training: Epoch [9/50], Step [1999], Loss: 0.06385216116905212, Training Accuracy: 97.63125000000001
[ Sat Jul 13 20:19:49 2024 ] 	Batch(2000/6809) done. Loss: 0.8922  lr:0.000001
[ Sat Jul 13 20:20:08 2024 ] 	Batch(2100/6809) done. Loss: 0.0698  lr:0.000001
[ Sat Jul 13 20:20:26 2024 ] 	Batch(2200/6809) done. Loss: 0.3295  lr:0.000001
[ Sat Jul 13 20:20:44 2024 ] 	Batch(2300/6809) done. Loss: 0.1233  lr:0.000001
[ Sat Jul 13 20:21:02 2024 ] 	Batch(2400/6809) done. Loss: 0.1460  lr:0.000001
[ Sat Jul 13 20:21:21 2024 ] 
Training: Epoch [9/50], Step [2499], Loss: 0.003633299609646201, Training Accuracy: 97.64500000000001
[ Sat Jul 13 20:21:21 2024 ] 	Batch(2500/6809) done. Loss: 0.0101  lr:0.000001
[ Sat Jul 13 20:21:39 2024 ] 	Batch(2600/6809) done. Loss: 0.3263  lr:0.000001
[ Sat Jul 13 20:21:58 2024 ] 	Batch(2700/6809) done. Loss: 0.0854  lr:0.000001
[ Sat Jul 13 20:22:17 2024 ] 	Batch(2800/6809) done. Loss: 0.0220  lr:0.000001
[ Sat Jul 13 20:22:35 2024 ] 	Batch(2900/6809) done. Loss: 0.1291  lr:0.000001
[ Sat Jul 13 20:22:54 2024 ] 
Training: Epoch [9/50], Step [2999], Loss: 0.03851766139268875, Training Accuracy: 97.64583333333333
[ Sat Jul 13 20:22:54 2024 ] 	Batch(3000/6809) done. Loss: 0.0301  lr:0.000001
[ Sat Jul 13 20:23:13 2024 ] 	Batch(3100/6809) done. Loss: 0.0097  lr:0.000001
[ Sat Jul 13 20:23:32 2024 ] 	Batch(3200/6809) done. Loss: 0.0988  lr:0.000001
[ Sat Jul 13 20:23:50 2024 ] 	Batch(3300/6809) done. Loss: 0.0545  lr:0.000001
[ Sat Jul 13 20:24:08 2024 ] 	Batch(3400/6809) done. Loss: 0.0416  lr:0.000001
[ Sat Jul 13 20:24:26 2024 ] 
Training: Epoch [9/50], Step [3499], Loss: 0.02689952217042446, Training Accuracy: 97.66785714285714
[ Sat Jul 13 20:24:26 2024 ] 	Batch(3500/6809) done. Loss: 0.0282  lr:0.000001
[ Sat Jul 13 20:24:44 2024 ] 	Batch(3600/6809) done. Loss: 0.0273  lr:0.000001
[ Sat Jul 13 20:25:02 2024 ] 	Batch(3700/6809) done. Loss: 0.0211  lr:0.000001
[ Sat Jul 13 20:25:20 2024 ] 	Batch(3800/6809) done. Loss: 0.0893  lr:0.000001
[ Sat Jul 13 20:25:38 2024 ] 	Batch(3900/6809) done. Loss: 0.0792  lr:0.000001
[ Sat Jul 13 20:25:55 2024 ] 
Training: Epoch [9/50], Step [3999], Loss: 0.06244666129350662, Training Accuracy: 97.67812500000001
[ Sat Jul 13 20:25:56 2024 ] 	Batch(4000/6809) done. Loss: 0.0431  lr:0.000001
[ Sat Jul 13 20:26:14 2024 ] 	Batch(4100/6809) done. Loss: 0.3577  lr:0.000001
[ Sat Jul 13 20:26:33 2024 ] 	Batch(4200/6809) done. Loss: 0.1557  lr:0.000001
[ Sat Jul 13 20:26:51 2024 ] 	Batch(4300/6809) done. Loss: 0.0030  lr:0.000001
[ Sat Jul 13 20:27:10 2024 ] 	Batch(4400/6809) done. Loss: 0.0367  lr:0.000001
[ Sat Jul 13 20:27:28 2024 ] 
Training: Epoch [9/50], Step [4499], Loss: 0.11275867372751236, Training Accuracy: 97.63055555555556
[ Sat Jul 13 20:27:28 2024 ] 	Batch(4500/6809) done. Loss: 0.0141  lr:0.000001
[ Sat Jul 13 20:27:46 2024 ] 	Batch(4600/6809) done. Loss: 0.1301  lr:0.000001
[ Sat Jul 13 20:28:04 2024 ] 	Batch(4700/6809) done. Loss: 0.0360  lr:0.000001
[ Sat Jul 13 20:28:22 2024 ] 	Batch(4800/6809) done. Loss: 0.0358  lr:0.000001
[ Sat Jul 13 20:28:40 2024 ] 	Batch(4900/6809) done. Loss: 0.0617  lr:0.000001
[ Sat Jul 13 20:28:57 2024 ] 
Training: Epoch [9/50], Step [4999], Loss: 0.0066566611640155315, Training Accuracy: 97.61
[ Sat Jul 13 20:28:58 2024 ] 	Batch(5000/6809) done. Loss: 0.5131  lr:0.000001
[ Sat Jul 13 20:29:15 2024 ] 	Batch(5100/6809) done. Loss: 0.0104  lr:0.000001
[ Sat Jul 13 20:29:33 2024 ] 	Batch(5200/6809) done. Loss: 0.0045  lr:0.000001
[ Sat Jul 13 20:29:51 2024 ] 	Batch(5300/6809) done. Loss: 0.1558  lr:0.000001
[ Sat Jul 13 20:30:09 2024 ] 	Batch(5400/6809) done. Loss: 0.0536  lr:0.000001
[ Sat Jul 13 20:30:27 2024 ] 
Training: Epoch [9/50], Step [5499], Loss: 0.02922549471259117, Training Accuracy: 97.58181818181818
[ Sat Jul 13 20:30:27 2024 ] 	Batch(5500/6809) done. Loss: 0.1069  lr:0.000001
[ Sat Jul 13 20:30:45 2024 ] 	Batch(5600/6809) done. Loss: 0.0567  lr:0.000001
[ Sat Jul 13 20:31:03 2024 ] 	Batch(5700/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 20:31:21 2024 ] 	Batch(5800/6809) done. Loss: 0.0569  lr:0.000001
[ Sat Jul 13 20:31:39 2024 ] 	Batch(5900/6809) done. Loss: 0.0360  lr:0.000001
[ Sat Jul 13 20:31:56 2024 ] 
Training: Epoch [9/50], Step [5999], Loss: 0.008916596882045269, Training Accuracy: 97.59791666666666
[ Sat Jul 13 20:31:57 2024 ] 	Batch(6000/6809) done. Loss: 0.0967  lr:0.000001
[ Sat Jul 13 20:32:15 2024 ] 	Batch(6100/6809) done. Loss: 0.1920  lr:0.000001
[ Sat Jul 13 20:32:33 2024 ] 	Batch(6200/6809) done. Loss: 0.4272  lr:0.000001
[ Sat Jul 13 20:32:51 2024 ] 	Batch(6300/6809) done. Loss: 0.0063  lr:0.000001
[ Sat Jul 13 20:33:09 2024 ] 	Batch(6400/6809) done. Loss: 0.0662  lr:0.000001
[ Sat Jul 13 20:33:27 2024 ] 
Training: Epoch [9/50], Step [6499], Loss: 0.19180677831172943, Training Accuracy: 97.61538461538461
[ Sat Jul 13 20:33:27 2024 ] 	Batch(6500/6809) done. Loss: 0.0285  lr:0.000001
[ Sat Jul 13 20:33:45 2024 ] 	Batch(6600/6809) done. Loss: 0.0769  lr:0.000001
[ Sat Jul 13 20:34:03 2024 ] 	Batch(6700/6809) done. Loss: 0.0830  lr:0.000001
[ Sat Jul 13 20:34:21 2024 ] 	Batch(6800/6809) done. Loss: 0.1028  lr:0.000001
[ Sat Jul 13 20:34:22 2024 ] 	Mean training loss: 0.0978.
[ Sat Jul 13 20:34:22 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 20:34:22 2024 ] Eval epoch: 10
[ Sat Jul 13 20:39:56 2024 ] 	Mean val loss of 7435 batches: 1.0769868900680886.
[ Sat Jul 13 20:39:56 2024 ] 
Validation: Epoch [9/50], Samples [47778.0/59477], Loss: 0.7047890424728394, Validation Accuracy: 80.33021167846395
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 1 : 374 / 500 = 74 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 2 : 427 / 499 = 85 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 3 : 407 / 500 = 81 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 4 : 423 / 502 = 84 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 5 : 467 / 502 = 93 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 6 : 431 / 502 = 85 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 7 : 466 / 497 = 93 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 8 : 483 / 498 = 96 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 9 : 388 / 500 = 77 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 10 : 203 / 500 = 40 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 11 : 185 / 498 = 37 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 12 : 414 / 499 = 82 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 13 : 484 / 502 = 96 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 14 : 480 / 504 = 95 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 15 : 422 / 502 = 84 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 16 : 393 / 502 = 78 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 17 : 437 / 504 = 86 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 18 : 420 / 504 = 83 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 19 : 464 / 502 = 92 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 20 : 461 / 502 = 91 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 21 : 475 / 503 = 94 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 22 : 440 / 504 = 87 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 23 : 445 / 503 = 88 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 24 : 403 / 504 = 79 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 25 : 489 / 504 = 97 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 26 : 473 / 504 = 93 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 27 : 411 / 501 = 82 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 28 : 342 / 502 = 68 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 29 : 326 / 502 = 64 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 30 : 335 / 501 = 66 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 31 : 420 / 504 = 83 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 32 : 425 / 503 = 84 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 33 : 420 / 503 = 83 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 34 : 479 / 504 = 95 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 35 : 458 / 503 = 91 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 36 : 394 / 502 = 78 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 37 : 445 / 504 = 88 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 38 : 427 / 504 = 84 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 39 : 457 / 498 = 91 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 40 : 378 / 504 = 75 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 41 : 475 / 503 = 94 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 42 : 463 / 504 = 91 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 43 : 344 / 503 = 68 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 44 : 450 / 504 = 89 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 45 : 426 / 504 = 84 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 46 : 420 / 504 = 83 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 47 : 424 / 503 = 84 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 48 : 439 / 503 = 87 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 49 : 383 / 499 = 76 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 50 : 428 / 502 = 85 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 51 : 471 / 503 = 93 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 52 : 433 / 504 = 85 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 53 : 430 / 497 = 86 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 54 : 459 / 480 = 95 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 55 : 386 / 504 = 76 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 56 : 408 / 503 = 81 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 57 : 486 / 504 = 96 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 58 : 480 / 499 = 96 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 59 : 491 / 503 = 97 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 60 : 417 / 479 = 87 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 61 : 419 / 484 = 86 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 62 : 395 / 487 = 81 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 63 : 451 / 489 = 92 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 64 : 374 / 488 = 76 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 65 : 458 / 490 = 93 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 66 : 315 / 488 = 64 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 67 : 370 / 490 = 75 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 68 : 284 / 490 = 57 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 69 : 365 / 490 = 74 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 70 : 234 / 490 = 47 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 71 : 198 / 490 = 40 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 72 : 202 / 488 = 41 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 73 : 287 / 486 = 59 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 74 : 280 / 481 = 58 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 75 : 281 / 488 = 57 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 76 : 323 / 489 = 66 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 77 : 312 / 488 = 63 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 78 : 359 / 488 = 73 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 79 : 450 / 490 = 91 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 80 : 395 / 489 = 80 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 81 : 315 / 491 = 64 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 82 : 321 / 491 = 65 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 83 : 251 / 489 = 51 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 84 : 380 / 489 = 77 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 85 : 385 / 489 = 78 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 86 : 437 / 491 = 89 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 87 : 440 / 492 = 89 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 88 : 375 / 491 = 76 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 89 : 401 / 492 = 81 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 90 : 255 / 490 = 52 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 91 : 398 / 482 = 82 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 92 : 374 / 490 = 76 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 93 : 368 / 487 = 75 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 94 : 418 / 489 = 85 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 95 : 417 / 490 = 85 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 96 : 467 / 491 = 95 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 97 : 465 / 490 = 94 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 98 : 444 / 491 = 90 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 99 : 446 / 491 = 90 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 100 : 450 / 491 = 91 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 101 : 435 / 491 = 88 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 102 : 300 / 492 = 60 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 103 : 403 / 492 = 81 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 104 : 313 / 491 = 63 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 105 : 269 / 491 = 54 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 106 : 282 / 492 = 57 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 107 : 410 / 491 = 83 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 108 : 392 / 492 = 79 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 109 : 315 / 490 = 64 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 110 : 410 / 491 = 83 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 111 : 448 / 492 = 91 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 112 : 457 / 492 = 92 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 113 : 438 / 491 = 89 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 114 : 406 / 491 = 82 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 115 : 422 / 492 = 85 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 116 : 403 / 491 = 82 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 117 : 429 / 492 = 87 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 118 : 439 / 490 = 89 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 119 : 444 / 492 = 90 %
[ Sat Jul 13 20:39:56 2024 ] Accuracy of 120 : 420 / 500 = 84 %
[ Sat Jul 13 20:39:56 2024 ] Training epoch: 11
[ Sat Jul 13 20:39:56 2024 ] 	Batch(0/6809) done. Loss: 0.0082  lr:0.000001
[ Sat Jul 13 20:40:14 2024 ] 	Batch(100/6809) done. Loss: 0.0218  lr:0.000001
[ Sat Jul 13 20:40:32 2024 ] 	Batch(200/6809) done. Loss: 0.2191  lr:0.000001
[ Sat Jul 13 20:40:50 2024 ] 	Batch(300/6809) done. Loss: 0.0302  lr:0.000001
[ Sat Jul 13 20:41:08 2024 ] 	Batch(400/6809) done. Loss: 0.0180  lr:0.000001
[ Sat Jul 13 20:41:26 2024 ] 
Training: Epoch [10/50], Step [499], Loss: 0.06796088069677353, Training Accuracy: 97.8
[ Sat Jul 13 20:41:26 2024 ] 	Batch(500/6809) done. Loss: 0.0610  lr:0.000001
[ Sat Jul 13 20:41:44 2024 ] 	Batch(600/6809) done. Loss: 0.2998  lr:0.000001
[ Sat Jul 13 20:42:02 2024 ] 	Batch(700/6809) done. Loss: 0.3773  lr:0.000001
[ Sat Jul 13 20:42:20 2024 ] 	Batch(800/6809) done. Loss: 0.1364  lr:0.000001
[ Sat Jul 13 20:42:39 2024 ] 	Batch(900/6809) done. Loss: 0.2464  lr:0.000001
[ Sat Jul 13 20:42:57 2024 ] 
Training: Epoch [10/50], Step [999], Loss: 0.12827147543430328, Training Accuracy: 97.6875
[ Sat Jul 13 20:42:57 2024 ] 	Batch(1000/6809) done. Loss: 0.0097  lr:0.000001
[ Sat Jul 13 20:43:15 2024 ] 	Batch(1100/6809) done. Loss: 0.0869  lr:0.000001
[ Sat Jul 13 20:43:33 2024 ] 	Batch(1200/6809) done. Loss: 0.0067  lr:0.000001
[ Sat Jul 13 20:43:51 2024 ] 	Batch(1300/6809) done. Loss: 0.0542  lr:0.000001
[ Sat Jul 13 20:44:09 2024 ] 	Batch(1400/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 20:44:27 2024 ] 
Training: Epoch [10/50], Step [1499], Loss: 0.021387619897723198, Training Accuracy: 97.64166666666667
[ Sat Jul 13 20:44:27 2024 ] 	Batch(1500/6809) done. Loss: 0.0088  lr:0.000001
[ Sat Jul 13 20:44:45 2024 ] 	Batch(1600/6809) done. Loss: 0.1877  lr:0.000001
[ Sat Jul 13 20:45:03 2024 ] 	Batch(1700/6809) done. Loss: 0.0064  lr:0.000001
[ Sat Jul 13 20:45:21 2024 ] 	Batch(1800/6809) done. Loss: 0.0098  lr:0.000001
[ Sat Jul 13 20:45:39 2024 ] 	Batch(1900/6809) done. Loss: 0.0773  lr:0.000001
[ Sat Jul 13 20:45:56 2024 ] 
Training: Epoch [10/50], Step [1999], Loss: 0.016800258308649063, Training Accuracy: 97.60625
[ Sat Jul 13 20:45:57 2024 ] 	Batch(2000/6809) done. Loss: 0.0915  lr:0.000001
[ Sat Jul 13 20:46:15 2024 ] 	Batch(2100/6809) done. Loss: 0.2645  lr:0.000001
[ Sat Jul 13 20:46:33 2024 ] 	Batch(2200/6809) done. Loss: 0.0305  lr:0.000001
[ Sat Jul 13 20:46:51 2024 ] 	Batch(2300/6809) done. Loss: 0.0839  lr:0.000001
[ Sat Jul 13 20:47:09 2024 ] 	Batch(2400/6809) done. Loss: 0.0473  lr:0.000001
[ Sat Jul 13 20:47:27 2024 ] 
Training: Epoch [10/50], Step [2499], Loss: 0.04347400367259979, Training Accuracy: 97.515
[ Sat Jul 13 20:47:27 2024 ] 	Batch(2500/6809) done. Loss: 0.0098  lr:0.000001
[ Sat Jul 13 20:47:46 2024 ] 	Batch(2600/6809) done. Loss: 0.0660  lr:0.000001
[ Sat Jul 13 20:48:04 2024 ] 	Batch(2700/6809) done. Loss: 0.0278  lr:0.000001
[ Sat Jul 13 20:48:22 2024 ] 	Batch(2800/6809) done. Loss: 0.0685  lr:0.000001
[ Sat Jul 13 20:48:40 2024 ] 	Batch(2900/6809) done. Loss: 0.0014  lr:0.000001
[ Sat Jul 13 20:48:57 2024 ] 
Training: Epoch [10/50], Step [2999], Loss: 0.04044153913855553, Training Accuracy: 97.56666666666666
[ Sat Jul 13 20:48:58 2024 ] 	Batch(3000/6809) done. Loss: 0.0914  lr:0.000001
[ Sat Jul 13 20:49:15 2024 ] 	Batch(3100/6809) done. Loss: 0.2818  lr:0.000001
[ Sat Jul 13 20:49:33 2024 ] 	Batch(3200/6809) done. Loss: 0.0631  lr:0.000001
[ Sat Jul 13 20:49:51 2024 ] 	Batch(3300/6809) done. Loss: 0.0324  lr:0.000001
[ Sat Jul 13 20:50:09 2024 ] 	Batch(3400/6809) done. Loss: 0.0410  lr:0.000001
[ Sat Jul 13 20:50:27 2024 ] 
Training: Epoch [10/50], Step [3499], Loss: 0.037145569920539856, Training Accuracy: 97.58571428571429
[ Sat Jul 13 20:50:27 2024 ] 	Batch(3500/6809) done. Loss: 0.1318  lr:0.000001
[ Sat Jul 13 20:50:45 2024 ] 	Batch(3600/6809) done. Loss: 0.0093  lr:0.000001
[ Sat Jul 13 20:51:03 2024 ] 	Batch(3700/6809) done. Loss: 0.1412  lr:0.000001
[ Sat Jul 13 20:51:21 2024 ] 	Batch(3800/6809) done. Loss: 0.1412  lr:0.000001
[ Sat Jul 13 20:51:39 2024 ] 	Batch(3900/6809) done. Loss: 0.0071  lr:0.000001
[ Sat Jul 13 20:51:56 2024 ] 
Training: Epoch [10/50], Step [3999], Loss: 0.017486179247498512, Training Accuracy: 97.575
[ Sat Jul 13 20:51:57 2024 ] 	Batch(4000/6809) done. Loss: 0.0520  lr:0.000001
[ Sat Jul 13 20:52:15 2024 ] 	Batch(4100/6809) done. Loss: 0.0534  lr:0.000001
[ Sat Jul 13 20:52:33 2024 ] 	Batch(4200/6809) done. Loss: 0.1401  lr:0.000001
[ Sat Jul 13 20:52:50 2024 ] 	Batch(4300/6809) done. Loss: 0.0438  lr:0.000001
[ Sat Jul 13 20:53:08 2024 ] 	Batch(4400/6809) done. Loss: 0.1124  lr:0.000001
[ Sat Jul 13 20:53:27 2024 ] 
Training: Epoch [10/50], Step [4499], Loss: 0.3227146565914154, Training Accuracy: 97.58611111111111
[ Sat Jul 13 20:53:27 2024 ] 	Batch(4500/6809) done. Loss: 0.1676  lr:0.000001
[ Sat Jul 13 20:53:46 2024 ] 	Batch(4600/6809) done. Loss: 0.0562  lr:0.000001
[ Sat Jul 13 20:54:04 2024 ] 	Batch(4700/6809) done. Loss: 0.0169  lr:0.000001
[ Sat Jul 13 20:54:23 2024 ] 	Batch(4800/6809) done. Loss: 0.0430  lr:0.000001
[ Sat Jul 13 20:54:41 2024 ] 	Batch(4900/6809) done. Loss: 0.0340  lr:0.000001
[ Sat Jul 13 20:54:59 2024 ] 
Training: Epoch [10/50], Step [4999], Loss: 0.01789146289229393, Training Accuracy: 97.5975
[ Sat Jul 13 20:54:59 2024 ] 	Batch(5000/6809) done. Loss: 0.0152  lr:0.000001
[ Sat Jul 13 20:55:17 2024 ] 	Batch(5100/6809) done. Loss: 0.2548  lr:0.000001
[ Sat Jul 13 20:55:35 2024 ] 	Batch(5200/6809) done. Loss: 0.0666  lr:0.000001
[ Sat Jul 13 20:55:53 2024 ] 	Batch(5300/6809) done. Loss: 0.2976  lr:0.000001
[ Sat Jul 13 20:56:11 2024 ] 	Batch(5400/6809) done. Loss: 0.0188  lr:0.000001
[ Sat Jul 13 20:56:29 2024 ] 
Training: Epoch [10/50], Step [5499], Loss: 0.013541709631681442, Training Accuracy: 97.62727272727273
[ Sat Jul 13 20:56:29 2024 ] 	Batch(5500/6809) done. Loss: 0.0504  lr:0.000001
[ Sat Jul 13 20:56:47 2024 ] 	Batch(5600/6809) done. Loss: 0.0070  lr:0.000001
[ Sat Jul 13 20:57:05 2024 ] 	Batch(5700/6809) done. Loss: 0.0719  lr:0.000001
[ Sat Jul 13 20:57:24 2024 ] 	Batch(5800/6809) done. Loss: 0.0373  lr:0.000001
[ Sat Jul 13 20:57:42 2024 ] 	Batch(5900/6809) done. Loss: 0.1498  lr:0.000001
[ Sat Jul 13 20:58:01 2024 ] 
Training: Epoch [10/50], Step [5999], Loss: 0.0232600886374712, Training Accuracy: 97.6
[ Sat Jul 13 20:58:01 2024 ] 	Batch(6000/6809) done. Loss: 0.1523  lr:0.000001
[ Sat Jul 13 20:58:20 2024 ] 	Batch(6100/6809) done. Loss: 0.0998  lr:0.000001
[ Sat Jul 13 20:58:38 2024 ] 	Batch(6200/6809) done. Loss: 0.1535  lr:0.000001
[ Sat Jul 13 20:58:57 2024 ] 	Batch(6300/6809) done. Loss: 0.5133  lr:0.000001
[ Sat Jul 13 20:59:15 2024 ] 	Batch(6400/6809) done. Loss: 0.0446  lr:0.000001
[ Sat Jul 13 20:59:33 2024 ] 
Training: Epoch [10/50], Step [6499], Loss: 0.07531861960887909, Training Accuracy: 97.6076923076923
[ Sat Jul 13 20:59:33 2024 ] 	Batch(6500/6809) done. Loss: 0.0612  lr:0.000001
[ Sat Jul 13 20:59:51 2024 ] 	Batch(6600/6809) done. Loss: 0.0853  lr:0.000001
[ Sat Jul 13 21:00:09 2024 ] 	Batch(6700/6809) done. Loss: 0.0807  lr:0.000001
[ Sat Jul 13 21:00:27 2024 ] 	Batch(6800/6809) done. Loss: 0.1886  lr:0.000001
[ Sat Jul 13 21:00:29 2024 ] 	Mean training loss: 0.0970.
[ Sat Jul 13 21:00:29 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 21:00:29 2024 ] Training epoch: 12
[ Sat Jul 13 21:00:29 2024 ] 	Batch(0/6809) done. Loss: 0.2109  lr:0.000001
[ Sat Jul 13 21:00:47 2024 ] 	Batch(100/6809) done. Loss: 0.1723  lr:0.000001
[ Sat Jul 13 21:01:05 2024 ] 	Batch(200/6809) done. Loss: 0.0706  lr:0.000001
[ Sat Jul 13 21:01:23 2024 ] 	Batch(300/6809) done. Loss: 0.0251  lr:0.000001
[ Sat Jul 13 21:01:41 2024 ] 	Batch(400/6809) done. Loss: 0.0204  lr:0.000001
[ Sat Jul 13 21:01:59 2024 ] 
Training: Epoch [11/50], Step [499], Loss: 0.0907726064324379, Training Accuracy: 97.6
[ Sat Jul 13 21:01:59 2024 ] 	Batch(500/6809) done. Loss: 0.0166  lr:0.000001
[ Sat Jul 13 21:02:17 2024 ] 	Batch(600/6809) done. Loss: 0.1475  lr:0.000001
[ Sat Jul 13 21:02:35 2024 ] 	Batch(700/6809) done. Loss: 0.0478  lr:0.000001
[ Sat Jul 13 21:02:53 2024 ] 	Batch(800/6809) done. Loss: 0.0723  lr:0.000001
[ Sat Jul 13 21:03:11 2024 ] 	Batch(900/6809) done. Loss: 0.0139  lr:0.000001
[ Sat Jul 13 21:03:30 2024 ] 
Training: Epoch [11/50], Step [999], Loss: 0.12216104567050934, Training Accuracy: 97.6625
[ Sat Jul 13 21:03:30 2024 ] 	Batch(1000/6809) done. Loss: 0.0013  lr:0.000001
[ Sat Jul 13 21:03:48 2024 ] 	Batch(1100/6809) done. Loss: 0.0621  lr:0.000001
[ Sat Jul 13 21:04:07 2024 ] 	Batch(1200/6809) done. Loss: 0.0950  lr:0.000001
[ Sat Jul 13 21:04:25 2024 ] 	Batch(1300/6809) done. Loss: 0.0849  lr:0.000001
[ Sat Jul 13 21:04:44 2024 ] 	Batch(1400/6809) done. Loss: 0.0337  lr:0.000001
[ Sat Jul 13 21:05:02 2024 ] 
Training: Epoch [11/50], Step [1499], Loss: 0.036431845277547836, Training Accuracy: 97.58333333333333
[ Sat Jul 13 21:05:03 2024 ] 	Batch(1500/6809) done. Loss: 0.0180  lr:0.000001
[ Sat Jul 13 21:05:21 2024 ] 	Batch(1600/6809) done. Loss: 0.0235  lr:0.000001
[ Sat Jul 13 21:05:40 2024 ] 	Batch(1700/6809) done. Loss: 0.0061  lr:0.000001
[ Sat Jul 13 21:05:58 2024 ] 	Batch(1800/6809) done. Loss: 0.0352  lr:0.000001
[ Sat Jul 13 21:06:17 2024 ] 	Batch(1900/6809) done. Loss: 0.0362  lr:0.000001
[ Sat Jul 13 21:06:35 2024 ] 
Training: Epoch [11/50], Step [1999], Loss: 0.24281522631645203, Training Accuracy: 97.59375
[ Sat Jul 13 21:06:36 2024 ] 	Batch(2000/6809) done. Loss: 0.0127  lr:0.000001
[ Sat Jul 13 21:06:54 2024 ] 	Batch(2100/6809) done. Loss: 0.0030  lr:0.000001
[ Sat Jul 13 21:07:12 2024 ] 	Batch(2200/6809) done. Loss: 0.0760  lr:0.000001
[ Sat Jul 13 21:07:30 2024 ] 	Batch(2300/6809) done. Loss: 0.0564  lr:0.000001
[ Sat Jul 13 21:07:48 2024 ] 	Batch(2400/6809) done. Loss: 0.1776  lr:0.000001
[ Sat Jul 13 21:08:06 2024 ] 
Training: Epoch [11/50], Step [2499], Loss: 0.07848949730396271, Training Accuracy: 97.64500000000001
[ Sat Jul 13 21:08:06 2024 ] 	Batch(2500/6809) done. Loss: 0.0039  lr:0.000001
[ Sat Jul 13 21:08:24 2024 ] 	Batch(2600/6809) done. Loss: 0.4722  lr:0.000001
[ Sat Jul 13 21:08:42 2024 ] 	Batch(2700/6809) done. Loss: 0.0164  lr:0.000001
[ Sat Jul 13 21:09:00 2024 ] 	Batch(2800/6809) done. Loss: 0.0163  lr:0.000001
[ Sat Jul 13 21:09:18 2024 ] 	Batch(2900/6809) done. Loss: 0.1165  lr:0.000001
[ Sat Jul 13 21:09:35 2024 ] 
Training: Epoch [11/50], Step [2999], Loss: 0.0303559061139822, Training Accuracy: 97.59166666666667
[ Sat Jul 13 21:09:36 2024 ] 	Batch(3000/6809) done. Loss: 0.1019  lr:0.000001
[ Sat Jul 13 21:09:53 2024 ] 	Batch(3100/6809) done. Loss: 0.1163  lr:0.000001
[ Sat Jul 13 21:10:11 2024 ] 	Batch(3200/6809) done. Loss: 0.0495  lr:0.000001
[ Sat Jul 13 21:10:30 2024 ] 	Batch(3300/6809) done. Loss: 0.0082  lr:0.000001
[ Sat Jul 13 21:10:49 2024 ] 	Batch(3400/6809) done. Loss: 0.1667  lr:0.000001
[ Sat Jul 13 21:11:07 2024 ] 
Training: Epoch [11/50], Step [3499], Loss: 0.08355839550495148, Training Accuracy: 97.55
[ Sat Jul 13 21:11:07 2024 ] 	Batch(3500/6809) done. Loss: 0.0267  lr:0.000001
[ Sat Jul 13 21:11:26 2024 ] 	Batch(3600/6809) done. Loss: 0.0100  lr:0.000001
[ Sat Jul 13 21:11:44 2024 ] 	Batch(3700/6809) done. Loss: 0.0342  lr:0.000001
[ Sat Jul 13 21:12:02 2024 ] 	Batch(3800/6809) done. Loss: 0.0389  lr:0.000001
[ Sat Jul 13 21:12:19 2024 ] 	Batch(3900/6809) done. Loss: 0.0620  lr:0.000001
[ Sat Jul 13 21:12:37 2024 ] 
Training: Epoch [11/50], Step [3999], Loss: 0.23690558969974518, Training Accuracy: 97.5375
[ Sat Jul 13 21:12:37 2024 ] 	Batch(4000/6809) done. Loss: 0.0220  lr:0.000001
[ Sat Jul 13 21:12:55 2024 ] 	Batch(4100/6809) done. Loss: 0.0187  lr:0.000001
[ Sat Jul 13 21:13:13 2024 ] 	Batch(4200/6809) done. Loss: 0.0665  lr:0.000001
[ Sat Jul 13 21:13:31 2024 ] 	Batch(4300/6809) done. Loss: 0.0834  lr:0.000001
[ Sat Jul 13 21:13:49 2024 ] 	Batch(4400/6809) done. Loss: 0.2824  lr:0.000001
[ Sat Jul 13 21:14:07 2024 ] 
Training: Epoch [11/50], Step [4499], Loss: 0.00047347042709589005, Training Accuracy: 97.55833333333334
[ Sat Jul 13 21:14:07 2024 ] 	Batch(4500/6809) done. Loss: 0.0454  lr:0.000001
[ Sat Jul 13 21:14:25 2024 ] 	Batch(4600/6809) done. Loss: 0.1453  lr:0.000001
[ Sat Jul 13 21:14:43 2024 ] 	Batch(4700/6809) done. Loss: 0.1542  lr:0.000001
[ Sat Jul 13 21:15:01 2024 ] 	Batch(4800/6809) done. Loss: 0.0098  lr:0.000001
[ Sat Jul 13 21:15:19 2024 ] 	Batch(4900/6809) done. Loss: 0.0161  lr:0.000001
[ Sat Jul 13 21:15:37 2024 ] 
Training: Epoch [11/50], Step [4999], Loss: 0.05470961332321167, Training Accuracy: 97.5675
[ Sat Jul 13 21:15:37 2024 ] 	Batch(5000/6809) done. Loss: 0.0099  lr:0.000001
[ Sat Jul 13 21:15:55 2024 ] 	Batch(5100/6809) done. Loss: 0.0193  lr:0.000001
[ Sat Jul 13 21:16:13 2024 ] 	Batch(5200/6809) done. Loss: 0.0494  lr:0.000001
[ Sat Jul 13 21:16:31 2024 ] 	Batch(5300/6809) done. Loss: 0.0860  lr:0.000001
[ Sat Jul 13 21:16:50 2024 ] 	Batch(5400/6809) done. Loss: 0.3364  lr:0.000001
[ Sat Jul 13 21:17:08 2024 ] 
Training: Epoch [11/50], Step [5499], Loss: 0.035549096763134, Training Accuracy: 97.57727272727273
[ Sat Jul 13 21:17:08 2024 ] 	Batch(5500/6809) done. Loss: 0.1129  lr:0.000001
[ Sat Jul 13 21:17:27 2024 ] 	Batch(5600/6809) done. Loss: 0.2365  lr:0.000001
[ Sat Jul 13 21:17:45 2024 ] 	Batch(5700/6809) done. Loss: 0.0242  lr:0.000001
[ Sat Jul 13 21:18:03 2024 ] 	Batch(5800/6809) done. Loss: 0.0349  lr:0.000001
[ Sat Jul 13 21:18:21 2024 ] 	Batch(5900/6809) done. Loss: 0.0664  lr:0.000001
[ Sat Jul 13 21:18:38 2024 ] 
Training: Epoch [11/50], Step [5999], Loss: 0.08569838106632233, Training Accuracy: 97.59791666666666
[ Sat Jul 13 21:18:39 2024 ] 	Batch(6000/6809) done. Loss: 0.1210  lr:0.000001
[ Sat Jul 13 21:18:57 2024 ] 	Batch(6100/6809) done. Loss: 0.0008  lr:0.000001
[ Sat Jul 13 21:19:15 2024 ] 	Batch(6200/6809) done. Loss: 0.4800  lr:0.000001
[ Sat Jul 13 21:19:32 2024 ] 	Batch(6300/6809) done. Loss: 0.0642  lr:0.000001
[ Sat Jul 13 21:19:51 2024 ] 	Batch(6400/6809) done. Loss: 0.0503  lr:0.000001
[ Sat Jul 13 21:20:09 2024 ] 
Training: Epoch [11/50], Step [6499], Loss: 0.14769962430000305, Training Accuracy: 97.59423076923078
[ Sat Jul 13 21:20:09 2024 ] 	Batch(6500/6809) done. Loss: 0.1865  lr:0.000001
[ Sat Jul 13 21:20:27 2024 ] 	Batch(6600/6809) done. Loss: 0.3337  lr:0.000001
[ Sat Jul 13 21:20:45 2024 ] 	Batch(6700/6809) done. Loss: 0.0703  lr:0.000001
[ Sat Jul 13 21:21:03 2024 ] 	Batch(6800/6809) done. Loss: 0.0173  lr:0.000001
[ Sat Jul 13 21:21:04 2024 ] 	Mean training loss: 0.0969.
[ Sat Jul 13 21:21:04 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 21:21:04 2024 ] Training epoch: 13
[ Sat Jul 13 21:21:05 2024 ] 	Batch(0/6809) done. Loss: 0.0752  lr:0.000001
[ Sat Jul 13 21:21:23 2024 ] 	Batch(100/6809) done. Loss: 0.1233  lr:0.000001
[ Sat Jul 13 21:21:41 2024 ] 	Batch(200/6809) done. Loss: 0.1505  lr:0.000001
[ Sat Jul 13 21:21:58 2024 ] 	Batch(300/6809) done. Loss: 0.0140  lr:0.000001
[ Sat Jul 13 21:22:16 2024 ] 	Batch(400/6809) done. Loss: 0.0332  lr:0.000001
[ Sat Jul 13 21:22:34 2024 ] 
Training: Epoch [12/50], Step [499], Loss: 0.08946474641561508, Training Accuracy: 97.475
[ Sat Jul 13 21:22:34 2024 ] 	Batch(500/6809) done. Loss: 0.0025  lr:0.000001
[ Sat Jul 13 21:22:52 2024 ] 	Batch(600/6809) done. Loss: 0.2226  lr:0.000001
[ Sat Jul 13 21:23:10 2024 ] 	Batch(700/6809) done. Loss: 0.1266  lr:0.000001
[ Sat Jul 13 21:23:28 2024 ] 	Batch(800/6809) done. Loss: 0.0143  lr:0.000001
[ Sat Jul 13 21:23:46 2024 ] 	Batch(900/6809) done. Loss: 0.4295  lr:0.000001
[ Sat Jul 13 21:24:04 2024 ] 
Training: Epoch [12/50], Step [999], Loss: 0.13815432786941528, Training Accuracy: 97.48750000000001
[ Sat Jul 13 21:24:04 2024 ] 	Batch(1000/6809) done. Loss: 0.0109  lr:0.000001
[ Sat Jul 13 21:24:22 2024 ] 	Batch(1100/6809) done. Loss: 0.2808  lr:0.000001
[ Sat Jul 13 21:24:40 2024 ] 	Batch(1200/6809) done. Loss: 0.1051  lr:0.000001
[ Sat Jul 13 21:24:58 2024 ] 	Batch(1300/6809) done. Loss: 0.0182  lr:0.000001
[ Sat Jul 13 21:25:16 2024 ] 	Batch(1400/6809) done. Loss: 0.0242  lr:0.000001
[ Sat Jul 13 21:25:33 2024 ] 
Training: Epoch [12/50], Step [1499], Loss: 0.03593744337558746, Training Accuracy: 97.55833333333334
[ Sat Jul 13 21:25:33 2024 ] 	Batch(1500/6809) done. Loss: 0.0119  lr:0.000001
[ Sat Jul 13 21:25:52 2024 ] 	Batch(1600/6809) done. Loss: 0.0033  lr:0.000001
[ Sat Jul 13 21:26:10 2024 ] 	Batch(1700/6809) done. Loss: 0.0009  lr:0.000001
[ Sat Jul 13 21:26:29 2024 ] 	Batch(1800/6809) done. Loss: 0.3930  lr:0.000001
[ Sat Jul 13 21:26:47 2024 ] 	Batch(1900/6809) done. Loss: 0.0716  lr:0.000001
[ Sat Jul 13 21:27:05 2024 ] 
Training: Epoch [12/50], Step [1999], Loss: 0.02420085482299328, Training Accuracy: 97.53125
[ Sat Jul 13 21:27:05 2024 ] 	Batch(2000/6809) done. Loss: 0.1778  lr:0.000001
[ Sat Jul 13 21:27:23 2024 ] 	Batch(2100/6809) done. Loss: 0.0153  lr:0.000001
[ Sat Jul 13 21:27:41 2024 ] 	Batch(2200/6809) done. Loss: 0.2686  lr:0.000001
[ Sat Jul 13 21:27:59 2024 ] 	Batch(2300/6809) done. Loss: 0.0486  lr:0.000001
[ Sat Jul 13 21:28:17 2024 ] 	Batch(2400/6809) done. Loss: 0.0408  lr:0.000001
[ Sat Jul 13 21:28:35 2024 ] 
Training: Epoch [12/50], Step [2499], Loss: 0.09795388579368591, Training Accuracy: 97.54
[ Sat Jul 13 21:28:35 2024 ] 	Batch(2500/6809) done. Loss: 0.0192  lr:0.000001
[ Sat Jul 13 21:28:53 2024 ] 	Batch(2600/6809) done. Loss: 0.1232  lr:0.000001
[ Sat Jul 13 21:29:11 2024 ] 	Batch(2700/6809) done. Loss: 0.0224  lr:0.000001
[ Sat Jul 13 21:29:29 2024 ] 	Batch(2800/6809) done. Loss: 0.0142  lr:0.000001
[ Sat Jul 13 21:29:46 2024 ] 	Batch(2900/6809) done. Loss: 0.0296  lr:0.000001
[ Sat Jul 13 21:30:04 2024 ] 
Training: Epoch [12/50], Step [2999], Loss: 0.01643785461783409, Training Accuracy: 97.53333333333333
[ Sat Jul 13 21:30:04 2024 ] 	Batch(3000/6809) done. Loss: 0.0846  lr:0.000001
[ Sat Jul 13 21:30:22 2024 ] 	Batch(3100/6809) done. Loss: 0.0200  lr:0.000001
[ Sat Jul 13 21:30:40 2024 ] 	Batch(3200/6809) done. Loss: 0.0527  lr:0.000001
[ Sat Jul 13 21:30:58 2024 ] 	Batch(3300/6809) done. Loss: 0.0188  lr:0.000001
[ Sat Jul 13 21:31:16 2024 ] 	Batch(3400/6809) done. Loss: 0.0048  lr:0.000001
[ Sat Jul 13 21:31:34 2024 ] 
Training: Epoch [12/50], Step [3499], Loss: 0.17604179680347443, Training Accuracy: 97.56785714285714
[ Sat Jul 13 21:31:34 2024 ] 	Batch(3500/6809) done. Loss: 0.0149  lr:0.000001
[ Sat Jul 13 21:31:52 2024 ] 	Batch(3600/6809) done. Loss: 0.0657  lr:0.000001
[ Sat Jul 13 21:32:11 2024 ] 	Batch(3700/6809) done. Loss: 0.0109  lr:0.000001
[ Sat Jul 13 21:32:29 2024 ] 	Batch(3800/6809) done. Loss: 0.0095  lr:0.000001
[ Sat Jul 13 21:32:46 2024 ] 	Batch(3900/6809) done. Loss: 0.0724  lr:0.000001
[ Sat Jul 13 21:33:04 2024 ] 
Training: Epoch [12/50], Step [3999], Loss: 0.041384465992450714, Training Accuracy: 97.56875000000001
[ Sat Jul 13 21:33:05 2024 ] 	Batch(4000/6809) done. Loss: 0.0193  lr:0.000001
[ Sat Jul 13 21:33:22 2024 ] 	Batch(4100/6809) done. Loss: 0.0076  lr:0.000001
[ Sat Jul 13 21:33:40 2024 ] 	Batch(4200/6809) done. Loss: 0.0288  lr:0.000001
[ Sat Jul 13 21:33:58 2024 ] 	Batch(4300/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 21:34:16 2024 ] 	Batch(4400/6809) done. Loss: 0.0160  lr:0.000001
[ Sat Jul 13 21:34:34 2024 ] 
Training: Epoch [12/50], Step [4499], Loss: 0.07960125803947449, Training Accuracy: 97.54722222222222
[ Sat Jul 13 21:34:34 2024 ] 	Batch(4500/6809) done. Loss: 0.0079  lr:0.000001
[ Sat Jul 13 21:34:52 2024 ] 	Batch(4600/6809) done. Loss: 0.0651  lr:0.000001
[ Sat Jul 13 21:35:10 2024 ] 	Batch(4700/6809) done. Loss: 0.0220  lr:0.000001
[ Sat Jul 13 21:35:28 2024 ] 	Batch(4800/6809) done. Loss: 0.0086  lr:0.000001
[ Sat Jul 13 21:35:47 2024 ] 	Batch(4900/6809) done. Loss: 0.2677  lr:0.000001
[ Sat Jul 13 21:36:04 2024 ] 
Training: Epoch [12/50], Step [4999], Loss: 0.0041814022697508335, Training Accuracy: 97.5425
[ Sat Jul 13 21:36:04 2024 ] 	Batch(5000/6809) done. Loss: 0.0231  lr:0.000001
[ Sat Jul 13 21:36:22 2024 ] 	Batch(5100/6809) done. Loss: 0.0318  lr:0.000001
[ Sat Jul 13 21:36:40 2024 ] 	Batch(5200/6809) done. Loss: 0.0313  lr:0.000001
[ Sat Jul 13 21:36:58 2024 ] 	Batch(5300/6809) done. Loss: 0.0141  lr:0.000001
[ Sat Jul 13 21:37:16 2024 ] 	Batch(5400/6809) done. Loss: 0.1030  lr:0.000001
[ Sat Jul 13 21:37:34 2024 ] 
Training: Epoch [12/50], Step [5499], Loss: 0.050580788403749466, Training Accuracy: 97.54318181818182
[ Sat Jul 13 21:37:34 2024 ] 	Batch(5500/6809) done. Loss: 0.0703  lr:0.000001
[ Sat Jul 13 21:37:52 2024 ] 	Batch(5600/6809) done. Loss: 0.0148  lr:0.000001
[ Sat Jul 13 21:38:10 2024 ] 	Batch(5700/6809) done. Loss: 0.0596  lr:0.000001
[ Sat Jul 13 21:38:28 2024 ] 	Batch(5800/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 21:38:46 2024 ] 	Batch(5900/6809) done. Loss: 0.1347  lr:0.000001
[ Sat Jul 13 21:39:04 2024 ] 
Training: Epoch [12/50], Step [5999], Loss: 0.06722240895032883, Training Accuracy: 97.57083333333333
[ Sat Jul 13 21:39:04 2024 ] 	Batch(6000/6809) done. Loss: 0.0427  lr:0.000001
[ Sat Jul 13 21:39:22 2024 ] 	Batch(6100/6809) done. Loss: 0.0285  lr:0.000001
[ Sat Jul 13 21:39:40 2024 ] 	Batch(6200/6809) done. Loss: 0.0094  lr:0.000001
[ Sat Jul 13 21:39:58 2024 ] 	Batch(6300/6809) done. Loss: 0.1568  lr:0.000001
[ Sat Jul 13 21:40:16 2024 ] 	Batch(6400/6809) done. Loss: 0.2470  lr:0.000001
[ Sat Jul 13 21:40:34 2024 ] 
Training: Epoch [12/50], Step [6499], Loss: 0.07106037437915802, Training Accuracy: 97.57884615384616
[ Sat Jul 13 21:40:34 2024 ] 	Batch(6500/6809) done. Loss: 0.0178  lr:0.000001
[ Sat Jul 13 21:40:52 2024 ] 	Batch(6600/6809) done. Loss: 0.0239  lr:0.000001
[ Sat Jul 13 21:41:10 2024 ] 	Batch(6700/6809) done. Loss: 0.1444  lr:0.000001
[ Sat Jul 13 21:41:29 2024 ] 	Batch(6800/6809) done. Loss: 0.0341  lr:0.000001
[ Sat Jul 13 21:41:30 2024 ] 	Mean training loss: 0.0976.
[ Sat Jul 13 21:41:30 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 21:41:30 2024 ] Training epoch: 14
[ Sat Jul 13 21:41:31 2024 ] 	Batch(0/6809) done. Loss: 0.0453  lr:0.000001
[ Sat Jul 13 21:41:49 2024 ] 	Batch(100/6809) done. Loss: 0.0022  lr:0.000001
[ Sat Jul 13 21:42:07 2024 ] 	Batch(200/6809) done. Loss: 0.1167  lr:0.000001
[ Sat Jul 13 21:42:26 2024 ] 	Batch(300/6809) done. Loss: 0.1889  lr:0.000001
[ Sat Jul 13 21:42:44 2024 ] 	Batch(400/6809) done. Loss: 0.1563  lr:0.000001
[ Sat Jul 13 21:43:02 2024 ] 
Training: Epoch [13/50], Step [499], Loss: 0.3097924590110779, Training Accuracy: 97.65
[ Sat Jul 13 21:43:02 2024 ] 	Batch(500/6809) done. Loss: 0.0211  lr:0.000001
[ Sat Jul 13 21:43:20 2024 ] 	Batch(600/6809) done. Loss: 0.0433  lr:0.000001
[ Sat Jul 13 21:43:38 2024 ] 	Batch(700/6809) done. Loss: 0.4278  lr:0.000001
[ Sat Jul 13 21:43:56 2024 ] 	Batch(800/6809) done. Loss: 0.0259  lr:0.000001
[ Sat Jul 13 21:44:14 2024 ] 	Batch(900/6809) done. Loss: 0.0065  lr:0.000001
[ Sat Jul 13 21:44:32 2024 ] 
Training: Epoch [13/50], Step [999], Loss: 0.04242478311061859, Training Accuracy: 97.46249999999999
[ Sat Jul 13 21:44:32 2024 ] 	Batch(1000/6809) done. Loss: 0.0217  lr:0.000001
[ Sat Jul 13 21:44:50 2024 ] 	Batch(1100/6809) done. Loss: 0.0113  lr:0.000001
[ Sat Jul 13 21:45:08 2024 ] 	Batch(1200/6809) done. Loss: 0.0575  lr:0.000001
[ Sat Jul 13 21:45:26 2024 ] 	Batch(1300/6809) done. Loss: 0.1854  lr:0.000001
[ Sat Jul 13 21:45:44 2024 ] 	Batch(1400/6809) done. Loss: 0.0255  lr:0.000001
[ Sat Jul 13 21:46:03 2024 ] 
Training: Epoch [13/50], Step [1499], Loss: 0.04670222848653793, Training Accuracy: 97.425
[ Sat Jul 13 21:46:03 2024 ] 	Batch(1500/6809) done. Loss: 0.0182  lr:0.000001
[ Sat Jul 13 21:46:21 2024 ] 	Batch(1600/6809) done. Loss: 0.0222  lr:0.000001
[ Sat Jul 13 21:46:40 2024 ] 	Batch(1700/6809) done. Loss: 0.0451  lr:0.000001
[ Sat Jul 13 21:46:58 2024 ] 	Batch(1800/6809) done. Loss: 0.1671  lr:0.000001
[ Sat Jul 13 21:47:16 2024 ] 	Batch(1900/6809) done. Loss: 0.1759  lr:0.000001
[ Sat Jul 13 21:47:35 2024 ] 
Training: Epoch [13/50], Step [1999], Loss: 0.1366586983203888, Training Accuracy: 97.6625
[ Sat Jul 13 21:47:35 2024 ] 	Batch(2000/6809) done. Loss: 0.4888  lr:0.000001
[ Sat Jul 13 21:47:53 2024 ] 	Batch(2100/6809) done. Loss: 0.0906  lr:0.000001
[ Sat Jul 13 21:48:11 2024 ] 	Batch(2200/6809) done. Loss: 0.0777  lr:0.000001
[ Sat Jul 13 21:48:30 2024 ] 	Batch(2300/6809) done. Loss: 0.2448  lr:0.000001
[ Sat Jul 13 21:48:48 2024 ] 	Batch(2400/6809) done. Loss: 0.0130  lr:0.000001
[ Sat Jul 13 21:49:07 2024 ] 
Training: Epoch [13/50], Step [2499], Loss: 0.014612607657909393, Training Accuracy: 97.655
[ Sat Jul 13 21:49:07 2024 ] 	Batch(2500/6809) done. Loss: 0.0314  lr:0.000001
[ Sat Jul 13 21:49:25 2024 ] 	Batch(2600/6809) done. Loss: 0.0884  lr:0.000001
[ Sat Jul 13 21:49:43 2024 ] 	Batch(2700/6809) done. Loss: 0.0987  lr:0.000001
[ Sat Jul 13 21:50:02 2024 ] 	Batch(2800/6809) done. Loss: 0.0721  lr:0.000001
[ Sat Jul 13 21:50:20 2024 ] 	Batch(2900/6809) done. Loss: 0.0778  lr:0.000001
[ Sat Jul 13 21:50:38 2024 ] 
Training: Epoch [13/50], Step [2999], Loss: 0.42009738087654114, Training Accuracy: 97.67083333333333
[ Sat Jul 13 21:50:38 2024 ] 	Batch(3000/6809) done. Loss: 0.0162  lr:0.000001
[ Sat Jul 13 21:50:56 2024 ] 	Batch(3100/6809) done. Loss: 0.0120  lr:0.000001
[ Sat Jul 13 21:51:14 2024 ] 	Batch(3200/6809) done. Loss: 0.0108  lr:0.000001
[ Sat Jul 13 21:51:32 2024 ] 	Batch(3300/6809) done. Loss: 0.1346  lr:0.000001
[ Sat Jul 13 21:51:50 2024 ] 	Batch(3400/6809) done. Loss: 0.0072  lr:0.000001
[ Sat Jul 13 21:52:08 2024 ] 
Training: Epoch [13/50], Step [3499], Loss: 0.010732936672866344, Training Accuracy: 97.62142857142857
[ Sat Jul 13 21:52:08 2024 ] 	Batch(3500/6809) done. Loss: 0.1218  lr:0.000001
[ Sat Jul 13 21:52:26 2024 ] 	Batch(3600/6809) done. Loss: 0.0867  lr:0.000001
[ Sat Jul 13 21:52:44 2024 ] 	Batch(3700/6809) done. Loss: 0.0128  lr:0.000001
[ Sat Jul 13 21:53:02 2024 ] 	Batch(3800/6809) done. Loss: 0.0011  lr:0.000001
[ Sat Jul 13 21:53:20 2024 ] 	Batch(3900/6809) done. Loss: 0.0033  lr:0.000001
[ Sat Jul 13 21:53:39 2024 ] 
Training: Epoch [13/50], Step [3999], Loss: 0.22081463038921356, Training Accuracy: 97.59375
[ Sat Jul 13 21:53:39 2024 ] 	Batch(4000/6809) done. Loss: 0.0973  lr:0.000001
[ Sat Jul 13 21:53:57 2024 ] 	Batch(4100/6809) done. Loss: 0.0161  lr:0.000001
[ Sat Jul 13 21:54:16 2024 ] 	Batch(4200/6809) done. Loss: 0.2532  lr:0.000001
[ Sat Jul 13 21:54:34 2024 ] 	Batch(4300/6809) done. Loss: 0.0713  lr:0.000001
[ Sat Jul 13 21:54:52 2024 ] 	Batch(4400/6809) done. Loss: 0.0027  lr:0.000001
[ Sat Jul 13 21:55:10 2024 ] 
Training: Epoch [13/50], Step [4499], Loss: 0.056377217173576355, Training Accuracy: 97.59444444444443
[ Sat Jul 13 21:55:10 2024 ] 	Batch(4500/6809) done. Loss: 0.1269  lr:0.000001
[ Sat Jul 13 21:55:28 2024 ] 	Batch(4600/6809) done. Loss: 0.0409  lr:0.000001
[ Sat Jul 13 21:55:46 2024 ] 	Batch(4700/6809) done. Loss: 0.0563  lr:0.000001
[ Sat Jul 13 21:56:05 2024 ] 	Batch(4800/6809) done. Loss: 0.0899  lr:0.000001
[ Sat Jul 13 21:56:23 2024 ] 	Batch(4900/6809) done. Loss: 0.1176  lr:0.000001
[ Sat Jul 13 21:56:41 2024 ] 
Training: Epoch [13/50], Step [4999], Loss: 0.2676754593849182, Training Accuracy: 97.6125
[ Sat Jul 13 21:56:41 2024 ] 	Batch(5000/6809) done. Loss: 0.0183  lr:0.000001
[ Sat Jul 13 21:56:59 2024 ] 	Batch(5100/6809) done. Loss: 0.1218  lr:0.000001
[ Sat Jul 13 21:57:17 2024 ] 	Batch(5200/6809) done. Loss: 0.0550  lr:0.000001
[ Sat Jul 13 21:57:35 2024 ] 	Batch(5300/6809) done. Loss: 0.0442  lr:0.000001
[ Sat Jul 13 21:57:53 2024 ] 	Batch(5400/6809) done. Loss: 0.0136  lr:0.000001
[ Sat Jul 13 21:58:12 2024 ] 
Training: Epoch [13/50], Step [5499], Loss: 0.35950297117233276, Training Accuracy: 97.57727272727273
[ Sat Jul 13 21:58:12 2024 ] 	Batch(5500/6809) done. Loss: 0.1814  lr:0.000001
[ Sat Jul 13 21:58:30 2024 ] 	Batch(5600/6809) done. Loss: 0.0486  lr:0.000001
[ Sat Jul 13 21:58:48 2024 ] 	Batch(5700/6809) done. Loss: 0.0485  lr:0.000001
[ Sat Jul 13 21:59:06 2024 ] 	Batch(5800/6809) done. Loss: 0.1124  lr:0.000001
[ Sat Jul 13 21:59:24 2024 ] 	Batch(5900/6809) done. Loss: 0.1488  lr:0.000001
[ Sat Jul 13 21:59:41 2024 ] 
Training: Epoch [13/50], Step [5999], Loss: 0.15413621068000793, Training Accuracy: 97.59583333333333
[ Sat Jul 13 21:59:42 2024 ] 	Batch(6000/6809) done. Loss: 0.0078  lr:0.000001
[ Sat Jul 13 22:00:00 2024 ] 	Batch(6100/6809) done. Loss: 0.1631  lr:0.000001
[ Sat Jul 13 22:00:17 2024 ] 	Batch(6200/6809) done. Loss: 0.2267  lr:0.000001
[ Sat Jul 13 22:00:35 2024 ] 	Batch(6300/6809) done. Loss: 0.0213  lr:0.000001
[ Sat Jul 13 22:00:53 2024 ] 	Batch(6400/6809) done. Loss: 0.0069  lr:0.000001
[ Sat Jul 13 22:01:11 2024 ] 
Training: Epoch [13/50], Step [6499], Loss: 0.08190979063510895, Training Accuracy: 97.57884615384616
[ Sat Jul 13 22:01:11 2024 ] 	Batch(6500/6809) done. Loss: 0.0859  lr:0.000001
[ Sat Jul 13 22:01:29 2024 ] 	Batch(6600/6809) done. Loss: 0.0060  lr:0.000001
[ Sat Jul 13 22:01:47 2024 ] 	Batch(6700/6809) done. Loss: 0.0453  lr:0.000001
[ Sat Jul 13 22:02:05 2024 ] 	Batch(6800/6809) done. Loss: 0.0057  lr:0.000001
[ Sat Jul 13 22:02:07 2024 ] 	Mean training loss: 0.0939.
[ Sat Jul 13 22:02:07 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 22:02:07 2024 ] Training epoch: 15
[ Sat Jul 13 22:02:07 2024 ] 	Batch(0/6809) done. Loss: 0.0270  lr:0.000001
[ Sat Jul 13 22:02:25 2024 ] 	Batch(100/6809) done. Loss: 0.0156  lr:0.000001
[ Sat Jul 13 22:02:43 2024 ] 	Batch(200/6809) done. Loss: 0.3897  lr:0.000001
[ Sat Jul 13 22:03:01 2024 ] 	Batch(300/6809) done. Loss: 0.0051  lr:0.000001
[ Sat Jul 13 22:03:20 2024 ] 	Batch(400/6809) done. Loss: 0.1062  lr:0.000001
[ Sat Jul 13 22:03:38 2024 ] 
Training: Epoch [14/50], Step [499], Loss: 0.021932266652584076, Training Accuracy: 97.39999999999999
[ Sat Jul 13 22:03:38 2024 ] 	Batch(500/6809) done. Loss: 0.2338  lr:0.000001
[ Sat Jul 13 22:03:57 2024 ] 	Batch(600/6809) done. Loss: 0.0185  lr:0.000001
[ Sat Jul 13 22:04:15 2024 ] 	Batch(700/6809) done. Loss: 0.0564  lr:0.000001
[ Sat Jul 13 22:04:33 2024 ] 	Batch(800/6809) done. Loss: 0.0070  lr:0.000001
[ Sat Jul 13 22:04:51 2024 ] 	Batch(900/6809) done. Loss: 0.0607  lr:0.000001
[ Sat Jul 13 22:05:09 2024 ] 
Training: Epoch [14/50], Step [999], Loss: 0.023205699399113655, Training Accuracy: 97.575
[ Sat Jul 13 22:05:09 2024 ] 	Batch(1000/6809) done. Loss: 0.1422  lr:0.000001
[ Sat Jul 13 22:05:27 2024 ] 	Batch(1100/6809) done. Loss: 0.0081  lr:0.000001
[ Sat Jul 13 22:05:45 2024 ] 	Batch(1200/6809) done. Loss: 0.2559  lr:0.000001
[ Sat Jul 13 22:06:03 2024 ] 	Batch(1300/6809) done. Loss: 0.0034  lr:0.000001
[ Sat Jul 13 22:06:21 2024 ] 	Batch(1400/6809) done. Loss: 0.2599  lr:0.000001
[ Sat Jul 13 22:06:39 2024 ] 
Training: Epoch [14/50], Step [1499], Loss: 0.09500683099031448, Training Accuracy: 97.65
[ Sat Jul 13 22:06:39 2024 ] 	Batch(1500/6809) done. Loss: 0.0333  lr:0.000001
[ Sat Jul 13 22:06:57 2024 ] 	Batch(1600/6809) done. Loss: 0.0052  lr:0.000001
[ Sat Jul 13 22:07:15 2024 ] 	Batch(1700/6809) done. Loss: 0.0811  lr:0.000001
[ Sat Jul 13 22:07:33 2024 ] 	Batch(1800/6809) done. Loss: 0.0465  lr:0.000001
[ Sat Jul 13 22:07:51 2024 ] 	Batch(1900/6809) done. Loss: 0.0036  lr:0.000001
[ Sat Jul 13 22:08:09 2024 ] 
Training: Epoch [14/50], Step [1999], Loss: 0.0638229101896286, Training Accuracy: 97.64375
[ Sat Jul 13 22:08:10 2024 ] 	Batch(2000/6809) done. Loss: 0.0328  lr:0.000001
[ Sat Jul 13 22:08:28 2024 ] 	Batch(2100/6809) done. Loss: 0.0029  lr:0.000001
[ Sat Jul 13 22:08:46 2024 ] 	Batch(2200/6809) done. Loss: 0.0142  lr:0.000001
[ Sat Jul 13 22:09:04 2024 ] 	Batch(2300/6809) done. Loss: 0.0658  lr:0.000001
[ Sat Jul 13 22:09:22 2024 ] 	Batch(2400/6809) done. Loss: 0.0039  lr:0.000001
[ Sat Jul 13 22:09:39 2024 ] 
Training: Epoch [14/50], Step [2499], Loss: 0.07251285761594772, Training Accuracy: 97.63
[ Sat Jul 13 22:09:40 2024 ] 	Batch(2500/6809) done. Loss: 0.0652  lr:0.000001
[ Sat Jul 13 22:09:58 2024 ] 	Batch(2600/6809) done. Loss: 0.0066  lr:0.000001
[ Sat Jul 13 22:10:16 2024 ] 	Batch(2700/6809) done. Loss: 0.0658  lr:0.000001
[ Sat Jul 13 22:10:34 2024 ] 	Batch(2800/6809) done. Loss: 0.0369  lr:0.000001
[ Sat Jul 13 22:10:52 2024 ] 	Batch(2900/6809) done. Loss: 0.0124  lr:0.000001
[ Sat Jul 13 22:11:09 2024 ] 
Training: Epoch [14/50], Step [2999], Loss: 0.05274055898189545, Training Accuracy: 97.6
[ Sat Jul 13 22:11:10 2024 ] 	Batch(3000/6809) done. Loss: 0.1537  lr:0.000001
[ Sat Jul 13 22:11:28 2024 ] 	Batch(3100/6809) done. Loss: 0.0729  lr:0.000001
[ Sat Jul 13 22:11:46 2024 ] 	Batch(3200/6809) done. Loss: 0.0400  lr:0.000001
[ Sat Jul 13 22:12:04 2024 ] 	Batch(3300/6809) done. Loss: 0.0476  lr:0.000001
[ Sat Jul 13 22:12:22 2024 ] 	Batch(3400/6809) done. Loss: 0.0165  lr:0.000001
[ Sat Jul 13 22:12:40 2024 ] 
Training: Epoch [14/50], Step [3499], Loss: 0.009410756640136242, Training Accuracy: 97.58928571428571
[ Sat Jul 13 22:12:40 2024 ] 	Batch(3500/6809) done. Loss: 0.0474  lr:0.000001
[ Sat Jul 13 22:12:58 2024 ] 	Batch(3600/6809) done. Loss: 0.3123  lr:0.000001
[ Sat Jul 13 22:13:16 2024 ] 	Batch(3700/6809) done. Loss: 0.0559  lr:0.000001
[ Sat Jul 13 22:13:34 2024 ] 	Batch(3800/6809) done. Loss: 0.0038  lr:0.000001
[ Sat Jul 13 22:13:52 2024 ] 	Batch(3900/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 22:14:10 2024 ] 
Training: Epoch [14/50], Step [3999], Loss: 0.03000834211707115, Training Accuracy: 97.59375
[ Sat Jul 13 22:14:10 2024 ] 	Batch(4000/6809) done. Loss: 0.0158  lr:0.000001
[ Sat Jul 13 22:14:28 2024 ] 	Batch(4100/6809) done. Loss: 0.1429  lr:0.000001
[ Sat Jul 13 22:14:46 2024 ] 	Batch(4200/6809) done. Loss: 0.7037  lr:0.000001
[ Sat Jul 13 22:15:04 2024 ] 	Batch(4300/6809) done. Loss: 0.0053  lr:0.000001
[ Sat Jul 13 22:15:23 2024 ] 	Batch(4400/6809) done. Loss: 0.1255  lr:0.000001
[ Sat Jul 13 22:15:41 2024 ] 
Training: Epoch [14/50], Step [4499], Loss: 0.023461677134037018, Training Accuracy: 97.5611111111111
[ Sat Jul 13 22:15:41 2024 ] 	Batch(4500/6809) done. Loss: 0.0261  lr:0.000001
[ Sat Jul 13 22:15:59 2024 ] 	Batch(4600/6809) done. Loss: 0.0094  lr:0.000001
[ Sat Jul 13 22:16:18 2024 ] 	Batch(4700/6809) done. Loss: 0.0475  lr:0.000001
[ Sat Jul 13 22:16:36 2024 ] 	Batch(4800/6809) done. Loss: 0.3876  lr:0.000001
[ Sat Jul 13 22:16:55 2024 ] 	Batch(4900/6809) done. Loss: 0.1075  lr:0.000001
[ Sat Jul 13 22:17:13 2024 ] 
Training: Epoch [14/50], Step [4999], Loss: 0.018981529399752617, Training Accuracy: 97.57249999999999
[ Sat Jul 13 22:17:13 2024 ] 	Batch(5000/6809) done. Loss: 0.0089  lr:0.000001
[ Sat Jul 13 22:17:31 2024 ] 	Batch(5100/6809) done. Loss: 0.0010  lr:0.000001
[ Sat Jul 13 22:17:50 2024 ] 	Batch(5200/6809) done. Loss: 0.0030  lr:0.000001
[ Sat Jul 13 22:18:08 2024 ] 	Batch(5300/6809) done. Loss: 0.0370  lr:0.000001
[ Sat Jul 13 22:18:26 2024 ] 	Batch(5400/6809) done. Loss: 0.0094  lr:0.000001
[ Sat Jul 13 22:18:45 2024 ] 
Training: Epoch [14/50], Step [5499], Loss: 0.04418511316180229, Training Accuracy: 97.58181818181818
[ Sat Jul 13 22:18:45 2024 ] 	Batch(5500/6809) done. Loss: 0.2964  lr:0.000001
[ Sat Jul 13 22:19:03 2024 ] 	Batch(5600/6809) done. Loss: 0.1455  lr:0.000001
[ Sat Jul 13 22:19:22 2024 ] 	Batch(5700/6809) done. Loss: 0.0724  lr:0.000001
[ Sat Jul 13 22:19:40 2024 ] 	Batch(5800/6809) done. Loss: 0.0781  lr:0.000001
[ Sat Jul 13 22:19:58 2024 ] 	Batch(5900/6809) done. Loss: 0.1016  lr:0.000001
[ Sat Jul 13 22:20:16 2024 ] 
Training: Epoch [14/50], Step [5999], Loss: 0.06511621922254562, Training Accuracy: 97.57708333333333
[ Sat Jul 13 22:20:17 2024 ] 	Batch(6000/6809) done. Loss: 0.0146  lr:0.000001
[ Sat Jul 13 22:20:35 2024 ] 	Batch(6100/6809) done. Loss: 0.1689  lr:0.000001
[ Sat Jul 13 22:20:53 2024 ] 	Batch(6200/6809) done. Loss: 0.0420  lr:0.000001
[ Sat Jul 13 22:21:11 2024 ] 	Batch(6300/6809) done. Loss: 0.0761  lr:0.000001
[ Sat Jul 13 22:21:30 2024 ] 	Batch(6400/6809) done. Loss: 0.0228  lr:0.000001
[ Sat Jul 13 22:21:49 2024 ] 
Training: Epoch [14/50], Step [6499], Loss: 0.004615945741534233, Training Accuracy: 97.58269230769231
[ Sat Jul 13 22:21:49 2024 ] 	Batch(6500/6809) done. Loss: 0.1129  lr:0.000001
[ Sat Jul 13 22:22:07 2024 ] 	Batch(6600/6809) done. Loss: 0.0172  lr:0.000001
[ Sat Jul 13 22:22:26 2024 ] 	Batch(6700/6809) done. Loss: 0.3891  lr:0.000001
[ Sat Jul 13 22:22:45 2024 ] 	Batch(6800/6809) done. Loss: 0.0857  lr:0.000001
[ Sat Jul 13 22:22:46 2024 ] 	Mean training loss: 0.0957.
[ Sat Jul 13 22:22:46 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sat Jul 13 22:22:46 2024 ] Training epoch: 16
[ Sat Jul 13 22:22:47 2024 ] 	Batch(0/6809) done. Loss: 0.0856  lr:0.000001
[ Sat Jul 13 22:23:05 2024 ] 	Batch(100/6809) done. Loss: 0.0718  lr:0.000001
[ Sat Jul 13 22:23:23 2024 ] 	Batch(200/6809) done. Loss: 0.1173  lr:0.000001
[ Sat Jul 13 22:23:40 2024 ] 	Batch(300/6809) done. Loss: 0.0812  lr:0.000001
[ Sat Jul 13 22:23:59 2024 ] 	Batch(400/6809) done. Loss: 0.0213  lr:0.000001
[ Sat Jul 13 22:24:16 2024 ] 
Training: Epoch [15/50], Step [499], Loss: 0.392537921667099, Training Accuracy: 97.89999999999999
[ Sat Jul 13 22:24:17 2024 ] 	Batch(500/6809) done. Loss: 0.0108  lr:0.000001
[ Sat Jul 13 22:24:34 2024 ] 	Batch(600/6809) done. Loss: 0.0177  lr:0.000001
[ Sat Jul 13 22:24:52 2024 ] 	Batch(700/6809) done. Loss: 0.0412  lr:0.000001
[ Sat Jul 13 22:25:10 2024 ] 	Batch(800/6809) done. Loss: 0.0822  lr:0.000001
[ Sat Jul 13 22:25:29 2024 ] 	Batch(900/6809) done. Loss: 0.0201  lr:0.000001
[ Sat Jul 13 22:25:47 2024 ] 
Training: Epoch [15/50], Step [999], Loss: 0.09928158670663834, Training Accuracy: 97.8125
[ Sat Jul 13 22:25:48 2024 ] 	Batch(1000/6809) done. Loss: 0.0212  lr:0.000001
[ Sat Jul 13 22:26:06 2024 ] 	Batch(1100/6809) done. Loss: 0.2866  lr:0.000001
[ Sat Jul 13 22:26:25 2024 ] 	Batch(1200/6809) done. Loss: 0.0375  lr:0.000001
[ Sat Jul 13 22:26:43 2024 ] 	Batch(1300/6809) done. Loss: 0.0103  lr:0.000001
[ Sat Jul 13 22:27:01 2024 ] 	Batch(1400/6809) done. Loss: 0.1909  lr:0.000001
[ Sat Jul 13 22:27:19 2024 ] 
Training: Epoch [15/50], Step [1499], Loss: 0.034105561673641205, Training Accuracy: 97.69166666666666
[ Sat Jul 13 22:27:19 2024 ] 	Batch(1500/6809) done. Loss: 0.0160  lr:0.000001
[ Sat Jul 13 22:27:37 2024 ] 	Batch(1600/6809) done. Loss: 0.1533  lr:0.000001
[ Sat Jul 13 22:27:55 2024 ] 	Batch(1700/6809) done. Loss: 0.1537  lr:0.000001
[ Sat Jul 13 22:28:13 2024 ] 	Batch(1800/6809) done. Loss: 0.1272  lr:0.000001
[ Sat Jul 13 22:28:30 2024 ] 	Batch(1900/6809) done. Loss: 0.0268  lr:0.000001
[ Sat Jul 13 22:28:48 2024 ] 
Training: Epoch [15/50], Step [1999], Loss: 0.027212277054786682, Training Accuracy: 97.725
[ Sat Jul 13 22:28:49 2024 ] 	Batch(2000/6809) done. Loss: 0.0044  lr:0.000001
[ Sat Jul 13 22:29:06 2024 ] 	Batch(2100/6809) done. Loss: 0.0118  lr:0.000001
[ Sat Jul 13 22:29:24 2024 ] 	Batch(2200/6809) done. Loss: 0.0409  lr:0.000001
[ Sat Jul 13 22:29:42 2024 ] 	Batch(2300/6809) done. Loss: 0.0759  lr:0.000001
[ Sat Jul 13 22:30:00 2024 ] 	Batch(2400/6809) done. Loss: 0.0903  lr:0.000001
[ Sat Jul 13 22:30:18 2024 ] 
Training: Epoch [15/50], Step [2499], Loss: 0.19904592633247375, Training Accuracy: 97.68
[ Sat Jul 13 22:30:18 2024 ] 	Batch(2500/6809) done. Loss: 0.1302  lr:0.000001
[ Sat Jul 13 22:30:36 2024 ] 	Batch(2600/6809) done. Loss: 0.0421  lr:0.000001
[ Sat Jul 13 22:30:54 2024 ] 	Batch(2700/6809) done. Loss: 0.0620  lr:0.000001
[ Sat Jul 13 22:31:12 2024 ] 	Batch(2800/6809) done. Loss: 0.0174  lr:0.000001
[ Sat Jul 13 22:31:30 2024 ] 	Batch(2900/6809) done. Loss: 0.9499  lr:0.000001
[ Sat Jul 13 22:31:48 2024 ] 
Training: Epoch [15/50], Step [2999], Loss: 0.008314693346619606, Training Accuracy: 97.6625
[ Sat Jul 13 22:31:48 2024 ] 	Batch(3000/6809) done. Loss: 0.2341  lr:0.000001
[ Sat Jul 13 22:32:06 2024 ] 	Batch(3100/6809) done. Loss: 0.1035  lr:0.000001
[ Sat Jul 13 22:32:24 2024 ] 	Batch(3200/6809) done. Loss: 0.1544  lr:0.000001
[ Sat Jul 13 22:32:41 2024 ] 	Batch(3300/6809) done. Loss: 0.2513  lr:0.000001
[ Sat Jul 13 22:32:59 2024 ] 	Batch(3400/6809) done. Loss: 0.0471  lr:0.000001
[ Sat Jul 13 22:33:17 2024 ] 
Training: Epoch [15/50], Step [3499], Loss: 0.5531116724014282, Training Accuracy: 97.63571428571429
[ Sat Jul 13 22:33:17 2024 ] 	Batch(3500/6809) done. Loss: 0.4264  lr:0.000001
[ Sat Jul 13 22:33:35 2024 ] 	Batch(3600/6809) done. Loss: 0.0962  lr:0.000001
[ Sat Jul 13 22:33:53 2024 ] 	Batch(3700/6809) done. Loss: 0.0098  lr:0.000001
[ Sat Jul 13 22:34:11 2024 ] 	Batch(3800/6809) done. Loss: 0.0686  lr:0.000001
[ Sat Jul 13 22:34:29 2024 ] 	Batch(3900/6809) done. Loss: 0.0370  lr:0.000001
[ Sat Jul 13 22:34:47 2024 ] 
Training: Epoch [15/50], Step [3999], Loss: 0.16163566708564758, Training Accuracy: 97.58125
[ Sat Jul 13 22:34:47 2024 ] 	Batch(4000/6809) done. Loss: 0.0176  lr:0.000001
[ Sat Jul 13 22:35:05 2024 ] 	Batch(4100/6809) done. Loss: 0.3701  lr:0.000001
[ Sat Jul 13 22:35:23 2024 ] 	Batch(4200/6809) done. Loss: 0.1906  lr:0.000001
[ Sat Jul 13 22:35:41 2024 ] 	Batch(4300/6809) done. Loss: 0.0164  lr:0.000001
[ Sat Jul 13 22:35:59 2024 ] 	Batch(4400/6809) done. Loss: 0.0036  lr:0.000001
[ Sat Jul 13 22:36:16 2024 ] 
Training: Epoch [15/50], Step [4499], Loss: 0.030019991099834442, Training Accuracy: 97.59166666666667
[ Sat Jul 13 22:36:17 2024 ] 	Batch(4500/6809) done. Loss: 0.4659  lr:0.000001
[ Sat Jul 13 22:36:34 2024 ] 	Batch(4600/6809) done. Loss: 0.0536  lr:0.000001
[ Sat Jul 13 22:36:52 2024 ] 	Batch(4700/6809) done. Loss: 0.0180  lr:0.000001
[ Sat Jul 13 22:37:10 2024 ] 	Batch(4800/6809) done. Loss: 0.1141  lr:0.000001
[ Sat Jul 13 22:37:29 2024 ] 	Batch(4900/6809) done. Loss: 0.0274  lr:0.000001
[ Sat Jul 13 22:37:47 2024 ] 
Training: Epoch [15/50], Step [4999], Loss: 0.024914823472499847, Training Accuracy: 97.6225
[ Sat Jul 13 22:37:48 2024 ] 	Batch(5000/6809) done. Loss: 0.0641  lr:0.000001
[ Sat Jul 13 22:38:06 2024 ] 	Batch(5100/6809) done. Loss: 0.0059  lr:0.000001
[ Sat Jul 13 22:38:25 2024 ] 	Batch(5200/6809) done. Loss: 0.0177  lr:0.000001
[ Sat Jul 13 22:38:43 2024 ] 	Batch(5300/6809) done. Loss: 0.2623  lr:0.000001
[ Sat Jul 13 22:39:01 2024 ] 	Batch(5400/6809) done. Loss: 0.0144  lr:0.000001
[ Sat Jul 13 22:39:18 2024 ] 
Training: Epoch [15/50], Step [5499], Loss: 0.3556901216506958, Training Accuracy: 97.63409090909092
[ Sat Jul 13 22:39:18 2024 ] 	Batch(5500/6809) done. Loss: 0.0147  lr:0.000001
[ Sat Jul 13 22:39:37 2024 ] 	Batch(5600/6809) done. Loss: 0.0399  lr:0.000001
[ Sat Jul 13 22:39:54 2024 ] 	Batch(5700/6809) done. Loss: 0.1277  lr:0.000001
[ Sat Jul 13 22:40:12 2024 ] 	Batch(5800/6809) done. Loss: 0.1021  lr:0.000001
[ Sat Jul 13 22:40:30 2024 ] 	Batch(5900/6809) done. Loss: 0.1505  lr:0.000001
[ Sat Jul 13 22:40:48 2024 ] 
Training: Epoch [15/50], Step [5999], Loss: 0.30325376987457275, Training Accuracy: 97.65
[ Sat Jul 13 22:40:48 2024 ] 	Batch(6000/6809) done. Loss: 0.0496  lr:0.000001
[ Sat Jul 13 22:41:06 2024 ] 	Batch(6100/6809) done. Loss: 0.0745  lr:0.000001
[ Sat Jul 13 22:41:24 2024 ] 	Batch(6200/6809) done. Loss: 0.1139  lr:0.000001
[ Sat Jul 13 22:41:42 2024 ] 	Batch(6300/6809) done. Loss: 0.0570  lr:0.000001
[ Sat Jul 13 22:42:00 2024 ] 	Batch(6400/6809) done. Loss: 0.0112  lr:0.000001
[ Sat Jul 13 22:42:18 2024 ] 
Training: Epoch [15/50], Step [6499], Loss: 0.011484015733003616, Training Accuracy: 97.63269230769231
[ Sat Jul 13 22:42:18 2024 ] 	Batch(6500/6809) done. Loss: 0.0211  lr:0.000001
[ Sat Jul 13 22:42:36 2024 ] 	Batch(6600/6809) done. Loss: 0.0282  lr:0.000001
[ Sat Jul 13 22:42:54 2024 ] 	Batch(6700/6809) done. Loss: 0.0567  lr:0.000001
[ Sat Jul 13 22:43:12 2024 ] 	Batch(6800/6809) done. Loss: 0.0326  lr:0.000001
[ Sat Jul 13 22:43:13 2024 ] 	Mean training loss: 0.0937.
[ Sat Jul 13 22:43:13 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 22:43:13 2024 ] Training epoch: 17
[ Sat Jul 13 22:43:14 2024 ] 	Batch(0/6809) done. Loss: 0.0239  lr:0.000001
[ Sat Jul 13 22:43:32 2024 ] 	Batch(100/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 22:43:50 2024 ] 	Batch(200/6809) done. Loss: 0.0094  lr:0.000001
[ Sat Jul 13 22:44:08 2024 ] 	Batch(300/6809) done. Loss: 0.2349  lr:0.000001
[ Sat Jul 13 22:44:26 2024 ] 	Batch(400/6809) done. Loss: 0.0562  lr:0.000001
[ Sat Jul 13 22:44:43 2024 ] 
Training: Epoch [16/50], Step [499], Loss: 0.03188720718026161, Training Accuracy: 97.6
[ Sat Jul 13 22:44:44 2024 ] 	Batch(500/6809) done. Loss: 0.1141  lr:0.000001
[ Sat Jul 13 22:45:01 2024 ] 	Batch(600/6809) done. Loss: 0.0270  lr:0.000001
[ Sat Jul 13 22:45:19 2024 ] 	Batch(700/6809) done. Loss: 0.0195  lr:0.000001
[ Sat Jul 13 22:45:37 2024 ] 	Batch(800/6809) done. Loss: 0.0042  lr:0.000001
[ Sat Jul 13 22:45:55 2024 ] 	Batch(900/6809) done. Loss: 0.0241  lr:0.000001
[ Sat Jul 13 22:46:13 2024 ] 
Training: Epoch [16/50], Step [999], Loss: 0.021755842491984367, Training Accuracy: 97.6375
[ Sat Jul 13 22:46:13 2024 ] 	Batch(1000/6809) done. Loss: 0.1906  lr:0.000001
[ Sat Jul 13 22:46:31 2024 ] 	Batch(1100/6809) done. Loss: 0.0534  lr:0.000001
[ Sat Jul 13 22:46:49 2024 ] 	Batch(1200/6809) done. Loss: 0.0569  lr:0.000001
[ Sat Jul 13 22:47:07 2024 ] 	Batch(1300/6809) done. Loss: 0.0453  lr:0.000001
[ Sat Jul 13 22:47:25 2024 ] 	Batch(1400/6809) done. Loss: 0.0689  lr:0.000001
[ Sat Jul 13 22:47:43 2024 ] 
Training: Epoch [16/50], Step [1499], Loss: 0.03149929642677307, Training Accuracy: 97.6
[ Sat Jul 13 22:47:43 2024 ] 	Batch(1500/6809) done. Loss: 0.1572  lr:0.000001
[ Sat Jul 13 22:48:01 2024 ] 	Batch(1600/6809) done. Loss: 0.0184  lr:0.000001
[ Sat Jul 13 22:48:19 2024 ] 	Batch(1700/6809) done. Loss: 0.0824  lr:0.000001
[ Sat Jul 13 22:48:37 2024 ] 	Batch(1800/6809) done. Loss: 0.0276  lr:0.000001
[ Sat Jul 13 22:48:55 2024 ] 	Batch(1900/6809) done. Loss: 0.0061  lr:0.000001
[ Sat Jul 13 22:49:12 2024 ] 
Training: Epoch [16/50], Step [1999], Loss: 0.07899326086044312, Training Accuracy: 97.6125
[ Sat Jul 13 22:49:13 2024 ] 	Batch(2000/6809) done. Loss: 0.4422  lr:0.000001
[ Sat Jul 13 22:49:30 2024 ] 	Batch(2100/6809) done. Loss: 0.2038  lr:0.000001
[ Sat Jul 13 22:49:48 2024 ] 	Batch(2200/6809) done. Loss: 0.0079  lr:0.000001
[ Sat Jul 13 22:50:06 2024 ] 	Batch(2300/6809) done. Loss: 0.0474  lr:0.000001
[ Sat Jul 13 22:50:24 2024 ] 	Batch(2400/6809) done. Loss: 0.0281  lr:0.000001
[ Sat Jul 13 22:50:42 2024 ] 
Training: Epoch [16/50], Step [2499], Loss: 0.06885437667369843, Training Accuracy: 97.605
[ Sat Jul 13 22:50:42 2024 ] 	Batch(2500/6809) done. Loss: 0.0773  lr:0.000001
[ Sat Jul 13 22:51:00 2024 ] 	Batch(2600/6809) done. Loss: 0.0355  lr:0.000001
[ Sat Jul 13 22:51:18 2024 ] 	Batch(2700/6809) done. Loss: 0.0306  lr:0.000001
[ Sat Jul 13 22:51:36 2024 ] 	Batch(2800/6809) done. Loss: 0.2591  lr:0.000001
[ Sat Jul 13 22:51:54 2024 ] 	Batch(2900/6809) done. Loss: 0.0392  lr:0.000001
[ Sat Jul 13 22:52:11 2024 ] 
Training: Epoch [16/50], Step [2999], Loss: 0.02883196994662285, Training Accuracy: 97.54166666666667
[ Sat Jul 13 22:52:12 2024 ] 	Batch(3000/6809) done. Loss: 0.0116  lr:0.000001
[ Sat Jul 13 22:52:30 2024 ] 	Batch(3100/6809) done. Loss: 0.0111  lr:0.000001
[ Sat Jul 13 22:52:48 2024 ] 	Batch(3200/6809) done. Loss: 0.2264  lr:0.000001
[ Sat Jul 13 22:53:05 2024 ] 	Batch(3300/6809) done. Loss: 0.1256  lr:0.000001
[ Sat Jul 13 22:53:23 2024 ] 	Batch(3400/6809) done. Loss: 0.1726  lr:0.000001
[ Sat Jul 13 22:53:41 2024 ] 
Training: Epoch [16/50], Step [3499], Loss: 0.09384289383888245, Training Accuracy: 97.53214285714286
[ Sat Jul 13 22:53:41 2024 ] 	Batch(3500/6809) done. Loss: 0.1464  lr:0.000001
[ Sat Jul 13 22:53:59 2024 ] 	Batch(3600/6809) done. Loss: 0.0187  lr:0.000001
[ Sat Jul 13 22:54:17 2024 ] 	Batch(3700/6809) done. Loss: 0.0124  lr:0.000001
[ Sat Jul 13 22:54:35 2024 ] 	Batch(3800/6809) done. Loss: 0.0212  lr:0.000001
[ Sat Jul 13 22:54:53 2024 ] 	Batch(3900/6809) done. Loss: 0.0482  lr:0.000001
[ Sat Jul 13 22:55:11 2024 ] 
Training: Epoch [16/50], Step [3999], Loss: 0.025995908305048943, Training Accuracy: 97.48125
[ Sat Jul 13 22:55:11 2024 ] 	Batch(4000/6809) done. Loss: 0.5905  lr:0.000001
[ Sat Jul 13 22:55:29 2024 ] 	Batch(4100/6809) done. Loss: 0.0492  lr:0.000001
[ Sat Jul 13 22:55:47 2024 ] 	Batch(4200/6809) done. Loss: 0.0738  lr:0.000001
[ Sat Jul 13 22:56:05 2024 ] 	Batch(4300/6809) done. Loss: 0.0243  lr:0.000001
[ Sat Jul 13 22:56:23 2024 ] 	Batch(4400/6809) done. Loss: 0.0701  lr:0.000001
[ Sat Jul 13 22:56:41 2024 ] 
Training: Epoch [16/50], Step [4499], Loss: 0.0388503260910511, Training Accuracy: 97.48333333333333
[ Sat Jul 13 22:56:41 2024 ] 	Batch(4500/6809) done. Loss: 0.1548  lr:0.000001
[ Sat Jul 13 22:56:59 2024 ] 	Batch(4600/6809) done. Loss: 0.0153  lr:0.000001
[ Sat Jul 13 22:57:17 2024 ] 	Batch(4700/6809) done. Loss: 0.0599  lr:0.000001
[ Sat Jul 13 22:57:35 2024 ] 	Batch(4800/6809) done. Loss: 0.1840  lr:0.000001
[ Sat Jul 13 22:57:53 2024 ] 	Batch(4900/6809) done. Loss: 0.1419  lr:0.000001
[ Sat Jul 13 22:58:10 2024 ] 
Training: Epoch [16/50], Step [4999], Loss: 0.05804567039012909, Training Accuracy: 97.55
[ Sat Jul 13 22:58:10 2024 ] 	Batch(5000/6809) done. Loss: 0.0205  lr:0.000001
[ Sat Jul 13 22:58:28 2024 ] 	Batch(5100/6809) done. Loss: 0.0746  lr:0.000001
[ Sat Jul 13 22:58:46 2024 ] 	Batch(5200/6809) done. Loss: 0.3458  lr:0.000001
[ Sat Jul 13 22:59:05 2024 ] 	Batch(5300/6809) done. Loss: 0.1522  lr:0.000001
[ Sat Jul 13 22:59:23 2024 ] 	Batch(5400/6809) done. Loss: 0.0154  lr:0.000001
[ Sat Jul 13 22:59:40 2024 ] 
Training: Epoch [16/50], Step [5499], Loss: 0.12245533615350723, Training Accuracy: 97.53863636363637
[ Sat Jul 13 22:59:40 2024 ] 	Batch(5500/6809) done. Loss: 0.3719  lr:0.000001
[ Sat Jul 13 22:59:58 2024 ] 	Batch(5600/6809) done. Loss: 0.1055  lr:0.000001
[ Sat Jul 13 23:00:16 2024 ] 	Batch(5700/6809) done. Loss: 0.1546  lr:0.000001
[ Sat Jul 13 23:00:34 2024 ] 	Batch(5800/6809) done. Loss: 0.0088  lr:0.000001
[ Sat Jul 13 23:00:52 2024 ] 	Batch(5900/6809) done. Loss: 0.0463  lr:0.000001
[ Sat Jul 13 23:01:10 2024 ] 
Training: Epoch [16/50], Step [5999], Loss: 0.022272581234574318, Training Accuracy: 97.5375
[ Sat Jul 13 23:01:10 2024 ] 	Batch(6000/6809) done. Loss: 0.0183  lr:0.000001
[ Sat Jul 13 23:01:28 2024 ] 	Batch(6100/6809) done. Loss: 0.0078  lr:0.000001
[ Sat Jul 13 23:01:46 2024 ] 	Batch(6200/6809) done. Loss: 0.0020  lr:0.000001
[ Sat Jul 13 23:02:04 2024 ] 	Batch(6300/6809) done. Loss: 0.2799  lr:0.000001
[ Sat Jul 13 23:02:22 2024 ] 	Batch(6400/6809) done. Loss: 0.0084  lr:0.000001
[ Sat Jul 13 23:02:40 2024 ] 
Training: Epoch [16/50], Step [6499], Loss: 0.07856930792331696, Training Accuracy: 97.53653846153846
[ Sat Jul 13 23:02:40 2024 ] 	Batch(6500/6809) done. Loss: 0.0432  lr:0.000001
[ Sat Jul 13 23:02:58 2024 ] 	Batch(6600/6809) done. Loss: 0.0246  lr:0.000001
[ Sat Jul 13 23:03:16 2024 ] 	Batch(6700/6809) done. Loss: 0.0984  lr:0.000001
[ Sat Jul 13 23:03:34 2024 ] 	Batch(6800/6809) done. Loss: 0.1857  lr:0.000001
[ Sat Jul 13 23:03:36 2024 ] 	Mean training loss: 0.0945.
[ Sat Jul 13 23:03:36 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 23:03:36 2024 ] Training epoch: 18
[ Sat Jul 13 23:03:36 2024 ] 	Batch(0/6809) done. Loss: 0.0128  lr:0.000001
[ Sat Jul 13 23:03:54 2024 ] 	Batch(100/6809) done. Loss: 0.0083  lr:0.000001
[ Sat Jul 13 23:04:12 2024 ] 	Batch(200/6809) done. Loss: 0.1953  lr:0.000001
[ Sat Jul 13 23:04:30 2024 ] 	Batch(300/6809) done. Loss: 0.0064  lr:0.000001
[ Sat Jul 13 23:04:48 2024 ] 	Batch(400/6809) done. Loss: 0.0101  lr:0.000001
[ Sat Jul 13 23:05:06 2024 ] 
Training: Epoch [17/50], Step [499], Loss: 0.10398315638303757, Training Accuracy: 98.075
[ Sat Jul 13 23:05:06 2024 ] 	Batch(500/6809) done. Loss: 0.0041  lr:0.000001
[ Sat Jul 13 23:05:24 2024 ] 	Batch(600/6809) done. Loss: 0.0278  lr:0.000001
[ Sat Jul 13 23:05:42 2024 ] 	Batch(700/6809) done. Loss: 0.1143  lr:0.000001
[ Sat Jul 13 23:06:00 2024 ] 	Batch(800/6809) done. Loss: 0.0066  lr:0.000001
[ Sat Jul 13 23:06:18 2024 ] 	Batch(900/6809) done. Loss: 0.1099  lr:0.000001
[ Sat Jul 13 23:06:35 2024 ] 
Training: Epoch [17/50], Step [999], Loss: 0.11669214069843292, Training Accuracy: 97.6625
[ Sat Jul 13 23:06:36 2024 ] 	Batch(1000/6809) done. Loss: 0.0302  lr:0.000001
[ Sat Jul 13 23:06:53 2024 ] 	Batch(1100/6809) done. Loss: 0.0105  lr:0.000001
[ Sat Jul 13 23:07:12 2024 ] 	Batch(1200/6809) done. Loss: 0.0375  lr:0.000001
[ Sat Jul 13 23:07:30 2024 ] 	Batch(1300/6809) done. Loss: 0.0101  lr:0.000001
[ Sat Jul 13 23:07:49 2024 ] 	Batch(1400/6809) done. Loss: 0.0087  lr:0.000001
[ Sat Jul 13 23:08:07 2024 ] 
Training: Epoch [17/50], Step [1499], Loss: 0.42683711647987366, Training Accuracy: 97.75833333333334
[ Sat Jul 13 23:08:07 2024 ] 	Batch(1500/6809) done. Loss: 0.1347  lr:0.000001
[ Sat Jul 13 23:08:26 2024 ] 	Batch(1600/6809) done. Loss: 0.0354  lr:0.000001
[ Sat Jul 13 23:08:44 2024 ] 	Batch(1700/6809) done. Loss: 0.0256  lr:0.000001
[ Sat Jul 13 23:09:02 2024 ] 	Batch(1800/6809) done. Loss: 0.0724  lr:0.000001
[ Sat Jul 13 23:09:20 2024 ] 	Batch(1900/6809) done. Loss: 0.0192  lr:0.000001
[ Sat Jul 13 23:09:37 2024 ] 
Training: Epoch [17/50], Step [1999], Loss: 0.024682501330971718, Training Accuracy: 97.79375
[ Sat Jul 13 23:09:38 2024 ] 	Batch(2000/6809) done. Loss: 0.0721  lr:0.000001
[ Sat Jul 13 23:09:56 2024 ] 	Batch(2100/6809) done. Loss: 0.0802  lr:0.000001
[ Sat Jul 13 23:10:15 2024 ] 	Batch(2200/6809) done. Loss: 0.0678  lr:0.000001
[ Sat Jul 13 23:10:33 2024 ] 	Batch(2300/6809) done. Loss: 0.5204  lr:0.000001
[ Sat Jul 13 23:10:52 2024 ] 	Batch(2400/6809) done. Loss: 0.0222  lr:0.000001
[ Sat Jul 13 23:11:10 2024 ] 
Training: Epoch [17/50], Step [2499], Loss: 0.3645043969154358, Training Accuracy: 97.725
[ Sat Jul 13 23:11:10 2024 ] 	Batch(2500/6809) done. Loss: 0.0255  lr:0.000001
[ Sat Jul 13 23:11:28 2024 ] 	Batch(2600/6809) done. Loss: 0.0049  lr:0.000001
[ Sat Jul 13 23:11:46 2024 ] 	Batch(2700/6809) done. Loss: 0.0469  lr:0.000001
[ Sat Jul 13 23:12:04 2024 ] 	Batch(2800/6809) done. Loss: 0.0587  lr:0.000001
[ Sat Jul 13 23:12:23 2024 ] 	Batch(2900/6809) done. Loss: 0.0686  lr:0.000001
[ Sat Jul 13 23:12:41 2024 ] 
Training: Epoch [17/50], Step [2999], Loss: 0.11410953849554062, Training Accuracy: 97.73333333333333
[ Sat Jul 13 23:12:41 2024 ] 	Batch(3000/6809) done. Loss: 0.0329  lr:0.000001
[ Sat Jul 13 23:13:00 2024 ] 	Batch(3100/6809) done. Loss: 0.0067  lr:0.000001
[ Sat Jul 13 23:13:19 2024 ] 	Batch(3200/6809) done. Loss: 0.0951  lr:0.000001
[ Sat Jul 13 23:13:37 2024 ] 	Batch(3300/6809) done. Loss: 0.1462  lr:0.000001
[ Sat Jul 13 23:13:56 2024 ] 	Batch(3400/6809) done. Loss: 0.0223  lr:0.000001
[ Sat Jul 13 23:14:14 2024 ] 
Training: Epoch [17/50], Step [3499], Loss: 0.04175909236073494, Training Accuracy: 97.72142857142858
[ Sat Jul 13 23:14:14 2024 ] 	Batch(3500/6809) done. Loss: 0.0507  lr:0.000001
[ Sat Jul 13 23:14:33 2024 ] 	Batch(3600/6809) done. Loss: 0.0752  lr:0.000001
[ Sat Jul 13 23:14:51 2024 ] 	Batch(3700/6809) done. Loss: 0.3653  lr:0.000001
[ Sat Jul 13 23:15:09 2024 ] 	Batch(3800/6809) done. Loss: 0.0153  lr:0.000001
[ Sat Jul 13 23:15:27 2024 ] 	Batch(3900/6809) done. Loss: 0.0574  lr:0.000001
[ Sat Jul 13 23:15:44 2024 ] 
Training: Epoch [17/50], Step [3999], Loss: 0.062114592641592026, Training Accuracy: 97.7375
[ Sat Jul 13 23:15:45 2024 ] 	Batch(4000/6809) done. Loss: 0.0179  lr:0.000001
[ Sat Jul 13 23:16:02 2024 ] 	Batch(4100/6809) done. Loss: 0.0752  lr:0.000001
[ Sat Jul 13 23:16:20 2024 ] 	Batch(4200/6809) done. Loss: 0.4221  lr:0.000001
[ Sat Jul 13 23:16:38 2024 ] 	Batch(4300/6809) done. Loss: 0.2620  lr:0.000001
[ Sat Jul 13 23:16:56 2024 ] 	Batch(4400/6809) done. Loss: 0.2122  lr:0.000001
[ Sat Jul 13 23:17:15 2024 ] 
Training: Epoch [17/50], Step [4499], Loss: 0.17005014419555664, Training Accuracy: 97.73055555555555
[ Sat Jul 13 23:17:15 2024 ] 	Batch(4500/6809) done. Loss: 0.1048  lr:0.000001
[ Sat Jul 13 23:17:33 2024 ] 	Batch(4600/6809) done. Loss: 0.0209  lr:0.000001
[ Sat Jul 13 23:17:52 2024 ] 	Batch(4700/6809) done. Loss: 0.0789  lr:0.000001
[ Sat Jul 13 23:18:11 2024 ] 	Batch(4800/6809) done. Loss: 0.2859  lr:0.000001
[ Sat Jul 13 23:18:29 2024 ] 	Batch(4900/6809) done. Loss: 0.1568  lr:0.000001
[ Sat Jul 13 23:18:46 2024 ] 
Training: Epoch [17/50], Step [4999], Loss: 0.021017752587795258, Training Accuracy: 97.745
[ Sat Jul 13 23:18:46 2024 ] 	Batch(5000/6809) done. Loss: 0.0451  lr:0.000001
[ Sat Jul 13 23:19:04 2024 ] 	Batch(5100/6809) done. Loss: 0.2187  lr:0.000001
[ Sat Jul 13 23:19:23 2024 ] 	Batch(5200/6809) done. Loss: 0.1277  lr:0.000001
[ Sat Jul 13 23:19:41 2024 ] 	Batch(5300/6809) done. Loss: 0.1077  lr:0.000001
[ Sat Jul 13 23:19:58 2024 ] 	Batch(5400/6809) done. Loss: 0.0317  lr:0.000001
[ Sat Jul 13 23:20:16 2024 ] 
Training: Epoch [17/50], Step [5499], Loss: 0.021109748631715775, Training Accuracy: 97.73181818181818
[ Sat Jul 13 23:20:16 2024 ] 	Batch(5500/6809) done. Loss: 0.0678  lr:0.000001
[ Sat Jul 13 23:20:34 2024 ] 	Batch(5600/6809) done. Loss: 0.1868  lr:0.000001
[ Sat Jul 13 23:20:52 2024 ] 	Batch(5700/6809) done. Loss: 0.0211  lr:0.000001
[ Sat Jul 13 23:21:10 2024 ] 	Batch(5800/6809) done. Loss: 0.0298  lr:0.000001
[ Sat Jul 13 23:21:28 2024 ] 	Batch(5900/6809) done. Loss: 0.6745  lr:0.000001
[ Sat Jul 13 23:21:46 2024 ] 
Training: Epoch [17/50], Step [5999], Loss: 0.07105952501296997, Training Accuracy: 97.69791666666666
[ Sat Jul 13 23:21:46 2024 ] 	Batch(6000/6809) done. Loss: 0.0134  lr:0.000001
[ Sat Jul 13 23:22:04 2024 ] 	Batch(6100/6809) done. Loss: 0.0112  lr:0.000001
[ Sat Jul 13 23:22:22 2024 ] 	Batch(6200/6809) done. Loss: 0.0559  lr:0.000001
[ Sat Jul 13 23:22:40 2024 ] 	Batch(6300/6809) done. Loss: 0.1726  lr:0.000001
[ Sat Jul 13 23:22:58 2024 ] 	Batch(6400/6809) done. Loss: 0.0359  lr:0.000001
[ Sat Jul 13 23:23:16 2024 ] 
Training: Epoch [17/50], Step [6499], Loss: 0.15734663605690002, Training Accuracy: 97.66538461538462
[ Sat Jul 13 23:23:16 2024 ] 	Batch(6500/6809) done. Loss: 0.0603  lr:0.000001
[ Sat Jul 13 23:23:34 2024 ] 	Batch(6600/6809) done. Loss: 0.0549  lr:0.000001
[ Sat Jul 13 23:23:52 2024 ] 	Batch(6700/6809) done. Loss: 0.0692  lr:0.000001
[ Sat Jul 13 23:24:10 2024 ] 	Batch(6800/6809) done. Loss: 0.0326  lr:0.000001
[ Sat Jul 13 23:24:11 2024 ] 	Mean training loss: 0.0960.
[ Sat Jul 13 23:24:11 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 23:24:12 2024 ] Training epoch: 19
[ Sat Jul 13 23:24:12 2024 ] 	Batch(0/6809) done. Loss: 0.0349  lr:0.000001
[ Sat Jul 13 23:24:30 2024 ] 	Batch(100/6809) done. Loss: 0.0340  lr:0.000001
[ Sat Jul 13 23:24:48 2024 ] 	Batch(200/6809) done. Loss: 0.2247  lr:0.000001
[ Sat Jul 13 23:25:06 2024 ] 	Batch(300/6809) done. Loss: 0.0709  lr:0.000001
[ Sat Jul 13 23:25:24 2024 ] 	Batch(400/6809) done. Loss: 0.3099  lr:0.000001
[ Sat Jul 13 23:25:41 2024 ] 
Training: Epoch [18/50], Step [499], Loss: 0.029523298144340515, Training Accuracy: 97.7
[ Sat Jul 13 23:25:42 2024 ] 	Batch(500/6809) done. Loss: 0.1361  lr:0.000001
[ Sat Jul 13 23:26:00 2024 ] 	Batch(600/6809) done. Loss: 0.0510  lr:0.000001
[ Sat Jul 13 23:26:18 2024 ] 	Batch(700/6809) done. Loss: 0.0334  lr:0.000001
[ Sat Jul 13 23:26:36 2024 ] 	Batch(800/6809) done. Loss: 0.0046  lr:0.000001
[ Sat Jul 13 23:26:54 2024 ] 	Batch(900/6809) done. Loss: 0.1056  lr:0.000001
[ Sat Jul 13 23:27:12 2024 ] 
Training: Epoch [18/50], Step [999], Loss: 0.07465381920337677, Training Accuracy: 97.9375
[ Sat Jul 13 23:27:12 2024 ] 	Batch(1000/6809) done. Loss: 0.0087  lr:0.000001
[ Sat Jul 13 23:27:30 2024 ] 	Batch(1100/6809) done. Loss: 0.0098  lr:0.000001
[ Sat Jul 13 23:27:48 2024 ] 	Batch(1200/6809) done. Loss: 0.1573  lr:0.000001
[ Sat Jul 13 23:28:05 2024 ] 	Batch(1300/6809) done. Loss: 0.0073  lr:0.000001
[ Sat Jul 13 23:28:23 2024 ] 	Batch(1400/6809) done. Loss: 0.0663  lr:0.000001
[ Sat Jul 13 23:28:41 2024 ] 
Training: Epoch [18/50], Step [1499], Loss: 0.10997692495584488, Training Accuracy: 97.95833333333334
[ Sat Jul 13 23:28:41 2024 ] 	Batch(1500/6809) done. Loss: 0.0431  lr:0.000001
[ Sat Jul 13 23:28:59 2024 ] 	Batch(1600/6809) done. Loss: 0.0185  lr:0.000001
[ Sat Jul 13 23:29:17 2024 ] 	Batch(1700/6809) done. Loss: 0.0158  lr:0.000001
[ Sat Jul 13 23:29:35 2024 ] 	Batch(1800/6809) done. Loss: 0.3102  lr:0.000001
[ Sat Jul 13 23:29:53 2024 ] 	Batch(1900/6809) done. Loss: 0.1000  lr:0.000001
[ Sat Jul 13 23:30:11 2024 ] 
Training: Epoch [18/50], Step [1999], Loss: 0.024236038327217102, Training Accuracy: 97.91250000000001
[ Sat Jul 13 23:30:11 2024 ] 	Batch(2000/6809) done. Loss: 0.0071  lr:0.000001
[ Sat Jul 13 23:30:29 2024 ] 	Batch(2100/6809) done. Loss: 0.0099  lr:0.000001
[ Sat Jul 13 23:30:47 2024 ] 	Batch(2200/6809) done. Loss: 0.4168  lr:0.000001
[ Sat Jul 13 23:31:04 2024 ] 	Batch(2300/6809) done. Loss: 0.0257  lr:0.000001
[ Sat Jul 13 23:31:22 2024 ] 	Batch(2400/6809) done. Loss: 0.0250  lr:0.000001
[ Sat Jul 13 23:31:40 2024 ] 
Training: Epoch [18/50], Step [2499], Loss: 0.09091551601886749, Training Accuracy: 97.84
[ Sat Jul 13 23:31:40 2024 ] 	Batch(2500/6809) done. Loss: 0.0021  lr:0.000001
[ Sat Jul 13 23:31:58 2024 ] 	Batch(2600/6809) done. Loss: 0.0145  lr:0.000001
[ Sat Jul 13 23:32:16 2024 ] 	Batch(2700/6809) done. Loss: 0.0269  lr:0.000001
[ Sat Jul 13 23:32:34 2024 ] 	Batch(2800/6809) done. Loss: 0.0814  lr:0.000001
[ Sat Jul 13 23:32:53 2024 ] 	Batch(2900/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 23:33:11 2024 ] 
Training: Epoch [18/50], Step [2999], Loss: 0.08474957942962646, Training Accuracy: 97.74583333333334
[ Sat Jul 13 23:33:11 2024 ] 	Batch(3000/6809) done. Loss: 0.0528  lr:0.000001
[ Sat Jul 13 23:33:30 2024 ] 	Batch(3100/6809) done. Loss: 0.1832  lr:0.000001
[ Sat Jul 13 23:33:49 2024 ] 	Batch(3200/6809) done. Loss: 0.0940  lr:0.000001
[ Sat Jul 13 23:34:06 2024 ] 	Batch(3300/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 23:34:25 2024 ] 	Batch(3400/6809) done. Loss: 0.0036  lr:0.000001
[ Sat Jul 13 23:34:43 2024 ] 
Training: Epoch [18/50], Step [3499], Loss: 0.207963764667511, Training Accuracy: 97.75
[ Sat Jul 13 23:34:43 2024 ] 	Batch(3500/6809) done. Loss: 0.0142  lr:0.000001
[ Sat Jul 13 23:35:02 2024 ] 	Batch(3600/6809) done. Loss: 0.0668  lr:0.000001
[ Sat Jul 13 23:35:20 2024 ] 	Batch(3700/6809) done. Loss: 0.1005  lr:0.000001
[ Sat Jul 13 23:35:39 2024 ] 	Batch(3800/6809) done. Loss: 0.3330  lr:0.000001
[ Sat Jul 13 23:35:58 2024 ] 	Batch(3900/6809) done. Loss: 0.0064  lr:0.000001
[ Sat Jul 13 23:36:16 2024 ] 
Training: Epoch [18/50], Step [3999], Loss: 0.0037655348423868418, Training Accuracy: 97.746875
[ Sat Jul 13 23:36:16 2024 ] 	Batch(4000/6809) done. Loss: 0.0075  lr:0.000001
[ Sat Jul 13 23:36:35 2024 ] 	Batch(4100/6809) done. Loss: 0.6927  lr:0.000001
[ Sat Jul 13 23:36:53 2024 ] 	Batch(4200/6809) done. Loss: 0.1686  lr:0.000001
[ Sat Jul 13 23:37:12 2024 ] 	Batch(4300/6809) done. Loss: 0.0415  lr:0.000001
[ Sat Jul 13 23:37:31 2024 ] 	Batch(4400/6809) done. Loss: 0.0106  lr:0.000001
[ Sat Jul 13 23:37:48 2024 ] 
Training: Epoch [18/50], Step [4499], Loss: 0.017207687720656395, Training Accuracy: 97.74166666666667
[ Sat Jul 13 23:37:49 2024 ] 	Batch(4500/6809) done. Loss: 0.3495  lr:0.000001
[ Sat Jul 13 23:38:06 2024 ] 	Batch(4600/6809) done. Loss: 0.0119  lr:0.000001
[ Sat Jul 13 23:38:24 2024 ] 	Batch(4700/6809) done. Loss: 0.0177  lr:0.000001
[ Sat Jul 13 23:38:43 2024 ] 	Batch(4800/6809) done. Loss: 0.0433  lr:0.000001
[ Sat Jul 13 23:39:00 2024 ] 	Batch(4900/6809) done. Loss: 0.0058  lr:0.000001
[ Sat Jul 13 23:39:18 2024 ] 
Training: Epoch [18/50], Step [4999], Loss: 0.0391048938035965, Training Accuracy: 97.725
[ Sat Jul 13 23:39:18 2024 ] 	Batch(5000/6809) done. Loss: 0.0922  lr:0.000001
[ Sat Jul 13 23:39:36 2024 ] 	Batch(5100/6809) done. Loss: 0.0508  lr:0.000001
[ Sat Jul 13 23:39:54 2024 ] 	Batch(5200/6809) done. Loss: 0.3816  lr:0.000001
[ Sat Jul 13 23:40:13 2024 ] 	Batch(5300/6809) done. Loss: 0.0150  lr:0.000001
[ Sat Jul 13 23:40:32 2024 ] 	Batch(5400/6809) done. Loss: 0.0067  lr:0.000001
[ Sat Jul 13 23:40:50 2024 ] 
Training: Epoch [18/50], Step [5499], Loss: 0.09911429136991501, Training Accuracy: 97.73409090909091
[ Sat Jul 13 23:40:50 2024 ] 	Batch(5500/6809) done. Loss: 0.1855  lr:0.000001
[ Sat Jul 13 23:41:09 2024 ] 	Batch(5600/6809) done. Loss: 0.0140  lr:0.000001
[ Sat Jul 13 23:41:27 2024 ] 	Batch(5700/6809) done. Loss: 0.0460  lr:0.000001
[ Sat Jul 13 23:41:46 2024 ] 	Batch(5800/6809) done. Loss: 0.0076  lr:0.000001
[ Sat Jul 13 23:42:04 2024 ] 	Batch(5900/6809) done. Loss: 0.0320  lr:0.000001
[ Sat Jul 13 23:42:23 2024 ] 
Training: Epoch [18/50], Step [5999], Loss: 0.06576062738895416, Training Accuracy: 97.71875
[ Sat Jul 13 23:42:23 2024 ] 	Batch(6000/6809) done. Loss: 0.0138  lr:0.000001
[ Sat Jul 13 23:42:41 2024 ] 	Batch(6100/6809) done. Loss: 0.0401  lr:0.000001
[ Sat Jul 13 23:42:59 2024 ] 	Batch(6200/6809) done. Loss: 0.2490  lr:0.000001
[ Sat Jul 13 23:43:17 2024 ] 	Batch(6300/6809) done. Loss: 0.0057  lr:0.000001
[ Sat Jul 13 23:43:35 2024 ] 	Batch(6400/6809) done. Loss: 0.0025  lr:0.000001
[ Sat Jul 13 23:43:53 2024 ] 
Training: Epoch [18/50], Step [6499], Loss: 0.0744585245847702, Training Accuracy: 97.72115384615384
[ Sat Jul 13 23:43:53 2024 ] 	Batch(6500/6809) done. Loss: 0.2220  lr:0.000001
[ Sat Jul 13 23:44:12 2024 ] 	Batch(6600/6809) done. Loss: 0.1507  lr:0.000001
[ Sat Jul 13 23:44:31 2024 ] 	Batch(6700/6809) done. Loss: 0.2690  lr:0.000001
[ Sat Jul 13 23:44:49 2024 ] 	Batch(6800/6809) done. Loss: 0.0393  lr:0.000001
[ Sat Jul 13 23:44:51 2024 ] 	Mean training loss: 0.0928.
[ Sat Jul 13 23:44:51 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sat Jul 13 23:44:51 2024 ] Training epoch: 20
[ Sat Jul 13 23:44:52 2024 ] 	Batch(0/6809) done. Loss: 0.0503  lr:0.000001
[ Sat Jul 13 23:45:10 2024 ] 	Batch(100/6809) done. Loss: 0.0419  lr:0.000001
[ Sat Jul 13 23:45:29 2024 ] 	Batch(200/6809) done. Loss: 0.1559  lr:0.000001
[ Sat Jul 13 23:45:47 2024 ] 	Batch(300/6809) done. Loss: 0.0035  lr:0.000001
[ Sat Jul 13 23:46:06 2024 ] 	Batch(400/6809) done. Loss: 0.1076  lr:0.000001
[ Sat Jul 13 23:46:24 2024 ] 
Training: Epoch [19/50], Step [499], Loss: 0.09831026941537857, Training Accuracy: 97.7
[ Sat Jul 13 23:46:24 2024 ] 	Batch(500/6809) done. Loss: 0.0708  lr:0.000001
[ Sat Jul 13 23:46:43 2024 ] 	Batch(600/6809) done. Loss: 0.0058  lr:0.000001
[ Sat Jul 13 23:47:01 2024 ] 	Batch(700/6809) done. Loss: 0.1226  lr:0.000001
[ Sat Jul 13 23:47:20 2024 ] 	Batch(800/6809) done. Loss: 0.0040  lr:0.000001
[ Sat Jul 13 23:47:39 2024 ] 	Batch(900/6809) done. Loss: 0.0964  lr:0.000001
[ Sat Jul 13 23:47:57 2024 ] 
Training: Epoch [19/50], Step [999], Loss: 0.024717386811971664, Training Accuracy: 97.7
[ Sat Jul 13 23:47:57 2024 ] 	Batch(1000/6809) done. Loss: 0.0155  lr:0.000001
[ Sat Jul 13 23:48:16 2024 ] 	Batch(1100/6809) done. Loss: 0.2426  lr:0.000001
[ Sat Jul 13 23:48:34 2024 ] 	Batch(1200/6809) done. Loss: 0.0126  lr:0.000001
[ Sat Jul 13 23:48:52 2024 ] 	Batch(1300/6809) done. Loss: 0.0057  lr:0.000001
[ Sat Jul 13 23:49:10 2024 ] 	Batch(1400/6809) done. Loss: 0.0603  lr:0.000001
[ Sat Jul 13 23:49:28 2024 ] 
Training: Epoch [19/50], Step [1499], Loss: 0.022102748975157738, Training Accuracy: 97.575
[ Sat Jul 13 23:49:28 2024 ] 	Batch(1500/6809) done. Loss: 0.0078  lr:0.000001
[ Sat Jul 13 23:49:46 2024 ] 	Batch(1600/6809) done. Loss: 0.0451  lr:0.000001
[ Sat Jul 13 23:50:04 2024 ] 	Batch(1700/6809) done. Loss: 0.0230  lr:0.000001
[ Sat Jul 13 23:50:22 2024 ] 	Batch(1800/6809) done. Loss: 0.0099  lr:0.000001
[ Sat Jul 13 23:50:40 2024 ] 	Batch(1900/6809) done. Loss: 0.0101  lr:0.000001
[ Sat Jul 13 23:50:58 2024 ] 
Training: Epoch [19/50], Step [1999], Loss: 0.02756987325847149, Training Accuracy: 97.6
[ Sat Jul 13 23:50:58 2024 ] 	Batch(2000/6809) done. Loss: 0.0195  lr:0.000001
[ Sat Jul 13 23:51:16 2024 ] 	Batch(2100/6809) done. Loss: 0.0636  lr:0.000001
[ Sat Jul 13 23:51:34 2024 ] 	Batch(2200/6809) done. Loss: 0.1578  lr:0.000001
[ Sat Jul 13 23:51:52 2024 ] 	Batch(2300/6809) done. Loss: 0.1206  lr:0.000001
[ Sat Jul 13 23:52:10 2024 ] 	Batch(2400/6809) done. Loss: 0.0022  lr:0.000001
[ Sat Jul 13 23:52:28 2024 ] 
Training: Epoch [19/50], Step [2499], Loss: 0.19434429705142975, Training Accuracy: 97.7
[ Sat Jul 13 23:52:28 2024 ] 	Batch(2500/6809) done. Loss: 0.0536  lr:0.000001
[ Sat Jul 13 23:52:46 2024 ] 	Batch(2600/6809) done. Loss: 0.0708  lr:0.000001
[ Sat Jul 13 23:53:04 2024 ] 	Batch(2700/6809) done. Loss: 0.0221  lr:0.000001
[ Sat Jul 13 23:53:22 2024 ] 	Batch(2800/6809) done. Loss: 0.0313  lr:0.000001
[ Sat Jul 13 23:53:40 2024 ] 	Batch(2900/6809) done. Loss: 0.0676  lr:0.000001
[ Sat Jul 13 23:53:59 2024 ] 
Training: Epoch [19/50], Step [2999], Loss: 0.17006267607212067, Training Accuracy: 97.70416666666667
[ Sat Jul 13 23:53:59 2024 ] 	Batch(3000/6809) done. Loss: 0.0421  lr:0.000001
[ Sat Jul 13 23:54:17 2024 ] 	Batch(3100/6809) done. Loss: 0.1761  lr:0.000001
[ Sat Jul 13 23:54:36 2024 ] 	Batch(3200/6809) done. Loss: 0.0090  lr:0.000001
[ Sat Jul 13 23:54:55 2024 ] 	Batch(3300/6809) done. Loss: 0.2056  lr:0.000001
[ Sat Jul 13 23:55:13 2024 ] 	Batch(3400/6809) done. Loss: 0.0765  lr:0.000001
[ Sat Jul 13 23:55:31 2024 ] 
Training: Epoch [19/50], Step [3499], Loss: 0.1523044854402542, Training Accuracy: 97.66071428571429
[ Sat Jul 13 23:55:32 2024 ] 	Batch(3500/6809) done. Loss: 0.0339  lr:0.000001
[ Sat Jul 13 23:55:50 2024 ] 	Batch(3600/6809) done. Loss: 0.0376  lr:0.000001
[ Sat Jul 13 23:56:09 2024 ] 	Batch(3700/6809) done. Loss: 0.0193  lr:0.000001
[ Sat Jul 13 23:56:27 2024 ] 	Batch(3800/6809) done. Loss: 0.1649  lr:0.000001
[ Sat Jul 13 23:56:45 2024 ] 	Batch(3900/6809) done. Loss: 0.0722  lr:0.000001
[ Sat Jul 13 23:57:03 2024 ] 
Training: Epoch [19/50], Step [3999], Loss: 0.04276075214147568, Training Accuracy: 97.684375
[ Sat Jul 13 23:57:03 2024 ] 	Batch(4000/6809) done. Loss: 0.0284  lr:0.000001
[ Sat Jul 13 23:57:21 2024 ] 	Batch(4100/6809) done. Loss: 0.0718  lr:0.000001
[ Sat Jul 13 23:57:39 2024 ] 	Batch(4200/6809) done. Loss: 0.1129  lr:0.000001
[ Sat Jul 13 23:57:57 2024 ] 	Batch(4300/6809) done. Loss: 0.0779  lr:0.000001
[ Sat Jul 13 23:58:15 2024 ] 	Batch(4400/6809) done. Loss: 0.4036  lr:0.000001
[ Sat Jul 13 23:58:33 2024 ] 
Training: Epoch [19/50], Step [4499], Loss: 0.20095384120941162, Training Accuracy: 97.6861111111111
[ Sat Jul 13 23:58:34 2024 ] 	Batch(4500/6809) done. Loss: 0.0303  lr:0.000001
[ Sat Jul 13 23:58:52 2024 ] 	Batch(4600/6809) done. Loss: 0.0189  lr:0.000001
[ Sat Jul 13 23:59:11 2024 ] 	Batch(4700/6809) done. Loss: 0.5066  lr:0.000001
[ Sat Jul 13 23:59:30 2024 ] 	Batch(4800/6809) done. Loss: 0.0073  lr:0.000001
[ Sat Jul 13 23:59:48 2024 ] 	Batch(4900/6809) done. Loss: 0.0200  lr:0.000001
[ Sun Jul 14 00:00:06 2024 ] 
Training: Epoch [19/50], Step [4999], Loss: 0.15300196409225464, Training Accuracy: 97.69
[ Sun Jul 14 00:00:06 2024 ] 	Batch(5000/6809) done. Loss: 0.0389  lr:0.000001
[ Sun Jul 14 00:00:24 2024 ] 	Batch(5100/6809) done. Loss: 0.0723  lr:0.000001
[ Sun Jul 14 00:00:42 2024 ] 	Batch(5200/6809) done. Loss: 0.0235  lr:0.000001
[ Sun Jul 14 00:01:00 2024 ] 	Batch(5300/6809) done. Loss: 0.1538  lr:0.000001
[ Sun Jul 14 00:01:18 2024 ] 	Batch(5400/6809) done. Loss: 0.0066  lr:0.000001
[ Sun Jul 14 00:01:35 2024 ] 
Training: Epoch [19/50], Step [5499], Loss: 0.09477919340133667, Training Accuracy: 97.63181818181819
[ Sun Jul 14 00:01:36 2024 ] 	Batch(5500/6809) done. Loss: 0.0095  lr:0.000001
[ Sun Jul 14 00:01:54 2024 ] 	Batch(5600/6809) done. Loss: 0.3497  lr:0.000001
[ Sun Jul 14 00:02:12 2024 ] 	Batch(5700/6809) done. Loss: 0.1451  lr:0.000001
[ Sun Jul 14 00:02:31 2024 ] 	Batch(5800/6809) done. Loss: 0.0230  lr:0.000001
[ Sun Jul 14 00:02:50 2024 ] 	Batch(5900/6809) done. Loss: 0.0063  lr:0.000001
[ Sun Jul 14 00:03:08 2024 ] 
Training: Epoch [19/50], Step [5999], Loss: 0.0831780880689621, Training Accuracy: 97.65
[ Sun Jul 14 00:03:08 2024 ] 	Batch(6000/6809) done. Loss: 0.3530  lr:0.000001
[ Sun Jul 14 00:03:26 2024 ] 	Batch(6100/6809) done. Loss: 0.1946  lr:0.000001
[ Sun Jul 14 00:03:44 2024 ] 	Batch(6200/6809) done. Loss: 0.1118  lr:0.000001
[ Sun Jul 14 00:04:02 2024 ] 	Batch(6300/6809) done. Loss: 0.3558  lr:0.000001
[ Sun Jul 14 00:04:20 2024 ] 	Batch(6400/6809) done. Loss: 0.0053  lr:0.000001
[ Sun Jul 14 00:04:38 2024 ] 
Training: Epoch [19/50], Step [6499], Loss: 0.13439886271953583, Training Accuracy: 97.6423076923077
[ Sun Jul 14 00:04:38 2024 ] 	Batch(6500/6809) done. Loss: 0.0583  lr:0.000001
[ Sun Jul 14 00:04:56 2024 ] 	Batch(6600/6809) done. Loss: 0.1495  lr:0.000001
[ Sun Jul 14 00:05:14 2024 ] 	Batch(6700/6809) done. Loss: 0.2305  lr:0.000001
[ Sun Jul 14 00:05:32 2024 ] 	Batch(6800/6809) done. Loss: 0.1712  lr:0.000001
[ Sun Jul 14 00:05:33 2024 ] 	Mean training loss: 0.0959.
[ Sun Jul 14 00:05:33 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 00:05:34 2024 ] Eval epoch: 20
[ Sun Jul 14 00:11:06 2024 ] 	Mean val loss of 7435 batches: 1.0524935411888985.
[ Sun Jul 14 00:11:06 2024 ] 
Validation: Epoch [19/50], Samples [47884.0/59477], Loss: 0.3296315670013428, Validation Accuracy: 80.50843183079174
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 1 : 366 / 500 = 73 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 2 : 427 / 499 = 85 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 3 : 408 / 500 = 81 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 4 : 428 / 502 = 85 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 5 : 469 / 502 = 93 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 6 : 434 / 502 = 86 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 7 : 467 / 497 = 93 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 8 : 483 / 498 = 96 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 9 : 400 / 500 = 80 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 10 : 205 / 500 = 41 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 11 : 181 / 498 = 36 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 12 : 430 / 499 = 86 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 13 : 485 / 502 = 96 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 14 : 479 / 504 = 95 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 15 : 429 / 502 = 85 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 16 : 376 / 502 = 74 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 17 : 442 / 504 = 87 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 18 : 418 / 504 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 19 : 463 / 502 = 92 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 20 : 466 / 502 = 92 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 21 : 475 / 503 = 94 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 22 : 435 / 504 = 86 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 23 : 445 / 503 = 88 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 24 : 401 / 504 = 79 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 25 : 491 / 504 = 97 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 26 : 470 / 504 = 93 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 27 : 413 / 501 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 28 : 355 / 502 = 70 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 29 : 301 / 502 = 59 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 30 : 337 / 501 = 67 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 31 : 429 / 504 = 85 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 32 : 425 / 503 = 84 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 33 : 419 / 503 = 83 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 34 : 483 / 504 = 95 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 35 : 470 / 503 = 93 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 36 : 412 / 502 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 37 : 439 / 504 = 87 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 38 : 417 / 504 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 39 : 459 / 498 = 92 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 40 : 395 / 504 = 78 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 41 : 477 / 503 = 94 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 42 : 459 / 504 = 91 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 43 : 341 / 503 = 67 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 44 : 443 / 504 = 87 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 45 : 428 / 504 = 84 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 46 : 417 / 504 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 47 : 431 / 503 = 85 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 48 : 436 / 503 = 86 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 49 : 385 / 499 = 77 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 50 : 427 / 502 = 85 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 51 : 471 / 503 = 93 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 52 : 447 / 504 = 88 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 53 : 422 / 497 = 84 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 54 : 455 / 480 = 94 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 55 : 364 / 504 = 72 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 56 : 417 / 503 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 57 : 480 / 504 = 95 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 58 : 480 / 499 = 96 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 59 : 491 / 503 = 97 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 60 : 421 / 479 = 87 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 61 : 415 / 484 = 85 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 62 : 396 / 487 = 81 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 63 : 451 / 489 = 92 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 64 : 375 / 488 = 76 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 65 : 454 / 490 = 92 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 66 : 316 / 488 = 64 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 67 : 375 / 490 = 76 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 68 : 280 / 490 = 57 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 69 : 364 / 490 = 74 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 70 : 210 / 490 = 42 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 71 : 201 / 490 = 41 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 72 : 195 / 488 = 39 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 73 : 280 / 486 = 57 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 74 : 288 / 481 = 59 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 75 : 285 / 488 = 58 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 76 : 333 / 489 = 68 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 77 : 337 / 488 = 69 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 78 : 376 / 488 = 77 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 79 : 453 / 490 = 92 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 80 : 409 / 489 = 83 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 81 : 295 / 491 = 60 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 82 : 327 / 491 = 66 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 83 : 269 / 489 = 55 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 84 : 386 / 489 = 78 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 85 : 366 / 489 = 74 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 86 : 446 / 491 = 90 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 87 : 437 / 492 = 88 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 88 : 372 / 491 = 75 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 89 : 407 / 492 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 90 : 252 / 490 = 51 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 91 : 407 / 482 = 84 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 92 : 364 / 490 = 74 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 93 : 373 / 487 = 76 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 94 : 410 / 489 = 83 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 95 : 417 / 490 = 85 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 96 : 466 / 491 = 94 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 97 : 466 / 490 = 95 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 98 : 451 / 491 = 91 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 99 : 448 / 491 = 91 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 100 : 450 / 491 = 91 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 101 : 434 / 491 = 88 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 102 : 283 / 492 = 57 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 103 : 407 / 492 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 104 : 307 / 491 = 62 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 105 : 277 / 491 = 56 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 106 : 263 / 492 = 53 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 107 : 405 / 491 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 108 : 394 / 492 = 80 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 109 : 318 / 490 = 64 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 110 : 420 / 491 = 85 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 111 : 460 / 492 = 93 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 112 : 457 / 492 = 92 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 113 : 442 / 491 = 90 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 114 : 409 / 491 = 83 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 115 : 433 / 492 = 88 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 116 : 407 / 491 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 117 : 447 / 492 = 90 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 118 : 441 / 490 = 89 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 119 : 445 / 492 = 90 %
[ Sun Jul 14 00:11:06 2024 ] Accuracy of 120 : 414 / 500 = 82 %
[ Sun Jul 14 00:11:06 2024 ] Training epoch: 21
[ Sun Jul 14 00:11:07 2024 ] 	Batch(0/6809) done. Loss: 0.0101  lr:0.000001
[ Sun Jul 14 00:11:25 2024 ] 	Batch(100/6809) done. Loss: 0.1959  lr:0.000001
[ Sun Jul 14 00:11:43 2024 ] 	Batch(200/6809) done. Loss: 0.1153  lr:0.000001
[ Sun Jul 14 00:12:01 2024 ] 	Batch(300/6809) done. Loss: 0.0131  lr:0.000001
[ Sun Jul 14 00:12:19 2024 ] 	Batch(400/6809) done. Loss: 0.5121  lr:0.000001
[ Sun Jul 14 00:12:36 2024 ] 
Training: Epoch [20/50], Step [499], Loss: 0.011780550703406334, Training Accuracy: 97.625
[ Sun Jul 14 00:12:36 2024 ] 	Batch(500/6809) done. Loss: 0.0338  lr:0.000001
[ Sun Jul 14 00:12:54 2024 ] 	Batch(600/6809) done. Loss: 0.0984  lr:0.000001
[ Sun Jul 14 00:13:12 2024 ] 	Batch(700/6809) done. Loss: 0.2028  lr:0.000001
[ Sun Jul 14 00:13:30 2024 ] 	Batch(800/6809) done. Loss: 0.0192  lr:0.000001
[ Sun Jul 14 00:13:48 2024 ] 	Batch(900/6809) done. Loss: 0.1816  lr:0.000001
[ Sun Jul 14 00:14:06 2024 ] 
Training: Epoch [20/50], Step [999], Loss: 0.04814210534095764, Training Accuracy: 97.55
[ Sun Jul 14 00:14:06 2024 ] 	Batch(1000/6809) done. Loss: 0.0655  lr:0.000001
[ Sun Jul 14 00:14:24 2024 ] 	Batch(1100/6809) done. Loss: 0.0746  lr:0.000001
[ Sun Jul 14 00:14:42 2024 ] 	Batch(1200/6809) done. Loss: 0.0436  lr:0.000001
[ Sun Jul 14 00:15:00 2024 ] 	Batch(1300/6809) done. Loss: 0.0265  lr:0.000001
[ Sun Jul 14 00:15:17 2024 ] 	Batch(1400/6809) done. Loss: 0.0122  lr:0.000001
[ Sun Jul 14 00:15:35 2024 ] 
Training: Epoch [20/50], Step [1499], Loss: 0.011302722617983818, Training Accuracy: 97.55
[ Sun Jul 14 00:15:35 2024 ] 	Batch(1500/6809) done. Loss: 0.0240  lr:0.000001
[ Sun Jul 14 00:15:53 2024 ] 	Batch(1600/6809) done. Loss: 0.0641  lr:0.000001
[ Sun Jul 14 00:16:11 2024 ] 	Batch(1700/6809) done. Loss: 0.0583  lr:0.000001
[ Sun Jul 14 00:16:29 2024 ] 	Batch(1800/6809) done. Loss: 0.0082  lr:0.000001
[ Sun Jul 14 00:16:47 2024 ] 	Batch(1900/6809) done. Loss: 0.2167  lr:0.000001
[ Sun Jul 14 00:17:05 2024 ] 
Training: Epoch [20/50], Step [1999], Loss: 0.016656601801514626, Training Accuracy: 97.60625
[ Sun Jul 14 00:17:05 2024 ] 	Batch(2000/6809) done. Loss: 0.1476  lr:0.000001
[ Sun Jul 14 00:17:23 2024 ] 	Batch(2100/6809) done. Loss: 0.0132  lr:0.000001
[ Sun Jul 14 00:17:42 2024 ] 	Batch(2200/6809) done. Loss: 0.0169  lr:0.000001
[ Sun Jul 14 00:18:00 2024 ] 	Batch(2300/6809) done. Loss: 0.0204  lr:0.000001
[ Sun Jul 14 00:18:18 2024 ] 	Batch(2400/6809) done. Loss: 0.1757  lr:0.000001
[ Sun Jul 14 00:18:36 2024 ] 
Training: Epoch [20/50], Step [2499], Loss: 0.09306307137012482, Training Accuracy: 97.68
[ Sun Jul 14 00:18:36 2024 ] 	Batch(2500/6809) done. Loss: 0.0270  lr:0.000001
[ Sun Jul 14 00:18:54 2024 ] 	Batch(2600/6809) done. Loss: 0.1135  lr:0.000001
[ Sun Jul 14 00:19:12 2024 ] 	Batch(2700/6809) done. Loss: 0.0481  lr:0.000001
[ Sun Jul 14 00:19:30 2024 ] 	Batch(2800/6809) done. Loss: 0.0108  lr:0.000001
[ Sun Jul 14 00:19:48 2024 ] 	Batch(2900/6809) done. Loss: 0.0718  lr:0.000001
[ Sun Jul 14 00:20:06 2024 ] 
Training: Epoch [20/50], Step [2999], Loss: 0.027984293177723885, Training Accuracy: 97.6875
[ Sun Jul 14 00:20:06 2024 ] 	Batch(3000/6809) done. Loss: 0.3448  lr:0.000001
[ Sun Jul 14 00:20:24 2024 ] 	Batch(3100/6809) done. Loss: 0.0253  lr:0.000001
[ Sun Jul 14 00:20:42 2024 ] 	Batch(3200/6809) done. Loss: 0.0876  lr:0.000001
[ Sun Jul 14 00:21:00 2024 ] 	Batch(3300/6809) done. Loss: 0.0993  lr:0.000001
[ Sun Jul 14 00:21:17 2024 ] 	Batch(3400/6809) done. Loss: 0.0013  lr:0.000001
[ Sun Jul 14 00:21:35 2024 ] 
Training: Epoch [20/50], Step [3499], Loss: 0.14832459390163422, Training Accuracy: 97.59285714285714
[ Sun Jul 14 00:21:35 2024 ] 	Batch(3500/6809) done. Loss: 0.0036  lr:0.000001
[ Sun Jul 14 00:21:53 2024 ] 	Batch(3600/6809) done. Loss: 0.0949  lr:0.000001
[ Sun Jul 14 00:22:11 2024 ] 	Batch(3700/6809) done. Loss: 0.0658  lr:0.000001
[ Sun Jul 14 00:22:29 2024 ] 	Batch(3800/6809) done. Loss: 0.0489  lr:0.000001
[ Sun Jul 14 00:22:47 2024 ] 	Batch(3900/6809) done. Loss: 0.0460  lr:0.000001
[ Sun Jul 14 00:23:05 2024 ] 
Training: Epoch [20/50], Step [3999], Loss: 0.023944005370140076, Training Accuracy: 97.609375
[ Sun Jul 14 00:23:05 2024 ] 	Batch(4000/6809) done. Loss: 0.0047  lr:0.000001
[ Sun Jul 14 00:23:23 2024 ] 	Batch(4100/6809) done. Loss: 0.0042  lr:0.000001
[ Sun Jul 14 00:23:41 2024 ] 	Batch(4200/6809) done. Loss: 0.0475  lr:0.000001
[ Sun Jul 14 00:23:59 2024 ] 	Batch(4300/6809) done. Loss: 0.0254  lr:0.000001
[ Sun Jul 14 00:24:17 2024 ] 	Batch(4400/6809) done. Loss: 0.0425  lr:0.000001
[ Sun Jul 14 00:24:35 2024 ] 
Training: Epoch [20/50], Step [4499], Loss: 0.0120110297575593, Training Accuracy: 97.58333333333333
[ Sun Jul 14 00:24:35 2024 ] 	Batch(4500/6809) done. Loss: 0.0288  lr:0.000001
[ Sun Jul 14 00:24:53 2024 ] 	Batch(4600/6809) done. Loss: 0.0577  lr:0.000001
[ Sun Jul 14 00:25:10 2024 ] 	Batch(4700/6809) done. Loss: 0.0184  lr:0.000001
[ Sun Jul 14 00:25:28 2024 ] 	Batch(4800/6809) done. Loss: 0.1050  lr:0.000001
[ Sun Jul 14 00:25:46 2024 ] 	Batch(4900/6809) done. Loss: 0.0332  lr:0.000001
[ Sun Jul 14 00:26:04 2024 ] 
Training: Epoch [20/50], Step [4999], Loss: 0.002748580649495125, Training Accuracy: 97.6325
[ Sun Jul 14 00:26:04 2024 ] 	Batch(5000/6809) done. Loss: 0.0532  lr:0.000001
[ Sun Jul 14 00:26:22 2024 ] 	Batch(5100/6809) done. Loss: 0.0221  lr:0.000001
[ Sun Jul 14 00:26:40 2024 ] 	Batch(5200/6809) done. Loss: 0.0375  lr:0.000001
[ Sun Jul 14 00:26:58 2024 ] 	Batch(5300/6809) done. Loss: 0.0315  lr:0.000001
[ Sun Jul 14 00:27:16 2024 ] 	Batch(5400/6809) done. Loss: 0.0396  lr:0.000001
[ Sun Jul 14 00:27:34 2024 ] 
Training: Epoch [20/50], Step [5499], Loss: 0.036068715155124664, Training Accuracy: 97.61818181818181
[ Sun Jul 14 00:27:34 2024 ] 	Batch(5500/6809) done. Loss: 0.3867  lr:0.000001
[ Sun Jul 14 00:27:52 2024 ] 	Batch(5600/6809) done. Loss: 0.2526  lr:0.000001
[ Sun Jul 14 00:28:10 2024 ] 	Batch(5700/6809) done. Loss: 0.2061  lr:0.000001
[ Sun Jul 14 00:28:28 2024 ] 	Batch(5800/6809) done. Loss: 0.3761  lr:0.000001
[ Sun Jul 14 00:28:46 2024 ] 	Batch(5900/6809) done. Loss: 0.2634  lr:0.000001
[ Sun Jul 14 00:29:03 2024 ] 
Training: Epoch [20/50], Step [5999], Loss: 0.030935918912291527, Training Accuracy: 97.61875
[ Sun Jul 14 00:29:04 2024 ] 	Batch(6000/6809) done. Loss: 0.0449  lr:0.000001
[ Sun Jul 14 00:29:22 2024 ] 	Batch(6100/6809) done. Loss: 0.2098  lr:0.000001
[ Sun Jul 14 00:29:41 2024 ] 	Batch(6200/6809) done. Loss: 0.0479  lr:0.000001
[ Sun Jul 14 00:29:59 2024 ] 	Batch(6300/6809) done. Loss: 0.0250  lr:0.000001
[ Sun Jul 14 00:30:18 2024 ] 	Batch(6400/6809) done. Loss: 0.0382  lr:0.000001
[ Sun Jul 14 00:30:36 2024 ] 
Training: Epoch [20/50], Step [6499], Loss: 0.2161398082971573, Training Accuracy: 97.5673076923077
[ Sun Jul 14 00:30:36 2024 ] 	Batch(6500/6809) done. Loss: 0.0551  lr:0.000001
[ Sun Jul 14 00:30:54 2024 ] 	Batch(6600/6809) done. Loss: 0.1717  lr:0.000001
[ Sun Jul 14 00:31:12 2024 ] 	Batch(6700/6809) done. Loss: 0.0375  lr:0.000001
[ Sun Jul 14 00:31:30 2024 ] 	Batch(6800/6809) done. Loss: 0.0804  lr:0.000001
[ Sun Jul 14 00:31:32 2024 ] 	Mean training loss: 0.0975.
[ Sun Jul 14 00:31:32 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 00:31:32 2024 ] Training epoch: 22
[ Sun Jul 14 00:31:32 2024 ] 	Batch(0/6809) done. Loss: 0.0531  lr:0.000001
[ Sun Jul 14 00:31:50 2024 ] 	Batch(100/6809) done. Loss: 0.0045  lr:0.000001
[ Sun Jul 14 00:32:08 2024 ] 	Batch(200/6809) done. Loss: 0.0194  lr:0.000001
[ Sun Jul 14 00:32:26 2024 ] 	Batch(300/6809) done. Loss: 0.0022  lr:0.000001
[ Sun Jul 14 00:32:44 2024 ] 	Batch(400/6809) done. Loss: 0.0318  lr:0.000001
[ Sun Jul 14 00:33:02 2024 ] 
Training: Epoch [21/50], Step [499], Loss: 0.14385835826396942, Training Accuracy: 97.425
[ Sun Jul 14 00:33:02 2024 ] 	Batch(500/6809) done. Loss: 0.0066  lr:0.000001
[ Sun Jul 14 00:33:20 2024 ] 	Batch(600/6809) done. Loss: 0.2273  lr:0.000001
[ Sun Jul 14 00:33:38 2024 ] 	Batch(700/6809) done. Loss: 0.0679  lr:0.000001
[ Sun Jul 14 00:33:56 2024 ] 	Batch(800/6809) done. Loss: 0.1287  lr:0.000001
[ Sun Jul 14 00:34:14 2024 ] 	Batch(900/6809) done. Loss: 0.0934  lr:0.000001
[ Sun Jul 14 00:34:31 2024 ] 
Training: Epoch [21/50], Step [999], Loss: 0.02730967290699482, Training Accuracy: 97.55
[ Sun Jul 14 00:34:31 2024 ] 	Batch(1000/6809) done. Loss: 0.0310  lr:0.000001
[ Sun Jul 14 00:34:49 2024 ] 	Batch(1100/6809) done. Loss: 0.0036  lr:0.000001
[ Sun Jul 14 00:35:07 2024 ] 	Batch(1200/6809) done. Loss: 0.0233  lr:0.000001
[ Sun Jul 14 00:35:25 2024 ] 	Batch(1300/6809) done. Loss: 0.0546  lr:0.000001
[ Sun Jul 14 00:35:43 2024 ] 	Batch(1400/6809) done. Loss: 0.0461  lr:0.000001
[ Sun Jul 14 00:36:01 2024 ] 
Training: Epoch [21/50], Step [1499], Loss: 0.5787780284881592, Training Accuracy: 97.425
[ Sun Jul 14 00:36:01 2024 ] 	Batch(1500/6809) done. Loss: 0.0848  lr:0.000001
[ Sun Jul 14 00:36:19 2024 ] 	Batch(1600/6809) done. Loss: 0.0357  lr:0.000001
[ Sun Jul 14 00:36:37 2024 ] 	Batch(1700/6809) done. Loss: 0.2140  lr:0.000001
[ Sun Jul 14 00:36:55 2024 ] 	Batch(1800/6809) done. Loss: 0.0075  lr:0.000001
[ Sun Jul 14 00:37:13 2024 ] 	Batch(1900/6809) done. Loss: 0.0188  lr:0.000001
[ Sun Jul 14 00:37:30 2024 ] 
Training: Epoch [21/50], Step [1999], Loss: 0.07224584370851517, Training Accuracy: 97.45
[ Sun Jul 14 00:37:31 2024 ] 	Batch(2000/6809) done. Loss: 0.1250  lr:0.000001
[ Sun Jul 14 00:37:49 2024 ] 	Batch(2100/6809) done. Loss: 0.2610  lr:0.000001
[ Sun Jul 14 00:38:06 2024 ] 	Batch(2200/6809) done. Loss: 0.0202  lr:0.000001
[ Sun Jul 14 00:38:24 2024 ] 	Batch(2300/6809) done. Loss: 0.0598  lr:0.000001
[ Sun Jul 14 00:38:42 2024 ] 	Batch(2400/6809) done. Loss: 0.0778  lr:0.000001
[ Sun Jul 14 00:39:00 2024 ] 
Training: Epoch [21/50], Step [2499], Loss: 0.07174583524465561, Training Accuracy: 97.54
[ Sun Jul 14 00:39:00 2024 ] 	Batch(2500/6809) done. Loss: 0.1307  lr:0.000001
[ Sun Jul 14 00:39:18 2024 ] 	Batch(2600/6809) done. Loss: 0.0364  lr:0.000001
[ Sun Jul 14 00:39:36 2024 ] 	Batch(2700/6809) done. Loss: 0.1234  lr:0.000001
[ Sun Jul 14 00:39:54 2024 ] 	Batch(2800/6809) done. Loss: 0.1373  lr:0.000001
[ Sun Jul 14 00:40:12 2024 ] 	Batch(2900/6809) done. Loss: 0.0910  lr:0.000001
[ Sun Jul 14 00:40:30 2024 ] 
Training: Epoch [21/50], Step [2999], Loss: 0.11464934051036835, Training Accuracy: 97.54166666666667
[ Sun Jul 14 00:40:30 2024 ] 	Batch(3000/6809) done. Loss: 0.0061  lr:0.000001
[ Sun Jul 14 00:40:48 2024 ] 	Batch(3100/6809) done. Loss: 0.0177  lr:0.000001
[ Sun Jul 14 00:41:06 2024 ] 	Batch(3200/6809) done. Loss: 0.0233  lr:0.000001
[ Sun Jul 14 00:41:24 2024 ] 	Batch(3300/6809) done. Loss: 0.0232  lr:0.000001
[ Sun Jul 14 00:41:42 2024 ] 	Batch(3400/6809) done. Loss: 0.1037  lr:0.000001
[ Sun Jul 14 00:41:59 2024 ] 
Training: Epoch [21/50], Step [3499], Loss: 0.05637899041175842, Training Accuracy: 97.58214285714286
[ Sun Jul 14 00:42:00 2024 ] 	Batch(3500/6809) done. Loss: 0.0647  lr:0.000001
[ Sun Jul 14 00:42:18 2024 ] 	Batch(3600/6809) done. Loss: 0.0031  lr:0.000001
[ Sun Jul 14 00:42:36 2024 ] 	Batch(3700/6809) done. Loss: 0.0801  lr:0.000001
[ Sun Jul 14 00:42:54 2024 ] 	Batch(3800/6809) done. Loss: 0.1510  lr:0.000001
[ Sun Jul 14 00:43:13 2024 ] 	Batch(3900/6809) done. Loss: 0.0178  lr:0.000001
[ Sun Jul 14 00:43:30 2024 ] 
Training: Epoch [21/50], Step [3999], Loss: 0.016678523272275925, Training Accuracy: 97.584375
[ Sun Jul 14 00:43:31 2024 ] 	Batch(4000/6809) done. Loss: 0.3655  lr:0.000001
[ Sun Jul 14 00:43:49 2024 ] 	Batch(4100/6809) done. Loss: 0.0182  lr:0.000001
[ Sun Jul 14 00:44:08 2024 ] 	Batch(4200/6809) done. Loss: 0.0284  lr:0.000001
[ Sun Jul 14 00:44:26 2024 ] 	Batch(4300/6809) done. Loss: 0.0233  lr:0.000001
[ Sun Jul 14 00:44:45 2024 ] 	Batch(4400/6809) done. Loss: 0.2462  lr:0.000001
[ Sun Jul 14 00:45:04 2024 ] 
Training: Epoch [21/50], Step [4499], Loss: 0.02465069107711315, Training Accuracy: 97.57222222222222
[ Sun Jul 14 00:45:04 2024 ] 	Batch(4500/6809) done. Loss: 0.0413  lr:0.000001
[ Sun Jul 14 00:45:22 2024 ] 	Batch(4600/6809) done. Loss: 0.1063  lr:0.000001
[ Sun Jul 14 00:45:41 2024 ] 	Batch(4700/6809) done. Loss: 0.0479  lr:0.000001
[ Sun Jul 14 00:45:59 2024 ] 	Batch(4800/6809) done. Loss: 0.0128  lr:0.000001
[ Sun Jul 14 00:46:18 2024 ] 	Batch(4900/6809) done. Loss: 0.0254  lr:0.000001
[ Sun Jul 14 00:46:36 2024 ] 
Training: Epoch [21/50], Step [4999], Loss: 0.04244237393140793, Training Accuracy: 97.5425
[ Sun Jul 14 00:46:37 2024 ] 	Batch(5000/6809) done. Loss: 0.0105  lr:0.000001
[ Sun Jul 14 00:46:55 2024 ] 	Batch(5100/6809) done. Loss: 0.0298  lr:0.000001
[ Sun Jul 14 00:47:14 2024 ] 	Batch(5200/6809) done. Loss: 0.0043  lr:0.000001
[ Sun Jul 14 00:47:32 2024 ] 	Batch(5300/6809) done. Loss: 0.0855  lr:0.000001
[ Sun Jul 14 00:47:50 2024 ] 	Batch(5400/6809) done. Loss: 0.0646  lr:0.000001
[ Sun Jul 14 00:48:07 2024 ] 
Training: Epoch [21/50], Step [5499], Loss: 0.011538336053490639, Training Accuracy: 97.53863636363637
[ Sun Jul 14 00:48:08 2024 ] 	Batch(5500/6809) done. Loss: 0.0126  lr:0.000001
[ Sun Jul 14 00:48:26 2024 ] 	Batch(5600/6809) done. Loss: 0.0040  lr:0.000001
[ Sun Jul 14 00:48:43 2024 ] 	Batch(5700/6809) done. Loss: 0.0338  lr:0.000001
[ Sun Jul 14 00:49:01 2024 ] 	Batch(5800/6809) done. Loss: 0.2308  lr:0.000001
[ Sun Jul 14 00:49:19 2024 ] 	Batch(5900/6809) done. Loss: 0.1076  lr:0.000001
[ Sun Jul 14 00:49:37 2024 ] 
Training: Epoch [21/50], Step [5999], Loss: 0.077303446829319, Training Accuracy: 97.5375
[ Sun Jul 14 00:49:37 2024 ] 	Batch(6000/6809) done. Loss: 0.0038  lr:0.000001
[ Sun Jul 14 00:49:55 2024 ] 	Batch(6100/6809) done. Loss: 0.0214  lr:0.000001
[ Sun Jul 14 00:50:13 2024 ] 	Batch(6200/6809) done. Loss: 0.0124  lr:0.000001
[ Sun Jul 14 00:50:31 2024 ] 	Batch(6300/6809) done. Loss: 0.1585  lr:0.000001
[ Sun Jul 14 00:50:49 2024 ] 	Batch(6400/6809) done. Loss: 0.1254  lr:0.000001
[ Sun Jul 14 00:51:07 2024 ] 
Training: Epoch [21/50], Step [6499], Loss: 0.05684908479452133, Training Accuracy: 97.54423076923077
[ Sun Jul 14 00:51:07 2024 ] 	Batch(6500/6809) done. Loss: 0.0078  lr:0.000001
[ Sun Jul 14 00:51:25 2024 ] 	Batch(6600/6809) done. Loss: 0.2079  lr:0.000001
[ Sun Jul 14 00:51:43 2024 ] 	Batch(6700/6809) done. Loss: 0.3294  lr:0.000001
[ Sun Jul 14 00:52:01 2024 ] 	Batch(6800/6809) done. Loss: 0.0094  lr:0.000001
[ Sun Jul 14 00:52:02 2024 ] 	Mean training loss: 0.0996.
[ Sun Jul 14 00:52:02 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 00:52:02 2024 ] Training epoch: 23
[ Sun Jul 14 00:52:03 2024 ] 	Batch(0/6809) done. Loss: 0.0232  lr:0.000001
[ Sun Jul 14 00:52:21 2024 ] 	Batch(100/6809) done. Loss: 0.0951  lr:0.000001
[ Sun Jul 14 00:52:39 2024 ] 	Batch(200/6809) done. Loss: 0.0412  lr:0.000001
[ Sun Jul 14 00:52:57 2024 ] 	Batch(300/6809) done. Loss: 0.2542  lr:0.000001
[ Sun Jul 14 00:53:16 2024 ] 	Batch(400/6809) done. Loss: 0.0069  lr:0.000001
[ Sun Jul 14 00:53:34 2024 ] 
Training: Epoch [22/50], Step [499], Loss: 0.009057402610778809, Training Accuracy: 97.425
[ Sun Jul 14 00:53:34 2024 ] 	Batch(500/6809) done. Loss: 0.0831  lr:0.000001
[ Sun Jul 14 00:53:52 2024 ] 	Batch(600/6809) done. Loss: 0.0163  lr:0.000001
[ Sun Jul 14 00:54:10 2024 ] 	Batch(700/6809) done. Loss: 0.0236  lr:0.000001
[ Sun Jul 14 00:54:28 2024 ] 	Batch(800/6809) done. Loss: 0.0134  lr:0.000001
[ Sun Jul 14 00:54:46 2024 ] 	Batch(900/6809) done. Loss: 0.0074  lr:0.000001
[ Sun Jul 14 00:55:04 2024 ] 
Training: Epoch [22/50], Step [999], Loss: 0.165349081158638, Training Accuracy: 97.65
[ Sun Jul 14 00:55:04 2024 ] 	Batch(1000/6809) done. Loss: 0.0045  lr:0.000001
[ Sun Jul 14 00:55:22 2024 ] 	Batch(1100/6809) done. Loss: 0.0067  lr:0.000001
[ Sun Jul 14 00:55:40 2024 ] 	Batch(1200/6809) done. Loss: 0.0277  lr:0.000001
[ Sun Jul 14 00:55:58 2024 ] 	Batch(1300/6809) done. Loss: 0.0744  lr:0.000001
[ Sun Jul 14 00:56:16 2024 ] 	Batch(1400/6809) done. Loss: 0.1146  lr:0.000001
[ Sun Jul 14 00:56:34 2024 ] 
Training: Epoch [22/50], Step [1499], Loss: 0.07601404935121536, Training Accuracy: 97.73333333333333
[ Sun Jul 14 00:56:34 2024 ] 	Batch(1500/6809) done. Loss: 0.0020  lr:0.000001
[ Sun Jul 14 00:56:52 2024 ] 	Batch(1600/6809) done. Loss: 0.0426  lr:0.000001
[ Sun Jul 14 00:57:10 2024 ] 	Batch(1700/6809) done. Loss: 0.1439  lr:0.000001
[ Sun Jul 14 00:57:29 2024 ] 	Batch(1800/6809) done. Loss: 0.1490  lr:0.000001
[ Sun Jul 14 00:57:47 2024 ] 	Batch(1900/6809) done. Loss: 0.0343  lr:0.000001
[ Sun Jul 14 00:58:06 2024 ] 
Training: Epoch [22/50], Step [1999], Loss: 0.11095721274614334, Training Accuracy: 97.71875
[ Sun Jul 14 00:58:06 2024 ] 	Batch(2000/6809) done. Loss: 0.1709  lr:0.000001
[ Sun Jul 14 00:58:24 2024 ] 	Batch(2100/6809) done. Loss: 0.0109  lr:0.000001
[ Sun Jul 14 00:58:42 2024 ] 	Batch(2200/6809) done. Loss: 0.0232  lr:0.000001
[ Sun Jul 14 00:59:00 2024 ] 	Batch(2300/6809) done. Loss: 0.1471  lr:0.000001
[ Sun Jul 14 00:59:18 2024 ] 	Batch(2400/6809) done. Loss: 0.1305  lr:0.000001
[ Sun Jul 14 00:59:36 2024 ] 
Training: Epoch [22/50], Step [2499], Loss: 0.14335297048091888, Training Accuracy: 97.78999999999999
[ Sun Jul 14 00:59:36 2024 ] 	Batch(2500/6809) done. Loss: 0.0135  lr:0.000001
[ Sun Jul 14 00:59:54 2024 ] 	Batch(2600/6809) done. Loss: 0.0404  lr:0.000001
[ Sun Jul 14 01:00:11 2024 ] 	Batch(2700/6809) done. Loss: 0.0061  lr:0.000001
[ Sun Jul 14 01:00:30 2024 ] 	Batch(2800/6809) done. Loss: 0.0352  lr:0.000001
[ Sun Jul 14 01:00:48 2024 ] 	Batch(2900/6809) done. Loss: 0.2154  lr:0.000001
[ Sun Jul 14 01:01:07 2024 ] 
Training: Epoch [22/50], Step [2999], Loss: 0.3011898100376129, Training Accuracy: 97.7375
[ Sun Jul 14 01:01:07 2024 ] 	Batch(3000/6809) done. Loss: 0.1118  lr:0.000001
[ Sun Jul 14 01:01:25 2024 ] 	Batch(3100/6809) done. Loss: 0.1807  lr:0.000001
[ Sun Jul 14 01:01:44 2024 ] 	Batch(3200/6809) done. Loss: 0.1860  lr:0.000001
[ Sun Jul 14 01:02:02 2024 ] 	Batch(3300/6809) done. Loss: 0.1933  lr:0.000001
[ Sun Jul 14 01:02:20 2024 ] 	Batch(3400/6809) done. Loss: 0.0192  lr:0.000001
[ Sun Jul 14 01:02:38 2024 ] 
Training: Epoch [22/50], Step [3499], Loss: 0.05442613735795021, Training Accuracy: 97.7
[ Sun Jul 14 01:02:38 2024 ] 	Batch(3500/6809) done. Loss: 0.0149  lr:0.000001
[ Sun Jul 14 01:02:56 2024 ] 	Batch(3600/6809) done. Loss: 0.0503  lr:0.000001
[ Sun Jul 14 01:03:14 2024 ] 	Batch(3700/6809) done. Loss: 0.1884  lr:0.000001
[ Sun Jul 14 01:03:32 2024 ] 	Batch(3800/6809) done. Loss: 0.1089  lr:0.000001
[ Sun Jul 14 01:03:50 2024 ] 	Batch(3900/6809) done. Loss: 0.0355  lr:0.000001
[ Sun Jul 14 01:04:08 2024 ] 
Training: Epoch [22/50], Step [3999], Loss: 0.12884360551834106, Training Accuracy: 97.659375
[ Sun Jul 14 01:04:08 2024 ] 	Batch(4000/6809) done. Loss: 0.1761  lr:0.000001
[ Sun Jul 14 01:04:26 2024 ] 	Batch(4100/6809) done. Loss: 0.0678  lr:0.000001
[ Sun Jul 14 01:04:44 2024 ] 	Batch(4200/6809) done. Loss: 0.0349  lr:0.000001
[ Sun Jul 14 01:05:02 2024 ] 	Batch(4300/6809) done. Loss: 0.2170  lr:0.000001
[ Sun Jul 14 01:05:20 2024 ] 	Batch(4400/6809) done. Loss: 0.0556  lr:0.000001
[ Sun Jul 14 01:05:38 2024 ] 
Training: Epoch [22/50], Step [4499], Loss: 0.004041994921863079, Training Accuracy: 97.675
[ Sun Jul 14 01:05:38 2024 ] 	Batch(4500/6809) done. Loss: 0.0526  lr:0.000001
[ Sun Jul 14 01:05:57 2024 ] 	Batch(4600/6809) done. Loss: 0.0124  lr:0.000001
[ Sun Jul 14 01:06:15 2024 ] 	Batch(4700/6809) done. Loss: 0.0505  lr:0.000001
[ Sun Jul 14 01:06:34 2024 ] 	Batch(4800/6809) done. Loss: 0.0993  lr:0.000001
[ Sun Jul 14 01:06:53 2024 ] 	Batch(4900/6809) done. Loss: 0.0275  lr:0.000001
[ Sun Jul 14 01:07:11 2024 ] 
Training: Epoch [22/50], Step [4999], Loss: 0.11058720201253891, Training Accuracy: 97.7025
[ Sun Jul 14 01:07:11 2024 ] 	Batch(5000/6809) done. Loss: 0.0037  lr:0.000001
[ Sun Jul 14 01:07:30 2024 ] 	Batch(5100/6809) done. Loss: 0.0173  lr:0.000001
[ Sun Jul 14 01:07:48 2024 ] 	Batch(5200/6809) done. Loss: 0.4052  lr:0.000001
[ Sun Jul 14 01:08:06 2024 ] 	Batch(5300/6809) done. Loss: 0.0286  lr:0.000001
[ Sun Jul 14 01:08:24 2024 ] 	Batch(5400/6809) done. Loss: 0.2215  lr:0.000001
[ Sun Jul 14 01:08:42 2024 ] 
Training: Epoch [22/50], Step [5499], Loss: 0.03890938684344292, Training Accuracy: 97.70681818181818
[ Sun Jul 14 01:08:42 2024 ] 	Batch(5500/6809) done. Loss: 0.0764  lr:0.000001
[ Sun Jul 14 01:09:00 2024 ] 	Batch(5600/6809) done. Loss: 0.1025  lr:0.000001
[ Sun Jul 14 01:09:18 2024 ] 	Batch(5700/6809) done. Loss: 0.2527  lr:0.000001
[ Sun Jul 14 01:09:36 2024 ] 	Batch(5800/6809) done. Loss: 0.0756  lr:0.000001
[ Sun Jul 14 01:09:54 2024 ] 	Batch(5900/6809) done. Loss: 0.0132  lr:0.000001
[ Sun Jul 14 01:10:12 2024 ] 
Training: Epoch [22/50], Step [5999], Loss: 0.023158887401223183, Training Accuracy: 97.67916666666666
[ Sun Jul 14 01:10:12 2024 ] 	Batch(6000/6809) done. Loss: 0.0058  lr:0.000001
[ Sun Jul 14 01:10:30 2024 ] 	Batch(6100/6809) done. Loss: 0.0074  lr:0.000001
[ Sun Jul 14 01:10:48 2024 ] 	Batch(6200/6809) done. Loss: 0.0590  lr:0.000001
[ Sun Jul 14 01:11:06 2024 ] 	Batch(6300/6809) done. Loss: 0.0401  lr:0.000001
[ Sun Jul 14 01:11:24 2024 ] 	Batch(6400/6809) done. Loss: 0.1686  lr:0.000001
[ Sun Jul 14 01:11:42 2024 ] 
Training: Epoch [22/50], Step [6499], Loss: 0.18600627779960632, Training Accuracy: 97.66153846153847
[ Sun Jul 14 01:11:42 2024 ] 	Batch(6500/6809) done. Loss: 0.0273  lr:0.000001
[ Sun Jul 14 01:12:00 2024 ] 	Batch(6600/6809) done. Loss: 0.2113  lr:0.000001
[ Sun Jul 14 01:12:18 2024 ] 	Batch(6700/6809) done. Loss: 0.0798  lr:0.000001
[ Sun Jul 14 01:12:36 2024 ] 	Batch(6800/6809) done. Loss: 0.1257  lr:0.000001
[ Sun Jul 14 01:12:37 2024 ] 	Mean training loss: 0.0945.
[ Sun Jul 14 01:12:37 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 01:12:37 2024 ] Training epoch: 24
[ Sun Jul 14 01:12:38 2024 ] 	Batch(0/6809) done. Loss: 0.0185  lr:0.000001
[ Sun Jul 14 01:12:56 2024 ] 	Batch(100/6809) done. Loss: 0.0087  lr:0.000001
[ Sun Jul 14 01:13:14 2024 ] 	Batch(200/6809) done. Loss: 0.0023  lr:0.000001
[ Sun Jul 14 01:13:31 2024 ] 	Batch(300/6809) done. Loss: 0.0338  lr:0.000001
[ Sun Jul 14 01:13:49 2024 ] 	Batch(400/6809) done. Loss: 0.0788  lr:0.000001
[ Sun Jul 14 01:14:07 2024 ] 
Training: Epoch [23/50], Step [499], Loss: 0.04739561676979065, Training Accuracy: 97.2
[ Sun Jul 14 01:14:07 2024 ] 	Batch(500/6809) done. Loss: 0.1765  lr:0.000001
[ Sun Jul 14 01:14:25 2024 ] 	Batch(600/6809) done. Loss: 0.0036  lr:0.000001
[ Sun Jul 14 01:14:43 2024 ] 	Batch(700/6809) done. Loss: 0.0679  lr:0.000001
[ Sun Jul 14 01:15:01 2024 ] 	Batch(800/6809) done. Loss: 0.0099  lr:0.000001
[ Sun Jul 14 01:15:19 2024 ] 	Batch(900/6809) done. Loss: 0.0297  lr:0.000001
[ Sun Jul 14 01:15:37 2024 ] 
Training: Epoch [23/50], Step [999], Loss: 0.14732755720615387, Training Accuracy: 97.32499999999999
[ Sun Jul 14 01:15:37 2024 ] 	Batch(1000/6809) done. Loss: 0.0257  lr:0.000001
[ Sun Jul 14 01:15:55 2024 ] 	Batch(1100/6809) done. Loss: 0.0364  lr:0.000001
[ Sun Jul 14 01:16:13 2024 ] 	Batch(1200/6809) done. Loss: 0.0261  lr:0.000001
[ Sun Jul 14 01:16:31 2024 ] 	Batch(1300/6809) done. Loss: 0.4102  lr:0.000001
[ Sun Jul 14 01:16:49 2024 ] 	Batch(1400/6809) done. Loss: 0.1273  lr:0.000001
[ Sun Jul 14 01:17:06 2024 ] 
Training: Epoch [23/50], Step [1499], Loss: 0.6403272747993469, Training Accuracy: 97.51666666666667
[ Sun Jul 14 01:17:07 2024 ] 	Batch(1500/6809) done. Loss: 0.0081  lr:0.000001
[ Sun Jul 14 01:17:25 2024 ] 	Batch(1600/6809) done. Loss: 0.0329  lr:0.000001
[ Sun Jul 14 01:17:43 2024 ] 	Batch(1700/6809) done. Loss: 0.0849  lr:0.000001
[ Sun Jul 14 01:18:01 2024 ] 	Batch(1800/6809) done. Loss: 0.0934  lr:0.000001
[ Sun Jul 14 01:18:18 2024 ] 	Batch(1900/6809) done. Loss: 0.0365  lr:0.000001
[ Sun Jul 14 01:18:36 2024 ] 
Training: Epoch [23/50], Step [1999], Loss: 0.024856895208358765, Training Accuracy: 97.48125
[ Sun Jul 14 01:18:37 2024 ] 	Batch(2000/6809) done. Loss: 0.0380  lr:0.000001
[ Sun Jul 14 01:18:54 2024 ] 	Batch(2100/6809) done. Loss: 0.0384  lr:0.000001
[ Sun Jul 14 01:19:12 2024 ] 	Batch(2200/6809) done. Loss: 0.1509  lr:0.000001
[ Sun Jul 14 01:19:30 2024 ] 	Batch(2300/6809) done. Loss: 0.0857  lr:0.000001
[ Sun Jul 14 01:19:48 2024 ] 	Batch(2400/6809) done. Loss: 0.0180  lr:0.000001
[ Sun Jul 14 01:20:06 2024 ] 
Training: Epoch [23/50], Step [2499], Loss: 0.0332975871860981, Training Accuracy: 97.46000000000001
[ Sun Jul 14 01:20:06 2024 ] 	Batch(2500/6809) done. Loss: 0.0111  lr:0.000001
[ Sun Jul 14 01:20:24 2024 ] 	Batch(2600/6809) done. Loss: 0.4009  lr:0.000001
[ Sun Jul 14 01:20:42 2024 ] 	Batch(2700/6809) done. Loss: 0.3207  lr:0.000001
[ Sun Jul 14 01:21:00 2024 ] 	Batch(2800/6809) done. Loss: 0.1446  lr:0.000001
[ Sun Jul 14 01:21:18 2024 ] 	Batch(2900/6809) done. Loss: 0.2840  lr:0.000001
[ Sun Jul 14 01:21:36 2024 ] 
Training: Epoch [23/50], Step [2999], Loss: 0.019484275951981544, Training Accuracy: 97.43333333333334
[ Sun Jul 14 01:21:36 2024 ] 	Batch(3000/6809) done. Loss: 0.1061  lr:0.000001
[ Sun Jul 14 01:21:54 2024 ] 	Batch(3100/6809) done. Loss: 0.0937  lr:0.000001
[ Sun Jul 14 01:22:12 2024 ] 	Batch(3200/6809) done. Loss: 0.0027  lr:0.000001
[ Sun Jul 14 01:22:30 2024 ] 	Batch(3300/6809) done. Loss: 0.0052  lr:0.000001
[ Sun Jul 14 01:22:48 2024 ] 	Batch(3400/6809) done. Loss: 0.2253  lr:0.000001
[ Sun Jul 14 01:23:06 2024 ] 
Training: Epoch [23/50], Step [3499], Loss: 0.016709262505173683, Training Accuracy: 97.41785714285714
[ Sun Jul 14 01:23:06 2024 ] 	Batch(3500/6809) done. Loss: 0.0227  lr:0.000001
[ Sun Jul 14 01:23:24 2024 ] 	Batch(3600/6809) done. Loss: 0.0717  lr:0.000001
[ Sun Jul 14 01:23:43 2024 ] 	Batch(3700/6809) done. Loss: 0.0258  lr:0.000001
[ Sun Jul 14 01:24:01 2024 ] 	Batch(3800/6809) done. Loss: 0.0616  lr:0.000001
[ Sun Jul 14 01:24:20 2024 ] 	Batch(3900/6809) done. Loss: 0.0220  lr:0.000001
[ Sun Jul 14 01:24:38 2024 ] 
Training: Epoch [23/50], Step [3999], Loss: 0.014225764200091362, Training Accuracy: 97.45625
[ Sun Jul 14 01:24:39 2024 ] 	Batch(4000/6809) done. Loss: 0.0096  lr:0.000001
[ Sun Jul 14 01:24:57 2024 ] 	Batch(4100/6809) done. Loss: 0.0638  lr:0.000001
[ Sun Jul 14 01:25:14 2024 ] 	Batch(4200/6809) done. Loss: 0.2412  lr:0.000001
[ Sun Jul 14 01:25:32 2024 ] 	Batch(4300/6809) done. Loss: 0.1261  lr:0.000001
[ Sun Jul 14 01:25:50 2024 ] 	Batch(4400/6809) done. Loss: 0.0495  lr:0.000001
[ Sun Jul 14 01:26:08 2024 ] 
Training: Epoch [23/50], Step [4499], Loss: 0.03670963644981384, Training Accuracy: 97.43888888888888
[ Sun Jul 14 01:26:08 2024 ] 	Batch(4500/6809) done. Loss: 0.1184  lr:0.000001
[ Sun Jul 14 01:26:26 2024 ] 	Batch(4600/6809) done. Loss: 0.1795  lr:0.000001
[ Sun Jul 14 01:26:44 2024 ] 	Batch(4700/6809) done. Loss: 0.0052  lr:0.000001
[ Sun Jul 14 01:27:02 2024 ] 	Batch(4800/6809) done. Loss: 0.0426  lr:0.000001
[ Sun Jul 14 01:27:20 2024 ] 	Batch(4900/6809) done. Loss: 0.0409  lr:0.000001
[ Sun Jul 14 01:27:38 2024 ] 
Training: Epoch [23/50], Step [4999], Loss: 0.1343257874250412, Training Accuracy: 97.45
[ Sun Jul 14 01:27:38 2024 ] 	Batch(5000/6809) done. Loss: 0.2053  lr:0.000001
[ Sun Jul 14 01:27:56 2024 ] 	Batch(5100/6809) done. Loss: 0.0117  lr:0.000001
[ Sun Jul 14 01:28:14 2024 ] 	Batch(5200/6809) done. Loss: 0.0539  lr:0.000001
[ Sun Jul 14 01:28:32 2024 ] 	Batch(5300/6809) done. Loss: 0.5240  lr:0.000001
[ Sun Jul 14 01:28:50 2024 ] 	Batch(5400/6809) done. Loss: 0.1961  lr:0.000001
[ Sun Jul 14 01:29:07 2024 ] 
Training: Epoch [23/50], Step [5499], Loss: 0.02862103097140789, Training Accuracy: 97.44545454545455
[ Sun Jul 14 01:29:08 2024 ] 	Batch(5500/6809) done. Loss: 0.0431  lr:0.000001
[ Sun Jul 14 01:29:26 2024 ] 	Batch(5600/6809) done. Loss: 0.0286  lr:0.000001
[ Sun Jul 14 01:29:44 2024 ] 	Batch(5700/6809) done. Loss: 0.1605  lr:0.000001
[ Sun Jul 14 01:30:01 2024 ] 	Batch(5800/6809) done. Loss: 0.1141  lr:0.000001
[ Sun Jul 14 01:30:19 2024 ] 	Batch(5900/6809) done. Loss: 0.0059  lr:0.000001
[ Sun Jul 14 01:30:37 2024 ] 
Training: Epoch [23/50], Step [5999], Loss: 0.2957676649093628, Training Accuracy: 97.47083333333333
[ Sun Jul 14 01:30:37 2024 ] 	Batch(6000/6809) done. Loss: 0.0286  lr:0.000001
[ Sun Jul 14 01:30:55 2024 ] 	Batch(6100/6809) done. Loss: 0.1219  lr:0.000001
[ Sun Jul 14 01:31:13 2024 ] 	Batch(6200/6809) done. Loss: 0.0263  lr:0.000001
[ Sun Jul 14 01:31:31 2024 ] 	Batch(6300/6809) done. Loss: 0.0275  lr:0.000001
[ Sun Jul 14 01:31:49 2024 ] 	Batch(6400/6809) done. Loss: 0.0108  lr:0.000001
[ Sun Jul 14 01:32:07 2024 ] 
Training: Epoch [23/50], Step [6499], Loss: 0.2608896791934967, Training Accuracy: 97.48653846153846
[ Sun Jul 14 01:32:07 2024 ] 	Batch(6500/6809) done. Loss: 0.0409  lr:0.000001
[ Sun Jul 14 01:32:25 2024 ] 	Batch(6600/6809) done. Loss: 0.0280  lr:0.000001
[ Sun Jul 14 01:32:43 2024 ] 	Batch(6700/6809) done. Loss: 0.1182  lr:0.000001
[ Sun Jul 14 01:33:01 2024 ] 	Batch(6800/6809) done. Loss: 0.2055  lr:0.000001
[ Sun Jul 14 01:33:02 2024 ] 	Mean training loss: 0.0999.
[ Sun Jul 14 01:33:02 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 01:33:03 2024 ] Training epoch: 25
[ Sun Jul 14 01:33:03 2024 ] 	Batch(0/6809) done. Loss: 0.1806  lr:0.000001
[ Sun Jul 14 01:33:21 2024 ] 	Batch(100/6809) done. Loss: 0.0061  lr:0.000001
[ Sun Jul 14 01:33:40 2024 ] 	Batch(200/6809) done. Loss: 0.1398  lr:0.000001
[ Sun Jul 14 01:33:58 2024 ] 	Batch(300/6809) done. Loss: 0.1304  lr:0.000001
[ Sun Jul 14 01:34:17 2024 ] 	Batch(400/6809) done. Loss: 0.0340  lr:0.000001
[ Sun Jul 14 01:34:35 2024 ] 
Training: Epoch [24/50], Step [499], Loss: 0.14156752824783325, Training Accuracy: 97.625
[ Sun Jul 14 01:34:35 2024 ] 	Batch(500/6809) done. Loss: 0.0733  lr:0.000001
[ Sun Jul 14 01:34:54 2024 ] 	Batch(600/6809) done. Loss: 0.0071  lr:0.000001
[ Sun Jul 14 01:35:12 2024 ] 	Batch(700/6809) done. Loss: 0.0582  lr:0.000001
[ Sun Jul 14 01:35:30 2024 ] 	Batch(800/6809) done. Loss: 0.0475  lr:0.000001
[ Sun Jul 14 01:35:48 2024 ] 	Batch(900/6809) done. Loss: 0.0916  lr:0.000001
[ Sun Jul 14 01:36:05 2024 ] 
Training: Epoch [24/50], Step [999], Loss: 0.05803672969341278, Training Accuracy: 97.6
[ Sun Jul 14 01:36:05 2024 ] 	Batch(1000/6809) done. Loss: 0.2056  lr:0.000001
[ Sun Jul 14 01:36:23 2024 ] 	Batch(1100/6809) done. Loss: 0.0052  lr:0.000001
[ Sun Jul 14 01:36:42 2024 ] 	Batch(1200/6809) done. Loss: 0.1310  lr:0.000001
[ Sun Jul 14 01:37:00 2024 ] 	Batch(1300/6809) done. Loss: 0.0434  lr:0.000001
[ Sun Jul 14 01:37:17 2024 ] 	Batch(1400/6809) done. Loss: 0.1298  lr:0.000001
[ Sun Jul 14 01:37:35 2024 ] 
Training: Epoch [24/50], Step [1499], Loss: 0.0452580526471138, Training Accuracy: 97.69166666666666
[ Sun Jul 14 01:37:35 2024 ] 	Batch(1500/6809) done. Loss: 0.0271  lr:0.000001
[ Sun Jul 14 01:37:53 2024 ] 	Batch(1600/6809) done. Loss: 0.0198  lr:0.000001
[ Sun Jul 14 01:38:11 2024 ] 	Batch(1700/6809) done. Loss: 0.0203  lr:0.000001
[ Sun Jul 14 01:38:29 2024 ] 	Batch(1800/6809) done. Loss: 0.1339  lr:0.000001
[ Sun Jul 14 01:38:47 2024 ] 	Batch(1900/6809) done. Loss: 0.1107  lr:0.000001
[ Sun Jul 14 01:39:05 2024 ] 
Training: Epoch [24/50], Step [1999], Loss: 0.07375980168581009, Training Accuracy: 97.7375
[ Sun Jul 14 01:39:05 2024 ] 	Batch(2000/6809) done. Loss: 0.0281  lr:0.000001
[ Sun Jul 14 01:39:23 2024 ] 	Batch(2100/6809) done. Loss: 0.1569  lr:0.000001
[ Sun Jul 14 01:39:41 2024 ] 	Batch(2200/6809) done. Loss: 0.0211  lr:0.000001
[ Sun Jul 14 01:39:59 2024 ] 	Batch(2300/6809) done. Loss: 0.0278  lr:0.000001
[ Sun Jul 14 01:40:17 2024 ] 	Batch(2400/6809) done. Loss: 0.3849  lr:0.000001
[ Sun Jul 14 01:40:34 2024 ] 
Training: Epoch [24/50], Step [2499], Loss: 0.012228497304022312, Training Accuracy: 97.795
[ Sun Jul 14 01:40:35 2024 ] 	Batch(2500/6809) done. Loss: 0.2319  lr:0.000001
[ Sun Jul 14 01:40:52 2024 ] 	Batch(2600/6809) done. Loss: 0.0641  lr:0.000001
[ Sun Jul 14 01:41:10 2024 ] 	Batch(2700/6809) done. Loss: 0.1169  lr:0.000001
[ Sun Jul 14 01:41:28 2024 ] 	Batch(2800/6809) done. Loss: 0.0558  lr:0.000001
[ Sun Jul 14 01:41:46 2024 ] 	Batch(2900/6809) done. Loss: 0.0481  lr:0.000001
[ Sun Jul 14 01:42:04 2024 ] 
Training: Epoch [24/50], Step [2999], Loss: 0.027197575196623802, Training Accuracy: 97.75833333333334
[ Sun Jul 14 01:42:04 2024 ] 	Batch(3000/6809) done. Loss: 0.0026  lr:0.000001
[ Sun Jul 14 01:42:22 2024 ] 	Batch(3100/6809) done. Loss: 0.1451  lr:0.000001
[ Sun Jul 14 01:42:40 2024 ] 	Batch(3200/6809) done. Loss: 0.0119  lr:0.000001
[ Sun Jul 14 01:42:59 2024 ] 	Batch(3300/6809) done. Loss: 0.2438  lr:0.000001
[ Sun Jul 14 01:43:17 2024 ] 	Batch(3400/6809) done. Loss: 0.1695  lr:0.000001
[ Sun Jul 14 01:43:36 2024 ] 
Training: Epoch [24/50], Step [3499], Loss: 0.2856625020503998, Training Accuracy: 97.72142857142858
[ Sun Jul 14 01:43:36 2024 ] 	Batch(3500/6809) done. Loss: 0.0398  lr:0.000001
[ Sun Jul 14 01:43:54 2024 ] 	Batch(3600/6809) done. Loss: 0.0136  lr:0.000001
[ Sun Jul 14 01:44:13 2024 ] 	Batch(3700/6809) done. Loss: 0.0122  lr:0.000001
[ Sun Jul 14 01:44:31 2024 ] 	Batch(3800/6809) done. Loss: 0.1412  lr:0.000001
[ Sun Jul 14 01:44:49 2024 ] 	Batch(3900/6809) done. Loss: 0.4573  lr:0.000001
[ Sun Jul 14 01:45:07 2024 ] 
Training: Epoch [24/50], Step [3999], Loss: 0.08134046941995621, Training Accuracy: 97.746875
[ Sun Jul 14 01:45:07 2024 ] 	Batch(4000/6809) done. Loss: 0.0444  lr:0.000001
[ Sun Jul 14 01:45:25 2024 ] 	Batch(4100/6809) done. Loss: 0.3776  lr:0.000001
[ Sun Jul 14 01:45:43 2024 ] 	Batch(4200/6809) done. Loss: 0.0116  lr:0.000001
[ Sun Jul 14 01:46:01 2024 ] 	Batch(4300/6809) done. Loss: 0.0069  lr:0.000001
[ Sun Jul 14 01:46:19 2024 ] 	Batch(4400/6809) done. Loss: 0.0407  lr:0.000001
[ Sun Jul 14 01:46:37 2024 ] 
Training: Epoch [24/50], Step [4499], Loss: 0.2326766699552536, Training Accuracy: 97.73611111111111
[ Sun Jul 14 01:46:37 2024 ] 	Batch(4500/6809) done. Loss: 0.0271  lr:0.000001
[ Sun Jul 14 01:46:55 2024 ] 	Batch(4600/6809) done. Loss: 0.0775  lr:0.000001
[ Sun Jul 14 01:47:13 2024 ] 	Batch(4700/6809) done. Loss: 0.3428  lr:0.000001
[ Sun Jul 14 01:47:32 2024 ] 	Batch(4800/6809) done. Loss: 0.0229  lr:0.000001
[ Sun Jul 14 01:47:50 2024 ] 	Batch(4900/6809) done. Loss: 0.1835  lr:0.000001
[ Sun Jul 14 01:48:08 2024 ] 
Training: Epoch [24/50], Step [4999], Loss: 0.05771335959434509, Training Accuracy: 97.6975
[ Sun Jul 14 01:48:08 2024 ] 	Batch(5000/6809) done. Loss: 0.0610  lr:0.000001
[ Sun Jul 14 01:48:26 2024 ] 	Batch(5100/6809) done. Loss: 0.0356  lr:0.000001
[ Sun Jul 14 01:48:44 2024 ] 	Batch(5200/6809) done. Loss: 0.1163  lr:0.000001
[ Sun Jul 14 01:49:02 2024 ] 	Batch(5300/6809) done. Loss: 0.1811  lr:0.000001
[ Sun Jul 14 01:49:20 2024 ] 	Batch(5400/6809) done. Loss: 0.0692  lr:0.000001
[ Sun Jul 14 01:49:38 2024 ] 
Training: Epoch [24/50], Step [5499], Loss: 0.030450981110334396, Training Accuracy: 97.72954545454546
[ Sun Jul 14 01:49:38 2024 ] 	Batch(5500/6809) done. Loss: 0.2142  lr:0.000001
[ Sun Jul 14 01:49:56 2024 ] 	Batch(5600/6809) done. Loss: 0.0201  lr:0.000001
[ Sun Jul 14 01:50:14 2024 ] 	Batch(5700/6809) done. Loss: 0.2490  lr:0.000001
[ Sun Jul 14 01:50:32 2024 ] 	Batch(5800/6809) done. Loss: 0.1946  lr:0.000001
[ Sun Jul 14 01:50:50 2024 ] 	Batch(5900/6809) done. Loss: 0.0131  lr:0.000001
[ Sun Jul 14 01:51:08 2024 ] 
Training: Epoch [24/50], Step [5999], Loss: 0.07689503580331802, Training Accuracy: 97.73541666666667
[ Sun Jul 14 01:51:09 2024 ] 	Batch(6000/6809) done. Loss: 0.2862  lr:0.000001
[ Sun Jul 14 01:51:27 2024 ] 	Batch(6100/6809) done. Loss: 0.2340  lr:0.000001
[ Sun Jul 14 01:51:45 2024 ] 	Batch(6200/6809) done. Loss: 0.1948  lr:0.000001
[ Sun Jul 14 01:52:03 2024 ] 	Batch(6300/6809) done. Loss: 0.0647  lr:0.000001
[ Sun Jul 14 01:52:21 2024 ] 	Batch(6400/6809) done. Loss: 0.1613  lr:0.000001
[ Sun Jul 14 01:52:39 2024 ] 
Training: Epoch [24/50], Step [6499], Loss: 0.0106448233127594, Training Accuracy: 97.70961538461539
[ Sun Jul 14 01:52:39 2024 ] 	Batch(6500/6809) done. Loss: 0.3952  lr:0.000001
[ Sun Jul 14 01:52:57 2024 ] 	Batch(6600/6809) done. Loss: 0.0083  lr:0.000001
[ Sun Jul 14 01:53:15 2024 ] 	Batch(6700/6809) done. Loss: 0.0396  lr:0.000001
[ Sun Jul 14 01:53:33 2024 ] 	Batch(6800/6809) done. Loss: 0.1276  lr:0.000001
[ Sun Jul 14 01:53:35 2024 ] 	Mean training loss: 0.0925.
[ Sun Jul 14 01:53:35 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 01:53:35 2024 ] Training epoch: 26
[ Sun Jul 14 01:53:36 2024 ] 	Batch(0/6809) done. Loss: 0.0313  lr:0.000001
[ Sun Jul 14 01:53:55 2024 ] 	Batch(100/6809) done. Loss: 0.0082  lr:0.000001
[ Sun Jul 14 01:54:13 2024 ] 	Batch(200/6809) done. Loss: 0.0156  lr:0.000001
[ Sun Jul 14 01:54:32 2024 ] 	Batch(300/6809) done. Loss: 0.1875  lr:0.000001
[ Sun Jul 14 01:54:51 2024 ] 	Batch(400/6809) done. Loss: 0.0051  lr:0.000001
[ Sun Jul 14 01:55:10 2024 ] 
Training: Epoch [25/50], Step [499], Loss: 0.29397261142730713, Training Accuracy: 97.775
[ Sun Jul 14 01:55:10 2024 ] 	Batch(500/6809) done. Loss: 0.0066  lr:0.000001
[ Sun Jul 14 01:55:28 2024 ] 	Batch(600/6809) done. Loss: 0.0298  lr:0.000001
[ Sun Jul 14 01:55:46 2024 ] 	Batch(700/6809) done. Loss: 0.0239  lr:0.000001
[ Sun Jul 14 01:56:04 2024 ] 	Batch(800/6809) done. Loss: 0.0674  lr:0.000001
[ Sun Jul 14 01:56:22 2024 ] 	Batch(900/6809) done. Loss: 0.1566  lr:0.000001
[ Sun Jul 14 01:56:40 2024 ] 
Training: Epoch [25/50], Step [999], Loss: 0.032508786767721176, Training Accuracy: 97.82499999999999
[ Sun Jul 14 01:56:40 2024 ] 	Batch(1000/6809) done. Loss: 0.0187  lr:0.000001
[ Sun Jul 14 01:56:58 2024 ] 	Batch(1100/6809) done. Loss: 0.0873  lr:0.000001
[ Sun Jul 14 01:57:17 2024 ] 	Batch(1200/6809) done. Loss: 0.0074  lr:0.000001
[ Sun Jul 14 01:57:35 2024 ] 	Batch(1300/6809) done. Loss: 0.1620  lr:0.000001
[ Sun Jul 14 01:57:53 2024 ] 	Batch(1400/6809) done. Loss: 0.0422  lr:0.000001
[ Sun Jul 14 01:58:11 2024 ] 
Training: Epoch [25/50], Step [1499], Loss: 0.09306058287620544, Training Accuracy: 97.8
[ Sun Jul 14 01:58:11 2024 ] 	Batch(1500/6809) done. Loss: 0.2467  lr:0.000001
[ Sun Jul 14 01:58:29 2024 ] 	Batch(1600/6809) done. Loss: 0.1114  lr:0.000001
[ Sun Jul 14 01:58:47 2024 ] 	Batch(1700/6809) done. Loss: 0.0385  lr:0.000001
[ Sun Jul 14 01:59:05 2024 ] 	Batch(1800/6809) done. Loss: 0.0536  lr:0.000001
[ Sun Jul 14 01:59:23 2024 ] 	Batch(1900/6809) done. Loss: 0.1734  lr:0.000001
[ Sun Jul 14 01:59:41 2024 ] 
Training: Epoch [25/50], Step [1999], Loss: 0.00698069017380476, Training Accuracy: 97.8
[ Sun Jul 14 01:59:41 2024 ] 	Batch(2000/6809) done. Loss: 0.0307  lr:0.000001
[ Sun Jul 14 02:00:00 2024 ] 	Batch(2100/6809) done. Loss: 0.0885  lr:0.000001
[ Sun Jul 14 02:00:19 2024 ] 	Batch(2200/6809) done. Loss: 0.0024  lr:0.000001
[ Sun Jul 14 02:00:38 2024 ] 	Batch(2300/6809) done. Loss: 0.0228  lr:0.000001
[ Sun Jul 14 02:00:56 2024 ] 	Batch(2400/6809) done. Loss: 0.0371  lr:0.000001
[ Sun Jul 14 02:01:15 2024 ] 
Training: Epoch [25/50], Step [2499], Loss: 0.31727296113967896, Training Accuracy: 97.8
[ Sun Jul 14 02:01:15 2024 ] 	Batch(2500/6809) done. Loss: 0.1317  lr:0.000001
[ Sun Jul 14 02:01:33 2024 ] 	Batch(2600/6809) done. Loss: 0.0622  lr:0.000001
[ Sun Jul 14 02:01:52 2024 ] 	Batch(2700/6809) done. Loss: 0.0640  lr:0.000001
[ Sun Jul 14 02:02:11 2024 ] 	Batch(2800/6809) done. Loss: 0.0574  lr:0.000001
[ Sun Jul 14 02:02:29 2024 ] 	Batch(2900/6809) done. Loss: 0.0375  lr:0.000001
[ Sun Jul 14 02:02:48 2024 ] 
Training: Epoch [25/50], Step [2999], Loss: 0.08262933045625687, Training Accuracy: 97.76666666666667
[ Sun Jul 14 02:02:48 2024 ] 	Batch(3000/6809) done. Loss: 0.0037  lr:0.000001
[ Sun Jul 14 02:03:06 2024 ] 	Batch(3100/6809) done. Loss: 0.0190  lr:0.000001
[ Sun Jul 14 02:03:25 2024 ] 	Batch(3200/6809) done. Loss: 0.0850  lr:0.000001
[ Sun Jul 14 02:03:43 2024 ] 	Batch(3300/6809) done. Loss: 0.0082  lr:0.000001
[ Sun Jul 14 02:04:01 2024 ] 	Batch(3400/6809) done. Loss: 0.1178  lr:0.000001
[ Sun Jul 14 02:04:19 2024 ] 
Training: Epoch [25/50], Step [3499], Loss: 0.0037052915431559086, Training Accuracy: 97.67857142857143
[ Sun Jul 14 02:04:19 2024 ] 	Batch(3500/6809) done. Loss: 0.2465  lr:0.000001
[ Sun Jul 14 02:04:37 2024 ] 	Batch(3600/6809) done. Loss: 0.0036  lr:0.000001
[ Sun Jul 14 02:04:55 2024 ] 	Batch(3700/6809) done. Loss: 0.0685  lr:0.000001
[ Sun Jul 14 02:05:13 2024 ] 	Batch(3800/6809) done. Loss: 0.0352  lr:0.000001
[ Sun Jul 14 02:05:31 2024 ] 	Batch(3900/6809) done. Loss: 0.0350  lr:0.000001
[ Sun Jul 14 02:05:49 2024 ] 
Training: Epoch [25/50], Step [3999], Loss: 0.2371232509613037, Training Accuracy: 97.69375
[ Sun Jul 14 02:05:49 2024 ] 	Batch(4000/6809) done. Loss: 0.0983  lr:0.000001
[ Sun Jul 14 02:06:07 2024 ] 	Batch(4100/6809) done. Loss: 0.1745  lr:0.000001
[ Sun Jul 14 02:06:25 2024 ] 	Batch(4200/6809) done. Loss: 0.0709  lr:0.000001
[ Sun Jul 14 02:06:43 2024 ] 	Batch(4300/6809) done. Loss: 0.0089  lr:0.000001
[ Sun Jul 14 02:07:01 2024 ] 	Batch(4400/6809) done. Loss: 0.0862  lr:0.000001
[ Sun Jul 14 02:07:18 2024 ] 
Training: Epoch [25/50], Step [4499], Loss: 0.025414466857910156, Training Accuracy: 97.65
[ Sun Jul 14 02:07:19 2024 ] 	Batch(4500/6809) done. Loss: 0.0043  lr:0.000001
[ Sun Jul 14 02:07:36 2024 ] 	Batch(4600/6809) done. Loss: 0.0016  lr:0.000001
[ Sun Jul 14 02:07:54 2024 ] 	Batch(4700/6809) done. Loss: 0.0677  lr:0.000001
[ Sun Jul 14 02:08:12 2024 ] 	Batch(4800/6809) done. Loss: 0.0869  lr:0.000001
[ Sun Jul 14 02:08:30 2024 ] 	Batch(4900/6809) done. Loss: 0.0497  lr:0.000001
[ Sun Jul 14 02:08:48 2024 ] 
Training: Epoch [25/50], Step [4999], Loss: 0.08577681332826614, Training Accuracy: 97.615
[ Sun Jul 14 02:08:48 2024 ] 	Batch(5000/6809) done. Loss: 0.0292  lr:0.000001
[ Sun Jul 14 02:09:06 2024 ] 	Batch(5100/6809) done. Loss: 0.0389  lr:0.000001
[ Sun Jul 14 02:09:24 2024 ] 	Batch(5200/6809) done. Loss: 0.1196  lr:0.000001
[ Sun Jul 14 02:09:43 2024 ] 	Batch(5300/6809) done. Loss: 0.0304  lr:0.000001
[ Sun Jul 14 02:10:01 2024 ] 	Batch(5400/6809) done. Loss: 0.0148  lr:0.000001
[ Sun Jul 14 02:10:19 2024 ] 
Training: Epoch [25/50], Step [5499], Loss: 0.055342767387628555, Training Accuracy: 97.59318181818182
[ Sun Jul 14 02:10:19 2024 ] 	Batch(5500/6809) done. Loss: 0.0509  lr:0.000001
[ Sun Jul 14 02:10:37 2024 ] 	Batch(5600/6809) done. Loss: 0.0760  lr:0.000001
[ Sun Jul 14 02:10:56 2024 ] 	Batch(5700/6809) done. Loss: 0.0450  lr:0.000001
[ Sun Jul 14 02:11:15 2024 ] 	Batch(5800/6809) done. Loss: 0.0822  lr:0.000001
[ Sun Jul 14 02:11:34 2024 ] 	Batch(5900/6809) done. Loss: 0.1177  lr:0.000001
[ Sun Jul 14 02:11:52 2024 ] 
Training: Epoch [25/50], Step [5999], Loss: 0.5653521418571472, Training Accuracy: 97.60416666666667
[ Sun Jul 14 02:11:53 2024 ] 	Batch(6000/6809) done. Loss: 0.1919  lr:0.000001
[ Sun Jul 14 02:12:11 2024 ] 	Batch(6100/6809) done. Loss: 0.0117  lr:0.000001
[ Sun Jul 14 02:12:30 2024 ] 	Batch(6200/6809) done. Loss: 0.0422  lr:0.000001
[ Sun Jul 14 02:12:48 2024 ] 	Batch(6300/6809) done. Loss: 0.1490  lr:0.000001
[ Sun Jul 14 02:13:07 2024 ] 	Batch(6400/6809) done. Loss: 0.0319  lr:0.000001
[ Sun Jul 14 02:13:25 2024 ] 
Training: Epoch [25/50], Step [6499], Loss: 0.09894227981567383, Training Accuracy: 97.60576923076924
[ Sun Jul 14 02:13:25 2024 ] 	Batch(6500/6809) done. Loss: 0.0028  lr:0.000001
[ Sun Jul 14 02:13:43 2024 ] 	Batch(6600/6809) done. Loss: 0.0271  lr:0.000001
[ Sun Jul 14 02:14:01 2024 ] 	Batch(6700/6809) done. Loss: 0.0178  lr:0.000001
[ Sun Jul 14 02:14:19 2024 ] 	Batch(6800/6809) done. Loss: 0.0108  lr:0.000001
[ Sun Jul 14 02:14:20 2024 ] 	Mean training loss: 0.0960.
[ Sun Jul 14 02:14:20 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 02:14:21 2024 ] Training epoch: 27
[ Sun Jul 14 02:14:21 2024 ] 	Batch(0/6809) done. Loss: 0.0269  lr:0.000001
[ Sun Jul 14 02:14:39 2024 ] 	Batch(100/6809) done. Loss: 0.0055  lr:0.000001
[ Sun Jul 14 02:14:57 2024 ] 	Batch(200/6809) done. Loss: 0.1318  lr:0.000001
[ Sun Jul 14 02:15:16 2024 ] 	Batch(300/6809) done. Loss: 0.0247  lr:0.000001
[ Sun Jul 14 02:15:34 2024 ] 	Batch(400/6809) done. Loss: 0.0294  lr:0.000001
[ Sun Jul 14 02:15:51 2024 ] 
Training: Epoch [26/50], Step [499], Loss: 0.021707752719521523, Training Accuracy: 98.075
[ Sun Jul 14 02:15:52 2024 ] 	Batch(500/6809) done. Loss: 0.1015  lr:0.000001
[ Sun Jul 14 02:16:09 2024 ] 	Batch(600/6809) done. Loss: 0.0849  lr:0.000001
[ Sun Jul 14 02:16:27 2024 ] 	Batch(700/6809) done. Loss: 0.0074  lr:0.000001
[ Sun Jul 14 02:16:45 2024 ] 	Batch(800/6809) done. Loss: 0.0804  lr:0.000001
[ Sun Jul 14 02:17:03 2024 ] 	Batch(900/6809) done. Loss: 0.0534  lr:0.000001
[ Sun Jul 14 02:17:21 2024 ] 
Training: Epoch [26/50], Step [999], Loss: 0.002440644660964608, Training Accuracy: 97.89999999999999
[ Sun Jul 14 02:17:21 2024 ] 	Batch(1000/6809) done. Loss: 0.0313  lr:0.000001
[ Sun Jul 14 02:17:39 2024 ] 	Batch(1100/6809) done. Loss: 0.0325  lr:0.000001
[ Sun Jul 14 02:17:57 2024 ] 	Batch(1200/6809) done. Loss: 0.0992  lr:0.000001
[ Sun Jul 14 02:18:15 2024 ] 	Batch(1300/6809) done. Loss: 0.3017  lr:0.000001
[ Sun Jul 14 02:18:33 2024 ] 	Batch(1400/6809) done. Loss: 0.0075  lr:0.000001
[ Sun Jul 14 02:18:51 2024 ] 
Training: Epoch [26/50], Step [1499], Loss: 0.03969912976026535, Training Accuracy: 97.69166666666666
[ Sun Jul 14 02:18:51 2024 ] 	Batch(1500/6809) done. Loss: 0.0329  lr:0.000001
[ Sun Jul 14 02:19:09 2024 ] 	Batch(1600/6809) done. Loss: 0.0440  lr:0.000001
[ Sun Jul 14 02:19:27 2024 ] 	Batch(1700/6809) done. Loss: 0.1035  lr:0.000001
[ Sun Jul 14 02:19:45 2024 ] 	Batch(1800/6809) done. Loss: 0.0189  lr:0.000001
[ Sun Jul 14 02:20:03 2024 ] 	Batch(1900/6809) done. Loss: 0.0754  lr:0.000001
[ Sun Jul 14 02:20:21 2024 ] 
Training: Epoch [26/50], Step [1999], Loss: 0.05537161976099014, Training Accuracy: 97.6375
[ Sun Jul 14 02:20:21 2024 ] 	Batch(2000/6809) done. Loss: 0.0084  lr:0.000001
[ Sun Jul 14 02:20:39 2024 ] 	Batch(2100/6809) done. Loss: 0.0805  lr:0.000001
[ Sun Jul 14 02:20:57 2024 ] 	Batch(2200/6809) done. Loss: 0.1843  lr:0.000001
[ Sun Jul 14 02:21:15 2024 ] 	Batch(2300/6809) done. Loss: 0.0132  lr:0.000001
[ Sun Jul 14 02:21:33 2024 ] 	Batch(2400/6809) done. Loss: 0.0394  lr:0.000001
[ Sun Jul 14 02:21:51 2024 ] 
Training: Epoch [26/50], Step [2499], Loss: 0.03722536563873291, Training Accuracy: 97.6
[ Sun Jul 14 02:21:51 2024 ] 	Batch(2500/6809) done. Loss: 0.0070  lr:0.000001
[ Sun Jul 14 02:22:09 2024 ] 	Batch(2600/6809) done. Loss: 0.0835  lr:0.000001
[ Sun Jul 14 02:22:27 2024 ] 	Batch(2700/6809) done. Loss: 0.0513  lr:0.000001
[ Sun Jul 14 02:22:45 2024 ] 	Batch(2800/6809) done. Loss: 0.0983  lr:0.000001
[ Sun Jul 14 02:23:03 2024 ] 	Batch(2900/6809) done. Loss: 0.1049  lr:0.000001
[ Sun Jul 14 02:23:21 2024 ] 
Training: Epoch [26/50], Step [2999], Loss: 0.01403595507144928, Training Accuracy: 97.63333333333334
[ Sun Jul 14 02:23:21 2024 ] 	Batch(3000/6809) done. Loss: 0.0538  lr:0.000001
[ Sun Jul 14 02:23:39 2024 ] 	Batch(3100/6809) done. Loss: 0.0133  lr:0.000001
[ Sun Jul 14 02:23:57 2024 ] 	Batch(3200/6809) done. Loss: 0.0167  lr:0.000001
[ Sun Jul 14 02:24:15 2024 ] 	Batch(3300/6809) done. Loss: 0.0499  lr:0.000001
[ Sun Jul 14 02:24:33 2024 ] 	Batch(3400/6809) done. Loss: 0.2522  lr:0.000001
[ Sun Jul 14 02:24:51 2024 ] 
Training: Epoch [26/50], Step [3499], Loss: 0.04174879938364029, Training Accuracy: 97.61428571428571
[ Sun Jul 14 02:24:51 2024 ] 	Batch(3500/6809) done. Loss: 0.0290  lr:0.000001
[ Sun Jul 14 02:25:09 2024 ] 	Batch(3600/6809) done. Loss: 0.0635  lr:0.000001
[ Sun Jul 14 02:25:27 2024 ] 	Batch(3700/6809) done. Loss: 0.0157  lr:0.000001
[ Sun Jul 14 02:25:45 2024 ] 	Batch(3800/6809) done. Loss: 0.0149  lr:0.000001
[ Sun Jul 14 02:26:03 2024 ] 	Batch(3900/6809) done. Loss: 0.2988  lr:0.000001
[ Sun Jul 14 02:26:21 2024 ] 
Training: Epoch [26/50], Step [3999], Loss: 0.0073324814438819885, Training Accuracy: 97.653125
[ Sun Jul 14 02:26:21 2024 ] 	Batch(4000/6809) done. Loss: 0.0752  lr:0.000001
[ Sun Jul 14 02:26:39 2024 ] 	Batch(4100/6809) done. Loss: 0.2134  lr:0.000001
[ Sun Jul 14 02:26:57 2024 ] 	Batch(4200/6809) done. Loss: 0.0140  lr:0.000001
[ Sun Jul 14 02:27:15 2024 ] 	Batch(4300/6809) done. Loss: 0.1593  lr:0.000001
[ Sun Jul 14 02:27:33 2024 ] 	Batch(4400/6809) done. Loss: 0.1911  lr:0.000001
[ Sun Jul 14 02:27:50 2024 ] 
Training: Epoch [26/50], Step [4499], Loss: 0.09793800115585327, Training Accuracy: 97.63333333333334
[ Sun Jul 14 02:27:51 2024 ] 	Batch(4500/6809) done. Loss: 0.0230  lr:0.000001
[ Sun Jul 14 02:28:08 2024 ] 	Batch(4600/6809) done. Loss: 0.0436  lr:0.000001
[ Sun Jul 14 02:28:26 2024 ] 	Batch(4700/6809) done. Loss: 0.1329  lr:0.000001
[ Sun Jul 14 02:28:45 2024 ] 	Batch(4800/6809) done. Loss: 0.0245  lr:0.000001
[ Sun Jul 14 02:29:02 2024 ] 	Batch(4900/6809) done. Loss: 0.0645  lr:0.000001
[ Sun Jul 14 02:29:20 2024 ] 
Training: Epoch [26/50], Step [4999], Loss: 0.02071288414299488, Training Accuracy: 97.6625
[ Sun Jul 14 02:29:20 2024 ] 	Batch(5000/6809) done. Loss: 0.0870  lr:0.000001
[ Sun Jul 14 02:29:38 2024 ] 	Batch(5100/6809) done. Loss: 0.1666  lr:0.000001
[ Sun Jul 14 02:29:56 2024 ] 	Batch(5200/6809) done. Loss: 0.3105  lr:0.000001
[ Sun Jul 14 02:30:14 2024 ] 	Batch(5300/6809) done. Loss: 0.0523  lr:0.000001
[ Sun Jul 14 02:30:32 2024 ] 	Batch(5400/6809) done. Loss: 0.1148  lr:0.000001
[ Sun Jul 14 02:30:50 2024 ] 
Training: Epoch [26/50], Step [5499], Loss: 0.028108026832342148, Training Accuracy: 97.66136363636365
[ Sun Jul 14 02:30:50 2024 ] 	Batch(5500/6809) done. Loss: 0.2978  lr:0.000001
[ Sun Jul 14 02:31:08 2024 ] 	Batch(5600/6809) done. Loss: 0.1613  lr:0.000001
[ Sun Jul 14 02:31:26 2024 ] 	Batch(5700/6809) done. Loss: 0.0040  lr:0.000001
[ Sun Jul 14 02:31:44 2024 ] 	Batch(5800/6809) done. Loss: 0.0693  lr:0.000001
[ Sun Jul 14 02:32:02 2024 ] 	Batch(5900/6809) done. Loss: 0.1112  lr:0.000001
[ Sun Jul 14 02:32:20 2024 ] 
Training: Epoch [26/50], Step [5999], Loss: 0.0019009019015356898, Training Accuracy: 97.67916666666666
[ Sun Jul 14 02:32:20 2024 ] 	Batch(6000/6809) done. Loss: 0.0623  lr:0.000001
[ Sun Jul 14 02:32:39 2024 ] 	Batch(6100/6809) done. Loss: 0.0118  lr:0.000001
[ Sun Jul 14 02:32:57 2024 ] 	Batch(6200/6809) done. Loss: 0.0286  lr:0.000001
[ Sun Jul 14 02:33:15 2024 ] 	Batch(6300/6809) done. Loss: 0.1048  lr:0.000001
[ Sun Jul 14 02:33:33 2024 ] 	Batch(6400/6809) done. Loss: 0.0978  lr:0.000001
[ Sun Jul 14 02:33:51 2024 ] 
Training: Epoch [26/50], Step [6499], Loss: 0.012088729068636894, Training Accuracy: 97.68461538461538
[ Sun Jul 14 02:33:51 2024 ] 	Batch(6500/6809) done. Loss: 0.0182  lr:0.000001
[ Sun Jul 14 02:34:09 2024 ] 	Batch(6600/6809) done. Loss: 0.0488  lr:0.000001
[ Sun Jul 14 02:34:27 2024 ] 	Batch(6700/6809) done. Loss: 0.0076  lr:0.000001
[ Sun Jul 14 02:34:45 2024 ] 	Batch(6800/6809) done. Loss: 0.0499  lr:0.000001
[ Sun Jul 14 02:34:46 2024 ] 	Mean training loss: 0.0935.
[ Sun Jul 14 02:34:46 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 02:34:46 2024 ] Training epoch: 28
[ Sun Jul 14 02:34:47 2024 ] 	Batch(0/6809) done. Loss: 0.0410  lr:0.000001
[ Sun Jul 14 02:35:05 2024 ] 	Batch(100/6809) done. Loss: 0.0044  lr:0.000001
[ Sun Jul 14 02:35:23 2024 ] 	Batch(200/6809) done. Loss: 0.1419  lr:0.000001
[ Sun Jul 14 02:35:41 2024 ] 	Batch(300/6809) done. Loss: 0.2171  lr:0.000001
[ Sun Jul 14 02:35:59 2024 ] 	Batch(400/6809) done. Loss: 0.0470  lr:0.000001
[ Sun Jul 14 02:36:16 2024 ] 
Training: Epoch [27/50], Step [499], Loss: 0.026298902928829193, Training Accuracy: 97.32499999999999
[ Sun Jul 14 02:36:17 2024 ] 	Batch(500/6809) done. Loss: 0.1226  lr:0.000001
[ Sun Jul 14 02:36:34 2024 ] 	Batch(600/6809) done. Loss: 0.0836  lr:0.000001
[ Sun Jul 14 02:36:52 2024 ] 	Batch(700/6809) done. Loss: 0.1619  lr:0.000001
[ Sun Jul 14 02:37:10 2024 ] 	Batch(800/6809) done. Loss: 0.0469  lr:0.000001
[ Sun Jul 14 02:37:28 2024 ] 	Batch(900/6809) done. Loss: 0.0807  lr:0.000001
[ Sun Jul 14 02:37:46 2024 ] 
Training: Epoch [27/50], Step [999], Loss: 0.026756903156638145, Training Accuracy: 97.575
[ Sun Jul 14 02:37:46 2024 ] 	Batch(1000/6809) done. Loss: 0.2334  lr:0.000001
[ Sun Jul 14 02:38:04 2024 ] 	Batch(1100/6809) done. Loss: 0.0916  lr:0.000001
[ Sun Jul 14 02:38:22 2024 ] 	Batch(1200/6809) done. Loss: 0.1504  lr:0.000001
[ Sun Jul 14 02:38:40 2024 ] 	Batch(1300/6809) done. Loss: 0.0138  lr:0.000001
[ Sun Jul 14 02:38:58 2024 ] 	Batch(1400/6809) done. Loss: 0.2069  lr:0.000001
[ Sun Jul 14 02:39:16 2024 ] 
Training: Epoch [27/50], Step [1499], Loss: 0.018209291622042656, Training Accuracy: 97.54166666666667
[ Sun Jul 14 02:39:16 2024 ] 	Batch(1500/6809) done. Loss: 0.1287  lr:0.000001
[ Sun Jul 14 02:39:34 2024 ] 	Batch(1600/6809) done. Loss: 0.0054  lr:0.000001
[ Sun Jul 14 02:39:52 2024 ] 	Batch(1700/6809) done. Loss: 0.1389  lr:0.000001
[ Sun Jul 14 02:40:10 2024 ] 	Batch(1800/6809) done. Loss: 0.0222  lr:0.000001
[ Sun Jul 14 02:40:28 2024 ] 	Batch(1900/6809) done. Loss: 0.2281  lr:0.000001
[ Sun Jul 14 02:40:45 2024 ] 
Training: Epoch [27/50], Step [1999], Loss: 0.10120551288127899, Training Accuracy: 97.575
[ Sun Jul 14 02:40:46 2024 ] 	Batch(2000/6809) done. Loss: 0.0120  lr:0.000001
[ Sun Jul 14 02:41:03 2024 ] 	Batch(2100/6809) done. Loss: 0.4167  lr:0.000001
[ Sun Jul 14 02:41:21 2024 ] 	Batch(2200/6809) done. Loss: 0.3133  lr:0.000001
[ Sun Jul 14 02:41:39 2024 ] 	Batch(2300/6809) done. Loss: 0.1187  lr:0.000001
[ Sun Jul 14 02:41:57 2024 ] 	Batch(2400/6809) done. Loss: 0.2586  lr:0.000001
[ Sun Jul 14 02:42:15 2024 ] 
Training: Epoch [27/50], Step [2499], Loss: 0.0808921754360199, Training Accuracy: 97.44500000000001
[ Sun Jul 14 02:42:15 2024 ] 	Batch(2500/6809) done. Loss: 0.0170  lr:0.000001
[ Sun Jul 14 02:42:33 2024 ] 	Batch(2600/6809) done. Loss: 0.1263  lr:0.000001
[ Sun Jul 14 02:42:51 2024 ] 	Batch(2700/6809) done. Loss: 0.0215  lr:0.000001
[ Sun Jul 14 02:43:09 2024 ] 	Batch(2800/6809) done. Loss: 0.0270  lr:0.000001
[ Sun Jul 14 02:43:27 2024 ] 	Batch(2900/6809) done. Loss: 0.0435  lr:0.000001
[ Sun Jul 14 02:43:45 2024 ] 
Training: Epoch [27/50], Step [2999], Loss: 0.019746197387576103, Training Accuracy: 97.52083333333333
[ Sun Jul 14 02:43:45 2024 ] 	Batch(3000/6809) done. Loss: 0.2649  lr:0.000001
[ Sun Jul 14 02:44:03 2024 ] 	Batch(3100/6809) done. Loss: 0.0056  lr:0.000001
[ Sun Jul 14 02:44:21 2024 ] 	Batch(3200/6809) done. Loss: 0.0485  lr:0.000001
[ Sun Jul 14 02:44:39 2024 ] 	Batch(3300/6809) done. Loss: 0.0366  lr:0.000001
[ Sun Jul 14 02:44:57 2024 ] 	Batch(3400/6809) done. Loss: 0.1250  lr:0.000001
[ Sun Jul 14 02:45:15 2024 ] 
Training: Epoch [27/50], Step [3499], Loss: 0.02648024447262287, Training Accuracy: 97.56785714285714
[ Sun Jul 14 02:45:15 2024 ] 	Batch(3500/6809) done. Loss: 0.0105  lr:0.000001
[ Sun Jul 14 02:45:33 2024 ] 	Batch(3600/6809) done. Loss: 0.0733  lr:0.000001
[ Sun Jul 14 02:45:51 2024 ] 	Batch(3700/6809) done. Loss: 0.0950  lr:0.000001
[ Sun Jul 14 02:46:09 2024 ] 	Batch(3800/6809) done. Loss: 0.0548  lr:0.000001
[ Sun Jul 14 02:46:27 2024 ] 	Batch(3900/6809) done. Loss: 0.0478  lr:0.000001
[ Sun Jul 14 02:46:45 2024 ] 
Training: Epoch [27/50], Step [3999], Loss: 0.08473526686429977, Training Accuracy: 97.55625
[ Sun Jul 14 02:46:45 2024 ] 	Batch(4000/6809) done. Loss: 0.0540  lr:0.000001
[ Sun Jul 14 02:47:03 2024 ] 	Batch(4100/6809) done. Loss: 0.0913  lr:0.000001
[ Sun Jul 14 02:47:21 2024 ] 	Batch(4200/6809) done. Loss: 0.1658  lr:0.000001
[ Sun Jul 14 02:47:39 2024 ] 	Batch(4300/6809) done. Loss: 0.2556  lr:0.000001
[ Sun Jul 14 02:47:58 2024 ] 	Batch(4400/6809) done. Loss: 0.0131  lr:0.000001
[ Sun Jul 14 02:48:15 2024 ] 
Training: Epoch [27/50], Step [4499], Loss: 0.018147513270378113, Training Accuracy: 97.58611111111111
[ Sun Jul 14 02:48:16 2024 ] 	Batch(4500/6809) done. Loss: 0.0113  lr:0.000001
[ Sun Jul 14 02:48:34 2024 ] 	Batch(4600/6809) done. Loss: 0.0632  lr:0.000001
[ Sun Jul 14 02:48:52 2024 ] 	Batch(4700/6809) done. Loss: 0.0318  lr:0.000001
[ Sun Jul 14 02:49:10 2024 ] 	Batch(4800/6809) done. Loss: 0.0680  lr:0.000001
[ Sun Jul 14 02:49:28 2024 ] 	Batch(4900/6809) done. Loss: 0.1533  lr:0.000001
[ Sun Jul 14 02:49:47 2024 ] 
Training: Epoch [27/50], Step [4999], Loss: 0.16815219819545746, Training Accuracy: 97.58250000000001
[ Sun Jul 14 02:49:47 2024 ] 	Batch(5000/6809) done. Loss: 0.2011  lr:0.000001
[ Sun Jul 14 02:50:05 2024 ] 	Batch(5100/6809) done. Loss: 0.0155  lr:0.000001
[ Sun Jul 14 02:50:24 2024 ] 	Batch(5200/6809) done. Loss: 0.0077  lr:0.000001
[ Sun Jul 14 02:50:42 2024 ] 	Batch(5300/6809) done. Loss: 0.0231  lr:0.000001
[ Sun Jul 14 02:51:00 2024 ] 	Batch(5400/6809) done. Loss: 0.0330  lr:0.000001
[ Sun Jul 14 02:51:18 2024 ] 
Training: Epoch [27/50], Step [5499], Loss: 0.14856350421905518, Training Accuracy: 97.57954545454545
[ Sun Jul 14 02:51:18 2024 ] 	Batch(5500/6809) done. Loss: 0.3589  lr:0.000001
[ Sun Jul 14 02:51:36 2024 ] 	Batch(5600/6809) done. Loss: 0.4364  lr:0.000001
[ Sun Jul 14 02:51:54 2024 ] 	Batch(5700/6809) done. Loss: 0.0157  lr:0.000001
[ Sun Jul 14 02:52:12 2024 ] 	Batch(5800/6809) done. Loss: 0.0048  lr:0.000001
[ Sun Jul 14 02:52:30 2024 ] 	Batch(5900/6809) done. Loss: 0.0237  lr:0.000001
[ Sun Jul 14 02:52:48 2024 ] 
Training: Epoch [27/50], Step [5999], Loss: 0.010388016700744629, Training Accuracy: 97.59583333333333
[ Sun Jul 14 02:52:49 2024 ] 	Batch(6000/6809) done. Loss: 0.0410  lr:0.000001
[ Sun Jul 14 02:53:07 2024 ] 	Batch(6100/6809) done. Loss: 0.0949  lr:0.000001
[ Sun Jul 14 02:53:25 2024 ] 	Batch(6200/6809) done. Loss: 0.0136  lr:0.000001
[ Sun Jul 14 02:53:43 2024 ] 	Batch(6300/6809) done. Loss: 0.0169  lr:0.000001
[ Sun Jul 14 02:54:01 2024 ] 	Batch(6400/6809) done. Loss: 0.1067  lr:0.000001
[ Sun Jul 14 02:54:19 2024 ] 
Training: Epoch [27/50], Step [6499], Loss: 0.02218257263302803, Training Accuracy: 97.60192307692309
[ Sun Jul 14 02:54:19 2024 ] 	Batch(6500/6809) done. Loss: 0.0802  lr:0.000001
[ Sun Jul 14 02:54:37 2024 ] 	Batch(6600/6809) done. Loss: 0.0142  lr:0.000001
[ Sun Jul 14 02:54:55 2024 ] 	Batch(6700/6809) done. Loss: 0.0911  lr:0.000001
[ Sun Jul 14 02:55:13 2024 ] 	Batch(6800/6809) done. Loss: 0.4675  lr:0.000001
[ Sun Jul 14 02:55:15 2024 ] 	Mean training loss: 0.0933.
[ Sun Jul 14 02:55:15 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 02:55:15 2024 ] Training epoch: 29
[ Sun Jul 14 02:55:15 2024 ] 	Batch(0/6809) done. Loss: 0.0114  lr:0.000001
[ Sun Jul 14 02:55:33 2024 ] 	Batch(100/6809) done. Loss: 0.0992  lr:0.000001
[ Sun Jul 14 02:55:51 2024 ] 	Batch(200/6809) done. Loss: 0.1436  lr:0.000001
[ Sun Jul 14 02:56:09 2024 ] 	Batch(300/6809) done. Loss: 0.0597  lr:0.000001
[ Sun Jul 14 02:56:28 2024 ] 	Batch(400/6809) done. Loss: 0.0853  lr:0.000001
[ Sun Jul 14 02:56:46 2024 ] 
Training: Epoch [28/50], Step [499], Loss: 0.2512492537498474, Training Accuracy: 97.15
[ Sun Jul 14 02:56:46 2024 ] 	Batch(500/6809) done. Loss: 0.2106  lr:0.000001
[ Sun Jul 14 02:57:04 2024 ] 	Batch(600/6809) done. Loss: 0.0209  lr:0.000001
[ Sun Jul 14 02:57:22 2024 ] 	Batch(700/6809) done. Loss: 0.0098  lr:0.000001
[ Sun Jul 14 02:57:40 2024 ] 	Batch(800/6809) done. Loss: 0.2290  lr:0.000001
[ Sun Jul 14 02:57:58 2024 ] 	Batch(900/6809) done. Loss: 0.0134  lr:0.000001
[ Sun Jul 14 02:58:16 2024 ] 
Training: Epoch [28/50], Step [999], Loss: 0.02845471166074276, Training Accuracy: 97.41250000000001
[ Sun Jul 14 02:58:16 2024 ] 	Batch(1000/6809) done. Loss: 0.0145  lr:0.000001
[ Sun Jul 14 02:58:34 2024 ] 	Batch(1100/6809) done. Loss: 0.1104  lr:0.000001
[ Sun Jul 14 02:58:52 2024 ] 	Batch(1200/6809) done. Loss: 0.2760  lr:0.000001
[ Sun Jul 14 02:59:10 2024 ] 	Batch(1300/6809) done. Loss: 0.2204  lr:0.000001
[ Sun Jul 14 02:59:28 2024 ] 	Batch(1400/6809) done. Loss: 0.2601  lr:0.000001
[ Sun Jul 14 02:59:46 2024 ] 
Training: Epoch [28/50], Step [1499], Loss: 0.07878830283880234, Training Accuracy: 97.56666666666666
[ Sun Jul 14 02:59:46 2024 ] 	Batch(1500/6809) done. Loss: 0.2168  lr:0.000001
[ Sun Jul 14 03:00:04 2024 ] 	Batch(1600/6809) done. Loss: 0.0197  lr:0.000001
[ Sun Jul 14 03:00:22 2024 ] 	Batch(1700/6809) done. Loss: 0.0448  lr:0.000001
[ Sun Jul 14 03:00:40 2024 ] 	Batch(1800/6809) done. Loss: 0.0029  lr:0.000001
[ Sun Jul 14 03:00:58 2024 ] 	Batch(1900/6809) done. Loss: 0.0366  lr:0.000001
[ Sun Jul 14 03:01:16 2024 ] 
Training: Epoch [28/50], Step [1999], Loss: 0.01859116554260254, Training Accuracy: 97.6125
[ Sun Jul 14 03:01:16 2024 ] 	Batch(2000/6809) done. Loss: 0.0059  lr:0.000001
[ Sun Jul 14 03:01:34 2024 ] 	Batch(2100/6809) done. Loss: 0.1127  lr:0.000001
[ Sun Jul 14 03:01:52 2024 ] 	Batch(2200/6809) done. Loss: 0.0225  lr:0.000001
[ Sun Jul 14 03:02:10 2024 ] 	Batch(2300/6809) done. Loss: 0.2063  lr:0.000001
[ Sun Jul 14 03:02:28 2024 ] 	Batch(2400/6809) done. Loss: 0.0399  lr:0.000001
[ Sun Jul 14 03:02:46 2024 ] 
Training: Epoch [28/50], Step [2499], Loss: 0.20770137012004852, Training Accuracy: 97.685
[ Sun Jul 14 03:02:46 2024 ] 	Batch(2500/6809) done. Loss: 0.0465  lr:0.000001
[ Sun Jul 14 03:03:04 2024 ] 	Batch(2600/6809) done. Loss: 0.0490  lr:0.000001
[ Sun Jul 14 03:03:22 2024 ] 	Batch(2700/6809) done. Loss: 0.0219  lr:0.000001
[ Sun Jul 14 03:03:41 2024 ] 	Batch(2800/6809) done. Loss: 0.1352  lr:0.000001
[ Sun Jul 14 03:04:00 2024 ] 	Batch(2900/6809) done. Loss: 0.1928  lr:0.000001
[ Sun Jul 14 03:04:18 2024 ] 
Training: Epoch [28/50], Step [2999], Loss: 0.11904878914356232, Training Accuracy: 97.64166666666667
[ Sun Jul 14 03:04:18 2024 ] 	Batch(3000/6809) done. Loss: 0.0060  lr:0.000001
[ Sun Jul 14 03:04:37 2024 ] 	Batch(3100/6809) done. Loss: 0.2599  lr:0.000001
[ Sun Jul 14 03:04:55 2024 ] 	Batch(3200/6809) done. Loss: 0.0498  lr:0.000001
[ Sun Jul 14 03:05:14 2024 ] 	Batch(3300/6809) done. Loss: 0.0173  lr:0.000001
[ Sun Jul 14 03:05:32 2024 ] 	Batch(3400/6809) done. Loss: 0.0299  lr:0.000001
[ Sun Jul 14 03:05:50 2024 ] 
Training: Epoch [28/50], Step [3499], Loss: 0.14502157270908356, Training Accuracy: 97.66071428571429
[ Sun Jul 14 03:05:50 2024 ] 	Batch(3500/6809) done. Loss: 0.0329  lr:0.000001
[ Sun Jul 14 03:06:08 2024 ] 	Batch(3600/6809) done. Loss: 0.0726  lr:0.000001
[ Sun Jul 14 03:06:26 2024 ] 	Batch(3700/6809) done. Loss: 0.0167  lr:0.000001
[ Sun Jul 14 03:06:45 2024 ] 	Batch(3800/6809) done. Loss: 0.0149  lr:0.000001
[ Sun Jul 14 03:07:04 2024 ] 	Batch(3900/6809) done. Loss: 0.0885  lr:0.000001
[ Sun Jul 14 03:07:22 2024 ] 
Training: Epoch [28/50], Step [3999], Loss: 0.025016164407134056, Training Accuracy: 97.665625
[ Sun Jul 14 03:07:22 2024 ] 	Batch(4000/6809) done. Loss: 0.0140  lr:0.000001
[ Sun Jul 14 03:07:41 2024 ] 	Batch(4100/6809) done. Loss: 0.0176  lr:0.000001
[ Sun Jul 14 03:07:59 2024 ] 	Batch(4200/6809) done. Loss: 0.2977  lr:0.000001
[ Sun Jul 14 03:08:18 2024 ] 	Batch(4300/6809) done. Loss: 0.1415  lr:0.000001
[ Sun Jul 14 03:08:37 2024 ] 	Batch(4400/6809) done. Loss: 0.3541  lr:0.000001
[ Sun Jul 14 03:08:55 2024 ] 
Training: Epoch [28/50], Step [4499], Loss: 0.01981693133711815, Training Accuracy: 97.67777777777778
[ Sun Jul 14 03:08:55 2024 ] 	Batch(4500/6809) done. Loss: 0.0396  lr:0.000001
[ Sun Jul 14 03:09:14 2024 ] 	Batch(4600/6809) done. Loss: 0.0930  lr:0.000001
[ Sun Jul 14 03:09:32 2024 ] 	Batch(4700/6809) done. Loss: 0.0405  lr:0.000001
[ Sun Jul 14 03:09:51 2024 ] 	Batch(4800/6809) done. Loss: 0.0812  lr:0.000001
[ Sun Jul 14 03:10:09 2024 ] 	Batch(4900/6809) done. Loss: 0.0600  lr:0.000001
[ Sun Jul 14 03:10:27 2024 ] 
Training: Epoch [28/50], Step [4999], Loss: 0.07212241739034653, Training Accuracy: 97.6475
[ Sun Jul 14 03:10:27 2024 ] 	Batch(5000/6809) done. Loss: 0.0543  lr:0.000001
[ Sun Jul 14 03:10:45 2024 ] 	Batch(5100/6809) done. Loss: 0.0035  lr:0.000001
[ Sun Jul 14 03:11:03 2024 ] 	Batch(5200/6809) done. Loss: 0.1740  lr:0.000001
[ Sun Jul 14 03:11:21 2024 ] 	Batch(5300/6809) done. Loss: 0.1617  lr:0.000001
[ Sun Jul 14 03:11:39 2024 ] 	Batch(5400/6809) done. Loss: 0.0019  lr:0.000001
[ Sun Jul 14 03:11:56 2024 ] 
Training: Epoch [28/50], Step [5499], Loss: 0.044413719326257706, Training Accuracy: 97.66136363636365
[ Sun Jul 14 03:11:56 2024 ] 	Batch(5500/6809) done. Loss: 0.0717  lr:0.000001
[ Sun Jul 14 03:12:14 2024 ] 	Batch(5600/6809) done. Loss: 0.1016  lr:0.000001
[ Sun Jul 14 03:12:32 2024 ] 	Batch(5700/6809) done. Loss: 0.0303  lr:0.000001
[ Sun Jul 14 03:12:50 2024 ] 	Batch(5800/6809) done. Loss: 0.4868  lr:0.000001
[ Sun Jul 14 03:13:08 2024 ] 	Batch(5900/6809) done. Loss: 0.1021  lr:0.000001
[ Sun Jul 14 03:13:26 2024 ] 
Training: Epoch [28/50], Step [5999], Loss: 0.07029478996992111, Training Accuracy: 97.64791666666667
[ Sun Jul 14 03:13:26 2024 ] 	Batch(6000/6809) done. Loss: 0.0640  lr:0.000001
[ Sun Jul 14 03:13:44 2024 ] 	Batch(6100/6809) done. Loss: 0.0674  lr:0.000001
[ Sun Jul 14 03:14:02 2024 ] 	Batch(6200/6809) done. Loss: 0.0569  lr:0.000001
[ Sun Jul 14 03:14:20 2024 ] 	Batch(6300/6809) done. Loss: 0.0295  lr:0.000001
[ Sun Jul 14 03:14:38 2024 ] 	Batch(6400/6809) done. Loss: 0.1913  lr:0.000001
[ Sun Jul 14 03:14:56 2024 ] 
Training: Epoch [28/50], Step [6499], Loss: 0.027208950370550156, Training Accuracy: 97.64807692307691
[ Sun Jul 14 03:14:56 2024 ] 	Batch(6500/6809) done. Loss: 0.0646  lr:0.000001
[ Sun Jul 14 03:15:15 2024 ] 	Batch(6600/6809) done. Loss: 0.0253  lr:0.000001
[ Sun Jul 14 03:15:34 2024 ] 	Batch(6700/6809) done. Loss: 0.0530  lr:0.000001
[ Sun Jul 14 03:15:52 2024 ] 	Batch(6800/6809) done. Loss: 0.0450  lr:0.000001
[ Sun Jul 14 03:15:54 2024 ] 	Mean training loss: 0.0914.
[ Sun Jul 14 03:15:54 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 03:15:54 2024 ] Training epoch: 30
[ Sun Jul 14 03:15:54 2024 ] 	Batch(0/6809) done. Loss: 0.0718  lr:0.000001
[ Sun Jul 14 03:16:12 2024 ] 	Batch(100/6809) done. Loss: 0.0240  lr:0.000001
[ Sun Jul 14 03:16:31 2024 ] 	Batch(200/6809) done. Loss: 0.1182  lr:0.000001
[ Sun Jul 14 03:16:49 2024 ] 	Batch(300/6809) done. Loss: 0.0324  lr:0.000001
[ Sun Jul 14 03:17:07 2024 ] 	Batch(400/6809) done. Loss: 0.2533  lr:0.000001
[ Sun Jul 14 03:17:25 2024 ] 
Training: Epoch [29/50], Step [499], Loss: 0.32439249753952026, Training Accuracy: 97.875
[ Sun Jul 14 03:17:26 2024 ] 	Batch(500/6809) done. Loss: 0.0040  lr:0.000001
[ Sun Jul 14 03:17:44 2024 ] 	Batch(600/6809) done. Loss: 0.2489  lr:0.000001
[ Sun Jul 14 03:18:02 2024 ] 	Batch(700/6809) done. Loss: 0.0769  lr:0.000001
[ Sun Jul 14 03:18:20 2024 ] 	Batch(800/6809) done. Loss: 0.3152  lr:0.000001
[ Sun Jul 14 03:18:39 2024 ] 	Batch(900/6809) done. Loss: 0.0094  lr:0.000001
[ Sun Jul 14 03:18:57 2024 ] 
Training: Epoch [29/50], Step [999], Loss: 0.0684775859117508, Training Accuracy: 97.6125
[ Sun Jul 14 03:18:57 2024 ] 	Batch(1000/6809) done. Loss: 0.1298  lr:0.000001
[ Sun Jul 14 03:19:15 2024 ] 	Batch(1100/6809) done. Loss: 0.2129  lr:0.000001
[ Sun Jul 14 03:19:34 2024 ] 	Batch(1200/6809) done. Loss: 0.0716  lr:0.000001
[ Sun Jul 14 03:19:52 2024 ] 	Batch(1300/6809) done. Loss: 0.0015  lr:0.000001
[ Sun Jul 14 03:20:10 2024 ] 	Batch(1400/6809) done. Loss: 0.1255  lr:0.000001
[ Sun Jul 14 03:20:29 2024 ] 
Training: Epoch [29/50], Step [1499], Loss: 0.12604276835918427, Training Accuracy: 97.625
[ Sun Jul 14 03:20:29 2024 ] 	Batch(1500/6809) done. Loss: 0.2242  lr:0.000001
[ Sun Jul 14 03:20:47 2024 ] 	Batch(1600/6809) done. Loss: 0.0973  lr:0.000001
[ Sun Jul 14 03:21:06 2024 ] 	Batch(1700/6809) done. Loss: 0.0278  lr:0.000001
[ Sun Jul 14 03:21:24 2024 ] 	Batch(1800/6809) done. Loss: 0.0141  lr:0.000001
[ Sun Jul 14 03:21:42 2024 ] 	Batch(1900/6809) done. Loss: 0.1134  lr:0.000001
[ Sun Jul 14 03:22:00 2024 ] 
Training: Epoch [29/50], Step [1999], Loss: 0.4914708733558655, Training Accuracy: 97.64375
[ Sun Jul 14 03:22:00 2024 ] 	Batch(2000/6809) done. Loss: 0.0357  lr:0.000001
[ Sun Jul 14 03:22:18 2024 ] 	Batch(2100/6809) done. Loss: 0.1809  lr:0.000001
[ Sun Jul 14 03:22:36 2024 ] 	Batch(2200/6809) done. Loss: 0.0762  lr:0.000001
[ Sun Jul 14 03:22:54 2024 ] 	Batch(2300/6809) done. Loss: 0.1363  lr:0.000001
[ Sun Jul 14 03:23:12 2024 ] 	Batch(2400/6809) done. Loss: 0.0388  lr:0.000001
[ Sun Jul 14 03:23:30 2024 ] 
Training: Epoch [29/50], Step [2499], Loss: 0.11245335638523102, Training Accuracy: 97.665
[ Sun Jul 14 03:23:30 2024 ] 	Batch(2500/6809) done. Loss: 0.2023  lr:0.000001
[ Sun Jul 14 03:23:48 2024 ] 	Batch(2600/6809) done. Loss: 0.0634  lr:0.000001
[ Sun Jul 14 03:24:06 2024 ] 	Batch(2700/6809) done. Loss: 0.0438  lr:0.000001
[ Sun Jul 14 03:24:24 2024 ] 	Batch(2800/6809) done. Loss: 0.0048  lr:0.000001
[ Sun Jul 14 03:24:42 2024 ] 	Batch(2900/6809) done. Loss: 0.0103  lr:0.000001
[ Sun Jul 14 03:25:00 2024 ] 
Training: Epoch [29/50], Step [2999], Loss: 0.012880364432930946, Training Accuracy: 97.64166666666667
[ Sun Jul 14 03:25:00 2024 ] 	Batch(3000/6809) done. Loss: 0.0534  lr:0.000001
[ Sun Jul 14 03:25:18 2024 ] 	Batch(3100/6809) done. Loss: 0.1260  lr:0.000001
[ Sun Jul 14 03:25:36 2024 ] 	Batch(3200/6809) done. Loss: 0.2960  lr:0.000001
[ Sun Jul 14 03:25:55 2024 ] 	Batch(3300/6809) done. Loss: 0.1248  lr:0.000001
[ Sun Jul 14 03:26:13 2024 ] 	Batch(3400/6809) done. Loss: 0.0062  lr:0.000001
[ Sun Jul 14 03:26:31 2024 ] 
Training: Epoch [29/50], Step [3499], Loss: 0.027992071583867073, Training Accuracy: 97.64642857142857
[ Sun Jul 14 03:26:31 2024 ] 	Batch(3500/6809) done. Loss: 0.0954  lr:0.000001
[ Sun Jul 14 03:26:50 2024 ] 	Batch(3600/6809) done. Loss: 0.0067  lr:0.000001
[ Sun Jul 14 03:27:08 2024 ] 	Batch(3700/6809) done. Loss: 0.2451  lr:0.000001
[ Sun Jul 14 03:27:27 2024 ] 	Batch(3800/6809) done. Loss: 0.0104  lr:0.000001
[ Sun Jul 14 03:27:45 2024 ] 	Batch(3900/6809) done. Loss: 0.1107  lr:0.000001
[ Sun Jul 14 03:28:02 2024 ] 
Training: Epoch [29/50], Step [3999], Loss: 0.026824841275811195, Training Accuracy: 97.6
[ Sun Jul 14 03:28:03 2024 ] 	Batch(4000/6809) done. Loss: 0.0301  lr:0.000001
[ Sun Jul 14 03:28:21 2024 ] 	Batch(4100/6809) done. Loss: 0.0279  lr:0.000001
[ Sun Jul 14 03:28:39 2024 ] 	Batch(4200/6809) done. Loss: 0.2413  lr:0.000001
[ Sun Jul 14 03:28:57 2024 ] 	Batch(4300/6809) done. Loss: 0.0374  lr:0.000001
[ Sun Jul 14 03:29:15 2024 ] 	Batch(4400/6809) done. Loss: 0.0788  lr:0.000001
[ Sun Jul 14 03:29:33 2024 ] 
Training: Epoch [29/50], Step [4499], Loss: 0.031928159296512604, Training Accuracy: 97.57222222222222
[ Sun Jul 14 03:29:33 2024 ] 	Batch(4500/6809) done. Loss: 0.0110  lr:0.000001
[ Sun Jul 14 03:29:52 2024 ] 	Batch(4600/6809) done. Loss: 0.0063  lr:0.000001
[ Sun Jul 14 03:30:10 2024 ] 	Batch(4700/6809) done. Loss: 0.0119  lr:0.000001
[ Sun Jul 14 03:30:28 2024 ] 	Batch(4800/6809) done. Loss: 0.0133  lr:0.000001
[ Sun Jul 14 03:30:46 2024 ] 	Batch(4900/6809) done. Loss: 0.1202  lr:0.000001
[ Sun Jul 14 03:31:04 2024 ] 
Training: Epoch [29/50], Step [4999], Loss: 0.013375429436564445, Training Accuracy: 97.59
[ Sun Jul 14 03:31:05 2024 ] 	Batch(5000/6809) done. Loss: 0.0110  lr:0.000001
[ Sun Jul 14 03:31:23 2024 ] 	Batch(5100/6809) done. Loss: 0.0085  lr:0.000001
[ Sun Jul 14 03:31:41 2024 ] 	Batch(5200/6809) done. Loss: 0.0262  lr:0.000001
[ Sun Jul 14 03:31:59 2024 ] 	Batch(5300/6809) done. Loss: 0.0679  lr:0.000001
[ Sun Jul 14 03:32:17 2024 ] 	Batch(5400/6809) done. Loss: 0.0646  lr:0.000001
[ Sun Jul 14 03:32:35 2024 ] 
Training: Epoch [29/50], Step [5499], Loss: 0.1293964982032776, Training Accuracy: 97.6
[ Sun Jul 14 03:32:35 2024 ] 	Batch(5500/6809) done. Loss: 0.0444  lr:0.000001
[ Sun Jul 14 03:32:53 2024 ] 	Batch(5600/6809) done. Loss: 1.1037  lr:0.000001
[ Sun Jul 14 03:33:11 2024 ] 	Batch(5700/6809) done. Loss: 0.1241  lr:0.000001
[ Sun Jul 14 03:33:29 2024 ] 	Batch(5800/6809) done. Loss: 0.0816  lr:0.000001
[ Sun Jul 14 03:33:47 2024 ] 	Batch(5900/6809) done. Loss: 0.1061  lr:0.000001
[ Sun Jul 14 03:34:05 2024 ] 
Training: Epoch [29/50], Step [5999], Loss: 0.003512017661705613, Training Accuracy: 97.57916666666667
[ Sun Jul 14 03:34:05 2024 ] 	Batch(6000/6809) done. Loss: 0.1689  lr:0.000001
[ Sun Jul 14 03:34:23 2024 ] 	Batch(6100/6809) done. Loss: 0.0891  lr:0.000001
[ Sun Jul 14 03:34:41 2024 ] 	Batch(6200/6809) done. Loss: 0.1525  lr:0.000001
[ Sun Jul 14 03:34:59 2024 ] 	Batch(6300/6809) done. Loss: 0.0598  lr:0.000001
[ Sun Jul 14 03:35:18 2024 ] 	Batch(6400/6809) done. Loss: 0.1106  lr:0.000001
[ Sun Jul 14 03:35:36 2024 ] 
Training: Epoch [29/50], Step [6499], Loss: 0.07545504719018936, Training Accuracy: 97.61730769230769
[ Sun Jul 14 03:35:36 2024 ] 	Batch(6500/6809) done. Loss: 0.0208  lr:0.000001
[ Sun Jul 14 03:35:54 2024 ] 	Batch(6600/6809) done. Loss: 0.0159  lr:0.000001
[ Sun Jul 14 03:36:12 2024 ] 	Batch(6700/6809) done. Loss: 0.0357  lr:0.000001
[ Sun Jul 14 03:36:30 2024 ] 	Batch(6800/6809) done. Loss: 0.0071  lr:0.000001
[ Sun Jul 14 03:36:31 2024 ] 	Mean training loss: 0.0936.
[ Sun Jul 14 03:36:31 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 03:36:32 2024 ] Eval epoch: 30
[ Sun Jul 14 03:42:05 2024 ] 	Mean val loss of 7435 batches: 1.067302980851466.
[ Sun Jul 14 03:42:05 2024 ] 
Validation: Epoch [29/50], Samples [47750.0/59477], Loss: 0.16119733452796936, Validation Accuracy: 80.28313465709434
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 1 : 388 / 500 = 77 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 2 : 416 / 499 = 83 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 3 : 421 / 500 = 84 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 4 : 415 / 502 = 82 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 5 : 469 / 502 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 6 : 419 / 502 = 83 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 7 : 467 / 497 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 8 : 484 / 498 = 97 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 9 : 391 / 500 = 78 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 10 : 216 / 500 = 43 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 11 : 181 / 498 = 36 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 12 : 411 / 499 = 82 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 13 : 485 / 502 = 96 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 14 : 480 / 504 = 95 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 15 : 446 / 502 = 88 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 16 : 363 / 502 = 72 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 17 : 447 / 504 = 88 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 18 : 423 / 504 = 83 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 19 : 464 / 502 = 92 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 20 : 453 / 502 = 90 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 21 : 471 / 503 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 22 : 424 / 504 = 84 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 23 : 450 / 503 = 89 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 24 : 418 / 504 = 82 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 25 : 492 / 504 = 97 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 26 : 470 / 504 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 27 : 423 / 501 = 84 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 28 : 368 / 502 = 73 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 29 : 342 / 502 = 68 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 30 : 321 / 501 = 64 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 31 : 424 / 504 = 84 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 32 : 419 / 503 = 83 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 33 : 414 / 503 = 82 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 34 : 486 / 504 = 96 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 35 : 471 / 503 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 36 : 397 / 502 = 79 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 37 : 435 / 504 = 86 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 38 : 429 / 504 = 85 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 39 : 453 / 498 = 90 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 40 : 385 / 504 = 76 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 41 : 474 / 503 = 94 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 42 : 452 / 504 = 89 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 43 : 332 / 503 = 66 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 44 : 444 / 504 = 88 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 45 : 415 / 504 = 82 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 46 : 401 / 504 = 79 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 47 : 423 / 503 = 84 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 48 : 436 / 503 = 86 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 49 : 379 / 499 = 75 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 50 : 427 / 502 = 85 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 51 : 463 / 503 = 92 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 52 : 455 / 504 = 90 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 53 : 421 / 497 = 84 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 54 : 453 / 480 = 94 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 55 : 364 / 504 = 72 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 56 : 414 / 503 = 82 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 57 : 484 / 504 = 96 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 58 : 483 / 499 = 96 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 59 : 491 / 503 = 97 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 60 : 415 / 479 = 86 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 61 : 419 / 484 = 86 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 62 : 390 / 487 = 80 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 63 : 457 / 489 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 64 : 372 / 488 = 76 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 65 : 459 / 490 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 66 : 317 / 488 = 64 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 67 : 367 / 490 = 74 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 68 : 282 / 490 = 57 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 69 : 359 / 490 = 73 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 70 : 191 / 490 = 38 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 71 : 219 / 490 = 44 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 72 : 191 / 488 = 39 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 73 : 290 / 486 = 59 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 74 : 265 / 481 = 55 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 75 : 265 / 488 = 54 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 76 : 327 / 489 = 66 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 77 : 319 / 488 = 65 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 78 : 370 / 488 = 75 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 79 : 456 / 490 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 80 : 413 / 489 = 84 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 81 : 302 / 491 = 61 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 82 : 329 / 491 = 67 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 83 : 262 / 489 = 53 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 84 : 392 / 489 = 80 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 85 : 380 / 489 = 77 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 86 : 432 / 491 = 87 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 87 : 425 / 492 = 86 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 88 : 360 / 491 = 73 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 89 : 409 / 492 = 83 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 90 : 273 / 490 = 55 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 91 : 401 / 482 = 83 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 92 : 369 / 490 = 75 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 93 : 366 / 487 = 75 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 94 : 414 / 489 = 84 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 95 : 408 / 490 = 83 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 96 : 466 / 491 = 94 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 97 : 463 / 490 = 94 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 98 : 445 / 491 = 90 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 99 : 448 / 491 = 91 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 100 : 449 / 491 = 91 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 101 : 435 / 491 = 88 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 102 : 273 / 492 = 55 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 103 : 393 / 492 = 79 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 104 : 289 / 491 = 58 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 105 : 273 / 491 = 55 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 106 : 277 / 492 = 56 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 107 : 408 / 491 = 83 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 108 : 395 / 492 = 80 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 109 : 341 / 490 = 69 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 110 : 423 / 491 = 86 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 111 : 458 / 492 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 112 : 460 / 492 = 93 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 113 : 445 / 491 = 90 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 114 : 403 / 491 = 82 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 115 : 437 / 492 = 88 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 116 : 400 / 491 = 81 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 117 : 447 / 492 = 90 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 118 : 438 / 490 = 89 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 119 : 449 / 492 = 91 %
[ Sun Jul 14 03:42:05 2024 ] Accuracy of 120 : 428 / 500 = 85 %
[ Sun Jul 14 03:42:05 2024 ] Training epoch: 31
[ Sun Jul 14 03:42:05 2024 ] 	Batch(0/6809) done. Loss: 0.1502  lr:0.000001
[ Sun Jul 14 03:42:23 2024 ] 	Batch(100/6809) done. Loss: 0.0467  lr:0.000001
[ Sun Jul 14 03:42:41 2024 ] 	Batch(200/6809) done. Loss: 0.0156  lr:0.000001
[ Sun Jul 14 03:43:00 2024 ] 	Batch(300/6809) done. Loss: 0.0673  lr:0.000001
[ Sun Jul 14 03:43:18 2024 ] 	Batch(400/6809) done. Loss: 0.0562  lr:0.000001
[ Sun Jul 14 03:43:36 2024 ] 
Training: Epoch [30/50], Step [499], Loss: 0.054013580083847046, Training Accuracy: 97.6
[ Sun Jul 14 03:43:37 2024 ] 	Batch(500/6809) done. Loss: 0.0484  lr:0.000001
[ Sun Jul 14 03:43:55 2024 ] 	Batch(600/6809) done. Loss: 0.0077  lr:0.000001
[ Sun Jul 14 03:44:13 2024 ] 	Batch(700/6809) done. Loss: 0.6274  lr:0.000001
[ Sun Jul 14 03:44:31 2024 ] 	Batch(800/6809) done. Loss: 0.0741  lr:0.000001
[ Sun Jul 14 03:44:49 2024 ] 	Batch(900/6809) done. Loss: 0.0985  lr:0.000001
[ Sun Jul 14 03:45:07 2024 ] 
Training: Epoch [30/50], Step [999], Loss: 0.06848926842212677, Training Accuracy: 97.725
[ Sun Jul 14 03:45:07 2024 ] 	Batch(1000/6809) done. Loss: 0.1734  lr:0.000001
[ Sun Jul 14 03:45:25 2024 ] 	Batch(1100/6809) done. Loss: 0.0246  lr:0.000001
[ Sun Jul 14 03:45:43 2024 ] 	Batch(1200/6809) done. Loss: 0.1262  lr:0.000001
[ Sun Jul 14 03:46:01 2024 ] 	Batch(1300/6809) done. Loss: 0.0111  lr:0.000001
[ Sun Jul 14 03:46:19 2024 ] 	Batch(1400/6809) done. Loss: 0.0209  lr:0.000001
[ Sun Jul 14 03:46:37 2024 ] 
Training: Epoch [30/50], Step [1499], Loss: 0.0012128630187362432, Training Accuracy: 97.85833333333333
[ Sun Jul 14 03:46:37 2024 ] 	Batch(1500/6809) done. Loss: 0.0099  lr:0.000001
[ Sun Jul 14 03:46:55 2024 ] 	Batch(1600/6809) done. Loss: 0.0180  lr:0.000001
[ Sun Jul 14 03:47:13 2024 ] 	Batch(1700/6809) done. Loss: 0.0755  lr:0.000001
[ Sun Jul 14 03:47:32 2024 ] 	Batch(1800/6809) done. Loss: 0.0135  lr:0.000001
[ Sun Jul 14 03:47:50 2024 ] 	Batch(1900/6809) done. Loss: 0.0357  lr:0.000001
[ Sun Jul 14 03:48:08 2024 ] 
Training: Epoch [30/50], Step [1999], Loss: 0.023345423862338066, Training Accuracy: 97.91875
[ Sun Jul 14 03:48:08 2024 ] 	Batch(2000/6809) done. Loss: 0.0069  lr:0.000001
[ Sun Jul 14 03:48:27 2024 ] 	Batch(2100/6809) done. Loss: 0.0118  lr:0.000001
[ Sun Jul 14 03:48:45 2024 ] 	Batch(2200/6809) done. Loss: 0.2570  lr:0.000001
[ Sun Jul 14 03:49:03 2024 ] 	Batch(2300/6809) done. Loss: 0.0127  lr:0.000001
[ Sun Jul 14 03:49:21 2024 ] 	Batch(2400/6809) done. Loss: 0.1781  lr:0.000001
[ Sun Jul 14 03:49:39 2024 ] 
Training: Epoch [30/50], Step [2499], Loss: 0.0298007782548666, Training Accuracy: 97.91499999999999
[ Sun Jul 14 03:49:39 2024 ] 	Batch(2500/6809) done. Loss: 0.0245  lr:0.000001
[ Sun Jul 14 03:49:57 2024 ] 	Batch(2600/6809) done. Loss: 0.0154  lr:0.000001
[ Sun Jul 14 03:50:16 2024 ] 	Batch(2700/6809) done. Loss: 0.2144  lr:0.000001
[ Sun Jul 14 03:50:34 2024 ] 	Batch(2800/6809) done. Loss: 0.0067  lr:0.000001
[ Sun Jul 14 03:50:52 2024 ] 	Batch(2900/6809) done. Loss: 0.0269  lr:0.000001
[ Sun Jul 14 03:51:10 2024 ] 
Training: Epoch [30/50], Step [2999], Loss: 0.02523137256503105, Training Accuracy: 97.89583333333334
[ Sun Jul 14 03:51:10 2024 ] 	Batch(3000/6809) done. Loss: 0.0530  lr:0.000001
[ Sun Jul 14 03:51:28 2024 ] 	Batch(3100/6809) done. Loss: 0.0097  lr:0.000001
[ Sun Jul 14 03:51:46 2024 ] 	Batch(3200/6809) done. Loss: 0.0875  lr:0.000001
[ Sun Jul 14 03:52:04 2024 ] 	Batch(3300/6809) done. Loss: 0.0423  lr:0.000001
[ Sun Jul 14 03:52:22 2024 ] 	Batch(3400/6809) done. Loss: 0.0786  lr:0.000001
[ Sun Jul 14 03:52:40 2024 ] 
Training: Epoch [30/50], Step [3499], Loss: 0.18317663669586182, Training Accuracy: 97.84642857142856
[ Sun Jul 14 03:52:40 2024 ] 	Batch(3500/6809) done. Loss: 0.0538  lr:0.000001
[ Sun Jul 14 03:52:58 2024 ] 	Batch(3600/6809) done. Loss: 0.0636  lr:0.000001
[ Sun Jul 14 03:53:16 2024 ] 	Batch(3700/6809) done. Loss: 0.0251  lr:0.000001
[ Sun Jul 14 03:53:34 2024 ] 	Batch(3800/6809) done. Loss: 0.0422  lr:0.000001
[ Sun Jul 14 03:53:52 2024 ] 	Batch(3900/6809) done. Loss: 0.0091  lr:0.000001
[ Sun Jul 14 03:54:10 2024 ] 
Training: Epoch [30/50], Step [3999], Loss: 0.3498697280883789, Training Accuracy: 97.81875
[ Sun Jul 14 03:54:10 2024 ] 	Batch(4000/6809) done. Loss: 0.1278  lr:0.000001
[ Sun Jul 14 03:54:28 2024 ] 	Batch(4100/6809) done. Loss: 0.1052  lr:0.000001
[ Sun Jul 14 03:54:47 2024 ] 	Batch(4200/6809) done. Loss: 0.0298  lr:0.000001
[ Sun Jul 14 03:55:05 2024 ] 	Batch(4300/6809) done. Loss: 0.0833  lr:0.000001
[ Sun Jul 14 03:55:23 2024 ] 	Batch(4400/6809) done. Loss: 0.0879  lr:0.000001
[ Sun Jul 14 03:55:41 2024 ] 
Training: Epoch [30/50], Step [4499], Loss: 0.31255027651786804, Training Accuracy: 97.7611111111111
[ Sun Jul 14 03:55:41 2024 ] 	Batch(4500/6809) done. Loss: 0.0611  lr:0.000001
[ Sun Jul 14 03:55:59 2024 ] 	Batch(4600/6809) done. Loss: 0.0247  lr:0.000001
[ Sun Jul 14 03:56:17 2024 ] 	Batch(4700/6809) done. Loss: 0.2634  lr:0.000001
[ Sun Jul 14 03:56:35 2024 ] 	Batch(4800/6809) done. Loss: 0.0229  lr:0.000001
[ Sun Jul 14 03:56:53 2024 ] 	Batch(4900/6809) done. Loss: 0.0654  lr:0.000001
[ Sun Jul 14 03:57:11 2024 ] 
Training: Epoch [30/50], Step [4999], Loss: 0.12441245466470718, Training Accuracy: 97.75
[ Sun Jul 14 03:57:11 2024 ] 	Batch(5000/6809) done. Loss: 0.0363  lr:0.000001
[ Sun Jul 14 03:57:30 2024 ] 	Batch(5100/6809) done. Loss: 0.0228  lr:0.000001
[ Sun Jul 14 03:57:48 2024 ] 	Batch(5200/6809) done. Loss: 0.0531  lr:0.000001
[ Sun Jul 14 03:58:06 2024 ] 	Batch(5300/6809) done. Loss: 0.0164  lr:0.000001
[ Sun Jul 14 03:58:25 2024 ] 	Batch(5400/6809) done. Loss: 0.0611  lr:0.000001
[ Sun Jul 14 03:58:43 2024 ] 
Training: Epoch [30/50], Step [5499], Loss: 0.009297131560742855, Training Accuracy: 97.71363636363637
[ Sun Jul 14 03:58:43 2024 ] 	Batch(5500/6809) done. Loss: 0.0118  lr:0.000001
[ Sun Jul 14 03:59:02 2024 ] 	Batch(5600/6809) done. Loss: 0.0064  lr:0.000001
[ Sun Jul 14 03:59:21 2024 ] 	Batch(5700/6809) done. Loss: 0.0485  lr:0.000001
[ Sun Jul 14 03:59:39 2024 ] 	Batch(5800/6809) done. Loss: 0.1527  lr:0.000001
[ Sun Jul 14 03:59:58 2024 ] 	Batch(5900/6809) done. Loss: 0.0050  lr:0.000001
[ Sun Jul 14 04:00:16 2024 ] 
Training: Epoch [30/50], Step [5999], Loss: 0.02671453356742859, Training Accuracy: 97.725
[ Sun Jul 14 04:00:16 2024 ] 	Batch(6000/6809) done. Loss: 0.1789  lr:0.000001
[ Sun Jul 14 04:00:34 2024 ] 	Batch(6100/6809) done. Loss: 0.0029  lr:0.000001
[ Sun Jul 14 04:00:52 2024 ] 	Batch(6200/6809) done. Loss: 0.0138  lr:0.000001
[ Sun Jul 14 04:01:10 2024 ] 	Batch(6300/6809) done. Loss: 0.0133  lr:0.000001
[ Sun Jul 14 04:01:28 2024 ] 	Batch(6400/6809) done. Loss: 0.0243  lr:0.000001
[ Sun Jul 14 04:01:46 2024 ] 
Training: Epoch [30/50], Step [6499], Loss: 0.2342502325773239, Training Accuracy: 97.71346153846154
[ Sun Jul 14 04:01:46 2024 ] 	Batch(6500/6809) done. Loss: 0.2253  lr:0.000001
[ Sun Jul 14 04:02:04 2024 ] 	Batch(6600/6809) done. Loss: 0.0404  lr:0.000001
[ Sun Jul 14 04:02:22 2024 ] 	Batch(6700/6809) done. Loss: 0.0141  lr:0.000001
[ Sun Jul 14 04:02:40 2024 ] 	Batch(6800/6809) done. Loss: 0.0684  lr:0.000001
[ Sun Jul 14 04:02:41 2024 ] 	Mean training loss: 0.0921.
[ Sun Jul 14 04:02:41 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 04:02:41 2024 ] Training epoch: 32
[ Sun Jul 14 04:02:42 2024 ] 	Batch(0/6809) done. Loss: 0.2259  lr:0.000001
[ Sun Jul 14 04:03:00 2024 ] 	Batch(100/6809) done. Loss: 0.0051  lr:0.000001
[ Sun Jul 14 04:03:18 2024 ] 	Batch(200/6809) done. Loss: 0.0716  lr:0.000001
[ Sun Jul 14 04:03:36 2024 ] 	Batch(300/6809) done. Loss: 0.2682  lr:0.000001
[ Sun Jul 14 04:03:54 2024 ] 	Batch(400/6809) done. Loss: 0.1126  lr:0.000001
[ Sun Jul 14 04:04:11 2024 ] 
Training: Epoch [31/50], Step [499], Loss: 0.15654364228248596, Training Accuracy: 97.25
[ Sun Jul 14 04:04:12 2024 ] 	Batch(500/6809) done. Loss: 0.1183  lr:0.000001
[ Sun Jul 14 04:04:30 2024 ] 	Batch(600/6809) done. Loss: 0.0362  lr:0.000001
[ Sun Jul 14 04:04:47 2024 ] 	Batch(700/6809) done. Loss: 0.3444  lr:0.000001
[ Sun Jul 14 04:05:05 2024 ] 	Batch(800/6809) done. Loss: 0.0658  lr:0.000001
[ Sun Jul 14 04:05:23 2024 ] 	Batch(900/6809) done. Loss: 0.0269  lr:0.000001
[ Sun Jul 14 04:05:41 2024 ] 
Training: Epoch [31/50], Step [999], Loss: 0.021056415513157845, Training Accuracy: 97.75
[ Sun Jul 14 04:05:41 2024 ] 	Batch(1000/6809) done. Loss: 0.0227  lr:0.000001
[ Sun Jul 14 04:05:59 2024 ] 	Batch(1100/6809) done. Loss: 0.0052  lr:0.000001
[ Sun Jul 14 04:06:17 2024 ] 	Batch(1200/6809) done. Loss: 0.0850  lr:0.000001
[ Sun Jul 14 04:06:35 2024 ] 	Batch(1300/6809) done. Loss: 0.0940  lr:0.000001
[ Sun Jul 14 04:06:53 2024 ] 	Batch(1400/6809) done. Loss: 0.0931  lr:0.000001
[ Sun Jul 14 04:07:11 2024 ] 
Training: Epoch [31/50], Step [1499], Loss: 0.02011098898947239, Training Accuracy: 97.675
[ Sun Jul 14 04:07:11 2024 ] 	Batch(1500/6809) done. Loss: 0.1462  lr:0.000001
[ Sun Jul 14 04:07:29 2024 ] 	Batch(1600/6809) done. Loss: 0.0302  lr:0.000001
[ Sun Jul 14 04:07:47 2024 ] 	Batch(1700/6809) done. Loss: 0.0094  lr:0.000001
[ Sun Jul 14 04:08:05 2024 ] 	Batch(1800/6809) done. Loss: 0.0753  lr:0.000001
[ Sun Jul 14 04:08:22 2024 ] 	Batch(1900/6809) done. Loss: 0.0650  lr:0.000001
[ Sun Jul 14 04:08:40 2024 ] 
Training: Epoch [31/50], Step [1999], Loss: 0.00391815323382616, Training Accuracy: 97.5375
[ Sun Jul 14 04:08:41 2024 ] 	Batch(2000/6809) done. Loss: 0.0812  lr:0.000001
[ Sun Jul 14 04:08:58 2024 ] 	Batch(2100/6809) done. Loss: 0.0544  lr:0.000001
[ Sun Jul 14 04:09:17 2024 ] 	Batch(2200/6809) done. Loss: 0.0121  lr:0.000001
[ Sun Jul 14 04:09:35 2024 ] 	Batch(2300/6809) done. Loss: 0.0210  lr:0.000001
[ Sun Jul 14 04:09:53 2024 ] 	Batch(2400/6809) done. Loss: 0.0919  lr:0.000001
[ Sun Jul 14 04:10:12 2024 ] 
Training: Epoch [31/50], Step [2499], Loss: 0.03430366516113281, Training Accuracy: 97.59
[ Sun Jul 14 04:10:12 2024 ] 	Batch(2500/6809) done. Loss: 0.6531  lr:0.000001
[ Sun Jul 14 04:10:31 2024 ] 	Batch(2600/6809) done. Loss: 0.1473  lr:0.000001
[ Sun Jul 14 04:10:50 2024 ] 	Batch(2700/6809) done. Loss: 0.2179  lr:0.000001
[ Sun Jul 14 04:11:09 2024 ] 	Batch(2800/6809) done. Loss: 0.0304  lr:0.000001
[ Sun Jul 14 04:11:27 2024 ] 	Batch(2900/6809) done. Loss: 0.1174  lr:0.000001
[ Sun Jul 14 04:11:45 2024 ] 
Training: Epoch [31/50], Step [2999], Loss: 0.024994689971208572, Training Accuracy: 97.60416666666667
[ Sun Jul 14 04:11:46 2024 ] 	Batch(3000/6809) done. Loss: 0.0812  lr:0.000001
[ Sun Jul 14 04:12:04 2024 ] 	Batch(3100/6809) done. Loss: 0.0180  lr:0.000001
[ Sun Jul 14 04:12:22 2024 ] 	Batch(3200/6809) done. Loss: 0.0399  lr:0.000001
[ Sun Jul 14 04:12:40 2024 ] 	Batch(3300/6809) done. Loss: 0.0603  lr:0.000001
[ Sun Jul 14 04:12:58 2024 ] 	Batch(3400/6809) done. Loss: 0.0487  lr:0.000001
[ Sun Jul 14 04:13:16 2024 ] 
Training: Epoch [31/50], Step [3499], Loss: 0.2524627149105072, Training Accuracy: 97.63928571428572
[ Sun Jul 14 04:13:16 2024 ] 	Batch(3500/6809) done. Loss: 0.0706  lr:0.000001
[ Sun Jul 14 04:13:34 2024 ] 	Batch(3600/6809) done. Loss: 0.0096  lr:0.000001
[ Sun Jul 14 04:13:52 2024 ] 	Batch(3700/6809) done. Loss: 0.0324  lr:0.000001
[ Sun Jul 14 04:14:10 2024 ] 	Batch(3800/6809) done. Loss: 0.0502  lr:0.000001
[ Sun Jul 14 04:14:28 2024 ] 	Batch(3900/6809) done. Loss: 0.0392  lr:0.000001
[ Sun Jul 14 04:14:46 2024 ] 
Training: Epoch [31/50], Step [3999], Loss: 0.0912756696343422, Training Accuracy: 97.6625
[ Sun Jul 14 04:14:47 2024 ] 	Batch(4000/6809) done. Loss: 0.1954  lr:0.000001
[ Sun Jul 14 04:15:05 2024 ] 	Batch(4100/6809) done. Loss: 0.0046  lr:0.000001
[ Sun Jul 14 04:15:23 2024 ] 	Batch(4200/6809) done. Loss: 0.0858  lr:0.000001
[ Sun Jul 14 04:15:41 2024 ] 	Batch(4300/6809) done. Loss: 0.3246  lr:0.000001
[ Sun Jul 14 04:16:00 2024 ] 	Batch(4400/6809) done. Loss: 0.0232  lr:0.000001
[ Sun Jul 14 04:16:17 2024 ] 
Training: Epoch [31/50], Step [4499], Loss: 0.008699906058609486, Training Accuracy: 97.6361111111111
[ Sun Jul 14 04:16:18 2024 ] 	Batch(4500/6809) done. Loss: 0.0291  lr:0.000001
[ Sun Jul 14 04:16:36 2024 ] 	Batch(4600/6809) done. Loss: 0.0879  lr:0.000001
[ Sun Jul 14 04:16:54 2024 ] 	Batch(4700/6809) done. Loss: 0.0202  lr:0.000001
[ Sun Jul 14 04:17:12 2024 ] 	Batch(4800/6809) done. Loss: 0.0073  lr:0.000001
[ Sun Jul 14 04:17:30 2024 ] 	Batch(4900/6809) done. Loss: 0.0215  lr:0.000001
[ Sun Jul 14 04:17:48 2024 ] 
Training: Epoch [31/50], Step [4999], Loss: 0.023341381922364235, Training Accuracy: 97.65
[ Sun Jul 14 04:17:48 2024 ] 	Batch(5000/6809) done. Loss: 0.2847  lr:0.000001
[ Sun Jul 14 04:18:06 2024 ] 	Batch(5100/6809) done. Loss: 0.0152  lr:0.000001
[ Sun Jul 14 04:18:24 2024 ] 	Batch(5200/6809) done. Loss: 0.0533  lr:0.000001
[ Sun Jul 14 04:18:42 2024 ] 	Batch(5300/6809) done. Loss: 0.0266  lr:0.000001
[ Sun Jul 14 04:19:00 2024 ] 	Batch(5400/6809) done. Loss: 0.0664  lr:0.000001
[ Sun Jul 14 04:19:18 2024 ] 
Training: Epoch [31/50], Step [5499], Loss: 0.04570477828383446, Training Accuracy: 97.6590909090909
[ Sun Jul 14 04:19:19 2024 ] 	Batch(5500/6809) done. Loss: 0.0419  lr:0.000001
[ Sun Jul 14 04:19:37 2024 ] 	Batch(5600/6809) done. Loss: 0.0086  lr:0.000001
[ Sun Jul 14 04:19:55 2024 ] 	Batch(5700/6809) done. Loss: 0.0988  lr:0.000001
[ Sun Jul 14 04:20:13 2024 ] 	Batch(5800/6809) done. Loss: 0.0226  lr:0.000001
[ Sun Jul 14 04:20:31 2024 ] 	Batch(5900/6809) done. Loss: 0.0401  lr:0.000001
[ Sun Jul 14 04:20:49 2024 ] 
Training: Epoch [31/50], Step [5999], Loss: 0.0055599212646484375, Training Accuracy: 97.61875
[ Sun Jul 14 04:20:49 2024 ] 	Batch(6000/6809) done. Loss: 0.0109  lr:0.000001
[ Sun Jul 14 04:21:07 2024 ] 	Batch(6100/6809) done. Loss: 0.1212  lr:0.000001
[ Sun Jul 14 04:21:25 2024 ] 	Batch(6200/6809) done. Loss: 0.0147  lr:0.000001
[ Sun Jul 14 04:21:43 2024 ] 	Batch(6300/6809) done. Loss: 0.0273  lr:0.000001
[ Sun Jul 14 04:22:01 2024 ] 	Batch(6400/6809) done. Loss: 0.0968  lr:0.000001
[ Sun Jul 14 04:22:19 2024 ] 
Training: Epoch [31/50], Step [6499], Loss: 0.08458925783634186, Training Accuracy: 97.61538461538461
[ Sun Jul 14 04:22:19 2024 ] 	Batch(6500/6809) done. Loss: 0.0407  lr:0.000001
[ Sun Jul 14 04:22:38 2024 ] 	Batch(6600/6809) done. Loss: 0.0684  lr:0.000001
[ Sun Jul 14 04:22:56 2024 ] 	Batch(6700/6809) done. Loss: 0.0046  lr:0.000001
[ Sun Jul 14 04:23:14 2024 ] 	Batch(6800/6809) done. Loss: 0.3297  lr:0.000001
[ Sun Jul 14 04:23:15 2024 ] 	Mean training loss: 0.0957.
[ Sun Jul 14 04:23:15 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 04:23:15 2024 ] Training epoch: 33
[ Sun Jul 14 04:23:16 2024 ] 	Batch(0/6809) done. Loss: 0.0114  lr:0.000001
[ Sun Jul 14 04:23:34 2024 ] 	Batch(100/6809) done. Loss: 0.1926  lr:0.000001
[ Sun Jul 14 04:23:53 2024 ] 	Batch(200/6809) done. Loss: 0.1379  lr:0.000001
[ Sun Jul 14 04:24:11 2024 ] 	Batch(300/6809) done. Loss: 0.4213  lr:0.000001
[ Sun Jul 14 04:24:30 2024 ] 	Batch(400/6809) done. Loss: 0.0151  lr:0.000001
[ Sun Jul 14 04:24:49 2024 ] 
Training: Epoch [32/50], Step [499], Loss: 0.005584813188761473, Training Accuracy: 97.875
[ Sun Jul 14 04:24:49 2024 ] 	Batch(500/6809) done. Loss: 0.0187  lr:0.000001
[ Sun Jul 14 04:25:08 2024 ] 	Batch(600/6809) done. Loss: 0.0779  lr:0.000001
[ Sun Jul 14 04:25:26 2024 ] 	Batch(700/6809) done. Loss: 0.0004  lr:0.000001
[ Sun Jul 14 04:25:45 2024 ] 	Batch(800/6809) done. Loss: 0.0025  lr:0.000001
[ Sun Jul 14 04:26:03 2024 ] 	Batch(900/6809) done. Loss: 0.0832  lr:0.000001
[ Sun Jul 14 04:26:21 2024 ] 
Training: Epoch [32/50], Step [999], Loss: 0.013049401342868805, Training Accuracy: 97.8
[ Sun Jul 14 04:26:21 2024 ] 	Batch(1000/6809) done. Loss: 0.0140  lr:0.000001
[ Sun Jul 14 04:26:39 2024 ] 	Batch(1100/6809) done. Loss: 0.0180  lr:0.000001
[ Sun Jul 14 04:26:58 2024 ] 	Batch(1200/6809) done. Loss: 0.0052  lr:0.000001
[ Sun Jul 14 04:27:16 2024 ] 	Batch(1300/6809) done. Loss: 0.0080  lr:0.000001
[ Sun Jul 14 04:27:34 2024 ] 	Batch(1400/6809) done. Loss: 0.0079  lr:0.000001
[ Sun Jul 14 04:27:51 2024 ] 
Training: Epoch [32/50], Step [1499], Loss: 0.09801818430423737, Training Accuracy: 97.8
[ Sun Jul 14 04:27:52 2024 ] 	Batch(1500/6809) done. Loss: 0.0312  lr:0.000001
[ Sun Jul 14 04:28:10 2024 ] 	Batch(1600/6809) done. Loss: 0.0113  lr:0.000001
[ Sun Jul 14 04:28:28 2024 ] 	Batch(1700/6809) done. Loss: 0.0393  lr:0.000001
[ Sun Jul 14 04:28:46 2024 ] 	Batch(1800/6809) done. Loss: 0.1638  lr:0.000001
[ Sun Jul 14 04:29:04 2024 ] 	Batch(1900/6809) done. Loss: 0.0133  lr:0.000001
[ Sun Jul 14 04:29:22 2024 ] 
Training: Epoch [32/50], Step [1999], Loss: 0.12562480568885803, Training Accuracy: 97.8375
[ Sun Jul 14 04:29:22 2024 ] 	Batch(2000/6809) done. Loss: 0.0246  lr:0.000001
[ Sun Jul 14 04:29:40 2024 ] 	Batch(2100/6809) done. Loss: 0.0292  lr:0.000001
[ Sun Jul 14 04:29:58 2024 ] 	Batch(2200/6809) done. Loss: 0.0378  lr:0.000001
[ Sun Jul 14 04:30:16 2024 ] 	Batch(2300/6809) done. Loss: 0.1736  lr:0.000001
[ Sun Jul 14 04:30:35 2024 ] 	Batch(2400/6809) done. Loss: 0.2315  lr:0.000001
[ Sun Jul 14 04:30:53 2024 ] 
Training: Epoch [32/50], Step [2499], Loss: 0.15940392017364502, Training Accuracy: 97.69
[ Sun Jul 14 04:30:53 2024 ] 	Batch(2500/6809) done. Loss: 0.0615  lr:0.000001
[ Sun Jul 14 04:31:11 2024 ] 	Batch(2600/6809) done. Loss: 0.0729  lr:0.000001
[ Sun Jul 14 04:31:29 2024 ] 	Batch(2700/6809) done. Loss: 0.3434  lr:0.000001
[ Sun Jul 14 04:31:47 2024 ] 	Batch(2800/6809) done. Loss: 0.0458  lr:0.000001
[ Sun Jul 14 04:32:05 2024 ] 	Batch(2900/6809) done. Loss: 0.1035  lr:0.000001
[ Sun Jul 14 04:32:23 2024 ] 
Training: Epoch [32/50], Step [2999], Loss: 0.01308402419090271, Training Accuracy: 97.68333333333334
[ Sun Jul 14 04:32:23 2024 ] 	Batch(3000/6809) done. Loss: 0.0686  lr:0.000001
[ Sun Jul 14 04:32:41 2024 ] 	Batch(3100/6809) done. Loss: 0.2902  lr:0.000001
[ Sun Jul 14 04:33:00 2024 ] 	Batch(3200/6809) done. Loss: 0.0534  lr:0.000001
[ Sun Jul 14 04:33:18 2024 ] 	Batch(3300/6809) done. Loss: 0.1144  lr:0.000001
[ Sun Jul 14 04:33:36 2024 ] 	Batch(3400/6809) done. Loss: 0.0601  lr:0.000001
[ Sun Jul 14 04:33:54 2024 ] 
Training: Epoch [32/50], Step [3499], Loss: 0.006120922043919563, Training Accuracy: 97.67857142857143
[ Sun Jul 14 04:33:54 2024 ] 	Batch(3500/6809) done. Loss: 0.0231  lr:0.000001
[ Sun Jul 14 04:34:12 2024 ] 	Batch(3600/6809) done. Loss: 0.0154  lr:0.000001
[ Sun Jul 14 04:34:31 2024 ] 	Batch(3700/6809) done. Loss: 0.0053  lr:0.000001
[ Sun Jul 14 04:34:49 2024 ] 	Batch(3800/6809) done. Loss: 0.0482  lr:0.000001
[ Sun Jul 14 04:35:08 2024 ] 	Batch(3900/6809) done. Loss: 0.0463  lr:0.000001
[ Sun Jul 14 04:35:27 2024 ] 
Training: Epoch [32/50], Step [3999], Loss: 0.013894427567720413, Training Accuracy: 97.71875
[ Sun Jul 14 04:35:27 2024 ] 	Batch(4000/6809) done. Loss: 0.5658  lr:0.000001
[ Sun Jul 14 04:35:46 2024 ] 	Batch(4100/6809) done. Loss: 0.0763  lr:0.000001
[ Sun Jul 14 04:36:04 2024 ] 	Batch(4200/6809) done. Loss: 0.0065  lr:0.000001
[ Sun Jul 14 04:36:23 2024 ] 	Batch(4300/6809) done. Loss: 0.0086  lr:0.000001
[ Sun Jul 14 04:36:42 2024 ] 	Batch(4400/6809) done. Loss: 0.0152  lr:0.000001
[ Sun Jul 14 04:37:00 2024 ] 
Training: Epoch [32/50], Step [4499], Loss: 0.08089141547679901, Training Accuracy: 97.69166666666666
[ Sun Jul 14 04:37:01 2024 ] 	Batch(4500/6809) done. Loss: 0.0998  lr:0.000001
[ Sun Jul 14 04:37:19 2024 ] 	Batch(4600/6809) done. Loss: 0.0061  lr:0.000001
[ Sun Jul 14 04:37:37 2024 ] 	Batch(4700/6809) done. Loss: 0.1156  lr:0.000001
[ Sun Jul 14 04:37:55 2024 ] 	Batch(4800/6809) done. Loss: 0.3172  lr:0.000001
[ Sun Jul 14 04:38:13 2024 ] 	Batch(4900/6809) done. Loss: 0.0869  lr:0.000001
[ Sun Jul 14 04:38:31 2024 ] 
Training: Epoch [32/50], Step [4999], Loss: 0.032967690378427505, Training Accuracy: 97.6825
[ Sun Jul 14 04:38:31 2024 ] 	Batch(5000/6809) done. Loss: 0.0403  lr:0.000001
[ Sun Jul 14 04:38:49 2024 ] 	Batch(5100/6809) done. Loss: 0.0057  lr:0.000001
[ Sun Jul 14 04:39:08 2024 ] 	Batch(5200/6809) done. Loss: 0.0043  lr:0.000001
[ Sun Jul 14 04:39:26 2024 ] 	Batch(5300/6809) done. Loss: 0.1570  lr:0.000001
[ Sun Jul 14 04:39:44 2024 ] 	Batch(5400/6809) done. Loss: 0.0068  lr:0.000001
[ Sun Jul 14 04:40:02 2024 ] 
Training: Epoch [32/50], Step [5499], Loss: 0.06214694678783417, Training Accuracy: 97.66136363636365
[ Sun Jul 14 04:40:02 2024 ] 	Batch(5500/6809) done. Loss: 0.0160  lr:0.000001
[ Sun Jul 14 04:40:20 2024 ] 	Batch(5600/6809) done. Loss: 0.0016  lr:0.000001
[ Sun Jul 14 04:40:38 2024 ] 	Batch(5700/6809) done. Loss: 0.0111  lr:0.000001
[ Sun Jul 14 04:40:56 2024 ] 	Batch(5800/6809) done. Loss: 0.0863  lr:0.000001
[ Sun Jul 14 04:41:14 2024 ] 	Batch(5900/6809) done. Loss: 0.1441  lr:0.000001
[ Sun Jul 14 04:41:32 2024 ] 
Training: Epoch [32/50], Step [5999], Loss: 0.29016515612602234, Training Accuracy: 97.66458333333333
[ Sun Jul 14 04:41:32 2024 ] 	Batch(6000/6809) done. Loss: 0.1086  lr:0.000001
[ Sun Jul 14 04:41:51 2024 ] 	Batch(6100/6809) done. Loss: 0.0284  lr:0.000001
[ Sun Jul 14 04:42:09 2024 ] 	Batch(6200/6809) done. Loss: 0.0328  lr:0.000001
[ Sun Jul 14 04:42:27 2024 ] 	Batch(6300/6809) done. Loss: 0.0179  lr:0.000001
[ Sun Jul 14 04:42:46 2024 ] 	Batch(6400/6809) done. Loss: 0.0377  lr:0.000001
[ Sun Jul 14 04:43:04 2024 ] 
Training: Epoch [32/50], Step [6499], Loss: 0.14245735108852386, Training Accuracy: 97.65384615384616
[ Sun Jul 14 04:43:04 2024 ] 	Batch(6500/6809) done. Loss: 0.1861  lr:0.000001
[ Sun Jul 14 04:43:22 2024 ] 	Batch(6600/6809) done. Loss: 0.2473  lr:0.000001
[ Sun Jul 14 04:43:40 2024 ] 	Batch(6700/6809) done. Loss: 0.1420  lr:0.000001
[ Sun Jul 14 04:43:59 2024 ] 	Batch(6800/6809) done. Loss: 0.0751  lr:0.000001
[ Sun Jul 14 04:44:00 2024 ] 	Mean training loss: 0.0916.
[ Sun Jul 14 04:44:00 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 04:44:00 2024 ] Training epoch: 34
[ Sun Jul 14 04:44:01 2024 ] 	Batch(0/6809) done. Loss: 0.0019  lr:0.000001
[ Sun Jul 14 04:44:19 2024 ] 	Batch(100/6809) done. Loss: 0.0222  lr:0.000001
[ Sun Jul 14 04:44:38 2024 ] 	Batch(200/6809) done. Loss: 0.0514  lr:0.000001
[ Sun Jul 14 04:44:56 2024 ] 	Batch(300/6809) done. Loss: 0.1708  lr:0.000001
[ Sun Jul 14 04:45:15 2024 ] 	Batch(400/6809) done. Loss: 0.0797  lr:0.000001
[ Sun Jul 14 04:45:33 2024 ] 
Training: Epoch [33/50], Step [499], Loss: 0.16883045434951782, Training Accuracy: 98.0
[ Sun Jul 14 04:45:33 2024 ] 	Batch(500/6809) done. Loss: 0.0595  lr:0.000001
[ Sun Jul 14 04:45:51 2024 ] 	Batch(600/6809) done. Loss: 0.0997  lr:0.000001
[ Sun Jul 14 04:46:09 2024 ] 	Batch(700/6809) done. Loss: 0.1997  lr:0.000001
[ Sun Jul 14 04:46:28 2024 ] 	Batch(800/6809) done. Loss: 0.0115  lr:0.000001
[ Sun Jul 14 04:46:46 2024 ] 	Batch(900/6809) done. Loss: 0.0867  lr:0.000001
[ Sun Jul 14 04:47:04 2024 ] 
Training: Epoch [33/50], Step [999], Loss: 0.08735860139131546, Training Accuracy: 97.9375
[ Sun Jul 14 04:47:04 2024 ] 	Batch(1000/6809) done. Loss: 0.1804  lr:0.000001
[ Sun Jul 14 04:47:22 2024 ] 	Batch(1100/6809) done. Loss: 0.0063  lr:0.000001
[ Sun Jul 14 04:47:40 2024 ] 	Batch(1200/6809) done. Loss: 0.0592  lr:0.000001
[ Sun Jul 14 04:47:58 2024 ] 	Batch(1300/6809) done. Loss: 0.0501  lr:0.000001
[ Sun Jul 14 04:48:16 2024 ] 	Batch(1400/6809) done. Loss: 0.5585  lr:0.000001
[ Sun Jul 14 04:48:34 2024 ] 
Training: Epoch [33/50], Step [1499], Loss: 0.062022626399993896, Training Accuracy: 97.81666666666666
[ Sun Jul 14 04:48:34 2024 ] 	Batch(1500/6809) done. Loss: 0.0292  lr:0.000001
[ Sun Jul 14 04:48:53 2024 ] 	Batch(1600/6809) done. Loss: 0.2376  lr:0.000001
[ Sun Jul 14 04:49:11 2024 ] 	Batch(1700/6809) done. Loss: 0.0707  lr:0.000001
[ Sun Jul 14 04:49:29 2024 ] 	Batch(1800/6809) done. Loss: 0.1178  lr:0.000001
[ Sun Jul 14 04:49:47 2024 ] 	Batch(1900/6809) done. Loss: 0.1761  lr:0.000001
[ Sun Jul 14 04:50:05 2024 ] 
Training: Epoch [33/50], Step [1999], Loss: 0.024151772260665894, Training Accuracy: 97.89999999999999
[ Sun Jul 14 04:50:05 2024 ] 	Batch(2000/6809) done. Loss: 0.9242  lr:0.000001
[ Sun Jul 14 04:50:23 2024 ] 	Batch(2100/6809) done. Loss: 0.1648  lr:0.000001
[ Sun Jul 14 04:50:41 2024 ] 	Batch(2200/6809) done. Loss: 0.0906  lr:0.000001
[ Sun Jul 14 04:50:59 2024 ] 	Batch(2300/6809) done. Loss: 0.0102  lr:0.000001
[ Sun Jul 14 04:51:17 2024 ] 	Batch(2400/6809) done. Loss: 0.2921  lr:0.000001
[ Sun Jul 14 04:51:35 2024 ] 
Training: Epoch [33/50], Step [2499], Loss: 0.015824129804968834, Training Accuracy: 97.82499999999999
[ Sun Jul 14 04:51:35 2024 ] 	Batch(2500/6809) done. Loss: 0.0298  lr:0.000001
[ Sun Jul 14 04:51:53 2024 ] 	Batch(2600/6809) done. Loss: 0.0216  lr:0.000001
[ Sun Jul 14 04:52:11 2024 ] 	Batch(2700/6809) done. Loss: 0.0250  lr:0.000001
[ Sun Jul 14 04:52:29 2024 ] 	Batch(2800/6809) done. Loss: 0.0115  lr:0.000001
[ Sun Jul 14 04:52:47 2024 ] 	Batch(2900/6809) done. Loss: 0.0595  lr:0.000001
[ Sun Jul 14 04:53:05 2024 ] 
Training: Epoch [33/50], Step [2999], Loss: 0.0061233313754200935, Training Accuracy: 97.77916666666667
[ Sun Jul 14 04:53:05 2024 ] 	Batch(3000/6809) done. Loss: 0.0078  lr:0.000001
[ Sun Jul 14 04:53:24 2024 ] 	Batch(3100/6809) done. Loss: 0.1126  lr:0.000001
[ Sun Jul 14 04:53:42 2024 ] 	Batch(3200/6809) done. Loss: 0.0170  lr:0.000001
[ Sun Jul 14 04:54:00 2024 ] 	Batch(3300/6809) done. Loss: 0.0322  lr:0.000001
[ Sun Jul 14 04:54:18 2024 ] 	Batch(3400/6809) done. Loss: 0.1794  lr:0.000001
[ Sun Jul 14 04:54:36 2024 ] 
Training: Epoch [33/50], Step [3499], Loss: 0.2933448851108551, Training Accuracy: 97.80714285714286
[ Sun Jul 14 04:54:36 2024 ] 	Batch(3500/6809) done. Loss: 0.0712  lr:0.000001
[ Sun Jul 14 04:54:54 2024 ] 	Batch(3600/6809) done. Loss: 0.0682  lr:0.000001
[ Sun Jul 14 04:55:12 2024 ] 	Batch(3700/6809) done. Loss: 0.0204  lr:0.000001
[ Sun Jul 14 04:55:30 2024 ] 	Batch(3800/6809) done. Loss: 0.1341  lr:0.000001
[ Sun Jul 14 04:55:48 2024 ] 	Batch(3900/6809) done. Loss: 0.2463  lr:0.000001
[ Sun Jul 14 04:56:06 2024 ] 
Training: Epoch [33/50], Step [3999], Loss: 0.040281280875205994, Training Accuracy: 97.80624999999999
[ Sun Jul 14 04:56:06 2024 ] 	Batch(4000/6809) done. Loss: 0.0125  lr:0.000001
[ Sun Jul 14 04:56:24 2024 ] 	Batch(4100/6809) done. Loss: 0.0681  lr:0.000001
[ Sun Jul 14 04:56:42 2024 ] 	Batch(4200/6809) done. Loss: 0.0135  lr:0.000001
[ Sun Jul 14 04:57:00 2024 ] 	Batch(4300/6809) done. Loss: 0.0097  lr:0.000001
[ Sun Jul 14 04:57:18 2024 ] 	Batch(4400/6809) done. Loss: 0.0587  lr:0.000001
[ Sun Jul 14 04:57:36 2024 ] 
Training: Epoch [33/50], Step [4499], Loss: 0.12236221879720688, Training Accuracy: 97.81666666666666
[ Sun Jul 14 04:57:36 2024 ] 	Batch(4500/6809) done. Loss: 0.0397  lr:0.000001
[ Sun Jul 14 04:57:55 2024 ] 	Batch(4600/6809) done. Loss: 0.0210  lr:0.000001
[ Sun Jul 14 04:58:13 2024 ] 	Batch(4700/6809) done. Loss: 0.5581  lr:0.000001
[ Sun Jul 14 04:58:32 2024 ] 	Batch(4800/6809) done. Loss: 0.0479  lr:0.000001
[ Sun Jul 14 04:58:50 2024 ] 	Batch(4900/6809) done. Loss: 0.0535  lr:0.000001
[ Sun Jul 14 04:59:08 2024 ] 
Training: Epoch [33/50], Step [4999], Loss: 0.05483204498887062, Training Accuracy: 97.7825
[ Sun Jul 14 04:59:08 2024 ] 	Batch(5000/6809) done. Loss: 0.0932  lr:0.000001
[ Sun Jul 14 04:59:26 2024 ] 	Batch(5100/6809) done. Loss: 0.1197  lr:0.000001
[ Sun Jul 14 04:59:44 2024 ] 	Batch(5200/6809) done. Loss: 0.1600  lr:0.000001
[ Sun Jul 14 05:00:02 2024 ] 	Batch(5300/6809) done. Loss: 0.0139  lr:0.000001
[ Sun Jul 14 05:00:20 2024 ] 	Batch(5400/6809) done. Loss: 0.0655  lr:0.000001
[ Sun Jul 14 05:00:38 2024 ] 
Training: Epoch [33/50], Step [5499], Loss: 0.0018798294477164745, Training Accuracy: 97.78863636363636
[ Sun Jul 14 05:00:38 2024 ] 	Batch(5500/6809) done. Loss: 0.1596  lr:0.000001
[ Sun Jul 14 05:00:56 2024 ] 	Batch(5600/6809) done. Loss: 0.1879  lr:0.000001
[ Sun Jul 14 05:01:14 2024 ] 	Batch(5700/6809) done. Loss: 0.0618  lr:0.000001
[ Sun Jul 14 05:01:32 2024 ] 	Batch(5800/6809) done. Loss: 0.1241  lr:0.000001
[ Sun Jul 14 05:01:50 2024 ] 	Batch(5900/6809) done. Loss: 0.0109  lr:0.000001
[ Sun Jul 14 05:02:08 2024 ] 
Training: Epoch [33/50], Step [5999], Loss: 0.029264817014336586, Training Accuracy: 97.76041666666667
[ Sun Jul 14 05:02:09 2024 ] 	Batch(6000/6809) done. Loss: 0.0851  lr:0.000001
[ Sun Jul 14 05:02:27 2024 ] 	Batch(6100/6809) done. Loss: 0.2515  lr:0.000001
[ Sun Jul 14 05:02:45 2024 ] 	Batch(6200/6809) done. Loss: 0.1269  lr:0.000001
[ Sun Jul 14 05:03:03 2024 ] 	Batch(6300/6809) done. Loss: 0.1048  lr:0.000001
[ Sun Jul 14 05:03:21 2024 ] 	Batch(6400/6809) done. Loss: 0.0070  lr:0.000001
[ Sun Jul 14 05:03:39 2024 ] 
Training: Epoch [33/50], Step [6499], Loss: 0.01711175963282585, Training Accuracy: 97.7673076923077
[ Sun Jul 14 05:03:39 2024 ] 	Batch(6500/6809) done. Loss: 0.0695  lr:0.000001
[ Sun Jul 14 05:03:57 2024 ] 	Batch(6600/6809) done. Loss: 0.1191  lr:0.000001
[ Sun Jul 14 05:04:15 2024 ] 	Batch(6700/6809) done. Loss: 0.0544  lr:0.000001
[ Sun Jul 14 05:04:34 2024 ] 	Batch(6800/6809) done. Loss: 0.2635  lr:0.000001
[ Sun Jul 14 05:04:35 2024 ] 	Mean training loss: 0.0870.
[ Sun Jul 14 05:04:35 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 05:04:35 2024 ] Training epoch: 35
[ Sun Jul 14 05:04:36 2024 ] 	Batch(0/6809) done. Loss: 0.1003  lr:0.000001
[ Sun Jul 14 05:04:54 2024 ] 	Batch(100/6809) done. Loss: 0.1470  lr:0.000001
[ Sun Jul 14 05:05:12 2024 ] 	Batch(200/6809) done. Loss: 0.0125  lr:0.000001
[ Sun Jul 14 05:05:30 2024 ] 	Batch(300/6809) done. Loss: 0.0284  lr:0.000001
[ Sun Jul 14 05:05:48 2024 ] 	Batch(400/6809) done. Loss: 0.0223  lr:0.000001
[ Sun Jul 14 05:06:07 2024 ] 
Training: Epoch [34/50], Step [499], Loss: 0.24440062046051025, Training Accuracy: 97.39999999999999
[ Sun Jul 14 05:06:07 2024 ] 	Batch(500/6809) done. Loss: 0.0795  lr:0.000001
[ Sun Jul 14 05:06:26 2024 ] 	Batch(600/6809) done. Loss: 0.0105  lr:0.000001
[ Sun Jul 14 05:06:44 2024 ] 	Batch(700/6809) done. Loss: 0.0089  lr:0.000001
[ Sun Jul 14 05:07:03 2024 ] 	Batch(800/6809) done. Loss: 0.4577  lr:0.000001
[ Sun Jul 14 05:07:21 2024 ] 	Batch(900/6809) done. Loss: 0.0312  lr:0.000001
[ Sun Jul 14 05:07:39 2024 ] 
Training: Epoch [34/50], Step [999], Loss: 0.0759187713265419, Training Accuracy: 97.475
[ Sun Jul 14 05:07:40 2024 ] 	Batch(1000/6809) done. Loss: 0.0155  lr:0.000001
[ Sun Jul 14 05:07:58 2024 ] 	Batch(1100/6809) done. Loss: 0.0619  lr:0.000001
[ Sun Jul 14 05:08:16 2024 ] 	Batch(1200/6809) done. Loss: 0.0227  lr:0.000001
[ Sun Jul 14 05:08:33 2024 ] 	Batch(1300/6809) done. Loss: 0.1176  lr:0.000001
[ Sun Jul 14 05:08:51 2024 ] 	Batch(1400/6809) done. Loss: 0.0596  lr:0.000001
[ Sun Jul 14 05:09:09 2024 ] 
Training: Epoch [34/50], Step [1499], Loss: 0.005243451334536076, Training Accuracy: 97.44166666666668
[ Sun Jul 14 05:09:09 2024 ] 	Batch(1500/6809) done. Loss: 0.0293  lr:0.000001
[ Sun Jul 14 05:09:27 2024 ] 	Batch(1600/6809) done. Loss: 0.0111  lr:0.000001
[ Sun Jul 14 05:09:45 2024 ] 	Batch(1700/6809) done. Loss: 0.0415  lr:0.000001
[ Sun Jul 14 05:10:03 2024 ] 	Batch(1800/6809) done. Loss: 0.0014  lr:0.000001
[ Sun Jul 14 05:10:21 2024 ] 	Batch(1900/6809) done. Loss: 0.0875  lr:0.000001
[ Sun Jul 14 05:10:39 2024 ] 
Training: Epoch [34/50], Step [1999], Loss: 0.2804998755455017, Training Accuracy: 97.50625000000001
[ Sun Jul 14 05:10:39 2024 ] 	Batch(2000/6809) done. Loss: 0.0266  lr:0.000001
[ Sun Jul 14 05:10:57 2024 ] 	Batch(2100/6809) done. Loss: 0.0157  lr:0.000001
[ Sun Jul 14 05:11:15 2024 ] 	Batch(2200/6809) done. Loss: 0.0511  lr:0.000001
[ Sun Jul 14 05:11:33 2024 ] 	Batch(2300/6809) done. Loss: 0.0156  lr:0.000001
[ Sun Jul 14 05:11:51 2024 ] 	Batch(2400/6809) done. Loss: 0.0119  lr:0.000001
[ Sun Jul 14 05:12:09 2024 ] 
Training: Epoch [34/50], Step [2499], Loss: 0.013531356118619442, Training Accuracy: 97.61999999999999
[ Sun Jul 14 05:12:09 2024 ] 	Batch(2500/6809) done. Loss: 0.0014  lr:0.000001
[ Sun Jul 14 05:12:27 2024 ] 	Batch(2600/6809) done. Loss: 0.1434  lr:0.000001
[ Sun Jul 14 05:12:45 2024 ] 	Batch(2700/6809) done. Loss: 0.1089  lr:0.000001
[ Sun Jul 14 05:13:03 2024 ] 	Batch(2800/6809) done. Loss: 0.4808  lr:0.000001
[ Sun Jul 14 05:13:21 2024 ] 	Batch(2900/6809) done. Loss: 0.3000  lr:0.000001
[ Sun Jul 14 05:13:38 2024 ] 
Training: Epoch [34/50], Step [2999], Loss: 0.06501628458499908, Training Accuracy: 97.6875
[ Sun Jul 14 05:13:38 2024 ] 	Batch(3000/6809) done. Loss: 0.3391  lr:0.000001
[ Sun Jul 14 05:13:56 2024 ] 	Batch(3100/6809) done. Loss: 0.1960  lr:0.000001
[ Sun Jul 14 05:14:15 2024 ] 	Batch(3200/6809) done. Loss: 0.0949  lr:0.000001
[ Sun Jul 14 05:14:33 2024 ] 	Batch(3300/6809) done. Loss: 0.0098  lr:0.000001
[ Sun Jul 14 05:14:51 2024 ] 	Batch(3400/6809) done. Loss: 0.0061  lr:0.000001
[ Sun Jul 14 05:15:09 2024 ] 
Training: Epoch [34/50], Step [3499], Loss: 0.08129256963729858, Training Accuracy: 97.61785714285715
[ Sun Jul 14 05:15:09 2024 ] 	Batch(3500/6809) done. Loss: 0.0903  lr:0.000001
[ Sun Jul 14 05:15:27 2024 ] 	Batch(3600/6809) done. Loss: 0.2129  lr:0.000001
[ Sun Jul 14 05:15:46 2024 ] 	Batch(3700/6809) done. Loss: 0.0439  lr:0.000001
[ Sun Jul 14 05:16:04 2024 ] 	Batch(3800/6809) done. Loss: 0.1457  lr:0.000001
[ Sun Jul 14 05:16:23 2024 ] 	Batch(3900/6809) done. Loss: 0.0957  lr:0.000001
[ Sun Jul 14 05:16:41 2024 ] 
Training: Epoch [34/50], Step [3999], Loss: 0.029765641316771507, Training Accuracy: 97.5875
[ Sun Jul 14 05:16:42 2024 ] 	Batch(4000/6809) done. Loss: 0.0170  lr:0.000001
[ Sun Jul 14 05:17:00 2024 ] 	Batch(4100/6809) done. Loss: 0.0611  lr:0.000001
[ Sun Jul 14 05:17:19 2024 ] 	Batch(4200/6809) done. Loss: 0.0579  lr:0.000001
[ Sun Jul 14 05:17:37 2024 ] 	Batch(4300/6809) done. Loss: 0.0355  lr:0.000001
[ Sun Jul 14 05:17:56 2024 ] 	Batch(4400/6809) done. Loss: 0.0988  lr:0.000001
[ Sun Jul 14 05:18:14 2024 ] 
Training: Epoch [34/50], Step [4499], Loss: 0.05727148801088333, Training Accuracy: 97.64166666666667
[ Sun Jul 14 05:18:14 2024 ] 	Batch(4500/6809) done. Loss: 0.2732  lr:0.000001
[ Sun Jul 14 05:18:32 2024 ] 	Batch(4600/6809) done. Loss: 0.0241  lr:0.000001
[ Sun Jul 14 05:18:50 2024 ] 	Batch(4700/6809) done. Loss: 0.0979  lr:0.000001
[ Sun Jul 14 05:19:08 2024 ] 	Batch(4800/6809) done. Loss: 0.3029  lr:0.000001
[ Sun Jul 14 05:19:26 2024 ] 	Batch(4900/6809) done. Loss: 0.0635  lr:0.000001
[ Sun Jul 14 05:19:44 2024 ] 
Training: Epoch [34/50], Step [4999], Loss: 0.1425703763961792, Training Accuracy: 97.6075
[ Sun Jul 14 05:19:44 2024 ] 	Batch(5000/6809) done. Loss: 0.0047  lr:0.000001
[ Sun Jul 14 05:20:02 2024 ] 	Batch(5100/6809) done. Loss: 0.0965  lr:0.000001
[ Sun Jul 14 05:20:20 2024 ] 	Batch(5200/6809) done. Loss: 0.0249  lr:0.000001
[ Sun Jul 14 05:20:38 2024 ] 	Batch(5300/6809) done. Loss: 0.0680  lr:0.000001
[ Sun Jul 14 05:20:56 2024 ] 	Batch(5400/6809) done. Loss: 0.0823  lr:0.000001
[ Sun Jul 14 05:21:14 2024 ] 
Training: Epoch [34/50], Step [5499], Loss: 0.013612005859613419, Training Accuracy: 97.63636363636363
[ Sun Jul 14 05:21:15 2024 ] 	Batch(5500/6809) done. Loss: 0.0283  lr:0.000001
[ Sun Jul 14 05:21:33 2024 ] 	Batch(5600/6809) done. Loss: 0.0418  lr:0.000001
[ Sun Jul 14 05:21:51 2024 ] 	Batch(5700/6809) done. Loss: 0.0394  lr:0.000001
[ Sun Jul 14 05:22:09 2024 ] 	Batch(5800/6809) done. Loss: 0.0935  lr:0.000001
[ Sun Jul 14 05:22:27 2024 ] 	Batch(5900/6809) done. Loss: 0.2305  lr:0.000001
[ Sun Jul 14 05:22:45 2024 ] 
Training: Epoch [34/50], Step [5999], Loss: 0.043817080557346344, Training Accuracy: 97.61875
[ Sun Jul 14 05:22:45 2024 ] 	Batch(6000/6809) done. Loss: 0.0400  lr:0.000001
[ Sun Jul 14 05:23:03 2024 ] 	Batch(6100/6809) done. Loss: 0.0092  lr:0.000001
[ Sun Jul 14 05:23:21 2024 ] 	Batch(6200/6809) done. Loss: 0.1456  lr:0.000001
[ Sun Jul 14 05:23:39 2024 ] 	Batch(6300/6809) done. Loss: 0.0042  lr:0.000001
[ Sun Jul 14 05:23:58 2024 ] 	Batch(6400/6809) done. Loss: 0.0173  lr:0.000001
[ Sun Jul 14 05:24:15 2024 ] 
Training: Epoch [34/50], Step [6499], Loss: 0.021660443395376205, Training Accuracy: 97.6076923076923
[ Sun Jul 14 05:24:16 2024 ] 	Batch(6500/6809) done. Loss: 0.0183  lr:0.000001
[ Sun Jul 14 05:24:33 2024 ] 	Batch(6600/6809) done. Loss: 0.0158  lr:0.000001
[ Sun Jul 14 05:24:51 2024 ] 	Batch(6700/6809) done. Loss: 0.0897  lr:0.000001
[ Sun Jul 14 05:25:09 2024 ] 	Batch(6800/6809) done. Loss: 0.0059  lr:0.000001
[ Sun Jul 14 05:25:11 2024 ] 	Mean training loss: 0.0904.
[ Sun Jul 14 05:25:11 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 05:25:11 2024 ] Training epoch: 36
[ Sun Jul 14 05:25:11 2024 ] 	Batch(0/6809) done. Loss: 0.0386  lr:0.000001
[ Sun Jul 14 05:25:30 2024 ] 	Batch(100/6809) done. Loss: 0.0443  lr:0.000001
[ Sun Jul 14 05:25:48 2024 ] 	Batch(200/6809) done. Loss: 0.1066  lr:0.000001
[ Sun Jul 14 05:26:06 2024 ] 	Batch(300/6809) done. Loss: 0.5416  lr:0.000001
[ Sun Jul 14 05:26:25 2024 ] 	Batch(400/6809) done. Loss: 0.0186  lr:0.000001
[ Sun Jul 14 05:26:43 2024 ] 
Training: Epoch [35/50], Step [499], Loss: 0.19323870539665222, Training Accuracy: 97.52499999999999
[ Sun Jul 14 05:26:43 2024 ] 	Batch(500/6809) done. Loss: 0.1864  lr:0.000001
[ Sun Jul 14 05:27:01 2024 ] 	Batch(600/6809) done. Loss: 0.0630  lr:0.000001
[ Sun Jul 14 05:27:19 2024 ] 	Batch(700/6809) done. Loss: 0.0025  lr:0.000001
[ Sun Jul 14 05:27:37 2024 ] 	Batch(800/6809) done. Loss: 0.0176  lr:0.000001
[ Sun Jul 14 05:27:55 2024 ] 	Batch(900/6809) done. Loss: 0.1229  lr:0.000001
[ Sun Jul 14 05:28:13 2024 ] 
Training: Epoch [35/50], Step [999], Loss: 0.05597423017024994, Training Accuracy: 97.55
[ Sun Jul 14 05:28:13 2024 ] 	Batch(1000/6809) done. Loss: 0.0220  lr:0.000001
[ Sun Jul 14 05:28:31 2024 ] 	Batch(1100/6809) done. Loss: 0.0873  lr:0.000001
[ Sun Jul 14 05:28:49 2024 ] 	Batch(1200/6809) done. Loss: 0.2387  lr:0.000001
[ Sun Jul 14 05:29:07 2024 ] 	Batch(1300/6809) done. Loss: 0.1816  lr:0.000001
[ Sun Jul 14 05:29:25 2024 ] 	Batch(1400/6809) done. Loss: 0.1078  lr:0.000001
[ Sun Jul 14 05:29:43 2024 ] 
Training: Epoch [35/50], Step [1499], Loss: 0.11349207907915115, Training Accuracy: 97.65833333333333
[ Sun Jul 14 05:29:43 2024 ] 	Batch(1500/6809) done. Loss: 0.1586  lr:0.000001
[ Sun Jul 14 05:30:02 2024 ] 	Batch(1600/6809) done. Loss: 0.0770  lr:0.000001
[ Sun Jul 14 05:30:20 2024 ] 	Batch(1700/6809) done. Loss: 0.0228  lr:0.000001
[ Sun Jul 14 05:30:38 2024 ] 	Batch(1800/6809) done. Loss: 0.1128  lr:0.000001
[ Sun Jul 14 05:30:56 2024 ] 	Batch(1900/6809) done. Loss: 0.1490  lr:0.000001
[ Sun Jul 14 05:31:14 2024 ] 
Training: Epoch [35/50], Step [1999], Loss: 0.02987220324575901, Training Accuracy: 97.6625
[ Sun Jul 14 05:31:14 2024 ] 	Batch(2000/6809) done. Loss: 0.0233  lr:0.000001
[ Sun Jul 14 05:31:32 2024 ] 	Batch(2100/6809) done. Loss: 0.0024  lr:0.000001
[ Sun Jul 14 05:31:50 2024 ] 	Batch(2200/6809) done. Loss: 0.0248  lr:0.000001
[ Sun Jul 14 05:32:08 2024 ] 	Batch(2300/6809) done. Loss: 0.0176  lr:0.000001
[ Sun Jul 14 05:32:26 2024 ] 	Batch(2400/6809) done. Loss: 0.1799  lr:0.000001
[ Sun Jul 14 05:32:45 2024 ] 
Training: Epoch [35/50], Step [2499], Loss: 0.27535417675971985, Training Accuracy: 97.675
[ Sun Jul 14 05:32:45 2024 ] 	Batch(2500/6809) done. Loss: 0.0312  lr:0.000001
[ Sun Jul 14 05:33:04 2024 ] 	Batch(2600/6809) done. Loss: 0.2872  lr:0.000001
[ Sun Jul 14 05:33:22 2024 ] 	Batch(2700/6809) done. Loss: 0.1737  lr:0.000001
[ Sun Jul 14 05:33:41 2024 ] 	Batch(2800/6809) done. Loss: 0.0088  lr:0.000001
[ Sun Jul 14 05:33:59 2024 ] 	Batch(2900/6809) done. Loss: 0.0031  lr:0.000001
[ Sun Jul 14 05:34:17 2024 ] 
Training: Epoch [35/50], Step [2999], Loss: 0.052488066256046295, Training Accuracy: 97.7625
[ Sun Jul 14 05:34:17 2024 ] 	Batch(3000/6809) done. Loss: 0.1363  lr:0.000001
[ Sun Jul 14 05:34:35 2024 ] 	Batch(3100/6809) done. Loss: 0.0507  lr:0.000001
[ Sun Jul 14 05:34:53 2024 ] 	Batch(3200/6809) done. Loss: 0.1753  lr:0.000001
[ Sun Jul 14 05:35:11 2024 ] 	Batch(3300/6809) done. Loss: 0.0504  lr:0.000001
[ Sun Jul 14 05:35:29 2024 ] 	Batch(3400/6809) done. Loss: 0.0253  lr:0.000001
[ Sun Jul 14 05:35:47 2024 ] 
Training: Epoch [35/50], Step [3499], Loss: 0.09065164625644684, Training Accuracy: 97.73214285714286
[ Sun Jul 14 05:35:47 2024 ] 	Batch(3500/6809) done. Loss: 0.0114  lr:0.000001
[ Sun Jul 14 05:36:05 2024 ] 	Batch(3600/6809) done. Loss: 0.1049  lr:0.000001
[ Sun Jul 14 05:36:23 2024 ] 	Batch(3700/6809) done. Loss: 0.0607  lr:0.000001
[ Sun Jul 14 05:36:41 2024 ] 	Batch(3800/6809) done. Loss: 0.0270  lr:0.000001
[ Sun Jul 14 05:36:58 2024 ] 	Batch(3900/6809) done. Loss: 0.2669  lr:0.000001
[ Sun Jul 14 05:37:16 2024 ] 
Training: Epoch [35/50], Step [3999], Loss: 0.18452206254005432, Training Accuracy: 97.71875
[ Sun Jul 14 05:37:16 2024 ] 	Batch(4000/6809) done. Loss: 0.0131  lr:0.000001
[ Sun Jul 14 05:37:34 2024 ] 	Batch(4100/6809) done. Loss: 0.1641  lr:0.000001
[ Sun Jul 14 05:37:52 2024 ] 	Batch(4200/6809) done. Loss: 0.0512  lr:0.000001
[ Sun Jul 14 05:38:10 2024 ] 	Batch(4300/6809) done. Loss: 0.1077  lr:0.000001
[ Sun Jul 14 05:38:28 2024 ] 	Batch(4400/6809) done. Loss: 0.0808  lr:0.000001
[ Sun Jul 14 05:38:46 2024 ] 
Training: Epoch [35/50], Step [4499], Loss: 0.14966706931591034, Training Accuracy: 97.71666666666667
[ Sun Jul 14 05:38:46 2024 ] 	Batch(4500/6809) done. Loss: 0.0802  lr:0.000001
[ Sun Jul 14 05:39:04 2024 ] 	Batch(4600/6809) done. Loss: 0.0695  lr:0.000001
[ Sun Jul 14 05:39:22 2024 ] 	Batch(4700/6809) done. Loss: 0.0062  lr:0.000001
[ Sun Jul 14 05:39:40 2024 ] 	Batch(4800/6809) done. Loss: 0.1561  lr:0.000001
[ Sun Jul 14 05:39:59 2024 ] 	Batch(4900/6809) done. Loss: 0.0883  lr:0.000001
[ Sun Jul 14 05:40:17 2024 ] 
Training: Epoch [35/50], Step [4999], Loss: 0.04545733705163002, Training Accuracy: 97.7125
[ Sun Jul 14 05:40:17 2024 ] 	Batch(5000/6809) done. Loss: 0.1709  lr:0.000001
[ Sun Jul 14 05:40:36 2024 ] 	Batch(5100/6809) done. Loss: 0.0539  lr:0.000001
[ Sun Jul 14 05:40:54 2024 ] 	Batch(5200/6809) done. Loss: 0.0270  lr:0.000001
[ Sun Jul 14 05:41:13 2024 ] 	Batch(5300/6809) done. Loss: 0.1481  lr:0.000001
[ Sun Jul 14 05:41:31 2024 ] 	Batch(5400/6809) done. Loss: 0.0570  lr:0.000001
[ Sun Jul 14 05:41:50 2024 ] 
Training: Epoch [35/50], Step [5499], Loss: 0.021513797342777252, Training Accuracy: 97.75
[ Sun Jul 14 05:41:50 2024 ] 	Batch(5500/6809) done. Loss: 0.1677  lr:0.000001
[ Sun Jul 14 05:42:09 2024 ] 	Batch(5600/6809) done. Loss: 0.1044  lr:0.000001
[ Sun Jul 14 05:42:27 2024 ] 	Batch(5700/6809) done. Loss: 0.0335  lr:0.000001
[ Sun Jul 14 05:42:44 2024 ] 	Batch(5800/6809) done. Loss: 0.1191  lr:0.000001
[ Sun Jul 14 05:43:02 2024 ] 	Batch(5900/6809) done. Loss: 0.0205  lr:0.000001
[ Sun Jul 14 05:43:20 2024 ] 
Training: Epoch [35/50], Step [5999], Loss: 0.018705138936638832, Training Accuracy: 97.73958333333333
[ Sun Jul 14 05:43:20 2024 ] 	Batch(6000/6809) done. Loss: 0.0687  lr:0.000001
[ Sun Jul 14 05:43:38 2024 ] 	Batch(6100/6809) done. Loss: 0.0560  lr:0.000001
[ Sun Jul 14 05:43:56 2024 ] 	Batch(6200/6809) done. Loss: 0.0276  lr:0.000001
[ Sun Jul 14 05:44:14 2024 ] 	Batch(6300/6809) done. Loss: 0.0232  lr:0.000001
[ Sun Jul 14 05:44:32 2024 ] 	Batch(6400/6809) done. Loss: 0.0163  lr:0.000001
[ Sun Jul 14 05:44:50 2024 ] 
Training: Epoch [35/50], Step [6499], Loss: 0.016154658049345016, Training Accuracy: 97.74038461538461
[ Sun Jul 14 05:44:50 2024 ] 	Batch(6500/6809) done. Loss: 0.0374  lr:0.000001
[ Sun Jul 14 05:45:08 2024 ] 	Batch(6600/6809) done. Loss: 0.0171  lr:0.000001
[ Sun Jul 14 05:45:26 2024 ] 	Batch(6700/6809) done. Loss: 0.0468  lr:0.000001
[ Sun Jul 14 05:45:44 2024 ] 	Batch(6800/6809) done. Loss: 0.0049  lr:0.000001
[ Sun Jul 14 05:45:45 2024 ] 	Mean training loss: 0.0934.
[ Sun Jul 14 05:45:45 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 05:45:45 2024 ] Training epoch: 37
[ Sun Jul 14 05:45:46 2024 ] 	Batch(0/6809) done. Loss: 0.0113  lr:0.000001
[ Sun Jul 14 05:46:04 2024 ] 	Batch(100/6809) done. Loss: 0.0037  lr:0.000001
[ Sun Jul 14 05:46:22 2024 ] 	Batch(200/6809) done. Loss: 0.0352  lr:0.000001
[ Sun Jul 14 05:46:40 2024 ] 	Batch(300/6809) done. Loss: 0.0216  lr:0.000001
[ Sun Jul 14 05:46:58 2024 ] 	Batch(400/6809) done. Loss: 0.1731  lr:0.000001
[ Sun Jul 14 05:47:16 2024 ] 
Training: Epoch [36/50], Step [499], Loss: 0.12929563224315643, Training Accuracy: 97.95
[ Sun Jul 14 05:47:16 2024 ] 	Batch(500/6809) done. Loss: 0.0218  lr:0.000001
[ Sun Jul 14 05:47:34 2024 ] 	Batch(600/6809) done. Loss: 0.2364  lr:0.000001
[ Sun Jul 14 05:47:51 2024 ] 	Batch(700/6809) done. Loss: 0.1347  lr:0.000001
[ Sun Jul 14 05:48:09 2024 ] 	Batch(800/6809) done. Loss: 0.3686  lr:0.000001
[ Sun Jul 14 05:48:27 2024 ] 	Batch(900/6809) done. Loss: 0.2134  lr:0.000001
[ Sun Jul 14 05:48:45 2024 ] 
Training: Epoch [36/50], Step [999], Loss: 0.2209707349538803, Training Accuracy: 98.0
[ Sun Jul 14 05:48:45 2024 ] 	Batch(1000/6809) done. Loss: 0.0237  lr:0.000001
[ Sun Jul 14 05:49:03 2024 ] 	Batch(1100/6809) done. Loss: 0.0073  lr:0.000001
[ Sun Jul 14 05:49:21 2024 ] 	Batch(1200/6809) done. Loss: 0.0235  lr:0.000001
[ Sun Jul 14 05:49:39 2024 ] 	Batch(1300/6809) done. Loss: 0.0008  lr:0.000001
[ Sun Jul 14 05:49:57 2024 ] 	Batch(1400/6809) done. Loss: 0.0062  lr:0.000001
[ Sun Jul 14 05:50:15 2024 ] 
Training: Epoch [36/50], Step [1499], Loss: 0.011248351074755192, Training Accuracy: 97.95833333333334
[ Sun Jul 14 05:50:15 2024 ] 	Batch(1500/6809) done. Loss: 0.0220  lr:0.000001
[ Sun Jul 14 05:50:33 2024 ] 	Batch(1600/6809) done. Loss: 0.0554  lr:0.000001
[ Sun Jul 14 05:50:51 2024 ] 	Batch(1700/6809) done. Loss: 0.0209  lr:0.000001
[ Sun Jul 14 05:51:09 2024 ] 	Batch(1800/6809) done. Loss: 0.0424  lr:0.000001
[ Sun Jul 14 05:51:27 2024 ] 	Batch(1900/6809) done. Loss: 0.1856  lr:0.000001
[ Sun Jul 14 05:51:44 2024 ] 
Training: Epoch [36/50], Step [1999], Loss: 0.006703490857034922, Training Accuracy: 97.925
[ Sun Jul 14 05:51:45 2024 ] 	Batch(2000/6809) done. Loss: 0.0088  lr:0.000001
[ Sun Jul 14 05:52:03 2024 ] 	Batch(2100/6809) done. Loss: 0.1312  lr:0.000001
[ Sun Jul 14 05:52:22 2024 ] 	Batch(2200/6809) done. Loss: 0.0040  lr:0.000001
[ Sun Jul 14 05:52:40 2024 ] 	Batch(2300/6809) done. Loss: 0.1107  lr:0.000001
[ Sun Jul 14 05:52:58 2024 ] 	Batch(2400/6809) done. Loss: 0.0952  lr:0.000001
[ Sun Jul 14 05:53:16 2024 ] 
Training: Epoch [36/50], Step [2499], Loss: 0.19047191739082336, Training Accuracy: 97.92
[ Sun Jul 14 05:53:16 2024 ] 	Batch(2500/6809) done. Loss: 0.0015  lr:0.000001
[ Sun Jul 14 05:53:34 2024 ] 	Batch(2600/6809) done. Loss: 0.0282  lr:0.000001
[ Sun Jul 14 05:53:52 2024 ] 	Batch(2700/6809) done. Loss: 0.0682  lr:0.000001
[ Sun Jul 14 05:54:10 2024 ] 	Batch(2800/6809) done. Loss: 0.0378  lr:0.000001
[ Sun Jul 14 05:54:28 2024 ] 	Batch(2900/6809) done. Loss: 0.1261  lr:0.000001
[ Sun Jul 14 05:54:46 2024 ] 
Training: Epoch [36/50], Step [2999], Loss: 0.08476879447698593, Training Accuracy: 97.88333333333334
[ Sun Jul 14 05:54:46 2024 ] 	Batch(3000/6809) done. Loss: 0.1510  lr:0.000001
[ Sun Jul 14 05:55:04 2024 ] 	Batch(3100/6809) done. Loss: 0.0507  lr:0.000001
[ Sun Jul 14 05:55:22 2024 ] 	Batch(3200/6809) done. Loss: 0.0396  lr:0.000001
[ Sun Jul 14 05:55:39 2024 ] 	Batch(3300/6809) done. Loss: 0.0213  lr:0.000001
[ Sun Jul 14 05:55:57 2024 ] 	Batch(3400/6809) done. Loss: 0.1529  lr:0.000001
[ Sun Jul 14 05:56:15 2024 ] 
Training: Epoch [36/50], Step [3499], Loss: 0.09660597145557404, Training Accuracy: 97.82499999999999
[ Sun Jul 14 05:56:15 2024 ] 	Batch(3500/6809) done. Loss: 0.3167  lr:0.000001
[ Sun Jul 14 05:56:33 2024 ] 	Batch(3600/6809) done. Loss: 0.1165  lr:0.000001
[ Sun Jul 14 05:56:52 2024 ] 	Batch(3700/6809) done. Loss: 0.0080  lr:0.000001
[ Sun Jul 14 05:57:10 2024 ] 	Batch(3800/6809) done. Loss: 0.0529  lr:0.000001
[ Sun Jul 14 05:57:29 2024 ] 	Batch(3900/6809) done. Loss: 0.0162  lr:0.000001
[ Sun Jul 14 05:57:47 2024 ] 
Training: Epoch [36/50], Step [3999], Loss: 0.009761927649378777, Training Accuracy: 97.84375
[ Sun Jul 14 05:57:48 2024 ] 	Batch(4000/6809) done. Loss: 0.0506  lr:0.000001
[ Sun Jul 14 05:58:06 2024 ] 	Batch(4100/6809) done. Loss: 0.0140  lr:0.000001
[ Sun Jul 14 05:58:23 2024 ] 	Batch(4200/6809) done. Loss: 0.1099  lr:0.000001
[ Sun Jul 14 05:58:41 2024 ] 	Batch(4300/6809) done. Loss: 0.1116  lr:0.000001
[ Sun Jul 14 05:58:59 2024 ] 	Batch(4400/6809) done. Loss: 0.0109  lr:0.000001
[ Sun Jul 14 05:59:17 2024 ] 
Training: Epoch [36/50], Step [4499], Loss: 0.8485642671585083, Training Accuracy: 97.80555555555556
[ Sun Jul 14 05:59:17 2024 ] 	Batch(4500/6809) done. Loss: 0.1041  lr:0.000001
[ Sun Jul 14 05:59:35 2024 ] 	Batch(4600/6809) done. Loss: 0.0115  lr:0.000001
[ Sun Jul 14 05:59:53 2024 ] 	Batch(4700/6809) done. Loss: 0.0098  lr:0.000001
[ Sun Jul 14 06:00:11 2024 ] 	Batch(4800/6809) done. Loss: 0.0217  lr:0.000001
[ Sun Jul 14 06:00:29 2024 ] 	Batch(4900/6809) done. Loss: 0.0232  lr:0.000001
[ Sun Jul 14 06:00:47 2024 ] 
Training: Epoch [36/50], Step [4999], Loss: 0.156175434589386, Training Accuracy: 97.7575
[ Sun Jul 14 06:00:47 2024 ] 	Batch(5000/6809) done. Loss: 0.0284  lr:0.000001
[ Sun Jul 14 06:01:05 2024 ] 	Batch(5100/6809) done. Loss: 0.0220  lr:0.000001
[ Sun Jul 14 06:01:23 2024 ] 	Batch(5200/6809) done. Loss: 0.0291  lr:0.000001
[ Sun Jul 14 06:01:41 2024 ] 	Batch(5300/6809) done. Loss: 0.0101  lr:0.000001
[ Sun Jul 14 06:01:58 2024 ] 	Batch(5400/6809) done. Loss: 0.4147  lr:0.000001
[ Sun Jul 14 06:02:16 2024 ] 
Training: Epoch [36/50], Step [5499], Loss: 0.020463669672608376, Training Accuracy: 97.75454545454545
[ Sun Jul 14 06:02:16 2024 ] 	Batch(5500/6809) done. Loss: 0.0780  lr:0.000001
[ Sun Jul 14 06:02:34 2024 ] 	Batch(5600/6809) done. Loss: 0.1916  lr:0.000001
[ Sun Jul 14 06:02:52 2024 ] 	Batch(5700/6809) done. Loss: 0.0101  lr:0.000001
[ Sun Jul 14 06:03:10 2024 ] 	Batch(5800/6809) done. Loss: 0.1619  lr:0.000001
[ Sun Jul 14 06:03:28 2024 ] 	Batch(5900/6809) done. Loss: 0.0371  lr:0.000001
[ Sun Jul 14 06:03:46 2024 ] 
Training: Epoch [36/50], Step [5999], Loss: 0.2707788646221161, Training Accuracy: 97.775
[ Sun Jul 14 06:03:46 2024 ] 	Batch(6000/6809) done. Loss: 0.0953  lr:0.000001
[ Sun Jul 14 06:04:04 2024 ] 	Batch(6100/6809) done. Loss: 0.0237  lr:0.000001
[ Sun Jul 14 06:04:22 2024 ] 	Batch(6200/6809) done. Loss: 0.0033  lr:0.000001
[ Sun Jul 14 06:04:40 2024 ] 	Batch(6300/6809) done. Loss: 0.3212  lr:0.000001
[ Sun Jul 14 06:04:58 2024 ] 	Batch(6400/6809) done. Loss: 0.0282  lr:0.000001
[ Sun Jul 14 06:05:16 2024 ] 
Training: Epoch [36/50], Step [6499], Loss: 0.7817240953445435, Training Accuracy: 97.76538461538462
[ Sun Jul 14 06:05:16 2024 ] 	Batch(6500/6809) done. Loss: 0.0490  lr:0.000001
[ Sun Jul 14 06:05:34 2024 ] 	Batch(6600/6809) done. Loss: 0.0092  lr:0.000001
[ Sun Jul 14 06:05:52 2024 ] 	Batch(6700/6809) done. Loss: 0.0948  lr:0.000001
[ Sun Jul 14 06:06:10 2024 ] 	Batch(6800/6809) done. Loss: 0.0553  lr:0.000001
[ Sun Jul 14 06:06:11 2024 ] 	Mean training loss: 0.0921.
[ Sun Jul 14 06:06:11 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 06:06:11 2024 ] Training epoch: 38
[ Sun Jul 14 06:06:12 2024 ] 	Batch(0/6809) done. Loss: 0.1370  lr:0.000001
[ Sun Jul 14 06:06:30 2024 ] 	Batch(100/6809) done. Loss: 0.0763  lr:0.000001
[ Sun Jul 14 06:06:49 2024 ] 	Batch(200/6809) done. Loss: 0.0935  lr:0.000001
[ Sun Jul 14 06:07:07 2024 ] 	Batch(300/6809) done. Loss: 0.0461  lr:0.000001
[ Sun Jul 14 06:07:25 2024 ] 	Batch(400/6809) done. Loss: 0.0149  lr:0.000001
[ Sun Jul 14 06:07:43 2024 ] 
Training: Epoch [37/50], Step [499], Loss: 0.032413300126791, Training Accuracy: 97.32499999999999
[ Sun Jul 14 06:07:44 2024 ] 	Batch(500/6809) done. Loss: 0.0385  lr:0.000001
[ Sun Jul 14 06:08:02 2024 ] 	Batch(600/6809) done. Loss: 0.0269  lr:0.000001
[ Sun Jul 14 06:08:20 2024 ] 	Batch(700/6809) done. Loss: 0.0387  lr:0.000001
[ Sun Jul 14 06:08:38 2024 ] 	Batch(800/6809) done. Loss: 0.1661  lr:0.000001
[ Sun Jul 14 06:08:56 2024 ] 	Batch(900/6809) done. Loss: 0.1183  lr:0.000001
[ Sun Jul 14 06:09:14 2024 ] 
Training: Epoch [37/50], Step [999], Loss: 0.048467494547367096, Training Accuracy: 97.5375
[ Sun Jul 14 06:09:14 2024 ] 	Batch(1000/6809) done. Loss: 0.0249  lr:0.000001
[ Sun Jul 14 06:09:32 2024 ] 	Batch(1100/6809) done. Loss: 0.0830  lr:0.000001
[ Sun Jul 14 06:09:51 2024 ] 	Batch(1200/6809) done. Loss: 0.0477  lr:0.000001
[ Sun Jul 14 06:10:09 2024 ] 	Batch(1300/6809) done. Loss: 0.1622  lr:0.000001
[ Sun Jul 14 06:10:27 2024 ] 	Batch(1400/6809) done. Loss: 0.2760  lr:0.000001
[ Sun Jul 14 06:10:45 2024 ] 
Training: Epoch [37/50], Step [1499], Loss: 0.042414430528879166, Training Accuracy: 97.64166666666667
[ Sun Jul 14 06:10:45 2024 ] 	Batch(1500/6809) done. Loss: 0.0044  lr:0.000001
[ Sun Jul 14 06:11:03 2024 ] 	Batch(1600/6809) done. Loss: 0.0273  lr:0.000001
[ Sun Jul 14 06:11:21 2024 ] 	Batch(1700/6809) done. Loss: 0.1016  lr:0.000001
[ Sun Jul 14 06:11:39 2024 ] 	Batch(1800/6809) done. Loss: 0.5999  lr:0.000001
[ Sun Jul 14 06:11:57 2024 ] 	Batch(1900/6809) done. Loss: 0.0224  lr:0.000001
[ Sun Jul 14 06:12:15 2024 ] 
Training: Epoch [37/50], Step [1999], Loss: 0.09017248451709747, Training Accuracy: 97.63125000000001
[ Sun Jul 14 06:12:15 2024 ] 	Batch(2000/6809) done. Loss: 0.1066  lr:0.000001
[ Sun Jul 14 06:12:33 2024 ] 	Batch(2100/6809) done. Loss: 0.2335  lr:0.000001
[ Sun Jul 14 06:12:51 2024 ] 	Batch(2200/6809) done. Loss: 0.1588  lr:0.000001
[ Sun Jul 14 06:13:09 2024 ] 	Batch(2300/6809) done. Loss: 0.0979  lr:0.000001
[ Sun Jul 14 06:13:28 2024 ] 	Batch(2400/6809) done. Loss: 0.0959  lr:0.000001
[ Sun Jul 14 06:13:45 2024 ] 
Training: Epoch [37/50], Step [2499], Loss: 0.0666753426194191, Training Accuracy: 97.655
[ Sun Jul 14 06:13:46 2024 ] 	Batch(2500/6809) done. Loss: 0.0513  lr:0.000001
[ Sun Jul 14 06:14:04 2024 ] 	Batch(2600/6809) done. Loss: 0.2171  lr:0.000001
[ Sun Jul 14 06:14:21 2024 ] 	Batch(2700/6809) done. Loss: 0.1848  lr:0.000001
[ Sun Jul 14 06:14:40 2024 ] 	Batch(2800/6809) done. Loss: 0.1400  lr:0.000001
[ Sun Jul 14 06:14:58 2024 ] 	Batch(2900/6809) done. Loss: 0.0072  lr:0.000001
[ Sun Jul 14 06:15:16 2024 ] 
Training: Epoch [37/50], Step [2999], Loss: 0.028581174090504646, Training Accuracy: 97.6
[ Sun Jul 14 06:15:16 2024 ] 	Batch(3000/6809) done. Loss: 0.0582  lr:0.000001
[ Sun Jul 14 06:15:34 2024 ] 	Batch(3100/6809) done. Loss: 0.0479  lr:0.000001
[ Sun Jul 14 06:15:53 2024 ] 	Batch(3200/6809) done. Loss: 0.1780  lr:0.000001
[ Sun Jul 14 06:16:11 2024 ] 	Batch(3300/6809) done. Loss: 0.0995  lr:0.000001
[ Sun Jul 14 06:16:29 2024 ] 	Batch(3400/6809) done. Loss: 0.0569  lr:0.000001
[ Sun Jul 14 06:16:47 2024 ] 
Training: Epoch [37/50], Step [3499], Loss: 0.20801514387130737, Training Accuracy: 97.60714285714286
[ Sun Jul 14 06:16:47 2024 ] 	Batch(3500/6809) done. Loss: 0.0335  lr:0.000001
[ Sun Jul 14 06:17:06 2024 ] 	Batch(3600/6809) done. Loss: 0.3764  lr:0.000001
[ Sun Jul 14 06:17:24 2024 ] 	Batch(3700/6809) done. Loss: 0.0023  lr:0.000001
[ Sun Jul 14 06:17:42 2024 ] 	Batch(3800/6809) done. Loss: 0.0539  lr:0.000001
[ Sun Jul 14 06:18:00 2024 ] 	Batch(3900/6809) done. Loss: 0.1005  lr:0.000001
[ Sun Jul 14 06:18:18 2024 ] 
Training: Epoch [37/50], Step [3999], Loss: 0.11461996287107468, Training Accuracy: 97.63125000000001
[ Sun Jul 14 06:18:18 2024 ] 	Batch(4000/6809) done. Loss: 0.2351  lr:0.000001
[ Sun Jul 14 06:18:36 2024 ] 	Batch(4100/6809) done. Loss: 0.1373  lr:0.000001
[ Sun Jul 14 06:18:54 2024 ] 	Batch(4200/6809) done. Loss: 0.0613  lr:0.000001
[ Sun Jul 14 06:19:12 2024 ] 	Batch(4300/6809) done. Loss: 0.1525  lr:0.000001
[ Sun Jul 14 06:19:30 2024 ] 	Batch(4400/6809) done. Loss: 0.0032  lr:0.000001
[ Sun Jul 14 06:19:48 2024 ] 
Training: Epoch [37/50], Step [4499], Loss: 0.017705393955111504, Training Accuracy: 97.62222222222222
[ Sun Jul 14 06:19:48 2024 ] 	Batch(4500/6809) done. Loss: 0.0242  lr:0.000001
[ Sun Jul 14 06:20:07 2024 ] 	Batch(4600/6809) done. Loss: 0.2074  lr:0.000001
[ Sun Jul 14 06:20:26 2024 ] 	Batch(4700/6809) done. Loss: 0.1639  lr:0.000001
[ Sun Jul 14 06:20:44 2024 ] 	Batch(4800/6809) done. Loss: 0.0008  lr:0.000001
[ Sun Jul 14 06:21:02 2024 ] 	Batch(4900/6809) done. Loss: 0.0865  lr:0.000001
[ Sun Jul 14 06:21:20 2024 ] 
Training: Epoch [37/50], Step [4999], Loss: 0.12148917466402054, Training Accuracy: 97.6225
[ Sun Jul 14 06:21:20 2024 ] 	Batch(5000/6809) done. Loss: 0.1835  lr:0.000001
[ Sun Jul 14 06:21:38 2024 ] 	Batch(5100/6809) done. Loss: 0.0658  lr:0.000001
[ Sun Jul 14 06:21:56 2024 ] 	Batch(5200/6809) done. Loss: 0.0321  lr:0.000001
[ Sun Jul 14 06:22:14 2024 ] 	Batch(5300/6809) done. Loss: 0.0901  lr:0.000001
[ Sun Jul 14 06:22:32 2024 ] 	Batch(5400/6809) done. Loss: 0.0114  lr:0.000001
[ Sun Jul 14 06:22:49 2024 ] 
Training: Epoch [37/50], Step [5499], Loss: 0.02635805308818817, Training Accuracy: 97.62954545454545
[ Sun Jul 14 06:22:50 2024 ] 	Batch(5500/6809) done. Loss: 0.2493  lr:0.000001
[ Sun Jul 14 06:23:08 2024 ] 	Batch(5600/6809) done. Loss: 0.0221  lr:0.000001
[ Sun Jul 14 06:23:26 2024 ] 	Batch(5700/6809) done. Loss: 0.1671  lr:0.000001
[ Sun Jul 14 06:23:43 2024 ] 	Batch(5800/6809) done. Loss: 0.0134  lr:0.000001
[ Sun Jul 14 06:24:01 2024 ] 	Batch(5900/6809) done. Loss: 0.0352  lr:0.000001
[ Sun Jul 14 06:24:20 2024 ] 
Training: Epoch [37/50], Step [5999], Loss: 0.004094168543815613, Training Accuracy: 97.62708333333333
[ Sun Jul 14 06:24:20 2024 ] 	Batch(6000/6809) done. Loss: 0.0039  lr:0.000001
[ Sun Jul 14 06:24:39 2024 ] 	Batch(6100/6809) done. Loss: 0.0071  lr:0.000001
[ Sun Jul 14 06:24:57 2024 ] 	Batch(6200/6809) done. Loss: 0.0655  lr:0.000001
[ Sun Jul 14 06:25:16 2024 ] 	Batch(6300/6809) done. Loss: 0.0156  lr:0.000001
[ Sun Jul 14 06:25:34 2024 ] 	Batch(6400/6809) done. Loss: 0.0031  lr:0.000001
[ Sun Jul 14 06:25:52 2024 ] 
Training: Epoch [37/50], Step [6499], Loss: 0.04831138253211975, Training Accuracy: 97.62884615384615
[ Sun Jul 14 06:25:52 2024 ] 	Batch(6500/6809) done. Loss: 0.0068  lr:0.000001
[ Sun Jul 14 06:26:11 2024 ] 	Batch(6600/6809) done. Loss: 0.1374  lr:0.000001
[ Sun Jul 14 06:26:29 2024 ] 	Batch(6700/6809) done. Loss: 0.0688  lr:0.000001
[ Sun Jul 14 06:26:48 2024 ] 	Batch(6800/6809) done. Loss: 0.0321  lr:0.000001
[ Sun Jul 14 06:26:50 2024 ] 	Mean training loss: 0.0932.
[ Sun Jul 14 06:26:50 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 06:26:50 2024 ] Training epoch: 39
[ Sun Jul 14 06:26:50 2024 ] 	Batch(0/6809) done. Loss: 0.0069  lr:0.000001
[ Sun Jul 14 06:27:08 2024 ] 	Batch(100/6809) done. Loss: 0.0141  lr:0.000001
[ Sun Jul 14 06:27:26 2024 ] 	Batch(200/6809) done. Loss: 0.0919  lr:0.000001
[ Sun Jul 14 06:27:44 2024 ] 	Batch(300/6809) done. Loss: 0.0849  lr:0.000001
[ Sun Jul 14 06:28:02 2024 ] 	Batch(400/6809) done. Loss: 0.0374  lr:0.000001
[ Sun Jul 14 06:28:20 2024 ] 
Training: Epoch [38/50], Step [499], Loss: 0.10584346204996109, Training Accuracy: 97.875
[ Sun Jul 14 06:28:20 2024 ] 	Batch(500/6809) done. Loss: 0.0426  lr:0.000001
[ Sun Jul 14 06:28:38 2024 ] 	Batch(600/6809) done. Loss: 0.3780  lr:0.000001
[ Sun Jul 14 06:28:56 2024 ] 	Batch(700/6809) done. Loss: 0.0152  lr:0.000001
[ Sun Jul 14 06:29:14 2024 ] 	Batch(800/6809) done. Loss: 0.2044  lr:0.000001
[ Sun Jul 14 06:29:32 2024 ] 	Batch(900/6809) done. Loss: 0.0025  lr:0.000001
[ Sun Jul 14 06:29:50 2024 ] 
Training: Epoch [38/50], Step [999], Loss: 0.00844662543386221, Training Accuracy: 97.8
[ Sun Jul 14 06:29:51 2024 ] 	Batch(1000/6809) done. Loss: 0.0295  lr:0.000001
[ Sun Jul 14 06:30:09 2024 ] 	Batch(1100/6809) done. Loss: 0.1252  lr:0.000001
[ Sun Jul 14 06:30:28 2024 ] 	Batch(1200/6809) done. Loss: 0.0949  lr:0.000001
[ Sun Jul 14 06:30:46 2024 ] 	Batch(1300/6809) done. Loss: 0.6416  lr:0.000001
[ Sun Jul 14 06:31:05 2024 ] 	Batch(1400/6809) done. Loss: 0.0381  lr:0.000001
[ Sun Jul 14 06:31:23 2024 ] 
Training: Epoch [38/50], Step [1499], Loss: 0.009829056449234486, Training Accuracy: 97.88333333333334
[ Sun Jul 14 06:31:23 2024 ] 	Batch(1500/6809) done. Loss: 0.0941  lr:0.000001
[ Sun Jul 14 06:31:42 2024 ] 	Batch(1600/6809) done. Loss: 0.1157  lr:0.000001
[ Sun Jul 14 06:32:00 2024 ] 	Batch(1700/6809) done. Loss: 0.1208  lr:0.000001
[ Sun Jul 14 06:32:18 2024 ] 	Batch(1800/6809) done. Loss: 0.2731  lr:0.000001
[ Sun Jul 14 06:32:36 2024 ] 	Batch(1900/6809) done. Loss: 0.0512  lr:0.000001
[ Sun Jul 14 06:32:53 2024 ] 
Training: Epoch [38/50], Step [1999], Loss: 0.09250515699386597, Training Accuracy: 97.925
[ Sun Jul 14 06:32:54 2024 ] 	Batch(2000/6809) done. Loss: 0.0390  lr:0.000001
[ Sun Jul 14 06:33:12 2024 ] 	Batch(2100/6809) done. Loss: 0.0122  lr:0.000001
[ Sun Jul 14 06:33:30 2024 ] 	Batch(2200/6809) done. Loss: 0.0146  lr:0.000001
[ Sun Jul 14 06:33:47 2024 ] 	Batch(2300/6809) done. Loss: 0.1870  lr:0.000001
[ Sun Jul 14 06:34:05 2024 ] 	Batch(2400/6809) done. Loss: 0.0142  lr:0.000001
[ Sun Jul 14 06:34:23 2024 ] 
Training: Epoch [38/50], Step [2499], Loss: 0.04691707715392113, Training Accuracy: 97.89
[ Sun Jul 14 06:34:23 2024 ] 	Batch(2500/6809) done. Loss: 0.2372  lr:0.000001
[ Sun Jul 14 06:34:41 2024 ] 	Batch(2600/6809) done. Loss: 0.0843  lr:0.000001
[ Sun Jul 14 06:34:59 2024 ] 	Batch(2700/6809) done. Loss: 0.0271  lr:0.000001
[ Sun Jul 14 06:35:17 2024 ] 	Batch(2800/6809) done. Loss: 0.0143  lr:0.000001
[ Sun Jul 14 06:35:35 2024 ] 	Batch(2900/6809) done. Loss: 0.0458  lr:0.000001
[ Sun Jul 14 06:35:53 2024 ] 
Training: Epoch [38/50], Step [2999], Loss: 0.1107523962855339, Training Accuracy: 97.88333333333334
[ Sun Jul 14 06:35:53 2024 ] 	Batch(3000/6809) done. Loss: 0.0163  lr:0.000001
[ Sun Jul 14 06:36:11 2024 ] 	Batch(3100/6809) done. Loss: 0.0093  lr:0.000001
[ Sun Jul 14 06:36:29 2024 ] 	Batch(3200/6809) done. Loss: 0.0050  lr:0.000001
[ Sun Jul 14 06:36:47 2024 ] 	Batch(3300/6809) done. Loss: 0.0381  lr:0.000001
[ Sun Jul 14 06:37:05 2024 ] 	Batch(3400/6809) done. Loss: 0.0155  lr:0.000001
[ Sun Jul 14 06:37:23 2024 ] 
Training: Epoch [38/50], Step [3499], Loss: 0.12050071358680725, Training Accuracy: 97.91428571428571
[ Sun Jul 14 06:37:23 2024 ] 	Batch(3500/6809) done. Loss: 0.0077  lr:0.000001
[ Sun Jul 14 06:37:41 2024 ] 	Batch(3600/6809) done. Loss: 0.0519  lr:0.000001
[ Sun Jul 14 06:37:59 2024 ] 	Batch(3700/6809) done. Loss: 0.0290  lr:0.000001
[ Sun Jul 14 06:38:17 2024 ] 	Batch(3800/6809) done. Loss: 0.0089  lr:0.000001
[ Sun Jul 14 06:38:35 2024 ] 	Batch(3900/6809) done. Loss: 0.0114  lr:0.000001
[ Sun Jul 14 06:38:52 2024 ] 
Training: Epoch [38/50], Step [3999], Loss: 0.08611655235290527, Training Accuracy: 97.89375
[ Sun Jul 14 06:38:53 2024 ] 	Batch(4000/6809) done. Loss: 0.0141  lr:0.000001
[ Sun Jul 14 06:39:11 2024 ] 	Batch(4100/6809) done. Loss: 0.3285  lr:0.000001
[ Sun Jul 14 06:39:28 2024 ] 	Batch(4200/6809) done. Loss: 0.1714  lr:0.000001
[ Sun Jul 14 06:39:46 2024 ] 	Batch(4300/6809) done. Loss: 0.0251  lr:0.000001
[ Sun Jul 14 06:40:04 2024 ] 	Batch(4400/6809) done. Loss: 0.1182  lr:0.000001
[ Sun Jul 14 06:40:22 2024 ] 
Training: Epoch [38/50], Step [4499], Loss: 0.13066719472408295, Training Accuracy: 97.86944444444444
[ Sun Jul 14 06:40:22 2024 ] 	Batch(4500/6809) done. Loss: 0.0262  lr:0.000001
[ Sun Jul 14 06:40:40 2024 ] 	Batch(4600/6809) done. Loss: 0.0656  lr:0.000001
[ Sun Jul 14 06:40:58 2024 ] 	Batch(4700/6809) done. Loss: 0.0419  lr:0.000001
[ Sun Jul 14 06:41:16 2024 ] 	Batch(4800/6809) done. Loss: 0.0271  lr:0.000001
[ Sun Jul 14 06:41:34 2024 ] 	Batch(4900/6809) done. Loss: 0.0123  lr:0.000001
[ Sun Jul 14 06:41:52 2024 ] 
Training: Epoch [38/50], Step [4999], Loss: 0.039866853505373, Training Accuracy: 97.87
[ Sun Jul 14 06:41:52 2024 ] 	Batch(5000/6809) done. Loss: 0.0469  lr:0.000001
[ Sun Jul 14 06:42:10 2024 ] 	Batch(5100/6809) done. Loss: 0.0413  lr:0.000001
[ Sun Jul 14 06:42:28 2024 ] 	Batch(5200/6809) done. Loss: 0.1401  lr:0.000001
[ Sun Jul 14 06:42:46 2024 ] 	Batch(5300/6809) done. Loss: 0.0035  lr:0.000001
[ Sun Jul 14 06:43:04 2024 ] 	Batch(5400/6809) done. Loss: 0.0191  lr:0.000001
[ Sun Jul 14 06:43:22 2024 ] 
Training: Epoch [38/50], Step [5499], Loss: 0.00817046593874693, Training Accuracy: 97.82272727272728
[ Sun Jul 14 06:43:22 2024 ] 	Batch(5500/6809) done. Loss: 0.0416  lr:0.000001
[ Sun Jul 14 06:43:40 2024 ] 	Batch(5600/6809) done. Loss: 0.0064  lr:0.000001
[ Sun Jul 14 06:43:58 2024 ] 	Batch(5700/6809) done. Loss: 0.3209  lr:0.000001
[ Sun Jul 14 06:44:16 2024 ] 	Batch(5800/6809) done. Loss: 0.0440  lr:0.000001
[ Sun Jul 14 06:44:34 2024 ] 	Batch(5900/6809) done. Loss: 0.0843  lr:0.000001
[ Sun Jul 14 06:44:52 2024 ] 
Training: Epoch [38/50], Step [5999], Loss: 0.00034344932646490633, Training Accuracy: 97.8375
[ Sun Jul 14 06:44:52 2024 ] 	Batch(6000/6809) done. Loss: 0.0249  lr:0.000001
[ Sun Jul 14 06:45:11 2024 ] 	Batch(6100/6809) done. Loss: 0.0171  lr:0.000001
[ Sun Jul 14 06:45:29 2024 ] 	Batch(6200/6809) done. Loss: 0.0145  lr:0.000001
[ Sun Jul 14 06:45:48 2024 ] 	Batch(6300/6809) done. Loss: 0.0479  lr:0.000001
[ Sun Jul 14 06:46:06 2024 ] 	Batch(6400/6809) done. Loss: 0.0443  lr:0.000001
[ Sun Jul 14 06:46:25 2024 ] 
Training: Epoch [38/50], Step [6499], Loss: 0.048477161675691605, Training Accuracy: 97.84615384615385
[ Sun Jul 14 06:46:25 2024 ] 	Batch(6500/6809) done. Loss: 0.0227  lr:0.000001
[ Sun Jul 14 06:46:43 2024 ] 	Batch(6600/6809) done. Loss: 0.0666  lr:0.000001
[ Sun Jul 14 06:47:02 2024 ] 	Batch(6700/6809) done. Loss: 0.2263  lr:0.000001
[ Sun Jul 14 06:47:21 2024 ] 	Batch(6800/6809) done. Loss: 0.1248  lr:0.000001
[ Sun Jul 14 06:47:22 2024 ] 	Mean training loss: 0.0902.
[ Sun Jul 14 06:47:22 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 06:47:22 2024 ] Training epoch: 40
[ Sun Jul 14 06:47:23 2024 ] 	Batch(0/6809) done. Loss: 0.0969  lr:0.000001
[ Sun Jul 14 06:47:41 2024 ] 	Batch(100/6809) done. Loss: 0.0463  lr:0.000001
[ Sun Jul 14 06:47:59 2024 ] 	Batch(200/6809) done. Loss: 0.1390  lr:0.000001
[ Sun Jul 14 06:48:17 2024 ] 	Batch(300/6809) done. Loss: 0.0083  lr:0.000001
[ Sun Jul 14 06:48:35 2024 ] 	Batch(400/6809) done. Loss: 0.1362  lr:0.000001
[ Sun Jul 14 06:48:53 2024 ] 
Training: Epoch [39/50], Step [499], Loss: 0.21533475816249847, Training Accuracy: 97.625
[ Sun Jul 14 06:48:53 2024 ] 	Batch(500/6809) done. Loss: 0.0715  lr:0.000001
[ Sun Jul 14 06:49:11 2024 ] 	Batch(600/6809) done. Loss: 0.0209  lr:0.000001
[ Sun Jul 14 06:49:29 2024 ] 	Batch(700/6809) done. Loss: 0.0690  lr:0.000001
[ Sun Jul 14 06:49:47 2024 ] 	Batch(800/6809) done. Loss: 0.0253  lr:0.000001
[ Sun Jul 14 06:50:04 2024 ] 	Batch(900/6809) done. Loss: 0.0146  lr:0.000001
[ Sun Jul 14 06:50:22 2024 ] 
Training: Epoch [39/50], Step [999], Loss: 0.38535740971565247, Training Accuracy: 97.6625
[ Sun Jul 14 06:50:22 2024 ] 	Batch(1000/6809) done. Loss: 0.0144  lr:0.000001
[ Sun Jul 14 06:50:40 2024 ] 	Batch(1100/6809) done. Loss: 0.0893  lr:0.000001
[ Sun Jul 14 06:50:58 2024 ] 	Batch(1200/6809) done. Loss: 0.0392  lr:0.000001
[ Sun Jul 14 06:51:16 2024 ] 	Batch(1300/6809) done. Loss: 0.2192  lr:0.000001
[ Sun Jul 14 06:51:34 2024 ] 	Batch(1400/6809) done. Loss: 0.0595  lr:0.000001
[ Sun Jul 14 06:51:52 2024 ] 
Training: Epoch [39/50], Step [1499], Loss: 0.03837781026959419, Training Accuracy: 97.71666666666667
[ Sun Jul 14 06:51:52 2024 ] 	Batch(1500/6809) done. Loss: 0.0449  lr:0.000001
[ Sun Jul 14 06:52:10 2024 ] 	Batch(1600/6809) done. Loss: 0.0613  lr:0.000001
[ Sun Jul 14 06:52:28 2024 ] 	Batch(1700/6809) done. Loss: 0.1064  lr:0.000001
[ Sun Jul 14 06:52:46 2024 ] 	Batch(1800/6809) done. Loss: 0.0073  lr:0.000001
[ Sun Jul 14 06:53:04 2024 ] 	Batch(1900/6809) done. Loss: 0.0607  lr:0.000001
[ Sun Jul 14 06:53:22 2024 ] 
Training: Epoch [39/50], Step [1999], Loss: 0.00824390072375536, Training Accuracy: 97.675
[ Sun Jul 14 06:53:22 2024 ] 	Batch(2000/6809) done. Loss: 0.0181  lr:0.000001
[ Sun Jul 14 06:53:40 2024 ] 	Batch(2100/6809) done. Loss: 0.0664  lr:0.000001
[ Sun Jul 14 06:53:58 2024 ] 	Batch(2200/6809) done. Loss: 0.0374  lr:0.000001
[ Sun Jul 14 06:54:16 2024 ] 	Batch(2300/6809) done. Loss: 0.3117  lr:0.000001
[ Sun Jul 14 06:54:34 2024 ] 	Batch(2400/6809) done. Loss: 0.1560  lr:0.000001
[ Sun Jul 14 06:54:52 2024 ] 
Training: Epoch [39/50], Step [2499], Loss: 0.07494962960481644, Training Accuracy: 97.64500000000001
[ Sun Jul 14 06:54:52 2024 ] 	Batch(2500/6809) done. Loss: 0.0055  lr:0.000001
[ Sun Jul 14 06:55:10 2024 ] 	Batch(2600/6809) done. Loss: 0.5567  lr:0.000001
[ Sun Jul 14 06:55:28 2024 ] 	Batch(2700/6809) done. Loss: 0.0894  lr:0.000001
[ Sun Jul 14 06:55:46 2024 ] 	Batch(2800/6809) done. Loss: 0.0437  lr:0.000001
[ Sun Jul 14 06:56:04 2024 ] 	Batch(2900/6809) done. Loss: 0.0088  lr:0.000001
[ Sun Jul 14 06:56:22 2024 ] 
Training: Epoch [39/50], Step [2999], Loss: 0.02320600487291813, Training Accuracy: 97.70416666666667
[ Sun Jul 14 06:56:22 2024 ] 	Batch(3000/6809) done. Loss: 0.0612  lr:0.000001
[ Sun Jul 14 06:56:40 2024 ] 	Batch(3100/6809) done. Loss: 0.0444  lr:0.000001
[ Sun Jul 14 06:56:58 2024 ] 	Batch(3200/6809) done. Loss: 0.0176  lr:0.000001
[ Sun Jul 14 06:57:17 2024 ] 	Batch(3300/6809) done. Loss: 0.0059  lr:0.000001
[ Sun Jul 14 06:57:35 2024 ] 	Batch(3400/6809) done. Loss: 0.0281  lr:0.000001
[ Sun Jul 14 06:57:54 2024 ] 
Training: Epoch [39/50], Step [3499], Loss: 0.08706749230623245, Training Accuracy: 97.75714285714285
[ Sun Jul 14 06:57:54 2024 ] 	Batch(3500/6809) done. Loss: 0.0419  lr:0.000001
[ Sun Jul 14 06:58:13 2024 ] 	Batch(3600/6809) done. Loss: 0.0168  lr:0.000001
[ Sun Jul 14 06:58:30 2024 ] 	Batch(3700/6809) done. Loss: 0.0608  lr:0.000001
[ Sun Jul 14 06:58:48 2024 ] 	Batch(3800/6809) done. Loss: 0.3350  lr:0.000001
[ Sun Jul 14 06:59:06 2024 ] 	Batch(3900/6809) done. Loss: 0.0232  lr:0.000001
[ Sun Jul 14 06:59:24 2024 ] 
Training: Epoch [39/50], Step [3999], Loss: 0.004560902714729309, Training Accuracy: 97.728125
[ Sun Jul 14 06:59:24 2024 ] 	Batch(4000/6809) done. Loss: 0.0533  lr:0.000001
[ Sun Jul 14 06:59:43 2024 ] 	Batch(4100/6809) done. Loss: 0.1526  lr:0.000001
[ Sun Jul 14 07:00:01 2024 ] 	Batch(4200/6809) done. Loss: 0.0036  lr:0.000001
[ Sun Jul 14 07:00:20 2024 ] 	Batch(4300/6809) done. Loss: 0.2485  lr:0.000001
[ Sun Jul 14 07:00:39 2024 ] 	Batch(4400/6809) done. Loss: 0.0847  lr:0.000001
[ Sun Jul 14 07:00:56 2024 ] 
Training: Epoch [39/50], Step [4499], Loss: 0.047936633229255676, Training Accuracy: 97.73888888888888
[ Sun Jul 14 07:00:57 2024 ] 	Batch(4500/6809) done. Loss: 0.0279  lr:0.000001
[ Sun Jul 14 07:01:15 2024 ] 	Batch(4600/6809) done. Loss: 0.0068  lr:0.000001
[ Sun Jul 14 07:01:32 2024 ] 	Batch(4700/6809) done. Loss: 0.1733  lr:0.000001
[ Sun Jul 14 07:01:51 2024 ] 	Batch(4800/6809) done. Loss: 0.0078  lr:0.000001
[ Sun Jul 14 07:02:08 2024 ] 	Batch(4900/6809) done. Loss: 0.0085  lr:0.000001
[ Sun Jul 14 07:02:26 2024 ] 
Training: Epoch [39/50], Step [4999], Loss: 0.24503542482852936, Training Accuracy: 97.745
[ Sun Jul 14 07:02:26 2024 ] 	Batch(5000/6809) done. Loss: 0.0263  lr:0.000001
[ Sun Jul 14 07:02:44 2024 ] 	Batch(5100/6809) done. Loss: 0.0079  lr:0.000001
[ Sun Jul 14 07:03:02 2024 ] 	Batch(5200/6809) done. Loss: 0.0488  lr:0.000001
[ Sun Jul 14 07:03:21 2024 ] 	Batch(5300/6809) done. Loss: 0.0131  lr:0.000001
[ Sun Jul 14 07:03:39 2024 ] 	Batch(5400/6809) done. Loss: 0.0374  lr:0.000001
[ Sun Jul 14 07:03:57 2024 ] 
Training: Epoch [39/50], Step [5499], Loss: 0.07924450188875198, Training Accuracy: 97.725
[ Sun Jul 14 07:03:57 2024 ] 	Batch(5500/6809) done. Loss: 0.0129  lr:0.000001
[ Sun Jul 14 07:04:15 2024 ] 	Batch(5600/6809) done. Loss: 0.0849  lr:0.000001
[ Sun Jul 14 07:04:33 2024 ] 	Batch(5700/6809) done. Loss: 0.2258  lr:0.000001
[ Sun Jul 14 07:04:51 2024 ] 	Batch(5800/6809) done. Loss: 0.0090  lr:0.000001
[ Sun Jul 14 07:05:09 2024 ] 	Batch(5900/6809) done. Loss: 0.0472  lr:0.000001
[ Sun Jul 14 07:05:27 2024 ] 
Training: Epoch [39/50], Step [5999], Loss: 0.12103553861379623, Training Accuracy: 97.70625
[ Sun Jul 14 07:05:27 2024 ] 	Batch(6000/6809) done. Loss: 0.1754  lr:0.000001
[ Sun Jul 14 07:05:45 2024 ] 	Batch(6100/6809) done. Loss: 0.0829  lr:0.000001
[ Sun Jul 14 07:06:03 2024 ] 	Batch(6200/6809) done. Loss: 0.0416  lr:0.000001
[ Sun Jul 14 07:06:21 2024 ] 	Batch(6300/6809) done. Loss: 0.0117  lr:0.000001
[ Sun Jul 14 07:06:39 2024 ] 	Batch(6400/6809) done. Loss: 0.0698  lr:0.000001
[ Sun Jul 14 07:06:57 2024 ] 
Training: Epoch [39/50], Step [6499], Loss: 0.05037612095475197, Training Accuracy: 97.70769230769231
[ Sun Jul 14 07:06:57 2024 ] 	Batch(6500/6809) done. Loss: 0.2038  lr:0.000001
[ Sun Jul 14 07:07:16 2024 ] 	Batch(6600/6809) done. Loss: 0.0732  lr:0.000001
[ Sun Jul 14 07:07:34 2024 ] 	Batch(6700/6809) done. Loss: 0.0502  lr:0.000001
[ Sun Jul 14 07:07:52 2024 ] 	Batch(6800/6809) done. Loss: 0.2045  lr:0.000001
[ Sun Jul 14 07:07:54 2024 ] 	Mean training loss: 0.0931.
[ Sun Jul 14 07:07:54 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 07:07:54 2024 ] Eval epoch: 40
[ Sun Jul 14 07:13:27 2024 ] 	Mean val loss of 7435 batches: 1.0719086044435624.
[ Sun Jul 14 07:13:27 2024 ] 
Validation: Epoch [39/50], Samples [47728.0/59477], Loss: 0.65375816822052, Validation Accuracy: 80.24614556887536
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 1 : 373 / 500 = 74 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 2 : 433 / 499 = 86 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 3 : 411 / 500 = 82 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 4 : 420 / 502 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 5 : 467 / 502 = 93 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 6 : 433 / 502 = 86 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 7 : 470 / 497 = 94 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 8 : 481 / 498 = 96 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 9 : 382 / 500 = 76 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 10 : 209 / 500 = 41 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 11 : 201 / 498 = 40 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 12 : 423 / 499 = 84 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 13 : 483 / 502 = 96 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 14 : 481 / 504 = 95 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 15 : 412 / 502 = 82 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 16 : 394 / 502 = 78 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 17 : 436 / 504 = 86 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 18 : 430 / 504 = 85 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 19 : 459 / 502 = 91 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 20 : 456 / 502 = 90 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 21 : 473 / 503 = 94 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 22 : 436 / 504 = 86 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 23 : 440 / 503 = 87 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 24 : 405 / 504 = 80 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 25 : 489 / 504 = 97 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 26 : 465 / 504 = 92 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 27 : 419 / 501 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 28 : 358 / 502 = 71 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 29 : 318 / 502 = 63 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 30 : 335 / 501 = 66 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 31 : 417 / 504 = 82 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 32 : 419 / 503 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 33 : 431 / 503 = 85 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 34 : 478 / 504 = 94 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 35 : 467 / 503 = 92 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 36 : 410 / 502 = 81 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 37 : 432 / 504 = 85 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 38 : 425 / 504 = 84 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 39 : 459 / 498 = 92 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 40 : 391 / 504 = 77 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 41 : 484 / 503 = 96 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 42 : 463 / 504 = 91 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 43 : 328 / 503 = 65 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 44 : 452 / 504 = 89 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 45 : 426 / 504 = 84 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 46 : 423 / 504 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 47 : 406 / 503 = 80 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 48 : 435 / 503 = 86 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 49 : 385 / 499 = 77 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 50 : 419 / 502 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 51 : 468 / 503 = 93 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 52 : 442 / 504 = 87 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 53 : 428 / 497 = 86 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 54 : 457 / 480 = 95 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 55 : 393 / 504 = 77 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 56 : 410 / 503 = 81 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 57 : 482 / 504 = 95 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 58 : 482 / 499 = 96 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 59 : 489 / 503 = 97 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 60 : 411 / 479 = 85 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 61 : 396 / 484 = 81 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 62 : 393 / 487 = 80 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 63 : 448 / 489 = 91 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 64 : 371 / 488 = 76 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 65 : 454 / 490 = 92 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 66 : 326 / 488 = 66 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 67 : 373 / 490 = 76 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 68 : 284 / 490 = 57 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 69 : 364 / 490 = 74 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 70 : 216 / 490 = 44 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 71 : 207 / 490 = 42 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 72 : 190 / 488 = 38 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 73 : 266 / 486 = 54 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 74 : 259 / 481 = 53 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 75 : 257 / 488 = 52 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 76 : 320 / 489 = 65 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 77 : 328 / 488 = 67 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 78 : 368 / 488 = 75 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 79 : 448 / 490 = 91 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 80 : 409 / 489 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 81 : 308 / 491 = 62 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 82 : 331 / 491 = 67 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 83 : 269 / 489 = 55 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 84 : 384 / 489 = 78 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 85 : 374 / 489 = 76 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 86 : 444 / 491 = 90 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 87 : 436 / 492 = 88 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 88 : 360 / 491 = 73 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 89 : 409 / 492 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 90 : 258 / 490 = 52 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 91 : 396 / 482 = 82 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 92 : 366 / 490 = 74 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 93 : 357 / 487 = 73 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 94 : 412 / 489 = 84 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 95 : 413 / 490 = 84 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 96 : 466 / 491 = 94 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 97 : 464 / 490 = 94 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 98 : 453 / 491 = 92 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 99 : 443 / 491 = 90 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 100 : 453 / 491 = 92 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 101 : 433 / 491 = 88 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 102 : 302 / 492 = 61 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 103 : 402 / 492 = 81 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 104 : 284 / 491 = 57 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 105 : 270 / 491 = 54 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 106 : 276 / 492 = 56 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 107 : 410 / 491 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 108 : 392 / 492 = 79 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 109 : 323 / 490 = 65 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 110 : 408 / 491 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 111 : 455 / 492 = 92 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 112 : 454 / 492 = 92 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 113 : 449 / 491 = 91 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 114 : 409 / 491 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 115 : 424 / 492 = 86 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 116 : 408 / 491 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 117 : 438 / 492 = 89 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 118 : 445 / 490 = 90 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 119 : 450 / 492 = 91 %
[ Sun Jul 14 07:13:27 2024 ] Accuracy of 120 : 419 / 500 = 83 %
[ Sun Jul 14 07:13:27 2024 ] Training epoch: 41
[ Sun Jul 14 07:13:28 2024 ] 	Batch(0/6809) done. Loss: 0.0473  lr:0.000001
[ Sun Jul 14 07:13:46 2024 ] 	Batch(100/6809) done. Loss: 0.0709  lr:0.000001
[ Sun Jul 14 07:14:04 2024 ] 	Batch(200/6809) done. Loss: 0.1372  lr:0.000001
[ Sun Jul 14 07:14:22 2024 ] 	Batch(300/6809) done. Loss: 0.0655  lr:0.000001
[ Sun Jul 14 07:14:40 2024 ] 	Batch(400/6809) done. Loss: 0.0197  lr:0.000001
[ Sun Jul 14 07:14:57 2024 ] 
Training: Epoch [40/50], Step [499], Loss: 0.1009322926402092, Training Accuracy: 97.675
[ Sun Jul 14 07:14:57 2024 ] 	Batch(500/6809) done. Loss: 0.0414  lr:0.000001
[ Sun Jul 14 07:15:15 2024 ] 	Batch(600/6809) done. Loss: 0.0056  lr:0.000001
[ Sun Jul 14 07:15:33 2024 ] 	Batch(700/6809) done. Loss: 0.3292  lr:0.000001
[ Sun Jul 14 07:15:51 2024 ] 	Batch(800/6809) done. Loss: 0.0886  lr:0.000001
[ Sun Jul 14 07:16:09 2024 ] 	Batch(900/6809) done. Loss: 0.0119  lr:0.000001
[ Sun Jul 14 07:16:27 2024 ] 
Training: Epoch [40/50], Step [999], Loss: 0.004437751602381468, Training Accuracy: 97.7625
[ Sun Jul 14 07:16:27 2024 ] 	Batch(1000/6809) done. Loss: 0.1184  lr:0.000001
[ Sun Jul 14 07:16:45 2024 ] 	Batch(1100/6809) done. Loss: 0.0030  lr:0.000001
[ Sun Jul 14 07:17:03 2024 ] 	Batch(1200/6809) done. Loss: 0.0256  lr:0.000001
[ Sun Jul 14 07:17:21 2024 ] 	Batch(1300/6809) done. Loss: 0.0989  lr:0.000001
[ Sun Jul 14 07:17:39 2024 ] 	Batch(1400/6809) done. Loss: 0.0160  lr:0.000001
[ Sun Jul 14 07:17:56 2024 ] 
Training: Epoch [40/50], Step [1499], Loss: 0.23976288735866547, Training Accuracy: 97.725
[ Sun Jul 14 07:17:56 2024 ] 	Batch(1500/6809) done. Loss: 0.0793  lr:0.000001
[ Sun Jul 14 07:18:14 2024 ] 	Batch(1600/6809) done. Loss: 0.1305  lr:0.000001
[ Sun Jul 14 07:18:32 2024 ] 	Batch(1700/6809) done. Loss: 0.0095  lr:0.000001
[ Sun Jul 14 07:18:50 2024 ] 	Batch(1800/6809) done. Loss: 0.1541  lr:0.000001
[ Sun Jul 14 07:19:08 2024 ] 	Batch(1900/6809) done. Loss: 0.0121  lr:0.000001
[ Sun Jul 14 07:19:26 2024 ] 
Training: Epoch [40/50], Step [1999], Loss: 0.04498540237545967, Training Accuracy: 97.68124999999999
[ Sun Jul 14 07:19:26 2024 ] 	Batch(2000/6809) done. Loss: 0.0269  lr:0.000001
[ Sun Jul 14 07:19:44 2024 ] 	Batch(2100/6809) done. Loss: 0.0956  lr:0.000001
[ Sun Jul 14 07:20:02 2024 ] 	Batch(2200/6809) done. Loss: 0.0029  lr:0.000001
[ Sun Jul 14 07:20:20 2024 ] 	Batch(2300/6809) done. Loss: 0.1426  lr:0.000001
[ Sun Jul 14 07:20:38 2024 ] 	Batch(2400/6809) done. Loss: 0.0116  lr:0.000001
[ Sun Jul 14 07:20:56 2024 ] 
Training: Epoch [40/50], Step [2499], Loss: 0.06719781458377838, Training Accuracy: 97.61999999999999
[ Sun Jul 14 07:20:56 2024 ] 	Batch(2500/6809) done. Loss: 0.0162  lr:0.000001
[ Sun Jul 14 07:21:14 2024 ] 	Batch(2600/6809) done. Loss: 0.0111  lr:0.000001
[ Sun Jul 14 07:21:32 2024 ] 	Batch(2700/6809) done. Loss: 0.0067  lr:0.000001
[ Sun Jul 14 07:21:50 2024 ] 	Batch(2800/6809) done. Loss: 0.0358  lr:0.000001
[ Sun Jul 14 07:22:08 2024 ] 	Batch(2900/6809) done. Loss: 0.0070  lr:0.000001
[ Sun Jul 14 07:22:26 2024 ] 
Training: Epoch [40/50], Step [2999], Loss: 0.09538548439741135, Training Accuracy: 97.6
[ Sun Jul 14 07:22:26 2024 ] 	Batch(3000/6809) done. Loss: 0.0012  lr:0.000001
[ Sun Jul 14 07:22:44 2024 ] 	Batch(3100/6809) done. Loss: 0.0708  lr:0.000001
[ Sun Jul 14 07:23:02 2024 ] 	Batch(3200/6809) done. Loss: 0.0276  lr:0.000001
[ Sun Jul 14 07:23:20 2024 ] 	Batch(3300/6809) done. Loss: 0.0033  lr:0.000001
[ Sun Jul 14 07:23:38 2024 ] 	Batch(3400/6809) done. Loss: 0.2032  lr:0.000001
[ Sun Jul 14 07:23:56 2024 ] 
Training: Epoch [40/50], Step [3499], Loss: 0.014750849455595016, Training Accuracy: 97.64285714285714
[ Sun Jul 14 07:23:56 2024 ] 	Batch(3500/6809) done. Loss: 0.0258  lr:0.000001
[ Sun Jul 14 07:24:14 2024 ] 	Batch(3600/6809) done. Loss: 0.0652  lr:0.000001
[ Sun Jul 14 07:24:32 2024 ] 	Batch(3700/6809) done. Loss: 0.0268  lr:0.000001
[ Sun Jul 14 07:24:51 2024 ] 	Batch(3800/6809) done. Loss: 0.0994  lr:0.000001
[ Sun Jul 14 07:25:09 2024 ] 	Batch(3900/6809) done. Loss: 0.0636  lr:0.000001
[ Sun Jul 14 07:25:27 2024 ] 
Training: Epoch [40/50], Step [3999], Loss: 0.026922278106212616, Training Accuracy: 97.625
[ Sun Jul 14 07:25:28 2024 ] 	Batch(4000/6809) done. Loss: 0.0114  lr:0.000001
[ Sun Jul 14 07:25:46 2024 ] 	Batch(4100/6809) done. Loss: 0.0263  lr:0.000001
[ Sun Jul 14 07:26:04 2024 ] 	Batch(4200/6809) done. Loss: 0.1097  lr:0.000001
[ Sun Jul 14 07:26:22 2024 ] 	Batch(4300/6809) done. Loss: 0.0699  lr:0.000001
[ Sun Jul 14 07:26:40 2024 ] 	Batch(4400/6809) done. Loss: 0.1798  lr:0.000001
[ Sun Jul 14 07:26:58 2024 ] 
Training: Epoch [40/50], Step [4499], Loss: 0.0353659950196743, Training Accuracy: 97.6361111111111
[ Sun Jul 14 07:26:58 2024 ] 	Batch(4500/6809) done. Loss: 0.0366  lr:0.000001
[ Sun Jul 14 07:27:16 2024 ] 	Batch(4600/6809) done. Loss: 0.0219  lr:0.000001
[ Sun Jul 14 07:27:34 2024 ] 	Batch(4700/6809) done. Loss: 0.0474  lr:0.000001
[ Sun Jul 14 07:27:52 2024 ] 	Batch(4800/6809) done. Loss: 0.1429  lr:0.000001
[ Sun Jul 14 07:28:10 2024 ] 	Batch(4900/6809) done. Loss: 0.0564  lr:0.000001
[ Sun Jul 14 07:28:28 2024 ] 
Training: Epoch [40/50], Step [4999], Loss: 0.0956375002861023, Training Accuracy: 97.6975
[ Sun Jul 14 07:28:28 2024 ] 	Batch(5000/6809) done. Loss: 0.5053  lr:0.000001
[ Sun Jul 14 07:28:46 2024 ] 	Batch(5100/6809) done. Loss: 0.0134  lr:0.000001
[ Sun Jul 14 07:29:04 2024 ] 	Batch(5200/6809) done. Loss: 0.0857  lr:0.000001
[ Sun Jul 14 07:29:22 2024 ] 	Batch(5300/6809) done. Loss: 0.1737  lr:0.000001
[ Sun Jul 14 07:29:40 2024 ] 	Batch(5400/6809) done. Loss: 0.0405  lr:0.000001
[ Sun Jul 14 07:29:58 2024 ] 
Training: Epoch [40/50], Step [5499], Loss: 0.004912299104034901, Training Accuracy: 97.71136363636363
[ Sun Jul 14 07:29:58 2024 ] 	Batch(5500/6809) done. Loss: 0.0169  lr:0.000001
[ Sun Jul 14 07:30:16 2024 ] 	Batch(5600/6809) done. Loss: 0.0695  lr:0.000001
[ Sun Jul 14 07:30:34 2024 ] 	Batch(5700/6809) done. Loss: 0.4999  lr:0.000001
[ Sun Jul 14 07:30:52 2024 ] 	Batch(5800/6809) done. Loss: 0.0355  lr:0.000001
[ Sun Jul 14 07:31:09 2024 ] 	Batch(5900/6809) done. Loss: 0.0358  lr:0.000001
[ Sun Jul 14 07:31:27 2024 ] 
Training: Epoch [40/50], Step [5999], Loss: 0.02599411830306053, Training Accuracy: 97.725
[ Sun Jul 14 07:31:28 2024 ] 	Batch(6000/6809) done. Loss: 0.1013  lr:0.000001
[ Sun Jul 14 07:31:45 2024 ] 	Batch(6100/6809) done. Loss: 0.0741  lr:0.000001
[ Sun Jul 14 07:32:03 2024 ] 	Batch(6200/6809) done. Loss: 0.0386  lr:0.000001
[ Sun Jul 14 07:32:21 2024 ] 	Batch(6300/6809) done. Loss: 0.2725  lr:0.000001
[ Sun Jul 14 07:32:39 2024 ] 	Batch(6400/6809) done. Loss: 0.0050  lr:0.000001
[ Sun Jul 14 07:32:57 2024 ] 
Training: Epoch [40/50], Step [6499], Loss: 0.04926721751689911, Training Accuracy: 97.70961538461539
[ Sun Jul 14 07:32:57 2024 ] 	Batch(6500/6809) done. Loss: 0.0387  lr:0.000001
[ Sun Jul 14 07:33:15 2024 ] 	Batch(6600/6809) done. Loss: 0.0507  lr:0.000001
[ Sun Jul 14 07:33:33 2024 ] 	Batch(6700/6809) done. Loss: 0.0714  lr:0.000001
[ Sun Jul 14 07:33:51 2024 ] 	Batch(6800/6809) done. Loss: 0.4796  lr:0.000001
[ Sun Jul 14 07:33:53 2024 ] 	Mean training loss: 0.0948.
[ Sun Jul 14 07:33:53 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 07:33:53 2024 ] Training epoch: 42
[ Sun Jul 14 07:33:53 2024 ] 	Batch(0/6809) done. Loss: 0.0240  lr:0.000001
[ Sun Jul 14 07:34:12 2024 ] 	Batch(100/6809) done. Loss: 0.0286  lr:0.000001
[ Sun Jul 14 07:34:30 2024 ] 	Batch(200/6809) done. Loss: 0.0045  lr:0.000001
[ Sun Jul 14 07:34:48 2024 ] 	Batch(300/6809) done. Loss: 0.0225  lr:0.000001
[ Sun Jul 14 07:35:07 2024 ] 	Batch(400/6809) done. Loss: 0.1575  lr:0.000001
[ Sun Jul 14 07:35:25 2024 ] 
Training: Epoch [41/50], Step [499], Loss: 0.035562194883823395, Training Accuracy: 97.725
[ Sun Jul 14 07:35:25 2024 ] 	Batch(500/6809) done. Loss: 0.0087  lr:0.000001
[ Sun Jul 14 07:35:43 2024 ] 	Batch(600/6809) done. Loss: 0.2253  lr:0.000001
[ Sun Jul 14 07:36:02 2024 ] 	Batch(700/6809) done. Loss: 0.0180  lr:0.000001
[ Sun Jul 14 07:36:20 2024 ] 	Batch(800/6809) done. Loss: 0.1672  lr:0.000001
[ Sun Jul 14 07:36:38 2024 ] 	Batch(900/6809) done. Loss: 0.2060  lr:0.000001
[ Sun Jul 14 07:36:56 2024 ] 
Training: Epoch [41/50], Step [999], Loss: 0.07247130572795868, Training Accuracy: 98.0125
[ Sun Jul 14 07:36:56 2024 ] 	Batch(1000/6809) done. Loss: 0.1697  lr:0.000001
[ Sun Jul 14 07:37:14 2024 ] 	Batch(1100/6809) done. Loss: 0.5182  lr:0.000001
[ Sun Jul 14 07:37:33 2024 ] 	Batch(1200/6809) done. Loss: 0.0262  lr:0.000001
[ Sun Jul 14 07:37:51 2024 ] 	Batch(1300/6809) done. Loss: 0.0525  lr:0.000001
[ Sun Jul 14 07:38:09 2024 ] 	Batch(1400/6809) done. Loss: 0.0017  lr:0.000001
[ Sun Jul 14 07:38:26 2024 ] 
Training: Epoch [41/50], Step [1499], Loss: 0.02188529074192047, Training Accuracy: 97.95833333333334
[ Sun Jul 14 07:38:27 2024 ] 	Batch(1500/6809) done. Loss: 0.1494  lr:0.000001
[ Sun Jul 14 07:38:45 2024 ] 	Batch(1600/6809) done. Loss: 0.0068  lr:0.000001
[ Sun Jul 14 07:39:03 2024 ] 	Batch(1700/6809) done. Loss: 0.0449  lr:0.000001
[ Sun Jul 14 07:39:21 2024 ] 	Batch(1800/6809) done. Loss: 0.0480  lr:0.000001
[ Sun Jul 14 07:39:39 2024 ] 	Batch(1900/6809) done. Loss: 0.0006  lr:0.000001
[ Sun Jul 14 07:39:57 2024 ] 
Training: Epoch [41/50], Step [1999], Loss: 0.0575203001499176, Training Accuracy: 97.775
[ Sun Jul 14 07:39:57 2024 ] 	Batch(2000/6809) done. Loss: 0.1973  lr:0.000001
[ Sun Jul 14 07:40:15 2024 ] 	Batch(2100/6809) done. Loss: 0.0172  lr:0.000001
[ Sun Jul 14 07:40:33 2024 ] 	Batch(2200/6809) done. Loss: 0.0056  lr:0.000001
[ Sun Jul 14 07:40:51 2024 ] 	Batch(2300/6809) done. Loss: 0.0694  lr:0.000001
[ Sun Jul 14 07:41:09 2024 ] 	Batch(2400/6809) done. Loss: 0.0112  lr:0.000001
[ Sun Jul 14 07:41:27 2024 ] 
Training: Epoch [41/50], Step [2499], Loss: 0.028904912993311882, Training Accuracy: 97.80499999999999
[ Sun Jul 14 07:41:27 2024 ] 	Batch(2500/6809) done. Loss: 0.2231  lr:0.000001
[ Sun Jul 14 07:41:45 2024 ] 	Batch(2600/6809) done. Loss: 0.0166  lr:0.000001
[ Sun Jul 14 07:42:03 2024 ] 	Batch(2700/6809) done. Loss: 0.0575  lr:0.000001
[ Sun Jul 14 07:42:22 2024 ] 	Batch(2800/6809) done. Loss: 0.0325  lr:0.000001
[ Sun Jul 14 07:42:40 2024 ] 	Batch(2900/6809) done. Loss: 0.0313  lr:0.000001
[ Sun Jul 14 07:42:58 2024 ] 
Training: Epoch [41/50], Step [2999], Loss: 0.04192966967821121, Training Accuracy: 97.8
[ Sun Jul 14 07:42:58 2024 ] 	Batch(3000/6809) done. Loss: 0.0367  lr:0.000001
[ Sun Jul 14 07:43:17 2024 ] 	Batch(3100/6809) done. Loss: 0.1315  lr:0.000001
[ Sun Jul 14 07:43:35 2024 ] 	Batch(3200/6809) done. Loss: 0.0227  lr:0.000001
[ Sun Jul 14 07:43:53 2024 ] 	Batch(3300/6809) done. Loss: 0.0148  lr:0.000001
[ Sun Jul 14 07:44:11 2024 ] 	Batch(3400/6809) done. Loss: 0.0045  lr:0.000001
[ Sun Jul 14 07:44:29 2024 ] 
Training: Epoch [41/50], Step [3499], Loss: 0.06567343324422836, Training Accuracy: 97.76785714285714
[ Sun Jul 14 07:44:29 2024 ] 	Batch(3500/6809) done. Loss: 0.1884  lr:0.000001
[ Sun Jul 14 07:44:47 2024 ] 	Batch(3600/6809) done. Loss: 0.0496  lr:0.000001
[ Sun Jul 14 07:45:05 2024 ] 	Batch(3700/6809) done. Loss: 0.0978  lr:0.000001
[ Sun Jul 14 07:45:23 2024 ] 	Batch(3800/6809) done. Loss: 0.0129  lr:0.000001
[ Sun Jul 14 07:45:41 2024 ] 	Batch(3900/6809) done. Loss: 0.0157  lr:0.000001
[ Sun Jul 14 07:45:59 2024 ] 
Training: Epoch [41/50], Step [3999], Loss: 0.42677950859069824, Training Accuracy: 97.753125
[ Sun Jul 14 07:45:59 2024 ] 	Batch(4000/6809) done. Loss: 0.0041  lr:0.000001
[ Sun Jul 14 07:46:17 2024 ] 	Batch(4100/6809) done. Loss: 0.0538  lr:0.000001
[ Sun Jul 14 07:46:35 2024 ] 	Batch(4200/6809) done. Loss: 0.0062  lr:0.000001
[ Sun Jul 14 07:46:53 2024 ] 	Batch(4300/6809) done. Loss: 0.0716  lr:0.000001
[ Sun Jul 14 07:47:11 2024 ] 	Batch(4400/6809) done. Loss: 0.0460  lr:0.000001
[ Sun Jul 14 07:47:29 2024 ] 
Training: Epoch [41/50], Step [4499], Loss: 0.022332068532705307, Training Accuracy: 97.77777777777777
[ Sun Jul 14 07:47:29 2024 ] 	Batch(4500/6809) done. Loss: 0.0130  lr:0.000001
[ Sun Jul 14 07:47:47 2024 ] 	Batch(4600/6809) done. Loss: 0.0280  lr:0.000001
[ Sun Jul 14 07:48:05 2024 ] 	Batch(4700/6809) done. Loss: 0.0054  lr:0.000001
[ Sun Jul 14 07:48:23 2024 ] 	Batch(4800/6809) done. Loss: 0.0149  lr:0.000001
[ Sun Jul 14 07:48:41 2024 ] 	Batch(4900/6809) done. Loss: 0.0228  lr:0.000001
[ Sun Jul 14 07:48:58 2024 ] 
Training: Epoch [41/50], Step [4999], Loss: 0.17060577869415283, Training Accuracy: 97.8
[ Sun Jul 14 07:48:59 2024 ] 	Batch(5000/6809) done. Loss: 0.0774  lr:0.000001
[ Sun Jul 14 07:49:16 2024 ] 	Batch(5100/6809) done. Loss: 0.1118  lr:0.000001
[ Sun Jul 14 07:49:35 2024 ] 	Batch(5200/6809) done. Loss: 0.2042  lr:0.000001
[ Sun Jul 14 07:49:53 2024 ] 	Batch(5300/6809) done. Loss: 0.1049  lr:0.000001
[ Sun Jul 14 07:50:12 2024 ] 	Batch(5400/6809) done. Loss: 0.0698  lr:0.000001
[ Sun Jul 14 07:50:30 2024 ] 
Training: Epoch [41/50], Step [5499], Loss: 0.0825827494263649, Training Accuracy: 97.7909090909091
[ Sun Jul 14 07:50:30 2024 ] 	Batch(5500/6809) done. Loss: 0.0066  lr:0.000001
[ Sun Jul 14 07:50:49 2024 ] 	Batch(5600/6809) done. Loss: 0.0093  lr:0.000001
[ Sun Jul 14 07:51:07 2024 ] 	Batch(5700/6809) done. Loss: 0.0382  lr:0.000001
[ Sun Jul 14 07:51:26 2024 ] 	Batch(5800/6809) done. Loss: 0.0420  lr:0.000001
[ Sun Jul 14 07:51:45 2024 ] 	Batch(5900/6809) done. Loss: 0.2149  lr:0.000001
[ Sun Jul 14 07:52:03 2024 ] 
Training: Epoch [41/50], Step [5999], Loss: 0.11891793459653854, Training Accuracy: 97.7625
[ Sun Jul 14 07:52:03 2024 ] 	Batch(6000/6809) done. Loss: 0.0020  lr:0.000001
[ Sun Jul 14 07:52:21 2024 ] 	Batch(6100/6809) done. Loss: 0.0199  lr:0.000001
[ Sun Jul 14 07:52:39 2024 ] 	Batch(6200/6809) done. Loss: 0.0242  lr:0.000001
[ Sun Jul 14 07:52:57 2024 ] 	Batch(6300/6809) done. Loss: 0.0865  lr:0.000001
[ Sun Jul 14 07:53:15 2024 ] 	Batch(6400/6809) done. Loss: 0.0420  lr:0.000001
[ Sun Jul 14 07:53:33 2024 ] 
Training: Epoch [41/50], Step [6499], Loss: 0.23009932041168213, Training Accuracy: 97.76923076923076
[ Sun Jul 14 07:53:34 2024 ] 	Batch(6500/6809) done. Loss: 0.0400  lr:0.000001
[ Sun Jul 14 07:53:52 2024 ] 	Batch(6600/6809) done. Loss: 0.0114  lr:0.000001
[ Sun Jul 14 07:54:11 2024 ] 	Batch(6700/6809) done. Loss: 0.0190  lr:0.000001
[ Sun Jul 14 07:54:29 2024 ] 	Batch(6800/6809) done. Loss: 0.0921  lr:0.000001
[ Sun Jul 14 07:54:31 2024 ] 	Mean training loss: 0.0891.
[ Sun Jul 14 07:54:31 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 07:54:31 2024 ] Training epoch: 43
[ Sun Jul 14 07:54:31 2024 ] 	Batch(0/6809) done. Loss: 0.0501  lr:0.000001
[ Sun Jul 14 07:54:49 2024 ] 	Batch(100/6809) done. Loss: 0.0357  lr:0.000001
[ Sun Jul 14 07:55:07 2024 ] 	Batch(200/6809) done. Loss: 0.2487  lr:0.000001
[ Sun Jul 14 07:55:25 2024 ] 	Batch(300/6809) done. Loss: 0.1934  lr:0.000001
[ Sun Jul 14 07:55:43 2024 ] 	Batch(400/6809) done. Loss: 0.0693  lr:0.000001
[ Sun Jul 14 07:56:02 2024 ] 
Training: Epoch [42/50], Step [499], Loss: 0.054609835147857666, Training Accuracy: 97.275
[ Sun Jul 14 07:56:02 2024 ] 	Batch(500/6809) done. Loss: 0.0215  lr:0.000001
[ Sun Jul 14 07:56:20 2024 ] 	Batch(600/6809) done. Loss: 0.0296  lr:0.000001
[ Sun Jul 14 07:56:38 2024 ] 	Batch(700/6809) done. Loss: 0.1067  lr:0.000001
[ Sun Jul 14 07:56:57 2024 ] 	Batch(800/6809) done. Loss: 0.0437  lr:0.000001
[ Sun Jul 14 07:57:15 2024 ] 	Batch(900/6809) done. Loss: 0.0515  lr:0.000001
[ Sun Jul 14 07:57:33 2024 ] 
Training: Epoch [42/50], Step [999], Loss: 0.015310138463973999, Training Accuracy: 97.475
[ Sun Jul 14 07:57:33 2024 ] 	Batch(1000/6809) done. Loss: 0.1219  lr:0.000001
[ Sun Jul 14 07:57:52 2024 ] 	Batch(1100/6809) done. Loss: 0.3430  lr:0.000001
[ Sun Jul 14 07:58:10 2024 ] 	Batch(1200/6809) done. Loss: 0.0164  lr:0.000001
[ Sun Jul 14 07:58:28 2024 ] 	Batch(1300/6809) done. Loss: 0.0391  lr:0.000001
[ Sun Jul 14 07:58:46 2024 ] 	Batch(1400/6809) done. Loss: 0.0428  lr:0.000001
[ Sun Jul 14 07:59:04 2024 ] 
Training: Epoch [42/50], Step [1499], Loss: 0.07424630224704742, Training Accuracy: 97.64166666666667
[ Sun Jul 14 07:59:04 2024 ] 	Batch(1500/6809) done. Loss: 0.0210  lr:0.000001
[ Sun Jul 14 07:59:23 2024 ] 	Batch(1600/6809) done. Loss: 0.0231  lr:0.000001
[ Sun Jul 14 07:59:41 2024 ] 	Batch(1700/6809) done. Loss: 0.0046  lr:0.000001
[ Sun Jul 14 07:59:59 2024 ] 	Batch(1800/6809) done. Loss: 0.0242  lr:0.000001
[ Sun Jul 14 08:00:17 2024 ] 	Batch(1900/6809) done. Loss: 0.0318  lr:0.000001
[ Sun Jul 14 08:00:35 2024 ] 
Training: Epoch [42/50], Step [1999], Loss: 0.009179809130728245, Training Accuracy: 97.7375
[ Sun Jul 14 08:00:35 2024 ] 	Batch(2000/6809) done. Loss: 0.0951  lr:0.000001
[ Sun Jul 14 08:00:53 2024 ] 	Batch(2100/6809) done. Loss: 0.0304  lr:0.000001
[ Sun Jul 14 08:01:11 2024 ] 	Batch(2200/6809) done. Loss: 0.0291  lr:0.000001
[ Sun Jul 14 08:01:29 2024 ] 	Batch(2300/6809) done. Loss: 0.0026  lr:0.000001
[ Sun Jul 14 08:01:47 2024 ] 	Batch(2400/6809) done. Loss: 0.0255  lr:0.000001
[ Sun Jul 14 08:02:05 2024 ] 
Training: Epoch [42/50], Step [2499], Loss: 0.011210586875677109, Training Accuracy: 97.75500000000001
[ Sun Jul 14 08:02:05 2024 ] 	Batch(2500/6809) done. Loss: 0.1130  lr:0.000001
[ Sun Jul 14 08:02:23 2024 ] 	Batch(2600/6809) done. Loss: 0.0250  lr:0.000001
[ Sun Jul 14 08:02:41 2024 ] 	Batch(2700/6809) done. Loss: 0.5509  lr:0.000001
[ Sun Jul 14 08:03:00 2024 ] 	Batch(2800/6809) done. Loss: 0.0150  lr:0.000001
[ Sun Jul 14 08:03:18 2024 ] 	Batch(2900/6809) done. Loss: 0.1485  lr:0.000001
[ Sun Jul 14 08:03:35 2024 ] 
Training: Epoch [42/50], Step [2999], Loss: 0.01728004775941372, Training Accuracy: 97.8375
[ Sun Jul 14 08:03:36 2024 ] 	Batch(3000/6809) done. Loss: 0.0327  lr:0.000001
[ Sun Jul 14 08:03:54 2024 ] 	Batch(3100/6809) done. Loss: 0.0346  lr:0.000001
[ Sun Jul 14 08:04:12 2024 ] 	Batch(3200/6809) done. Loss: 0.1032  lr:0.000001
[ Sun Jul 14 08:04:30 2024 ] 	Batch(3300/6809) done. Loss: 0.1132  lr:0.000001
[ Sun Jul 14 08:04:48 2024 ] 	Batch(3400/6809) done. Loss: 0.0023  lr:0.000001
[ Sun Jul 14 08:05:06 2024 ] 
Training: Epoch [42/50], Step [3499], Loss: 0.015005259774625301, Training Accuracy: 97.8
[ Sun Jul 14 08:05:06 2024 ] 	Batch(3500/6809) done. Loss: 0.0877  lr:0.000001
[ Sun Jul 14 08:05:24 2024 ] 	Batch(3600/6809) done. Loss: 0.0033  lr:0.000001
[ Sun Jul 14 08:05:42 2024 ] 	Batch(3700/6809) done. Loss: 0.0355  lr:0.000001
[ Sun Jul 14 08:06:00 2024 ] 	Batch(3800/6809) done. Loss: 0.0111  lr:0.000001
[ Sun Jul 14 08:06:18 2024 ] 	Batch(3900/6809) done. Loss: 0.4377  lr:0.000001
[ Sun Jul 14 08:06:36 2024 ] 
Training: Epoch [42/50], Step [3999], Loss: 0.008178191259503365, Training Accuracy: 97.71875
[ Sun Jul 14 08:06:36 2024 ] 	Batch(4000/6809) done. Loss: 0.0240  lr:0.000001
[ Sun Jul 14 08:06:54 2024 ] 	Batch(4100/6809) done. Loss: 0.0285  lr:0.000001
[ Sun Jul 14 08:07:12 2024 ] 	Batch(4200/6809) done. Loss: 0.0196  lr:0.000001
[ Sun Jul 14 08:07:30 2024 ] 	Batch(4300/6809) done. Loss: 0.0072  lr:0.000001
[ Sun Jul 14 08:07:48 2024 ] 	Batch(4400/6809) done. Loss: 0.0790  lr:0.000001
[ Sun Jul 14 08:08:06 2024 ] 
Training: Epoch [42/50], Step [4499], Loss: 0.004343160428106785, Training Accuracy: 97.71666666666667
[ Sun Jul 14 08:08:06 2024 ] 	Batch(4500/6809) done. Loss: 0.0659  lr:0.000001
[ Sun Jul 14 08:08:24 2024 ] 	Batch(4600/6809) done. Loss: 0.0156  lr:0.000001
[ Sun Jul 14 08:08:42 2024 ] 	Batch(4700/6809) done. Loss: 0.0743  lr:0.000001
[ Sun Jul 14 08:09:00 2024 ] 	Batch(4800/6809) done. Loss: 0.0094  lr:0.000001
[ Sun Jul 14 08:09:18 2024 ] 	Batch(4900/6809) done. Loss: 0.1337  lr:0.000001
[ Sun Jul 14 08:09:36 2024 ] 
Training: Epoch [42/50], Step [4999], Loss: 0.013006780296564102, Training Accuracy: 97.7475
[ Sun Jul 14 08:09:36 2024 ] 	Batch(5000/6809) done. Loss: 0.1938  lr:0.000001
[ Sun Jul 14 08:09:54 2024 ] 	Batch(5100/6809) done. Loss: 0.0401  lr:0.000001
[ Sun Jul 14 08:10:13 2024 ] 	Batch(5200/6809) done. Loss: 0.1012  lr:0.000001
[ Sun Jul 14 08:10:31 2024 ] 	Batch(5300/6809) done. Loss: 0.0121  lr:0.000001
[ Sun Jul 14 08:10:49 2024 ] 	Batch(5400/6809) done. Loss: 0.0345  lr:0.000001
[ Sun Jul 14 08:11:06 2024 ] 
Training: Epoch [42/50], Step [5499], Loss: 0.41017085313796997, Training Accuracy: 97.79318181818182
[ Sun Jul 14 08:11:07 2024 ] 	Batch(5500/6809) done. Loss: 0.2324  lr:0.000001
[ Sun Jul 14 08:11:25 2024 ] 	Batch(5600/6809) done. Loss: 0.0485  lr:0.000001
[ Sun Jul 14 08:11:43 2024 ] 	Batch(5700/6809) done. Loss: 0.0439  lr:0.000001
[ Sun Jul 14 08:12:01 2024 ] 	Batch(5800/6809) done. Loss: 0.0527  lr:0.000001
[ Sun Jul 14 08:12:19 2024 ] 	Batch(5900/6809) done. Loss: 0.0887  lr:0.000001
[ Sun Jul 14 08:12:37 2024 ] 
Training: Epoch [42/50], Step [5999], Loss: 0.05320892110466957, Training Accuracy: 97.78750000000001
[ Sun Jul 14 08:12:37 2024 ] 	Batch(6000/6809) done. Loss: 0.0634  lr:0.000001
[ Sun Jul 14 08:12:55 2024 ] 	Batch(6100/6809) done. Loss: 0.0055  lr:0.000001
[ Sun Jul 14 08:13:13 2024 ] 	Batch(6200/6809) done. Loss: 0.0331  lr:0.000001
[ Sun Jul 14 08:13:31 2024 ] 	Batch(6300/6809) done. Loss: 0.0335  lr:0.000001
[ Sun Jul 14 08:13:50 2024 ] 	Batch(6400/6809) done. Loss: 0.1931  lr:0.000001
[ Sun Jul 14 08:14:09 2024 ] 
Training: Epoch [42/50], Step [6499], Loss: 0.02466399222612381, Training Accuracy: 97.77692307692307
[ Sun Jul 14 08:14:09 2024 ] 	Batch(6500/6809) done. Loss: 0.0061  lr:0.000001
[ Sun Jul 14 08:14:27 2024 ] 	Batch(6600/6809) done. Loss: 0.0190  lr:0.000001
[ Sun Jul 14 08:14:45 2024 ] 	Batch(6700/6809) done. Loss: 0.0792  lr:0.000001
[ Sun Jul 14 08:15:03 2024 ] 	Batch(6800/6809) done. Loss: 0.0033  lr:0.000001
[ Sun Jul 14 08:15:04 2024 ] 	Mean training loss: 0.0901.
[ Sun Jul 14 08:15:04 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 08:15:04 2024 ] Training epoch: 44
[ Sun Jul 14 08:15:05 2024 ] 	Batch(0/6809) done. Loss: 0.0064  lr:0.000001
[ Sun Jul 14 08:15:23 2024 ] 	Batch(100/6809) done. Loss: 0.1057  lr:0.000001
[ Sun Jul 14 08:15:41 2024 ] 	Batch(200/6809) done. Loss: 0.0354  lr:0.000001
[ Sun Jul 14 08:15:59 2024 ] 	Batch(300/6809) done. Loss: 0.0256  lr:0.000001
[ Sun Jul 14 08:16:18 2024 ] 	Batch(400/6809) done. Loss: 0.0155  lr:0.000001
[ Sun Jul 14 08:16:36 2024 ] 
Training: Epoch [43/50], Step [499], Loss: 0.046754542738199234, Training Accuracy: 97.7
[ Sun Jul 14 08:16:36 2024 ] 	Batch(500/6809) done. Loss: 0.0782  lr:0.000001
[ Sun Jul 14 08:16:54 2024 ] 	Batch(600/6809) done. Loss: 0.1450  lr:0.000001
[ Sun Jul 14 08:17:12 2024 ] 	Batch(700/6809) done. Loss: 0.1253  lr:0.000001
[ Sun Jul 14 08:17:31 2024 ] 	Batch(800/6809) done. Loss: 0.6980  lr:0.000001
[ Sun Jul 14 08:17:49 2024 ] 	Batch(900/6809) done. Loss: 0.0161  lr:0.000001
[ Sun Jul 14 08:18:07 2024 ] 
Training: Epoch [43/50], Step [999], Loss: 0.20108644664287567, Training Accuracy: 97.7375
[ Sun Jul 14 08:18:07 2024 ] 	Batch(1000/6809) done. Loss: 0.0856  lr:0.000001
[ Sun Jul 14 08:18:25 2024 ] 	Batch(1100/6809) done. Loss: 0.0604  lr:0.000001
[ Sun Jul 14 08:18:43 2024 ] 	Batch(1200/6809) done. Loss: 0.0293  lr:0.000001
[ Sun Jul 14 08:19:01 2024 ] 	Batch(1300/6809) done. Loss: 0.0490  lr:0.000001
[ Sun Jul 14 08:19:19 2024 ] 	Batch(1400/6809) done. Loss: 0.1562  lr:0.000001
[ Sun Jul 14 08:19:37 2024 ] 
Training: Epoch [43/50], Step [1499], Loss: 0.006894989404827356, Training Accuracy: 97.8
[ Sun Jul 14 08:19:37 2024 ] 	Batch(1500/6809) done. Loss: 0.0429  lr:0.000001
[ Sun Jul 14 08:19:55 2024 ] 	Batch(1600/6809) done. Loss: 0.4217  lr:0.000001
[ Sun Jul 14 08:20:13 2024 ] 	Batch(1700/6809) done. Loss: 0.0442  lr:0.000001
[ Sun Jul 14 08:20:31 2024 ] 	Batch(1800/6809) done. Loss: 0.0607  lr:0.000001
[ Sun Jul 14 08:20:49 2024 ] 	Batch(1900/6809) done. Loss: 0.0813  lr:0.000001
[ Sun Jul 14 08:21:07 2024 ] 
Training: Epoch [43/50], Step [1999], Loss: 0.036685504019260406, Training Accuracy: 97.78750000000001
[ Sun Jul 14 08:21:07 2024 ] 	Batch(2000/6809) done. Loss: 0.1752  lr:0.000001
[ Sun Jul 14 08:21:25 2024 ] 	Batch(2100/6809) done. Loss: 0.1375  lr:0.000001
[ Sun Jul 14 08:21:43 2024 ] 	Batch(2200/6809) done. Loss: 0.0281  lr:0.000001
[ Sun Jul 14 08:22:02 2024 ] 	Batch(2300/6809) done. Loss: 0.1526  lr:0.000001
[ Sun Jul 14 08:22:20 2024 ] 	Batch(2400/6809) done. Loss: 0.0470  lr:0.000001
[ Sun Jul 14 08:22:38 2024 ] 
Training: Epoch [43/50], Step [2499], Loss: 0.05414188653230667, Training Accuracy: 97.78
[ Sun Jul 14 08:22:38 2024 ] 	Batch(2500/6809) done. Loss: 0.0258  lr:0.000001
[ Sun Jul 14 08:22:56 2024 ] 	Batch(2600/6809) done. Loss: 0.0206  lr:0.000001
[ Sun Jul 14 08:23:14 2024 ] 	Batch(2700/6809) done. Loss: 0.0342  lr:0.000001
[ Sun Jul 14 08:23:32 2024 ] 	Batch(2800/6809) done. Loss: 0.0829  lr:0.000001
[ Sun Jul 14 08:23:50 2024 ] 	Batch(2900/6809) done. Loss: 0.0365  lr:0.000001
[ Sun Jul 14 08:24:08 2024 ] 
Training: Epoch [43/50], Step [2999], Loss: 0.09826032817363739, Training Accuracy: 97.80833333333334
[ Sun Jul 14 08:24:08 2024 ] 	Batch(3000/6809) done. Loss: 0.1859  lr:0.000001
[ Sun Jul 14 08:24:26 2024 ] 	Batch(3100/6809) done. Loss: 0.0567  lr:0.000001
[ Sun Jul 14 08:24:44 2024 ] 	Batch(3200/6809) done. Loss: 0.0356  lr:0.000001
[ Sun Jul 14 08:25:02 2024 ] 	Batch(3300/6809) done. Loss: 0.0258  lr:0.000001
[ Sun Jul 14 08:25:20 2024 ] 	Batch(3400/6809) done. Loss: 0.0039  lr:0.000001
[ Sun Jul 14 08:25:38 2024 ] 
Training: Epoch [43/50], Step [3499], Loss: 0.016596512869000435, Training Accuracy: 97.82142857142857
[ Sun Jul 14 08:25:38 2024 ] 	Batch(3500/6809) done. Loss: 0.0038  lr:0.000001
[ Sun Jul 14 08:25:56 2024 ] 	Batch(3600/6809) done. Loss: 0.2757  lr:0.000001
[ Sun Jul 14 08:26:14 2024 ] 	Batch(3700/6809) done. Loss: 0.2300  lr:0.000001
[ Sun Jul 14 08:26:32 2024 ] 	Batch(3800/6809) done. Loss: 0.0274  lr:0.000001
[ Sun Jul 14 08:26:50 2024 ] 	Batch(3900/6809) done. Loss: 0.0587  lr:0.000001
[ Sun Jul 14 08:27:08 2024 ] 
Training: Epoch [43/50], Step [3999], Loss: 0.09794304519891739, Training Accuracy: 97.84375
[ Sun Jul 14 08:27:08 2024 ] 	Batch(4000/6809) done. Loss: 0.0517  lr:0.000001
[ Sun Jul 14 08:27:26 2024 ] 	Batch(4100/6809) done. Loss: 0.0397  lr:0.000001
[ Sun Jul 14 08:27:44 2024 ] 	Batch(4200/6809) done. Loss: 0.1814  lr:0.000001
[ Sun Jul 14 08:28:03 2024 ] 	Batch(4300/6809) done. Loss: 0.0065  lr:0.000001
[ Sun Jul 14 08:28:21 2024 ] 	Batch(4400/6809) done. Loss: 0.0233  lr:0.000001
[ Sun Jul 14 08:28:39 2024 ] 
Training: Epoch [43/50], Step [4499], Loss: 0.012702224776148796, Training Accuracy: 97.83333333333334
[ Sun Jul 14 08:28:39 2024 ] 	Batch(4500/6809) done. Loss: 0.0058  lr:0.000001
[ Sun Jul 14 08:28:57 2024 ] 	Batch(4600/6809) done. Loss: 0.0177  lr:0.000001
[ Sun Jul 14 08:29:15 2024 ] 	Batch(4700/6809) done. Loss: 0.0158  lr:0.000001
[ Sun Jul 14 08:29:33 2024 ] 	Batch(4800/6809) done. Loss: 0.0183  lr:0.000001
[ Sun Jul 14 08:29:51 2024 ] 	Batch(4900/6809) done. Loss: 0.0096  lr:0.000001
[ Sun Jul 14 08:30:09 2024 ] 
Training: Epoch [43/50], Step [4999], Loss: 0.06704480946063995, Training Accuracy: 97.8425
[ Sun Jul 14 08:30:09 2024 ] 	Batch(5000/6809) done. Loss: 0.1277  lr:0.000001
[ Sun Jul 14 08:30:27 2024 ] 	Batch(5100/6809) done. Loss: 0.0301  lr:0.000001
[ Sun Jul 14 08:30:46 2024 ] 	Batch(5200/6809) done. Loss: 0.1268  lr:0.000001
[ Sun Jul 14 08:31:04 2024 ] 	Batch(5300/6809) done. Loss: 0.0832  lr:0.000001
[ Sun Jul 14 08:31:22 2024 ] 	Batch(5400/6809) done. Loss: 0.0048  lr:0.000001
[ Sun Jul 14 08:31:40 2024 ] 
Training: Epoch [43/50], Step [5499], Loss: 0.031438302248716354, Training Accuracy: 97.82954545454545
[ Sun Jul 14 08:31:40 2024 ] 	Batch(5500/6809) done. Loss: 0.0144  lr:0.000001
[ Sun Jul 14 08:31:59 2024 ] 	Batch(5600/6809) done. Loss: 0.1676  lr:0.000001
[ Sun Jul 14 08:32:17 2024 ] 	Batch(5700/6809) done. Loss: 0.0411  lr:0.000001
[ Sun Jul 14 08:32:35 2024 ] 	Batch(5800/6809) done. Loss: 0.0337  lr:0.000001
[ Sun Jul 14 08:32:54 2024 ] 	Batch(5900/6809) done. Loss: 0.0944  lr:0.000001
[ Sun Jul 14 08:33:12 2024 ] 
Training: Epoch [43/50], Step [5999], Loss: 0.010140834376215935, Training Accuracy: 97.80208333333333
[ Sun Jul 14 08:33:12 2024 ] 	Batch(6000/6809) done. Loss: 0.0440  lr:0.000001
[ Sun Jul 14 08:33:30 2024 ] 	Batch(6100/6809) done. Loss: 0.0025  lr:0.000001
[ Sun Jul 14 08:33:49 2024 ] 	Batch(6200/6809) done. Loss: 0.1724  lr:0.000001
[ Sun Jul 14 08:34:07 2024 ] 	Batch(6300/6809) done. Loss: 0.0464  lr:0.000001
[ Sun Jul 14 08:34:25 2024 ] 	Batch(6400/6809) done. Loss: 0.1512  lr:0.000001
[ Sun Jul 14 08:34:43 2024 ] 
Training: Epoch [43/50], Step [6499], Loss: 0.024490226060152054, Training Accuracy: 97.775
[ Sun Jul 14 08:34:43 2024 ] 	Batch(6500/6809) done. Loss: 0.0147  lr:0.000001
[ Sun Jul 14 08:35:01 2024 ] 	Batch(6600/6809) done. Loss: 0.1670  lr:0.000001
[ Sun Jul 14 08:35:19 2024 ] 	Batch(6700/6809) done. Loss: 0.0318  lr:0.000001
[ Sun Jul 14 08:35:37 2024 ] 	Batch(6800/6809) done. Loss: 0.0683  lr:0.000001
[ Sun Jul 14 08:35:39 2024 ] 	Mean training loss: 0.0897.
[ Sun Jul 14 08:35:39 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 08:35:39 2024 ] Training epoch: 45
[ Sun Jul 14 08:35:40 2024 ] 	Batch(0/6809) done. Loss: 0.0190  lr:0.000001
[ Sun Jul 14 08:35:58 2024 ] 	Batch(100/6809) done. Loss: 0.0104  lr:0.000001
[ Sun Jul 14 08:36:16 2024 ] 	Batch(200/6809) done. Loss: 0.0450  lr:0.000001
[ Sun Jul 14 08:36:35 2024 ] 	Batch(300/6809) done. Loss: 0.0150  lr:0.000001
[ Sun Jul 14 08:36:53 2024 ] 	Batch(400/6809) done. Loss: 0.0444  lr:0.000001
[ Sun Jul 14 08:37:11 2024 ] 
Training: Epoch [44/50], Step [499], Loss: 0.020120983943343163, Training Accuracy: 97.725
[ Sun Jul 14 08:37:11 2024 ] 	Batch(500/6809) done. Loss: 0.0306  lr:0.000001
[ Sun Jul 14 08:37:29 2024 ] 	Batch(600/6809) done. Loss: 0.0068  lr:0.000001
[ Sun Jul 14 08:37:47 2024 ] 	Batch(700/6809) done. Loss: 0.0050  lr:0.000001
[ Sun Jul 14 08:38:05 2024 ] 	Batch(800/6809) done. Loss: 0.0284  lr:0.000001
[ Sun Jul 14 08:38:23 2024 ] 	Batch(900/6809) done. Loss: 0.1009  lr:0.000001
[ Sun Jul 14 08:38:40 2024 ] 
Training: Epoch [44/50], Step [999], Loss: 0.036049533635377884, Training Accuracy: 97.82499999999999
[ Sun Jul 14 08:38:40 2024 ] 	Batch(1000/6809) done. Loss: 0.0207  lr:0.000001
[ Sun Jul 14 08:38:58 2024 ] 	Batch(1100/6809) done. Loss: 0.1933  lr:0.000001
[ Sun Jul 14 08:39:16 2024 ] 	Batch(1200/6809) done. Loss: 0.0543  lr:0.000001
[ Sun Jul 14 08:39:34 2024 ] 	Batch(1300/6809) done. Loss: 0.0132  lr:0.000001
[ Sun Jul 14 08:39:52 2024 ] 	Batch(1400/6809) done. Loss: 0.0446  lr:0.000001
[ Sun Jul 14 08:40:10 2024 ] 
Training: Epoch [44/50], Step [1499], Loss: 0.062276989221572876, Training Accuracy: 97.90833333333333
[ Sun Jul 14 08:40:10 2024 ] 	Batch(1500/6809) done. Loss: 0.0378  lr:0.000001
[ Sun Jul 14 08:40:28 2024 ] 	Batch(1600/6809) done. Loss: 0.1892  lr:0.000001
[ Sun Jul 14 08:40:46 2024 ] 	Batch(1700/6809) done. Loss: 0.0211  lr:0.000001
[ Sun Jul 14 08:41:04 2024 ] 	Batch(1800/6809) done. Loss: 0.0523  lr:0.000001
[ Sun Jul 14 08:41:22 2024 ] 	Batch(1900/6809) done. Loss: 0.0226  lr:0.000001
[ Sun Jul 14 08:41:39 2024 ] 
Training: Epoch [44/50], Step [1999], Loss: 0.06534935534000397, Training Accuracy: 97.85000000000001
[ Sun Jul 14 08:41:40 2024 ] 	Batch(2000/6809) done. Loss: 0.2282  lr:0.000001
[ Sun Jul 14 08:41:58 2024 ] 	Batch(2100/6809) done. Loss: 0.0160  lr:0.000001
[ Sun Jul 14 08:42:16 2024 ] 	Batch(2200/6809) done. Loss: 0.2326  lr:0.000001
[ Sun Jul 14 08:42:34 2024 ] 	Batch(2300/6809) done. Loss: 0.1872  lr:0.000001
[ Sun Jul 14 08:42:52 2024 ] 	Batch(2400/6809) done. Loss: 0.0675  lr:0.000001
[ Sun Jul 14 08:43:09 2024 ] 
Training: Epoch [44/50], Step [2499], Loss: 0.06846016645431519, Training Accuracy: 97.75500000000001
[ Sun Jul 14 08:43:10 2024 ] 	Batch(2500/6809) done. Loss: 0.2718  lr:0.000001
[ Sun Jul 14 08:43:27 2024 ] 	Batch(2600/6809) done. Loss: 0.0194  lr:0.000001
[ Sun Jul 14 08:43:45 2024 ] 	Batch(2700/6809) done. Loss: 0.0248  lr:0.000001
[ Sun Jul 14 08:44:04 2024 ] 	Batch(2800/6809) done. Loss: 0.0359  lr:0.000001
[ Sun Jul 14 08:44:21 2024 ] 	Batch(2900/6809) done. Loss: 0.2800  lr:0.000001
[ Sun Jul 14 08:44:39 2024 ] 
Training: Epoch [44/50], Step [2999], Loss: 0.11577501147985458, Training Accuracy: 97.8125
[ Sun Jul 14 08:44:39 2024 ] 	Batch(3000/6809) done. Loss: 0.0029  lr:0.000001
[ Sun Jul 14 08:44:57 2024 ] 	Batch(3100/6809) done. Loss: 0.7419  lr:0.000001
[ Sun Jul 14 08:45:15 2024 ] 	Batch(3200/6809) done. Loss: 0.0162  lr:0.000001
[ Sun Jul 14 08:45:34 2024 ] 	Batch(3300/6809) done. Loss: 0.0447  lr:0.000001
[ Sun Jul 14 08:45:52 2024 ] 	Batch(3400/6809) done. Loss: 0.0163  lr:0.000001
[ Sun Jul 14 08:46:10 2024 ] 
Training: Epoch [44/50], Step [3499], Loss: 0.07501552253961563, Training Accuracy: 97.80357142857142
[ Sun Jul 14 08:46:10 2024 ] 	Batch(3500/6809) done. Loss: 0.0385  lr:0.000001
[ Sun Jul 14 08:46:28 2024 ] 	Batch(3600/6809) done. Loss: 0.0231  lr:0.000001
[ Sun Jul 14 08:46:46 2024 ] 	Batch(3700/6809) done. Loss: 0.0673  lr:0.000001
[ Sun Jul 14 08:47:04 2024 ] 	Batch(3800/6809) done. Loss: 0.0189  lr:0.000001
[ Sun Jul 14 08:47:22 2024 ] 	Batch(3900/6809) done. Loss: 0.0241  lr:0.000001
[ Sun Jul 14 08:47:39 2024 ] 
Training: Epoch [44/50], Step [3999], Loss: 0.12077847123146057, Training Accuracy: 97.85000000000001
[ Sun Jul 14 08:47:40 2024 ] 	Batch(4000/6809) done. Loss: 0.0644  lr:0.000001
[ Sun Jul 14 08:47:58 2024 ] 	Batch(4100/6809) done. Loss: 0.0501  lr:0.000001
[ Sun Jul 14 08:48:16 2024 ] 	Batch(4200/6809) done. Loss: 0.1999  lr:0.000001
[ Sun Jul 14 08:48:34 2024 ] 	Batch(4300/6809) done. Loss: 0.2698  lr:0.000001
[ Sun Jul 14 08:48:52 2024 ] 	Batch(4400/6809) done. Loss: 0.2560  lr:0.000001
[ Sun Jul 14 08:49:10 2024 ] 
Training: Epoch [44/50], Step [4499], Loss: 0.012800806201994419, Training Accuracy: 97.85555555555555
[ Sun Jul 14 08:49:10 2024 ] 	Batch(4500/6809) done. Loss: 0.0962  lr:0.000001
[ Sun Jul 14 08:49:28 2024 ] 	Batch(4600/6809) done. Loss: 0.0044  lr:0.000001
[ Sun Jul 14 08:49:46 2024 ] 	Batch(4700/6809) done. Loss: 0.0871  lr:0.000001
[ Sun Jul 14 08:50:04 2024 ] 	Batch(4800/6809) done. Loss: 0.0975  lr:0.000001
[ Sun Jul 14 08:50:21 2024 ] 	Batch(4900/6809) done. Loss: 0.0439  lr:0.000001
[ Sun Jul 14 08:50:39 2024 ] 
Training: Epoch [44/50], Step [4999], Loss: 0.5328400135040283, Training Accuracy: 97.8475
[ Sun Jul 14 08:50:39 2024 ] 	Batch(5000/6809) done. Loss: 0.2700  lr:0.000001
[ Sun Jul 14 08:50:57 2024 ] 	Batch(5100/6809) done. Loss: 0.0242  lr:0.000001
[ Sun Jul 14 08:51:15 2024 ] 	Batch(5200/6809) done. Loss: 0.0183  lr:0.000001
[ Sun Jul 14 08:51:33 2024 ] 	Batch(5300/6809) done. Loss: 0.1558  lr:0.000001
[ Sun Jul 14 08:51:51 2024 ] 	Batch(5400/6809) done. Loss: 0.0720  lr:0.000001
[ Sun Jul 14 08:52:09 2024 ] 
Training: Epoch [44/50], Step [5499], Loss: 0.0022906141821295023, Training Accuracy: 97.8409090909091
[ Sun Jul 14 08:52:09 2024 ] 	Batch(5500/6809) done. Loss: 0.0220  lr:0.000001
[ Sun Jul 14 08:52:27 2024 ] 	Batch(5600/6809) done. Loss: 0.0063  lr:0.000001
[ Sun Jul 14 08:52:45 2024 ] 	Batch(5700/6809) done. Loss: 0.0952  lr:0.000001
[ Sun Jul 14 08:53:03 2024 ] 	Batch(5800/6809) done. Loss: 0.0839  lr:0.000001
[ Sun Jul 14 08:53:21 2024 ] 	Batch(5900/6809) done. Loss: 0.0985  lr:0.000001
[ Sun Jul 14 08:53:38 2024 ] 
Training: Epoch [44/50], Step [5999], Loss: 0.008326544426381588, Training Accuracy: 97.80208333333333
[ Sun Jul 14 08:53:39 2024 ] 	Batch(6000/6809) done. Loss: 0.0672  lr:0.000001
[ Sun Jul 14 08:53:57 2024 ] 	Batch(6100/6809) done. Loss: 0.1023  lr:0.000001
[ Sun Jul 14 08:54:15 2024 ] 	Batch(6200/6809) done. Loss: 0.0079  lr:0.000001
[ Sun Jul 14 08:54:32 2024 ] 	Batch(6300/6809) done. Loss: 0.0890  lr:0.000001
[ Sun Jul 14 08:54:51 2024 ] 	Batch(6400/6809) done. Loss: 0.0245  lr:0.000001
[ Sun Jul 14 08:55:08 2024 ] 
Training: Epoch [44/50], Step [6499], Loss: 0.06507691740989685, Training Accuracy: 97.79615384615384
[ Sun Jul 14 08:55:08 2024 ] 	Batch(6500/6809) done. Loss: 0.0055  lr:0.000001
[ Sun Jul 14 08:55:26 2024 ] 	Batch(6600/6809) done. Loss: 0.0945  lr:0.000001
[ Sun Jul 14 08:55:44 2024 ] 	Batch(6700/6809) done. Loss: 0.0377  lr:0.000001
[ Sun Jul 14 08:56:02 2024 ] 	Batch(6800/6809) done. Loss: 0.0514  lr:0.000001
[ Sun Jul 14 08:56:04 2024 ] 	Mean training loss: 0.0906.
[ Sun Jul 14 08:56:04 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 08:56:04 2024 ] Training epoch: 46
[ Sun Jul 14 08:56:05 2024 ] 	Batch(0/6809) done. Loss: 0.0129  lr:0.000001
[ Sun Jul 14 08:56:23 2024 ] 	Batch(100/6809) done. Loss: 0.1074  lr:0.000001
[ Sun Jul 14 08:56:41 2024 ] 	Batch(200/6809) done. Loss: 0.5468  lr:0.000001
[ Sun Jul 14 08:57:00 2024 ] 	Batch(300/6809) done. Loss: 0.0194  lr:0.000001
[ Sun Jul 14 08:57:18 2024 ] 	Batch(400/6809) done. Loss: 0.1483  lr:0.000001
[ Sun Jul 14 08:57:36 2024 ] 
Training: Epoch [45/50], Step [499], Loss: 0.021369991824030876, Training Accuracy: 98.15
[ Sun Jul 14 08:57:36 2024 ] 	Batch(500/6809) done. Loss: 0.0264  lr:0.000001
[ Sun Jul 14 08:57:55 2024 ] 	Batch(600/6809) done. Loss: 0.1012  lr:0.000001
[ Sun Jul 14 08:58:13 2024 ] 	Batch(700/6809) done. Loss: 0.1613  lr:0.000001
[ Sun Jul 14 08:58:32 2024 ] 	Batch(800/6809) done. Loss: 0.0379  lr:0.000001
[ Sun Jul 14 08:58:50 2024 ] 	Batch(900/6809) done. Loss: 0.1230  lr:0.000001
[ Sun Jul 14 08:59:07 2024 ] 
Training: Epoch [45/50], Step [999], Loss: 0.0090687470510602, Training Accuracy: 98.1375
[ Sun Jul 14 08:59:08 2024 ] 	Batch(1000/6809) done. Loss: 0.1921  lr:0.000001
[ Sun Jul 14 08:59:26 2024 ] 	Batch(1100/6809) done. Loss: 0.4017  lr:0.000001
[ Sun Jul 14 08:59:44 2024 ] 	Batch(1200/6809) done. Loss: 0.2418  lr:0.000001
[ Sun Jul 14 09:00:02 2024 ] 	Batch(1300/6809) done. Loss: 0.1639  lr:0.000001
[ Sun Jul 14 09:00:20 2024 ] 	Batch(1400/6809) done. Loss: 0.2841  lr:0.000001
[ Sun Jul 14 09:00:38 2024 ] 
Training: Epoch [45/50], Step [1499], Loss: 0.058211445808410645, Training Accuracy: 98.03333333333333
[ Sun Jul 14 09:00:38 2024 ] 	Batch(1500/6809) done. Loss: 0.0753  lr:0.000001
[ Sun Jul 14 09:00:56 2024 ] 	Batch(1600/6809) done. Loss: 0.0240  lr:0.000001
[ Sun Jul 14 09:01:14 2024 ] 	Batch(1700/6809) done. Loss: 0.0110  lr:0.000001
[ Sun Jul 14 09:01:33 2024 ] 	Batch(1800/6809) done. Loss: 0.1174  lr:0.000001
[ Sun Jul 14 09:01:51 2024 ] 	Batch(1900/6809) done. Loss: 0.1087  lr:0.000001
[ Sun Jul 14 09:02:10 2024 ] 
Training: Epoch [45/50], Step [1999], Loss: 0.03211420774459839, Training Accuracy: 98.04375
[ Sun Jul 14 09:02:10 2024 ] 	Batch(2000/6809) done. Loss: 0.4782  lr:0.000001
[ Sun Jul 14 09:02:29 2024 ] 	Batch(2100/6809) done. Loss: 0.0643  lr:0.000001
[ Sun Jul 14 09:02:47 2024 ] 	Batch(2200/6809) done. Loss: 0.0439  lr:0.000001
[ Sun Jul 14 09:03:05 2024 ] 	Batch(2300/6809) done. Loss: 0.0813  lr:0.000001
[ Sun Jul 14 09:03:24 2024 ] 	Batch(2400/6809) done. Loss: 0.0304  lr:0.000001
[ Sun Jul 14 09:03:43 2024 ] 
Training: Epoch [45/50], Step [2499], Loss: 0.03235134109854698, Training Accuracy: 97.985
[ Sun Jul 14 09:03:43 2024 ] 	Batch(2500/6809) done. Loss: 0.3504  lr:0.000001
[ Sun Jul 14 09:04:02 2024 ] 	Batch(2600/6809) done. Loss: 0.0636  lr:0.000001
[ Sun Jul 14 09:04:21 2024 ] 	Batch(2700/6809) done. Loss: 0.0762  lr:0.000001
[ Sun Jul 14 09:04:40 2024 ] 	Batch(2800/6809) done. Loss: 0.0398  lr:0.000001
[ Sun Jul 14 09:04:58 2024 ] 	Batch(2900/6809) done. Loss: 0.1482  lr:0.000001
[ Sun Jul 14 09:05:16 2024 ] 
Training: Epoch [45/50], Step [2999], Loss: 0.19044053554534912, Training Accuracy: 97.96666666666667
[ Sun Jul 14 09:05:17 2024 ] 	Batch(3000/6809) done. Loss: 0.0112  lr:0.000001
[ Sun Jul 14 09:05:35 2024 ] 	Batch(3100/6809) done. Loss: 0.0156  lr:0.000001
[ Sun Jul 14 09:05:53 2024 ] 	Batch(3200/6809) done. Loss: 0.0132  lr:0.000001
[ Sun Jul 14 09:06:12 2024 ] 	Batch(3300/6809) done. Loss: 0.1521  lr:0.000001
[ Sun Jul 14 09:06:30 2024 ] 	Batch(3400/6809) done. Loss: 0.3430  lr:0.000001
[ Sun Jul 14 09:06:48 2024 ] 
Training: Epoch [45/50], Step [3499], Loss: 0.06382782012224197, Training Accuracy: 97.91785714285714
[ Sun Jul 14 09:06:48 2024 ] 	Batch(3500/6809) done. Loss: 0.0192  lr:0.000001
[ Sun Jul 14 09:07:06 2024 ] 	Batch(3600/6809) done. Loss: 0.0117  lr:0.000001
[ Sun Jul 14 09:07:24 2024 ] 	Batch(3700/6809) done. Loss: 0.0152  lr:0.000001
[ Sun Jul 14 09:07:43 2024 ] 	Batch(3800/6809) done. Loss: 0.0691  lr:0.000001
[ Sun Jul 14 09:08:01 2024 ] 	Batch(3900/6809) done. Loss: 0.1147  lr:0.000001
[ Sun Jul 14 09:08:20 2024 ] 
Training: Epoch [45/50], Step [3999], Loss: 0.007752005476504564, Training Accuracy: 97.975
[ Sun Jul 14 09:08:20 2024 ] 	Batch(4000/6809) done. Loss: 0.0689  lr:0.000001
[ Sun Jul 14 09:08:39 2024 ] 	Batch(4100/6809) done. Loss: 0.0108  lr:0.000001
[ Sun Jul 14 09:08:57 2024 ] 	Batch(4200/6809) done. Loss: 0.0572  lr:0.000001
[ Sun Jul 14 09:09:16 2024 ] 	Batch(4300/6809) done. Loss: 0.0455  lr:0.000001
[ Sun Jul 14 09:09:34 2024 ] 	Batch(4400/6809) done. Loss: 0.3418  lr:0.000001
[ Sun Jul 14 09:09:53 2024 ] 
Training: Epoch [45/50], Step [4499], Loss: 0.008009819313883781, Training Accuracy: 97.96111111111111
[ Sun Jul 14 09:09:53 2024 ] 	Batch(4500/6809) done. Loss: 0.0152  lr:0.000001
[ Sun Jul 14 09:10:11 2024 ] 	Batch(4600/6809) done. Loss: 0.0201  lr:0.000001
[ Sun Jul 14 09:10:29 2024 ] 	Batch(4700/6809) done. Loss: 0.0688  lr:0.000001
[ Sun Jul 14 09:10:47 2024 ] 	Batch(4800/6809) done. Loss: 0.0401  lr:0.000001
[ Sun Jul 14 09:11:06 2024 ] 	Batch(4900/6809) done. Loss: 0.0639  lr:0.000001
[ Sun Jul 14 09:11:24 2024 ] 
Training: Epoch [45/50], Step [4999], Loss: 0.07153470814228058, Training Accuracy: 97.955
[ Sun Jul 14 09:11:24 2024 ] 	Batch(5000/6809) done. Loss: 0.0118  lr:0.000001
[ Sun Jul 14 09:11:42 2024 ] 	Batch(5100/6809) done. Loss: 0.0119  lr:0.000001
[ Sun Jul 14 09:12:00 2024 ] 	Batch(5200/6809) done. Loss: 0.0353  lr:0.000001
[ Sun Jul 14 09:12:18 2024 ] 	Batch(5300/6809) done. Loss: 0.0308  lr:0.000001
[ Sun Jul 14 09:12:36 2024 ] 	Batch(5400/6809) done. Loss: 0.0180  lr:0.000001
[ Sun Jul 14 09:12:53 2024 ] 
Training: Epoch [45/50], Step [5499], Loss: 0.10019506514072418, Training Accuracy: 97.9590909090909
[ Sun Jul 14 09:12:54 2024 ] 	Batch(5500/6809) done. Loss: 0.0402  lr:0.000001
[ Sun Jul 14 09:13:12 2024 ] 	Batch(5600/6809) done. Loss: 0.0192  lr:0.000001
[ Sun Jul 14 09:13:30 2024 ] 	Batch(5700/6809) done. Loss: 0.0470  lr:0.000001
[ Sun Jul 14 09:13:49 2024 ] 	Batch(5800/6809) done. Loss: 0.1199  lr:0.000001
[ Sun Jul 14 09:14:07 2024 ] 	Batch(5900/6809) done. Loss: 0.0577  lr:0.000001
[ Sun Jul 14 09:14:25 2024 ] 
Training: Epoch [45/50], Step [5999], Loss: 0.05036366358399391, Training Accuracy: 97.96458333333334
[ Sun Jul 14 09:14:25 2024 ] 	Batch(6000/6809) done. Loss: 0.0789  lr:0.000001
[ Sun Jul 14 09:14:43 2024 ] 	Batch(6100/6809) done. Loss: 0.0362  lr:0.000001
[ Sun Jul 14 09:15:01 2024 ] 	Batch(6200/6809) done. Loss: 0.0426  lr:0.000001
[ Sun Jul 14 09:15:19 2024 ] 	Batch(6300/6809) done. Loss: 0.0060  lr:0.000001
[ Sun Jul 14 09:15:37 2024 ] 	Batch(6400/6809) done. Loss: 0.1015  lr:0.000001
[ Sun Jul 14 09:15:55 2024 ] 
Training: Epoch [45/50], Step [6499], Loss: 0.010283799842000008, Training Accuracy: 97.94615384615385
[ Sun Jul 14 09:15:56 2024 ] 	Batch(6500/6809) done. Loss: 0.0452  lr:0.000001
[ Sun Jul 14 09:16:14 2024 ] 	Batch(6600/6809) done. Loss: 0.0222  lr:0.000001
[ Sun Jul 14 09:16:33 2024 ] 	Batch(6700/6809) done. Loss: 0.0238  lr:0.000001
[ Sun Jul 14 09:16:51 2024 ] 	Batch(6800/6809) done. Loss: 0.0635  lr:0.000001
[ Sun Jul 14 09:16:53 2024 ] 	Mean training loss: 0.0882.
[ Sun Jul 14 09:16:53 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 09:16:53 2024 ] Training epoch: 47
[ Sun Jul 14 09:16:54 2024 ] 	Batch(0/6809) done. Loss: 0.0221  lr:0.000001
[ Sun Jul 14 09:17:11 2024 ] 	Batch(100/6809) done. Loss: 0.0128  lr:0.000001
[ Sun Jul 14 09:17:29 2024 ] 	Batch(200/6809) done. Loss: 0.0040  lr:0.000001
[ Sun Jul 14 09:17:47 2024 ] 	Batch(300/6809) done. Loss: 0.1781  lr:0.000001
[ Sun Jul 14 09:18:05 2024 ] 	Batch(400/6809) done. Loss: 0.0122  lr:0.000001
[ Sun Jul 14 09:18:23 2024 ] 
Training: Epoch [46/50], Step [499], Loss: 0.05672267824411392, Training Accuracy: 98.075
[ Sun Jul 14 09:18:23 2024 ] 	Batch(500/6809) done. Loss: 0.1193  lr:0.000001
[ Sun Jul 14 09:18:41 2024 ] 	Batch(600/6809) done. Loss: 0.0253  lr:0.000001
[ Sun Jul 14 09:18:59 2024 ] 	Batch(700/6809) done. Loss: 0.6967  lr:0.000001
[ Sun Jul 14 09:19:17 2024 ] 	Batch(800/6809) done. Loss: 0.0522  lr:0.000001
[ Sun Jul 14 09:19:36 2024 ] 	Batch(900/6809) done. Loss: 0.0086  lr:0.000001
[ Sun Jul 14 09:19:54 2024 ] 
Training: Epoch [46/50], Step [999], Loss: 0.18010400235652924, Training Accuracy: 98.0
[ Sun Jul 14 09:19:54 2024 ] 	Batch(1000/6809) done. Loss: 0.0864  lr:0.000001
[ Sun Jul 14 09:20:12 2024 ] 	Batch(1100/6809) done. Loss: 0.0141  lr:0.000001
[ Sun Jul 14 09:20:30 2024 ] 	Batch(1200/6809) done. Loss: 0.0779  lr:0.000001
[ Sun Jul 14 09:20:49 2024 ] 	Batch(1300/6809) done. Loss: 0.3871  lr:0.000001
[ Sun Jul 14 09:21:07 2024 ] 	Batch(1400/6809) done. Loss: 0.1752  lr:0.000001
[ Sun Jul 14 09:21:26 2024 ] 
Training: Epoch [46/50], Step [1499], Loss: 0.22052831947803497, Training Accuracy: 97.95
[ Sun Jul 14 09:21:26 2024 ] 	Batch(1500/6809) done. Loss: 0.3400  lr:0.000001
[ Sun Jul 14 09:21:45 2024 ] 	Batch(1600/6809) done. Loss: 0.0048  lr:0.000001
[ Sun Jul 14 09:22:03 2024 ] 	Batch(1700/6809) done. Loss: 0.3147  lr:0.000001
[ Sun Jul 14 09:22:21 2024 ] 	Batch(1800/6809) done. Loss: 0.3253  lr:0.000001
[ Sun Jul 14 09:22:39 2024 ] 	Batch(1900/6809) done. Loss: 0.0910  lr:0.000001
[ Sun Jul 14 09:22:56 2024 ] 
Training: Epoch [46/50], Step [1999], Loss: 0.029364479705691338, Training Accuracy: 97.9375
[ Sun Jul 14 09:22:57 2024 ] 	Batch(2000/6809) done. Loss: 0.0961  lr:0.000001
[ Sun Jul 14 09:23:15 2024 ] 	Batch(2100/6809) done. Loss: 0.0045  lr:0.000001
[ Sun Jul 14 09:23:32 2024 ] 	Batch(2200/6809) done. Loss: 0.0142  lr:0.000001
[ Sun Jul 14 09:23:50 2024 ] 	Batch(2300/6809) done. Loss: 0.0376  lr:0.000001
[ Sun Jul 14 09:24:08 2024 ] 	Batch(2400/6809) done. Loss: 0.0368  lr:0.000001
[ Sun Jul 14 09:24:26 2024 ] 
Training: Epoch [46/50], Step [2499], Loss: 0.06961993873119354, Training Accuracy: 97.955
[ Sun Jul 14 09:24:26 2024 ] 	Batch(2500/6809) done. Loss: 0.0117  lr:0.000001
[ Sun Jul 14 09:24:44 2024 ] 	Batch(2600/6809) done. Loss: 0.0366  lr:0.000001
[ Sun Jul 14 09:25:02 2024 ] 	Batch(2700/6809) done. Loss: 0.0513  lr:0.000001
[ Sun Jul 14 09:25:20 2024 ] 	Batch(2800/6809) done. Loss: 0.0047  lr:0.000001
[ Sun Jul 14 09:25:38 2024 ] 	Batch(2900/6809) done. Loss: 0.0924  lr:0.000001
[ Sun Jul 14 09:25:56 2024 ] 
Training: Epoch [46/50], Step [2999], Loss: 0.2728393077850342, Training Accuracy: 97.88333333333334
[ Sun Jul 14 09:25:56 2024 ] 	Batch(3000/6809) done. Loss: 0.0079  lr:0.000001
[ Sun Jul 14 09:26:14 2024 ] 	Batch(3100/6809) done. Loss: 0.0796  lr:0.000001
[ Sun Jul 14 09:26:32 2024 ] 	Batch(3200/6809) done. Loss: 0.0059  lr:0.000001
[ Sun Jul 14 09:26:50 2024 ] 	Batch(3300/6809) done. Loss: 0.0015  lr:0.000001
[ Sun Jul 14 09:27:08 2024 ] 	Batch(3400/6809) done. Loss: 0.1702  lr:0.000001
[ Sun Jul 14 09:27:26 2024 ] 
Training: Epoch [46/50], Step [3499], Loss: 0.004021562170237303, Training Accuracy: 97.87142857142858
[ Sun Jul 14 09:27:26 2024 ] 	Batch(3500/6809) done. Loss: 0.0516  lr:0.000001
[ Sun Jul 14 09:27:45 2024 ] 	Batch(3600/6809) done. Loss: 0.0284  lr:0.000001
[ Sun Jul 14 09:28:04 2024 ] 	Batch(3700/6809) done. Loss: 0.0108  lr:0.000001
[ Sun Jul 14 09:28:22 2024 ] 	Batch(3800/6809) done. Loss: 0.0784  lr:0.000001
[ Sun Jul 14 09:28:41 2024 ] 	Batch(3900/6809) done. Loss: 0.0268  lr:0.000001
[ Sun Jul 14 09:28:59 2024 ] 
Training: Epoch [46/50], Step [3999], Loss: 0.011396035552024841, Training Accuracy: 97.85625
[ Sun Jul 14 09:28:59 2024 ] 	Batch(4000/6809) done. Loss: 0.0448  lr:0.000001
[ Sun Jul 14 09:29:18 2024 ] 	Batch(4100/6809) done. Loss: 0.0139  lr:0.000001
[ Sun Jul 14 09:29:36 2024 ] 	Batch(4200/6809) done. Loss: 0.3942  lr:0.000001
[ Sun Jul 14 09:29:55 2024 ] 	Batch(4300/6809) done. Loss: 0.1298  lr:0.000001
[ Sun Jul 14 09:30:14 2024 ] 	Batch(4400/6809) done. Loss: 0.0975  lr:0.000001
[ Sun Jul 14 09:30:32 2024 ] 
Training: Epoch [46/50], Step [4499], Loss: 0.20261266827583313, Training Accuracy: 97.84166666666667
[ Sun Jul 14 09:30:32 2024 ] 	Batch(4500/6809) done. Loss: 0.0493  lr:0.000001
[ Sun Jul 14 09:30:50 2024 ] 	Batch(4600/6809) done. Loss: 0.0244  lr:0.000001
[ Sun Jul 14 09:31:08 2024 ] 	Batch(4700/6809) done. Loss: 0.0238  lr:0.000001
[ Sun Jul 14 09:31:26 2024 ] 	Batch(4800/6809) done. Loss: 0.1810  lr:0.000001
[ Sun Jul 14 09:31:44 2024 ] 	Batch(4900/6809) done. Loss: 0.0261  lr:0.000001
[ Sun Jul 14 09:32:01 2024 ] 
Training: Epoch [46/50], Step [4999], Loss: 0.040358755737543106, Training Accuracy: 97.91
[ Sun Jul 14 09:32:02 2024 ] 	Batch(5000/6809) done. Loss: 0.0487  lr:0.000001
[ Sun Jul 14 09:32:20 2024 ] 	Batch(5100/6809) done. Loss: 0.0066  lr:0.000001
[ Sun Jul 14 09:32:38 2024 ] 	Batch(5200/6809) done. Loss: 0.1449  lr:0.000001
[ Sun Jul 14 09:32:56 2024 ] 	Batch(5300/6809) done. Loss: 0.0199  lr:0.000001
[ Sun Jul 14 09:33:14 2024 ] 	Batch(5400/6809) done. Loss: 0.1658  lr:0.000001
[ Sun Jul 14 09:33:31 2024 ] 
Training: Epoch [46/50], Step [5499], Loss: 0.032366760075092316, Training Accuracy: 97.85227272727272
[ Sun Jul 14 09:33:31 2024 ] 	Batch(5500/6809) done. Loss: 0.1100  lr:0.000001
[ Sun Jul 14 09:33:49 2024 ] 	Batch(5600/6809) done. Loss: 0.1473  lr:0.000001
[ Sun Jul 14 09:34:07 2024 ] 	Batch(5700/6809) done. Loss: 0.0401  lr:0.000001
[ Sun Jul 14 09:34:25 2024 ] 	Batch(5800/6809) done. Loss: 0.0868  lr:0.000001
[ Sun Jul 14 09:34:43 2024 ] 	Batch(5900/6809) done. Loss: 0.0544  lr:0.000001
[ Sun Jul 14 09:35:01 2024 ] 
Training: Epoch [46/50], Step [5999], Loss: 0.26182955503463745, Training Accuracy: 97.86666666666667
[ Sun Jul 14 09:35:01 2024 ] 	Batch(6000/6809) done. Loss: 0.0031  lr:0.000001
[ Sun Jul 14 09:35:20 2024 ] 	Batch(6100/6809) done. Loss: 0.0469  lr:0.000001
[ Sun Jul 14 09:35:38 2024 ] 	Batch(6200/6809) done. Loss: 0.0158  lr:0.000001
[ Sun Jul 14 09:35:56 2024 ] 	Batch(6300/6809) done. Loss: 0.0611  lr:0.000001
[ Sun Jul 14 09:36:14 2024 ] 	Batch(6400/6809) done. Loss: 0.0385  lr:0.000001
[ Sun Jul 14 09:36:32 2024 ] 
Training: Epoch [46/50], Step [6499], Loss: 0.09656965732574463, Training Accuracy: 97.87307692307692
[ Sun Jul 14 09:36:32 2024 ] 	Batch(6500/6809) done. Loss: 0.0039  lr:0.000001
[ Sun Jul 14 09:36:50 2024 ] 	Batch(6600/6809) done. Loss: 0.0394  lr:0.000001
[ Sun Jul 14 09:37:08 2024 ] 	Batch(6700/6809) done. Loss: 0.0723  lr:0.000001
[ Sun Jul 14 09:37:26 2024 ] 	Batch(6800/6809) done. Loss: 0.0308  lr:0.000001
[ Sun Jul 14 09:37:27 2024 ] 	Mean training loss: 0.0892.
[ Sun Jul 14 09:37:27 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 09:37:27 2024 ] Training epoch: 48
[ Sun Jul 14 09:37:28 2024 ] 	Batch(0/6809) done. Loss: 0.0849  lr:0.000001
[ Sun Jul 14 09:37:46 2024 ] 	Batch(100/6809) done. Loss: 0.1756  lr:0.000001
[ Sun Jul 14 09:38:05 2024 ] 	Batch(200/6809) done. Loss: 0.0096  lr:0.000001
[ Sun Jul 14 09:38:22 2024 ] 	Batch(300/6809) done. Loss: 0.1171  lr:0.000001
[ Sun Jul 14 09:38:40 2024 ] 	Batch(400/6809) done. Loss: 0.0036  lr:0.000001
[ Sun Jul 14 09:38:58 2024 ] 
Training: Epoch [47/50], Step [499], Loss: 0.022998381406068802, Training Accuracy: 97.95
[ Sun Jul 14 09:38:58 2024 ] 	Batch(500/6809) done. Loss: 0.0632  lr:0.000001
[ Sun Jul 14 09:39:16 2024 ] 	Batch(600/6809) done. Loss: 0.0099  lr:0.000001
[ Sun Jul 14 09:39:34 2024 ] 	Batch(700/6809) done. Loss: 0.0541  lr:0.000001
[ Sun Jul 14 09:39:52 2024 ] 	Batch(800/6809) done. Loss: 0.0318  lr:0.000001
[ Sun Jul 14 09:40:10 2024 ] 	Batch(900/6809) done. Loss: 0.2181  lr:0.000001
[ Sun Jul 14 09:40:28 2024 ] 
Training: Epoch [47/50], Step [999], Loss: 0.2746577262878418, Training Accuracy: 97.89999999999999
[ Sun Jul 14 09:40:28 2024 ] 	Batch(1000/6809) done. Loss: 0.0134  lr:0.000001
[ Sun Jul 14 09:40:46 2024 ] 	Batch(1100/6809) done. Loss: 0.0054  lr:0.000001
[ Sun Jul 14 09:41:04 2024 ] 	Batch(1200/6809) done. Loss: 0.2572  lr:0.000001
[ Sun Jul 14 09:41:21 2024 ] 	Batch(1300/6809) done. Loss: 0.0290  lr:0.000001
[ Sun Jul 14 09:41:39 2024 ] 	Batch(1400/6809) done. Loss: 0.1214  lr:0.000001
[ Sun Jul 14 09:41:57 2024 ] 
Training: Epoch [47/50], Step [1499], Loss: 0.10392719507217407, Training Accuracy: 98.08333333333333
[ Sun Jul 14 09:41:57 2024 ] 	Batch(1500/6809) done. Loss: 0.0708  lr:0.000001
[ Sun Jul 14 09:42:16 2024 ] 	Batch(1600/6809) done. Loss: 0.2415  lr:0.000001
[ Sun Jul 14 09:42:34 2024 ] 	Batch(1700/6809) done. Loss: 0.1075  lr:0.000001
[ Sun Jul 14 09:42:52 2024 ] 	Batch(1800/6809) done. Loss: 0.1623  lr:0.000001
[ Sun Jul 14 09:43:10 2024 ] 	Batch(1900/6809) done. Loss: 0.0793  lr:0.000001
[ Sun Jul 14 09:43:27 2024 ] 
Training: Epoch [47/50], Step [1999], Loss: 0.010359613224864006, Training Accuracy: 97.95625
[ Sun Jul 14 09:43:28 2024 ] 	Batch(2000/6809) done. Loss: 0.0320  lr:0.000001
[ Sun Jul 14 09:43:46 2024 ] 	Batch(2100/6809) done. Loss: 0.0155  lr:0.000001
[ Sun Jul 14 09:44:05 2024 ] 	Batch(2200/6809) done. Loss: 0.2479  lr:0.000001
[ Sun Jul 14 09:44:24 2024 ] 	Batch(2300/6809) done. Loss: 0.0521  lr:0.000001
[ Sun Jul 14 09:44:42 2024 ] 	Batch(2400/6809) done. Loss: 0.0472  lr:0.000001
[ Sun Jul 14 09:45:00 2024 ] 
Training: Epoch [47/50], Step [2499], Loss: 0.10269828140735626, Training Accuracy: 97.96000000000001
[ Sun Jul 14 09:45:01 2024 ] 	Batch(2500/6809) done. Loss: 0.1473  lr:0.000001
[ Sun Jul 14 09:45:19 2024 ] 	Batch(2600/6809) done. Loss: 0.0396  lr:0.000001
[ Sun Jul 14 09:45:37 2024 ] 	Batch(2700/6809) done. Loss: 0.0251  lr:0.000001
[ Sun Jul 14 09:45:55 2024 ] 	Batch(2800/6809) done. Loss: 0.0235  lr:0.000001
[ Sun Jul 14 09:46:13 2024 ] 	Batch(2900/6809) done. Loss: 0.0514  lr:0.000001
[ Sun Jul 14 09:46:31 2024 ] 
Training: Epoch [47/50], Step [2999], Loss: 0.09768949449062347, Training Accuracy: 97.95416666666667
[ Sun Jul 14 09:46:31 2024 ] 	Batch(3000/6809) done. Loss: 0.1215  lr:0.000001
[ Sun Jul 14 09:46:50 2024 ] 	Batch(3100/6809) done. Loss: 0.0153  lr:0.000001
[ Sun Jul 14 09:47:08 2024 ] 	Batch(3200/6809) done. Loss: 0.4050  lr:0.000001
[ Sun Jul 14 09:47:26 2024 ] 	Batch(3300/6809) done. Loss: 0.0489  lr:0.000001
[ Sun Jul 14 09:47:44 2024 ] 	Batch(3400/6809) done. Loss: 0.2832  lr:0.000001
[ Sun Jul 14 09:48:02 2024 ] 
Training: Epoch [47/50], Step [3499], Loss: 0.19501885771751404, Training Accuracy: 97.97142857142858
[ Sun Jul 14 09:48:02 2024 ] 	Batch(3500/6809) done. Loss: 0.1346  lr:0.000001
[ Sun Jul 14 09:48:20 2024 ] 	Batch(3600/6809) done. Loss: 0.0035  lr:0.000001
[ Sun Jul 14 09:48:38 2024 ] 	Batch(3700/6809) done. Loss: 0.2591  lr:0.000001
[ Sun Jul 14 09:48:56 2024 ] 	Batch(3800/6809) done. Loss: 0.0031  lr:0.000001
[ Sun Jul 14 09:49:14 2024 ] 	Batch(3900/6809) done. Loss: 0.0026  lr:0.000001
[ Sun Jul 14 09:49:32 2024 ] 
Training: Epoch [47/50], Step [3999], Loss: 0.026764657348394394, Training Accuracy: 97.984375
[ Sun Jul 14 09:49:32 2024 ] 	Batch(4000/6809) done. Loss: 0.0803  lr:0.000001
[ Sun Jul 14 09:49:50 2024 ] 	Batch(4100/6809) done. Loss: 0.2623  lr:0.000001
[ Sun Jul 14 09:50:08 2024 ] 	Batch(4200/6809) done. Loss: 0.0094  lr:0.000001
[ Sun Jul 14 09:50:26 2024 ] 	Batch(4300/6809) done. Loss: 0.0489  lr:0.000001
[ Sun Jul 14 09:50:44 2024 ] 	Batch(4400/6809) done. Loss: 0.0085  lr:0.000001
[ Sun Jul 14 09:51:01 2024 ] 
Training: Epoch [47/50], Step [4499], Loss: 0.13216140866279602, Training Accuracy: 98.01666666666667
[ Sun Jul 14 09:51:02 2024 ] 	Batch(4500/6809) done. Loss: 0.0464  lr:0.000001
[ Sun Jul 14 09:51:20 2024 ] 	Batch(4600/6809) done. Loss: 0.0030  lr:0.000001
[ Sun Jul 14 09:51:37 2024 ] 	Batch(4700/6809) done. Loss: 0.4645  lr:0.000001
[ Sun Jul 14 09:51:55 2024 ] 	Batch(4800/6809) done. Loss: 0.0719  lr:0.000001
[ Sun Jul 14 09:52:13 2024 ] 	Batch(4900/6809) done. Loss: 0.0057  lr:0.000001
[ Sun Jul 14 09:52:31 2024 ] 
Training: Epoch [47/50], Step [4999], Loss: 0.03149816021323204, Training Accuracy: 98.015
[ Sun Jul 14 09:52:31 2024 ] 	Batch(5000/6809) done. Loss: 0.5618  lr:0.000001
[ Sun Jul 14 09:52:49 2024 ] 	Batch(5100/6809) done. Loss: 0.0014  lr:0.000001
[ Sun Jul 14 09:53:07 2024 ] 	Batch(5200/6809) done. Loss: 0.0909  lr:0.000001
[ Sun Jul 14 09:53:25 2024 ] 	Batch(5300/6809) done. Loss: 0.0264  lr:0.000001
[ Sun Jul 14 09:53:43 2024 ] 	Batch(5400/6809) done. Loss: 0.0305  lr:0.000001
[ Sun Jul 14 09:54:01 2024 ] 
Training: Epoch [47/50], Step [5499], Loss: 0.04749395698308945, Training Accuracy: 98.00227272727273
[ Sun Jul 14 09:54:01 2024 ] 	Batch(5500/6809) done. Loss: 0.0597  lr:0.000001
[ Sun Jul 14 09:54:19 2024 ] 	Batch(5600/6809) done. Loss: 0.0028  lr:0.000001
[ Sun Jul 14 09:54:37 2024 ] 	Batch(5700/6809) done. Loss: 0.0319  lr:0.000001
[ Sun Jul 14 09:54:56 2024 ] 	Batch(5800/6809) done. Loss: 0.1065  lr:0.000001
[ Sun Jul 14 09:55:15 2024 ] 	Batch(5900/6809) done. Loss: 0.0359  lr:0.000001
[ Sun Jul 14 09:55:33 2024 ] 
Training: Epoch [47/50], Step [5999], Loss: 0.016800543293356895, Training Accuracy: 98.0125
[ Sun Jul 14 09:55:33 2024 ] 	Batch(6000/6809) done. Loss: 0.0157  lr:0.000001
[ Sun Jul 14 09:55:52 2024 ] 	Batch(6100/6809) done. Loss: 0.0517  lr:0.000001
[ Sun Jul 14 09:56:10 2024 ] 	Batch(6200/6809) done. Loss: 0.0218  lr:0.000001
[ Sun Jul 14 09:56:29 2024 ] 	Batch(6300/6809) done. Loss: 0.3093  lr:0.000001
[ Sun Jul 14 09:56:48 2024 ] 	Batch(6400/6809) done. Loss: 0.1402  lr:0.000001
[ Sun Jul 14 09:57:06 2024 ] 
Training: Epoch [47/50], Step [6499], Loss: 0.12505784630775452, Training Accuracy: 97.95384615384616
[ Sun Jul 14 09:57:06 2024 ] 	Batch(6500/6809) done. Loss: 0.0095  lr:0.000001
[ Sun Jul 14 09:57:24 2024 ] 	Batch(6600/6809) done. Loss: 0.0620  lr:0.000001
[ Sun Jul 14 09:57:42 2024 ] 	Batch(6700/6809) done. Loss: 0.1539  lr:0.000001
[ Sun Jul 14 09:58:00 2024 ] 	Batch(6800/6809) done. Loss: 0.3307  lr:0.000001
[ Sun Jul 14 09:58:02 2024 ] 	Mean training loss: 0.0867.
[ Sun Jul 14 09:58:02 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 09:58:02 2024 ] Training epoch: 49
[ Sun Jul 14 09:58:02 2024 ] 	Batch(0/6809) done. Loss: 0.1387  lr:0.000001
[ Sun Jul 14 09:58:21 2024 ] 	Batch(100/6809) done. Loss: 0.0257  lr:0.000001
[ Sun Jul 14 09:58:39 2024 ] 	Batch(200/6809) done. Loss: 0.0462  lr:0.000001
[ Sun Jul 14 09:58:58 2024 ] 	Batch(300/6809) done. Loss: 0.0473  lr:0.000001
[ Sun Jul 14 09:59:17 2024 ] 	Batch(400/6809) done. Loss: 0.0122  lr:0.000001
[ Sun Jul 14 09:59:35 2024 ] 
Training: Epoch [48/50], Step [499], Loss: 0.1797681301832199, Training Accuracy: 97.625
[ Sun Jul 14 09:59:35 2024 ] 	Batch(500/6809) done. Loss: 0.0722  lr:0.000001
[ Sun Jul 14 09:59:54 2024 ] 	Batch(600/6809) done. Loss: 0.1274  lr:0.000001
[ Sun Jul 14 10:00:12 2024 ] 	Batch(700/6809) done. Loss: 0.0053  lr:0.000001
[ Sun Jul 14 10:00:31 2024 ] 	Batch(800/6809) done. Loss: 0.0111  lr:0.000001
[ Sun Jul 14 10:00:49 2024 ] 	Batch(900/6809) done. Loss: 0.0322  lr:0.000001
[ Sun Jul 14 10:01:08 2024 ] 
Training: Epoch [48/50], Step [999], Loss: 0.10729055851697922, Training Accuracy: 97.6375
[ Sun Jul 14 10:01:08 2024 ] 	Batch(1000/6809) done. Loss: 0.0044  lr:0.000001
[ Sun Jul 14 10:01:26 2024 ] 	Batch(1100/6809) done. Loss: 0.1485  lr:0.000001
[ Sun Jul 14 10:01:45 2024 ] 	Batch(1200/6809) done. Loss: 0.0096  lr:0.000001
[ Sun Jul 14 10:02:04 2024 ] 	Batch(1300/6809) done. Loss: 0.1873  lr:0.000001
[ Sun Jul 14 10:02:22 2024 ] 	Batch(1400/6809) done. Loss: 0.1057  lr:0.000001
[ Sun Jul 14 10:02:41 2024 ] 
Training: Epoch [48/50], Step [1499], Loss: 0.03585559129714966, Training Accuracy: 97.78333333333333
[ Sun Jul 14 10:02:41 2024 ] 	Batch(1500/6809) done. Loss: 0.0854  lr:0.000001
[ Sun Jul 14 10:02:59 2024 ] 	Batch(1600/6809) done. Loss: 0.0515  lr:0.000001
[ Sun Jul 14 10:03:18 2024 ] 	Batch(1700/6809) done. Loss: 0.1463  lr:0.000001
[ Sun Jul 14 10:03:36 2024 ] 	Batch(1800/6809) done. Loss: 0.0243  lr:0.000001
[ Sun Jul 14 10:03:55 2024 ] 	Batch(1900/6809) done. Loss: 0.1515  lr:0.000001
[ Sun Jul 14 10:04:13 2024 ] 
Training: Epoch [48/50], Step [1999], Loss: 0.014957735314965248, Training Accuracy: 97.8125
[ Sun Jul 14 10:04:14 2024 ] 	Batch(2000/6809) done. Loss: 0.0049  lr:0.000001
[ Sun Jul 14 10:04:32 2024 ] 	Batch(2100/6809) done. Loss: 0.0914  lr:0.000001
[ Sun Jul 14 10:04:51 2024 ] 	Batch(2200/6809) done. Loss: 0.0667  lr:0.000001
[ Sun Jul 14 10:05:09 2024 ] 	Batch(2300/6809) done. Loss: 0.0188  lr:0.000001
[ Sun Jul 14 10:05:28 2024 ] 	Batch(2400/6809) done. Loss: 0.0360  lr:0.000001
[ Sun Jul 14 10:05:46 2024 ] 
Training: Epoch [48/50], Step [2499], Loss: 0.03523547202348709, Training Accuracy: 97.85000000000001
[ Sun Jul 14 10:05:46 2024 ] 	Batch(2500/6809) done. Loss: 0.1898  lr:0.000001
[ Sun Jul 14 10:06:04 2024 ] 	Batch(2600/6809) done. Loss: 0.0474  lr:0.000001
[ Sun Jul 14 10:06:22 2024 ] 	Batch(2700/6809) done. Loss: 0.0247  lr:0.000001
[ Sun Jul 14 10:06:40 2024 ] 	Batch(2800/6809) done. Loss: 0.0157  lr:0.000001
[ Sun Jul 14 10:06:58 2024 ] 	Batch(2900/6809) done. Loss: 0.0141  lr:0.000001
[ Sun Jul 14 10:07:15 2024 ] 
Training: Epoch [48/50], Step [2999], Loss: 0.03722885623574257, Training Accuracy: 97.91666666666666
[ Sun Jul 14 10:07:16 2024 ] 	Batch(3000/6809) done. Loss: 0.1990  lr:0.000001
[ Sun Jul 14 10:07:33 2024 ] 	Batch(3100/6809) done. Loss: 0.0389  lr:0.000001
[ Sun Jul 14 10:07:52 2024 ] 	Batch(3200/6809) done. Loss: 0.1230  lr:0.000001
[ Sun Jul 14 10:08:10 2024 ] 	Batch(3300/6809) done. Loss: 0.1399  lr:0.000001
[ Sun Jul 14 10:08:29 2024 ] 	Batch(3400/6809) done. Loss: 0.2999  lr:0.000001
[ Sun Jul 14 10:08:47 2024 ] 
Training: Epoch [48/50], Step [3499], Loss: 0.06725072115659714, Training Accuracy: 97.97142857142858
[ Sun Jul 14 10:08:47 2024 ] 	Batch(3500/6809) done. Loss: 0.0576  lr:0.000001
[ Sun Jul 14 10:09:06 2024 ] 	Batch(3600/6809) done. Loss: 0.1515  lr:0.000001
[ Sun Jul 14 10:09:24 2024 ] 	Batch(3700/6809) done. Loss: 0.1602  lr:0.000001
[ Sun Jul 14 10:09:42 2024 ] 	Batch(3800/6809) done. Loss: 0.0114  lr:0.000001
[ Sun Jul 14 10:09:59 2024 ] 	Batch(3900/6809) done. Loss: 0.0613  lr:0.000001
[ Sun Jul 14 10:10:17 2024 ] 
Training: Epoch [48/50], Step [3999], Loss: 0.011413896456360817, Training Accuracy: 97.96875
[ Sun Jul 14 10:10:18 2024 ] 	Batch(4000/6809) done. Loss: 0.0044  lr:0.000001
[ Sun Jul 14 10:10:35 2024 ] 	Batch(4100/6809) done. Loss: 0.0182  lr:0.000001
[ Sun Jul 14 10:10:53 2024 ] 	Batch(4200/6809) done. Loss: 0.2848  lr:0.000001
[ Sun Jul 14 10:11:11 2024 ] 	Batch(4300/6809) done. Loss: 0.1003  lr:0.000001
[ Sun Jul 14 10:11:29 2024 ] 	Batch(4400/6809) done. Loss: 0.0888  lr:0.000001
[ Sun Jul 14 10:11:47 2024 ] 
Training: Epoch [48/50], Step [4499], Loss: 0.10984378308057785, Training Accuracy: 97.93055555555556
[ Sun Jul 14 10:11:47 2024 ] 	Batch(4500/6809) done. Loss: 0.1727  lr:0.000001
[ Sun Jul 14 10:12:05 2024 ] 	Batch(4600/6809) done. Loss: 0.0343  lr:0.000001
[ Sun Jul 14 10:12:23 2024 ] 	Batch(4700/6809) done. Loss: 0.0293  lr:0.000001
[ Sun Jul 14 10:12:41 2024 ] 	Batch(4800/6809) done. Loss: 0.0372  lr:0.000001
[ Sun Jul 14 10:12:59 2024 ] 	Batch(4900/6809) done. Loss: 0.1138  lr:0.000001
[ Sun Jul 14 10:13:18 2024 ] 
Training: Epoch [48/50], Step [4999], Loss: 0.026407595723867416, Training Accuracy: 97.92999999999999
[ Sun Jul 14 10:13:18 2024 ] 	Batch(5000/6809) done. Loss: 0.0461  lr:0.000001
[ Sun Jul 14 10:13:36 2024 ] 	Batch(5100/6809) done. Loss: 0.0926  lr:0.000001
[ Sun Jul 14 10:13:55 2024 ] 	Batch(5200/6809) done. Loss: 0.0018  lr:0.000001
[ Sun Jul 14 10:14:14 2024 ] 	Batch(5300/6809) done. Loss: 0.0081  lr:0.000001
[ Sun Jul 14 10:14:32 2024 ] 	Batch(5400/6809) done. Loss: 0.0846  lr:0.000001
[ Sun Jul 14 10:14:50 2024 ] 
Training: Epoch [48/50], Step [5499], Loss: 0.02254137583076954, Training Accuracy: 97.89999999999999
[ Sun Jul 14 10:14:50 2024 ] 	Batch(5500/6809) done. Loss: 0.0606  lr:0.000001
[ Sun Jul 14 10:15:08 2024 ] 	Batch(5600/6809) done. Loss: 0.0482  lr:0.000001
[ Sun Jul 14 10:15:27 2024 ] 	Batch(5700/6809) done. Loss: 0.0057  lr:0.000001
[ Sun Jul 14 10:15:45 2024 ] 	Batch(5800/6809) done. Loss: 0.2013  lr:0.000001
[ Sun Jul 14 10:16:04 2024 ] 	Batch(5900/6809) done. Loss: 0.0023  lr:0.000001
[ Sun Jul 14 10:16:22 2024 ] 
Training: Epoch [48/50], Step [5999], Loss: 0.003445520531386137, Training Accuracy: 97.88958333333333
[ Sun Jul 14 10:16:23 2024 ] 	Batch(6000/6809) done. Loss: 0.1490  lr:0.000001
[ Sun Jul 14 10:16:40 2024 ] 	Batch(6100/6809) done. Loss: 0.0006  lr:0.000001
[ Sun Jul 14 10:16:58 2024 ] 	Batch(6200/6809) done. Loss: 0.0168  lr:0.000001
[ Sun Jul 14 10:17:16 2024 ] 	Batch(6300/6809) done. Loss: 0.1160  lr:0.000001
[ Sun Jul 14 10:17:34 2024 ] 	Batch(6400/6809) done. Loss: 0.1179  lr:0.000001
[ Sun Jul 14 10:17:52 2024 ] 
Training: Epoch [48/50], Step [6499], Loss: 0.030751528218388557, Training Accuracy: 97.89423076923077
[ Sun Jul 14 10:17:52 2024 ] 	Batch(6500/6809) done. Loss: 0.1257  lr:0.000001
[ Sun Jul 14 10:18:10 2024 ] 	Batch(6600/6809) done. Loss: 0.0140  lr:0.000001
[ Sun Jul 14 10:18:28 2024 ] 	Batch(6700/6809) done. Loss: 0.1293  lr:0.000001
[ Sun Jul 14 10:18:46 2024 ] 	Batch(6800/6809) done. Loss: 0.0314  lr:0.000001
[ Sun Jul 14 10:18:48 2024 ] 	Mean training loss: 0.0869.
[ Sun Jul 14 10:18:48 2024 ] 	Time consumption: [Data]01%, [Network]89%
[ Sun Jul 14 10:18:48 2024 ] Training epoch: 50
[ Sun Jul 14 10:18:48 2024 ] 	Batch(0/6809) done. Loss: 0.0966  lr:0.000001
[ Sun Jul 14 10:19:06 2024 ] 	Batch(100/6809) done. Loss: 0.0097  lr:0.000001
[ Sun Jul 14 10:19:24 2024 ] 	Batch(200/6809) done. Loss: 0.0345  lr:0.000001
[ Sun Jul 14 10:19:42 2024 ] 	Batch(300/6809) done. Loss: 0.2034  lr:0.000001
[ Sun Jul 14 10:20:00 2024 ] 	Batch(400/6809) done. Loss: 0.1150  lr:0.000001
[ Sun Jul 14 10:20:18 2024 ] 
Training: Epoch [49/50], Step [499], Loss: 0.20190028846263885, Training Accuracy: 97.45
[ Sun Jul 14 10:20:18 2024 ] 	Batch(500/6809) done. Loss: 0.0815  lr:0.000001
[ Sun Jul 14 10:20:36 2024 ] 	Batch(600/6809) done. Loss: 0.5218  lr:0.000001
[ Sun Jul 14 10:20:54 2024 ] 	Batch(700/6809) done. Loss: 0.0357  lr:0.000001
[ Sun Jul 14 10:21:12 2024 ] 	Batch(800/6809) done. Loss: 0.3102  lr:0.000001
[ Sun Jul 14 10:21:30 2024 ] 	Batch(900/6809) done. Loss: 0.0707  lr:0.000001
[ Sun Jul 14 10:21:48 2024 ] 
Training: Epoch [49/50], Step [999], Loss: 0.03193487972021103, Training Accuracy: 97.55
[ Sun Jul 14 10:21:48 2024 ] 	Batch(1000/6809) done. Loss: 0.0483  lr:0.000001
[ Sun Jul 14 10:22:06 2024 ] 	Batch(1100/6809) done. Loss: 0.0719  lr:0.000001
[ Sun Jul 14 10:22:24 2024 ] 	Batch(1200/6809) done. Loss: 0.0360  lr:0.000001
[ Sun Jul 14 10:22:42 2024 ] 	Batch(1300/6809) done. Loss: 0.0218  lr:0.000001
[ Sun Jul 14 10:23:00 2024 ] 	Batch(1400/6809) done. Loss: 0.0311  lr:0.000001
[ Sun Jul 14 10:23:17 2024 ] 
Training: Epoch [49/50], Step [1499], Loss: 0.0104317432269454, Training Accuracy: 97.65
[ Sun Jul 14 10:23:18 2024 ] 	Batch(1500/6809) done. Loss: 0.0139  lr:0.000001
[ Sun Jul 14 10:23:36 2024 ] 	Batch(1600/6809) done. Loss: 0.7751  lr:0.000001
[ Sun Jul 14 10:23:54 2024 ] 	Batch(1700/6809) done. Loss: 0.1440  lr:0.000001
[ Sun Jul 14 10:24:12 2024 ] 	Batch(1800/6809) done. Loss: 0.1411  lr:0.000001
[ Sun Jul 14 10:24:29 2024 ] 	Batch(1900/6809) done. Loss: 0.0193  lr:0.000001
[ Sun Jul 14 10:24:47 2024 ] 
Training: Epoch [49/50], Step [1999], Loss: 0.0072150686755776405, Training Accuracy: 97.70625
[ Sun Jul 14 10:24:47 2024 ] 	Batch(2000/6809) done. Loss: 0.0125  lr:0.000001
[ Sun Jul 14 10:25:05 2024 ] 	Batch(2100/6809) done. Loss: 0.1701  lr:0.000001
[ Sun Jul 14 10:25:23 2024 ] 	Batch(2200/6809) done. Loss: 0.0661  lr:0.000001
[ Sun Jul 14 10:25:41 2024 ] 	Batch(2300/6809) done. Loss: 0.0383  lr:0.000001
[ Sun Jul 14 10:25:59 2024 ] 	Batch(2400/6809) done. Loss: 0.0613  lr:0.000001
[ Sun Jul 14 10:26:17 2024 ] 
Training: Epoch [49/50], Step [2499], Loss: 0.07332159578800201, Training Accuracy: 97.82
[ Sun Jul 14 10:26:18 2024 ] 	Batch(2500/6809) done. Loss: 0.0810  lr:0.000001
[ Sun Jul 14 10:26:36 2024 ] 	Batch(2600/6809) done. Loss: 0.0033  lr:0.000001
[ Sun Jul 14 10:26:55 2024 ] 	Batch(2700/6809) done. Loss: 0.0274  lr:0.000001
[ Sun Jul 14 10:27:13 2024 ] 	Batch(2800/6809) done. Loss: 0.1614  lr:0.000001
[ Sun Jul 14 10:27:31 2024 ] 	Batch(2900/6809) done. Loss: 0.0038  lr:0.000001
[ Sun Jul 14 10:27:49 2024 ] 
Training: Epoch [49/50], Step [2999], Loss: 0.006863192189484835, Training Accuracy: 97.8
[ Sun Jul 14 10:27:50 2024 ] 	Batch(3000/6809) done. Loss: 0.0264  lr:0.000001
[ Sun Jul 14 10:28:07 2024 ] 	Batch(3100/6809) done. Loss: 0.0310  lr:0.000001
[ Sun Jul 14 10:28:25 2024 ] 	Batch(3200/6809) done. Loss: 0.0603  lr:0.000001
[ Sun Jul 14 10:28:43 2024 ] 	Batch(3300/6809) done. Loss: 0.0235  lr:0.000001
[ Sun Jul 14 10:29:01 2024 ] 	Batch(3400/6809) done. Loss: 0.0303  lr:0.000001
[ Sun Jul 14 10:29:19 2024 ] 
Training: Epoch [49/50], Step [3499], Loss: 0.19286030530929565, Training Accuracy: 97.84642857142856
[ Sun Jul 14 10:29:19 2024 ] 	Batch(3500/6809) done. Loss: 0.0453  lr:0.000001
[ Sun Jul 14 10:29:37 2024 ] 	Batch(3600/6809) done. Loss: 0.0536  lr:0.000001
[ Sun Jul 14 10:29:56 2024 ] 	Batch(3700/6809) done. Loss: 0.0676  lr:0.000001
[ Sun Jul 14 10:30:14 2024 ] 	Batch(3800/6809) done. Loss: 0.0773  lr:0.000001
[ Sun Jul 14 10:30:33 2024 ] 	Batch(3900/6809) done. Loss: 0.1349  lr:0.000001
[ Sun Jul 14 10:30:50 2024 ] 
Training: Epoch [49/50], Step [3999], Loss: 0.6128204464912415, Training Accuracy: 97.83125
[ Sun Jul 14 10:30:51 2024 ] 	Batch(4000/6809) done. Loss: 0.1101  lr:0.000001
[ Sun Jul 14 10:31:09 2024 ] 	Batch(4100/6809) done. Loss: 0.0189  lr:0.000001
[ Sun Jul 14 10:31:27 2024 ] 	Batch(4200/6809) done. Loss: 0.1333  lr:0.000001
[ Sun Jul 14 10:31:44 2024 ] 	Batch(4300/6809) done. Loss: 0.0231  lr:0.000001
[ Sun Jul 14 10:32:02 2024 ] 	Batch(4400/6809) done. Loss: 0.1415  lr:0.000001
[ Sun Jul 14 10:32:21 2024 ] 
Training: Epoch [49/50], Step [4499], Loss: 0.03283653408288956, Training Accuracy: 97.86666666666667
[ Sun Jul 14 10:32:21 2024 ] 	Batch(4500/6809) done. Loss: 0.4950  lr:0.000001
[ Sun Jul 14 10:32:40 2024 ] 	Batch(4600/6809) done. Loss: 0.0261  lr:0.000001
[ Sun Jul 14 10:32:58 2024 ] 	Batch(4700/6809) done. Loss: 0.1024  lr:0.000001
[ Sun Jul 14 10:33:16 2024 ] 	Batch(4800/6809) done. Loss: 0.0109  lr:0.000001
[ Sun Jul 14 10:33:34 2024 ] 	Batch(4900/6809) done. Loss: 0.0236  lr:0.000001
[ Sun Jul 14 10:33:51 2024 ] 
Training: Epoch [49/50], Step [4999], Loss: 0.019161056727170944, Training Accuracy: 97.8875
[ Sun Jul 14 10:33:51 2024 ] 	Batch(5000/6809) done. Loss: 0.0479  lr:0.000001
[ Sun Jul 14 10:34:09 2024 ] 	Batch(5100/6809) done. Loss: 0.1586  lr:0.000001
[ Sun Jul 14 10:34:28 2024 ] 	Batch(5200/6809) done. Loss: 0.0529  lr:0.000001
[ Sun Jul 14 10:34:45 2024 ] 	Batch(5300/6809) done. Loss: 0.0613  lr:0.000001
[ Sun Jul 14 10:35:03 2024 ] 	Batch(5400/6809) done. Loss: 0.0715  lr:0.000001
[ Sun Jul 14 10:35:21 2024 ] 
Training: Epoch [49/50], Step [5499], Loss: 0.009385170415043831, Training Accuracy: 97.88636363636364
[ Sun Jul 14 10:35:21 2024 ] 	Batch(5500/6809) done. Loss: 0.0610  lr:0.000001
[ Sun Jul 14 10:35:39 2024 ] 	Batch(5600/6809) done. Loss: 0.6100  lr:0.000001
[ Sun Jul 14 10:35:58 2024 ] 	Batch(5700/6809) done. Loss: 0.1710  lr:0.000001
[ Sun Jul 14 10:36:16 2024 ] 	Batch(5800/6809) done. Loss: 0.0039  lr:0.000001
[ Sun Jul 14 10:36:35 2024 ] 	Batch(5900/6809) done. Loss: 0.0833  lr:0.000001
[ Sun Jul 14 10:36:53 2024 ] 
Training: Epoch [49/50], Step [5999], Loss: 0.023351749405264854, Training Accuracy: 97.89166666666667
[ Sun Jul 14 10:36:53 2024 ] 	Batch(6000/6809) done. Loss: 0.0406  lr:0.000001
[ Sun Jul 14 10:37:11 2024 ] 	Batch(6100/6809) done. Loss: 0.0121  lr:0.000001
[ Sun Jul 14 10:37:29 2024 ] 	Batch(6200/6809) done. Loss: 0.0022  lr:0.000001
[ Sun Jul 14 10:37:47 2024 ] 	Batch(6300/6809) done. Loss: 0.0230  lr:0.000001
[ Sun Jul 14 10:38:05 2024 ] 	Batch(6400/6809) done. Loss: 0.0278  lr:0.000001
[ Sun Jul 14 10:38:23 2024 ] 
Training: Epoch [49/50], Step [6499], Loss: 0.16327136754989624, Training Accuracy: 97.92692307692307
[ Sun Jul 14 10:38:23 2024 ] 	Batch(6500/6809) done. Loss: 0.8595  lr:0.000001
[ Sun Jul 14 10:38:41 2024 ] 	Batch(6600/6809) done. Loss: 0.1087  lr:0.000001
[ Sun Jul 14 10:38:58 2024 ] 	Batch(6700/6809) done. Loss: 0.0061  lr:0.000001
[ Sun Jul 14 10:39:17 2024 ] 	Batch(6800/6809) done. Loss: 0.0871  lr:0.000001
[ Sun Jul 14 10:39:18 2024 ] 	Mean training loss: 0.0856.
[ Sun Jul 14 10:39:18 2024 ] 	Time consumption: [Data]01%, [Network]90%
[ Sun Jul 14 10:39:18 2024 ] Eval epoch: 50
[ Sun Jul 14 10:44:51 2024 ] 	Mean val loss of 7435 batches: 1.0696732455471778.
[ Sun Jul 14 10:44:51 2024 ] 
Validation: Epoch [49/50], Samples [47820.0/59477], Loss: 0.5264378786087036, Validation Accuracy: 80.40082721051836
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 1 : 380 / 500 = 76 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 2 : 421 / 499 = 84 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 3 : 390 / 500 = 78 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 4 : 419 / 502 = 83 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 5 : 464 / 502 = 92 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 6 : 415 / 502 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 7 : 468 / 497 = 94 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 8 : 481 / 498 = 96 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 9 : 394 / 500 = 78 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 10 : 220 / 500 = 44 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 11 : 196 / 498 = 39 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 12 : 407 / 499 = 81 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 13 : 484 / 502 = 96 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 14 : 483 / 504 = 95 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 15 : 434 / 502 = 86 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 16 : 373 / 502 = 74 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 17 : 442 / 504 = 87 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 18 : 414 / 504 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 19 : 462 / 502 = 92 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 20 : 453 / 502 = 90 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 21 : 471 / 503 = 93 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 22 : 432 / 504 = 85 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 23 : 446 / 503 = 88 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 24 : 402 / 504 = 79 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 25 : 491 / 504 = 97 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 26 : 465 / 504 = 92 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 27 : 416 / 501 = 83 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 28 : 347 / 502 = 69 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 29 : 326 / 502 = 64 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 30 : 325 / 501 = 64 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 31 : 416 / 504 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 32 : 418 / 503 = 83 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 33 : 414 / 503 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 34 : 484 / 504 = 96 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 35 : 455 / 503 = 90 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 36 : 412 / 502 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 37 : 438 / 504 = 86 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 38 : 431 / 504 = 85 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 39 : 450 / 498 = 90 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 40 : 381 / 504 = 75 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 41 : 471 / 503 = 93 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 42 : 461 / 504 = 91 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 43 : 329 / 503 = 65 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 44 : 447 / 504 = 88 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 45 : 423 / 504 = 83 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 46 : 416 / 504 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 47 : 412 / 503 = 81 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 48 : 429 / 503 = 85 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 49 : 378 / 499 = 75 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 50 : 432 / 502 = 86 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 51 : 463 / 503 = 92 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 52 : 440 / 504 = 87 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 53 : 425 / 497 = 85 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 54 : 452 / 480 = 94 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 55 : 381 / 504 = 75 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 56 : 415 / 503 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 57 : 486 / 504 = 96 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 58 : 483 / 499 = 96 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 59 : 491 / 503 = 97 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 60 : 417 / 479 = 87 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 61 : 420 / 484 = 86 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 62 : 398 / 487 = 81 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 63 : 454 / 489 = 92 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 64 : 376 / 488 = 77 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 65 : 452 / 490 = 92 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 66 : 332 / 488 = 68 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 67 : 367 / 490 = 74 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 68 : 279 / 490 = 56 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 69 : 369 / 490 = 75 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 70 : 220 / 490 = 44 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 71 : 214 / 490 = 43 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 72 : 185 / 488 = 37 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 73 : 288 / 486 = 59 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 74 : 277 / 481 = 57 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 75 : 280 / 488 = 57 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 76 : 321 / 489 = 65 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 77 : 326 / 488 = 66 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 78 : 372 / 488 = 76 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 79 : 450 / 490 = 91 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 80 : 402 / 489 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 81 : 308 / 491 = 62 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 82 : 335 / 491 = 68 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 83 : 259 / 489 = 52 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 84 : 387 / 489 = 79 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 85 : 382 / 489 = 78 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 86 : 444 / 491 = 90 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 87 : 436 / 492 = 88 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 88 : 376 / 491 = 76 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 89 : 404 / 492 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 90 : 291 / 490 = 59 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 91 : 401 / 482 = 83 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 92 : 377 / 490 = 76 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 93 : 377 / 487 = 77 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 94 : 421 / 489 = 86 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 95 : 419 / 490 = 85 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 96 : 467 / 491 = 95 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 97 : 464 / 490 = 94 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 98 : 450 / 491 = 91 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 99 : 449 / 491 = 91 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 100 : 451 / 491 = 91 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 101 : 437 / 491 = 89 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 102 : 280 / 492 = 56 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 103 : 403 / 492 = 81 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 104 : 302 / 491 = 61 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 105 : 267 / 491 = 54 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 106 : 271 / 492 = 55 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 107 : 402 / 491 = 81 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 108 : 397 / 492 = 80 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 109 : 331 / 490 = 67 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 110 : 422 / 491 = 85 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 111 : 457 / 492 = 92 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 112 : 453 / 492 = 92 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 113 : 448 / 491 = 91 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 114 : 412 / 491 = 83 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 115 : 436 / 492 = 88 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 116 : 404 / 491 = 82 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 117 : 437 / 492 = 88 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 118 : 444 / 490 = 90 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 119 : 451 / 492 = 91 %
[ Sun Jul 14 10:44:51 2024 ] Accuracy of 120 : 415 / 500 = 83 %
[ Sun Jul 14 10:44:51 2024 ] Load weights from ./prova20/epoch49_model.pt.
[ Sun Jul 14 10:50:39 2024 ] Load weights from prova20/epoch80_model.pt.
[ Sun Jul 14 10:50:39 2024 ] Eval epoch: 1
[ Sun Jul 14 10:56:23 2024 ] 	Mean test loss of 7435 batches: 1.0362929846293554.
[ Sun Jul 14 10:56:24 2024 ] 	Class1 Precision: 87.06%, Recall: 83.40%
[ Sun Jul 14 10:56:24 2024 ] 	Class2 Precision: 76.22%, Recall: 75.00%
[ Sun Jul 14 10:56:24 2024 ] 	Class3 Precision: 76.39%, Recall: 85.57%
[ Sun Jul 14 10:56:24 2024 ] 	Class4 Precision: 88.74%, Recall: 80.40%
[ Sun Jul 14 10:56:24 2024 ] 	Class5 Precision: 88.44%, Recall: 82.27%
[ Sun Jul 14 10:56:24 2024 ] 	Class6 Precision: 94.68%, Recall: 92.23%
[ Sun Jul 14 10:56:24 2024 ] 	Class7 Precision: 90.55%, Recall: 85.86%
[ Sun Jul 14 10:56:24 2024 ] 	Class8 Precision: 98.31%, Recall: 93.76%
[ Sun Jul 14 10:56:24 2024 ] 	Class9 Precision: 96.39%, Recall: 96.59%
[ Sun Jul 14 10:56:24 2024 ] 	Class10 Precision: 80.21%, Recall: 76.20%
[ Sun Jul 14 10:56:24 2024 ] 	Class11 Precision: 64.22%, Recall: 42.00%
[ Sun Jul 14 10:56:24 2024 ] 	Class12 Precision: 42.73%, Recall: 39.56%
[ Sun Jul 14 10:56:24 2024 ] 	Class13 Precision: 90.26%, Recall: 83.57%
[ Sun Jul 14 10:56:24 2024 ] 	Class14 Precision: 94.26%, Recall: 94.82%
[ Sun Jul 14 10:56:24 2024 ] 	Class15 Precision: 93.37%, Recall: 95.04%
[ Sun Jul 14 10:56:24 2024 ] 	Class16 Precision: 77.36%, Recall: 85.06%
[ Sun Jul 14 10:56:24 2024 ] 	Class17 Precision: 81.18%, Recall: 76.49%
[ Sun Jul 14 10:56:24 2024 ] 	Class18 Precision: 81.68%, Recall: 88.49%
[ Sun Jul 14 10:56:24 2024 ] 	Class19 Precision: 83.76%, Recall: 83.93%
[ Sun Jul 14 10:56:24 2024 ] 	Class20 Precision: 94.83%, Recall: 91.43%
[ Sun Jul 14 10:56:24 2024 ] 	Class21 Precision: 94.19%, Recall: 90.44%
[ Sun Jul 14 10:56:24 2024 ] 	Class22 Precision: 89.29%, Recall: 94.43%
[ Sun Jul 14 10:56:24 2024 ] 	Class23 Precision: 85.05%, Recall: 86.90%
[ Sun Jul 14 10:56:24 2024 ] 	Class24 Precision: 90.18%, Recall: 89.46%
[ Sun Jul 14 10:56:24 2024 ] 	Class25 Precision: 87.23%, Recall: 81.35%
[ Sun Jul 14 10:56:24 2024 ] 	Class26 Precision: 91.09%, Recall: 97.42%
[ Sun Jul 14 10:56:24 2024 ] 	Class27 Precision: 95.93%, Recall: 93.45%
[ Sun Jul 14 10:56:24 2024 ] 	Class28 Precision: 88.51%, Recall: 83.03%
[ Sun Jul 14 10:56:24 2024 ] 	Class29 Precision: 58.61%, Recall: 70.52%
[ Sun Jul 14 10:56:24 2024 ] 	Class30 Precision: 51.38%, Recall: 63.15%
[ Sun Jul 14 10:56:24 2024 ] 	Class31 Precision: 73.79%, Recall: 64.07%
[ Sun Jul 14 10:56:24 2024 ] 	Class32 Precision: 84.84%, Recall: 82.14%
[ Sun Jul 14 10:56:24 2024 ] 	Class33 Precision: 87.34%, Recall: 83.70%
[ Sun Jul 14 10:56:24 2024 ] 	Class34 Precision: 69.93%, Recall: 82.31%
[ Sun Jul 14 10:56:24 2024 ] 	Class35 Precision: 94.86%, Recall: 95.24%
[ Sun Jul 14 10:56:24 2024 ] 	Class36 Precision: 76.06%, Recall: 92.84%
[ Sun Jul 14 10:56:24 2024 ] 	Class37 Precision: 77.54%, Recall: 80.48%
[ Sun Jul 14 10:56:24 2024 ] 	Class38 Precision: 80.26%, Recall: 86.31%
[ Sun Jul 14 10:56:24 2024 ] 	Class39 Precision: 82.95%, Recall: 85.91%
[ Sun Jul 14 10:56:24 2024 ] 	Class40 Precision: 91.90%, Recall: 91.16%
[ Sun Jul 14 10:56:24 2024 ] 	Class41 Precision: 76.20%, Recall: 78.77%
[ Sun Jul 14 10:56:24 2024 ] 	Class42 Precision: 94.44%, Recall: 94.63%
[ Sun Jul 14 10:56:24 2024 ] 	Class43 Precision: 93.87%, Recall: 91.07%
[ Sun Jul 14 10:56:24 2024 ] 	Class44 Precision: 88.68%, Recall: 65.41%
[ Sun Jul 14 10:56:24 2024 ] 	Class45 Precision: 81.41%, Recall: 86.90%
[ Sun Jul 14 10:56:24 2024 ] 	Class46 Precision: 83.56%, Recall: 83.73%
[ Sun Jul 14 10:56:24 2024 ] 	Class47 Precision: 86.64%, Recall: 82.34%
[ Sun Jul 14 10:56:24 2024 ] 	Class48 Precision: 89.98%, Recall: 82.11%
[ Sun Jul 14 10:56:24 2024 ] 	Class49 Precision: 84.60%, Recall: 86.28%
[ Sun Jul 14 10:56:24 2024 ] 	Class50 Precision: 91.99%, Recall: 75.95%
[ Sun Jul 14 10:56:24 2024 ] 	Class51 Precision: 94.31%, Recall: 85.86%
[ Sun Jul 14 10:56:24 2024 ] 	Class52 Precision: 92.83%, Recall: 92.64%
[ Sun Jul 14 10:56:24 2024 ] 	Class53 Precision: 82.69%, Recall: 89.09%
[ Sun Jul 14 10:56:24 2024 ] 	Class54 Precision: 77.05%, Recall: 87.12%
[ Sun Jul 14 10:56:24 2024 ] 	Class55 Precision: 91.45%, Recall: 93.54%
[ Sun Jul 14 10:56:24 2024 ] 	Class56 Precision: 91.00%, Recall: 74.21%
[ Sun Jul 14 10:56:24 2024 ] 	Class57 Precision: 89.61%, Recall: 82.31%
[ Sun Jul 14 10:56:24 2024 ] 	Class58 Precision: 92.00%, Recall: 95.83%
[ Sun Jul 14 10:56:24 2024 ] 	Class59 Precision: 93.58%, Recall: 96.39%
[ Sun Jul 14 10:56:24 2024 ] 	Class60 Precision: 93.90%, Recall: 98.01%
[ Sun Jul 14 10:56:24 2024 ] 	Class61 Precision: 81.31%, Recall: 88.10%
[ Sun Jul 14 10:56:24 2024 ] 	Class62 Precision: 82.35%, Recall: 86.78%
[ Sun Jul 14 10:56:24 2024 ] 	Class63 Precision: 85.81%, Recall: 80.70%
[ Sun Jul 14 10:56:24 2024 ] 	Class64 Precision: 91.62%, Recall: 91.62%
[ Sun Jul 14 10:56:24 2024 ] 	Class65 Precision: 90.66%, Recall: 75.61%
[ Sun Jul 14 10:56:24 2024 ] 	Class66 Precision: 74.07%, Recall: 93.27%
[ Sun Jul 14 10:56:24 2024 ] 	Class67 Precision: 62.02%, Recall: 66.60%
[ Sun Jul 14 10:56:24 2024 ] 	Class68 Precision: 77.71%, Recall: 76.12%
[ Sun Jul 14 10:56:24 2024 ] 	Class69 Precision: 49.73%, Recall: 57.35%
[ Sun Jul 14 10:56:24 2024 ] 	Class70 Precision: 73.87%, Recall: 76.73%
[ Sun Jul 14 10:56:24 2024 ] 	Class71 Precision: 44.82%, Recall: 43.27%
[ Sun Jul 14 10:56:24 2024 ] 	Class72 Precision: 37.36%, Recall: 42.24%
[ Sun Jul 14 10:56:24 2024 ] 	Class73 Precision: 46.67%, Recall: 41.60%
[ Sun Jul 14 10:56:24 2024 ] 	Class74 Precision: 50.45%, Recall: 57.20%
[ Sun Jul 14 10:56:24 2024 ] 	Class75 Precision: 57.53%, Recall: 61.95%
[ Sun Jul 14 10:56:24 2024 ] 	Class76 Precision: 42.43%, Recall: 55.12%
[ Sun Jul 14 10:56:24 2024 ] 	Class77 Precision: 56.04%, Recall: 68.30%
[ Sun Jul 14 10:56:24 2024 ] 	Class78 Precision: 55.78%, Recall: 69.26%
[ Sun Jul 14 10:56:24 2024 ] 	Class79 Precision: 73.77%, Recall: 76.64%
[ Sun Jul 14 10:56:24 2024 ] 	Class80 Precision: 88.85%, Recall: 92.65%
[ Sun Jul 14 10:56:24 2024 ] 	Class81 Precision: 83.09%, Recall: 82.41%
[ Sun Jul 14 10:56:24 2024 ] 	Class82 Precision: 71.99%, Recall: 63.34%
[ Sun Jul 14 10:56:24 2024 ] 	Class83 Precision: 71.69%, Recall: 63.95%
[ Sun Jul 14 10:56:24 2024 ] 	Class84 Precision: 52.32%, Recall: 52.97%
[ Sun Jul 14 10:56:24 2024 ] 	Class85 Precision: 72.73%, Recall: 80.16%
[ Sun Jul 14 10:56:24 2024 ] 	Class86 Precision: 67.01%, Recall: 78.94%
[ Sun Jul 14 10:56:24 2024 ] 	Class87 Precision: 92.05%, Recall: 89.61%
[ Sun Jul 14 10:56:24 2024 ] 	Class88 Precision: 92.62%, Recall: 89.23%
[ Sun Jul 14 10:56:24 2024 ] 	Class89 Precision: 80.00%, Recall: 76.58%
[ Sun Jul 14 10:56:24 2024 ] 	Class90 Precision: 79.88%, Recall: 83.13%
[ Sun Jul 14 10:56:24 2024 ] 	Class91 Precision: 68.67%, Recall: 53.67%
[ Sun Jul 14 10:56:24 2024 ] 	Class92 Precision: 86.77%, Recall: 82.99%
[ Sun Jul 14 10:56:24 2024 ] 	Class93 Precision: 66.67%, Recall: 76.73%
[ Sun Jul 14 10:56:24 2024 ] 	Class94 Precision: 85.05%, Recall: 74.74%
[ Sun Jul 14 10:56:24 2024 ] 	Class95 Precision: 84.63%, Recall: 86.71%
[ Sun Jul 14 10:56:24 2024 ] 	Class96 Precision: 92.26%, Recall: 85.10%
[ Sun Jul 14 10:56:24 2024 ] 	Class97 Precision: 97.29%, Recall: 95.11%
[ Sun Jul 14 10:56:24 2024 ] 	Class98 Precision: 95.67%, Recall: 94.69%
[ Sun Jul 14 10:56:24 2024 ] 	Class99 Precision: 91.85%, Recall: 91.85%
[ Sun Jul 14 10:56:24 2024 ] 	Class100 Precision: 95.29%, Recall: 90.63%
[ Sun Jul 14 10:56:24 2024 ] 	Class101 Precision: 94.54%, Recall: 91.65%
[ Sun Jul 14 10:56:24 2024 ] 	Class102 Precision: 87.55%, Recall: 88.80%
[ Sun Jul 14 10:56:24 2024 ] 	Class103 Precision: 62.18%, Recall: 59.15%
[ Sun Jul 14 10:56:24 2024 ] 	Class104 Precision: 92.14%, Recall: 78.66%
[ Sun Jul 14 10:56:24 2024 ] 	Class105 Precision: 60.32%, Recall: 61.30%
[ Sun Jul 14 10:56:24 2024 ] 	Class106 Precision: 67.33%, Recall: 55.40%
[ Sun Jul 14 10:56:24 2024 ] 	Class107 Precision: 60.96%, Recall: 56.50%
[ Sun Jul 14 10:56:24 2024 ] 	Class108 Precision: 79.26%, Recall: 83.30%
[ Sun Jul 14 10:56:24 2024 ] 	Class109 Precision: 82.92%, Recall: 80.89%
[ Sun Jul 14 10:56:24 2024 ] 	Class110 Precision: 74.61%, Recall: 67.76%
[ Sun Jul 14 10:56:24 2024 ] 	Class111 Precision: 81.37%, Recall: 84.52%
[ Sun Jul 14 10:56:24 2024 ] 	Class112 Precision: 90.78%, Recall: 94.11%
[ Sun Jul 14 10:56:24 2024 ] 	Class113 Precision: 97.22%, Recall: 92.48%
[ Sun Jul 14 10:56:24 2024 ] 	Class114 Precision: 96.96%, Recall: 91.04%
[ Sun Jul 14 10:56:24 2024 ] 	Class115 Precision: 82.73%, Recall: 83.91%
[ Sun Jul 14 10:56:24 2024 ] 	Class116 Precision: 94.70%, Recall: 87.20%
[ Sun Jul 14 10:56:24 2024 ] 	Class117 Precision: 90.32%, Recall: 81.67%
[ Sun Jul 14 10:56:24 2024 ] 	Class118 Precision: 81.78%, Recall: 89.43%
[ Sun Jul 14 10:56:24 2024 ] 	Class119 Precision: 89.18%, Recall: 89.18%
[ Sun Jul 14 10:56:24 2024 ] 	Class120 Precision: 93.96%, Recall: 91.67%
[ Sun Jul 14 10:56:24 2024 ] 	Class1 Top1: 83.40%
[ Sun Jul 14 10:56:24 2024 ] 	Class2 Top1: 75.00%
[ Sun Jul 14 10:56:24 2024 ] 	Class3 Top1: 85.57%
[ Sun Jul 14 10:56:24 2024 ] 	Class4 Top1: 80.40%
[ Sun Jul 14 10:56:24 2024 ] 	Class5 Top1: 82.27%
[ Sun Jul 14 10:56:24 2024 ] 	Class6 Top1: 92.23%
[ Sun Jul 14 10:56:24 2024 ] 	Class7 Top1: 85.86%
[ Sun Jul 14 10:56:24 2024 ] 	Class8 Top1: 93.76%
[ Sun Jul 14 10:56:24 2024 ] 	Class9 Top1: 96.59%
[ Sun Jul 14 10:56:24 2024 ] 	Class10 Top1: 76.20%
[ Sun Jul 14 10:56:24 2024 ] 	Class11 Top1: 42.00%
[ Sun Jul 14 10:56:24 2024 ] 	Class12 Top1: 39.56%
[ Sun Jul 14 10:56:24 2024 ] 	Class13 Top1: 83.57%
[ Sun Jul 14 10:56:24 2024 ] 	Class14 Top1: 94.82%
[ Sun Jul 14 10:56:24 2024 ] 	Class15 Top1: 95.04%
[ Sun Jul 14 10:56:24 2024 ] 	Class16 Top1: 85.06%
[ Sun Jul 14 10:56:24 2024 ] 	Class17 Top1: 76.49%
[ Sun Jul 14 10:56:24 2024 ] 	Class18 Top1: 88.49%
[ Sun Jul 14 10:56:24 2024 ] 	Class19 Top1: 83.93%
[ Sun Jul 14 10:56:24 2024 ] 	Class20 Top1: 91.43%
[ Sun Jul 14 10:56:24 2024 ] 	Class21 Top1: 90.44%
[ Sun Jul 14 10:56:24 2024 ] 	Class22 Top1: 94.43%
[ Sun Jul 14 10:56:24 2024 ] 	Class23 Top1: 86.90%
[ Sun Jul 14 10:56:24 2024 ] 	Class24 Top1: 89.46%
[ Sun Jul 14 10:56:24 2024 ] 	Class25 Top1: 81.35%
[ Sun Jul 14 10:56:24 2024 ] 	Class26 Top1: 97.42%
[ Sun Jul 14 10:56:24 2024 ] 	Class27 Top1: 93.45%
[ Sun Jul 14 10:56:24 2024 ] 	Class28 Top1: 83.03%
[ Sun Jul 14 10:56:24 2024 ] 	Class29 Top1: 70.52%
[ Sun Jul 14 10:56:24 2024 ] 	Class30 Top1: 63.15%
[ Sun Jul 14 10:56:24 2024 ] 	Class31 Top1: 64.07%
[ Sun Jul 14 10:56:24 2024 ] 	Class32 Top1: 82.14%
[ Sun Jul 14 10:56:24 2024 ] 	Class33 Top1: 83.70%
[ Sun Jul 14 10:56:24 2024 ] 	Class34 Top1: 82.31%
[ Sun Jul 14 10:56:24 2024 ] 	Class35 Top1: 95.24%
[ Sun Jul 14 10:56:24 2024 ] 	Class36 Top1: 92.84%
[ Sun Jul 14 10:56:24 2024 ] 	Class37 Top1: 80.48%
[ Sun Jul 14 10:56:24 2024 ] 	Class38 Top1: 86.31%
[ Sun Jul 14 10:56:24 2024 ] 	Class39 Top1: 85.91%
[ Sun Jul 14 10:56:24 2024 ] 	Class40 Top1: 91.16%
[ Sun Jul 14 10:56:24 2024 ] 	Class41 Top1: 78.77%
[ Sun Jul 14 10:56:24 2024 ] 	Class42 Top1: 94.63%
[ Sun Jul 14 10:56:24 2024 ] 	Class43 Top1: 91.07%
[ Sun Jul 14 10:56:24 2024 ] 	Class44 Top1: 65.41%
[ Sun Jul 14 10:56:24 2024 ] 	Class45 Top1: 86.90%
[ Sun Jul 14 10:56:24 2024 ] 	Class46 Top1: 83.73%
[ Sun Jul 14 10:56:24 2024 ] 	Class47 Top1: 82.34%
[ Sun Jul 14 10:56:24 2024 ] 	Class48 Top1: 82.11%
[ Sun Jul 14 10:56:24 2024 ] 	Class49 Top1: 86.28%
[ Sun Jul 14 10:56:24 2024 ] 	Class50 Top1: 75.95%
[ Sun Jul 14 10:56:24 2024 ] 	Class51 Top1: 85.86%
[ Sun Jul 14 10:56:24 2024 ] 	Class52 Top1: 92.64%
[ Sun Jul 14 10:56:24 2024 ] 	Class53 Top1: 89.09%
[ Sun Jul 14 10:56:24 2024 ] 	Class54 Top1: 87.12%
[ Sun Jul 14 10:56:24 2024 ] 	Class55 Top1: 93.54%
[ Sun Jul 14 10:56:24 2024 ] 	Class56 Top1: 74.21%
[ Sun Jul 14 10:56:24 2024 ] 	Class57 Top1: 82.31%
[ Sun Jul 14 10:56:24 2024 ] 	Class58 Top1: 95.83%
[ Sun Jul 14 10:56:24 2024 ] 	Class59 Top1: 96.39%
[ Sun Jul 14 10:56:24 2024 ] 	Class60 Top1: 98.01%
[ Sun Jul 14 10:56:24 2024 ] 	Class61 Top1: 88.10%
[ Sun Jul 14 10:56:24 2024 ] 	Class62 Top1: 86.78%
[ Sun Jul 14 10:56:24 2024 ] 	Class63 Top1: 80.70%
[ Sun Jul 14 10:56:24 2024 ] 	Class64 Top1: 91.62%
[ Sun Jul 14 10:56:24 2024 ] 	Class65 Top1: 75.61%
[ Sun Jul 14 10:56:24 2024 ] 	Class66 Top1: 93.27%
[ Sun Jul 14 10:56:24 2024 ] 	Class67 Top1: 66.60%
[ Sun Jul 14 10:56:24 2024 ] 	Class68 Top1: 76.12%
[ Sun Jul 14 10:56:24 2024 ] 	Class69 Top1: 57.35%
[ Sun Jul 14 10:56:24 2024 ] 	Class70 Top1: 76.73%
[ Sun Jul 14 10:56:24 2024 ] 	Class71 Top1: 43.27%
[ Sun Jul 14 10:56:24 2024 ] 	Class72 Top1: 42.24%
[ Sun Jul 14 10:56:24 2024 ] 	Class73 Top1: 41.60%
[ Sun Jul 14 10:56:24 2024 ] 	Class74 Top1: 57.20%
[ Sun Jul 14 10:56:24 2024 ] 	Class75 Top1: 61.95%
[ Sun Jul 14 10:56:24 2024 ] 	Class76 Top1: 55.12%
[ Sun Jul 14 10:56:24 2024 ] 	Class77 Top1: 68.30%
[ Sun Jul 14 10:56:24 2024 ] 	Class78 Top1: 69.26%
[ Sun Jul 14 10:56:24 2024 ] 	Class79 Top1: 76.64%
[ Sun Jul 14 10:56:24 2024 ] 	Class80 Top1: 92.65%
[ Sun Jul 14 10:56:24 2024 ] 	Class81 Top1: 82.41%
[ Sun Jul 14 10:56:24 2024 ] 	Class82 Top1: 63.34%
[ Sun Jul 14 10:56:24 2024 ] 	Class83 Top1: 63.95%
[ Sun Jul 14 10:56:24 2024 ] 	Class84 Top1: 52.97%
[ Sun Jul 14 10:56:24 2024 ] 	Class85 Top1: 80.16%
[ Sun Jul 14 10:56:24 2024 ] 	Class86 Top1: 78.94%
[ Sun Jul 14 10:56:24 2024 ] 	Class87 Top1: 89.61%
[ Sun Jul 14 10:56:24 2024 ] 	Class88 Top1: 89.23%
[ Sun Jul 14 10:56:24 2024 ] 	Class89 Top1: 76.58%
[ Sun Jul 14 10:56:24 2024 ] 	Class90 Top1: 83.13%
[ Sun Jul 14 10:56:24 2024 ] 	Class91 Top1: 53.67%
[ Sun Jul 14 10:56:24 2024 ] 	Class92 Top1: 82.99%
[ Sun Jul 14 10:56:24 2024 ] 	Class93 Top1: 76.73%
[ Sun Jul 14 10:56:24 2024 ] 	Class94 Top1: 74.74%
[ Sun Jul 14 10:56:24 2024 ] 	Class95 Top1: 86.71%
[ Sun Jul 14 10:56:24 2024 ] 	Class96 Top1: 85.10%
[ Sun Jul 14 10:56:24 2024 ] 	Class97 Top1: 95.11%
[ Sun Jul 14 10:56:24 2024 ] 	Class98 Top1: 94.69%
[ Sun Jul 14 10:56:24 2024 ] 	Class99 Top1: 91.85%
[ Sun Jul 14 10:56:24 2024 ] 	Class100 Top1: 90.63%
[ Sun Jul 14 10:56:24 2024 ] 	Class101 Top1: 91.65%
[ Sun Jul 14 10:56:24 2024 ] 	Class102 Top1: 88.80%
[ Sun Jul 14 10:56:24 2024 ] 	Class103 Top1: 59.15%
[ Sun Jul 14 10:56:24 2024 ] 	Class104 Top1: 78.66%
[ Sun Jul 14 10:56:24 2024 ] 	Class105 Top1: 61.30%
[ Sun Jul 14 10:56:24 2024 ] 	Class106 Top1: 55.40%
[ Sun Jul 14 10:56:24 2024 ] 	Class107 Top1: 56.50%
[ Sun Jul 14 10:56:24 2024 ] 	Class108 Top1: 83.30%
[ Sun Jul 14 10:56:24 2024 ] 	Class109 Top1: 80.89%
[ Sun Jul 14 10:56:24 2024 ] 	Class110 Top1: 67.76%
[ Sun Jul 14 10:56:24 2024 ] 	Class111 Top1: 84.52%
[ Sun Jul 14 10:56:24 2024 ] 	Class112 Top1: 94.11%
[ Sun Jul 14 10:56:24 2024 ] 	Class113 Top1: 92.48%
[ Sun Jul 14 10:56:24 2024 ] 	Class114 Top1: 91.04%
[ Sun Jul 14 10:56:24 2024 ] 	Class115 Top1: 83.91%
[ Sun Jul 14 10:56:24 2024 ] 	Class116 Top1: 87.20%
[ Sun Jul 14 10:56:24 2024 ] 	Class117 Top1: 81.67%
[ Sun Jul 14 10:56:24 2024 ] 	Class118 Top1: 89.43%
[ Sun Jul 14 10:56:24 2024 ] 	Class119 Top1: 89.18%
[ Sun Jul 14 10:56:24 2024 ] 	Class120 Top1: 91.67%
[ Sun Jul 14 10:56:24 2024 ] 	Top1: 80.42%
[ Sun Jul 14 10:56:24 2024 ] 	Class1 Top5: 96.00%
[ Sun Jul 14 10:56:24 2024 ] 	Class2 Top5: 91.80%
[ Sun Jul 14 10:56:24 2024 ] 	Class3 Top5: 95.59%
[ Sun Jul 14 10:56:24 2024 ] 	Class4 Top5: 96.40%
[ Sun Jul 14 10:56:24 2024 ] 	Class5 Top5: 91.04%
[ Sun Jul 14 10:56:24 2024 ] 	Class6 Top5: 98.61%
[ Sun Jul 14 10:56:24 2024 ] 	Class7 Top5: 95.62%
[ Sun Jul 14 10:56:24 2024 ] 	Class8 Top5: 97.38%
[ Sun Jul 14 10:56:24 2024 ] 	Class9 Top5: 98.19%
[ Sun Jul 14 10:56:24 2024 ] 	Class10 Top5: 95.20%
[ Sun Jul 14 10:56:24 2024 ] 	Class11 Top5: 83.60%
[ Sun Jul 14 10:56:24 2024 ] 	Class12 Top5: 86.95%
[ Sun Jul 14 10:56:24 2024 ] 	Class13 Top5: 94.99%
[ Sun Jul 14 10:56:24 2024 ] 	Class14 Top5: 99.60%
[ Sun Jul 14 10:56:24 2024 ] 	Class15 Top5: 99.01%
[ Sun Jul 14 10:56:24 2024 ] 	Class16 Top5: 99.20%
[ Sun Jul 14 10:56:24 2024 ] 	Class17 Top5: 97.01%
[ Sun Jul 14 10:56:24 2024 ] 	Class18 Top5: 96.83%
[ Sun Jul 14 10:56:24 2024 ] 	Class19 Top5: 96.03%
[ Sun Jul 14 10:56:24 2024 ] 	Class20 Top5: 97.61%
[ Sun Jul 14 10:56:24 2024 ] 	Class21 Top5: 97.81%
[ Sun Jul 14 10:56:24 2024 ] 	Class22 Top5: 98.61%
[ Sun Jul 14 10:56:24 2024 ] 	Class23 Top5: 96.83%
[ Sun Jul 14 10:56:24 2024 ] 	Class24 Top5: 97.02%
[ Sun Jul 14 10:56:24 2024 ] 	Class25 Top5: 94.25%
[ Sun Jul 14 10:56:24 2024 ] 	Class26 Top5: 99.01%
[ Sun Jul 14 10:56:24 2024 ] 	Class27 Top5: 98.41%
[ Sun Jul 14 10:56:24 2024 ] 	Class28 Top5: 90.62%
[ Sun Jul 14 10:56:24 2024 ] 	Class29 Top5: 91.63%
[ Sun Jul 14 10:56:24 2024 ] 	Class30 Top5: 87.85%
[ Sun Jul 14 10:56:24 2024 ] 	Class31 Top5: 89.62%
[ Sun Jul 14 10:56:24 2024 ] 	Class32 Top5: 94.84%
[ Sun Jul 14 10:56:24 2024 ] 	Class33 Top5: 92.84%
[ Sun Jul 14 10:56:24 2024 ] 	Class34 Top5: 95.63%
[ Sun Jul 14 10:56:24 2024 ] 	Class35 Top5: 99.01%
[ Sun Jul 14 10:56:24 2024 ] 	Class36 Top5: 97.22%
[ Sun Jul 14 10:56:24 2024 ] 	Class37 Top5: 93.82%
[ Sun Jul 14 10:56:24 2024 ] 	Class38 Top5: 95.83%
[ Sun Jul 14 10:56:24 2024 ] 	Class39 Top5: 97.42%
[ Sun Jul 14 10:56:24 2024 ] 	Class40 Top5: 97.59%
[ Sun Jul 14 10:56:24 2024 ] 	Class41 Top5: 94.44%
[ Sun Jul 14 10:56:24 2024 ] 	Class42 Top5: 99.40%
[ Sun Jul 14 10:56:24 2024 ] 	Class43 Top5: 99.80%
[ Sun Jul 14 10:56:24 2024 ] 	Class44 Top5: 93.24%
[ Sun Jul 14 10:56:24 2024 ] 	Class45 Top5: 94.05%
[ Sun Jul 14 10:56:24 2024 ] 	Class46 Top5: 92.06%
[ Sun Jul 14 10:56:24 2024 ] 	Class47 Top5: 94.84%
[ Sun Jul 14 10:56:24 2024 ] 	Class48 Top5: 98.41%
[ Sun Jul 14 10:56:24 2024 ] 	Class49 Top5: 96.62%
[ Sun Jul 14 10:56:24 2024 ] 	Class50 Top5: 94.99%
[ Sun Jul 14 10:56:24 2024 ] 	Class51 Top5: 98.01%
[ Sun Jul 14 10:56:24 2024 ] 	Class52 Top5: 97.22%
[ Sun Jul 14 10:56:24 2024 ] 	Class53 Top5: 98.02%
[ Sun Jul 14 10:56:24 2024 ] 	Class54 Top5: 98.59%
[ Sun Jul 14 10:56:24 2024 ] 	Class55 Top5: 99.58%
[ Sun Jul 14 10:56:24 2024 ] 	Class56 Top5: 97.22%
[ Sun Jul 14 10:56:24 2024 ] 	Class57 Top5: 96.42%
[ Sun Jul 14 10:56:24 2024 ] 	Class58 Top5: 99.21%
[ Sun Jul 14 10:56:24 2024 ] 	Class59 Top5: 100.00%
[ Sun Jul 14 10:56:24 2024 ] 	Class60 Top5: 99.40%
[ Sun Jul 14 10:56:24 2024 ] 	Class61 Top5: 94.99%
[ Sun Jul 14 10:56:24 2024 ] 	Class62 Top5: 96.90%
[ Sun Jul 14 10:56:24 2024 ] 	Class63 Top5: 90.76%
[ Sun Jul 14 10:56:24 2024 ] 	Class64 Top5: 95.30%
[ Sun Jul 14 10:56:24 2024 ] 	Class65 Top5: 89.14%
[ Sun Jul 14 10:56:24 2024 ] 	Class66 Top5: 96.12%
[ Sun Jul 14 10:56:24 2024 ] 	Class67 Top5: 89.75%
[ Sun Jul 14 10:56:24 2024 ] 	Class68 Top5: 90.20%
[ Sun Jul 14 10:56:24 2024 ] 	Class69 Top5: 89.18%
[ Sun Jul 14 10:56:24 2024 ] 	Class70 Top5: 92.65%
[ Sun Jul 14 10:56:24 2024 ] 	Class71 Top5: 92.86%
[ Sun Jul 14 10:56:24 2024 ] 	Class72 Top5: 92.04%
[ Sun Jul 14 10:56:24 2024 ] 	Class73 Top5: 86.68%
[ Sun Jul 14 10:56:24 2024 ] 	Class74 Top5: 88.27%
[ Sun Jul 14 10:56:24 2024 ] 	Class75 Top5: 90.23%
[ Sun Jul 14 10:56:24 2024 ] 	Class76 Top5: 91.60%
[ Sun Jul 14 10:56:24 2024 ] 	Class77 Top5: 87.93%
[ Sun Jul 14 10:56:24 2024 ] 	Class78 Top5: 85.66%
[ Sun Jul 14 10:56:24 2024 ] 	Class79 Top5: 91.80%
[ Sun Jul 14 10:56:24 2024 ] 	Class80 Top5: 95.71%
[ Sun Jul 14 10:56:24 2024 ] 	Class81 Top5: 92.23%
[ Sun Jul 14 10:56:24 2024 ] 	Class82 Top5: 86.76%
[ Sun Jul 14 10:56:24 2024 ] 	Class83 Top5: 85.13%
[ Sun Jul 14 10:56:24 2024 ] 	Class84 Top5: 86.50%
[ Sun Jul 14 10:56:24 2024 ] 	Class85 Top5: 93.87%
[ Sun Jul 14 10:56:24 2024 ] 	Class86 Top5: 92.64%
[ Sun Jul 14 10:56:24 2024 ] 	Class87 Top5: 96.54%
[ Sun Jul 14 10:56:24 2024 ] 	Class88 Top5: 94.31%
[ Sun Jul 14 10:56:24 2024 ] 	Class89 Top5: 90.22%
[ Sun Jul 14 10:56:24 2024 ] 	Class90 Top5: 94.11%
[ Sun Jul 14 10:56:24 2024 ] 	Class91 Top5: 83.88%
[ Sun Jul 14 10:56:24 2024 ] 	Class92 Top5: 94.19%
[ Sun Jul 14 10:56:24 2024 ] 	Class93 Top5: 91.02%
[ Sun Jul 14 10:56:24 2024 ] 	Class94 Top5: 91.58%
[ Sun Jul 14 10:56:24 2024 ] 	Class95 Top5: 94.27%
[ Sun Jul 14 10:56:24 2024 ] 	Class96 Top5: 92.24%
[ Sun Jul 14 10:56:24 2024 ] 	Class97 Top5: 96.54%
[ Sun Jul 14 10:56:24 2024 ] 	Class98 Top5: 96.12%
[ Sun Jul 14 10:56:24 2024 ] 	Class99 Top5: 95.11%
[ Sun Jul 14 10:56:24 2024 ] 	Class100 Top5: 95.32%
[ Sun Jul 14 10:56:24 2024 ] 	Class101 Top5: 96.54%
[ Sun Jul 14 10:56:24 2024 ] 	Class102 Top5: 93.28%
[ Sun Jul 14 10:56:24 2024 ] 	Class103 Top5: 87.60%
[ Sun Jul 14 10:56:24 2024 ] 	Class104 Top5: 92.28%
[ Sun Jul 14 10:56:24 2024 ] 	Class105 Top5: 88.59%
[ Sun Jul 14 10:56:24 2024 ] 	Class106 Top5: 88.39%
[ Sun Jul 14 10:56:24 2024 ] 	Class107 Top5: 89.84%
[ Sun Jul 14 10:56:24 2024 ] 	Class108 Top5: 93.08%
[ Sun Jul 14 10:56:24 2024 ] 	Class109 Top5: 91.46%
[ Sun Jul 14 10:56:24 2024 ] 	Class110 Top5: 91.84%
[ Sun Jul 14 10:56:24 2024 ] 	Class111 Top5: 93.89%
[ Sun Jul 14 10:56:24 2024 ] 	Class112 Top5: 97.56%
[ Sun Jul 14 10:56:24 2024 ] 	Class113 Top5: 97.76%
[ Sun Jul 14 10:56:24 2024 ] 	Class114 Top5: 98.37%
[ Sun Jul 14 10:56:24 2024 ] 	Class115 Top5: 94.91%
[ Sun Jul 14 10:56:24 2024 ] 	Class116 Top5: 97.76%
[ Sun Jul 14 10:56:24 2024 ] 	Class117 Top5: 93.28%
[ Sun Jul 14 10:56:24 2024 ] 	Class118 Top5: 97.15%
[ Sun Jul 14 10:56:24 2024 ] 	Class119 Top5: 97.14%
[ Sun Jul 14 10:56:24 2024 ] 	Class120 Top5: 97.15%
[ Sun Jul 14 10:56:24 2024 ] 	Top5: 94.19%
[ Sun Jul 14 13:08:52 2024 ] Load weights from prova20/epoch20_model.pt.
[ Sun Jul 14 13:08:52 2024 ] Eval epoch: 1
[ Sun Jul 14 13:14:36 2024 ] 	Mean test loss of 7435 batches: 1.0524935411888985.
[ Sun Jul 14 13:14:36 2024 ] 	Class1 Precision: 85.54%, Recall: 82.80%
[ Sun Jul 14 13:14:36 2024 ] 	Class2 Precision: 79.22%, Recall: 73.20%
[ Sun Jul 14 13:14:36 2024 ] 	Class3 Precision: 77.64%, Recall: 85.57%
[ Sun Jul 14 13:14:36 2024 ] 	Class4 Precision: 87.55%, Recall: 81.60%
[ Sun Jul 14 13:14:36 2024 ] 	Class5 Precision: 84.25%, Recall: 85.26%
[ Sun Jul 14 13:14:36 2024 ] 	Class6 Precision: 94.94%, Recall: 93.43%
[ Sun Jul 14 13:14:36 2024 ] 	Class7 Precision: 89.86%, Recall: 86.45%
[ Sun Jul 14 13:14:36 2024 ] 	Class8 Precision: 98.73%, Recall: 93.96%
[ Sun Jul 14 13:14:36 2024 ] 	Class9 Precision: 96.60%, Recall: 96.99%
[ Sun Jul 14 13:14:36 2024 ] 	Class10 Precision: 76.78%, Recall: 80.00%
[ Sun Jul 14 13:14:36 2024 ] 	Class11 Precision: 63.66%, Recall: 41.00%
[ Sun Jul 14 13:14:36 2024 ] 	Class12 Precision: 45.48%, Recall: 36.35%
[ Sun Jul 14 13:14:36 2024 ] 	Class13 Precision: 86.69%, Recall: 86.17%
[ Sun Jul 14 13:14:36 2024 ] 	Class14 Precision: 91.68%, Recall: 96.61%
[ Sun Jul 14 13:14:36 2024 ] 	Class15 Precision: 92.65%, Recall: 95.04%
[ Sun Jul 14 13:14:36 2024 ] 	Class16 Precision: 77.58%, Recall: 85.46%
[ Sun Jul 14 13:14:36 2024 ] 	Class17 Precision: 81.56%, Recall: 74.90%
[ Sun Jul 14 13:14:36 2024 ] 	Class18 Precision: 84.84%, Recall: 87.70%
[ Sun Jul 14 13:14:36 2024 ] 	Class19 Precision: 86.72%, Recall: 82.94%
[ Sun Jul 14 13:14:36 2024 ] 	Class20 Precision: 94.88%, Recall: 92.23%
[ Sun Jul 14 13:14:36 2024 ] 	Class21 Precision: 93.95%, Recall: 92.83%
[ Sun Jul 14 13:14:36 2024 ] 	Class22 Precision: 89.29%, Recall: 94.43%
[ Sun Jul 14 13:14:36 2024 ] 	Class23 Precision: 86.48%, Recall: 86.31%
[ Sun Jul 14 13:14:36 2024 ] 	Class24 Precision: 91.19%, Recall: 88.47%
[ Sun Jul 14 13:14:36 2024 ] 	Class25 Precision: 87.55%, Recall: 79.56%
[ Sun Jul 14 13:14:36 2024 ] 	Class26 Precision: 92.99%, Recall: 97.42%
[ Sun Jul 14 13:14:36 2024 ] 	Class27 Precision: 96.91%, Recall: 93.25%
[ Sun Jul 14 13:14:36 2024 ] 	Class28 Precision: 90.57%, Recall: 82.44%
[ Sun Jul 14 13:14:36 2024 ] 	Class29 Precision: 55.04%, Recall: 70.72%
[ Sun Jul 14 13:14:36 2024 ] 	Class30 Precision: 52.17%, Recall: 59.96%
[ Sun Jul 14 13:14:36 2024 ] 	Class31 Precision: 74.39%, Recall: 67.27%
[ Sun Jul 14 13:14:36 2024 ] 	Class32 Precision: 82.50%, Recall: 85.12%
[ Sun Jul 14 13:14:36 2024 ] 	Class33 Precision: 85.17%, Recall: 84.49%
[ Sun Jul 14 13:14:36 2024 ] 	Class34 Precision: 68.35%, Recall: 83.30%
[ Sun Jul 14 13:14:36 2024 ] 	Class35 Precision: 94.52%, Recall: 95.83%
[ Sun Jul 14 13:14:36 2024 ] 	Class36 Precision: 79.12%, Recall: 93.44%
[ Sun Jul 14 13:14:36 2024 ] 	Class37 Precision: 76.30%, Recall: 82.07%
[ Sun Jul 14 13:14:36 2024 ] 	Class38 Precision: 83.14%, Recall: 87.10%
[ Sun Jul 14 13:14:36 2024 ] 	Class39 Precision: 84.58%, Recall: 82.74%
[ Sun Jul 14 13:14:36 2024 ] 	Class40 Precision: 88.10%, Recall: 92.17%
[ Sun Jul 14 13:14:36 2024 ] 	Class41 Precision: 77.91%, Recall: 78.37%
[ Sun Jul 14 13:14:36 2024 ] 	Class42 Precision: 92.98%, Recall: 94.83%
[ Sun Jul 14 13:14:36 2024 ] 	Class43 Precision: 95.03%, Recall: 91.07%
[ Sun Jul 14 13:14:36 2024 ] 	Class44 Precision: 86.33%, Recall: 67.79%
[ Sun Jul 14 13:14:36 2024 ] 	Class45 Precision: 81.14%, Recall: 87.90%
[ Sun Jul 14 13:14:36 2024 ] 	Class46 Precision: 82.47%, Recall: 84.92%
[ Sun Jul 14 13:14:36 2024 ] 	Class47 Precision: 86.51%, Recall: 82.74%
[ Sun Jul 14 13:14:36 2024 ] 	Class48 Precision: 87.78%, Recall: 85.69%
[ Sun Jul 14 13:14:36 2024 ] 	Class49 Precision: 88.26%, Recall: 86.68%
[ Sun Jul 14 13:14:36 2024 ] 	Class50 Precision: 90.80%, Recall: 77.15%
[ Sun Jul 14 13:14:36 2024 ] 	Class51 Precision: 95.74%, Recall: 85.06%
[ Sun Jul 14 13:14:36 2024 ] 	Class52 Precision: 91.28%, Recall: 93.64%
[ Sun Jul 14 13:14:36 2024 ] 	Class53 Precision: 80.83%, Recall: 88.69%
[ Sun Jul 14 13:14:36 2024 ] 	Class54 Precision: 79.47%, Recall: 84.91%
[ Sun Jul 14 13:14:36 2024 ] 	Class55 Precision: 87.33%, Recall: 94.79%
[ Sun Jul 14 13:14:36 2024 ] 	Class56 Precision: 91.69%, Recall: 72.22%
[ Sun Jul 14 13:14:36 2024 ] 	Class57 Precision: 88.91%, Recall: 82.90%
[ Sun Jul 14 13:14:36 2024 ] 	Class58 Precision: 92.13%, Recall: 95.24%
[ Sun Jul 14 13:14:36 2024 ] 	Class59 Precision: 94.12%, Recall: 96.19%
[ Sun Jul 14 13:14:36 2024 ] 	Class60 Precision: 94.42%, Recall: 97.61%
[ Sun Jul 14 13:14:36 2024 ] 	Class61 Precision: 86.09%, Recall: 87.89%
[ Sun Jul 14 13:14:36 2024 ] 	Class62 Precision: 83.84%, Recall: 85.74%
[ Sun Jul 14 13:14:36 2024 ] 	Class63 Precision: 84.62%, Recall: 81.31%
[ Sun Jul 14 13:14:36 2024 ] 	Class64 Precision: 92.04%, Recall: 92.23%
[ Sun Jul 14 13:14:36 2024 ] 	Class65 Precision: 88.44%, Recall: 76.84%
[ Sun Jul 14 13:14:36 2024 ] 	Class66 Precision: 76.05%, Recall: 92.65%
[ Sun Jul 14 13:14:36 2024 ] 	Class67 Precision: 62.33%, Recall: 64.75%
[ Sun Jul 14 13:14:36 2024 ] 	Class68 Precision: 78.95%, Recall: 76.53%
[ Sun Jul 14 13:14:36 2024 ] 	Class69 Precision: 50.18%, Recall: 57.14%
[ Sun Jul 14 13:14:36 2024 ] 	Class70 Precision: 80.18%, Recall: 74.29%
[ Sun Jul 14 13:14:36 2024 ] 	Class71 Precision: 44.87%, Recall: 42.86%
[ Sun Jul 14 13:14:36 2024 ] 	Class72 Precision: 36.48%, Recall: 41.02%
[ Sun Jul 14 13:14:36 2024 ] 	Class73 Precision: 45.99%, Recall: 39.96%
[ Sun Jul 14 13:14:36 2024 ] 	Class74 Precision: 48.36%, Recall: 57.61%
[ Sun Jul 14 13:14:36 2024 ] 	Class75 Precision: 59.50%, Recall: 59.88%
[ Sun Jul 14 13:14:36 2024 ] 	Class76 Precision: 39.36%, Recall: 58.40%
[ Sun Jul 14 13:14:36 2024 ] 	Class77 Precision: 59.78%, Recall: 68.10%
[ Sun Jul 14 13:14:36 2024 ] 	Class78 Precision: 53.41%, Recall: 69.06%
[ Sun Jul 14 13:14:36 2024 ] 	Class79 Precision: 71.35%, Recall: 77.05%
[ Sun Jul 14 13:14:36 2024 ] 	Class80 Precision: 89.35%, Recall: 92.45%
[ Sun Jul 14 13:14:36 2024 ] 	Class81 Precision: 81.31%, Recall: 83.64%
[ Sun Jul 14 13:14:36 2024 ] 	Class82 Precision: 72.13%, Recall: 60.08%
[ Sun Jul 14 13:14:36 2024 ] 	Class83 Precision: 68.55%, Recall: 66.60%
[ Sun Jul 14 13:14:36 2024 ] 	Class84 Precision: 50.56%, Recall: 55.01%
[ Sun Jul 14 13:14:36 2024 ] 	Class85 Precision: 74.95%, Recall: 78.94%
[ Sun Jul 14 13:14:36 2024 ] 	Class86 Precision: 73.20%, Recall: 74.85%
[ Sun Jul 14 13:14:36 2024 ] 	Class87 Precision: 90.47%, Recall: 90.84%
[ Sun Jul 14 13:14:36 2024 ] 	Class88 Precision: 91.81%, Recall: 88.82%
[ Sun Jul 14 13:14:36 2024 ] 	Class89 Precision: 81.05%, Recall: 75.76%
[ Sun Jul 14 13:14:36 2024 ] 	Class90 Precision: 77.52%, Recall: 82.72%
[ Sun Jul 14 13:14:36 2024 ] 	Class91 Precision: 72.00%, Recall: 51.43%
[ Sun Jul 14 13:14:36 2024 ] 	Class92 Precision: 85.32%, Recall: 84.44%
[ Sun Jul 14 13:14:36 2024 ] 	Class93 Precision: 68.81%, Recall: 74.29%
[ Sun Jul 14 13:14:36 2024 ] 	Class94 Precision: 85.75%, Recall: 76.59%
[ Sun Jul 14 13:14:36 2024 ] 	Class95 Precision: 85.95%, Recall: 83.84%
[ Sun Jul 14 13:14:36 2024 ] 	Class96 Precision: 89.48%, Recall: 85.10%
[ Sun Jul 14 13:14:36 2024 ] 	Class97 Precision: 97.49%, Recall: 94.91%
[ Sun Jul 14 13:14:36 2024 ] 	Class98 Precision: 94.14%, Recall: 95.10%
[ Sun Jul 14 13:14:36 2024 ] 	Class99 Precision: 89.31%, Recall: 91.85%
[ Sun Jul 14 13:14:36 2024 ] 	Class100 Precision: 95.12%, Recall: 91.24%
[ Sun Jul 14 13:14:36 2024 ] 	Class101 Precision: 94.94%, Recall: 91.65%
[ Sun Jul 14 13:14:36 2024 ] 	Class102 Precision: 88.57%, Recall: 88.39%
[ Sun Jul 14 13:14:36 2024 ] 	Class103 Precision: 65.66%, Recall: 57.52%
[ Sun Jul 14 13:14:36 2024 ] 	Class104 Precision: 89.06%, Recall: 82.72%
[ Sun Jul 14 13:14:36 2024 ] 	Class105 Precision: 61.16%, Recall: 62.53%
[ Sun Jul 14 13:14:36 2024 ] 	Class106 Precision: 67.89%, Recall: 56.42%
[ Sun Jul 14 13:14:36 2024 ] 	Class107 Precision: 61.59%, Recall: 53.46%
[ Sun Jul 14 13:14:36 2024 ] 	Class108 Precision: 76.85%, Recall: 82.48%
[ Sun Jul 14 13:14:36 2024 ] 	Class109 Precision: 83.12%, Recall: 80.08%
[ Sun Jul 14 13:14:36 2024 ] 	Class110 Precision: 76.81%, Recall: 64.90%
[ Sun Jul 14 13:14:36 2024 ] 	Class111 Precision: 83.33%, Recall: 85.54%
[ Sun Jul 14 13:14:36 2024 ] 	Class112 Precision: 91.27%, Recall: 93.50%
[ Sun Jul 14 13:14:36 2024 ] 	Class113 Precision: 96.21%, Recall: 92.89%
[ Sun Jul 14 13:14:36 2024 ] 	Class114 Precision: 96.93%, Recall: 90.02%
[ Sun Jul 14 13:14:36 2024 ] 	Class115 Precision: 83.30%, Recall: 83.30%
[ Sun Jul 14 13:14:36 2024 ] 	Class116 Precision: 95.37%, Recall: 88.01%
[ Sun Jul 14 13:14:36 2024 ] 	Class117 Precision: 86.23%, Recall: 82.89%
[ Sun Jul 14 13:14:36 2024 ] 	Class118 Precision: 79.68%, Recall: 90.85%
[ Sun Jul 14 13:14:36 2024 ] 	Class119 Precision: 85.80%, Recall: 90.00%
[ Sun Jul 14 13:14:36 2024 ] 	Class120 Precision: 93.68%, Recall: 90.45%
[ Sun Jul 14 13:14:37 2024 ] 	Class1 Top1: 82.80%
[ Sun Jul 14 13:14:37 2024 ] 	Class2 Top1: 73.20%
[ Sun Jul 14 13:14:37 2024 ] 	Class3 Top1: 85.57%
[ Sun Jul 14 13:14:37 2024 ] 	Class4 Top1: 81.60%
[ Sun Jul 14 13:14:37 2024 ] 	Class5 Top1: 85.26%
[ Sun Jul 14 13:14:37 2024 ] 	Class6 Top1: 93.43%
[ Sun Jul 14 13:14:37 2024 ] 	Class7 Top1: 86.45%
[ Sun Jul 14 13:14:37 2024 ] 	Class8 Top1: 93.96%
[ Sun Jul 14 13:14:37 2024 ] 	Class9 Top1: 96.99%
[ Sun Jul 14 13:14:37 2024 ] 	Class10 Top1: 80.00%
[ Sun Jul 14 13:14:37 2024 ] 	Class11 Top1: 41.00%
[ Sun Jul 14 13:14:37 2024 ] 	Class12 Top1: 36.35%
[ Sun Jul 14 13:14:37 2024 ] 	Class13 Top1: 86.17%
[ Sun Jul 14 13:14:37 2024 ] 	Class14 Top1: 96.61%
[ Sun Jul 14 13:14:37 2024 ] 	Class15 Top1: 95.04%
[ Sun Jul 14 13:14:37 2024 ] 	Class16 Top1: 85.46%
[ Sun Jul 14 13:14:37 2024 ] 	Class17 Top1: 74.90%
[ Sun Jul 14 13:14:37 2024 ] 	Class18 Top1: 87.70%
[ Sun Jul 14 13:14:37 2024 ] 	Class19 Top1: 82.94%
[ Sun Jul 14 13:14:37 2024 ] 	Class20 Top1: 92.23%
[ Sun Jul 14 13:14:37 2024 ] 	Class21 Top1: 92.83%
[ Sun Jul 14 13:14:37 2024 ] 	Class22 Top1: 94.43%
[ Sun Jul 14 13:14:37 2024 ] 	Class23 Top1: 86.31%
[ Sun Jul 14 13:14:37 2024 ] 	Class24 Top1: 88.47%
[ Sun Jul 14 13:14:37 2024 ] 	Class25 Top1: 79.56%
[ Sun Jul 14 13:14:37 2024 ] 	Class26 Top1: 97.42%
[ Sun Jul 14 13:14:37 2024 ] 	Class27 Top1: 93.25%
[ Sun Jul 14 13:14:37 2024 ] 	Class28 Top1: 82.44%
[ Sun Jul 14 13:14:37 2024 ] 	Class29 Top1: 70.72%
[ Sun Jul 14 13:14:37 2024 ] 	Class30 Top1: 59.96%
[ Sun Jul 14 13:14:37 2024 ] 	Class31 Top1: 67.27%
[ Sun Jul 14 13:14:37 2024 ] 	Class32 Top1: 85.12%
[ Sun Jul 14 13:14:37 2024 ] 	Class33 Top1: 84.49%
[ Sun Jul 14 13:14:37 2024 ] 	Class34 Top1: 83.30%
[ Sun Jul 14 13:14:37 2024 ] 	Class35 Top1: 95.83%
[ Sun Jul 14 13:14:37 2024 ] 	Class36 Top1: 93.44%
[ Sun Jul 14 13:14:37 2024 ] 	Class37 Top1: 82.07%
[ Sun Jul 14 13:14:37 2024 ] 	Class38 Top1: 87.10%
[ Sun Jul 14 13:14:37 2024 ] 	Class39 Top1: 82.74%
[ Sun Jul 14 13:14:37 2024 ] 	Class40 Top1: 92.17%
[ Sun Jul 14 13:14:37 2024 ] 	Class41 Top1: 78.37%
[ Sun Jul 14 13:14:37 2024 ] 	Class42 Top1: 94.83%
[ Sun Jul 14 13:14:37 2024 ] 	Class43 Top1: 91.07%
[ Sun Jul 14 13:14:37 2024 ] 	Class44 Top1: 67.79%
[ Sun Jul 14 13:14:37 2024 ] 	Class45 Top1: 87.90%
[ Sun Jul 14 13:14:37 2024 ] 	Class46 Top1: 84.92%
[ Sun Jul 14 13:14:37 2024 ] 	Class47 Top1: 82.74%
[ Sun Jul 14 13:14:37 2024 ] 	Class48 Top1: 85.69%
[ Sun Jul 14 13:14:37 2024 ] 	Class49 Top1: 86.68%
[ Sun Jul 14 13:14:37 2024 ] 	Class50 Top1: 77.15%
[ Sun Jul 14 13:14:37 2024 ] 	Class51 Top1: 85.06%
[ Sun Jul 14 13:14:37 2024 ] 	Class52 Top1: 93.64%
[ Sun Jul 14 13:14:37 2024 ] 	Class53 Top1: 88.69%
[ Sun Jul 14 13:14:37 2024 ] 	Class54 Top1: 84.91%
[ Sun Jul 14 13:14:37 2024 ] 	Class55 Top1: 94.79%
[ Sun Jul 14 13:14:37 2024 ] 	Class56 Top1: 72.22%
[ Sun Jul 14 13:14:37 2024 ] 	Class57 Top1: 82.90%
[ Sun Jul 14 13:14:37 2024 ] 	Class58 Top1: 95.24%
[ Sun Jul 14 13:14:37 2024 ] 	Class59 Top1: 96.19%
[ Sun Jul 14 13:14:37 2024 ] 	Class60 Top1: 97.61%
[ Sun Jul 14 13:14:37 2024 ] 	Class61 Top1: 87.89%
[ Sun Jul 14 13:14:37 2024 ] 	Class62 Top1: 85.74%
[ Sun Jul 14 13:14:37 2024 ] 	Class63 Top1: 81.31%
[ Sun Jul 14 13:14:37 2024 ] 	Class64 Top1: 92.23%
[ Sun Jul 14 13:14:37 2024 ] 	Class65 Top1: 76.84%
[ Sun Jul 14 13:14:37 2024 ] 	Class66 Top1: 92.65%
[ Sun Jul 14 13:14:37 2024 ] 	Class67 Top1: 64.75%
[ Sun Jul 14 13:14:37 2024 ] 	Class68 Top1: 76.53%
[ Sun Jul 14 13:14:37 2024 ] 	Class69 Top1: 57.14%
[ Sun Jul 14 13:14:37 2024 ] 	Class70 Top1: 74.29%
[ Sun Jul 14 13:14:37 2024 ] 	Class71 Top1: 42.86%
[ Sun Jul 14 13:14:37 2024 ] 	Class72 Top1: 41.02%
[ Sun Jul 14 13:14:37 2024 ] 	Class73 Top1: 39.96%
[ Sun Jul 14 13:14:37 2024 ] 	Class74 Top1: 57.61%
[ Sun Jul 14 13:14:37 2024 ] 	Class75 Top1: 59.88%
[ Sun Jul 14 13:14:37 2024 ] 	Class76 Top1: 58.40%
[ Sun Jul 14 13:14:37 2024 ] 	Class77 Top1: 68.10%
[ Sun Jul 14 13:14:37 2024 ] 	Class78 Top1: 69.06%
[ Sun Jul 14 13:14:37 2024 ] 	Class79 Top1: 77.05%
[ Sun Jul 14 13:14:37 2024 ] 	Class80 Top1: 92.45%
[ Sun Jul 14 13:14:37 2024 ] 	Class81 Top1: 83.64%
[ Sun Jul 14 13:14:37 2024 ] 	Class82 Top1: 60.08%
[ Sun Jul 14 13:14:37 2024 ] 	Class83 Top1: 66.60%
[ Sun Jul 14 13:14:37 2024 ] 	Class84 Top1: 55.01%
[ Sun Jul 14 13:14:37 2024 ] 	Class85 Top1: 78.94%
[ Sun Jul 14 13:14:37 2024 ] 	Class86 Top1: 74.85%
[ Sun Jul 14 13:14:37 2024 ] 	Class87 Top1: 90.84%
[ Sun Jul 14 13:14:37 2024 ] 	Class88 Top1: 88.82%
[ Sun Jul 14 13:14:37 2024 ] 	Class89 Top1: 75.76%
[ Sun Jul 14 13:14:37 2024 ] 	Class90 Top1: 82.72%
[ Sun Jul 14 13:14:37 2024 ] 	Class91 Top1: 51.43%
[ Sun Jul 14 13:14:37 2024 ] 	Class92 Top1: 84.44%
[ Sun Jul 14 13:14:37 2024 ] 	Class93 Top1: 74.29%
[ Sun Jul 14 13:14:37 2024 ] 	Class94 Top1: 76.59%
[ Sun Jul 14 13:14:37 2024 ] 	Class95 Top1: 83.84%
[ Sun Jul 14 13:14:37 2024 ] 	Class96 Top1: 85.10%
[ Sun Jul 14 13:14:37 2024 ] 	Class97 Top1: 94.91%
[ Sun Jul 14 13:14:37 2024 ] 	Class98 Top1: 95.10%
[ Sun Jul 14 13:14:37 2024 ] 	Class99 Top1: 91.85%
[ Sun Jul 14 13:14:37 2024 ] 	Class100 Top1: 91.24%
[ Sun Jul 14 13:14:37 2024 ] 	Class101 Top1: 91.65%
[ Sun Jul 14 13:14:37 2024 ] 	Class102 Top1: 88.39%
[ Sun Jul 14 13:14:37 2024 ] 	Class103 Top1: 57.52%
[ Sun Jul 14 13:14:37 2024 ] 	Class104 Top1: 82.72%
[ Sun Jul 14 13:14:37 2024 ] 	Class105 Top1: 62.53%
[ Sun Jul 14 13:14:37 2024 ] 	Class106 Top1: 56.42%
[ Sun Jul 14 13:14:37 2024 ] 	Class107 Top1: 53.46%
[ Sun Jul 14 13:14:37 2024 ] 	Class108 Top1: 82.48%
[ Sun Jul 14 13:14:37 2024 ] 	Class109 Top1: 80.08%
[ Sun Jul 14 13:14:37 2024 ] 	Class110 Top1: 64.90%
[ Sun Jul 14 13:14:37 2024 ] 	Class111 Top1: 85.54%
[ Sun Jul 14 13:14:37 2024 ] 	Class112 Top1: 93.50%
[ Sun Jul 14 13:14:37 2024 ] 	Class113 Top1: 92.89%
[ Sun Jul 14 13:14:37 2024 ] 	Class114 Top1: 90.02%
[ Sun Jul 14 13:14:37 2024 ] 	Class115 Top1: 83.30%
[ Sun Jul 14 13:14:37 2024 ] 	Class116 Top1: 88.01%
[ Sun Jul 14 13:14:37 2024 ] 	Class117 Top1: 82.89%
[ Sun Jul 14 13:14:37 2024 ] 	Class118 Top1: 90.85%
[ Sun Jul 14 13:14:37 2024 ] 	Class119 Top1: 90.00%
[ Sun Jul 14 13:14:37 2024 ] 	Class120 Top1: 90.45%
[ Sun Jul 14 13:14:37 2024 ] 	Top1: 80.45%
[ Sun Jul 14 13:14:37 2024 ] 	Class1 Top5: 96.40%
[ Sun Jul 14 13:14:37 2024 ] 	Class2 Top5: 88.80%
[ Sun Jul 14 13:14:37 2024 ] 	Class3 Top5: 95.79%
[ Sun Jul 14 13:14:37 2024 ] 	Class4 Top5: 96.60%
[ Sun Jul 14 13:14:37 2024 ] 	Class5 Top5: 93.23%
[ Sun Jul 14 13:14:37 2024 ] 	Class6 Top5: 98.41%
[ Sun Jul 14 13:14:37 2024 ] 	Class7 Top5: 96.22%
[ Sun Jul 14 13:14:37 2024 ] 	Class8 Top5: 97.79%
[ Sun Jul 14 13:14:37 2024 ] 	Class9 Top5: 98.39%
[ Sun Jul 14 13:14:37 2024 ] 	Class10 Top5: 95.40%
[ Sun Jul 14 13:14:37 2024 ] 	Class11 Top5: 84.40%
[ Sun Jul 14 13:14:37 2024 ] 	Class12 Top5: 84.74%
[ Sun Jul 14 13:14:37 2024 ] 	Class13 Top5: 96.39%
[ Sun Jul 14 13:14:37 2024 ] 	Class14 Top5: 99.60%
[ Sun Jul 14 13:14:37 2024 ] 	Class15 Top5: 99.01%
[ Sun Jul 14 13:14:37 2024 ] 	Class16 Top5: 98.80%
[ Sun Jul 14 13:14:37 2024 ] 	Class17 Top5: 97.21%
[ Sun Jul 14 13:14:37 2024 ] 	Class18 Top5: 96.03%
[ Sun Jul 14 13:14:37 2024 ] 	Class19 Top5: 95.44%
[ Sun Jul 14 13:14:37 2024 ] 	Class20 Top5: 97.21%
[ Sun Jul 14 13:14:37 2024 ] 	Class21 Top5: 98.21%
[ Sun Jul 14 13:14:37 2024 ] 	Class22 Top5: 98.81%
[ Sun Jul 14 13:14:37 2024 ] 	Class23 Top5: 97.02%
[ Sun Jul 14 13:14:37 2024 ] 	Class24 Top5: 96.62%
[ Sun Jul 14 13:14:37 2024 ] 	Class25 Top5: 94.25%
[ Sun Jul 14 13:14:37 2024 ] 	Class26 Top5: 99.01%
[ Sun Jul 14 13:14:37 2024 ] 	Class27 Top5: 98.61%
[ Sun Jul 14 13:14:37 2024 ] 	Class28 Top5: 91.22%
[ Sun Jul 14 13:14:37 2024 ] 	Class29 Top5: 91.43%
[ Sun Jul 14 13:14:37 2024 ] 	Class30 Top5: 87.65%
[ Sun Jul 14 13:14:37 2024 ] 	Class31 Top5: 89.42%
[ Sun Jul 14 13:14:37 2024 ] 	Class32 Top5: 96.23%
[ Sun Jul 14 13:14:37 2024 ] 	Class33 Top5: 94.04%
[ Sun Jul 14 13:14:37 2024 ] 	Class34 Top5: 96.02%
[ Sun Jul 14 13:14:37 2024 ] 	Class35 Top5: 99.21%
[ Sun Jul 14 13:14:37 2024 ] 	Class36 Top5: 96.82%
[ Sun Jul 14 13:14:37 2024 ] 	Class37 Top5: 95.02%
[ Sun Jul 14 13:14:37 2024 ] 	Class38 Top5: 96.03%
[ Sun Jul 14 13:14:37 2024 ] 	Class39 Top5: 96.23%
[ Sun Jul 14 13:14:37 2024 ] 	Class40 Top5: 97.59%
[ Sun Jul 14 13:14:37 2024 ] 	Class41 Top5: 93.85%
[ Sun Jul 14 13:14:37 2024 ] 	Class42 Top5: 99.40%
[ Sun Jul 14 13:14:37 2024 ] 	Class43 Top5: 99.80%
[ Sun Jul 14 13:14:37 2024 ] 	Class44 Top5: 94.04%
[ Sun Jul 14 13:14:37 2024 ] 	Class45 Top5: 94.44%
[ Sun Jul 14 13:14:37 2024 ] 	Class46 Top5: 93.85%
[ Sun Jul 14 13:14:37 2024 ] 	Class47 Top5: 94.84%
[ Sun Jul 14 13:14:37 2024 ] 	Class48 Top5: 98.81%
[ Sun Jul 14 13:14:37 2024 ] 	Class49 Top5: 96.42%
[ Sun Jul 14 13:14:37 2024 ] 	Class50 Top5: 95.59%
[ Sun Jul 14 13:14:37 2024 ] 	Class51 Top5: 97.61%
[ Sun Jul 14 13:14:37 2024 ] 	Class52 Top5: 98.41%
[ Sun Jul 14 13:14:37 2024 ] 	Class53 Top5: 98.02%
[ Sun Jul 14 13:14:37 2024 ] 	Class54 Top5: 97.99%
[ Sun Jul 14 13:14:37 2024 ] 	Class55 Top5: 99.79%
[ Sun Jul 14 13:14:37 2024 ] 	Class56 Top5: 96.63%
[ Sun Jul 14 13:14:37 2024 ] 	Class57 Top5: 96.62%
[ Sun Jul 14 13:14:37 2024 ] 	Class58 Top5: 99.40%
[ Sun Jul 14 13:14:37 2024 ] 	Class59 Top5: 100.00%
[ Sun Jul 14 13:14:37 2024 ] 	Class60 Top5: 99.40%
[ Sun Jul 14 13:14:37 2024 ] 	Class61 Top5: 94.99%
[ Sun Jul 14 13:14:37 2024 ] 	Class62 Top5: 96.07%
[ Sun Jul 14 13:14:37 2024 ] 	Class63 Top5: 91.79%
[ Sun Jul 14 13:14:37 2024 ] 	Class64 Top5: 95.50%
[ Sun Jul 14 13:14:37 2024 ] 	Class65 Top5: 89.75%
[ Sun Jul 14 13:14:37 2024 ] 	Class66 Top5: 95.92%
[ Sun Jul 14 13:14:37 2024 ] 	Class67 Top5: 88.93%
[ Sun Jul 14 13:14:37 2024 ] 	Class68 Top5: 89.80%
[ Sun Jul 14 13:14:37 2024 ] 	Class69 Top5: 88.16%
[ Sun Jul 14 13:14:37 2024 ] 	Class70 Top5: 91.43%
[ Sun Jul 14 13:14:37 2024 ] 	Class71 Top5: 93.47%
[ Sun Jul 14 13:14:37 2024 ] 	Class72 Top5: 92.04%
[ Sun Jul 14 13:14:37 2024 ] 	Class73 Top5: 87.09%
[ Sun Jul 14 13:14:37 2024 ] 	Class74 Top5: 88.89%
[ Sun Jul 14 13:14:37 2024 ] 	Class75 Top5: 90.23%
[ Sun Jul 14 13:14:37 2024 ] 	Class76 Top5: 92.21%
[ Sun Jul 14 13:14:37 2024 ] 	Class77 Top5: 86.50%
[ Sun Jul 14 13:14:37 2024 ] 	Class78 Top5: 86.89%
[ Sun Jul 14 13:14:37 2024 ] 	Class79 Top5: 92.21%
[ Sun Jul 14 13:14:37 2024 ] 	Class80 Top5: 95.31%
[ Sun Jul 14 13:14:37 2024 ] 	Class81 Top5: 93.87%
[ Sun Jul 14 13:14:37 2024 ] 	Class82 Top5: 86.35%
[ Sun Jul 14 13:14:37 2024 ] 	Class83 Top5: 87.17%
[ Sun Jul 14 13:14:37 2024 ] 	Class84 Top5: 88.75%
[ Sun Jul 14 13:14:37 2024 ] 	Class85 Top5: 93.05%
[ Sun Jul 14 13:14:37 2024 ] 	Class86 Top5: 91.62%
[ Sun Jul 14 13:14:37 2024 ] 	Class87 Top5: 95.93%
[ Sun Jul 14 13:14:37 2024 ] 	Class88 Top5: 94.51%
[ Sun Jul 14 13:14:37 2024 ] 	Class89 Top5: 89.21%
[ Sun Jul 14 13:14:37 2024 ] 	Class90 Top5: 94.11%
[ Sun Jul 14 13:14:37 2024 ] 	Class91 Top5: 81.43%
[ Sun Jul 14 13:14:37 2024 ] 	Class92 Top5: 95.23%
[ Sun Jul 14 13:14:37 2024 ] 	Class93 Top5: 90.41%
[ Sun Jul 14 13:14:37 2024 ] 	Class94 Top5: 92.20%
[ Sun Jul 14 13:14:37 2024 ] 	Class95 Top5: 94.07%
[ Sun Jul 14 13:14:37 2024 ] 	Class96 Top5: 92.24%
[ Sun Jul 14 13:14:37 2024 ] 	Class97 Top5: 96.54%
[ Sun Jul 14 13:14:37 2024 ] 	Class98 Top5: 96.12%
[ Sun Jul 14 13:14:37 2024 ] 	Class99 Top5: 95.32%
[ Sun Jul 14 13:14:37 2024 ] 	Class100 Top5: 95.72%
[ Sun Jul 14 13:14:37 2024 ] 	Class101 Top5: 96.54%
[ Sun Jul 14 13:14:37 2024 ] 	Class102 Top5: 93.28%
[ Sun Jul 14 13:14:37 2024 ] 	Class103 Top5: 86.18%
[ Sun Jul 14 13:14:37 2024 ] 	Class104 Top5: 93.50%
[ Sun Jul 14 13:14:37 2024 ] 	Class105 Top5: 88.80%
[ Sun Jul 14 13:14:37 2024 ] 	Class106 Top5: 86.76%
[ Sun Jul 14 13:14:37 2024 ] 	Class107 Top5: 87.80%
[ Sun Jul 14 13:14:37 2024 ] 	Class108 Top5: 93.08%
[ Sun Jul 14 13:14:37 2024 ] 	Class109 Top5: 92.07%
[ Sun Jul 14 13:14:37 2024 ] 	Class110 Top5: 90.61%
[ Sun Jul 14 13:14:37 2024 ] 	Class111 Top5: 93.48%
[ Sun Jul 14 13:14:37 2024 ] 	Class112 Top5: 96.95%
[ Sun Jul 14 13:14:37 2024 ] 	Class113 Top5: 97.76%
[ Sun Jul 14 13:14:37 2024 ] 	Class114 Top5: 99.19%
[ Sun Jul 14 13:14:37 2024 ] 	Class115 Top5: 94.09%
[ Sun Jul 14 13:14:37 2024 ] 	Class116 Top5: 97.76%
[ Sun Jul 14 13:14:37 2024 ] 	Class117 Top5: 93.28%
[ Sun Jul 14 13:14:37 2024 ] 	Class118 Top5: 97.56%
[ Sun Jul 14 13:14:37 2024 ] 	Class119 Top5: 97.96%
[ Sun Jul 14 13:14:37 2024 ] 	Class120 Top5: 96.34%
[ Sun Jul 14 13:14:37 2024 ] 	Top5: 94.20%
